\newpage
\section{Sampling for Benchmark}

In this section, we present additional details about the process adopted for creating the FI-Hard and EmoSet-Hard data subsets. This includes details of implementation, followed by examples of the varying difficulty levels targeted to be included through the benchmark generation process. 

\subsection{Subsampling Data based on Fine-tuning}
\label{app:finetune_vit_impl}

We first present specific implementation details of the fine-tuning process. We use the base size of ViT \cite{dosovitskiy2020vit}, pretrained on ImageNet-21K \cite{ridnik2021imagenet} and fine-tuned on ImageNet 2012 \cite{russakovsky2015imagenet}. For fine-tuning on EmoSet, we add 3 linear layers, each followed by dropout layers (p=0.2) and a non-linearity of ReLU \cite{nair2010rectified}. Training is carried out for 30 epochs, creating an 80:20 split into training and holdout sets. As the number of samples in FI is significantly smaller, we use the model pre-trained on EmoSet as the starting point for fine-tuning on FI. Making the final classification layer trainable, we update the weights of the pretrained model, based on the FI dataset. The training for FI follows a similar 80:20 split of training and unseen validation data, and is carried out for 30 epochs. In both cases, the models are optimized with Stochastic Gradient Descent, using an initial learning rate of 0.05, momentum of 0.9, and weight decay set to 0.00005. Along with SGD, Cosine Annealing scheduler is used. The objective is simply minimizing the multi-class Cross-Entropy Loss. The models are fine-tuned on single A40 GPUs with 4 cores. The total time taken for fine-tuning EmoSet and FI was around 5 hours and 3 hours respectively. 

\begin{table}[t]
\small
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{ccccc}
    \toprule
         EmoSet-Hard & FI-Hard & Abstract & ArtPhoto & Emotion6 \\
         \midrule
         2998 & 2976 & 250 & 805 & 1980 \\
         \bottomrule
    \end{tabular}}
    \caption{The final number of images in each of datasets used in our benchmark and evaluation experiments.}
    \label{tab:data_stats}
\end{table}


\begin{figure*}
    \centerline{
    \includegraphics[width=\textwidth]{latex/media/class_distribution.pdf}
    }
    \caption{The distribution of different emotion classes in the final evaluation sets considered. The numbers of samples in different emotion classes, in EmoSet-Hard and FI-Hard are proportional to the original class distribution in the candidate sets they are obtained from by subsampling.}
    \label{fig:data_class_dist}
\end{figure*}

\begin{figure*}
    \centerline{
    \includegraphics[width=0.8\textwidth]{latex/media/appendix/class_distribution_fine.pdf}
    }
    \caption{The distribution of different emotion classes in the final evaluation sets considered, grouped according to the broader Sentiment and Arousal categories. The grouping is shown only for the datasets considered in fine-grained class-specific analysis.}
\end{figure*}

Once the model is fine-tuned on the entire EmoSet and FI datasets, as described in the main body, the prediction probabilities are used to further filter out the most obvious or easy samples. The probability values for the correctly classified samples in EmoSet are in the range \([0.31, 1.0]\), and for FI are within in \([0.32, 1.0]\), with most probability values lying above 0.9. We choose the threshold of 0.8 for both EmoSet and FI, keeping the value close to the average of the range, but slightly higher, to account for the higher frequency of probability values greater than 0.9. Thus, the final data subsets contain samples that are either incorrectly predicted by the fine-tuned model, or are predicted correctly with probability less than 0.8. Intuitively, it includes examples that are harder to classify, contain less obvious expressions of emotion, or can potentially belong to multiple emotion classes. The final numbers of samples in each data subset is described in Table \ref{tab:data_stats}. In the final subsamples, we also retain the original emotion class distribution of each dataset, as represented through Fig. \ref{fig:data_class_dist}. As we do not train or fine-tune any models, the varied class distribution is not detrimental to our analysis. Further, to account for the unequal class distribution, we report the weighted F1 scores for all analysis.

\begin{figure*}
    \centerline{
    \includegraphics[scale=0.55]{latex/media/appendix/emoset_example_full.pdf}}
    \caption{Examples from the created \textbf{EmoSet-Hard} dataset. For Contentment and Excitement, no instances are found that are predicted correctly with a probability less than 0.8. For all other categories, the two leftmost examples describe instances that are correctly predicted, but with a probability less than 0.8. The next example shows an image predicted incorrectly. Finally, the rightmost example for all categories show the correctly predicted samples, which have probability of prediction higher than 0.8.}
    \label{fig:benchmark_example_emoset}
\end{figure*}

\begin{figure*}
    \centerline{
    \includegraphics[scale=0.8]{latex/media/appendix/fi_example_full.pdf}}
    \caption{Examples from the created \textbf{FI-Hard} dataset. Similar to EmoSet-Hard, for Contentment and Sadness, no instances are found that are predicted correctly with a probability less than 0.8. For all other categories, the two leftmost examples describe instances that are correctly predicted, but with a probability less than 0.8. The next example shows an image predicted incorrectly. Finally, the rightmost example for all categories show the correctly predicted samples, which have probability of prediction higher than 0.8.}
    \label{fig:benchmark_example_fi}
\end{figure*}

\subsection{Manual Analysis of Difficulty of Images}
\label{app:benchmark_difficulty}

We present examples from EmoSet-Hard and FI-Hard to demonstrate the qualitative difference in difficulty in predicting evoked emotions. Note that the \textit{examples may contain images that evoke strong negative emotions in the viewer}. As seen in Fig. \ref{fig:benchmark_example_emoset} and Fig. \ref{fig:benchmark_example_fi}, for each emotion category, the first 3 samples from the left are included in the final datasets, as they are either predicted correctly with a probability below 0.8 or are predicted incorrectly by the fine-tuned ViT models. 

Consider the examples from EmoSet-Hard described in Fig. \ref{fig:benchmark_example_emoset}. From the examples for \textit{Amusement}, the image with children is classified with the highest probability of belonging to this emotion class, followed by the image showing toys. The image of the squirrel, although predicted to belong to the Amusement class, is done so with a significantly lower probability. This hints at the bias within the dataset that leads models to associate certain elements in the image (children, toys, amusement parks, etc.) to the emotion class of Amusement. Thus, images with relatively uncommon elements, which may or may not be commonly associated with the Amusement emotion class, are included in the EmoSet-Hard set.
Another example of this can be seen in the images shown for \textit{Anger}, \textit{Disgust} and \textit{Fear} classes, where images that are more colorful or show toys or small children are classified into the Amusement category, disregarding the deeper context within the images. Further, as seen in the incorrectly classified example from the category of \textit{Awe}, the facial expressions of the children in the image lead the image to be misclassified to belong to Sadness. Thus, instances with relatively more uncommon elements are included in the EmoSet-Hard set based on our strategy. 

The examples from FI-Hard, as shown in Fig. \ref{fig:benchmark_example_fi} also testify to more difficult samples being chosen. The set includes images containing visual elements that can easily be correlated with certain emotion classes, but originally belong to different emotion categories. For instance, the misclassified images shown under \textit{Disgust} and \textit{Fear} categories contain toys, or colorfully dressed people. They are included in the FI-Hard dataset as potentially difficult instances to predict. 

\begin{figure*}
    \centering
    \includegraphics[width=0.8\textwidth]{latex/media/appendix/prompt1.pdf}
    \caption{The prompt Simple Multimodal Classification}
    \label{fig:exp1_prompt}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=0.7\textwidth]{latex/media/shuffled_prompt_1.pdf}
    \caption{The prompt for shuffled order of emotions with positive emotions first.}
    \label{fig:shuffled_prompt_1}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=0.7\textwidth]{latex/media/shuffled_prompt_2.pdf}
    \caption{The prompt for shuffled order of emotions with negative emotions first.}
    \label{fig:shuffled_prompt_2}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=0.7\textwidth]{latex/media/no_label_prompt.pdf}
    \caption{The prompt for open-vocabulary emotion prediction.}
    \label{fig:no_labeL_prompt}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=0.7\textwidth]{latex/media/positive_persona_prompt.pdf}
    \caption{The prompt for adopting positive persona.}
    \label{fig:positive_persona_prompt}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=0.7\textwidth]{latex/media/negative_persona_prompt.pdf}
    \caption{The prompt for adopting negative persona.}
    \label{fig:negative_persona_prompt}
\end{figure*}


\begin{figure*}
    \centering
    \includegraphics[width=0.7\textwidth]{latex/media/appendix/prompt2.pdf}
    \caption{The prompt Explanation-based Reasoning}
    \label{fig:exp2_prompt}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=0.7\textwidth]{latex/media/appendix/prompt4.pdf}
    \caption{The prompt for Contextual Reasoning}
    \label{fig:exp3_prompt}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=0.7\textwidth]{latex/media/appendix/prompt3.pdf}
    \caption{The prompt Caption-Based Reasoning}
    \label{fig:exp4_prompt}
\end{figure*}

\section{Main Experiments}

In this section, we provide additional details for all of our experiments. This includes details of implementation such as the resources, time or specific prompts used, and supplemental results.  

\subsection{Implementation Details}
\label{app:exp_impl}

To evaluate all of the open models, we use Huggingface \footnote{https://huggingface.co/models}. GPT4-o is evaluated using the OpenAI API \footnote{https://openai.com/index/openai-api/}. The open models are loaded in their full sizes, and run using GPUs (A40 with four cores). The maximum number of tokens to be generated is capped at 160, and is sufficient for all experiments. The time taken for the evaluation is influenced by the evaluation format, with the format of contextual reasoning (Section \ref{sec:reasoning}) taking the longest time, owing to the higher number of tokens required to be generated as output. The results reported are obtained through single runs of each type of prompt, owing to the significant computational and monetary resources required for using the models. 

\subsection{Emotion Properties Analyzed}
\label{app:emotion_properties}

We provide a categorization of the fine-grained emotion classes into broader positive and negative sentiment categories in Table \ref{tab:emotion_property_classification}. Note that we do this only for the emotion categories belonging to the popular 8-class emotion model \cite{mikels2005emotional}, as we consider only the constituent datasets adhering to this model of classification for the fine-grained analysis.
\begin{table}[t]
\small
    \centering
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{ccc}
    \toprule
         Arousal/Sentiment & Positive & Negative \\
         \midrule
         High Arousal & Amusement, Excitement, & Fear, Anger, \\
         & Awe & Disgust \\
         Low Arousal & Contentment & Sadness \\
         \bottomrule
    \end{tabular}}
    \caption{Categorization of fine-grained emotion classes based on the broader Sentiment class and Arousal levels.}
    \label{tab:emotion_property_classification}
\end{table}

\subsection{Prompts Used}
\label{app:prompts}

We include the exact prompts included in this section, in the Figures \ref{fig:exp1_prompt} \ref{fig:shuffled_prompt_1}, \ref{fig:shuffled_prompt_2}, \ref{fig:no_labeL_prompt}, \ref{fig:positive_persona_prompt}, \ref{fig:negative_persona_prompt}, \ref{fig:exp2_prompt}, \ref{fig:exp3_prompt}, and \ref{fig:exp4_prompt}. 

We use a specific template format in the prompts, with the first line of each prompt being the following: "Imagine you are like a human, capable of feeling emotions, and an
image is shown to you.". We include this specifically to bypass content moderation policies in some models, that were otherwise leading the models to abstain from responding for some image samples. Although there was no overtly offensive or obscene content in the datasets we rely on, a large number of samples depict extreme (negative) emotions. We observed by experimenting with and without this specific starting line, that providing this warning helped in obtaining responses for most of the image samples. 

Also, in the current stage of our study, we include only zero-shot prompting strategies for evaluation. At the time of conducting experiments, some of the models included in the evaluation framework were incapable of reasoning over multiple visual inputs. Thus, providing other models with visual few-shot examples would give them an unfair edge. However, we do experiment with few-shot examples in textual form (results not included in this paper) for a small subset of the data. Precisely, we provide a caption-like description of images, along with the corresponding emotion evoked. We observe that this leads to further confused responses for models like LLaVA, and thus avoid using any few-shot examples for our large-scale evaluation experiments.

\subsection{Additional Results}
\label{app:additional_results}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{latex/media/appendix/expected_difference.pdf}
    \caption{Average Difference between the most similar emotion class label and the next most similar emotion class label, given any model prediction, for both correct and incorrect predictions.}
    \label{fig:expected_diff_fine_grained}
\end{figure}

\begin{table*}[t]
    \centering
    \resizebox{\textwidth}{!}{
    \begin{tabular}{ccccccccc}
    \toprule
    Model Family & Amusement & Awe & Contentment & Excitement & Anger & Disgust & Fear & Sadness  \\
    \midrule
    GPT & 0.35 & 0.38 & \worst{0.31} & 0.49 & 0.36 & \best{0.63} & 0.54 & 0.40 \\
    LLaVA & 0.45 & 0.15 & 0.36 & 0.24 & \worst{0.09} & \best{0.47} & 0.21 & 0.37 \\
    LLaVA-Next & 0.44 & 0.24 & 0.31 & 0.37 & \worst{0.06} & \best{0.50} & 0.35 & 0.37 \\
    Qwen-VL & 0.29 & 0.27 & 0.25 & \best{0.52} & \worst{0.22} & 0.38 & 0.50 & 0.28\\
    \bottomrule
    \end{tabular}}
    \caption{Aggregated class-wise F1 scores for Simple Multimodal Classification. Model families include the F1 scores of each constituent model of different sizes (applicable for LLaVa and LLaVA-Next). The top-most F1 score achieved by each model family, across all fine-grained emotion classes, is highlighted in green, while the worst score is highlighted in red.}
    \label{tab:fine-grained-classwise-f1}
\end{table*}

\begin{table*}
    \centering
    \resizebox{\textwidth}{!}{
    \begin{tabular}{ccccc}
    \toprule
    Model Family & Positive-High Arousal & Positive-Low Arousal & Negative-High Arousal & Negative-Low Arousal \\
    \midrule
    GPT & 0.41 & \worst{0.31} & \best{0.51} & 0.40 \\ 
    LLaVA & 0.28 & 0.36 & \worst{0.26} & \best{0.37} \\ 
    LLaVA-Next & 0.35 & 0.31 & \worst{0.30} & \best{0.37} \\ 
    Qwen-VL & 0.36 & \worst{0.25} & \best{0.37} & 0.28 \\
    \bottomrule
    \end{tabular}}
    \caption{F1 scores for each (Sentiment, Arousal) category, averaged across model types, datasets, for the simple multimodal classification setting. The best and worst overall score for each model is highlighted in green and red respectively.}
    \label{tab:fine-grained-arousal}
\end{table*}

\begin{table*}
    \centering
    \resizebox{\textwidth}{!}{
    \begin{tabular}{ccccccccc}
    \toprule
    Model Family & Amusement & Awe & Contentment & Excitement & Anger & Disgust & Fear & Sadness  \\
    \midrule
    GPT & 0.41 & 0.30 & \worst{0.22} & 0.55 & \best{0.92} & 0.63 & 0.57 & 0.31 \\
    LLaVA & 0.42 & \worst{0.13} & 0.27 & 0.31 & \best{0.97} & 0.33 & 0.57 & 0.28 \\
    LLaVA-Next & 0.42 & 0.33 & \worst{0.23} & 0.75 & \best{0.95} & 0.43 & 0.50 & 0.28 \\
    Qwen-VL & 0.54 & 0.42 & \worst{0.15} & 0.41 & \best{0.85} & 0.42 & 0.58 & 0.45 \\
    \bottomrule
    \end{tabular}}
    \caption{Aggregated class-wise Precision scores for Simple Multimodal Classification. Model families include the Precision scores of each constituent model of different sizes (applicable for LLaVa and LLaVA-Next). The top-most Precision score achieved by each model family, across all fine-grained emotion classes, is highlighted in green, while the worst score is shown in red.}
    \label{tab:fine-grained-classwise-precision}
\end{table*}

\begin{table*}
    \centering
    \resizebox{\textwidth}{!}{
    \begin{tabular}{ccccccccc}
    \toprule
    Model Family & Amusement & Awe & Contentment & Excitement & Anger & Disgust & Fear & Sadness  \\
    \midrule
    GPT & 0.30 & 0.52 & 0.52 & 0.44 & \worst{0.23} & \best{0.62} & 0.51 & 0.55 \\
    LLaVA & 0.54 & 0.18 & 0.57 & 0.19 & \worst{0.05} & \best{0.78} & 0.15 & 0.60 \\
    LLaVA-Next & 0.48 & 0.38 & 0.50 & 0.26 & \worst{0.03} & \best{0.64} & 0.28 & 0.57 \\
    Qwen-VL & 0.20 & 0.20 & \best{0.75} & \best{0.75} & \worst{0.12} & 0.36 & 0.43 & 0.20 \\
    \bottomrule
    \end{tabular}}
    \caption{Aggregated class-wise Recall scores for Simple Multimodal Classification. Model families include the Recall scores of each constituent model of different sizes (applicable for LLaVa and LLaVA-Next). The top-most Recall score achieved by each model family, across all fine-grained emotion classes, is highlighted in green, while the worst scores are shown in red.}
    \label{tab:fine-grained-classwise-recall}
\end{table*}


\subsubsection{Fine-Grained Class-Wise Performance}

We present some additional results concerning the fine-grained performance of models. Table \ref{tab:fine-grained-classwise-f1} shows the average F1 scores achieved by each model family on the fine-grained emotion classes. The results are calculated by averaging scores on the EmoSet-Hard, FI-Hard, Abstract and ArtPhoto subsets of our benchmark, as they follow the 8-class classification of emotions. Interestingly, all model families, apart from Qwen-VL, consistently achieve the highest individual F1 score on the fine-grained category of disgust. Further, all models other than GPT4-o show the worst performance on the category of Anger. This is also in line with results presented in Section \ref{sec:exp_rq1}, where GPT4-o is seen to perform significantly better on negative emotion categories. In Table \ref{tab:fine-grained-arousal}, we also present an aggregate of the F1 scores, by grouping emotions based on the sentiment (positive or negative) and arousal (high or low) categories. 

We also show the class-wise Precision and Recall in Table \ref{tab:fine-grained-classwise-precision} and \ref{tab:fine-grained-classwise-recall}. It is worth noting that the precision scores for Anger are consistently the highest, while it is also the category with the worst F1 score for most models. In contrast to that, the recall scores for Anger are consistently the lowest, showing that a high number of false negatives affects the overall performance of models on this category the most. The recall scores are the highest for the Disgust category (except for Qwen-VL), which is also the class where models achieve the highest F1 scores. Overall, a complementary relationship can be seen for the precision and recall scores for most categories, and can be investigated deeply for further analyses and improvements in future work.

\subsubsection{Predicting Emotions Without Any Labels}
\label{app:no_label}

We study the capability of models to make fine-grained, distinct emotion predictions in further detail, in addition to the results presented in Section \ref{sec:no_label}. Recall that for all open-vocabulary prediction experiments, we calculate the semantic similarity of the model prediction with all emotion label classes, and assign the prediction to the class with the highest similarity. We now try to understand whether the model predicts an emotion that is truly closest semantically to a single emotion class, or it predicts a generic emotion word that could be considered almost as similar to multiple other emotion classes. In other words, we consider whether the maximum similarity score is significantly different from the second-largest similarity score between a given model prediction and the original emotion classes. Formally, given a model prediction \(o_i\), and the set of original class labels \(C\), we first calculate the maximum similarity to assign the prediction to a particular label class: 

\begin{equation}
\label{eq:maximum_similarity}
s_{\text{max}} = \max_{k}\, (\,sim\,(\,o_i, c_k\,))\, \forall\, c_k \in C
\end{equation}

Using this, we assign \(o_i\) to the label class as follows:

\begin{equation}
\label{eq:maximum_similarity_class}
j = \argmax_k\, (\,sim\,(\,o_i, c_k\,))\, \forall\, c_k \in C   
\end{equation}

\begin{equation}
\label{eq:prediction}
    \text{prediction}(o_i) = c_j
\end{equation}

Now, we calculate the second-largest similarity score as follows: 
\begin{equation}
\label{eq:second_maximum_similarity}
    s_{\text{second\_max}} = \max_{k}\, (\,sim\,(\,o_i, c_k\,))\, \forall\, c_k \in C \setminus c_j
\end{equation}
Then, we calculate the difference between the maximum possible similarity and the second maximum similarity between the model prediction and all the original label classes using Equations \ref{eq:maximum_similarity} and \ref{eq:second_maximum_similarity}, as: 

\begin{equation}
    {d} = s_{\text{max}} - s_{\text{second\_max}}
\end{equation}
Intuitively, \(d\) represents how different the final assigned class label is from the next most likely class label. We plot the expected values of \(d\) for all models in Fig. \ref{fig:expected_diff_fine_grained}. LLaVA (particularly LLaVA 13B) makes the most clearly distinguished predictions among all other models, as demonstrated by the highest expected difference between the maximum and second maximum similarity score for its predictions. Also, for all models, the difference is clearer when they make predictions that are eventually determined to belong to the correct label class. For incorrect predictions, the difference is much smaller, meaning that model predictions are unclear and have a close likelihood to belong to multiple different classes. GPT4-o makes the least well-distinguished predictions, which also agrees with the results presented in Fig. \ref{fig:no_label_f1} on the percentage of fine-grained predictions made by each model.


\subsubsection{Reasoning-Based Experiments}

We present additional results for the reasoning-based experiments. In Table \ref{tab:exp1_accuracies}, we first present the accuracy scores using the simple classification format where the corresponding model operation is \(M_{\text{c}}(\cdot)\). The best and worst-performing models are highlighted using green and red respectively. In Tables \ref{tab:exp2_accuracies}, \ref{tab:exp3_accuracies} and \ref{tab:exp4_accuracies}, we present the accuracy scores of models when prompted with explanation-based, contextual, and caption-based reasoning strategies respectively. For all of these tables, we use the green color to highlight the best-performing model for each dataset. Further, using orange, we designate the model that performed the worst for a particular dataset in the simplest setting (can be verified from Table \ref{tab:exp1_accuracies}) and accompany that with the changes due to the intervention applied. We show through this that the reasoning-based prompting strategies lead to improvements for most of the worst-performing initial combinations. 


\begin{table*}
    \centering
    \small
    \begin{tabular}{cccccc}
        \toprule
         Model & Emotion6 & Abstract & ArtPhoto & FI & EmoSet (Hard) \\
         \midrule
         Qwen-VL & 54.2 & 26.5 & \colorbox{red!20}{35.7} & \colorbox{red!20}{31.3} & 42.5 \\
         \midrule
         LLaVA (7B) & \colorbox{red!20}{42.1} & \colorbox{green!20}{\textbf{29.1}} & 41.0 & \colorbox{green!20}{\textbf{59.2}} & \colorbox{red!20}{20.4} \\
         LLaVA (13B) & 59.6 & 20.1 & 42.8 & 38.8 & 34.7 \\
         \midrule
         LLaVA-NEXT (Vicuna 7B) & 57.9 & 27.3 & 40.7 & 56.8 & 26.2 \\
         LLaVA-NEXT (Mistral 7B) & 61.0 & \colorbox{red!40}{13.4} & 43.1 &  38.2 & 37.3 \\
         LLaVA-NEXT (Vicuna 13B) & 58.8 & 16.1 & 45.7 & 44.6 & 34.7 \\
         \midrule
         GPT4-o & \colorbox{green!40}{\textbf{66.4}} & 18.8 & \colorbox{green!20}{\textbf{48.4}} & 45.5 & \colorbox{green!20}{\textbf{45.6}} \\
         \midrule  
    \end{tabular}
    \caption{Average Accuracy Scores for Simple Multimodal Classification. The best and worst-performing models on each dataset
are highlighted in green and red colors respectively. The overall best-performing model is shown in a brighter
green color (GPT4-o on Emotion6), whereas the overall worst-performing model is shown in a brighter red color
(LLaVA-NEXT (Mistral 7B) on Abstract)}
    \label{tab:exp1_accuracies}
\end{table*}

\begin{table*}
    \centering
    \small
    \resizebox{\textwidth}{!}{
    \begin{tabular}{cccccc}
        \toprule
         Model & Emotion6 & Abstract & ArtPhoto & FI & EmoSet (Hard) \\
         \midrule
         Qwen-VL & 53.99 & \best{26.7} & \improved{39.87} \increase{(+4.11)} & \improved{27.94} \decrease{(-3.4)} & 45.26 \\
         \midrule
         LLaVA (7B) & \improved{49.5} \increase{(+7.3)} & 20.9 & 39.74 & 52.71 & \improved{19.6} \decrease{(-0.8)} \\
         LLaVA (13B) & 59.40 & 25.61 & 42.52 & 39.48 & 31.12 \\
         \midrule
         LLaVA-NEXT (Vicuna 7B) & 59.24 & 20.45  & 41.24 & \best{55.77} & 29.61 \\
         LLaVA-NEXT (Mistral 7B) & 61.45 & \improved{14.03} \increase{(+0.62)} & 42.74 & 40.31 & 35.91  \\
         LLaVA-NEXT (Vicuna 13B) & 55.73 & 20.71 & 45.43 \decrease & 43.68 & 35.66 \\
         \midrule
         GPT4-o & \best{66.11} & 17.72 & \best{48.83} & 43.98 & \best{45.45} \\
         \midrule  
    \end{tabular}}
    \caption{Accuracy Scores for Classification with Explanations. The changes in accuracy points (\%) compared to simple classification are shown alongside the actual values for the originally worst-performing models. The highest scores achieved are highlighted in green.}
    \label{tab:exp2_accuracies}
\end{table*}

\begin{table*}
    \centering
    \small
    \resizebox{\textwidth}{!}{
    \begin{tabular}{cccccc}
        \toprule
         Model & Emotion6 & Abstract & ArtPhoto & FI & EmoSet (Hard) \\
         \midrule
         Qwen-VL & 54.5 & 25.45 & \improved{34.2} \decrease{(-1.6)} & \best{44.4} \increase{(+13.05)} & 32.6 \\
         \midrule
         LLaVA (7B) & \improved{51.0} \increase{(+8.84)} & 18.4 & 27.9 & 36.77 & \improved{23.1} \increase{(+2.7)} \\
         LLaVA (13B) & 56.26  & 23.53 & 37.96 & 29.43 & 41.4  \\
         \midrule
         LLaVA-NEXT (Vicuna 7B) & 49.66 & 19.42 & 24.8 & 23.6 & 22.3 \\
         LLaVA-NEXT (Mistral 7B) & 59.5 & \improved{25.5} \increase{(+12.07)} & 36.9 & 33.0 & 46.17 \\
         LLaVA-NEXT (Vicuna 13B) & 57.48 & 23.21 & 38.5 & 39.15 & 34.6 \\
         \midrule
         GPT4-o & \best{64.72} & \best{30.35} & \best{49.57} & 41.0 & \best{48.34} \\
         \midrule  
    \end{tabular}}
    \caption{Accuracy Scores for Classification with Contextual Reasoning. The changes in accuracy points (\%) compared to simple classification are shown alongside the actual values for the originally worst-performing models. The highest scores achieved are highlighted in green.}
    \label{tab:exp3_accuracies}
\end{table*}

\begin{table*}
    \centering
    \small
    \resizebox{\textwidth}{!}{
    \begin{tabular}{cccccc}
        \toprule
         Model & Emotion6 & Abstract & ArtPhoto & FI & EmoSet (Hard) \\
         \midrule
         Qwen-VL & 56.67 & 17.51 & \improved{33.72} \decrease{(-2.04)} & \improved{31.56} \increase{(+0.2)} & 38.4 \\
         \midrule
         LLaVA (7B) & \improved{43.53} \increase{(+1.4)} & \best{25.9} & 38.43 & \best{44.04} & \improved{35.6} \increase{(+15.15)} \\
         LLaVA (13B) & 54.4 & 22.92 & 36.67 & 29.3 & 38.33 \\
         \midrule
         LLaVA-NEXT (Vicuna 7B) & 50.95 & 21.33 & 40.0 & 34.65 & 38.0  \\
         LLaVA-NEXT (Mistral 7B) & 60.58 & \improved{15.41} \increase{(+2.01)} & 43.71 & 33.57 & \best{47.41} \\
         LLaVA-NEXT (Vicuna 13B) & 55.5 & 23.04 & 39.16 & 31.67 & 40.69 \\
         \midrule
         GPT4-o & \best{65.82} & 20.77 & \best{48.82} & 40.65 & 46.21 \\
         \midrule  
    \end{tabular}}
    \caption{Accuracy Scores for Classification with Caption-Based Reasoning. The changes in accuracy points (\%) compared to simple classification are shown alongside the actual values for the originally worst-performing models. The highest scores achieved are highlighted in green.}
    \label{tab:exp4_accuracies}
\end{table*}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{latex/media/appendix/humeval_datasets.pdf}
    \caption{The frequency of agreement with model predicted label (cream/beige), the dataset ground truth label (green), both labels (yellow), and neither label (orange), for each dataset, averaged across all models studied.}
    \label{fig:humeval_datasets}
\end{figure}

\subsubsection{Error Analysis and Human Evaluation}
\label{app:humeval}

We provide additional details about the analysis of model errors and the manual evaluation conducted. To create the different error categories, as we characterize specific emotion classes based on their sentiment and arousal, we focus only on EmoSet-Hard, FI-Hard, Abstract and ArtPhoto, as they use the same 8-class labels. Based on the 3 different categories of errors defined in Section \ref{sec:exp_rq3}, we sample around 10 error examples for each category, for each model and dataset. It leads to a total of around 500 error samples being annotated. We consider only the smallest variants for LLaVA (7B) and LLaVA-Next (Vicuna 7B) for the current stage of the study. Our annotators are primarily graduate student volunteers from the authors' team. We specifically use annotations from people aware of research in computing for emotions, owing to several reasons: (a) the current stage of the study is small-scale, (b) clear knowledge about evoked emotions and nuanced emotion categories is valuable for our study, and (c) to ensure high-quality annotations, which is usually compromised when aggregating large numbers of crowd-sourced annotations. The annotators are located geographically within North America. For each annotation turn, the visual stimuli is displayed for about 3-5 seconds, along with the model prediction and the dataset ground truth. However, it is not disclosed which label is model prediction and which one is the ground truth to eliminate any bias in annotation. 

In addition to the results we present in Section \ref{sec:exp_rq3}, we also include an additional analysis of how human agreement varies with each dataset considered. We plot the average frequency (across all models) of human agreement with the model predictions, ground truth label or both, for each dataset in Fig. \ref{fig:humeval_datasets}. For all datasets, on average, human annotations agree with the model predictions more often. This is not the case only for Abstract, where human agreement with the dataset ground truth is significantly higher than with model predictions, meaning that Abstract provides the most reliable ground truth labels. 

% \begin{table*}
%     \centering
%     \begin{tabular}{cccccc}
%         \toprule
%          Model & Emotion6 & Abstract & ArtPhoto & FI & EmoSet (Hard) \\
%          \midrule
%          Qwen-VL & 0.584 & 0.181 & 0.525 & \colorbox{red!20}{0.528} & 0.511 \\
%          \midrule
%          LLaVA (7B) & \colorbox{red!20}{0.546} & 0.221 & \colorbox{red!20}{0.364} & 0.533 & \colorbox{red!20}{0.264} \\
%          LLaVA (13B) & 0.619 & 0.291 & 0.539 & 0.582 & 0.585 \\
%          \midrule
%          LLaVA-NEXT (Vicuna 7B) & 0.614 & \colorbox{red!40}{0.148} & 0.538 & \colorbox{green!20}{\textbf{0.629}} & 0.603 \\
%          LLaVA-NEXT (Mistral 7B) & 0.613 & 0.135 & 0.565 & 0.546 & 0.601 \\
%          LLaVA-NEXT (Vicuna 13B) & \colorbox{green!40}{\textbf{0.644}} & 0.258 & 0.560 & 0.602 & 0.622 \\
%          \midrule
%          GPT4-o & 0.633 & \colorbox{green!20}{\textbf{0.303}} & \colorbox{green!20}{\textbf{0.590}} & 0.596 & \colorbox{green!20}{\textbf{0.633}} \\
%          \midrule  
%     \end{tabular}
%     \caption{Precision Scores for Simple Multimodal Classification. The best and worst precision scores on each dataset
% are highlighted in green and red colors respectively.}
%     \label{tab:exp1_precision}
% \end{table*}

% \begin{table*}
%     \centering
%     \begin{tabular}{cccccc}
%         \toprule
%          Model & Emotion6 & Abstract & ArtPhoto & FI & EmoSet (Hard) \\
%          \midrule
%          Qwen-VL & 0.542 & 0.265 & \colorbox{red!20}{0.357} & \colorbox{red!20}{0.313} & 0.425 \\
%          \midrule
%          LLaVA (7B) & \colorbox{red!20}{0.421} & \colorbox{green!20}{0.29} & 0.410 & \colorbox{green!20}{0.592} & \colorbox{red!20}{0.204} \\
%          LLaVA (13B) & 0.596 & 0.201 & 0.428 & 0.388 & 0.347 \\
%          \midrule
%          LLaVA-NEXT (Vicuna 7B) & 0.579 & 0.273 & 0.407 & 0.568 & 0.263 \\
%          LLaVA-NEXT (Mistral 7B) & 0.61 & \colorbox{red!40}{0.134} & 0.431 & 0.382 & 0.373 \\
%          LLaVA-NEXT (Vicuna 13B) & 0.589 & 0.161 & 0.457 & 0.446 & 0.348 \\
%          \midrule
%          GPT4-o & \colorbox{green!40}{\textbf{0.664}} & 0.188 & \colorbox{green!20}{\textbf{0.483}} & 0.455 & \colorbox{green!20}{\textbf{0.456}} \\
%          \midrule  
%     \end{tabular}
%     \caption{Recall Scores for Simple Multimodal Classification. The best and worst recall scores for each dataset
% are highlighted in green and red colors respectively.}
%     \label{tab:exp1_recall}
% \end{table*}









% Additional Emotion Property Analyzed:   \textit{Arousal Bias:} In addition to the distinction made along the Valence dimension, we further categorize fine-grained emotion classes along the Arousal dimension. This leads to the creation of four sub-categories: Positive Low Arousal Emotions (eg. Contentment), Positive High Arousal Emotions (eg. Amusement, Excitement, etc.), Negative Low Arousal Emotions (eg. Sadness) and Negative High Arousal Emotions (eg. Fear, Anger, etc.). We consider the bias exhibited by the model to high or low arousal emotions, within the same sentiment category (positive or negative). Given a ground truth label \(l\), model predicted class \(c\), a set of positive low arousal emotions \(S_{PL}\), positive high arousal emotions \(S_{PH}\), negative low arousal emotions \(S_{NL}\) and negative high arousal emotions \(S_{NH}\), we calculate the four biases as follows: 
%     \begin{equation}
%         p_{pl} = p\,(c \in S_{PL}\, |\, l \in S_{PH}),
%     \end{equation}
%     \begin{equation}
%         p_{ph} = p\,(c \in S_{PH}\, |\, l \in S_{PL}),
%     \end{equation}
%     \begin{equation}
%         p_{nl} = p\,(c \in S_{NL}\, |\, l \in S_{NH}),
%     \end{equation}    
%     \begin{equation}
%         p_{nh} = p\,(c \in S_{NH}\, |\, l \in S_{NL}).
%     \end{equation}
%     Intuitively, the biases calculate how often the model prefers an emotion category with opposing arousal properties.  

% Similarly, we also study the models' preference toward high or low-arousal emotions. For positive emotions, as shown in Fig. \ref{fig:positive_arousal_bias_simple}, all models other than Qwen-VL show a higher preference for high arousal emotions (eg., excitement, amusement, awe) over low arousal emotions (eg, contentment). For negative emotions (Fig. \ref{fig:negative_arousal_bias_simple}), the preference is more divided: Qwen-VL and GPT4-o prefer high-arousal emotions whereas the LLaVA family prefers low-arousal categories. The difference in arousal bias is however much more pronounced for positive emotions compared to negative emotion categories. Qwen-VL also shows the largest difference in biases, both for sentiment bias, and negative arousal bias. 