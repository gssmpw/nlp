\section{Related Work}
\label{sec:related_work}
 
Methods studying emotions using LLMs have included using theories grounded in psychology to develop evaluation metrics \cite{wang2023emotional, regan2024can}, generating explanations given suitable image-emotion pairs \cite{deng2023socratis}. Efforts have also been made in the direction of fine-tuning LLMs like LLaMA \cite{touvron2023llama}, BLOOM \cite{workshop2022bloom} to create experts on emotional understanding, through instruction tuning \cite{liu2024emollms}. Training-free enhancement methods have been approached to create emotionally conditioned generations for downstream tasks like image captioning or generating a news headline \cite{li2024enhancing}. 

Few recent works also study emotions with multimodal language models. A recent method proposes visual instruction tuning to improve the performance of open models in evoked emotion prediction \cite{xie2024emovit}, using a resource-intensive method of generating synthetic data and fine-tuning models. Another recent effort evaluates Vision-Language Models (VLMs) for expressed emotion recognition, but includes only a single dataset, and depends on auxiliary models to complete intermediate tasks for the VLMs being evaluated \cite{etesam2024contextual}. Vision-language models have also been employed to generate additional contextual information which is used subsequently for training a Q-Former-based module for expressed emotion prediction \cite{xenos2024vllms}. Despite these promising recent research efforts in the area of emotional understanding with VLMs, to the best of our knowledge, the capabilities of advanced Vision-Language Models in \textit{evoked emotion recognition} have thus far not been comprehensively analyzed. 

% Thus, in this paper, we critically evaluate popular VLMs on important datasets for evoked emotion recognition, probing their capabilities in multiple settings, and diving deep into the results to derive implications for the way datasets are created and evaluated for emotional understanding. 

% difference between the recent work Contextual Emotion Recognition using Large Vision Language Models: 
% 1. comprehensive analysis, includes multiple datasets, models, and difficulty levels 
% 2. does not require additional training or fine-tuning or the help of auxiliary models for segmentation or captioning 
% 3. does not aim at comparing with additional baselines as that is already done by this paper - still our methods achieve higher values of F1 
% 4. This work is NOT evoked emotion recognition
