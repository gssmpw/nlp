\section{Introduction}
\label{sec:introduction}

Equipping Artificial Intelligence (AI) systems with the capability to understand emotions is important for sensitive and effective interaction with human users in diverse applications \cite{kolakowska2014emotion, zhao2018predicting, yang2021stimuli, wang2023unlocking}. This has been approached in the past through development of deep architectures, including multimodal and context-aware methods suited for specific downstream applications \cite{lee2019context, mittal2020emoticon, hoang2021context}. The advent of Large Language Models (LLMs), however, has brought about a significant shift in focus. LLMs are now adapted or tuned to achieve what task-specific deep learning models were employed for. As a first step in understanding the inherent capabilities of popular large general-purpose models, before adapting them for specific tasks, LLMs have been evaluated through multi-faceted benchmarking experiments. This ranges from evaluating LLMs in objective \cite{hendrycks2021measuring, lu2022learn} and subjective task settings \cite{ziems2022moral, khandelwal2024moral, fung2024massively}. 

Studies exploring emotions in the context of LLMs span both benchmarking and tuning efforts \cite{xie2024emovit, xenos2024vllms, etesam2024contextual}. Several works focus on evaluating text-only language models for emotional capabilities \cite{liu2024emollms, wang2023emotional} or the use of emotional stimuli to enhance the performance of LLMs in other tasks \cite{li2023large, li2024good}. A few recent works also venture beyond the single modality of text, to approximate the human process of emotion perception more closely. Such works focus primarily on tuning large Vision-language models (VLMs) \cite{xie2024emovit, xenos2024vllms, etesam2024contextual}. However, most of the recent explorations concentrate either on specific datasets and models or directly target resource-intensive instruction tuning without highlighting the specific need to do so. While they present impressive results on overall quantitative metrics, there remains a notable lack of comprehensive and critical evaluation studies to illuminate the precise capabilities, weaknesses, and vulnerabilities of large models when performing emotion recognition in a multimodal setting. 

To address this gap, in this paper, we present an extensive evaluation of popular VLMs for emotion recognition. We analyze their performance from lenses of accuracy and robustness, while also characterizing the causes for errors made by them. We investigate the specific task of evoked emotion recognition, because of (a) its widespread practical relevance in domains such as social interactions \cite{wieser2012reduced, jyoti2016survey, awal2021angrybert}, online e-commerce \cite{sanchez2020opinion}, artistic content creation and recommendation \cite{wang2023unlocking}, etc., and, (b) the non-trivial nature of the task, involving simultaneous multimodal and affective understanding to use implicit affective cues to predict exact, detailed emotions \cite{wang2023unlocking}, which is different from application-oriented tasks where the emotion information is atleast partially present with the model \cite{deng2023socratis, li2024enhancing}. In evaluating VLMs for evoked emotion recognition, we specifically ask the following research questions:  
\begin{itemize}
    \item \textit{RQ1:} How well do VLMs recognize evoked emotions given images and a textual prompt? 
    \item \textit{RQ2:} How robust are the models to minor and major variations in the prompts? 
    \item \textit{RQ3:} What are the types of errors seen in the VLM responses and why do they occur? 
\end{itemize}

We first compile existing image-based emotion datasets to create an \underline{Ev}oked \underline{E}motion benchmark of challenging difficulty, \textsc{EvE}. Using \textsc{EvE}, we evaluate 7 popular VLMs on the task of evoked emotion recognition. Beyond presenting metrics of correctness, in our analysis, we delve deep into additional aspects such as preference exhibited by models towards certain sentiments. We design 8 different settings to study the robustness of models to perturbations in prompts. These include shuffling the order of emotion labels in prompts, open-vocabulary classification, adopting emotional perspectives, and using self-reasoning mechanisms. Finally, we create a formal framework to analyze mistakes made by VLMs and conduct a human study to localize the causes of such mistakes. 

Our key findings show that at the current state, VLMs are inept at predicting emotions evoked by images. We show that VLMs are significantly sensitive to the order in which class labels are presented in the prompts, and perform poorly when no labels are presented. We find that prompting VLMs to adopt an emotional persona has a drastic negative impact on their performance. We also observe that self-reasoning mechanisms help in the case of certain models. This is especially applicable for mechanisms that involve breaking the emotion recognition task down into more tractable sub-components (eg., captioning + reasoning). Finally, through our human study, we elucidate that factors leading to the poor VLM performance pertain not only to the model capabilities but also depend on the data used and task difficulty. We use our findings to further discuss important considerations to improve the emotion perception capability of VLMs. \footnote{We make all code and data available at: \url{https://github.com/sreebhattacharyya/Eve_Benchmark}}.

% % ---------------------------------------------------------------------------------------------------------------------------------------------

% Equipping Artificial Intelligence (AI) systems with the capability to understand emotions is a major component of creating more sensitive and accessible systems. Research in Affective Computing \cite{picard2000affective} has focused on this overarching goal by primarily addressing the tasks of emotion recognition and generation. Emotion recognition is sub-categorized into evoked and expressed emotion recognition, studying emotions evoked in individuals or explicitly expressed through emotional actions \cite{wang2023unlocking}. It has previously been approached using deep learning-based multimodal and context-aware methods  \cite{lee2019context, mittal2020emoticon, hoang2021context}, and has widespread applications in creating empathetic personal assistants, assisting human-computer interaction, aiding in mental health interventions, or regulating social network behaviors positively \cite{kolakowska2014emotion, zhao2018predicting, yang2021stimuli, wang2023unlocking}. 

% The advent of Large Language Models (LLMs) has shifted focus on using them to achieve what task-specific deep learning models were employed for. Language models have been benchmarked for their capabilities in understanding subjective human phenomena such as social knowledge \cite{yin2022geomlama}, morals \cite{ziems2022moral, khandelwal2024moral}, or culture \cite{fung2024massively, liu2023multilingual}. Studies exploring emotional understanding in language models have focused on benchmarking text-only models \cite{liu2024emollms}, fine-tuning or instruction-tuning using specific models \cite{xie2024emovit, xenos2024vllms}, or on specific emotion datasets \cite{etesam2024contextual}. Most of these works focus on presenting singular quantitative metrics defining the performance of large models, and lack depth of analysis, which is crucial to determine whether large models can be employed for emotion-related tasks in practical settings. Further, focusing on text-only LLMs precludes the inclusion of broader context. A multimodal study also resembles practical scenarios more closely, where humans are exposed to multifaceted stimuli, such as in conversations, on social media, or when interacting with a personal assistant system. A thorough analysis of the current capabilities and weaknesses of large models in multimodal emotion recognition is thus important to determine what precise improvements are needed, and how to achieve them. 

% To comprehensively evaluate and understand the multimodal emotion recognition capabilities of large models, in this work, we firstly present an emotion recognition benchmark Eve. Using the benchmark, we assess the performance of a large number of models, and critically analyze their performance, using multiple evaluation settings. We seek to primarily answer the following research questions: 


% In answering these research questions, our key contributions are as follows: 

% \begin{itemize}
%     \item We present \textsc{EvE} - a modified multimodal \underline{ev}oked \underline{e}motion analysis benchmark, suited for the evaluation of VLMs. The benchmark is created using existing emotion datasets and contains instances that are especially difficult to classify. 

%     \item We conduct an extensive analysis on a range of VLMs, including GPT4-omni, and the open-source models LLaVA (7B and 13B) \cite{liu2024visual}, LLaVA-NEXT (Vicuna 7B, Mistral 7B, Vicuna 13B) \cite{liu2024llavanext}, and Qwen-VL \cite{bai2023qwen}, for evoked emotion prediction. The evaluation includes simple classification, studying variations in prompts like shuffled order of class labels, giving the model a persona, and different prompting strategies with reasoning.  
    
%     \item We investigate the different types of errors by the models, and categorize them into different groups. FINISH WRITING THIS.
%     \footnote{\url{https://anonymous.4open.science/r/Eve_Benchmark-B55A/}}.
% \end{itemize}
 