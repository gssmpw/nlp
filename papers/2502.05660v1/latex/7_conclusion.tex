\section{Conclusion}
\label{sec:conclusion}

In this paper, we present the first extensive evaluation of popular vision-language models for the task of evoked emotion recognition. We first introduce a sizeable and representative benchmark for the task. Using several probing strategies, we study how well VLMs perform emotion recognition zero-shot, including whether they display biases to certain emotions, are sensitive to perturbations in prompts, or are capable of self-reasoning for emotion prediction. We then analyze the different types of errors made by the models, and aim to identify potential causes through a human study. Finally, we include a broader discussion on the usage of VLMs for emotion-related tasks, and useful practices for emotion research, especially when releasing new datasets or benchmarks. We hope our study highlights the need 