\section{Heavy State Selection}\label{sec:heavy_state_selection}
\begin{algorithm}[t]
    \DontPrintSemicolon
    \SetAlgoLined
    \caption{Heavy State Selection}\label{alg:heavy_state_selection}
    \KwIn{Subcircuit outputs $p_{j,i}^{\{e|e\in E_j\}}$ for subcircuit $C_j$, edge permutation $e$ and binary state $i$.
    Maximum number of states $s_{\max}$.}
    Initialize $x_j=\emptyset$, $\forall j$.\;
    \ForEach{$C_j$, $i$}{
        Compute the $L2$ norm for all edge permutations $||p_{j,i}||\equiv\sqrt{\sum_{e}(p_{j,i}^e)^2}$.\;
        Add $arg\max||p_{j,i}||$ to $x_j$.\;
        Remove $\max||p_{j,i}||$.\;
    }
    Sort $||p_{j,i}||$ in descending order.\;
    \While{$\prod_j|x_j|<s_{\max}$}{
        Pop $||p_{j,i}||$ and select binary state $i$ of $C_j$.\;
        Add $i$ to $x_j$.\;
    }
    \Return{Heavy states $x_j$ for each $C_j$}
\end{algorithm}

While tensor network contraction significantly reduces the co-processing overhead,
it still incurs significant memory overhead to reconstruct the full $2^n$ Hilbert space.
When running QC applications, users do not seek every single binary state amplitude,
but rather are only interested to know the ones with higher amplitudes.
TensorQC proposes an automatic Heavy State Selection (HSS) protocol to only reconstruct the most important binary states.
This technique greatly reduces the co-processing overhead,
while retaining  most of the amplitude weights.

Algorithm~\ref{alg:heavy_state_selection} outlines the HSS algorithm to prune subcircuit outputs prior to tensor network contraction.
For a given subcircuit entry $p_j^{\{e\in E_j\}}$,
HSS computes the $L2$ norm of every binary state,
and retains the subcircuit binary states with higher weights.
HSS hence only retains the most significant subcircuit output binary states.
Suppose HSS ends up retaining $|x_j|$ binary states for each subcircuit $C_j$,
the output dimension reduces from $2^n$ to $\prod_j |x_j|$.