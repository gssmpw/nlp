\section{Hybrid Cost Model}\label{sec:cost_model}
DHQC incorporates both quantum and classical computational costs,
presenting a trade-off between the complexity of tensor networks and the quantum resources required.
Numerous cuts lead to complex tensor networks,
increasing classical costs but also producing smaller subcircuits that require smaller QPUs.
Conversely, fewer cuts may not significantly reduce subcircuit sizes,
but result in simpler tensor networks that are easier to contract.

To estimate QPU runtime,
we assume the following operational times using IBM QPU data~\cite{ibm_quantum}:
\begin{enumerate}
    \item One layer of quantum gates runs in $t_g=10^{-7}$ seconds.
    \item Quantum measurements take $t_m=10^{-6}$ seconds.
\end{enumerate}

Additionally, TensorQC configures the number of shots equal to the number of quantum states per subcircuit,
with limits ranging from $2^{10}$ to $2^{20}$.
We define the following parameters for subcircuits:
\begin{enumerate}
    \item $u_i$: Number of upstream cut qubits in subcircuit $C_i$.
    \item $d_i$: Number of downstream cut qubits in subcircuit $C_i$.
    \item $w_i$: Width or the number of qubits in subcircuit $C_i$.
    \item $t_i$: Depth or the number of quantum gate layers in subcircuit $C_i$.
\end{enumerate}
The total QPU runtime in seconds is calculated as:
\begin{equation}
    T_{QPU}\equiv \sum_{i=1}^m 3^{u_i}\times4^{d_i}\times2^{w_i}\times(t_i\times t_g + t_m)\label{eq:qpu_runtime}
\end{equation}
The subcircuits can be run in parallel since there is no quantum data dependency between them,
allowing for execution on multiple QPUs simultaneously.
Consequently, having more QPUs linearly decreases the QPU runtime.

The tensor network contraction sequence is optimized using heuristic algorithms to minimize classical computation costs,
quantified by the number of floating point operations (FLOPs).
The classical runtime is then estimated by dividing the total contraction cost by the computational throughput of the backend classical system:
\begin{equation}
    T_{classical}\equiv \frac{C_{TN}}{FLOPs}\label{eq:classical_runtime}
\end{equation}
where $C_{TN}$ is the cost of contracting a tensor network and $FLOPs$ is the floating point operations per second of a backend GPU.

Overall, TensorQC seeks to minimize the sum of QPU and classical co-processing runtimes.