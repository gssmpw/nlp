\subsection{What: Adapting Tensor Network Contraction}\label{sec:tensor_network}
\begin{figure}[t]
    \centering
    \begin{subfigure}{0.4\textwidth}
        \centering
        \includegraphics[width=.8\textwidth]{figures/tensor_contraction.png}
        \caption{A pairwise tensor contraction with $1$ shared index $j$,
        which is the inner dimension being contracted.
        $i,k$ are the outer dimensions of the resulting big tensor $C$.
        The contraction cost is defined as the number of multiplications required.}
        \label{fig:tensor_contraction}
    \end{subfigure}
    \begin{subfigure}{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/subcircuit_contraction.png}
        \caption{Reconstructing a pair of subcircuits means multiplying and summing over the cut edges in between.}
        \label{fig:subcircuit_contraction}
    \end{subfigure}
    \caption{Reconstructing two subcircuits is equivalent to a pairwise tensor contraction.}
    \label{fig:tensor_network_equivalence}
\end{figure}
Tensor networks have been widely used in classical simulations of quantum systems~\cite{vidal2003efficient,vidal2004efficient,schollwock2011density,verstraete2004matrix,tindall2023efficient}.
Tensors are essentially multidimensional arrays.
For example,
scalars are $0$-dimensional tensors,
vectors are $1$-dimensional tensors,
matrices are $2$-dimensional tensors, and so on.
Additionally, the indices of a tensor serve as coordinates that pinpoint the location of elements within the tensor.
The dimension of a tensor index indicates the number of elements along a specific axis.

A tensor network is a graphical representation that decomposes a large tensor into a product of smaller tensors interconnected through their indices.
Figure~\ref{fig:tensor_contraction} illustrates this concept with a tensor network that represents a large tensor $C$ using two smaller tensors $A$ and $B$.
In this example, $i, j, k$ denote the tensor indices.
Tensor $A$ has dimensions $\dim(i) \times \dim(j)$, tensor $B$ has dimensions $\dim(j) \times \dim(k)$,
and the resultant tensor $C$ has dimensions $\dim(i) \times \dim(k)$.
This specific network features a single shared index $j$ between $A$ and $B$.
Generally, pairs of tensors in a network can share multiple indices,
which facilitates more complex interactions and dimensional relationships within the network.

Tensor contraction involves merging smaller tensors into a larger tensor through multiplication and summation along their shared indices.
Figure~\ref{fig:tensor_contraction} provides a straightforward example of this process using just two tensors, $A$ and $B$.
Specifically, to contract tensors $A$ and $B$,
their elements are multiplied and then summed over the common index $j$.
The outcome of this contraction is a larger tensor $C$,
which is indexed by $i$ and $k$.
Furthermore, the computational cost of this operation,
in terms of floating point multiplications,
is determined by the product $\dim(i) \times \dim(j) \times \dim(k)$,
representing the combined dimensions of the indices involved in the contraction.

Figure~\ref{fig:subcircuit_contraction} illustrates the process of reconstructing two subcircuits,
treating each subcircuit output as a tensor indexed by the cut edges $e\in E_j$.
Varying the bases on these cut edges generates different output vectors $p_j^{\{e\}}$ for each subcircuit.
Notably, each cut edge is associated with a dimension of $4$,
corresponding to the permutation of bases $\{I,X,Y,Z\}$.
The reconstruction of a pair of subcircuits involves adjusting the bases on the shared cut edges,
multiplying the respective outputs,
and summing the results.
This process of combining the outputs from two subcircuits is mathematically analogous to contracting a pair of tensors,
where the tensors represent the outputs of the subcircuits.

This equivalence can be extended to any number of subcircuits.
Typically, a tensor network comprising $m$ tensors corresponds to dividing a circuit into $m$ subcircuits.
The shared indices between these tensors represent the cut edges connecting the subcircuits.
The process of multiplying the outputs from these subcircuits is mathematically analogous to contracting the tensor network,
which involves a sequence of $m-1$ pairwise contractions.
Similarly, the reconstruction of subcircuits entails sequentially combining pairs of subcircuits until all are merged into a single output.