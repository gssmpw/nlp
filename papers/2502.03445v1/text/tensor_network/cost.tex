\subsection{Tensor Network Contraction Cost}\label{sec:contraction_cost}
\begin{figure}[t]
    \centering
    \begin{subfigure}{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/contraction_step_1.pdf}
        \caption{Contracting subcircuits $1$ and $2$ requires $4^1\times4^2=64$ multiplications.}
        \label{fig:contraction_step_1}
    \end{subfigure}
    \begin{subfigure}{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/contraction_step_2.pdf}
        \caption{Contracting the already contracted subcircuits $1,2$ and $3$ requires $4^1\times4^1\times4^1=64$ multiplications.}
        \label{fig:contraction_step_2}
    \end{subfigure}
    \begin{subfigure}{0.2\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/contraction_step_3.pdf}
        \caption{Contracting the already contracted subcircuits $1,2,3$ and $4$ requires $4^2=16$ multiplications.}
        \label{fig:contraction_step_3}
    \end{subfigure}
    \caption{Contracting the tensor network example from Figure~\ref{fig:prior_inefficiencies}.
    The solid boxes show the pair of subcircuits being contracted.
    The dashed edges inside the boxes are the inner dimensions at every contraction.
    The edges across the boundary of the boxes are the outer dimensions at every contraction.
    Tensor network contraction only requires $144$ multiplications in total.}
    \label{fig:contraction_steps}
\end{figure}

The computation cost of the co-processing is captured by the number of floating point multiplications.
Consider the computation cost to reconstruct one quantum state from the subcircuits in Figure~\ref{fig:prior_inefficiencies} using the prior method.
The prior method separately computes $4^4$ cut edge bases.
Each edge base permutation is a product of four subcircuits,
hence three multiplications.
Consequently, the prior method requires $4^4\times3=768$ floating point multiplications.

On the other hand,
Figure~\ref{fig:contraction_steps} demonstrates the application of tensor network contraction to process the same subcircuits.
This method involves sequentially multiplying subcircuit outputs until all are fully contracted.
According to Figure~\ref{fig:tensor_network_equivalence},
the cost of contracting a pair of subcircuits is determined by the product of the dimensions of all the cut edges.
With each cut edge cycling through four bases,
$\dim(e_i)=4,\forall i\in\{1,2,3,4\}$.
As a result, tensor network contraction of the same subcircuits requires only $144$ multiplications,
thereby reducing the computational expense by more than fivefold in this straightforward example.

Tensor networks offer a substantial reduction in the reconstruction costs associated with circuit cutting.
However, minimizing the overhead of tensor contraction remains challenging due to its complexity.
A general tensor network with $|E|$ edges can be contracted in $|E|!$ different sequences.
Although every sequence ultimately yields the correct result,
the associated contraction costs vary significantly.
Indeed, it is not uncommon for some sequences to be intractable,
while others are many orders of magnitude cheaper to compute.
The cost of contraction is influenced by several factors,
including the structure of the network and the dimensions of the tensors involved.
Regrettably, determining the optimal contraction sequence is an NP-hard problem,
making it computationally challenging to identify the most efficient approach~\cite{markov2008simulating, arnborg1987complexity, chi1997optimizing}.

Calculating an upper bound for the contraction complexity during subcircuit reconstruction is relatively straightforward.
Figure~\ref{fig:contraction_steps} shows that the total contraction cost is the sum of the costs of each individual contraction step.
The cost for each step is determined by the combined dimensions of both the internal and external cut edges of a tensor pair,
known as the \emph{contraction edges}.
Therefore, the cost for each contraction step is $4^{\#cuts}$,
where $\#cuts$ represents the total number of internal and external cuts involved in each contraction step.
Consequently, the overall contraction cost is upper-bounded by the following expression:
\begin{equation}
    \mathcal{O}(4^{K_{\max}}m)\label{eq:tn_upper_bound}
\end{equation}
where $K_{\max}$ is the max number of cut edges at any contraction step,
and $m$ is the number of subcircuits.
In contrast, the computation cost of the prior method is given by:
\begin{equation}
    \mathcal{O}(4^{|E|}m)\label{eq:prior_complexity}
\end{equation}

\subsection{Exponential Advantage from Tensor Network}
Let us analyze the exponential benefits of tensor network contractions through the following scenarios:
\begin{enumerate}
    \item Two subcircuits: $|E|=K_{max}$ since all cuts are on the two subcircuits and there is only one contraction step.
    The tensor network contraction cost is the same as ~\cite{tang2021cutqc}.
    \item Three subcircuits: The first tensor contraction step involves all cut edges and $K_1=|E|$.
    However, the second contraction step has $K_2<|E|$.
    Tensor contraction's total cost is hence $4^{K_1}+4^{K_2}$,
    while~\cite{tang2021cutqc} is $2\times4^{|E|}$.
    This results in cost savings, though not on an exponential scale.
    \item General cases with more than three subcircuits:
    Since an edge connects only two subcircuits,
    it follows that $K_1<|E|$ for the first contraction.
    For subsequent contractions, $K_i<|E|$ as some edges have already been contracted
\end{enumerate}
Therefore, $K_{\max}<|E|$ when there are more than three subcircuits.
Tensor network contraction hence introduces an exponential computation advantage over the prior method in general cases.