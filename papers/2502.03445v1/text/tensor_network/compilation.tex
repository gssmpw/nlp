\subsection{How: Tensor Network Compilation}\label{sec:tensor_network_compilation}
\subsubsection{Determining Contraction Order}\label{sec:determining_contraction_order}
In the context of tensor network contraction,
the order in which subcircuits are contracted does not impact the accuracy,
yet it significantly influences computational overhead.
Although determining the optimal contraction sequence for a tensor network is NP-hard,
numerous heuristics have been developed to identify high-quality sequences,
making it a vibrant area of research across various fields~\cite{robertson1991graph,markov2008simulating,orus2014practical}.
For instance, TensorQC leverages the CoTenGra software~\cite{gray2021hyper} to optimize the contraction order,
enhancing performance without compromising output integrity.

\subsubsection{Managing Memory Constraints}
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/slicing.pdf}
    \caption{Slicing one cut edge to express a tensor network as a summation of smaller tensor networks.}
    \label{fig:slicing}
\end{figure}

To ensure that the computations are both efficient and feasible within the hardware limitations,
it is crucial that the input subcircuit tensors fit within the GPU's memory constraints.
TensorQC implements an index slicing strategy that breaks down some cut edges to decompose the full tensor network into a summation of smaller tensor networks,
thereby reducing the size of the tensors to be processed.
For example, slicing the edge $e_4$ (as shown in Figure~\ref{fig:slicing}) results in four smaller tensor networks,
each representing a different fixed basis of $e_4$.
This method allows TensorQC to manage memory usage effectively by continuously slicing the most size-reducing cut edge until the tensor sizes are compatible with the GPU's memory capacity.

Moreover, to prevent memory overflow during the tensor contraction process,
a second-level slicing strategy is employed.
This method further divides the indices of individual tensor networks that were sliced in the previous step,
ensuring that both input and intermediate tensors remain within memory limits.
This two-tiered approach to tensor slicing ensures that all components fit within the available GPU memory.