\subsection{Backends}\label{sec:backends}
The classical post-processing in the experiments runs on a single Nvidia $A10G$ GPU.
QPUs nowadays are still too small and noisy to even support running medium size subcircuits for meaningful analysis.
Instead, we use random numbers as the subcircuit output to focus on demonstrating the post-processing runtimes for large benchmarks.
In addition, we use noiseless classical simulators to study the HSS approximation effects for small benchmarks.
We expect more reliable QPUs to enable a full evaluation of TensorQC for practical applications.
We further assume distributing subcircuits to $10$ small QPUs to quantify the QPU runtime,
a reasonable assumption as IBM cloud provides around $15$ QPUs nowadays.
In practice, we set $K_t=10$ and $Q_{\max}=10^4$ for the Greedy Graph Growing heuristics.