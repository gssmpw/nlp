\subsection{Metrics}
There are three key metrics this paper looks at, namely runtime, quantum resources and accuracy.

\paragraph{Runtime, faster is better}
The runtime for purely QPU executions without cutting is simply the QPU time to run a circuit as is,
which is the fastest.
The TensorQC runtime is the combined quantum and classical runtime defined in~\ref{eq:cut_objective}.
Our experiments capture the real GPU wall clock runtime to reconstruct $1$ million arbitrary quantum states.

\paragraph{Quantum Area, smaller is better}
The quantum resource requirement is loosely defined as the product of the circuit width and depth,
called the `quantum area'.
Classical simulations require $0$ quantum area as it does not use QPUs at all.
For purely QPU executions, it is simply the product of the number of qubits and the circuit depth of the input quantum circuit.
For TensorQC, it is defined on the largest subcircuit produced from cutting.
The rationale is that QPUs must be able to support the workloads with a certain quantum area at high accuracy to produce accurate results.
While many hardware and software factors affect the QPUs' ability to support quantum workloads,
a smaller quantum area generally puts less burden on the quantum resources.

\paragraph{Amplitude Reconstruction Ratio, higher is better}
HSS only retains a subset of the output states,
hence does not account for the full probability amplitudes.
We represent the amount of amplitudes reconstructed as:
\begin{equation}
    P_{HSS}=\sum_{i\in \{HSS\}}P_i
\end{equation}
where $P_i$ is the reconstructed output of the full circuit,
$\{HSS\}$ indicates the set of binary states retained by HSS.
$P_{HSS}=1$ means HSS captures all the non-zero binary states.

In addition, different benchmarks have different max possible $P_{HSS}$
for a given number of sampled states,
noted by $P_{HSS}^{max}$.
This is captured by the sum of amplitudes of the top $|HSS|$ states in its error-free output:
\begin{equation}
    P_{HSS}^{max}\equiv\sum_{i=1}^{|HSS|}P_i^{\textrm{true, sorted}}
\end{equation}
where $P^{\textrm{true, sorted}}$ is the sorted error-free amplitude distribution of a benchmark circuit.
$P_{HSS}^{max}$ solely depends on the benchmark circuit itself and dictates the best possible performance of any sampling algorithms.
For example, $GHZ$ has a highly skewed output landscape with $2$ solution states sharing all the amplitudes.
Hence $P_{HSS}^{max}=1$ for $|HSS|\geq2$.
On the other hand, $Regular$, $Erdos$, $AQFT$, and $Supremacy$ benchmarks have more evenly distributed output landscapes,
without any dominating states.
Therefore, the ratio between the two represents the fraction of the max possible amplitude reconstructed given a certain $|HSS|$ limit,
ranging from $0$ to $1$:
\begin{equation}
    r_{P,HSS}\equiv P_{HSS}/P_{HSS}^{max}\label{eq:hss_amplitude_ratio}
\end{equation}

\paragraph{HSS Data Efficiency, higher is better}
We aim to quantify the data efficiency of the HSS protocol by stating,
“The HSS protocol retains $X\%$ of the total amplitudes while reconstructing only $Y\%$ of the binary states”,
where higher $X$ and lower $Y$ are preferred.

While $P_{HSS}$ represents $X$,
the number of reconstruction states $|HSS|$ as a fraction of the size of the Hilbert space for a $n$ qubit benchmark quantifies $Y$:
\begin{equation}
    r_{|HSS|}\equiv\frac{|HSS|}{2^n}
\end{equation}
Both $P_{HSS}$ and $r_{|HSS|}$ range between $0$ and $1$.
We therefore quantify the HSS data efficiency by the ratio between the two:
\begin{equation}
    \eta_{HSS}\equiv\frac{P_{HSS}}{r_{|HSS|}}\label{eq:hss_efficiency}
\end{equation}
Note that if $|HSS|=2^n$,
the HSS protocol reconstructs all the binary states and $P_{HSS}=r_{|HSS|}=1$.

Our goal for the HSS protocol is to achieve more amplitude reconstruction using fewer states,
a data efficiency represented by a higher $\eta_{HSS}$.
$\eta_{HSS}=1$ represents a one-to-one trade-off between the amplitude reconstruction and the number of states sampled.