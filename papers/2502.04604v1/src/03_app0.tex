\section{Methodology}\label{sec:approach}


\subsection{Overview}\label{subsec:overview}
MonoEmbed follows, overall, the same structure as most decomposition approaches. Fig     \ref{fig:overview} showcases two main components: \textbf{Analysis} and \textbf{Inference}. The design and application of our approach is done in two phases.

The first phase revolves around training and preparing the LM of the \textbf{Analysis} component. It involves the creation of a triplet samples dataset based on Microservices applications, the selection between different Pre-Trained LMs and the fine-tuning of the model through a Contrastive Learning method.

The second phase showcases the usage of MonoEmbed on monolithic applications with the Fine-Tuned LM. In this phase, the \textbf{Analysis} component processes the monolith's source code using the LM, generating a feature matrix  \( (N{\times}M) \), where \( (N) \) is the number of classes and \( (M) \) is the embedding vector size. The \textbf{Inference} component then normalizes and partitions this data with a clustering algorithm to produce the decomposition, a set of microservices with distinct classes. For this research, we limit our input to source code at the class level granularity.



\begin{figure}[ht]
\centering
\includegraphics[width=\linewidth]{img/03/fig_overview.pdf}
\caption{An overview of the suggested decomposition approach.} \label{fig:overview}
\end{figure}


% \subsection{Selecting the Embedding Models}
\input{src/03_app3_selection}

% \subsection{Fine-Tuning the Model}
\input{src/03_app4_finetune}

% \subsection{Creating the Datasets}
\input{src/03_app5_datasets}

\subsection{Generating the Decompositions}\label{subsec:inference}
The \textbf{Inference} component in MonoEmbed is responsible for generating decompositions using the embeddings from the \textbf{Analysis} component. By relying on advanced LLMs and our fine-tuning process, the high-level and efficient embeddings will enable better performance while reducing the complexity of the inference. While more advanced approaches such as GNNs can be used for this component, the current implementation of this phase will focus on clustering algorithms. In fact, these algorithms often rely on distances and similarities between the feature vectors of their inputs. Which is why EMs that generate high quality embeddings are well suited with clustering algorithms. In addition, we employ a normalization step, using the z-score standardization method to scale the feature values within the embeddings matrix. This step helps mitigate the impact of variance between feature values.

The output of the \textbf{Inference} component is a vector of integer values which specifies the suggested microservices decomposition. As such, a decomposition is defined as $D = [M_1, M_2, ..., M_K]$. Each microservice in the decomposition is defined as a subset of classes  $M_i = \{c_j | j \in [0, N]\}$ and $|M_i| < N$ where $N$ is the number of classes.