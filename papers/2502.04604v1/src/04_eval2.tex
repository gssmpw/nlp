\subsection{RQ2: The Quality of Fine-Tuned Model Embeddings}\label{subsec:ftmodels}
\subsubsection{Experimental Setup}

For this research question, we would like to, first, evaluate how much are we able to improve the Pre-Trained models through fine-tuning and, second, to identify the impact of the scale of the training set data samples on the quality of the embeddings. For fine-tuning the models, we generate a dataset as described in section \ref{subsec:finetune}. We exclude all of the evaluation applications described in Table \ref{tab:microapps} from the training dataset. The generated dataset has approximately 340K unique triplet samples. For the evaluation, we used the same score and applications described in section \ref{subsubsec:rq1metrics} and section \ref{subsubsec:rq1dataset}. We present the score for each application and their mean where the 3 best models are highlighted with bold text, double underline and simple underline.



\subsubsection{Implementation Details}

In addition to the process described in section \ref{subsec:finetune}, we provide the implementation details for the chosen models: For \textit{UnixCoder} and \textit{CodeT5+}, we used the recommended learning rate value of 2e-5 \cite{chi2019finetunebert} for fine-tuning the models.  For the rest of the models, we train them using the LoRA method due to their sizes. In the case of \textit{LLM2Vec}, we fine-tune the MNTP Llama 3 version of the proposed model \cite{parishad2024llm2vecmntpmodelcard} instead of the supervised version \cite{parishad2024llm2vecsupervisedmodelcard} based on the authors' training observations \cite{parishad2024llm2vec}. We used the same setup and hyper-parameters they suggest\footnote{\url{https://github.com/McGill-NLP/llm2vec}}. For \textit{GritLM}, we used the author's configuration and hyper-parameters\footnote{\url{https://github.com/ContextualAI/gritlm}}. As for \textit{SFR-Mistral}, we add a "lasttoken" pooling layer to the model and used the same setup as \textit{LLM2Vec} for the training. 







\subsubsection{Base Models Comparison (RQ2-A)}
% for new line
\phantom{1}

\textbf{Motivation}: For the first objective in this research question, we fine-tune 5 models among the 10 models that achieved the best mean score in Table \ref{tab:appscores}: \textit{SFR-Mistral}, \textit{GritLM}, \textit{UnixCoder}, \textit{CodeT5+} and \textit{LLM2Vec}. The selection of these models relied on their availability, their category and our available resources. All of the models were trained on 10\% of the dataset (34K samples). \textit{SFR-Mistral}, \textit{UnixCoder} and \textit{CodeT5+} were chosen because they achieved the best scores among their respective groups. We select \textit{GritLM} to represent the group of LMEs since we were not able to fine-tune \textit{NVEmbed} on our resources with the same training setup as the rest of the models. \textit{GritLM} is a unified model (i.e. it has been trained for both embedding and generative tasks) while \textit{LLM2Vec}, through its modular 3-step tuning process, is specialized for embeddings. For this reason, we decided to consider \textit{LLM2Vec} as well.

% For this reason, \textit{LLM2Vec} and due to its modular 3-step tuning process, we decided to consider \textit{LLM2Vec} as well.

\textbf{Results}: Table \ref{tab:ftscores_models} shows the evaluation results of the Fine-Tuned (FT) Models. We include the results of the Pre-Trained model \textit{VoyageAI} as a reference. The models are sorted based on their average performance. We can observe in the table that all of the models managed to outperform the benchmark \textit{VoyageAI} on all of the evaluation applications with the exception of the application \textit{AcmeAir} and \textit{ME-gritlm} and \textit{ME-codet5+} in \textit{ESKanban}. In fact, there is a 0.075 reduction in score between \textit{ME-llm2vec} and the benchmark. Both \textit{ME-llm2vec} and \textit{ME-SFR-Mistral} had the best mean scores and outperformed the rest on most applications with \textit{ME-llm2vec} having a slight edge. \textit{ME-gritlm}, unlike the other LLMs, was surpassed by \textit{ME-unixcoder} showcasing the advantage of using a purely representation learning focused LLM like \textit{LLM2Vec}. 






\begin{table}[]



    \centering
    \caption{Fine-tuned models' scores on the evaluation applications. }
    \label{tab:ftscores_models}
    \resizebox{\linewidth}{!}{%



    
   \begin{tabular}{l|lllllll}
\toprule
         Model &               Mean &          PetClinic &            AcmeAir &           ESKanban &           CESource &            SEMicro &              SEMod \\
\midrule
    ME-llm2vec &    \textbf{0.5626} &    \textbf{0.5499} &    \uuline{0.6008} &    \textbf{0.4870} &    \textbf{0.6159} & \underline{0.5819} & \underline{0.5401} \\
ME-SFR-Mistral &    \uuline{0.5666} &    \uuline{0.5579} & \underline{0.6215} &    \uuline{0.4943} &    \uuline{0.6254} &    \uuline{0.5757} &    \textbf{0.5247} \\
  ME-unixcoder & \underline{0.5968} & \underline{0.5895} &             0.6386 & \underline{0.5646} &             0.6315 &             0.5978 &             0.5586 \\
     ME-gritlm &             0.5999 &             0.5970 &             0.6612 &             0.6084 &             0.6506 &    \textbf{0.5486} &    \uuline{0.5337} \\
    ME-codet5+ &             0.6071 &             0.6009 &             0.6502 &             0.6020 & \underline{0.6256} &             0.5955 &             0.5687 \\
      VoyageAI &             0.6387 &             0.6900 &    \textbf{0.5944} &             0.5752 &             0.6678 &             0.6684 &             0.6364 \\
\bottomrule
\end{tabular}





}
\end{table}




\subsubsection{Sample Size Comparison (RQ2-B)}
% for new line
\phantom{1}

\textbf{Motivation}: For the second objective, we train each of the models \textit{UnixCoder}, \textit{CodeT5+} and \textit{LLM2Vec} on different numbers of samples: 3K, 34K and 340K samples ( 1\%, 10\% and 100\% of the generated dataset). We evaluate the impact of augmenting the dataset from the perspective of the number of samples without modifying the number of repositories.


\textbf{Results}: Table \ref{tab:ftscores_samples} shows the quality scores of the fine-tuned models on different scales of training samples. The results show that all of the models were able to surpass \textit{VoyageAI}'s score in \ref{tab:ftscores_models} even at only 3K training samples. In fact, the model \textit{ME-llm2vec} with 3K samples achieved better scores than both \textit{ME-unixcoder} and \textit{ME-codet5+} when trained on 340K samples. By increasing the scale of the dataset, we can observe that \textit{ME-llm2vec} continues to improve. However, transitioning from 34K to 340K samples had a less significant effect than the transition from 3K to 34K. This result showcases the diminishing returns of scaling up the number of samples. Improving the model's performance even further will likely require increasing the number of repositories in the dataset or incorporating a more complex sample selection process. Notably, the \textit{ME-llm2vec} that was trained with 340K samples surpassed all of the models in Table \ref{tab:ftscores_models}. While both \textit{ME-unixcoder} and \textit{ME-codet5+} did benefit from more samples, \textit{ME-unixcoder} improvement rate was higher than \textit{ME-codet5+}'s, and thus switching their ranking after 34K samples. 


% \begin{tcolorbox}[colback=gray!10!white, colframe=gray!90!black]
% \textbf{Summary:} Even with a small sample size, we can adapt the models to the decomposition problem and surpass Pre-Trained models. Increasing the training data scale will continue to improve these models further but at an exponentially decreasing rate. 
% \end{tcolorbox} 
% all of our fine-tuned models managed to surpass the Pre-Trained models, with \textit{ME-llm2vec} and \textit{ME-SFR-Mistral} achieving significantly better score.
\begin{tcolorbox}[colback=gray!10!white, colframe=gray!90!black]
\textbf{Summary:} RQ2-A shows that all of our models managed to surpass the Pre-Trained models, with \textit{ME-llm2vec} achieving a significantly better score while RQ2-B shows that it is achievable even with a small sample size. Increasing the scale further will improve these models but at an exponentially decreasing rate. 
\end{tcolorbox} 

\begin{table}[]



    \centering
    \caption{Fine-tuned models' scores per number of training samples.}
    \label{tab:ftscores_samples}
    \resizebox{\linewidth}{!}{%



    
    \begin{tabular}{l|l|lllllll}
\toprule
\makecell{Samples} &  \makecell{Model} & \makecell{Mean} & \makecell{PetClinic} & \makecell{AcmeAir} & \makecell{ESKanban} & \makecell{CESource} & \makecell{SEMicro} & \makecell{SEMod} \\
\midrule
   &   ME-llm2vec &    \textbf{0.5483} &    \textbf{0.5386} &    \textbf{0.5932} &    \uuline{0.5009} &    \uuline{0.6211} &    \textbf{0.5440} &    \textbf{0.4922} \\
   \makecell{340K} & ME-unixcoder &             0.5881 &             0.5973 &             0.6633 &             0.5393 &             0.6293 & \underline{0.5792} & \underline{0.5199} \\
   &   ME-codet5+ &             0.5956 &             0.5648 &             0.6827 &             0.5583 &             0.6678 &             0.5931 &    \uuline{0.5067} \\
\midrule
    &   ME-llm2vec &    \uuline{0.5626} &    \uuline{0.5499} &    \uuline{0.6008} &    \textbf{0.4870} &    \textbf{0.6159} &             0.5819 &             0.5401 \\
    \makecell{34K} & ME-unixcoder &             0.5968 &             0.5895 &             0.6386 &             0.5646 &             0.6315 &             0.5978 &             0.5586 \\
    &   ME-codet5+ &             0.6071 &             0.6009 &             0.6502 &             0.6020 & \underline{0.6256} &             0.5955 &             0.5687 \\
\midrule
     &   ME-llm2vec & \underline{0.5851} & \underline{0.5639} & \underline{0.6054} & \underline{0.5259} &             0.6275 &             0.6116 &             0.5764 \\
     \makecell{3K} &   ME-codet5+ &             0.6079 &             0.6141 &             0.6746 &             0.5904 &             0.6455 &    \uuline{0.5751} &             0.5475 \\
      & ME-unixcoder &             0.6195 &             0.6184 &             0.6596 &             0.5364 &             0.6514 &             0.6462 &             0.6051 \\
\bottomrule
\end{tabular}





}
\end{table}