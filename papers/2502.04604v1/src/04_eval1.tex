\subsection{RQ1: The Quality of Pre-Trained Model Embeddings}
\subsubsection{Experimental Setup}
In this phase, we evaluate potential embedding models in the context of the decomposition problem. More specifically, we evaluate the quality of the vectors generated by the models in Table \ref{tab:ptmodels}, where the closer a microservice’s classes’ vectors are, the better the representation is. We include the 4 static analysis representations from section \ref{subsec:mexample} as benchmarks. 



\subsubsection{Implementation Details}
We generated class-level embedding vectors for the evaluation applications as follows: \textit{Code2Vec} \cite{alon2018code2vec} and \textit{Code2Seq} \cite{alon2019code2seq} can generate embeddings only at the method-level which is why we define their class-level embeddings as the mean of their corresponding methods \cite{aldebagy2021code2vec}. For the rest, we generate the class embeddings directly. We used the 512-context Java version of \textit{CuBERT}. For \textit{CodeBERT}, \textit{GraphCodeBERT}, and \textit{UnixCoder}, we selected the best-performing input modality based on the mean evaluation scores. We used the last token pooling method for generating sequence embeddings in the case of DT LLMs. \textit{GritLM}, \textit{NVEmbed}, and \textit{LLM2Vec} embeddings were generated based on the authors' shared code and models. For \textit{LLM2Vec}, we used the supervised learning version \cite{parishad2024llm2vecsupervisedmodelcard} of MNTP-LLama3 \cite{aimeta2024llama3modelcard} fine-tune. For the CEM providers, we report results from the best-performing model. 



\subsubsection{Evaluation Dataset}\label{subsubsec:rq1dataset}
We select a dataset of 6 microservices and modular applications with varying scales as shown in Table \ref{tab:microapps}. The applications Spring Petclinic Microservices \cite{microapps2024petclinic}, Acme Air \cite{monoapps2024acmeair}, Eventuate Kanban Board \cite{microapps2024kanban} and Event Sourcing \cite{microapps2024eventsourcing} were selected due to their usage in decomposition approaches \cite{kalia2021mono2micro,mathai2022chgnn,khaled2022msextractor,khaled2022hydecomp}. We include the microservices \cite{microapps2024socialeditionmicroservices}  and modular monolith \cite{microapps2024socialeditionmodular} versions of the monolithic application Social Edition which has been used in the decomposition context \cite{faustino2022stepwise}.

% to bridge the gap between microservices and monolith inputs.


\begin{table}[ht]
\caption{Microservices and Modular Monolith application data.}
\label{tab:microapps}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}lllll@{}}
\toprule
Application                      & Short Name & \# of classes & \# of methods & \# of microservices \\ \midrule
PetClinic Microservices    & PetClinic  & 36            & 52           & 7                   \\
Acme Air                 & AcmeAir  & 86           & 758          & 7                  \\
Eventuate Kanban Board                 & ESKanban  & 112           & 659          & 19                  \\
Microservices Event Sourcing & CESource  & 119           & 648          & 12                  \\
Social Edition Modular       & SEMod  & 376           & 3175          & 9                   \\
Social Edition Microservices   & SEMicro  & 509           & 4224         & 9                   \\ \bottomrule
\end{tabular}
}
\end{table}

\subsubsection{Evaluation Metrics}\label{subsubsec:rq1metrics}
For any application, the closer the embeddings of classes that should belong to the same microservices are and the farther away those of different microservices are from each other the better the quality of the embedding model. We present in Algorithm \ref{alg:embscore} the pseudo-code for measuring a score that can quantify this quality. We calculate the Embedding Quality Score across all evaluation repositories. For each application, the algorithm extracts classes and their associated microservices, generates standardized embeddings of their source code using the given model, and gets all unique permutations of these embeddings. Each permutation corresponds to a label where 1 means classes are in the same microservice and 0 if not. It then computes cosine similarities between the pairs, normalizes them, and calculates a balanced binary cross-entropy loss score comparing the similarities to the actual microservice associations. We chose the balanced loss function instead of the standard log loss function since the number of labels 0 significantly outnumbers label 1. The algorithm returns both the mean score across all repositories and a list of individual repository scores which we will use for evaluating the model. The lower the score is, the closer the embeddings to the actual microservices' structure and the better their quality is.

\begin{algorithm}[htbp]
\caption{Embedding Quality Score}\label{alg:embscore}
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\Input{model, repositories}
\Output{meanScore, allScores}
\BlankLine
\For{repo in repositories}{
    code, services = extractClasses(repo)\\
    embeddings = standarize(model.encode(code))\\
    couples, labels = toPairwise(embeddings, services)\\
    similarities = minMax(cosine(couples))\\
    allScores.add(balancedLogLoss(similarities, labels))\\
}
\Return mean(allScores), allScores
\end{algorithm}


\subsubsection{Results}
The Table \ref{tab:rq1_results} showcases the evaluation results for the Pre-Trained models we considered on the evaluation applications and their mean score. The models are sorted by their mean score from lowest (best) to highest (worst). We highlight the 3 best models in each application and the mean column with bold, double underline and simple underline. Overall, the mean score results in Table \ref{tab:rq1_results} demonstrate a similar pattern to the model rankings in MTEB\cref{footnote:mteb}. The first 8 ranks in our benchmark were achieved by LLMs and CEMs showcasing their adaptability. In fact, \textit{VoyageAI}, had the best score while the other 2 CEMs reached the $6^{th}$ and $7^{th}$ ranks. Nonetheless, all LMEs managed to perform well in comparison where \textit{NVEmbed} achieved the second lowest mean score and \textit{GritLM} and \textit{LLM2Vec} had better or similar scores to the other CEMs. However, most DT LLMs underachieved in comparison and were surpassed by much smaller ETs and benchmarks, with the exception of \textit{SFR Mistral} and \textit{E5 Mistral} which landed the $3^{rd}$ and $4^{th}$ ranks. Unlike the rest of the DTs, these models were fine-tuned for representation learning which explains their performance. \textit{DeepSeek Coder} and \textit{CodeLLama}, code specialized DT LLMs, scored worse than most models but better than \textit{Llama-3}, a generalist LLM, indicating that training objectives and model architecture outweigh domain specialization. However, most ETs performed worse than LLMs and even benchmarks like \textit{ST-Interactions}. The unified models \textit{UnixCoder} and \textit{CodeT5+} are an exception as they managed to land the $9^{th}$ and $10^{th}$ place, thus competing with the CEMs and LMEs. The final ranks were occupied by the traditional Embedding Models \textit{Code2Vec} and \textit{Code2Seq} ($20^{th}$ and $21^{st}$) and \textit{ST-Calls}. The method-level analysis of these models is a likely reason for their performance since they can struggle with annotation-based applications such as \textit{PetClinic} where most classes are data models. While \textit{ST-Interactions} achieved the $11^{th}$, beating most models we considered and even had the best score for PetClinic, \textit{ST-Calls} had significantly higher scores on all applications. We believe that this difference is due to the sparse nature of the dependency matrix in \textit{ST-Calls} unlike \textit{ST-Interactions}. However, we would like to note that applying static analysis on Microservices applications does not reflect their performance on Monoliths due to the lack of inter-microservices interactions. The lack of misleading dependencies can lead to better scores (\textit{ST-Interactions}) while the sparsity can lead to worse scores (\textit{ST-Calls}).

\begin{tcolorbox}[colback=gray!10!white, colframe=gray!90!black]
\textbf{Summary:} We show that LLM based encoders, like VoyageAI, NVEmbed and SFR Mistral, generate better representation vectors in the decomposition context than the methods employed in related works (e.g. static analysis, Code2Vec, CodeBERT). 
\end{tcolorbox}






\begin{table}[h]
    \centering
    \caption{Embedding model scores on the evaluation applications.}
    \label{tab:rq1_results}
    \resizebox{\linewidth}{!}{%
    
        
\begin{tabular}{ll|lllllll}
\toprule
{} &            Model &                Mean &           PetClinic &             AcmeAir &            ESKanban &            CESource &             SEMicro &               SEMod \\
\midrule
1  &         VoyageAI &     \textbf{0.6387} &              0.6900 &     \textbf{0.5944} &     \uuline{0.5752} &              0.6678 &     \textbf{0.6684} &     \textbf{0.6364} \\
2  &          NVEmbed &     \uuline{0.6405} &              0.6833 &     \uuline{0.6150} &     \textbf{0.5710} &  \underline{0.6617} &  \underline{0.6712} &     \uuline{0.6409} \\
3  &      SFR Mistral &  \underline{0.6409} &     \uuline{0.6510} &              0.6360 &  \underline{0.5787} &     \textbf{0.6588} &              0.6765 &              0.6445 \\
4  &       E5 Mistral &              0.6417 &  \underline{0.6521} &              0.6360 &              0.5814 &     \uuline{0.6596} &              0.6764 &              0.6449 \\
5  &           GritLM &              0.6465 &              0.6670 &              0.6270 &              0.5970 &              0.6660 &              0.6737 &              0.6482 \\
6  &           OpenAI &              0.6482 &              0.6897 &  \underline{0.6215} &              0.5850 &              0.6798 &     \uuline{0.6702} &  \underline{0.6429} \\
7  &           Cohere &              0.6506 &              0.6666 &              0.6285 &              0.5920 &              0.6779 &              0.6934 &              0.6450 \\
8  &          LLM2Vec &              0.6507 &              0.6717 &              0.6433 &              0.5995 &              0.6651 &              0.6738 &              0.6510 \\
9  &        UnixCoder &              0.6560 &              0.6694 &              0.6661 &              0.5955 &              0.6739 &              0.6796 &              0.6514 \\
10 &          CodeT5+ &              0.6562 &              0.6848 &              0.6523 &              0.5989 &              0.6790 &              0.6716 &              0.6508 \\
11 &  ST-Interactions &              0.6651 &     \textbf{0.6430} &              0.6687 &              0.6455 &              0.6688 &              0.6843 &              0.6801 \\
12 &    GraphCodeBERT &              0.6728 &              0.6978 &              0.6514 &              0.6278 &              0.7041 &              0.6884 &              0.6669 \\
13 &           SM-BoW &              0.7070 &              0.7316 &              0.7412 &              0.6295 &              0.7227 &              0.7405 &              0.6765 \\
14 &      DeepSeek Coder   &              0.7104 &              0.7225 &              0.6975 &              0.6785 &              0.7239 &              0.7253 &              0.7143 \\
15 &         SM-TFIDF &              0.7122 &              0.7444 &              0.7156 &              0.6608 &              0.7200 &              0.7393 &              0.6933 \\
16 &   CodeLlama &              0.7138 &              0.7171 &              0.6970 &              0.6801 &              0.7403 &              0.7288 &              0.7192 \\
17 &          Llama-3 &              0.7240 &              0.7076 &              0.7108 &              0.7110 &              0.7408 &              0.7412 &              0.7329 \\
18 &         CodeBERT &              0.7365 &              0.7984 &              0.6790 &              0.7108 &              0.7662 &              0.7425 &              0.7221 \\
19 &           CuBERT &              0.8121 &              0.8243 &              0.7571 &              0.8127 &              0.8472 &              0.8398 &              0.7913 \\
20 &         Code2Vec &              1.3176 &              3.4025 &              0.9142 &              0.8453 &              1.1773 &              0.8365 &              0.7299 \\
21 &         Code2Seq &              1.4238 &              3.4787 &              1.0459 &              0.9711 &              1.3216 &              0.8796 &              0.8460 \\
22 &         ST-Calls &              7.6066 &             12.4624 &              9.7032 &              8.6021 &              8.3805 &              3.8777 &              2.6134 \\
\bottomrule
\end{tabular}



    
    
    %
    }


\end{table}

