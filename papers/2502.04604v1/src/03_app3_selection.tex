\subsection{Model Selection}\label{subsec:mselection}





While various analysis methods could satisfy our definition of the analysis component, even including static analysis approaches, this research primarily focuses on utilizing Language Models to transform source code into meaningful feature vectors. A study of the usage of Pre-Trained Models \cite{niu2023codeembedreview} in Software Enginnering in 2023 showcases the large number of potential models, which has been growing significantly ever since. For this reason, the selection of an appropriate LM from the diverse array of options is complex, considering factors such as source code nature, desired representation granularity, and computational efficiency, all of which can impact decomposition task performance. To facilitate understanding and inform the selection process, we provide an overview of potential embedding models, categorizing them by types and characteristics. Table \ref{tab:ptmodels} summarizes the models used and their respective categories. It includes the following columns: \textit{Size} (approximate number of parameters), \textit{Context} (maximum input tokens), \textit{Code} (Pre-Trained on source code), and \textit{Modality} (acceptable input types for each model).




\begin{table}[]
\caption{Metadata of the potential Pre-Trained Language Models}
\label{tab:ptmodels}
\resizebox{\linewidth}{!}{%
\begin{tabular}{llllll}
\hline
Model Name                                              & Group           & Size & Context & Code & Modality    \\ \hline
Code2Vec \cite{alon2018code2vec}               & Static       & \textless{}1M & -    & Yes & AST paths \\
Code2Seq \cite{alon2019code2seq}               & RNN    & \textless{}1M & -    & Yes & AST paths \\
CuBERT \cite{kanade2020cubert}         & ET    & 340M & 2048        & Yes      & PL          \\
CodeBERT \cite{feng2020codebert}       & ET    & 125M & 512         & Yes      & NL-PL       \\
GraphCodeBERT \cite{guo2021graphcodebert}      & ET & 125M                           & 512  & Yes & DF-NL-PL  \\
UniXcoder \cite{guo2022unixcoder}              & ET & 125M                           & 1024 & Yes & DF-NL-PL  \\
CodeT5+ \cite{wang2023codet5p}         & EDT & 110M & 512         & Yes      & PL          \\
Meta Llama 3 \cite{aimeta2024llama3modelcard}     & DT & 8B                           & 8K  & No & NL        \\
SFR Mistral \cite{SFRAIResearch2024sfrmistral} & DT & 7B                             & 4096 & No  & NL        \\
E5 Mistral \cite{wang2024e5mistral}    & DT    & 7B   & 4096        & No       & NL          \\
CodeLlama \cite{rozière2024codellama}  & DT    & 7B   & 100K        & Yes      & NL          \\
DeepSeek Coder \cite{guo2024deepseekcoder}     & DT & 6.7B                           & 16K  & Yes & NL        \\
GritLM \cite{muennighoff2024gritlm}    & LME    & 7B   & 4096        & No       & Instruct-NL          \\
NVEmbed \cite{lee2024nvembed}          & LME             & 7B   & 4096        & No       & Instruct-NL \\
LLM2Vec \cite{parishad2024llm2vec}     & LME             & 8B   & 512         & No       & Instruct-NL \\
VoyageAI \cite{voyageai2024embeddings} & CEM   & -    & 16K         & No       & NL          \\
OpenAI \cite{openai2024embeddings}     & CEM   & -    & 8191        & No       & NL          \\
Cohere \cite{cohere2024embeddings}     & CEM   & -    & 512         & No       & NL          \\ \hline
\end{tabular}%
}
\end{table}





\subsubsection{Static and RNN based EMs}
\textit{Code2Vec} \cite{alon2018code2vec} is a static EM that was tailored for encoding code sample ASTs into static vector representations. It was extended by \textit{Code2Seq} \cite{alon2019code2seq}, a RNN-based EM, to incorporate input context.

\subsubsection{Encoder-only Transformers (ET)}
In our research, we considered several prominent ET models trained on software engineering problems and source code \cite{niu2023codeembedreview}. We selected \textit{CuBERT} \cite{kanade2020cubert} for its Java source code pairs modality. \textit{CodeBERT} \cite{feng2020codebert} and \textit{GraphCodeBERT} \cite{guo2021graphcodebert} were chosen for their unique multi-modal approach, incorporating Natural Language (NL), Programming Language (PL), and Data-Flow (DF) information. Additionally, we include \textit{UniXcoder} \cite{guo2022unixcoder}, which, although based on \textit{RoBERTa} \cite{liu2019roberta} like its predecessors, stands out as a unified model trained on both representation and generative tasks, offering enhanced potential for fine-tuning.
\subsubsection{Encoder-Decoder Transformers (EDT)}
\textit{CodeT5+} \cite{wang2023codet5p} is an Encoder-Decoder LLM that was trained on a large and diverse set of objectives so that it efficiently adapts to downstream tasks. We include its encoder in our evaluation.
\subsubsection{Decoder-only Transformers (DT)}
While DT LLMs are primarily designed for generative tasks, certain models \cite{wang2024e5mistral} have demonstrated strong performance in representation benchmarks, notably the Massive Text Embedding Benchmark (MTEB)\footnote{\label{footnote:mteb}\url{https://huggingface.co/spaces/mteb/leaderboard}} \cite{muennighoff2022mteb}. Our study encompasses a diverse range of DT models, as detailed in Table \ref{tab:ptmodels}, spanning from general-purpose models \cite{aimeta2024llama3modelcard} to those specialized for code-related tasks \cite{rozière2024codellama}.
\subsubsection{Language Model Embeddings (LME)}
We define LMEs as the DT LLMS that have been adapted for representation learning. We include state-of-the-art LMEs such as \textit{LLM2Vec} \cite{parishad2024llm2vec}, \textit{NV-Embed} \cite{lee2024nvembed} and \textit{GritLM} \cite{muennighoff2024gritlm}.
\subsubsection{Closed-source Embedding Models (CEM)}
CEMs refer to LLM based proprietary tools, like \textit{VoyageAI} \cite{voyageai2024embeddings}, designed to generate vector representations of text. They are accessed through APIs, enabling embedding generation without direct access to the underlying model. 