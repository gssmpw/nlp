\subsection{Background}
\subsubsection{Embedding Models (EMs)} and in particular Neural-Network (NN) based Embedding Models like Word2Vec \cite{mikolov2013word2vec1} transform input sequences (e.g. word tokens, code samples) into continuous vector representations in a high-dimensional space. These embedding vectors capture semantic and syntactic relationships between the tokens, allowing similar concepts to have similar vector representations. 

\subsubsection{Language Models (LMs)} are probabilistic models that learn to predict the likelihood of a sequence of words. Modern LMs are based on NN architectures like Recurrent Neural Networks (RNN) \cite{alon2019code2seq} and Transformers \cite{devlin2019bert,liu2019roberta,radford2018gpt1}. Large Language Models (LLMs) often refers to Pre-Trained Neural LMs based on the Transformers \cite{vaswani2017attention} architecture and have more than 1 Billion parameters. While LMs and EMs are not the same, recent NN-based LMs such as BERT \cite{devlin2019bert} and GPT \cite{radford2018gpt1} can generate rich contextual representations and, therefore, can be utilized as EMs. For the rest of the paper, we will use both terms interchangeably.

\subsubsection{Transformers} are advanced NN architectures designed to handle sequential data using the self-attention mechanism \cite{vaswani2017attention}.  Transformers consist of an encoder-decoder structure, where the encoder processes the input sequence to generate contextual embeddings and the decoder uses these embeddings to produce the output sequence. While encoder-decoder transformers \cite{raffel2020t5} are versatile for a wide range of sequence-to-sequence tasks, encoder-only transformers, such as BERT \cite{devlin2019bert}, are more often used as EMs to generate efficient contextual embeddings. Decoder-only Transformers, such as the GPTs \cite{radford2018gpt1,brown2020gpt3}, have proven to be most successful on generative problems. However, recent efforts have been able to adapt the LLM decoder-only transformers into EMs \cite{springer2024llmrepetition,parishad2024llm2vec,muennighoff2024gritlm,SFRAIResearch2024sfrmistral} in order to take advantage of their capabilities.




\subsubsection{Contrastive Learning (CL)} is a self-supervised representation learning approach where an EM is trained to recognize similarities and differences between data samples. The core idea is to push similar or related samples (positive pairs) closer together in the embedding space, while pushing dissimilar samples (negative pairs) farther apart. 