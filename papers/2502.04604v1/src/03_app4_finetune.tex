
\subsection{Fine-Tuning the Model}\label{subsec:finetune}




\begin{figure}
\centering
\includegraphics[width=\linewidth]{img/03/fig_cl_example.pdf}
\caption{An example of Contrastive Learning training.} \label{fig:clexample}
\end{figure}


The EM in our analysis is used to encode source code into vectors where classes from the same microservices have similar vectors, and those from different microservices are farther apart. Through the model selection step, we created a benchmark of Pre-Trained Models. Now, we propose a Contrastive Learning based fine-tuning approach to adapt these models and create embeddings suited for the decomposition task. Let's consider the example in Fig. \ref{fig:clexample}. Code samples \textit{A} and \textit{B} belong to the same microservice \textit{Car} while \textit{C} belongs to the microservice \textit{Person}. Due to the near-identical source code of \textit{A} and \textit{C}, Pre-Trained Model embeddings are closer together, with \textit{B} farther apart (Fig. \ref{fig:clexample} part 2). Using CL with a tailored objective function and dataset, the model should learn to prioritize the semantic similarity between \textit{A} and \textit{B} over the syntax similarity between \textit{A} and \textit{C}, as shown in part 3.

In particular, we employ Contrastive Learning with the triplet loss function \cite{florian2015triplet}. The input to this function is a triplet of Java classes: \textit{anchor}, \textit{positive}, and \textit{negative}. The \textit{anchor} and \textit{positive} belong to the same microservice while the \textit{negative} class belongs to a different microservice. Using \textit{hard negatives} in triplet CL has been shown to enhance the training \cite{florian2015triplet} where \textit{hard negatives} refer to samples that are similar to the \textit{anchor} but should not be regrouped together. This is why the \textit{negative}, in our case, must belong to the same application. In fact, source code samples from the same application often share structural and syntactical similarities. Thus, using them as \textit{hard negatives} encourages the model to focus on the semantics related to microservices and business logic instead. The triplet loss is defined as follows: 
\begin{equation}\label{eq:tripletloss}
L(a, p, n) = \max(0, \|a - p\|_2 - \|a - n\|_2 + \alpha)
\end{equation}
Where \(a\), \(p\) and \(n\) are the embeddings of the \textit{anchor}, \textit{positive} and \textit{negative} samples. \(\alpha\) is the margin between \textit{positive} and \textit{negative} pairs and \(\|\cdot\|_2\) denotes the Euclidean norm.





For the smaller models (e.g. UnixCoder), we train all of their weights. As for LLMs, we employ the Low-Rank Adaptation (LoRA) method \cite{edward2022lora} to significantly reduce the computational cost and memory requirements while keeping a competitive performance. LoRA has been utilized by recent state-of-the-art models \cite{parishad2024llm2vec,lee2024nvembed}. As for DT LLMs, we adopted the following instruction to enhance model performance \cite{muennighoff2024gritlm,lee2024nvembed,parishad2024llm2vec}:

\begin{center}
\resizebox{\columnwidth}{!}{
\texttt{Given the source code, retrieve the bounded contexts;}}
\end{center}
