\subsection{RQ3: The Clustering Algorithms}\label{subsec:rq3clustering}







\begin{figure}[htbp]
\centering
\includegraphics[width=\linewidth]{img/04/fig_rq3_boxplots_concat.pdf}
\caption{Evaluation $F_\beta$ scores for each clustering algorithm.} \label{fig:rq3boxplots}
\end{figure}


\subsubsection{Experimental Setup}
We experiment with different clustering algorithms and evaluate their general performance on the decomposition problem when used in conjunction with different LMs and across different input configurations. We select 7 popular clustering algorithms and we chose multiple values for their most impactful hyper-parameters. Using this setup, we generate multiple decompositions with the FT-Models: \textit{ME-llm2vec-340K}, \textit{ME-SFR-Mistral}, \textit{ME-gritlm}, \textit{ME-codet5-340K} and \textit{ME-unixcoder-340K} on the applications from Table \ref{tab:microapps}. We include, as well, the benchmarks VoyageAI and NVEmbed.




\subsubsection{Metrics}
To evaluate the clustering algorithms in the context of our problem, we need to use metrics that can quantify how close is the generated decomposition from the actual microservices. For this reason, we rely on the relationship between the classes and the microservices they belong to. For an application, for which we know the true microservices, we can evaluate a decomposition by transforming it into a binary vector of unique class permutations. Each value represents whether the corresponding class couple is in the same microservice or not. This vector represents the "predictions" while the "labels" can be extracted using the same approach on the true partitioning. The evaluation score, afterwards, is measured by applying the $F_\beta$ formula on the predictions and labels vectors in order to account for both precision and recall. $F_\beta$, with a $\beta$ value equal to 0.25, was chosen over the more commonly used $F_1$ score because of the unbalanced nature of the relationship vectors \cite{khaled2024rldec}. Higher values of $F_\beta$ are better.


\subsubsection{Results}

Fig. \ref{fig:rq3boxplots} presents $F_\beta$ scores for various clustering algorithms with all models and with \textit{ME-llm2vec-340K} results. For All models, \textit{Affinity Propagation} \cite{brenden2007affinity} achieved the highest median $F_\beta$, followed closely by \textit{K-Means}, which showed higher maximum and minimum values. \textit{Hierarchical} clustering \cite{ward1963hierarchical}, despite having a lower median score, achieved the highest $F_\beta$. Both \textit{HDSCAN} \cite{ricardo2013hdbscan} and \textit{OPTICS} \cite{ankerst1999optics} had better scores than the algorithm they extend, \textit{DBSCAN} \cite{ester1996dbscan}, with \textit{HDSCAN} reaching a notably higher maximum than \textit{OPTICS}. Mean-Shift \cite{comaniciu2002meanshift}, however, had the lowest scores. In Fig. \ref{fig:rq3boxplots} for \textit{ME-llm2vec}, most algorithms showed higher median scores while their order is intact. Notably, \textit{K-Means}' median score increased significantly, approaching that of \textit{Affinity Propagation}.

\textit{K-Means} and \textit{Hierarchical} have achieved high maximum scores making them equally valid choices if the target number of microservices is known. However, \textit{Affinity Propagation} scored higher on average making it a better default option. Although \textit{HDSCAN} is less dependent on hyper-parameters, its low median score shows that it is not as consistent. Inspecting the hyper-parameter details further, we observed that damping values in the range (0.5, 0.8) for \textit{Affinity Propagation} provide a consistent score which can drop when above this threshold. 

% As for similarity-based algorithms, like \textit{HDSCAN}, we observed that the cosine similarity provides better results on average.

\begin{tcolorbox}[colback=gray!10!white, colframe=gray!90!black]
\textbf{Summary:} Affinity Propagation provides on average better results while K-Means and Hierarchical have higher scores when given the number of microservices.
\end{tcolorbox}