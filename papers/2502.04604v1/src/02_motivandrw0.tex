


% \subsection{Motivating example}
\input{src/02_motivandrw1_example}

\section{Background and Related Work}

% \subsection{Background}
\input{src/02_motivandrw2_background}







\subsection{Related Work}
Most decomposition approaches rely on static analysis or dynamic analysis in order to extract microservices from a monolithic application. For example, TopicDec \cite{brito2021topicmodeling} relies on Topic Modeling methods and static analysis to extract domain and syntax level representation of the classes in a monolith. In a similar fashion, HierDec \cite{khaled2022hierdecomp} and MSExtractor \cite{khaled2022msextractor,saidani2019msextractor} apply static analysis and traditional Natural Language Processing pipelines to extract the structural and semantic relationships between the classes. On the other hand, approaches such as FoSCI \cite{jin2021fosci} and Mono2Micro \cite{kalia2021mono2micro} rely mainly on dynamic analysis in conjunction with clustering or genetic algorithms to recommend microservices. HyDec \cite{khaled2022hydecomp} attempts to combine both analysis methods and avoid their drawbacks. Other methods have used different inputs and representations such as MEM \cite{mazlami2017mem} which relied on the version history and software evolution \cite{benomar2015evolution} and the semi-automated methods \cite{daoud2023multimodeldec,li2019dataflowdec} like ServiceCutter \cite{gysel2016servicecutter} which utilized the design artifacts or dataflow diagrams. More recently, there has been an interest in incorporating the databases in the decomposition as seen by the approaches CHGNN \cite{mathai2022chgnn}, CARGO \cite{vikram2022cargo} and DataCentric \cite{yamina2022datacentric}. 




Several Neural Network-based approaches have been proposed for the decomposition problem. CoGCN \cite{desai2021cogcn} employs a Graph Neural Network (GNN) to learn class partitioning and detect outliers through dynamic analysis. Deeply \cite{yedida2023deeply} and CHGNN \cite{mathai2022chgnn} enhance this method with hyper-parameter tuning, a novel loss function and additional input representations while GDC-DVF \cite{qian2023gdcdvf} combines structural and business representations in its GNN-based approach. MicroMiner \cite{trabelsi2023microminer} offers a 3-step framework, utilizing CodeBERT \cite{feng2020codebert} embeddings and a classifier to categorize source code fragments. In contrast, Code2VecDec \cite{aldebagy2021code2vec} leverages the Code2Vec \cite{alon2018code2vec} embedding model to generate class vectors, which are then used for clustering. While GNN-based decomposition methods incorporate some representation learning, it's limited to their training applications and input analysis approaches. For example, CoGCN's encoder learns class embeddings but requires training for each application based on structural and dynamic analysis results. To our knowledge, only MicroMiner and Code2VecDec have utilized Transformers or static embedding models in decompositions. Our approach extends this by comparing the performance of various models, including state-of-the-art Large Language Models, and proposing a fine-tuning method to enhance their effectiveness. This strategy aims to deepen the use of such models in addressing the decomposition problem, going beyond the limitations of existing methods.