
\documentclass[sigconf]{acmart}



% \usepackage{cite}
% \usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\usepackage{url}
\def\UrlBreaks{\do\/\do-}
% \usepackage{breakurl}
% \usepackage[breaklinks]{hyperref}
% \usepackage[breaklinks,hidelinks]{hyperref}
\usepackage{booktabs}

\usepackage{cleveref}[2012/02/15]
\crefformat{footnote}{#2\footnotemark[#1]#3}
\usepackage[linesnumbered,commentsnumbered,ruled,vlined]{algorithm2e}

%Not important
\usepackage{color,soul}

\usepackage{ulem}
\usepackage{makecell}
\usepackage{tcolorbox}





\begin{document}



\title{Contrastive Learning-Enhanced Large Language Models for Monolith-to-Microservice Decomposition}



\author{Khaled Sellami}
\email{khaled.sellami.1@ulaval.ca}
\orcid{0000-0002-6595-2489}
\affiliation{%
  \institution{Laval University}
  \city{Québec}
  \state{QC}
  \country{Canada}
}

\author{Mohamed Aymen Saied}
\email{mohamed-aymen.saied@ift.ulaval.ca}
\orcid{0000-0002-9488-645X}
\affiliation{%
  \institution{Laval University}
  \city{Québec}
  \state{QC}
  \country{Canada}
}



\begin{abstract}
  As Monolithic applications evolve, they become increasingly difficult to maintain and improve, leading to scaling and organizational issues. The Microservices architecture, known for its modularity, flexibility and scalability, offers a solution for large-scale applications allowing them to adapt and meet the demand on an ever increasing user base. Despite its advantages, migrating from a monolithic to a microservices architecture is often costly and complex, with the decomposition step being a significant challenge. This research addresses this issue by introducing MonoEmbed, a Language Model based approach for automating the decomposition process. MonoEmbed leverages state-of-the-art Large Language Models (LLMs) and representation learning techniques to generate representation vectors for monolithic components, which are then clustered to form microservices. By evaluating various pre-trained models and applying fine-tuning techniques such as Contrastive Learning and Low Rank Adaptation (LoRA), MonoEmbed aims to optimize these representations for microservice partitioning. The evaluation of the fine-tuned models showcases that they were able to significantly improve the quality of the representation vectors when compared with  pre-trained models and traditional representations. The proposed approach was benchmarked against existing decomposition methods, demonstrating superior performance in generating cohesive and balanced microservices for monolithic applications with varying scales. 
\end{abstract}



\settopmatter{printacmref=false}
\renewcommand\footnotetextcopyrightpermission[1]{}
\settopmatter{printacmref=false, printccs=false, printfolios=false}
\acmConference[ ]{ }{ }{ }
\acmBooktitle{ }
\acmPrice{}
\acmISBN{}
\acmDOI{}
\fancyhead[LE]{}   % Left side of even pages
\fancyhead[RO]{}   % Right side of odd pages


\keywords{Monolith Decompostion, Microservice, Language Model, Contrastive Learning}


\maketitle

% \section{Introduction}
\input{src/01_introduction}

% \section{Related Work}
\input{src/02_motivandrw0}

\input{src/03_app0}


\input{src/04_eval0}


% \section{Discussion and Threats to Validity}
\input{src/05_discussion}

\section{Conclusion}
We propose, MonoEmbed, a Language Model-based decomposition approach for efficient source code representation in monolithic applications. After evaluating various Pre-Trained Language Models, we fine-tune them for the decomposition task using Contrastive Learning. Our approach significantly improves embedding vector quality, creating similar representations for classes within the same microservices while those in different microservices are distinct. Experiments with clustering algorithms demonstrate that Affinity Propagation with our models better reflects original microservices. Compared to existing benchmarks, MonoEmbed generates more consistent and cohesive decompositions and scales effectively with larger applications. In future work, we would like to experiment with integrating our models with existing decomposition methods and explore new granularities and modalities.




\bibliographystyle{acm}
\bibliography{main}


\end{document}
\endinput

