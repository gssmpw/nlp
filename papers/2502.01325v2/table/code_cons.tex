\begin{table}
    \footnotesize
    \centering
    \caption{Cohen's Kappa consistency analysis between GPT-4o and human experts. This table shows the inter-rater agreement (Cohenâ€™s Kappa) between human experts (Expert 1 to Expert 4) and GPT-4o. Each value represents the level of agreement between expert pairs or between GPT-4o and individual experts. The final column provides Kappa values comparing GPT-4o to the consensus coding derived from majority voting among experts. In case of a tie, an additional education expert was consulted for arbitration. This consensus coding serves as the benchmark for consistency. Higher Kappa values indicate stronger agreement.}
    \label{tab:consistency_analysis} 
    \begin{tabular}{llccccc}
        \toprule
        \multirow{2}{*}{\textbf{Coding Task}} & \multirow{2}{*}{\textbf{Expert}} & \multicolumn{4}{c}{\textbf{Cohen's Kappa Value}} & \multirow{2}{*}{\textbf{Kappa Value w/ Consensus}} \\
        \cmidrule(lr){3-6}
        &  & Expert 1 & Expert 2 & Expert 3 & Expert 4 & \\
        \midrule
        \multirow{6}{*}{\textit{Conflict Coding}} 
        & Expert 1 & -- & 0.330 & 0.348 & 0.409 & \textbf{0.519} \\
        & Expert 2 & 0.330 & -- & 0.405 & 0.392 & \textbf{0.510} \\
        & Expert 3 & 0.348 & 0.405 & -- & 0.738 & \textbf{0.768} \\
        & Expert 4 & 0.409 & 0.392 & 0.738 & -- & \textbf{0.804} \\
        & \textbf{GPT-4o} & \textbf{0.410} & \textbf{0.444} & \textbf{0.458} & \textbf{0.500} & \textbf{0.517} \\
        \midrule
        \multirow{6}{*}{\textit{Behaviour Coding}} 
        & Expert 1 & -- & 0.473 & 0.457 & 0.467 & \textbf{0.587} \\
        & Expert 2 & 0.473 & -- & 0.524 & 0.627 & \textbf{0.697} \\
        & Expert 3 & 0.457 & 0.524 & -- & 0.802 & \textbf{0.797} \\
        & Expert 4 & 0.467 & 0.627 & 0.802 & -- & \textbf{0.852} \\
        & \textbf{GPT-4o} & \textbf{0.574} & \textbf{0.708} & \textbf{0.562} & \textbf{0.653} & \textbf{0.724} \\
        \bottomrule
    \end{tabular}
\end{table}


