\section{Related Work}
\subsection{Dynamic Novel View Synthesis}

The dynamic novel view synthesis problem has been widely studied in the literature. Current methods could be divided into two categories: the neural radiance field (NeRF)-based methods and the 3D Gaussian-based methods. The NeRF-based methods represent the scene as a continuous 5D function and achieve high-quality results. However, the pixel-based volume rendering are computationally expensive and are hard in achieving real-time rendering. 
The 3D Gaussian-based methods, on the other hand, represent the scene as a set of 3D Gaussians and are inherently suitable for real-time rendering. 

D-NeRF \cite{Dnerf} employs a large deformation MLP to capture the intricate coordinate transitions in dynamic simulated environments. Similarly, DyNeRF \cite{Dynerf} encodes temporal data using long vectors and decodes the features into radiance fields with large MLPs, enabling high-quality novel view synthesis in real dynamic scenes. In \cite{park2023temporal}, an interpolation-based MLP representation for dynamic scenes is introduced, offering a significant speedup over DyNeRF. 

In the realm of tensor-based representations,  K-Planes \cite{kplanes} and HexPlane \cite{HexPlane_} decompose the 4D field $(x,y,z,t)$ into the product of 2D planes, achieving similar high-fidelity results with an intermediate model size (200MB) and training time (100 minutes) on the DyNeRF dataset.  HyperReel \cite{hyperreel} uses a network to predict sample locations and models dynamic scenes via a key-frame-based TensoRF \cite{tensorf} representation, ensuring high fidelity and real-time rendering without the need for custom CUDA kernels. 


The third category encompasses  voxel-based representations. TineuVox \cite{tineuvox} utilizes the deformation MLP from D-NeRF and incorporates a 3D voxel grid with multi-scale interpolation, resulting in high fidelity and fast convergence on the D-NeRF dataset. On the other hand, MixVoxels \cite{mixvoxels} calculates a variation field by measuring the standard deviation of all pixels over time, allowing it to decompose dynamic scenes into static and dynamic voxels. This approach, with its distinct representations for static and dynamic objects, achieves both high-fidelity results and extremely fast convergence (15 minutes) on the DyNeRF dataset. Additionally, \cite{nerfplayer} trains Instant-NGP \cite{ingp} and TensoRF \cite{tensorf} offline, streaming the models through feature channels for dynamic scene rendering. CD-NGP \cite{cdngp} splits the scene into multiple chunks and train different branches accordingly. They use a large base hash table to store the spatial features of the scene and use small auxiliary hash tables to cope with feature transitions, achieving high-quality results with a small memory footprint. 


4D Gaussian point clouds \cite{4Dgaussians,gs4dhust} and 3D Gaussians with deformation networks \cite{deformable3Dgaussians} are employed for real-time, high-fidelity rendering of dynamic scenes.