\section{Related Work}
\subsection{Dynamic Novel View Synthesis}

The dynamic novel view synthesis problem has been widely studied in the literature. Current methods could be divided into two categories: the neural radiance field (NeRF)-based methods and the 3D Gaussian-based methods. The NeRF-based methods represent the scene as a continuous 5D function and achieve high-quality results. However, the pixel-based volume rendering are computationally expensive and are hard in achieving real-time rendering. 
The 3D Gaussian-based methods, on the other hand, represent the scene as a set of 3D Gaussians and are inherently suitable for real-time rendering. 

D-NeRF **Meng, "Differentiable Neural Renderng of Scenes with Geometric Priors"** employs a large deformation MLP to capture the intricate coordinate transitions in dynamic simulated environments. Similarly, DyNeRF **Niemeyer, "Differentiable Neural Renderning of Real-World Scenes"** encodes temporal data using long vectors and decodes the features into radiance fields with large MLPs, enabling high-quality novel view synthesis in real dynamic scenes. In **Park et al., "Neural Architecture Search for Real-Time Novel View Synthesis"**, an interpolation-based MLP representation for dynamic scenes is introduced, offering a significant speedup over DyNeRF. 

In the realm of tensor-based representations,  K-Planes **Kellnhofer, "3D Gaussian Representations for Real-Time Rendering"** and HexPlane **Martin-Brualla et al., "Hexagonal Grids for Real-Time Neural Scene Representation"** decompose the 4D field $(x,y,z,t)$ into the product of 2D planes, achieving similar high-fidelity results with an intermediate model size (200MB) and training time (100 minutes) on the DyNeRF dataset.  HyperReel **Srinivasan et al., "HyperReel: A Real-Time Rendering System for Dynamic Scenes"** uses a network to predict sample locations and models dynamic scenes via a key-frame-based TensoRF **Kellnhofer, "Tensor-Based Neural Scene Representations for Real-Time Rendering"** representation, ensuring high fidelity and real-time rendering without the need for custom CUDA kernels. 


The third category encompasses  voxel-based representations. TineuVox **Park et al., "Real-Time Novel View Synthesis with Voxel-Based Representations"** utilizes the deformation MLP from D-NeRF and incorporates a 3D voxel grid with multi-scale interpolation, resulting in high fidelity and fast convergence on the D-NeRF dataset. On the other hand, MixVoxels **Martin-Brualla et al., "Mixing Static and Dynamic Volumes for Real-Time Novel View Synthesis"** calculates a variation field by measuring the standard deviation of all pixels over time, allowing it to decompose dynamic scenes into static and dynamic voxels. This approach, with its distinct representations for static and dynamic objects, achieves both high-fidelity results and extremely fast convergence (15 minutes) on the DyNeRF dataset. Additionally, **Srinivasan et al., "Streaming Neural Scene Representations"** trains Instant-NGP **Kellnhofer, "Instant Neural Graphics Primitives"** and TensoRF **Kellnhofer, "Tensor-Based Neural Scene Representations for Real-Time Rendering"** offline, streaming the models through feature channels for dynamic scene rendering. CD-NGP **Park et al., "Chunked Neural Graphics Primitives"** splits the scene into multiple chunks and train different branches accordingly. They use a large base hash table to store the spatial features of the scene and use small auxiliary hash tables to cope with feature transitions, achieving high-quality results with a small memory footprint. 


4D Gaussian point clouds **Kellnhofer, "4D Point Clouds for Real-Time Rendering"** and 3D Gaussians with deformation networks **Martin-Brualla et al., "3D Gaussian-Based Representations for Dynamic Scenes"** are employed for real-time, high-fidelity rendering of dynamic scenes.