\section{Related Work}
\vspace{-2mm}

\noindent \textbf{Post-training KV Cache Compression.} 
Post-training KV cache compression methods____ fall into four categories: token-wise eviction, token-wise merging, static layer-wise reduction, and quantization. Token-wise eviction (e.g., StreamingLLM____) retains key tokens for sequence generation, while $\text{H}{2}\text{O}$____, SnapKV ____,ParallelComp____, and UNComp____ focus on compact subsets, potentially sacrificing context. Token-wise merging (e.g., CaM____, $\text{D}{2} \text{O}$____) re-integrates tokens to maintain context. Static layer-wise reduction (e.g., PyramidKV____) linearly reduces cache across layers but ignores inter-layer attention variations. Quantization (e.g., KIVI____, Gear____) balances memory and precision.
%
Most methods focus on text-based KV compression, overlooking multimodal contexts. LOOK-M____ addresses multimodal compression but uses fixed allocation, neglecting inter-layer attention differences. MEDA introduces a multimodal attention entropy-guided dynamic allocation to address this.

% \text{ }

\noindent \textbf{Vision Token Compression for MLLMs.} 
Classical approaches such as MobileVLM____, LLaVA-Prumerge____, MADTP____, and FastV____ focus on reducing image tokens, which dominate the total token count, to accelerate inference by removing redundancies. MobileVLM____ uses a lightweight projector with average pooling to compress visual tokens, while LLaVA-Prumerge____ and MADTP____ adopt adaptive strategies to reduce tokens while preserving performance. FastV____ offers a plug-and-play solution that optimizes early layer computations and prunes visual tokens in later layers.
%
In contrast, MEDA focuses on multimodal KV cache compression through a dynamic layer-wise allocation strategy, eliminating the need for additional fine-tuning and enhance the efficiency of multimodal long-context generative inference.

% \text{ }

\noindent \textbf{Long-context MLLMs.}  
Recent works have expanded MLLMs' multimodal long-context capabilities through additional training. ____ leverage Blockwise RingAttention for scalable long-sequence training.  LongVA____ first pre-trains LLMs on long-text sequences and then aligns Long LLMs using short vision data to generalize to multimodal long-text contexts. LongLLaVA____ modifies the model architecture by integrating Mamba and Transformer blocks and employs a progressive training strategy using multiple images. Video-XL____ introduces visual context latent summarization to train models for handling even longer multimodal token sequences.
%
In contrast, MEDA introduces a dynamic KV cache optimization algorithm, enhancing long-context multimodal inference without additional training and is compatible with these methods.


\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.9\textwidth]{latex/figs/final_meda_crop.pdf}
    % \vspace{-0.15in}
    \caption{\small{ 
Illustration of MEDA's multimodal attention entropy-guided dynamic KV cache allocation and merging strategy.}}
    % \vspace{-0.15in}
    \label{fig: samplen}
\end{figure*}

%Methodology of MEDA