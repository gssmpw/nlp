@article{Cao2024MADTPMA,
  title={MADTP: Multimodal Alignment-Guided Dynamic Token Pruning for Accelerating Vision-Language Transformer},
  author={Jianjian Cao and Peng Ye and Shengze Li and Chong Yu and Yansong Tang and Jiwen Lu and Tao Chen},
  journal={ArXiv},
  year={2024},
  volume={abs/2403.02991},
  url={https://api.semanticscholar.org/CorpusID:268248344}
}

@article{Chen2024AnII,
  title={An Image is Worth 1/2 Tokens After Layer 2: Plug-and-Play Inference Acceleration for Large Vision-Language Models},
  author={Liang Chen and Haozhe Zhao and Tianyu Liu and Shuai Bai and Junyang Lin and Chang Zhou and Baobao Chang},
  journal={ArXiv},
  year={2024},
  volume={abs/2403.06764},
  url={https://api.semanticscholar.org/CorpusID:268358224}
}

@article{Li2024SnapKVLK,
  title={SnapKV: LLM Knows What You are Looking for Before Generation},
  author={Yuhong Li and Yingbing Huang and Bowen Yang and Bharat Venkitesh and Acyr F. Locatelli and Hanchen Ye and Tianle Cai and Patrick Lewis and Deming Chen},
  journal={ArXiv},
  year={2024},
  volume={abs/2404.14469},
  url={https://api.semanticscholar.org/CorpusID:269303164}
}

@article{Shang2024LLaVAPruMergeAT,
  title={LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal Models},
  author={Yuzhang Shang and Mu Cai and Bingxin Xu and Yong Jae Lee and Yan Yan},
  journal={ArXiv},
  year={2024},
  volume={abs/2403.15388},
  url={https://api.semanticscholar.org/CorpusID:268667281}
}

@article{Xiao2023EfficientSL,
  title={Efficient Streaming Language Models with Attention Sinks},
  author={Guangxuan Xiao and Yuandong Tian and Beidi Chen and Song Han and Mike Lewis},
  journal={ArXiv},
  year={2023},
  volume={abs/2309.17453},
  url={https://api.semanticscholar.org/CorpusID:263310483}
}

@inproceedings{Zhang2024CaMCM,
  title={CaM: Cache Merging for Memory-efficient LLMs Inference},
  author={Yuxin Zhang and Yuxuan Du and Gen Luo and Yunshan Zhong and Zhenyu Zhang and Shiwei Liu and Rongrong Ji},
  booktitle={International Conference on Machine Learning},
  year={2024},
  url={https://api.semanticscholar.org/CorpusID:272330701}
}

@article{chu2024mobilevlm,
  title={MobileVLM V2: Faster and Stronger Baseline for Vision Language Model},
  author={Chu, Xiangxiang and Qiao, Limeng and Zhang, Xinyu and Xu, Shuang and Wei, Fei and Yang, Yang and Sun, Xiaofei and Hu, Yiming and Lin, Xinyang and Zhang, Bo and others},
  journal={arXiv preprint arXiv:2402.03766},
  year={2024}
}

@article{kang2024gear,
  title={GEAR: An Efficient KV Cache Compression Recipefor Near-Lossless Generative Inference of LLM},
  author={Kang, Hao and Zhang, Qingru and Kundu, Souvik and Jeong, Geonhwa and Liu, Zaoxing and Krishna, Tushar and Zhao, Tuo},
  journal={arXiv preprint arXiv:2403.05527},
  year={2024}
}

@article{liu2024contemporary,
  title={Contemporary model compression on large language models inference},
  author={Liu, Dong},
  journal={arXiv preprint arXiv:2409.01990},
  year={2024}
}

@article{liu2024kivi,
  title={KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache},
  author={Liu, Zirui and Yuan, Jiayi and Jin, Hongye and Zhong, Shaochen and Xu, Zhaozhuo and Braverman, Vladimir and Chen, Beidi and Hu, Xia},
  journal={arXiv preprint arXiv:2402.02750},
  year={2024}
}

@article{liu2024world,
  title={World model on million-length video and language with blockwise ringattention},
  author={Liu, Hao and Yan, Wilson and Zaharia, Matei and Abbeel, Pieter},
  journal={arXiv preprint arXiv:2402.08268},
  year={2024}
}

@article{shu2024video,
  title={Video-XL: Extra-Long Vision Language Model for Hour-Scale Video Understanding},
  author={Shu, Yan and Zhang, Peitian and Liu, Zheng and Qin, Minghao and Zhou, Junjie and Huang, Tiejun and Zhao, Bo},
  journal={arXiv preprint arXiv:2409.14485},
  year={2024}
}

@article{wan2023efficient,
  title={Efficient large language models: A survey},
  author={Wan, Zhongwei and Wang, Xin and Liu, Che and Alam, Samiul and Zheng, Yu and Liu, Jiachen and Qu, Zhongnan and Yan, Shen and Zhu, Yi and Zhang, Quanlu and others},
  journal={arXiv preprint arXiv:2312.03863},
  year={2023}
}

@article{wan2024d2o,
  title={D2O: Dynamic Discriminative Operations for Efficient Generative Inference of Large Language Models},
  author={Wan, Zhongwei and Wu, Xinjian and Zhang, Yu and Xin, Yi and Tao, Chaofan and Zhu, Zhihong and Wang, Xin and Luo, Siqi and Xiong, Jing and Zhang, Mi},
  journal={arXiv preprint arXiv:2406.13035},
  year={2024}
}

@article{wan2024look,
  title={Look-m: Look-once optimization in kv cache for efficient multimodal long-context inference},
  author={Wan, Zhongwei and Wu, Ziang and Liu, Che and Huang, Jinfa and Zhu, Zhihong and Jin, Peng and Wang, Longyue and Yuan, Li},
  journal={arXiv preprint arXiv:2406.18139},
  year={2024}
}

@article{wang2024longllava,
  title={LongLLaVA: Scaling Multi-modal LLMs to 1000 Images Efficiently via Hybrid Architecture},
  author={Wang, Xidong and Song, Dingjie and Chen, Shunian and Zhang, Chen and Wang, Benyou},
  journal={arXiv preprint arXiv:2409.02889},
  year={2024}
}

@article{xiong2024uncomp,
  title={UNComp: Uncertainty-Aware Long-Context Compressor for Efficient Large Language Model Inference},
  author={Xiong, Jing and Shen, Jianghan and Ye, Fanghua and Tao, Chaofan and Wan, Zhongwei and Lu, Jianqiao and Wu, Xun and Zheng, Chuanyang and Guo, Zhijiang and Kong, Lingpeng and others},
  journal={arXiv preprint arXiv:2410.03090},
  year={2024}
}

@article{xiong2025parallelcomp,
  title={ParallelComp: Parallel Long-Context Compressor for Length Extrapolation},
  author={Xiong, Jing and Shen, Jianghan and Zheng, Chuanyang and Wan, Zhongwei and Zhao, Chenyang and Yang, Chiwun and Ye, Fanghua and Yang, Hongxia and Kong, Lingpeng and Wong, Ngai},
  journal={arXiv preprint arXiv:2502.14317},
  year={2025}
}

@article{zhang2024h2o,
  title={H2o: Heavy-hitter oracle for efficient generative inference of large language models},
  author={Zhang, Zhenyu and Sheng, Ying and Zhou, Tianyi and Chen, Tianlong and Zheng, Lianmin and Cai, Ruisi and Song, Zhao and Tian, Yuandong and R{\'e}, Christopher and Barrett, Clark and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{zhang2024long,
  title={Long context transfer from language to vision},
  author={Zhang, Peiyuan and Zhang, Kaichen and Li, Bo and Zeng, Guangtao and Yang, Jingkang and Zhang, Yuanhan and Wang, Ziyue and Tan, Haoran and Li, Chunyuan and Liu, Ziwei},
  journal={arXiv preprint arXiv:2406.16852},
  year={2024}
}

@article{zhang2024pyramidkv,
  title={PyramidKV: Dynamic KV Cache Compression based on Pyramidal Information Funneling},
  author={Zhang, Yichi and Gao, Bofei and Liu, Tianyu and Lu, Keming and Xiong, Wayne and Dong, Yue and Chang, Baobao and Hu, Junjie and Xiao, Wen and others},
  journal={arXiv preprint arXiv:2406.02069},
  year={2024}
}

