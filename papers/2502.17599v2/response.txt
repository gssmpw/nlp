\section{Related Work}
\vspace{-2mm}

\noindent \textbf{Post-training KV Cache Compression.} 
Post-training KV cache compression methods **Zhang, "StreamingLLM"** fall into four categories: token-wise eviction, token-wise merging, static layer-wise reduction, and quantization. Token-wise eviction (e.g., **Tang, "StreamingLLM"**) retains key tokens for sequence generation, while $\text{H}{2}\text{O}$**Kong, "H2O"**, SnapKV **Liu, "SnapKV"**, ParallelComp **Li, "ParallelComp"**, and UNComp **Wang, "UNComp"** focus on compact subsets, potentially sacrificing context. Token-wise merging (e.g., CaM **Zhou, "CaM"**, $\text{D}{2} \text{O}$**Chen, "D2O"**) re-integrates tokens to maintain context. Static layer-wise reduction (e.g., PyramidKV **Guo, "PyramidKV"**) linearly reduces cache across layers but ignores inter-layer attention variations. Quantization (e.g., KIVI **Xu, "KIVI"**, Gear **Wang, "Gear"**) balances memory and precision.
%
Most methods focus on text-based KV compression, overlooking multimodal contexts. LOOK-M**Zhang, "LOOKM"** addresses multimodal compression but uses fixed allocation, neglecting inter-layer attention differences. MEDA introduces a multimodal attention entropy-guided dynamic allocation to address this.

% \text{ }

\noindent \textbf{Vision Token Compression for MLLMs.} 
Classical approaches such as MobileVLM **Zhou, "MobileVLM"**, LLaVA-Prumerge **Wang, "LLaVA-Prumerge"**, MADTP **Liu, "MADTP"**, and FastV**Li, "FastV"** focus on reducing image tokens, which dominate the total token count, to accelerate inference by removing redundancies. MobileVLM **Zhou, "MobileVLM"** uses a lightweight projector with average pooling to compress visual tokens, while LLaVA-Prumerge **Wang, "LLaVA-Prumerge"** and MADTP **Liu, "MADTP"** adopt adaptive strategies to reduce tokens while preserving performance. FastV **Li, "FastV"** offers a plug-and-play solution that optimizes early layer computations and prunes visual tokens in later layers.
%
In contrast, MEDA focuses on multimodal KV cache compression through a dynamic layer-wise allocation strategy, eliminating the need for additional fine-tuning and enhance the efficiency of multimodal long-context generative inference.

% \text{ }

\noindent \textbf{Long-context MLLMs.}  
Recent works have expanded MLLMs' multimodal long-context capabilities through additional training. **Chen, "Blockwise RingAttention"** leverage Blockwise RingAttention for scalable long-sequence training.  LongVA**Liu, "LongVA"** first pre-trains LLMs on long-text sequences and then aligns Long LLMs using short vision data to generalize to multimodal long-text contexts. LongLLaVA**Wang, "LongLLaVA"** modifies the model architecture by integrating Mamba and Transformer blocks and employs a progressive training strategy using multiple images. Video-XL**Zhou, "Video-XL"** introduces visual context latent summarization to train models for handling even longer multimodal token sequences.
%
In contrast, MEDA introduces a dynamic KV cache optimization algorithm, enhancing long-context multimodal inference without additional training and is compatible with these methods.


\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.9\textwidth]{latex/figs/final_meda_crop.pdf}
    % \vspace{-0.15in}
    \caption{\small{ 
Illustration of MEDA's multimodal attention entropy-guided dynamic KV cache allocation and merging strategy.}}
    % \vspace{-0.15in}
    \label{fig: samplen}
\end{figure*}