% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").



@article{meta2024introducing,
  title={Introducing meta llama 3: The most capable openly available llm to date},
  author={Meta, AI},
  journal={Meta AI.},
  year={2024}
}

@article{xiong2024uncomp,
  title={UNComp: Uncertainty-Aware Long-Context Compressor for Efficient Large Language Model Inference},
  author={Xiong, Jing and Shen, Jianghan and Ye, Fanghua and Tao, Chaofan and Wan, Zhongwei and Lu, Jianqiao and Wu, Xun and Zheng, Chuanyang and Guo, Zhijiang and Kong, Lingpeng and others},
  journal={arXiv preprint arXiv:2410.03090},
  year={2024}
}

@article{wan2023efficient,
  title={Efficient large language models: A survey},
  author={Wan, Zhongwei and Wang, Xin and Liu, Che and Alam, Samiul and Zheng, Yu and Liu, Jiachen and Qu, Zhongnan and Yan, Shen and Zhu, Yi and Zhang, Quanlu and others},
  journal={arXiv preprint arXiv:2312.03863},
  year={2023}
}
@article{Yang2023TheDO,
  title={The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision)},
  author={Zhengyuan Yang and Linjie Li and Kevin Lin and Jianfeng Wang and Chung-Ching Lin and Zicheng Liu and Lijuan Wang},
  journal={ArXiv},
  year={2023},
  volume={abs/2309.17421},
  url={https://api.semanticscholar.org/CorpusID:263310951}
}

@article{Yin2023ASO,
  title={A Survey on Multimodal Large Language Models},
  author={Shukang Yin and Chaoyou Fu and Sirui Zhao and Ke Li and Xing Sun and Tong Xu and Enhong Chen},
  journal={ArXiv},
  year={2023},
  volume={abs/2306.13549},
  url={https://api.semanticscholar.org/CorpusID:259243718}
}

@article{team2023gemini,
  title={Gemini: a family of highly capable multimodal models},
  author={Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}

@article{Song2024MileBenchBM,
  title={MileBench: Benchmarking MLLMs in Long Context},
  author={Dingjie Song and Shunian Chen and Guiming Hardy Chen and Fei Yu and Xiang Wan and Benyou Wang},
  journal={ArXiv},
  year={2024},
  volume={abs/2404.18532},
  url={https://api.semanticscholar.org/CorpusID:269449774}
}

@article{Zhang2023H2OHO,
  title={H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models},
  author={Zhenyu (Allen) Zhang and Ying Sheng and Tianyi Zhou and Tianlong Chen and Lianmin Zheng and Ruisi Cai and Zhao Song and Yuandong Tian and Christopher R{\'e} and Clark W. Barrett and Zhangyang Wang and Beidi Chen},
  journal={ArXiv},
  year={2023},
  volume={abs/2306.14048},
  url={https://api.semanticscholar.org/CorpusID:259263947}
}

@article{Chu2024MobileVLMVF,
  title={MobileVLM V2: Faster and Stronger Baseline for Vision Language Model},
  author={Xiangxiang Chu and Limeng Qiao and Xinyu Zhang and Shuang Xu and Fei Wei and Yang Yang and Xiaofei Sun and Yiming Hu and Xinyang Lin and Bo Zhang and Chunhua Shen},
  journal={ArXiv},
  year={2024},
  volume={abs/2402.03766},
  url={https://api.semanticscholar.org/CorpusID:267500104}
}
@article{Chen2023InternVLSU,
  title={InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks},
  author={Zhe Chen and Jiannan Wu and Wenhai Wang and Weijie Su and Guo Chen and Sen Xing and Zhong Muyan and Qinglong Zhang and Xizhou Zhu and Lewei Lu and Bin Li and Ping Luo and Tong Lu and Yu Qiao and Jifeng Dai},
  journal={ArXiv},
  year={2023},
  volume={abs/2312.14238},
  url={https://api.semanticscholar.org/CorpusID:266521410}
}
@article{Liu2023VisualIT,
  title={Visual Instruction Tuning},
  author={Haotian Liu and Chunyuan Li and Qingyang Wu and Yong Jae Lee},
  journal={ArXiv},
  year={2023},
  volume={abs/2304.08485},
  url={https://api.semanticscholar.org/CorpusID:258179774}
}

@article{Ge2023ModelTY,
  title={Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs},
  author={Suyu Ge and Yunan Zhang and Liyuan Liu and Minjia Zhang and Jiawei Han and Jianfeng Gao},
  journal={ArXiv},
  year={2023},
  volume={abs/2310.01801},
  url={https://api.semanticscholar.org/CorpusID:263609075}
}
@article{Ren2024OnTE,
  title={On the Efficacy of Eviction Policy for Key-Value Constrained Generative Language Model Inference},
  author={Siyu Ren and Kenny Q. Zhu},
  journal={ArXiv},
  year={2024},
  volume={abs/2402.06262},
  url={https://api.semanticscholar.org/CorpusID:267617273}
}

@article{Li2024SnapKVLK,
  title={SnapKV: LLM Knows What You are Looking for Before Generation},
  author={Yuhong Li and Yingbing Huang and Bowen Yang and Bharat Venkitesh and Acyr F. Locatelli and Hanchen Ye and Tianle Cai and Patrick Lewis and Deming Chen},
  journal={ArXiv},
  year={2024},
  volume={abs/2404.14469},
  url={https://api.semanticscholar.org/CorpusID:269303164}
}

################### for related work ###############################33
############ token merge ######################3

@article{lin2024rotation,
  title={Rotation and Permutation for Advanced Outlier Management and Efficient Quantization of LLMs},
  author={Lin, Haokun and Xu, Haobo and Wu, Yichen and Cui, Jingzhi and Zhang, Yingtao and Mou, Linzhan and Song, Linqi and Sun, Zhenan and Wei, Ying},
  journal={arXiv preprint arXiv:2406.01721},
  year={2024}
}

@inproceedings{lin2024mope,
  title={Mope-clip: Structured pruning for efficient vision-language models with module-wise pruning error metric},
  author={Lin, Haokun and Bai, Haoli and Liu, Zhili and Hou, Lu and Sun, Muyi and Song, Linqi and Wei, Ying and Sun, Zhenan},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={27370--27380},
  year={2024}
}
@article{liu2024intactkv,
  title={IntactKV: Improving Large Language Model Quantization by Keeping Pivot Tokens Intact},
  author={Liu, Ruikang and Bai, Haoli and Lin, Haokun and Li, Yuening and Gao, Han and Xu, Zhengzhuo and Hou, Lu and Yao, Jun and Yuan, Chun},
  journal={arXiv preprint arXiv:2403.01241},
  year={2024}
}
@inproceedings{zhang2024plug,
  title={Plug-and-play: An efficient post-training pruning method for large language models},
  author={Zhang, Yingtao and Bai, Haoli and Lin, Haokun and Zhao, Jialin and Hou, Lu and Cannistraci, Carlo Vittorio},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}
#########################

@article{liu2024contemporary,
  title={Contemporary model compression on large language models inference},
  author={Liu, Dong},
  journal={arXiv preprint arXiv:2409.01990},
  year={2024}
}

@article{wang2024svd,
  title={Svd-llm: Truncation-aware singular value decomposition for large language model compression},
  author={Wang, Xin and Zheng, Yu and Wan, Zhongwei and Zhang, Mi},
  journal={arXiv preprint arXiv:2403.07378},
  year={2024}
}

@article{shen2024famba,
  title={Famba-v: Fast vision mamba with cross-layer token fusion},
  author={Shen, Hui and Wan, Zhongwei and Wang, Xin and Zhang, Mi},
  journal={arXiv preprint arXiv:2409.09808},
  year={2024}
}
@article{li2024uncertaintyrag,
  title={UncertaintyRAG: Span-Level Uncertainty Enhanced Long-Context Modeling for Retrieval-Augmented Generation},
  author={Li, Zixuan and Xiong, Jing and Ye, Fanghua and Zheng, Chuanyang and Wu, Xun and Lu, Jianqiao and Wan, Zhongwei and Liang, Xiaodan and Li, Chengming and Sun, Zhenan and others},
  journal={arXiv preprint arXiv:2410.02719},
  year={2024}
}

@article{wang2024iot,
  title={Iot in the era of generative ai: Vision and challenges},
  author={Wang, Xin and Wan, Zhongwei and Hekmati, Arvin and Zong, Mingyu and Alam, Samiul and Zhang, Mi and Krishnamachari, Bhaskar},
  journal={IEEE Internet Computing},
  year={2024},
  publisher={IEEE}
}


@article{wan2024electrocardiogram,
  title={Electrocardiogram Instruction Tuning for Report Generation},
  author={Wan, Zhongwei and Liu, Che and Wang, Xin and Tao, Chaofan and Shen, Hui and Peng, Zhenwu and Fu, Jie and Arcucci, Rossella and Yao, Huaxiu and Zhang, Mi},
  journal={arXiv preprint arXiv:2403.04945},
  year={2024}
}


@article{wan2024med,
  title={Med-unic: Unifying cross-lingual medical vision-language pre-training by diminishing bias},
  author={Wan, Zhongwei and Liu, Che and Zhang, Mi and Fu, Jie and Wang, Benyou and Cheng, Sibo and Ma, Lei and Quilodr{\'a}n-Casas, C{\'e}sar and Arcucci, Rossella},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{wan2022g,
  title={G-map: general memory-augmented pre-trained language model for domain tasks},
  author={Wan, Zhongwei and Yin, Yichun and Zhang, Wei and Shi, Jiaxin and Shang, Lifeng and Chen, Guangyong and Jiang, Xin and Liu, Qun},
  journal={arXiv preprint arXiv:2212.03613},
  year={2022}
}

@inproceedings{liu2024etp,
  title={Etp: Learning transferable ecg representations via ecg-text pre-training},
  author={Liu, Che and Wan, Zhongwei and Cheng, Sibo and Zhang, Mi and Arcucci, Rossella},
  booktitle={ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={8230--8234},
  year={2024},
  organization={IEEE}
}


@article{liu2024zero,
  title={Zero-shot ecg classification with multimodal learning and test-time clinical knowledge enhancement},
  author={Liu, Che and Wan, Zhongwei and Ouyang, Cheng and Shah, Anand and Bai, Wenjia and Arcucci, Rossella},
  journal={arXiv preprint arXiv:2403.06659},
  year={2024}
}

@article{zheng2024structure,
  title={Structure-based Drug Design Benchmark: Do 3D Methods Really Dominate?},
  author={Zheng, Kangyu and Lu, Yingzhou and Zhang, Zaixi and Wan, Zhongwei and Ma, Yao and Zitnik, Marinka and Fu, Tianfan},
  journal={arXiv e-prints},
  pages={arXiv--2406},
  year={2024}
}

@article{wan2023spatio,
  title={Spatio-temporal Contrastive Learning-enhanced GNNs for Session-based Recommendation},
  author={Wan, Zhongwei and Liu, Xin and Wang, Benyou and Qiu, Jiezhong and Li, Boyu and Guo, Ting and Chen, Guangyong and Wang, Yang},
  journal={ACM Transactions on Information Systems},
  volume={42},
  number={2},
  pages={1--26},
  year={2023},
  publisher={ACM New York, NY, USA}
}

@article{wan2023text,
  title={Text classification: A perspective of deep learning methods},
  author={Wan, Zhongwei},
  journal={arXiv preprint arXiv:2309.13761},
  year={2023}
}
@article{wan2024meit,
  title={MEIT: Multi-modal electrocardiogram instruction tuning on large language models for report generation},
  author={Wan, Zhongwei and Liu, Che and Wang, Xin and Tao, Chaofan and Shen, Hui and Peng, Zhenwu and Fu, Jie and Arcucci, Rossella and Yao, Huaxiu and Zhang, Mi},
  journal={arXiv preprint arXiv:2403.04945},
  year={2024}
}
@article{liu2024benchmarking,
  title={Benchmarking and boosting radiology report generation for 3D high-resolution medical images},
  author={Liu, Che and Wan, Zhongwei and Wang, Yuqi and Shen, Hui and Wang, Haozhe and Zheng, Kangyu and Zhang, Mi and Arcucci, Rossella},
  journal={arXiv preprint arXiv:2406.07146},
  year={2024}
}
@article{gong2024neuroclips,
  title={NeuroClips: Towards High-fidelity and Smooth fMRI-to-Video Reconstruction},
  author={Gong, Zixuan and Bao, Guangyin and Zhang, Qi and Wan, Zhongwei and Miao, Duoqian and Wang, Shoujin and Zhu, Lei and Wang, Changwei and Xu, Rongtao and Hu, Liang and others},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={51655--51683},
  year={2024}
}
@inproceedings{zhu2024dglf,
  title={DGLF: A Dual Graph-based Learning Framework for Multi-modal Sarcasm Detection},
  author={Zhu, Zhihong and Shen, Kefan and Chen, Zhaorun and Zhang, Yunyan and Chen, Yuyan and Jiao, Xiaoqi and Wan, Zhongwei and Xie, Shaorong and Liu, Wei and Wu, Xian and others},
  booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  pages={2900--2912},
  year={2024}
}
@article{huang2024evolver,
  title={Evolver: Chain-of-Evolution Prompting to Boost Large Multimodal Models for Hateful Meme Detection},
  author={Huang, Jinfa and Pan, Jinsheng and Wan, Zhongwei and Lyu, Hanjia and Luo, Jiebo},
  journal={arXiv preprint arXiv:2407.21004},
  year={2024}
}
@article{chen2025recent,
  title={Recent Advances in Large Langauge Model Benchmarks against Data Contamination: From Static to Dynamic Evaluation},
  author={Chen, Simin and Chen, Yiming and Li, Zexin and Jiang, Yifan and Wan, Zhongwei and He, Yixin and Ran, Dezhi and Gu, Tianle and Li, Haizhou and Xie, Tao and others},
  journal={arXiv preprint arXiv:2502.17521},
  year={2025}
}

@article{liang2020many,
  title={Many-Objective Estimation of Distribution Optimization Algorithm Based on WGAN-GP},
  author={Liang, Zhenyu and Li, Yunfan and Wan, Zhongwei},
  journal={arXiv preprint arXiv:2003.08295},
  year={2020}
}
@article{liang2020large,
  title={Large scale many-objective optimization driven by distributional adversarial networks},
  author={Liang, Zhenyu and Li, Yunfan and Wan, Zhongwei},
  journal={arXiv preprint arXiv:2003.07013},
  year={2020}
}

@article{bolya2022token,
  title={Token merging: Your vit but faster},
  author={Bolya, Daniel and Fu, Cheng-Yang and Dai, Xiaoliang and Zhang, Peizhao and Feichtenhofer, Christoph and Hoffman, Judy},
  journal={arXiv preprint arXiv:2210.09461},
  year={2022}
}
@inproceedings{Yun2024FocusOT,
  title={Focus on the Core: Efficient Attention via Pruned Token Compression for Document Classification},
  author={Jungmin Yun and Mihyeon Kim and Youngbin Kim},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2024},
  url={https://api.semanticscholar.org/CorpusID:266167105}
}

@article{Tang2023DynamicTP,
  title={Dynamic Token Pruning in Plain Vision Transformers for Semantic Segmentation},
  author={Quan Tang and Bowen Zhang and Jiajun Liu and Fagui Liu and Yifan Liu},
  journal={2023 IEEE/CVF International Conference on Computer Vision (ICCV)},
  year={2023},
  pages={777-786},
  url={https://api.semanticscholar.org/CorpusID:260379178}
}
@article{Song2022CPViTCV,
  title={CP-ViT: Cascade Vision Transformer Pruning via Progressive Sparsity Prediction},
  author={Zhuoran Song and Yihong Xu and Zhezhi He and Li Jiang and Naifeng Jing and Xiaoyao Liang},
  journal={ArXiv},
  year={2022},
  volume={abs/2203.04570},
  url={https://api.semanticscholar.org/CorpusID:247319015}
}
@inproceedings{Kong2021SPViTEF,
  title={SPViT: Enabling Faster Vision Transformers via Latency-Aware Soft Token Pruning},
  author={Zhenglun Kong and Peiyan Dong and Xiaolong Ma and Xin Meng and Wei Niu and Mengshu Sun and Bin Ren and Minghai Qin and Hao Tang and Yanzhi Wang},
  booktitle={European Conference on Computer Vision},
  year={2021},
  url={https://api.semanticscholar.org/CorpusID:245537400}
}

@inproceedings{Devlin2019BERTPO,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  booktitle={North American Chapter of the Association for Computational Linguistics},
  year={2019},
  url={https://api.semanticscholar.org/CorpusID:52967399}
}

@article{Shang2024LLaVAPruMergeAT,
  title={LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal Models},
  author={Yuzhang Shang and Mu Cai and Bingxin Xu and Yong Jae Lee and Yan Yan},
  journal={ArXiv},
  year={2024},
  volume={abs/2403.15388},
  url={https://api.semanticscholar.org/CorpusID:268667281}
}

@inproceedings{wei2023joint,
  title={Joint token pruning and squeezing towards more aggressive compression of vision transformers},
  author={Wei, Siyuan and Ye, Tianzhu and Zhang, Shen and Tang, Yao and Liang, Jiajun},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={2092--2101},
  year={2023}
}

@article{chu2024mobilevlm,
  title={MobileVLM V2: Faster and Stronger Baseline for Vision Language Model},
  author={Chu, Xiangxiang and Qiao, Limeng and Zhang, Xinyu and Xu, Shuang and Wei, Fei and Yang, Yang and Sun, Xiaofei and Hu, Yiming and Lin, Xinyang and Zhang, Bo and others},
  journal={arXiv preprint arXiv:2402.03766},
  year={2024}
}


@article{Cao2023PuMerPA,
  title={PuMer: Pruning and Merging Tokens for Efficient Vision Language Models},
  author={Qingqing Cao and Bhargavi Paranjape and Hannaneh Hajishirzi},
  journal={ArXiv},
  year={2023},
  volume={abs/2305.17530},
  url={https://api.semanticscholar.org/CorpusID:258959382}
}
@article{Cao2024MADTPMA,
  title={MADTP: Multimodal Alignment-Guided Dynamic Token Pruning for Accelerating Vision-Language Transformer},
  author={Jianjian Cao and Peng Ye and Shengze Li and Chong Yu and Yansong Tang and Jiwen Lu and Tao Chen},
  journal={ArXiv},
  year={2024},
  volume={abs/2403.02991},
  url={https://api.semanticscholar.org/CorpusID:268248344}
}

@article{Xiao2023EfficientSL,
  title={Efficient Streaming Language Models with Attention Sinks},
  author={Guangxuan Xiao and Yuandong Tian and Beidi Chen and Song Han and Mike Lewis},
  journal={ArXiv},
  year={2023},
  volume={abs/2309.17453},
  url={https://api.semanticscholar.org/CorpusID:263310483}
}

@article{Chen2024AnII,
  title={An Image is Worth 1/2 Tokens After Layer 2: Plug-and-Play Inference Acceleration for Large Vision-Language Models},
  author={Liang Chen and Haozhe Zhao and Tianyu Liu and Shuai Bai and Junyang Lin and Chang Zhou and Baobao Chang},
  journal={ArXiv},
  year={2024},
  volume={abs/2403.06764},
  url={https://api.semanticscholar.org/CorpusID:268358224}
}

##################### extra ###############

@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@Misc{accelerate,
  title =        {Accelerate: Training and inference at scale made simple, efficient and adaptable.},
  author =       {Sylvain Gugger and Lysandre Debut and Thomas Wolf and Philipp Schmid and Zachary Mueller and Sourab Mangrulkar and Marc Sun and Benjamin Bossan},
  howpublished = {\url{https://github.com/huggingface/accelerate}},
  year =         {2022}
}
@article{Aminabadi2022DeepSpeedIE,
  title={DeepSpeed- Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale},
  author={Reza Yazdani Aminabadi and Samyam Rajbhandari and Minjia Zhang and Ammar Ahmad Awan and Cheng Li and Du Li and Elton Zheng and Jeff Rasley and Shaden Smith and Olatunji Ruwase and Yuxiong He},
  journal={SC22: International Conference for High Performance Computing, Networking, Storage and Analysis},
  year={2022},
  pages={1-15},
}
@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}
@article{bai2023longbench,
  title={Longbench: A bilingual, multitask benchmark for long context understanding},
  author={Bai, Yushi and Lv, Xin and Zhang, Jiajie and Lyu, Hongchang and Tang, Jiankai and Huang, Zhidian and Du, Zhengxiao and Liu, Xiao and Zeng, Aohan and Hou, Lei and others},
  journal={arXiv preprint arXiv:2308.14508},
  year={2023}
}

@article{rae2019compressive,
  title={Compressive transformers for long-range sequence modelling},
  author={Rae, Jack W and Potapenko, Anna and Jayakumar, Siddhant M and Lillicrap, Timothy P},
  journal={arXiv preprint arXiv:1911.05507},
  year={2019}
}

@article{gao2021framework,
  title={A framework for few-shot language model evaluation},
  author={Gao, Leo and Tow, Jonathan and Biderman, Stella and Black, Sid and DiPofi, Anthony and Foster, Charles and Golding, Laurence and Hsu, Jeffrey and McDonell, Kyle and Muennighoff, Niklas and others},
  journal={Version v0. 0.1. Sept},
  pages={8},
  year={2021}
}
@article{zheng2024judging,
  title={Judging llm-as-a-judge with mt-bench and chatbot arena},
  author={Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@article{Touvron2023LLaMAOA,
  title={LLaMA: Open and Efficient Foundation Language Models},
  author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timoth{\'e}e Lacroix and Baptiste Rozi{\`e}re and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
  journal={ArXiv},
  year={2023},
  volume={abs/2302.13971},
  url={https://api.semanticscholar.org/CorpusID:257219404}
}
@inproceedings{DBLP:conf/iclr/DosovitskiyB0WZ21,
  author       = {Alexey Dosovitskiy and
                  Lucas Beyer and
                  Alexander Kolesnikov and
                  Dirk Weissenborn and
                  Xiaohua Zhai and
                  Thomas Unterthiner and
                  Mostafa Dehghani and
                  Matthias Minderer and
                  Georg Heigold and
                  Sylvain Gelly and
                  Jakob Uszkoreit and
                  Neil Houlsby},
  title        = {An Image is Worth 16x16 Words: Transformers for Image Recognition
                  at Scale},
  booktitle    = {{ICLR}},
  publisher    = {OpenReview.net},
  year         = {2021}
}

@inproceedings{meng2022adavit,
  title={Adavit: Adaptive vision transformers for efficient image recognition},
  author={Meng, Lingchen and Li, Hengduo and Chen, Bor-Chun and Lan, Shiyi and Wu, Zuxuan and Jiang, Yu-Gang and Lim, Ser-Nam},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={12309--12318},
  year={2022}
}
@inproceedings{yin2022vit,
  title={A-vit: Adaptive tokens for efficient vision transformer},
  author={Yin, Hongxu and Vahdat, Arash and Alvarez, Jose M and Mallya, Arun and Kautz, Jan and Molchanov, Pavlo},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10809--10818},
  year={2022}
}


@article{wolf2019huggingface,
  title={Huggingface's transformers: State-of-the-art natural language processing},
  author={Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, R{\'e}mi and Funtowicz, Morgan and others},
  journal={arXiv preprint arXiv:1910.03771},
  year={2019}
}
@article{ainslie2023gqa,
  title={Gqa: Training generalized multi-query transformer models from multi-head checkpoints},
  author={Ainslie, Joshua and Lee-Thorp, James and de Jong, Michiel and Zemlyanskiy, Yury and Lebr{\'o}n, Federico and Sanghai, Sumit},
  journal={arXiv preprint arXiv:2305.13245},
  year={2023}
}

@article{shazeer2019fast,
  title={Fast transformer decoding: One write-head is all you need},
  author={Shazeer, Noam},
  journal={arXiv preprint arXiv:1911.02150},
  year={2019}
}

@online{Llama3,
  author = {Meta},
  title = {Introducing Meta Llama 3: The most capable openly available LLM to date},
  year = 2024,
  url = {https://ai.meta.com/blog/meta-llama-3/},
  urldate = {2024-04-25}
}

@article{reid2024gemini,
  title={Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
  author={Reid, Machel and Savinov, Nikolay and Teplyashin, Denis and Lepikhin, Dmitry and Lillicrap, Timothy and Alayrac, Jean-baptiste and Soricut, Radu and Lazaridou, Angeliki and Firat, Orhan and Schrittwieser, Julian and others},
  journal={arXiv preprint arXiv:2403.05530},
  year={2024}
}


@article{Hunter1986TheEW,
  title={The exponentially weighted moving average},
  author={J. Stuart Hunter},
  journal={Journal of Quality Technology},
  year={1986},
  volume={18},
  pages={203-210},
  url={https://api.semanticscholar.org/CorpusID:124743553}
}

@article{velivckovic2017graph,
  title={Graph attention networks},
  author={Veli{\v{c}}kovi{\'c}, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Lio, Pietro and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1710.10903},
  year={2017}
}

@inproceedings{DBLP:conf/nips/BusbridgeRALDCW23,
  author       = {Dan Busbridge and
                  Jason Ramapuram and
                  Pierre Ablin and
                  Tatiana Likhomanenko and
                  Eeshan Gunesh Dhekane and
                  Xavier Suau Cuadros and
                  Russell Webb},
  title        = {How to Scale Your {EMA}},
  booktitle    = {NeurIPS},
  year         = {2023}
}

@article{DBLP:journals/corr/abs-2202-07800,
  author       = {Youwei Liang and
                  Chongjian Ge and
                  Zhan Tong and
                  Yibing Song and
                  Jue Wang and
                  Pengtao Xie},
  title        = {Not All Patches are What You Need: Expediting Vision Transformers
                  via Token Reorganizations},
  journal      = {CoRR},
  volume       = {abs/2202.07800},
  year         = {2022}
}

@article{chiang2023vicuna,
  title={Vicuna: An open-source chatbot impressing gpt-4 with 90\%* chatgpt quality, March 2023},
  author={Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E and others},
  journal={URL https://lmsys. org/blog/2023-03-30-vicuna},
  volume={3},
  number={5},
  year={2023}
}

@article{xiong2022self,
  title={Self-consistent reasoning for solving math word problems},
  author={Xiong, Jing and Wan, Zhongwei and Hu, Xiping and Yang, Min and Li, Chengming},
  journal={arXiv preprint arXiv:2210.15373},
  year={2022}
}
@article{zhang2024llama,
  title={Llama-berry: Pairwise optimization for o1-like olympiad-level mathematical reasoning},
  author={Zhang, Di and Wu, Jianbo and Lei, Jingdi and Che, Tong and Li, Jiatong and Xie, Tong and Huang, Xiaoshui and Zhang, Shufei and Pavone, Marco and Li, Yuqiang and others},
  journal={arXiv preprint arXiv:2410.02884},
  year={2024}
}
@article{tao2024scaling,
  title={Scaling laws with vocabulary: Larger models deserve larger vocabularies},
  author={Tao, Chaofan and Liu, Qian and Dou, Longxu and Muennighoff, Niklas and Wan, Zhongwei and Luo, Ping and Lin, Min and Wong, Ngai},
  journal={arXiv preprint arXiv:2407.13623},
  year={2024}
}

@article{xiong2024autoregressive,
  title={Autoregressive Models in Vision: A Survey},
  author={Xiong, Jing and Liu, Gongye and Huang, Lun and Wu, Chengyue and Wu, Taiqiang and Mu, Yao and Yao, Yuan and Shen, Hui and Wan, Zhongwei and Huang, Jinfa and others},
  journal={arXiv preprint arXiv:2411.05902},
  year={2024}
}


@article{xiong2025parallelcomp,
  title={ParallelComp: Parallel Long-Context Compressor for Length Extrapolation},
  author={Xiong, Jing and Shen, Jianghan and Zheng, Chuanyang and Wan, Zhongwei and Zhao, Chenyang and Yang, Chiwun and Ye, Fanghua and Yang, Hongxia and Kong, Lingpeng and Wong, Ngai},
  journal={arXiv preprint arXiv:2502.14317},
  year={2025}
}
@article{shen2025efficient,
  title={Efficient Diffusion Models: A Survey},
  author={Shen, Hui and Zhang, Jingxuan and Xiong, Boning and Hu, Rui and Chen, Shoufa and Wan, Zhongwei and Wang, Xin and Zhang, Yu and Gong, Zixuan and Bao, Guangyin and others},
  journal={arXiv preprint arXiv:2502.06805},
  year={2025}
}

@article{xin2025v,
  title={V-PETL Bench: A Unified Visual Parameter-Efficient Transfer Learning Benchmark},
  author={Xin, Yi and Luo, Siqi and Liu, Xuyang and Zhou, Haodi and Cheng, Xinyu and Lee, Christina E and Du, Junlong and Wang, Haozhe and Chen, MingCai and Liu, Ting and others},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={80522--80535},
  year={2025}
}

@article{DBLP:journals/corr/abs-2301-13848,
  author       = {Tianyi Zhang and
                  Faisal Ladhak and
                  Esin Durmus and
                  Percy Liang and
                  Kathleen R. McKeown and
                  Tatsunori B. Hashimoto},
  title        = {Benchmarking Large Language Models for News Summarization},
  journal      = {CoRR},
  volume       = {abs/2301.13848},
  year         = {2023}
}

@article{roziere2023code,
  title={Code llama: Open foundation models for code},
  author={Roziere, Baptiste and Gehring, Jonas and Gloeckle, Fabian and Sootla, Sten and Gat, Itai and Tan, Xiaoqing Ellen and Adi, Yossi and Liu, Jingyu and Remez, Tal and Rapin, J{\'e}r{\'e}my and others},
  journal={arXiv preprint arXiv:2308.12950},
  year={2023}
}

@article{Yang2024NoTL,
  title={No Token Left Behind: Reliable KV Cache Compression via Importance-Aware Mixed Precision Quantization},
  author={June Yong Yang and Byeongwook Kim and Jeongin Bae and Beomseok Kwon and Gunho Park and Eunho Yang and Se Jung Kwon and Dongsoo Lee},
  journal={ArXiv},
  year={2024},
  volume={abs/2402.18096},
  url={https://api.semanticscholar.org/CorpusID:268041747}
}

@article{kamalloo2023evaluating,
  title={Evaluating open-domain question answering in the era of large language models},
  author={Kamalloo, Ehsan and Dziri, Nouha and Clarke, Charles LA and Rafiei, Davood},
  journal={arXiv preprint arXiv:2305.06984},
  year={2023}
}

@article{zhang2024h2o,
  title={H2o: Heavy-hitter oracle for efficient generative inference of large language models},
  author={Zhang, Zhenyu and Sheng, Ying and Zhou, Tianyi and Chen, Tianlong and Zheng, Lianmin and Cai, Ruisi and Song, Zhao and Tian, Yuandong and R{\'e}, Christopher and Barrett, Clark and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{aminabadi2022deepspeed,
  title={Deepspeed-inference: enabling efficient inference of transformer models at unprecedented scale},
  author={Aminabadi, Reza Yazdani and Rajbhandari, Samyam and Awan, Ammar Ahmad and Li, Cheng and Li, Du and Zheng, Elton and Ruwase, Olatunji and Smith, Shaden and Zhang, Minjia and Rasley, Jeff and others},
  booktitle={SC22: International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--15},
  year={2022},
  organization={IEEE}
}

@inproceedings{sheng2023flexgen,
  title={Flexgen: High-throughput generative inference of large language models with a single gpu},
  author={Sheng, Ying and Zheng, Lianmin and Yuan, Binhang and Li, Zhuohan and Ryabinin, Max and Chen, Beidi and Liang, Percy and R{\'e}, Christopher and Stoica, Ion and Zhang, Ce},
  booktitle={International Conference on Machine Learning},
  pages={31094--31116},
  year={2023},
  organization={PMLR}
}

@article{DBLP:journals/corr/abs-1911-02150,
  author       = {Noam Shazeer},
  title        = {Fast Transformer Decoding: One Write-Head is All You Need},
  journal      = {CoRR},
  volume       = {abs/1911.02150},
  year         = {2019}
}

@inproceedings{DBLP:conf/emnlp/AinslieLJZLS23,
  author       = {Joshua Ainslie and
                  James Lee{-}Thorp and
                  Michiel de Jong and
                  Yury Zemlyanskiy and
                  Federico Lebr{\'{o}}n and
                  Sumit Sanghai},
  title        = {{GQA:} Training Generalized Multi-Query Transformer Models from Multi-Head
                  Checkpoints},
  booktitle    = {{EMNLP}},
  pages        = {4895--4901},
  publisher    = {Association for Computational Linguistics},
  year         = {2023}
}


@article{ge2023model,
  title={Model tells you what to discard: Adaptive kv cache compression for llms},
  author={Ge, Suyu and Zhang, Yunan and Liu, Liyuan and Zhang, Minjia and Han, Jiawei and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2310.01801},
  year={2023}
}

@inproceedings{DBLP:conf/nips/Zhang00CZC0TRBW23,
  author       = {Zhenyu Zhang and
                  Ying Sheng and
                  Tianyi Zhou and
                  Tianlong Chen and
                  Lianmin Zheng and
                  Ruisi Cai and
                  Zhao Song and
                  Yuandong Tian and
                  Christopher R{\'{e}} and
                  Clark W. Barrett and
                  Zhangyang Wang and
                  Beidi Chen},
  title        = {{H2O:} Heavy-Hitter Oracle for Efficient Generative Inference of Large
                  Language Models},
  booktitle    = {NeurIPS},
  year         = {2023}
}

@inproceedings{DBLP:conf/nips/LiuDLWXXKS23,
  author       = {Zichang Liu and
                  Aditya Desai and
                  Fangshuo Liao and
                  Weitao Wang and
                  Victor Xie and
                  Zhaozhuo Xu and
                  Anastasios Kyrillidis and
                  Anshumali Shrivastava},
  title        = {Scissorhands: Exploiting the Persistence of Importance Hypothesis
                  for {LLM} {KV} Cache Compression at Test Time},
  booktitle    = {NeurIPS},
  year         = {2023}
}

@article{DBLP:journals/corr/abs-2402-06262,
  author       = {Siyu Ren and
                  Kenny Q. Zhu},
  title        = {On the Efficacy of Eviction Policy for Key-Value Constrained Generative
                  Language Model Inference},
  journal      = {CoRR},
  volume       = {abs/2402.06262},
  year         = {2024}
}

@article{DBLP:journals/corr/abs-2309-17453,
  author       = {Guangxuan Xiao and
                  Yuandong Tian and
                  Beidi Chen and
                  Song Han and
                  Mike Lewis},
  title        = {Efficient Streaming Language Models with Attention Sinks},
  journal      = {CoRR},
  volume       = {abs/2309.17453},
  year         = {2023}
}

@article{DBLP:journals/corr/abs-2402-18096,
  author       = {June Yong Yang and
                  Byeongwook Kim and
                  Jeongin Bae and
                  Beomseok Kwon and
                  Gunho Park and
                  Eunho Yang and
                  Se Jung Kwon and
                  Dongsoo Lee},
  title        = {No Token Left Behind: Reliable {KV} Cache Compression via Importance-Aware
                  Mixed Precision Quantization},
  journal      = {CoRR},
  volume       = {abs/2402.18096},
  year         = {2024}
}

@article{DBLP:journals/corr/abs-2110-14168,
  author       = {Karl Cobbe and
                  Vineet Kosaraju and
                  Mohammad Bavarian and
                  Mark Chen and
                  Heewoo Jun and
                  Lukasz Kaiser and
                  Matthias Plappert and
                  Jerry Tworek and
                  Jacob Hilton and
                  Reiichiro Nakano and
                  Christopher Hesse and
                  John Schulman},
  title        = {Training Verifiers to Solve Math Word Problems},
  journal      = {CoRR},
  volume       = {abs/2110.14168},
  year         = {2021}
}

@article{DBLP:journals/corr/abs-2402-14762,
  author       = {Ge Bai and
                  Jie Liu and
                  Xingyuan Bu and
                  Yancheng He and
                  Jiaheng Liu and
                  Zhanhui Zhou and
                  Zhuoran Lin and
                  Wenbo Su and
                  Tiezheng Ge and
                  Bo Zheng and
                  Wanli Ouyang},
  title        = {MT-Bench-101: {A} Fine-Grained Benchmark for Evaluating Large Language
                  Models in Multi-Turn Dialogues},
  journal      = {CoRR},
  volume       = {abs/2402.14762},
  year         = {2024}
}
@article{xiao2023efficient,
  title={Efficient streaming language models with attention sinks},
  author={Xiao, Guangxuan and Tian, Yuandong and Chen, Beidi and Han, Song and Lewis, Mike},
  journal={arXiv preprint arXiv:2309.17453},
  year={2023}
}

@article{almazrouei2023falcon,
  title={The falcon series of open language models},
  author={Almazrouei, Ebtesam and Alobeidli, Hamza and Alshamsi, Abdulaziz and Cappelli, Alessandro and Cojocaru, Ruxandra and Debbah, M{\'e}rouane and Goffinet, {\'E}tienne and Hesslow, Daniel and Launay, Julien and Malartic, Quentin and others},
  journal={arXiv preprint arXiv:2311.16867},
  year={2023}
}

@article{wu2024retrieval,
  title={Retrieval Head Mechanistically Explains Long-Context Factuality},
  author={Wu, Wenhao and Wang, Yizhong and Xiao, Guangxuan and Peng, Hao and Fu, Yao},
  journal={arXiv preprint arXiv:2404.15574},
  year={2024}
}

@misc{kamradt2023needle,
  title={Needle in a Haystack--pressure testing LLMs},
  author={Kamradt, G},
  year={2023}
}


###################  kv quant
@article{hooper2024kvquant,
  title={KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization},
  author={Hooper, Coleman and Kim, Sehoon and Mohammadzadeh, Hiva and Mahoney, Michael W and Shao, Yakun Sophia and Keutzer, Kurt and Gholami, Amir},
  journal={arXiv preprint arXiv:2401.18079},
  year={2024}
}





@article{yue2024wkvquant,
  title={Wkvquant: Quantizing weight and key/value cache for large language models gains more},
  author={Yue, Yuxuan and Yuan, Zhihang and Duanmu, Haojie and Zhou, Sifan and Wu, Jianlong and Nie, Liqiang},
  journal={arXiv preprint arXiv:2402.12065},
  year={2024}
}

@article{liu2024kivi,
  title={KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache},
  author={Liu, Zirui and Yuan, Jiayi and Jin, Hongye and Zhong, Shaochen and Xu, Zhaozhuo and Braverman, Vladimir and Chen, Beidi and Hu, Xia},
  journal={arXiv preprint arXiv:2402.02750},
  year={2024}
}

@article{kang2024gear,
  title={GEAR: An Efficient KV Cache Compression Recipefor Near-Lossless Generative Inference of LLM},
  author={Kang, Hao and Zhang, Qingru and Kundu, Souvik and Jeong, Geonhwa and Liu, Zaoxing and Krishna, Tushar and Zhao, Tuo},
  journal={arXiv preprint arXiv:2403.05527},
  year={2024}
}

@article{cobbe2021training,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@article{DBLP:journals/corr/abs-2102-04664,
  author    = {Shuai Lu and
               Daya Guo and
               Shuo Ren and
               Junjie Huang and
               Alexey Svyatkovskiy and
               Ambrosio Blanco and
               Colin B. Clement and
               Dawn Drain and
               Daxin Jiang and
               Duyu Tang and
               Ge Li and
               Lidong Zhou and
               Linjun Shou and
               Long Zhou and
               Michele Tufano and
               Ming Gong and
               Ming Zhou and
               Nan Duan and
               Neel Sundaresan and
               Shao Kun Deng and
               Shengyu Fu and
               Shujie Liu},
  title     = {CodeXGLUE: {A} Machine Learning Benchmark Dataset for Code Understanding
               and Generation},
  journal   = {CoRR},
  volume    = {abs/2102.04664},
  year      = {2021}
}

@inproceedings{DBLP:conf/acl/LinHE22,
  author       = {Stephanie Lin and
                  Jacob Hilton and
                  Owain Evans},
  title        = {TruthfulQA: Measuring How Models Mimic Human Falsehoods},
  booktitle    = {{ACL} {(1)}},
  pages        = {3214--3252},
  publisher    = {Association for Computational Linguistics},
  year         = {2022}
}
@article{DBLP:journals/tacl/ReddyCM19,
  author       = {Siva Reddy and
                  Danqi Chen and
                  Christopher D. Manning},
  title        = {CoQA: {A} Conversational Question Answering Challenge},
  journal      = {Trans. Assoc. Comput. Linguistics},
  volume       = {7},
  pages        = {249--266},
  year         = {2019}
}

@article{chen2021evaluating,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}

@article{dong2024get,
  title={Get More with LESS: Synthesizing Recurrence with KV Cache Compression for Efficient LLM Inference},
  author={Dong, Harry and Yang, Xinyu and Zhang, Zhenyu and Wang, Zhangyang and Chi, Yuejie and Chen, Beidi},
  journal={arXiv preprint arXiv:2402.09398},
  year={2024}
}

@article{nawrot2024dynamic,
  title={Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference},
  author={Nawrot, Piotr and {\L}a{\'n}cucki, Adrian and Chochowski, Marcin and Tarjan, David and Ponti, Edoardo M},
  journal={arXiv preprint arXiv:2403.09636},
  year={2024}
}

@inproceedings{chen2023diffrate,
  title={Diffrate: Differentiable compression rate for efficient vision transformers},
  author={Chen, Mengzhao and Shao, Wenqi and Xu, Peng and Lin, Mingbao and Zhang, Kaipeng and Chao, Fei and Ji, Rongrong and Qiao, Yu and Luo, Ping},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={17164--17174},
  year={2023}
}

@article{zhang2024mg,
  title={MG-ViT: A Multi-Granularity Method for Compact and Efficient Vision Transformers},
  author={Zhang, Yu and Liu, Yepeng and Miao, Duoqian and Zhang, Qi and Shi, Yiwei and Hu, Liang},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}


@article{ott2019fairseq,
  title={fairseq: A fast, extensible toolkit for sequence modeling},
  author={Ott, Myle and Edunov, Sergey and Baevski, Alexei and Fan, Angela and Gross, Sam and Ng, Nathan and Grangier, David and Auli, Michael},
  journal={arXiv preprint arXiv:1904.01038},
  year={2019}
}

@inproceedings{wolf2020transformers,
  title={Transformers: State-of-the-art natural language processing},
  author={Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, R{\'e}mi and Funtowicz, Morgan and others},
  booktitle={Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations},
  pages={38--45},
  year={2020}
}

@article{zhao2024explainability,
  title={Explainability for large language models: A survey},
  author={Zhao, Haiyan and Chen, Hanjie and Yang, Fan and Liu, Ninghao and Deng, Huiqi and Cai, Hengyi and Wang, Shuaiqiang and Yin, Dawei and Du, Mengnan},
  journal={ACM Transactions on Intelligent Systems and Technology},
  volume={15},
  number={2},
  pages={1--38},
  year={2024},
  publisher={ACM New York, NY}
}

@inproceedings{Zhao2024TowardsUH,
  title={Towards Uncovering How Large Language Model Works: An Explainability Perspective},
  author={Haiyan Zhao and Fan Yang and Bo Shen and Himabindu Lakkaraju and Mengnan Du},
  year={2024},
  url={https://api.semanticscholar.org/CorpusID:267740636}
}

@inproceedings{dang2021nearest,
  title={Nearest neighbor matching for deep clustering},
  author={Dang, Zhiyuan and Deng, Cheng and Yang, Xu and Wei, Kun and Huang, Heng},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={13693--13702},
  year={2021}
}

@article{briakou2023searching,
  title={Searching for Needles in a Haystack: On the Role of Incidental Bilingualism in PaLM's Translation Capability},
  author={Briakou, Eleftheria and Cherry, Colin and Foster, George},
  journal={arXiv preprint arXiv:2305.10266},
  year={2023}
}

@article{wu2023ppt,
  title={PPT: Token Pruning and Pooling for Efficient Vision Transformers},
  author={Wu, Xinjian and Zeng, Fanhu and Wang, Xiudong and Wang, Yunhe and Chen, Xinghao},
  journal={arXiv preprint arXiv:2310.01812},
  year={2023}
}

@article{wang2024multimodal,
  title={Multimodal Needle in a Haystack: Benchmarking Long-Context Capability of Multimodal Large Language Models},
  author={Wang, Hengyi and Shi, Haizhou and Tan, Shiwei and Qin, Weiyi and Wang, Wenyuan and Zhang, Tunyu and Nambi, Akshay and Ganu, Tanuja and Wang, Hao},
  journal={arXiv preprint arXiv:2406.11230},
  year={2024}
}

@article{yang2024seed,
  title={SEED-Story: Multimodal Long Story Generation with Large Language Model},
  author={Yang, Shuai and Ge, Yuying and Li, Yang and Chen, Yukang and Ge, Yixiao and Shan, Ying and Chen, Yingcong},
  journal={arXiv preprint arXiv:2407.08683},
  year={2024}
}

########################### new add #########################################

@inproceedings{Zhang2024CaMCM,
  title={CaM: Cache Merging for Memory-efficient LLMs Inference},
  author={Yuxin Zhang and Yuxuan Du and Gen Luo and Yunshan Zhong and Zhenyu Zhang and Shiwei Liu and Rongrong Ji},
  booktitle={International Conference on Machine Learning},
  year={2024},
  url={https://api.semanticscholar.org/CorpusID:272330701}
}
@article{wan2024d2o,
  title={D2O: Dynamic Discriminative Operations for Efficient Generative Inference of Large Language Models},
  author={Wan, Zhongwei and Wu, Xinjian and Zhang, Yu and Xin, Yi and Tao, Chaofan and Zhu, Zhihong and Wang, Xin and Luo, Siqi and Xiong, Jing and Zhang, Mi},
  journal={arXiv preprint arXiv:2406.13035},
  year={2024}
}

@article{wan2024look,
  title={Look-m: Look-once optimization in kv cache for efficient multimodal long-context inference},
  author={Wan, Zhongwei and Wu, Ziang and Liu, Che and Huang, Jinfa and Zhu, Zhihong and Jin, Peng and Wang, Longyue and Yuan, Li},
  journal={arXiv preprint arXiv:2406.18139},
  year={2024}
}

@article{zhang2024pyramidkv,
  title={PyramidKV: Dynamic KV Cache Compression based on Pyramidal Information Funneling},
  author={Zhang, Yichi and Gao, Bofei and Liu, Tianyu and Lu, Keming and Xiong, Wayne and Dong, Yue and Chang, Baobao and Hu, Junjie and Xiao, Wen and others},
  journal={arXiv preprint arXiv:2406.02069},
  year={2024}
}

@article{yang2024pyramidinfer,
  title={PyramidInfer: Pyramid KV Cache Compression for High-throughput LLM Inference},
  author={Yang, Dongjie and Han, XiaoDong and Gao, Yan and Hu, Yao and Zhang, Shilin and Zhao, Hai},
  journal={arXiv preprint arXiv:2405.12532},
  year={2024}
}

@article{shu2024video,
  title={Video-XL: Extra-Long Vision Language Model for Hour-Scale Video Understanding},
  author={Shu, Yan and Zhang, Peitian and Liu, Zheng and Qin, Minghao and Zhou, Junjie and Huang, Tiejun and Zhao, Bo},
  journal={arXiv preprint arXiv:2409.14485},
  year={2024}
}

@article{zhang2024long,
  title={Long context transfer from language to vision},
  author={Zhang, Peiyuan and Zhang, Kaichen and Li, Bo and Zeng, Guangtao and Yang, Jingkang and Zhang, Yuanhan and Wang, Ziyue and Tan, Haoran and Li, Chunyuan and Liu, Ziwei},
  journal={arXiv preprint arXiv:2406.16852},
  year={2024}
}


@article{wang2024longllava,
  title={LongLLaVA: Scaling Multi-modal LLMs to 1000 Images Efficiently via Hybrid Architecture},
  author={Wang, Xidong and Song, Dingjie and Chen, Shunian and Zhang, Chen and Wang, Benyou},
  journal={arXiv preprint arXiv:2409.02889},
  year={2024}
}

@article{liu2024world,
  title={World model on million-length video and language with blockwise ringattention},
  author={Liu, Hao and Yan, Wilson and Zaharia, Matei and Abbeel, Pieter},
  journal={arXiv preprint arXiv:2402.08268},
  year={2024}
}

@misc{liu2024llava,
  title={Llava-next: Improved reasoning, ocr, and world knowledge},
  author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Li, Bo and Zhang, Yuanhan and Shen, Sheng and Lee, Yong Jae},
  year={2024}
}

@misc{zhang2024llavanextvideo,
  title={LLaVA-NeXT: A Strong Zero-shot Video Understanding Model},
  url={https://llava-vl.github.io/blog/2024-04-30-llava-next-video/},
  author={Zhang, Yuanhan and Li, Bo and Liu, haotian and Lee, Yong jae and Gui, Liangke and Fu, Di and Feng, Jiashi and Liu, Ziwei and Li, Chunyuan},
  month={April},
  year={2024}
}


@article{Heilbron2015ActivityNetAL,
  title={ActivityNet: A large-scale video benchmark for human activity understanding},
  author={Fabian Caba Heilbron and Victor Escorcia and Bernard Ghanem and Juan Carlos Niebles},
  journal={2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2015},
  pages={961-970},
  url={https://api.semanticscholar.org/CorpusID:1710722}
}
@article{Zhang2021AlignmentAB,
  title={Alignment Attention by Matching Key and Query Distributions},
  author={Shujian Zhang and Xinjie Fan and Huangjie Zheng and Korawat Tanwisuth and Mingyuan Zhou},
  journal={ArXiv},
  year={2021},
  volume={abs/2110.12567},
  url={https://api.semanticscholar.org/CorpusID:239768788}
}
@article{maaz2023video,
  title={Video-chatgpt: Towards detailed video understanding via large vision and language models},
  author={Maaz, Muhammad and Rasheed, Hanoona and Khan, Salman and Khan, Fahad Shahbaz},
  journal={arXiv preprint arXiv:2306.05424},
  year={2023}
}

@article{xue2024longvila,
  title={LongVILA: Scaling Long-Context Visual Language Models for Long Videos},
  author={Xue, Fuzhao and Chen, Yukang and Li, Dacheng and Hu, Qinghao and Zhu, Ligeng and Li, Xiuyu and Fang, Yunhao and Tang, Haotian and Yang, Shang and Liu, Zhijian and others},
  journal={arXiv preprint arXiv:2408.10188},
  year={2024}
}
@article{zhang2024worldqa,
  title={WorldQA: Multimodal World Knowledge in Videos through Long-Chain Reasoning},
  author={Zhang, Yuanhan and Zhang, Kaichen and Li, Bo and Pu, Fanyi and Setiadharma, Christopher Arif and Yang, Jingkang and Liu, Ziwei},
  journal={arXiv preprint arXiv:2405.03272},
  year={2024}
}

@article{wang2024tarsier,
  title={Tarsier: Recipes for Training and Evaluating Large Video Description Models},
  author={Wang, Jiawei and Yuan, Liping and Zhang, Yuchen},
  journal={arXiv preprint arXiv:2407.00634},
  year={2024}
}

@article{de2005tutorial,
  title={A tutorial on the cross-entropy method},
  author={De Boer, Pieter-Tjerk and Kroese, Dirk P and Mannor, Shie and Rubinstein, Reuven Y},
  journal={Annals of operations research},
  volume={134},
  pages={19--67},
  year={2005},
  publisher={Springer}
}
@article{Dao2022FlashAttentionFA,
  title={FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness},
  author={Tri Dao and Daniel Y. Fu and Stefano Ermon and Atri Rudra and Christopher R'e},
  journal={ArXiv},
  year={2022},
  volume={abs/2205.14135},
  url={https://api.semanticscholar.org/CorpusID:249151871}
}
@inproceedings{gu2024minillm,
  title={MiniLLM: Knowledge distillation of large language models},
  author={Gu, Yuxian and Dong, Li and Wei, Furu and Huang, Minlie},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}

@article{zhang2024video,
  title={Video Instruction Tuning With Synthetic Data},
  author={Zhang, Yuanhan and Wu, Jinming and Li, Wei and Li, Bo and Ma, Zejun and Liu, Ziwei and Li, Chunyuan},
  journal={arXiv preprint arXiv:2410.02713},
  year={2024}
}

@inproceedings{zhai2023stabilizing,
  title={Stabilizing transformer training by preventing attention entropy collapse},
  author={Zhai, Shuangfei and Likhomanenko, Tatiana and Littwin, Etai and Busbridge, Dan and Ramapuram, Jason and Zhang, Yizhe and Gu, Jiatao and Susskind, Joshua M},
  booktitle={International Conference on Machine Learning},
  pages={40770--40803},
  year={2023},
  organization={PMLR}
}