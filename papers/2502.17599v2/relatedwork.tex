\section{Related Work}
\vspace{-2mm}

\noindent \textbf{Post-training KV Cache Compression.} 
Post-training KV cache compression methods~\cite{wan2023efficient, liu2024contemporary} fall into four categories: token-wise eviction, token-wise merging, static layer-wise reduction, and quantization. Token-wise eviction (e.g., StreamingLLM~\cite{Xiao2023EfficientSL}) retains key tokens for sequence generation, while $\text{H}{2}\text{O}$~\cite{zhang2024h2o}, SnapKV \cite{Li2024SnapKVLK},ParallelComp~\cite{xiong2025parallelcomp}, and UNComp~\cite{xiong2024uncomp} focus on compact subsets, potentially sacrificing context. Token-wise merging (e.g., CaM~\cite{Zhang2024CaMCM}, $\text{D}{2} \text{O}$~\cite{wan2024d2o}) re-integrates tokens to maintain context. Static layer-wise reduction (e.g., PyramidKV~\cite{zhang2024pyramidkv}) linearly reduces cache across layers but ignores inter-layer attention variations. Quantization (e.g., KIVI~\cite{liu2024kivi}, Gear~\cite{kang2024gear}) balances memory and precision.
%
Most methods focus on text-based KV compression, overlooking multimodal contexts. LOOK-M~\cite{wan2024look} addresses multimodal compression but uses fixed allocation, neglecting inter-layer attention differences. MEDA introduces a multimodal attention entropy-guided dynamic allocation to address this.

% \text{ }

\noindent \textbf{Vision Token Compression for MLLMs.} 
Classical approaches such as MobileVLM~\cite{chu2024mobilevlm}, LLaVA-Prumerge~\cite{Shang2024LLaVAPruMergeAT}, MADTP~\cite{Cao2024MADTPMA}, and FastV~\cite{Chen2024AnII} focus on reducing image tokens, which dominate the total token count, to accelerate inference by removing redundancies. MobileVLM~\cite{chu2024mobilevlm} uses a lightweight projector with average pooling to compress visual tokens, while LLaVA-Prumerge~\cite{Shang2024LLaVAPruMergeAT} and MADTP~\cite{Cao2024MADTPMA} adopt adaptive strategies to reduce tokens while preserving performance. FastV~\cite{Chen2024AnII} offers a plug-and-play solution that optimizes early layer computations and prunes visual tokens in later layers.
%
In contrast, MEDA focuses on multimodal KV cache compression through a dynamic layer-wise allocation strategy, eliminating the need for additional fine-tuning and enhance the efficiency of multimodal long-context generative inference.

% \text{ }

\noindent \textbf{Long-context MLLMs.}  
Recent works have expanded MLLMs' multimodal long-context capabilities through additional training. \citet{liu2024world} leverage Blockwise RingAttention for scalable long-sequence training.  LongVA~\cite{zhang2024long} first pre-trains LLMs on long-text sequences and then aligns Long LLMs using short vision data to generalize to multimodal long-text contexts. LongLLaVA~\cite{wang2024longllava} modifies the model architecture by integrating Mamba and Transformer blocks and employs a progressive training strategy using multiple images. Video-XL~\cite{shu2024video} introduces visual context latent summarization to train models for handling even longer multimodal token sequences.
%
In contrast, MEDA introduces a dynamic KV cache optimization algorithm, enhancing long-context multimodal inference without additional training and is compatible with these methods.


\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.9\textwidth]{latex/figs/final_meda_crop.pdf}
    % \vspace{-0.15in}
    \caption{\small{ 
Illustration of MEDA's multimodal attention entropy-guided dynamic KV cache allocation and merging strategy.}}
    % \vspace{-0.15in}
    \label{fig: samplen}
\end{figure*}

%Methodology of MEDA