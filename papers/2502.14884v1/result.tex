\section{Experiments}
\label{sec:experiments}

%（8.11）In this section, we initially conduct comparative experiments on defect classification and segmentation of SEM images across various few-shot regimes to showcase the efficacy and potential applications of our method. Following this, we proceed with comprehensive ablation experiments to validate the effectiveness of each component mentioned.

\subsection{Experiments Settings}
\label{sec:experiments-settings}

%\noindent
%\textbf{Evaluation metrics.}
%For segmentation tasks, we utilize \textit{image-level Area Under the Receiver Operating Characteristic-curve} (iAUROC) and \textit{pixel-level} AUROC (pAUROC), as well as \textit{pixel-level $F_{1}$-score at optimal threshold} ($F_{1}$-$max$). For classification, we apply widely used evaluation metrics: Accuracy, Precision, Recall, and the $F_{1}$ score, to provide a clearer assessment in light of potential data imbalances.
%to balance precision and recall, optimizing against data imbalance.

%\noindent
%\textbf{Implementation details.}
%In our experiments, we follow WinCLIP \cite{Jeong_2023_CVPR} in adopting the OpenCLIP3 CLIP implementation and its publicly accessible pre-trained models. Specifically, we utilize the LAION-400M-based CLIP model equipped with ViT-B/$16+$. The input image, with a $240 \times 240$ resolution, is split into $15 \times 15$ patches, each measuring $16 \times 16$. A learnable classification $CLS$ token is added at the beginning of the $225$ embeddings. The image encoder backbone consists of $12$ layers; we divide them into $4$ encoding blocks, \textit{i.e.}, $m = 4$. Thus, each encoding block contains $3$ layers, corresponding to $3$ dual path blocks, namely, $n = 3$. 

Evaluation metrics include iAUROC, pAUROC, and pixel-level $F_{1}$-$max$ for segmentation, and Accuracy, Precision, Recall, and $F_{1}$ score for classification.
We utilize the LAION-400M-based CLIP model equipped with ViT-B/16+ for our experiments.  The image encoder backbone consists of 12 layers, we divide them into 4 encoding blocks, i.e., m = 4. Thus, each encoding block contains 3 layers, corresponding to 3 dual path blocks, namely, n = 3.
All experiments are conducted on NVIDIA RTX $4090$. 
% Our model occupies $4.02$ GB of GPU memory during training and inference. 
%（8.11）For fine-tuning strategies, we employ the Adam optimizer for parameter updates, the learning rate is set to $0.0001$, and the fine-tuning lasts $50$ epochs with a batch size of $1$. The hyperparameter $\alpha$ in \Cref{eq:classification} is set to $0.8$.
For fine-tuning strategies, we employ the Adam optimizer for parameter updates. The hyperparameter $\alpha$ in \Cref{eq:classification} is set to $0.8$.

\subsection{Benchmarks and Baselines}
For defect segmentation performance, we primarily compare our method with WinCLIP$+$ \cite{Jeong_2023_CVPR}, PromptAD \cite{li2024promptad}, DRA \cite{ding2022catching}, and AnomalyGPT \cite{gu2024anomalygpt} under a series of few-shot settings. These methods represent popular anomaly detection (AD) approaches and recent \textit{state-of-the-art} (SOTA) AD models. Both WinCLIP and PromptAD are based on CLIP for anomaly detection. Thus, we configure them with ViT-B/16+, pre-trained on LAION-400M. These baselines are introduced in detail in \Cref{sec:preliminaries}. 

\begin{figure*}[tb!]
  \centering
  \includegraphics[width=0.98\textwidth, trim={0cm 5cm 0cm 3cm}, clip]{figs/mask.pdf}
  \caption{Visualization of 10-shot segmentation.}
  \label{Visualization of 10-shot segmentation}
\end{figure*}

%（8.11）Due to the lack of multi-category classification capabilities in the aforementioned methods, we opt for models pre-trained on ImageNet-21K \cite{Dataset-2021NeurIPS-ImageNet-21K} to compare classification performance. These include ViT \cite{dosovitskiy2020image}, ResNet101 \cite{he2016deep}, ResNet50+ViT \cite{dosovitskiy2020image}, and EfficientNet \cite{tan2019efficientnet}.Each model is then fine-tuned on our SEM dataset with $10$-shot samples. Finally, we compare their performance with our SEM-CLIP model on the same test set. 
Given the lack of multi-category classification in previous methods, we compare classification performance using models pre-trained on ImageNet-21K \cite{Dataset-2021NeurIPS-ImageNet-21K}, including ViT \cite{dosovitskiy2020image}, ResNet50+ViT \cite{dosovitskiy2020image}, ResNet101 \cite{he2016deep}, and EfficientNet \cite{tan2019efficientnet}. Each model is fine-tuned on our SEM dataset with $10$-shot samples and compared to our SEM-CLIP model on the same test set.

\subsection{Results Analysis}
\noindent
\textbf{Segmentation performance comparisons.}
%We tested the performance metrics iAUROC, pAUROC, and $F1$-$max$ score under 1-shot, 2-shot, 5-shot, and 10-shot settings, as shown in \Cref{few-shot-iAUROC}, \Cref{few-shot-pAUROC}, and \Cref{few-shot-F1}. The results demonstrate that SEM-CLIP outperforms other methods across all shot settings. 
%（8.11）We tested the performance metrics iAUROC, pAUROC, and $F1$-$max$ score under various shot settings, as shown in \Cref{few-shot-iAUROC}. 
We evaluated iAUROC, pAUROC, and $F1$-$max$ scores across various shot settings, as shown in \Cref{few-shot-iAUROC}.
The results show that SEM-CLIP outperforms the SOTA scores in BSL across all few-shot settings. Specifically, our method improved by $1.4\uparrow/0.4\uparrow/1.7\uparrow$ in the $1$-shot setting, $1.5\uparrow/0.2\uparrow/11.3\uparrow$ in the $2$-shot setting, $2.1\uparrow/1.9\uparrow/0.4\uparrow$ in the $5$-shot setting, and $1.3\uparrow/0.4\uparrow/1.5\uparrow$ in the $10$-shot setting.

%At the same time, we present representative visual samples of each defect category using different methods under the 10-shot setting, as shown in \Cref{Visualization of 10-shot segmentation}.
%（8.11）Additionally, under the $10$-shot setting, we present representative visual samples for each defect category using different methods, as shown in \Cref{Visualization of 10-shot segmentation}.
%It can be observed that SEM-CLIP achieves very precise defect localization and segmentation. It is not affected by irrelevant background information, which can distinctly identify the ambiguous boundaries between normal and defective areas with remarkable clarity.
%（8.11）Obviously, SEM-CLIP achieves precise defect localization and segmentation unaffected by irrelevant background information, which can distinctly identify the ambiguous boundaries between normal and defective areas with remarkable clarity.
Additionally, under the $10$-shot setting, SEM-CLIP demonstrated precise defect localization and segmentation, effectively distinguishing between normal and defective areas, as shown in \Cref{Visualization of 10-shot segmentation}.


%\begin{figure}[tb!]
%    \centering
%    \includegraphics[width=\linewidth, trim={3cm 2cm 10cm 1.5cm}, clip]{figs/mask1.pdf}
%    \caption{Visualization of 10-shot segmentation.}
%    \label{Visualization of zero-shot segmentation}
%\end{figure}


%\begin{table}[tb!]
%    \centering
%    \caption{Comparison of anomaly segmentation (AS) performance under few-shot}
%    \resizebox{\linewidth}{!}
%    {
%        \begin{threeparttable}
            %\setlength{\tabcolsep}{1.75mm}
%            {
%                \begin{tabular}{c|c|c|c}
%                    \toprule
%                    Models   & pAUROC (\%)   & PRO (\%)  & F1 (\%) \\
%                    \midrule
%                    UNet\cite{-}              & -- & -- & -- \\
%                    SegNet\cite{-}   & --  & -- & --  \\
%                    PaDiM\cite{defard2021padim}           & -- & -- & --  \\
%                    WinCLIP\cite{Jeong_2023_CVPR}   & --  & -- & --  \\
%                    SAA\cite{cao_segment_2023}   & --  & -- & --  \\
%                    MaskCLIP\cite{zhou2022maskclip}           & -- & -- & --  \\
%                    RegAD\cite{huang2022regad}           & -- & -- & --  \\
%                    \midrule
%                   \textbf{SEM-CLIP (Ours)}& \textbf{98.6} & \textbf{76.2} & \textbf{83.8} \\
%                    \bottomrule
%                \end{tabular}
%            }
%        \end{threeparttable}
%    }
%    \label{few-shot-seg}
%\end{table}


%\begin{table}[htbp]
%  \centering
%  \caption{Comparison of anomaly segmentation (AS) performance}
%  \begin{tabular}{@{}l l c c c c@{}}
%    \toprule
%    Setup & Models & iAUROC(\%) & pAUROC(\%) & PRO(\%) & \(\text{F1-max}\)(\%) \\
    % \midrule
    % \multirow{3}{*}{0-shot} & Trans-MM &   &  &  \\
    %                          & AnomalyCLIP\cite{zhou2024anomalyclip} &   &  &  \\
    %                          \cline{2-5}
    %                          & SEM-CLIP (ours) & 79.2 & 29.6 & 21.1 \\
%    \midrule
%    \multirow{4}{*}{1-shot}  & PaDiM\cite{defard2021padim} &  &  &  & \\
%                             & WinCLIP\cite{Jeong_2023_CVPR} &  &  &  & \\
%                             & MaskCLIP\cite{zhou2022maskclip} &  &  &  & \\
%                             & SAA\cite{cao_segment_2023}   & --  & -- & -- &  \\
 %                            & RegAD\cite{huang2022regad}           & -- & -- & -- & \\
  %                           & AnomalyGPT\cite{gu2024anomalygpt}         & -- & -- & -- & \\
 %                            \cline{2-6}
  %                           & SEM-CLIP (ours) & 98 & 96.7 & 62.6 & 69.6 \\
 %   \midrule
 %   \multirow{4}{*}{2-shot}  & PaDiM\cite{defard2021padim} &  &  &  & \\
 %                            & WinCLIP\cite{Jeong_2023_CVPR} &  &  &  & \\
 %                            & MaskCLIP\cite{zhou2022maskclip} &  &  &  & \\
 %                            & SAA\cite{cao_segment_2023}   & --  & -- & -- & \\
 %                            & RegAD\cite{huang2022regad}           & -- & -- & -- & \\
 %                            & AnomalyGPT\cite{gu2024anomalygpt}         & -- & -- & -- & \\
 %                            \cline{2-6}
 %                            & SEM-CLIP (ours) & 98.8 & 96.8 & 68.5 & 74.4 \\
 %   \midrule
 %   \multirow{4}{*}{5-shot}  & PaDiM\cite{defard2021padim} &  &  &  &  \\
 %                            & WinCLIP\cite{Jeong_2023_CVPR} &  &  &  &  \\
 %                            & MaskCLIP\cite{zhou2022maskclip} &  &  &  &  \\
 %                            & SAA\cite{cao_segment_2023}   & --  & -- & -- &  \\
 %                            & RegAD\cite{huang2022regad}           & -- & -- & -- &  \\
 %                            & AnomalyGPT\cite{gu2024anomalygpt}         & -- & -- & -- &  \\
 %                            \cline{2-6}
 %                            & SEM-CLIP (ours) & 99.7 & 97.8 & 71.7 & 78.6 \\
 %   \midrule                             
 %   \multirow{4}{*}{10-shot}  & PaDiM\cite{defard2021padim} &  &  &  &  \\
 %                            & WinCLIP\cite{Jeong_2023_CVPR} &  &  &  &  \\
 %                            & MaskCLIP\cite{zhou2022maskclip} &  &  &  &  \\
 %                            & SAA\cite{cao_segment_2023}   & --  & -- & -- &  \\
 %                            & RegAD\cite{huang2022regad}           & -- & -- & -- &  \\
 %                            & AnomalyGPT\cite{gu2024anomalygpt}         & -- & -- & -- &  \\
 %                            \cline{2-6}
 %                            & SEM-CLIP (ours) & 99.8 & 98.6 & 76.2 & 83.8 \\
%    \bottomrule
%  \end{tabular}
%  \label{few-shot-seg}
%\end{table}


\begin{table}[tb!]
    %\centering
    \caption{Comparison of evaluation metrics (iAUROC/pAUROC/F1-max) under different shot settings (\%).}
    \resizebox{1\linewidth}{!}
    {
        \begin{threeparttable}
            %\setlength{\tabcolsep}{1.75mm}
            {
                \begin{tabular}{l c c c c}
                    \toprule
                    Models & 1-shot & 2-shot & 5-shot & 10-shot \\
                    \midrule
                     % PaDiM \cite{defard2021padim} &  &  &  & \\
                     WinCLIP$+$ \cite{Jeong_2023_CVPR} & 51.4/84.5/28.5 & 55.5/85.3/29.5 & 64.9/86.1/29.7 & 72.2/87.0/31.1 \\
                     PromptAD \cite{li2024promptad}           & 94.1/95.8/58.2 & 96.1/96.5/60.4 & 96.3/96.9/61.5 & 97.8/97.3/62.7\\
                     % MaskCLIP \cite{zhou2022maskclip} &  &  &  & \\
                     % SAA \cite{cao_segment_2023}           & --  & -- & -- & \\
                     % RegAD \cite{huang2022regad}           & -- & -- & -- & \\
                     DRA \cite{ding2022catching} & \underline{96.6}/81.2/\underline{67.9} & \underline{97.3}/91.7/\underline{70.5} & \underline{97.6}/\underline{96.9}/\underline{78.2} & \underline{98.5}/\underline{98.2}/\underline{82.3}\\ 
                     %AnomalyGPT \cite{gu2024anomalygpt} & 86.77 & 89.79 & 86.32 &86.40 \\
                     AnomalyGPT \cite{gu2024anomalygpt} & 86.8/\underline{96.3}/61.6 & 89.8/\underline{96.6}/63.1 & 86.3/96.5/65.8 &86.4/96.5/65.2 \\
                    \midrule
                   \textbf{SEM-CLIP (Ours)}& \textbf{98.0/96.7/69.6} & \textbf{98.8/96.8/74.4} & \textbf{99.7/97.8/78.6} & \textbf{99.8/98.6/83.8} \\
                    \bottomrule
                \end{tabular}
            }
        \end{threeparttable}
    }
    \label{few-shot-iAUROC}
\end{table}

%\begin{table}[tb!]
%    \centering
%    \caption{Comparison of evaluation metrics under different shot settings (\%).}
%    \resizebox{1\linewidth}{!}
%    {
%        \begin{threeparttable}
%            %\setlength{\tabcolsep}{1.75mm}
%            {
%                \begin{tabular}{l c c c}
%                    \toprule
%                    Models & iAUROC & pAUROC & F1-max \\
%                    \midrule
%                     % PaDiM \cite{defard2021padim} &  &  &  & \\
%                     WinCLIP$+$ \cite{Jeong_2023_CVPR} & 51.4/55.5/64.9/72.2 & 84.5/85.3/86.1/87.0 & 28.5/29.5/29.7/31.1 \\
 %                    PromptAD \cite{li2024promptad}   & 94.1/96.1/96.3/97.8 & 95.8/96.5/\underline{96.9}/97.3 & 58.2/60.4/61.5/62.7\\
                     % MaskCLIP \cite{zhou2022maskclip} &  &  &  & \\
                     % SAA \cite{cao_segment_2023}           & --  & -- & -- & \\
                     % RegAD \cite{huang2022regad}           & -- & -- & -- & \\
%                     DRA \cite{ding2022catching} & \underline{96.6}/\underline{97.3}/\underline{97.6}/\underline{98.5} & 81.2/91.7/96.9/\underline{98.2} & \underline{67.9}/\underline{70.5}/\underline{78.2}/\underline{82.3} \\ 
                     %AnomalyGPT \cite{gu2024anomalygpt} & 86.77 & 89.79 & 86.32 &86.40 \\
%                     AnomalyGPT \cite{gu2024anomalygpt} & 86.8/89.8/86.3/86.4 & \underline{96.3}/\underline{96.6}/96.5/96.5 & 61.6/63.1/65.8/65.2 \\
%                    \midrule
%                   \textbf{SEM-CLIP (Ours)}& \textbf{98.0/98.8/99.7/99.8} & \textbf{96.7/96.8/97.8/98.6} & \textbf{69.6/74.4/78.6/83.8} \\
%                    \bottomrule
%                \end{tabular}
%            }
%        \end{threeparttable}
%    }
%    \label{few-shot-iAUROC}
%\end{table}

%\begin{table}[h]
%\begin{table}[tb!]
%    \centering
%    \caption{Comparison of image-level AUROC (\%).}
%    \resizebox{0.86\linewidth}{!}
%    {
%        \begin{threeparttable}
%            %\setlength{\tabcolsep}{1.75mm}
%            {
%                \begin{tabular}{l c c c c}
%                    \toprule
%                    Models & 1-shot & 2-shot & 5-shot & 10-shot \\
%                    \midrule
%                     % PaDiM \cite{defard2021padim} &  &  &  & \\
%                     WinCLIP$+$ \cite{Jeong_2023_CVPR} & 51.4 & 55.5 & 64.9 & 72.2 \\
%                     PromptAD \cite{li2024promptad}           & 94.1 & 96.1 & 96.3 & 97.8\\
%                     % MaskCLIP \cite{zhou2022maskclip} &  &  &  & \\
%                     % SAA \cite{cao_segment_2023}           & --  & -- & -- & \\
%                     % RegAD \cite{huang2022regad}           & -- & -- & -- & \\
%                     DRA \cite{ding2022catching} & \underline{96.6} & \underline{97.3} & %\underline{97.6} & \underline{98.5}\\ 
%                     %AnomalyGPT \cite{gu2024anomalygpt} & 86.77 & 89.79 & 86.32 &86.40 \\
%                     AnomalyGPT \cite{gu2024anomalygpt} & 86.8 & 89.8 & 86.3 &86.4 \\
%                    \midrule
%                   \textbf{SEM-CLIP (Ours)}& \textbf{98.0} & \textbf{98.8} & \textbf{99.7} & \textbf{99.8} \\
%                    \bottomrule
%                \end{tabular}
%            }
%        \end{threeparttable}
%    }
%    \label{few-shot-iAUROC}
%\end{table}

%\begin{table}[h]
%\begin{table}[tb!]
 %   \centering
 %   \caption{Comparison of pixel-level AUROC (\%).}
%    \resizebox{0.86\linewidth}{!}
%    {
%        \begin{threeparttable}
%            %\setlength{\tabcolsep}{1.75mm}
 %           {
%                \begin{tabular}{l c c c c}
%                    \toprule
 %                   Models & 1-shot & 2-shot & 5-shot & 10-shot \\
 %                   \midrule
 %                    %PaDiM \cite{defard2021padim} &  &  &  & \\
 %                    WinCLIP$+$ \cite{Jeong_2023_CVPR} & 84.5 & 85.3 & 86.1 & 87.0 \\
%                     PromptAD \cite{li2024promptad}           & 95.8 & 96.5 & \underline{96.9} & 97.3\\
%                     % SAA \cite{cao_segment_2023}           & --  & -- & -- & \\
%                     %RegAD \cite{huang2022regad}           & -- & -- & -- & \\
 %                    DRA \cite{ding2022catching} & 81.2 & 91.7 & \underline{96.9} & \underline{98.2}\\ 
%                     %AnomalyGPT \cite{gu2024anomalygpt} & 96.27 & 96.56 & 96.53 & 96.53\\
%                    AnomalyGPT \cite{gu2024anomalygpt} & \underline{96.3} & \underline{96.6} & 96.5 & 96.5\\
 %                   \midrule
  %                 \textbf{SEM-CLIP (Ours)}& \textbf{96.7} & \textbf{96.8} & \textbf{97.8} & \textbf{98.6} \\
 %                   \bottomrule
%                \end{tabular}
%            }
 %       \end{threeparttable}
 %   }
%    \label{few-shot-pAUROC}
%\end{table}

%\begin{table}[h]
%\begin{table}[tb!]
%    \centering
%    \caption{Comparison of $F1$-$max$ score (\%).}
%    \resizebox{0.86\linewidth}{!}
%    {
%        \begin{threeparttable}
%            %\setlength{\tabcolsep}{1.75mm}
%            {
%                \begin{tabular}{l c c c c}
%                    \toprule
 %                   Models & 1-shot & 2-shot & 5-shot & 10-shot \\
%                    \midrule
%                     %PaDiM \cite{defard2021padim} &  &  &  & \\
%                     WinCLIP$+$ \cite{Jeong_2023_CVPR} & 28.5 & 29.5 & 29.7 & 31.1 \\
%                     PromptAD \cite{li2024promptad}        & 58.2 & 60.4 & 61.5 & 62.7\\
%                     % SAA \cite{cao_segment_2023}           & --  & -- & -- & \\
%                     % RegAD \cite{huang2022regad}           & -- & -- & -- & \\
%                     DRA \cite{ding2022catching}  & \underline{67.9} & \underline{70.5} & \underline{78.2} & \underline{82.3}\\ 
%                     AnomalyGPT \cite{gu2024anomalygpt}  & 61.6 & 63.1 & 65.8 & 65.2 \\
%                    \midrule
%                   \textbf{SEM-CLIP (Ours)}& \textbf{69.6} & \textbf{74.4} & \textbf{78.6} & \textbf{83.8} \\
%                    \bottomrule
%                \end{tabular}
%            }
%        \end{threeparttable}
%    }
%    \label{few-shot-F1}
%\end{table}

\noindent
\textbf{Classification performance comparisons.}
%（8.11）As shown in \Cref{few-shot-class}, our method achieves the best accuracy, recall, and $F_{1}$ score results. However, the pre-trained EfficientNet model surpasses ours in precision. This advantage is likely due to the vast variability in the natural ImageNet dataset, which provides the model with substantial prior knowledge, and the advanced regularization techniques used in EfficientNet. 
%（8.11）Nonetheless, the overall accuracy of EfficientNet is lower, indicating its weaker recognition capabilities. 

SEM-CLIP excels in nearly all metrics, especially in the $F_{1}$ score, demonstrating its ability to identify defect categories while minimizing the false negatives. This makes it ideal for our SEM image classification task involving imbalanced defect categories.
As shown in \Cref{few-shot-class}, our method achieves the highest accuracy, recall, and $F_{1}$ score, although the pre-trained EfficientNet model surpasses ours in precision. This advantage is likely due to EfficientNet's extensive prior knowledge of the diverse ImageNet dataset and advanced regularization techniques. However, EfficientNet's lower overall accuracy suggests weaker recognition capabilities. SEM-CLIP excels in nearly all metrics, particularly in the $F_{1}$ score, highlighting its ability to accurately identify defect categories while minimizing false negatives, making it ideal for SEM image classification with imbalanced categories. The confusion matrix in \Cref{10-shot matrix} shows that SEM-CLIP classifies most defects with high accuracy, though it struggles with the ``particle'' category. This challenge arises from the varied morphologies of particles, which are easily confused with other defects, especially inflim, as these are essentially particles embedded within the film, sharing similar morphology, as shown in \Cref{fig:defect-complexity}.

%\begin{figure}[tb!]
\begin{figure}[h]
    \centering
    \includegraphics[width=0.82\linewidth]{figs/matrix.pdf}
    \caption{Classification confusion matrix of 10-shot.}
    \label{10-shot matrix}
\end{figure}

\begin{table}[tb!]
    \centering
    \caption{Comparison of defect classification performance (\%).}
    \resizebox{0.82\linewidth}{!}
    {
        \begin{threeparttable}
            %\setlength{\tabcolsep}{1.75mm}
            {
                \begin{tabular}{l|c|c|c|c}
                    \toprule
                    Models & Accuracy & Precision & Recall  & $F_{1}$ \\
                    \midrule
                    %CLIP \cite{radford2021learning}  & --  & -- & -- & -- \\
                    % CLIP-Art \cite{Conde_2021_CVPR}  & & & --  & -- & -- & -- \\
                    ViT \cite{dosovitskiy2020image} & 81.2  & 78.5 & 84.5 & 78.9 \\
                    ResNet101 \cite{he2016deep}  & 71.4  & 72.8 & 76.1 & 70.2 \\
                    ResNet50+ViT \cite{dosovitskiy2020image} & 81.2 & 75.8 & 85.3 & 78.4 \\
                    %ResNet-101 \cite{he2016deep}  & 10\% & --  & -- & -- & -- \\
                    EfficientNet \cite{tan2019efficientnet}  & 78.5  & \textbf{89.5} & 83.3 & 81.6 \\
                    %EfficientNet \cite{tan2019efficientnet}  & 10\% & --  & -- & -- & -- \\
                    \midrule
                   \textbf{SEM-CLIP (Ours)} & \textbf{83.7} & 87.2 & \textbf{86.7} & \textbf{84.4} \\
                    \bottomrule
                \end{tabular}
            }
        \end{threeparttable}
    }
    \label{few-shot-class}
\end{table}

%（8.11）To visually assess the classification performance of SEM-CLIP, we produced its confusion matrix under the 10-shot setting, as displayed in \Cref{10-shot matrix}. The matrix reveals that SEM-CLIP classifies most defect categories with high accuracy, but the primary challenge lies with the ``particle'' category. 
%（8.11）Due to particles' varied morphologies, they can easily be confused with other defects, especially with inflim. This is because Infilm defects are essentially particles embedded within the film, and their morphology is extremely similar to standalone particles, as illustrated in \Cref{fig:defect-complexity}.

%Since the varied morphologies of particles and their occurrence at any stage of the manufacturing process lead to their capture against highly variable backgrounds. This variability easily confuses particles with other defects, and we can see that this is particularly severe with infilm. This is because Infilm defects are essentially particles embedded within the film, and their morphology is extremely similar to that of standalone particles, as illustrated in \Cref{fig:defect-complexity}.



\subsection{Abalation Studies}

\noindent
\textbf{SEM-CLIP for defect Segmentation.}
%（8.11）We first investigate the effects of fine-tuning with few-shot samples. In \Cref{Ablation}, ``w/o Transformation Layer'' indicates that the Transformation Layer is not employed to fine-tune the image embeddings \( \vec{F_{j}} \) into \( \vec{F_{j}}' \). Instead, \( \vec{F_{j}} \) is directly used to produce segmentation results. We visualize these segmentation outcomes in \Cref{fig:interference}. 
We first examined the impact of fine-tuning with few-shot samples. In \Cref{Ablation}, ``w/o Transformation Layer'' indicates that the Transformation Layer was not used, resulting in direct use of \( \vec{F_{j}} \) for segmentation, as shown in \Cref{fig:interference}. 
Our SEM images are captured from the production line and display textual information at the top and bottom of the image. Without fine-tuning, the model tends to identify this textual information as defects erroneously. Furthermore, the lack of understanding regarding the complexity of SEM image backgrounds also makes it susceptible to mistakenly classifying normal background patterns as defects.

%（8.11）Additionally, we compare the impact of designed prompts in \Cref{Ablation}. ``w/o Detailed Prompt'' indicates that instead of using a detailed state-level prompt, we just listed all possible defect categories, such as ``\{ \} with a scratch.'', ``\{ \} with a hole.'' The results show that using prompts enriched with domain expert knowledge, such as ``\{ \} image with a linear scratch.'' demonstrates a more effective guiding effect.

We also assessed the influence of prompt design. ``w/o Detailed Prompt'' refers to using generic prompts instead of detailed, expert-informed ones. The results show that detailed prompts, like ``\{ \} image with a linear scratch'' are more effective.

\begin{figure}[tb!]
    \centering
    \includegraphics[width=0.84\linewidth]{figs/interference.pdf}
    %\caption{Segmentation results w/o and w/ the Transformation layer. The first row displays results w/o Transformation Layer, where no fine-tuning was applied while the second row is the results after 10-shot fine-tuning. (a) textual information interference; (b) background patterns interference.}
ok    \caption{Segmentation results w/o (top row) and w/ (bottom row) the Transformation Layer after 10-shot fine-tuning: (a) textual information interference; (b) background patterns interference.}
    \label{fig:interference}
\end{figure}

%\begin{table}[h]
%\begin{table}[tb!]
%    \centering
%    \caption{Ablation Studies on 10-shot Segmentation (\%).}
%    \resizebox{0.94\linewidth}{!}
 %   {
%        \begin{threeparttable}
 %           {
%                \begin{tabular}{c|c|c|c}
%                    \toprule
%                    Method  & iAUROC & pAUROC & $F_{1}$-$max$ \\
%                    \midrule
 %                   w/o Transformation Layer & 86.8 & 79.2 & 29.6 \\
%                    w/o Detailed Prompt & 99.6 & 98.1 & 82.1 \\
%                    \textbf{SEM-CLIP} & \textbf{99.8} & \textbf{98.6} & \textbf{83.8} \\
%                    \bottomrule
%                \end{tabular}
%            }
%        \end{threeparttable}
 %   }
%    \label{Ablation-Segmentation}
%\end{table}

%(8.11) We then examine the impact of multi-layer features on defect segmentation. Our SEM-CLIP model utilizes outputs from $4$ encoding blocks, including $4$ vanilla image embeddings $[\vec{F_{1}}, \vec{F_{2}}, \vec{F_{3}}, \vec{F_{4}}]$ and $4$ new image embeddings $[\vec{V_{1}}, \vec{V_{2}}, \vec{V_{3}}, \vec{V_{4}}]$, to compute defect maps. In \Cref{Ablation}, ``w/o multi-layer'' refers to using only the outputs from the last encoding block of the image encoder, namely $\vec{F_{4}}$ and $\vec{V_{4}}$. It is evident that incorporating multi-layer information yields superior segmentation results.
 Lastly, we analyzed the role of multi-layer features. Our SEM-CLIP model uses outputs from four encoding blocks, including vanilla and new image embeddings, to compute defect maps. "w/o multi-layer" refers to using only the last encoding block’s outputs. Incorporating multi-layer information significantly improves segmentation performance.


\noindent
\textbf{SEM-CLIP for defect Classification.}
%(8.11)\Cref{Ablation} displays the impact of different components on classification performance. ``w/o $\vec{P_S}$'' indicates that the model does not consider the prior knowledge of CLIP and directly outputs the classification predictions from the classification head, corresponding to setting $\alpha = 1$ in \Cref{eq:classification}. Conversely, ``w/o \(\vec{P_C}\)'' represents reliance solely on text prompt-guided predictions for classification, corresponding to \(\alpha = 0\). We can see that relying solely on the pre-trained CLIP model for classifying SEM defects is not feasible due to poor performance. Conversely, a classification head fine-tuned with few-shot samples is significantly better. These results can be attributed to the fact that CLIP is pre-trained by a vast array of image-text pairs available online, which equips it to make accurate judgments in common application scenarios. However, for specific industrial applications under unique conditions, few-shot samples are still required for fine-tuning to guide the learning process effectively. This underscores the importance of few-shot learning in such specialized tasks, although general models provide a good starting point.

%(8.11)In \Cref{Ablation}, for the classification task, ``w/o multi-layer'' indicates that instead of utilizing $CLS$ tokens from four different layers $[\vec{f^{CLS}_1}, \vec{f^{CLS}_2}, \vec{f^{CLS}_3}, \vec{f^{CLS}_4}]$, we use only the $CLS$ token $\vec{f^{CLS}_4}$ from the last layer of the image encoder. The outcome demonstrates that employing a multi-layer approach allows for a more comprehensive capture of both global and local image features. This enhanced feature detection significantly aids the model in better understanding defect information, thereby achieving superior classification performance.
\Cref{Ablation} shows the effects of various components on classification. ``w/o \( \vec{P_S} \)'' indicates the exclusion of CLIP’s prior knowledge, leading to classification based solely on the classification head, as in \Cref{eq:classification} with \( \alpha = 1 \). ``w/o \( \vec{P_C} \)'' relies only on text prompt-guided predictions (\( \alpha = 0 \)). The results demonstrate that solely relying on pre-trained CLIP is inadequate for SEM defect classification. Fine-tuning with few-shot samples significantly improves performance, highlighting the importance of few-shot learning in specialized tasks.
For classification, ``w/o multi-layer'' refers to using only the last layer’s CLS token. The results show that employing a multi-layer approach enhances feature detection, leading to superior classification performance by capturing both global and local image features.

%\begin{table}[tb!]
%\begin{table}[h]
%    \centering
%    \caption{Ablation Studies on 10-shot Classification (\%).}
%    \resizebox{0.84\linewidth}{!}
%    {
 %       \begin{threeparttable}
%            %\setlength{\tabcolsep}{1.75mm}
%            {
%                \begin{tabular}{c|c|c|c|c}
%                    \toprule
%                    Method  & Accuracy  & Precision & Recall  & $F_{1}$ \\
%                    \midrule
                    %w/o $\vec{4CLS}$& 80.5 & 75.6 & 83.8 & 77.7 \\
%                    w/o $\vec{P_{S}}$& 83.6 & 87.2 & 86.6 & 84.3 \\
%                    w/o $\vec{P_{C}}$& 25.7 & 20.2 & 30.1 & 16.0 \\
%                    \textbf{SEM-CLIP} & \textbf{83.7} & \textbf{87.2} & \textbf{86.7} & \textbf{84.4} \\
%                    \bottomrule
%                \end{tabular}
%            }
%        \end{threeparttable}
%    }
%    \label{Ablation-Classification}
%\end{table}
%\vspace{-2pt}
%\begin{table}[tb!]
%\begin{table}[h]
%    \centering
%    \caption{Ablation Studies on multi-layer (10-Shot).}
%    \resizebox{\linewidth}{!}
%    {
%        \begin{threeparttable}
 %           %\setlength{\tabcolsep}{1.75mm}
 %           {
 %               \begin{tabular}{c c c c c c c c}
 %                   \toprule
 %                   \multirow{2}{*}{Methods} & \multicolumn{3}{c}{Segmentation (\%)} & \multicolumn{4}{c}{Classification (\%)} \\
  %                  \cmidrule(r){2-4} \cmidrule(l){5-8}
 %                   & iAUROC & pAUROC & $F_{1}$-$max$ & Acc. & Prec. & Recall & $F_{1}$ \\
 %                   \midrule
 %                   w/o multi-layer & 99.4 & 96.2 & 75.9 & 80.5 & 75.6 & 83.8 & 77.7 \\
 %                   \textbf{SEM-CLIP} & \textbf{99.8} & \textbf{98.6} & \textbf{83.8} & \textbf{83.7} & \textbf{87.2} & \textbf{86.7} & \textbf{84.4} \\
  %                  \bottomrule
  %              \end{tabular}
  %          }
  %      \end{threeparttable}
  %  }
 %   \label{Ablation-multi-layer}
%\end{table}

\begin{table}[tb!]
%\begin{table}[h]
    \centering
    \caption{Ablation Studies under the 10-Shot setting.}
    \resizebox{\linewidth}{!}
    {
        \begin{threeparttable}
            %\setlength{\tabcolsep}{1.75mm}
            {
                \begin{tabular}{c c c c c c c c}
                    \toprule
                    \multirow{2}{*}{Methods} & \multicolumn{3}{c}{Segmentation (\%)} & \multicolumn{4}{c}{Classification (\%)} \\
                    \cmidrule(r){2-4} \cmidrule(l){5-8}
                    & iAUROC & pAUROC & $F_{1}$-$max$ & Acc. & Prec. & Recall & $F_{1}$ \\
                    \midrule
                    w/o Transformation Layer & 86.8 & 79.2 & 29.6 & - & - & - & - \\
                    w/o Detailed Prompt & 99.6 & 98.1 & 82.1 & - & - & - & -  \\
                    w/o $\vec{P_{S}}$  & - & - & - & 83.6 & 87.2 & 86.6 & 84.3 \\
                    w/o $\vec{P_{C}}$  & - & - & - & 25.7 & 20.2 & 30.1 & 16.0 \\
                    w/o multi-layer & 99.4 & 96.2 & 75.9 & 80.5 & 75.6 & 83.8 & 77.7 \\
                    \midrule
                    \textbf{SEM-CLIP} & \textbf{99.8} & \textbf{98.6} & \textbf{83.8} & \textbf{83.7} & \textbf{87.2} & \textbf{86.7} & \textbf{84.4} \\
                    \bottomrule
                \end{tabular}
            }
        \end{threeparttable}
    }
    \label{Ablation}
\end{table}

