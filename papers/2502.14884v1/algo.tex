%\vec{x}
\section{SEM-CLIP Framework}
\label{sec:framework}

%\begin{figure}[tp]
\begin{figure}[h]
  \centering
  \includegraphics[width=0.94\linewidth]{figs/defect-complexity.pdf}
  \caption{Complexity of defect morphologies. (a) Differences within the same type; (b) Similarity between different types. }
  \label{fig:defect-complexity}
\end{figure}

In this section, we introduce SEM-CLIP, as shown in \Cref{fig:SEM-CLIP}, specifically designed for classifying and segmenting wafer surface defects under the few-shot setting. Initially, we construct a text prompt incorporating expert knowledge regarding wafer surface defect patterns. This prompt enables us to avoid detailed labels for each sample. Following this, we implement a dual path block by adding a $V$-$V$ attention block to the transformer block within the vanilla ViT architecture \cite{dosovitskiy2020image}. 
%We extract features at various levels from this architecture and employ a new method to remove redundant features for calculating similarity, ultimately achieving precise defect classification and segmentation results. 
We extract features at various levels from this architecture and employ a new method to remove redundant features to calculate similarity. Additionally, we fine-tune the Transformation Layer and Classification Head using few-shot samples, ultimately achieving precise defect classification and segmentation results.

\subsection{Text Prompt Design}
\begin{figure*}[tb!]
  \centering
  \includegraphics[width=0.86\textwidth]{figs/Framework-2.pdf}
  \caption{Our SEM-CLIP framework. }
  \label{fig:SEM-CLIP}
\end{figure*}

Due to the complexity of integrated circuit manufacturing processes, wafer surface defects can vary greatly in appearance, resulting in significant morphological differences within the same type of defect and similar textures between different types of defects \Cref{fig:defect-complexity}. Consequently, it is essential to utilize domain expert knowledge to refine the rough cues such as ``anomaly'' or ``defect'' into more detailed descriptions of defect morphologies by useful prior information about the target defect areas. 
For instance, defects of the ``scratch'' type typically appear as fine, long, linear marks in the \textit{back-end-of-line} (BEOL) processes but may manifest as fish-scale patterns in the \textit{front-end-of-line} (FEOL) processes. 
These elliptical depressions, which exhibit a continuous distribution, can easily be mistaken for hole-type defects without careful observation.

This task employs a composite prompt structure, as illustrated in \Cref{fig:text_prompt}.  We decompose the prompts into template-level and state-level components, where the state-level prompts provide detailed descriptions of the possible appearances of each type of defect, such as ``\{ \} image with a linear scratch'' or ``\{ \} image with fish scale-shaped scratches''. Additionally, since scanning electron microscopes can produce blur due to focusing issues or variations in image brightness caused by different electron beam intensities, the template-level prompts can describe the effects on SEM images, such as ``a blurry photo of the \{ \}'' or ``a dark photo of a \{ \}''. Finally, by replacing the \textit{state} in the template-level prompts with the state-level prompts, we combine them to form the final text prompts.

The text prompts are designed and shared for all SEM images. During the practical application of our model and the analysis of query images, there is no need to adjust the prompts.
\begin{figure}[tb!]
  \centering
  \includegraphics[width=0.90\linewidth]{figs/text-prompt.pdf}
  \caption{Text prompts are built on state-level prompts and template-level prompts. }
  \label{fig:text_prompt}
\end{figure}

\subsection{Image Feature Extraction}

For SEM images, the variability and complexity of background patterns tend to interfere with defect detection, which is undesirable. Recent studies have reported that $Q$-$K$ self-attention \cite{DNN-2017NeurIPS-Attention} may lead to incorrectly establishing connections in semantically irrelevant areas
%(such as between background and foreground)
, resulting in dispersed attention \cite{li2023clip}. The vanilla self-attention mechanism is described as follows:
\begin{equation}
   \text{Attention}(\vec{Q}, \vec{K}, \vec{V}) = \text{Softmax}\left( \frac{\vec{Q}\vec{K}^\top}{\sqrt{d_k}} \right) \vec{V}.
\label{eq:QKV}
\end{equation}

In contrast, $V$-$V$ attention \cite{li2023clip}, by directly comparing and associating similar feature values, can more accurately focus on relevant feature areas, effectively reducing interference from the background. The $V$-$V$ attention is formulated as follows:
\begin{equation}
    \text{Attention}( \vec{V}, \vec{V}, \vec{V} ) = \text{Softmax}\left(\frac{\vec{V}\vec{V}^\top}{\sqrt{d_k}}\right) \vec{V}.
    \label{eq:VVV}
\end{equation}

%The query $\left(Q\right)$ and key $\left(K\right)$ components are designed to capture dependencies among various features and identify matching features across the entire image. As illustrated in \Cref{eq:QKV}, if the key is not properly parameterized, the model may incorrectly align with irrelevant image regions to the query. In contrast, \Cref{eq:VVV} directly computes the self-correlation of the value $\left(V\right)$, strengthening the connections between similar features. This enables the model to focus more on semantically similar or related areas, avoiding distractions from irrelevant regions. 

Therefore, we modify the vanilla CLIP ViT \cite{dosovitskiy2020image} backbone for feature extraction by adding a branch while retaining the vanilla transformer structure. This branch incorporates the $V$-$V$ attention block, constructing a new dual-path block, and the encoding block is composed of $n$ dual-path blocks. The entire ViT backbone contains $m$ encoding blocks, as shown in \Cref{fig:SEM-CLIP}. 
Taking the $i$-th dual-path block within the $j$-th encoding block as an example, the input is $\vec{F_{j}^{i-1}}$, and it gives two outputs:
\begin{align}
\vec{F_{j}^{i}} &= \text{Arch}_{\vec{QKV}}(\vec{F_{j}^{i-1}}) + \vec{F_{j}^{i-1}}, \\
\vec{V_{j}^{i}} &= \text{Arch}_{\vec{VVV}}(\vec{F_{j}^{i-1}}),
\end{align}
where $\text{Arch}_{\vec{QKV}}$ and $\text{Arch}_{\vec{VVV}}$ signify the vanilla $QKV$ block and the $VVV$ block respectively. $\vec{F_{j}^{i}}$ and $\vec{V_{j}^{i}}$ denote the outputs of these two blocks.

The input of the $j$-th encoding block is the output of the last layer (the $n$-th dual-path block) of the $ \left(j-1\right) $-th encoding block:
\begin{align}
\vec{F_{j}^{0}} &= \vec{F_{j-1}^{n}} = \vec{F_{j-1}}.
\end{align}
Therefore, for the $j$-th encoding block, the output is:
\begin{align}
\vec{F_{j}} &= \vec{F_{j}^{n}} = \text{Arch}_{\vec{QKV}}(\vec{F_{j}^{n-1}}) + \vec{F_{j}^{n-1}}, \\
\vec{V_{j}} &= \sum_{i=0}^{n} \vec{V_{j}^{i}}.
\end{align}

We extract features at multi-levels from the output of the encoding block, resulting in $m$ vanilla image embeddings $[\vec{F_{1}}, \vec{F_{2}}, \cdots, \vec{F_{m}}]$ and $m$ new image embeddings $[\vec{V_{1}}, \vec{V_{2}}, \cdots, \vec{V_{m}}]$ transformed by $V$-$V$ attention.

Notably, the weights for our vanilla $QKV$ block are loaded from the weight file of the pre-trained CLIP image encoder. Additionally, the $VVV$ block parameters are directly copied from those in the $QKV$ block. We merely modify the method of data computation rather than the data itself. Therefore, retraining is unnecessary.

\subsection{Defect Segmentation}
\label{sub:Defect Segmentation}
When using a pre-trained CLIP model for zero-shot defect segmentation, the typical method is directly calculating the similarity between text and image embeddings to get a defect map. However, this approach is not suitable for our task. Although we have constructed a detailed textual prompt with expert knowledge,
%which provides richer information than class labels and better leverages the strong semantic priors of aligned images and texts,images are continuous signals with fine-grained details and complex semantics. 
the text still struggles to fully describe all information for corresponding images, especially for our unusual SEM images.
%especially those in our unusual SEM dataset, As shown in \Cref{fig:complex background}
This means our problem cannot be addressed with a zero-shot approach. Instead, it requires few-shot samples for fine-tuning. In this study, we adopt a few-shot learning approach 
%and further enhance it by incorporating the $V$-$V$ attention block 
to improve the detection of SEM defects. The specific implementation details are as follows:

%\begin{figure}[tb!]
%    \centering
%    \includegraphics[width=\linewidth, trim={0.8cm 2.8cm 0.8cm 2.5cm}, clip]{figs/complex background.pdf}
%    \caption{SEM images (without defects) containing complex information}
%    \label{fig:complex background}
%\end{figure}

First, we utilize a pre-trained CLIP text encoder to transform the text prompt $\vec{T}$ into a text embedding $\vec{t}$:
\begin{align}
\vec{t} &= \text{TextEncoder}(\vec{T}).
\end{align}

As mentioned in the previous section, we modify the structure of the image encoder, resulting in two different types of image embeddings, denoted as $\vec{F}$ and $\vec{V}$. These embeddings are extracted from $m$ different levels.

\noindent
\textbf{Segmentation based on $\vec{F}$.}
The vanilla image embedding $ \vec{F} = \{\vec{f}^{CLS}, \vec{f}^1, \vec{f}^2, \ldots, \vec{f}^T \} $, where $ \vec{f}^{CLS}$ serves as the $CLS$ token aggregating the global features of the image, commonly used in image-level defect detection, consider applying it to defect classification tasks. 
%$ \vec{F}[1:] = \{\vec{f}^1, \vec{f}^2, \ldots, \vec{f}^T\} $ contains more local features and can be considered for pixel-level defect segmentation. Therefore, when matching between image and text, we use $ \vec{F}[1:] $ and the text embedding $ \vec{t} $ to compute cosine similarity.
$ \vec {F}[1:] = \{\vec{f}^1, \vec{f}^2, \ldots, \vec{f}^T\} $ contains more detailed information, so we use it for pixel-level defect segmentation. 

%Although we have constructed a detailed textual prompt with expert knowledge, which provides richer information than class labels and better leverages the strong semantic priors of aligned images and texts, images are continuous signals with fine-grained details and complex semantics. The text still struggles to fully describe all the information in the corresponding images, especially those in our unusual SEM dataset. 
To enhance the model's understanding of our application scenario, we introduce a transformation layer fine-tuned with a few samples. Specifically, this transformation layer functions by mapping the image embeddings to a joint embedding space through a linear layer. The input for the mapping is represented as $[\vec{F_{1}}[1:], \vec{F_{2}}[1:], \cdots, \vec{F_{m}}[1:]]$, and the output is $[\vec{F_{1}^{'}}, \vec{F_{2}}^{'}, \cdots, \vec{F_{m}}^{'}]$. Taking the output image embedding $\vec{F_{j}}$ from the $j$-th encoding block as an example, the mapping process is as follows:
\begin{align}
\vec{F_{j}}^{'}=\text{Transformation}(\vec{F_{j}}[1:]).
\end{align}
%where $\vec{k_{j}}$ and $\vec{b_{j}}$ respectively represent the weights and bias of the transformation layer.

For the transformed vanilla image embedding $\vec{F_{j}^{'}}$, we calculate its cosine similarity with the text embedding $\vec{t}$. The formula is as follows:
\begin{align}
\vec{s(F_{j}^{'}, t)} = \vec{\frac{F_{j}^{'} \cdot t}{\|\vec{F_{j}^{'}}\|_{2} \|t\|_{2}}},
\label{eq:similarity}
\end{align}
where $ \vec{F_{j}^{'} \cdot t} $ represents the dot product of $ \vec{F_{j}^{'}}$ and $ \vec{t}$, $ \vec{ \|F_{j}^{'}\|_{2} } $ and $\vec{ \|t\|_{2} }$ are the $L2$ norms of the vectors $\vec{F_{j}^{'}}$ and $ \vec{t}$ along $C$ dimension. 
%respectively.

%We calculate the defect map produced by the output $\vec{F_{j}}$ of the $j$-th dual path block, guided by the text prompt:
After processing through the softmax layer, we obtain the defect map calculated from $\vec{F_{j}}$ of the $j$-th encoding block:
\begin{align}
%\vec{A_j^F} = \text{Softmax}(\text{Upsample}(\vec{s(F_{j}^{'}, t)})),
\vec{A_j^F} = \text{Softmax}(\vec{s(F_{j}^{'}, t)}),
\end{align}
and then sum the defect maps corresponding to $ m $ vanilla images embeddings to obtain the segmentation result $\vec{A^F}$,
\begin{align}
\vec{A^F = \sum_{j=1}^{m} A_j^F}.
\end{align}

\noindent
\textbf{Segmentation based on $\vec{V}$.}
Similar to the operations performed on $\vec{F}$, for the new image embedding $\vec{V}$, we discard the $CLS$ token to obtain $\vec{V}[1:]$ 
%to calculate similarity with the text embedding $\vec{t}$. 
to calculate the defect map.
%Observations indicate that identical erroneous bright spots often appear in non-defective areas regardless of the text prompt \cite{li2023clip}. Research indicates that identifying and removing these irrelevant bright spots as redundant features can effectively mitigate noise in predicted segmentation results \cite{li2023clip}. 
Research indicates that erroneous bright spots often appear in the same non-defective areas regardless of the textual prompts. Identifying and removing these irrelevant bright spots as redundant features can effectively reduce noise in the predicted segmentation results \cite{li2023clip}.
Taking the output of the $j$-th encoding block $\vec{V_{j}}$ as an example, the specific operations are as follows:

First, perform $L2$ normalization on the image embedding $\vec{V}[1:]$ and text embedding $\vec{t}$, and then conduct element-wise multiplication to generate a multiplied feature $\vec{V_{j}^{m}}$ containing information from both image and text:
\begin{align}
\vec{V_j^m} = \frac{\vec{V_j}}{\|\vec{V_j\|_2}} \odot \frac{\vec{t}}{\|\vec{t\|_2}}.
\end{align}

We calculate the mean of the multiplied feature $\vec{V_{j}^{m}}$ to obtain the redundant feature $\vec{V_j^r}$ :
\begin{align}
\vec{V}_j^r = \text{mean}(\vec{V}_j^m),
\end{align}
then remove the redundant feature $\vec{V_j^r}$ from the multiplied feature $\vec{V_{j}^{m}}$ to get the defect map:
\begin{align}
\vec{A^V_j} = \text{Softmax}(\vec{V_j^m} - \vec{V_j^r}). 
\end{align}

Sum defect maps corresponding to $m$ new image embeddings $\vec{V}$ to get the segmentation result $\vec{A^V}$:
\begin{align}
\vec{A^V} = \vec{\sum_{j=1}^{m} A^V_j}.
\end{align}

%The final overall segmentation defect map is given by:
Considering the segmentation results from these two image embeddings, the final overall defect map is given by:
\begin{align}
\vec{A} = \vec{A^F} + \vec{A^V}.
\end{align}
%This combines the anomaly maps derived from both the original image embeddings \( F \) and the new image embeddings \( V \), providing a comprehensive assessment of anomalies in the segmentation.

\subsection{Defect Classification}
%Due to the self-supervised contrastive learning ability of the CLIP model \cite{radford2021learning}, it can comprehend the semantic relationship between images and text, thereby possessing zero-shot classification capability.
The self-supervised contrastive learning ability of CLIP \cite{radford2021learning} enables it to understand the semantic relationships between images and text, thereby possessing zero-shot classification capability.
Specifically, the CLIP model encodes the query image $\vec{X}$ to obtain image embeddings, then computes the inner product between the image embeddings with all possible text embeddings, obtaining the label corresponding to the maximum inner product as the classification result. Thereby, we can directly utilize \Cref{eq:similarity}. Since there are $m$ different similarity scores corresponding to $m$ different level image embeddings, we take the maximum score as follows:
\begin{align}
\vec{s_{max}} = \text{Max}(\vec{s(F_{j}^{'}, t)}),  j=1, \cdots, m.
\end{align}
The classification prediction probability obtained through similarity calculation is given by:
\begin{align}
\vec{P_{S}}= \text{Softmax}(\vec{s_{max}}).
\end{align}

Although CLIP's contrastive learning capability enables direct completion of image classification tasks, as we mentioned in \Cref{sub:Defect Segmentation}, it is challenging for pre-trained vision-language models to achieve satisfactory performance directly in specific scenarios. Therefore, we require a few SEM defect images for fine-tuning.

%Inspired by the Vision Transformer, as mentioned in the previous section, the $CLS$ token is the first token in the vanilla image embedding $F$. This token is an Extra Learnable $[CLS]$ Embedding that aggregates global features by gathering information from other tokens during the subsequent image encoding process, thus avoiding bias towards certain tokens. Consequently, it is natural to consider utilizing this $CLS$ token for classification purposes. Since there are $m$ dual path blocks, we obtain $m$ vanilla image embeddings $\vec{F}$. Therefore, the classification embeddings are represented as:
Inspired by the Vision Transformer \cite{dosovitskiy2020image}, which utilizes an extra learnable $[CLS]$ embedding to aggregate information from other tokens during the subsequent image encoding process, resulting in a $CLS$ token aggregating global features, we naturally consider using it to implement classification functionality. The $CLS$ token occupies the first encoding position in the vanilla image embedding $\vec{F}$. Since there are $m$ encoding blocks, we obtain m vanilla image embeddings $\vec{F}$. The classification $CLS$ vectors are represented as:
\begin{align}
\vec{F_C} = [\vec{f^{CLS}_1}, \vec{f^{CLS}_2}, \cdots, \vec{f^{CLS}_m}],
\end{align}
%having identified effective feature vectors $\vec{F_C}$, we then proceed to train a simple classification head, such as a linear classifier, to obtain the classification prediction probabilities $\vec{P_{C}}$ from the classification head:
After obtaining effective feature vectors $\vec{F_C}$, we then use it to fine-tune a simple classification head, such as a linear classifier, resulting in the classification prediction probability $\vec{P_{C}}$:
\begin{align}
\vec{F_{C}^{'}} = \vec{W \cdot F_C} + \vec{b},\\
\vec{P_{C}} = \text{Softmax}(\vec{F_{C}^{'}}),
\end{align}
%where $\vec{W}$ denotes the weight matrix, and $\vec{b}$ represents the bias. 
here $\vec{W}$ denotes the weight matrix, and $\vec{b}$ signifies the bias of the classification head.

The final classification prediction probabilities are derived from the image-text contrast score calculated by CLIP and the prediction scores of the classification head, expressed as follows:
\begin{align}
\vec{P} =\vec{(1-\alpha)} \cdot \vec{P_{S}} + \vec{\alpha} \cdot \vec{P_{C}},
\label{eq:classification}
\end{align}
where $\vec{\alpha}$ is a scalar weight that balances these two probabilities.
