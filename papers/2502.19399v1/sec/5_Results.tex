\section{Results and discussion}
\label{sec:Results}

\noindent
We have implemented DROID in Python 3.8.8 on a 64-bit RHEL 7.9 server with a 2.2GHz Intel\textsuperscript{\tiny\textregistered} Xeon\textsuperscript{\tiny\textregistered} Silver 4114 CPU. We used HSPICE for circuit simulation using models from a commercial low-power 65nm technology. The characterization to build timing models for the cells, as described in Section~\ref{sec:Simulation}-\ref{subsec:timing_capture}, took roughly 14 hours. This is a one-time step at a technology node, similar to standard-cell library characterization, and its results can be reused across all designs and all simulations in the technology node. Therefore, like standard-cell library characterization, this amortization makes the runtime acceptable. 

\noindent
\underline{Comparison of DROID with GenAdler and HSPICE}
Fig.~\ref{fig:waveforms_comparison} shows period waveforms of five ROs in a {5$\times$5} A2A array as predicted by DROID, GenAdler, and HSPICE, for the same initial conditions, showing the evolution of RO states.  The GenAdler simulation is based on available code~\cite{wang2019matlab} and uses the tanh approximation for $f_{J_{ij}}$.
In all plots, the period of each of the five ROs, labeled RO$_0$ through RO$_4$, evolves over the simulation until it reaches its final value.  As pointed out in Section~\ref{sec:Background}-\ref{subsec:ro_ising}, the system tries to synchronize to a common period after which the phase differences between ROs become constant.  It can be seen that the period waveform from DROID in Fig.~\ref{fig:droid_wave} matches that from HSPICE (Fig.~\ref{fig:hspice_wave}), but the waveform from  GenAdler (Fig.~\ref{fig:genAdler_wave}) is noticeably different from HSPICE.
Inaccuracies in the GenAdler waveforms are attributed to the limitations discussed in Section~\ref{sec:limitations_of_ct_approx}. 

\noindent
\underline{Runtime trend as a function of array size}
We apply DROID to the silicon-proven A2A-coupled RO array~\cite{Lo2023} (Section~\ref{sec:A2A}). Table~\ref{tbl:runtime} compares the runtimes of DROID and HSPICE for various array sizes, simulated up to 100ns, for a single initial condition, over three array sizes, 5$\times$5, 20$\times$20, and 50$\times$50, for random Ising problems. The HSPICE runtime increases superlinearly with the array dimension, and is prohibitively large even for a single initial condition; in contrast, DROID is computationally efficient. 

\input{tables/simulation_runtimes}

The analysis above considers a single graph topology and a single initial condition on the graph. We examine the scalability of DROID by determining the runtime for simulating the solution of Ising problems on A2A arrays of sizes up to 50$\times$50, across various random graphs, and for multiple initial conditions.  Recall from Section~\ref{sec:A2A}, that an A2A array of size N$\times$N is equivalent to a hexagonal/King's graph array with $\sim$$N^2$ coupled ROs~\cite{Tabi21, Lucas2019}; therefore, a 50$\times$50 A2A array has equivalent computation power to a $\sim$2500-spin planar array, and avoids the problems of weakened spins caused by minor embedding in planar RO arrays.  The random graphs are characterized by the value of graph density, defined as $2|E|/(|V||V|-1)$, where $|V|$ is the number of vertices (i.e., ROs in the array) and $|E|$ is edge count: the density varies from 0 for a graph with no edges to 1 for a complete graph. 

DROID was exercised on arrays of various sizes: 5$\times$5, 10$\times$10, 20$\times$20, 32$\times$32, and 50$\times$50.  
For each size, we generate 80 random problems, 20 problems per graph density $\in \{0.4, 0.6, 0.8, 1.0\}$, with coupling values from a uniform probability distribution over $[-7, +7]-\{0\}$.
Each problem was simulated with different initial conditions up to synchronization with a maximum simulation time of 250$\mu$s. The distribution of DROID runtimes that attained synchronization for each array size is shown in Fig.~\ref{fig:runtime}. Each box shows the lower quartile, median, and upper quartile of the runtimes over more than 1000 simulations for each array size. 
As can be seen from the figure, the median runtime of synchronized runs for a 50$\times$50 array is 10 minutes and the upper quartile is 19.1 minutes. Such an experiment is not possible on HSPICE, since even a single run takes over 16 hours to simulate a 50$\times$50 array for 100ns, and 1000 simulations would take more than 20 months.

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.6\linewidth]{img/runtime_trend_250us_percentage_tol.pdf}
    \vspace{-4mm}
    \caption{Distribution of DROID runtimes for various A2A array sizes.
    }
    \label{fig:runtime}
    \vspace{-6mm}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{img/phase_evolution.pdf}
    \vspace{-6mm}
    \caption{The evolution of RO-phases from random to binarized phases for a particular problem; phases are represented by the color bar at right.
    }
    \label{fig:phase_evolution}
    \vspace{-6mm}
\end{figure}

Fig.~\ref{fig:phase_evolution} shows the evolution of phases for a representative problem in a 50-spin A2A system, over multiple snapshots of time, starting from random phases. The blue and red colors correspond to the $\pm 1$ spins. As the state of the array evolves, the spins go through the continuum between the two states, shown by the color bar, until they converge to the final state, seen after 700 cycles.

\noindent
\underline{Comparison with hardware}
Since the A2A Ising hardware platform does not allow us to probe terminals to observe the states of the ROs, we compare the synchronized state from the hardware with that from a software simulation. The A2A hardware has a 50$\times$50 array~\cite{Lo2023}, using one RO from the array as a sampling clock and another as the reference spin. An arbitrary 48-spin Hamiltonian needs $(48+1)$ A2A ROs after accounting for the reference spin to implement the linear terms. Therefore, as a test set, we generate a set of 48-spin problems for five random values of graph density, \{0.2, 0.4, 0.6, 0.8, 1.0\}, with 50 problems for each density. As before, the coupling values for edges are chosen uniformly in $[-7, +7]-\{0\}$. For each problem, 100 samples obtained from the hardware are compared with DROID under randomized initial conditions.
We compare the distributions of Hamiltonian~\eqref{eq:ham}, for the sample of random coupling weights, for the solutions from the hardware solver and the simulator. 

The distributions of the optimized Hamiltonian from DROID and the hardware solver, for a specific problem instance with a density of 0.2, are shown in Fig.~\ref{fig:distribution}. The x-axis is normalized to the optimum Hamiltonian for the problem, and the bin size is 5\% of the optimum Hamiltonian value, i.e., 0.05 units on this normalized axis. Visually, the histograms are seen to be very similar.

\begin{figure}[htb]
\centering
\includegraphics[width=0.65\linewidth]{img/DROID_vs_HW_0p2_p47.pdf}
\vspace{-4mm}
\caption{Distributions of the Hamiltonian from 100 samples of a problem of density 0.2 obtained from (a)~DROID and (b)~the hardware. The X-axis has been normalized to the best Hamiltonian value obtained for the problem.}
\label{fig:distribution}
\vspace{-4mm}
\end{figure} 

Quantitatively, we use Earth Mover Distance (EMD)~\cite{Rubner98} to measure the similarity of the two distributions. The EMD quantifies the amount of work required to transform one distribution to the other and is robust to small changes to the bin size.  Intuitively, if we view one distribution as a mass of earth spread in space and the other as a collection of holes in the very same space, the EMD measures the least amount of work needed to fill the holes with earth; if the two distributions are identical, the work is zero~\cite{Rubner98}. A large EMD value suggests dissimilarity of distributions as more work is required to transform one distribution to the other, and therefore small values of EMD are desirable. The EMD between the distributions in Fig.~\ref{fig:distribution} is 0.009: this can be thought of as moving 9 of 100 samples over a distance of 10\% of the x-axis, which indicates that the distributions are close.

\input{tables/compare_distribution}

Table~\ref{tab:droid_hw_comparison} summarizes the EMD between the distributions across problems from the test set described earlier. The mean, minimum, maximum, and standard deviation of the EMD values for each graph density value correspond to a row in the table.  The small EMD values indicate that over the test set, the distribution of solutions from DROID resembles that from the hardware. 
