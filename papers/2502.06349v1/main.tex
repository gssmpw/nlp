
\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs} %

\usepackage{hyperref}


\usepackage{booktabs}       %
\usepackage{hyperref}
\usepackage{url}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{subcaption}
\usepackage{multirow}

\hypersetup{
    colorlinks=False,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}
\newcommand{\ED}{\mathbb{E}_{x \sim p}}
\newcommand{\EDhat}{\mathbb{E}_{x \sim \hat{p}}}
\newcommand{\EDhatk}{\mathbb{E}_{x \sim \hat{p}_k}}
\newcommand{\EDk}{\mathbb{E}_{x \sim p_k}}
\newcommand{\hD}{h_{p}^*}
\newcommand{\hDhat}{h_{\hat{p}}^*}
\newcommand{\hDhatk}{h_{\hat{p}_k}^*}
\newcommand{\hDk}{h_{p_k}^*}
\newcommand{\dH}{d_{\calH\Delta\calH}}
\newcommand{\D}{p}
\newcommand{\Dc}{p_k}



\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage[linesnumbered,lined,boxed,commentsnumbered,ruled,longend]{algorithm2e}


\usepackage{enumitem}
\newcommand{\calD}{{p}}
\newcommand{\calH}{{\mathcal{H}}}
\newcommand{\calX}{{\mathcal{X}}}
\newcommand{\calY}{{\mathcal{Y}}}
\newcommand{\calL}{{\mathcal{L}}}

\newcommand{\wj}[1]{\textcolor{black}{{#1}}}
\newcommand{\shr}[1]{\textcolor{blue}{{#1}}}
\newcommand{\sh}[1]{\textcolor{black}{{#1}}}
\newcommand{\hs}[1]{\textcolor{purple}{{#1}}}


\usepackage{hyperref}
\usepackage[numbers,square]{natbib}



\usepackage[arxiv]{icml2025}


\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage[capitalize,noabbrev]{cleveref}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{thm}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{cor}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\usepackage[textsize=tiny]{todonotes}


\icmltitlerunning{Provably Near-Optimal Federated Ensemble Distillation with Negligible Overhead}

\newcounter{tableeqn}[table]
\renewcommand{\thetableeqn}{\thetable.\arabic{tableeqn}}
\newcounter{tablesubeqn}[tableeqn]
\renewcommand{\thetablesubeqn}{\thetableeqn\alph{tablesubeqn}}

\begin{document}

\twocolumn[
\icmltitle{Provably Near-Optimal Federated Ensemble Distillation\\ with Negligible Overhead}



\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Won-Jun Jang}{KAIST}
\icmlauthor{Hyeon-Seo Park}{KAIST}
\icmlauthor{Si-Hyeon Lee}{KAIST}
\end{icmlauthorlist}

\icmlaffiliation{KAIST}{School of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, South Korea}

\icmlcorrespondingauthor{Si-Hyeon Lee}{sihyeon@kaist.ac.kr}

\icmlkeywords{Federated learning, ensemble distillation, data heterogeneity, generative adversarial network, ICML}

\vskip 0.3in
]



\printAffiliationsAndNotice{}  %

\begin{abstract}
{Federated ensemble distillation addresses client heterogeneity by generating pseudo-labels for an unlabeled server dataset based on client predictions and training the server model using the pseudo-labeled dataset. The unlabeled server dataset can either be pre-existing or generated through a data-free approach. The effectiveness of this approach critically depends on the method of assigning weights to client predictions when creating pseudo-labels, especially in highly heterogeneous settings. Inspired by theoretical results from GANs, we propose a provably near-optimal weighting method that leverages client discriminators trained with a server-distributed generator and local datasets. Our experiments on various image classification tasks demonstrate that the proposed method significantly outperforms baselines.  Furthermore, we show that the additional communication cost, client-side privacy leakage, and client-side computational overhead introduced by our method are negligible, both in scenarios with and without a pre-existing server dataset.}
\end{abstract}


\section{Introduction}
Federated learning (FL)~\citep{FedAVG} has received substantial attention in both industry and academia as a promising distributed learning approach. It enables numerous clients to collaboratively train a global model without sharing their private data. A major concern in deploying FL in practice is the severe data heterogeneity across clients. 
In the real world, it's probable that clients possess non-IID (identical and independently distributed) data distributions. It is known that the data heterogeneity results in unstable  convergence and performance degradation~\citep{li2019convergence, wang2020tackling,  li2021fedrs, kairouz2021advances, huang2023understanding, karimireddy2020scaffold}.


To address the data heterogeneity issue, various approaches have been taken, including regularizing the objectives of the client models \citep{karimireddy2020scaffold, FedProx, liang2019variance, FedGKD, mendieta2022local},  normalizing features or weights~\citep{dong2022spherefed, kim2023fedfn}, utilizing past round models~\citep{FedGKD, wang2023fedcda}, sharing feature information~\citep{dai2023tackling, yang2024fedfed, tang2024fedimpro}, introducing personalized layers~\citep{huang2023understanding}, \sh{and learning the average input-output relation of client models through ensemble distillation}~\citep{chang2019cronus, gong2021ensemble, FedHKT, sattler2020communication, FedDF,  Fed-ET, xing2022efficient, park2024overcoming, Da, tang2022virtual, zhang2022fedzkt,zhang2023towards}. \sh{In particular, the last approach, federated ensemble distillation, has recently gained significant attention for its effectiveness in mitigating data heterogeneity and for its advantage of being effectively applicable to heterogeneous client models. It requires an unlabeled dataset at the server, for which pseudo labels are created based on client predictions. By training on this pseudo-labeled dataset at the server, the server distills the knowledge from the clients. This additional dataset can be either public \citep{chang2019cronus, gong2021ensemble, FedHKT, sattler2020communication}, held only by the server due to its exceptional data collection capability~\citep{FedDF,  Fed-ET, xing2022efficient, park2024overcoming}, or obtained through a data-free approach~\citep{Da, tang2022virtual, zhang2022fedzkt,zhang2023towards}.}
Note that the performance of ensemble distillation depends on the quality of the pseudo-labels, which ultimately translates into a problem of appropriately assigning weights to client predictions for each data point, particularly in situations of data heterogeneity. 
In this research stream of federated ensemble distillation, early studies like FedDF~\citep{FedDF} applied uniform weighting. Subsequently, algorithms such as Fed-ET~\citep{Fed-ET}, FedHKT~\citep{FedHKT}, FedDS~\citep{park2024overcoming}, and DaFKD~\citep{Da} emerged, which utilize metrics like variance, entropy, and judgement of client discriminator as indicators of confidence in client predictions for weighting. However, analysis regarding the rationale behind optimal weighting remains scarce.


\begin{figure*}

\vskip -0.05in
    \centering
    \includegraphics[width=0.9\linewidth]{figs/fig1_.pdf}
    
\vskip -0.1in
    \caption{A toy example of decision boundaries of aggregated models.    Each point represents data, and its color represents the label. The background color represents the decision boundary of each model in the RGB channels. The oracle decision boundary, shown by the black lines, corresponds to the $x$-axis and $y$-axis. For aggregated models, we consider the parameter-averaged model~\citep{FedAVG}  and  ensemble-distilled models using uniform weighting~\citep{FedDF},  variance weighting~\citep{Fed-ET}, entropy weighting~\citep{FedHKT, park2024overcoming},  domain-aware  weighting~\citep{Da}, and ours. Detailed settings are provided in Appendix \ref{asec:exp1}.}
    \label{fig:1}
\vskip -0.18in
\end{figure*}







In this paper, we suggest a novel weighting method for federated ensemble distillation that outperforms previous methods (Fig. \ref{fig:1}), with theoretically justified optimality based on some results in generative adversarial networks (GANs)~\citep{GAN}. Our main contributions are summarized in the following: 


\begin{itemize}
\item  We propose \textbf{FedGO}: \textbf{F}ederated \textbf{E}nsemble \textbf{D}istillation with \textbf{G}AN-based \textbf{O}ptimality. Our algorithm incorporates a novel weighting method using the client discriminators that are trained at the clients based on the generator distributed from the server.

\item The optimality of our proposed weighting method is theoretically justified. We define an optimal model ensemble and show that a knowledge-distilled model from an optimal model ensemble achieves the optimal performance, within an inherent gap due to the difference between the spanned hypothesis class of ensemble model and the hypothesis class of a single model. Then, based on the theoretical result for vanilla GAN~\citep{GAN}%
, we show that our weighting method using client discriminators constitutes an optimal model ensemble. 

\item \wj{We experimentally demonstrate significant improvements of FedGO over existing research both in final performance and convergence speed on multiple image datasets (CIFAR-10/100, ImageNet100). In particular, we demonstrate performance across various scenarios, including cases where the server holds an unlabeled dataset different from the client datasets and where the server does not hold an unlabeled dataset, requiring data-free approaches. Furthermore, we provide a comprehensive analysis of communication cost, privacy leakage, and computational burden for the proposed method, {showing that client-side overheads are negligible  both in scenarios with and without a pre-existing server dataset.}}

\end{itemize}






\section{System Model and Related Work}\label{sec:2}
\paragraph{Federated Learning}
In federated learning, the goal is to cooperatively train a global model based on data distributed among \(K\) clients, by exchanging the models between a server and the clients. 


We focus on classification tasks in this paper. Let  \(\mathcal{X}\) denote the data domain and $y$ denote the labeling function that outputs the label of the data \(x \in \mathcal{X}\). A model \(f(\cdot;\theta)\) is parameterized by \(\theta\in\Theta\) where $\Theta$ is the set of model parameters  and \(\mathcal{H}=\{h|h(\cdot)=f(\cdot;\theta), \theta\in\Theta\}\) denotes 
the class of parameterized models.  For a distribution $q$ on $\mathcal{X}$, $h_q^*$  denotes the expected loss minimizer on  $q$, i.e., \(h_{q}^*\triangleq\arg \min_{h\in\mathcal{H}}\mathcal{L}_{q}(h)\), where $\mathcal{L}_{q}(h)= \mathbf{E}_{q}[l(h(x),y(x))]$ and $l$ is the loss function. 
Client $k$ possesses a (labeled) dataset \(S_k\) of $n_k$ data points, sampled over \(\mathcal{X}\)  i.i.d. according to  distribution \(p_k\). Then \(p = \sum_{k=1}^K\pi_k\cdot p_k\), where \(\pi_k = \frac{n_k}{\sum_{k'=1}^K n_{k'}}\), is the average of client data distribution. The objective of federated learning is given as follows:
\begin{align}
    &\min_{h\in\calH}\mathcal{L}_p(h)=\min_{h\in\calH}\mathbf{E}_{p}[l(h(x),y(x))]\label{eqn:1}\\
    &=\min_{h\in\calH}\sum_{k=1}^K\pi_k\cdot\mathbf{E}_{p_k}[l(h(x),y(x))]=\min_{h\in\calH}\sum_{k=1}^K\pi_k\cdot\mathcal{L}_{p_k}(h). \label{eqn:eq2}
\end{align}

\begin{algorithm*}[!htb]
\caption{Federated learning with $K$ clients for $T$ communication rounds, with ensemble distillation exploiting unlabeled dataset on the server. Client $k$ possesses $n_k$ data points, and the fraction $C$ of clients participate in each communication round. $f(\cdot;\theta)$ stands for the model with parameter $\theta$, and $\mu$ stands for the step size.}
\label{alg:s}
\begin{algorithmic}
\STATE {\bfseries Require:} Client labeled dataset $\{S_k\}_{k=1}^K$, server unlabeled dataset $U$
 \STATE Initialize server model {$f(\cdot; \theta_s^0)$} with parameter $\theta_s^0$
    \FOR {communication round $t=1$ to $T$}
    \STATE $A^t\gets$ sample $\lfloor{C\cdot K}\rfloor$ clients
    \makeatletter
    \newcommand{\PARFOR}[1]{\FOR{\textbf{parallel} #1}} %
    \makeatother
    \PARFOR {client $k\in A^t$}
    \STATE $\theta_k^t\gets ClientUpdate(\theta_s^{t-1}, S_k)$ \COMMENT{Gradient update $\theta_s^{t-1}$ with $S_k$}
    \ENDFOR
    \STATE $\theta_s^t\gets\sum_{k\in A^t}\frac{n_k}{\sum_{i\in A^t}n_{i}}\cdot\theta_k^t$
    \FOR {server train epoch $e=1$ to $E_s$}
    \FOR {unlabeled minibatch $u\in U$}
    \STATE $\tilde y(u)\gets \sigma(\sum_{k\in A^t}w_k(u)\cdot f(u;\theta_k^t))$ \COMMENT{Label as a weighted sum of client predictions}
    \STATE $\theta_s^t\gets\theta_s^t-\mu\cdot\nabla_{\theta_s^t}\text{KL}(\tilde y(u), \sigma(f(u;\theta_s^t)))$ \COMMENT{Ensemble distillation} \label{algeq:11}
    \ENDFOR
    \ENDFOR
    \ENDFOR
\STATE \Return {$f(\cdot; \theta_s^T)$}
\end{algorithmic}
\end{algorithm*}

In each communication round \(t\), a subset \(A^t\) of clients downloads the current server model and trains it based on $S_k$ with the objective of minimizing $\mathcal{L}_{p_k}(h)$. Then it sends the trained model to the server. The server aggregates these client models to update the server model. The aforementioned procedure is repeated at the next communication round. For the aggregation of client models at the server, the FedAVG algorithm~\citep{FedAVG} constructs the server model with parameter $\theta_s^{t}$ for round \(t\) as the average of model parameters ${\theta_k^t}$ for \(k\in A^t\) received in round \(t\) (line 7 of Algorithm \ref{alg:s}).  
When the client data distributions are homogeneous, each \(p_k\) is same as \(p\) and hence \(\mathcal{L}_{p_k}\) becomes same as \(\mathcal{L}_p\).
However, when the client data distributions are heterogeneous, \(\mathcal{L}_{p_k}\) and \(\mathcal{L}_p\) are not same, leading to a significant degradation in the convergence rate of FedAVG to the global optimum~\citep{li2019convergence}. 




In the following, we introduce federated ensemble distillation using  an unlabeled dataset on the server  to address client data heterogeneity.


\vskip -0.2in

\paragraph{Federated Ensemble Distillation}
To address client data heterogeneity, there has been a line of research on federated ensemble distillation  using an unlabeled dataset on the server. \sh{This unlabeled dataset may either be available from the outset~\citep{FedDF, Fed-ET, FedHKT, park2024overcoming} or produced  through a generator trained as a part of FL by taking a data-free approach~\citep{rasouli2020fedgan, guerraoui2020fegan, li2022ifl, wang2023fedmed, fan2020federated, behera2022fedsyn, hardy2019md, xiong2023federated, zhang2021dance, zhang2023novel, Da, zhang2022fedzkt, zhang2023towards}.}  With the unlabeled dataset, the server model undergoes additional training to learn the average input-output relationship of client models. 


\begin{table*}[!htb]\centering
\vskip -0.15in
\caption{Generalization bound of the ensemble model of ensemble distillation algorithms. For any \(\delta \in (0, 1)\) and $\sigma>0$, the generalization bounds hold with probability $1-\delta$. Here, \(\hat{p}_k\) is the empirical distribution by sampling $n_k$ data points i.i.d. according to $p_k$, 
\(d_{\mathcal{H}\triangle\mathcal{H}}\) denotes the discrepancy between two distributions, \(\lambda_k=\inf_h \mathcal{L}_{p_k}(h)+\mathcal{L}_{p}(h)\), and \(\tau_{\calH}\) is growth function bounded by polynomial of the VC-dimension of $\calH$.}
\vskip 0.1in
\def\arraystretch{0.9}
\scalebox{0.8}{
\begin{tabular}{clr}\toprule
Algorithm & \multicolumn{2}{c}{Generalization Bound}\\ \midrule
FedDF~\citep{FedDF}, Fed-ET~\citep{Fed-ET} & $\mathcal{L}_{p}(\sum_{k=1}^K \alpha_k h_{\hat{p}_k}^*)\leq \sum_{k=1}^K \alpha_k\cdot \left[\mathcal{L}_{{\hat{p}_k}}(h_{\hat{p}_k}^*)+\frac{1}{2}d_{\mathcal{H}\triangle\mathcal{H}}(p_k, p)+\lambda_k+O\left(\frac{\log(\delta^{-1})}{\sqrt{n_k}}\right)\right]$ &
\refstepcounter{tableeqn} (\thetableeqn)\label{eq:df_gen_bound}\\
DaFKD~\citep{Da} & $\mathcal{L}_{\calD}(\sum_{k=1}^Kw_k\cdot \hDhatk) \leq (K+1)\cdot\sum_{k=1}^K\frac{1}{K}\cdot\left[\mathcal{L}_{\hat\calD_k}(\hDhatk)+\sqrt{\frac{\sigma^2\log\frac{2K}{\delta}}{2n_k}}\right]$ &
\refstepcounter{tableeqn} (\thetableeqn)\label{eq:da_gen_bound} \\
\textbf{Ours, Theorem} \ref{athm:c1} &$ \mathcal{L}_{\calD}(\sum_{k=1}^Kw_k^*\cdot \hDhatk)\leq \sum_{k=1}^K\pi_k \cdot\left[\calL_{\hat\calD_k}(\hDhatk)+\frac{4+\sqrt{\log(\tau_\calH(2n_k))}}{(\delta/K)\sqrt{2n_k}}\right]$ &
\refstepcounter{tableeqn}(\thetableeqn)\label{eq:our_gen_bound} \\
\bottomrule\end{tabular}
}
\label{tab:gen_bound}
\vskip -0.18in
\end{table*}

Algorithm~\ref{alg:s} describes this federated ensemble distillation, when the client and server model structures are the same. Here  $\sigma$ represents the softmax function, and KL denotes the Kullback-Leibler divergence. If the model output already includes the softmax activation, then the softmax function is omitted in lines 10 and 11. After averaging client model parameters in line 7, the performance of the server model depends on the quality of the pseudo-labels, as the server model undergoes additional training with those pseudo-labels. Moreover, the quality of pseudo-labels \(\tilde{y}(\cdot)\) relies on designing the weighting function \(w_k(\cdot)\), which determines the weighting of client $k$'s output. Therefore, designing a better-performing ensemble distillation during the server update ultimately boils down to designing a better-performing weighting function.





\wj{For the weighting function, FedDF~\citep{FedDF} uses uniform weights for each client, i.e., \(w_k(x) = \frac{1}{|A^t|}\) for all \(k\) in \(A^t\). Subsequently, algorithms assigning higher weights to the outputs of more confident clients have been proposed. In Fed-ET~\citep{Fed-ET}, higher weights are assigned to models with larger output logit variance, i.e., \(w_k(x) = \frac{{\text{Var}(f(x;\theta_k^t))}}{{\sum_{i \in A^t} \text{Var}(f(x;\theta_i^t))}}\). \sh{
FedHKT~\citep{FedHKT} and FedDS~\citep{park2024overcoming} allocate higher weights  to models with  smaller output softmax entropy, i.e., \(w_k(x) = \frac{{\exp(-\text{Entropy}(\sigma(f(x;\theta_k^t)))/\tau)}}{{\sum_{i \in A^t} \exp(-\text{Entropy}(\sigma(f(x;\theta_i^t)))/\tau)}}\)}, \sh{where $\tau$ is the temperature parameter.}
In DaFKD~\citep{Da}, while training a global generator and client discriminators at each round, ensemble distillation is performed on unlabeled dataset generated by the global generator by assigning higher weights to models with larger discriminator outputs, i.e., \(w_k(x) = \frac{D_k^t(x)}{\sum_{i \in A^t} D_i^t(x)}\) where $D_k^t$ is the client $k$'s discriminator against the global generator at round $t$.}




\wj{For theoretical aspects, generalization bounds of an ensemble model are presented in Table~\ref{tab:gen_bound} for a binary classification task under $\ell_1$ loss. %
For \(n_k=\frac{n}{K}\) for all $k$, a generalization bound for an ensemble model with fixed weights \(\alpha_1, ..., \alpha_K\) with $\textstyle \sum_k \alpha_k=1$ is given as~\eqref{eq:df_gen_bound}~\citep{FedDF, Fed-ET}, with weight function $w_k(x) = \frac{D_k^t(x)}{\sum_{i \in A^t} D_i^t(x))}$ is given as~\eqref{eq:da_gen_bound}~\citep{Da}, and with our weight function $w_k^*(x)$ described in Section~\ref{sec:3.1} is given as~\eqref{eq:our_gen_bound}.}


\wj{These bounds relate the loss of an ensemble model (the LHS of \eqref{eq:df_gen_bound}, \eqref{eq:da_gen_bound} and \eqref{eq:our_gen_bound}) to the average empirical loss of client models (the first term in the RHS of \eqref{eq:df_gen_bound}, \eqref{eq:da_gen_bound} and \eqref{eq:our_gen_bound}). %
Note that the bound \eqref{eq:df_gen_bound} assumes a fixed weight per client irrelevant to data points, hence there is a lack of analysis for assigning varying weights per data point. Furthermore, \eqref{eq:df_gen_bound} provides a generalization bound that becomes loose as it scales with the average distribution discrepancy between $p$ and $p_k$ across clients. The bound \eqref{eq:da_gen_bound} assumes a specific weighting function of each data point, but it is too loose because it becomes vacuous as $K$ increases. In contrast, \eqref{eq:our_gen_bound}, which is derived in this paper, provides a refined generalization bound for weighting functions. To the best of our knowledge, this is the tightest bound currently provided, as detailed in Theorem~\ref{athm:c1}.}%

Moreover, in federated ensemble distillation, our ultimate interest is in the loss of the server model, knowledge-distilled from the ensemble model. Note that the hypothesis class of ensemble models is in general larger than that of single models, and hence there exists an inherent gap between the losses of an ensemble model and the knowledge-distilled model. However, the above bounds do not provide an analysis on this gap. 

In Section \ref{sec:3.1}, we define an optimal model ensemble and show that the server model knowledge-distilled from an optimal model ensemble achieves the optimal loss within the gap arising from the distillation step, which depends on the inherent difference between the hypothesis classes of the server model and the ensemble model, along with the distribution discrepancy between the average client distribution $p$ and the distribution $p_s$ of unlabeled data on the server.





\paragraph{Generative Adversarial Network} The generative adversarial networks (GANs) are a class of powerful generative models composed of a generator and a discriminator~\citep{GAN, WGAN-GP, DCGAN, InfoGAN, CycleGAN, StarGAN, StyleGAN}. They are trained in an unsupervised learning manner, requiring no class labels. The discriminator aims to distinguish between real images from the dataset and fake images generated by the generator. Meanwhile, the generator strives to produce images that can fool the discriminator.%

In \citet{GAN}, the authors showed  that the output of an optimal discriminator against a fixed generator can be expressed in terms of distributions of real and fake images. %
\begin{theorem}(\citealp[Proposition 1.]{GAN}) For a fixed generator \(G\), let \(p_g\) and \(p_{\text{data}}\) denote the density functions of the generated distribution by $G$ and the real data distribution, respectively. Then the output of an optimal discriminator \(D\) for input data \(x \) is given as follows:\label{thm:gan}
\begin{align}
    D(x)=\frac{p_{\text{data}}(x)}{p_{\text{data}}(x)+p_{g}(x)}. \label{gan}
\end{align}
\end{theorem}

Using the above result, we develop a method of assigning weights to client predictions   in Section \ref{sec:3}.  

\section{Proposed Method}\label{sec:3}
In this section, we propose a weighting method for  federated ensemble distillation. First, theoretical results are presented in Section \ref{sec:3.1}. In Section \ref{sec:3.1.1}, we define an optimal model ensemble and give a bound on the loss of the server model knowledge-distilled from an optimal model ensemble. 
Next, in Section \ref{subsubsection:3.1.2}, we propose a client weighting method to construct an optimal model ensemble, based on Theorem \ref{thm:gan}.  In Section \ref{sec:3.2}, we introduce our FedGO algorithm, leveraging the theoretical results.  We note that a generalization bound of an ensemble model with our proposed weighting method comparable with \eqref{eq:df_gen_bound} is provided in Appendix \ref{sec:general}.%

\subsection{Theoretical Results} \label{sec:3.1} 
\subsubsection{Ensemble Distillation with Optimal Model Ensemble}  \label{sec:3.1.1}
We first define an optimal model ensemble.
\begin{definition} \label{def:opt_ens}
    For $K$ clients, the ensemble of their models and weight functions \(\{ (h_k, w_k) \}_{k=1}^K\) is said to be an optimal model ensemble if the following holds:
    \begin{align}
        \mathcal{L}_p \left( \sum_{k=1}^K w_k \cdot h_k \right) &= \mathbf{E}_{p}\left[l\left(\sum_{k=1}^K w_k(x)\cdot h_k(x),y(x)\right)\right] \nonumber\\
        &\leq \min_{h\in\mathcal{H}}\mathcal{L}_{p}(h)=\mathcal{L}_p (h_p^*).
    \end{align}
\end{definition}

We remind that the objective of federated learning is to train a model that minimizes the expected loss over the average client distribution $p$ as shown in \eqref{eqn:1}. If  \(\{ (h_k, w_k) \}_{k=1}^K\) is an optimal model ensemble, its expected loss over $p$ is less than or equal to the minimum expected loss over $p$ achievable by a single model, i.e., $\min_{h\in\mathcal{H}}\mathcal{L}_{p}(h)$. 

However, we cannot guarantee that a knowledge-distilled model from an optimal model ensemble would be optimal, i.e., achieve $\min_{h\in\mathcal{H}}\mathcal{L}_{p}(h)$, due to the following two reasons: 1) the ensemble model $\sum_{k=1}^K w_k \cdot h_k$ may lie outside the hypothesis class $\mathcal{H}$ of a single model and 2) the distribution used for knowledge distillation (the distribution $p_s$ of unlabeled data on the server) can be different from $p$.  
In the following theorem, we present a bound on the expected loss over $p$ of a single model by taking into account these factors. For two hypotheses \(h, h' \in \mathcal{H}\) and a distribution \(q\) over \(\mathcal{X}\), the expected difference between \(h\) and \(h'\) over \(q\), denoted \(\mathcal{L}_{q}(h,h')\), is defined as follows:
\begin{align}
    \mathcal{L}_{q}(h,h')\triangleq\mathbf{E}_{q}\left[\left(l(h(x), h'(x)\right)\right].
\end{align}










\begin{theorem}\label{thm:1}
    (Informal) Let $\bar{\calH}\triangleq \{\sum_{k=1}^Kw_k\cdot h_k|h_j\in\calH, w_j:\calX\rightarrow[0, 1], \sum_{k=1}^Kw_k(x)=1,   j=1,\cdots, K, x\in \calX \}$ be the spanned hypothesis class, $\calD_s$ be a distribution on $\calX$, and $\{ (h_k, w_k) \}_{k=1}^K$ be an ensemble of client models and weight functions. Then for any $h\in \mathcal{H}$, the following holds:  
    \begin{align}
        \mathcal{L}_{\calD}(h)&\leq \mathcal{L}_{\calD}(\sum_{k=1}^Kw_k\cdot h_{k})+\mathcal{L}_{{\calD}_s}(h, \sum_{k=1}^Kw_k\cdot h_{k})\nonumber\\
        &+\frac{1}{2}d_{\bar{\calH}\triangle\bar{\calH}}(\calD, \calD_s).
    \end{align}
\end{theorem}
The formal statement and the proof of the above theorem are in Appendix \ref{asec:proof2}. Let us provide a brief sketch of the proof. Utilizing the results from~\citet{ben2006analysis} and~\citet{crammer2008learning}, we have \(\mathcal{L}_{\calD}(h)\leq \mathcal{L}_{\calD}(\sum_{k=1}^Kw_k\cdot h_{k})+\mathcal{L}_{\calD}(h, \sum_{k=1}^Kw_k\cdot h_{k})\). Then from the triangular inequality, we obtain \(\mathcal{L}_{\calD}(h, \sum_{k=1}^Kw_k\cdot h_{k})\leq \mathcal{L}_{{\calD}_s}(h, \sum_{k=1}^Kw_k\cdot h_{k})+|\mathcal{L}_{\calD}(h, \sum_{k=1}^Kw_k\cdot h_{k})-\mathcal{L}_{{\calD}_s}(h, \sum_{k=1}^Kw_k\cdot h_{k})|\). Now the desired inequality is obtained by applying the results from~\citealp[Lemma 3]{Ben-David}. 

From Theorem \ref{thm:1}, we can ascertain the following. The loss of the server model \(h\) is bounded by the sum of three losses:
1) expected loss of the ensemble model over \(p\),
2) difference between \(h\) and \(\sum_{k=1}^K w_k \cdot h_k\) over \(p_s\), and 
3) the distribution discrepancy between \(p\) and \(p_s\).

The following corollary is a direct consequence of Theorem \ref{thm:1} and Definition \ref{def:opt_ens}. %

\begin{cor}\label{cor:1}
    (Informal) For an optimal model ensemble  $\{ (h_k, w_k) \}_{k=1}^K$, the following holds for any $h\in \mathcal{H}$:
        \begin{align}
        \mathcal{L}_{p}(h_{p}^*) \leq\mathcal{L}_{\calD}(h)&\leq \mathcal{L}_{\calD}(h_{\calD}^*)+\mathcal{L}_{p_s}(h, \sum_{k=1}^Kw_k\cdot h_k)\nonumber\\
        &+\frac{1}{2}d_{\bar{\calH}\triangle\bar{\calH}}(\calD, \calD_s).
    \end{align}
\end{cor}
Corollary \ref{cor:1} demonstrates the powerfulness of an optimal model ensemble. If an optimal model ensemble is constituted, the difference between the expected loss of the server model over \(p\) and the minimum expected loss \sh{$\mathcal{L}_{p}(h_{p}^*)=\min_{h\in\mathcal{H}}\mathcal{L}_{p}(h)$} is bounded by the distillation loss, which depends on the inherent difference between the hypothesis class $\calH$ and the spanned hypothesis class $\bar{\calH}$, along with the distribution discrepancy between $p$ and $p_s$. %

In the next subsection, we propose a weighting method to constitute an optimal model ensemble. 



\subsubsection{Client Weighting for Optimal Model Ensemble}\label{subsubsection:3.1.2}

Let us assume that the server has models \(\{h_{p_k}^*\}_{k=1}^K\) trained by clients based on their respective data distributions \(\{p_k\}_{k=1}^K\). In the following theorem, we present  weight functions \(\{w_k\}_{k=1}^K\) such that the ensemble of \(\{h_{p_k}^*, w_k\}_{k=1}^K\) constitutes an optimal model ensemble. 
\begin{theorem}\label{thm:2}
    Let the loss function \(l\) be convex. Define the client weight functions \(\{w_k^*\}_{k=1}^K\) as follows:
    \begin{align}
        w_k^*(x)\triangleq\frac{n_k \cdot p_k(x)}{\sum_{i=1}^K n_i \cdot p_i(x)}=\frac{\pi_k \cdot p_k(x)}{\sum_{i=1}^K \pi_i \cdot p_i(x)}.
    \end{align}
Then, the ensemble  \(\{h_{p_k}^*, w_k^*\}_{k=1}^K\) is an optimal model ensemble\sh{, i.e., $\textstyle\mathcal{L}_p \left( \sum_{k} w_k^* \cdot h_{p_k}^* \right)  \leq \mathcal{L}_p (h_p^*)$}.
\end{theorem}
Theorem \ref{thm:2} follows from some manipulations based on the convexity of the loss and the definitions of \(w_k^*\)'s and \( h_{\Dc}^*\)'s, and its full proof is provided in Appendix \ref{asec:proof3}.




\begin{table*}[!htb]

\vskip -0.15in
  \caption{\sh{A comprehensive analysis of additional communication burden, privacy leakage, and computational burden caused by the proposed weighting method, compared to FedAVG.}}\label{tab:GOsetting}
  \centering\setlength\tabcolsep{0pt}
  
\vskip 0.1in
  \scalebox{0.82}{
    \begin{tabular*}{1.2\linewidth}{@{\extracolsep{\fill}} ccccccc }
    \toprule 
    \multirow{2.4}{*} {$\begin{array}{c}
         \text{Extra} \\
         \text{Server Dataset}
    \end{array}$}
    
    &\multirow{2.4}{*} {$\begin{array}{c}
         \text{Generator} \\
         \text{Preparation}
    \end{array}$}
    & \multirow{2.4}{*} {$\begin{array}{c}
         \text{Distillation} \\
         \text{Dataset}
    \end{array}$}
    &\multirow{2.4}{*} {$\begin{array}{c}
         \text{Communication} \\
         \text{Cost}
    \end{array}$}
    & \multicolumn{2}{c}{Privacy Leakage}
    &\multirow{2.4}{*} {$\begin{array}{c}
         \text{Client-side} \\
         \text{Computational Burden}
    \end{array}$}\\

    \cmidrule(r){5-6}
    &&&&Server-side&Client-side\\


    \midrule
    S1 &G1 &D1 &Negligible &Non-negligible &Negligible &Negligible\\
    S1 &G2 &D1 &Negligible &Non-negligible &Negligible &Negligible\\
    S2 &G2 &D2 &Negligible &- &Negligible &Negligible\\
    S2 &G3 &{D2} &{Negligible} &- &{Negligible} &{Negligible} \\ \bottomrule
  \end{tabular*}
  }
  
\vskip -0.18in
\end{table*}

Theorem \ref{thm:2} demonstrates that for data point $x$, weighting according to each client's proportion of having $x$ constitutes an optimal model ensemble. However, even if weighting each client according to Theorem \ref{thm:2} constitues an optimal model ensemble, it is not feasible without knowing the data distribution \(p_k\) of each client. Theorem \ref{thm:3} addresses this issue based on Theorem \ref{thm:gan}  and provides hints on how to implement an optimal model ensemble. %

\begin{definition}
     (Odds): For \(\phi \in (0, 1)\), its odds value  \(\Phi\) is defined as \(\Phi(\phi)=\frac{\phi}{1-\phi}\).
\end{definition}

\begin{theorem}\label{thm:3}
For a fixed generator \(G\) with generating distribution $p_g$, let \(D_k\) be an optimal discriminator for  generator $G$ and client $k$'s distribution $p_k$. Assume that \(D_k\) outputs a value over \((0, 1)\) using a sigmoid activation function, and let 
\(\Phi_k(x) \triangleq \Phi(D_k(x))\). Then, for \(x \in supp(p_g)\), the following holds:
    \begin{align}
        \frac{n_k \cdot \Phi_k(x)}{\sum_{i=1}^K n_i \cdot \Phi_i(x)}=\frac{\pi_k \cdot p_k(x)}{\sum_{i=1}^K \pi_i \cdot p_i(x)}= w_k^*(x). \label{gan_weight}
    \end{align}
\end{theorem}


Theorem \ref{thm:3} is a direct consequence of Theorem \ref{thm:gan}, because \( \Phi_k(x) = \frac{p_k(x)}{p_g(x)} \) from Theorem \ref{thm:gan}. 
Theorem~\ref{thm:3} indicates that if the server once receives the optimal discriminators $\{D_k\}_{k=1}^K$ trained by the clients, it can use those discriminators to calculate the weights for optimal model ensemble. Note that the generator $G$ only needs to generate a wide distribution capable of producing sufficiently diverse samples. Therefore, one can use an off-the-shelf  generator pretrained on a large dataset. 



\subsection{Proposed Algorithm: FedGO} \label{sec:3.2}


By leveraging the theoretical results in Section \ref{sec:3.1}, we propose FedGO  that constitutes an optimal model ensemble and performs knowledge distillation. \sh{The main technical novelty of FedGO lies in implementing the optimal weighting function $w_k^*$ using client discriminators, which is a versatile technique that can be integrated to both the following scenarios with/without extra server dataset: (S1) the server holds an extra unlabeled dataset; (S2) the server holds no unlabeled dataset, thus a data-free approach is needed.}

For completeness, let us describe how the FedGO algorithm can be adapted depending on the cases (S1) and (S2). FedGO largely consists of two stages: pre-FL and main-FL. In the pre-FL stage, the server and the clients exchange the generator and the discriminators. {First, the server obtains a generator through one of the following three methods, and distributes the generator to the clients: (G1) train a generator with an unlabeled dataset on the server, which is possible under (S1); (G2) load an off-the-shelf generator pretrained on a sufficiently rich dataset; or (G3) train a generator through an FL approach, e.g., using FedGAN~\citep{rasouli2020fedgan}.}

After receiving the generator, each client trains its own discriminator based on its dataset and sends the discriminator to the server. %

The main-FL stage operates according to Algorithm \ref{alg:s}, except that 
the server assigns weights for pseudo-labeling according to \eqref{gan_weight} using the client discriminators. \sh{For the server unlabeled dataset $U$ used for distillation, which we call distillation dataset, we consider the following cases:} (D1) use the same dataset held by the server, which is pos-
sible under (S1); (D2) {produce a distillation dataset using the generator.}

\sh{A comprehensive analysis of additional communication cost, privacy leakage, and computational burden according to the methods for obtaining the generator and distillation set is provided in Table~\ref{tab:GOsetting}, which shows the trade-off among the methods. In particular, an extra dataset at the server makes the communication cost and the client-side privacy  and computational burden negligible, at the expense of server-side privacy leakage. In the absence of server dataset, the use of an off-the-shelf generator makes all the burdens negligible, but it can be challenging to secure an off-the-shelf generator whose generation distribution is similar to the client data distribution.} Lastly, the data-free approach (G3)+(D2) does not require an extra server dataset or an external generator. \wj{While it requires extra communication to prepare the generator, the additional communication burden, privacy leakage and computational burden on the client side remains negligible due to the relatively small number of communication rounds involved, unlike existing data-free methods such as DaFKD~\citep{Da}, which involve both generator and model exchanges in every communication round. 
By decoupling generator preparation from model training, FedGO with (G3)+(D2) provides a lightweight solution.}


A detailed description of FedGO and explanation for Table ~\ref{tab:GOsetting} can be found in Appendices~\ref{asec:alg_desc} and~\ref{asec:privacy}, respectively.



\begin{table*}[!htb]

\vskip -0.15in
  \caption{\sh{Server test accuracy (\%)} of our FedGO and  baselines on three image datasets at the 100-th communication round. A smaller $\alpha$ indicates higher heterogeneity. }\label{tab:1}
  
  \vskip 0.1in
  \def\arraystretch{0.92}
  \centering\setlength\tabcolsep{0pt}
    \begin{tabular*}{\linewidth}{@{\extracolsep{\fill}} ccccccc }
    \toprule
    &\multicolumn{2}{c}{CIFAR-10} &\multicolumn{2}{c}{CIFAR-100} &\multicolumn{2}{c}{ImageNet100}                   \\
    \cmidrule(r){2-3}\cmidrule(r){4-5}\cmidrule(r){6-7}
     & $\alpha=0.1$& $\alpha=0.05$ & $\alpha=0.1$& $\alpha=0.05$& $\alpha=0.1$& $\alpha=0.05$\\
    \midrule
    Central Training & \multicolumn{2}{c}{85.33$\pm0.25$} &\multicolumn{2}{c}{51.72$\pm$0.65} &\multicolumn{2}{c}{43.20$\pm$1.00}\\
    
    FedAVG & 58.65$\pm$5.75 & 46.61$\pm$8.54 & 38.93$\pm$0.74 & 36.66$\pm$0.97 & 29.44$\pm$0.41 & 27.58$\pm$0.88\\
    {FedProx} & {64.69$\pm$2.15} & {55.56$\pm$9.86} & {38.21$\pm$0.95}& {34.44$\pm$1.26} & {29.96$\pm$0.66} & {26.99$\pm$0.97}\\ 
    {SCAFFOLD} & {61.20$\pm$3.98} & {50.10$\pm$10.00} & {38.15$\pm$0.80}& {36.14$\pm$1.06} & {29.13$\pm$0.79} & {27.08$\pm$0.69}\\ 
    {FedDisco} & {56.78$\pm$7.22} & {48.08$\pm$8.35} & {38.81$\pm$1.02}& {36.86$\pm$0.88} & {29.69$\pm$0.66} & {27.54$\pm$0.51}\\ 
    FedDF & 71.56$\pm$5.09 & 59.53$\pm$9.88 & 42.74$\pm$1.22 & 37.18$\pm$1.03 & 33.48$\pm$1.00 & 30.94$\pm$1.60\\
    FedGKD$^+$ & 72.59$\pm$4.10 & 59.96$\pm$8.60 & 43.35$\pm$1.14 & 40.47$\pm$1.00 & 34.10$\pm$0.67 & 31.42$\pm$0.93\\
    {DaFKD} & {71.52$\pm$5.56} & {67.51$\pm$10.77} & {44.12$\pm$2.25} & {39.50$\pm$0.85} & {33.34$\pm$0.69} & {31.59$\pm$1.46}\\
    \textbf{FedGO (ours)} & \textbf{79.62}$\pm$4.36 & \textbf{72.35}$\pm$9.01 & \textbf{44.66}$\pm$1.27 & \textbf{41.04}$\pm$0.99 & \textbf{34.20}$\pm$0.71 & \textbf{31.70}$\pm$1.55\\    
    \bottomrule
  \end{tabular*}
  
\vskip -0.18in
\end{table*}


\begin{table*}[!htb]
  \caption{The number of communication rounds to achieve a test accuracy of at least \( \text{Acc}_{\text{target}}\).}\label{tab:2}
  \def\arraystretch{0.92}
  
  \vskip 0.1in
  \centering\setlength\tabcolsep{0pt}
    \begin{tabular*}{\linewidth}{@{\extracolsep{\fill}} ccccccc }
    \toprule
    &\multicolumn{2}{c}{CIFAR-10} &\multicolumn{2}{c}{CIFAR-100} &\multicolumn{2}{c}{ImageNet100}                   \\
    \cmidrule(r){2-3}\cmidrule(r){4-5}\cmidrule(r){6-7}
     & $\alpha=0.1$& $\alpha=0.05$ & $\alpha=0.1$& $\alpha=0.05$& $\alpha=0.1$& $\alpha=0.05$\\    
    $\text{Acc}_\text{target}$ & 60\%& 45\% & 35\%& 35\% & 25\% & 25\% \\
    \midrule
    FedAVG & 65.6$\pm$22.8 & 47.4$\pm$14.9 & 42.4$\pm$12.8 & 76.0$\pm$8.5 & 22.2$\pm$3.1 & 43.8$\pm$7.3\\
    {FedProx} & {38.0$\pm$9.1} & {33.0$\pm$12.7} & 45.6$\pm$5.9 & 86.0$\pm$11.8 & 20.8$\pm$3.8& 47.6$\pm$5.8\\ 
    {SCAFFOLD} & {53.2$\pm$14.6} & {45.8$\pm$19.7} & 47.4$\pm$4.2 & 76.2$\pm$9.0 & 22.2$\pm$2.2& 47.8$\pm$8.4\\ 
    {FedDisco} & {63.4$\pm$27.8} & {44.0$\pm$11.6} & 44.6$\pm$4.5 & 76.2$\pm$9.3 & 21.8$\pm$3.2& 51.2$\pm$5.6\\ 
    FedDF & 5.4$\pm$1.4 & 6.0$\pm$1.5 & 15.2$\pm$5.7 & 78.0$\pm$23.8 & 9.4$\pm$1.9 & 22.0$\pm$5.7\\
    FedGKD$^+$ & 5.6$\pm$1.6 & 4.2$\pm$1.2 & 12.6$\pm$3.3 & 39.8$\pm$19.6 & 9.0$\pm$1.4& 14.8$\pm$2.5\\
    {DaFKD} & {5.6$\pm$1.4} & {3.0$\pm$0.6} & 13.4$\pm$5.4 & 50.2$\pm$27.9 & 9.0$\pm$2.8 & 15.6$\pm$4.1\\
    \textbf{FedGO (ours)} & \textbf{3.0}$\pm$0.9 & \textbf{2.0}$\pm$0.6 & \textbf{11.0}$\pm$2.1 & \textbf{25.4}$\pm$9.1 & \textbf{8.4}$\pm$1.0 & \textbf{12.6}$\pm$1.6\\    \bottomrule
  \end{tabular*}
  
\vskip -0.18in
\end{table*}




\section{Experimental Results} \label{sec:4}

In this section, we present the experimental results. All experimental results were obtained using five different random seeds, and the reported results are presented as the mean \(\pm\) standard deviation.

\subsection{Experimental Setting}\label{sec:expsetting}
\paragraph{Datasets and FL Setup} We employed datasets CIFAR-10/100~\citep{krizhevsky2009learning} (MIT license) and downsampled ImageNet100~\citep{imagenet100, chrabaszcz2017downsampled}. Unless specified otherwise, the entire client dataset corresponds to half of the specified client dataset (half for each class), and each client dataset is sampled from the entire client dataset according to Dirichlet($\alpha$), akin to setups in ~\citet{FedDF, Fed-ET}. $\alpha$ is set to 0.1 and 0.05 to represent data-heterogeneous scenarios. 
 The server dataset corresponds to half of the specified server dataset  (half for each class) without labels. If not otherwise specified, the  server dataset and the client datasets partition the same dataset disjointly. \sh{We considered 20 and 100 clients (20 clients if not specified otherwise), assuming that 40\% of the clients participate in each communication round.}  


\paragraph{Models and Baselines} For architecture, we employed ResNet-18~\citep{ResNet} with batch normalization layers~\citep{ioffe2015batch}. 
For baselines, we considered the vanilla FedAVG~\citep{FedAVG}, FedProx~\cite{FedProx}, SCAFFOLD~\cite{karimireddy2020scaffold} and FedDisco~\cite{FedDisco} that do not \sh{perform ensemble distillation}%
, FedDF~\citep{FedDF}, FedGKD$^+$~\citep{FedGKD} \sh{and DaFKD~\citep{Da}} that incorporate ensemble distillation%
. For comparison with other weighting methods, %
we considered the variance-based weighting method of \citet{Fed-ET}, the entropy-based methods of \citet{FedHKT} and \citet{park2024overcoming}, and the domain-aware method of \citet{Da}, described in Section \ref{sec:2}. %
As an upper bound of the performance, we also compared with central training that trains the server model directly using the entire client dataset.   
\sh{FedGO and DaFKD require image generators and discriminators. For the generator, we considered the three approaches  (G1), (G2), and (G3) in Section~\ref{sec:3.2}. For (G1) and (G3), we adopted the model architecture and training method proposed in WGAN-GP~\citep{WGAN-GP}. For (G2), we employed StyleGAN-XL~\citep{sauer2022stylegan}, pretrained on ImageNet~\citep{krizhevsky2012imagenet}.
Unless specified otherwise, we assume (G1).} For discriminators, we utilized a 4-layer CNN. More experimental details are provided in Appendix~\ref{asec:exps}.


\subsection{Results}


\begin{table*}[!htb]

\vskip -0.15in
  \caption{\sh{Server test accuracy (\%)} of our FedGO with a generator trained with the unlabeled dataset on the server %
  (Scratch) and with an off-the-shelf generator %
  pretrained on ImageNet (Pretrained) on three image datasets with $\alpha=0.05$.} %
  
  \label{tab:4}
  
\vskip 0.1in
  \centering\setlength\tabcolsep{2pt}
    \begin{tabular*}{\linewidth}{@{\extracolsep{\fill}} ccccccc }
    \toprule
    &\multicolumn{2}{c}{CIFAR-10} &\multicolumn{2}{c}{CIFAR-100} &\multicolumn{2}{c}{ImageNet100}                   \\
    \cmidrule(r){2-3}\cmidrule(r){4-5}\cmidrule(r){6-7}
    Generator & Scratch & Pretrained& Scratch & Pretrained& Scratch & Pretrained\\    
    \midrule
    Accuracy   & 72.35$\pm$9.01 & \textbf{74.40}$\pm$6.97 &\textbf{41.04}$\pm$0.99 & \textbf{41.04}$\pm$0.79 & 31.70$\pm$1.55 & \textbf{32.72}$\pm$0.18 \\    \bottomrule
  \end{tabular*}
  
\vskip -0.15in
\end{table*}
\paragraph{Test Accuracy and Convergence Speed}
Table \ref{tab:1} shows the test accuracy \sh{of the server model }and Table \ref{tab:2} presents the communication rounds required for the server model to achieve target accuracy $(\text{Acc}_\text{target})$ for the first time, for the baselines and FedGO, on CIFAR-10/100 and ImageNet100 datasets. Our FedGO algorithm exhibits the smallest performance gap from the central training and the fastest convergence speed across all the datasets and data heterogeneity settings. 


\sh{For CIFAR-10 with $\alpha=0.1$, our FedGO algorithm shows a performance improvement of over 7\%p compared to the baselines.} However, we observe a diminishing gain for CIFAR-100 and ImageNet100. We argue in Appendix~\ref{asec:enscomp} that this is not due to the marginal improvement in FedGO's ensemble performance, but rather due to larger distillation loss as the server model more struggles to keep up with the performance of the ensemble model.






\paragraph{Comparison of Weighting Methods}
Figure \ref{fig:2} shows the \sh{ensemble test accuracy} along with communication rounds on the CIFAR-10 dataset, according to weighting methods. We evaluated \sh{ensemble test accuracy} to compare the efficacy of each method in generating pseudo-labels. For the baseline weighting methods, we considered the uniform \sh{\citep{FedDF}, the variance-based \citep{Fed-ET}, the entropy-based  \citep{FedHKT,park2024overcoming}, and the domain-aware \citep{Da} methods.} 
For fair comparison, all the baselines follow the same steps except the \sh{weighting methods.} %
The effectiveness of our weighting method is demonstrated by its \sh{ensemble test accuracy} outperforming all the other weighting methods over all communication rounds.

\begin{figure}[!htb]

\vskip -0.15in
    \centering
    \begin{subfigure}{0.95\linewidth}\includegraphics[width=\linewidth]{figs/weighting_method_CIFAR_10_0.1.pdf}
    \vskip -0.09in
    \caption{$\alpha=0.1$}
    \end{subfigure}
    \begin{subfigure}{0.95\linewidth}\includegraphics[width=\linewidth]{figs/weighting_method_CIFAR_10_0.05.pdf}
    \vskip -0.09in
    \caption{$\alpha=0.05$}
    \end{subfigure}
\vskip -0.11in
    
    \caption{\sh{Ensemble test accuracy (\%)} of FedGO and other baseline weighting methods over communication rounds on CIFAR-10 with $\alpha=0.1$ and $\alpha=0.05$.}%
    \label{fig:2}
\vskip -0.15in
\end{figure}




\paragraph{FedGO with a Pretrained Generator} %
If there exists a pretrained generator capable of generating sufficiently diverse data, the server can distribute the pretrained generator to clients instead of training a generator from scratch using the server's unlabeled dataset, \sh{which corresponds to the case (G2) in Section~\ref{sec:3.2}. This approach has the advantage of saving the server's computing resources required for training a generator.} %



Table \ref{tab:4} reports the performance of FedGO for  various datasets with  $\alpha=0.05$, when using a generator trained with the server's unlabeled dataset versus using a generator pretrained on ImageNet~\citep{krizhevsky2012imagenet}. We observe that utilizing the pretrained generator results in superior performance on CIFAR-10 and ImageNet100, whereas it remains the same for CIFAR-100. A key factor contributing to performance enhancement seems to be the larger model structure of the pretrained generator and its training with a richer dataset. This enhances the generalization performance of client discriminators, enabling optimal weighting even for test data. However, since the assumption of Theorem \ref{thm:3} does not hold for \(x \in \text{supp}(p) \setminus \text{supp}(p_g)\), the portion of data for which an optimal weighting is guaranteed decreases as the portion of \(p\)'s support not covered by \(p_g\) increases, potentially leading to performance degradation. We note that  ImageNet100 is a subset of ImageNet, and ImageNet includes the classes of CIFAR-10 except deer. However, there are several classes of CIFAR-100 not included in ImageNet, which could possibly result in no performance gain. %




\paragraph{More Results} In Appendix \ref{asec:addexp}, we provide more experimental results. We report ensemble test accuracy of the baselines and FedGO, demonstrating a larger improvement compared to test accuracy. \wj{We also provide results for scenarios involving larger clients or different model structures, cases where the server dataset is different from the client datasets, and data-free approaches when no server dataset is available, showing significant performance gains over the baselines. Additionally, we report the performance of FedGO with a reduced server dataset and various discriminator training epochs, showing that even with only 20\% of the server dataset, FedGO achieves a performance gain of 15\%p over FedAVG. Furthermore, FedGO outperforms the baselines even with significantly fewer discriminator training epochs. }%

\sh{In Appendix ~\ref{asec:privacy}, a comprehensive analysis of communication costs, privacy, and computational costs for FedGO and baselines is provided. %
}

\section{Conclusion} \label{sec:5}
We proposed the FedGO algorithm, which effectively addresses the challenge of client data heterogeneity. Our algorithm was proposed based on theoretical analysis of optimal ensemble distillation, and various experimental results demonstrated its high performance and fast convergence rate \sh{under various scenarios with and without extra server dataset.}  Due to page limit, limitation and broader impact of our work are provided in Appendices \ref{limitation} and \ref{broaderimpact}, respectively. 

{

\bibliography{main}
\bibliographystyle{icml2025}




}



\newpage
\appendix
\onecolumn


\section{Formal Statement and Proof of Theorem \ref{thm:1}}
\label{asec:proof2}

In addition to the setups and definitions introduced in Section \ref{sec:2},  we assume binary classification task, i.e., $y(x)\in[0, 1]$ and $h(x)\in\{0, 1\}$, coupled with $\ell_1$ loss. 

We first present some definitions and a lemma. 

\begin{definition}
    (\citealp[Definition 1]{kifer2004detecting}) For two distributions ${q}$ and ${q'}$ over a domain $\calX$, let $\calH$ denote a hypothesis class on $\calX$ and $I(h)$ for $h\in \calH$ denote the set $\{x\in \calX: h(x)=1\}$. The $\calH$-divergence between ${q}$ and ${q'}$ is
\begin{align}
    d_{\calH}({q}, {q'})=2\sup_{h\in\calH}|Pr_{x\sim {q}}[I(h)]-Pr_{x\sim {q'}}[I(h)]|.
\end{align}
\end{definition}
    

\begin{definition}
    For a hypothesis space $\calH$, the symmetric difference hypothesis space $\calH\triangle\calH$ is the set of hypotheses
    \begin{align}
        g\in \calH\triangle\calH \Leftrightarrow g(x)=h(x)\oplus h'(x) \mbox{ for some } h, h'\in\calH
    \end{align}
    where  $\oplus$ is the XOR function.
\end{definition}

\begin{lemma}\label{lemma:2}
    For hypotheses $h, h'\in\calH$ and distributions ${q}, {q'}$ on $\calX$, we have 
    \begin{align}
        |\mathcal{L}_{{q}}(h, h')-\mathcal{L}_{{q'}}(h, h')|\leq \frac{1}{2}d_{\calH\triangle\calH}({q}, {q'}).
    \end{align}
\end{lemma}
\begin{proof} By the definition of $\calH\triangle\calH$-distance, we have 
\begin{align}
    d_{\calH\triangle\calH}({q}, {q'})&=2\sup_{h\in\calH}|Pr_{x\sim {q}}[h(x)\neq h'(x)]-Pr_{x\sim {q'}}[h(x)\neq h'(x)]|\\
    &=2\sup_{h\in\calH}|\mathcal{L}_{{q}}(h, h')-\mathcal{L}_{{q'}}(h, h')|\\
    &\geq2|\mathcal{L}_{{q}}(h, h')-\mathcal{L}_{{q'}}(h, h')|,
\end{align}
which completes the proof.
\end{proof}

Now we are ready to present the formal statement and proof of Theorem \ref{thm:1}.
\begin{thm}\label{athm:4}
    For binary classification task with $\ell_1$ loss, consider hypothesis class $\calH$ such that $h\in \calH$ outputs 0 or 1 and its spanned hypothesis class  $\bar{\calH}\triangleq \{\sum_{k=1}^Kw_k\cdot h_k|h_k\in\calH, w_k:\calX\rightarrow[0, 1] $ for all $k=1, ..., K, \sum_{k=1}^Kw_k=1\}$.  For any $h\in\calH$ and  $(\sum_{k=1}^Kw_k\cdot h_k)\in\bar\calH$, the following holds: 
    \begin{align}
        \mathcal{L}_{\calD}(h)\leq \mathcal{L}_{\calD}(\sum_{k=1}^Kw_k\cdot h_k)+\mathcal{L}_{{\calD}_s}(h, \sum_{k=1}^Kw_k\cdot h_k)+\frac{1}{2}d_{\bar{\calH}\triangle\bar{\calH}}(\calD, p_s).
    \end{align}
\end{thm}
\begin{proof}     We have
\begin{align}
    \mathcal{L}_{\calD}(h)&=\mathbf{E}_{\calD}[l(h(x), y(x)]\\
    &\leq \mathbf{E}_{\calD}[l(h(x), (\sum_{k=1}^Kw_k\cdot h_k)(x)]+ \mathbf{E}_{\calD}[l((\sum_{k=1}^Kw_k\cdot h_k)(x), y(x)]\\
    &=\mathcal{L}_{\calD}(\sum_{k=1}^Kw_k\cdot h_k)+\mathcal{L}_{\calD}(h, \sum_{k=1}^Kw_k\cdot h_k) \label{eqn:21}
\end{align}
by triangle inequality~\citep{ben2006analysis, crammer2008learning}.

Since $A\leq B+|A-B|$, letting $A=\mathcal{L}_{\calD}(h, \sum_{k=1}^Kw_k\cdot h_k)$, $B=\mathcal{L}_{{\calD}_s}(h, \sum_{k=1}^Kw_k\cdot h_k)$, the RHS of \eqref{eqn:21} is upper-bounded by 
\begin{align}
    &\mathcal{L}_{\calD}(\sum_{k=1}^Kw_k\cdot h_k)+\mathcal{L}_{{\calD}_s}(h, \sum_{k=1}^Kw_k\cdot h_k)+|\mathcal{L}_{\calD}(h, \sum_{k=1}^Kw_k\cdot h_k)-\mathcal{L}_{{\calD}_s}(h, \sum_{k=1}^Kw_k\cdot h_k)|\\
    &=\mathcal{L}_{\calD}(\sum_{k=1}^Kw_k\cdot h_k)+\mathcal{L}_{{\calD}_s}(h, \sum_{k=1}^Kw_k\cdot h_k)+\frac{1}{2}d_{\bar{\calH}\triangle\bar{\calH}}(\calD, \calD_s)
\end{align}

by the definition of $d_{\bar{\calH}\triangle\bar{\calH}}$ and Lemma \ref{lemma:2}. Thus, we prove Theorem \ref{athm:4}.
\end{proof}



\section{Proof of Theorem \ref{thm:2}}
\label{asec:proof3}%

Before we start the proof of Theorem \ref{thm:2}, we present the following lemma with the setups and definitions introduced in Section \ref{sec:2}.

\begin{lemma}\label{lemma:b1}
    Let the loss function $l$ be convex and $\{h_k\}_{k=1}^K\subset\calH$. For the weight functions $\{w_k^*\}_{k=1}^K$ defined in Theorem \ref{thm:2}, the following  holds:
    \begin{align}
        \mathcal{L}_\D(\textstyle\sum_k& w_k^*\cdot h_k)\leq\textstyle\sum_k \pi_k \cdot\mathcal{L}_{\Dc}(h_k).
    \end{align}
\end{lemma}
\begin{proof} 
Note that
\begin{align}
    \mathcal{L}_\D(\textstyle\sum_k w_k^*\cdot{h_k})    &=\ED\left[l\left(\textstyle\sum_k w_k^*(x)\cdot{h_k}(x), y(x)\right)\right]\\
    &=\int l\left(\textstyle\sum_k w_k^*(x)\cdot{h_k}(x), y(x)\right) \cdot \D(x) dx\\
    &=\int l\left(\textstyle\sum_k w_k^*(x)\cdot{h_k}(x), y(x)\right) \cdot \textstyle\sum_j\pi_j\cdot p_j(x) dx\\
    &=\int l\left(\textstyle\sum_k w_k^*(x)\cdot{h_k}(x), \textstyle\sum_k w_k^*(x)\cdot y(x)\right) \cdot \textstyle\sum_j\pi_j\cdot p_j(x) dx\\
    &\leq\int\left(\textstyle\sum_k w_k^*(x)\cdot l\left( {h_k}(x), y(x)\right)\right) \cdot \textstyle\sum_j\pi_j \cdot p_j(x) dx \label{aeqn:14}\\
    &=\int \textstyle\sum_k \frac{\pi_k(x)\cdot p_k(x)}{\sum_{i=1}^K\pi_i \cdot p_i(x)}\cdot l\left({h_k}(x), y(x)\right) \cdot \textstyle\sum_j\pi_j\cdot p_j(x) dx\\
    &=\sum_k \int \pi_k \cdot p_k(x)\cdot l\left({h_k}(x), y(x)\right)  dx\\
    &=\sum_k \pi_k \cdot \int  l\left({h_k}(x), y(x)\right) \cdot p_k(x) dx\\
    &=\sum_k \pi_k \cdot\mathcal{L}_{\Dc}({h_k})\label{aeqn:18},
\end{align}
where \eqref{aeqn:14} holds due to the convexity of loss function $l(\cdot ,\cdot)$. This completes the proof. 
\end{proof}

Now we present the proof of Theorem \ref{thm:2}.
\begin{proof}
For $h\in\calH$, we have  
\begin{align}
    \mathcal{L}_\D(h)&=\ED\left[l\left(h(x), y(x)\right)\right]\\
    &=\int  l\left(h(x), y(x)\right) \cdot p(x)dx\\
    &=\int l\left(h(x), y(x)\right) \cdot \textstyle\sum_k\pi_k\cdot p_k(x) dx\\
    &=\textstyle\sum_k \pi_k \cdot \int \left[l\left(h(x), y(x)\right)\right] \cdot p_k(x) dx\\
    &= \textstyle\sum_k \pi_k \cdot\mathcal{L}_{\Dc}(h)\\
    &\geq \textstyle\sum_k \pi_k \cdot\mathcal{L}_{\Dc}(h_{\Dc}^*).\label{aeqn:9}
\end{align}
Hence, it suffices to show that 
\begin{align}
    \mathcal{L}_\D(\textstyle\sum_k& w_k^*\cdot\hDk)\leq\textstyle\sum_k \pi_k \cdot\mathcal{L}_{\Dc}(h_{\Dc}^*),
\end{align}

and this is the direct result of Lemma \ref{lemma:b1} with $\{h_k\}_{k=1}^K=\{h_{\Dc}^*\}_{k=1}^K$.
\end{proof}



    









\section{Generalization Bound with Empirical Loss Minimizer}
\label{sec:general}
In this section, we present the generalization loss bound of the ensemble of empirical loss minimizers of clients with our weighting method.

\begin{thm}\label{athm:c1}
    For binary classification task with $\ell_1$ loss, the following holds for our weighting function \(\{w_k^*\}_{k=1}^K\) defined in Theorem \ref{thm:2}: 
    \begin{align}
         \mathcal{L}_{\calD}(\sum_{k=1}^Kw_k^*\cdot \hDhatk)&\leq\sum_{k=1}^K\pi_k\cdot\left[\mathcal{L}_{\hat{\D}_k}(\hDhatk)+\frac{4+\sqrt{\log(\tau_\calH(2n_k))}}{(\delta/K)\sqrt{2n_k}}\right]\\
         &\leq \mathcal{L}_{\hat\calD}(\hDhat)+\sum_{k=1}^K\pi_k\cdot \frac{4+\sqrt{\log(\tau_\mathcal{H}(2n_k))}}{(\delta/K)\cdot \sqrt{2n_k}},
    \end{align}
    where  \(\hat{p}_k\) is the empirical distribution by sampling $n_k$ data points i.i.d. according to $p_k$, $\hat\calD=\sum_{k=1}^K\pi_k\cdot \hat{\calD}_k$, and \(\tau_{\calH}\) is growth function bounded by polynomial of the VC-dimension of $\calH$. 
\end{thm}

Compared to \eqref{eq:da_gen_bound}, we can see that the ensemble of empirical loss minimizers with our weighting method has a tighter generalization bound without the factor of $(K+1)$. 

Before we prove Theorem \ref{athm:c1}, we present the following theorem.

\begin{thm}(\citealp[Theorem 6.11]{learningtheory}\label{athm:c2}) Let $\calH$ be a hypothesiss class and let $\tau_{\calH}$ be its growth function. Then, for every distribution $q$ on $\calX$ and every $\delta\in(0, 1)$, with probability of at least $1-\delta$ over the $m$ i.i.d. choice of $S\sim q^{m}$ with its empirical distribution $\hat q$, we have
\begin{align}
    |\calL_q(h)-\calL_{\hat q}(h)|\leq\frac{4+\sqrt{\log(\tau_\calH(2m))}}{\delta\sqrt{2m}}.
\end{align}
\end{thm}
We also present the bound of growth function $\tau_\calH$.
\begin{lemma}
    (\citealp[Lemma 6.10]{learningtheory})
    Let $\calH$ be a hypothesis class with VC-dimension of $\calH$ is smaller than $d$, i.e. $VCDim(\calH)\leq d<\infty$. Then, for all $m$, $\tau_\calH(m)\leq\sum_{i=0}^d \binom{m}{i}$. In particular, if $m>d+1$, then $\tau_\calH(m)\leq(em/d)^d$, where $e$ is Euler's number.
\end{lemma}
Now we present the proof of Theorem \ref{athm:c1}.

\begin{proof}
    By the result of Lemma \ref{lemma:b1} with $\{h_k\}_{k=1}^K=\{\hDhatk\}_{k=1}^K$, we can derive
    \begin{align}
        \mathcal{L}_\D(\textstyle\sum_k w_k^*\cdot\hDhatk)\leq\textstyle\sum_k \pi_k \cdot\mathcal{L}_{\Dc}(\hDhatk).
    \end{align}
    Also we note that $S_k\sim p_k^{n_k}$. We can derive the following inequality for $k=1, ..., K$ using Theorem \ref{athm:c2}. With probability at least of $1-(\delta/K)$,
    \begin{align}
        \mathcal{L}_{\Dc}(\hDhatk) \leq \mathcal{L}_{\hat{\D}_k}(\hDhatk)+\frac{4+\sqrt{\log(\tau_\calH(2n_k))}}{(\delta/K)\sqrt{2n_k}}.
    \end{align}
    where \(\tau_{\calH}\) is growth function bounded by polynomial of the VC-dimension of $\calH$. 

    By the union bound, we have  
    \begin{align}
        P & \left[\bigcap_{k=1}^K  \left(\mathcal{L}_{\Dc}(\hDhatk) \leq \mathcal{L}_{\hat{\D}_k}(\hDhatk)+\frac{4+\sqrt{\log(\tau_\calH(2n_k))}}{(\delta/K)\sqrt{2n_k}}\right)\right]\\
        &= 1-P\left[\bigcup_{k=1}^K\left(\mathcal{L}_{\Dc}(\hDhatk) \geq \mathcal{L}_{\hat{\D}_k}(\hDhatk)+\frac{4+\sqrt{\log(\tau_\calH(2n_k))}}{(\delta/K)\sqrt{2n_k}}\right)\right]\\
        &\geq 1-\sum_{k=1}^K P\left[\left(\mathcal{L}_{\Dc}(\hDhatk) \geq \mathcal{L}_{\hat{\D}_k}(\hDhatk)+\frac{4+\sqrt{\log(\tau_\calH(2n_k))}}{(\delta/K)\sqrt{2n_k}}\right)\right]\\
        & \geq 1-\sum_{k=1}^K (\delta/K)\\
        & \geq 1-\delta.
    \end{align}

Hence, with probability at least $1-\delta$, following inequality holds for all $k=1, .., K$:
\begin{align}
    \mathcal{L}_{\Dc}(\hDhatk) \leq \mathcal{L}_{\hat{\D}_k}(\hDhatk)+\frac{4+\sqrt{\log(\tau_\calH(2n_k))}}{(\delta/K)\sqrt{2n_k}}.
\end{align}
Furthermore, by definition of $\hat\calD$,
\begin{align}
    \calL_{\hat\calD}(\hDhat)&=\textstyle\sum_k \pi_k \cdot\mathcal{L}_{\hat{\D}_k}(\hDhat)\\
    & \geq \textstyle\sum_k \pi_k \cdot\mathcal{L}_{\hat{\D}_k}(\hDhatk).
\end{align}

By combining the above results, with probability of at least $1-\delta$, we have

\begin{align}
        \mathcal{L}_\D(\textstyle\sum_{k=1}^K w_k^*\cdot\hDhatk)&\leq\textstyle\sum_k \pi_k \cdot\mathcal{L}_{\Dc}(\hDhatk)\\
        &\leq\sum_{k=1}^K\pi_k \cdot\left[\mathcal{L}_{\hat{\D}_k}(\hDhatk)+\frac{4+\sqrt{\log(\tau_\calH(2n_k))}}{(\delta/K)\sqrt{2n_k}}\right]\\
        &\leq\sum_{k=1}^K\left[\pi_k \cdot\mathcal{L}_{\hat{\D}_k}(\hDhatk)+\pi_k \cdot\frac{4+\sqrt{\log(\tau_\calH(2n_k))}}{(\delta/K)\sqrt{2n_k}}\right]\\
        &\leq\calL_{\hat\calD}(\hDhat)+\sum_{k=1}^K\pi_k \cdot\frac{4+\sqrt{\log(\tau_\calH(2n_k))}}{(\delta/K)\sqrt{2n_k}}.
\end{align}
This completes the proof.
\end{proof}

\section{Description of FedGO}\label{asec:alg_desc}

Figure \ref{fig:3a} illustrates the operation of FedGO. Algorithm \ref{alg:2} presents a pseudo-code of FedGO.  For training   discriminators, each client optimizes the GAN loss with respect to its labeled dataset. A pseudo-code of client discriminator update is provided in Algorithm \ref{alg:3}. %

\begin{figure}[!htb]
    \centering
    \includegraphics[width=\linewidth]{figs/FedGOalgorithm.pdf}
    \caption{Illustration of our FedGO algorithm.}\label{fig:3a}
\end{figure}


\begin{algorithm}[!htb]
\caption{FedGO algorithm with $K$ clients for $T$ communication rounds. $f(\cdot;\theta)$ stands for the model with parameter $\theta$, $\mu$ stands for the step size, and $\Phi_k(x)$ stands for the odds value of $D_k(x)$.}
\label{alg:2}
\begin{algorithmic}
\REQUIRE Client labeled dataset $\{S_k\}_{k=1}^K$%
\STATE Initialize server model $f(\cdot; \theta_s^0)$ with parameter $\theta_s^0$
    \STATE Prepare generate $G$ and unlabeled dataset $U$ \COMMENT{\sh{By one of the methods in Table~\ref{tab:GOsetting}}}
    \makeatletter
    \newcommand{\PARFOR}[1]{\FOR{\textbf{parallel} #1}} %
    \makeatother
    \PARFOR {client $k\in\{1, 2, ..., K\}$}
    \STATE $D_k\gets DiscriminatorUpdate(G, S_k)$ \COMMENT{Detailed in Algorithm \ref{alg:3}}
    \ENDFOR
    \FOR {communication round $t=1$ to $T$}
    \STATE $A^t\gets$ sample $\lfloor{C\cdot K}\rfloor$ clients
    \PARFOR {client $k\in A^t$}
    \STATE $\theta_k^t\gets ClientUpdate(\theta_s^{t-1}, S_k)$ \COMMENT {Gradient update $\theta_s^{t-1}$ with $S_k$}
    \ENDFOR
    \STATE $\theta_s^t\gets\sum_{k\in A^t}\frac{n_k}{\sum_{i\in A^t}n_{i}}\cdot\theta_k^t$
    \FOR {server train epoch $e=1$ to $E_s$}
    \FOR {unlabeled minibatch $u\in U$}
    \STATE $\tilde y(u)\gets \sigma(\sum_{k\in A^t}w_k^*(u)\cdot f(u;\theta_k^t))$ \COMMENT{$w_k^*(u)=\frac{n_k\cdot \Phi_k(u)}{\sum_{i\in A^t}n_i\cdot \Phi_i(u)}$}
    \STATE $\theta_s^t\gets\theta_s^t-\mu\cdot\nabla_{\theta_s^t}\text{KL}(\tilde y(u), \sigma(f(u;\theta_s^t)))$
    \ENDFOR
    \ENDFOR
    \ENDFOR
\STATE \Return $f(\cdot; \theta_s^T)$
\end{algorithmic}
\end{algorithm}


\begin{algorithm}[!htb]
\caption{Discriminator update for $E_d$ epochs. $\mu_d$ stands for the step size and $D(\cdot; \theta)$ is the parameterized discriminator with parameter $\theta$.}
\label{alg:3}
\begin{algorithmic}
\REQUIRE Generator $G$, labeled dataset $S$
\STATE Initialize discriminator model $D(\cdot; \theta_d^0)$ with parameter $\theta_d^0$
    \FOR {epoch $e=1$ to $E_d$}
    \STATE $\theta_d^e\gets \theta_d^{e-1}$
    \FOR {minibatch $m\in S$}
    \STATE $(x_{real}, y)\gets$ (images, labels) pair of minimatch $m$
    \STATE $x_{fake}\gets$ generated images by generator $G$
    \STATE $Loss_{GAN}(D(\cdot; \theta_d^e))\gets \log(D(x_{real}; \theta_d^e)+\log(1-D(x_{fake}; \theta_d^e))$ 
    \STATE $\theta_d^e\gets\theta_d^e-\mu_d\nabla_{\theta_d^e}Loss_{GAN}(D(\cdot; \theta_d^e))$ \COMMENT{Update with gradient for vanilla GAN loss}
    \ENDFOR
    \ENDFOR
\STATE \Return {$D(\cdot; \theta_d^{E_d}))$}
\end{algorithmic}
\end{algorithm}

\section{Experimental Details}\label{asec:exp}
All experiments were conducted in Python 3.8.12 environment using a 64-core Intel 2.90GHz Xeon Gold 6226R CPU with 512GB memory, and an RTX 3090 GPU. We also implemented the algorithms using PyTorch with version 1.11.0.
\subsection{Detailed Experimental Setting and Analysis of Toy Example (Figure \ref{fig:1})}\label{asec:exp1}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figs/fig1_a.pdf}
    \includegraphics[width=\linewidth]{figs/fig1_.pdf}
    \caption{Top row represents the server and clients' datasets. Bottom row,  showing the decision boundaries of the aggregated models,  is the same as Figure \ref{fig:1} and copied here for ease of analysis.}
    \label{fig:4a}
\end{figure}

For the toy example in Figure \ref{fig:1}, the dataset is generated from a mixture of four Gaussian distributions, each with a variance of 3. The top row of Figure \ref{fig:4a} shows the global data distribution and the datasets held by four clients. Each point represents data, with its color indicating the class label: data from Gaussians with means at (4, 4) and (-4, -4) are labeled as Red, data from the Gaussian with mean at (-4, 4) as Blue, and data from the Gaussian with mean at (4, -4) as Green. Each Gaussian provides 300 data samples. Each client holds 90\% of data from the Gaussian whose mean is in a certain quadrant (the 3rd, 4th, 2nd, 1st quadrants for Clients 1, 2, 3, and 4, respectively), and the remaining 10\% from Gaussians with means in the other quadrants. The clients' global dataset comprises 1200 samples, with 300 from each Gaussian. The server unlabeled dataset comprises 300 data, uniformly distributed on  the square \([-12, 12] \times [-12, 12]\).

Each client trains a 3-layer MLP classifier for 2 epochs using its dataset, and a 3-layer discriminator for 1 epoch using its dataset as real dataset and server dataset as fake dataset. We used Adam~\citep{kingma2014adam} with learning rate 0.001 and $(\beta_1, \beta_2)=(0.9, 0.999)$ for classifier optimizer, and RMSprop~\citep{rmsprop} with learning rate 0.00005 for discriminator optimizer. Also we used a batch size of 64 for both. 

The bottom row of Figure \ref{fig:4a} (same as Figure \ref{fig:1}) illustrates the decision boundaries of server models. The leftmost plot is from the model with averaged client model parameters, while the remaining plots are from the server models trained via ensemble distillation for 2 epochs using pseudo-labeled dataset: the global dataset is  pseudo-labeled using uniform weighting~\citep{FedDF}%
, variance weighting~\citep{Fed-ET}%
, entropy weighting~\citep{FedHKT, park2024overcoming}%
, domain-aware weighting~\citep{Da}
, and  our weighting method. The background color indicates the decision boundary in RGB channels. Given the Gaussian distributions, the optimal decision rule is red in the 1st and 3rd quadrants, blue in the 2nd quadrant, and green in the 4th quadrant. Thus, the oracle decision boundary aligns with the x-axis and y-axis, depicted by black lines.

The averaged parameter model exhibits a blurred decision boundary compared to models trained via ensemble distillation. Furthermore, among the models with ensemble distillation, the decision boundary of the model trained via our weighting method is closest to the oracle decision boundary.


\subsection{Detailed Experimental Settings for Image Classification Tasks} \label{asec:exps}
\paragraph{Hyperparameter Tuning}
We identified the best-performing hyperparameters on CIFAR-100 with Dirichlet \(\alpha = 0.05\) and used the same values for other settings. During the ensemble distillation process, we trained both clients and server with the Adam optimizer~\citep{kingma2014adam} at a learning rate of 0.001 with batch size 64, without weight decay. The $(\beta_1, \beta_2)$ parameters for Adam were set to (0.9, 0.999). Additionally, we applied cosine annealing~\citep{loshchilov2016sgdr} to decay the server learning rate until the final communication round $T=100$ as in~\citet{FedDF}, except for the results of \ref{asubsec:hetero} and \ref{asubsec:datafree}. 

For the client and server classifier training epochs, we performed a grid search to find the optimal number of training epochs. The initial grid was \{5, 10, 30, 50\}, and the experiments were conducted with 30 client epochs and 10 server epochs (\(E_s = 10\)) for CIFAR-10/100.  To leverage the increased number of steps due to the additional number of data, experiments on ImageNet100 were conducted with 10 client classifier epochs and 3 server classifier epochs (\(E_s = 3\)).

To train the generator utilized by our FedGO from scratch, we trained the WGAN-GP model following the training method proposed in~\citet{WGAN-GP}. The generator and discriminator of WGAN-GP were trained using the Adam optimizer with a learning rate of 0.0002 and \((\beta_1, \beta_2) = (0, 0.9)\). The training was conducted with a batch size of 64 until the generator completed 100,000 gradient steps. The generator was updated every 5 steps of the discriminator, and a gradient penalty coefficient \(\lambda\) of 10 was used.

\wj{When training a generator in a data-free setting, i.e., the case (G3), we utilized the FedGAN~\cite{rasouli2020fedgan} algorithm. The generator was trained for only \(T' = 5\) communication rounds, with each local generator and discriminator trained for 3 epochs per round.}

For the client discriminator, we adopted the hyperparameters from \href{https://github.com/Ksuryateja/DCGAN-MNIST-pytorch/blob/master/gan_mnist.py}{https://github.com/Ksuryateja/DCGAN-MNIST-pytorch/blob/master/gan$\_$mnist.py} and trained it with a batch size of 64 for 30 epochs for CIFAR-10/100, and for 10 epochs for ImageNet100. The optimizer Adam was used with a learning rate of 0.0002, and \((\beta_1, \beta_2) = (0.5, 0.999)\).

\sh{FedProx~\citep{FedProx} introduces a proximal term to the client training loss, which helps to address heterogeneity by penalizing large deviations from the server model. The proximal term is multiplied by a coefficient \(\mu\) and added to the primary objective loss. We performed a grid search to tune the value of \(\mu\) from \{0.1, 0.05, 0.01, 0.005, 0.001, 0.0005, 0.0001\}, and chose the best value \(\mu = 0.001\).}

\wj{FedDisco~\citep{FedDisco} determines client parameter aggregation coefficient by leveraging both the dataset size and the discrepancy between local and global category distributions. Here, $a$ controls the influence of the discrepancy, while $b$ adjusts the baseline aggregation coefficient. Following the FedDisco paper, we adopted the best-performing parameters \(a = 0.5\) and \(b = 0.1\), which were tuned for optimal performance for CIFAR-10.}

\sh{FedHKT~\citep{FedHKT} and FedDS~\citep{park2024overcoming} introduce a temperature parameter $\tau>0$, which allows client weights to approach uniform weighting as $\tau$ increases. By following \citet{FedHKT}, we set $\tau=1$.}

FedGKD~\citep{FedGKD} introduces an additional buffer of length \(M\) on the server, where the server model is stored after each round. The server then creates an additional model with averaged parameters from the models stored in the buffer and sends this model to the clients each round. Each client uses a temperature parameter \(\tau\) to compute the knowledge distillation loss on the received additional model, multiplies this loss by \(\gamma/2\), and adds it to the primary objective loss. Consequently, it is necessary to tune three additional hyperparameters: \(M\), \(\tau\), and \(\gamma\). We conducted a grid search with \(M\) and \(\tau\) in \{1, 3, 5, 10\} and \(\gamma\) in \{0.1, 0.05, 0.01, 0.005, 0.001\}. The best performing parameters were \(M = 5\), \(\tau = 3\), and \(\gamma = 0.001\).

Similar to our FedGO, DaFKD~\citep{Da} utilizes discriminators to implement client weighting function. However, unlike FedGO, DaFKD trains the generator and discriminators collaboratively. To focus on the weighting method, the domain-aware weighting method in Figure \ref{fig:2} is implemented by only modifying the weighting step in our FedGO algorithm.

\paragraph{Model Implementation}
We used ResNet-18~\citep{ResNet} as the classification model, following the implementation from \href{https://github.com/kuangliu/pytorch-cifar/blob/master/models/resnet.py}{https://github.com/kuangliu/pytorch-cifar/blob/master/models/resnet.py}. Additionally, our FedGO requires extra generator and discriminator models. When training the generator from scratch, we utilized the WGAN-GP model as proposed in~\citet{WGAN-GP}, following its official open-source implementation\footnote{ \href{https://github.com/igul222/improved_wgan_training}{https://github.com/igul222/improved$\_$wgan$\_$training}}. We re-implemented this code in PyTorch for our experiments. For a pretrained off-the-shelf generator, we utilized StyleGAN-XL~\citep{sauer2022stylegan} model pretrained on ImageNet~\citep{krizhevsky2012imagenet} with resolution of 32$\times$32. We downloaded the model parameters from \href{https://github.com/autonomousvision/stylegan-xl}{https://github.com/autonomousvision/stylegan-xl} and implemented the model using these parameters. 
For the client discriminator, we adopted a simple 4-layer CNN discriminator, following the implementation from \href{https://github.com/Ksuryateja/DCGAN-MNIST-pytorch/blob/master/gan_mnist.py}{https://github.com/Ksuryateja/DCGAN-MNIST-pytorch/blob/master/gan$\_$mnist.py}.
To address the widely known overfitting issue of the discriminator~\citep{adlam2019investigating, yang2022improving} and the resulting dominance of client weights, we employed a composition of two sigmoid activations for the discriminator output. This ensures that the odds value $\Phi_k$ for client $k$'s discriminator $D_k$ is constrained between 1 and $e$.

\paragraph{Heterogeneous Client Data Split}

To introduce non-iid distributions among client datasets, we ensured that each client's distribution follows a Dirichlet distribution Dir($\alpha$), similar as in~\citet{FedDF, wang2020federated, marfoq2022personalized, li2023fedtp}. As the parameter $\alpha$ increases, each client tends to have a more homogeneous distribution, whereas smaller $\alpha$ values result in increased data heterogeneity among clients. We conducted experiments for each dataset with $\alpha$ values of 0.1 and 0.05. The number of data samples that each client has per class for CIFAR-10/100 datasets with $\alpha$ values of 0.1 and 0.05 is illustrated in Figures \ref{fig:5a} and \ref{fig:6a}. It's worth noting that ImageNet100 also has 100 classes, so the trends observed in CIFAR-100 would likely align with those in ImageNet100. We can observe that when $\alpha=0.05$, the difference in the number of data samples per  class for each client is more pronounced compared to when $\alpha=0.1$. This results in more skewed distributions for individual clients.

\begin{figure*}[!htb]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
    \includegraphics[width=\textwidth]{figs/cifar10_n_data_for_cls_for_c_0.1.pdf}
    \caption{$\alpha=0.1$}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
    \includegraphics[width=\textwidth]
    {figs/cifar10_n_data_for_cls_for_c_0.05.pdf}
    \caption{$\alpha=0.05$}
    \end{subfigure}
    \caption{Client data split for CIFAR-10 with $\alpha=0.1, 0.05$.}
    \label{fig:5a}
\end{figure*}

\begin{figure*}[!htb]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
    \includegraphics[width=\textwidth]{figs/cifar100_n_data_for_cls_for_c_0.1.pdf}
    \caption{$\alpha=0.1$}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
    \includegraphics[width=\textwidth]
    {figs/cifar100_n_data_for_cls_for_c_0.05.pdf}
    \caption{$\alpha=0.05$}
    \end{subfigure}
    \caption{Client data split for CIFAR-100 with $\alpha=0.1, 0.05$.}
    \label{fig:6a}
\end{figure*}

\paragraph{Details for Dataset}
We normalized the pixel values of all image datasets to fall within the range $[-1, 1]$, ensuring that the generated data also has pixel values within this range. Additionally, for both the training datasets of clients and the server's unlabeled dataset, we conducted further data augmentation using PyTorch's random horizontal flip.

\paragraph{Selection of \( \text{Acc}_{\text{target}} \)}

We used the highest multiple of 5 of the test accuracy (\%) achieved by the FedAVG algorithm within 100 rounds for all five different random seeds as \( \text{Acc}_{\text{target}} \) for Table \ref{tab:2}.

\section{Additional Experimental Results}\label{asec:addexp}


\subsection{Results with 100 Clients} 
\sh{Figure \ref{fig:100c} shows (a) the test accuracy of the server model, (b) the test accuracy of the ensemble model, and (c) the test loss of the ensemble model during the training process for $K=100$ clients on CIFAR-10 dataset with $\alpha=0.05$. The latter two measures were evaluated only for algorithms incorporating ensemble distillation. FedGO achieves the test accuracy of 69.52\%, which is slightly lower than 72.35\% with 20 clients (Table \ref{tab:1}). In comparison, FedAVG, FedProx, FedDF, FedGKD$^+$, and DaFKD show significant performance drops to 33.40\%, 35.07\%, 44.36\%, 45.44\%, and 59.62\%, respectively. This demonstrates that even in settings with a large number of clients, FedGO exhibits robust performance compared to the baselines.}

\sh{In terms of the test accuracy and the test loss of the ensemble model, FedGO consistently demonstrates superior performance across all rounds compared to the baseline algorithms. Furthermore, unlike the baseline algorithms, whose test loss initially decreases but then becomes unstable and increases from early rounds, FedGO's loss converges with small deviation.} %

\begin{figure*}[!htb]
    \centering
    \begin{subfigure}[b]{0.3\textwidth}\includegraphics[width=\linewidth]{figs/Acc_comparison_n_c_100.pdf}
    \caption{\sh{Server test accuracy}}
    \end{subfigure}
    \begin{subfigure}[b]{0.3\textwidth}\includegraphics[width=\linewidth]{figs/Ensemble_comparison_n_c_100.pdf}
    \caption{\sh{Ensemble test accuracy}}
    \end{subfigure}
    \begin{subfigure}[b]{0.3\textwidth}\includegraphics[width=\linewidth]{figs/Loss_comparison_n_c_100.pdf}
    \caption{\sh{Ensemble test loss}}
    \end{subfigure}
    \caption{\sh{Server test accuracy (\%), test accuracy of the ensemble model (\%), and test loss of the ensemble model of our FedGO and baselines for 100 clients on CIFAR-10 dataset with $\alpha=0.05$.}}
    \label{fig:100c}
\end{figure*}

\subsection{Ensemble Test Accuracy Comparison and Analysis}\label{asec:enscomp}

Figure \ref{fig:7a} shows the ensemble test accuracy on the server's unlabeled dataset during the training process for our FedGO algorithm and the baseline ensemble algorithms: FedDF, FedGKD$^+$, and \sh{DaFKD}. It demonstrates that using pseudo-labels generated by theoretically guaranteed weighting methods allows the server to achieve higher final performance and faster convergence.

However, in Table \ref{tab:1} of the paper, the performance gap between our method and the baselines on CIFAR-100 and ImageNet100 was not as large as that on CIFAR-10. We infer the reason from Theorem \ref{thm:1}. The second term on the RHS of Theorem \ref{thm:1} can be interpreted as the distillation loss due to the difference between the hypothesis class and the spanned hypothesis class.  Even if our ensemble is close to optimal, the knowledge-distilled server model may not follow the performance of the ensemble if it hard for a single model to learn the pseudo-labels, and we conjectures that it becomes harder as the number of classes increases. %

To support our hypothesis, we show the minimum of mean distillation loss for five different random seeds during 100 round of communication rounds  in Table \ref{tab:5a}. The distillation loss increases progressively from CIFAR-10 to CIFAR-100 to ImageNet100. In addition, the distillation loss is higher for $\alpha=0.05$ than for $\alpha=0.1$, which explains why the gap from the central training is larger for $\alpha=0.05$.


\begin{figure*}[!htb]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
    \includegraphics[width=\textwidth]{figs/Ensemble_comparison_cifar10_0.1.pdf}
    \caption{CIFAR-10 with $\alpha=0.1$}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
    \includegraphics[width=\textwidth]
    {figs/Ensemble_comparison_cifar10_0.05.pdf}
    \caption{CIFAR-10 with $\alpha=0.05$}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{0.49\textwidth}
    \includegraphics[width=\textwidth]{figs/Ensemble_comparison_cifar100_0.1.pdf}
    \caption{CIFAR-100 with $\alpha=0.1$}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
    \includegraphics[width=\textwidth]
    {figs/Ensemble_comparison_cifar100_0.05.pdf}
    \caption{CIFAR-100 with $\alpha=0.05$}
    \end{subfigure}\hfill\begin{subfigure}[b]{0.49\textwidth}
    \includegraphics[width=\textwidth]{figs/Ensemble_comparison_imagenet100_0.1.pdf}
    \caption{ImageNet100 with $\alpha=0.1$}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
    \includegraphics[width=\textwidth]   {figs/Ensemble_comparison_imagenet100_0.05.pdf}
    \caption{ImageNet100 with $\alpha=0.05$}
    \end{subfigure}\hfill
    
    \caption{Ensemble test accuracy\sh{ (\%)} of FedGO and baselines over communication rounds on three image datasets with $\alpha=0.1, 0.05$.}
    \label{fig:7a}
\end{figure*}


\begin{table}[!htb]

  \caption{Minimum mean distillation loss of FedGO on three image datasets with $\alpha=0.1, 0.05$.}\label{tab:5a}
  \vskip 0.1in
  \centering\setlength\tabcolsep{0pt}
    \begin{tabular*}{\linewidth}{@{\extracolsep{\fill}} cccc }
    \toprule 
    Dataset &CIFAR-10 & CIFAR-100 & ImageNet100\\    
    \midrule
    $\alpha=0.1$ & 0.175 & 0.237 & 0.363\\
    $\alpha=0.05$ & 0.266 & 0.348 &  0.539 \\    \bottomrule
  \end{tabular*}
\end{table}

\subsection{Ensemble Distillation with a Different Server Dataset} \label{asubsec:hetero}
Our theoretical justification of constituting an optimal ensemble in Corollary \ref{cor:1} allows heterogeneity between the server data distribution \( p_s \) and the client average distribution \( p \). To demonstrate the effectiveness of FedGO when $p_s\neq p$ which makes more sense in practice, we report the results when clients have the half of the CIFAR-10 dataset and the server has the half of the CIFAR-100 (unlabeled) dataset, in Table \ref{tab:hetero}. The experimental results demonstrate that ensemble distillation even with heterogeneous server dataset is helpful in improving the performance. Furthermore, by employing optimal model ensemble, our FedGO algorithm, with theoretical performance guarantee, shows improvement over FedDF and DaFKD. 

\begin{table}[!htb]
  \caption{Server test accuracy\sh{ (\%)} and ensemble test accuracy\sh{ (\%)} of our FedGO and baselines with heterogeneous server dataset: CIFAR-10 for client dataset and CIFAR-100 for server's unlabeled dataset.}\label{tab:hetero}
  \vskip 0.1in
  \centering\setlength\tabcolsep{0pt}
    \begin{tabular*}{\linewidth}{@{\extracolsep{\fill}} cccccc }
    \toprule
    & &FedAVG& FedDF& DaFKD & \textbf{FedGO (ours)}\\    
    \midrule
    \multirow{2.4}{*}{$\alpha=0.1$}&Server test accuracy&58.65$\pm$5.75& 59.89$\pm$1.88 & 60.84$\pm$2.65 & \textbf{60.92}$\pm$1.95 \\  
    &Ensemble test accuracy & - & 62.62$\pm$0.90 & 63.88 $\pm$ 2.02 & \textbf{64.23}$\pm$1.29 \\
    \midrule
    \multirow{2.4}{*}{$\alpha=0.05$}&Server test accuracy&46.61$\pm$8.54&49.21$\pm$4.48 & 52.31$\pm$4.26 & \textbf{52.89}$\pm$3.47 \\  
    &Ensemble test accuracy & - & 56.06$\pm$207 & 59.30 $\pm$1.33 & \textbf{60.43} $\pm$0.56 \\
    \bottomrule
  \end{tabular*}
\end{table}

\subsection{{Results with Alternative Model Architectures}} \label{asec:altmodel} 

{In the main paper, we conducted experiments with ResNet-18 model structure. In this subsection, we present the results with  VGG11~\citep{vgg} (with BatchNorm Layers~\citep{ioffe2015batch}) and ResNet-50 models. For VGG11, both the client and server models are trained using SGD with a learning rate of 0.01 and momentum of 0.9, and all the other settings including hyperparameters are kept identical to those in the main paper. We implemented VGG11 based on \href{https://github.com/chengyangfu/pytorch-vgg-cifar10}{https://github.com/chengyangfu/pytorch-vgg-cifar10}. For ResNet-50, all the settings including optimizer and hyperparameters are set to the same as the main paper. Table \ref{tab:alt}  presents the server test accuracy of FedGO and baseline algorithms with the aforementioned model structures on CIFAR-10 with $\alpha=0.1$ after 100 communication rounds.}

\begin{table}[!htb]
  \caption{{Server test accuracy (\%) of central training, FedDF, FedGKD$^+$ and FedGO on CIFAR-10 with $\alpha=0.1$ after 100 communication rounds, when utilizing VGG11 and ResNet-50.}}
  \vskip 0.1in
  \centering\setlength\tabcolsep{0pt}
    \begin{tabular*}{\linewidth}{@{\extracolsep{\fill}} cccc}
    \toprule
    &VGG11 &ResNet-50 \\
    \midrule
    Central training &83.27 $\pm$ 0.60 &85.12 $\pm$ 0.44 \\
    FedDF &68.59 $\pm$ 4.65 &65.21 $\pm$ 4.62 \\
    FedGKD$^+$ & 67.81$\pm$ 3.60& 66.21$\pm$ 3.01\\
    \textbf{FedGO (ours)} &72.53 $\pm$ 4.10 &75.52 $\pm$ 4.30 \\
    \bottomrule
    \end{tabular*} \label{tab:alt}
\end{table}

{We can see that our FedGO algorithm consistently achieves performance gains over FedDF and FedGKD$^+$ across different model structures. }

\subsection{Data-free FedGO} \label{asubsec:datafree} \sh{In practice, the server may have no extra dataset. In this case, we first prepare a generator and then generate a distillation dataset using the generator. The generator can either be an off-the-shelf pretrained model or trained through an FL approach~\citep{rasouli2020fedgan, guerraoui2020fegan, li2022ifl, wang2023fedmed, fan2020federated, behera2022fedsyn, hardy2019md, xiong2023federated, zhang2021dance, zhang2023novel}, corresponding to the 3rd and 4th scenarios in Table~\ref{tab:GOsetting} in our main paper, respectively.} 

\sh{Figure~\ref{fig:G2} and Table~\ref{tab:G3} present the results for the two data-free approaches with 100 clients on CIFAR-10 dataset. We employed styleGAN~\citep{StyleGAN} pretrained with ImageNet dataset for the off-the-shelf generator, and applied the FedGAN algorithm~\citep{rasouli2020fedgan} for training a generator. %
For both approaches, our FedGO shows performance gains in server test accuracy, ensemble test accuracy, and ensemble test loss compared to the baseline algorithms, including the uniform weighting of FedDF~\citep{FedDF} and the domain-aware weighting of DaFKD~\citep{Da}. \wj{Moreover, we observe that ensemble test accuracy is higher than server test accuracy, which can be attributed to the increased discrepancy between the distillation dataset distribution \( p_s \) and the average client distribution \( p \). This discrepancy amplifies the gap between the ensemble model’s loss and the server model’s loss, as analyzed in Theorem~\ref{thm:1} of the main paper.}}

\sh{In both data-free approaches, we have \( p_s = p_g \), under which our weighting method is optimal for \( \forall x \in p_s = p_g \) from Theorem~\ref{thm:3} in our main paper. Note that the distance between \( p \) and \( p_g = p_s \) for the generator  trained from FedGAN is smaller than that for the off-the-shelf generator. Consequently, despite using a simpler generator trained on a smaller dataset, we observe that the performance of FedGO using the generator trained from FedGAN is slightly better than that using the off-the-shelf generator. }


\sh{Finally, we can observe performance degradation compared to the case where the distillation is performed on a real dataset. This can be attributed to the naive reuse of generated images, which has been identified as a cause of performance degradation~\citep{yoon2024redifine, wang2024generated}. An interesting future work would be on improving the performance of knowledge distillation using generated images. Still,  experimental results demonstrate that ensemble distillation is beneficial in improving performance even with generated images. Furthermore, by employing an optimal model ensemble, our FedGO shows improvement over FedDF and DaFKD.} %





\begin{figure}[!htb]
    \centering
    \begin{subfigure}[b]{0.3\textwidth}\includegraphics[width=\linewidth]{figs/Acc_comparison_G2.pdf}
    \caption{\sh{Server test accuracy}}
    \end{subfigure}
    \begin{subfigure}[b]{0.3\textwidth}\includegraphics[width=\linewidth]{figs/Ensemble_comparison_G2.pdf}
    \caption{\sh{Ensemble test accuracy}}
    \end{subfigure}
    \begin{subfigure}[b]{0.3\textwidth}\includegraphics[width=\linewidth]{figs/Loss_comparison_G2.pdf}
    \caption{\sh{Ensemble test loss}}
    \end{subfigure}
    \caption{\sh{Test accuracy of server model (\%), ensemble test accuracy (\%), and test loss of ensemble model of the data-free FedGO with an off-the-shelf generator (the case (G2)+(D2) of Table~\ref{tab:GOsetting}) and baselines with 100 clients on the CIFAR-10 dataset with $\alpha=0.05$.}}
    \label{fig:G2}
\end{figure}





\begin{table}[!htb]
    \centering
    \caption{Test accuracy of server model (\%), ensemble test accuracy (\%), and test loss of ensemble model of the data-free FedGO at 100 communication round, with a generator trained from FedGAN (the case (G3)+(D2) of Table~\ref{tab:GOsetting}) and baselines with 100 clients on the CIFAR-10 dataset with $\alpha=0.05$.}
  \vskip 0.1in
    \scalebox{0.88}{
    \begin{tabular}{cccccccc}
    \toprule
    Method & FedAVG & FedProx & SCAFFOLD & FedDisco & FedDF & DaFKD & \textbf{FedGO (ours)}\\
    \midrule
        Server Test Accuracy & 18.12$\pm$6.50 & 21.22$\pm$10.03 & 16.42$\pm$4.69 & 21.11$\pm$7.33 & 25.81$\pm$5.35 & 23.71$\pm$4.99 & \textbf{27.26}$\pm$2.32 \\
       Ensemble Test Accuracy & - & - & - & - & 37.01$\pm$5.22 & 35.07$\pm$5.40 & \textbf{38.84}$\pm$2.45\\
       Ensemble Test Loss &  - & - & - & - & 1.76$\pm$0.09 & 1.84$\pm$0.15 & \textbf{1.69}$\pm$0.06 \\\bottomrule
    \end{tabular}
    }
    \label{tab:G3}
\end{table}










\subsection{{Impact of Server Model Hyperparameters on Performance}}

\subsubsection{{Amount of Unlabeled Data}}

\sh{Figure \ref{fig:8a} shows the test accuracy of the server model and the test accuracy of the ensemble model during the training process for our FedGO algorithm. We conducted experiments by reducing the server dataset size to 50\% and 20\% of the size assumed in our main CIFAR-10 experiments. For these experiments, the server epochs were adjusted to ensure the same number of gradient steps: doubled for 50\% and quintupled for 20\%, while keeping other hyperparameters the same.}

\sh{Figure \ref{fig:8a} demonstrates that when the server dataset size decreases, the test accuracy of the ensemble model remains nearly consistent, while that of the server model decreases. This suggests that even with pseudo-labels of similar quality, the performance of the server model can decline as the server dataset size  decreases. This can be interpreted as the server model becoming more prone to overfitting as the distillation dataset becomes smaller~\citep{hinton2015distilling}. Note that FedGO has the performance improvement of about 15\% over FedAVG even with only 20\% of the dataset, which corresponds to 20\% of the total client dataset size.}

\begin{figure}[!htb]
    \centering
    \begin{subfigure}[b]{0.4\textwidth}\includegraphics[width=0.95\linewidth]{figs/Acc_comparison_n_s_ratio.pdf}
    \caption{Server test accuracy}
    \end{subfigure}
    \begin{subfigure}[b]{0.4\textwidth}\includegraphics[width=0.95\linewidth]{figs/Ensemble_comparison_n_s_ratio.pdf}
    \caption{Ensemble test accuracy}
    \end{subfigure}
    \caption{\sh{Server test accuracy\sh{ (\%)} and ensemble test accuracy\sh{ (\%)} of our FedGO on the CIFAR-10 dataset with $\alpha=0.05$, according to the size of the unlabeled dataset at the server. In the legend, $X\%$ means that the size of the unlabeled dataset at the server is reduced to $X\%$ of the size assumed in our main CIFAR-10 setting.}}
    \label{fig:8a}
\end{figure}


\subsubsection{{Server Model Training Epochs}}
 {Table \ref{tab:f.5.2} shows the impact of server model training epochs on FedGO’s performance on CIFAR-10 with $\alpha=0.1$ after 100 communication rounds. Using 5 epochs outperforms 1 epoch, with minimal performance differences beyond 5 epochs. Notably, even with only 1 epoch, FedGO significantly outperforms all the baselines trained with 10 server epochs in Table \ref{tab:1}.}

 \begin{table}[!htb]
  \caption{{Server test accuracy (\%) and ensemble test accuracy (\%) of FedGO on CIFAR-10 with $\alpha=0.1$ after 100 communication rounds, according to the number of server model training epochs.}}\label{tab:f.5.2}
  \vskip 0.1in
  \centering\setlength\tabcolsep{0pt}
    \begin{tabular*}{\linewidth}{@{\extracolsep{\fill}} ccccc}
    \toprule 
     Epoch &1 & 5 & 10 &20 \\    
    \midrule
Server Test Accuracy &74.03$\pm$6.41 &79.56$\pm$5.30 &79.62$\pm$4.36 &78.32$\pm$5.13 \\
Ensemble Test Accuracy &77.16$\pm$0.88 &80.97$\pm$0.87 &81.56$\pm$0.48 &81.39$\pm$0.75 \\
\bottomrule
  \end{tabular*}
\end{table}

\subsubsection{{Server Model Learning Rate Decay}}
{In the main paper, we used cosine learning rate decay by following the experimental setting of FedDF. As shown in  Table \ref{tab:f.5.3}, the absence of learning rate decay results in further performance improvement. Specifically, an ensemble test accuracy of 85.20\% is achieved, which is comparable to the central training model’s accuracy of 85.33\%, demonstrating the effectiveness of our provably near-optimal weighting method.}

\begin{table}[!htb]
  \caption{{Server test accuracy (\%) and ensemble test accuracy (\%) of FedGO on CIFAR-10 with $\alpha=0.1$ after 100 communication rounds, with and without learning rate decay during server model training. }}\label{tab:f.5.3}
  \vskip 0.1in
  \centering\setlength\tabcolsep{0pt}
    \begin{tabular*}{\linewidth}{@{\extracolsep{\fill}} ccc}
    \toprule 
     &\multicolumn{2}{c}{FedGO} \\
     &with LR decay &without LR decay \\
    \midrule
Server Test Accuracy &79.62$\pm$4.36 &80.18$\pm$2.16 \\
Ensemble Test Accuracy &81.56$\pm$0.48 &85.20$\pm$1.33 \\
\bottomrule
  \end{tabular*}
\end{table}

\subsection{{Impact of Generator and Discriminator Quality on Performance}}

\subsubsection{{Generator Training Steps}}
{Table \ref{tab:f.6.1} shows  the performance of our FedGO with varying generator training steps (100,000 in the main setup) alongside baseline algorithms after 50 communication rounds, while keeping all other settings unchanged from the main setup. FedGO with the generator trained for 25,000 steps performs better than that with the randomly initialized generator (0 steps), with little performance improvement beyond 25,000 steps. Remarkably, even a randomly initialized generator outperforms FedDF with uniform weighting and achieves performance comparable to DaFKD with a generator trained for 100,000 steps.}

\begin{table}[!htb]
  \caption{{Server test accuracy (\%) and ensemble test accuracy (\%) of FedGO on CIFAR-10 with $\alpha=0.1$ after 50 communication rounds, according to the number of generator training steps. }}\label{tab:f.6.1}
  \vskip 0.1in
  \centering\setlength\tabcolsep{0pt}
    \scalebox{0.8}{\begin{tabular*}{1.25\linewidth}{@{\extracolsep{\fill}} cccccccc}
    \toprule 
     &FedDF &DaFKD &\multicolumn{5}{c}{\textbf{FedGO (ours)}} \\
     \cmidrule(lr){4-8}
     Generator Training Steps &- &100,000 &0 &25,000 &50,000 &75,000 &100,000 \\
    \midrule
    Server Test Accuracy &70.18 $\pm$ 2.56 &71.42 $\pm$ 3.11 &71.12 $\pm$ 2.07 &76.74 $\pm$ 3.16 &78.43 $\pm$ 0.99 &78.89 $\pm$ 1.55 &78.24 $\pm$ 1.61 \\  
    Ensemble Test Accuracy & $73.55 \pm 2.41$ & $74.54 \pm 2.80$ & $74.88 \pm 1.63$ & $79.12 \pm 1.97$ & $80.72 \pm 0.75$ & $80.87 \pm 0.98$ & $80.82 \pm 0.82$ \\
    \bottomrule
  \end{tabular*}}
\end{table}

\subsubsection{{Discriminator Training Epochs}}

Table \ref{tab:7a} shows the final performance of the FedGO algorithm for different numbers of discriminator training epochs on CIFAR-10 with $\alpha=0.05$. It can be seen that training the discriminator more times results in better final performance. Additionally, we note that among the baselines in Table \ref{tab:1} and Figure \ref{fig:2}, except DaFKD which originally trains the generator and discriminators at each round, the highest performance is achieved by the variance weighting method, with the test accuracy of 67.51$\pm$10.77\%, indicating that there is a performance gain from the FedGO algorithm with just 5 epochs of discriminator training.

\begin{table}[!htb]
  \caption{\sh{Server} test accuracy\sh{ (\%)} of FedGO on CIFAR-10 with $\alpha=0.05$ at the 100-th communication round, according to the number of discriminator training epochs at the clients. }\label{tab:7a}
  \vskip 0.1in
  \centering\setlength\tabcolsep{0pt}
    \begin{tabular*}{\linewidth}{@{\extracolsep{\fill}} cccccc }
    \toprule 
     Epoch &1 & 5 & 10 &30 &50\\    
    \midrule
Accuracy    &63.96$\pm$9.03 & 71.38$\pm$7.76& 70.84$\pm$8.88  & 72.35$\pm$9.01 & \textbf{76.92}$\pm$5.08\\    \bottomrule
  \end{tabular*}
\end{table}

\subsubsection{{Discriminator Architectures }}

{Table~\ref{tab:DiscStruct} presents the number of parameters, the number of FLOPs required for the forward computation, and the  performance of FedGO on CIFAR-10 with $\alpha = 0.1$ at the 100-th communication round, when the following three different client discriminator structures are used: 
\begin{itemize}
\item CNN: The baseline architecture used in the main setting. It consists of four convolutional layers.
\item CNN+MLP: A variation of the CNN architecture, where the last two convolutional layers in the CNN are replaced by a single multi-layer perceptron (MLP) layer, resulting in a three-layer shallow network.
\item ResNet: A deeper architecture based on ResNet-8, an 8-layer residual network.
\end{itemize}}


\begin{table}[!ht]
\caption{{Server test accuracy (\%) of FedGO on CIFAR-10 with $\alpha=0.1$ at the 100-th communication round along with the number of parameters and the number of FLOPs for the forward computation, according to different client discriminator structures. }}
  \vskip 0.1in
\centering
\begin{tabular}{cccc}
\toprule
&\multicolumn{3}{c}{FedGO}\\
\cmidrule(r){2-4}
Discriminator Structure       & CNN & CNN+MLP  & ResNet \\ \midrule
Number of Parameters    & 662,528  & 142,336& 1,230,528     \\
FLOPs         & 17.6 MFLOPs & 9.18 MFLOPs & 51.1 MFLOPs     \\ \midrule
Server Test Accuracy & 79.62$\pm$4.36 & 79.71$\pm$4.71 & 78.73$\pm$5.03\\
\bottomrule

\end{tabular}
\label{tab:DiscStruct}
\end{table}

{Table~\ref{tab:DiscStruct} shows almost identical performances regardless of  client discriminator architectures, demonstrating the robustness of FedGO to the discriminator architecture. In particular, the CNN+MLP discriminator, which has less than a quarter of the parameters and around the half of the FLOPs compared to the original CNN structure,  achieves similar performance. }



\section{\sh{Comprehensive Analysis of Communication, Privacy, and Computational Complexity }}\label{asec:privacy}

\sh{Let us provide a detailed explanation for Table \ref{tab:GOsetting}. If the server dataset is available from the outset (first two rows in Table \ref{tab:GOsetting}), we only need one-shot communication of generator (from the server to the clients) and discriminators (from the clients to the server). Hence, additional communication burden and client-side privacy leakage are negligible.}
{In particular, for our experiments, the parameters of the ResNet-18 classifier are  approximately 90MB when stored as a PyTorch state\_dict. In comparison, the generator and discriminator models are 4.61MB and 2.53MB, respectively. Over 100 communication rounds, during which ResNet-18 is transmitted repeatedly, the additional communication burden introduced by FedGO is nearly negligible.} 
However, the server dataset is used for distillation for each communication round, incurring non-negligible privacy leakage on the server side. If there is no server dataset (last two rows in Table \ref{tab:GOsetting}), there is no additional privacy leakage on the server side. If we use a pretrained generator instead (third row), additional communication burden, client-side privacy leakage and computational burden are negligible, but it is challenging in general to secure an off-the-shelf generator which generates data with a distribution similar to the client data distribution. {To train a generator through FL (last row), multiple rounds of GAN exchanges between the server and clients are required; however, since our FedGO requires a small number of GAN exchanges in pre-FL, compared to the number of model exchanges in main FL, the additional communication burden, client-side privacy leakage, and computational burden are still negligible.}

\sh{In the following, we provide a quantitative analysis of additional privacy leakage of FedGO compared to FedAVG, and an explicit comparison of computational cost for FedGO and baselines.}

\subsection{Privacy Analysis}
\sh{For privacy measure, we consider local differential privacy (LDP) \cite{laplace} which is widely accepted both in academia and industry. Note that when the data is provided $n$ times by independently applying an LDP mechanism with privacy budget $\epsilon$ for each provision, the total privacy budget becomes $n\epsilon$ from the parallel composition result~\citep{dwork2014algorithmic}. }

\sh{Let $T$ denote  the total number of communication rounds in the main-FL stage. {For the case (G3) in Section \ref{sec:3.2}, let $T'$ denote the total number of communication rounds required to train a GAN in the pre-FL stage, which is \wj{1/20} of $T$ in our experiment.} For simplicity, we assume that every client participates in FL for each communication round. Let $\epsilon_M, \epsilon_D$, and $\epsilon_G$ denote the privacy budgets of LDP mechanisms applied to the classifier, discriminator, generator sent from each client at each communication round, respectively. Let $\hat{\epsilon}_M$ and $\hat{\epsilon}_G$ denote the privacy budgets of LDP mechanisms applied to the classifier and the generator sent from the server when the server uses its own dataset in case of (S1) for training the generator and for distillation, respectively.}

\sh{Table \ref{tab:9} shows the client-side and the server-side total privacy leakage of FedAVG and FedGO under various scenarios. For FedAVG, each client provides the classifier $T$ times, and hence the client-side total privacy leakage becomes $T\cdot \epsilon_M$. Let us first analyze the additional client-side privacy leakage of FedGO under various scenarios. For FedGO with the method (G1)+(D1), (G2)+(D1), or (G2)+(D2), the client sends its discriminator only once, incurring extra privacy leakage of $\epsilon_D$, which is negligible with large $T$. {For FedGO with (G3)+(D2), the clients need to send the discriminator and the generator for $T'$ times to train a GAN in the pre-FL stage, leading to additional privacy leakage of $T'\cdot (\epsilon_D+\epsilon_G)$; however, since $T'$ is only \wj{1/20} of $T$ in our experiment, the additional privacy leakage is negligible.} Next, server-side privacy issues arise  only when the server has its own dataset. If the server trains the generator from its dataset and provides it to the clients for the case of (G1), it yields the privacy leakage of $\hat{\epsilon}_G$. In addition, if the server uses its dataset for distillation and applies an LDP mechanism with privacy budget $\hat{\epsilon}_M$ to the classifier for each communication round for the case of (D1), it results in a non-negligible amount of additional privacy leakage $T\cdot \hat{\epsilon}_M$.}

\wj{We consider this as a future work direction: developing methods for training ensemble distillation while preserving LDP constraints in classifiers or discriminators.}






\begin{table}[!htb]
  \caption{\sh{Quantitative analysis of the client-side and the server-side total privacy leakage of FedAVG and FedGO under various scenarios. }}\label{tab:9}
  \vskip 0.1in
  \centering\setlength\tabcolsep{0pt}
    \begin{tabular*}{\linewidth}{@{\extracolsep{\fill}} c|cccccc }
    \toprule 
     \multicolumn{2}{c}{} &Client-side &Server-side \\    
    \midrule
 \multicolumn{2}{c}{FedAVG} &$T \cdot \epsilon_M$ &$-$  \\
 \multirow{3.8}{*}{\quad \quad FedGO  \quad\quad} & (G1)+(D1) &$T \cdot \epsilon_M + \epsilon_D$ &$\hat{\epsilon}_G + T \cdot \hat{\epsilon}_M$  \\
  &  (G2)+(D1) &$T \cdot \epsilon_M + \epsilon_D$ &$T \cdot \hat{\epsilon}_M$  \\
  &  (G2)+(D2) &$T \cdot \epsilon_M + \epsilon_D$ &$-$  \\
  &  (G3)+{(D2)} &$T' \cdot (\epsilon_D + \epsilon_G) + T \cdot \epsilon_M + \epsilon_D$ &$-$  \\
\bottomrule
  \end{tabular*}
\end{table}


\subsection{Computational Cost Comparison}
Table \ref{tab:6a} shows the floating point operations (FLOPs) during CIFAR-10 training for the baselines and FedGO \sh{with the four scenarios described in Table \ref{tab:GOsetting}}. 1 MFLOP represents \(10^6\) FLOPs.

First, on the client side, the computational cost for \sh{FedGO with (G1) or (G2)} is comparable to that of FedAVG and FedDF, which only optimize the client’s vanilla supervised loss. The cost is roughly half of the cost of FedGKD$^+$, which includes a regularization term in the client objective. This reduction is because the client only needs to train the discriminator only once during the pre-FL stage. In each round, FedAVG and FedDF compute 4.17e+7 MFLOPs per client update, whereas the computational cost for training a client's discriminator is 3.29e+7 MFLOPs—less than the cost for one round of classifier training. The additional computation cost for \sh{FedGO with (G1) or (G2)} is therefore minimal, especially considering its fast convergence speed.\footnote{We note that the computational cost exceeds 2\%, rather than being below 1\%, because in each round, only \(C=0.4\) proportion of clients are sampled to participate in federated learning, rather than full client participation.} \sh{Note that the client-side computational cost of FedProx is same as that of FedAVG because the proximal term computation is negligible.}
{For FedGO with (G3), clients train the generator using an FL approach during the pre-FL stage. Although this process adds computational overhead to the client side, the number of FL rounds required for training the generator is relatively small—only \wj{1/20} of the rounds needed for the main FL stage in our experiment. Consequently, this results in only a slight increase in the overall computational cost compared to FedAVG. In contrast, data-free FL algorithms like DaFKD~\citep{Da} exchange both the generator and model in every communication round, resulting in significant overhead associated with utilizing the generator when many rounds for model training are needed. By decoupling generator preparation from model training, FedGO with (G3)+(D2) avoids this issue, reducing the additional overhead. }

\sh{Next, on the server side, in FedGO with (G1)+(D1), training a generator using server dataset involves significant additional computation compared to FedDF due to the 100,000 steps required for training a ResNet-based generator and discriminator. However, given that federated learning typically involves a server with ample resources and clients with limited computational resources, this increase in server-side computation is more affordable in practice, compared to increasing the computational burden on clients.} {Furthermore,  while the computation for training the generator is irrelevant to the number of clients,  the computation required for pseudo-labeling scales linearly with the number of clients.  Note that the total computational cost in Table \ref{tab:6a} assumes 20 clients. In real-world scenarios, where 100+ clients may participate in FL,  the relative proportion of the computational cost for training the generator will decrease.}

\sh{Note that using an off-the-shelf generator reduces the additional server-side computational cost of FedGO. FedGO with (G2)+(D1) requires approximately 2\% more computation than FedDF, but achieves a significant performance gain of about 13\%p on CIFAR-10 with \(\alpha=0.05\) in Table \ref{tab:1}. The reason why FedGO with (G2)+(D2) has a higher server-side computational cost compared to FedGO with (G2)+(D1) is that FedGO with (G2)+(D2) generates distillation dataset using a heavy generator, StyleGAN.} %

\sh{FedGO with (G3)+(D2) also generates distillation dataset using a generator but the generator used here is lighter than StyleGAN generator used in (G2)+(D2). The computational cost for the generation of distillation dataset in FedGO with (G3)+(D2) is 2.11e+7 MFLOPs which is negligible compared to the computational cost for pseudo-labelling and ensemble distillation which is 5.07e+10 MFLOPs. On the other hand, note that the computational cost of FedGO with (G3)+(D2) is slightly lower than DaFKD while both train a generator using an FL approach. The reduction mainly comes from the difference that FedGO with (G3)+(D2) generates a distillation dataset only once after the training of the generator, while DaFKD updates the distillation dataset in every communication round. 
Finally, note that the server-side computational cost of FedGO with (G3)+(D2) is comparable to the case (G2)+(D1), even though (G3) trains a generator through an FL approach. This is because  the server's role is limited to averaging the clients' generator and discriminator, incurring negligible additional computational cost on the server side.}



\begin{table}[!htb]
  \caption{The number of MFLOPs for training our FedGO and baselines on CIFAR-10 for 100 communication rounds. %
  }\label{tab:6a}
  \vskip 0.1in
  \centering\setlength\tabcolsep{0pt}
  
  \scalebox{0.85}{
    \begin{tabular*}{1.15\linewidth}{@{\extracolsep{\fill}} cccccccccc }
    \toprule 
    &\multirow{2.4}{*}{FedAVG} &\multirow{2.4}{*}{FedProx} & \multirow{2.4}{*}{FedDF} & \multirow{2.4}{*}{FedGKD$^+$}& \multirow{2.4}{*}{DaFKD}&\multicolumn{4}{c}{FedGO (ours)}\\
    \cmidrule(r){7-10}
     &&&&&& (G1)+(D1)&(G2)+(D1) & (G2)+(D2) & (G3)+{(D2)}\\    
    \midrule
    Client-side &3.33e+10 & 3.33e+10 & 3.33e+10 & 6.67e+10 & 8.81e+11& 3.40e+10 & 3.40e+10& 3.40e+10& 3.52e+10\\
    Server-side & 7.82e+3 & 7.82e+3 & 5.00e+10& 5.00e+10 & 5.28e+10 &  1.39e+11 &5.07e+10 &  6.01e+10& 5.07e+10 \\
    Total & 3.33e+10 & 3.33e+10 & 8.33e+10 & 1.17e+11 & 9.34e+11 & 1.73e+11 & 8.47e+10&9.41e+10& 8.59e+10\\\bottomrule
  \end{tabular*}}
\end{table}


\section{Limitation}\label{limitation}
Although our FedGO algorithm can be extended to model heterogeneous scenarios as in FedDF, we found it challenging to define an optimal model ensemble for multiple hypothesis classes.  Consequently, it appears difficult to apply the results of Theorem \ref{thm:1} and Corollary \ref{cor:1} in such cases.

\section{Broader Impacts}\label{broaderimpact}
In this work, we proposed a federated learning algorithm that demonstrates strong performance in scenarios where client data is heterogeneous. This capability makes our approach highly effective for distributed learning in many practical situations, where data across different clients can vary significantly. By efficiently handling such data diversity, our algorithm holds the potential to enhance the applicability and robustness of federated learning systems in real-world applications.




\end{document}
