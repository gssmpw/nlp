\section{Related works} \label{sec:related_works}
\paragraph{Deep generative models for synthetic tabular data generation}
Generative models for tabular data have become increasingly important and have widespread applications~\cite {privacy, TabCSDI, privacy_health}. For example, CTGAN and TAVE~\citep{ctgan} deal with mixed-type tabular data generation using the basic GAN~\citep{gan} and VAE~\citep{vae} framework. GOGGLE~\citep{goggle} incorporates Graph Attention Networks in a VAE framework such that the correlation between different data columns can be explicitly learned. Recently, inspired by the success of Diffusion models in image generation, a lot of diffusion-based methods have been proposed, such as TabDDPM~\citep{tabddpm}, STaSy~\citep{stasy}, CoDi~\citep{codi}, and TabSyn~\citep{tabsyn}.

\paragraph{LLMs for synthetic data generation.}
Collecting high-quality training data for advanced deep-learning models is often costly and time-consuming. Researchers have recently explored using pre-trained large language models (LLMs) to generate synthetic datasets as a promising alternative. While LLMs have shown proficiency in generating high-quality synthetic text data, their ability to accurately replicate input data distributions at scale remains uncertain \citep{xu2024llms}. Studies like Curated-LLM \citep{cllm} demonstrate LLMs' effectiveness in augmenting tabular data in low-data scenarios, but their application to large-scale input data is still unclear. GReaT \citep{great}, another approach using GPT-2, generates synthetic tabular data but requires fine-tuning for each new dataset.

