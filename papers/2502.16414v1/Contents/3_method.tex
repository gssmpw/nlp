\section{Preliminaries}
\label{preliminaries}
\paragraph{Notation.}  Tabular dataset refers to data organized in a tabular format with $N$ row and $D$ columns, where each row denotes a data record or sample, and each column denotes an attribute or feature. Each attribute can be either discrete (e.g. categorical) or continuous (e.g. real number $\mathbb{R}$). We use $\mathbb{P}(\boldsymbol{x})$ to denote the probability distribution of $\boldsymbol{x}$.

\paragraph{Data Setup.} We have access to a training dataset of $N$ samples: $\mathcal{D}_{train} = \{\boldsymbol{x}_i\}_{i=1}^N$, each sample $\boldsymbol{x}_i$ is \textit{i.i.d.} drawn from an unknown distribution $\mathbb{P}(\boldsymbol{x})$. 

\paragraph{Objective.} The goal is to generate a \textbf{new} dataset $\mathcal{D}_{syn}=\{\hat{\boldsymbol{x}}_{i}\}_{i=1}^N$ such that $\hat{\boldsymbol{x}}_i$ is \textit{i.i.d.} sampled from $\mathbb{P}(\boldsymbol{x})$. Direct copy of training data is not allowed.

\paragraph{Serialization.}
As LLMs primarily process text input, it is necessary to convert tabular data into a suitable textual format. There are many serialization formats for tabular data, such as JSON \citep{singha2023tabular}, Markdown \citep{sui2024table}, Sentences \citep{great}, etc. Notably, the JSON format is widely supported by LLMs, with models like GPT-4o capable of generating structured outputs in JSON format through constrained decoding \citep{liu2024we}.
Therefore, in this study, we adopt a JSON format to serialize tabular data. For instance, a row from a table containing three columns—\texttt{name} (categorical), \texttt{age} (numerical), and \texttt{city} (categorical)—is transformed into a JSON object: $\{\texttt{{name:`Alice', age:25, city:`New York'}}\}$. For a table comprising $N$ rows, the serialized data becomes a list of $N$ JSON objects. See Appendix \ref{lst:json} for the implementation of the JSON schema.
During each prompting iteration, \modelname retrieves a subset of these JSON objects to serve as in-context examples. This process will be elaborated upon in subsequent sections. 

\begin{figure*}[t!]
  \centering
  \includegraphics[width=\textwidth]{Figs/main_fig.pdf}
  \caption{Overview of \modelname framework. We generate synthetic samples in batches, at each prompt iteration, \modelname retrieves a subset of real samples that acts as a \textit{residual} between the currently generated samples and the real data. The residual samples will be used as in-context examples to prompt LLMs in the next iteration. The full prompt template is available in the Appendix \ref{lst:prompt}.}
  \label{fig:overview}
\end{figure*}


\section{\modelname}
\label{method}
This section presents \modelname framework for tabular data generation. \modelname retrieves a subset of samples from the training dataset that satisfy two properties: 1) \textbf{Local}: at each prompting iteration, the LLMs can effectively extract patterns from the in-context examples; 2) \textbf{Global}: after enough iterations, the overall generated samples mimic the distribution of the real samples. In the following, we will introduce each component of \modelname in detail.

\subsection{LLM Generation with In-context Examples}
Our key observation is LLMs have strong prior distribution, and LLMs tend to generate samples following their prior distribution, neglecting the in-context examples, see Figure \ref{fig:in-context-examples}. Formally, given in-context examples, we assume the LLMs generate samples following a mixture distribution:
\begin{definition}[\textbf{LLM Generation Distribution}] \label{def:gen_dist}
Given the empirical distribution of in-context example: $\mathbb{P}_{ic}$. We define the LLMs generation distribution to be the following mixture of distributions:
\begin{equation} \label{eq:gen_dist}
\mathbb{P}_{gen} := \lambda \mathbb{P}_{llm} + (1-\lambda) \mathbb{P}_{ic}
\end{equation}
where $\mathbb{P}_{llm}$ is the prior distribution of LLMs, 
$\lambda\in[0,1]$. To sample from $\mathbb{P}_{gen}$, we first sample an index $z$ from a categorical distribution over $\{0,1\}$ with parameter $\lambda$, then sample from the corresponding distribution: 
\begin{equation} \nonumber
  \mathbb{P}_{gen}(\boldsymbol{x}) = \begin{cases} 
    \mathbb{P}_{llm}(\boldsymbol{x}) & \text{if } z = 1 \\
    \mathbb{P}_{ic}(\boldsymbol{x}) & \text{if } z = 0 
  \end{cases}
\end{equation} 
\end{definition}

Definition \ref{def:gen_dist} quantifies how the in-context examples steer the LLMs' generation from its own prior distribution towards the target distribution. Intuitively, the more in-context examples being provided, $\lambda$ will be closer to $0$, meaning the LLMs is more likely to generate samples following the empirical distribution of in-context examples. In practice, due to the limited context window of LLMs, only a small number of in-context examples can be provided, thus we expect $\lambda$ to be close to $1$. 

\subsection{In-context Examples Selection}
Recall our goal is to let LLMs generate samples that follow the same distribution as the training table, i.e. $\mathbb{P}_{gen} \approx \mathbb{P}_{train}$. It is tempting to choose the in-context examples by sampling from the empirical distribution of the training table, i.e. $\mathbb{P}_{ic} = \mathbb{P}_{train}$ \citep{cllm}. However, as the LLMs' generation is affected by the prior distribution $\mathbb{P}_{llm}$, the actual output distribution of LLMs would be $\mathbb{P}_{gen} = \lambda \mathbb{P}_{llm} + (1-\lambda) \mathbb{P}_{train}$, which is not our target distribution $\mathbb{P}_{train}$. Instead, a more plausible way is to select in-context examples s.t., when combined with $\lambda$ proportion of data generated from $\mathbb{P}_{llm}$, the resulting distribution is close to $\mathbb{P}_{train}$. In other words, the in-context examples can be understood as the \textit{residual} of $\mathbb{P}_{train}$ w.r.t. $\mathbb{P}_{llm}$. Formally, we introduce the definition of residual as follows:
\begin{definition}[\textbf{Residual}]
Let $\boldsymbol{X}$ be a set of $N$ $i.i.d.$ samples from a data distribution $\mathbb{P}(\boldsymbol{x})$, and let $\boldsymbol{Y}$ be an arbitrary set of samples with the same dimension as $\boldsymbol{X}$. We define the \textbf{residual} (abbrev. \texttt{RES}) of $\boldsymbol{X}$ w.r.t. $\boldsymbol{Y}$ as a subset of $n$ samples of $\boldsymbol{X}$ such that, when concatenated with $\boldsymbol{Y}$, the empirical distribution of the concatenated samples is most similar to the data distribution $\mathbb{P}(\boldsymbol{x})$:
\begin{equation}
  \label{eq:residual}
  \texttt{RES}(\boldsymbol{X}, \boldsymbol{Y}, n) := \mathop{\arg\min}_{\boldsymbol{X}' \subseteq \boldsymbol{X}, |\boldsymbol{X}'| = n} d(\boldsymbol{X}, \boldsymbol{Y} \cup \boldsymbol{X}')
\end{equation}
where $d$ can be any distance metric between two empirical distributions.
\end{definition}

\begin{remark}
  In our case, $\boldsymbol{X}$ is the real tabular samples, and $\boldsymbol{Y}$ is the current generated samples by a LLM. Intuitively, the residual samples capture the part of the real samples that LLM has not yet grasped, thus named as \textit{residual}. To prevent overly long context prompts when interacting with the LLM, we enforce an upper-bound 
  $n$ on the size of the residual samples. 
  In our experiments, we set $n=500$ and instantiate $d$ as Jensen-Shannon Divergence (JSD) and Kolmogorov-Smirnov Distance (KSD).
\end{remark}

% The brute-force way of computing the residual is computationally infeasible for large $N$ and $n$. To address this challenge, we propose a simple heuristic for sampling the residual, we present the details in the following section.
Since brute-force way of computing the residual is computationally prohibitive for large $N$ and $n$, we introduce a heuristic for sampling the residual. We describe details in the following—pseudo-code is provided in Appendix \ref{appendix:res_alg}.

\subsection{Compute Residual}
We propose to use a simple heuristic to shrink the search space. Specifically, we first randomly select a column, then we group the real samples $\boldsymbol{X}$ based on the value of the selected column\footnote{For categorical columns, we group by the categorical values. For continuous columns, we discretize them into a fixed number of bins and group by the bin index.}. Each group of samples is then concatenated with the generated samples $\boldsymbol{Y}$. Finally, we select the group that has the smallest distance to the real samples $\boldsymbol{X}$ as the residual. The time complexity of this heuristic search algorithm is $O(N)$. Additionally, the final residual subset always exhibits a consistent pattern—either sharing the same category or falling within a narrow numerical range in one of its columns. We hypothesize that this simple pattern makes the residual samples particularly effective as in-context examples for LLMs (see Figure \ref{fig:in-context-examples} (c)). 

\input{Tables/fidelity}

% \begin{remark}
% Note that in the heuristic search, we further constrain the residual samples to be selected from grouped samples, i.e. the subset always has the same attribute value on the selected column. This design choice can make the pattern of the residual samples more simple and consistent, thus easier for LLMs to do in-context learning.
% \end{remark}


\subsection{Table Generation by \modelname}
\modelname can be easily integrated with LLMs to generate high-quality synthetic tabular data. See Fig. \ref{fig:overview} for an overview of the procedure. Here are the concrete steps involved in this procedure:
\begin{enumerate}[leftmargin=*]
  \item \textbf{In-context Prompting:}  For the first iteration, we randomly select $n$ samples from the real dataset $\boldsymbol{X}$ as the initial set of in-context examples.
  Otherwise, we plug the residual samples computed in the previous iteration into the prompt template to prompt LLMs. 
  We append the generated samples into $\boldsymbol{Y}$.
  \item \textbf{Residual Computation:} We then compute the residual of $\boldsymbol{X}$ w.r.t. $\boldsymbol{Y}$: $\texttt{RES}(\boldsymbol{X}, \boldsymbol{Y}, n)$. Specifically, if the current iteration is an even number, we instantiate $d$ as JSD, otherwise, we instantiate $d$ as KSD.
  \item \textbf{Iterative Refinement:} Repeat the above steps until enough synthetic samples are generated.
\end{enumerate}

