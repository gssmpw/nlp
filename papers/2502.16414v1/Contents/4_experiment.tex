\section{Experiments}
\label{experiments}
We validate the performance of \modelname through extensive experiments. In particular, we investigate the following questions:
\begin{itemize}[leftmargin=*]
    \item Can \modelname improve generation quality compared to previous LLM-based methods? (Table. \ref{tbl:fidelity}, \ref{tbl:utility}, Fig. \ref{fig:2d-california})
    \item How does \modelname perform, compared to training-based deep generative models, under a data-scarcity setting? (Fig. \ref{fig:low-resource})
    \item Does \modelname generate new synthetic data instead of copying the training dataset? (Fig. \ref{fig:dcr})
\end{itemize}

\subsection{Setup}
\paragraph{Datasets.}
We select five real-world tabular datasets containing both numerical and categorical attributes: \textbf{Adult, Default, Shoppers, Magic} and \textbf{California}. The statistics of the datasets are summarized in Table \ref{tbl:stat-dataset} in Appendix \ref{appendix:datasets}.

\paragraph{Baselines.}
To comprehensively assess \modelname's performance, we conduct comparisons against a wide range of traditional deep generative models and LLM-based methods, which we categorize into the following two groups:
\begin{itemize}[leftmargin=*]
    \item \textbf{Deep generative models:} 1) VAE-based method TVAE \citep{ctgan}, 2) GAN-based method CTGAN \citep{ctgan}, 3) Diffusion-based method TabSyn \citep{tabsyn}, TabDDPM \citep{tabddpm}, CoDi \citep{codi}, STaSy \citep{stasy}, 4) Autoregressive method TabMT \citep{tabmt}, RealTabformer (RTF) \citep{realtabformer}.
    \item \textbf{LLM-based methods:} 1) with fine-tuning: GReaT \citep{great} 2) without fine-tuning: CLLM \cite{cllm}. CLLM was originally employed with GPT-3.5 and GPT-4, to ensure a fair comparison to CLLM, we employ CLLM with stronger models: GPT-4o-mini and GPT-4o, and we keep all the other experimental settings the same as ours.
\end{itemize}
To the best of our knowledge, CLLM \cite{cllm} is the only previous work that is training-free and solely based on in-context learning (excluding the curation step). \modelname falls in this setting.

\paragraph{Implementation details.}
Our main experiments employ GPT-4o-mini and GPT-4o as the LLMs. For all LLMs, we set the temperature to $1.0$. We generate 3000 samples ($N=3000$) for each dataset.
Each experiment is conducted 5 times and the average results are reported. 

% For hyperparameters in Equation \ref{eq:residual}, we set the number of selected in-context examples as $500$ and the distance metric as Jensen-Shannon Divergence (JSD) and Kolmogorov-Smirnov Distance (KSD).


\paragraph{Evaluation metrics.}
We evaluate the synthetic tabular data from three distinct dimensions: \circlednumber{1} \textit{Fidelity} - if the synthetic data faithfully recovers the ground-truth data distribution. We evaluate fidelity by 5 metrics: 1) Marginal distribution through Kolmogorov-Sirnov Test, 2) Pair-wise column correlation (Corr.) by computing Pearson Correlation, 3) Classifier Two Sample Test (C2ST) 4) Precision and Recall,  5) Jensen-Shannon Divergence (JSD). \circlednumber{2} \textit{Utility} - the utility of the synthetic data when used to train downstream models, we use the Train-on-Synthetic-then-Test (TSTR) protocol to evaluate the AUC score of XGBoost model on predicting the target column of each dataset. \circlednumber{3} \textit{Privacy} - if the synthetic data is not copied from the real records, we employ the Distance to Closest Record (DCR) metric.
We defer the full description of the metrics to Appendix \ref{appendix:metrics}.


Notably, previous works \citep{great, cllm} on evaluating LLMs for tabular data generation focus only on Machine Learning Utility and Privacy protection. Our paper fills this gap by providing the first comprehensive evaluation of LLMs' ability on tabular data synthesis.

\input{Tables/utility}

\subsection{\modelname outperforms LLM-based baseline methods} \label{sec:bsl}


As shown in Table \ref{tbl:fidelity}, \modelname consistently outperforms current LLM-based approaches on fidelity metrics, including both the training-free method CLLM and the fine-tuning-based method GReaT. Specifically, when using GPT-4o-mini, \modelname improves fidelity scores by a margin of 3.5$\%–42.2\%$, and when using GPT-4o, the improvements range from $1.6\%$ to 34.1\% across various metrics. Notably, the highest gains are observed in Recall: 42.2\% improvement with GPT-4o-mini and 34.1\% with GPT-4o.
Recall measures whether the synthetic data adequately covers the diverse spectrum of the real data; thus, improved Recall signifies enhanced diversity in the synthesized samples. This significant improvement is attributable to \modelname’s strategy of computing residual samples at each prompt iteration. These residual samples target underrepresented regions of the data distribution, thereby enriching the overall diversity of the synthetic data. This observation further validates the effectiveness of \modelname’s residual-based iterative refinement mechanism.

\subsection{\modelname outperforms deep generative models under data-scarcity}

One important application of tabular data synthesis is addressing data scarcity. In many cases, we have access to only a limited number of real data points, yet we require a much larger dataset to adequately train our downstream models. To generate sufficient training data, generative models can be employed.
In our experiments, we evaluate the performance of \modelname in comparison with other deep generative models under data-scarce conditions. To simulate such scenarios, we created training sets by randomly sampling 100, 500, 1000, 2000, and 3000 rows from the Default dataset. The generative models were then trained on these subsets, and the synthesized data’s quality was evaluated using the original full training set of 30,000 rows.
As shown in Figure \ref{fig:low-resource}, deep generative models like TVAE, CTGAN, and TabDDPM exhibit a significant drop in performance when trained on limited data. In contrast, \modelname and CLLM maintain performance comparable to the full-data setting. This is attributed to the strong prior distribution provided by large language models (LLMs). Notably, the performance between \modelname and CLLM is very similar because, in data-scarce scenarios, the entire training set can be used as in-context learning examples for the LLMs. Hence, the \textit{residual} sampling effectively degenerates to random sampling.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.95\linewidth]{Figs/few_shot_quality_score.pdf}
  \caption{\textbf{Quality comparison under data-scarcity}. \modelname and CLLM achieves the highest quality score under the few-shot setting. TabSyn and GReaT fail to generate meaningful data.}
  \label{fig:low-resource}
\end{figure}

\subsection{\modelname does not copy training data}
In Figure \ref{fig:dcr}, we illustrate the distributions of the L2 distances between the synthetic data and both the training and holdout datasets for CLLM, GReaT, REaLTabFormer (RTF), and \modelname. Notably, \modelname exhibits nearly identical distributions for the training and holdout sets, suggesting it is less prone to copying the training data. In contrast, CLLM and GReaT show more disparate distributions, indicating a higher likelihood of relying on the training data.


%In contrast, CLLM, GReaT and RealTabformer (RTF) have higher proportions of synthetic data closer to the training set, indicating that these methods are more likely to copy the training data.
\vspace{-2em}
\begin{figure*}[t!]
  \centering
  \includegraphics[width=1.0\textwidth,angle=0]{Figs/dcr.pdf}
  \caption{\textbf{Privacy comparison}: Distributions of the DCR scores between the synthetic dataset and the training/holdout datasets. \modelname and Curated-LLM (CLLM) are both employed with GPT-4o-mini. \label{fig:dcr}}
\end{figure*}

\begin{figure*}[h!]
  \centering
  \vspace{-10pt}
  \includegraphics[width=1.0\linewidth,height=0.30\textheight]{Figs/california_combined.pdf}
  \caption{\textbf{Visual comparison}: 2D scatter plot of \texttt{Longitude} and \texttt{Latitude} attributes of California dataset. Real represents the original training datasets. All sets are downsampled to 3000 rows for better visualization.  \modelname generates spatially coherent synthetic data that closely matches the distribution of the original dataset.}
  \label{fig:2d-california}
  \vspace{-10pt}
\end{figure*}

\vspace{16pt}

\subsection{Ablation Study}
\paragraph{Effect of $d$.} We examine the effect of the distribution distance metric $d$ used for quantifying residual in Equation \ref{eq:residual}. We test \modelname (w. GPT-4o-mini) with only KSD or JSD metric and compare it with our alternating strategy (KSD+JSD) on the California dataset. As shown in Table \ref{tbl:distance}, the alternating strategy achieves the best performance. 

\begin{table}[h]
\vspace{-2mm}
\centering
\begin{tabularx}{\linewidth}{%
  >{\centering\arraybackslash}X | 
  >{\centering\arraybackslash}X | 
  >{\centering\arraybackslash}X | 
  >{\centering\arraybackslash}X%
}
\toprule
\textbf{$d$} & KSD & JSD & KSD+JSD \\
\midrule
Marg. & $92.43${\tiny$\pm 0.005$} & 90.72{\tiny$\pm 0.009$} & $92.48${\tiny$\pm 0.008$} \\[1mm]
Corr. & $88.41${\tiny$\pm 0.022$} & 90.67{\tiny$\pm 0.021$} & $91.24${\tiny$\pm 0.016$} \\
\bottomrule
\end{tabularx}
%\vspace{-.3em}
\caption{Ablation study for $d$.}
\label{tbl:distance}
\end{table}
\vspace{-1.0em}

\paragraph{Effect of Large Language Models.}
In this section, we investigate the impact of large language model (LLM) capabilities on \modelname's performance. We evaluate \modelname using LLMs of varying parameter sizes, including Gemini-1.5-Flash, Gemini-1.5-Pro \citep{gemini}, Claude-3-Haiku, Claude-3-Sonnet \citep{claude3}, LLaMA-3.1 8B, LLaMA-3.1 405B \citep{llama3}, and Qwen2 \citep{qwen2}. We assess the marginal metric on the California dataset, with results presented in Table \ref{tbl:ablation_model}. Our findings reveal a correlation between LLM capacity and synthetic data generation quality. As the LLMs' capacity increases, the quality of generated synthetic data improves. We hypothesize that this improvement stems from larger models' enhanced ability to capture and reproduce complex patterns within the data, resulting in more realistic synthetic outputs. This relationship underscores the importance of model capacity in generating high-quality synthetic data.

\input{Tables/ablation_model}



% \subsection{Visualization}
% In Fig.~\ref{fig:2d-california}, we present a qualitative comparison of the original and generated samples by \modelname and baseline methods, for the (\texttt{Longitude}, \texttt{Latitude}) feature of the California dataset. We observe that CLLM fails to generate samples in certain regions, e.g. (\texttt{Longitude}: $[-122, -120]$, \texttt{Latitude}: $[40, 42]$), while \modelname generates realistic samples in this region. Overall, \modelname generates the most realistic samples, compared to other models.

% \begin{figure*}[h]
%   \centering
%   \vspace{-10pt}
%   \includegraphics[width = 1.0\linewidth]{Figs/california_combined.pdf}
%   \caption{2D joint distribution density of `Longitude' and `Latitude' of California dataset.} 
%   \label{fig:2d-california}
% \end{figure*}

