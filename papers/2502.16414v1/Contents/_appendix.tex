\onecolumn
\section{Appendix} \label{sec:appendix}

\subsection{Prompts used for generating tabular data} \label{lst:prompt}
This prompt template is used in Section~\ref{method} to generate realistic data that follows the same distribution as the given real data.

% [language=java]
\begin{lstlisting} 
You are a synthetic data generator tasked with creating new tabular data samples that closely mirror the distribution and characteristics of the original dataset.

# Instruction
1. Analyze the provided real samples carefully.
2. Generate synthetic data that maintains the statistical properties of the real data.
3. Ensure all attributes cover their full expected ranges, including less common or extreme values.
4. Maintain the relationships and correlations between different attributes.
5. Preserve the overall distribution of the real data while introducing realistic variations.

# Key points to consider
- Replicate the data types of each column (e.g., numerical, categorical).
- Match the range and distribution of numerical attributes.
- Maintain the frequency distribution of categorical attributes.
- Reflect any patterns or trends present in the original data.
- Introduce realistic variability to avoid exact duplication.

# Real samples
{data}

# Output format:
Please present the generated data in a JSON format, structured as a list of objects, where each object represents a single data point with all attributes.

\end{lstlisting}

\subsection{Dummy Prompt} \label{appendix:dummy_prompt}
The following prompt only contains the column names, but not any actual data in it. It is used to produce the results in Fig.\ref{fig:in-context-examples} (a).
\begin{lstlisting}
You are a synthetic data generator tasked with creating new tabular data samples that closely mirror the distribution and characteristics of the original dataset.
Generate 50 samples of synthetic data.
    
Each sample should include the following attributes:
{attributes_list}

Make sure that the numbers make sense for each attribute. 

Output Format:
Present the generated data in a JSON format, structured as a list of objects, where each object represents a single data point with all attributes.

\end{lstlisting}


\subsection{JSON Schema} \label{lst:json}
The following code define the JSON data class for the structured output function of GPT-4o and GPT-4o-mini.
\begin{lstlisting}
def create_json_model(df: pd.DataFrame, dataname=None) -> BaseModel:
    fields = {}
    
    for column in df.columns:
        if df[column].dtype == 'object':  
            fields[column] = (str, ...)
        elif df[column].dtype == 'int64':
            fields[column] = (int, ...)
        elif df[column].dtype == 'float64':
            fields[column] = (float, ...)
        elif df[column].dtype == 'bool':
            fields[column] = (bool, ...)
        else:
            raise TypeError(f"Unexpected dtype for column {column}: {df[column].dtype}")
    
    JSONModel = create_model(dataname, **fields)
    
    class JSONListModel(BaseModel):
        JSON: List[JSONModel]

    return JSONListModel
\end{lstlisting}

\subsection{Heuristic for computing residual}
In this section, we provide the pseudo-code of our heuristic strategy for computing the residual.
\begin{algorithm}
\caption{Compute residual \label{appendix:res_alg}}
\begin{algorithmic}[1]
\Require current dataset $X$, target dataset $Y$, distribution distance $d$.
\State Randomly select a column index $j$
\If{column $j$ is categorical}
    \State Let $C_j$ be the number of categories in column $j$
    \State Group samples in $Y$ into $C_j$ number of subsets based on its category on column $j$, denote the set of subsets by $(Y_j^i)_{i=1}^{C_j}$
\Else
    \State Quantize column $i$ into 50 bins
    \State $C_j\leftarrow$ 50
    \State Group samples in $Y$ into $C_j$ number of subsets based on its bin index on column $j$, denote the set of subsets by $(Y_j^i)_{i=1}^{C_j}$
\EndIf
\For{$i = 1$ to $C_j$} 
    \State Compute distance between $Y_j^i \cup X$ and $Y$: $d_i = d(Y_j^i \cup X, Y)$
\EndFor
%\State Sort distances in ascending order
\State \Return subset $Y_j^i$ that attains the minimal distance.
\end{algorithmic}
\end{algorithm}

\subsection{Datasets} \label{appendix:datasets}
We use five real-world datasets of varying scales, and all of them are available at Kaggle\footnote{\url{https://www.kaggle.com/}} or the UCI Machine Learning repository\footnote{\url{https://archive.ics.uci.edu/}}. We consider five datasets containing both numerical and catergorical attributes: California\footnote{\url{https://www.kaggle.com/datasets/camnugent/california-housing-prices}}, Magic\footnote{\url{https://archive.ics.uci.edu/dataset/159/magic+gamma+telescope}}, Adult\footnote{\url{https://archive.ics.uci.edu/dataset/2/adult}}, Default\footnote{\url{https://archive.ics.uci.edu/dataset/350/default+of+credit+card+clients}}, Shoppers\footnote{\url{https://archive.ics.uci.edu/dataset/468/online+shoppers+purchasing+intention+dataset}}. The statistics of these datasets are presented in Table~\ref{tbl:stat-dataset}. 
\begin{table}[h] 
    \centering
    \caption{Statistics of datasets. \# Num stands for the number of numerical columns, and \# Cat stands for the number of categorical columns.} 
    \label{tbl:stat-dataset}
    \small
    \begin{threeparttable}
    {
    \scalebox{0.95}
    {
	\begin{tabular}{lccccccccc}
            \toprule[0.8pt]
            \textbf{Dataset}  &  \# Rows  & \# Num & \# Cat & \# Train  & \# Test  \\
            \midrule 
            \textbf{California} Housing  & $20,640$ & $9$ & 1 & $18,390$ & $2,250$   \\
            % \textbf{Letter} Recognition & $20,000$ & $16$ & 1 & $14,000$ & $6,000$ \\
            % \textbf{Gesture} Phase Segmentation & $9,522$ & $32$ & 1 & $6,665$ & $2,857$ \\
            \textbf{Magic} Gamma Telescope & $19,021$ & $10$ & 1 & $17,118$ & $1,903$  \\
            % Dry \textbf{Bean} & $13,610$ & $17$ & - & $9,527$ & $4,083$ \\
            % \midrule
            \textbf{Adult} Income & $32,561$ & $6$ & $8$ & $22,792$ & $9,769$ \\
            \textbf{Default} of Credit Card Clients & $30,000$ & $14$ & $10$ & $27,000$ & $3,000$\\
            Online \textbf{Shoppers} Purchase & $12,330$ & $10$ & $7$ & $11,098$ & $1,232$ \\
            % \textbf{Beijing} PM2.5 data & $43,824$ & $6$ & $5$ & $29,229$ & $12,528$\\
            % Online \textbf{News} Popularity & $39,644$ & $45$ & $2$ & $27,790$ & $11,894$  \\
		\bottomrule[1.0pt] 
		\end{tabular}
   }
  }        
  \end{threeparttable}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Evaluation Metrics} \label{appendix:metrics}
\paragraph{Fidelity}
To evaluate if the generated data can faithfully recover the ground-truth data distribution, we employ the following metrics: 
1) \textbf{Marginal distribution:} The Marginal metric evaluates if each column's marginal distribution is faithfully recovered by the
synthetic data. We use Kolmogorov-Sirnov Test for continuous data and Total Variation Distance for discrete data.
2) \textbf{Pair-wise column correlation}: This metric evaluates if the correlation between every two columns in the real data is captured by the synthetic data. We compute the Pearson Correlation between all pairs of columns then take average. In addition, we present joint density plots for the Longitude and Latitude features in the California Housing data set in Figure \ref{fig:2d-california}.  
3) \textbf{Classifier Two Sample Test (C2ST):} This metric evaluates how difficult it is to distinguish real data from synthetic data. Specifically, we create an augmented table that has all the rows of real data and all the rows of synthetic data. Add an extra column to keep track of whether each original row is real or synthetic. Then we train a Logistic Regression classifier to distinguish real and synthetic rows. 
4) \textbf{Precision and Recall:} Precision measures the quality of generated samples. High precision means the generated samples are realistic and similar to the true data distribution. Recall measures how much of the true data distribution is covered by the generated distribution. High recall means the model captures most modes/variations present in the true data.
5) \textbf{Jensen-Shannon Divergence (JSD):} This metric evaluates the Jensen-Shannon divergence \citep{jsd} between the distributions of real data and synthetic data.


\paragraph{Utility} 
We evaluate the utility of the generated data by assessing their performance in Machine Learning Efficiency (MLE). Following the previous works \cite{tabsyn},  we first split a real table into a real training and a real testing set. The generative models are trained on the real training set, from which a synthetic set of equivalent size is sampled. This synthetic data is then used to train a classification/regression model (XGBoost Classifier and XGBoost Regressor~\citep{xgboost}), which will be evaluated using the real testing set. The performance of MLE is measured by the AUC score for classification tasks and RMSE for regression tasks.

\paragraph{Privacy}
A high-quality synthetic dataset should accurately reflect the underlying distribution of the original data, rather than merely replicating it. To assess this, we employ the Distance to Closest Record (DCR) metric. We begin by splitting the real data into two equal parts: a training set and a holdout set. Using the training set, we generate a synthetic dataset. We then measure the distances between each synthetic data point and its nearest neighbor in both the training and holdout sets. In theory, if both sets are drawn from the same distribution, and if the synthetic data effectively captures this distribution, we should observe an equal proportion (around 50$\%$) of synthetic samples closer to each set. However, if the synthetic data simply copies the training set, a significantly higher percentage would be closer to the training set, well exceeding the expected $50\%$.


\subsection{Scalability of \modelname}
To evaluate the scalability of \modelname, we compare \modelname with CLLM on a large-scale dataset: Covertype dataset. This dataset consists of 581,012 instances and 54 features. \modelname and CLLM use iterative in-context learning to generate samples, thus the running time of these two methods are agnostic to the size of the training dataset, making them scalable to large datasets. In the following table, we compare \modelname with CLLM, employed with both GPT-4o mini and GPT-4o. 
\begin{table}[!t]
\begin{center}
\caption{Performance Comparison on the \textbf{Covertype} Dataset. For all the metrics except AUC, we scale the metrics to a base of 
$10^{-2}$, and reverse it so that lower values indicate better performance. For AUC, the higher the better.}
\begin{tabular}{lrrrrrrr}
\hline
Model & Marginal $\downarrow$ & Corr $\downarrow$ & C2ST $\downarrow$ & Precision $\downarrow$ & Recall $\downarrow$ & JSD $\downarrow$ & AUC $\uparrow$ \\
\hline
%\multicolumn{8}{l}{\textbf{Performance with GPT-4o}} \\
\hline
CLLM w. GPT-4o & 3.28 & 10.70 & 55.67 & 32.32 & 1.95 & 0.6078 & 0.8822 \\
TabGEN w. GPT-4o & 2.83 & 11.10 & 49.91 & 24.03 & 1.27 & 0.5109 & 0.9070 \\
Improvement (\%) & $\redbf{13.72\%}$ & - & $\redbf{10.34\%}$ & $\redbf{25.62\%}$ & $\redbf{34.87\%}$ & $\redbf{15.94\%}$ & $\redbf{2.81\%}$ \\
\hline
%\multicolumn{8}{l}{\textbf{Performance with GPT-4o mini}} \\
\hline
CLLM w. GPT-4o mini & 5.35 & 13.70 & 0.7796 & 0.3225 & 0.0591 & 0.8743 & 0.7113 \\
TabGEN w. GPT-4o mini & 5.09 & 12.62 & 0.7554 & 0.2876 & 0.0436 & 0.9353 & 0.8311 \\
Improvement (\%) & $\redbf{4.86\%}$ & $\redbf{7.88\%}$ & $\redbf{3.11\%}$ & $\redbf{10.81\%}$ & $\redbf{26.25\%}$ & - & $\redbf{16.86\%}$ \\
\hline
\end{tabular}
\end{center}
\end{table}

The results demonstrate that \modelname outperforms CLLM on most of the metrics. Notably, the Recall metric again shows the greatest improvement: 34.85\% on GPT-4o and 26.25\% on GPT-4o mini. This observation is consistent with our original findings in Sec. \ref{sec:bsl}. We believe these results strongly support \modelname's scalability to larger, more complex datasets.

% \subsection{Experimental Results}

% \begin{table}[h] 
%   \centering
%   \caption{Error rate (\%) of \textbf{column-wise density estimation}. Lower values indicate more accurate estimation (superior results). } 
%   \label{tbl:exp-shape}
%   \small
%   \begin{threeparttable}
%   {
%   \scalebox{0.92}
%   {
% \begin{tabular}{lccccc|cccc}
%           \toprule[0.8pt]
%           \textbf{Method} & \textbf{Adult} & \textbf{Default} & \textbf{Shoppers} & \textbf{Magic}  & \textbf{California}&  \textbf{Average}  \\
%           \midrule 
%            SMOTE & $1.60${\tiny$\pm 0.23$} & $1.48${\tiny$\pm0.15$} & $2.68${\tiny$\pm0.19$} & $0.91${\tiny$\pm0.05$}  &$1.15${\tiny$\pm0.14$} &    $2.30$ & \\
%           \midrule
%           CTGAN    & $16.84${\tiny$\pm$ $0.03$} & $16.83${\tiny$\pm$$0.04$} & $21.15${\tiny$\pm0.10$} & $9.81${\tiny$\pm0.08$}  &$12.84${\tiny$\pm0.09$} & $17.02$  \\
%           TVAE     & $14.22${\tiny$\pm0.08$} & $10.17${\tiny$\pm$$0.05$} & $24.51${\tiny$\pm0.06$} & $8.25${\tiny$\pm0.06$}  &  $5.37${\tiny$\pm0.06$}  & $15.49$   \\
%           GReaT     & $12.12${\tiny$\pm$$0.04$} & $19.94${\tiny$\pm$$0.06$}  & $14.51${\tiny$\pm0.12$}  &  $16.16${\tiny$\pm0.09$}   & $10.25${\tiny$\pm0.20$}  & $14.20$ \\
%           STaSy    & $11.29${\tiny$\pm0.06$} & $5.77${\tiny$\pm0.06$} & $9.37${\tiny$\pm0.09$} & $6.29${\tiny$\pm0.13$}   & $30.09${\tiny$\pm0.12$}  &   $7.72$ \\
%           CoDi  & $21.38${\tiny$\pm0.06$}  & $15.77${\tiny$\pm$ $0.07$}  & $31.84${\tiny$\pm0.05$}  & $11.56${\tiny$\pm0.26$}  & $16.94${\tiny$\pm0.02$} &    $21.63$  \\
%           TabDDPM & $1.75${\tiny$\pm0.03$}  & $1.57${\tiny$\pm$ $0.08$}  & $2.72${\tiny$\pm0.13$}  & $1.01${\tiny$\pm0.09$}   & $19.98${\tiny$\pm0.05$}  &  $14.52$ \\
%           TabSyn   & $0.58${\tiny$\pm0.06$} & $0.85${\tiny$\pm0.04$} & $1.43${\tiny$\pm0.24$} & $0.88${\tiny$\pm0.09$}  & $1.12${\tiny$\pm0.05$}  &  $1.08$ \\
%           \midrule
%           \modelname  & $13.19${\tiny$\pm0.16$} & $10.99${\tiny$\pm0.33$} & $5.10${\tiny$\pm0.24$} & $6.58${\tiny$\pm0.55$}  & $4.49${\tiny$\pm0.21$}  &  $8.07$ \\
%           % Improv. &  $\redbf{66.9 \% \downarrow} $ & $ \redbf{45.9\% \downarrow} $ & $\redbf{47.4\% \downarrow}$ & $\redbf{12.9\% \downarrow} $  & $\redbf{13.8\% \downarrow }$  &  $\redbf{76.2\% \downarrow }$ &  $\redbf{86.0\% \downarrow }$ &  \\
%   \bottomrule[1.0pt] 
%   \end{tabular}
%             }
%             }
%     % \begin{tablenotes}
%     %       \item[1] GOGGLE fixes the random seed during sampling in the official codes, and we follow it for consistency.
%     %       \item[2] GReaT cannot be applied on News because of the maximum length limit.
%     % \item[3] TabDDPM fails to generate meaningful content on the News dataset.
%     %   \end{tablenotes}
% \end{threeparttable}

% \end{table}

% \begin{table}[h] 
%   \centering
%   \caption{Error rate (\%) of \textbf{pair-wise column} correlation score. } 
%   \label{tbl:exp-trend}
%   \small
% % \begin{threeparttable}
%   {
%   \scalebox{0.93}
%   {
% \begin{tabular}{lccccc|ccc}
%           \toprule[0.8pt]
%           \textbf{Method} & \textbf{Adult} & \textbf{Default} & \textbf{Shoppers} & \textbf{Magic}  & \textbf{California} & \textbf{Average}  \\
%           \midrule 
%            SMOTE & $3.28${\tiny$\pm0.29$} & $8.41${\tiny$\pm0.38$} & $3.56${\tiny$\pm0.22$} & $3.16${\tiny$\pm0.41$}  &$2.39${\tiny$\pm0.35$} &    $4.36$ & \\
%           \midrule
%           CTGAN    & $20.23${\tiny$\pm1.20$} & $26.95${\tiny$\pm0.93$} & $13.08${\tiny$\pm0.16$} & $7.00${\tiny$\pm0.19$} &  $22.95${\tiny$\pm0.08$}  &   $15.93$     \\
%           TVAE     & $14.15${\tiny$\pm0.88$} & $19.50${\tiny$\pm$$0.95$} & $18.67${\tiny$\pm0.38$} &  $5.82${\tiny$\pm0.49$}  &  $18.01${\tiny$\pm0.08$}  & $13.72$   \\
%           GReaT    & $17.59${\tiny$\pm0.22$} & $70.02${\tiny$\pm$$0.12$} & $45.16${\tiny$\pm0.18$} & $10.23${\tiny$\pm0.40$}  & $59.60${\tiny$\pm0.55$}    & $44.24$  \\
%           STaSy    & $14.51${\tiny$\pm0.25$} & $5.96${\tiny$\pm$$0.26$}  & $8.49${\tiny$\pm0.15$} & $6.61${\tiny$\pm0.53$}  & $8.00${\tiny$\pm0.10$} &  $7.77$     \\
%           CoDi  & $22.49${\tiny$\pm0.08$}  & $68.41${\tiny$\pm$$0.05$}  & $17.78${\tiny$\pm0.11$}  & $6.53${\tiny$\pm0.25$} & $7.07${\tiny$\pm0.15$}  & $22.23$   \\ 
%           TabDDPM & $3.01${\tiny$\pm0.25$}  & $4.89${\tiny$\pm0.10$}  & $6.61${\tiny$\pm0.16$} & $1.70${\tiny$\pm0.22$} & $2.71${\tiny$\pm0.09$} &  $5.34$ \\
%           TabSyn  & $1.54${\tiny$\pm0.27$} & $2.05${\tiny$\pm0.12$} & $2.07${\tiny$\pm0.21$}  & $1.06${\tiny$\pm0.31$}   &  $2.24${\tiny$\pm0.28$}  & $1.73$  \\
%           \midrule
%           \modelname  & $25.70${\tiny$\pm0.27$} & $22.25${\tiny$\pm0.12$} & $20.04${\tiny$\pm0.21$}  & $5.66${\tiny$\pm0.32$}   &  $10.65${\tiny$\pm0.28$}  &  $16.86$  \\
%           % Improve. &  $\redbf{48.8\% \downarrow} $ & $\redbf{58.1\% \downarrow} $ & $\redbf{68.7\% \downarrow}$  & $\redbf{37.6\% \downarrow} $  & $\redbf{17.3\% \downarrow} $  &  $\redbf{53.1\% \downarrow}$ & $\redbf{67.6\% \downarrow}$&  \\
%   \bottomrule[1.0pt] 
%   \end{tabular}
% }
% }
% % \end{threeparttable}
% \end{table}


% % \begin{table}[h] 
% %   \centering
% %   \caption{Wesserstain distance between the empirical distribution of real data and the synthetic data generated by different methods. Lower values indicate better generation performance (superior results). } 
% %   \label{tbl:exp-wasserstein}
% %   \small
% %   \begin{threeparttable}
% %   {
% %   \scalebox{0.92}
% %   {
% % \begin{tabular}{lccccc|cccc}
% %           \toprule[0.8pt]
% %           \textbf{Method} & \textbf{Adult} & \textbf{Default} & \textbf{Shoppers} & \textbf{Magic}  & \textbf{California}&  \textbf{Average}  \\
% %           \midrule 
% %            SMOTE & $0.162$ & $0.096$ & $0.210$ & $0.002$  &$0.001$ &    $0.09$ & \\
% %           \midrule
% %           CTGAN    & $0.832$ & $1.362$ & $0.891$ & $0.014$  &$0.22$ & $0.664$  \\
% %           TVAE     & $1.103$ & $0.478$ & $1.116$ & $0.006$  &  $0.007$  & $0.542$   \\
% %           % GReaT     & $12.12${\tiny$\pm$$0.04$} & $19.94${\tiny$\pm$$0.06$}  & $14.51${\tiny$\pm0.12$}  &  $16.16${\tiny$\pm0.09$}   & $10.25${\tiny$\pm0.20$}  & $14.20$ \\
% %           STaSy    & $0.630$ & $0.417$ & $0.870$ & $0.057$   & $0.701$  &   $0.535$ \\
% %           CoDi  & $1.500$  & $1.201$  & $1.293$  & $0.057$  & $0.049$ &    $0.820$  \\
% %           % TabDDPM & $1.75${\tiny$\pm0.03$}  & $1.57${\tiny$\pm$ $0.08$}  & $2.72${\tiny$\pm0.13$}  & $1.01${\tiny$\pm0.09$}   & $19.98${\tiny$\pm0.05$}  &  $14.52$ \\
% %           TabSyn   & $0.308$ & $0.201$ & $0.4370$ & $0.0032$  & $0.002$  &  $0.190$ \\
% %           \midrule
% %           \modelname  & $3.465$ & $3.478$ & $2.62$ & $0.915$  & $34.71$  &  $8.07$ \\
% %   \bottomrule[1.0pt] 
% %   \end{tabular}
% %             }
% %             }
% % \end{threeparttable}

% % \end{table}


% \begin{table}[h] 
%   \centering
%   \caption{AUC scores of \textbf{Machine Learning Efficiency}. $\uparrow$ indicates that the higher the score, the better the performance.} 
%   \label{tbl:exp-mle}
%   \small
%   \begin{threeparttable}
%   {
%   \scalebox{0.92}
%   {
% \begin{tabular}{lccccc|ccc}
%           \toprule[0.8pt]
%           \multirow{2}{*}{Methods} & {\textbf{Adult}} &{\textbf{Default}} & \textbf{Shoppers} & {\textbf{Magic}} &   {\textbf{California}}$^{1}$ & {\textbf{Average Gap}} \\
%           \cmidrule{2-8} 
%           & AUC $\uparrow$ & AUC $\uparrow$ &  AUC $\uparrow$ &  AUC $\uparrow$  & AUC $\uparrow$ &  $\%$ \\
%           \midrule 
%           Real & $.927${\tiny$\pm.000$} & $.770${\tiny$\pm.005$} & $.926${\tiny$\pm.001$}  & $.946${\tiny$\pm.001$}  & -  & $0\%$  \\
%           \midrule
%           SMOTE & $.899${\tiny$\pm.007$} &$.741${\tiny$\pm.009$} & $.911${\tiny$\pm.012$} & $.934${\tiny$\pm.008$} & -  &  $9.39\%$  \\
%           \midrule
%           CTGAN & $.886${\tiny$\pm.002$} &$.696${\tiny$\pm.005$} & $.875${\tiny$\pm.009$} & $.855${\tiny$\pm.006$}    & - & $8.4\%$ \\
%           TVAE  & $.878${\tiny$\pm.004$} &$.724 ${\tiny$\pm.005$} & $.871${\tiny$\pm.006$} & $.887${\tiny$\pm.003$} & - & $6.9\%$  \\
%           GReaT & $.913${\tiny$\pm.003$} &$.755${\tiny$\pm.006$} & $.902${\tiny$\pm.005$} & $.888${\tiny$\pm.008$}  & - &  $3.3\%$  \\
%           STaSy  & $.906${\tiny$\pm.001$} & $.752${\tiny$\pm.006$} & $.914${\tiny$\pm.005$} & $.934${\tiny$\pm.003$}  & - & $1.9\%$  \\ 
%           CoDi & $.871${\tiny$\pm.006$} & $.525${\tiny$\pm.006$} & $.865${\tiny$\pm.006$} & $.932${\tiny$\pm.003$}   & - &  $10.7\%$  \\
%           TabDDPM & $.907${\tiny$\pm.001$}  & $.758${\tiny$\pm.004$}& $.918${\tiny$\pm.005$} & $.935${\tiny$\pm.003$} & - & $1.65\%$ \\
%           TabSyn & $.915${\tiny$\pm.002$} & $.764${\tiny$\pm.004$} & $.920${\tiny$\pm.005$} & $.938${\tiny$\pm.002$} & - & $1.1\%$ \\
%           \midrule
%           \modelname  & $.894${\tiny$\pm.009$} & $.634${\tiny$\pm.112$} & $.792${\tiny$\pm.005$} & $.896${\tiny$\pm.006$} & - & $10.1\%$ &  
%           \\
%   \bottomrule[1.0pt] 
%   \end{tabular}
% }
% }
%    \begin{tablenotes}
%    \item[1] AUC score for California dataset is not available because the dataset contains extremely rare classes.
%       \end{tablenotes}
% \end{threeparttable}
% \end{table}



% \begin{table}[h]
%   \centering
%   \caption{The absolute difference between Normalized Distance to Closest Records (DCR) and 1. The smaller the better. } 
%   \label{tbl:exp-dcr}
%   \small
%   \begin{threeparttable}
%   {
%   \scalebox{0.92}
%   {
% \begin{tabular}{lccccc|cccc}
%           \toprule[0.8pt]
%           \textbf{Method} & \textbf{Adult} & \textbf{Default} & \textbf{Shoppers} & \textbf{Magic}  & \textbf{California}&  \textbf{Average}  \\
%           \midrule 
%            SMOTE & $0.411$ & $0.093$ & $0.210$ & $0.102$  &$0.103$ &    $0.184$ & \\
%           \midrule
%           CTGAN    & $0.020$ & $0.004$ & $0.017$ & $0.014$  &$0.008$ & $0.013$  \\
%           TVAE     & $0.007$ & $0.002$ & $0.024$ & $0.000$  &  $0.000$  & $0.007$   \\
%           % GReaT     & $12.12${\tiny$\pm$$0.04$} & $19.94${\tiny$\pm$$0.06$}  & $14.51${\tiny$\pm0.12$}  &  $16.16${\tiny$\pm0.09$}   & $10.25${\tiny$\pm0.20$}  & $14.20$ \\
%           STaSy    & $0.009$ & $0.005$ & $0.004$ & $0.010$   & $0.020$  &   $0.010$ \\
%           CoDi  & $0.007$  & $0.011$  & $0.005$  & $0.010$  & $0.005$ &    $0.008$  \\
%           % TabDDPM & $1.75${\tiny$\pm0.03$}  & $1.57${\tiny$\pm$ $0.08$}  & $2.72${\tiny$\pm0.13$}  & $1.01${\tiny$\pm0.09$}   & $19.98${\tiny$\pm0.05$}  &  $14.52$ \\
%           TabSyn   & $0.012$ & $0.001$ & $0.003$ & $0.002$  & $0.004$  &  $0.004$ \\
%           \midrule
%           \modelname  & $0.001$ & $0.004$ & $0.005$ & $0.042$  & $0.011$  &  $0.013$ \\
%   \bottomrule[1.0pt] 
%   \end{tabular}
%             }
%             }
% \end{threeparttable}

% \end{table}