\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{subcaption}
\usepackage{colortbl}
\usepackage{textcomp}
\usepackage{cleveref}
\usepackage{listings}
\usepackage{tcolorbox}
\usepackage{xcolor}
\usepackage{makecell}
\usepackage{xspace} 
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{array}
\usepackage{url}
\usepackage{enumitem}
\Crefformat{figure}{#2Fig.~#1#3}
\Crefmultiformat{figure}{Figs.~#2#1#3}{ and~#2#1#3}{, #2#1#3}{ and~#2#1#3}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}


\makeatletter
\DeclareDocumentCommand\highLightSwitch{m m}{%
  \setlength{\fboxsep}{1pt}\color{#1}\colorbox{#2}
}
\makeatother
\definecolor{red}{rgb}{1,0,0}
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.92,0.92,0.92}
%\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
% Define the style
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    %postbreak=\mbox{\textcolor{gray}{$\hookrightarrow$}\space},
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    %numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2,
}

\lstset{style=mystyle}


\def\name{\textsc{Metamon}\xspace}

\newcommand{\fixme}[1]{\textcolor{red}{#1}}

\begin{document}

\title{\name: Finding Inconsistencies between Program Documentation and Behavior using Metamorphic LLM Queries\thanks{This work has been supported by the National Research Foundation of Korea (NRF) funded by the Korean government MSIT (RS-2023-00208998), and the Institute of Information Communications Technology Planning \& Evaluation (IITP) grant funded by the Korea government (MSIT) (RS-2022-II220995).}}
% \title{Conference Paper Title*\\
% {\footnotesize \textsuperscript{*}Note: Sub-titles are not captured in Xplore and
% should not be used}
% }

\author{\IEEEauthorblockN{Hyeonseok Lee}
\IEEEauthorblockA{\textit{School of Computing} \\
\textit{KAIST}\\
Daejeon, Republic of Korea \\
christmas@kaist.ac.kr}
\and
\IEEEauthorblockN{Gabin An}
\IEEEauthorblockA{\textit{School of Computing} \\
\textit{KAIST}\\
Daejeon, Republic of Korea \\
gabin.an@kaist.ac.kr}
\and
\IEEEauthorblockN{Shin Yoo}
\IEEEauthorblockA{\textit{School of Computing} \\
\textit{KAIST}\\
Daejeon, Republic of Korea \\
shin.yoo@kaist.ac.kr}
}


\maketitle

\begin{abstract}
Code documentation can, if written precisely, help developers better understand the code they accompany. However, unlike code, code documentation cannot be automatically verified via execution, potentially leading to inconsistencies between documentation and the actual behavior. While such inconsistencies can be harmful for the developer’s understanding of the code, checking and finding them remains a costly task due to the involvement of human engineers. This paper proposes \name, which uses an existing search-based test generation technique to capture the current program behavior in the form of test cases, and subsequently uses LLM-based code reasoning to identify the generated regression test oracles that are not consistent with the program specifications in the documentation. \name is supported in this task by metamorphic testing and self-consistency. An empirical evaluation against 9,482 pairs of code documentation and code snippets, generated using five open-source projects from Defects4J v2.0.1, shows that \name can classify the code-and-documentation inconsistencies with a precision of 0.72 and a recall of 0.48. 
\end{abstract}

\begin{IEEEkeywords}
Metamorphic Testing, Code Documentation, Large Language Model, Oracle Problem, Regression Test
\end{IEEEkeywords}

\section{Introduction}

Code documentation, such as comments or docstrings, is a human-readable description of the source code and its behavior. While languages like Java and Python provide documentation formats that can be processed by machines to produce structured documents (\texttt{PyDoc} and \texttt{JavaDoc}, respectively), primarily code documentation aims to aid the human understanding of the code that they accompany~\cite{de2006documentation}. As such, it is critical that code documentation is consistent with the behavior of the code they accompany. If the documentation is out of sync with the program behavior, developers may understand the existing code incorrectly or imprecisely, potentially leading to buggy code changes. 

Finding such inconsistencies between code documentation and program behavior is an important but challenging problem, due to the fact that documentation is written in natural languages whereas program behavior is described in programming langauges. Checking the semantic consistency across natural and programming languages has been primarily reserved only for human developers. However, the cost of manual inspection means that such checks cannot be performed frequently enough, resulting in inconsistencies in real-world projects~\cite{evolutionOfCodeComments}. Existing attempts to detect these inconsistencies either depend on rule-based approaches~\cite{tan2007icomment, tan2011acomment, tan2012tcomment, ratol2017detecting} or textual similarities~\cite{corazza2018coherence, panthaplackel2021deep, rabbi2020detecting} to make the task more tractable.

Recently, LLMs have shown significant capabilities to perform logical reasoning across the barrier of natural and programming languages. In addition to successfully synthesizing code from natural language specifications~\cite{li2022competition}, LLMs can generate bug reproducing tests from bug reports written in natural language~\cite{kang2023large}, or incorporate error messages from synthesized test cases to improve the code coverage iteratively~\cite{schafer2023adaptive}. These capabilities are directly relevant to the task of checking documentation inconsistencies, and LLMs have been evaluated for the task~\cite{li2024mutation}. However, existing techniques based on LLMs tend to simply prompt LLMs to spot inconsistencies between code and documentation and falls short of actually checking the dynamic program behaviour. Given that LLMs can hallucinate~\cite{huang2023survey}, augmenting LLM-based inconsistency checking with a more concrete exploration of program behavior seems to be a missed opportunity.

This paper presents \name, an automated technique that finds inconsistencies between code documentation and program behavior via the use of a search-based test data generation technique and LLMs. To capture program behavior in more detail, instead of presenting the code verbatim in prompts, \name uses EvoSuite~\cite{Evosuite} to generate regression test cases. These test cases not only achieve high structural coverage (and thereby expose program behavior in more depth) but also capture the current program behavior in the form of assertions that record the program output. \name uses LLMs to judge whether these regression test oracles are correct or not with respect to the current documentation. To further enhance its performance, \name adopts a couple of prompt engineering strategies. First, \name uses metamorphic LLM queries: given a prompt $P$ and an answer $A$, \name will form a subsequent query prompt $P'$ and an expected answer $A'$ by transforming $(P, A)$ with Metamorphic Relations (MRs). Second, \name uses Chain-of-Thoughts~\cite{wei2022chain} and self-consistency~\cite{wang2022self} to improve the accuracy of the responses from the LLM. 

We evaluate \name using 9,482 pairs of Java methods and their documentation, taken from five open source projects in Defects4J~\cite{just2014defects4j} version 2.0.1: the dataset contains both consistent and inconsistent pairs, since we generate half of the pairs using the buggy versions. \name can classify the consistency between the code documentation and the program behavior captured in test cases with a precision of 0.72 and a recall of 0.48. An ablation study shows that both the metamorphic queries, and the advanced prompt engineering techniques, contribute positively to the performance. 

The technical contributions of this paper are as follows:

\begin{itemize}[leftmargin=10pt]
\item We present \name, an LLM-based technique that can check inconsistencies between documentation and program behavior. \name captures program behavior by generating regression test cases using EvoSuite, and uses metamorphic LLM queries and tailored prompt engineering techniques to enhance its performance.

\item We perform an empirical evaluation of \name based on 9,482 pairs of documentation and source code extracted from five projects in Defects4J v2.0.1. An ablation study shows that metamorphic LLM queries and self-consistency-based scoring all contribute to the final performance.

\item We make the implementation of \name, along with the dataset of 9,482 pairs of code documentation and method/test code snippets, publicly available for replicability: \url{https://figshare.com/s/dd17b119d40d4bf3176a}
\end{itemize}

The rest of the paper is structured as follows: \Cref{sec:background} provides an overview of the background and related research. \Cref{sec:methodology} presents our approach, \name. Detailed experimental settings and research questions for evaluating \name are covered in \Cref{sec:setup}, while \Cref{sec:results} details the findings, and \Cref{sec:conclusion} concludes.

\section{Background and Related Work} % 개념, metamorphic, prompt engineering
\label{sec:background}

% Existing survey work~\cite{fan2023large} noted that \emph{Test Correctness}, or Oracle Problem\fixme{Oracle Problem}, is one of the challenge for not only the traditional test generation technique but also for the LLM-based software testing.


\subsection{Code-and-Documentation Inconsistency}

Code documentation refers to the descriptions of specifications and requirements of the program, or explanations of the source code, written with the purpose of improving code readability, facilitating easier maintenance, supporting collaboration among developers, and enhancing code reusability. However, it is common for documentation and source code to become out-of-sync over time, which can happen when only code is updated without the documentation or vice versa~\cite{evolutionOfCodeComments}. Analyzing the consistency between documentation and source code has traditionally been a challenging task. Some approaches have used rule-based methods to detect specific types of inconsistencies, such as issues related to locking~\cite{tan2007icomment}, interrupt-related concurrency bugs~\cite{tan2011acomment}, null value-related exceptions~\cite{tan2012tcomment}, and identifier renaming~\cite{ratol2017detecting}. Other techniques have treated the inconsistency detection problem as a text similarity problem, employing machine learning models~\cite{corazza2018coherence, panthaplackel2021deep, rabbi2020detecting}, which often requires a dedicated training step.

Recently, LLMs have shown remarkable capabilities in understanding both natural and programming languages, enabling them to be applied to tasks such as code generation from specifications~\cite{li2022competition, shin2023prompt} or document generation from source code~\cite{sun2023automatic, ahmed2023improving, shin2023prompt}. This suggests that checking the consistency between code and documentation is now becoming increasingly feasible. For example, a recent study shows that GPT-4~\cite{achiam2023gpt} can identify subtle inconsistencies between code and its documentation~\cite{li2024mutation}. However, this study does not focus directly on the problem of inconsistency detection, but rather uses it as a means of assessing the code understanding capabilities of LLMs. In comparison, we propose a novel LLM-based approach that checks the consistency between program behavior (captured by regression tests) and their specifications captured in documentation, rather than directly comparing source code and documentation. We also introduce the concept of metamorphic prompting. 

% identifying the consistency between two artifacts becoming 


% However, LLMs have limitations in the number of tokens they can accommodate, and overly complex prompts can degrade their performance. %Therefore, relying solely on source code and documentation as prompts has its limitations. Furthermore, understanding a program's behavior based on the source code is inefficient. Hence, we propose generating regression tests based on the source code to create input-output pairs and verify their consistency with the documentation.
        
% \subsection{Large Language Models}
% LLMs are a type of Artificial Intelligence (AI) model that is trained on a massive amount of text datasets, containing text from online articles, books, and even program source code. This allows them to learn patterns and semantics of language. Since its emergence around 2019, LLMs have been increasingly applied in the field of Software Engineering (SE)~\cite{fan2023large, zhang2023survey} such as code generation~\cite{li2022competition} or test generation~\cite{lemieux2023codamosa, kang2023large}.
% % One of the key tasks in LLM for SE is software test generation, and many researches already have shown promising results. For example, combining LLMs with existing test generation tools, such as EvoSuite~\cite{Evosuite}, demonstrates higher coverage than LLM-only or traditional baselines\fixme{CODAMOSA}. LLMs have also shown promise in automatically generating tests from natural language bug reports, successfully reproducing around one-third of the reported failures\fixme{LIBRO}.

% However, despite these advances and promises, LLMs are not without weaknesses. A particularly critical issue is the tendency towards \emph{hallucinations}, where models produce outputs that are plausible yet factually incorrect. To mitigate hallucination, techniques such as self-consistency~\cite{wang2022self} and Chain-of-Thought (CoT)~\cite{wei2022chain} prompting have been actively employed. Self-consistency improves the reliability of LLMs by querying the model multiple times with the same question and identifying the most consistent answer, while CoT prompting enhances the accuracy of answers by having the model break down a complex problem into a step-by-step sequence of intermediate reasoning steps, rather than just providing a final answer.
% In our work, we also leverage these techniques to make the use of LLMs more reliable and trustworthy when evaluating the consistency between the generated test oracles and program documentation. This reduces the likelihood of hallucinations or factually incorrect information being presented to users.


\subsection{Metamorphic Testing}
Metamorphic testing~\cite{segura2016survey} is a testing technique that aims to reveal faults when there is no explicit test oracle. In metamorphic testing, the correctness of a program is not based on the expected output (from oracles): rather, it is based on the relationships between different inputs and their corresponding outputs, known as Metamorphic Relations (MRs). For example, in a program that calculates the square of a number, i.e., $f(x) = x^2$, the metamorphic relation could be that the square of the negative of a number is equal to the square of the number, i.e, $a = -b \rightarrow f(a) = f(b)$. Metamorphic testing has been successfully applied to machine learning models~\cite{applis2021assessing, murphy2008properties, xie2011testing}, which are essentially untestable~\cite{Weyuker1982aa}.

In our work, we use the concept of metamorphic testing to assess the reliability of LLMs in identifying inconsistencies between program specifications and behavior. 
% Specifically, we focus on the MRs that exist between different types of assertion statements in regression tests, which capture the current program behavior.
 % The key idea is that if a program's behavior satisfies certain MRs, then an LLM's responses should also exhibit the same MRs when presented with inverted queries. 
By examining the alignment of the LLM's responses with the expected MRs, we can assess the model's reliability and consistency in comprehending the underlying relationships between the program documentation and behavior. Note that this approach can also be seen as a form of LLM self-consistency, as the LLM should produce opposite outputs for inverted queries if it is truly consistent.



% \begin{figure*}
%     \centering
%     \includegraphics[width=0.9\textwidth]{image/overview.pdf}
%     \caption{Overview of \name}
%     \label{fig:overview}
% \end{figure*}

\section{methodology}\label{sec:methodology}

In this paper, we introduce \name, a novel approach that uses LLM to automatically identify inconsistencies between program documentation and behavior. Given the program documentation for a method that meets certain quality criteria (Step A), instead of directly analyzing the method's source code, \name generates regression tests to capture the current semantics of the target method (Step B). To enhance the reliability of the evaluation, \name generates two types of prompts based on metamorphic relations: the original/transformed-version prompts that ask whether the captured program behavior in the original/transformed versions of the regression test aligns well with the program documentation (Step C). Subsequently, each type of prompt is queried to LLM multiple times, and the answers are recorded (Step D). The final set of responses from the LLM is aggregated to compute a \emph{consistency} score, which numerically represents the extent to which the given specification in method documentation aligns with the captured method behavior (Step E). In the following sections, we provide more detailed explanations for each step of \name approach.

\subsection{Selecting Method Documentation}
The quantity and quality of documentation vary across projects and even down to the level of individual classes or methods, influenced by factors such as a method's complexity and its significance within the project. To ensure a fair evaluation of \name, we focus on documentation containing specifications that meet a set of minimum criteria.

Components in documentation essential for our analysis include descriptions of method input parameters and expected output values. This description is needed when constructing unit tests, which are divided into the test prefix/setup and the test oracle. The test prefix/setup initiates the method with appropriate inputs and drives the unit under test to an interesting state, while the test oracle specifies a condition that the resultant state should satisfy. These specifications are typically documented in Java using \texttt{@param} and \texttt{@return} tags. Thus, our selection process prioritizes methods whose specifications clearly delineate these aspects, ensuring \name is assessed against well-defined and actionable criteria.

\lstset{
    basicstyle=\ttfamily\scriptsize
}
\begin{figure*}[htb]
    \begin{tabular}{@{}p{0.48\linewidth} p{0.48\linewidth}@{}}
        \begin{subfigure}{\linewidth} % Left side - Code
            \lstinputlisting[language=Java]{code.java} % Insert your code file name here
        \subcaption{In Lang-1f, \textbf{ClassUtils.Java}-(mutated line 306)}
        \end{subfigure}
        & 
        \begin{subfigure}{\linewidth} % Right side - Spec and Test
            \lstinputlisting[language=Java]{specification.java} % Insert your test code file name here
            \subcaption{Javadoc}
            \lstinputlisting[language=Java]{test.java} % Insert your test code file name here
            \subcaption{Regression Test Case Generated by EvoSuite}
        \end{subfigure}
    \end{tabular}
    \caption{An example of a buggy source code along with its corresponding Javadoc and EvoSuite-generated regression test case}
    \label{fig:example}
\end{figure*}
\lstset{
    basicstyle=\ttfamily\footnotesize
}

\subsection{Capturing Program behavior using Regression Tests}
We employ automatically generated regression tests to produce a textual representation of the current program behavior. This textual representation serves as input for \name, enabling us to compare the program's captured behavior against its specifications. Regression test generation tools, such as EvoSuite~\cite{Evosuite}, are typically used to generate tests that help ensure that future updates do not inadvertently disrupt the existing functionality of the program. 
% Our approach takes advantage of these tools to capture the current program behavior, as these tools operate under the assumption that the program under the test is correctly implemented, regardless of whether it aligns with the developer's intention or not.

\Cref{fig:example} illustrates an example of a regression test generated for the \texttt{ClassUtils.getPackageName} method in the Apache Commons Lang project, which has an incorrect oracle. This test demonstrates the consequences of an artificially introduced bug in the method's return statement. When \texttt{ClassUtils.getPackageName("line.separator")} is invoked with the current version of the program, it erroneously returns \texttt{"ine"}. This output is in conflict with the method's documented expected behavior, which is to accurately return \texttt{"line"} as the package name for the input.

\subsection{Prompt Engineering based on Metamorphic Relations}
% LLMs inherently produce variable outcomes due to their probabilistic nature, challenging the consistency of their responses to queries. Recognizing this, previous studies have focused on prompt engineering, a strategy aimed at optimizing query formulations to enhance LLM performance by exploring various \emph{asking} strategies. 

\begin{table}[t]
    \centering
    \caption{Oracle transformations based on MR}
    \scalebox{0.9}{
    \begin{tabular}{l|l}
    \toprule
    \textbf{Transformation} & {\textbf{Description}} \\ \midrule
    MR\_T2F          & Replacing \texttt{assertTrue} to \texttt{assertFalse}             \\ 
    MR\_F2T          & Replacing \texttt{assertFalse} to \texttt{assertTrue}             \\ 
    MR\_N2NN         & Replacing \texttt{assertNull} to \texttt{assertNotNull}            \\ 
    MR\_NN2N         & Replacing \texttt{assertNotNull} to \texttt{assertNull}           \\ 
    MR\_E2NE         & Replacing \texttt{assertEquals} to \texttt{assertNotEquals}       \\ 
    MR\_NE2E         & Replacing \texttt{assertNotEquals} to \texttt{assertEquals}       \\ 
    MR\_S2NS         & Replacing \texttt{assertSame} to \texttt{assertNotSame}            \\ 
    MR\_NS2S         & Replacing \texttt{assertNotSame} to \texttt{assertSame}            \\ \bottomrule
    \end{tabular}
    }
    \label{tab:transformation}
\end{table}
% Assertion 별 갯수

\name uses Metamorphic Relations (MRs) to improve the reliability of the LLM responses. These MRs are grounded in two core components: \textit{the input transformation} and \textit{the output relation}~\cite{segura2016survey}. 
The MRs employed in \name are defined as follows:
\begin{equation}
\begin{split}
R = \{&(a_{1}, a_{2}, \textit{Exec}(t,a_{1}), \textit{Exec}(t,a_{2})) \mid a_{1}=\neg a_{2} \\
&\rightarrow \textit{Exec}(t,a_{1}) = \lnot \textit{Exec}(t,a_{2})\}
\end{split}
\end{equation}
where $a_{1}$ and $a_{2}$ are assertion predicates that negate each other, the function \textit{Exec} returns the execution result (true if pass, false if fail) for test input \textit{t} against the given assertion. Note that the execution results for \textit{t} with $a_{1}$ should be different from that of $a_{2}$ in order to satisfy the output relation.
To generate a transformed test case, we apply a transformation as described in \Cref{tab:transformation} to the source test case generated at Step B. Through \textit{input transformation}, we negate the semantics of the original assertion. Subsequently, to verify whether \textit{the output relation} is met, we compare test outcomes to ensure that a pass in one execution directly corresponds to a fail in its counterpart, and vice versa. Notably, we filter out original assertions that do not lead to clear metamorphic relations, such as assertions that expect thrown exceptions (e.g., \texttt{assertThrows}).

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{image/prompt_3.pdf}
    \caption{An example of metamorphic prompt}
    \label{fig:prompt}
\end{figure}

In the final stage of the prompt construction, we enhance the model's problem-solving capability by incorporating the Chain-of-Thought technique~\cite{Wei2024aa}. This approach explicitly outlines each reasoning step, thereby improving the model's analytical processes. It facilitates systematic evaluation of the model's reasoning and ensures clarity in its responses, significantly enhancing the reliability and interpretability of outcomes. The process involves several key steps:
\begin{itemize}
\item \textbf{Step 1:} Identifying Method Signature
\item \textbf{Step 2:} Identifying Method Description
\item \textbf{Step 3:} Evaluating Test Case
\item \textbf{Step 4:} Asking Confirmatory Question
\item \textbf{Step 5:} Labeling Oracle
\end{itemize}

We use the term \textit{metamorphic prompt} to refer to a pair of an original prompt and its transformed prompt. The prompt example referenced by \Cref{fig:example} is illustrated in \Cref{fig:prompt}. The texts with gray background are automatically filled in by \name for both the original and the transformed prompts, while the sections with blue and red backgrounds represent the content for the original and transformed prompts, respectively. 





\subsection{Querying LLM}

Using the generated prompt, \name queries the LLM to respond to the reasoning steps (Step D). We present both the original version and the transformed-version prompt to the LLM, each version $n$ times, adopting the self-consistency prompt engineering. For the output, we instruct the LLM to label the correctness of the given test case and assertion with respect to the given documentation using three distinct types: $\texttt{<correct>}$, $\texttt{<undecidable>}$ and $\texttt{<incorrect>}$. Given that the test oracles (representing the program's behavior) in the original version and the transformed version are opposite to each other, we would expect the responses from an LLM to differ for each prompt if the LLM is consistent and can judge the consistency between the given document and the test case.

\subsection{Scoring based on LLM Responses}{\label{sec:methodology:scoring}}

We aggregate LLM responses and convert these responses into scores. The scoring methodology differs for original and transformed prompts, and is calculated as follows: 

\subsubsection{Original Prompts} For original prompts, the final label obtained from the LLM's responses is converted into a numerical form using the following function:
$$
f_{orig}(r) = \begin{cases}
+1, & \text{if } r = \texttt{<correct>}\\
0, & \text{if } r = \texttt{<undecidable>}\\
-1, & \text{if } r = \texttt{<incorrect>}
\end{cases}
$$
Suppose that we query the original-version prompt $n$ times and the answers are $\{r_1, \cdots, r_n\}$, we aggregate the answers by taking the sum of the numerical values, $\text{score}_{orig}(\{r_1, \cdots, r_n\}) = \sum_{i=1}^{n}f_{orig}(r)$, which ranges from $-n$ to $n$. This score represents the degree of alignment or misalignment between the program's actual behavior and its intended specification in documentation, as perceived by the LLM. A score of $n$ indicates that all $n$ responses from the LLM unanimously confirmed a consistency between the program behavior and specifications. Conversely, a score of $-n$ signifies that all responses identified an inconsistency.
% Conversely, a score of $n$ indicates that the $n$ responses unanimously identified consistency between the program's behavior and specifications across all $n$ responses.

\subsubsection{Transformed Prompts}
The scoring system for transformed prompts is deliberately inverted: 
% to have the opposite meaning of the original prompts:
$$
f_{tran}(r) = \begin{cases}
+1, & \text{if } r = \texttt{<incorrect>}\\
0, & \text{if } r = \texttt{<undecidable>}\\
-1, & \text{if } r = \texttt{<correct>}
\end{cases}
$$
Similarly, the answers from $n$ queries are aggregated as: $\text{score}_{tran}(\{r_1', \cdots, r_n'\}) = \sum_{i=1}^{n}f_{tran}(r')$, which also ranges from $-n$ to $n$, representing the degree of alignment or misalignment between the program's behavior and its specification. For instance, a score of $-n$ indicates that the LLM consistently found a consistency between the behavior and specification of the \textit{inversed} program across all $n$ responses. This, in turn, indicates an inconsistency between the behavior and specification of the original program, aligning with $f_{orig}$.
% \newcommand{\greenC}{\colorbox{green!40}{C}}
% \newcommand{\redI}{\colorbox{red!40}{I}}
% \newcommand{\yellowU}{\colorbox{yellow!40}{U}}
% \newcommand{\greenI}{\colorbox{green!40}{I}}
% \newcommand{\redC}{\colorbox{red!40}{C}}

% \begin{table}[t]
% \centering
% \caption{Scoring system for metamorphic prompts; \texttt{C}, \texttt{U}, and \texttt{I} denote $\texttt{<correct>}$, $\texttt{<undecidable>}$, and $\texttt{<incorrect>}$, respectively.}
% \scalebox{0.9}{
% \begin{tabular}{llrl}
% \toprule
% \textbf{Original}                                     & \textbf{Transformed}                                  & \textbf{Score} & \textbf{Interpretation} \\ \midrule
% \texttt{\greenC \greenC \greenC \greenC \greenC}      & \texttt{\greenI \greenI \greenI \greenI \greenI}      & 1.0                 & Complete alignment \\
% \texttt{\greenC \greenC \greenC \yellowU \redI}       & \texttt{\greenI \greenI \greenI \yellowU \redC}       & 0.4                 & Moderate alignment\\
% \texttt{\greenC \greenC \greenC \greenC \greenC}      & \texttt{\redC \redC \redC \redC \redC}      & 0.0                 & Neutral/undecidable alignment\\
% \texttt{\yellowU \yellowU \yellowU \yellowU \yellowU} & \texttt{\yellowU \yellowU \yellowU \yellowU \yellowU} & 0.0                 & Neutral/undecidable alignment\\
% \texttt{\greenC \greenC \yellowU \redI \redI}         & \texttt{\greenI \greenI \yellowU \redC \redC}         & 0.0                 & Neutral/undecidable alignment\\
% \texttt{\greenC \yellowU \redI \redI \redI}           & \texttt{\greenI \yellowU \redC \redC \redC}           & -0.4                & Moderate misalignment\\
% \texttt{\redI \redI \redI \redI \redI}                & \texttt{\redC \redC \redC \redC \redC}                & -1.0                & Complete misalignment\\ \bottomrule
% \end{tabular}
% }
% \label{tab:scoring_system}
% \end{table}

We then aggregate scores from both the original and transformed prompts by taking the sum of both scores. When querying both types of prompts $n$ times, the final score will range from $[-2n, 2n]$, where $2n$ represents a scenario where all $n$ responses for the original prompt are $\texttt{<correct>}$, and all $n$ responses for the transformed prompt are $\texttt{<incorrect>}$ (or vice versa). We then normalize this score to a score ranging in $[-1, +1]$ by dividing it by $2n$.
% as follows:
% \begin{equation}
% \begin{split}
%     \text{Score}_{norm} = \frac{1}{2} \left(\frac{\text{score}_{orig}}{n} + \frac{\text{score}_{tran}}{n}\right)
% \end{split}
% \label{eq:score_norm}
% \end{equation}
% In this normalized scoring system, a score of $-1$ indicates that the LLM consistently identifies inconsistencies between the program's specification and its captured behavior across all queries, while a score of $+1$ indicates that the LLM consistently identifies a complete alignment between the program's specification and its captured behavior across all queries.
% \Cref{tab:scoring_system} demonstrates how the scoring system captures various scenarios, ranging from complete alignment to complete misalignment, as well as neutral and intermediate cases, based on the LLM's responses to the original and transformed prompts.



%In contrast to the majority voting approach typically adopted in self-consistency studies, our evaluation process implements a scoring system designed to enhance practicality and offer a more flexible analysis.

%In our approach, the scoring system is fundamentally designed to aggregate LLM's responses across multiple answers to derive an overarching judgment about the alignment or misalignment between given specifications and tests. This methodology is designed to interpret the collective set of LLM responses, determining the general trend in its understanding of whether the specifications and tests are aligned or not.
\begin{table}[h]
\centering
\caption{Details of \name dataset}
\resizebox{\columnwidth}{!}{
\begin{tabular}{l|r|rr|r}
\toprule
\multirow{2}{*}{\textbf{Projects}} & \multirow{2}{*}{\textbf{\# Mutants}} & \multicolumn{3}{c}{\textbf{\# Test}} \\
\cmidrule{3-5}
 & &\textbf{w/ incorrect oracle} & \textbf{w/ correct oracle} & \textbf{Total}\\\midrule
Chart         &   11,589    & 2,684      & 2,684         & 5,368           \\ 
Closure       &   343       & 93         & 93            & 186            \\ 
Lang          &   4,723     & 594        & 594           & 1,188          \\ 
Math          &   11,168    & 740        & 740           & 1,480          \\ 
Time          &   1,983     & 630        & 630           & 1,260          \\ \midrule
Total         &   29,806    & 4,741      & 4,741         & 9,482          \\ \bottomrule
\end{tabular}
}
\label{tab:dataset}
\end{table}

\section{Evaluation setup}
\label{sec:setup}

\subsection{Dataset}

We evaluate our approach on a carefully constructed dataset comprising $9,482$ pairs of tests and documentation. This dataset, shown in \Cref{tab:dataset}, is evenly balanced, containing $4,800$ tests with incorrect oracles and an equal number of tests with correct oracles. We construct this dataset from five open-source projects included in Defects4J v2.0.1 as follows:

\subsubsection{\textbf{Documentation Quality Assessment}} For each project, we examine the documentation quality of each method to confirm it contains descriptions for both parameters and return conditions within the latest fixed version for each project (e.g., Chart-1f, Closure-1f). Only methods with documentation satisfying these criteria move to the next step. As illustrated in \Cref{tab:dataset}, despite its large size, many methods from the Closure project were filtered out during this phase.

\subsubsection{\textbf{Mutant Injection}} 
% This step involves artificially introducing modifications to the program semantics. We create a set of methods injected with first-order mutants with Major~\cite{just2011major}, a mutation testing tool designed for Java programs, on methods that have passed the documentation quality assessment.
This step artificially alters the program semantics to create inconsistencies between the code and its corresponding documentation, which are then used to evaluate our technique. Specifically, we generate first-order mutants using Major~\cite{just2011major}, a mutation testing tool for Java programs, in methods that have passed the documentation quality assessment. We aim to generate a broad range of inconsistent code-documentation pairs, as the original pairs from Defects4J do not cover sufficiently diverse inconsistencies.

\subsubsection{\textbf{Regression Test Generation}} Due to the high cost of generating regression test cases, we randomly choose no more than 10 mutants per method, prioritizing mutants that modify distinct lines of code to ensure diversity. However, if a method produces fewer than 10 mutants, we accept the set as is, without sampling. After selecting the mutants out of about 30,000 mutants from five projects, we employ EvoSuite to generate regression tests targeting the chosen mutated methods.

\subsubsection{\textbf{Oracle Identification}} Automatically generated tests are executed against the latest fixed version of the program, identifying the outcomes as failing or passing.
A failing test indicates a test with an incorrect oracle that captures the behavior modified by the mutants injected into the program, while a passing test has a correct oracle that conforms to the original program behavior.
%Since mutants vary in difficulty to kill, failing tests are fewer. To ensure a fair comparison, we randomly sample passing tests to match the number of failing tests.
Since some mutants may be difficult to kill, the number of failing tests is significantly smaller than that of the passing ones. To ensure a fair comparison, we randomly sample passing tests to match the number of failing tests.
%To ensure a balanced representation, we adjust the ratio by randomly selecting passing tests to equal the number of failing tests, facilitating a fairer comparison.

\subsection{Experimental Settings}
As a LLM model, we use \texttt{GPT-3.5-Turbo-0613} provided by OpenAI with a default parameter setting of temperature 0.7.
% All experiments are conducted on a machine equipped with Intel Core i7-7700 CPU and 32GB memory, running Ubuntu 16.04. 
We use EvoSuite version 1.0.7 and Major version 1.3.4 for test case and mutant generation, respectively.

\subsection{Research Questions}
We ask the following research questions in this paper.

\subsubsection{\textbf{RQ1. What is the effectiveness of \name?}} 

To answer this question, we apply \name (with $n=5$) to the 9,482 regression test cases shown in \Cref{tab:dataset}. Our analysis focuses on assessing how well the normalized score computed from each metamorphic prompt corresponds with the ground truth. We evaluate how effectively \name identifies misalignment between regression oracles and documentations by examining the precision and recall against different thresholds.
% Lastly, to further validate the effectiveness of our methodology, we trained a classifier using features derived from the counts of $\texttt{<correct>}$, $\texttt{<undecidable>}$, and $\texttt{<incorrect>}$ responses for each original/transformed prompt.

\subsubsection{\textbf{RQ2. How does each component affect the performance of \name?}} 
As described in \Cref{sec:methodology}, \name employs techniques such as metamorphic relations and self-consistency to enhance the reliability of the LLM. To examine the impact of these techniques on \name, we conduct an ablation study. This study also evaluates the essential role of the $\texttt{<undecidable>}$ label, which is used in cases where making a determination based solely on the provided prompt is difficult. We investigate the model's performance using only two labels, $\texttt{<correct>}$ and $\texttt{<incorrect>}$, to assess the utility of the $\texttt{<undecidable>}$ label.

% \subsubsection{\textbf{RQ3. What impact does the assertion type have on \name's performance?}} 
% As outlined in \Cref{tab:transformation}, \name includes transformations for 6 assertion types. These types are used to ensure a program functions as intended, with each type being utilized at different frequencies. We examine the effectiveness of \name across each type to better understand its performance.

\subsubsection{\textbf{RQ3. In what circumstance does \name fail to identify inconsistencies?}} 
In RQ3, we conduct a qualitative analysis to identify the environments in which \name fails to detect inconsistencies accurately. Specifically, we examine cases where \name reports consistencies with high confidence, even though the test-specification pairs were, in fact, inconsistent. This examination aims to uncover the causes behind these misjudgments.

\section{Results}
\label{sec:results}

% This section presents the answers to our research questions based on the results of our empirical evaluations.

\subsection{\textbf{RQ1.} Effectiveness of \name}

\begin{figure}[t]
    \centering 
    \includegraphics[width=\columnwidth]{image/RQ1_1.pdf}
    \caption{Incorrect oracle detection of \name}
    \label{fig:stack_bar}
\end{figure}


\begin{table}[t]
\centering
\caption{Precision, Recall, and F1 at different thresholds}
\scalebox{0.95}{
\begin{tabular}{lrrr|lrrr}
\toprule
\textbf{Score} & \textbf{Pre.} & \textbf{Rec.} & \textbf{F1} & \textbf{Score} & \textbf{Pre.} & \textbf{Rec.} & \textbf{F1} \\\midrule
$\leq -0.1$      & 0.722               & 0.480            & 0.576 & $\leq -0.6$      & 0.967               & 0.099            & 0.180     \\
$\leq -0.2$      & 0.808               & 0.361            & 0.499 & $\leq -0.7$      & 0.971               & 0.064            & 0.120     \\
$\leq -0.3$      & 0.873               & 0.267            & 0.409 & $\leq -0.8$      & 0.973               & 0.046            & 0.087     \\
$\leq -0.4$      & 0.926               & 0.199            & 0.328 & $\leq -0.9$      & 1.000               & 0.021            & 0.042     \\
$\leq -0.5$      & 0.952               & 0.134            & 0.235 & $\leq -1.0$      & 1.000               & 0.014            & 0.027      \\ \bottomrule
\end{tabular}}
\label{tab:metrics}
\end{table}

To answer RQ1, we present the results of \name (with $n=5$) evaluated on the five projects listed in \Cref{tab:dataset}. \Cref{fig:stack_bar} shows the distribution of the number of metamorphic prompts across the normalized score, with the red indicating those based on tests with incorrect oracles, and the green for correct ones. The majority of evaluations result in a score of 0.0, where the \name either responded $\texttt{<undecidable>}$ for all queries, or produced both positive and negative scores that canceled each other out. This shows the difficulty in assessing the correctness of oracles from developer-written documentation that might lack sufficient detail for clear evaluation (we used the documentation exactly as written by developers). The blue line depicts the proportion of metamorphic prompts constructed from tests with incorrect oracles (denoted as red in the bar plot) relative to all metamorphic prompts corresponding to each score. The red dotted line represents a baseline of 0.5, reflecting the balanced nature of our dataset, which includes an equal number of tests with correct and incorrect oracles. A point close to 100\% at the score -1.0 signifies that the test-specification pairs steadily identified by \name to be inconsistent were indeed pairs with inconsistencies. Conversely, the point near 0\% at the score 1.0 means that the pairs confidently classified by the \name as aligned are very rarely associated with an incorrect oracle.

To further evaluate the performance of \name in detecting the test oracles inconsistent with documentation, we analyze the precision and recall at different scoring thresholds, as detailed in \Cref{tab:metrics}. The oracle in a test is classified as \emph{incorrect} if the normalized score is equal to or lower than a specified threshold.
For example, when the threshold is set to -0.1, all instances with negative scores are classified as incorrect. At this threshold, the precision is 0.722 while the recall is 0.480. As the threshold is lowered, the precision increases but the recall drops.
These results indicate that choosing an appropriate threshold based on user requirements can balance utility and performance.

% \begin{table}[t]
%     \centering
%     \caption{Cross project validation results}
%     \label{tab:crossproject}
%     \scalebox{0.83}{
%     \begin{tabular}{lccc|lccc}
%     \toprule
%     \textbf{Training} & \textbf{Precision} & \textbf{Recall} &\textbf{F1} & \textbf{Training} & \textbf{Precision} & \textbf{Recall} &\textbf{F1} \\ \midrule
%     Chart            & 0.736 & 0.556 & 0.634   & Math             & 0.659 & 0.827 & 0.733   \\
%     Closure          & 0.677 & 0.635 & 0.655   & Time             & 0.707 & 0.748 & 0.727   \\  \cline{5-8}
%     Lang             & 0.693 & 0.806 & 0.745   & Average          & 0.694 & 0.714 & 0.699   \\
%     \bottomrule
%     \end{tabular}}
% \end{table}
% Std Deviation    & 0.026 & 0.104 & 0.045   \\


% We also assess the generalizability of \name by training an inconsistency identification classifier using the LLM response data from one project and evaluating it on data from the remaining four. We use the Logistic Regression model from the \texttt{scikit-learn} library, with its default settings. The model is trained and tested on features derived from the counts of labels for each original and transformed prompt. The cross-project validation results, as shown in \Cref{tab:crossproject}, show consistent performance across the different projects, and show superior performance to the simple thresholding strategy, notably the higher recall. The average precision is 0.694, the average recall is 0.714, and the average F1-score is 0.699. The result indicates that the model for identifying inconsistencies generalizes well across the different projects.

\begin{tcolorbox}[colback=gray!20, boxsep=0pt, left=1mm, right=1mm, top=1mm, bottom=1mm]
\textbf{Answer to RQ1:} \name effectively identified misalignment between documentation and test, demonstrating a high precision of 0.722 and a recall of 0.480 in detecting inconsistencies. When applying stricter thresholds, precision can be set to nearly 100\%.
\end{tcolorbox}
%To further quantify \name's performance, we constructed a logistic regression model based on LLM's response result to the prompts. It considers the count of $\texttt{<correct>}$, $\texttt{<incorrect>}$, and $\texttt{<undecidable>}$ responses for both original and metamorphic prompts as features. We used a 5-fold cross-validation approach for model training, which resulted in a precision of 0.6952, a recall of 0.6967, and a precision-recall AUC (Area Under the Curve) of 0.7806. Table 4 shows the performance of the model according to the training data used.
% Assertion 별 성능 비교 

\subsection{\textbf{RQ2:} Ablation Study}

\begin{figure*}[t]
     \centering
     \begin{subfigure}[h]{0.30\linewidth}
         \centering
         \includegraphics[width=0.9\columnwidth]{image/RQ2_1.pdf}
         \caption{metamorphic}
         \label{fig:metamorphic_ratio}
     \end{subfigure}
     \begin{subfigure}[h]{0.30\linewidth}
         \centering
         \includegraphics[width=0.9\columnwidth]{image/RQ2_2.pdf}
         \caption{self-consistency}
         \label{fig:selfconsistency}
     \end{subfigure}
     \begin{subfigure}[h]{0.30\linewidth}
         \centering
         \includegraphics[width=0.9\columnwidth]{image/two_label_ratio.pdf}
         \caption{without $\texttt{<undecidable>}$ label}
         \label{fig:metamorphic_ratio_two}
     \end{subfigure}
        \caption{An impact of metamorphic relations, self-consistency, and labels}
        \label{fig:metamorphic_selfconsistency}
\end{figure*}

% \begin{table*}[tb]
%     \centering
%     \caption{Area under the precision-recall curve for various $n$ values and prompt types used (higher values indicate better performance)}
%     \label{tab:pr_auc_queries}
%     \scalebox{0.9}{
%     \begin{tabular}{lccccccccccccccc}
%     \toprule
%     & \multicolumn{5}{c}{\textbf{Only original prompt}} & \multicolumn{5}{c}{\textbf{Only transformed prompt}} & \multicolumn{5}{c}{\textbf{Metamorphic prompt}}\\
%     \cmidrule(lr){2-6} \cmidrule(lr){7-11} \cmidrule(lr){12-16}
%     & \textbf{n=1} & \textbf{n=2} & \textbf{n=3} & \textbf{n=4} & \textbf{n=5} & \textbf{n=1} & \textbf{n=2} & \textbf{n=3} & \textbf{n=4} & \textbf{n=5} & \textbf{n=1} & \textbf{n=2} & \textbf{n=3} & \textbf{n=4} & \textbf{n=5} \\
%     \midrule
%     Chart    & 0.5291 & 0.4917 & 0.4798 & 0.4725 & 0.4707  & 0.7441 & 0.7627 & 0.7764 & 0.7863 & 0.7925 & 0.6615 & 0.7020 & 0.7268 & 0.7407 & 0.7549 \\
%     Closure  & 0.6075 & 0.5516 & 0.5311 & 0.5479 & 0.5444  & 0.5544 & 0.5455 & 0.5460 & 0.5403 & 0.5470 & 0.5848 & 0.5632 & 0.5458 & 0.5656 & 0.5708 \\
%     Lang     & 0.7653 & 0.7862 & 0.7984 & 0.8056 & 0.8089  & 0.7442 & 0.7727 & 0.7950 & 0.8038 & 0.8108 & 0.8068 & 0.8487 & 0.8697 & 0.8787 & 0.8839 \\
%     Math     & 0.5775 & 0.5363 & 0.5390 & 0.5427 & 0.5413  & 0.5966 & 0.5860 & 0.5906 & 0.5909 & 0.5914 & 0.5625 & 0.5598 & 0.5793 & 0.5874 & 0.5926 \\
%     Time     & 0.7286 & 0.7440 & 0.7567 & 0.7598 & 0.7688  & 0.6597 & 0.6727 & 0.6631 & 0.6728 & 0.6843 & 0.7233 & 0.7603 & 0.7778 & 0.7893 & 0.8032 \\
%     \midrule
%     Total    & 0.5989 & 0.5733 & 0.5689 & 0.5667 & 0.5669  & 0.7109 & 0.7277 & 0.7394 & 0.7483 & 0.7549 & 0.6772 & 0.7106 & 0.7332 & 0.7449 & 0.7556 \\
%     \bottomrule
%     \end{tabular}
%     }
% \end{table*}

\Cref{fig:metamorphic_ratio} shows the ratio of tests with incorrect oracles per their scores when \name is applied using only the original (orange) and the transformed (green) prompts. The blue line and red-dotted line follow the same representation as in RQ1. If metamorphic relations are not employed, i.e., assessing inconsistencies with just original prompts, the capability of \name to detect incorrect oracles shows a marked decrease. Specifically, when scores range between -1.0 and 0.0, the ratio drops below the baseline, leading to outcomes that are essentially indistinguishable from random alignment judgments. However, when incorporating the outcomes from the transformed prompts, as observed in RQ1, it is almost always the case that prompts receiving low scores from \name are strongly associated with incorrect oracles. 

\Cref{fig:selfconsistency} illustrates the impact of the number of queries to the LLM on the performance of \name. When the number of queries, denoted as \textit{n} in \Cref{sec:methodology:scoring}, for both the original and transformed prompts increases, we observe a higher ratio of incorrect oracles within the score ranges from -1.0 to 0.0, and a lower ratio of incorrect oracles within the range of [0,1]. Additionally, the observed gain in performance grows smaller as $n$ increases, suggesting that \name may be converging with respect to the number of queries. 

% These findings are further supported by the area under the precision-recall curve (where scores below a certain threshold are classified as indicating inconsistency) for different $n$ values and prompt types shown in \Cref{tab:pr_auc_queries}. We observe that incorporating metamorphic relations leads to better performance compared to using only one type of prompt. Additionally, the effect of self-consistency results in improved performance as $n$ increases. However, this improvement diminishes over time, indicating that \name may be nearing a performance plateau.


% \Cref{tab:pr_auc_queries} presents the PR-AUC based on the number of queries and the type of prompts used, aggregated for each project. At the project level, we can observe that the use of metamorphic relations enhances the capability of \name to detect inconsistencies between documentation and test. Furthermore, due to the effect of self-consistency, an increase in the number of queries leads to improved performance. However, the margin of performance gain decreases over time, indicating a convergence towards a certain point.

\begin{table}[t]
    \centering
    \caption{Spearman's correlation coefficient ($\rho$) and the p-value between scores and the ratio of incorrect oracles}
        \scalebox{0.95}{
        \begin{tabular}{l|rr|rr}
        \toprule
          & \multicolumn{2}{c|}{w/o $\texttt{<undecidable>}$} & \multicolumn{2}{c}{w/ $\texttt{<undecidable>}$}\\\cline{2-5}
          & $\mathbf{\rho}$ & p-value & $\mathbf{\rho}$ & p-value \\ 
        \midrule
        Metamorphic Prompt & -0.977  & 3.81e-13 & \textbf{-0.992}  & 1.75e-18\\
        Original Prompt    & -0.700  & 1.65e-02 & -0.355  & 2.84e-01\\
        Transformed Prompt & -0.955  & 4.99e-06 & -0.991  & 3.76e-09 \\
        \bottomrule
        \end{tabular}
        }
    \label{tab:correlation}
\end{table}

We also explore the need for the $\texttt{<undecidable>}$ label. \Cref{fig:metamorphic_ratio_two} shows the results obtained using \name but without the $\texttt{<undecidable>}$ label, i.e., the LLM is forced to label each assertion as $\texttt{<correct>}$ or $\texttt{<incorrect>}$. It shows a weaker correlation between the scores and the ratio of oracles classified as incorrect at each score. This observation is supported by the Spearman's correlation coefficient values presented in \Cref{tab:correlation}. Spearman's correlation coefficient measures the strength and direction of a monotonic relationship between two variables. As can be seen in \Cref{tab:correlation}, including $\texttt{<undecidable>}$ produces a stronger negative correlation across metamorphic prompts as evidenced by Spearman's correlation coefficient values approaching -1.
These findings suggest that including the $\texttt{<undecidable>}$ label can improve the effectiveness of our approach, especially in situations where the program specifications in the documentation may not clearly define the program's semantics.
\begin{tcolorbox}[colback=gray!20, boxsep=0pt, left=1mm, right=1mm, top=1mm, bottom=1mm]
\textbf{Answer to RQ2:} The ablation study shows that metamorphic relations, self-consistency, and the $\texttt{<undecidable>}$ label enhance the effectiveness of \name.
\end{tcolorbox}

% \begin{table}[t]
% \centering
% \caption{Area under the precision-recall curve (PR-AUC) for different assertion types (higher values indicate better performance)}
% \label{tab:pr_auc_assertion}
% \resizebox{\columnwidth}{!}{
% \begin{tabular}{l|rrr|r}
% \toprule
% & \multicolumn{3}{c|}{\textbf{\# Tests w/ Oracles}} & \multirow{2}{*}{\textbf{PR-AUC}}\\
% % \cmidrule(lr){2-4} \cmidrule(lr){5-7}
% & \textbf{Incorrect} & \textbf{Correct} & \textbf{Total} & \\
% \midrule
% \texttt{assertNull} & 73 & 35 & 108 & 0.9469 \\
% \texttt{assertTrue} & 1,690 & 781 & 2,390 & 0.9138 \\
% \texttt{assertSame} & 178 & 136 & 314 & 0.9058 \\
% \texttt{assertFalse} & 986 & 1288 & 2,274 & 0.8161 \\
% \texttt{assertNotNull} & 26 & 33 & 59 & 0.6874 \\
% \texttt{assertEquals} & 1,860 & 2,368 & 4,228 & 0.4803 \\
% \texttt{assertNotSame} & 9 & 100 & 109 & 0.1992 \\
% \bottomrule
% \end{tabular}
% }
% \end{table}

% \begin{table}[ht]
%     \centering
%     \caption{PR-AUC for each type of assertion}
%     \label{tab:pr_auc_assertion}
%     \resizebox{\columnwidth}{!}{
%     \begin{tabular}{lcrrrr}
%     \toprule
%     & \multicolumn{2}{c}{\textbf{\# Tests}} & \multicolumn{3}{c}{\textbf{PR-AUC}} \\
%     \cmidrule(lr){2-3} \cmidrule(lr){4-6}
%     & \textbf{w/ incorrect oracle} & \multirow{2}{*}{\textbf{Total}} & \multirow{2}{*}{\textbf{Original }} & \multirow{2}{*}{\textbf{Transformed }} & \multirow{2}{*}{\textbf{Metamorphic }} \\
%     & \textbf{w/ correct oracle} &  &  &  &  \\
%     \midrule
%     \multirow{2}{*}{\textbf{assertTrue}} & 1,690 & \multirow{2}{*}{2,390} & \multirow{2}{*}{0.8435} & \multirow{2}{*}{0.8625} & \multirow{2}{*}{0.9138} \\
%     & 781 & & & & \\ \midrule
%     \multirow{2}{*}{\textbf{assertFalse}} & 986 & \multirow{2}{*}{2,274} & \multirow{2}{*}{0.5030} & \multirow{2}{*}{0.8671} & \multirow{2}{*}{0.8161} \\
%     & 1288 & & & & \\ \midrule
%     \multirow{2}{*}{\textbf{assertNull}} & 73 & \multirow{2}{*}{108} & \multirow{2}{*}{0.9352} & \multirow{2}{*}{0.8872} & \multirow{2}{*}{0.9469} \\
%     & 35 & & & & \\ \midrule
%     \multirow{2}{*}{\textbf{assertNotNull}} & 26 & \multirow{2}{*}{59} & \multirow{2}{*}{0.5806} & \multirow{2}{*}{0.7345} & \multirow{2}{*}{0.6874} \\
%     & 33 & & & & \\ \midrule
%     \multirow{2}{*}{\textbf{assertEquals}} & 1,860 & \multirow{2}{*}{4,228} & \multirow{2}{*}{0.4225} & \multirow{2}{*}{0.5403} & \multirow{2}{*}{0.4803} \\
%     & 2,368 & & & & \\ \midrule
%     \multirow{2}{*}{\textbf{assertSame}} & 178 & \multirow{2}{*}{314} & \multirow{2}{*}{0.9098} & \multirow{2}{*}{0.7522} & \multirow{2}{*}{0.9058} \\
%     & 136 & & & & \\ \midrule
%     \multirow{2}{*}{\textbf{assertNotSame}} & 9 & \multirow{2}{*}{109} & \multirow{2}{*}{0.2070} & \multirow{2}{*}{0.2039} & \multirow{2}{*}{0.1992} \\
%     & 100 & & & & \\
%     \bottomrule
%     \end{tabular}
%     }
% \end{table}

% \subsection{\textbf{RQ3:} Performance Differences by Assertion Type}


% \Cref{tab:pr_auc_assertion} shows the accuracy of \name in classifying test-documentation pairs according to assertion types, as measured by the area under the precision-recall curve (where scores below a certain threshold are classified as indicating inconsistency). These assertion types are determined by EvoSuite, based on the characteristics of the methods. Notably, the dataset does not include the \texttt{assertNotEquals} type.
% Our result reveals that the classification performance of \name is superior for \texttt{assertTrue}, \texttt{assertNull}, \texttt{assertSame}, and \texttt{assertFalse} assertions. Conversely, it is less effective for \texttt{assertEquals} and \texttt{assertNotSame} assertions.
% % The reason behind this performance can likely be attributed to the inherent characteristics of LLMs, particularly the imbalance in their training data. While the exact training dataset utilized for the LLM is not publicly disclosed, it is reasonable to infer that \texttt{assertTrue} and \texttt{assertFalse} assertions feature more prevalently in the corpus used by the training, while \texttt{assertNotSame} is utilized less frequently.
% The reason behind this performance may be attributed to the imbalance in their training data. While the exact training dataset utilized for the LLM is not publicly disclosed, Tufano et al.~\cite{tufano2020unit} report that \texttt{assertEquals}, \texttt{assertTrue}, and \texttt{assertFalse} are the most frequent assertion statements in Java open-source projects. Based on this, we suspect that \texttt{assertTrue} and \texttt{assertFalse}  feature more prevalently in the corpus used by the training, and \texttt{assertNotSame} less frequently. For \texttt{assertEquals}, the oracle problem is known to be particularly difficult due to the need for judging precise expected values~\cite{dinella2022toga}, potentially explaining the subpar performance of LLMs.


% \begin{tcolorbox}[colback=gray!20, boxsep=0pt, left=1mm, right=1mm, top=1mm, bottom=1mm]
% \textbf{Answer to RQ3:}  Assertions frequently used in real-world scenarios, such as \texttt{assertTrue}, \texttt{assertFalse}, and \texttt{assertNull}, achieve higher PR-AUC scores. Conversely, less commonly applied assertions like \texttt{assertNotSame} register lower PR-AUC values. 
% \end{tcolorbox}

\begin{table}[t]
    \centering
    \caption{Analysis of false alarms in \name}
    \label{tab:llm_performance}
    \scalebox{0.9}{
    \begin{tabular}{lcccc|c}
    \toprule
     & \textbf{Chart} & \textbf{Lang} & \textbf{Math} & \textbf{Time} & \textbf{Total}\\
    \midrule
    Lack of Specification & 2 & 0 & 0 & 0 & 2\\
    Need for Contextual information    & 1 & 0 & 2 & 0 & 3\\ 
    LLM Underperformance  & 0 & 8 & 7 & 1 & 16\\
    % \midrule
    % Total                 & 3 & 8 & 9 & 1\\
    \bottomrule
    \end{tabular}
    }
\end{table}

\begin{figure}[t]
    \centering
    \lstinputlisting[basicstyle=\ttfamily\scriptsize, language=Java]{RQ4_1_1.java} % Insert your test code file name here
    \lstinputlisting[basicstyle=\ttfamily\scriptsize, language=Java]{RQ4_1_2.java} % Insert your test code file name here
    \caption{An example of \textit{Lack of Specification Detail}}
    \label{fig:specification}
\end{figure}

\subsection{\textbf{RQ3:} Qualitative Analysis}

In RQ3, we analyze instances where \name identified tests with incorrect oracles as correct, reflected by normalized scores of 0.8 or higher, denoting high confidence. Among the 9,482 pairs analyzed, 21 were identified as exhibiting this discrepancy, and we manually inspected the reasons behind these. We categorize the causes of misclassification into three primary reasons, \textit{Lack of Specification Detail}, \textit{Need for Contextual Information}, and \textit{LLM Underperformance}, whose distributions across projects are shown in \Cref{tab:llm_performance}.

\begin{figure}[t]
    \centering
    \lstinputlisting[basicstyle=\ttfamily\scriptsize, language=Java]{RQ4_2_1.java} % Insert your test code file name here
    \lstinputlisting[basicstyle=\ttfamily\scriptsize, language=Java]{RQ4_2_2.java} % Insert your test code file name here
    \caption{An example of \textit{Need for Contextual Information}}
    \label{fig:complexity}
\end{figure}

\noindent\textbf{Lack of Specification Detail:} An example of this category is in \Cref{fig:specification}. Although the specification mentions that the method is supposed to convert special characters to JavaScript escape sequences, it does not provide concrete examples of special characters. This lack of detailed specification makes it challenging to assess the correctness of the test oracles.

\begin{figure}[t]
    \lstinputlisting[basicstyle=\ttfamily\scriptsize, language=Java]{RQ4_3_1.java}
    \includegraphics[width=\columnwidth]{image/RQ4_new.pdf}
    \caption{An example of \textit{Underperformace of LLMs}}
    \label{fig:underperformance}
\end{figure}


\noindent\textbf{Need for Contextual Information:} An example of this category is in \Cref{fig:complexity} containing the specification of the \texttt{isSimilarTo} function from \texttt{Math}. As the properties that a Plan object acquires upon creation are not known, it becomes challenging to accurately evaluate the test outcomes. 

\noindent\textbf{LLM Underperformance:} \Cref{fig:underperformance} shows a simple test case that the LLM fails to classify correctly due to its lack of mathematical reasoning capabilities. However, the reasoning abilities of LLMs have been improving over time. When we re-evaluated all prompts associated with this kind of case using GPT-4, the average normalized score improved to $-0.156$, reflecting the improved arithmetic capabilities, suggesting that such issues may further diminish as LLM performance continues to enhance.

\begin{tcolorbox}[colback=gray!20, boxsep=0pt, left=1mm, right=1mm, top=1mm, bottom=1mm]
\textbf{Answer to RQ3:} The majority of false alarms were due to the limitations in the reasoning capabilities of the LLMs. Additionally, depending on projects, factors such as the lack of detailed specifications and the need for contextual information also contributed to these issues.
\end{tcolorbox}

% \section{Threats to Validity}
% \label{sec:ttv}

% A potential threat to internal validity is the randomness associated with querying LLMs. Although mitigated by multiple queries, variations in responses due to LLM nondeterminism could still influence the outcomes. Taking this randomness into account, we evaluated the performance on a sufficiently large dataset of 9,482 instances.
% %, and in RQ1, we assessed \name's performance for each score threshold. 

% Threats to external validity concern any factors that may limit the generalization of our results. The underlying LLM, GPT-3, has likely been trained with Defects4J projects, resulting in data leakage. However, \name relies on regression tests generated from mutated methods of Defects4J, and not its source code directly, making it less likely that the language model is explicitly familiar with the content in the prompt.
% % Another potential threat is the use of artificial mutants in generating the dataset. According to Just et al.\cite{just2014mutants}, there is actually a coupling effect between real faults and the mutants generated by commonly used mutation operators, existing in more than 73\% of cases.

% Threats to construct validity concern how well the used metrics measure the properties we aim to evaluate. We adopt established evaluation metrics, precision, recall, and F1-score, to evaluate the \name's performance in detecting code-and-documentation inconsistencies, and try to balance the dataset by undersampling tests with correct oracles.

\section{Conclusion}
\label{sec:conclusion}
% This paper introduces \name, an LLM-based technique designed to automatically identify inconsistencies between the program specification and actual behavior. \name transforms regression oracles generated for a program by utilizing metamorphic relations, and based on this, it creates prompts for both the original and transformed versions. These prompts are subsequently queried to an LLM using a self-consistency approach, from which we obtain a score of the LLM's perceived degree of alignment or misalignment between the actual behavior of the program and its specification.
This paper introduces \name, an LLM-based technique designed to automatically identify inconsistencies between a program's documentation and its actual behavior, as captured by regression test oracles. To address the issue of LLM hallucinations, \name captures program behavior via the regression test cases generated by EvoSuite and applies metamorphic prompts. Our experiments with 9,482 test-documentation pairs derived from Defects4J show that \name can effectively identify inconsistency with a precision of 0.72 and a recall of 0.48. We hope to expand upon these results by exploring its capabilities when used in conjunction with existing techniques such as fault localization and automatic program repair.

\bibliographystyle{IEEEtran}
\bibliography{ref}
\end{document}
