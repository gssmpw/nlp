@inproceedings{langley00,
 author    = {P. Langley},
 title     = {Crafting Papers on Machine Learning},
 year      = {2000},
 pages     = {1207--1216},
 editor    = {Pat Langley},
 booktitle     = {Proceedings of the 17th International Conference
              on Machine Learning (ICML 2000)},
 address   = {Stanford, CA},
 publisher = {Morgan Kaufmann}
}

@TechReport{mitchell80,
  author = 	 "T. M. Mitchell",
  title = 	 "The Need for Biases in Learning Generalizations",
  institution =  "Computer Science Department, Rutgers University",
  year = 	 "1980",
  address =	 "New Brunswick, MA",
}

@article{lii2024llava,
  title={LLaVA-NeXT-Interleave: Tackling Multi-image, Video, and 3D in Large Multimodal Models},
  author={Li, Feng and Zhang, Renrui and Zhang, Hao and Zhang, Yuanhan and Li, Bo and Li, Wei and Ma, Zejun and Li, Chunyuan},
  journal={ICLR 2025 Spotlight},
  year={2024}
}

@article{zhang2024mathverse,
  title={Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems?},
  author={Zhang, Renrui and Jiang, Dongzhi and Zhang, Yichi and Lin, Haokun and Guo, Ziyu and Qiu, Pengshuo and Zhou, Aojun and Lu, Pan and Chang, Kai-Wei and Gao, Peng and others},
  journal={ECCV 2024},
  year={2024}
}

@article{guo2024sam2point,
  title={Sam2point: Segment any 3d as videos in zero-shot and promptable manners},
  author={Guo*, Ziyu and Zhang*\#, Renrui and Zhu, Xiangyang and Tong, Chengzhuo and Gao, Peng and Li, Chunyuan and Heng, Pheng-Ann},
  journal={arXiv preprint arXiv:2408.16768},
  year={2024}
}

@inproceedings{zhang2024llama,
  title={LLaMA-adapter: Efficient fine-tuning of large language models with zero-initialized attention},
  author={Zhang, Renrui and Han, Jiaming and Liu, Chris and Zhou, Aojun and Lu, Pan and Qiao, Yu and Li, Hongsheng and Gao, Peng},
  booktitle={ICLR 2024},
  year={2024}
}

@article{jiang2024mmsearch,
  title={Mmsearch: Benchmarking the potential of large models as multi-modal search engines},
  author={Jiang, Dongzhi and Zhang, Renrui and Guo, Ziyu and Wu, Yanmin and Lei, Jiayi and Qiu, Pengshuo and Lu, Pan and Chen, Zehui and Song, Guanglu and Gao, Peng and others},
  journal={ICLR 2025},
  year={2024}
}

@article{zhang2024mavis,
  title={Mavis: Mathematical visual instruction tuning with an automatic data engine},
  author={Zhang, Renrui and Wei, Xinyu and Jiang, Dongzhi and Guo, Ziyu and Li, Shicheng and Zhang, Yichi and Tong, Chengzhuo and Liu, Jiaming and Zhou, Aojun and Wei, Bin and others},
  journal={arXiv preprint arXiv:2407.08739},
  year={2024}
}

@article{jia2024lift3d,
  title={Lift3D Foundation Policy: Lifting 2D Large-Scale Pretrained Models for Robust 3D Robotic Manipulation},
  author={Jia, Yueru and Liu, Jiaming and Chen, Sixiang and Gu, Chenyang and Wang, Zhilue and Luo, Longzan and Lee, Lily and Wang, Pengwei and Wang, Zhongyuan and Zhang, Renrui and others},
  journal={arXiv preprint arXiv:2411.18623},
  year={2024}
}

@article{peng2024chimera,
  title={Chimera: Improving generalist model with domain-specific experts},
  author={Peng, Tianshuo and Li, Mingsheng and Zhou, Hongbin and Xia, Renqiu and Zhang, Renrui and Bai, Lei and Mao, Song and Wang, Bin and He, Conghui and Zhou, Aojun and others},
  journal={arXiv preprint arXiv:2412.05983},
  year={2024}
}

@article{guo2025can,
  title={Can We Generate Images with CoT? Let's Verify and Reinforce Image Generation Step by Step},
  author={Guo, Ziyu and Zhang, Renrui and Tong, Chengzhuo and Zhao, Zhizheng and Gao, Peng and Li, Hongsheng and Heng, Pheng-Ann},
  journal={arXiv preprint arXiv:2501.13926},
  year={2025}
}

@article{lei2025imagine,
  title={IMAGINE-E: Image Generation Intelligence Evaluation of State-of-the-art Text-to-Image Models},
  author={Lei, Jiayi and Zhang, Renrui and Hu, Xiangfei and Lin, Weifeng and Li, Zhen and Sun, Wenjian and Du, Ruoyi and Zhuo, Le and Li, Zhongyu and Li, Xinyue and others},
  journal={arXiv preprint arXiv:2501.13920},
  year={2025}
}

@phdthesis{kearns89,
  author = {M. J. Kearns},
  title =  {Computational Complexity of Machine Learning},
  school = {Department of Computer Science, Harvard University},
  year =   {1989}
}

@Book{MachineLearningI,
  editor = 	 "R. S. Michalski and J. G. Carbonell and T.
		  M. Mitchell",
  title = 	 "Machine Learning: An Artificial Intelligence
		  Approach, Vol. I",
  publisher = 	 "Tioga",
  year = 	 "1983",
  address =	 "Palo Alto, CA"
}

@Book{DudaHart2nd,
  author =       "R. O. Duda and P. E. Hart and D. G. Stork",
  title =        "Pattern Classification",
  publisher =    "John Wiley and Sons",
  edition =      "2nd",
  year =         "2000"
}

@misc{anonymous,
  title= {Suppressed for Anonymity},
  author= {Author, N. N.},
  year= {2021}
}

@InCollection{Newell81,
  author =       "A. Newell and P. S. Rosenbloom",
  title =        "Mechanisms of Skill Acquisition and the Law of
                  Practice", 
  booktitle =    "Cognitive Skills and Their Acquisition",
  pages =        "1--51",
  publisher =    "Lawrence Erlbaum Associates, Inc.",
  year =         "1981",
  editor =       "J. R. Anderson",
  chapter =      "1",
  address =      "Hillsdale, NJ"
}


@Article{Samuel59,
  author = 	 "A. L. Samuel",
  title = 	 "Some Studies in Machine Learning Using the Game of
		  Checkers",
  journal =	 "IBM Journal of Research and Development",
  year =	 "1959",
  volume =	 "3",
  number =	 "3",
  pages =	 "211--229"
}

















/Related Work


@inproceedings{SpatialVLM24,
	author = {Chen, Boyuan and Xu, Zhuo and Kirmani, Sean and Ichter, Brian and Driess, Danny and Florence, Pete and Sadigh, Dorsa and Guibas, Leonidas and Xia, Fei},
	booktitle = CVPR,
	title = {Spatialvlm: Endowing vision-language models with spatial reasoning capabilities},
	year = {2024}}

@inproceedings{PointNet,
	author = {Charles Ruizhongtai Qi and Hao Su and Kaichun Mo and Leonidas J. Guibas},
	booktitle = CVPR,
	pages = {77--85},
	title = {PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation},
	year = {2017}}

@inproceedings{PointNet++,
	author = {Charles Ruizhongtai Qi and Li Yi and Hao Su and Leonidas J. Guibas},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/nips/QiYSG17.bib},
	booktitle = NIPS,
	pages = {5099--5108},
	timestamp = {Thu, 21 Jan 2021 15:15:21 +0100},
	title = {PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space},
	year = {2017}}

@article{DGCNN,
	author = {Yue Wang and Yongbin Sun and Ziwei Liu and Sanjay E. Sarma and Michael M. Bronstein and Justin M. Solomon},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/tog/WangSLSBS19.bib},
	journal = TOG,
	number = {5},
	pages = {146:1--146:12},
	timestamp = {Tue, 02 Aug 2022 14:54:57 +0200},
	title = {Dynamic Graph {CNN} for Learning on Point Clouds},
	volume = {38},
	year = {2019}}

@article{ThreeDQA22,
	author = {Ye, Shuquan and Chen, Dongdong and Han, Songfang and Liao, Jing},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	title = {3D question answering},
	year = {2022}}

@inproceedings{SQA3D23,
	author = {Xiaojian Ma and Silong Yong and Zilong Zheng and Qing Li and Yitao Liang and Song{-}Chun Zhu and Siyuan Huang},
	booktitle = ICLR,
	title = {{SQA3D:} Situated Question Answering in 3D Scenes},
	year = {2023}}

@inproceedings{ScanRefer,
	author = {Dave Zhenyu Chen and Angel X. Chang and Matthias Nie{\ss}ner},
	booktitle = ECCV,
	title = {ScanRefer: 3D Object Localization in {RGB-D} Scans Using Natural Language},
	year = {2020}}

@inproceedings{Scan2Cap21,
	author = {Dave Zhenyu Chen and Ali Gholami and Matthias Nie{\ss}ner and Angel X. Chang},
	booktitle = CVPR,
	title = {Scan2Cap: Context-Aware Dense Captioning in {RGB-D} Scans},
	year = {2021}}

@inproceedings{PointCLIP22,
	author = {Renrui Zhang and Ziyu Guo and Wei Zhang and Kunchang Li and Xupeng Miao and Bin Cui and Yu Qiao and Peng Gao and Hongsheng Li},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/corr/abs-2112-02413.bib},
	booktitle = CVPR,
	timestamp = {Tue, 21 Jun 2022 15:17:37 +0200},
	title = {PointCLIP: Point Cloud Understanding by {CLIP}},
	year = {2022}}

@inproceedings{OpenShape23,
	author = {Minghua Liu and Ruoxi Shi and Kaiming Kuang and Yinhao Zhu and Xuanlin Li and Shizhong Han and Hong Cai and Fatih Porikli and Hao Su},
	booktitle = NeurIPS,
	title = {OpenShape: Scaling Up 3D Shape Representation Towards Open-World Understanding},
	year = {2023}}

@inproceedings{CLIPFO3D23,
	author = {Junbo Zhang and Runpei Dong and Kaisheng Ma},
	booktitle = ICCVW,
	title = {{CLIP-FO3D:} Learning Free Open-world 3D Scene Representations from 2D Dense {CLIP}},
	year = {2023}}

@article{pointbind23,
	author = {Ziyu Guo and Renrui Zhang and Xiangyang Zhu and Yiwen Tang and Xianzheng Ma and Jiaming Han and Kexin Chen and Peng Gao and Xianzhi Li and Hongsheng Li and Pheng{-}Ann Heng},
	eprint = {2309.00615},
	eprinttype = {arXiv},
	journal = {CoRR},
	title = {Point-Bind {\&} Point-LLM: Aligning Point Cloud with Multi-modality for 3D Understanding, Generation, and Instruction Following},
	volume = {abs/2309.00615},
	year = {2023}}

@inproceedings{imagebind23,
	author = {Girdhar, Rohit and El-Nouby, Alaaeldin and Liu, Zhuang and Singh, Mannat and Alwala, Kalyan Vasudev and Joulin, Armand and Misra, Ishan},
	booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
	pages = {15180--15190},
	title = {Imagebind: One embedding space to bind them all},
	year = {2023}}

@article{pointllm23,
	author = {Runsen Xu and Xiaolong Wang and Tai Wang and Yilun Chen and Jiangmiao Pang and Dahua Lin},
	eprint = {2308.16911},
	eprinttype = {arXiv},
	journal = {CoRR},
	title = {PointLLM: Empowering Large Language Models to Understand Point Clouds},
	volume = {abs/2308.16911},
	year = {2023}}

@inproceedings{3DVista23,
	author = {Ziyu Zhu and Xiaojian Ma and Yixin Chen and Zhidong Deng and Siyuan Huang and Qing Li},
	booktitle = ICCV,
	title = {3D-VisTA: Pre-trained Transformer for 3D Vision and Text Alignment},
	year = {2023}}

@inproceedings{3DLLM23,
	author = {Yining Hong and Haoyu Zhen and Peihao Chen and Shuhong Zheng and Yilun Du and Zhenfang Chen and Chuang Gan},
	booktitle = NeurIPS,
	title = {3D-LLM: Injecting the 3D World into Large Language Models},
	year = {2023}}

@inproceedings{ImagePartStates18,
	author = {Cewu Lu and Hao Su and Yonglu Li and Yongyi Lu and Li Yi and Chi{-}Keung Tang and Leonidas J. Guibas},
	booktitle = CVPR,
	title = {Beyond Holistic Object Recognition: Enriching Image Understanding With Part States},
	year = {2018}}

@inproceedings{NOCSPose19,
	author = {He Wang and Srinath Sridhar and Jingwei Huang and Julien Valentin and Shuran Song and Leonidas J. Guibas},
	booktitle = CVPR,
	title = {Normalized Object Coordinate Space for Category-Level 6D Object Pose and Size Estimation},
	year = {2019}}

@inproceedings{GenOHDiffusion24,
	author = {Xueyi Liu and Li Yi},
	booktitle = ICLR,
	title = {Gene{OH} Diffusion: Towards Generalizable Hand-Object Interaction Denoising via Denoising Diffusion},
	year = {2024}}

@inproceedings{UniDexGrasp23,
	author = {Xu, Yinzhen and Wan, Weikang and Zhang, Jialiang and Liu, Haoran and Shan, Zikang and Shen, Hao and Wang, Ruicheng and Geng, Haoran and Weng, Yijia and Chen, Jiayi and Liu, Tengyu and Yi, Li and Wang, He},
	booktitle = CVPR,
	title = {UniDexGrasp: Universal Robotic Dexterous Grasping via Learning Diverse Proposal Generation and Goal-Conditioned Policy},
	year = {2023}}

@inproceedings{PaLME23,
	author = {Danny Driess and Fei Xia and Mehdi S. M. Sajjadi and Corey Lynch and Aakanksha Chowdhery and Brian Ichter and Ayzaan Wahid and Jonathan Tompson and Quan Vuong and Tianhe Yu and Wenlong Huang and Yevgen Chebotar and Pierre Sermanet and Daniel Duckworth and Sergey Levine and Vincent Vanhoucke and Karol Hausman and Marc Toussaint and Klaus Greff and Andy Zeng and Igor Mordatch and Pete Florence},
	booktitle = ICML,
	title = {PaLM-E: An Embodied Multimodal Language Model},
	year = {2023}}

@inproceedings{RoboCook23,
	author = {Haochen Shi and Huazhe Xu and Samuel Clarke and Yunzhu Li and Jiajun Wu},
	booktitle = CoRL,
	title = {RoboCook: Long-Horizon Elasto-Plastic Object Manipulation with Diverse Tools},
	year = {2023}}

@article{hong20243d,
	author = {Hong, Yining and Zhen, Haoyu and Chen, Peihao and Zheng, Shuhong and Du, Yilun and Chen, Zhenfang and Gan, Chuang},
	journal = {Advances in Neural Information Processing Systems},
	title = {3d-llm: Injecting the 3d world into large language models},
	volume = {36},
	year = {2024}}

@article{xu2023pointllm,
	author = {Xu, Runsen and Wang, Xiaolong and Wang, Tai and Chen, Yilun and Pang, Jiangmiao and Lin, Dahua},
	journal = {arXiv preprint arXiv:2308.16911},
	title = {Pointllm: Empowering large language models to understand point clouds},
	year = {2023}}

@article{qi2024shapellm,
	author = {Qi, Zekun and Dong, Runpei and Zhang, Shaochen and Geng, Haoran and Han, Chunrui and Ge, Zheng and Yi, Li and Ma, Kaisheng},
	journal = {arXiv preprint arXiv:2402.17766},
	title = {ShapeLLM: Universal 3D Object Understanding for Embodied Interaction},
	year = {2024}}

@article{chat3d,
	author = {Wang, Zehan and Huang, Haifeng and Zhao, Yang and Zhang, Ziang and Zhao, Zhou},
	journal = {arXiv preprint arXiv:2308.08769},
	title = {Chat-3D: Data-efficiently Tuning Large Language Model for Universal Dialogue of 3D Scenes},
	year = {2023}}

@article{scenellm,
	author = {Fu, Rao and Liu, Jingyu and Chen, Xilun and Nie, Yixin and Xiong, Wenhan},
	journal = {arXiv preprint arXiv:2403.11401},
	title = {Scene-llm: Extending language model for 3d visual understanding and reasoning},
	year = {2024}}


@misc{VLM:Fuyu-8b,
	author = {Bavishi, Rohan and Elsen, Erich and Hawthorne, Curtis and Nye, Maxwell and Odena, Augustus and Somani, Arushi and Ta\c{s}\i{}rlar, Sa\u{g}nak},
	title = {Introducing our Multimodal Models},
	url = {https://www.adept.ai/blog/fuyu-8b},
	year = {2023},
	bdsk-url-1 = {https://www.adept.ai/blog/fuyu-8b}}


@article{VLM:OtterHD,
	author = {Bo Li and Peiyuan Zhang and Jingkang Yang and Yuanhan Zhang and Fanyi Pu and Ziwei Liu},
	journal = {arXiv: 2311.04219},
	title = {OtterHD: {A} High-Resolution Multi-modality Model},
	year = {2023}}

@article{VLM:MANTIS,
	author = {Jiang, Dongfu and He, Xuan and Zeng, Huaye and Wei, Con and Ku, Max and Liu, Qian and Chen, Wenhu},
	journal = {arXiv:2405.01483},
	title = {MANTIS: Interleaved Multi-Image Instruction Tuning},
	year = {2024}}

@misc{VLM:surveyMMB,
	archiveprefix = {arXiv},
	author = {Lin Li and Guikun Chen and Hanrong Shi and Jun Xiao and Long Chen},
	eprint = {2409.18142},
	primaryclass = {cs.AI},
	title = {A Survey on Multimodal Benchmarks: In the Era of Large AI Models},
	url = {https://arxiv.org/abs/2409.18142},
	year = {2024},
	bdsk-url-1 = {https://arxiv.org/abs/2409.18142}}

@inproceedings{VLP:CLIP,
	author = {Alec Radford and Jong Wook Kim and Chris Hallacy and Aditya Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},
	booktitle = {ICML},
	pages = {8748--8763},
	title = {Learning Transferable Visual Models From Natural Language Supervision},
	volume = {139},
	year = {2021}}

@misc{TransF:Vicuna,
	author = {Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.},
	month = {March},
	title = {Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\%* ChatGPT Quality},
	url = {https://lmsys.org/blog/2023-03-30-vicuna/},
	year = {2023},
	bdsk-url-1 = {https://lmsys.org/blog/2023-03-30-vicuna/}}

@article{TransF:Qwen,
	author = {Jinze Bai and Shuai Bai and Yunfei Chu and Zeyu Cui and Kai Dang and Xiaodong Deng and Yang Fan and Wenbin Ge and Yu Han and Fei Huang and Binyuan Hui and Luo Ji and Mei Li and Junyang Lin and Runji Lin and Dayiheng Liu and Gao Liu and Chengqiang Lu and Keming Lu and Jianxin Ma and Rui Men and Xingzhang Ren and Xuancheng Ren and Chuanqi Tan and Sinan Tan and Jianhong Tu and Peng Wang and Shijie Wang and Wei Wang and Shengguang Wu and Benfeng Xu and Jin Xu and An Yang and Hao Yang and Jian Yang and Shusheng Yang and Yang Yao and Bowen Yu and Hongyi Yuan and Zheng Yuan and Jianwei Zhang and Xingxuan Zhang and Yichang Zhang and Zhenru Zhang and Chang Zhou and Jingren Zhou and Xiaohuan Zhou and Tianhang Zhu},
	journal = {arXiv: 2309.16609},
	title = {Qwen Technical Report},
	year = {2023}}

@article{cai2024internlm2,
	author = {Cai, Zheng and Cao, Maosong and Chen, Haojiong and Chen, Kai and Chen, Keyu and Chen, Xin and Chen, Xun and Chen, Zehui and Chen, Zhi and Chu, Pei and others},
	journal = {arXiv preprint arXiv:2403.17297},
	title = {Internlm2 technical report},
	year = {2024}}

@article{zhou2024transfusion,
	author = {Zhou, Chunting and Yu, Lili and Babu, Arun and Tirumala, Kushal and Yasunaga, Michihiro and Shamis, Leonid and Kahn, Jacob and Ma, Xuezhe and Zettlemoyer, Luke and Levy, Omer},
	journal = {arXiv preprint arXiv:2408.11039},
	title = {Transfusion: Predict the next token and diffuse images with one multi-modal model},
	year = {2024}}

@article{emu3,
	author = {Wang, Xinlong and Zhang, Xiaosong and Luo, Zhengxiong and Sun, Quan and Cui, Yufeng and Wang, Jinsheng and Zhang, Fan and Wang, Yueze and Li, Zhen and Yu, Qiying and Zhao, Yingli and Ao, Yulong and Min, Xuebin and Li, Tao and Wu, Boya and Zhao, Bo and Zhang, Bowen and Wang, Liangdong and Liu, Guang and He, Zheqi and Yang, Xi and Liu, Jingjing and Lin, Yonghua and Huang, Tiejun and Wang, Zhongyuan},
	journal = {arXiv: 2409.18869},
	title = {Emu3: Next-Token Prediction is All You Need},
	year = {2024}}

@article{team2023gemini,
	author = {Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others},
	journal = {arXiv preprint arXiv:2312.11805},
	title = {Gemini: a family of highly capable multimodal models},
	year = {2023}}

@article{xie2024show,
	author = {Xie, Jinheng and Mao, Weijia and Bai, Zechen and Zhang, David Junhao and Wang, Weihao and Lin, Kevin Qinghong and Gu, Yuchao and Chen, Zhijie and Yang, Zhenheng and Shou, Mike Zheng},
	journal = {arXiv preprint arXiv:2408.12528},
	title = {Show-o: One single transformer to unify multimodal understanding and generation},
	year = {2024}}

@inproceedings{vqgan,
	author = {Esser, Patrick and Rombach, Robin and Ommer, Bjorn},
	booktitle = {Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
	pages = {12873--12883},
	title = {Taming transformers for high-resolution image synthesis},
	year = {2021}}

@article{diao2024EVE,
	author = {Diao, Haiwen and Cui, Yufeng and Li, Xiaotong and Wang, Yueze and Lu, Huchuan and Wang, Xinlong},
	journal = {arXiv preprint arXiv:2406.11832},
	title = {Unveiling Encoder-Free Vision-Language Models},
	year = {2024}}

@article{solo,
	author = {Chen, Yangyi and Wang, Xingyao and Peng, Hao and Ji, Heng},
	journal = {arXiv preprint arXiv:2407.06438},
	title = {A Single Transformer for Scalable Vision-Language Modeling},
	year = {2024}}

@article{mono_internvl,
	author = {Luo, Gen and Yang, Xue and Dou, Wenhan and Wang, Zhaokai and Dai, Jifeng and Qiao, Yu and Zhu, Xizhou},
	journal = {arXiv preprint arXiv:2410.08202},
	title = {Mono-InternVL: Pushing the Boundaries of Monolithic Multimodal Large Language Models with Endogenous Visual Pre-training},
	year = {2024}}

@article{qwen,
	author = {Jinze Bai and Shuai Bai and Yunfei Chu and Zeyu Cui and Kai Dang and Xiaodong Deng and Yang Fan and Wenbin Ge and Yu Han and Fei Huang and Binyuan Hui and Luo Ji and Mei Li and Junyang Lin and Runji Lin and Dayiheng Liu and Gao Liu and Chengqiang Lu and Keming Lu and Jianxin Ma and Rui Men and Xingzhang Ren and Xuancheng Ren and Chuanqi Tan and Sinan Tan and Jianhong Tu and Peng Wang and Shijie Wang and Wei Wang and Shengguang Wu and Benfeng Xu and Jin Xu and An Yang and Hao Yang and Jian Yang and Shusheng Yang and Yang Yao and Bowen Yu and Hongyi Yuan and Zheng Yuan and Jianwei Zhang and Xingxuan Zhang and Yichang Zhang and Zhenru Zhang and Chang Zhou and Jingren Zhou and Xiaohuan Zhou and Tianhang Zhu},
	journal = {arXiv preprint arXiv:2309.16609},
	title = {Qwen Technical Report},
	year = {2023}}

@article{team2024chameleon,
	author = {ChameleonTeam},
	journal = {arXiv preprint arXiv:2405.09818},
	title = {Chameleon: Mixed-modal early-fusion foundation models},
	year = {2024}}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{bai2023qwen,
  title={Qwen technical report},
  author={Bai, Jinze and Bai, Shuai and Chu, Yunfei and Cui, Zeyu and Dang, Kai and Deng, Xiaodong and Fan, Yang and Ge, Wenbin and Han, Yu and Huang, Fei and others},
  journal={arXiv preprint arXiv:2309.16609},
  year={2023}
}

@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}

@article{li2024llava,
  title={Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models},
  author={Li, Feng and Zhang, Renrui and Zhang, Hao and Zhang, Yuanhan and Li, Bo and Li, Wei and Ma, Zejun and Li, Chunyuan},
  journal={arXiv preprint arXiv:2407.07895},
  year={2024}
}

@article{liu2024visual,
  title={Visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  journal={Advances in neural information processing systems},
  volume={36},
  year={2024}
}

@article{guo2023point,
  title={Point-bind \& point-llm: Aligning point cloud with multi-modality for 3d understanding, generation, and instruction following},
  author={Guo, Ziyu and Zhang, Renrui and Zhu, Xiangyang and Tang, Yiwen and Ma, Xianzheng and Han, Jiaming and Chen, Kexin and Gao, Peng and Li, Xianzhi and Li, Hongsheng and others},
  journal={arXiv preprint arXiv:2309.00615},
  year={2023}
}

@inproceedings{xu2025pointllm,
  title={Pointllm: Empowering large language models to understand point clouds},
  author={Xu, Runsen and Wang, Xiaolong and Wang, Tai and Chen, Yilun and Pang, Jiangmiao and Lin, Dahua},
  booktitle={European Conference on Computer Vision},
  pages={131--147},
  year={2025},
  organization={Springer}
}

@inproceedings{zhang2023learning,
  title={Learning 3d representations from 2d pre-trained models via image-to-point masked autoencoders},
  author={Zhang, Renrui and Wang, Liuhui and Qiao, Yu and Gao, Peng and Li, Hongsheng},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={21769--21780},
  year={2023}
}

@inproceedings{liu2021swin,
  title={Swin transformer: Hierarchical vision transformer using shifted windows},
  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={10012--10022},
  year={2021}
}

@article{oquab2023dinov2,
  title={Dinov2: Learning robust visual features without supervision},
  author={Oquab, Maxime and Darcet, Timoth{\'e}e and Moutakanni, Th{\'e}o and Vo, Huy and Szafraniec, Marc and Khalidov, Vasil and Fernandez, Pierre and Haziza, Daniel and Massa, Francisco and El-Nouby, Alaaeldin and others},
  journal={arXiv preprint arXiv:2304.07193},
  year={2023}
}

@inproceedings{pang2022masked,
  title={Masked autoencoders for point cloud self-supervised learning},
  author={Pang, Yatian and Wang, Wenxiao and Tay, Francis EH and Liu, Wei and Tian, Yonghong and Yuan, Li},
  booktitle={European conference on computer vision},
  pages={604--621},
  year={2022},
  organization={Springer}
}

@article{zhang2022point,
  title={Point-m2ae: multi-scale masked autoencoders for hierarchical point cloud pre-training},
  author={Zhang, Renrui and Guo, Ziyu and Gao, Peng and Fang, Rongyao and Zhao, Bin and Wang, Dong and Qiao, Yu and Li, Hongsheng},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27061--27074},
  year={2022}
}

@inproceedings{qi2023contrast,
  title={Contrast with reconstruct: Contrastive 3d representation learning guided by generative pretraining},
  author={Qi, Zekun and Dong, Runpei and Fan, Guofan and Ge, Zheng and Zhang, Xiangyu and Ma, Kaisheng and Yi, Li},
  booktitle={International Conference on Machine Learning},
  pages={28223--28243},
  year={2023},
  organization={PMLR}
}

@inproceedings{deitke2023objaverse,
  title={Objaverse: A universe of annotated 3d objects},
  author={Deitke, Matt and Schwenk, Dustin and Salvador, Jordi and Weihs, Luca and Michel, Oscar and VanderBilt, Eli and Schmidt, Ludwig and Ehsani, Kiana and Kembhavi, Aniruddha and Farhadi, Ali},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={13142--13153},
  year={2023}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, A},
  journal={Advances in Neural Information Processing Systems},
  year={2017}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@article{chiang2023vicuna,
  title={Vicuna: An open-source chatbot impressing gpt-4 with 90\%* chatgpt quality},
  author={Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E and others},
  journal={See https://vicuna. lmsys. org (accessed 14 April 2023)},
  volume={2},
  number={3},
  pages={6},
  year={2023}
}

@inproceedings{zhang2023starting,
  title={Starting from non-parametric networks for 3d point cloud analysis},
  author={Zhang, Renrui and Wang, Liuhui and Wang, Yali and Gao, Peng and Li, Hongsheng and Shi, Jianbo},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={5344--5353},
  year={2023}
}

@inproceedings{xie2020pointcontrast,
  title={Pointcontrast: Unsupervised pre-training for 3d point cloud understanding},
  author={Xie, Saining and Gu, Jiatao and Guo, Demi and Qi, Charles R and Guibas, Leonidas and Litany, Or},
  booktitle={Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part III 16},
  pages={574--591},
  year={2020},
  organization={Springer}
}

@article{khosla2020supervised,
  title={Supervised contrastive learning},
  author={Khosla, Prannay and Teterwak, Piotr and Wang, Chen and Sarna, Aaron and Tian, Yonglong and Isola, Phillip and Maschinot, Aaron and Liu, Ce and Krishnan, Dilip},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={18661--18673},
  year={2020}
}

@article{zhou2023uni3d,
  title={Uni3d: Exploring unified 3d representation at scale},
  author={Zhou, Junsheng and Wang, Jinsheng and Ma, Baorui and Liu, Yu-Shen and Huang, Tiejun and Wang, Xinlong},
  journal={arXiv preprint arXiv:2310.06773},
  year={2023}
}

@article{dai2023instructblip,
  title={Instructblip: Towards general-purpose vision-language models with instruction tuning. arxiv 2023},
  author={Dai, Wenliang and Li, Junnan and Li, D and Tiong, AMH and Zhao, J and Wang, W and Li, B and Fung, P and Hoi, S},
  journal={arXiv preprint arXiv:2305.06500},
  volume={2},
  year={2023}
}

@article{hong20233d,
  title={3d-llm: Injecting the 3d world into large language models},
  author={Hong, Yining and Zhen, Haoyu and Chen, Peihao and Zheng, Shuhong and Du, Yilun and Chen, Zhenfang and Gan, Chuang},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={20482--20494},
  year={2023}
}

@article{qi2017pointnet++,
  title={Pointnet++: Deep hierarchical feature learning on point sets in a metric space},
  author={Qi, Charles Ruizhongtai and Yi, Li and Su, Hao and Guibas, Leonidas J},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{chen2024grounded,
  title={Grounded 3D-LLM with Referent Tokens},
  author={Chen, Yilun and Yang, Shuai and Huang, Haifeng and Wang, Tai and Lyu, Ruiyuan and Xu, Runsen and Lin, Dahua and Pang, Jiangmiao},
  journal={arXiv preprint arXiv:2405.10370},
  year={2024}
}

@article{diao2024unveiling,
  title={Unveiling Encoder-Free Vision-Language Models},
  author={Diao, Haiwen and Cui, Yufeng and Li, Xiaotong and Wang, Yueze and Lu, Huchuan and Wang, Xinlong},
  journal={arXiv preprint arXiv:2406.11832},
  year={2024}
}

@inproceedings{yu2022point,
  title={Point-bert: Pre-training 3d point cloud transformers with masked point modeling},
  author={Yu, Xumin and Tang, Lulu and Rao, Yongming and Huang, Tiejun and Zhou, Jie and Lu, Jiwen},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={19313--19322},
  year={2022}
}

@inproceedings{guo2023viewrefer,
  title={Viewrefer: Grasp the multi-view knowledge for 3d visual grounding},
  author={Guo, Zoey and Tang, Yiwen and Zhang, Ray and Wang, Dong and Wang, Zhigang and Zhao, Bin and Li, Xuelong},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={15372--15383},
  year={2023}
}

@inproceedings{tang2024point,
  title={Point-PEFT: Parameter-efficient fine-tuning for 3D pre-trained models},
  author={Tang, Yiwen and Zhang, Ray and Guo, Zoey and Ma, Xianzheng and Zhao, Bin and Wang, Zhigang and Wang, Dong and Li, Xuelong},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={6},
  pages={5171--5179},
  year={2024}
}

@inproceedings{tang2024any2point,
  title={Any2point: Empowering any-modality large models for efficient 3d understanding},
  author={Tang, Yiwen and Zhang, Ray and Liu, Jiaming and Guo, Zoey and Zhao, Bin and Wang, Zhigang and Gao, Peng and Li, Hongsheng and Wang, Dong and Li, Xuelong},
  booktitle={European Conference on Computer Vision},
  pages={456--473},
  year={2024},
  organization={Springer}
}