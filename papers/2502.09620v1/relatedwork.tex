\section{Related Work}
\textbf{3D LMM.} 
Recent advancements in integrating large language models (LLMs) with 3D data have led to significant progress in both object-level and scene-level understanding.
At the object level, early approaches like~\cite{hong20243d} utilize 2D rendering to leverage 2D LLMs, but this sacrifices geometric details. More recent models, including Point-Bind LLM~\cite{guo2023point}, PointLLM~\cite{xu2023pointllm} and ShapeLLM~\cite{qi2024shapellm}, directly encode point clouds and align them with LLMs, by combining the 3D encoder with a powerful language model, effectively fusing geometric, appearance, and linguistic information.
At the scene level, models like Chat-3D~\cite{chat3d} and Scene-LLM~\cite{scenellm} focus on understanding complex spatial relationships through dialogue and tasks like captioning. 
Scene-LLM~\cite{scenellm} enhances embodied agents' abilities in interactive 3D indoor environments by integrating both scene-level and egocentric 3D information.
Grounded 3D-LLM~\cite{chen2024grounded} utilizes referent tokens to reference specific objects within 3D scenes, enabling tasks such as object detection and language grounding.

\textbf{Encoder-free Vision-Language Models.} 
Traditional vision-language models (VLMs) often rely on vision encoders to extract visual features before processing them with language models, integrating image encoders like CLIP~\cite{VLP:CLIP} and DINO V2~\cite{oquab2023dinov2}. 
However, recent efforts have explored encoder-free VLMs for their simplicity. 
Approaches like~\cite{team2024chameleon, xie2024show} use VQ tokenizers~\cite{vqgan} or linear projection layers~\cite{diao2024EVE, solo} to represent images.
Fuyu-8B~\cite{VLM:Fuyu-8b}, a pure decoder-only model, directly processes image patches through linear projections, handling high-resolution images but showing only average performance. 
The EVE~\cite{diao2024unveiling} eliminates the need for a separate vision encoder by bridging vision-language representation within a unified decoder and enhancing visual recognition capabilities through additional supervision.


\begin{figure}[!h]
%\vspace{-0.1cm}
\centering
\includegraphics[width=0.9\linewidth]{loss2.png}
\vspace{-0.2cm}
\caption{\textbf{Variants of Point Cloud Self-Supervised Learning Losses.} 
(a) The Variant of Masked Modeling Loss, (b) The Variant of Reconstruction Loss, (c) The Variant of Hybrid Semantic Loss.}
  \label{loss2}
\end{figure}