\section{Related Work}
\textbf{3D LMM.} 
Recent advancements in integrating large language models (LLMs) with 3D data have led to significant progress in both object-level and scene-level understanding.
At the object level, early approaches like____ utilize 2D rendering to leverage 2D LLMs, but this sacrifices geometric details. More recent models, including Point-Bind LLM____, PointLLM____ and ShapeLLM____, directly encode point clouds and align them with LLMs, by combining the 3D encoder with a powerful language model, effectively fusing geometric, appearance, and linguistic information.
At the scene level, models like Chat-3D____ and Scene-LLM____ focus on understanding complex spatial relationships through dialogue and tasks like captioning. 
Scene-LLM____ enhances embodied agents' abilities in interactive 3D indoor environments by integrating both scene-level and egocentric 3D information.
Grounded 3D-LLM____ utilizes referent tokens to reference specific objects within 3D scenes, enabling tasks such as object detection and language grounding.

\textbf{Encoder-free Vision-Language Models.} 
Traditional vision-language models (VLMs) often rely on vision encoders to extract visual features before processing them with language models, integrating image encoders like CLIP____ and DINO V2____. 
However, recent efforts have explored encoder-free VLMs for their simplicity. 
Approaches like____ use VQ tokenizers____ or linear projection layers____ to represent images.
Fuyu-8B____, a pure decoder-only model, directly processes image patches through linear projections, handling high-resolution images but showing only average performance. 
The EVE____ eliminates the need for a separate vision encoder by bridging vision-language representation within a unified decoder and enhancing visual recognition capabilities through additional supervision.


\begin{figure}[!h]
%\vspace{-0.1cm}
\centering
\includegraphics[width=0.9\linewidth]{loss2.png}
\vspace{-0.2cm}
\caption{\textbf{Variants of Point Cloud Self-Supervised Learning Losses.} 
(a) The Variant of Masked Modeling Loss, (b) The Variant of Reconstruction Loss, (c) The Variant of Hybrid Semantic Loss.}
  \label{loss2}
\end{figure}