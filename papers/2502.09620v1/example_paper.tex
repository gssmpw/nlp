%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}
\usepackage{caption}
\usepackage{enumitem}
% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
%\usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{makecell}
\usepackage{multirow}
% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

\usepackage{ulem}
\usepackage{xspace}
\newcommand{\enel}{\textsc{Enel}\xspace}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Exploring the Potential of Encoder-free Architectures in 3D LMMs}

\begin{document}


\twocolumn[
\icmltitle{Exploring the Potential of Encoder-free Architectures in 3D LMMs}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}
\icmlsetsymbol{pl}{$\dagger$}


\begin{icmlauthorlist}
\icmlauthor{Yiwen Tang}{equal,jkgb,yyy}
\icmlauthor{Zoey Guo}{equal,comp}
\icmlauthor{Zhuhao Wang}{equal,sch}
\icmlauthor{Ray Zhang}{equal,pl,comp}
\icmlauthor{Qizhi Chen}{yyy}
\icmlauthor{Junli Liu}{yyy}
\icmlauthor{Delin Qu}{yyy}\vspace{0.1cm}\\
%\icmlauthor{}{sch}
\icmlauthor{Zhigang Wang}{yyy}
\icmlauthor{Dong Wang}{yyy}
\icmlauthor{Xuelong Li}{yyy}
\icmlauthor{Bin Zhao}{jkgb,yyy}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\vspace{0.3cm}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{Shanghai AI Laboratory}
\icmlaffiliation{comp}{The Chinese University of Hong Kong}
\icmlaffiliation{sch}{Tsinghua University}
\icmlaffiliation{jkgb}{Northwestern Polytechnical University}

% \icmlcorrespondingauthor{Firstname1 Lastname1}{first1.last1@xxx.edu}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}



% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]
%\printAffiliationsAndNotice{}% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution $^\dagger$Project Lead} % otherwise use the standard text.

\begin{abstract}
Encoder-free architectures have been preliminarily explored in the 2D visual domain, yet it remains an open question whether they can be effectively applied to 3D understanding scenarios. 
In this paper, we present the first comprehensive investigation into the potential of encoder-free architectures to overcome the challenges of encoder-based 3D Large Multimodal Models (LMMs). 
These challenges include the failure to adapt to varying point cloud resolutions and the point features from the encoder not meeting the semantic needs of Large Language Models (LLMs).
We identify key aspects for 3D LMMs to remove the encoder and enable the LLM to assume the role of the 3D encoder: 
1) We propose the LLM-embedded Semantic Encoding strategy in the pre-training stage, exploring the effects of various point cloud self-supervised losses. And we present the Hybrid Semantic Loss to extract high-level semantics.
2) We introduce the Hierarchical Geometry Aggregation strategy in the instruction tuning stage.
This incorporates inductive bias into the LLM early layers to focus on the local details of the point clouds.
To the end, we present the first Encoder-free 3D LMM, \textbf{\enel}, whose 7B model rivals the current state-of-the-art ShapeLLM-13B, achieving 55.0\%, 50.92\%, and 42.7\% on the classification, captioning, and VQA tasks, respectively.
Our results demonstrate the encoder-free architecture to be highly promising in the field of 3D LMMs.
Code is released at \url{https://github.com/Ivan-Tang-3D/ENEL}.
\end{abstract}

\section{Introduction}
\label{introduction}

Large Language Models (LLMs)~\cite{touvron2023llama, bai2023qwen, jiang2023mistral,cai2024internlm2} have gained unprecedented attention for their proficiency in understanding and generating complex language scenarios.
Building upon these advances, many recent efforts have been made to develop Large Multimodal Models (LMMs), empowering LLMs with the capability to interpret multimodal information, such as 2D images~\cite{liu2024visual,li2024llava,zhang2024llama,zhang2024mavis,zhang2024mathverse,lii2024llava} and 3D point clouds~\cite{guo2023point,xu2025pointllm,guo2023viewrefer,guo2024sam2point,PointCLIP22,jia2024lift3d}.

Mainstream LMMs are typically encoder-based, relying on heavyweight yet powerful multimodal encoders (e.g., CLIP~\cite{VLP:CLIP} for 2D~\cite{liu2021swin,oquab2023dinov2} and I2P-MAE~\cite{zhang2023learning} for 3D). While these pre-trained encoders offer robust multimodal embeddings enriched with pre-existing knowledge, they also introduce challenges that could limit the future advancement of multimodal understanding.

Specifically for 3D LMMs, the encoder-based architecture has the following potential drawbacks: 
(1) \textbf{\textit{Point Cloud Resolution Limitation.}}
3D encoders are often pre-trained on point cloud data at a fixed resolution, such as 1,024 points in PointLLM~\cite{xu2025pointllm}. However, during inference, the resolution of point clouds may vary (e.g., 8,192 or 512 points). This difference between training and inference resolutions can result in the loss of spatial information when extracting 3D embeddings, leading to difficulties for LLMs to comprehend, as showcased in Figure~\ref{intro} (a).
(2) \textbf{\textit{Embedding Semantic Discrepancy.}}
3D encoders are typically pre-trained using self-supervised methods like MAE~\cite{pang2022masked, tang2024point, tang2024any2point} and contrastive learning~\cite{xie2020pointcontrast,qi2023contrast}, but these training objectives may not align with the specific semantic needs of LLMs. In other words, they may not capture the most relevant semantics for LLMs to understand 3D objects, as visualized in Figure~\ref{intro} (b). Even when a projection layer is used to connect 3D encoders with LLMs, simple MLPs are often insufficient for a complete semantic transformation.
Given these issues, we ask: \textit{Is it possible to explore an encoder-free architecture for 3D LMMs, eliminating the 3D encoder and instead integrating its functionality directly within the LLM itself?}


% Take 3D LMMs as an example 

% with the visual embedding, but also incorporates specialized structural designs, such as convolutional layers with local biases and self-attention mechanisms that capture contextual interactions.
% However, such an architecture often suffers from limitations in handling images with varying resolutions and aspect ratios.
% Moreover, the features provided by the pre-trained encoder are not optimal for the LLM, as the targets during the training of the encoder and the LLM are misaligned. 
% For instance, CLIP utilizes contrastive learning, whereas LLMs typically rely on next-token prediction.
% Therefore, researchers have proposed encoder-free 2D LLMs, which bypass the vision encoder. 
% Instead, the emergence of visual perception capabilities relies on the pre-trained knowledge embedded within the LLM.
% This approach not only addresses the issues but also achieves performance on par with encoder-based 2D LLMs.

% Similarly, encoder-based 3D LLMs have the following potential drawbacks: 
% % (1) High-Dimensional Data Complexity. The high-dimensional complexity of 3D point cloud data requires more sophisticated encoding mechanisms, which significantly increase computational demands and processing time. 
% (1) \textbf{Point Cloud Resolution Limitations.}
% The 3D encoder is typically pre-trained on point cloud data with a fixed resolution.
% As a result, altering the density of the input point cloud during inference can lead to the loss of important spatial information due to the gap between the training and inference stages.
% However, the Patch Embedding module is insensitive to changes in resolution. Its non-parametric robust positional encoding, without the fixed-point pretraining, contributes to relatively stable feature outputs.
% (2) \textbf{Encoder Feature Inadequacy.}
% The features output by the 3D encoder, derived from training objectives that are inconsistent with those of the LLM, are not the most suitable for the LLM to understand 3D objects. 
% Meanwhile, the features cannot be easily transformed into consistent semantic representations through MLPs.
% These raise the question: \textit{Is it possible to explore the encoder-free architecture in the 3D LLMs, transferring the role handled by the 3D encoder to the unified LLM architecture?}

% Although the encoder-free approach has seen preliminary success in 2D visual understanding, the application of encoder-free architectures to 3D understanding tasks has not yet been fully explored for its effectiveness.
% Given the significant discrepancies between the data structures, we conduct a systematic and comprehensive investigation. 
\begin{figure*}[t!]
%\vspace{-0.1cm}
\centering
    \includegraphics[width=0.9\linewidth]{intro3.png}
  \caption{\textbf{Issues of encoder-based 3D LMMs.} 
  (a) \textbf{Point Cloud Resolution Limitation.}
During training, the point cloud size (P.T. size) and point token size (P.T. size) are fixed at 8192 and 512, respectively. 
And we adjust these two sizes during inference, point cloud size from 2K to 16K and the corresponding point token size from 128 to 2048.
We evaluate them on the captioning task of the Objaverse benchmark using GPT-4 scores as the evaluation metric.
  (b) \textbf{Embedding Semantic Discrepancy.}
  We visualize the attention scores of the average text token to the point tokens, where \textbf{red indicates higher values.}
  The point tokens in the encoder-free architecture exhibit stronger textual semantic relevance needed for the LLM.}
  \label{intro}
\end{figure*}

In this paper, we present the first systematic investigation into the potential of an encoder-free architecture for 3D LMMs. To minimize external influences and ensure clarity, we use the pioneering and sufficiently concise PointLLM~\cite{xu2025pointllm} as our encoder-based baseline, which consists of two progressive training stages: pre-training and instruction tuning. We evaluate the performance on 3D classification~\cite{deitke2023objaverse} and 3D captioning~\cite{deitke2023objaverse} tasks. Specifically, to remove the encoder while mitigating any performance degradation, we explore solutions to the following two key questions:

\textit{\textbf{(1)} How can we compensate for the high-level 3D semantics originally extracted by the 3D encoder?}
In 3D LMMs, the raw point cloud input is first passed through a token embedding module for low-level tokenization, before being processed by the main 3D encoder, usually a Transformer~\cite{vaswani2017attention}, to generate high-level embeddings. Skipping the encoder entirely poses a challenge in capturing the complex spatial structures of 3D point clouds.
To address this, we propose a strategy called \textbf{LLM-embedded Semantic Encoding} in the pre-training stage. First, we adopt a simple yet effective token embedding module that captures as much informative semantic content as possible. These 3D tokens are then directly fed into the LLM. Next, we aim to shift the responsibility of capturing high-level 3D semantics to the LLM itself. To facilitate this, we make the early layers of the LLM learnable, allowing them to specialize in 3D encoding. To guide this process, we explore various 3D self-supervised loss functions, such as reconstruction loss, masked modeling loss, and distillation loss, and ultimately propose the Hybrid Semantic Loss as the most effective choice.

\textbf{Observation:} \textit{Our adopted token embedding module, learnable LLM layers, and Hybrid Semantic Loss achieve comparable effectiveness to that of a pre-trained 3D encoder, effectively substituting it for high-level 3D semantics.}

\begin{figure*}[t!]
%\vspace{-0.1cm}
    \includegraphics[width=0.98\linewidth]{pipeline.pdf}
  \caption{\textbf{Overall Pipeline of \enel.} 
  The training is divided into two stages: the pre-training stage and the instruction tuning stage. In the first stage, we set the first \( K \) layers to be learnable and apply the proposed Hybrid Semantic Loss to embed high-level semantics into the LLM. In the second stage, we adopt the Hierarchical Geometric Aggregation strategy to capture local structures of point clouds.}
  \label{pipeline}
\end{figure*}

\textit{\textbf{(2)} How can we integrate inductive bias into LLMs for better perception of 3D geometric structures?}
Traditional 3D encoders typically embed explicit inductive bias into their architectures to progressively capture multi-level 3D geometries. For instance, models like Point-M2AE~\cite{zhang2022point} use a local-to-global hierarchy, which is a concept also common in convolutional layers for 2D image processing~\cite{he2016deep}. In contrast, LLMs employ standard Transformer architectures, where each layer processes the same number of tokens, representing the same semantic level across the network.
In the absence of the encoder, we introduce the approach of \textbf{Hierarchical Geometry Aggregation} during the second fine-tuning stage. In the early layers of the LLM, we aggregate 3D tokens based on their geometric distribution using Farthest Point Sampling (FPS) and $k$-Nearest Neighbor ($k$-NN) sampling. This approach enables the LLM to gradually integrate detailed 3D semantics and develop a more holistic understanding of the 3D object. In the later layers, we reverse this aggregation, propagating the tokens back to their original distribution to maintain the fine-grained representation necessary for effective semantic communication.

\textbf{Observation:}
\textit{We find that this hierarchical design can facilitate the acquisition of multi-level knowledge and better comprehend the 3D geometries of complex point clouds.}
% further adjusting the Patch Embedding structure and the number of learnable layers in the LLM better harnesses the potential of the encoder-free architecture. 

Through a series of experimental investigations, we have uncovered the strong potential of applying encoder-free architecture to the 3D LMM domain.
% , showcasing performance that surpasses encoder-based approaches. 
Building on our insights, we introduce \textbf{\enel}, an \textbf{EN}coder-fre\textbf{E} 3D \textbf{L}MM evolved from Vicuna-7B~\cite{chiang2023vicuna} using the same training dataset from PointLLM.
Notably, without any 3D encoders, \enel achieves comparable performance to the current state-of-the-art ShapeLLM-13B~\cite{qi2024shapellm}, attaining scores of 55.0\% and 50.92\% on the classification and captioning tasks, respectively.
We hope \enel may provide the community with a scalable and effective path for adapting the encoder-free architecture to 3D scenarios.

Our main contributions are summarized as follows:

\textbullet\ We present the first comprehensive empirical study of applying encoder-free architectures to the 3D LMM domain, offering valuable insights for the field.

\textbullet\ We aim to transfer the original roles of 3D encoders to the LLM itself, and propose the LLM-embedded Semantic Encoding and Hierarchical Geometry Aggregation strategy, both of which have been validated as effective.

\textbullet\ We further introduce \textbf{\enel}, a concise and well-performed encoder-free 3D LMM, which achieves 50.92\%, 55.0\% and 42.70\% on 3D captioning, classification, and 3D VQA tasks, respectively, on par with existing encoder-based models.


\section{Investigation of Encoder-free 3D LMM}

Encoder-free modeling has been explored in the 2D vision domain to address issues related to image resolution and deployment overload. 
In this study, we conduct a comprehensive investigation to analyze the feasibility of adopting encoder-free architectures for 3D understanding tasks.

\subsection{Overall Architecture}


\textbf{Task Formulation.}

We present the first attempt to extend the encoder-free architecture to 3D LMMs, such as PointLLM~\cite{xu2023pointllm} and ShapeLLM~\cite{qi2024shapellm}, in order to efficiently handle the complex tasks like embodied agent~\cite{guo2023viewrefer} and vision-language navigation. 
We select PointLLM as the baseline model for the exploration and evaluate the performance of different strategies on the Objaverse dataset~\cite{deitke2023objaverse}, using GPT-4 scores combined with traditional metrics as our evaluation metrics.  
The benchmark is highly challenging, as it requires the model to achieve high-quality alignment between 3D semantics and textual space while also capturing the intricate geometric structures of 3D objects. 

In the overall architecture, the encoder-free 3D LMM directly utilizes a token embedding module to convert point cloud data into discrete point tokens, which are then concatenated with text tokens to serve as input to the LLM. 
As shown in Figure~\ref{pipeline}, to assume the role of the encoder, the LLM is guided to extract high-level semantic features of the point clouds and acquire multi-level knowledge from both global and local perspectives. 
In the subsequent sections, we primarily explore two strategies within the encoder-free architecture: LLM-embedded Semantic Encoding (Section \ref{selr}) and Hierarchical Geometry Aggregation (Section \ref{felr}).

\begin{table}[tb]
\caption{\textbf{Token Embedding.} We evaluate the performance on the Objaverse benchmark and adopt PointLLM-7B as the baseline model. 'Cls' and 'Cap' represent classification and captioning tasks, respectively. S-BERT refers to the Sentence-BERT. T.E. stands for our designed token embedding module.}
\vspace{0.15cm}
\centering
\begin{tabular}{l|c|cc}
\toprule
\multirow{2}{*}{\textbf{Method}} & \multicolumn{1}{c|}{\textbf{Cls}} & \multicolumn{2}{c}{\textbf{Cap}} \\ \cmidrule{2-4} 
 & \textbf{GPT-4} & \textbf{GPT-4} & \textbf{S-BERT} \\
 \midrule
PointLLM-7B & 53.00 & 44.85 & 47.47 \\ 
\midrule
 - Encoder & 35.50 & 33.37 & 41.19 \\ 
 \midrule
+ 2-layer T.E. & 42.50 & 41.35 & 44.25 \\ 
\textbf{+ 3-layer T.E.} & \textbf{47.31} & \textbf{43.86} & \textbf{45.89} \\ 
+ 4-layer T.E. & 45.00 & 42.99 & 44.51 \\ 
\bottomrule
\end{tabular}
\label{tab:results_pe}
%\vspace{-0.2cm}
\end{table}

\textbf{Token Embedding.}
We first remove the encoder of PointLLM and adopt the original token embedding~\cite{yu2022point}. However, the coarse structural design results in a significant performance degradation, as observed in Table~\ref{tab:results_pe}, where the GPT-4 scores for the classification and captioning tasks decrease by 17.5\% and 10.48\%, respectively.
To mitigate excessive information loss and provide refined local features to the LLM, we adopt a small network with a limited number of parameters, which is a lightweight variant of Point-PN~\cite{zhang2023starting}. 
Specifically, for the input \( \{ P_i \}_{i=1}^{N} \), we apply Farthest Point Sampling (FPS) for downsampling the number of points, k-Nearest Neighbors (k-NN) with group size k for local aggregation, and learnable linear layers for feature encoding. 
After a series of repetitive operations and the projection layer, we transform the point cloud into high-dimensional vectors \( \{ F_i \}_{i=1}^{M} \in \mathbb{R}^{M \times D_1} \).
In Table~\ref{tab:results_pe}, we experiment with token embedding at different depths and find that three layers yield the best performance, while two layers fail to capture complex point features and four layers introduce noise.


\textbf{Further 3D Encoding.}
We discover that the absence of the encoder results in a lack of context modeling in point cloud feature processing.
Therefore, we attempt to have the early layers of the LLM take on the encoder's role in capturing global interactions of features, further encoding the point cloud features.
In the pre-training stage, we set the first K layers of the frozen LLM to be learnable, utilizing the self-attention mechanism to capture global geometric structures. 
Meanwhile, we experiment with both the original learning rate and a smaller learning rate. 
As shown in Table~\ref{tab:results_layer}, the smaller learning rate generally leads to better results. 
This is because a smaller learning rate can make the optimization process of the early layers more stable.
Based on the designed token embedding module, setting the first four layers to be learnable yields the best results, as it effectively encodes low-level features into high-level representations with considerable computational efficiency.

\begin{table}[tb]
\caption{\textbf{Further 3D Encoding.} We set the LLM early layers to be learnable. LR represents the learning rate during the pre-training stage, with the original learning rate set to 2e-3.}
% \vspace{1pt}
\vspace{0.15cm}
\centering
\resizebox{0.47\textwidth}{!}{
\begin{tabular}{l|c|c|cc}
\toprule
\multirow{2}{*}{\textbf{Method}} & \multirow{2}{*}{\textbf{LR}} & \multicolumn{1}{c|}{\textbf{Cls}} & \multicolumn{2}{c}{\textbf{Cap}} \\ \cmidrule{3-5} 
 & & \textbf{GPT-4} & \textbf{GPT-4} & \textbf{S-BERT} \\ 
 \midrule
PointLLM-7B &2e-3 & 53.00 & 44.85 & 47.47 \\ 
\midrule
\multirow{2}{*}{+ 2 learnable layers} &2e-3 & 41.06 & 42.23 & 45.92 \\ 
                                    &4e-4 & 45.5 & 44.72 & 47.35 \\ 
\midrule
\multirow{2}{*}{\textbf{+ 4 learnable layers}} &2e-3 & 44.85 & 41.53 & 46.77 \\  
                                    &\textbf{4e-4} & \textbf{49.11} & \textbf{45.39} & \textbf{47.71} \\ 
\midrule
\multirow{2}{*}{+ 8 learnable layers} &2e-3 & 43.76 & 39.71 & 42.38 \\  
                                    &4e-4 & 48.00 & 44.49 & 47.21 \\ 
\bottomrule
\end{tabular}
}
\label{tab:results_layer}
%\vspace{-0.4cm}
\end{table}

\subsection{LLM-embedded Semantic Encoding}
\label{selr}
The lack of the 3D encoder results in insufficient encoding of point cloud semantic information, which greatly hinders the LLM to understand the structural details of point clouds. 
Most existing 3D encoders use self-supervised losses to embed the high-level semantics of point clouds into the transformer, primarily categorized into four types: Masked Modeling Loss~\cite{pang2022masked}, Reconstruction Loss~\cite{qi2023contrast}, Contrastive Loss~\cite{khosla2020supervised}, and Knowledge Distillation Loss~\cite{zhang2023learning}. 
Based on the proposed token embedding module and LLM learnable early layers, we implement and evaluate the effects of these losses on the encoder-free 3D LMM in the pre-training stage, as described in Figure~\ref{loss}.
Finally, we propose the Hybrid Semantic Loss, which assists the LLM to learn the relationship between local spatial information in the point clouds and grasp the high-level 3D semantics.

\begin{figure*}[t!]
%\vspace{-0.1cm}
    \includegraphics[width=0.98\linewidth]{loss1.pdf}
\vspace{-0.3cm}
  \caption{\textbf{Point Cloud Self-Supervised Learning Losses.} 
In the pre-training stage, we explore common self-supervised learning losses for the encoder-free 3D LMM: (a) Masked Modeling Loss, (b) Reconstruction Loss, (c) Contrastive Loss, and (d) Knowledge Distillation Loss.
The (e) represents our proposed Hybrid Semantic Loss, specifically designed for the encoder-free architecture.}
  \label{loss}
\end{figure*}

\textbf{Masked Modeling Loss.}
In the pre-training stage, we apply the Masked Modeling Loss to the point tokens processed by the LLM, as shown in Figure~\ref{loss} (a).
Through the Farthest Point Sampling (FPS) and k-Nearest Neighbors (k-NN) algorithms in the token embedding, the point clouds \( \{ P_i \}_{i=1}^{N} \) are divided into point patches \( \{ G_i \}_{i=1}^{M} \in \mathbb{R}^{M \times k \times 3} \) and the corresponding point tokens \( \{ F_i \}_{i=1}^{M} \). 
We randomly mask the point tokens with a masking ratio \( r \), and replace them with learnable tokens.
The masked feature tokens can be denoted as \( \{ F_{\text{gt}_i} \}_{i=1}^{M*r} \), which serve as the ground truth for the loss computation.
After the learnable tokens are concatenated with visible tokens and processed by the LLM, a linear layer is applied to extract the point tokens  \( \{ F_{\text{pre}_i} \}_{i=1}^{M*r} \in \mathbb{R}^{M*r \times D_1} \), and the Mean Squared Error (MSE) is computed between the predicted \(  F_{\text{pre}} \) and the ground truth \(  F_{\text{gt}} \).
The optimization can be written as
\begin{equation}
\mathcal{L}_{\text{mask}} = \frac{1}{M*r} \sum_{i=1}^{M*r} \left( \| F_{\text{pre}_i} - F_{\text{gt }_i} \|_2^2 \right).
\end{equation}
The specific process of applying Masked Modeling to point patches \( G \) is detailed in Appendix~\ref{exp}.

\textbf{Reconstruction Loss.}
After the point feature tokens \( \{ F_i \}_{i=1}^{M} \) are encoded by the LLM, the tokens are transformed to the point patches \( \{ G_{\text{pre}_i} \}_{i=1}^{M} \in \mathbb{R}^{M \times k \times 3} \) through a linear layer. 
We utilize the $l_2$ chamfer distance to align the predicted \(  G_{\text{pre}} \) with the ground truth \(  G \), reconstructing the original spatial information, as illustrated in Figure~\ref{loss} (b).
This approach encourages the LLM to learn the high-level semantics of the point cloud while preserving the critical structure and key features of the point cloud input.
The optimization target $L_{\text{recon}}$ can be written as
\begin{equation}
\frac{1}{M} \sum_{i=1}^{M} \left( \min_{j} \| a_i - b_j \|_2^2 + \min_{j} \| b_i - a_j \|_2^2 \right),
\end{equation}
where $a = G_{\text{pre}}$ and $b = G$.
The detailed procedure for reconstructing feature tokens F can be found in Appendix~\ref{exp}.

\begin{table}[tb]
\caption{\textbf{LLM-embedded Semantic Encoding.} In the pre-training stage, we explore the effects of various self-supervised learning losses targeting point tokens. \( \Psi \) represents a mask ratio of 60\%, while \( \Phi \) represents a mask ratio of 30\%.
The subscript ${\text{patch}}$ and ${\text{feat}}$ represent the loss target.
For Hybrid Semantic Loss, the subscript ${\text{patch}}$ and ${\text{feat}}$ represent the masked modeling target, while the reconstruction target is the corresponding ${\text{feat}}$ and ${\text{patch}}$.}
\vspace{0.15cm}
\centering
\resizebox{0.45\textwidth}{!}{
\begin{tabular}{l|c|c|c}
\toprule
\multirow{2}{*}{\textbf{Method}} & \multicolumn{1}{c|}{\textbf{Cls}} & \multicolumn{2}{c}{\textbf{Cap}} \\ \cmidrule{2-4} 
 & \textbf{GPT-4} & \textbf{GPT-4} & \textbf{S-BERT} \\ 
\midrule
PointLLM-7B & 53.00 & 44.85 & 47.47 \\ 
\midrule
Masked Modeling Loss$_{\text{patch}}$$^{\Psi}$ & 48.50 & 45.34 & 46.36 \\ 
Masked Modeling Loss$_{\text{patch}}$$^{\Phi}$ & 50.00 & 46.80 & 47.29 \\ 
Masked Modeling Loss$_{\text{feat}}$$^{\Psi}$ & 50.00 & 45.80 & 46.29 \\ 
Masked Modeling Loss$_{\text{feat}}$$^{\Phi}$ & 49.50 & 47.35 & 47.93 \\ 
\midrule
Reconstruction Loss$_{\text{patch}}$ & 49.50 & 46.96 & 47.33 \\ 
Reconstruction Loss$_{\text{feat}}$ & 48.50 & 45.95 & 47.18 \\ 
\midrule
Contrastive Loss & 43.50 & 42.91 & 44.77 \\ 
Knowledge Distillation Loss & 49.50 & 45.43 & 47.09 \\ 
\midrule
Hybrid Semantic Loss$_{\text{patch}}$ & 50.50 & 46.84 & 47.59 \\ 
Hybrid Semantic Loss$_{\text{feat}}$ & 52.00 & 48.51 & 48.06 \\ 
\textbf{+ Position Embedding} & \textbf{53.00} & \textbf{48.85} & \textbf{48.00} \\
\bottomrule
\end{tabular}
}
\label{tab:results_loss}
\vspace{-0.4cm}
\end{table}

\textbf{Contrastive Loss.}
We conduct contrastive learning~\cite{khosla2020supervised} at the point cloud level, where we contrast two transformed versions of the point cloud in the Figure~\ref{loss} (c). 
Given a sampled point cloud \( \{ P_i \}_{i=1}^{N} \), we apply two random geometric transformations \( T_1 \) and \( T_2 \), including rotation and translation, to obtain \( P_{T1} \) and \( P_{T2} \). 
The two augmented point clouds are separately paired with the original text query and processed through the LLM to obtain their respective feature tokens \( F_{T1} \in \mathbb{R}^{M \times D_1} \) and \( F_{T2} \in \mathbb{R}^{M \times D_1} \). 
Within the mini-batch, the two feature tokens derived from the same point cloud serve as positive pairs, while they are considered negative pairs with other point clouds. 
Based on the NCESoftmaxLoss, we aim to maximize the similarity of positive pairs and minimize the similarity of negative pairs, forcing the LLM to learn the geometric equivariance of point clouds. 
The specific formula $\mathcal{L}_{\text{contrast}}$ is as follows:
\begin{equation}
\frac{1}{B} \sum_{i=1}^{B} \left( - \log \frac{\exp(\mathbf{F}_{T1_i} \cdot \mathbf{F}_{T2_i} / \tau)}{\sum_{j=1}^{B} \exp(\mathbf{F}_{T1_i} \cdot \mathbf{F}_{T2_j} / \tau)} \right),
\end{equation}
where B stands for the training batch size.

\textbf{Knowledge Distillation Loss.}
We select the powerful Uni3D-L~\cite{zhou2023uni3d} as the teacher encoder, input the point cloud into the 3D encoder, and obtain the output feature \( F_{\text{teacher}} \in \mathbb{R}^{M \times D_2} \). 
The Mean Squared Error (MSE) between the LLM output tokens \( F_{\text{student}} \) and \( F_{\text{teacher}} \) is computed to align \( F_{\text{student}} \) as closely as possible to \( F_{\text{teacher}} \), thereby transferring the knowledge embedded in the 3D encoder to the LLM. 
By obtaining additional supervision from the Uni3D, the LLM better captures the complex structures in the point cloud data, as displayed in Figure~\ref{loss} (d). 
The objective function can be defined as
\begin{equation}
\mathcal{L}_{\text{KD}} = \frac{1}{M} \sum_{i=1}^{M} \left( \| F_{\text{student}_i} - F_{\text{teacher}_i} \|_2^2 \right).
\end{equation}

\begin{table}[tb]
\caption{\textbf{Hierarchical Geometry Aggregation.} In the instruction tuning stage, we conduct the experiments of Hierarchical Geometry Aggregation strategy. 
$l$ represents the number of aggregation and propagation operations.
$H$ refers to the LLM layers between \( l \) aggregation and \( l \) propagation operations.
+ Self-Attn. represents the incorporation of the gated self-attention in the aggregation.
}
\vspace{0.15cm}
\centering
\resizebox{0.35\textwidth}{!}{
\begin{tabular}{c|c|c|c}
\toprule
\multirow{2}{*}{\textbf{Method}} & \multicolumn{1}{c|}{\textbf{Cls}} & \multicolumn{2}{c}{\textbf{Cap}} \\ \cmidrule{2-4} 
 & \textbf{GPT-4} & \textbf{GPT-4} & \textbf{S-BERT} \\ 
 \midrule
PointLLM-7B & 53.00 & 44.85 & 47.47 \\ 
\midrule
$l$=1 & 52.50 & 48.86 & 48.14 \\ 
$l$=2 & 50.00 & 46.76 & 47.95 \\ 
$l$=3 & 48.00 & 45.51 & 46.85 \\ 
\midrule
$H$=2 & 53.50 & 49.13 & 48.33 \\ 
$H$=4 & 52.50 & 48.39 & 47.75 \\  
$H$=8 & 51.00 & 48.95 & 47.97 \\ 
\midrule
\textbf{+ Self-Attn.} & \textbf{55.00} & \textbf{50.92} & \textbf{48.61} \\ 
\bottomrule
\end{tabular}
}
\label{tab:results_agg}
%\vspace{-0.4cm}
\end{table}

\textbf{Experiments and Insights.}
As shown in Table~\ref{tab:results_loss}, we compare the effects of common self-supervised learning losses in the pre-training stage, where they are summed with the LLM cross-entropy loss~\cite{touvron2023llama}, each with a coefficient of 1. 
The observations are summarized as below:

\begin{itemize}[left=0pt]
\item \textbf{The point cloud self-supervised learning losses generally benefit the encoder-free 3D LMM.}
Compared to previous experimental results, where the GPT scores for the classification and captioning tasks are 49.11\% and 45.39\%, the self-supervised losses bring about the significant improvements.
This is because the self-supervised learning loss forces transformations on the complex point clouds through certain task design. 
This encourages the LLM to not simply memorize specific point cloud data but to learn the underlying geometric relationships and high-level semantic information. 
\item \textbf{Among the self-supervised learning losses, the Masked Modeling Loss demonstrates the strongest performance improvement.}
It achieves GPT-4 scores of 49.5\% and 47.35\% for classification and captioning tasks, respectively. 
% Overall, the trend shows that Masked Modeling Loss outperforms Reconstruction Loss, which in turn surpasses Knowledge Distillation Loss, followed by Contrastive Loss. 
The application of the masked modeling to the point features facilitates the embedding of high-level semantics from point clouds into the LLM.
However, the mask ratio is directly proportional to the training optimization difficulty, and increasing it from 30\% to 60\% results in a performance degradation.
In addition, explicitly reconstructing point patches is not as effective as masked modeling in capturing the critical features of the input, but it does help the LLM learn the complex patterns within point clouds. 
Knowledge Distillation Loss falls short compared to the first two losses.
Finally, Contrastive Loss, which fails to extract the detailed semantics, achieves the lowest performance.
\end{itemize}

\begin{figure}[t!]
%\vspace{-0.1cm}
\centering
    \includegraphics[scale=0.27]{geometry.pdf}
  \caption{\textbf{Hierarchical Geometry Aggregation Strategy.} 
  In the instruction tuning stage, we apply aggregation and propagation operations to the point tokens to capture the local structural details.}
  \label{geometry}
\end{figure}

\begin{table*}[tb]
    \centering
    \caption{\textbf{Comparison of different models on various 3D understanding tasks.}
    A primary focus is placed on GPT-4 evaluation, along with data-driven metrics (Sentence-BERT and SimCSE).}
\vspace{0.15cm}
    \resizebox{0.93\textwidth}{!}{
    \renewcommand{\arraystretch}{1.2}
    \setlength{\tabcolsep}{6pt} % Adjust column spacing
    \begin{tabular}{l|cccccc|c|c}
        \toprule
        \multirow{2}{*}{\textbf{Model}} & \multicolumn{6}{c|}{\textbf{Cap}} & \multicolumn{1}{c|}{\textbf{Cls}} & \multicolumn{1}{c}{\textbf{QA}} \\ \cmidrule{2-9} 
        & \textbf{GPT-4} & \textbf{Sentence-BERT} & \textbf{SimCSE} & \textbf{BLEU-1} & \textbf{ROUGE-L} & \textbf{METEOR} & \textbf{GPT-4} & \textbf{GPT-4} \\
        \midrule
        InstructBLIP-7B~\cite{dai2023instructblip} & 45.34 & 47.41 & 48.48 & 4.27 & 8.28 & 12.99 & 43.50 & -- \\
        InstructBLIP-13B~\cite{dai2023instructblip} & 44.97 & 45.90 & 48.86 & 4.65 & 8.85 & 13.23 & 34.25 & -- \\
        LLaVA-7B~\cite{liu2024visual}  & 46.71 & 45.61 & 47.10 & 3.64 & 7.70 & 12.14 & 50.00 & -- \\
        LLaVA-13B~\cite{liu2024visual}  & 38.28 & 46.37 & 45.90 & 4.02 & 8.15 & 12.58 & 51.75 & 47.90 \\
        \midrule
        3D-LLM~\cite{hong20233d} & 33.42 & 44.48 & 43.68 & \textbf{16.91} & \textbf{19.48} & \textbf{19.73} & 45.25 & -- \\
        PointLLM-7B~\cite{xu2023pointllm}  & 44.85 & 47.47 & 48.55 & 3.87 & 7.30 & 11.92 & 53.00 & 41.20 \\
        PointLLM-13B~\cite{xu2023pointllm} & 48.15 & 47.91 & 49.12 & 3.83 & 7.23 & 12.26 & 54.00 & 46.60 \\
        ShapeLLM-7B~\cite{qi2024shapellm}  & 46.92 & 48.20 & 49.23 & -- & -- & -- & 54.50 & 47.40 \\
        ShapeLLM-13B~\cite{qi2024shapellm} & 48.94 & 48.52 & \textbf{49.98} & -- & -- & -- & 54.00 & \textbf{53.10} \\
        \textbf{\enel-7B} & \textbf{50.92} & \textbf{48.61} & 49.31 & 3.88 & 7.20 & 12.50 & \textbf{55.00} & 42.70  \\
        \bottomrule
    \end{tabular}
    }
    \label{tab:results}
\end{table*}

\textbf{Hybrid Semantic Loss.} 
Based on the experimental results above, we propose the self-supervised learning loss specifically designed for the encoder-free 3D LMMâ€”Hybrid Semantic Loss, as showcased in Figure~\ref{loss} (e). 
We apply a masking ratio \( r \) to randomly mask point tokens from the token embedding. The masked tokens and the corresponding patches are referred to as \( \{ F_{\text{mask}_i} \}_{i=1}^{M*r}\) and \( \{ G_{\text{mask}_i} \}_{i=1}^{M*r}\), respectively. 
The remaining tokens are denoted as\( \{ F_{\text{vis}_i} \}_{i=1}^{M*(1-r)}\) and \( \{ G_{\text{vis}_i} \}_{i=1}^{M*(1-r)}\). 
For the masked portion, we adopt masked modeling, and for the visible portion, we use the reconstruction strategy. 
The inverse modeling process is described in Appendix~\ref{exp}.
Considering the autoregressive nature of the LLM and the unordered attribute of point clouds, we directly concatenate learnable tokens \( \{ F_{\text{learn}_i} \}_{i=1}^{M*r}\) to the end of \( F_{\text{vis}} \), replacing the masked tokens. 
After passing point tokens through the LLM, we compute the MSE between \( F_{\text{learn}} \) and \( F_{\text{mask}} \). 
The visible features \( F_{\text{vis}} \) are transformed into \( G_{\text{pred}} \), and the \( L_2 \) Chamfer distance is computed between \( G_{\text{pred}} \) and \( G_{\text{vis}} \). 
These two are added to the original cross-entropy loss with coefficients all equal to 1.
This approach not only embeds high-level semantics into the LLM but also ensures geometric consistency throughout the point cloud learning process. 
With a mask ratio of 30\%, it achieves 52.00\% and 48.51\% for the classification and captioning tasks. 
Adding positional encodings to the learnable and visible tokens at each layer of the LLM further yields improvements of +1\% and +0.34\%, respectively, enhancing the perception of spatial positional information.

\subsection{Hierarchical Geometry Aggregation}
\label{felr}

3D encoders are designed with specific structures tailored for point clouds, such as local-to-global hierarchy~\cite{zhang2022point} for exploring the geometric structure of the point cloud. 
However, in encoder-free architectures, the LLM itself does not have an explicit local modeling module. 
The self-attention mechanism is intended for modeling global interactions. 
Therefore, building upon the proposed Hybrid Semantic Loss, we explore in the instruction tuning stage how to enable the LLM to actively perceive 3D local details and complement the learned global semantics. 
To this end, we propose the Hierarchical Geometry Aggregation strategy in the LLM early layers. 

\begin{figure*}[t!]
%\vspace{-0.1cm}
    \includegraphics[width=0.93\linewidth]{vis.pdf}
  \caption{\textbf{Difference in Semantic Encoding.} 
By visualizing the attention scores of the average text token to the point tokens on the Objaverse dataset, we compare the semantic encoding potential of encoder-based and encoder-free architectures, where red indicates higher values.
And (a) represents chairs, (b) represents airplanes, and (c) represents lamps.}
  \label{vis}
\end{figure*}


\textbf{Implementation Details.}
As depicted in Figure~\ref{geometry}, from the LLM second layer, the input point tokens \( \{ F_{\text{input}_i} \}_{i=1}^{M} \), based on their corresponding coordinates \( \{ P_{\text{input}_i} \}_{i=1}^{M} \), are downsampled using the Farthest Point Sampling (FPS), reducing the token number from \( M \) to \( M/2 \), denoted as \( F_{\text{input}}^c \), which serve as the local centers. 
Then, using the k-Nearest Neighbor (k-NN) algorithm, we obtain the neighboring points \( F_{\text{input}}^n \in \mathbb{R}^{M/2 \times k \times D_1} \) for the center points. 
For \( F_{\text{input}}^n \), we employ the gated self-attention mechanism for intra-group interactions, grasping the local geometric structure. 
We multiply the self-attention output by a learnable parameter initialized from zero to adaptively adjust the required knowledge. 
We formulate it as
\begin{align}
\begin{split}
    {F_{\text{input}}^{n}}' = tanh(\alpha)*\text{Self-Attn.}({F_{\text{input}}^{n}})+{F_{\text{input}}^{n}}.
\end{split}
\end{align}
On top of this, we apply pooling to fuse the features ${F_{\text{input}}^{n}}'$ within each neighbor and add them to the original center tokens \( F_{\text{input}}^c \), yielding aggregated tokens \( \{ F_{\text{agg}_i}^1 \}_{i=1}^{M/2} \), formulated as
\begin{align}
\begin{split}
    {F_{\text{agg}}^1} = \text{Pooling}({F_{\text{input}}^{n}}')+{F_{\text{input}}^{c}}.
\end{split}
\end{align}
Then we perform \( l-1 \) iterations of geometry aggregation, resulting in \( \{ F_{\text{agg}_i}^l \}_{i=1}^{M/2^l} \).
To ensure that the LLM fully extracts the local information, we choose to perform further semantic modeling using \( H \) LLM layers after aggregation operations. 
This allows the model to learn the interactions between local information while preventing the loss of fine-grained geometric details.
Subsequently, we perform \( l \) iterations of geometry propagation. Following the approach of PointNet++~\cite{qi2017pointnet++}, we propagate the aggregated features \( F_{\text{agg}}^l \) from the local center points to their surrounding \( k \) neighboring points, generating \( \{ F_{\text{pro}_i}^1 \}_{i=1}^{M/2^{(l-1)}} \). After \( l \) iterations, we obtain point tokens of length \( M \), which are then processed by the remaining $L-2l-(H+1)$ layers.

\textbf{Experiments and Insights.} 
We conduct step-by-step experiments on the Hierarchical Geometry Aggregation strategy, sequentially evaluating the impacts of the number of aggregation and propagation operations (\( l \)), the number of LLM layers between aggregation and propagation (\( H \)), and the incorporation of the gated self-attention mechanism.

\begin{itemize}[left=0pt]
\item
\textbf{The best performance is achieved when $l$ is set to 1 and the performance decreases as \( l \) increases.}
As observed in Table~\ref{tab:results_agg}, performing single aggregation and propagation operation achieves 48.86\% and 52.5\% performance on the captioning and classification tasks, respectively, while additional operations lead to a performance drop of 3\%-4\%. 
This is because as \( l \) increases, repeated aggregation operations progressively simplify spatial relationships, causing the loss of fine-grained geometric structures in the point clouds.
\item
\textbf{Compared to setting \( H \) to 4 or 8, the highest performance is achieved when \( H \) is set to 2}. 
It reaches 53.5\% and 49.13\% on the classification and captioning tasks, respectively.
The excessive number of LLM layers between aggregation and propagation can lead to the oversmoothing of the aggregated local information, resulting in the loss of local structural details.
\item
\textbf{The introduction of the gated self-attention mechanism effectively enhances performance}, achieving the GPT score of 55\% in the classification task and 50.92\% in the captioning task.
The adaptive control of attention output ensures that global contextual information is utilized only when necessary, preventing it from disrupting local geometric structures. 
Additionally, it allows the model to adjust to different tasks.
\end{itemize}

\section{Results and Visualization}
\label{resvis}

\textbf{Results.}
In Table~\ref{tab:results}, on the Objaverse benchmark~\cite{deitke2023objaverse}, \enel-7B achieves the GPT score of 50.92\% on the 3D object captioning task, setting a new state-of-the-art (SOTA) performance.
In traditional metrics, Sentence-BERT and SimCSE reaches 48.61\% and 49.31\%, respectively, comparable to the ShapeLLM-13B.  
For the 3D object classification task, \enel-7B outperformes prior encoder-based 3D LMMs, achieving a GPT score of 55\%. 
Given the same training dataset as PointLLM, these results validate the effectiveness of our proposed \textbf{LLM-embedded Semantic Encoding} and \textbf{Hierarchical Geometry Aggregation} strategies for the encoder-free architecture.  
Additionally, on the 3D-VQA task of the 3D MM-Vet dataset~\cite{qi2024shapellm}, despite the lack of spatial and embodied interaction-related data in the training set, \enel achieves the GPT score of 42.7\%, surpassing PointLLM-7B by 1.5\%.

\textbf{Visualization.}
In the Figure~\ref{vis}, we visualize the attention scores between the average text token and the point tokens in the last layer of both PointLLM and \enel. 
Three object categories, including the chair, the airplane, and the desk lamp, are selected from the Objaverse dataset~\cite{deitke2023objaverse}. 
In the Figure~\ref{vis}, red indicates higher values. 
We observe that in encoder-based 3D LMMs, the semantic relevance between the text tokens and the processed point tokens is relatively 
low. In contrast, \enel, with its encoder-free architecture, achieves a high correlation between the features of the two different modalities, with the average text token focusing on key geometric structures of the objects, such as the backrest of the chair, the wings of the airplane, and the lampshade of the desk lamp.

\section{Conclusion}
\label{conclusion}
In this study, we investigate the potential of the encoder-free architecture in 3D understanding. 
Through a systematic analysis, we demonstrate that transferring the functionality of the 3D encoder to the LLM itself can effectively compensate for the performance degradation caused by the removal of the 3D encoder. 
To achieve this, we introduce the LLM-embedded Semantic Encoding strategy and the Hierarchical Geometry Aggregation strategy in the pre-training and instruction tuning stages. 
These strategies enable the encoding of high-level point cloud semantics while capturing critical local information. 
Our experiments highlight the promising prospects of the encoder-free architecture.
% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\nocite{langley00}

\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Work}

\textbf{3D LMM.} 
Recent advancements in integrating large language models (LLMs) with 3D data have led to significant progress in both object-level and scene-level understanding.
At the object level, early approaches like~\cite{hong20243d} utilize 2D rendering to leverage 2D LLMs, but this sacrifices geometric details. More recent models, including Point-Bind LLM~\cite{guo2023point}, PointLLM~\cite{xu2023pointllm} and ShapeLLM~\cite{qi2024shapellm}, directly encode point clouds and align them with LLMs, by combining the 3D encoder with a powerful language model, effectively fusing geometric, appearance, and linguistic information.
At the scene level, models like Chat-3D~\cite{chat3d} and Scene-LLM~\cite{scenellm} focus on understanding complex spatial relationships through dialogue and tasks like captioning. 
Scene-LLM~\cite{scenellm} enhances embodied agents' abilities in interactive 3D indoor environments by integrating both scene-level and egocentric 3D information.
Grounded 3D-LLM~\cite{chen2024grounded} utilizes referent tokens to reference specific objects within 3D scenes, enabling tasks such as object detection and language grounding.

\textbf{Encoder-free Vision-Language Models.} 
Traditional vision-language models (VLMs) often rely on vision encoders to extract visual features before processing them with language models, integrating image encoders like CLIP~\cite{VLP:CLIP} and DINO V2~\cite{oquab2023dinov2}. 
However, recent efforts have explored encoder-free VLMs for their simplicity. 
Approaches like~\cite{team2024chameleon, xie2024show} use VQ tokenizers~\cite{vqgan} or linear projection layers~\cite{diao2024EVE, solo} to represent images.
Fuyu-8B~\cite{VLM:Fuyu-8b}, a pure decoder-only model, directly processes image patches through linear projections, handling high-resolution images but showing only average performance. 
The EVE~\cite{diao2024unveiling} eliminates the need for a separate vision encoder by bridging vision-language representation within a unified decoder and enhancing visual recognition capabilities through additional supervision.


\begin{figure}[!h]
%\vspace{-0.1cm}
\centering
\includegraphics[width=0.9\linewidth]{loss2.png}
\vspace{-0.2cm}
\caption{\textbf{Variants of Point Cloud Self-Supervised Learning Losses.} 
(a) The Variant of Masked Modeling Loss, (b) The Variant of Reconstruction Loss, (c) The Variant of Hybrid Semantic Loss.}
  \label{loss2}
\end{figure}


\section{Experimental Settings}
\textbf{Implementation Details.} We use the LLaMA model~\cite{touvron2023llama} as our LLM backbone, with the 7B Vicuna-v1.1~\cite{chiang2023vicuna} checkpoint as the default setting. In the token embedding layer, the point cloud is first processed by a linear layer to expand its dimension from 6 to 288. The input point cloud initially consists of 8192 points, followed by three iterations of farthest point sampling (FPS), reducing the size to 512, 256, and 128, respectively. After each FPS operation, k-Nearest Neighbors (k-NN) is applied with a cluster size of 81. And geometric features are extracted using triangular encoding, followed by linear layers that progressively increase the dimension to 576, 1152, and 2304. Finally, the projection layer maps the features to the LLM dimension of 4096. 
In the pre-training stage, we unfreeze the first four LLM layers. Within the LLM-embedded Semantic Encoding strategy, Hybrid Semantic Loss applies masked modeling to 30\% of the tokens and reconstructs the patches for the remaining 70\% visible tokens.
In the instruction tuning stage, we apply geometric aggregation in the second LLM layer, reducing the number of point tokens from 128 to 64. 
MaxMean pooling is used to retain more information. After passing through two LLM layers, the geometric aggregation is applied in the fifth layer to restore the point size count to 128.

\textbf{Training and Evaluation Details.} 
During the two-stage training, each stage utilizes the same dataset and preprocessing method as PointLLM. All training are conducted on 4 $\times$ 80G A100 GPUs in BF16 precision, utilizing FlashAttention, the AdamW optimizer, and a cosine learning rate schedule. During the pre-training stage, the model is trained for three epochs with a batch size of 128 and a learning rate of 4e-4. 
In the instruction tuning stage, it is conducted for three epochs with batch size of 32 and a learning rate of 2e-5.
The GPT-4 model~\cite{achiam2023gpt} used for classification and captioning tasks evaluation refers to ``gpt-4-0613" version consistent with PointLLM~\cite{xu2023pointllm}. In contrast, the GPT-4 model employed for QA performance evaluation corresponds to ``gpt-4-0125" version aligning with ShapeLLM~\cite{qi2024shapellm}. Additionally, the GPT evaluation prompts for classification and captioning are identical to those used in PointLLM, while the prompts for QA follow those in ShapeLLM.

% \samepage
\begin{table*}[tb]
    \centering
    \caption{\textbf{Ablation Experiments.}
    We begin the ablation experiments by changing the single configuration of the module from \enel.
    \( \Psi \) represents a mask ratio of 60\%, while \( \Phi \) represents a mask ratio of 30\%. For Hybrid Semantic Loss, the subscript $patch$ and $feat$ represent the masked modeling target, while the reconstruction target is the corresponding $feat$ and $patch$.
    $l$ represents the number of aggregation and propagation operations.
    $H$ refers to the LLM layers between \( l \) aggregation and \( l \) propagation operations.
    $O$ refers to the LLM layer between two individual aggregation or propagation operations.}
\vspace{0.15cm}
    \resizebox{0.9\textwidth}{!}{
    \renewcommand{\arraystretch}{1.2}
    \setlength{\tabcolsep}{6pt} % Adjust column spacing
    \begin{tabular}{l|cccccc|c}
        \toprule
        \multirow{2}{*}{\textbf{Model}} & \multicolumn{6}{c|}{\textbf{Cap}} & \multicolumn{1}{c|}{\textbf{Cls}} \\ \cmidrule{2-8} 
        & \textbf{GPT-4} & \textbf{Sentence-BERT} & \textbf{SimCSE} & \textbf{BLEU-1} & \textbf{ROUGE-L} & \textbf{METEOR} & \textbf{GPT-4} \\
        \midrule
        \textbf{\enel-7B} & 50.92 & 48.61 & 49.31 & 3.88 & 7.20 & 12.50 & 55.00  \\
        \midrule
        -- Hybrid Semantic Loss & 47.19 & 48.07 & 48.31 & 3.46 & 7.41 & 11.84 & 50.61  \\  
        \midrule
        Hybrid Semantic Loss$_{patch}$$^{\Phi}$ & 49.05 & 48.82 & 49.20 & 4.01 & 7.25 & 12.38 & 52.20 \\ 
        {Hybrid Semantic Loss$_{patch}$}$^{\Psi}$ & 48.96 & 48.38 & 49.00 & 3.66 & 6.97 & 11.98 & 52.00 \\  
        Hybrid Semantic Loss$_{feat}$$^{\Psi}$ & 49.63 & 48.00 & 48.62 & 3.78 & 6.88 & 12.33 & 51.50 \\ 
        \midrule
        -- gate mechanism & 49.26 & 48.41 & 48.93 & 3.71 & 7.12 & 12.47 & 53.50 \\ 
        \midrule
        l=2,H=2,O=0 & 48.81 & 48.10 & 48.57 & 3.70 & 6.99 & 12.01 & 51.50 \\ 
        l=2,H=4,O=0 & 49.02 & 48.47 & 48.61 & 3.65 & 7.10 & 12.31 & 52.00 \\ 
        l=2,H=2,O=2 & 48.96 & 47.96 & 48.89 & 3.80 & 7.05 & 12.55 & 52.00 \\ 
        l=2,H=4,O=2 & 49.58 & 48.70 & 48.84 & 3.84 & 7.56 & 12.76 & 53.00 \\ 
        \bottomrule
    \end{tabular}
                                                                                           }
    \label{tab:results_ablat1}
\end{table*}

\section{More Experiments}

\subsection{Variants of Point Cloud Self-Supervised Learning Losses.}
\label{exp}
In the Figure~\ref{loss2}, we exhibit the other variants of Masked Modeling Loss, Reconstruction Loss and Hybrid Semantic Loss.

As seen in Figure~\ref{loss2} (a), in the Masked Modeling Loss, after the learnable tokens  are processed by the LLM, the tokens are transformed to the point patches \( \{ G_{\text{pre}_i} \}_{i=1}^{M*r} \in \mathbb{R}^{M*r \times k \times 3} \) through a linear layer. 
We utilize the $l_2$ chamfer distance to align the predicted \(  G_{\text{pre}} \) with the point patches \( G_{mask} \) corresponding to the masked tokens, reconstructing the spatial information.
The optimization can be written as
\begin{equation}
\frac{1}{M*r} \sum_{i=1}^{M*r} \left( \min_{j} \| a_i - b_j \|_2^2 + \min_{j} \| b_i - a_j \|_2^2 \right),
\end{equation}
where $a = G_{\text{pre}}$ and $b = G_{mask}$.

As shown in Figure~\ref{loss2} (b), after the point feature tokens \( \{ F_i \}_{i=1}^{M} \) are encoded by the LLM, the Mean Squared Error (MSE) is computed between the predicted \(  F_{\text{pre}} \) and the ground truth \(  F \).
The optimization can be written as
\begin{equation}
\mathcal{L}_{\text{mask}} = \frac{1}{M} \sum_{i=1}^{M} \left( \| F_{\text{pre}_i} - F_{\text{ }_i} \|_2^2 \right).
\end{equation}

Finally, in the Figure~\ref{loss2} (c) Hybrid Semantic Loss, the masked tokens and the corresponding patches are referred to as \( \{ F_{\text{mask}_i} \}_{i=1}^{M*r}\) and \( \{ G_{\text{mask}_i} \}_{i=1}^{M*r}\), respectively. 
The remaining tokens are denoted as\( \{ F_{\text{vis}_i} \}_{i=1}^{M*(1-r)}\) and \( \{ G_{\text{vis}_i} \}_{i=1}^{M*(1-r)}\). 
After passing point tokens through the LLM, we compute the MSE between \( F_{\text{pre}} \) and \( F_{\text{vis}} \). 
The learnable tokens \( F_{\text{learn}} \) are transformed into \( G_{\text{pred}} \), and the \( L_2 \) Chamfer distance is computed between \( G_{\text{pred}} \) and \( G_{\text{mask}} \). 
These two are added to the original cross-entropy loss with coefficients all equal to 1.

\subsection{More Ablation Experiments}

We begin the ablation experiments starting from the \enel, which is the reverse order compared to the experiments in the main text, as showcased in Tabel~\ref{tab:results_ablat1}

\textbf{The Effects of LLM-embedded Semantic Encoding Strategy.}
In the Table~\ref{tab:results_ablat1}, on the basis of \enel, removing the Hybrid Semantic Loss during the pre-training stage significantly degrades performance. The GPT-4 score for the captioning task drops from 50.92\% to 47.19\%, and the GPT-4 score for the classification task decreases to 50.61\%. This is because the proposed self-supervised learning function for point clouds effectively captures the detailed structures and high-level semantics of the point clouds.

Based on \enel, we find that setting the mask ratio in the Hybrid Semantic Loss to 30\% consistently yields better results than 60\%. 
Additionally, the configuration where the masked token part predicts features while the visible token part reconstructs patches outperforms the reverse settingâ€”where the masked token part predicts patches and the visible token part reconstructs features. 
This phenomenon can be explained as follows: a mask ratio of 30\% retains critical information while facilitating the model to effectively utilize the visible tokens to derive the masked parts.
When the mask ratio is set too high, the model fails to learn the global context knowledge adequately. 
Moreover, when the masked token part is tasked with predicting features, the model focuses on learning the high-level context semantics, while the patch reconstruction aids in accurately capturing low-level details. In contrast, when the masked token part predicts patches, the model becomes excessively dependent on local features during the process of semantic reconstruction.

\textbf{The Effects of Hierarchical Geometry Aggregation Strategy.}
After removing the gating mechanism in the self-attention of the aggregation operation, the performance drops to 49.26\% and 53.50\% on the captioning and classification tasks, respectively. The gating mechanism helps the model to adaptively filter information, allowing it to focus on more discriminative features. Without the dynamic adjustment to focus on different parts of the input, the generated text from the LLM lacks accuracy and coherence, leading to a decrease in performance.

The performance generally degrades with an increasing number of aggregation and propagation operations. This degradation can be attributed to the progressive loss of local geometric details through repeated aggregation operations, while multiple propagation operations typically rely on interpolation which tends to amplify high-frequency noise.
We observe that increasing the number of LLM layers between the final aggregation operation and the first propagation operation leads to improved performance. This suggests that cascaded aggregation operations necessitate deeper architectural capacity for high-level feature abstraction, as insufficient network depth may lead to degradation of hierarchical representations.
Furthermore, the presence of LLM layers between each aggregation or propagation operation enhances performance by allowing the model to process and transform compressed information. Through self-attention mechanisms, these intermediate layers can recapture and restore details lost during the aggregation process.

\section{Model Output}
In Figure~\ref{output}, we showcase more model output, where our \enel provides precise and diverse responses with multi-modal 3D instruction input.
\samepage
\begin{figure}[h]
%\vspace{-0.1cm}
    \includegraphics[width=\linewidth]{output3.png}
  \caption{\textbf{\enel Output Examples.} 
We demonstrate that \enel provides precise and diverse responses when addressing different problems.}
  \label{output}
\end{figure}


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
