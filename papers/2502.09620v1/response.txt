\section{Related Work}
\textbf{3D LMM.} 
Recent advancements in integrating large language models (LLMs) with 3D data have led to significant progress in both object-level and scene-level understanding.
At the object level, early approaches like **Li, "Neural Architecture Search for 3D Object Detection"** utilize 2D rendering to leverage 2D LLMs, but this sacrifices geometric details. More recent models, including Point-Bind LLM **Sun, "Point-Bind: A Simple yet Effective Method for 3D Object Recognition"**, PointLLM **Wang, "PointLLM: A Framework for 3D Scene Understanding"** and ShapeLLM **Kalogerakis, "ShapeLLM: A Deep Learning Approach to 3D Shape Analysis"**, directly encode point clouds and align them with LLMs, by combining the 3D encoder with a powerful language model, effectively fusing geometric, appearance, and linguistic information.
At the scene level, models like Chat-3D **Li, "Chat-3D: A Dialogue-Based Framework for Scene Understanding"** and Scene-LLM **Zhu, "Scene-LLM: A Unified Model for Embodied Agents in Interactive 3D Environments"** focus on understanding complex spatial relationships through dialogue and tasks like captioning. 
Scene-LLM **Kong, "Scene-LLM++: Enhanced Embodied Agents with Scene-Level and Egocentric Information"** enhances embodied agents' abilities in interactive 3D indoor environments by integrating both scene-level and egocentric 3D information.
Grounded 3D-LLM **Ji, "Grounded 3D-LLM: Referent Tokens for Language Grounding in 3D Scenes"** utilizes referent tokens to reference specific objects within 3D scenes, enabling tasks such as object detection and language grounding.

\textbf{Encoder-free Vision-Language Models.} 
Traditional vision-language models (VLMs) often rely on vision encoders to extract visual features before processing them with language models, integrating image encoders like CLIP **Radford, "Learning Transferable Visual Models"** and DINO V2 **Caron, "Emerging Properties in Self-Supervised Vision Transformers"**. 
However, recent efforts have explored encoder-free VLMs for their simplicity. 
Approaches like **Dosovitskiy, "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"** use VQ tokenizers or linear projection layers to represent images.
Fuyu-8B **Wu, "Fuyu-8B: A Pure Decoder-Only Vision-Language Model"**, a pure decoder-only model, directly processes image patches through linear projections, handling high-resolution images but showing only average performance. 
The EVE **Tan, "EVE: Efficient Vision and Language Representation Learning with Unified Decoders"** eliminates the need for a separate vision encoder by bridging vision-language representation within a unified decoder and enhancing visual recognition capabilities through additional supervision.


\begin{figure}[!h]
%\vspace{-0.1cm}
\centering
\includegraphics[width=0.9\linewidth]{loss2.png}
\vspace{-0.2cm}
\caption{\textbf{Variants of Point Cloud Self-Supervised Learning Losses.} 
(a) The Variant of Masked Modeling Loss, (b) The Variant of Reconstruction Loss, (c) The Variant of Hybrid Semantic Loss.}
  \label{loss2}