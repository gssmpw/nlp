@misc{VLM:Fuyu-8b,
	author = {Bavishi, Rohan and Elsen, Erich and Hawthorne, Curtis and Nye, Maxwell and Odena, Augustus and Somani, Arushi and Ta\c{s}\i{}rlar, Sa\u{g}nak},
	title = {Introducing our Multimodal Models},
	url = {https://www.adept.ai/blog/fuyu-8b},
	year = {2023},
	bdsk-url-1 = {https://www.adept.ai/blog/fuyu-8b}}

@inproceedings{VLP:CLIP,
	author = {Alec Radford and Jong Wook Kim and Chris Hallacy and Aditya Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},
	booktitle = {ICML},
	pages = {8748--8763},
	title = {Learning Transferable Visual Models From Natural Language Supervision},
	volume = {139},
	year = {2021}}

@article{chat3d,
	author = {Wang, Zehan and Huang, Haifeng and Zhao, Yang and Zhang, Ziang and Zhao, Zhou},
	journal = {arXiv preprint arXiv:2308.08769},
	title = {Chat-3D: Data-efficiently Tuning Large Language Model for Universal Dialogue of 3D Scenes},
	year = {2023}}

@article{chen2024grounded,
  title={Grounded 3D-LLM with Referent Tokens},
  author={Chen, Yilun and Yang, Shuai and Huang, Haifeng and Wang, Tai and Lyu, Ruiyuan and Xu, Runsen and Lin, Dahua and Pang, Jiangmiao},
  journal={arXiv preprint arXiv:2405.10370},
  year={2024}
}

@article{diao2024EVE,
	author = {Diao, Haiwen and Cui, Yufeng and Li, Xiaotong and Wang, Yueze and Lu, Huchuan and Wang, Xinlong},
	journal = {arXiv preprint arXiv:2406.11832},
	title = {Unveiling Encoder-Free Vision-Language Models},
	year = {2024}}

@article{diao2024unveiling,
  title={Unveiling Encoder-Free Vision-Language Models},
  author={Diao, Haiwen and Cui, Yufeng and Li, Xiaotong and Wang, Yueze and Lu, Huchuan and Wang, Xinlong},
  journal={arXiv preprint arXiv:2406.11832},
  year={2024}
}

@article{guo2023point,
  title={Point-bind \& point-llm: Aligning point cloud with multi-modality for 3d understanding, generation, and instruction following},
  author={Guo, Ziyu and Zhang, Renrui and Zhu, Xiangyang and Tang, Yiwen and Ma, Xianzheng and Han, Jiaming and Chen, Kexin and Gao, Peng and Li, Xianzhi and Li, Hongsheng and others},
  journal={arXiv preprint arXiv:2309.00615},
  year={2023}
}

@article{hong20243d,
	author = {Hong, Yining and Zhen, Haoyu and Chen, Peihao and Zheng, Shuhong and Du, Yilun and Chen, Zhenfang and Gan, Chuang},
	journal = {Advances in Neural Information Processing Systems},
	title = {3d-llm: Injecting the 3d world into large language models},
	volume = {36},
	year = {2024}}

@article{oquab2023dinov2,
  title={Dinov2: Learning robust visual features without supervision},
  author={Oquab, Maxime and Darcet, Timoth{\'e}e and Moutakanni, Th{\'e}o and Vo, Huy and Szafraniec, Marc and Khalidov, Vasil and Fernandez, Pierre and Haziza, Daniel and Massa, Francisco and El-Nouby, Alaaeldin and others},
  journal={arXiv preprint arXiv:2304.07193},
  year={2023}
}

@article{qi2024shapellm,
	author = {Qi, Zekun and Dong, Runpei and Zhang, Shaochen and Geng, Haoran and Han, Chunrui and Ge, Zheng and Yi, Li and Ma, Kaisheng},
	journal = {arXiv preprint arXiv:2402.17766},
	title = {ShapeLLM: Universal 3D Object Understanding for Embodied Interaction},
	year = {2024}}

@article{scenellm,
	author = {Fu, Rao and Liu, Jingyu and Chen, Xilun and Nie, Yixin and Xiong, Wenhan},
	journal = {arXiv preprint arXiv:2403.11401},
	title = {Scene-llm: Extending language model for 3d visual understanding and reasoning},
	year = {2024}}

@article{solo,
	author = {Chen, Yangyi and Wang, Xingyao and Peng, Hao and Ji, Heng},
	journal = {arXiv preprint arXiv:2407.06438},
	title = {A Single Transformer for Scalable Vision-Language Modeling},
	year = {2024}}

@article{team2024chameleon,
	author = {ChameleonTeam},
	journal = {arXiv preprint arXiv:2405.09818},
	title = {Chameleon: Mixed-modal early-fusion foundation models},
	year = {2024}}

@inproceedings{vqgan,
	author = {Esser, Patrick and Rombach, Robin and Ommer, Bjorn},
	booktitle = {Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
	pages = {12873--12883},
	title = {Taming transformers for high-resolution image synthesis},
	year = {2021}}

@article{xie2024show,
	author = {Xie, Jinheng and Mao, Weijia and Bai, Zechen and Zhang, David Junhao and Wang, Weihao and Lin, Kevin Qinghong and Gu, Yuchao and Chen, Zhijie and Yang, Zhenheng and Shou, Mike Zheng},
	journal = {arXiv preprint arXiv:2408.12528},
	title = {Show-o: One single transformer to unify multimodal understanding and generation},
	year = {2024}}

@article{xu2023pointllm,
	author = {Xu, Runsen and Wang, Xiaolong and Wang, Tai and Chen, Yilun and Pang, Jiangmiao and Lin, Dahua},
	journal = {arXiv preprint arXiv:2308.16911},
	title = {Pointllm: Empowering large language models to understand point clouds},
	year = {2023}}

