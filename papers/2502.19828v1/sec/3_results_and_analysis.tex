% \section{Results and Analysis}
% \label{sec:results}

% \subsection{Text Encoder}
% In this study, we examined the performance of a single-layer classifier in various scenarios involving two, three, four, and five objects. The results of these experiments, summarized in Table 1, demonstrate that across all models examined, the classifier's accuracy in categorizing the first object is significantly higher than for other objects.
% This finding indicates that in the textual representation generated by the CLIP text encoder, the presence of the first object is more pronounced than other objects, and the influence of subsequent objects on the overall text representation gradually diminishes. More detailed information about this experiment is provided in the appendix of the paper.
% In terms of representation, as predicted by the results of the previous experiment, the first object in most cases shows the highest similarity and proximity to the entire phrase in the representation space. Table 2 presents the results of the representation experiment in the text space. Additional results for this section can be found in the appendix.
% Both of these experiments serve as compelling evidence for the fact that the CLIP text encoder exhibits a bias towards the first object. This finding implies that the final text representation is formed from a weighted sum with significantly different weights for each object.

% \begin{figure} [h!]
%     \centering
%     \includegraphics[width=\columnwidth]{sec/images/simple_classifier.pdf}
%     \caption{Performance Comparison of Different Models on Four Objects}
%     \label{fig:performance_comparison}
% \end{figure}

% \subsection{Image Encoder}
% % \begin{table}[h!]
% %     \centering
% %     \begin{tabular}{ll}
% %         \toprule
% %         \textbf{Model} & \textbf{Values} \\
% %         \midrule
% %         CLIP openAI & \begin{tabular}[c]{@{}l@{}}\textbf{First Object: 0.87}\\ Second Object: 0.31\\ Third Object: 0.32\\ Fourth Object: 0.74\end{tabular} \\
% %         \midrule
% %         CLIP LAION & \begin{tabular}[c]{@{}l@{}}\textbf{First Object: 0.99}\\ Second Object: 0.32\\ Third Object: 0.30\\ Fourth Object: 0.45\end{tabular} \\
% %         \midrule
% %         CLIP Datacomp & \begin{tabular}[c]{@{}l@{}}\textbf{First Object: 0.99}\\ Second Object: 0.23\\ Third Object: 0.33\\ Fourth Object: 0.58\end{tabular} \\
% %         \midrule
% %         SIGLIP & \begin{tabular}[c]{@{}l@{}}\textbf{First Object: 0.97}\\ Second Object: 0.73\\ Third Object: 0.33\\ Fourth Object: 0.06\end{tabular} \\
% %         \midrule
% %         NegCLIP & \begin{tabular}[c]{@{}l@{}}\textbf{First Object: 0.99}\\ Second Object: 0.28\\ Third Object: 0.31\\ Fourth Object: 0.44\end{tabular} \\
% %         \midrule
% %         BERT & \begin{tabular}[c]{@{}l@{}}First Object: 0.64\\ Second Object: 0.29\\ Third Object: 0.34\\ \textbf{Fourth Object: 0.98}\end{tabular} \\
% %         \midrule
% %         SBERT & \begin{tabular}[c]{@{}l@{}}\textbf{First Object: 0.94}\\ Second Object: 0.11\\ Third Object: 0.05\\ Fourth Object: 0.18\end{tabular} \\
% %         \bottomrule
% %     \end{tabular}
% %     \caption{Model Performance on Different Objects}
% %     \label{tab:model_performance}
% % \end{table}
% In the visual component of our research, we replicated the single-layer classifier experiments and image representation analysis for scenarios involving two, three, four, and five objects. The aim of these experiments was to investigate whether the location of an object within an image or its size influences the extent of its presence in the final image representation.

% As shown in Table 3, the results of the single-layer classifier experiment indicate that an object positioned at the center of the image has a more prominent presence in the final image embedding compared to other objects. Furthermore, the results presented in Table 4 suggest that larger objects also have a greater presence in the final embedding. More detailed experiments in this area are available in the appendix.

% In the image representation experiments, Tables 5 and 6 provide similar observations to the previous experiment. These results confirm that the presence of an object in the center of the image or its larger size relative to other objects leads to a more significant presence of that object in the final image representation. Additional experiments in this domain are also available in the appendix.

% These findings demonstrate that the CLIP model exhibits greater sensitivity to the spatial location and size of objects when processing images. This sensitivity can have a significant impact on the overall system performance in image processing tasks.



\section{Results and Analysis}
\label{sec:results}
Our experiments revealed significant biases in both the text and image encoders of the CLIP model. This section presents our findings, organized by the encoder type and experiment.

\subsection{Text Encoder Analysis}



\subsubsection{Text-based Object Classification (TOC)}



The performance of the text encoder in the TOC experiment demonstrated a significant bias towards the first object mentioned in the text descriptions. As shown in Table \ref{tab:tor_results}, the classification accuracy for the first object was considerably higher than for subsequent objects. This suggests that the first object mentioned in a textual description is represented more prominently in the final textual representation.
%%%%% Version 5 object
% \begin{table}[ht]
% \centering
% \scriptsize
% \setlength{\tabcolsep}{2pt}
% \caption{Performance of Various Models on SimCO and ComCO Datasets}

% \label{tab:toc_results}
% \begin{tabular}{llccccc}
% \toprule
% Dataset & Model & First Obj & Second Obj & Third Obj & Fourth Obj & Fifth Obj\\ 
% \midrule
% \multirow{5}{*}{SimCO} 
%  & CLIP openAI   & \textbf{89.79} & 26.33 & 20.74 & 24.69 & 50.29 \\
%  & CLIP LAION    & \textbf{98.43} & 25.51 & 19.81 & 23.15 & 41.07 \\
%  & CLIP Datacomp & \textbf{98.89} & 16.51 & 21.29 & 26.92 & 48.52 \\
%  & SIGLIP        & \textbf{97.79} & 71.67 & 27.41 & 6.29 & 6.48 \\
%  & NegCLIP       & \textbf{96.83} & 15.50 & 17.54 & 22.58 & 33.62 \\
% \midrule
% \multirow{5}{*}{ComCO}   
%  & CLIP openAI   & \textbf{86.13} & 22.11 & 19.43 & 28.03 & 68.37 \\
%  & CLIP LAION    & \textbf{98.76} & 20.91 & 18.11 & 20.77 & 33.54 \\
%  & CLIP Datacomp & \textbf{99.13} & 14.75 & 19.89 & 25.72 & 47.11 \\
%  & SIGLIP        & \textbf{97.63} & 70.57 & 32.34 & 5.42 & 5.72 \\
%  & NegCLIP       & \textbf{99.03} & 16.69 & 16.51 & 22.25 & 34.29 \\
% \bottomrule
% \end{tabular}
% \end{table}
% \begin{table}[ht]
% \centering
% \scriptsize
% \setlength{\tabcolsep}{3pt}
% \renewcommand{\arraystretch}{1.2}
% \caption{Performance of various CLIP models on TOC for SimCO and ComCO datasets}
% \label{tab:toc_results}
% \begin{tabular}{llcccc}
% \toprule
% \rowcolor[HTML]{EFEFEF}
% Dataset & Model & \textbf{First Obj} & \textbf{Second Obj} & \textbf{Third Obj} & \textbf{Fourth Obj} \\ 
% \midrule
% \multirow{5}{*}{SimCO} 
%  & \textit{CLIP openAI} & \textbf{89.79} & 26.33 & 20.74 & 24.69 \\
%  & \textit{CLIP LAION} & \textbf{98.43} & 25.51 & 19.81 & 23.15 \\
%  & \textit{CLIP Datacomp} & \textbf{98.89} & 16.51 & 21.29 & 26.92 \\
%  & \textit{SIGLIP} & \textbf{97.79} & 71.67 & 27.41 & 6.29 \\
%  & \textit{NegCLIP} & \textbf{96.83} & 15.50 & 17.54 & 22.58 \\
% \midrule
% \multirow{5}{*}{ComCO}   
%  & \textit{CLIP openAI} & \textbf{86.13} & 22.11 & 19.43 & 28.03 \\
%  & \textit{CLIP LAION} & \textbf{98.76} & 20.91 & 18.11 & 20.77 \\
%  & \textit{CLIP Datacomp} & \textbf{99.13} & 14.75 & 19.89 & 25.72 \\
%  & \textit{SIGLIP} & \textbf{97.63} & 70.57 & 32.34 & 5.42 \\
%  & \textit{NegCLIP} & \textbf{99.03} & 16.69 & 16.51 & 22.25 \\
% \bottomrule
% \end{tabular}
% \end{table}


%  \vspace{-0.3in}

\subsubsection{Text-based Object Retrieval (TOR)}

The TOR experiment further reinforced the presence of bias in the text encoder. Table \ref{tab:tor_results} presents the retrieval accuracy, where the first object mentioned in the descriptions consistently showed the highest retrieval accuracy. This indicates that the initial object exerts a dominant influence on the overall text representation, making it more likely to be retrieved accurately compared to subsequent objects.

%%%%% Version 5 object
% \begin{table}[ht]
% \centering
% \scriptsize
% \caption{Performance of Various Models on SimCO and ComCO Datasets}

% \label{tab:tor_results}
% \setlength{\tabcolsep}{2pt}
% \begin{tabular}{llccccc}
% \toprule
% Dataset & Model & First Obj & Second Obj & Third Obj & Fourth Obj & Fifth Obj\\ 
% \midrule
% \multirow{5}{*}{SimCO} 
%  & CLIP openAI   & \textbf{71.71} & 10.59 & 2.99 & 2.71 & 12 \\
%  & CLIP LAION    & \textbf{74.07} & 9.51 & 4.48 & 2.8 & 9.14 \\
%  & CLIP Datacomp & \textbf{66.43} & 16.12 & 6.59 & 4.99 & 5.87 \\
%  & SIGLIP        & \textbf{49.47} & 13.32 & 3.39 & 11.97 & 21.25 \\
%  & NegCLIP       & \textbf{85} & 10.39 & 3.12 & 1.24 & 0.2 \\
% \midrule
% \multirow{5}{*}{ComCO}   
%  & CLIP openAI   & \textbf{56.81} & 18.69 & 12.20 & 7.22 & 5.09 \\
%  & CLIP LAION    & \textbf{57.66} & 20.05 & 11.73 & 4.56 & 3.13 \\
%  & CLIP Datacomp & \textbf{71.04} & 13.18 & 8.09 & 5.73 & 4.84 \\
%  & SIGLIP        & \textbf{40.77} & 22.48 & 15.81 & 11.51 & 9.43 \\
%  & NegCLIP       & \textbf{49.82} & 25.03 & 15.36 & 7.02 & 2.77 \\
% \bottomrule
% \end{tabular}
% \end{table}
% \begin{table}[ht]
% \centering
% \scriptsize
% \caption{Performance of Various Models on SimCO and ComCO Datasets}

% \label{tab:tor_results}
% \setlength{\tabcolsep}{2pt}
% \begin{tabular}{llcccc}
% \toprule
% Dataset & Model & First Obj & Second Obj & Third Obj & Fourth Obj \\ 
% \midrule
% \multirow{5}{*}{SimCO} 
%  & CLIP openAI   & \textbf{71.71} & 10.59 & 2.99 & 2.71 \\
%  & CLIP LAION    & \textbf{74.07} & 9.51 & 4.48 & 2.8 \\
%  & CLIP Datacomp & \textbf{66.43} & 16.12 & 6.59 & 4.99 \\
%  & SIGLIP        & \textbf{49.47} & 13.32 & 3.39 & 11.97 \\
%  & NegCLIP       & \textbf{85} & 10.39 & 3.12 & 1.24 \\
% \midrule
% \multirow{5}{*}{ComCO}   
%  & CLIP openAI   & \textbf{56.81} & 18.69 & 12.20 & 7.22 \\
%  & CLIP LAION    & \textbf{57.66} & 20.05 & 11.73 & 4.56 \\
%  & CLIP Datacomp & \textbf{71.04} & 13.18 & 8.09 & 5.73 \\
%  & SIGLIP        & \textbf{40.77} & 22.48 & 15.81 & 11.51 \\
%  & NegCLIP       & \textbf{49.82} & 25.03 & 15.36 & 7.02 \\
% \bottomrule
% \end{tabular}
% \end{table}

\begin{table}[ht]
\centering
\scriptsize
\setlength{\tabcolsep}{3pt}
\renewcommand{\arraystretch}{1.2}
\caption{Performance of various CLIP models on TOC and TOR for ComCO datasets}
\label{tab:tor_results}
\begin{tabular}{llcccc}
\toprule
\rowcolor[HTML]{EFEFEF}
Task & Model & \textbf{First Obj} & \textbf{Second Obj} & \textbf{Third Obj} & \textbf{Fourth Obj} \\ 
\midrule
\multirow{5}{*}{TOC} 
 & \textit{CLIP openAI} & \textbf{87.17} & 30.60 & 31.69 & 74.49 \\
 & \textit{CLIP LAION}\cite{schuhmann2022laion} & \textbf{98.89} & 31.64 &20.90 & 47.76 \\
 & \textit{CLIP Datacomp}\cite{gadre2024datacomp} & \textbf{99.46} & 22.82 & 32.93 & 58.18 \\
 & \textit{SIGLIP}\cite{zhai2023sigmoid} & \textbf{97.27} & 72.51 & 33.25 & 5.79 \\
 & \textit{NegCLIP}\cite{yuksekgonul2023and} & \textbf{98.73} & 28.05 & 30.83 & 43.82 \\
\midrule
\multirow{5}{*}{TOR}   
 & \textit{CLIP openAI} & \textbf{48.20} & 26.01 & 10.74 & 8.74 \\
 & \textit{CLIP LAION} & \textbf{63.96} & 21.59 & 10.68 & 3.76 \\
 & \textit{CLIP Datacomp} & \textbf{71.13} & 16.26 & 8.74 & 3.87 \\
 & \textit{SIGLIP} & \textbf{58.11} & 21.16 & 10.99 & 9.73 \\
 & \textit{NegCLIP} & \textbf{51.63} & 28.92 & 14.86 & 4.59 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Image Encoder Analysis}



\subsubsection{Image-based Object Classification (IOC)}
IOC experiment revealed that larger objects in an image significantly influence the final visual representation more than smaller objects. This size-related bias is evident across various models and datasets. As detailed in Table \ref{tab:ior_size}, the classification accuracy for larger objects was consistently higher, indicating that the image encoder prioritizes these objects in its representations. 
% Table \ref{tab:ioc_results} shows the results of the IOC experiment.

%%%%% Version 5 object
% \begin{table}[ht]
% \centering
% \caption{Performance of Various Models on SimCO and ComCO Datasets}

% \label{tab:ioc_results}
% \scriptsize
% \setlength{\tabcolsep}{2pt}
% \begin{tabular}{llccccc}
% \toprule
% Dataset & Model & First Obj & Second Obj & Third Obj & Fourth Obj & Fifth Obj\\ 
% \midrule
% \multirow{5}{*}{SimCO} 
%  & CLIP openAI   & 45.19 & \textbf{100.0} & 38.38  & 39.0  & - \\
%  & CLIP LAION    & 50.5  & \textbf{100.0 }& 41.81  & 43.94 & - \\
%  & CLIP Datacomp & 48.94 &\textbf{ 100.0} & 38.38 & 45.06  & - \\
%  & SIGLIP        & 47.0 & \textbf{100.0}  & 38.5 & 41.06  & - \\
%  & NegCLIP       & 42.0  & \textbf{100.0}  & 37.25  & 46.94 & - \\
% \midrule
% \multirow{5}{*}{ComCO}   
%  & CLIP openAI   & 19.32 & \textbf{99.94} & 21.89  & 22.39  & - \\
%  & CLIP LAION    & 19.76   & \textbf{100.0 }& 17.57   & 18.89  & - \\
%  & CLIP Datacomp & 20.64 &\textbf{ 100.0} & 21.01  & 19.01  & - \\
%  & SIGLIP        & 18.95 & \textbf{100.0}  & 15.57 & 17.57  & - \\
%  & NegCLIP       & 21.89  & \textbf{100.0}  & 23.64  & 31.33 & - \\
% \bottomrule
% \end{tabular}
% \end{table}

% \begin{table}[ht]
% \centering
% \caption{Performance of various CLIP models on IOC for SimCO and ComCO datasets. Second Object is largest.}
% \label{tab:ioc_results}
% \scriptsize
% \setlength{\tabcolsep}{3pt}
% \renewcommand{\arraystretch}{1.2}
% \begin{tabular}{llcccc}
% \toprule
% \rowcolor[HTML]{E4E8F2}
% Dataset & Model & \textbf{First Obj} & \textbf{Second Obj} & \textbf{Third Obj} & \textbf{Fourth Obj} \\ 
% \midrule
% \multirow{5}{*}{SimCO} 
%  & \textit{CLIP openAI} & 45.19 & \textbf{100.0} & 38.38 & 39.0 \\
%  & \textit{CLIP LAION} & 50.5 & \textbf{100.0} & 41.81 & 43.94 \\
%  & \textit{CLIP Datacomp} & 48.94 & \textbf{100.0} & 38.38 & 45.06 \\
%  & \textit{SIGLIP} & 47.0 & \textbf{100.0} & 38.5 & 41.06 \\
%  & \textit{NegCLIP} & 42.0 & \textbf{100.0} & 37.25 & 46.94 \\
% \midrule
% \multirow{5}{*}{ComCO}   
%  & \textit{CLIP openAI} & 19.32 & \textbf{99.94} & 21.89 & 22.39 \\
%  & \textit{CLIP LAION} & 19.76 & \textbf{100.0} & 17.57 & 18.89 \\
%  & \textit{CLIP Datacomp} & 20.64 & \textbf{100.0} & 21.01 & 19.01 \\
%  & \textit{SIGLIP} & 18.95 & \textbf{100.0} & 15.57 & 17.57 \\
%  & \textit{NegCLIP} & 21.89 & \textbf{100.0} & 23.64 & 31.33 \\
% \bottomrule
% \end{tabular}
% \end{table}
% These results indicate that larger objects have a more prominent presence in the final image embedding compared to smaller objects.



\subsubsection{Image-based Object Retrieval (IOR)}
The IOR experiment confirmed the significant influence of larger objects on the image encoder's performance. Table \ref{tab:ior_size} shows that larger objects in multi-object images were more frequently and accurately identified in single-object image searches. This suggests a strong size-related bias in the image encoder, where larger objects are given more weight in the final image representation.  

%%%%% Version 5 object
% \begin{table}[ht]
% \centering
% \scriptsize
% \caption{Performance of Various Models on SimCO and ComCO Datasets}

% \label{tab:ior_size}
% \setlength{\tabcolsep}{2pt}
% \begin{tabular}{llccccc}
% \toprule
% Dataset & Model & First Obj & Second Obj & Third Obj & Fourth Obj & Fifth Obj\\ 
% \midrule
% \multirow{5}{*}{SimCO} 
%  & CLIP openAI   & 13.01 & \textbf{70.55} & 7.53  & 8.90  & - \\
%  & CLIP LAION    & 4.67   & \textbf{86.92 }& 3.74   & 4.67  & - \\
%  & CLIP Datacomp & 3.43 &\textbf{ 89.71} & 3.61  & 3.25  & - \\
%  & SIGLIP        & 1.28 & \textbf{91.03}  & 2.99 & 4.7  & - \\
%  & NegCLIP       & 10.4  & \textbf{74.40}  & 7.2  & 8.00 & - \\
% \midrule
% \multirow{5}{*}{ComCO}   
%  & CLIP openAI   & 14.29 & \textbf{67.86} & 7.14  & 10.71  & - \\
%  & CLIP LAION    & 5.48   & \textbf{91.78 }& 1.74   & 1.0  & - \\
%  & CLIP Datacomp & 3.91 &\textbf{ 93.30} & 1.12  & 1.68  & - \\
%  & SIGLIP        & 2.24 & \textbf{94.03}  & 1.49 & 2.24  & - \\
%  & NegCLIP       & 0.0  & \textbf{79.55}  & 2.27  & 18.18 & - \\
% \bottomrule
% \end{tabular}
% \end{table}

\begin{table}[ht]
\centering
\scriptsize
\setlength{\tabcolsep}{3pt}
\renewcommand{\arraystretch}{1.2}
\caption{Performance of various CLIP models on IOC and IOR for ComCO dataset. The second object is the largest.}
\label{tab:ior_size}
\begin{tabular}{llcccc}
\toprule
\rowcolor[HTML]{E4E8F2}
Task & Model & \textbf{Large Object} & \textbf{Small Obj 1} & \textbf{Small Obj 2} & \textbf{Small Obj 3} \\ 
\midrule
\multirow{5}{*}{IOC} 
 & \textit{CLIP openAI} & \textbf{99.94} & 19.32 & 21.89 & 22.39 \\
 & \textit{CLIP LAION} & \textbf{100.0} & 19.76 & 17.57 & 18.89 \\
 & \textit{CLIP Datacomp} & \textbf{100.0} & 20.64 & 21.01 & 19.01 \\
 & \textit{SIGLIP} & \textbf{100.0} & 18.95 & 15.57 & 17.57 \\
 & \textit{NegCLIP} & \textbf{100.0} & 21.89 & 23.64 & 31.33 \\
\midrule
\multirow{5}{*}{IOR}   
 & \textit{CLIP openAI} & \textbf{67.86} & 14.29 & 7.14 & 10.71 \\
 & \textit{CLIP LAION} & \textbf{91.78} & 5.48 & 2.74 & 0.00 \\
 & \textit{CLIP Datacomp} & \textbf{93.30} & 3.91 & 1.12 & 1.68 \\
 & \textit{SIGLIP} & \textbf{94.03} & 2.24 & 1.49 & 2.24 \\
 & \textit{NegCLIP} & \textbf{79.55} & 0.00 & 2.27 & 18.19 \\
\bottomrule
\end{tabular}
\end{table}



Experiments on SimCO and additional experiments can be found in \ref{app:toc},\ref{app:tor},\ref{app:ioc},\ref{app:ior},\ref{app:toc-long},\ref{app:tor-long}.
% \subsection{Summary of Findings}

% Our experiments reveal two significant biases in the CLIP model:

% \begin{enumerate}
%     \item \textbf{Text Encoder Bias:} The model shows a strong preference for the first mentioned object in textual descriptions.
%     \item \textbf{Image Encoder Bias:} The model exhibits greater sensitivity to larger objects in images.
% \end{enumerate}

% These biases can significantly impact the overall system performance in various vision-language tasks, particularly in multi-object scenarios.