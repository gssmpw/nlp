% \begin{abstract}
% Contrastive Language-Image Pre-training (CLIP) models have shown remarkable performance in zero-shot classification tasks, but their efficacy in processing complex multi-object scenarios remains challenging. This study presents a comprehensive analysis of CLIP's performance limitations in multi-object contexts through controlled experiments. We introduce two custom datasets, \textit{SimCO} and \textit{CompCO}, designed to evaluate CLIP's image and text encoders in various multi-object configurations. Our findings reveal significant biases in both encoders: the image encoder favors larger objects, while the text encoder prioritizes objects mentioned first in descriptions. We hypothesize that these biases originate from CLIP's training process and provide evidence through analyses of the COCO dataset and CLIP's training progression. Our experiments demonstrate how these biases substantially impact CLIP's performance in image-caption matching tasks, particularly when the objects sizes, and their order in the caption are manipulated. This work contributes valuable insights into CLIP's behavior in complex visual environments and highlights areas for improvement in future vision-language models.
% \end{abstract}


% \begin{abstract}
% Contrastive Language-Image Pre-training (CLIP) models have shown remarkable performance in zero-shot classification tasks, but their efficacy in processing complex multi-object scenarios remains challenging. This study presents a comprehensive analysis of CLIP's performance limitations in multi-object contexts through controlled experiments. We introduce two custom datasets, \textit{SimCO} and \textit{CompCO}, designed to evaluate CLIP's image and text encoders in various multi-object configurations. Our findings reveal significant biases in both encoders: the image encoder favors larger objects, while the text encoder prioritizes objects mentioned first in descriptions. We hypothesize that these biases originate from CLIP's training process and provide evidence through analyses of the COCO dataset and CLIP's training progression. Our experiments demonstrate how these biases impact CLIP's performance in image-caption matching tasks, particularly when object sizes and their order in the caption are manipulated. This work contributes valuable insights into CLIP's behavior in complex visual environments and highlights areas for improvement in future vision-language models.
% \end{abstract}

\begin{abstract}
    Contrastive Language-Image Pre-training (CLIP) models have demonstrated remarkable performance in zero-shot classification tasks, yet their efficacy in handling complex multi-object scenarios remains challenging. This study presents a comprehensive analysis of CLIP's performance limitations in multi-object contexts through controlled experiments. We introduce two custom datasets, SimCO and CompCO, to evaluate CLIP's image and text encoders in various multi-object configurations. Our findings reveal significant biases in both encoders: the image encoder favors larger objects, while the text encoder prioritizes objects mentioned first in descriptions. We hypothesize these biases originate from CLIP's training process and provide evidence through analyses of the COCO dataset and CLIP's training progression. Additionally, we extend our investigation to Stable Diffusion models, revealing that biases in the CLIP text encoder significantly impact text-to-image generation tasks. Our experiments demonstrate how these biases affect CLIP's performance in image-caption matching and generation tasks, particularly when manipulating object sizes and their order in captions. This work contributes valuable insights into CLIP's behavior in complex visual environments and highlights areas for improvement in future vision-language models.
\end{abstract}