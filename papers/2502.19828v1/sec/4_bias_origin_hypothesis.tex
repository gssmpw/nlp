% \section{Bias Origin Hypothesis}
% \label{sec:bias_origin}
% In the previous section, through various experiments, we observed two significant biases in the CLIP model: a bias towards larger objects in images and a bias towards the first object in text. In this section, we investigate the causes of these two biases.
% The bias in the image domain is intuitive. This is due to the structure of vision transformers. The final representation in these models is obtained using the cls token, which is influenced by the patches into which the ViT model divides the entire image during processing. Naturally, the larger an object is, the more patches it will encompass, and consequently, it will have a greater impact on the final CLS token. As we observed in the appendix, this bias exists in all ViT models.
% The bias in the text domain, however, requires more in-depth analysis. Our hypothesis is that the image-side bias (larger objects have a more prominent presence in the final representation) is induced into the text side during contrastive training, causing bias in the text domain. In fact, our hypothesis is that in most image-text pair datasets used in training CLIP models, larger objects appear earlier in the text. Since larger objects in the image have a greater presence in the final image embedding, and the goal of CLIP training is to bring the text and image embeddings closer together, the text-side embedding also tends towards having the first object more prominently represented in the final embedding.
% We have several proofs for this hypothesis:
% \subsection{First Experiment}
% In this experiment, we analyze the COCO dataset, which is a small but similar collection to the multi-billion sample datasets used in CLIP training. In this analysis, we first used the Llama3 model to extract objects present in each caption in this dataset. In the next step, using an object detection model called OWL, we extracted the bbox of each of the objects extracted by llama3 in the corresponding image to the caption and calculated the area of the bbox. We then examined in what percentage of cases the larger object in the image appeared earlier in the text.
% Graph 6 shows the results of this evaluation. As can be inferred from this assessment, in the majority of cases, the larger object in the image also appears earlier in the text. This serves as evidence for our hypothesis.

% \begin{figure} [h!]
%     \centering
%     \includegraphics[width=\columnwidth]{sec/images/coco-train-object-size.pdf}
%     \caption{Performance Comparison of Different Models on Four Objects}
%     \label{fig:performance_comparison}
% \end{figure}

% \subsection{Second Experiment}
% In this experiment, we use checkpoints of the CLIP model trained on the Laion dataset. Our goal is to observe whether this text-side bias existed in the text encoder from the beginning or was gradually induced after CLIP training. For this purpose, we repeat the component representation experiment that we conducted in section 3. The difference is that we perform this experiment in three different states: when the model has seen no data, when the model has seen about 2 billion samples, and when the model has seen about 4 billion samples. As can be seen from Table 7, this bias did not initially exist on the text side, and gradually, with CLIP training, this bias was induced from the image side to the text side.
% \begin{figure} [h!]
%     \centering
%     \includegraphics[width=\columnwidth]{sec/images/during-train.pdf}
%     \caption{Performance Comparison of Different Models on Four Objects}
%     \label{fig:performance_comparison}
% \end{figure}

% \subsection{Third Experiment}
% To support our assumption that larger objects appear faster on the text side, which causes this bias to be induced, we designed a different experiment. In this experiment, we first divided the names of objects in the COCO dataset into two groups: large objects and small objects. The large object group includes objects like cars, ships, and elephants, which are considered large objects in the real world. The small object group includes objects like spoons, scissors, etc., which are considered small objects in the real world.
% We then performed the component representation experiment on the text side in two different scenarios. In the first scenario, an object from the large group is always present as the first object in the text, and an object from the small object group is present as the second object. The second scenario is exactly the opposite, meaning an object from the small group is always present as the first object in the text, and an object from the large object group is present as the second object.
% As can be seen from Table 10, in the second scenario, the bias towards the first object has decreased compared to the first scenario. This experiment also shows that the image-side bias has been induced into the text side, causing objects that are visually large in the real world to have a more prominent presence in the text-side representation of CLIP.


\section{Bias Origin Hypothesis}

This section explores the potential origins of these biases and presents evidence supporting our hypotheses.

\subsection{Image Encoder Bias}
The image domain's bias towards larger objects can be attributed to the structure of vision transformers (ViT) in CLIP's image encoder. Larger objects occupy more patches in the ViT's image division, potentially exerting greater influence on the final CLS token representation.
% Our experiments (see appendix) suggest this bias is consistent across all ViT models, not just CLIP.

% \subsection{Text Encoder Bias}
% The bias in the text domain towards the first mentioned object is less intuitive and requires deeper analysis. We hypothesize that this bias is induced during CLIP's contrastive training process. Our main hypothesis posits that the image-side bias (favoring larger objects) is transferred to the text side during contrastive training, causing a bias towards objects mentioned first in the text.
% This hypothesis is supported by two key arguments. First, in most image-text pair datasets used for CLIP training, larger objects tend to be mentioned earlier in the text descriptions. Second, the contrastive training objective aims to align text and image embeddings, potentially transferring the image-side bias to the text side.
% To validate this hypothesis, we conducted three experiments:
\subsection{Text Encoder Bias}
The bias in the text domain, favoring the first-mentioned object, is more subtle and warrants deeper examination. We hypothesize that this bias originates from CLIP's contrastive training process, which may transfer the image-side bias (preference for larger objects) to the text side. This hypothesis is founded on two key observations:
\begin{enumerate}
\item In CLIP training datasets, larger objects tend to be mentioned earlier in text descriptions.
\item The contrastive training procedure aligns text and image embeddings, potentially facilitating the transfer of biases between modalities.
\end{enumerate}
To validate this hypothesis, we conducted two experimental studies:

% \subsection{Experiment 1: COCO Dataset Analysis}
% We conducted an analysis of the COCO dataset, which shares similarities with the larger datasets used in CLIP training. Our methodology involved using the LLaMA3 model to extract objects from each caption, followed by employing the OWL2 object detection model\cite{minderer2024scaling} to identify bounding boxes for these objects in the corresponding images. We then calculated the area of each bounding box and examined the correlation between object size in the image and its position in the text description. The results of this analysis are presented in Figure \ref{fig:coco_analysis}, which illustrates the percentage of cases where the larger object in the image appeared earlier in the text.

\subsection{Experiment 1: COCO Dataset Analysis}
We analyzed the COCO dataset, which is similar to CLIP training datasets, to investigate object size and text position correlation. Our method used LLaMA3 \cite{touvron2023llama} to extract objects from captions and OWL2 \cite{minderer2024scaling} for object detection in images. We calculated bounding box areas and examined the correlation between object size and text position. Figure \ref{fig:coco_analysis} shows the percentage of cases where larger objects appeared earlier in the text. More information can be found in the \ref{app:coco-anlysis} .







\begin{figure} [h!]
    \centering
    \includegraphics[width=\columnwidth]{sec/images/dis_objects.pdf}
    \caption{In the COCO dataset, the larger objects in an image are typically mentioned earlier in the captions}
    \label{fig:coco_analysis}
\end{figure}
  
% \subsection{Experiment 2: CLIP Training Progression}

% We examined how the text-side bias develops during CLIP training using checkpoints from the LAION dataset training process.

% \textbf{Methodology:} Repeated the component representation experiment at three stages:
% \begin{itemize}
%     \item Pre-training (0 samples seen)
%     \item Mid-training (approximately 2 billion samples seen)
%     \item Late-training (approximately 4 billion samples seen)
% \end{itemize}

% \textbf{Results:} Table \ref{tab:training_progression} shows the development of the text-side bias over the course of training.

% \begin{table}[h]
%     \centering
%     \caption{Text-side bias development during CLIP training}
%     \label{tab:training_progression}
%     \begin{tabular}{lccc}
%         \hline
%         Training Stage & First Object & Second Object & Third Object \\
%         \hline
%         Pre-training & x\% & y\% & z\% \\
%         Mid-training & a\% & b\% & c\% \\
%         Late-training & d\% & e\% & f\% \\
%         \hline
%     \end{tabular}
% \end{table}

% \subsection{Experiment 2: CLIP Training Progression}
% \label{sec:bias_origin}
% We examined how the text-side bias develops during CLIP training using checkpoints from the LAION dataset training process. This experiment extends our Text-based Object Retrieval (TOR) analysis to different stages of the training process.

% \textbf{Methodology:} 
% We repeated the TOR experiment, focusing on the retrieval rate for the first object mentioned in the text, at five stages of training: when the model had seen 2 billion, 4 billion, 6 billion, 8 billion, and 10 billion samples.

% \textbf{Results:} 
% Figure \ref{fig:training_progression} shows the development of the text-side bias over the course of training, as measured by the TOR rate for the first object.

% \begin{figure} [h!]
%     \centering
%     \includegraphics[width=\columnwidth]{sec/images/during-train.pdf}
%     \caption{Performance Comparison of Different Models on Four Objects}
%     \label{fig:training_progression}
% \end{figure}

% As evident from Figure \ref{fig:training_progression}, the TOR rate for the first object increases steadily as the model is exposed to more training samples. This trend is consistent across scenarios with different numbers of objects (3, 4, 5, 6, 7, and 8 objects), as indicated by the different lines on the graph.

% The results demonstrate that the bias towards the first object in text representations was not present in the initial model but developed gradually during training. This supports our hypothesis that the text-side bias is induced during the contrastive learning process, likely as a result of alignment with the image encoder's bias towards larger, more prominent objects.

% The consistent increase in TOR rate across different multi-object scenarios suggests that this bias is a fundamental characteristic of the CLIP training process, rather than an artifact specific to a particular number of objects.
% \subsection{Experiment 2: CLIP Training Progression}
% \label{sec:bias_origin}

% To investigate the development of text-side bias during CLIP training, we extended our Text-based Object Retrieval (TOR) analysis to different stages of the training process using checkpoints from the LAION dataset training. We conducted the TOR experiment at five distinct training stages, corresponding to when the model had been exposed to 2 billion, 4 billion, 6 billion, 8 billion, and 10 billion samples.

% Figure \ref{fig:training_progression} illustrates the evolution of the text-side bias throughout the training process, as measured by the TOR rate for the different objects. The graph reveals a clear trend: the TOR rate for the first object increases steadily as the model is exposed to more training samples. 

% \begin{figure} [h!]
%     \centering
%     \includegraphics[width=\columnwidth]{sec/images/coco_analysis.pdf}
%     \caption{Evolution of Text-based Object Retrieval (TOR) rate for the different object across different training stages and object scenarios}
%     \label{fig:training_progression}
% \end{figure}

% The results provide compelling evidence that the bias towards the first object in text representations was not inherent in the initial model but developed gradually during training. This observation supports our hypothesis that the text-side bias is induced during the contrastive learning process, likely as a result of alignment with the image encoder's bias towards larger, more prominent objects.

 \vspace{-0.1in}
\subsection{Experiment 2: CLIP Training Progression}
\label{sec:bias_origin}
We examined text-side bias development during CLIP training using TOR analysis at five training stages (2, 4, 6, 8, and 10 billion samples) on the LAION dataset.

Figure \ref{fig:training_progression} shows the evolution of text-side bias through TOR rates for different objects. The graph reveals a steady increase in TOR rate for the first object as training progresses.

\begin{figure}[h!]
    \centering
    \includegraphics[width=\columnwidth]{sec/images/coco_analysis.pdf}
    \caption{Evolution of TOR rate across training stages}
    \label{fig:training_progression}
\end{figure}

These results suggest that the bias towards the first object in text representations developed gradually during training, supporting our hypothesis that text-side bias is induced during contrastive learning, likely aligning with the image encoder's bias towards larger, more prominent objects.

% Furthermore, the consistent increase in TOR rate across different multi-object scenarios suggests that this bias is a fundamental characteristic of the CLIP training process, rather than an artifact specific to a particular number of objects. This finding underscores the pervasive nature of the text-side bias in CLIP models and its potential implications for downstream applications.
% \subsection{Experiment 3: Object Size Influence}

% We investigated how the real-world size of objects influences their representation in CLIP's text encoder.

% \textbf{Methodology:}
% \begin{enumerate}
%     \item Divided COCO dataset object names into "large" and "small" groups based on real-world size.
%     \item Conducted component representation experiments in two scenarios:
%     \begin{itemize}
%         \item Scenario 1: Large object first, small object second in text
%         \item Scenario 2: Small object first, large object second in text
%     \end{itemize}
% \end{enumerate}

% \textbf{Results:} Table \ref{tab:size_influence} shows the impact of object size on text representation bias.

% \begin{table}[h]
%     \centering
%     \caption{Influence of object size on text representation bias}
%     \label{tab:size_influence}
%     \begin{tabular}{lcc}
%         \hline
%         Scenario & First Object & Second Object \\
%         \hline
%         Large-Small & 70.64 & 29.36 \\
%         Small-Large & 57.92 & 42.08 \\
%         \hline
%     \end{tabular}
% \end{table}

% \subsection{Conclusion}

% These experiments provide strong evidence for our hypothesis that the text-side bias in CLIP is induced by the image-side bias during contrastive training. This transfer of bias is facilitated by the tendency in training datasets for larger objects to be mentioned earlier in text descriptions. This finding has significant implications for understanding and potentially mitigating biases in multimodal AI models like CLIP.
% \subsection{Experiment 3: Object Size Influence on TOR}

% We investigated how real-world object size influences CLIP's text encoder representations using the TOR method. We categorized COCO dataset objects as "large" or "small" and conducted TOR experiments with two text scenarios: large object followed by small, and vice versa.

% Table \ref{tab:size_influence} shows the TOR rates:

% \begin{table}[h]
%     \centering
%     \scriptsize
%     \setlength{\tabcolsep}{10pt}
%     \renewcommand{\arraystretch}{1.2}
%     \caption{Performance of various CLIP models on original and reordered captions for SimCO and ComCO datasets}
%     \label{tab:size_influence}
%     \begin{tabular}{lcc}
%         \toprule
%         \rowcolor[HTML]{D9EAD3}
%         Scenario & \textbf{First Object} & \textbf{Second Object} \\
%         \midrule
%         Large-Small & 70.64 & 29.36 \\
%         Small-Large & 57.92 & 42.08 \\
%         \bottomrule
%     \end{tabular}
% \end{table}


% The TOR rates indicate that object size significantly influences text representation bias. In the Large-Small scenario, there's a strong bias towards the first object (70.64\%). In the Small-Large scenario, while the first object still has higher TOR rate (57.92\%), the bias is less pronounced.

% These findings suggest that both textual order and object size impact CLIP's text representations in TOR, with larger objects receiving more weight even when mentioned second.