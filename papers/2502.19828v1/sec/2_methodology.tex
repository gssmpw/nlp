\section{Methodology}
\label{sec:methodology}
\subsection{Dataset Design}
To evaluate CLIP models in multi-object scenarios under controlled conditions, we created two datasets: \textbf{SimCO} and \textbf{CompCO}, using Blender for precise control over objects' number, location, and size (see Fig \ref{fig:datasets}, \ref{fig:simco}, \ref{fig:comco}).

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{sec/images/Group1.pdf}
    \caption{Example images from the SimCO and CompCO datasets.}
    \label{fig:datasets}
\end{figure}


\begin{itemize}
    \item \textbf{SimCO Dataset}: Inspired by the CLEVER dataset \cite{johnson2016clevrdiagnosticdatasetcompositional}, SimCO includes 17 basic geometric objects, compared to CLEVER, which only contains 3 such objects. This dataset tests the modelâ€™s ability to handle simple shapes and configurations under controlled conditions.
    
    \item \textbf{CompCO Dataset}: Derived from the COCO dataset \cite{lin2015microsoftcococommonobjects}, CompCO comprises 72 complex and commonly occurring objects. It evaluates the model's performance with more realistic and intricate object arrangements, better matching images in the real-world scenarios.
\end{itemize}

Both datasets contain images with 2, 3, 4, and 5 objects, each paired with a caption accurately describing the objects. This ensures high control and minimizes confounding factors, providing a robust platform for evaluating CLIP models. Additional information can be found in the \ref{app:dataset}.


\subsection{Experimental Setup}

To evaluate CLIP's performance in multi-object scenarios, we designed a series of experiments focusing on both the image and text encoders. Our goal is to assess how each encoder processes and represents multiple objects in their respective modalities.

\subsubsection{Image Encoder Analysis}
To investigate potential biases related to object size, we conducted two experiments:

% We hypothesized that the image encoder exhibits biases related to the object size. To investigate this, we conducted two experiments:
\begin{enumerate}
    \item \textbf{Image-based Object Classification (IOC):} \textbf{\textit{For each object}} in a multi-object image, we trained a single-layer classifier on the vector representations generated by the CLIP image encoder. We then calculated the classification accuracy on test set for individual objects within the images.
    
    % \item \textbf{Image-based Object Retrieval (IOR):} For each multi-object image, we identified most similar single-object images using the CLIP image encoder's representations. We then computed the percentage of cases where the nearest single-object image corresponded to each object in the multi-object image (e.g., first object, second object, etc.).
    \item \textbf{Image-based Object Retrieval (IOR):} For each multi-object image, we identified the most similar single-object images using the CLIP image encoder's representations. We then computed the percentage of cases where the nearest single-object image corresponded to each object in the multi-object image, categorized by size (e.g., largest object, smaller object). 
    This allowed us to quantify CLIP's tendency to prioritize certain objects based on their relative sizes within the multi-object scene.
\end{enumerate}



% These experiments aimed to determine whether larger objects in the image have a more significant impact on the model's final visual representation.

\subsubsection{Text Encoder Analysis}

To assess how object mention order affects text representations, we performed two experiments:
% For the text encoder, we designed experiments to determine the extent to which each object mentioned in a multi-object textual description contributes to CLIP's final text representation, relative to its position in the text:

\begin{enumerate}
    \item \textbf{Text-based Object Classification (TOC):} \textbf{\textit{For each object}} mentioned in a multi-object text description, we train a single-layer binary classifier on the vector representations produced by the CLIP text encoder. We then assess the classification accuracy for individual objects within the text.
    
    \item \textbf{Text-based Object Retrieval (TOR):} For each multi-object text description, we identify the most similar single-object text using the CLIP text encoder's representation. We then calculate the percentage of cases where the nearest single-object text matches to each mentioned object in the multi-object text description (e.g., first mentioned, second mentioned, etc.).
\end{enumerate}

% These experiments were designed to reveal any potential biases in the text encoder related to the order of object mentions in the textual descriptions.

