\section{Conclusion}
Our study reveals significant biases in CLIP's image and text encoders, favoring larger objects and first-mentioned items respectively. These biases, demonstrated through our SimCO and CompCO datasets, substantially impact CLIP's performance in multi-object scenarios. The observed performance drops when manipulating object sizes and mention order underscore CLIP's limitations in handling complex visual environments. These findings highlight the need for more balanced training approaches in vision-language models to mitigate such biases. Future work should focus on developing techniques to address these limitations, advancing the field towards more robust and versatile AI systems capable of accurately interpreting multi-faceted real-world information.