

% \section{Impact of Encoder Biases on CLIP Model Performance}
% \label{sec:impact}
% In the previous section, we demonstrated through numerous experiments that biases exist in both the image encoder and text encoder of the CLIP model, and we examined the causes of these biases. In this section, we aim to show how these biases affect the overall performance of the CLIP model, specifically in the image-caption matching task. Our objective is to illustrate how these biases result in a noticeable decrease in the CLIP model's performance.

% To evaluate this, we conduct experiments involving scenarios with two to five objects per image. For each image, we provide two captions: one correct and one incorrect. The correct caption mentions all objects present in the image, while the incorrect caption replaces one of the objects with a different object. This experiment is carried out in two distinct scenarios:

% \begin{enumerate}
%     \item \textbf{Original:} The order of object appearance in the captions is the same from the first object up to, but not including, the last object. The object known to be larger in the image is placed first. The last object differs between the two captions.
%     \item \textbf{Reordered:} The order of object appearance in the correct caption is altered by moving the first object, which is also the largest object, to the end of the caption(see Figure \ref{fig:first_fig} for an example).
% \end{enumerate}

% As shown in Table \ref{table:performance_drop}, there is a significant drop in model performance when the incorrect caption places the large object at the beginning. Detailed results for other scenarios are provided in the appendix.

% These observations highlight how biases in both the text and image encoders lead to a substantial decrease in CLIP's performance in multi-object scenarios.

% % \begin{table}[ht]
% % \centering
% % \scriptsize
% % \setlength{\tabcolsep}{5pt}
% % \caption{Performance of Various Models on SimCO and ComCO Datasets}

% % \label{tab:toc_results}
% % \begin{tabular}{llcc}
% % \toprule
% % Dataset & Model & Original & Reordered \\ 
% % \midrule
% % \multirow{5}{*}{SimCO} 
% %  & CLIP openAI   & 89.79 & 26.33 \\
% %  & CLIP LAION    & 98.43 & 25.51 \\
% %  & CLIP Datacomp & 98.89 & 16.51 \\
% %  & SIGLIP        & 97.79 & 71.67 \\
% %  & NegCLIP       & 96.83 & 15.50 \\
% % \midrule
% % \multirow{5}{*}{ComCO}   
% %  & CLIP openAI   & 80.48 & 60.71 \\
% %  & CLIP LAION    & 82.82 & 50.62 \\
% %  & CLIP Datacomp & 86.24 & 63.99 \\
% %  & SIGLIP        & 84.73  & 72.36 \\
% %  & NegCLIP       & 76.68 & 46.94 \\
% % \bottomrule
% % \end{tabular}
% % \end{table}


% \begin{table}[ht]
% \centering

% \scriptsize
% \setlength{\tabcolsep}{6pt}
% \renewcommand{\arraystretch}{1.2}
% \caption{Performance of Various Models on SimCO and ComCO Datasets}
%  \label{table:performance_drop}
% \begin{tabular}{llcc}
% \toprule
% \rowcolor[HTML]{EFEFEF}
% Dataset & Model & \textbf{Original} & \textbf{Reordered} \\ 
% \midrule
% % \multirow{11}{*}{SimCO} 
% %  & \textit{ViT-H-14-quickgelu (DFN)\cite{fang2023datafilteringnetworks}} & 88.36 & 73.58 \\
% %  & \textit{ViT-SO400M-SigLIP\cite{zhai2023sigmoidlosslanguageimage}} & 83.32 & 70.56 \\
% %  & \textit{ViT-L-14 (datacomp)\cite{gadre2023datacompsearchgenerationmultimodal}} & 87.81 & 73.62 \\
% %  & \textit{xlm-roberta-large-ViT-H-14} & 86.64 & 76.29 \\
% %  & \textit{ViT-L-14 (laion2b)\cite{schuhmann2022laion5bopenlargescaledataset}} & 82.43 & 69.92 \\
% %  & \textit{ViT-L-14 (openai)\cite{radford2021learningtransferablevisualmodels}} & 74.04 & 63.42 \\
% %  & \textit{ViT-B-32 (datacomp)\cite{gadre2023datacompsearchgenerationmultimodal}} & 78.87 & 65.57 \\
% %  & \textit{ViT-B-32 (laion2b)\cite{schuhmann2022laion5bopenlargescaledataset}} & 78.29 & 69.52 \\
% %  & \textit{ViT-B-32 (metaclip)\cite{xu2024demystifyingclipdata}} & 72.67 & 60.87 \\
% %  & \textit{ViT-B-32 (openai)\cite{radford2021learningtransferablevisualmodels}} & 67.00 & 57.72 \\
% %  & \textit{NegCLIP\cite{yuksekgonul2023visionlanguagemodelsbehavelike}} & 69.01 & 55.06 \\
% % \midrule
% \multirow{11}{*}{ComCO}   
%  & \textit{ViT-H-14-quickgelu (DFN)} & 88.22 & 61.33 \\
%  & \textit{ViT-SO400M-SigLIP\cite{zhai2023sigmoidlosslanguageimage}} & 84.73 & 72.36 \\
%  & \textit{ViT-L-14 (datacomp)\cite{gadre2023datacompsearchgenerationmultimodal}} & 86.24 & 67.50 \\
%  & \textit{xlm-roberta-large-ViT-H-14} & 83.83 & 64.75 \\
%  & \textit{ViT-L-14 (laion2b)\cite{schuhmann2022laion5bopenlargescaledataset}} & 82.82 & 50.62 \\
%  & \textit{ViT-L-14 (openai)\cite{radford2021learningtransferablevisualmodels}} & 80.48 & 60.71 \\
%  & \textit{ViT-B-32 (datacomp)\cite{gadre2023datacompsearchgenerationmultimodal}} & 75.09 & 47.42 \\
%  & \textit{ViT-B-32 (laion2b)\cite{schuhmann2022laion5bopenlargescaledataset}} & 75.81 & 51.67 \\
%  & \textit{ViT-B-32 (metaclip)\cite{xu2024demystifyingclipdata}} & 74.47 & 51.67 \\
%  & \textit{ViT-B-32 (openai)\cite{radford2021learningtransferablevisualmodels}} & 73.22 & 52.23 \\
%  & \textit{NegCLIP\cite{yuksekgonul2023visionlanguagemodelsbehavelike}} & 76.68 & 46.94 \\
% \bottomrule
% \end{tabular}
% \end{table}

% \section{Impact of Encoder Biases on CLIP Model Performance}
% \label{sec:impact}
% \subsection{Image-Text Matching}
% In the previous section, we showed that biases exist in both the image and text encoders of the CLIP model. Here, we illustrate how these biases affect the model's performance in the image-caption matching task, leading to a noticeable performance decrease.

% We conducted experiments with images containing 2 to 5 objects. Each image had two captions: one correct and one incorrect. The correct caption listed all objects, while the incorrect one replaced one object with a different one. We tested two scenarios:

% \begin{enumerate}
%     \item \textbf{Original:} The object order in the captions is the same up to the last object, with the larger object first. The last object differs between captions.
%     \item \textbf{Reordered:} The order in the correct caption is altered by moving the largest object to the end (see Figure \ref{fig:first_fig} for an example).
% \end{enumerate}

% As shown in Table \ref{table:performance_drop}, model performance significantly drops when the incorrect caption places the large object first. Detailed results for other scenarios are provided in the appendix.

% These findings highlight how biases in the text and image encoders lead to a substantial performance decrease in multi-object scenarios.

% \begin{table}[ht]
% \centering
% \scriptsize
% \setlength{\tabcolsep}{6pt}
% \renewcommand{\arraystretch}{1.2}
% \caption{Performance of Various Models on SimCO and ComCO Datasets}
%  \label{table:performance_drop}
% \begin{tabular}{llcc}
% \toprule
% \rowcolor[HTML]{EFEFEF}
% Dataset & Model & \textbf{Original} & \textbf{Reordered} \\ 
% \midrule
% \multirow{8}{*}{ComCO}   
%  & \textit{ViT-H-14-quickgelu (DFN)} & 88.22 & 61.33 \\
%  & \textit{ViT-SO400M-SigLIP} & 84.73 & 72.36 \\
%  & \textit{ViT-L-14 (datacomp)} & 86.24 & 67.50 \\
%  & \textit{xlm-roberta-large-ViT-H-14} & 83.83 & 64.75 \\
%  & \textit{ViT-L-14 (laion2b)} & 82.82 & 50.62 \\
%  & \textit{ViT-L-14 (openai)} & 80.48 & 60.71 \\
%  % & \textit{ViT-B-32 (datacomp)} & 75.09 & 47.42 \\
%  % & \textit{ViT-B-32 (laion2b)} & 75.81 & 51.67 \\
%  % & \textit{ViT-B-32 (metaclip)} & 74.47 & 51.67 \\
%  & \textit{ViT-B-32 (openai)} & 73.22 & 52.23 \\
%  & \textit{NegCLIP} & 76.68 & 46.94 \\
% \bottomrule
% \end{tabular}
% \end{table}


% \subsection{Stable Diffusion}

% Stable Diffusion models, which are text-to-image generation models, utilize the CLIP text encoder to encode the input text. To observe the impact of CLIP's bias on the performance of these models, we designed the following experiment. We generated 1,000 multi-object images using the prompt "obj1 and obj2 and obj3 and obj4" with these models, leveraging objects from the COCO dataset. We then used the powerful LLAVA model to evaluate the presence of each object mentioned in the prompt within the generated images. The results are shown in Table 4.



% \begin{table}[ht]
% \centering
% \scriptsize
% \setlength{\tabcolsep}{3pt}
% \renewcommand{\arraystretch}{1.2}
% \caption{Performance of various CLIP models on TOC and TOR for ComCO datasets}
% \label{tab:tor_results2}
% \begin{tabular}{lcccc}
% \toprule
% \rowcolor[HTML]{EFEFEF}
% Model & \textbf{First Obj} & \textbf{Second Obj} & \textbf{Third Obj} & \textbf{Fourth Obj} \\ 
% \midrule 
% \textit{SD v1.4} & 17.8 & 14.6 & 13.4 & 12.0 \\
% \textit{SD V2} & 18.7 & 16.3 & 14.3 & 14.3 \\
% \textit{SD-XL} & 33.3 & 29.5 & 25.6 & 25.5 \\
% \bottomrule
% \end{tabular}
% \end{table}

% Overall, generating multi-object images is a challenging task for these models. However, as the results indicate, the bias in the text encoder has a clear impact on these models.


\section{Practical Impacts of Encoder Biases}
\label{sec:impact}

\subsection{Image-Text Matching}
In the previous section, we showed that biases exist in both the image and text encoders of the CLIP model. Here, we illustrate how these biases affect the model's performance in the image-caption matching task, leading to a noticeable performance decrease.

We conducted experiments with images containing 2 to 5 objects. Each image had two captions: one correct and one incorrect. The correct caption listed all objects, while the incorrect one replaced one object with a different one. We tested two scenarios:

\begin{enumerate}
    \item \textbf{Original:} The object order in the captions is the same up to the last object, with the larger object first. The last object differs between captions.
    \item \textbf{Reordered:} The order in the correct caption is altered by moving the largest object to the end (see Figure \ref{fig:first_fig} for an example).
\end{enumerate}

As shown in Table \ref{table:performance_drop}, model performance significantly drops when the incorrect caption places the large object first. Detailed results for other scenarios and dataset are provided in the \ref{app:imtexmatch}.

These findings highlight how biases in the text and image encoders lead to a substantial performance decrease in multi-object scenarios.

\begin{table}[ht]
\centering
\scriptsize
\setlength{\tabcolsep}{6pt}
\renewcommand{\arraystretch}{1.2}
\caption{Performance of Various Models on SimCO and ComCO Datasets}
 \label{table:performance_drop}
\begin{tabular}{llcc}
\toprule
\rowcolor[HTML]{EFEFEF}
Dataset & Model & \textbf{Original} & \textbf{Reordered} \\ 
\midrule
\multirow{5}{*}{ComCO}   
& \textit{CLIP openAI} & 80.48 & 60.71 \\
& \textit{CLIP LAION} & 82.82 & 50.62 \\
& \textit{CLIP Datacomp} & 86.24 & 63.99 \\
 & \textit{SIGLIP} & 84.73 & 74.09 \\
 & \textit{NegCLIP} & 76.68 & 46.94 \\
\bottomrule
\end{tabular}
\end{table}


\subsection{Text to image geneation}
To observe CLIP's bias impact on Stable Diffusion models, we generated 1,000 multi-object images using prompts with four objects from the COCO dataset. LLAVA model \cite{liu2024visual} evaluated object presence in generated images. Results in Table \ref{tab:tor_results2} show the bias's clear impact, with earlier-mentioned objects appearing more frequently.

\begin{table}[ht]
\centering
\scriptsize
\setlength{\tabcolsep}{3pt}
\renewcommand{\arraystretch}{1.2}
\caption{Object presence in Stable Diffusion-generated images }
\label{tab:tor_results2}
\begin{tabular}{lcccc}
\toprule
\rowcolor[HTML]{EFEFEF}
Model & \textbf{First Obj} & \textbf{Second Obj} & \textbf{Third Obj} & \textbf{Fourth Obj} \\ 
\midrule 
\textit{SD v1.4} & 17.8 & 14.6 & 13.4 & 12.0 \\
\textit{SD V2} & 18.7 & 16.3 & 14.3 & 14.3 \\
\textit{SD-XL} \cite{podell2023sdxl} & 33.3 & 29.5 & 25.6 & 25.5 \\
\bottomrule
\end{tabular}
\end{table}

While multi-object image generation remains challenging, these results demonstrate the text encoder bias's impact on Stable Diffusion models.