@inproceedings{Involution,
	title={Involution: Inverting the inherence of convolution for visual recognition},
	author={Li, Duo and Hu, Jie and Wang, Changhu and Li, Xiangtai and She, Qi and Zhu, Lei and Zhang, Tong and Chen, Qifeng},
	booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
	pages={12321--12330},
	year={2021}
}

@inproceedings{Learnedqueries,
	title={Learned queries for efficient local attention},
	author={Arar, Moab and Shamir, Ariel and Bermano, Amit H},
	booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
	pages={10841--10852},
	year={2022}
}

@inproceedings{LlamamoeV1,
	title={Llama-moe: Building mixture-of-experts from llama with continual pre-training},
	author={Zhu, Tong and Qu, Xiaoye and Dong, Daize and Ruan, Jiacheng and Tong, Jingqi and He, Conghui and Cheng, Yu},
	booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
	pages={15913--15923},
	year={2024}
}

@article{LlamamoeV2,
	title={LLaMA-MoE v2: Exploring Sparsity of LLaMA from Perspective of Mixture-of-Experts with Post-Training},
	author={Qu, Xiaoye and Dong, Daize and Hu, Xuyang and Zhu, Tong and Sun, Weigao and Cheng, Yu},
	journal={arXiv preprint arXiv:2411.15708},
	year={2024}
}

@inproceedings{Perceiver,
	title={Perceiver: General perception with iterative attention},
	author={Jaegle, Andrew and Gimeno, Felix and Brock, Andy and Vinyals, Oriol and Zisserman, Andrew and Carreira, Joao},
	booktitle={International conference on machine learning},
	pages={4651--4664},
	year={2021},
	organization={PMLR}
}

@inproceedings{RepLKNet,
	title={Scaling up your kernels to 31x31: Revisiting large kernel design in cnns},
	author={Ding, Xiaohan and Zhang, Xiangyu and Han, Jungong and Ding, Guiguang},
	booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
	pages={11963--11975},
	year={2022}
}

@inproceedings{Settransformer,
	title={Set transformer: A framework for attention-based permutation-invariant neural networks},
	author={Lee, Juho and Lee, Yoonho and Kim, Jungtaek and Kosiorek, Adam and Choi, Seungjin and Teh, Yee Whye},
	booktitle={International conference on machine learning},
	pages={3744--3753},
	year={2019},
	organization={PMLR}
}

@article{Volo,
	title={Volo: Vision outlooker for visual recognition},
	author={Yuan, Li and Hou, Qibin and Jiang, Zihang and Feng, Jiashi and Yan, Shuicheng},
	journal={IEEE transactions on pattern analysis and machine intelligence},
	volume={45},
	number={5},
	pages={6575--6586},
	year={2022},
	publisher={IEEE}
}

@inproceedings{acnet,
	title={Acnet: Strengthening the kernel skeletons for powerful cnn via asymmetric convolution blocks},
	author={Ding, Xiaohan and Guo, Yuchen and Ding, Guiguang and Han, Jungong},
	booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
	pages={1911--1920},
	year={2019}
}

@inproceedings{cbam,
	title={Cbam: Convolutional block attention module},
	author={Woo, Sanghyun and Park, Jongchan and Lee, Joon-Young and Kweon, In So},
	booktitle={Proceedings of the European conference on computer vision (ECCV)},
	pages={3--19},
	year={2018}
}

@article{cct,
	title={Escaping the big data paradigm with compact transformers},
	author={Hassani, Ali and Walton, Steven and Shah, Nikhil and Abuduweili, Abulikemu and Li, Jiachen and Shi, Humphrey},
	journal={arXiv preprint arXiv:2104.05704},
	year={2021}
}

@inproceedings{cswin,
	title={Cswin transformer: A general vision transformer backbone with cross-shaped windows},
	author={Dong, Xiaoyi and Bao, Jianmin and Chen, Dongdong and Zhang, Weiming and Yu, Nenghai and Yuan, Lu and Chen, Dong and Guo, Baining},
	booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
	pages={12124--12134},
	year={2022}
}

@inproceedings{cvt,
	title={Cvt: Introducing convolutions to vision transformers},
	author={Wu, Haiping and Xiao, Bin and Codella, Noel and Liu, Mengchen and Dai, Xiyang and Yuan, Lu and Zhang, Lei},
	booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
	pages={22--31},
	year={2021}
}

@article{deepseekv1,
	title={Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models},
	author={Dai, Damai and Deng, Chengqi and Zhao, Chenggang and Xu, RX and Gao, Huazuo and Chen, Deli and Li, Jiashi and Zeng, Wangding and Yu, Xingkai and Wu, Y and others},
	journal={arXiv preprint arXiv:2401.06066},
	year={2024}
}

@inproceedings{deformablev1,
	title={Deformable convolutional networks},
	author={Dai, Jifeng and Qi, Haozhi and Xiong, Yuwen and Li, Yi and Zhang, Guodong and Hu, Han and Wei, Yichen},
	booktitle={Proceedings of the IEEE international conference on computer vision},
	pages={764--773},
	year={2017}
}

@inproceedings{deformablev2,
	title={Deformable convnets v2: More deformable, better results},
	author={Zhu, Xizhou and Hu, Han and Lin, Stephen and Dai, Jifeng},
	booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
	pages={9308--9316},
	year={2019}
}

@inproceedings{densenet,
	title={Densely connected convolutional networks},
	author={Huang, Gao and Liu, Zhuang and Van Der Maaten, Laurens and Weinberger, Kilian Q},
	booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
	pages={4700--4708},
	year={2017}
}

@article{dilated,
	title={Multi-scale context aggregation by dilated convolutions},
	author={Yu, F},
	journal={arXiv preprint arXiv:1511.07122},
	year={2015}
}

@article{focal,
	title={Focal self-attention for local-global interactions in vision transformers},
	author={Yang, Jianwei and Li, Chunyuan and Zhang, Pengchuan and Dai, Xiyang and Xiao, Bin and Yuan, Lu and Gao, Jianfeng},
	journal={arXiv preprint arXiv:2107.00641},
	year={2021}
}

@article{goyal2021coordination,
	title={Coordination among neural modules through a shared global workspace},
	author={Goyal, Anirudh and Didolkar, Aniket and Lamb, Alex and Badola, Kartikeya and Ke, Nan Rosemary and Rahaman, Nasim and Binas, Jonathan and Blundell, Charles and Mozer, Michael and Bengio, Yoshua},
	journal={arXiv preprint arXiv:2103.01197},
	year={2021}
}

@inproceedings{internimage,
	title={Internimage: Exploring large-scale vision foundation models with deformable convolutions},
	author={Wang, Wenhai and Dai, Jifeng and Chen, Zhe and Huang, Zhenhang and Li, Zhiqi and Zhu, Xizhou and Hu, Xiaowei and Lu, Tong and Lu, Lewei and Li, Hongsheng and others},
	booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
	pages={14408--14419},
	year={2023}
}

@inproceedings{metaformer,
	title={Metaformer is actually what you need for vision},
	author={Yu, Weihao and Luo, Mi and Zhou, Pan and Si, Chenyang and Zhou, Yichen and Wang, Xinchao and Feng, Jiashi and Yan, Shuicheng},
	booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
	pages={10819--10829},
	year={2022}
}

@inproceedings{mmoe,
	title={Modeling task relationships in multi-task learning with multi-gate mixture-of-experts},
	author={Ma, Jiaqi and Zhao, Zhe and Yi, Xinyang and Chen, Jilin and Hong, Lichan and Chi, Ed H},
	booktitle={Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery \& data mining},
	pages={1930--1939},
	year={2018}
}

@article{mobilevit,
	title={Mobilevit: light-weight, general-purpose, and mobile-friendly vision transformer},
	author={Mehta, Sachin and Rastegari, Mohammad},
	journal={arXiv preprint arXiv:2110.02178},
	year={2021}
}

@article{moesurvey,
	title={A survey on mixture of experts},
	author={Cai, Weilin and Jiang, Juyong and Wang, Fan and Tang, Jing and Kim, Sunghun and Huang, Jiayi},
	journal={arXiv preprint arXiv:2407.06204},
	year={2024}
}

@article{p2t,
	title={P2T: Pyramid pooling transformer for scene understanding},
	author={Wu, Yu-Huan and Liu, Yun and Zhan, Xin and Cheng, Ming-Ming},
	journal={IEEE transactions on pattern analysis and machine intelligence},
	volume={45},
	number={11},
	pages={12760--12771},
	year={2022},
	publisher={IEEE}
}

@inproceedings{pelk,
	title={PeLK: Parameter-efficient Large Kernel ConvNets with Peripheral Convolution},
	author={Chen, Honghao and Chu, Xiangxiang and Ren, Yongjian and Zhao, Xin and Huang, Kaiqi},
	booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
	pages={5557--5567},
	year={2024}
}

@article{pvtv2,
	title={Pvt v2: Improved baselines with pyramid vision transformer},
	author={Wang, Wenhai and Xie, Enze and Li, Xiang and Fan, Deng-Ping and Song, Kaitao and Liang, Ding and Lu, Tong and Luo, Ping and Shao, Ling},
	journal={Computational Visual Media},
	volume={8},
	number={3},
	pages={415--424},
	year={2022},
	publisher={Springer}
}

@inproceedings{repvgg,
	title={Repvgg: Making vgg-style convnets great again},
	author={Ding, Xiaohan and Zhang, Xiangyu and Ma, Ningning and Han, Jungong and Ding, Guiguang and Sun, Jian},
	booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
	pages={13733--13742},
	year={2021}
}

@inproceedings{repvit,
	title={Repvit: Revisiting mobile cnn from vit perspective},
	author={Wang, Ao and Chen, Hui and Lin, Zijia and Han, Jungong and Ding, Guiguang},
	booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
	pages={15909--15920},
	year={2024}
}

@inproceedings{resnet,
	title={Deep residual learning for image recognition},
	author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
	pages={770--778},
	year={2016}
}

@inproceedings{senet,
	title={Squeeze-and-excitation networks},
	author={Hu, Jie and Shen, Li and Sun, Gang},
	booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
	pages={7132--7141},
	year={2018}
}

@article{slak,
	title={More convnets in the 2020s: Scaling up kernels beyond 51x51 using sparsity},
	author={Liu, Shiwei and Chen, Tianlong and Chen, Xiaohan and Chen, Xuxi and Xiao, Qiao and Wu, Boqian and K{\"a}rkk{\"a}inen, Tommi and Pechenizkiy, Mykola and Mocanu, Decebal and Wang, Zhangyang},
	journal={arXiv preprint arXiv:2207.03620},
	year={2022}
}

@inproceedings{slide,
	title={Slide-transformer: Hierarchical vision transformer with local self-attention},
	author={Pan, Xuran and Ye, Tianzhu and Xia, Zhuofan and Song, Shiji and Huang, Gao},
	booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
	pages={2082--2091},
	year={2023}
}

@inproceedings{swin,
	title={Swin transformer: Hierarchical vision transformer using shifted windows},
	author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
	booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
	pages={10012--10022},
	year={2021}
}

@article{transformer,
	title={Attention is all you need},
	author={Vaswani, A},
	journal={Advances in Neural Information Processing Systems},
	year={2017}
}

@inproceedings{valanarasu2021medical,
	title={Medical transformer: Gated axial-attention for medical image segmentation},
	author={Valanarasu, Jeya Maria Jose and Oza, Poojan and Hacihaliloglu, Ilker and Patel, Vishal M},
	booktitle={Medical image computing and computer assisted intervention--MICCAI 2021: 24th international conference, Strasbourg, France, September 27--October 1, 2021, proceedings, part I 24},
	pages={36--46},
	year={2021},
	organization={Springer}
}

