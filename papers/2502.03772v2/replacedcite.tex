\section{Related Work}
\label{sec:related}
\subsection{Convolutional Neural Networks}
Convolutional Neural Networks (CNNs) have revolutionized computer vision, serving as a cornerstone for tasks such as image classification, object detection, and semantic segmentation ____. Their success stems from their ability to autonomously learn hierarchical features directly from raw pixel data, eliminating the need for manual feature engineering. Key inductive biases, including spatial locality and translational invariance, enable CNNs to efficiently capture local patterns while maintaining robustness to positional variations in natural images.

Despite these strengths, conventional CNNs exhibit limitations in medical imaging applications. Their restricted receptive fields hinder the integration of global contextual information, which is critical for interpreting complex anatomical structures and subtle pathological features [3]. This constraint becomes particularly detrimental when local regions require semantic dependencies spanning the entire image.Recent advancements aim to enhance CNN generalizability through three synergistic strategies: (1) Expanding receptive fields via enlarged kernels ____ or structural re-parameterization____; (2) Incorporating adaptive operators such as dilated____ or deformable convolutions ____ to dynamically adjust spatial sampling; (3) Integrating attention mechanisms____ to refine feature discriminability. These innovations preserve CNNs' local feature extraction strengths while enhancing global contextual modeling, positioning them as adaptable frameworks for medical image analysis

\subsection{Vision Transformers}
In natural language processing (NLP), Transformers____ have demonstrated exceptional capabilities in modeling long-range dependencies. This success has inspired researchers to explore the application of Transformer architectures to computer vision, where the ViT represents a landmark development. Recent works have explored applying ViT to various vision tasks: image classification, object detection, image segmentation, depth estimation, image generation, video processing, and others. Despite its success, ViT also presents certain limitations. One notable drawback is its reliance on large-scale datasets for effective training, as Transformers lack the inductive biases inherent in CNNs, such as locality and translation invariance____. As a result, ViTs may struggle to generalize effectively when trained on smaller datasets____. Additionally, ViTs are computationally demanding, particularly due to their self-attention mechanisms, which scale quadratically with image size, leading to high memory and processing costs. 

To enhance the efficiency of ViTs while maintaining performance, several studies____ have proposed methods that incorporate local self-attention mechanisms or pooling operations. By restricting self-attention to localized regions of an image or employing pooling to condense feature maps, these optimizations not only streamline processing but also ensure that the models remain adept at handling diverse vision tasks, thereby achieving a balance between computational efficiency and model efficacy. Several other studies have primarily concentrated on diversifying the ViT architectures and enhancing their adaptability, such as the Focal Transformer____, MobileViT____, and Axial-Attention Transformer____, each contributing to the field with innovative approaches to attention mechanisms and knowledge distillation techniques.
\subsection{Latent Space and Sparse Learning}
The concept of Latent Space is pivotal in machine learning for capturing the intrinsic features of data. Its utility in deep learning models is exemplified by intermediate layer representations, which are crucial for various tasks____. To enhance efficiency and prediction accuracy, learned queries have been explored in architectures such as SetTransformers____ and Perceiver ____ networks, where they project inputs into a lower-dimensional space. Goyal et al.____ further advanced this concept by leveraging learned queries as a shared workspace to reduce computational complexity in Transformers. In addition to these developments, attention mechanisms have also been optimized through the incorporation of learnable tokens. Models such as Involution____, VOLO____, and QnA____ demonstrate how learnable tokens can replace traditional queries or keys, generating dynamic affinity matrices that significantly boost model performance. Collectively, these innovations underscore the importance of Latent Space in driving the evolution and enhancing the capabilities of deep learning models.

Sparse learning stands as a pivotal approach within machine learning, particularly for its effectiveness in optimizing computational resources and enhancing model performance. A prime example of this approach is the Mixture-of-Experts (MOE) framework, which partitions models into specialized experts, selectively activating only the relevant subset to process inputs and thereby achieving computational efficiency. The Multi-Mixture-of-Experts (MMoE)____ model epitomizes MOE's application in multi-task learning, designed to explicitly learn task relationships from data. It enables the sharing of expert sub-models across various tasks while simultaneously training a gating network to optimize performance for each individual task. Within the realm of large language model(LLM), MOE has attracted considerable scholarly attention; studies such as DeepSeek____, and LLaMA-MoE____ are at the forefront of research, showcasing the potential of MOE in scaling up large language models  with a consistent number of activated parameters. In summary, MOE emerges as a potent sparse representation model, and it is recommended that readers consult reference ____ for an in-depth exploration of the subject.