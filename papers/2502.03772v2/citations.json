[
  {
    "index": 0,
    "papers": [
      {
        "key": "resnet",
        "author": "He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian",
        "title": "Deep residual learning for image recognition"
      },
      {
        "key": "densenet",
        "author": "Huang, Gao and Liu, Zhuang and Van Der Maaten, Laurens and Weinberger, Kilian Q",
        "title": "Densely connected convolutional networks"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "RepLKNet",
        "author": "Ding, Xiaohan and Zhang, Xiangyu and Han, Jungong and Ding, Guiguang",
        "title": "Scaling up your kernels to 31x31: Revisiting large kernel design in cnns"
      },
      {
        "key": "slak",
        "author": "Liu, Shiwei and Chen, Tianlong and Chen, Xiaohan and Chen, Xuxi and Xiao, Qiao and Wu, Boqian and K{\\\"a}rkk{\\\"a}inen, Tommi and Pechenizkiy, Mykola and Mocanu, Decebal and Wang, Zhangyang",
        "title": "More convnets in the 2020s: Scaling up kernels beyond 51x51 using sparsity"
      },
      {
        "key": "pelk",
        "author": "Chen, Honghao and Chu, Xiangxiang and Ren, Yongjian and Zhao, Xin and Huang, Kaiqi",
        "title": "PeLK: Parameter-efficient Large Kernel ConvNets with Peripheral Convolution"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "acnet",
        "author": "Ding, Xiaohan and Guo, Yuchen and Ding, Guiguang and Han, Jungong",
        "title": "Acnet: Strengthening the kernel skeletons for powerful cnn via asymmetric convolution blocks"
      },
      {
        "key": "repvgg",
        "author": "Ding, Xiaohan and Zhang, Xiangyu and Ma, Ningning and Han, Jungong and Ding, Guiguang and Sun, Jian",
        "title": "Repvgg: Making vgg-style convnets great again"
      },
      {
        "key": "repvit",
        "author": "Wang, Ao and Chen, Hui and Lin, Zijia and Han, Jungong and Ding, Guiguang",
        "title": "Repvit: Revisiting mobile cnn from vit perspective"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "dilated",
        "author": "Yu, F",
        "title": "Multi-scale context aggregation by dilated convolutions"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "deformablev1",
        "author": "Dai, Jifeng and Qi, Haozhi and Xiong, Yuwen and Li, Yi and Zhang, Guodong and Hu, Han and Wei, Yichen",
        "title": "Deformable convolutional networks"
      },
      {
        "key": "deformablev2",
        "author": "Zhu, Xizhou and Hu, Han and Lin, Stephen and Dai, Jifeng",
        "title": "Deformable convnets v2: More deformable, better results"
      },
      {
        "key": "internimage",
        "author": "Wang, Wenhai and Dai, Jifeng and Chen, Zhe and Huang, Zhenhang and Li, Zhiqi and Zhu, Xizhou and Hu, Xiaowei and Lu, Tong and Lu, Lewei and Li, Hongsheng and others",
        "title": "Internimage: Exploring large-scale vision foundation models with deformable convolutions"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "senet",
        "author": "Hu, Jie and Shen, Li and Sun, Gang",
        "title": "Squeeze-and-excitation networks"
      },
      {
        "key": "cbam",
        "author": "Woo, Sanghyun and Park, Jongchan and Lee, Joon-Young and Kweon, In So",
        "title": "Cbam: Convolutional block attention module"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "transformer",
        "author": "Vaswani, A",
        "title": "Attention is all you need"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "cvt",
        "author": "Wu, Haiping and Xiao, Bin and Codella, Noel and Liu, Mengchen and Dai, Xiyang and Yuan, Lu and Zhang, Lei",
        "title": "Cvt: Introducing convolutions to vision transformers"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "cct",
        "author": "Hassani, Ali and Walton, Steven and Shah, Nikhil and Abuduweili, Abulikemu and Li, Jiachen and Shi, Humphrey",
        "title": "Escaping the big data paradigm with compact transformers"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "swin",
        "author": "Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining",
        "title": "Swin transformer: Hierarchical vision transformer using shifted windows"
      },
      {
        "key": "slide",
        "author": "Pan, Xuran and Ye, Tianzhu and Xia, Zhuofan and Song, Shiji and Huang, Gao",
        "title": "Slide-transformer: Hierarchical vision transformer with local self-attention"
      },
      {
        "key": "cswin",
        "author": "Dong, Xiaoyi and Bao, Jianmin and Chen, Dongdong and Zhang, Weiming and Yu, Nenghai and Yuan, Lu and Chen, Dong and Guo, Baining",
        "title": "Cswin transformer: A general vision transformer backbone with cross-shaped windows"
      },
      {
        "key": "p2t",
        "author": "Wu, Yu-Huan and Liu, Yun and Zhan, Xin and Cheng, Ming-Ming",
        "title": "P2T: Pyramid pooling transformer for scene understanding"
      },
      {
        "key": "pvtv2",
        "author": "Wang, Wenhai and Xie, Enze and Li, Xiang and Fan, Deng-Ping and Song, Kaitao and Liang, Ding and Lu, Tong and Luo, Ping and Shao, Ling",
        "title": "Pvt v2: Improved baselines with pyramid vision transformer"
      },
      {
        "key": "metaformer",
        "author": "Yu, Weihao and Luo, Mi and Zhou, Pan and Si, Chenyang and Zhou, Yichen and Wang, Xinchao and Feng, Jiashi and Yan, Shuicheng",
        "title": "Metaformer is actually what you need for vision"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "focal",
        "author": "Yang, Jianwei and Li, Chunyuan and Zhang, Pengchuan and Dai, Xiyang and Xiao, Bin and Yuan, Lu and Gao, Jianfeng",
        "title": "Focal self-attention for local-global interactions in vision transformers"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "mobilevit",
        "author": "Mehta, Sachin and Rastegari, Mohammad",
        "title": "Mobilevit: light-weight, general-purpose, and mobile-friendly vision transformer"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "valanarasu2021medical",
        "author": "Valanarasu, Jeya Maria Jose and Oza, Poojan and Hacihaliloglu, Ilker and Patel, Vishal M",
        "title": "Medical transformer: Gated axial-attention for medical image segmentation"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "goyal2021coordination",
        "author": "Goyal, Anirudh and Didolkar, Aniket and Lamb, Alex and Badola, Kartikeya and Ke, Nan Rosemary and Rahaman, Nasim and Binas, Jonathan and Blundell, Charles and Mozer, Michael and Bengio, Yoshua",
        "title": "Coordination among neural modules through a shared global workspace"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "Settransformer",
        "author": "Lee, Juho and Lee, Yoonho and Kim, Jungtaek and Kosiorek, Adam and Choi, Seungjin and Teh, Yee Whye",
        "title": "Set transformer: A framework for attention-based permutation-invariant neural networks"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "Perceiver",
        "author": "Jaegle, Andrew and Gimeno, Felix and Brock, Andy and Vinyals, Oriol and Zisserman, Andrew and Carreira, Joao",
        "title": "Perceiver: General perception with iterative attention"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "goyal2021coordination",
        "author": "Goyal, Anirudh and Didolkar, Aniket and Lamb, Alex and Badola, Kartikeya and Ke, Nan Rosemary and Rahaman, Nasim and Binas, Jonathan and Blundell, Charles and Mozer, Michael and Bengio, Yoshua",
        "title": "Coordination among neural modules through a shared global workspace"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "Involution",
        "author": "Li, Duo and Hu, Jie and Wang, Changhu and Li, Xiangtai and She, Qi and Zhu, Lei and Zhang, Tong and Chen, Qifeng",
        "title": "Involution: Inverting the inherence of convolution for visual recognition"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "Volo",
        "author": "Yuan, Li and Hou, Qibin and Jiang, Zihang and Feng, Jiashi and Yan, Shuicheng",
        "title": "Volo: Vision outlooker for visual recognition"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "Learnedqueries",
        "author": "Arar, Moab and Shamir, Ariel and Bermano, Amit H",
        "title": "Learned queries for efficient local attention"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "mmoe",
        "author": "Ma, Jiaqi and Zhao, Zhe and Yi, Xinyang and Chen, Jilin and Hong, Lichan and Chi, Ed H",
        "title": "Modeling task relationships in multi-task learning with multi-gate mixture-of-experts"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "deepseekv1",
        "author": "Dai, Damai and Deng, Chengqi and Zhao, Chenggang and Xu, RX and Gao, Huazuo and Chen, Deli and Li, Jiashi and Zeng, Wangding and Yu, Xingkai and Wu, Y and others",
        "title": "Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "LlamamoeV1",
        "author": "Zhu, Tong and Qu, Xiaoye and Dong, Daize and Ruan, Jiacheng and Tong, Jingqi and He, Conghui and Cheng, Yu",
        "title": "Llama-moe: Building mixture-of-experts from llama with continual pre-training"
      },
      {
        "key": "LlamamoeV2",
        "author": "Qu, Xiaoye and Dong, Daize and Hu, Xuyang and Zhu, Tong and Sun, Weigao and Cheng, Yu",
        "title": "LLaMA-MoE v2: Exploring Sparsity of LLaMA from Perspective of Mixture-of-Experts with Post-Training"
      }
    ]
  },
  {
    "index": 23,
    "papers": [
      {
        "key": "moesurvey",
        "author": "Cai, Weilin and Jiang, Juyong and Wang, Fan and Tang, Jing and Kim, Sunghun and Huang, Jiayi",
        "title": "A survey on mixture of experts"
      }
    ]
  }
]