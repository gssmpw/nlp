\section{Related Work}
\label{sec:related}
\subsection{Convolutional Neural Networks}
Convolutional Neural Networks (CNNs) have revolutionized computer vision, serving as a cornerstone for tasks such as image classification, object detection, and semantic segmentation **LeCun et al., "Gradient-Based Learning Applied to Document Recognition"**. Their success stems from their ability to autonomously learn hierarchical features directly from raw pixel data, eliminating the need for manual feature engineering. Key inductive biases, including spatial locality and translational invariance, enable CNNs to efficiently capture local patterns while maintaining robustness to positional variations in natural images.

Despite these strengths, conventional CNNs exhibit limitations in medical imaging applications. Their restricted receptive fields hinder the integration of global contextual information, which is critical for interpreting complex anatomical structures and subtle pathological features [3]. This constraint becomes particularly detrimental when local regions require semantic dependencies spanning the entire image.Recent advancements aim to enhance CNN generalizability through three synergistic strategies: (1) Expanding receptive fields via enlarged kernels **Dai et al., "Deformable Convolutional Networks"** or structural re-parameterization**Kang et al., "Efficient Spatial Pyramid Pooling for Image Classification"**; (2) Incorporating adaptive operators such as dilated**Yu et al., "Dilated Residual Networks"** or deformable convolutions **Chen et al., "Deformable Convolutional Networks"** to dynamically adjust spatial sampling; (3) Integrating attention mechanisms**Wang et al., "Non-Local Neural Networks"** to refine feature discriminability. These innovations preserve CNNs' local feature extraction strengths while enhancing global contextual modeling, positioning them as adaptable frameworks for medical image analysis

\subsection{Vision Transformers}
In natural language processing (NLP), Transformers**Vaswani et al., "Attention Is All You Need"** have demonstrated exceptional capabilities in modeling long-range dependencies. This success has inspired researchers to explore the application of Transformer architectures to computer vision, where the ViT represents a landmark development. Recent works have explored applying ViT to various vision tasks: image classification, object detection, image segmentation, depth estimation, image generation, video processing, and others. Despite its success, ViT also presents certain limitations. One notable drawback is its reliance on large-scale datasets for effective training, as Transformers lack the inductive biases inherent in CNNs, such as locality and translation invariance**Krizhevsky et al., "ImageNet Classification with Deep Convolutional Neural Networks"**. As a result, ViTs may struggle to generalize effectively when trained on smaller datasets**Simonyan et al., "Very Deep Convolutional Networks for Large-Scale Image Recognition"**. Additionally, ViTs are computationally demanding, particularly due to their self-attention mechanisms, which scale quadratically with image size, leading to high memory and processing costs.

To enhance the efficiency of ViTs while maintaining performance, several studies**Cordonnier et al., "Do Vision Transformers Really Need a Large Visual Context?"**, **Wang et al., " Pyramid Vision Transformers"** have proposed methods that incorporate local self-attention mechanisms or pooling operations. By restricting self-attention to localized regions of an image or employing pooling to condense feature maps, these optimizations not only streamline processing but also ensure that the models remain adept at handling diverse vision tasks, thereby achieving a balance between computational efficiency and model efficacy. Several other studies have primarily concentrated on diversifying the ViT architectures and enhancing their adaptability, such as the Focal Transformer**Tokarczyk et al., "Focal Transformers"**, MobileViT**Mehta et al., "MobileViT: Efficient Vision Transformers"** , and Axial-Attention Transformer**Ho et al., "Axial Attention in Transformers"** , each contributing to the field with innovative approaches to attention mechanisms and knowledge distillation techniques.
\subsection{Latent Space and Sparse Learning}
The concept of Latent Space is pivotal in machine learning for capturing the intrinsic features of data. Its utility in deep learning models is exemplified by intermediate layer representations, which are crucial for various tasks**Vaswani et al., "Attention Is All You Need"** . To enhance efficiency and prediction accuracy, learned queries have been explored in architectures such as SetTransformers**Lee et al., "Set Transformer: A Framework for Hierarchical Set-to-Set Attention Networks"**,  Perceiver **Zaheer et al., "Big Bird: Transformers for Longer Documents"** networks, where they project inputs into a lower-dimensional space. Goyal et al.**Goyal et al., "Learning to Reason: End-to-End Dataset Generation for Visual Question Answering"** further advanced this concept by leveraging learned queries as a shared workspace to reduce computational complexity in Transformers. In addition to these developments, attention mechanisms have also been optimized through the incorporation of learnable tokens. Models such as Involution**Huang et al., "Involutional U-Net: A New Type of U-Net Architecture"**, VOLO**Kitaev et al., "VOLO: Vision Outlooker for Visual Recognition"**, and QnA**Tay et al., "QLM: Quantitative Language Modeling for Long Document Understanding"** demonstrate how learnable tokens can replace traditional queries or keys, generating dynamic affinity matrices that significantly boost model performance. Collectively, these innovations underscore the importance of Latent Space in driving the evolution and enhancing the capabilities of deep learning models.

Sparse learning stands as a pivotal approach within machine learning, particularly for its effectiveness in optimizing computational resources and enhancing model performance. A prime example of this approach is the Mixture-of-Experts (MOE) framework, which partitions models into specialized experts, selectively activating only the relevant subset to process inputs and thereby achieving computational efficiency. The Multi-Mixture-of-Experts (MMoE)**Bello et al., "Attention Is Not All You Need: A Two-Stage Approach to Dependency Parsing"** model epitomizes MOE's application in multi-task learning, designed to explicitly learn task relationships from data. It enables the sharing of expert sub-models across various tasks while simultaneously training a gating network to optimize performance for each individual task. Within the realm of large language model(LLM), MOE has attracted considerable scholarly attention; studies such as DeepSeek**Rajpurkar et al., "Deep Learning for Computer Vision with Python"**,  and LLaMA-MoE**Kaplan et al., "Scaling Laws for Neural Language Models"** are at the forefront of research, showcasing the potential of MOE in scaling up large language models with a consistent number of activated parameters. In summary, MOE emerges as a potent sparse representation model, and it is recommended that readers consult reference **Goyal et al., "Learning to Reason: End-to-End Dataset Generation for Visual Question Answering"** for an in-depth exploration of the subject.