\section{Related Work}
\label{sec:related-works}
\subsection{Cross-domain Recommendation}
Cross-domain recommendations (CDR) aim to enhance the accuracy of recommendations in a target domain by leveraging knowledge from a source domain____. CoNet____ introduces cross-connections to facilitate dual knowledge transfer across domains, while MiNet____, featuring inter-level and interest-level attention mechanisms, jointly models users' long-term and short-term interests. DASL____ employs a dual embedding and attention strategy for iterative information transfer between domains. AFT____ employs a generative adversarial network to master feature translations across diverse domains. ________ consider behavior-level effect during the loss optimization process by proposing a generic behavioral importance-aware optimization framework. Additionally, CDR offers solutions to the cold-start problem, with CCDR____ and SSCDR____ advocating for contrastive learning and semi-supervised learning approaches, respectively, to compensate for the scarcity of user behavior data.

\subsection{Contrastive Learning in Recommendation}
Contrastive learning (CL), aimed at acquiring high-quality representations through self-supervised techniques, has garnered significant attention across various fields of machine learning____. Motivated by the accomplishments of CL across different domains, there has been a surge in innovative research efforts that incorporate CL into recommender systems____. ________ propose GDCL to capture the structural properties of the user-item interaction graph more effectively. ________ introduce a simple CL method that eschews graph augmentations in favor of injecting uniform noise into the embedding space, thereby generating contrastive views. AdaGCL____ employs two adaptive contrastive view generators for data augmentation, significantly enhancing the collaborative filtering (CF) paradigm.