\section{Related Work}
\label{sec:related-works}
\subsection{Cross-domain Recommendation}
Cross-domain recommendations (CDR) aim to enhance the accuracy of recommendations in a target domain by leveraging knowledge from a source domain**Chen, "Cross-Domain Recommendation via Knowledge Transfer"**. CoNet**Liu, et al., "CoNet: Cooperative Neural Networks for Cross-Domain Recommendation"**, introducing cross-connections to facilitate dual knowledge transfer across domains, while MiNet**Wu, et al., "Mixture of Inter-Level and Interest-Level Attention Mechanisms for Cross-Domain Recommendation"**, featuring inter-level and interest-level attention mechanisms, jointly models users' long-term and short-term interests. DASL**Ding, et al., "Dual Embedding and Attention Strategy for Iterative Information Transfer between Domains"**, employing a dual embedding and attention strategy for iterative information transfer between domains. AFT**Feng, et al., "Adversarial Feature Translation for Cross-Domain Recommendation"**, employs a generative adversarial network to master feature translations across diverse domains. ________ consider behavior-level effect during the loss optimization process by proposing a generic behavioral importance-aware optimization framework. Additionally, CDR offers solutions to the cold-start problem, with CCDR**Wang, et al., "Contrastive Cross-Domain Recommendation"** and SSCDR**Sun, et al., "Semi-Supervised Contrastive Cross-Domain Recommendation"**, advocating for contrastive learning and semi-supervised learning approaches, respectively, to compensate for the scarcity of user behavior data.

\subsection{Contrastive Learning in Recommendation}
Contrastive learning (CL), aimed at acquiring high-quality representations through self-supervised techniques, has garnered significant attention across various fields of machine learning**Chen, et al., "Improved Baselines and Bayesian Active Transfer Learning"**. Motivated by the accomplishments of CL across different domains, there has been a surge in innovative research efforts that incorporate CL into recommender systems**Wang, et al., "Deep Transfer Learning for Cross-Domain Recommendation"**. ________ propose GDCL to capture the structural properties of the user-item interaction graph more effectively. ________ introduce a simple CL method that eschews graph augmentations in favor of injecting uniform noise into the embedding space, thereby generating contrastive views. AdaGCL**Liu, et al., "Adaptive Contrastive View Generators for Deep Transfer Learning"**, employing two adaptive contrastive view generators for data augmentation, significantly enhancing the collaborative filtering (CF) paradigm.