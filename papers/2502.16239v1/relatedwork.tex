\section{Related Work}
\label{sec:related-works}
\subsection{Cross-domain Recommendation}
Cross-domain recommendations (CDR) aim to enhance the accuracy of recommendations in a target domain by leveraging knowledge from a source domain~\cite{zhao2023beyond,zhang2023collaborative,zhao2023cross,liu2024graph}. CoNet~\cite{hu2018conet} introduces cross-connections to facilitate dual knowledge transfer across domains, while MiNet~\cite{ouyang2020minet}, featuring inter-level and interest-level attention mechanisms, jointly models users' long-term and short-term interests. DASL~\cite{li2021dual} employs a dual embedding and attention strategy for iterative information transfer between domains. AFT~\cite{hao2021adversarial} employs a generative adversarial network to master feature translations across diverse domains. ~\citeauthor{chen2023cross}~\cite{chen2023cross} consider behavior-level effect during the loss optimization process by proposing a generic behavioral importance-aware optimization framework. Additionally, CDR offers solutions to the cold-start problem, with CCDR~\cite{xie2022contrastive} and SSCDR~\cite{kang2019semi} advocating for contrastive learning and semi-supervised learning approaches, respectively, to compensate for the scarcity of user behavior data.

\subsection{Contrastive Learning in Recommendation}
Contrastive learning (CL), aimed at acquiring high-quality representations through self-supervised techniques, has garnered significant attention across various fields of machine learning~\cite{chen2020simple,zhu2021graph,gutmann2010noise}. Motivated by the accomplishments of CL across different domains, there has been a surge in innovative research efforts that incorporate CL into recommender systems~\cite{qiu2021memory,zhou2020s3,ma2020disentangled}. ~\citeauthor{zhang2022diffusion}~\cite{zhang2022diffusion} propose GDCL to capture the structural properties of the user-item interaction graph more effectively. \citeauthor{yu2022graph}~\cite{yu2022graph} introduce a simple CL method that eschews graph augmentations in favor of injecting uniform noise into the embedding space, thereby generating contrastive views. AdaGCL~\cite{jiang2023adaptive} employs two adaptive contrastive view generators for data augmentation, significantly enhancing the collaborative filtering (CF) paradigm.