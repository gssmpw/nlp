\section{Related Works}
\noindent This section introduces LNL methods related to STCT.

\subsection{Sample Selection Methods}

This category of methods mitigates the adverse effects of label noise by identifying noisy samples from the contaminated training datasets. Various dynamic learning features, such as training loss, can serve as criteria for sample selection ____. After distinguishing between clean and noisy data, a robust classifier can be trained by assigning smaller weights to noisy samples ____ or by discarding noisy labels and converting the problem into a semi-supervised learning task ____. Despite the remarkable performance, these methods heavily rely on manually designed sample selection rules, which may limit their generalization across different datasets ____.

\subsection{Label Correction Methods}

These methods refurbish training labels to prevent neural networks from overfitting to noisy data. To achieve this, some methods use neural network predictions as pseudo labels to update noisy labels in the training data ____. Another line of works employ unsupervised contrastive learning to obtain robust representations and perform label correction based on the geometric properties of the training data ____. For instance, Li et al. applied a smoothness constraint on neighboring samples based on geometric relationships in the embedding space to correct noisy labels ____. Unfortunately, these geometric or probabilistic property based label correction methods generally exhibit severe performance degradation in high noise rate scenarios ____.

\subsection{Meta Learning Based Methods}
With the core idea of learning to learn ____, meta learning based LNL methods take sample weights ____ or training labels ____ as hyper-parameters, and learn appropriate hyper-parameters through designing different meta tasks to alleviate the impact of noisy labels. Among these, meta learning based label correction methods have demonstrated impressive results. For example, DMLP proposed a non-nested framework which trained a robust classifier through decoupling the label correction process into label-free representation learning and a linear meta label purifier ____. Although effective, these methods require a clean validation set to assess model performance on the clean distribution, which incurs extra annotation cost and contradicts the settings of the LNL problem. What's more, some studies attempted to generate a validation set using pseudo labels or sample selection techniques. For example, FSR ____ proposed using the historical gradient information to select high-confidence clean samples, while FaMUS ____ constructed a clean validation set based on training loss. Despite the fact that a pseudo-clean validation set can be collected through these methods, the manually designed rules may introduce a biased estimation of the clean distribution, limiting model performance ____.

Unlike these existing meta learning based methods, our STCT proposes a novel noisy meta label correction framework which uses noisy data as the validation set to combat label noise, eliminating the reliance on extra clean data without introducing biased estimation.

\begin{figure*}[]
    \centering
    \includegraphics[width=1.0\textwidth]{figure/fig3.pdf}
    \caption{The overall framework of STCT. NMC constructs a linear model in the embedding space and uses the sampled noisy validation set to evaluate model performance, enabling label correction. SRL selects clean samples from the corrected training labels and uses semi-supervised learning to improve the inter-class separability of the embedding features extracted by the encoder.}
    \label{fig:diagram_method}
\end{figure*}