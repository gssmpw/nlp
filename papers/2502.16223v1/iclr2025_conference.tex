
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

\input{math_commands.tex}
\input{macro}

\pdfoutput=1

\title{Prompt as Knowledge Bank: Boost Vision-language model via Structural Representation for  zero-shot medical detection}



\newcommand{\equalcontrib}{\ast} 
\newcommand{\correspondingauthor}{\dagger}

\newcommand{\leader}{\ddagger}

\author{Yuguang Yang$^{\equalcontrib,1,2 }$, \textbf{Tongfei Chen}$^{\equalcontrib,3}$, Haoyu Huang$^{3,5}$, \textbf{Linlin Yang}$^{\correspondingauthor,4}$, Chunyu Xie$^{\correspondingauthor,2}$, \\ \textbf{Dawei Leng}$^{\leader,2}$,  \textbf{Xianbin Cao$^{1}$}, \textbf{Baochang Zhang}$^{3, 6}$ \\ 
~\\
$^{1}$School of Electronic Information Engineering, Beihang University, China\\
$^{2}$360 AI Research, Qihoo 360, China\\
$^{3}$School of Artificial Intelligence, Beihang University, China\\
$^{4}$State Key Laboratory of Media Convergence and Communication, \\~~Communication University of China, China\\
$^{5}$National Superior College for Engineers, Beihang University, China\\
$^{6}$Artificial Intelligence Research Center, Lobachevsky State University,\\~~ Nizhny Novgorod 603022, Russia
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle
\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\footnotetext[1]{Co-First Authors. {\{guangbuaa, tfchen\}@buaa.edu.cn}}
\footnotetext[2]{Corresponding Authors.  {lyang@cuc.edu.cn}, {xiechunyu@360.cn}}
\footnotetext[3]{Project Lead.}

\vspace{-15pt}
\begin{abstract}
Zero-shot medical detection enhances existing models without relying on annotated medical images, offering significant clinical value. By using grounded vision-language models (GLIP) with detailed disease descriptions as prompts, doctors can flexibly incorporate new disease characteristics to improve detection performance. However, current methods often oversimplify prompts as mere equivalents to disease names and lacks the ability to incorporate visual cues, leading to coarse image-description alignment.
To address this, we propose StructuralGLIP, a framework that encodes prompts into a latent knowledge bank, enabling more context-aware and fine-grained alignment. By selecting and matching the most relevant features from image representations and the knowledge bank at layers, StructuralGLIP captures nuanced relationships between image patches and target descriptions. This approach also supports category-level prompts, which can remain fixed across all instances of the same category and provide more comprehensive information compared to instance-level prompts. Our experiments show that StructuralGLIP outperforms previous methods across various zero-shot and fine-tuned medical detection benchmarks. The code will be available at \url{https://github.com/CapricornGuang/StructuralGLIP}.
\end{abstract}


\section{Introduction}
\label{sec:intro}

Zero-shot medical detection is crucial in healthcare as it enhances detection capabilities without requiring additional annotated medical images, even after model fine-tuning~\citep{badawi2024review, mahapatra2021medical, qin2023medical}. This is particularly valuable in clinical settings, where doctors often encounter new disease characteristics not previously documented. In such cases, clinicians can temporarily create custom prompts to guide the detection process, allowing models to adapt to novel scenarios more effectively. Recent studies have explored the potential of grounded language-image pre-training models (GLIP)~\citep{phan2024decomposing, tiu2022expert, li2022grounded, yao2022detclip} to reduce dependence on annotations by leveraging prior knowledge.
These models conduct detection by contrasting image features with descriptive texts, known as \textit{contextual prompts}, generated by visual question-answer models for query objects. To adapt GLIP to the medical domain, recent works~\citep{qin2023medical, wu2023zero, guo2023multiple} have employed medically enhanced question-answer models like PubMedBERT~\citep{gu2021domain} and BLIP~\citep{li2022blip} to create attribute-rich prompts. These prompts capture nuanced characteristics of query targets, improving domain adaptation and performance beyond traditional supervised training.

However, existing contextual prompt-based methods often suffer from coarse alignment between images and target descriptions, resulting in two key issues. \textbf{First}, these methods typically treat prompts as contexts that are equivalent to the target, easily causing distribution shift problems to the target's representation. Despite incorporating the prior about the target, they also introduce distracting information about the target. This leads to misalignment between the target and the actual visual cues in the image (see Fig.~\ref{fig:vis-bccd-polyp}). 
\textbf{Second}, category-level descriptions can not  be effectively encoded within the context, which often contain ambiguous vocabularies such as "tissue with pink or red color, irregular or round shape" for a "bump". This causes that the most relevant prompt can not be precisely matched with the input image.

\begin{figure}[t]
  \centering
\includegraphics[width=0.9\textwidth]{teaser.pdf}
  \caption{(a) {Contextual prompt methods directly concatenate the prompt and target. 
  (b) Our structural prompt method encodes prompts into a latent knowledge bank.}}
  \label{fig:teaser}
  \vspace{-20pt}
\end{figure}

To address the aforementioned issues, we present\ours, a novel zero-shot medical detection model that derives \textit{structural representations}, which are delicately organized sets of features specifically designed to represent the nuances of the target and the input image. Specifically, as shown in Fig.~\ref{fig:teaser}, instead of simply concatenating prompts with the target, StructuralGLIP adopts a dual-branch architecture. The main branch processes the target name and input image, while the auxiliary branch encodes the prompts into a latent knowledge bank. At each layer, rather than directly performing cross-modal fusion between vision and language features, StructuralGLIP introduces a mutual selection mechanism. This mechanism matches vision features from the main branch with relevant prompt features stored in the latent knowledge bank, where we extract latent prompt tokens and latent vision tokens that both highly relevant to the target and the current input image, forming fine-grained structural representations. Once these structural representations are formed, the image and language features from the main branch are fused with the selected prompt tokens via cross-modality multi-head attention~\citep{vaswani2017attention}. This enhances the overall feature alignment and improves the fusion process within the main branch. Conceptually, the hierarchical knowledge bank in StructuralGLIP functions like a memory system~\citep{bi2021dual, paivio2013imagery}. As the image is processed, relevant knowledge is dynamically retrieved from the bank. This enables the model to better align the image features with the prompt information, resulting in more accurate and context-aware detection.


In this way, StructuralGLIP can address the challenge of effectively utilizing category-level prompts, which provide broader yet consistent information for all instances within the same category (see Fig.~\ref{fig:vis} for visualization). StructuralGLIP’s instance-wise selection mechanism ensures that even fixed category-level prompts are dynamically aligned with the specific visual features of each instance. This not only improves detection precision but also enhances efficiency, as category-level prompts can remain fixed across instances of the same category.
To validate the proposed method, we benchmark\ours~against previous state-of-the-art methods on eight datasets under endoscopy, microscopy, photography, and radiology four imaging conditions, and conduct a comprehensive analysis towards\ours's structural representations. 
The primary contributions of our work are as follows:
\begin{itemize}[leftmargin=14pt, itemsep = -2pt, topsep = 0pt] 
\item We introduce StructuralGLIP, a novel architecture that achieves adaptive, context-aware alignment between visual features and target descriptions by utilizing a dual-branch structure with mutual selection, enhancing the precision of medical object detection. 
\item We propose the use of category-level prompts, which remain fixed for all instances of the same target. Unlike instance-level prompts, category-level prompts provide more comprehensive prior knowledge about the target disease, reducing the need for prompt generation for each individual image while maintaining strong detection performance. 
\item We explore zero-shot medical detection in more practical settings by demonstrating how zero-shot enhancement can further improve the performance of models fine-tuned on medical data. StructuralGLIP not only surpasses fully supervised methods such as RetinaNet but also seamlessly integrates into GLIP models fine-tuned on medical datasets, achieving an average improvement of +4\% AP.
\end{itemize}




\section{Related Work}
\textbf{Zero-shot medical detection} aims to identify and locate pathology concepts in medical images without relying on annotated data from the target domain~\citep{vilouras2024zero, qin2023medical, paul2021generalized, mahapatra2021medical, 2020ssns, 2022SOP}. Classical strategies include cross-domain generalization~\citep{adaptzero,capellan2024zero} and unsupervised learning~\citep{2020ssns,2022SOP,paul2021generalized}. Cross-domain generalization utilizes data from related domains under varied conditions, such as different imaging techniques~\citep{adaptzero} or demographic differences~\citep{capellan2024zero}, to adapt models across diverse scenarios. Unsupervised learning methods leverage side information to bypass direct supervision, such as using cell nuclei structure for image resolution analysis~\citep{2020ssns}, employing GANs with public annotations to enhance mask quality~\citep{2022SOP}, and correlating medical reports with disease features to increase detection accuracy~\citep{paul2021generalized}. However, these methods are often tightly coupled to specific data priors and exhibit a considerable performance gap compared to supervised models, limiting their clinical significance.

Recent approaches have integrated expert-level knowledge into vision-language models trained on natural images to facilitate domain transfer~\citep{liuchatgpt, lai2024carzero, tiu2022expert, wu2023medklip, zhang2023knowledge}. However, most of these efforts focus on medical classification, while the more practical and complex task of medical detection remains underexplored. For example,~\citep{qin2023medical} conducted a comprehensive study on medical detection using prompts generated by a medically-enhanced language model, PubMedBERT~\citep{gu2021domain}. Follow-up studies~\citep{wu2023zero, lu2023visual, phan2024decomposing} employed BLIP~\citep{li2022blip} to generate image-specific linguistic attributes, or used GPT~\citep{achiam2023gpt} to detail target concepts with nuanced descriptions. Recent work~\citep{guo2023multiple} further advanced this approach by introducing an ensemble strategy for fusing multiple prompts to improve detection accuracy. However, these methods require unique prompts for each instance, significantly reducing efficiency. Our method, \ours, addresses these challenges by introducing a vision-language model that leverages a knowledge bank to store a wide range of prompts, enabling instance-dynamic prompt selection in the latent feature space.




\textbf{Knowledge-bank-based prompt method} is initially developed for continual learning, which utilizes a prompt pool designed to enhance cross-domain generalization~\citep{2022L2P, wang2022dualprompt, 2023codaprompt, 2023attriclip, du2022learning}. 
Previous works~\citep{2022L2P, wang2022dualprompt} select top-$k$ prompts aligned with input image features, facilitating domain-specific modeling.
Recent advances have evolved this strategy, replacing the top-$k$ prompt selection
with a more flexible continuous prompt fusion strategy~\citep{2023codaprompt}, exploring its potential for vision-language model~\citep{2023attriclip}, and expanding applications to open-vocabulary detection tasks~\citep{du2022learning}.
{However, these methods typically require an additional training phase and are restricted to prompt retrieval in the input layer. In contrast,~\ours~explores a linguistically accessible avenue by directly utilizing the attributes predefined by the generative models and embeds these attribute prompts into a hierarchy knowledge bank situated within an auxiliary branch to achieve a layer-wise selection process.
}

\section{Methodology}

\subsection{Preliminaries}
\begin{figure}[ht]
    \centering
    \includegraphics[width=1\linewidth]{settings.pdf}
    \caption{Experimental settings for zero-shot medical detection and enhancement. 
    }
    \label{fig:settings}
    
    \vspace{-10pt}
\end{figure}



\textbf{Zero-shot medical object detection} means improving the model's medical detection performance without the use of supervised image labels. This formulation emphasizes "further improvement without supervised images", which contains two experimental settings. Firstly, in the classical setting, the model, without fine-tuning on medical datasets, uses pre-trained vision-language models with prompts to infer medical concepts (see Fig.~\ref{fig:settings}
(a) and (b)). Secondly, considering the clinical setting prefers supervised models for their excellent performance, we propose a zero-shot enhancement setting. This involves fine-tuning the model on medical datasets first, and then using prompts to further improve performance on unseen medical images, without requiring additional labels (see Fig.~\ref{fig:settings}(c) and (d)). This setting mirrors real-world clinical needs, where models can be continuously improved with new knowledge without the need for labeled data.


\textbf{GLIP} redefines object detection as a phrase-grounding task by employing a late fusion dual-tower architecture to align image and text features. It uses separate backbones ${Enc}_{I}$ and ${Enc}_{T}$ to extract initial encodings $O^{0}$ and $P^{0}$ for images and text, respectively. These features are then integrated through a cross-modal multi-head attention module ($\text{X-MHA}$), enabling fine-grained interaction between the modalities. The integration of image and text features through the deep fusion module ($\text{X-MHA}$) is formalized as follows:

\begin{equation} 
    O^{i}_{t2v}, P^{i}_{v2t} = \text{X-MHA}(O^{i}, P^{i}), \end{equation} \begin{equation} O^{i+1} = f_{I}^{i}(O^{i} + O^{i}_{t2v}), \quad P^{i+1} = f_{L}^{i}(P^{i} + P^{i}_{v2t}), \end{equation}

where $f_{I}^{i}$ and $f_{L}^{i}$ are the $i^{\text{th}}$ encoder layers for images and text, respectively, and $i \in [1, N]$. After $N$ layers of interaction, the final image and text representations are denoted as $O^{N}$ and $P^{N}$, respectively. These representations are used as input to the RPN for generating object proposals:

\begin{equation} R_{\text{GLIP}} = \text{RPN}(O^{N}, P^{N}), \end{equation}

where $R_{\text{GLIP}}$ denotes the set of region proposals of GLIP generated by the RPN. Each proposal $r \in R$ is characterized by its bounding box coordinates and a confidence score, indicating the likelihood of the region containing the target object.


\subsection{Zero-shot Dual-branch Prompt Framework}

In the proposed\ours~framework, we introduce a novel zero-shot architecture to achieve fine-grained alignment between target description and medical images. The overall pipeline is shown in Fig.~\ref{fig:pipeline}.

\textbf{Structurally separated auxiliary and main branches.} \ours~adopts a dual-branch architecture. The main branch processes the target name and input image, while the auxiliary branch encodes the prompts into a latent knowledge bank. Given the object target $T$ and the prompt $Prompt$, the initial representations $T^{0}$ and $B^{0}$ are obtained as follows:

\begin{equation} T^{0} = \text{Enc}_{L_1}({T}), \quad B^{0} = \text{Enc}_{L_2}({Prompt}), \end{equation}

where $\text{Enc}_{L_1}$ and $\text{Enc}_{L_2}$ are language backbones with shared parameters for the main and auxiliary branches, respectively. Here, $T^{0}$ and $B^{0} \in \mathbb{R}^{N_{l} \times D}$ represent the initial encoded features of the target and prompts, with [PAD] tokens used to pad the input sentences to a uniform length $N_{l}$. The encoded prompts $B^{0}$ are then processed through the language encoder layers:

\begin{equation} B^{i} = f_{L_2}^{i}(B^{i-1}), \end{equation}

where $f_{L_2}^{i}$ is the $i^{\text{th}}$ language encoder layer of the auxiliary branch, and $B^{i}$ denotes representation of prompt bank at the $i^{\text{th}}$ layer.


\textbf{Mutual prompt selection mechanism for structural representation in the auxiliary branch.} This mechanism identifies mutually relevant tokens between the visual tokens from the main branch and the linguistic tokens from the auxiliary branch. For selecting the Top-$P$ relevant visual tokens from the latent representation of the input image, we calculate their similarity with latent prompt features. The visual and linguistic representations in the $i$-th layer are denoted as $O^{i}_{q} \in \mathbb{R}^{N_{v} \times D}$ and $B^{i}_{q} \in \mathbb{R}^{N_{l} \times D}$, respectively. We have the following:

\begin{equation} 
O^{i}_{q} = [\bm{o}_{1}^{i}, \bm{o}_{2}^{i}, \ldots, \bm{o}_{N_{v}}^{i}], \quad \mathcal{K}^{i}_{v} = \text{Top-}P^{max} \left( \left[ \text{key} = \bm{o}_{j}^{i}, \text{value} = \bm{o}_{j}^{i} \bm{B}_{q}^{i} \right]_{j=1}^{N_{v}} \right ),
\end{equation}

where $\text{Top-}P^{max} \left( [\text{key}, \text{value}] \right)$ denotes selecting the keys with the Top-$P$ maximal values, $\mathcal{K}^{i}_{v}$ is the selected visual tokens in the $i$-th encoder layer, and $N_{v}$ is the token length of the visual encoder. 
Similarly, to select the Top-$Q$ tokens from the latent representation of the prompt, we use the similarity to the selected visual tokens $\mathcal{K}^{i}_{v}$:

\begin{equation} B^{i}_{q} = [\bm{b}_{1}^{i}, \bm{b}_{2}^{i}, \ldots, \bm{b}_{N_{l}}^{i}], \quad \mathcal{K}^{i}_{l} = \text{Top-}Q^{max} \left( \left[ \text{key} = \bm{b}_{j}^{i}, \text{value} =  \bm{b}_{j}^{i} \mathcal{K}^{i}_{v} \right]_{j=1}^{N_{l}} \right), \end{equation}

where $\text{Top-}Q^{max} \left( [\text{key}, \text{value}] \right)$ denotes selecting the keys with the Top-$Q$ maximal values. $\mathcal{K}^{i}_{l}$ is the selected linguistic tokens in the $i$-th layer, and $N_{l}$ is the token length of the language encoder. These selected prompt tokens $\mathcal{K}^{i}_{v}$ and $\mathcal{K}^{i}_{l}$ are highly relevant to the target and the current input image,
forming fine-grained structural representations.

\begin{figure}[t]
  \centering
  \includegraphics[width=\textwidth]{pipeline.pdf}
  \caption{Pipeline of the proposed method and the automatic prompt generation.}
  \label{fig:pipeline}
  \vspace{-15pt}
\end{figure}

\textbf{Deep fusion with the vision-language prompt in the main branch.} Once the structural representations are obtained, we serve these selected prompt tokens $\mathcal{K}^{i}_{v}$ and $\mathcal{K}^{i}_{l}$  as latent prompts for the deep fusion process of GLIP. Instead of using the auxiliary language encoder to enhance the features, the main branch’s vision and language encoders leverage the knowledge from the selected tokens $\mathcal{K}_{l}^{i}$ and $\mathcal{K}_{v}^{i}$. This ensures that comprehensive knowledge from the prompts can be extracted precisely and applied in an instance-wise manner to enhance the detection process. Specifically, we employ a multi-head attention (MHA) mechanism~\cite{vaswani2017attention} for  ($\mathcal{K}_{v}^{i}$, $T_{q}^{i}$) and ($\mathcal{K}_{l}^{i}$, $O_{q}^{i}$):


\begin{equation} 
O_{t2v}^{\text{top}Q} = \text{MHA}(\text{Q}=\mathcal{K}_{v}^{i}, \text{KV}=T_{q}^{i}) \quad
 T_{v2t}^{\text{top}P} = \text{MHA}(\text{Q}=\mathcal{K}_{l}^{i}, \text{KV}=O_{q}^{i}),
\end{equation}
where $\text{Q}$ denotes the query item and $\text{KV}$ denotes the key and value items for MHA, and $O_{t2v}^{\text{top}Q}$, $T_{v2t}^{\text{top}P}$ is the input image and target representations that incorporate the prior knowledge about the target from the selected tokens, respectively. These representations are then combined with the original layer representation using the following residual connection:

\begin{equation} O^{i+1} = f_{I}^{i}(O^{i} + O_{t2v}^{\text{top}Q}), \quad T^{i+1} = f_{L}^{i}(T^{i} + T_{v2t}^{\text{top}P}). \end{equation}

This deep fusion mechanism ensures that the model dynamically integrates relevant prompts at each layer, significantly enhancing instance-specific adaptation for zero-shot medical detection. After $N$ layers of interaction, we obtain the final image and text representations from the main branch, denoted as $O^{N}$ and $T^{N}$, respectively. Here, $T^{N}$ represents the target's representation, which has fused prompt information relevant to the current instance, achieving a more precise alignment with $O^{N}$.
These representations are then used as input to the RPN for generating object proposals:

\begin{equation} R_{\text{StructuralGLIP}} = \text{RPN}(O^{N}, T^{N}), \end{equation}

where $R_{\text{StructuralGLIP}}$ denotes the set of region proposals generated by the RPN in StructuralGLIP. This process effectively combines the structural representations from both the visual and language modalities to achieve accurate and context-aware zero-shot detection.






\subsection{Instance/Category-level Medical Prompt Automatic Generation}


As shown in Fig.~\ref{fig:pipeline}(b), we propose a dual-level prompt generation mechanism that constructs a comprehensive prompt repository at both the instance and category levels. This enables StructuralGLIP to dynamically apply the most relevant knowledge during inference, significantly improving detection accuracy.

\textbf{Instance-level Prompt Generation.} For each medical image, we generate instance-specific prompts to capture unique visual features such as shape, color, and morphology using a Visual Question Answering (VQA) model like BLIP~\cite{li2022blip}. We query the model with targeted questions (e.g., “What is the shape of the polyp?”), and the responses form a set of instance-level contextual prompts (e.g., “[pink-white, bump-like, round]”). This process ensures that the model can dynamically adapt to the specific characteristics of each image, providing fine-grained descriptions that are crucial for precise detection.

\textbf{Category-level Prompt Generation.} In parallel, we construct a category-level prompt bank containing general attributes relevant to each medical category. Using a language model like GPT-4, we generate detailed descriptions for common attributes such as shape, color, and morphology (e.g., “typical shapes of polyps include round, oval, and nodular-like”). This enriched prompt bank serves as a static reference, enabling the model to capture the broader context of each category and generalize effectively across diverse medical cases. Finally, we gather all attributes from the instance-level prompt and concatenate them with the GPT-4 augmented results to derive the category-level prompt (displayed in Appendix~\ref{app:detailed category-level prompt}). 

\textbf{Application.} Category-level prompts provide comprehensive information for entire classes of medical images and remain fixed across all images within the same category, offering higher efficiency compared to instance-specific prompts. Despite this advantage, prior methods~\cite{guo2023multiple, qin2023medical, wu2023zero} have not fully benefited from general prompts (see Tab.~\ref{tab:category_instance_prompt_comparison}) due to their lack of adaptive prompt selection. StructuralGLIP, however, utilizes an instance-wise selection mechanism that supports category-level prompts effectively. This allows the model to dynamically select the most relevant prompts from the prompt bank, achieving performance comparable to or even better than instance-level prompts on certain datasets. This demonstrates that our method can efficiently leverage general prompts to enhance zero-shot detection without the need for instance-specific generation.

\vspace{-10pt}
\section{Experiment}
\vspace{-10pt}

We illustrate our experiment settings in Fig.~\ref{fig:settings}, where we design four distinct settings to evaluate the model's performance. In Sec.~\ref{sec:exp_zsd}, we follow traditional zero-shot setups to evaluate StructuralGLIP in a zero-shot setting without any fine-tuning on medical datasets, using both instance-specific and category-specific prompts (see Fig.~\ref{fig:settings}(a) and (b)). In Sec.~\ref{sec:exp_zse}, we simulate clinical environments where supervised models are typically preferred. Here, we fine-tune the backbone of the proposed methods, \ie, GLIP, (without using prompts) on medical datasets to form a refined detector. After fine-tuning, we incorporate linguistic prompts for the target disease into StructuralGLIP to perform zero-shot enhancement, evaluating the model's ability to improve performance even after fine-tuning (see Fig.~\ref{fig:settings}(c) and (d)). The fine-tuned details are provided in Appendix~\ref{app:training_details}.

\subsection{Experimental Setup}


\textbf{Datasets.} We select four types of medical imaging datasets involving eight benchmarks: 1) Endoscopy datasets for polyp detection: ClinicDB~\cite{cvc-clinicDB1, cvc-clinicDB2}, ColonDB~\cite{cvc-colondb}, Kvasir~\cite{kvasir}, ETIS~\cite{ETIS};
2) Microscopy dataset: BCCD~\cite{BCCD} for blood cells detection;
3) Photography dataset: ISIC-2016 for skin lesions detection (benign lesion; malignant lesion);
4) Radiology image datasets: TBX11K~\cite{TBX11k} for tuberculosis detection in lung X-rays. Detailed elaboration is given in the Appendix~\ref{app:dataset_intro}.

\textbf{Metric and baseline.} To evaluate our approach, we primarily benchmark against recent studies, mainly following Qin~\etal~\cite{qin2023medical}~(2023) and Wu~\etal~\cite{wu2023zero}~(2023).
Our baselines include recent GLIP-based methods (vanilla GLIP~\cite{GLIP}, MIU-VL~\cite{qin2023medical}, and AutoPrompter~\cite{wu2023zero}) for instance-specific prompt generation setting, and works attempt to use category-specific prompt for detection (MPT~\cite{guo2023multiple}, and its variants MPT+SoftNMS~\cite{softnms}, MPT+WBF~\cite{wbf}). For zero-shot enhancement experiments, fully supervised detection models (RetinaNet~\cite{2020RetinaNet} and DyHead~\cite{2021Dyhead}) are also included to provide a comprehensive evaluation landscape for our zero-shot enhancement experiments. The training details of the GLIP are elaborated in Appendix~\ref{app:training_details}.




\subsection{Results of zero-shot medical detection}

\label{sec:exp_zsd}




\begin{table}[t]
    \centering
    \setlength{\tabcolsep}{0.3pt}  % 调整列间距，确保对齐
    \renewcommand{\arraystretch}{1}  % 设置行间距为1.5倍
    \caption{Comparative experiment results on zero-shot medical detection across seven datasets, where 
    \colorbox{lightgray!30}{\textbf{gray-shaded rows}} represent the instance-level prompt results, while the unshaded rows represent the category-level prompt results.
}
    \begin{tabular}{lccccccccccccccccc}  % l 表示左对齐，调整列的数量以适应数据
        \toprule
             Methods & \multicolumn{2}{c}{CVC-300} & \multicolumn{2}{c}{Kvasir} & \multicolumn{2}{c}{ColonDB} & \multicolumn{2}{c}{ClinicDB} & \multicolumn{2}{c}{ETIS} & \multicolumn{2}{c}{ISIC 2016} & \multicolumn{2}{c}{BCCD} & \multicolumn{2}{c}{Avg.} \\ 
             & AP & AP50 & AP & AP50 & AP & AP50 & AP & AP50 & AP & AP50 & AP & AP50 & AP & AP50 & AP & AP50 \\ \midrule
             
             \rowcolor{lightgray!30}  % 设置背景颜色，instance-specific prompt
             GLIP & 29.8 & 37.9 & 25.9 & 33.6 & 21.7 & 32.4 & 22.1 & 29.6 & 6.7 & 9.7 & 10.5 & 20.0 & 8.9 & 18.4 & 17.9 & 25.9 \\ 
             \rowcolor{lightgray!30} 
             MIU-VL & 36.5 & 66.6 & 28.7 & 36.6 & 19.8 & 35.6 & 28.2 & \textbf{40.6} & 9.4 & 15.4 & 21.7 & 35.7 & 11.4 & 20.4 & 22.2 & 35.8 \\ 
             \rowcolor{lightgray!30} 
             AutoPrompter & 52.7 & 70.6 & 30.4 & 39.7 & 31.9 & 45.9 & 22.0 & 30.6 & 17.7 & 26.5 & 19.9 & 32.9 & 12.9 & 22.3 & 26.7 & 38.3 \\
             \rowcolor{lightgray!30} 
             Ours (instance) & \textbf{54.3} & \textbf{72.8} & \textbf{34.7} & \textbf{43.1} & \textbf{35.3} & \textbf{51.3} & \textbf{28.6} & 38.2 & \textbf{22.2} & \textbf{31.9} & \textbf{27.7} & \textbf{40.8} & \textbf{13.5} & \textbf{24.1} & \textbf{30.9} & \textbf{43.1} \\ \midrule
    
             MPT \textit{w.} WBF & 3.27 & 9.40 & 12.2 & 14.4 & 14.2 & 19.1 & 11.2 & 14.0 & 12.0 & 17.0 & 1.13 & 5.37 & 1.22 & 4.75 & 7.8 & 12.0 \\
             MPT \textit{w.} Cluster & 36.7 & 47.5 & 12.0 & 17.0 & 11.9 & 21.4 & 11.2 & 14.0 & 12.0 & 17.0 & 19.8 & 30.9 & 14.3 & 33.8 & 16.8 & 25.9 \\
             Ours (category) & \textbf{63.9} & \textbf{89.8} & \textbf{42.0} & \textbf{50.5} & \textbf{42.1} & \textbf{66.0} & \textbf{42.0} & \textbf{57.0} & \textbf{30.4} & \textbf{40.3} & \textbf{21.8} & \textbf{33.5} & \textbf{23.6} & \textbf{40.9} & \textbf{37.9} & \textbf{54.0} \\ \bottomrule
    \end{tabular}

    \vspace{-10pt}
    \label{tab:zero_shot_detection_results}
\end{table}


 

\textbf{Superior transfer performance across various medical scenarios.} \textit{For fairness, we ensured that all methods used consistent prompts for a fair comparison.} For instance-specific prompt methods, we utilized BLIP~\citep{li2022blip} as the vision-question answering model for all approaches, except for vanilla GLIP~\citep{GLIP}, which directly used the target name as text input. For MIU-VL~\citep{qin2023medical}, we additionally used PubMedBert~\cite{gu2021domain} to generate prompts specific to the target disease. AutoPrompter~\citep{wu2023zero} uses GLIP to produce the initial bounding box with instance-specific prompt and refine them with a self-training process with Yolo-X~\citep{zheng2021yolox}. The experimental results are shown in Tab.~\ref{tab:zero_shot_detection_results} (instance-specific prompt), where all prompt methods enhance the original GLIP model's performance by providing additional descriptions. Among them, \ours~achieved the greatest improvement, with an average +4.2\% AP $\uparrow$, +4.8\% AP50 $\uparrow$ across seven datasets. We do not exhibit the results for the radiology dataset TBX-11k here, as the initial performance of the GLIP model on this dataset was poor, and the performance improvement for each prompt method is not distinguishable. 

\textbf{Knowledge bank facilitates category-level prompts.} In this experiment, we focus on the effectiveness of category-level prompts generated by BLIP and GPT-4, which expand attributes related to the target across different dimensions such as colors, shapes, textures, and locations. These category-level prompts, being about 10 times longer than instance-specific prompts, remain consistent across all instances within the same class, and their details are provided in Appendix~\ref{app:detailed category-level prompt}. To benchmark against other methods, we include the MPT~\citep{guo2023multiple} approach, which is built upon the GLIP backbone and designed specifically to handle category-level prompts. MPT employs different prompt ensemble strategies, such as Weighted Box Fusion (WBF) and clustering, to split the category prompts into multiple groups and fuse the outputs for improved performance. Tab.~\ref{tab:zero_shot_detection_results} shows the performance comparison under different ensemble strategies.
As seen in the results, StructuralGLIP achieves superior average performance across seven datasets compared to MPT. More importantly, StructuralGLIP consistently outperforms instance-specific prompt methods when utilizing category-level prompts (a +7\% average AP $\uparrow$ across seven datasets). This suggests that StructuralGLIP can effectively harness the richer and more comprehensive information encoded in the category prompts.

We attribute this advantage to the dual-branch architecture of StructuralGLIP, where the prompts and image features are separated into an auxiliary and main branch, respectively. By introducing an instance-wise selection mechanism, StructuralGLIP can dynamically select the most relevant parts of the category prompt based on the input image. To further verify this, we directly feed the category prompt for GLIP to obtain GLIP's performance under the category prompt and follow MIU-VL to obtain its performance under the instance prompt, As shown in Tab.~\ref{tab:category_instance_prompt_comparison}. The results demonstrate a significant improvement (average AP of 24.8 $\rightarrow$ 49.3) in StructuralGLIP's performance compared to the vanilla GLIP with category-level prompts. 
Interestingly, by comparing the performance of GLIP between using category-level prompt (see Tab.~\ref{tab:category_instance_prompt_comparison}) and instance-level prompt (see Tab.~\ref{tab:zero_shot_detection_results}), vanilla GLIP exhibit performance degradation when category prompts are employed (average AP of 25.7 $\rightarrow$ 24.8). In contrast, StructuralGLIP shows a significant AP improvement (39.2 $\rightarrow$ 49.3). This highlights the advantage of StructuralGLIP's knowledge modeling and its ability to dynamically extract the most relevant prompt information for each instance, effectively leveraging the comprehensive knowledge provided by category-level prompts.





\begin{table}[t]
    \centering
    \setlength{\tabcolsep}{1.3pt}  % 调整列之间的间距
    \renewcommand{\arraystretch}{1.2}  % 调整行间距
    \begin{minipage}[t]{0.45\linewidth}  % 将宽度调整到0.48\linewidth
        \centering
    \caption{AP\% of vanilla GLIP and the proposed methods with instance-specific (I) and category-specific (C) prompt under zero-shot detection setting.\\}
    \begin{tabular}{lccccccccccc}
        \toprule
        \multirow{2}{*}{\textbf{}} & \multicolumn{2}{c}{{CVC-300}} & \multicolumn{2}{c}{{ClinicDB}} & \multicolumn{2}{c}{{Kvasir}}  & \multicolumn{2}{c}{{Avg.}} \\
        \textbf{} & {C} & {I} & {C} & {I} & {C} & {I}  & {C} & {I}\\
        \midrule
        GLIP & 34.3 & 29.8 & 17.9 & 22.1 & 22.3 & 25.9  & 24.8 & 25.9\\
        ours & \textbf{63.9} & \textbf{54.3} & \textbf{42.0} & \textbf{28.6} & \textbf{42.0} & \textbf{34.9} & \textbf{48.3} & \textbf{39.2} \\
        \bottomrule
    \end{tabular}
    \label{tab:category_instance_prompt_comparison}
    \end{minipage}%
    \hfill
    \begin{minipage}[t]{0.52\linewidth}  % 将宽度调整到
        \centering
        \caption{AP\% of fine-tuned GLIP and the proposed methods with instance-specific (I) and category-specific (C) prompt under zero-shot enhancement setting.\\}
    \begin{tabular}{lccccccccccc}
        \toprule
        \multirow{2}{*}{\textbf{}} & \multicolumn{2}{c}{{CVC-300}} & \multicolumn{2}{c}{{ClinicDB}} & \multicolumn{2}{c}{{Kvasir}}  & \multicolumn{2}{c}{{Avg.}} \\
        \textbf{} & {C} & {I} & {C} & {I} & {C} & {I} & {C} & {I}\\
        \midrule
        GLIP & 70.0 & 67.5 & 54.3 & 63.0 & 44.5 & 51.1&56.2 & 60.5  \\
        ours & \textbf{77.2} & \textbf{74.9} & \textbf{70.4} & \textbf{68.4} & \textbf{71.3} & \textbf{69.6} & \textbf{72.9} & \textbf{70.9} \\
        \bottomrule
    \end{tabular}
    \label{tab:ablation_category_prompt_finetuned}

        
    \end{minipage}
    \vspace{-15pt}
\end{table}

\subsection{Results of zero-shot enhancement for medical detection}\label{sec:exp_zse}

\begin{table}[t]
    \centering
    \renewcommand{\arraystretch}{1}  % 设置行间距为1.5倍
    \setlength{\tabcolsep}{0.3pt}  % Adjust column spacing
    \caption{Comparative zero-shot enhancement experiment results across datasets, s, where 
    \colorbox{lightgray!30}{\textbf{gray-shaded rows}} represent the instance-level prompt results, while the last unshaded block represents the category-level prompt results.}
    \begin{tabular}{lcccccccccccccccc}  
    \toprule
    Methods & \multicolumn{2}{c}{Kvasir} & \multicolumn{2}{c}{ColonDB} & \multicolumn{2}{c}{ClinicDB} & \multicolumn{2}{c}{ETIS} & \multicolumn{2}{c}{CVC-300} & \multicolumn{2}{c}{ISIC 2016} & \multicolumn{2}{c}{BCCD} &    \multicolumn{2}{c}{TBX-11k}
    
     \\ 
    & AP & AP50 & AP & AP50 & AP & AP50 & AP & AP50 & AP & AP50 & AP & AP50 & AP & AP50 & AP & AP50  \\ \midrule
    FasterRCNN & 63.4 & - & 44.1 & - & 71.6  & - & 44.5  & - & 59.4 & - & 50.3 & - & 56.9 & - & 33.9 & 73.9  \\
    RetinaNet & 64.1 & - & 49.8 & - & 71.9 & - & 46.6 & - & 61.6 & - & \textbf{54.0} & - & 56.7 & - &37.0 & 77.9 \\
    \midrule
    
    \rowcolor{lightgray!30} 
    GLIP & 64.8 & 82.2 & 56.8 & 79.1 & 65.1 & 82.6 & 60.4 & 77.0 & 75.2 & 95.9 & 39.9 & 50.9 & 55.4 & 78.2 & 35.2 & 75.3 \\ 
    
    \rowcolor{lightgray!30} 
    MIU-VL & 67.7 & 86.2 & 48.8 & 75.2 & 63.0 & 82.6 & 48.9 & 68.8 & 67.5 & \textbf{97.2} & 29.7 & 38.7 & 44.5 & 58.9  & 35.5& 76.7  \\
    
    \rowcolor{lightgray!30}
    AutoPrompter & \textbf{70.0} & 87.5 & 57.8 & \textbf{81.3} & 67.5 & 85.3 & 59.6 & 76.8 & \textbf{75.2} & 97.1 & 37.3 & 49.0 & 23.4 & 33.2 & 35.7 & 76.5  \\
    
    \rowcolor{lightgray!30}
    Ours (instance) & 69.6 & \textbf{87.9} & \textbf{58.1} & 81.0 & \textbf{68.4} & \textbf{87.5} & \textbf{60.3} & \textbf{77.0} & 74.9 & 96.3 & {49.5} & \textbf{62.7} & \textbf{56.9} & \textbf{80.2} & \textbf{37.3} & \textbf{78.2}  \\ \midrule
    
    MPT+Cluster & 25.1 & 30.0 & 22.3 & 29.5 & 24.8 & 29.3 & 24.7 & 29.8 & 33.4 & 41.5 & 25.6 & 33.7 & 22.8 & 30.6 & 31.4 & 68.2  \\
    
    Ours (category) & \textbf{71.3} & \textbf{89.0} & \textbf{62.0} & \textbf{85.3} & \textbf{70.4} & \textbf{88.2} & \textbf{62.4} & \textbf{79.5} & \textbf{77.2} & \textbf{96.5} & {45.9} & \textbf{58.3} & \textbf{57.8} & \textbf{82.4} & \textbf{37.8} & \textbf{79.2} \\ \bottomrule
    \end{tabular}
    \label{tab:zero_shot_enhancement_results}
    \vspace{-15pt}
\end{table}




\textbf{StructuralGLIP surpasses the fully-supervised methods.}  In this experiment, we evaluate zero-shot enhancement and also compare fine-tuned GLIP-based models with classic object detection models, such as FasterRCNN~\cite{2015fasterRCNN} and RetinaNet~\cite{2020RetinaNet}, which were fully supervised. As shown in Tab.~\ref{tab:zero_shot_enhancement_results}, while the refined GLIP performs similarly to the supervised RetinaNet (55.2 \textit{vs.} 56.6 average AP), incorporating instance-level prompts with StructuralGLIP raises the performance to 59.3 AP, a noTab. +2.7\% improvement. For category-level prompts, StructuralGLIP achieves an average AP of 60.6, showing a slight improvement over instance-level prompts. However, \textit{given that category-level prompts remain fixed across all images of the same class and can be pre-encoded in our auxiliary branch}, this performance boost comes with only the inference cost for calculating the attention matrix of prompt, further demonstrating the efficiency of our approach.

\textbf{StructuralGLIP facilitates further improvement on fine-tuned models.} Interestingly, we observe that not all prompt-based methods effectively enhance a fine-tuned GLIP model. As shown in Tab.~\ref{tab:zero_shot_enhancement_results}, methods like MIU-VL and AutoPrompter experience performance degradation when applied to the refined GLIP (MIU-VL: 56.6 $\rightarrow$ 50.7 AP, AutoPrompter: 56.6 $\rightarrow$ 53.3 AP). This decline likely occurs because these methods treat prompts as simple contextual information for the target name. During fine-tuning, only the target name is used as the linguistic input, causing a significant distribution shift when prompts are introduced during inference.  In contrast, \ours~ encodes prompts into a latent knowledge bank via the auxiliary branch, where prompts are used to construct structural representations during vision-language fusion. However, the final RPN inference still relies on the target name representation. In this way, StructuralGLIP incorporates additional knowledge about the target and alleviates the distribution shifting problem at the same time. This approach allows StructuralGLIP to achieve further performance gains on fine-tuned GLIP (56.6 $\rightarrow$ 59.3 AP).




\subsection{Ablation and analysis}
\label{sec:structural_represent}
\begin{table}[t]
    \centering
    \setlength{\tabcolsep}{1.3pt}  % 调整列之间的间距
    \renewcommand{\arraystretch}{1.2}  % 调整行间距
    \begin{minipage}[t]{0.45\linewidth}  % 将宽度调整到0.45\linewidth
        \centering
        \label{tab:ablation_PQ}
        \caption{Ablation on Top-Q (y-axis) and Top-P (x-axis) with CVC-300 dataset (AP) under zero-shot medical detection setting (instance-level).}
        \begin{tabular}{c|ccccc}
            \toprule
            \textbf{Top-Q $\downarrow$ Top-P $\rightarrow$} & \textbf{1} & \textbf{5} & \textbf{10} & \textbf{15} & \textbf{20} \\
            \toprule
            \textbf{5} & 15.1 & 50.1 & 53.4 & 52.4 & 51.9 \\
            \textbf{10} & {19.9} & 47.1 & 56.5 & 55.9 & 55.3 \\
            \textbf{15} & {19.9} & 47.1 & 56.0 & 55.4 & 54.9 \\
            \textbf{20} & {17.9} & 48.2 & 55.2 & 54.9 & 54.9 \\
            \bottomrule
        \end{tabular}
    \end{minipage}%
    \hfill
    \begin{minipage}[t]{0.5\linewidth}  % 将宽度调整到0.45\linewidth
        \centering
        \setlength{\tabcolsep}{0.7pt}  % 调整列之间的间距
        \label{tab:ablation_category_prompt_methods}
        \caption{Ablation results (AP\%) on the generation of category prompt using VQA and GPT.}
        \vspace{20pt}  % Adding space before table content
        \begin{tabular}{lcccc}
            \toprule
            \textbf{Methods} & {Kvasir} & {ColonDB} & {ClinicDB} & {ETIS} \\
            \midrule
            MPT+VQA+GPT & 12.2 & 14.2 & 11.2 & 12.0 \\
            Ours+VQA & 37.6 & 38.9 & 38.8 & 26.3 \\
            Ours+VQA+GPT & \textbf{42.0} & \textbf{42.1} & \textbf{42.0} & \textbf{30.4} \\
            \bottomrule
        \end{tabular}
    \end{minipage}
    \vspace{-10pt}
\end{table}






\textbf{Prompt as Knowledge Bank.} StructuralGLIP uses a dual-branch architecture and mutual selection mechanism to encode prompts into a latent knowledge bank, effectively supporting category-level prompts with rich attribute knowledge. To validate this, we directly feed category-level prompts of StructuralGLIP and instance-level prompt of MIU-VL for a vanilla GLIP to gain its performance with category-level prompt and instance-level prompt, respectively. As shown in Tab.~\ref{tab:category_instance_prompt_comparison}, when employing category-level prompt, GLIP suffers a performance degradation  (25.9$\rightarrow$24.8) while StructuralGLIP gains additional performance improvement (39.2$\rightarrow$48.3). This indicates that mutual selection helps StructuralGLIP effectively leverage category prompts by selecting the most relevant information for each image. \textbf{Besides}, another important advantage of embedding prompts as a knowledge bank is that this design enables the precise integration of additional knowledge without affecting the distribution of the target representation. To validate this, we conducted ablation studies on the fine-tuned GLIP model, where only the target name is used during the training phase. Then, we evaluate the performance of GLIP and \ours~ under a zero-shot enhancement setting. We present the experimental result in Tab.~\ref{tab:ablation_category_prompt_finetuned}. Similar to the analysis in Tab.~\ref{tab:category_instance_prompt_comparison}, the proposed \ours~ effectively incorporates the knowledge from the bank without a performance degradation. As discussed in Sec.~\ref{sec:exp_zse}, with dual-branch architecture, the knowledge bank functions as a residual feature in thee modality fusing phase, which prevents the distribution shift by encoding prompts separately from the target representation, ensuring smooth integration of prompt knowledge during inference.


\begin{figure}[t]
    \centering
    \label{fig:ablation-layer}
    \includegraphics[width=\linewidth]{layer-ablation.pdf}
    \vspace{-20pt}
    \caption{Ablation towards the fusing layer of the proposed method}
    \vspace{-20pt}
\end{figure}

\textbf{Category-prompt generation methods.} To validate the effectiveness of incorporating the prompt generated with the large language model, we exhibit the comparison of only using the VQA model and combing the results of VQA and GPT without fine-tuning the GLIP model in Tab.~\ref{tab:ablation_category_prompt_methods}. Our experimental results show that GPT can provide more comprehensive knowledge about the target and further improve the performance.

 

\textbf{Ablation on the fusing layer.} 
We performed an ablation study to analyze how the layer at which the latent knowledge bank is fused impacts the performance. Fig.~\ref{fig:ablation-layer} illustrates two fusion strategies. \textbf{The first strategy}, shown in Fig.~\ref{fig:ablation-layer}(a), explores the effect of fusing at specific layers. The results indicate that fusion at Layer 5 yields the highest performance across multiple datasets, suggesting that this layer contains the most relevant features for effectively incorporating prompt knowledge. In contrast, earlier layers such as Layer 1 and Layer 2 exhibit lower performance, likely due to their focus on low-level features that are less compatible with the semantic richness of the prompts. \textbf{The {second} strategy}, depicted in Fig.~\ref{fig:ablation-layer}(b), investigates the effect of starting from a specific layer and fusing through to the last layer (Layer 6). The results reveal a hierarchical pattern (Fusing Layer4- Layer6>Layer5-Layer6>Layer6), where starting fusion from Layer 4 and continuing to Layer 6 achieves the best results. This indicates a progressively integrating the prompt knowledge at deeper layers allows the model to better utilize the information from the knowledge bank, rather than directly fusing at the last layer. A further analysis of this hierarchy characteristic is conducted in Appendix~\ref{app:structural-representation-analysis}, and more insights into the improvement of \ours~ are shown in Appendix~\ref{app:attn-distribution}.


\textbf{Ablation on $Q$ and$P$.} We explore the joint effects of hyper-parameters of the selected visual tokens and prompt numbers $Q$ and $P$ on CVC-300 under zero-shot detection without the fine-tuned model. As shown in Tab.~\ref{tab:ablation_PQ}, the model achieves the optimal performance with \(P = 10\) and \(Q = 10\). Further increasing \(P\) or \(Q\) introduces redundant information and degrades performance. 

\textbf{More important analysis} towards the category-specific prompt, selection mechanism, the effect of the selected LLM to generate prompts are attached in Appendix~\ref{app:more-analysis}.



\section{Conclusion and Limitation}
\vspace{-10pt}
We introduced StructuralGLIP, a novel zero-shot medical detection model that achieves fine-grained alignment between target descriptions and medical images. Unlike prior works that directly transfer vision-language models to the medical domain, we extended zero-shot medical detection to a more practical setting by exploring both category-level prompts and zero-shot enhancement. Through extensive experiments, we demonstrated that StructuralGLIP excels under these conditions, significantly outperforming existing methods. In future work, we aim to extend the applicability of StructuralGLIP to more diverse medical and non-medical domains, potentially improving its adaptability to varied visual conditions and more complex multimodal tasks.



\section{Acknowledge}
The work was supported by the National Key Research and Development Program of China (Grant No. 2023YFC3306401), and the Beijing Natural Science Foundation (No. L244043), and the National Natural Science Foundation of
China (No. 62406298); This work was supported by the Analytical Center for the Government of the Russian Federation (agreement identifier 000000D730324P540002, grant No 70-2023-001320 dated 27.12.2023).

\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\appendix

\input{appendix}



\end{document}











