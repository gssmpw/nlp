\section{Related Work}
\textbf{Zero-shot medical detection} aims to identify and locate pathology concepts in medical images without relying on annotated data from the target domain~\citep{vilouras2024zero, qin2023medical, paul2021generalized, mahapatra2021medical, 2020ssns, 2022SOP}. Classical strategies include cross-domain generalization~\citep{adaptzero,capellan2024zero} and unsupervised learning~\citep{2020ssns,2022SOP,paul2021generalized}. Cross-domain generalization utilizes data from related domains under varied conditions, such as different imaging techniques~\citep{adaptzero} or demographic differences~\citep{capellan2024zero}, to adapt models across diverse scenarios. Unsupervised learning methods leverage side information to bypass direct supervision, such as using cell nuclei structure for image resolution analysis~\citep{2020ssns}, employing GANs with public annotations to enhance mask quality~\citep{2022SOP}, and correlating medical reports with disease features to increase detection accuracy~\citep{paul2021generalized}. However, these methods are often tightly coupled to specific data priors and exhibit a considerable performance gap compared to supervised models, limiting their clinical significance.

Recent approaches have integrated expert-level knowledge into vision-language models trained on natural images to facilitate domain transfer~\citep{liuchatgpt, lai2024carzero, tiu2022expert, wu2023medklip, zhang2023knowledge}. However, most of these efforts focus on medical classification, while the more practical and complex task of medical detection remains underexplored. For example,~\citep{qin2023medical} conducted a comprehensive study on medical detection using prompts generated by a medically-enhanced language model, PubMedBERT~\citep{gu2021domain}. Follow-up studies~\citep{wu2023zero, lu2023visual, phan2024decomposing} employed BLIP~\citep{li2022blip} to generate image-specific linguistic attributes, or used GPT~\citep{achiam2023gpt} to detail target concepts with nuanced descriptions. Recent work~\citep{guo2023multiple} further advanced this approach by introducing an ensemble strategy for fusing multiple prompts to improve detection accuracy. However, these methods require unique prompts for each instance, significantly reducing efficiency. Our method, \ours, addresses these challenges by introducing a vision-language model that leverages a knowledge bank to store a wide range of prompts, enabling instance-dynamic prompt selection in the latent feature space.




\textbf{Knowledge-bank-based prompt method} is initially developed for continual learning, which utilizes a prompt pool designed to enhance cross-domain generalization~\citep{2022L2P, wang2022dualprompt, 2023codaprompt, 2023attriclip, du2022learning}. 
Previous works~\citep{2022L2P, wang2022dualprompt} select top-$k$ prompts aligned with input image features, facilitating domain-specific modeling.
Recent advances have evolved this strategy, replacing the top-$k$ prompt selection
with a more flexible continuous prompt fusion strategy~\citep{2023codaprompt}, exploring its potential for vision-language model~\citep{2023attriclip}, and expanding applications to open-vocabulary detection tasks~\citep{du2022learning}.
{However, these methods typically require an additional training phase and are restricted to prompt retrieval in the input layer. In contrast,~\ours~explores a linguistically accessible avenue by directly utilizing the attributes predefined by the generative models and embeds these attribute prompts into a hierarchy knowledge bank situated within an auxiliary branch to achieve a layer-wise selection process.
}