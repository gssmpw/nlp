\clearpage
\appendix

\renewcommand\thefigure{\Alph{figure}}
\renewcommand\thetable{\Alph{table}}
\setcounter{figure}{0}
\setcounter{table}{0}

\section{More Important Analysis and Ablation.}\label{app:more-analysis}


\textbf{Can category-specific prompts be applied to all methods?} We compare the performance of several methods when category-specific prompts, generated by BLIP and GPT4, are used. Tab.~\ref{tab:category-prompt-diff-methods} presents the AP and AP@50 metrics across multiple datasets, including CVC300, ClinicDB, and Kvasir.

\begin{table}[ht]
\centering
\caption{AP and AP@50 Performance comparison of different methods when applied category-level prompt (BLIP + GPT4).}
\label{tab:category-prompt-diff-methods}
\begin{tabular}{cccc}
\toprule
{Method} & {CVC300 AP \& AP@50} & {ClinicDB AP \& AP@50} & {Kvasir AP \& AP@50} \\ \toprule
{StructuralGLIP} & 63.9, 89.8 & 42.0, 57.0 & 42.0, 50.5 \\
{MIU-VL} & 34.3, 53.2 & 17.9, 26.5 & 22.3, 39.4 \\
{AutoPrompter } & 30.9, 37.4 & 15.8, 31.3 & 17.5, 26.2 \\
\bottomrule
\end{tabular}
\end{table}

The results presented in Tab.~\ref{tab:category-prompt-diff-methods} clearly demonstrate the superior performance of StructuralGLIP when applied with category-specific prompts, especially in terms of both AP and AP@50 metrics. StructuralGLIP achieves significantly higher detection performance across all datasets compared to MIU-VL and AutoPrompter. For instance, on the CVC300 dataset, StructuralGLIP reaches an AP@50 of 89.8, which is nearly 36\% higher than MIU-VL and more than 50\% higher than AutoPrompter.

Additionally, StructuralGLIP's performance on the ClinicDB and Kvasir datasets also outperforms the other methods by a substantial margin. These findings suggest that StructuralGLIP is particularly effective in adapting to category-specific prompts and achieving high-quality results across a variety of medical image datasets. On the other hand, both MIU-VL and AutoPrompter show much lower performance, particularly in the AP@50 metric, indicating that their methods do not leverage category-specific prompts as effectively as StructuralGLIP. This underperformance highlights the robustness of StructuralGLIP's approach in utilizing detailed prompts to enhance model accuracy.

In conclusion, these results further reinforce the importance of StructuralGLIP's capability to utilize category-specific prompts effectively, making it the method of choice for high-performance detection tasks in medical imaging.


\textbf{Can the selection mechanism help against the noisy knowledge?} In practice, we can not guarantee that the used prompt is precise and clean. Therefore, how well a method can perform on a noisy prompt is very important. To evaluate the robustness of the mutual selection process, we conducted additional experiments on the BCCD dataset, which includes red blood cells, white blood cells, and platelets. In these experiments, we introduced noisy knowledge by mixing attributes from unrelated categories and tested the performance of StructuralGLIP. The results are shown in Tab.~\ref{tab:noisy-knowledge-analysis}. In this experiment, "(X, X)" denotes using prompts solely for category X during detection of category X, while "(X, Y)" indicates the introduction of prompts from category Y when detecting category X.


\begin{table}[ht]
\centering
\caption{StructuralGLIP's AP@50 performance with noisy knowledge under zero-shot detection setup on BCCD datasets.}
\label{tab:noisy-knowledge-analysis}
\begin{tabular}{|c|c|c|c|}
\hline
{Model} & \textbf{/} & \textbf{Red Blood Cells} & \textbf{White Blood Cells} \\
\hline
\makecell{StructuralGLIP} & \textbf{Red Blood Cells} & 32.7 & 32.4 \\
& \textbf{White Blood Cells} & 60.5 & 61.0 \\
\hline
\makecell{GLIP} & \textbf{Red Blood Cells} & 21.1 & 15.6 \\
& \textbf{White Blood Cells} & 28.3 & 38.7 \\
\hline
\end{tabular}
\end{table}

The results show that GLIP suffers significant performance degradation when noisy knowledge is introduced, while StructuralGLIP maintains high accuracy. This demonstrates the robustness of the mutual-selection mechanism, which effectively filters out irrelevant information and selects the most relevant prompts for the task at hand.


\textbf{How to assess the quality of the generated prompt?} We evaluate the quality of prompts using CLIP-Score, which measures the cosine similarity between the embeddings of cropped regions (e.g., disease regions) and their corresponding prompts. This evaluation was extended to compare the prompts generated by different multi-modal vision-language models (MLLMs), such as BLIP, LLaVa-7b, and Qwen2-VL-7b, where the MLLMs are used as Vision Question Answering (VQA) models. The results are summarized in Tab.~\ref{tab:clipscore-prompt-tbx11k} \& ~\ref{tab:clipscore-prompt-cvc300}, comparing both instance-level and category-level prompts.

\begin{table}[ht]
\centering
\caption{CLIP-Score and detection performance under zero-shot enhancement setting on TBX-11k.}
\label{tab:clipscore-prompt-tbx11k}
\begin{tabular}{cccc}\toprule
{Model} & {BLIP} & {LLaVa-7b} & {Qwen2-VL-7b}  \\
\toprule
{Instance-level prompt's CLIP-Score} & 0.284 & 0.290 & 0.294 \\
{Instance-level AP@50} & 0.780 & 0.783 & 0.792 \\

{Category-level prompt's CLIP-Score} & 0.264 & 0.262 & 0.288 \\
{Category-level AP@50} & 0.782 & 0.765 & 0.790 \\
\bottomrule
\end{tabular}
\end{table}
\begin{table}[ht]
\centering
\caption{CLIP-Score and detection performance under zero-shot setting on CVC-300.}
\label{tab:clipscore-prompt-cvc300}
\begin{tabular}{ccc}
\toprule
{VQA Model} & {Instance-level CLIP-Score} & {Instance-level AP@50} \\
\toprule
{BLIP} & 0.259 & 72.8 \\
{LLaVa-7b} & 0.272 & 73.9 \\
{Qwen2-VL-7b} & 0.270 & 74.3 \\
\bottomrule
\end{tabular}
\end{table}

The results show a clear positive correlation between CLIP-Score values and detection performance (AP@50). This validates the effectiveness of CLIP-Score as a reliable metric for evaluating the quality of prompts. The analysis underscores the utility of CLIP-Score as an additional evaluation metric for VQA-generated prompts.


\textbf{Ablation on the choice of LLMs for Category Prompts.} To assess the impact of different LLMs on performance, we evaluated StructuralGLIP using category-level prompts generated by GPT-4, LLaVa-7b, and Qwen2-VL-7b. The results are summarized in Tab.~\ref{tab:abl-prompt-model}, comparing their performance across multiple datasets.

\begin{table}[ht]
\centering
\caption{Ablation on the choice of LLMs when generating category prompts for zero-shot enhancement (AP@50).}
\label{tab:abl-prompt-model}
\begin{tabular}{ccc}
\toprule
{LLM Model} & {TBX-11k} & {CVC-300} \\
\toprule
{GPT-4} & 79.2 & 96.5 \\
{LLaVa-7b} & 76.5 & 73.9 \\
{Qwen2-VL-7b} & 79.0 & 89.0 \\
\bottomrule
\end{tabular}
\end{table}

The experimental results indicate that the choice of LLM significantly influences the final performance. In contrast, as shown in Tab.~\ref{tab:clipscore-prompt-tbx11k}, the selection of different VQA models has relatively minimal impact on the quality of generated prompts. This is because the LLM-based prompt expansion process primarily relies on the model's internal knowledge and memory of the appearance attributes related to the target lesion, rather than utilizing example images.

Therefore, when using LLMs for prompt expansion, selecting high-performing models is crucial to ensure the generation of reliable prompts. Additionally, for detecting rare diseases, prompts generated through LLM expansion may not be applicable, as these models might lack sufficient domain knowledge of less common conditions. This experiment provides valuable insights into the scenarios where LLM-based prompt expansion can be effectively utilized.


\textbf{Ablation on the LLM model to generate prompts.} The results in Tab.\ref{tab:clipscore-prompt-tbx11k}\&\ref{tab:abl-prompt-model} shows that the performance of StrucutralGLIP can be largely affected under different choices LLM to generate prompt. Therefore, we use Qwen2-VL-7B as the VQA model, which has the best performance in prompt generating, to generate instance-level prompts and compare the performance of StructuralGLIP with AutoPrompter. The results are shown in Tab.~\ref{tab:compare-qwen2-vl}.

\begin{table}[ht]
\centering
\caption{AP and AP@50 performance of StructuralGLIP and AutoPrompter with different VQA models for Instance-Level Prompts.}
\label{tab:compare-qwen2-vl}
\begin{tabular}{ccccc}
\toprule
\textbf{} & {Colondb} & {Kvasir} & {Etis} & {Clinicdb} \\
\toprule
{AutoPrompter AP@50} & 0.513 & 0.431 & 0.240 & 0.318 \\
{AutoPrompter AP} & 0.353 & 0.347 & 0.178 & 0.233 \\

{StructuralGLIP AP@50} & 0.549 & 0.440 & 0.288 & 0.376 \\
{StructuralGLIP AP} & 0.373 & 0.359 & 0.193 & 0.291 \\
\bottomrule
\end{tabular}
\end{table}

The results in Tab.~\ref{tab:compare-qwen2-vl} demonstrate that StructuralGLIP consistently outperforms AutoPrompter in terms of both AP@50 and AP metrics across all datasets. The results in Tab.~\ref{tab:compare-diff-vqa} demonstrate that StructuralGLIP consistently outperforms AutoPrompter with different vQA models for instance-level prompt generation.

These results confirm that the effectiveness of SturcturalGLIP can be transferred well to different quality of prompts.

\begin{table}[ht]
\centering
\caption{AP and AP@50 Performance of StructuralGLIP and AutoPrompter with different VQA Model for Instance-Level Prompts.}
\label{tab:compare-diff-vqa}
\begin{tabular}{ccc}
\toprule
{Method} & {CVC-300 AP@50} & {ClinicDB AP@50} \\
\toprule
{StructuralGLIP} (BLIP) & 72.8 & 38.2 \\
{StructuralGLIP} (LLaVa-7B) & 73.9 & 37.9 \\
{StructuralGLIP} (Qwen2-VL-7B) & {74.3} & {37.6} \\
\midrule
{AutoPrompter} (BLIP) & 70.6 & 30.6 \\
{AutoPrompter} (LLaVa-7B) & 70.9 & 30.9 \\
{AutoPrompter} (Qwen2-VL-7B) & 75.0 & 31.8 \\
\bottomrule
\end{tabular}
\end{table}

\section{Comparative Visualization}
~\label{sec:sample results}


We illustrate the detection results on the ColonDB and BCCD dataset in Fig.~\ref{fig:vis-bccd-polyp}, where we employ the vanilla GLIP for these prompt-based methods. Intuitively, both AutoPrompter and MIU-VL struggle with either over-detection or missing critical targets. This is likely due to the coarse alignment between vision and target representations, leading to false positives and missed detections. For example, in ColonDB, both methods produce inconsistent bounding boxes, failing to accurately localize the polyp. On the other hand, \ours demonstrates more precise localization with category-level prompts, leading to fewer missed targets and improved confidence scores.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.8\textwidth]{vis_bccd_polyp.pdf}
  \caption{Visualization Results on the ColonDB and BCCD datasets.}
  \label{fig:vis-bccd-polyp}
  \vspace{-13pt}
\end{figure}

We demonstrate example detection results on the ISIC2016 and TBX11K datasets in Fig.~\ref{fig:vis} below, where we employ the fine-tuned GLIP and AutoPrompter for comparison. As shown in Fig.~\ref{fig:vis}(a), it is evident that 
vanilla GLIP and AutoPrompter fail to produce correct classification results for lesion detection. In contrast, our method, benefiting from the category-level prompt, makes corrects classification. For radiographic datasets, our instance method achieves higher confidence scores using the same prompts with AutoPrompter.

\begin{figure}[ht]
  \centering
  \includegraphics[width=1\textwidth]{appendix2.pdf}
  \caption{Visualization Resultson the ISIC2016 and TBX11K datasets.}
  \label{fig:vis}
\end{figure}


\section{Dataset Introduction}~\label{app:dataset_intro}
We select four types of medical imaging datasets involving eight benchmarks: 

1) Endoscopy datasets for polyp detection: ClinicDB~\cite{cvc-clinicDB1, cvc-clinicDB2}, ColonDB~\cite{cvc-colondb}, Kvasir~\cite{kvasir}, ETIS~\cite{ETIS}.
There are 2,248 images and 2,374 bboxes in total.
The complete training and validation images for the entire benchmark are 1160 and 290, respectively. And the number of test set images for CVC-300, CVC-ClinicDB, CVC-ColonDB, Kvasir,
and ETIS datasets are 60, 62, 380, 100, and 196 respectively. The primary challenge involves highly variable polyp appearances, obscured views due to mucus and bleeding, and low contrast against surrounding tissues. 

2) Microscopy dataset: BCCD~\cite{BCCD} for blood cell detection
(white blood cells, red blood cells, and platelets).  The BCCD dataset is designed for blood cell detection tasks, including
three classes: white blood cells, red blood cells, and platelets. There are 874 images with 11,789
bboxes for the entire BCCD dataset.

3) Photography dataset: ISIC-2016 for skin lesions detection (benign lesion, malignant lesion). 
The ISIC-16 dataset consists of 1,279 images with
1,282 bboxes for benign skin lesions and melanoma detection, divided into 720/180/379 images for training, validation, and testing. This dataset pose difficulties due to the small size and high density of the targets, and variations in staining which affect visual clarity and consistency. 

4) Radiology image datasets: TBX11k~\cite{TBX11k} for tuberculosis detection in lung x-rays. These datasets are challenging due to the subtle nature of disease indicators, which can obscure key features. The TBX11K dataset
is used for tuberculosis detection in the lung, including 799 images and 1,211 bbox labels. Moreover,
this dataset is divided into 479/120/200 images for training, validation, and testing sets, respectively.

We demonstrate some example images of these datasets in Fig.~\ref{fig:data-vis} below. 

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{data-vis.pdf}
  \caption{Examples of medical images under different imaging conditions.}
  \label{fig:data-vis}
\end{figure}

\section{Training Details for GLIP's Zero-shot Enhancement Experiment}~\label{app:training_details}

 The zero-shot enhancement aims to further improve the performance of models after supervised training on the downstream datasets. We follow~\cite{qin2023medical} to use a fine-tuned GLIP model optimized with the Adam optimizer\cite{adam}, where the initial learning rate is set to $1 \times 10^{-4}$ ($1 \times 10^{-5}$ for the BERT text encoder). A weight decay of 0.05 is applied to prevent over-fitting, and the bottom two layers of the image encoder are frozen to preserve fundamental features. Our expressive prompts tailored to the characteristics of the target's appearance are generated using GPT-4~\cite{achiam2023gpt}. Full details of these prompts are available in the Appendix.







\section{Analysis for the hierarchical characteristic of the structural representation.}
\label{app:structural-representation-analysis}

To validate the hierarchical nature of the structural representations derived through layer-wise prompt retrieval, we analyzed the distribution of selected prompts across six GLIP model encoder layers. Specifically, we conducted this analysis using \ours~ with category-level prompts on two medical detection tasks: BCCD red blood cell detection and ClinicDB polyp detection. Prompts were categorized into four types: color, location, shape, and texture (see detailed prompt categories in Appendix~\ref{app:detailed category-level prompt}). The results, shown in Fig.~\ref{fig:strutural_analysis}, reveal distinct differences in the frequency of selected prompt types across layers (see Tab.~\ref{tab:frequence selected in validation} for the concrete value of the figure). In both tasks, color prompts were consistently selected across all layers, highlighting its importance in medical detection. For ClinicDB, the frequency of shape and texture prompts increased in deeper layers, indicating that these features become more relevant as the model abstracts more complex attributes. In contrast, for red blood cell detection, color remains the predominant feature across layers, while the selection of shape and texture prompts decreases.  This analysis demonstrates that StructuralGLIP can dynamically retrieve task-relevant prompts from the knowledge bank at different layers, confirming the adaptable nature of our zero-shot medical detection framework. 


\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth, height=4cm]{attributes-analysis.pdf}
  \caption{Results of the selected frequency (y-axis) of different types of prompts across the network's layer (x-axis).}
  \label{fig:strutural_analysis}
  \vspace{-10pt}
\end{figure}


\begin{table}[ht]
    \centering
    \setlength{\tabcolsep}{1.8pt}
    \begin{tabular}{c|cc|cc|cc|cc}\toprule
         Attribute&  \multicolumn{2}{c|}{colors}&  \multicolumn{2}{c|}{locations}&  \multicolumn{2}{c|}{shapes}&  \multicolumn{2}{c}{texture}\\
 \midrule
 Dataset& polyp& red blood cell& polyp& red blood cell& polyp& red blood cell& polyp& red blood cell\\
 \midrule
         layer1&  1436&  1192
&  183
&  233
&  770
&  474
&  467
&  233
\\
         layer2&  1468&  1073
&  473
&  17
&  690
&  562
&  572
&  149
\\
         layer3&  1475&  1109
&  158
&  122
&  702
&  499
&  223
&  136
\\
         layer4&  1505&  1160
&  436
&  133
&  1131
&  653
&  883
&  245
\\
         layer5&  1498&  1062
&  416
&  59
&  1163
&  565
&  779
&  269
\\
         layer6&  1450&  1117&  182
&  21&  904
&  433&  452
&  48\\
\midrule
    \end{tabular}
    \caption{The selected frequencies of different types of prompts across GLIP's different layers}
    \label{tab:frequence selected in validation}
\end{table}



\section{Effect of prompt quality.}

As shown in Tab.~\ref{prompt show}, we present the detailed target description (query name with prompt) for red blood cells detection task, and only using the category name ``red blood cells'' as input to the model of GLIP resulted in poor detection performance, with an AP of just 1.7\%. 
From Tab.~\ref{prompt show}, it is evident that the design of prompt methods significantly affects the ability of vision-language models to utilize prompts for domain enhancement. For instance, our method, with the simple prompt, ``pink oval'', outperforms MIU-VL, which uses multiple types of attributes, achieving a +7.4\%AP50 and +4.3\%AP improvement. This improvement is attributed to our structural representation, which achieves hierarchical vision-language alignment, thereby enhancing the utilization of prompts for medical image analysis. 
Additionally, the structural representation involves fine-grained vision-language alignment, enabling precise selection of attribute tokens from prompts. This capability allows our method to effectively incorporate more comprehensive prompts, leading to a further improvement of 5.1\%AP50 based on the simple short prompt. This demonstrates the effectiveness and robustness of our approach in generating and utilizing prompts for zero-shot detection tasks, showcasing its superiority in achieving efficient and accurate medical image analysis.




\begin{table}[ht]

    \caption{Comparison of different methods with different prompts based on the red blood detection task of BCCD dataset (complete prompts are shown in Appendix~\ref{app:detailed category-level prompt}).}
    \renewcommand{\arraystretch}{1} % 调整行间距的比例
    \centering \setlength{\tabcolsep}{1.0pt}
    \begin{tabular}{cccc}\toprule
        Methods &   Target Description (name+prompt) &  AP&  AP50\\
         \midrule
         GLIP& [\textit{name}] red blood cells&  1.7&  4.3\\
         
         MIU-VL
&  [\textit{name}]  + red color + spherical shape + in birth&  12.0&  24.7\\

          AutoPrompter
&  [\textit{name}] + pink oval &  12.6&  27.0\\

         Ours &  [\textit{name}] +  pink oval&  \textbf{16.3}&  \textbf{34.4}\\  \midrule
          \multirow{2}{*}{\shortstack{MPT+Cluste}} 
         &  [\textit{name}] +  (four prompts below)  & \multirow{2}{*}{\shortstack{12.5}} & \multirow{2}{*}{\shortstack{25.6}} \\
         & [flesh-colored, pink, round, blood] & & \\
 Ours (category) & \begin{tabular}{@{}l@{}}  [\textit{name}]+ [\textit{color}] pale bright,~\etal   + [\textit{shape}] oval round,~\etal  \\ + [\textit{texture}] smooth rough~\etal + [\textit{location}]
 peripheral central,~\etal
 \end{tabular}& \textbf{19.3}& \textbf{39.5}\\
    \bottomrule
    \end{tabular}
    \label{prompt show}
\end{table}




\section{Detailed Category-prompt for the medical datasets}\label{app:detailed category-level prompt}
% Detailed prompts involving poly and red blood cells in the analysis and ablation experiments are outlined in Table \ref{tab:prompt details}.
\begin{longtable}{p{1cm}|p{3cm}|p{3cm}|p{3cm}|p{3cm}}
\toprule
 & Colors & Shapes & Textures & Locations \\
\hline
\centering Polyp & 
\textit{white, yellow, orange, red, brown, pink, pale, tan, gray-white, gold, cream, ruby, turquoise, indigo, violet} &
\textit{octagon, circle, round, heart, oblong, oval, small, rounded, jagged, wide, large, bulbous, spherical, circular, irregular, diamond} &
\textit{smooth, textured, cracked, striped, shiny, dull, speckled, raised, rough, granular, grooved, glossy, veined, pigmented, uneven, mottled, interwoven, lines, patches, complex, reticular, structure} &
\textit{rectal, mucosal, elevated, demarcated, creased, folded, isolated, clustered, solitary, honeycombed }\\
\hline
\centering Red Blood Cells & 
\textit{pale, bright red, dark red, pinkish, crimson, ruby, coral, salmon, cherry, scarlet, rusty, maroon, wine, burgundy, rosy, flamingo, peach, copper, mahogany, terracotta} &
\textit{disc-shaped, oval, round, elongated, spherical, ring-like, bean-shaped, crescent, irregular, biconcave, elliptical, cuboidal, triangular, squamous, fusiform, polygonal, rod-shaped, fibrillar, amorphous, lobed} &
\textit{smooth, rough, granular, fibrous, glossy, matte, sticky, velvety, spongy, creased, crystalline, jelly-like, pitted, wrinkled, spiny, bumpy, flaky, mucous, papillary, striated} &
\textit{peripheral, central, upper, lower, medial, lateral, distal, proximal, anterior, posterior, cervical, thoracic, abdominal, pelvic, inguinal, axillary, oral, nasal, occipital parietal}\\
\hline
\centering White Blood Cells & 
\textit{purple, white, pink, gray, blue, translucent, lavender, milky, yellow, pale, clear, light purple, ivory, cream, faint blue, silver, off-white, light gray, opalescent} &
\textit{round, oval, irregular, lobed, segmented, spherical, kidney-shaped, amoeboid, polymorphous, triangular, elongated, bean-shaped, cuboidal, crescent, spindle-shaped, fusiform, irregularly-shaped, star-shaped, flattened, discoid} &
\textit{granular, rough, smooth, wrinkled, spongy, matte, glossy, fibrous, pitted, veined, speckled, raised, lobulated, ridged, reticular, grooved, folded, striated, flaky, nodular, uneven} &
\textit{circulating, peripheral, thoracic, abdominal, pelvic, cervical, axillary, lymphatic, spleen, marrow, mediastinal, proximal, distal, inguinal, occipital, parietal, cranial, vertebral, lumbar, sacral} \\
\hline
\centering Platelets & 
\textit{yellow, gray, pink, translucent, clear, beige, orange, white, pale yellow, light gray, light pink, golden, amber, straw, ivory, light orange, peach, tan, light brown, opalescent} &
\textit{small, round, oval, irregular, disc-shaped, spiked, star-shaped, elongated, granular, fragmented, jagged, ring-shaped, crescent, cuboidal, polygonal, fibrillar, amorphous, fusiform, spherical, irregularly-shaped} &
\textit{granular, smooth, rough, spongy, fibrous, pitted, wrinkled, matte, glossy, veined, lobulated, striated, flaky, nodular, reticular, ridged, bumpy, raised, speckled, uneven, lumpy} &
\textit{circulating, peripheral, marrow, spleen, liver, thoracic, abdominal, lymphatic, distal, proximal, cervical, axillary, cranial, vertebral, sacral, pelvic, mediastinal, inguinal, parietal, occipital} \\
\hline

\centering Benign Lesion & 
\textit{light brown, tan, pale pink, beige, ivory, light yellow, flesh-colored, clear, translucent, white, pink, light red, off-white, cream, soft yellow, gray, peach, faint brown, faint yellow, light orange} &
\textit{round, oval, smooth-edged, well-defined, regular, flat, slightly raised, small, lobulated, dome-shaped, circular, symmetrical, uniform, elongated, flat-topped, irregular, semi-spherical, oblong, disc-shaped, heart-shaped} &
\textit{smooth, glossy, matte, uniform, fine, clear, unbroken, even, polished, soft, thin, flat, reticular, striated, nodular, shallow, granular, homogeneous, light-textured, delicate} &
\textit{superficial, epidermal, dermal, non-invasive, isolated, peripheral, central, facial, limb, torso, scalp, back, upper, lower, anterior, posterior, lateral, abdominal, neck, arm} \\
\hline
\centering Malig-nant Lesion & 
\textit{dark brown, black, red, purple, blue, gray, deep red, maroon, dark purple, crimson, burgundy, dark gray, navy, violet, yellowish, pale gray, dark pink, reddish-brown, orange, tan} &
\textit{irregular, asymmetric, poorly-defined, multi-lobed, jagged, raised, ulcerated, irregular-edged, large, deep, multi-colored, nodular, star-shaped, rough-edged, uneven, angular, oblong, rough, distorted, fragmented} &
\textit{rough, scaly, granular, ulcerated, cracked, irregular, firm, thick, pitted, fibrous, bumpy, crusty, glossy, uneven, speckled, reticular, indurated, papillary, pigmented, veined} &
\textit{invasive, dermal, subcutaneous, nodal, systemic, spread, clustered, axial, limb, facial, scalp, back, chest, abdominal, upper, lower, lateral, posterior, anterior, proximal, distal} \\
\hline
\end{longtable}



\begin{longtable}{p{1cm}|p{3cm}|p{3cm}|p{3cm}|p{3cm}}
\hline
\centering Tuber-culosis & 
\textit{white, gray, patchy, cloudy, translucent, opaque, pale, faint, bright, shadowed, dull, smoky, hazy, diffused, misty, dense, light gray, speckled, milky, gray-white} &
\textit{irregular, nodular, patchy, lobular, diffuse, multi-focal, rounded, asymmetrical, large, small, streaked, segmented, thickened, elongated, fragmented, scattered, spotty, uneven, consolidated, granular} &
\textit{rough, fibrotic, granular, nodular, scarred, thick, textured, coarse, uneven, reticular, banded, streaked, fibrous, pitted, grooved, layered, striated, indurated, dense, veined} &
\textit{apical, upper lobe, lower lobe, central, peripheral, posterior, anterior, lateral, mediastinal, pleural, diaphragmatic, tracheal, hilum, bronchiolar, thoracic, cervical, upper, lower, rib, clavicle} \\
\bottomrule
\end{longtable}



\section{Analysis towards the feature distribution of StructuralGLIP.}
\label{app:attn-distribution}
To provide more insight into the improvement brought by StructuralGLIP, we focus on vision and language features input into the RPN detection model before and after applying our proposed approach. We conduct experiments on five datasets (cvc300, colondb, clinicdb, kvasir, etis) on the vanilla GLIP without fine-tuning and calculate the average value of vision and target representation's attention matrix (termed "average attention strength"). The prompt we used is category-level prompt. Then, we employ a kernel density estimation method to estimate the distribution of average attention strength. We found that there is a significant increase in average attention strength for the proposed StructuralGLIP compared with the vanilla GLIP, indicating better alignment between vision and language representations.



\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{attention-value-density.pdf}
  \caption{Feature distribution of the average attention strength using KDE estimation on five datasets (cvc300, colondb, clinicdb, kvasir etis). The x-axis is the value and the y-axis is the density.}
  \label{fig:attention-value-density}
  \vspace{-13pt}
\end{figure}

\section{Visualization on natural images}
\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{vis-natural-data.pdf}
  \caption{Examples of natural images with GLIP and StructuralGLIP.}
  \label{fig:natural-data-vis}
\end{figure}
Here we demonstrate the visualization of the proposed StructuralGLIP and GLIP on natural images. The prompt for each category is as follows:

\begin{enumerate}
    \item {Cosmos: broad, delicate, slightly ruffled petals radiating symmetrically around a vibrant yellow center, with a lightweight and airy appearance that contrasts beautifully against the surrounding colors}
    \item {Peony: lush, voluminous, soft, delicate, vibrant, radiant, layered, rounded, ruffled, full, graceful, elegant, eye-catching, rich, luxurious, intricate, symmetrical, silky, and captivating}
    \item {Hematite: flat and irregularly shaped, with a coarse and slightly grainy surface indicative of its iron-rich composition}
\end{enumerate}

