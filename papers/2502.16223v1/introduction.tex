\section{Introduction}
\label{sec:intro}

Zero-shot medical detection is crucial in healthcare as it enhances detection capabilities without requiring additional annotated medical images, even after model fine-tuning~\citep{badawi2024review, mahapatra2021medical, qin2023medical}. This is particularly valuable in clinical settings, where doctors often encounter new disease characteristics not previously documented. In such cases, clinicians can temporarily create custom prompts to guide the detection process, allowing models to adapt to novel scenarios more effectively. Recent studies have explored the potential of grounded language-image pre-training models (GLIP)~\citep{phan2024decomposing, tiu2022expert, li2022grounded, yao2022detclip} to reduce dependence on annotations by leveraging prior knowledge.
These models conduct detection by contrasting image features with descriptive texts, known as \textit{contextual prompts}, generated by visual question-answer models for query objects. To adapt GLIP to the medical domain, recent works~\citep{qin2023medical, wu2023zero, guo2023multiple} have employed medically enhanced question-answer models like PubMedBERT~\citep{gu2021domain} and BLIP~\citep{li2022blip} to create attribute-rich prompts. These prompts capture nuanced characteristics of query targets, improving domain adaptation and performance beyond traditional supervised training.

However, existing contextual prompt-based methods often suffer from coarse alignment between images and target descriptions, resulting in two key issues. \textbf{First}, these methods typically treat prompts as contexts that are equivalent to the target, easily causing distribution shift problems to the target's representation. Despite incorporating the prior about the target, they also introduce distracting information about the target. This leads to misalignment between the target and the actual visual cues in the image (see Fig.~\ref{fig:vis-bccd-polyp}). 
\textbf{Second}, category-level descriptions can not  be effectively encoded within the context, which often contain ambiguous vocabularies such as "tissue with pink or red color, irregular or round shape" for a "bump". This causes that the most relevant prompt can not be precisely matched with the input image.

\begin{figure}[t]
  \centering
\includegraphics[width=0.9\textwidth]{teaser.pdf}
  \caption{(a) {Contextual prompt methods directly concatenate the prompt and target. 
  (b) Our structural prompt method encodes prompts into a latent knowledge bank.}}
  \label{fig:teaser}
  \vspace{-20pt}
\end{figure}

To address the aforementioned issues, we present\ours, a novel zero-shot medical detection model that derives \textit{structural representations}, which are delicately organized sets of features specifically designed to represent the nuances of the target and the input image. Specifically, as shown in Fig.~\ref{fig:teaser}, instead of simply concatenating prompts with the target, StructuralGLIP adopts a dual-branch architecture. The main branch processes the target name and input image, while the auxiliary branch encodes the prompts into a latent knowledge bank. At each layer, rather than directly performing cross-modal fusion between vision and language features, StructuralGLIP introduces a mutual selection mechanism. This mechanism matches vision features from the main branch with relevant prompt features stored in the latent knowledge bank, where we extract latent prompt tokens and latent vision tokens that both highly relevant to the target and the current input image, forming fine-grained structural representations. Once these structural representations are formed, the image and language features from the main branch are fused with the selected prompt tokens via cross-modality multi-head attention~\citep{vaswani2017attention}. This enhances the overall feature alignment and improves the fusion process within the main branch. Conceptually, the hierarchical knowledge bank in StructuralGLIP functions like a memory system~\citep{bi2021dual, paivio2013imagery}. As the image is processed, relevant knowledge is dynamically retrieved from the bank. This enables the model to better align the image features with the prompt information, resulting in more accurate and context-aware detection.


In this way, StructuralGLIP can address the challenge of effectively utilizing category-level prompts, which provide broader yet consistent information for all instances within the same category (see Fig.~\ref{fig:vis} for visualization). StructuralGLIPâ€™s instance-wise selection mechanism ensures that even fixed category-level prompts are dynamically aligned with the specific visual features of each instance. This not only improves detection precision but also enhances efficiency, as category-level prompts can remain fixed across instances of the same category.
To validate the proposed method, we benchmark\ours~against previous state-of-the-art methods on eight datasets under endoscopy, microscopy, photography, and radiology four imaging conditions, and conduct a comprehensive analysis towards\ours's structural representations. 
The primary contributions of our work are as follows:
\begin{itemize}[leftmargin=14pt, itemsep = -2pt, topsep = 0pt] 
\item We introduce StructuralGLIP, a novel architecture that achieves adaptive, context-aware alignment between visual features and target descriptions by utilizing a dual-branch structure with mutual selection, enhancing the precision of medical object detection. 
\item We propose the use of category-level prompts, which remain fixed for all instances of the same target. Unlike instance-level prompts, category-level prompts provide more comprehensive prior knowledge about the target disease, reducing the need for prompt generation for each individual image while maintaining strong detection performance. 
\item We explore zero-shot medical detection in more practical settings by demonstrating how zero-shot enhancement can further improve the performance of models fine-tuned on medical data. StructuralGLIP not only surpasses fully supervised methods such as RetinaNet but also seamlessly integrates into GLIP models fine-tuned on medical datasets, achieving an average improvement of +4\% AP.
\end{itemize}

