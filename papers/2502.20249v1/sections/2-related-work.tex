\vspace{-1mm}
\section{Related Work}
\vspace{-1mm}
Our research pertains to 3 main aspects:
% that we review below: 
unconstrained gaze estimation, temporal gaze modeling, 
and generalization using additional data and labels to bridge the domain gap between controlled setups and real-world data.\\
%
%
\mypartitle{Unconstrained Gaze Estimation.} 
%
%As discussed in introduction, 
Most 3D gaze estimation models address the frontal face gaze prediction task 
\cite{zhang2017s,chen2018appearance,Rt-gene,cheng2020coarse, cheng2022gaze,abdelrahman2023l2cs,yan2023gaze}, 
relying on normalized frontal face crop as input. 
%
These methods tend to fail under partial occlusion of the eyes due to extreme head pose. Nevertheless, at 90-135 head pose yaw, a significant part of one eyeball is still often visible and informative for gaze estimation \cite{Gaze360}.
For this reason, few works tackle the most challenging setting of
%setting in gaze estimation, so-called 
``physically unconstrained gaze estimation" without constraint on the head pose. 
%
Kellnhofer \etal~\cite{Gaze360} are the first to collect a physically unconstrained 3D gaze dataset Gaze360 and develop a method that used head crop as input. Then, combining different head crop scales proved to be beneficial \cite{chen2020360} since more resolution helps on the frontal face while more context is beneficial for profiles and back heads. Following this idea, MCGaze \cite{guan2023end} used a spatiotemporal interaction module between head, face, and eye features in an end-to-end manner to extract local eyes and global head features. These approaches focus on within-data performance, while in this work we aim to improve both within and the generalization as discussed in the following section. \\
%
\mypartitle{Generalization in the Wild.} Bridging the dataset's domain gap challenge is crucial for 3D gaze estimation in real-world applications. 
Two trends have been explored to adapt to specific target domains effectively: One leverages few labeled samples, while the other uses only unlabeled samples~\cite{liu2021generalizing,cheng2022puregaze,wang2022contrastive,ververas20223dgazenet,Gaze360,jindal2024spatio}.
In contrast, gaze generalization models focus on enhancing cross-domain performance without any prior knowledge of the target domain. For instance, the methods proposed in \cite{cheng2022puregaze,bao2022generalizing,wang2022contrastive} demonstrate improved generalization by learning robust general features (\eg via image rotation consistency) for gaze estimation across varying conditions. Even if those methods focus on constrained settings with face crop as input, we compare our approach with them to show the effectiveness in frontal face generalization.\\
%
Furthermore, to improve in-the-wild generalization, researchers seek to exploit diverse weak gaze labels that can be easily or automatically generated on in-the-wild data. 
In this direction, Zang \etal~\cite{MPS} automatically generates a new 3D gaze dataset, MPSGaze, by blending on images of people from the  Widerface datasets~\cite{yang2016wider}, eyes from images of the  ETH-Xgaze dataset with known 3D gaze and similar head pose. 
%
While this greatly improves diversity with more than 10000 new identities, this method generates only near frontal faces and might impact the appearance of the face. 
%
In another study, Ververas \etal \cite{ververas20223dgazenet} used eyeball fitting techniques to create pseudo-3D gaze on new face datasets. They improved generalization, but their work is also restricted to frontal faces. 
Finally, Kothari \etal \cite{kothari2021weakly} used a weakly-supervised learning framework for improved generalization using pseudo 3D gaze labels from 2D gaze LAEO labeled datasets. 
%
However, as acknowledged by the authors, the 2D gaze distribution of LAEO is limited horizontally. 
In our work, we follow this idea but leverage a more diverse gaze distribution and natural scene 2D gaze label obtained from the annotation of where people look in the scene. Using a self-training learning approach with generated 3D pseudo labels via geometric projection, we show improved within and cross-dataset generalization on unconstrained Gaze360 and GFIE \cite{GFIE} and frontal MPIIFaceGaze~\cite{MPIIGaze} datasets.\\
%
\begin{figure*}[htbp]
    \centering
    \includegraphics[width=\linewidth]{images/main_approach.pdf}
    \vspace{-2mm}
    \caption{
    \textbf{Our \framework training framework.}
    1. In the first stage, we train a Gaze Transformer (\model) on both image and video 3D gaze datasets.
    %in a supervised manner. 
    2. Using the trained network, 3D gaze is inferred on 2D gaze dataset. Then, a geometric rotation is applied to generate a pseudo 3D gaze label from the inferred 3D gaze that is aligned to the 2D ground truth gaze label in the image plane.
    3. In the second stage, we train a similar gaze network as in 1. using available 3D gaze datasets and gaze following datasets with 3D pseudo labels.
    }
    \label{fig:approach}
    \vspace{-4mm}
\end{figure*}
%
\mypartitle{Dynamic 3D Gaze Estimation} 
%Dynamic gaze estimation 
has not been extensively explored due to the lack of available datasets. 
EYEDIAP~\cite{Eyediap} and EVE \cite{EVE} are video datasets collected in constrained settings, resulting in mostly frontal poses. 
In this particular context, Park \etal~\cite{EVE} and Palmero \etal~\cite{palmero2018recurrent} estimated the gaze from face crop image sequences but only showed marginal improvement compared to static methods. Indeed, it is questionable if eyeball dynamics have temporal dependencies besides the ones due to specific tasks or scenarios (\eg reading). 
%
Nevertheless, in unconstrained settings with low resolution and head pose dynamic scenarios, temporal methods show benefits in encoding the head and eye dynamics \cite{Gaze360,nonaka2022dynamic,chen2020360,guan2023end}. 
%
For instance, seen from a far distance, head and body orientation dynamic 
revealed to be an important prior for gaze estimation when eyes are barely visible \cite{nonaka2022dynamic}.
%
Unconstrained video gaze data is challenging to collect. Beyond Gaze360, GFIE \cite{GFIE} is the only other 3D gaze dataset for gaze following, using complex laser setups, yet it is limited to indoor settings and lacks natural scene and gaze dynamics. The scarcity of video 3D gaze datasets hampers the development of video-based methods that generalize to real-world data. To address this, we propose a unified model trained on both image and video datasets, demonstrating improved video prediction through diverse, large-scale data. \\
%
Furthermore, current video-based gaze estimation methods typically employ a backbone to extract features from image sequences, followed by a Recurrent Neural Network (RNN) to capture temporal dynamics \cite{Gaze360,EVE,palmero2018recurrent,jindal2024spatio,chen2020360,kothari2021weakly}. However, these approaches do not explicitly model the spatiotemporal interactions in the input sequence. To address this limitation, we investigate a spatio-temporal model to encode subtle eye motion or head pose changes in the input sequence directly. \\
