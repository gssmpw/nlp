

\input{tables/ablation_framework}
\input{tables/ablation_paradigm}
\input{tables/ablation_omni}


\vspace{-4mm}
\section{Experiments}
\vspace{-1mm}

\subsection{Datasets}

In this work, we employ five 3D gaze datasets:
two video unconstrained datasets for training and evaluation: Gaze360 (G360)~\cite{Gaze360}, GFIE~\cite{GFIE}, and three constrained only for generalization MPSGaze (MPS)~\cite{MPS}, MPIIFaceGaze (MPII)~\cite{MPIIGaze} and EYEDIAP (EDIAP)~\cite{Eyediap} (EDIAP), with only EDIAP being a video dataset. 
As shown in \cref{fig:data_distribution}, G360 and GFIE differ considerably in their gaze distribution, which makes cross-dataset evaluations challenging. 
%
In addition, we consider the 2D gaze following dataset GazeFollow \cite{recasens2015they} (GF), which contains more than 100k images with gaze target annotations.\\
%
The details of the six datasets are presented in the supplementary materials. 
%
Nevertheless, as authors have been using many subsets of G360 for evaluation, we clarify the test splits to avoid any confusion. 
We followed the split of \cite{Gaze360}: G360 Full corresponds to "All 360°" (all the test set); G360 180 corresponds to "Front 180°" (gaze within 90°); and G360 40 to "Front Facing" (gaze within 20°). Additionally, we consider G360 Back (gaze above 90°)~\cite{chen2020360} and G360 Face (all detected faces), used in many studies~\cite{zhang2017s,chen2018appearance,Rt-gene, cheng2020coarse, cheng2022gaze, abdelrahman2023l2cs, yan2023gaze, catruna2024crossgaze}. G360 Face 180 or 40 corresponds to the detected face with a gaze within 90° or 20°.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{images/gaze_distribution.png}
    \vspace{-2mm}
    \caption{\textbf{Dataset gaze distribution.} Gaze in polar coordinates.}
    \label{fig:data_distribution}
    \vspace{-4mm}
\end{figure}

\vspace{-2mm}
\subsection{Implementation Details}
\vspace{-1mm}

Each dataset has different head bounding boxes ground truth. 
To avoid discrepancies in cropping, 
we standardize the input by using a robust pre-trained head detector\footnote{https://github.com/zhangda1018/yolov5-crowdhuman} train on the CrowdHuman dataset~\cite{shao2018crowdhuman}. 
%
We match the detected and ground truth bounding boxes to get the final head crop bounding box. Furthermore, we downscale the head bounding box by 10\% and resize it to $224 \times 224$ pixels. We show the impact of the head bounding box size in the supplementary materials. An 8-frame head crop clip is used for the video modalities, and the frame rate is unified across all video datasets.
All the backbones used in this work are pre-trained on Imagnet for static backbones and ImageNet-1K, Kinetics-400, and SUN RGB-D for Swin3D. Please refer to the supplementary materials for training and data augmentation details.

\vspace{-2mm}
\subsection{Ablation Study}
\vspace{-2mm}
\mypartitle{Does gaze following label improve 3D gaze estimation?}
In \cref{tab:ablation_framework}, we evaluate various methods for learning from 2D gaze following labels (GF). We find that, with few exceptions, incorporating GF consistently improves 3D gaze estimation. This underscores the importance of broader training diversity for robust 3D gaze estimation. The specific details and advantages of each approach are discussed in the following section.\\
%
\mypartitle{Self-Training Weakly-Supervised learning framework.} In \cref{tab:ablation_framework}, we perform ablation experiments related to the learning framework. In our experiments, we train with our model \model on two 3D gaze datasets namely G360 and GFIE. The first baseline experiment is to train on a 3D gaze dataset in a standard supervised manner. Then, there are three possibilities to leverage additional 2D datasets such as GF. The first one is the Weakly-Supervised (WS) method defined by a specific loss applied only on the x, y 3D gaze prediction coordinate for GF batch samples supervised by the 2D GT label. The second approach Self-Training (ST) is similar to our \framework approach described in \cref{fig:approach} but in the second stage, 3DPred is used to supervise the training. Finally, the last approach \framework is our proposed approach described in \cref{fig:approach} and \cref{sec:self_taining}. 
%
Compared to ST, \framework achieves higher accuracy across all evaluations except when trained on G360 and tested on EDIAP. This indicates that relying solely on 3DPred lacks diversity in gaze distribution, as it mostly follows the training data distribution. By incorporating our 3DGP label, we obtain a more robust gaze vector that enhances accuracy. 
Furthermore, when compared to using only 2D labels in the WS method, \framework performs slightly better overall, particularly on the frontal EDIAP and MPII benchmarks. This suggests that datasets, where the z component is significant (e.g., frontal views), require more than 2D supervision. Overall, our method is either the best or the second best (by a small difference) demonstrating the effectiveness of our \framework learning framework. \\
%
\mypartitle{Gaze Model Network.}
In \cref{tab:ablation_model}, we present the results concerning the model architecture. In our experiments, we train on G360 in a supervised manner with different training modalities combinations ( I, V, and I\&V). We compare our model \model with baseline models (see \cref{sec:model}) that can also handle different training data modalities. First, we notice that when models are trained on both image and video, our \model model is the best model on G360. It suggests that spatiotemporal learning from the input is beneficial for gaze estimation, especially in non-frontal scenarios. Additionally, within each model, training on both modalities improves image evaluation but slightly reduces video evaluation. However, modality-specific models are limited to their own data type which limits the available training dataset. Indeed, in the next section, we show that combining modalities can result in cross-modal generalization. Given these findings, our model stands out as a reliable and versatile option for robust 3D gaze estimation.
\\
\mypartitle{Does training on additional image datasets help video generalization?} 
As observed in \cref{tab:ablation_framework}, training our \model model with \framework, which includes a diverse image dataset (GF), not only improves image generalization but, more notably, enhances generalization on videos. Compared to a supervised method, our approach improves image GFIE evaluation by 38\% and video GFIE by 34\%. A similar trend is observed when training on GFIE and evaluating on G360 Full. 
Furthermore, the modality of the training data plays an important role, as observed in \cref{tab:ablation_cross_modal}. Indeed, when training with both an image and a video dataset such as G360V+GF, at test time, evaluation on images (respectively videos) will be dominantly affected by the characteristics of the image (respectively video) training data. When we train on G3360V+GF, the results on GFIE image have a 15.7° angular error against 17.9° on video. Interestingly, when trained on G360I\&V+GF, the generalization modality gap is reduced with 15.9° on image and 15.5° on video. A Similar trend is observed when training on GFIE.  

\mypartitle{Temporal Context.} 
Temporal dynamic plays a crucial role in unconstrained gaze estimation, as evidenced in \cref{tab:ablation_framework} with our \model model trained on both modalities (I\&V).
In all configurations, video predictions consistently outperform image-based predictions.
Other important observations emerged from visual and quantitative analyses and are discussed in the supplementary material.

\input{tables/domain_generalization_cvpr}
\input{tables/Gaze360_within_constrained}
\vspace{-1mm}
\subsection{Comparison with State-of-the-art (SOTA).}
\vspace{-1mm}
\mypartitle{Within-Dataset Experiments.}
%
In the following, we focus on within-dataset experiments.
In \cref{tab:main_results}, we compare our results with the state-of-the-art methods on G360 and GFIE. We report results using our model \model in a supervised manner and using our \framework learning framework. Our model trained in a supervised manner is SOTA on image G360 Full and GFIE with 2\% and 13\% relative improvement, respectively. On video inference, MSA+Seq is slightly better (12.6° vs 12.5° ours) since it uses an average of multiple input scales.
More importantly, when trained with gaze following labels like GF using our \framework learning framework, we outperform all the SOTA on image and video by 5\% on G360 Full image, 3\% on G360 Full video, and 36\% on GFIE image. In contrast, Kothari \etal~\cite{kothari2021weakly} don't improve when using the LAEO label (AVA) in a weakly-supervised framework. 
%
Additionally, in \cref{tab:within_constraint}, we compared our method trained on G360 Full to methods trained on detected face subset G360 Face. Given that the state-of-the-art methods are specifically designed for near-frontal faces, our supervised model \model is not SOTA but demonstrates competitive performance. When including gaze following label using our \framework framework, it shows very competitive results and SOTA performance on G360 Face video (9.92° vs 10.05°), G360 Face 180 video (9.84° vs 9.75°), and G360 Face 40 (8.62° vs 8.30°). Therefore, compared to methods using tight face crops (increasing eye resolution), our \framework approach proved to be competitive on near frontal view. We include a comparison with SOTA trained on G360 and evaluated on constrained benchmarks MPII and EDIAP in supplementary materials. \\
% 
\mypartitle{Cross-Dataset Experiments. }
In this section, we emphasize on cross dataset experiments. In \cref{tab:main_results}, we compare our method with SOTA methods on generalization on G360 (bottom part).
Among the few approaches that explore generalization on Gaze360, Kothari \etal~\cite{kothari2021weakly} provides the most relevant comparison. In contrast, 3DGazeNet~\cite{ververas20223dgazenet} provides cross-dataset generalization on G360 Face but only works on frontal faces requiring face and eye crop as input.  
Our results show that our \framework framework trained with various available 3D gaze datasets (GFIE, or GFIE+MPS) always improves generalization. For instance, when tested on G360 Full image and video, it always outperforms our supervised approach by 40\% and 20\% when trained with GFIE or GFIE+MPS, respectively. A similar trend is observed when trained on G360 and tested on GFIE. Therefore, it confirms that our framework using gaze follow labels is effective for improved generalization. \\
%
When compared to Kothari \etal~\cite{kothari2021weakly} trained on LAEO labels (AVA), our \framework approach trained using GFIE+GF shows better performance on G360 Full but is behind on G360 40 because GFIE doesn't contain frontal samples. In contrast, when trained using GFIE+MPS+GF, it improves over Kothari on both G360 Full and 40.\\
%
\mypartitle{Limitations. }
As expected when using our framework, the generalization improvement is tight to the training diversity used in the pre-training stage. In cross-dataset experiments in \cref{tab:main_results}, compared to our supervised model, we observe that when trained using GFIE our framework improves more on G360 Back and less on G360 40 because of the non-frontal distribution of GFIE. 



