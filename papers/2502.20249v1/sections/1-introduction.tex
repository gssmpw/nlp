
\section{Introduction}
\label{sec:intro}

\noindent Non-verbal behaviors play a crucial role in human communication, often conveying more information than words alone. Among the various forms of non-verbal cues, eye gaze stands out as an important signal for understanding human behavior, including attention, communication, intents, and mental state. 
Consequently, gaze signals have been used in many applications. Some applications require accurate gaze for frontal head pose such as AR/VR \cite{burova2020utilizing}, 3D avatar animation \cite{richard2021audio}, human-computer interaction \cite{andrist2014conversational}, and driver behavior monitoring \cite{kasahara2022look}. While other applications focus on robust 3D gaze estimation from a wide range of head poses such as medical and psychological analysis \cite{kleinke1986gaze} or human-robot interaction \cite{admoni2017social}. 

In this paper, our goal is to develop a robust 3D gaze estimation for in-the-wild applications with unconstrained head pose and real-world environments. In the literature, this refers to the less explored and highly challenging problem of ``physically unconstrained gaze estimation"~\cite{Gaze360}. 



\mypartitle{Motivations.}
%
Estimating gaze in unconstrained, real-world settings poses unique challenges not fully addressed by current lab-based datasets, which are primarily collected in controlled screen-target setups \cite{Rt-gene, GazeCapture, MPIIGaze, Eyediap, EVE, ETH}. While these datasets have enabled recent approaches to achieve high accuracy in frontal 3D gaze estimation from monocular images \cite{zhang2017s, chen2018appearance, Rt-gene, cheng2020coarse, cheng2022gaze, abdelrahman2023l2cs, yan2023gaze, catruna2024crossgaze}, their effectiveness declines in real-world scenarios. This limitation stems from restricted gaze distributions, lab-specific conditions, limited subject diversity, and reliance on potentially noisy head-pose estimates for normalizing eye and face images~\cite{zhang2018revisiting}.

To address the lack of data for unconstrained gaze estimation, Gaze360 \cite{Gaze360} and GFIE \cite{GFIE} were developed. Although these datasets have advanced the field, models trained on them continue to struggle with challenging, real-world conditions (see \cref{fig:intro}), particularly when facing extreme head poses, partial eye occlusions, varying resolutions from diverse camera-to-subject distances, and a wide range of appearances (e.g., skin tones, hairstyles, facial expressions). This limitation is largely due to insufficient diversity in the training data, as collecting high-quality, naturally occurring, and diverse 3D gaze data is complex, resource-intensive, and not easily scalable.

To overcome these limitations, researchers have explored using “secondary” labels that are easier to obtain, \eg by relying on internet data. 
For example, Kothari \etal~\cite{kothari2021weakly} leveraged 2D gaze direction labels from the “Looking at Each Other” (LAEO) dataset~\cite{marin2019laeo}. By applying geometric constraints and head-size heuristics, they generated pseudo-3D gaze data.

While this approach showed some generalization improvement, the authors noted that LAEO's gaze distribution is primarily horizontal and requires images containing at least two people with mutual gaze, which limits sample diversity and availability.

Here we aim to utilize more general 2D gaze annotations from the gaze following task \cite{recasens2015they, Chong_2018_ECCV}. Although the ground truth for gaze following is defined as the 2D pixel location a person in the scene is looking at, we can repurpose it as 2D gaze direction ground truth. Compared to LAEO, gaze following datasets offer greater diversity, with broader gaze distributions and a wider variety of natural scenes.
%

In addition, in contrast to \cite{kothari2021weakly}, we propose a Self-Training Weakly-Supervised Gaze Estimation (\framework) framework using a two-stage training approach without relying on heuristics or relative depth estimation to generate pseudo-3D gaze labels. First, we train a gaze network on existing 3D gaze datasets. We then use this network's predictions on gaze-following data, combined with 2D gaze ground truth, to create 3D gaze pseudo-labels. In the second stage, we retrain a similar gaze network using both 3D gaze datasets and gaze-following datasets with these pseudo-labels.
%
Our approach minimizes the need for labor-intensive, unconstrained 3D gaze labeling and demonstrates significant improvements over state-of-the-art methods in both within-domain and cross-domain generalization on Gaze360, GFIE, and MPIIFaceGaze \cite{MPIIGaze}.



% Problem Method
Given the scarcity of in-the-wild 3D gaze datasets, 
another challenge lies in how to leverage both image and video data effectively. 
%
Modality-specific models restrict the training set to modality-specific datasets, limiting their ability to benefit from all available resources. While static models can draw on large image datasets such as GazeFollow \cite{Chong_2018_ECCV}, temporal dynamics are also essential for robust 3D gaze estimation in unconstrained environments \cite{Gaze360, nonaka2022dynamic}, 
and is particularly valuable when the eye region is obscured,
%or difficult to discern, 
whether due to occlusions, low resolution, or blinking (see \cref{fig:intro}).

To address this, one approach is to pre-train a model on images and transfer the weights to a temporal model using techniques like filter inflation, where 2D filters are extended to 3D models, as done in prior adaptations for video tasks \cite{carreira2017quo}. However, this method is more suited to fine-tuning and risks catastrophic forgetting, where the model loses pre-trained knowledge~\cite{mccloskey1989catastrophic}.
%
Alternatively, images can be duplicated to simulate fixed-length video clips, allowing for training on both image and video datasets in a temporal model. However, this can impair the learned gaze dynamics, as synthetic clips lack genuine motion information.


Transformers offer a promising solution for handling multiple modalities. Inspired by recent work \cite{girdhar2022omnivore, girdhar2023omnimae}, we propose a Gaze Transformer (\model) designed to encode both image and video inputs into a shared representation. 
This allows us to leverage labeled datasets more effectively,
by training jointly on image and video data. 
We  demonstrate better cross-modal generalization, 
and that using image datasets enhances video gaze prediction, thus enabling a more versatile and robust 3D gaze estimation model.


\mypartitle{Contributions.}
They can be summarized as:
%
\begin{itemize}
\item {\bf \framework, a novel learning framework enhancing generalization.} 
%
To address the lack of diverse, naturalistic 3D gaze datasets, we leverage 2D gaze-following datasets using 3D pseudo labels. Combining these with 3D gaze datasets in a two-stage manner, we demonstrate improved 3D generalization on several benchmarks.
%
\item {\bf A visual modality agnostic Gaze Transformer (\model) architecture 
making efficient use of existing gaze datasets.}
%
By allowing simultaneous learning from 3D gaze image and video datasets, it outperforms modality-specific models, resulting in better static and dynamic gaze representations, better capturing spatiotemporal patterns in head sequences compared to the state-of-the-art.
%
\item {\bf State-of-the-art results.} 
Our approach surpasses existing methods in both unconstrained (Gaze360, GFIE) and constrained (MPIIFaceGaze) environments, achieving superior results in within- and cross-dataset evaluations.
\end{itemize}
%
These contributions position our approach as ideal for real-world unconstrained 3D gaze estimation applications.
