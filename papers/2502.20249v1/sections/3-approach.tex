


\vspace{-2mm}
\section{\framework Method}
\vspace{-1mm}

%
\subsection{Self-Training Pipeline}
\label{sec:self_taining}

\vspace{-1mm}

We propose a two-stage training pipeline for gaze estimation to leverage any 2D gaze datasets, as presented in \cref{fig:approach}. 
In the first stage, a gaze network is trained on image and video 3D gaze datasets in a supervised manner. Next, the network is used to infer 3D gaze on 2D gaze datasets. 
Since only the gaze's depth is missing in the 2D gaze label, we employ a geometric transformation to generate a robust pseudo-3D gaze label from the inferred 3D gaze that is aligned with the 2D gaze label. We assume that a model pre-trained on unconstrained 3D gaze datasets provides a good prior z-estimation. In the second stage, a similar gaze network is trained in a supervised regime using both gaze following data with 3D pseudo labels and 3D gaze datasets. 

\vspace{-1mm}
\subsection{ Gaze Transformer (\model)}
\label{sec:model}
\vspace{-1mm}

\mypartitle{Model Architecture Motivation.}
%
Accurate and robust 3D gaze estimation in the wild requires three key capabilities: capturing fine local details from the eye region when visible; extracting global information from head orientation which is particularly useful when the eyes are occluded or partially obfuscated; capturing small motion of head pose and eyes in the temporal domain to capture subtle gaze shifts. A model capable of training on both image and video data is especially valuable, as it broadens the range of available training datasets. 
Convolutional Neural Networks (CNNs) excel in frontal gaze estimation due to their ability to extract local eye features \cite{Gaze360,abdelrahman2023l2cs,yan2023gaze} but may encounter more difficulty in global reasoning (i.e. merging pose and eye information), and extending CNNs to handle temporal data within a single modality-agnostic model is challenging.
Vision Transformers (ViTs) \cite{dosovitskiy2020image}, as noted by Cheng \etal~\cite{cheng2022gaze}, are less suited for gaze estimation since they may miss critical local details, especially when the eye region is split across multiple patches.
In contrast, hierarchical transformer architecture \cite{liu2021swin,ryali2023hiera} offers a flexible architecture to capture both local and global features. For instance, the Swin Transformer, which uses smaller patches (typically $4\times4$ vs $16\times16$ in standard ViTs), is better able to capture fine local details. Its "shifted window" mechanism, which applies self-attention within local windows that shift at regular intervals, effectively aggregates local and global context. Extending the Swin Transformer to the temporal dimension has proven successful for temporal tasks on several benchmarks~\cite {liu2022video}.
Additionally, transformers are versatile, recent work has demonstrated their effectiveness when trained on both image and video datasets within a single model~\cite{girdhar2022omnivore, girdhar2023omnimae}. Inspired by these approaches, we introduce our Gaze Transformer, \model, with several modifications for 3D gaze estimation, as illustrated in \cref{fig:approach} and detailed in the following sections.\\
%
\mypartitle{Patchifier.}
%
The model needs a common representation format to encode both image and video input. 
%
Following \cite{dosovitskiy2020image,liu2021swin,feichtenhofer2022masked,tong2022videomae,girdhar2022omnivore}, images and videos are represented as 4D tensors $\mathrm{\textbf{X}} \in \mathbb{R}^{T \times H\times W \times 3}$, 
with  $T=1$ for an image $\mathrm{\textbf{I}}$,  and $T>1$ for a video clip $\mathrm{\textbf{V}}$.
%
Then, the input $\mathrm{\textbf{X}}$ is divided into a collection $\{ \Inputpatch_i \}^N$ of 4D sub-tensor (patches) 
$\Inputpatch_i \in \mathbb{R}^{\mathrm{t} \times \mathrm{h} \times \mathrm{w} \times 3}$,  as presented in \cref{fig:approach}. 
%
Following \cite{girdhar2022omnivore,tong2022videomae, girdhar2023omnimae,marin2019laeo}, we use $t=2$.
%
When working with image only,  we duplicate the image instead of zero-padding because we find better cross modalities generalization from video to image. Then, a shared linear layer and LayerNorm are applied to project the patches to a token representation. \\
%
\mypartitle{Encoder.}
The tokens from the patchifier are then fed into a tiny Swin3D hierarchical spatiotemporal encoder. It relies on self-attention within nearby tokens in a spatiotemporal window that is shifted every time. In addition, it uses two sets of relative positional encoding: one spatial and one temporal. Because of the hierarchical representation, the number of tokens is reduced by patch merging layers as the network
gets deeper. The temporal output dimension is reduced by a factor of two. The output tokens are then fed to a gaze decoder module.  \\
\mypartitle{Gaze Decoder.}
We first apply a mean spatial pooling on the output tokens, followed by an interpolation function to double the temporal dimension to match the input length (for images, interpolation is skipped). Finally, a shared MLP with a single hidden layer is applied to each token to predict a normalized 3D gaze vector. \\
%
\mypartitle{Baseline Networks.} 
Different approaches exist to process image and video in a single model. To compare the performance of our \model model, we develop in addition two baselines.
Given that the static Swin(2D) transformer gives good performance on gaze estimation. We add a temporal encoder to model the gaze dynamic. Therefore, we develop Swin(2D)-LSTM which first processes a set of images using Swin and outputs a set of embedding for each image. Then, it is fed to a bidirectional LSTM followed by a shared gaze MLP on each output to produce a gaze vector. 
%
Similarly, the second baseline called Swin(2D)-Tr replaces the LSTM by a transformer. The output tokens from each image are projected to a lower dimension followed by a LayerNorm and absolute spatiotemporal encoding. 
Then, the transformer is applied to the spatiotemporal output token. Finally, a spatial mean pooling is applied followed by a similar gaze MLP. Both architectures are input agnostic and are compared in an ablation study.


\subsection{Pseudo 3D Gaze Generation }
\vspace{-1mm}
Creating 2D gaze datasets is easier than creating 3D gaze datasets, as annotation can be completed after the images have been collected, unlike 3D gaze which can not be annotated by humans and require a special setup. 
%, which is simpler than collecting 3D gaze data. 
As a result, 2D datasets like GazeFollow offer a broad gaze, head pose distribution, and head/face appearance diversity. 
There are different possibilities to leverage such a dataset which will be discussed in the ablation sections.  
For instance, Kothari \etal~\cite{kothari2021weakly} used 2D gaze from LAEO labels and 3D fitted head models for z-direction estimation. 
%
In contrast, as presented in \cref{fig:approach}, our 3D pseudo gaze generation method assumes that a pre-trained model trained on unconstrained 3D gaze datasets can provide a good prior z-estimation, which is confirmed by our experiments. Combining the z component of predicted 3D gaze with 2D gaze ground truth %via geometric projection 
provides a robust pseudo 3D gaze label. 
Then, using this pseudo label as an additional label during training, we report improvement in unconstrained generalization. \\
%
\mypartitle{Geometric Projection.}
%
The predicted 3D Gaze (3DPred) $\Gazepredicted$ and the 2D ground truth (2D GT) $\textbf{v} = (v_x,v_y)$ are combined such that the image plane projection of the pseudo 3D gaze (3DGP) $\Gazepseudo$ is aligned with the 2D ground truth $\textbf{v}$. 
%
Therefore, a rotation is applied to $\Gazepredicted$ around the z-axis such that $\Gazepseudo$ has the same x,y direction as $\textbf{v}$:
\vspace{-0.1cm}
\begin{equation}
    \Gazepseudo = ( v_x \|(\Gazepredictedcoordinate_x,\Gazepredictedcoordinate_y) \|_2, v_y \|(\Gazepredictedcoordinate_x,\Gazepredictedcoordinate_y) \|_2, \Gazepredictedcoordinate_z)
\end{equation}
%
\vspace{-6mm}
\subsection{Training Strategy}
\vspace{-1mm}

In both training stages, the objective is to train our \model model on a collection of both image and video datasets with gaze label 
$\{(\mathrm{\textbf{X}},\textbf{g})_j\}$ where $\textbf{g} \in \mathbb{R}^{T \times 3}$ with T=1 for images, which creates different training challenges to be addressed.
%
\mypartitle{Video Training Data.}
%
As our model is modality agnostic, a video dataset can be 
considered both as a set of video clips or as a collection of images.
%
These views of the data are not equivalent, as, typically, considering the data as video-clip at training will impact more inference on videos at test time rather than on images. Hence, a video dataset can be used twice as an image or video training dataset\footnote{By convention, when reporting experiments, for video datasets, we will add the suffix I when it is considered as an image dataset, V in the video case, and I\&V when it is used twice as image and as video dataset.}.
%
We will see in ablations that it can impact the modality generalization capability.

\mypartitle{Mini-batch Strategy.}
%
Different mini-batch strategies have been proposed in the literature to handle multiple datasets.
One approach mixes samples from each of the datasets, but in our case, this requires careful implementation because images and videos don't have the same dimensions.
Another strategy creates batches from one dataset at a time and alternates between them. This approach has proven effective in previous work \cite{girdhar2022omnivore,hu2021unit} and we followed this approach here. 
In addition, dataset size imbalance is another challenge, as dataset sizes range from 30k to 120k samples. To address this, we balance the datasets by oversampling smaller ones and undersampling larger ones so that each dataset contributes equally during an epoch.\\
%
\mypartitle{Loss.}
%
For training the model, we utilize a temporal weighted average of the angular loss, which represents the angular difference between the predicted gaze vector $\hat{\textbf{g}}$ and the ground truth ${\textbf{g}}$ in degree:\\[-3mm]
%
\begin{equation}
    \mathcal{L}_{gaze}(\hat{\textbf{g}}, \textbf{g}) = \frac{1}{T}\sum^T_{t=1}\frac{180}{\pi}\arccos(\frac{\hat{\textbf{g}}_t^T\textbf{g}_t}{\|\hat{\textbf{g}}_t\| \|\textbf{g}_t\|})
\end{equation}

