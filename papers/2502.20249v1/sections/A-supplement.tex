

\mypartitle{What is expected?} The supplementary material consists of datasets details, experiments details, and extended experiments analysis mentioned in the main paper. In addition, videos of qualitative examples of our method on VideoAttentionTarget further demonstrate the robustness in challenging real-world scenarios.  

\section{Datasets Details}

\subsection{Datasets}
\label{sec:supp_dataset}
\mypartitle{ Gaze360 (G360).} \cite{Gaze360} is video 3D gaze datasets. It is collected in both indoor and outdoor environments in unconstrained setting, which contains 3D gaze of 238 subjects with a wide-range head pose and gaze direction. G360 is recorded at 8FPS. In all of our experiments, we always used the same training set as \cite{Gaze360} with 126928 samples. For the test set, we followed the split of \cite{Gaze360} where G360 Full corresponds to "All 360°" (the entire test set) with 25969 samples, G360 180 corresponds to "Front 180°" (gaze within 90°) with 20322 samples, and G360 40 to "Front Facing" (gaze within 20°) with 3995 samples. In addition to those splits, we consider G360 Back (gaze above 90°) \cite{chen2020360} with 5647 samples and finally G360 Face (all detected faces) with 16031 samples, which is used in many constrained gaze studies \cite{zhang2017s,chen2018appearance,Rt-gene,cheng2020coarse,cheng2022gaze,abdelrahman2023l2cs,yan2023gaze,catruna2024crossgaze}. When we refer to G360 Face 180 (15895 samples), it corresponds to the detected face with a gaze within 90°, a subset of G360 180, the same for G360 Face 40 with 3687 samples. We used the validation set described in \cite{Gaze360} with 17038 samples.

\mypartitle{GFIE.} \cite{GFIE} is a video 3D gaze dataset collected indoors with 71799 frames from 61 subjects (27 male and 34 female). It is an unconstrained dataset with a wide range of head poses. It was collected for gaze following task; using a complex calibrated laser setup, they can infer the 3D gaze from the eye to the visual target direction. They recorded people doing various indoor activities at 30 fps. We follow the data splits described in \cite{GFIE}, 59217 for training, 6281 for validation, and 6281 for testing.

\mypartitle{MPSGaze (MPS).} \cite{MPS} is a modified 3D gaze datasets that has been automatically generated using ETH-Xgaze \cite{ETH} eyes. They apply a blending technique on people from the Widerface~\cite{yang2016wider} dataset to put eyes with a known 3D gaze from ETH on heads with similar head poses. This dataset is diverse, with more than 10k identities and challenging poses, appearances, and lighting conditions. However, the blending process reduces the quality of the visual appearance, and it contains only near frontal head poses and no back view. We used the same training and test split with 24282 samples in training and 6277 samples in testing. No validation is defined in this work. 

\mypartitle{EYEDIAP (EDIAP).} \cite{Eyediap} is a 3D gaze video dataset. It includes videos from 16 subjects (30 fps), using either screen targets (CS, DS subset EDIAP) or 3D floating balls ( FT subset EDIAP-FT) as gaze targets. It is a constrained setup with mainly frontal head poses. 
%Compared to EDIAP with screen target, EDIAP-FT contains a wider gaze and head pose distribution. 
Following \cite{wang2022contrastive,Cheng2021Survey}, we used the evaluation set under screen target session (CS, DS, namely EDIAP) with 16674 samples from 14 subjects. 
%In addition, we consider the floating target session (FT, namely EDIAP-FT) with 8346 samples.

\mypartitle{MPIIFaceGaze (MPII).} \cite{MPIIGaze} is a 3D gaze image dataset collected from 15 subjects in a screen-based gaze target setup, resulting in a constrained dataset with mostly frontal head pose. We follow the standard evaluation protocol \cite{MPIIGaze,wang2022contrastive,Cheng2021Survey}, which selects 3000 images from each subject to form an evaluation set for a total of 45000 samples. 

\mypartitle{GazeFollow (GF).} \cite{recasens2015they} is a 2D gaze image dataset annotated on in the wild dataset for the gaze the following task. The 2D target label corresponds to where a given person is looking at in the image. It is a diverse dataset that includes various head poses, appearances, scenes, and lighting conditions. Overall, it has around 130K annotated person-target instances in 122K images.

\subsection{Video Processing}
As mentioned in the main section, for video clip input, our approach predicts the 3D gaze from an 8-frame video clip. However, video datasets have different frame rates, which can impact the gaze prediction. In this work, since G360 has a lower frame rate, we resample EYEDIAP and GFIE to match G360's frame rate of 8 fps. 

\subsection{Gaze Representation} 
Working with different 3D gaze datasets requires a unified way to define and represent the 3D gaze vector. Usually, in constrained gaze estimation, studies use data normalization to map the input image to a normalized space where a virtual camera is used to warp the face patch out of the original input image according to the 3D head pose \cite{ETH}. Thus, the gaze is expressed in this virtual camera coordinate defined by the 3D head pose. \\
%
However, in unconstrained settings, it is not possible to get access to a robust and reliable 3D head pose; thus, we follow the gaze representation of Gaze360 \cite{Gaze360} in the ``Eye coordinate system". The practical interpretation of the eye coordinate system is that the positive x-axis points to the left, the positive y-axis points up, and the positive z-axis points away from the camera, \ie [-1,0,0] is a gaze looking to the right or [0,0,-1] straight into the camera from the camera's point of view, irrespective of subjects position in the world. The origin of the gaze vector is the middle of the eyes, except for MPS and MPII, where the gaze origin is the average of 3D eyes and mouth landmarks resulting in an origin located at the middle of the nose, and for GF, we used the center of the head bounding box as the origin.

\begin{figure}
    \centering
    \includegraphics[width=0.99\linewidth]{images/head_crop_plot.png}
    \vspace{-4mm}
    \caption{Input head crop using different scales. In our work, a scale of -0.1 is used and proved to be effective in both constrained and frontal face setting \cref{sec:crop_size} }
    \label{fig:head_crop_input}
\end{figure}

\section{Experiments Details}

\mypartitle{Metric.}
We follow the test split described in the state-of-the-art method and explained in \cref{sec:supp_dataset}. As a metric, we use the standard angular error in degrees between the predicted and ground truth gaze prediction \cite{Gaze360,ETH,Eyediap,MPIIGaze}. Previous methods reporting video evaluation used a 7-frame video clip and predict the middle frame gaze direction. Since our approach outputs eight gaze directions from an 8-frame video clip, for a fair comparison, we use the 4th gaze prediction of an 8-frame video clip to compute the evaluation metric. 

\mypartitle{Training.}
We used the same setup in all the experiments to be as fair as possible. All the models are trained for a minimum of 20 epochs. We used an early stopping on the validation set with a patience of 10 epochs. We use the AdamW optimizer \cite{AdamW} with a learning rate of 1e-4 and a cosine annealing schedule with a 5 epochs linear warmup (from 2e-5 to 1e-4). For evaluation, we report the performance of the best model defined by the best angular error on the validation set.

\mypartitle{Data augmentation.}
Data augmentation is crucial for robust gaze estimation in the wild. In this work, we used standard data augmentation techniques. First, we applied jittering during the head crop to introduce slight variations in scale and aspect ratio, which reduces the model's sensitivity to noisy or imprecise head bounding boxes. Next, color jittering was applied by adjusting brightness, contrast, and saturation, making the model more resilient to diverse lighting conditions commonly encountered in real-world scenarios. Since gaze labels, such as those in the GF 2D dataset, may exhibit bias toward one side, we applied horizontal flipping to the images while appropriately adjusting the gaze direction, ensuring more balanced training data in the yaw gaze direction. These augmentations collectively improved the model’s ability to handle variations in data and enhance its generalization to unseen environments.

\section{Additional Experiments}

\subsection{Effect of Head Crop Size}
\label{sec:crop_size}
As mentioned by Chen \etal~\cite{chen2020360}, the input head crop scale impacts the 3D gaze estimation. We find that the effect on the prediction depends on the head orientation. \cref{fig:head_crop_input} illustrates the different inputs with different head crop scales. As shown in \cref{fig:supp_head_crop_effect}, a smaller head crop tighter to the face improves 3D gaze estimation on frontal head poses, while a larger head crop improves gaze on the non-frontal head pose. Indeed, as shown in \cref{fig:head_crop_input}, a tighter crop increases the eye resolution in the image and a larger crop provides more context about the head orientation and upper body orientation, which gives a strong prior for the gaze direction when eyes are not visible.
In the context of gaze estimation in the wild, a scale of -10\% is part of the Pareto front as illustrated in \cref{fig:supp_head_crop_effect} and is also the best on the G360 Full image as shown in \cref{fig:supp_head_crop_angular}. Therefore, it is a reasonable trade-off between frontal and back view performance. We use it for all our experiments.

\begin{figure}[htbp]
    \centering
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=0.99\textwidth]{images/Head_crop_angular_error.png}
        \caption{Effect of head bounding box scale as input on the 3D gaze angular error on G360 Full test set. A scale ratio of 0.1 corresponds to a 10\% bounding box scale.}
        \label{fig:supp_head_crop_angular}
    \end{subfigure}
    
    
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=0.8\textwidth]{images/Head_crop_effect.png}
        \caption{Effect of head bounding box scale on the angular error with respect to G360 Back and G360 40 test subset.}
        \label{fig:supp_head_crop_effect}
    \end{subfigure}
    
    \caption{\textbf{Effect of head crop size.}}
    \label{fig:head_crop}
\end{figure}

\subsection{Constrain Gaze Evaluation}

\input{tables/domain_generalization_constrained}

The objective of this work is to improve unconstrained gaze estimation in the wild. As seen in \cref{sec:crop_size}, compared to a tight face crop a larger crop improves gaze in challenging head pose. Therefore, a larger crop is more suited to our objective.  In contrast, some methods specialize in frontal gaze estimation and rely on tight face crops, which provide better resolution for the eye regions. While this is not a fully fair comparison, we compare our approach to these constrained methods for generalization on constraint benchmarks. Note that for the constrained methods, models are trained and tested only on a subset of detected faces (G360 Face), while in our approach the model is trained on G360 Full. \\
As shown in \cref{tab:generalization_constrained}, on MPII, the supervised GaT lags behind the best method by 6\%. On EDIAP, GaT is 21\% behind the best method in image evaluation but narrows the gap to 13\% when evaluated on videos. Then, when using our \framework learning framework including GF labels, we observe an important improvement on MPII with state-of-the-art angular error of 6.43 compared to 7 from CDG. On EDIAP the improvement is marginal. Compared to EDIAP, MPII has more diversity in lighting conditions and environment. GF doesn't contain a lot of frontal gaze direction but has a broad diversity of environments. Therefore, the improvement on MPII should come from the additional diversity that GF brings but this is not useful for EDIAP prediction. 
While constrained methods excel in frontal settings, they fail in unconstrained scenarios. Our approach, which achieves state-of-the-art performance in unconstrained environments (G360, GFIE) while remaining competitive in constrained settings (MPII, EDIAP), proves to be a versatile and robust solution for gaze estimation in the wild.

\subsection{Qualitative Analysis}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{images/plot_image_video_gaze360.png}
    \caption{ \textbf{Image vs video predictions, where does it help?}. GaT trained on G360I\&V and tested on G360 Full image and video. The difference between image and video angular error with respect to the ground truth gaze directions from the camera ([0,0,-1]). The mean and standard deviation are displayed for each 10° bin. Positive values indicate better performance in video prediction compared to image prediction. }
    \label{fig:graph_img_vs_video}
\end{figure}

\mypartitle{When does temporal context contribute most effectively?}
As seen in the main paper, video prediction consistently outperforms image prediction. To understand the significance of temporal context in gaze estimation, we examined cases with large angular errors between image and video predictions. Several key observations emerged. As illustrated in \cref{fig:temporal_pros} in the first two rows, temporal context proves valuable during blinks, as it allows the model to interpolate gaze direction when the eyes are closed. If the head pose is not informative, temporal context helps disambiguate between blinking and looking down since the eyes are not visible, as shown in row 1. Additionally, when individuals are viewed entirely from behind (rows 6-7), video inferences provide a more consistent gaze direction in relation to time. Thus, there is less jittering and it might improve the prediction accuracy. In rows 4-5, the head and eye motion can be used in video prediction to improve the gaze direction. Finally, it can help in case of occlusion, as seen in row 3.\\
Furthermore, we explore the impact of image- and video-based prediction with respect to gaze direction. Indeed, we expected more improvement when people are from the back since additional head motion cues can be useful for gaze estimation. In the results, video prediction on G360 Back clearly improves image prediction. In addition, in \cref{fig:graph_img_vs_video}, we plot the difference between image and video prediction angular error for different gaze directions. If we look at the trend, video prediction seems to be better, especially for gaze over 150°, but given the standard deviation, it might not be a statistically significant observation. A more detailed analysis by considering only cases where there is a head motion can better highlight the impact of video prediction.\\
\mypartitle{What are the limitations of temporal context for gaze?}
We investigate prediction made on the VideoAttentionTarget~\cite{Chong_2020_CVPR} (VAT) videos using our \framework framework and \model model. VAT is a challenging dataset with real-world scenarios, various appearances, and diverse gaze distribution, making it well-suited for assessing our approach. Our qualitative analysis reveals two limitations of video-based inference compared to image-based inference using our model. The first limitation arises in cases of rapid head rotation, as illustrated in \cref{fig:temporal_cons}, temporal context may be misused, leading to predictions that do not align with the actual gaze. It might be because no rapid head motion is present in the G360 training sets. 
The second aspect involves cases of ``gaze recentering", where the gaze direction returns to its initial position following a shift. This behavior can occur very rapidly, within just 3-4 frames. Due to the smoothing effect in the temporal modeling, the predicted gaze may not exhibit the same amplitude as the actual movement. Indeed, this behavior is not present in the G360 dataset, and the use of videos sampled at 8 frames per second may limit the ability to capture fine-grained gaze dynamics. However, such behavior is better captured during image-based inference. This highlights a trade-off: while video-based inference provides smoother and more robust predictions, image-based inference offers greater accuracy but can result in jittery outputs. To mitigate the lack of natural gaze behavior we apply our \framework framework using 2D gaze video data from VAT. Unfortunately, since current benchmarks don't contain natural gaze behavior, the results don't show quantitative improvement. Further research to evaluate this aspect is needed.   \\
\mypartitle{In which scenarios does \framework with GazeFollow labels provide the most benefit?}
We demonstrated the advantages of \framework with GazeFollow labels across various benchmarks, both within- and cross-datasets. But in which scenarios does it outperform supervised methods trained solely on G360? To address this question, we analyze predictions made in real-world scenarios using the VideoAttentionTarget (VAT) dataset~\cite{Chong_2020_CVPR}. Our findings reveal that \framework achieves the most notable improvements in cases of extreme head poses, particularly when the head is facing downward, as shown in \cref{fig:sup_vs_stwsge}. It is also more robust to appearance diversity like hair partially occluding the face or varying skin tones. It also helps in difficult lighting conditions and low-resolution inputs.
Additionally, we include a video (provided in the supplementary materials) displaying predictions on VAT with an explanation, enabling a direct comparison between the two methods and a clearer visualization of our approach's performance on real-world data.

\begin{figure*}[t]
    \centering
    \includegraphics[width=1\linewidth]{images/presentation1.pdf}
    \caption{ \textbf{Illustration of image against video prediction.} Comparison between single-image (frame 0) and video predictions (frame -3 to 4). We use our \framework learning framework with \model trained on G360 and GF. All examples are from G360 test set. Rows 1-2 illustrate eye blinks, Row 3 shows an example of occlusion, Rows 4-5 demonstrate frontal head/eyes motion, and Rows 6-7 depict back view prediction. In the last row, the first two frames are not part of the test subset. Arrows in \textcolor{red}{red} represent image predictions, and arrows in \textcolor{magenta}{magenta} are video predictions. The angular error between groundtruth and prediction is displayed in red at the top right corner. The circles in the images represent unit disks where 3D gaze vectors are projected onto the image plane (x,y in yellow) and a top view (x,z in blue)}
    \label{fig:temporal_pros}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=1\linewidth]{images/presentation2_over.pdf}
    \caption{\textbf{Illustration of image and video prediction in case of rapid head motion.} We use our \framework learning framework with \model trained on G360 and GF. All examples are from VideoAttentionTarget~\cite{Chong_2020_CVPR} (VAT). Arrows in \textcolor{red}{red} represent image predictions, and arrows in \textcolor{magenta}{magenta} are video predictions. The circles in the images represent unit disks where 3D gaze vectors are projected onto the image plane (x,y in yellow) and a top view (x,z in blue). Note that since VAT has a frame per second (fps) of 24 and G360 has a fps of 8, we show the temporal context used for video inference corresponding to 8 fps. }
    \label{fig:temporal_cons}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=1\linewidth]{images/presentation3.pdf}
    \caption{\textbf{Illustration of supervised against \framework learning framework with GazeFollow label.} We use in both experiments our \model model. All examples are from VideoAttentionTarget~\cite{Chong_2020_CVPR} (VAT). Arrows in \textcolor{blue}{blue} represent image predictions with supervised \model trained on G360, and arrows in \textcolor{red}{red} are image predictions with \framework \model trained on G360 and GF. The circles in the images represent unit disks where 3D gaze vectors are projected onto the image plane (x,y in yellow) and a top view (x,z in blue). }
    \label{fig:sup_vs_stwsge}
\end{figure*}
