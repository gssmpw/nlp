
\vspace{-2mm}
\section{Conclusion}
\vspace{-1mm}

In this work, we introduced the \framework learning framework, which leverages weakly annotated images with 2D gaze datasets, such as gaze-follow labels, to enhance appearance diversity and broaden gaze distributions across natural scenes. We also presented our Gaze Transformer, \model, which improves performance and supports both image and video training. By combining \framework and \model, we achieve significant gains in both within- and cross-dataset experiments, reaching state-of-the-art results on GFIE and Gaze360. Additionally, we demonstrate effective cross-modal generalization, a critical capability given the scarcity of video datasets. We believe our approach is a promising solution for robust 3D gaze tracking in the wild, suitable for a range of challenging applications.