% CVPR 2025 Paper Template; see https://github.com/cvpr-org/author-kit

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
%\usepackage{cvpr}              % To produce the CAMERA-READY version
% \usepackage[review]{cvpr}      % To produce the REVIEW version
\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version
%\usepackage{comment}
\usepackage{arydshln}
% Import additional packages in the preamble file, before hyperref
\input{preamble}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, 
% e.g. with the file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete *.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you should be clear).
\definecolor{cvprblue}{rgb}{0.21,0.49,0.74}
\usepackage[pagebackref,breaklinks,colorlinks,allcolors=cvprblue]{hyperref}

%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\paperID{7487} % *** Enter the Paper ID here
\def\confName{CVPR}
\def\confYear{2025}

%%%%%%%%% Custom latex
\input{notations}

%%%%%%%%% AUTHORS - PLEASE UPDATE
\author{Pierre Vuillecard\\
IDIAP, EPFL\\
{\tt\small pierre.vuillecard@idiap.ch}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Jean-Marc Odobez\\
IDIAP, EPFL\\
% First line of institution2 address\\
{\tt\small odobez@idiap.ch}
}

\begin{document}

%%%%%%%% TITLE - PLEASE UPDATE
\title{ \papertitle }

\twocolumn[{%
\renewcommand\twocolumn[1][]{#1}%
\maketitle


\vspace{-1.2cm}
\begin{center}
    \centering
    \captionsetup{type=figure}
    \includegraphics[width=1.\textwidth]{images/Main_figure_gaze.pdf}
    \vspace{-6mm}
    \captionof{figure}{
    \textbf{Significance of ST-WSGE.} Our self-training based weakly-supervised framework for robust 3D gaze estimation in real-world conditions (e.g., varying appearance, extreme poses, resolution, and occlusion).  
     All predictions used our image and video agnostic Gaze Transformer (\model) model.
     Top row: importance of the training diversity using ST-WSGE and GazeFollow (GF) for generalization compared to standard supervised methods. 
     Bottom row: influence of temporal context between image and video inference.  Circles in  images represent unit disks where 3D gaze vectors are projected onto the image plane (x, y in yellow) and a top-down view (x, z in blue). Images from VideoAttentionTarget, GFIE, and MPIIFaceGaze datasets.
}
    \label{fig:intro}
\end{center}%
}]

%%%%%%%%% ABSTRACT

\begin{abstract}
Accurate 3D gaze estimation in unconstrained real-world environments remains a significant challenge due to variations in appearance, head pose, occlusion, and the limited availability of in-the-wild 3D gaze datasets. To address these challenges, we introduce a novel Self-Training Weakly-Supervised Gaze Estimation framework (\framework). This two-stage learning framework leverages diverse 2D gaze datasets, such as gaze-following data, which offer rich variations in appearances, natural scenes, and gaze distributions, and proposes an approach to generate 3D pseudo-labels and enhance model generalization.
Furthermore, traditional modality-specific models, designed separately for images or videos, limit the effective use of available training data. To overcome this, we propose the Gaze Transformer (\model), a modality-agnostic architecture capable of simultaneously learning static and dynamic gaze information from both image and video datasets. By combining 3D video datasets with 2D gaze target labels from gaze following tasks, our approach achieves the following key contributions:
(i) Significant state-of-the-art improvements in within-domain and cross-domain generalization on unconstrained benchmarks like Gaze360 and GFIE, with notable cross-modal gains in video gaze estimation;
(ii) Superior cross-domain performance on datasets such as MPIIFaceGaze and Gaze360 compared to frontal face methods.
Code and pre-trained models will be released to the community.
\end{abstract}
\vspace{-0.5cm}




%\maketitle

%%%%%%%%% BODY TEXT
\input{sections/1-introduction}
\input{sections/2-related-work}
\input{sections/3-approach}
\input{sections/4-experiments}
\input{sections/5-conclusion}

\clearpage
{
    \small
    \bibliographystyle{ieeenat_fullname}
    \bibliography{reference}
}

% WARNING: do not forget to delete the supplementary pages from your submission 

%%%%%%%%% Merge with supplemental materials %%%%%%%%%%


\twocolumn[{%
 \centering
 \LARGE \papertitle \\[1em]
 \large Supplementary Material\\[1em]
}]

%\begin{center}
%\textbf{\large Supplemental Materials: Modality Agnostic 3D Gaze Estimation in the Wild}
%\end{center}

%%%%%%%%% Merge with supplemental materials %%%%%%%%%%
%%%%%%%%% Prefix a "S" to all equations, figures, tables and reset the counter %%%%%%%%%%
\renewcommand*{\thesection}{\Alph{section}}
\setcounter{section}{0}
\setcounter{equation}{0}
\setcounter{figure}{0}
\setcounter{table}{0}
%\setcounter{page}{1}
\makeatletter
\renewcommand{\theequation}{S\arabic{equation}}
\renewcommand{\thefigure}{S\arabic{figure}}
\renewcommand{\thetable}{S\arabic{table}}

%\renewcommand{\bibnumfmt}[1]{[S#1]}
%\renewcommand{\citenumfont}[1]{S#1}
%%%%%%%%%% Prefix a "S" to all equations, figures, tables and reset the counter %%%%%%%%%%

\input{sections/A-supplement}


\end{document}

