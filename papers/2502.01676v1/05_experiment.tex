\section{Experiments and Results}

\input{tables/results}

\subsection{Toxicity Detection Performance and Analysis} 
\label{sec:experiment_performance}

\paragraph{Evaluation Metric.} Because of the in-balanced labels in the testing set, we report Precision, Recall, F1, Accuracy scores. Furthermore, we report Cohen's Kappa between the human label and each model performance.


\paragraph{Toxicity Detection Model.}
The toxicity detection models have predict very low probability of being toxic for all the sentences: the mean value cross the entire testing set is 0.03 and the max value is only 0.32. These probability is much lower than a threshold (e.g. 0.4) used in previous work. Meaning that the toxic semantic meaning in the peer-review context is largely different from the general domain. It is because usually a toxic peer-review does not contain overtly hostile or abusive language, which is often a defining characteristic of toxic reviews in general domain. Therefore, while it might be seen as problematic or unprofessional, it is not detected as ``toxic'' by a general toxic detection model. This emphasize the need for a new dataset and a specific model or model design for toxic peer-review detection task. 
% We further analyze the threshold of deciding the toxicity, and we start from 0.01 and add 0.01 every step until we reach the highest probability cross the entire dataset 0.32. 
% Figure~\ref{} shows the trend regarding the threshold and the Cohen's Kapa. 

\paragraph{Sentiment Analysis Model.}
We present the results of the two ways of toxicity prediction by a sentiment analysis model in Table~\ref{tab:detector_result}. 
The result show that the second way (i.e. the negative score is higher than both neural and positive) achieve higher alignment with human judgement. While the result does not reach a substantial alignment with human, it is even better than most of the open source LLMs which are at much larger parameters. 
This shows that negative sentiment is align with toxicity to some degree. 

\paragraph{Simple Prompt with LLMs.}
To prompt the LLMs, we first use a very simple and straightforward prompt as shown in Figure~\ref{fig:simple_prompt}. 
The results in Table~\ref{tab:detector_result} shows that Qwen-7B model achieve the best alignment with human compared to other open-source LLMs and even the close-source GPT-3.5 and GPT-4. 

\begin{figure}[h]
\lstset{frameround=fttt}
\begin{lstlisting}[frame=trBL,linewidth=1.01\columnwidth,breaklines=true,breakautoindent=false,breakindent=0pt,numbers=none]
Given a sentence from a peer-review, please judge if this sentence is toxic or not toxic.
Sentence: {sentence}
If it is toxic, generate label 1, otherwise, generate label 0. 1 or 0? Answer:
\end{lstlisting}
    \caption{Simple Prompt for LLMs to Conduct the Toxicity Detection Task.}
    \label{fig:simple_prompt}
\end{figure}

\input{tables/prompt_results}
\paragraph{Detailed Instruction with LLMs.} 
We provide more concrete definition of toxic peer-review sentence definition and extend the prompt with each subcategories definition. However, when given such detailed prompt to the open-source models, all models do not generate an intended answers (e.g. 0/1 or toxic/non toxic). Therefore, we do not present the performance for the open-source model with the detailed instruction. On the other hand, the close-source models, GPT-3.5 and GPT-4 can follow the instruction and achieve much better alignment compared to the previous simple prompt as shown in the first block performance in Table~\ref{tab:prompt_detector_result}. Encouragingly, the GPT-4 achieve 0.56 Cohen's Kappa score with human. 
Meanwhile, we also prompt the model to generate a confidence score of its answer. 
The min/max/mean values of the confidence is 70\%, 89\%, and 100\%, this shows that the model is quite confidence with its answer in most of the cases. 
We use the confidence to select the sentences to further compute the Cohen's Kappa Scores. 
As shown in Figure~\ref{fig:confidence}, choosing a higher confidence yield higher alignment and when the confidence is 100\%, the model reach a perfect alignment with the human judgement. 
However,  a higher threshold also means less sentences are being judged, the number of sentences being selected for the threshold shown in the figure are: 313, 300, 300, 258, 248, 53, 3. 

\begin{figure}
    \centering
    \includegraphics[width=0.95\linewidth]{images/confidence.png}
    \caption{The Cohen's Kappa of GPT-4 Regarding to the Confidence Threshold.}
    \label{fig:confidence}
\end{figure}





\paragraph{Toxicity Definition Instruction with LLMs.} 
The last prompt that we experiment is the a summary of the toxicity as shown Figure~\ref{fig:summary_prompt}. We compare this  result (bottom block) with the detailed instruction prompt result in Table~\ref{tab:prompt_detector_result}.
Both model performance decreases significantly, this showcase the importance of the detailed instruction for detecting the toxicity in the peer-review context. 

Meanwhile, we also prompt the model to generate a confidence score of its answer. 
The min/max/mean values of the confidence is 70\%, 89\%, 100\%, this shows that the model is quite confidence with its answer in most of the cases. 
We choose a high confidence threshold 90\% to select the cases to further compute the Cohen's Kappa Scores, 


\begin{figure}[h]
\lstset{frameround=fttt}
\begin{lstlisting}[frame=trBL,linewidth=1.01\columnwidth,breaklines=true,breakautoindent=false,breakindent=0pt,numbers=none]
Peer review is vital to the scientific process, but feedback can sometimes have a negative emotional impact on paper authors. To be specific, sentences which feature emotive, rhetorical, narrativizing, universalizing, and/or subjective language, comments which lack substance, helpfulness, or specific, actionable guidance for improvement, personal attacks or author-focused critiques, and sentences which fixate excessively on the negative aspects of a paper are of particular concern.

\end{lstlisting}
    \caption{Toxicity Definition Summary}
    \label{fig:summary_prompt}
\end{figure}

\input{tables/detoxified_examples}
\input{tables/human_disagreement}
\subsection{Toxicity Revision}

We also investigate if LLMs are capable of rewriting toxic sentences while maintaining the original critiques, an important aspect of ensuring constructiveness in the scientific review environment. We sample 10 sentences from each toxicity subcategory (with exception that personal attack type only has 5 sentences in the entire testing set), and prompt (Figure~\ref{fig:revision_prompt}) GPT-3.5 to revise them. 
Then we evaluate the revisions and judge whether the re-written sentences are less toxic compared to the original ones. We find that the revisions are favorable 80\% of the time (28/35 revisions) suggesting that the model is generally competent. we have provided more examples in Table~\ref{tab:detoxified_sentence}. 
For most of the ``lack of constructive feedback'', the model simply rephrase the sentence such as revise the original sentence ``Major baselines are missing.'' to ``The paper would benefit from including major baselines for a more comprehensive analysis.''. 
While the constructiveness does not improve, the annotator still reports that the revision is more polite and thus more preferable. On the other hand, it is almost not possible to make a comment more constructive without reading the paper, therefore, we suggest that for this type of toxicity, rather than ask the model to rewrite the sentence, the model should remind the reviewer to give more constructive feedback. 
\begin{figure}[h]
\lstset{frameround=fttt}
\begin{lstlisting}[frame=trBL,linewidth=1.01\columnwidth,breaklines=true,breakautoindent=false,breakindent=0pt,numbers=none]
This text is from a scientific paper review:
{sentence}
Revise this sentence such that it maintains the original critique but delivers it in a more friendly, professional and encouraging manner. Make minimal changes to the original text.
Your revision: 
\end{lstlisting}
    \caption{Revison Prompt.}
    \label{fig:revision_prompt}
\end{figure}


\subsection{Human Disagreement}
During our annotations, although the annotators have conducted detailed discussion, there are still some cases that they did not reach agreement. We exclude those data point in our final testing set, however, investigating these examples further can potentially be beneficial to improve our toxicity guideline. 
In Table~\ref{tab:disagreement_sentence}, we present such cases and the different comments from the annotators.  






