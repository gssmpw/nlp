\section{Introduction}

Peer-review is essential to the editorial process and serves as the foundation of the publication system. 
It requires reviewers to have expertise in the research areas relevant to the submitted manuscripts. Reviewers are expected to provide constructive feedback in a polite and professional manner, facilitating a fair and productive assessment~\cite{mavrogenis2019evaluate,mavrogenis2020good}. 
A good peer-review can support and improve scientific communication and the dissemination of research.
However, the peer review process can inadvertently lead to the occurrence of toxic reviews if not handled with ethical consideration. Such reviews, characterized by harsh and unconstructive criticism, rude comments, and unprofessionalism, can severely impact authors, causing feelings of discouragement, anger, or even depression~\cite{baron1988negative,mavrogenis2020good}. This, in turn, may hinder scientific progress~\citep{ffdd70d0c1784f26aad78a19af959025,gerwing2020quantifying}. 
The prevalence of this issue is underscored by a survey in which 58\% of 1,106 participants reported having received an unprofessional review~\citep{silbiger2019unprofessional}.

Despite the importance of addressing toxicity in peer reviews, this area has received relatively little attention in current literature. Most efforts in the domain of toxicity detection are focusing on social media comments~\cite{cheng2022bias,hosseinmardi2015detection,ghosh2021detoxy,bensalem2024toxic}. As a result, there is a urgent need for systematic approaches to detect and mitigate toxic content in peer reviews.
Our research aims to fill this gap by introducing the first dataset to detect toxic sentences in peer reviews. 

To create a high-quality dataset, we begin by collecting a suit of reviews from the Open Review platform. 
One challenge is how to annotate the collected review since the definition of toxic review are not clearly documented. 
Therefore, we first define four characteristics of sentences that have high risk of being toxic by surveying a list of literature~\citep{silbiger2019unprofessional,mavrogenis2019evaluate,rogers2020can}, scientific interview~\footnote{\url{https://www.nature.com/nature-index/news/linda-beaumont-research-journals-should-take-action-against-toxic-peer-reviews}} and analyze the conference review gudeline such as ACL Rolling Review Guideline~\footnote{\url{https://aclrollingreview.org/reviewertutorial#6-check-for-lazy-thinking}},  which focus on the instruction of writing a good review and the clues of bad reviews. 
Then, five undergraduate annotators firstly identify potential toxic sentences from randomly selected reviews. 
Such sentences are further judged by senior researchers. 
To ensure the reliability of the dataset, the annotation procedure is conducted into two phases, and each phase consist of an independent judgement and a discussion among the annotators. Eventually only sentences with consenting from all annotators are included in our testing set. 
To this end, we have collect a high quality dataset with 313 sentences for testing. 

Using our dataset, we benchmarked multiple models, including toxicity detection model\footnote{We use the perspective API.}, sentiment analysis models~\cite{sanh2019distilbert}, and large language models (LLMs)~\cite{jiang2023mistral,touvron2023llama}, to perform toxic review detection task. We evaluated the toxic detection performance using precision, recall, F1 score, and Cohen's Kappa score, comparing these results with human judgments.
Our findings reveal that existing toxicity detection models struggle to accurately identify toxic content within the peer-review context. Conversely, a sentiment analysis model achieved higher alignment with human judgments compared to most open-source LLMs. Among the models tested, the closed-source model, particularly GPT-4, demonstrated the best performance. We further explored the impact of prompts on GPT-4's performance, discovering that detailed instructions significantly improve the model's performance and alignment with human judgments.
Additionally, we prompted the model to generate a confidence score with its predictions. Our results show that this confidence score is a reliable indicator of better alignment with human judgments, with a 7\% improvement in alignment when the confidence threshold is set at 95\%.
Beyond detection, we also explored the potential of LLMs to revise toxic sentences. We presented both the original and revised sentences to human evaluators, who reported that 80\% of the revisions were preferable, indicating a potential possibility of using LLMs to detoxify peer reviews.

Recognizing the absence of a formal definition for toxic reviews and the need for standardized preventative measures, our research aims to highlight the importance of this issue and contribute to the discourse by establishing key criteria for preventing toxic reviews. The four characteristics we have identified can serve as the foundation for developing guidelines in reviewer training programs, helping to preemptively mitigate toxic reviews or significantly reduce their occurrence. Furthermore, our experiments with LLM suggests their applications in post-review detection: they can identify potentially toxic language to a moderate degree. Ultimately, our goal is to foster a healthy and supportive research environment, enhancing both the mental well-being of individuals and the overall advancement of the research community.
To summarize our contribution in this paper: 
\begin{itemize}
\item We define four characteristic of toxic sentences in peer-review context as no previous work have provided comprehensive definition. We acknowledge that our definition might not be the most precise and can be improved further, but if a sentence falls into any of the categories, have high potential risk of being toxic. 
\item We develop the first benchmark dataset for detecting toxic sentences in peer-review context. This dataset follows rigorous human annotation to ensure its reliability.
\item We benchmark the performance of multiple models including existing toxicity classification model, sentiment analysis models, and advanced LLMs on our dataset and the results suggests that the open-source models are all struggling with our task. On the other hand, with proper instruction, the open-source models achieve acceptable agreement with human (e.g. 0.54 Cohen's Kappa Score). 
% \item Our research suggests some potential ways to improve the positive and health environment of research community. 
% \item We finally fine-tune multiple language models on our dataset which outperform the zeroshot baselines by X\%. We believe that our fine-tuned model will be beneficial for the community and will open-source the models and datasets upon the publication of this work.  
\end{itemize}