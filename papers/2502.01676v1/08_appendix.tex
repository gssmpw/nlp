\appendix
% \section{Agreement Between the Sub-categories}

% We further compute the agreement between the two annotators on the each categories of our final testing set. While the agreement between the toxic and non-toxic label is 1 (we only include those labels that belongs)


\section{Toxic Definition}

In Figure~\ref{fig:toxic_definition}, we show the four sub-categories of our toxic definition and examples. 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.95\linewidth]{images/toxic_definition.pdf}
    \caption{The Definition and Examples of Toxic Peer Review. }
    \label{fig:toxic_definition}
\end{figure}

\section{Conferences of the Reviews}

Our testing datasets are collected from 6 conferences or workshops and each one has the peer reviews public available on OpenReview Platform. 
In Table~\ref{tab:conference}, We list the conferences from which our testing set reviews were sourced.

\input{tables/conference}




\section{General Toxicity Detection Model}

As mentioned in \S\ref{sec:experiment_performance}, the toxicity detection model generates very low probabilities of toxicity for the sentences in our testing set. Figure~\ref{fig:toxic_distribution} illustrates the distribution of the predicted probabilities across the entire dataset, with the majority of values being less than 0.05. We further analyzed various threshold values to determine toxicity: if the predicted probability exceeds a given threshold, the sentence is classified as toxic; otherwise, it is classified as non-toxic. We began with a minimum threshold of 0.006, incrementing by 0.005 to establish subsequent thresholds. Figure~\ref{fig:threshold_toxic_model} depicts the relationship between the threshold and the Cohen's Kappa score. The maximum Cohen's Kappa score, approximately 0.25, falls within the range of [0.006, 0.06]. This suggests that potentially toxic peer-review sentences are generally assigned very low toxicity probabilities, indicating that the semantic interpretation of toxicity in peer review differs significantly from that in the general domain on which the toxicity model was trained.



\begin{figure}
    \centering
    \includegraphics[width=0.95\linewidth]{images/toxic_model_distribution.png}
    \caption{The Distribution of the Predicted Toxicity Probability Given by An Toxic Detection Model.}
    \label{fig:toxic_distribution}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.95\linewidth]{images/perspective_threshold.png}
    \caption{Enter Caption}
    \label{fig:threshold_toxic_model}
\end{figure}

\section{GPT-4 high Confidence Examples} 

As we mentioned in the main paper that the confidence is an good indicator for achieving good alignment with human judgement. Here we show the three examples where GPT-4 is 100\% confidence its answer and all of them are aligned with human judgement. 
Example 1 (Non-toxic): The random features are fixed once sampled from the base measure of the corresponding kernel.
Example 2 (Toxic): In short, the paper is not properly written nor well organized; is hard to read with vague contributions and vague positioning with respect to the state of the art.
Example 3 (Non-Toxic): The major experimental evaluations (Fig. 2 and Fig. 3) are based on the $m^2$ coverage after k steps and the plots are cut at 1000 steps.