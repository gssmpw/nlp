\section{Paper Review Toxicity Dataset}

\subsection{Paper Review Collection }
To collect paper reviews, we utilized the OpenReview API, starting with a comprehensive list of all venues between 2018 and 2023, including conferences and workshops. 
% The diversity of schemas used by different conferences on OpenReview posed a challenge for data collection. 
% To address this, we focused on the most common schemas across various conferences, enabling us to maximize our data collection 
Consequently, we successfully collected 50,108, spanning 47 unique conferences and workshops 
Among the total collection, we randomly sample 1,495 for the annotations, and 
on average, each review consisted of 1,553 characters. However, due to the heavy human annotations work, we are not using all the processed review. The distribution of the final testing set will be discussed in \S\ref{sec:human_annotation}.

\subsection{Toxicity Review Guideline}
\label{sec:guideline}

Many journals have guidelines and codes of conduct for peer-reviewers,  but the unacceptable behaviour are often difficult to be found on journal websites~\footnote{\url{https://www.nature.com/nature-index/news/linda-beaumont-research-journals-should-take-action-against-toxic-peer-reviews}}. 
ACL Rolling Review is an exception, where a list of lazy review behavior have been mentioned~\footnote{\url{https://aclrollingreview.org/reviewertutorial#6-check-for-lazy-thinking}}. Nevertheless, ``toxic'' behavior have not been clear defined and nevertheless become a norm of peer-review. 
Therefore, from multiple different sources (professional interview, scientific paper, human studies), we have summarized the unacceptable behaviors as follows to define a sentence that can be toxic.  

\paragraph{Emotive Comments.}  
Using emotive or sarcastic language is often the hardest for authors to cope with~\footnote{\url{https://www.nature.com/articles/d41586-020-03394-y}}. 
Such language include the use of the following words or phrases: speaker-oriented adverbs such as surprisingly, obviously and disappointingly, subjective adjectives (e.g. careless), depreciatory modifiers (e.g. even, just), unnecessary expression including narrativizing: (e.g. , ``At this point, I almost stopped reading''), rhetorical questioning (for example, ``Did the authors even read the submission guidelines?''), universalizing (for example, ``As anyone/everyone/any expert knows''), speculating (for example, ``I bet the outlier observations were omitted''), expressive punctuation, such as exclamation marks or scare quotes (for example, ``This is not correct!'').
Similarly, in a professional interview, an experienced journal editor has mentioned that a comment with personal emotion can be toxic, such as using the world ``silly'', or phrase `` they have no idea what they’re doing''. 

\paragraph{Lack of Constructive Feedback.} 
Refers to a situation where feedback provided to someone lacks substance, helpfulness, or guidance for improvement.
Instead of offering specific suggestions, actionable advice, or positive reinforcement, this type of feedback may be vague, overly critical, or unproductive. 
Example1: \textit{``Comparison in experiments looks meaningless.''}
Example2: \textit{``The analysis is shallow.''}
In addition, if the criticism is lack of reference or citations, such sentences are falling into this categories. Example3: \textit{``The paper is not well written and organized.''} 
The reviewer should refer to what problems happens, an alternative sentence with the same criticism can be \textit{``The paper is not well written and organized, examples will be given below.''}

\paragraph{Personal Attack.} 
are directed at the author(s) rather than the nature or quality of the work~\cite{silbiger2019unprofessional}.  Example1: \textit{``The author’s previous work has been unreliable, and this paper is no different. They should find a different career.''} Example2: \textit{``Perhaps the authors are not used to submitting to this conferences as the paper lacks some essential components to it.''}


\paragraph{Excessive Negativity.} A sentence that overly emphasizes flaws of a paper without acknowledging any merits, which can contributes to a toxic review environment. This usually happens in the summary or conclusion of a review.
Example1: \textit{"The paper is not properly written nor well organized; is hard to read with vague contributions and vague positioning with respect to the state of the art."} 
Example2: \textit{``This paper has no significance."}
Example3: \textit{``This paper cannot be revised so as to be suitable for our journal.''}

\subsection{Human Annotation}
\label{sec:human_annotation}
We designed a rigorous annotation process to ensure the reliability of our dataset. Initially, five undergraduate students majoring in computer science identified toxic sentences. These annotations were then verified by three senior researchers. To further enhance agreement among the senior researchers, we conducted a two-phase annotation process: first with 50 sentences, followed by a larger set of 300 sentences.
In each phase, the annotation process was consistent: each annotator independently rated the sentences, after which they discussed any sentences where there was disagreement and could adjust their ratings if necessary. Conducting a smaller set of annotations served two purposes: first, it helped the annotators reach substantial agreement before moving on to the more time-consuming larger set; second, based on Cohen's Kappa score, the two annotators with the highest agreement were selected to annotate the larger set of 300 sentences.
In the initial round of 50 sentences, the Cohen's Kappa scores among the three annotators were 0.74, 0.52, and 0.34, with a Fleiss' Kappa score of 0.55. After discussing the sentences with disagreements, the Fleiss' Kappa score increased to 0.63, and the highest Cohen's Kappa score between two annotators rose to 0.83. The two annotators with the highest Kappa scores were then assigned an additional 300 sentences. In this larger set, the initial Cohen's Kappa score was 0.60, which significantly increased to 0.92 after discussion.
In the experimental section, we will analyze the sentences where the two reviewers failed to reach a final agreement. Ultimately, we combined 36 sentences where all three annotators agreed in the first round and 277 sentences where the two selected annotators agreed in the second round, resulting in a testing set comprising 182 non-toxic and 131 toxic sentences (Table \ref{tab:statistic}). 

\input{tables/toxic_dataset_statistic}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{images/decision_1.png}
    \caption{Distribution of Six Decision Categories of the Reviews of the \underline{Entire Testing Data}.}
    \label{fig:review_categories_1}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{images/decision_2.png}
    \caption{Distribution of Six Categories of the Reviews of the \underline{Toxic Sentences}.}
    \label{fig:review_categories_2}
\end{figure}

\paragraph{Review Distribution.}
% \begin{figure}
%     \centering
%     \includegraphics[width=0.35\textwidth]{images/decision_1.png}
%     \caption{Distribution of Five Decision Categories of the Reviews the \underline{Entire Testing Data}.}
%     \label{fig:review_categories_1}
%     \vspace{0.8cm} % Adjust this value to reduce the space between figures
%     \includegraphics[width=0.35\textwidth]{images/decision_2.png}
%     \caption{Distribution of Five Categories of the Reviews of the \underline{Toxic Sentences}.}
%     \label{fig:review_categories_2}
% \end{figure}

% The reviews of those sentences included in our finial testing set are grouped in five categories: Strong Reject, Moderate Reject, Weak Reject, Weak Accept, Moderate Accept, Strong Accept. The distribution of each categories for our final testing set is shown in Figure~\ref{fig:review_categories_1}. 
% We also show the distribution of the decision categories of only toxic sentences in Figure~\ref{fig:review_categories_2}. The distribution shift toward more reject decision. 
% This suggests that reviewer who give a reject decision should be more cautious about their decision 

The reviews of the sentences included in our final testing set are categorized into five groups: Strong Reject, Moderate Reject, Weak Reject, Weak Accept, Moderate Accept, and Strong Accept. The distribution of these categories within our final testing set is illustrated in Figure~\ref{fig:review_categories_1}. Additionally, Figure~\ref{fig:review_categories_2} shows the distribution of decision categories specifically for toxic sentences, revealing a shift toward more reject decisions. Even though sentences are analyzed individually, this trend suggests that reviews with overall reject decision receive more toxic comment which is well align with our expectation but could be extremely discouraging for the authors.

\paragraph{Subcategories Distribution.}
We present the distribution of  toxic-subcategories in Figure~\ref{fig:toxic_categories}.
Note that a toxic sentence could belong to multiple categories. 
The most common category is ``lack of constructive feedback'', follows by ``Excessive Negativity'', ``Emotive Comments'', and ``Personal Attack''. 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{images/toxic_categories.png}
    \caption{The Distribution of  Toxic-Subcategories.}
    \label{fig:toxic_categories}
\end{figure}

% and the second highest type is weak reject. We then take a closer look at these two subsets regarding the each categories of the toxic review.  

% \begin{figure}
%     \centering
% \includegraphics[width=0.70\linewidth]{images/decision_1.png}
%     \caption{Distribution of Five Decision Categories of the Reviews the Entire Testing Data.}
%     \label{fig:review_categories}
% \end{figure}
% \begin{figure}
%     \centering
% \includegraphics[width=0.70\linewidth]{images/decision_2.png}
%     \caption{Distribution of Five Categories of the Reviews of The Toxic Sentences.}
%     \label{fig:review_categories}
% \end{figure}



% \begin{figure}
%     \centering
%     \begin{minipage}{0.45\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{images/decision_1.png}
%         \caption{Distribution of Five Decision Categories of the Reviews the \underline{Entire Testing Data}.}
%     \end{minipage}
%     \hspace{3cm} % Adjust this value to reduce the space
%     \begin{minipage}{0.45\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{images/decision_2.png}
%         \caption{Distribution of Five Categories of the Reviews of the \underline{Toxic Sentences}.}
%     \end{minipage}
% \end{figure}


\paragraph{Discussion Among the Annotators.} 
The main discussion focuses on two key questions: 1) how to differentiate between assertive tones and emotive language, and 2) what constitutes a ``lack of constructive feedback.''

For the first question, some ambiguous sentences include ``Would a superior method here not just be to take this brain information directly?'' and ``The authors lack any clear discussion on how their work is directly relevant to RRL.'' In the first example, one annotator identified "just" as a depreciatory modifier, which falls under the category of \underline{Emotive Comments} (See \S\ref{sec:guideline}). Removing this word does not change the criticism. In the second example, the word ``any'' was discussed, with one annotator noting that removing it would weaken the criticism, categorizing it as an assertive tone. After discussion, the annotators agreed that if a word does not belong to any of the categories outlined in \underline{Emotive Comments} and its removal weakens the criticism, it should be classified as assertive rather than toxic.

For the second question, some ambiguous sentences were ``The proposed copy mechanism is not clear,'' ``The technical contribution of the paper is limited,'' and ``Besides, adding the binary feature in the embedding is not necessary; the LSTM model could learn such sequential correlation.'' The first two examples refer to broad aspects without providing enough detail for the authors to make improvements. In the third example, although the criticism initially seems valid, it is not supported by evidence. Therefore, we concluded that criticisms lacking helpful suggestions for future improvement or not supported by reasons should be considered non-constructive.


\paragraph{Remark.} 
We acknowledge that due to the lack of formal definition of toxicity and few literature are available, our toxicity definition in \S\ref{sec:guideline} can raise different opinion among researchers.  
However, we believe that if a review sentence falls into any categories in our toxicity guideline, it should attract extra attention and the reviewer who writes the sentence should be extra cautious. Furthermore, due to the subject nature of toxicity, the annotations can suffer from inconsistency, therefore, we have designed two stages of annotations and only include those sentences all reviewers are agreed in our final testing set.  


% \subsection{Toxicity Classification Task} 

% \paragraph{Task Formulation.}
% In the classification task, the goal is to classify a sentence as either non-toxic or toxic. Any sentence that belongs to one of the five categories (\S\ref{sec:guideline}) will be classified as toxic. Note that, while we have the five sub-categories of toxicity, we decided to formulate the toxicity classification task as a binary classification for simplicity. Next, we will describe two methods of annotation.


% \paragraph{Data Generation by Human Annotation.}
% We distribute a unique set of sentences to each annotator (five in total). 
% In both the test and training sets, the annotation is completed in two steps: annotate and verify.  
% Specifically, an annotator assigns a label, and then another annotator is called to verify this label.  
% Additionally, when annotators felt unsure about a specific sentence, they had a "consult" category to ask the other annotators to help with the classification. 
% Ultimately, the test set showed a break down of 2839 non toxic sentences to 960 toxic sentences. The annotators marked 26.07\% of sentences toxic. 


% \paragraph{Data Generation by GPT-4 Annotation with Human Verification.} 

% While we mainly depend on human annotation, 
% human annotation is extremely time consuming, and inspired by the success of LLM on annotating data~\cite{he2024if}, we investigate a more efficient way to annotation using GPT-4. 
% We provide an entire review to the GPT-4 API and ask it to output toxic sentences, which are then verified by humans. This process speeds up the annotation process. We use the following prompt along with the review.
% \begin{lstlisting}[linewidth=\columnwidth,breaklines=true,breakautoindent=false,breakindent=0pt]
% Identify sentences which might have a needlessly negative impact on the paper authors and might accordingly benefit from re-wording/re-phrasing. 
% \end{lstlisting}
% \label{fig:prompt1}

% Under both human and machine-human-in-the-loop annotations, we finally construct a toxicity review detection and revision benchmark with the statistic shown in Table~\ref{tab:statistic}. 

% \paragraph{Evaluation Metrics}
% Since the labels are unbalanced, we measured macro F1, precision,  recall for each class. We also use accuracy for standard purpose. 



% \subsection{Detoxify Generation Task} 

% \paragraph{Task Formulation.}
% Given a toxic sentence from a review, the goal is to detoxify the sentence without adding unfaithful content. 

% \paragraph{Data Generation.} We construct a detoxified sentence set for training purposes using a machine-human-in-the-loop method. Specifically, given a toxic sentence, we first prompt ChatGPT to revise it using the prompt below. Then, an annotator reviews the reworded sentence. Only those revisions that improve sentiment while maintaining the original sentence's critical content are approved. Revisions that do not meet this threshold or contain errors or hallucinations are rejected. We provide 2233 toxic sentence revisions, and 1402 human-reviewed revisions remain in the training set.
% For the testing set, we only provide the toxic sentences (i.e. 960) but not the revised version, as there could be many way to revise, simply provide one revision will not be sufficient for the evaluation. 

% \begin{lstlisting}[linewidth=\columnwidth,breaklines=true,breakautoindent=false,breakindent=0pt]
% This text was found in a Paper Review.
% Text: "{sentence}"
% Reword this sentence such that it delivers the critism in a friendly but professional and encouraging manner. 
% Make minimal changes to the original text.
% Reworded Sentence:
% \end{lstlisting}
%     % \caption{Our prompt for generating synthetic data with GPT-4.}
% \label{fig:prompt2}

% \paragraph{Evaluation Metric.}
% We employ human to compare the original toxic sentence and revised sentence and select the sentence by 
% Besides, we also design two automatic judgement.
% Given an original toxic sentence and a detoxified sentence, we ask ChatGPT which is less toxic. The prompt of ChatGPT is given in Appendix. 
% Furthermore, we also use our fine-tuned toxic detection model (see \S\ref{sec:methods}) to obtain the toxic rate of the revised sentence,
% a lower rate indicates less toxic exhibit in the revised sentence.  



