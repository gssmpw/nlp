\section{Conclusion and Future Work}
Our work is the first to explore toxicity in peer review. Lacking a comprehensive guideline, we first defined toxicity and identified four key toxic categories. We developed a two-stage annotation process, ensuring our toxicity detection annotations are reliable and our final testing dataset includes only sentences with unanimous agreement among annotators. 
We then benchmarked various models, including a general toxic detection, a sentiment analysis model, and both open-source and closed-source large language models. Our results suggest that Open-source models struggled to align with human judgments, highlighting the challenge of detecting toxicity in peer reviews. Conversely, closed-source models like GPT-3.5 and GPT-4 showed much better alignment, suggesting their potential with careful use. Future work could involve using these models to generate synthetic data for fine-tuning open-source models.

\input{limitation}