\section{Experiments and Results}

% \begin{itemize}
% \item
% tables, ablation studies, test set description??
% \end{itemize}




\subsection{Detection Task Results and Analysis}

\paragraph{Zero Shot Results and Analysis}
The evaluation of models' zeroshot performance is presented in Table~\ref{tab:detector_result}. 
The zero-shot performance of the models we tested correlates intuitively to model size, with ChatGPT, the largest, having reasonably strong zero-shot performance, and Zephyr, with 3 billion parameters, having the weakest. However, Llama 3, far smaller than ChatGPT at only 8 billion parameters, achieved the best zero-shot performance among the LLMs that we tested, surpassing ChatGPT's macro F1 performance by 5.1\%. 

\paragraph{Fine-tuning Results and Analysis}
Both LoRA and Full fine-tuning results are showcased in Table~\ref{tab:detector_ft_result}. 
All fine-tuned models improved by at least 7.9\% relative to their zero-shot performances, with Zephyr 3B experiencing the greatest degree of improvement. Though it initially lagged behind other models, Zephyr gained nearly 20\% in terms of macro F1. Among fine-tuned models, Mistral 7B achieved the best performance, with a macro F1 of 0.87. Results seem to show that even smaller large language models can achieve similar and reasonably high performance at the task of toxic sentence classification. That said, all fine-tuned models remain more confident with non-toxic sentences than toxic sentences.

\input{tables/fine_tune_results}

\paragraph{Compare Traditional ML with LLM Fine-tuning Performance}

A random forest trained on manually-specified features extracted by the fine-tuned DeBerta-v3 model performed well, particularly as it is a technique which did not require fine-tuning. It scored consistently well in precision and recall on both the toxic class as well as on the macro level, surpassing the macro F1 zero-shot performance of all LLMs, with the exception of Llama 3 8B (0.831 vs 0.840). However, this technique did not beat the fine-tuned models.

Using the features extracted from the DeBerta-v3 model, we were able to analyze our annotations visually on the basis of key words and phrases associated with our annotation scheme. This allowed for some visual confirmation of annotation consistency, as well as a sense of how distinct various categories might be for a language model in general. In Figure 1, we show a PCA-2 representation of our human-identified training data with our final toxic and non-toxic labels. The two Principal Component axes visualized account for roughly 44\% of the variance in the data, PC1 being associated primarily with phrases 'blunt criticism' and 'negative judgement', and negatively associated with phrases like 'encouraging, constructive, specific', while PC2 is associated with terms such as 'specific feedback' and 'constructive criticism'.
\begin{figure}[ht]
    \centering
    \includegraphics[width=1\linewidth]{images/PCA2_toxic-non.png}
    \caption{PCA-2 Representation of Human-Classified Training Sentences.}
    \label{fig:PCA2 - toxic vs not}
\end{figure}

% A PCA-3 representation was particularly informative during the annotation process because it allowed us to visually confirm that a variety of annotation sub-classes were grouped appropriately. It also allowed us to predict that sub-classes with higher overlap might prove most difficult for our models.

The groupings in the PC visualizations indicated that our annotation scheme was internally consistent, even with many distinct subcategories. 

\subsection{Revision Task Results}
\input{tables/results}
\paragraph{Zero Shot Results and Analysis}

We present the Zero-shot performance in Table~\ref{gen_zeroshot_result}. It shows that revisions tend to have significant qualitative drawbacks in our evaluation, both in style and in content. In terms of style, their rephrasings are often very verbose, frequently being at least twice as long as the original sentence. Maintaining consistent formatting also seems to be very challenging, and base models will, more often than not, modify numbers without cause. Additionally, they will at times try to be more helpful or make more suggestions than would be appropriate or make sense in the context of a paper review. The content of critiques will occasionally be tweaked subtly, such that it may not convey exactly what was intended by the author. Furthermore, the revisions will often make some of the same mistakes made in the original review; some revisions maintain a lack of an explicitly constructive tone that was present in the original review. A few examples of these issues are present in Table~\ref{tab:detoxified_sentence}.

\input{tables/zeroshot_gen_task}

\paragraph{Fine-tuning Results and Analysis}

The results of this evaluation are presented in Table~\ref{tab:gen_ft_result}. We conducted an evaluation of sentence revision performance across all revision models on a common set of 100 test set sentences. We either 'Approved' or 'Rejected' a model's revision based on a few standards; If a revision accurately conveyed the original critique, and altered the tone to be more encouraging or constructive than the original sentence in a way that makes sense in the paper review context, it was Approved, otherwise Rejected. Similarly, we prompted GPT-4 to approve or reject the revised sentences on the basis of these criteria as well (Full prompt in Appendix).

\input{tables/fine_tune_gen_task}

In the estimation of both we human reviewers, as well as GPT-4, GPT-3.5 produced the highest quality revisions, which gives us some additional confidence that it was a good choice for the creation of our revision dataset. However, the other results were not so clear. Interestingly, while Zephyr's revisions improved by 15 points out of 100 through fine-tuning according to our human review, and Mistral experienced an improvement, though a milder one, in the estimation of GPT-4, the inverse was not true; Human review determined that Mistral's revision quality decreased substantially as a result of fine-tuning, while GPT-4 found that Zephyr's decreased by 5 points. One observation for why this might be the case is that the fine-tuned Mistral model in particular tended to hallucinate frequently when processing numbers, acronyms, or unfamiliar names. For example, when presented with the sentence

\textit{"The De-Identification Quality metric which you use would obviously get the optimal 20\% accuracy here as the we have no way of associating images of brains with images of faces, and even if we do, you have not demonstrated that you method would perform well on it either."}

\noindent Mistral's revision was

\textit{"The evaluation metric used, Deferred Identification, would achieve a 70 percent accuracy score in this scenario, as it is challenging to identify images with brain structures as having a specific facial expression. It would be beneficial to further explore the performance of the method on this task."}

Zephyr, on the other hand, has generally shown greater strength at learning to maintain specifics and formatting in the fine-tune. Some examples can be viewed in Table 7.

In order to get a better sense of whether our fine-tuned models' revisions represented a reasonable degree of improvement over the base models' revisions, we conducted an additional experiment with GPT-4. For both Mistral and Zephyr, we randomly selected 500 pairs of corresponding base and fine-tuned revisions. For a given pair, we randomized the order to avoid bias, and prompted GPT-4 to select which of the two sentences, "from scientific paper reviews," "was more conducive to a productive and encouraging review environment." (Full prompt in Appendix A) With this context, GPT-4 selected Mistral's fine-tuned revisions over its base model revisions 89.0\% of the time, and Zephyr's fine-tuned revisions 87.8\% of the time (Table 6). These strong results are likely due to the fact that this comparison does not attempt to filter for hallucinations or whether a revision correctly conveys a particular critique. That said, this result highlights the success of the fine-tuning process in shaping a productive and encouraging tone in both of these models.
\input{tables/detoxified_examples}