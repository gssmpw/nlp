\section{Benchmarks}
\label{sec:methods}
% \subsection{Random Forest Classifier}
% To ensure annotation consistency, a manually-directed feature extraction method was implemented. We manually selected several dozen keywords/phrases which corresponded to a variety of tones of emotional impact and professionalism, such as, ``not helpful'', ``specific feedback'', ``active language'', and ``personal attack''.  A fine-tuned DeBerta-v3 model (MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli \cite{laurer2022less}) was used to generate feature values for each sentence in our human-annotated dataset based on these keywords and phrases. We utilized Principal Component Analysis (PCA) to reduce the features to a 2- and 3-dimensional space. This allowed for intuitive visualization to assess the consistency of our annotations. The first three Principal Components (PCs) explained 49.15\% of the variance in the data (PC1: 32.98\%, PC2: 11.45\%, PC3: 4.72\%). Additionally, these features served as inputs to a random forest classifier to predict sentence toxicity.  

\paragraph{Task Formulation.}
Based on our created dataset, we formulate a binary classification task for toxic detection in the peer-review. The goal is to classify a sentence as either non-toxic or toxic. Any sentence that belongs to one of the four categories (\S\ref{sec:guideline}) will be classified as toxic. 
Note that, while we have the four sub-categories of toxicity, we decided to formulate the toxicity classification task as a binary classification for simplicity and more fine-grained classification will be an future work. 


\subsection{Toxic Detection Models} 


% \textcolor{red}{Give some details about perspective API and justify that it has been widely used in toxic detection in general domain}
To understand whether existing automatic toxicity classifiers could be used to detect toxicity in peer review, we evaluate text in our dataset using the Perspective API\footnote{\url{https://perspectiveapi.com/}}. The Perspective API has been widely used in previous studies on automatic toxicity detection \cite{deshpande2023toxicity,faal2023reward,wen2023unveiling,howard2024uncovering} and outputs scores in the range of $(0,1)$ which quantify the likelihood of text containing various attributes related to toxicity. 
The model output 8 aspects: ``identity attack'', ``severe toxicity'',``profanity'', ``sexually explicit'',  ``toxcity'', ``threat'', ``insult'', and ``flirtation''.
We specifically use the Toxicity attribute returned by the Perspective API.

\subsection{Sentiment Analysis Models} 

As toxic peer reviews are also negative in nature, our task can be viewed as related to the problem of sentiment analysis~\cite{socher2013recursive}.
Consequently, we use a pre-trained sentiment classifier\footnote{\url{https://huggingface.co/lxyuan/distilbert-base-multilingual-cased-sentiments-student}} as another baseline for automatic detection of toxic reviews. The classifier returns separate scores in the range of $(0,1)$ quantifying the probability of a text sequence containing a positive, neutral, or negative sentiment. 
We use two way to decide if a sentence is toxic or not. 
SA-1: a sentence is toxic if the negative score is higher than the positive score, otherwise the sentence is non-toxic. 
SA-2:  a sentence is toxic if the negative score is higher than both positive and neutral score.

% \subsection{Large Language Model}

% We fine-tuned LLMs on both toxic classification and detoxification task. We give different instruction to distinguish each task as shown in the following. \\ 

% \noindent For classification:

% \begin{lstlisting}[linewidth=\columnwidth,breaklines=true,breakautoindent=false,breakindent=0pt]
% You are an assistant helping with paper reviews. 
% You will be tasked to classify sentences as 'J' or 'V'
% 'J' is positive or 'J' is encouraging. 
% 'J' has a neutral tone or 'J' is not discouraging and not encouraging.
% 'V' is overly blunt or 'V' contains excessive negativity and no constructive feedback.
% 'V' contains an accusatory tone or 'V' contains sweeping generalizations or 'V' contains personal attacks.
% Text: "{sentence}"
% Please classify this text as either 'J' or 'V'. Only output 'J' or 'V' with no additional explanation.
% \end{lstlisting}
%     % \caption{Our prompt for classification fine-tuning.}
% \label{fig:prompt3}

% \noindent For revision:

% \begin{lstlisting}[linewidth=\columnwidth,breaklines=true,breakautoindent=false,breakindent=0pt]
% You are an assistant that helps users revise Paper Reviews.
% Paper reviews exist to provide authors of academic research papers constructive criticism.
% This is text found in a review.
% This text was classified as 'toxic'
% Text: "{sentence}"
% Please revise this text such that it maintains the criticism in the original text and delivers it in a friendly but professional manner. Make minimal changes to the original text.
% \end{lstlisting}
%     % \caption{Our prompt for revision fine-tuning.}
% \label{fig:prompt4}

% We fine-tuned Zephyr-3B~\cite{tunstall2023zephyr} and Mistral 7B~\cite{jiang2023mistral}  using Parameter-Efficient Fine-Tuning (PEFT) techniques, specifically Low-Rank Adaptation (LoRA)~\cite{hu2021lora} and 4-bit training.  
% We applied distributed training on four NVIDIA RTX 3070s. 
% This set up allowed us to handle the large memory requirements. 
% % For the initial checkpoint evaluation, we tested the model on a smaller validation set comprising 500 toxic and 500 non-toxic sentences. 
% % To assess the improvement in toxicity, we used the model to revise the toxic sentences, and then asked the model to classify the revised sentence.
\subsection{Large Language Models} 
Large language models (LLMs) have shown significant promise in various natural language processing tasks. 
Therefore, we also extensively evaluate the state-of-the-art LLMs' performance on detecting toxic sentences in peer-review. We evaluate both open-source and closed-source models, covering a range of model sizes. For the open-source models, we evaluate Gemma-7B~\cite{team2024gemma}, Qwen-7B~\cite{bai2023qwen}, Mistral-7B~\cite{jiang2023mistral}, LLaMA3-8B, and LLaMA3-70B~\cite{touvron2023llama}, utilizing the model checkpoints available on the Hugging Face platform. For the closed-source models, we assess both GPT-3.5 and GPT-4.




