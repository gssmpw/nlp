[
  {
    "index": 0,
    "papers": [
      {
        "key": "nichol2021improved",
        "author": "Nichol, Alexander Quinn and Dhariwal, Prafulla",
        "title": "{Improved denoising diffusion probabilistic models}"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "kong2020diffwave",
        "author": "Kong, Zhifeng and Ping, Wei and Huang, Jiaji and Zhao, Kexin and Catanzaro, Bryan",
        "title": "{DiffWave: A Versatile Diffusion Model for Audio Synthesis}"
      },
      {
        "key": "tae2022editts",
        "author": "Jaesung Tae and Hyeongju Kim and Taesu Kim",
        "title": "{EdiTTS: Score-based Editing for Controllable Text-to-Speech}"
      },
      {
        "key": "Shen2023NaturalSpeech2L",
        "author": "Kai Shen and Zeqian Ju and Xu Tan and Yanqing Liu and Yichong Leng and Lei He and Tao Qin and Sheng Zhao and Jiang Bian",
        "title": "{NaturalSpeech 2: Latent Diffusion Models are Natural and Zero-Shot Speech and Singing Synthesizers}"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "ho2022video",
        "author": "Ho, Jonathan and Salimans, Tim and Gritsenko, Alexey A and Chan, William and Norouzi, Mohammad and Fleet, David J",
        "title": "{Video Diffusion Models}"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "saharia2022photorealistic",
        "author": "Saharia, Chitwan and Chan, William and Saxena, Saurabh and Li, Lala and Whang, Jay and Denton, Emily and Ghasemipour, Seyed Kamyar Seyed and Ayan, Burcu Karagol and Mahdavi, S Sara and Lopes, Rapha Gontijo and others",
        "title": "{Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding}"
      },
      {
        "key": "ramesh2022hierarchical",
        "author": "Ramesh, Aditya and Dhariwal, Prafulla and Nichol, Alex and Chu, Casey and Chen, Mark",
        "title": "{Hierarchical text-conditional image generation with clip latents}"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "li2022diffusion",
        "author": "Li, Xiang Lisa and Thickstun, John and Gulrajani, Ishaan and Liang, Percy and Hashimoto, Tatsunori B",
        "title": "{Diffusion-LM Improves Controllable Text Generation}"
      },
      {
        "key": "gulrajani2023likelihoodbased",
        "author": "Ishaan Gulrajani and Tatsunori Hashimoto",
        "title": "{Likelihood-Based Diffusion Language Models}"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "lou2024discrete",
        "author": "Lou, Aaron and Meng, Chenlin and Ermon, Stefano",
        "title": "{Discrete diffusion modeling by estimating the ratios of the data distribution}"
      },
      {
        "key": "richemond2022categorical",
        "author": "Richemond, Pierre H and Dieleman, Sander and Doucet, Arnaud",
        "title": "{Categorical SDEs with Simplex Diffusion}"
      },
      {
        "key": "gong2024scalingdiffusionlm",
        "author": "Shansan Gong and Shivam Agarwal and Yizhe Zhang and Jiacheng Ye and Lin Zheng and Mukai Li and Chenxin An and Peilin Zhao and Wei Bi and Jiawei Han and Hao Peng and Lingpeng Kong",
        "title": "{Scaling Diffusion Language Models via Adaptation from Autoregressive Models}"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "karimi-mahabadi-etal-2024-tess",
        "author": "Karimi Mahabadi, Rabeeh and Ivison, Hamish and Tae, Jaesung and Henderson, James and Beltagy, Iz and Peters, Matthew and Cohan, Arman",
        "title": "{TESS}: Text-to-Text Self-Conditioned Simplex Diffusion"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "han2022ssd",
        "author": "Han, Xiaochuang and Kumar, Sachin and Tsvetkov, Yulia",
        "title": "{SSD-LM: Semi-autoregressive Simplex-based Diffusion Language Model for Text Generation and Modular Control}"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "han-etal-2024-david",
        "author": "Han, Xiaochuang and Kumar, Sachin and Tsvetkov, Yulia and Ghazvininejad, Marjan",
        "title": "{D}avid helps Goliath: Inference-Time Collaboration Between Small Specialized and Large General Diffusion {LM}s"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "NEURIPS2020_1457c0d6",
        "author": "Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario",
        "title": "{Language Models are Few-Shot Learners}"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "mishra-etal-2022-cross",
        "author": "Mishra, Swaroop and Khashabi, Daniel and Baral, Chitta and Hajishirzi, Hannaneh",
        "title": "Cross-Task Generalization via Natural Language Crowdsourcing Instructions"
      },
      {
        "key": "wei2022finetuned",
        "author": "Jason Wei and Maarten Bosma and Vincent Zhao and Kelvin Guu and Adams Wei Yu and Brian Lester and Nan Du and Andrew M. Dai and Quoc V Le",
        "title": "{Finetuned Language Models are Zero-Shot Learners}"
      },
      {
        "key": "sanh2022multitask",
        "author": "Victor Sanh and Albert Webson and Colin Raffel and Stephen Bach and Lintang Sutawika and Zaid Alyafeai and Antoine Chaffin and Arnaud Stiegler and Arun Raja and Manan Dey and M Saiful Bari and Canwen Xu and Urmish Thakker and Shanya Sharma Sharma and Eliza Szczechla and Taewoon Kim and Gunjan Chhablani and Nihal Nayak and Debajyoti Datta and Jonathan Chang and Mike Tian-Jian Jiang and Han Wang and Matteo Manica and Sheng Shen and Zheng Xin Yong and Harshit Pandey and Rachel Bawden and Thomas Wang and Trishala Neeraj and Jos Rozen and Abheesht Sharma and Andrea Santilli and Thibault Fevry and Jason Alan Fries and Ryan Teehan and Teven Le Scao and Stella Biderman and Leo Gao and Thomas Wolf and Alexander M Rush",
        "title": "{Multitask Prompted Training Enables Zero-Shot Task Generalization}"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "wang2023how",
        "author": "Yizhong Wang and Hamish Ivison and Pradeep Dasigi and Jack Hessel and Tushar Khot and Khyathi Chandu and David Wadden and Kelsey MacMillan and Noah A. Smith and Iz Beltagy and Hannaneh Hajishirzi",
        "title": "{How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources}"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "wei2022finetuned",
        "author": "Jason Wei and Maarten Bosma and Vincent Zhao and Kelvin Guu and Adams Wei Yu and Brian Lester and Nan Du and Andrew M. Dai and Quoc V Le",
        "title": "{Finetuned Language Models are Zero-Shot Learners}"
      },
      {
        "key": "sanh2022multitask",
        "author": "Victor Sanh and Albert Webson and Colin Raffel and Stephen Bach and Lintang Sutawika and Zaid Alyafeai and Antoine Chaffin and Arnaud Stiegler and Arun Raja and Manan Dey and M Saiful Bari and Canwen Xu and Urmish Thakker and Shanya Sharma Sharma and Eliza Szczechla and Taewoon Kim and Gunjan Chhablani and Nihal Nayak and Debajyoti Datta and Jonathan Chang and Mike Tian-Jian Jiang and Han Wang and Matteo Manica and Sheng Shen and Zheng Xin Yong and Harshit Pandey and Rachel Bawden and Thomas Wang and Trishala Neeraj and Jos Rozen and Abheesht Sharma and Andrea Santilli and Thibault Fevry and Jason Alan Fries and Ryan Teehan and Teven Le Scao and Stella Biderman and Leo Gao and Thomas Wolf and Alexander M Rush",
        "title": "{Multitask Prompted Training Enables Zero-Shot Task Generalization}"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "han2024transferlearningtextdiffusion",
        "author": "Kehang Han and Kathleen Kenealy and Aditya Barua and Noah Fiedel and Noah Constant",
        "title": "{Transfer Learning for Text Diffusion Models}"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "savinov2022stepunrolled",
        "author": "Nikolay Savinov and Junyoung Chung and Mikolaj Binkowski and Erich Elsen and Aaron van den Oord",
        "title": "{Step-unrolled Denoising Autoencoders for Text Generation}"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "ye2023diffusionlanguagemodelsperform",
        "author": "Jiasheng Ye and Zaixiang Zheng and Yu Bao and Lihua Qian and Quanquan Gu",
        "title": "{Diffusion Language Models Can Perform Many Tasks with Scaling and Instruction-Finetuning}"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "zheng2024reparameterizeddiscretediffusionmodel",
        "author": "Lin Zheng and Jianbo Yuan and Lei Yu and Lingpeng Kong",
        "title": "{A Reparameterized Discrete Diffusion Model for Text Generation}"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "conneau-etal-2020-unsupervised",
        "author": "Conneau, Alexis and Khandelwal, Kartikay and Goyal, Naman and Chaudhary, Vishrav and Wenzek, Guillaume and Guzm{\\'a}n, Francisco and Grave, Edouard and Ott, Myle and Zettlemoyer, Luke and Stoyanov, Veselin",
        "title": "Unsupervised Cross-lingual Representation Learning at Scale"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "gong2024scalingdiffusionlm",
        "author": "Shansan Gong and Shivam Agarwal and Yizhe Zhang and Jiacheng Ye and Lin Zheng and Mukai Li and Chenxin An and Peilin Zhao and Wei Bi and Jiawei Han and Hao Peng and Lingpeng Kong",
        "title": "{Scaling Diffusion Language Models via Adaptation from Autoregressive Models}"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "austin2021structured",
        "author": "Jacob Austin and Daniel D. Johnson and Jonathan Ho and Daniel Tarlow and Rianne van den Berg",
        "title": "{Structured Denoising Diffusion Models in Discrete State-Spaces}"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "touvron2023llama2openfoundation",
        "author": "Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom",
        "title": "{Llama 2: Open Foundation and Fine-Tuned Chat Models}"
      }
    ]
  }
]