\begin{table*}[]
\centering
\adjustbox{max width=\textwidth}{
\begin{tabular}{lcccccc|c}
\toprule
\textbf{Model} & \textbf{Alp. Eval} & \textbf{SQuAD} & \textbf{TriviaQA} & \textbf{IFEval} & \textbf{BBH} & \textbf{GSM8k} & \textbf{GSM8k (ft)} \\
\midrule
%\multicolumn{8}{c}{\textbf{AR Baselines}} \\ \midrule
Mistral v0.1 AR & \underline{77.1} & \underline{86.0} & 50.4 & 36.8 & \underline{43.3} & 52.5 & 51.2 \\
\quad w/ cont. pretrain & 73.6 & 71.4 & 20.5 & 32.0 & 32.2 & 40.7 & - \\ 
Mistral v0.3 AR & 63.3 & 48.9 & 36.7 & \underline{62.7} & 38.4 & \underline{54.7} & 45.6 \\
\midrule
%\multicolumn{8}{c}{\textbf{Diffusion LM Baselines}} \\ \midrule
DiffuLlama & 0.2 & 34.9 & 19.7 & 14.4 & 1.9 & 0.0 & 63.1$*$ \\
Flan-XLM-R-D XXL & 0.0 & 41.0 & 0.0 & 12.2 & 1.2 & 3.0 & 12.8$^\dagger$ \\ \midrule
%\multicolumn{8}{c}{\textbf{\textbf{\modelname}}} \\ \midrule
\modelname~\textbf{(ours) v0.1} & \textbf{63.1} & \textbf{85.4} & 49.3 & 30.5 & 8.4 & 14.5 & \underline{\textbf{66.6}} \\
\quad w/o diffusion adaptation & 0 & 3.6 & 2.9 & 12.9 & 1.0 & 1.1 & 0.2 \\ 
\modelname~\textbf{(ours) v0.3} & 62.2 & 84.8 & \textbf{\underline{53.8}} & \textbf{54.6} & \textbf{10.8} & \textbf{36.5} & 59.2 \\
\bottomrule
\end{tabular}}
\caption{Performance of various models on downstream tasks after undergoing instruction tuning, including DiffuLlama~\citep{gong2024scalingdiffusionlm} and Flan-XLR-R-D~\citep{ye2023diffusionlanguagemodelsperform}. `GSM8k (ft)' refers to finetuning on the augmented GSM8k symbolic dataset proposed by \citet{ye2024diffusion} and then evaluating on GSM8k, following \citet{gong2024scalingdiffusionlm}.
We \textbf{bold} the best result from a diffusion LM and \underline{underline} the best overall result.
* Number from \citet{gong2024scalingdiffusionlm}.
$\dagger$ Number from \citet{ye2024diffusion} when finetuning on a different distilled GSM8k dataset.}
\label{tab:instruction_performance}
\end{table*}