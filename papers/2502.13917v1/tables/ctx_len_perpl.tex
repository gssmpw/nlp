\begin{table}[]
\centering
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Ctx. Len.} & 512 & 2048 & 2048 \\
\textbf{Train Steps} & 35,000 & 10,000 & 200,000 \\ \midrule
Perp. & 24.3 & 12.2 & \textbf{11.3} \\
d-1 & \textbf{0.62} & 0.58 & 0.59 \\
d-2 & \textbf{0.87} & 0.84 & 0.85 \\
d-3 & \textbf{0.93} & 0.91 & 0.91 \\
d-4 & \textbf{0.94} & 0.93 & 0.93 \\
Mauve & 0.95 & \textbf{0.97} & 0.92 \\
Entropy & 6.31 & 6.21 & \textbf{6.34} \\ \bottomrule
\end{tabular}
\caption{Intrinsic pretraining metrics when pretraining Mistral 7b v0.1 base with varying context lengths and pretraining steps. We find that using a longer context length and training for longer improves performance.}
\label{tab:ctx_len_exp}
\end{table}