@article{2020t5,
	title        = {{Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer}},
	author       = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
	year         = 2020,
	journal      = {Journal of Machine Learning Research},
	volume       = 21,
	number       = 140,
	pages        = {1--67},
	url          = {http://jmlr.org/papers/v21/20-074.html}
}
@misc{alpaca_eval,
	title        = {{AlpacaEval: An Automatic Evaluator of Instruction-following Models}},
	author       = {Xuechen Li and Tianyi Zhang and Yann Dubois and Rohan Taori and Ishaan Gulrajani and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto},
	year         = 2023,
	month        = 5,
	journal      = {GitHub repository},
	publisher    = {GitHub},
	howpublished = {\url{https://github.com/tatsu-lab/alpaca_eval}}
}
@inproceedings{austin2021structured,
	title        = {{Structured Denoising Diffusion Models in Discrete State-Spaces}},
	author       = {Jacob Austin and Daniel D. Johnson and Jonathan Ho and Daniel Tarlow and Rianne van den Berg},
	year         = 2021,
	booktitle    = {NeurIPS}
}
@misc{bai2022traininghelpfulharmlessassistant,
	title        = {{Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback}},
	author       = {Yuntao Bai and Andy Jones and Kamal Ndousse and Amanda Askell and Anna Chen and Nova DasSarma and Dawn Drain and Stanislav Fort and Deep Ganguli and Tom Henighan and Nicholas Joseph and Saurav Kadavath and Jackson Kernion and Tom Conerly and Sheer El-Showk and Nelson Elhage and Zac Hatfield-Dodds and Danny Hernandez and Tristan Hume and Scott Johnston and Shauna Kravec and Liane Lovitt and Neel Nanda and Catherine Olsson and Dario Amodei and Tom Brown and Jack Clark and Sam McCandlish and Chris Olah and Ben Mann and Jared Kaplan},
	year         = 2022,
	url          = {https://arxiv.org/abs/2204.05862},
	eprint       = {2204.05862},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@article{Bavarian2022EfficientTO,
	title        = {{Efficient Training of Language Models to Fill in the Middle}},
	author       = {Mohammad Bavarian and Heewoo Jun and Nikolas A. Tezak and John Schulman and Christine McLeavey and Jerry Tworek and Mark Chen},
	year         = 2022,
	journal      = {ArXiv},
	volume       = {abs/2207.14255}
}
@inproceedings{behnamghader2024llmvec,
	title        = {{{LLM}2Vec: Large Language Models Are Secretly Powerful Text Encoders}},
	author       = {Parishad BehnamGhader and Vaibhav Adlakha and Marius Mosbach and Dzmitry Bahdanau and Nicolas Chapados and Siva Reddy},
	year         = 2024,
	booktitle    = {First Conference on Language Modeling},
	url          = {https://openreview.net/forum?id=IW1PR7vEBf}
}
@inproceedings{Bentivogli09thefifth,
	title        = {{The Fifth PASCAL Recognizing Textual Entailment Challenge}},
	author       = {Luisa Bentivogli and Ido Dagan and Hoa Trang Dang and Danilo Giampiccolo and Bernardo Magnini},
	year         = 2009,
	booktitle    = {TAC}
}
@inproceedings{bleu,
	title        = {{{B}leu: a Method for Automatic Evaluation of Machine Translation}},
	author       = {Papineni, Kishore  and Roukos, Salim  and Ward, Todd  and Zhu, Wei-Jing},
	year         = 2002,
	month        = jul,
	booktitle    = {Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics},
	publisher    = {Association for Computational Linguistics},
	address      = {Philadelphia, Pennsylvania, USA},
	pages        = {311--318},
	doi          = {10.3115/1073083.1073135},
	url          = {https://aclanthology.org/P02-1040}
}
@article{campbell2022continuous,
	title        = {{A Continuous Time Framework for Discrete Denoising Models}},
	author       = {Campbell, Andrew and Benton, Joe and De Bortoli, Valentin and Rainforth, Tom and Deligiannidis, George and Doucet, Arnaud},
	year         = 2022,
	journal      = {arXiv preprint arXiv:2205.14987}
}
@inproceedings{chang2022maskgit,
	title        = {{Maskgit: Masked generative image transformer}},
	author       = {Chang, Huiwen and Zhang, Han and Jiang, Lu and Liu, Ce and Freeman, William T},
	year         = 2022,
	booktitle    = {CVPR}
}
@article{chen2022analog,
	title        = {{Analog bits: Generating discrete data using diffusion models with self-conditioning}},
	author       = {Chen, Ting and Zhang, Ruixiang and Hinton, Geoffrey},
	year         = 2022,
	journal      = {arXiv preprint arXiv:2208.04202}
}
@article{cobbe2021gsm8k,
	title        = {{Training Verifiers to Solve Math Word Problems}},
	author       = {Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and Hesse, Christopher and Schulman, John},
	year         = 2021,
	journal      = {arXiv preprint arXiv:2110.14168}
}
@inproceedings{dagan2005pascal,
	title        = {{The pascal recognising textual entailment challenge}},
	author       = {Dagan, Ido and Glickman, Oren and Magnini, Bernardo},
	year         = 2005,
	booktitle    = {Machine Learning Challenges Workshop}
}
@inproceedings{dao2022flashattention,
	title        = {{Flash{A}ttention: Fast and Memory-Efficient Exact Attention with {IO}-Awareness}},
	author       = {Dao, Tri and Fu, Daniel Y. and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
	year         = 2022,
	booktitle    = {Advances in Neural Information Processing Systems (NeurIPS)}
}
@inproceedings{dao2023flashattention2,
	title        = {{Flash{A}ttention-2: Faster Attention with Better Parallelism and Work Partitioning}},
	author       = {Dao, Tri},
	year         = 2024,
	booktitle    = {International Conference on Learning Representations (ICLR)}
}
@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,
	title        = {{DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}},
	author       = {{DeepSeek-AI}},
	year         = 2025,
	url          = {https://arxiv.org/abs/2501.12948},
	eprint       = {2501.12948},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@article{Deshpande2018FastDA,
	title        = {{Fast, Diverse and Accurate Image Captioning Guided by Part-Of-Speech}},
	author       = {Aditya Deshpande and Jyoti Aneja and Liwei Wang and Alexander G. Schwing and David A. Forsyth},
	year         = 2018,
	journal      = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
	pages        = {10687--10696}
}
@article{dhariwal2021diffusion,
	title        = {{Diffusion models beat gans on image synthesis}},
	author       = {Dhariwal, Prafulla and Nichol, Alexander},
	year         = 2021,
	journal      = {NeurIPS}
}
@article{dhingra2017quasar,
	title        = {{Quasar: Datasets for question answering by search and reading}},
	author       = {Dhingra, Bhuwan and Mazaitis, Kathryn and Cohen, William W},
	year         = 2017,
	journal      = {arXiv preprint arXiv:1707.03904}
}
@article{dieleman2022continuous,
	title        = {{Continuous diffusion for categorical data}},
	author       = {Dieleman, Sander and Sartran, Laurent and Roshannai, Arman and Savinov, Nikolay and Ganin, Yaroslav and Richemond, Pierre H and Doucet, Arnaud and Strudel, Robin and Dyer, Chris and Durkan, Conor and others},
	year         = 2022,
	journal      = {arXiv preprint arXiv:2211.15089}
}
@article{dong2023raft,
	title        = {{Raft: Reward ranked finetuning for generative foundation model alignment}},
	author       = {Dong, Hanze and Xiong, Wei and Goyal, Deepanshu and Pan, Rui and Diao, Shizhe and Zhang, Jipeng and Shum, Kashun and Zhang, Tong},
	year         = 2023,
	journal      = {arXiv preprint arXiv:2304.06767}
}
@article{du2022diverse,
	title        = {{Diverse Text Generation via Variational Encoder-Decoder Models with Gaussian Process Priors}},
	author       = {Du, Wanyu and Zhao, Jianqiao and Wang, Liwei and Ji, Yangfeng},
	year         = 2022,
	journal      = {arXiv preprint arXiv:2204.01227}
}
@inproceedings{genie,
	title        = {{Text generation with diffusion language models: a pre-training approach with continuous paragraph denoise}},
	author       = {Lin, Zhenghao and Gong, Yeyun and Shen, Yelong and Wu, Tong and Fan, Zhihao and Lin, Chen and Duan, Nan and Chen, Weizhu},
	year         = 2023,
	booktitle    = {Proceedings of the 40th International Conference on Machine Learning},
	location     = {Honolulu, Hawaii, USA},
	publisher    = {JMLR.org},
	series       = {ICML'23},
	abstract     = {In this paper, we introduce a novel dIffusion language modEl pre-training framework for text generation, which we call GENIE. GENIE is a large-scale pre-trained diffusion language model that consists of an encoder and a diffusion-based decoder, which can generate text by gradually transforming a random noise sequence into a coherent text sequence. To pre-train GENIE on a large-scale language corpus, we design a new continuous paragraph denoise objective, which encourages the diffusion-decoder to reconstruct a clean text paragraph from a corrupted version while preserving the semantic and syntactic coherence. We evaluate GENIE on four downstream text generation benchmarks, namely XSUM, CNN/DAILYMAIL, GIGAWORD, and COMMONGEN. Our experimental results show that GENIE achieves comparable performance with the state-of-the-art autoregressive models on these benchmarks, and generates more diverse text samples. The code and models of GENIE are available at https://github.com/microsoft/ProphetNet/tree/master/GENIE.},
	articleno    = 867,
	numpages     = 14
}
@inproceedings{ghazvininejad2019mask,
	title        = {{Mask-Predict: Parallel Decoding of Conditional Masked Language Models}},
	author       = {Ghazvininejad, Marjan and Levy, Omer and Liu, Yinhan and Zettlemoyer, Luke},
	year         = 2019,
	booktitle    = {EMNLP-IJCNLP}
}
@inproceedings{gong2022diffuseq,
	title        = {{{DiffuSeq}: Sequence to Sequence Text Generation with Diffusion Models}},
	author       = {Gong, Shansan and Li, Mukai and Feng, Jiangtao and Wu, Zhiyong and Kong, Lingpeng},
	year         = 2023,
	booktitle    = {International Conference on Learning Representations, ICLR}
}
@misc{gong2024scalingdiffusionlm,
	title        = {{Scaling Diffusion Language Models via Adaptation from Autoregressive Models}},
	author       = {Shansan Gong and Shivam Agarwal and Yizhe Zhang and Jiacheng Ye and Lin Zheng and Mukai Li and Chenxin An and Peilin Zhao and Wei Bi and Jiawei Han and Hao Peng and Lingpeng Kong},
	year         = 2024,
	url          = {https://arxiv.org/abs/2410.17891},
	eprint       = {2410.17891},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@misc{gpt-neo,
	title        = {{GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow}},
	author       = {Black, Sid and Leo, Gao and Wang, Phil and Leahy, Connor and Biderman, Stella},
	year         = 2021,
	month        = mar,
	publisher    = {Zenodo},
	doi          = {10.5281/zenodo.5297715},
	url          = {https://doi.org/10.5281/zenodo.5297715},
	note         = {{If you use this software, please cite it using these metadata.}},
	version      = {1.0}
}
@misc{grattafiori2024llama3herdmodels,
	title        = {{The Llama 3 Herd of Models}},
	author       = {{Llama Team}},
	year         = 2024,
	url          = {https://arxiv.org/abs/2407.21783},
	eprint       = {2407.21783},
	archiveprefix = {arXiv},
	primaryclass = {cs.AI}
}
@article{gu2019levenshtein,
	title        = {{Levenshtein transformer}},
	author       = {Gu, Jiatao and Wang, Changhan and Zhao, Junbo},
	year         = 2019,
	journal      = {NeurIPS}
}
@article{gulrajani2023likelihood,
	title        = {{Likelihood-Based Diffusion Language Models}},
	author       = {Gulrajani, Ishaan and Hashimoto, Tatsunori B},
	year         = 2023,
	journal      = {arXiv preprint arXiv:2305.18619}
}
@inproceedings{gulrajani2023likelihoodbased,
	title        = {{Likelihood-Based Diffusion Language Models}},
	author       = {Ishaan Gulrajani and Tatsunori Hashimoto},
	year         = 2023,
	booktitle    = {Thirty-seventh Conference on Neural Information Processing Systems},
	url          = {https://openreview.net/forum?id=e2MCL6hObn}
}
@article{han2022ssd,
	title        = {{SSD-LM: Semi-autoregressive Simplex-based Diffusion Language Model for Text Generation and Modular Control}},
	author       = {Han, Xiaochuang and Kumar, Sachin and Tsvetkov, Yulia},
	year         = 2022,
	journal      = {arXiv preprint arXiv:2210.17432}
}
@misc{han2023ssd2,
	title        = {{SSD-2: Scaling and Inference-time Fusion of Diffusion Language Models}},
	author       = {Xiaochuang Han and Sachin Kumar and Yulia Tsvetkov and Marjan Ghazvininejad},
	year         = 2023,
	eprint       = {2305.14771},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@article{Han2023SSD2SA,
	title        = {{SSD-2: Scaling and Inference-time Fusion of Diffusion Language Models}},
	author       = {Xiaochuang Han and Sachin Kumar and Yulia Tsvetkov and Marjan Ghazvininejad},
	year         = 2023,
	journal      = {ArXiv},
	volume       = {abs/2305.14771},
	url          = {https://api.semanticscholar.org/CorpusID:258865241}
}
@misc{han2024transferlearningtextdiffusion,
	title        = {{Transfer Learning for Text Diffusion Models}},
	author       = {Kehang Han and Kathleen Kenealy and Aditya Barua and Noah Fiedel and Noah Constant},
	year         = 2024,
	url          = {https://arxiv.org/abs/2401.17181},
	eprint       = {2401.17181},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@inproceedings{hermann2015teaching,
	title        = {{Teaching machines to read and comprehend}},
	author       = {Hermann, Karl Moritz and Kocisky, Tomas and Grefenstette, Edward and Espeholt, Lasse and Kay, Will and Suleyman, Mustafa and Blunsom, Phil},
	year         = 2015,
	booktitle    = {NeurIPS}
}
@article{ho2020denoising,
	title        = {{Denoising diffusion probabilistic models}},
	author       = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
	year         = 2020,
	journal      = {NeurIPS}
}
@inproceedings{ho2022classifier,
	title        = {{Classifier-free diffusion guidance}},
	author       = {Ho, Jonathan and Salimans, Tim},
	year         = 2021,
	booktitle    = {NeurIPS Workshop DGMs Applications}
}
@inproceedings{ho2022video,
	title        = {{Video Diffusion Models}},
	author       = {Ho, Jonathan and Salimans, Tim and Gritsenko, Alexey A and Chan, William and Norouzi, Mohammad and Fleet, David J},
	year         = 2022,
	booktitle    = {ICLR Workshop on Deep Generative Models for Highly Structured Data}
}
@inproceedings{holtzman2019curious,
	title        = {{The Curious Case of Neural Text Degeneration}},
	author       = {Holtzman, Ari and Buys, Jan and Du, Li and Forbes, Maxwell and Choi, Yejin},
	year         = 2019,
	booktitle    = {ICLR}
}
@article{hoogeboom2021argmax,
	title        = {{Argmax flows and multinomial diffusion: Learning categorical distributions}},
	author       = {Hoogeboom, Emiel and Nielsen, Didrik and Jaini, Priyank and Forr{\'e}, Patrick and Welling, Max},
	year         = 2021,
	journal      = {NeurIPS}
}
@inproceedings{hoogeboom2022autoregressive,
	title        = {{Autoregressive Diffusion Models}},
	author       = {Emiel Hoogeboom and Alexey A. Gritsenko and Jasmijn Bastings and Ben Poole and Rianne van den Berg and Tim Salimans},
	year         = 2022,
	booktitle    = {ICLR}
}
@inproceedings{huang2024large,
	title        = {{Large Language Models Cannot Self-Correct Reasoning Yet}},
	author       = {Jie Huang and Xinyun Chen and Swaroop Mishra and Huaixiu Steven Zheng and Adams Wei Yu and Xinying Song and Denny Zhou},
	year         = 2024,
	booktitle    = {The Twelfth International Conference on Learning Representations},
	url          = {https://openreview.net/forum?id=IkmD3fKBPQ}
}
@misc{ivison2023camelschangingclimateenhancing,
	title        = {{Camels in a Changing Climate: Enhancing LM Adaptation with Tulu 2}},
	author       = {Hamish Ivison and Yizhong Wang and Valentina Pyatkin and Nathan Lambert and Matthew Peters and Pradeep Dasigi and Joel Jang and David Wadden and Noah A. Smith and Iz Beltagy and Hannaneh Hajishirzi},
	year         = 2023,
	url          = {https://arxiv.org/abs/2311.10702},
	eprint       = {2311.10702},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@inproceedings{ivison2024unpacking,
	title        = {{Unpacking DPO and PPO: Disentangling Best Practices for Learning from Preference Feedback}},
	author       = {Hamish Ivison and Wang, Yizhong and Liu, Jiacheng and Wu, Zeqiu and Pyatkin, Valentina and Lambert, Nathan and Smith, Noah A. and Choi, Yejin and Hajishirzi, Hannaneh},
	year         = 2024,
	booktitle    = {NeurIPS},
	url          = {https://arxiv.org/abs/2406.09279},
	eprint       = {2406.09279},
	code         = {https://github.com/allenai/open-instruct}
}
@inproceedings{j2018generating,
	title        = {{Generating Wikipedia by Summarizing Long Sequences}},
	author       = {Peter J. Liu and Mohammad Saleh and Etienne Pot and Ben Goodrich and Ryan Sepassi and Lukasz Kaiser and Noam Shazeer},
	year         = 2018,
	booktitle    = {International Conference on Learning Representations},
	url          = {https://openreview.net/forum?id=Hyg0vbWC-}
}
@inproceedings{jiang2020neural,
	title        = {{Neural CRF Model for Sentence Alignment in Text Simplification}},
	author       = {Jiang, Chao and Maddela, Mounica and Lan, Wuwei and Zhong, Yang and Xu, Wei},
	year         = 2020,
	booktitle    = {ACL}
}
@misc{jiang2023mistral7b,
	title        = {{Mistral 7B}},
	author       = {Albert Q. Jiang and Alexandre Sablayrolles and Arthur Mensch and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Florian Bressand and Gianna Lengyel and Guillaume Lample and Lucile Saulnier and Lélio Renard Lavaud and Marie-Anne Lachaux and Pierre Stock and Teven Le Scao and Thibaut Lavril and Thomas Wang and Timothée Lacroix and William El Sayed},
	year         = 2023,
	url          = {https://arxiv.org/abs/2310.06825},
	eprint       = {2310.06825},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@misc{jiang2024mistral7bv03,
	title        = {{Mistral 7B v0.3}},
	author       = {Albert Jiang and Alexandre Sablayrolles and Alexis Tacnet and Antoine Roux and Arthur Mensch and Audrey Herblin-Stoop and Baptiste Bout and Baudouin de Monicault and Blanche Savary and Bam4d and Caroline Feldman and Devendra Singh Chaplot and Diego de las Casas and Eleonore Arcelin and Emma Bou Hanna and Etienne Metzger and Gianna Lengyel and Guillaume Bour and Guillaume Lample and Harizo Rajaona and Jean-Malo Delignon and Jia Li and Justus Murke and Louis Martin and Louis Ternon and Lucile Saulnier and Lélio Renard Lavaud and Margaret Jennings and Marie Pellat and Marie Torelli and Marie-Anne Lachaux and Nicolas Schuhl and Patrick von Platen and Pierre Stock and Sandeep Subramanian and Sophia Yang and Szymon Antoniak and Teven Le Scao and Thibaut Lavril and Timothée Lacroix and Théophile Gervet and Thomas Wang and Valera Nemychnikova and William El Sayed and William Marshall},
	year         = 2024,
	url          = {https://huggingface.co/mistralai/Mistral-7B-v0.3}
}
@misc{kaplan2020scalinglawsneurallanguage,
	title        = {{Scaling Laws for Neural Language Models}},
	author       = {Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei},
	year         = 2020
}
@inproceedings{kong2020diffwave,
	title        = {{DiffWave: A Versatile Diffusion Model for Audio Synthesis}},
	author       = {Kong, Zhifeng and Ping, Wei and Huang, Jiaji and Zhao, Kexin and Catanzaro, Bryan},
	year         = 2020,
	booktitle    = {ICLR}
}
@misc{kumar2024traininglanguagemodelsselfcorrect,
	title        = {{Training Language Models to Self-Correct via Reinforcement Learning}},
	author       = {Aviral Kumar and Vincent Zhuang and Rishabh Agarwal and Yi Su and John D Co-Reyes and Avi Singh and Kate Baumli and Shariq Iqbal and Colton Bishop and Rebecca Roelofs and Lei M Zhang and Kay McKinney and Disha Shrivastava and Cosmin Paduraru and George Tucker and Doina Precup and Feryal Behbahani and Aleksandra Faust},
	year         = 2024,
	url          = {https://arxiv.org/abs/2409.12917},
	eprint       = {2409.12917},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@inproceedings{kwon2023efficient,
	title        = {{Efficient Memory Management for Large Language Model Serving with PagedAttention}},
	author       = {Woosuk Kwon and Zhuohan Li and Siyuan Zhuang and Ying Sheng and Lianmin Zheng and Cody Hao Yu and Joseph E. Gonzalez and Hao Zhang and Ion Stoica},
	year         = 2023,
	booktitle    = {Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles}
}
@misc{lambert2024rewardbench,
	title        = {{RewardBench: Evaluating Reward Models for Language Modeling}},
	author       = {Nathan Lambert and Valentina Pyatkin and Jacob Morrison and LJ Miranda and Bill Yuchen Lin and Khyathi Chandu and Nouha Dziri and Sachin Kumar and Tom Zick and Yejin Choi and Noah A. Smith and Hannaneh Hajishirzi},
	year         = 2024,
	eprint       = {2403.13787},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@misc{lambert2024tulu3,
	title        = {{Tülu 3: Pushing Frontiers in Open Language Model Post-Training}},
	author       = {Nathan Lambert and Jacob Morrison and Valentina Pyatkin and Shengyi Huang and Hamish Ivison and Faeze Brahman and Lester James V. Miranda and Alisa Liu and Nouha Dziri and Shane Lyu and Yuling Gu and Saumya Malik and Victoria Graf and Jena D. Hwang and Jiangjiang Yang and Ronan Le Bras and Oyvind Tafjord and Chris Wilhelm and Luca Soldaini and Noah A. Smith and Yizhong Wang and Pradeep Dasigi and Hannaneh Hajishirzi},
	year         = 2024,
	email        = {tulu@allenai.org}
}
@misc{laskin2022incontextreinforcementlearningalgorithm,
	title        = {{In-context Reinforcement Learning with Algorithm Distillation}},
	author       = {Michael Laskin and Luyu Wang and Junhyuk Oh and Emilio Parisotto and Stephen Spencer and Richie Steigerwald and DJ Strouse and Steven Hansen and Angelos Filos and Ethan Brooks and Maxime Gazeau and Himanshu Sahni and Satinder Singh and Volodymyr Mnih},
	year         = 2022,
	url          = {https://arxiv.org/abs/2210.14215},
	eprint       = {2210.14215},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@inproceedings{lewis2020bart,
	title        = {{BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension}},
	author       = {Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Veselin and Zettlemoyer, Luke},
	year         = 2020,
	booktitle    = {ACL}
}
@inproceedings{li-etal-2016-diversity,
	title        = {{A Diversity-Promoting Objective Function for Neural Conversation Models}},
	author       = {Li, Jiwei  and Galley, Michel  and Brockett, Chris  and Gao, Jianfeng  and Dolan, Bill},
	year         = 2016,
	booktitle    = {NAACL}
}
@inproceedings{li2022diffusion,
	title        = {{Diffusion-LM Improves Controllable Text Generation}},
	author       = {Li, Xiang Lisa and Thickstun, John and Gulrajani, Ishaan and Liang, Percy and Hashimoto, Tatsunori B},
	year         = 2022,
	booktitle    = {NeurIPS}
}
@inproceedings{li2022diffusionlm,
	title        = {{Diffusion-{LM} Improves Controllable Text Generation}},
	author       = {Xiang Lisa Li and John Thickstun and Ishaan Gulrajani and Percy Liang and Tatsunori Hashimoto},
	year         = 2022,
	booktitle    = {NeurIPS},
	editor       = {Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho}
}
@misc{liu2019robertarobustlyoptimizedbert,
	title        = {{RoBERTa: A Robustly Optimized BERT Pretraining Approach}},
	author       = {Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
	year         = 2019,
	url          = {https://arxiv.org/abs/1907.11692},
	eprint       = {1907.11692},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@inproceedings{lmharms,
	title        = {{Taxonomy of Risks Posed by Language Models}},
	author       = {Weidinger, Laura and Uesato, Jonathan and Rauh, Maribeth and Griffin, Conor and Huang, Po-Sen and Mellor, John and Glaese, Amelia and Cheng, Myra and Balle, Borja and Kasirzadeh, Atoosa and Biles, Courtney and Brown, Sasha and Kenton, Zac and Hawkins, Will and Stepleton, Tom and Birhane, Abeba and Hendricks, Lisa Anne and Rimell, Laura and Isaac, William and Haas, Julia and Legassick, Sean and Irving, Geoffrey and Gabriel, Iason},
	year         = 2022,
	booktitle    = {Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency},
	location     = {Seoul, Republic of Korea},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {FAccT '22},
	pages        = {214–229},
	doi          = {10.1145/3531146.3533088},
	isbn         = 9781450393522,
	url          = {https://doi.org/10.1145/3531146.3533088},
	numpages     = 16,
	keywords     = {risk assessment, technology risks, responsible innovation, responsible AI, language models}
}
@article{lou2024discrete,
	title        = {{Discrete diffusion modeling by estimating the ratios of the data distribution}},
	author       = {Lou, Aaron and Meng, Chenlin and Ermon, Stefano},
	year         = 2024,
	journal      = {arXiv preprint arXiv:2310.16834}
}
@inproceedings{mahabadi2021parameter,
	title        = {{Parameter-efficient Multi-task Fine-tuning for Transformers via Shared Hypernetworks}},
	author       = {Mahabadi, Rabeeh Karimi and Ruder, Sebastian and Dehghani, Mostafa and Henderson, James},
	year         = 2021,
	booktitle    = {ACL}
}
@misc{min2024imitateexploreselfimprovereproduction,
	title        = {{Imitate, Explore, and Self-Improve: A Reproduction Report on Slow-thinking Reasoning Systems}},
	author       = {Yingqian Min and Zhipeng Chen and Jinhao Jiang and Jie Chen and Jia Deng and Yiwen Hu and Yiru Tang and Jiapeng Wang and Xiaoxue Cheng and Huatong Song and Wayne Xin Zhao and Zheng Liu and Zhongyuan Wang and Ji-Rong Wen},
	year         = 2024,
	url          = {https://arxiv.org/abs/2412.09413},
	eprint       = {2412.09413},
	archiveprefix = {arXiv},
	primaryclass = {cs.AI}
}
@inproceedings{narayan-etal-2018-dont,
	title        = {{Don{'}t Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization}},
	author       = {Narayan, Shashi  and Cohen, Shay B.  and Lapata, Mirella},
	year         = 2018,
	booktitle    = {EMNLP}
}
@inproceedings{NEURIPS2020_1457c0d6,
	title        = {{Language Models are Few-Shot Learners}},
	author       = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	year         = 2020,
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = 33,
	pages        = {1877--1901},
	url          = {https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
	editor       = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin}
}
@article{newsela,
	title        = {{Problems in Current Text Simplification Research: New Data Can Help}},
	author       = {Xu, Wei  and Callison-Burch, Chris  and Napoles, Courtney},
	year         = 2015,
	journal      = {Transactions of the Association for Computational Linguistics},
	publisher    = {MIT Press},
	address      = {Cambridge, MA},
	volume       = 3,
	pages        = {283--297},
	doi          = {10.1162/tacl_a_00139},
	url          = {https://aclanthology.org/Q15-1021},
	abstract     = {Simple Wikipedia has dominated simplification research in the past 5 years. In this opinion paper, we argue that focusing on Wikipedia limits simplification research. We back up our arguments with corpus analysis and by highlighting statements that other researchers have made in the simplification literature. We introduce a new simplification dataset that is a significant improvement over Simple Wikipedia, and present a novel quantitative-comparative approach to study the quality of simplification data resources.}
}
@inproceedings{nichol2021improved,
	title        = {{Improved denoising diffusion probabilistic models}},
	author       = {Nichol, Alexander Quinn and Dhariwal, Prafulla},
	year         = 2021,
	booktitle    = {ICML}
}
@misc{ouyang2022traininglanguagemodelsfollow,
	title        = {{Training language models to follow instructions with human feedback}},
	author       = {Long Ouyang and Jeff Wu and Xu Jiang and Diogo Almeida and Carroll L. Wainwright and Pamela Mishkin and Chong Zhang and Sandhini Agarwal and Katarina Slama and Alex Ray and John Schulman and Jacob Hilton and Fraser Kelton and Luke Miller and Maddie Simens and Amanda Askell and Peter Welinder and Paul Christiano and Jan Leike and Ryan Lowe},
	year         = 2022,
	url          = {https://arxiv.org/abs/2203.02155},
	eprint       = {2203.02155},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@inproceedings{pillutla-etal:mauve:neurips2021,
	title        = {{MAUVE: Measuring the Gap Between Neural Text and Human Text using Divergence Frontiers}},
	author       = {Pillutla, Krishna and Swayamdipta, Swabha and Zellers, Rowan and Thickstun, John and Welleck, Sean and Choi, Yejin and Harchaoui, Zaid},
	year         = 2021,
	booktitle    = {NeurIPS}
}
@article{pillutla2021mauve,
	title        = {{Mauve: Measuring the gap between neural text and human text using divergence frontiers}},
	author       = {Pillutla, Krishna and Swayamdipta, Swabha and Zellers, Rowan and Thickstun, John and Welleck, Sean and Choi, Yejin and Harchaoui, Zaid},
	year         = 2021,
	journal      = {NeurIPS}
}
@inproceedings{pmlr-v162-nichol22a,
	title        = {{{GLIDE}: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models}},
	author       = {Nichol, Alexander Quinn and Dhariwal, Prafulla and Ramesh, Aditya and Shyam, Pranav and Mishkin, Pamela and Mcgrew, Bob and Sutskever, Ilya and Chen, Mark},
	year         = 2022,
	booktitle    = {ICML},
	editor       = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan}
}
@inproceedings{pmlr-v235-bachmann24a,
	title        = {{The Pitfalls of Next-Token Prediction}},
	author       = {Bachmann, Gregor and Nagarajan, Vaishnavh},
	year         = 2024,
	month        = {21--27 Jul},
	booktitle    = {Proceedings of the 41st International Conference on Machine Learning},
	publisher    = {PMLR},
	series       = {Proceedings of Machine Learning Research},
	volume       = 235,
	pages        = {2296--2318},
	url          = {https://proceedings.mlr.press/v235/bachmann24a.html},
	editor       = {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
	pdf          = {https://raw.githubusercontent.com/mlresearch/v235/main/assets/bachmann24a/bachmann24a.pdf},
	abstract     = {Can a mere next-token predictor faithfully model human thinking? Our work is aimed at crystallizing this intuitive concern, which is currently fragmented in the literature. First, we emphasize isolating the two phases of next-token prediction that are often conflated: autoregression during inference vs. teacher-forcing during training. We argue that the previously-identified problem of "exponential error accumulation" is a symptom of autoregressive inference. But more concerningly, we identify that teacher-forcing can let the model fit the training data by cheating, causing total in-distribution failure. We design a minimal planning task where empirically both the Transformer and the Mamba architecture fail in this manner - remarkably, despite the task being easy to learn. Overall, our work consolidates these and other essential arguments surrounding next-token prediction. We hope this effort can ground future discussions and inspire explorations beyond the next-token prediction paradigm.}
}
@article{radford2019language,
	title        = {{Language models are unsupervised multitask learners}},
	author       = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
	year         = 2019,
	journal      = {OpenAI blog}
}
@article{raffel2019exploring,
	title        = {{Exploring the limits of transfer learning with a unified text-to-text transformer}},
	author       = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
	year         = 2020,
	journal      = {JMLR}
}
@article{ramesh2022hierarchical,
	title        = {{Hierarchical text-conditional image generation with clip latents}},
	author       = {Ramesh, Aditya and Dhariwal, Prafulla and Nichol, Alex and Chu, Casey and Chen, Mark},
	year         = 2022,
	journal      = {arXiv preprint arXiv:2204.06125}
}
@article{reid2022diffuser,
	title        = {{Diffuser: Discrete diffusion via edit-based reconstruction}},
	author       = {Reid, Machel and Hellendoorn, Vincent J and Neubig, Graham},
	year         = 2022,
	journal      = {arXiv preprint arXiv:2210.16886}
}
@article{richemond2022categorical,
	title        = {{Categorical SDEs with Simplex Diffusion}},
	author       = {Richemond, Pierre H and Dieleman, Sander and Doucet, Arnaud},
	year         = 2022,
	journal      = {arXiv preprint arXiv:2210.14784}
}
@article{roberta,
	title        = {{RoBERTa: {A} Robustly Optimized {BERT} Pretraining Approach}},
	author       = {Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
	year         = 2019,
	journal      = {CoRR},
	volume       = {abs/1907.11692},
	url          = {http://arxiv.org/abs/1907.11692},
	eprinttype   = {arXiv},
	eprint       = {1907.11692},
	timestamp    = {Thu, 01 Aug 2019 08:59:33 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-1907-11692.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{rouge,
	title        = {{{ROUGE}: A Package for Automatic Evaluation of Summaries}},
	author       = {Lin, Chin-Yew},
	year         = 2004,
	month        = jul,
	booktitle    = {Text Summarization Branches Out},
	publisher    = {Association for Computational Linguistics},
	address      = {Barcelona, Spain},
	pages        = {74--81},
	url          = {https://aclanthology.org/W04-1013}
}
@article{rte2,
	title        = {{The second PASCAL recognising textual entailment challenge}},
	author       = {Bar-Haim, Roy and Dagan, Ido and Dolan, Bill and Ferro, Lisa and Giampiccolo, Danilo},
	year         = 2006,
	journal      = {Second PASCAL Challenges Workshop on Recognising Textual Entailment}
}
@inproceedings{saharia2022photorealistic,
	title        = {{Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding}},
	author       = {Saharia, Chitwan and Chan, William and Saxena, Saurabh and Li, Lala and Whang, Jay and Denton, Emily and Ghasemipour, Seyed Kamyar Seyed and Ayan, Burcu Karagol and Mahdavi, S Sara and Lopes, Rapha Gontijo and others},
	year         = 2022,
	booktitle    = {NeurIPS}
}
@inproceedings{sanh2022multitask,
	title        = {{Multitask Prompted Training Enables Zero-Shot Task Generalization}},
	author       = {Victor Sanh and Albert Webson and Colin Raffel and Stephen Bach and Lintang Sutawika and Zaid Alyafeai and Antoine Chaffin and Arnaud Stiegler and Arun Raja and Manan Dey and M Saiful Bari and Canwen Xu and Urmish Thakker and Shanya Sharma Sharma and Eliza Szczechla and Taewoon Kim and Gunjan Chhablani and Nihal Nayak and Debajyoti Datta and Jonathan Chang and Mike Tian-Jian Jiang and Han Wang and Matteo Manica and Sheng Shen and Zheng Xin Yong and Harshit Pandey and Rachel Bawden and Thomas Wang and Trishala Neeraj and Jos Rozen and Abheesht Sharma and Andrea Santilli and Thibault Fevry and Jason Alan Fries and Ryan Teehan and Teven Le Scao and Stella Biderman and Leo Gao and Thomas Wolf and Alexander M Rush},
	year         = 2022,
	booktitle    = {International Conference on Learning Representations},
	url          = {https://openreview.net/forum?id=9Vrb9D0WI4}
}
@inproceedings{savinov2021step,
	title        = {{Step-unrolled Denoising Autoencoders for Text Generation}},
	author       = {Savinov, Nikolay and Chung, Junyoung and Binkowski, Mikolaj and Elsen, Erich and van den Oord, Aaron},
	year         = 2021,
	booktitle    = {ICLR}
}
@inproceedings{savinov2022stepunrolled,
	title        = {{Step-unrolled Denoising Autoencoders for Text Generation}},
	author       = {Nikolay Savinov and Junyoung Chung and Mikolaj Binkowski and Erich Elsen and Aaron van den Oord},
	year         = 2022,
	booktitle    = {International Conference on Learning Representations},
	url          = {https://openreview.net/forum?id=T0GpzBQ1Fg6}
}
@article{Shen2023NaturalSpeech2L,
	title        = {{NaturalSpeech 2: Latent Diffusion Models are Natural and Zero-Shot Speech and Singing Synthesizers}},
	author       = {Kai Shen and Zeqian Ju and Xu Tan and Yanqing Liu and Yichong Leng and Lei He and Tao Qin and Sheng Zhao and Jiang Bian},
	year         = 2023,
	journal      = {ArXiv},
	volume       = {abs/2304.09116}
}
@inproceedings{sohl2015deep,
	title        = {{Deep unsupervised learning using nonequilibrium thermodynamics}},
	author       = {Sohl-Dickstein, Jascha and Weiss, Eric and Maheswaranathan, Niru and Ganguli, Surya},
	year         = 2015,
	booktitle    = {ICML}
}
@article{song2019generative,
	title        = {{Generative modeling by estimating gradients of the data distribution}},
	author       = {Song, Yang and Ermon, Stefano},
	year         = 2019,
	journal      = {NeurIPS}
}
@inproceedings{song2020denoising,
	title        = {{Denoising Diffusion Implicit Models}},
	author       = {Song, Jiaming and Meng, Chenlin and Ermon, Stefano},
	year         = 2020,
	booktitle    = {ICLR}
}
@inproceedings{song2021scorebased,
	title        = {{Score-Based Generative Modeling through Stochastic Differential Equations}},
	author       = {Yang Song and Jascha Sohl-Dickstein and Diederik P. Kingma and Abhishek Kumar and Stefano Ermon and Ben Poole},
	year         = 2021,
	booktitle    = {ICLR}
}
@inproceedings{song2023consistency,
	title        = {{Consistency Models}},
	author       = {Yang Song and Prafulla Dhariwal and Mark Chen and Ilya Sutskever},
	year         = 2023,
	booktitle    = {ICML}
}
@article{srivastava2022beyond,
	title        = {{Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models}},
	author       = {Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam and Brown, Adam R and Santoro, Adam and Gupta, Aditya and Garriga-Alonso, Adri{\`a} and others},
	year         = 2022,
	journal      = {arXiv preprint arXiv:2206.04615}
}
@article{strudel2022self,
	title        = {{Self-conditioned embedding diffusion for text generation}},
	author       = {Strudel, Robin and Tallec, Corentin and Altch{\'e}, Florent and Du, Yilun and Ganin, Yaroslav and Mensch, Arthur and Grathwohl, Will and Savinov, Nikolay and Dieleman, Sander and Sifre, Laurent and others},
	year         = 2022,
	journal      = {arXiv preprint arXiv:2211.04236}
}
@inproceedings{sun2022score,
	title        = {{Score-based Continuous-time Discrete Diffusion Models}},
	author       = {Sun, Haoran and Yu, Lijun and Dai, Bo and Schuurmans, Dale and Dai, Hanjun},
	year         = 2023,
	journal      = {ICLR}
}
@article{suzgun2022challenging,
	title        = {{Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them}},
	author       = {Suzgun, Mirac and Scales, Nathan and Sch{\"a}rli, Nathanael and Gehrmann, Sebastian and Tay, Yi and Chung, Hyung Won and Chowdhery, Aakanksha and Le, Quoc V and Chi, Ed H and Zhou, Denny and and Wei, Jason},
	year         = 2022,
	journal      = {arXiv preprint arXiv:2210.09261}
}
@inproceedings{tae2022editts,
	title        = {{EdiTTS: Score-based Editing for Controllable Text-to-Speech}},
	author       = {Jaesung Tae and Hyeongju Kim and Taesu Kim},
	year         = 2022,
	booktitle    = {Interspeech}
}
@inproceedings{tay2023ul,
	title        = {{{UL}2: Unifying Language Learning Paradigms}},
	author       = {Yi Tay and Mostafa Dehghani and Vinh Q. Tran and Xavier Garcia and Jason Wei and Xuezhi Wang and Hyung Won Chung and Dara Bahri and Tal Schuster and Steven Zheng and Denny Zhou and Neil Houlsby and Donald Metzler},
	year         = 2023,
	booktitle    = {The Eleventh International Conference on Learning Representations},
	url          = {https://openreview.net/forum?id=6ruVLB727MC}
}
@misc{touvron2023llama2openfoundation,
	title        = {{Llama 2: Open Foundation and Fine-Tuned Chat Models}},
	author       = {Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
	year         = 2023,
	url          = {https://arxiv.org/abs/2307.09288},
	eprint       = {2307.09288},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@inproceedings{valmeekam2023on,
	title        = {{On the Planning Abilities of Large Language Models - A Critical Investigation}},
	author       = {Karthik Valmeekam and Matthew Marquez and Sarath Sreedharan and Subbarao Kambhampati},
	year         = 2023,
	booktitle    = {Thirty-seventh Conference on Neural Information Processing Systems},
	url          = {https://openreview.net/forum?id=X6dEqXIsEW}
}
@article{vaswani2017attention,
	title        = {{Attention is all you need}},
	author       = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
	year         = 2017,
	journal      = {NeurIPS}
}
@inproceedings{wang2018glue,
	title        = {{{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding}},
	author       = {Alex Wang and Amanpreet Singh and Julian Michael and Felix Hill and Omer Levy and Samuel R. Bowman},
	year         = 2019,
	booktitle    = {ICLR}
}
@inproceedings{wang2023how,
	title        = {{How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources}},
	author       = {Yizhong Wang and Hamish Ivison and Pradeep Dasigi and Jack Hessel and Tushar Khot and Khyathi Chandu and David Wadden and Kelsey MacMillan and Noah A. Smith and Iz Beltagy and Hannaneh Hajishirzi},
	year         = 2023,
	booktitle    = {Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
	url          = {https://openreview.net/forum?id=w4zZNC4ZaV}
}
@inproceedings{wei2022chain,
	title        = {{Chain of Thought Prompting Elicits Reasoning in Large Language Models}},
	author       = {Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and brian ichter and Fei Xia and Ed H. Chi and Quoc V Le and Denny Zhou},
	year         = 2022,
	booktitle    = {Advances in Neural Information Processing Systems},
	editor       = {Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho}
}
@inproceedings{wei2022finetuned,
	title        = {{Finetuned Language Models are Zero-Shot Learners}},
	author       = {Jason Wei and Maarten Bosma and Vincent Zhao and Kelvin Guu and Adams Wei Yu and Brian Lester and Nan Du and Andrew M. Dai and Quoc V Le},
	year         = 2022,
	booktitle    = {International Conference on Learning Representations},
	url          = {https://openreview.net/forum?id=gEZrGCozdqR}
}
@inproceedings{welleck2019neural,
	title        = {{Neural Text Generation With Unlikelihood Training}},
	author       = {Welleck, Sean and Kulikov, Ilia and Roller, Stephen and Dinan, Emily and Cho, Kyunghyun and Weston, Jason},
	year         = 2019,
	booktitle    = {ICLR}
}
@inproceedings{wolf-etal2020transformers,
	title        = {{Transformers: State-of-the-Art Natural Language Processing}},
	author       = {Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush},
	year         = 2020,
	booktitle    = {EMNLP: System Demonstrations}
}
@inproceedings{xie2024travelplanner,
	title        = {{TravelPlanner: A Benchmark for Real-World Planning with Language Agents}},
	author       = {Xie, Jian and Zhang, Kai and Chen, Jiangjie and Zhu, Tinghui and Lou, Renze and Tian, Yuandong and Xiao, Yanghua and Su, Yu},
	year         = 2024,
	booktitle    = {Forty-first International Conference on Machine Learning}
}
@article{xiong2023gibbs,
	title        = {{Gibbs sampling from human feedback: A provable kl-constrained framework for rlhf}},
	author       = {Xiong, Wei and Dong, Hanze and Ye, Chenlu and Zhong, Han and Jiang, Nan and Zhang, Tong},
	year         = 2023,
	journal      = {arXiv preprint arXiv:2312.11456}
}
@article{xu2016optimizing,
	title        = {{Optimizing statistical machine translation for text simplification}},
	author       = {Xu, Wei and Napoles, Courtney and Pavlick, Ellie and Chen, Quanze and Callison-Burch, Chris},
	year         = 2016,
	journal      = {TACL}
}
@misc{ye2023diffusionlanguagemodelsperform,
	title        = {{Diffusion Language Models Can Perform Many Tasks with Scaling and Instruction-Finetuning}},
	author       = {Jiasheng Ye and Zaixiang Zheng and Yu Bao and Lihua Qian and Quanquan Gu},
	year         = 2023,
	url          = {https://arxiv.org/abs/2308.12219},
	eprint       = {2308.12219},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@misc{ye2023dinoiser,
	title        = {{DINOISER: Diffused Conditional Sequence Learning by Manipulating Noises}},
	author       = {Jiasheng Ye and Zaixiang Zheng and Yu Bao and Lihua Qian and Mingxuan Wang},
	year         = 2023,
	eprint       = {2302.10025},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@inproceedings{ye2024diffusion,
	title        = {{Diffusion of Thought: Chain-of-Thought Reasoning in Diffusion Language Models}},
	author       = {Jiacheng Ye and Shansan Gong and Liheng Chen and Lin Zheng and Jiahui Gao and Han Shi and Chuan Wu and Xin Jiang and Zhenguo Li and Wei Bi and Lingpeng Kong},
	year         = 2024,
	booktitle    = {The Thirty-eighth Annual Conference on Neural Information Processing Systems},
	url          = {https://openreview.net/forum?id=G0v0TxX01N}
}
@article{yuan2022seqdiffuseq,
	title        = {{SeqDiffuSeq: Text Diffusion with Encoder-Decoder Transformers}},
	author       = {Yuan, Hongyi and Yuan, Zheng and Tan, Chuanqi and Huang, Fei and Huang, Songfang},
	year         = 2022,
	journal      = {arXiv preprint arXiv:2212.10325}
}
@inproceedings{zhang2017sentence,
	title        = {{Sentence Simplification with Deep Reinforcement Learning}},
	author       = {Zhang, Xingxing and Lapata, Mirella},
	year         = 2017,
	booktitle    = {EMNLP}
}
@inproceedings{zhang2019bertscore,
	title        = {{{BERT}score: Evaluating text generation with {BERT}}},
	author       = {Zhang, Tianyi and Kishore, Varsha and Wu, Felix and Weinberger, Kilian Q and Artzi, Yoav},
	year         = 2020,
	booktitle    = {ICLR}
}
@inproceedings{zhang2020revisiting,
	title        = {{Revisiting Few-sample BERT Fine-tuning}},
	author       = {Zhang, Tianyi and Wu, Felix and Katiyar, Arzoo and Weinberger, Kilian Q and Artzi, Yoav},
	year         = 2021,
	booktitle    = {ICLR}
}
@inproceedings{zhang2023planner,
	title        = {{{PLANNER}: Generating Diversified Paragraph via Latent Language Diffusion Model}},
	author       = {Yizhe Zhang and Jiatao Gu and Zhuofeng Wu and Shuangfei Zhai and Joshua M. Susskind and Navdeep Jaitly},
	year         = 2023,
	booktitle    = {Thirty-seventh Conference on Neural Information Processing Systems},
	url          = {https://openreview.net/forum?id=SLwy8UVS8Y}
}
@misc{zheng2024reparameterizeddiscretediffusionmodel,
	title        = {{A Reparameterized Discrete Diffusion Model for Text Generation}},
	author       = {Lin Zheng and Jianbo Yuan and Lei Yu and Lingpeng Kong},
	year         = 2024,
	url          = {https://arxiv.org/abs/2302.05737},
	eprint       = {2302.05737},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@article{zhou2023instruction,
	title        = {{Instruction-Following Evaluation for Large Language Models}},
	author       = {Zhou, Jeffrey and Lu, Tianjian and Mishra, Swaroop and Brahma, Siddhartha and Basu, Sujoy and Luan, Yi and Zhou, Denny and Hou, Le},
	year         = 2023,
	journal      = {arXiv preprint arXiv:2311.07911}
}
