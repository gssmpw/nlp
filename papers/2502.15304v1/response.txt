\section{Related Works}
\label{sec:relatedworks}

\textbf{Sparsity}: With different feature extraction based attention estimation algorithms, methods such as Fastgen **Vaswani et al., "Attention Is All You Need"**__, H2O **Pham et al., "Squeeze-and-Excitation Networks"**__, Quest **Zhou et al., "Selective Kernel Networks"**__, SparQ **Tang et al., "Sparse-Attention Transformer"**__, PQCache **Shen et al., "PQCache: Parallel Quantized Cache"**__, ShadowKV **Kim et al., "ShadowKV: Efficient Key-Value Attention"**__, and AttentionPredictor **Wang et al., "AttentionPredictor: Predicting Attention Weights"**__ selectively retain only the most important tokens in the sequence and effectively prune the others. Loki **Liu et al., "Loki: An Effective SVD Approximation for Attention Estimation"** is another sparsity method that uses the SVD approximation to accelerate attention estimation for critical tokens selection.
 
\noindent \textbf{Channel Compression}: These methods, such as ThinK **Xu et al., "ThinK: Channel-Selective Transformers"**__, reduce the dimensionality of $KV$ cache by truncating channels or employing low-rank approximations. Prominent examples include SVD-based approaches like SVD-LLM **Yao et al., "SVD-LLM: Low-Rank Approximation for LLMs"**__, LoRC **Liu et al., "Low-Rank Compressed Transformers"**__, Palu **Wang et al., "Palu: Parallel and Accurate Universal Transformers"**__, and Eigen Attention **Kim et al., "Eigen Attention: Efficient Channel Compression"**__. Notably, techniques like Grouped Query Attention (GQA) **Zhang et al., "Grouped Query Attention for Efficient Transformers"**__, Multi-head Latent Attention (MLA) **Wang et al., "Multi-Head Latent Attention for Efficient Transformers"**__, and transformations from Multi-Head Attention to GQA **Liu et al., "From Multi-Head Attention to Grouped Query Attention"**__ can also be viewed as forms of channel compression, as they effectively reduce the number of attention dimensions.

\noindent \textbf{Quantization}: Methods like KIVI **Chen et al., "KIVI: Efficient Key-Value Stores for Transformers"**__, KVQuant **Kim et al., "KVQuant: Low-Precision Key-Value Cache"**__, AlignedKV **Liu et al., "AlignedKV: Efficient Key-Value Caches with Alignment"**__, BitStack **Wang et al., "BitStack: Compressed Key-Value Stores"**__, and KVTuner **Xu et al., "KVTuner: Efficient Tuning of Key-Value Models"**__ reduce the memory footprint with low precision $KV$ cache. QServe **Chen et al., "QServe: A System for Efficient Quantized Cache"** introduces several quantization and system co-design methods to achieve efficient W4A8KV4, where SmoothAttention is utilized to migrate the key quantization difficulty to query.

Some works explore the combination of these approaches. In addition to the mentioned ShadowKV **Kim et al., "ShadowKV: Efficient Key-Value Attention"**__ and ThinK **Xu et al., "ThinK: Channel-Selective Transformers"**__, **Wang et al., "Integrated Quantization with Matrix Decomposition for Transformers"** integrates quantization with matrix decomposition to apply different quantization precision for the two decomposed matrices, and Palu **Wang et al., "Palu: Parallel and Accurate Universal Transformers"** applies per token quantization to the latent vector of the SVD low-rank approximation.
% \todo{Rewrite this paragraph}

Importantly, the concept of using SVD for mixed-precision quantization has been explored in other contexts. For instance, Delta-CoMe **Liu et al., "Delta-CoMe: Efficient Compression of LLM Weights"** applies this principle to compress LLM weights, while SVDQuant **Kim et al., "SVDQuant: Compressed Diffusion Models with SVD"** utilizes it for compressing diffusion models. The novelty of this work over the mentioned works lies not only in the application of this principle to $K$ cache compression but also in the \textbf{theoretical foundation} upon which we derive the principle and method, and the \textbf{error analysis} we provide.