@misc{ainslie_gqa_2023,
	title = {{GQA}: {Training} {Generalized} {Multi}-{Query} {Transformer} {Models} from {Multi}-{Head} {Checkpoints}},
	shorttitle = {{GQA}},
	url = {http://arxiv.org/abs/2305.13245},
	doi = {10.48550/arXiv.2305.13245},
	abstract = {Multi-query attention (MQA), which only uses a single key-value head, drastically speeds up decoder inference. However, MQA can lead to quality degradation, and moreover it may not be desirable to train a separate model just for faster inference. We (1) propose a recipe for uptraining existing multi-head language model checkpoints into models with MQA using 5\% of original pre-training compute, and (2) introduce grouped-query attention (GQA), a generalization of multi-query attention which uses an intermediate (more than one, less than number of query heads) number of key-value heads. We show that uptrained GQA achieves quality close to multi-head attention with comparable speed to MQA.},
	urldate = {2025-02-04},
	publisher = {arXiv},
	author = {Ainslie, Joshua and Lee-Thorp, James and Jong, Michiel de and Zemlyanskiy, Yury and Lebrón, Federico and Sanghai, Sumit},
	month = dec,
	year = {2023},
	note = {arXiv:2305.13245 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\h84392984\\Zotero\\storage\\M65D38CQ\\Ainslie 等 - 2023 - GQA Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints.pdf:application/pdf;Snapshot:C\:\\Users\\h84392984\\Zotero\\storage\\6VSRF8FF\\2305.html:text/html},
}

@misc{chang_palu_2024,
	title = {Palu: {Compressing} {KV}-{Cache} with {Low}-{Rank} {Projection}},
	shorttitle = {Palu},
	url = {http://arxiv.org/abs/2407.21118},
	abstract = {Post-training KV-Cache compression methods typically either sample a subset of effectual tokens or quantize the data into lower numerical bit width. However, these methods cannot exploit redundancy in the hidden dimension of the KV tensors. This paper presents a hidden dimension compression approach called Palu, a KV-Cache compression framework that utilizes low-rank projection to reduce inference-time LLM memory usage. Palu decomposes the linear layers into low-rank matrices, caches compressed intermediate states, and reconstructs the full keys and values on the fly. To improve accuracy, compression rate, and efficiency, Palu further encompasses (1) a medium-grained low-rank decomposition scheme, (2) an efficient rank search algorithm, (3) low-rank-aware quantization compatibility enhancements, and (4) optimized GPU kernels with operators fusion. Extensive experiments with popular LLMs show that Palu compresses KV-Cache by 50\% while maintaining strong accuracy and delivering up to 1.89x on the RoPE-based attention module. When combined with quantization, Palu's inherent quantization-friendly design yields small to negligible extra accuracy degradation while saving additional memory than quantization-only methods and achieving up to 2.91x speedup for the RoPE-based attention. Moreover, it maintains comparable or even better accuracy (up to 1.19 lower perplexity) compared to quantization-only methods. These results demonstrate Palu's superior capability to effectively address the efficiency and memory challenges of LLM inference posed by KV-Cache. Our code is publicly available at: https://github.com/shadowpa0327/Palu},
	urldate = {2024-11-20},
	publisher = {arXiv},
	author = {Chang, Chi-Chih and Lin, Wei-Cheng and Lin, Chien-Yu and Chen, Chong-Yan and Hu, Yu-Fang and Wang, Pei-Shuo and Huang, Ning-Chi and Ceze, Luis and Abdelfattah, Mohamed S. and Wu, Kai-Chiang},
	month = nov,
	year = {2024},
	note = {arXiv:2407.21118 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:C\:\\Users\\h84392984\\Zotero\\storage\\S6RZAW6U\\Chang 等 - 2024 - Palu Compressing KV-Cache with Low-Rank Projection.pdf:application/pdf;Snapshot:C\:\\Users\\h84392984\\Zotero\\storage\\YYVRP3PZ\\2407.html:text/html},
}

@misc{chen_optimised_2024,
	title = {Optimised {Grouped}-{Query} {Attention} {Mechanism} for {Transformers}},
	url = {http://arxiv.org/abs/2406.14963},
	doi = {10.48550/arXiv.2406.14963},
	abstract = {Grouped-query attention (GQA) has been widely adopted in LLMs to mitigate the complexity of multi-head attention (MHA). To transform an MHA to a GQA, neighbour queries in MHA are evenly split into groups where each group shares the value and key layers. In this work, we propose AsymGQA, an activation-informed approach to asymmetrically grouping an MHA to a GQA for better model performance. Our AsymGQA outperforms the GQA within the same model size budget. For example, AsymGQA LLaMA-2-7B has an accuracy increase of 7.5\% on MMLU compared to neighbour grouping. Our approach addresses the GQA's trade-off problem between model performance and hardware efficiency.},
	urldate = {2025-01-10},
	publisher = {arXiv},
	author = {Chen, Yuang and Zhang, Cheng and Gao, Xitong and Mullins, Robert D. and Constantinides, George A. and Zhao, Yiren},
	month = jun,
	year = {2024},
	note = {arXiv:2406.14963 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\h84392984\\Zotero\\storage\\3THTJPV7\\Chen 等 - 2024 - Optimised Grouped-Query Attention Mechanism for Transformers.pdf:application/pdf;Snapshot:C\:\\Users\\h84392984\\Zotero\\storage\\JWISV45V\\2406.html:text/html},
}

@misc{deepseek-ai_deepseek-r1_2025,
	title = {{DeepSeek}-{R1}: {Incentivizing} {Reasoning} {Capability} in {LLMs} via {Reinforcement} {Learning}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {{DeepSeek}-{R1}},
	url = {https://arxiv.org/abs/2501.12948},
	doi = {10.48550/ARXIV.2501.12948},
	abstract = {We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.},
	urldate = {2025-02-03},
	publisher = {arXiv},
	author = {{DeepSeek-AI} and Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and others},
	year = {2025},
	note = {Version Number: 1},
	keywords = {Artificial Intelligence (cs.AI), Computation and Language (cs.CL), FOS: Computer and information sciences, Machine Learning (cs.LG)},
	file = {Preprint PDF:C\:\\Users\\h84392984\\Zotero\\storage\\LLLGQWR6\\DeepSeek-AI 等 - 2025 - DeepSeek-R1 Incentivizing Reasoning Capability in LLMs via Reinforcement Learning.pdf:application/pdf},
}

@article{ge2023fastgen,
  title={Model tells you what to discard: Adaptive kv cache compression for llms},
  author={Ge, Suyu and Zhang, Yunan and Liu, Liyuan and Zhang, Minjia and Han, Jiawei and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2310.01801},
  year={2023}
}

@article{hooper2024kvquant,
  title={Kvquant: Towards 10 million context length llm inference with kv cache quantization},
  author={Hooper, Coleman and Kim, Sehoon and Mohammadzadeh, Hiva and Mahoney, Michael W and Shao, Yakun Sophia and Keutzer, Kurt and Gholami, Amir},
  journal={arXiv preprint arXiv:2401.18079},
  year={2024}
}

@misc{jin_align_2024,
	title = {Align {Attention} {Heads} {Before} {Merging} {Them}: {An} {Effective} {Way} for {Converting} {MHA} to {GQA}},
	shorttitle = {Align {Attention} {Heads} {Before} {Merging} {Them}},
	url = {http://arxiv.org/abs/2412.20677},
	doi = {10.48550/arXiv.2412.20677},
	abstract = {Large language models have been shown to perform well on a variety of natural language processing problems. However, as the model size and the input sequence's length increase, the rapid increase of KV Cache significantly slows down inference speed. Therefore GQA model, as an alternative to MHA model, has been widely introduced into LLMs. In this work, we propose a low-cost method for pruning MHA models into GQA models with any compression ratio of key-value heads. Our method is based on \${\textbackslash}mathit\{L\_0\}\$ masks to gradually remove redundant parameters. In addition, we apply orthogonal transformations to attention heads without changing the model to increase similarity between attention heads before pruning training, in order to further improve performance of the model. Our method can be compatible with rotary position embedding (RoPE), which means the model after training can be fully adapted to the mainstream standard GQA framework. Experiments demonstrate that our strategy can compress up to 87.5\% of key-value heads of the LLaMA2-7B model without too much performance degradation, just achieved through supervised fine-tuning.},
	urldate = {2025-01-08},
	publisher = {arXiv},
	author = {Jin, Qingyun and Song, Xiaohui and Zhou, Feng and Qin, Zengchang},
	month = dec,
	year = {2024},
	note = {arXiv:2412.20677 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\h84392984\\Zotero\\storage\\5KTYNE4A\\Jin 等 - 2024 - Align Attention Heads Before Merging Them An Effective Way for Converting MHA to GQA.pdf:application/pdf;Snapshot:C\:\\Users\\h84392984\\Zotero\\storage\\LH7MVNJB\\2412.html:text/html},
}

@misc{li2025kvtuner,
      title={KVTuner: Sensitivity-Aware Layer-wise Mixed Precision KV Cache Quantization for Efficient and Nearly Lossless LLM Inference}, 
      author={Xing Li and Zeyu Xing and Yiming Li and Linping Qu and Hui-Ling Zhen and Wulong Liu and Yiwu Yao and Sinno Jialin Pan and Mingxuan Yuan},
      year={2025},
      eprint={2502.04420},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2502.04420}, 
}

@misc{li_svdquant_2024,
	title = {{SVDQuant}: {Absorbing} {Outliers} by {Low}-{Rank} {Components} for 4-{Bit} {Diffusion} {Models}},
	shorttitle = {{SVDQuant}},
	url = {http://arxiv.org/abs/2411.05007},
	abstract = {Diffusion models have been proven highly effective at generating high-quality images. However, as these models grow larger, they require significantly more memory and suffer from higher latency, posing substantial challenges for deployment. In this work, we aim to accelerate diffusion models by quantizing their weights and activations to 4 bits. At such an aggressive level, both weights and activations are highly sensitive, where conventional post-training quantization methods for large language models like smoothing become insufficient. To overcome this limitation, we propose SVDQuant, a new 4-bit quantization paradigm. Different from smoothing which redistributes outliers between weights and activations, our approach absorbs these outliers using a low-rank branch. We first consolidate the outliers by shifting them from activations to weights, then employ a high-precision low-rank branch to take in the weight outliers with Singular Value Decomposition (SVD). This process eases the quantization on both sides. However, na{\textbackslash}"\{{\textbackslash}i\}vely running the low-rank branch independently incurs significant overhead due to extra data movement of activations, negating the quantization speedup. To address this, we co-design an inference engine Nunchaku that fuses the kernels of the low-rank branch into those of the low-bit branch to cut off redundant memory access. It can also seamlessly support off-the-shelf low-rank adapters (LoRAs) without the need for re-quantization. Extensive experiments on SDXL, PixArt-\${\textbackslash}Sigma\$, and FLUX.1 validate the effectiveness of SVDQuant in preserving image quality. We reduce the memory usage for the 12B FLUX.1 models by 3.5\${\textbackslash}times\$, achieving 3.0\${\textbackslash}times\$ speedup over the 4-bit weight-only quantized baseline on the 16GB laptop 4090 GPU, paving the way for more interactive applications on PCs. Our quantization library and inference engine are open-sourced.},
	urldate = {2024-11-21},
	publisher = {arXiv},
	author = {Li, Muyang and Lin, Yujun and Zhang, Zhekai and Cai, Tianle and Li, Xiuyu and Guo, Junxian and Xie, Enze and Meng, Chenlin and Zhu, Jun-Yan and Han, Song},
	month = nov,
	year = {2024},
	note = {arXiv:2411.05007 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:C\:\\Users\\h84392984\\Zotero\\storage\\X6V79UTK\\Li 等 - 2024 - SVDQuant Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models.pdf:application/pdf;Snapshot:C\:\\Users\\h84392984\\Zotero\\storage\\QHRLZ79V\\2411.html:text/html},
}

@misc{lin_qserve_2024,
	title = {{QServe}: {W4A8KV4} {Quantization} and {System} {Co}-design for {Efficient} {LLM} {Serving}},
	shorttitle = {{QServe}},
	url = {http://arxiv.org/abs/2405.04532},
	abstract = {Quantization can accelerate large language model (LLM) inference. Going beyond INT8 quantization, the research community is actively exploring even lower precision, such as INT4. Nonetheless, state-of-the-art INT4 quantization techniques only accelerate low-batch, edge LLM inference, failing to deliver performance gains in large-batch, cloud-based LLM serving. We uncover a critical issue: existing INT4 quantization methods suffer from significant runtime overhead (20-90\%) when dequantizing either weights or partial sums on GPUs. To address this challenge, we introduce QoQ, a W4A8KV4 quantization algorithm with 4-bit weight, 8-bit activation, and 4-bit KV cache. QoQ stands for quattuor-octo-quattuor, which represents 4-8-4 in Latin. QoQ is implemented by the QServe inference library that achieves measured speedup. The key insight driving QServe is that the efficiency of LLM serving on GPUs is critically influenced by operations on low-throughput CUDA cores. Building upon this insight, in QoQ algorithm, we introduce progressive quantization that can allow low dequantization overhead in W4A8 GEMM. Additionally, we develop SmoothAttention to effectively mitigate the accuracy degradation incurred by 4-bit KV quantization. In the QServe system, we perform compute-aware weight reordering and take advantage of register-level parallelism to reduce dequantization latency. We also make fused attention memory-bound, harnessing the performance gain brought by KV4 quantization. As a result, QServe improves the maximum achievable serving throughput of Llama-3-8B by 1.2x on A100, 1.4x on L40S; and Qwen1.5-72B by 2.4x on A100, 3.5x on L40S, compared to TensorRT-LLM. Remarkably, QServe on L40S GPU can achieve even higher throughput than TensorRT-LLM on A100. Thus, QServe effectively reduces the dollar cost of LLM serving by 3x. Code is available at https://github.com/mit-han-lab/qserve.},
	urldate = {2024-11-20},
	publisher = {arXiv},
	author = {Lin, Yujun and Tang, Haotian and Yang, Shang and Zhang, Zhekai and Xiao, Guangxuan and Gan, Chuang and Han, Song},
	month = may,
	year = {2024},
	note = {arXiv:2405.04532 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Performance},
	file = {Preprint PDF:C\:\\Users\\h84392984\\Zotero\\storage\\WWKJE2E4\\Lin 等 - 2024 - QServe W4A8KV4 Quantization and System Co-design for Efficient LLM Serving.pdf:application/pdf;Snapshot:C\:\\Users\\h84392984\\Zotero\\storage\\7UBKAEKM\\2405.html:text/html},
}

@misc{liu_kivi_2023,
	title = {{KIVI}: {A} {Tuning}-{Free} {Asymmetric} 2bit {Quantization} for {KV} {Cache}},
	shorttitle = {{KIVI}},
	url = {http://arxiv.org/abs/2402.02750},
	doi = {10.13140/RG.2.2.28167.37282},
	abstract = {Efficiently serving large language models (LLMs) requires batching of many requests to reduce the cost per request. Yet, with larger batch sizes and longer context lengths, the key-value (KV) cache, which stores attention keys and values to avoid re-computations, significantly increases memory demands and becomes the new bottleneck in speed and memory usage. Additionally, the loading of the KV cache causes the computational core to be idle, which limits the inference speed. A straightforward and effective solution to reduce KV cache size is quantization, which decreases the total bytes taken by KV cache. However, there is a lack of in-depth studies that explore the element distribution of KV cache to understand the hardness and limitation of KV cache quantization. To fill the gap, we conducted a comprehensive study on the element distribution in KV cache of popular LLMs. Our findings indicate that the key cache should be quantized per-channel, i.e., group elements along the channel dimension and quantize them together. In contrast, the value cache should be quantized per-token. From this analysis, we developed a tuning-free 2bit KV cache quantization algorithm named KIVI. With hardware-friendly implementation, KIVI can enable Llama, Falcon, and Mistral models to maintain almost the same quality while using \${\textbackslash}mathbf\{2.6{\textbackslash}times\}\$ less peak memory (including model weight). This reduction in memory usage enables up to \${\textbackslash}mathbf\{4{\textbackslash}times\}\$ larger batch size, bringing \${\textbackslash}mathbf\{2.35{\textbackslash}times {\textbackslash}sim 3.47{\textbackslash}times\}\$ throughput on real LLM inference workload. The source code is available at https://github.com/jy-yuan/KIVI.},
	urldate = {2024-11-15},
	publisher = {arXiv},
	author = {Liu, Zirui and Yuan, Jiayi and Jin, Hongye and Zhong, Shaochen and Xu, Zhaozhuo and Braverman, Vladimir and Chen, Beidi and Hu, Xia},
	year = {2023},
	note = {arXiv:2402.02750 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Performance},
	file = {Preprint PDF:C\:\\Users\\h84392984\\Zotero\\storage\\IRB3ZARZ\\Liu 等 - 2023 - KIVI A Tuning-Free Asymmetric 2bit Quantization for KV Cache.pdf:application/pdf;Snapshot:C\:\\Users\\h84392984\\Zotero\\storage\\RIE4DJU4\\2402.html:text/html},
}

@misc{liu_unlocking_2024,
	title = {Unlocking {Data}-free {Low}-bit {Quantization} with {Matrix} {Decomposition} for {KV} {Cache} {Compression}},
	url = {http://arxiv.org/abs/2405.12591},
	doi = {10.48550/arXiv.2405.12591},
	abstract = {Key-value{\textasciitilde}(KV) caching is an important technique to accelerate the inference of large language models{\textasciitilde}(LLMs), but incurs significant memory overhead. To compress the size of KV cache, existing methods often compromise precision or require extra data for calibration, limiting their practicality in LLM deployment. In this paper, we introduce {\textbackslash}textbf\{DecoQuant\}, a novel data-free low-bit quantization technique based on tensor decomposition methods, to effectively compress KV cache. Our core idea is to adjust the outlier distribution of the original matrix by performing tensor decomposition, so that the quantization difficulties are migrated from the matrix to decomposed local tensors. Specially, we find that outliers mainly concentrate on small local tensors, while large tensors tend to have a narrower value range. Based on this finding, we propose to apply low-bit quantization to the large tensor, while maintaining high-precision representation for the small tensor. Furthermore, we utilize the proposed quantization method to compress the KV cache of LLMs to accelerate the inference and develop an efficient dequantization kernel tailored specifically for DecoQuant. Through extensive experiments, DecoQuant demonstrates remarkable efficiency gains, showcasing up to a \${\textbackslash}sim\$75{\textbackslash}\% reduction in memory footprint while maintaining comparable generation quality.},
	urldate = {2024-12-17},
	publisher = {arXiv},
	author = {Liu, Peiyu and Gao, Ze-Feng and Zhao, Wayne Xin and Ma, Yipeng and Wang, Tao and Wen, Ji-Rong},
	month = may,
	year = {2024},
	note = {arXiv:2405.12591 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\h84392984\\Zotero\\storage\\PK25WYIY\\Liu 等 - 2024 - Unlocking Data-free Low-bit Quantization with Matrix Decomposition for KV Cache Compression.pdf:application/pdf;Snapshot:C\:\\Users\\h84392984\\Zotero\\storage\\78R34JMS\\2405.html:text/html},
}

@misc{ping_delta-come_2024,
	title = {Delta-{CoMe}: {Training}-{Free} {Delta}-{Compression} with {Mixed}-{Precision} for {Large} {Language} {Models}},
	shorttitle = {Delta-{CoMe}},
	url = {http://arxiv.org/abs/2406.08903},
	doi = {10.48550/arXiv.2406.08903},
	abstract = {Fine-tuning is a crucial process for adapting large language models (LLMs) to diverse applications. In certain scenarios, such as multi-tenant serving, deploying multiple LLMs becomes necessary to meet complex demands. Recent studies suggest decomposing a fine-tuned LLM into a base model and corresponding delta weights, which are then compressed using low-rank or low-bit approaches to reduce costs. In this work, we observe that existing low-rank and low-bit compression methods can significantly harm the model performance for task-specific fine-tuned LLMs (e.g., WizardMath for math problems). Motivated by the long-tail distribution of singular values in the delta weights, we propose a delta quantization approach using mixed-precision. This method employs higher-bit representation for singular vectors corresponding to larger singular values. We evaluate our approach on various fine-tuned LLMs, including math LLMs, code LLMs, chat LLMs, and even VLMs. Experimental results demonstrate that our approach performs comparably to full fine-tuned LLMs, surpassing both low-rank and low-bit baselines by a considerable margin. Additionally, we show that our method is compatible with various backbone LLMs, such as Llama-2, Llama-3, and Mistral, highlighting its generalizability.},
	urldate = {2025-01-16},
	publisher = {arXiv},
	author = {Ping, Bowen and Wang, Shuo and Wang, Hanqing and Han, Xu and Xu, Yuzhuang and Yan, Yukun and Chen, Yun and Chang, Baobao and Liu, Zhiyuan and Sun, Maosong},
	month = nov,
	year = {2024},
	note = {arXiv:2406.08903 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\h84392984\\Zotero\\storage\\AQGKSIT3\\Ping 等 - 2024 - Delta-CoMe Training-Free Delta-Compression with Mixed-Precision for Large Language Models.pdf:application/pdf;Snapshot:C\:\\Users\\h84392984\\Zotero\\storage\\VPFPQGAM\\2406.html:text/html},
}

@misc{ribar_sparq_2024,
	title = {{SparQ} {Attention}: {Bandwidth}-{Efficient} {LLM} {Inference}},
	shorttitle = {{SparQ} {Attention}},
	url = {http://arxiv.org/abs/2312.04985},
	doi = {10.48550/arXiv.2312.04985},
	abstract = {The computational difficulties of large language model (LLM) inference remain a significant obstacle to their widespread deployment. The need for many applications to support long input sequences and process them in large batches typically causes token-generation to be bottlenecked by data transfer. For this reason, we introduce SparQ Attention, a technique for increasing the inference throughput of LLMs by utilising memory bandwidth more efficiently within the attention layers, through selective fetching of the cached history. Our proposed technique can be applied directly to off-the-shelf LLMs during inference, without requiring any modification to the pre-training setup or additional fine-tuning. We show that SparQ Attention brings up to 8x savings in attention data transfers without substantial drops in accuracy, by evaluating Llama 2 and 3, Mistral, Gemma and Pythia models on a wide range of downstream tasks.},
	urldate = {2025-01-02},
	publisher = {arXiv},
	author = {Ribar, Luka and Chelombiev, Ivan and Hudlass-Galley, Luke and Blake, Charlie and Luschi, Carlo and Orr, Douglas},
	month = sep,
	year = {2024},
	note = {arXiv:2312.04985 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\h84392984\\Zotero\\storage\\N8ZGK3NP\\Ribar 等 - 2024 - SparQ Attention Bandwidth-Efficient LLM Inference.pdf:application/pdf;Snapshot:C\:\\Users\\h84392984\\Zotero\\storage\\K3V3TZUD\\2312.html:text/html},
}

@misc{saxena_eigen_2024,
	title = {Eigen {Attention}: {Attention} in {Low}-{Rank} {Space} for {KV} {Cache} {Compression}},
	shorttitle = {Eigen {Attention}},
	url = {http://arxiv.org/abs/2408.05646},
	doi = {10.48550/arXiv.2408.05646},
	abstract = {Large language models (LLMs) represent a groundbreaking advancement in the domain of natural language processing due to their impressive reasoning abilities. Recently, there has been considerable interest in increasing the context lengths for these models to enhance their applicability to complex tasks. However, at long context lengths and large batch sizes, the key-value (KV) cache, which stores the attention keys and values, emerges as the new bottleneck in memory usage during inference. To address this, we propose Eigen Attention, which performs the attention operation in a low-rank space, thereby reducing the KV cache memory overhead. Our proposed approach is orthogonal to existing KV cache compression techniques and can be used synergistically with them. Through extensive experiments over OPT, MPT, and Llama model families, we demonstrate that Eigen Attention results in up to 40\% reduction in KV cache sizes and up to 60\% reduction in attention operation latency with minimal drop in performance. Code is available at https://github.com/UtkarshSaxena1/EigenAttn.},
	urldate = {2025-01-16},
	publisher = {arXiv},
	author = {Saxena, Utkarsh and Saha, Gobinda and Choudhary, Sakshi and Roy, Kaushik},
	month = nov,
	year = {2024},
	note = {arXiv:2408.05646 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:C\:\\Users\\h84392984\\Zotero\\storage\\22ZREHAS\\Saxena 等 - 2024 - Eigen Attention Attention in Low-Rank Space for KV Cache Compression.pdf:application/pdf;Snapshot:C\:\\Users\\h84392984\\Zotero\\storage\\ACTWUMJI\\2408.html:text/html},
}

@misc{singhania_loki_2024,
	title = {Loki: {Low}-rank {Keys} for {Efficient} {Sparse} {Attention}},
	shorttitle = {Loki},
	url = {http://arxiv.org/abs/2406.02542},
	doi = {10.48550/arXiv.2406.02542},
	abstract = {Inference on large language models (LLMs) can be expensive in terms of the compute and memory costs involved, especially when long sequence lengths are used. In particular, the self-attention mechanism used in LLM inference contributes significantly to these costs, which has sparked an interest in approximating the self-attention computation to reduce such costs. In this work, we propose to approximate self-attention by focusing on the dimensionality of key vectors computed in the attention block. Our analysis reveals that key vectors lie in a significantly lower-dimensional space, consistently across several datasets and models. Exploiting this observation, we propose Loki, a novel sparse attention method that ranks and selects tokens in the KV-cache based on attention scores computed in low-dimensional space. Our evaluations show that Loki is able to speed up the attention computation due to reduced data movement (load/store) and compute costs while maintaining the efficacy of the models better than other popular approximation methods.},
	urldate = {2025-02-05},
	publisher = {arXiv},
	author = {Singhania, Prajwal and Singh, Siddharth and He, Shwai and Feizi, Soheil and Bhatele, Abhinav},
	month = nov,
	year = {2024},
	note = {arXiv:2406.02542 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\h84392984\\Zotero\\storage\\2T5VQ3ZZ\\Singhania 等 - 2024 - Loki Low-rank Keys for Efficient Sparse Attention.pdf:application/pdf;Snapshot:C\:\\Users\\h84392984\\Zotero\\storage\\YSNTGGGV\\2406.html:text/html},
}

@misc{sun_shadowkv_2024,
	title = {{ShadowKV}: {KV} {Cache} in {Shadows} for {High}-{Throughput} {Long}-{Context} {LLM} {Inference}},
	shorttitle = {{ShadowKV}},
	url = {http://arxiv.org/abs/2410.21465},
	abstract = {With the widespread deployment of long-context large language models (LLMs), there has been a growing demand for efficient support of high-throughput inference. However, as the key-value (KV) cache expands with the sequence length, the increasing memory footprint and the need to access it for each token generation both result in low throughput when serving long-context LLMs. While various dynamic sparse attention methods have been proposed to speed up inference while maintaining generation quality, they either fail to sufficiently reduce GPU memory consumption or introduce significant decoding latency by offloading the KV cache to the CPU. We present ShadowKV, a high-throughput long-context LLM inference system that stores the low-rank key cache and offloads the value cache to reduce the memory footprint for larger batch sizes and longer sequences. To minimize decoding latency, ShadowKV employs an accurate KV selection strategy that reconstructs minimal sparse KV pairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks, including RULER, LongBench, and Needle In A Haystack, and models like Llama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and Qwen2-7B-128K, we demonstrate that it can support up to 6\${\textbackslash}times\$ larger batch sizes and boost throughput by up to 3.04\${\textbackslash}times\$ on an A100 GPU without sacrificing accuracy, even surpassing the performance achievable with infinite batch size under the assumption of infinite GPU memory. The code is available at https://github.com/bytedance/ShadowKV.},
	urldate = {2024-11-15},
	publisher = {arXiv},
	author = {Sun, Hanshi and Chang, Li-Wen and Bao, Wenlei and Zheng, Size and Zheng, Ningxin and Liu, Xin and Dong, Harry and Chi, Yuejie and Chen, Beidi},
	month = oct,
	year = {2024},
	note = {arXiv:2410.21465 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\h84392984\\Zotero\\storage\\EK93Z4PH\\Sun 等 - 2024 - ShadowKV KV Cache in Shadows for High-Throughput Long-Context LLM Inference.pdf:application/pdf;Snapshot:C\:\\Users\\h84392984\\Zotero\\storage\\4XKCEHM9\\2410.html:text/html},
}

@misc{tan_alignedkv_2024,
	title = {{AlignedKV}: {Reducing} {Memory} {Access} of {KV}-{Cache} with {Precision}-{Aligned} {Quantization}},
	shorttitle = {{AlignedKV}},
	url = {http://arxiv.org/abs/2409.16546},
	doi = {10.48550/arXiv.2409.16546},
	abstract = {Model quantization has become a crucial technique to address the issues of large memory consumption and long inference times associated with LLMs. Mixed-precision quantization, which distinguishes between important and unimportant parameters, stands out among numerous quantization schemes as it achieves a balance between precision and compression rate. However, existing approaches can only identify important parameters through qualitative analysis and manual experiments without quantitatively analyzing how their importance is determined. We propose a new criterion, so-called 'precision alignment', to build a quantitative framework to holistically evaluate the importance of parameters in mixed-precision quantization. Our observations on floating point addition under various real-world scenarios suggest that two addends should have identical precision, otherwise the information in the higher-precision number will be wasted. Such an observation offers an essential principle to determine the precision of each parameter in matrix multiplication operation. As the first step towards applying the above discovery to large model inference, we develop a dynamic KV-Cache quantization technique to effectively reduce memory access latency. Different from existing quantization approaches that focus on memory saving, this work directly aims to accelerate LLM inference through quantifying floating numbers. The proposed technique attains a 25\% saving of memory access and delivers up to 1.3x speedup in the computation of attention in the decoding phase of LLM, with almost no loss of precision.},
	urldate = {2025-01-23},
	publisher = {arXiv},
	author = {Tan, Yifan and Wang, Haoze and Yan, Chao and Deng, Yangdong},
	month = oct,
	year = {2024},
	note = {arXiv:2409.16546 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\h84392984\\Zotero\\storage\\3LVGUS6W\\Tan 等 - 2024 - AlignedKV Reducing Memory Access of KV-Cache with Precision-Aligned Quantization.pdf:application/pdf;Snapshot:C\:\\Users\\h84392984\\Zotero\\storage\\VN9P32DL\\2409.html:text/html},
}

@misc{tang_quest_2024,
	title = {Quest: {Query}-{Aware} {Sparsity} for {Efficient} {Long}-{Context} {LLM} {Inference}},
	shorttitle = {Quest},
	url = {http://arxiv.org/abs/2406.10774},
	abstract = {As the demand for long-context large language models (LLMs) increases, models with context windows of up to 128K or 1M tokens are becoming increasingly prevalent. However, long-context LLM inference is challenging since the inference speed decreases significantly as the sequence length grows. This slowdown is primarily caused by loading a large KV cache during self-attention. Previous works have shown that a small portion of critical tokens will dominate the attention outcomes. However, we observe the criticality of a token highly depends on the query. To this end, we propose Quest, a query-aware KV cache selection algorithm. Quest keeps track of the minimal and maximal Key values in KV cache pages and estimates the criticality of a given page using Query vectors. By only loading the Top-K critical KV cache pages for attention, Quest significantly speeds up self-attention without sacrificing accuracy. We show that Quest can achieve up to 2.23x self-attention speedup, which reduces inference latency by 7.03x while performing well on tasks with long dependencies with negligible accuracy loss. Code is available at http://github.com/mit-han-lab/Quest .},
	urldate = {2024-11-15},
	publisher = {arXiv},
	author = {Tang, Jiaming and Zhao, Yilong and Zhu, Kan and Xiao, Guangxuan and Kasikci, Baris and Han, Song},
	month = aug,
	year = {2024},
	note = {arXiv:2406.10774 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\h84392984\\Zotero\\storage\\3XZRND4G\\Tang 等 - 2024 - Quest Query-Aware Sparsity for Efficient Long-Context LLM Inference.pdf:application/pdf;Snapshot:C\:\\Users\\h84392984\\Zotero\\storage\\5WKVRNWA\\2406.html:text/html},
}

@misc{wang_bitstack_2024,
	title = {{BitStack}: {Fine}-{Grained} {Size} {Control} for {Compressed} {Large} {Language} {Models} in {Variable} {Memory} {Environments}},
	shorttitle = {{BitStack}},
	url = {http://arxiv.org/abs/2410.23918},
	doi = {10.48550/arXiv.2410.23918},
	abstract = {Large language models (LLMs) have revolutionized numerous applications, yet their deployment remains challenged by memory constraints on local devices. While scaling laws have enhanced LLM capabilities, the primary bottleneck has shifted from {\textbackslash}textit\{capability\} to {\textbackslash}textit\{availability\}, emphasizing the need for efficient memory management. Traditional compression methods, such as quantization, often require predefined compression ratios and separate compression processes for each setting, complicating deployment in variable memory environments. In this paper, we introduce {\textbackslash}textbf\{BitStack\}, a novel, training-free weight compression approach that enables megabyte-level trade-offs between memory usage and model performance. By leveraging weight decomposition, BitStack can dynamically adjust the model size with minimal transmission between running memory and storage devices. Our approach iteratively decomposes weight matrices while considering the significance of each parameter, resulting in an approximately 1-bit per parameter residual block in each decomposition iteration. These blocks are sorted and stacked in storage as basic transmission units, with different quantities loaded based on current memory availability. Extensive experiments across a wide range of tasks demonstrate that, despite offering fine-grained size control, BitStack consistently matches or surpasses strong quantization baselines, particularly at extreme compression ratios. To the best of our knowledge, this is the first decomposition-based method that effectively bridges the gap to practical compression techniques like quantization. Code is available at https://github.com/xinghaow99/BitStack.},
	urldate = {2024-12-23},
	publisher = {arXiv},
	author = {Wang, Xinghao and Wang, Pengyu and Wang, Bo and Zhang, Dong and Zhou, Yunhua and Qiu, Xipeng},
	month = oct,
	year = {2024},
	note = {arXiv:2410.23918 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:C\:\\Users\\h84392984\\Zotero\\storage\\N4MG4TV2\\Wang 等 - 2024 - BitStack Fine-Grained Size Control for Compressed Large Language Models in Variable Memory Environm.pdf:application/pdf;Snapshot:C\:\\Users\\h84392984\\Zotero\\storage\\ASQG3BJ2\\2410.html:text/html},
}

@misc{wang_svd-llm_2024,
	title = {{SVD}-{LLM}: {Truncation}-aware {Singular} {Value} {Decomposition} for {Large} {Language} {Model} {Compression}},
	shorttitle = {{SVD}-{LLM}},
	url = {http://arxiv.org/abs/2403.07378},
	doi = {10.48550/arXiv.2403.07378},
	abstract = {The advancements in Large Language Models (LLMs) have been hindered by their substantial sizes, which necessitate LLM compression methods for practical deployment. Singular Value Decomposition (SVD) offers a promising solution for LLM compression. However, state-of-the-art SVD-based LLM compression methods have two key limitations: truncating smaller singular values may lead to higher compression loss, and the lack of update on the compressed weight after SVD truncation. In this work, we propose SVD-LLM, a new SVD-based LLM compression method that addresses the limitations of existing methods. SVD-LLM incorporates a truncation-aware data whitening strategy to ensure a direct mapping between singular values and compression loss. Moreover, SVD-LLM adopts a layer-wise closed-form model parameter update strategy to compensate for accuracy degradation under high compression ratios. We evaluate SVD-LLM on a total of 10 datasets and eight models from three different LLM families at four different scales. Our results demonstrate the superiority of SVD-LLM over state-of-the-arts, especially at high model compression ratios.},
	urldate = {2024-12-12},
	publisher = {arXiv},
	author = {Wang, Xin and Zheng, Yu and Wan, Zhongwei and Zhang, Mi},
	month = may,
	year = {2024},
	note = {arXiv:2403.07378 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\h84392984\\Zotero\\storage\\RMBGN4R4\\Wang 等 - 2024 - SVD-LLM Truncation-aware Singular Value Decomposition for Large Language Model Compression.pdf:application/pdf;Snapshot:C\:\\Users\\h84392984\\Zotero\\storage\\7MIY9Q3I\\2403.html:text/html},
}

@misc{xu_think_2024,
	title = {{ThinK}: {Thinner} {Key} {Cache} by {Query}-{Driven} {Pruning}},
	shorttitle = {{ThinK}},
	url = {http://arxiv.org/abs/2407.21018},
	abstract = {Large Language Models (LLMs) have revolutionized the field of natural language processing, achieving unprecedented performance across a variety of applications. However, their increased computational and memory demands present significant challenges, especially when handling long sequences. This paper focuses on the long-context scenario, addressing the inefficiencies in KV cache memory consumption during inference. Unlike existing approaches that optimize the memory based on the sequence length, we identify substantial redundancy in the channel dimension of the KV cache, as indicated by an uneven magnitude distribution and a low-rank structure in the attention weights. In response, we propose ThinK, a novel query-dependent KV cache pruning method designed to minimize attention weight loss while selectively pruning the least significant channels. Our approach not only maintains or enhances model accuracy but also achieves a reduction in KV cache memory costs by over 20\% compared with vanilla KV cache eviction and quantization methods. For instance, ThinK integrated with KIVI can achieve a 2.8x reduction in peak memory usage while maintaining nearly the same quality, enabling up to a 5x increase in batch size when using a single GPU. Extensive evaluations on the LLaMA and Mistral models across various long-sequence datasets verified the efficiency of ThinK, establishing a new baseline algorithm for efficient LLM deployment without compromising performance.},
	urldate = {2024-11-15},
	publisher = {arXiv},
	author = {Xu, Yuhui and Jie, Zhanming and Dong, Hanze and Wang, Lei and Lu, Xudong and Zhou, Aojun and Saha, Amrita and Xiong, Caiming and Sahoo, Doyen},
	month = oct,
	year = {2024},
	note = {arXiv:2407.21018 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:C\:\\Users\\h84392984\\Zotero\\storage\\PRR7H6V6\\Xu 等 - 2024 - ThinK Thinner Key Cache by Query-Driven Pruning.pdf:application/pdf;Snapshot:C\:\\Users\\h84392984\\Zotero\\storage\\JHGB25PP\\2407.html:text/html},
}

@article{yang2025attentionpredictor,
  title={AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference},
  author={Yang, Qingyue and Wang, Jie and Li, Xing and Wang, Zhihai and Chen, Chen and Chen, Lei and Yu, Xianzhi and Liu, Wulong and Hao, Jianye and Yuan, Mingxuan and others},
  journal={arXiv preprint arXiv:2502.04077},
  year={2025}
}

@article{zhang2024h2o,
  title={H2o: Heavy-hitter oracle for efficient generative inference of large language models},
  author={Zhang, Zhenyu and Sheng, Ying and Zhou, Tianyi and Chen, Tianlong and Zheng, Lianmin and Cai, Ruisi and Song, Zhao and Tian, Yuandong and R{\'e}, Christopher and Barrett, Clark and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{zhang2024pqcache,
  title={Pqcache: Product quantization-based kvcache for long context llm inference},
  author={Zhang, Hailin and Ji, Xiaodong and Chen, Yilin and Fu, Fangcheng and Miao, Xupeng and Nie, Xiaonan and Chen, Weipeng and Cui, Bin},
  journal={arXiv preprint arXiv:2407.12820},
  year={2024}
}

@misc{zhang_lorc_2024,
	title = {{LoRC}: {Low}-{Rank} {Compression} for {LLMs} {KV} {Cache} with a {Progressive} {Compression} {Strategy}},
	shorttitle = {{LoRC}},
	url = {http://arxiv.org/abs/2410.03111},
	doi = {10.48550/arXiv.2410.03111},
	abstract = {The Key-Value (KV) cache is a crucial component in serving transformer-based autoregressive large language models (LLMs), enabling faster inference by storing previously computed KV vectors. However, its memory consumption scales linearly with sequence length and batch size, posing a significant bottleneck in LLM deployment. Existing approaches to mitigate this issue include: (1) efficient attention variants integrated in upcycling stages, which requires extensive parameter tuning thus unsuitable for pre-trained LLMs; (2) KV cache compression at test time, primarily through token eviction policies, which often overlook inter-layer dependencies and can be task-specific. This paper introduces an orthogonal approach to KV cache compression. We propose a low-rank approximation of KV weight matrices, allowing for plug-in integration with existing transformer-based LLMs without model retraining. To effectively compress KV cache at the weight level, we adjust for layerwise sensitivity and introduce a progressive compression strategy, which is supported by our theoretical analysis on how compression errors accumulate in deep networks. Our method is designed to function without model tuning in upcycling stages or task-specific profiling in test stages. Extensive experiments with LLaMA models ranging from 8B to 70B parameters across various tasks show that our approach significantly reduces the GPU memory footprint while maintaining performance.},
	urldate = {2025-02-03},
	publisher = {arXiv},
	author = {Zhang, Rongzhi and Wang, Kuang and Liu, Liyuan and Wang, Shuohang and Cheng, Hao and Zhang, Chao and Shen, Yelong},
	month = oct,
	year = {2024},
	note = {arXiv:2410.03111 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\h84392984\\Zotero\\storage\\PD7AIG6S\\Zhang 等 - 2024 - LoRC Low-Rank Compression for LLMs KV Cache with a Progressive Compression Strategy.pdf:application/pdf;Snapshot:C\:\\Users\\h84392984\\Zotero\\storage\\GJ6GUD26\\2410.html:text/html},
}

