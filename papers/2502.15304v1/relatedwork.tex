\section{Related Works}
\label{sec:relatedworks}

\textbf{Sparsity}: With different feature extraction based attention estimation algorithms, methods such as Fastgen \citep{ge2023fastgen}, H2O \citep{zhang2024h2o}, Quest \citep{tang_quest_2024}, SparQ \citep{ribar_sparq_2024}, PQCache \citep{zhang2024pqcache}, ShadowKV \citep{sun_shadowkv_2024}, and AttentionPredictor \citep{yang2025attentionpredictor} selectively retain only the most important tokens in the sequence and effectively prune the others. Loki \cite{singhania_loki_2024} is another sparsity method that uses the SVD approximation to accelerate attention estimation for critical tokens selection.
 
\noindent \textbf{Channel Compression}: These methods, such as ThinK \citep{xu_think_2024}, reduce the dimensionality of $KV$ cache by truncating channels or employing low-rank approximations. Prominent examples include SVD-based approaches like SVD-LLM \citep{wang_svd-llm_2024}, LoRC \citep{zhang_lorc_2024}, Palu \citep{chang_palu_2024}, and Eigen Attention \cite{saxena_eigen_2024}. Notably, techniques like Grouped Query Attention (GQA) \cite{ainslie_gqa_2023}, Multi-head Latent Attention (MLA) \citep{deepseek-ai_deepseek-r1_2025}, and transformations from Multi-Head Attention to GQA \citep{jin_align_2024, chen_optimised_2024} can also be viewed as forms of channel compression, as they effectively reduce the number of attention dimensions.

\noindent \textbf{Quantization}: Methods like KIVI \citep{liu_kivi_2023}, KVQuant \cite{hooper2024kvquant}, AlignedKV \citep{tan_alignedkv_2024}, BitStack \cite{wang_bitstack_2024}, and KVTuner \citep{li2025kvtuner} reduce the memory footprint with low precision $KV$ cache. QServe \cite{lin_qserve_2024} introduces several quantization and system co-design methods to achieve efficient W4A8KV4, where SmoothAttention is utilized to migrate the key quantization difficulty to query.

Some works explore the combination of these approaches. In addition to the mentioned ShadowKV \citep{sun_shadowkv_2024} and ThinK \cite{xu_think_2024}, \cite{liu_unlocking_2024} integrates quantization with matrix decomposition to apply different quantization precision for the two decomposed matrices, and Palu \cite{chang_palu_2024} applies per token quantization to the latent vector of the SVD low-rank approximation.
% \todo{Rewrite this paragraph}

Importantly, the concept of using SVD for mixed-precision quantization has been explored in other contexts. For instance, Delta-CoMe \citep{ping_delta-come_2024} applies this principle to compress LLM weights, while SVDQuant \citep{li_svdquant_2024} utilizes it for compressing diffusion models. The novelty of this work over the mentioned works lies not only in the application of this principle to $K$ cache compression but also in the \textbf{theoretical foundation} upon which we derive the principle and method, and the \textbf{error analysis} we provide.