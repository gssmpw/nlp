
@misc{tang_quest_2024,
	title = {Quest: {Query}-{Aware} {Sparsity} for {Efficient} {Long}-{Context} {LLM} {Inference}},
	shorttitle = {Quest},
	url = {http://arxiv.org/abs/2406.10774},
	abstract = {As the demand for long-context large language models (LLMs) increases, models with context windows of up to 128K or 1M tokens are becoming increasingly prevalent. However, long-context LLM inference is challenging since the inference speed decreases significantly as the sequence length grows. This slowdown is primarily caused by loading a large KV cache during self-attention. Previous works have shown that a small portion of critical tokens will dominate the attention outcomes. However, we observe the criticality of a token highly depends on the query. To this end, we propose Quest, a query-aware KV cache selection algorithm. Quest keeps track of the minimal and maximal Key values in KV cache pages and estimates the criticality of a given page using Query vectors. By only loading the Top-K critical KV cache pages for attention, Quest significantly speeds up self-attention without sacrificing accuracy. We show that Quest can achieve up to 2.23x self-attention speedup, which reduces inference latency by 7.03x while performing well on tasks with long dependencies with negligible accuracy loss. Code is available at http://github.com/mit-han-lab/Quest .},
	urldate = {2024-11-15},
	publisher = {arXiv},
	author = {Tang, Jiaming and Zhao, Yilong and Zhu, Kan and Xiao, Guangxuan and Kasikci, Baris and Han, Song},
	month = aug,
	year = {2024},
	note = {arXiv:2406.10774 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\h84392984\\Zotero\\storage\\3XZRND4G\\Tang 等 - 2024 - Quest Query-Aware Sparsity for Efficient Long-Context LLM Inference.pdf:application/pdf;Snapshot:C\:\\Users\\h84392984\\Zotero\\storage\\5WKVRNWA\\2406.html:text/html},
}

@misc{xu_think_2024,
	title = {{ThinK}: {Thinner} {Key} {Cache} by {Query}-{Driven} {Pruning}},
	shorttitle = {{ThinK}},
	url = {http://arxiv.org/abs/2407.21018},
	abstract = {Large Language Models (LLMs) have revolutionized the field of natural language processing, achieving unprecedented performance across a variety of applications. However, their increased computational and memory demands present significant challenges, especially when handling long sequences. This paper focuses on the long-context scenario, addressing the inefficiencies in KV cache memory consumption during inference. Unlike existing approaches that optimize the memory based on the sequence length, we identify substantial redundancy in the channel dimension of the KV cache, as indicated by an uneven magnitude distribution and a low-rank structure in the attention weights. In response, we propose ThinK, a novel query-dependent KV cache pruning method designed to minimize attention weight loss while selectively pruning the least significant channels. Our approach not only maintains or enhances model accuracy but also achieves a reduction in KV cache memory costs by over 20\% compared with vanilla KV cache eviction and quantization methods. For instance, ThinK integrated with KIVI can achieve a 2.8x reduction in peak memory usage while maintaining nearly the same quality, enabling up to a 5x increase in batch size when using a single GPU. Extensive evaluations on the LLaMA and Mistral models across various long-sequence datasets verified the efficiency of ThinK, establishing a new baseline algorithm for efficient LLM deployment without compromising performance.},
	urldate = {2024-11-15},
	publisher = {arXiv},
	author = {Xu, Yuhui and Jie, Zhanming and Dong, Hanze and Wang, Lei and Lu, Xudong and Zhou, Aojun and Saha, Amrita and Xiong, Caiming and Sahoo, Doyen},
	month = oct,
	year = {2024},
	note = {arXiv:2407.21018 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:C\:\\Users\\h84392984\\Zotero\\storage\\PRR7H6V6\\Xu 等 - 2024 - ThinK Thinner Key Cache by Query-Driven Pruning.pdf:application/pdf;Snapshot:C\:\\Users\\h84392984\\Zotero\\storage\\JHGB25PP\\2407.html:text/html},
}

@misc{liu_kivi_2023,
	title = {{KIVI}: {A} {Tuning}-{Free} {Asymmetric} 2bit {Quantization} for {KV} {Cache}},
	shorttitle = {{KIVI}},
	url = {http://arxiv.org/abs/2402.02750},
	doi = {10.13140/RG.2.2.28167.37282},
	abstract = {Efficiently serving large language models (LLMs) requires batching of many requests to reduce the cost per request. Yet, with larger batch sizes and longer context lengths, the key-value (KV) cache, which stores attention keys and values to avoid re-computations, significantly increases memory demands and becomes the new bottleneck in speed and memory usage. Additionally, the loading of the KV cache causes the computational core to be idle, which limits the inference speed. A straightforward and effective solution to reduce KV cache size is quantization, which decreases the total bytes taken by KV cache. However, there is a lack of in-depth studies that explore the element distribution of KV cache to understand the hardness and limitation of KV cache quantization. To fill the gap, we conducted a comprehensive study on the element distribution in KV cache of popular LLMs. Our findings indicate that the key cache should be quantized per-channel, i.e., group elements along the channel dimension and quantize them together. In contrast, the value cache should be quantized per-token. From this analysis, we developed a tuning-free 2bit KV cache quantization algorithm named KIVI. With hardware-friendly implementation, KIVI can enable Llama, Falcon, and Mistral models to maintain almost the same quality while using \${\textbackslash}mathbf\{2.6{\textbackslash}times\}\$ less peak memory (including model weight). This reduction in memory usage enables up to \${\textbackslash}mathbf\{4{\textbackslash}times\}\$ larger batch size, bringing \${\textbackslash}mathbf\{2.35{\textbackslash}times {\textbackslash}sim 3.47{\textbackslash}times\}\$ throughput on real LLM inference workload. The source code is available at https://github.com/jy-yuan/KIVI.},
	urldate = {2024-11-15},
	publisher = {arXiv},
	author = {Liu, Zirui and Yuan, Jiayi and Jin, Hongye and Zhong, Shaochen and Xu, Zhaozhuo and Braverman, Vladimir and Chen, Beidi and Hu, Xia},
	year = {2023},
	note = {arXiv:2402.02750 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Performance},
	file = {Preprint PDF:C\:\\Users\\h84392984\\Zotero\\storage\\IRB3ZARZ\\Liu 等 - 2023 - KIVI A Tuning-Free Asymmetric 2bit Quantization for KV Cache.pdf:application/pdf;Snapshot:C\:\\Users\\h84392984\\Zotero\\storage\\RIE4DJU4\\2402.html:text/html},
}

@misc{sun_shadowkv_2024,
	title = {{ShadowKV}: {KV} {Cache} in {Shadows} for {High}-{Throughput} {Long}-{Context} {LLM} {Inference}},
	shorttitle = {{ShadowKV}},
	url = {http://arxiv.org/abs/2410.21465},
	abstract = {With the widespread deployment of long-context large language models (LLMs), there has been a growing demand for efficient support of high-throughput inference. However, as the key-value (KV) cache expands with the sequence length, the increasing memory footprint and the need to access it for each token generation both result in low throughput when serving long-context LLMs. While various dynamic sparse attention methods have been proposed to speed up inference while maintaining generation quality, they either fail to sufficiently reduce GPU memory consumption or introduce significant decoding latency by offloading the KV cache to the CPU. We present ShadowKV, a high-throughput long-context LLM inference system that stores the low-rank key cache and offloads the value cache to reduce the memory footprint for larger batch sizes and longer sequences. To minimize decoding latency, ShadowKV employs an accurate KV selection strategy that reconstructs minimal sparse KV pairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks, including RULER, LongBench, and Needle In A Haystack, and models like Llama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and Qwen2-7B-128K, we demonstrate that it can support up to 6\${\textbackslash}times\$ larger batch sizes and boost throughput by up to 3.04\${\textbackslash}times\$ on an A100 GPU without sacrificing accuracy, even surpassing the performance achievable with infinite batch size under the assumption of infinite GPU memory. The code is available at https://github.com/bytedance/ShadowKV.},
	urldate = {2024-11-15},
	publisher = {arXiv},
	author = {Sun, Hanshi and Chang, Li-Wen and Bao, Wenlei and Zheng, Size and Zheng, Ningxin and Liu, Xin and Dong, Harry and Chi, Yuejie and Chen, Beidi},
	month = oct,
	year = {2024},
	note = {arXiv:2410.21465 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\h84392984\\Zotero\\storage\\EK93Z4PH\\Sun 等 - 2024 - ShadowKV KV Cache in Shadows for High-Throughput Long-Context LLM Inference.pdf:application/pdf;Snapshot:C\:\\Users\\h84392984\\Zotero\\storage\\4XKCEHM9\\2410.html:text/html},
}

@misc{chang_palu_2024,
	title = {Palu: {Compressing} {KV}-{Cache} with {Low}-{Rank} {Projection}},
	shorttitle = {Palu},
	url = {http://arxiv.org/abs/2407.21118},
	abstract = {Post-training KV-Cache compression methods typically either sample a subset of effectual tokens or quantize the data into lower numerical bit width. However, these methods cannot exploit redundancy in the hidden dimension of the KV tensors. This paper presents a hidden dimension compression approach called Palu, a KV-Cache compression framework that utilizes low-rank projection to reduce inference-time LLM memory usage. Palu decomposes the linear layers into low-rank matrices, caches compressed intermediate states, and reconstructs the full keys and values on the fly. To improve accuracy, compression rate, and efficiency, Palu further encompasses (1) a medium-grained low-rank decomposition scheme, (2) an efficient rank search algorithm, (3) low-rank-aware quantization compatibility enhancements, and (4) optimized GPU kernels with operators fusion. Extensive experiments with popular LLMs show that Palu compresses KV-Cache by 50\% while maintaining strong accuracy and delivering up to 1.89x on the RoPE-based attention module. When combined with quantization, Palu's inherent quantization-friendly design yields small to negligible extra accuracy degradation while saving additional memory than quantization-only methods and achieving up to 2.91x speedup for the RoPE-based attention. Moreover, it maintains comparable or even better accuracy (up to 1.19 lower perplexity) compared to quantization-only methods. These results demonstrate Palu's superior capability to effectively address the efficiency and memory challenges of LLM inference posed by KV-Cache. Our code is publicly available at: https://github.com/shadowpa0327/Palu},
	urldate = {2024-11-20},
	publisher = {arXiv},
	author = {Chang, Chi-Chih and Lin, Wei-Cheng and Lin, Chien-Yu and Chen, Chong-Yan and Hu, Yu-Fang and Wang, Pei-Shuo and Huang, Ning-Chi and Ceze, Luis and Abdelfattah, Mohamed S. and Wu, Kai-Chiang},
	month = nov,
	year = {2024},
	note = {arXiv:2407.21118 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:C\:\\Users\\h84392984\\Zotero\\storage\\S6RZAW6U\\Chang 等 - 2024 - Palu Compressing KV-Cache with Low-Rank Projection.pdf:application/pdf;Snapshot:C\:\\Users\\h84392984\\Zotero\\storage\\YYVRP3PZ\\2407.html:text/html},
}

@misc{lin_qserve_2024,
	title = {{QServe}: {W4A8KV4} {Quantization} and {System} {Co}-design for {Efficient} {LLM} {Serving}},
	shorttitle = {{QServe}},
	url = {http://arxiv.org/abs/2405.04532},
	abstract = {Quantization can accelerate large language model (LLM) inference. Going beyond INT8 quantization, the research community is actively exploring even lower precision, such as INT4. Nonetheless, state-of-the-art INT4 quantization techniques only accelerate low-batch, edge LLM inference, failing to deliver performance gains in large-batch, cloud-based LLM serving. We uncover a critical issue: existing INT4 quantization methods suffer from significant runtime overhead (20-90\%) when dequantizing either weights or partial sums on GPUs. To address this challenge, we introduce QoQ, a W4A8KV4 quantization algorithm with 4-bit weight, 8-bit activation, and 4-bit KV cache. QoQ stands for quattuor-octo-quattuor, which represents 4-8-4 in Latin. QoQ is implemented by the QServe inference library that achieves measured speedup. The key insight driving QServe is that the efficiency of LLM serving on GPUs is critically influenced by operations on low-throughput CUDA cores. Building upon this insight, in QoQ algorithm, we introduce progressive quantization that can allow low dequantization overhead in W4A8 GEMM. Additionally, we develop SmoothAttention to effectively mitigate the accuracy degradation incurred by 4-bit KV quantization. In the QServe system, we perform compute-aware weight reordering and take advantage of register-level parallelism to reduce dequantization latency. We also make fused attention memory-bound, harnessing the performance gain brought by KV4 quantization. As a result, QServe improves the maximum achievable serving throughput of Llama-3-8B by 1.2x on A100, 1.4x on L40S; and Qwen1.5-72B by 2.4x on A100, 3.5x on L40S, compared to TensorRT-LLM. Remarkably, QServe on L40S GPU can achieve even higher throughput than TensorRT-LLM on A100. Thus, QServe effectively reduces the dollar cost of LLM serving by 3x. Code is available at https://github.com/mit-han-lab/qserve.},
	urldate = {2024-11-20},
	publisher = {arXiv},
	author = {Lin, Yujun and Tang, Haotian and Yang, Shang and Zhang, Zhekai and Xiao, Guangxuan and Gan, Chuang and Han, Song},
	month = may,
	year = {2024},
	note = {arXiv:2405.04532 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Performance},
	file = {Preprint PDF:C\:\\Users\\h84392984\\Zotero\\storage\\WWKJE2E4\\Lin 等 - 2024 - QServe W4A8KV4 Quantization and System Co-design for Efficient LLM Serving.pdf:application/pdf;Snapshot:C\:\\Users\\h84392984\\Zotero\\storage\\7UBKAEKM\\2405.html:text/html},
}

@misc{li_svdquant_2024,
	title = {{SVDQuant}: {Absorbing} {Outliers} by {Low}-{Rank} {Components} for 4-{Bit} {Diffusion} {Models}},
	shorttitle = {{SVDQuant}},
	url = {http://arxiv.org/abs/2411.05007},
	abstract = {Diffusion models have been proven highly effective at generating high-quality images. However, as these models grow larger, they require significantly more memory and suffer from higher latency, posing substantial challenges for deployment. In this work, we aim to accelerate diffusion models by quantizing their weights and activations to 4 bits. At such an aggressive level, both weights and activations are highly sensitive, where conventional post-training quantization methods for large language models like smoothing become insufficient. To overcome this limitation, we propose SVDQuant, a new 4-bit quantization paradigm. Different from smoothing which redistributes outliers between weights and activations, our approach absorbs these outliers using a low-rank branch. We first consolidate the outliers by shifting them from activations to weights, then employ a high-precision low-rank branch to take in the weight outliers with Singular Value Decomposition (SVD). This process eases the quantization on both sides. However, na{\textbackslash}"\{{\textbackslash}i\}vely running the low-rank branch independently incurs significant overhead due to extra data movement of activations, negating the quantization speedup. To address this, we co-design an inference engine Nunchaku that fuses the kernels of the low-rank branch into those of the low-bit branch to cut off redundant memory access. It can also seamlessly support off-the-shelf low-rank adapters (LoRAs) without the need for re-quantization. Extensive experiments on SDXL, PixArt-\${\textbackslash}Sigma\$, and FLUX.1 validate the effectiveness of SVDQuant in preserving image quality. We reduce the memory usage for the 12B FLUX.1 models by 3.5\${\textbackslash}times\$, achieving 3.0\${\textbackslash}times\$ speedup over the 4-bit weight-only quantized baseline on the 16GB laptop 4090 GPU, paving the way for more interactive applications on PCs. Our quantization library and inference engine are open-sourced.},
	urldate = {2024-11-21},
	publisher = {arXiv},
	author = {Li, Muyang and Lin, Yujun and Zhang, Zhekai and Cai, Tianle and Li, Xiuyu and Guo, Junxian and Xie, Enze and Meng, Chenlin and Zhu, Jun-Yan and Han, Song},
	month = nov,
	year = {2024},
	note = {arXiv:2411.05007 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:C\:\\Users\\h84392984\\Zotero\\storage\\X6V79UTK\\Li 等 - 2024 - SVDQuant Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models.pdf:application/pdf;Snapshot:C\:\\Users\\h84392984\\Zotero\\storage\\QHRLZ79V\\2411.html:text/html},
}

@misc{qin_mooncake_2024,
	title = {Mooncake: {A} {KVCache}-centric {Disaggregated} {Architecture} for {LLM} {Serving}},
	shorttitle = {Mooncake},
	url = {http://arxiv.org/abs/2407.00079},
	abstract = {Mooncake is the serving platform for Kimi, a leading LLM service provided by Moonshot AI. It features a KVCache-centric disaggregated architecture that separates the prefill and decoding clusters. It also leverages the underutilized CPU, DRAM, and SSD resources of the GPU cluster to implement a disaggregated cache of KVCache. The core of Mooncake is its KVCache-centric scheduler, which balances maximizing overall effective throughput while meeting latency-related Service Level Objectives (SLOs). Unlike traditional studies that assume all requests will be processed, Mooncake faces challenges due to highly overloaded scenarios. To mitigate these, we developed a prediction-based early rejection policy. Experiments show that Mooncake excels in long-context scenarios. Compared to the baseline method, Mooncake can achieve up to a 525\% increase in throughput in certain simulated scenarios while adhering to SLOs. Under real workloads, Mooncake's innovative architecture enables Kimi to handle 75\% more requests.},
	urldate = {2024-11-22},
	publisher = {arXiv},
	author = {Qin, Ruoyu and Li, Zheming and He, Weiran and Zhang, Mingxing and Wu, Yongwei and Zheng, Weimin and Xu, Xinran},
	month = jul,
	year = {2024},
	note = {arXiv:2407.00079 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Hardware Architecture},
	file = {Preprint PDF:C\:\\Users\\h84392984\\Zotero\\storage\\QV8TGFUD\\Qin 等 - 2024 - Mooncake A KVCache-centric Disaggregated Architecture for LLM Serving.pdf:application/pdf;Snapshot:C\:\\Users\\h84392984\\Zotero\\storage\\IM4LJFCK\\2407.html:text/html},
}

@misc{yu_super_2024,
	title = {The {Super} {Weight} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2411.07191},
	doi = {10.48550/arXiv.2411.07191},
	abstract = {Recent works have shown a surprising result: a small fraction of Large Language Model (LLM) parameter outliers are disproportionately important to the quality of the model. LLMs contain billions of parameters, so these small fractions, such as 0.01\%, translate to hundreds of thousands of parameters. In this work, we present an even more surprising finding: Pruning as few as a single parameter can destroy an LLM's ability to generate text -- increasing perplexity by 3 orders of magnitude and reducing zero-shot accuracy to guessing. We propose a data-free method for identifying such parameters, termed super weights, using a single forward pass through the model. We additionally find that these super weights induce correspondingly rare and large activation outliers, termed super activations. When preserved with high precision, super activations can improve simple round-to-nearest quantization to become competitive with state-of-the-art methods. For weight quantization, we similarly find that by preserving the super weight and clipping other weight outliers, round-to-nearest quantization can scale to much larger block sizes than previously considered. To facilitate further research into super weights, we provide an index of super weight coordinates for common, openly available LLMs.},
	urldate = {2024-12-04},
	publisher = {arXiv},
	author = {Yu, Mengxia and Wang, De and Shan, Qi and Reed, Colorado and Wan, Alvin},
	month = nov,
	year = {2024},
	note = {arXiv:2411.07191 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:C\:\\Users\\h84392984\\Zotero\\storage\\AYPYSQVQ\\Yu 等 - 2024 - The Super Weight in Large Language Models.pdf:application/pdf;Snapshot:C\:\\Users\\h84392984\\Zotero\\storage\\4XVYJVJ5\\2411.html:text/html},
}

@misc{baalen_gptvq_2024,
	title = {{GPTVQ}: {The} {Blessing} of {Dimensionality} for {LLM} {Quantization}},
	shorttitle = {{GPTVQ}},
	url = {http://arxiv.org/abs/2402.15319},
	doi = {10.48550/arXiv.2402.15319},
	abstract = {In this work we show that the size versus accuracy trade-off of neural network quantization can be significantly improved by increasing the quantization dimensionality. We propose the GPTVQ method, a new fast method for post-training vector quantization (VQ) that scales well to Large Language Models (LLMs). Our method interleaves quantization of one or more columns with updates to the remaining unquantized weights, using information from the Hessian of the per-layer output reconstruction MSE. Quantization codebooks are initialized using an efficient data-aware version of the EM algorithm. The codebooks are then updated, and further compressed by using integer quantization and SVD-based compression. GPTVQ establishes a new state-of-the art in the size vs accuracy trade-offs on a wide range of LLMs such as Llama-v2 and Mistral. Furthermore, our method is efficient: on a single H100 it takes between 3 and 11 hours to process a Llamav2-70B model, depending on quantization setting. Lastly, with on-device timings for VQ decompression on a mobile CPU we show that VQ leads to improved latency compared to using a 4-bit integer format.},
	urldate = {2024-12-04},
	publisher = {arXiv},
	author = {Baalen, Mart van and Kuzmin, Andrey and Nagel, Markus and Couperus, Peter and Bastoul, Cedric and Mahurin, Eric and Blankevoort, Tijmen and Whatmough, Paul},
	month = feb,
	year = {2024},
	note = {arXiv:2402.15319 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\h84392984\\Zotero\\storage\\EQ2EBDTJ\\Baalen 等 - 2024 - GPTVQ The Blessing of Dimensionality for LLM Quantization.pdf:application/pdf;Snapshot:C\:\\Users\\h84392984\\Zotero\\storage\\QTRQPEZT\\2402.html:text/html},
}

@misc{guo_intlora_2024,
	title = {{IntLoRA}: {Integral} {Low}-rank {Adaptation} of {Quantized} {Diffusion} {Models}},
	shorttitle = {{IntLoRA}},
	url = {http://arxiv.org/abs/2410.21759},
	doi = {10.48550/arXiv.2410.21759},
	abstract = {Fine-tuning large-scale text-to-image diffusion models for various downstream tasks has yielded impressive results. However, the heavy computational burdens of tuning large models prevent personal customization. Recent advances have attempted to employ parameter-efficient fine-tuning (PEFT) techniques to adapt the floating-point (FP) or quantized pre-trained weights. Nonetheless, the adaptation parameters in existing works are still restricted to FP arithmetic, hindering hardware-friendly acceleration. In this work, we propose IntLoRA, to further push the efficiency limits by using integer type (INT) low-rank parameters to adapt the quantized diffusion models. By working in the integer arithmetic, our IntLoRA offers three key advantages: (i) for fine-tuning, the pre-trained weights are quantized, reducing memory usage; (ii) for storage, both pre-trained and low-rank weights are in INT which consumes less disk space; (iii) for inference, IntLoRA weights can be naturally merged into quantized pre-trained weights through efficient integer multiplication or bit-shifting, eliminating additional post-training quantization. Extensive experiments demonstrate that IntLoRA can achieve performance on par with or even superior to the vanilla LoRA, accompanied by significant efficiency improvements. Code is available at {\textbackslash}url\{https://github.com/csguoh/IntLoRA\}.},
	urldate = {2024-12-05},
	publisher = {arXiv},
	author = {Guo, Hang and Li, Yawei and Dai, Tao and Xia, Shu-Tao and Benini, Luca},
	month = oct,
	year = {2024},
	note = {arXiv:2410.21759 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:C\:\\Users\\h84392984\\Zotero\\storage\\SAHUMQB6\\Guo 等 - 2024 - IntLoRA Integral Low-rank Adaptation of Quantized Diffusion Models.pdf:application/pdf;Snapshot:C\:\\Users\\h84392984\\Zotero\\storage\\AZGVYUWQ\\2410.html:text/html},
}

@misc{wang_svd-llm_2024,
	title = {{SVD}-{LLM}: {Truncation}-aware {Singular} {Value} {Decomposition} for {Large} {Language} {Model} {Compression}},
	shorttitle = {{SVD}-{LLM}},
	url = {http://arxiv.org/abs/2403.07378},
	doi = {10.48550/arXiv.2403.07378},
	abstract = {The advancements in Large Language Models (LLMs) have been hindered by their substantial sizes, which necessitate LLM compression methods for practical deployment. Singular Value Decomposition (SVD) offers a promising solution for LLM compression. However, state-of-the-art SVD-based LLM compression methods have two key limitations: truncating smaller singular values may lead to higher compression loss, and the lack of update on the compressed weight after SVD truncation. In this work, we propose SVD-LLM, a new SVD-based LLM compression method that addresses the limitations of existing methods. SVD-LLM incorporates a truncation-aware data whitening strategy to ensure a direct mapping between singular values and compression loss. Moreover, SVD-LLM adopts a layer-wise closed-form model parameter update strategy to compensate for accuracy degradation under high compression ratios. We evaluate SVD-LLM on a total of 10 datasets and eight models from three different LLM families at four different scales. Our results demonstrate the superiority of SVD-LLM over state-of-the-arts, especially at high model compression ratios.},
	urldate = {2024-12-12},
	publisher = {arXiv},
	author = {Wang, Xin and Zheng, Yu and Wan, Zhongwei and Zhang, Mi},
	month = may,
	year = {2024},
	note = {arXiv:2403.07378 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\h84392984\\Zotero\\storage\\RMBGN4R4\\Wang 等 - 2024 - SVD-LLM Truncation-aware Singular Value Decomposition for Large Language Model Compression.pdf:application/pdf;Snapshot:C\:\\Users\\h84392984\\Zotero\\storage\\7MIY9Q3I\\2403.html:text/html},
}

@misc{noauthor_dobi-svd_nodate,
	title = {Dobi-{SVD}: {Differential} {SVD} for {LLM} {Compression} and {Some} {New} {Perspectives}},
	file = {5809_Dobi_SVD_Differential_SVD:C\:\\Users\\h84392984\\Zotero\\storage\\E7KZDY3Q\\5809_Dobi_SVD_Differential_SVD.pdf:application/pdf},
}

@misc{liu_unlocking_2024,
	title = {Unlocking {Data}-free {Low}-bit {Quantization} with {Matrix} {Decomposition} for {KV} {Cache} {Compression}},
	url = {http://arxiv.org/abs/2405.12591},
	doi = {10.48550/arXiv.2405.12591},
	abstract = {Key-value{\textasciitilde}(KV) caching is an important technique to accelerate the inference of large language models{\textasciitilde}(LLMs), but incurs significant memory overhead. To compress the size of KV cache, existing methods often compromise precision or require extra data for calibration, limiting their practicality in LLM deployment. In this paper, we introduce {\textbackslash}textbf\{DecoQuant\}, a novel data-free low-bit quantization technique based on tensor decomposition methods, to effectively compress KV cache. Our core idea is to adjust the outlier distribution of the original matrix by performing tensor decomposition, so that the quantization difficulties are migrated from the matrix to decomposed local tensors. Specially, we find that outliers mainly concentrate on small local tensors, while large tensors tend to have a narrower value range. Based on this finding, we propose to apply low-bit quantization to the large tensor, while maintaining high-precision representation for the small tensor. Furthermore, we utilize the proposed quantization method to compress the KV cache of LLMs to accelerate the inference and develop an efficient dequantization kernel tailored specifically for DecoQuant. Through extensive experiments, DecoQuant demonstrates remarkable efficiency gains, showcasing up to a \${\textbackslash}sim\$75{\textbackslash}\% reduction in memory footprint while maintaining comparable generation quality.},
	urldate = {2024-12-17},
	publisher = {arXiv},
	author = {Liu, Peiyu and Gao, Ze-Feng and Zhao, Wayne Xin and Ma, Yipeng and Wang, Tao and Wen, Ji-Rong},
	month = may,
	year = {2024},
	note = {arXiv:2405.12591 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\h84392984\\Zotero\\storage\\PK25WYIY\\Liu 等 - 2024 - Unlocking Data-free Low-bit Quantization with Matrix Decomposition for KV Cache Compression.pdf:application/pdf;Snapshot:C\:\\Users\\h84392984\\Zotero\\storage\\78R34JMS\\2405.html:text/html},
}

@article{kolda_tensor_2009,
	title = {Tensor {Decompositions} and {Applications}},
	volume = {51},
	issn = {0036-1445, 1095-7200},
	url = {https://epubs.siam.org/doi/10.1137/07070111X},
	doi = {10.1137/07070111X},
	language = {en},
	number = {3},
	urldate = {2024-12-17},
	journal = {SIAM Review},
	author = {Kolda, Tamara G. and Bader, Brett W.},
	month = aug,
	year = {2009},
	pages = {455--500},
	file = {PDF:C\:\\Users\\h84392984\\Zotero\\storage\\YMDA8RF3\\Kolda和Bader - 2009 - Tensor Decompositions and Applications.pdf:application/pdf},
}

@misc{zhao_lookahead_2024,
	title = {Lookahead: {An} {Inference} {Acceleration} {Framework} for {Large} {Language} {Model} with {Lossless} {Generation} {Accuracy}},
	shorttitle = {Lookahead},
	url = {http://arxiv.org/abs/2312.12728},
	doi = {10.48550/arXiv.2312.12728},
	abstract = {As Large Language Models (LLMs) have made significant advancements across various tasks, such as question answering, translation, text summarization, and dialogue systems, the need for accuracy in information becomes crucial, especially for serious financial products serving billions of users like Alipay. However, for a real-world product serving millions of users, the inference speed of LLMs becomes a critical factor compared to a mere experimental model. Hence, this paper presents a generic framework for accelerating the inference process, resulting in a substantial increase in speed and cost reduction for our LLM-based scenarios, with lossless generation accuracy. In the traditional inference process, each token is generated sequentially by the LLM, leading to a time consumption proportional to the number of generated tokens. To enhance this process, our framework, named {\textbackslash}textit\{lookahead\}, introduces a {\textbackslash}textit\{multi-branch\} strategy. Instead of generating a single token at a time, we propose a Trie-based retrieval and verification mechanism to be able to accept several tokens at a forward step. Our strategy offers two distinct advantages: (1) it guarantees absolute correctness of the output, avoiding any approximation algorithms, and (2) the worst-case performance of our approach is equivalent to the conventional process. We conduct extensive experiments to demonstrate the significant improvements achieved by applying our inference acceleration framework. Our framework is widely deployed in Alipay since April 2023, and obtain remarkable 2.66x to 6.26x speedup. Our code is available at https://github.com/alipay/PainlessInferenceAcceleration.},
	urldate = {2024-12-17},
	publisher = {arXiv},
	author = {Zhao, Yao and Xie, Zhitian and Liang, Chen and Zhuang, Chenyi and Gu, Jinjie},
	month = may,
	year = {2024},
	note = {arXiv:2312.12728 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Information Retrieval},
	file = {Preprint PDF:C\:\\Users\\h84392984\\Zotero\\storage\\5225YSI2\\Zhao 等 - 2024 - Lookahead An Inference Acceleration Framework for Large Language Model with Lossless Generation Acc.pdf:application/pdf;Snapshot:C\:\\Users\\h84392984\\Zotero\\storage\\65BF8EAP\\2312.html:text/html},
}

@misc{wang_bitstack_2024,
	title = {{BitStack}: {Fine}-{Grained} {Size} {Control} for {Compressed} {Large} {Language} {Models} in {Variable} {Memory} {Environments}},
	shorttitle = {{BitStack}},
	url = {http://arxiv.org/abs/2410.23918},
	doi = {10.48550/arXiv.2410.23918},
	abstract = {Large language models (LLMs) have revolutionized numerous applications, yet their deployment remains challenged by memory constraints on local devices. While scaling laws have enhanced LLM capabilities, the primary bottleneck has shifted from {\textbackslash}textit\{capability\} to {\textbackslash}textit\{availability\}, emphasizing the need for efficient memory management. Traditional compression methods, such as quantization, often require predefined compression ratios and separate compression processes for each setting, complicating deployment in variable memory environments. In this paper, we introduce {\textbackslash}textbf\{BitStack\}, a novel, training-free weight compression approach that enables megabyte-level trade-offs between memory usage and model performance. By leveraging weight decomposition, BitStack can dynamically adjust the model size with minimal transmission between running memory and storage devices. Our approach iteratively decomposes weight matrices while considering the significance of each parameter, resulting in an approximately 1-bit per parameter residual block in each decomposition iteration. These blocks are sorted and stacked in storage as basic transmission units, with different quantities loaded based on current memory availability. Extensive experiments across a wide range of tasks demonstrate that, despite offering fine-grained size control, BitStack consistently matches or surpasses strong quantization baselines, particularly at extreme compression ratios. To the best of our knowledge, this is the first decomposition-based method that effectively bridges the gap to practical compression techniques like quantization. Code is available at https://github.com/xinghaow99/BitStack.},
	urldate = {2024-12-23},
	publisher = {arXiv},
	author = {Wang, Xinghao and Wang, Pengyu and Wang, Bo and Zhang, Dong and Zhou, Yunhua and Qiu, Xipeng},
	month = oct,
	year = {2024},
	note = {arXiv:2410.23918 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:C\:\\Users\\h84392984\\Zotero\\storage\\N4MG4TV2\\Wang 等 - 2024 - BitStack Fine-Grained Size Control for Compressed Large Language Models in Variable Memory Environm.pdf:application/pdf;Snapshot:C\:\\Users\\h84392984\\Zotero\\storage\\ASQG3BJ2\\2410.html:text/html},
}

@misc{ribar_sparq_2024,
	title = {{SparQ} {Attention}: {Bandwidth}-{Efficient} {LLM} {Inference}},
	shorttitle = {{SparQ} {Attention}},
	url = {http://arxiv.org/abs/2312.04985},
	doi = {10.48550/arXiv.2312.04985},
	abstract = {The computational difficulties of large language model (LLM) inference remain a significant obstacle to their widespread deployment. The need for many applications to support long input sequences and process them in large batches typically causes token-generation to be bottlenecked by data transfer. For this reason, we introduce SparQ Attention, a technique for increasing the inference throughput of LLMs by utilising memory bandwidth more efficiently within the attention layers, through selective fetching of the cached history. Our proposed technique can be applied directly to off-the-shelf LLMs during inference, without requiring any modification to the pre-training setup or additional fine-tuning. We show that SparQ Attention brings up to 8x savings in attention data transfers without substantial drops in accuracy, by evaluating Llama 2 and 3, Mistral, Gemma and Pythia models on a wide range of downstream tasks.},
	urldate = {2025-01-02},
	publisher = {arXiv},
	author = {Ribar, Luka and Chelombiev, Ivan and Hudlass-Galley, Luke and Blake, Charlie and Luschi, Carlo and Orr, Douglas},
	month = sep,
	year = {2024},
	note = {arXiv:2312.04985 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\h84392984\\Zotero\\storage\\N8ZGK3NP\\Ribar 等 - 2024 - SparQ Attention Bandwidth-Efficient LLM Inference.pdf:application/pdf;Snapshot:C\:\\Users\\h84392984\\Zotero\\storage\\K3V3TZUD\\2312.html:text/html},
}

@misc{sun_triforce_2024,
	title = {{TriForce}: {Lossless} {Acceleration} of {Long} {Sequence} {Generation} with {Hierarchical} {Speculative} {Decoding}},
	shorttitle = {{TriForce}},
	url = {http://arxiv.org/abs/2404.11912},
	doi = {10.48550/arXiv.2404.11912},
	abstract = {With large language models (LLMs) widely deployed in long content generation recently, there has emerged an increasing demand for efficient long-sequence inference support. However, key-value (KV) cache, which is stored to avoid re-computation, has emerged as a critical bottleneck by growing linearly in size with the sequence length. Due to the auto-regressive nature of LLMs, the entire KV cache will be loaded for every generated token, resulting in low utilization of computational cores and high latency. While various compression methods for KV cache have been proposed to alleviate this issue, they suffer from degradation in generation quality. We introduce TriForce, a hierarchical speculative decoding system that is scalable for long sequence generation. This approach leverages the original model weights and dynamic sparse KV cache via retrieval as a draft model, which serves as an intermediate layer in the hierarchy and is further speculated by a smaller model to reduce its drafting latency. TriForce not only facilitates impressive speedups for Llama2-7B-128K, achieving up to 2.31\${\textbackslash}times\$ on an A100 GPU but also showcases scalability in handling even longer contexts. For the offloading setting on two RTX 4090 GPUs, TriForce achieves 0.108s/token\${\textbackslash}unicode\{x2014\}\$only half as slow as the auto-regressive baseline on an A100, which attains 7.78\${\textbackslash}times\$ on our optimized offloading system. Additionally, TriForce performs 4.86\${\textbackslash}times\$ than DeepSpeed-Zero-Inference on a single RTX 4090 GPU. TriForce's robustness is highlighted by its consistently outstanding performance across various temperatures. The code is available at https://github.com/Infini-AI-Lab/TriForce.},
	urldate = {2025-01-03},
	publisher = {arXiv},
	author = {Sun, Hanshi and Chen, Zhuoming and Yang, Xinyu and Tian, Yuandong and Chen, Beidi},
	month = aug,
	year = {2024},
	note = {arXiv:2404.11912 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\h84392984\\Zotero\\storage\\9E8N4VHG\\Sun 等 - 2024 - TriForce Lossless Acceleration of Long Sequence Generation with Hierarchical Speculative Decoding.pdf:application/pdf;Snapshot:C\:\\Users\\h84392984\\Zotero\\storage\\YZ2FWY8I\\2404.html:text/html},
}

@misc{scetbon_low-rank_2024,
	title = {Low-{Rank} {Correction} for {Quantized} {LLMs}},
	url = {http://arxiv.org/abs/2412.07902},
	doi = {10.48550/arXiv.2412.07902},
	abstract = {We consider the problem of model compression for Large Language Models (LLMs) at post-training time, where the task is to compress a well-trained model using only a small set of calibration input data. In this work, we introduce a new low-rank approach to correct for quantization errors of {\textbackslash}emph\{activations\} in LLMs: we propose to add low-rank weight matrices in full precision that act on the {\textbackslash}emph\{unquantized\} activations. We then solve a joint optimization problem over the quantized representation of the weights and additional low-rank weight matrices to quantize both weights and activations. We focus on the case of 4-bit weight-and-activation quantization (W4A4). Using ranks equivalent to 10{\textbackslash}\% of the original weight matrix size, our approach reduces the accuracy gap with the original model by more than 50{\textbackslash}\%. Using ranks equivalent to 30{\textbackslash}\% of the original weight matrix, the accuracy gap is closed completely. We demonstrate our results on four recent LLMs, namely Llama-2, Llama-3, Phi-3 and Mixtral models.},
	urldate = {2025-01-07},
	publisher = {arXiv},
	author = {Scetbon, Meyer and Hensman, James},
	month = dec,
	year = {2024},
	note = {arXiv:2412.07902 [stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\h84392984\\Zotero\\storage\\G6NTRV3M\\Scetbon和Hensman - 2024 - Low-Rank Correction for Quantized LLMs.pdf:application/pdf;Snapshot:C\:\\Users\\h84392984\\Zotero\\storage\\M7RZKDWZ\\2412.html:text/html},
}

@misc{jin_align_2024,
	title = {Align {Attention} {Heads} {Before} {Merging} {Them}: {An} {Effective} {Way} for {Converting} {MHA} to {GQA}},
	shorttitle = {Align {Attention} {Heads} {Before} {Merging} {Them}},
	url = {http://arxiv.org/abs/2412.20677},
	doi = {10.48550/arXiv.2412.20677},
	abstract = {Large language models have been shown to perform well on a variety of natural language processing problems. However, as the model size and the input sequence's length increase, the rapid increase of KV Cache significantly slows down inference speed. Therefore GQA model, as an alternative to MHA model, has been widely introduced into LLMs. In this work, we propose a low-cost method for pruning MHA models into GQA models with any compression ratio of key-value heads. Our method is based on \${\textbackslash}mathit\{L\_0\}\$ masks to gradually remove redundant parameters. In addition, we apply orthogonal transformations to attention heads without changing the model to increase similarity between attention heads before pruning training, in order to further improve performance of the model. Our method can be compatible with rotary position embedding (RoPE), which means the model after training can be fully adapted to the mainstream standard GQA framework. Experiments demonstrate that our strategy can compress up to 87.5\% of key-value heads of the LLaMA2-7B model without too much performance degradation, just achieved through supervised fine-tuning.},
	urldate = {2025-01-08},
	publisher = {arXiv},
	author = {Jin, Qingyun and Song, Xiaohui and Zhou, Feng and Qin, Zengchang},
	month = dec,
	year = {2024},
	note = {arXiv:2412.20677 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\h84392984\\Zotero\\storage\\5KTYNE4A\\Jin 等 - 2024 - Align Attention Heads Before Merging Them An Effective Way for Converting MHA to GQA.pdf:application/pdf;Snapshot:C\:\\Users\\h84392984\\Zotero\\storage\\LH7MVNJB\\2412.html:text/html},
}

@misc{chen_optimised_2024,
	title = {Optimised {Grouped}-{Query} {Attention} {Mechanism} for {Transformers}},
	url = {http://arxiv.org/abs/2406.14963},
	doi = {10.48550/arXiv.2406.14963},
	abstract = {Grouped-query attention (GQA) has been widely adopted in LLMs to mitigate the complexity of multi-head attention (MHA). To transform an MHA to a GQA, neighbour queries in MHA are evenly split into groups where each group shares the value and key layers. In this work, we propose AsymGQA, an activation-informed approach to asymmetrically grouping an MHA to a GQA for better model performance. Our AsymGQA outperforms the GQA within the same model size budget. For example, AsymGQA LLaMA-2-7B has an accuracy increase of 7.5\% on MMLU compared to neighbour grouping. Our approach addresses the GQA's trade-off problem between model performance and hardware efficiency.},
	urldate = {2025-01-10},
	publisher = {arXiv},
	author = {Chen, Yuang and Zhang, Cheng and Gao, Xitong and Mullins, Robert D. and Constantinides, George A. and Zhao, Yiren},
	month = jun,
	year = {2024},
	note = {arXiv:2406.14963 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\h84392984\\Zotero\\storage\\3THTJPV7\\Chen 等 - 2024 - Optimised Grouped-Query Attention Mechanism for Transformers.pdf:application/pdf;Snapshot:C\:\\Users\\h84392984\\Zotero\\storage\\JWISV45V\\2406.html:text/html},
}

@misc{zhang_more_2024,
	title = {More {Tokens}, {Lower} {Precision}: {Towards} the {Optimal} {Token}-{Precision} {Trade}-off in {KV} {Cache} {Compression}},
	shorttitle = {More {Tokens}, {Lower} {Precision}},
	url = {http://arxiv.org/abs/2412.12706},
	doi = {10.48550/arXiv.2412.12706},
	abstract = {As large language models (LLMs) process increasing context windows, the memory usage of KV cache has become a critical bottleneck during inference. The mainstream KV compression methods, including KV pruning and KV quantization, primarily focus on either token or precision dimension and seldom explore the efficiency of their combination. In this paper, we comprehensively investigate the token-precision trade-off in KV cache compression. Experiments demonstrate that storing more tokens in the KV cache with lower precision, i.e., quantized pruning, can significantly enhance the long-context performance of LLMs. Furthermore, in-depth analysis regarding token-precision trade-off from a series of key aspects exhibit that, quantized pruning achieves substantial improvements in retrieval-related tasks and consistently performs well across varying input lengths. Moreover, quantized pruning demonstrates notable stability across different KV pruning methods, quantization strategies, and model scales. These findings provide valuable insights into the token-precision trade-off in KV cache compression. We plan to release our code in the near future.},
	urldate = {2025-01-13},
	publisher = {arXiv},
	author = {Zhang, Jiebin and Zhu, Dawei and Song, Yifan and Wu, Wenhao and Kuang, Chuqiao and Li, Xiaoguang and Shang, Lifeng and Liu, Qun and Li, Sujian},
	month = dec,
	year = {2024},
	note = {arXiv:2412.12706 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\h84392984\\Zotero\\storage\\6YWPB8HX\\Zhang 等 - 2024 - More Tokens, Lower Precision Towards the Optimal Token-Precision Trade-off in KV Cache Compression.pdf:application/pdf;Snapshot:C\:\\Users\\h84392984\\Zotero\\storage\\AKUJF9CP\\2412.html:text/html},
}

@misc{saxena_eigen_2024,
	title = {Eigen {Attention}: {Attention} in {Low}-{Rank} {Space} for {KV} {Cache} {Compression}},
	shorttitle = {Eigen {Attention}},
	url = {http://arxiv.org/abs/2408.05646},
	doi = {10.48550/arXiv.2408.05646},
	abstract = {Large language models (LLMs) represent a groundbreaking advancement in the domain of natural language processing due to their impressive reasoning abilities. Recently, there has been considerable interest in increasing the context lengths for these models to enhance their applicability to complex tasks. However, at long context lengths and large batch sizes, the key-value (KV) cache, which stores the attention keys and values, emerges as the new bottleneck in memory usage during inference. To address this, we propose Eigen Attention, which performs the attention operation in a low-rank space, thereby reducing the KV cache memory overhead. Our proposed approach is orthogonal to existing KV cache compression techniques and can be used synergistically with them. Through extensive experiments over OPT, MPT, and Llama model families, we demonstrate that Eigen Attention results in up to 40\% reduction in KV cache sizes and up to 60\% reduction in attention operation latency with minimal drop in performance. Code is available at https://github.com/UtkarshSaxena1/EigenAttn.},
	urldate = {2025-01-16},
	publisher = {arXiv},
	author = {Saxena, Utkarsh and Saha, Gobinda and Choudhary, Sakshi and Roy, Kaushik},
	month = nov,
	year = {2024},
	note = {arXiv:2408.05646 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:C\:\\Users\\h84392984\\Zotero\\storage\\22ZREHAS\\Saxena 等 - 2024 - Eigen Attention Attention in Low-Rank Space for KV Cache Compression.pdf:application/pdf;Snapshot:C\:\\Users\\h84392984\\Zotero\\storage\\ACTWUMJI\\2408.html:text/html},
}

@misc{ping_delta-come_2024,
	title = {Delta-{CoMe}: {Training}-{Free} {Delta}-{Compression} with {Mixed}-{Precision} for {Large} {Language} {Models}},
	shorttitle = {Delta-{CoMe}},
	url = {http://arxiv.org/abs/2406.08903},
	doi = {10.48550/arXiv.2406.08903},
	abstract = {Fine-tuning is a crucial process for adapting large language models (LLMs) to diverse applications. In certain scenarios, such as multi-tenant serving, deploying multiple LLMs becomes necessary to meet complex demands. Recent studies suggest decomposing a fine-tuned LLM into a base model and corresponding delta weights, which are then compressed using low-rank or low-bit approaches to reduce costs. In this work, we observe that existing low-rank and low-bit compression methods can significantly harm the model performance for task-specific fine-tuned LLMs (e.g., WizardMath for math problems). Motivated by the long-tail distribution of singular values in the delta weights, we propose a delta quantization approach using mixed-precision. This method employs higher-bit representation for singular vectors corresponding to larger singular values. We evaluate our approach on various fine-tuned LLMs, including math LLMs, code LLMs, chat LLMs, and even VLMs. Experimental results demonstrate that our approach performs comparably to full fine-tuned LLMs, surpassing both low-rank and low-bit baselines by a considerable margin. Additionally, we show that our method is compatible with various backbone LLMs, such as Llama-2, Llama-3, and Mistral, highlighting its generalizability.},
	urldate = {2025-01-16},
	publisher = {arXiv},
	author = {Ping, Bowen and Wang, Shuo and Wang, Hanqing and Han, Xu and Xu, Yuzhuang and Yan, Yukun and Chen, Yun and Chang, Baobao and Liu, Zhiyuan and Sun, Maosong},
	month = nov,
	year = {2024},
	note = {arXiv:2406.08903 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\h84392984\\Zotero\\storage\\AQGKSIT3\\Ping 等 - 2024 - Delta-CoMe Training-Free Delta-Compression with Mixed-Precision for Large Language Models.pdf:application/pdf;Snapshot:C\:\\Users\\h84392984\\Zotero\\storage\\VPFPQGAM\\2406.html:text/html},
}

@misc{tan_alignedkv_2024,
	title = {{AlignedKV}: {Reducing} {Memory} {Access} of {KV}-{Cache} with {Precision}-{Aligned} {Quantization}},
	shorttitle = {{AlignedKV}},
	url = {http://arxiv.org/abs/2409.16546},
	doi = {10.48550/arXiv.2409.16546},
	abstract = {Model quantization has become a crucial technique to address the issues of large memory consumption and long inference times associated with LLMs. Mixed-precision quantization, which distinguishes between important and unimportant parameters, stands out among numerous quantization schemes as it achieves a balance between precision and compression rate. However, existing approaches can only identify important parameters through qualitative analysis and manual experiments without quantitatively analyzing how their importance is determined. We propose a new criterion, so-called 'precision alignment', to build a quantitative framework to holistically evaluate the importance of parameters in mixed-precision quantization. Our observations on floating point addition under various real-world scenarios suggest that two addends should have identical precision, otherwise the information in the higher-precision number will be wasted. Such an observation offers an essential principle to determine the precision of each parameter in matrix multiplication operation. As the first step towards applying the above discovery to large model inference, we develop a dynamic KV-Cache quantization technique to effectively reduce memory access latency. Different from existing quantization approaches that focus on memory saving, this work directly aims to accelerate LLM inference through quantifying floating numbers. The proposed technique attains a 25\% saving of memory access and delivers up to 1.3x speedup in the computation of attention in the decoding phase of LLM, with almost no loss of precision.},
	urldate = {2025-01-23},
	publisher = {arXiv},
	author = {Tan, Yifan and Wang, Haoze and Yan, Chao and Deng, Yangdong},
	month = oct,
	year = {2024},
	note = {arXiv:2409.16546 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\h84392984\\Zotero\\storage\\3LVGUS6W\\Tan 等 - 2024 - AlignedKV Reducing Memory Access of KV-Cache with Precision-Aligned Quantization.pdf:application/pdf;Snapshot:C\:\\Users\\h84392984\\Zotero\\storage\\VN9P32DL\\2409.html:text/html},
}

@article{mirsky_symmetric_1960,
	title = {{SYMMETRIC} {GAUGE} {FUNCTIONS} {AND} {UNITARILY} {INVARIANT} {NORMS}},
	volume = {11},
	issn = {0033-5606, 1464-3847},
	url = {https://academic.oup.com/qjmath/article-lookup/doi/10.1093/qmath/11.1.50},
	doi = {10.1093/qmath/11.1.50},
	language = {en},
	number = {1},
	urldate = {2025-02-03},
	journal = {The Quarterly Journal of Mathematics},
	author = {Mirsky, L.},
	year = {1960},
	pages = {50--59},
}

@misc{grattafiori_llama_2024,
	title = {The {Llama} 3 {Herd} of {Models}},
	url = {http://arxiv.org/abs/2407.21783},
	doi = {10.48550/arXiv.2407.21783},
	abstract = {Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.},
	urldate = {2025-02-03},
	publisher = {arXiv},
	author = {Grattafiori, Aaron and Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Vaughan, Alex and others},
	month = nov,
	year = {2024},
	note = {arXiv:2407.21783 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:C\:\\Users\\h84392984\\Zotero\\storage\\2HG989QJ\\Grattafiori 等 - 2024 - The Llama 3 Herd of Models.pdf:application/pdf;Snapshot:C\:\\Users\\h84392984\\Zotero\\storage\\UPV8ZILH\\2407.html:text/html},
}

@misc{qwen_qwen25_2025,
	title = {Qwen2.5 {Technical} {Report}},
	url = {http://arxiv.org/abs/2412.15115},
	doi = {10.48550/arXiv.2412.15115},
	abstract = {In this report, we introduce Qwen2.5, a comprehensive series of large language models (LLMs) designed to meet diverse needs. Compared to previous iterations, Qwen 2.5 has been significantly improved during both the pre-training and post-training stages. In terms of pre-training, we have scaled the high-quality pre-training datasets from the previous 7 trillion tokens to 18 trillion tokens. This provides a strong foundation for common sense, expert knowledge, and reasoning capabilities. In terms of post-training, we implement intricate supervised finetuning with over 1 million samples, as well as multistage reinforcement learning. Post-training techniques enhance human preference, and notably improve long text generation, structural data analysis, and instruction following. To handle diverse and varied use cases effectively, we present Qwen2.5 LLM series in rich sizes. Open-weight offerings include base and instruction-tuned models, with quantized versions available. In addition, for hosted solutions, the proprietary models currently include two mixture-of-experts (MoE) variants: Qwen2.5-Turbo and Qwen2.5-Plus, both available from Alibaba Cloud Model Studio. Qwen2.5 has demonstrated top-tier performance on a wide range of benchmarks evaluating language understanding, reasoning, mathematics, coding, human preference alignment, etc. Specifically, the open-weight flagship Qwen2.5-72B-Instruct outperforms a number of open and proprietary models and demonstrates competitive performance to the state-of-the-art open-weight model, Llama-3-405B-Instruct, which is around 5 times larger. Qwen2.5-Turbo and Qwen2.5-Plus offer superior cost-effectiveness while performing competitively against GPT-4o-mini and GPT-4o respectively. Additionally, as the foundation, Qwen2.5 models have been instrumental in training specialized models such as Qwen2.5-Math, Qwen2.5-Coder, QwQ, and multimodal models.},
	urldate = {2025-02-03},
	publisher = {arXiv},
	author = {Qwen and Yang, An and Yang, Baosong and Zhang, Beichen and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and Wei, Haoran and Lin, Huan and Yang, Jian and Tu, Jianhong and Zhang, Jianwei and Yang, Jianxin and Yang, Jiaxi and Zhou, Jingren and Lin, Junyang and Dang, Kai and Lu, Keming and Bao, Keqin and Yang, Kexin and Yu, Le and Li, Mei and Xue, Mingfeng and Zhang, Pei and Zhu, Qin and Men, Rui and Lin, Runji and Li, Tianhao and Tang, Tianyi and Xia, Tingyu and Ren, Xingzhang and Ren, Xuancheng and Fan, Yang and Su, Yang and Zhang, Yichang and Wan, Yu and Liu, Yuqiong and Cui, Zeyu and Zhang, Zhenru and Qiu, Zihan},
	month = jan,
	year = {2025},
	note = {arXiv:2412.15115 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\h84392984\\Zotero\\storage\\DWCHBMTY\\Qwen 等 - 2025 - Qwen2.5 Technical Report.pdf:application/pdf;Snapshot:C\:\\Users\\h84392984\\Zotero\\storage\\GSA64ZNS\\2412.html:text/html},
}

@misc{hsieh_ruler_2024,
	title = {{RULER}: {What}'s the {Real} {Context} {Size} of {Your} {Long}-{Context} {Language} {Models}?},
	shorttitle = {{RULER}},
	url = {http://arxiv.org/abs/2404.06654},
	doi = {10.48550/arXiv.2404.06654},
	abstract = {The needle-in-a-haystack (NIAH) test, which examines the ability to retrieve a piece of information (the "needle") from long distractor texts (the "haystack"), has been widely adopted to evaluate long-context language models (LMs). However, this simple retrieval-based test is indicative of only a superficial form of long-context understanding. To provide a more comprehensive evaluation of long-context LMs, we create a new synthetic benchmark RULER with flexible configurations for customized sequence length and task complexity. RULER expands upon the vanilla NIAH test to encompass variations with diverse types and quantities of needles. Moreover, RULER introduces new task categories multi-hop tracing and aggregation to test behaviors beyond searching from context. We evaluate 17 long-context LMs with 13 representative tasks in RULER. Despite achieving nearly perfect accuracy in the vanilla NIAH test, almost all models exhibit large performance drops as the context length increases. While these models all claim context sizes of 32K tokens or greater, only half of them can maintain satisfactory performance at the length of 32K. Our analysis of Yi-34B, which supports context length of 200K, reveals large room for improvement as we increase input length and task complexity. We open source RULER to spur comprehensive evaluation of long-context LMs.},
	urldate = {2025-02-03},
	publisher = {arXiv},
	author = {Hsieh, Cheng-Ping and Sun, Simeng and Kriman, Samuel and Acharya, Shantanu and Rekesh, Dima and Jia, Fei and Zhang, Yang and Ginsburg, Boris},
	month = aug,
	year = {2024},
	note = {arXiv:2404.06654 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\h84392984\\Zotero\\storage\\3DH5JMPV\\Hsieh 等 - 2024 - RULER What's the Real Context Size of Your Long-Context Language Models.pdf:application/pdf;Snapshot:C\:\\Users\\h84392984\\Zotero\\storage\\TMSKHKQ3\\2404.html:text/html},
}

@misc{openai_gpt-4_2024,
	title = {{GPT}-4 {Technical} {Report}},
	url = {http://arxiv.org/abs/2303.08774},
	doi = {10.48550/arXiv.2303.08774},
	abstract = {We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10\% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.},
	urldate = {2025-02-03},
	publisher = {arXiv},
	author = {OpenAI and Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and others},
	month = mar,
	year = {2024},
	note = {arXiv:2303.08774 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\h84392984\\Zotero\\storage\\GXIBX58Q\\OpenAI 等 - 2024 - GPT-4 Technical Report.pdf:application/pdf;Snapshot:C\:\\Users\\h84392984\\Zotero\\storage\\46NJ92L7\\2303.html:text/html},
}

@misc{deepseek-ai_deepseek-r1_2025,
	title = {{DeepSeek}-{R1}: {Incentivizing} {Reasoning} {Capability} in {LLMs} via {Reinforcement} {Learning}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {{DeepSeek}-{R1}},
	url = {https://arxiv.org/abs/2501.12948},
	doi = {10.48550/ARXIV.2501.12948},
	abstract = {We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.},
	urldate = {2025-02-03},
	publisher = {arXiv},
	author = {{DeepSeek-AI} and Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and others},
	year = {2025},
	note = {Version Number: 1},
	keywords = {Artificial Intelligence (cs.AI), Computation and Language (cs.CL), FOS: Computer and information sciences, Machine Learning (cs.LG)},
	file = {Preprint PDF:C\:\\Users\\h84392984\\Zotero\\storage\\LLLGQWR6\\DeepSeek-AI 等 - 2025 - DeepSeek-R1 Incentivizing Reasoning Capability in LLMs via Reinforcement Learning.pdf:application/pdf},
}

@misc{vaswani_attention_2023,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	doi = {10.48550/arXiv.1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	urldate = {2025-02-03},
	publisher = {arXiv},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = aug,
	year = {2023},
	note = {arXiv:1706.03762 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\h84392984\\Zotero\\storage\\RNLD7QVA\\Vaswani 等 - 2023 - Attention Is All You Need.pdf:application/pdf;Snapshot:C\:\\Users\\h84392984\\Zotero\\storage\\ITJ9IEGI\\1706.html:text/html},
}

@misc{pope_efficiently_2022,
	title = {Efficiently {Scaling} {Transformer} {Inference}},
	url = {http://arxiv.org/abs/2211.05102},
	doi = {10.48550/arXiv.2211.05102},
	abstract = {We study the problem of efficient generative inference for Transformer models, in one of its most challenging settings: large deep models, with tight latency targets and long sequence lengths. Better understanding of the engineering tradeoffs for inference for large Transformer-based models is important as use cases of these models are growing rapidly throughout application areas. We develop a simple analytical model for inference efficiency to select the best multi-dimensional partitioning techniques optimized for TPU v4 slices based on the application requirements. We combine these with a suite of low-level optimizations to achieve a new Pareto frontier on the latency and model FLOPS utilization (MFU) tradeoffs on 500B+ parameter models that outperforms the FasterTransformer suite of benchmarks. We further show that with appropriate partitioning, the lower memory requirements of multiquery attention (i.e. multiple query heads share single key/value head) enables scaling up to 32x larger context lengths. Finally, we achieve a low-batch-size latency of 29ms per token during generation (using int8 weight quantization) and a 76\% MFU during large-batch-size processing of input tokens, while supporting a long 2048-token context length on the PaLM 540B parameter model.},
	urldate = {2025-02-03},
	publisher = {arXiv},
	author = {Pope, Reiner and Douglas, Sholto and Chowdhery, Aakanksha and Devlin, Jacob and Bradbury, James and Levskaya, Anselm and Heek, Jonathan and Xiao, Kefan and Agrawal, Shivani and Dean, Jeff},
	month = nov,
	year = {2022},
	note = {arXiv:2211.05102 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\h84392984\\Zotero\\storage\\PUI8NZ4I\\Pope 等 - 2022 - Efficiently Scaling Transformer Inference.pdf:application/pdf;Snapshot:C\:\\Users\\h84392984\\Zotero\\storage\\L2I6I4J6\\2211.html:text/html},
}

@misc{zhang_lorc_2024,
	title = {{LoRC}: {Low}-{Rank} {Compression} for {LLMs} {KV} {Cache} with a {Progressive} {Compression} {Strategy}},
	shorttitle = {{LoRC}},
	url = {http://arxiv.org/abs/2410.03111},
	doi = {10.48550/arXiv.2410.03111},
	abstract = {The Key-Value (KV) cache is a crucial component in serving transformer-based autoregressive large language models (LLMs), enabling faster inference by storing previously computed KV vectors. However, its memory consumption scales linearly with sequence length and batch size, posing a significant bottleneck in LLM deployment. Existing approaches to mitigate this issue include: (1) efficient attention variants integrated in upcycling stages, which requires extensive parameter tuning thus unsuitable for pre-trained LLMs; (2) KV cache compression at test time, primarily through token eviction policies, which often overlook inter-layer dependencies and can be task-specific. This paper introduces an orthogonal approach to KV cache compression. We propose a low-rank approximation of KV weight matrices, allowing for plug-in integration with existing transformer-based LLMs without model retraining. To effectively compress KV cache at the weight level, we adjust for layerwise sensitivity and introduce a progressive compression strategy, which is supported by our theoretical analysis on how compression errors accumulate in deep networks. Our method is designed to function without model tuning in upcycling stages or task-specific profiling in test stages. Extensive experiments with LLaMA models ranging from 8B to 70B parameters across various tasks show that our approach significantly reduces the GPU memory footprint while maintaining performance.},
	urldate = {2025-02-03},
	publisher = {arXiv},
	author = {Zhang, Rongzhi and Wang, Kuang and Liu, Liyuan and Wang, Shuohang and Cheng, Hao and Zhang, Chao and Shen, Yelong},
	month = oct,
	year = {2024},
	note = {arXiv:2410.03111 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\h84392984\\Zotero\\storage\\PD7AIG6S\\Zhang 等 - 2024 - LoRC Low-Rank Compression for LLMs KV Cache with a Progressive Compression Strategy.pdf:application/pdf;Snapshot:C\:\\Users\\h84392984\\Zotero\\storage\\GJ6GUD26\\2410.html:text/html},
}

@misc{ainslie_gqa_2023,
	title = {{GQA}: {Training} {Generalized} {Multi}-{Query} {Transformer} {Models} from {Multi}-{Head} {Checkpoints}},
	shorttitle = {{GQA}},
	url = {http://arxiv.org/abs/2305.13245},
	doi = {10.48550/arXiv.2305.13245},
	abstract = {Multi-query attention (MQA), which only uses a single key-value head, drastically speeds up decoder inference. However, MQA can lead to quality degradation, and moreover it may not be desirable to train a separate model just for faster inference. We (1) propose a recipe for uptraining existing multi-head language model checkpoints into models with MQA using 5\% of original pre-training compute, and (2) introduce grouped-query attention (GQA), a generalization of multi-query attention which uses an intermediate (more than one, less than number of query heads) number of key-value heads. We show that uptrained GQA achieves quality close to multi-head attention with comparable speed to MQA.},
	urldate = {2025-02-04},
	publisher = {arXiv},
	author = {Ainslie, Joshua and Lee-Thorp, James and Jong, Michiel de and Zemlyanskiy, Yury and Lebrón, Federico and Sanghai, Sumit},
	month = dec,
	year = {2023},
	note = {arXiv:2305.13245 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\h84392984\\Zotero\\storage\\M65D38CQ\\Ainslie 等 - 2023 - GQA Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints.pdf:application/pdf;Snapshot:C\:\\Users\\h84392984\\Zotero\\storage\\6VSRF8FF\\2305.html:text/html},
}

@misc{singhania_loki_2024,
	title = {Loki: {Low}-rank {Keys} for {Efficient} {Sparse} {Attention}},
	shorttitle = {Loki},
	url = {http://arxiv.org/abs/2406.02542},
	doi = {10.48550/arXiv.2406.02542},
	abstract = {Inference on large language models (LLMs) can be expensive in terms of the compute and memory costs involved, especially when long sequence lengths are used. In particular, the self-attention mechanism used in LLM inference contributes significantly to these costs, which has sparked an interest in approximating the self-attention computation to reduce such costs. In this work, we propose to approximate self-attention by focusing on the dimensionality of key vectors computed in the attention block. Our analysis reveals that key vectors lie in a significantly lower-dimensional space, consistently across several datasets and models. Exploiting this observation, we propose Loki, a novel sparse attention method that ranks and selects tokens in the KV-cache based on attention scores computed in low-dimensional space. Our evaluations show that Loki is able to speed up the attention computation due to reduced data movement (load/store) and compute costs while maintaining the efficacy of the models better than other popular approximation methods.},
	urldate = {2025-02-05},
	publisher = {arXiv},
	author = {Singhania, Prajwal and Singh, Siddharth and He, Shwai and Feizi, Soheil and Bhatele, Abhinav},
	month = nov,
	year = {2024},
	note = {arXiv:2406.02542 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\h84392984\\Zotero\\storage\\2T5VQ3ZZ\\Singhania 等 - 2024 - Loki Low-rank Keys for Efficient Sparse Attention.pdf:application/pdf;Snapshot:C\:\\Users\\h84392984\\Zotero\\storage\\YSNTGGGV\\2406.html:text/html},
}


@article{yuan2024kvcompression,
  title={Kv cache compression, but what must we give in return? a comprehensive benchmark of long context capable approaches},
  author={Yuan, Jiayi and Liu, Hongyi and Chuang, Yu-Neng and Li, Songchen and Wang, Guanchu and Le, Duy and Jin, Hongye and Chaudhary, Vipin and Xu, Zhaozhuo and Liu, Zirui and others},
  journal={arXiv preprint arXiv:2407.01527},
  year={2024}
}

@inproceedings{liu2024cachegen,
  title={CacheGen: KV Cache Compression and Streaming for Fast Large Language Model Serving},
  author={Liu, Yuhan and Li, Hanchen and Cheng, Yihua and Ray, Siddhant and Huang, Yuyang and Zhang, Qizheng and Du, Kuntai and Yao, Jiayi and Lu, Shan and Ananthanarayanan, Ganesh and others},
  booktitle={Proceedings of the ACM SIGCOMM 2024 Conference},
  pages={38--56},
  year={2024}
}

@article{liu2024kivi,
  title={Kivi: A tuning-free asymmetric 2bit quantization for kv cache},
  author={Liu, Zirui and Yuan, Jiayi and Jin, Hongye and Zhong, Shaochen and Xu, Zhaozhuo and Braverman, Vladimir and Chen, Beidi and Hu, Xia},
  journal={arXiv preprint arXiv:2402.02750},
  year={2024}
}

@article{hooper2024kvquant,
  title={Kvquant: Towards 10 million context length llm inference with kv cache quantization},
  author={Hooper, Coleman and Kim, Sehoon and Mohammadzadeh, Hiva and Mahoney, Michael W and Shao, Yakun Sophia and Keutzer, Kurt and Gholami, Amir},
  journal={arXiv preprint arXiv:2401.18079},
  year={2024}
}

@article{dong2024qaq,
  title={QAQ: Quality Adaptive Quantization for LLM KV Cache},
  author={Dong, Shichen and Cheng, Wen and Qin, Jiayu and Wang, Wei},
  journal={arXiv preprint arXiv:2403.04643},
  year={2024}
}

@article{duanmu2024skvq,
  title={SKVQ: Sliding-window Key and Value Cache Quantization for Large Language Models},
  author={Duanmu, Haojie and Yuan, Zhihang and Li, Xiuhong and Duan, Jiangfei and Zhang, Xingcheng and Lin, Dahua},
  journal={arXiv preprint arXiv:2405.06219},
  year={2024}
}

@article{liu2024intactkv,
  title={IntactKV: Improving Large Language Model Quantization by Keeping Pivot Tokens Intact},
  author={Liu, Ruikang and Bai, Haoli and Lin, Haokun and Li, Yuening and Gao, Han and Xu, Zhengzhuo and Hou, Lu and Yao, Jun and Yuan, Chun},
  journal={arXiv preprint arXiv:2403.01241},
  year={2024}
}

@article{yang2024mikv,
  title={No token left behind: Reliable kv cache compression via importance-aware mixed precision quantization},
  author={Yang, June Yong and Kim, Byeongwook and Bae, Jeongin and Kwon, Beomseok and Park, Gunho and Yang, Eunho and Kwon, Se Jung and Lee, Dongsoo},
  journal={arXiv preprint arXiv:2402.18096},
  year={2024}
}

@article{he2024zipvl,
  title={ZipVL: Efficient Large Vision-Language Models with Dynamic Token Sparsification and KV Cache Compression},
  author={He, Yefei and Chen, Feng and Liu, Jing and Shao, Wenqi and Zhou, Hong and Zhang, Kaipeng and Zhuang, Bohan},
  journal={arXiv preprint arXiv:2410.08584},
  year={2024}
}

@article{he2024zipcache,
  title={ZipCache: Accurate and Efficient KV Cache Quantization with Salient Token Identification},
  author={He, Yefei and Zhang, Luoming and Wu, Weijia and Liu, Jing and Zhou, Hong and Zhuang, Bohan},
  journal={arXiv preprint arXiv:2405.14256},
  year={2024}
}

@article{lee2024fliptoken,
  title={Improving Conversational Abilities of Quantized Large Language Models via Direct Preference Alignment},
  author={Lee, Janghwan and Park, Seongmin and Hong, Sukjin and Kim, Minsoo and Chang, Du-Seong and Choi, Jungwook},
  journal={arXiv preprint arXiv:2407.03051},
  year={2024}
}

@article{qin2407mooncake,
  title={Mooncake: A kvcache-centric disaggregated architecture for llm serving},
  author={Qin, Ruoyu and Li, Zheming and He, Weiran and Zhang, Mingxing and Wu, Yongwei and Zheng, Weimin and Xu, Xinran},
  journal={URL https://arxiv. org/abs/2407.00079},
  year={2024}

}

@article{zhang2024h2o,
  title={H2o: Heavy-hitter oracle for efficient generative inference of large language models},
  author={Zhang, Zhenyu and Sheng, Ying and Zhou, Tianyi and Chen, Tianlong and Zheng, Lianmin and Cai, Ruisi and Song, Zhao and Tian, Yuandong and R{\'e}, Christopher and Barrett, Clark and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{ge2023fastgen,
  title={Model tells you what to discard: Adaptive kv cache compression for llms},
  author={Ge, Suyu and Zhang, Yunan and Liu, Liyuan and Zhang, Minjia and Han, Jiawei and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2310.01801},
  year={2023}
}

@article{liu2024scissorhands,
  title={Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time},
  author={Liu, Zichang and Desai, Aditya and Liao, Fangshuo and Wang, Weitao and Xie, Victor and Xu, Zhaozhuo and Kyrillidis, Anastasios and Shrivastava, Anshumali},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{lee2024infinigen,
  title={$\{$InfiniGen$\}$: Efficient generative inference of large language models with dynamic $\{$KV$\}$ cache management},
  author={Lee, Wonbeom and Lee, Jungi and Seo, Junghwan and Sim, Jaewoong},
  booktitle={18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24)},
  pages={155--172},
  year={2024}
}

@article{zhang2024pqcache,
  title={Pqcache: Product quantization-based kvcache for long context llm inference},
  author={Zhang, Hailin and Ji, Xiaodong and Chen, Yilin and Fu, Fangcheng and Miao, Xupeng and Nie, Xiaonan and Chen, Weipeng and Cui, Bin},
  journal={arXiv preprint arXiv:2407.12820},
  year={2024}
}

@article{tang2024quest,
  title={Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference},
  author={Tang, Jiaming and Zhao, Yilong and Zhu, Kan and Xiao, Guangxuan and Kasikci, Baris and Han, Song},
  journal={arXiv preprint arXiv:2406.10774},
  year={2024}
}

@inproceedings{kwon2023vllm,
  title={Efficient memory management for large language model serving with pagedattention},
  author={Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph and Zhang, Hao and Stoica, Ion},
  booktitle={Proceedings of the 29th Symposium on Operating Systems Principles},
  pages={611--626},
  year={2023}
}

@article{tao2024asymkv,
  title={AsymKV: Enabling 1-Bit Quantization of KV Cache with Layer-Wise Asymmetric Quantization Configurations},
  author={Tao, Qian and Yu, Wenyuan and Zhou, Jingren},
  journal={arXiv preprint arXiv:2410.13212},
  year={2024}
}

@article{allen2024physicsllm,
  title={Physics of language models: Part 3.3, knowledge capacity scaling laws},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi},
  journal={arXiv preprint arXiv:2404.05405},
  year={2024}
}

@article{adnan2024keyformer,
  title={Keyformer: Kv cache reduction through key tokens selection for efficient generative inference},
  author={Adnan, Muhammad and Arunkumar, Akhil and Jain, Gaurav and Nair, Prashant and Soloveychik, Ilya and Kamath, Purushotham},
  journal={Proceedings of Machine Learning and Systems},
  volume={6},
  pages={114--127},
  year={2024}
}

@article{li2024snapkv,
  title={Snapkv: Llm knows what you are looking for before generation},
  author={Li, Yuhong and Huang, Yingbing and Yang, Bowen and Venkitesh, Bharat and Locatelli, Acyr and Ye, Hanchen and Cai, Tianle and Lewis, Patrick and Chen, Deming},
  journal={arXiv preprint arXiv:2404.14469},
  year={2024}
}

@article{sun2024shadowkv,
  title={Shadowkv: Kv cache in shadows for high-throughput long-context llm inference},
  author={Sun, Hanshi and Chang, Li-Wen and Bao, Wenlei and Zheng, Size and Zheng, Ningxin and Liu, Xin and Dong, Harry and Chi, Yuejie and Chen, Beidi},
  journal={arXiv preprint arXiv:2410.21465},
  year={2024}
}

@article{wang2024model,
  title={Model tells you where to merge: Adaptive kv cache merging for llms on long-context tasks},
  author={Wang, Zheng and Jin, Boxiao and Yu, Zhongzhi and Zhang, Minjia},
  journal={arXiv preprint arXiv:2407.08454},
  year={2024}
}

@article{wan2024look-m,
  title={Look-m: Look-once optimization in kv cache for efficient multimodal long-context inference},
  author={Wan, Zhongwei and Wu, Ziang and Liu, Che and Huang, Jinfa and Zhu, Zhihong and Jin, Peng and Wang, Longyue and Yuan, Li},
  journal={arXiv preprint arXiv:2406.18139},
  year={2024}
}

@article{liu2024minicache,
  title={MiniCache: KV Cache Compression in Depth Dimension for Large Language Models},
  author={Liu, Akide and Liu, Jing and Pan, Zizheng and He, Yefei and Haffari, Gholamreza and Zhuang, Bohan},
  journal={arXiv preprint arXiv:2405.14366},
  year={2024}
}

@article{frantar2022gptq,
  title={Gptq: Accurate post-training quantization for generative pre-trained transformers},
  author={Frantar, Elias and Ashkboos, Saleh and Hoefler, Torsten and Alistarh, Dan},
  journal={arXiv preprint arXiv:2210.17323},
  year={2022}
}

@article{lin2024awq,
  title={AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration},
  author={Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang and Chen, Wei-Ming and Wang, Wei-Chen and Xiao, Guangxuan and Dang, Xingyu and Gan, Chuang and Han, Song},
  journal={Proceedings of Machine Learning and Systems},
  volume={6},
  pages={87--100},
  year={2024}
}
@inproceedings{xiao2023smoothquant,
  title={Smoothquant: Accurate and efficient post-training quantization for large language models},
  author={Xiao, Guangxuan and Lin, Ji and Seznec, Mickael and Wu, Hao and Demouth, Julien and Han, Song},
  booktitle={International Conference on Machine Learning},
  pages={38087--38099},
  year={2023},
  organization={PMLR}
}

@article{liu2024spinquant,
  title={SpinQuant--LLM quantization with learned rotations},
  author={Liu, Zechun and Zhao, Changsheng and Fedorov, Igor and Soran, Bilge and Choudhary, Dhruv and Krishnamoorthi, Raghuraman and Chandra, Vikas and Tian, Yuandong and Blankevoort, Tijmen},
  journal={arXiv preprint arXiv:2405.16406},
  year={2024}
}

@article{li2024eagle,
  title={Eagle: Speculative sampling requires rethinking feature uncertainty},
  author={Li, Yuhui and Wei, Fangyun and Zhang, Chao and Zhang, Hongyang},
  journal={arXiv preprint arXiv:2401.15077},
  year={2024}
}

@article{cai2024medusa,
  title={Medusa: Simple llm inference acceleration framework with multiple decoding heads},
  author={Cai, Tianle and Li, Yuhong and Geng, Zhengyang and Peng, Hongwu and Lee, Jason D and Chen, Deming and Dao, Tri},
  journal={arXiv preprint arXiv:2401.10774},
  year={2024}
}

@article{sun2024triforce,
  title={Triforce: Lossless acceleration of long sequence generation with hierarchical speculative decoding},
  author={Sun, Hanshi and Chen, Zhuoming and Yang, Xinyu and Tian, Yuandong and Chen, Beidi},
  journal={arXiv preprint arXiv:2404.11912},
  year={2024}
}

@inproceedings{zhao2024lookahead,
  title={Lookahead: An inference acceleration framework for large language model with lossless generation accuracy},
  author={Zhao, Yao and Xie, Zhitian and Liang, Chen and Zhuang, Chenyi and Gu, Jinjie},
  booktitle={Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  pages={6344--6355},
  year={2024}
}

@article{zhang2024qhitter,
  title={Q-Hitter: A Better Token Oracle for Efficient LLM Inference via Sparse-Quantized KV Cache},
  author={Zhang, Zhenyu and Liu, Shiwei and Chen, Runjin and Kailkhura, Bhavya and Chen, Beidi and Wang, Atlas},
  journal={Proceedings of Machine Learning and Systems},
  volume={6},
  pages={381--394},
  year={2024}
}

@article{ma2023llmpruner,
  title={Llm-pruner: On the structural pruning of large language models},
  author={Ma, Xinyin and Fang, Gongfan and Wang, Xinchao},
  journal={Advances in neural information processing systems},
  volume={36},
  pages={21702--21720},
  year={2023}
}

@article{zeng2023learning,
  title={Learning to skip for language modeling},
  author={Zeng, Dewen and Du, Nan and Wang, Tao and Xu, Yuanzhong and Lei, Tao and Chen, Zhifeng and Cui, Claire},
  journal={arXiv preprint arXiv:2311.15436},
  year={2023}
}

@article{elhoushi2024layerskip,
  title={Layer skip: Enabling early exit inference and self-speculative decoding},
  author={Elhoushi, Mostafa and Shrivastava, Akshat and Liskovich, Diana and Hosmer, Basil and Wasti, Bram and Lai, Liangzhen and Mahmoud, Anas and Acun, Bilge and Agarwal, Saurabh and Roman, Ahmed and others},
  journal={arXiv preprint arXiv:2404.16710},
  year={2024}
}

@article{sprague2023musr,
  title={Musr: Testing the limits of chain-of-thought with multistep soft reasoning},
  author={Sprague, Zayne and Ye, Xi and Bostrom, Kaj and Chaudhuri, Swarat and Durrett, Greg},
  journal={arXiv preprint arXiv:2310.16049},
  year={2023}
}

@article{rein2023gpqa,
  title={Gpqa: A graduate-level google-proof q\&a benchmark},
  author={Rein, David and Hou, Betty Li and Stickland, Asa Cooper and Petty, Jackson and Pang, Richard Yuanzhe and Dirani, Julien and Michael, Julian and Bowman, Samuel R},
  journal={arXiv preprint arXiv:2311.12022},
  year={2023}
}

@article{xiao2024duoattention,
  title={Duoattention: Efficient long-context llm inference with retrieval and streaming heads},
  author={Xiao, Guangxuan and Tang, Jiaming and Zuo, Jingwei and Guo, Junxian and Yang, Shang and Tang, Haotian and Fu, Yao and Han, Song},
  journal={arXiv preprint arXiv:2410.10819},
  year={2024}
}

@article{tang2024razorattention,
  title={Razorattention: Efficient kv cache compression through retrieval heads},
  author={Tang, Hanlin and Lin, Yang and Lin, Jing and Han, Qingsen and Hong, Shikuan and Yao, Yiwu and Wang, Gongyi},
  journal={arXiv preprint arXiv:2407.15891},
  year={2024}
}

@article{xiao2023streamingllm,
  title={Efficient streaming language models with attention sinks},
  author={Xiao, Guangxuan and Tian, Yuandong and Chen, Beidi and Han, Song and Lewis, Mike},
  journal={arXiv preprint arXiv:2309.17453},
  year={2023}
}

@inproceedings{ester1996dbscan,
  title={A density-based algorithm for discovering clusters in large spatial databases with noise},
  author={Ester, Martin and Kriegel, Hans-Peter and Sander, J{\"o}rg and Xu, Xiaowei and others},
  booktitle={kdd},
  volume={96},
  number={34},
  pages={226--231},
  year={1996}
}

@misc{eval-harness,
  author       = {Gao, Leo and Tow, Jonathan and Abbasi, Baber and Biderman, Stella and Black, Sid and DiPofi, Anthony and Foster, Charles and Golding, Laurence and Hsu, Jeffrey and Le Noac'h, Alain and Li, Haonan and McDonell, Kyle and Muennighoff, Niklas and Ociepa, Chris and Phang, Jason and Reynolds, Laria and Schoelkopf, Hailey and Skowron, Aviya and Sutawika, Lintang and Tang, Eric and Thite, Anish and Wang, Ben and Wang, Kevin and Zou, Andy},
  title        = {A framework for few-shot language model evaluation},
  month        = 07,
  year         = 2024,
  publisher    = {Zenodo},
  version      = {v0.4.3},
  doi          = {10.5281/zenodo.12608602},
  url          = {https://zenodo.org/records/12608602}
}

@article{kang2024gear,
  title={Gear: An efficient kv cache compression recipefor near-lossless generative inference of llm},
  author={Kang, Hao and Zhang, Qingru and Kundu, Souvik and Jeong, Geonhwa and Liu, Zaoxing and Krishna, Tushar and Zhao, Tuo},
  journal={arXiv preprint arXiv:2403.05527},
  year={2024}
}

@inproceedings{zhang2024cam,
  title={CaM: Cache Merging for Memory-efficient LLMs Inference},
  author={Zhang, Yuxin and Du, Yuxuan and Luo, Gen and Zhong, Yunshan and Zhang, Zhenyu and Liu, Shiwei and Ji, Rongrong},
  booktitle={Forty-first International Conference on Machine Learning},
year={2024}
}

@inproceedings{sheng2023flexgen,
  title={Flexgen: High-throughput generative inference of large language models with a single gpu},
  author={Sheng, Ying and Zheng, Lianmin and Yuan, Binhang and Li, Zhuohan and Ryabinin, Max and Chen, Beidi and Liang, Percy and R{\'e}, Christopher and Stoica, Ion and Zhang, Ce},
  booktitle={International Conference on Machine Learning},
  pages={31094--31116},
  year={2023},
  organization={PMLR}
}

@article{lin2024qserve,
  title={Qserve: W4a8kv4 quantization and system co-design for efficient llm serving},
  author={Lin, Yujun and Tang, Haotian and Yang, Shang and Zhang, Zhekai and Xiao, Guangxuan and Gan, Chuang and Han, Song},
  journal={arXiv preprint arXiv:2405.04532},
  year={2024}
}

@article{dao2022flashattention,
  title={Flashattention: Fast and memory-efficient exact attention with io-awareness},
  author={Dao, Tri and Fu, Dan and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={16344--16359},
  year={2022}
}

@webpage{
    OpenAI2024PromptCache,
    author={OpenAI},
    url={https://openai.com/index/api-prompt-caching/},
    year={2024}
}

@webpage{
    DeepSeek2024ContextCache,
    author={DeepSeek},
    url={https://api-docs.deepseek.com/guides/kv_cache},
    year={2024}
}

@webpage{
    SoftAge_AI2024Multi_turn_softage,
    author={SoftAge-AI},
    url={https://huggingface.co/datasets/SoftAge-AI/multi-turn_dataset},
    year={2024}
}



@inproceedings{akiba2019optuna,
  title={Optuna: A next-generation hyperparameter optimization framework},
  author={Akiba, Takuya and Sano, Shotaro and Yanase, Toshihiko and Ohta, Takeru and Koyama, Masanori},
  booktitle={Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery \& data mining},
  pages={2623--2631},
  year={2019}
}

@article{zhang2007moea,
  title={MOEA/D: A multiobjective evolutionary algorithm based on decomposition},
  author={Zhang, Qingfu and Li, Hui},
  journal={IEEE Transactions on evolutionary computation},
  volume={11},
  number={6},
  pages={712--731},
  year={2007},
  publisher={IEEE}
} 

@article{jin2024ragcache,
  title={RAGCache: Efficient Knowledge Caching for Retrieval-Augmented Generation},
  author={Jin, Chao and Zhang, Zili and Jiang, Xuanlin and Liu, Fangyue and Liu, Xin and Liu, Xuanzhe and Jin, Xin},
  journal={arXiv preprint arXiv:2404.12457},
  year={2024}
}

@article{cobbe2021gsm8k,
  title={Training Verifiers to Solve Math Word Problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and Hesse, Christopher and Schulman, John},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@article{huang2024ceval,
  title={C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models},
  author={Huang, Yuzhen and Bai, Yuzhuo and Zhu, Zhihao and Zhang, Junlei and Zhang, Jinghan and Su, Tangjun and Liu, Junteng and Lv, Chuancheng and Zhang, Yikai and Fu, Yao and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{hendrycks2020mmlu,
  title={Measuring massive multitask language understanding},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2009.03300},
  year={2020}
}

@article{joshi2017triviaqa,
  title={Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension},
  author={Joshi, Mandar and Choi, Eunsol and Weld, Daniel S and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1705.03551},
  year={2017}
}

@article{lai2017race,
  title={Race: Large-scale reading comprehension dataset from examinations},
  author={Lai, Guokun and Xie, Qizhe and Liu, Hanxiao and Yang, Yiming and Hovy, Eduard},
  journal={arXiv preprint arXiv:1704.04683},
  year={2017}
}

@article{lin2021truthfulqa,
  title={Truthfulqa: Measuring how models mimic human falsehoods},
  author={Lin, Stephanie and Hilton, Jacob and Evans, Owain},
  journal={arXiv preprint arXiv:2109.07958},
  year={2021}
}

@article{yang2024qwen2.5,
  title={Qwen2. 5 Technical Report},
  author={Yang, An and Yang, Baosong and Zhang, Beichen and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and Wei, Haoran and others},
  journal={arXiv preprint arXiv:2412.15115},
  year={2024}
}

@article{dubey2024llama3.1,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}

@article{shoeybi2019megatron,
  title={Megatron-lm: Training multi-billion parameter language models using model parallelism},
  author={Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:1909.08053},
  year={2019}
}

@article{zheng2024sglang,
  title={Sglang: Efficient execution of structured language model programs},
  author={Zheng, Lianmin and Yin, Liangsheng and Xie, Zhiqiang and Sun, Chuyue and Huang, Jeff and Yu, Cody Hao and Cao, Shiyi and Kozyrakis, Christos and Stoica, Ion and Gonzalez, Joseph E and others},
  journal={arXiv preprint arXiv:2312.07104},
  year={2024}
}

@misc{2023lmdeploy,
    title={LMDeploy: A Toolkit for Compressing, Deploying, and Serving LLM},
    author={LMDeploy Contributors},
    howpublished = {\url{https://github.com/InternLM/lmdeploy}},
    year={2023}
}

@article{liu2024deepseekv3,
  title={Deepseek-v3 technical report},
  author={Liu, Aixin and Feng, Bei and Xue, Bing and Wang, Bingxuan and Wu, Bochao and Lu, Chengda and Zhao, Chenggang and Deng, Chengqi and Zhang, Chenyu and Ruan, Chong and others},
  journal={arXiv preprint arXiv:2412.19437},
  year={2024}
}

@article{gloeckle2024mtp,
  title={Better \& faster large language models via multi-token prediction},
  author={Gloeckle, Fabian and Idrissi, Badr Youbi and Rozi{\`e}re, Baptiste and Lopez-Paz, David and Synnaeve, Gabriel},
  journal={arXiv preprint arXiv:2404.19737},
  year={2024}
}

@article{stern2018blockwise,
  title={Blockwise parallel decoding for deep autoregressive models},
  author={Stern, Mitchell and Shazeer, Noam and Uszkoreit, Jakob},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@article{wolf2020transformers,
  title={Transformers: State-of-the-Art Natural Language Processing},
  author={Wolf, Thomas},
  journal={arXiv preprint arXiv:1910.03771},
  year={2020}
}

@article{yang2025attentionpredictor,
  title={AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference},
  author={Yang, Qingyue and Wang, Jie and Li, Xing and Wang, Zhihai and Chen, Chen and Chen, Lei and Yu, Xianzhi and Liu, Wulong and Hao, Jianye and Yuan, Mingxuan and others},
  journal={arXiv preprint arXiv:2502.04077},
  year={2025}
}

@misc{li2025kvtuner,
      title={KVTuner: Sensitivity-Aware Layer-wise Mixed Precision KV Cache Quantization for Efficient and Nearly Lossless LLM Inference}, 
      author={Xing Li and Zeyu Xing and Yiming Li and Linping Qu and Hui-Ling Zhen and Wulong Liu and Yiwu Yao and Sinno Jialin Pan and Mingxuan Yuan},
      year={2025},
      eprint={2502.04420},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2502.04420}, 
}

@article{bai2023longbench,
  title={LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding},
  author={Bai, Yushi and Lv, Xin and Zhang, Jiajie and Lyu, Hongchang and Tang, Jiankai and Huang, Zhidian and Du, Zhengxiao and Liu, Xiao and Zeng, Aohan and Hou, Lei and Dong, Yuxiao and Tang, Jie and Li, Juanzi},
  journal={arXiv preprint arXiv:2308.14508},
  year={2023}
}