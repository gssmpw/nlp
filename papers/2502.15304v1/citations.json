[
  {
    "index": 0,
    "papers": [
      {
        "key": "ge2023fastgen",
        "author": "Ge, Suyu and Zhang, Yunan and Liu, Liyuan and Zhang, Minjia and Han, Jiawei and Gao, Jianfeng",
        "title": "Model tells you what to discard: Adaptive kv cache compression for llms"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "zhang2024h2o",
        "author": "Zhang, Zhenyu and Sheng, Ying and Zhou, Tianyi and Chen, Tianlong and Zheng, Lianmin and Cai, Ruisi and Song, Zhao and Tian, Yuandong and R{\\'e}, Christopher and Barrett, Clark and others",
        "title": "H2o: Heavy-hitter oracle for efficient generative inference of large language models"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "tang_quest_2024",
        "author": "Tang, Jiaming and Zhao, Yilong and Zhu, Kan and Xiao, Guangxuan and Kasikci, Baris and Han, Song",
        "title": "Quest: {Query}-{Aware} {Sparsity} for {Efficient} {Long}-{Context} {LLM} {Inference}"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "ribar_sparq_2024",
        "author": "Ribar, Luka and Chelombiev, Ivan and Hudlass-Galley, Luke and Blake, Charlie and Luschi, Carlo and Orr, Douglas",
        "title": "{SparQ} {Attention}: {Bandwidth}-{Efficient} {LLM} {Inference}"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "zhang2024pqcache",
        "author": "Zhang, Hailin and Ji, Xiaodong and Chen, Yilin and Fu, Fangcheng and Miao, Xupeng and Nie, Xiaonan and Chen, Weipeng and Cui, Bin",
        "title": "Pqcache: Product quantization-based kvcache for long context llm inference"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "sun_shadowkv_2024",
        "author": "Sun, Hanshi and Chang, Li-Wen and Bao, Wenlei and Zheng, Size and Zheng, Ningxin and Liu, Xin and Dong, Harry and Chi, Yuejie and Chen, Beidi",
        "title": "{ShadowKV}: {KV} {Cache} in {Shadows} for {High}-{Throughput} {Long}-{Context} {LLM} {Inference}"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "yang2025attentionpredictor",
        "author": "Yang, Qingyue and Wang, Jie and Li, Xing and Wang, Zhihai and Chen, Chen and Chen, Lei and Yu, Xianzhi and Liu, Wulong and Hao, Jianye and Yuan, Mingxuan and others",
        "title": "AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "singhania_loki_2024",
        "author": "Singhania, Prajwal and Singh, Siddharth and He, Shwai and Feizi, Soheil and Bhatele, Abhinav",
        "title": "Loki: {Low}-rank {Keys} for {Efficient} {Sparse} {Attention}"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "xu_think_2024",
        "author": "Xu, Yuhui and Jie, Zhanming and Dong, Hanze and Wang, Lei and Lu, Xudong and Zhou, Aojun and Saha, Amrita and Xiong, Caiming and Sahoo, Doyen",
        "title": "{ThinK}: {Thinner} {Key} {Cache} by {Query}-{Driven} {Pruning}"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "wang_svd-llm_2024",
        "author": "Wang, Xin and Zheng, Yu and Wan, Zhongwei and Zhang, Mi",
        "title": "{SVD}-{LLM}: {Truncation}-aware {Singular} {Value} {Decomposition} for {Large} {Language} {Model} {Compression}"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "zhang_lorc_2024",
        "author": "Zhang, Rongzhi and Wang, Kuang and Liu, Liyuan and Wang, Shuohang and Cheng, Hao and Zhang, Chao and Shen, Yelong",
        "title": "{LoRC}: {Low}-{Rank} {Compression} for {LLMs} {KV} {Cache} with a {Progressive} {Compression} {Strategy}"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "chang_palu_2024",
        "author": "Chang, Chi-Chih and Lin, Wei-Cheng and Lin, Chien-Yu and Chen, Chong-Yan and Hu, Yu-Fang and Wang, Pei-Shuo and Huang, Ning-Chi and Ceze, Luis and Abdelfattah, Mohamed S. and Wu, Kai-Chiang",
        "title": "Palu: {Compressing} {KV}-{Cache} with {Low}-{Rank} {Projection}"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "saxena_eigen_2024",
        "author": "Saxena, Utkarsh and Saha, Gobinda and Choudhary, Sakshi and Roy, Kaushik",
        "title": "Eigen {Attention}: {Attention} in {Low}-{Rank} {Space} for {KV} {Cache} {Compression}"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "ainslie_gqa_2023",
        "author": "Ainslie, Joshua and Lee-Thorp, James and Jong, Michiel de and Zemlyanskiy, Yury and Lebr\u00f3n, Federico and Sanghai, Sumit",
        "title": "{GQA}: {Training} {Generalized} {Multi}-{Query} {Transformer} {Models} from {Multi}-{Head} {Checkpoints}"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "deepseek-ai_deepseek-r1_2025",
        "author": "{DeepSeek-AI} and Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and others",
        "title": "{DeepSeek}-{R1}: {Incentivizing} {Reasoning} {Capability} in {LLMs} via {Reinforcement} {Learning}"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "jin_align_2024",
        "author": "Jin, Qingyun and Song, Xiaohui and Zhou, Feng and Qin, Zengchang",
        "title": "Align {Attention} {Heads} {Before} {Merging} {Them}: {An} {Effective} {Way} for {Converting} {MHA} to {GQA}"
      },
      {
        "key": "chen_optimised_2024",
        "author": "Chen, Yuang and Zhang, Cheng and Gao, Xitong and Mullins, Robert D. and Constantinides, George A. and Zhao, Yiren",
        "title": "Optimised {Grouped}-{Query} {Attention} {Mechanism} for {Transformers}"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "liu_kivi_2023",
        "author": "Liu, Zirui and Yuan, Jiayi and Jin, Hongye and Zhong, Shaochen and Xu, Zhaozhuo and Braverman, Vladimir and Chen, Beidi and Hu, Xia",
        "title": "{KIVI}: {A} {Tuning}-{Free} {Asymmetric} 2bit {Quantization} for {KV} {Cache}"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "hooper2024kvquant",
        "author": "Hooper, Coleman and Kim, Sehoon and Mohammadzadeh, Hiva and Mahoney, Michael W and Shao, Yakun Sophia and Keutzer, Kurt and Gholami, Amir",
        "title": "Kvquant: Towards 10 million context length llm inference with kv cache quantization"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "tan_alignedkv_2024",
        "author": "Tan, Yifan and Wang, Haoze and Yan, Chao and Deng, Yangdong",
        "title": "{AlignedKV}: {Reducing} {Memory} {Access} of {KV}-{Cache} with {Precision}-{Aligned} {Quantization}"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "wang_bitstack_2024",
        "author": "Wang, Xinghao and Wang, Pengyu and Wang, Bo and Zhang, Dong and Zhou, Yunhua and Qiu, Xipeng",
        "title": "{BitStack}: {Fine}-{Grained} {Size} {Control} for {Compressed} {Large} {Language} {Models} in {Variable} {Memory} {Environments}"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "li2025kvtuner",
        "author": "Xing Li and Zeyu Xing and Yiming Li and Linping Qu and Hui-Ling Zhen and Wulong Liu and Yiwu Yao and Sinno Jialin Pan and Mingxuan Yuan",
        "title": "KVTuner: Sensitivity-Aware Layer-wise Mixed Precision KV Cache Quantization for Efficient and Nearly Lossless LLM Inference"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "lin_qserve_2024",
        "author": "Lin, Yujun and Tang, Haotian and Yang, Shang and Zhang, Zhekai and Xiao, Guangxuan and Gan, Chuang and Han, Song",
        "title": "{QServe}: {W4A8KV4} {Quantization} and {System} {Co}-design for {Efficient} {LLM} {Serving}"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "sun_shadowkv_2024",
        "author": "Sun, Hanshi and Chang, Li-Wen and Bao, Wenlei and Zheng, Size and Zheng, Ningxin and Liu, Xin and Dong, Harry and Chi, Yuejie and Chen, Beidi",
        "title": "{ShadowKV}: {KV} {Cache} in {Shadows} for {High}-{Throughput} {Long}-{Context} {LLM} {Inference}"
      }
    ]
  },
  {
    "index": 23,
    "papers": [
      {
        "key": "xu_think_2024",
        "author": "Xu, Yuhui and Jie, Zhanming and Dong, Hanze and Wang, Lei and Lu, Xudong and Zhou, Aojun and Saha, Amrita and Xiong, Caiming and Sahoo, Doyen",
        "title": "{ThinK}: {Thinner} {Key} {Cache} by {Query}-{Driven} {Pruning}"
      }
    ]
  },
  {
    "index": 24,
    "papers": [
      {
        "key": "liu_unlocking_2024",
        "author": "Liu, Peiyu and Gao, Ze-Feng and Zhao, Wayne Xin and Ma, Yipeng and Wang, Tao and Wen, Ji-Rong",
        "title": "Unlocking {Data}-free {Low}-bit {Quantization} with {Matrix} {Decomposition} for {KV} {Cache} {Compression}"
      }
    ]
  },
  {
    "index": 25,
    "papers": [
      {
        "key": "chang_palu_2024",
        "author": "Chang, Chi-Chih and Lin, Wei-Cheng and Lin, Chien-Yu and Chen, Chong-Yan and Hu, Yu-Fang and Wang, Pei-Shuo and Huang, Ning-Chi and Ceze, Luis and Abdelfattah, Mohamed S. and Wu, Kai-Chiang",
        "title": "Palu: {Compressing} {KV}-{Cache} with {Low}-{Rank} {Projection}"
      }
    ]
  },
  {
    "index": 26,
    "papers": [
      {
        "key": "ping_delta-come_2024",
        "author": "Ping, Bowen and Wang, Shuo and Wang, Hanqing and Han, Xu and Xu, Yuzhuang and Yan, Yukun and Chen, Yun and Chang, Baobao and Liu, Zhiyuan and Sun, Maosong",
        "title": "Delta-{CoMe}: {Training}-{Free} {Delta}-{Compression} with {Mixed}-{Precision} for {Large} {Language} {Models}"
      }
    ]
  },
  {
    "index": 27,
    "papers": [
      {
        "key": "li_svdquant_2024",
        "author": "Li, Muyang and Lin, Yujun and Zhang, Zhekai and Cai, Tianle and Li, Xiuyu and Guo, Junxian and Xie, Enze and Meng, Chenlin and Zhu, Jun-Yan and Han, Song",
        "title": "{SVDQuant}: {Absorbing} {Outliers} by {Low}-{Rank} {Components} for 4-{Bit} {Diffusion} {Models}"
      }
    ]
  }
]