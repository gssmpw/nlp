\section{Related Works}
\label{sec:relatedworks}

\textbf{Sparsity}: With different feature extraction based attention estimation algorithms, methods such as Fastgen ____, H2O ____, Quest ____, SparQ ____, PQCache ____, ShadowKV ____, and AttentionPredictor ____ selectively retain only the most important tokens in the sequence and effectively prune the others. Loki ____ is another sparsity method that uses the SVD approximation to accelerate attention estimation for critical tokens selection.
 
\noindent \textbf{Channel Compression}: These methods, such as ThinK ____, reduce the dimensionality of $KV$ cache by truncating channels or employing low-rank approximations. Prominent examples include SVD-based approaches like SVD-LLM ____, LoRC ____, Palu ____, and Eigen Attention ____. Notably, techniques like Grouped Query Attention (GQA) ____, Multi-head Latent Attention (MLA) ____, and transformations from Multi-Head Attention to GQA ____ can also be viewed as forms of channel compression, as they effectively reduce the number of attention dimensions.

\noindent \textbf{Quantization}: Methods like KIVI ____, KVQuant ____, AlignedKV ____, BitStack ____, and KVTuner ____ reduce the memory footprint with low precision $KV$ cache. QServe ____ introduces several quantization and system co-design methods to achieve efficient W4A8KV4, where SmoothAttention is utilized to migrate the key quantization difficulty to query.

Some works explore the combination of these approaches. In addition to the mentioned ShadowKV ____ and ThinK ____, ____ integrates quantization with matrix decomposition to apply different quantization precision for the two decomposed matrices, and Palu ____ applies per token quantization to the latent vector of the SVD low-rank approximation.
% \todo{Rewrite this paragraph}

Importantly, the concept of using SVD for mixed-precision quantization has been explored in other contexts. For instance, Delta-CoMe ____ applies this principle to compress LLM weights, while SVDQuant ____ utilizes it for compressing diffusion models. The novelty of this work over the mentioned works lies not only in the application of this principle to $K$ cache compression but also in the \textbf{theoretical foundation} upon which we derive the principle and method, and the \textbf{error analysis} we provide.