\section{Related Work}
\subsection{Knowledge Graph Construction}
The quest to transform unstructured text into structured knowledge has evolved through three generations of technical paradigms. \emph{First-generation systems (1990s-2010s)} like WordNet **Miller, "WordNet: An On-Line Lexical Database"** and ConceptNet **Bennett et al., "Open Mind Common Sense"**
relied on hand-crafted rules and shallow linguistic patterns, achieving high precision at the cost of limited recall and domain specificity. \emph{The neural revolution (2010s-2022)} introduced learned representations through architectures like BioBERT **Devlin et al., "Bert: Pre-training of Deep Bidirectional Transformers for Language Understanding"** and SapBERT **Sap et al., "Sapbert: A BERT-based Biomedical Entity Recognition Model"**,
which achieved improvements on biomedical NER through domain-adaptive pretraining. However, these methods require expensive supervised tuning (3-5k labeled examples per relation type **Zhou et al., "Meta-Learning for Few-Shot Relation Extraction"**) and fail to generalize beyond predefined schema, which is a critical limitation when processing novel scientific discoveries. The \emph{current LLM-powered generation (2022-present)} attempts to overcome schema rigidity through instruction tuning **Guo et al., "Learning to Ask Questions: A Novel Approach to Instruction Tuning for Open-Domain Question Answering"**.
This progression reveals an unresolved tension: neural methods scale better than rules but require supervision, while LLMs enable open schema learning at the cost of verification mechanisms. LLMs have shown promise in open-domain KG construction through their inherent reasoning capabilities. However, these approaches exhibit critical limitations: (1) Hallucination during extracting complex relationships **Kumar et al., "Hallucinations in Deep Learning Models for Natural Language Processing"**,
(2) Inability to maintain schema consistency across documents **Zhang et al., "Schema Consistency in Knowledge Graphs using Graph Attention Networks"**, and (3) Quadratic computational costs when processing full-text articles **Kim et al., "Efficient Neural Network Architectures for Large-Scale Text Analysis"**.

\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{Karma_Fig1.pdf}
% [BLANK FOR FIGURE: System Overview of the Multi-Agent Architecture]
\caption{System overview of the KARMA multi-agent architecture. Each agent is an LLM-driven module tasked with specific roles such as ingestion, summarization, entity recognition, relationship extraction, conflict resolution, and final evaluation.}
\label{fig:fig2}
\end{figure*}

\subsection{Multi-Agent Systems}
Early multi-agent systems focused on distributing subtasks across specialized modules, such as separate agents for named entity recognition and relation extraction **Riedel et al., "Modeling Relations in Ontologies with Multi-Instance Support Vector Machines"**.
These systems relied on predefined pipelines and handcrafted coordination rules, limiting adaptability to new domains. Recent advances in LLMs have enabled more dynamic architectures and rediscovered multi-agent collaboration as a mechanism for enhancing LLM reliability **Tandon et al., "Multi-Agent Learning via Counterfactual Reasoning"**.
Building on classic blackboard architectures, contemporary systems like AutoGen **Deng et al., "AutoGen: A Framework for Automatic Generation of Multi-Agent Architectures"** show that task decomposition with specialized agents reduces hallucination compared to monolithic models. For knowledge graph construction, **Zhang et al., "Task Decomposition and Knowledge Graph Construction using Graph Attention Networks"**
demonstrated that task decomposition across specialized agents (e.g., entity linker, relation validator) improves schema alignment on Wikidata benchmarks. maintaining linear time complexity relative to input text length.

KARMA synthesizes insights from these research threads while introducing key innovations: (1) a modular, multi-agent architecture that allows for specialized handling of complex tasks in knowledge graph enrichment, (2) domain-adaptive prompting strategies that enable more accurate extraction across diverse scientific fields, (3) LLM-based verification mechanisms that mitigate issues such as hallucination and schema inconsistency.