\section{Related Work}
\subsection{Knowledge Graph Construction}
The quest to transform unstructured text into structured knowledge has evolved through three generations of technical paradigms. \emph{First-generation systems (1990s-2010s)} like WordNet ____ and ConceptNet ____ relied on hand-crafted rules and shallow linguistic patterns, achieving high precision at the cost of limited recall and domain specificity. \emph{The neural revolution (2010s-2022)} introduced learned representations through architectures like BioBERT ____ and SapBERT ____, which achieved improvements on biomedical NER through domain-adaptive pretraining. However, these methods require expensive supervised tuning (3-5k labeled examples per relation type ____) and fail to generalize beyond predefined schema, which is a critical limitation when processing novel scientific discoveries. The \emph{current LLM-powered generation (2022-present)} attempts to overcome schema rigidity through instruction tuning ____. This progression reveals an unresolved tension: neural methods scale better than rules but require supervision, while LLMs enable open schema learning at the cost of verification mechanisms. LLMs have shown promise in open-domain KG construction through their inherent reasoning capabilities. However, these approaches exhibit critical limitations: (1) Hallucination during extracting complex relationships ____, (2) Inability to maintain schema consistency across documents ____, and (3) Quadratic computational costs when processing full-text articles ____.

\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{Karma_Fig1.pdf}
% [BLANK FOR FIGURE: System Overview of the Multi-Agent Architecture]
\caption{System overview of the KARMA multi-agent architecture. Each agent is an LLM-driven module tasked with specific roles such as ingestion, summarization, entity recognition, relationship extraction, conflict resolution, and final evaluation.}
\label{fig:fig2}
\end{figure*}

\subsection{Multi-Agent Systems}
Early multi-agent systems focused on distributing subtasks across specialized modules, such as separate agents for named entity recognition and relation extraction ____. These systems relied on predefined pipelines and handcrafted coordination rules, limiting adaptability to new domains. Recent advances in LLMs have enabled more dynamic architectures and rediscovered multi-agent collaboration as a mechanism for enhancing LLM reliability ____. Building on classic blackboard architectures, contemporary systems like AutoGen ____ show that task decomposition with specialized agents reduces hallucination compared to monolithic models. For knowledge graph construction, ____ demonstrated that task decomposition across specialized agents (e.g., entity linker, relation validator) improves schema alignment on Wikidata benchmarks. maintaining linear time complexity relative to input text length.

KARMA synthesizes insights from these research threads while introducing key innovations: (1) a modular, multi-agent architecture that allows for specialized handling of complex tasks in knowledge graph enrichment, (2) domain-adaptive prompting strategies that enable more accurate extraction across diverse scientific fields, (3) LLM-based verification mechanisms that mitigate issues such as hallucination and schema inconsistency.