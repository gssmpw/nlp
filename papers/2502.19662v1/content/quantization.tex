\section{Quantization Framework}
\label{section:quantization}
\begin{comment}
Our quantization framework for accelerating large language model (LLM) inference introduces a novel, selective approach focused on reducing critical delay by prioritizing low-critical-delay weights. The design is optimized to maintain model accuracy while improving inference speed through adaptive quantization at multiple levels, including per-tile, per-channel, and layer sensitivity granularity. This methodology addresses computational efficiency challenges in high-dimensional LLMs by targeting only the weights that most impact inference delays.

\subsection{Uniform and Non-Uniform Quantization} 
% Previous studies have demonstrated that the primary goal of quantization techniques is to minimize the Mean Square Error (MSE) between the original and the quantized model.
Quantization is a powerful technique for reducing the memory footprint of neural network models by decreasing the bitwidth of their parameters. The literature on quantization can be broadly categorized into two main approaches: uniform and non-uniform methods.

\noindent \textbf{Uniform Quantization} Uniform quantization maps a continuous range of values to discrete levels that are evenly spaced. In a $b$-bit quantization scheme, the data $x$ is mapped to $2^b$ distinct levels. The quantization step $s$, also known as the scaling factor, is defined as:

\begin{equation}
s = \frac{x_{\text{max}} - x_{\text{min}}}{2^b}
\end{equation}

The quantization function $q(x)$ maps the data $x$ to the nearest quantization level, given by:
\begin{equation}
q(x) = x_{\text{min}} + is \quad \text{for} \quad i = 0, 1, 2, \ldots, 2^b-1
\end{equation}

This approach ensures that each quantized level is equidistant from its neighbors, resulting in a fixed-length representation for each interval. 

\noindent {\textbf{Non-uniform Quantization}} Non-uniform quantization, in contrast, allows for variable-sized intervals. Let $T = \{t_0 = -\infty, t_1, t_2, \ldots, t_{M-1}, t_M = +\infty\}$ be the set of thresholds that define the boundaries between quantization intervals. These thresholds partition the source range $X$ into $M+1$ disjoint regions $R_k = [t_{k-1}, t_k)$, for $k = 1, 2, \ldots, M+1$. 

The quantization function $q(x)$ assigns an input signal $x$ to the closest representation level $y_k$ based on which interval $R_k$ it falls into:
\begin{equation}
q(x) = y_k \quad \text{if} \quad x \in R_k
\end{equation}

The key difference is that the decision thresholds $(T)$ are not necessarily equally spaced, allowing for a more flexible allocation of quantization levels based on the data distribution within the source range $X$.
\end{comment}

Our quantization framework for LLM inference introduces a timing-aware strategy, detailed in Algorithm~\ref{quantization}, which prioritizes weights with low critical-path delays to minimize latency while preserving model fidelity. The adaptive method operates across levels and layer sensitivity, optimizing performance by focusing on weights most critical to efficiency. The framework comprises three key components: \circled{1} sensitivity-aware uniform quantization to identify and preserve critical weights \textit{(Lines 1-3)}, \circled{2} critical-path delay aware non-uniform quantization to optimize weight patterns for hardware efficiency \textit{(Lines 4-10)}, and \circled{3} adaptive DVFS to maximize performance by matching quantization levels with optimal operating frequencies.

\subsection{Sensitivity-aware Uniform Quantization}
\begin{figure}[t!]
	\scriptsize
	\centering
	\input{graphs/sensitive}
	\caption{Distribution highlighting sensitive weights important for accuracy.} 
	\label{fig:weight_distribution}
\end{figure}

The framework begins with a sensitivity analysis of model weights, identifying weights values that can tolerate quantization without significantly affecting accuracy, as outlined in Algorithm 1. The framework initially separates \textit{outliers} (outside the blue lines) and \textit{salient weights} (in red) from normal values, as shown in Fig.\ref{fig:weight_distribution}.

\noindent \textbf{Outliers \& Salient Weights:} We incorporate outlier removal to manage extreme weight values based on inter-quartile range scaling. To compute outliers in the weight distribution, we employ the 3$\sigma$ rule~\cite{olive}. Outliers are identified as values lying beyond three standard deviations from the mean. 

% This helps detect and manage extreme values that may affect model performance. 

% \noindent \textbf{Extremely-Salient Weights:} 
From the normal values obtained after this distribution, we rely on Taylor series expansion to estimate the most salient weights in the model. Following~\cite{squeezellm}, we use an approximation to the Hessian \(H \) based on the Fisher information matrix \( F \), which can be calculated over a sample dataset \( D \) as
\begin{equation}
F = \frac{1}{|D|} \sum_{d \in D} g_d g_d^{\top},
\label{eq:fisher}
\end{equation}
where \( g \) is the gradient and \( H \approx F \). This only requires computing the gradient for a set of samples. For each weight tensor \( W \), the weight sensitivity is computed as \(\Lambda_{W} = F \). Weights with higher \( \Lambda_{W} \) values are considered more salient due to their significant impact on the model's output. We preserve the top 0.05\% of the weights based on this criterion. Cumulatively, both outliers and extremely salient weight values correspond to less than 0.5\% of the total weight values. For this reason, we handle these weight values separately and apply per-channel quantization for this set of weight values, isolating them to maintain model precision.

\begin{algorithm}
\caption{Quantization Framework}
\label{quantization}
{\footnotesize
\begin{algorithmic}[1]
%\begin{footnotesize}
\Require calibration dataset $X$, pre-trained weight matrix $W$, gradient $G$
\Require number of bits $n$, tile size $t$, quantile threshold $k$, target frequencies $f_1$, $f_2$
\Ensure Quantized weight matrix $W_q$

\State $W_s, S \leftarrow \text{ExtractSalientValues}(W, G)$ \Comment{Isolate values with high saliency}
\State $W_o, O \leftarrow \text{ExtractOutliers}(W_s)$ \Comment{Separate outlier weights}
\State $W_{s,o}^{q} \leftarrow \text{Quantize}(W_{s} + W_{o})$  \Comment{Quantize outliers and salient weights}

\State $W_t \leftarrow 
\text{ReshapeIntoTiles}(\text{PadMatrix}(W_o, t), t)$ \Comment{Tile reshaping}

\State $\Lambda_{T_k} \leftarrow \text{CalculateTileSensitivities}(G)$ \Comment{Compute sensitivity for each tile}
\State $M_l, M_h \leftarrow \text{CreateMasks}(M, \text{ComputeAdaptiveK}(\Lambda_{T_k}, k))$ \Comment{Classify tiles as low or high sensitivity}
\State $W_{l,i}, W_{h,i} \leftarrow W_{t,i} \odot M_{l,i}, W_{t,i} \odot M_{h,i}$ \Comment{Apply masks}
\State $W_{l,i} \leftarrow \text{Quantize}(W_{l,i}, f_1)$ \Comment{Quantize low-sensitivity tiles}
\State $W_{h,i} \leftarrow \text{Quantize}(W_{h,i}, f_2)$  \Comment{Quantize high-sensitivity tiles}
\State $W_q \leftarrow W_{l,i}, W_{h,i}, W_{s,o}$
\State \Return $W_q$
%\end{footnotesize}
\end{algorithmic}
}
\end{algorithm}
%\vspace{-.5cm}

\subsection{Critical-path delay aware Non-Uniform Quantization} 
\label{section:crit_quant}
Uniform Quantization discretizes continuous values into \(2^b\) evenly spaced levels. On the other hand, non-uniform quantization adapts to the data distribution using variable interval sizes defined by thresholds \( T \), which partition the input range into regions \( R_k = [t_{k-1}, t_k) \). Each region \( R_k \) is assigned a representation level \( y_k \), where \( y_k \) is the quantized value corresponding to data points within \( R_k \). In this work, we leverage non-uniform quantization to more efficiently map the distribution of weights to specific values that reduce the critical-path delays (as discussed in Sec.\ref{section:motivation}), thereby optimizing frequency and energy consumption.

% To do so, we target weights impacting bottleneck layers are selectively quantized using e.g., attention and projection layers) are optimized for efficient inference.

\noindent \textbf{Tile-Based Sensitivity Analysis:} To optimize the model for efficient inference on hardware, the weight tensors are divided into fixed-size tiles (\(128 \times 128\) by default). Specifically, the sensitivity of each tile is evaluated 
as the sum of the absolute values of the gradients for each tile, normalized by the size of the tile, based on Eq.\ref{eq:fisher}. For a given $k$th tile \( T_k \), we compute a \textit{per-tile sensitivity score} \( \Lambda_{T_k} \) using a diagonal approximation of the Fisher information matrix:
\begin{equation}
\Lambda_{T_k} = \frac{\sum_{i,j} g_{k,i,j}^2}{\text{tile\_rows} \times \text{tile\_cols}}
\end{equation}

where \( g_{k,i,j} \) denotes the gradient of the loss with respect to each weight in the \( k \)-th tile, and \( \text{tile\_rows} \times \text{tile\_cols} \) represents the total number of elements within the tile. This score captures the average Fisher information across all weights in the tile, providing a quantitative measure of the tile's sensitivity in relation to its influence on the model's output.

\noindent \textbf{Tile Sensitivity Mapping}: 
To balance hardware efficiency and model accuracy, tiles in each layer are classified as \underline{low-sensitive} or \underline{high-sensitive} based on their relative importance. Determining a fixed sensitivity threshold for each layer is challenging, as weight distributions vary significantly across layers. To address this, we employ a dynamic tile sensitivity mapping strategy that adapts to the cumulative sensitivity distribution of each layer.

The process starts by computing the sensitivity of all tiles in a given layer, derived as the normalized sum of absolute gradient magnitudes within each tile. Sensitivities are then sorted in descending order to rank tiles by importance. A cumulative sum of these sorted sensitivities is calculated and normalized against the total layer sensitivity, generating a cumulative distribution curve from 0 to 1.

The mapping threshold \(k \) is derived from this curve and represents the fraction of tiles classified as low-sensitive, ensuring a specified percentage of total sensitivity (e.g., 95\%) is retained. Tiles contributing most to overall sensitivity are marked high-sensitive, while the rest are classified as low-sensitive. Mathematically, \(k \) is the ratio of the index where cumulative sensitivity exceeds the threshold to the total number of tiles, defaulting to 1.0 if no such index exists.

Once \(k \) is determined, boolean masks separate tiles into low- and high-sensitivity categories. Low-sensitivity tiles are quantized more aggressively, while high-sensitivity tiles retain higher precision to preserve performance. The adaptive quantization and computation flow based on the DVFS characteristics are described in detail in Sec.\ref{section:execution}.

% \subsection{Algorithm Design}


% \subsection{Tile-Based Weight Mapping and Distribution}

% To optimize hardware deployment, the framework reshapes weight tensors based on tile sizes suited for GPU and TPU hardware. Each weight matrix is partitioned into fixed-size tiles (128 $\times$ 128 by default) and then quantized based on their sensitivity classification. This enables efficient batching of low-importance tiles on high-utilization GPUs, reducing computational load and minimizing memory transfers.
% \begin{itemize}

    % \item \textbf{Hardware Implementation:}     
    % Low-sensitivity tiles are mapped onto under-utilized GPUs in a multi-GPU setup, redistributing computational loads based on the memory and processing capacity of each device. This enhances model throughput and scales efficiently across GPUs, essential for real-time deployment.
% \end{itemize}

% \subsection{Model Optimization and Integration}

