\section{Introduction}
\begin{comment}
\begin{itemize}
    \item LLMs are expensive
    \item Model compression, quantization
    \item Weight distribution
    \item Why current quantization works are bad?
    \item Proposal
    \item Contribution
\end{itemize}
\end{comment}

Transformer-based large language models (LLMs) have grown exponentially, increasing in size by 100 times every two years, far outpacing hardware improvements that have only seen a 3.1$\times$ increase every two years~\cite{L2M2, ainmodelsize}. This trend makes model inference costly, as demonstrated by models like Llama with 65 billion parameters and GPT-4 with 1.76 trillion parameters, which require substantial computational resources~\cite{llm_energy}. To address this, model compression techniques like quantization~\cite{awq, quip, 10705489, i-llm, bitnet} have become crucial, reducing computational complexity by using narrower bit lengths to decrease memory and accelerate computation. While Quantization-Aware Training (QAT)~\cite{qat} offers precise weight updates, it is impractical for large models due to high training costs, making Post-Training Quantization (PTQ)~\cite{awq, smoothquant} a more feasible alternative, despite potential accuracy losses.

Recent research~\cite{} indicates that transformer-based models exhibit greater parameter distribution variation than CNN-based models, with weight tensors often following a bell-shaped pattern that resembles both Gaussian and Laplace distributions. These models are particularly sensitive to a small fraction of outlier and salient weights, making standard quantization approaches less effective. As illustrated in Fig.~\ref{fig}, a small subset of weights (less than 0.1\% to 1\%) represent salient features, and quantizing these weights with higher precision significantly reduces quantization error. Additionally, outliers—weights with extreme values—must be carefully managed to preserve model accuracy.

Quantization techniques often focus on bit-width reduction but typically treat hardware units like Multiply-Accumulate (MAC) as black boxes, overlooking the impact of specific weight values on circuit efficiency. Some weights activate fewer signal paths, causing variations in critical path delay and energy consumption. In systolic arrays, such as those in TPUs, performance is constrained by the slowest operation, defined by the critical-path weights. To handle large models efficiently, weights are divided into weight tiles—blocks sized for on-chip memory to optimize processing. This tiling enables efficient data handling, and optimizing weights within tiles boosts performance and energy efficiency without changing the architecture. In weight-stationary dataflows, where weights remain fixed across input batches, weight optimization within tiles is vital for sustained efficiency. While PowerPruning and similar work have leveraged energy patterns to prune weights in CNNs, applying these methods to large language models (LLMs) is challenging due to their size and sensitivity to weight distributions.


Based on our findings, we propose a novel timing-aware and weight-sensitive PTQ framework and architecture, termed \textbf{SystolicX}. SystolicX employs a hybrid quantization approach, applying non-uniform quantization for most weights and uniform quantization for a small subset of highly sensitive weights. Given that the vast majority (99.9\%) of weights fall into the non-sensitive category, SystolicX achieves high accuracy in large language models with minimal hardware overhead due to three primary factors: (1) by dynamically adjusting quantization precision based on weight sensitivity, SystolicX reduces quantization error while preserving model accuracy with minimal impact on computational load; (2) it leverages a weight-stationary dataflow in systolic arrays to maximize reuse of quantized weights, reducing memory access and improving energy efficiency; and (3) it integrates seamlessly with existing systolic array and GPU architectures, enabling scalable deployment with consistent performance improvements.

\begin{itemize} 
\item \textbf{Timing and Energy Analysis}: Comprehensive evaluation of MAC circuit timing and energy variations. 
\item \textbf{\textit{name} Quantization Framework}: A hardware-aware quantization method utilizing timing, energy, and weight sensitivity to boost LLM efficiency. 
\item \textbf{Architectural Integration}: Efficient integration techniques for \textit{name} into systolic arrays and GPUs, enhancing performance while preserving accuracy.
\end{itemize}


The structure of the paper is as follows. Section~\ref{section:motivation} discusses the motivation behind this work. Sections~\ref{section:quantization} and~\ref{section:execution} provide details on the quantization framework and the execution model. The experimental results are presented in Section~\ref{section:evaluation}, and the conclusions are summarized in Section~\ref{section:conclusion}.


%Though, all these previous quantization techniques only consider bitwidth as the input, considering the MAC unit design as a black-box.
%A key insight is that the critical path and energy consumption of operations vary significantly based on specific weight values, as some weights activate fewer signal propagations in the circuit, leading to varying processing times. For systolic-based accelerators, the performance is limited by the slowest operation, which depends on weight values that define the critical delay. Optimizing weights within each tile can improve both performance and energy efficiency. This becomes even more important in weight-stationary dataflows, where weights remain fixed while processing large batches of inputs. Prior work, such as PowerPruning, has leveraged energy patterns to prune weights in CNN-based models, but extending these techniques to LLMs is still an active research area.



\section{Introduction}
Transformer-based large language models (LLMs) have experienced exponential growth, increasing 100-fold every two years, vastly outpacing the 3.1$\times$ advancements in hardware over the same period. This growing disparity has made inference prohibitively expensive, as exemplified by models like LLaMA~\cite{} with 65 billion parameters and GPT-4~\cite{} with 1.76 trillion parameters, which demand substantial computational resources~\cite{llm_energy}. Adding to this challenge is the fragmented landscape of hardware accelerators, marked by an increase in specialized designs optimized for distinct LLM quantization techniques and specific data types. Quantization, which reduces model size and computational cost by lowering the bit-width of weights and activations, is essential for addressing these efficiency demands. While these accelerators excel for certain quantization schemes, they lack the versatility needed for broader deployment on mainstream platforms like TPUs and GPUs. Consequently, there is a critical need for flexible, hardware-aware quantization strategies that can accommodate the diversity of current and emerging hardware architectures.

\textbf{\textit{Complexity of quantizing LLMs.}} Quantization for LLMs is particularly complex because of the wide dynamic range of their weight distributions.
% , often exhibiting Gaussian or Laplace-like pattern. 
A small but crucial subset of weights—less than 1\%—are highly sensitive, making naive quantization techniques vulnerable to significant loss in fidelity~\cite{}. 
As shown in Fig.~\ref{fig}, these weight distributions underscore the difficulty of achieving efficient quantization. 
Research indicates that higher precision must be preserved for these critical weights to substantially reduce quantization errors, highlighting the nuanced challenges involved in quantizing these models. %Furthermore, effective management of outliers is crucial to maintaining overall model performance, emphasizing the nuanced approach required for quantizing these models.

\textbf{\textit{Limitations of hardware-agnostic techniques.}} Despite their significance, most existing quantization techniques overly focus on reducing bit-width without considering hardware-specific factors, often treating critical components like Multiply-Accumulate (MAC) units as a black box. 
% overlooking how specific weight values impact circuit efficiency. In reality, certain weights activate fewer signal paths, causing variations in critical path delays and energy consumption. This variability is particularly pronounced in systolic arrays, such as those in TPUs, where performance is constrained by operations defined by critical-path weights. 
This overlooks how certain weight values can influence circuit efficiency and energy use, especially in architectures like systolic arrays (e.g., TPUs), where variations in critical-path delays and energy consumption emerge based on weight sensitivity.
Additionally, efficiently managing LLMs requires partitioning weights into tiles optimized for on-chip memory, enhancing data management and improving performance and energy efficiency. In weight-stationary dataflows, where weights remain fixed across input batches, fine-tuning these weight tiles is crucial for sustained efficiency. While approaches like PowerPruning~\cite{} have used energy-based pruning techniques along with repetitive model re-training to recover accuracy loss in CNNs, applying these methods to LLMs remains challenging due to their vast scale and sensitivity to weight distributions.

\begin{figure}[]
	\scriptsize
	\centering
	\includegraphics[width=\columnwidth]{images/framework.pdf}
	\caption{HALO quantization framework, using architectural details to yield Pareto-optimal trade-offs for diverse deployments.} 
	\label{fig:framework}
	%\vspace{-.5cm}
\end{figure}
\textbf{\textit{HALO is the answer.}} We introduce \textbf{HALO}, a timing-aware and weight-sensitive PTQ framework designed for LLMs. 
HALO employs a hybrid quantization approach: the small subset of sensitive weights is quantized uniformly to preserve precision, while the majority of weights, deemed less sensitive, are quantized non-uniformly. This strategy ensures high model accuracy while minimizing hardware overhead. 
HALO achieves this through three key strategies: (1) aligning quantization with circuit-level timing constraints to maximize performance, (2) tile-wise sensitivity analysis to dynamically enable selective frequency scaling for certain tiles, and (3) adaptive frequency scaling to enhance energy efficiency. Fig.~\ref{fig:framework} provides an overview of the framework, that incorporates architectural details, like tile sizes and MAC unit delay and energy profiles, to generate trade-off points for various deployment scenarios. This adaptable approach is applicable across a wide range of current and future accelerator architectures. 

Below, we outline our main contributions.

\begin{itemize} 
\item \textbf{Timing and Energy Analysis}: Comprehensive evaluation of MAC circuit timing and energy variations. 
\item \textbf{HALO Quantization Framework}: A hardware-aware quantization method utilizing timing, energy, and weight sensitivity to boost LLM efficiency. 
\item \textbf{Architectural Integration}: Efficient integration techniques for HALO into systolic arrays and GPUs, enhancing performance while preserving accuracy.
\end{itemize}

The structure of the paper is as follows. Section~\ref{section:motivation} describes the motivation for this work. Sections~\ref{section:quantization} and~\ref{section:execution} provide an overview of the quantization framework and execution model. Section~\ref{section:evaluation} presents the experimental results, followed by Section~\ref{sec:relatedworks}, which reviews related research emphasizing our contributions. Finally, Section~\ref{section:conclusion} summarizes the conclusions.





\section{Introduction}
\begin{comment}
Transformer-based large language models (LLMs) have experienced exponential growth, increasing 100-fold every two years, vastly outpacing the 3.1$\times$ advancements in hardware over the same period. This growing disparity has made inference prohibitively expensive, as exemplified by models like LLaMA~\cite{} with 65 billion parameters and GPT-4~\cite{} with 1.76 trillion parameters, which demand substantial computational resources~\cite{llm_energy}. Adding to this challenge is the fragmented landscape of hardware accelerators, marked by an increase in specialized designs optimized for distinct LLM quantization techniques and specific data types. Quantization, which reduces model size and computational cost by lowering the bit-width of weights and activations, is essential for addressing these efficiency demands. While these accelerators excel for certain quantization schemes, they lack the versatility needed for broader deployment on mainstream platforms like TPUs and GPUs. Consequently, there is a critical need for flexible, hardware-aware quantization strategies that can accommodate the diversity of current and emerging hardware architectures.
\end{comment}

{\bf Tulika: May consider updating the title to HALO: Hardware-aware quantization for Energy-Efficient LLM Acceleration to make it more general}

Transformer-based large language models (LLMs) have grown exponentially, increasing 100-fold every two years, far outpacing the 3.1× improvements in hardware. This widening gap has made inference increasingly costly, as seen with models like LLaMA (65 billion parameters) and GPT-4 (1.76 trillion parameters), which require vast computational resources. The challenge is further complicated by a fragmented landscape of hardware accelerators, each optimized for specific LLM quantization techniques and data types. Quantization, which reduces model size and cost by lowering bit-width, is crucial for efficiency. However, many accelerators lack the flexibility needed for broader deployment on mainstream platforms like TPUs and GPUs, underscoring the need for adaptable, hardware-aware quantization strategies for diverse architectures.


{\bf Tulika: Trying to connect to DVFS early on through the next few paragraphs. Please modify}
Quantization has emerged as an important technique to reduce the size and computational cost of models by decreasing the bit-width of their weights and activations. However, traditional quantization approaches solely focus on reducing bit-widths without considering their impact on circuit-level hardware performance. In TPUs, Multiply-Accumulate (MAC) units, integral components for matrix multiplications that are central to LLM inference, occupy 77-80\% of chip area and consume between 50-89\% of the total power. Under quantization, MAC units exhibit substantial variability in their performance and power characteristics depending on specific weight patterns. Certain weight patterns allow for shorter critical paths in MAC units, resulting in faster, higher-frequency operation, while other weight patterns induce longer critical paths, limiting the operating speed of the entire system {\bf add forward pointer to Figure 2 and 3}
. This variability presents an unexplored opportunity for optimizing inference performance and energy efficiency by intelligently managing weight quantization. Yet, existing quantization techniques fail to leverage this potential. 

In particular, most hardware accelerators for executing LLMs, such as TPUs and GPUs, support Dynamic Voltage and Frequency Scaling (DVFS) to balance performance with power. Quantization directly influences the critical path delays in MAC units, which in turn determines how well DVFS can be utilized to optimize both performance and energy efficiency. Our key insight is that by carefully selecting quantization levels that correspond to favorable critical path delays, we can enable higher operating frequencies while maintaining model accuracy. 

We introduce HALO, a Hardware-Aware Low critical-path-delay quantization framework that bridges the gap between quantization and DVFS by integrating them into a unified optimization strategy. HALO utilizes three main strategies to achieve this:
\begin{enumerate}
\item {Weight Sensitivity Analysis:} HALO performs sensitivity analysis to distinguish weights that are crucial for maintaining model accuracy from those that can tolerate more aggressive quantization. 
\item {Critical Weight Quantization and DVFS Alignment:} For the weight that are critical to accuracy, HALO selects quantization levels that are optimized for efficient circuit timing. This minimizes the critical path delays in MAC units, thereby allowing higher operating frequencies under DVFS and ensures that the high-precision components of the model can operate as efficiently as possible without compromising accuracy.
\item {Aggressive Quantization for Less Sensitive Weights:} For less critical weights, HALO applies more aggressive quantization while ensuring that selected quantization levels correspond to weight patterns that result in minimal critical path delays. This optimization enables more aggressive frequency scaling, reducing power consumption without significantly affecting model fidelity.
\end{enumerate}

This integrated approach yields substantial benefits in LLM inference. \textbf{include numbers} The improvements stem from HALO’s ability to co-optimize quantization levels with DVFS operating points, effectively bridging the disconnect between model compression and hardware adaptation.



\textbf{\textit{Complexity of quantizing LLMs.}} Quantization for LLMs is particularly complex because of the wide dynamic range of their weight distributions, often exhibiting Gaussian or Laplace-like pattern. 
These distributions include salient features and outliers.
%, as depicted in Fig.~\ref{fig}. 
Salient features, which constitute only 0.1\% to 1\% of the weights, are crucial for encoding essential model information and must be preserved to maintain performance. Outliers, on the other hand, are weights that deviate significantly from the mean, complicating naive quantization approaches. Effective quantization requires higher precision for both components, as even minor errors can lead to performance degradation. This highlights the necessity for adaptive strategies that address the sensitivities of weights within the model. %Furthermore, effective management of outliers is crucial to maintaining overall model performance, emphasizing the nuanced approach required for quantizing these models.

\textbf{\textit{Limitations of hardware-agnostic techniques.}} Despite their significance, most existing quantization techniques overly focus on reducing bit-width without considering hardware-specific factors, often treating critical components like Multiply-Accumulate (MAC) units as a black box. 
% overlooking how specific weight values impact circuit efficiency. In reality, certain weights activate fewer signal paths, causing variations in critical path delays and energy consumption. This variability is particularly pronounced in systolic arrays, such as those in TPUs, where performance is constrained by operations defined by critical-path weights. 
This overlooks how certain weight values can influence circuit efficiency and energy use, especially in architectures like systolic arrays (e.g., TPUs), where variations in critical-path delays and energy consumption emerge based on weight sensitivity.
Additionally, efficiently managing LLMs requires partitioning weights into tiles optimized for on-chip memory, enhancing data management and improving performance and energy efficiency. In weight-stationary dataflows, where weights remain fixed across input batches, fine-tuning these weight tiles is crucial for sustained efficiency. While approaches like PowerPruning~\cite{} have used energy-based pruning techniques along with repetitive model re-training to recover accuracy loss in CNNs, applying these methods to LLMs remains challenging due to their vast scale and sensitivity to weight distributions.

\begin{comment}
\begin{figure}[]
	\scriptsize
	\centering
	\includegraphics[width=\columnwidth]{images/framework.pdf}
	\caption{HALO quantization framework, using architectural details to yield Pareto-optimal trade-offs for diverse deployments.} 
	\label{fig:framework}
	%\vspace{-.5cm}
\end{figure}
\end{comment}

\begin{figure}[t!]
	\scriptsize
	\centering
	\includegraphics[width=\columnwidth]{images/framework_1.pdf}
	\caption{HALO quantization framework, using architectural details to yield Pareto-optimal trade-offs for diverse deployments.} 
	\label{fig:framework}
	%\vspace{-.5cm}
\end{figure}
\begin{comment}
\textbf{\textit{HALO is the answer.}} We introduce \textbf{HALO}, a timing-aware and weight-sensitive PTQ framework designed for LLMs. 
HALO employs a hybrid quantization approach: the small subset of sensitive weights is quantized uniformly to preserve precision, while the majority of weights, deemed less sensitive, are quantized non-uniformly. This strategy ensures high model accuracy while minimizing hardware overhead. 
HALO achieves this through three key strategies: (1) aligning quantization with circuit-level timing constraints to maximize performance, (2) tile-wise sensitivity analysis to dynamically enable selective frequency scaling for certain tiles, and (3) adaptive frequency scaling to enhance energy efficiency. Fig.~\ref{fig:framework} provides an overview of the framework, that incorporates architectural details, like tile sizes and MAC unit delay and energy profiles, to generate trade-off points for various deployment scenarios. This adaptable approach is applicable across a wide range of current and future accelerator architectures. 
\end{comment}
\textbf{\textit{HALO is the answer.}} We present \textbf{HALO}, a timing-aware and weight-sensitive PTQ framework for LLMs. HALO employs a hybrid quantization approach, uniformly quantizing a small subset of sensitive weights to maintain precision while using non-uniform quantization for the majority of less sensitive weights, based on circuit-level trends, to ensure high model accuracy. Dynamic Voltage and Frequency Scaling (DVFS), a vital feature in modern hardware, provides smooth, adaptive adjustments of both voltage and frequency to optimize performance and energy. With up to 181 configurations in NVIDIA GA100 GPUs and {\color{red}??} in TPUs, DVFS enables efficient computation. HALO leverages DVFS through three key strategies: (1) aligning quantization parameters with timing constraints to maximize operational efficiency; (2) conducting tile-wise sensitivity analysis to determine the optimal DVFS configuration for each tile; and (3) dynamically adjusting the frequency and voltage settings of system to optimize energy consumption while enhancing performance. Fig.~\ref{fig:framework} illustrates overview of HALO, that incorporates architectural details to derive trade-off points for various deployment scenarios. This adaptable approach is applicable across a wide range of current and future accelerator architectures. 

Below, we outline our main contributions.

\begin{itemize} 
\item \textbf{Timing and Energy Analysis}: Comprehensive evaluation of MAC circuit timing and energy variations. 
\item \textbf{HALO Quantization Framework}: A hardware-aware quantization method utilizing timing, energy, and weight sensitivity to boost LLM efficiency. 
\item \textbf{Architectural Integration}: Efficient integration techniques for HALO into systolic arrays and GPUs, enhancing performance while preserving accuracy.
\end{itemize}

The structure of the paper is as follows. Section~\ref{section:motivation} describes the motivation for this work. Sections~\ref{section:quantization} and~\ref{section:execution} provide an overview of the quantization framework and execution model. Section~\ref{section:evaluation} presents the experimental results, followed by Section~\ref{sec:relatedworks}, which reviews related research emphasizing our contributions. Finally, Section~\ref{section:conclusion} summarizes the conclusions.
