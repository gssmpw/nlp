\section{Quantization Framework}
\label{section:quantization}
\begin{comment}
Our quantization framework for accelerating large language model (LLM) inference introduces a novel, selective approach focused on reducing critical delay by prioritizing low-critical-delay weights. The design is optimized to maintain model accuracy while improving inference speed through adaptive quantization at multiple levels, including per-tile, per-channel, and layer sensitivity granularity. This methodology addresses computational efficiency challenges in high-dimensional LLMs by targeting only the weights that most impact inference delays.

\subsection{Uniform and Non-Uniform Quantization} 
% Previous studies have demonstrated that the primary goal of quantization techniques is to minimize the Mean Square Error (MSE) between the original and the quantized model.
Quantization is a powerful technique for reducing the memory footprint of neural network models by decreasing the bitwidth of their parameters. The literature on quantization can be broadly categorized into two main approaches: uniform and non-uniform methods.

\noindent \textbf{Uniform Quantization} Uniform quantization maps a continuous range of values to discrete levels that are evenly spaced. In a $b$-bit quantization scheme, the data $x$ is mapped to $2^b$ distinct levels. The quantization step $s$, also known as the scaling factor, is defined as:

\begin{equation}
s = \frac{x_{\text{max}} - x_{\text{min}}}{2^b}
\end{equation}

The quantization function $q(x)$ maps the data $x$ to the nearest quantization level, given by:
\begin{equation}
q(x) = x_{\text{min}} + is \quad \text{for} \quad i = 0, 1, 2, \ldots, 2^b-1
\end{equation}

This approach ensures that each quantized level is equidistant from its neighbors, resulting in a fixed-length representation for each interval. 

\noindent {\textbf{Non-uniform Quantization}} Non-uniform quantization, in contrast, allows for variable-sized intervals. Let $T = \{t_0 = -\infty, t_1, t_2, \ldots, t_{M-1}, t_M = +\infty\}$ be the set of thresholds that define the boundaries between quantization intervals. These thresholds partition the source range $X$ into $M+1$ disjoint regions $R_k = [t_{k-1}, t_k)$, for $k = 1, 2, \ldots, M+1$. 

The quantization function $q(x)$ assigns an input signal $x$ to the closest representation level $y_k$ based on which interval $R_k$ it falls into:
\begin{equation}
q(x) = y_k \quad \text{if} \quad x \in R_k
\end{equation}

The key difference is that the decision thresholds $(T)$ are not necessarily equally spaced, allowing for a more flexible allocation of quantization levels based on the data distribution within the source range $X$.
\end{comment}

Our quantization framework for LLM inference introduces a timing-aware strategy, prioritizing weights with low critical delays to reduce latency while preserving model fidelity. The adaptive method operates at multiple levels, per-tile, per-channel, and layer sensitivity, optimizing performance by targeting the weights most impactful to efficiency.


\subsection{Sensitivity-aware Quantization}

The framework begins with a sensitivity analysis of model weights, identifying weights values that can tolerate quantization without significantly affecting model accuracy. The framework initially separates \textit{outliers} (in blue) and \textit{salient weights} (in red) from normal values, as shown in Fig~\ref{}. 


\noindent \textbf{Outlier Detection \& Pruning:} We incorporate outlier removal to manage extreme weight values based on inter-quartile range scaling. To compute outliers in the weight distribution, we employ the 3$\sigma$ rule~\cite{}. Outliers are identified as values lying beyond three standard deviations from the mean. This helps in detecting and managing extreme values that may affect model performance. 

\noindent \textbf{Extremely-Salient Weights:} From the normal values obtained after this distribution, we rely on Taylor series expansion to estimate the most salient weights in the model. Following~\cite{squeezellm}, we use an approximation to the Hessian \(H \) based on the Fisher information matrix \( F \), which can be calculated over a sample dataset \( D \) as
\begin{equation}
F = \frac{1}{|D|} \sum_{d \in D} g_d g_d^{\top},
\end{equation}
where \( g \) is the gradient and \( H \approx F \). This only requires computing the gradient for a set of samples. For each weight tensor \( W \), the weight sensitivity is computed as:
\begin{equation}
\Lambda_{W} = F = \left( \frac{1}{|D|} \sum_{d \in D} g_d g_d^{\top} \right)
\end{equation}

Weights with higher \( \Lambda_{W} \) values are considered more salient due to their significant impact on the model's output. We preserve the top 0.05\% of the weights based on this criterion. Cumulatively, both outliers and extremely salient weight values correspond to less than 0.1\% of the total weight values. For this reason, we handle these weight values separately and apply per-channel quantization for this set of weight values, isolating them to maintain model precision.

\subsection{Critical Delay-aware Non-Uniform Quantization} 

Uniform Quantization discretizes continuous values into \(2^b\) evenly spaced levels. On the other hand, non-uniform quantization adapts to the data distribution using variable interval sizes defined by thresholds \( T \), which partition the input range into regions \( R_k = [t_{k-1}, t_k) \). Each region \( R_k \) is assigned a representation level \( y_k \), where \( y_k \) is the quantized value corresponding to data points within \( R_k \). This method efficiently allocates quantization levels to match the density of the data, enhancing precision for complex or skewed distributions.

\textcolor{red}{circle back to motivation section}

Weights impacting bottleneck layers are selectively quantized using delay-sensitive parameters, ensuring that speed-critical tasks (e.g., attention and projection layers) are optimized for efficient inference.

\noindent \textbf{Tile-Based Quantization with Adaptive Sensitivity:} To optimize the model for efficient inference on hardware, the weight tensors are divided into fixed-size tiles (e.g., \(128 \times 128\)). Each tile's sensitivity is evaluated using a modified approach based on the Fisher information matrix, which provides insight into the impact of each weight tile on model performance.

For a given tile \( T_i \), we compute a \textit{per-tile sensitivity score} \( \Lambda_{T_i} \) using a diagonal approximation of the Fisher information matrix:
\begin{equation}
\Lambda_{T_i} = \frac{1}{|T_i|} \sum_{(j,k) \in T_i} g_{jk}^2
\end{equation}

where \( g_{jk} \) represents the gradient of the loss with respect to each weight \( w_{jk} \) in tile \( T_i \). This score aggregates the Fisher information values over all weights in the tile, quantifying the sensitivity of that tile in terms of its impact on the model's output.

% Tiles with high sensitivity scores \( \Lambda_{T_i} \) are crucial for model accuracy, so we quantize these tiles with higher precision. In contrast, low-sensitivity tiles can tolerate more aggressive quantization, enabling greater memory savings without significant performance loss. This sensitivity-aware quantization approach helps balance computational efficiency and model accuracy, especially for deployment on high-performance hardware such as GPUs and TPUs.

\noindent \textbf{Adaptive Quantile Mapping}: An adaptive quantile function is used to select a quantization threshold that retains the most sensitive weights. The quantile threshold $k$ is computed based on the cumulative distribution of tile sensitivities, ensuring that a specified percentage (e.g., 95\%) of the sensitivity contribution is preserved. Tiles with sensitivity scores below this threshold are considered low-sensitivity and are quantized more aggressively. This approach dynamically adjusts $k$ based on the distribution of tile sensitivities across the model's weights, optimizing the trade-off between quantization error and model accuracy.

\noindent \textbf{Quantization Process by Sensitivity Levels}: Based on the computed quantile threshold, tiles are divided into low-sensitivity and high-sensitivity categories. Low-sensitivity tiles are subjected to higher quantization ratios, while high-sensitivity tiles are either minimally quantized or preserved at higher precision. This selective tiling approach achieves a balance between memory savings and inference speed without sacrificing model precision.

\begin{algorithm}
\caption{Sensitivity-Based Tile Quantization}
\begin{algorithmic}[1]
\begin{footnotesize}
\Require calibration dataset $X$, pre-trained weight matrix $W$, gradient $G$
\Require number of bits $n$, tile size $t$, quantile threshold $k$, target frequencies $f_1$, $f_2$
\Ensure quantized weight matrix $W_q$

\State $W_s, S \leftarrow \text{RemoveSensitiveValues}(W, G)$ \Comment{Remove values with high sensitivity}
\State $W_o, O \leftarrow \text{GenerateOutliers}(W_s)$ \Comment{Separate outlier weights}
\State $W_t \leftarrow \text{ReshapeIntoTiles}(\text{PadMatrix}(W_o, t), t)$ \Comment{Tile reshaping for parallel processing}

\State $T_s \leftarrow \text{CalculateTileSensitivities}(G)$ \Comment{Compute sensitivity for each tile}
\State $M_l, M_h \leftarrow \text{CreateMasks}(T_s, \text{ComputeAdaptiveK}(T_s, k))$ \Comment{Generate masks for low and high sensitivity tiles}

\For{each tile $i$ in $W_t$}
    \State $W_{l,i}, W_{h,i} \leftarrow W_{t,i} \odot M_{l,i}, W_{t,i} \odot M_{h,i}$ \Comment{Separate low and high sensitivity regions}
    \State $s_{l,i}, s_{h,i} \leftarrow \text{CalculateScale}(W_{l,i}, n), \text{CalculateScale}(W_{h,i}, n)$
    \State $W_{l,i} \leftarrow \text{SystolicQuantize}(W_{l,i} / s_{l,i}, f_1) \odot s_{l,i}$ \Comment{Quantize low-sensitivity weights}
    \State $W_{h,i} \leftarrow \text{SystolicQuantize}(W_{h,i} / s_{h,i}, f_2) \odot s_{h,i}$ \Comment{Quantize high-sensitivity weights}
    \State $W_{q,i} \leftarrow W_{l,i} + W_{h,i}$ \Comment{Reconstruct quantized tile}
\EndFor

\State $W_q \leftarrow \text{ReshapeFromTiles}(W_q) + \text{QuantizeRemaining}(O + S, n)$ \Comment{Final assembly of quantized matrix}
\State \Return $W_q$
\end{footnotesize}
\end{algorithmic}
\end{algorithm}

% \subsection{Algorithm Design}


% \subsection{Tile-Based Weight Mapping and Distribution}

% To optimize hardware deployment, the framework reshapes weight tensors based on tile sizes suited for GPU and TPU hardware. Each weight matrix is partitioned into fixed-size tiles (128 $\times$ 128 by default) and then quantized based on their sensitivity classification. This enables efficient batching of low-importance tiles on high-utilization GPUs, reducing computational load and minimizing memory transfers.
% \begin{itemize}

    % \item \textbf{Hardware Implementation:}     
    % Low-sensitivity tiles are mapped onto under-utilized GPUs in a multi-GPU setup, redistributing computational loads based on the memory and processing capacity of each device. This enhances model throughput and scales efficiently across GPUs, essential for real-time deployment.
% \end{itemize}

% \subsection{Model Optimization and Integration}

