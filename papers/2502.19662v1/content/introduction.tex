\section{Introduction}

%\textcolor{red}{\bf Tulika: I made a detailed pass. Please take care of my comments in overleaf chat}

\begin{comment}
Transformer-based large language models (LLMs) have experienced exponential growth, increasing 100-fold every two years, vastly outpacing the 3.1$\times$ advancements in hardware over the same period. This growing disparity has made inference prohibitively expensive, as exemplified by models like LLaMA~\cite{} with 65 billion parameters and GPT-4~\cite{} with 1.76 trillion parameters, which demand substantial computational resources~\cite{llm_energy}. Adding to this challenge is the fragmented landscape of hardware accelerators, marked by an increase in specialized designs optimized for distinct LLM quantization techniques and specific data types. Quantization, which reduces model size and computational cost by lowering the bit-width of weights and activations, is essential for addressing these efficiency demands. While these accelerators excel for certain quantization schemes, they lack the versatility needed for broader deployment on mainstream platforms like TPUs and GPUs. Consequently, there is a critical need for flexible, hardware-aware quantization strategies that can accommodate the diversity of current and emerging hardware architectures.
\end{comment}

%{\bf Tulika: May consider updating the title to HALO: Hardware-aware quantization for Energy-Efficient LLM Acceleration to make it more general}

Transformer-based large language models (LLMs) have grown exponentially, increasing 100-fold every two years, far outpacing the 3.1Ã— improvements in hardware. This widening gap has made inference increasingly costly, as seen with models like LLaMA (65 billion parameters) and GPT-4 (1.76 trillion parameters), which require vast computational resources. Quantization, which reduces model size and cost by lowering bit-width, is crucial for efficiency. However, implementing quantization is challenging because of the diverse, fragmented landscape of hardware accelerators, each optimized for specific quantization techniques and data types. Existing quantization techniques fail to account for these diverse hardware, highlighting the need for adaptable strategies that can optimize model efficiency across different accelerator architectures by leveraging circuit-level characteristics.


%{\bf Tulika: Trying to connect to DVFS early on through the next few paragraphs. Please modify}
\textbf{{Limitations of hardware-agnostic techniques.}} 
%Quantization has emerged as an important technique to reduce the size and computational cost of models by decreasing the bit-width of their weights and activations. However, traditional quantization approaches solely focus on reducing bit-widths without considering their impact on circuit-level hardware performance. 
Most existing quantization techniques~\cite{10705489} overly focus on reducing bit-width without considering hardware-specific factors, often treating critical components like Multiply-Accumulate (MAC) units as a black box.
In ML accelerators like TPUs, MAC units used for matrix multiplications central to LLM inference, occupy 77-80\% of the chip area and account for 50-89\% of total power consumption~\cite{flextpu}. Our key observation is that MAC units exhibit significant variability in performance and power characteristics, depending on specific weight patterns after quantization. Through detailed circuit analysis, we discover that certain weight configurations enable shorter critical paths, allowing the MAC units to operate at higher frequencies, whereas others result in longer critical paths that constrain the system's overall operating speed (as illustrated in Fig.\ref{fig:frequency_trend} and Fig.\ref{fig:power_trend}). This variability highlights an unexplored opportunity to enhance inference performance and energy efficiency through more sophisticated, hardware-aware weight quantization strategies. Yet, current techniques remain oblivious to this crucial quantization-hardware interplay.

\begin{figure}[t!]
	\scriptsize
	\centering
	\includegraphics[width=\columnwidth]{images/framework_2.pdf}
	\caption{HALO quantization framework, using architectural details to yield Pareto-optimal trade-offs for diverse deployments.} 
	\label{fig:framework}
	%\vspace{-.5cm}
\end{figure}

\textbf{{HALO is the answer.}}
Hardware accelerators for LLMs, such as GPUs, use Dynamic Voltage and Frequency Scaling (DVFS) to balance performance and power. NVIDIA GA100 GPUs, for instance, support up to 181 DVFS configurations~\cite{gpu_dvfs_181}. Quantization directly influences the critical-path delays in MAC units, which in turn determines how well DVFS can be utilized to optimize both performance and energy efficiency. Our key insight is that by carefully selecting quantization levels that correspond to favorable critical-path delays, we can enable higher operating frequencies while maintaining model accuracy.

We present \emph{HALO}, Hardware-Aware LOw critical-path delay quantization framework, as illustrated in Fig.\ref{fig:framework}, which seamlessly integrates quantization and DVFS into a unified optimization strategy.
HALO takes hardware specifications and design objectives as inputs and outputs Pareto-optimal quantized models with optimized DVFS schedules for deployment on target accelerators.
HALO employs three key approaches to achieve this:
\begin{enumerate}
\item Weight Sensitivity Analysis: HALO analyzes weight sensitivity to identify critical weights essential for maintaining model accuracy and those that can withstand aggressive quantization.
\item Sensitivity-aware Uniform Quantization and DVFS Alignment: For weights essential to model accuracy, HALO selects quantization levels with a broader dynamic range to ensure precision. DVFS frequencies are configured so that high-precision components operate efficiently, optimizing performance while preserving accuracy.
\item Critical-path delay aware Non-Uniform Quantization: HALO applies aggressive quantization to less sensitive weights, ensuring chosen levels correspond to favorable weight patterns that keep critical path delays minimal. This strategy supports more aggressive frequency scaling, significantly reducing power consumption with minimal impact on model fidelity.
\end{enumerate}

This integrated approach delivers significant benefits for LLM inference, achieving on average 270\% performance gains and 51\% energy savings. These improvements are driven by HALO's ability to co-optimize quantization levels with DVFS operating points, effectively bridging the gap between model compression and hardware adaptation.
Below, we outline our main contributions.

\begin{itemize} 
\item \textbf{Timing and Energy Analysis}: Comprehensive evaluation of MAC circuit timing and energy variations. 
\item \textbf{HALO Quantization Framework}: A hardware-aware quantization method utilizing timing, energy, and weight sensitivity to boost LLM efficiency. 
\item \textbf{Architectural Integration}: Efficient integration techniques for HALO into systolic arrays and GPUs, enhancing performance while preserving accuracy.
\end{itemize}
The paper is structured as follows: Sec.~\ref{section:motivation} outlines the motivation, Sec.~\ref{section:quantization} details the quantization framework and DVFS strategy, Sec.~\ref{section:evaluation} presents experimental results, Sec.~\ref{sec:relatedworks} reviews related work, and Sec.~\ref{section:conclusion} concludes the study.
%The structure of the paper is as follows. Section~\ref{section:motivation} describes the motivation for this work. Section~\ref{section:quantization} provides an overview of the quantization framework and execution model. Section~\ref{section:evaluation} presents the experimental results, followed by Section~\ref{sec:relatedworks}, which reviews related research emphasizing our contributions. Finally, Section~\ref{section:conclusion} summarizes the conclusions.

\begin{comment}
\textbf{\textit{Complexity of quantizing LLMs.}} Quantization for LLMs is particularly complex because of the wide dynamic range of their weight distributions, often exhibiting Gaussian or Laplace-like pattern. 
These distributions include salient features and outliers.
%, as depicted in Fig.~\ref{fig}. 
Salient features, which constitute only 0.1\% to 1\% of the weights, are crucial for encoding essential model information and must be preserved to maintain performance. Outliers, on the other hand, are weights that deviate significantly from the mean, complicating naive quantization approaches. Effective quantization requires higher precision for both components, as even minor errors can lead to performance degradation. This highlights the necessity for adaptive strategies that address the sensitivities of weights within the model. %Furthermore, effective management of outliers is crucial to maintaining overall model performance, emphasizing the nuanced approach required for quantizing these models.

\textbf{\textit{Limitations of hardware-agnostic techniques.}} Despite their significance, most existing quantization techniques overly focus on reducing bit-width without considering hardware-specific factors, often treating critical components like Multiply-Accumulate (MAC) units as a black box. 
% overlooking how specific weight values impact circuit efficiency. In reality, certain weights activate fewer signal paths, causing variations in critical path delays and energy consumption. This variability is particularly pronounced in systolic arrays, such as those in TPUs, where performance is constrained by operations defined by critical-path weights. 
This overlooks how certain weight values can influence circuit efficiency and energy use, especially in architectures like systolic arrays (e.g., TPUs), where variations in critical-path delays and energy consumption emerge based on weight sensitivity.
Additionally, efficiently managing LLMs requires partitioning weights into tiles optimized for on-chip memory, enhancing data management and improving performance and energy efficiency. In weight-stationary dataflows, where weights remain fixed across input batches, fine-tuning these weight tiles is crucial for sustained efficiency. While approaches like PowerPruning~\cite{} have used energy-based pruning techniques along with repetitive model re-training to recover accuracy loss in CNNs, applying these methods to LLMs remains challenging due to their vast scale and sensitivity to weight distributions.

\begin{figure}[t!]
	\scriptsize
	\centering
	\includegraphics[width=\columnwidth]{images/framework_1.pdf}
	\caption{HALO quantization framework, using architectural details to yield Pareto-optimal trade-offs for diverse deployments.} 
	\label{fig:framework}
	%\vspace{-.5cm}
\end{figure}
\ begin{comment}
\textbf{\textit{HALO is the answer.}} We introduce \textbf{HALO}, a timing-aware and weight-sensitive PTQ framework designed for LLMs. 
HALO employs a hybrid quantization approach: the small subset of sensitive weights is quantized uniformly to preserve precision, while the majority of weights, deemed less sensitive, are quantized non-uniformly. This strategy ensures high model accuracy while minimizing hardware overhead. 
HALO achieves this through three key strategies: (1) aligning quantization with circuit-level timing constraints to maximize performance, (2) tile-wise sensitivity analysis to dynamically enable selective frequency scaling for certain tiles, and (3) adaptive frequency scaling to enhance energy efficiency. Fig.~\ref{fig:framework} provides an overview of the framework, that incorporates architectural details, like tile sizes and MAC unit delay and energy profiles, to generate trade-off points for various deployment scenarios. This adaptable approach is applicable across a wide range of current and future accelerator architectures. 
\ end{comment}
\textbf{\textit{HALO is the answer.}} We present \textbf{HALO}, a timing-aware and weight-sensitive PTQ framework for LLMs. HALO employs a hybrid quantization approach, uniformly quantizing a small subset of sensitive weights to maintain precision while using non-uniform quantization for the majority of less sensitive weights, based on circuit-level trends, to ensure high model accuracy. Dynamic Voltage and Frequency Scaling (DVFS), a vital feature in modern hardware, provides smooth, adaptive adjustments of both voltage and frequency to optimize performance and energy. With up to 181 configurations in NVIDIA GA100 GPUs and {\color{red}??} in TPUs, DVFS enables efficient computation. HALO leverages DVFS through three key strategies: (1) aligning quantization parameters with timing constraints to maximize operational efficiency; (2) conducting tile-wise sensitivity analysis to determine the optimal DVFS configuration for each tile; and (3) dynamically adjusting the frequency and voltage settings of system to optimize energy consumption while enhancing performance. Fig.~\ref{fig:framework} illustrates overview of HALO, that incorporates architectural details to derive trade-off points for various deployment scenarios. This adaptable approach is applicable across a wide range of current and future accelerator architectures. 
\end{comment}
