\section{Related Works}
\label{sec:relatedworks}
\begin{comment}
\textbf{Dynamic Voltage and Frequency Scaling (DVFS)}
DVFS is widely used in CPUs and GPUs for power management (e.g., via NVIDIA’s nvidia-smi). Recent studies have explored multi-rail voltage supplies~\cite{multirail_1, multirail_2} and integrated voltage regulators~\cite{regulator_1, regulator_2}, enhancing energy efficiency. In GPUs, Qing et al.\cite{gpu_concurrent_kernels} developed models to optimize kernel combinations for energy efficiency, while Guerreiro et al.\cite{gpu_performance_models} created application-specific DVFS settings. GreenMM~\cite{greenmm} targets energy savings in matrix multiplication with undervolting, and NVIDIA's GALS-based design~\cite{nvidia_gals} introduces finer power control for GPU accelerators. For accelerators, platforms like Blades~\cite{blades} optimize voltage and frequency based on data characteristics, and DESCNet~\cite{descnet} applies sector-level power-gating in DNN accelerators. Unlike existing techniques, HALO uses DVFS to dynamically adjust frequency based on weight sensitivity, optimizing energy efficiency without relying on complex hardware modifications.
\end{comment}
\noindent \textbf{Quantization Accelerators.}
Several approaches aim to handle quantization in neural networks, particularly for LLMs. GOBO~\cite{gobo} and OlAccel~\cite{olaccel} use mixed precision to maintain accuracy, while Bitfusion~\cite{bitfusion} dynamically adapts precision levels to model needs. However, many focus on traditional DNNs, which are easier to quantize than LLMs. Methods like GOBO rely on full-precision compute units, while OlAccel uses coordinate lists for outliers.
%HALO’s approach can be combined with these strategies to enhance performance without disrupting existing architectures. 
%{\bf Peh: I think u shd highlight that your technique is orthogonal. In the future, you should show a result that combines mixed precision at the application level + your hw aware technique.}
HALO introduces a framework that is orthogonal to existing strategies and integrates seamlessly with accelerators. When applied to mixed-precision accelerators equipped with encoder/decoder components, this not only boosts performance but also reduces DRAM accesses by 59.06\%, thereby enhancing overall system efficiency.

\noindent \textbf{Quantization Methods.}
Post-training quantization (PTQ) methods like GPTQ~\cite{gptq} and AWQ~\cite{awq} focus on reducing LLM weights to 3 or 4 bits while preserving critical weight precision. Techniques such as outlier-aware quantization~\cite{owq} and SqueezeLLM~\cite{squeezellm} distinguish between sensitive and non-sensitive weights. Recent methods like SmoothQuant~\cite{smoothquant} shift quantization challenges to weights, allowing both weights and activations to be compressed to 8 bits. HALO builds on these techniques to maintain LLM accuracy by leveraging underlying hardware DVFS.% while optimizing performance.
% {\color{red}cite minifloats\cite{10705489}}

\begin{comment}
\subsection{Dynamic Voltage and Frequency Scaling (DVFS)}
Employing DVFS is a common practice to manage power usage in processors such as CPUs and GPUs (e.g.,  nvidia-smi by Nvidia). 
Various studies have explored DVFS with multi-rail voltage supplies~\cite{multirail_1, multirail_2} and fully-integrated voltage regulators~\cite{regulator_1, regulator_2, regulator_3}, aiming to improve power management.

\textbf{DVFS for GPUs.}
Qing et al.~\cite{gpu_concurrent_kernels} leverage the capability of modern GPUs to execute multiple concurrent kernels. They developed power-performance models to determine optimal kernel combinations for GPU deployment.
Guerreiro et al.~\cite{gpu_performance_models} categorized applications based on their sensitivity to DVFS. They created representative models for each category and used these to estimate optimal DVFS settings.
GreenMM~\cite{greenmm} focuses specifically on reducing energy consumption in matrix multiplication, a critical operation in AI workloads. The approach combines GPU undervolting with Algorithm-Based Fault Tolerance (ABFT) to achieve energy savings while maintaining computational integrity under aggressive undervolting conditions.
NVIDIA has developed a spatial accelerator incorporating Globally Asynchronous Locally Synchronous (GALS) domains~\cite{nvidia_gals}. This design, enabled by bisynchronous FIFOs, allows for more granular power management across different accelerator components, improving overall power efficiency.

\textbf{DVFS for Accelerators.}
Blades~\cite{blades} introduces an automated platform for selecting optimal voltage and frequency values based on input data characteristics.
Ultra-elastic CGRAs~\cite{uecgra} implements fine-grained DVFS to accelerate inter-iteration loop dependencies, utilizing a novel ratiochronous clocking scheme for low-latency crossings in the interconnect.
DESCNet~\cite{descnet} proposes partitioning scratchpad memory (SPM) into multiple sectors and applies sector-level power-gating for DNN accelerators during inference, improving energy efficiency.

While fine-grained DVFS techniques offer benefits, they often incur significant hardware overhead. Some research explores novel, non-traditional approaches to avoid these overheads, but these can be challenging to implement in modern accelerators.
HALO is the first work in leveraging DVFS to surpass the worst-case critical path of the circuit based on weight values, without relying on complex circuit techniques.

\subsection{Quantization Accelerators.}
Several methods have been proposed to tackle the challenges of quantization in deep neural networks, especially for LLMs. 
GOBO~\cite{gobo}, OlAccel~\cite{olaccel}, and DRQ~\cite{drq} introduce mixed precision quantization techniques that maintain higher precision for important weights while using lower precision for the rest.
This approach can, however, lead to random and unaligned memory access patterns.
Bitfusion~\cite{bitfusion} and DRQ support mixed precision through the spatial and temporal combination of low-bit PEs, respectively.
Bitfusion's dynamic bit-level fusion technique adapts to each layer's requirements. 
However, their results mainly focus on traditional DNN models, which are generally easier to quantize than LLMs.

GOBO quantizes only weights and stores compressed weights in off-chip memory, still relying on full precision FP16/FP32 compute units of GPUs.
For outlier storage, GOBO and OlAccel use coordinate lists to indicate outlier locations in the matrix. 
BiScaled DNNs~\cite{biscaleddnn} employ block sparse indices, while DRQ utilizes direct bitmaps for outliers.

ANT~\cite{ant} proposes a fixed-length adaptive data type to achieve low-bit quantization. 
Olive~\cite{olive} observes the sparsity of outlier values and prunes normal values adjacent to outliers, allowing higher precision encoding of outliers. 
Both methods introduce new data types requiring specialized and complex MAC unit designs, which are challenging to incorporate into current GPU and TPU architectures.

HALO offers an orthogonal approach that can be integrated with any of these methods to speed up execution time further and improve overall performance. 

\subsection{Quantization Methods}
GPTQ~\cite{gptq} is a post-training quantization (PTQ) method that compresses LLM weights to 3 or 4 bits instead of the usual 8 bits. 
It employs layer-wise quantization with Optimal Brain Quantization (OBQ) to update weights using inverse Hessian information.
AWQ~\cite{awq} identifies a small subset of weights, called salient weights, that are crucial for minimizing quantization loss. By preserving these salient weights in high precision, AWQ achieves better performance than GPTQ. 
Similarly, outlier-aware quantization~\cite{owq} and SPQR~\cite{spqr} highlights that a few weights are sensitive to activation outliers and assign them high precision.
SqueezeLLM~\cite{squeezellm} separates sensitive and non-sensitive weights into dense and sparse decompositions.
Recently, SmoothQuant~\cite{smoothquant} proposed shifting the quantization difficulty from activations to weights, achieving lossless quantization of both weights and activations to 8 bits.

HALO leverages these insights, building on them to maintain the original accuracy of LLMs.
\end{comment}