\section{Evaluation}
\label{section:evaluation}
\begin{comment}
In this section, we evaluate the accuracy of varied LLM models with HALO quantization.
We also demonstrate HALO's speedup, energy efficiency, and area overhead over systolic arrays and GPUs.
\end{comment}
This section evaluates the accuracy of LLM models with HALO quantization and demonstrates its speedup and energy efficiency compared to systolic arrays and GPUs.
%{\bf Peh: I do not think DVFS should be considered as area overhead, since the hardware should already support DVFS}

\subsection{Implementation Details}
\begin{comment}
To assess the Systolic array based accelerator’s performance, we develop a specialized simulator  and implement the Systolic array design, featuring a global DVFS unit, in Verilog. Energy measurements are obtained by synthesizing the design in GlobalFoundries 22nm technology using topographical mode, incorporating parasitics for routing overhead. The implementation is verified through behavioral simulations.

For GPU integration and evaluation, we modified and extended AccelSim with the configuration of the NVIDIA 2080 Ti architecture. Energy estimation was performed using AccelWattch, GPUWattch, and CACTI. Since most Transformer layers rely heavily on matrix multiplication, we used CUTLASS, NVIDIA's open-source library for efficient matrix operations.
\end{comment}
% {\color{red} Shivam on quantization framework}
\noindent \textbf{Datasets and Models.}
We evaluate the effectiveness of HALO using various models, including LLaMA2~\cite{llama2} and OPT~\cite{opt} family of models.
%, and Mistral~\cite{mistral}. 
We conduct language modeling evaluation using the C4~\cite{c4} and WikiText2~\cite{wikitext} datasets. We report \textit{perplexity} as the measure of the performance of the model. 

\noindent \textbf{Hyperparameters \& Baselines.} We evaluate our approach with state-of-the-art quantization methods, including SmoothQuant~\cite{smoothquant} and Round-To-Nearest (RTN) quantization, under varying weight precision with activations fixed at 8 bits. Quantization is applied to computationally intensive operators such as attention and linear layers, using per-channel quantization for weights and per-token static quantization for activations. For SmoothQuant, original smoothing factors for activation scaling are used. Additionally, we retain 0.45\% of values, comprising 0.05\% sensitivity-critical values and 0.4\% outliers, with sensitivity measured using 100 random samples from the C4 training set.

\begin{comment}\noindent \textbf{Hardware Setup.} To evaluate the performance of the systolic array, we develop a custom simulator and implement the design, including a global DVFS unit, in Verilog. The design is synthesized in a commercial 22nm technology using topographical mode, incorporating parasitics for routing overhead. The implementation is verified through behavioral simulations. 
For GPU evaluations, we extend AccelSim~\cite{accelsim} to model the NVIDIA 2080 Ti architecture and run it using the DVFS configurations from Table~\ref{tab:dvfs_levels}. For energy estimation, we use AccelWattch~\cite{accelwattch} and GPUWattch~\cite{gpuwattch}.
%For GPU integration, we extend AccelSim~\cite{accelsim} to model the NVIDIA 2080 Ti architecture and use DVFS configurations from Table\ref{tab:dvfs_levels} AccelWattch~\cite{accelwattch}, GPUWattch~\cite{gpuwattch} for energy estimation. 
As matrix multiplication is central to Transformer layers, we employ NVIDIA’s CUTLASS~\cite{cutlass} library for optimized matrix operations.
\end{comment}

\noindent \textbf{Hardware Setup.} To evaluate performance of the systolic array, we develop a custom simulator and implement the design, including a global DVFS unit, in SystemVerilog. The design is synthesized in a 22nm commercial technology using topographical mode with parasitics for routing overhead and verified through behavioral simulations. For GPU evaluations, we extend AccelSim~\cite{accelsim} to model the NVIDIA 2080 Ti architecture using the DVFS configurations from Table~\ref{tab:dvfs_levels}. Energy estimations are performed using AccelWattch~\cite{accelwattch} and GPUWattch~\cite{gpuwattch}.

\begin{comment}
Our proposed framework seamlessly integrates with PyTorch, applying custom quantization through transformer-specific functions (e.g., \texttt{quantize\_llama\_like} for LLaMA models and \texttt{quantize\_opt} for OPT models). Key functions include:
\begin{itemize}
    \item \textbf{Quantization Function Variants:} Functions such as \texttt{quantize\_weight\_per\_tile\_sensitivity\_absmax} and \texttt{quantize\_weight\_per\_channel\_absmax} provide tailored quantization for different weight distributions.
    \item \textbf{Caching and On-Demand Scaling Adjustments:} Sensitive weights and cached scale factors are dynamically adjusted based on threshold requirements, ensuring adaptability in high-throughput scenarios.
\end{itemize}

This modular framework provides a critical-delay-focused, sensitivity-aware approach to quantization, enabling efficient acceleration of LLM inference without compromising model accuracy.
\end{comment}

% Our proposed framework integrates seamlessly with PyTorch, offering custom quantization through transformer-specific functions. 

% For instance, \texttt{\small quantize\_llama\_like} is tailored for LLaMA models, while \texttt{\small quantize\_opt} is designed for OPT models. The framework provides several key functionalities:

% \begin{itemize} 
% \item \textbf{Quantization Function Variants:} To accommodate diverse weight distributions, functions such as \texttt{\small quantize\_weight\_per\_tile\_sensitivity\_absmax} and \texttt{\small quantize\_weight\_per\_channel\_absmax} apply quantization at different granularities, targeting either tile-level sensitivity or channel-level maximum values for effective precision control. 
% \item \textbf{Adaptive Caching and Scaling Mechanisms:} Sensitive weights and cached scale factors are dynamically adjusted based on threshold requirements, ensuring adaptability in high-throughput scenarios. 
% \end{itemize}

% This modular, sensitivity-aware framework emphasizes critical-delay optimization, facilitating efficient quantization that accelerates LLM inference while preserving model accuracy.

\subsection{Accuracy Results}
\begin{comment}
\begin{table*}[h!]
\centering
\resizebox{0.9\textwidth}{!}{
\begin{tabular}{clcc|c|cc|ccccc}
\hline
\multirow{2}{*}{PPL$\downarrow$} & & \multicolumn{2}{c|}{Llama-2} & Mistral & \multicolumn{2}{c|}{Falcon} & \multicolumn{5}{c}{OPT}\\
\cline{3-12}
 & & 7B & 13B & 7B & 7B& 40B&  1.3B&2.7B& 6.7B&13B &30B\\
\hline
Ideal& FP16& 5.47& 4.95& 5.253& 6.59& 5.228&   14.72&12.47&  10.86&10.12&9.56\\
\hline
\multirow{3}{*}{\shortstack{Round to Nearest (RTN)\\WxA8}}& x = 8& 5.57& 4.94& & & &   15.22&&  &&\\
 & x = 4& 7.35& 5.46& & & & 81.23& 122.51& 350.29& 5835.78&3717.11\\& x = 3& 19480.50& 2552.73& & & &   13477.29&33606.98&  16200.57&17547.32&9122.24\\
\hline
\multirow{3}{*}{\shortstack{Smoothquant\\WxA8}}& x = 8& 5.51& 4.92& & & &   &&  &&\\
 & x = 4& 266.28& 1342.26& & & &   41.78&5948.06&  9472.94&7413.02&\\
 & x = 3& 50327.61& 14314.63& & & & 12958.09& 12352.28& 8606.94& 9411.82&\\\hline
 \multirow{3}{*}{HALO (Tile=128)}& Perf-opt& 6.37& 5.47& & 7.26&  &  16.92&13.61&  11.54&10.84&9.95\\
 & Acc-opt& 5.94& 5.20& & & & 15.59& 12.98& 11.15& 10.58&9.71\\
 & Bal& 6.01& 5.44& & & & 16.06& 13.15& 11.16& 10.73&9.84\\
\hline
\end{tabular}
}
\caption{Accuracy of LLMs on the WikiText~\cite{wikitext} dataset. The metric here is perplexity - lower values indicate better accuracy.}
\label{tab:llm}
\end{table*}
\end{comment}

\begin{table*}[ht]
\centering
\resizebox{0.9\textwidth}{!}{
\begin{tabular}{clcccc|cccc}
\hline
\multirow{2}{*}{PPL$\downarrow$}  && \multicolumn{4}{c|}{WikiText} & \multicolumn{4}{c}{C4} \\
 && Llama2-7B& Llama2-13B& OPT-1.3B& OPT-30B& Llama2-7B& Llama2-13B& OPT-1.3B& OPT-30B\\\hline
Ideal& FP16& 5.47& 4.95& 14.72& 9.56& 7.52& 6.98& 16.96& 11.84\\\hline
\multirow{3}{*}{\shortstack{RTN\\WxA8}}  &x = 8& 5.58& 4.94& 15.23& 9.89& 7.69& 7.04& 17.65 & 12.35\\
&x = 4& 7.36& 5.47& 81.23& 3717.12& 10.23& 7.71& 76.94& 4162.58\\
&x = 3& 19480.51& 2552.73& 13477.29& 9122.24& NaN& 1951.28& 6719.15& 6136.87\\\hline
\multirow{3}{*}{\shortstack{SmoothQuant\\WxA8}}  &x = 8& 5.51& 4.93& 14.76& 9.61& 7.56& 7.04& 16.52& 12.03\\
&x = 4& 7.26& 5.64&18.47& 14.44& 10.16& 8.00& 20.35& 31.10\\
&x = 3& 8406.97& 572.53& 3780.82& 1074.64& NaN& 1145.67&  1188.41& 1116.33\\\hline
\multirow{3}{*}{\shortstack{HALO (Tile=128)}}  &Perf-opt (BW)& 6.37 (3.03)& 5.47 (3.03)& 16.92 (3.08)& 9.95 (3.04)& 8.87& 7.78& 18.43& 12.24\\
&Acc-opt (BW)& 5.94 (3.88)& 5.20 (3.80)& 15.59 (3.96)& 9.71 (3.75)& 8.23& 7.40& 17.29& 12.05\\
&Bal (BW)& 6.01 (3.75)& 5.44 (3.06)& 16.06 (3.82)& 9.84 (3.42)& 8.34& 7.73& 17.68& 12.13\\
HALO (Tile=64)&Bal (BW)& 5.89 (3.62)& 5.31 (3.05)& 15.82 (3.64)& 9.76 (3.40)& 8.31& 7.50& 17.44& 12.10\\
HALO (Tile=32)& Bal (BW)& 5.63 (3.33)& 5.01 (3.03)&  15.56 (3.50)& 9.62 (3.24)& 8.04& 7.19& 17.08&12.05\\\hline
\end{tabular}
}
\caption{LLM accuracy on WikiText~\cite{wikitext} and C4~\cite{c4}, measured by perplexity with a sequence length of 2048. Lower values indicate higher accuracy. For HALO, approximate weight bit-width values are reported in brackets alongside perplexity.}
\label{tab:llm}
\vspace{-.5cm}
\end{table*}

\begin{comment}
Our evaluation benchmarks HALO against 16-bit floating-point (FP16) precision, where both weights and activations are represented in FP16, and integer quantization schemes denoted as WxA8. In these configurations, WxA8 signifies 8-bit integer activations and x-bit integer weights, where x equals 8, 4, or 3. 
We compare our method to Round to Nearest (RTN), which often incurs accuracy loss at lower bit widths, and SmoothQuant, which redistributes quantization difficulty to mitigate errors. 
As detailed in Table\ref{tab:llm}, HALO consistently achieve accuracy comparable to FP16, with perplexity increases kept under 0.5 across all models.
Our comparisons specifically include W3A8 and W4A8 baselines because they align with the quantization levels in our design. The MAC characteristics in Sec.\ref{section:motivation} reveal that aggresive quantization of low- and high-sensitivity tiles group weights into buckets containing only 9 and 16 weights, making these baselines particularly relevant for evaluating our approach. These configurations allow for a more direct comparison to HALO's quantization approach, as they effectively match the weight partitioning in our framework.
We introduce three variants of HALO: \textit{perf-opt}, which prioritizes performance at the cost of higher perplexity (lower accuracy); \textit{acc-opt}, which preserves accuracy close to FP16 levels; and \textit{bal}, which offers a balanced trade-off between performance and accuracy. Results are provided across various tile sizes of the systolic array architecture, as discussed in Sec.\ref{sec:tile_sizes}, underscoring HALO's ability to balance efficiency and model fidelity effectively.
\end{comment}
Our evaluation benchmarks HALO against 16-bit floating-point (FP16) precision, with both weights and activations in FP16, and integer quantization schemes, denoted as WxA8. Here, WxA8 represents 8-bit integer activations and x-bit integer weights, where x is 8, 4, or 3. We compare our method to Round to Nearest (RTN), which often loses accuracy at lower bit widths, and SmoothQuant, which redistributes quantization difficulty to reduce errors. As shown in Table~\ref{tab:llm}, HALO consistently achieves accuracy comparable to FP16, with perplexity increases under 0.5 across models. We specifically include W3A8 and W4A8 baselines because they align with our quantization levels. The MAC characteristics in Sec.\ref{section:motivation} indicate that aggressive quantization of low- and high-sensitivity tiles groups weights into buckets of 9 and 16, making these baselines relevant for evaluating HALO's approach. We introduce three HALO variants: \textit{perf-opt} for performance, \textit{acc-opt} for preserving accuracy close to FP16 level, and \textit{bal} for a balanced trade-off. Results across various systolic array tile sizes in Sec.\ref{sec:tile_sizes} highlight HALO's balance of efficiency and model fidelity.

%\textbf{MMLU Benchmarking}

\subsection{Systolic Array Performance and Energy}
\begin{figure}[t!]
	\scriptsize
	\centering
	\input{graphs/performance_sys}
	\caption{Normalized execution time across quantization methods. Lower values denote faster execution, emphasizing HALO's efficiency.} 
	\label{fig:performance_sys}
	%\vspace{-.5cm}
\end{figure}

\begin{figure}[t!]
	\scriptsize
	\centering
	\input{graphs/pareto_frontier}
    \vspace{-0.2cm}
	\caption{Normalized performance vs. perplexity, showing the knee point for optimal efficiency-accuracy trade-off.} 
	\label{fig:pareto_frontier}
	%\vspace{-.5cm}
\end{figure}

\begin{figure}[t!]
	\scriptsize
	\centering
	\input{graphs/energy_sys}
	\caption{Normalized energy consumed across quantization methods.} 
	\label{fig:energy_sys}
\end{figure}

Fig.\ref{fig:performance_sys} shows that HALO delivers significant latency speedup across all models, outperforming traditional methods like FP16 and W8A8, which struggle with outliers. HALO achieves 353\% improvement over FP16, 87\% over W8A8, 76\% over W4A8, and 74\% over W3A8, while maintaining better accuracy than W4A8 and W3A8. Fig.\ref{fig:pareto_frontier} highlights the trade-off between normalized performance and perplexity, with the knee point marking the optimal balance. The \textit{bal} variant aligns with this point, making it ideal for performance-critical LLM deployments.

%Fig.\ref{fig:energy_sys} presents energy consumption split into Static and Dynamic (Core, Buffer, and DRAM) components. {\bf Peh: Unclear how u split into static/dynamic} 
Fig.\ref{fig:energy_sys} shows energy consumption split into static (or idle energy consumption) and dynamic energy from switching in core, buffer, and memories.
FP16 incurs the highest energy usage due to uniform 16-bit operations, while W8A8 sees moderate reductions but lacks flexibility in adapting to weight sensitivity. W4A8 achieves better energy efficiency but rises slightly with model size, and W3A8 has the lowest energy consumption, though with minor fluctuations. HALO variants employ dynamic frequency scaling, optimizing energy efficiency based on weight sensitivity. This results in energy consumption only 12\% higher than W3A8 and 10\% higher than W4A8, with the \textit{bal} variant providing a superior performance-to-efficiency trade-off.

\subsection{Impact of Tile Size on System Efficiency}
\label{sec:tile_sizes}
\begin{figure}[t!]
	\scriptsize
	\centering
	\input{graphs/performance_tile_sizes}
	\caption{Normalized execution time of the systolic array for different tile sizes. HALO results are shown for the \textit{bal} configuration.} 
	\label{fig:tile_sizes}
	%\vspace{-.5cm}
\end{figure}

Fig.\ref{fig:tile_sizes} illustrates the performance of HALO with varying Systolic array tile sizes, presented as HALO-128, HALO-64, and HALO-32. We evaluate tile sizes of $128\times128$, $64\times64$, and $32\times32$, finding that smaller tiles, such as $32\times32$, achieve on average 15\% higher performance than $128\times128$ and 7\% higher than $64\times64$. This is because the framework quantizes more tiles into higher frequency ranges, enhancing efficiency. As shown in Table~\ref{tab:llm}, notably larger models exhibit better perplexity with smaller tile sizes, highlighting the importance of selecting optimal tile sizes for effective tile-wise quantization.

\vspace{-0.2cm}
\subsection{GPU Performance and Energy}
\begin{figure}[t!]
	\scriptsize
	\centering
	\input{graphs/performance_gpu}
	\caption{Normalized execution time on GPUs.} 
	\label{fig:performance_gpu}
\end{figure}

\begin{figure}[t !]
	\scriptsize
	\centering
	\input{graphs/energy_gpu}
	\caption{Normalized energy on GPUs.} 
	\label{fig:energy_gpu}
\end{figure}

As shown in Fig.\ref{fig:performance_gpu}, HALO on GPU outperforms \textit{W8A8} by effectively managing weight sensitivity, achieving consistent speedups across models through dynamic frequency adjustments that maximize performance.
Fig.\ref{fig:energy_gpu} shows the normalized energy comparison, considering constant, static, and dynamic power. Constant power refers to the energy consumed by the GPU’s peripheral circuitry, while dynamic power includes contributions from DRAM, caches, register file, and processing elements (CUDA and tensor cores).
Although \textit{W8A8} achieves the lowest normalized energy consumption, HALO\textit{(acc-opt)} and HALO\textit{(bal)} offer a more balanced energy profile. Meanwhile, HALO\textit{(perf-opt)} delivers higher performance at a modest energy cost. These results underscore HALO's ability to balance energy efficiency and performance based on specific optimization objectives.