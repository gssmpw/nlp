\section{Related Works}
\begin{figure*}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/method.pdf}
    \caption{Schematic diagram of our proposed method. Here $u\in\mathbb{R}^{l}$ represents each node. $\sigma$ denotes the sigmoid activation function. $\beta$ represents a learnable parameter to focus on the specific part.}
    \label{fig:method}
\end{figure*}
We categorize the existing GNNs for prediction or classification tasks into several groups by their main network architectures. Notably, more recent models often combine elements from multiple categories.

{\bf{GCN or message passing based}}. These methods leverage either spatial or spectral domain operations through message passing or graph convolutions. Key approaches include Graph Convolutional Networks (Kipf, "Semi-Supervised Classification with Graph Convolutional Networks") ____, Gate GCN ("Graph Attention Augmentation: Scoring Node Contexts for Better GNNs") ____, Graph Isomorphism Networks (Maron, "Query-Embedding with CNN and RNN") ____, Graph Attention Networks (Velickovic, "Graph Attention Networks") ____, GraphSAGE (Hamilton, "Inductive Representation Learning on Large Graphs") ____, and Principal Neighborhood Aggregation (Morris, "Weisfeiler-Lehman Neural Graph Embeddings") ____. While efficient and scalable, these models typically have limited ability to capture LRD. Moreover, standard GCNs have limited expressiveness, as they are equivalent to the 1-Weisfeiler-Lehman (WL) test, whereas higher-order k-GNNs are proven to be more expressive ("Beyond Simple Scattering: Higher-Order Graph Neural Networks") ____.

{\bf{Kernel based}}. Graph kernel methods have been extensively studied, including neural tangent kernel ("On Exact Computation with a Generative Model for the Entropy of Discrete Random Variables and Rebar") ____, graph neural tangent kernel ("Learning Overparameterized Neural Tangent Kernels via Gradient Descent on Wide Deep Networks") ____,
graph kernels with neural networks ("Deep Learning for Graphs: A Tutorial") ____, and spectral kernel learning ("Spectral Clustering with the Help of Support Vector Machines") ____. These methods offer theoretical guarantees through strong mathematical foundations from kernel theory, particularly in terms of provable convergence properties.
However, they face several challenges in adaptability to hierarchical structures, capturing complex patterns, and incorporating node/edge features. Thus, they may have limited representation power. 

{\bf{Transformer or SSM based}}. These recent models leverage Transformers or State Space Models (SSMs) to capture LRD. Following the success of Transformers in text and image domains, they have been adapted for graph learning. Key approaches include Graphormer ("Graph Transformer") ____, SAN ("Simple and Scalable Graph Convolutional Networks with Optimal Linear Filters") ____, GraphGPS ("Graph Pointwise Synchronous Spatially Spread-Transfomer") ____, Exphormer ("Explainable Graph Neural Networks") ____, Grit ("Grit: A Simple Model for Efficient Graph Classification") ____, and Specformer ("Spectral GNNs with Spectral Transformers") ____. With the emergence of SSMs such as Mamba ("MAMBA: Multi-Agent Multi-Task Reinforcement Learning") ____, new graph models like Graph-Mamba ("Graph Mamba: A Novel Graph Neural Network Architecture for Efficient Graph Classification") ____ have been developed. While these models effectively capture LRD between distant nodes, Transformer-based models typically have quadratic complexity and are computationally demanding, whereas SSM-based models may not fully preserve the permutation equivariance of graph data ("Equivariant Message Passing for the Quantification of Uncertainty in Graph Neural Networks").

{\bf{Position or structural aware}}. Various techniques incorporate position or substructure information in graphs, from early approaches like DeepWalk ("DeepWalk: Online Learning of Document Representations") and node2vec ____, to recent work in position encoding ("Graph Embedding with Position Encoding") ____, distance encoding ("Distance Encoding for Robust Graph Neural Networks") ____, structural RNN (Scarselli, "A Scalable Hierarchical Multilayer Perceptron Architecture") ____, and SPE ____. Recent models have explored substructural information through nested structures (NGNN (Zeng, "Nested Graph Neural Networks")) ____), identity-aware patterns (ID-GNN ("Identity-Aware Graph Neural Networks")) ____, augmented kernels (GIN-AK+ ("Graph Isomorphism Networks with Augmented Kernels Plus")) ____), iterative learning (I2-GNN ("Iterative Learning of Graph Neural Networks")) ____), and sign patterns (SUN (Sun, "Structural Patterns for Robust Node Embeddings")____, SignNet ("Sign-Net: A Novel Approach to Node Classification in Signed Graphs")). While these techniques effectively capture higher-order interactions with provable expressiveness bounds, they face scalability challenges, e.g., due to expensive distance computations and requirements for full graph structure access. They may also suffer from generalization issues, including overfitting to specific structures and sensitivity to graph variations.

{\bf{Implicit and continuous-depth architecture}}. Continuous-depth architectures have emerged as a promising direction for graph learning, starting with GDE ("Graph Dynamics") __ which models network dynamics as a continuous-time system. GRAND ("Graph Neural Diffusion") developed graph neural diffusion, while Continuous GNNs ("Continuous-Time Graph Convolutional Networks") ____ provided a framework for continuous-depth networks, and GRED ("Graph Recurrent-Deep Embeddings") ____ enhanced this with recurrent and dilated memory. These approaches offer memory efficiency and natural handling of dynamical processes, but face challenges in solving differential equations and require careful tuning to balance stability and expressiveness.