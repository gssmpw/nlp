%article{wang2022equivariant,
  title={Equivariant and stable positional encoding for more powerful graph neural networks},
  author={Wang, Haorui and Yin, Haoteng and Zhang, Muhan and Li, Pan},
  journal={arXiv preprint arXiv:2203.00199},
  year={2022}
}

@article{huang2023stability,
  title={On the stability of expressive positional encodings for graph neural networks},
  author={Huang, Yinan and Lu, William and Robinson, Joshua and Yang, Yu and Zhang, Muhan and Jegelka, Stefanie and Li, Pan},
  journal={arXiv preprint arXiv:2310.02579},
  year={2023}
}

@article{huang2024can,
  title={What Can We Learn from State Space Models for Machine Learning on Graphs?},
  author={Huang, Yinan and Miao, Siqi and Li, Pan},
  journal={arXiv preprint arXiv:2406.05815},
  year={2024}
}

%article{lim2022sign,
  title={Sign and basis invariant networks for spectral graph representation learning},
  author={Lim, Derek and Robinson, Joshua and Zhao, Lingxiao and Smidt, Tess and Sra, Suvrit and Maron, Haggai and Jegelka, Stefanie},
  journal={arXiv preprint arXiv:2202.13013},
  year={2022}
}

@inproceedings{wu2022stabilizing,
  title={Stabilizing and enhancing link prediction through deepened graph auto-encoders},
  author={Wu, Xinxing and Cheng, Qiang},
  booktitle={IJCAI: proceedings of the conference},
  volume={2022},
  pages={3587},
  year={2022},
  organization={NIH Public Access}
}


@article{zhang2024expressive,
  title={On the Expressive Power of Spectral Invariant Graph Neural Networks},
  author={Zhang, Bohang and Zhao, Lingxiao and Maron, Haggai},
  journal={arXiv preprint arXiv:2406.04336},
  year={2024}
}

%article{kipf2016semi,
  title={Semi-supervised classification with graph convolutional networks},
  author={Kipf, Thomas N and Welling, Max},
  journal={arXiv preprint arXiv:1609.02907},
  year={2016}
}

@article{bresson2017residual,
  title={Residual gated graph convnets},
  author={Bresson, Xavier and Laurent, Thomas},
  journal={arXiv preprint arXiv:1711.07553},
  year={2017}
}


@inproceedings{langley00,
 author    = {P. Langley},
 title     = {Crafting Papers on Machine Learning},
 year      = {2000},
 pages     = {1207--1216},
 editor    = {Pat Langley},
 booktitle     = {Proceedings of the 17th International Conference
              on Machine Learning (ICML 2000)},
 address   = {Stanford, CA},
 publisher = {Morgan Kaufmann}
}

@TechReport{mitchell80,
  author = 	 "T. M. Mitchell",
  title = 	 "The Need for Biases in Learning Generalizations",
  institution =  "Computer Science Department, Rutgers University",
  year = 	 "1980",
  address =	 "New Brunswick, MA",
}

@phdthesis{kearns89,
  author = {M. J. Kearns},
  title =  {Computational Complexity of Machine Learning},
  school = {Department of Computer Science, Harvard University},
  year =   {1989}
}

@Book{MachineLearningI,
  editor = 	 "R. S. Michalski and J. G. Carbonell and T.
		  M. Mitchell",
  title = 	 "Machine Learning: An Artificial Intelligence
		  Approach, Vol. I",
  publisher = 	 "Tioga",
  year = 	 "1983",
  address =	 "Palo Alto, CA"
}

@Book{DudaHart2nd,
  author =       "R. O. Duda and P. E. Hart and D. G. Stork",
  title =        "Pattern Classification",
  publisher =    "John Wiley and Sons",
  edition =      "2nd",
  year =         "2000"
}

@misc{anonymous,
  title= {Suppressed for Anonymity},
  author= {Author, N. N.},
  year= {2021}
}

@InCollection{Newell81,
  author =       "A. Newell and P. S. Rosenbloom",
  title =        "Mechanisms of Skill Acquisition and the Law of
                  Practice", 
  booktitle =    "Cognitive Skills and Their Acquisition",
  pages =        "1--51",
  publisher =    "Lawrence Erlbaum Associates, Inc.",
  year =         "1981",
  editor =       "J. R. Anderson",
  chapter =      "1",
  address =      "Hillsdale, NJ"
}


@Article{Samuel59,
  author = 	 "A. L. Samuel",
  title = 	 "Some Studies in Machine Learning Using the Game of
		  Checkers",
  journal =	 "IBM Journal of Research and Development",
  year =	 "1959",
  volume =	 "3",
  number =	 "3",
  pages =	 "211--229"
}
@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{freitas2020large,
  title={A large-scale database for graph representation learning},
  author={Freitas, Scott and Dong, Yuxiao and Neil, Joshua and Chau, Duen Horng},
  journal={arXiv preprint arXiv:2011.07682},
  year={2020}
}


@article{su2024roformer,
  title={Roformer: Enhanced transformer with rotary position embedding},
  author={Su, Jianlin and Ahmed, Murtadha and Lu, Yu and Pan, Shengfeng and Bo, Wen and Liu, Yunfeng},
  journal={Neurocomputing},
  volume={568},
  pages={127063},
  year={2024},
  publisher={Elsevier}
}

@inproceedings{yun2019transformers,
  title={Are Transformers universal approximators of sequence-to-sequence functions?},
  author={Yun, Chulhee and Bhojanapalli, Srinadh and Rawat, Ankit Singh and Reddi, Sashank and Kumar, Sanjiv},
  booktitle={International Conference on Learning Representations},
  year={2019}
}


@article{cummins2020deep,
  title={Deep data flow analysis},
  author={Cummins, Chris and Leather, Hugh and Fisches, Zacharias and Ben-Nun, Tal and Hoefler, Torsten and O'Boyle, Michael},
  journal={arXiv preprint arXiv:2012.01470},
  year={2020}
}

@inproceedings{cummins2021programl,
  title={Programl: A graph-based program representation for data flow analysis and compiler optimizations},
  author={Cummins, Chris and Fisches, Zacharias V and Ben-Nun, Tal and Hoefler, Torsten and O’Boyle, Michael FP and Leather, Hugh},
  booktitle={International Conference on Machine Learning},
  pages={2244--2253},
  year={2021},
  organization={PMLR}
}


@inproceedings{brauckmann2020compiler,
  title={Compiler-based graph representations for deep learning models of code},
  author={Brauckmann, Alexander and Goens, Andr{\'e}s and Ertel, Sebastian and Castrillon, Jeronimo},
  booktitle={Proceedings of the 29th International Conference on Compiler Construction},
  pages={201--211},
  year={2020}
}


@inproceedings{choromanski2023taming,
  title={Taming graph kernels with random features},
  author={Choromanski, Krzysztof Marcin},
  booktitle={International Conference on Machine Learning},
  pages={5964--5977},
  year={2023},
  organization={PMLR}
}


@article{reid2024quasi,
  title={Quasi-monte carlo graph random features},
  author={Reid, Isaac and Weller, Adrian and Choromanski, Krzysztof M},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{zaheer2020big,
  title={Big bird: Transformers for longer sequences},
  author={Zaheer, Manzil and Guruganesh, Guru and Dubey, Kumar Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={17283--17297},
  year={2020}
}

@article{choromanski2020rethinking,
  title={Rethinking attention with performers},
  author={Choromanski, Krzysztof and Likhosherstov, Valerii and Dohan, David and Song, Xingyou and Gane, Andreea and Sarlos, Tamas and Hawkins, Peter and Davis, Jared and Mohiuddin, Afroz and Kaiser, Lukasz and others},
  journal={arXiv preprint arXiv:2009.14794},
  year={2020}
}

@article{kitaev2020reformer,
  title={Reformer: The efficient transformer},
  author={Kitaev, Nikita and Kaiser, {\L}ukasz and Levskaya, Anselm},
  journal={arXiv preprint arXiv:2001.04451},
  year={2020}
}

@article{kalman1960new,
  title={A new approach to linear filtering and prediction problems},
  author={Kalman, Rudolph Emil},
  year={1960}
}

@article{gu2020hippo,
  title={Hippo: Recurrent memory with optimal polynomial projections},
  author={Gu, Albert and Dao, Tri and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1474--1487},
  year={2020}
}


@article{tay2020long,
  title={Long range arena: A benchmark for efficient transformers},
  author={Tay, Yi and Dehghani, Mostafa and Abnar, Samira and Shen, Yikang and Bahri, Dara and Pham, Philip and Rao, Jinfeng and Yang, Liu and Ruder, Sebastian and Metzler, Donald},
  journal={arXiv preprint arXiv:2011.04006},
  year={2020}
}


@article{duvenaud2015convolutional,
  title={Convolutional networks on graphs for learning molecular fingerprints},
  author={Duvenaud, David K and Maclaurin, Dougal and Iparraguirre, Jorge and Bombarell, Rafael and Hirzel, Timothy and Aspuru-Guzik, Al{\'a}n and Adams, Ryan P},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}

@article{stokes2020deep,
  title={A deep learning approach to antibiotic discovery},
  author={Stokes, Jonathan M and Yang, Kevin and Swanson, Kyle and Jin, Wengong and Cubillos-Ruiz, Andres and Donghia, Nina M and MacNair, Craig R and French, Shawn and Carfrae, Lindsey A and Bloom-Ackermann, Zohar and others},
  journal={Cell},
  volume={180},
  number={4},
  pages={688--702},
  year={2020},
  publisher={Elsevier}
}

%article{rampavsek2022recipe,
  title={Recipe for a general, powerful, scalable graph transformer},
  author={Ramp{\'a}{\v{s}}ek, Ladislav and Galkin, Michael and Dwivedi, Vijay Prakash and Luu, Anh Tuan and Wolf, Guy and Beaini, Dominique},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={14501--14515},
  year={2022}
}

@inproceedings{wang2021molecular,
  title={Molecular graph contrastive learning with parameterized explainable augmentations},
  author={Wang, Yingheng and Min, Yaosen and Shao, Erzhuo and Wu, Ji},
  booktitle={2021 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)},
  pages={1558--1563},
  year={2021},
  organization={IEEE}
}

@inproceedings{fan2019graph,
  title={Graph neural networks for social recommendation},
  author={Fan, Wenqi and Ma, Yao and Li, Qing and He, Yuan and Zhao, Eric and Tang, Jiliang and Yin, Dawei},
  booktitle={The world wide web conference},
  pages={417--426},
  year={2019}
}

@article{xiong2021graph,
  title={Graph neural networks for automated de novo drug design},
  author={Xiong, Jiacheng and Xiong, Zhaoping and Chen, Kaixian and Jiang, Hualiang and Zheng, Mingyue},
  journal={Drug discovery today},
  volume={26},
  number={6},
  pages={1382--1393},
  year={2021},
  publisher={Elsevier}
}

@article{guo2020deep,
  title={A deep graph neural network-based mechanism for social recommendations},
  author={Guo, Zhiwei and Wang, Heng},
  journal={IEEE Transactions on Industrial Informatics},
  volume={17},
  number={4},
  pages={2776--2783},
  year={2020},
  publisher={IEEE}
}


@inproceedings{kipf2016semi,
  title={Semi-Supervised Classification with Graph Convolutional Networks},
  author={Kipf, Thomas N and Welling, Max},
  booktitle={International Conference on Learning Representations},
  year={2016}
}

@article{fung2021benchmarking,
  title={Benchmarking graph neural networks for materials chemistry},
  author={Fung, Victor and Zhang, Jiaxin and Juarez, Eric and Sumpter, Bobby G},
  journal={npj Computational Materials},
  volume={7},
  number={1},
  pages={84},
  year={2021},
  publisher={Nature Publishing Group UK London}
}


@inproceedings{xu2018powerful,
  title={How Powerful are Graph Neural Networks?},
  author={Xu, Keyulu and Hu, Weihua and Leskovec, Jure and Jegelka, Stefanie},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@article{corso2020principal,
  title={Principal neighbourhood aggregation for graph nets},
  author={Corso, Gabriele and Cavalleri, Luca and Beaini, Dominique and Li{\`o}, Pietro and Veli{\v{c}}kovi{\'c}, Petar},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={13260--13271},
  year={2020}
}

%article{ying2021transformers,
  title={Do transformers really perform badly for graph representation?},
  author={Ying, Chengxuan and Cai, Tianle and Luo, Shengjie and Zheng, Shuxin and Ke, Guolin and He, Di and Shen, Yanming and Liu, Tie-Yan},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={28877--28888},
  year={2021}
}

@article{rampavsek2022recipe,
  title={Recipe for a general, powerful, scalable graph transformer},
  author={Ramp{\'a}{\v{s}}ek, Ladislav and Galkin, Michael and Dwivedi, Vijay Prakash and Luu, Anh Tuan and Wolf, Guy and Beaini, Dominique},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={14501--14515},
  year={2022}
}

@inproceedings{shirzad2023exphormer,
  title={Exphormer: Sparse transformers for graphs},
  author={Shirzad, Hamed and Velingker, Ameya and Venkatachalam, Balaji and Sutherland, Danica J and Sinop, Ali Kemal},
  booktitle={International Conference on Machine Learning},
  pages={31613--31632},
  year={2023},
  organization={PMLR}
}

@article{zhou2020graph,
  title={Graph neural networks: A review of methods and applications},
  author={Zhou, Jie and Cui, Ganqu and Hu, Shengding and Zhang, Zhengyan and Yang, Cheng and Liu, Zhiyuan and Wang, Lifeng and Li, Changcheng and Sun, Maosong},
  journal={AI open},
  volume={1},
  pages={57--81},
  year={2020},
  publisher={Elsevier}
}



%inproceedings{morris2019weisfeiler,
  title={Weisfeiler and leman go neural: Higher-order graph neural networks},
  author={Morris, Christopher and Ritzert, Martin and Fey, Matthias and Hamilton, William L and Lenssen, Jan Eric and Rattan, Gaurav and Grohe, Martin},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={33},
  pages={4602--4609},
  year={2019}
}

@article{dwivedi2022long,
  title={Long range graph benchmark},
  author={Dwivedi, Vijay Prakash and Ramp{\'a}{\v{s}}ek, Ladislav and Galkin, Michael and Parviz, Ali and Wolf, Guy and Luu, Anh Tuan and Beaini, Dominique},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={22326--22340},
  year={2022}
}


@inproceedings{di2023over,
  title={On over-squashing in message passing neural networks: The impact of width, depth, and topology},
  author={Di Giovanni, Francesco and Giusti, Lorenzo and Barbero, Federico and Luise, Giulia and Lio, Pietro and Bronstein, Michael M},
  booktitle={International Conference on Machine Learning},
  pages={7865--7885},
  year={2023},
  organization={PMLR}
}


@article{rusch2023survey,
  title={A survey on oversmoothing in graph neural networks},
  author={Rusch, T Konstantin and Bronstein, Michael M and Mishra, Siddhartha},
  journal={arXiv preprint arXiv:2303.10993},
  year={2023}
}

@article{chen2020can,
  title={Can graph neural networks count substructures?},
  author={Chen, Zhengdao and Chen, Lei and Villar, Soledad and Bruna, Joan},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={10383--10395},
  year={2020}
}

@article{kreuzer2021rethinking,
  title={Rethinking graph transformers with spectral attention},
  author={Kreuzer, Devin and Beaini, Dominique and Hamilton, Will and L{\'e}tourneau, Vincent and Tossou, Prudencio},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={21618--21629},
  year={2021}
}


@article{kim2022pure,
  title={Pure transformers are powerful graph learners},
  author={Kim, Jinwoo and Nguyen, Dat and Min, Seonwoo and Cho, Sungjun and Lee, Moontae and Lee, Honglak and Hong, Seunghoon},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={14582--14595},
  year={2022}
}

@inproceedings{chen2022structure,
  title={Structure-aware transformer for graph representation learning},
  author={Chen, Dexiong and O’Bray, Leslie and Borgwardt, Karsten},
  booktitle={International Conference on Machine Learning},
  pages={3469--3489},
  year={2022},
  organization={PMLR}
}

%article{wang2024state,
  title={State space model for new-generation network alternative to transformers: A survey},
  author={Wang, Xiao and Wang, Shiao and Ding, Yuhe and Li, Yuehang and Wu, Wentao and Rong, Yao and Kong, Weizhe and Huang, Ju and Li, Shihao and Yang, Haoxiang and others},
  journal={arXiv preprint arXiv:2404.09516},
  year={2024}
}


@ARTICLE{2020arXiv200604768W,
       author = {{Wang}, Sinong and {Li}, Belinda Z. and {Khabsa}, Madian and {Fang}, Han and {Ma}, Hao},
        title = "{Linformer: Self-Attention with Linear Complexity}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
         year = 2020,
        month = jun,
          eid = {arXiv:2006.04768},
        pages = {arXiv:2006.04768},
          doi = {10.48550/arXiv.2006.04768},
archivePrefix = {arXiv},
       eprint = {2006.04768},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2020arXiv200604768W},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{Child2019GeneratingLS,
  title={Generating Long Sequences with Sparse Transformers},
  author={Rewon Child and Scott Gray and Alec Radford and Ilya Sutskever},
  journal={ArXiv},
  year={2019},
  volume={abs/1904.10509},
  url={https://api.semanticscholar.org/CorpusID:129945531}
}


@inproceedings{
alon2021on,
title={On the Bottleneck of Graph Neural Networks and its Practical Implications},
author={Uri Alon and Eran Yahav},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=i80OPhOCVH2}
}

@inproceedings{
topping2022understanding,
title={Understanding over-squashing and bottlenecks on graphs via curvature},
author={Jake Topping and Francesco Di Giovanni and Benjamin Paul Chamberlain and Xiaowen Dong and Michael M. Bronstein},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=7UmjRGzp-A}
}

@inproceedings{wolf-etal-2020-transformers,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Wolf, Thomas  and
      Debut, Lysandre  and
      Sanh, Victor  and
      Chaumond, Julien  and
      Delangue, Clement  and
      Moi, Anthony  and
      Cistac, Pierric  and
      Rault, Tim  and
      Louf, Remi  and
      Funtowicz, Morgan  and
      Davison, Joe  and
      Shleifer, Sam  and
      von Platen, Patrick  and
      Ma, Clara  and
      Jernite, Yacine  and
      Plu, Julien  and
      Xu, Canwen  and
      Le Scao, Teven  and
      Gugger, Sylvain  and
      Drame, Mariama  and
      Lhoest, Quentin  and
      Rush, Alexander",
    editor = "Liu, Qun  and
      Schlangen, David",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = oct,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-demos.6",
    doi = "10.18653/v1/2020.emnlp-demos.6",
    pages = "38--45",
    abstract = "Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at \url{https://github.com/huggingface/transformers}.",
}


@InProceedings{Liu_2021_ICCV,
    author    = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
    title     = {Swin Transformer: Hierarchical Vision Transformer Using Shifted Windows},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2021},
    pages     = {10012-10022}
}


@article{Dwivedi2020AGO,
  title={A Generalization of Transformer Networks to Graphs},
  author={Vijay Prakash Dwivedi and Xavier Bresson},
  journal={ArXiv},
  year={2020},
  volume={abs/2012.09699},
  url={https://api.semanticscholar.org/CorpusID:229298019}
}

@article{JMLR:v24:22-0567,
  author  = {Vijay Prakash Dwivedi and Chaitanya K. Joshi and Anh Tuan Luu and Thomas Laurent and Yoshua Bengio and Xavier Bresson},
  title   = {Benchmarking Graph Neural Networks},
  journal = {Journal of Machine Learning Research},
  year    = {2023},
  volume  = {24},
  number  = {43},
  pages   = {1--48},
  url     = {http://jmlr.org/papers/v24/22-0567.html}
}

@inproceedings{NEURIPS2021_f1c15925,
 author = {Ying, Chengxuan and Cai, Tianle and Luo, Shengjie and Zheng, Shuxin and Ke, Guolin and He, Di and Shen, Yanming and Liu, Tie-Yan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {28877--28888},
 publisher = {Curran Associates, Inc.},
 title = {Do Transformers Really Perform Badly for Graph Representation?},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/f1c1592588411002af340cbaedd6fc33-Paper.pdf},
 volume = {34},
 year = {2021}
}


@inproceedings{
dwivedi2022graph,
title={Graph Neural Networks with Learnable Structural and Positional Representations},
author={Vijay Prakash Dwivedi and Anh Tuan Luu and Thomas Laurent and Yoshua Bengio and Xavier Bresson},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=wTTjnvGphYj}
}


@InProceedings{pmlr-v162-chen22r,
  title = 	 {Structure-Aware Transformer for Graph Representation Learning},
  author =       {Chen, Dexiong and O'Bray, Leslie and Borgwardt, Karsten},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {3469--3489},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/chen22r/chen22r.pdf},
  url = 	 {https://proceedings.mlr.press/v162/chen22r.html},
  abstract = 	 {The Transformer architecture has gained growing attention in graph representation learning recently, as it naturally overcomes several limitations of graph neural networks (GNNs) by avoiding their strict structural inductive biases and instead only encoding the graph structure via positional encoding. Here, we show that the node representations generated by the Transformer with positional encoding do not necessarily capture structural similarity between them. To address this issue, we propose the Structure-Aware Transformer, a class of simple and flexible graph Transformers built upon a new self-attention mechanism. This new self-attention incorporates structural information into the original self-attention by extracting a subgraph representation rooted at each node before computing the attention. We propose several methods for automatically generating the subgraph representation and show theoretically that the resulting representations are at least as expressive as the subgraph representations. Empirically, our method achieves state-of-the-art performance on five graph prediction benchmarks. Our structure-aware framework can leverage any existing GNN to extract the subgraph representation, and we show that it systematically improves performance relative to the base GNN model, successfully combining the advantages of GNNs and Transformers. Our code is available at https://github.com/BorgwardtLab/SAT.}
}
@inproceedings{NEURIPS2020_c8512d14,
 author = {Zaheer, Manzil and Guruganesh, Guru and Dubey, Kumar Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and Ahmed, Amr},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {17283--17297},
 publisher = {Curran Associates, Inc.},
 title = {Big Bird: Transformers for Longer Sequences},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/c8512d142a2d849725f31a9a7a361ab9-Paper.pdf},
 volume = {33},
 year = {2020}
}




@InProceedings{pmlr-v119-katharopoulos20a,
  title = 	 {Transformers are {RNN}s: Fast Autoregressive Transformers with Linear Attention},
  author =       {Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos and Fleuret, Fran{\c{c}}ois},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {5156--5165},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/katharopoulos20a/katharopoulos20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/katharopoulos20a.html},
  abstract = 	 {Transformers achieve remarkable performance in several tasks but due to their quadratic complexity, with respect to the input’s length, they are prohibitively slow for very long sequences. To address this limitation, we express the self-attention as a linear dot-product of kernel feature maps and make use of the associativity property of matrix products to reduce the complexity from $\bigO{N^2}$ to $\bigO{N}$, where $N$ is the sequence length. We show that this formulation permits an iterative implementation that dramatically accelerates autoregressive transformers and reveals their relationship to recurrent neural networks. Our \emph{Linear Transformers} achieve similar performance to vanilla Transformers and they are up to 4000x faster on autoregressive prediction of very long sequences.}
}

@article{dwivedi2023benchmarking,
  title={Benchmarking graph neural networks},
  author={Dwivedi, Vijay Prakash and Joshi, Chaitanya K and Luu, Anh Tuan and Laurent, Thomas and Bengio, Yoshua and Bresson, Xavier},
  journal={Journal of Machine Learning Research},
  volume={24},
  number={43},
  pages={1--48},
  year={2023}
}

@article{belkin2003laplacian,
  title={Laplacian eigenmaps for dimensionality reduction and data representation},
  author={Belkin, Mikhail and Niyogi, Partha},
  journal={Neural computation},
  volume={15},
  number={6},
  pages={1373--1396},
  year={2003},
  publisher={MIT Press}
}

@article{krizhevsky2017imagenet,
  title={ImageNet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  journal={Communications of the ACM},
  volume={60},
  number={6},
  pages={84--90},
  year={2017},
  publisher={AcM New York, NY, USA}
}

@article{zhu2021understanding,
  title={Understanding the generalization benefit of model invariance from a data perspective},
  author={Zhu, Sicheng and An, Bang and Huang, Furong},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={4328--4341},
  year={2021}
}

@article{giles1987learning,
  title={Learning, invariance, and generalization in high-order neural networks},
  author={Giles, C Lee and Maxwell, Tom},
  journal={Applied optics},
  volume={26},
  number={23},
  pages={4972--4978},
  year={1987},
  publisher={Optica Publishing Group}
}


@article{lecun1998gradient,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={Ieee}
}

@article{krizhevsky2012imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  journal={Advances in neural information processing systems},
  volume={25},
  year={2012}
}

@article{kazemnejad2024impact,
  title={The impact of positional encoding on length generalization in transformers},
  author={Kazemnejad, Amirhossein and Padhi, Inkit and Natesan Ramamurthy, Karthikeyan and Das, Payel and Reddy, Siva},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{chien2021adaptive,
  title={Adaptive Universal Generalized PageRank Graph Neural Network},
  author={Chien, Eli and Peng, Jianhao and Li, Pan and Milenkovic, Olgica},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@inproceedings{nguyen2023revisiting,
  title={Revisiting over-smoothing and over-squashing using ollivier-ricci curvature},
  author={Nguyen, Khang and Hieu, Nong Minh and Nguyen, Vinh Duc and Ho, Nhat and Osher, Stanley and Nguyen, Tan Minh},
  booktitle={International Conference on Machine Learning},
  pages={25956--25979},
  year={2023},
  organization={PMLR}
}

@inproceedings{chen2020measuring,
  title={Measuring and relieving the over-smoothing problem for graph neural networks from the topological view},
  author={Chen, Deli and Lin, Yankai and Li, Wei and Li, Peng and Zhou, Jie and Sun, Xu},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={34},
  pages={3438--3445},
  year={2020}
}

@article{keriven2022not,
  title={Not too little, not too much: a theoretical analysis of graph (over) smoothing},
  author={Keriven, Nicolas},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={2268--2281},
  year={2022}
}

@inproceedings{indyk1998approximate,
  title={Approximate nearest neighbors: towards removing the curse of dimensionality},
  author={Indyk, Piotr and Motwani, Rajeev},
  booktitle={Proceedings of the thirtieth annual ACM symposium on Theory of computing},
  pages={604--613},
  year={1998}
}

@article{daras2020smyrf,
  title={Smyrf-efficient attention using asymmetric clustering},
  author={Daras, Giannis and Kitaev, Nikita and Odena, Augustus and Dimakis, Alexandros G},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={6476--6489},
  year={2020}
}

@inproceedings{zandieh2023kdeformer,
  title={Kdeformer: Accelerating transformers via kernel density estimation},
  author={Zandieh, Amir and Han, Insu and Daliri, Majid and Karbasi, Amin},
  booktitle={International Conference on Machine Learning},
  pages={40605--40623},
  year={2023},
  organization={PMLR}
}

@inproceedings{han2023hyperattention,
  title={HyperAttention: Long-context Attention in Near-Linear Time},
  author={Han, Insu and Jayaram, Rajesh and Karbasi, Amin and Mirrokni, Vahab and Woodruff, David and Zandieh, Amir},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}


@article{perepechko2009number,
  title={The number of fixed length cycles in an undirected graph. Explicit formulae in case of small lengths},
  author={Perepechko, SN and Voropaev, AN},
  journal={Mathematical Modeling and Computational Physics (MMCP2009)},
  volume={148},
  year={2009}
}


@inproceedings{zhao2021stars,
  title={From Stars to Subgraphs: Uplifting Any GNN with Local Structure Awareness},
  author={Zhao, Lingxiao and Jin, Wei and Akoglu, Leman and Shah, Neil},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@inproceedings{huang2022boosting,
  title={Boosting the Cycle Counting Power of Graph Neural Networks with I$^{2}$-GNNs},
  author={Huang, Yinan and Peng, Xingang and Ma, Jianzhu and Zhang, Muhan},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}

@article{knyazev2001toward,
  title={Toward the optimal preconditioned eigensolver: Locally optimal block preconditioned conjugate gradient method},
  author={Knyazev, Andrew V},
  journal={SIAM journal on scientific computing},
  volume={23},
  number={2},
  pages={517--541},
  year={2001},
  publisher={SIAM}
}

@article{wu2018moleculenet,
  title={MoleculeNet: a benchmark for molecular machine learning},
  author={Wu, Zhenqin and Ramsundar, Bharath and Feinberg, Evan N and Gomes, Joseph and Geniesse, Caleb and Pappu, Aneesh S and Leswing, Karl and Pande, Vijay},
  journal={Chemical science},
  volume={9},
  number={2},
  pages={513--530},
  year={2018},
  publisher={Royal Society of Chemistry}
}

@article{hu2020open,
  title={Open graph benchmark: Datasets for machine learning on graphs},
  author={Hu, Weihua and Fey, Matthias and Zitnik, Marinka and Dong, Yuxiao and Ren, Hongyu and Liu, Bowen and Catasta, Michele and Leskovec, Jure},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={22118--22133},
  year={2020}
}

%article{bresson2017residual,
  title={Residual gated graph convnets},
  author={Bresson, Xavier and Laurent, Thomas},
  journal={arXiv preprint arXiv:1711.07553},
  year={2017}
}

@inproceedings{velivckovic2018graph,
  title={Graph Attention Networks},
  author={Veli{\v{c}}kovi{\'c}, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Li{\`o}, Pietro and Bengio, Yoshua},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@article{frasca2022understanding,
  title={Understanding and extending subgraph gnns by rethinking their symmetries},
  author={Frasca, Fabrizio and Bevilacqua, Beatrice and Bronstein, Michael and Maron, Haggai},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={31376--31390},
  year={2022}
}

@inproceedings{you2021identity,
  title={Identity-aware graph neural networks},
  author={You, Jiaxuan and Gomes-Selman, Jonathan M and Ying, Rex and Leskovec, Jure},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={35},
  number={12},
  pages={10737--10745},
  year={2021}
}

%inproceedings{shirzad2023exphormer,
  title={Exphormer: Sparse transformers for graphs},
  author={Shirzad, Hamed and Velingker, Ameya and Venkatachalam, Balaji and Sutherland, Danica J and Sinop, Ali Kemal},
  booktitle={International Conference on Machine Learning},
  pages={31613--31632},
  year={2023},
  organization={PMLR}
}

@inproceedings{ma2023graph,
  title={Graph inductive biases in transformers without message passing},
  author={Ma, Liheng and Lin, Chen and Lim, Derek and Romero-Soriano, Adriana and Dokania, Puneet K and Coates, Mark and Torr, Philip and Lim, Ser-Nam},
  booktitle={International Conference on Machine Learning},
  pages={23321--23337},
  year={2023},
  organization={PMLR}
}

@inproceedings{bo2022specformer,
  title={Specformer: Spectral Graph Neural Networks Meet Transformers},
  author={Bo, Deyu and Shi, Chuan and Wang, Lele and Liao, Renjie},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}

@article{velingker2024affinity,
  title={Affinity-aware graph networks},
  author={Velingker, Ameya and Sinop, Ali and Ktena, Ira and Veli{\v{c}}kovi{\'c}, Petar and Gollapudi, Sreenivas},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{nikolentzos2020random,
  title={Random walk graph neural networks},
  author={Nikolentzos, Giannis and Vazirgiannis, Michalis},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={16211--16222},
  year={2020}
}

@article{mialon2021graphit,
  title={Graphit: Encoding graph structure in transformers},
  author={Mialon, Gr{\'e}goire and Chen, Dexiong and Selosse, Margot and Mairal, Julien},
  journal={arXiv preprint arXiv:2106.05667},
  year={2021}
}

@article{everingham2010pascal,
  title={The pascal visual object classes (voc) challenge},
  author={Everingham, Mark and Van Gool, Luc and Williams, Christopher KI and Winn, John and Zisserman, Andrew},
  journal={International journal of computer vision},
  volume={88},
  pages={303--338},
  year={2010},
  publisher={Springer}
}

@article{achanta2012slic,
  title={SLIC superpixels compared to state-of-the-art superpixel methods},
  author={Achanta, Radhakrishna and Shaji, Appu and Smith, Kevin and Lucchi, Aurelien and Fua, Pascal and S{\"u}sstrunk, Sabine},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={34},
  number={11},
  pages={2274--2282},
  year={2012},
  publisher={IEEE}
}

@article{singh2016satpdb,
  title={SATPdb: a database of structurally annotated therapeutic peptides},
  author={Singh, Sandeep and Chaudhary, Kumardeep and Dhanda, Sandeep Kumar and Bhalla, Sherry and Usmani, Salman Sadullah and Gautam, Ankur and Tuknait, Abhishek and Agrawal, Piyush and Mathur, Deepika and Raghava, Gajendra PS},
  journal={Nucleic acids research},
  volume={44},
  number={D1},
  pages={D1119--D1126},
  year={2016},
  publisher={Oxford University Press}
}

@article{wu2024simplifying,
  title={Simplifying and empowering transformers for large-graph representations},
  author={Wu, Qitian and Zhao, Wentao and Yang, Chenxiao and Zhang, Hengrui and Nie, Fan and Jiang, Haitian and Bian, Yatao and Yan, Junchi},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{kong2023goat,
  title={GOAT: A global transformer on large-scale graphs},
  author={Kong, Kezhi and Chen, Jiuhai and Kirchenbauer, John and Ni, Renkun and Bruss, C Bayan and Goldstein, Tom},
  booktitle={International Conference on Machine Learning},
  pages={17375--17390},
  year={2023},
  organization={PMLR}
}

@inproceedings{chen2022nagphormer,
  title={NAGphormer: A Tokenized Graph Transformer for Node Classification in Large Graphs},
  author={Chen, Jinsong and Gao, Kaiyuan and Li, Gaichao and He, Kun},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}

@article{wu2022nodeformer,
  title={Nodeformer: A scalable graph structure learning transformer for node classification},
  author={Wu, Qitian and Zhao, Wentao and Li, Zenan and Wipf, David P and Yan, Junchi},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27387--27401},
  year={2022}
}

@inproceedings{wu2023kdlgt,
  title={KDLGT: a linear graph transformer framework via kernel decomposition approach},
  author={Wu, Yi and Xu, Yanyang and Zhu, Wenhao and Song, Guojie and Lin, Zhouchen and Wang, Liang and Liu, Shaoguo},
  booktitle={Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence},
  pages={2370--2378},
  year={2023}
}

@inproceedings{ding2024recurrent,
  title={Recurrent Distance Filtering for Graph Representation Learning},
  author={Ding, Yuhui and Orvieto, Antonio and He, Bobby and Hofmann, Thomas},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}

@article{pascanu2013difficulty,
  title={On the difficulty of training recurrent neural networks},
  author={Pascanu, R},
  journal={arXiv preprint arXiv:1211.5063},
  year={2013}
}

@article{zaheer2017deep,
  title={Deep sets},
  author={Zaheer, Manzil and Kottur, Satwik and Ravanbakhsh, Siamak and Poczos, Barnabas and Salakhutdinov, Russ R and Smola, Alexander J},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}


@inproceedings{smola2003kernels,
  title={Kernels and regularization on graphs},
  author={Smola, Alexander J and Kondor, Risi},
  booktitle={Learning Theory and Kernel Machines: 16th Annual Conference on Learning Theory and 7th Kernel Workshop, COLT/Kernel 2003, Washington, DC, USA, August 24-27, 2003. Proceedings},
  pages={144--158},
  year={2003},
  organization={Springer}
}


@inproceedings{wang2022equivariant,
  title={Equivariant and Stable Positional Encoding for More Powerful Graph Neural Networks},
  author={Wang, Haorui and Yin, Haoteng and Zhang, Muhan and Li, Pan},
  booktitle={International Conference on Learning Representations},
  year={2022}
}

@inproceedings{huang2024stability,
  title={On the Stability of Expressive Positional Encodings for Graph Neural Networks},
  author={Huang, Yinan and Lu, William and Robinson, Joshua and Yang, Yu and Zhang, Muhan and Jegelka, Stefanie and Li, Pan},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}

@inproceedings{you2019position,
  title={Position-aware graph neural networks},
  author={You, Jiaxuan and Ying, Rex and Leskovec, Jure},
  booktitle={International conference on machine learning},
  pages={7134--7143},
  year={2019},
  organization={PMLR}
}


@article{li2019optimizing,
  title={Optimizing generalized pagerank methods for seed-expansion community detection},
  author={Li, Pan and Chien, I and Milenkovic, Olgica},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}
%article{li2020distance,
  title={Distance encoding: Design provably more powerful neural networks for graph representation learning},
  author={Li, Pan and Wang, Yanbang and Wang, Hongwei and Leskovec, Jure},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={4465--4478},
  year={2020}
}
@inproceedings{perozzi2014deepwalk,
  title={Deepwalk: Online learning of social representations},
  author={Perozzi, Bryan and Al-Rfou, Rami and Skiena, Steven},
  booktitle={Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining},
  pages={701--710},
  year={2014}
}
@inproceedings{grover2016node2vec,
  title={node2vec: Scalable feature learning for networks},
  author={Grover, Aditya and Leskovec, Jure},
  booktitle={Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining},
  pages={855--864},
  year={2016}
}

@inproceedings{jain2016structural,
  title={Structural-rnn: Deep learning on spatio-temporal graphs},
  author={Jain, Ashesh and Zamir, Amir R and Savarese, Silvio and Saxena, Ashutosh},
  booktitle={Proceedings of the ieee conference on computer vision and pattern recognition},
  pages={5308--5317},
  year={2016}
}

@inproceedings{chien2020adaptive,
  title={Adaptive Universal Generalized PageRank Graph Neural Network},
  author={Chien, Eli and Peng, Jianhao and Li, Pan and Milenkovic, Olgica},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@article{gleich2015pagerank,
  title={PageRank beyond the web},
  author={Gleich, David F},
  journal={siam REVIEW},
  volume={57},
  number={3},
  pages={321--363},
  year={2015},
  publisher={SIAM}
}

@article{zhi2023gaussian,
  title={Gaussian processes on graphs via spectral kernel learning},
  author={Zhi, Yin-Cong and Ng, Yin Cheng and Dong, Xiaowen},
  journal={IEEE Transactions on Signal and Information Processing over Networks},
  volume={9},
  pages={304--314},
  year={2023},
  publisher={IEEE}
}

@article{page1999pagerank,
  title={The pagerank citation ranking: Bringing order to the web},
  author={Page, Lawrence and Brin, Sergey and Motwani, Rajeev and Winograd, Terry and others},
  year={1999},
  publisher={Citeseer}
}

@inproceedings{morris2019weisfeiler,
  title={Weisfeiler and leman go neural: Higher-order graph neural networks},
  author={Morris, Christopher and Ritzert, Martin and Fey, Matthias and Hamilton, William L and Lenssen, Jan Eric and Rattan, Gaurav and Grohe, Martin},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={33},
  number={01},
  pages={4602--4609},
  year={2019}
}
@article{du2019graph,
  title={Graph neural tangent kernel: Fusing graph neural networks with graph kernels},
  author={Du, Simon S and Hou, Kangcheng and Salakhutdinov, Russ R and Poczos, Barnabas and Wang, Ruosong and Xu, Keyulu},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}
@article{jacot2018neural,
  title={Neural tangent kernel: Convergence and generalization in neural networks},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}
@article{hamilton2017inductive,
  title={Inductive representation learning on large graphs},
  author={Hamilton, Will and Ying, Zhitao and Leskovec, Jure},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{poli2019graph,
  title={Graph neural ordinary differential equations},
  author={Poli, Michael and Massaroli, Stefano and Park, Junyoung and Yamashita, Atsushi and Asama, Hajime and Park, Jinkyoo},
  journal={arXiv preprint arXiv:1911.07532},
  year={2019}
}
@inproceedings{chamberlain2021grand,
  title={Grand: Graph neural diffusion},
  author={Chamberlain, Ben and Rowbottom, James and Gorinova, Maria I and Bronstein, Michael and Webb, Stefan and Rossi, Emanuele},
  booktitle={International conference on machine learning},
  pages={1407--1418},
  year={2021},
  organization={PMLR}
}
@inproceedings{xhonneux2020continuous,
  title={Continuous graph neural networks},
  author={Xhonneux, Louis-Pascal and Qu, Meng and Tang, Jian},
  booktitle={International conference on machine learning},
  pages={10432--10441},
  year={2020},
  organization={PMLR}
}
@article{ying2018hierarchical,
  title={Hierarchical graph representation learning with differentiable pooling},
  author={Ying, Zhitao and You, Jiaxuan and Morris, Christopher and Ren, Xiang and Hamilton, Will and Leskovec, Jure},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{feng2024were,
  title={Were rnns all we needed?},
  author={Feng, Leo and Tung, Frederick and Ahmed, Mohamed Osama and Bengio, Yoshua and Hajimirsadegh, Hossein},
  journal={arXiv preprint arXiv:2410.01201},
  year={2024}
} 

@article{chung2007heat,
  title={The heat kernel as the pagerank of a graph},
  author={Chung, Fan},
  journal={Proceedings of the National Academy of Sciences},
  volume={104},
  number={50},
  pages={19735--19740},
  year={2007},
  publisher={National Acad Sciences}
}

@article{xiao2003resistance,
  title={Resistance distance and Laplacian spectrum},
  author={Xiao, Wenjun and Gutman, Ivan},
  journal={Theoretical chemistry accounts},
  volume={110},
  pages={284--289},
  year={2003},
  publisher={Springer}
}
@article{ahamed2024timemachine,
  title={Timemachine: A time series is worth 4 mambas for long-term forecasting},
  author={Ahamed, Md Atik and Cheng, Qiang},
  journal={arXiv preprint arXiv:2403.09898},
  year={2024}
}

@article{bousquet2002stability,
  title={Stability and generalization},
  author={Bousquet, Olivier and Elisseeff, Andr{\'e}},
  journal={The Journal of Machine Learning Research},
  volume={2},
  pages={499--526},
  year={2002},
  publisher={JMLR. org}
} 

@article{shalev2010learnability,
  title={Learnability, stability and uniform convergence},
  author={Shalev-Shwartz, Shai and Shamir, Ohad and Srebro, Nathan and Sridharan, Karthik},
  journal={The Journal of Machine Learning Research},
  volume={11},
  pages={2635--2670},
  year={2010},
  publisher={JMLR. org}
}
@article{palacios2001resistance,
  title={Resistance distance in graphs and random walks},
  author={Palacios, Jos{\'e} Luis},
  journal={International Journal of Quantum Chemistry},
  volume={81},
  number={1},
  pages={29--33},
  year={2001},
  publisher={Wiley Online Library}
}

@article{lipman2010biharmonic,
  title={Biharmonic distance},
  author={Lipman, Yaron and Rustamov, Raif M and Funkhouser, Thomas A},
  journal={ACM Transactions on Graphics (TOG)},
  volume={29},
  number={3},
  pages={1--11},
  year={2010},
  publisher={ACM New York, NY, USA}
}

@article{li2020distance,
  title={Distance encoding: Design provably more powerful neural networks for graph representation learning},
  author={Li, Pan and Wang, Yanbang and Wang, Hongwei and Leskovec, Jure},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={4465--4478},
  year={2020}
}

@article{zhang2021nested,
  title={Nested graph neural networks},
  author={Zhang, Muhan and Li, Pan},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={15734--15747},
  year={2021}
}


@article{paige1972computational,
  title={Computational variants of the Lanczos method for the eigenproblem},
  author={Paige, Christopher C},
  journal={IMA Journal of Applied Mathematics},
  volume={10},
  number={3},
  pages={373--381},
  year={1972},
  publisher={Oxford University Press}
}

@article{lanczos1950iteration,
  title={An iteration method for the solution of the eigenvalue problem of linear differential and integral operators},
  author={Lanczos, Cornelius},
  year={1950},
  publisher={United States Governm. Press Office Los Angeles, CA}
}

@inproceedings{maskey2022generalized,
  title={Generalized Laplacian Positional Encoding for Graph Representation Learning},
  author={Maskey, Sohir and Parviz, Ali and Thiessen, Maximilian and St{\"a}rk, Hannes and Sadikaj, Ylli and Maron, Haggai},
  booktitle={NeurIPS 2022 Workshop on Symmetry and Geometry in Neural Representations},
  year={2022}
}

@article{miao2024locality,
  title={Locality-Sensitive Hashing-Based Efficient Point Transformer with Applications in High-Energy Physics},
  author={Miao, Siqi and Lu, Zhiyuan and Liu, Mia and Duarte, Javier and Li, Pan},
  journal={International Conference on Machine Learning},
  year={2024}
}

@inproceedings{dwivedi2021graph,
  title={Graph Neural Networks with Learnable Structural and Positional Representations},
  author={Dwivedi, Vijay Prakash and Luu, Anh Tuan and Laurent, Thomas and Bengio, Yoshua and Bresson, Xavier},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@inproceedings{lim2022sign,
  title={Sign and Basis Invariant Networks for Spectral Graph Representation Learning},
  author={Lim, Derek and Robinson, Joshua David and Zhao, Lingxiao and Smidt, Tess and Sra, Suvrit and Maron, Haggai and Jegelka, Stefanie},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}




@inproceedings{beaini2021directional,
  title={Directional graph networks},
  author={Beaini, Dominique and Passaro, Saro and L{\'e}tourneau, Vincent and Hamilton, Will and Corso, Gabriele and Li{\`o}, Pietro},
  booktitle={International Conference on Machine Learning},
  pages={748--758},
  year={2021},
  organization={PMLR}
}


@inproceedings{cai2023connection,
  title={On the connection between mpnn and graph transformer},
  author={Cai, Chen and Hy, Truong Son and Yu, Rose and Wang, Yusu},
  booktitle={International Conference on Machine Learning},
  pages={3408--3430},
  year={2023},
  organization={PMLR}
}
@inproceedings{wang2022powerful,
  title={How powerful are spectral graph neural networks},
  author={Wang, Xiyuan and Zhang, Muhan},
  booktitle={International Conference on Machine Learning},
  pages={23341--23362},
  year={2022},
  organization={PMLR}
}

@article{ying2021transformers,
  title={Do transformers really perform badly for graph representation?},
  author={Ying, Chengxuan and Cai, Tianle and Luo, Shengjie and Zheng, Shuxin and Ke, Guolin and He, Di and Shen, Yanming and Liu, Tie-Yan},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={28877--28888},
  year={2021}
}


@inproceedings{gu2021efficiently,
  title={Efficiently Modeling Long Sequences with Structured State Spaces},
  author={Gu, Albert and Goel, Karan and Re, Christopher},
  booktitle={International Conference on Learning Representations},
  year={2021}
}


@article{gu2021combining,
  title={Combining recurrent, convolutional, and continuous-time models with linear state space layers},
  author={Gu, Albert and Johnson, Isys and Goel, Karan and Saab, Khaled and Dao, Tri and Rudra, Atri and R{\'e}, Christopher},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={572--585},
  year={2021}
}

@article{gu2023mamba,
  title={Mamba: Linear-time sequence modeling with selective state spaces},
  author={Gu, Albert and Dao, Tri},
  journal={arXiv preprint arXiv:2312.00752},
  year={2023}
}



@article{wang2024graph,
  title={Graph-mamba: Towards long-range graph sequence modeling with selective state spaces},
  author={Wang, Chloe and Tsepa, Oleksii and Ma, Jun and Wang, Bo},
  journal={arXiv preprint arXiv:2402.00789},
  year={2024}
}

@article{behrouz2024graph,
  title={Graph Mamba: Towards Learning on Graphs with State Space Models},
  author={Behrouz, Ali and Hashemi, Farnoosh},
  journal={arXiv preprint arXiv:2402.08678},
  year={2024}
}


@inproceedings{NEURIPS2022_9156b0f6,
 author = {Gupta, Ankit and Gu, Albert and Berant, Jonathan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {22982--22994},
 publisher = {Curran Associates, Inc.},
 title = {Diagonal State Spaces are as Effective as Structured State Spaces},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/9156b0f6dfa9bbd18c79cc459ef5d61c-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}


@inproceedings{
smith2023simplified,
title={Simplified State Space Layers for Sequence Modeling},
author={Jimmy T.H. Smith and Andrew Warrington and Scott Linderman},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=Ai8Hw3AXqks}
}

@inproceedings{
li2023what,
title={What Makes Convolutional Models Great on Long Sequence Modeling?},
author={Yuhong Li and Tianle Cai and Yi Zhang and Deming Chen and Debadeepta Dey},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=TGJSPbRpJX-}
}




@InProceedings{pmlr-v202-poli23a,
  title = 	 {Hyena Hierarchy: Towards Larger Convolutional Language Models},
  author =       {Poli, Michael and Massaroli, Stefano and Nguyen, Eric and Fu, Daniel Y and Dao, Tri and Baccus, Stephen and Bengio, Yoshua and Ermon, Stefano and Re, Christopher},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {28043--28078},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/poli23a/poli23a.pdf},
  url = 	 {https://proceedings.mlr.press/v202/poli23a.html},
  abstract = 	 {Recent advances in deep learning have relied heavily on the use of large Transformers due to their ability to learn at scale. However, the core building block of Transformers, the attention operator, exhibits quadratic cost in sequence length, limiting the amount of context accessible. Existing subquadratic methods based on low-rank and sparse approximations need to be combined with dense attention layers to match Transformers at scale, indicating a gap in capability. In this work, we propose Hyena, a subquadratic drop-in replacement for attention constructed by interleaving implicitly parametrized long convolutions and data-controlled gating. In challenging reasoning tasks on sequences of thousands to hundreds of thousands of tokens, Hyena improves accuracy by more than 50 points over operators relying on state-space models, transfer functions, and other implicit and explicit methods, matching attention-based models. We set a new state-of-the-art for dense-attention-free architectures on language modeling in standard datasets WikiText103 and The Pile, reaching Transformer quality with a 20% reduction in training compute required at sequence length 2k. Hyena operators are 2x faster than highly optimized attention at sequence length 8k, with speedups of 100x at 64k.}
}


@inproceedings{
peng2023rwkv,
title={{RWKV}: Reinventing {RNN}s for the Transformer Era},
author={Bo Peng and Eric Alcaide and Quentin Gregory Anthony and Alon Albalak and Samuel Arcadinho and Stella Biderman and Huanqi Cao and Xin Cheng and Michael Nguyen Chung and Leon Derczynski and Xingjian Du and Matteo Grella and Kranthi Kiran GV and Xuzheng He and Haowen Hou and Przemyslaw Kazienko and Jan Kocon and Jiaming Kong and Bart{\l}omiej Koptyra and Hayden Lau and Jiaju Lin and Krishna Sri Ipsit Mantri and Ferdinand Mom and Atsushi Saito and Guangyu Song and Xiangru Tang and Johan S. Wind and Stanis{\l}aw Wo{\'z}niak and Zhenyuan Zhang and Qinghua Zhou and Jian Zhu and Rui-Jie Zhu},
booktitle={The 2023 Conference on Empirical Methods in Natural Language Processing},
year={2023},
url={https://openreview.net/forum?id=7SaXczaBpG}
}


@misc{
sun2024retentive,
title={Retentive Network: A Successor to Transformer for Large Language Models},
author={Yutao Sun and Li Dong and Shaohan Huang and Shuming Ma and Yuqing Xia and Jilong Xue and Jianyong Wang and Furu Wei},
year={2024},
url={https://openreview.net/forum?id=UU9Icwbhin}
}


@article{Behrouz2024GraphMT,
  title={Graph Mamba: Towards Learning on Graphs with State Space Models},
  author={Ali Behrouz and Farnoosh Hashemi},
  journal={ArXiv},
  year={2024},
  volume={abs/2402.08678},
  url={https://api.semanticscholar.org/CorpusID:267636928}
}


@inproceedings{mehta2022long,
  title={Long Range Language Modeling via Gated State Spaces},
  author={Mehta, Harsh and Gupta, Ankit and Cutkosky, Ashok and Neyshabur, Behnam},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}

@inproceedings{ma2022mega,
  title={Mega: Moving Average Equipped Gated Attention},
  author={Ma, Xuezhe and Zhou, Chunting and Kong, Xiang and He, Junxian and Gui, Liangke and Neubig, Graham and May, Jonathan and Zettlemoyer, Luke},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}

@inproceedings{fu2022hungry,
  title={Hungry Hungry Hippos: Towards Language Modeling with State Space Models},
  author={Fu, Daniel Y and Dao, Tri and Saab, Khaled Kamal and Thomas, Armin W and Rudra, Atri and Re, Christopher},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}

@inproceedings{wang2023selective,
  title={Selective structured state-spaces for long-form video understanding},
  author={Wang, Jue and Zhu, Wentao and Wang, Pichao and Yu, Xiang and Liu, Linda and Omar, Mohamed and Hamid, Raffay},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={6387--6397},
  year={2023}
}

@book{durbin2012time,
  title={Time series analysis by state space methods},
  author={Durbin, James and Koopman, Siem Jan},
  volume={38},
  year={2012},
  publisher={OUP Oxford}
}

@book{hyndman2008forecasting,
  title={Forecasting with exponential smoothing: the state space approach},
  author={Hyndman, Rob and Koehler, Anne B and Ord, J Keith and Snyder, Ralph D},
  year={2008},
  publisher={Springer Science \& Business Media}
}

@article{graves2013generating,
  title={Generating sequences with recurrent neural networks},
  author={Graves, Alex},
  journal={arXiv preprint arXiv:1308.0850},
  year={2013}
}

@article{sutskever2014sequence,
  title={Sequence to sequence learning with neural networks},
  author={Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}


@article{rangapuram2018deep,
  title={Deep state space models for time series forecasting},
  author={Rangapuram, Syama Sundar and Seeger, Matthias W and Gasthaus, Jan and Stella, Lorenzo and Wang, Yuyang and Januschowski, Tim},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}


@inproceedings{romero2021ckconv,
  title={CKConv: Continuous Kernel Convolution For Sequential Data},
  author={Romero, David W and Kuzina, Anna and Bekkers, Erik J and Tomczak, Jakub Mikolaj and Hoogendoorn, Mark},
  booktitle={International Conference on Learning Representations},
  year={2021}
}



@inproceedings{smith2022simplified,
  title={Simplified State Space Layers for Sequence Modeling},
  author={Smith, Jimmy TH and Warrington, Andrew and Linderman, Scott},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}

@inproceedings{hasani2022liquid,
  title={Liquid Structural State-Space Models},
  author={Hasani, Ramin and Lechner, Mathias and Wang, Tsun-Hsuan and Chahine, Makram and Amini, Alexander and Rus, Daniela},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}


@article{gu2022parameterization,
  title={On the parameterization and initialization of diagonal state space models},
  author={Gu, Albert and Goel, Karan and Gupta, Ankit and R{\'e}, Christopher},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={35971--35983},
  year={2022}
}

@inproceedings{fu2023simple,
  title={Simple hardware-efficient long convolutions for sequence modeling},
  author={Fu, Daniel Y and Epstein, Elliot L and Nguyen, Eric and Thomas, Armin W and Zhang, Michael and Dao, Tri and Rudra, Atri and R{\'e}, Christopher},
  booktitle={International Conference on Machine Learning},
  pages={10373--10391},
  year={2023},
  organization={PMLR}
}

@inproceedings{qin2022toeplitz,
  title={Toeplitz Neural Network for Sequence Modeling},
  author={Qin, Zhen and Han, Xiaodong and Sun, Weixuan and He, Bowen and Li, Dong and Li, Dongxu and Dai, Yuchao and Kong, Lingpeng and Zhong, Yiran},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}



@article{wang2024state,
  title={State space model for new-generation network alternative to transformers: A survey},
  author={Wang, Xiao and Wang, Shiao and Ding, Yuhe and Li, Yuehang and Wu, Wentao and Rong, Yao and Kong, Weizhe and Huang, Ju and Li, Shihao and Yang, Haoxiang and others},
  journal={arXiv preprint arXiv:2404.09516},
  year={2024}
}

@article{tustin1947method,
  title={A method of analysing the behaviour of linear systems in terms of time series},
  author={Tustin, Arnold},
  journal={Journal of the Institution of Electrical Engineers-Part IIA: Automatic Regulators and Servo Mechanisms},
  volume={94},
  number={1},
  pages={130--142},
  year={1947},
  publisher={IET}
}

@article{dao2024transformers,
  title={Transformers are SSMs: Generalized models and efficient algorithms through structured state space duality},
  author={Dao, Tri and Gu, Albert},
  journal={arXiv preprint arXiv:2405.21060},
  year={2024}
}



@article{gupta2022diagonal,
  title={Diagonal state spaces are as effective as structured state spaces},
  author={Gupta, Ankit and Gu, Albert and Berant, Jonathan},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={22982--22994},
  year={2022}
}

@article{pan2024hetegraph,
  title={HeteGraph-Mamba: Heterogeneous Graph Learning via Selective State Space Model},
  author={Pan, Zhenyu and Jeong, Yoonsung and Liu, Xiaoda and Liu, Han},
  journal={arXiv preprint arXiv:2405.13915},
  year={2024}
}

@article{zhao2024grassnet,
  title={GrassNet: State Space Model Meets Graph Neural Network},
  author={Zhao, Gongpei and Wang, Tao and Jin, Yi and Lang, Congyan and Li, Yidong and Ling, Haibin},
  journal={arXiv preprint arXiv:2408.08583},
  year={2024}
}
@inproceedings{pytorch_geometric,
  title={Fast Graph Representation Learning with {PyTorch Geometric}},
  author={Fey, Matthias and Lenssen, Jan E.},
  booktitle={ICLR Workshop on Representation Learning on Graphs and Manifolds},
  year={2019},
}