\section{Related Works}
\begin{figure*}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/method.pdf}
    \caption{Schematic diagram of our proposed method. Here $u\in\mathbb{R}^{l}$ represents each node. $\sigma$ denotes the sigmoid activation function. $\beta$ represents a learnable parameter to focus on the specific part.}
    \label{fig:method}
\end{figure*}
We categorize the existing GNNs for prediction or classification tasks into several groups by their main network architectures. Notably, more recent models often combine elements from multiple categories.

{\bf{GCN or message passing based}}. These methods leverage either spatial or spectral domain operations through message passing or graph convolutions. Key approaches include Graph Convolutional Networks (GCN) ____, Gate GCN ____, Graph Isomorphism Networks (GIN) ____, Graph Attention Networks (GAT) ____, GraphSAGE ____, and Principal Neighborhood Aggregation (PNA) ____. While efficient and scalable, these models typically have limited ability to capture LRD. Moreover, standard GCNs have limited expressiveness, as they are equivalent to the 1-Weisfeiler-Lehman (WL) test, whereas higher-order k-GNNs are proven to be more expressive ____.

{\bf{Kernel based}}. Graph kernel methods have been extensively studied, including neural tangent kernel ____, graph neural tangent kernel ____,
graph kernels with neural networks ____, and spectral kernel learning ____. These methods offer theoretical guarantees through strong mathematical foundations from kernel theory, particularly in terms of provable convergence properties.
However, they face several challenges in adaptability to hierarchical structures, capturing complex patterns, and incorporating node/edge features. Thus, they may have limited representation power. 

{\bf{Transformer or SSM based}}. These recent models leverage Transformers or State Space Models (SSMs) to capture LRD. Following the success of Transformers in text and image domains, they have been adapted for graph learning. Key approaches include Graphormer ____, SAN ____, GraphGPS ____, Exphormer ____, Grit ____, and Specformer ____. With the emergence of SSMs such as Mamba ____, new graph models like Graph-Mamba ____ have been developed. While these models effectively capture LRD between distant nodes, Transformer-based models typically have quadratic complexity and are computationally demanding, whereas SSM-based models may not fully preserve the permutation equivariance of graph data ____.

{\bf{Position or structural aware}}. Various techniques incorporate position or substructure information in graphs, from early approaches like DeepWalk ____ and node2vec ____, to recent work in position encoding ____, distance encoding ____, structural RNN ____, and SPE ____. Recent models have explored substructural information through nested structures (NGNN ____), identity-aware patterns (ID-GNN ____), augmented kernels (GIN-AK+ ____), iterative learning (I2-GNN ____), and sign patterns (SUN ____, SignNet ____). While these techniques effectively capture higher-order interactions with provable expressiveness bounds, they face scalability challenges, e.g., due to expensive distance computations and requirements for full graph structure access. They may also suffer from generalization issues, including overfitting to specific structures and sensitivity to graph variations.

{\bf{Implicit and continuous-depth architecture}}. Continuous-depth architectures have emerged as a promising direction for graph learning, starting with GDE ____ which models network dynamics as a continuous-time system. GRAND ____ developed graph neural diffusion, while Continuous GNNs ____ provided a framework for continuous-depth networks, and GRED ____ enhanced this with recurrent and dilated memory. These approaches offer memory efficiency and natural handling of dynamical processes, but face challenges in solving differential equations and require careful tuning to balance stability and expressiveness.