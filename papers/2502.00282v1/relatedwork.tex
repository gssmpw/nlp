\section{Related Works}
\begin{figure*}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/method.pdf}
    \caption{Schematic diagram of our proposed method. Here $u\in\mathbb{R}^{l}$ represents each node. $\sigma$ denotes the sigmoid activation function. $\beta$ represents a learnable parameter to focus on the specific part.}
    \label{fig:method}
\end{figure*}
We categorize the existing GNNs for prediction or classification tasks into several groups by their main network architectures. Notably, more recent models often combine elements from multiple categories.

{\bf{GCN or message passing based}}. These methods leverage either spatial or spectral domain operations through message passing or graph convolutions. Key approaches include Graph Convolutional Networks (GCN) \cite{kipf2016semi}, Gate GCN \cite{bresson2017residual}, Graph Isomorphism Networks (GIN) \cite{xu2018powerful}, Graph Attention Networks (GAT) \cite{velivckovic2018graph}, GraphSAGE \cite{hamilton2017inductive}, and Principal Neighborhood Aggregation (PNA) \cite{corso2020principal}. While efficient and scalable, these models typically have limited ability to capture LRD. Moreover, standard GCNs have limited expressiveness, as they are equivalent to the 1-Weisfeiler-Lehman (WL) test, whereas higher-order k-GNNs are proven to be more expressive \cite{morris2019weisfeiler}.

{\bf{Kernel based}}. Graph kernel methods have been extensively studied, including neural tangent kernel \cite{jacot2018neural}, graph neural tangent kernel \cite{du2019graph},
graph kernels with neural networks \cite{morris2019weisfeiler}, and spectral kernel learning \cite{zhi2023gaussian}. These methods offer theoretical guarantees through strong mathematical foundations from kernel theory, particularly in terms of provable convergence properties.
However, they face several challenges in adaptability to hierarchical structures, capturing complex patterns, and incorporating node/edge features. Thus, they may have limited representation power. 

{\bf{Transformer or SSM based}}. These recent models leverage Transformers or State Space Models (SSMs) to capture LRD. Following the success of Transformers in text and image domains, they have been adapted for graph learning. Key approaches include Graphormer \cite{ying2021transformers}, SAN \cite{kreuzer2021rethinking}, GraphGPS \cite{rampavsek2022recipe}, Exphormer \cite{shirzad2023exphormer}, Grit \cite{ma2023graph}, and Specformer \cite{bo2022specformer}. With the emergence of SSMs such as Mamba \cite{gu2023mamba}, new graph models like Graph-Mamba \cite{wang2024state} have been developed. While these models effectively capture LRD between distant nodes, Transformer-based models typically have quadratic complexity and are computationally demanding, whereas SSM-based models may not fully preserve the permutation equivariance of graph data \cite{zhang2024expressive}.

{\bf{Position or structural aware}}. Various techniques incorporate position or substructure information in graphs, from early approaches like DeepWalk \cite{perozzi2014deepwalk} and node2vec \cite{grover2016node2vec}, to recent work in position encoding \cite{you2019position}, distance encoding \cite{li2020distance}, structural RNN \cite{jain2016structural}, and SPE \cite{huang2024stability}. Recent models have explored substructural information through nested structures (NGNN \cite{zhang2021nested}), identity-aware patterns (ID-GNN \cite{you2021identity}), augmented kernels (GIN-AK+ \cite{zhao2021stars}), iterative learning (I2-GNN \cite{huang2022boosting}), and sign patterns (SUN \cite{frasca2022understanding}, SignNet \cite{lim2022sign}). While these techniques effectively capture higher-order interactions with provable expressiveness bounds, they face scalability challenges, e.g., due to expensive distance computations and requirements for full graph structure access. They may also suffer from generalization issues, including overfitting to specific structures and sensitivity to graph variations.

{\bf{Implicit and continuous-depth architecture}}. Continuous-depth architectures have emerged as a promising direction for graph learning, starting with GDE \cite{poli2019graph} which models network dynamics as a continuous-time system. GRAND \cite{chamberlain2021grand} developed graph neural diffusion, while Continuous GNNs \cite{xhonneux2020continuous} provided a framework for continuous-depth networks, and GRED \cite{ding2024recurrent} enhanced this with recurrent and dilated memory. These approaches offer memory efficiency and natural handling of dynamical processes, but face challenges in solving differential equations and require careful tuning to balance stability and expressiveness.