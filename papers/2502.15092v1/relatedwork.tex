\section{Related Work}
%\subsection{Large Language Model Compression}
\noindent \textbf{Large Language Model Compression.}
LLM compression methods aim to make LLMs more efficient for deployment~\cite{wan2023efficient, zhu2023survey}. Common techniques include quantization, pruning, knowledge distillation, and low-rank decomposition. Quantization reduces the precision of model weights and activations, thereby decreasing memory usage and computational load.~\cite{dettmers2022gpt3, kimsqueezellm, shen2024efficient,Lin2023AWQAW}.
Pruning involves removing less significant weights or neurons from the model to create a sparser architecture. However, pruning often necessitates retraining to recover potential performance degradation~\cite{ma2023llm, ashkboos2024slicegpt, zhong2024blockpruner, hsieh2023distilling}. Knowledge distillation transfers knowledge from a large ``teacher'' model to a smaller ``student'' model by training the latter to replicate the former's outputs. This approach enables the student model to achieve performance comparable to the teacher model while being more efficient~\cite{zhong2024revisiting, muralidharan2024compact, hsieh2023distilling}. Low-rank decomposition compresses weight matrices by factorizing them into smaller, computationally efficient components~\cite{Hsu2022FWSVD, yuan2023asvd, wang2024svd}.

%\subsection{Low-rank Decomposition Compression}
\vspace{1mm}
\noindent \textbf{Low-rank Decomposition Compression.}
A significant portion of existing low-rank compression techniques~\cite{li2023losparse, noach2020compressing, chen2023ternary, Hsu2022FWSVD, yuan2023asvd, wang2024svd} relies on Singular Value Decomposition~(SVD)~\cite{stewart1993early, wall2003singular} to factorize and compress model parameters.
% , thereby reducing model size while preserving performance. 
Although original SVD~\cite{stewart1993early, wall2003singular} decomposes weight matrices into lower-rank components for reconstruction, its objective may lead to information loss if not tailored for downstream tasks. Improved SVD-based methods address these limitations. For example, FWSVD~\cite{Hsu2022FWSVD} leverages Fisher information to preserve critical components, ASVD~\cite{yuan2023asvd} scales singular values based on activation sensitivity, and SVD-LLM~\cite{wang2024svd} incorporates data whitening and layer-wise updates to compensate for accuracy loss at high compression ratios.

However, these approaches depend on a fixed descending order of singular values from SVD, allowing only truncation adjustments without re-evaluating the true significance of each decomposed component. In contrast, our method introduces a learnable mechanism that re-evaluates component importance end-to-end, enabling a more adaptive and effective compression strategy.

\begin{figure*}[ht]
\centering
\includegraphics[width=\textwidth]{Fig2_pipeline.pdf}
\vspace{-8mm}
\caption{Illustration of the overall framework of SoCo. The pre-trained weight matrix $W$ is decomposed into $U \Sigma V^\top$, where $\Sigma$ is a diagonal matrix with diagonal elements arranged in descending order. a) Existing SVD-based compression methods truncate the smaller singular values and their corresponding vectors in $U$ and rows in $V^\top$. b) The proposed SoCo assigns a diagonal matrix $S$ as importance scores to singular values in $\Sigma$. 
% During training, only $S$ is updated, while the other parameters are frozen. 
After training, singular values with an importance score below a given threshold~(e.g., 0.5) are pruned. 
In particular, singular values with importance scores larger than the threshold rescale the preserved singular values to compensate the loss from pruned components.}
\label{fig:model}
\vskip -0.05in
\end{figure*}