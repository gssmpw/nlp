@article{Hsu2022FWSVD,
  title={Language model compression with weighted low-rank factorization},
  author={Yen-Chang Hsu and Ting Hua and Sung-En Chang and Qiang Lou and Yilin Shen and Hongxia Jin},
  journal={ICLR},
  year={2022},
}

@article{Lin2023AWQAW,
  title={AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration},
  author={Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang and Chen, Wei-Ming and Wang, Wei-Chen and Xiao, Guangxuan and Dang, Xingyu and Gan, Chuang and Han, Song},
  journal={MLSys},
  year={2024}
}

@article{ashkboos2024slicegpt,
  title={Slicegpt: Compress large language models by deleting rows and columns},
  author={Ashkboos, Saleh and Croci, Maximilian L and Nascimento, Marcelo Gennari do and Hoefler, Torsten and Hensman, James},
  journal={ICLR},
  year={2024}
}

@article{chen2023ternary,
  title={Ternary Singular Value Decomposition as a Better Parameterized Form in Linear Mapping},
  author={Chen, Boyu and Chen, Hanxuan and He, Jiao and Sun, Fengyu and Jui, Shangling},
  journal={arXiv preprint arXiv:2308.07641},
  year={2023}
}

@article{dettmers2022gpt3,
  title={Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale},
  author={Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},
  journal={NeurIPS},
  year={2022}
}

@article{hsieh2023distilling,
  title={Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes},
  author={Hsieh, Cheng-Yu and Li, Chun-Liang and Yeh, Chih-Kuan and Nakhost, Hootan and Fujii, Yasuhisa and Ratner, Alexander and Krishna, Ranjay and Lee, Chen-Yu and Pfister, Tomas},
  journal={ACL},
  year={2023}
}

@inproceedings{kimsqueezellm,
  title={SqueezeLLM: Dense-and-Sparse Quantization},
  author={Kim, Sehoon and Hooper, Coleman Richard Charles and Gholami, Amir and Dong, Zhen and Li, Xiuyu and Shen, Sheng and Mahoney, Michael W and Keutzer, Kurt},
  booktitle={ICML},
  year={2024}
}

@inproceedings{li2023losparse,
  title={Losparse: Structured compression of large language models based on low-rank and sparse approximation},
  author={Li, Yixiao and Yu, Yifan and Zhang, Qingru and Liang, Chen and He, Pengcheng and Chen, Weizhu and Zhao, Tuo},
  booktitle={ICML},
  year={2023},
}

@article{ma2023llm,
  title={Llm-pruner: On the structural pruning of large language models},
  author={Ma, Xinyin and Fang, Gongfan and Wang, Xinchao},
  journal={NeurIPS},
  year={2023}
}

@inproceedings{muralidharan2024compact,
  title={Compact language models via pruning and knowledge distillation},
  author={Muralidharan, Saurav and Sreenivas, Sharath Turuvekere and Joshi, Raviraj Bhuminand and Chochowski, Marcin and Patwary, Mostofa and Shoeybi, Mohammad and Catanzaro, Bryan and Kautz, Jan and Molchanov, Pavlo},
  booktitle={NeurIPS},
  year={2024}
}

@inproceedings{noach2020compressing,
  title={Compressing pre-trained language models by matrix decomposition},
  author={Noach, Matan Ben and Goldberg, Yoav},
  booktitle={AACL},
  year={2020}
}

@article{shen2024efficient,
  title={Efficient post-training quantization with fp8 formats},
  author={Shen, Haihao and Mellempudi, Naveen and He, Xin and Gao, Qun and Wang, Chang and Wang, Mengni},
  journal={MLSys},
  year={2024}
}

@article{stewart1993early,
  title={On the early history of the singular value decomposition},
  author={Stewart, Gilbert W},
  journal={SIAM review},
  year={1993},
}

@incollection{wall2003singular,
  title={Singular value decomposition and principal component analysis},
  author={Wall, Michael E and Rechtsteiner, Andreas and Rocha, Luis M},
  booktitle={A practical approach to microarray data analysis},
  pages={91--109},
  year={2003},
  publisher={Springer}
}

@article{wan2023efficient,
  title={Efficient large language models: A survey},
  author={Wan, Zhongwei and Wang, Xin and Liu, Che and Alam, Samiul and Zheng, Yu and Liu, Jiachen and Qu, Zhongnan and Yan, Shen and Zhu, Yi and Zhang, Quanlu and others},
  journal={arXiv preprint arXiv:2312.03863},
  year={2023}
}

@article{wang2024svd,
  title={Svd-llm: Truncation-aware singular value decomposition for large language model compression},
  author={Wang, Xin and Zheng, Yu and Wan, Zhongwei and Zhang, Mi},
  journal={ICLR},
  year={2024}
}

@article{yuan2023asvd,
  title={Asvd: Activation-aware singular value decomposition for compressing large language models},
  author={Yuan, Zhihang and Shang, Yuzhang and Song, Yue and Wu, Qiang and Yan, Yan and Sun, Guangyu},
  journal={arXiv preprint arXiv:2312.05821},
  year={2023}
}

@article{zhong2024blockpruner,
  title={BlockPruner: Fine-grained Pruning for Large Language Models},
  author={Zhong, Longguang and Wan, Fanqi and Chen, Ruijun and Quan, Xiaojun and Li, Liangzhi},
  journal={arXiv preprint arXiv:2406.10594},
  year={2024}
}

@article{zhong2024revisiting,
  title={Revisiting knowledge distillation for autoregressive language models},
  author={Zhong, Qihuang and Ding, Liang and Shen, Li and Liu, Juhua and Du, Bo and Tao, Dacheng},
  journal={ACL},
  year={2024}
}

@article{zhu2023survey,
  title={A survey on model compression for large language models},
  author={Zhu, Xunyu and Li, Jian and Liu, Yong and Ma, Can and Wang, Weiping},
  journal={arXiv preprint arXiv:2308.07633},
  year={2023}
}

