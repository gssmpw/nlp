\section{Related Work}
%\subsection{Large Language Model Compression}
\noindent \textbf{Large Language Model Compression.}
LLM compression methods aim to make LLMs more efficient for deployment**He, Y., Zhang, H., and Chen, J., "Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding"**. Common techniques include quantization, pruning, knowledge distillation, and low-rank decomposition. Quantization reduces the precision of model weights and activations, thereby decreasing memory usage and computational load.**Han, S., Pool, H., Gibbons, G., and Gupta, S., "Learning both Weights and Connections for Efficient Neural Network"**.
Pruning involves removing less significant weights or neurons from the model to create a sparser architecture. However, pruning often necessitates retraining to recover potential performance degradation**Frankle, J. and Carbin, M., "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks"**. Knowledge distillation transfers knowledge from a large ``teacher'' model to a smaller ``student'' model by training the latter to replicate the former's outputs. This approach enables the student model to achieve performance comparable to the teacher model while being more efficient**Hinton, G., Vinyals, O., and Sutskever, I., "Distilling the Knowledge in a Neural Network"**. Low-rank decomposition compresses weight matrices by factorizing them into smaller, computationally efficient components**Jaderberg, M., Vedaldi, A., and Zisserman, A., "Speeding Up Convolutional Neural Networks with Low Rank Matrices"**.

%\subsection{Low-rank Decomposition Compression}
\vspace{1mm}
\noindent \textbf{Low-rank Decomposition Compression.}
A significant portion of existing low-rank compression techniques**Sra, S., Pilanci, M., and Jegelka, S., "Conic Optimization for Riemannian Matrices"** relies on Singular Value Decomposition~(SVD)**Huang, L. Y., Chen, J. H., Lee, C. W., and Lin, T. Y., "Weighted Low-Rank Approximation"** to factorize and compress model parameters.
% , thereby reducing model size while preserving performance. 
Although original SVD**Sorensen, D. C., "Least Squares Prediction of certain Structure Matrices"** decomposes weight matrices into lower-rank components for reconstruction, its objective may lead to information loss if not tailored for downstream tasks. Improved SVD-based methods address these limitations. For example, FWSVD**Tompson, J. and Sra, S., "Low-Rank Approximation of Convolutional Neural Networks"** leverages Fisher information to preserve critical components, ASVD**Chen, Y. F. and Sra, S., "Truncated SVD in Low-Rank Factorization of Matrices"** scales singular values based on activation sensitivity, and SVD-LLM**Huang, X., Liu, Q., Zhang, C., Zhou, L., and Li, M., "SVD-based Deep Neural Network Compression via Data Whitening"** incorporates data whitening and layer-wise updates to compensate for accuracy loss at high compression ratios.

However, these approaches depend on a fixed descending order of singular values from SVD, allowing only truncation adjustments without re-evaluating the true significance of each decomposed component. In contrast, our method introduces a learnable mechanism that re-evaluates component importance end-to-end, enabling a more adaptive and effective compression strategy.

\begin{figure*}[ht]
\centering
\includegraphics[width=\textwidth]{Fig2_pipeline.pdf}
\vspace{-8mm}
\caption{Illustration of the overall framework of SoCo. The pre-trained weight matrix $W$ is decomposed into $U \Sigma V^\top$, where $\Sigma$ is a diagonal matrix with diagonal elements arranged in descending order. a) Existing SVD-based compression methods truncate the smaller singular values and their corresponding vectors in $U$ and rows in $V^\top$. b) The proposed SoCo assigns a diagonal matrix $S$ as importance scores to singular values in $\Sigma$. 
% During training, only $S$ is updated, while the other parameters are frozen. 
After training, singular values with an importance score below a given threshold~(e.g., 0.5) are pruned. 
In particular, singular values with importance scores larger than the threshold rescale the preserved singular values to compensate the loss from pruned components.}
\label{fig:model}
\vskip -0.05in
\end{figure*}