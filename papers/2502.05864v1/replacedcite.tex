\section{Related Work}
\subsection{Multiplex Graph Neural Networks}
Multiplex Graph Neural Networks (MGNNs) utilize relation-aware message-passing to capture complex relationships and diverse semantics within multiplex graphs. For instance, RGCN ____ initially embeds each graph view separately using view-specific GCNs ____, and then employs an average pooling on these multiple node embeddings to generate final embeddings. Note that the graph convolutions in RGCN can be substituted with other classical GNNs, such as SAGE ____, GAT ____, or SGC ____. NARS ____ uses SGC to generate node embeddings for each graph view and combines them through a learnable 1D convolution. Additionally, HAN ____ applies GAT to encode each view and proposes a semantic attention mechanism to aggregate resulted node embeddings. HPN ____ designs a semantic propagation mechanism to reduce semantic confusion, also employing the attention-based multi-view fusion mechanism. Despite these advancements, the inherent structural dependency of MGNNs presents challenges for deployment in latency-sensitive applications that require rapid inference.

\subsection{GNN-to-MLP Knowledge Distillation}
In response to latency concerns, recent studies have sought to bridge the gaps between powerful GNNs and lightweight MLPs through knowledge distillation ____. A pioneering effort, GLNN ____, directly transfers knowledge from teacher GNNs to vanilla MLPs by imposing Kullback-Leibler divergence between their logits. To distill reliable knowledge, KRD ____ develops a reliable sampling strategy while RKD-MLP ____ utilizes a meta-policy to filter out unreliable soft labels. FF-G2M ____ employs both low- and high-frequency components in the spectral domain for comprehensive knowledge distillation. NOSMOG ____ enhances the performance and robustness of student MLPs by introducing positional features, representational similarity distillation, and adversarial feature augmentation. VQGraph ____ learns a powerful new graph representation space by directly labeling nodes according to their diverse local structures for distillation. AdaGMLP ____ addresses the challenges of insufficient training data and incomplete test data through ensemble learning and node alignment, while MTAAM ____ amalgamates various GNNs into a super teacher. 
% , while HGMD ____ distills hardness-aware knowledge at the subgraph level. 
LLP ____ and MUGSI ____ propose GNN-to-MLP frameworks tailored for link prediction and graph classification, and LightHGNN ____ extends this methodology to hypergraphs. However, the distillation of MGNNs into MLPs for multiplex graphs remains unexplored.