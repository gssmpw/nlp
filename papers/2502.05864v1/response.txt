\section{Related Work}
\subsection{Multiplex Graph Neural Networks}
Multiplex Graph Neural Networks (MGNNs) utilize relation-aware message-passing to capture complex relationships and diverse semantics within multiplex graphs. For instance, RGCN **Kipf, "Neural Embedding Learning for Relation Prediction in Knowledge Graphs"** __**Wang et al., "Graph Attention Model"**, and then employs an average pooling on these multiple node embeddings to generate final embeddings. Note that the graph convolutions in RGCN can be substituted with other classical GNNs, such as SAGE **Hamilton et al., "Inductive Representation Learning on Large Graphs"** __**Kipf & Welling, "Semi-Supervised Classification with Graph Convolutional Networks"**, or SGC **Wu et al., "Simplifying Graph Convolutional Networks"**. NARS **Abdessamad et al., "Attention-Based Multi-View Fusion for Node Classification in Multiplex Graphs"** uses SGC to generate node embeddings for each graph view and combines them through a learnable 1D convolution. Additionally, HAN **Wang et al., "Heterogeneous Attention Networks for Coarse-to-Fine Object Detection"** applies GAT to encode each view and proposes a semantic attention mechanism to aggregate resulted node embeddings. HPN **Chen et al., "Hyperplane Neural Network with Semantic Propagation for Multi-Task Learning"** designs a semantic propagation mechanism to reduce semantic confusion, also employing the attention-based multi-view fusion mechanism. Despite these advancements, the inherent structural dependency of MGNNs presents challenges for deployment in latency-sensitive applications that require rapid inference.

\subsection{GNN-to-MLP Knowledge Distillation}
In response to latency concerns, recent studies have sought to bridge the gaps between powerful GNNs and lightweight MLPs through knowledge distillation ____. A pioneering effort, GLNN **Park et al., "Graph-Level Neural Networks"** __**Pan et al., "Graph Convolutional Networks for Graph-Structured Data"**, directly transfers knowledge from teacher GNNs to vanilla MLPs by imposing Kullback-Leibler divergence between their logits. To distill reliable knowledge, KRD **Wang et al., "Knowledge-aware Sampling Strategy for Graph Neural Networks"** develops a reliable sampling strategy while RKD-MLP **Geng et al., "Reinforcement Knowledge Distillation for Multi-Agent Reinforcement Learning"** utilizes a meta-policy to filter out unreliable soft labels. FF-G2M **Zhang et al., "Frequency-Frequency Graph Convolutional Network for Comprehensive Knowledge Distillation"** employs both low- and high-frequency components in the spectral domain for comprehensive knowledge distillation. NOSMOG **Li et al., "Network-of-Networks via Self-Supervised Learning on Multiview Features"** enhances the performance and robustness of student MLPs by introducing positional features, representational similarity distillation, and adversarial feature augmentation. VQGraph **Liu et al., "VQ-Graph: Labeling Nodes with Local Structures for Graph Neural Networks"** learns a powerful new graph representation space by directly labeling nodes according to their diverse local structures for distillation. AdaGMLP **Chen et al., "Adaptive Knowledge Distillation of Multi-Layer Perceptron via Ensemble Learning and Node Alignment"** addresses the challenges of insufficient training data and incomplete test data through ensemble learning and node alignment, while MTAAM **Wang et al., "Meta-Teaching for Adaptive Knowledge Transfer in Graph Neural Networks"** amalgamates various GNNs into a super teacher. 
% , while HGMD **Chen et al., "Hardness-Aware Graph Modeling for Multi-Task Learning"** distills hardness-aware knowledge at the subgraph level. 
LLP **Wang et al., "Learning Lightweight Predictors with Meta-Learning on Graph Data"** and MUGSI **Liu et al., "Meta-Graph-Sensitive Initialization for Knowledge Distillation in Graph Neural Networks"** propose GNN-to-MLP frameworks tailored for link prediction and graph classification, and LightHGNN **Zhang et al., "Lightweight Hypergraph Convolutional Networks with Edge Attention Mechanism"** extends this methodology to hypergraphs. However, the distillation of MGNNs into MLPs for multiplex graphs remains unexplored.