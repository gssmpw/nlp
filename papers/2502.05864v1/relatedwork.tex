\section{Related Work}
\subsection{Multiplex Graph Neural Networks}
Multiplex Graph Neural Networks (MGNNs) utilize relation-aware message-passing to capture complex relationships and diverse semantics within multiplex graphs. For instance, RGCN \cite{RGCN} initially embeds each graph view separately using view-specific GCNs \cite{GCN}, and then employs an average pooling on these multiple node embeddings to generate final embeddings. Note that the graph convolutions in RGCN can be substituted with other classical GNNs, such as SAGE \cite{SAGE}, GAT \cite{GAT}, or SGC \cite{SGC}. NARS \cite{NARS} uses SGC to generate node embeddings for each graph view and combines them through a learnable 1D convolution. Additionally, HAN \cite{HAN} applies GAT to encode each view and proposes a semantic attention mechanism to aggregate resulted node embeddings. HPN \cite{HPN} designs a semantic propagation mechanism to reduce semantic confusion, also employing the attention-based multi-view fusion mechanism. Despite these advancements, the inherent structural dependency of MGNNs presents challenges for deployment in latency-sensitive applications that require rapid inference.

\subsection{GNN-to-MLP Knowledge Distillation}
In response to latency concerns, recent studies have sought to bridge the gaps between powerful GNNs and lightweight MLPs through knowledge distillation \cite{KD}. A pioneering effort, GLNN \cite{GLNN}, directly transfers knowledge from teacher GNNs to vanilla MLPs by imposing Kullback-Leibler divergence between their logits. To distill reliable knowledge, KRD \cite{KRD} develops a reliable sampling strategy while RKD-MLP \cite{RKD-MLP} utilizes a meta-policy to filter out unreliable soft labels. FF-G2M \cite{FF-G2M} employs both low- and high-frequency components in the spectral domain for comprehensive knowledge distillation. NOSMOG \cite{NOSMOG} enhances the performance and robustness of student MLPs by introducing positional features, representational similarity distillation, and adversarial feature augmentation. VQGraph \cite{VQGraph} learns a powerful new graph representation space by directly labeling nodes according to their diverse local structures for distillation. AdaGMLP \cite{AdaGMLP} addresses the challenges of insufficient training data and incomplete test data through ensemble learning and node alignment, while MTAAM \cite{MTAAM} amalgamates various GNNs into a super teacher. 
% , while HGMD \cite{HGMD} distills hardness-aware knowledge at the subgraph level. 
LLP \cite{LLP} and MUGSI \cite{MuGSI} propose GNN-to-MLP frameworks tailored for link prediction and graph classification, and LightHGNN \cite{LightHGNN} extends this methodology to hypergraphs. However, the distillation of MGNNs into MLPs for multiplex graphs remains unexplored.