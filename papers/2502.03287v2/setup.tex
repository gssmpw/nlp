\section{Experimental Setup} \label{sec:expsetup}

We define our hardware model and the two event-based vision benchmarks that we use in our experiments. In these experiments, we use STEMS and our hybrid schedule exploration as defined in Section \ref{sec:methods}.
%We also explain how we perform a hybrid schedule exploration on each benchmark by trimming the search space, to find the optimal inter-layer optimizations under stringent on-chip memory resources.

\subsection{Hardware Architecture} 


For our hardware model, we opt for an output stationary architecture, as it seems more natural to maximize the stationarity of neuron states inside the stationary accumulators, similar to other neuromorphic architectures \cite{10132492, 9855834}. We adapt and modify the Meta VR prototype architecture \cite{Sumbul2022System-LevelAvatars} as our hardware model, shown in Figure \ref{fig:meta_hw}. The PE array is a 16x32 output-stationary systolic array. The accelerator contains two local buffers for inputs and weights and the PE array parallelizes 32 output channels and 16 output spatial positions. Additionally, a global SRAM is integrated on-chip, which we vary in size during our experiments. We ignore the non-linear activation function (e.g. LIF, ReLU) as it is performed much less often than input integration\cite{zigzag}.

Each PE has an accumulator, which holds either the partial output sum of an ANN layer or the membrane potential of an SNN layer. It accumulates two inputs in parallel, which can be spiking synaptic operation (SOP) or valued multiply-and-accumulate (MAC). We assume 4-bit weight and 4-b inputs (for non-spiking features) and adjust the local buffer sizes accordingly, as 4-bit bit-width is sufficient for accurate inference \cite{9996763, 10.1007/978-3-031-44207-0_34}. While the PEs use 16-bit accumulators to store the membrane potential or partial output sum, quantization research shows that 12 bits are sufficient for storing neurons in the global SRAM \cite{10.1007/978-3-031-44207-0_34}.


%We make small adjustments to the PE units to support both spiking (SOP) and valued (MAC) convolutions. We also reduced the PEs' precision to 4-bit weights and inputs, and 16-bit accumulators. However, we assume neuron states are stored in a more compressed manner in 12-bit. To account for the adjustment in operand precision, the number of local buffer banks was reduced in half.

We estimate our hardware model in 22 nm FDX technology. By synthesizing and measuring the PE unit and the SRAM buffers, we estimate the model's energy parameters. DRAM energy is based on reported values in literature \cite{Vogelsang2010UnderstandingMemories, Gao2017TETRIS, Cavigelli2017Origami:Accelerator, Jouppi2021TenProduct, stream}. Table \ref{tab:mem_mac_energy} summarizes the energy cost of each component. We assume 128 KB SRAM banks for the global buffer as in \cite{Sumbul2022System-LevelAvatars}, regardless of its total size. However, global buffer energy has little effect on this work's results as most data movement is between the local buffers and PEs.

%The chip is supported by global SRAM buffers. For our experiments, we use one unified global SRAM consisting of 128 KB banks. We vary the number of banks during our experiments to highlight the effects of scheduling under tight memory constraints. To estimate on-chip energy, we synthesized the SRAM banks and the PE unit in 22nm FDX technology. DRAM access energy is based on reported values in literature \cite{Vogelsang2010UnderstandingMemories, Gao2017TETRIS, Cavigelli2017Origami:Accelerator, Jouppi2021TenProduct, stream}. 

\begin{figure}[t]
    \centering
    \includegraphics[width=.6\columnwidth]{Figures/hwarch.pdf}
    \caption{Meta VR Prototype \cite{Sumbul2022System-LevelAvatars} architecture diagram. (Left) Memory hierarchy and systolic array of PEs. (Right) PE details: reduction-of-2 MACs/SOPs. I: Input, W: Weight, O: Output, S: State.}
    \label{fig:meta_hw}
\end{figure}

\input{mem_mac_energy}







\subsection{Benchmarks}

%For our experiments, we use two spatio-temporal benchmarks that use Dynamic Vision Sensor (DVS) recordings. DVS cameras are biologically inspired, as they encode information similar to the retina, in a temporally compressed manner. They are considered the neuromorphic vision sensors. %We measure sparsity per layer by considering worst-case spike rate during test-set inference. 

For our experiments, we use two SoTA SNN models for two event-based vision benchmarks. We use the Spike-Element-Wise (SEW-) ResNet-18\cite{sewresnet} model for the CIFAR10-DVS\cite{10.3389/fnins.2017.00309} dataset, and the hybrid RED-LIF\cite{yik2024neurobench} model for the Gen4 dataset\cite{red}. The SEW-ResNet CIFAR10-DVS model is fully LIF-based and is easy to train. Hence, it can be easily used for neuron state optimization, which requires multiple training sessions. Both models are trained using surrogate gradients and back-propagation through time\cite{8891809}.

%We use the hybrid RED-LIF\cite{yik2024neurobench} model for the Gen4 dataset\cite{red}, as Gen4 is currently the largest available event-based vision dataset, and we use the SEW-ResNet\cite{sewresnet} model for the CIFAR10-DVS\cite{10.3389/fnins.2017.00309} dataset, as it is simpler to train and can be used for neuron state optimization exploration.
%We use 
%, the SEW-ResNet\cite{sewresnet} CIFAR-10 DVS\cite{10.3389/fnins.2017.00309} model, and the RED-LIF\cite{yik2024neurobench} Gen4\cite{red} model. These models are trained using surrogate gradients and backpropagation through time\cite{8891809}.

\subsubsection{CIFAR10-DVS SEW-ResNet-18}

CIFAR10-DVS dataset \cite{10.3389/fnins.2017.00309} is a recording of CIFAR10 images using a DVS camera. The 128x128 DVS input is pre-processed into 16 timesteps with 2 input channels as in \cite{sewresnet}. SEW-ResNet is a recent architecture for deep residual learning in SNN \cite{sewresnet}, which applies a spike-element-wise (SEW) residual connection. The SEW-ResNet-18 model consists of 7 SEW blocks. Each SEW block consists of convolutional LIF layers, a SEW residual connection, and a 2x2 spike max-pooling. We use a novel SEW OR function (\textit{g}) instead of the SEW ADD function used in \cite{sewresnet}, to preserve the low precision of the spikes without losing accuracy compared to SEW ADD\cite{sewresnet}. 
%\hl{However, we did not explore this on other SEW-ResNet models, where SEW ADD may improve performance.} 

%Figure \ref{fig:SEW}(a) shows the block diagram of a SEW block.
% \begin{figure}[!t]
% \centering
% \includegraphics[width=\columnwidth]{Figures/sew-resnet-2.pdf}
% \caption{(a) SEW block, featuring 2 convolution LIF layers and SEW OR function. (b) SEW-ResNet CIFAR10-DVS model, consisting of 7 SEW-Blocks. The numbers inside each block indicate the number of channels. Each CIFAR10-DVS input sample is encoded in 16 timesteps.} %SEW-5 removes the hidden states from LIF layers in blocks 0 and 1.}
% \label{fig:SEW}
% \end{figure}

Most of the SEW model's memory, features, and computations are in the first two blocks, with 80\% of neuron states and features in the first block, and 15\% in the second block. %, and only 5\% in the remaining blocks.

\subsubsection{Gen4 \textbf{R}ecurrent \textbf{E}vent-camera \textbf{D}etector (RED-LIF)} 

The Gen4 dataset is by far the largest event-based vision dataset currently available. It features hours of street recordings with millions of bounding boxes for cars, pedestrians, and bikers, recorded with a 1 Megapixel DVS camera \cite{red}. 

The original RED architecture was trained on the earlier Gen1 dataset. It is a \textit{hybrid model} which consists of two parts. First, it consists of feed-forward convolutional and squeeze-and-excitation layers to extract spatial features. Then, the high-level spatial features are passed through convolutional LSTM layers to learn spatial-temporal features.  LSTM layers are necessary for learning features over time. The RED model achieves a mean average precision (mAP) of 0.44 \cite{red}. Inspired by RED, RED-LIF  was developed with 3 feed-forward residual blocks followed by 5 convolution LIF layers. It achieves a mean average precision (mAP) of 0.29 on the Gen4 dataset\cite{yik2024neurobench}. Similarly to RED, the 640x360 DVS input stream is pre-processed into 6 channels and each training sample consists of 12 timesteps\cite{red}. 

%Figure \ref{fig:redlif} shows the structure of the RED-LIF model\cite{yik2024neurobench}. 
The three feed-forward blocks contain most of the features and most of the computation, with 8.3 GMACs per timestep compared to 870 MSOPs per timestep in the LIF layers (ignoring sparsity). Most of the model's memory is in the LIF layers, with 2.6M weights and 1.3M neuron states compared to 0.4M weights in the feed-forward blocks. While the LIF layers have high sparsity (average 93\%), the feed-forward analogue blocks have an average sparsity rate of only 50\%. 

%It is clear that the feed-forward blocks are feature-intense and may favor layer fusion, while the convolution LIF layers are memory-intense and may favor time batching.


% \begin{figure}[!t]
% \centering
% \includegraphics[width=\columnwidth]{Figures/RED-LIF (1).pdf}
% \caption{The RED-LIF hybrid model. It consists of feed-forward residual blocks, followed by convolutional spiking LIF layers. Each input sample is encoded in 12 timesteps.}
% \label{fig:redlif}
% \end{figure}

\subsection{Validation and scalability}

To verify STEMS, we created a cycle-accurate simulation of some of STEMS's generated schedules on our target hardware. The resulting data movement from our simulation matched the cost estimates generated from STEMS. To demonstrate STEM's scalability, we also performed our experiments on the SEW-ResNet-152 model. Results are reported in the appendix.

%We also performed experiments on the SEW-ResNet-152 model to demonstrate STEMS's scalability

%(see Appendix} \ref{sec:apndx}.


%Similar to RED-LIF, first two blocks of the SEW model contain most features. However, these two block also contain 95\% of the model's hidden state memory, which is almost 85\% of the model's memory (weights and hidden states). Hence, these block prefer both time-batch and depth-first scheduling.

% \subsubsection{SEW-ResNet (SEW) memory optimization}

% Deep Learning models learn features hierarchically, creating high-level semantic features in deeper layers \cite{10.1145/1553374.1553453}. Learning temporal relationships from high-level semantic features reduces the complexity and memory footprint of a model and avoids learning dynamics of low-level features which are usually unnecessary and less stable \cite{red}. Moreover, earlier blocks have larger feature maps due to pooling. 

% For DSNNs, we can remove neuron state memory from earlier blocks (i.e forget membrane potential between timesteps), to significantly reduce the model's memory footprint without compromising accuracy. For example, removing memory from the input up-sampling layer and the first 2 blocks of SEW-ResNet152 \cite{sewresnet} reduces the network's neuron state memory by 45\%. We preserve the spike-based neuron activation and surrogate gradient, to have minim effect and the same computational complexity. 

% If we examine the SEW model, we see the first two block contain 95\% of the features and hidden states, and over 90\% of the compute. The first block alone accounts for 75\% of the model's features, hidden states, and compute. We perform an ablation study on the model's hidden state memory by removing memory from earlier blocks, as shown in Table \ref{tab:sew}. 


% \begin{table}[!t]
% \caption{SEW-ResNet CIFAR10-DVS test accuracy with less neuron state (internal memory)}
% \label{tab:sew}
% \centering
% \begin{tabular}{|c||c|}
% \hline
% Model & Test Accuracy\\
% \hline
% SEW-7 & 73.5\% \\
% SEW-6 & 72.1\% \\
% \textbf{SEW-5} & \textbf{74.3\%} \\
% SEW-4 & 73\% \\
% SEW-3 & 73.2\% \\
% SEW-2 & 67.1\% \\
% SEW-1 & 67.4\% \\
% SEW-0 & 65.4\% \\
% \hline
% \end{tabular}
% \end{table}

% By removing neuron hidden state from the first two blocks (SEW-5), we reduce the model's hidden state memory by 95\% and improve test accuracy. Additionally, SEW-5 has the same properties as DNNs and RED-LIF where most feature and compute are in the earlier blocks (block 0 and 1), and most memory is in later blocks. While in SEW-7, most features and most memory are in block 0 and 1. Assuming 12-bit states and 4-bit weights, the hidden states in SEW-7 require 5.5 MB of memory, while in SEW-5 require only 0.3 MB of memory, and weights require 0.5 MB. SEW-7 and SEW-5 models have the same average sparsity rates of 93\%. Except for the input layer, both models performs SOP operations. 


%Overall, the results show that using only spatial spike-based information, the model can achieve 65.4\% accuracy. While granting it the ability to correlate spatial information over time can boost accuracy to 74.3\%.



%To show the effects of time batching in more detail and how it interacts with depth-first scheduling, we perform another exploration. For the SEW-7, we show the effects of batching time gradually, by batching 1 (ST), 2, 4, 8, and 16 (TB) timesteps.


