\section{Introduction}

Spiking Neural Networks (SNNs) are biologically inspired neural networks that have gained increasing popularity as a solution for efficient edge AI, due to their sparse spike-based computation. SNNs use state-full neuron models that evolve over time akin to recurrent neural networks, producing sparse spikes over time \cite{8891809}. Direct SNN training has seen recent success, with accuracy on par with other deep learning solutions \cite{sewresnet, 10376840}. Due to their state-full event-driven nature, SNNs are promising candidates for (spatial-)temporal problems such as event-based vision using event-based cameras (i.e Dynamic Vision Sensors (DVS)), which has recently gained popularity as an efficient solution for embedded computer vision~\cite{gallego2020event}.


While SNNs might have better compute efficiency compared to Artificial Neural Networks (ANNs), the performance of deep models is usually dominated by off-chip data movement due to the large amount of memory (weights and neuron states) and features (spikes) that exceed on-chip capacity. Hence, limiting off-chip data movement can be crucial for SNN energy efficiency on memory-constrained devices. Additionally, recent digital neuromorphic architectures consist of shared memory hierarchies and parallel Processing Elements (PEs)~\cite{10132492, 9855834}, which provide opportunities for data-reuse and parallel compute, improving their performance on large SNN models compared to earlier architectures that lack shared memory hierarchies and have poor data reuse \cite{senapati2022thorneuromorphicprocessor}. 


Mapping SNNs on an embedded architecture with limited on-chip memory can be challenging. SNNs have recurrent hidden states (i.e neuron states), which need to be remembered and stored throughout SNN inference. Storing neuron states off-chip results in less favorable performance, as they are continuously moved on-chip, updated, and stored back. Mapping design space exploration (DSE) tools have recently been developed to explore the mapping space of image processing kernels to tensor computing architectures, to minimize data movement and improve performance with loop nest optimization, using search heuristics and analytical cost models~\cite{zigzag,9923807}. Other tools also explore the inter-layer mapping space and apply (spatial) layer fusion to reduce (off-chip) data movement by orders of magnitude on memory-constrained devices\cite{stream, Sioutas2020ScheduleSF, 9646923, 8667835}. Inspired by such tools\cite{zigzag, stream}, 
we explore inter-layer mapping optimizations
for SNNs. This has resulted in the following contributions:

 \begin{itemize}
     \item STEMS: Spatial temporal mapping exploration tool for SNNs. STEMS applies intra- and inter-layer mapping optimizations in space and time. %(Sec. \ref{sec:ahm})
     \item By applying STEMS's inter-layer DSE on two event-based vision SNN benchmarks, we minimize the effect of neuron states on data movement and demonstrate up to 12x reduction in off-chip data movement and up to 5x reduction in energy. %(Sec \ref{sec:Shd}).  (Sec. \ref{sec:hb})
     \item With lessons learned from recent event-based vision models \cite{red,10204090,9749022}, we explore neuron state optimization for one of our benchmarks. The results show a 20x reduction in neuron states without accuracy loss and a 1.4x reduction in energy using STEMS. %(Sect. \ref{sec:mem}) (Sect. \ref{sec:memop}). %Using STEMS, we show that 1.4x energy reduction and has a better structure .
    % We demonstrate up to 12x in off-chip traffic and 5x in energy reduction, with inter-layer mapping (Sec \ref{sec:Shd}). This is achieved by applying STEM for inter-layer mapping exploration for two event-based vision DSNN benchmarks (Gen4 and CIFAR10-DVS) (Sec. \ref{sec:hb}).
     %\item We demonstrate 20x reduction in hidden state memory, improved schedule and energy, and no accuracy loss on the CIFAR10-DVS benchmark (Sec. \ref{sec:memop}) by exploring, with STEM, hidden state memory optimization (Sec. \ref{sec:mem}). 
 \end{itemize}

 This article is organized as follows: Sections \ref{sec:background} and \ref{sec:related_work} provide background on SNNs and inter-layer mapping, and a brief literature review on SNN mapping. Section \ref{sec:methods} explains STEMS, our mapping space, and the neuron state memory optimization method, followed by our experimental setup and results in Sections \ref{sec:expsetup} and \ref{sec:experiments}. Finally, the article ends with a discussion, conclusions, and recommendations for future work.
