@inproceedings{kour2014real,
  title={Real-time segmentation of on-line handwritten arabic script},
  author={Kour, George and Saabne, Raid},
  booktitle={Frontiers in Handwriting Recognition (ICFHR), 2014 14th International Conference on},
  pages={417--422},
  year={2014},
  organization={IEEE}
}

@inproceedings{kour2014fast,
  title={Fast classification of handwritten on-line Arabic characters},
  author={Kour, George and Saabne, Raid},
  booktitle={Soft Computing and Pattern Recognition (SoCPaR), 2014 6th International Conference of},
  pages={312--318},
  year={2014},
  organization={IEEE}
}

@article{hadash2018estimate,
  title={Estimate and Replace: A Novel Approach to Integrating Deep Neural Networks with Existing Applications},
  author={Hadash, Guy and Kermany, Einat and Carmeli, Boaz and Lavi, Ofer and Kour, George and Jacovi, Alon},
  journal={arXiv preprint arXiv:1804.09028},
  year={2018}
}

@article{gallego2020event,
  title={Event-based vision: A survey},
  author={Gallego, Guillermo and Delbr{\"u}ck, Tobi and Orchard, Garrick and Bartolozzi, Chiara and Taba, Brian and Censi, Andrea and Leutenegger, Stefan and Davison, Andrew J and Conradt, J{\"o}rg and Daniilidis, Kostas and others},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={44},
  number={1},
  pages={154--180},
  year={2020},
  publisher={IEEE}
}

@inproceedings{
yao2023spikedriven,
title={Spike-driven Transformer},
author={Man Yao and JiaKui Hu and Zhaokun Zhou and Li Yuan and Yonghong Tian and Bo XU and Guoqi Li},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=9FmolyOHi5}
}

@misc{senapati2022thorneuromorphicprocessor,
      title={THOR -- A Neuromorphic Processor with 7.29G TSOP$^2$/mm$^2$Js Energy-Throughput Efficiency}, 
      author={Mayank Senapati and Manil Dev Gomony and Sherif Eissa and Charlotte Frenkel and Henk Corporaal},
      year={2022},
      eprint={2212.01696},
      archivePrefix={arXiv},
      primaryClass={cs.NE},
      url={https://arxiv.org/abs/2212.01696}, 
}
@InProceedings{10.1007/978-3-031-44207-0_34,
author="Eissa, Sherif
and Corradi, Federico
and de Putter, Floran
and Stuijk, Sander
and Corporaal, Henk",
editor="Iliadis, Lazaros
and Papaleonidas, Antonios
and Angelov, Plamen
and Jayne, Chrisina",
title="QMTS: Fixed-point Quantization for Multiple-timescale Spiking Neural Networks",
booktitle="Artificial Neural Networks and Machine Learning -- ICANN 2023",
year="2023",
publisher="Springer Nature Switzerland",
address="Cham",
pages="407--419",
abstract="Spiking Neural Networks (SNNs) represent a promising solution for streaming applications at the edge that have strict performance and energy requirements. However, implementing SNNs efficiently at the edge requires model quantization to reduce memory and compute requirements. In this paper, we provide methods to quantize a prominent neuron model for temporally rich problems, the parameterized Adaptive Leaky-Integrate-and-Fire (p-ALIF). p-ALIF neurons combine the computational simplicity of Integrate-and-Fire neurons, with accurate learning at multiple timescales, activation sparsity, and increased dynamic range, due to adaptation and heterogeneity. p-ALIF neurons have shown state-of-the-art (SoTA) performance on temporal tasks such as speech recognition and health monitoring. Our method, QMTS, separates SNN quantization into two stages, allowing one to explore different quantization levels efficiently. QMTS search heuristics are tailored for leaky heterogeneous neurons. We demonstrate QMTS on several temporal benchmarks, showing up to 40x memory reduction and 4x sparser synaptic operations with little accuracy loss, compared to 32-bit float.",
isbn="978-3-031-44207-0"
}

@INPROCEEDINGS{9996763,
  author={De Putter, Floran and Corporaal, Henk},
  booktitle={2022 25th Euromicro Conference on Digital System Design (DSD)}, 
  title={Quantization: how far should we go?}, 
  year={2022},
  volume={},
  number={},
  pages={373-382},
  keywords={Deep learning;Quantization (signal);Computational modeling;Memory management;Random access memory;Maintenance engineering;Brain modeling},
  doi={10.1109/DSD57027.2022.00057}}

@ARTICLE{8667835,
  author={Goetschalckx, Koen and Verhelst, Marian},
  journal={IEEE Journal on Emerging and Selected Topics in Circuits and Systems}, 
  title={Breaking High-Resolution CNN Bandwidth Barriers With Enhanced Depth-First Execution}, 
  year={2019},
  volume={9},
  number={2},
  pages={323-331},
  keywords={System-on-chip;Bandwidth;Memory management;Kernel;Image resolution;Neural networks;Neural networks;memory management;high resolution imaging;neural network hardware},
  doi={10.1109/JETCAS.2019.2905361}}

@ARTICLE{9646923,
  author={Waeijen, Luc and Sioutas, Savvas and Peemen, Maurice and Lindwer, Menno and Corporaal, Henk},
  journal={IEEE Access}, 
  title={ConvFusion: A Model for Layer Fusion in Convolutional Neural Networks}, 
  year={2021},
  volume={9},
  number={},
  pages={168245-168267},
  keywords={Schedules;Convolutional neural networks;Processor scheduling;Optimal scheduling;Costs;Computational modeling;Memory management;Energy efficiency;modeling;neural networks;scheduling},
  doi={10.1109/ACCESS.2021.3134930}}
@ARTICLE{9749022,
  author={Li, Jianing and Li, Jia and Zhu, Lin and Xiang, Xijie and Huang, Tiejun and Tian, Yonghong},
  journal={IEEE Transactions on Image Processing}, 
  title={Asynchronous Spatio-Temporal Memory Network for Continuous Event-Based Object Detection}, 
  year={2022},
  volume={31},
  number={},
  pages={2975-2987},
  keywords={Object detection;Cameras;Detectors;Task analysis;Streaming media;Recurrent neural networks;Meters;Object detection;event cameras;event-based vision;deep neural networks;neuromorphic engineering},
  doi={10.1109/TIP.2022.3162962}}

@INPROCEEDINGS{10204090,
  author={Gehrig, Mathias and Scaramuzza, Davide},
  booktitle={2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Recurrent Vision Transformers for Object Detection with Event Cameras}, 
  year={2023},
  volume={},
  number={},
  keywords={Visualization;Tracking;Object detection;Computer architecture;Cameras;Transformers;Robustness;Recognition: Categorization;detection;retrieval},
  doi={10.1109/CVPR52729.2023.01334}}
%  pages={13884-13893},

@inproceedings{Vogelsang2010UnderstandingMemories,
    title = {{Understanding the Energy Consumption of Dynamic Random Access Memories}},
    year = {2010},
    booktitle = {2010 43rd Annual IEEE/ACM International Symposium on Microarchitecture},
    author = {Vogelsang, Thomas},
    month = {12},
    pages = {363--374},
    publisher = {IEEE},
    url = {http://ieeexplore.ieee.org/document/5695550/},
    isbn = {978-1-4244-9071-4},
    doi = {10.1109/MICRO.2010.42}
}

@article{10.1145/3296979.3192371,
author = {Lin, Chit-Kwan and Wild, Andreas and Chinya, Gautham N. and Lin, Tsung-Han and Davies, Mike and Wang, Hong},
title = {Mapping spiking neural networks onto a manycore neuromorphic architecture},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/3296979.3192371},
doi = {10.1145/3296979.3192371},
abstract = {We present a compiler for Loihi, a novel manycore neuromorphic processor that features a programmable, on-chip learning engine for training and executing spiking neural networks (SNNs). An SNN is distinguished from other neural networks in that (1) its independent computing units, or "neurons", communicate with others only through spike messages; and (2) each neuron evaluates local learning rules, which are functions of spike arrival and departure timings, to modify its local state. The collective neuronal state dynamics of an SNN form a nonlinear dynamical system that can be cast as an unconventional model of computation. To realize such an SNN on Loihi requires each constituent neuron to locally store and independently update its own spike timing information. However, each Loihi core has limited resources for this purpose and these must be shared by neurons assigned to the same core. In this work, we present a compiler for Loihi that maps the neurons of an SNN onto and across Loihi's cores efficiently. We show that a poor neuron-to-core mapping can incur significant energy costs and address this with a greedy algorithm that compiles SNNs onto Loihi in a power-efficient manner. In so doing, we highlight the need for further development of compilers for this new, emerging class of architectures.},
journal = {SIGPLAN Not.},
month = {jun},
pages = {78–89},
numpages = {12},
keywords = {spiking neural networks, small-world networks, neuromorphic computing, neuromorphic architectures, compilers}
}


@inproceedings{10.1145/3192366.3192371,
author = {Lin, Chit-Kwan and Wild, Andreas and Chinya, Gautham N. and Lin, Tsung-Han and Davies, Mike and Wang, Hong},
title = {Mapping spiking neural networks onto a manycore neuromorphic architecture},
year = {2018},
isbn = {9781450356985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3192366.3192371},
doi = {10.1145/3192366.3192371},
abstract = {We present a compiler for Loihi, a novel manycore neuromorphic processor that features a programmable, on-chip learning engine for training and executing spiking neural networks (SNNs). An SNN is distinguished from other neural networks in that (1) its independent computing units, or "neurons", communicate with others only through spike messages; and (2) each neuron evaluates local learning rules, which are functions of spike arrival and departure timings, to modify its local state. The collective neuronal state dynamics of an SNN form a nonlinear dynamical system that can be cast as an unconventional model of computation. To realize such an SNN on Loihi requires each constituent neuron to locally store and independently update its own spike timing information. However, each Loihi core has limited resources for this purpose and these must be shared by neurons assigned to the same core. In this work, we present a compiler for Loihi that maps the neurons of an SNN onto and across Loihi's cores efficiently. We show that a poor neuron-to-core mapping can incur significant energy costs and address this with a greedy algorithm that compiles SNNs onto Loihi in a power-efficient manner. In so doing, we highlight the need for further development of compilers for this new, emerging class of architectures.},
booktitle = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {78–89},
numpages = {12},
keywords = {spiking neural networks, small-world networks, neuromorphic computing, neuromorphic architectures, compilers},
location = {Philadelphia, PA, USA},
series = {PLDI 2018}
}


@inproceedings{Gao2017TETRIS,
    title = {{TETRIS}},
    year = {2017},
    booktitle = {Proceedings of the Twenty-Second International Conference on Architectural Support for Programming Languages and Operating Systems},
    author = {Gao, Mingyu and Pu, Jing and Yang, Xuan and Horowitz, Mark and Kozyrakis, Christos},
    month = {4},
    pages = {751--764},
    publisher = {ACM},
    url = {https://dl.acm.org/doi/10.1145/3037697.3037702},
    address = {New York, NY, USA},
    isbn = {9781450344654},
    doi = {10.1145/3037697.3037702}
}
@article{Cavigelli2017Origami:Accelerator,
    title = {{Origami: A 803-GOp/s/W Convolutional Network Accelerator}},
    year = {2017},
    journal = {IEEE Transactions on Circuits and Systems for Video Technology},
    author = {Cavigelli, Lukas and Benini, Luca},
    number = {11},
    month = {11},
    pages = {2461--2475},
    volume = {27},
    url = {http://ieeexplore.ieee.org/document/7514941/},
    doi = {10.1109/TCSVT.2016.2592330},
    issn = {1051-8215}
}
@ARTICLE{9855834,

  author={Chen, Qinyu and Gao, Chang and Fu, Yuxiang},

  journal={IEEE Transactions on Very Large Scale Integration (VLSI) Systems}, 

  title={Cerebron: A Reconfigurable Architecture for Spatiotemporal Sparse Spiking Neural Networks}, 

  year={2022},

  volume={30},

  number={10},

  pages={1425-1437},

  doi={10.1109/TVLSI.2022.3196839}}


@ARTICLE{10143982,

  author={Kang, Lei and Yang, Xu and Zhang, Chi and Yu, Shuangming and Dou, Runjiang and Li, Wenchang and Shi, Cong and Liu, Jian and Wu, Nanjian and Liu, Liyuan},

  journal={IEEE Transactions on Circuits and Systems II: Express Briefs}, 

  title={A 24.3 μJ/Image SNN Accelerator for DVS-Gesture With WS-LOS Dataflow and Sparse Methods}, 

  year={2023},

  volume={70},

  number={11},

  pages={4226-4230},

  doi={10.1109/TCSII.2023.3282589}}


@ARTICLE{10132492,

  author={Fang, Chaoming and Wang, Chuanqing and Zhao, Shiqi and Tian, Fengshi and Yang, Jie and Sawan, Mohamad},

  journal={IEEE Transactions on Biomedical Circuits and Systems}, 

  title={A 510 $\mu$W 0.738-mm$^{2}$ 6.2-pJ/SOP Online Learning Multi-Topology SNN Processor With Unified Computation Engine in 40-nm CMOS}, 

  year={2023},

  volume={17},

  number={3},

  pages={507-520},

  doi={10.1109/TBCAS.2023.3279367}}


@ARTICLE{10121702,

  author={Kim, Sangyeob and Kim, Sangjin and Um, Soyeon and Kim, Soyeon and Lee, Juhyoung and Yoo, Hoi-Jun},

  journal={IEEE Journal of Solid-State Circuits}, 

  title={SNPU: An Energy-Efficient Spike Domain Deep-Neural-Network Processor With Two-Step Spike Encoding and Shift-and-Accumulation Unit}, 

  year={2023},

  volume={58},

  number={10},

  pages={2812-2825},

  doi={10.1109/JSSC.2023.3270442}}


@ARTICLE{8528875,

  author={Frenkel, Charlotte and Lefebvre, Martin and Legat, Jean-Didier and Bol, David},

  journal={IEEE Transactions on Biomedical Circuits and Systems}, 

  title={A 0.086-mm$^2$ 12.7-pJ/SOP 64k-Synapse 256-Neuron Online-Learning Digital Spiking Neuromorphic Processor in 28-nm CMOS}, 

  year={2019},

  volume={13},

  number={1},

  pages={145-158},

  doi={10.1109/TBCAS.2018.2880425}}


@ARTICLE{7229264,

  author={Akopyan, Filipp and Sawada, Jun and Cassidy, Andrew and Alvarez-Icaza, Rodrigo and Arthur, John and Merolla, Paul and Imam, Nabil and Nakamura, Yutaka and Datta, Pallab and Nam, Gi-Joon and Taba, Brian and Beakes, Michael and Brezzo, Bernard and Kuang, Jente B. and Manohar, Rajit and Risk, William P. and Jackson, Bryan and Modha, Dharmendra S.},

  journal={IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems}, 

  title={TrueNorth: Design and Tool Flow of a 65 mW 1 Million Neuron Programmable Neurosynaptic Chip}, 

  year={2015},

  volume={34},

  number={10},

  pages={1537-1557},

  doi={10.1109/TCAD.2015.2474396}}

@ARTICLE{8259423,

  author={Davies, Mike and Srinivasa, Narayan and Lin, Tsung-Han and Chinya, Gautham and Cao, Yongqiang and Choday, Sri Harsha and Dimou, Georgios and Joshi, Prasad and Imam, Nabil and Jain, Shweta and Liao, Yuyun and Lin, Chit-Kwan and Lines, Andrew and Liu, Ruokun and Mathaikutty, Deepak and McCoy, Steven and Paul, Arnab and Tse, Jonathan and Venkataramanan, Guruguhanathan and Weng, Yi-Hsin and Wild, Andreas and Yang, Yoonseok and Wang, Hong},

  journal={IEEE Micro}, 

  title={Loihi: A Neuromorphic Manycore Processor with On-Chip Learning}, 

  year={2018},

  volume={38},

  number={1},

  pages={82-99},

  doi={10.1109/MM.2018.112130359}}

@inproceedings{Jouppi2021TenProduct,
    title = {{Ten Lessons From Three Generations Shaped Google’s TPUv4i : Industrial Product}},
    year = {2021},
    booktitle = {2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA)},
    author = {Jouppi, Norman P. and Hyun Yoon, Doe and Ashcraft, Matthew and Gottscho, Mark and Jablin, Thomas B. and Kurian, George and Laudon, James and Li, Sheng and Ma, Peter and Ma, Xiaoyu and Norrie, Thomas and Patil, Nishant and Prasad, Sushma and Young, Cliff and Zhou, Zongwei and Patterson, David},
    month = {6},
    pages = {1--14},
    publisher = {IEEE},
    url = {https://ieeexplore.ieee.org/document/9499913/},
    isbn = {978-1-6654-3333-4},
    doi = {10.1109/ISCA52012.2021.00010}
}
@ARTICLE{10.3389/fnins.2017.00309,
  
AUTHOR={Li, Hongmin and Liu, Hanchao and Ji, Xiangyang and Li, Guoqi and Shi, Luping},   
	 
TITLE={CIFAR10-DVS: An Event-Stream Dataset for Object Classification},      
	
JOURNAL={Frontiers in Neuroscience},      
	
VOLUME={11},           
	
YEAR={2017},      
	  
URL={https://www.frontiersin.org/articles/10.3389/fnins.2017.00309},       
	
DOI={10.3389/fnins.2017.00309},      
	
ISSN={1662-453X},   
   
ABSTRACT={Neuromorphic vision research requires high-quality and appropriately challenging event-stream datasets to support continuous improvement of algorithms and methods. However, creating event-stream datasets is a time-consuming task, which needs to be recorded using the neuromorphic cameras. Currently, there are limited event-stream datasets available. In this work, by utilizing the popular computer vision dataset CIFAR-10, we converted 10,000 frame-based images into 10,000 event streams using a dynamic vision sensor (DVS), providing an event-stream dataset of intermediate difficulty in 10 different classes, named as “CIFAR10-DVS.” The conversion of event-stream dataset was implemented by a repeated closed-loop smooth (RCLS) movement of frame-based images. Unlike the conversion of frame-based images by moving the camera, the image movement is more realistic in respect of its practical applications. The repeated closed-loop image movement generates rich local intensity changes in continuous time which are quantized by each pixel of the DVS camera to generate events. Furthermore, a performance benchmark in event-driven object classification is provided based on state-of-the-art classification algorithms. This work provides a large event-stream dataset and an initial benchmark for comparison, which may boost algorithm developments in even-driven pattern recognition and object classification.}
}
@INPROCEEDINGS {9522934,
author = {A. Abdolrashidi and L. Wang and S. Agrawal and J. Malmaud and O. Rybakov and C. Leichner and L. Lew},
booktitle = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
title = {Pareto-Optimal Quantized ResNet Is Mostly 4-bit},
year = {2021},
volume = {},
issn = {},
pages = {3085-3093},
abstract = {Quantization has become a popular technique to compress neural networks and reduce compute cost, but most prior work focuses on studying quantization without changing the network size. Many real-world applications of neural networks have compute cost and memory budgets, which can be traded off with model quality by changing the number of parameters. In this work, we use ResNet as a case study to systematically investigate the effects of quantization on inference compute cost-quality tradeoff curves. Our results suggest that for each bfloat16 ResNet model, there are quantized models with lower cost and higher ac-curacy; in other words, the bfloat16 compute cost-quality tradeoff curve is Pareto-dominated by the 4-bit and 8-bit curves, with models primarily quantized to 4-bit yielding the best Pareto curve. Furthermore, we achieve state-of-the-art results on ImageNet for 4-bit ResNet-50 with quantization-aware training, obtaining a top-1 eval accuracy of 77.09%. We demonstrate the regularizing effect of quantization by measuring the generalization gap. The quantization method we used is optimized for practicality: It requires little tuning and is designed with hardware capabilities in mind. Our work motivates further research into optimal numeric formats for quantization, as well as the development of machine learning accelerators supporting these formats. As part of this work, we contribute a quantization library written in JAX, which is open-sourced at https://github.com/google-research/google-research/tree/master/aqt.},
keywords = {training;analytical models;quantization (signal);computational modeling;neural networks;libraries;hardware},
doi = {10.1109/CVPRW53098.2021.00345},
url = {https://doi.ieeecomputersociety.org/10.1109/CVPRW53098.2021.00345},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {jun}
}


@inproceedings{Sumbul2022System-LevelAvatars,
    title = {{System-Level Design and Integration of a Prototype AR/VR Hardware Featuring a Custom Low-Power DNN Accelerator Chip in 7nm Technology for Codec Avatars}},
    year = {2022},
    booktitle = {2022 IEEE Custom Integrated Circuits Conference (CICC)},
    author = {Sumbul, H. Ekin and Wu, Tony F. and Li, Yuecheng and Sarwar, Syed Shakib and Koven, William and Murphy-Trotzky, Eli and Cai, Xingxing and Ansari, Elnaz and Morris, Daniel H. and Liu, Huichu and Kim, Doyun and Beigne, Edith and Labs, Reality and {Meta}},
    month = {4},
    pages = {01--08},
    publisher = {IEEE},
    url = {https://ieeexplore.ieee.org/document/9772810/},
    isbn = {978-1-6654-0756-4},
    doi = {10.1109/CICC53496.2022.9772810}
}
@ARTICLE{7738524,
  author={Chen, Yu-Hsin and Krishna, Tushar and Emer, Joel S. and Sze, Vivienne},
  journal={IEEE Journal of Solid-State Circuits}, 
  title={Eyeriss: An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks}, 
  year={2017},
  volume={52},
  number={1},
  pages={127-138},
  keywords={Shape;Random access memory;Computer architecture;Throughput;Clocks;Neural networks;Hardware;Convolutional neural networks (CNNs);dataflow processing;deep learning;energy-efficient accelerators;spatial architecture},
  doi={10.1109/JSSC.2016.2616357}}

@article{Sioutas2020ScheduleSF,
  title={Schedule Synthesis for Halide Pipelines on GPUs},
  author={Savvas Sioutas and Sander Stuijk and Twan Basten and Henk Corporaal and Lou J. Somers},
  journal={ACM Transactions on Architecture and Code Optimization (TACO)},
  year={2020},
  volume={17},
  pages={1 - 25},
  url={https://api.semanticscholar.org/CorpusID:221217398}
}


@inproceedings{sewresnet,
 author = {Fang, Wei and Yu, Zhaofei and Chen, Yanqi and Huang, Tiejun and Masquelier, Timoth\'{e}e and Tian, Yonghong},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {Deep Residual Learning in Spiking Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/afe434653a898da20044041262b3ac74-Paper.pdf},
 year = {2021}
}

@inproceedings{red,
author = {Perot, Etienne and de Tournemire, Pierre and Nitti, Davide and Masci, Jonathan and Sironi, Amos},
title = {Learning to detect objects with a 1 megapixel event camera},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Event cameras encode visual information with high temporal precision, low datarate, and high-dynamic range. Thanks to these characteristics, event cameras are particularly suited for scenarios with high motion, challenging lighting conditions and requiring low latency. However, due to the novelty of the field, the performance of event-based systems on many vision tasks is still lower compared to conventional frame-based solutions. The main reasons for this performance gap are: the lower spatial resolution of event sensors, compared to frame cameras; the lack of large-scale training datasets; the absence of well established deep learning architectures for event-based processing. In this paper, we address all these problems in the context of an event-based object detection task. First, we publicly release the first high-resolution large-scale dataset for object detection. The dataset contains more than 14 hours recordings of a 1 megapixel event camera, in automotive scenarios, together with 25M bounding boxes of cars, pedestrians, and two-wheelers, labeled at high frequency. Second, we introduce a novel recurrent architecture for event-based detection and a temporal consistency loss for better-behaved training. The ability to compactly represent the sequence of events into the internal memory of the model is essential to achieve high accuracy. Our model outperforms by a large margin feed-forward event-based architectures. Moreover, our method does not require any reconstruction of intensity images from events, showing that training directly from raw events is possible, more efficient, and more accurate than passing through an intermediate intensity image. Experiments on the dataset introduced in this work, for which events and gray level images are available, show performance on par with that of highly tuned and studied frame-based detectors.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {1396},
numpages = {14},
location = {Vancouver, BC, Canada},
series = {NIPS'20}
}

  
@inproceedings{10.1145/1553374.1553453,
author = {Lee, Honglak and Grosse, Roger and Ranganath, Rajesh and Ng, Andrew Y.},
title = {Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations},
year = {2009},
isbn = {9781605585161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1553374.1553453},
doi = {10.1145/1553374.1553453},
abstract = {There has been much interest in unsupervised learning of hierarchical generative models such as deep belief networks. Scaling such models to full-sized, high-dimensional images remains a difficult problem. To address this problem, we present the convolutional deep belief network, a hierarchical generative model which scales to realistic image sizes. This model is translation-invariant and supports efficient bottom-up and top-down probabilistic inference. Key to our approach is probabilistic max-pooling, a novel technique which shrinks the representations of higher layers in a probabilistically sound way. Our experiments show that the algorithm learns useful high-level visual features, such as object parts, from unlabeled images of objects and natural scenes. We demonstrate excellent performance on several visual recognition tasks and show that our model can perform hierarchical (bottom-up and top-down) inference over full-sized images.},
booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
pages = {609–616},
numpages = {8},
location = {Montreal, Quebec, Canada},
series = {ICML '09}
}

  
@misc{das2022realtime,
      title={Real-Time Scheduling of Machine Learning Operations on Heterogeneous Neuromorphic SoC}, 
      author={Anup Das},
      year={2022},
      eprint={2209.14777},
      archivePrefix={arXiv},
      primaryClass={cs.AR}
}
@Article{s23146548,
AUTHOR = {Xue, Jianwei and Xie, Lisheng and Chen, Faquan and Wu, Liangshun and Tian, Qingyang and Zhou, Yifan and Ying, Rendong and Liu, Peilin},
TITLE = {EdgeMap: An Optimized Mapping Toolchain for Spiking Neural Network in Edge Computing},
JOURNAL = {Sensors},
VOLUME = {23},
YEAR = {2023},
NUMBER = {14},
ARTICLE-NUMBER = {6548},
URL = {https://www.mdpi.com/1424-8220/23/14/6548},
PubMedID = {37514842},
ISSN = {1424-8220},
ABSTRACT = {Spiking neural networks (SNNs) have attracted considerable attention as third-generation artificial neural networks, known for their powerful, intelligent features and energy-efficiency advantages. These characteristics render them ideally suited for edge computing scenarios. Nevertheless, the current mapping schemes for deploying SNNs onto neuromorphic hardware face limitations such as extended execution times, low throughput, and insufficient consideration of energy consumption and connectivity, which undermine their suitability for edge computing applications. To address these challenges, we introduce EdgeMap, an optimized mapping toolchain specifically designed for deploying SNNs onto edge devices without compromising performance. EdgeMap consists of two main stages. The first stage involves partitioning the SNN graph into small neuron clusters based on the streaming graph partition algorithm, with the sizes of neuron clusters limited by the physical neuron cores. In the subsequent mapping stage, we adopt a multi-objective optimization algorithm specifically geared towards mitigating energy costs and communication costs for efficient deployment. EdgeMap\&mdash;evaluated across four typical SNN applications\&mdash;substantially outperforms other state-of-the-art mapping schemes. The performance improvements include a reduction in average latency by up to 19.8%, energy consumption by 57%, and communication cost by 58%. Moreover, EdgeMap exhibits an impressive enhancement in execution time by a factor of 1225.44\&times;, alongside a throughput increase of up to 4.02\&times;. These results highlight EdgeMap\&rsquo;s efficiency and effectiveness, emphasizing its utility for deploying SNN applications in edge computing scenarios.},
DOI = {10.3390/s23146548}
}




@misc{huynh2022implementing,
      title={Implementing Spiking Neural Networks on Neuromorphic Architectures: A Review}, 
      author={Phu Khanh Huynh and M. Lakshmi Varshika and Ankita Paul and Murat Isik and Adarsha Balaji and Anup Das},
      year={2022},
      eprint={2202.08897},
      archivePrefix={arXiv},
      primaryClass={cs.NE}
}

@ARTICLE{stream,
  author={Symons, Arne and Mei, Linyan and Colleman, Steven and Houshmand, Pouya and Karl, Sebastian and Verhelst, Marian},
  journal={IEEE Transactions on Computers}, 
  title={Stream: Design Space Exploration of Layer-Fused DNNs on Heterogeneous Dataflow Accelerators}, 
  year={2025},
  volume={74},
  number={1},
  pages={237-249},
  keywords={Computer architecture;Resource management;Micromechanical devices;Computers;Space exploration;Random access memory;Radio frequency;Processor scheduling;Memory management;Artificial neural networks;Deep neural networks;layer fusion;design space exploration;heterogeneous systems;dataflow;accelerators},
  doi={10.1109/TC.2024.3477938}}
@INPROCEEDINGS{stream_old,

  author={Symons, Arne and Mei, Linyan and Colleman, Steven and Houshmand, Pouya and Karl, Sebastian and Verhelst, Marian},

  booktitle={2023 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)}, 

  title={Stream: A Modeling Framework for Fine-grained Layer Fusion on Multi-core DNN Accelerators}, 

  year={2023},

  volume={},

  number={},

  pages={355-357},

  doi={10.1109/ISPASS57527.2023.00051}}


@INPROCEEDINGS {9923807,
author = {Y. Wu and P. Tsai and A. Parashar and V. Sze and J. S. Emer},
booktitle = {2022 55th IEEE/ACM International Symposium on Microarchitecture (MICRO)},
title = {Sparseloop: An Analytical Approach To Sparse Tensor Accelerator Modeling},
year = {2022},
volume = {},
issn = {},
pages = {1377-1395},
abstract = {In recent years, many accelerators have been proposed to efficiently process sparse tensor algebra applications (e.g., sparse neural networks). However, these proposals are single points in a large and diverse design space. The lack of systematic description and modeling support for these sparse tensor accelerators impedes hardware designers from efficient and effective design space exploration. This paper first presents a unified taxonomy to systematically describe the diverse sparse tensor accelerator design space. Based on the proposed taxonomy, it then introduces Sparseloop, the first fast, accurate, and flexible analytical modeling framework to enable early-stage evaluation and exploration of sparse tensor accelerators. Sparseloop comprehends a large set of architecture specifications, including various dataflows and sparse acceleration features (e.g., elimination of zero-based compute). Using these specifications, Sparseloop evaluates a design’s processing speed and energy efficiency while accounting for data movement and compute incurred by the employed dataflow, including the savings and overhead introduced by the sparse acceleration features using stochastic density models. Across representative accelerator designs and workloads, Sparseloop achieves over 2000× faster modeling speed than cycle-level simulations, maintains relative performance trends, and achieves 0.1% to 8% average error. The paper also presents example use cases of Sparseloop in different accelerator design flows to reveal important design insights.},
keywords = {analytical models;tensors;systematics;computational modeling;taxonomy;neural networks;stochastic processes},
doi = {10.1109/MICRO56248.2022.00096},
url = {https://doi.ieeecomputersociety.org/10.1109/MICRO56248.2022.00096},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {oct}
}


@ARTICLE{10032591,

  author={Yao, Man and Zhao, Guangshe and Zhang, Hengyu and Hu, Yifan and Deng, Lei and Tian, Yonghong and Xu, Bo and Li, Guoqi},

  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 

  title={Attention Spiking Neural Networks}, 

  year={2023},

  volume={45},

  number={8},

  pages={9393-9410},

  doi={10.1109/TPAMI.2023.3241201}}
@article{song2022dfsynthesizer,
  title={DFSynthesizer: Dataflow-based synthesis of spiking neural networks to neuromorphic hardware},
  author={Song, Shihao and Chong, Harry and Balaji, Adarsha and Das, Anup and Shackleford, James and Kandasamy, Nagarajan},
  journal={ACM Transactions on Embedded Computing Systems (TECS)},
  volume={21},
  number={3},
  pages={1--35},
  year={2022},
  publisher={ACM New York, NY}
}
@article{titirsha2021endurance,
  title={Endurance-aware mapping of spiking neural networks to neuromorphic hardware},
  author={Titirsha, Twisha and Song, Shihao and Das, Anup and Krichmar, Jeffrey and Dutt, Nikil and Kandasamy, Nagarajan and Catthoor, Francky},
  journal={IEEE Transactions on Parallel and Distributed Systems},
  volume={33},
  number={2},
  pages={288--301},
  year={2021},
  publisher={IEEE}
}
@inproceedings{titirsha2020thermal,
  title={Thermal-aware compilation of spiking neural networks to neuromorphic hardware},
  author={Titirsha, Twisha and Das, Anup},
  booktitle={International Workshop on Languages and Compilers for Parallel Computing},
  pages={134--150},
  year={2020},
  organization={Springer}
}
@ARTICLE{10363652,

  author={Xiao, Chao and He, Xu and Yang, Zhijie and Xiao, Xun and Wang, Yao and Gong, Rui and Tie, Junbo and Wang, Lei and Xu, Weixia},

  journal={IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems}, 

  title={Hierarchical Mapping of Large-Scale Spiking Convolutional Neural Networks Onto Resource-Constrained Neuromorphic Processor}, 

  year={2023},

  volume={},

  number={},

  pages={1-1},

  doi={10.1109/TCAD.2023.3344070}}

@InProceedings{10.1007/3-540-57659-2_18,
author="Kennedy, Ken
and McKinley, Kathryn S.",
editor="Banerjee, Utpal
and Gelernter, David
and Nicolau, Alex
and Padua, David",
title="Maximizing loop parallelism and improving data locality via loop fusion and distribution",
booktitle="Languages and Compilers for Parallel Computing",
year="1994",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="301--320",
abstract="Loop fusion is a program transformation that merges multiple loops into one. It is effective for reducing the synchronization overhead of parallel loops and for improving data locality. This paper presents three results for fusion: (1) a new algorithm for fusing a collection of parallel and sequential loops, minimizing parallel loop synchronization while maximizing parallelism; (2) a proof that performing fusion to maximize data locality is NP-hard; and (3) two polynomial-time algorithms for improving data locality. These techniques also apply to loop distribution, which is shown to be essentially equivalent to loop fusion. Our approach is general enough to support other fusion heuristics. Preliminary experimental results validate our approach for improving performance by exploiting data locality and increasing the granularity of parallelism.",
isbn="978-3-540-48308-3"
}

@book{gerstner2014neuronal,
  title={Neuronal dynamics: From single neurons to networks and models of cognition},
  author={Gerstner, Wulfram and Kistler, Werner M and Naud, Richard and Paninski, Liam},
  year={2014},
  publisher={Cambridge University Press}
}
@article{NASTEA1997180,
title = {Load-Balanced Sparse Matrix–Vector Multiplication on Parallel Computers},
journal = {Journal of Parallel and Distributed Computing},
volume = {46},
number = {2},
pages = {180-193},
year = {1997},
issn = {0743-7315},
doi = {https://doi.org/10.1006/jpdc.1997.1361},
url = {https://www.sciencedirect.com/science/article/pii/S0743731597913617},
author = {Sorin G. Nastea and Ophir Frieder and Tarek El-Ghazawi},
keywords = {sparse matrix–vector, multiplication, load balancing, parallel computations, greedy allocation, optimized message passing},
abstract = {We considered the load-balanced multiplication of a large sparse matrix with a large sequence of vectors on parallel computers. We propose a method that combines fast load-balancing with efficient message-passing techniques to alleviate computational and inter-node communications challenges. The performance of the proposed method was evaluated on benchmark as well as on synthetically generated matrices and compared with the current work. It is shown that, by using our approach, a tangible improvement over prior work can be obtained, particularly for very sparse and skewed matrices. Moreover, it is also shown that I/O overhead for this problem can be efficiently amortized through I/O latency hiding and overall load-balancing.}
}

@misc{yik2024neurobench,
      title={NeuroBench: A Framework for Benchmarking Neuromorphic Computing Algorithms and Systems}, 
      author={Jason Yik and Korneel Van den Berghe and Douwe den Blanken and Younes Bouhadjar and Maxime Fabre and Paul Hueber and Denis Kleyko and Noah Pacik-Nelson and Pao-Sheng Vincent Sun and Guangzhi Tang and Shenqi Wang and Biyan Zhou and Soikat Hasan Ahmed and George Vathakkattil Joseph and Benedetto Leto and Aurora Micheli and Anurag Kumar Mishra and Gregor Lenz and Tao Sun and Zergham Ahmed and Mahmoud Akl and Brian Anderson and Andreas G. Andreou and Chiara Bartolozzi and Arindam Basu and Petrut Bogdan and Sander Bohte and Sonia Buckley and Gert Cauwenberghs and Elisabetta Chicca and Federico Corradi and Guido de Croon and Andreea Danielescu and Anurag Daram and Mike Davies and Yigit Demirag and Jason Eshraghian and Tobias Fischer and Jeremy Forest and Vittorio Fra and Steve Furber and P. Michael Furlong and William Gilpin and Aditya Gilra and Hector A. Gonzalez and Giacomo Indiveri and Siddharth Joshi and Vedant Karia and Lyes Khacef and James C. Knight and Laura Kriener and Rajkumar Kubendran and Dhireesha Kudithipudi and Yao-Hong Liu and Shih-Chii Liu and Haoyuan Ma and Rajit Manohar and Josep Maria Margarit-Taulé and Christian Mayr and Konstantinos Michmizos and Dylan Muir and Emre Neftci and Thomas Nowotny and Fabrizio Ottati and Ayca Ozcelikkale and Priyadarshini Panda and Jongkil Park and Melika Payvand and Christian Pehle and Mihai A. Petrovici and Alessandro Pierro and Christoph Posch and Alpha Renner and Yulia Sandamirskaya and Clemens JS Schaefer and André van Schaik and Johannes Schemmel and Samuel Schmidgall and Catherine Schuman and Jae-sun Seo and Sadique Sheik and Sumit Bam Shrestha and Manolis Sifalakis and Amos Sironi and Matthew Stewart and Kenneth Stewart and Terrence C. Stewart and Philipp Stratmann and Jonathan Timcheck and Nergis Tömen and Gianvito Urgese and Marian Verhelst and Craig M. Vineyard and Bernhard Vogginger and Amirreza Yousefzadeh and Fatima Tuz Zohora and Charlotte Frenkel and Vijay Janapa Reddi},
      year={2024},
      eprint={2304.04640},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}
@inproceedings{10.1145/3531437.3539704,
author = {Sharma, Deepika and Ankit, Aayush and Roy, Kaushik},
title = {Identifying Efficient Dataflows for Spiking Neural Networks},
year = {2022},
isbn = {9781450393546},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531437.3539704},
doi = {10.1145/3531437.3539704},
abstract = {Deep feed-forward Spiking Neural Networks (SNNs) trained using appropriate learning algorithms have been shown to match the performance of state-of-the-art Artificial Neural Networks (ANNs). The inputs to an SNN layer are 1-bit spikes distributed over several timesteps. In addition, along with the standard artificial neural network (ANN) data structures, SNNs require one additional data structure – the membrane potential (Vmem) for each neuron which is updated every timestep. Hence, the dataflow requirements for energy-efficient hardware implementation of SNNs can be different from the standard ANNs. In this paper, we propose optimal dataflows for deep spiking neural network layers. To evaluate the energy and latency of different dataflows, we considered three hardware architectures with varying on-chip resources to represent a class of spatial accelerators. We developed a set of rules leading to optimum dataflow for SNNs that achieve more than 90\% improvement in Energy-Delay Product (EDP) compared to the baseline for some workloads and architectures.},
booktitle = {Proceedings of the ACM/IEEE International Symposium on Low Power Electronics and Design},
articleno = {2},
numpages = {6},
keywords = {artificial neural networks, dataflows, deep spiking neural networks},
location = {Boston, MA, USA},
series = {ISLPED '22}
}

  
@InProceedings{neuproma,
author="Xiao, Chao
and Chen, Jihua
and Wang, Lei",
editor="Liu, Shaoshan
and Wei, Xiaohui",
title="NeuProMa: A Toolchain for Mapping Large-Scale Spiking Convolutional Neural Networks onto Neuromorphic Processor",
booktitle="Network and Parallel Computing",
year="2022",
publisher="Springer Nature Switzerland",
address="Cham",
pages="129--142",
abstract="Neuromorphic processors, the new generation of brain-inspired non-von Neumann computing systems, have the potential to perform complex computations with more energy efficiency than conventional architectures. Neuromorphic processors typically implement the spiking neural network (SNN)-based applications. However, a non-optimized mapping of SNNs onto the neuromorphic processor may increase the on-chip communication delay and data exchange between the off-chip and on-chip memory, especially when the size of the SNNs exceeds the capacity of the processor limited by the on-chip resources. This paper proposes a toolchain, called NeuProMa, to map large-scale spiking convolutional neural networks (SCNNs) onto resource-constrained neuromorphic processors. We exploit the implicit regular connections in the SCNNs and split the SCNNs into multiple sub-networks while reducing the data exchange between the off-chip and on-chip memory. Then, we partition the sub-networks into multiple clusters sequentially in a specific order, which significantly reduces the spike messages between neuromorphic cores. Finally, NeuProMa dispatches the clusters to the neuromorphic cores, minimizing the maximum workload of the routers. Our experiments using six SCNN-based applications show that NeuProMa can significantly reduce the data exchange between the off-chip and on-chip memory, and reduce the spike latency and energy consumption by up to 17{\%} and 85{\%}, respectively, compared with the state-of-the-art.",
isbn="978-3-031-21395-3"
}


@article{balaji2020run,
  title={Run-time mapping of spiking neural networks to neuromorphic hardware},
  author={Balaji, Adarsha and Marty, Thibaut and Das, Anup and Catthoor, Francky},
  journal={Journal of Signal Processing Systems},
  volume={92},
  pages={1293--1302},
  year={2020},
  publisher={Springer}
}

@INPROCEEDINGS{8342201,

  author={Das, Anup and Wu, Yuefeng and Huynh, Khanh and Dell'Anna, Francesco and Catthoor, Francky and Schaafsma, Siebren},

  booktitle={2018 Design, Automation \& Test in Europe Conference \& Exhibition (DATE)}, 

  title={Mapping of local and global synapses on spiking neuromorphic hardware}, 

  year={2018},

  volume={},

  number={},

  pages={1217-1222},

  doi={10.23919/DATE.2018.8342201}}


@inproceedings{10.1145/3386263.3406900,
author = {Li, Shiming and Guo, Shasha and Zhang, Limeng and Kang, Ziyang and Wang, Shiying and Shi, Wei and Wang, Lei and Xu, Weixia},
title = {SNEAP: A Fast and Efficient Toolchain for Mapping Large-Scale Spiking Neural Network onto NoC-based Neuromorphic Platform},
year = {2020},
isbn = {9781450379441},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3386263.3406900},
doi = {10.1145/3386263.3406900},
abstract = {Spiking neural network (SNN), as the third generation of artificial neural networks, has been widely adopted in vision and audio tasks. Nowadays, many neuromorphic platforms support SNN simulation and adopt Network-on-Chips (NoC) architecture for multi-cores interconnection. However, a large volume and run-time communication on the interconnection has a significant effect on performance of the platform. In this paper, we propose a toolchain called SNEAP (Spiking NEural network mAPping toolchain) for mapping SNNs to neuromorphic platforms with multi-cores, which aims to reduce the energy and latency brought by spike communication on the interconnection.SNEAP includes two key steps: partitioning the SNN to reduce the spikes communicated between partitions, and mapping the partitions of SNN to the NoC to reduce average hop of spikes under the constraint of hardware resources. SNEAP effectively reduces the energy and latency on the NoC-based neuromorphic platform and spend less time than other toolchains.The experimental results show that SNEAP can achieve average 418X reduction in end-to-end execution time, and reduce energy consumption and spike latency, on average, by 23\% and 51\% respectively, compared with SpiNeMap.},
booktitle = {Proceedings of the 2020 on Great Lakes Symposium on VLSI},
pages = {9–14},
numpages = {6},
keywords = {spiking neural network, neuromorphic platform, mapping},
location = {Virtual Event, China},
series = {GLSVLSI '20}
}

@INPROCEEDINGS{9996702,

  author={Eissa, Sherif and Stuijk, Sander and Corporaal, Henk},

  booktitle={2022 25th Euromicro Conference on Digital System Design (DSD)}, 

  title={DNAsim: Evaluation Framework for Digital Neuromorphic Architectures}, 

  year={2022},

  volume={},

  number={},

  pages={438-445},

  doi={10.1109/DSD57027.2022.00065}}

@Article{s22197248,
AUTHOR = {Xiao, Chao and Chen, Jihua and Wang, Lei},
TITLE = {Optimal Mapping of Spiking Neural Network to Neuromorphic Hardware for Edge-AI},
JOURNAL = {Sensors},
VOLUME = {22},
YEAR = {2022},
NUMBER = {19},
ARTICLE-NUMBER = {7248},
URL = {https://www.mdpi.com/1424-8220/22/19/7248},
PubMedID = {36236344},
ISSN = {1424-8220},
ABSTRACT = {Neuromorphic hardware, the new generation of non-von Neumann computing system, implements spiking neurons and synapses to spiking neural network (SNN)-based applications. The energy-efficient property makes the neuromorphic hardware suitable for power-constrained environments where sensors and edge nodes of the internet of things (IoT) work. The mapping of SNNs onto neuromorphic hardware is challenging because a non-optimized mapping may result in a high network-on-chip (NoC) latency and energy consumption. In this paper, we propose NeuMap, a simple and fast toolchain, to map SNNs onto the multicore neuromorphic hardware. NeuMap first obtains the communication patterns of an SNN by calculation that simplifies the mapping process. Then, NeuMap exploits localized connections, divides the adjacent layers into a sub-network, and partitions each sub-network into multiple clusters while meeting the hardware resource constraints. Finally, we employ a meta-heuristics algorithm to search for the best cluster-to-core mapping scheme in the reduced searching space. We conduct experiments using six realistic SNN-based applications to evaluate NeuMap and two prior works (SpiNeMap and SNEAP). The experimental results show that, compared to SpiNeMap and SNEAP, NeuMap reduces the average energy consumption by 84% and 17% and has 55% and 12% lower spike latency, respectively.},
DOI = {10.3390/s22197248}
}


@inproceedings{qmts,
author = {Eissa, Sherif and Corradi, Federico and de Putter, Floran and Stuijk, Sander and Corporaal, Henk},
title = {QMTS: Fixed-point Quantization for Multiple-timescale Spiking Neural Networks},
year = {2023},
isbn = {978-3-031-44206-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-44207-0_34},
doi = {10.1007/978-3-031-44207-0_34},
abstract = {Spiking Neural Networks (SNNs) represent a promising solution for streaming applications at the edge that have strict performance and energy requirements. However, implementing SNNs efficiently at the edge requires model quantization to reduce memory and compute requirements. In this paper, we provide methods to quantize a prominent neuron model for temporally rich problems, the parameterized Adaptive Leaky-Integrate-and-Fire (p-ALIF). p-ALIF neurons combine the computational simplicity of Integrate-and-Fire neurons, with accurate learning at multiple timescales, activation sparsity, and increased dynamic range, due to adaptation and heterogeneity. p-ALIF neurons have shown state-of-the-art (SoTA) performance on temporal tasks such as speech recognition and health monitoring. Our method, QMTS, separates SNN quantization into two stages, allowing one to explore different quantization levels efficiently. QMTS search heuristics are tailored for leaky heterogeneous neurons. We demonstrate QMTS on several temporal benchmarks, showing up to 40x memory reduction and 4x sparser synaptic operations with little accuracy loss, compared to 32-bit float.},
booktitle = {Artificial Neural Networks and Machine Learning – ICANN 2023: 32nd International Conference on Artificial Neural Networks, Heraklion, Crete, Greece, September 26–29, 2023, Proceedings, Part I},
pages = {407–419},
numpages = {13},
keywords = {neuromorphic computing, spiking neural networks, quantization},
location = {Heraklion, Greece}
}

  

@ARTICLE{zigzag,

  author={Mei, Linyan and Houshmand, Pouya and Jain, Vikram and Giraldo, Sebastian and Verhelst, Marian},

  journal={IEEE Transactions on Computers}, 

  title={ZigZag: Enlarging Joint Architecture-Mapping Design Space Exploration for DNN Accelerators}, 

  year={2021},

  volume={70},

  number={8},

  pages={1160-1174},

  doi={10.1109/TC.2021.3059962}}

@article{
doi:10.1126/science.adh1174,
author = {Dharmendra S. Modha  and Filipp Akopyan  and Alexander Andreopoulos  and Rathinakumar Appuswamy  and John V. Arthur  and Andrew S. Cassidy  and Pallab Datta  and Michael V. DeBole  and Steven K. Esser  and Carlos Ortega Otero  and Jun Sawada  and Brian Taba  and Arnon Amir  and Deepika Bablani  and Peter J. Carlson  and Myron D. Flickner  and Rajamohan Gandhasri  and Guillaume J. Garreau  and Megumi Ito  and Jennifer L. Klamo  and Jeffrey A. Kusnitz  and Nathaniel J. McClatchey  and Jeffrey L. McKinstry  and Yutaka Nakamura  and Tapan K. Nayak  and William P. Risk  and Kai Schleupen  and Ben Shaw  and Jay Sivagnaname  and Daniel F. Smith  and Ignacio Terrizzano  and Takanori Ueda },
title = {Neural inference at the frontier of energy, space, and time},
journal = {Science},
volume = {382},
number = {6668},
pages = {329-335},
year = {2023},
doi = {10.1126/science.adh1174},
URL = {https://www.science.org/doi/abs/10.1126/science.adh1174},
eprint = {https://www.science.org/doi/pdf/10.1126/science.adh1174},
abstract = {Computing, since its inception, has been processor-centric, with memory separated from compute. Inspired by the organic brain and optimized for inorganic silicon, NorthPole is a neural inference architecture that blurs this boundary by eliminating off-chip memory, intertwining compute with memory on-chip, and appearing externally as an active memory chip. NorthPole is a low-precision, massively parallel, densely interconnected, energy-efficient, and spatial computing architecture with a co-optimized, high-utilization programming model. On the ResNet50 benchmark image classification network, relative to a graphics processing unit (GPU) that uses a comparable 12-nanometer technology process, NorthPole achieves a 25 times higher energy metric of frames per second (FPS) per watt, a 5 times higher space metric of FPS per transistor, and a 22 times lower time metric of latency. Similar results are reported for the Yolo-v4 detection network. NorthPole outperforms all prevalent architectures, even those that use more-advanced technology processes. The amount of data humans process and send around the globe on a daily basis is astonishing. However, the energy cost involved is high, and there is a strong need for designing energy-efficient devices. Modha et al. describe a chip with a neural inspired architecture, called NorthPole, that achieves substantially higher performance, energy efficiency, and area efficiency compared with other comparable architectures (see the Perspective by Iyer and Roychowdhury). A key feature of this chip is the recognition that for almost all kinds of computing, access to memory plays as important a role as logic processing. Unlike analog in-memory computing, this purely digital system has the option of tailoring the bit precision as needed, which allows for optimization of the power usage. —Peter Stern A neural chip combines memory and processing functions to achieve high-performance and high-efficiency computing on artificial intelligence–related tasks.}}

@INPROCEEDINGS{10376840,

  author={Su, Qiaoyi and Chou, Yuhong and Hu, Yifan and Li, Jianing and Mei, Shijie and Zhang, Ziyang and Li, Guoqi},

  booktitle={2023 IEEE/CVF International Conference on Computer Vision (ICCV)}, 

  title={Deep Directly-Trained Spiking Neural Networks for Object Detection}, 

  year={2023},

  volume={},

  number={},

  pages={6532-6542},

  doi={10.1109/ICCV51070.2023.00603}}


@ARTICLE{8891809,

  author={Neftci, Emre O. and Mostafa, Hesham and Zenke, Friedemann},

  journal={IEEE Signal Processing Magazine}, 

  title={Surrogate Gradient Learning in Spiking Neural Networks: Bringing the Power of Gradient-Based Optimization to Spiking Neural Networks}, 

  year={2019},

  volume={36},

  number={6},

  pages={51-63},

  doi={10.1109/MSP.2019.2931595}}
