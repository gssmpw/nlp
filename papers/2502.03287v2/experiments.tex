\section{Experiments} 
\label{sec:experiments}

We perform three experiments. First, we highlight the effects of time batching on the reuse of neuron states. For that, we perform a time batching analysis on the SEW-ResNet-18 model, as it is a fully LIF-based model. Then, we explore our memory optimization from Subsection \ref{sec:mem} on the SEW-ResNet-18 model. Finally, we perform our hybrid schedule exploration on the RED-LIF model, the SEW-ResNet-18 model, and the optimized SEW-ResNet-18 model.
%In our experiments, we first want to highlight the effect of time-batching on Conv LIF layers in SEW-7. Then, we show the results of our hybrid schedule exploration for each benchmark under tight memory constraints.

\subsection{Time batching analysis}

 In this experiment, we gradually apply time batching to the SEW-ResNet-18 model. We use a 1 MB global buffer, which is insufficient for storing all neuron states. Out of 16 timesteps (per sample), we batch 1T (ST), 2T, 4T, 8T, and 16T (TB).


%In these experiments, we gradually apply time-batching to the SEW-7 model, to show its effect in increasing the memory temporal re-use. We use a 1 MB global buffer memory, which is insufficient to store all 5.5 MB of hidden state. We start with a single timestep schedule (ST-LBL), then batch timesteps gradually (2T-LBL, 4T-LBL, 8T-LBL, TB(16T)-LBL). 

\begin{figure*}[!t]
    \centering
    \includegraphics[width=\textwidth]{Figures/sew7T.pdf}
    \caption{Effects of time batching on SEW-ResNet-18 model with 1MB on-chip memory; energy breakdown per inference.}
    %512 KB on-chip memory RED-LIF baseline schedule and optimal hybrid schedule energy breakdown per inference.
    \label{fig:sew7-tb}
\end{figure*}


As Figure \ref{fig:sew7-tb} shows, the more timesteps we batch, the less neuron states traffic to DRAM. When time is batched completely, neuron states never leak to DRAM, but are rather generated and discarded on-chip during an inference (16 timesteps). However, intermediate feature sizes grow larger the more we batch time. The spikes start to leak to DRAM after batching 4 timesteps, and it gets worse the more timesteps you batch. Applying a layer fusion schedule in combination with time batching can counteract the effect of enlarged intermediate features in time by shrinking them in space, at the cost of weight (spatial) reuse.
%This comes at reduced re-use of memory in space, which affects weight re-use. 

%Having LIF layers across the entire workload, SEW-7 is dominated by spikes and hidden states rather than weights. TB-DF schedule performs well when the model's bottleneck in memory comes from  

\subsection{SEW-ResNet-18 neuron state optimization}
\label{sec:memop}
% Deep Learning models learn features hierarchically, creating high-level semantic features in deeper layers \cite{10.1145/1553374.1553453}. Learning temporal relationships from high-level semantic features reduces the complexity and memory footprint of a model and avoids learning dynamics of low-level features which are usually unnecessary and less stable \cite{red}. Moreover, earlier blocks have larger feature maps due to pooling. 

% For DSNNs, we can remove neuron state memory from earlier blocks (i.e forget membrane potential between timesteps), to significantly reduce the model's memory footprint without compromising accuracy. For example, removing memory from the input up-sampling layer and the first 2 blocks of SEW-ResNet152 \cite{sewresnet} reduces the network's neuron state memory by 45\%. We preserve the spike-based neuron activation and surrogate gradient, to have minim effect and the same computational complexity. 

We perform an ablation study on the neuron states of the SEW-ResNet-18 model. Following the approach of Section \ref{sec:mem}, we remove the hidden states from earlier blocks while preserving the same spike-based functionality. The results of our study are shown in Table \ref{tab:sew}. We define each model by the number of blocks with neuron states (SEW-X). 

%If we examine the SEW-ResNet model, we see the first two block contain 95\% of the features and hidden states, and over 90\% of the compute. The first block alone accounts for 75\% of the model's features, hidden states, and compute. We perform an ablation study on the model's hidden state memory by removing memory from earlier blocks , as shown in Table \ref{tab:sew}. We do this while
%We define each model by the number of block containing


\begin{table}[!t]
\caption{SEW-ResNet-18 hidden state memory ablation study.}
\label{tab:sew}
\centering
\begin{tabular}{|c||c|c|c|}
\hline
Model & Test Accuracy & \# Neuron States & \% Neuron States\\
\hline
SEW-7 & 73.5\% & 46M & 100\%\\
SEW-6 & 72.1\% & 8.6M & 20\%\\
\textbf{SEW-5} & \textbf{74.3\%} & 2.3M & 5\%\\
SEW-4 & 73\% & 750K & 2\%\\
SEW-3 & 73.2\% & 360K & 1\%\\
SEW-2 & 67.1\% & 61K & 0.1\% \\
SEW-1 & 67.4\% & 12K & 0.03\%\\
SEW-0 & 65.4\% & 0 & 0\%\\
\hline
\end{tabular}
\end{table}

By removing the neuron state from the first two blocks (SEW-5), we reduce the memory of the neuron state of the model by 95\% and slightly improve its test accuracy. Note that SEW-5 has similar structural properties as ANNs and RED-LIF where most features and computation are in earlier blocks (blocks 0 and 1), and most memory is in later blocks. While in the original model (SEW-7), most features, memory, and computation are in block 0 and 1. Assuming 12-bit states and 4-bit weights, the neuron states in SEW-7 require 5.5 MB of memory, while in SEW-5 require only 0.3 MB of memory, and weights require 0.5 MB. SEW-7 and SEW-5 models have the same average sparsity rates of 93\%. Except for the input layer, both models perform SOP operations. 

\subsection{Hybrid schedule exploration}
\label{sec:Shd}

%Inter-layer optimizations are relevant when on-chip memory is insufficient for a layer-by-layer schedule. Under stringent memory requirements, a single schedule (e.g. single-timestep layer-fused ST-LF) across the whole workload can be sub-optimal. In such cases, it is better to deploy different schedules to cater to different characteristics across the workload. 


We perform hybrid schedule exploration on the RED-LIF model, and the original (SEW-7) and optimized (SEW-5) SEW-ResNet-18 models, under different on-chip memory constraints. We choose stringent memory sizes (512 KB for RED-LIF and 128 KB for SEW-ResNet-18) to highlight the benefits of inter-layer optimizations in reducing memory requirements for efficient inference. We report here a few of our results and report the rest of the results in the appendix. 

%We consecutively apply layer fusion from the first block for all models, as they have larger feature sizes compared to other blocks. While for time batching, we apply it from the last block for the RED-LIF and SEW-5 models, and from the first block for the SEW-7 model, due to their distribution of memory.

%We highlight here the best schedule results of our exploration under harsher memory constraints, to see if hybrid schedules can push down the barrier of on-chip memory requirements. For the full exploration and other on-chip memory sizes, please refer to the appendix.

%We describe each schedule by the number of blocks scheduled DF and the number of blocks schedules TB (e.g [1, 2]: 1 DF block, 2 TB blocks). The block ids are indicated in Figure 5 and Figure 6 for RED-LIF and SEW models respectively.




\subsubsection{RED-LIF}
%We explore time batching starting from the output block, and layer fusion starting from the input block, as explained in Subsection \ref{sec:hb}.
We present the results of hybrid schedule exploration for RED-LIF with a 512 KB on-chip global buffer. We explore time batching starting from the output block and layer fusion starting from the input block, since the feed-forward part contains most of the model's features, while the LIF layers contain most of the model's memory.

Figure \ref{fig:redlif-512} shows the DRAM energy consumed per inference (12 timesteps) for all 81 possible schedules, where the horizontal axis represents the number of blocks that are time-batched (in this case, from the output side) and the vertical axis represents the number of blocks that are layer-fused (always from the input side). The best schedule deploys a single-timestep layer-fused (ST-LF) schedule for the feed-forward blocks and a time-batched layer-by-layer (TB-LBL) schedule for the LIF layers. Such results agree with the claim that blocks with more features favor schedules that minimize intermediate features, while blocks with more memory favor schedules that maximize memory re-use. The optimal hybrid schedule is illustrated in Figure \ref{fig:red-best}.

\begin{figure}[t]
    \centering
    \includegraphics[width=.7\columnwidth]
    {Figures/Appendix/Meta_prototype_like_single_core_512KB-RED_LIF_T-DSEwBarr-DRAM-per_inf.pdf}
    \caption{RED-LIF hybrid schedule exploration with 512 KB on-chip memory; DRAM energy per inference.}
    \label{fig:redlif-512}
\end{figure}


Figure \ref{fig:red-sch} shows a complete breakdown of the energy consumed by the baseline schedule (ST-LBL) and the optimal hybrid schedule (ST-LF$|$TB-LBL). Inter-layer optimizations reduced off-chip data movement by 7.3x and energy consumption by 1.9x, compared to the baseline schedule. 





%We explored 9x9 different schedules for RED-LIF using a 512 KB global buffer memory. Applying ST-DF to the feed-forward residual blocks, and TB-LBL to the recurrent LIF blocks, results in the best performance among different schedules. As Figure \ref{fig:red-sch} shows, our hybrid schedule improves DRAM traffic by 7.3x, and overall energy by 1.9x, compared to the baseline ST-LBL schedule. 

\begin{figure}[t]
    \centering
    \includegraphics[width=.7\columnwidth]{Figures/redfig.pdf}
    \caption{RED-LIF baseline schedule and optimal hybrid schedule with 512 KB on-chip memory; energy breakdown per inference.}
    \label{fig:red-sch}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=.7\columnwidth]
    {Figures/RED-best.pdf}
    \caption{Optimal hybrid RED-LIF schedule with 512 KB on-chip memory.}
    \label{fig:red-best}
\end{figure}

%As discussed earlier, the feed-forward blocks seem to favor ST-DF schedules as they are dominated by features, while the Conv LIF layers seem to favor TB-LBL schedules. Indeed, after performing a schedule exploration, we find out that this is indeed the best schedule for RED-LIF under these memory constraint (512 KB global buffer), as shown in Figure X. Our hybrid schedule improves DRAM traffic by 7.3x, and overall energy by 1.9x, compared to a baseline ST-LBL schedule.



\subsubsection{SEW-ResNet-18}

We present the results of the hybrid schedule exploration for the SEW-ResNet-18 with a 128 KB on-chip global buffer. For the original model (SEW-7), we explore both time batching and layer fusion starting from the input block, as earlier blocks have  large amounts of both features and memory due to their large LIF layers. For the optimized model (SEW-5), we explore time batching starting from the output block, and layer fusion starting from the input block, as it does not have LIF neurons in the early blocks. Figure \ref{fig:sew57-sch} shows a complete breakdown of the energy consumed per inference (16 timesteps) by the baseline schedules and the optimal hybrid schedules for SEW-7 and SEW-5. %Figure \ref{fig:sew-best} illustrates the optimal hybrid schedules for the SEW-7 and SEW-5 models. 

\begin{figure*}[!t]
    \centering
    \includegraphics[width=\textwidth]{Figures/sew5v7.pdf}
    \caption{SEW-7 and SEW-5 baseline schedules and optimal hybrid schedules with 128 KB on-chip memory; energy breakdown per inference.}
    \label{fig:sew57-sch}
\end{figure*}

For the SEW-7 model, the optimal hybrid schedule prioritizes time batching (temporal reuse of memory) throughout the workload, as shown in Figure \ref{fig:sew7-best}. However, earlier blocks that have more features and less weights prioritize layer fusion to reduce the size of intermediate features at the expense of (spatial) weight reuse, while later blocks that have more weights and fewer features prioritize layer-by-layer scheduling to maximize (spatial) weight reuse at the expense of larger intermediate features. Inter-layer optimizations reduced off-chip data movement by 12x, and energy consumption by 5x, compared to the baseline schedule. 



For the SEW-5 model, the optimal hybrid schedule is similar to that of RED-LIF, as shown in Figure \ref{fig:sew5-best}. It deploys a single-timestep layer-fused schedule (ST-LF) for the first two optimized blocks, to limit the size of intermediate features (spikes). After removing the hidden state from these blocks, they no longer have a large memory. For later blocks that contain LIF neurons, it deploys a time-batched layer-by-layer schedule (TB-LBL) to maximize memory reuse in space and time, since these blocks have more memory (weights and hidden states) and fewer features (spikes). Inter-layer optimizations reduced SEW-5's off-chip data movement by 18x, and energy consumption by 2x, compared to the baseline. 

\begin{figure}[t]
    \centering
    \includegraphics[width=.7\columnwidth]
    {Figures/sew7-best-1.pdf}
    \caption{Optimal hybrid SEW-7 schedule with 128 KB on-chip memory.}
    \label{fig:sew7-best}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=.7\columnwidth]
    {Figures/sew5-best.pdf}
    \caption{Optimal hybrid SEW-5 schedule with 128 KB on-chip memory.}
    \label{fig:sew5-best}
\end{figure}

If we compare the two models' baseline schedules, SEW-5 has 5x less off-chip data movement and 4x less energy consumption than SEW-7. If we compare their best schedules, SEW-5 has 7.5x less off-chip data movement and 1.4x less energy consumption than SEW-7. The energy gains are diminished for the best schedules compared to the baseline schedules because DRAM is less dominant for the best schedules.

%The overall energy gains are diminished when comparing the optimal schedules, because DRAM consumption is less dominant in those schedules.



%prioritizes time-batching (temporal re-use of memory) across the entire workload. However, earlier blocks that have more features and less weights prioritize depth-first scheduling to reducing the size of intermediate features at the expense of (spatial) weight re-use, while later blocks that have more weights and less features prioritize layer-by-layer scheduling to maximize (spatial) weight re-use at the expense of larger intermediate features. Our inter-layer scheduling optimizations improved DRAM traffic by 12x, and improved the overall energy consumption by 5x, compared to the baseline schedule. 


%We explored 8x8 different schedules for SEW-7 using a 128 KB global buffer memory. Most performance gains are obtained when the first two blocks are time-batched, and the first block is executed depth-first. The best schedule applies time-batching on the whole workload, and only the first block (75\% of spikes) is executed depth-first. Applying TB-DF on the whole workload, as in Section 6.1, results in sub-optimal memory spatial re-use. 


% \begin{figure}[t]
%     \centering
%     \includegraphics{Figures/SEW7.pdf}
%     \caption{SEW-7 hybrid schedule exploration results, using 128 KB on-chip memory.}
%     \label{fig:sew7-sch}
% \end{figure}

% Figure \ref{fig:sew7-sch} shows all three schedules. TB-DF schedule improves DRAM by 8.8x, and energy by 4.3x, compared to ST-LBL. The hybrid schedule further improves DRAM by 1.4x, and energy by 1.1x, compared to TB-DF. In total, the hybrid schedule improves DRAM by 12x, and energy by 5x, compared to ST-LBL schedule.


% \subsubsection*{SEW-5 schedule exploration}

% We explored 8x8 different schedules for SEW-5 using a 128 KB global buffer memory. Most performance gains are obtained when the first block is executed depth-first, and when some of the output blocks are time-batched. The best schedule is similar to RED-LIF's, where it applies ST-DF schedule on the first two feed-forward blocks (no hidden state), and TB-LBL schedule on the recurrent (LIF) blocks.

% \begin{figure}[t]
%     \centering
%     \includegraphics{Figures/SEW5.pdf}
%     \caption{SEW-5 hybrid schedule exploration results, using 128 KB on-chip memory.}
%     \label{fig:sew5-sch}
% \end{figure}

% By applying our hybrid schedule, SEW-5's DRAM traffic is improved by 18x, and energy is reduced by 2x, as shown in Figure \ref{fig:sew5-sch}. If we compared SEW-5 with SEW-7, SEW-5's baseline schedule has 5x less DRAM traffic and 4x less energy compared to the SEW-7's baseline schedule, while between the optimum schedules there's a reduction in DRAM traffic by 7.5x, and a reduction in energy by 1.4x.

% SEW-5 model is computationally similar to SEW-7. However, in terms of distribution of features and memory, it has a similar structure to the RED-LIF model. Under similar memory constraints to SEW-7 (128 KB), the best schedule for SEW-5 is to apply ST-DF schedule for the first two feed-forward blocks (stateless spiking neurons), and TB-LBL schedule for the remaining blocks. 

% The baseline SEW-5 schedule has 5x less DRAM traffic and 4x less energy compared to the baseline SEW-7 schedule ,while the optimum SEW-5 schedule has 7.5x less DRAM traffic and 1.4x less energy compared to the optimum SEW-7 schedule. For the SEW-5, hybrid scheduling improved DRAM traffic by 18x, and reduced energy by 2x, as shown in Figure X.

% Optimizing hidden state memory results in a model with much smaller hidden state memory footprint, better scheduling (balanced distribution of memory and features), and has been argued to make training easier and at least as good as an unoptimized model \cite{red}. 

