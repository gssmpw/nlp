
\section{Background}\label{sec:background}
%This section provides background on DSNN models, analytical AHM, and depth-first time-first schedules. 

This section introduces the SNN algorithm commonly used in vision applications and the general behavior of SNNs, as well as inter-layer mapping (i.e., layer fusion). 

\subsection{Spiking Neural Network Algorithm}

Recent advances have enabled direct training of large SNNs for complex event-based vision applications such as object detection and recognition~\cite{sewresnet, 10376840}. SNNs have inter-layer connections (i.e. projections) similar to ANNs such as (strided) spatial convolution layers, batch normalization, residual connections, and spatial pooling. However, there are two key differences between SNNs and ANNs. SNNs operate using sparse spike-based features and have self-recurrent activation functions inspired by biological neurons, whereas ANNs operate on real-valued features and have nonrecurrent (i.e., memoryless) activation functions. SNN neuron activation function is self-recurrent because it depends on the neuron's internal state~\cite{8891809}. Additionally, SNN inference happens over time, while ANN inference is not necessarily over time.  %and non-leaky integrate-and-fire (IF) neuron models.
%\subsubsection*{Leaky Integerate-and-Fire (LIF) neuron model}

SNNs commonly implement the leaky integrate-and-fire (LIF) neuron model, especially for vision applications.This is due to its simplicity in implementation in analog and digital systems, as well as its ability to replicate basic features of biological neurons such as leakage, integration of spikes, and all-or-none action potential generation~\cite{gerstner2014neuronal}. The LIF neuron model has one recurrent state, its membrane potential ($V_{mem}$). Figure \ref{fig:lif} illustrates the dynamics of an LIF neuron. The input current is integrated into the neuron's membrane potential, and the neuron fires and resets when it exceeds its firing threshold, whereas the membrane potential leaks over time.


%During discrete timestep (a.k.a algorithmic timestep), a neuron may fire at most once. Input spikes of one timestep are integrated, followed by the neuron's activation function (i.e. leak and fire). 
%In practice, this neuron state acts as an output partial-sum which is remembered across time.
% At each discrete timestep
\begin{figure}[!t]
\centering
\includegraphics[width=.75\columnwidth]{Figures/lif-f.pdf}
\caption{Leaky Integrate-and-Fire (LIF) neuron. Colors highlight its 3 features: Leakage, integration, and fire-and-reset. The membrane potential ($V_{mem}$), which is its only state, accumulates input current and controls firing.}% }
\label{fig:lif}
\end{figure}
%($V_{mem}$) is its only state, which

SNN neurons are simulated digitally using a discrete timestep model, with inferences taking a specific number of timesteps according to the SNN model and initializing neuron states at the beginning of inference.
The LIF discrete timestep model can be simplified to

\begin{equation}
    I[t] = \sum_{proj}{W S_{in}[t]}
\end{equation}
\begin{equation}
V_{mem}[t] = \alpha V_{mem}[t-1] + I[t] - S_{out}[t-1] I_{reset}
\end{equation}
 \begin{equation}
S_{out}[t] = V_{mem}[t] \geq V_{thr} 
 \end{equation}

where $t$ is the discrete timestep, $V_{mem}$ is the membrane potential, $\alpha$ is the membrane leakage, $S_{in}$ and $S_{out}$ are the input and output spikes, respectively, $I$ is the input current, and $V_{thr}$ and $I_{reset}$ are the firing threshold and reset current, respectively. An SNN is computed by traversing these equations across space (i.e., neurons, layers) and time.

Figure \ref{fig:state} illustrates the general behavior of an SNN layer during a discrete timestep: \textbf{1.} the input spikes are integrated and modulated by weights (i.e. synapses); \textbf{2.} the input current and output spike are used to update the neuron state, including its membrane potential; \textbf{3.} if the membrane potential exceeds the firing threshold state, it fires. Neuron models can have multiple hidden states (for example, multicompartment models, adaptive models\cite{gerstner2014neuronal}).

%During input spike integration, such state acts similarly to a partial output sum accumulator in an ANN layer, while other states (e.g. threshold adaptation) are only used once per timestep during neuron activation (e.g. LIF leaking and firing).%, as shown in Figure \ref{fig:state}. 

\begin{figure}[!t]
\centering
\includegraphics[width=.65\columnwidth]{Figures/dsnn.pdf}
\caption{Computation graph of one SNN layer during one discrete timestep. The neuron states are used and updated during activation.}
\label{fig:state}
\end{figure}


\subsection{Layer Fusion in ANNs}
\label{sec:lf}

ANN workloads consist of cascaded blocks, where each block is a directed acyclic graph of layer operators such as spatial convolution, addition (i.e. residual connection), and spatial reductions. Intra-layer loop nest optimizations can improve data reuse and reduce data movement within an operator \cite{zigzag}. However, off-loading and reloading features between operators may be detrimental to performance, which is dominated by off-chip data movement. Hence, applying inter-layer optimizations by considering all operators simultaneously can provide better mapping solutions.% for a DNN.

The typical ANN inter-layer mapping is layer-by-layer (LBL), where one layer is completely scheduled before successive layers can start. In LBL schedules, complete inter-layer features are cached on-chip, if possible, or off-chip. On the other hand, layer-fused (LF) schedules, where layers are partitioned into nodes and interleaved across spatial dimensions (typically line-by-line) can significantly improve performance by reducing the size of cached intermediate features ~\cite{stream, Sioutas2020ScheduleSF, 9646923, 8667835}. This comes at the cost of reducing intra-layer weight reuse across space (\textit{spatial memory reuse}) and increasing the variable lifetime of the weights, which increases the storage of cached weights. Figure \ref{fig:dnn-sch} illustrates LBL and LF schedules on a simple 3-layer ANN block with 1D convolution kernels. 


%Layer-fused schedules interleave the execution of multiple operators, reducing the size of intermediate features. This is done by breaking each operator into smaller (one line) operators. However, this increases the variable life-time of the weights used in the schedule, as they are shared by all small intra-layer operators. Given an LF schedule of $N$ ANN operators, all operator weights are cached. In addition, some features (lines) are cached, as they are reused as input by multiple small operators. 

%Layer-fused schedules reduce the spatial size of intermediate features between layers, as shown in Figure \ref{fig:dnn-sch}, but reduce reuse of memory (weights) across spatial dimensions (\textit{spatial reuse}) and increase their variable lifetime. Layer fusion can be applied with different granularities, from executing the entire feature map (layer fusion factor 1) to executing 1 row at a time (layer fusion factor H, where H is the interleaved spatial dimension). This can be expressed by the follow relationships:
%\begin{equation}
%    MSR =\frac{H \times W}{LFF}
%\end{equation}
%\begin{equation}
%    IF = \frac{K\times H \times W}{LFF}
%\end{equation}

%where $MSR$, $IF$, and $LFF$ are the memory spatial reuse, the size of intermediate features, and the layer fusion factor respectively, and $K$, $H$, and $W$ are the featuremap channel, heights, and width dimensions. In this work, we consider row-by-row fusion, as in Figure \ref{fig:dnn-sch} ($LFF=H$).

 

\begin{figure*}[!t]
\centering
\includegraphics[width=\textwidth]{Figures/sch-23.pdf}
\caption{Illustration of (a) Layer-by-layer (LBL) schedule versus (b) layer-fused schedule for a 3-layer ANN block with 1D convolution connection (for simplicity). Each layer has output dimensions of size K x H x W, where K is number of channel and H and W are the height and width respectively. Layer-by-layer schedule can maximize reuse of weights, while layer-fused schedule reduces the size of intermediate features.} % }
\label{fig:dnn-sch}
\end{figure*}

Layer fusion is complementary to input batching. In input batching, multiple input frames are consumed simultaneously, resulting in larger intermediate features, but better reuse of weights (memory). In vision classification models, early blocks have shallow channel dimensions and large spatial dimensions. The channel dimensions grow deeper in deeper layers and the spatial dimensions shrink due to spatial pooling. Hence, earlier blocks tend to have more features, while later blocks tend to have more memory (weights). Hence, earlier blocks typically prefer layer fusion to reduce intermediate feature sizes, while later blocks typically prefer LBL schedules and input-batching to maximize memory (weights) reuse.

%\subsection{Mapping DSE tools}



%As SNNs and ANNs have similar connectivity structure, we develop STEMS by leveraging the Stream\cite{stream} and Zigzag\cite{zigzag} DSE frameworks to model SNNs. Such tools rely on analytical models to quickly explore the mapping space. 

%and explore the mapping space of DSNNs in this work by leveraging existing AHM tools for DNNs. We extend Stream\cite{stream}, which uses Zigzag\cite{zigzag} inside its pipeline. Such tools rely on models to speed up simulation and enable quick exploration of a large mapping space. 

%\subsubsection{Zigzag}

%Zigzag \cite{zigzag} is a polyhedral-based model for single layer (kernel) mapping. The hardware model consists of multiple levels of (shared) memories, and a parallel compute array. The layer's execution is described using nested loops across its different dimensions. Nested loops are broken into their prime factors to explore the temporal mapping space. 

%The temporal mapping describes the order in which a layer is executed. The loop nest prime factors are permuted to explore possible loop tiling and re-ordering. The cost estimation model extracts key mapping metrics such as data reuse, memory access, and data bandwidth between memories. The hardware model translates these metrics to energy and latency estimates. By leveraging its fast analytical model, Zigzag can explore many possible loop permutations to find pareto solutions within its search space.

%\subsubsection{Stream}

%Stream~\cite{stream} is a graph-based framework for fine-grained multi-layer scheduling on heterogeneous multi-core accelerators. Stream users can cut layers of a workload graph along different dimensions, forming smaller computation nodes and a computation node graph. Each unique computation node to core mapping is evaluated using Zigzag's mapping and cost estimation model (energy and latency). Computation nodes are scheduled depth-first. Hence, workload cuts along spatial dimensions create a layer-fused schedule. Computation nodes are mapped to cores using a genetic algorithm. During scheduling, Stream manages on-chip memories (i.e data eviction policies) and estimates data movement between cores and between a core and off-chip memory. 




% AHM tools, such as Zigzag \cite{zigzag} and Timeloop\cite{9923807}, model hardware architectures consisting of multi-level memories and parallel compute arrays. Such tools can map linear kernels, consisting of weights, inputs and outputs, onto hardware architectures. Analytical models and search heuristics have gained popularity due to the large mapping space which prohibits accurate simulation of all design point.

% Possible mappings are explored via loop tiling and re-ordering, which define the temporal dataflow and how different operands are stored at each memory level. Such mapping choices affect data reuse and data access counts of different operands at different memory levels. These effects are evaluated over an analytical hardware model, producing energy and latency estimates. 

% Stream \cite{stream} extends Zigzag for multi-layer multi-core mapping. In Stream, the workload graph is broken into a finer computation node graph, based on user-defined intra-layer cuts. Then, the mapping of each unique node to each unique core is analytically explored using Zigzag. Finally, a genetic algorithm is used to allocate each layer to a core, where performance is evaluated using an inter-node scheduling heuristic that prioritizes latency or memory.




