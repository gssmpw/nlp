\section{Conclusion} \label{sec:conclusion}

In this study, we investigated inter-layer mapping optimizations for event-based vision SNNs models using STEMS, our mapping exploration tool for SNNs. STEMS is the first tool to provide intra-layer and inter-layer mapping optimizations for SNNs and temporal workloads by extending recent tools to support SNNs and explore the spatio-temporal mapping space. Additionally, we enable our exploration by effectively trimming the inter-layer mapping space.

Our experiments show the benefits of layer fusion and time batching to the different stages of SNN models and hybrid SNN models; where time batching improves reuse of neuron states and weights (memory), while layer fusion helps reduce the size of intermediate spikes/features. Under stringent memory requirements, we have shown up to 12x reduction in off-chip traffic and up to 5x reduction in energy consumption. We have also shown how to turn an SNN model into a hybrid model, by minimally changing the neuron model to remove neuron states. We have shown how hybrid models can have a better structure for scheduling, better energy efficiency (1.4x), less memory footprint (20x less states), and even better task accuracy for event-based vision tasks. 


%make several key insights into the mapping problem, including \Circled{1} the benefits generally preferred schedules by DSNNs under tight memory requirements, where we showed tendency toward time-batching and a tendency for earlier blocks to combine time-batching with layer-fusion (depth-first), \Circled{2} generally preferred schedules by hybrid models under tight memory requirements, where we showed that feed-forward blocks have a tendency towards layer-fusion and recurrent blocks have a tendency towards time-batching, \Circled{3} the benefits of hybrid models over DSNNs, in memory requirements, scheduling, and even accuracy, as well as \Circled{4} means to structure and optimize DSNNs like hybrid models, and experiments showing its benefits in scheduling, energy, and accuracy. Our experiments demonstrate significant improvements in external data traffic by up to 18x, and in overall energy consumption by up to 5x, using inter-layer scheduling optimizations under tight memory requirements.


% In conclusion, we provide valuable insights into scheduling DSNNs and we offer model optimization.



% insights:
% 1 Algorithm and scaling granularity are interrelated. Compared to MinMax, LSQ is more performant. POC scaling only improves accuracy when operating in a low-bit parameter-efficient regime.
% 2 No straightforward relationship exists between energy consumption and quantization precision.
% 3 LBL scheduling saves energy, with greater benefits for feature-dominant networks
% 4 The majority of energy consumed is due to DRAM, mainly caused by fetching all weights once
% 5 Employing the academic graph results in increased accuracy, but may result in DRAM spilling and, therefore, high energy cost.
% 6 As the width multiplier increases, accuracy increases with a decreasing rate.
% 7 Energy increases exponentially with the width multiplier.
% 8 INT4 quantization combined with academic graph structure is superior to other precision levels.
% Most important: 8, 5, 4, 2




  % regular IEEE prefers the singular form
\section*{Acknowledgment}
This work has been funded by the Dutch Organization for Scientific Research (NWO) as part of P16-25 eDL project 7.