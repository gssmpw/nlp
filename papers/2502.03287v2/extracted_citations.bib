@inproceedings{10.1145/3192366.3192371,
author = {Lin, Chit-Kwan and Wild, Andreas and Chinya, Gautham N. and Lin, Tsung-Han and Davies, Mike and Wang, Hong},
title = {Mapping spiking neural networks onto a manycore neuromorphic architecture},
year = {2018},
isbn = {9781450356985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3192366.3192371},
doi = {10.1145/3192366.3192371},
abstract = {We present a compiler for Loihi, a novel manycore neuromorphic processor that features a programmable, on-chip learning engine for training and executing spiking neural networks (SNNs). An SNN is distinguished from other neural networks in that (1) its independent computing units, or "neurons", communicate with others only through spike messages; and (2) each neuron evaluates local learning rules, which are functions of spike arrival and departure timings, to modify its local state. The collective neuronal state dynamics of an SNN form a nonlinear dynamical system that can be cast as an unconventional model of computation. To realize such an SNN on Loihi requires each constituent neuron to locally store and independently update its own spike timing information. However, each Loihi core has limited resources for this purpose and these must be shared by neurons assigned to the same core. In this work, we present a compiler for Loihi that maps the neurons of an SNN onto and across Loihi's cores efficiently. We show that a poor neuron-to-core mapping can incur significant energy costs and address this with a greedy algorithm that compiles SNNs onto Loihi in a power-efficient manner. In so doing, we highlight the need for further development of compilers for this new, emerging class of architectures.},
booktitle = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {78–89},
numpages = {12},
keywords = {spiking neural networks, small-world networks, neuromorphic computing, neuromorphic architectures, compilers},
location = {Philadelphia, PA, USA},
series = {PLDI 2018}
}

@inproceedings{10.1145/3386263.3406900,
author = {Li, Shiming and Guo, Shasha and Zhang, Limeng and Kang, Ziyang and Wang, Shiying and Shi, Wei and Wang, Lei and Xu, Weixia},
title = {SNEAP: A Fast and Efficient Toolchain for Mapping Large-Scale Spiking Neural Network onto NoC-based Neuromorphic Platform},
year = {2020},
isbn = {9781450379441},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3386263.3406900},
doi = {10.1145/3386263.3406900},
abstract = {Spiking neural network (SNN), as the third generation of artificial neural networks, has been widely adopted in vision and audio tasks. Nowadays, many neuromorphic platforms support SNN simulation and adopt Network-on-Chips (NoC) architecture for multi-cores interconnection. However, a large volume and run-time communication on the interconnection has a significant effect on performance of the platform. In this paper, we propose a toolchain called SNEAP (Spiking NEural network mAPping toolchain) for mapping SNNs to neuromorphic platforms with multi-cores, which aims to reduce the energy and latency brought by spike communication on the interconnection.SNEAP includes two key steps: partitioning the SNN to reduce the spikes communicated between partitions, and mapping the partitions of SNN to the NoC to reduce average hop of spikes under the constraint of hardware resources. SNEAP effectively reduces the energy and latency on the NoC-based neuromorphic platform and spend less time than other toolchains.The experimental results show that SNEAP can achieve average 418X reduction in end-to-end execution time, and reduce energy consumption and spike latency, on average, by 23\% and 51\% respectively, compared with SpiNeMap.},
booktitle = {Proceedings of the 2020 on Great Lakes Symposium on VLSI},
pages = {9–14},
numpages = {6},
keywords = {spiking neural network, neuromorphic platform, mapping},
location = {Virtual Event, China},
series = {GLSVLSI '20}
}

@inproceedings{10.1145/3531437.3539704,
author = {Sharma, Deepika and Ankit, Aayush and Roy, Kaushik},
title = {Identifying Efficient Dataflows for Spiking Neural Networks},
year = {2022},
isbn = {9781450393546},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531437.3539704},
doi = {10.1145/3531437.3539704},
abstract = {Deep feed-forward Spiking Neural Networks (SNNs) trained using appropriate learning algorithms have been shown to match the performance of state-of-the-art Artificial Neural Networks (ANNs). The inputs to an SNN layer are 1-bit spikes distributed over several timesteps. In addition, along with the standard artificial neural network (ANN) data structures, SNNs require one additional data structure – the membrane potential (Vmem) for each neuron which is updated every timestep. Hence, the dataflow requirements for energy-efficient hardware implementation of SNNs can be different from the standard ANNs. In this paper, we propose optimal dataflows for deep spiking neural network layers. To evaluate the energy and latency of different dataflows, we considered three hardware architectures with varying on-chip resources to represent a class of spatial accelerators. We developed a set of rules leading to optimum dataflow for SNNs that achieve more than 90\% improvement in Energy-Delay Product (EDP) compared to the baseline for some workloads and architectures.},
booktitle = {Proceedings of the ACM/IEEE International Symposium on Low Power Electronics and Design},
articleno = {2},
numpages = {6},
keywords = {artificial neural networks, dataflows, deep spiking neural networks},
location = {Boston, MA, USA},
series = {ISLPED '22}
}

@ARTICLE{7738524,
  author={Chen, Yu-Hsin and Krishna, Tushar and Emer, Joel S. and Sze, Vivienne},
  journal={IEEE Journal of Solid-State Circuits}, 
  title={Eyeriss: An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks}, 
  year={2017},
  volume={52},
  number={1},
  pages={127-138},
  keywords={Shape;Random access memory;Computer architecture;Throughput;Clocks;Neural networks;Hardware;Convolutional neural networks (CNNs);dataflow processing;deep learning;energy-efficient accelerators;spatial architecture},
  doi={10.1109/JSSC.2016.2616357}}

@INPROCEEDINGS{8342201,

  author={Das, Anup and Wu, Yuefeng and Huynh, Khanh and Dell'Anna, Francesco and Catthoor, Francky and Schaafsma, Siebren},

  booktitle={2018 Design, Automation \& Test in Europe Conference \& Exhibition (DATE)}, 

  title={Mapping of local and global synapses on spiking neuromorphic hardware}, 

  year={2018},

  volume={},

  number={},

  pages={1217-1222},

  doi={10.23919/DATE.2018.8342201}}

@INPROCEEDINGS{9996702,

  author={Eissa, Sherif and Stuijk, Sander and Corporaal, Henk},

  booktitle={2022 25th Euromicro Conference on Digital System Design (DSD)}, 

  title={DNAsim: Evaluation Framework for Digital Neuromorphic Architectures}, 

  year={2022},

  volume={},

  number={},

  pages={438-445},

  doi={10.1109/DSD57027.2022.00065}}

@misc{das2022realtime,
      title={Real-Time Scheduling of Machine Learning Operations on Heterogeneous Neuromorphic SoC}, 
      author={Anup Das},
      year={2022},
      eprint={2209.14777},
      archivePrefix={arXiv},
      primaryClass={cs.AR}
}

@misc{huynh2022implementing,
      title={Implementing Spiking Neural Networks on Neuromorphic Architectures: A Review}, 
      author={Phu Khanh Huynh and M. Lakshmi Varshika and Ankita Paul and Murat Isik and Adarsha Balaji and Anup Das},
      year={2022},
      eprint={2202.08897},
      archivePrefix={arXiv},
      primaryClass={cs.NE}
}

@InProceedings{neuproma,
author="Xiao, Chao
and Chen, Jihua
and Wang, Lei",
editor="Liu, Shaoshan
and Wei, Xiaohui",
title="NeuProMa: A Toolchain for Mapping Large-Scale Spiking Convolutional Neural Networks onto Neuromorphic Processor",
booktitle="Network and Parallel Computing",
year="2022",
publisher="Springer Nature Switzerland",
address="Cham",
pages="129--142",
abstract="Neuromorphic processors, the new generation of brain-inspired non-von Neumann computing systems, have the potential to perform complex computations with more energy efficiency than conventional architectures. Neuromorphic processors typically implement the spiking neural network (SNN)-based applications. However, a non-optimized mapping of SNNs onto the neuromorphic processor may increase the on-chip communication delay and data exchange between the off-chip and on-chip memory, especially when the size of the SNNs exceeds the capacity of the processor limited by the on-chip resources. This paper proposes a toolchain, called NeuProMa, to map large-scale spiking convolutional neural networks (SCNNs) onto resource-constrained neuromorphic processors. We exploit the implicit regular connections in the SCNNs and split the SCNNs into multiple sub-networks while reducing the data exchange between the off-chip and on-chip memory. Then, we partition the sub-networks into multiple clusters sequentially in a specific order, which significantly reduces the spike messages between neuromorphic cores. Finally, NeuProMa dispatches the clusters to the neuromorphic cores, minimizing the maximum workload of the routers. Our experiments using six SCNN-based applications show that NeuProMa can significantly reduce the data exchange between the off-chip and on-chip memory, and reduce the spike latency and energy consumption by up to 17{\%} and 85{\%}, respectively, compared with the state-of-the-art.",
isbn="978-3-031-21395-3"
}

@ARTICLE{stream,
  author={Symons, Arne and Mei, Linyan and Colleman, Steven and Houshmand, Pouya and Karl, Sebastian and Verhelst, Marian},
  journal={IEEE Transactions on Computers}, 
  title={Stream: Design Space Exploration of Layer-Fused DNNs on Heterogeneous Dataflow Accelerators}, 
  year={2025},
  volume={74},
  number={1},
  pages={237-249},
  keywords={Computer architecture;Resource management;Micromechanical devices;Computers;Space exploration;Random access memory;Radio frequency;Processor scheduling;Memory management;Artificial neural networks;Deep neural networks;layer fusion;design space exploration;heterogeneous systems;dataflow;accelerators},
  doi={10.1109/TC.2024.3477938}}

@inproceedings{titirsha2020thermal,
  title={Thermal-aware compilation of spiking neural networks to neuromorphic hardware},
  author={Titirsha, Twisha and Das, Anup},
  booktitle={International Workshop on Languages and Compilers for Parallel Computing},
  pages={134--150},
  year={2020},
  organization={Springer}
}

@article{titirsha2021endurance,
  title={Endurance-aware mapping of spiking neural networks to neuromorphic hardware},
  author={Titirsha, Twisha and Song, Shihao and Das, Anup and Krichmar, Jeffrey and Dutt, Nikil and Kandasamy, Nagarajan and Catthoor, Francky},
  journal={IEEE Transactions on Parallel and Distributed Systems},
  volume={33},
  number={2},
  pages={288--301},
  year={2021},
  publisher={IEEE}
}

@ARTICLE{zigzag,

  author={Mei, Linyan and Houshmand, Pouya and Jain, Vikram and Giraldo, Sebastian and Verhelst, Marian},

  journal={IEEE Transactions on Computers}, 

  title={ZigZag: Enlarging Joint Architecture-Mapping Design Space Exploration for DNN Accelerators}, 

  year={2021},

  volume={70},

  number={8},

  pages={1160-1174},

  doi={10.1109/TC.2021.3059962}}

