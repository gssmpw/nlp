\section{Methods}
\label{sec:methods}
%we presents the methods of our study. First, we demonstrate how
We present an overview on STEMS and its stages, as well as the inter-layer mapping space explored with STEMS. We also motivate and explain our neuron state reduction exploration for spatial (temporal) tasks such as event-based vision.

%Mapping DSE for SNNs is under-explored. To explore loop nest optimizations and inter-layer optimzations for SNNs, we develop STEMS. STEMS is based on existing mapping DSE tools for ANNs, namely Zigzag and Stream. In addition, we motivate and explain how we explore neuron state reduction for spatial-temporal tasks such as event-based vision.

%We also motivate and explain our neuron state memory optimization for SNNs solving spatial-temporal tasks such as event-based vision. 

\subsection{STEMS overview}
\label{sec:ahm}

%\subsubsection*{Tool overview}

STEMS is a mapping design space exploration tool for SNNs based on Stream, a state-of-the-art tool for multi-core ANN mapping \cite{stream}. STEMS explores both intra-layer mapping with loop nest optimizations, and inter-layer mapping with workload partitioning and scheduling along spatial (i.e., layer fusion) and temporal dimensions (i.e. time batching). Compared to Stream, STEMS supports temporal and stateful workloads such as SNNs and enables their inter-layer mapping exploration along spatial and temporal dimensions.

%\textcolor{red}{To-do: Add overview figure?}

%to-do add figure
Figure \ref{fig:stems} shows an overview of STEMS. STEMS breaks down the mapping problem into pipeline stages similar to Stream. STEMS has three user-defined inputs: the workload and accelerator descriptions, and user-defined workload cuts along spatial and temporal dimensions. STEMS pipeline can be described in the following four stages: \textbf{1.} user input is parsed, and the workload and accelerator models are generated; \textbf{2.} fine-grain workload graph is generated according to user-defined workload cuts; \textbf{3.} intra-layer mapping is generated by optimizing each unique tile-core mapping; \textbf{4.} inter-layer schedule is generated by scheduling the fine-grained workload graph in a specific order while managing hardware resources. For multi-core accelerators, a genetic algorithm can be used in this stage to explore layer-to-core assignments.

\begin{figure}[t]
    \centering
    \includegraphics[width=.45\columnwidth]{Figures/STEMSv2-cropped.pdf}
    \caption{Overview of the STEMS framework.}

    \label{fig:stems}
\end{figure}

%



\subsection{STEMS components}


\subsubsection{Input parsing}

In this stage, the workload and accelerator models are generated from user input. 

The accelerator model consists of an external memory and one or more computation cores connected in a NoC. The NoC has a mesh structure and is defined by the data bandwidth and data movement costs between cores and between any core and external memory. Only one core can request data from the external memory at a time. Each core is described by a memory hierarchy that can store operand tiles at different memory levels, and an array of parallel processing elements. The memory blocks in the hierarchy are defined by their size, bandwidth (bits per cycle), and energy (per bit). While the processing elements are defined by their energy per operation, size, and unrolled dimensions.

The workload is defined as an acyclic graph of operators. Each operator consists of a set of operands (e.g Weights, Neuron States, Features) and a set of dimensions (e.g Time, Space, Channels) and a set of linear projections defined by the relationship between operands (e.g Convolution). 

To support SNNs and temporal workloads, neuron states and time are incorporated as additional workload features. Two types of neuron state can be supported: the accumulator state (i.e. membrane potential of LIF neurons) and other states used during neuron activation (i.e. threshold adaptation, multi-compartment neuron states). Each operator has a block id that defines the granularity of user-defined workload cuts.

%A block id is also used to define the granularity of hybrid schedules. The hybrid schedules used for each block are explained later in the hybrid schedule exploration section.

\subsubsection{Fine-grain workload graph generation}

In this stage, a fine-grained workload graph is generated out of the workload graph and user-defined workload cuts. Such cuts define how the operators in each block are partitioned into smaller \textit{computation tiles} (CT) that are used in intra- and inter- layer mapping stages. Such partitioning also defines how these computation tiles will be scheduled in the inter-layer mapping stage. 

To support SNNs and temporal workloads, we introduced the temporal dimension to user-defined cuts and the graph generation process. Each operator can be partitioned along the spatial dimension, the temporal dimension, or both. The fine-grain graph is generated with proper inter-layer edges, as well as intra-layer edges. Intra-layer edges ensure a proper temporal schedule and an efficient spatial schedule. The IDs of the computation tiles include spatial and temporal dimensions.


\subsubsection{Intra-layer mapping}

In this stage, energy and latency are estimated for each unique CT-core combination using Zigzag \cite{zigzag}. If a CT does not fit on a core, the external memory is added to the core during cost estimation. Zigzag explores intra-layer mapping by breaking down the mapping space to loop prime factors. These factors are permuted and partitioned into different memory levels for each operand. For each valid mapping (i.e. tiles fit into their allocated memories), latency and energy are estimated. The latency and energy models translate the temporal mapping into data movement rates and volumes and allocated memory sizes. The latency model can identify memory-bound and compute-bound schedules and estimate latency accordingly. Memory-bound schedules occur because of data movement rates that are larger than the available memory bandwidth. The energy model is based on the volume of data movement between different levels of memory and the number of operations. We configure Zigzag to choose the best cost estimates based solely on energy.

To support SNNs, neuron state behavior and time are incorporated into Zigzag. Neuron states are added as explicit operands and their behavior is modeled into the memory allocation and data movement models. As a result, their costs are reflected in the latency and energy models. On the other hand, time is added as an explicit dimension of workloads (e.g., SNN layers), which increases the size of the temporal mapping space. The neuron states are reused over different timesteps; however, there is a chronological dependency between neuron updates over time, due to the non-linear activation at the end of each timestep. In other words, evaluating a neuron (and updating its state) in one timestep requires evaluating it first for previous timesteps. This scheduling constraint limits and reduces the temporal mapping space, where input-relevant loops (e.g. input channels, kernel projection) need to be iterated before temporal loops can be iterated (and possibly batched together). Between successive temporal iterations, all iterated neurons (i.e., iterated output-relevant loops such as output channels and spatial dimensions) are activated, and their output spikes are generated.


\subsubsection{Inter-layer mapping}
\label{sec:schedule}
In this stage, computation tiles are scheduled according to layer-to-code allocations. For multi-core accelerators, different layer-to-core allocations are explored using a genetic algorithm which optimizes latency and energy. Latency and energy costs of one allocation are generated using an inter-core graph scheduler. 

The inter-layer scheduling order is defined according to the workload partitioning. If a layer is spatially partitioned, it is said to be in a layer-fused schedule. Additionally, a new scheduling dimension is introduced for SNNs and other recurrent networks; time (sequence). In contrast to layer fusion (Section \ref{sec:lf}), batching multiple timesteps enlarges intermediate features. However, it increases memory reuse (neuron state and weights) across time (\textit{temporal memory reuse}). Such effects are similar to input batching. In addition, neuron states are typically reset to zero at the beginning of an inference. Hence, batching all timesteps maximizes reuse of memory over time and may completely eliminate the off-chip data movement of neuron states under memory constraints. We call this a time-batched schedule (TB), in contrast to single-timestep schedule (ST) where only one timestep is scheduled at a time. Between TB and ST, we can batch just a few timesteps (e.g., 2T, 4T, 8T).

Temporal techniques can be applied orthogonal to spatial techniques (LBL and LF), resulting in different spatio-temporal schedules. In Figure \ref{fig:schedules}, we illustrate four combinations of spatio-temporal schedules which we use in our inter-layer schedule exploration: \textit{single-timestep layer-by-layer} (ST-LBL), \textit{time-batched layer-by-layer} (TB-LBL), \textit{single-timestep layer-fused} (ST-LF), and \textit{time-batched layer-fused} (TB-LF). Each schedule provides a different trade-off between spatial memory reuse, temporal memory reuse, and intermediate feature sizes. Batching $T$ timesteps \textit{increases} size of intermediate features and temporal memory reuse by $T$, while layer fusion \textit{decreases} size of intermediate features and spatial memory reuse, resulting in the different schedule properties highlighted in Figure \ref{fig:schedules}. Workload blocks with large feature sizes tend to profit from layer fusion, as it reduces the size of intermediate features, while workload blocks with large memory sizes (weights and/or neuron states) tend to profit from time batching, as it increases memory reuse across time. These four types of schedules can be applied separately to different blocks in an SNN, catering to different block characteristics (memory vs. features). In addition to these schedule, we can have different degrees of time batching resulting in different degrees of temporal reuse and intermediate feature sizes. The inter-layer schedule is generated according to user-defined cuts, as in Figure \ref{fig:schedules}.

%To enable spatial-temporal schedules, STEMS cuts the workload into computation nodes along spatial and temporal dimensions and schedules them accordingly as in Figure \ref{fig:schedules}.


%Recurrent networks have persistent features (a.k.a hidden states), that are remembered during an inference sequence. Such states are necessary to provide the network with a temporal memory to correlate features over time (sequence).


%The natural way to schedule an SNN workload across time is in chronological order. In such case, the complete workload graph is executed for one timestep, before the following timestep can start. We call this a single-timestep schedule (ST). Similar to input batching, multiple timesteps can be batched together (i.e. evaluated consecutively in one computation node). We can batch a few timesteps (e.g. 2T, 4T, 8T) or we can batch an entire inference sequence. We refer to a fully batched schedule as time-batched (TB) schedule, in contrast to single-timestep (ST) schedule.  

%If a computation node is partitioned along the spatial dimension, it is fused with other spatially-partitioned neighboring nodes. While temporal partitions define how timesteps are batched and scheduled together in one computation node. In Figure \ref{fig:schedules}, we illustrate four combinations of spatial-temporal schedules which we use in our: \textit{single-timestep layer-by-layer} (ST-LBL), \textit{time-batched layer-by-layer} (TB-LBL), \textit{single-timestep layer-fused} (ST-LF), and \textit{time-batched layer-fused} (TB-LF). Each schedule provides a different trade-off between spatial memory re-use, temporal memory re-use, and intermediate feature sizes. Time batching $T$ timesteps ($TBF=T$) \textit{increases} size of intermediate features and temporal memory re-use by $T$, while layer fusion ($LFF=H$) \textit{decreases} size of intermediate features and spatial memory re-use, resulting in the different schedule properties highlighted in Figure \ref{fig:schedules}. Feature-intense blocks tend to profit from layer fusion as it reduces size of intermediate features, while memory-intense blocks tend to profit from time batching as it increases memory re-use across time. These four types of schedules can be applied separately to different blocks in an SNN, catering to different block characteristics (memory vs features). In addition to these schedule, we can have different degrees of time batching resulting in different degrees of temporal re-use and intermediate feature sizes. To enable spatial-temporal schedules, STEMS cuts the workload into computation nodes along spatial and temporal dimensions and schedules them accordingly as in Figure \ref{fig:schedules}.

\begin{figure*}[!t]
    \centering
    \includegraphics[width=\textwidth]{Figures/snn-sch-2.pdf}
    \caption{Illustration of different inter-layer schedules: (a) single-timestep layer-by-layer (ST-LBL) schedule, (b) time-batched layer-by-layer (TB-LBL) schedule, (c) single-timestep layer-fused (ST-LF) schedule, and (d) time-batched layer-fused (TB-LF) schedule for a 3-layer 3-timestep SNN block with 1D convolution connection (for simplicity). Some arrows are removed for readability. Each layer has output dimensions of size T x K x H x W, where K is number of channel, T is the number timesteps, and H and W are the height and width respectively. Time batching increases reuse of memory over time (temporal memory reuse) at the expense of larger intermediate feature sizes, while layer-fusion decreases reuse of memory in space (spatial memory reuse) but has smaller intermediate feature sizes.}
    \label{fig:schedules}
\end{figure*}

During scheduling, a memory management unit and a communication management unit are used to orchestrate inter-layer schedule. The communication management unit manages inter-core data links and schedules inter-core and off-chip data transfers, while the memory management unit manages and allocates necessary memory space on each core. We assume a scratchpad memory structure. If a scheduled computation tile is too large to fit on its core, all tensors stored on the core's scratchpad memory (other than those required) are evicted, and the external memory data link is blocked to service the current computation tile. Otherwise, tensors are only evicted to free up scratchpad memory space if necessary or if they are no longer needed. The tensors to be evicted are ranked based on their size and priority, where priority is the number of times this tensor would be used in the future.

%\textcolor{red}{In our experiments, we test several tensor eviction policies. Such policies rank caches tensors according to their size, priority (how many times each tensor will be used by future computation nodes), and whether or not they need to be written back to external memory (as constant operands are not written back). We go into more details on these policies in Appendix [ref]}.

%In our experiments, we test different cache eviction policies that are functions of tensor sizes, remaining fan-out nodes, and whether the tensor requires write-back to external memory or not.


\subsection{Hybrid Schedule Exploration}
\label{sec:hb}
%[to-do address comment on what is N]

Given a workload with $N$ blocks, we would like to explore different possible schedules per block. In this exploration, we consider the spatio-temporal schedules in Figure \ref{fig:schedules}. An exhaustive search of this space results in $4^N$ possible schedules, which is not feasible for larger network sizes. Instead, we apply our knowledge regarding the trade-offs of these schedules. We rank blocks according to their feature sizes and order, and according to their memory size and order. We apply layer fusion and/or time batching to blocks in this ranking order. This results in $N+1$ possible ways (from $0$ blocks to $N$ blocks) to apply fusion or time batching, resulting in a total of $(N+1)^2$ possible schedules.

% We use STEMS to explore possible hybrid spatial-temporal SNN schedules. As different blocks have different properties in terms of computation, features and memory, they may favor different schedules. In our experiments, we explore the four possible schedules in Figure \ref{fig:schedules} for each workload block (defined by block id). However, this results in $4^N$ possible schedules for a workload with $N$ blocks. 

% To improve the scaling of this exploration, we trim this space by applying fusion and/or batching in a certain order. We rank each block according to the size of features, and according to the size of memory. Fusion is applied to the blocks with more features first, while batching is applied to blocks with more memory first. Hence, there are only $N+1$ ways to fuse blocks, and $N+1$ ways to batch blocks. This results in a total of $(N+1)^2$ schedules.

%Layer fusion is applied to the most feature-intense blocks first before it is applied to less feature-intense blocks, while time batching is applied to the most memory-intense blocks first before it is applied to less memory-intense blocks. This reduces the exponential scaling to a polynomial scaling.



%We start with fusion from blocks with larger spatial dimensions, and time batching from blocks with more memory. Hence, for an SNN with $N$ blocks, we only have $N+1$ possible spatial schedules and $N+1$ possible temporal schedules. This significantly reduces the search space from $4^N$ to $(N+1)^2$.

% The relationship between spatial-temporal schedules, intermediate features, and memory re-use can expressed as follows:


% \begin{equation}
%     MSR =\frac{H \times W}{LFF}
% \end{equation}
% \begin{equation}
%     MTR = TBF
% \end{equation}
% \begin{equation}
%     IF = \frac{TBF \times K\times H \times W}{LFF}
% \end{equation}

% where $MSR$, $MTR$, $IF$, $LFF$, and $TBF$ are the memory spatial re-use, memory temporal re-use, size of intermediate features, layer fusion factor, and time-batching factor respectively, and $T$, $K$, $H$, and $W$ are the featuremap time, channel, heights, and width dimensions. 



% \subsection{SNN mapping support}

% To support SNNs, we extend the state-of-the-art with two key features of SNNs: time, and neuron states. Additionally, we define the spatial-temporal mapping space for SNNs and how we effectively reduce it.

% \subsubsection{Neuron State}

%SNN's self-recurrent behavior stems from its persistent neuron states.

%We introduced the behavior of SNN neuron states

%We leverage this knowledge by measuring the re-use and data movement of LIF membrane potential (i.e. its hidden state) similar to a partial output sum accumulator. For other neuron states, we only need to consider loading and updating them once every timestep during neuron activation. Additionally, we ignore the initial load and final store of states, due to network reset at the beginning of an inference. 


%STEMS models neuron state as an explicit operand, with other operands such input, output spikes, and weights. As a result, it reflects the effects of mapping on memory access, data re-use, data transfers, and allocated memory on the intra-layer cost model. Additionally, when a computation node is scheduled, STEMS ensures that its respective neuron states are loaded on-chip, and when it is ejected from on-chip memory it has to be written-back off-chip.











% Include here the four schedule89uuuuuuuuuuur5589999999999999999999999999999999999999999999999999990ft

%Time is a natural component of SNN computation. Neuron models are approximated using a discrete timestep model. SNN inference (i.e execution) is performed over a specific time duration (number of discrete timesteps). Time is incorporated as an extra dimension in the SNN's workload. Algorithm~\ref{alg:snn} shows the behavior of a single LIF neuron over time. At each discrete timestep, input spikes are integrated into a neuron's membrane potential, and it is activated once every timestep where it may spike and reset. While input and output features/spikes are time dependent, the neuron's hidden state is time independent (same state is re-used across time). However, there is a scheduling constraint where neuron states must be updated in a chronological order. This constraint limits the temporal mapping space, where input-relevant loops (e.g. input channels, kernel projection) need to be iterated before temporal loops can be iterated (and possibly batched together). This can be seen in Algorithm \ref{alg:alg1}, where input loops are written below the temporal loops. Between successive temporal iterations, all iterated output-relevant (e.g. output channels, spatial dimensions) neurons needs to be activated and their output spikes for the current timestep are generated.

% \begin{algorithm}[H]
% \caption{LIF Neuron computation over time}\label{alg:alg1}
% \begin{algorithmic}

% \FOR{T in time}
% \FOR{I in Inputs}
%     \STATE $V_{mem} += I[T] \times W$ 
%     \hfill\algorithmiccomment{integrate input spikes}
% \ENDFOR
%     \STATE $S_{out}[T], V_{mem} = LIF(V_{mem})$
%     \hfill\algorithmiccomment{activate neuron (i.e leak and fire)}
% \ENDFOR
% \end{algorithmic}
% \label{alg:snn}
% \end{algorithm}

% Batching multiple timesteps increases memory re-use (both hidden state and weights) across time, similar to input batching effect. Additionally, DSNN hidden states are typically reset to zero at the beginning of an inference sequence. Hence, batching all timesteps in a DSNN inference maximizes reuse of hidden states and may completely eliminate their external memory traffic under memory constraints.

 % \begin{figure}[!t]
% \centering
% \includegraphics[width=\columnwidth]{figures/dsnn.pdf}
% \caption{Computation graph of one SNN layer. The neuron hidden states are used and updated during activation.}
% \label{fig:state}
% \end{figure}

  

%Our tool is based on Zigzag and Stream. 

%First, we explain how we incorporate two key features to model SNNs: neuron state and time. By modelling SNN features in Zigzag and Stream, we can apply intra-layer mapping optimizations by leveraging Zigzag's temporal mapping engine. Then we introduce inter-layer spatial-temporal schedules, which we enable with our scheduler. Finally, we define our trimmed inter-layer mapping space which we explore in our experiments.

%we explain how we enable inter-layer mapping optimizations by  extend the inter-layer scheduler to explore inter-layer mapping in spatial and temporal dimensions. 



% Neuron states are internal memories that evolve over time. The LIF model has one internal state per neuron; the membrane potential ($V_{mem}$). More complex neuron models can have more than one neuron state (e.g multi-compartment models, adaptive models) \cite{gerstner2014neuronal}. The output spikemap of an SNN layer is a function of its inputs and its internal state, as was illustrated in Figure \ref{fig:state}.


%We implement neuron states as explicit operands, which are updated and remembered over time. While neuron states are stored and loaded between timesteps, the initial load and final store in an inference sequence are often unnecessary, as most SNN models are reset between them. The membrane potential of a neuron acts similar to output partial-sums in ANNs, with the same data re-use opportunities. Other neuron states, such as adaptive threshold, are applied point-wise to the activation function, with no re-use opportunities within the same timestep \cite{gerstner2014neuronal}. By modelling neuron states as explicit operands, the effects of their mapping on memory access, data re-use, data transfers, and allocated memory are reflected in the cost model. 


%\subsubsection{Time}
%Neuron models are approximated using a discrete timestep model, where SNN inference (i.e execution) is performed over a specific time duration (number of discrete timesteps).
%Time is a natural component of SNN computation. STEMS incorporates time as an extra dimension in the SNN's workload. Algorithm~\ref{alg:snn} shows the behavior of a single LIF neuron over time. At each discrete timestep, input spikes are integrated into a neuron's membrane potential, and it is updated and activated once every timestep where it may spike and reset.

% \begin{algorithm}[H]
% \caption{LIF Neuron computation over time}\label{alg:alg1}
% \begin{algorithmic}
% \FOR{T in time}
% \FOR{I in Inputs}
%     \STATE $V_{mem} += S[T] \times W$ 
%     \hfill\algorithmiccomment{integrate input spikes}
% \ENDFOR
%     \STATE $S_{out}[T], V_{mem} = LIF(V_{mem})$
%     \hfill\algorithmiccomment{activate neuron (i.e leak and fire)}
% \ENDFOR
% \end{algorithmic}
% \label{alg:snn}
% \end{algorithm}

% Neuron states are re-used over different timesteps. However, there is a chronological dependency between neuron state updates over time, due to the non-linear activation at the end of each timestep. In other words, evaluating a neuron in one timestep requires first evaluating it for previous timesteps. This scheduling constraint limits the temporal mapping space, where input-relevant loops (e.g. input channels, kernel projection) need to be iterated before temporal loops can be iterated (and possibly batched together). Between successive temporal iterations, all iterated neurons (i.e output-relevant loops such as output channels, spatial dimensions) need to be activated and their output spikes for the current timestep are generated.
%This can be seen in Algorithm \ref{alg:alg1}, where input loops are written below the temporal loops. 


%Time is implemented in STEMS as an explicit dimension of our workload. This enlarges the mapping space for SNNs by an order of magnitude compared to traditional neural networks. Additionally, we enforce the mapping constraint that ensures chronological update and activation of neuron states and their spikes over time. Luckily, this constraint prunes the mapping space significantly.


%In Stream, we introduce graph cuts along the time dimension to explore different time-first schedules.




 

% \subsection{Sparsity}

% Sparsity is generally utilized for data compression and for compute and memory access skipping. 

% Our extended tool accepts spike traces as numpy array, and offers three options to utilize sparsity: 
% \begin{itemize}
%     \item Compute gating: The PE energy costs are scaled down by sparsity.
%     \item Register access gating: The register access of an operand can be scaled down by sparsity.
%     \item Compute skipping: The compute latency can be scaled down by sparsity.
% \end{itemize}

% We do not consider storing spikes in a compressed manner, as having a balance between data compression and memory access overhead is challenging given the high dimensionality and small payload of convolutional spike tiles. We also leave spike compression of external memory traffic to future work.

%\subsubsection{Spatial-Temporal Mapping Space}\label{sec:schedule}

%\subsection{Spatio-Temporal Mapping Space}


% \subsection{Multi-layer scheduling}


%DNN models typically consist of cascaded block, where each block is a directed acyclic graph of layer operators. Such operators include spatial convolution, addition (i.e residual connection), and spatial reductions. Intra-layer mapping optimizations can improve data re-use and reduce data movement within an operator. However, off-loading and re-loading features between operators may result in unfavorable performance that is dominated by off-chip memory traffic. Hence, applying inter-layer optimizations by simultaneously considering all operators in the workload graph can provide better mapping solutions for a DNN.

%A complete solution to DNN Algorithm-to-Hardware Mapping needs to simultaneously consider all layers/operations of a network, and apply inter-layer optimizations. 

%The most natural way to map a DNN is to schedule it layer-by-layer (LBL), where one layer is completed before successive layers can start. Layer-fused (LF) schedules can improve multi-layer mapping performance, where layers are interleaved across their spatial dimensions \cite{Sioutas2020ScheduleSF, 9646923}. This technique reduces the spatial size of intermediate features between layers, but reduces spatial re-use of weights (memory) and increases their variable lifetime. Layer fusion is complementary to input batching. In input batching, multiple input frames are consumed simultaneously, resulting in larger intermediate feature, but better re-use of weights (memory). 





 %($N+1$ temporal schedules x $N+1$ spatial schedules).

%So for RED-LIF, we apply time batching from the output block, while for SEW-ResNet, we apply time batching from the input block. This reduces our search space for N blocks from $4^N$ to $(N+1)^2$ (from 16,384 to 64 for SEW-ResNet).

%Stream supports layer fusion by cutting workload operators along spatial dimensions and can schedule computation nodes based on their spatial order. To enable spatio-temporal schedules that use layer fusion and time batching, STEMS cuts workload operators along spatial and temporal dimensions and can schedule computation nodes based on their spatial and temporal order.

%In Stream, the scheduler supports cutting the workload graph along spatial dimensions and layer fused (i.e depth-first) execution only. To enable spatio-temporal exploration, we cut the workload graph along both dimensions. To explore single timestep execution, cuts are made along the temporal dimension, while to explore layer fused execution, cuts are made along spatial dimensions. This also requires extensions to the scheduler,  to consider spatial and temporal dimensions of ready-to-schedule computation nodes.

%Additionally, we extend the scheduler Each unique computation node is mapped to hardware using Zigzag's polyhedral-based model, prioritizing mappings with minimum energy. 

% To explore depth-first and time-first schedules, we cut workloads into computation node graphs, by cutting along spatial dimensions (X or Y) to explore depth-first schedules, and along the time dimension to explore time-first schedules. By cutting the computation nodes differently, we can explore different degrees of layer fusion and time batching. Each unique computation node is mapped to hardware using Zigzag's analytical mapping search, prioritizing mappings with minimum energy. 

% For each layer/block in the workload, we have four scheduling options: time-first depth-first (TB-F), time-first layer-by-layer (TB-LBL), single-timestep depth-first (ST-F), and single-timestep layer-by-layer (ST-LBL). Figure \ref{fig:schedules} illustrates and explains all four schedules.

% \begin{figure}[!t]
% \centering
% \includegraphics[width=3.5in]{Figures/schedulesnn.pdf}
% \caption{Different Schedules for a 4 layers 3 timesteps SNN. (a) Single-timestep layer-by-layer (ST-LBL) schedule, where the network is executed one timestep at a time. (b) Time-first Layer-by-layer (TB-LBL) schedule, where the network is executed multiple timesteps at a time. (c) Single-timestep depth-first (ST-F), where each layer is split into multiple computation nodes along spatial dimensions. and (d) Time-first depth-first (TB-F) schedules, which combines both scheduling techniques.}
% \label{fig:schedules}
% \end{figure}

%[To-do? schedule barriers]

% Fusing an entire workload graph completely can result in poor re-use of network parameters, which have to be ejected and re-loaded multiple times. Depth-first schedules can have barriers between workloads. Such barriers split the computation graph, such that the workloads before the barrier are executed before the workloads after the barrier can start. In Figure \ref{fig:schedules2}, we illustrates two ST-F2 schedules for an SNN with 4 layers and 2 timesteps, one with a barrier and one without. 


% \begin{figure}[!t]
% \centering
% \includegraphics[width=3.5in]{Figures/schedulebarrs.pdf}
% \caption{Illustration of ST-F2 schedule (a) without barriers, (b) with barrier.}
% \label{fig:schedules2}
% \end{figure}

% Using barriers is already known in DNN literature \cite{8667835}, and has similar effects for DSNN. Barrier locations can be deducted by analyzing workload requirements against on-chip memory size \cite{Sioutas2020ScheduleSF}. 
% We show how placing barriers can improve external memory traffic for large DSNNs in our results. We leave the   mathematical memory-based fusion (barrier) analysis for future work.

\subsection{Neuron State Optimization}
\label{sec:mem}

Neuron states enable learning patterns over time. They provide the network with the sequential memory necessary to learn (spatio-)temporal tasks. Deep learning models learn spatial features hierarchically, creating high-level semantic features in deeper layers \cite{10.1145/1553374.1553453}. Recent research in training networks on spatiotemporal tasks has shown that temporal learning in earlier blocks provides little to no gain and comes at a significant memory cost \cite{red,10204090,yik2024neurobench,9749022}. The best strategy is often to apply spatial feature extraction in earlier blocks and only learn sequences from higher-level features in later blocks. This reduces model complexity and memory footprint and avoids learning dynamics of low-level features which are usually unnecessary and less stable

%Additionally, since earlier blocks tend to have large feature sizes, adding hidden states to those features comes at a significant memory overhead.

Hence, we propose a neuron state optimization targeting spatio-temporal problems such as event-based computer vision. We propose to remove neuron state memory (i.e. forget them between timesteps) from early SNN blocks. This optimization significantly reduces the number of neuron states. For example, removing neuron states from the first 11 blocks out of the 50 blocks of a SEW-ResNet-152 \cite{sewresnet} network reduces its neuron states by approximately 40\%. We remove neuron states while preserving the spike-based activation and surrogate gradient to have minimal effect on the computational model and make a fair comparison. In STEMS, we model such layers as normal ANN layers with spike-based features.


%Event-based cameras, or Dynamic Vision Sensors (DVS), capture changes in illumination to produce a sparse stream of events with high temporal resolution and high dynamic range. Because of such properties, event-based computer vision has recently gained popularity in research. 
%Learning the sparse event-streams of event-based cameras requires learning relationships between events both in space and across time. SNNs learn spatial and temporal relationships using convolution and state-ful LIF neurons respectively.  Recent research in event-based vision \cite{red,10204090,yik2024neurobench,9749022} shows that learning temporal relationships from high-level semantic features, rather than raw data, reduces model complexity and memory footprint and avoids learning dynamics of low-level features which are usually unnecessary and less stable. Hidden states in earlier blocks are also more costly, as earlier blocks have larger spatial dimensions. Hence, such blocks become both feature-intense and memory-intense, resulting in conflicting scheduling demands between fusion and time batching. While removing earlier hidden states creates a balanced workload where earlier blocks tend to be feature-intense and later blocks tend to be memory-intense.

%Moreover, earlier blocks have larger feature maps due to pooling, thus having hidden featuremap states that are persistent over time comes at a significant memory footprint.

%For SNNs, we can remove neuron states from earlier blocks (i.e. forget them between timesteps) to significantly reduce neuron states. For example, removing neuron states from the input up-sampling layer and the first 2 blocks of a SEW-ResNet152 \cite{sewresnet} model reduces its neuron states by 45\%. We remove neuron states while preserving the spike-based activation and surrogate gradient, to have minimal effect on the computational model and make a fair comparison. In STEMS, we model such layers as normal ANN layers with spike-based features.