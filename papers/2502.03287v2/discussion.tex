\section{Discussion} \label{sec:discussion}

This study has led to some interesting insights on SNNs.
%Our exploration on DSNN mapping has led to interesting insights:


\subsection*{Proper scheduling can mitigate the effects of neuron state}

In our experiments, we have shown how spatial-temporal inter-layer mapping optimizations can avoid a lot of DRAM traffic. Under stringent memory capacity, a single-timestep layer-by-layer schedule causes an off-chip traffic of neuron state that is detrimental to energy efficiency (Figure \ref{fig:sew7-tb}). Inter-layer scheduling optimizations drastically reduce on-chip memory requirements for efficient scheduling. 

% \subsection*{SNN memory shuffling over time}

% SNNs are executed over time. If the SNN memory (weights and neuron states) do not fit on-chip, a single-timestep schedule will cause repeated shuffling of the SNN memory between on-chip and off-chip memory at least once every timestep.

% \subsection*{Event-based vision SNN models contain too many neurons}

% Computer vision models begin with blocks having large spatial dimensions. Using spiking neurons with neuron states 
% comes at a significant cost and low returns for these early blocks. Under tight memory constraints, these blocks, which contain most neurons and spikes, tend to favor a time-batched layer-fused schedule (TB-LF). A TB-LF schedule maximizes the re-use of neuron state (TB), while trying to reduce intermediate feature maps (LF). On the other hand, later blocks tend to favor a time-batched layer-by-layer schedule (TB-LBL), to maximize re-use of memory (weights and neurons) in both space and time.

% \subsection*{Time-batching reduces memory off-chip traffic}

% Time batching allows the weights and hidden states of a recurrent model to be re-used more efficiently across timesteps. When an SNN model's memory (neuron states and weights) can not fit on-chip, batching timesteps reduces the amount of times they are shuffled to off-chip memory and back. However, time batching creates larger intermediate features. When intermediate features become a problem, we can combine time batching with layer fusion to reduce their size.


%When a recurrent model can not fit on-chip, batching timesteps improves re-use of hidden states and reduces their shuffling to external memory and back. However, this leads to temporally larger features. 


\subsection*{Optimal schedules are often hybrid schedules}

We have seen in our experiments how optimal schedules are hybrid, where different blocks favor different scheduling strategies. Early blocks that have no hidden states favor a single-timestep layer-fused (ST-LF) schedule, as they have more features and less memory due to their large spatial dimensions and shallow channel dimension. Late blocks favor time-batched layer-by-layer (TB-LBL) schedules, as they have more memory and less features due to their small spatial dimensions and deep channel dimension. Early blocks in a full SNN model favor time-batch layer-fused (TB-LF) schedules, as they have more features and memory (neuron states). This is demonstrated in Figures \ref{fig:red-best}, \ref{fig:sew7-best}, and \ref{fig:sew5-best}.

%Early SNN blocks tend to have more neurons and spikes than weights, so they favor time-batched layer-fused (TB-LF) schedules. While early blocks in ANNs and hybrid models tend to have more features than weights (and no hidden states), so they favor single-timestep layer-fused (ST-LF) schedules. Late blocks in most vision models tend to have more memory (in weights and possible hidden states) than features, hence they favor time-batched layer-by-layer (TB-LBL) schedules. 




%DSNN models tend to schedule earlier blocks in a time-batched depth-first manner (TB-DF), to maximize the re-use of hidden states while limiting intermediate spike size as much as possible. While later blocks tend to TB-LBL schedules, to maximize memory re-use, as they tend to have more memory (weights and hidden states) than features (spikes).
\subsection*{Event-based vision SNN models do not need most neurons}

We have shown how most features and neuron states of a computer vision SNN model are in its earlier blocks. The neuron states in such blocks are expensive and have low accuracy returns. Hybrid models consisting of an ANN followed by an SNN are more suitable for event-based vision than fully recurrent models. This is due to the following reasons:

\subsubsection*{Memory benefits}

Not having hidden states in earlier blocks leads to significantly less neuron state overhead, as earlier blocks have large spatial dimensions, as shown in Table \ref{tab:sew}. Significantly reducing the number of neuron states may also allow them to easily remain in on-chip memory.

\subsubsection*{Structural benefits}

Time-batching (TB) and layer fusion (i.e. LF) act on different dimensions (temporal vs. spatial) for different reasons (memory reuse vs. reducing intermediate features). Having a hybrid structure leads to a more balanced workload structure where the two strategies (TB and LF) do not compete within the same critical blocks. In a hybrid model, earlier blocks have much more features than memory, leading to a layer-fused schedule, while later blocks have much more memory than features, leading to a time-batched schedule. The same applies to ANN models and input batching.

\subsubsection*{Algorithmic benefits}

Learning spatial features first and transforming raw data into higher-level spatial features make temporal learning arguably simpler\cite{red}. Our experiments on SEW-ResNet-18 agree with the literature that removing hidden states from earlier blocks has little effect on accuracy\cite{10204090}.


\subsection*{Neuron state optimization creates hybrid SNN models}
Reducing neuron states from earlier blocks, as done in Section \ref{sec:memop}, creates a structure similar to hybrid models. Even if the whole workload is spiking, the optimized blocks act as feed-forward layers as they do not have any hidden states. The optimized models have a distribution of features, hidden states, and weights similar to hybrid models. Additionally, our experiments show, under tight memory constraints, that SEW-5's best schedule is similar to RED-LIF's best schedule. Hence, optimizing neuron state memory, as in Section \ref{sec:mem}, brings the benefits of hybrid models to SNNs by improving their schedule, energy efficiency, and even accuracy. 

\section{Future Work}

We have the following suggestions for future work:

% \subsection{Completing the design space}

% In this study, we could not exhaustively search the inter-layer mapping space. In addition to the complete design space from Section 5.3, we can also have fusion barriers and half-line depth-first schedules \cite{8667835}. 

% Although the graph-based approach is arguably analytical, cutting the workload in space and time leads to larger graph size and simulation run-time. Additionally, most of the simulation is redundant, as the schedule tend to have some steady states. Hence, a more concise and more mathematical approach is possible. If such approach is descriptive enough, it can quickly and completely explore the large design space.

\subsection*{Improving on-chip energy}

This work focused on optimizing off-chip traffic with limited on-chip memory. Future work can look at improving the on-chip energy. For that, we have the following suggestions:
\begin{itemize}
    \item \textbf{Sparsity Utilization:} to reduce the compute cost of sparse SNN layers. 
    \item \textbf{Compute-in-memory:} to further reduce compute cost.
    \item \textbf{Heterogeneous accelerators:} to serve the different properties of hybrid models.
\end{itemize}
 %for sparse SNN layers
%

%Effectively utilizing sparsity in SNNs can lead to significant reduction in compute cost.



%Another way to improve compute cost is to use novel compute-in-memory architectures, which can perform more energy efficient computation. 



%As earlier blocks and later blocks in a hybrid DL model have contrasting properties and perefer different schedules, they may also favor different on-chip data-flows. Earlier blocks may favor a dataflow with good feature re-use, while later blocks may favor a dataflow with good memory re-use. Additionally, sparsity levels tend to increase in deeper blocks leading to more favorable returns (as sparsity utilization does not come for free). Hence, sparsity utilization and event-driven execution can be more beneficial for later blocks than for earlier blocks in a workload.

\subsection*{Code generation}

In this work, we used a simulation model for performance estimation only. Future work can be towards completing the loop with code generation for hardware. 

\subsection*{Exploring other recurrent models}

SNNs are self-recurrent models, where the hidden states of different neurons do not directly affect each other. Other recurrent models can have relationships between its hidden states. For example, in a convolution LSTM layer, the hidden states interact together through a convolution kernel. Hence, updating one state requires knowing other neighboring states. This prohibits time-batched layer-fused (TB-LF) schedules, due to the spatial dependency between states.

There are also other emerging transformer-like SNN networks \cite{yao2023spikedriven}. Such large networks have a completely different structure compared to image-based convolutional SNNs. Such networks can be explored in future work using STEMS.

%Inter-layer scheduling along temporal and

%Inter-layer scheduling in space and time makes code generation and compiler support more challenging. Future research needs to invest in compiler support for such complicated inter-layer schedules.

%Generating code becomes more and more challenging with spatial and temporal scheduling techniques. Investing work and research into DL compilers to enable inter-layer optimizations can significant improve hardware efficiency.

