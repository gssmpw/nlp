\section{Related Work}
\label{subsec:related}



\paragraph{Inversion bias.}  
Given a matrix $\A \in \RR^{n \times d}$, the matrix inverse $(\A^\top\A)^{-1}$ is fundamental in ML, numerical computation, and statistics.  
Examples include linear functions $(\A^\top\A)^{-1}\y$ that are crucial to Newton's methods~\citep{boyd2004convex}, quadratic forms $\ba_i^\top(\A^\top\A)^{-1}\ba_i$ (with $\ba^\top_i$ the $i^{th}$ row of $\A$) in computing matrix leverage scores~\citep{drineas2012fast}, and trace forms, $\tr \mathbf{L} (\A^\top\A)^{-1} $ for some given $\mathbf{L}$, of interest in uncertainty quantification~\citep{kalantzis2013Accelerating} and experimental designs~\citep{pukelsheim2006optimal}.  
In the case of tall matrices with $n\gg d$, random sketching applies to efficiently reduce the computational overhead of $(\A^\top\A)^{-1}$, by using a sketch $\tilde\A=\S\A$ of $\A$ for random matrix $\S\in\RR^{m\times n}$ with $m\ll n$. 
It has been shown recently~\citep{michal2020precise,derezinski2021newtonless,derezinski2021sparse} that these sketched inverses are \emph{biased} for \emph{unstructured} sub-gaussian $\S$, with $\EE[(\tilde \A^\top\tilde \A)^{-1}] \not \approx (\A^\top\A)^{-1}$. 
In this paper, we consider the (often practically more interesting) case of \emph{structured} random matrix $\S$, including random sampling matrices (\Cref{def:RS}) and randomized Hadamard transforms (\Cref{def:srht}).


\paragraph{Random sampling.}
Random sampling is at the core of RandNLA~\citep{drineas2006fast1,mahoney2011randomized,ma2015statistical,DM16_CACM,DM21_NoticesAMS,randnla_kdd24_TR}, and it plays a central role in fast matrix multiplication~\citep{drineas2006fast1}, approximate regression~\citep{drineas2006sampling}, and low-rank approximation~\citep{cohen2017input}, to name a few. 
It is of particular interest in scenarios where the dataset is massive and cannot be stored and/or computed on a single machine, e.g., the census data~\citep{wang2018optimal} and online network data~\citep{deng2024subsampling}.
See \Cref{def:RS} below for a formal definition of random sampling and discussions thereafter for commonly-used sampling schemes including (exact and approximate) leverage score sampling~\citep{mahoney2011randomized,cohen2017input}, shrinkage leverage sampling~\citep{ma2015statistical}, as well as optimal subsampling~\citep{wang2018optimal,wang2021optimal,yu2022optimal,ma2022asymptotic}. 
In this paper, instead of providing classical JL and subspace embedding-type results on random sampling, we precisely characterize (and correct) the inversion bias for a variety of commonly-used random sampling schemes.
 


\paragraph{Sub-sampled Newton.}
Sub-sampled Newton (SSN) methods propose to approximate the Hessian in Newton's method using a small subset of samples, and they have been extensively studied within the fields of ML, RandNLA, and optimization~\citep{xu2016subsampled,bollapragada2019exact,roosta2019subsampled,xu2020newton,ye2021approximate}.
Although these fast optimization methods are easy to implement, their convergence rates are challenging to analyze.
Existing results often depend on the Hessian condition number or the Lipschitz constant and fall short of, e.g., the \emph{problem-independent} convergence rates achieved by sub-gaussian Newton Sketch~\citep{lacotte2019faster,derezinski2021newtonless}.
In this paper, we establish the first \emph{problem-independent} local convergence rates for SSN that closely align with Newton Sketch. 
This addresses the convergence guarantee gap identified in Iterative Hessian Sketch~\citep{pilanci2016iterative} for random sampling.



\paragraph{Random matrix theory (RMT).}
RMT studies the (limiting) eigenspectra of large-dimensional random matrices~\citep{anderson2010introduction} and finds its applications in signal processing and communication~\citep{couillet2011random}, statistical finance~\citep{plerou2002Random}, optimization~\citep{paquette2021SGDa,paquette2023Haltinga}, and more recently in large-scale ML~\citep{pennington2017nonlinear,fan2020spectra,mei2021generalization,couillet2022RMT4ML}.
A recent line of work~\cite{liao2020random,liao2021sparse,liao2021hessian} has highlighted non-trivial connections between RMT and RandNLA that this paper further develops.