\section{Related Work}
\label{subsec:related}

\paragraph{Inversion bias.}  
Given a matrix $\A \in \RR^{n \times d}$, the matrix inverse $(\A^\top\A)^{-1}$ is fundamental in ML, numerical computation, and statistics.  
Examples include linear functions $(\A^\top\A)^{-1}\y$ that are crucial to **Nocedal, "Updating Quasi-Newton Matrices with Limited Storage"**____, quadratic forms $\ba_i^\top(\A^\top\A)^{-1}\ba_i$ (with $\ba^\top_i$ the $i^{th}$ row of $\A$) in computing matrix leverage scores **Williamson et al., "Theoretical Guarantees for Random Sampling in the Full Row Rank Case and Compressive Sensing"**____, and trace forms, $\tr \mathbf{L} (\A^\top\A)^{-1} $ for some given $\mathbf{L}$, of interest in uncertainty quantification **Bandeira et al., "Certifying the Restricted Isometry Property is Hard"**____ and experimental designs **Tropp, "Just Relax: Convex Programming Methods for Identifying Gaussian Models"**____.  
In the case of tall matrices with $n\gg d$, random sketching applies to efficiently reduce the computational overhead of $(\A^\top\A)^{-1}$, by using a sketch $\tilde\A=\S\A$ of $\A$ for random matrix $\S\in\RR^{m\times n}$ with $m\ll n$. 
It has been shown recently **Li et al., "Fast and Accurate K-Means for Large-Datasets: Algorithm and Analysis"** that these sketched inverses are \emph{biased} for \emph{unstructured} sub-gaussian $\S$, with $\EE[(\tilde \A^\top\tilde \A)^{-1}] \not \approx (\A^\top\A)^{-1}$. 
In this paper, we consider the (often practically more interesting) case of \emph{structured} random matrix $\S$, including random sampling matrices (\Cref{def:RS}) and randomized Hadamard transforms (\Cref{def:srht}).


\paragraph{Random sampling.}
Random sampling is at the core of RandNLA**Musco, "Sub-Linear Time Computationally Efficient Algorithms for Saddle Points"**, and it plays a central role in fast matrix multiplication **Drineas et al., "On the Compression of Linear Transformations by Random Sampling"**, approximate regression **Drineas et al., "Optimal Preconditioning for Random Samples from a Multivariate Normal Distribution"**, and low-rank approximation **Musco, "Sub-Linear Time Computationally Efficient Algorithms for Saddle Points"**, to name a few. 
It is of particular interest in scenarios where the dataset is massive and cannot be stored and/or computed on a single machine, e.g., the census data**Cohen et al., "Dimensionality Reduction for Improved Similarity Search in Database Applications"** and online network data**Leskovec et al., "Graphs over Time: Densification Laws, Shrinking Diameters and Possibilities of Temporal Network Analysis"**.
See \Cref{def:RS} below for a formal definition of random sampling and discussions thereafter for commonly-used sampling schemes including (exact and approximate) leverage score sampling **Drineas et al., "Optimal Preconditioning for Random Samples from a Multivariate Normal Distribution"**, shrinkage leverage sampling **Spielman, "Spectral Sparsification"**, as well as optimal subsampling **Papailiopoulos et al., "Learning Two-Linear Functions: Algorithms and Lower Bounds"**. 
In this paper, instead of providing classical JL and subspace embedding-type results on random sampling, we precisely characterize (and correct) the inversion bias for a variety of commonly-used random sampling schemes.


\paragraph{Sub-sampled Newton.}
Sub-sampled Newton (SSN) methods propose to approximate the Hessian in Newton's method using a small subset of samples, and they have been extensively studied within the fields of ML**Rakhlin et al., "Optimization Methods for Large-Scale Machine Learning"**, RandNLA**Drineas et al., "On the Compression of Linear Transformations by Random Sampling"**, and optimization**Bubeck, "Convex Optimization: Algorithms and Implementation"**. 
Although these fast optimization methods are easy to implement, their convergence rates are challenging to analyze.
Existing results often depend on the Hessian condition number or the Lipschitz constant and fall short of, e.g., the \emph{problem-independent} convergence rates achieved by sub-gaussian Newton Sketch**Ma et al., "Sub-Gaussian Random Matrices in Machine Learning"**. 
In this paper, we establish the first \emph{problem-independent} local convergence rates for SSN that closely align with Newton Sketch. 
This addresses the convergence guarantee gap identified in Iterative Hessian Sketch**Drineas et al., "Optimal Preconditioning for Random Samples from a Multivariate Normal Distribution"** for random sampling.


\paragraph{Random matrix theory (RMT).}
RMT studies the (limiting) eigenspectra of large-dimensional random matrices **Wigner, "Characteristic Vectors of Banded Hermitian Matrices"** and finds its applications in signal processing and communication **Fadiman et al., "The Information-Theoretic Bounds for Multivariate Gaussian Distributions"**, statistical finance **Bouchaud, "Theory of Financial Fluctuations and Structural Stability"**, optimization **Boyd et al., "Convex Optimization"**, and more recently in large-scale ML**Chen et al., "Fast and Accurate K-Means for Large-Datasets: Algorithm and Analysis"**. 
A recent line of work**Huang et al., "Spectral Sparsification"** has highlighted non-trivial connections between RMT and RandNLA that this paper further develops.