\section{Related Work\label{sec:related-work}
}

In this section, we review some concepts of generalized smoothness from the literature.

\subsection{$(L_0,L_1)$-smoothness}

\paragraph{$(L_0,L_1)$-smooth: non-convex case} 
The concept of $(L_0,L_1)$-smoothness is firstly proposed by \citet{Zhang2020Why} based on empirical observations from LSTMs~\citep{merity2018regularizing}, which allows the function to have an affine-bounded Hessian norm. Under this new condition, they analyze gradient clipping~\citep{mikolov2012statistical,pmlr-v28-pascanu13} for deterministic non-convex optimization and derive a complexity of $O(\epsilon^{-2})$\footnote{In non-convex settings, the goal is to find $\epsilon$-stationary point $\x$ satisfying $\norm{\nabla f(\x)}_2\le\epsilon$. In convex settings, we aim at $\epsilon$-suboptimal point $\x$ satisfying $f(\x)-f^*\le\epsilon$. The meaning of $\epsilon$ depends of the context.}, which matches the lower bound~\citep{carmon2020lower} up to constant factors. They further extend to stochastic settings and provide an $O(\epsilon^{-4})$ complexity bound under the uniformly bounded noise assumption.

\paragraph{$(L_0,L_1)$-smooth: convex case}
\citet{pmlr-v202-koloskova23a} show that gradient clipping has $O(\epsilon^{-1})$ complexity, matching the classic result in deterministic optimization.~\citet{takezawa2024polyak} establish the same complexity bound for gradient descent with polyak stepsizes~\citep{polyak1987introduction}. However, besides $(L_0,L_1)$-smoothness, these two works further impose an additional $L$-smooth assumption, where $L$ could be significantly larger than $L_0$ and $L_1$.~\citet{gorbunov2024methods} address the limitation and recover their results without the extra $L$-smoothness assumption. Moreover, they study a variant of adaptive gradient descent~\citep{malitsky2020adaptive} and provide an $O(\epsilon^{-1})$ complexity result, albeit with worse constant terms compared to the original one. For acceleration schemes in convex optimization, they modify the method of similar triangles (MST)~\citep{gasnikov2018universal} and prove an optimal complexity of $O(\epsilon^{-0.5})$.

\paragraph{Other explorations}
Ever since~\citet{Zhang2020Why} proposed $(L_0,L_1)$-smoothness condition, this generalized notion of smoothness has been flourishing in minimax optimization~\citep{pmlr-v235-xian24a}, bilevel optimization~\citep{hao2024bilevel,pmlr-v235-gong24d}, multi-objective optimization~\citep{zhang2024MOO}, sign-based optimization~\citep{crawshaw2022robustness}, distributionally robust optimization~\citep{jin2021nonconvexdro} and variational inequality~\citep{vankov2024adaptive,vankov2024generalized}. Numerous attempts have been made to refine existing algorithms under this weaker assumption, including variance reduction~\citep{reisizadeh2023variance}, clipping/normalized gradient~\citep{zhang2020improved,pmlr-v130-qian21a,DBLP:journals/chinaf/ZhaoXL21,pmlr-v238-hubler24a,yang2024independently}, error feedback~\citep{khirirat2024error} and trust region methods~\citep{Xie2024trustregion}. Notably,~\citet{pmlr-v178-faw22a,pmlr-v195-faw23a,pmlr-v195-wang23a,Li23adam,wang2024provable,zhang2024gs} explore the convergence of AdaGrad~\citep{JMLR:v12:duchi11a}, RMSprop~\citep{hinton2012neural}, and Adam~\citep{kingma15adam} under generalized smoothness. Beyond the optimization community, it has also garnered significant attention in federated learning~\citep{khirirat2024communication,demidovich2024methods} and meta-learning~\citep{chayti2024metalearning}.

\subsection{$\alpha$-symmetric Generalized Smoothness}
% An important generalization of $(L_0,L_1)$-smoothness is $\alpha$-symmetric $(L_0,L_1)$-smoothness proposed by~\citet{chen2023generalized}, which introduces symmetry into the original formulation and also allows the dependency on the gradient norm to be polynomial with degree of $\alpha$. For deterministic non-convex optimization,~\citet{chen2023generalized} establish the optimal complexity of $O(\epsilon^{-2})$ for a variant of normalized gradient descent~\citep{nesterov1984minimization,cortes2006finite}. They also show that the popular SPIDER algorithm~\citep{fang2018spider} achieves the optimal $O(\epsilon^{-3})$ complexity in the stochastic setting.
An important generalization of $(L_0,L_1)$-smoothness is $\alpha$-symmetric $(L_0,L_1)$-smoothness proposed by~\citet{chen2023generalized}, which introduces symmetry into the original formulation and also allows the dependency on the gradient norm to be polynomial with degree of $\alpha$. The formal definition is given as follows:
% \citet{chen2023generalized} propose $\alpha$-symmetric generalized smoothness condition of the form
\begin{equation}
    \norm{\nabla f(\x)-\nabla f(\y)}_2\le\brac{L_0+L_1\sup_{\theta\in[0,1]}\norm{\nabla f\brac{\theta\x+(1-\theta)\y}}_2}\norm{\x-\y}_2,\ \forall\x,\y\in\mathcal{E},\label{eq:alpha-symmetric}
\end{equation}
for some $L_0,L_1\in\R_+$. For deterministic non-convex optimization,~\citet{chen2023generalized} establish the optimal complexity of $O(\epsilon^{-2})$ for a variant of normalized gradient descent~\citep{nesterov1984minimization,cortes2006finite}. They also show that the popular SPIDER algorithm~\citep{fang2018spider} achieves the optimal $O(\epsilon^{-3})$ complexity in the stochastic setting. As pointed out by~\citet{chen2023generalized,vankov2024optimizing}, any twice-differentiable function satisfying~\eqref{eq:alpha-symmetric} is also $(L_0^{'},L_1^{'})$-smooth for different constant factors. As an extension,~\citet{NeurIPS:2024:Jiang} study sign-based finite-sum non-convex optimization and obtain an improved complexity over SignSVRG algorithm~\citep{chzhen2023signsvrg}.

\subsection{$\ell$-smoothness}
Recently,~\citet{Li2023GS} significantly generalize $(L_0,L_1)$-smooth to $\ell$-smooth, which assumes $\norm{\nabla^2 f(\x)}_2\le \ell(\norm{\nabla f(\x)}_2)$ for an arbitrary non-decreasing function $\ell$. The enhanced flexibility of $\ell$ accommodates a broader range of practical ML problems~\citep{devlin-etal-2019-bert,caron2021emerging,radford2021learning}, and can model certain real-world problems where $(L_0,L_1)$-smoothness fails~\citep{cooper2024theoretical}. Under this condition, they study gradient descent for strongly-convex, convex, and non-convex objectives, obtaining classic results of $O(\log(\epsilon^{-1}))$, $O(\epsilon^{-1})$, and $O(\epsilon^{-2})$, respectively. They also prove that Nesterov's accelerated gradient method~\citep{nesterov1983method} attains the optimal $O(\epsilon^{-0.5})$ complexity for convex objectives. Furthermore, they delve into stochastic non-convex optimization and show that stochastic gradient descent achieves the optimal complexity of $O(\epsilon^{-4})$ under finite variance condition, matching the lower bound in~\citet{arjevani2023lower}.
\setParDis

\citet{tyurin2024toward} study gradient descent for deterministic non-convex optimization and achieve optimal convergence rates (up to constant factors) without the need for a sub-quadratic \(\ell\) as required in~\citet{Li2023GS}. However, they impose an additional assumption of bounded gradients, which significantly simplifies the analysis.~\citet{NeurIPS'24:LocalSmooth} study online convex optimization problems by assuming each online function $f_t:\X\mapsto\R$ is $\ell_t$-smooth and that $\ell_t(\cdot)$ can be queried arbitrarily by the learner. Based on $\ell_t$-smoothness, they derive gradient-variation regret bounds for convex and strongly convex functions, albeit with the limitation of assuming a globally constant upper bound on the adversary's smoothness. 

\subsection{Other generalizations of classic smoothness}
It is known that a function $f$ is $L$-smooth if $Lh-f$ is convex~\citep{nesterov2018lectures} for $h(\cdot)=\norm{\cdot}^2/2$. Advancements have been made to relax $h$ into an arbitrary Legendre function, leading to the development of a notion called relative smoothness~\citep{Bauschke17descentlemma,lu18relativesmooth}, which substitutes the canonical quadratic upper bound with $f(\x)\le f(\y)+\inner{\nabla f(\y)}{\x-\y}+LB_h(\x,\y)$, where $B_h$ is the Bregman divergence associated with $h$. More recently,~\citet{mishkin2024directional} propose directional smoothness, a measure of local gradient variation that preserves the global smoothness along specific directions. To address the imbalanced Hessian spectrum distribution in practice~\citep{sagun2016eigenvalues,pan2022eigencurve},~\citet{jiang2024convergence,liu2024adagrad} propose anisotropic smoothness and obtain improved bounds for adaptive gradient methods.