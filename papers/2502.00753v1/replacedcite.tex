\section{Related Work\label{sec:related-work}
}

In this section, we review some concepts of generalized smoothness from the literature.

\subsection{$(L_0,L_1)$-smoothness}

\paragraph{$(L_0,L_1)$-smooth: non-convex case} 
The concept of $(L_0,L_1)$-smoothness is firstly proposed by ____ based on empirical observations from LSTMs____, which allows the function to have an affine-bounded Hessian norm. Under this new condition, they analyze gradient clipping____ for deterministic non-convex optimization and derive a complexity of $O(\epsilon^{-2})$\footnote{In non-convex settings, the goal is to find $\epsilon$-stationary point $\x$ satisfying $\norm{\nabla f(\x)}_2\le\epsilon$. In convex settings, we aim at $\epsilon$-suboptimal point $\x$ satisfying $f(\x)-f^*\le\epsilon$. The meaning of $\epsilon$ depends of the context.}, which matches the lower bound____ up to constant factors. They further extend to stochastic settings and provide an $O(\epsilon^{-4})$ complexity bound under the uniformly bounded noise assumption.

\paragraph{$(L_0,L_1)$-smooth: convex case}
____ show that gradient clipping has $O(\epsilon^{-1})$ complexity, matching the classic result in deterministic optimization.____ establish the same complexity bound for gradient descent with polyak stepsizes____. However, besides $(L_0,L_1)$-smoothness, these two works further impose an additional $L$-smooth assumption, where $L$ could be significantly larger than $L_0$ and $L_1$.____ address the limitation and recover their results without the extra $L$-smoothness assumption. Moreover, they study a variant of adaptive gradient descent____ and provide an $O(\epsilon^{-1})$ complexity result, albeit with worse constant terms compared to the original one. For acceleration schemes in convex optimization, they modify the method of similar triangles (MST)____ and prove an optimal complexity of $O(\epsilon^{-0.5})$.

\paragraph{Other explorations}
Ever since____ proposed $(L_0,L_1)$-smoothness condition, this generalized notion of smoothness has been flourishing in minimax optimization____, bilevel optimization____, multi-objective optimization____, sign-based optimization____, distributionally robust optimization____ and variational inequality____. Numerous attempts have been made to refine existing algorithms under this weaker assumption, including variance reduction____, clipping/normalized gradient____, error feedback____ and trust region methods____. Notably,____ explore the convergence of AdaGrad____, RMSprop____, and Adam____ under generalized smoothness. Beyond the optimization community, it has also garnered significant attention in federated learning____ and meta-learning____.

\subsection{$\alpha$-symmetric Generalized Smoothness}
% An important generalization of $(L_0,L_1)$-smoothness is $\alpha$-symmetric $(L_0,L_1)$-smoothness proposed by____, which introduces symmetry into the original formulation and also allows the dependency on the gradient norm to be polynomial with degree of $\alpha$. For deterministic non-convex optimization,____ establish the optimal complexity of $O(\epsilon^{-2})$ for a variant of normalized gradient descent____. They also show that the popular SPIDER algorithm____ achieves the optimal $O(\epsilon^{-3})$ complexity in the stochastic setting.
An important generalization of $(L_0,L_1)$-smoothness is $\alpha$-symmetric $(L_0,L_1)$-smoothness proposed by____, which introduces symmetry into the original formulation and also allows the dependency on the gradient norm to be polynomial with degree of $\alpha$. The formal definition is given as follows:
% ____ propose $\alpha$-symmetric generalized smoothness condition of the form
\begin{equation}
    \norm{\nabla f(\x)-\nabla f(\y)}_2\le\brac{L_0+L_1\sup_{\theta\in[0,1]}\norm{\nabla f\brac{\theta\x+(1-\theta)\y}}_2}\norm{\x-\y}_2,\ \forall\x,\y\in\mathcal{E},\label{eq:alpha-symmetric}
\end{equation}
for some $L_0,L_1\in\R_+$. For deterministic non-convex optimization,____ establish the optimal complexity of $O(\epsilon^{-2})$ for a variant of normalized gradient descent____. They also show that the popular SPIDER algorithm____ achieves the optimal $O(\epsilon^{-3})$ complexity in the stochastic setting. As pointed out by____, any twice-differentiable function satisfying~\eqref{eq:alpha-symmetric} is also $(L_0^{'},L_1^{'})$-smooth for different constant factors. As an extension,____ study sign-based finite-sum non-convex optimization and obtain an improved complexity over SignSVRG algorithm____.

\subsection{$\ell$-smoothness}
Recently,____ significantly generalize $(L_0,L_1)$-smooth to $\ell$-smooth, which assumes $\norm{\nabla^2 f(\x)}_2\le \ell(\norm{\nabla f(\x)}_2)$ for an arbitrary non-decreasing function $\ell$. The enhanced flexibility of $\ell$ accommodates a broader range of practical ML problems____, and can model certain real-world problems where $(L_0,L_1)$-smoothness fails____. Under this condition, they study gradient descent for strongly-convex, convex, and non-convex objectives, obtaining classic results of $O(\log(\epsilon^{-1}))$, $O(\epsilon^{-1})$, and $O(\epsilon^{-2})$, respectively. They also prove that Nesterov's accelerated gradient method____ attains the optimal $O(\epsilon^{-0.5})$ complexity for convex objectives. Furthermore, they delve into stochastic non-convex optimization and show that stochastic gradient descent achieves the optimal complexity of $O(\epsilon^{-4})$ under finite variance condition, matching the lower bound in____.
\setParDis

____ study gradient descent for deterministic non-convex optimization and achieve optimal convergence rates (up to constant factors) without the need for a sub-quadratic \(\ell\) as required in____. However, they impose an additional assumption of bounded gradients, which significantly simplifies the analysis.____ study online convex optimization problems by assuming each online function $f_t:\X\mapsto\R$ is $\ell_t$-smooth and that $\ell_t(\cdot)$ can be queried arbitrarily by the learner. Based on $\ell_t$-smoothness, they derive gradient-variation regret bounds for convex and strongly convex functions, albeit with the limitation of assuming a globally constant upper bound on the adversary's smoothness. 

\subsection{Other generalizations of classic smoothness}
It is known that a function $f$ is $L$-smooth if $Lh-f$ is convex____ for $h(\cdot)=\norm{\cdot}^2/2$. Advancements have been made to relax $h$ into an arbitrary Legendre function, leading to the development of a notion called relative smoothness____, which substitutes the canonical quadratic upper bound with $f(\x)\le f(\y)+\inner{\nabla f(\y)}{\x-\y}+LB_h(\x,\y)$, where $B_h$ is the Bregman divergence associated with $h$. More recently,____ propose directional smoothness, a measure of local gradient variation that preserves the global smoothness along specific directions. To address the imbalanced Hessian spectrum distribution in practice____,____ propose anisotropic smoothness and obtain improved bounds for adaptive gradient methods.