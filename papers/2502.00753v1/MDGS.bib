%%% generalized smoothness
@inproceedings{Li2023GS,
 author = {Li, Haochuan and Qian, Jian and Tian, Yi and Rakhlin, Alexander and Jadbabaie, Ali},
 booktitle = {Advances in Neural Information Processing Systems 36 (NeurIPS)},
 pages = {40238--40271},
 title = {Convex and Non-convex Optimization Under Generalized Smoothness},
 year = {2023}
}

@inproceedings{Zhang2020Why,
    title={Why Gradient Clipping Accelerates Training: A Theoretical Justification for Adaptivity},
    author={Jingzhao Zhang and Tianxing He and Suvrit Sra and Ali Jadbabaie},
    booktitle={The 8th International Conference on Learning Representations (ICLR)},
    year={2020},
}

@inproceedings{Li23adam,
 author = {Li, Haochuan and Rakhlin, Alexander and Jadbabaie, Ali},
 booktitle = {Advances in Neural Information Processing Systems 36 (NeurIPS)},
 pages = {52166--52196},
 title = {Convergence of Adam Under Relaxed Assumptions},
 year = {2023}
}


@inproceedings{chen2023generalized,
  title={Generalized-smooth nonconvex optimization is as efficient as smooth nonconvex optimization},
  author={Chen, Ziyi and Zhou, Yi and Liang, Yingbin and Lu, Zhaosong},
  booktitle={Proceedings of the 40th International Conference on Machine Learning (ICML)},
  pages={5396--5427},
  year={2023},
}

@InProceedings{pmlr-v195-faw23a,
  title = 	 {Beyond Uniform Smoothness: A Stopped Analysis of Adaptive SGD},
  author =       {Faw, Matthew and Rout, Litu and Caramanis, Constantine and Shakkottai, Sanjay},
  booktitle = 	 {Proceedings of the 36th Conference on Learning Theory (COLT)},
  pages = 	 {89--160},
  year = 	 {2023},
}

@InProceedings{pmlr-v195-wang23a,
  title = 	 {Convergence of AdaGrad for Non-convex Objectives: Simple Proofs and Relaxed Assumptions},
  author =       {Wang, Bohan and Zhang, Huishuai and Ma, Zhiming and Chen, Wei},
  booktitle = 	 {Proceedings of the 36th Conference on Learning Theory (COLT)},
  pages = 	 {161--190},
  year = 	 {2023},
}

@inproceedings{NeurIPS'24:LocalSmooth,
    author = {Yan-Feng Xie and Peng Zhao and Zhi-Hua Zhou},
    title = {Gradient-Variation Online Learning under Generalized Smoothness},
    booktitle = {Advances in Neural Information Processing Systems 37 (NeurIPS)},
    year = {2024},
    pages = {to appear}
}

@article{Bauschke17descentlemma,
    author = {Bauschke, Heinz H. and Bolte, J\'{e}r\^{o}me and Teboulle, Marc},
    title = {A Descent Lemma Beyond Lipschitz Gradient Continuity: First-Order Methods Revisited and Applications},
    year = {2017},
    volume = {42},
    number = {2},
    journal = {Mathematics of Operations Research},
    pages = {330-348},
}

@article{lu18relativesmooth,
    author = {Lu, Haihao and Freund, Robert M. and Nesterov, Yurii},
    title = {Relatively Smooth Convex Optimization by First-Order Methods, and Applications},
    journal = {SIAM Journal on Optimization},
    volume = {28},
    number = {1},
    pages = {333-354},
    year = {2018},
}

@inproceedings{mishkin2024directional,
    title={Directional Smoothness and Gradient Methods: Convergence and Adaptivity},
    author={Aaron Mishkin and Ahmed Khaled and Yuanhao Wang and Aaron Defazio and Robert M. Gower},
    booktitle={Advances in Neural Information Processing Systems 37 (NeurIPS)},
    pages={to appear},
    year={2024},
}

@inproceedings{cooper2024theoretical,
  title={A theoretical study of the {$(L_0, L_1)$}-smoothness condition in deep learning},
  author={Cooper, Y},
  booktitle={OPT 2024: 16th Annual Workshop on Optimization for Machine Learning},
  year={2024},
}

@inproceedings{NeurIPS:2024:Jiang,
    author = {Wei Jiang and Sifan Yang and Wenhao Yang and Lijun Zhang},    
    title = {Efficient Sign-Based Optimization: Accelerating Convergence via Variance Reduction},
    booktitle = {Advances in Neural Information Processing Systems 37 (NeurIPS)},    
    pages = {to appear},    
    year = {2024},
}

@article{vankov2024optimizing,
  title={Optimizing {$(L_0, L_1)$}-Smooth Functions by Gradient Methods},
  author={Vankov, Daniil and Rodomanov, Anton and Nedich, Angelia and Sankar, Lalitha and Stich, Sebastian U},
  journal={arXiv preprint arXiv:2410.10800},
  year={2024}
}

@inproceedings{crawshaw2022robustness,
  title={Robustness to unbounded smoothness of generalized signsgd},
  author={Crawshaw, Michael and Liu, Mingrui and Orabona, Francesco and Zhang, Wei and Zhuang, Zhenxun},
  booktitle={Advances in Neural Information Processing Systems 35 (NeurIPS)},
  pages={9955--9968},
  year={2022}
}

@inproceedings{hao2024bilevel,
    title={Bilevel Optimization under Unbounded Smoothness: A New Algorithm and Convergence Analysis},
    author={Jie Hao and Xiaochuan Gong and Mingrui Liu},
    booktitle={The 12th International Conference on Learning Representations (ICLR)},
    year={2024},
}

@InProceedings{pmlr-v235-gong24d,
  title = 	 {A Nearly Optimal Single Loop Algorithm for Stochastic Bilevel Optimization under Unbounded Smoothness},
  author =       {Gong, Xiaochuan and Hao, Jie and Liu, Mingrui},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning (ICML)},
  pages = 	 {15854--15892},
  year = 	 {2024},
}

@article{demidovich2024methods,
  title={Methods with Local Steps and Random Reshuffling for Generally Smooth Non-Convex Federated Optimization},
  author={Demidovich, Yury and Ostroukhov, Petr and Malinovsky, Grigory and Horv{\'a}th, Samuel and Tak{\'a}{\v{c}}, Martin and Richt{\'a}rik, Peter and Gorbunov, Eduard},
  journal={arXiv preprint arXiv:2412.02781},
  year={2024}
}

@inproceedings{Xie2024trustregion, 
    title={Trust Region Methods for Nonconvex Stochastic Optimization beyond Lipschitz Smoothness}, 
    booktitle={Proceedings of the 38th AAAI Conference on Artificial Intelligence}, 
    author={Xie, Chenghan and Li, Chenxi and Zhang, Chuwen and Deng, Qi and Ge, Dongdong and Ye, Yinyu}, 
    year={2024},
    pages={16049-16057} 
}

@article{khirirat2024error,
  title={Error Feedback under {$(L_0, L_1)$}-Smoothness: Normalization and Momentum},
  author={Khirirat, Sarit and Sadiev, Abdurakhmon and Riabinin, Artem and Gorbunov, Eduard and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:2410.16871},
  year={2024}
}

@InProceedings{pmlr-v235-xian24a,
  title = 	 {Delving into the Convergence of Generalized Smooth Minimax Optimization},
  author =       {Xian, Wenhan and Chen, Ziyi and Huang, Heng},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning (ICML)},
  pages = 	 {54191--54211},
  year = 	 {2024},
}

@article{zhang2024MOO,
  title={On the Convergence of Multi-objective Optimization under Generalized Smoothness},
  author={Zhang, Qi and Xiao, Peiyao and Ji, Kaiyi and Zou, Shaofeng},
  journal={arXiv preprint arXiv:2405.19440},
  year={2024}
}


@InProceedings{vankov2024adaptive,
  title = 	 {Generalized Smooth Variational Inequalities: Methods with Adaptive Stepsizes},
  author =       {Vankov, Daniil and Nedich, Angelia and Sankar, Lalitha},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning (ICML)},
  pages = 	 {49137--49170},
  year = 	 {2024},
}

@article{vankov2024generalized,
  title={Generalized Smooth Stochastic Variational Inequalities: Almost Sure Convergence and Convergence Rates},
  author={Vankov, Daniil and Nedich, Angelia and Sankar, Lalitha},
  journal={arXiv preprint arXiv:2410.12334},
  year={2024}
}

@inproceedings{khirirat2024communication,
  title={Communication-efficient Algorithms Under Generalized Smoothness Assumptions},
  author={Khirirat, Sarit and Sadiev, Abdurakhmon and Riabinin, Artem and Gorbunov, Eduard and Richt{\'a}rik, Peter},
  booktitle={OPT 2024: 16th Annual Workshop on Optimization for Machine Learning},
  year={2024}
}

@inproceedings{anonymous2024hybrid,
    title={Hybrid Fine-Tuning of {LLM}s: Theoretical Insights on Generalized Smoothness and Convergence},
    author={Anonymous},
    booktitle={Submitted to The 13th International Conference on Learning Representations (ICLR)},
    year={2024},
    url={https://openreview.net/forum?id=8aKygnbEFX},
    note={under review}
}

@article{jiang2024convergence,
  title={Convergence analysis of adaptive gradient methods under refined smoothness and noise assumptions},
  author={Jiang, Ruichen and Maladkar, Devyani and Mokhtari, Aryan},
  journal={arXiv preprint arXiv:2406.04592},
  year={2024}
}

@article{liu2024adagrad,
  title={AdaGrad under Anisotropic Smoothness},
  author={Liu, Yuxing and Pan, Rui and Zhang, Tong},
  journal={arXiv preprint arXiv:2406.15244},
  year={2024}
}

@inproceedings{anonymous2024revisiting,
    title={Revisiting Large-Scale Non-convex Distributionally Robust Optimization},
    author={Anonymous},
    booktitle={Submitted to The 13th International Conference on Learning Representations (ICLR)},
    year={2024},
    url={https://openreview.net/forum?id=JYwVijuNA7},
    note={under review}
}

@article{reisizadeh2023variance,
  title={Variance-reduced clipping for non-convex optimization},
  author={Reisizadeh, Amirhossein and Li, Haochuan and Das, Subhro and Jadbabaie, Ali},
  journal={arXiv preprint arXiv:2303.00883},
  year={2023}
}

@article{chayti2024metalearning,
  title={A New First-Order Meta-Learning Algorithm with Convergence Guarantees},
  author={Chayti, El Mahdi and Jaggi, Martin},
  journal={arXiv preprint arXiv:2409.03682},
  year={2024}
}

@inproceedings{zhang2020improved,
  title={Improved analysis of clipping algorithms for non-convex optimization},
  author={Zhang, Bohang and Jin, Jikai and Fang, Cong and Wang, Liwei},
  booktitle={Advances in Neural Information Processing Systems 33 (NeurIPS)},
  pages={15511--15521},
  year={2020}
}

@article{yang2024independently,
  title={Independently-Normalized SGD for Generalized-Smooth Nonconvex Optimization},
  author={Yang, Yufeng and Tripp, Erin and Sun, Yifan and Zou, Shaofeng and Zhou, Yi},
  journal={arXiv preprint arXiv:2410.14054},
  year={2024}
}

@InProceedings{pmlr-v238-hubler24a,
  title = 	 {Parameter-Agnostic Optimization under Relaxed Smoothness},
  author =       {H\"{u}bler, Florian and Yang, Junchi and Li, Xiang and He, Niao},
  booktitle = 	 {Proceedings of the 27th International Conference on Artificial Intelligence and Statistics (AISTATS)},
  pages = 	 {4861--4869},
  year = 	 {2024},
}

@inproceedings{wang2024provable,
  title={Provable adaptivity of adam under non-uniform smoothness},
  author={Wang, Bohan and Zhang, Yushun and Zhang, Huishuai and Meng, Qi and Sun, Ruoyu and Ma, Zhi-Ming and Liu, Tie-Yan and Luo, Zhi-Quan and Chen, Wei},
  booktitle={Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  pages={2960--2969},
  year={2024}
}

@article{gorbunov2024methods,
  title={Methods for convex {$(L_0,L_1)$}-smooth optimization: Clipping, acceleration, and adaptivity},
  author={Gorbunov, Eduard and Tupitsa, Nazarii and Choudhury, Sayantan and Aliev, Alen and Richt{\'a}rik, Peter and Horv{\'a}th, Samuel and Tak{\'a}{\v{c}}, Martin},
  journal={arXiv preprint arXiv:2409.14989},
  year={2024}
}

@article{tyurin2024toward,
  title={Toward a Unified Theory of Gradient Descent under Generalized Smoothness},
  author={Tyurin, Alexander},
  journal={arXiv preprint arXiv:2412.11773},
  year={2024}
}

@inproceedings{takezawa2024polyak,
  title={Parameter-free Clipped Gradient Descent Meets
 Polyak},
  author={Takezawa, Yuki and Bao, Han and Sato, Ryoma and Niwa, Kenta and Yamada, Makoto},
  booktitle = {Advances in Neural Information Processing Systems 37 (NeurIPS)},    
  pages = {to appear}, 
  year={2024}
}

@InProceedings{pmlr-v130-qian21a,
  title = 	 { Understanding Gradient Clipping In Incremental Gradient Methods },
  author =       {Qian, Jiang and Wu, Yuren and Zhuang, Bojin and Wang, Shaojun and Xiao, Jing},
  booktitle = 	 {Proceedings of The 24th International Conference on Artificial Intelligence and Statistics (AISTATS)},
  pages = 	 {1504--1512},
  year = 	 {2021},
}

@article{DBLP:journals/chinaf/ZhaoXL21,
  author       = {Shen{-}Yi Zhao and
                  Yin{-}Peng Xie and
                  Wu{-}Jun Li},
  title        = {On the convergence and improvement of stochastic normalized gradient
                  descent},
  journal      = {Science China Information Sciences},
  volume       = {64},
  number       = {3},
  year         = {2021},
}
%%% generalized smoothness

%%% books
@book{evans2018measure,
  title={Measure theory and fine properties of functions},
  author={Evans, Lawrence Craig},
  year={2018},
  publisher={Routledge},
}

@article{orabona2019intro,
  author       = {Francesco Orabona},
  title        = {A Modern Introduction to Online Learning},
  journal      = {arXiv preprint arXiv:1912.13213v6},
  year         = {2019},
}

@book{williams1991probability,
  title={Probability with martingales},
  author={Williams, David},
  year={1991},
  publisher={Cambridge University Press}
}

@book{durrett2019probability,
  title={Probability: theory and examples},
  author={Durrett, Rick},
  volume={49},
  year={2019},
  publisher={Cambridge University Press}
}

@book{nesterov2013introductory,
  title={Introductory lectures on convex optimization: A basic course},
  author={Nesterov, Yurii},
  volume={87},
  year={2013},
  publisher={Springer Science \& Business Media}
}

@book{nesterov2018lectures,
  title={Lectures on convex optimization},
  author={Nesterov, Yurii and others},
  volume={137},
  year={2018},
  publisher={Springer}
}

@book{lan2020first,
  title={First-order and stochastic optimization methods for machine learning},
  author={Lan, Guanghui},
  volume={1},
  year={2020},
  publisher={Springer}
}

@article{bubeck2015convex,
  title={Convex optimization: Algorithms and complexity},
  author={Bubeck, S{\'e}bastien and others},
  journal={Foundations and Trends{\textregistered} in Machine Learning},
  volume={8},
  number={3-4},
  pages={231--357},
  year={2015},
  publisher={Now Publishers, Inc.}
}

@book{cesa2006prediction,
  title={Prediction, learning, and games},
  author={Cesa-Bianchi, Nicolo and Lugosi, G{\'a}bor},
  year={2006},
  publisher={Cambridge University Press}
}

@book{shalev2014understanding,
  title={Understanding machine learning: From theory to algorithms},
  author={Shalev-Shwartz, Shai and Ben-David, Shai},
  year={2014},
  publisher={Cambridge University Press}
}

@book{facchinei2007finite,
  title={Finite-Dimensional Variational Inequalities and Complementarity Problems},
  author={Facchinei, Francisco and Pang, Jong-Shi},
  year={2007},
  publisher={Springer Science \& Business Media}
}

@incollection{nesterov2000squared,
  title={Squared functional systems and optimization problems},
  author={Nesterov, Yurii},
  booktitle={High Performance Optimization},
  pages={405--440},
  year={2000},
  publisher={Springer}
}

@book{boyd2004convex,
  title={Convex optimization},
  author={Boyd, Stephen and Vandenberghe, Lieven},
  year={2004},
  publisher={Cambridge University Press}
}

@article{d2021acceleration,
  title={Acceleration methods},
  author={d’Aspremont, Alexandre and Scieur, Damien and Taylor, Adrien and others},
  journal={Foundations and Trends{\textregistered} in Optimization},
  volume={5},
  number={1-2},
  pages={1--245},
  year={2021},
  publisher={Now Publishers, Inc.}
}

@book{polyak1987introduction,
  title={Introduction to optimization},
  author={Polyak, Boris T},
  year={1987},
  publisher={New York, Optimization Software}
}
%%% books

%%% last-iterate
@InProceedings{pmlr-v28-shamir13,
  title = 	 {Stochastic Gradient Descent for Non-smooth Optimization: Convergence Results and Optimal Averaging Schemes},
  author = 	 {Shamir, Ohad and Zhang, Tong},
  booktitle = 	 {Proceedings of the 30th International Conference on Machine Learning (ICML)},
  pages = 	 {71--79},
  year = 	 {2013},
}

@InProceedings{pmlr-v99-harvey19a,
  title = 	 {Tight analyses for non-smooth stochastic gradient descent},
  author =       {Harvey, Nicholas J.~A. and Liaw, Christopher and Plan, Yaniv and Randhawa, Sikander},
  booktitle = 	 {Proceedings of the 32nd Conference on Learning Theory (COLT)},
  pages = 	 {1579--1613},
  year = 	 {2019},
}

@article{jain2021making,
    author = {Jain, Prateek and Nagaraj, Dheeraj M. and Netrapalli, Praneeth},
    title = {Making the Last Iterate of SGD Information Theoretically Optimal},
    journal = {SIAM Journal on Optimization},
    volume = {31},
    number = {2},
    pages = {1108-1130},
    year = {2021},
}

@article{orabona2020last,
  title={Last iterate of sgd converges (even in unbounded domains)},
  author={Orabona, Francesco},
  journal={Parameter-free Learning and Optimization Algorithms},
  url={https://parameterfree.com/2020/08/07/last-iterate-of-sgd-converges-even-in-unbounded-domains/},
  year={2020}
}

@inproceedings{liu2024revisiting,
    title={Revisiting the Last-Iterate Convergence of Stochastic Gradient Methods},
    author={Zijian Liu and Zhengyuan Zhou},
    booktitle={The 12th International Conference on Learning Representations (ICLR)},
    year={2024},
}

@inproceedings{ICML:2024:Zhang,
    author = {Lijun Zhang and Haomin Bai and Wei-Wei Tu and Ping Yang and Yao Hu},
    title = {Efficient Stochastic Approximation of Minimax Excess Risk Optimization},
    booktitle = {Proceedings of the 41st International Conference on Machine Learning (ICML)},
    pages = {58599--58630},
    year = {2024},
}

@article{zhang2024gdro,
  author       = {Lijun Zhang and Haomin Bai and Peng Zhao and Tianbao Yang and Zhi-Hua Zhou},
  title        = {Stochastic Approximation Approaches to Group Distributionally Robust Optimization and Beyond},
  journal      = {arXiv preprint arXiv:2302.09267v5},
  year         = {2024},
}

@article{cai2022arxiv,
  title={Tight last-iterate convergence of the extragradient and the optimistic gradient descent-ascent algorithm for constrained monotone variational inequalities},
  author={Cai, Yang and Oikonomou, Argyris and Zheng, Weiqiang},
  journal={arXiv preprint arXiv:2204.09228v3},
  year={2022}
}

@inproceedings{cai2022lastiterate,
 author = {Cai, Yang and Oikonomou, Argyris and Zheng, Weiqiang},
 booktitle = {Advances in Neural Information Processing Systems 35 (NeurIPS)},
 pages = {33904--33919},
 title = {Finite-Time Last-Iterate Convergence for Learning in Multi-Player Games},
 year = {2022}
}

@inproceedings{Gorbunov2022lastiterate,
 author = {Gorbunov, Eduard and Taylor, Adrien and Gidel, Gauthier},
 booktitle = {Advances in Neural Information Processing Systems 35 (NeurIPS)},
 pages = {21858--21870},
 title = {Last-Iterate Convergence of Optimistic Gradient Method for Monotone Variational Inequalities},
 year = {2022}
}

@inproceedings{hsieh2019convergence,
  title={On the convergence of single-call stochastic extra-gradient methods},
  author={Hsieh, Yu-Guan and Iutzeler, Franck and Malick, J{\'e}r{\^o}me and Mertikopoulos, Panayotis},
  booktitle={Advances in Neural Information Processing Systems 32 (NeurIPS)},
  pages={6938--6948},
  year={2019}
}

@inproceedings{golowich2020last,
  title={Last iterate is slower than averaged iterate in smooth convex-concave saddle point problems},
  author={Golowich, Noah and Pattathil, Sarath and Daskalakis, Constantinos and Ozdaglar, Asuman},
  booktitle={Proceedings of the 33rd Conference on Learning Theory (COLT)},
  pages={1758--1784},
  year={2020},
}

@inproceedings{wei21linear,
  title={Linear Last-iterate Convergence in Constrained Saddle-point Optimization},
  author={Wei, Chen-Yu and Lee, Chung-Wei and Zhang, Mengxiao and Luo, Haipeng},
  booktitle={The 9th International Conference on Learning Representations (ICLR)},
  year={2021}
}
%%% last-iterate

%%% high-probability-convergence
@InProceedings{pmlr-v202-liu23aa,
  title = 	 {High Probability Convergence of Stochastic Gradient Methods},
  author =       {Liu, Zijian and Nguyen, Ta Duy and Nguyen, Thien Hang and Ene, Alina and Nguyen, Huy},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning (ICML)},
  pages = 	 {21884--21914},
  year = 	 {2023},
}

@InProceedings{pmlr-v202-sadiev23a,
  title = 	 {High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance},
  author =       {Sadiev, Abdurakhmon and Danilova, Marina and Gorbunov, Eduard and Horv\'{a}th, Samuel and Gidel, Gauthier and Dvurechensky, Pavel and Gasnikov, Alexander and Richt\'{a}rik, Peter},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning (ICML)},
  pages = 	 {29563--29648},
  year = 	 {2023},
}

@inproceedings{ICML:2024:Liu,
    author = {Langqi Liu and Yibo Wang and Lijun Zhang},
    title = {High-Probability Bound for Non-Smooth Non-Convex Stochastic Optimization with Heavy Tails},
    booktitle = {Proceedings of the 41st International Conference on Machine Learning (ICML)},
    pages = {32122--32138},
    year = {2024},
}

@article{ghadimi2012optimal,
  title={Optimal stochastic approximation algorithms for strongly convex stochastic composite optimization i: A generic algorithmic framework},
  author={Ghadimi, Saeed and Lan, Guanghui},
  journal={SIAM Journal on Optimization},
  volume={22},
  number={4},
  pages={1469--1492},
  year={2012},
  publisher={SIAM}
}

@article{ghadimi2013stochastic,
  title={Stochastic first-and zeroth-order methods for nonconvex stochastic programming},
  author={Ghadimi, Saeed and Lan, Guanghui},
  journal={SIAM Journal on Optimization},
  volume={23},
  number={4},
  pages={2341--2368},
  year={2013},
  publisher={SIAM}
}
%%% high-probability-convergence

%%% variational inequality
@article{nemirovski2004prox,
  title={Prox-method with rate of convergence {$O(1/t)$} for variational inequalities with {L}ipschitz continuous monotone operators and smooth convex-concave saddle point problems},
  author={Nemirovski, Arkadi},
  journal={SIAM Journal on Optimization},
  volume={15},
  number={1},
  pages={229--251},
  year={2004},
  publisher={SIAM}
}

@article{nesterov2007dual,
  title={Dual extrapolation and its applications to solving variational inequalities and related problems},
  author={Nesterov, Yurii},
  journal={Mathematical Programming},
  volume={109},
  number={2},
  pages={319--344},
  year={2007},
  publisher={Springer}
}

@article{popov1980modification,
  title={A modification of the Arrow-Hurwicz method for search of saddle points},
  author={Popov, Leonid Denisovich},
  journal={Mathematical notes of the Academy of Sciences of the USSR},
  volume={28},
  pages={845--848},
  year={1980},
  publisher={Springer}
}

@article{korpelevich1976extragradient,
  title={The extragradient method for finding saddle points and other problems},
  author={Korpelevich, Galina M},
  journal={Matecon},
  volume={12},
  pages={747--756},
  year={1976}
}

@InProceedings{azizian21omdsvi,
  title = 	 {The Last-Iterate Convergence Rate of Optimistic Mirror Descent in Stochastic Variational Inequalities},
  author =       {Azizian, Wa\"iss and Iutzeler, Franck and Malick, J\'er\^ome and Mertikopoulos, Panayotis},
  booktitle = 	 {Proceedings of the 34th Conference on Learning Theory (COLT)},
  pages = 	 {326--358},
  year = 	 {2021},
}

@article{juditsky2011solving,
  title={Solving variational inequalities with stochastic mirror-prox algorithm},
  author={Juditsky, Anatoli and Nemirovski, Arkadi and Tauvel, Claire},
  journal={Stochastic Systems},
  volume={1},
  number={1},
  pages={17--58},
  year={2011},
  publisher={INFORMS}
}
%%% variational inequality

%%% mirror descent
@article{nemirovskij1983problem,
  title={Problem complexity and method efficiency in optimization},
  author={Arkadi Semen Nemirovski and David Berkovich Yudin},
  year={1983},
  journal={Wiley-Interscience},
  publisher={Wiley-Interscience}
}

@article{beck2003mirror,
  title={Mirror descent and nonlinear projected subgradient methods for convex optimization},
  author={Beck, Amir and Teboulle, Marc},
  journal={Operations Research Letters},
  volume={31},
  number={3},
  pages={167--175},
  year={2003},
  publisher={Elsevier}
}

@article{nemirovski2009robust,
  title={Robust stochastic approximation approach to stochastic programming},
  author={Nemirovski, Arkadi and Juditsky, Anatoli and Lan, Guanghui and Shapiro, Alexander},
  journal={SIAM Journal on Optimization},
  volume={19},
  number={4},
  pages={1574--1609},
  year={2009},
  publisher={SIAM}
}

@article{lei2017analysis,
  title={Analysis of online composite mirror descent algorithm},
  author={Lei, Yunwen and Zhou, Ding-Xuan},
  journal={Neural Computation},
  volume={29},
  number={3},
  pages={825--860},
  year={2017},
}

@inproceedings{zhou17smd,
author = {Zhou, Zhengyuan and Mertikopoulos, Panayotis and Bambos, Nicholas and Boyd, Stephen and Glynn, Peter},
title = {Stochastic mirror descent in variationally coherent optimization problems},
year = {2017},
booktitle = {Advances in Neural Information Processing Systems 31 (NIPS)},
pages = {7043–7052},
}

@article{zhang2018convergence,
  title={On the convergence rate of stochastic mirror descent for nonsmooth nonconvex optimization},
  author={Zhang, Siqi and He, Niao},
  journal={arXiv preprint arXiv:1806.04781},
  year={2018}
}

@inproceedings{huang2021efficient,
  title={Efficient mirror descent ascent methods for nonsmooth minimax problems},
  author={Huang, Feihu and Wu, Xidong and Huang, Heng},
  booktitle={Advances in Neural Information Processing Systems 34 (NeurIPS)},
  pages={10431--10443},
  year={2021}
}

@inproceedings{liu2023MirrorDiffusion,
 author = {Liu, Guan-Horng and Chen, Tianrong and Theodorou, Evangelos and Tao, Molei},
 booktitle = {Advances in Neural Information Processing Systems 36 (NeurIPS)},
 pages = {42898--42917},
 title = {Mirror Diffusion Models for Constrained and Watermarked Generation},
 year = {2023}
}

@article{lan2023policy,
  title={Policy mirror descent for reinforcement learning: Linear convergence, new sampling complexity, and generalized problem classes},
  author={Lan, Guanghui},
  journal={Mathematical Programming},
  volume={198},
  number={1},
  pages={1059--1106},
  year={2023},
  publisher={Springer}
}

@inproceedings{MontgomeryNIPS2016Guided,
 author = {Montgomery, William H and Levine, Sergey},
 booktitle = {Advances in Neural Information Processing Systems 29 (NIPS)},
 pages = {4008--4016},
 title = {Guided Policy Search via Approximate Mirror Descent},
 year = {2016}
}

@article{ben2001ordered,
  title={The ordered subsets mirror descent optimization method with applications to tomography},
  author={Ben-Tal, Aharon and Margalit, Tamar and Nemirovski, Arkadi},
  journal={SIAM Journal on Optimization},
  volume={12},
  number={1},
  pages={79--108},
  year={2001},
  publisher={SIAM}
}


@InProceedings{pmlr-v130-ajanthan21a,
  title = 	 {Mirror Descent View for Neural Network Quantization},
  author =       {Ajanthan, Thalaiyasingam and Gupta, Kartik and Torr, Philip and Hartley, Richard and Dokania, Puneet},
  booktitle = 	 {Proceedings of the 24th International Conference on Artificial Intelligence and Statistics (AISTATS)},
  pages = 	 {2809--2817},
  year = 	 {2021},
}

@inproceedings{allen2017linear,
  title={Linear Coupling: An Ultimate Unification of Gradient and Mirror Descent},
  author={Allen-Zhu, Zeyuan and Orecchia, Lorenzo},
  booktitle={8th Innovations in Theoretical Computer Science Conference (ITCS)},
  pages = {3:1--3:22},
  year={2017},
}

@InProceedings{pmlr-v125-zhang20a,
  title = 	 {Wasserstein Control of Mirror Langevin Monte Carlo},
  author =       {Zhang, Kelvin Shuangjian and Peyr\'e, Gabriel and Fadili, Jalal and Pereyra, Marcelo},
  booktitle = 	 {Proceedings of 33rd Conference on Learning Theory (COLT)},
  pages = 	 {3814--3841},
  year = 	 {2020},
}
%%% mirror descent

%%% optimistic
@InProceedings{chiang2012online,
  title = {Online Optimization with Gradual Variations},
  author = {Chiang, Chao-Kai and Yang, Tianbao and Lee, Chia-Jung and Mahdavi, Mehrdad and Lu, Chi-Jen and Jin, Rong and Zhu, Shenghuo},
  booktitle = {Proceedings of the 25th Annual Conference on Learning Theory (COLT)},
  pages = {6.1--6.20},
  year = {2012},
}

@inproceedings{rakhlin2013online,
  title={Online learning with predictable sequences},
  author={Rakhlin, Alexander and Sridharan, Karthik},
  booktitle={Proceedings of the 26th Annual Conference on Learning Theory (COLT)},
  pages={993--1019},
  year={2013},
}

@inproceedings{rakhlin2013optimization,
  title={Optimization, learning, and games with predictable sequences},
  author={Rakhlin, Alexander and Sridharan, Karthik},
  booktitle={Advances in Neural Information Processing Systems 26 (NIPS)},
  pages = {3066–3074},
  year={2013}
}

@article{nguyen2018extragradient,
  title={Extragradient method in optimization: Convergence and complexity},
  author={Nguyen, Trong Phong and Pauwels, Edouard and Richard, Emile and Suter, Bruce W},
  journal={Journal of Optimization Theory and Applications},
  volume={176},
  pages={137--162},
  year={2018},
  publisher={Springer}
}

@article{JMLR:2024:Chen,
author={Sijia Chen and Yu-Jie Zhang and Wei-Wei Tu and Peng Zhao and Lijun Zhang},
journal={Journal of Machine Learning Research (JMLR)},
title={Optimistic Online Mirror Descent for Bridging Stochastic and Adversarial Online Convex Optimization},
year={2024},
volume={25},
number={178},
pages={1--62},
}

@inproceedings{NeurIPS:2024:Wang,
    author = {Yibo Wang and Sijia Chen and Wei Jiang and Wenhao Yang and Yuanyu Wan and Lijun Zhang},    
    title = {Online Composite Optimization Between Stochastic and Adversarial Environments},
    booktitle = {Advances in Neural Information Processing Systems 37 (NeurIPS)},    
    pages = {to appear},    
    year = {2024},
}

@article{mokhtari2020convergence,
  title={Convergence rate of {$O(1/k)$} for optimistic gradient and extragradient methods in smooth convex-concave saddle point problems},
  author={Mokhtari, Aryan and Ozdaglar, Asuman E and Pattathil, Sarath},
  journal={SIAM Journal on Optimization},
  volume={30},
  number={4},
  pages={3230--3251},
  year={2020},
  publisher={SIAM}
}
%%% optimistic

%%% noise
@article{Bottou18optimization,
    author = {Bottou, L\'{e}on and Curtis, Frank E. and Nocedal, Jorge},
    title = {Optimization Methods for Large-Scale Machine Learning},
    journal = {SIAM Review},
    volume = {60},
    number = {2},
    pages = {223-311},
    year = {2018},
}

@article{liu2023gs,
  author       = {Zijian Liu and Srikanth Jagabathula and Zhengyuan Zhou},
  title        = {Near-Optimal Non-Convex Stochastic Optimization under Generalized Smoothness},
  journal      = {arXiv preprint arXiv:2302.06032v2},
  year         = {2023},
}

@InProceedings{pmlr-v178-faw22a,
  title = 	 {The Power of Adaptivity in SGD: Self-Tuning Step Sizes with Unbounded Gradients and Affine Variance},
  author =       {Faw, Matthew and Tziotis, Isidoros and Caramanis, Constantine and Mokhtari, Aryan and Shakkottai, Sanjay and Ward, Rachel},
  booktitle = 	 {Proceedings of the 35th Conference on Learning Theory (COLT)},
  pages = 	 {313--355},
  year = 	 {2022},
}

@article{hwang1986multiplicative,
  title={Multiplicative errors-in-variables models with applications to recent data released by the US Department of Energy},
  author={Hwang, Jiunn T},
  journal={Journal of the American Statistical Association},
  volume={81},
  number={395},
  pages={680--688},
  year={1986},
  publisher={Taylor \& Francis}
}

@inproceedings{Loh11high,
    author = {Loh, Po-Ling and Wainwright, Martin J.},
    title = {High-dimensional regression with noisy and missing data: provable guarantees with non-convexity},
    year = {2011},
    booktitle = {Advances in Neural Information Processing Systems 25 (NIPS)},
    pages = {2726–2734},
}

@article{sancho1982analytical,
  title={Analytical and numerical studies of multiplicative noise},
  author={Sancho, Jos{\'e} M and San Miguel, M and Katz, SL and Gunton, JD},
  journal={Physical Review A},
  volume={26},
  number={3},
  pages={1589},
  year={1982},
  publisher={APS}
}

@article{lopez2003polarimetric,
  title={Polarimetric SAR speckle noise model},
  author={L{\'o}pez-Mart{\'\i}nez, Carlos and Fabregas, Xavier},
  journal={IEEE Transactions on Geoscience and Remote Sensing},
  volume={41},
  number={10},
  pages={2232--2242},
  year={2003},
  publisher={IEEE}
}

@article{aubert2008variational,
  title={A variational approach to removing multiplicative noise},
  author={Aubert, Gilles and Aujol, Jean-Francois},
  journal={SIAM Journal on Applied Mathematics},
  volume={68},
  number={4},
  pages={925--946},
  year={2008},
  publisher={SIAM}
}

@inproceedings{hodgkinson2021multiplicative,
  title={Multiplicative noise and heavy tails in stochastic optimization},
  author={Hodgkinson, Liam and Mahoney, Michael},
  booktitle={Proceedings of the 38th International Conference on Machine Learning (ICML)},
  pages={4262--4274},
  year={2021},
}

@inproceedings{gurbuzbalaban2021heavy,
  title={The heavy-tail phenomenon in SGD},
  author={Gurbuzbalaban, Mert and Simsekli, Umut and Zhu, Lingjiong},
  booktitle={Proceedings of the 38th International Conference on Machine Learning (ICML)},
  pages={3964--3975},
  year={2021},
}

@inproceedings{cutkosky2021high,
  title={High-probability bounds for non-convex stochastic optimization with heavy tails},
  author={Cutkosky, Ashok and Mehta, Harsh},
  booktitle={Advances in Neural Information Processing Systems 34 (NeurIPS)},
  pages={4883--4895},
  year={2021}
}

@inproceedings{Diakonikolas2023obliviousNoise,
 author = {Diakonikolas, Ilias and Karmalkar, Sushrut and Park, Jong Ho and Tzamos, Christos},
 booktitle = {Advances in Neural Information Processing Systems 36 (NeurIPS)},
 pages = {19673--19699},
 title = {First Order Stochastic Optimization with Oblivious Noise},
 year = {2023}
}

@inproceedings{shi2021rmsprop,
    title={{RMS}prop converges with proper hyper-parameter},
    author={Naichen Shi and Dawei Li and Mingyi Hong and Ruoyu Sun},
    booktitle={The 9th International Conference on Learning Representations (ICLR)},
    year={2021},
}

@inproceedings{jin2021nonconvexdro,
 author = {Jin, Jikai and Zhang, Bohang and Wang, Haiyang and Wang, Liwei},
 booktitle = {Advances in Neural Information Processing Systems 34 (NeurIPS)},
 pages = {2771--2782},
 title = {Non-convex Distributionally Robust Optimization: Non-asymptotic Analysis},
 year = {2021}
}

@article{zhang2024gs,
  author       = {Qi Zhang and Yi Zhou and Shaofeng Zou},
  title        = {Convergence Guarantees for RMSProp and Adam in Generalized-smooth Non-convex Optimization with Affine Noise Variance},
  journal      = {arXiv preprint arXiv:2404.01436v2},
  year         = {2024},
}

@inproceedings{hong2024on,
    title={On Convergence of Adam for Stochastic Optimization under Relaxed Assumptions},
    author={Yusu Hong and Junhong Lin},
    booktitle={Advances in Neural Information Processing Systems 37 (NeurIPS)},
    year={2024},
    pages = {to appear},
}

@InProceedings{pmlr-v202-koloskova23a,
  title = 	 {Revisiting Gradient Clipping: Stochastic bias and tight convergence guarantees},
  author =       {Koloskova, Anastasia and Hendrikx, Hadrien and Stich, Sebastian U},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning (ICML)},
  pages = 	 {17343--17363},
  year = 	 {2023},
}

@inproceedings{wang2023adamlowerbound,
 author = {Wang, Bohan and Fu, Jingwen and Zhang, Huishuai and Zheng, Nanning and Chen, Wei},
 booktitle = {Advances in Neural Information Processing Systems 36 (NeurIPS)},
 pages = {39006--39032},
 title = {Closing the gap between the upper bound and lower bound of Adam\textquotesingle s iteration complexity},
 year = {2023}
}

@article{carmon2020lower,
  title={Lower bounds for finding stationary points I},
  author={Carmon, Yair and Duchi, John C and Hinder, Oliver and Sidford, Aaron},
  journal={Mathematical Programming},
  volume={184},
  number={1},
  pages={71--120},
  year={2020},
  publisher={Springer}
}

@article{arjevani2023lower,
  title={Lower bounds for non-convex stochastic optimization},
  author={Arjevani, Yossi and Carmon, Yair and Duchi, John C and Foster, Dylan J and Srebro, Nathan and Woodworth, Blake},
  journal={Mathematical Programming},
  volume={199},
  number={1},
  pages={165--214},
  year={2023},
  publisher={Springer}
}

@inproceedings{NeurIPS:2018:Zhang:A,
    author = {Lijun Zhang and Zhi-Hua Zhou},
    title = {$\ell_1$-regression with Heavy-tailed Distributions},
    booktitle = {Advances in Neural Information Processing Systems 31 (NeurIPS)},
    pages = {1076--1086},
    year = {2018},
}

@article{liu2024nonconvex,
  title={Nonconvex Stochastic Optimization under Heavy-Tailed Noises: Optimal Convergence without Gradient Clipping},
  author={Liu, Zijian and Zhou, Zhengyuan},
  journal={arXiv preprint arXiv:2412.19529},
  year={2024}
}
%%% noise

@article{polyak1963gradient,
  title={Gradient methods for minimizing functionals},
  author={Polyak, Boris Teodorovich},
  journal={Zhurnal vychislitel'noi matematiki i matematicheskoi fiziki},
  volume={3},
  number={4},
  pages={643--653},
  year={1963},
  publisher={Russian Academy of Sciences, Branch of Mathematical Sciences}
}

@article{lojasiewicz1963propriete,
  title={Une propri{\'e}t{\'e} topologique des sous-ensembles analytiques r{\'e}els},
  author={Lojasiewicz, Stanislaw},
  journal={Les {\'e}quations aux d{\'e}riv{\'e}es partielles},
  volume={117},
  pages={87--89},
  year={1963}
}

@inproceedings{yu24egdro,
  title = 	 {Efficient Algorithms for Empirical Group Distributionally Robust Optimization and Beyond},
  author =       {Yu, Dingzhi and Cai, Yunuo and Jiang, Wei and Zhang, Lijun},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning (ICML)},
  pages = 	 {57384--57414},
  year = 	 {2024},
}

@inproceedings{srebro2010smoothness,
 author = {Srebro, Nathan and Sridharan, Karthik and Tewari, Ambuj},
 booktitle = {Advances in Neural Information Processing Systems 23 (NIPS)},
 pages = {2199-2207},
 title = {Smoothness, Low Noise and Fast Rates},
 year = {2010}
}

@inproceedings{COLT:2017:Zhang,
title = {Empirical Risk Minimization for Stochastic Convex Optimization: ${O}(1/n)$- and ${O}(1/n^2)$-type of Risk Bounds},
author = {Lijun Zhang and Tianbao Yang and Rong Jin},
booktitle = {Proceedings of the 30th Conference on Learning Theory (COLT)},
year = {2017},
pages = {1954--1979},
}

@inproceedings{COLT:2019:Zhang,
title = {Stochastic Approximation of Smooth and Strongly Convex Functions: Beyond the ${O}(1/{T})$ Convergence Rate},
author = {Lijun Zhang and Zhi-Hua Zhou},
booktitle = {Proceedings of the 32nd Conference on Learning Theory (COLT)},
year = {2019},
pages = {3160--3179},
}

@inproceedings{xu2020second,
  title={Second-order optimization for non-convex machine learning: An empirical study},
  author={Xu, Peng and Roosta, Fred and Mahoney, Michael W},
  booktitle={Proceedings of the 2020 SIAM International Conference on Data Mining},
  pages={199--207},
  year={2020},
}

@article{agarwal2017second,
  title={Second-order stochastic optimization for machine learning in linear time},
  author={Agarwal, Naman and Bullins, Brian and Hazan, Elad},
  journal={Journal of Machine Learning Research (JMLR)},
  volume={18},
  number={116},
  pages={1--40},
  year={2017}
}

@article{davis2019proximally,
  title={Proximally guided stochastic subgradient method for nonsmooth, nonconvex problems},
  author={Davis, Damek and Grimmer, Benjamin},
  journal={SIAM Journal on Optimization},
  volume={29},
  number={3},
  pages={1908--1930},
  year={2019},
  publisher={SIAM}
}

@article{parrilo2003semidefinite,
  title={Semidefinite programming relaxations for semialgebraic problems},
  author={Parrilo, Pablo A},
  journal={Mathematical Programming},
  volume={96},
  pages={293--320},
  year={2003},
  publisher={Springer}
}

@inproceedings{devlin-etal-2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    year = "2019",
    publisher = "Association for Computational Linguistics",
    pages = "4171--4186",
}

@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={Proceedings of the 38th International Conference on Machine Learning (ICML)},
  pages={8748--8763},
  year={2021},
}

@inproceedings{caron2021emerging,
  title={Emerging properties in self-supervised vision transformers},
  author={Caron, Mathilde and Touvron, Hugo and Misra, Ishan and J{\'e}gou, Herv{\'e} and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision (CVPR)},
  pages={9650--9660},
  year={2021}
}

@article{sagun2016eigenvalues,
  title={Eigenvalues of the hessian in deep learning: Singularity and beyond},
  author={Sagun, Levent and Bottou, Leon and LeCun, Yann},
  journal={arXiv preprint arXiv:1611.07476},
  year={2016}
}

@inproceedings{pan2022eigencurve,
    title={Eigencurve: Optimal Learning Rate Schedule for {SGD} on Quadratic Objectives with Skewed Hessian Spectrums},
    author={Rui Pan and Haishan Ye and Tong Zhang},
    booktitle={The 10th International Conference on Learning Representations (ICLR)},
    year={2022},
}

@inproceedings{merity2018regularizing,
    title={Regularizing and Optimizing {LSTM} Language Models},
    author={Stephen Merity and Nitish Shirish Keskar and Richard Socher},
    booktitle={The 6th International Conference on Learning Representations (ICLR)},
    year={2018},
}

@misc{hinton2012neural,
  title={Neural networks for machine learning lecture 6a overview of mini-batch gradient descent},
  author={Hinton, Geoffrey and Srivastava, Nitish and Swersky, Kevin},
  year={2012},
}

@inproceedings{kingma15adam,
  author={Diederik P. Kingma and Jimmy Ba},
  title={Adam: A Method for Stochastic Optimization},
  year={2015},
  booktitle={The 3rd International Conference on Learning Representations (ICLR)},
}

@inproceedings{defazio2019ineffectiveness,
  title={On the ineffectiveness of variance reduced optimization for deep learning},
  author={Defazio, Aaron and Bottou, Leon},
  booktitle={Advances in Neural Information Processing Systems 33 (NeurIPS)},
  pages={1755--1765},
  year={2019}
}

@article{conrad2018equivalence,
  title={Equivalence of norms},
  author={Conrad, Keith},
  journal={Expository Paper, University of Connecticut, Storrs, heruntergeladen von},
  volume={17},
  number={2018},
  year={2018}
}

@article{JMLR:v12:duchi11a,
  author  = {John Duchi and Elad Hazan and Yoram Singer},
  title   = {Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
  journal = {Journal of Machine Learning Research (JMLR)},
  year    = {2011},
  volume  = {12},
  number  = {61},
  pages   = {2121--2159},
}

@article{robbins1951stochastic,
  title={A stochastic approximation method},
  author={Robbins, Herbert and Monro, Sutton},
  journal={The Annals of Mathematical Statistics},
  pages={400--407},
  year={1951},
  publisher={JSTOR}
}

@InProceedings{pmlr-v28-pascanu13,
  title = 	 {On the difficulty of training recurrent neural networks},
  author = 	 {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
  booktitle = 	 {Proceedings of the 30th International Conference on Machine Learning (ICML)},
  pages = 	 {1310--1318},
  year = 	 {2013},
}

@article{mikolov2012statistical,
  title={Statistical language models based on neural networks},
  author={Mikolov, Tom{\'a}{\v{s}} and others},
  year={2012},
  journal={PhD thesis, Brno University of Technology},
  publisher={Brno University of Technology}
}

@article{nesterov1984minimization,
  title={Minimization methods for nonsmooth convex and quasiconvex functions},
  author={Nesterov, Yurii},
  journal={Matekon},
  volume={29},
  number={3},
  pages={519--531},
  year={1984}
}

@article{cortes2006finite,
  title={Finite-time convergent gradient flows with applications to network consensus},
  author={Cort{\'e}s, Jorge},
  journal={Automatica},
  volume={42},
  number={11},
  pages={1993--2000},
  year={2006},
  publisher={Elsevier}
}

@inproceedings{fang2018spider,
 author = {Fang, Cong and Li, Chris Junchi and Lin, Zhouchen and Zhang, Tong},
 booktitle = {Advances in Neural Information Processing Systems 31 (NeurIPS)},
 pages = {689--699},
 title = {SPIDER: Near-Optimal Non-Convex Optimization via Stochastic Path-Integrated Differential Estimator},
 year = {2018}
}

@inproceedings{nesterov1983method,
  title={A method for solving the convex programming problem with convergence rate {$O(1/k^2)$}},
  author={Nesterov, Yurii},
  booktitle={Dokl akad nauk Sssr},
  volume={269},
  pages={543},
  year={1983}
}

@article{chzhen2023signsvrg,
  title={SignSVRG: fixing SignSGD via variance reduction},
  author={Chzhen, Evgenii and Schechtman, Sholom},
  journal={arXiv preprint arXiv:2305.13187},
  year={2023}
}

@inproceedings{malitsky2020adaptive,
  title={Adaptive Gradient Descent without Descent},
  author={Malitsky, Yura and Mishchenko, Konstantin},
  booktitle={Proceedings of the 37th International Conference on Machine Learning (ICML)},
  pages={6702--6712},
  year={2020},
}

@article{gasnikov2018universal,
  title={Universal method for stochastic composite optimization problems},
  author={Gasnikov, Alexander Vladimirovich and Nesterov, Yu E},
  journal={Computational Mathematics and Mathematical Physics},
  volume={58},
  pages={48--64},
  year={2018},
  publisher={Springer}
}

@inproceedings{NEURIPS2019PYTORCH,
 author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
 booktitle = {Advances in Neural Information Processing Systems 32 (NeurIPS)},
 pages = {8026-8037},
 title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
 year = {2019}
}

@inproceedings{bhattiprolu2019approximability,
  title={Approximability of {$p\to q$} matrix norms: generalized krivine rounding and hypercontractive hardness},
  author={Bhattiprolu, Vijay and Ghosh, Mrinalkanti and Guruswami, Venkatesan and Lee, Euiwoong and Tulsiani, Madhur},
  booktitle={Proceedings of the 30th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA)},
  pages={1358--1368},
  year={2019},
}

@inproceedings{bhaskara2011approximating,
  title={Approximating matrix p-norms},
  author={Bhaskara, Aditya and Vijayaraghavan, Aravindan},
  booktitle={Proceedings of the 22nd annual ACM-SIAM symposium on Discrete Algorithms (SODA)},
  pages={497--511},
  year={2011},
  organization={SIAM}
}

@article{pisier2012grothendieck,
  title={Grothendieck’s theorem, past and present},
  author={Pisier, Gilles},
  journal={Bulletin of the American Mathematical Society},
  volume={49},
  number={2},
  pages={237--323},
  year={2012}
}

@article{khot2012grothendieck,
  title={Grothendieck-Type Inequalities in Combinatorial Optimization},
  author={Khot, Subhash and Naor, Assaf},
  journal={Communications on Pure and Applied Mathematics},
  volume={65},
  number={7},
  pages={992--1035},
  year={2012},
}