 \documentclass[pmlr,11pt]{jmlr}
% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf

%\usepackage{pdflscape}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{paralist,amsmath, amssymb, bm}
\usepackage{algorithm,algorithmic}
\usepackage{times}

%\usepackage{jmlr2e}
%\usepackage{jmlr}
%\usepackage[colorlinks,
%            linkcolor=blue,
%            citecolor=blue,
%            urlcolor=magenta,
%            linktocpage,
%            plainpages=false]{hyperref}

% added packages
\usepackage[capitalize,noabbrev]{cleveref}
\usepackage{chngcntr}
\usepackage{apptools}
\AtAppendix{\counterwithin{lem}{section}}

\makeatletter
\newcounter{ALC@tempcntr}% Temporary counter for storage
\newcommand{\LCOMMENT}[1]{%
    \setcounter{ALC@tempcntr}{\arabic{ALC@rem}}% Store old counter
    \setcounter{ALC@rem}{1}% To avoid printing line number
    \item \{#1\}% Display comment + does not increment list item counter
    \setcounter{ALC@rem}{\arabic{ALC@tempcntr}}% Restore old counter
}%
\makeatother

\makeatletter
\AtBeginDocument{\Hy@breaklinkstrue}
\makeatother

\newtheorem{thm}{Theorem}
\newtheorem{prop}{Proposition}
\newtheorem{lem}{Lemma}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{ass}{Assumption}
\newtheorem{defn}{Definition}

\def \y {\mathbf{y}}
\def \E {\mathbb{E}}
\def \x {\mathbf{x}}
%\def \v {\mathbf{v}}
\def \bv {\mathbf{v}}
\def \g {\mathbf{g}}
% \def \L {\mathcal{L}}
\def \D {\mathcal{D}}
\def \O {\widetilde{O}}

\def \bo {\mathbf{o}}

\def \z {\mathbf{z}}
\def \Z {\mathcal{Z}}

\def \u {\mathbf{u}}
\def \H {\mathcal{H}}
\def \w {\mathbf{w}}
\def \bv {\mathbf{v}}
\def \R {\mathbb{R}}
\def \zn {\mathbb{Z}_+}

\def \regret {\mbox{regret}}
\def \Uh {\widehat{U}}
\def \P {\mathcal{P}}
\def \F {\mathcal{F}}
\def \G {\mathcal{G}}
\def \Q {\mathcal{Q}}
\def \W {\mathcal{W}}
\def \N {\mathcal{N}}
\def \A {\mathcal{A}}
\def \q {\mathbf{q}}
\def \bv {\mathbf{v}}
\def \M {\mathcal{M}}
\def \c {\mathbf{c}}
\def \ph {\widehat{p}}
\def \d {\mathbf{d}}
\def \p {\mathbf{p}}
\def \q {\mathbf{q}}
\def \db {\bar{\d}}
\def \dbb {\bar{d}}
\def \I {\mathcal{I}}
\def \xt {\widetilde{\x}}
\def \f {\mathbf{f}}
\def \a {\mathbf{a}}
\def \b {\mathbf{b}}
\def \ft {\widetilde{\f}}
\def \bt {\widetilde{\b}}
\def \h {\mathbf{h}}
% \def \B {\mathbf{B}}
\def \SS {\mathcal{S}}
\def \tb {\tilde{b}}
\def \bts {\widetilde{b}}
\def \fts {\widetilde{f}}
\def \Gh {\widehat{G}}
\def \bh {\widehat{b}}
\def \fh {\widehat{f}}
\def \ub {\bar{\u}}
\def \tg {\tilde{g}}
\def \zt {\widetilde{\z}}
\def \zh {\hat{\z}}
\def \zts {\widetilde{z}}
\def \s {\mathbf{s}}
\def \gh {\hat{\g}}
\def \gt {\tilde{\g}}
\def \lb {\mathbf{l}}

\def \bta {\bm \delta}
\def \Pb {\overline{P}}

\def \rhoh {\widehat{\rho}}
\def \hh {\widehat{\h}}
\def \C {\mathcal{C}}
\def \V {\mathcal{V}}
\def \t {\mathbf{t}}
\def \m {\mathbf{m}}
\def \xh {\widehat{\x}}
% \def \xb {\bar{\x}}
% modified to longer line
\def \xb {\overline{\x}}
\def \Ut {\widetilde{U}}
\def \wt {\widetilde{\w}}
\def \Th {\widehat{T}}
\def \Ot {\tilde{\mathcal{O}}}
\def \nb {\bar{n}}
\def \K {\mathcal{K}}
\def \T {\mathcal{T}}
\def \F {\mathcal{F}}
\def \ft{\widetilde{f}}
% \def \xt {\widetilde{x}}
\def \Rt {\mathcal{R}}
\def \Rb {\bar{\Rt}}
\def \wb {\bar{\w}}
\def \qb {\bar{\q}}
\def \bb {\mu}
\def \fh {\widehat{f}}
\def \wh {\widehat{\w}}
\def \qh {\widehat{\q}}
\def \lh {\widehat{\lambda}}
\def \e {\mathbf{e}}
\def \B {\mathcal{B}}
\def \X {\mathcal{X}}
\def \Y {\mathcal{Y}}
\def \bv {\mathbf{v}}

\def \Lt  {\widetilde{L}}

%\def \Dw {D_w}
%\def \aw {\alpha_w}

\def \aw {\alpha}

\def \N {\mathbb{N}}
\def \MR {R}
\def \ind {\mathbb{I}}

\def \bsga {\bar{\sigma}}
\def \bSga {\bar{\Sigma}}

%% add by PengZ
%\usepackage{dsfont}
\newcommand{\inner}[2]{\langle #1, #2 \rangle}
\def \hats {\hat{s}}
\def \hatsb {\hat{\s}}
\def \tildes {\tilde{s}}
\def \tildesb {\tilde{\s}}
\newcommand \indicator[1]{\mathbb{I}[#1]}

\DeclareMathOperator*{\Reg}{Regret}
\DeclareMathOperator*{\WAReg}{A-Regret}
\DeclareMathOperator*{\SAReg}{SA-Regret}
\DeclareMathOperator*{\DReg}{D-Regret}
\DeclareMathOperator*{\poly}{poly}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\sgn}{sign}
\DeclareMathOperator*{\erf}{erf}

\DeclareMathOperator*{\RS}{R-S}
\DeclareMathOperator*{\ARS}{A-R-S}

%%% added by ydz
\newcommand{\norm}[1]{\left\Vert#1\right\Vert}
\newcommand{\dnorm}[1]{\left\Vert#1\right\Vert_*}
\def \tdr {\widetilde{r}}
\def \tdel {\widetilde{\ell}}
\def \Pr    {\mathbb{P}}
\def \eps {\bm{\epsilon}}
\def \one {\mathbf{1}}
\newcommand{\sqbrac}[1]{\left[#1\right]}
\newcommand{\brac}[1]{\left(#1\right)}
\newcommand{\cbrac}[1]{\left\{#1\right\}}
\newcommand{\crefpairconjunction}{ and }
\newcommand{\creflastconjunction}{ and }
\crefname{ass}{Assumption}{Assumptions}
\crefname{defn}{Definition}{Definitions}
\crefname{lem}{Lemma}{Lemmas}
\crefname{thm}{Theorem}{Theorems}

\setlength\parindent{0pt}
% 这里把间距设置为0.3 cm，你们可以自己调整
\newcommand{\setParDis}{\vspace{7.5pt} }
\usepackage{fancybox}
\usepackage{threeparttable}
\usepackage{booktabs} % for professional table lines
\usepackage{bbding}
\usepackage{enumerate}
\usepackage{subcaption}

% Strut macros for skipping spaces above and below text in tables.
\def\abovestrut#1{\rule[0in]{0in}{#1}\ignorespaces}
\def\belowstrut#1{\rule[-#1]{0in}{#1}\ignorespaces}

\def\abovespace{\abovestrut{0.20in}}
\def\aroundspace{\abovestrut{0.20in}\belowstrut{0.10in}}
\def\belowspace{\belowstrut{0.10in}}

\def\abovespaceLarge{\abovestrut{0.30in}}
\def\belowspaceLarge{\belowstrut{0.15in}}

% added by yudz
% \usepackage[capitalize,noabbrev]{cleveref}
% \crefname{definition}{definition}{definitions} % For lowercase reference
% \Crefname{definition}{Definition}{Definitions} % For capitalized reference

\begin{document}

\title[Mirror Descent Under Generalized Smoothness]{Mirror Descent Under Generalized Smoothness}

\coltauthor{%
\Name{Dingzhi Yu}\textsuperscript{1,2}\Email{yudz@lamda.nju.edu.cn}\\
 % \AND
\Name{Wei Jiang}\textsuperscript{1}\Email{jiangw@lamda.nju.edu.cn}\\
\Name{Yuanyu Wan}\textsuperscript{3}\Email{wanyy@zju.edu.cn}\\
 % \AND
 \Name{Lijun Zhang}\textsuperscript{1,2} \Email{zhanglj@lamda.nju.edu.cn}\\
 \addr \addr{1} National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing 210023, China\\
 \addr \addr{2} School of Artificial Intelligence, Nanjing University, Nanjing 210023, China\\
 \addr \addr{3} School of Software Technology, Zhejiang University, Ningbo 315100, China
 % \AND
 %  \Name{Wei-Wei Tu} \Email{tuwwcn@gmail.com}\\
 % \addr 4Paradigm Inc., Beijing 100000, China
}
\maketitle

\begin{abstract}
    Smoothness is crucial for attaining fast rates in first-order optimization. However, many optimization problems in modern machine learning involve non-smooth objectives. Recent studies relax the smoothness assumption by allowing the Lipschitz constant of the gradient to grow with respect to the gradient norm, which accommodates a broad range of objectives in practice. Despite this progress, existing generalizations of smoothness are restricted to Euclidean geometry with $\ell_2$-norm and only have theoretical guarantees for optimization in the Euclidean space. In this paper, we address this limitation by introducing a new $\ell*$-smoothness concept that measures the norm of Hessian in terms of a general norm and its dual, and establish convergence for mirror-descent-type algorithms, matching the rates under the classic smoothness. Notably, we propose a generalized self-bounding property that facilitates bounding the gradients via controlling suboptimality gaps, serving as a principal component for convergence analysis. Beyond deterministic optimization, we establish an anytime convergence for stochastic mirror descent based on a new bounded noise condition that encompasses the widely adopted bounded or affine noise assumptions.
\end{abstract}

\section{Introduction}

This paper considers the convex optimization problem $\min_{\x\in\X}\ f(\x)$, where $f$ is a convex differentiable function defined on the domain $\X$. It is well known that gradient descent converges at the rate of $O(1/\sqrt{T})$ for any Lipschitz continuous $f$~\citep{nesterov2013introductory}. 
% For faster rates of \( O(1/T) \), it is typically required that $f$ is smooth, i.e., has a Lipschitz continuous gradient. 
If $f$ is smooth, i.e., has a Lipschitz continuous gradient, then a faster rate of \( O(1/T) \) can be obtained. Furthermore, an optimal \( O(1/T^2) \) rate is attained by incorporating acceleration schemes~\citep{nesterov1983method}.
However, optimization problems arising from modern machine learning (ML) are typically non-smooth~\citep{defazio2019ineffectiveness}. Even for standard objectives such as $\ell_2$-regression, the global smoothness parameter could still be unbounded~\citep{gorbunov2024methods}.
\setParDis

Recently, efforts have been made to bridge this gap between theory and practice.~\citet{Zhang2020Why} propose a generalized $(L_0,L_1)$-smooth condition, which assumes $\norm{\nabla^2 f(\x)}_2\le L_0+L_1\norm{\nabla f(\x)}_2$ for nonnegative constants $L_0,L_1$, on the basis of extensive experimental findings on language and vision models. Under this condition, they analyze the convergence of gradient clipping~\citep{mikolov2012statistical} and theoretically justify its advantages over gradient descent during neural network training. Later,~\citet{Li2023GS} propose $\ell$-smoothness which replaces the affine function specified by $(L_0,L_1)$ with an arbitrary non-decreasing sub-quadratic function $\ell(\cdot)$, i.e., $\norm{\nabla^2 f(\x)}_2\le \ell(\norm{\nabla f(\x)}_2)$. The mild condition on \(\ell(\cdot)\) ensures compatibility with objectives in modern ML and sheds light on new algorithmic designs.
\setParDis

However, existing studies on generalized smoothness suffer from a critical limitation: the definition is limited to $\ell_2$-norm and thus can only relate to optimization algorithms based on gradient descent in the Euclidean space.
In particular, none of the existing work has considered mirror descent~\citep{beck2003mirror}, a powerful optimization algorithm tailored for non-Euclidean problems. Over the years, algorithmic adaptivity to diverse geometries has received tremendous success in traditional ML~\citep{ben2001ordered,pmlr-v125-zhang20a}, reinforcement learning~\citep{MontgomeryNIPS2016Guided,lan2023policy}, and deep learning~\citep{pmlr-v130-ajanthan21a,liu2023MirrorDiffusion}, to name a few. Given the indispensable role of mirror descent in these advancements, a natural question arises:
% \setParDis
\begin{center}
    \shadowbox{\begin{minipage}[t]{0.75\columnwidth}%
Can we extend mirror descent under the generalized smoothness condition?
\end{minipage}} 
\end{center}
% \setParDis

We answer this question affirmatively by devising a new notion of generalized smoothness called $\ell*$-smoothness, which is defined under an arbitrary norm rather than $\ell_2$-norm, with full coverage of the $\ell$-smoothness proposed by~\citet{Li2023GS}.
To be more specific, we provide a fine-grained characterization of the matrix norm by utilizing a general norm and its dual. 
Under the $\ell*$-smoothness condition, we establish the convergence of mirror descent, accelerated mirror descent, optimistic mirror descent, and mirror prox algorithms, all matching classic results under $L$-smoothness in deterministic convex optimization. 
Similar to~\citet{Li2023GS}, the key ingredient in our analysis is to connect the gradients' dual norm with the suboptimality gap. To this end, we establish a generalized self-bounding property under $\ell*$-smoothness (\cref{lem:pl-inequality}), which bounds the gradient's dual norm by the product of the suboptimality gap and the local Lipschitz constant. For different types of mirror descent algorithms, we employ various strategies to demonstrate that the suboptimality gaps along the optimization trajectory can be bounded by the initial one, which may be of independent interest.
\setParDis

Furthermore, we extend our methodology to stochastic convex optimization by proposing a generalized bounded noise condition that significantly broadens the widely used assumption of bounded or affine noise. Inspired by~\citet{orabona2020last}, we bound suboptimality gaps along the optimization trajectory by converting the high-probability convergence of average-iterate to the last-iterate. Equipped with this powerful tool, we establish an anytime convergence rate of $\O(1/\sqrt{t})$ for stochastic mirror descent. Our main results for both deterministic and stochastic convex optimization is summarized in~\cref{tab:1}. Below, we introduce the notations used in this manuscript.
% \setParDis

% The rest of the paper is organized as follows.~\cref{sec:related-work} give a brief literature review on generalized smoothness.~\cref{sec:gs-function-class} introduces our refined notion of generalized smoothness and its properties.~\cref{sec:algorithms} formally presents the theoretical results for deterministic algorithms.~\cref{sec:smd} includes the new assumption on the noise and the theoretical guarantee for stochastic optimization. Below, we introduce the notations used in this manuscript.

\paragraph{Notations}  
The optimal value of the objective is denoted by $f^*:=\inf_{\x\in\X}\ f(\x)$. We denote the set of non-negative real numbers by \(\mathbb{R}_+\) and the set of positive real numbers by \(\mathbb{R}_{++}\). Positive integers and natural numbers are represented by \(\N_+\) and \(\mathbb{N}\), respectively. Let \(\norm{\cdot}\) represent a general norm on a finite-dimensional Banach space \(\mathcal{E}\), with its corresponding dual norm defined as \(\dnorm{\x} := \sup_{\y \in \mathcal{E}} \cbrac{\inner{\x}{\y} \mid \norm{\y} \le 1}\) for any $\x\in\mathcal{E}^*$, where $\mathcal{E}^*$ is the dual space of $\mathcal{E}$. We denote the ball centered at \(\x_0\) with radius \(r_0\) by \(B(\x_0, r_0) := \cbrac{\x \in \mathcal{E} \mid \norm{\x - \x_0} \le r_0}\). The $n$-dimensional all-ones vector and the one-hot vectors are denoted by $\one_n$ and $\cbrac{\e_i}_{i=1}^n$, respectively. Additionally, we use the \(\O(\cdot)\) notation to hide absolute constants and poly-logarithmic factors in \(t\).

\begin{table}[t]
    % \begin{center}
    \centering
    \begin{threeparttable}
        \caption{Summary of our main results.}
        \label{tab:1}
        \begin{tabular}{cccc}
            \toprule
            \textbf{Algorithm} & \textbf{Convergence Rate\tnote{*}} & \textbf{Type\tnote{**}} \\
            \midrule
            Mirror Descent~\citep{beck2003mirror} & $O\brac{1/T}$~(\cref{thm:mirror-descent}) & \texttt{a}\&\texttt{l} \\
            Accelerated Mirror Descent~\citep{lan2020first} & $O\brac{1/T^2}$~(\cref{thm:accelerated-mirror-descent}) & \texttt{l}\\
            Optimistic Mirror Descent~\citep{chiang2012online} & $O\brac{1/T}$~(\cref{thm:optimistic-mirror-descent}) & \texttt{a}\\
            Mirror Prox~\citep{nemirovski2004prox} & $O\brac{1/T}$~(\cref{thm:mirror-prox}) & \texttt{a}\\
            Stochastic Mirror Descent~\citep{nemirovski2009robust} & $\O\brac{1/\sqrt{t}}$~(\cref{thm:stochastic-mirror-descent}) & \texttt{a}\\
            \bottomrule
        \end{tabular}
        \begin{tablenotes}
            \footnotesize
            \item[*] \( T \) denotes the total number of iterations, while \( t \) represents anytime convergence for any \( t \in \N_+ \).
            \item[**] This column indicates the types of convergence, either average-iterate (\texttt{a}) or last-iterate (\texttt{l}).
        \end{tablenotes}
    \end{threeparttable}
    % \end{center}
\end{table}

\section{Related Work\label{sec:related-work}}

In this section, we review some concepts of generalized smoothness from the literature.

\subsection{$(L_0,L_1)$-smoothness}

\paragraph{$(L_0,L_1)$-smooth: non-convex case} 
The concept of $(L_0,L_1)$-smoothness is firstly proposed by \citet{Zhang2020Why} based on empirical observations from LSTMs~\citep{merity2018regularizing}, which allows the function to have an affine-bounded Hessian norm. Under this new condition, they analyze gradient clipping~\citep{mikolov2012statistical,pmlr-v28-pascanu13} for deterministic non-convex optimization and derive a complexity of $O(\epsilon^{-2})$\footnote{In non-convex settings, the goal is to find $\epsilon$-stationary point $\x$ satisfying $\norm{\nabla f(\x)}_2\le\epsilon$. In convex settings, we aim at $\epsilon$-suboptimal point $\x$ satisfying $f(\x)-f^*\le\epsilon$. The meaning of $\epsilon$ depends of the context.}, which matches the lower bound~\citep{carmon2020lower} up to constant factors. They further extend to stochastic settings and provide an $O(\epsilon^{-4})$ complexity bound under the uniformly bounded noise assumption.

\paragraph{$(L_0,L_1)$-smooth: convex case}
\citet{pmlr-v202-koloskova23a} show that gradient clipping has $O(\epsilon^{-1})$ complexity, matching the classic result in deterministic optimization.~\citet{takezawa2024polyak} establish the same complexity bound for gradient descent with polyak stepsizes~\citep{polyak1987introduction}. However, besides $(L_0,L_1)$-smoothness, these two works further impose an additional $L$-smooth assumption, where $L$ could be significantly larger than $L_0$ and $L_1$.~\citet{gorbunov2024methods} address the limitation and recover their results without the extra $L$-smoothness assumption. Moreover, they study a variant of adaptive gradient descent~\citep{malitsky2020adaptive} and provide an $O(\epsilon^{-1})$ complexity result, albeit with worse constant terms compared to the original one. For acceleration schemes in convex optimization, they modify the method of similar triangles (MST)~\citep{gasnikov2018universal} and prove an optimal complexity of $O(\epsilon^{-0.5})$.

\paragraph{Other explorations}
Ever since~\citet{Zhang2020Why} proposed $(L_0,L_1)$-smoothness condition, this generalized notion of smoothness has been flourishing in minimax optimization~\citep{pmlr-v235-xian24a}, bilevel optimization~\citep{hao2024bilevel,pmlr-v235-gong24d}, multi-objective optimization~\citep{zhang2024MOO}, sign-based optimization~\citep{crawshaw2022robustness}, distributionally robust optimization~\citep{jin2021nonconvexdro} and variational inequality~\citep{vankov2024adaptive,vankov2024generalized}. Numerous attempts have been made to refine existing algorithms under this weaker assumption, including variance reduction~\citep{reisizadeh2023variance}, clipping/normalized gradient~\citep{zhang2020improved,pmlr-v130-qian21a,DBLP:journals/chinaf/ZhaoXL21,pmlr-v238-hubler24a,yang2024independently}, error feedback~\citep{khirirat2024error} and trust region methods~\citep{Xie2024trustregion}. Notably,~\citet{pmlr-v178-faw22a,pmlr-v195-faw23a,pmlr-v195-wang23a,Li23adam,wang2024provable,zhang2024gs} explore the convergence of AdaGrad~\citep{JMLR:v12:duchi11a}, RMSprop~\citep{hinton2012neural}, and Adam~\citep{kingma15adam} under generalized smoothness. Beyond the optimization community, it has also garnered significant attention in federated learning~\citep{khirirat2024communication,demidovich2024methods} and meta-learning~\citep{chayti2024metalearning}.

\subsection{$\alpha$-symmetric Generalized Smoothness}
% An important generalization of $(L_0,L_1)$-smoothness is $\alpha$-symmetric $(L_0,L_1)$-smoothness proposed by~\citet{chen2023generalized}, which introduces symmetry into the original formulation and also allows the dependency on the gradient norm to be polynomial with degree of $\alpha$. For deterministic non-convex optimization,~\citet{chen2023generalized} establish the optimal complexity of $O(\epsilon^{-2})$ for a variant of normalized gradient descent~\citep{nesterov1984minimization,cortes2006finite}. They also show that the popular SPIDER algorithm~\citep{fang2018spider} achieves the optimal $O(\epsilon^{-3})$ complexity in the stochastic setting.
An important generalization of $(L_0,L_1)$-smoothness is $\alpha$-symmetric $(L_0,L_1)$-smoothness proposed by~\citet{chen2023generalized}, which introduces symmetry into the original formulation and also allows the dependency on the gradient norm to be polynomial with degree of $\alpha$. The formal definition is given as follows:
% \citet{chen2023generalized} propose $\alpha$-symmetric generalized smoothness condition of the form
\begin{equation}
    \norm{\nabla f(\x)-\nabla f(\y)}_2\le\brac{L_0+L_1\sup_{\theta\in[0,1]}\norm{\nabla f\brac{\theta\x+(1-\theta)\y}}_2}\norm{\x-\y}_2,\ \forall\x,\y\in\mathcal{E},\label{eq:alpha-symmetric}
\end{equation}
for some $L_0,L_1\in\R_+$. For deterministic non-convex optimization,~\citet{chen2023generalized} establish the optimal complexity of $O(\epsilon^{-2})$ for a variant of normalized gradient descent~\citep{nesterov1984minimization,cortes2006finite}. They also show that the popular SPIDER algorithm~\citep{fang2018spider} achieves the optimal $O(\epsilon^{-3})$ complexity in the stochastic setting. As pointed out by~\citet{chen2023generalized,vankov2024optimizing}, any twice-differentiable function satisfying~\eqref{eq:alpha-symmetric} is also $(L_0^{'},L_1^{'})$-smooth for different constant factors. As an extension,~\citet{NeurIPS:2024:Jiang} study sign-based finite-sum non-convex optimization and obtain an improved complexity over SignSVRG algorithm~\citep{chzhen2023signsvrg}.

\subsection{$\ell$-smoothness}
Recently,~\citet{Li2023GS} significantly generalize $(L_0,L_1)$-smooth to $\ell$-smooth, which assumes $\norm{\nabla^2 f(\x)}_2\le \ell(\norm{\nabla f(\x)}_2)$ for an arbitrary non-decreasing function $\ell$. The enhanced flexibility of $\ell$ accommodates a broader range of practical ML problems~\citep{devlin-etal-2019-bert,caron2021emerging,radford2021learning}, and can model certain real-world problems where $(L_0,L_1)$-smoothness fails~\citep{cooper2024theoretical}. Under this condition, they study gradient descent for strongly-convex, convex, and non-convex objectives, obtaining classic results of $O(\log(\epsilon^{-1}))$, $O(\epsilon^{-1})$, and $O(\epsilon^{-2})$, respectively. They also prove that Nesterov's accelerated gradient method~\citep{nesterov1983method} attains the optimal $O(\epsilon^{-0.5})$ complexity for convex objectives. Furthermore, they delve into stochastic non-convex optimization and show that stochastic gradient descent achieves the optimal complexity of $O(\epsilon^{-4})$ under finite variance condition, matching the lower bound in~\citet{arjevani2023lower}.
\setParDis

\citet{tyurin2024toward} study gradient descent for deterministic non-convex optimization and achieve optimal convergence rates (up to constant factors) without the need for a sub-quadratic \(\ell\) as required in~\citet{Li2023GS}. However, they impose an additional assumption of bounded gradients, which significantly simplifies the analysis.~\citet{NeurIPS'24:LocalSmooth} study online convex optimization problems by assuming each online function $f_t:\X\mapsto\R$ is $\ell_t$-smooth and that $\ell_t(\cdot)$ can be queried arbitrarily by the learner. Based on $\ell_t$-smoothness, they derive gradient-variation regret bounds for convex and strongly convex functions, albeit with the limitation of assuming a globally constant upper bound on the adversary's smoothness. 

\subsection{Other generalizations of classic smoothness}
It is known that a function $f$ is $L$-smooth if $Lh-f$ is convex~\citep{nesterov2018lectures} for $h(\cdot)=\norm{\cdot}^2/2$. Advancements have been made to relax $h$ into an arbitrary Legendre function, leading to the development of a notion called relative smoothness~\citep{Bauschke17descentlemma,lu18relativesmooth}, which substitutes the canonical quadratic upper bound with $f(\x)\le f(\y)+\inner{\nabla f(\y)}{\x-\y}+LB_h(\x,\y)$, where $B_h$ is the Bregman divergence associated with $h$. More recently,~\citet{mishkin2024directional} propose directional smoothness, a measure of local gradient variation that preserves the global smoothness along specific directions. To address the imbalanced Hessian spectrum distribution in practice~\citep{sagun2016eigenvalues,pan2022eigencurve},~\citet{jiang2024convergence,liu2024adagrad} propose anisotropic smoothness and obtain improved bounds for adaptive gradient methods.

\section{Generalized Smooth Function Class\label{sec:gs-function-class}}
In this section, we formally introduce a refined notion of generalized smoothness on the basis of~\citet{Li2023GS}, so as to accommodate non-Euclidean geometries. 

\subsection{Definitions and Properties}

Note that in the Euclidean case, characterizing smoothness either by gradient or Hessian is straightforward due to the nice property $\norm{\cdot}_2=\norm{\cdot}_{2,*}$. For the non-Euclidean case where $\norm{\cdot}\neq\norm{\cdot}_{*}$, the previous formulation \(\norm{\nabla^2 f(\x)}_2 \le \ell(\norm{\nabla f(\x)}_2)\) in~\citet{Li2023GS} fails to account for the inherent heterogeneity between the norm and its dual. To clarify, we first rewrite the definition in~\citet{Li2023GS} into \(\sup_{\h \in \mathcal{E}\backslash  \cbrac{\mathbf{0}}} \cbrac{\norm{\nabla^2 f(\x) \h}_2 / \norm{\h}_2}\le \ell(\norm{\nabla f(\x)}_2)\). Then, we underline that the mapping induced by the Hessian matrix (i.e., $\nabla^2 f(\x)\h$) and the gradient (i.e., $\nabla f(\x)$) lie in the dual space \(\mathcal{E}^*\)~\citep[Theorem~2.1.6]{nesterov2018lectures}, necessitating the use of the dual norm. To this end, we reformulate the definition into $\sup_{\h\in\mathcal{E}\backslash\cbrac{\mathbf{0}}}\cbrac{\norm{\nabla^2f(\x)\h}_*/\norm{\h}} \le \ell(\norm{\nabla f(\x)}_*)$, where the dual norm $\dnorm{\cdot}$ is imposed to measure the size of $\nabla^2f(\x)\h$ and $\nabla f(\x)$. The formal definition is given below.

\begin{defn}\label{def:ell-smooth}
    ($\ell*$-smoothness) A differentiable function $f:\X\mapsto\R$ belongs to the class $\mathcal{F}_{\ell}(\norm{\cdot})$ w.r.t.~a non-decreasing continuous link function $\ell:\R_+\mapsto\R_{++}$ if 
    \begin{equation}
        \forall \h\in\X, \norm{\nabla^2f(\x)\h}_*\le\ell(\norm{\nabla f(\x)}_*)\norm{\h}\label{eq:ell-smooth}
    \end{equation}
    holds almost everywhere for $\x\in\X$.
\end{defn}

\begin{remark}
    Through careful analysis, we discover that it suffices to take the supremum over the domain $\X$ rather than the whole space $\mathcal{E}$, indicating that~\cref{def:ell-smooth} enjoys a slightly weaker condition than~\citet{Li2023GS}.
    % Note that~\eqref{eq:ell-smooth} is equivalent to $\sup_{\h\in\X}\cbrac{\norm{\nabla^2f(\x)\h}_*/\norm{\h}} \le \ell(\norm{\nabla f(\x)}_*)$. Our definition enjoys a slightly weaker condition than~\citet{Li2023GS}, as the supremum only needs to be taken over $\h\in\X$ rather than $\h\in\mathcal{E}$. 
\end{remark}

\begin{remark}
    When \(\norm{\cdot} = \norm{\cdot}_2\),~\cref{def:ell-smooth} simplifies to the \(\ell\)-smoothness definition in~\citet{Li2023GS}, which recovers the $L$-smooth condition~\citep{nesterov2018lectures} and recently proposed $(L_0,L_1)$-smoothness~\citep{Zhang2020Why} with $\ell(\alpha)\equiv L$ and $\ell(\alpha)= L_0+L_1\alpha$, respectively. In fact, due to the equivalence of norms~\citep{boyd2004convex}, \(\ell\)-smoothness can be transformed into \(\ell*\)-smoothness with a rescaled \(\ell(\cdot)\). Whereas, the rescaling brings an additional factor dependent on the problem dimension, ultimately giving rise to \emph{dimension-dependent} convergence rates.
\end{remark}

In general, our formulation encompasses a broader class of functions and also plays a pivotal role in applying mirror-descent-type optimization algorithms. We emphasize that strict twice-differentiability is not required here, as~\cref{def:ell-smooth} permits~\eqref{eq:ell-smooth} to fail on a set of measure zero\footnote{One can also follow the similar arguments as in~\citet{Zhang2020Why} to relax the twice-differentiability condition by considering $\limsup_{\h\to\mathbf{0}}\dnorm{\nabla f(\x+\h)-\nabla f(\x)}/\norm{\h}\le \ell(\dnorm{\nabla f(\x)})$.}.~\cref{def:ell-smooth} can be seen as a global condition on the Hessian, but it lacks local characterization of the gradients, which proves to be essential for convergence analysis~\citep{Zhang2020Why}. Therefore, we introduce the following definition to capture the local Lipschitzness of the gradient, which is equivalent to~\cref{def:ell-smooth} under mild conditions, as illustrated in the sequel.

\begin{defn}\label{def:ell-r-smooth}
    ($(\ell,r)*$-smoothness) A differentiable function $f:\X\mapsto\R$ belongs to the class $\mathcal{F}_{\ell,r}(\norm{\cdot})$ w.r.t.~a non-decreasing continuous link function $\ell:\R_+\mapsto\R_{++}$ and a non-increasing continuous radius function $r:\R_+\mapsto\R_{++}$ if (i) $\forall\x\in\X,\B(\x,r(\norm{\nabla f(\x)}_*))\subseteq\X$; (ii)
    \begin{equation}
       \forall \x_1,\x_2\in\mathcal B(\x,r(\norm{\nabla f(\x)}_*)),\ \norm{\nabla f(\x_1)-\nabla f(\x_2)}_*\le\ell(\norm{\nabla f(\x)}_*)\norm{ \x_1-\x_2}. \label{eq:ell-r-smooth} 
    \end{equation}
\end{defn}

Similar to~\citet{Li2023GS}, we introduce the following standard assumption to bridge the gap between two function classes.

\begin{ass}
    The objective function $f$ is differentiable and closed within its open domain $\X$.\label{ass:closed-f}
\end{ass}

Note that when the optimization problem degenerates to the unconstrained setting, this assumption holds naturally for all continuous functions~\citep{boyd2004convex}. Now we present the following proposition, indicating that $\mathcal{F}_{\ell}(\norm{\cdot})$ and $\mathcal{F}_{\ell,r}(\norm{\cdot})$ are nearly equivalent.

\begin{prop}(Equivalence between two definitions of generalized smoothness)
\begin{enumerate}[(i)]
    \item $\mathcal{F}_{\ell,r}(\norm{\cdot})\subseteq\mathcal{F}_{\ell}(\norm{\cdot})$;
    \item under~\cref{ass:closed-f}, if $f\in\mathcal{F}_{\ell}(\norm{\cdot})$, then $f\in\mathcal{F}_{\tdel,\tdr}(\norm{\cdot})$ where $\tdel(\alpha)=\ell(\alpha+G)$ and $\tdr(\alpha)={G}/{\tdel(\alpha)}$ for any $G\in\R_{++}$.
\end{enumerate}
    % The function class satisfies $\mathcal{F}_{\ell,r}(\norm{\cdot})\subseteq\mathcal{F}_{\ell}(\norm{\cdot})$, and under~\cref{ass:closed-f}, $\mathcal{F}_{\ell}(\norm{\cdot})\subseteq$
    \label{prop:smooth-equivalence}
\end{prop}

For simplicity, we restrict our focus to \(\mathcal{F}_{\ell}(\norm{\cdot})\) for the algorithms discussed hereafter. Using~\cref{prop:smooth-equivalence}, we can transform~\cref{def:ell-smooth} into~\cref{def:ell-r-smooth}, enabling a quantitative depiction of the local smoothness property around a point \(\x \in \X\), as demonstrated in the following lemma.

\begin{lem}\label{lem:effective-L-smooth}
    For any $f\in\mathcal{F}_{\ell}(\norm{\cdot})$ satisfying $\dnorm{\nabla f(\x)}\le G\in\R_{+}$ and given $\x\in\X$, the following properties hold:
    \begin{enumerate}[(i)]
        \item $\B(\x,G/L)\subseteq\X$ where $L:=\ell(2G)$;
        \item $\forall\x_1,\x_2\in\B(\x,G/L),\ \norm{\nabla f(\x_1)-\nabla f(\x_2)}_*\le L\norm{ \x_1-\x_2}$;
        \item $\forall\x_1,\x_2\in\B(\x,G/L),\ f(\x_1)\le f(\x_2)+\inner{\nabla f(\x_2)}{\x_1-\x_2}+L\norm{\x_1-\x_2}^2/2$.
    \end{enumerate}
\end{lem}

\cref{lem:effective-L-smooth} provides (i) an optimistic estimate of the local Lipschitz parameter \(\ell(2\dnorm{\nabla f(\x)})\), and (ii) functional estimation bounds analogous to \(L\)-smoothness. This establishes a connection between $\mathcal{F}_{\ell}(\norm{\cdot})$ and the $L$-smooth function class, demonstrating that the former exhibits similar properties as the latter in a local region.
% making it possible to adapt techniques from the textbook analysis of \(L\)-smoothness and apply them to $\ell*$-smooth functions. 
As~\cref{lem:effective-L-smooth} requires $\dnorm{\nabla f(\x)}\le G$, the boundedness of gradients plays a critical role in this transformation which will be discussed further in~\cref{sec:main-idea}.

\subsection{Examples\label{sec:examples}}

\citet{Li2023GS} provide a series of concrete examples satisfying the \(\ell\)-smooth condition. Since our formulation generalizes theirs, these example functions also satisfy \(\ell*\)-smoothness. Nonetheless, we provide additional examples to illustrate the potential emergence of \emph{dimension-dependent} factors, highlighting the advantages of the algorithmic adaptivity brought by mirror descent. The comprehensive discussions can be found in~\cref{app:sec:example}.
\setParDis

Consider the function $h(\x)=\x^\top(\one_n-\e_1)(\one_n-\e_1)^\top\x/2,\x\in\Delta_n$, where $\Delta_n=\{\x\ge\mathbf{0},\one_n^\top\x=1|\x\in\R^n\}$ is the $(n-1)$-dimensional simplex. We have the following proposition.

\begin{prop}\label{prop:example-function}
    (i) $h\in\mathcal{F}_{\widehat{\ell}}(\norm{\cdot}_2)$ with $\widehat{\ell}(\cdot)\equiv n-1$; (ii) $h\in\mathcal{F}_{\tdel}(\norm{\cdot})$ with $\tdel(\cdot)\equiv 1$.
\end{prop}

\begin{remark}
    Under $\ell$-smoothness, the function $h$ inevitably has a smoothness parameter that scales linearly to the dimension $n$. Whereas, under $\ell*$-smoothness, the dependence of the dimension no longer exists. 
\end{remark}

\begin{figure}
% \vspace{-0.30cm}
\captionsetup{justification=centering}
    \centering
    \includegraphics[width=\linewidth]{fig/ratio_6_201_3.pdf}
    % \vspace{-0.85cm}
    \caption{The slope ratio $\widetilde{L}_1/\widehat{L}_1$ w.r.t.~dimension $n$ and the fitted curve.\label{fig:slope}}
    % \vspace{-0.45cm}
\end{figure}
Previous theoretical analyses primarily focus on constant link functions \(\widehat{\ell}\) and \(\tdel\). In addition, we present empirical evidence regarding affine link functions. Consider the following example:
% The previous theoretical justifications involve constant link functions $\widehat{\ell}$ and $\tdel$. Besides this, we provide empirical evidence for affine link functions. Consider the following example function:
\begin{equation}
    h(\x)=\frac{\brac{ \sum_{i=1}^n x_i^2 }^2}{4}  + \prod_{i=1}^n x_i^2 + \sum_{i=1}^n e^{5x_i},
\end{equation}
where $\x=(x_1,\cdots,x_n)\in\Delta_n$. Numerical experiments suggest that $\norm{\nabla^2 h(\x)}_2\le\widehat{L}_0+\widehat{L}_1\norm{\nabla h(\x)}$ ($h\in\mathcal{F}_{\widehat{\ell}}(\norm{\cdot}_2)$) and $\sup_{\u\in\Delta_n}\norm{\nabla^2 h(\x)\u}_{\infty}\le\widetilde{L}_0+\widetilde{L}_1\norm{\nabla h(\x)}_{\infty}$ ($h\in\mathcal{F}_{\tdel}(\norm{\cdot})$), that is, for the Euclidean setup and non-Euclidean setup, the smoothness of $h$ can both be captured by affine functions. Specifically, we estimate the values of $\widehat{L}_0,\widehat{L}_1,\widetilde{L}_0,\widetilde{L}_1$ under different values of $n$. Then, we plot the slope ratio $\widetilde{L}_1/\widehat{L}_1$ w.r.t.~the dimension $n$, and the results are shown in~\cref{fig:slope}.  
The empirical evidence indicates that smoothness depicted by~\cref{def:ell-smooth} can be much tighter than $\ell$-smoothness, with a constant factor approximately $1.87*n^{-0.43}$. As shown by~\citet{Li2023GS} and~\cref{thm:mirror-descent} in this paper, the convergence rate is proportional to the link function $\ell$, suggesting that mirror descent eventually improves the convergence rate in terms of \emph{dimension dependent} factors (see~\cref{app:sec:constant-factor}). 

\section{Mirror Descent Meets Generalized Smoothness\label{sec:algorithms}}

This section formally presents the convergence results for a series of first-order optimization algorithms based on mirror descent, whose analyses can be found in~\cref{sec:analysis}.

\subsection{Preliminaries}

First, we state the general setup of mirror descent algorithms~\citep{beck2003mirror}.

\begin{defn}
    We call a continuous function $\psi:\X\mapsto\R_+$ a distance-generating function with modulus $\alpha$ w.r.t.~$\norm{\cdot}$, if (i) the set $\X^o=\{\x\in \X|\partial\psi(\x)\neq \emptyset\}$ is convex; (ii) $\psi$ is continuously differentiable and $\alpha$-strongly convex w.r.t.~$\norm{\cdot}$, i.e.,~$\inner{\nabla\psi(\x_1)-\nabla\psi(\x_2)}{\x_1-\x_2}\ge \alpha\norm{\x_1-\x_2}^2,\ \forall \x_1,\x_2\allowbreak\in \X^o$.
\end{defn}

\begin{defn}\label{def:Bregman}
    Define the Bregman function $B:\X\times \X^o\mapsto\R_+$ associated with the distance-generating function $\psi$ as
    \begin{equation}
        B(\x,\x^o)=\psi(\x)-\psi(\x^o)-\inner{\nabla\psi(\x^o)}{\x-\x^o},\quad\forall(\x,\x^o)\in\X\times \X^o.
    \end{equation}
\end{defn}

We equip domain $\X$ with a distance-generating function $\psi(\cdot)$, which is, WLOG, $1$-strongly convex w.r.t.~a certain norm $\norm{\cdot}$ endowed on $\mathcal{E}$. We also define the corresponding Bregman divergence according to~\cref{def:Bregman}. Then, we introduce the following assumptions on the domain and the objective function. The former is standard for mirror descent~\citep{nemirovski2009robust}, while the latter controls the growth rate of $\ell(\cdot)$, as also required in~\citet{Li2023GS}.

\begin{ass}\label{ass:bound-domain}
    The domain $\X$ is bounded in the sense that
    \begin{equation}
        \max_{\x\in\X}\psi(\x)-\min_{\x\in\X}\psi(\x)\le D^2,\quad D\in\R_{++}.\label{eq:ass:bound-domain}
    \end{equation}
\end{ass}

\begin{ass}\label{ass:sub-quadratic-ell}
    The objective function satisfies $f\in\mathcal{F}_{\ell}(\norm{\cdot})$, where $\ell$ is sub-quadratic, i.e., $\lim_{\alpha\to \infty}\allowbreak\alpha^2/\ell(\alpha)=\infty$.
    % The link function $\ell$ is sub-quadratic: $\lim_{\alpha\to \infty}\alpha^2/\ell(\alpha)=\infty$. The objective function satisfies $f\in\mathcal{F}_{\ell}(\norm{\cdot})$.
\end{ass}

For convenience, we also define the following prox-mapping function~\citep{nemirovski2004prox}:
\begin{equation}
    \P_{\y}(\g)=\argmin_{\x\in\X}\cbrac{\inner{\g}{\x}+B(\x,\y)},\quad\forall\y\in\X,\g\in\mathcal{E}^*,\label{eq:prox-mapping}
\end{equation}
which serves as the basic component of the optimization algorithms being considered.

\subsection{Main Idea and Technical Challenges\label{sec:main-idea}}

As discussed previously, bounding the gradients is a prerequisite for~\cref{lem:effective-L-smooth}, which enables us to reduce the $\ell*$-smoothness analysis to the conventional analysis of $L$-smooth functions. In the Euclidean case,~\citet{Li2023GS} bound the gradients along the optimization trajectory by tracking the gradients \emph{directly}. More specifically, they manage to show that for gradient descent, the $\ell_2$-norm of the gradients decrease monotonically provided that the learning rates are sufficiently small. However, this vital discovery builds upon the geometry of the Euclidean setting, i.e., $\inner{\x}{\x}=\norm{\x}_2^2$, which is invalid in the non-Euclidean setting. 

\paragraph{Technical challenges} 
The underlying problem is that for mirror descent, the dual norms of the gradients do not necessarily form a monotonic decreasing sequence. The difficulty stems from the nature of mirror descent, where the descent step is performed in the dual space, making the explicit analysis of the dual norms very difficult and rarely addressed. Even under \(L\)-smoothness, tracking the gradients directly for mirror descent remains challenging, as discussed in~\citet{zhang2018convergence,lan2020first,huang2021efficient} and references therein, unless the problem exhibits special structures~\citep{zhou17smd} or extra assumptions are exerted~\citep{lei2017analysis}.

\paragraph{Main idea} To tackle this problem, we introduce the following \emph{generalized version} of the reversed Polyak-\L{}ojasiewicz inequality~\citep{polyak1963gradient,lojasiewicz1963propriete}, aka the self-bounding property of smooth functions~\citep{srebro2010smoothness,COLT:2017:Zhang,COLT:2019:Zhang}.

\begin{lem}
    For any $f\in\mathcal{F}_{\ell}(\norm{\cdot})$, we have $\dnorm{\nabla f(\x)}^2\le 2\ell(2\dnorm{\nabla f(\x)})(f(\x)-f^*)$ for any $\x\in\X$.\label{lem:pl-inequality}
\end{lem}

\cref{lem:pl-inequality} measures the growth of gradient's dual norm by the suboptimality gap $f(\x)-f^*$. By~\cref{ass:sub-quadratic-ell}, if we further assume $\ell(\alpha)=(\alpha/2)^\beta,\beta\in[0,2)$, then $\dnorm{\nabla f(\x)}\le 2[f(\x)-f^*]^{1/(2-\beta)}$ is immediately implied. Hence,~\cref{lem:pl-inequality} suggests that $\dnorm{\nabla f(\x)}$ is effectively under control if $\ell(\cdot)$ does not grow too rapidly. In this way, the gradient can be bounded by the suboptimality gap of the objective function, which is generally much easier in the context of mirror descent. We rigorously show that the suboptimality gap at an arbitrary iterate is bounded by that of the starting point (see~\cref{lem:md-trajectory-grad-bound}), which further suggests the boundness of the gradients. Once the gradients have an upper bound, we can construct an effective smoothness parameter $L$ that serves as an optimistic estimation of the local curvatures along the trajectory. Formally, we define
\begin{equation}
    G:=\sup\cbrac{\alpha\in\R_{+}|\alpha^2\le2\ell (2\alpha)(f(\x_0)-f^*)},\text{ and } L:=\ell(2G),\label{eq:G&L}
\end{equation}
where $\x_0=\argmin_{\x\in\X}\psi(\x)$ is selected as the starting point for all optimization algorithms considered in this paper, whose trajectory provably satisfies $\dnorm{\nabla f(\x_t)}\le G$ (will be stated rigorously in the theorems to come) and consequently, $\ell(2\dnorm{\nabla f(\x_t)})\le L$. We also emphasize that both $G$ and $L$ are absolute constants, i.e., $G,L<\infty$ (\cref{lem:grad-norm-suboptimality}), which is essential to the validity of our theoretical derivations. 
% One may interpret them as our initial \emph{guesses} of the true parameters. In our analysis, we can show that these \emph{guesses} are adequately optimistic, i.e., they are upper bounds of the underlying local curvature.

\subsection{Mirror Descent and Accelerated Mirror Descent\label{sec:md-amd}}
In this section, we provide theoretical guarantees for mirror descent and its accelerated variant under $\ell*$-smoothness. We start with the basic mirror descent algorithm~\citep{nemirovskij1983problem,beck2003mirror} given by
\begin{equation}
    \x_{t+1}=\P_{\x_t}(\eta_t\nabla f(\x_t)),\label{eq:mirror-descent}
\end{equation}
where $\eta_t$ represents the learning rates, and the prox-mapping function $\P_\cdot(\cdot)$ is defined in~\eqref{eq:prox-mapping}. Our convergence analysis relies on the interesting fact that the suboptimality gap along the trajectory is a non-increasing sequence: $f(\x_t)-f^*\le f(\x_{t-1})-f^*\le\cdots\le f(\x_0)-f^*$, provided that learning rates are chosen appropriately. Consequently, the gradients along the trajectory are bounded by~\cref{lem:pl-inequality}, and so are the local Lipschitz constants. The formal statements are illustrated in the following theorem.

\begin{thm}\label{thm:mirror-descent}
    Under~\cref{ass:closed-f,ass:bound-domain,ass:sub-quadratic-ell}, if $0<\eta_t=\eta\le 1/L$, then the gradients along the trajectory generated by~\eqref{eq:mirror-descent} satisfy $\dnorm{\nabla f(\x_t)}\le G,\forall t\in\N$, and the average-iterate convergence rate is given by
    \begin{equation}
        f(\xb_T)-f^*\le \frac{D^2}{\eta T},
    \end{equation}
    where $\xb_T=\sum_{t=1}^{T}\x_t/T$. Moreover, the last-iterate convergence rate is given by
    \begin{equation}
        f(\x_T)-f^*\le \frac{D^2}{\eta T}.
    \end{equation}
\end{thm}

Our convergence result recovers the classical $O(1/T)$ rate for deterministic convex optimization \citep{bubeck2015convex,lan2020first}, for both the average-iterate and the last-iterate.
\setParDis

Next, we investigate the accelerated version of mirror descent. The acceleration scheme for smooth convex optimization originates from~\citet{nesterov1983method} and has numerous variants~\citep{allen2017linear,d2021acceleration}. In this paper, we study one of its simplest versions~\citep{lan2020first}:
\begin{equation}
    \begin{aligned}
        &\y_t=(1-\alpha_t)\x_{t-1}+\alpha_t\z_{t-1},\\
        &\z_t=\P_{\z_{t-1}}(\eta_t\nabla f(\y_t)),\\
        &\x_t=(1-\alpha_t)\x_{t-1}+\alpha_t\z_t.
    \end{aligned}\label{eq:accelerated-mirror-descent}
\end{equation}
We present the following theoretical guarantee, which achieves the optimal $O(1/T^2)$ convergence rate of first-order methods~\citep{nesterov2018lectures}. 

\begin{thm}
    Under~\cref{ass:closed-f,ass:bound-domain,ass:sub-quadratic-ell}, define\footnote{Due to technical reasons, $L=\ell(4G)$ needs to be slightly larger than the one used in mirror descent. This adjustment affects only accelerated mirror descent; other algorithms still use \(L = \ell(2G)\).}
    \begin{equation}
    \begin{aligned}
        L:=\ell(4G),\tau:=\left\lceil\frac{4\sqrt{2}DL}{G}\right\rceil,\eta:=\min\cbrac{1,\frac{3(\tau-1)}{2(\tau-3)(\tau-2)}},\eta_t:=\frac{t\eta}{2L},\alpha_t:=\frac{2}{t+1}.
    \end{aligned}        
    \end{equation}
    Then the convergence rate of~\eqref{eq:accelerated-mirror-descent} is given by 
    \begin{equation}
        f(\x_T)-f^*\le \frac{D^2L}{\eta T(T+1)},\label{eq:thm:amd}
    \end{equation}
    and the gradients along the trajectory satisfy $\max\{\dnorm{\nabla f(\y_t)},2\dnorm{\nabla f(\x_t)}\}\le 2G$ for all $t\in\N$.\label{thm:accelerated-mirror-descent}    
\end{thm}

\begin{remark}
    The general idea is similar to mirror descent, that is, bounding the gradients along the trajectory via controlling the suboptimality gaps. However, as~\eqref{eq:accelerated-mirror-descent} maintains three sequences, the analysis becomes significantly more challenging compared to the single sequence $\cbrac{\x_t}_{t\in\N}$ in~\eqref{eq:mirror-descent}. While \(\dnorm{\nabla f(\x_t)} \lesssim G\) follows directly from the convergence in~\eqref{eq:thm:amd}, it cannot be directly extended to the sequence \(\{\y_t\}_{t\in\N}\), whose gradients also need to be bounded. To address this issue, we focus on the key quantity \(e_t := \norm{\y_t - \x_{t-1}}\), which measures the distance between the two sequences. If \(e_t \lesssim G/L\), we can readily deduce \(\dnorm{\nabla f(\y_t)} \lesssim G\) by leveraging the local smoothness property in~\cref{def:ell-r-smooth}. To achieve this goal, we develop a ``time partition" technique illustrated in~\cref{lem:amd-induction}. Specifically, when \(t\) is below the threshold $\tau$, we bound $e_t$ by a contraction mapping, which effectively limits its growth for small $t$. Conversely, when $t$ exceeds $\tau$, $e_t$ provably decays hyperbolically. Combining the two cases, we can derive that \(e_t \lesssim G/L\), which suggests the boundness of the gradients.
\end{remark}

\paragraph{Comparison with~\citet[Theorem~4.4]{Li2023GS}}
For $f\in\mathcal{F}_{\ell}(\norm{\cdot}_2)$,~\citet{Li2023GS} study a variant of the Nesterov's accelerated gradient algorithm (NAG) and also recover the optimal bound. They introduce an auxiliary sequence into the original NAG, which aims to aggressively stabilize the optimization trajectory. The stabilization mainly refers to $\norm{\y_t-\x_t}$ in their language, sharing the similar spirit as $e_t$ in our paper. To this end, they have to use a smaller step size ($\eta\simeq1/L^2$) and impose a more complex analysis than the original NAG. Unlike their framework, we concentrate on a more stable acceleration scheme in~\eqref{eq:accelerated-mirror-descent}, without manually tuning down the base learning rate $\eta$ or bringing in any additional algorithmic components. Moreover, our proof is much simpler and more intuitive, as reflected in our constructive ways of analyzing the stability term $e_t$ by partitioning the timeline. It is also worth mentioning that we do not need the fine-grained characterization on the modulus of $\ell$, while~\citet{Li2023GS} further assume $\ell(u)=O(u^\alpha),\alpha\in(0,2]$.

\subsection{Optimistic Mirror Descent and Mirror Prox}

In this subsection, we consider another genre of the classic optimization algorithm: optimistic mirror descent and mirror prox. Since these algorithms are often treated as a single framework in the online learning community~\citep{JMLR:2024:Chen,NeurIPS:2024:Wang}, we adopt the terminology in~\citet{mokhtari2020convergence,azizian21omdsvi} to avoid potential confusion. 
\setParDis

Unlike mirror descent, optimistic mirror descent and mirror prox perform two prox-mappings per iteration and incorporate current observed information to estimate the functional curvature. Firstly, we consider the following optimistic mirror descent algorithm~\citep{popov1980modification,chiang2012online,rakhlin2013online,rakhlin2013optimization}, which requires $1$ gradient query per iteration.
\begin{equation}
    \begin{aligned}
    &\y_t=\P_{\x_{t-1}}(\eta\nabla f(\y_{t-1})),\\
    &\x_t=\P_{\x_{t-1}}(\eta\nabla f(\y_t)),
    \end{aligned}\label{eq:optimistic-mirror-descent}
\end{equation}
where we initialize $\y_0=\x_0=\argmin_{\x\in\X}\psi(\x)$. The following theorem establishes the classic $O(1/T)$ convergence rate for this algorithm.

\begin{thm}
    Under~\cref{ass:closed-f,ass:bound-domain,ass:sub-quadratic-ell}, if $0<\eta\le 1/(3L)$, then the gradients along the trajectory generated by~\eqref{eq:optimistic-mirror-descent} satisfy $\max\{\dnorm{\nabla f(\y_t)},\dnorm{\nabla f(\x_t)}\}\le G,\forall t\in\N$, and the convergence rate is given by
    \begin{equation}
        f(\overline{\y}_T)-f^*\le \frac{D^2}{\eta T},
    \end{equation}
    where $\overline{\y}_T=\sum_{t=1}^{T}\y_t/T$.\label{thm:optimistic-mirror-descent}
\end{thm}

Next, we investigate the mirror prox algorithm~\citep{korpelevich1976extragradient,nemirovski2004prox,nesterov2007dual}, which is defined via the following updates:
\begin{equation}
    \begin{aligned}
    &\y_t=\P_{\x_{t-1}}(\eta\nabla f(\x_{t-1})),\\
    &\x_t=\P_{\x_{t-1}}(\eta\nabla f(\y_t)).
    \end{aligned}\label{eq:mirror-prox}
\end{equation}
The only difference between~\eqref{eq:optimistic-mirror-descent} and~\eqref{eq:mirror-prox} is that the latter does not reuse gradient information, which results in $2$ gradient queries per iteration. For a comprehensive overview and comparison between these two algorithms, one may refer to~\citet{azizian21omdsvi,cai2022lastiterate}. The same $O(1/T)$ convergence rate is also obtained, as shown in the theorem below.

\begin{thm}
    Under~\cref{ass:closed-f,ass:bound-domain,ass:sub-quadratic-ell}, if $0<\eta\le 1/(2L)$, then the gradients along the trajectory generated by~\eqref{eq:mirror-prox} satisfy $\max\{\dnorm{\nabla f(\y_t)},\dnorm{\nabla f(\x_t)}\}\le G,\forall t\in\N$, and the convergence rate is given by
    \begin{equation}
        f(\overline{\y}_T)-f^*\le \frac{D^2}{\eta T},
    \end{equation}
    where $\overline{\y}_T=\sum_{t=1}^{T}\y_t/T$.\label{thm:mirror-prox}
\end{thm}

The high-level idea behind~\cref{thm:optimistic-mirror-descent,thm:mirror-prox} aligns with that in~\cref{sec:md-amd}, i.e., bounding the dual norms of the gradients by analyzing suboptimality gaps. However, the implication from the last-iterate convergence in~\cref{thm:mirror-descent,thm:accelerated-mirror-descent} to the bounded suboptimality gap does not apply for optimistic mirror descent and mirror prox, as elaborated below.

\paragraph{Hardness results on the last-iterate}
Different from the algorithms in~\cref{sec:md-amd} which enjoys a non-asymptotic last-iterate convergence under $L$-smoothness, the last-iterate of~\eqref{eq:optimistic-mirror-descent} as well as~\eqref{eq:mirror-prox} had been only known to converge asymptotically, as shown by~\citet{popov1980modification,hsieh2019convergence} for~\eqref{eq:optimistic-mirror-descent};~\citet{korpelevich1976extragradient,facchinei2007finite} for~\eqref{eq:mirror-prox}. Even in the Euclidean setting, the finite-time last-iterate convergence is very challenging~\citep{golowich2020last,wei21linear,Gorbunov2022lastiterate} and requires complicated analysis as well as intricate techniques~\citep{cai2022arxiv}, e.g., computer-aided proofs based on sum-of-squares programming~\citep{nesterov2000squared,parrilo2003semidefinite}.

\paragraph{Circumvent the last-iterate analysis}
Recall the discussions in~\cref{sec:main-idea} that it suffices to ensure the boundness of the suboptimality gap, which can be implied by the last-iterate convergence. Since the latter one is a stronger statement, we can \emph{bypass} the last-iterate convergence analysis, and control the suboptimality gap \emph{directly} through careful analysis. Therefore, the cumbersome procedures in~\citet{cai2022arxiv} can be avoided. Below, we briefly outline our approach.
\setParDis

\paragraph{Solution} 
For the mirror prox method, we establish the descent property for both sequences (Lemma \ref{lem:mp-trajectory-grad-bound}), i.e., $ \max\cbrac{f(\x_t),f(\y_t)}\le f(\x_{t-1})$, which guarantees the bounded suboptimality gap. For the optimistic mirror descent method, we track the stability term $\norm{\y_t-\y_{t-1}}$ and show that it grows no faster than the geometric series (\cref{lem:omd-trajectory-grad-bound}). The upper bound on stability terms suggests that the optimization trajectory is effectively stabilized, ultimately yielding $ \max\cbrac{f(\x_t),f(\y_t)}\le f(\x_0)$. Consequently, we bound both the suboptimality gap and the gradient.

\section{Stochastic Convex Optimization\label{sec:smd}}

In this section, we go one step further to study stochastic convex optimization (SCO) under $\ell*$-smoothness.~\cref{sec:smd-setup} introduces the setup of SCO with a new noise assumption, while~\cref{sec:smd-theorem} gives the main result, providing a high-probability anytime convergence guarantee.

\subsection{Setup\label{sec:smd-setup}}

Consider the following stochastic mirror descent (SMD) algorithm~\citep{nemirovski2009robust}:
\begin{equation}
    \x_{t+1}=\P_{\x_t}(\eta_{t+1}\g_t),\label{eq:stochastic-mirror-descent}
\end{equation}
where $\g_t$ is an estimate of the true gradient $\nabla f(\x_t)$. Let $\eps_t:=\g_t-\nabla f(\x_t)$ denote the noise. We make the following assumption on $\eps_t$, which is helpful for the high probability convergence.

\begin{ass}\label{ass:sigma-noise}
    For all $t\in\N$, the stochastic gradients are unbiased:
    \begin{equation}
        \E_{t-1}[\eps_t]=0,
    \end{equation}
    where the expectation $\E_{t-1}$ is conditioned on the past stochasticity $\cbrac{\eps_s}_{s=0}^{t-1}$. The noise satisfies the generalized bounded condition w.r.t.~a non-decreasing polynomial function $\sigma: \R_+\mapsto\R_{++}$: 
    \begin{equation}
        \dnorm{\eps_t}\le\sigma(\dnorm{\nabla f(\x_t)}),\quad\text{a.s.}
    \end{equation}  
    Moreover, the degree of the polynomial $\sigma$ is finite: $\operatorname{deg}(\sigma)<+\infty$.
\end{ass}

This generalized bounded noise\footnote{The noise is bounded when the gradient is bounded but can become unbounded if the gradient explodes.} assumption is inspired by the affine variance condition initially proposed by~\citet{Bottou18optimization}, where the noise level depends on the norm of the true gradient and is captured via an affine function.~\cref{ass:sigma-noise} encompasses the uniformly bounded noise~\citep{Zhang2020Why} and affine noise~\citep{liu2023gs,hong2024on} condition with $\sigma(\alpha)\equiv\sigma$ and $\sigma(\alpha)\equiv\sigma_0+\sigma_1\alpha$, respectively. Moreover, we underline that~\cref{ass:sigma-noise} is never stronger than the commonly assumed finite moment or sub-Gaussian noise condition, with thorough discussions deferred to~\cref{app:sec:noise}.

% Our formulation further relaxes the link function $\sigma$ from an affine function to an \emph{arbitrary} polynomial. We believe such generalization is meaningful and can model a series of stochastic environments where the noise is potentially unbounded~\citep{hwang1986multiplicative,Loh11high,Diakonikolas2023obliviousNoise} or has heavy tails~\citep{gurbuzbalaban2021heavy,cutkosky2021high}. Below, we briefly compare~\cref{ass:sigma-noise} to the common ones adopted in the existing literature.

\subsection{Theoretical Guarantees\label{sec:smd-theorem}}

Extension from deterministic optimization to SCO is highly non-trivial, for the sake of the following two quantities that need to be controlled.  

\paragraph{Suboptimality gap}
% Due to the presence of stochasticity, the error of each iteration aggregates and propagates, making the descent property invalid, that is, the sequence of suboptimality gaps $\cbrac{f(\x_t)-f^*}_{t\in\N}$ is no longer monotonic. In deterministic optimization, this sequence can be directly linked to convergence analysis. Whereas, the classic analysis of stochastic gradient descent (SGD)~\citep{robbins1951stochastic} only targets the average-iterate. A fruitful line of work~\citep{pmlr-v28-shamir13,pmlr-v99-harvey19a,orabona2020last,jain2021making} investigates the last-iterate convergence of SGD, with a caveat of either only valid in Euclidean geometry, or limited to compact domains, or requires uniformly bounded noise.~Although~\citet{liu2024revisiting} circumvent these drawbacks, they still assume a sub-Gaussian noise, making their analysis non-transferrable to generalized smoothness. Our solution is motivated by~\citet{orabona2020last}, who elegantly convert the average-iterate of SGD to the in-expectation last-iterate convergence for non-smooth objectives under Euclidean domains. By a subtle analysis of the martingales and a closer examination of their technique, we develop a high-probability average-iterate-to-last-iterate conversion framework for $\ell*$-smooth objectives in non-Euclidean settings.

% Stochasticity introduces error accumulation and propagation, rendering the descent property invalid. Consequently, the sequence of suboptimality gaps, \(\cbrac{f(\x_t) - f^*}_{t \in \N}\), is no longer monotonic. 
Due to the inherent stochasticity, the sequence \(\cbrac{f(\x_t) - f^*}_{t \in \N}\) may become unbounded. Recall from deterministic optimization, we directly link suboptimality gaps to the last-iterate convergence. However, stochastic gradient descent (SGD)~\citep{robbins1951stochastic} primarily focuses on the average-iterate, and therefore, such link does not hold. A fruitful line of work~\citep{pmlr-v28-shamir13,pmlr-v99-harvey19a,orabona2020last,jain2021making} explore the last-iterate convergence of SGD but often remain confined to Euclidean settings, compact domains, or uniformly bounded noise. Although~\citet{liu2024revisiting} overcome some of these limitations, their reliance on sub-Gaussian noise makes the analysis incompatible with generalized smoothness.
Our solution is motivated by~\citet{orabona2020last}, who elegantly converts average-iterate convergence of SGD to last-iterate in-expectation convergence for non-smooth objectives within Euclidean domains. Through a careful analysis of martingales and a refinement of their techniques, we extend the framework to $\ell*$-smooth objectives in non-Euclidean settings. Leveraging this conversion framework, we prove that suboptimality gaps are bounded for all iterations simultaneously with high probability, indicating that the gradients are also bounded for all $t\in\N$. Such ``anytime" characteristic is vital for our analysis, since~\cref{lem:effective-L-smooth} needs to be applied to every round of SMD.

\paragraph{Dual norm of the stochastic gradient} 

In the context of SCO, we still need to exploit the local smoothness property (\cref{lem:effective-L-smooth}), which requires the distance between consecutive iterates to be bounded. In the deterministic case, we have \(\norm{\x_{t+1} - \x_t} \le \dnorm{\nabla f(\x_t)}\), and by~\cref{lem:pl-inequality}, the boundedness of $\dnorm{\nabla f(\x_t)}$ is inferred from finite suboptimality gaps. In contrast, for the stochastic case, it holds that \(\norm{\x_{t+1} - \x_t} \le \dnorm{\g_t}\), where the challenge lies in bounding $\dnorm{\g_t}$. Clearly, the self-bounding property in~\cref{lem:pl-inequality} fails to bridge the stochastic gradient and the suboptimality gap.
To address this, we leverage the noise model in~\cref{ass:sigma-noise} to connect the noise level with the dual norm of the true gradient. As the suboptimality gap is bounded with high probability, the boundedness of the true gradient follows directly, implying that the noise is almost surely bounded via \(\sigma(\cdot)\). Finally, we apply a union bound over these events to establish bounds on the stochastic gradient.

\setParDis

Based on the above reasoning, our analysis framework significantly differs from that of~\citet[Section~5.2]{Li2023GS}, who utilize a stopping-time technique~\citep{williams1991probability} to show that the two key quantities are bounded before the stopping time \(T\) with high probability. However, their method has two limitations: (i) it requires prior knowledge of the total number of iterations \(T\); (ii) it lacks a convergence guarantee for the outputs generated before the final round. Moreover, their approach is ill-suited for SMD, as it heavily relies on the Euclidean inner product such that \(\inner{\x}{\x} = \norm{\x}_2^2\). Our main result is presented in~\cref{thm:stochastic-mirror-descent}, whose proof is outlined in~\cref{sec:proof-of-smd}. 

\begin{thm}\label{thm:stochastic-mirror-descent}
    Under~\cref{ass:closed-f,ass:bound-domain,ass:sub-quadratic-ell,ass:sigma-noise}, set 
    \begin{equation}
        % \eta_t=\min\cbrac{\frac{1}{2\ell(2\dnorm{\nabla f(\x_{t-1})})},\frac{\dnorm{\nabla f(\x_{t-1})}}{2\ell(2\dnorm{\nabla f(\x_{t-1})})\sigma(\dnorm{\nabla f(\x_{t-1})})},\frac{D}{\sigma(\dnorm{\nabla f(\x_{t-1})})\sqrt{t}}}.
        0<\eta_t\le\min\cbrac{\frac{\min\cbrac{1,\dnorm{\nabla f(\x_{t-1})}/\sigma(\dnorm{\nabla f(\x_{t-1})})}}{2\ell(2\dnorm{\nabla f(\x_{t-1})})},\frac{D}{\sigma(\dnorm{\nabla f(\x_{t-1})})\sqrt{t}}}.
        % \eta_t=\min\cbrac{\frac{\min\cbrac{1,\frac{\dnorm{\nabla f(\x_{t-1})}}{\sigma(\dnorm{\nabla f(\x_{t-1})})}}}{2\ell(2\dnorm{\nabla f(\x_{t-1})})},\frac{D}{\sigma(\dnorm{\nabla f(\x_{t-1})})\sqrt{t}}}.
    \end{equation}
    % $\eta_t=\min\cbrac{\frac{1}{2\ell(2\dnorm{\nabla f(\x_{t-1})})},\frac{\dnorm{\nabla f(\x_{t-1})}}{2\ell(2\dnorm{\nabla f(\x_{t-1})})\sigma(\dnorm{\nabla f(\x_{t-1})})},\frac{D}{\sigma(\dnorm{\nabla f(\x_{t-1})})\sqrt{t}}}$. 
    With high probability, we have the following anytime convergence rate:
    \begin{equation}
        f(\xb_t)-f^*=\O\brac{\frac{1}{\sqrt{t}}},\quad\forall t\in\N_+,
    \end{equation}
    where $\xb_t=\sum_{s=1}^t\eta_s\x_s/\sum_{s=1}^t\eta_s$. Moreover, the gradients along the trajectory satisfy $\dnorm{\nabla f(\x_t)}\allowbreak=\O(1),\forall t\in\N$.
\end{thm}

\begin{remark}
    \cref{thm:stochastic-mirror-descent} achieves the classic \(\O(1/\sqrt{t})\) anytime convergence rate~\citep{pmlr-v28-shamir13,ICML:2024:Zhang} in the classic SCO. Note that existing results rely on a deterministic learning rate schedule proportional to \(1/\sqrt{t}\). However, this strategy is too coarse to capture the local features of the objective under $\ell*$-smoothness. Consequently, we incorporate local curvature information into the learning rates, akin to the approach in~\citet{NeurIPS'24:LocalSmooth}, who consider online convex optimization under \(\ell\)-smoothness.
\end{remark}

\section{Analysis\label{sec:analysis}}
We present the proofs of the main theorems in~\cref{sec:algorithms,sec:smd}, where the lemmas involved can be found in~\cref{app:sec:md,app:sec:amd,app:sec:omd,app:sec:mp}.

\subsection{Mirror Descent}
\begin{proof}{\textbf{of~\cref{thm:mirror-descent}.}}
    According to the descent property in~\eqref{eq:descent-property}, we have $f(\x_{t+1})-f^*\le f(\x_t)-f^*\le \cdots\le f(\x_0)-f^*$. By~\cref{lem:md-trajectory-grad-bound}, we deduce that for $\dnorm{\nabla f(\x_t)}\le G<\infty$ holds for all $t\in\N$, where $G:=\sup\cbrac{\alpha\in\R_{+}|\alpha^2\le2(f(\x_0)-f^*)\ell (2\alpha)}$. After establishing such an upper bound for the dual norm of the gradients along the trajectory, we can cope with generalized smooth functions in a similar way as the global smooth functions.
    Similar to~\eqref{eq:md-iter}, we consider the update of mirror descent in~\eqref{eq:mirror-descent}:
    \begin{equation}
        \forall\x\in\X,\ \inner{\eta_t\nabla f(\x_t)}{\x_{t+1}-\x}\le B(\x,\x_t)-B(\x,\x_{t+1})-B(\x_{t+1},\x_t).\label{eq:md-basic-iter}
    \end{equation}
    Note that the local Lipschitz constant $\ell(2\dnorm{\nabla f(\x_t)})$ is bounded by $L=\ell(2G)$. By~\cref{lem:md-stability} and $\eta_t=\eta\le \frac{1}{L}$, we have
    \begin{equation}
        \norm{\x_{t+1}-\x_t}\le \eta\dnorm{\nabla f(\x_t)}\le \frac{G}{L}.
    \end{equation}
    Therefore, the distance between $\x_{t+1}$ and $\x_t$ is small enough to satisfy the conditions in~\cref{lem:effective-L-smooth}. Using the effective smoothness characterization, we obtain
    \begin{equation}
        f(\x_{t+1})\le f(\x_t)+\inner{\nabla f(\x_t)}{\x_{t+1}-\x_t}+\frac{L}{2}\norm{\x_{t+1}-\x_t}^2.\label{eq:md-iter-quadratic-bound}
    \end{equation}
    Next, we divide the inner product term $\inner{\eta\nabla f(\x_t)}{\x_{t+1}-\x}$ into two parts, and bound them separately:
    \begin{equation}
        \begin{aligned}
            \inner{\nabla f(\x_t)}{\x_{t+1}-\x}=&\inner{\nabla f(\x_t)}{\x_{t}-\x}+\inner{\nabla f(\x_t)}{\x_{t+1}-\x_t}\\\ge& f(\x_t)-f(\x)+f(\x_{t+1})-f(\x_t)-\frac{L}{2}\norm{\x_{t+1}-\x_t}^2\\=&f(\x_{t+1})-f(\x)-\frac{L}{2}\norm{\x_{t+1}-\x_t}^2,\label{eq:md-basic-iter-smooth}
        \end{aligned}
    \end{equation}
    where the inequality uses the convexity of $f$ and~\eqref{eq:md-iter-quadratic-bound}. Combining~\eqref{eq:md-basic-iter} with~\eqref{eq:md-basic-iter-smooth}, we derive
    \begin{equation}
        \eta\sqbrac{f(\x_{t+1})-f(\x)}-\frac{\eta L}{2}\norm{\x_{t+1}-\x_t}^2\le B(\x,\x_t)-B(\x,\x_{t+1})-B(\x_{t+1},\x_t)
    \end{equation}
    After rearrangement and due to the fact that $B(\x_{t+1},\x_t)\ge \frac{1}{2}\norm{\x_{t+1}-\x_t}^2$, we have
    \begin{equation}
        \begin{aligned}
            \eta\sqbrac{f(\x_{t+1})-f(\x)}\le& B(\x,\x_t)-B(\x,\x_{t+1})+\frac{\eta L-1}{2}\norm{\x_{t+1}-\x_t}^2\\\le& B(\x,\x_t)-B(\x,\x_{t+1})
        \end{aligned}
    \end{equation}
    Telescoping it and using Jensen's Inequality,
    \begin{equation}
        \begin{aligned}
            f(\xb_T)-f(\x)\le& \frac{\sum_{t=0}^{T-1}\eta\sqbrac{f(\x_{t+1})-f(\x)}}{\eta T}\\\le& \frac{\sum_{t=0}^{T-1}B(\x,\x_t)-B(\x,\x_{t+1})}{\eta T}\le\frac{B(\x,\x_0)}{
            \eta T},
        \end{aligned}
    \end{equation}
    Taking maximum on both sides of the inequality and under~\cref{ass:bound-domain}, the rate of convergence of the average-iterate is given by:
    \begin{equation}
        f(\xb_T)-f^*\le\frac{\max_{\x\in\X}B(\x,\x_0)}{\eta T}\le \frac{D^2}{\eta T}.
    \end{equation}
    As for the last iterate, recall the descent property in~\eqref{eq:descent-property}, we have $\sum_{t=0}^{T-1}f(\x_{t+1})-f(\x)\le T[f(\x_{T})-f(\x)]$. Thus, the convergence rate is exactly the same as that of $\xb_T$.
\end{proof}
\subsection{Accelerated Mirror Descent}
\begin{proof}{\textbf{of~\cref{thm:accelerated-mirror-descent}.}}
    By~\cref{lem:amd-induction}, we immediately conclude that
    \begin{equation}
        \max\{\dnorm{\nabla f(\y_t)},2\dnorm{\nabla f(\x_t)}\}\le 2G<\infty,\quad\forall t\in\N_+.
    \end{equation}
    Similar to~\eqref{eq:xt-yt-bound}, we have $\norm{\x_t-\y_t}\le 2G/L$. Then, we invoke~\cref{lem:amd-descent} to derive the following:
    \begin{equation}
        \frac{\eta t(t+1)}{4L}\sqbrac{f(\x_t)-f(\x)}\le \frac{\eta t(t-1)}{4L}\sqbrac{f(\x_{t-1})-f(\x)}+B(\x,\z_{t-1})-B(\x,\z_{t}),
    \end{equation}
    which holds for all $t\in\N_+$ and $\x\in\X$. Summing from $1$ to $T$, we obtain
    \begin{equation}
        \frac{\eta T(T+1)}{4L}\sqbrac{f(\x_T)-f(\x)}\le B(\x,\z_0)-B(\x,\z_T)\le B(\x,\x_0).
    \end{equation}
    Maximizing over $\x\in\X$ and use~\cref{ass:bound-domain}, we have
    \begin{equation}
        \frac{\eta T(T+1)}{4L}\sqbrac{f(\x_T)-f^*}\le \max_{\x\in\X}B(\x,\x_0)\le D^2.
    \end{equation}
    A simple rearrangement yields the desired convergence result.
\end{proof}
\subsection{Optimistic Mirror Descent}
\begin{proof}{\textbf{of~\cref{thm:optimistic-mirror-descent}.}} 
    Applying~\cref{lem:mp-algebra} to the updates defined in~\eqref{eq:optimistic-mirror-descent}, we have
    \begin{equation}
        \begin{aligned}
            \forall\x\in\X,\eta\inner{\nabla f(\y_t)}{\y_t-\x}\le& B(\x,\x_{t-1})-B(\x,\x_t)-B(\x_t,\y_t)-B(\y_t,\x_{t-1})\\&+\eta\inner{\nabla f(\y_t)-\nabla f(\y_{t-1})}{\y_t-\x_t}.
        \end{aligned}\label{eq:thm:omd-first-result}
    \end{equation}
    By~\cref{lem:omd-trajectory-grad-bound}, we have $\norm{\y_t-\y_{t-1}}\le\frac{G}{L}\sum_{s=1}^t1/3^s\le \frac{G}{2L}<\frac{G}{L}$ as we specify $\gamma=1/3$, which satisfies the condition in~\cref{lem:inner-product-smooth-bound}. Hence, the following result holds:
    \begin{equation}
        \eta\inner{\nabla f(\y_t)-\nabla f(\y_{t-1})}{\y_t-\x_t}\le\frac{1}{12}\norm{\y_t-\y_{t-1}}^2+\frac{1}{3}\norm{\y_t-\x_t}^2,
    \end{equation}
    where we set $\beta=2/3$ in~\cref{lem:inner-product-smooth-bound}. Plugging this into~\eqref{eq:thm:omd-first-result} and recalling the property of Bregman divergence, we have
    \begin{equation}
        \begin{aligned}
            \eta\inner{\nabla f(\y_t)}{\y_t-\x}\le& B(\x,\x_{t-1})-B(\x,\x_t)-\frac{1}{2}\norm{\x_t-\y_t}^2-\frac{1}{2}\norm{\y_t-\x_{t-1}}^2\\&+\frac{1}{6}\norm{\y_t-\x_{t-1}}^2+\frac{1}{6}\norm{\x_{t-1}-\y_{t-1}}^2+\frac{1}{3}\norm{\y_t-\x_t}^2\\\le&B(\x,\x_{t-1})-B(\x,\x_t)+\frac{1}{6}\norm{\x_{t-1}-\y_{t-1}}^2-\frac{1}{6}\norm{\x_{t}-\y_{t}}^2,
        \end{aligned}
    \end{equation}
    which holds for any $\x\in\X$. Telescoping sum, we obtain
    \begin{equation}
        \sum_{t=1}^T\inner{\eta \nabla f(\y_t)}{\y_t-\x}\le B(\x,\x_0)-B(\x,\x_T)-\frac{1}{6}\norm{\x_{T}-\y_{T}}^2\le B(\x,\x_0).
    \end{equation}
    Under~\cref{ass:bound-domain} and Jensen's Inequality, we derive the following convergence rate:
    \begin{equation}
        f(\overline{\y}_T)-f^*\le \frac{1}{\eta T}\max_{\x\in\X}\sum_{t=1}^T\inner{\eta \nabla f(\y_t)}{\y_t-\x}\le\frac{\max_{\x\in\X}B(\x,\x_0)}{\eta T}\le\frac{D^2}{\eta T},
    \end{equation}
    where $\overline{\y}_T=\sum_{t=1}^{T}\y_t/T$.
\end{proof}
\subsection{Mirror Prox}
\begin{proof}{\textbf{of~\cref{thm:mirror-prox}.}}
    By~\cref{lem:mp-trajectory-grad-bound}, we have $\dnorm{\nabla f(\y_t)}\le G<\infty,\dnorm{\nabla f(\x_t)}\le G<\infty$ for all $t\in\N_+$. Since the bound for $\nabla f(\x_0)=\nabla f(\y_0)$ naturally holds, so we obtain
    \begin{equation}
        \max\cbrac{\dnorm{\nabla f(\y_t)},\dnorm{\nabla f(\x_t)}}\le G<\infty,\quad \forall  t\in\N.\label{eq:thm:grad-norm-bound}
    \end{equation}
    Now it suffices to show the convergence rate is at the order of $O(1/T)$. Choosing $\x_0=\x_{t-1},\x_1=y_t,\g_1=\eta\nabla f(\x_{t-1}),\x_2=\x_t,\g_2=\eta\nabla f(\y_t)$ in~\cref{lem:mp-algebra}, we have
    \begin{equation}
        \begin{aligned}
            \forall\x\in\X,\inner{\eta\nabla f(\y_t)}{\y_t-\x}\le& B(\x,\x_{t-1})-B(\x,\x_t)-B(\x_t,\y_t)-B(\y_t,\x_{t-1})\\&-\eta\inner{\nabla f(\x_{t-1})-\nabla f(\y_t)}{\y_t-\x_t}.
        \end{aligned}\label{eq:thm:mp-initial-result}
    \end{equation}
    By~\cref{lem:md-stability} and~\eqref{eq:thm:grad-norm-bound}, we have
    \begin{equation}
        \norm{\y_t-\x_{t-1}}\le\eta\dnorm{\nabla f(\x_{t-1})}\le \frac{G}{2L}\le \frac{G}{L}.
    \end{equation}
    As $\eta\le1/(2L)$, we set $\beta=1$ in~\cref{lem:inner-product-smooth-bound} to derive
    \begin{equation}
        \eta\inner{\nabla f(\y_t)-\nabla f(\x_{t-1})}{\y_t-\x_t}\le\frac{1}{8}\norm{\y_t-\x_{t-1}}^2+\frac{1}{2}\norm{\y_t-\x_t}^2.\label{eq:thm:mp-inner-product-bound}
    \end{equation}
    Combine~\eqref{eq:thm:mp-initial-result} with~\eqref{eq:thm:mp-inner-product-bound}:
    \begin{equation}
        \begin{aligned}
            \forall\x\in\X,\inner{\eta\nabla f(\y_t)}{\y_t-\x}\le& B(\x,\x_{t-1})-B(\x,\x_t)-\frac{1}{2}\norm{\x_t-\y_t}^2-\frac{1}{2}\norm{\y_t-\x_{t-1}}^2\\&+\frac{1}{8}\norm{\y_t-\x_{t-1}}^2+\frac{1}{2}\norm{\y_t-\x_t}^2\\\le&B(\x,\x_{t-1})-B(\x,\x_t)-\frac{3}{8}\norm{\y_t-\x_{t-1}}^2,
        \end{aligned}
    \end{equation}
    where we also use the property of Bregman divergence. By convexity of $f$, we have
    \begin{equation}
        \forall\x\in\X,f(\y_t)-f(\x)\le \inner{\nabla f(\y_t)}{\y_t-\x}\le\frac{B(\x,\x_{t-1})-B(\x,\x_t)}{\eta}.
    \end{equation}
    Telescoping and taking maximum on both sides of the inequality, we obtain
    \begin{equation}
        \sum_{t=1}^T [f(\y_t)-f^*]\le \frac{\max_{\x\in\X}[B(\x,\x_{0})-B(\x,\x_T)]}{\eta}\le \frac{D^2}{\eta},
    \end{equation}
    where the last inequality uses~\cref{ass:bound-domain}. Since the output is the average of iterates, i.e., $\overline{\y}_T=\sum_{t=1}^T\y_t/T$, we finish the proof by Jensen's Inequality.
    \begin{equation}
        f(\overline{\y}_T)-f^*\le\frac{1}{T}\sum_{t=1}^T [f(\y_t)-f^*]\le\frac{D^2}{\eta T}.
    \end{equation}
\end{proof}
\subsection{Stochastic Mirror Descent\label{sec:proof-of-smd}}
\begin{proof}{\textbf{of~\cref{thm:stochastic-mirror-descent}.}}
    In the proof of~\cref{lem:smd-trajectory}, we have shown that with probability at least $1-\delta$ (see~\eqref{eq:avr-iter},~\eqref{eq:sum-eta_s-sigma_s-1^2},~\eqref{eq:mds-union-bound} and~\eqref{eq:q_t-whp}),
    \begin{equation}
        \sum_{s=1}^t\eta_sq_s\le  D^2+D^2(1+\ln t)+2D^2\sqrt{\brac{1+\ln t}\ln\frac{2t^3}{\delta}},\quad\forall t\in\N_+.
    \end{equation}
    For the average-iterate up to $t$-th round, by the convexity of $f$ and Jensen's Inequality, we have
    \begin{equation}
        \begin{aligned}
            &f(\xb_t)-f^*\le \frac{\sum_{s=1}^t\eta_sq_s}{\sum_{s=1}^t\eta_s}\le \frac{\overbrace{D^2(2+\ln t)+2D^2\sqrt{\brac{1+\ln t}\ln\frac{2t^3}{\delta}}}^{D_t}}{\sum_{s=1}^t\eta_s}\\\le&\frac{D_t}{\sum_{s=1}^t\frac{\min\cbrac{1,G_{s-1}/\sigma_{s-1}}}{2L_{s-1}}}+\frac{D_t}{\sum_{s=1}^t\frac{D}{\sigma_{s-1}\sqrt{s}}}\le\frac{\widetilde{L}_{t-1}^{\max}D_t}{t}+\frac{\sigma_{t-1}^{\max}D_t}{D\sqrt{t}},
        \end{aligned}
    \end{equation}
    where the second line is derived via the flooring technique~\citep{pmlr-v202-liu23aa,liu2024revisiting}; the last line uses $\sum_{s=1}^t1/\sqrt{s}\ge \sqrt{t}$ and the definitions below:
    \begin{equation}
        \widetilde{L}_{t}^{\max}:=\max\cbrac{L_0,\max_{1\le s\le t}\sqbrac{L_s\max\brac{1,\frac{\sigma_{s-1}}{G_{s-1}}}}},\quad \sigma_{t}^{\max}:=\max_{0\le s\le t}\sigma_s.
    \end{equation}
    By~\cref{lem:smd-trajectory}, we have $G_t=\O(1),\ L_t=\O(1),\ \sigma_t=\O(1)$ that holds with high probability along the trajectory. Therefore, we have $\sigma_{t-1}^{\max}=\O(1),\widetilde{L}_{t-1}^{\max}=\O(1)$. In view of $D_t=\O(1)$, we conclude our proof by combining these quantities:
    \begin{equation}
    \begin{aligned}
        f(\xb_t)-f^*\le&\frac{\widetilde{L}_{t-1}^{\max}\brac{D^2(2+\ln t)+2D^2\sqrt{\brac{1+\ln t}\ln\frac{2t^3}{\delta}}}}{t}\\&+\frac{\sigma_{t-1}^{\max}\brac{D(2+\ln t)+2D\sqrt{\brac{1+\ln t}\ln\frac{2t^3}{\delta}}}}{\sqrt{t}}=\O\brac{\frac{1}{\sqrt{t}}}.
    \end{aligned}        
    \end{equation}
\end{proof}

\section{Conclusion}
In this paper, we propose a refined smoothness notion called $\ell*$-smoothness, which characterizes the norm of the Hessian by leveraging a general norm and its dual. This extension encompasses the current generalized smooth conditions while highlighting non-Euclidean problem geometries. We establish new convergence results of mirror descent, accelerated mirror descent, optimistic mirror descent, and mirror prox algorithms under this setting. Beyond deterministic optimization, we further delve into SMD and propose a new bounded noise condition that substantially broadens the commonly assumed bounded/affine noise. Notably, we achieve a high-probability convergence for SMD that holds for all iterations simultaneously. 

\bibliography{MDGS}
\newpage
\appendix

% \section{Supplementary Related Work\label{app:sec:related-work}}

% In this section, we add more discussions on the development of generalized smoothness.

% \paragraph{$(L_0,L_1)$-smoothness}
% Ever since~\citet{Zhang2020Why} proposed $(L_0,L_1)$-smoothness condition, this generalized notion of smoothness has been flourishing in minimax optimization~\citep{pmlr-v235-xian24a}, bilevel optimization~\citep{hao2024bilevel,pmlr-v235-gong24d}, multi-objective optimization~\citep{zhang2024MOO}, sign-based optimization~\citep{crawshaw2022robustness}, distributionally robust optimization~\citep{jin2021nonconvexdro} and variational inequality~\citep{vankov2024adaptive,vankov2024generalized}. Numerous attempts have been made to refine existing algorithms under this weaker assumption, including variance reduction~\citep{reisizadeh2023variance}, clipping/normalized gradient~\citep{zhang2020improved,pmlr-v130-qian21a,DBLP:journals/chinaf/ZhaoXL21,pmlr-v238-hubler24a,yang2024independently}, error feedback~\citep{khirirat2024error} and trust region methods~\citep{Xie2024trustregion}. Notably,~\citet{pmlr-v178-faw22a,pmlr-v195-faw23a,pmlr-v195-wang23a,Li23adam,wang2024provable,zhang2024gs} explore the convergence of AdaGrad~\citep{JMLR:v12:duchi11a}, RMSprop~\citep{hinton2012neural}, and Adam~\citep{kingma15adam} under generalized smoothness. Beyond the optimization community, it has also garnered significant attention in federated learning~\citep{khirirat2024communication,demidovich2024methods} and meta-learning~\citep{chayti2024metalearning}.

% \paragraph{$\alpha$-symmetric generalized smooth}
% \citet{chen2023generalized} propose $\alpha$-symmetric generalized smoothness condition of the form
% \begin{equation}
%     \norm{\nabla f(\x)-\nabla f(\y)}_2\le\brac{L_0+L_1\sup_{\theta\in[0,1]}\norm{\nabla f\brac{\theta\x+(1-\theta)\y}}_2}\norm{\x-\y}_2,\ \forall\x,\y\in\mathcal{E},\label{eq:alpha-symmetric}
% \end{equation}
% for some $L_0,L_1\in\R_+$. As pointed out by~\citet{chen2023generalized,vankov2024optimizing}, any twice-differentiable function satisfying~\eqref{eq:alpha-symmetric} is also $(L_0^{'},L_1^{'})$-smooth for different constant factors. As an extension,~\citet{NeurIPS:2024:Jiang} study sign-based finite-sum non-convex optimization and obtain an improved complexity over SignSVRG algorithm~\citep{chzhen2023signsvrg}.

% \paragraph{$\ell$-smoothness}
% \citet{tyurin2024toward} study gradient descent for deterministic non-convex optimization and achieve optimal convergence rates (up to constant factors) without the need for a sub-quadratic \(\ell\) as required in~\citet{Li2023GS}. However, they impose an additional assumption of bounded gradients, which significantly simplifies the analysis.~\citet{NeurIPS'24:LocalSmooth} study online convex optimization problems by assuming each online function $f_t:\X\mapsto\R$ is $\ell_t$-smooth and that $\ell_t(\cdot)$ can be queried arbitrarily by the learner. Based on $\ell_t$-smoothness, they derive gradient-variation regret bounds for convex and strongly convex functions, albeit with the limitation of assuming a globally constant upper bound on the adversary's smoothness. 

% \paragraph{Other generalizations of classic smoothness}
% It is known that a function $f$ is $L$-smooth if $Lh-f$ is convex~\citep{nesterov2018lectures} for $h(\cdot)=\norm{\cdot}^2/2$. Advancements have been made to relax $h$ into an arbitrary Legendre function, leading to the development of a notion called relative smoothness~\citep{Bauschke17descentlemma,lu18relativesmooth}, which substitutes the canonical quadratic upper bound with $f(\x)\le f(\y)+\inner{\nabla f(\y)}{\x-\y}+LB_h(\x,\y)$, where $B_h$ is the Bregman divergence associated with $h$. More recently,~\citet{mishkin2024directional} propose directional smoothness, a measure of local gradient variation that preserves the global smoothness along specific directions. To address the imbalanced Hessian spectrum distribution in practice~\citep{sagun2016eigenvalues,pan2022eigencurve},~\citet{jiang2024convergence,liu2024adagrad} propose anisotropic smoothness and obtain improved bounds for adaptive gradient methods.

% \newpage

\section{Properties of Generalized Smooth Function Classes\label{app:sec:gsfc}}

In this section, we demonstrate some basic properties of the function class $\mathcal{F}_{\ell}(\norm{\cdot})$ as well as $\mathcal{F}_{\ell,r}(\norm{\cdot})$. \cref{lem:line-norm-bound} is a generalized version of Lemma A.4 of~\citet{Li2023GS}, which only supports $f\in\mathcal{F}_{\ell}(\norm{\cdot}_2)$. Since the original derivations did not utilize the inherent property of the Euclidean norm, i.e., $\inner{\cdot}{\cdot}=\norm{\cdot}_2^2$, we can adapt its proof to accommodate an arbitrary norm $\norm{\cdot}$.

\begin{lem}
    Let $f\in\mathcal{F}_{\ell}(\norm{\cdot})$. For any $\xt,\x\in\X$, denote $\x(s):=s\xt+(1-s)\x$. If $\x(s)\in\X$ holds for all $0\le s\le 1$ and $\norm{\xt-\x}\le\frac{G}{\ell(\norm{\nabla f(\x)}_*+G)}$ for any $G\in\R_{++}$, then we have
    \begin{equation}
        \max_{0\le s\le 1}\dnorm{\nabla f(\x(s))}\le\dnorm{\nabla f(\x)}+G.
    \end{equation}\label{lem:line-norm-bound}
\end{lem}

\begin{proof}
    First, define $g(s):=\dnorm{\nabla f(\x(s))}$ for $0\le s\le 1$. Since $\x(s)\in\X$, $\g(s)$ is differentiable almost everywhere by~\cref{def:ell-smooth}. Hence, the following relation holds almost everywhere over $(0,1)$:
    \begin{equation}
        \begin{aligned}
            g^\prime(s)=&\lim_{t\to s}\frac{g(t)-g(s)}{t-s}\le \lim_{t\to s}\frac{\dnorm{\nabla f(\x(t))-\nabla f(\x(s))}}{t-s}\\=&\dnorm{\lim_{t\to s}\frac{\nabla f(\x(t))-\nabla f(\x(s))}{t-s}}=\dnorm{\nabla^2 f(\x(s))(\xt-\x)}\le \ell(g(s))\norm{\xt-\x},
        \end{aligned}
    \end{equation}
    where the first inequality uses the triangle inequality and the last inequality uses~\cref{def:ell-smooth}. Next, we define $h(u):=\ell(u)\norm{\xt-\x}$ and $k(u):=\int_0^u1/h(v)\mathrm{d}v$. We immediately conclude that $g^\prime(s)\le h(g(s))$ holds almost everywhere over $(0,1)$, which satisfies the condition of a generalized version of the Grönwall’s inequality~\citep[Lemma~A.3]{Li2023GS}. Hence, we have
    \begin{equation}
        k(\dnorm{\nabla f(\xt)})=k(g(1))\le k(g(0))+1=k(\dnorm{\nabla f(\x}))+1.
    \end{equation}
    Based on this result we can derive
    \begin{equation}
        \begin{aligned}
            k(\dnorm{\nabla f(\xt)})\norm{\xt-\x}\le& k(g(0))\norm{\xt-\x}+\norm{\xt-\x}\\\le& \int_0^{g(0)}\frac{\norm{\xt-\x}}{\ell(u)\norm{\xt-\x}}\mathrm{d}u+\frac{G}{\ell(\norm{\nabla f(\x)}_*+G)}\\\le&\int_0^{\dnorm{\nabla f(\x)}}\frac{1}{\ell(u)}\mathrm{d}u+\int_{\dnorm{\nabla f(\x)}}^{\dnorm{\nabla f(\x)}+G}\frac{1}{\ell(u)}\mathrm{d}u\\=&k(\dnorm{\nabla f(\x)}+G)\norm{\xt-\x},
        \end{aligned}
    \end{equation}
    where the second inequality uses the bound for $\norm{\xt-\x}$ and the last inequality use the non-decreasing property of $\ell$. By the definition of $k$, it is obvious to see that $k$ is increasing. Then we can conclude that
    \begin{equation}
        \dnorm{\nabla f(\xt)}\le\dnorm{\nabla f(\x)}+G.\label{eq:Li-lemma-a4}
    \end{equation}
    Note that RHS of~\eqref{eq:Li-lemma-a4} involves the anchor point $\x$. Under the given conditions, the closed line segment between $\xt$ and $\x$ is contained in $\X$. Then, for any $0\le \tilde s\le 1$, we also have $\cbrac{\x(s)|0\le s\le \tilde s}\subseteq\X$ and $\norm{\x(\tilde{s})-\x}\le\frac{G}{\ell(\norm{\nabla f(\x)}_*+G)}$. Therefore, we can invoke~\eqref{eq:Li-lemma-a4} to show that 
    \begin{equation}
        \dnorm{\nabla f(\x(\tilde{s}))}\le\dnorm{\nabla f(\x)}+G,\quad\forall\tilde s\in[0,1].
    \end{equation}
    Maximizing over $0\le \tilde s\le 1$ yields the desired result.
\end{proof}

In the following, we provide the omitted proof of~\cref{prop:smooth-equivalence}.\\

\begin{proof}\textbf{of~\cref{prop:smooth-equivalence}.}
    The proof is very similar to that of~\citet[Proposition~3.2]{Li2023GS}, except that the derivations based on $\norm{\cdot}_2$ should be replaced by an arbitrary norm $\norm{\cdot}$.
    
    \paragraph{Part 1. $\mathcal{F}_{\ell,r}(\norm{\cdot})\subseteq\mathcal{F}_{\ell}(\norm{\cdot})$.} \ \\  
    
    Given an $\x\in\X$ where $\nabla^2f(\x)$ exists, according to~\cref{def:ell-r-smooth}, the following relation holds 
    \begin{equation}
        \norm{\nabla f(\x+s\h)-\nabla f(\x)}_*\le \ell(\norm{\nabla f(\x)}_*)\cdot s\norm{\h}
    \end{equation}
    for any $\h\in\X$ and any $s\in\R_{++}$ satisfying $s\norm{\h}\le r(\norm{\nabla f(\x)}_*)$. Then, based on the continuity of the dual norm, we deduce that
    \begin{equation}
        \begin{aligned}
            \norm{\nabla^2f(\x)\h}_*=&\norm{\lim_{s\to 0}\frac{\nabla f(\x+s\h)-\nabla f(\x)}{s}}_*\\=&\lim_{s\to 0}\norm{\frac{\nabla f(\x+s\h)-\nabla f(\x)}{s}}_*\le\ell(\norm{\nabla f(\x)}_*)\cdot \norm{\h}.
        \end{aligned}
    \end{equation}
    Next, we prove that $\nabla^2f(\x)$ exists almost everywhere. We briefly list the following two key steps, the detailed derivations can be found in~\citet[Proposition~3.2]{Li2023GS}.
    \begin{enumerate}
        \item Invoke Rademacher’s Theorem~\citep{evans2018measure} to show that $f$ is twice differentiable almost everywhere within the ball $\B(\x,r(\norm{\nabla f(\x)}_*))$.
        \item Leverage the discretization as well as the covering technique to show that $f$ is twice differentiable almost everywhere within the domain $\X$.
    \end{enumerate}
    \paragraph{Part 2. Under~\cref{ass:closed-f}, $\mathcal{F}_{\ell}(\norm{\cdot})\subseteq\mathcal{F}_{\tdel,\tdr}(\norm{\cdot})$ where $\tdel(\alpha)=\ell(\alpha+G),\tdr(\alpha)=\frac{G}{\tdel(\alpha)}$ for any $G\in\R_{++}$.}\ \\

    First, we show that $\B(\x,\tdr(\norm{\nabla f(\x)}_*))\subseteq\X$ for any $\x\in\X$ by contradiction. For any $\xt\in \B(\x,\tdr(\norm{\nabla f(\x)}_*))$, suppose $\xt\notin\X$. Define
    \begin{equation}
        \x(s):=s\xt+(1-s)\x,\ 0\le s\le 1\text{ and } s_b:=\inf\cbrac{s|\x(s)\notin\X}.
    \end{equation}
    By the definition of $s_b$ and~\cref{ass:closed-f}, we have $\lim_{s\to s_b}f(\x(s))=\infty$ and $\x(s)\in\X$ for $0\le s<s_b$. Then we have
    \begin{equation}
    \begin{aligned}
        \forall s\in[0,s_b):f(\x(s))= &f(\x)+\int_{0}^{s}\inner{\nabla f(\x(u))}{\xt-\x}{\mathrm d}u\\\le&f(\x)+s\cdot\max_{0\le u\le s}\norm{\nabla f(\x(u))}_*\cdot\norm{\xt-\x}\\\le&f(\x)+\brac{\norm{\nabla f(\x)}_*+G}\norm{\xt-\x}<\infty,
    \end{aligned}
    \end{equation}
    where the last inequality uses~\cref{lem:line-norm-bound} because the following condition is satisfied
    \begin{equation}
        \norm{\xt-\x}\le \tdr(\norm{\nabla f(\x)}_*)=G/\ell(\norm{\nabla f(\x)}_*+G)
    \end{equation}
    by definition. This contradicts $\lim_{s\to s_b}f(\x(s))=\infty$. Thus we have $\xt\in\X$ and consequently, $\B(\x,\tdr(\norm{\nabla f(\x)}_*))\subseteq\X$.
    Next, we prove~\eqref{eq:ell-r-smooth} in~\cref{def:ell-r-smooth}. For any $\x_1,\x_2\in\B(\x,\tdr(\norm{\nabla f(\x)}_*))$, define $\x_{1,2}(s):=s\x_1+(1-s)\x_2$ for $0\le s\le 1$. Since $\x_{1,2}(s)\in\B(\x,\tdr(\norm{\nabla f(\x)}_*))$, we obtain
    \begin{equation}
        \begin{aligned}
            \dnorm{\nabla f(\x_1)-\nabla f(\x_2)}=&\dnorm{\int_0^1\nabla^2f\brac{\x_{1,2}(s)}(\x_1-\x_2){\mathrm d}s}\\\le&\int_0^1\ell\brac{\dnorm{\nabla f\brac{\x_{1,2}(s)}}}\cdot\norm{\x_1-\x_2}{\mathrm d}s\\\le&\max_{0\le s\le 1}\ell\brac{\dnorm{\nabla f\brac{\x_{1,2}(s)}}}\cdot\norm{\x_1-\x_2}\\\le&\ell\brac{\dnorm{\nabla f(\x)}+G}\cdot\norm{\x_1-\x_2},
        \end{aligned}
    \end{equation}
    where the first inequality uses~\cref{def:ell-smooth} and the last inequality uses~\cref{lem:line-norm-bound} and the non-decreasing property of $\ell$.
\end{proof}

% \begin{proof}\textbf{of~\cref{prop:norm-smooth-equivalence}.}
%     The proof is quite straightforward by~\cref{def:ell-smooth} and the equivalence between different norms. For any $f\in\mathcal{F}_{\ell}(\norm{\cdot})$,
%     \begin{equation}
%         \begin{aligned}
%             \forall\h\in\X:m_*\norm{\nabla^2f(\x)\h}_2\le& \dnorm{\nabla^2f(\x)\h}\\\le& \ell(\dnorm{\nabla f(\x)})\norm{\h}\le \ell(M_*\norm{\nabla f(\x)}_2)M\norm{\h}_2,
%         \end{aligned}
%     \end{equation}
%     which is essentially $\norm{\nabla^2f(\x)\h}_2\le\tdel(\dnorm{\nabla f(\x)})\norm{\h}_2$ for any $\h\in\X$. Thus we have $f\in\mathcal{F}_{\ell}(\norm{\cdot}_2)$ and consequently yields $\mathcal{F}_{\ell}(\norm{\cdot})\subseteq\mathcal{F}_{\tdel}(\norm{\cdot}_2)$.
% \end{proof}

We present the following effective/local smoothness property derived under generalized smoothness.~\cref{lem:effective-L-smooth} is extremely useful as it closes the gap between generalized smoothness and classical smoothness.

\begin{proof}\textbf{of~\cref{lem:effective-L-smooth}.}
    % To prove this lemma, we need to modify the existing result for $f\in\mathcal{F}_{\ell,r}(\norm{\cdot}_2)$~\citep[Lemma~3.3]{Li2023GS}. That is, we will prove the three analogous statements for $f\in\mathcal{F}_{\ell,r}(\norm{\cdot})$. Then we use the equivalence between generalized smooth function classes to argue that the statements also hold for $f\in\mathcal{F}_{\ell}(\norm{\cdot})$. 
    To prove this lemma, we adapt the existing result for \(f \in \mathcal{F}_{\ell,r}(\norm{\cdot}_2)\)~\citep[Lemma~3.3]{Li2023GS}. Specifically, we establish three analogous statements for \(f \in \mathcal{F}_{\ell,r}(\norm{\cdot})\) and then leverage the equivalence between generalized smooth function classes to conclude that these statements also hold for \(f \in \mathcal{F}_{\ell}(\norm{\cdot})\).
    By~\cref{def:ell-r-smooth}, we have $\ell(\dnorm{\nabla f(\x)})\le\ell(G)$ and $r(G)\le r(\dnorm{\nabla f(\x)})$. Hence, it follows that (i) $\B(\x,r(G))\subseteq\B(\x,r(\dnorm{\nabla f(\x)}))\subseteq\X$; and (ii) $\forall\x_1,\x_2\in\B(\x,r(G))$:
    \begin{equation}
        \norm{\nabla f(\x_1)-\nabla f(\x_2)}_*\le \ell(\dnorm{\nabla f(\x)})\norm{ \x_1-\x_2}\le\ell(G)\norm{ \x_1-\x_2}.\label{eq:effL2nd}
    \end{equation}
    Statements (i) and (ii) are similar to statements 1 and 2 in the lemma. Now we proceed to the quadratic upper bound. Define $\x(s)=s\x_1+(1-s)\x_2$ for $0\le s\le 1$. It's clear that $\x(s),\x_2\in \B(\x,r(G))$, so according to~\eqref{eq:effL2nd}, we have
    \begin{equation}
        \dnorm{\nabla f(\x(s))-\nabla f(\x_2)}\le \ell(G)\norm{\x_1-\x_2}.
    \end{equation}
    Then we can derive a local quadratic upper bound (denoted by (iii)) as follows
    \begin{equation}
        \begin{aligned}
            f(\x_1)-f(\x_2)=&\int_0^1\inner{\nabla f(\x(s))}{\x_1-\x_2}\mathrm{d}s\\=&\inner{\nabla f(\x_2)}{\x_1-\x_2}+\int_0^1\inner{\nabla f(\x(s))-\nabla f(\x_2)}{\x_1-\x_2}\mathrm{d}s\\\le&\inner{\nabla f(\x_2)}{\x_1-\x_2}+\int_0^1\dnorm{\nabla f(\x(s))-\nabla f(\x_2)}\norm{\x_1-\x_2}\mathrm{d}s\\\le&\inner{\nabla f(\x_2)}{\x_1-\x_2}+\frac{\ell(G)}{2}\norm{\x_1-\x_2}^2.
        \end{aligned}
    \end{equation}
    At last, we apply the equivalence of generalized smooth function classes to show that the same derivations also hold for $f\in\mathcal{F}_{\ell}(\norm{\cdot})$. In this case, it suffices to set $L=\ell(2G)$ and $r(G)=G/L$ according to~\cref{prop:smooth-equivalence}. Then the statements (i), (ii), and (iii) can be easily transformed into the ones in~\cref{lem:effective-L-smooth}, which concludes the proof.
\end{proof}

The following lemma is introduced out of convenience, for the sake of frequent usages in the next few sections.

\begin{lem}
    Under~\cref{ass:sub-quadratic-ell}, let $\dnorm{\nabla f(\x)}\le G\in\R_{+}$ for any given $\x\in\X$. If $0<\eta\le\gamma/\ell(2G)$ for some $\gamma\in(0,1)$, then for any $\x_1,\x_2\in\B(\x,G/\ell(2G))$ and any $\x_3,\x_4\in\X$, 
    \begin{equation}                
    \eta\inner{\nabla f(\x_1)-\nabla f(\x_2)}{\x_3-\x_4}\le\frac{\gamma^2}{2\beta}\norm{\x_1-\x_2}^2+\frac{\beta}{2}\norm{\x_3-\x_4}^2,
    \end{equation}
    holds for any $\beta\in\R_{++}$.\label{lem:inner-product-smooth-bound}
\end{lem}

\begin{proof}
    Since $\dnorm{\nabla f(\x)}\le G$ and $\x_1,\x_2\in\B(\x,G/\ell(2G))$, by~\cref{lem:effective-L-smooth}, we have
    \begin{equation}
        \dnorm{\nabla f(\x_1)-\nabla f(\x_2)}\le \ell(2G)\norm{\x_1-\x_2}.
    \end{equation}
    According to Hölder's Inequality, we have
    \begin{equation}
        \forall\beta\in\R_{++}:\eta\inner{\nabla f(\x_1)-\nabla f(\x_2)}{\x_3-\x_4}\le\frac{\eta^2}{2\beta}\dnorm{\nabla f(\x_1)-\nabla f(\x_2)}^2+\frac{\beta}{2}\norm{\x_3-\x_4}^2.
    \end{equation}
    Combining these two inequalities with the condition that $0<\eta\le\gamma/\ell(2G)$ completes the proof.
\end{proof}

Lastly, we provide the omitted proof of~\cref{lem:pl-inequality} in~\cref{sec:main-idea}.~\cref{lem:pl-inequality} is also a generalized version of the reversed Polyak-\L{}ojasiewicz Inequality in~\citet[Lemma~3.5]{Li2023GS}. Since their derivations rely on the inherent properties of the Euclidean norm, we need to take a different approach to prove this lemma.\\

% \begin{lem}
%     For any $f\in\mathcal{F}_{\ell}(\norm{\cdot})$, we have $\dnorm{\nabla f(\x)}^2\le 2\ell(2\dnorm{\nabla f(\x)})(f(\x)-f^*)$ for any $\x\in\X$.\label{lem:pl-inequality}
% \end{lem}

\begin{proof}\textbf{of~\cref{lem:pl-inequality}.}
    For any $\x\in\X$ and any $\h$ satisfying $\norm{\h}\le \dnorm{\nabla f(\x)}/\ell(2\dnorm{\nabla f(\x)})$, by~\cref{lem:effective-L-smooth}, the following relation holds:
    \begin{equation}
        \inner{\nabla f(\x)}{\h}-\frac{\ell(2\dnorm{\nabla f(\x)})}{2}\norm{\h}^2\le f(\x)-f(\x-\h)\le f(\x)-f^*.\label{eq:pl-proof-effective-smooth}
    \end{equation}
    In the unconstrained setting, we can leverage the conjugate of the square norm, i.e., \protect{$\sup_{\h}\inner{\x}{\h}-\norm{\h}^2/2=\dnorm{\x}^2/2$}~\citep[Example~5.11]{orabona2019intro} to cope with LHS of~\eqref{eq:pl-proof-effective-smooth}. Now that we are dealing with the vector lying within a small ball centered at $\x$, we should apply the clipping technique to lower bound the inner product. Let $\u$ be any unit vector with $\inner{\nabla f(\x)}{\u}=\dnorm{\nabla f(\x)}$. Then we scale $\u$ to fit the radius of the ball $\B(\x,\dnorm{\nabla f(\x)}/\ell(2\dnorm{\nabla f(\x)}))$, i.e., set $\bv=\u\cdot\dnorm{\nabla f(\x)}/\ell(2\dnorm{\nabla f(\x)})$ so that $\norm{\bv}=\dnorm{\nabla f(\x)}/\ell(2\dnorm{\nabla f(\x)})$. Since~\eqref{eq:pl-proof-effective-smooth} holds for any $\h$ satisfying $\norm{\h}\le \dnorm{\nabla f(\x)}/\ell(2\dnorm{\nabla f(\x)})$, we take $\h=\bv$ and obtain
    \begin{equation}
    \begin{aligned}
        &\frac{\dnorm{\nabla f(\x)}}{\ell(2\dnorm{\nabla f(\x)})}\inner{\nabla f(\x)}{\u}-\frac{\ell(2\dnorm{\nabla f(\x)})}{2}\sqbrac{\frac{\dnorm{\nabla f(\x)}}{\ell(2\dnorm{\nabla f(\x)})}}^2\\=&\frac{\dnorm{\nabla f(\x)}^2}{2\ell(2\dnorm{\nabla f(\x)})}\le f(\x)-f^*.
    \end{aligned}        
    \end{equation}
\end{proof}

The following lemma goes one step further to give a quantitative characterization between the gradient's dual norm and the suboptimality gap.

\begin{lem}
    Under~\cref{ass:sub-quadratic-ell}, for a given $\x\in\X$ satisfying $f(\x)-f^*\le F$, we have $\dnorm{\nabla f(\x)}\le G<\infty$ and $G^2=2\ell(2G)F$, where $G:=\sup\cbrac{\alpha\in\R_{+}|\alpha^2\le2F\ell (2\alpha)}$.\label{lem:grad-norm-suboptimality}
\end{lem}

\begin{proof}
    Based on~\cref{lem:pl-inequality}, the proof is analogous to that of~\citet[Corollary~3.6]{Li2023GS}, which involves merely an $f\in\mathcal{F}_{\ell}(\norm{\cdot}_2)$. Since we have a sub-quadratic $\ell$, we clearly have $\lim_{\alpha\to\infty}\frac{\alpha^2}{2\ell\brac{2\alpha}}=\infty$. So for $F\in\R_+,\exists\alpha_F\in\R_{++}$, such that $\forall\alpha>\alpha_F,\frac{\alpha^2}{2\ell\brac{2\alpha}}>F$. Therefore, if $\frac{\alpha^2}{2\ell\brac{2\alpha}}\le F$ holds for some $\alpha$, then it must be that $\alpha\le\alpha_F$. Hence, our construction of $G$ implies that $G\le\alpha_F<\infty$. We conclude that $\dnorm{\nabla f(\x)}\le G$ by~\cref{lem:pl-inequality}. Lastly, $G^2=2\ell(2G)F$ is directly implied by the compactness of the set $\cbrac{\alpha\in\R_{+}|\alpha^2\le2F\ell (2\alpha)}$. 
\end{proof}
\newpage

\section{The Gain of Algorithmic Adaptivity\label{app:sec:example}}

In this section, we present the omitted details for~\cref{sec:examples}.

\subsection{Theoretical Justifications\label{app:sec:theoretical-justifications}}

We show the example function $h(\x)=\x^\top(\one_n-\e_1)(\one_n-\e_1)^\top\x/2,\x\in\Delta_n$ suffers from an extra \emph{dimension-dependent} smoothness under $\ell$-smoothness, as opposed to $\ell*$-smoothness.
\setParDis

\begin{proof}\textbf{of~\cref{prop:example-function}.}
    Denote $(\one_n-\e_1)(\one_n-\e_1)^\top$ by $A$. For the quadratic function $h(\x)=\frac{1}{2}\x^\top A\x$ where $A=A^\top$, we have
    \begin{equation}
        \nabla h(\x)=A\x,\quad\nabla^2 h(\x)=A.
    \end{equation}
    Since $A=(\one_n-\e_1)(\one_n-\e_1)^\top$ is a rank-1 matrix, we have
    \begin{equation}
        \norm{\nabla^2 h(\x)}_2=\norm{\one_n-\e_1}_2\norm{\one_n-\e_1}_2=\sqrt{n-1}\cdot\sqrt{n-1}=n-1.
    \end{equation}
    Hence, for $h\in\mathcal{F}_{\widehat{\ell}}(\norm{\cdot}_2)$, we have $\widehat{\ell}(\cdot)\equiv n-1$. By~\cref{def:ell-smooth}, for $h\in\mathcal{F}_{\tdel}(\norm{\cdot})$, we aim to find the link function $\tdel(\cdot)$ such that
    \begin{equation}
    \begin{aligned}
        &\forall \u\in\Delta_n, \norm{\nabla^2h(\x)\u}_\infty\le\tdel(\norm{\nabla h(\x)}_\infty)\norm{\u}_1\\\Longleftrightarrow&\sup_{\u\in\Delta_n}\norm{\nabla^2h(\x)\u}_\infty\le\tdel(\norm{\nabla h(\x)}_\infty).
    \end{aligned}        
    \end{equation}
    We proceed by computing 
    \begin{equation}
        \begin{aligned}
&\sup_{\u\in\Delta_n}\norm{A\u}_\infty=\sup_{\u\in\Delta_n}\norm{(\one_n-\e_1)[(\one_n-\e_1)^\top\u]}_\infty\\=&\sup_{\u\in\Delta_n}(\one_n-\e_1)^\top\u=\sup_{\u\in\Delta_n}\sum_{i=2}^{n}u_i=1,
        \end{aligned}
    \end{equation}
    where we make use of $\norm{\one_n-\e_1}_\infty=1$. Therefore, we conclude that $\tdel(\cdot)\equiv 1$.
\end{proof}

The additional factor $n-1$ will inevitably incur a \emph{dimension-dependent} convergence rate, which will be stated explicitly in~\cref{app:sec:constant-factor}.

\subsection{Empirical Justifications\label{app:sec:empirical-justifications}}

\begin{figure*}[t]
    \centering
    \captionsetup{justification=centering}
    \subfigure[\(n=5\)]{\includegraphics[width=.245\textwidth]{fig/ell_form_5.pdf}\label{fig:ell_form_5}}
    \subfigure[\(n=10\)]{\includegraphics[width=.245\textwidth]{fig/ell_form_10.pdf}\label{fig:ell_form_10}}
    \subfigure[\(n=15\)]{\includegraphics[width=.245\textwidth]{fig/ell_form_15.pdf}\label{fig:ell_form_15}}
    \subfigure[\(n=20\)]{\includegraphics[width=.245\textwidth]{fig/ell_form_20.pdf}\label{fig:ell_form_20}}
    
    \subfigure[\(n=25\)]{\includegraphics[width=.245\textwidth]{fig/ell_form_25.pdf}\label{fig:ell_form_25}}
    \subfigure[\(n=30\)]{\includegraphics[width=.245\textwidth]{fig/ell_form_30.pdf}\label{fig:ell_form_30}}
    \subfigure[\(n=35\)]{\includegraphics[width=.245\textwidth]{fig/ell_form_35.pdf}\label{fig:ell_form_35}}
    \subfigure[\(n=40\)]{\includegraphics[width=.245\textwidth]{fig/ell_form_40.pdf}\label{fig:ell_form_40}}
    
    \subfigure[\(n=45\)]{\includegraphics[width=.245\textwidth]{fig/ell_form_45.pdf}\label{fig:ell_form_45}}
    \subfigure[\(n=50\)]{\includegraphics[width=.245\textwidth]{fig/ell_form_50.pdf}\label{fig:ell_form_50}}
    \subfigure[\(n=100\)]{\includegraphics[width=.245\textwidth]{fig/ell_form_100.pdf}\label{fig:ell_form_100}}
    \subfigure[\(n=150\)]{\includegraphics[width=.245\textwidth]{fig/ell_form_150.pdf}\label{fig:ell_form_150}}
    
    \caption{Comparison of $\mathcal{F}_{\widehat{\ell}}(\norm{\cdot}_2)$ and $h\in\mathcal{F}_{\tdel}(\norm{\cdot})$ generalized smooth functions class for different values of \(n\).\label{fig:ell_form_grid}}
\end{figure*}

We focus on the following example function:
\begin{equation}
    h(\x)=\frac{\brac{ \sum_{i=1}^n x_i^2 }^2}{4}  + \prod_{i=1}^n x_i^2 + \sum_{i=1}^n e^{5x_i},\quad\forall\x\in\Delta_n.\label{eq:example-function}
\end{equation}
For the Euclidean setup $h\in\mathcal{F}_{\widehat{\ell}}(\norm{\cdot}_2)$ as well as the non-Euclidean setup $h\in\mathcal{F}_{\tdel}(\norm{\cdot})$, the smoothness can be modeled by
\begin{equation}
    \norm{\nabla^2 h(\x)}_2\le \widehat{\ell}(\norm{\nabla h(\x)}_2),\text{ and }\sup_{\u\in\Delta_n}\norm{\nabla^2 h(\x)\u}_\infty\le \widehat{\ell}(\norm{\nabla h(\x)}_\infty),
\end{equation}
respectively.
% For different values of $n$, we conduct numerical experiments to study the coarse form of $\widehat{\ell}(\cdot)$ and $\tdel(\cdot)$. We compute and plot $ \norm{\nabla^2 h(\x)}_2$ w.r.t.~$\norm{\nabla h(\x)}_2$ for $h\in\mathcal{F}_{\widehat{\ell}}(\norm{\cdot}_2)$, where the gradient and the Hessian can be calculated via autograd in Pytorch~\citep{NEURIPS2019PYTORCH}. Similarly, we compute $\sup_{\u\in\Delta_n}\norm{\nabla^2 h(\x)\u}_\infty$ and $\norm{\nabla h(\x)}_\infty$ for $h\in\mathcal{F}_{\tdel}(\norm{\cdot})$, where the difficulty lies in the tricky term $\sup_{\u\in\Delta_n}\norm{\nabla^2 h(\x)\u}_\infty$. Luckily, as a special case of the $p\to q$ matrix norm problem~\citep{bhaskara2011approximating}, constant factor approximation algorithms are known for $p\ge 2\ge q$~\citep{khot2012grothendieck,pisier2012grothendieck,bhattiprolu2019approximability}. Therefore, we can also plot $\sup_{\u\in\Delta_n}\norm{\nabla^2 h(\x)\u}_\infty$ w.r.t.~$\norm{\nabla h(\x)}_\infty$. The results are shown in~\cref{fig:ell_form_grid}, from which we can roughly see that the generalized smooth relation for both cases can be realized by an affine function as follows:
For various values of \(n\), we conduct numerical experiments to examine the coarse forms of \(\widehat{\ell}(\cdot)\) and \(\tdel(\cdot)\). Specifically, we compute and plot \(\norm{\nabla^2 h(\x)}_2\) against \(\norm{\nabla h(\x)}_2\) for \(h \in \mathcal{F}_{\widehat{\ell}}(\norm{\cdot}_2)\), leveraging PyTorch’s autograd functionality~\citep{NEURIPS2019PYTORCH} to calculate gradients and Hessians. Similarly, for \(h \in \mathcal{F}_{\tdel}(\norm{\cdot})\), we compute \(\sup_{\u \in \Delta_n} \norm{\nabla^2 h(\x)\u}_\infty\) and \(\norm{\nabla h(\x)}_\infty\). The main challenge lies in evaluating \(\sup_{\u \in \Delta_n} \norm{\nabla^2 h(\x)\u}_\infty\), which, as a special case of the \(p \to q\) matrix norm problem~\citep{bhaskara2011approximating}, can be approximated using constant-factor algorithms for \(p \geq 2 \geq q\)~\citep{khot2012grothendieck, pisier2012grothendieck, bhattiprolu2019approximability}. In our $\infty\to1$ case, it suffices to model it as a linear programming problem with constraints $\u\in\Delta_n$ and then solve it numerically. Consequently, we are able to plot \(\sup_{\u \in \Delta_n} \norm{\nabla^2 h(\x)\u}_\infty\) against \(\norm{\nabla h(\x)}_\infty\), as well. The results, shown in~\cref{fig:ell_form_grid}, suggest that the generalized smooth relation in both cases can be approximated by the affine functions shown below:
\begin{equation}
    \widehat{\ell}(\alpha)=\widehat{L}_0+\widehat{L}_1\alpha,\quad \tdel(\alpha)=\widetilde{L}_0+\widetilde{L}_1\alpha.\label{eq:affine-ell-both}
\end{equation}
We find this characterization reasonable based on observations from~\cref{fig:ell_form_grid}: when \(n > 20\), the scattered points align into a straight line. To this end, we fit the values of \(\widehat{L}_0\), \(\widehat{L}_1\), \(\widetilde{L}_0\), and \(\widetilde{L}_1\) using $500$ randomly sampled points from the simplex \(\Delta_n\). 
\setParDis

Based on the modeling in~\eqref{eq:affine-ell-both}, we can see that the slopes \(\widehat{L}_1\) and \(\widetilde{L}_1\) determine the magnitude of smoothness along the optimization trajectory (e.g., for some constant $0<G<\infty$, $\widehat{\ell}(G)=\widehat{L}_0+\widehat{L}_1G$), to a great extent. To clarify, note that the definition of $G$ in~\eqref{eq:G&L} has a closed-form solution:
\begin{equation}
\begin{aligned}
    &G=\sup\cbrac{\alpha\in\R_+\left|\alpha^2\le 2F\brac{\widetilde{L}_0+\widetilde{L}_1
    \alpha}\right.}\\=&F\widetilde{L}_1+\sqrt{2F\widetilde{L}_0+F^2\widetilde{L}_1^2}\le 2F\widetilde{L}_1+\sqrt{2F\widetilde{L}_0},
\end{aligned}  
\end{equation}
where $f(\x_0)-f^*$ is denoted by $F$. The above relation implies that $G$ grows linearly w.r.t.~$\widetilde{L}_1$ and $\sqrt{\widetilde{L}_0}$. As a result, by~\eqref{eq:G&L}, we compute the following effective smoothness parameter:
\begin{equation}
    L=\tdel(2G)=\widehat{L}_0+2\widehat{L}_1G\le \widehat{L}_0+4F\widehat{L}_1^2+2F\widehat{L}_1\sqrt{2F\widetilde{L}_0}.
\end{equation}
Therefore, we conclude that $\widetilde{L}_1$ is the dominant term affecting the smoothness parameter $L$.
\setParDis

Due to the reasoning above, we concentrate on the slopes \(\widetilde{L}_1, \widehat{L}_1\) to analyze and compare the characteristics of the link functions $\widehat{\ell}$ and $\tdel$. To further explore this, we examine the ratio \(\widetilde{L}_1 / \widehat{L}_1\) for various values of \(n\). Specifically, we vary \(n\) from \(6\) to \(198\) in increments of \(3\) and fit a function of the form \(g(n) = a \cdot n^{-b}, a, b > 0\). The results, shown in~\cref{fig:slope}, reveal that the gap between \(\tdel\) and \(\widehat{\ell}\) is approximately at the order of \(O(n^{-0.4})\).

\subsection{Comparison on the Convergence Rates\label{app:sec:constant-factor}}
% Lastly, we explicitly discuss the gain in the convergence rates. The spirit of this discussion goes back to~\citet[(2.59) and (2.60)]{nemirovski2009robust}, and is further developed in~\citet[Section~3.2]{lan2020first}. Consider a generalized smooth function $f$ defined on the simplex $\Delta_n$, where $f$ satisfies $f\in\mathcal{F}_{\widehat{\ell}}(\norm{\cdot}_2)$ and $f\in\mathcal{F}_{\tdel}(\norm{\cdot})$, with $\widehat{\ell}$ and $\tdel$ satisfying~\eqref{eq:affine-ell-both}. According to~\citet[Theorem~4.2]{Li2023GS}, gradient descent provably converges at 
% \begin{equation}
%     \widehat{\ell}(\norm{\nabla f(\x_0)}_2)\norm{\x_0-\x_*}_2^2/(2T)\le \widehat{\ell}(\norm{\nabla f(\x_0)}_2)\cdot /(2T)\triangleq R_1(T),
% \end{equation}
% where we use $\norm{\x_0-\x_*}_2^2\le\sup_{\x,\y\in\Delta_n}\le 1$. According to~\cref{thm:mirror-descent}, mirror descent provably converges at 
% \begin{equation}
%    \tdel(G)D^2/T\le\tdel(G)\ln{n}/T\triangleq R_2(T),
% \end{equation}
% where we specify the diameter of the simplex as $\sqrt{\ln n}$~\citep{nemirovski2009robust}. Then, the ratio of these convergence rates is
% \begin{equation}
%     \frac{R_2(T)}{R_1(T)}=\frac{\tdel(G)\ln{n}}{\widehat{\ell}(\norm{\nabla f(\x_0)}_2)}\simeq\frac{\tdel(\norm{\nabla f(\x_0)}_\infty)\ln{n}}{\widehat{\ell}(\norm{\nabla f(\x_0)}_2)}\simeq \frac{\widetilde{L}_1\ln{n}}{\widehat{L}_1}\simeq\frac{\ln n}{n^{0.4}},
% \end{equation}
% which indicates the edge of mirror descent over gradient descent, i.e., reduce the dependency on the dimension factor.

Finally, we explicitly discuss the improvement in convergence rates. The similar discussions traces back to~\citet[(2.59) and (2.60)]{nemirovski2009robust} and is further developed in~\citet[Section~3.2]{lan2020first}. Consider a generalized smooth function \(f\) defined on the simplex \(\Delta_n\), where \(f\) satisfies \(f \in \mathcal{F}_{\widehat{\ell}}(\norm{\cdot}_2)\) and \(f \in \mathcal{F}_{\tdel}(\norm{\cdot})\). According to~\citet[Theorem~4.2]{Li2023GS}, gradient descent achieves a convergence rate of  
\begin{equation}
    \frac{\widehat{\ell}(\norm{\nabla f(\x_0)}_2) \norm{\x_0 - \x_*}_2^2}{2T} \leq \frac{\widehat{\ell}(\norm{\nabla f(\x_0)}_2)}{2T} \triangleq R_1(T),
\end{equation}
where we use \(\norm{\x_0 - \x_*}_2^2 \leq \sup_{\x, \y \in \Delta_n} \norm{\x - \y}_2^2 = 1\). For mirror descent, as established in~\cref{thm:mirror-descent}, the convergence rate is given by  
\begin{equation}
    \frac{\tdel(G) D^2}{T} \leq \frac{\tdel(G) \ln n}{T} \triangleq R_2(T),
\end{equation}
where the diameter of the simplex is specified as \(\sqrt{\ln n}\)~\citep{nemirovski2009robust}. First, we consider the example function $h(\x)=\x^\top(\one_n-\e_1)(\one_n-\e_1)^\top\x/2,\x\in\Delta_n$ in~\cref{app:sec:theoretical-justifications}. By~\cref{prop:example-function}, we have \(h \in \mathcal{F}_{\widehat{\ell}}(\norm{\cdot}_2)\) and \(h \in \mathcal{F}_{\tdel}(\norm{\cdot})\) with $\widehat{\ell}(\cdot)\equiv n-1$ and $\tdel(\cdot)\equiv 1$. Hence, the above convergence rates are valid and the ratio of the convergence rates is
\begin{equation}
    \frac{R_2(T)}{R_1(T)} = \frac{\tdel(G) \ln n}{\widehat{\ell}(\norm{\nabla f(\x_0)}_2)}=\frac{\ln n}{n-1},
\end{equation}
suggesting that mirror descent defined in~\eqref{eq:mirror-descent} provably improves the convergence rate by a factor of $n/\ln n$. The effect will become prominent when the dimension is large.
\setParDis

Next, our focal point shifts to the example function $h(\cdot)$ defined in~\eqref{eq:example-function}. In view of discussions made in~\cref{app:sec:empirical-justifications}, we conclude that \(h \in \mathcal{F}_{\widehat{\ell}}(\norm{\cdot}_2)\) and \(h \in \mathcal{F}_{\tdel}(\norm{\cdot})\) with \(\widehat{\ell}\) and \(\tdel\) satisfying~\eqref{eq:affine-ell-both}. Moreover, we have \(\widetilde{L}_1 / \widehat{L}_1\simeq n^{-0.4}\). Then, the ratio of the convergence rates can written as  
\begin{equation}
    \frac{R_2(T)}{R_1(T)} = \frac{\tdel(G) \ln n}{\widehat{\ell}(\norm{\nabla f(\x_0)}_2)} \simeq \frac{\tdel(\norm{\nabla f(\x_0)}_\infty) \ln n}{\widehat{\ell}(\norm{\nabla f(\x_0)}_2)} \simeq \frac{\widetilde{L}_1 \ln n}{\widehat{L}_1} \simeq \frac{\ln n}{n^{0.4}},
\end{equation}
underlining the advantage of mirror descent over gradient descent by reducing the dependency on the dimensionality factor.

\newpage
\section{Analysis for Mirror Descent\label{app:sec:md}}
% change the phrase using newly formed assumptions.

% \begin{lem}
%     Suppose $f\in\mathcal{F}_{\ell,r}(\norm{\cdot})$. Under~\cref{ass:closed-f,ass:bound-domain}, it holds that for any $\x\in\X$:
%     \begin{equation}
%         \dnorm{\nabla f(\x)}\le G_r+2\sqrt{2}D\ell(G_r),
%     \end{equation}
%     where we define $G_r:=\inf\cbrac{\alpha\in\R_{+}|r(\alpha)\ge2\sqrt{2}D}$.
% \end{lem}

% \begin{proof}
%     Under~\cref{ass:bound-domain}, we can easily assert that the distance between two arbitrary points in the domain is bounded~\citep[Proposition A.6]{yu24egdro}, i.e., 
%     \begin{equation}
%         \forall\x_1,\x_2\in\X:\norm{\x_1-\x_2}\le 2\sqrt{2}D.
%     \end{equation}
%     Under~\cref{ass:closed-f}, for any $f\in\mathcal{F}_{\ell,r}(\norm{\cdot})$, there exists a point $\xt^*\in\X$, such that $\dnorm{\nabla f(\xt^*)}\le G_r$. 
% \end{proof}

% \begin{lem}
%     For any $f\in\mathcal{F}_{\ell}(\norm{\cdot})$ where $\ell$ is sub-quadratic (i.e., $\lim_{\alpha\to \infty}\alpha^2/\ell(\alpha)=\infty$) and for a given $\x\in\X$ satisfying $f(\x)-\inf_{\x\in\X}f(\x)\le F$, we have $\dnorm{\nabla f(\x)}\le G<\infty$ where $G:=\sup\cbrac{\alpha\in\R_{+}|m_*\alpha^2\le2FMM_*^2\ell (2M_*\alpha/m_*)}$.
% \end{lem}

% \begin{proof}
%     According to~\cref{prop:norm-smooth-equivalence}, we have $f\in\mathcal{F}_{\tdel}(\norm{\cdot}_2)$ where $\tdel(\alpha)=\frac{M}{m_*}\ell(M_*\alpha)$. Now that we have verified the generalized smoothness property of $f$ under the Euclidean norm, we can use the existing tools~\citep[Lemma~3.5]{Li2023GS} to establish the relation between the norm of the gradient and the suboptimality gap of the function. For any $\x\in\X$,
%     \begin{equation}
%         \norm{\nabla f(\x)}_2^2\le2\tdel(2\norm{\nabla f(\x)}_2) (f(\x)-\inf_{\x\in\X}f(\x)).
%     \end{equation}
%     Using the equivalence between different norms and the non-decreasing property of $\ell$, we obtain
%     \begin{equation}
%         \dnorm{\nabla f(\x)}^2\le M_*^2\norm{\nabla f(\x)}_2^2,
%     \end{equation}
%     \begin{equation}
%         \tdel(2\norm{\nabla f(\x)}_2)=\frac{M}{m_*}\ell\left(2M_*\norm{\nabla f(\x)}_2\right)\le\frac{M}{m_*}\ell\brac{2\frac{M_*}{m_*}\dnorm{\nabla f(\x)}}.
%     \end{equation}
%     Combing the above inequalities and using $f(\x)-\inf_{\x\in\X}f(\x)\le F$, we obtain
%     \begin{equation}
%         \dnorm{\nabla f(\x)}^2\le\frac{2MM_*^2}{m_*}\ell\brac{2\frac{M_*}{m_*}\dnorm{\nabla f(\x)}}\cdot F.\label{eq:pl-ineq-modified}
%     \end{equation}
%     Since we have a sub-quadratic $\ell$, we clearly have $\lim_{\alpha\to\infty}\frac{\alpha^2}{\frac{2MM_*^2}{m_*}\ell\brac{2\frac{M_*}{m_*}\alpha}}=\infty$. So for $F\in\R_+,\exists\alpha_F\in\R_{++}$, such that $\forall\alpha>\alpha_F,\frac{\alpha^2}{\frac{2MM_*^2}{m_*}\ell\brac{\frac{M_*}{m_*}\alpha}}>F$. Therefore, if $\frac{\alpha^2}{\frac{2MM_*^2}{m_*}\ell\brac{\frac{M_*}{m_*}\alpha}}\le F$ holds for some $\alpha$, then it must be that $\alpha\le\alpha_F$. Hence, our construction of $G$ implies that $G\le\alpha_F<\infty$. Lastly, we conclude that $\dnorm{\nabla f(\x)}\le G$ by~\eqref{eq:pl-ineq-modified}.
% \end{proof}

We present the following standard lemma for mirror descent, which can be found in~\citet{bubeck2015convex} and~\citet{lan2020first}.
\begin{lem}
    For any $\g\in\mathcal{E}^*,\x_0\in\X$, define $\x^+=\P_{\x_0}(\g)$, then we have $\norm{\x^+-\x_0}\le\dnorm{\g}$.\label{lem:md-stability}
\end{lem}

\begin{proof}
    Using the first-order optimality condition for mirror descent as well as the three-point equation for Bregman functions on the update~\citep{nemirovski2004prox}, we have
    \begin{equation}
        \inner{\g}{\x^+-\x}\le B(\x,\x_0)-B(\x,\x^+)-B(\x^+,\x_0),\quad\forall\x\in\X.\label{eq:md-iter}
    \end{equation}
    Take $\x=\x_0$ and rearrange it,
    \begin{equation}
        \inner{\g}{\x_0-\x^+}\ge B(\x_0,\x^+)+B(\x^+,\x_0).
    \end{equation}
    Using Cauchy-Schwarz Inequality and the property of Bregman divergence, we obtain
    \begin{equation}
    \begin{aligned}
        \dnorm{\g}\norm{\x_0-\x^+}\ge& \inner{\g}{\x_0-\x^+}\\\ge& B(\x_0,\x^+)+B(\x^+,\x_0)\ge \frac{1}{2}\norm{\x_0-\x^+}^2+\frac{1}{2}\norm{\x^+-\x_0}^2
    \end{aligned}       
    \end{equation}
    If $\norm{\x_0-\x^+}=0$,~\cref{lem:md-stability} is naturally true. Otherwise, dividing both sides of the inequality by $\norm{\x_0 - \x^+}$ yields the result.
\end{proof}

\begin{lem}
    Under~\cref{ass:closed-f,ass:bound-domain,ass:sub-quadratic-ell}, for any $\x_0\in\X$ satisfying $f(\x_0)-f^*\le F$, define $G:=\sup\cbrac{\alpha\in\R_{+}|\alpha^2\le2F\ell (2\alpha)}$ and $\x^+=\P_{\x_0}(\eta\nabla f(\x_0))$. If $0<\eta\le1/\ell(2G)$, we have $f(\x^+)\le f(\x_0)$ and $\dnorm{\nabla f(\x^+)}\le G<\infty$.\label{lem:md-trajectory-grad-bound}
\end{lem}

\begin{proof}
    Choose $\g=\eta\nabla f(\x_0)$ and $\x=\x_0$ in~\eqref{eq:md-iter}, we obtain
    \begin{equation}
        \inner{\eta\nabla f(\x_0)}{\x^+-\x_0}\le -B(\x_0,\x^+)-B(\x^+,\x_0).\label{eq:md-step-special}
    \end{equation}
    Under~\cref{ass:closed-f,ass:bound-domain}, we know that $f^*>-\infty$, which gives $f(\x_0)-f^*<\infty$. By~\cref{lem:grad-norm-suboptimality}, we immediately deduce that $\dnorm{\nabla f(\x_0)}\le G<\infty$. Then we invoke~\cref{lem:md-stability} to control the distance between two iterates:
    \begin{equation}
        \norm{\x^+-\x_0}\le\eta\dnorm{\nabla f(\x_0)}\le \frac{\dnorm{\nabla f(\x_0)}}{\ell(2G)}\le \frac{G}{\ell(2G)},
    \end{equation}
    where we use $\eta\le 1/\ell(2G)$. The conditions of~\cref{lem:effective-L-smooth} are now satisfied, so we use the effective smoothness property to obtain
    \begin{equation}
        f(\x^+)\le f(\x_0)+\inner{\nabla f(\x_0)}{\x^+-\x_0}+\frac{\ell(2G)}{2}\norm{\x^+-\x_0}^2.
    \end{equation}
    Multiplying by $\eta$ and adding this to~\eqref{eq:md-step-special}, we have
    \begin{equation}
    \begin{aligned}
        \eta f(\x^+)\le& \eta f(\x_0)+\frac{\eta\ell(2G)}{2}\norm{\x^+-\x_0}^2-B(\x_0,\x^+)-B(\x^+,\x_0)\\\le&\eta f(\x_0)+ \frac{\eta\ell(2G)-1}{2}\norm{\x^+-\x_0}^2-B(\x_0,\x^+)\\\le& \eta f(\x_0),
    \end{aligned}        
    \end{equation}
    where the second inequality uses $\B(\x^+,\x_0)\ge\frac{1}{2}\norm{\x^+-\x_0}^2$ and the last inequality uses $\eta\le 1/\ell(2G)$ and the non-negativity of Bregman divergence. Finally, we obtain the following descent property:
    \begin{equation}
        f(\x^+)-f^*\le f(\x_0)-f^*.\label{eq:descent-property}
    \end{equation}
    Invoking~\cref{lem:grad-norm-suboptimality} again finishes the proof.
\end{proof}


\newpage
\section{Analysis for Accelerated Mirror Descent\label{app:sec:amd}}

The lemmas for this section are stated under the conditions in~\cref{thm:accelerated-mirror-descent}. For simplicity, we do not specify this point below.

\begin{lem}
    Define $e_t:=\norm{\y_t-\x_{t-1}}$ for any $t\in\N_+$, then we have $e_1=e_2=0$. If 
    \begin{equation}
        e_{t-1}\le G/L,\quad\dnorm{\nabla f(\x_{t-2})}\le G\label{eq:et-lem-condition}
    \end{equation}
    hold for any $t\ge2$, we further have
    \begin{equation}
        e_t=\frac{2}{t+1}\norm{\x_{t-1}-\z_{t-1}} \le\frac{(t-2)(\eta(t-1)+t)}{t(t+1)}e_{t-1}+\frac{(t-2)(t-1)\eta G}{t(t+1)L},\quad\forall t\ge 2.
    \end{equation}\label{lem:amd-et}
\end{lem}

\begin{proof}
    By~\eqref{eq:accelerated-mirror-descent}, the following holds for any $t\ge 3$:
    \begin{equation}
        \y_t-\x_{t-1}=\alpha_t(\z_{t-1}-\x_{t-1})=\alpha_t(1-\alpha_{t-1})(\z_{t-1}-\x_{t-2}).
    \end{equation}
    We consider the last term above:
    \begin{equation}
        \begin{aligned}
            \norm{\z_{t-1}-\x_{t-2}}\le \norm{\z_{t-1}-\z_{t-2}}+\norm{\z_{t-2}-\x_{t-2}}\le \eta_{t-1}\dnorm{\nabla f(\y_{t-1})}+\frac{e_{t-1}}{\alpha_{t-1}},
        \end{aligned}
    \end{equation}
    where the last inequality is due to~\eqref{eq:accelerated-mirror-descent} and~\cref{lem:md-stability}. Then we use a similar idea as in~\cref{lem:line-norm-bound} to bound $\dnorm{\nabla f(\y_{t-1})}$:
    \begin{equation}
        \begin{aligned}
            \dnorm{\nabla f(\y_{t-1})}\le \dnorm{\nabla f(\y_{t-1})-\nabla f(\x_{t-2})}+\dnorm{\nabla f(\x_{t-2})}\le Le_{t-1}+G,
        \end{aligned}
    \end{equation}
    where we use~\eqref{eq:et-lem-condition} and~\cref{lem:effective-L-smooth}. Combine the previous results, we obtain
    \begin{equation}
        \begin{aligned}
            e_t\le&\alpha_t(1-\alpha_{t-1})\sqbrac{\eta_{t-1}(Le_{t-1}+G)+\frac{e_{t-1}}{\alpha_{t-1}}}\\\le&\frac{2}{t+1}\cdot\frac{t-2}{t}\sqbrac{\frac{\eta (t-1)G}{2L}+\frac{\eta (t-1)e_{t-1}}{2}+\frac{te_{t-1}}{2}}\\\le&\frac{(t-2)(t-1)}{t(t+1)}\cdot\frac{\eta G}{L}+\frac{(t-2)(\eta(t-1)+t)}{t(t+1)}\cdot e_{t-1}.
        \end{aligned}
    \end{equation}
    For $e_2$, we also have $e_2=2/(2+1)\norm{\x_1-\z_1}=0$ based on~\eqref{eq:accelerated-mirror-descent}. $e_1=0$ is a direct consequence of our initialization.
\end{proof}

\begin{lem}
    If $\x_t,\y_t$ satisfies $\dnorm{\nabla f(\y_t)}\le 2G$ and $\norm{\x_t-\y_t}\le 2G/L$, then for any $\x\in\X$:
    \begin{equation}
        \frac{\eta t(t+1)}{4L}\sqbrac{f(\x_t)-f(\x)}\le \frac{\eta t(t-1)}{4L}\sqbrac{f(\x_{t-1})-f(\x)}+B(\x,\z_{t-1})-B(\x,\z_{t}),\forall t\in\N_+.
    \end{equation}\label{lem:amd-descent}
\end{lem}

\begin{proof}
    In view of~\cref{lem:effective-L-smooth} as well as the given condition, we have
    \begin{equation}
        \begin{aligned}
            f(\x_t)\le& f(\y_t)+\inner{\nabla f(\y_t)}{\x_t-\y_t}+\frac{L}{2}\norm{\x_t-\y_t}^2\\=&(1-\alpha_t)\sqbrac{f(\y_t)+\inner{\nabla f(\y_t)}{\x_{t-1}-\y_t}}\\&+\alpha_t\sqbrac{f(\y_t)+\inner{\nabla f(\y_t)}{\z_t-\y_t}}+\frac{\alpha_t^2L}{2}\norm{\z_t-\z_{t-1}}^2\\\le&(1-\alpha_t)f(\x_{t-1})+\alpha_t\sqbrac{f(\y_t)+\inner{\nabla f(\y_t)}{\z_t-\y_t}+\frac{1}{\eta_t}B(\z_t,\z_{t-1})},
        \end{aligned}
    \end{equation}
    where the first equality is due to $\x_t=(1-\alpha_t)\x_{t-1}+\alpha_t\z_t$ and $\x_t-\y_t=\alpha_t(\z_t-\z_{t-1})$; the first inequality uses the convexity of $f$, the property of Bregman divergence and the fact $\alpha_t L\le\eta_t$ (since $\eta\le 1$). Similar to~\eqref{eq:md-iter}, the following holds for any $\x\in\X$:
    \begin{equation}
        \inner{\nabla f(\y_t)}{\z_t-\x}\le\frac{B(\x,\z_{t-1})-B(\x,\z_t)-B(\z_t,\z_{t-1})}{\eta_t}.
    \end{equation}
    Plugging the above relation into the previous derivations, we obtain
    \begin{equation}
        \begin{aligned}
            f(\x_t)\le&(1-\alpha_t)f(\x_{t-1})+\alpha_t\sqbrac{f(\y_t)+\inner{\nabla f(\y_t)}{\x-\y_t}+\frac{B(\x,\z_{t-1})-B(\x,\z_t)}{\eta_t}}\\\le&(1-\alpha_t)f(\x_{t-1})+\alpha_tf(\x)+\frac{\alpha_t}{\eta_t}\sqbrac{B(\x,\z_{t-1})-B(\x,\z_t)},
        \end{aligned}
    \end{equation}
    where the second step uses the convexity of $f$. A simple rearrangement yields
    \begin{equation}
        \frac{\eta_t}{\alpha_t}\sqbrac{f(\x_t)-f(\x)}\le\frac{(1-\alpha_t)\eta_t}{\alpha_t}\sqbrac{f(\x_{t-1})-f(\x)}+B(\x,\z_{t-1})-B(\x,\z_t).
    \end{equation}
    Specifying the value of $\alpha_t$ and $\eta_t$ completes the proof.
\end{proof}

\begin{lem}\label{lem:amd-induction}
    The trajectory generated by~\eqref{eq:accelerated-mirror-descent} satisfies the following relations for all $t\in\N_+$:
    \begin{enumerate}
        \item (bounded suboptimality gap) $\quad f(\x_t)-f^*\le f(\x_0)-f^*$.
        \item (bounded gradients) $\qquad\qquad\ \dnorm{\nabla f(\x_t)}\le G<\infty,\dnorm{\nabla f(\y_t)}\le 2G<\infty$.
        \item (bounded sequence) $\qquad\qquad\ \  e_t\le G/L$.
    \end{enumerate}
\end{lem}

\begin{proof}
    We prove this lemma by induction.
    \paragraph{Part 1. Base Case} \ \\
    For $t=1$, we clearly have $\y_1=\z_0=\x_0$ and thus $\dnorm{\nabla f(\y_1)}\le G$. Also, we have $\norm{\x_1-\y_1}=\norm{\z_1-\z_0}\le\eta_1\dnorm{\nabla f(\z_0)}\le G/(2L)$. In this scenario, the conditions of~\cref{lem:amd-descent} are satisfied, we immediately obtain
    \begin{equation}
        \frac{\eta }{2L}\sqbrac{f(\x_1)-f(\x_0)}\le -B(\z_0,\z_1)\le 0,
    \end{equation}
    by choosing $\x=\x_0=\z_0$. Then we conclude that $f(\x_1)-f^*\le f(\x_0)-f^*$. By~\cref{lem:grad-norm-suboptimality}, we have $\dnorm{\nabla f(\x_t)}\le G<\infty$. By definition, we have $e_1=0$, which completes the proof for this part.
    \paragraph{Part 2. Induction Step} \ \\ 
    Suppose the statements hold for all $t\le s-1$ where $s\ge 2$. Our goal is to show that they also hold for $t=s$. The main idea is to show the conditions of~\cref{lem:amd-descent} hold via~\cref{lem:amd-et}. Then we establish the descent property under~\cref{lem:amd-descent}, which is the key to the bounded suboptimality gap as well as the bounded gradients. First, we consider the third statement by splitting the timeline.
    
    \textbf{Case (i). $s\ge\tau\ge 4$}
    
    Recall the definition of $\tau$ in~\cref{thm:accelerated-mirror-descent}:
    \begin{equation}
        \tau=\left\lceil\frac{4\sqrt{2}DL}{G}\right\rceil= \left\lceil\frac{8\sqrt{2}D}{\frac{2G}{L}}\right\rceil\ge\left\lceil\frac{8\sqrt{2}D}{2\sqrt{2}D}\right\rceil\ge4.
    \end{equation}
    where we use $2G/L\le 2\sqrt{2}D$~\citep[Proposition~A.6]{yu24egdro}. Then we have
    \begin{equation}
        e_s=\frac{2}{s+1}\norm{\x_{t-1}-\z_{t-1}}\le \frac{2}{\tau}\cdot2\sqrt{2}D\le \frac{4\sqrt{2}D}{\frac{4\sqrt{2}DL}{G}}\le\frac{G}{L}.
    \end{equation}
    % Specifically, if $4\sqrt{2}DL/G\in\N_+$, then we also have $e_{\tau-1}=2/\tau\norm{\x_{t-1}-\z_{t-1}}\le G/L$.
    \textbf{Case (ii). $2\le s\le \tau-1$}

    From the induction basis, we see that the conditions of~\cref{lem:amd-et} are satisfied. Then we have
    \begin{equation}
    \begin{aligned}
        e_s\le&\frac{(s-2)(\eta(s-1)+s)}{s(s+1)}e_{s-1}+\frac{(s-2)(s-1)\eta G}{s(s+1)L}\\\le&\underbrace{\frac{(\tau-3)(\eta(\tau-2)+\tau-1)}{\tau(\tau-1)}}_{q_\tau}\cdot e_{s-1}+\underbrace{\frac{(\tau-3)(\tau-2)\eta }{\tau(\tau-1)}}_{a_\tau}\cdot\frac{G}{L}.
    \end{aligned}        
    \end{equation}
    We aim to bound $e_s$ as a contraction mapping. For the ratio parameter, we have
    \begin{equation}
        q_\tau=\frac{\tau-3}{\tau}+\frac{(\tau-3)(\tau-2)\eta}{\tau(\tau-1)}\le1-\frac{3}{\tau}+\frac{3}{2\tau} <1,
    \end{equation}
    where we use the definition of $\eta$. Then we obtain
    \begin{equation}
        e_s-\frac{a_\tau}{1-q_\tau}\le q_\tau(e_{s-1}-\frac{a_\tau}{1-q_\tau})\le\cdots \le q_\tau^{s-2}(e_2-\frac{a_\tau}{1-q_\tau})\overset{e_2=0}{\le} 0.
    \end{equation}
    Thus,
    \begin{equation}
        \begin{aligned}
            e_s\le \frac{a_\tau}{1-q_\tau}\le\frac{\frac{3}{2\tau}}{\frac{3}{\tau}-\frac{3}{2\tau}}\cdot\frac{G}{L}\le\frac{G}{L}.
        \end{aligned}
    \end{equation}
    Therefore, we have proven the third statement.

    Next, we show that the conditions of~\cref{lem:amd-descent} are satisfied.
    \begin{equation}
        \begin{aligned}
            \dnorm{\nabla f(\y_{s})}\le \dnorm{\nabla f(\y_{s})-\nabla f(\x_{s-1})}+\dnorm{\nabla f(\x_{s-1})}\le Le_{s-1}+G\le 2G,
        \end{aligned}
    \end{equation}
    where we use the induction basis $\dnorm{\nabla f(\x_{s-1})}\le G,e_s\le G/L$ and~\cref{lem:effective-L-smooth}. We also have
    \begin{equation}
        \norm{\x_t-\y_t}=\alpha_t\norm{\z_t-\z_{t-1}}\le\alpha_t\eta_t\dnorm{\nabla f(\y_t)}\le \frac{t\eta G}{(t+1)L}\le \frac{G}{L}.\label{eq:xt-yt-bound}
    \end{equation}
    Then we invoke~\cref{lem:amd-descent} to get the following
    \begin{equation}
        \frac{\eta s(s+1)}{4L}\sqbrac{f(\x_s)-f(\x_0)}\le \frac{\eta s(s-1)}{4L}\sqbrac{f(\x_{s-1})-f(\x_0)}+B(\x_0,\z_{s-1})-B(\x_0,\z_{s}),
    \end{equation}
    which also holds for all $1\le t\le s-1$ by induction. Summing from $t=1$ to $t=s$, we obtain
    \begin{equation}
        \frac{\eta s(s+1)}{4L}\sqbrac{f(\x_s)-f(\x_0)}\le -B(\x_0,\z_s)\le 0,
    \end{equation}
    which yields $f(\x_t)-f^*\le f(\x_0)-f^*$. $\dnorm{\nabla f(\x_t)}\le G<\infty$ is immediately derived using~\cref{lem:grad-norm-suboptimality}. Finally, the three statements are all proven, which completes the proof.
\end{proof}

\newpage

\section{Analysis for Optimistic Mirror Descent\label{app:sec:omd}}

\begin{lem}
    Consider the $t$-th iteration of the optimistic mirror descent algorithm defined in~\eqref{eq:optimistic-mirror-descent}. The following relations hold:
    \begin{equation}
    \begin{aligned}
        \eta(f(\y_t)-f(\x_{t-1}))\le&\eta\inner{\nabla f(\y_t)-\nabla f(\y_{t-1})}{\y_t-\x_{t-1}}\\&-\frac{1}{2}\norm{\x_t-\y_t}^2-\frac{1}{2}\norm{\y_t-\x_{t-1}}^2-\frac{1}{2}\norm{\x_t-\x_{t-1}}^2,
    \end{aligned}        
    \end{equation}
    \begin{equation}
    \begin{aligned}
        \eta(f(\x_t)-f(\x_{t-1}))\le&\eta\inner{\nabla f(\x_t)-\nabla f(\y_{t-1})}{\y_t-\x_{t-1}}\\&+\eta\inner{\nabla f(\x_t)-\nabla f(\y_{t})}{\x_t-\y_t}\\&-\frac{1}{2}\norm{\x_t-\y_t}^2-\frac{1}{2}\norm{\y_t-\x_{t-1}}^2-\frac{1}{2}\norm{\x_t-\x_{t-1}}^2.
    \end{aligned}        
    \end{equation}\label{lem:recursive-function-value-bound}
\end{lem}

\begin{proof}
    By~\cref{lem:mp-algebra}, we have
    \begin{align}
        &\forall\x\in\X,\inner{\eta\nabla f(\y_{t-1})}{\y_t-\x}\le B(\x,\x_{t-1})-B(\x,\y_t)-B(\y_t,\x_{t-1});\label{eq:omd-1st-update}\\
        &\forall\x\in\X,\inner{\eta\nabla f(\y_t)}{\x_t-\x}\le B(\x,\x_{t-1})-B(\x,\x_t)-B(\x_t,\x_{t-1}).\label{eq:omd-2nd-update}
    \end{align}
    Choose $\x=\x_t$ in~\eqref{eq:omd-1st-update}, $\x=\x_{t-1}$ in~\eqref{eq:omd-2nd-update} and then add them together:
    \begin{equation}
    \begin{aligned}
        &\inner{\eta\nabla f(\y_{t-1})}{\y_t-\x_t}+\inner{\eta\nabla f(\y_t)}{\x_t-\x_{t-1}}\\\le& -B(\x_t,\y_t)-B(\y_t,\x_{t-1})-B(\x_{t-1},\x_t).
    \end{aligned}        
    \end{equation}
    By convexity of $f$ and the property of Bregman function, we obtain
    \begin{equation}
        \begin{aligned}
            &\eta(f(\y_t)-f(\x_{t-1}))\\\le&\inner{\eta \nabla f(\y_t)}{\y_t-\x_{t-1}}\\=&\eta\inner{\nabla f(\y_t)-\nabla f(\y_{t-1})}{\y_t-\x_t}+\inner{\eta f(\y_{t-1})}{\y_t-\x_t}+\inner{\eta f(\y_t)}{\x_t-\x_{t-1}}\\\le&\eta\inner{\nabla f(\y_t)-\nabla f(\y_{t-1})}{\y_t-\x_t}-\frac{1}{2}\norm{\x_t-\y_t}^2-\frac{1}{2}\norm{\y_t-\x_{t-1}}^2-\frac{1}{2}\norm{\x_t-\x_{t-1}}^2.
        \end{aligned}
    \end{equation}
    To prove the second bound, we first notice that
    \begin{equation}
        \begin{aligned}
            \inner{\nabla f(\x_t)}{\x_t-\x_{t-1}}=&\underbrace{\inner{\nabla f(\y_{t-1})}{\y_t-\x_{t-1}}}_{A_t}+\underbrace{\inner{\nabla f(\x_t)-\nabla f(\y_{t-1})}{\y_t-\x_{t-1}}}_{B_t}\\&+\underbrace{\inner{\nabla f(\y_{t})}{\x_{t}-\y_{t-1}}}_{C_t}+\underbrace{\inner{\nabla f(\x_t)-\nabla f(\y_{t})}{\x_{t}-\y_{t}}}_{D_t}.
        \end{aligned}
    \end{equation}
    Similar to the proof of~\cref{lem:mp-descent-condition}, we bound the four terms separately. To bound $A_t$ and $C_t$, we choose $\x=\x_{t-1}$ in~\eqref{eq:omd-1st-update}, $\x=\y_{t}$ in~\eqref{eq:omd-2nd-update}. Adding them up, we have
    \begin{equation}
        \begin{aligned}
            \eta(A_t+C_t)=&\inner{\eta\nabla f(\y_{t-1})}{\y_t-\x_{t-1}}+\inner{\eta\nabla f(\y_t)}{\x_t-\y_{t}}\\\le& -B(\x_{t-1},\y_t)-B(\y_t,\x_{t})-B(\x_{t},\x_{t-1}).
        \end{aligned}
    \end{equation}
    By convexity of $f$ and the property of Bregman function, we obtain
    \begin{equation}
    \begin{aligned}
        \eta(f(\x_t)-f(\x_{t-1}))\le&\eta\inner{\nabla f(\x_t)}{\x_t-\x_{t-1}}\\\le &\eta (B_t+ D_t)-\frac{1}{2}\norm{\x_{t-1}-\y_t}^2-\frac{1}{2}\norm{\y_t-\x_{t}}^2-\frac{1}{2}\norm{\x_t-\x_{t-1}}^2,
    \end{aligned}        
    \end{equation}
    which completes the proof by plugging in the definitions of $B_t$ and $D_t$.
\end{proof}

\begin{lem}\label{lem:omd-trajectory-grad-bound}
    Under~\cref{ass:closed-f,ass:bound-domain,ass:sub-quadratic-ell}, consider the updates in~\eqref{eq:optimistic-mirror-descent} and define $G:=\sup\{\alpha\in\R_{+}|\alpha^2\le2(f(\x_0)-f^*)\ell (2\alpha)\}$. If $0<\eta\le\gamma/\ell(2G)$ for some $\gamma\in(0,1/3]$, the following statements holds for all $t\in\N_+$:
    \begin{enumerate}
        \item (bounded suboptimality gap) $\quad f(\y_t)-f^*\le f(\x_0)-f^*,f(\x_t)-f^*\le  f(\x_0)-f^*$;
        \item (bounded gradients) $\quad\quad\quad\quad\ \dnorm{\nabla f(\y_t)}\le G<\infty,\dnorm{\nabla f(\x_t)}\le G<\infty$.
        \item (tracking $\{\y_t\}_{t\in\N}$ sequence) $\quad\norm{\y_t-\y_{t-1}}\le\frac{G}{L}\sum_{s=1}^t\gamma^s\le \frac{\gamma G}{(1-\gamma)L}<\frac{G}{L}$.
    \end{enumerate}   
\end{lem}
% remember to tune the value of eta
\begin{proof}
    The proof resembles that of~\cref{lem:md-trajectory-grad-bound}, which involves induction. For convenience, we denote $L:=\ell(2G)$.
    \paragraph{Part 1. Base Case} \ \\ 
    Consider the first round of~\eqref{eq:optimistic-mirror-descent}. Similar to the proof of~\cref{lem:md-trajectory-grad-bound}, we argue that $f(\y_0)-f^*=f(\x_0)-f^*<\infty$. By~\cref{lem:pl-inequality}, we have $\dnorm{\nabla f(\y_0)}=\dnorm{\nabla f(\y_0)}\le G<\infty$. According to~\cref{lem:recursive-function-value-bound}, we have
    \begin{equation}
    \begin{aligned}
        \eta(f(\y_1)-f(\x_{0}))\le&\eta\inner{\nabla f(\y_1)-\nabla f(\y_{0})}{\y_1-\x_{0}}\\&-\frac{1}{2}\norm{\x_1-\y_1}^2-\frac{1}{2}\norm{\y_1-\x_{0}}^2-\frac{1}{2}\norm{\x_1-\x_{0}}^2.
    \end{aligned}        
    \end{equation}   
    It suffices to control the inner product term. By~\cref{lem:md-stability} as well as our initialization $\x_0=\y_0$, we have
    \begin{equation}
        \norm{\y_1-\y_0}=\norm{\y_1-\x_0}\le\eta\dnorm{\nabla f(\x_0)}\le\frac{\gamma G}{L}\le \frac{G}{L}.
    \end{equation}
    Then we use~\cref{lem:inner-product-smooth-bound} to establish the following upper bound:
    \begin{equation}
        \eta\inner{\nabla f(\y_1)-\nabla f(\y_0)}{\y_1-\x_1}\le\frac{\gamma^2}{2}\norm{\y_1-\y_0}^2+\frac{1}{2}\norm{\y_1-\x_1}^2.
    \end{equation}
    Noticing $\x_0=\y_0$ and $\gamma^2<1$, we obtain
    \begin{equation}
        \eta(f(\y_1)-f(\x_0))\le-\frac{1}{2}\norm{\x_1-\x_0}^2\le 0.
    \end{equation}
    Now we clearly have $f(\y_1)-f^*\le f(\x_0)-f^*$ as $\eta>0$. By~\cref{lem:grad-norm-suboptimality}, we conclude that $\dnorm{\nabla f(\y_1)}\le G$. Next, we prove the other half of the statements. By~\cref{lem:recursive-function-value-bound}, we have
    \begin{equation}
    \begin{aligned}
        \eta(f(\x_1)-f(\x_{0}))\le&\eta\inner{\nabla f(\x_1)-\nabla f(\y_0)}{\y_1-\x_0}+\eta\inner{\nabla f(\x_1)-\nabla f(\y_1)}{\x_1-\y_1}\\&-\frac{1}{2}\norm{\x_0-\y_1}^2-\frac{1}{2}\norm{\y_1-\x_1}^2-\frac{1}{2}\norm{\x_1-\x_0}^2.
    \end{aligned}\label{eq:omd-f-x_1-x_0-bound}  
    \end{equation}
    In the language of~\cref{lem:recursive-function-value-bound}, we shall establish an upper bound for the terms $\eta B_1$ and $\eta D_1$, i.e., the inner product terms in the above inequality. By~\cref{lem:md-stability}, we have
    \begin{equation}
        \norm{\x_1-\y_0}=\norm{\x_1-\x_0}\le\eta\dnorm{\nabla f(\y_1)}\le\frac{\gamma G}{L}\le \frac{G}{L},
    \end{equation}
    and by~\cref{lem:mp-algebra},
    \begin{equation}
        \norm{\x_1-\y_1}\le\eta\dnorm{\nabla f(\y_1)-\nabla f(\y_0)}\le\eta L\norm{\y_1-\y_0}\le\frac{\gamma^2G}{L}\le \frac{G}{L},
    \end{equation}
    where the second inequality uses $\norm{\y_1-\y_0}\le\frac{G}{L}$ and~\cref{lem:effective-L-smooth}. Then we invoke~\cref{lem:inner-product-smooth-bound} to derive the following bounds for $\eta B_1$ (set $\beta=1$) as well as $\eta D_1$ (set $\beta=\frac{1}{2}$):
    \begin{align}
        &\eta B_1=\eta\inner{\nabla f(\x_1)-\nabla f(\y_0)}{\y_1-\x_0}\le \frac{\gamma^2}{2}\norm{\x_1-\y_0}^2+\frac{1}{2}\norm{\y_1-\x_0}^2.\\
        &\eta D_1=\eta\inner{\nabla f(\x_1)-\nabla f(\y_1)}{\x_1-\y_1}\le \frac{1}{2}\norm{\x_1-\y_1}^2.
    \end{align}
    Combine~\eqref{eq:omd-f-x_1-x_0-bound} with the above inequalities, we obtain
    \begin{equation}
    \begin{aligned}
        \eta(f(\x_1)-f(\x_{0}))\le&\frac{\gamma^2}{2}\norm{\x_1-\y_0}^2+\frac{1}{2}\norm{\y_1-\x_0}^2+\frac{1}{2}\norm{\x_1-\y_1}^2\\&-\frac{1}{2}\norm{\x_0-\y_1}^2-\frac{1}{2}\norm{\y_1-\x_1}^2-\frac{1}{2}\norm{\x_1-\x_0}^2\\\le&-\frac{1-\gamma^2}{2}\norm{\x_1-\x_0}^2\overset{\gamma^2<1}{\le} 0,
    \end{aligned}  
    \end{equation}
    where we use the initialization setup of $\x_0=\y_0$. Consequently, we conclude that $f(\x_1)-f^*\le f(\x_0)-f^*$. $\dnorm{\nabla f(\y_1)}\le G$ is straightforward after a standard application of~\cref{lem:grad-norm-suboptimality}.
    \paragraph{Part 2. Induction Step} \ \\ 
    Suppose the function values and the gradients are bounded before the $t$-th iteration. Thus we have $\dnorm{\nabla f(\y_{t-1})}\le G,\dnorm{\nabla f(\x_{t-1})}\le G$ and $\norm{\y_{t-1}-\y_{t-2}}\le\frac{G}{L}\sum_{s=1}^{t-1}\gamma^s\le\frac{\gamma G}{(1-\gamma)L}$. We prove \textbf{statement 3} first, which tracks the amount of change of the sequence $\{\y_t\}_{t\in\N}$.
    \begin{equation}
        \begin{aligned}
            \norm{\y_t-\y_{t-1}}\le& \norm{\y_t-\x_{t-1}}+\norm{\x_{t-1}-\y_{t-1}}\\\le&\eta\dnorm{\nabla f(\y_{t-1})}+\eta\dnorm{\nabla f(\y_{t-1})-\nabla f(\y_{t-2})}\\\le&\eta G+\eta L\norm{\y_{t-1}-\y_{t-2}}\\\le&\frac{\gamma G}{L}+\frac{\gamma G}{L}\sum_{s=1}^{t-1}\gamma^s=\frac{ G}{L}\sum_{s=1}^{t}\gamma^s\le\frac{\gamma G}{(1-\gamma)L},
        \end{aligned}
    \end{equation}
    where the second inequality uses~\cref{lem:mp-algebra} and the third one use~\cref{lem:effective-L-smooth}. Now we examine the rest statements for the $t$-th round of the algorithm. By~\cref{lem:recursive-function-value-bound}, we have 
    \begin{equation}
    \begin{aligned}
        &\eta(f(\x_t)-f(\x_{t-1}))\\\le&\underbrace{\eta\inner{\nabla f(\x_t)-\nabla f(\y_{t-1})}{\y_t-\x_{t-1}}}_{B_t}+\underbrace{\eta\inner{\nabla f(\x_{t})-\nabla f(\y_{t})}{\x_t-\y_{t}}}_{D_t}\\&-\frac{1}{2}\norm{\x_t-\y_t}^2-\frac{1}{2}\norm{\y_t-\x_{t-1}}^2-\frac{1}{2}\norm{\x_t-\x_{t-1}}^2.
    \end{aligned}\label{eq:f(x_t)-f(x_[t-1])}        
    \end{equation}
    To leverage the effective smoothness property, we need to carefully control the distance quantity, namely $\norm{\x_t-\y_t}$ and $\norm{\x_t-\y_{t-1}}$. Based on the bound of $\norm{\y_t-\y_{t-1}}$, by~\cref{lem:mp-algebra,lem:effective-L-smooth} we have
    \begin{equation}
        \norm{\x_t-\y_t}\le \eta\dnorm{\nabla f(\y_{t})-\nabla f(\y_{t-1})}\le\eta L\norm{\y_t-\y_{t-1}}\le \frac{G}{L}\sum_{s=2}^{t+1}\gamma^s\le\frac{\gamma^2 G}{(1-\gamma)L}<\frac{G}{L},
    \end{equation}
    and
    \begin{equation}
        \norm{\x_t-\y_{t-1}}\le\norm{\x_t-\y_t}+\norm{\y_t-\y_{t-1}}\le\frac{(\gamma^2+\gamma) G}{(1-\gamma)L}<\frac{G}{L}.
    \end{equation}
    With these prepared, we invoke~\cref{lem:inner-product-smooth-bound} and choose $\beta=2\gamma^2/(1-2\gamma)$ therein to derive
    \begin{equation}
    \begin{aligned}
        B_t\le&\frac{1-2\gamma}{4}\norm{\x_t-\y_{t-1}}^2+\frac{\gamma^2}{1-2\gamma}\norm{\y_t-\x_{t-1}}^2\\\le&\frac{1-2\gamma}{2}\norm{\x_t-\x_{t-1}}^2+\frac{1-2\gamma}{2}\norm{\x_{t-1}-\y_{t-1}}^2+\frac{1}{2}\norm{\y_t-\x_{t-1}}^2,
    \end{aligned}\label{eq:B_t}
    \end{equation}
    where we use the value range requirement of $\gamma\in(0,1/3]$. Similarly, choosing $\beta=\gamma$ in~\cref{lem:inner-product-smooth-bound} gives
    \begin{equation}
        D_t=\eta\inner{\nabla f(\x_{t})-\nabla f(\y_{t})}{\x_t-\y_{t}}\le\gamma\norm{\x_t-\y_t}^2.\label{eq:D_t}
    \end{equation}
    Adding~\eqref{eq:B_t} and~\eqref{eq:D_t} to~\eqref{eq:f(x_t)-f(x_[t-1])}, we obtain
    \begin{equation}
    \begin{aligned}
        &\eta(f(\x_t)-f(\x_{t-1}))\\\le&\frac{1-2\gamma}{2}\norm{\x_t-\x_{t-1}}^2+\frac{1-2\gamma}{2}\norm{\x_{t-1}-\y_{t-1}}^2+\frac{1}{2}\norm{\y_t-\x_{t-1}}^2+\gamma\norm{\x_t-\y_t}^2\\&-\frac{1}{2}\norm{\x_t-\y_t}^2-\frac{1}{2}\norm{\y_t-\x_{t-1}}^2-\frac{1}{2}\norm{\x_t-\x_{t-1}}^2\\\le&\frac{1-2\gamma}{2}\norm{\x_{t-1}-\y_{t-1}}^2-\frac{1-2\gamma}{2}\norm{\x_{t}-\y_{t}}^2.
    \end{aligned}\label{eq:final-f(x_t)-f(x_[t-1])}       
    \end{equation}
    By induction, it's easy to verify that the above relation holds for all iterations before the current one. Summing them up, we have
    \begin{equation}
    \begin{aligned}
        \eta(f(\x_{t-1})-f(\x_0))&=\sum_{s=1}^{t-1}\eta(f(\x_s)-f(\x_{s-1}))\\&\le\sum_{s=1}^{t-1}\frac{1-2\gamma}{2}\norm{\x_{s-1}-\y_{s-1}}^2-\frac{1-2\gamma}{2}\norm{\x_{s}-\y_{s}}^2\\&\overset{\x_0=\y_0}{=}-\frac{1-2\gamma}{2}\norm{\x_{t-1}-\y_{t-1}}^2.
        % \\\le& \frac{1-2\gamma}{2}\norm{\x_{0}-\y_{0}}^2-\frac{1-2\gamma}{2}\norm{\x_{t-1}-\y_{t-1}}^2\le 0.
    \end{aligned}\label{eq:f(x_[t-1])-f(x_0)}       
    \end{equation}
    Combining~\eqref{eq:final-f(x_t)-f(x_[t-1])} with~\eqref{eq:f(x_[t-1])-f(x_0)}, we obtain the following bound for the suboptimality gap of $\x_t$:
    \begin{equation}
        \eta(f(\x_t)-f(\x_0))\le-\frac{1-2\gamma}{2}\norm{\x_{t}-\y_{t}}^2\le 0.
    \end{equation}
    Obviously, we have $f(\x_t)-f^*\le f(\x_0)-f^*$ as $\eta>0$. Applying~\cref{lem:grad-norm-suboptimality} yields $\dnorm{\nabla f(\x_t)}\le G$.
    It remains to show the suboptimality as well as the gradient bound for $\y_t$. By~\cref{lem:recursive-function-value-bound}, we have
    \begin{equation}
    \begin{aligned}
        &\eta(f(\y_t)-f(\x_{t-1}))\\\le&\underbrace{\eta\inner{\nabla f(\y_t)-\nabla f(\x_{t-1})}{\y_t-\x_{t-1}}}_{E_t}+\underbrace{\eta\inner{\nabla f(\x_{t-1})-\nabla f(\y_{t-1})}{\y_t-\x_{t-1}}}_{F_t}\\&-\frac{1}{2}\norm{\x_t-\y_t}^2-\frac{1}{2}\norm{\y_t-\x_{t-1}}^2-\frac{1}{2}\norm{\x_t-\x_{t-1}}^2.
    \end{aligned}\label{eq:f(y_t)-f(x_[t-1])}        
    \end{equation}
    \begin{sloppypar}
    Before using~\cref{lem:inner-product-smooth-bound} to bound the terms $E_t$ and $F_t$, we need to verify the applicability of~\cref{lem:effective-L-smooth}. \protect{By~\cref{lem:md-stability},} we have
    \end{sloppypar}   
    \begin{equation}
        \norm{\y_t-\x_{t-1}}\le \eta\dnorm{\nabla f(\y_{t-1})}\le\frac{\gamma G}{L}\le \frac{G}{L}. 
    \end{equation}
    Choosing $\beta=\gamma$ in~\cref{lem:inner-product-smooth-bound}, $E_t$ can be bounded as follows:
    \begin{equation}
        E_t=\eta\inner{\nabla f(\y_t)-\nabla f(\x_{t-1})}{\y_t-\x_{t-1}}\le\gamma\norm{\y_t-\x_{t-1}}^2.\label{eq:E_t-bound}
    \end{equation}
    Similar routine should be done for $F_t$. By~\cref{lem:mp-algebra,lem:effective-L-smooth}, we have
    \begin{equation}
    \begin{aligned}
        \norm{\x_{t-1}-\y_{t-1}}\le&\eta\dnorm{\nabla f(\y_{t-1})-\nabla f(\y_{t-2})}\le\eta L\norm{\y_{t-1}-\y_{t-2}}\\\le&\frac{G}{L}\sum_{s=1}^{t-1}\gamma^{s+1}\le\frac{\gamma^2 G}{(1-\gamma)L}<\frac{G}{L}.
    \end{aligned}         
    \end{equation}
    Choosing $\beta=\gamma^2/(1-2\gamma)$ in~\cref{lem:inner-product-smooth-bound}, $F_t$ can be bounded as follows:
    \begin{equation}
        F_t\le\frac{1-2\gamma}{2}\norm{\x_{t-1}-\y_{t-1}}^2+\frac{\gamma^2}{2(1-2\gamma)}\norm{\y_t-\x_{t-1}}^2.\label{eq:F_t-bound}
    \end{equation}
    Adding~\eqref{eq:E_t-bound} and~\eqref{eq:F_t-bound} to~\eqref{eq:f(y_t)-f(x_[t-1])}, we obtain
    \begin{equation}
    \begin{aligned}
        &\eta(f(\y_t)-f(\x_{t-1}))\\\le&\gamma\norm{\y_t-\x_{t-1}}^2+\frac{1-2\gamma}{2}\norm{\x_{t-1}-\y_{t-1}}^2+\frac{\gamma^2}{2(1-2\gamma)}\norm{\y_t-\x_{t-1}}^2\\&-\frac{1}{2}\norm{\x_t-\y_t}^2-\frac{1}{2}\norm{\y_t-\x_{t-1}}^2-\frac{1}{2}\norm{\x_t-\x_{t-1}}^2\\\le&\frac{1-2\gamma}{2}\norm{\x_{t-1}-\y_{t-1}}^2-\frac{1-2\gamma}{2}\norm{\x_t-\y_t}^2,
    \end{aligned}       
    \end{equation}
    where the last inequality is due to $\gamma^2/(1-2\gamma)+2\gamma\le 1$ for any $\gamma\in(0,1/3]$. Adding~\eqref{eq:f(x_[t-1])-f(x_0)} to the above inequality, we have
    \begin{equation}
        \eta(f(\y_t)-f(\x_0))\le-\frac{1-2\gamma}{2}\norm{\x_{t}-\y_{t}}^2\le 0.
    \end{equation}
    We immediately see that $f(\y_t)-f^*\le f(\x_0)-f^*$. Then $\dnorm{\nabla f(\x_t)}\le G$ is a direct result based on~\cref{lem:grad-norm-suboptimality}. Combining \textbf{Part 1} with \textbf{Part 2} finishes the proof.
\end{proof}

\newpage

\section{Analysis for Mirror Prox\label{app:sec:mp}}

The following lemma describes some basics of the main building blocks of the algorithms that require two prox-mapping per iteration~\citep{korpelevich1976extragradient,popov1980modification}. The techniques primarily stem from the seminal work of~\citet{nemirovski2004prox}. For completeness, we provide the proof here. 

\begin{lem}
    For any $\g_1,\g_2\in\mathcal{E}^*$ and any $\x_0\in\X$ satisfying the following algorithmic dynamics:
    \begin{equation}
        \x_1=\P_{\x_0}(\g_1),\quad\x_2=\P_{\x_0}(\g_2),
    \end{equation}
    we have
    \begin{equation}
        \quad\forall\x\in\X,\inner{\g_1}{\x_1-\x}\le B(\x,\x_0)-B(\x,\x_1)-B(\x_1,\x_0),\label{eq:1st-update}
    \end{equation}
    \begin{equation}
        \quad\forall\x\in\X,\inner{\g_2}{\x_2-\x}\le B(\x,\x_0)-B(\x,\x_2)-B(\x_2,\x_0),\label{eq:2nd-update}
    \end{equation}
    \begin{equation}
        \begin{aligned}
            \forall\x\in\X,\inner{\nabla f(\x_1)}{\x_1-\x}\le& B(\x,\x_0)-B(\x,\x_2)-B(\x_2,\x_1)-B(\x_1,\x_0)\\&-\inner{\g_1-\g_2}{\x_1-\x_2}+\inner{\nabla f(\x_1)-\g_2}{\x_1-\x},
        \end{aligned}
    \end{equation}
    and the continuity of the prox-mapping is characterized by\begin{equation}
        \norm{\x_1-\x_2}=\norm{\P_{\x_0}(\g_1)-\P_{\x_0}(\g_2)}\le\dnorm{\g_1-\g_2}.\label{eq:prox-mapping-continuity}
    \end{equation}\label{lem:mp-algebra}
\end{lem}

\begin{proof}
    Since $\psi(\cdot)$ is 1-strongly convex, we can leverage an existing tool~\citep[Lemma~2.1]{nemirovski2004prox}, which exploits the continuity of the operator $\P_{\x}(\cdot)$, to show~\eqref{eq:prox-mapping-continuity}.~\eqref{eq:1st-update} and~\eqref{eq:2nd-update} is immediately proven after using the first-order optimality condition for projection under Bregman divergence~\citep[Lemma~3.1]{nemirovski2004prox} on both updates.     
    Taking $\x=\x_2$ in~\eqref{eq:1st-update} and adding~\eqref{eq:2nd-update}, we obtain
    \begin{equation}
        \inner{\g_2}{\x_2-\x}+\inner{\g_1}{\x_1-\x_2}\le B(\x,\x_0)-B(\x,\x_2)-B(\x_2,\x_1)-B(\x_1,\x_0),\quad\forall\x\in\X.
    \end{equation}
    Rearrange the terms as below:
    \begin{equation}
        \begin{aligned}
            \forall\x\in\X,\inner{\g_2}{\x_1-\x}\le& B(\x,\x_0)-B(\x,\x_2)-B(\x_2,\x_1)-B(\x_1,\x_0)\\&-\inner{\g_1-\g_2}{\x_1-\x_2}.
        \end{aligned}
    \end{equation}
    Adding $\inner{\nabla f(\x_1)-\g_2}{\x_1-\x}$ to both sides of this inequality completes the proof.
\end{proof}

\begin{lem}\label{lem:mp-trajectory-grad-bound}
    Under~\cref{ass:closed-f,ass:bound-domain,ass:sub-quadratic-ell}, consider the updates in~\eqref{eq:mirror-prox}. If $0<\eta\le1/2\ell(2G)$ where $G:=\sup\cbrac{\alpha\in\R_{+}|\alpha^2\le2(f(\x_0)-f^*)\ell (2\alpha)}$, the following statements holds for all $t\in\N_+$:
    \begin{enumerate}
        \item (descent property) $\quad\quad f(\x_t)\le f(\x_{t-1}),f(\y_t)\le  f(\x_{t-1})$;
        \item (bounded gradients) $\quad\dnorm{\nabla f(\y_t)}\le G<\infty,\dnorm{\nabla f(\x_t)}\le G<\infty$.
    \end{enumerate}    
\end{lem}

\begin{proof}
    We prove this lemma by induction. 
    \paragraph{Part 1. Base Case} \ \\ 
    We consider the case where $t=1$. Similar to the proof of~\cref{lem:md-trajectory-grad-bound}, we argue that $f(\y_0)-f^*=f(\x_0)-f^*<\infty$. By~\cref{lem:pl-inequality}, we have $\dnorm{\nabla f(\y_0)}=\dnorm{\nabla f(\y_0)}\le G<\infty$. Now we look at the sequence $\cbrac{\y_t}_{t\in\N}$ that mirror prox maintains, whose dynamics are exactly like mirror descent. For convenience, we denote $\ell(2G)$ by $L$. As $\eta\le1/(2L)\le1/L$, we can apply~\cref{lem:md-trajectory-grad-bound} to conclude that $f(\y_1)\le f(\x_0)=f(\y_0)$ and $\dnorm{\nabla f(\y_1)}\le G$. Next, we apply~\cref{lem:mp-descent-condition} to the first round of the mirror prox algorithm. It's straightforward to verify that the conditions are satisfied for the case where $s=1$. Hence, we have $f(\x_1)\le f(\x_0)$. By~\cref{lem:grad-norm-suboptimality}, we conclude that $\dnorm{\nabla f(\x_1)}\le G$.
    \paragraph{Part 2. Induction Step} \ \\ 
    Suppose the statements hold for all $t\le s-1$ where $s>2$. So we have $\dnorm{\nabla f(\y_{s-1})}\le G$ and $\dnorm{\nabla f(\x_{s-1})}\le G$. We also have $f(\x_{s-1})\le f(\x_{s-2})\le f(\x_{0})$. Now we consider the case where $t=s$. Similar to the base case, we invoke~\cref{lem:md-trajectory-grad-bound} to immediately conclude that $f(\y_s)\le f(\x_{s-1})\le f(\x_0)$ and $\dnorm{\nabla f(\y_s)}\le G$. Next, we prove the other half of the statements. Note that the conditions in~\cref{lem:mp-descent-condition} are all satisfied by the previous derivations. So we invoke~\cref{lem:mp-descent-condition} to deduce that $f(\x_s)\le f(\x_{s-1})$, which immediately implies $f(\x_s)\le f(\x_0)$. Then $\dnorm{\nabla f(\x_s)}\le G$ is a direct result based on~\cref{lem:grad-norm-suboptimality}. Combining \textbf{Part 1} with \textbf{Part 2} finishes the proof.
\end{proof}

\begin{lem}
    Under~\cref{ass:closed-f,ass:bound-domain,ass:sub-quadratic-ell}, consider the following updates for any $s\in\N_+$:
    % \begin{equation}
    % \begin{aligned}
    % &\y_s=\P_{\x_{s-1}}(\eta\nabla f(\x_{s-1})),\\
    % &\x_s=\P_{\x_{s-1}}(\eta\nabla f(\y_s)),
    % \end{aligned}
    \begin{equation}
    \y_s=\P_{\x_{s-1}}(\eta\nabla f(\x_{s-1})),\quad
    \x_s=\P_{\x_{s-1}}(\eta\nabla f(\y_s)),
    \end{equation}
    which satisfies $0<\eta\le 1/2\ell(2G)$ for some $G\in\R_{+}$. If $\dnorm{\nabla f(\x_{s-1})}\le G$ and $\dnorm{\nabla f(\y_s)}\le G$ hold, then we have $f(\x_s)\le f(\x_{s-1})$.\label{lem:mp-descent-condition} 
\end{lem}

\begin{proof}
    Notice that
    \begin{equation}
        \begin{aligned}
            \inner{\nabla f(\x_s)}{\x_s-\x_{s-1}}=&\underbrace{\inner{\nabla f(\y_s)}{\x_s-\y_{s}}}_{A_s}+\underbrace{\inner{\nabla f(\x_s)-\nabla f(\y_s)}{\x_s-\y_{s}}}_{B_s}\\&+\underbrace{\inner{\nabla f(\x_{s-1})}{\y_{s}-\x_{s-1}}}_{C_s}+\underbrace{\inner{\nabla f(\x_s)-\nabla f(\x_{s-1})}{\y_{s}-\x_{s-1}}}_{D_s}.
        \end{aligned}
    \end{equation}
    We bound the four terms separately. $A_s$ and $C_s$ can be bounded by the property of prox-mapping~\citep[Lemma~3.1]{nemirovski2004prox}. Choosing $\x_1=\y_s,\x_0=\x_{s-1},\g_1=\eta\nabla f(\x_{s-1}),\x=\x_{s-1}$ in~\eqref{eq:1st-update}, we obtain
    \begin{equation}
        \eta C_s=\inner{\eta\nabla f(\x_{s-1})}{\y_s-\x_{s-1}}\le -B(\x_{s-1},\y_s)-B(\y_s,\x_{s-1}).
    \end{equation}
    Choosing $\x_2=\x_s,\x_0=\x_{s-1},\g_2=\eta\nabla f(\y_{s}),\x=\y_{s}$ in~\eqref{eq:2nd-update}, we obtain
    \begin{equation}
        \eta A_s=\inner{\eta\nabla f(\y_{s})}{\x_s-\y_{s}}\le B(\y_{s},\x_{s-1})-B(\y_{s},\x_s)-B(\x_s,\x_{s-1}).
    \end{equation}
    Adding them together, we have
    \begin{equation}
    \begin{aligned}
        \eta A_s+\eta C_s\le& -B(\x_{s-1},\y_s)-B(\y_{s},\x_s)-B(\x_s,\x_{s-1})\\\le&-\frac{1}{2}\norm{\x_{s-1}-\y_s}^2-\frac{1}{2}\norm{\y_s-\x_{s}}^2-\frac{1}{2}\norm{\x_s-\x_{s-1}}^2.
    \end{aligned}\label{eq:A_s+C_s}        
    \end{equation}
    For $B_s$ and $D_s$, we shall use the effective smoothness property. For convenience, denote by $L:=\ell(2G)$ as we did in~\cref{lem:effective-L-smooth}. Observe that 
    \begin{equation}
        \norm{\x_s-\x_{s-1}}\le\eta\dnorm{\nabla f(\y_{s})}\le \frac{G}{2L}\le \frac{G}{L}
    \end{equation}
    by~\cref{lem:md-stability} and the given condition. Then we call~\cref{lem:inner-product-smooth-bound} to bound $\eta D_s$ as follows:
    \begin{equation}
        \eta D_s=\eta \inner{\nabla f(\x_s)-\nabla f(\x_{s-1})}{\y_{s}-\x_{s-1}}\le\frac{1}{8}\norm{\x_s-\x_{s-1}}^2+\frac{1}{2}\norm{\y_{s}-\x_{s-1}}^2 \label{eq:D_s} 
    \end{equation}
    As for $B_s$, we still need to verify that the distance between $\x_s$ and $\y_s$ is small enough in order to apply~\cref{lem:effective-L-smooth}. We utilize~\eqref{eq:prox-mapping-continuity} to show that
    \begin{equation}
        \norm{\x_s-\y_s}=\norm{\P_{\x_{s-1}}(\eta\nabla f(\y_s))-\P_{\x_{s-1}}(\eta\nabla f(\x_{s-1}))}\le \eta \dnorm{\nabla f(\y_s)-\nabla f(\x_{s-1})}.
    \end{equation}
    Then by~\cref{lem:md-stability} and $\dnorm{\nabla f(\x_{s-1})}\le G$, it follows
    \begin{equation}
        \norm{\y_s-\x_{s-1}}\le \eta\dnorm{\nabla f(\x_{s-1})}\le\frac{G}{2L}\le\frac{G}{L}.
    \end{equation}
    Using~\cref{lem:effective-L-smooth}, we deduce that
    \begin{equation}
        \norm{\x_s-\y_s}\le \eta \dnorm{\nabla f(\y_s)-\nabla f(\x_{s-1})}\le \eta L\norm{\y_s-\x_{s-1}}\le \frac{G}{4L}\le\frac{G}{L}.
    \end{equation}
    Utilizing~\cref{lem:effective-L-smooth} again along with Cauchy-Schwarz Inequality, we have
    \begin{equation}
    \begin{aligned}
        \eta B_s=&\eta \inner{\nabla f(\x_s)-\nabla f(\y_s)}{\x_s-\y_{s}}\\\le& \eta \dnorm{\nabla f(\x_s)-\nabla f(\y_s)}\norm{\x_s-\y_{s}}\le\eta L\norm{\x_s-\y_{s}}^2\le \frac{1}{2}\norm{\x_s-\y_{s}}^2.
    \end{aligned}\label{eq:B_s}        
    \end{equation}
    Adding up~\eqref{eq:A_s+C_s},~\eqref{eq:D_s} and~\eqref{eq:B_s}, we obtain
    \begin{equation}
    \begin{aligned}
        \eta(A_s+B_s+C_s+D_s)\le&-\frac{1}{2}\norm{\x_{s-1}-\y_s}^2-\frac{1}{2}\norm{\y_s-\x_{s}}^2-\frac{1}{2}\norm{\x_s-\x_{s-1}}^2\\&+\frac{1}{8}\norm{\x_s-\x_{s-1}}^2+\frac{1}{2}\norm{\y_{s}-\x_{s-1}}^2+\frac{1}{2}\norm{\x_s-\y_{s}}^2\\=&-\frac{3}{8}\norm{\x_s-\x_{s-1}}^2\le 0.
    \end{aligned}         
    \end{equation}
    Since $\eta >0$, it directly yields
    \begin{equation}
        f(\x_s)-f(\x_{s-1})\le \inner{\nabla f(\x_s)}{\x_s-\x_{s-1}}=A_s+B_s+C_s+D_s\le 0.
    \end{equation}
\end{proof}
    
\newpage

\section{Omitted Details for Stochastic Convex Optimization}

In this section, we present the technical details for~\cref{sec:smd}, including further discussions regarding~\cref{ass:sigma-noise} in~\cref{app:sec:noise} and omitted proofs for SMD in~\cref{app:sec:smd}.

\subsection{Comparison of Noise Assumptions\label{app:sec:noise}}

Below, we briefly compare~\cref{ass:sigma-noise} with commonly used assumptions in stochastic optimization.

\paragraph{Bounded noise}
When $\sigma(\cdot)$ degenerates to a positive constant,~\cref{ass:sigma-noise} recovers the uniformly bounded noise condition in the seminal work on $(L_0,L_1)$-smoothness~\citep{Zhang2020Why}, and is also commonly adopted in other subfield~\citep{pmlr-v202-koloskova23a,zhang2024gdro,yu24egdro}.

\paragraph{Affine noise}
If $\sigma(\alpha)=\sigma_0+\sigma_1\alpha$,~\cref{ass:sigma-noise} recovers the almost sure $(\sigma_0,\sigma_1)$-affine noise condition in~\citet{liu2023gs,hong2024on}. Its in-expectation version is also widely adopted in the existing literature~\citep{shi2021rmsprop,jin2021nonconvexdro,wang2023adamlowerbound,NeurIPS:2024:Jiang}.

\paragraph{Finite moment} Another popular setting is that the stochastic gradient has bounded $p$-th moment for some $p\in(1,2]$~\citep{pmlr-v202-sadiev23a,ICML:2024:Liu}, which includes the uniformly bounded variance condition~\citep{ghadimi2012optimal,ghadimi2013stochastic,arjevani2023lower} with $p=2$. We underline that the generalized bounded noise is never stronger than this finite moment condition (cf.~\cref{prop:assumption-strength}). 

\paragraph{Sub-Gaussian noise} This condition is strictly stronger than the finite variance condition and is extremely effective in deriving high-probability bounds~\citep{juditsky2011solving,liu2024revisiting}. As demonstrated by~\cref{prop:assumption-strength},~\cref{ass:sigma-noise} can not imply sub-Gaussian noise.

\paragraph{Generalized heavy-tailed noise}
Recently,~\citet{liu2024nonconvex} propose a relaxation of the traditional heavy-tailed noises assumption (e.g., finite moment condition) in the form $\E_{t-1}[\norm{\eps_t}_2^p]\le \sigma_0^p+\sigma_1^p\norm{\nabla f(\x_t)}_2^p$ for some $p\in(1,2],\sigma_0,\sigma_1\in\R_+$. The almost sure version of the generalized heavy-tailed noise corresponds to our formulation with $\sigma(\alpha)=\sigma_0^p+\sigma_1^p\alpha^p$. While~\cref{ass:sigma-noise} and this condition do not imply each other, both can model heavy-tailed noise environments.
\setParDis

In real-world applications, it is common that the noise level is positively correlated with the true signal, as considered in most multiplicative noise models~\citep{sancho1982analytical,lopez2003polarimetric,aubert2008variational,hodgkinson2021multiplicative}.
To this end, we leverage an \emph{arbitrary} non-decreasing polynomial $\sigma(\cdot)$ to capture this relation. We believe this formulation is expressive enough to accommodate diverse stochastic environments where the noise is potentially unbounded~\citep{hwang1986multiplicative,Loh11high,Diakonikolas2023obliviousNoise} or has heavy tails~\citep{NeurIPS:2018:Zhang:A,gurbuzbalaban2021heavy,cutkosky2021high}. 
\setParDis

Lastly, we present a simple proof to demonstrate the relative strength of our generalized bounded noise assumption compared to the finite moment condition. A similar argument is presented in~\citet[Example~A.1]{liu2024nonconvex}, who construct a counterexample that satisfies the generalized heavy-tailed noise condition but does not satisfy the finite moment condition.

\begin{prop}\label{prop:assumption-strength}
    \cref{ass:sigma-noise} does not imply the finite moment condition given below:
    \begin{equation}
        \forall t\in\N,p\in(1,2],\quad\E_{t-1}[\dnorm{\eps_t}^p]<+\infty,
    \end{equation}
    where the expectation $\E_{t-1}$ is conditioned on the past stochasticity $\cbrac{\eps_s}_{s=0}^{t-1}$.
\end{prop}

\begin{proof}
    Consider the special case where
    \begin{equation*}
        \text{(i) }p=2;\ \text{(ii) }\sigma(\alpha)=\sigma_0+\sigma_1\alpha\text{ for some }\sigma_0,\sigma_1>0;\ \text{(iii) }\dnorm{\eps_t}=\sigma(\dnorm{\nabla f(\x_t)})\text{ for all }t\in\N.
    \end{equation*}
    In this case, the finite moment condition degenerates to the finite variance assumption~\citep{ghadimi2012optimal,ghadimi2013stochastic,arjevani2023lower}. It suffices to show that the variance could be unbounded under~\cref{ass:sigma-noise} with equality condition in (iii). We proceed with
    \begin{equation}
    \begin{aligned}
        \E_{t-1}\sqbrac{\dnorm{\eps_t}^2}=& \E_{t-1}\sqbrac{\sigma_0+\sigma_1\dnorm{\nabla f(\x_t)}}^2\\=&\sigma_0^2+2\sigma_0\sigma_1\E_{t-1}\sqbrac{\dnorm{\nabla f(\x_t)}}+\sigma_1^2\E_{t-1}\sqbrac{\dnorm{\nabla f(\x_t)}^2}.
    \end{aligned}        
    \end{equation}
    Intuitively, $\dnorm{\nabla f(\x_t)}$ can not be directly controlled and thus could potentially diverge to infinity. For instance, consider a more extreme case where $f(\x)=\x^2$ and the algorithm outputs $\x_t=t\x_0$. In this way, we have
    \begin{equation}
    \begin{aligned}
        \E_{t-1}\sqbrac{\dnorm{\eps_t}^2}=&\sigma_0^2+4\sigma_0\sigma_1\E_{t-1}\sqbrac{\dnorm{\x_t}}+4\sigma_1^2\E_{t-1}\sqbrac{\dnorm{\x_t}^2}\\=&\sigma_0^2+4t\sigma_0\sigma_1\E_{t-1}\dnorm{\x_0}+4t^2\sigma_1^2\E_{t-1}\dnorm{\x_0}^2\to\infty,\text{ if }t\to\infty,
    \end{aligned}        
    \end{equation}
    which clearly contradicts the finite variance condition. Hence,~\cref{ass:sigma-noise} is not strictly stronger than the finite moment condition.
\end{proof}

\subsection{Analysis for Stochastic Mirror Descent\label{app:sec:smd}}

The lemmas in this subsection hold under the conditions in~\cref{thm:stochastic-mirror-descent}. We do not restate this point for simplicity. First, we present the following two technical lemmas.~\cref{lem:azuma} is the Hoeffding-Azuma inequality for martingales, which is often used as a standard tool to derive the high probability bound.~\cref{lem:last-iter-sequence} is a powerful technique to transform from the convergence behavior from the average-iterate to the last-iterate.

\begin{lem} \label{lem:azuma}
\citep{cesa2006prediction} Let $V_1, V_2,  \ldots$  be a martingale difference sequence with respect to some sequence $X_1, X_2, \ldots$ such that $V_i \in [A_i , A_i + c_i ]$ for some random variable $A_i$, measurable with respect to $X_1, \ldots , X_{i-1}$ and a positive constant $c_i$. If $S_n = \sum_{i=1}^n V_i$, then for any $t > 0$,
\begin{equation}
    \Pr[ S_n > t] \leq \exp \left( -\frac{2t^2}{\sum_{i=1}^n c_i^2} \right).
\end{equation}
\end{lem}

\begin{lem} \label{lem:last-iter-sequence}
    \citep[Lemma~1]{orabona2020last} Let $\cbrac{\eta_s}_{s=1}^t$ be a non-increasing sequence of positive numbers and $\cbrac{q_s}_{s=1}^t$ be a sequence of positive numbers. Then
    \begin{equation}
        \eta_tq_t\le \frac{1}{t}\sum_{s=1}^t\eta_sq_s+\sum_{k=1}^{t-1}\frac{1}{k(k+1)}\sum_{s=t-k+1}^t\eta_s\brac{q_s-q_{t-k}}.
    \end{equation}
\end{lem}

Next, we move on to the dynamics of SMD defined in~\eqref{eq:stochastic-mirror-descent}. For convenience, we define the following quantities, which are frequently used in our analysis.
\begin{equation}
    \begin{aligned}
        \forall t\in\N,\quad G_t:=\dnorm{\nabla f(\x_t)},\ L_t:=\ell(2G_t),\ \sigma_t:=\sigma(G_t)
    \end{aligned}
\end{equation}

\begin{lem}
    If $\eta_{t+1}\le\min\cbrac{1/(2L_t),G_t/(2L_t\sigma_t)}$, then for any $\x\in\X$ we have
    \begin{equation}
        \eta_{t+1}[f(\x_{t+1})-f(\x)]\le B(\x,\x_t)-B(\x,\x_{t+1})+\eta_{t+1}\inner{\eps_t}{\x-\x_{t}}+\eta_{t+1}^2\sigma_t^2.
    \end{equation}
    \label{lem:smd-iter-bound}
\end{lem}

\begin{proof}
    By~\cref{lem:mp-algebra}, we can derive
    % \begin{equation}
    %     \forall\x\in\X,\ \inner{\eta\nabla f(\x_t;\xi_t)}{\x_{t+1}-\x}\le B(\x,\x_t)-B(\x,\x_{t+1})-B(\x_{t+1},\x_t),
    % \end{equation}
    % which holds for any $t\in\N$. Rearrange and take $\x$ to be any $\x_*\in\argmin_{\x\in\X}f(\x)$,
    \begin{equation}
    \begin{aligned}
        \forall\x\in\X,\ \inner{\eta_{t+1}\nabla f(\x_t)}{\x_{t+1}-\x}\le& B(\x,\x_t)-B(\x,\x_{t+1})-B(\x_{t+1},\x_t)\\&+\eta_{t+1}\inner{\nabla f(\x_t)-\g_t}{\x_{t+1}-\x}.
    \end{aligned}\label{eq:smd-basic}     
    \end{equation}
    % where the existence of $\x_*$ can be implied by~\cref{ass:closed-f,ass:bound-domain}. Now let's focus on the iterates before the stopping time $\tau$. By~\eqref{eq:stopping-time}, we know that
    % \begin{equation}
    %     \forall t< \tau:\ f(\x_t)-f^*\le F,\quad \dnorm{\eps_t}\le \frac{2 G}{25\eta L}.
    % \end{equation}
    % It follows immediately from~\cref{lem:grad-norm-suboptimality} that $\dnorm{\nabla f(\x_t)}\le G$. Thus, we can upper bound the distance between two adjacent iterates as the following:
    % Under the condition $\dnorm{\nabla f(\x_t)}\le G$, 
    In view of the step size $\eta_{t+1}\le\min\cbrac{1/(2L_t),G_t/(2L_t\sigma_t)}$, we exploit the generalized smoothness property as follows
    \begin{equation}
    \begin{aligned}
        \norm{\x_{t+1}-\x_t}\le&\eta_{t+1}\dnorm{\g_t}\le \eta_{t+1} (\dnorm{\nabla f(\x_t)}+\dnorm{\eps_t})\\\le&\frac{G_t}{2L_t}+\frac{G_t}{2L_t\sigma_t}\cdot\sigma(\dnorm{\nabla f(\x_t)})\le\frac{G_t}{L_t},
    \end{aligned}
        \label{eq:x_t+1-x_t}
    \end{equation}
    where the first step is due to~\cref{lem:md-stability}; the last step leverages the non-decreasing property of $\sigma(\cdot)$. Now that~\eqref{eq:x_t+1-x_t} holds, we can utilize the local smoothness property in~\cref{lem:effective-L-smooth} and we follow the same steps as in~\eqref{eq:md-iter-quadratic-bound} and~\eqref{eq:md-basic-iter-smooth} to derive
    \begin{equation}
    \inner{\eta_{t+1}\nabla f(\x_t)}{\x_{t+1}-\x_*}\ge \eta_{t+1}(f(\x_{t+1})-f^*)-\frac{\eta_{t+1} L}{2}\norm{\x_{t+1}-\x_t}^2.
    \end{equation}
    Combining the above inequality with~\eqref{eq:smd-basic}, we obtain
    \begin{equation}
    \begin{aligned}
        \eta_{t+1}[f(\x_{t+1})-f(\x)]\le& B(\x,\x_t)-B(\x,\x_{t+1})-B(\x_{t+1},\x_t)+\frac{\eta_{t+1} L}{2}\norm{\x_{t+1}-\x_t}^2\\&+\eta_{t+1}\inner{\eps_t}{\x-\x_{t}}+\eta_{t+1}\inner{\eps_t}{\x_t-\x_{t+1}}\\\le&B(\x,\x_t)-B(\x,\x_{t+1})+\eta_{t+1}\inner{\eps_t}{\x-\x_{t}}-\frac{1}{2}\norm{\x_{t+1}-\x_t}^2\\&+\frac{1}{4}\norm{\x_{t+1}-\x_t}^2+\eta_{t+1}^2\dnorm{\eps_t}^2+\frac{1}{4}\norm{\x_{t+1}-\x_t}^2\\\le&B(\x,\x_t)-B(\x,\x_{t+1})+\eta_{t+1}\inner{\eps_t}{\x-\x_{t}}+\eta_{t+1}^2\sigma_t^2,
    \end{aligned}    
    \end{equation}
    where the second inequality uses~\cref{lem:inner-product-smooth-bound} and $\eta_{t+1} \le 1/(2L_t)$. 
\end{proof}

\begin{lem}\label{lem:smd-trajectory}
    For any $\delta\in(0,1)$, define
    \begin{equation}
        \begin{aligned}
        &\widetilde{F}_t:=30\max\cbrac{1,\frac{\sigma_{s-1}}{G_{s-1}}}L_{t-1}D^2\sqrt{\ln\frac{16}{\delta}}+12D\sigma_{t-1}\sqrt{\ln\frac{16}{\delta}}+4D\sigma_{t-1}\sqrt{\ln\frac{2t^3}{\delta}}\ln t,\\       &\widetilde{G}_t:=\sup\cbrac{\alpha\in\R_{+}|\alpha^2\le2\widetilde{F}_t\ell (2\alpha)},\quad \forall t\in\N_+.
        \end{aligned}
    \end{equation}
    Under~\cref{ass:sub-quadratic-ell,ass:sigma-noise}, with probability at least $1-\delta$, the following holds simultaneously for all $t\in\N_+$:
    \begin{equation}
        f(\x_t)-f^*\le \widetilde{F}_t= \O(1),\ G_t\le\widetilde{G}_t=\O(1),\ L_t=\O(1),\ \sigma_t=\O(1).
    \end{equation}
\end{lem}

\begin{proof}
    We apply~\cref{lem:smd-iter-bound} to get
    \begin{equation}
        \sum_{s=t_1}^{t_2}\eta_{s}\sqbrac{f(\x_s)-f(\x)}\le B(\x,\x_{t_1-1})-B(\x,\x_{t_2})+\sum_{s=t_1-1}^{t_2-1}\brac{\eta_{s+1}\inner{\eps_s}{\x-\x_s}+\eta_{s+1}^2\sigma_s^2},\label{eq:t1-t2}
    \end{equation}
    which holds for any $\x\in\X$ and any indices $t_1\le t_2,t_1,t_2\in\N$. For convenience, we define
     \begin{equation}
         q_s:=f(\x_{s})-f^*,\ V_s:=\eta_{s+1}\inner{\eps_s}{\x_*-\x_{s}},\ U_s^k:=\eta_{s+1}\inner{\eps_s}{\x_{t-k}-\x_{s}}
     \end{equation}
     for all $0\le s\le t$, where $\x_*\in\argmin_{\x\in\X}f(\x)$ (the existence of $\x_*$ can be implied by~\cref{ass:closed-f,ass:bound-domain}). Obviously, we have $q_s\ge 0$ and $\cbrac{V_s}_{s=0}^{t-1},\cbrac{U_s^k}_{s=t-k+1}^{t-1}$ are martingale difference sequences. Then we have
    % Choosing $\x=\x_*\in\argmin_{\x\in\X}f(\x)$, we obtain
    \begin{equation}
        \sum_{s=1}^t\eta_sq_s\le B(\x_*,\x_0)+\sum_{s=0}^{t-1}V_s+\eta_{s+1}^2\sigma_s^2\overset{\eqref{eq:ass:bound-domain}}{\le} D^2+\sum_{s=1}^{t}\eta_s^2\sigma_{s-1}^2+\sum_{s=0}^{t-1}V_s.\label{eq:avr-iter}
    \end{equation}
    The above bound usually relates to the average-iterate. To incorporate~\cref{lem:last-iter-sequence}, we consider the following relation:
    \begin{equation}
    \begin{aligned}
        &\sum_{s=t-k+1}^t\eta_s(q_s-q_{t-k})=\sum_{s=t-k+1}^t\eta_s[f(\x_s)-f(\x_{t-k})]\\\overset{\eqref{eq:t1-t2}}{\le} &B(\x_{t-k},\x_{t-k})-B(\x_{t-k},\x_{t})+\sum_{s=t-k}^{t-1}\brac{\eta_{s+1}\inner{\eps_s}{\x_{t-k}-\x_s}+\eta_{s+1}^2\sigma_s^2}\\\le&\sum_{s=t-k+1}^{t}\eta_{s}^2\sigma_{s-1}^2+\sum_{s=t-k+1}^{t-1}U_s^k.
    \end{aligned}
    \end{equation}
     With these preparations, we invoke~\cref{lem:last-iter-sequence} to deduce
    \begin{equation}
    \begin{aligned}
        \eta_tq_t\le&\frac{1}{t}\sum_{s=1}^t\eta_sq_s+\sum_{k=1}^{t-1}\frac{1}{k(k+1)}\sum_{s=t-k+1}^t\eta_s\brac{q_s-q_{t-k}}\\\le&\frac{D^2}{t}+\frac{1}{t}\sum_{s=1}^{t}\eta_{s}^2\sigma_{s-1}^2+\frac{1}{t}\sum_{s=0}^{t-1}V_s^t\\&+\sum_{k=1}^{t-1}\frac{1}{k(k+1)}\sum_{s=t-k+1}^{t}\eta_{s}^2\sigma_{s-1}^2+\sum_{k=1}^{t-1}\frac{1}{k(k+1)}\sum_{s=t-k+1}^{t-1}U_s^k.
        % \\\le&\frac{D^2}{\eta t}+\eta\sigma_G^2\brac{1+\ln{t}}+\frac{1}{t}\sum_{s=0}^{t-1}V_s^t+\sum_{k=1}^{t-1}\frac{1}{k(k+1)}\sum_{s=t-k}^{t-1}V_s^k,
    \end{aligned}\label{eq:eta_tq_t}
    \end{equation}
    % where we make use of the well-known fact $\sum_{t=t_1}^{t_2}1/t\le\int_{t_1-1}^{t_2}1/t\mathrm{d}t=\ln{[t_2/(t_1-1)]}$.
    % \begin{equation}
    %     A_t=\sum_{k=1}^{t-1}\frac{1}{k(k+1)}\sqrt{\sum_{s=t-k+1}^t\frac{1}{s}}
    % \end{equation}
    Next, we bound the terms in~\eqref{eq:eta_tq_t}. Recalling the definition of $\eta_s\le D/(\sigma_{s-1}\sqrt{s})$, we have
    \begin{equation}
    \begin{aligned}
        \sum_{s=1}^{t}\eta_{s}^2\sigma_{s-1}^2&\le D^2\sum_{s=1}^{t}\frac{1}{s}\le D^2+D^2\int_{1}^t\frac{1}{s}\mathrm{d}s=D^2\brac{1+\ln{t}},\\
        \sum_{s=t-k+1}^{t}\eta_{s}^2\sigma_{s-1}^2&\le D^2\sum_{s=t-k+1}^{t}\frac{1}{s}\le D^2\int_{t-k}^t\frac{1}{s}\mathrm{d}s=D^2\ln\brac{\frac{t}{t-k}}\le \frac{kD^2}{t-k}.
    \end{aligned}\label{eq:sum-eta_s-sigma_s-1^2}     
    \end{equation}
    Then
    \begin{equation}
    \begin{aligned}
        &\sum_{k=1}^{t-1}\frac{1}{k(k+1)}\sum_{s=t-k+1}^{t}\eta_{s}^2\sigma_{s-1}^2\le\sum_{k=1}^{t-1}\frac{D^2}{k(k+1)}\cdot\frac{k+1}{t-k}\\=& \sum_{k=1}^{t-1}\frac{D^2}{kt}+\sum_{k=1}^{t-1}\frac{D^2}{(t-k)t}=\frac{2D^2}{t}\sum_{k=1}^{t-1}\frac{1}{t}\le\frac{2D^2[1+\ln(t-1)]}{t}.
    \end{aligned}
    \end{equation}
    Next, we utilize~\cref{lem:azuma} to bound the sum of the martingale difference sequence. In view of $\norm{\x_1-\x_2}\le 2\sqrt{2}D,\forall\x_1,\x_2\in\X$ (cf.~\citep[Proposition~A.6]{yu24egdro}), we have
    \begin{equation*}
    \begin{aligned}
        &V_s\le \eta_{s+1}\dnorm{\eps_s}\norm{\x_*-\x_s}\le \frac{D}{\sigma_s\sqrt{s+1}}\sigma_s\cdot2\sqrt{2}D=\frac{2\sqrt{2}D^2}{\sqrt{s+1}},\\&U_s^k\le \eta_{s+1}\dnorm{\eps_s}\norm{\x_{t-k}-\x_s}\le \frac{D}{\sigma_s\sqrt{s+1}}\sigma_s\cdot2\sqrt{2}D=\frac{2\sqrt{2}D^2}{\sqrt{s+1}}.
    \end{aligned}
    \end{equation*}
    where we make use of $\eta_{s}\le D/(\sigma_{s-1}\sqrt{s})$. Hence, we invoke~\cref{lem:azuma} to deduce that with probability at least $1-\delta/t$,
    \begin{equation}
        \sum_{s=0}^{t-1}V_s^t\le \sqrt{\frac{1}{2}\sum_{s=0}^{t-1}\brac{\frac{2\sqrt{2}D^2}{\sqrt{s+1}}}^2\ln\frac{1}{\delta}}\le2D^2\sqrt{\brac{1+\ln t}\ln\frac{t}{\delta}}.
    \end{equation}
    Similarly, for a given index $k$, with probability at least $1-\delta/t$,
    \begin{equation}
        \sum_{s=t-k+1}^{t-1}U_s^k\le \sqrt{\frac{1}{2}\sum_{s=t-k+1}^{t-1}\brac{\frac{2\sqrt{2}D^2}{\sqrt{s+1}}}^2\ln\frac{1}{\delta}}\le2D^2\sqrt{\ln\brac{\frac{t}{t-k+1}}\ln\frac{t}{\delta}}.
    \end{equation}
    Taking the union bound over $k\in[t-1]$ and $\cbrac{V_s^t}_{s=0}^{t-1}$, we conclude that with probability at least $1-\delta$,
    \begin{equation}
        \sum_{s=0}^{t-1}V_s^t\le 2D^2\sqrt{\brac{1+\ln t}\ln\frac{t}{\delta}},\ \sum_{s=t-k+1}^{t-1}U_s^k\le2D^2\sqrt{\ln\brac{\frac{t}{t-k+1}}\ln\frac{t}{\delta}},\label{eq:mds-union-bound}
    \end{equation}
    hold simultaneously for all $k\in[t-1]$. We proceed to bound the second term.
    \begin{equation}
    \begin{aligned}
        &\sum_{k=1}^{t-1}\frac{1}{k(k+1)}\sum_{s=t-k+1}^{t-1}U_s^k\\\le& 2D^2\sqrt{\ln\frac{t}{\delta}}\sum_{k=1}^{t-1}\frac{1}{k(k+1)}\cdot\ln\brac{1+\frac{k}{t-k}}\cdot\frac{1}{\sqrt{\ln\brac{\frac{t}{t-k}}}}\\\le&2D^2\sqrt{\ln\frac{t}{\delta}}\underbrace{\sum_{k=1}^{t-1}\frac{1}{k(t-k)}\cdot\frac{1}{\sqrt{\ln\brac{\frac{t}{t-k}}}}}_{\texttt{term-A}}.
    \end{aligned}
    \end{equation}
    Then,
    \begin{equation}
    \begin{aligned}
        \texttt{term-A}=&\sum_{k=1}^{t-1}\frac{1}{kt}\cdot\frac{1}{\sqrt{\ln\brac{\frac{t}{t-k}}}}+\sum_{k=1}^{t-1}\frac{1}{t(t-k)}\cdot\frac{1}{\sqrt{\ln\brac{\frac{t}{t-k}}}}\\=&\frac{1}{t}\sum_{k=1}^{t-1}\frac{1}{k}\brac{\frac{1}{\sqrt{\ln\brac{\frac{t}{t-k}}}}+\frac{1}{\sqrt{\ln\brac{\frac{t}{k}}}}}\\\overset{\eqref{eq:lnfact}}{\le}&\frac{1}{t}\sum_{k=1}^{t-1}\frac{1}{k}\brac{\sqrt{t}+\frac{1}{\sqrt{\ln t}}}\le\frac{1+\ln t}{\sqrt{t}}+\frac{1+\ln t}{t\sqrt{\ln t}}
    \end{aligned}
    \end{equation}
    where we use the relation $x/(1+x)\le \ln(1+x),\forall x>-1$ and the following fact
    \begin{equation}
        \frac{1}{\sqrt{\ln\brac{\frac{t}{t-k}}}}+\frac{1}{\sqrt{\ln\brac{\frac{t}{k}}}}\in\left(\frac{2}{\sqrt{\ln 2}},\frac{1}{\sqrt{\ln\brac{1+\frac{1}{t-1}}}}+\frac{1}{\sqrt{\ln{t}}}\right].\label{eq:lnfact}
    \end{equation}
    Now that each term in~\eqref{eq:eta_tq_t} has been calculated, we combine all these results to obtain
    \begin{equation}
    \begin{aligned}
        \eta_tq_t\le&\frac{D^2}{t}+\frac{D^2(1+\ln t)}{t}+\frac{2D^2[1+\ln(t-1)]}{t}\\&+\frac{2D^2\sqrt{\brac{1+\ln t}\ln\frac{t}{\delta}}}{t}+2D^2\sqrt{\ln\frac{t}{\delta}}\brac{\frac{1+\ln t}{\sqrt{t}}+\frac{1+\ln t}{t\sqrt{\ln t}}}.
    \end{aligned}
    \end{equation}
    which holds with probability at least $1-\delta$. Dividing by $\eta_t$ and taking union bound on $\cbrac{q_s}_{s\in\N_+}$~\citep{ICML:2024:Zhang}, we obtain
    \begin{equation}
    \begin{aligned}
        \forall t\in \N_+,\quad q_t\le&\frac{2L_{t-1}D^2(4+3\ln t)}{t}+\frac{4L_{t-1}D^2\sqrt{\brac{1+\ln t}\ln\frac{2t^3}{\delta}}}{t}\\&+4L_{t-1}D^2\sqrt{\ln\frac{2t^3}{\delta}}\brac{\frac{1+\ln t}{\sqrt{t}}+\frac{1+\ln t}{t\sqrt{\ln t}}}\\&+\frac{D\sigma_{t-1}(4+3\ln t)}{\sqrt{t}}+\frac{2D\sigma_{t-1}\sqrt{\brac{1+\ln t}\ln\frac{2t^3}{\delta}}}{\sqrt{t}}\\&+2D\sigma_{t-1}\sqrt{\ln\frac{2t^3}{\delta}}(1+\ln t)+2D\sigma_{t-1}\sqrt{\ln\frac{2t^3}{\delta}}\frac{1+\ln t}{\sqrt{t\ln t}},
    \end{aligned}\label{eq:q_t-whp}
    \end{equation}
    where we use the well-known fact $\sum_{t=1}^{+\infty}1/t^2=\pi^2/6<2$. Denote by $F_t$ as the RHS of the above inequality. For all $2\le t\le T$, we have
    \begin{equation}
    \begin{aligned}
        F_t\le
        % & L_{t-1}D^2+\frac{D\sigma_{t-1}(4+3\ln 2)}{\sqrt{2}}+2D\sigma_{t-1}\sqrt{\ln\frac{4}{\delta}}\frac{1+\ln 2}{\sqrt{2\ln 2}}+2D\sigma_{t-1}\sqrt{\ln\frac{T^2}{\delta}}(1+\ln T)\\\le&
        30\max\cbrac{1,\frac{\sigma_{s-1}}{G_{s-1}}}L_{t-1}D^2\sqrt{\ln\frac{16}{\delta}}+12D\sigma_{t-1}\sqrt{\ln\frac{16}{\delta}}+4D\sigma_{t-1}\sqrt{\ln\frac{2t^3}{\delta}}\ln t=\widetilde{F}_t.
    \end{aligned}       
    \end{equation}
    We will show $\widetilde{F}_t=\O(1)$ by induction in the sequel. 
    % Define
    % \begin{equation}
    %     \forall s\in[t],\ \widetilde{G}_s:=\sup\cbrac{\alpha\in\R_{+}|\alpha^2\le2F_s\ell (2\alpha)}.
    % \end{equation}
    For the base case, we have $G_0=\dnorm{\nabla f(\x_0)}<\infty$. Hence $L_0,\sigma_0$ are absolute constants. Since the high probability bound~\eqref{eq:q_t-whp} holds, we clearly have $F_1\le \widetilde{F}_1<\infty$ and $G_1\le \widetilde{G}_1<\infty$ by~\cref{lem:grad-norm-suboptimality}. For the induction step, we have $G_{t-1}=\O(1)$ from the induction basis. Then under~\cref{ass:sub-quadratic-ell,ass:sigma-noise},
    \begin{equation}
        L_{t-1}=\ell(2G_{t-1})=\O(1),\ \sigma_{t-1}=\sigma(G_{t-1})=\O(1),
    \end{equation}
    hold with probability at least $1-\delta$, which implies $F_t=\O(1)$. Then we invoke~\cref{lem:grad-norm-suboptimality} again to deduce that $G_{t}=\O(1)$. Combining the base case and the induction step finishes the proof.
    
    % If $G_{t-1}=\O(1)$, then under~\cref{ass:sub-quadratic-ell,ass:sigma-noise},
    % \begin{equation}
    %     L_{t-1}=\ell(2G_{t-1})=\O(1),\ \sigma_{t-1}=\sigma(G_{t-1})=\O(1),
    % \end{equation}
    % hold with probability at least $1-\delta$, which implies $F_t=\O(1)$. Since~\eqref{eq:q_t-whp} holds simultaneously for all $t\in[T]$, we have $G_{t-1}\le \widetilde{G}_{t-1}=\O(1)$.
    
    % From the induction basis $G_{t-1}=\O(1)$ and under~\cref{ass:sub-quadratic-ell,ass:sigma-noise}, we know that with probability at least $1-\delta$, 
    
    % Thus, we can finally conclude that $F_t=\O(1)$. By setting $\widetilde{G}_t:=\sup\cbrac{\alpha\in\R_{+}|\alpha^2\le2F_t\ell (2\alpha)}=\O(1)$, we have $\dnorm{\nabla f(\x_t)}\le \widetilde{G}_t=\O(1)$ by~\cref{lem:grad-norm-suboptimality}.
\end{proof}

% \begin{lem}
    
% \end{lem}

% \begin{proof}
%      We prove this lemma by induction. First, we specify the notations used in this proof. For all $0\le t\le T-1$, we define
%      \begin{equation}
%          q_t:=\max\cbrac{0,f(\x_{t})-f(\x_0)},\ V_t:=\inner{\eps_t}{\x_0-\x_{t}}.
%      \end{equation}
%     \paragraph{Part 1. Base Case} \ \\

%     \paragraph{Part 2. Induction Step} \ \\
%     Suppose the statement holds for all rounds before $t$. Then we have $\dnorm{\nabla f(\x_t)}\le G$. By~\cref{lem:smd-iter-bound}, the following holds for all $0\le s\le t-1$,
%      \begin{equation}
%         \eta[f(\x_{s+1})-f(\x_0)]\le B(\x_0,\x_s)-B(\x_0,\x_{s+1})+\eta\inner{\eps_s}{\x_0-\x_{s}}+\eta^2\sigma_G^2.
%     \end{equation}
% \end{proof}

\end{document}
