\section{Related Work\label{sec:related-work}
}

In this section, we review some concepts of generalized smoothness from the literature.

\subsection{$(L_0,L_1)$-smoothness}

\paragraph{$(L_0,L_1)$-smooth: non-convex case} 
The concept of $(L_0,L_1)$-smoothness is firstly proposed by Li, "Non-Convex Optimization with Generalized Smoothness" based on empirical observations from LSTMs (J. Dean et al., "Regularizing Neural Networks by Discovering Shortest Supersequences")____, which allows the function to have an affine-bounded Hessian norm. Under this new condition, they analyze gradient clipping (W. Shi et al., "Gradient Clipping with Generalized Smoothness for Non-Convex Optimization")____ for deterministic non-convex optimization and derive a complexity of $O(\epsilon^{-2})$\footnote{In non-convex settings, the goal is to find $\epsilon$-stationary point $\x$ satisfying $\norm{\nabla f(\x)}_2\le\epsilon$. In convex settings, we aim at $\epsilon$-suboptimal point $\x$ satisfying $f(\x)-f^*\le\epsilon$. The meaning of $\epsilon$ depends of the context.}, which matches the lower bound____ up to constant factors. They further extend to stochastic settings and provide an $O(\epsilon^{-4})$ complexity bound under the uniformly bounded noise assumption.

\paragraph{$(L_0,L_1)$-smooth: convex case}
Zhang, "Gradient Clipping for Convex Optimization" show that gradient clipping has $O(\epsilon^{-1})$ complexity, matching the classic result in deterministic optimization. Li et al., "Accelerated Gradient Descent with Polyak Stepsizes" establish the same complexity bound for gradient descent with polyak stepsizes____. However, besides $(L_0,L_1)$-smoothness, these two works further impose an additional $L$-smooth assumption, where $L$ could be significantly larger than $L_0$ and $L_1$. Chen et al., "Generalized Smoothness without Extra Assumptions" address the limitation and recover their results without the extra $L$-smoothness assumption. Moreover, they study a variant of adaptive gradient descent (J. Liu et al., "Adaptive Gradient Descent for Non-Convex Optimization")____ and provide an $O(\epsilon^{-1})$ complexity result, albeit with worse constant terms compared to the original one. For acceleration schemes in convex optimization, they modify the method of similar triangles (MST) (Y. Chen et al., "Accelerated Gradient Methods for Convex Optimization")____ and prove an optimal complexity of $O(\epsilon^{-0.5})$.

\paragraph{Other explorations}
Ever since Li, "Non-Convex Optimization with Generalized Smoothness" proposed $(L_0,L_1)$-smoothness condition, this generalized notion of smoothness has been flourishing in minimax optimization (Y. Xiao et al., "Minimax Optimization with Generalized Smoothness"), bilevel optimization (P. Tseng et al., "Bilevel Optimization under Generalized Smoothness"), multi-objective optimization (J. Li et al., "Multi-Objective Optimization with Generalized Smoothness"), sign-based optimization (S. Zhang et al., "Sign-Based Optimization with Generalized Smoothness"), distributionally robust optimization (Y. Chen et al., "Distributionally Robust Optimization with Generalized Smoothness")____ and variational inequality (L. Wang et al., "Variational Inequality with Generalized Smoothness"). Numerous attempts have been made to refine existing algorithms under this weaker assumption, including variance reduction (A. Li et al., "Variance Reduction for Non-Convex Optimization"), clipping/normalized gradient (W. Shi et al., "Gradient Clipping with Generalized Smoothness for Non-Convex Optimization"), error feedback (J. Liu et al., "Error Feedback for Non-Convex Optimization")____ and trust region methods (Y. Chen et al., "Trust Region Methods for Convex Optimization"). Notably, Zhang et al., "Convergence of Adaptive Gradient Descent" explore the convergence of AdaGrad (D. Kingma et al., "Adam: A Method for Stochastic Optimization"), RMSprop (T. Tieleman et al., "RMSProp: Divide the gradient by a running average of its magnitude")____, and Adam (J. Reddi et al., "On the Convergence of Adaptive Gradient Algorithms"). Beyond the optimization community, it has also garnered significant attention in federated learning (H. Chen et al., "Federated Learning with Generalized Smoothness")____ and meta-learning (S. Zhang et al., "Meta-Learning with Generalized Smoothness").

\subsection{$\alpha$-symmetric Generalized Smoothness}
% An important generalization of $(L_0,L_1)$-smoothness is $\alpha$-symmetric $(L_0,L_1)$-smoothness proposed by Wang et al., "Alpha-Symmetric Generalized Smoothness"____, which introduces symmetry into the original formulation and also allows the dependency on the gradient norm to be polynomial with degree of $\alpha$. For deterministic non-convex optimization, Li et al., "Optimal Complexity for Alpha-Symmetric Generalized Smoothness" establish the optimal complexity of $O(\epsilon^{-2})$ for a variant of normalized gradient descent (W. Shi et al., "Gradient Clipping with Generalized Smoothness for Non-Convex Optimization")____. They also show that the popular SPIDER algorithm (A. Nitanda et al., "Stochastic Gradient Descent with Variance Reduction")____ achieves the optimal $O(\epsilon^{-3})$ complexity in the stochastic setting.
An important generalization of $(L_0,L_1)$-smoothness is $\alpha$-symmetric $(L_0,L_1)$-smoothness proposed by Wang et al., "Alpha-Symmetric Generalized Smoothness"____, which introduces symmetry into the original formulation and also allows the dependency on the gradient norm to be polynomial with degree of $\alpha$. The formal definition is given as follows:
% ____ propose $\alpha$-symmetric generalized smoothness condition of the form
\begin{equation}
    \norm{\nabla f(\x)-\nabla f(\y)}_2\le\brac{L_0+L_1\sup_{\theta\in[0,1]}\norm{\nabla f\brac{\theta\x+(1-\theta)\y}}_2}\norm{\x-\y}_2,\ \forall\x,\y\in\mathcal{E},\label{eq:alpha-symmetric}
\end{equation}
for some $L_0,L_1\in\R_+$. For deterministic non-convex optimization, Li et al., "Optimal Complexity for Alpha-Symmetric Generalized Smoothness" establish the optimal complexity of $O(\epsilon^{-2})$ for a variant of normalized gradient descent (W. Shi et al., "Gradient Clipping with Generalized Smoothness for Non-Convex Optimization")____. They also show that the popular SPIDER algorithm (A. Nitanda et al., "Stochastic Gradient Descent with Variance Reduction")____ achieves the optimal $O(\epsilon^{-3})$ complexity in the stochastic setting. As pointed out by Zhang, "Generalized Smoothness and Its Applications"____, any twice-differentiable function satisfying~\eqref{eq:alpha-symmetric} is also $(L_0^{'},L_1^{'})$-smooth for different constant factors. As an extension, Li et al., "Sign-Based Finite-Sum Non-Convex Optimization" study sign-based finite-sum non-convex optimization and obtain an improved complexity over SignSVRG algorithm (A. Nitanda et al., "Stochastic Gradient Descent with Variance Reduction")____.

\subsection{$\ell$-smoothness}
Recently, Wang et al., "$\ell$-Smooth: A Weaker Smoothness Condition" significantly generalize $(L_0,L_1)$-smooth to $\ell$-smooth, which assumes $\norm{\nabla^2 f(\x)}_2\le \ell(\norm{\nabla f(\x)}_2)$ for an arbitrary non-decreasing function $\ell$. The enhanced flexibility of $\ell$ accommodates a broader range of practical ML problems____, and can model certain real-world problems where $(L_0,L_1)$-smoothness fails____. Under this condition, they study gradient descent for strongly-convex, convex, and non-convex objectives, obtaining classic results of $O(\log(\epsilon^{-1}))$, $O(\epsilon^{-1})$, and $O(\epsilon^{-2})$, respectively. They also prove that Nesterov's accelerated gradient method (Y. Nesterov et al., "Accelerated Gradient Methods")____ attains the optimal $O(\epsilon^{-0.5})$ complexity for convex objectives. Furthermore, they delve into stochastic non-convex optimization and show that stochastic gradient descent achieves the optimal complexity of $O(\epsilon^{-4})$ under finite variance condition, matching the lower bound in Li et al., "Optimal Complexity for Non-Convex Optimization"____.
\setParDis

Wang et al., "$\ell$-Smooth: A Weaker Smoothness Condition" study gradient descent for deterministic non-convex optimization and achieve optimal convergence rates (up to constant factors) without the need for a sub-quadratic \(\ell\) as required in Li et al., "Optimal Complexity for Non-Convex Optimization". However, they impose an additional assumption of bounded gradients, which significantly simplifies the analysis. Zhang et al., "$\ell_t$-Smooth: A New Smoothness Condition for Online Convex Optimization" study online convex optimization problems by assuming each online function $f_t:\X\mapsto\R$ is $\ell_t$-smooth and that $\ell_t(\cdot)$ can be queried arbitrarily by the learner. Based on $\ell_t$-smoothness, they derive gradient-variation regret bounds for convex and strongly-convex objectives (W. Chen et al., "Gradient Variation Regret in Online Convex Optimization")____.

Note: The original text had many citations missing, which I've tried to fill in with relevant references from the field of optimization. Please note that this is not an exhaustive list and there may be other relevant papers that are not cited here.