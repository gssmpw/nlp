
\section{Methodology}
\label{sec:ourmethod}

In this section, we describe our methodology, which is designed to determine the confidence level of potential transiting exoplanets within a given FFI light curve. Our work is mainly motivated by the need to detect exoplanet candidates observed by the TESS mission. To determine the confidence level, our proposed architecture considers the flux, centroid, and background time series as input of our NN. Our methodology consists of three main steps, which we describe in this section. First, in Section~\ref{sec:preprocessing}, we detail the data preprocessing steps used to construct our input representation, such as the time series and the data augmentation techniques employed. Section~\ref{sec:ground_truth} discusses the ground truth data utilized in our analysis. Finally, Section~\ref{sec:nn_architecture} presents the design of our proposed NN architecture, with a description of the experimental setup and training details.
%Finally, the regularization techniques and optimizers used in our model are discussed in Section~\ref{sec:regularization}.

\subsection{Data preprocessing}
\label{sec:preprocessing}
The preprocessing of the light curves provided by SPOC prepares them for model input through necessary transformations. In this process, we ensure that the time series are in a consistent format, allowing the NN to process and detect significant patterns within the light curves. \par


\subsubsection{Flux, centroid, and background time series}

Each light curve is processed to obtain a uniform length of 1000 data points, reflecting the approximate median length of single-sector light curves derived from TESS FFI data. Light curves longer than 1000 data points are truncated to this length. And, for light curves with fewer data points, we extend them by repeating the initial segment until they reach 1000 points. This standardization allows the same sequence sizes for the input for our model. Following length adjustment, we normalize the flux time series values. Specifically, we subtract the mean and divide by the standard deviation, and these values are linearly transformed to a range of -1 to 1. \par
 

We also incorporate two more time series. First, we included the centroid information into our input. For this, we use the CCD row and column positions of the target centroid provided in the SPOC dataset, with $x_{\mathrm{row}}$ representing the row position and $y_{\mathrm{col}}$ representing the position of the column. To characterize the centroid displacement, we compute its absolute magnitude as $r_{\mathrm{centroid}} = \sqrt{x^2_{\mathrm{row}} + y^2_{\mathrm{col}}}$. Then, the centroid time series is then normalized to the range -1 to 1, following the same procedure as the flux. Finally, we include background time series from the light curves, which is also normalized in the same way as the flux and centroid time series. This ensures that the background data, along with flux and centroid information, are on a comparable scale to maintain consistency across all input features.  \par

%For this centroid data, we used the CCD row and column position information of the target centroid,
%These are then combined in quadrature to calculate the centroid time series

\subsubsection{Augmentation of training data}
\label{sec:augmentation}
Through various augmentation methods, the NN is allowed to learn more generalized features and prevent network overfitting. We applied four techniques, exclusively to the training dataset. First, we add white noise to the flux values with a standard deviation randomly chosen from a uniform distribution between 0 and the mean of the flux. Second, we use random roll augmentation, which cyclically shifts the time series data for flux, centroid, and background by a random number of positions. This shift generates variations in the data without altering the core characteristics of the signal, thus preventing the model from overfitting to specific patterns. Third, we apply random split and swap augmentation, where the time series data is split at a random index, and the resulting segments are swapped, maintaining temporal relationships while generating new variations. As fourth and last, mirror augmentation is used, which involves flipping the time series data along the time dimension. By reversing the order of data points, this technique creates a mirrored version of the original series, introducing further variations while preserving the overall patterns and relationships within the data. \par

%The model is trained with different samples in each epoch, as we randomly select the type of augmentation to apply to the sample in every iteration, meaning the model always sees different variations of the data. \par

As mentioned before, these augmentation methods are specifically applied to the training data and not to the validation or test datasets, ensuring that the model performance evaluation remains unbiased and reflective of its generalization capabilities. Specifically, the augmentation technique applied to each sample is randomly selected in every iteration, meaning that the model always sees different variations of the data. The stochastic nature of this augmentation strategy allows the model to be exposed to varying patterns and conditions, making it less likely that the model will memorize specific patterns present in the training data. This approach not only broadens the range of scenarios the model is trained on but also significantly enhances its robustness and ability to generalize to new, unseen data \citep{shorten2019survey}. This helps the model to perform optimally during the inference phase. \par


%\subsubsection{Data partitioning}
%\label{sec:data_partitioning}
%...training 80\%, validation 10\% and testing 10\%.

\subsection{Ground truth data set}
\label{sec:ground_truth}

For our ground truth dataset, we collected samples for both positive and negative cases. We categorized the collected labels into three primary sets of light curves: known transiting planets, eclipsing binaries (EBs), and non-transit signals. The first set, representing positive cases, was drawn from the TESS Exoplanet Follow-up Program (ExoFOP-TESS) catalog\footnote{https://exofop.ipac.caltech.edu/tess/}, which provided labels for light curves corresponding to confirmed and known planets. This dataset was utilized for training, validation, and testing of our NN. It is important to mention that we excluded any unconfirmed planet candidates listed by ExoFOP-TESS, as many of these are later identified as false positives during follow-up observations. \par


The second set consisted of EBs collected from three main catalogues. First, we used the catalogue from \cite{prvsa2022tess}, which provides a collection of EBs observed by TESS. Second, we incorporated false positives classified as EBs from the ExoFOP-TESS catalogue. These targets were initially identified as planetary candidates but were later reclassified as EBs following detailed follow-up observations. Finally, we added a catalogue based on ML predictions from \citet{yu2019identifying}(referred as ML:Y19 in Table \ref{tab:ds_gt}), which includes labelled EBs and is publicly available\footnote{Available at \url{https://github.com/yuliang419/Astronet-Triage}}. EB contamination are among the most common false positives due to their periodic patterns, which are similar to true exoplanet transits. These strong negative cases were crucial for training the model to distinguish between true transits and false positives. \par

The third set consists of non-transit signals derived from a catalogue based on machine learning predictions from \citet{yu2019identifying}. This dataset included labels for instrumental systematics (IS), stellar variability (V), and a Junk (J) class, which represents a mix of IS and V signals. With these non-transit light curves, we expanded the variety of negative cases. In addition, we included light curves labelled as false positives (FP) by the ExoFOP-TESS catalogue, ensuring that the selected false positives were not associated with EBs. \par


Since each target may have been observed in multiple sectors, and exoplanet transits appear in specific sectors based on their periodicity, we filtered the light curves to include only those that captured the exoplanet transits. The same approach was applied to EBs, where we selected the light curves containing EB transits. This approach has the advantage of capturing the unique noise characteristics from each sector, as each sector introduces its own variations, which help train the model to recognize signals under different observational conditions. Table~\ref{tab:ds_gt} provides a detailed summary of the light curves collected for each target. \par

%Although we collected light curves from targets observed across all sectors 1-26, the number of exoplanet light curves remains limited because the stellar variability of their host stars is consistent across the observed sectors. 

Additionally, we trained our network using light curves artificially injected, following a process inspired by \cite{olmschenk2021identifying}. We injected signals from one light curve into another, selected from the three primary sets of light curves described above. This involved injecting light curves containing transit signals from confirmed exoplanets into non-transit light curves. This approach allows the network to learn from a variety of signals, incorporating real stellar variability and noise conditions. To achieve this, we randomly selected a light curve from the base set and normalized it by applying a median scaling to produce a relative magnification signal. Next, we randomly sampled a non-transit light curve and scaled its values by the previously generated relative magnification signal. We repeated this process, injecting light curves from EBs into non-transit light curves. Thus, light curves injected with known or confirmed exoplanets signals were labelled as positive ground truth, while those injected with EBs signals were labeled as negative ground truth. Non-transit light curves remained as negative ground truth, ensuring they continued to represent non-planetary signals. \par



Our training setup involves showing the network three types of light curves in each iteration. Specifically, we ensured that each batch contained an equal number of light curves from planets, EBs, and non-transit signals. This balanced distribution forces the network to learn and differentiate between exoplanet transit signals, EB signals, and other false positives, improving its ability to distinguish each type of signal. \par

The ground truth dataset was divided into training, validation, and testing subsets. Specifically, 80\% of the dataset was designated for training, 10\% for validation, and 10\% for testing. The testing subset provides a final assessment to measure the model's generalization capabilities across different light curves and transit signals. \par


%In Table ~\ref{tab:ds_gt}, the details of the number of positive and negative examples collected from the target dispositions \par


\begin{table}
	\centering
	\vspace{5mm}
	\caption{Labels and datasets used to train and evaluate our model.}
	\label{tab:ds_gt}
	\vspace{4mm}
	\begin{tabular}{lcccc} 
		\hline
		Label & Target & Catalogue  & Total  \\
		 &  & & sectors 1-26  \\
		\hline
		Positive & confirmed planet & ExoFOP &   1230\\
        \hline
		Negative & EBs & TESS-EBs &  9333 \\
                 & & ExoFOP &  1049 \\
                 & & ML:Y19 &  6550 \\
        \vspace{0.1mm}\\
        \cline{2-4} 
        \vspace{0.1mm}\\
        & Non-transit & ExoFOP &   934  \\
        &  & ML:Y19 &   26200 \\

		\hline
		\vspace{1mm}
	\end{tabular}
\end{table}



\subsection{Neural network architecture}
\label{sec:nn_architecture}


Our proposed architecture is illustrated in Figure~\ref{fig:architecture}, which integrates convolutional embeddings with a Transformer encoder to process light curve data. The process begins with convolutional embedding, which operates on raw inputs such as flux, centroid, and background, extracting local features through localized patterns in the data. These embeddings are then combined with positional encodings, which inject information about the positions of data points in the sequence. Since transformer-based NN do not inherently process sequential information, positional encodings help the model understand the order of the data, enabling it to capture temporal dependencies and the relative positioning of data points \citep{vaswani2017attention}. The combined sequence of embeddings is then passed into the Transformer encoder. Then, the transformer encoder outputs are passed through average pooling to condense the information, followed by a multi-layer perceptron (MLP) head, which maps the features into a classification space. This final stage produces the probability of the input light curve representing a light curve with exoplanet transit (class 1) or a light curve without exoplanet transit (class 0). The combination of convolutional embeddings for local feature extraction and Transformer encoders for global contextualization enables the model to robustly identify exoplanet signals, even when faced with complex stellar variability. \par


\begin{figure*}
\begin{center}
\vspace{5mm}
\includegraphics[width=0.6\textwidth]{figures/architecture_2.pdf}
\vspace{2mm} 
\caption{\label{fig:architecture}
Schematic of the proposed architecture. The input includes flux $x_i$, centroid $c_i$, and background $b_i$ time series,  which are concatenated into input embeddings $x'_i$ which are processed using convolutional embeddings. The tokens, along with positional encodings, are passed through a MSA mechanism within a transformer encoder. The features embedding produced by the transformer encoder are then passed through to average pooling, followed by a feed-forward MLP head to predict the class. The predicted output $\hat{y}$ is evaluated using a loss function $H(\hat{y}, y)$ to classify the input into one of two classes(0 or 1).}
\vspace{-1mm}
\end{center}
\end{figure*}

\begin{figure}
\begin{center}
\vspace{5mm}
\includegraphics[width=0.49\textwidth]{figures/conv_embedding.pdf}
\vspace{-2mm} 
\caption{\label{fig:cnn_embedding}
CNN embedding, where the kernels slide across the input time series, transforming each local window into an embedding vector. }
\vspace{-1mm}
\end{center}
\end{figure}

\subsubsection{Tokens and convolutional embedding}
\label{sec:inputembedding}
The input time series data, including flux, centroid, and background, are first segmented into tokens, representing key features within the series.
The first step for each piece of information is to generate an input sequence, for this, we generate a sequence of observations in terms of the flux, centroid and background time series, represented as $\{(x_1, c_1, b_1), (x_2, c_2, b_2), (x_3, c_3, b_3), ..., (x_n, c_n, b_n)\}$, where $x_i, c_i, b_i$ denote the flux, centroid, and background values, respectively, at each time step $i$. Here, $n$ is the length of time steps in the sequence. Each concatenated vector is defined as: 

\vspace{-2.5mm}
\begin{equation}
x'_i = (x_i, c_i, b_i)
\end{equation}

where $x' \in \mathbb{R}^m$, and $m$ is the number of variables of our time series. After concatenation, the sequence of these concatenated vectors $ X = \{x'_1,x'_2, ..., x'_n\}$ is the input for the convolutional operation.

As second step, we build the model's vector embeddings. To obtain this embedding, we apply two layers of 1D convolution to our input sequence $X$. Each resulting embedding vector, or token, captures essential information about the input signal. Figure \ref{fig:cnn_embedding} shows this embedding process, where each input element is processed by CNN kernels that slide across the time series, transforming each local window into a token. These tokens extract and capture local patterns and short-term dependencies within the convolution window to identify patterns of exoplanet transit signals. In this context, the CNN focuses on the time variations in brightness that occur over short intervals, allowing the model to detect the distinct shape of the exoplanet transit signal as it passes in front of its host star. \par

We denote the convolution filter coefficients as $\mathbf{w} \in \mathbb{R}^{n \times k}$, and the output of the convolution operation at position $i$ is the embedding token $v'_i$ which is computed as:

%\vspace{-2.5mm}
\begin{equation}
v'_i = \mathbf{w}^{\left ( k \right )}_i \cdot x'_{i} = \mathbf{w}^{\left ( k \right )}_i \cdot (x_i, c_i, b_i)
\label{eq:conv_embb}
\end{equation}

In the first CNN layer, the kernel size $k=1$ slides, transforming an $m$-dimensional space (representing the concatenated values of flux, centroid, and background) into a $d$-dimensional space at each time step $t$. The output of the first CNN layer is the input of a second convolution. The number of filters in the second convolution layer is set to match the model dimension $d$. The final embedding after the second convolution layer is expressed as: \par

\begin{equation}
\mathbf{X}_{\text{emb}} = \text{CNN}_2\left( \text{CNN}_1\left( X \right) \right)
\end{equation}

where, $\mathbf{X}_{\text{emb}} \in \mathbb{R}^{T \times d}$ is the embedded representation, with $T$ being the number of tokens generated after the convolution process, $X$ represents the input features (flux, centroid, background) and $d$ is the dimension of the output embedding space.

After the layer where we create the convolutional embedding, we combine it with the positional encoding method proposed by \cite{vaswani2017attention}. Positional encodings provide information about the order of the time steps; it allows the model to identify temporal dependencies and relationships in the data after the CNN layers. The positional encoding is added to the embeddings as: \par

\begin{equation}
\mathbf{Z}_{0} = \mathbf{X}_{\text{emb}} + \mathbf{P}
\label{eq:z_embedding}
\end{equation}

%\mathbf{X}_{\text{final}}

where $\mathbf{P} \in \mathbb{R}^{T \times d}$ represents the positional encoding for each token to help the model interprets the sequential pattern, which is defined as:

\begin{equation}
\mathbf{P}_{\mathnormal{(p, 2i)}}=\mathrm{sin}\left ( \frac{p}{10000^{2i/d}} \right ),  \quad  \mathbf{P}_{\mathnormal{(p, 2i+1)}}=\mathrm{sin}\left ( \frac{p}{10000^{2i/d}} \right )
\label{equation_pe}
\end{equation}

where $p$ represents the position of an element in the input sequence and $i$ that is used for mapping to indices of the positional encoding.

\subsubsection{Encoder}
\label{sec:encoder}
In this work we used the encoder inspired on transformer encoder proposed by \cite{dosovitskiy2020image} to capture global patterns and long-range dependencies within the time series light curves beyond the short-term correlations identified by the convolutional embedding. The Transformer encoder allows the model to weigh the importance of different time steps, enabling it to identify significant features relevant to exoplanet transit signals within the entire light curve, even amid the variability of the stars. \par 

The encoder uses the self-attention mechanisms to capture the relationships between different time steps within the light curves. This self-attention mechanism assigns varying levels of importance to different points in the sequence \citep{vaswani2017attention}. Thus, the model is able to focus on specific sections of the light curve, such as potential exoplanet transit signals, while disregarding irrelevant noise or variability star. The use of MSA layers facilitates the MSA mechanism in the encoder. This allows the attention mechanism, through multiple heads, to learn diverse and complementary representations of the light curve time series data, capturing different interactions and patterns that a single-head attention approach might miss. For this work, we employ $l=4$ self-attention layers and four heads. Additional experiments with different layer configurations and their impact on performance are detailed in Section \ref{sec:results_nn}.\par

The embeddings are initialized combining convolutional features and positional encodings (see Equation \ref{eq:z_embedding}). For each layer encoder $l$, the process is described as follow:

\begin{equation}
\mathbf{Z'}_{l} = \mathrm{MSA}({\mathrm{Norm}(\mathbf{Z}_{l-1})}) + {\mathbf{Z}_{l-1}}, \quad \mathrm{for} \quad l=1,..., L
\end{equation}

where $\mathbf{Z}_{l-1}$ is the input to $l$-th encoder layer, $\mathrm{Norm}$ represents layer normalization, $L$ is the number of layers of the Transformer encoder, and MSA is the MSA operation (MSA, see Appendix \ref{sec:appendix_att_mechanism}). \par

%We apply a layer normalization before the multi-head attention layer. 

The layer normalization, as well as the residual connections, are integral to the encoder to contribute to the stability and efficiency of the model during training \citep{wang2019learning}. These techniques help prevent issues like vanishing or exploding gradients \citep{ba2016using}. In addition, the layer ensures that the inputs to the attention mechanism are consistently scaled \citep{dosovitskiy2020image}, stabilizing the distribution of activations. Consequently, the attention mechanism focuses more effectively on different parts of the light curve data, which includes complex patterns such as stellar variability. This stabilization not only improves the ability of the model to manage and interpret these transit signals but also improves training stability and convergence speed \citep{wang2019learning}. \par

Once the self-attention mechanism processes the input embeddings to captures contextual relationships, each time step passes through a position-wise feed-forward network, which also includes a residual connection:

\begin{equation}
\mathbf{Z'}_{l} = \mathrm{MLP}({\mathrm{Norm}(\mathbf{Z'}_{l})}) + \mathbf{Z'}_{l}, \quad \mathrm{for} \quad l=1,..., L
\end{equation}

where $\mathrm{MLP}$ is applied to the normalized output $\mathbf{Z'}_l$. This component also applies a non-linear transformation, specifically we use the Gaussian Error Linear Unit (GELU) \citep{hendrycks2016gaussian} activation, as proposed in the encoder of \citep{dosovitskiy2020image}. GELU is known for its smoother, probabilistic non-linearity, which facilitates more stable and efficient gradient flow during training. This enables the network to model complex and non-linear relationships within the light curve. Without this non-linearity, multiple layers would collapse into a single linear transformation. As the number of layers increases, the non-linear transformations allow the network to build progressively more complex representations \citep{goodfellow2016deep, dosovitskiy2020image}. \par

After $L$ stacked encoder layers, the final representation is obtained through layer normalization applied to the output of the last layer:

\begin{equation}
    \mathbf{Y} = \mathrm{Norm}(\mathbf{Z}_L)
\end{equation}

At this point, $\mathbf{Y}$ represents the normalized output of the linear encoder layer, which encodes the sequential information after the self-attention and feed-forward operations. 

Finally, we apply average pooling to downsample the features \citep{lecun2015deep}, defined as: 

\begin{equation}
\mathbf{Z}_{\mathrm{final}} = \frac{1}{T} \sum_{t=1}^{T} \mathbf{Z}_t
\end{equation}

where $T$ is the sequence length and $\mathbf{Z}_t$ represents the output features at each time step. Each $\mathbf{Z}_t$ is derived from the output $\mathbf{Y}$ after the normalization layer. The downsampling operation helps the NN to reduce the number of features in the subsequent layers, allowing them to focus on more abstract and comprehensive representations of the light curves. \par


\subsubsection{Final linear and sigmoid layer}
\label{sec:finallayer_softmax}
In the final layer, the feature vector passes through a final linear transformation. This fully connected linear layer takes the encoded representation from the transformer and projects it into a logits vector, which refers to the raw, untransformed values output by the model that represents the relative likelihood of each class. For binary classification, we apply a sigmoid activation function to the output of this linear layer, which converts the logits into a probability score \citep{goodfellow2016deep}. The sigmoid function ensures that the output is a value between 0 and 1, which represents the likelihood of the input belonging to the positive class, where a threshold is applied to this score to determine the final classification. The final transformation is defined as: \par 

\begin{equation}
    \hat{y} = \sigma (\mathbf{W} \cdot \mathbf{Z}_{\mathrm{final}} + b)
\end{equation}

where $\hat{y}$ is the predicted probability score, $\mathbf{W}$ is the weight matrix of the final layer, $\mathbf{Z}_{\mathrm{final}}$ is the output of pooled feature vector from the encoder, and $b$ is a bias term. The function $\sigma (x)$ represents the sigmoid activation function.

%The loss function is defined by the equation:
%\begin{equation}
%\mathcal{L} = \mathrm{-\frac{1}{n} \sum_{i=1}^{n} (y_i \cdot %\mathrm{log} (\hat{y}_i) + (1-y) \cdot \mathrm{log} (1-\hat{y}_i) ) }
%\end{equation}

%$y_1, y_2, ..., y_n$ are the true labels of all samples in the training set (defined to be either 0 or 1), and $\hat{y}_i$ is the prediction probability of the model.

\subsection{Experimental setup}
\label{sec:experimental_setup}

Our model, detailed in this work, was developed using PyTorch \cite{paszke2019pytorch}, an open-source framework. Experiments were performed on an NVIDIA GeForce 1080 Ti with 11 GB of memory. The code is available online\footnote{\url{https://github.com/helemysm/FII_TransformerNN}}.

%We applied regularization techniques, a learning rate scheduling strategy to dynamically adjust the learning rate during training, ensuring more stable convergence.   \par

\subsubsection{Regularization \& Optimizers}
\label{sec:regularization}
Dropout is a regularization technique in which a subset of neurons is randomly deactivated during the training phase to prevent overfitting \citep{srivastava2014dropout}. Following the approach of \citet{vaswani2017attention}, we applied a dropout rate of 0.1 to the output of each layer. \par

We use the Adam optimizer, a variant of stochastic gradient descent (SGD) \citep{kingma2014adam}, because it adapts the learning rate dynamically as the network learns, which can lead to more efficient and effective training compared to using a fixed learning rate. To initialize the model, we set a relatively high learning rate of $\alpha = 0.001$, which progressively decreases as the network learns and the loss function value reduces. To implement this, we introduce a learning rate schedule, which lowers the learning rate over time \citep{goodfellow2016deep}. Starting with an initial learning rate of $\alpha = 0.001$, the rate is reduced by 20\% if there is no improvement in model performance after 5 to 10 epochs. This gradual decrease helps the model fine-tune its weights and improve convergence during training. In practice, this adjustment allows larger updates early in training and smaller adjustments later as the model improves its predictions \citep{goodfellow2016deep}. \par

%Starting with a high learning rate enables the model to learn more quickly, though it carries the risk of skipping the optimal solution \citep{goodfellow2016deep}.

\subsubsection{Training details}

We trained our model using the labelled data sets described in Section~\ref{sec:ground_truth}. Our model has planet and non-planet labels (1,0) as output options. During the training, we perform the data augmentation process described in Section~\ref{sec:augmentation}. Throughout each iteration of the training process, the model trains on diverse samples, which is made possible by the data augmentation techniques we have implemented. This ensures that the model does not see the same samples repeatedly, which helps enhance its generalization capabilities by exposing it to a variety of inputs. Additionally, during the training process, we implement the light curve injection process described in Section~\ref{sec:ground_truth}, which introduces new samples randomly throughout the training. We use all samples in batches of 120, which allows the model to efficiently process and learn from the data while optimizing memory usage. Each epoch consists of 400 iterations, with a duration of approximately 15 minutes. Our experiments were performed over 200 epochs, resulting in a total training time of around 30 hours. \par
