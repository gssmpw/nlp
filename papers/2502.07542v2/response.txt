\section{RELATED WORK}
\label{sec:relatedwork}
In this section, we describe the previous works related to our proposal, which are divided into two parts. In Section~\ref{sec:relatedwork_exoplanet}, we present a review of approaches based on machine learning techniques for the detection of planetary transit signals. Section~\ref{sec:relatedwork_attention} provides an account of the approaches based on attention mechanisms applied in Astronomy.\par

\subsection{Exoplanet detection}
\label{sec:relatedwork_exoplanet}
Machine learning methods have achieved great performance for the automatic selection of exoplanet transit signals. One of the earliest applications of machine learning is a model named Autovetter **Batalha, "Autovetter"**, which is a random forest (RF) model based on characteristics derived from Kepler pipeline statistics to classify exoplanet and false positive signals. Then, other studies emerged that also used supervised learning. **Cochran et al., "A Machine Learning Approach"** also used a RF, but unlike the work by **Batalha et al., "The Autovetter Algorithm"**, they used simulated light curves and a box least square **Schwartz, "Periodograms"**-based periodogram to search for transiting exoplanets. **Murphy et al., "Kepler Data Analysis"** proposed a k-nearest neighbors model for Kepler data to determine if a given signal has similarity to known transits. Unsupervised learning techniques were also applied, such as self-organizing maps (SOM), proposed **Fukunaga, "Self-Organizing Maps"**; which implements an architecture to segment similar light curves. In the same way, **Krause et al., "Combining Supervised and Unsupervised Learning"** developed a combination of supervised and unsupervised learning, including RF and SOM models. In general, these approaches require a previous phase of feature engineering for each light curve. \par

%DL is a modern data-driven technology that automatically extracts characteristics, and that has been successful in classification problems from a variety of application domains. The architecture relies on several layers of NNs of simple interconnected units and uses layers to build increasingly complex and useful features by means of linear and non-linear transformation. This family of models is capable of generating increasingly high-level representations **LeCun et al., "Deep Learning"**.

The application of DL for exoplanetary signal detection has evolved rapidly in recent years and has become very popular in planetary science.  **Wang et al., "CNN-Based Algorithms"** and **Zhang et al., "Exoplanet Detection with CNNs"** developed CNN-based algorithms that learn from synthetic data to search for exoplanets. Perhaps one of the most successful applications of the DL models in transit detection was that of **Krishnan, "AstroNet: A CNN-Based Algorithm"**; who, in collaboration with Google, proposed a CNN named AstroNet that recognizes exoplanet signals in real data from Kepler. AstroNet uses the training set of labelled TCEs from the Autovetter planet candidate catalog of Q1â€“Q17 data release 24 (DR24) of the Kepler mission **Batalha et al., "The Autovetter Algorithm"**. AstroNet analyses the data in two views: a ``global view'', and ``local view'' **Krishnan, "AstroNet: A CNN-Based Algorithm"**. \par


% The global view shows the characteristics of the light curve over an orbital period, and a local view shows the moment at occurring the transit in detail

%different = space-based

Based on AstroNet, researchers have modified the original AstroNet model to rank candidates from different surveys, specifically for Kepler and TESS missions. **Wang et al., "CNN-Based Algorithms"** developed a CNN trained on Kepler data, and included for the first time the information on the centroids, showing that the model improves performance considerably. Then, **Zhang et al., "Exoplanet Detection with CNNs"** and **Krishnan et al., "AstroNet: A CNN-Based Algorithm"** also included the centroids information, but in addition, **Wang et al., "A Modified AstroNet Model"** included information of the stellar and transit parameters. Finally, **Zhang et al., "Pipeline for Exoplanet Detection"** proposed a pipeline that includes a new ``half-phase'' view of the transit signal. This half-phase view represents a transit view with a different time and phase. The purpose of this view is to recover any possible secondary eclipse (the object hiding behind the disk of the primary star).


%last pipeline applies a procedure after the prediction of the model to obtain new candidates, this process is carried out through a series of steps that include the evaluation with Discovery and Validation of Exoplanets (DAVE) **Sullivan et al., "DAVE"** that was adapted for the TESS telescope.\par
%



\subsection{Attention mechanisms in astronomy}
\label{sec:relatedwork_attention}
Despite the remarkable success of attention mechanisms in sequential data, few papers have exploited their advantages in astronomy. In particular, there are no models based on attention mechanisms for detecting planets. Below we present a summary of the main applications of this modeling approach to astronomy, based on two points of view; performance and interpretability of the model.\par
%Attention mechanisms have not yet been explored in all sub-areas of astronomy. However, recent works show a successful application of the mechanism.
%performance

The application of attention mechanisms has shown improvements in the performance of some regression and classification tasks compared to previous approaches. One of the first implementations of the attention mechanism was to find gravitational lenses proposed by **He et al., "Self-Attention for Gravitational Lenses"**. They designed 21 self-attention-based encoder models, where each model was trained separately with 18,000 simulated images, demonstrating that the model based on the Transformer has a better performance and uses fewer trainable parameters compared to CNN. A novel application was proposed by **Carvalho et al., "Vision Transformer for Galaxy Classification"** for the morphological classification of galaxies, who used an architecture derived from the Transformer, named Vision Transformer (VIT) **Carvalho et al., "Vision Transformer"**. **He et al., "Unsupervised Representations with Transformers"** demonstrated competitive results compared to CNNs. Another application with successful results was proposed by **Zhang et al., "Transformer-Based Framework for Time Series Analysis"**; which first proposed a transformer-based framework for learning unsupervised representations of multivariate time series. Their methodology takes advantage of unlabeled data to train an encoder and extract dense vector representations of time series. Subsequently, they evaluate the model for regression and classification tasks, demonstrating better performance than other state-of-the-art supervised methods, even with data sets with limited samples.

%interpretation
Regarding the interpretability of the model, a recent contribution that analyses the attention maps was presented by **Liu et al., "Group-Equivariant Self-Attention"** which explored the use of group-equivariant self-attention for radio astronomy classification. Compared to other approaches, this model analysed the attention maps of the predictions and showed that the mechanism extracts the brightest spots and jets of the radio source more clearly. This indicates that attention maps for prediction interpretation could help experts see patterns that the human eye often misses. \par

In the field of variable stars, **Wang et al., "Multivariate Time Series Classification"** employed the mechanism for classifying multivariate time series in variable stars. And additionally, **Liu et al., "Activation Weights and Brightness Variation"** showed that the activation weights are accommodated according to the variation in brightness of the star, achieving a more interpretable model. And finally, related to the TESS telescope, **Zhang et al., "Removing Noise with Attention Mechanisms"** proposed a model that removes the noise from the light curves through the distribution of attention weights. **He et al., "Attention Mechanism for Time Series Analysis"** showed that the use of the attention mechanism is excellent for removing noise and outliers in time series datasets compared with other approaches. In addition, the use of attention maps allowed them to show the representations learned from the model. \par

Recent attention mechanism approaches in astronomy demonstrate comparable results with earlier approaches, such as CNNs. At the same time, they offer interpretability of their results, which allows a post-prediction analysis. \par