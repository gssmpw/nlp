In this section, we describe the Multi-Head Self-Attention (MSA) mechanism used in the encoder. 

Given an input sequence of embeddings $\mathbf{Z}_{l-1}$ where $T$ is the sequence length and $d$ is the dimensionality of the embeddings, the self-attention mechanism computes a set of attention scores. It is computed using three vectors, the queries $\mathbf{Q}$, keys $\mathbf{K}$, and values $\mathbf{V}$. Vectors $Q$ and $K$ are compared to calculate similarities and obtain scores/weights for vector $V$. To compute these vectors as three different subspace representations, the input embeddings is projected with learned weight matrices defined as:

\begin{equation}
\begin{split}
\mathbf{Q=Z}_{l-1}\mathbf{W^{\mathnormal{Q}} \in \mathbb{R}^{\mathnormal{n \times d_q}}},\\
\mathbf{K=Z}_{l-1}\mathbf{W^{\mathnormal{K}} \in \mathbb{R}^{\mathnormal{n \times d_k}}},\\
\mathbf{V=Z}_{l-1}\mathbf{W^{\mathnormal{V}} \in \mathbb{R}^{\mathnormal{n \times d_v}}}.
\label{eq:attention_qkv}
\end{split}
\end{equation}

where $\mathbf{W}^{\mathnormal{Q}}$, $\mathbf{W}^{\mathnormal{K}}$, $\mathbf{W}^{\mathnormal{V}}$ are the learned weight matrices, and their dimensionalities are $d \times d_k$, with $d_k$ represents the dimensions of the queries and keys. 

Next, we calculate the attention scores using the scaled dot-product, self-attention matrix $\mathbf{A} \in \mathbb{R}^{n \times d_v}$ can be expressed as:

\begin{equation}
\mathbf{A}=\mathrm{Attention\mathbf{(Q,K,V)} = softmax \left ( \frac{\mathbf{QK}^\top}{\sqrt{d_k}} \right ) \mathbf{V}}
\label{eq:scaled-dot-product}
\end{equation}

The attention layer takes the input in the form of those three parameters: queries $\mathbf{Q}$, keys $\mathbf{K}$ and values $\mathbf{V}$. These three parameters have similar structure, where each element in the sequence is represented by a vector. The query, key, and value vectors are derived from the input embeddings and they are linearly projected $h$ times, allowing their computations to be repeated multiple times in parallel \citep{vaswani2017attention}. The outputs of these individual heads are concatenated and passed through a linear projection to create the final output defined by:

\begin{equation}
\label{multihead_eq}
\mathrm{MSA}(\mathbf{Z}_{l-1}) = \mathrm{MultiHead\mathbf{(Q,K,V)} = 
\mathbf{W}^O \begin{bmatrix}
 h_1\\
.\\
.\\
.\\
 h_h
\end{bmatrix}
}
\end{equation}

with learned output weights $\mathbf{W}^O \in \mathbb{R} ^{hd_v\times d} $, and each attention head $h_i$ computes the attention as follows:

\begin{equation}
    h_i = \mathrm{Attention({\mathbf{Q}}_i, {\mathbf{K}}_i,{\mathbf{V}}_i) }
\end{equation}

The resulting multi-head attention output is then passed through a residual connection and layer normalization:

\begin{equation}
\mathbf{Z'}_{l} = \mathrm{MSA}({\mathrm{Norm}(\mathbf{Z}_{l-1})}) + {\mathbf{Z}_{l-1}}, \quad \mathrm{for} \quad l=1,..., L
\end{equation}

%Then, those results are concatenated and we obtain the final values, enabling the model to focus on different parts of the input sequence in parallel.
