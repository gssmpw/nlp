\section{Background Theory of Transformer Encoder}
\label{sec:background}

%The Transformer Encoder \cite{vaswani2017attention}, originally designed for NLP, has been adapted for various tasks.
% due to its efficiency and scalability compared to traditional recurrent architectures. 

Recent developments in DL have introduced new approaches to processing sequential data. Among these, the Transformer model proposed by \cite{vaswani2017attention} has recently demonstrated significant success in capturing long-range dependencies compared to previous approaches focused on sequential data \citep{hawkins2004problem, lakew2018comparison, karita2019comparative}.  Initially developed for Natural Language Processing, it has since been adapted to various domains, including time-series analysis. In NLP, data typically consists of discrete tokens, such as words or characters, arranged in a sequence to form sentences or documents. In astronomy, by contrast, the data often comes in the form of continuous measurements, such as light curves that represent stellar brightness over time. (In the present time-series context, for example, the term range refers to the length of the time baseline between separate elements within the data.) Both are sequential data, where the order of elements within the sequence is essential to uncovering patterns and relationships. \par

The original Transformer model consists of an encoder and a decoder \citep{vaswani2017attention}. In this work, we focus on the encoder. The encoder processes the input sequence to generate a representation, which captures the essential patterns and features of the data. \par

The core of the Transformer encoder is a mechanism called self-attention, which builds upon the original attention mechanism proposed by \cite{bahdanau2014neural}. Self-attention enables the NN to ``attend'' to all parts of the input sequence simultaneously. This mechanism allows the NN to evaluate the importance of each element in the sequence in relation to every other element, weighing how much each data point should contribute to the final representation. It enables the NN to capture both local and long-range dependencies. Unlike traditional methods that focus on short-range dependencies, self-attention allows the Transformer to understand the full temporal structure of the data \citep{lakew2018comparison, karita2019comparative}. \par


Figure \ref{fig:self-attention} shows the core of the Transformer encoder, and provides a detailed view of the self-attention mechanism. The input sequence is represented as a series of data points, denoted as $(x_1, x_2, \ldots, x_n)$, where $n$ is the length of the sequence. This input is first encoded into a continuous vector space through the embedding layer, and each $x_i$ is represented as a vector $x_{\mathrm{emb}_i} \in \mathbb{R}^d$, where $d$ is the embedding dimension. Vector embedding is a way to represent data symbols (such as words, observations, or images) in a continuous, transformed space. Embeddings can be numeric representations or generated through NNs. As shown in Figure \ref{fig:self-attention}(a), this representation is then combined with a positional encoding, which incorporates information about the order of the data points, capturing their relationships in the temporal dimension \citep{vaswani2017attention}, such as patterns and relationships that depend on the temporal order of the data, such as periodic trends or sequential dependencies. The positional encoding is essential because the Transformer architecture, unlike RNN or CNN, does not have an inherent way to understand the order of elements in a sequence \citep{gehring2017convolutional}. This combined representation is then passed to the encoder. Within the encoder, the self-attention mechanism captures both local and global dependencies, enabling the extraction of complex features and patterns.  \par

The self-attention mechanism used within the encoder to process the input sequence, in this case a light curve. The self-attention process involves three components: queries ${Q}$, keys ${K}$ and values ${V}$. As shown in Figure \ref{fig:self-attention}(b), each element in the sequence generates a query, which asks how much attention it should pay to the other elements. The query is compared to keys, which represent the context, and an attention score is calculated. The score is calculated using the scaled dot-product of ${Q}$ and the transpose of ${K}$. These values are then passed through a softmax function to convert the values into a probability distribution that sums to 1, while sharpening the distribution. This score determines how much focus each element should give to others. \par

Finally, ${V}$, weighted by the attention scores, are then combined to produce the final output of the self-attention feature map. This feature map encapsulates the relationships across the sequence and contributes to the encoded representation $z$. In the case of multihead self-attention (MSA), multiple attention mechanisms are applied in parallel, capturing different aspects of the relationships across the sequence (see the Appendix \ref{sec:appendix_att_mechanism} for more details). The decoder then uses this encoded representation $z$ to produce outputs such as probability estimates for the presence of planetary transit signals into a light curve. This approach enables the model to detect exoplanet transit signals directly from the complete light curve without prior transit parameters even if only one transit is exhibited during the entire observation sequence.  \par



\begin{figure*}
\begin{center}
\vspace{5mm}
\includegraphics[width=0.87\textwidth]{figures/transformer_encoder.pdf}
\caption{\label{fig:self-attention}
(a) the transformer encoder, which processes time series inputs using positional encodings combined with input embeddings, and computes feature representations through a self-attention mechanism. (b) explains the self-attention mechanism, where the $Q$, $K$ matrices are used to calculate attention scores. These scores are then applied to the $V$ to produce the self-attention feature map.}
\vspace{-1mm}
\end{center}
\end{figure*}


\if 0
\begin{figure}
\begin{center}
\vspace{5mm}
\includegraphics[width=0.33\textwidth]{figures/encoder.pdf}
\caption{\label{fig:encoder}
Structure of the transformer encoder process. The input sequence is first transformed into a vector representation by the embedding layer of neurons that reduces the dimensionality of the input data sequence. This reduced, vectorized data is subsequently combined with a positional encoding to incorporate information about the temporal order of the sequence. The encoder, which incorporates a multi-headed self-attention mechanism, described in detail in Appendix \ref{sec:appendix_att_mechanism},  subsequently processes the combined representation to capture contextual relationships and generate the feature representations. }
\vspace{-1mm}
\end{center}
\end{figure}

\fi

%The encoder then processes this sequence of embeddings to generate a encoded representation $z$ which captures the patterns within the light curve. 

%These embeddings capture the characteristics of the input data in a vector space that is ready for processing by the encoder.

Transformers have shown significant potential for encoding time series data in astronomy \citep[e.g.][]{allam2021paying, zerveas2021transformer, morvan2022don}. In the field of exoplanet science, \cite{10.1093/mnras/stad1173} implemented a Transformer model specifically tailored to distinguish exoplanet transit signals from false positives, highlighting the capability of the architecture to identify and interpret the characteristics of exoplanet signals. In the context of exoplanet transit signal detection, the ability of the Transformer to model long-range dependencies is highly beneficial, as it can identify signal patterns associated with stellar variability that might otherwise be mistaken for false positives. This allows the model to accurately distinguish true planetary transits within the entire light curve, even amid the variability of the stars.  \par

