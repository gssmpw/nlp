\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% \usepackage[font=footnotesize,labelfont=bf]{caption}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{subcaption}

% \usepackage{algpseudocode}
\usepackage{graphicx} % For including tables
\usepackage{booktabs} % For professional table formatting
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{xcolor,tcolorbox}
\usepackage{amsmath}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{English Please: Evaluating Machine Translation for Multilingual Bug Reports}

% \title{Conference Paper Title*\\
% {\footnotesize \textsuperscript{*}Note: Sub-titles are not captured in Xplore and
% should not be used}
% \thanks{Identify applicable funding agency here. If none, delete this.}
% }

% Google Cloud Translation API
% Microsoft Azure Translator
% DeepL API
% Amazon Translate
% IBM Watson Language Translator

\author{\IEEEauthorblockN{1\textsuperscript{st} Avinash Patil}
\IEEEauthorblockA{
\textit{Juniper Networks Inc.}\\
Sunnyvale, USA \\
patila@juniper.net \\ ORCID: 0009-0002-6004-370X}
\and
\IEEEauthorblockN{2\textsuperscript{nd} Aryan Jadon}
\IEEEauthorblockA{
\textit{Juniper Networks Inc.}\\
Sunnyvale, USA \\
aryanj@juniper.net \\
ORCID: 0000-0002-2991-9913}
}

% \author{\IEEEauthorblockN{1\textsuperscript{st} Avinash Patil}
% \IEEEauthorblockA{
% \textit{Juniper Networks Inc.}\\
% Bengaluru, IND \\
% patila@juniper.net \\ ORCID: 0009-0002-6004-370X}
% \and
% \IEEEauthorblockN{2\textsuperscript{nd} Aryan Jadon}
% \IEEEauthorblockA{
% \textit{Juniper Networks Inc.}\\
% Bengaluru, IND \\
% aryanj@juniper.net \\
% ORCID: 0000-0002-2991-9913}
% }


\maketitle

\begin{abstract}
Accurate translation of bug reports is critical for efficient collaboration in global software development. In this study, we conduct the first comprehensive evaluation of machine translation (MT) performance on bug reports, analyzing the capabilities of DeepL, AWS Translate, and ChatGPT using data from the Visual Studio Code GitHub repository, specifically focusing on reports labeled with the english-please tag. To thoroughly assess the accuracy and effectiveness of each system, we employ multiple machine translation metrics, including BLEU, BERTScore, COMET, METEOR, and ROUGE. Our findings indicate that DeepL consistently outperforms the other systems across most automatic metrics, demonstrating strong lexical and semantic alignment. AWS Translate performs competitively, particularly in METEOR, while ChatGPT lags in key metrics. This study underscores the importance of domain adaptation for translating technical texts and offers guidance for integrating automated translation into bug-triaging workflows. Moreover, our results establish a foundation for future research to refine machine translation solutions for specialized engineering contexts. The code and dataset for this paper are available at GitHub -\url{https://github.com/av9ash/English-Please}.
\newline
\end{abstract}

\begin{IEEEkeywords}
Multilingual Bug Reports, Machine Translation (MT), ChatGPT, AWS Translate, DeepL, Visual Studio Code, english-please, BLEU, BERT Score, COMET, METEOR, ROUGE, Software Development, Domain-Specific Translation, Technical Text Localization, Large Language Models
\end{IEEEkeywords}

\section{Introduction}
As global software development increasingly depends on multilingual collaboration, the accurate translation of technical documents has become a crucial challenge. Among these, bug reports present unique difficulties due to using domain-specific jargon, software version references, and embedded code snippets—elements that often challenge generic machine translation (MT) systems~\cite{chu2018survey}. While recent advances in neural MT have significantly improved translation fluency and accuracy~\cite{bahdanau2014neural, Vaswani2017}, their effectiveness on specialized technical texts, such as software bug reports, remains an open question.

This study examines bug reports from the official Visual Studio Code (VS Code) GitHub repository~\cite{VSCodeRepo}, specifically those labeled \textit{english-please}. This tag indicates that the original report was submitted in a language other than English and requires translation for broader accessibility. Ensuring accurate translations of these reports is essential—minor errors or misinterpretations can obscure the nature and severity of a bug, hindering efficient debugging and software maintenance~\cite{arnaoudova2016linguistic}.

We evaluate the translation performance of three leading MT systems: ChatGPT, AWS Translate, and DeepL. Each system translates non-English bug reports into English. We assess their outputs using five established quality metrics: \textit{BLEU}~\cite{papineni2002bleu}, \textit{BERTScore}~\cite{zhang2019bertscore}, \textit{COMET}~\cite{rei2020comet}, \textit{METEOR}~\cite{banerjee2005meteor}, and \textit{ROUGE}~\cite{Lin2004}. 

By leveraging these diverse metrics, we aim to understand translation quality comprehensively. Specifically, this paper addresses the following research questions:

\begin{itemize}
    \item \textbf{RQ1:} Which system most accurately identifies the source language?
    \item \textbf{RQ2:} How do ChatGPT, AWS Translate, and DeepL compare in preserving the technical precision of VS Code bug reports?
\end{itemize}

By answering these questions, we provide insights into the strengths and limitations of modern MT systems in software development contexts. Our findings will help researchers and practitioners optimize the integration of automated translation into bug-triaging workflows.

The remainder of this paper is structured as follows: Section~\ref{sec:related_work} contextualizes our work within the existing literature, Section~\ref{sec:data} describes our dataset and preprocessing approach, Section~\ref{sec:methodology} details our experimental methodology, Section~\ref{sec:results} presents results and analysis, Section~\ref{sec:discussion} discusses implications and limitations, and Section~\ref{sec:conclusion} concludes with final remarks and future directions.

\section{Related Work}
\label{sec:related_work}
Research on machine translation (MT) has advanced considerably over the past two decades, transitioning from rule-based and statistical paradigms to sophisticated neural approaches. Early neural models introduced by Bahdanau \emph{et al.}~\cite{bahdanau2014neural} demonstrated how attention mechanisms improve translation by aligning source and target words at each decoding step. Subsequent architectures, notably the Transformer~\cite{Vaswani2017}, showcased even more significant gains through multi-head attention and parallelizable layers. These innovations have substantially improved translation quality for general-domain text.

Despite the progress in neural MT, domain adaptation remains a critical challenge. Chu and Wang~\cite{Chu2018} provide a comprehensive survey of strategies for adapting neural models to specialized domains. They note that while generic training corpora enhance fluency and coverage, specialized domains—such as software engineering—require customized approaches to handle domain-specific lexicons and contexts effectively. Domain adaptation also affects translation evaluation, as metrics calibrated on general text may not accurately reflect correctness in specialized scenarios~\cite{Koehn2010}.

Within software engineering, bug reports constitute a unique category of text that often includes code snippets, API references, stack traces, and project-specific jargon. For instance, Arnaoudova \emph{et al.}~\cite{arnaoudova2016linguistic} highlights the linguistic complexity of software artifacts, emphasizing how ambiguities or errors in terminology can hinder comprehension and resolution of software issues.

Evaluating MT quality for these specialized texts necessitates both classic and modern metrics. BLEU~\cite{papineni2002bleu}, METEOR~\cite{banerjee2005meteor}, and ROUGE~\cite{Lin2004} have been widely adopted for general-purpose translation and summarization tasks. However, more recent metrics such as BERTScore~\cite{zhang2019bertscore} and COMET~\cite{rei2020comet} offer improved semantic sensitivity by leveraging transformer-based models for embedding and contextual analysis. These modern metrics may be particularly beneficial in software engineering contexts, where preserving the precise meaning of technical details is essential for successful bug resolution.

Finally, while industry-driven solutions such as AWS Translate, DeepL, and ChatGPT are commonly employed for multilingual documentation and user support, published comparative analyses specific to software bug reports remain limited. Existing studies typically focus on localization scenarios or general test suites that may not capture the nuanced requirements of bug triaging and software maintenance. As such, a deeper examination of how these systems handle authentic bug reports is necessary and timely. Our work aims to fill this gap by systematically evaluating these three MT solutions on bug reports from the Visual Studio Code repository, offering insights into their strengths and limitations in real-world software engineering.


\section{Dataset}
\label{sec:data}

The dataset used in this study comprises bug reports extracted from the Visual Studio Code GitHub repository~\cite{VSCodeRepo}, specifically focusing on those labeled with the \textit{english-please} tag. This label indicates that the original submission was written in a language other than English, providing a clear signal for multilingual content. We collected issues spanning five years (March 2019--June 2024), as represented in \textbf{Figure~\ref{fig:br_ot}}, to ensure a sufficient variety of bug types, user environments, and technical contexts.

To facilitate a reliable evaluation, we applied several filtering steps:

\begin{enumerate}
    \item \textbf{Issue Validity:} We excluded duplicates and issue threads lacking a clear bug description or reproduction steps. Furthermore, \textbf{Figure~\ref{fig:top20l}} illustrates the top 20 labels in the dataset.
    
    \item \textbf{Language Scope:} Our focus was on non-English reports to align with the \textit{english-please} label, ensuring the dataset authentically represented multilingual content. Additionally, \textbf{Table~\ref{tab:lang_distribution}} provides a detailed breakdown of the language distribution of bug reports, highlighting the linguistic diversity within our dataset.

    \item \textbf{Content Cleaning:} We removed sensitive user data, standardized code snippets, and retained only the textual elements necessary for translating bug descriptions, error messages, and steps to reproduce.
\end{enumerate}

Each curated issue was paired with a reference English translation verified by one or more bilingual contributors. This yielded a final corpus of 1,300 multilingual bug reports, reflecting a broad spectrum of functional, UI, and performance-related issues. Bug reports can vary substantially in their level of technical detail and specialized terminology. Our goal was to capture this diversity to provide a robust test set for evaluating the translation performance of ChatGPT, AWS Translate, and DeepL.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.48\textwidth]{br_ot.png}
    \caption{Bug reports over time.}
    \label{fig:br_ot}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.48\textwidth]{t20_labels.png}
    \caption{Top 20 most common labels in bug reports.}
    \label{fig:top20l}
\end{figure}

\begin{table}[htbp]
\caption{Language distribution of bug reports.}
\label{tab:lang_distribution}
\centering
\renewcommand{\arraystretch}{1.2} % Adjust row height for readability
\begin{tabular}{l r}
\hline
\textbf{Language} & \textbf{Number of Reports} \\
\hline
\multicolumn{2}{l}{\textbf{Most common languages}} \\
Chinese (Simplified) (zh-CN) & 637 \\
Portuguese (pt) & 151 \\
English (en) & 141 \\
Japanese (ja) & 76 \\
Russian (ru) & 75 \\
\hline
\multicolumn{2}{l}{\textbf{Less frequent languages (10--66)}} \\
\multicolumn{2}{p{6cm}}{Spanish (es), French (fr), Korean (ko), Traditional Chinese (zh-TW), German (de), Italian (it), Turkish (tr)} \\
\hline
\multicolumn{2}{l}{\textbf{Rare languages (\textless5)}} \\
\multicolumn{2}{p{6cm}}{Persian (fa), Ukrainian (uk), Polish (pl), Czech (cs), Arabic (ar), Indonesian (id), Serbian (sr)} \\
\hline
\end{tabular}
\end{table}

\section{Methodology}
\label{sec:methodology}

This section describes the experimental design used to compare the performance of ChatGPT, AWS Translate, and DeepL on the multilingual bug reports presented in Section~2. Our methodology encompasses the data collection and preparation pipeline, the translation process for each system, and the evaluation metrics employed to gauge translation quality. 

% Figure~\ref{fig:method-overview} provides a high-level workflow overview.

\subsection{Experimental Setup}
We followed a systematic process aligned with prior MT evaluation studies~\cite{Chu2018,Koehn2010} to ensure reproducibility and reliability:

\begin{enumerate}
    \item \textbf{Preprocessing:} Consistent formatting is crucial when comparing outputs from different MT engines~\cite{arnaoudova2016linguistic}. We standardized the bug report structure by removing extraneous metadata (e.g., GitHub labels and user mentions) and retaining only text relevant to problem descriptions, error messages, and reproduction steps.
    
    \item \textbf{Annotation Protocol:} Each bug report in the test set was accompanied by a reference English translation validated by bilingual reviewers. These reference translations serve as ground truth for our evaluation metrics~\cite{Koehn2010}.
\end{enumerate}

\subsection{Translation Systems}
\label{sec:translation-systems}

We evaluated three MT services:

\begin{enumerate}
    \item \textbf{ChatGPT:} We accessed ChatGPT through its API, ensuring uniform prompts to elicit translations. Recent neural architectures~\cite{Vaswani2017} underpin large language models like ChatGPT, enabling it to handle various linguistic phenomena. However, ChatGPT’s internal model updates can lead to temporal variations in performance.
    
    \item \textbf{AWS Translate:} This commercial solution offers a ready-to-use platform for general-domain translations. While AWS Translate has demonstrated strong performance on conventional text, it may face domain adaptation challenges for bug-report content~\cite{Chu2018}.
    
    \item \textbf{DeepL:} DeepL’s core engine leverages Transformer-based methods and is reputed for its robust handling of European languages. Nevertheless, the translation of domain-specific terms (e.g., method names, code snippets) remains uncertain in specialized contexts~\cite{arnaoudova2016linguistic}.
\end{enumerate}

We submitted each of the 1,300 bug reports to these three systems via their respective APIs. To maintain fairness, we preserved all formatting (code blocks, bullet points), and no system received additional context beyond the raw bug description.


\subsection{Evaluation Metrics}
\label{sec:eval_metrics}

Following best practices in MT research, we employed five automatic metrics to capture different facets of translation quality:

\begin{itemize}
    \item \textbf{BLEU}~\cite{papineni2002bleu}: A precision-oriented metric that measures $n$-gram overlaps between the system translation and the reference. Despite its popularity, BLEU can sometimes undervalue semantic coherence in specialized domains~\cite{Koehn2010}.

    \vspace{6pt}
    \noindent
    \textit{Formulation.}
    BLEU computes modified $n$-gram precision and multiplies it by a brevity penalty (BP) to discourage overly short hypotheses. Given a candidate translation $C$ and one or more references $R$:
    \begin{align}
        p_n &= \frac{\sum_{\text{n-gram}\in C} \min\bigl(\text{Count}_{C}(\text{n-gram}), \text{Count}_{R}(\text{n-gram})\bigr)}
                    {\sum_{\text{n-gram}\in C} \text{Count}_{C}(\text{n-gram})}, \\[6pt]
        \mathrm{BP} &= 
        \begin{cases}
            1, & \text{if } c > r,\\
            e^{1 - \tfrac{r}{c}}, & \text{if } c \le r,
        \end{cases}
    \end{align}
    where $c$ is the length of the candidate and $r$ is the effective reference length. The overall BLEU (often BLEU-4) is then:
    \begin{equation}
        \mathrm{BLEU} \;=\;
        \mathrm{BP} \;\times\; \exp\Bigl(\sum_{n=1}^{N} w_{n} \log p_n\Bigr),
    \end{equation}
    with $w_{n}$ typically set to $\tfrac{1}{N}$ and $N=4$.

    \item \textbf{METEOR}~\cite{banerjee2005meteor}: Improves upon BLEU by considering synonyms, stemming, and partial matches, thus often correlating better with human judgment.

    \vspace{6pt}
    \noindent
    \textit{Formulation.}
    METEOR first aligns unigrams from candidate and reference (accounting for stems, synonyms, and exact matches), then computes:
    \begin{align}
        P &= \frac{\text{\# of matched unigrams}}{\text{\# of unigrams in candidate}}, \\[6pt]
        R &= \frac{\text{\# of matched unigrams}}{\text{\# of unigrams in reference}}, \\[6pt]
        F_{\alpha} &= \frac{P R}{\alpha P + (1-\alpha)R}.
    \end{align}
    where $\alpha$ is often set to 0.9. A fragmentation penalty $\mathrm{Penalty}$, based on how the matched chunks are distributed, is applied:
    \begin{equation}
        \mathrm{Penalty} = \gamma \left(\frac{C}{M}\right)^\beta,
    \end{equation}
    where $C$ is the number of contiguous matched chunks, $M$ is the total number of matches, and $\gamma,\beta$ are hyperparameters (commonly $\gamma=0.5$, $\beta=3$). The final METEOR score is:
    \begin{equation}
        \mathrm{METEOR} = F_{\alpha}\, \bigl(1 - \mathrm{Penalty}\bigr).
    \end{equation}

    \item \textbf{ROUGE}~\cite{Lin2004}: Originally designed for summarization tasks, ROUGE assesses recall by tracking overlapping $n$-grams and sequences. Applied here, it highlights how comprehensively a translation covers the reference content.

    \vspace{6pt}
    \noindent
    \textit{Formulation (ROUGE-N).}
    The most common variant, ROUGE-N, measures the $n$-gram overlap. For a reference $R$ and candidate $C$:
    \newline
    
    \begin{equation}
        \begin{aligned}
            % \mathrm{ROUGE}\text{-}N &= \\
            &\frac{
                \sum\limits_{\text{n-gram} \,\in\, R} 
                \min\bigl(\mathrm{Count}_{C}(\text{n-gram}), 
                          \mathrm{Count}_{R}(\text{n-gram})\bigr)
            }
            {
                \sum\limits_{\text{n-gram} \,\in\, R} 
                \mathrm{Count}_{R}(\text{n-gram})
            }
        \end{aligned}
    \end{equation}

    This yields a recall-oriented view (though precision or $F$-measure versions also exist).

    \item \textbf{BERT Score}~\cite{zhang2019bertscore}: Leverages contextual embeddings from Transformer-based models to measure semantic alignment. Particularly useful for identifying subtle meaning shifts in technical text.

    \vspace{6pt}
    \noindent
    \textit{Formulation.}
    Let $\mathbf{c}_j$ be the embedding of the $j$-th token in candidate $C$ and $\mathbf{r}_i$ the embedding of the $i$-th token in reference $R$. Using cosine similarity,
    \[
      s(\mathbf{c}_j, \mathbf{r}_i) \;=\; 
      \frac{\mathbf{c}_j \cdot \mathbf{r}_i}{\|\mathbf{c}_j\|\;\|\mathbf{r}_i\|}.
    \]
    Then BERT Score precision and recall are:
    \begin{align}
      \mathrm{Precision} &= 
        \frac{1}{|C|} \sum_{c_j \in C} \max_{r_i \in R}\,s(\mathbf{c}_j,\mathbf{r}_i), \\[4pt]
      \mathrm{Recall} &=
        \frac{1}{|R|} \sum_{r_i \in R} \max_{c_j \in C}\,s(\mathbf{r}_i,\mathbf{c}_j),
    \end{align}
    and the $F_1$ score is
    \[
      \mathrm{F1} = \frac{2\,(\mathrm{Precision} \times \mathrm{Recall})}
                         {\mathrm{Precision} + \mathrm{Recall}}.
    \]
    Reported BERT Score is typically this F1 value.

    \item \textbf{COMET}~\cite{rei2020comet}: A newer neural-based framework that ranks translations through pretrained language models, offering a more holistic approach to semantic and syntactic quality evaluation.

    \vspace{6pt}
    \noindent
    \textit{Formulation (Conceptual).}
    COMET uses a pretrained encoder (e.g., XLM-R) to obtain embeddings for the source $x$, hypothesis (candidate) $y$, and reference $r$:
    \[
      \mathbf{h}_x = \mathrm{Enc}(x), \quad
      \mathbf{h}_y = \mathrm{Enc}(y), \quad
      \mathbf{h}_r = \mathrm{Enc}(r).
    \]
    These embeddings are fed into a learned regressor $\phi_W$:
    \[
      \hat{Q}(x,y,r) \;=\; \phi_{W}\!\bigl(\mathbf{h}_x, \mathbf{h}_y, \mathbf{h}_r\bigr),
    \]
    producing a quality estimate. The final COMET score is:
    \begin{equation}
      \mathrm{COMET}(x,y,r) \;=\; \hat{Q}(x,y,r).
    \end{equation}
    Since $\phi_W$ is trained on human judgments, COMET effectively learns to predict translation quality across diverse contexts.

    \item \textbf{Accuracy}\cite{manning2008introduction}: A fundamental metric used to evaluate classification models, measuring the proportion of correctly predicted instances over the total number of instances.
    
    \vspace{6pt}
    \noindent
    \textit{Formulation.}  
    Given a classification task with predictions $\hat{y}$ and ground-truth labels $y$, accuracy is defined in terms of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN):
    \[
      \mathrm{Accuracy} = \frac{\text{TP} + \text{TN}}{\text{TP} + \text{TN} + \text{FP} + \text{FN}}.
    \]
    This metric provides an overall measure of correctness but may be insufficient for imbalanced datasets. 
    
    For multi-class classification with $N$ samples and $K$ classes, accuracy extends to:
    \[
      \mathrm{Accuracy} = \frac{1}{N} \sum_{i=1}^{N} \mathbb{1}(\hat{y}_i = y_i),
    \]
    where $\mathbb{1}(\cdot)$ is the indicator function that returns 1 if the prediction matches the true label, otherwise 0.

    \item \textbf{Precision, Recall, and F1-score}\cite{manning2008introduction}: These metrics provide deeper insight into classification performance, especially for imbalanced datasets.

    \begin{itemize}
        \item \textbf{Precision (per language)}: Measures how many predicted labels for a given language are actually correct.
        \[
        \mathrm{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}}.
        \]
        
        \item \textbf{Recall (per language)}: Measures how many actual labels for a given language were correctly predicted.
        \[
        \mathrm{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}}.
        \]

        \item \textbf{F1-score}: The harmonic mean of precision and recall, balancing both metrics.
        \[
        \mathrm{F1} = 2 \times \frac{\mathrm{Precision} \times \mathrm{Recall}}{\mathrm{Precision} + \mathrm{Recall}}.
        \]

    \end{itemize}

    \item \textbf{Confusion Matrix}: A structured table that helps visualize how often each language is correctly classified or misclassified as another. The confusion matrix for $K$ classes is defined as:

    \[
    C_{i,j} = \text{Number of instances of class } i \text{ classified as class } j.
    \]

    The diagonal elements represent correct predictions, while off-diagonal elements indicate misclassifications. This visualization is especially useful for understanding error patterns across different languages.

\end{itemize}

\section{Results}
\label{sec:results}
This section presents the findings of our evaluation of ChatGPT, AWS Translate, and DeepL on the curated bug report dataset. Figure \ref{fig:violin_plots} presents the distribution of translation model scores using violin plots.

\begin{figure*}[htbp]
    \centering
    \caption{Violin plots of different MT evaluation metrics across AWS, GPT, and DeepL translation tools.}
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{violin_plot_BERT.png}
        \caption{BERT Scores}
        \label{fig:bert}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{violin_plot_BLEU.png}
        \caption{BLEU Scores}
        \label{fig:bleu}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{violin_plot_COMET.png}
        \caption{COMET Scores}
        \label{fig:comet}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{violin_plot_METEOR.png}
        \caption{METEOR Scores}
        \label{fig:meteor}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{violin_plot_ROUGE-L.png}
        \caption{ROUGE-L Scores}
        \label{fig:rouge}
    \end{subfigure}

    \label{fig:violin_plots}
\end{figure*}

\subsection{Automatic Metrics}
\label{subsec:auto_metrics}

Table~\ref{tab:auto_metrics} summarizes the results of the five automatic evaluation metrics—BLEU, METEOR, ROUGE, BERTScore, and COMET—calculated over 1,300 multilingual bug reports. We highlighted the highest scores for each metric in bold.

\begin{table}[htbp]
    \centering
    \caption{Automatic metric scores of translation systems.}
    \label{tab:auto_metrics}
    \renewcommand{\arraystretch}{1.2} % Adjust row height for readability
    \begin{tabular}{lccc}
        \toprule
        \textbf{Metric (Mean)} & \textbf{ChatGPT} & \textbf{DeepL} & \textbf{AWS Translate} \\
        \midrule
        BLEU          & 0.42  & \textbf{0.53}  & 0.49  \\
        METEOR        & 0.60  & 0.59  & \textbf{0.62}  \\
        COMET         & 0.23  & \textbf{0.32}  & \textbf{0.32}  \\
        BERTScore (F1) & 0.73  & \textbf{0.76}  & 0.75  \\
        ROUGE-L (F1)   & 0.64  & \textbf{0.68}  & 0.67  \\
        \bottomrule
    \end{tabular}
\end{table}

\noindent\textbf{BLEU}: DeepL achieved the highest BLEU score (0.53), followed by AWS Translate (0.49), with ChatGPT scoring the lowest (0.42), suggesting that DeepL had the most substantial n-gram overlap with the reference translations. However, BLEU alone may not fully capture nuanced meaning in technical text.

\noindent\textbf{METEOR}: AWS Translate led with a score of 0.62, outperforming ChatGPT (0.60) and DeepL (0.59), indicating that AWS Translate better aligns with reference translations in terms of synonyms, stems, and paraphrases.

\noindent\textbf{ROUGE}: DeepL achieved the highest ROUGE-L F1 score (0.68), followed closely by AWS Translate (0.67) and ChatGPT (0.64). Since bug reports often contain specific error messages and repeated domain terms, a higher ROUGE score may indicate a tendency toward literal phrasing.

\noindent\textbf{BERTScore}: DeepL attained the highest BERTScore F1 (0.76), suggesting strong semantic alignment with reference translations. AWS Translate followed with 0.75, while ChatGPT scored the lowest at 0.73, highlighting minor differences in capturing contextual meaning across bug descriptions.

\noindent\textbf{COMET}: DeepL and AWS Translate achieved the highest COMET score (0.32), with ChatGPT trailing at 0.23. Since COMET relies on pre-trained language models to assess semantic fidelity, these results suggest that DeepL and AWS Translate maintained firmer semantic consistency in domain-specific text such as bug reports.

\noindent\textbf{Accuracy}:
Table~\ref{tab:translation_comparison} compares the three translation models—ChatGPT, DeepL, and AWS Translate—for source language identification. While AWS Translate achieved the highest accuracy (0.4786), ChatGPT performed better in precision (0.7518), suggesting a higher proportion of correct language identifications among the predicted instances. DeepL, while achieving moderate performance across all metrics, showed a balance between recall (0.6958) and F1-score (0.6634). The confusion matrix is visualized in \textbf{Figure~\ref{fig:cm_plots}}.

\begin{table}[htbp]
    \centering
    \caption{Language identification scores.}
    \label{tab:translation_comparison}
    \renewcommand{\arraystretch}{1.2} % Adjust row height for readability
    \begin{tabular}{lccc}
        \toprule
        \textbf{Metric} & \textbf{ChatGPT} & \textbf{DeepL} & \textbf{AWS Translate} \\
        \midrule
        Accuracy  & 0.415  & 0.464  & \textbf{0.479}  \\
        Precision & \textbf{0.752}  & 0.651  & 0.712  \\
        Recall    & 0.760  & 0.696  & \textbf{0.786}  \\
        F1-score  & 0.717  & 0.663  & \textbf{0.737}  \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{figure*}[htbp]
    \centering
    \caption{Confusion matrices for language identification by AWS Translate, DeepL, and ChatGPT.}
    
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{cm_aws_src_lang.png}
        \caption{AWS Translate}
        \label{fig:aws_cm}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{cm_deepL_src_lang.png}
        \caption{DeepL}
        \label{fig:deep_cm}
    \end{subfigure}
    
    \vspace{0.5cm} % Add some vertical space between rows

    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{cm_gpt_src_lang.png}
        \caption{ChatGPT}
        \label{fig:gpt_cm}
    \end{subfigure}

    \label{fig:cm_plots}
\end{figure*}

\subsection{Summary of Findings}
DeepL outperformed ChatGPT and AWS Translate across most translation metrics, particularly BLEU, ROUGE, BERTScore, and COMET, demonstrating superior accuracy and semantic alignment. AWS Translate achieved the highest METEOR score, indicating more vigorous handling of synonyms, stemming, and paraphrasing. ChatGPT exhibited the weakest performance across all metrics, reflecting lower n-gram overlap and reduced semantic consistency.

No single model consistently outperformed the others across all evaluation criteria for language identification. The optimal choice depends on whether precision, recall or overall accuracy is prioritized for multilingual translation tasks.

\section{Discussion}
\label{sec:discussion}

The results presented in Section~V illustrate both the promise and limitations of applying leading MT systems to the specialized domain of bug reports. DeepL’s strong performance across most automatic metrics underscores its effectiveness in handling structured text and maintaining semantic consistency. AWS Translate also demonstrated strengths, particularly in METEOR, while ChatGPT lagged in multiple metrics. However, as noted by Chu and Wang~\cite{chu2018survey}, domain adaptation remains a significant challenge for neural MT systems, especially when handling specialized tokens such as code references or UI labels. These findings highlight the need for further MT model refinement to accommodate domain-specific terminology in software engineering tasks better.

\subsection{Interpreting Translation Metrics}
The discrepancies among BLEU, METEOR, and ROUGE scores highlight the inherent limitations of each metric in capturing the full spectrum of translation quality~\cite{Koehn2010}. For instance, DeepL’s higher BLEU and ROUGE scores indicate strong lexical and structural alignment, yet ChatGPT’s lower BERTScore and COMET results suggest weaker semantic consistency. Interestingly, AWS Translate led in METEOR but did not achieve the highest scores in other semantic metrics, suggesting strengths in lexical variation but challenges in deeper contextual alignment. These findings align with prior observations that purely $n$-gram-based metrics can fail to capture contextual nuances in technical text~\cite{arnaoudova2016linguistic}. By contrast, metrics like BERTScore and COMET, where DeepL and AWS Translate outperformed ChatGPT, leverage contextual embeddings to provide a richer view of semantic fidelity, an approach increasingly recommended for domain-specific evaluations~\cite{Zhang2019, Rei2020}.

\subsection{Contextual Limitations and Model Updates}
One noteworthy caveat involves the evolving nature of cloud-based MT systems. ChatGPT, for instance, is updated regularly, potentially causing temporal variations in translation performance. AWS Translate and DeepL also deploy algorithmic improvements over time. As with prior studies on continuously evolving MT frameworks~\cite{Koehn2010}, our findings represent a snapshot of performance rather than a static benchmark. Future research should consider conducting repeated evaluations over extended periods to capture longitudinal trends better.

\subsection{Implications for Software Engineering}
Our study highlights the necessity of domain-specific adaptation when translating bug reports, echoing the broader literature on specialized MT tasks~\cite{Chu2018}. Software teams may benefit from integrating glossaries or custom dictionaries into translation pipelines. Such resources could address recurring terminology (e.g., function names, proprietary module references) that appear throughout a codebase. Tools that automatically identify and preserve code tokens could also mitigate the risk of incorrect “translations” of critical technical elements.

\subsection{Limitations}
Although our dataset includes a representative variety of bug reports from the Visual Studio Code repository, it focuses on a single project with its ecosystem of terminology and user community. Expanding the dataset to include bug reports from multiple software systems with diverse technology stacks could assess wider applicability. Moreover, we primarily relied on bilingual software engineers for initial labeling; while their expertise adds domain knowledge, cross-lingual proficiency levels, and personal biases can still influence subjective judgments.

\subsection{Summary}
Our findings emphasize that while the latest MT services can produce increasingly fluent and contextually coherent translations, they still exhibit non-trivial deficiencies in domain adequacy. Bridging this gap requires a combination of advanced neural architectures, specialized training approaches, and the integration of software-specific resources. This work thus contributes to a clearer understanding of the interplay between general-purpose MT systems and the demands of real-world bug reporting scenarios, setting the stage for future improvements and cross-disciplinary collaborations.

\section{Conclusion and Future Work}
\label{sec:conclusion}

This paper has examined the capability of three leading machine translation (MT) services—DeepL, AWS Translate, and ChatGPT—to accurately translate multilingual bug reports sourced from the Visual Studio Code repository. Our findings indicate that AWS Translate performs best in language identification (RQ1), achieving the highest accuracy (0.479) and F1-score (0.737), followed by DeepL and ChatGPT, suggesting that AWS Translate is more effective in detecting source languages, which is crucial for efficient translation workflows.

In preserving the technical precision of VS Code bug reports (RQ2), DeepL consistently outperforms the other systems across most automatic metrics, including BLEU, ROUGE, and BERTScore, demonstrating strong lexical and semantic fidelity. AWS Translate performs competitively, particularly in METEOR, suggesting it handles paraphrases well, while ChatGPT lags in most metrics, indicating weaker alignment with the original bug report content.

\subsection{Key Observations}
\begin{itemize}
    \item \textbf{Language Identification Performance (RQ1):} AWS Translate achieved the highest accuracy and F1-score, suggesting it is the most reliable source language detecting. DeepL followed closely, while ChatGPT had the lowest identification accuracy.
    
    \item \textbf{Lexical vs. Semantic Accuracy (RQ2):} DeepL achieved the highest BLEU, ROUGE, and BERTScore results, indicating strong preservation of technical precision in bug reports. AWS Translate performed best in METEOR, highlighting its ability to handle paraphrases, while ChatGPT lagged in most metrics.

    \item \textbf{Metric Variability:} The discrepancies across BLEU, METEOR, ROUGE, BERTScore, and COMET emphasize the importance of multi-metric evaluations. While n-gram-based metrics capture lexical matches, embedding-based approaches such as BERTScore and COMET provide a deeper assessment of semantic alignment~\cite{Zhang2019, Rei2020}.
\end{itemize}

\subsection{Future Directions}
Several avenues warrant further investigation:

\begin{enumerate}
    \item \textbf{Custom Domain Adapters:} Building on existing work~\cite{Chu2018}, future research could explore specialized glossaries or fine-tuned models to mitigate the loss of critical technical details, involving integrating bug report ontologies or code-aware tokenization methods into the translation pipeline.

    \item \textbf{Longitudinal Analysis:} Cloud-based MT services are frequently updated, potentially altering translation quality over time~\cite{Koehn2010}. Periodic re-evaluations could reveal performance trends and help practitioners decide when to re-translate or re-verify critical documentation.

    \item \textbf{Broader Dataset Coverage:} Although this study focused on VS Code bug reports, expanding to other large-scale projects across different programming languages and technology stacks would yield a more generalizable understanding of MT performance in software engineering contexts.
\end{enumerate}

In conclusion, while modern MT services have made significant strides, particularly in fluency and general-domain accuracy, further adaptation is required to meet the nuanced needs of multilingual bug reporting fully. This paper provides a framework for evaluating domain-specific translation tasks. We hope these findings will spur further innovations in integrating MT within software engineering pipelines, ultimately enabling more seamless and efficient global collaboration.

\bibliographystyle{IEEEtran}
\nocite{*}
\bibliography{ref}
\vspace{12pt}

\end{document}
