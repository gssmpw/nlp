\section{Related Work}
\label{sec:related_work}
Research on machine translation (MT) has advanced considerably over the past two decades, transitioning from rule-based and statistical paradigms to sophisticated neural approaches. Early neural models introduced by Bahdanau \emph{et al.}~\cite{bahdanau2014neural} demonstrated how attention mechanisms improve translation by aligning source and target words at each decoding step. Subsequent architectures, notably the Transformer~\cite{Vaswani2017}, showcased even more significant gains through multi-head attention and parallelizable layers. These innovations have substantially improved translation quality for general-domain text.

Despite the progress in neural MT, domain adaptation remains a critical challenge. Chu and Wang~\cite{Chu2018} provide a comprehensive survey of strategies for adapting neural models to specialized domains. They note that while generic training corpora enhance fluency and coverage, specialized domains—such as software engineering—require customized approaches to handle domain-specific lexicons and contexts effectively. Domain adaptation also affects translation evaluation, as metrics calibrated on general text may not accurately reflect correctness in specialized scenarios~\cite{Koehn2010}.

Within software engineering, bug reports constitute a unique category of text that often includes code snippets, API references, stack traces, and project-specific jargon. For instance, Arnaoudova \emph{et al.}~\cite{arnaoudova2016linguistic} highlights the linguistic complexity of software artifacts, emphasizing how ambiguities or errors in terminology can hinder comprehension and resolution of software issues.

Evaluating MT quality for these specialized texts necessitates both classic and modern metrics. BLEU~\cite{papineni2002bleu}, METEOR~\cite{banerjee2005meteor}, and ROUGE~\cite{Lin2004} have been widely adopted for general-purpose translation and summarization tasks. However, more recent metrics such as BERTScore~\cite{zhang2019bertscore} and COMET~\cite{rei2020comet} offer improved semantic sensitivity by leveraging transformer-based models for embedding and contextual analysis. These modern metrics may be particularly beneficial in software engineering contexts, where preserving the precise meaning of technical details is essential for successful bug resolution.

Finally, while industry-driven solutions such as AWS Translate, DeepL, and ChatGPT are commonly employed for multilingual documentation and user support, published comparative analyses specific to software bug reports remain limited. Existing studies typically focus on localization scenarios or general test suites that may not capture the nuanced requirements of bug triaging and software maintenance. As such, a deeper examination of how these systems handle authentic bug reports is necessary and timely. Our work aims to fill this gap by systematically evaluating these three MT solutions on bug reports from the Visual Studio Code repository, offering insights into their strengths and limitations in real-world software engineering.