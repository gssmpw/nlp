\vspace{-0.3cm}
\section{Background and Preliminaries}
\vspace{-0.15cm}
This section introduces the foundational concepts relevant to our methodology and baselines on CMs, CFG, and inversion-based image editing. In the remaining discussion, we denote \(\boldsymbol{x}_0\) as a sample from the target data distribution, denoted as \(p_\text{data}(\boldsymbol{x})\). 

\vspace{-0.15cm}
\subsection{EDM and Consistency Models}
\label{sec:edm_cm}
\vspace{-0.15cm}

In this paper, we mainly consider EDM \cite{karras2022elucidating}, a popular formulation of DM, diffusion scheduler, and ODE widely used by CMs \cite{song2023consistency,song2023improved,li2024bidirectional,ect}. We introduce two ways of learning discrete-time CMs, CD and CT, originally in the first CM paper \cite{song2023consistency}, as well as the continuous-time CM from ECT \cite{ect}.  

\noindent{\bf EDM.} The stochastic forward process of EDM is described by, \(\text{d}\boldsymbol{x}_t = \boldsymbol{x}_t\text{d}t + \sqrt{2t}\text{d}\boldsymbol{w}_t\), where \(t \in [0, t_\text{max}]\), and \(\boldsymbol{w}_t\) denotes the Brownian motion. With a sample from the dataset, \(\boldsymbol{x}_0 \sim p_\text{data}(\boldsymbol{x})\), and a random noise, \( \boldsymbol{z} \sim \mathcal{N}(\boldsymbol{z};\boldsymbol{0},\mathbf{I})\), EDM learns the reverse process via the \textit{denoising objective}, 
\begin{equation}
    \mathbb{E} \left[ \lambda(t) \left\| D_{\theta}(\boldsymbol{x}_t) - \boldsymbol{x}_0 \right\|_2^2 \right],
    \label{eq:edm-obj}
\end{equation}
where \(\boldsymbol{x}_t = \boldsymbol{x}_0+t\boldsymbol{z}\), \(\lambda(t)\) is a weighting function w.r.t. \(t\), and \(D_{\theta}\) is further parametrized by \(D_{\theta}(\boldsymbol{x}_t; t) = c_{\text{skip}}(t) \boldsymbol{x}_t + c_{\text{out}}(t) F_{\theta}(c_{\text{in}}(t) \boldsymbol{x}_t; c_{\text{noise}}(t))\), allowing the network to predict residual information from the signal and noise mixture. Besides, the \textit{preconditioning terms}, \(c_{\text{skip}}, c_{\text{out}}, c_{\text{in}}\), ensure unit variance of both the model's target and input. For readers interested in the derivations, please refer to Appendix B.6 from the EDM paper.

\noindent{\bf Discrete-time CM.}
\label{p:discreteCM}We focus our discussion on the family of CMs that model the same ODE as EDM. CMs aim to inference the solution of the PF-ODE at \(t=0\) directly or with few NFEs. To achieve this, CMs are optimized against the \textit{consistency objective},
\begin{equation}
    \mathbb{E}[\lambda(t_{n+1})d(D_{\theta}(\boldsymbol{x}_{t_{n+1}},t_{n+1}),D_{{\theta}^{-}}( \boldsymbol{x}_{t_{n}},t_{n}))],
    \label{eq:cm-obj}
\end{equation}
where \(\{ t_n \}_{n=0}^{N}\) is the discrete timestep sequence defined throughout the course of training, and \(d(\boldsymbol{\cdot},\boldsymbol{\cdot})\) is the distance function. The \textit{noisier term} \(\boldsymbol{x}_{t_{n+1}}\) is given by \(\boldsymbol{x}_0 + t_{n+1} \boldsymbol{z}\), while the \textit{cleaner term} \(\boldsymbol{x}_{t_n}\) can be obtained from the instantaneous change at \(\boldsymbol{x}_{t_{n+1}}\) using a pretrained EDM or by \(\boldsymbol{x}_{t_{n+1}} + (t_n - t_{n+1}) \boldsymbol{z}\), sharing the same noise direction \(\boldsymbol{z}\). The former approach, known as Consistency Distillation (CD), \textit{distills knowledge} from a pretrained EDM, while the latter data-driven method is called Consistency Training (CT). At training, gradient updates are applied only to the weights predicting the noisier term, while the weights for the cleaner term, denoted \(\theta^-\), remains frozen. In CMs, the preconditioning terms look nearly identical to the ones used in EDM to ensure the \textit{boundary condition} of the consistency objective: at \(t=t_\text{min}\), \(D_{\theta}(\boldsymbol{x}) = \boldsymbol{x}\), with \(c_{\text{skip}}(t_\text{min}) = 1\), and \(c_{\text{out}}(t_\text{min}) = 0\), where \(t_\text{min} \in [0,t_\text{max}]\) is the lowest noise at training for stability \cite{song2023consistency}.

\noindent{\bf Continuous-time CM.} We focus on continuous-time CMs introduced in ECT \cite{ect}. ECT is a consistency training/tuning algorithm replacing the discrete timesteps \(\{ t_n \}_{n=0}^{N}\) with a continuous-time schedule. ECT samples noise scales following a lognormal distribution, \(t \sim p(t) = \textit{LogNormal}(P_\textit{mean}, P_\textit{std}), r \sim p(r|t)\) where \(t > r\), optimizing Eq. \ref{eq:cm-obj} between noisy samples \(x_t\) and \(x_r\) along the same noise direction. Throughout training, ECT reduces \(\Delta t:=t-r\) to \(\text{d}t\) via an exponentially decreasing schedule with a base of \(\frac{1}{2}\), e.g., \(\Delta t (t)=\frac{t}{2^{\left\lfloor k/d \right\rfloor}}n(t)\), where \(k, d\) is the training and doubling iterations, and \(n(t)\) is a sigmoid adjusting function. Our iGCT is trained under the same continuous-time schedule used by ECT. To establish DM independence, iGCT's training curriculum begins with diffusion training by setting \(r=t_\text{min}\).

\vspace{-0.15cm}
\subsection{Classifier-free Guidance}
\vspace{-0.15cm}

CFG is a technique in DMs that improves image quality by jointly optimizing an unconditional and conditional denoising objective \cite{ho2022classifier}. At inference, guidance is achieved by extrapolating conditional and unconditional passes using factor \(w\).
\begin{equation}
\frac{\text{d}\boldsymbol{x}}{\text{d}t} = \frac{-\left(w D_{\theta}(\boldsymbol{x}|c;t) + (1-w)D_{\theta}(\boldsymbol{x}|\emptyset;t) - \boldsymbol{x}\right)}{t},
\end{equation}
when \(w>1\), the update direction is encouraged by the conditional term, \(c\), and discouraged from the unconditional term, \(\emptyset\). In practice, high guidance enables high-fidelity image generation \cite{rombach2022high,podell2023sdxlimprovinglatentdiffusion,dieleman2022guidance,ho2022imagenvideohighdefinition,Meng_2023_CVPR} and precise image editing \cite{mokady2023null,miyake2023negative,han2024proxedit,garibi2024renoise,huberman2024edit,starodubcev2024invertible}. High CFG guidance causes deviation from the original PF-ODE, leading to mode drop and overly saturated images \cite{saharia2022photorealistictexttoimagediffusionmodels,kynkäänniemi2024applyingguidancelimitedinterval,bradley2024classifierfreeguidancepredictorcorrector}. Further comparisons of guidance are provided in Sec. \ref{sec:method-gct} and Fig. \ref{fig:1d_cfg_igct}.

% talk about guidance for LCM
\noindent{\bf Guidance in Latent Consistency Models.}
Latent CMs \cite{luo2023latent} are distilled from latent DMs that synthesizes high-resolution images requiring 2-4 NFEs. At distillation, the cleaner term \(x_{t_n}\) relies on an ODE solver, \(\Psi\), a teacher DM, \(D_{\phi}\), to compute the incorporated CFG knowledge into the consistency objective (Eq. \ref{eq:cm-obj}), e.g.,
\vspace{-0.6em}
\begin{equation}
    \begin{aligned}
    \boldsymbol{x}_{t_n} &= \boldsymbol{x}_{t_{n+1}} + (t_n - t_{n+1}) \cdot [w\Psi(\boldsymbol{x}_{t_{n+1}}, t_{n+1}, c; D_{\phi}) \\
    & + (1-w) \Psi(\boldsymbol{x}_{t_{n+1}}, t_{n+1}, \emptyset; D_{\phi}) ],
    \end{aligned}
\label{eq:lcm}
\end{equation}
where the guidance range \(w \in [w_\text{min},w_\text{max}]\) is chosen as a hyper-parameter during distillation. 
\vspace{-0.15cm}
\subsection{Inversion-based Image Editing}
\vspace{-0.15cm}
Inversion-based editing aims to modify subjects in real images by aligning the forward process of diffusion models with the DDIM inversion trajectory~\cite{song2022denoisingdiffusionimplicitmodels}. Previous works achieve this by tuning learnable representations during the editing process, which often requires time-consuming per-image optimization~\cite{dong2023prompt,li2023stylediffusion,mokady2023null}. While BCM \cite{li2024bidirectional}, a CM that demonstrates capabilities in both image generation and inversion, iGCT distinguishes itself by decoupling the inversion module from the generation. This separation allows for the flexibility for guidance generation with iGCT's denoiser. Despite these advancements, achieving inversion-based editing in one-step guided CMs remains an unexplored challenge. As a pioneering work, iGCT addresses this challenge by focusing on category-based editing, offering a novel approach to the problem. Further analysis and results are provided in Section \ref{sec:image-editing}.