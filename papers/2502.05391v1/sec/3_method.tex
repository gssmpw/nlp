\begin{figure}[t!]
    \centering
    \includegraphics[width=0.475\textwidth]{fig/1d_cfg_igct.png} 
    \vspace{-1em}
    \caption{Dynamics of guidance, CFG vs iGCT, 1D toy example with 2 modes at \(x=2\) and \(-2\). Under high guidance, CFG overshoots (in \textcolor{orange}{orange}), \textbf{causing the modes to fall off and producing high contrast values}. Our iGCT (in \textcolor{green}{green}) is able to \textbf{preserve the modes} same as the data distribution (in \textcolor{blue}{blue}). }
    \vspace{-1.5em}
    \label{fig:1d_cfg_igct}  % Label for referencing the figure
\vspace{-0.5em}
\end{figure}

\vspace{-0.15cm}
\section{Invertible Guided Consistency Training}
\vspace{-0.15cm}

\label{sec:method}
This section begins by introducing the approach for data-driven guidance learning with CT, followed by a discussion on the invertible aspect. All together, we present Invertible Guided Consistency Training (iGCT), a one-stage training framework that supports few step, guided image generation and image editing (Fig. \ref{fig:method_overview}). iGCT requires less gpu training hours compared to the total time required for the two-stage framework, i.e., optimizing CFG EDM + guided CD (Table \ref{table:compute_resources}). The resulting iGCT achieves higher precision \cite{kynkäänniemi2019improvedprecisionrecallmetric} compared to a CFG trained DM and a CM distilled from it \cite{song2023consistency,luo2023latent}, allowing better FID under high guidance (Fig. \ref{fig:results_fid_prec_rec}).  


\begin{figure}[t!]  % Use 'h' to suggest placing the figure here
    \centering
    \includegraphics[width=0.4\textwidth]{fig/method_overview.png} 
    \vspace{-1em}
    \caption{\textbf{Overview of iGCT.} As oppose to the denoiser, the noiser learns to map image to noise by swapping the model's input at training, i.e., the noisier sample \(\boldsymbol{x}_t\) is \(\boldsymbol{x}_r\)'s \textit{target} (See Eq. \ref{eq:inv-loss} for details). The predicted noise latent and denoised image distribution is aligned using the reconstruction loss. }
    \vspace{-1.0em}
    \label{fig:method_overview}  % Label for referencing the figure
\end{figure}

\vspace{-0.3cm}
\subsection{The "Curse of Uncondition" and Guided CT}
\label{sec:method-gct}
\vspace{-0.15cm}

For distillation-based CM, guidance is achieved by emphasizing the conditional while negating the unconditional score predicted by a teacher DM. Associated with the input condition \(c\), the same random noise \(\boldsymbol{z}\) that is used to produce the noisy sample \(\boldsymbol{x}_t\) guides toward its original clean image \(\boldsymbol{x}_0\) in conditional CT. Following the notation of continuous-time CM in Sec. \ref{sec:edm_cm}, as \(\Delta t \rightarrow \text{d}t\) with progressively finer discretization during training, the expected instantaneous change from random noise converges to the underlying true conditional noise, denoted as \(\boldsymbol{\epsilon}_{c}\).

\begin{figure}[t!]  
    \centering
    \includegraphics[width=0.475\textwidth]{fig/method_gct.png} 
    \vspace{-1.2em}
    \caption{In traditional conditional CT, estimating the unconditional noise \(\epsilon_{\emptyset}\) from a noisy sample \(\boldsymbol{x}_t\) requires averaging over all potential clean images (left). By \textbf{decoupling the target class \(c^\text{tar}\) from the source image \(\boldsymbol{x}_0^{\text{src}}\)}, we can estimate \(\epsilon_{\emptyset}\) with \(z\) as how unconditional CT predicts noise, and \(\epsilon_{\text{c}}^{\text{tar}}\) with \(z^*\) like how conditional CT predicts noise (right).}
    \vspace{-1.2em}
    \label{fig:method_gct}  % Label for referencing the figure ``''
\end{figure}

However, finding the true unconditional noise is challenging without a DM model. Since guided CMs are defined as \(D_\theta(\boldsymbol{x}_t, t, c, w)\), the sampled noise \(\boldsymbol{z}\) that generated \(\boldsymbol{x}_t\) inherently depends on \(c\), making it unsuitable as an unconditional noise estimate. In other words, \(\boldsymbol{z}\) associates with the class \(c\) inevitably. Prior works address this by training the CM on a denoising objective with the condition masked \cite{hu2024efficienttextdrivenmotiongeneration}, i.e., \(\emptyset\), , allowing it to function as a DM when unconditioned, and switch back to CM when conditioned. To further highlight this challenge, a tempting approach is to approximate the unconditional noise by averaging among random data; however, this requires the conditional density of source samples given \(\boldsymbol{x}_t\), which is effectively what a DM model is trained to model in the absence of class labels.

To address these challenges, we decouple the condition from the source image that generated \(\boldsymbol{x}_t\). We denote \(c^\text{tar}\) as the target condition and \(\boldsymbol{x}_0^{\text{src}}\) as the source image before diffusion, i.e., \(\boldsymbol{x}_t = \boldsymbol{x}_0^{\text{src}} + t \boldsymbol{z}\). As \(\boldsymbol{x}_t\) originates from data independent of \(c^\text{tar}\), we can treat the noise \(\boldsymbol{z}\) as an estimate of the true unconditional noise. The direction from \(\boldsymbol{x}_t\) to a random sample \(\boldsymbol{x}_0^{\text{tar}}\) associated with \(c^\text{tar}\) serves as an estimate of the true conditional noise. Thus, we define the guided direction as the extrapolation between \(\boldsymbol{z}\) and \(\boldsymbol{z}^* := \frac{\boldsymbol{x}_t-\boldsymbol{x}_0^{\text{tar}}}{t}\), yielding \(\boldsymbol{x}_{r} = \boldsymbol{x}_t + (r-t) [w\boldsymbol{z}^*+(1-w)\boldsymbol{z}]\) as the input to the target for the consistency objective (Fig. \ref{fig:method_gct}).

To effectively span the PF-ODE trajectory with sampled data \(\boldsymbol{x}_t\) and \(\boldsymbol{x}_r\) as a coherent \textit{chain}, our loss function incorporates both guided consistency training and the original CT objective (Eq. \ref{eq:cm-obj}) under a continuous-time schedule. Specifically, for high noise levels \(t\), the target \(\boldsymbol{x}_r\) is more likely generated through the above decoupled approach, serving as the target for \(D_\theta(\boldsymbol{x}_t, t, c, w)\) to learn guidance. For low noise samples, \(\boldsymbol{x}_r\) is generated solely via the shared noise \(z\) assuming that \(\epsilon_\emptyset\) and \(\epsilon_{\text{c}}\) are indistinguishable when the sample closely resembles the clean image. We compare the guidance logic of CFG and iGCT using a 1D diffusion toy example with target modes at \(x=2\) and \(-2\) (Fig. \ref{fig:1d_cfg_igct}). CFG shows an overshooting trajectory and generates extreme values away from the modes. In contrast, iGCT produces modes that align with the data distribution. 
The guided CT loss is formulated as follows:
\begin{equation}
     \mathcal{L}_\text{gct} = \lambda(t) d(D_{\theta}(\boldsymbol{x}_t,t,c,w),D_{{\theta}^-}(\boldsymbol{x}_r,r,c,w)). 
    \label{eq:gct-loss}
\end{equation},
where \(\lambda(t)=1/(t-r)\) is the weighting function defined by the reciprocal of the step \(\Delta t\) following ECT \cite{ect}. 

\vspace{-0.15cm}
\subsection{Inverse Consistency Training}
\vspace{-0.15cm}

\label{sec:method-ict}
Besides few-step guided image generation with CT, we introduce inverse CT, which leverages the consistency objective in reverse to map the target distribution back to Gaussian. This deterministic inverse mapping facilitates precise image editing, traditionally performed via DDIM requiring numerous NFEs. By training on the consistency objective, our model directly infers the PF-ODE endpoint efficiently. We define the model that maps images to noise as the \textit{noiser}, denoted \(N_\varphi\).

Following ECT \cite{ect}, we sample \(t \sim p(t) = \textit{LogNormal}(P_\textit{mean}, P_\textit{std})\), and set \(r = t - \Delta t\) by progressively reducing \(\Delta t\) over training. This approach emphasizes importance sampling of pairs \(\boldsymbol{x}_t\) and \(\boldsymbol{x}_r\) in the lower region, where learning the PF-ODE path is more complex and challenging compared to higher noise levels \cite{karras2022elucidating, song2023improved}. As oppose to ECT, we define the weighting function as \(\lambda(t)' = \Delta t(t) / t_\text{max}\), and treat \(\boldsymbol{x}_t\) as the target for inverse CT, optimizing the objective \(\mathbb{E}[\lambda(t)'d(N_\varphi(\boldsymbol{x}_r,r,c),N_{{\varphi}^{-}}( \boldsymbol{x}_t,t,c))]\). Similar to EDM and CMs, our noiser is further preconditioned. We define \(N_{\varphi}(\boldsymbol{x}_t,t,c) = c_\text{skip}(t)\boldsymbol{x}_t + c_\text{out} F_{\varphi}(c_\text{in}\boldsymbol{x}_t, t, c) \), where \(c_\text{in}(t) = 1/\sqrt{t^2+\sigma_\text{data}^2}\), \(c_\text{out}(t) = t_\text{max} - t\) and \(c_\text{skip}(t) = 1\), serving as the boundary conditions while preserving the unit variance property. The proof of unit variance is provided in Appendix \ref{appendix:unit-variance}. The inverse CT loss is given by:
\begin{equation}
     \mathcal{L}_\text{inv} = \lambda(t)'d(N_{\varphi}(\boldsymbol{x}_r,r,c),N_{{\varphi}^-}(\boldsymbol{x}_t,t,c)). 
    \label{eq:inv-loss}
\end{equation}
Following iCD \cite{starodubcev2024invertible}, we found that adding a reconstruction loss is critical to aligning the noiser's latent output with the denoiser's input noise. The reconstruction loss is defined as:
\begin{equation}
     \mathcal{L}_\text{recon} = d(D_{\theta}(N_{\varphi}(\boldsymbol{x}_0,t_\text{min},c),t_\text{max},c,0), \boldsymbol{x}_0),
    \label{eq:recon-loss}
\end{equation}
Putting them all together, the summarized loss terms for our invertible Guided Consistency Training is, \(\mathcal{L} = \mathcal{L}_\text{gct} + \mathcal{L}_\text{inv} + \lambda_{\text{recon}}\mathcal{L}_\text{recon} \). Please refer to Appendix \ref{appendix:iGCT} and Table \ref{tab:igct_training_configs} for the full training algorithm and hyperparameters.

\begin{figure*}[t] 
    \centering
    \includegraphics[width=0.94\textwidth]{fig/results_guidance.png} 
    \vspace{-1.3em}
    \caption{Guided generation across levels of guidance scale for CIFAR-10 \textit{"ship"} and ImageNet64 \textit{"robin"}. The top shows images generated via CFG-based methods (\textit{"ship"}: Guided-CD, 1 NFE, \textit{"robin"}: CFG-EDM, 18 NFEs), the below are results generated from our iGCT, 1 NFE. CFG tends to produce high-contrast colors at higher guidance levels, while \textbf{iGCT maintains a consistent overall tone like \(w=0\)} without altering the style.}
    \vspace{-1.5em}
    \label{fig:results_guidance}
\end{figure*}