%%
%% This is file `sample-manuscript.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `all,proceedings,bibtex,manuscript')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-manuscript.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
\documentclass[manuscript,screen]{acmart}
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}


\usepackage{color,array}
\usepackage{multirow} 
\usepackage{float} 
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{longtable}
\usepackage{graphicx}
\usepackage{pgfplots}
\usepackage{pgfplotstable}
\usepackage{lscape} % Enables landscape mode
\usepackage{pgfplots}
\usepackage{xcolor} % Package for custom colors
\usepackage{comment}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmlicensed}
\copyrightyear{2018}
\acmYear{2018}
\acmDOI{XXXXXXX.XXXXXXX}
%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[Conference acronym 'XX]{Make sure to enter the correct
  conference title from your rights confirmation email}{June 03--05,
  2018}{Woodstock, NY}
%%
%%  Uncomment \acmBooktitle if the title of the proceedings is different
%%  from ``Proceedings of ...''!
%%
%%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%%  June 03--05, 2018, Woodstock, NY}
\acmISBN{978-1-4503-XXXX-X/2018/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}


%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Cross-Modality Investigation on WESAD Stress Classification}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Eric Oliver}
\email{eoliver4@patriots.uttyler.edu}
\orcid{0009-0001-6807-5135}
\author{Sagnik Dakshit}
\email{sdakshit@uttyler.edu}
\orcid{0000-0001-9339-6259}
\affiliation{%
  \institution{The University of Texas at Tyler}
  \city{Tyler}
  \state{Texas}
  \country{USA}
}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{E.Oliver and S.Dakshit}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
Deep learning's growing prevalence has driven its widespread use in healthcare, where AI and sensor advancements enhance diagnosis, treatment, and monitoring. In mobile health, AI-powered tools enable early diagnosis and continuous monitoring of conditions like stress. Wearable technologies and multimodal physiological data have made stress detection increasingly viable, but model efficacy depends on data quality, quantity, and modality. This study develops transformer models for stress detection using the WESAD dataset, training on electrocardiograms (ECG), electrodermal activity (EDA), electromyography (EMG), respiration rate (RESP), temperature (TEMP), and 3-axis accelerometer (ACC) signals.  The results demonstrate the effectiveness of single-modality transformers in analyzing physiological signals, achieving state-of-the-art performance with accuracy, precision and recall values in the range of $99.73\%$ to $99.95\%$ for stress detection. Furthermore, this study explores cross-modal performance and also explains the same using 2D visualization of the learned embedding space and quantitative analysis based on data variance. Despite the large body of work on stress detection and monitoring, the robustness and generalization of these models across different modalities has not been explored. This research represents one of the initial efforts to interpret embedding spaces for stress detection, providing valuable information on cross-modal performance. 


\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10003120.10003138.10003140</concept_id>
       <concept_desc>Human-centered computing~Ubiquitous and mobile computing systems and tools</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10010147.10010257.10010293.10010294</concept_id>
       <concept_desc>Computing methodologies~Neural networks</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10010147.10010257.10010339</concept_id>
       <concept_desc>Computing methodologies~Cross-validation</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10010147.10010257.10010258.10010259.10010263</concept_id>
       <concept_desc>Computing methodologies~Supervised learning by classification</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10010405.10010444.10010449</concept_id>
       <concept_desc>Applied computing~Health informatics</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10010405.10010444.10010446</concept_id>
       <concept_desc>Applied computing~Consumer health</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Human-centered computing~Ubiquitous and mobile computing systems and tools}
\ccsdesc[500]{Computing methodologies~Neural networks}
\ccsdesc[500]{Computing methodologies~Cross-validation}
\ccsdesc[500]{Computing methodologies~Supervised learning by classification}
\ccsdesc[500]{Applied computing~Health informatics}
\ccsdesc[500]{Applied computing~Consumer health}
%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Artificial Intelligence (AI), Health Informatics,  Affective Computing, Deep Learning, Explainable artificial intelligence}

\received{20 February 2007}
\received[revised]{12 March 2009}
\received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}

The growing interest in deep learning has led to its widespread adoption across diverse applications, particularly due to its capacity to extract meaningful patterns from raw multimedia data. One of the most transformative domains benefiting from deep learning advancements is healthcare, especially with the emergence of mobile health (mHealth) technologies. The integration of artificial intelligence (AI) into healthcare has revolutionized treatment, diagnosis, and continuous monitoring, enabling more accurate early detection of medical conditions and improved long-term disease management. AI-driven tools now facilitate real-time monitoring, personalized interventions, and predictive analytics, significantly enhancing patient outcomes while reducing the burden on healthcare providers. A key driver of this AI adoption in healthcare is the increasing availability of diverse data modalities, facilitated by advancements in sensor technologies. Wearable devices and smart health monitoring systems can now collect extensive physiological data, including heart rate variability (HRV), electrodermal activity (EDA), respiration rate (RESP), and other biometric indicators. The proliferation of such multimodal data streams, combined with enhanced computational resources, has enabled the development of sophisticated deep learning architectures capable of learning complex relationships across different physiological signals. These architectures leverage large-scale neural networks with numerous parameters to enhance prediction accuracy and improve generalizability across patient populations. However, despite these advancements, a significant challenge remains in ensuring cross-generalization of model performance across different modalities even on the same task. The lack of thorough investigations into cross-modality generalization limits the practical deployment of these models, as they may not perform consistently across different data sources. In this study, we explore the feasibility of developing high-performance deep learning classification models for the critical healthcare monitoring task of stress detection. Specifically, we investigate not only the ability of these models to accurately classify stress levels using multimodal physiological data but also their performance in cross-modality settings. By evaluating how well models trained on one set of sensor data generalize to another, we aim to address a critical gap in current research and contribute to the development of more robust and adaptable AI-driven healthcare solutions for the task of multiclass stress classification.

Stress levels have increased substantially in recent times due to a combination of societal, economic, and personal factors. The rapid pace of contemporary life, in conjunction with escalating work demands and persistent connectivity through digital devices, has obscured the demarcation between professional and personal spheres. Moreover, social pressures, uncertainty regarding the future, and the widespread prevalence of burnout have contributed to rendering stress a pervasive issue in contemporary society. These trends underscore the necessity for efficacious stress management and monitoring interventions. Persistent exposure to stress can lead to the development of chronic conditions with substantial implications for both physical and mental health. Chronic stress disrupts the body's homeostatic mechanisms, resulting in long-term complications such as cardiovascular diseases, compromised immune function, and mental health disorders including anxiety and depression. Furthermore, it negatively impacts productivity, sleep patterns, and overall quality of life. If left unmanaged, chronic stress can exacerbate pre-existing conditions and contribute to the onset of new health problems and potentially lead to drug abuse. This underscores the importance of implementing effective stress monitoring and intervention strategies.

Stress remains a significant factor influencing human health and productivity. The advent of wearable technology has made it possible to utilize physiological signals as non-invasive and practical methods for stress monitoring. Recent advancements in artificial intelligence (AI) and sensor technology have revolutionized stress detection and management. Contemporary wearable devices now include highly sensitive sensors that capture subtle physiological changes, while AI algorithms process these signals to extract actionable patterns. This synergy between sensor technology and AI has enabled real-time and reliable stress detection. This study investigates the feasibility of stress detection using popular physiological signal modalities of electrocardiograms (ECG), electrodermal activity (EDA), electromyography (EMG), respiration rate (RESP), temperature (TEMP) and 3-axis accelerometer signals (ACC)  using the WESAD dataset, a robust benchmark for stress-related studies. We developed transformer models for each modality, demonstrating the efficacy of the physiological signal modalities in stress detection. Additionally, we investigate the cross-modal performance on the same task and explain the performances through visualizations of class clusters in the embedding space of the trained transformer models, providing insights for future research. The contributions of this study can be summarized as follows:

\begin{itemize}
    \item This investigation achieves state-of-the-art accuracy for multiclass stress detection, which includes the categories of neutral, stress, and amusement, utilizing the public WESAD dataset.
    \item This investigation demonstrates the effectiveness of single-modality attention transformers in analyzing physiological signals for stress classification without requiring high-parameter multimodal models.
    \item To the best of the authors' knowledge, this study represents the first examination of cross-modal performance in stress detection and elucidates this phenomenon through an investigation of the learned embedding space of the trained transformer models.
\end{itemize}


\begin{figure*}[tp]
    \centering
    \includegraphics[]{Images/Transformer-Page-2.drawio__1_.pdf.pdf}
    \caption{Our Proposed Transformer Architecture}
    \label{fig:image7}
\end{figure*}


\section{Related Works}

Stress detection utilizing physiological signals has been extensively investigated through various machine learning (ML) and deep learning (DL) approaches. Research has focused on optimizing model architectures, feature engineering, and dataset preprocessing to enhance classification performance. This section reviews studies employing different physiological signals and particularly discusses those using the WESAD dataset for stress classification.

\subsection{Traditional and Early Machine Learning Approaches}
Early studies in stress detection utilized conventional machine learning techniques that required significant feature engineering. For instance, Heyat et al. \cite{bios12060427} demonstrated stress classification using decision trees applied to ECG signals collected from a smart T-shirt with an embedded cardiac electrode, while AlShorman et al. \cite{alshorman2022frontal} employed methods such as Support Vector Machines and Naive Bayes on features extracted via Fast Fourier Transform. Andric et al. \cite{andric2024anticipating} employed a Random Forest classifier on WESAD and the Stress-Predict dataset, achieving $96\%$ accuracy. While their preprocessing and feature extraction methods increase computational complexity, the approach in this study achieves high precision and recall with minimal preprocessing. Nazeer et al. \cite{nazeer2024improved} explored preprocessing strategies, context modeling, and branch classifiers using XGBoost for stress detection. Their feature extraction pipeline is comprehensive, but this study achieves comparable accuracy with less preprocessing. Mazumdar et al. \cite{mazumdarml} analyzed various classifiers and identified ECG, EDA, and temperature as key features, with XGBoost achieving the highest accuracy. Bhanushali et al. \cite{9184466} employed Decision Tree, Random Forest, and XGBoost classifiers for stress classification on WESAD. Although pioneering, these approaches frequently necessitated carefully designed preprocessing pipelines to address noise and variability in physiological data. 

\subsection{Deep Learning Approaches}

With the advent of deep learning and its aptness in ingesting raw unstructured data, researchers initiated investigations into Convolutional Neural Networks (CNNs), Long Short-Term Memory networks (LSTMs) and Transformer models to automatically learn representations from raw or minimally processed signals. Narwat et al. \cite{10463214} evaluated KNN, XGBoost, and CNN on WESAD, demonstrating the advantages of deep learning. Investigations such as those conducted by Mohapatra \cite{9964761} and Gil-Martin et al. \cite{9669993} used CNN models for stress detection on speech data across seven emotional categories from a Kaggle dataset and combined CNNs with Fourier Transform preprocessing and Leave-One-Subject-Out cross-validation, for classification across multiple stress and emotional states. 


LSTM-based models have demonstrated improvements over CNNs in capturing longitudinal relationships, as reported by Nigam et al. \cite{nigam2021improved} utilized Respiban chest sensors with LSTM to classify stress, optimizing hyperparameters to improve performance. Similarly, Nath et al. \cite{nath2022machine} implemented an LSTM-based stress detection model for older adults using BVP and EDA signals, comparing its performance to  traditional machine learning approaches of logistic regression, KNN, SVM, and Random Forest. Malviya and Mal et al. \cite{malviya2022novel} developed hybrid a CNN-BLSTM model incorporating Discrete Wavelet Transform (DWT) for stress classification, while Song et al. \cite{song2024stress} employed a combination of LSTM and Xception models to fuse 1D and spectrogram-based ECG features, achieving $99.51\%$ accuracy. While their approach slightly outperforms others in ECG classification, it requires higher computational resources. Additionally, Kumar et al. \cite{kumar2024deep} employed CNNs and time-frequency representations (TFRs), such as Short-Time Fourier Transformation (STFT) and Continuous Wavelet Transform (CWT) for ECG and EDA-based stress classification. More recently, transformer architectures have emerged as a promising alternative, primarily due to their capacity to capture long-range dependencies without substantial reliance on handcrafted features. Wu et al. \cite{wu2023transformer} proposed a multimodal transformer incorporating EDA, BVP, and temperature signals for stress classification across multiple datasets. Self-supervised learning (SSL) has been investigated for emotion recognition by Gotz et al. \cite{gotz2023self} who proposed a modality-agnostic Transformer (MATS2L) trained on ECG and EDA signals, demonstrating the benefits of SSL for feature extraction, while mitigating the necessity for extensive preprocessing.

To facilitate generalization and transferability, Albaladejo-Gonzales et al. \cite{albaladejo2023evaluating} examined transfer learning for stress detection using heart rate data from WESAD and SWELL-KW datasets, with a Multilayer Perceptron (MLP) demonstrating superior performance. Fang et al. \cite{fang2022towards} introduced a domain generalization framework trained on WESAD and BioVid to improve model transferability. Although their method enhances generalizability, it does not achieve the same accuracy as models trained without multi-dataset adaptation. On the other hand, Li et al. \cite{li2024comparison} analyzed both approaches and found that personalized models tend to perform better.

\subsection{Gaps and the Current Contribution}

The extant literature demonstrates significant advancements from traditional methodologies to sophisticated deep learning models. However, the majority of prior research exhibits two notable limitations. First, there exists a substantial reliance on complex feature extraction and preprocessing for traditional machine learning approaches, which may impede real-time application. Second, existing studies predominantly focus on optimizing performance within individual modalities, resulting in a paucity of understanding regarding cross-modal generalization. In contrast, this study not only adapts transformer models for direct application to raw physiological data but also systematically evaluates both intramodality accuracy and cross-modality generalization. Through the utilization of UMAP visualizations and quantitative variance analysis, this research provides novel insights into the transferability of learned representations across diverse sensor types, thereby addressing a critical gap in stress detection research.


\section{Background and Methodology}
 In this section, we introduce the WESAD dataset, which was used for training and evaluation of transformer models on the task of stress detection, followed by an introduction to the transformer architecture and the proposed architecture for this study.
 
\subsection{Dataset}
\label{dataset}
In this investigation, the Wearable Stress and Affect Detection (WESAD) dataset \cite{schmidt2018introducing}, specifically on the raw physiological data obtained from a chest-worn RespiBAN device, was used for experimentation. WESAD is a widely recognized resource in the field of affective computing, particularly for the detection and classification of emotional states, such as stress, amusement, and neutrality. The dataset comprised recordings of 15 participants, aged between 24 and 31 years, who were subjected to a series of controlled experimental conditions designed to elicit specific emotional states. Each participant underwent a sequence of activities including a neutral baseline period, an amusement phase induced by watching humorous video clips, a stress phase induced by the Trier Social Stress Test (TSST), and a recovery period. The labels in the WESAD dataset are structured as 0 for the neutral state, 1 for stress, and 2 for amusement. 

For this analysis, we exclusively utilized raw data collected from the RespiBAN chest device, which provides high-resolution physiological signals at a uniform sampling rate of 700 Hz. The signal modalities of interest include electrocardiogram (ECG), electrodermal activity (EDA), electromyography (EMG), respiration rate (RESP), temperature (TEMP), and three-axis accelerometer (ACC) data. All 15 participants in the dataset were included in the training and testing set in a random 85:15 ratio, allowing for a comprehensive analysis across a diverse group of subjects. This inclusive approach enables the development of robust models capable of generalizing across different individuals, thus enhancing the reliability and applicability of our findings for stress detection.


\subsection{Transformer Architecture}
\label{archi}


\subsubsection{Transformers}
The transformer architecture \cite{vaswani2017attention}, represents a significant advance in the design of neural networks, particularly for natural language processing (NLP) tasks. The transformer introduced the self-attention mechanism, enabling effective representation of both long-range dependencies and relationships within data using an encoder-decoder structure with a self-attention mechanism, which computes the relative importance of each element in a sequence with respect to all other elements. The encoder traditionally consists of a multihead self-attention mechanism and a position-wise fully connected feed-forward network, while the decoder mirrors this structure, but includes an additional layer for attending to the encoder's output allowing superior performance. 

Their success in NLP has led to variants and adaptations, such as Vision Transformers (ViTs) on 2D image modalities and 1D time-series signals, although in limited capacity. Two key components that enable this adaptation are patch embeddings and positional embeddings, which allow the model to effectively capture the spatial and sequential relationships in the data. Since Transformers lack an inherent understanding of the spatial structure of images, authors Dosovitskiy et al. \cite{dosovitskiy2020image} proposed the use of positional embeddings alongside patch embeddings for encoding the relative positions of the patches within the original image. This ensures that the model can recognize the spatial arrangement of patches, which is crucial for classification. For 1D time-series signals, the signal is divided into overlapping or non-overlapping segments, referred to as patches, which capture the temporal dynamics over a fixed window and is flattened and projected into a higher dimensional space to create a patch embedding \cite{wu2021autoformer}.

\subsubsection{Proposed Architecture}
Our transformer architecture follows a hierarchical design, beginning with an Input Layer where data is divided into smaller segments through Patch Creation and transformed into high-dimensional vectors via Patch Embedding. To retain spatial relationships, Positional Encoding is applied, allowing the model to capture sequential dependencies. The core of the model consists of multiple Attention Blocks, each incorporating multihead Self-Attention to focus on different aspects of the input simultaneously, followed by Dropout Layers for regularization, Layer Normalization for stable training, and Dense Layers with Relu activation functions to refine feature representations. These blocks process information iteratively, enhancing feature extraction and representation learning. Finally, the model outputs predictions through a Dense Layer followed by a Softmax Activation, producing a probability distribution over the target classes. In this model, we used the Adam optimizer for computational efficiency and minimal hyper-parameter tuning, with a learning rate of $0.0001$, to ensure gradual, stable weight updates. Categorical cross entropy was employed as the loss function for multi-class classification. All models were trained for $50$ epochs with a batch size of $32$ balancing efficiency and generalization while benefiting from mini-batch gradient descent. This architecture enables scalable, robust processing of structured data, ensuring generalization across diverse learning tasks.

\subsection{Experimental Design}

In this study, we focused on developing a state-of-the-art stress classifier using 1D transformer architecture and investigated cross-modality performance on the same task. Additionally, we discuss interpretations of the cross-modal performance through reduced dimensional visualizations and high-dimensional data variance. .

Our proposed workflow adheres to a structured processing of physiological data and classifying stress levels utilizing transformer-based deep learning models. This study uses raw physiological signals, including ECG, EDA, RESP, TEMP, EMG, and 3-axis ACC modalities obtained from the WESAD dataset as illustrated in Section \ref{dataset}. We trained multiple transformer models as illustrated in Section \ref{archi} on the individual physiological modalities and evaluated them on the same modality as well as on different modalities. The cross-modal evaluation assesses the generalization of models trained on one modality when applied to another, analyzing performance discrepancies through embedding space visualizations and quantitative variance analysis. 


\section{Experiments and Results}

In this section, we train and evaluate our model through two sets of experiments for a comprehensive evaluation of models of all the modalities of physiological data as well as the cross-generalization ability of the models trained and test on different physiological signals for the task of neutral, stress, and amusement classification. To the best of author's knowledge, this is the first investigation of cross-modal performance for stress detection. Our two experiments are structured as follows:
\begin{itemize}
    \item Experiment A: Models trained and tested on the same modality.
    \item Experiment B: Models trained and tested on different modalities. 
\end{itemize}

For experiment B, to evaluate the cross-modal performance, the models trained in experiment A were reused and tested on the other modalities to avoid introducing any biases from random initialization of weights or data batches during retraining.

\subsection{Experiment A: Models trained and tested on the same modality}
\label{expA}

\begin{table}[tp]
\centering
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Model Modality} & \textbf{Val Accuracy} & \textbf{Val Precision} & \textbf{Val Recall} \\ \hline
ECG & 0.9994 & 0.9995 & 0.9994 \\ \hline
EDA & 0.9995 & 0.9995 & 0.9995 \\ \hline
Resp & 0.9995 & 0.9995 & 0.9995 \\ \hline
Temp & 0.9993 & 0.9993 & 0.9993 \\ \hline
EMG & 0.9991 & 0.9991 & 0.9991 \\ \hline
ACC\_C1& 0.9986 & 0.9986 & 0.9986 \\ \hline
ACC\_C2& 0.9973 & 0.9973 & 0.9973 \\ \hline
ACC\_C3 & 0.9983 & 0.9983 & 0.9982 \\ \hline
\end{tabular}
\caption{Performance metrics of models trained and tested on individual physiological signal modalities for the task of stress classification.}
\label{tab:individual}
\end{table}


\begin{table*}[tp]
\centering
\renewcommand{\arraystretch}{0.8}
\resizebox{0.8\textwidth}{!}{%
\large
\begin{tabular}{|l|l|c|c|c|c|}
\hline
\textbf{Source Modality} & \textbf{Target Modality} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} & \textbf{Accuracy} \\ \hline
\multirow{7}{*}{ECG}    & EDA  & 0.92 & 0.93 & 0.92 & 0.93 \\ 
                         & Resp & 0.93 & 0.93 & 0.93 & 0.93 \\ 
                         & Temp & 0.92 & 0.93 & 0.92 & 0.93 \\ 
                         & EMG  & 0.92 & 0.93 & 0.92 & 0.93 \\ 
                         & ACC C1  & 0.55 & 0.52 & 0.53 & 0.52 \\
                         & ACC C2  & 0.54 & 0.53 & 0.53 & 0.53 \\
                         & ACC C3  & 0.54 & 0.53 & 0.53 & 0.53 \\
                         \hline
                        
\multirow{7}{*}{EDA}    & ECG  & 0.92 & 0.92 & 0.92 & 0.92 \\ 
                         & Resp & 0.93 & 0.93 & 0.93 & 0.93 \\ 
                         & Temp & 0.93 & 0.93 & 0.93 & 0.93 \\ 
                         & EMG  & 0.93 & 0.93 & 0.93 & 0.93 \\ 
                         & ACC C1  & 0.51 & 0.46 & 0.55 & 0.46 \\
                         & ACC C2  & 0.58 & 0.49 & 0.52 & 0.49 \\
                         & ACC C3  & 0.59 & 0.53 & 0.55 & 0.53 \\
                         \hline
                        
\multirow{7}{*}{Resp}   & ECG  & 0.93 & 0.93 & 0.93 & 0.93 \\ 
                         & EDA  & 0.93 & 0.93 & 0.93 & 0.93 \\ 
                         & Temp & 0.93 & 0.93 & 0.93 & 0.93 \\ 
                         & EMG  & 0.93 & 0.93 & 0.93 & 0.93 \\ 
                         & ACC C1  & 0.54 & 0.47 & 0.47 & 0.47 \\
                         & ACC C2  & 0.54 & 0.49 & 0.50 & 0.49 \\
                         & ACC C3  & 0.55 & 0.48 & 0.49 & 0.48 \\
                         \hline
                         
\multirow{7}{*}{Temp}   & ECG  & 0.93 & 0.93 & 0.93 & 0.93 \\ 
                         & EDA  & 0.93 & 0.93 & 0.93 & 0.93 \\ 
                         & Resp & 0.93 & 0.93 & 0.93 & 0.93 \\ 
                         & EMG  & 0.93 & 0.93 & 0.93 & 0.93 \\ 
                         & ACC C1  & 0.53 & 0.42 & 0.44 & 0.42 \\ 
                         & ACC C2  & 0.52 & 0.45 & 0.46 & 0.45 \\
                         & ACC C3  & 0.52 & 0.46 & 0.47 & 0.46 \\
                         \hline
\multirow{7}{*}{EMG}    & ECG  & 0.92 & 0.93 & 0.92 & 0.93 \\ 
                         & EDA  & 0.93 & 0.93 & 0.92 & 0.93 \\ 
                         & Resp & 0.92 & 0.93 & 0.92 & 0.93 \\ 
                         & Temp & 0.92 & 0.92 & 0.92 & 0.92 \\ 
                         & ACC C1  & 0.51 & 0.40 & 0.37 & 0.40 \\ 
                         & ACC C2  & 0.52 & 0.52 & 0.51 & 0.52 \\
                         & ACC C3  & 0.46 & 0.46 & 0.45 & 0.46 \\
                         \hline
\multirow{7}{*}{ACC C1} & ECG  & 0.49 & 0.34 & 0.22 & 0.34 \\ 
                         & EDA  & 0.49 & 0.34 & 0.22 & 0.34 \\ 
                         & Resp & 0.49 & 0.33 & 0.21 & 0.33 \\ 
                         & Temp & 0.49 & 0.33 & 0.21 & 0.33 \\ 
                         & EMG  & 0.50 & 0.34 & 0.22 & 0.34 \\ 
                         & ACC C2  & 0.47 & 0.50 & 0.48 & 0.50 \\
                         & ACC C3  & 0.46 & 0.48 & 0.47 & 0.48 \\
                         \hline
\multirow{7}{*}{ACC C2} & ECG  & 0.46 & 0.35 & 0.25 & 0.35 \\ 
                         & EDA  & 0.48 & 0.36 & 0.26 & 0.36 \\ 
                         & Resp & 0.46 & 0.35 & 0.24 & 0.35 \\ 
                         & Temp & 0.47 & 0.36 & 0.26 & 0.36 \\ 
                         & EMG  & 0.47 & 0.36 & 0.026 & 0.36 \\ 
                         & ACC C1  & 0.48 & 0.50 & 0.49 & 0.50 \\
                         & ACC C3  & 0.49 & 0.49 & 0.49 & 0.49 \\
                         \hline
\multirow{7}{*}{ACC C3} & ECG  & 0.48 & 0.35 & 0.26 & 0.35 \\ 
                         & EDA  & 0.47 & 0.35 & 0.25 & 0.35 \\ 
                         & Resp & 0.48 & 0.35 & 0.25 & 0.35 \\ 
                         & Temp & 0.48 & 0.35 & 0.26 & 0.35 \\ 
                         & EMG  & 0.48 & 0.35 & 0.26 & 0.35 \\ 
                         & ACC C1  & 0.47 & 0.49 & 0.48 & 0.49 \\
                         & ACC C2  & 0.49 & 0.50 & 0.49 & 0.50 \\
                         \hline
\end{tabular}%
}
\caption{Performance metrics of models cross tested on other modalities. Cross-modal performance metrics for transformer models trained on different physiological modalities and evaluated across target modalities.}
\label{tab:cross-testing}
\end{table*}


\begin{table*}[tp]
\centering
\captionsetup{justification=centering} % This centers the caption across the full table width
\resizebox{\textwidth}{!}{%
\begin{tabular}{|l|l|c|c|c|c|}
\hline
\textbf{Source Modality} & \textbf{Target Modality} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} & \textbf{Accuracy} \\ \hline
\multirow{3}{*}{ECG}   & ACC C1 & 0.55 & 0.52 & 0.53 & 0.52 \\ 
                       & ACC C2 & 0.54 & 0.53 & 0.53 & 0.53 \\ 
                       & ACC C3 & 0.54 & 0.53 & 0.53 & 0.53 \\ \hline
\multirow{3}{*}{EDA}   & ACC C1 & 0.51 & 0.46 & 0.55 & 0.46 \\ 
                       & ACC C2 & 0.58 & 0.49 & 0.52 & 0.49 \\ 
                       & ACC C3 & 0.59 & 0.53 & 0.55 & 0.53 \\ \hline
\multirow{3}{*}{Resp}  & ACC C1 & 0.54 & 0.47 & 0.47 & 0.47 \\ 
                       & ACC C2 & 0.54 & 0.49 & 0.50 & 0.49 \\ 
                       & ACC C3 & 0.55 & 0.48 & 0.49 & 0.48 \\ \hline
\multirow{3}{*}{Temp}  & ACC C1 & 0.53 & 0.42 & 0.44 & 0.42 \\ 
                       & ACC C2 & 0.52 & 0.45 & 0.46 & 0.45 \\ 
                       & ACC C3 & 0.52 & 0.46 & 0.47 & 0.46 \\ \hline
\multirow{3}{*}{EMG}   & ACC C1 & 0.51 & 0.40 & 0.37 & 0.40 \\ 
                       & ACC C2 & 0.52 & 0.52 & 0.51 & 0.52 \\ 
                       & ACC C3 & 0.46 & 0.46 & 0.45 & 0.46 \\ \hline
\multirow{2}{*}{ACC C1} & ACC C2 & 0.48 & 0.50 & 0.49 & 0.50 \\ 
                        & ACC C3 & 0.47 & 0.49 & 0.48 & 0.49 \\ \hline
\multirow{2}{*}{ACC C2} & ACC C1 & 0.47 & 0.50 & 0.48 & 0.50 \\ 
                        & ACC C3 & 0.49 & 0.50 & 0.49 & 0.50 \\ \hline
\multirow{2}{*}{ACC C3} & ACC C1 & 0.47 & 0.49 & 0.48 & 0.49 \\ 
                        & ACC C2 & 0.49 & 0.50 & 0.49 & 0.50 \\ \hline
\end{tabular}%
}
\caption{Testing Data Results for ACC C1, ACC C2, and ACC C3 (Weighted Average Metrics). This table shows the cross-modal performance of transformer models trained on various physiological modalities (ECG, EDA, Resp, Temp, EMG) and evaluated on accelerometer channels (ACC C1, ACC C2, and ACC C3).}
\label{tab:cross-acc}
\end{table*}

The results of this investigation on stress detection, for various physiological modalities as achieved through the transformer model illustrated in Section \ref{archi}, are recorded in Table \ref{tab:individual}. Each of the six modalities and three channels for the accelerometer data were evaluated based on validation accuracy, precision, and recall. Comparable performances were observed for all the modalities with \textit{EDA} and \textit{Respiration} emerging as the best performing modalities. As recorded in table \ref{tab:individual}, \textit{EDA} achieved a validation accuracy of $99.95\%$, with equally high validation precision and recall of $99.95\%$. Similarly, \textit{Respiration} demonstrated exceptional results, with a validation accuracy of $99.95\%$, precision of $99.95\%$, and recall of $99.95\%$. In comparison, other modalities such as \textit{ECG} achieved a comparable validation accuracy of $99.94\%$, with validation precision and recall of $99.95\%$ and $99.94\%$, respectively. \textit{Temperature} also performed strongly, with a validation accuracy of $99.93\%$, as well as precision and recall of $99.93\%$. \textit{EMG} with a comparable performance, achieved a validation accuracy of $99.91\%$, with validation precision and recall of $99.91\%$. The accelerometer data is 3-channel and thus three separate models were trained  on each of the three axes represented as \textit{C1,C2, and C3} respectively to comprehensively evaluate the viability and performance of each channel. Among the accelerometer channels, \textit{ACC\_C1}, \textit{ACC\_C2}, and \textit{ACC\_C3} exhibited validation accuracies of $99.86\%$, $99.73\%$, and $99.83\%$, respectively, with corresponding precision and recall values consistent across metrics. As can be observed from the reported results, all models architectures achieve comparable performances and can be used as mediums for stress detection from wearable sensors. The state-of-the-art results obtained are compared with existing work in the literature in Section \ref{comparison} demonstrating their improvement over other methods. Furthermore, the statistically insignificant difference in the results can be attributed to device and collection noise, which is not the investigation of this paper.


\subsection{Experiment B: Models trained and tested on different modalities}
\label{cross-modal}
In this section, we report the results of cross-modality generalization using the models trained in Experiment A on particular modalities. This is particularly done to avoid introducing any biases from random initialization of weights or data batches during retraining. To the best of author's knowledge, this is the first investigation on cross-modal generalization for the task of stress classification. The results summarized in Table \ref{tab:cross-testing} elucidate the performance of stress detection models trained with individual modalities and evaluated on various target modalities. Across the majority of source-target pairs, physiological signals such as ECG, EDA, Respiration (Resp), Temperature (Temp), and Electromyography (EMG) demonstrated consistent and high metrics, with precision, recall, F1-score, and accuracy exceeding $92\%$. This underscores the robustness of these modalities and transformer architecture for stress detection. Respiration and Temperature emerged as particularly reliable modalities, achieving uniformly high scores (precision, recall, F1, and accuracy of $93\%$) when evaluated against other modalities. EDA and ECG also exhibited robust performance, with metrics consistently around $92-93\%$, further emphasizing their utility in stress detection systems. 

In contrast, accelerometer data (ACC C1) exhibited significantly lower performance metrics, with accuracy ranging from $33\%$ to $52\%$, and corresponding reductions in precision, recall, and F1-scores. This reflects the inherent challenges of utilizing accelerometer data for stress detection, likely due to its sensitivity to motion artifacts and lower physiological specificity compared to other modalities. In conclusion, these findings suggest that integrating high-performing modalities such as Respiration, Temperature, EDA, EMG, and ECG can lead to robust stress detection systems. However, the limited performance of accelerometer data highlights the need for further research into preprocessing techniques or hybrid approaches to enhance its contribution in multi-modal frameworks.


\section{Interpreting Cross-Modality Performance}

In this section, we discuss the importance of embedding space, it's visualization and quantification to understand the cross-modal performances observed and reported in Section \ref{cross-modal}.

\subsection{Embedding Space}

Embedding space refers to a mathematical construct where high-dimensional data points, are represented as vectors in a lower-dimensional space. This space is structured such that similar data points are positioned in closer proximity, while dissimilar points are more distant. Embeddings are widely utilized in machine learning and deep learning to interpret the underlying relationships in data, facilitating tasks such as classification, clustering, or retrieval. Techniques such as Principal Component Analysis (PCA) \cite{mackiewicz1993principal}, t-SNE \cite{van2008visualizing}, and UMAP \cite{mcinnes2018umap} are frequently employed to visualize and analyze these spaces, elucidating the inherent patterns and structures within the data, including the development of prototypical spaces \cite{dakshit2024abstaining}. The structure of the embedding space is critical for model performance, as it directly influences the capacity to generalize and transfer learn representations to novel tasks or data \cite{bengio2013representation, mikolov2013distributed, radford2018improving, bojanowski2017enriching}. 


\begin{figure*}[tp]
    \centering
    % First row
    \begin{subfigure}{0.48\textwidth}
        \includegraphics[width=\linewidth]{Images/ECGSingleModality.png}
        \caption{ECG}
        \label{fig:image5}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.48\textwidth}
        \includegraphics[width=\linewidth]{Images/EDASingleModality.png}
        \caption{EDA}
        \label{fig:image4}
    \end{subfigure}

    \vspace{0.3cm} % Adjusted vertical space

    % Second row
    \begin{subfigure}{0.48\textwidth}
        \includegraphics[width=\linewidth]{Images/TempSingleModality.png}
        \caption{TEMP}
        \label{fig:image1}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.48\textwidth}
        \includegraphics[width=\linewidth]{Images/RespSingleModality.png}
        \caption{RESP}
        \label{fig:image2}
    \end{subfigure}

    \vspace{0.3cm} % Adjusted vertical space

    % Third row
    \begin{subfigure}{0.48\textwidth}
        \includegraphics[width=\linewidth]{Images/EMGSingleModality.png}
        \caption{EMG}
        \label{fig:image3}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.48\textwidth}
        \includegraphics[width=\linewidth]{Images/ACC_C1SingleModality.png}
        \caption{ACC C1}
        \label{fig:image6}
    \end{subfigure}

    \caption{UMAP Visualization of embedding space for modalities. Only Channel 1 is presented for ACC as this has the worst performance over other two channels.}
\label{UMAP}
\end{figure*}

\subsection{Visualization of embedding space}

Uniform Manifold Approximation and Projection (UMAP) \cite{mcinnes2018umap} is a dimensionality reduction technique extensively utilized to visualize high-dimensional data in a lower-dimensional space, predominantly 2D or 3D. It is particularly effective for embedding space visualization, in which complex high-dimensional feature representations are projected into an interpretable form. UMAP operates by constructing a high-dimensional graph representation of the data and subsequently optimizing a low-dimensional embedding that preserves the local and global structure of the original data. Compared to alternative methods such as t-SNE \cite{van2008visualizing}, UMAP demonstrates superior computational efficiency and enhanced preservation of global structure while maintaining meaningful local clusters. In our study, UMAP is used to project the high-dimensional embeddings generated by the transformer models into a 2D space. This visualization aids in understanding how well the model differentiates between the three stress states—Neutral, Stress, and Amusement across different physiological modalities through clustering.


Fig \ref{UMAP} presents UMAP visualization of the learned embedding spaces for various physiological modalities extracted from the trained transformer models illustrated in Experiment A. This visualization provides insights into each individual model's capacity to differentiate between stress states (Neutral, Stress, and Amusement). Each modality’s embedding space is displayed in a separate sub-figure as illustrated in Figure \ref{fig:image5} for ECG, Figure \ref{fig:image4} for EDA, Figure \ref{fig:image1} for TEMP, Figure \ref{fig:image2} for RESP, Figure \ref{fig:image3} for EMG and Figure \ref{fig:image6} for ACC C1. Legends within each figure denote the stress states. Additionally, it is prudent to note that we investigate only channel 1 for ACC as the representative channel, since ACC modality channels demonstrate poor cross-modal performance, with Channel 1 having the worst recall and precision consistently across all the investigated modalities. Various key observations from these visualizations explain the cross-modal performance.  

The embeddings for ECG in Figure \ref{fig:image5} and EDA in Figure \ref{fig:image4}, demonstrate well-defined clusters, indicating robust class separability and suggesting that these modalities effectively capture stress-related physiological changes, as evidenced by their high classification accuracy, precision, and recall. In contrast, the embeddings for TEMP Figure \ref{fig:image1} and RESP Figure \ref{fig:image2}, exhibit some overlap between clusters, suggesting moderate class separability. This visual observation is consistent with their slightly lower performance metrics in cross-modal evaluations, implying that although these modalities provide valuable information for stress classification, their exhibited increased variance impacts separability. The embeddings for EMG Figure \ref{fig:image3} and ACC C1 Figure \ref{fig:image6}, display significant overlap, particularly between neutral and amusement states, and correlates with the higher in-class variance reported in the later section and aligning with the lower cross-modal classification performance, underscoring the challenge of using these features for stress detection.

The UMAP visualizations provide a comprehensive summary of the model's internal feature representations. The distinct separation of clusters in modalities such as ECG and EDA corresponds with their superior quantitative metrics, while the overlapping clusters in TEMP, RESP, EMG, and ACC elucidate the relative challenges in generalization. These observations substantiate that reduced variance in the learned embeddings tends to result in improved classification and cross-modal transferability. Additionally, the observed clustering patterns corroborate the findings from the variance analysis presented in the next section, where higher variance in modalities such as EMG and ACC correlates with weaker generalization and lower cross-modal performance.

\subsection{Quantitative variance explanation}


\begin{table*}[tp]
    \centering
    \begin{tabular}{|c|c|c|c|}
        \hline
        Class Label & 0 & 1 & 2 \\
        \hline
        ECG & 3.044E-05 & 5.402E-04 & 2.728E-03 \\
        \hline
        EDA & 5.117E-04 & 3.889E-04 & 1.078E-03 \\
        \hline
        RESP & 3.994E-04 & 1.914E-04 & 2.355E-03 \\
        \hline
        TEMP & 2.199E-04 & 1.080E-04 & 7.786E-03 \\
        \hline
        EMG & 3.332E-06 & 5.116E-04 & 3.214E-03 \\
        \hline
        % Not sure if we need to update these or not
        ACC C1 (X Axis) & 1.493E-03 & 3.525-03 & 8.287E-04 \\
        \hline
        ACC C2 (Y Axis) & 1.182E-02 & 3.001E-03 & 4.098E-03 \\
        \hline
        ACC C3 (Z Axis) & 1.984E-03 & 1.946E-03 & 5.395E-03 \\
        \hline
    \end{tabular}
    \caption{In-Class Variance; Class 0: Neutral, Class 1: Stress, Class 2: Amusement}
    \label{tab:variance}
\end{table*}

Understanding the role of data variance is crucial in evaluating a model's generalization capabilities. Generalization refers to a model's ability to perform accurately on novel, previously unseen data, and is influenced by the bias-variance trade-off. High variance in a model can lead to overfitting, wherein the model captures noise in the training data, resulting in poor performance on unseen data. Conversely, low variance may cause underfitting, wherein the model oversimplifies the data patterns, also leading to suboptimal generalization. Balancing bias and variance is essential to achieve optimal model performance. In this section, we present the quantified in-class variance values, computed in the higher dimensional embedding space, to understand the impact on cross-modal performance. 


Cross-modality generalization refers to the ability of a model trained on one physiological modality, such as ECG, to effectively classify data from another modality, such as EDA. Utilizing the variance values from Table \ref{tab:variance}, we analyze the observed trends in cross-modality classification performance reported in Tables \ref{tab:cross-testing} and \ref{tab:cross-acc} across the three emotional states: Neutral (Class 0), Stress (Class 1), and Amusement (Class 2). This study investigates cross-modal performance through visualization of the cluster distribution in the learned embedding space and, quantitatively by computing the in-class variance for each of the neutral, stress and amusement states. The visualization results are illustrated in Figure \ref{UMAP} (a-f) and the variance values are presented in Table \ref{tab:variance}. 

For the neutral class (Class 0), Table \ref{tab:cross-testing} and Table \ref{tab:cross-acc} jointly indicate that most physiological signals generalize effectively across modalities, with the exception of accelerometer-based features (ACC C1, C2, C3), which demonstrate poor performance. This trend can be elucidated by the variance values in Table \ref{tab:variance}, where ECG, EDA, RESP, EMG and TEMP exhibit relatively low to moderate variance with values ECG: 3.044E-05, RESP: 2.1994E-04, EDA: 5.177E-04, TEMP: 2.199E-04, and EMG: 3.322E-06. These lower variance levels suggest that these features maintain relative stability in neutral conditions, thereby enhancing their transferability. In contrast, the accelerometer data exhibit significantly higher variance (e.g., ACC C1: 1.493E-03, ACC C2: 1.182E-02), indicating that motion artifacts contribute to the difficulty in neutral state classification. Given that low-variance features typically enhance generalization, the low variance of non-accelerometer modalities supports their strong generalization in Table \ref{tab:cross-testing}, whereas the high variance of accelerometer data results in poor cross-modality performance. This is further justified by the theory that models trained on low variance data do not generalize well to high-variance datasets \cite{okazaki2023estimator}.  


For  stress (Class 1), investigation results demonstrate robust cross-modal generalization among ECG, EDA, RESP, EMG and TEMP, with precision and recall values consistently approximating 92-93\%. This observation is corroborated by the variance values presented in Table \ref{tab:variance}, where ECG (5.402E-04), EDA (3.889E-04), RESP (1.914E-04), EMG (5.116E-04) and TEMP (1.080E-04) exhibit comparable levels of variance, indicating that physiological responses to stress are relatively consistent across modalities, further substantiating its reliability in stress detection. Contrastingly, ACC C1 exhibits a higher variance of 3.525E-03, which results in its diminished generalization performance as evidenced in Tables \ref{tab:cross-testing} and \ref{tab:cross-acc}. Given that stress induces distinct physiological changes, the relatively consistent variance across ECG, EDA, RESP, TEMP, and EMG facilitates robust cross-modality generalization, whereas the high variance of accelerometer data impedes its effective generalization between modalities.  

For state of amusement (Class 2), Tables \ref{tab:cross-testing} and \ref{tab:cross-acc} demonstrate that cross-modal generalization remains high for ECG, EDA, and RESP, but exhibits a slight decrease in cross performance on ACC for TEMP and EMG. Additionally, ACC modalities fail to generalize to any other modalities as reported in Table \ref{tab:cross-testing}. The variance values in Table \ref{tab:variance} reveal that amusement exhibits the highest variance across all features, particularly in TEMP (7.786E-03) and EMG (3.214E-03). The elevated variance in these modalities suggests substantial physiological fluctuations during amusement, such as increased body temperature or muscle activity associated with laughter, rendering them less transferable across modalities. High variance generally results in weaker generalization due to increased feature variability, which is reflected in the lower cross-modal accuracy for TEMP and EMG. However, EDA and RESP, which maintain relatively stable variance levels, continue to achieve strong cross-modal generalization. 

The cross-modality performances observed can be elucidated by examining the variance characteristics of diverse physiological signals. Physiological features with low variance, such as ECG, EDA, and RESP, demonstrate superior generalization across modalities, whereas features exhibiting higher variance, including TEMP, EMG, and ACC, display diminished generalization capabilities. Stress classification benefits from a balanced variance distribution across features, resulting in optimal cross-modality performance. Conversely, amusement classification is more susceptible to high variance, particularly in TEMP and EMG, which compromises their capacity for effective generalization. These findings underscore the significance of variance in determining the robustness of physiological features for cross-modality generalization. 



\section{Comparison to Existing Work}
\label{comparison}

\begin{figure*}[tp] % or [tp] for better top-page enforcement
    \centering
    \includegraphics[width=\textwidth, height=7cm]{Images/AccuracyF1.png}
    \caption{Comparison of reported accuracy and F1 scores of existing research on WESAD.}
    \label{image5}
\end{figure*}

\begin{figure*}[tp]
    \centering
    \includegraphics[width=\textwidth, height=7cm]{Images/AccuracyPrecisionRecall.png}
    \caption{Comparison of accuracy, precision, and recall of existing research on WESAD.}
    \label{image6}
\end{figure*}

In this section, we compare the performance of our proposed model with existing research literature. The literature on stress detection using WESAD is extensive; therefore, only comparable works with either reported accuracy and F1 score (Figure \ref{image5}) or Precision, Recall and Accuracy (Figure \ref{image6}) are discussed in this section. It is important to note that, to the best of our knowledge and search efforts, we did not identify any research on cross-modality investigation, which is consequently not compared in this section.

Garg et al. \cite{garg2021stress} focused on stress detection using multimodal data, emphasizing the role of wearable sensors and machine learning (ML)-based classifiers. The authors integrated physiological signals with contextual data to enhance classification accuracy. A significant contribution of their work is the fusion of multiple sensor data streams for more robust stress detection. Their study reports F1-scores of 67.6\% and 65.73\% for three-class classification. Additionally, Schmidt et al. \cite{schmidt2018introducing} introduced the WESAD dataset, which has become a benchmark for numerous stress detection models, significantly contributing to reproducibility in stress research. Their models achieved an accuracy of 76. 5\% and an F1 score of 72. 5\% for the classification of three classes. Kumar et al. \cite{kumar2024resp} employed convolutional neural networks (CNNs) and recurrent neural networks (RNNs), reporting an accuracy of 87.7\% and an F1-score of 85.2\%. Similarly, Li et al. \cite{info:doi/10.2196/52171} achieved a precision of 95.1\% and an F1 score of 91.7\% using a CNN-based encoder with a feedforward neural network. Narwat et al. \cite{10463214} report high performance across three machine learning models for stress classification, achieving 96.8\% with XGBoost, 95.5\% with KNN. In comparison, Mazumdar et al. \cite{mazumdarml} achieved a precision of 98.8\% and a F1 score of 98.7\% on the same task and setup using XGBoost. As illustrated in Figure \ref{image5}, the performance metrics reported in these studies demonstrate advances in stress classification. Our proposed transformer model surpasses the best-performing model, improving accuracy by 1.1\% and the F1-score by 1.2\%. 

Additionally, in Figure \ref{image6}, we compare the performance of our transformer model with relevant works from the literature, focusing on precision and recall metrics in addition to accuracy and F1-score. Narwat et al. \cite{10463214} reported high performance in multiclass stress classification, achieving 98.7\% with CNN architectures for precision and recall. In contrast, Benita et al. \cite{10481604} achieved high accuracy using a CNN architecture but reported poor precision and recall scores, indicating significant overfitting to irrelevant features. Similarly, Andric et al. \cite{10.1007/978-3-031-66538-7_39} demonstrate a model that fails to learn relevant classification features, as evidenced by an accuracy significantly higher than its precision and recall.

Compared to prior research on the WESAD dataset for three-class stress classification, our transformer model stands out, achieving the highest accuracy, precision, and recall, demonstrating the superiority of transformer architectures for this task. Furthermore, we are unable to compare our cross-testing results due to the absence of reported cross-modality evaluations on WESAD or any other dataset for stress classification.

\section{Conclusion}

Stress detection utilizing physiological signals has emerged as a critical area of research, particularly with the increasing adoption of wearable technologies for real-time health monitoring. However, ensuring robust classification performance across diverse sensor modalities remains a significant challenge due to variations in physiological responses and data distributions. This study develops and evaluates transformer-based models for stress classification using the WESAD dataset, demonstrating state-of-the-art accuracy across multiple physiological modalities, including ECG, EDA, RESP, TEMP, EMG, and 3-axis ACC on data collected from a wearable chest sensor. Our investigation elucidates the effectiveness of self-attention mechanisms in capturing intricate temporal dependencies within physiological signals, enabling high-performance stress classification, achieving scores up to $99.9\%$, without extensive preprocessing.  Furthermore, a comprehensive cross-modality evaluation, reveals how variance and distribution in embedding space influences generalizability across modalities. Future research will explore multi-modal fusion techniques and domain adaptation strategies to further enhance cross-modality generalization, enabling more reliable stress detection models along with the ability to learn cross-generalizable feature association between different modalities for improved generalization.


%%
%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.
\begin{acks}
This research paper was revised with the assistance of AI-based tools for language refinement and grammatical corrections. However, the analysis, results and their interpretation and overall scholarly contributions remain the sole responsibility of the authors.
\end{acks}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-base}



\end{document}
\endinput
%%
%% End of file `sample-manuscript.tex'.
