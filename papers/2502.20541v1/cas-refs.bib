@misc{touvron2023llama,
      title={Llama 2: Open Foundation and Fine-Tuned Chat Models}, 
      author={Hugo Touvron et al.},
      year={2023},
      eprint={2307.09288},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@book{mctear2022conversational,
  title={Conversational ai: Dialogue systems, conversational agents, and chatbots},
  author={McTear, Michael},
  year={2022},
  publisher={Springer Nature}
}

@article{yang2024automatic,
  title={Automatic text summarization for government news reports based on multiple features},
  author={Yang, Yanni and Tan, Yiting and Min, Jintao and Huang, Zhengwei},
  journal={The Journal of Supercomputing},
  volume={80},
  number={3},
  pages={3212--3228},
  year={2024},
  publisher={Springer}
}

@article{van2024adapted,
  title={Adapted large language models can outperform medical experts in clinical text summarization},
  author={Van Veen, Dave and Van Uden, Cara and Blankemeier, Louis and Delbrouck, Jean-Benoit and Aali, Asad and Bluethgen, Christian and Pareek, Anuj and Polacin, Malgorzata and Reis, Eduardo Pontes and Seehofnerov{\'a}, Anna and others},
  journal={Nature medicine},
  volume={30},
  number={4},
  pages={1134--1142},
  year={2024},
  publisher={Nature Publishing Group US New York}
}

@article{nllb2024scaling,
  title={Scaling neural machine translation to 200 languages},
  author={NLLB Team and others},
  journal={Nature},
  volume={630},
  number={8018},
  pages={841},
  year={2024},
  publisher={Nature Publishing Group}
}

@book{alto2023modern,
  title={Modern Generative AI with ChatGPT and OpenAI Models: Leverage the capabilities of OpenAI's LLM for productivity and innovation with GPT3 and GPT4},
  author={Alto, Valentina},
  year={2023},
  publisher={Packt Publishing Ltd}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@inproceedings{liu2021robustly,
  title={A robustly optimized BERT pre-training approach with post-training},
  author={Liu, Zhuang and Lin, Wayne and Shi, Ya and Zhao, Jun},
  booktitle={China National Conference on Chinese Computational Linguistics},
  pages={471--484},
  year={2021},
  organization={Springer}
}

@article{lan2019albert,
  title={Albert: A lite bert for self-supervised learning of language representations},
  author={Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
  journal={arXiv preprint arXiv:1909.11942},
  year={2019}
}

@article{sanh2019distilbert,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  journal={arXiv preprint arXiv:1910.01108},
  year={2019}
}

@article{rogers2021primer,
  title={A primer in BERTology: What we know about how BERT works},
  author={Rogers, Anna and Kovaleva, Olga and Rumshisky, Anna},
  journal={Transactions of the Association for Computational Linguistics},
  volume={8},
  pages={842--866},
  year={2021},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@article{lee2020biobert,
  title={BioBERT: a pre-trained biomedical language representation model for biomedical text mining},
  author={Lee, Jinhyuk and Yoon, Wonjin and Kim, Sungdong and Kim, Donghyeon and Kim, Sunkyu and So, Chan Ho and Kang, Jaewoo},
  journal={Bioinformatics},
  volume={36},
  number={4},
  pages={1234--1240},
  year={2020},
  publisher={Oxford University Press}
}

@article{buehler2024mechgpt,
  title={MechGPT, a Language-Based Strategy for Mechanics and Materials Modeling That Connects Knowledge Across Scales, Disciplines, and Modalities},
  author={Buehler, Markus J},
  journal={Applied Mechanics Reviews},
  volume={76},
  number={2},
  pages={021001},
  year={2024},
  publisher={American Society of Mechanical Engineers}
}



@article{chandrasekhar2024amgpt,
  title={AMGPT: a Large Language Model for Contextual Querying in Additive Manufacturing},
  author={Chandrasekhar, Achuth and Chan, Jonathan and Ogoke, Francis and Ajenifujah, Olabode and Farimani, Amir Barati},
  journal={arXiv preprint arXiv:2406.00031},
  year={2024}
}

@article{lewis2020retrieval,
  title={Retrieval-augmented generation for knowledge-intensive nlp tasks},
  author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={9459--9474},
  year={2020}
}


@article{cao2024machine,
  title={Machine Learning in Membrane Design: From Property Prediction to AI-Guided Optimization},
  author={Cao, Zhonglin and Barati Farimani, Omid and Ock, Janghoon and Barati Farimani, Amir},
  journal={Nano Letters},
  volume={24},
  number={10},
  pages={2953--2960},
  year={2024},
  publisher={ACS Publications}
}

@article{barati2024fast,
  title={Fast Water Desalination with a Graphene--MoS2 Nanoporous Heterostructure},
  author={Barati Farimani, Omid and Cao, Zhonglin and Barati Farimani, Amir},
  journal={ACS Applied Materials \& Interfaces},
  year={2024},
  publisher={ACS Publications}
}

@inproceedings{reimers-gurevych-2019-sentence,
    title = "Sentence-{BERT}: Sentence Embeddings using {S}iamese {BERT}-Networks",
    author = "Reimers, Nils  and
      Gurevych, Iryna",
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    pages = "3982--3992",
    abstract = "BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations ({\textasciitilde}65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.",
}

@article{weisemantic,
author = {Wei, Wang and Barnaghi, Payam and Bargiela, Andrzej},
year = {2008},
month = {01},
pages = {},
title = {Search with meanings: An overview of semantic search systems},
volume = {3},
journal = {International Journal of Communications of SIWN}
}

@inproceedings{devlin-etal-2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

@inproceedings{lu-etal-2022-clinicalt5,
    title = "{C}linical{T}5: A Generative Language Model for Clinical Text",
    author = "Lu, Qiuhao  and
      Dou, Dejing  and
      Nguyen, Thien",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    pages = "5436--5443",
    abstract = "In the past few years, large pre-trained language models (PLMs) have been widely adopted in different areas and have made fundamental improvements over a variety of downstream tasks in natural language processing (NLP). Meanwhile, domain-specific variants of PLMs are being proposed to address the needs of domains that demonstrate a specific pattern of writing and vocabulary, e.g., BioBERT for the biomedical domain and ClinicalBERT for the clinical domain. Recently, generative language models like BART and T5 are gaining popularity with their competitive performance on text generation as well as on tasks cast as generative problems. However, in the clinical domain, such domain-specific generative variants are still underexplored. To address this need, our work introduces a T5-based text-to-text transformer model pre-trained on clinical text, i.e., ClinicalT5. We evaluate the proposed model both intrinsically and extrinsically over a diverse set of tasks across multiple datasets, and show that ClinicalT5 dramatically outperforms T5 in the domain-specific tasks and compares favorably with its close baselines.",
}

@inproceedings{aiayn,
  title     = {Attention is All you Need},
  author    = {Ashish Vaswani and Noam M. Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2017},
  pages     = {5998--6008}
}

@article{gpt3,
  author       = {Tom B. Brown and
                  Benjamin Mann and
                  Nick Ryder and
                  Melanie Subbiah and
                  Jared Kaplan and
                  Prafulla Dhariwal and
                  Arvind Neelakantan and
                  Pranav Shyam and
                  Girish Sastry and
                  Amanda Askell and
                  Sandhini Agarwal and
                  Ariel Herbert{-}Voss and
                  Gretchen Krueger and
                  Tom Henighan and
                  Rewon Child and
                  Aditya Ramesh and
                  Daniel M. Ziegler and
                  Jeffrey Wu and
                  Clemens Winter and
                  Christopher Hesse and
                  Mark Chen and
                  Eric Sigler and
                  Mateusz Litwin and
                  Scott Gray and
                  Benjamin Chess and
                  Jack Clark and
                  Christopher Berner and
                  Sam McCandlish and
                  Alec Radford and
                  Ilya Sutskever and
                  Dario Amodei},
  title        = {Language Models are Few-Shot Learners},
  journal      = {CoRR},
  volume       = {abs/2005.14165},
  year         = {2020},
  eprinttype    = {arXiv},
  eprint       = {2005.14165},
  timestamp    = {Thu, 25 May 2023 10:38:31 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2005-14165.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@software{Liu_2022,
  cff-version = {1.2.0},
  title = {LlamaIndex},
  url = {https://github.com/jerryjliu/llama_index},
  author = {Liu, Jerry},
  orcid = {0000-0002-6694-3517},
  date-released = {2022-11-1}
}

@software{Chase_2022,
  cff-version = {1.2.0},
  title = {LangChain},
  url = {https://github.com/langchain-ai/langchain},
  author = {Chase, Harrison},
  date-released = {2022-10-17},
  message = {If you use this software, please cite it as below.}
}

@misc{melz2023enhancing,
      title={Enhancing LLM Intelligence with ARM-RAG: Auxiliary Rationale Memory for Retrieval Augmented Generation}, 
      author={Eric Melz},
      year={2023},
      eprint={2311.04177},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{lewis_rag,
 author = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K\"{u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt\"{a}schel, Tim and Riedel, Sebastian and Kiela, Douwe},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {9459--9474},
 publisher = {Curran Associates, Inc.},
 title = {Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks},
 volume = {33},
 year = {2020}
}

@misc{chen2023benchmarking,
      title={Benchmarking Large Language Models in Retrieval-Augmented Generation}, 
      author={Jiawei Chen and Hongyu Lin and Xianpei Han and Le Sun},
      year={2023},
      eprint={2309.01431},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{mao,
  author = {Yuning Mao and Pengcheng He and Xiaodong Liu and Yelong Shen and Jianfeng Gao and Jiawei Han and Weizhu Chen},
  year = {2021},
  month = {01},
  pages = {4089--4100},
  title = {Generation-Augmented Retrieval for Open-Domain Question Answering},
  booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)}
}


@misc{pal2023medhalt,
      title={Med-HALT: Medical Domain Hallucination Test for Large Language Models}, 
      author={Ankit Pal and Logesh Kumar Umapathi and Malaikannan Sankarasubbu},
      year={2023},
      eprint={2307.15343},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{mikolov2013efficient,
      title={Efficient Estimation of Word Representations in Vector Space}, 
      author={Tomas Mikolov and Kai Chen and Greg Corrado and Jeffrey Dean},
      year={2013},
      eprint={1301.3781},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@book{manning2008introduction,
  title={Introduction to Information Retrieval},
  author={Manning, C.D. and Raghavan, P. and Sch{\"u}tze, H.},
  isbn={9781139472104},
  year={2008},
  publisher={Cambridge University Press}
}

@article{elmo,
  author       = {Matthew E. Peters and
                  Mark Neumann and
                  Mohit Iyyer and
                  Matt Gardner and
                  Christopher Clark and
                  Kenton Lee and
                  Luke Zettlemoyer},
  title        = {Deep contextualized word representations},
  journal      = {CoRR},
  volume       = {abs/1802.05365},
  year         = {2018},
  eprinttype    = {arXiv},
  eprint       = {1802.05365},
  timestamp    = {Mon, 13 Aug 2018 16:48:54 +0200},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{Khandelwal2019GeneralizationTM,
  title={Generalization through Memorization: Nearest Neighbor Language Models},
  author={Urvashi Khandelwal and Omer Levy and Dan Jurafsky and Luke Zettlemoyer and Mike Lewis},
  journal={ArXiv},
  year={2019},
  volume={abs/1911.00172},
}
@misc{li2022survey,
      title={A Survey on Retrieval-Augmented Text Generation}, 
      author={Huayang Li and Yixuan Su and Deng Cai and Yan Wang and Lemao Liu},
      year={2022},
      eprint={2202.01110},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{Raffel2019ExploringTL,
  title={Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  author={Colin Raffel and Noam M. Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  journal={J. Mach. Learn. Res.},
  year={2019},
  volume={21},
  pages={140:1-140:67}
}

@article{Radford2018ImprovingLU,
  title={Improving Language Understanding by Generative Pre-Training},
  author={Alec Radford and Karthik Narasimhan},
  year={2018},
  journal={OpenAI Technical Report}
}

@article{Touvron2023LLaMAOA,
  title={LLaMA: Open and Efficient Foundation Language Models},
  author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timoth{\'e}e Lacroix and Baptiste Rozi{\`e}re and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
  journal={ArXiv},
  year={2023},
  volume={abs/2302.13971}
}
@article{Jiang2024MixtralOE,
  title={Mixtral of Experts},
  author={Albert Q. Jiang and Alexandre Sablayrolles and Antoine Roux and Arthur Mensch and Blanche Savary and Chris Bamford and Devendra Singh Chaplot and Diego de Las Casas and Emma Bou Hanna and Florian Bressand and Gianna Lengyel and Guillaume Bour and Guillaume Lample and L'elio Renard Lavaud and Lucile Saulnier and Marie-Anne Lachaux and Pierre Stock and Sandeep Subramanian and Sophia Yang and Szymon Antoniak and Teven Le Scao and Th{\'e}ophile Gervet and Thibaut Lavril and Thomas Wang and Timoth{\'e}e Lacroix and William El Sayed},
  journal={ArXiv},
  year={2024},
  volume={abs/2401.04088},
}

@inproceedings{Beltagy2019SciBERTAP,
  title={SciBERT: A Pretrained Language Model for Scientific Text},
  author={Iz Beltagy and Kyle Lo and Arman Cohan},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  year={2019},
  pages={3615--3620}
}

@article{Lee2019BioBERTAP,
  title={BioBERT: a pre-trained biomedical language representation model for biomedical text mining},
  author={Jinhyuk Lee and Wonjin Yoon and Sungdong Kim and Donghyeon Kim and Sunkyu Kim and Chan Ho So and Jaewoo Kang},
  journal={Bioinformatics},
  year={2019},
  volume={36},
  pages={1234 - 1240},
}

@article{Huang2019ClinicalBERTMC,
  title={ClinicalBERT: Modeling Clinical Notes and Predicting Hospital Readmission},
  author={Kexin Huang and Jaan Altosaar and Rajesh Ranganath},
  journal={ArXiv},
  year={2019},
  volume={abs/1904.05342}
}

@article{Buehler2024GenerativeRO,
  title={Generative Retrieval-Augmented Ontologic Graph and Multiagent Strategies for Interpretive Large Language Model-Based Materials Design},
  author={Markus J. Buehler},
  journal={ACS Engineering Au},
  year={2024},
  volume={4},
  pages={241 - 277}
}

@article{Chithrananda2020ChemBERTaLS,
  title={ChemBERTa: Large-Scale Self-Supervised Pretraining for Molecular Property Prediction},
  author={Seyone Chithrananda and Gabriel Grand and Bharath Ramsundar},
  journal={ArXiv},
  year={2020},
  volume={abs/2010.09885},
}
@article{Gao2022GeoBERTPG,
  title={GeoBERT: Pre-Training Geospatial Representation Learning on Point-of-Interest},
  author={Yunfan Gao and Yun Xiong and Siqi Wang and Haofen Wang},
  journal={Applied Sciences},
  year={2022},
}

@inproceedings{Lewis2019BARTDS,
  title={BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension},
  author={Mike Lewis and Yinhan Liu and Naman Goyal and Marjan Ghazvininejad and Abdel-rahman Mohamed and Omer Levy and Veselin Stoyanov and Luke Zettlemoyer},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  year={2020},
  pages={7871--7880}
}


@misc{yuan2022biobart,
      title={BioBART: Pretraining and Evaluation of A Biomedical Generative Language Model}, 
      author={Hongyi Yuan and Zheng Yuan and Ruyi Gan and Jiaxing Zhang and Yutao Xie and Sheng Yu},
      year={2022},
      eprint={2204.03905},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{wang2023survey,
      title={Survey on Factuality in Large Language Models: Knowledge, Retrieval and Domain-Specificity}, 
      author={Cunxiang Wang and Xiaoze Liu and Yuanhao Yue and Xiangru Tang and Tianhang Zhang and Cheng Jiayang and Yunzhi Yao and Wenyang Gao and Xuming Hu and Zehan Qi and Yidong Wang and Linyi Yang and Jindong Wang and Xing Xie and Zheng Zhang and Yue Zhang},
      year={2023},
      eprint={2310.07521},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{bran2023chemcrow,
      title={ChemCrow: Augmenting large-language models with chemistry tools}, 
      author={Andres M Bran and Sam Cox and Oliver Schilter and Carlo Baldassari and Andrew D White and Philippe Schwaller},
      year={2023},
      eprint={2304.05376},
      archivePrefix={arXiv},
      primaryClass={physics.chem-ph}
}

@misc{mccabe2023multiple,
      title={Multiple Physics Pretraining for Physical Surrogate Models}, 
      author={Michael McCabe and Bruno Régaldo-Saint Blancard and Liam Holden Parker and Ruben Ohana and Miles Cranmer and Alberto Bietti and Michael Eickenberg and Siavash Golkar and Geraud Krawezik and Francois Lanusse and Mariel Pettee and Tiberiu Tesileanu and Kyunghyun Cho and Shirley Ho},
      year={2023},
      eprint={2310.02994},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{lanusse2023astroclip,
      title={AstroCLIP: Cross-Modal Pre-Training for Astronomical Foundation Models}, 
      author={Francois Lanusse and Liam Parker and Siavash Golkar and Miles Cranmer and Alberto Bietti and Michael Eickenberg and Geraud Krawezik and Michael McCabe and Ruben Ohana and Mariel Pettee and Bruno Regaldo-Saint Blancard and Tiberiu Tesileanu and Kyunghyun Cho and Shirley Ho},
      year={2023},
      eprint={2310.03024},
      archivePrefix={arXiv},
      primaryClass={astro-ph.IM}
}

@misc{zhu2024large,
      title={Can Large Language Models Understand Context?}, 
      author={Yilun Zhu and Joel Ruben Antony Moniz and Shruti Bhargava and Jiarui Lu and Dhivya Piraviperumal and Site Li and Yuan Zhang and Hong Yu and Bo-Hsiang Tseng},
      year={2024},
      eprint={2402.00858},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{Ock2023CatalystPP,
author = {Ock, Janghoon and Guntuboina, Chakradhar and Barati Farimani, Amir},
title = {Catalyst Energy Prediction with CatBERTa: Unveiling Feature Exploration Strategies through Large Language Models},
journal = {ACS Catalysis},
volume = {13},
number = {24},
pages = {16032-16044},
year = {2023},
doi = {10.1021/acscatal.3c04956},
eprint = {https://doi.org/10.1021/acscatal.3c04956}
}

@inproceedings{NEURIPS2022_b1efde53,
 author = {Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul F and Leike, Jan and Lowe, Ryan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {27730--27744},
 publisher = {Curran Associates, Inc.},
 title = {Training language models to follow instructions with human feedback},
 volume = {35},
 year = {2022}
}

@misc{mukherjee2024polaris,
      title={Polaris: A Safety-focused LLM Constellation Architecture for Healthcare}, 
      author={Subhabrata Mukherjee and Paul Gamble and Markel Sanz Ausin and Neel Kant and Kriti Aggarwal and Neha Manjunath and Debajyoti Datta and Zhengliang Liu and Jiayuan Ding and Sophia Busacca and Cezanne Bianco and Swapnil Sharma and Rae Lasko and Michelle Voisard and Sanchay Harneja and Darya Filippova and Gerry Meixiong and Kevin Cha and Amir Youssefi and Meyhaa Buvanesh and Howard Weingram and Sebastian Bierman-Lytle and Harpreet Singh Mangat and Kim Parikh and Saad Godil and Alex Miller},
      year={2024},
      eprint={2403.13313},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@inproceedings{liu-etal-2023-llm,
    title = "{LLM}-{FP}4: 4-Bit Floating-Point Quantized Transformers",
    author = "Liu, Shih-yang  and
      Liu, Zechun  and
      Huang, Xijie  and
      Dong, Pingcheng  and
      Cheng, Kwang-Ting",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    pages = "592--605",
    abstract = "We propose LLM-FP4 for quantizing both weights and activations in large language models (LLMs) down to 4-bit floating-point values, in a post-training manner. Existing post-training quantization (PTQ) solutions are primarily integer-based and struggle with bit widths below 8 bits. Compared to integer quantization, floating-point (FP) quantization is more flexible and can better handle long-tail or bell-shaped distributions, and it has emerged as a default choice in many hardware platforms. One characteristic of FP quantization is that its performance largely depends on the choice of exponent bits and clipping range. In this regard, we construct a strong FP-PTQ baseline by searching for the optimal quantization parameters. Furthermore, we observe a high inter-channel variance and low intra-channel variance pattern in activation distributions, which adds activation quantization difficulty. We recognize this pattern to be consistent across a spectrum of transformer models designed for diverse tasks such as LLMs, BERT, and Vision Transformer models. To tackle this, we propose per-channel activation quantization and show that these additional scaling factors can be reparameterized as exponential biases of weights, incurring a negligible cost. Our method, for the first time, can quantize both weights and activations in the LLaMA-13B to only 4-bit and achieves an average score of 63.1 on the common sense zero-shot reasoning tasks, which is only 5.8 lower than the full-precision model, significantly outperforming the previous state-of-the-art by 12.7 points. Code is available at: https://github.com/nbasyl/LLM-FP4.",
}

@misc{gong2024makes,
      title={What Makes Quantization for Large Language Models Hard? An Empirical Study from the Lens of Perturbation}, 
      author={Zhuocheng Gong and Jiahao Liu and Jingang Wang and Xunliang Cai and Dongyan Zhao and Rui Yan},
      year={2024},
      eprint={2403.06408},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{li2024evaluating,
      title={Evaluating Quantized Large Language Models}, 
      author={Shiyao Li and Xuefei Ning and Luning Wang and Tengxuan Liu and Xiangsheng Shi and Shengen Yan and Guohao Dai and Huazhong Yang and Yu Wang},
      year={2024},
      eprint={2402.18158},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{jadhav2024llm,
  title={LLM-3D Print: Large Language Models To Monitor and Control 3D Printing},
  author={Jadhav, Yayati and Pak, Peter and Farimani, Amir Barati},
  journal={arXiv preprint arXiv:2408.14307},
  year={2024}
}

@article{mechgpt2024buehler,
    author = {Buehler, Markus J.},
    title = "{MechGPT, a Language-Based Strategy for Mechanics and Materials Modeling That Connects Knowledge Across Scales, Disciplines, and Modalities}",
    journal = {Applied Mechanics Reviews},
    volume = {76},
    number = {2},
    pages = {021001},
    year = {2024},
    month = {01},
    issn = {0003-6900}
}

@misc{hu2021lora,
      title={LoRA: Low-Rank Adaptation of Large Language Models}, 
      author={Edward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
      year={2021},
      eprint={2106.09685},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{dettmers2023qlora,
      title={QLoRA: Efficient Finetuning of Quantized LLMs}, 
      author={Tim Dettmers and Artidoro Pagnoni and Ari Holtzman and Luke Zettlemoyer},
      year={2023},
      eprint={2305.14314},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{chiang2024llamp,
      title={LLaMP: Large Language Model Made Powerful for High-fidelity Materials Knowledge Retrieval and Distillation}, 
      author={Yuan Chiang and Chia-Hong Chou and Janosh Riebesell},
      year={2024},
      eprint={2401.17244},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{lála2023paperqa,
      title={PaperQA: Retrieval-Augmented Generative Agent for Scientific Research}, 
      author={Jakub Lála and Odhran O'Donoghue and Aleksandar Shtedritski and Sam Cox and Samuel G. Rodriques and Andrew D. White},
      year={2023},
      eprint={2312.07559},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{wang2024rat,
      title={RAT: Retrieval Augmented Thoughts Elicit Context-Aware Reasoning in Long-Horizon Generation}, 
      author={Zihao Wang and Anji Liu and Haowei Lin and Jiaqi Li and Xiaojian Ma and Yitao Liang},
      year={2024},
      eprint={2403.05313},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{ke2024development,
      title={Development and Testing of Retrieval Augmented Generation in Large Language Models -- A Case Study Report}, 
      author={YuHe Ke and Liyuan Jin and Kabilan Elangovan and Hairil Rizal Abdullah and Nan Liu and Alex Tiong Heng Sia and Chai Rick Soh and Joshua Yi Min Tung and Jasmine Chiat Ling Ong and Daniel Shu Wei Ting},
      year={2024},
      eprint={2402.01733},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{lewis2021retrievalaugmented,
      title={Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks}, 
      author={Patrick Lewis and Ethan Perez and Aleksandra Piktus and Fabio Petroni and Vladimir Karpukhin and Naman Goyal and Heinrich Küttler and Mike Lewis and Wen-tau Yih and Tim Rocktäschel and Sebastian Riedel and Douwe Kiela},
      year={2021},
      eprint={2005.11401},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{wei2023chainofthought,
      title={Chain-of-Thought Prompting Elicits Reasoning in Large Language Models}, 
      author={Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Brian Ichter and Fei Xia and Ed Chi and Quoc Le and Denny Zhou},
      year={2023},
      eprint={2201.11903},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@INPROCEEDINGS{xiao,
  author={Xiao, Chuan and Wang, Wei and Lin, Xuemin and Shang, Haichuan},
  booktitle={2009 IEEE 25th International Conference on Data Engineering}, 
  title={Top-k Set Similarity Joins}, 
  year={2009},
  volume={},
  number={},
  pages={916-927},
  keywords={Upper bound;Web pages;Filtering;Euclidean distance;Data engineering;Pattern recognition;Large-scale systems;Couplings;Data mining;Time factors}
}

@inproceedings{hinton,
  title = {Distilling the Knowledge in a Neural Network},
  author = {Geoffrey Hinton and Oriol Vinyals and Jeffrey Dean},
  year = {2015},
  booktitle = {Proceedings of the NIPS Deep Learning and Representation Learning Workshop},
  pages = {1--9}
}

@article{Gunawan_2018,
year = {2018},
month = {mar},
publisher = {IOP Publishing},
volume = {978},
number = {1},
pages = {012120},
author = {D Gunawan and C A Sembiring and M A Budiman},
title = {The Implementation of Cosine Similarity to Calculate Text Relevance between Two Documents},
journal = {Journal of Physics: Conference Series},
abstract = {Rapidly increasing number of web pages or documents leads to topic specific filtering in order to find web pages or documents efficiently. This is a preliminary research that uses cosine similarity to implement text relevance in order to find topic specific document. This research is divided into three parts. The first part is text-preprocessing. In this part, the punctuation in a document will be removed, then convert the document to lower case, implement stop word removal and then extracting the root word by using Porter Stemming algorithm. The second part is keywords weighting. Keyword weighting will be used by the next part, the text relevance calculation. Text relevance calculation will result the value between 0 and 1. The closer value to 1, then both documents are more related, vice versa.}
}

@article{10.1145/3560815,
author = {Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham},
title = {Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {9},
issn = {0360-0300},
abstract = {This article surveys and organizes research works in a new paradigm in natural language processing, which we dub “prompt-based learning.” Unlike traditional supervised learning, which trains a model to take in an input x and predict an output y as P(y|x), prompt-based learning is based on language models that model the probability of text directly. To use these models to perform prediction tasks, the original input x is modified using a template into a textual string prompt x′ that has some unfilled slots, and then the language model is used to probabilistically fill the unfilled information to obtain a final string x̂, from which the final output y can be derived. This framework is powerful and attractive for a number of reasons: It allows the language model to be pre-trained on massive amounts of raw text, and by defining a new prompting function the model is able to perform few-shot or even zero-shot learning, adapting to new scenarios with few or no labeled data. In this article, we introduce the basics of this promising paradigm, describe a unified set of mathematical notations that can cover a wide variety of existing work, and organize existing work along several dimensions, e.g., the choice of pre-trained language models, prompts, and tuning strategies. To make the field more accessible to interested beginners, we not only make a systematic review of existing works and a highly structured typology of prompt-based concepts but also release other resources, e.g., a website including constantly updated survey and paperlist.},
journal = {ACM Comput. Surv.},
month = {jan},
articleno = {195},
numpages = {35},
keywords = {Pre-trained language models, prompting}
}

@inproceedings{Yao2023LANGUAGEM,
  title={LANGUAGE MODELS},
  author={Shunyu Yao and Jeffrey Zhao and Dian Yu and Nan Du and Izhak Shafran and Karthik Narasimhan and Yuan Cao},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  year={2023},
  pages={4089--4100},
}


@article{jadhav2024large,
  title={Large language model agent as a mechanical designer},
  author={Jadhav, Yayati and Farimani, Amir Barati},
  journal={arXiv preprint arXiv:2404.17525},
  year={2024}
}

@misc{huggingface2023optimize,
  author = {Hugging Face},
  title = {Optimizing your LLM in production},
  howpublished = {\url{https://huggingface.co/docs/transformers/performance}},
  year = {2023},
  note = {Accessed: 2024-05-17}
}

@misc{huggingface2024llama,
  author = {Hugging Face},
  title = {LLaMA 7B GPU Memory Requirement - Transformers - Hugging Face Forums},
  howpublished = {\url{https://discuss.huggingface.co/t/llama-7b-gpu-memory-requirement/487}},
  year = {2024},
  note = {Accessed: 2024-05-17}
}

@misc{huggingface2024optimize,
  author = {Hugging Face},
  title = {Optimizing LLMs for Speed and Memory},
  howpublished = {\url{https://huggingface.co/blog/optimizing-llms-for-speed-and-memory}},
  year = {2024},
  note = {Accessed: 2024-05-17}
}

@misc{pak2024thermopore,
      title={ThermoPore: Predicting Part Porosity Based on Thermal Images Using Deep Learning}, 
      author={Peter Myung-Won Pak and Francis Ogoke and Andrew Polonsky and Anthony Garland and Dan S. Bolintineanu and Dan R. Moser and Michael J. Heiden and Amir Barati Farimani},
      year={2024},
      eprint={2404.16882},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{ogoke2024deep,
      title={Deep Learning for Melt Pool Depth Contour Prediction From Surface Thermal Images via Vision Transformers}, 
      author={Francis Ogoke and Peter Myung-Won Pak and Alexander Myers and Guadalupe Quirarte and Jack Beuth and Jonathan Malen and Amir Barati Farimani},
      year={2024},
      eprint={2404.17699},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{Everton2016ReviewOI,
  title={Review of in-situ process monitoring and in-situ metrology for metal additive manufacturing},
  author={Sarah K. Everton and Matthias Hirsch and Petros Stravroulakis and Richard K. Leach and Adam Thomas Clare},
  journal={Materials \& Design},
  year={2016},
  volume={95},
  pages={431-445},
}

@article{Scime2018,
  title={Anomaly Detection and Classification in a Laser Powder Bed Additive Manufacturing Process using a Trained Computer Vision Algorithm},
  author={Luke Scime and Jack L. Beuth},
  journal={Additive manufacturing},
  year={2018},
  volume={19},
  pages={114-126},
}

@article{AKBARI2022,
title = {MeltpoolNet: Melt pool characteristic prediction in Metal Additive Manufacturing using machine learning},
journal = {Additive Manufacturing},
volume = {55},
pages = {102817},
year = {2022},
issn = {2214-8604},
doi = {https://doi.org/10.1016/j.addma.2022.102817},
author = {Parand Akbari and Francis Ogoke and Ning-Yu Kao and Kazem Meidani and Chun-Yu Yeh and William Lee and Amir {Barati Farimani}},
keywords = {Additive manufacturing, Machine learning, Melt pool, Process map},
abstract = {Characterizing melt pool shape and geometry is essential in Metal Additive Manufacturing (MAM) to control the printing process, and avoid defects. Predicting melt pool flaws based on process parameters and powder material is difficult due to the complex nature of MAM processes. Machine learning (ML) techniques can be useful in connecting process parameters to the type of flaws in the melt pool. In this work, we introduced a comprehensive framework for benchmarking ML for melt pool characterization. An extensive experimental dataset has been collected from more than 80 MAM articles containing MAM processing conditions, materials, melt pool dimensions, melt pool modes and flaw types. We introduced physics-aware MAM featurization, versatile ML models, and evaluation metrics to create a comprehensive learning framework for melt pool defect and geometry prediction. This benchmark can serve as a basis for melt pool control and process optimization. In addition, data-driven explicit models have been identified to estimate melt pool geometry from process parameters and material properties. These models have been shown to outperform Rosenthal estimation for melt pool geometry while maintaining interpretability.}
}

@article{Qi2019ApplyingNM,
  title={Applying Neural-Network-Based Machine Learning to Additive Manufacturing: Current Applications, Challenges, and Future Perspectives},
  author={Xin Bo Qi and Guofeng Chen and Yong Li and Cheng Xuan and Changpeng Li},
  journal={Engineering},
  year={2019},
}

@InProceedings{glorot10a,
  title = 	 {Understanding the difficulty of training deep feedforward neural networks},
  author = 	 {Glorot, Xavier and Bengio, Yoshua},
  booktitle = 	 {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {249--256},
  year = 	 {2010},
  editor = 	 {Teh, Yee Whye and Titterington, Mike},
  volume = 	 {9},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Chia Laguna Resort, Sardinia, Italy},
  month = 	 {13--15 May},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf},
  abstract = 	 {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future.  We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1.  Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.}
}
@article{Agarwal2024LitLLMAT,
  title={LitLLM: A Toolkit for Scientific Literature Review},
  author={Shubham Agarwal and Issam Hadj Laradji and Laurent Charlin and Christopher Pal},
  journal={ArXiv},
  year={2024},
  volume={abs/2402.01788}
}

@article{pandiyan2022situ,
  title={In situ quality monitoring in direct energy deposition process using co-axial process zone imaging and deep contrastive learning},
  author={Pandiyan, Vigneashwara and Cui, Di and Le-Quang, Tri and Deshpande, Pushkar and Wasmer, Kilian and Shevchik, Sergey},
  journal={Journal of Manufacturing Processes},
  volume={81},
  pages={1064--1075},
  year={2022},
  publisher={Elsevier}
}


@article{estalaki2022predicting,
  title={Predicting defects in laser powder bed fusion using in-situ thermal imaging data and machine learning},
  author={Estalaki, Sina Malakpour and Lough, Cody S and Landers, Robert G and Kinzel, Edward C and Luo, Tengfei},
  journal={Additive Manufacturing},
  volume={58},
  pages={103008},
  year={2022},
  publisher={Elsevier}
}

@article{almasri2023data,
  title={A data-driven topology optimization approach to handle geometrical manufacturing constraints in the earlier steps of the design phase},
  author={Almasri, Waad and Danglade, Florence and Bettebghor, Dimitri and Adjed, Faouzi and Ababsa, Fakhreddine},
  journal={Procedia CIRP},
  volume={119},
  pages={377--383},
  year={2023},
  publisher={Elsevier}
}

@conference{donmez,
  author = {Yan Lu and Paul Witherell and M Donmez},
  title = {A Collaborative Data Management System for Additive Manufacturing},
  year = {2017},
  month = {2017-08-09},
  publisher = {37th Computers and Information in Engineering Conference (CIE), Cleveland, OH},
  language = {en},
}

@article{Jignasu2023TowardsFA,
  title={Towards Foundational AI Models for Additive Manufacturing: Language Models for G-Code Debugging, Manipulation, and Comprehension},
  author={Anushrut Jignasu and Kelly O. Marshall and Baskar Ganapathysubramanian and Aditya Balu and Chinmay Hegde and Adarsh Krishnamurthy},
  journal={ArXiv},
  year={2023},
  volume={abs/2309.02465},
}


@proceedings{AMMD,
    author = {Lu, Yan and Yang, Zhuo and Eddy, Douglas and Krishnamurty, Sundar},
    title = "{Self-Improving Additive Manufacturing Knowledge Management}",
    volume = {Volume 1B: 38th Computers and Information in Engineering Conference},
    series = {International Design Engineering Technical Conferences and Computers and Information in Engineering Conference},
    pages = {V01BT02A016},
    year = {2018},
    month = {08},
    abstract = "{The current additive manufacturing (AM) product development environment is far from being mature. Both software applications and workflow management tools are very limited due to the lack of knowledge supporting engineering decision making. AM knowledge includes design rules, operation guidance, and predictive models, etc., which play a critical role in the development of AM products, from the selection of a process and material, lattice and support structure design, process parameter optimization to in-situ process control, part qualification and even material development. At the same time, massive AM simulation and experimental data sets are being accumulated, stored, and processed by the AM community. This paper proposes a four-tier framework for self-improving additive manufacturing knowledge management, which defines two processes: bottom-up data-driven knowledge engineering and top-down goal-oriented active data generation. The processes are running in parallel and connected by users, therefore forming a closed loop, through which AM knowledge can evolve continuously and in an automated way.}",
    doi = {10.1115/DETC2018-85996},
    eprint = {https://asmedigitalcollection.asme.org/IDETC-CIE/proceedings-pdf/IDETC-CIE2018/51739/V01BT02A016/2475346/v01bt02a016-detc2018-85996.pdf},
}

@article{DFAM,
title = {Machine learning and knowledge graph based design rule construction for additive manufacturing},
journal = {Additive Manufacturing},
volume = {37},
pages = {101620},
year = {2021},
issn = {2214-8604},
doi = {https://doi.org/10.1016/j.addma.2020.101620},
author = {Hyunwoong Ko and Paul Witherell and Yan Lu and Samyeon Kim and David W. Rosen},
keywords = {Additive manufacturability, Data, Design for additive manufacturing, Design rule, Machine learning, Knowledge},
abstract = {Additive Manufacturing (AM) is becoming data-intensive while increasingly generating newly available data. The availability of AM data provides Design for AM (DfAM) with a newfound opportunity to construct AM design rules with improved understanding of AM’s influence on part qualities. To seize the opportunity, this paper proposes a novel approach for AM design rule construction based on machine learning and knowledge graph. First, this paper presents a framework that enables i) deploying machine learning for extracting knowledge on predictive additive manufacturability from data, ii) adopting ontology with knowledge graphs as a knowledge base for storing both a priori and newfound AM knowledge, and iii) reasoning with knowledge for deriving data-driven prescriptive AM design rules. Second, this paper presents a methodology that constructs knowledge on predictive additive manufacturability and prescriptive AM design rules. In the methodology, we formalize knowledge representations, extractions, and reasoning, which enhances automated and autonomous construction and improvements of AM design rules. The methodology then employs a machine learning algorithm of Classification and Regression Tree on measurement data from National Institute of Standards and Technology for construction of a Laser Powder Bed Fusion-specific design rule for overhang features. This work supports AI related decision-making in additive manufacturability analysis and (re-)design for AM and guides machine learning to addressing problems related to AM design rules. This work is also meaningful as it provides sharable AM design rule knowledge with the AM society.}
}

@article{MPNet,
  title={MPNet: Masked and Permuted Pre-training for Language Understanding},
  author={Kaitao Song and Xu Tan and Tao Qin and Jianfeng Lu and Tie-Yan Liu},
  journal={ArXiv},
  year={2020},
  volume={abs/2004.09297},
}

@inproceedings{dimassi2021dispersed,
  title={From Dispersed Knowledge to Ontology: A Proposal for Formalizing and Integrating 4D Printing in Design},
  author={Dimassi, Saoussen and Demoly, Fr{\'e}d{\'e}ric and Cruz, Christophe and Gomes, Samuel},
  booktitle={IFIP International Conference on Product Lifecycle Management},
  pages={80--95},
  year={2021},
  organization={Springer}
}

@article{dimassi2023knowledge,
  title={A knowledge recommendation approach in design for multi-material 4D printing based on semantic similarity vector space model and case-based reasoning},
  author={Dimassi, Saoussen and Demoly, Fr{\'e}d{\'e}ric and Belkebir, Hadrien and Cruz, Christophe and Kim, Kyoung-Yun and Gomes, Samuel and Qi, H Jerry and Andr{\'e}, Jean-Claude},
  journal={Computers in Industry},
  volume={145},
  pages={103824},
  year={2023},
  publisher={Elsevier}
}

@article{salton1975vector,
  title={A vector space model for automatic indexing},
  author={Salton, Gerard and Wong, Anita and Yang, Chung-Shu},
  journal={Communications of the ACM},
  volume={18},
  number={11},
  pages={613--620},
  year={1975},
  publisher={ACM New York, NY, USA}
}

@inproceedings{Muennighoff2022MTEBMT,
  title={MTEB: Massive Text Embedding Benchmark},
  author={Niklas Muennighoff and Nouamane Tazi and Loic Magne and Nils Reimers},
  booktitle={Conference of the European Chapter of the Association for Computational Linguistics},
  year={2022},
}

@software{openai_tiktoken_2023,
  author = {OpenAI},
  title = {TikToken},
  year = {2023},
  url = {https://github.com/openai/tiktoken}
}

@article{subwordtoken,
  title={Assessing the Importance of Frequency versus Compositionality for Subword-based Tokenization in NMT},
  author={Benoist Wolleb and Romain Silvestri and Giorgos Vernikos and Ljiljana Dolamic Andrei Popescu-Belis},
  journal={ArXiv},
  year={2023},
  volume={abs/2306.01393},
}

@misc{elsapy,
  author = {Elsevier Dev},
  title = {elsapy: A Python client for Elsevier APIs},
  year = {2024},
  url = {https://github.com/ElsevierDev/elsapy},
  note = {Accessed: 2024-07-18}
}


@article{Fei2024RetrievalMR,
  title={Retrieval Meets Reasoning: Dynamic In-Context Editing for Long-Text Understanding},
  author={WeiZhi Fei and Xueyan Niu and Guoqing Xie and Yanhua Zhang and Bo Bai and Lei Deng and Wei Han},
  journal={ArXiv},
  year={2024},
  volume={abs/2406.12331},
}

@article{jiang2024longrag,
  title={LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs},
  author={Jiang, Ziyan and Ma, Xueguang and Chen, Wenhu},
  journal={arXiv preprint arXiv:2406.15319},
  year={2024}
}

@misc{dubey2024llama3herdmodels,
      title={The Llama 3 Herd of Models}, 
      author={Abhimanyu Dubey et al.},
      year={2024},
      eprint={2407.21783},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
}

@article{buehlerokg2024,
author = {Buehler, Markus J.},
title = {Generative Retrieval-Augmented Ontologic Graph and Multiagent Strategies for Interpretive Large Language Model-Based Materials Design},
journal = {ACS Engineering Au},
volume = {4},
number = {2},
pages = {241-277},
year = {2024},
doi = {10.1021/acsengineeringau.3c00058},
}

@article{choudhary2023chemnlp,
  title={ChemNLP: a natural language-processing-based library for materials chemistry text data},
  author={Choudhary, Kamal and Kelley, Mathew L},
  journal={The Journal of Physical Chemistry C},
  volume={127},
  number={35},
  pages={17545--17555},
  year={2023},
  publisher={ACS Publications}
}

@article{yik2024chatgpt,
  title={ChatGPT Convincingly Explains Organic Chemistry Reaction Mechanisms Slightly Inaccurately with High Levels of Explanation Sophistication},
  author={Yik, Brandon J and Dood, Amber J},
  journal={Journal of Chemical Education},
  volume={101},
  number={5},
  pages={1836--1846},
  year={2024},
  publisher={ACS Publications}
}

@article{clark2023investigating,
  title={Investigating the use of an artificial intelligence chatbot with general chemistry exam questions},
  author={Clark, Ted M},
  journal={Journal of Chemical Education},
  volume={100},
  number={5},
  pages={1905--1916},
  year={2023},
  publisher={ACS Publications}
}

@article{leon2023chatgpt,
  title={ChatGPT needs a chemistry tutor too},
  author={Leon, Alfredo J and Vidhani, Dinesh},
  journal={Journal of Chemical Education},
  volume={100},
  number={10},
  pages={3859--3865},
  year={2023},
  publisher={ACS Publications}
}

@article{watts2023comparing,
  title={Comparing student and generative artificial intelligence chatbot responses to organic chemistry writing-to-learn assignments},
  author={Watts, Field M and Dood, Amber J and Shultz, Ginger V and Rodriguez, Jon-Marc G},
  journal={Journal of Chemical Education},
  volume={100},
  number={10},
  pages={3806--3817},
  year={2023},
  publisher={ACS Publications}
}




@article{brown2020language,
  title={Language Models are Few-Shot Learners},
  author={Brown, Tom B and Mann, Benjamin and Ryder, Nick and others},
  journal={arXiv preprint arXiv:2005.14165},
  year={2020}
}

@article{howard2018universal,
  title={Universal Language Model Fine-tuning for Text Classification},
  author={Howard, Jeremy and Ruder, Sebastian},
  journal={arXiv preprint arXiv:1801.06146},
  year={2018}
}

@article{nazem2008nanotechnology,
  title={Nanotechnology solutions for Alzheimer's disease: advances in research tools, diagnostic methods and therapeutic agents},
  author={Nazem, Amir and Mansoori, G Ali},
  journal={Journal of Alzheimer's disease},
  volume={13},
  number={2},
  pages={199--223},
  year={2008},
  publisher={IOS Press}
}
@article{ebrahimi2014reliability,
  title={Reliability for Drug targeting in cancer treatment through nanotechnology (A psychometric approach)},
  author={Ebrahimi, Nader and Mansoori, G Ali},
  journal={molecules},
  volume={1},
  number={8},
  year={2014}
}
@article{mansoori2017introduction,
  title={An introduction to nanoscience and nanotechnology},
  author={Mansoori, G Ali},
  journal={Nanoscience and plant--soil systems},
  pages={3--20},
  year={2017},
  publisher={Springer}
}
@article{yousaf2008nanoscience,
  title={Why Nanoscience and Nanotechnology? What is there for us?},
  author={Yousaf, AS and Ali, Salamat},
  journal={Journal of Faculty of Engineering \& Technology},
  volume={5},
  pages={11--20},
  year={2008}
}
@article{de2019drug,
  title={Drug delivery with extracellular vesicles: from imagination to innovation},
  author={De Jong, Olivier G and Kooijmans, Sander AA and Murphy, Daniel E and Jiang, Linglei and Evers, Martijn JW and Sluijter, Joost PG and Vader, Pieter and Schiffelers, Raymond M},
  journal={Accounts of chemical research},
  volume={52},
  number={7},
  pages={1761--1770},
  year={2019},
  publisher={ACS Publications}
}
@article{lecun2015deep,
  title={Deep learning},
  author={LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  journal={nature},
  volume={521},
  number={7553},
  pages={436--444},
  year={2015},
  publisher={Nature Publishing Group UK London}
}

@manual{selenium,
  title        = {SeleniumHQ/selenium: A suite of tools for web browser automation},
  author       = {SeleniumHQ and Contributors},
  year         = {2025},
  url          = {https://github.com/SeleniumHQ/selenium},
  note         = {Accessed: 2025-01-12},
}



