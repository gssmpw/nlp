\section{Introduction}

Large language models (LLMs)~\cite{gpt3, chen2023extending, mistral, gemma2} have demonstrated remarkable efficacy across diverse domains~\cite{li2024ecomgpt, wu2024chateda, lu2024chameleon} due to their advanced capabilities in semantic understanding and text generation. However, their emergent abilities follow the scaling law~\cite{bahri2024explaining, naveed2023comprehensive, raiaan2024review}, which leads to state-of-the-art LLMs typically comprising billions of parameters. For instance, the DeepSeek-V3~\cite{liu2024deepseek} model, with its 671 billion parameters, requires 2,664 million H800 GPU hours for training. This resource-intensive training and fine-tuning process presents significant barriers to the democratization of LLMs. As a result, decentralized training~\cite{yuan2022decentralized, ryabinin2023swarm} is gaining increasing attention as a promising solution to mitigate these resource challenges.

Decentralized training is mainly based on parallel training (e.g., \textit{pipeline parallelism}~\cite{narayanan2019pipedream}), which distributes training computations across heterogeneous computing devices (typically GPUs) in a pipeline, with each device acting as a distinct stage. Unlike traditional federated learning (FL), which is based on data parallelism~\cite{li2014scaling, luo2020prague}, pipeline parallelism allocates model layers across devices, facilitating the concurrent processing of multiple data batches over successive stages. During decentralized training, each stage transmits activations during forward propagation and gradients during backward propagation to iteratively update model parameters. This approach enhances memory utilization and alleviates computational bottlenecks. Frameworks such as GPipe~\cite{huang2019gpipe} and Megatron-LM~\cite{narayanan2021efficient} effectively balance resource constraints with training efficiency, supporting the democratization of LLMs.

As research on the robustness of decentralized training progresses, the security vulnerabilities of this framework have become increasingly evident. However, most existing studies~\cite{thorpe2023bamboo, jang2023oobleck, duan2024parcae} primarily focus on addressing fault tolerance issues related to hardware failures in pipeline parallelism, often neglecting the impact of human threats. While some research~\cite{lu2024position} has examined the role of attackers, demonstrating that malicious stages in decentralized training can significantly disrupt training outcomes and hinder model convergence, this study typically assumes that attackers can control any stage of decentralized training. Such strong assumptions about the attackers' capabilities make the attack methods impractical in real-world training scenarios, where tampering with transmitted values is highly likely to be detected by the training initiator. Furthermore, the above studies fail to address privacy risks, which could lead to more severe consequences~\cite{bethany2024large}.

Motivated by this gap, we aim to investigate whether malicious stages in decentralized training can steal privacy without disrupting the training process. However, implementing this privacy reconstruction attack presents a significant challenge: decentralized training differs substantially from traditional training methods, such as localized training or FL. In traditional training, attackers may have access to a complete model copy~\cite{li2023sentence,morris2023text} or its inputs and corresponding outputs~\cite{huang2024transferable}. In contrast, within the decentralized training, malicious stages can only access the transmitted values between stages. This raises a critical research question: \textit{How to steal privacy, such as training data, solely through transmitted values in decentralized training?}


To address this critical research question, this paper first introduces the \textbf{\textit{\underline{A}ctivation \underline{I}nversion \underline{A}ttack}} (AIA) targeting decentralized training. Specifically, we demonstrate how a malicious stage in decentralized training can steal training data by exploiting activations through a two-step process. In the first step: \textbf{Shadow Dataset Construction}, the attacker creates a shadow dataset of text-activation pairs using a public dataset, aiming to align the data distribution of the shadow dataset with that of the actual training process. In the second step: \textbf{Attack Model Training}, the attacker trains a generative model using the shadow dataset to learn the mapping from activations to text labels. The attacker then reconstructs the corresponding training data from victim activations. In summary, the contributions of this paper are as follows:


\begin{itemize}[nolistsep, leftmargin=*, topsep=0pt]

    \item We identify a novel attack surface, marking the first attempt to steal private training data within decentralized training frameworks.

    \item We propose a two-step attack framework, AIA, that steals training data through activations in decentralized training without detection.

    \item We conduct a comprehensive evaluation of the effectiveness of AIA, demonstrating its character-level capability for training data reconstruction. Specifically, AIA achieves 62\% accuracy in stealing private emails when fine-tuning GPT2-XL.
    
\end{itemize}