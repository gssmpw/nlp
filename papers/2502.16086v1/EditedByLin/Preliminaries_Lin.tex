\section{Preliminaries}

\subsection{Threat Model}
\label{sec:threat_model}

\noindent{\textbf{Attack scenario.}}
% We propose a decentralized training framework consisting of $K$ computation stages, where $M_i$ represents the sub-layers (e.g., decode layers in LLMs) of the $i$-th stage. During training iteration $t$, $M_i$ transmits activations $a_i^{(t)}$ to $M_{i+1}$ and gradients $g_i^{(t)}$ to $M_{i-1}$.
We consider a decentralized training scenario where the user intends to fine-tune a pre-trained model ${M}_\text{pre}$ using their private dataset $\mathcal{D}_\text{vic}$, resulting in a fine-tuned model ${M}_\text{fine}$. The framework consists of $K$ stages, where $M_i$ represents the sub-layers (e.g., decode layers in LLMs) of the $i$-th stage. During training iteration $t$, $M_i$ transmits activations $\bm a_i^{(t)}$ to $M_{i+1}$ and gradients $\bm g_i^{(t)}$ to $M_{i-1}$. However, an unmonitored decentralized training framework may introduce an honest-but-curious stage as an attacker.

\noindent{\textbf{Attacker's goals.}}
The attacker's objective is to reconstruct character-level training data $\bm d^{(t)}$ from $\mathcal{D}_\text{vic}$ during iteration $t$ in victim decentralized training. Additionally, the attacker seeks to conceal their malicious activities, executing the attack without disrupting the training process to avoid detection by the training initiator or other detection mechanisms.

\noindent{\textbf{Attacker's knowledge.}}
We assume the attacker, as the $i_\text{att}$-th stage, has access to all information related to its own stage, including the sub-layers $M_{i_\text{att}}$ and transmitted data $\bm a_{i_\text{att}}$ and $\bm g_{i_\text{att}}$. This enables the attacker to infer the architecture of ${M}_\text{fine}$ based on the structure of $M_{i_\text{att}}$. However, the attacker is assumed to have no access to other training-related information, such as transmitted data between benign stages or auxiliary information about the training data. This assumption is realistic, as it facilitates the deployment of this attack in real-world decentralized training environments.


\subsection{Motivation}
\label{sec:act_cos}

In Section \ref{sec:threat_model}, it is established that attackers can only reconstruct training data through the transmitted values during the victim model's training process, such as activations and gradients. This section discusses the challenges of using gradients to conduct such attacks and explores the feasibility of using activations to achieve similar objectives.

% In decentralized training, using gradients to steal training data faces a major challenge: The unknowability of global gradients. In decentralized training, the gradients received at each stage correspond to the current sub-layers, which significantly differs from previous studies on gradient-based privacy leakage that rely on global gradients.

In decentralized training, traditional deep gradient leakage attacks encounter a significant limitation: the unavailability of the global model and global gradients. Previous researches~\cite {zhu2019deep, gupta2022recovering, balunovic2022lamp} focus on training or searching for a set of texts that, through the victim modelâ€™s gradient, approximate the leaked gradient to reconstruct private data. However, in decentralized training, each stage only has access to a partial model and gradients, making it difficult to reconstruct data through gradients.

% In contrast, to further explore changes in activations for the same data before and after fine-tuning LLMs, we have conducted a preliminary experiment: As shown in Figure~\ref{fig:layer_idx_act_cos}, we fine-tune three common LLMs using four different datasets and record the cosine similarity of activations for the same data sample before and after fine-tuning (specific experimental settings can be found in Section 5.1). The results show that as the layer index increases, the changes in the decoder layers closer to the lm\_head layer become more pronounced. Nevertheless, the activation similarities in the final layers still exceed 50\%. During fine-tuning, we observed that the decoder layers in earlier stages show minimal variation, with activation similarities nearly reaching 100\%. This demonstrates that activations are highly correlated with the training data, making it feasible to use activations to steal training data.

In contrast, reconstructing data using the intermediate outputs of the victim model is much more straightforward, as these intermediate outputs can be directly used as inputs to train the attack model~\cite{pasquini2021unleashing,li2023sentence}. Inspired by this, we examine the cosine similarity between $\bm a_i^{(t)}$ for $\bm d^{(t)}$ in ${M}_\text{pre}$ and ${M}_\text{fine}$ across layer index $i$ (experimental details can be found in Section \ref{sec:experiment_setup}). As shown in Figure \ref{fig:layer_idx_act_cos}, activation similarity in early layers approaches 100\%, while similarity in later layers remains above 50\%. These results suggest that the activations of the same data exhibit minimal variation before and after fine-tuning, indicating a strong correlation between activations and the training data. This preliminary experiment provides key insights for our attacks in Section \ref{sec:AIA}.


% The most critical aspect of activation inversion attacks is ensuring that the features of the shadow activations closely resemble those of the victim activations. This similarity is essential for the trained attack model to have transferability. Figure \ref{fig:layer_idx_act_cos} illustrates the cosine similarity of activations across different layers before and after fine-tuning on the GPT2-XL, Bloom-7B1, and LLaMA3-8B pre-trained models using various datasets. It is observed that as the layer index increases, the changes in the decoder layers nearer to the lm\_head layer become more pronounced. Nevertheless, the activation similarities in the final layers still exceed 50\%. During the fine-tuning process, we note that the decoder layers in the earlier stages show minimal variation, with activation similarities nearly reaching 100\%. This indicates that directly using a pre-trained model as the shadow model without additional training can still produce shadow activations closely resembling the victim activations.
\begin{figure}[t]
  \includegraphics[width=\linewidth]{figures/layer_idx_cos.pdf} 
  \caption {Cosine similarity between activations for the same data in the pre-trained model and the fine-tuned model across layer index.}
  \label{fig:layer_idx_act_cos}
  \vspace{-1em}
\end{figure}