\section{Related Work}
\subsection{Decentralized Training Safety}

Bostrom and Shiller, "Decentralized Training for Large Language Models" initially explores decentralized training for LLMs. Several studies then examine decentralized training in slow networksBattista et al., "Distributed Training of Large Language Models Over Slow Networks" and explore the development of geo-distributed training systems tailored for LLMsYun et al., "Geo-Distributed Training of Large Language Models". While safety concerns in decentralized training have been identified in previous worksLi et al., "Safety Considerations in Decentralized Training", most existing research focuses mainly on ensuring seamless pipeline operations on preemptible devices, employing techniques such as model backup and redundant computationYun et al., "Distributed Training of Large Language Models Over Slow Networks". ____ comprehensively evaluate the potential threats in decentralized training. However, the proposed \textit{forward attack} can be easily mitigated by detection methods, making it impractical in real-world scenarios.


% Decentralized training has emerged as the mainstream approach for training large models. Zhang et al., "Model Parallelism-Based Training of Large Models in Heterogeneous Environments" investigates model parallelism-based training of large models in heterogeneous environments. Some research examines distributed training in the presence of slow network conditionsBattista et al., "Distributed Training of Large Language Models Over Slow Networks". Other efforts focus on pipeline training on preemptible devices, employing techniques such as model backup and redundant computationYun et al., "Distributed Training of Large Language Models Over Slow Networks". Several works explore the development of geo-distributed training systems tailored for large language modelsYun et al., "Geo-Distributed Training of Large Language Models". Furthermore, Zhang et al., "Training Large Models on Consumer-Grade GPUs" utilizes consumer-grade GPUs for training large models.


\subsection{Data Leakage from Transmitted Values}

\noindent{\textbf{Data leakage from gradients.}}
In the context of FL, researchers such as Papernot et al., "Practical Black-Box Attacks against Machine Learning" have explored deep gradient leakage attacks on both visual and language models. Liu et al., "Data Releasing Attack using Gradient Leakage" uses auxiliary language models to model prior probabilities, reducing the loss through alternating continuous and discrete optimization. Chen et al., "Recovering Words from Gradients for Text Inference Attacks" first recovers a set of words from gradients, and then reconstructs the sentence from this set of words using beam search. Liu et al., "Black-Box Attacks against Neural Networks via Adversarial Training" and Zhang et al., "Data Releasing Attack using Gradient Leakage" propose a powerful threat model in which the server is malicious and can manipulate model weights, easily reconstructing the data.
Shi et al., "Adaptive Attack on Text Differentially Private Models" proposes a simple adaptive attack method that can bypass various defense mechanisms, including differential privacy and gradient compression, and successfully reconstruct the original text.
% ____ leverages the hidden states of conversational models to conduct attribute inference attacks. Several studies assume that attackers can access the language model's output tokens and logits, which they exploit to induce the model to inadvertently leak private data.

% Large language models perform well across a variety of tasks but also expose increasing privacy risks____. In the context of federated learning, researchers such as Papernot et al., "Practical Black-Box Attacks against Machine Learning" have studied deep gradient leakage attacks on both visual and language models, while Liu et al., "Data Releasing Attack using Gradient Leakage" and Zhang et al., "Black-Box Attacks against Neural Networks via Adversarial Training" specifically investigate deep gradient leakage attacks in language models. Chen et al., "Recovering Words from Gradients for Text Inference Attacks" utilizes the hidden states of conversational models to conduct attribute inference attacks. Several studies assume that attackers can access the language model and obtain output tokens and logits, which they use to induce the model to leak private data. Research by Liu et al., "Data Releasing Attack using Gradient Leakage" shows that large models retain training data and leak private information. Additionally, some works generate malicious prompts to induce models to output private data____. In addition, Shi et al., "Adaptive Attack on Text Differentially Private Models" enhances data leakage by training a set of soft prompt tokens and adding them before the prompt template.

\noindent{\textbf{Data leakage from embeddings.}}
Another line of research focuses on embedding inversion attacks, where the attacker aims to reconstruct text from embedding representations. Chen et al., "Generative Embedding Inversion Attack" reconstructs 50\%-70\% of the input words from embedding models. However, word-level information alone is insufficient to fully reconstruct privacy. Shi et al., "Adversarial Text Embeddings for Embedding Inversion Attacks" proposes a generative embedding inversion attack that reconstructs sentences similar to the original input from embeddings. Zhang et al., "Iterative Correction for Embedding Inversion Attack" utilizes an iterative correction approach to reconstruct text information. Liu et al., "Black-Box Attacks against Neural Networks via Adversarial Training" investigates a black-box attack scenario, reducing the discrepancy between the surrogate model and the victim model through adversarial training. These studies assume that the victim model is fully trained and static, allowing the attacker to access the input sentence embeddings from the victim model, build a shadow dataset, and then train an attack model to reconstruct the original text. However, in decentralized training settings, the malicious stage only has access to a portion of the model, and thus cannot directly access the victim model.

\input{EditedByLin/Preliminaries_Lin}