% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
% \usepackage[review]{acl}
\usepackage{acl}
% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{tcolorbox}
\usepackage{tabularx} % 支持自动调整列宽
\usepackage{lipsum}   % 仅用于生成示例文本
\usepackage{bm} % 导入bm宏包

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{array}
\usepackage{float}


% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Stealing Training Data from Large Language Models \\ in Decentralized Training through Activation Inversion Attack}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Chenxi Dai\thanks{Equal contribution} \and
        Lin Lu\footnotemark[1] \and 
        Pan Zhou\thanks{Corresponding author} \\
  Huazhong University of Science of Technology \\
  \{dcx001,loserlulin,panzhou\}@hust.edu.cn}

% \usepackage[T1]{fontenc}
% \usepackage[utf8]{inputenc}
% \usepackage{authblk}

% \author[a]{Chenxi Dai}
% \author[a]{Lin Lu}
% \author[a]{Pan Zhou C \thanks{Corresponding author: panzhou@hust.edu.cn}}
% \affil[a]{Huazhong University of Science of Technology}




%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\begin{abstract}
Decentralized training has become a resource-efficient framework to democratize the training of large language models (LLMs). However, the privacy risks associated with this framework, particularly due to the potential inclusion of sensitive data in training datasets, remain unexplored. This paper identifies a novel and realistic attack surface: the privacy leakage from training data in decentralized training, and proposes \textit{activation inversion attack} (AIA) for the first time. AIA first constructs a shadow dataset comprising text labels and corresponding activations using public datasets. Leveraging this dataset, an attack model can be trained to reconstruct the training data from activations in victim decentralized training. We conduct extensive experiments on various LLMs and publicly available datasets to demonstrate the susceptibility of decentralized training to AIA. These findings highlight the urgent need to enhance security measures in decentralized training to mitigate privacy risks in training LLMs.
\end{abstract}


% Previous research has focused on fully trained and fixed models, whereas we explore a part of the model that changes dynamically during pipeline fine-tuning. To reduce the cost of the attack, we directly use pre-trained models as shadow models, and experiments validate the rationality and effectiveness of this approach. 

% \section{Introduction}
% Deep neural networks (DNNs), particularly large language models (LLMs)~\cite{gpt3,chen2023extending,mistral,gemma2}, have achieved remarkable success and found widespread applications across various domains due to their exceptional performance~\cite{li2024ecomgpt,wu2024chateda,lu2024chameleon}. However, the performance of these models is closely tied to both their size and the scale of the datasets they are trained on~\cite{bahri2024explaining}, which in turn leads to increasingly demanding requirements in terms of device memory and computational time. For instance, the training of the DeepSeek-V3 model, with 671 billion parameters, on a dataset of 14.8 trillion tokens required 2.664 million H800 GPU hours~\cite{liu2024deepseek}. As a result, distributed training across multiple devices has become a common approach for training modern large-scale models.

% To address this challenge, existing DNN training systems often employ model parallelism~\cite{Lepikhin2020Gshard,narayanan2021efficient,zheng2022alpa}, which divides a DNN into multiple segments and places them on devices with sufficient memory. Pipeline parallelism~\cite{huang2019gpipe,narayanan2019pipedream,narayanan2021memory}, on the other hand, assigns different stages of the model to different devices in a sequential manner. Since each device can simultaneously process different stages of different data, this approach significantly enhances resource utilization. Compared to traditional data parallelism~\cite{li2014scaling,luo2020prague}, pipeline parallelism alleviates the issue of memory constraints and computation bottlenecks, making it a prevalent paradigm for training large models today.

% Deep neural networks (DNNs), particularly large language models (LLMs)~\cite{gpt3,chen2023extending,mistral,gemma2}, have achieved significant success. However, their computational demands have increased exponentially due to the massive scale of their parameters and the high requirements for training data~\cite{naveed2023comprehensive,raiaan2024review}. For instance, the DeepSeek-V3 model, which trained 671 billion parameters on 14.8 trillion tokens, consumed 2.664 million H800 GPU hours~\cite{liu2024deepseek}. This exponential growth necessitates distributed training across multiple devices, and decentralized training based on pipeline parallelism has become a key solution. Unlike traditional data parallelism~\cite{li2014scaling,luo2020prague}, pipeline parallelism~\cite{narayanan2019pipedream} strategically divides model layers across devices, enabling concurrent processing of different data batches in successive stages. This approach not only optimizes memory utilization but also alleviates computational bottlenecks. Represented by frameworks such as GPipe~\cite{huang2019gpipe} and Megatron-LM~\cite{narayanan2021efficient}, this distributed training paradigm effectively balances resource limitations with training efficiency, forming the foundation of modern LLM development.

% However, the training paradigm of pipeline parallelism differs from that of data parallelism or local training, and introduces several new risks. Existing research~\cite{thorpe2023bamboo,jang2023oobleck,duan2024parcae} predominantly focuses on addressing fault tolerance related to hardware crashes in pipeline parallelism, largely overlooking the impact of human threats. While the potential of poisoning attacks in pipeline parallelism has been explored, these studies~\cite{lu2024position} typically assume the presence of an adversary capable of randomly controlling a specific stage within the pipeline. Such an adversary could manipulate activations or gradient values at that stage, potentially delaying or completely preventing model convergence. Although these manipulations can indeed disrupt the training process, they are generally easy to detect through standard anomaly detection methods. As a result, these attacks are unlikely to cause significant, long-term damage to the training process or the model itself.

% Despite this, there are subtler forms of threats in pipeline parallelism that require further attention. Inspired by leakage from gradients in federated learning~\cite{zhu2019deep,zhao2020idlg} and embedding inversion attacks~\cite{li2023sentence,chen2024text}, this paper introduces the first activation inversion attack under decentralized training. Unlike previous work, this attack does not require modifying the transmitted values during decentralized training; instead, it only requires access to the transmitted values to achieve the attack goal. Specifically, we consider a system in which a pipeline fine-tunes a large language model, with one of the stages being honest-but-curious. During the training process, this stage performs forward propagation and gradient calculation as usual but attempts to reconstruct the original training data from intermediate data. Since this stage only possesses a portion of the model, it cannot directly obtain activation values from sentence tokens, nor can it easily reconstruct data via gradients. Our attack aims to achieve the following two goals: 1) \textbf{the shadow model} is used to mimic the behavior of the victim model, even as the victim model evolves during fine-tuning; 2) \textbf{the attack model} is trained using a dataset constructed from the shadow model and then attempts to attack the victim model, attempting to reverse-engineer the original text from the activation values.

% To achieve the first goal, we used existing pre-trained models as shadow models and demonstrated the validity of this approach through experimentation. For the second goal, we employed a variety of model architectures as attack models to explore the effectiveness of text reconstruction.

% To validate the effectiveness of our attack, we conducted extensive experiments on three popular models: GPT2-XL~\cite{gpt2}, Bloom-7B1~\cite{bloom}, and LLaMA3-8B~\cite{llama3}. The experimental results show that the perplexity of the reconstructed text is consistently around 10, indicating that the reconstructed text closely resembles the original text. To further investigate the potential harm of activation inversion attacks, we constructed a Personally Identifiable Information (PII) dataset for privacy item extraction experiments. The results demonstrate that a large number of private items can be accurately reconstructed, with near 100\% recovery rates for the PII type of birthday and job.

\input{EditedByLin/Introduction_Lin}


\section{Related Work}

\subsection{Decentralized Training Safety}

\citet{yuan2022decentralized} initially explores decentralized training for LLMs. Several studies then examine decentralized training in slow networks~\cite{ryabinin2023swarm, wang2023cocktailsgd} and explore the development of geo-distributed training systems tailored for LLMs~\cite{gandhi2024improving, tang2024fusionllm}. While safety concerns in decentralized training have been identified in previous works~\cite{tang2023fusionai, borzunov2022training}, most existing research focuses mainly on ensuring seamless pipeline operations on preemptible devices, employing techniques such as model backup and redundant computation~\cite{thorpe2023bamboo, jang2023oobleck}. \citet{lu2024position} comprehensively evaluate the potential threats in decentralized training. However, the proposed \textit{forward attack} can be easily mitigated by detection methods, making it impractical in real-world scenarios.


% Decentralized training has emerged as the mainstream approach for training large models. \citet{yuan2022decentralized} investigates model parallelism-based training of large models in heterogeneous environments. Some research examines distributed training in the presence of slow network conditions~\cite{ryabinin2023swarm,wang2023cocktailsgd}. Other efforts focus on pipeline training on preemptible devices, employing techniques such as model backup and redundant computation~\cite{thorpe2023bamboo,jang2023oobleck}. Several works explore the development of geo-distributed training systems tailored for large language models~\cite{gandhi2024improving,tang2024fusionllm}. Furthermore, \citet{tang2023fusionai} utilizes consumer-grade GPUs for training large models.


\subsection{Data Leakage from Transmitted Values}

\noindent{\textbf{Data leakage from gradients.}}
In the context of FL, researchers such as \citet{zhu2019deep} have explored deep gradient leakage attacks on both visual and language models. 
\citet{balunovic2022lamp} uses auxiliary language models to model prior probabilities, reducing the loss through alternating continuous and discrete optimization. \citet{gupta2022recovering} first recovers a set of words from gradients, and then reconstructs the sentence from this set of words using beam search. \citet{fowl2022decepticons} and \citet{boenisch2023curious} propose a powerful threat model in which the server is malicious and can manipulate model weights, easily reconstructing the data.
\citet{wu2023learning} proposes a simple adaptive attack method that can bypass various defense mechanisms, including differential privacy and gradient compression, and successfully reconstruct the original text.
% \citet{li2022you} leverages the hidden states of conversational models to conduct attribute inference attacks. Several studies assume that attackers can access the language model's output tokens and logits, which they exploit to induce the model to inadvertently leak private data.

% Large language models perform well across a variety of tasks but also expose increasing privacy risks~\cite{das2024security,yan2024protecting}. In the context of federated learning, researchers such as \citet{zhu2019deep} have studied deep gradient leakage attacks on both visual and language models, while \citet{gupta2022recovering} and \citet{balunovic2022lamp} specifically investigate deep gradient leakage attacks in language models. \citet{li2022you} utilizes the hidden states of conversational models to conduct attribute inference attacks. Several studies assume that attackers can access the language model and obtain output tokens and logits, which they use to induce the model to leak private data. Research by \citet{carlini2021extracting} shows that large models retain training data and leak private information. Additionally, some works generate malicious prompts to induce models to output private data\cite{huang2022large,nakka2024pii}. In addition, \citet{SPT} enhances data leakage by training a set of soft prompt tokens and adding them before the prompt template.

\noindent{\textbf{Data leakage from embeddings.}}
Another line of research focuses on embedding inversion attacks, where the attacker aims to reconstruct text from embedding representations. \citet{song2020information} reconstructs 50\%-70\% of the input words from embedding models. However, word-level information alone is insufficient to fully reconstruct privacy. \citet{li2023sentence} proposes a generative embedding inversion attack that reconstructs sentences similar to the original input from embeddings. \citet{morris2023text} utilizes an iterative correction approach to reconstruct text information. \citet{huang2024transferable} investigates a black-box attack scenario, reducing the discrepancy between the surrogate model and the victim model through adversarial training. These studies assume that the victim model is fully trained and static, allowing the attacker to access the input sentence embeddings from the victim model, build a shadow dataset, and then train an attack model to reconstruct the original text. However, in decentralized training settings, the malicious stage only has access to a portion of the model, and thus cannot directly access the victim model.

\input{EditedByLin/Preliminaries_Lin}


\section{AIA: \textit{Activation Inversion Attack}}
\label{sec:AIA}
\begin{figure*}[t]
  \includegraphics[width=\textwidth]{figures/Figure_2.pdf}
  % \caption{Overview of the activation inversion attack (AIA). In an honest-but-curious decentralized training system, the victim model $M_{\text{vic}}$ is fine-tuned using its private data $\mathcal{D}_{\text{vic}}$. At one stage of the pipeline, which is set to be "curious," intermediate activation values $\mathcal{D}_{\text{vic}}$ during training are recorded, and shadow activations $\mathcal{D}_{\text{sha}}$ are acquired from the shadow model $M_{\text{sha}}$. These activations are then used to train an attack model $M_{\text{att}}$, which attempts to invert the private data.}
  \caption{Overview of Activation Inversion Attack (AIA). In a decentralized training system, the victim model $M_{\text{vic}}$ undergoes fine-tuning using private data $\mathcal{D}_{\text{vic}}$, which may contain personally identifiable information values (highlighted in yellow). An honest-but-curious attacker controlling the $i_{\text{att}}$-th stage of the pipeline: (1) records intermediate activation values $\bm a_{i_\text{att}-1}^{(t)}$ captured during the training process, and (2) collects shadow activations $\mathcal{D}_{\text{sha}}$ from the shadow model $M_{\text{sha}}$ to train the attack model $M_{\text{att}}$. Finally, the attacker uses $M_{\text{att}}$ to reconstruct the private data $\mathcal{D}_{\text{vic}}$, with the red and purple text representing precisely recovered and mostly recovered PII data, respectively.}
  \label{fig:system}
  \vspace{-1em}
\end{figure*}


We introduce AIA, a framework for training data reconstruction through activations in decentralized training. During the victim model training, an attacker at the $i_\text{att}$-th stage has access to the activations $\bm a_{i_\text{att}-1}^{(t)}$ passed from $M_{i_\text{att}-1}$ during forward propagation. We denote the mapping function from the original training data $\bm d_\text{vic}^{(t)}$ to $\bm a_{i_\text{att}-1}^{(t)}$ as $f_{[1:i_\text{att}-1]}^{(t)}(\cdot)$. Therefore, we can conclude that: 
$$
\bm a_{i_\text{att}-1}^{(t)}=f_{[1:i_\text{att}-1]}^{(t)}(\bm d_\text{vic}^{(t)})
$$
The attacker's goal can thus be simplified to constructing a mapping function $\phi \approx  (f_{[1:i_\text{att}-1]}^{(t)})^{-1}(\cdot)$ that reconstructs $\bm d_\text{vic}^{(t)}$ from $\bm a_{i_\text{att}-1}^{(t)}$. AIA adopts a learning-based approach by training a generative model to perform this reconstruction. In simple terms, AIA consists of two steps: (1) \textbf{Shadow Dataset Construction}: The attacker first generates a shadow dataset containing text labels and corresponding activations leveraging a public dataset. (2) \textbf{Attack Model Training}: The attacker then uses $\mathcal{D}_\text{sha}$ to train a generative attack model ${M}_\text{att}$ that learns the mapping function $\phi$. Finally, the attacker inputs the actual activations transmitted during the victim model training into ${M}_\text{att}$ to reconstruct the training data. We provide a detailed description of these two steps in the following.



% \subsection{Pipeline Parallelism Training}
% Consider a pipeline training system in which a language model is partitioned into multiple segments, with each segment trained at a different stage. Except for the first and last stages, one intermediate stage is "curious." This curious stage does not directly interfere with the model's training process, as such actions would be easily detectable. Instead, it is primarily interested in the victim's private training data. When the victim fine-tunes the language model with their own data within this system, the curious stage covertly stores the intermediate activation values during training. These activations are subsequently used to invert and reconstruct the user’s original dataset. This process does not affect the model’s training performance, but it does require additional storage space, meaning that the user remains unaware of this activity.

% \subsection{Shadow Model}
% The primary goal of the shadow model is to generate shadow activations from the attacker's shadow dataset, ensuring that their distribution closely mirrors that of the victim activations. However, since the fine-tuning process of the victim model is dynamic, the victim activations change with each training iteration. This presents a significant challenge: maintaining the consistency of the shadow activations with the victim activations is nearly impossible. First, the attacker has no access to the victim's private data, making it difficult to align the shadow model’s activations with the victim’s. Second, fine-tuning the shadow model simultaneously with the fine-tuning of the victim model incurs substantial computational costs. The difficulty in achieving precise alignment between shadow and victim activations underscores the inherent challenges in conducting an effective activation inversion attack.

% Fortunately, we observe that pre-trained models released by various institutions and organizations have undergone thorough training, demonstrating impressive generalization capabilities. As a result, when users fine-tune these models on their own private data, the changes in activations are relatively small. This observation allows us to directly utilize the pre-trained weights of various models from Hugging Face as the shadow model. In other words, we do not need to invest any additional effort in fine-tuning the shadow model, significantly reducing the cost of the attack. The rationale behind using pre-trained model weights directly as the shadow model is further elaborated through experiments in Section \ref{sec:act_cos}.

\subsection{Step 1: Shadow Dataset Construction}

Since the attacker cannot access $\mathcal{D}_\text{vic}$, a straightforward approach is to construct a shadow dataset $\mathcal{D}_\text{sha}$ using a public dataset $\mathcal{D}_\text{pub}$. Specifically, we use the frozen pre-trained LLM $M_\text{pre}$ as the shadow model $M_\text{sha}$, with the same type of the victim model, to generate shadow activations $\bm a_\text{sha}$, i.e., 
$$
\bm a_\text{sha}=M_{\text{sha}[1:i_\text{att} -1]}(\bm d_\text{pub})
$$
where $\bm d_\text{pub} \in \mathcal{D}_\text{pub}$. The rationale for this approach is analyzed in Section \ref{sec:act_cos}: the generalizability of $M_\text{pre}$ ensures that the activations remain relatively stable when fine-tuning the victim model $M_\text{vic}$ on $\mathcal{D}_\text{vic}$, allowing us to directly leverage the pre-trained weights from HuggingFace as $M_\text{sha}$. In other words, no additional effort is required to train $M_\text{sha}$, significantly reducing the cost of AIA.

\subsection{Step 2: Attack Model Training}
Next, we focus on training ${M}_\text{att}$ using the shadow dataset $\mathcal{D}_\text{sha}=\{(\bm a_\text{sha}, \bm d_\text{pub})\}$. ${M}_\text{att}$ is designed to take activations as input and output the distribution probabilities of the generated text. It consists of a set of decoder layers and an \texttt{lm\_head} layer. Structurally, it differs from a standard language model by the absence of the initial embedding layer. 
% In terms of training methodology, the loss function is set for a text reconstruction task, rather than a text generation task commonly used in language models. 
% The training objective is defined as:
Similar to the recent work~\cite{li2023sentence}, the training objective is to minimize the standard language model loss using teacher forcing~\cite{williams1989learning}:
$$
  L = - \sum_{k=1}^{N} \log P(y_k | x_1, x_2, \dots, x_{k-1})
$$
where $y_k$ is the target word, and $x_i$ represent the input activations. 
% This approach aims to reconstruct the original text, rather than generate new content as in traditional language modeling tasks. 
Finally, we input the activations $\bm a_{i_\text{att}-1}^{(t)}$ to ${M}_\text{att}$ and obtain $\bm d_\text{vic}^{(t)}$.



% \section{Challenges}
% \subsection{Victim Activations vs Shadow Activations}
% \label{sec:act_cos}
% The most critical aspect of activation inversion attacks is ensuring that the features of the shadow activations closely resemble those of the victim activations. This similarity is essential for the trained attack model to have transferability. Figure \ref{fig:layer_idx_act_cos} illustrates the cosine similarity of activations across different layers before and after fine-tuning on the GPT2-XL, Bloom-7B1, and LLaMA3-8B pre-trained models using various datasets. It is observed that as the layer index increases, the changes in the decoder layers nearer to the lm\_head layer become more pronounced. Nevertheless, the activation similarities in the final layers still exceed 50\%. During the fine-tuning process, we note that the decoder layers in the earlier stages show minimal variation, with activation similarities nearly reaching 100\%. This indicates that directly using a pre-trained model as the shadow model without additional training can still produce shadow activations closely resembling the victim activations.
% \begin{figure}[t]
%   \includegraphics[width=\linewidth]{figures/layer_idx_cos.pdf} 
%   \caption {Cosine Similarity of Activations Before and After Fine-Tuning Across Layer Indices.}
%   \label{fig:layer_idx_act_cos}
% \end{figure}






\section{Experiments}
\subsection{Experimental Setup}
\noindent\textbf{Victim models.}
\label{sec:experiment_setup}
We conduct experiments on three models: GPT2-XL~\cite{gpt2}, Bloom-7B1~\cite{bloom}, and LLaMA3-8B~\cite{llama3}, which have 48, 30, and 32 decoder layers, respectively. We directly download the pre-trained models from HuggingFace and use them as $M_\text{sha}$ to collect $\mathcal{D}_\text{sha}$. 
To investigate the effects of AIA under extreme conditions, we fine-tune $M_\text{vic}$ for 5 epochs on the corresponding dataset to induce overfitting on the privacy data, thereby maximizing the feature gap between $\mathcal{D}_\text{vic}$ and $\mathcal{D}_\text{sha}$.
% To explore the effects of AIA under extreme conditions, we fine-tune $M_\text{vic}$ for 5 epochs on the corresponding dataset to induce overfitting on the clean data. 
The training process is divided into 6 stages, with the assumption that the third stage is malicious. 
% Therefore, unless otherwise specified, all experiments are conducted at the one-third point of each model’s layers. 
The architecture of the attack model is identical to that of the victim model, with all attack models set to 12 decoder layers. 
% During the training of the attack model, the sequence length is set to 160. For fine-tuning the victim models, the sequence length is set to 1600 for LLaMA3-8B and Bloom-7B1, and 800 for GPT2-XL. The AdamW optimizer is used for all training and fine-tuning processes, with learning rates set to 5e-5 for GPT2-XL and Bloom-7B1, and 7e-5 for LLaMA3-8B, along with an epsilon value of 1e-8.

\noindent\textbf{Datasets.}
%Large language models are typically trained for text generation tasks using an autoregressive approach. 
% We use WikiText~\cite{wikitext} as the attacker's known dataset $\mathcal{D}_\text{pub}$ to construct the shadow dataset $\mathcal{D}_\text{sha}$. The WikiText dataset is a collection of high-quality, clean, and large-scale English text extracted from Wikipedia articles. The victim datasets $\mathcal{D}_\text{vic}$ include ArXiv, OpenWebText~\cite{openwebtext}, The Pile~\cite{pile}, and a public PII dataset\footnote{https://github.com/zzzzsdaw/PII-dataset} containing sensitive information. The ArXiv dataset is a large-scale collection of scientific papers from arXiv. The OpenWebText dataset is a high-quality, large-scale corpus of English web content curated from URLs shared on Reddit with high karma. The Pile is a diverse, 800GB English text dataset designed for training large language models, combining content from 22 high-quality sources, including books, academic papers, code, and web text. The PII dataset consists of 1,000 instances of sensitive information and includes 10 personally identifiable information (PII) types, including \textit{phone numbers}, \textit{email addresses}, and \textit{home addresses}, in a structured format. Figure \ref{fig:PII_data_example} presents an example of a PII data item. These data are randomly generated using regular expressions and do not represent real private information.
We use the WikiText~\cite{wikitext} dataset as the attacker's known dataset $\mathcal{D}_\text{pub}$ to construct the shadow dataset $\mathcal{D}_\text{sha}$. The victim datasets $\mathcal{D}_\text{vic}$ include ArXiv, OpenWebText~\cite{openwebtext}, The Pile~\cite{pile}, and a public PII dataset\footnote{https://github.com/zzzzsdaw/PII-dataset}, which contains sensitive information.  An example of a PII data item is shown in Figure \ref{fig:PII_data_example}.

\begin{figure}[t]
  \includegraphics[width=\columnwidth]{figures/PII.pdf}
  \caption{An example of PII data and baseline attacks. The private data includes information such as names, phone numbers, and email addresses. The True-Prefix attack leverages other private attributes to prompt the model to generate the target private attribute, while the SPT attack employs a trained soft prompt added before the query template to extract private information.}
  \label{fig:PII_data_example}
  \vspace{-1em}
\end{figure}


\noindent\textbf{Baselines.}
In the privacy leakage experiments, we adopt the following two methods as baselines. The two methods do not apply to decentralized training, we use them solely for comparison to illustrate the potential risks of our attack. Their attack examples can be seen in Figure \ref{fig:PII_data_example}.

\begin{itemize}[nolistsep, leftmargin=*, topsep=0pt]

    \item \textit{True-Prefix Attack}~\cite{true-prefix} utilizes real prefixes from $\mathcal{D}_\text{vic}$ to prompt the model. In our experiments, we use real PII data of other types within each PII item as the prompt, attempting to induce the model to output the value of the target PII type.

    \item \textit{SPT Attack}~\cite{SPT} trains an additional set of prompt embeddings, which are appended to the original query template. We train the prompt embeddings using 64 PII data pairs, during which the victim model remains frozen and does not require gradient updates.
    
\end{itemize}



% \noindent\textit{True-Prefix Attack~\cite{true-prefix}.} The true prefix attack utilizes real prefixes from the fine-tuning dataset to prompt the model. In our experiments, we use real PII data of other types within each PII item as the prompt, attempting to induce the model to output the value of target PII type.

% \noindent\textit{SPT Attack~\cite{SPT}.} The SPT attack trains an additional set of prompt embeddings, which are appended to the original query template. We train the prompt embeddings using 64 PII data pairs, during which the victim model remains frozen and does not require gradient updates.


\noindent\textbf{Evaluation metrics.}
To evaluate the quality of text reconstruction, we employ the following four metrics. 


\begin{itemize}[nolistsep, leftmargin=*, topsep=0pt]

    \item \textit{Perplexity}~\cite{perplexity} assesses the model's capability by measuring the probability distribution of its outputs, with lower values indicating better performance.

    \item \textit{ROUGE}~\cite{rouge} measures the similarity between the generated text and reference text by comparing overlapping words or phrases.

    \item \textit{BLEU}~\cite{bleu} evaluates the similarity between generated text and reference text based on n-gram overlap and is commonly used in machine translation tasks.

    \item \textit{Embedding cosine similarity} calculates the semantic similarity between the generated text and reference text using the all-MiniLM-L6-v2 model\footnote{https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2}~\cite{minilmv2}.
    
\end{itemize}

% \textbf{Perplexity}~\cite{perplexity} assesses the model's capability by measuring the probability distribution of its outputs, with lower values indicating better performance. \textbf{ROUGE}~\cite{rouge} measures the similarity between the generated text and reference text by comparing overlapping words or phrases. \textbf{BLEU}~\cite{bleu} evaluates the similarity between generated text and reference text based on n-gram overlap and is commonly used in machine translation tasks. \textbf{Embedding cosine similarity} calculates the semantic similarity between the generated text and reference text using the MiniLM model~\cite{minilmv2}.

% In the privacy leakage experiments, we evaluate the attack success rates of our method and two baselines in exactly matching the value of the target PII type. Precise matching is defined as the ability to output numbers and letters in the correct sequence while ignoring spaces and special characters, which do not affect the evaluation of precision.

In the privacy leakage experiments, we evaluate the \textit{attack success rate (ASR)} of our AIA method and two baselines in precisely recovering the values of the target PII types. Precise recovery is defined as correctly outputting the digits and letters in the correct order. During the matching process between the generated data and the original private data, spaces and special characters, such as '-', are ignored, as they do not affect the identification of private data values. The \textit{ASR} is calculated as the ratio of the number of precisely recovered data entries to the total amount of data.


% \section{Results}
\subsection{Text Reconstruction}
\input{tables/base_result}
Table~\ref{tab:base_result} presents the performance of AIA across different victim LLMs and datasets. The results indicate that the perplexity of the generated sentences remains below 20, with most values under 10, suggesting that the reconstructed text is relatively fluent and closely aligns with the original fine-tuning data. Both ROUGE-1 and BLEU-1 scores exceed 0.7, with the highest result reaching nearly 0.95, which confirms that the majority of words from the original fine-tuning data are accurately recovered. ROUGE-L scores are generally higher than ROUGE-2, indicating that the generated text maintains high global similarity while exhibiting slightly lower local continuity. However, this slight discontinuity in certain lexical elements has minimal impact on human readability. We further compute the cosine similarity between the embeddings of the generated text and the original text, with values ranging from 0.77 to 0.96, confirming a high level of semantic similarity. These results validate the effectiveness of AIA in reconstructing the original fine-tuning data.

\subsection{Privacy Leakage}
% \input{tables/pii_compare}
\input{tables/pii_attack_model}

\noindent{\textbf{Results compared with baselines.}}
We compare the ASR of AIA with the baselines on the PII types of email and phone, with the detailed results presented in Table~\ref{tab:pii_compare}. The findings indicate that our method performs effectively on both phone numbers and email addresses. For instance, the Bloom-7B1 model achieves precise recovery rates of 41\% for phone numbers and 61\% for email addresses. Even the relatively less effective LLaMA3-8B model accurately recovers 15\% of phone numbers and 41\% of email addresses. 

In contrast, the \textit{True-Prefix Attack} and \textit{SPT Attack} exhibit poor performance, showing minimal success in recovering phone numbers. On the Bloom-7B1 model, both baselines recover only a small portion of email addresses, with ASR of 18\% and 10\%, respectively. We hypothesize that this discrepancy arises from the structure of the PII dataset, where email prefixes consist of a person's name combined with random numbers, enhancing the model's memory of the email. The GPT2-XL model recovers only 2\% to 4\% of email addresses, significantly lower than Bloom-7B1, likely due to its smaller size and weaker capacity for data retention. Notably, neither baseline is able to recover any private data accurately on the LLaMA3-8B model. This may be attributed to the LLaMA3-8B model's alignment and data protection mechanisms implemented during pre-training, which results in the frequent generation of placeholders such as “[email protected]”.

% making it resistant to being induced to output private data. The frequent generation of placeholders such as “[email protected]” further supports this observation.

\begin{figure*}[t]
  \includegraphics[width=\textwidth]{figures/PII_attack_example2.pdf}
  \caption{Three comparative examples of generated texts versus original data. The yellow text represents the original PII data, while the red and purple texts represent precisely recovered and mostly recovered PII data, respectively. The text recovery performance improves from left to right.}
  \label{fig:PII_attack_example}
  \vspace{-1em}
\end{figure*}

\input{tables/pii_attack_result}

\noindent{\textbf{Results on various PII types.}}
Table~\ref{tab:pii_attack_result} presents the ASR of AIA in precisely recovering the seven PII types: fax, birthday, SSN, address, job, bitcoin, and UUID. Remarkably, the ASR for birthdays and jobs approaches 100\%. Birthdays, which are short and highly structured numerical sequences, likely benefit from the model's pre-training exposure to similar formats, resulting in minimal changes to their semantic encoding after fine-tuning. Jobs, typically consisting of one to three words, are relatively easier to recover compared to other PII types. This observation is further supported by the ROUGE-1 and BLEU-1 results on the PII dataset across different victim LLMs shown in Table~\ref{tab:base_result}.

All victim models exhibit strong recovery performance for PII types other than Bitcoin addresses and UUID, with recovery rates generally ranging from one-third to over half of the data. Owing to the inherent irregularity and extended length characteristics of Bitcoin addresses and UUIDs, precise reconstruction is significantly more challenging. Specifically, only the GPT2-XL model achieves a recovery rate of approximately 20\% for the two PII types, while the ASR for Bloom-7B1 and LLaMA3-8B remains below 10\%. Notably, even in cases of incomplete reconstruction, the generated outputs maintain substantial proximity to ground truth values, exhibiting only minor character-level discrepancies in alphanumeric sequences (e.g., single-letter substitutions or partial numeric mismatches).

Figure~\ref{fig:PII_attack_example} shows three comparison examples between the generated text and the original private data, with the quality of text reconstruction improving from left to right. The majority of common words and PII data can be precisely recovered, as indicated by the red highlights in the figure. However, the recovery of less frequent words (e.g., "Bitcoin") and special characters (e.g., "@") tends to be less successful. Additionally, the recovery of named entities may occasionally be imprecise. For long character sequences, such as phone numbers or UUIDs, over 80\% of the characters are typically recovered, although some minor errors in individual characters or capitalization issues may occur, as highlighted in purple in the figure.
% Achieving precise recovery of such machine-irrelevant character sequences remains a challenging task.

\subsection{Ablation Study}
To explore the factors influencing the attack performance of AIA, we conducted three sets of ablation experiments on the decoder layer index, model size, and attack model architecture. The conclusions are as follows: 
\begin{itemize}[nolistsep, leftmargin=*, topsep=0pt]
    \item As the layer index increases, the attack performance decreases; however, the original private data can still be recovered to some extent. 
    \item The attack performance is independent of model size and AIA performs well in all model sizes.
    \item The attack performance is highly sensitive to the architecture of the attack model, with different architectures leading to poorer attack results.
\end{itemize}

\subsubsection{Decoder Layer Index}
Figure~\ref{fig:layer_idx} illustrates the trend of PPL on GPT2-XL and Bloom-7B1 models as the attacker's decoder layer index varies. The results show that as the decoder layer index increases, i.e., as the data leakage layer moves closer to the output layers, the overall attack effectiveness declines. This observation aligns with the trend described in Section \ref{sec:act_cos}, where the cosine similarity of activations before and after fine-tuning decreases as the decoder layer index increases. The decline in attack performance can be attributed to the greater changes in the activations of the decoder layers that is closer to the output layer during fine-tuning. 
% These layers are heavily involved in generating the final output and, therefore, undergo more significant updates to adapt to the fine-tuning dataset. As a result, the original patterns in these layers that could be exploited for text inversion become less stable, leading to reduced attack effectiveness.

Interestingly, when the cosine similarity of activations before and after fine-tuning drops below 60\% for a particular decoder layer, the perplexity of the generated text remains below 40. This indicates that the generated sentences become less natural, with noticeable grammatical or contextual inconsistencies, which suggests a reduction in the fluency and coherence of the generated texts. However, despite these linguistic limitations, the attacker is still able to infer the original fine-tuning data to a certain extent. This highlights the robustness of AIA, even when the stage controlled by the attacker is positioned further back in the pipeline.

% \begin{tcolorbox}[colframe=black,colback=gray!10,arc=3mm,boxrule=0.5mm]
% \textbf{Takeaways:} As the layer index increases, attack performance decreases, though some private data can still be recovered.
% \end{tcolorbox}

\begin{figure}[t]
  % \includegraphics[width=0.48\linewidth]{figures/gpt2_layer_idx.pdf} \hfill
  % \includegraphics[width=0.48\linewidth]{figures/bloom_layer_idx.pdf}
  \includegraphics[width=\linewidth]{figures/layer_idx_ppl.pdf} 
  \caption {The attack performance of AIA on GPT2-XL and Bloom-7B1 models as the attacker's decoder layer index varies, with the attack performance generally decreasing as the layer index increases.}
  \label{fig:layer_idx}
  \vspace{-1em}
\end{figure}

\subsubsection{Model Size}
\input{tables/model_size}
Table~\ref{tab:model_size} systematically presents the experimental results for GPT2 and Bloom models with varying parameter scales. To ensure comprehensive experiments, we select three representative configurations for each model family: the GPT2 series includes 355M, 774M, and 1.5B parameter variants, while the Bloom series comprises 560M, 1.7B, and 7.1B parameter configurations. 
% Notably, in the attack model architecture design, we implemented an adaptive depth configuration strategy that dynamically sets the decoder layers to one-third of the target model's depth (e.g., employing an 8-layer decoder for the 24-layer GPT2-355M model). 
The experimental results demonstrate that the attack performance of AIA is highly dependent on the victim dataset, and it maintains stable performance across different model sizes, with most PPL consistently below 10, ROUGE-L scores exceeding 0.9, and BLEU-4 scores above 0.6 in most cases. 
% These quantitative findings underscore the robustness and generalizability of AIA in attacking models of varying sizes.

% \begin{tcolorbox}[colframe=black,colback=gray!10,arc=3mm,boxrule=0.5mm]
% \textbf{Takeaways:} AIA performs well across models of different sizes.
% \end{tcolorbox}

\subsubsection{Attack Model Architecture}
% \input{tables/attack_model}
To explore the impact of the attack model architecture on attack performance, we conduct experiments using Mistral~\cite{mistral} and Qwen2.5~\cite{qwen2.5} as attack model architectures and compare them to the victim model architecture. Each attack model is configured with six decoder layers. As shown in Table \ref{tab:attack_model}, while all attack models exhibit excellent performance when trained on the shadow dataset, their effectiveness significantly declines when transitioning to inverting the victim dataset after switching the attack model architecture. Notably, even the best-performing configuration on GPT2-XL still yields perplexity values ranging from 24 to 120. On the Bloom-7B1 and LLaMA3-8B models, the perplexity can even reach values above a thousand, rendering AIA almost completely ineffective.

% \begin{tcolorbox}[colframe=black,colback=gray!10,arc=3mm,boxrule=0.5mm]
% \textbf{Takeaways:} AIA is highly sensitive to the architecture of the attack model.
% \end{tcolorbox}

% Two main factors contribute to these results. First, the variation in attack performance is partially due to the differing distributions of the datasets used. Since the shadow dataset and victim dataset may have different underlying characteristics, it is expected that a model trained on one would struggle to generalize well to the other. Secondly, the intermediate activation values are highly intertwined with the model architecture itself. Each model operates within its own distinct semantic space, meaning that the activations in Mistral and Qwen2.5 differ substantially from those in the victim models. This architectural misalignment leads to overfitting in the attack model, as it fails to generalize effectively to the new data. Therefore, even though the attack models perform well during training on the shadow dataset, they struggle to perform similarly when applied to victim datasets with different architecture-specific nuances.





\section{Conclusion}
% In this paper, we propose an honest-but-curious pipeline training system and introduce a text inversion attack based on intermediate activations. Extensive experimental evaluations demonstrate that modern pretrained large language models exhibit strong generalization capabilities, such that fine-tuning with specific datasets leads to minimal updates to the model. We establish the feasibility of recovering unknown fine-tuning data from pretrained models. Furthermore, we use a set of PII datasets to evaluate the potential for privacy data leakage, showing that a significant portion of private data can be accurately recovered. Although text inversion attacks in pipeline training systems can have severe consequences, defenses against such attacks remain underexplored. We call on researchers to address this critical privacy risk and to develop effective, low-cost defense mechanisms to counteract text inversion attacks.
In this paper, we explore the privacy risks inherent in decentralized training, particularly in scenarios where an honest-but-curious attacker exists in the pipeline. Despite lacking access to the complete model weights, we demonstrate the feasibility of simulating the victim model using a pre-trained model and introduce Activation Inversion Attack (AIA). We conduct extensive experiments on various large language models and public datasets to emphasize the effectiveness of our attack. As the application of decentralized training continues to grow, we call for the development of effective defense measures to mitigate the risk of AIA.


\section*{Limitations}
Our method has a key limitation: the architecture of the attack model must be consistent with that of the clean model. While the attack model performs well on the shadow dataset when using different architectures, its effectiveness significantly decreases when applied to the clean dataset. This constraint limits the flexibility in choosing the attack model. Additionally, the generated text exhibits issues such as lack of fluency, inconsistencies in letter casing, errors with special characters, uncommon words, and difficulty in accurately recovering long sequences. These observations indicate that our method is influenced by the challenges of transferring to unknown data distributions and the variations introduced during model fine-tuning.


\section*{Ethics Statement}
We declare that all authors of this paper adhere to the ACM Code of Ethics and uphold its code of conduct. This paper investigates activation inversion attack in decentralized training. The objective of our work is to highlight the potential data leakage risks associated with decentralized training, aiming to encourage the community to give greater attention to privacy protection in such settings and to advocate for measures to prevent such information leaks. No real sensitive data is used in our experiments; all experiments are conducted with publicly available datasets. The data in the PII dataset we use is randomly generated and does not represent actual private information. All models employed in this study are open-source and thus do not pose any threat to proprietary models.



% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\input{main.bbl}
% \bibliography{main}

\newpage
\appendix

\section{Hyperparameters}
During the training of the attack model, the sequence length is set to 160. For fine-tuning the victim models, the sequence length is set to 1600 for LLaMA3-8B and Bloom-7B1, and 800 for GPT2-XL. The AdamW\cite{loshchilov2017decoupled} optimizer is used for all training and fine-tuning processes, with learning rates set to 5e-5 for GPT2-XL and Bloom-7B1, and 7e-5 for LLaMA3-8B, along with an epsilon value of 1e-8.

\section{Datasets}
The WikiText dataset serves as a high-quality, clean, and large-scale collection of English text extracted from Wikipedia articles, providing a solid foundation for creating the shadow dataset for the attacker's model. The ArXiv dataset is a large-scale collection of scientific papers from the arXiv repository. The OpenWebText dataset is a high-quality, large-scale corpus of English web content, curated from URLs shared on Reddit with high karma. The Pile is an 800GB, diverse English text dataset designed for training large language models, combining content from 22 high-quality sources, including books, academic papers, code, and web text. The PII dataset consists of 1,000 instances of sensitive information and includes 10 types of personally identifiable information (PII), such as phone numbers, email addresses, and home addresses, presented in a structured format. These data are randomly generated using regular expressions and do not represent real private information.

\section{Toolkits}
We use the NLTK package to measure the BLEU score, the rouge\_score library to calculate the ROUGE score, and scikit-learn to compute the cosine similarity.

\section{True-Prefix and SPT Attack Examples}
Figure \ref{fig:true_prefix_example} and Figure \ref{fig:spt_example} present two examples of True-Prefix~\cite{true-prefix} and SPT~\cite{SPT} attacks, respectively. In the True-Prefix attack, we insert real data of additional PII types, such as address or birthday, before the prompt templates, as shown in the blue sections in Figure \ref{fig:true_prefix_example}. In the SPT attack, we train on 64 PII data pairs for 5 epochs to obtain the soft prompt embeddings, which are set to a length of 10. The soft prompt embeddings are then concatenated before the prompt templates. During the training, the victim model remains frozen, with no gradient updates applied.

\begin{figure}[H]
  \includegraphics[width=\linewidth]{figures/True-Prefix_example.pdf} 
  \caption {Two True-Prefix attack examples. Blue text represents the real private data, while green and red text indicate successful and failed privacy theft, respectively.}
  \label{fig:true_prefix_example}
  \vspace{-1em}
\end{figure}

\begin{figure}[H]
  \includegraphics[width=\linewidth]{figures/SPT_example.pdf} 
  \caption {Two SPT attack examples. Orange text represents the soft prompt embeddings, with green and red text indicating successful and failed privacy theft, respectively.}
  \label{fig:spt_example}
  \vspace{-1em}
\end{figure}

\end{document}
