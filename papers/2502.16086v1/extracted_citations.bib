@article{SPT,
  title={Propile: Probing privacy leakage in large language models},
  author={Kim, Siwon and Yun, Sangdoo and Lee, Hwaran and Gubri, Martin and Yoon, Sungroh and Oh, Seong Joon},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{balunovic2022lamp,
  title={Lamp: Extracting text from gradients with language model priors},
  author={Balunovic, Mislav and Dimitrov, Dimitar and Jovanovi{\'c}, Nikola and Vechev, Martin},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={7641--7654},
  year={2022}
}

@inproceedings{boenisch2023curious,
  title={When the curious abandon honesty: Federated learning is not private},
  author={Boenisch, Franziska and Dziedzic, Adam and Schuster, Roei and Shamsabadi, Ali Shahin and Shumailov, Ilia and Papernot, Nicolas},
  booktitle={2023 IEEE 8th European Symposium on Security and Privacy (EuroS\&P)},
  pages={175--199},
  year={2023},
  organization={IEEE}
}

@inproceedings{borzunov2022training,
  title={Training transformers together},
  author={Borzunov, Alexander and Ryabinin, Max and Dettmers, Tim and Lhoest, Quentin and Saulnier, Lucile and Diskin, Michael and Jernite, Yacine and Wolf, Thomas},
  booktitle={NeurIPS 2021 Competitions and Demonstrations Track},
  pages={335--342},
  year={2022},
  organization={PMLR}
}

@inproceedings{carlini2021extracting,
  title={Extracting training data from large language models},
  author={Carlini, Nicholas and Tramer, Florian and Wallace, Eric and Jagielski, Matthew and Herbert-Voss, Ariel and Lee, Katherine and Roberts, Adam and Brown, Tom and Song, Dawn and Erlingsson, Ulfar and others},
  booktitle={30th USENIX Security Symposium (USENIX Security 21)},
  pages={2633--2650},
  year={2021}
}

@article{das2024security,
  title={Security and privacy challenges of large language models: A survey},
  author={Das, Badhan Chandra and Amini, M Hadi and Wu, Yanzhao},
  journal={ACM Computing Surveys},
  year={2024},
  publisher={ACM New York, NY}
}

@article{fowl2022decepticons,
  title={Decepticons: Corrupted transformers breach privacy in federated learning for language models},
  author={Fowl, Liam and Geiping, Jonas and Reich, Steven and Wen, Yuxin and Czaja, Wojtek and Goldblum, Micah and Goldstein, Tom},
  journal={arXiv preprint arXiv:2201.12675},
  year={2022}
}

@article{gandhi2024improving,
  title={Improving training time and GPU utilization in geo-distributed language model training},
  author={Gandhi, Rohan and Tandon, Karan and Bhattacherjee, Debopam and Padmanabhan, Venkata N and others},
  journal={arXiv preprint arXiv:2411.14458},
  year={2024}
}

@article{gupta2022recovering,
  title={Recovering private text in federated learning of language models},
  author={Gupta, Samyak and Huang, Yangsibo and Zhong, Zexuan and Gao, Tianyu and Li, Kai and Chen, Danqi},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={8130--8143},
  year={2022}
}

@article{huang2022large,
  title={Are large pre-trained language models leaking your personal information?},
  author={Huang, Jie and Shao, Hanyin and Chang, Kevin Chen-Chuan},
  journal={arXiv preprint arXiv:2205.12628},
  year={2022}
}

@inproceedings{huang2024transferable,
    title = "Transferable Embedding Inversion Attack: Uncovering Privacy Risks in Text Embeddings without Model Queries",
    author="Huang, Yu-Hsiang  and
      Tsai, Yuche  and
      Hsiao, Hsiang  and
      Lin, Hong-Yi  and
      Lin, Shou-De",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    year={2024},
    pages = "4193--4205",
}

@inproceedings{jang2023oobleck,
  title={Oobleck: Resilient distributed training of large models using pipeline templates},
  author={Jang, Insu and Yang, Zhenning and Zhang, Zhen and Jin, Xin and Chowdhury, Mosharaf},
  booktitle={Proceedings of the 29th Symposium on Operating Systems Principles},
  pages={382--395},
  year={2023}
}

@article{li2022you,
  title={You don't know my favorite color: Preventing dialogue representations from revealing speakers' private personas},
  author={Li, Haoran and Song, Yangqiu and Fan, Lixin},
  journal={arXiv preprint arXiv:2205.10228},
  year={2022}
}

@inproceedings{li2023sentence,
  title={Sentence Embedding Leaks More Information than You Expect: Generative Embedding Inversion Attack to Recover the Whole Sentence},
  author={Li, Haoran and Xu, Mingshi and Song, Yangqiu},
  booktitle={Findings of the Association for Computational Linguistics: ACL 2023},
  pages={14022--14040},
  year={2023}
}

@InProceedings{lu2024position,
  title={Position: Exploring the Robustness of Pipeline-Parallelism-Based Decentralized Training},
  author={Lu, Lin and Dai, Chenxi and Tao, Wangcheng and Yuan, Binhang and Sun, Yanan and Zhou, Pan},
  booktitle={Proceedings of the 41st International Conference on Machine Learning},
  pages={32978--32989},
  year={2024},
  volume={235},
  series={Proceedings of Machine Learning Research},
}

@article{morris2023text,
  title={Text embeddings reveal (almost) as much as text},
  author={Morris, John X and Kuleshov, Volodymyr and Shmatikov, Vitaly and Rush, Alexander M},
  journal={arXiv preprint arXiv:2310.06816},
  year={2023}
}

@inproceedings{nakka2024pii,
  title={PII-Compass: Guiding LLM training data extraction prompts towards the target PII via grounding},
  author={Nakka, Krishna and Frikha, Ahmed and Mendes, Ricardo and Jiang, Xue and Zhou, Xuebing},
  booktitle={Proceedings of the Fifth Workshop on Privacy in Natural Language Processing},
  pages={63--73},
  year={2024}
}

@inproceedings{ryabinin2023swarm,
  title={Swarm parallelism: Training large models can be surprisingly communication-efficient},
  author={Ryabinin, Max and Dettmers, Tim and Diskin, Michael and Borzunov, Alexander},
  booktitle={International Conference on Machine Learning},
  pages={29416--29440},
  year={2023},
  organization={PMLR}
}

@inproceedings{song2020information,
  title={Information leakage in embedding models},
  author={Song, Congzheng and Raghunathan, Ananth},
  booktitle={Proceedings of the 2020 ACM SIGSAC conference on computer and communications security},
  pages={377--390},
  year={2020}
}

@article{tang2023fusionai,
  title={Fusionai: Decentralized training and deploying llms with massive consumer-level gpus},
  author={Tang, Zhenheng and Wang, Yuxin and He, Xin and Zhang, Longteng and Pan, Xinglin and Wang, Qiang and Zeng, Rongfei and Zhao, Kaiyong and Shi, Shaohuai and He, Bingsheng and others},
  journal={arXiv preprint arXiv:2309.01172},
  year={2023}
}

@article{tang2024fusionllm,
  title={Fusionllm: A decentralized llm training system on geo-distributed gpus with adaptive compression},
  author={Tang, Zhenheng and Kang, Xueze and Yin, Yiming and Pan, Xinglin and Wang, Yuxin and He, Xin and Wang, Qiang and Zeng, Rongfei and Zhao, Kaiyong and Shi, Shaohuai and others},
  journal={arXiv preprint arXiv:2410.12707},
  year={2024}
}

@inproceedings{thorpe2023bamboo,
  title={Bamboo: Making preemptible instances resilient for affordable training of large $\{$DNNs$\}$},
  author={Thorpe, John and Zhao, Pengzhan and Eyolfson, Jonathan and Qiao, Yifan and Jia, Zhihao and Zhang, Minjia and Netravali, Ravi and Xu, Guoqing Harry},
  booktitle={20th USENIX Symposium on Networked Systems Design and Implementation (NSDI 23)},
  pages={497--513},
  year={2023}
}

@inproceedings{wang2023cocktailsgd,
  title={Cocktailsgd: Fine-tuning foundation models over 500mbps networks},
  author={Wang, Jue and Lu, Yucheng and Yuan, Binhang and Chen, Beidi and Liang, Percy and De Sa, Christopher and Re, Christopher and Zhang, Ce},
  booktitle={International Conference on Machine Learning},
  pages={36058--36076},
  year={2023},
  organization={PMLR}
}

@inproceedings{wu2023learning,
  title={Learning to invert: Simple adaptive attacks for gradient inversion in federated learning},
  author={Wu, Ruihan and Chen, Xiangyu and Guo, Chuan and Weinberger, Kilian Q},
  booktitle={Uncertainty in Artificial Intelligence},
  pages={2293--2303},
  year={2023},
  organization={PMLR}
}

@article{yan2024protecting,
  title={On protecting the data privacy of large language models (llms): A survey},
  author={Yan, Biwei and Li, Kun and Xu, Minghui and Dong, Yueyan and Zhang, Yue and Ren, Zhaochun and Cheng, Xiuzhen},
  journal={arXiv preprint arXiv:2403.05156},
  year={2024}
}

@article{yuan2022decentralized,
  title={Decentralized training of foundation models in heterogeneous environments},
  author={Yuan, Binhang and He, Yongjun and Davis, Jared and Zhang, Tianyi and Dao, Tri and Chen, Beidi and Liang, Percy S and Re, Christopher and Zhang, Ce},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={25464--25477},
  year={2022}
}

@article{zhu2019deep,
  title={Deep leakage from gradients},
  author={Zhu, Ligeng and Liu, Zhijian and Han, Song},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

