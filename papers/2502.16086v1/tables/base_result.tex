% tables.tex

\begin{table*}[ht]
\centering
\caption{Text reconstruction performance of GPT2-XL, Bloom-7B1, and LLaMA3-8B on four datasets. For all metrics except PPL, higher values indicate better performance.}
\label{tab:base_result}
\resizebox{0.94\textwidth}{!}{\begin{tabular}{ccc ccc ccc c}
\toprule[2pt]
\multirow{2}{*}{\textbf{Victim Model}} & \multirow{2}{*}{\textbf{Dataset}} & \multirow{2}{*}{\textbf{PPL}} & \multicolumn{3}{c}{\textbf{ROUGE}} & \multicolumn{3}{c}{\textbf{BLEU}} & \multirow{2}{*}{\textbf{COS}} \\ 
\cmidrule(lr){4-6} \cmidrule(lr){7-9}
                              &                          &                      & \textbf{ROUGE-1}   & \textbf{ROUGE-2}   & \textbf{ROUGE-L}    & \textbf{BLEU-1}    & \textbf{BLEU-2}    & \textbf{BLEU-4}    &                      \\ \hline
\multirow{4}{*}{GPT2-XL}      & PIIs                     & 3.73                 & 0.84     & 0.74     & 0.84      & 0.77     & 0.71     & 0.59     & 0.89                 \\
                              & openwebtext              & 3.09                 & 0.95     & 0.90     & 0.95      & 0.88     & 0.84     & 0.77     & 0.94                 \\
                              & arxiv                    & 5.43                 & 0.92     & 0.85     & 0.92      & 0.81     & 0.75     & 0.64     & 0.92                 \\
                              & pile                     & 1.65                 & 0.98     & 0.95     & 0.98      & 0.95     & 0.93     & 0.89     & 0.97                 \\ \hline
\multirow{4}{*}{Bloom-7B1}    & PIIs                     & 14.82                & 0.80     & 0.67     & 0.80      & 0.67     & 0.60     & 0.47     & 0.89                 \\
                              & openwebtext              & 4.64                 & 0.95     & 0.92     & 0.95      & 0.89     & 0.86     & 0.80     & 0.95                 \\
                              & arxiv                    & 15.45                & 0.91     & 0.83     & 0.90      & 0.77     & 0.70     & 0.56     & 0.90                 \\
                              & pile                     & 2.09                 & 0.97     & 0.95     & 0.97      & 0.95     & 0.93     & 0.90     & 0.95                 \\ \hline
\multirow{4}{*}{LLaMA3-8B}    & PIIs                     & 7.36                 & 0.80     & 0.67     & 0.79      & 0.73     & 0.66     & 0.54     & 0.77                 \\
                              & openwebtext              & 6.50                 & 0.93     & 0.88     & 0.93      & 0.88     & 0.84     & 0.77     & 0.88                 \\
                              & arxiv                    & 9.26                 & 0.88     & 0.78     & 0.88      & 0.80     & 0.73     & 0.60     & 0.83                 \\
                              & pile                     & 2.18                 & 0.96     & 0.93     & 0.96      & 0.94     & 0.92     & 0.89     & 0.92                 \\ \bottomrule[1.5pt]
\end{tabular}}
\vspace{-1em}
\end{table*}

% \begin{table*}[ht]
% \centering
% \caption{Text reconstruction performance of GPT-2-XL, Bloom-7B1, and LLaMA3-8B on four datasets. For all metrics except PPL, higher values indicate better performance.}
% \label{tab:base_result}
% \resizebox{\textwidth}{!}{
% \begin{tabular}{ccc|ccc|ccc|c}
% \toprule[2pt]
% % \hline
% \multirow{2}{*}{Victim Model} & \multirow{2}{*}{Dataset} & \multirow{2}{*}{PPL} & \multicolumn{3}{c|}{Rouge}      & \multicolumn{3}{c|}{Bleu}      & \multirow{2}{*}{Cos} \\ \cline{4-9}
%                               &                          &                      & Rouge1   & Rouge2   & RougeL    & Bleu1    & Bleu2    & Bleu4    &                      \\ \hline
% \multirow{4}{*}{Gpt2-xl}      & PIIs                     & 3.73                 & 0.84     & 0.74     & 0.84      & 0.77     & 0.71     & 0.59     & 0.89                 \\
%                               & openwebtext              & 3.09                 & 0.95     & 0.90     & 0.95      & 0.88     & 0.84     & 0.77     & 0.94                 \\
%                               & arxiv                    & 5.43                 & 0.92     & 0.85     & 0.92      & 0.81     & 0.75     & 0.64     & 0.92                 \\
%                               & pile                     & 1.65                 & 0.98     & 0.95     & 0.98      & 0.95     & 0.93     & 0.89     & 0.97                 \\ \hline
% \multirow{4}{*}{Bloom-7b1}    & PIIs                     & 14.82                & 0.80     & 0.67     & 0.80      & 0.67     & 0.60     & 0.47     & 0.89                 \\
%                               & openwebtext              & 4.64                 & 0.95     & 0.92     & 0.95      & 0.89     & 0.86     & 0.80     & 0.95                 \\
%                               & arxiv                    & 15.45                & 0.91     & 0.83     & 0.90      & 0.77     & 0.70     & 0.56     & 0.90                 \\
%                               & pile                     & 2.09                 & 0.97     & 0.95     & 0.97      & 0.95     & 0.93     & 0.90     & 0.95                 \\ \hline
% \multirow{4}{*}{Llama3-8b}    & PIIs                     & 7.36                 & 0.80     & 0.67     & 0.79      & 0.73     & 0.66     & 0.54     & 0.77                 \\
%                               & openwebtext              & 6.50                 & 0.93     & 0.88     & 0.93      & 0.88     & 0.84     & 0.77     & 0.88                 \\
%                               & arxiv                    & 9.26                 & 0.88     & 0.78     & 0.88      & 0.80     & 0.73     & 0.60     & 0.83                 \\
%                               & pile                     & 2.18                 & 0.96     & 0.93     & 0.96      & 0.94     & 0.92     & 0.89     & 0.92                 \\ \bottomrule[1.5pt]

% \end{tabular}
% }
% \end{table*}