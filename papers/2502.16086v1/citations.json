[
  {
    "index": 0,
    "papers": [
      {
        "key": "yuan2022decentralized",
        "author": "Yuan, Binhang and He, Yongjun and Davis, Jared and Zhang, Tianyi and Dao, Tri and Chen, Beidi and Liang, Percy S and Re, Christopher and Zhang, Ce",
        "title": "Decentralized training of foundation models in heterogeneous environments"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "ryabinin2023swarm",
        "author": "Ryabinin, Max and Dettmers, Tim and Diskin, Michael and Borzunov, Alexander",
        "title": "Swarm parallelism: Training large models can be surprisingly communication-efficient"
      },
      {
        "key": "wang2023cocktailsgd",
        "author": "Wang, Jue and Lu, Yucheng and Yuan, Binhang and Chen, Beidi and Liang, Percy and De Sa, Christopher and Re, Christopher and Zhang, Ce",
        "title": "Cocktailsgd: Fine-tuning foundation models over 500mbps networks"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "gandhi2024improving",
        "author": "Gandhi, Rohan and Tandon, Karan and Bhattacherjee, Debopam and Padmanabhan, Venkata N and others",
        "title": "Improving training time and GPU utilization in geo-distributed language model training"
      },
      {
        "key": "tang2024fusionllm",
        "author": "Tang, Zhenheng and Kang, Xueze and Yin, Yiming and Pan, Xinglin and Wang, Yuxin and He, Xin and Wang, Qiang and Zeng, Rongfei and Zhao, Kaiyong and Shi, Shaohuai and others",
        "title": "Fusionllm: A decentralized llm training system on geo-distributed gpus with adaptive compression"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "tang2023fusionai",
        "author": "Tang, Zhenheng and Wang, Yuxin and He, Xin and Zhang, Longteng and Pan, Xinglin and Wang, Qiang and Zeng, Rongfei and Zhao, Kaiyong and Shi, Shaohuai and He, Bingsheng and others",
        "title": "Fusionai: Decentralized training and deploying llms with massive consumer-level gpus"
      },
      {
        "key": "borzunov2022training",
        "author": "Borzunov, Alexander and Ryabinin, Max and Dettmers, Tim and Lhoest, Quentin and Saulnier, Lucile and Diskin, Michael and Jernite, Yacine and Wolf, Thomas",
        "title": "Training transformers together"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "thorpe2023bamboo",
        "author": "Thorpe, John and Zhao, Pengzhan and Eyolfson, Jonathan and Qiao, Yifan and Jia, Zhihao and Zhang, Minjia and Netravali, Ravi and Xu, Guoqing Harry",
        "title": "Bamboo: Making preemptible instances resilient for affordable training of large $\\{$DNNs$\\}$"
      },
      {
        "key": "jang2023oobleck",
        "author": "Jang, Insu and Yang, Zhenning and Zhang, Zhen and Jin, Xin and Chowdhury, Mosharaf",
        "title": "Oobleck: Resilient distributed training of large models using pipeline templates"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "lu2024position",
        "author": "Lu, Lin and Dai, Chenxi and Tao, Wangcheng and Yuan, Binhang and Sun, Yanan and Zhou, Pan",
        "title": "Position: Exploring the Robustness of Pipeline-Parallelism-Based Decentralized Training"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "yuan2022decentralized",
        "author": "Yuan, Binhang and He, Yongjun and Davis, Jared and Zhang, Tianyi and Dao, Tri and Chen, Beidi and Liang, Percy S and Re, Christopher and Zhang, Ce",
        "title": "Decentralized training of foundation models in heterogeneous environments"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "ryabinin2023swarm",
        "author": "Ryabinin, Max and Dettmers, Tim and Diskin, Michael and Borzunov, Alexander",
        "title": "Swarm parallelism: Training large models can be surprisingly communication-efficient"
      },
      {
        "key": "wang2023cocktailsgd",
        "author": "Wang, Jue and Lu, Yucheng and Yuan, Binhang and Chen, Beidi and Liang, Percy and De Sa, Christopher and Re, Christopher and Zhang, Ce",
        "title": "Cocktailsgd: Fine-tuning foundation models over 500mbps networks"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "thorpe2023bamboo",
        "author": "Thorpe, John and Zhao, Pengzhan and Eyolfson, Jonathan and Qiao, Yifan and Jia, Zhihao and Zhang, Minjia and Netravali, Ravi and Xu, Guoqing Harry",
        "title": "Bamboo: Making preemptible instances resilient for affordable training of large $\\{$DNNs$\\}$"
      },
      {
        "key": "jang2023oobleck",
        "author": "Jang, Insu and Yang, Zhenning and Zhang, Zhen and Jin, Xin and Chowdhury, Mosharaf",
        "title": "Oobleck: Resilient distributed training of large models using pipeline templates"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "gandhi2024improving",
        "author": "Gandhi, Rohan and Tandon, Karan and Bhattacherjee, Debopam and Padmanabhan, Venkata N and others",
        "title": "Improving training time and GPU utilization in geo-distributed language model training"
      },
      {
        "key": "tang2024fusionllm",
        "author": "Tang, Zhenheng and Kang, Xueze and Yin, Yiming and Pan, Xinglin and Wang, Yuxin and He, Xin and Wang, Qiang and Zeng, Rongfei and Zhao, Kaiyong and Shi, Shaohuai and others",
        "title": "Fusionllm: A decentralized llm training system on geo-distributed gpus with adaptive compression"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "tang2023fusionai",
        "author": "Tang, Zhenheng and Wang, Yuxin and He, Xin and Zhang, Longteng and Pan, Xinglin and Wang, Qiang and Zeng, Rongfei and Zhao, Kaiyong and Shi, Shaohuai and He, Bingsheng and others",
        "title": "Fusionai: Decentralized training and deploying llms with massive consumer-level gpus"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "zhu2019deep",
        "author": "Zhu, Ligeng and Liu, Zhijian and Han, Song",
        "title": "Deep leakage from gradients"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "balunovic2022lamp",
        "author": "Balunovic, Mislav and Dimitrov, Dimitar and Jovanovi{\\'c}, Nikola and Vechev, Martin",
        "title": "Lamp: Extracting text from gradients with language model priors"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "gupta2022recovering",
        "author": "Gupta, Samyak and Huang, Yangsibo and Zhong, Zexuan and Gao, Tianyu and Li, Kai and Chen, Danqi",
        "title": "Recovering private text in federated learning of language models"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "fowl2022decepticons",
        "author": "Fowl, Liam and Geiping, Jonas and Reich, Steven and Wen, Yuxin and Czaja, Wojtek and Goldblum, Micah and Goldstein, Tom",
        "title": "Decepticons: Corrupted transformers breach privacy in federated learning for language models"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "boenisch2023curious",
        "author": "Boenisch, Franziska and Dziedzic, Adam and Schuster, Roei and Shamsabadi, Ali Shahin and Shumailov, Ilia and Papernot, Nicolas",
        "title": "When the curious abandon honesty: Federated learning is not private"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "wu2023learning",
        "author": "Wu, Ruihan and Chen, Xiangyu and Guo, Chuan and Weinberger, Kilian Q",
        "title": "Learning to invert: Simple adaptive attacks for gradient inversion in federated learning"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "li2022you",
        "author": "Li, Haoran and Song, Yangqiu and Fan, Lixin",
        "title": "You don't know my favorite color: Preventing dialogue representations from revealing speakers' private personas"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "das2024security",
        "author": "Das, Badhan Chandra and Amini, M Hadi and Wu, Yanzhao",
        "title": "Security and privacy challenges of large language models: A survey"
      },
      {
        "key": "yan2024protecting",
        "author": "Yan, Biwei and Li, Kun and Xu, Minghui and Dong, Yueyan and Zhang, Yue and Ren, Zhaochun and Cheng, Xiuzhen",
        "title": "On protecting the data privacy of large language models (llms): A survey"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "zhu2019deep",
        "author": "Zhu, Ligeng and Liu, Zhijian and Han, Song",
        "title": "Deep leakage from gradients"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "gupta2022recovering",
        "author": "Gupta, Samyak and Huang, Yangsibo and Zhong, Zexuan and Gao, Tianyu and Li, Kai and Chen, Danqi",
        "title": "Recovering private text in federated learning of language models"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "balunovic2022lamp",
        "author": "Balunovic, Mislav and Dimitrov, Dimitar and Jovanovi{\\'c}, Nikola and Vechev, Martin",
        "title": "Lamp: Extracting text from gradients with language model priors"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "li2022you",
        "author": "Li, Haoran and Song, Yangqiu and Fan, Lixin",
        "title": "You don't know my favorite color: Preventing dialogue representations from revealing speakers' private personas"
      }
    ]
  },
  {
    "index": 23,
    "papers": [
      {
        "key": "carlini2021extracting",
        "author": "Carlini, Nicholas and Tramer, Florian and Wallace, Eric and Jagielski, Matthew and Herbert-Voss, Ariel and Lee, Katherine and Roberts, Adam and Brown, Tom and Song, Dawn and Erlingsson, Ulfar and others",
        "title": "Extracting training data from large language models"
      }
    ]
  },
  {
    "index": 24,
    "papers": [
      {
        "key": "huang2022large",
        "author": "Huang, Jie and Shao, Hanyin and Chang, Kevin Chen-Chuan",
        "title": "Are large pre-trained language models leaking your personal information?"
      },
      {
        "key": "nakka2024pii",
        "author": "Nakka, Krishna and Frikha, Ahmed and Mendes, Ricardo and Jiang, Xue and Zhou, Xuebing",
        "title": "PII-Compass: Guiding LLM training data extraction prompts towards the target PII via grounding"
      }
    ]
  },
  {
    "index": 25,
    "papers": [
      {
        "key": "SPT",
        "author": "Kim, Siwon and Yun, Sangdoo and Lee, Hwaran and Gubri, Martin and Yoon, Sungroh and Oh, Seong Joon",
        "title": "Propile: Probing privacy leakage in large language models"
      }
    ]
  },
  {
    "index": 26,
    "papers": [
      {
        "key": "song2020information",
        "author": "Song, Congzheng and Raghunathan, Ananth",
        "title": "Information leakage in embedding models"
      }
    ]
  },
  {
    "index": 27,
    "papers": [
      {
        "key": "li2023sentence",
        "author": "Li, Haoran and Xu, Mingshi and Song, Yangqiu",
        "title": "Sentence Embedding Leaks More Information than You Expect: Generative Embedding Inversion Attack to Recover the Whole Sentence"
      }
    ]
  },
  {
    "index": 28,
    "papers": [
      {
        "key": "morris2023text",
        "author": "Morris, John X and Kuleshov, Volodymyr and Shmatikov, Vitaly and Rush, Alexander M",
        "title": "Text embeddings reveal (almost) as much as text"
      }
    ]
  },
  {
    "index": 29,
    "papers": [
      {
        "key": "huang2024transferable",
        "author": "Huang, Yu-Hsiang  and\nTsai, Yuche  and\nHsiao, Hsiang  and\nLin, Hong-Yi  and\nLin, Shou-De",
        "title": "Transferable Embedding Inversion Attack: Uncovering Privacy Risks in Text Embeddings without Model Queries"
      }
    ]
  }
]