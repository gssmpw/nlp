@inproceedings{li2024ecomgpt,
  title={Ecomgpt: Instruction-tuning large language models with chain-of-task tasks for e-commerce},
  author={Li, Yangning and Ma, Shirong and Wang, Xiaobin and Huang, Shen and Jiang, Chengyue and Zheng, Hai-Tao and Xie, Pengjun and Huang, Fei and Jiang, Yong},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={17},
  pages={18582--18590},
  year={2024}
}

@article{wu2024chateda,
  title={Chateda: A large language model powered autonomous agent for eda},
  author={Wu, Haoyuan and He, Zhuolun and Zhang, Xinyun and Yao, Xufeng and Zheng, Su and Zheng, Haisheng and Yu, Bei},
  journal={IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  year={2024},
  publisher={IEEE}
}

@article{lu2024chameleon,
  title={Chameleon: Plug-and-play compositional reasoning with large language models},
  author={Lu, Pan and Peng, Baolin and Cheng, Hao and Galley, Michel and Chang, Kai-Wei and Wu, Ying Nian and Zhu, Song-Chun and Gao, Jianfeng},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{bahri2024explaining,
  title={Explaining neural scaling laws},
  author={Bahri, Yasaman and Dyer, Ethan and Kaplan, Jared and Lee, Jaehoon and Sharma, Utkarsh},
  journal={Proceedings of the National Academy of Sciences},
  volume={121},
  number={27},
  pages={e2311878121},
  year={2024},
  publisher={National Academy of Sciences}
}

@article{naveed2023comprehensive,
  title={A comprehensive overview of large language models},
  author={Naveed, Humza and Khan, Asad Ullah and Qiu, Shi and Saqib, Muhammad and Anwar, Saeed and Usman, Muhammad and Akhtar, Naveed and Barnes, Nick and Mian, Ajmal},
  journal={arXiv preprint arXiv:2307.06435},
  year={2023}
}

@article{raiaan2024review,
  title={A review on large Language Models: Architectures, applications, taxonomies, open issues and challenges},
  author={Raiaan, Mohaimenul Azam Khan and Mukta, Md Saddam Hossain and Fatema, Kaniz and Fahad, Nur Mohammad and Sakib, Sadman and Mim, Most Marufatul Jannat and Ahmad, Jubaer and Ali, Mohammed Eunus and Azam, Sami},
  journal={IEEE Access},
  year={2024},
  publisher={IEEE}
}

@article{liu2024deepseek,
  title={Deepseek-v3 technical report},
  author={Liu, Aixin and Feng, Bei and Xue, Bing and Wang, Bingxuan and Wu, Bochao and Lu, Chengda and Zhao, Chenggang and Deng, Chengqi and Zhang, Chenyu and Ruan, Chong and others},
  journal={arXiv preprint arXiv:2412.19437},
  year={2024}
}

@article{Lepikhin2020Gshard,
  title={Gshard: Scaling giant models with conditional computation and automatic sharding},
  author={Lepikhin, Dmitry and Lee, HyoukJoong and Xu, Yuanzhong and Chen, Dehao and Firat, Orhan and Huang, Yanping and Krikun, Maxim and Shazeer, Noam and Chen, Zhifeng},
  journal={arXiv preprint arXiv:2006.16668},
  year={2020}
}

@inproceedings{zheng2022alpa,
  title={Alpa: Automating inter-and $\{$Intra-Operator$\}$ parallelism for distributed deep learning},
  author={Zheng, Lianmin and Li, Zhuohan and Zhang, Hao and Zhuang, Yonghao and Chen, Zhifeng and Huang, Yanping and Wang, Yida and Xu, Yuanzhong and Zhuo, Danyang and Xing, Eric P and others},
  booktitle={16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)},
  pages={559--578},
  year={2022}
}

@inproceedings{narayanan2021efficient,
  title={Efficient large-scale language model training on gpu clusters using megatron-lm},
  author={Narayanan, Deepak and Shoeybi, Mohammad and Casper, Jared and LeGresley, Patrick and Patwary, Mostofa and Korthikanti, Vijay and Vainbrand, Dmitri and Kashinkunti, Prethvi and Bernauer, Julie and Catanzaro, Bryan and others},
  booktitle={Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--15},
  year={2021}
}

@article{huang2019gpipe,
  title={Gpipe: Efficient training of giant neural networks using pipeline parallelism},
  author={Huang, Yanping and Cheng, Youlong and Bapna, Ankur and Firat, Orhan and Chen, Dehao and Chen, Mia and Lee, HyoukJoong and Ngiam, Jiquan and Le, Quoc V and Wu, Yonghui and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{narayanan2019pipedream,
  title={PipeDream: Generalized pipeline parallelism for DNN training},
  author={Narayanan, Deepak and Harlap, Aaron and Phanishayee, Amar and Seshadri, Vivek and Devanur, Nikhil R and Ganger, Gregory R and Gibbons, Phillip B and Zaharia, Matei},
  booktitle={Proceedings of the 27th ACM symposium on operating systems principles},
  pages={1--15},
  year={2019}
}

@inproceedings{narayanan2021memory,
  title={Memory-efficient pipeline-parallel dnn training},
  author={Narayanan, Deepak and Phanishayee, Amar and Shi, Kaiyu and Chen, Xie and Zaharia, Matei},
  booktitle={International Conference on Machine Learning},
  pages={7937--7947},
  year={2021},
  organization={PMLR}
}

@inproceedings{li2014scaling,
  title={Scaling distributed machine learning with the parameter server},
  author={Li, Mu and Andersen, David G and Park, Jun Woo and Smola, Alexander J and Ahmed, Amr and Josifovski, Vanja and Long, James and Shekita, Eugene J and Su, Bor-Yiing},
  booktitle={11th USENIX Symposium on operating systems design and implementation (OSDI 14)},
  pages={583--598},
  year={2014}
}

@inproceedings{luo2020prague,
  title={Prague: High-performance heterogeneity-aware asynchronous decentralized training},
  author={Luo, Qinyi and He, Jiaao and Zhuo, Youwei and Qian, Xuehai},
  booktitle={Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems},
  pages={401--416},
  year={2020}
}

@inproceedings{thorpe2023bamboo,
  title={Bamboo: Making preemptible instances resilient for affordable training of large $\{$DNNs$\}$},
  author={Thorpe, John and Zhao, Pengzhan and Eyolfson, Jonathan and Qiao, Yifan and Jia, Zhihao and Zhang, Minjia and Netravali, Ravi and Xu, Guoqing Harry},
  booktitle={20th USENIX Symposium on Networked Systems Design and Implementation (NSDI 23)},
  pages={497--513},
  year={2023}
}

@inproceedings{jang2023oobleck,
  title={Oobleck: Resilient distributed training of large models using pipeline templates},
  author={Jang, Insu and Yang, Zhenning and Zhang, Zhen and Jin, Xin and Chowdhury, Mosharaf},
  booktitle={Proceedings of the 29th Symposium on Operating Systems Principles},
  pages={382--395},
  year={2023}
}

@inproceedings{duan2024parcae,
  title={Parcae: Proactive,$\{$Liveput-Optimized$\}$$\{$DNN$\}$ Training on Preemptible Instances},
  author={Duan, Jiangfei and Song, Ziang and Miao, Xupeng and Xi, Xiaoli and Lin, Dahua and Xu, Harry and Zhang, Minjia and Jia, Zhihao},
  booktitle={21st USENIX Symposium on Networked Systems Design and Implementation (NSDI 24)},
  pages={1121--1139},
  year={2024}
}

@InProceedings{lu2024position,
  title={Position: Exploring the Robustness of Pipeline-Parallelism-Based Decentralized Training},
  author={Lu, Lin and Dai, Chenxi and Tao, Wangcheng and Yuan, Binhang and Sun, Yanan and Zhou, Pan},
  booktitle={Proceedings of the 41st International Conference on Machine Learning},
  pages={32978--32989},
  year={2024},
  volume={235},
  series={Proceedings of Machine Learning Research},
}

@article{das2024security,
  title={Security and privacy challenges of large language models: A survey},
  author={Das, Badhan Chandra and Amini, M Hadi and Wu, Yanzhao},
  journal={ACM Computing Surveys},
  year={2024},
  publisher={ACM New York, NY}
}

@article{yan2024protecting,
  title={On protecting the data privacy of large language models (llms): A survey},
  author={Yan, Biwei and Li, Kun and Xu, Minghui and Dong, Yueyan and Zhang, Yue and Ren, Zhaochun and Cheng, Xiuzhen},
  journal={arXiv preprint arXiv:2403.05156},
  year={2024}
}

@article{zhu2019deep,
  title={Deep leakage from gradients},
  author={Zhu, Ligeng and Liu, Zhijian and Han, Song},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{zhao2020idlg,
  title={idlg: Improved deep leakage from gradients},
  author={Zhao, Bo and Mopuri, Konda Reddy and Bilen, Hakan},
  journal={arXiv preprint arXiv:2001.02610},
  year={2020}
}

@inproceedings{li2023sentence,
  title={Sentence Embedding Leaks More Information than You Expect: Generative Embedding Inversion Attack to Recover the Whole Sentence},
  author={Li, Haoran and Xu, Mingshi and Song, Yangqiu},
  booktitle={Findings of the Association for Computational Linguistics: ACL 2023},
  pages={14022--14040},
  year={2023}
}

@article{williams1989learning,
  title={A learning algorithm for continually running fully recurrent neural networks},
  author={Williams, Ronald J and Zipser, David},
  journal={Neural computation},
  volume={1},
  number={2},
  pages={270--280},
  year={1989},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~â€¦}
}

@inproceedings{chen2024text,
  title={Text embedding inversion security for multilingual language models},
  author={Chen, Yiyi and Lent, Heather and Bjerva, Johannes},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={7808--7827},
  year={2024}
}

@article{balunovic2022lamp,
  title={Lamp: Extracting text from gradients with language model priors},
  author={Balunovic, Mislav and Dimitrov, Dimitar and Jovanovi{\'c}, Nikola and Vechev, Martin},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={7641--7654},
  year={2022}
}

@article{gupta2022recovering,
  title={Recovering private text in federated learning of language models},
  author={Gupta, Samyak and Huang, Yangsibo and Zhong, Zexuan and Gao, Tianyu and Li, Kai and Chen, Danqi},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={8130--8143},
  year={2022}
}

@article{fowl2022decepticons,
  title={Decepticons: Corrupted transformers breach privacy in federated learning for language models},
  author={Fowl, Liam and Geiping, Jonas and Reich, Steven and Wen, Yuxin and Czaja, Wojtek and Goldblum, Micah and Goldstein, Tom},
  journal={arXiv preprint arXiv:2201.12675},
  year={2022}
}

@inproceedings{boenisch2023curious,
  title={When the curious abandon honesty: Federated learning is not private},
  author={Boenisch, Franziska and Dziedzic, Adam and Schuster, Roei and Shamsabadi, Ali Shahin and Shumailov, Ilia and Papernot, Nicolas},
  booktitle={2023 IEEE 8th European Symposium on Security and Privacy (EuroS\&P)},
  pages={175--199},
  year={2023},
  organization={IEEE}
}

@article{li2022you,
  title={You don't know my favorite color: Preventing dialogue representations from revealing speakers' private personas},
  author={Li, Haoran and Song, Yangqiu and Fan, Lixin},
  journal={arXiv preprint arXiv:2205.10228},
  year={2022}
}

@inproceedings{wu2023learning,
  title={Learning to invert: Simple adaptive attacks for gradient inversion in federated learning},
  author={Wu, Ruihan and Chen, Xiangyu and Guo, Chuan and Weinberger, Kilian Q},
  booktitle={Uncertainty in Artificial Intelligence},
  pages={2293--2303},
  year={2023},
  organization={PMLR}
}

@inproceedings{carlini2021extracting,
  title={Extracting training data from large language models},
  author={Carlini, Nicholas and Tramer, Florian and Wallace, Eric and Jagielski, Matthew and Herbert-Voss, Ariel and Lee, Katherine and Roberts, Adam and Brown, Tom and Song, Dawn and Erlingsson, Ulfar and others},
  booktitle={30th USENIX Security Symposium (USENIX Security 21)},
  pages={2633--2650},
  year={2021}
}

@article{huang2022large,
  title={Are large pre-trained language models leaking your personal information?},
  author={Huang, Jie and Shao, Hanyin and Chang, Kevin Chen-Chuan},
  journal={arXiv preprint arXiv:2205.12628},
  year={2022}
}

@inproceedings{nakka2024pii,
  title={PII-Compass: Guiding LLM training data extraction prompts towards the target PII via grounding},
  author={Nakka, Krishna and Frikha, Ahmed and Mendes, Ricardo and Jiang, Xue and Zhou, Xuebing},
  booktitle={Proceedings of the Fifth Workshop on Privacy in Natural Language Processing},
  pages={63--73},
  year={2024}
}

@inproceedings{song2020information,
  title={Information leakage in embedding models},
  author={Song, Congzheng and Raghunathan, Ananth},
  booktitle={Proceedings of the 2020 ACM SIGSAC conference on computer and communications security},
  pages={377--390},
  year={2020}
}

@article{morris2023text,
  title={Text embeddings reveal (almost) as much as text},
  author={Morris, John X and Kuleshov, Volodymyr and Shmatikov, Vitaly and Rush, Alexander M},
  journal={arXiv preprint arXiv:2310.06816},
  year={2023}
}

@inproceedings{huang2024transferable,
    title = "Transferable Embedding Inversion Attack: Uncovering Privacy Risks in Text Embeddings without Model Queries",
    author="Huang, Yu-Hsiang  and
      Tsai, Yuche  and
      Hsiao, Hsiang  and
      Lin, Hong-Yi  and
      Lin, Shou-De",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    year={2024},
    pages = "4193--4205",
}

@article{yuan2022decentralized,
  title={Decentralized training of foundation models in heterogeneous environments},
  author={Yuan, Binhang and He, Yongjun and Davis, Jared and Zhang, Tianyi and Dao, Tri and Chen, Beidi and Liang, Percy S and Re, Christopher and Zhang, Ce},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={25464--25477},
  year={2022}
}

@inproceedings{ryabinin2023swarm,
  title={Swarm parallelism: Training large models can be surprisingly communication-efficient},
  author={Ryabinin, Max and Dettmers, Tim and Diskin, Michael and Borzunov, Alexander},
  booktitle={International Conference on Machine Learning},
  pages={29416--29440},
  year={2023},
  organization={PMLR}
}

@inproceedings{wang2023cocktailsgd,
  title={Cocktailsgd: Fine-tuning foundation models over 500mbps networks},
  author={Wang, Jue and Lu, Yucheng and Yuan, Binhang and Chen, Beidi and Liang, Percy and De Sa, Christopher and Re, Christopher and Zhang, Ce},
  booktitle={International Conference on Machine Learning},
  pages={36058--36076},
  year={2023},
  organization={PMLR}
}

@article{gandhi2024improving,
  title={Improving training time and GPU utilization in geo-distributed language model training},
  author={Gandhi, Rohan and Tandon, Karan and Bhattacherjee, Debopam and Padmanabhan, Venkata N and others},
  journal={arXiv preprint arXiv:2411.14458},
  year={2024}
}

@article{tang2024fusionllm,
  title={Fusionllm: A decentralized llm training system on geo-distributed gpus with adaptive compression},
  author={Tang, Zhenheng and Kang, Xueze and Yin, Yiming and Pan, Xinglin and Wang, Yuxin and He, Xin and Wang, Qiang and Zeng, Rongfei and Zhao, Kaiyong and Shi, Shaohuai and others},
  journal={arXiv preprint arXiv:2410.12707},
  year={2024}
}

@article{tang2023fusionai,
  title={Fusionai: Decentralized training and deploying llms with massive consumer-level gpus},
  author={Tang, Zhenheng and Wang, Yuxin and He, Xin and Zhang, Longteng and Pan, Xinglin and Wang, Qiang and Zeng, Rongfei and Zhao, Kaiyong and Shi, Shaohuai and He, Bingsheng and others},
  journal={arXiv preprint arXiv:2309.01172},
  year={2023}
}

@article{gpt2,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{gpt3,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{bloom,
  title={Bloom: A 176b-parameter open-access multilingual language model},
  author={Le Scao, Teven and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ili{\'c}, Suzana and Hesslow, Daniel and Castagn{\'e}, Roman and Luccioni, Alexandra Sasha and Yvon, Fran{\c{c}}ois and Gall{\'e}, Matthias and others},
  year={2023}
}

@article{gemma2,
  title={Gemma 2: Improving open language models at a practical size},
  author={Team, Gemma and Riviere, Morgane and Pathak, Shreya and Sessa, Pier Giuseppe and Hardin, Cassidy and Bhupatiraju, Surya and Hussenot, L{\'e}onard and Mesnard, Thomas and Shahriari, Bobak and Ram{\'e}, Alexandre and others},
  journal={arXiv preprint arXiv:2408.00118},
  year={2024}
}

@article{chen2023extending,
  title={Extending context window of large language models via positional interpolation},
  author={Chen, Shouyuan and Wong, Sherman and Chen, Liangjian and Tian, Yuandong},
  journal={arXiv preprint arXiv:2306.15595},
  year={2023}
}

@article{llama3,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@misc{wikitext,
      title={Pointer Sentinel Mixture Models},
      author={Stephen Merity and Caiming Xiong and James Bradbury and Richard Socher},
      year={2016},
      eprint={1609.07843},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{openwebtext,
    title={OpenWebText Corpus},
    author={Gokaslan, Aaron and Cohen, Vanya and Pavlick, Ellie and Tellex, Stefanie},
    howpublished={\url{http://Skylion007.github.io/OpenWebTextCorpus}},
    year={2019}
}

@article{pile,
  title={The {P}ile: An 800{GB} dataset of diverse text for language modeling},
  author={Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and others},
  journal={arXiv preprint arXiv:2101.00027},
  year={2020}
}

@inproceedings{rouge,
  title={Rouge: A package for automatic evaluation of summaries},
  author={Lin, Chin-Yew},
  booktitle={Text summarization branches out},
  pages={74--81},
  year={2004}
}

@article{perplexity,
  title={Perplexityâ€”a measure of the difficulty of speech recognition tasks},
  author={Jelinek, Fred and Mercer, Robert L and Bahl, Lalit R and Baker, James K},
  journal={The Journal of the Acoustical Society of America},
  volume={62},
  number={S1},
  pages={S63--S63},
  year={1977},
  publisher={Acoustical Society of America}
}

@inproceedings{bleu,
  title={Bleu: a method for automatic evaluation of machine translation},
  author={Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  booktitle={Proceedings of the 40th annual meeting of the Association for Computational Linguistics},
  pages={311--318},
  year={2002}
}

@article{minilmv2,
  title={Minilmv2: Multi-head self-attention relation distillation for compressing pretrained transformers},
  author={Wang, Wenhui and Bao, Hangbo and Huang, Shaohan and Dong, Li and Wei, Furu},
  journal={arXiv preprint arXiv:2012.15828},
  year={2020}
}

@inproceedings{true-prefix,
  title={Extracting training data from large language models},
  author={Carlini, Nicholas and Tramer, Florian and Wallace, Eric and Jagielski, Matthew and Herbert-Voss, Ariel and Lee, Katherine and Roberts, Adam and Brown, Tom and Song, Dawn and Erlingsson, Ulfar and others},
  booktitle={30th USENIX Security Symposium (USENIX Security 21)},
  pages={2633--2650},
  year={2021}
}

@article{SPT,
  title={Propile: Probing privacy leakage in large language models},
  author={Kim, Siwon and Yun, Sangdoo and Lee, Hwaran and Gubri, Martin and Yoon, Sungroh and Oh, Seong Joon},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{qwen2.5,
  title={Qwen2. 5 Technical Report},
  author={Yang, An and Yang, Baosong and Zhang, Beichen and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and Wei, Haoran and others},
  journal={arXiv preprint arXiv:2412.15115},
  year={2024}
}

@article{mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}

@inproceedings{borzunov2022training,
  title={Training transformers together},
  author={Borzunov, Alexander and Ryabinin, Max and Dettmers, Tim and Lhoest, Quentin and Saulnier, Lucile and Diskin, Michael and Jernite, Yacine and Wolf, Thomas},
  booktitle={NeurIPS 2021 Competitions and Demonstrations Track},
  pages={335--342},
  year={2022},
  organization={PMLR}
}

@inproceedings{pasquini2021unleashing,
  title={Unleashing the tiger: Inference attacks on split learning},
  author={Pasquini, Dario and Ateniese, Giuseppe and Bernaschi, Massimo},
  booktitle={Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security},
  pages={2113--2129},
  year={2021}
}

@article{bethany2024large,
  title={Large language model lateral spear phishing: A comparative study in large-scale organizational settings},
  author={Bethany, Mazal and Galiopoulos, Athanasios and Bethany, Emet and Karkevandi, Mohammad Bahrami and Vishwamitra, Nishant and Najafirad, Peyman},
  journal={arXiv preprint arXiv:2401.09727},
  year={2024}
}

@article{qi2024spearbot,
  title={SpearBot: Leveraging Large Language Models in a Generative-Critique Framework for Spear-Phishing Email Generation},
  author={Qi, Qinglin and Luo, Yun and Xu, Yijia and Guo, Wenbo and Fang, Yong},
  journal={arXiv preprint arXiv:2412.11109},
  year={2024}
}

@article{loshchilov2017decoupled,
  title={Decoupled weight decay regularization},
  author={Loshchilov, I},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}