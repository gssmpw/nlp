\section{Related Work}
\subsection{Decentralized Training Safety}

\citet{yuan2022decentralized} initially explores decentralized training for LLMs. Several studies then examine decentralized training in slow networks~\cite{ryabinin2023swarm, wang2023cocktailsgd} and explore the development of geo-distributed training systems tailored for LLMs~\cite{gandhi2024improving, tang2024fusionllm}. While safety concerns in decentralized training have been identified in previous works~\cite{tang2023fusionai, borzunov2022training}, most existing research focuses mainly on ensuring seamless pipeline operations on preemptible devices, employing techniques such as model backup and redundant computation~\cite{thorpe2023bamboo, jang2023oobleck}. \citet{lu2024position} comprehensively evaluate the potential threats in decentralized training. However, the proposed \textit{forward attack} can be easily mitigated by detection methods, making it impractical in real-world scenarios.


% Decentralized training has emerged as the mainstream approach for training large models. \citet{yuan2022decentralized} investigates model parallelism-based training of large models in heterogeneous environments. Some research examines distributed training in the presence of slow network conditions~\cite{ryabinin2023swarm,wang2023cocktailsgd}. Other efforts focus on pipeline training on preemptible devices, employing techniques such as model backup and redundant computation~\cite{thorpe2023bamboo,jang2023oobleck}. Several works explore the development of geo-distributed training systems tailored for large language models~\cite{gandhi2024improving,tang2024fusionllm}. Furthermore, \citet{tang2023fusionai} utilizes consumer-grade GPUs for training large models.


\subsection{Data Leakage from Transmitted Values}

\noindent{\textbf{Data leakage from gradients.}}
In the context of FL, researchers such as \citet{zhu2019deep} have explored deep gradient leakage attacks on both visual and language models. 
\citet{balunovic2022lamp} uses auxiliary language models to model prior probabilities, reducing the loss through alternating continuous and discrete optimization. \citet{gupta2022recovering} first recovers a set of words from gradients, and then reconstructs the sentence from this set of words using beam search. \citet{fowl2022decepticons} and \citet{boenisch2023curious} propose a powerful threat model in which the server is malicious and can manipulate model weights, easily reconstructing the data.
\citet{wu2023learning} proposes a simple adaptive attack method that can bypass various defense mechanisms, including differential privacy and gradient compression, and successfully reconstruct the original text.
% \citet{li2022you} leverages the hidden states of conversational models to conduct attribute inference attacks. Several studies assume that attackers can access the language model's output tokens and logits, which they exploit to induce the model to inadvertently leak private data.

% Large language models perform well across a variety of tasks but also expose increasing privacy risks~\cite{das2024security,yan2024protecting}. In the context of federated learning, researchers such as \citet{zhu2019deep} have studied deep gradient leakage attacks on both visual and language models, while \citet{gupta2022recovering} and \citet{balunovic2022lamp} specifically investigate deep gradient leakage attacks in language models. \citet{li2022you} utilizes the hidden states of conversational models to conduct attribute inference attacks. Several studies assume that attackers can access the language model and obtain output tokens and logits, which they use to induce the model to leak private data. Research by \citet{carlini2021extracting} shows that large models retain training data and leak private information. Additionally, some works generate malicious prompts to induce models to output private data\cite{huang2022large,nakka2024pii}. In addition, \citet{SPT} enhances data leakage by training a set of soft prompt tokens and adding them before the prompt template.

\noindent{\textbf{Data leakage from embeddings.}}
Another line of research focuses on embedding inversion attacks, where the attacker aims to reconstruct text from embedding representations. \citet{song2020information} reconstructs 50\%-70\% of the input words from embedding models. However, word-level information alone is insufficient to fully reconstruct privacy. \citet{li2023sentence} proposes a generative embedding inversion attack that reconstructs sentences similar to the original input from embeddings. \citet{morris2023text} utilizes an iterative correction approach to reconstruct text information. \citet{huang2024transferable} investigates a black-box attack scenario, reducing the discrepancy between the surrogate model and the victim model through adversarial training. These studies assume that the victim model is fully trained and static, allowing the attacker to access the input sentence embeddings from the victim model, build a shadow dataset, and then train an attack model to reconstruct the original text. However, in decentralized training settings, the malicious stage only has access to a portion of the model, and thus cannot directly access the victim model.

\input{EditedByLin/Preliminaries_Lin}