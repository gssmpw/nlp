\section{Related Work}
\subsection{Decentralized Training Safety}

____ initially explores decentralized training for LLMs. Several studies then examine decentralized training in slow networks____ and explore the development of geo-distributed training systems tailored for LLMs____. While safety concerns in decentralized training have been identified in previous works____, most existing research focuses mainly on ensuring seamless pipeline operations on preemptible devices, employing techniques such as model backup and redundant computation____. ____ comprehensively evaluate the potential threats in decentralized training. However, the proposed \textit{forward attack} can be easily mitigated by detection methods, making it impractical in real-world scenarios.


% Decentralized training has emerged as the mainstream approach for training large models. ____ investigates model parallelism-based training of large models in heterogeneous environments. Some research examines distributed training in the presence of slow network conditions____. Other efforts focus on pipeline training on preemptible devices, employing techniques such as model backup and redundant computation____. Several works explore the development of geo-distributed training systems tailored for large language models____. Furthermore, ____ utilizes consumer-grade GPUs for training large models.


\subsection{Data Leakage from Transmitted Values}

\noindent{\textbf{Data leakage from gradients.}}
In the context of FL, researchers such as ____ have explored deep gradient leakage attacks on both visual and language models. 
____ uses auxiliary language models to model prior probabilities, reducing the loss through alternating continuous and discrete optimization. ____ first recovers a set of words from gradients, and then reconstructs the sentence from this set of words using beam search. ____ and ____ propose a powerful threat model in which the server is malicious and can manipulate model weights, easily reconstructing the data.
____ proposes a simple adaptive attack method that can bypass various defense mechanisms, including differential privacy and gradient compression, and successfully reconstruct the original text.
% ____ leverages the hidden states of conversational models to conduct attribute inference attacks. Several studies assume that attackers can access the language model's output tokens and logits, which they exploit to induce the model to inadvertently leak private data.

% Large language models perform well across a variety of tasks but also expose increasing privacy risks____. In the context of federated learning, researchers such as ____ have studied deep gradient leakage attacks on both visual and language models, while ____ and ____ specifically investigate deep gradient leakage attacks in language models. ____ utilizes the hidden states of conversational models to conduct attribute inference attacks. Several studies assume that attackers can access the language model and obtain output tokens and logits, which they use to induce the model to leak private data. Research by ____ shows that large models retain training data and leak private information. Additionally, some works generate malicious prompts to induce models to output private data____. In addition, ____ enhances data leakage by training a set of soft prompt tokens and adding them before the prompt template.

\noindent{\textbf{Data leakage from embeddings.}}
Another line of research focuses on embedding inversion attacks, where the attacker aims to reconstruct text from embedding representations. ____ reconstructs 50\%-70\% of the input words from embedding models. However, word-level information alone is insufficient to fully reconstruct privacy. ____ proposes a generative embedding inversion attack that reconstructs sentences similar to the original input from embeddings. ____ utilizes an iterative correction approach to reconstruct text information. ____ investigates a black-box attack scenario, reducing the discrepancy between the surrogate model and the victim model through adversarial training. These studies assume that the victim model is fully trained and static, allowing the attacker to access the input sentence embeddings from the victim model, build a shadow dataset, and then train an attack model to reconstruct the original text. However, in decentralized training settings, the malicious stage only has access to a portion of the model, and thus cannot directly access the victim model.

\input{EditedByLin/Preliminaries_Lin}