\documentclass[a4paper,UKenglish,cleveref, autoref, thm-restate]{lipics-v2021}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

\bibliographystyle{plainurl}% the mandatory bibstyle

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{thmtools} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}



\newcounter{mycounter}
\setcounter{mycounter}{0}
\newtheorem{myexample}[mycounter]{Example}

\title{Polynomial Time Learning-Augmented Algorithms for NP-hard Permutation Problems}


\author{Evripidis Bampis}{Sorbonne Universit\'e, CNRS, LIP6, F-75005 Paris, France}{evripidis.bampis@lip6.fr}{}{}

\author{Bruno Escoffier}{Sorbonne Universit\'e, CNRS, LIP6, F-75005 Paris, France}{bruno.escoffier@lip6.fr}{}{}

\author{Dimitris Fotakis}{National Technical University of Athens, Greece\\ Archimedes Research Unit, Athena RC, Greece}{fotakis@cs.ntua.gr}{}{}

\author{Panagiotis Patsilinakos}{Universit\'e Paris-Dauphine, Universit\'e PSL, CNRS, LAMSADE, 75016, Paris, France}{patsilinakos@gmail.com}{}{}

\author{Michalis Xefteris}{Sorbonne Universit\'e, CNRS, LIP6, F-75005 Paris, France}{mxefteris@hotmail.com}{}{}



\authorrunning{E. Bampis, B. Escoffier, D. Fotakis, P. Patsilinakos and M. Xefteris} %TODO mandatory. First: Use abbreviated first/middle names. Second (only in severe cases): Use first author plus 'et al.'

\Copyright{Evripidis Bampis, Bruno Escoffier, Dimitris Fotakis, Panagiotis Patsilinakos, Michalis Xefteris} %TODO mandatory, please use full first names. LIPIcs license is "CC-BY";  http://creativecommons.org/licenses/by/3.0/

\ccsdesc[500]{Theory of computation~Design and analysis of algorithms}
%\ccsdesc[100]{\textcolor{red}{Replace ccsdesc macro with valid one}} %TODO mandatory: Please choose ACM 2012 classifications from https://dl.acm.org/ccs/ccs_flat.cfm 

\keywords{Learning-Augmented Algorithms, Algorithms with predictions, Permutation problems}
%TODO mandatory; please add comma-separated list of keywords

\category{} %optional, e.g. invited paper

\relatedversion{} %optional, e.g. full version hosted on arXiv, HAL, or other respository/website
%\relatedversiondetails[linktext={opt. text shown instead of the URL}, cite=DBLP:books/mk/GrayR93]{Classification (e.g. Full Version, Extended Version, Previous Version}{URL to related version} %linktext and cite are optional

%\supplement{}%optional, e.g. related research data, source code, ... hosted on a repository like zenodo, figshare, GitHub, ...
%\supplementdetails[linktext={opt. text shown instead of the URL}, cite=DBLP:books/mk/GrayR93, subcategory={Description, Subcategory}, swhid={Software Heritage Identifier}]{General Classification (e.g. Software, Dataset, Model, ...)}{URL to related version} %linktext, cite, and subcategory are optional

\funding{}%optional, to capture a funding statement, which applies to all authors. Please enter author specific funding statements as fifth argument of the \author macro.

\acknowledgements{}

\nolinenumbers %uncomment to disable line numbering




%Editor-only macros:: begin (do not touch as author)%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\EventEditors{John Q. Open and Joan R. Access}
%\EventNoEds{2}
%\EventLongTitle{42nd Conference on Very Important Topics (CVIT 2016)}
%\EventShortTitle{CVIT 2016}
%\EventAcronym{CVIT}
%\EventYear{2016}
%\EventDate{December 24--27, 2016}
%\EventLocation{Little Whinging, United Kingdom}
%\EventLogo{}
%\SeriesVolume{42}
%\ArticleNo{23}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\begin{document}

\maketitle

\begin{abstract}
We consider a learning-augmented framework for NP-hard permutation problems. The algorithm has access to predictions telling, given a pair $u,v$ of elements, whether $u$ is before $v$ or not in an optimal solution. Building on the work of Braverman and Mossel (SODA 2008), we show that for a class of optimization problems  including scheduling, network design and other graph permutation problems, these predictions allow to solve them in polynomial time with high probability, provided that predictions are true with probability at least $1/2+\epsilon$. Moreover, this can be achieved with a parsimonious access to the predictions. 
\end{abstract}

\section{Introduction}

In recent years, advancements in Machine Learning (ML) have significantly influenced progress in solving optimization problems across a wide range of fields. By leveraging historical data, ML predictors are utilized every day to tackle numerous challenges. These developments have motivated researchers in algorithms to incorporate ML predictions into algorithm design for optimization problems. This has given rise to the vastly growing field of \emph{learning-augmented algorithms}, also known as \emph{algorithms with predictions}. In this framework, it is assumed that predictions about a problem's input are provided by a black-box ML model. The objective is to use these predictions to develop algorithms that outperform existing ones when the predictions are sufficiently accurate.

The idea of learning-augmented algorithms was initially introduced by Mahdian,
Nazerzadeh, and Saberi, who applied it to the problem of allocating online advertisement space for budget-constrained advertisers~\cite{la_2}. Later, Lykouris and Vassilvitskii formalized the framework, studying the online caching problem using predictions~\cite{lykouris}. The main emphasis in the field of algorithms with predictions has been on online optimization, as predicting the future of a partially unknown input instance is a natural approach. However, in the past few years, the field has expanded into various other areas. An almost complete list of papers in the field can be found in~\cite{website}.

More relevant to this work, algorithms with predictions have been used to address NP-hard optimization problems and overcome their computational challenges. The first learning-augmented algorithms applied to NP-hard problems were focused on clustering, as seen in~\cite{gamlath, clustering, clustering_2}. Moreover, several papers have studied \textsc{MaxCut} with predictions, including~\cite{approximations_with_predictions, cohenaddad, constraint, maxcut_4}. Cohen-Addad et al. investigated the approximability of \textsc{MaxCut} with predictions in two models~\cite{cohenaddad}. In the first model, similar to the one used in this work, they assumed predictions for each vertex (on its position in an optimal cut) that are correct with probability $1/2+\epsilon$, and presented a polynomial-time $(0.878 + \Tilde{\Omega}(\epsilon^4))$-approximation algorithm. In the second model, they receive a correct prediction for each vertex with probability $\epsilon$ (and no information otherwise) and designed a $(0.858 + \Omega(\epsilon))$-approximation algorithm. Ghoshal et al. also studied \textsc{MaxCut} and \textsc{Max2-Lin} in both models~\cite{constraint}. Furthermore, in~\cite{braverman} they studied Maximum Independent Set within the framework of learning-augmented algorithms, adopting the aforementioned first model.
Finally, in~\cite{antoniadis} they studied approximation algorithms with predictions for several NP-hard optimization problems within a prediction model different from the one used in this work. 

In this paper, we design learning-augmented algorithms for NP-hard optimization problems. Our approach does not use all available predictions for the problem at hand but instead utilizes the predictor selectively. This aligns with the concept of parsimonious algorithms, introduced in~\cite{kumar}, which aim to limit the number of predictions used, assuming that obtaining additional predictions can be computationally expensive. Here, we consider problems whose feasible solutions can be represented as permutations. There are $n$ input elements for an optimization problem denoted by $a_1, \dots, a_n$. A permutation (ordering) $\sigma$ corresponds to the solution $(a_{\sigma(1)},\dots,a_{\sigma(n)})$.

Regarding the prediction model, we adopt the following probabilistic framework. For each pair $i, j$ we can get a prediction query $q(a_i, a_j)$ that denotes whether $a_i$ precedes $a_j$ or not in a fixed optimal solution (permutation). Each prediction is independently correct with probability at least $1/2+\epsilon$, for $\epsilon>0$. An algorithm has access to $n \choose 2$ predictions. A formal description of the model is given in Section~\ref{sec:prelim}. 

In this work, we use these prediction queries to solve NP-hard optimization problems exactly with high probability. We design a novel framework which is capable of handling permutation optimization problems that exhibit one of two key properties: the \emph{decomposition} property and the \emph{$c$-locality} property in their objective function (see Section~\ref{sec:dec} for formal definitions).

The decomposition property states that solving a subproblem $\mathcal{I}(i,j)$ (between positions $i$ and $j$ in the permutation) of the optimization problem at hand optimally depends only on the set of elements in positions in $[i,j]$, the permutation $\sigma(i,j)$ of these elements in $[i,j]$, and the set of elements to the left of $i$ and to the right of $j$, but not on their order. On the other hand, the $c$-locality property states that the cost function of the problem depends only locally (with respect to the permutation) on pairs of distinct elements.

More specifically, we adjust and extend the approach of Braverman and Mossel~\cite{braverman2, braverman1} for the problem of sorting from noisy information and give the following theorem for a family of optimization problems (see Section~\ref{sec:proof} for its proof).

\begin{restatable}{theorem}{MainTheorem}
\label{theo:main}
If the objective function of a permutation optimization problem $P$ either exhibits the decomposition property or is $c$-local, then $P$ can be solved exactly with high probability in polynomial time, using $O(n \log n)$ prediction queries .
\end{restatable}

Therefore, if a permutation optimization problem is either decomposable or $c$-local, it can be solved with high probability in polynomial time within our prediction-based framework. To illustrate these properties, we examine several example problems: Maximum Acyclic Subgraph, Minimum Linear Arrangement, a scheduling problem as examples of decomposable problems, the Traveling Salesperson Problem (TSP) and and social welfare maximization in keyword auctions with externalities and window size as representatives of $c$-local problems. All these are well-known NP-hard problems and cannot be solved exactly in polynomial time without predictions unless $P=NP$. Moreover, the framework can naturally be extended to address a variety of other NP-hard problems with similar structural properties.

Another important aspect of our framework is that it does not query all possible pairs but instead makes only $O(n \log n)$ queries, making it parsimonious with respect to the number of predictions used.

The proof of Theorem~\ref{theo:main} demonstrates that, for the permutation problems under consideration, knowing an approximation of each $\sigma^*(i)$ (in an optimal solution $\sigma^*$) within an additive $O(\log n)$ bound is sufficient to solve the problem in polynomial time. In Section~\ref{sec:hardness}, we first show that this $O(\log n)$ approximation is not always sufficient for polynomial time solvability. Finally, we prove that the $O(\log n)$ bound is tight, as there are decomposable and $c$-local problems where an additive approximation of $f(n)\log n$ is not enough to solve these problems in polynomial time, for any unbounded function $f$.



\section{Background and Overview}
\label{sec:prelim}

\subsection{Definitions}

Formally, we consider the following probabilistic prediction model, which is inspired by the noisy query model studied in~\cite{braverman2}.

\begin{definition} \label{def:pred}
Let $A=\{a_1,\dots,a_n\}$ and $\sigma^*$ be a permutation from $[1,n]$ to $[1,n]$. For each pair $(a_\ell,a_t)$ in $A\choose 2$ the result of a prediction query for $(a_\ell,a_t)$, with respect to $\sigma^*$, is $q(a_\ell,a_t) \in \{-1,1\}$ where  $q(a_\ell,a_t) = -q(a_t,a_\ell)$. We assume that:
\begin{itemize}
    \item for each $1\leq i<j\leq n$ the probability that
$q(a_{\sigma^*(i)},a_{\sigma^*(j)}) = 1$ is at least $ \frac{1}{2} + \epsilon$, $0<\epsilon<1/2$,
    \item the queries 
$
\left\{ q(a_\ell,a_t) : 1 \leq \ell < t \leq n \right\}
$
are independent conditioned on $\sigma^*$. 
\end{itemize} 

\end{definition}
In this definition, the query $q(a_\ell,a_t)$ asks whether $a_\ell$ precedes $a_t$ in $(a_{\sigma^*(1)},\dots,a_{\sigma^*(n)})$ or not. The first item states that the prediction is correct with probability at least $1/2+\epsilon$.


Given the prediction queries, we are interested in finding a permutation that maximizes the number of agreements with the queries. Formally:

\begin{definition}
Given $n \choose 2$ prediction queries $q(a_\ell,a_t)$, the score $s_q(\pi)$ 
of a permutation $\pi:[1,n]\rightarrow [1,n]$ is given by
\begin{equation} 
 \label{eq:score}
    s_q(\pi) = \sum_{i< j} q(a_{\pi(i)},a_{\pi(j)}).
\end{equation}
We say that a permutation $\pi^*$ is s-optimal if $\pi^*$ 
is a maximizer of~(\ref{eq:score}) among all permutations. 
\end{definition}

The {\em Noisy Sorting Without Resampling (NSWR)} 
problem, defined in~\cite{braverman2}, is the problem of finding 
an $s$-optimal permutation $\pi^*$ with respect to a (hidden) permutation $\sigma^*$ assuming that $q$ satisfies Definition~\ref{def:pred} with $p=1/2+\epsilon, \epsilon>0$. 

%When each prediction has probability to be correct exactly equal to $p$, then the s-optimal solution to the SNSA problem is identical with the maximum likelihood order that is consistent with $q$.

As mentioned in the introduction, we consider in this work permutation problems, i.e.,  optimization problems whose solutions of an instance $I$ are permutations of $n$ elements of a set $A$ of $I$ (vertices or edges in a graph, jobs in a scheduling problem,\dots). So the goal is to maximize or minimize $f_I(\sigma)$ for $\sigma:[1,n]\rightarrow [1,n]$. Here, $a_{\sigma(i)}\in A$ is the element of $A$ that is in position $i$ in the permutation (i.e., the permutation is $(a_{\sigma(1)},a_{\sigma(2)},\dots,a_{\sigma(n)})$). To deal with feasibility constraints, $f_I(\sigma)=\infty$ if $\sigma$ is unfeasible (for a minimization problem, $-\infty$ for a maximization problem).

A core part of our work will focus on permutation problems for which we have an additional information, which is an approximation of $\sigma^*(i)$ for an optimal solution $\sigma^*$. We formalize this in the following definition. 

\begin{definition}
    Given a permutation problem $P$ and a permutation $(a_1,a_2,\dots,a_n)$, $P$ is {\it $k$-position enhanced} if we know that there exists an optimal solution $\sigma^*$ such that for all $i$, $|\sigma^*(i)-i|\leq k$.
\end{definition}



\subsection{Framework Overview}

The NSWR problem has been introduced and studied in~\cite{braverman2}. They showed the following result.

\begin{theorem}{\cite{braverman2}}\label{theorem:mainbraverman}
There exists a randomized algorithm that for any $\alpha > 0$ finds an optimal solution
to NSWR with $p=1/2+\epsilon, \epsilon>0$ in time $n^{O((\alpha+1)\epsilon^{-4})}$ except with probability $n^{-\alpha}$. Moreover, the algorithm asks $O(n\log n)$ queries.
\end{theorem}

The proof of this theorem mainly relies on two results. The first one shows that with high probability an optimal solution $\pi^*$ of NSWR is close to the ``hidden'' permutation $\sigma^*$. 
    

%Let us now state one theorem and two lemmas from~\cite{braverman2}, adapted to align with our model, which will serve as fundamental tools throughout this paper.

\begin{theorem}{\cite{braverman2}} 
\label{theo:log}
Consider the NSWR problem, with $p=1/2+\epsilon, \epsilon>0$, with respect to a permutation $\sigma^*$ and let $\pi^*$ be any s-optimal order assuming that $q$ satisfies Definition~\ref{def:pred}. Let $\alpha>0$. Then there exists a constant $c(\alpha, \epsilon)$ such that except with probability $O(n^{-\alpha})$ it holds that 
$$\max_i |\sigma^*(i) - \pi^*(i)| \le c \cdot \log n = O( \log n).$$
\end{theorem}
The second result is a dynamic programming (DP) algorithm showing that $k$-position enhanced NSWR is solvable in $O(2^{O(k)}n^2)$. Then a specific iterative procedure allows for the computation of an optimal solution of NSWR. Very roughly speaking, the use of queries allows for a $k$-position enhancement for NSWR with $k=O(\log n)$ (thanks to Theorem~\ref{theo:log}), and then the DP algorithm works in polynomial time $O(2^{O(k)}n^2)=n^{O(1)}$.

In this work, we build upon these results to tackle various problems in our prediction setting. Roughly speaking, our framework first generates a warm-start solution using the prediction queries and then utilizes this solution to solve the problem with dynamic programming. The idea of leveraging predictions to obtain a warm-start solution has been explored in a series of papers in the literature~\cite{warm-start_1, warm-start_2}. The following lemma, which makes a connection with the aforementioned results on NSWR, will allow us to get the polynomial time algorithms claimed in Theorem~\ref{theo:main}.

\begin{lemma}\label{lemma:polytime}
    Suppose that a $k$-position enhanced version of permutation problem $P$ is solvable in polynomial time for $k=O(\log n)$. Then $P$ can be solved exactly in polynomial time with high probability, using $O(n \log n)$ prediction queries.
\end{lemma}
\begin{proof}
    Let $\sigma^*$ be an optimal permutation for an instance of the optimization problem $P$. 
    By making $O(n\log n)$ queries, according to Theorem~\ref{theorem:mainbraverman}  we can get in polynomial time with high probability an optimal solution $\pi^*$ for the NSWR problem relative to $\sigma^*$.

    From Theorem~\ref{theo:log}, we know that with high probability $|\sigma^*(i)-\pi^*(i)|=O(\log n)$ for all $i$. Equivalently, for all $j$: 
$$|\sigma^* \circ \pi^{*-1}(j)-j|=O(\log n).$$
    As by assumption the $k$-position enhanced  version of  $P$ is solvable in polynomial time for $k=O(\log n)$, we can find $\sigma^*\circ \pi^{*-1}$, hence $\sigma^*$, in polynomial time.
\end{proof}
Then, we mainly exhibit sufficient conditions for a permutation problem to be polynomial time solvable when being $k$-positioned enhanced, for $k=O(\log n)$. These conditions (decomposability and $c$-locality) and their illustration on classical optimization problems are given in Section~\ref{sec:dec}. The proofs that under these conditions $O(\log n)$-positioned enhanced permutation problems are polynomial time solvable are in Section~\ref{sec:proof}. They are based on DP algorithms, one of which being a generalization of the one of~\cite{braverman2}.

%We now state the lemma of~\cite{braverman2} that shows that if a list is pre-sorted so that each element is at most $k$ positions away from its location in an s-optimal ordering, then the s-optimal ordering can be found in time $O(n \cdot k^2 \cdot 2^{6k})$.

% \begin{lemma}{\cite{braverman2}}
% \label{lem:presort}
% Let $a_1$, $a_2$, $\ldots$,  $a_n$ be $n$ elements 
% together with prediction queries $q$. Assume that 
% we are given that there is an s-optimal ordering 
% $a_{\sigma(1)},a_{\sigma(2)},\ldots,a_{\sigma(n)}$, such
% that $|\sigma(i)-i|\le k$ for all $i$.  
% Then we can find such an s-optimal $\sigma$ in time $O(n \cdot k^2 \cdot 2^{6k})$. 
% \end{lemma}



%Let us also restate a lemma proven in~\cite{braverman2}. This lemma is a consequence of the proof of Theorem~\ref{theo:log}.

%\begin{lemma}{\cite{braverman2}}
%\label{lem:chain}
%Let $\sigma$ be a permutation and $S_1\subset\ldots\subset S_n$ be a chain of randomly chosen subsets with $|S_k|=k$. 
%Denote by $\sigma_k$ an s-optimal ordering on $S_k$ wrt $\sigma$.
%Then with probability $\ge 1-n^{-\beta}/2$,
%for each  $\sigma_k$ and for each $i$, $|i-\sigma_k(i)|<c_1\log n$, where 
%$c_1 = O(\gamma^{-4}(\beta+1))$ is a constant.
%\end{lemma}

%It is important to note that the aforementioned theorem and lemmas from~\cite{braverman2} are independent of the problem at hand. Based on this crucial observation, we can generalize this approach to a family of optimization problems.


\section{Decomposability and $c$-locality}\label{sec:dec}

%As mentioned in the introduction, we consider permutation problems, i.e.,  optimization problems whose solutions of an instance $I$ are permutations of $n$ elements of a set $U$\bruno{Denoted by $A$ in introduction. Notation must be adjusted.} of $I$ (vertices or edges in a graph, jobs in a scheduling problem,\dots). So the goal is to maximize or minimize $f_I(\sigma)$ for $\sigma:[1,n]\rightarrow [1,n]$. Here, $u_{\sigma(i)}\in U$ is the element of $U$ that is in position $i$ in the permutation (i.e., the permutation is $(u_{\sigma(1)},u_{\sigma(2)},\dots,u_{\sigma(n)})$). To deal with feasibility constraints, $f_I(\sigma)=\infty$ if $\sigma$ is unfeasible (for a minimization problem, $-\infty$ for a maximization problem).

We now give the definitions for the decomposition property and $c$-locality, and illustrate them with some problems expressible in these ways. As explained before, each of these properties will allow to design DP algorithms, which is the key step to derive Theorem~\ref{theo:main}.  


\subsection{Decomposition property}

To design DP algorithms, we will consider subproblems. Intuitively, for $i<j$ we will consider subproblems of finding and ordering elements in positions $i$ to $j$, i.e., $(a_{\sigma(i)},\dots,a_{\sigma(j)})$, and need a recurrence that allows expressing the subproblem between $i$ and $j$ as a combination of the subproblems between $i$ and $s$, and between $s+1$ and $j$ (for $i<s<j$). A difficulty is that finding and ordering elements from positions $i$ to $j$ typically depends on the elements before (between positions $1$ and $i-1$) and after (between positions $j+1$ and $n$), and on their respective ordering. Roughly speaking, our decomposition property holds when finding and ordering elements from positions $i$ to $j$ only depends on the {\it set} of elements before $i$ and after $j$, not on their particular ordering. 

We now formalize this idea, and illustrate it on three different problems. For a permutation $\sigma:[1,n]\rightarrow [1,n]$, we denote $\sigma(i,j)$ the subpermutation of $\sigma$ on $[i,j]$, $S_\sigma(i,j)$ the set $\{\sigma(i),\dots,\sigma(j)\}$ (indices in positions between $i$ and $j$),  $L_\sigma(k)=S_\sigma(1,k)$ (first $k$ indices, $L$ stands for left) and  $R_\sigma(k)=S_\sigma(k,n)$ (indices in position between $k$ and $n$, $R$ stands for right).

%The decomposition property states that solving a subproblem $\mathcal{I}(i,j)$ optimally depends only on the set of elements in $[i,j]$, the permutation $\sigma(i,j)$ of these elements in $[i,j]$, and the set of elements to the left of $i$ and to the right of $j$, but not on their order. More formally, we have the following definition:


\begin{definition}
\label{def:decomp}
    Let $P$ be a permutation problem, with objective function $f$. We say that $P$ fulfills the decomposition property if there exists a function $g$ such that:
\begin{itemize}
	\item $f_I(\sigma)=g(1,n,\emptyset,\emptyset,\sigma)$;
	\item For any $i<s<j$ in $\{1,\dots,n\}$ and permutation $\sigma$:
\begin{align*}
     g(i,j,L_\sigma(i-1),R_{\sigma}(j+1), \sigma(i,j)) =  \nonumber  &g(i,s,L_\sigma(i-1),R_\sigma (s+1), \sigma(i,s)) \nonumber \\
    & + g(s+1,j,L_\sigma(s),R_{\sigma}(j+1), \sigma(s+1,j))  \nonumber\\
    & + h(L_\sigma(i-1), R_\sigma(j+1), S(i,s), S(s+1,j)), 
\end{align*}	
\noindent for some function $h$.	
\end{itemize}    
     
\end{definition}
Note that $h$ does not depend on the permutation of any elements, only on sets of elements. In this definition, we express the objective function on subproblem $(i,j)$ by a function $g$ that depends on the subpermutation $\sigma(i,j)$ between $i$ and $j$ and on sets (not positions) of elements before $i$ and after $j$ ($L_\sigma(i-1)$ and $R_\sigma(j+1)$). 

The decomposition property states that the value is the sum of the value on the subproblem $(i,s)$ (referred to as the ``left call'' in the sequel), the value on the subproblem $(s+1,j)$ (referred to as the ``right call''), and a quantity (function $h$) that depends only on the sets (not the ordered sets) of elements involved in the decomposition.


%\begin{definition}
%\label{def:decomp}
%    Let $P$ be an . Let $i \leq j$, $S(i,j)$ denote the set of elements between indices $i$ and $j$ in the permutation, $\sigma(i,j)$ denote the permutation of the elements in $S(i,j)$ within $[i,j]$, $S_l$ denote the set of elements to the left of $i$, and $S_r$ denote the set of elements to the right of $j$ in the permutation of the $n$ elements. 

%    We say that problem $P$ exhibits the decomposition property if the cost $f$ of the subproblem $\mathcal{I}(i,j)$ can be decomposed into the costs of the subproblems $\mathcal{I}(i,s)$ and $\mathcal{I}(s+1,j)$ for every $s$ with $i \leq s \leq j$, in the following way:    
%\begin{align*}
%    f(i,j,S_l,S(i,j), \sigma(i,j)) &= \nonumber \\&f(i,s,S_l,S_r \cup S(s+1,j), \sigma(i,s)) \nonumber\\
%    + &f(s+1,j,S_l \cup S(i,s),S_r, \sigma(s+1,j)) \nonumber\\
%    + &g(S_l, S_r, S(i,s), S(s+1,j)),
%\end{align*}
%for a cost function $g$ that does not depend on the permutation of any elements. 
%\end{definition}


\paragraph{Maximum Acyclic Subgraph.}

In the Maximum Acyclic Subgraph problem, we are given a simple directed graph $G=(V,E)$. The goal is to find a permutation $\sigma:[1,n]\rightarrow [1,n]$ which maximizes $f(\sigma)=|\{(v_{\sigma(i)},v_{\sigma(j)})\in E: i<j\}|$. This is a well known NP-hard problem \cite{Kar72}.

As this problem only depends on the relative order of elements (endpoints of arcs) and not on their exact position, it is easy to express it in the form of Definition~\ref{def:decomp}. For given $i<j$ and $\sigma(i,j)$, we simply define: 
$$g(i,j,L,R,\sigma(i,j))= |\{(v_{\sigma(\ell)},v_{\sigma(r)})\in E: i\leq \ell < r \leq j\}|.$$
This is just the number of arcs ``well ordered'' by $\sigma$ with both endpoints between positions $i$ and $j$ ($g$ is independent of $L$ and $R$). Obviously the first item of Definition~\ref{def:decomp} is verified (for $i=1$ and $j=n$ we count the total number of arcs ``well ordered'' by $\sigma$).

For the second item, $g(i,s,L_\sigma(i-1),R_\sigma (s+1), \sigma(i,s))$ (resp. $ g(s+1,j,L_\sigma(s),R_{\sigma}(j+1), \sigma(s+1,j))$) counts the number of arcs well ordered by $\sigma$ with both endpoints in positions between $i$ and $s$ (resp. between $s+1$ and $j$). So, to get $g(i,j,L_\sigma(i-1),R_\sigma(j+1),\sigma(i,j))$, we only have to add arcs $(v_{\sigma(\ell)},v_{\sigma(r)})$ with $i\leq \ell \leq s$ and $s+1\leq r\leq j$. In other words, if we denote $cut(A,B)$ the number of arcs $(v_\ell,v_r)$ with $\ell \in A$ and $r\in B$, we have $$ h(L_\sigma(i-1), R_\sigma(j+1), S(i,s), S(s+1,j)=cut(S(i,s),S(s+1,j)).$$

Note that this immediately generalizes to any problem whose objective function only depends on the relative order of endpoints of arcs (for instance the weighted version of Maximum Acyclic Subgraph), not on their exact positions. 

%N{_\sigma(i,j)}+cut(L_\sigma(i-1),S_\sigma(i,j))+cut(R_\sigma(j+1),S_\sigma(i,j))$$
%where $N{_\sigma(i,j)}=|\{(v_{\sigma(\ell)},v_{\sigma(r)})\in A: i\leq \ell < r \leq j\}|$ is the number of arcs ``well ordered'' in positions between $i$ and $j$. Then 

%Let us denote $cut(A,B)$ be the set of arcs $(u,v)$ with $u\in A$ and $v\in B$. 

\paragraph{Minimum Linear Arrangement.}
Let $G = (V, E)$ be a simple undirected graph. Given a permutation $\sigma:[1,n]\rightarrow [1,n]$, let us call the weight of an edge as the absolute difference between the positions assigned to its endpoints in $\sigma$. The Minimum Linear Arrangement problem consists of finding a permutation of the vertices of $G$ such that the sum of the weights of its edges is minimized. More formally, we would like to find a permutation $\sigma:[1,n]\rightarrow [1,n]$ that minimizes $ \sum_{\{v_{\sigma(i)}, v_{\sigma(j)}\} \in E} |j-i|$. 

While Maximum Acyclic Subgraph deals (only) with the order of endpoints of arcs in the permutation, Minimum Linear Arrangement depends on their exact position. We now show that the decomposition property also allows to deal with such a problem. The idea is the following: consider $i<j$, and fix the subpermutation $\sigma(i,j)$, the sets $L_\sigma(i-1)$ of elements ``on the left'', and  $R_\sigma(j+1)$ of elements ``on the right''. Then (see Figure~\ref{fig:mla1} for an illustration of the contribution of edges):
\begin{itemize}
    \item for an edge with both endpoints in $S(i,j)$, we know its exact contribution (as $\sigma(i,j)$ is fixed). These edges have a global contribution $N_1=\sum_{\{v_{\sigma(\ell)}, v_{\sigma(r)}\} \in E,i\leq \ell,r\leq j} |r-\ell|$.  Note that $N_1$ depends only on $\sigma(i,j)$.
    \item For an edge with one endpoint in the left part $L_\sigma(i-1)$ and one endpoint in $S(i,j)$, i.e., $\{v_{\sigma(\ell)}, v_{\sigma(r)}\}$ with $\ell <i$ and $i\leq r\leq j$, we will charge only for this edge the (yet partial) contribution $r-i$ (instead of $r-\ell$, as the exact left position is not fixed yet). Let $N_2$ be the sum of these contributions (note that $N_2$ depends only on $L_\sigma(i-1)$ and $\sigma(i,j)$).
    \item Similarly, for an edge with one endpoint in the right part $R_\sigma(j+1)$ and one endpoint in $S(i,j)$, i.e., $\{v_{\sigma(\ell)}, v_{\sigma(r)}\}$ with $i\leq \ell \leq j$ and $j< r$, we will charge only for this edge the (yet partial) contribution $j-\ell$. Let $N_3$ be the sum of these  contributions (note that $N_3$ depends only on $R_\sigma(j+1)$ and $\sigma(i,j)$).
\end{itemize}
We then define $g(i,j,L_\sigma(i-1),R_\sigma(j+1),\sigma(i,j))=N_1+N_2+N_3$.

\begin{figure}[!h]
\begin{center}
\begin{tikzpicture}[scale=0.8]

\draw (1,6) node[left] {$1$} -- (3,6) node[right] {$i$};
\draw (3.4,6)  -- (7,6) node[right] {$j$};
\draw (7.5,6)  -- (9,6) node[right] {$n$};
\draw[dashed] (3.2,5.7) -- (3.2,2.5);
\draw[dashed] (7.2,5.7) -- (7.2,2.5);
\draw (5,5) node[above] {$\ell$} -- (6.5,5) node[above] {$r$};

\draw (3.2,4)  -- (6,4) node[above] {$r$};
\draw[dashed] (2,4) node[above] {$\ell$} -- (3.2,4);


\draw[dashed] (7.2,3)  -- (8,3) node[above] {$r$};
\draw (4.1,3) node[above] {$\ell$} -- (7.2,3);



\end{tikzpicture}
\caption{Contribution in $g$ of edges, depending on the positions of their extremities. The contribution is in solid line. If $\ell<i$ and $r>j$ the contribution is 0.}
  \label{fig:mla1}
\end{center}
\end{figure}


It is clear that the first item of Definition~\ref{def:decomp} is satisfied (when $i=1$ and $j=n$, $N_1$ equals the objective function, $N_2=N_3=0$).


For the second item, let $i<s<j$. Consider an edge $\{v_{\sigma(\ell)},v_{\sigma(r)}\}$. We show, for each possible case, how to recover the contribution of this edge to subproblem $(i,j)$ (i.e., in $g(i,j,L_\sigma(i-1),R_\sigma(j+1),\sigma(i,j))$), see Figure~\ref{fig:mla2} for an illustration:
\begin{itemize}
    \item If both $\ell$ and $r$ are in $[i,s]$, its contribution in $g(i,j,L_\sigma(i-1),R_{\sigma}(j+1), \sigma(i,j))$ is already counted in the ``left call'' $g(i,s,L_\sigma(i-1),R_\sigma (s+1), \sigma(i,s))$.
    \item Similarly, if both $\ell$ and $r$ are in $[s+1,j]$, its contribution in  $g(i,j,L_\sigma(i-1),R_{\sigma}(j+1), \sigma(i,j))$ is already counted in the ``right call'' $g(s+1,j,L_\sigma(s),R_{\sigma}(j+1), \sigma(s+1,j))$.
    \item If $i\leq \ell \leq s$ and $s+1\leq r \leq j$, then the contribution is $(s-\ell)$ in the left call and  $r-(s+1)$ in the right call, so in total $r-\ell-1$. It only missed 1 to get the correct contribution $r-\ell$. So for these edges we have to add in total a contribution $C_1=cut(S(i,s),S(s+1,j))$. 
    \item If $\ell <i$ and $i\leq r \leq s$, then the contribution on the left call is $r-i$, the correct one. Similarly, if $s+1\leq \ell \leq j$ and $j+1\leq r$ the contribution of the right call is the correct one ($j-\ell$).
     \item If $\ell <i$ and $s+1\leq r \leq j$, the contribution in the left call is 0, the one in the right call is $r-(s+1)$. It misses $s+1-i$ to get the correct contribution $r-i$. So for these edges we have to add in total $C_2=(s+1-i) \cdot cut(L_\sigma(i-1),S(s+1,j))$.
     \item Similarly, if $i\leq \ell \leq s$ and $r> j$, to get the correct charge we miss $C_3=(j-s) \cdot cut(R_\sigma(j+1),S(i,s))$.
\end{itemize}
Then, we define $h(L_\sigma(i-1), R_\sigma(j+1), S(i,s), S(s+1,j)=C_1+C_2+C_3$.

\begin{figure}[!h]
\begin{center}
\begin{tikzpicture}[scale=0.74]

\draw (1,6) node[left] {$1$} -- (3,6) node[right] {$i$};
\draw (3.4,6)  -- (5,6) node[right] {$s$};
\draw (5.5,6)  -- (6,6) node[right] {$s+1$};
\draw (7.2,6)  -- (9,6) node[right] {$j$};
\draw (9.5,6)  -- (11,6) node[right] {$n$};

\draw[dashed] (3.2,5.7) -- (3.2,0.5);
\draw[dashed] (9.2,5.7) -- (9.2,0.5);

\draw[dashed] (5.2,5.7) -- (5.2,0.5);
\draw[dashed] (6.5,5.7) -- (6.5,0.5);


\draw[blue] (4,5) node[above] {$\ell$} -- (4.8,5) node[above] {$r$};

\draw[orange] (7.5,5) node[above] {$\ell$} -- (8.5,5) node[above] {$r$};

\draw[blue] (4.2,4) node[above] {$\ell$} -- (5.2,4);
\draw[red] (5.2,4)  -- (6.5,4);
\draw[orange] (6.5,4)  -- (8.2,4) node[above] {$r$};

\draw[blue] (3.2,3)  -- (4.5,3) node[above] {$r$};
\draw[dashed] (2,3) node[above] {$\ell$} -- (3.2,3);

\draw[dashed] (9.2,3)  -- (10,3) node[above] {$r$};
\draw[orange] (8.5,3) node[above] {$\ell$} -- (9.2,3);


\draw[dashed] (2.2,2) node[above] {$\ell$} -- (3.2,2);
\draw[red] (3.2,2)  -- (6.5,2);
\draw[orange] (6.5,2)  -- (8.2,2) node[above] {$r$};


\draw[blue] (4,1) node[above] {$\ell$} -- (5.2,1);
\draw[red] (5.2,1)  -- (9.2,1);
\draw[dashed] (9.2,1)  -- (10.2,1) node[above] {$r$};


\end{tikzpicture}
\caption{Decomposition in $g$: blue (resp. orange) corresponds to the contribution in the left call (resp. right call), red corresponds to missing contributions (that are counted in $h$). If $\ell<i$ and $r>j$ the contribution is 0.}
  \label{fig:mla2}
\end{center}
\end{figure}


\paragraph{Single-machine Sum of Completion Time Problem.}
We consider the following scheduling problem (denoted $1|prec|\sum C_j$ in the classical Graham's notation for scheduling problems), known to be strongly NP-hard~\cite{Sequencing-jobs}: we are given a set $J$ of $n$ jobs  and a set of precedence constraints forming a DAG $(J,A)$. The goal is to order the jobs, respecting precedence constraints, so as to minimize the sum of completion time of jobs. 

Consider $i<j$, $\sigma(i,j)$ and $L_\sigma(i-1)$. With this information, we can compute the completion time of each job in $S(i,j)$ (as we know the jobs before them ($L_\sigma(i-1)$) and the order $\sigma(i,j)$ of jobs in $S(i,j)$). So we simply define $g$ as the sum of these completion times ($g$ depends on $S(i,j)$ and $L_\sigma(i-1)$), with of course value $\infty$ if the order $\sigma(i,j)$ violates any constraints. 

Item 1 of Definition~\ref{def:decomp} is trivially satisfied. For item 2, the left call computes the sum of completion times for jobs in $S(i,s)$, and the right call the sum of completion times for jobs in $S(s+1,j)$. Then $h$ is $\infty$ if a constraint is violated (between a job whose position is between $s+1$ and $j$, and a job whose position is between $i$ and $s$), and 0 otherwise. 

Here again, this generalizes to many other single machine scheduling problems (involving other constraints and/or weights on jobs and/or other objective functions like sum of tardiness).


\subsection{$c$-locality}

The $c$-locality property states that the cost function of the problem depends only locally (with respect to the permutation/solution) on pairs of distinct elements. More formally, we have the following definition. 

\begin{definition}
    \label{def:c-local}
    Let $P$ be a permutation problem, with cost function $f$. We say that $P$ is $c$-local if, on an instance $I$ asking for a permutation $\sigma$ on a set $A=\{a_1,\dots,a_n\}$:
    $$f_I(\sigma) = \sum_{i} cost_I(a_{\sigma(i-c)},a_{\sigma(i-c+1)}, \dots,a_{\sigma(i)}).$$
%\begin{equation*}
%    f_I(\sigma) = \sum_{i,j: |i-j|\le c} cost_I(a_{\sigma(i)}, a_{\sigma(j)}).
%\end{equation*}
for some cost function\footnote{To be more precise, to deal with the cases $i\leq c$ in the sum it should be $cost_I(a_{\sigma(t)},a_{\sigma(t+1)},\dots, a_{\sigma(i)})$ for $t=\max\{i-c,1\}$.} $cost_I$.
\end{definition}

\smallskip\noindent\textbf{TSP.} Given a complete graph on set $V$ of vertices and a distance function $d(v_i,v_j)$ on pair of vertices, TSP asks to find a permutation $\sigma$ that minimizes $\sum_{i=1}^n d(v_{\sigma(i)},v_{\sigma(i+1)})$ (where $v_{\sigma(n+1)}$ is $v_{\sigma(1)}$). The problem is then trivially 1-local (we can easily reformulate to get rid of the last term $d(v_{\sigma(n)},v_{\sigma(1)})$).

Other routing problems can be shown to be $1$-local as well.

%{\it Any $c$-local problem for $c\geq 2$?}

\smallskip\noindent\textbf{Social Welfare Maximization in Keyword Auctions with Externalities.}
%
In standard Keyword Auctions~\cite{EdelmanOS2007,Varian2007}, a set $N = \{1, \ldots, n\}$ of advertisers compete over a set $K = \{1, \ldots, k\}$ of advertisement (or simply ad) slots, where $k \leq n$. Each player $i \in N$ has a valuation $v_i$ per click and their ad has an intrinsic click probability $q_i \in (0, 1]$. Each slot $j \in K$ is associated with a click-through rate (ctr) $\lambda_j$, with $1 \geq \lambda_1 > \lambda_2 > \cdots \lambda_k > 0$. The overall ctr of an ad $i$ in slot $j$ is $\lambda_j q_i$. 
%
In the following, we assume that $k = n$, i.e., the number of slots $k$ is equal to the number of ads $n$, for simplicity, by setting  $\lambda_{k+1} = \cdots = \lambda_n = 0$ in case where $k < n$. 

We aim to compute an assignment $\pi : N \to K$ of ads to slots (which for $k = n$ is a permutation of ads) so that the resulting expected \emph{social welfare}, which is $\sum_{i=1}^n v_i \lambda_{\pi(i)} q_i$, is maximized. 

In Keyword Auctions with Externalities \cite{GattiRSV2018,FotakisKT2011}, the overall click-through rate of an ad $i$ appearing in slot $\pi(i)$ also depends on the ads appearing in the $c$ slots above $\pi(i) - c, \pi(i) - c+1, \ldots, \pi(i)-1$, where $c$ is the window size. 
%
The influence of an ad $j$ on the ctr of an ad $i$ is quantified by a function $w_{ji} : [n] \to [0,1]$ of their distance in $\pi$. Specifically, we let $d_\pi(j,i) = \pi(i)-\pi(j)$ and let $w_{ji}(d_\pi(j, i))$ be $j$'s influence on $i$'s crt, given their slots assigned by $\pi$. To enforce the condition on the window size, we assume that $w_{ji}(x) = 0$, if $x \leq 0$ or $x > c$. Then, in the model of Keyword Auctions with Externalities presented in \cite{FotakisKT2011}, the influence on $i$'s ctr of the other ads appearing the $c$ slots above $i$ in $\pi$ is:
%
$$ Q_i(\pi) = 1 - (1-q_i) \prod_{j = \pi^{-1}(\ell): \ell = \max\{ \pi(i) - c, 1\}}^{\max\{\pi(i)-1, 1\}}\Big(1-q_{j}w_{ji}(d_\pi(j, i))\Big).$$
%
For simplicity, we only present calculation of ctr for the 
case of \emph{positive externalities} from \cite{FotakisKT2011} (the  calculation of ctr for the case of  \emph{negative externalities} is defined similarly). 

We aim to compute an assignment $\pi : N \to [n]$ of ads to slots so that the resulting expected social welfare under externalities, which is $\sum_{i=1}^n v_i \lambda_{\pi(i)} Q_i(\pi)$, is maximized. 

Social welfare maximization in keyword auctions with externalities and window size $c$ is shown to be NP-hard in \cite{FotakisKT2011,GattiRSV2018}. It is not hard to verify that computing the maximum social welfare under externalities in the model of \cite{GattiRSV2018,FotakisKT2011} is an example of a $c$-local permutation problem, because the overall ctr of each ad $i$ depends on the ads appearing in the $c$ slots above 


\section{Proof of Theorem~\ref{theo:main}}
\label{sec:proof}
In this section, we present the proof of Theorem~\ref{theo:main}, the main result of this paper. Using Lemma~\ref{lemma:polytime}, what remains to be shown is that if a problem is decomposable or $c$-local, then it is polynomial time solvable when being $O(\log n)$-position enhanced. We provide a corresponding DP algorithm for decomposition in Lemma~\ref{lem:presort_decomp} (which extends the one of~\cite{braverman2}) and for $c$-locality in Lemma~\ref{lem:presort_local}


%Before proceeding with the proof, we introduce two lemmas that demonstrate the following: if a list (permutation) is presorted such that each element is at most $k$ positions away from its location in an optimal permutation for a permutation problem $P$ (where $P$ is either decomposable or $c$-local), then the optimal permutation can be efficiently determined using dynamic programming.
%These lemmas extend and generalize Lemma~\ref{lem:presort} of~\cite{braverman2}.

%\subsection{Algorithms for presorted permutations}

%We begin by proving the lemma for the case where the permutation optimization problem $P$ satisfies the decomposition property.  \bruno{We may also define $k$-Positioned-Enhanced Permutation problems, as in the last section? Not sure, just a suggestion.}



\begin{lemma}
\label{lem:presort_decomp}
If a permutation optimization problem $P$ is decomposable, then its $k$-position enhanced version is solvable in time $O(n^{d+1} \cdot 2^{O(k)})$ if the value of a solution is computable in time $O(n^d)$.
\end{lemma}

%The value of $k$ for our applications will be $O(\log n)$.
This gives a polynomial computation time when $k=O(\log n)$.

\begin{proof}
We will use dynamic programming to find an optimal solution for the optimization problem $P$. Let $\sigma^*$ be an (unknown) optimal permutation such that $|\sigma^*(i)-i|\leq c\log n$ for all $i$. 

Let $i<j$ be any indices. Let $I^*(i,j)$ denote the elements in positions between $i$ and $j$ in $\sigma$, i.e.,  
$$I^*(i,j)=\{a_{\sigma(i)}, a_{\sigma(i+1)},\ldots, a_{\sigma(j)}\}.$$ 
Moreover, let %we will also guess the elements on the left of position $i$


$$I^*_L(i)=\{a_{\sigma(1)}, a_{\sigma(2)},\ldots, a_{\sigma(i-1)}\}$$ 
be the elements of the left of $i$ and 
$$I^*_R(j)=\{a_{\sigma(j+1)}, a_{\sigma(j+2)},\ldots, a_{\sigma(n)}\}$$
the elements on the right of $j$.
By assumption, we have  
$I^{-}_L(i) \subseteq I^*_L(i) \subseteq I^{+}_L(i)$, $I^{-}(i,j) \subseteq I^*(i,j) \subseteq I^{+}(i,j)$ and $I^{-}_R(j) \subseteq I^*_R(j) \subseteq I^{+}_R(j)$ where
$$I^+_L(i) = \{a_{1},a_{2},\ldots,a_{i+k-1}\},$$ 
$$I^-_L(i) = \{a_{1},a_{2},\ldots,a_{i-k-1}\},
$$
$$
I^+(i,j) = \{a_{i-k},a_{i-k+1},\ldots,a_{j+k}\},$$ 
$$I^-(i,j) = \{a_{i+k},a_{i+k+1},\ldots,a_{j-k}\},
$$
$$
I^+_R(j) = \{a_{j+1-k},a_{j+2-k},\ldots,a_{n}\},$$ 
$$I^-_R(j) = \{a_{j+1+k},a_{j+2+k},\ldots,a_{n}\}.
$$

Thus, selecting the sets $I^*_L(i)$, $I^*(i,j)$ and $I^*_R(j)$ involves selecting $k$ elements from a list of $2k$ elements for $I^*_L(i)$ and $2k$ elements from $4k$ elements for $I^*(i,j)$.  Once $I^*_L(i)$, $I^*(i,j)$ are fixed, the remaining elements belong to $I^*_R(j)$ (see Figure~\ref{fig:dp_sets} for an illustration). Thus the number of different guesses we have to do is bounded by $2^{2k+4k}=2^{6k}$. 

\begin{figure}[!h]
\begin{center}

\begin{tikzpicture}[scale=0.56]

    % Horizontal axis line
    \draw[thick] (0,0) -- (14,0);

    % Vertical dashed lines
    \foreach \x in {0, 2, 4, 6, 8, 10, 12, 14} {
        \draw[gray, dashed] (\x, 0) -- (\x, 0.15);
    }

    \draw[gray, dashed] (0,0) -- (0,-1);
    \draw[gray, dashed] (1.8,0) -- (1.8,-0.5);
    \draw[gray, dashed] (5.8,0) -- (5.8, -1);
    \draw[gray, dashed] (6,0) -- (6,-1.5);
    \draw[gray, dashed] (8,0) -- (8,-1.5);
    \draw[gray, dashed] (2,0) -- (2,-2);
    \draw[gray, dashed] (12,0) -- (12,-2);
    \draw[gray, dashed] (12.2,0) -- (12.2,-2.5);
    \draw[gray, dashed] (8.2,0) -- (8.2,-3);
    \draw[gray, dashed] (14,0) -- (14,-3);
    


    % Labels along the top
    \node[above] at (0,0) {$a_1$};
    \node[above] at (2,0) {$a_{i-k}$};
    \node[above] at (4,0) {$a_i$};
    \node[above] at (6,0) {$a_{i+k}$};
    \node[above] at (8,0) {$a_{j-k}$};
    \node[above] at (10,0) {$a_j$};
    \node[above] at (12,0) {$a_{j+k}$};
    \node[above] at (14,0) {$a_n$};

    % Horizontal intervals with labels
    % I_L^- and I_L^+
    \draw[blue, <-] (0, -0.5) -- (0.6, -0.5);
    \draw[blue, ->] (1.2, -0.5) -- (1.8, -0.5);
    \node[blue] at (0.9, -0.5) {$I_L^-$};

    \draw[blue, <-] (0, -1) -- (2.6, -1);
    \draw[blue, ->] (3.2,-1) -- (5.8,-1);
    \node[blue] at (2.9, -1) {$I_L^+$};

    % I^- and I^+
    \draw[green, <-] (6, -1.5) -- (6.7, -1.5);
    \draw[green, ->] (7.3, -1.5) -- (8, -1.5);
    \node[green] at (7, -1.5) {$I^-$};

    \draw[green, <-] (2, -2) -- (6.7, -2);
    \draw[green, ->] (7.3, -2) -- (12, -2);
    \node[green] at (7, -2) {$I^+$};

    % I_R^- and I_R^+
    \draw[red, <-] (12.2, -2.5) -- (12.8, -2.5);
    \draw[red, ->] (13.3, -2.5) -- (14, -2.5);
    \node[red] at (13.1, -2.5) {$I_R^-$};

    \draw[red, <-] (8.2, -3) -- (10.8, -3);
    \draw[red, ->] (11.4, -3) -- (14, -3);
    \node[red] at (11.1, -3) {$I_R^+$};

\end{tikzpicture}
\caption{Selecting the set $I^*$ involves choosing $j - i + 1$ elements that include all elements of $I^-$ and are contained within $I^+$ (highlighted in green). This corresponds to selecting $2k$ elements from the list $\{a_{i-k}, a_{i-k+1}, \dots, a_{i+k-1}, a_{j-k+1}, \dots, a_{j+k}\}$ of $4k$ elements.
Similarly, selecting $I^*_L$ requires choosing $i - 1$ elements that include $I^-_L$ and are contained within $I^+_L$ (in blue). This involves selecting $k$ elements from the list 
$\{a_{i-k}, \dots, a_{i+k-1}\}$ of $2k$ elements. The same logic applies for $I^*_R$ (in red).}
\label{fig:dp_sets}
\end{center}
\end{figure}

Now, let us define for any $i,j$ the set $\mathcal{S}(i,j)$ of sets $I'(i,j)$ of size $j-i+1$ such that $I^{-}(i,j) \subseteq I'(i,j) \subseteq I^{+}(i,j)$. We have that $I^*(i,j)\in \mathcal{S}(i,j)$.

Moreover, let $\mathcal{L}(i-1)$ be the set of sets $I_L'(i)$ of size $i-1$ such that $I^{-}_L(i) \subseteq I'_L(i) \subseteq I^{+}_L(i)$. We have that $I^*_L(i) \in \mathcal{L}(i-1)$. Similarly, we define the set $\mathcal{R}(j+1)$ of sets $I_R'(j)$ of size $n-j$.

We now define the following problem $\mathcal{I}(i,j)$: for each $I'(i,j)\in \mathcal{S}(i,j)$, each $I'_L(i)\in \mathcal{L}(i-1)$ and each $I'_R(j)\in \mathcal{R}(j+1)$ such that $I'(i,j)$, $I'_L(i)$ and $I'_R(j)$ are pairwise disjoint, find a permutation $\sigma'$ of elements in $I'(i,j)$ such that:
\begin{itemize}
    \item $|\sigma'(t)-t|\leq k$,
    \item $\{a_{\sigma'(i)},\dots,a_{\sigma'(j)}\}=I'(i,j)$ 
    \item $\sigma'(i,j)$ optimizes the function $g$ given in the decomposition property of $P$.%\bruno{might be said better.}
%    \textcolor{red}{it optimizes the objective function of $P$.}
\end{itemize}


%Basically, this decomposition property states that solving a subproblem $\mathcal{I}(i,j)$ optimally depends only on the set of elements in $[i,j]$ ($S(i,j)$), the permutation $\sigma(i,j)$ of these elements in $[i,j]$, and the set of elements to the left of $i$ ($L_{\sigma}(i-1)$) and the set of elements to the right of $j$ ($R_{\sigma}(j+1)$), but not on their order.\bruno{I think that we shoud remove the part from ``Basically'' to the end of the paragraph. We already said that several times.}

Based on the decomposition property of $g$, we will solve the problem via dynamic programming. Note that we find an optimal solution by solving $\mathcal{I}(1,n)$. Assume for the sake of simplicity that $n$ is a power of two. We will solve $\mathcal{I}(1,n)$ using the solutions of $\mathcal{I}(1,n/2)$ and $\mathcal{I}(n/2+1,n)$, and so on. We solve the leaves of the tree (containing only one element) in constant time, and we have it total $n-1$ subproblems. 


Let us explain how we solve $\mathcal{I}(i,j)$ using the solutions of $\mathcal{I}(i,s)$ and $\mathcal{I}(s+1,j)$, where $i \le s \le j$. 

Let $I'(i,j)\in \mathcal{S}(i,j)$, $I_L'(i)\in \mathcal{L}(i-1)$ and $I_R'(j)\in \mathcal{R}(j+1)$. Thanks to the decomposition property on $g$ (see Definition~\ref{def:decomp}), $I'(i,j)$, $I_L'(i)$ and $I_R'(j)$ being fixed, an optimal $\sigma'(i,j)$ is composed of an optimal on $[i,s]$ and an optimal solution on $[s+1,j]$. There are at most $2^{O(k)}$ choices to partition $I'(i,j)$ into two sets $I'(i,i+s)\in \mathcal{S}(i,s)$ and $I'(s+1,j)\in \mathcal{S}(s+1,j)$. So we can compute an optimal solution of our subproblem using $2^{O(k)}$ calls to the DP table (on subproblems in $\mathcal{I}(i,s)$ and $\mathcal{I}(s+1,j)$.
\end{proof}

%For each such choice we look up an optimum solution for $\mathcal{I}(i,i+s)$ and for $\mathcal{I}(i+s+1,j)$ in the
%dynamic programming table according to the aforementioned formula.
%Among all possible choices we pick the best one. This is done 
%by computing the cost value of $f$ for the joined interval, 
%and takes at most $O(n^2)$ time. Thus the total 
%cost will be $O(n^3 \cdot 2^{O(k)})$.

%%%%%%%% OLD VERSION
%Let us explain how we solve $\mathcal{I}(i,j)$, where $i \le s \le j$, using the solutions of $\mathcal{I}(i,s)$ and $\mathcal{I}(s+1,j)$. The cost of the subproblem $\mathcal{I}(i,j)$ can be decomposed to the subproblems of $\mathcal{I}(i,s)$ and $\mathcal{I}(s+1,j)$ in the following way:
%\begin{equation*}
%    f_I(\sigma) = g(1,n,\emptyset, \emptyset, \sigma).
%\end{equation*}
%\begin{align}
%\label{eq:decomp}
%    g(i,j,L_{\sigma}(i-1)&, R_{\sigma}(j+1), \sigma(i,j)) = \nonumber \\&g(i,s,L_{\sigma}(i-1), R_{\sigma}(s+1), \sigma(i,s)) \nonumber\\
%    + &g(s+1,j,L_{\sigma}(s), R_{\sigma}(j+1), \sigma(s+1,j)) \nonumber\\
%    + &h(L_{\sigma}(s), R_{\sigma}(s), S(i,s), S(s+1,j)).
%\end{align}

%Let $I'(i,j)\in S'(i,j)$, $I_L'(i,j)\in L_{\sigma'}(i-1)$ and $I_R'(i,j)\in R_{\sigma'}(j+1)$. There are at most $2^{O(k)}$ choices for the elements of $I'(i,i+s)$ $R_{\sigma'}(i+s+1)$ $L_{\sigma'}(i-1)$. 
%For each such choice we look up an optimum solution for $\mathcal{I}(i,i+s)$ and for $\mathcal{I}(i+s+1,j)$ in the
%dynamic programming table according to the aforementioned formula.
%Among all possible choices we pick the best one. This is done 
%by computing the cost value of $f$ for the joined interval, 
%and takes at most $O(n^2)$ time. Thus the total 
%cost will be $O(n^3 \cdot 2^{O(k)})$.
%%%%%%%% END OLD VERSION
%\end{proof}

Now we can also prove a similar lemma for the case of a $c$-local permutation optimization problem, using a different DP.

\begin{lemma}
\label{lem:presort_local}
Let $P$ be a $c$-local permutation  problem. Its $k$-position enhanced version is solvable in time $O(n \cdot 2^{2k} \cdot k^{c+1})$.
\end{lemma}

\begin{proof}
    We will use again dynamic programming to find an optimal solution for problem $P$. Let $i$ be any index. Let $I^*(i)$ denote the elements in an optimal order in positions between $1$ and $i$ in $\sigma$, i.e.,
    $I^*(i) = \{ a_{\sigma(1)}, \dots, a_{\sigma(i)} \}.$

    By assumption, we have that
    $I^{-}(i) \subseteq I^*(i) \subseteq I^{+}(i)$, where
    $$
    I^-(i) = \{a_{1},a_{2},\ldots,a_{i-k}\},\quad 
    I^+(i) = \{a_{1},a_{2},\ldots,a_{i+k}\}.
    $$

Thus, selecting the set $I^*(i)$ involves selecting $k$ elements from a list of $2k$ elements. Therefore, the number of different guesses we have to do is bounded by $2^{2k}$.

Let us now define for any $i$ the set $S(i)$ of sets $I'(i)$ of size $i$ such that $I^{-}(i) \subseteq I'(i) \subseteq I^{+}(i)$. We have that $I^*(i) \in S(i)$.

Similarly, we let $I^*(i+1,i+c)$ denote the elements in positions between $i+1,i+c$ in $\sigma$. Here, we have to select each of these $c$ elements among a list of $2k+1$ elements, so the number of different guesses is bounded by $O(k^{c})$ (as $c$ is a constant). Moreover, for each guess we also consider all different permutations of the $c$ elements (denoted by $\sigma(i,i+c)$) which takes constant time (as $c$ is constant).

We also define for any $i$ the set $\mathcal{S}(i+1,i+c)$ of sets $I'(i+1,i+c)$ of size $c$ such that $I^{-}(i+1,i+c) \subseteq I'(i+1,i+c) \subseteq I^{+}(i+1,i+c)$. We have that $I^*(i+1,i+c) \in \mathcal{S}(i+1,i+c)$.

Let us now explain how we solve the problem using dynamic programming which is based on the property of $c$-locality. 
For every entry of the DP table we have the following:
%
%\begin{align*}
%    DP(&S(i), a_{\sigma(i+1)}, \dots, a_{\sigma(i+c)})=\\ &\min_{\text{over all }\sigma(i,i+c-1)} \big\{ DP(S(i), a_{\sigma(i+1)}, \dots, a_{\sigma(i+c-1)}) \\+ &\sum_{j=1}^{i+c-1} d(a_{\sigma(j)}, a_{\sigma(i+c)}) \big\}.
%\end{align*}
%
\begin{align*}
    DP(S(i), a_{\sigma(i+1)}, \dots, a_{\sigma(i+c)})= &\min_{\sigma(i)} \big\{ DP(S(i)\setminus \{\sigma(i)\}, a_{\sigma(i)}, \dots, a_{\sigma(i+c-1)}) \\ &+ 
    cost_I(a_{\sigma(i)},a_{\sigma(i+1)}, \dots,a_{\sigma(i+c)}) \big\}.
%    \sum_{j=i}^{i+c-1} d(a_{\sigma(j)}, a_{\sigma(i+c)}) \big\}.
\end{align*}

There are at most $2k+1$ choices for $a_{\sigma(i)}$, so each DP entry can be computed in time $O(k)$. Overall, we have that the running time is $O(n \cdot 2^{2k} \cdot k^{c+1})$. 
\end{proof}

%\subsection{Proof of main Theorem}

%We are now ready to give the proof of Theorem~\ref{theo:main}. 

%\MainTheorem*

%\textcolor{blue}{Bruno: I do not think we shall cite and use Lemma~\ref{lem:chain}. If I am not wrong, what we need is precisely the fact that SNSA can be solved whp in polynomial time (with $O(n\log n)$ queries), which is Theorems 1.1 and 1.3 in Braverman. Then, from this result and Theorem 2.3, the DP directly allows to prove our theorem. This is imo clearer, and avoids copy pasting the proof which uses the (not that clear) Lemma~\ref{lem:chain}.}

%\begin{proof}
%    For the first step of the proof we will follow a procedure given in~\cite{braverman2}.
%    Consider a randomly chosen chain of sets $ S_1 \subset \ldots \subset S_n $ such that $ |S_k| = k $. According to Lemma \ref{lem:chain}, for any s-optimal order $\sigma_k$ of $S_k$ and for each $i$, it holds that $|i - \sigma_k(i)| < c_1 \log n$ with high probability. We will compute the orders $\sigma_k$ iteratively until reaching $\sigma_n$, which represents an s-optimal order for the sorting problem. Let $\{a_k\} = S_k - S_{k-1}$. Assuming $\sigma_{k-1}$ has been computed, the goal is to compute $\sigma_k$. 

%    To do so, we first insert $a_k$ into a position close to its s-optimal location as follows. Divide $S_k$ into blocks $B_1, B_2, \ldots, B_s$, each of size $c_1 \log n$. By comparing $a_k$ with each element in the blocks (using the prediction queries) and taking majority, we can identify its block within an error of $\pm 2$ with high probability. Consequently, $a_k$ is placed within $3 c_1 \log n$ positions of its s-optimal location.

%    After inserting $a_k$, we obtain an ordering of $S_k$ where each element is at most $3 c_1 \log n$ positions away from its s-optimal location. Therefore, each element is within $4 c_1 \log n$ positions of its s-optimal location in $\sigma_k$. By applying Lemma \ref{lem:presort}, $\sigma_k$ can be computed in $O(n^{24 c_1 + 2})$ time. This procedure is repeated iteratively. In~\cite{braverman2}, they also showed that this procedure can be modified to give a query complexity of $O(n \log n)$.

%    Now we have a permutation $\pi_0$ in which each element is at most $O(\log n)$ positions away from an $s$-optimal permutation for the optimization problem $P$. By Theorem~\ref{theo:log} each element of an $s$-optimal permutation is also at most $O(\log n)$ positions away from the optimal solution of problem $P$. Therefore, each element of permutation $\pi_0$ is at most $O(\log n)$ positions away from the optimal solution of problem $P$. 

%    Finally, given permutation $\pi_0$, we apply Lemma~\ref{lem:presort_decomp} for decomposable problems or Lemma~\ref{lem:presort_local} for $c$-local problems to solve $P$ exactly with high probability in polynomial time.


%\end{proof}


\section{About position-enhanced permutation problems}
\label{sec:hardness}

The dynamic programming algorithms of Lemmas~\ref{lem:presort_decomp} and~\ref{lem:presort_local} show that $O(\log n)$-position enhanced versions of decomposable or $c$-local permutation problems are solvable in polynomial time. In this section, we show two complementary results on position-enhanced versions of permutation problems that, though technically quite easy, enlight  interesting limitations to Lemma~\ref{lemma:polytime}:
\begin{itemize}
	\item First we show that there are some permutation problems which remain hard to solve in polynomial time even when being $O(\log n)$-position enhanced.
	\item Second, we show that this bound $O(\log n)$ is tight, meaning that there are decomposable problems and $c$-local problems which remain hard when being $f(n)\log n$-position enhanced, as soon as $f$ is unbounded. 
\end{itemize}	


\subsection{A $\log(n)$-position-enhanced hard problem}

Let us consider the following problem called Permutation Clique: we are given a graph $G=(V,E)$ on $n=t^2$ vertices $v_{i,j}$, $i,j=1,\dots,t$. The goal is to determine whether there exists a permutation $\sigma$ over $\{1,\dots,t\}$ such that the $t$ vertices $v_{i,\sigma(i)}$, $i=1,\dots,t$, form a clique in $G$. If one puts vertices in a tabular of size $t\times t$, then we want to find a clique of $t$ vertices with exactly one vertex per row and one per column. This problem is trivially solvable in time $2^{O(t\log t)}$. Interestingly, in \cite{LokshtanovMS18} they showed the following, where ETH stands for Exponential Time Hypothesis:

\begin{proposition}\cite{LokshtanovMS18}
Under ETH, Permutation Clique is not solvable in time $2^{o(t\log t)}$.  
\end{proposition}

We show that this problem remains hard even when being $O(\log t)$-position enhanced. Formally, the problem is defined as follows. We are given an instance $G=(V,E)$ of Permutation Clique on $t^2$ vertices, and we want to  determine if there is a permutation $\sigma$ over $\{1,\dots,t\}$ such that:
\begin{itemize}
	\item[(1)] $|\sigma(i)-i|\leq  c \log t$ for all $i=1,\dots,t$;
	\item [(2)]  $\{v_{i,\sigma(i)}:i=1,\dots,t\} $ is a clique in $G$.
\end{itemize}

\begin{proposition}
Under ETH, $(c\log t)$-Positioned-Enhanced Permutation Clique is not solvable in polynomial time (for any $c>0$).  
\end{proposition}
\begin{proof}
	Let us consider an instance $G=(V,E)$ of Permutation Clique, on $n=t^2$ vertices $\{v_{i,j}:i,j=1,\dots,t\}$. Let $t'=2^{t/c}$. We build from $G$ a graph $G'=(V',E')$ on $n'=t'^2$ vertices $\{v'_{i,j}:i,j=1,\dots,t'\}$ where:
\begin{itemize}
	\item For $i_1,i_2,j_1,j_2\leq t$: $v'_{i_1,j_1}$ is adjacent to $v'_{i_2,j_2}$ if and only if $v_{i_1,j_1}$ is adjacent to $v_{i_2,j_2}$ in $G$;
	\item For $i,j\leq t$ and $\ell>t$: $v'_{i,j}$ is adjacent to $v_{\ell,\ell}$;
	\item For $\ell_1,\ell_2>t$: $v_{\ell_1,\ell_1}$ is adjacent to $v_{\ell_2,\ell_2}$.  
\end{itemize}
By construction, a (non trivial) clique in $G'$ is composed by a clique in $G$ plus `diagonal' vertices  $v_{\ell,\ell}$. Hence, there is a permutation clique of size $t'$ in $G'$ if and only if there is a permutation clique of size $t$ in $G$. Moreover, we know by construction that if there is a permutation clique induced by $\sigma'$ in $G'$, then for $\ell>t$ $\sigma(\ell)=\ell$ (diagonal vertices), and for $\ell\leq t$ $\sigma(\ell)\leq t$. Then in any case, $|\sigma(\ell)-\ell|\leq t = c \log t'$.

Suppose that we answer to $(c\log t')$-Positioned-Enhanced Permutation Clique in polynomial time $O(n'^c)$. The construction of $G'$ takes time $O(n')$, so we can solve Permutation Clique in time $O(n'^{c})=O(t'^{c'})=2^{O(t)}$. This is in contradiction with ETH. 
\end{proof}	

\subsection{Hardness results for worse than $O(\log n)$-position-enhancement}

Let us now go back to decomposable and $c$-local problems, and show that the $O(\log n)$-position enhancement is necessary for some problems.

%Let us consider the $f\log(n)$-Positioned-Enhanced Max Acyclic subgraph, which is defined as Max Acyclic subgraph but with the extra knowledge that there is an optimal permutation $\sigma^*$ such that for all $i$ $|\sigma^*(i)-i|\leq f(n)\log(n)$, where $n$ is the number of vertices of the graph.

\begin{proposition}\label{prop:hardloc}
	For any unbounded increasing function $f$, $f(n)\log n$-Positioned-Enhanced Max Acyclic Subgraph is not solvable in polynomial time under ETH.
\end{proposition}

\begin{proof}
	We make a reduction for Max Acyclic Subgraph, which is not solvable in time $2^{o(n)}$ under ETH (from the linear reduction from vertex cover \cite{Kar72} and the hardness of vertex cover~\cite{ImpagliazzoPZ01}). Let $G=(V,E)$ be a directed graph. We build a graph $G'$ by adding to $G$ dummy vertices so that $G'$ has $N=2^{n/f(n)}$ vertices. We keep the arcs that are in $G$ but do not add any new arcs. Then the order in which we put dummy vertices does not change the objective function. So we know that there is an optimal solution such that $\sigma(i)=i$ for $i>n$ and $\sigma(i)\leq n$ for $i\leq n$, and there $\sigma$ restricted to $\{1,\dots,n\}$ is an optimal solution for $G$.
So we know that $|\sigma(i)-i|\leq n=f(n)\log N\leq f(N)\log N$.

Now, suppose that we can solve $f(n)\log n$-Positioned-Enhanced Max Acyclic Subgraph in polynomial time $O(N^c)$. Then we would solve Max Acyclic Subgraph in time $O(2^{cn/f(n)})=2^{o(n)}$ as $f$ is unbounded. This is impossible under ETH. 	
\end{proof}

Similar proofs can be easily derived for Min Linear Arrangement (same proof) and the single-machine sum of completion time problem considered in Section~\ref{sec:dec} (just add dummy jobs of processing time 0).


\begin{proposition}\label{prop:tsphard}
	For any unbounded increasing function $f$, $f(n)\log n$-Positioned-Enhanced TSP is not solvable in polynomial time under ETH.
\end{proposition}


\begin{proof}
The proof is similar to the one of Proposition~\ref{prop:hardloc}. This time, we add dummy vertices that are all at distance 0 from a given initial vertex $v$ (and all distances between them are 0), so that there are $N=2^{n/f(n)}$ vertices in total. Then we know that there exists an optimal solution in which these vertices are ranked last in the permutation (the solution starts with $v$), so we get as previously $f(N)\log N$-position enhancement ``for free''. We get the same conclusion, using the fact that TSP is not solvable in time $2^{o(n)}$ under ETH~\cite{ImpagliazzoPZ01}.
\end{proof}

\bibliography{bibliography}


\end{document}