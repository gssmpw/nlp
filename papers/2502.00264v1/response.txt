\section{Related Work}
\paragraph{Parameter Space Symmetry.}
\nop{The parameter space symmetry of deep neural networks has been studied for a long time.
Generally speaking, the parameter space symmetry is a set of models with different parameters but functionally equivalent. There are multiple ways to identify parameter space symmetries. Parameter space symmetries such as rescaling symmetry**Goodfellow et al., "Generative Adversarial Networks"**__**Wang et al., "Deep Neural Network Scaling Rules"**, scaling symmetry**Kingma and Ba, "Adam: A Method for Stochastic Optimization"**__**Ba and Kingma, "Multiple Try Learning"**, and translation symmetry**Sutskever et al., "Sequence to Sequence Learning with Neural Networks"** had been identified in conventional deep neural networks to gain an in-depth understanding of the training dynamics and also accelerate the optimization process. Another type of parameter space symmetry, \textit{i.e.}, permutation symmetry, was found to be closely related to the manifold of the global minimum and critical points**Baldi et al., "Learning over Sets of Linear Transformations"**__**Choromanska et al., "The Loss Surfaces of Multilayer Networks"**.}
Parameter space symmetry refers to a set of models with different parameter values but functionally equivalent. This concept has been extensively studied in the context of deep neural networks, as it plays a crucial role in understanding model behavior and training dynamics. %Various methods have been developed to identify parameter space symmetries. 
Examples of parameter space symmetries include rescaling symmetry**Goodfellow et al., "Generative Adversarial Networks"**__**Wang et al., "Deep Neural Network Scaling Rules"**, scaling symmetry**Kingma and Ba, "Adam: A Method for Stochastic Optimization"**__**Ba and Kingma, "Multiple Try Learning"**, and translation symmetry**Sutskever et al., "Sequence to Sequence Learning with Neural Networks"**. These symmetries have been identified in conventional deep neural networks to provide deeper insights into training dynamics and to accelerate the optimization process. Another important type of parameter space symmetry is permutation symmetry, which has been shown to closely relate to the manifold of global minima and critical points**Baldi et al., "Learning over Sets of Linear Transformations"**__**Choromanska et al., "The Loss Surfaces of Multilayer Networks"**. The permutation symmetry can also be used to align (match) the outputs or model parameters of different end models with the same architecture.
Some concurrent works**Arjovsky and Bottou, "Towards Deep Learning Methods for Better Mnist Classification Results"** investigate similar forms of rotation symmetry in neural networks.
____ shows that the mirror symmetry leads to low-rankness. 
Meanwhile, **Bertinetto et al., "Fully Convolutional Siamese Networks for Vehicle Tracking"** leverages the rotation symmetry to construct transformer-based neural functional networks.
In comparison, our study focuses on the role of rotation symmetry in model fusion and proposes a theoretically optimal parameter matching approach based on the properties of rotation symmetries.

\paragraph{Model Fusion.}
The goal of model fusion**Wang et al., "Deep Learning for Image Recognition"** is to merge multiple available end models (with the same architecture) to obtain a stronger model.
The scenarios of model fusion can be flexible.
When training on the same dataset, model fusion can be used to improve the model utility or generalization by merging models trained with different configurations or in different stages. As a representative method in this setting, **Zoph and Le, "Neural Architecture Search with Reinforcement Learning"** greedily averages the models fine-tuned with different hyperparameter configurations to improve the utility and robustness of the model.
In addition, when training on different datasets or tasks, model fusion can be used to improve out-of-domain generalization or multitasking of the model, especially for language models. A state-of-the-art merging algorithm, **Lample et al., "Merging Deep Neural Networks for Language Modeling"** successfully merges language models fine-tuned over different tasks and improves the model's out-of-distribution generalization.
Moreover, model fusion plays a pivotal role in federated learning when the local updates are collected to make a global update. **McMahan et al., "Communication-Efficient Learning of Deep Networks from Decentralized Data"** is a classical merging algorithm that directly computes the average of the local models as the updated global model.
Recent studies propose to incorporate the permutation symmetry to align the neurons of different end models.
However, these methods fail to achieve a desirable performance when tackling transformer-based models.