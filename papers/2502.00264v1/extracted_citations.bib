@inproceedings{ainsworth2023git,
  title={Git Re-Basin: Merging Models modulo Permutation Symmetries},
  author={Ainsworth, Samuel and Hayase, Jonathan and Srinivasa, Siddhartha},
  booktitle={International Conference on Learning Representations},
  year={2023}
}

@article{arpit2022ensemble,
  title={Ensemble of averages: Improving model selection and boosting performance in domain generalization},
  author={Arpit, Devansh and Wang, Huan and Zhou, Yingbo and Xiong, Caiming},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={8265--8277},
  year={2022}
}

@article{badrinarayanan2015symmetry,
  title={Symmetry-invariant optimization in deep networks},
  author={Badrinarayanan, Vijay and Mishra, Bamdev and Cipolla, Roberto},
  journal={arXiv preprint arXiv:1511.01754},
  year={2015}
}

@inproceedings{benton2021loss,
  title={Loss surface simplexes for mode connecting volumes and fast ensembling},
  author={Benton, Gregory and Maddox, Wesley and Lotfi, Sanae and Wilson, Andrew Gordon Gordon},
  booktitle={International Conference on Machine Learning},
  pages={769--779},
  year={2021}
}

@article{brea2019weight,
  title={Weight-space symmetry in deep networks gives rise to permutation saddles, connected by equal-loss valleys across the loss landscape},
  author={Brea, Johanni and Simsek, Berfin and Illing, Bernd and Gerstner, Wulfram},
  journal={arXiv preprint arXiv:1907.02911},
  year={2019}
}

@article{cha2021swad,
  title={Swad: Domain generalization by seeking flat minima},
  author={Cha, Junbum and Chun, Sanghyuk and Lee, Kyungjae and Cho, Han-Cheol and Park, Seunghyun and Lee, Yunsung and Park, Sungrae},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={22405--22418},
  year={2021}
}

@article{choshen2022fusing,
  title={Fusing finetuned models for better pretraining},
  author={Choshen, Leshem and Venezian, Elad and Slonim, Noam and Katz, Yoav},
  journal={arXiv preprint arXiv:2204.03044},
  year={2022}
}

@article{du2018algorithmic,
  title={Algorithmic regularization in learning deep homogeneous models: Layers are automatically balanced},
  author={Du, Simon S and Hu, Wei and Lee, Jason D},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@inproceedings{entezari2022role,
  title={The Role of Permutation Invariance in Linear Mode Connectivity of Neural Networks},
  author={Entezari, Rahim and Sedghi, Hanie and Saukh, Olga and Neyshabur, Behnam},
  booktitle={International Conference on Learning Representations},
  year={2022}
}

@article{fukumizu2000local,
  title={Local minima and plateaus in hierarchical structures of multilayer perceptrons},
  author={Fukumizu, Kenji and Amari, Shun-ichi},
  journal={Neural networks},
  volume={13},
  number={3},
  pages={317--327},
  year={2000}
}

@inproceedings{gupta2020stochastic,
  title={Stochastic Weight Averaging in Parallel: Large-Batch Training That Generalizes Well},
  author={Gupta, Vipul and Serrano, Santiago Akle and DeCoste, Dennis},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@inproceedings{imfeld2024transformer,
  title={Transformer Fusion with Optimal Transport},
  author={Imfeld, Moritz and Graldi, Jacopo and Giordano, Marco and Hofmann, Thomas and Anagnostidis, Sotiris and Singh, Sidak Pal},
  booktitle={International Conference on Learning Representations},
  year={2024}
}

@inproceedings{izmailov2018averaging,
  title={Averaging weights leads to wider optima and better generalization},
  author={Izmailov, Pavel and Podoprikhin, Dmitrii and Garipov, Timur and Vetrov, Dmitry and Wilson, Andrew Gordon},
  booktitle={34th Conference on Uncertainty in Artificial Intelligence (UAI)},
  pages={876--885},
  year={2018}
}

@inproceedings{jin2023dataless,
  title={Dataless Knowledge Fusion by Merging Weights of Language Models},
  author={Jin, Xisen and Ren, Xiang and Preotiuc-Pietro, Daniel and Cheng, Pengxiang},
  booktitle={International Conference on Learning Representations},
  year={2023}
}

@article{konevcny2016federated,
  title={Federated learning: Strategies for improving communication efficiency},
  author={Kone{\v{c}}n{\`y}, Jakub and McMahan, H Brendan and Yu, Felix X and Richt{\'a}rik, Peter and Suresh, Ananda Theertha and Bacon, Dave},
  journal={arXiv preprint arXiv:1610.05492},
  year={2016}
}

@inproceedings{kunin2021neural,
  title={Neural Mechanics: Symmetry and Broken Conservation Laws in Deep Learning Dynamics},
  author={Kunin, Daniel and Sagastuy-Brena, Javier and Ganguli, Surya and Yamins, Daniel LK and Tanaka, Hidenori},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@inproceedings{li2022branch,
  title={Branch-Train-Merge: Embarrassingly Parallel Training of Expert Language Models},
  author={Li, Margaret and Gururangan, Suchin and Dettmers, Tim and Lewis, Mike and Althoff, Tim and Smith, Noah A and Zettlemoyer, Luke},
  booktitle={First Workshop on Interpolation Regularizers and Beyond at NeurIPS 2022},
  year={2022}
}

@article{li2023deep,
  title={Deep model fusion: A survey},
  author={Li, Weishi and Peng, Yong and Zhang, Miao and Ding, Liang and Hu, Han and Shen, Li},
  journal={arXiv preprint arXiv:2309.15698},
  year={2023}
}

@article{matena2022merging,
  title={Merging models with fisher-weighted averaging},
  author={Matena, Michael S and Raffel, Colin A},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={17703--17716},
  year={2022}
}

@inproceedings{mcmahan2017communication,
  title={Communication-efficient learning of deep networks from decentralized data},
  author={McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Aguera},
  booktitle={Artificial intelligence and statistics},
  pages={1273--1282},
  year={2017}
}

@inproceedings{meng2019g,
  title={G-SGD: Optimizing ReLU Neural Networks in its Positively Scale-Invariant Space},
  author={Meng, Qi and Zheng, Shuxin and Zhang, Huishuai and Chen, Wei and Ye, Qiwei and Ma, Zhi-Ming and Yu, Nenghai and Liu, Tie-Yan},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@article{neyshabur2015path,
  title={Path-sgd: Path-normalized optimization in deep neural networks},
  author={Neyshabur, Behnam and Salakhutdinov, Russ R and Srebro, Nati},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}

@article{rame2022diverse,
  title={Diverse weight averaging for out-of-distribution generalization},
  author={Rame, Alexandre and Kirchmeyer, Matthieu and Rahier, Thibaud and Rakotomamonjy, Alain and Gallinari, Patrick and Cord, Matthieu},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={10821--10836},
  year={2022}
}

@inproceedings{simsek2021geometry,
  title={Geometry of the loss landscape in overparameterized neural networks: Symmetries and invariances},
  author={Simsek, Berfin and Ged, Fran{\c{c}}ois and Jacot, Arthur and Spadaro, Francesco and Hongler, Cl{\'e}ment and Gerstner, Wulfram and Brea, Johanni},
  booktitle={International Conference on Machine Learning},
  pages={9722--9732},
  year={2021}
}

@article{singh2020model,
  title={Model fusion via optimal transport},
  author={Singh, Sidak Pal and Jaggi, Martin},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={22045--22055},
  year={2020}
}

@article{tran2024equivariant,
  title={Equivariant Neural Functional Networks for Transformers},
  author={Tran, Viet-Hoang and Vo, Thieu N and The, An Nguyen and Huu, Tho Tran and Nguyen-Nhat, Minh-Khoi and Tran, Thanh and Pham, Duy-Tung and Nguyen, Tan Minh},
  journal={arXiv preprint arXiv:2410.04209},
  year={2024}
}

@inproceedings{wang2020federated,
  title={Federated Learning with Matched Averaging},
  author={Wang, Hongyi and Yurochkin, Mikhail and Sun, Yuekai and Papailiopoulos, Dimitris and Khazaeni, Yasaman},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@inproceedings{wortsman2022model,
  title={Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time},
  author={Wortsman, Mitchell and Ilharco, Gabriel and Gadre, Samir Ya and Roelofs, Rebecca and Gontijo-Lopes, Raphael and Morcos, Ari S and Namkoong, Hongseok and Farhadi, Ali and Carmon, Yair and Kornblith, Simon and others},
  booktitle={International conference on machine learning},
  pages={23965--23998},
  year={2022}
}

@article{zhao2022symmetry,
  title={Symmetry teleportation for accelerated optimization},
  author={Zhao, Bo and Dehmamy, Nima and Walters, Robin and Yu, Rose},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={16679--16690},
  year={2022}
}

@inproceedings{zhao2023symmetries,
  title={Symmetries, Flat Minima, and the Conserved Quantities of Gradient Flow},
  author={Zhao, Bo and Ganev, Iordan and Walters, Robin and Yu, Rose and Dehmamy, Nima},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2023}
}

@inproceedings{zhao2024improving,
  title={Improving Convergence and Generalization Using Parameter Symmetries},
  author={Zhao, Bo and Gower, Robert M and Walters, Robin and Yu, Rose},
  booktitle={International Conference on Learning Representations},
  year={2024}
}

@inproceedings{ziyin2024symmetry,
  title={Symmetry Induces Structure and Constraint of Learning},
  author={Liu, Ziyin},
  booktitle={International Conference on Machine Learning},
  year={2024}
}

