\section{Related Work}
\paragraph{Parameter Space Symmetry.}
\nop{The parameter space symmetry of deep neural networks has been studied for a long time.
Generally speaking, the parameter space symmetry is a set of models with different parameters but functionally equivalent. There are multiple ways to identify parameter space symmetries. Parameter space symmetries such as rescaling symmetry____, scaling symmetry____, and translation symmetry____ had been identified in conventional deep neural networks to gain an in-depth understanding of the training dynamics and also accelerate the optimization process____. Another type of parameter space symmetry, \textit{i.e.}, permutation symmetry, was found to be closely related to the manifold of the global minimum and critical points____.}
Parameter space symmetry refers to a set of models with different parameter values but functionally equivalent. This concept has been extensively studied in the context of deep neural networks, as it plays a crucial role in understanding model behavior and training dynamics. %Various methods have been developed to identify parameter space symmetries. 
Examples of parameter space symmetries include rescaling symmetry____, scaling symmetry____, and translation symmetry____. These symmetries have been identified in conventional deep neural networks to provide deeper insights into training dynamics and to accelerate the optimization process____. Another important type of parameter space symmetry is permutation symmetry, which has been shown to closely relate to the manifold of global minima and critical points____. The permutation symmetry can also be used to align (match) the outputs or model parameters of different end models with the same architecture____.
Some concurrent works____ investigate similar forms of rotation symmetry in neural networks.
____ shows that the mirror symmetry leads to low-rankness. 
Meanwhile, ____ leverages the rotation symmetry to construct transformer-based neural functional networks.
In comparison, our study focuses on the role of rotation symmetry in model fusion and proposes a theoretically optimal parameter matching approach based on the properties of rotation symmetries.

\paragraph{Model Fusion.}
The goal of model fusion____ is to merge multiple available end models (with the same architecture) to obtain a stronger model.
The scenarios of model fusion can be flexible.
When training on the same dataset, model fusion can be used to improve the model utility or generalization by merging models trained with different configurations or in different stages____.
As a representative method in this setting, ModelSoup____ greedily averages the models fine-tuned with different hyperparameter configurations to improve the utility and robustness of the model.
In addition, when training on different datasets or tasks, model fusion can be used to improve out-of-domain generalization or multitasking of the model____, especially for language models.
A state-of-the-art merging algorithm, RegMean____, successfully merges language models fine-tuned over different tasks and improves the model's out-of-distribution generalization.
Moreover, model fusion plays a pivotal role in federated learning____ when the local updates are collected to make a global update.
FedAvg____ is a classical merging algorithm that directly computes the average of the local models as the updated global model.
Recent studies propose to incorporate the permutation symmetry to align the neurons of different end models____.
However, these methods fail to achieve a desirable performance when tackling transformer-based models____.