\section{Related Work}
\paragraph{Parameter Space Symmetry.}
\nop{The parameter space symmetry of deep neural networks has been studied for a long time.
Generally speaking, the parameter space symmetry is a set of models with different parameters but functionally equivalent. There are multiple ways to identify parameter space symmetries. Parameter space symmetries such as rescaling symmetry~\citep{neyshabur2015path,badrinarayanan2015symmetry,du2018algorithmic,meng2019g}, scaling symmetry~\citep{kunin2021neural}, and translation symmetry~\citep{kunin2021neural} had been identified in conventional deep neural networks to gain an in-depth understanding of the training dynamics and also accelerate the optimization process~\citep{zhao2022symmetry,zhao2023symmetries,zhao2024improving}. Another type of parameter space symmetry, \textit{i.e.}, permutation symmetry, was found to be closely related to the manifold of the global minimum and critical points~\citep{fukumizu2000local,brea2019weight,simsek2021geometry,benton2021loss,entezari2022role,ainsworth2023git}.}
Parameter space symmetry refers to a set of models with different parameter values but functionally equivalent. This concept has been extensively studied in the context of deep neural networks, as it plays a crucial role in understanding model behavior and training dynamics. %Various methods have been developed to identify parameter space symmetries. 
Examples of parameter space symmetries include rescaling symmetry~\citep{neyshabur2015path,badrinarayanan2015symmetry,du2018algorithmic,meng2019g}, scaling symmetry~\citep{kunin2021neural}, and translation symmetry~\citep{kunin2021neural}. These symmetries have been identified in conventional deep neural networks to provide deeper insights into training dynamics and to accelerate the optimization process~\citep{zhao2022symmetry,zhao2023symmetries,zhao2024improving}. Another important type of parameter space symmetry is permutation symmetry, which has been shown to closely relate to the manifold of global minima and critical points~\citep{fukumizu2000local,brea2019weight,simsek2021geometry,benton2021loss,entezari2022role,ainsworth2023git}. The permutation symmetry can also be used to align (match) the outputs or model parameters of different end models with the same architecture~\citep{singh2020model,wang2020federated,ainsworth2023git,imfeld2024transformer}.
Some concurrent works~\citep{ziyin2024symmetry,tran2024equivariant} investigate similar forms of rotation symmetry in neural networks.
\citet{ziyin2024symmetry} shows that the mirror symmetry leads to low-rankness. 
Meanwhile, \citet{tran2024equivariant} leverages the rotation symmetry to construct transformer-based neural functional networks.
In comparison, our study focuses on the role of rotation symmetry in model fusion and proposes a theoretically optimal parameter matching approach based on the properties of rotation symmetries.

\paragraph{Model Fusion.}
The goal of model fusion~\citep{li2023deep} is to merge multiple available end models (with the same architecture) to obtain a stronger model.
The scenarios of model fusion can be flexible.
When training on the same dataset, model fusion can be used to improve the model utility or generalization by merging models trained with different configurations or in different stages~\citep{izmailov2018averaging,gupta2020stochastic,cha2021swad,wortsman2022model,rame2022diverse,arpit2022ensemble}.
As a representative method in this setting, ModelSoup~\citep{wortsman2022model} greedily averages the models fine-tuned with different hyperparameter configurations to improve the utility and robustness of the model.
In addition, when training on different datasets or tasks, model fusion can be used to improve out-of-domain generalization or multitasking of the model~\citep{matena2022merging,choshen2022fusing,li2022branch,jin2023dataless}, especially for language models.
A state-of-the-art merging algorithm, RegMean~\citep{jin2023dataless}, successfully merges language models fine-tuned over different tasks and improves the model's out-of-distribution generalization.
Moreover, model fusion plays a pivotal role in federated learning~\citep{konevcny2016federated,mcmahan2017communication,wang2020federated} when the local updates are collected to make a global update.
FedAvg~\citep{mcmahan2017communication} is a classical merging algorithm that directly computes the average of the local models as the updated global model.
Recent studies propose to incorporate the permutation symmetry to align the neurons of different end models~\citep{wang2020federated,singh2020model,ainsworth2023git}.
However, these methods fail to achieve a desirable performance when tackling transformer-based models~\citep{jin2023dataless}.