%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{enumitem}
\usepackage{bm}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


\newcommand{\nop}[1]{}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Beyond the Permutation Symmetry of Transformers: The Role of Rotation for Model Fusion}

\begin{document}

\twocolumn[
\icmltitle{Beyond the Permutation Symmetry of Transformers:\\The Role of Rotation for Model Fusion}
% Beyond Permutation: Exploring Rotation Symmetry in Transformer Parameter Space for Enhanced Model Fusion

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Binchi Zhang}{equal,uva}
\icmlauthor{Zaiyi Zheng}{equal,uva}
\icmlauthor{Zhengzhang Chen}{nec}
\icmlauthor{Jundong Li}{uva}
\end{icmlauthorlist}

\icmlaffiliation{uva}{University of Virginia}
\icmlaffiliation{nec}{NEC Laboratories America}

\icmlcorrespondingauthor{Jundong Li}{jundong@virginia.edu}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

% \printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
% % \nop{Symmetry in the parameter space of deep neural networks (DNNs) has proven useful in many deep learning applications.
% % A well-known example is permutation symmetry in Multi-Layer Perceptrons (MLPs), where permuting the rows of weight matrices in one layer and applying the corresponding inverse permutation to adjacent layers results in a functionally equivalent model.
% % Although the permutation symmetry comprehensively explores the equivalence set for MLPs, it fails to do so for transformers due to the discrete nature of permutation.}
% Symmetry in the parameter space of deep neural networks (DNNs) has shown significant utility across various deep learning applications. 
% % In Multi-Layer Perceptrons (MLPs), for example, permuting the rows of weight matrices in one layer and applying the inverse permutation to adjacent layers yields a functionally equivalent model. 
% A well-known example in Multi-Layer Perceptrons (MLPs) is permutation symmetry, where permuting the rows of weight matrices in one layer and applying the inverse permutation to adjacent layers results in a functionally equivalent model.
% Although the permutation symmetry comprehensively characterizes the equivalence set for MLPs, \textit{its discrete nature} constrains its effectiveness for transformers. 
% % While permutation symmetry has been extensively explored in neural networks such as MLPs and Convolutional Neural Networks (CNNs), its study in transformers remains nascent.
% % In this paper, we move beyond permutation symmetry and introduce rotation symmetry, a novel type of parameter space symmetry for transformers. 
% % Based on this new insight, we propose an optimal parameter matching algorithm, designed as a plug-and-play module to facilitate model fusion.
% In this paper, we introduce \textbf{rotation symmetry}, a novel form of parameter space symmetry for transformers that generalizes permutation symmetry by rotating the parameter matrices in self-attention layers. Unlike permutation symmetry, rotation symmetry operates in \textit{a continuous domain}, thereby significantly expanding the equivalence set for transformers. Based on rotation symmetry, we further propose a theoretically optimal parameter matching algorithm that serves as a plug-and-play module to enhance model fusion. We evaluate our approach using pre-trained transformers across multiple Natural Language Processing (NLP) and vision tasks. Experimental results demonstrate that our rotation symmetry-based matching algorithm substantially improves model fusion, highlighting the potential of parameter space symmetry to facilitate model fusion.
% \nop{In this paper, we move beyond permutation symmetry and introduce rotation symmetry, a novel type of parameter space symmetry for transformers by rotating the parameter matrices in self-attention layers. 
% Rotation symmetry extends the permutation symmetry into a continuous domain, significantly expanding the equivalence set for transformers.
% More importantly, we propose a theoretically optimal parameter matching algorithm based on rotation symmetry, as a plug-and-play module to facilitate model fusion.
% We evaluate our method based on pre-trained transformers with multiple Natural Language Processing (NLP) and vision tasks.
% Experimental results demonstrate that our rotation symmetry-based matching algorithm greatly improves model fusion and highlights the potential of parameter space symmetry in advancing model fusion.}
% This study provides a novel understanding of symmetry in transformer parameter space and offers a practical method to enhance model fusion, paving the way for further exploration.
Symmetry in the parameter space of deep neural networks (DNNs) has proven beneficial for various deep learning applications. 
A well-known example is the permutation symmetry in Multi-Layer Perceptrons (MLPs), where permuting the rows of weight matrices in one layer and applying the inverse permutation to adjacent layers yields a functionally equivalent model. 
While permutation symmetry fully characterizes the equivalence set for MLPs, its discrete nature limits its utility for transformers.
In this paper, we introduce rotation symmetry, a novel form of parameter space symmetry for transformers that generalizes permutation symmetry by rotating parameter matrices in self-attention layers. 
Unlike permutation symmetry, rotation symmetry operates in a continuous domain, thereby significantly expanding the equivalence set for transformers. 
Based on this property, we propose a theoretically optimal parameter matching algorithm as a plug-and-play module to enhance model fusion. 
We evaluate our approach using pre-trained transformers across diverse natural language and vision tasks. 
Experimental results demonstrate that our rotation symmetry-based matching algorithm substantially improves model fusion, highlighting the potential of parameter space symmetry to facilitate model fusion.
Our code is available on 
% \url{https://anonymous.4open.science/r/TransformerPermutation-8800}.
\url{https://github.com/zhengzaiyi/RotationSymmetry}.
\end{abstract}

\section{Introduction}
Parameter space symmetry is an intriguing property of neural networks that has garnered increasing attention in recent years~\citep{du2018algorithmic,armenta2021representation,kunin2021neural,simsek2021geometry,entezari2022role,grigsby2023hidden}. 
One of the most studied forms of parameter space symmetry is permutation symmetry~\citep{ainsworth2023git,entezari2022role}. 
For instance, in a two-layer MLP, permuting the rows of the weight matrix in the first layer and applying the corresponding inverse permutation to the second layer results in a functionally equivalent model, \textit{i.e.}, the outputs of the original and permuted models remain identical for any given input~\citep{ainsworth2023git}.
All functionally equivalent models corresponding to weight permutations form an equivalence set, which provides theoretical insights into neural network optimization, such as the linear mode connectivity of loss landscapes~\citep{entezari2022role,zhou2023going}. 
In addition, permutation symmetry has also proven helpful in advancing neural network applications, such as model fusion~\citep{singh2020model,ainsworth2023git} and optimization~\citep{zhao2024improving}.

Although parameter space symmetry has been extensively studied in classical neural network architectures, such as MLPs and CNNs, the understanding of its application in transformers~\citep{vaswani2017attention} remains limited. 
% unexplored.
Transformers have seen rapid advancements in recent years, achieving remarkable success in a wide range of applications~\citep{yun2019graph,lewis2020bart,raffel2020exploring,clark2020electra,liu2021swin,dosovitskiy2021imageworth16x16words,zhou2021informer,he2024explaining,zhu2024understanding,zheng2024kg}.
The transformer architecture is built upon two primary submodules: feedforward networks and self-attention layers. 
% The feedforward network, which is structurally similar to MLPs, naturally inherits the permutation symmetry studied in existing literature. 
The feedforward network, which is structurally similar to MLPs, naturally inherits the permutation symmetry that has been extensively studied in the existing literature. 
% In contrast, the self-attention layer, with its unique attention mechanism, introduces challenges that existing parameter space symmetries cannot address. 
Self-attention layers, on the other hand, involve a unique attention mechanism powered by matrix products of queries, keys, and values, which introduce additional potentials for symmetry beyond permutations. 
Permutation symmetries limit the equivalence set of neural networks to \textit{discrete operations}, which aligns well with MLPs due to their element-wise activations (\textit{e.g.}, ReLU~\citep{glorot2011deep}). 
In contrast, \textit{the continuous nature} of the matrix operations in self-attention layers necessitates more flexible operations to fully characterize their equivalence set.

In this paper, we introduce \textbf{rotation symmetry}, a novel form of parameter space symmetry for self-attention layers in transformers. 
Specifically, %we jointly consider the query and key matrices and show that applying a rotation to the query matrix, followed by the corresponding inverse rotation to the key matrix, preserves the query-key product. 
we analyze the query and key matrices jointly and demonstrate that applying a rotation to the query matrix, followed by the corresponding inverse rotation to the key matrix, preserves the query-key product. 
Additionally, we find that the same rotation rule can be applied to the value and output matrices.
%Similarly, we observe that the same rotation rule applies to the value and output matrices.
Our findings provide novel insights into the functional invariance of the attention mechanism and extend the permutation symmetry~\citep{singh2020model,wang2020federated,tatro2020optimizing,entezari2022role,ainsworth2023git,imfeld2024transformer} from discrete spaces to continuous spaces, which significantly extends the scope of parameter space symmetries for transformers. %significantly extending the scope of parameter space symmetries for transformers.

%To further demonstrate the benefits of our proposed rotation symmetry for transformers, we focus on the utility of parameter space symmetry in \textbf{model fusion}.
To further demonstrate the benefits of rotation symmetry for transformers, we explore its utility in \textbf{model fusion}. 
The goal of model fusion is to merge multiple well-trained end models in the parameter space to %achieve an improved overall utility with a single merged model.
produce a single merged model with improved overall utility.
Model fusion is widely adopted across various settings, such as hyperparameter tuning (where end models are trained on the same benchmark) and multi-task learning (where end models are trained with different tasks)~\citep{jin2023dataless}.
Unlike ensemble learning~\citep{dietterich2000ensemble,lakshminarayanan2017simple,sagi2018ensemble,dong2020survey}, model fusion can work in a data-agnostic manner, %making it feasible for privacy-sensitive scenarios 
making it suitable for privacy-sensitive scenarios such as federated learning~\citep{yurochkin2019bayesian,wang2020federated}.

The existing literature has demonstrated that the performance of model fusion is closely tied to the distance between the end models~\citep{wortsman2022model}.
Inspired by this finding, we propose a parameter matching algorithm that selects functionally equivalent end models from the equivalence class determined by rotation symmetry.
This approach ensures that the selected representative models are closer in parameter space, resulting in a smaller inner distance.
To achieve this, we formulate the problem of parameter matching as an optimization problem with orthogonal constraints.
%Relying on the continuous nature of rotation symmetry, we propose a closed-form solution to this problem.
Leveraging the continuous nature of rotation symmetry, we propose a closed-form solution to this problem. %Additionally, our parameter matching algorithm is efficient, easy to implement, and can be seamlessly integrated as a plug-and-play module for model fusion.
Our parameter matching algorithm is highly efficient, easy to implement, and can be seamlessly incorporated as a plug-and-play module for model fusion. 
To evaluate its effectiveness, we conduct extensive experiments with pre-trained transformers on real-world NLP and vision tasks. 
The experimental results demonstrate that incorporating rotation symmetry into parameter matching improves model fusion effectively and efficiently. 
Furthermore, additional experiments reveal that %even matching a subset of parameters can yield distinct performance gains in model fusion, highlighting the practical utility of our approach.
even matching a subset of parameters can lead to notable performance improvements, highlighting the practical utility of our approach. 
Our contributions are threefold:
\begin{itemize}[leftmargin=*]
\item We introduce a novel rotation symmetry for the attention mechanism in transformers, extending the concept of symmetry to a continuous space.
\item %Leveraging rotation symmetry, we propose a theoretically optimal parameter matching algorithm designed to enhance the effectiveness of model fusion for transformers.
Building on rotation symmetry, we propose a theoretically optimal parameter matching algorithm that improves the effectiveness of model fusion in transformers.
\item %We conduct extensive experiments to verify the efficacy of our proposed parameter matching algorithm, highlighting its potential to advance model fusion through parameter space symmetry.
Through extensive experiments, we validate the efficacy of our proposed parameter matching algorithm, demonstrating its potential to advance model fusion through parameter space symmetry.
\end{itemize}

\section{Preliminaries}
\nop{We first define some notations for further discussion.
Let $\bm{A}[i,:]$, $\bm{A}[:,j]$, and $\bm{A}[i,j]$ denote the $i$-th row vector, $j$-th column vector, and the $(i,j)$ element of the matrix $\bm{A}$, respectively.
Let $\bm{1}$ denote the all-ones vector.
We introduce the permutation symmetry of neural networks using MLP as an example. 
We first consider an $L$-layer MLP model}

To facilitate further discussion, we begin by defining the notations. 
Let $\bm{A}[i,:]$, $\bm{A}[:,j]$, and $\bm{A}[i,j]$ denote the $i$-th row vector, the $j$-th column vector, and the $(i,j)$-th element of the matrix $\bm{A}$, respectively. Additionally, let $\bm{1}$ denote an all-ones vector. We now illustrate the concept of permutation symmetry in neural networks, using an MLP as an example. Consider an $L$-layer MLP model defined as:
\begin{equation}\label{eq:mlp}
f_{\bm{W}}(\bm{X})=\bm{Z}^{(L)},\ \bm{Z}^{(l)}=\sigma(\bm{Z}^{(l-1)}(\bm{W}^{(l)})^\top+\bm{b}^{(l)}),
\end{equation}
where $\bm{Z}^{(0)}=\bm{X}$ is the input feature matrix, $\bm{b}^{(l)}$ is the bias vector ($\bm{b}^{(l)}$ is a row vector and can be seen as broadcast over all rows in \Cref{eq:mlp}), $\bm{W}=\{\bm{W}^{(l)},\bm{b}^{(l)}\}_{l=1,\dots,L}$ collects all learnable parameters, and $\sigma(\cdot)$ stands for a non-linear activation function, such as ReLU~\citep{nair2010rectified}. A loss function $\mathcal{L}(f_{\bm{W}}(\bm{X}),\bm{Y})$ is used to measure the distance between the model prediction and the ground truth label $\bm{Y}$. 

%We then consider the permutation symmetry of the MLP model.
To analyze the permutation symmetry in the MLP model, let $\bm{P}\in\mathcal{P}$ be a permutation matrix, where $\bm{P}[i,j]\in\{0,1\}$ and $\bm{P}[i,:]\bm{1}=\bm{P}[:,j]^\top\bm{1}=1$ for any $i$, $j$.
%All permutation matrices are orthogonal matrices~\citep{strang1976linear}, \textit{i.e.}, $\bm{P}^\top=\bm{P}^{-1}$.
All permutation matrices are orthogonal~\citep{strang1976linear}, satisfying $\bm{P}^\top = \bm{P}^{-1}$.
Consequently, for layer $l$ and $l+1$, we have:
\begin{equation}\label{eq:permutation_symmetry}
\scriptsize
\begin{aligned}
\bm{Z}^{(l+1)}&=\sigma\left(\sigma(\bm{Z}^{(l-1)}(\bm{W}^{(l)})^\top+\bm{b}^{(l)})(\bm{W}^{(l+1)})^\top+\bm{b}^{(l+1)}\right) \\
&=\sigma\left(\sigma(\bm{Z}^{(l-1)}(\bm{P}^\top\bm{W}^{(l)})^\top+\bm{b}^{(l)}\bm{P})(\bm{W}^{(l+1)}\bm{P})^\top+\bm{b}^{(l+1)}\right).
\end{aligned}
\end{equation}
The third equal sign holds because the element-wise activation function $\sigma$ is decoupled from column permutation (\textit{i.e.}, being multiplied by $\bm{P}$).
%Based on \Cref{eq:permutation_symmetry}, we can obtain that for layers $l$ and $l+1$, the mapping 
Based on \Cref{eq:permutation_symmetry}, it follows that for layers $l$ and $l+1$, the mappings
$\bm{W}^{(l)}\rightarrow\bm{P}^\top\bm{W}^{(l)}$, $\bm{b}^{(l)}\rightarrow\bm{b}^{(l)}\bm{P}$, $\bm{W}^{(l+1)}\rightarrow\bm{W}^{(l+1)}\bm{P}$ preserves the output $\bm{Z}^{(l+1)}$ for any input $\bm{Z}^{(l-1)}$.
%For every two layers, a similar mapping independently exists based on a specific permutation matrix, denoted as 
For each pair of adjacent layers, a similar mapping exists independently based on a specific permutation matrix, denoted as $\bm{P}^{(l)}$ for layers $l$ and $l+1$. %In total, we have a mapping that preserves the prediction of the whole $L$-layer MLP for any input data $\bm{X}$: 
Consequently, for the entire $L$-layer MLP, there exists a mapping that preserves the model's predictions for any input $\bm{X}$:
$\bm{W}^{(l)}\rightarrow(\bm{P}^{(l)})^\top\bm{W}^{(l)}\bm{P}^{(l-1)}$, $\bm{b}^{(l)}\rightarrow\bm{b}^{(l)}\bm{P}^{(l)}$.
%Put another way of thinking; if we let 
Equivalently, if we define
$\bm{W}^\prime=\{(\bm{P}^{(l)})^\top\bm{W}^{(l)}\bm{P}^{(l-1)},\bm{b}^{(l)}\bm{P}^{(l)}\}_{l=1,\dots,L}$ for any $\bm{P}^{(l)}\in\mathcal{P}$ ($l=1,\dots,L-1$) and $\bm{P}^{(0)}=\bm{P}^{(L)}=\bm{I}$, then we have $f_{\bm{W}^\prime}(\bm{X})=f_{\bm{W}}(\bm{X})$ for any $\bm{X}$.
This phenomenon is referred to as the \textit{permutation symmetry} of the parameter space~\citep{godfrey2022symmetries,hecht1990algebraic,navon2023equivariant,rossi2023permutation,simsek2021geometry}.
%Using the permutation symmetry, we are able to find an equivalence class of the functionally equivalent model parameters, which is known as
Leveraging permutation symmetry allows us to identify an equivalence class of functionally equivalent model parameters, which is known as \textit{permutation invariance}~\citep{ainsworth2023git,entezari2022role,lubana2023mechanistic}. %We let $\pi$ denote the corresponding equivalence relation determined by the permutation invariance, \textit{e.g.}, $\bm{W}^\prime=\pi(\bm{W})$ for the aforementioned original parameters $\bm{W}$ and permuted parameters $\bm{W}^\prime$.
We denote the equivalence relation induced by permutation invariance as $\pi$, where, for example, $\bm{W}^\prime = \pi(\bm{W})$ represents the equivalence between the original parameters $\bm{W}$ and the permuted parameters $\bm{W}^\prime$.

\section{Parameter Space Symmetry of Transformers}

\nop{Transformers~\citep{vaswani2017attention} have revolutionized the field of deep learning by offering a distinctive architecture that excels in handling sequential data, particularly in natural language processing (NLP) and beyond~\citep{brown2020language,devlin2019bert,liu2021swin,radford2021learning}. 
The power of transformers derives from its two main components\footnote{We follow the architecture in the original transformer paper~\citep{vaswani2017attention}.}, feedforward networks and self-attention layers~\citep{vaswani2017attention}.
To investigate the parameter space symmetry of transformers, we next focus on the two key modules separately.}

Transformers~\citep{vaswani2017attention} have revolutionized deep learning with their ability to handle sequential data effectively, particularly in natural language processing (NLP) and other fields~\citep{brown2020language,devlin2019bert,liu2021swin,radford2021learning}. Their success is primarily driven by two key components\footnote{We follow the architecture in the original transformer paper~\citep{vaswani2017attention}.}: feedforward networks and self-attention layers~\citep{vaswani2017attention}. To better understand the parameter space symmetry of transformers, we examine these two core modules individually in the following sections.

\subsection{Permutation Symmetry of Feedforward Networks}
We first look at the feedforward networks.
The feedforward network adopted in the transformer blocks is a two-layer MLP model which can be written as
\begin{equation}\label{eq:ffn}
\small
FFN(\bm{X})=LN(ReLU(\bm{X}\bm{W}_i^\top+\bm{b}_i)\bm{W}_o^\top+\bm{b}_o+\bm{X}),
\end{equation}
where $LN$ denotes the Layer Normalization operator~\citep{ba2016layer}.
Different from~\cite{vaswani2017attention}, we include the residual connection~\citep{he2016deep} and layer normalization modules into the formula of the feedforward network (and the self-attention layer mentioned later).
According to \Cref{eq:permutation_symmetry} and the analysis in Preliminaries, the feedforward networks have the permutation symmetry property and the equivalence class determined by permutation invariance is defined as
\begin{equation}\label{eq:match_ffn}
% \small
\bm{W}_i\rightarrow\bm{P}^\top\bm{W}_i,\ \bm{b}_i\rightarrow\bm{b}_i\bm{P},\ \bm{W}_o\rightarrow\bm{W}_o\bm{P},\ \bm{b}_o\rightarrow\bm{b}_o,
\end{equation}
where $\bm{P}\in\mathcal{P}$ is a permutation matrix. 
It is worth noting that the permutations of different feedforward networks in a transformer are independent due to the scalable modular design.
Consequently, we are able to flexibly compute the permutation invariance equivalence class of each $FFN$ module in a transformer model.

\subsection{Rotation Symmetry of Self-attention Layers}
We then focus on the self-attention layers.
In this paper, we introduce the \textit{rotation symmetry}%~\footnote{This type of symmetry is referred to as double rotation symmetry in~\citep{ziyin2024symmetry}. For simplicity, we use the term rotation symmetry throughout this paper.} 
of self-attention layers.
For MLPs, we have to switch the order of multiplying $\bm{P}$ and passing the activation function $\sigma$ (the third equal sign in \Cref{eq:permutation_symmetry}), requiring the matrix $\bm{P}$ to be a permutation matrix.
In contrast, the self-attention layer does not contain an element-wise activation function, enabling a wider range of the matrix $\bm{P}$ in self-attention layers.
The self-attention layer can be written as
% \begin{equation}\label{eq:attn}
% \scriptsize
% \begin{aligned}
% ATTN(\bm{X})&=LN\left(Cat_{h=1}^H\left\{\bm{X}_{QKV}^h\right\}\bm{W}_O^\top+\bm{b}_O+\bm{X}\right), \\
% \bm{X}_{QKV}^h&=S\left(\frac{\left(\bm{X}(\bm{W}_Q^h)^\top+\bm{b}_Q^h\right)\left(\bm{X}(\bm{W}_K^h)^\top+\bm{b}_K^h\right)^\top}{\sqrt{d}}\right) \\
% &\quad\quad\quad\quad\cdot\left(\bm{X}(\bm{W}_V^h)^\top+\bm{b}_V^h\right),
% \end{aligned}
% \end{equation}
$$
\small
\begin{aligned}
ATTN(\bm{X})&=LN\left(Cat_{h=1}^H\left\{\bm{X}_{QKV}^h\right\}\bm{W}_O^\top+\bm{b}_O+\bm{X}\right), \\
\bm{X}_{QKV}^h&=Sftmx\left(\bm{X}_{Q}^h(\bm{X}_{K}^h)^\top/\sqrt{d_k}\right)\cdot\bm{X}_{V}^h,
\end{aligned}
$$
where $Cat$ stands for the operator concatenating the outputs of multi-head attention, $Sftmx(\cdot)$ denotes the softmax operator, $H$ denotes the number of multi-heads, and the subscripts $Q$, $K$, $V$, and $O$ denote Query, Key, Value, and Output, respectively.

We first transform the query and key matrices as 
$$
\scriptsize
\begin{aligned}
\bm{X}_{Q}^h(\bm{X}_{K}^h)^\top&=(\bm{X}(\bm{W}_Q^h)^\top+\bm{b}_Q^h)(\bm{X}(\bm{W}_K^h)^\top+\bm{b}_K^h)^\top \\
&=(\bm{X}(\bm{W}_Q^h)^\top+\bm{b}_Q^h)\bm{R}\bm{R}^\top(\bm{X}(\bm{W}_K^h)^\top+\bm{b}_K^h)^\top \\
&=(\bm{X}(\bm{R}^\top\bm{W}_Q^h)^\top+\bm{b}_Q^h\bm{R})(\bm{X}(\bm{R}^\top\bm{W}_K^h)^\top+\bm{b}_K^h\bm{R})^\top,
\end{aligned}
$$
where $\bm{R}$ is a rotation matrix, \textit{i.e.}, $\bm{R}\bm{R}^\top=\bm{I}$. It is worth noting that each multi-head corresponds to a specific rotation matrix $\bm{R}$.
Let $\bm{W}_O=[\bm{W}_O^1\ \bm{W}_O^2\ \cdots\ \bm{W}_O^H]$ and we then rewrite the concatenating operation (with the product by $\bm{W}_O$) as $\sum_{h=1}^HS(\cdots)(\bm{X}(\bm{W}_V^h)^\top+\bm{b}_V^h)(\bm{W}_O^h)^\top$.
Similar to the Q-K case, we can transform the value and output matrices as $(\bm{X}(\bm{W}_V^h)^\top+\bm{b}_V^h)(\bm{W}_O^h)^\top=(\bm{X}(\bm{W}_V^h)^\top+\bm{b}_V^h)\bm{R}\bm{R}^\top(\bm{W}_O^h)^\top=(\bm{X}(\bm{R}^\top\bm{W}_V^h)^\top+\bm{b}_V^h\bm{R})(\bm{W}_O^h\bm{R})^\top$ for each multi-head $h$, where $\bm{R}$ is a rotation matrix.
Finally, we derive an equivalence class of the parameters in a self-attention layer determined by rotation invariance as
\begin{equation}\label{eq:match_attn}
\small
\begin{aligned}
&\bm{W}_Q^h\rightarrow(\bm{R}_{qk}^h)^\top\bm{W}_Q^h,\ \bm{b}_Q^h\rightarrow\bm{b}_Q^h\bm{R}_{qk}^h, \\ 
&\bm{W}_K^h\rightarrow(\bm{R}_{qk}^h)^\top\bm{W}_K^h,\ \bm{b}_K^h\rightarrow\bm{b}_K^h\bm{R}_{qk}^h, \bm{W}_V^h\rightarrow(\bm{R}_{vo}^h)^\top\bm{W}_V^h, \\ 
&\bm{b}_V^h\rightarrow\bm{b}_V^h\bm{R}_{vo}^h,\ \bm{W}_O^h\rightarrow\bm{W}_O^h\bm{R}_{vo}^h,\ \bm{b}_O\rightarrow\bm{b}_O,
\end{aligned}
\end{equation}
where $\bm{R}_{qk}^h$ and $\bm{R}_{vo}^h$ are rotation matrices for $h=1,\dots,H$.
The progress from permutation to rotation extends the symmetry of transformers to a continuous space and enhances our understanding of the parameter space symmetry of attention mechanism.
The denseness of the symmetry allows for the choice of better invariant models to analyze transformers' loss landscapes.
For better clarity, we provide an intuitive illustration of the rotation symmetry in \Cref{fig:rotation_symmetry}.
\begin{figure}[t]
\centering
% \vspace{-2mm}
\includegraphics[clip, trim=3.5cm 5cm 3.5cm 5cm, width=0.9\linewidth]{images/rotation_symmetry.pdf}
\vspace{-3mm}
\caption{The rotation symmetry of self-attention layers.}
\label{fig:rotation_symmetry}
\vspace{-3mm}
\end{figure}

\section{Symmetry for Model Fusion}
In this section, we explore the benefit of the rotation symmetry of transformers in model fusion~\citep{li2023deep,matena2022merging,wortsman2022model,yadav2023ties,jin2023dataless,daheim2024model,yang2024adamerging}.
Model fusion is proposed to merge multiple given end models trained in different settings (\textit{e.g.}, upon different datasets and hyperparameter settings) in the parameter space to improve model utility and robustness.
Compared with ensemble learning~\citep{dietterich2000ensemble,sagi2018ensemble,lakshminarayanan2017simple}, model fusion has a lower inference-stage complexity without requiring access to the training data.
Most existing methods of model fusion conduct a weighted averaging of different end models, \textit{e.g.}, direct averaging~\citep{wortsman2022model}, Fisher-weighted averaging~\citep{matena2022merging}, and regression-mean averaging~\citep{jin2023dataless}.
We next show the potential of exploiting the permutation and rotation symmetry as a plug-and-play module to improve the model fusion techniques.

\subsection{Background and Motivation}
Let $\bm{W}_1,\dots,\bm{W}_k$ denote $k$ different end models (after training or pre-training) with the same architecture.
The goal of model fusion is to merge the given $k$ end models in the parameter space and obtain a single model.
If the given models are trained over different datasets, we can expect the merged model to have better utility and out-of-distribution robustness~\citep{jin2023dataless}.
The theoretical results in previous literature~\citep{wortsman2022model} have shown that \textit{strong convexity} and \textit{closer end models} can boost the utility of direct model fusion.
Consequently, the primary advantage of permutation (and rotation) symmetry applying to model fusion is that we can substitute the end models with corresponding carefully chosen equivalent models (in the equivalence class determined by permutation and rotation invariance) to make the selected end models more concentrated, \textit{i.e.}, closer to each other.
This step is usually called \textbf{parameter matching}~\citep{singh2020model,wang2020federated,ainsworth2023git}, aka parameter or neuron alignment.
Parameter matching, which aims to reduce the distance between end models, can naturally yield closer end models and improve model fusion performance.
On the other hand, different end models can lie in the basins of different local optimums regarding the highly non-convex nature of transformers.
\begin{figure}[t]
\centering
% \vspace{-1mm}
\includegraphics[clip, trim=8.5cm 4cm 8cm 3.5cm, width=0.75\linewidth]{images/diagram.pdf}
\vspace{-3mm}
\caption{An intuitive example of the usage of parameter space symmetry for model fusion. The background shows the contour map of the loss landscape in the model parameter space. A and B are the original end models to be merged, AB is the result of naive model fusion, and AB' is the result of model fusion with parameter matching.}
\label{fig:model_fusion}
\vspace{-3mm}
\end{figure}
Previous studies have verified that parameter matching can merge different end models toward a single low-loss basin, \textit{i.e.}, the loss value along the linear interpolation between matched models shows an approximately flat or convex curve~\citep{entezari2022role,ainsworth2023git}. 
This property is called \textit{linear mode connectivity} and is regarded as a weak form of convexity~\citep{ainsworth2023git}.
As a result, parameter matching can also help improve the convexity of the objective in the area adjacent to the end models.
We showcase an intuitive example of using parameter space symmetry to improve model fusion in \Cref{fig:model_fusion} to illustrate the intuition behind parameter matching.
We can observe that the merged model after parameter matching (AB') has a lower loss value, \textit{i.e.}, better utility than the naive merged model (AB).

\subsection{Parameter Matching}
Next, our primary goal is to develop a practical parameter matching algorithm to minimize the distance between different end models. We begin by merging two models $\bm{W}_1$ and $\bm{W}_2$.

\paragraph{Matching two FFNs.}
Let $\{\bm{W}_{i_k},\bm{b}_{i_k},\bm{W}_{o_k},\bm{b}_{o_k}\}_{k=1,2}$ denote the model parameters in the two feedforward networks to be merged where $k$ denote the index of the networks.
We have already derived the equivalence relation of parameters in the feedforward networks determined by permutation invariance as \Cref{eq:match_ffn}. 
Based on \Cref{eq:match_ffn}, we can formulate parameter matching as an optimization problem as follows.
\begin{equation}\label{eq:obj_ffn}
% \small
\begin{aligned}
&\mathrm{argmin}_{\bm{P}_1,\bm{P}_2\in\mathcal{S}}\|\bm{P}_1^\top\bm{W}_{i_1}-\bm{P}_2^\top\bm{W}_{i_2}\|^2_F \\
&\quad+\|\bm{b}_{i_1}\bm{P}_1-\bm{b}_{i_2}\bm{P}_2\|^2_F+\|\bm{W}_{o_1}\bm{P}_1-\bm{W}_{o_2}\bm{P}_2\|^2_F.
\end{aligned}
\end{equation}
As shown in \Cref{eq:obj_ffn}, the goal is to find the functionally equivalent models from the equivalence classes $\pi(\bm{W}_1)$ and $\pi(\bm{W}_2)$ determined by $\bm{P}_1$ and $\bm{P}_2$, which has the smallest $\ell$-2 distance in the parameter space.
Following the method~\citep{ainsworth2023git}, we reformulate the optimization problem shown in \Cref{eq:obj_ffn} as a \textit{linear assignment problem}:
\begin{equation}\label{eq:obj_ffn_lap}
\mathrm{argmax}_{\bm{P}\in\mathcal{S}}\left<\bm{P},\bm{W}_{i_1}\bm{W}_{i_2}^\top+\bm{b}_{i_1}^\top\bm{b}_{i_2}+\bm{W}_{o_1}^\top\bm{W}_{o_2}\right>_F.
\end{equation}
% Hereby, we take the first term $\|\bm{P}_1^\top\bm{W}_{i_1}-\bm{P}_2^\top\bm{W}_{i_2}\|^2_F$ as an example to show the derivation of \Cref{eq:obj_ffn_lap}.
% First, as $\bm{P}_1$ and $\bm{P}_2$ are both permutation matrices (nonsingular), we let $\bm{P}=\bm{P}_1\bm{P}_2^{-1}$ and then have
% \begin{equation}
% \begin{aligned}
% &\mathrm{argmin}_{\bm{P}_1,\bm{P}_2\in\mathcal{S}}\|\bm{P}_1^\top\bm{W}_{i_1}-\bm{P}_2^\top\bm{W}_{i_2}\|^2_F \\
% =&\mathrm{argmin}_{\bm{P}\in\mathcal{S}}\|\bm{P}^\top\bm{W}_{i_1}-\bm{W}_{i_2}\|^2_F.
% \end{aligned}
% \end{equation}
% The second equal sign holds as multiplying a permutation matrix preserves the Frobenius norm.
% Subsequently, we have
% \begin{equation}
% \begin{aligned}
% &\mathrm{argmin}_{\bm{P}\in\mathcal{S}}\|\bm{P}^\top\bm{W}_{i_1}-\bm{W}_{i_2}\|^2_F \\
% % =&\mathrm{argmin}_{\bm{P}\in\mathcal{S}}\left<\bm{P}^\top\bm{W}_{i_1}-\bm{W}_{i_2},\ \bm{P}^\top\bm{W}_{i_1}-\bm{W}_{i_2}\right>_F \\
% =&\mathrm{argmax}_{\bm{P}\in\mathcal{S}}\left<\bm{W}_{i_2},\ \bm{P}^\top\bm{W}_{i_1}\right>_F \\
% % =&\mathrm{argmax}_{\bm{P}\in\mathcal{S}}\ \mathrm{tr}\left(\bm{W}_{i_2}^\top\bm{P}^\top\bm{W}_{i_1}\right) \\
% =&\mathrm{argmax}_{\bm{P}\in\mathcal{S}}\ \mathrm{tr}\left(\bm{P}^\top\bm{W}_{i_1}\bm{W}_{i_2}^\top\right). 
% % &=\mathrm{argmax}_{\bm{P}\in\mathcal{S}}\left<\bm{P},\bm{W}_{i_1}\bm{W}_{i_2}^\top\right>_F.
% \end{aligned}
% \end{equation}
% The derivation is finished by the definition of the Frobenius inner product $\left<\bm{P},\bm{W}_{i_1}\bm{W}_{i_2}^\top\right>_F=\mathrm{tr}\left(\bm{P}^\top\bm{W}_{i_1}\bm{W}_{i_2}^\top\right)$.
% The rest two terms can be derived similarly. 
Linear assignment problems such as~\Cref{eq:obj_ffn_lap} have been well-studied in previous literature~\citep{martello1987linear,burkard1999linear} and can be solved precisely by Hungarian Algorithm~\citep{martello1987linear}.
After calculating the value of $\bm{P}$, we substitute $\bm{P}=\bm{P}_1\bm{P}_2^{-1}$ back to \Cref{eq:match_ffn} and let $\bm{P}_2=\bm{I}$ (for simplicity), then we obtain the matched parameter as $\bm{W}_{i_1}\rightarrow\bm{P}^\top\bm{W}_{i_1},\ \bm{b}_{i_1}\rightarrow\bm{b}_{i_1}\bm{P},\ \bm{W}_{o_1}\rightarrow\bm{W}_{o_1}\bm{P},\ \bm{b}_{o_1}\rightarrow\bm{b}_{o_1}$.
Parameters of the second model $\{\bm{W}_{i_2},\bm{b}_{i_2},\bm{W}_{o_2},\bm{b}_{o_2}\}$ remain unchanged, which makes the second model an anchor model for matching.

\paragraph{Matching two ATTNs.}
Let the parameters in the two self-attention layers to be merged be $\{\bm{W}_{Q_k}^i,\bm{b}_{Q_k}^i,\bm{W}_{K_k}^i,\bm{b}_{K_k}^i,\bm{W}_{V_k}^i,\bm{b}_{V_k}^i,\bm{W}_{O_k}^i,\bm{b}_{O_k}^i\}_{k=1,2;i=1\dots H}$ where $k$ denote the index of the layers.
The equivalence relation of these parameters in the self-attention layers in terms of rotation invariance is shown in \Cref{eq:match_attn}.
Similar to matching FFNs, our goal is to match the parameters of one (source) attention layer to the other (target) attention layer where the matched parameters are supposed to be \textit{closest} to the target parameters while being functionally equivalent when feeding with any input data, \textit{i.e.}, in the equivalence class determined by rotation invariance. 
The goal can be formulated as an optimization problem. 
\begin{equation}\label{eq:obj_attn}
\small
\begin{aligned}
\min_{\bm{R}_{qk}^h,\bm{R}_{vo}^h\in\mathcal{R}}&\left\|\left[
\begin{array}{cc}
(\bm{W}_{Q_1}^h)^\top & (\bm{W}_{Q_2}^h)^\top \\
\bm{b}_{Q_1}^h & \bm{b}_{Q_2}^h \\
\end{array}\right]\left[
\begin{array}{c}
\bm{R}_{qk1}^h \\
-\bm{R}_{qk2}^h \\
\end{array}\right]\right\|_F^2 \\
+&\left\|\left[
\begin{array}{cc}
(\bm{W}_{K_1}^h)^\top & (\bm{W}_{K_2}^h)^\top \\
\bm{b}_{K_1}^h & \bm{b}_{K_2}^h \\
\end{array}\right]\left[
\begin{array}{c}
\bm{R}_{qk1}^h \\
-\bm{R}_{qk2}^h \\
\end{array}\right]\right\|_F^2 \\
+&\left\|\left[
\begin{array}{cc}
(\bm{W}_{V_1}^h)^\top & (\bm{W}_{V_2}^h)^\top \\
\bm{b}_{V_1}^h & \bm{b}_{V_2}^h \\
\end{array}\right]\left[
\begin{array}{c}
\bm{R}_{vo1}^h \\
-\bm{R}_{vo2}^h \\
\end{array}\right]\right\|_F^2 \\
+&\left\|\left[
\begin{array}{cc}
\bm{W}_{O_1}^h & \bm{W}_{O_2}^h \\
\bm{0} & \bm{0} \\
\end{array}\right]\left[
\begin{array}{c}
\bm{R}_{vo1}^h \\
-\bm{R}_{vo2}^h \\
\end{array}\right]\right\|_F^2,
\end{aligned}
\end{equation}
where $\mathcal{R}$ denotes the set of rotation matrices.
To solve this problem, we divide \Cref{eq:obj_attn} into two separate optimization problems (the first line in terms of $\bm{R}_{qk1}^h$, $\bm{R}_{qk2}^h$ as the first objective and the second line in terms of $\bm{R}_{vo1}^h$, $\bm{R}_{vo2}^h$ as the second objective).
Considering the similar formulation of these two optimization problems, in this paper, we propose the following theorem to solve both problems.
\begin{theorem}\label{thm:solution}
The following optimization problem has a closed-form solution.
\begin{equation}
% \small
\begin{aligned}
\min_{\bm{R}_1,\bm{R}_2\in\mathcal{R}}&\left\|\left[
\begin{array}{cc}
\bm{W}_{Q_1}^\top & \bm{W}_{Q_2}^\top \\
\bm{b}_{Q_1} & \bm{b}_{Q_2} \\
\end{array}\right]\left[
\begin{array}{c}
\bm{R}_1 \\
-\bm{R}_2 \\
\end{array}\right]\right\|_F^2 \\
+&\left\|\left[
\begin{array}{cc}
\bm{W}_{K_1}^\top & \bm{W}_{K_2}^\top \\
\bm{b}_{K_1} & \bm{b}_{K_2} \\
\end{array}\right]\left[
\begin{array}{c}
\bm{R}_1 \\
-\bm{R}_2 \\
\end{array}\right]\right\|_F^2.
\end{aligned}
\end{equation}
The solution is given by
\begin{equation}
\bm{R}_1=\bm{U}\bm{V}^\top,\bm{R}_2=\bm{I},
\end{equation}
where $\bm{I}$ is the identity matrix and $\bm{U}\bm{\Sigma}\bm{V}^\top=\bm{W}_{Q_1}\bm{W}_{Q_2}^\top+\bm{W}_{K_1}\bm{W}_{K_2}^\top+\bm{b}_{Q_1}^\top\bm{b}_{Q_2}+\bm{b}_{K_1}^\top\bm{b}_{K_2}$ is the result of eigendecomposition.
\end{theorem}
We leave a detailed proof of \Cref{thm:solution} in the appendix.
According to \Cref{thm:solution}, we can obtain the algorithm to match two self-attention layers shown in \Cref{alg:attn_matching}.
\Cref{alg:attn_matching} can be seen as an adaptation of the Kabsch algorithm~\citep{kabsch1976solution,umeyama1991least}.
Without loss of generality, we let the parameters in the second ($k=2$) self-attention layer be the anchor and conduct rotation for the first layer ($k=1$).
The denseness of rotation symmetry helps reduce the distance between the end models after parameter matching as \Cref{alg:attn_matching} shows.
% In addition, we mainly focus on parameter matching in our study. Our parameter matching algorithm can be easily adapted to activation matching~\citep{singh2020model,ainsworth2023git} which aims to minimize the distance between the outputs of the end models.

% \begin{algorithm}[t]
% \caption{Matching two self-attention layers.}\label{alg:attn_matching}
% \textbf{Input:} Model parameters of the attention layers \{$\bm{W}_{Q_k}^h$,$\bm{b}_{Q_k}^h$,$\bm{W}_{K_k}^h$,$\bm{b}_{K_k}^h$,$\bm{W}_{V_k}^h$,$\bm{b}_{V_k}^h$,$\bm{W}_{O_k}^h$,$\bm{b}_{O_k}^h$\}$_{k=1,2;h=1,\dots,H}$. \\
% \textbf{Output:} The optimally matched parameters from source parameters ($k=1$) to anchor parameters ($k=2$, maintain unchanged).
% \begin{algorithmic}[1]
% \STATE QK solution: $\bm{R}_{qk1}^h=\bm{U}_{qk}^h(\bm{V}_{qk}^h)^\top,\bm{R}_{qk2}^h=\bm{I}$, where $\bm{U}_{qk}^h\bm{\Sigma}_{qk}^h(\bm{V}_{qk}^h)^\top=\bm{W}_{Q_1}^h(\bm{W}_{Q_2}^h)^\top+\bm{W}_{K_1}^h(\bm{W}_{K_2}^h)^\top+(\bm{b}_{Q_1}^h)^\top\bm{b}_{Q_2}^h+(\bm{b}_{K_1}^h)^\top\bm{b}_{K_2}^h$.
% \STATE QK matching: $\bm{W}_{Q_1}^h\rightarrow(\bm{R}_{qk1}^h)^\top\bm{W}_{Q_1}^h$, $\bm{W}_{K_1}^h\rightarrow(\bm{R}_{qk1}^h)^\top\bm{W}_{K_1}^h$, $\bm{b}_{Q_1}^h\rightarrow\bm{b}_{Q_1}^h\bm{R}_{qk1}^h$, $\bm{b}_{K_1}^h\rightarrow\bm{b}_{K_1}^h\bm{R}_{qk1}^h$.
% \STATE VO solution: $\bm{R}_{vo1}^h=\bm{U}_{vo}^h(\bm{V}_{vo}^h)^\top,\bm{R}_{vo2}^h=\bm{I}$, where $\bm{U}_{vo}^h\bm{\Sigma}_{vo}^h(\bm{V}_{vo}^h)^\top=\bm{W}_{V_1}^h(\bm{W}_{V_2}^h)^\top+(\bm{W}_{O_1}^h)^\top\bm{W}_{O_2}^h+(\bm{b}_{V_1}^h)^\top\bm{b}_{V_2}^h$.
% \STATE VO matching: $\bm{W}_{V_1}^h\rightarrow(\bm{R}_{vo1}^h)^\top\bm{W}_{V_1}^h$, $\bm{W}_{O_1}^h\rightarrow\bm{W}_{O_1}^h\bm{R}_{vo1}^h$, $\bm{b}_{V_1}^h\rightarrow\bm{b}_{V_1}^h\bm{R}_{vo1}^h$.
% \end{algorithmic}
% \end{algorithm}

\paragraph{Scaling Matching.}
We find that the rescaling symmetry~\citep{neyshabur2015path,meng2019g} can be leveraged jointly with our proposed rotation symmetry in the parameter matching algorithm.
For instance, for the (simplified) Q-K product $\bm{W}_Q\bm{W}_K$, we can find a rescaling operation that preserves the functionality of the Q-K product $a\bm{W}_Q\cdot\frac{1}{a}\bm{W}_K$ where $a\neq0$ is a real number. 
By adding a scalar variable to each parameter matrix, we can formulate the objective of rescaling symmetry as follows (taking the Q-K product as an example).
\begin{equation}\label{eq:rescaling_objective}
% \small
\begin{aligned}
\min_a&\left\|a\bm{W}_{Q_1}^\prime-\bm{W}_{Q_2}^\prime\right\|^2+\left\|a\bm{b}_{Q_1}^\prime-\bm{b}_{Q_2}^\prime\right\|^2 \\
&+\left\|\bm{W}_{Q_1}^\prime/a-\bm{W}_{Q_2}^\prime\right\|^2+\left\|\bm{b}_{Q_1}^\prime/a-\bm{b}_{Q_2}^\prime\right\|^2,
\end{aligned}
\end{equation}
where $a$ is the rescaling variable and $^\prime$ denotes the parameters after rotation symmetry-based matching.
Here, we still set model 2 as the anchor model and conduct the rescaling operation to model 1 to align with model 2.
We can easily solve the optimality condition of~\Cref{eq:rescaling_objective} as
\begin{equation}\label{eq:rescaling_derivative}
% \small
\begin{aligned}
\left(\left\|\bm{W}_{Q_1}^\prime\right\|^2+\left\|\bm{b}_{Q_1}^\prime\right\|^2\right)a^4-\left\|\bm{W}_{K_1}^\prime\right\|^2-\left\|\bm{b}_{K_1}^\prime\right\|^2& \\
-\left(\left<\bm{W}_{Q_1}^\prime,\bm{W}_{Q_2}^\prime\right>+\left<\bm{b}_{Q_1}^\prime,\bm{b}_{Q_2}^\prime\right>\right)a^3& \\
+\left(\left<\bm{W}_{K_1}^\prime,\bm{W}_{K_2}^\prime\right>+\left<\bm{b}_{K_1}^\prime,\bm{b}_{K_2}^\prime\right>\right)a&=0.
\end{aligned}
\end{equation}
The roots of \Cref{eq:rescaling_derivative} can be derived using numerical methods as the value of $a$.
We then conduct the rescaling operation to model 1 as $\bm{W}_{Q_1}^\prime\rightarrow a\bm{W}_{Q_1}^\prime$, $\bm{b}_{Q_1}^\prime\rightarrow a\bm{b}_{Q_1}^\prime$, $\bm{W}_{K_1}^\prime\rightarrow\frac{1}{a}\bm{W}_{K_1}^\prime$, $\bm{b}_{K_1}^\prime\rightarrow\frac{1}{a}\bm{b}_{K_1}^\prime$.
It is worth noting that the rescaling symmetry-based matching is conducted after the rotation symmetry-based matching.
Using the rescaling operation, we extend the rotation matrices to orthogonal matrices with larger norms.
Nevertheless, the end models are close to each other in practical scenarios so the value of $a$ is usually close to 1.

\paragraph{Complexity.}
We next provide a brief analysis of the complexity of our proposed parameter matching algorithm.
We let the hidden dimension of the target transformer be $d$, the layer of the target transformer be $L$, and the number of attention heads be $H$.
For the parameter matching of feedforward networks, the linear assignment problem can be solved in $O(d^3)$ by the Hungarian algorithm~\citep{kuhn1955hungarian,martello1987linear}.
Additionally, to solve the optimization problem in \Cref{eq:obj_attn}, \Cref{alg:attn_matching} requires the complexity of $O(d^3)$ for eigendecomposition. 
Hence, the complexity of our proposed full parameter matching algorithm of a transformer is $O(d^3LH)$, similar to the complexity of the feedforward.
It is worth noting that the complexity can be further reduced in two ways.
The first way is to match a subset of layers instead of all.
In this way, the complexity of our proposed parameter matching algorithm is $O(d^3L_sH)$ where $L_s$ denotes the number of selected layers.
The second way is to match each unit module (a single feedforward network or a single attention layer) in parallel.
The decoupling of matching different modules makes it easy to implement multiprocessing and the overall complexity becomes $O(d^3LH/p)$ where $p$ is the number of processes in parallel.

\begin{table}[t]
\centering
\small
\tabcolsep = 1.6pt
\renewcommand{\arraystretch}{1.1}
\caption{Experimental results of in-domain (Emotion and NER) and out-of-domain (NER-CoNLL) model fusion for two base language models over Emotion and NER tasks.}
\label{tab:glue1}
\aboverulesep = 0pt
\belowrulesep = 0pt
% \vspace{-3mm}
\begin{tabular}{l|cc|cc|cc}
\toprule
 & \multicolumn{2}{c|}{Emotion} & \multicolumn{2}{c|}{NER} & \multicolumn{2}{c}{NER-CoNLL} \\
& Roberta & Deberta & Roberta & Deberta & Roberta & Deberta \\
\midrule
Simple & 35.87 & 2.99 & 60.88 & 27.54 & 26.86 & 10.80 \\
+match & 35.87 & 2.99 & 60.88 & \textbf{31.31} & \textbf{26.87} & \textbf{23.30} \\
\midrule
Fisher & 44.02 & 35.95 & 54.55 & 33.20 & \textbf{23.06} & 12.53 \\
+match & \textbf{44.05} & \textbf{35.98} & \textbf{54.58} & \textbf{33.83} & 23.05 & \textbf{18.33} \\
\midrule
Regmean & 35.87 & 2.99 & 60.88 & 27.54 & 26.86 & 10.80 \\
+match & \textbf{39.95} & 2.99 & 60.88 & \textbf{31.31} & \textbf{26.87} & \textbf{14.06} \\
\bottomrule
\end{tabular}
\vspace{-2mm}
\end{table}

\begin{table}[t]
\centering
\tabcolsep = 3pt
\renewcommand{\arraystretch}{1.1}
\caption{Experimental results of ViT merging over image classification task. OT represents OT-acts-EMD. }
\label{tab:vit}
\aboverulesep = 0pt
\belowrulesep = 0pt
% \vspace{-3mm}
\begin{tabular}{l|cccc}
\toprule
 & Simple & Fisher & Regmean & OT \\
\midrule
w/o matching & 7.60 & 17.96 & 14.24 & 32.08 \\
w/ matching & \textbf{10.22} & \textbf{18.61} & 15.31 & 32.50 \\
w/ scaling matching & 10.19 & 18.58 & \textbf{15.35} & \textbf{32.53} \\
\bottomrule
\end{tabular}
\vspace{-2mm}
\end{table}

% \begin{figure}[t]
% \centering
% % \vspace{-1mm}
% \includegraphics[width=0.8\linewidth]{icml2025/images/vit.pdf}
% \vspace{-3mm}
% \caption{The experimental results of ViT merging over image classification task. OT represents OT-acts-EMD.}\label{fig:vit}
% \vspace{-3mm}
% \end{figure}

\section{Experiments}
% In this section, we conduct experiments to evaluate our proposed parameter matching algorithm based on the rotation symmetry of transformers.
% In particular, we aim to answer the following research questions:
% RQ1: Can our proposed parameter matching algorithm improve the performance of model fusion for transformer-based models?
% RQ2: To what extent can our proposed rotation symmetry benefit the selection of parameter space symmetry for transformers?
% RQ3: As a plug-in model, does our model introduce significant additional computational complexity?
% RQ4: Is matching each transformer layer equally important? Can we improve the efficiency without compromising the utility by matching only a subset of layers?

In this section, we conduct experiments to evaluate the effectiveness of our proposed parameter matching algorithm based on rotation symmetry. Specifically, we aim to answer the following research questions:
RQ1: Can our proposed parameter matching algorithm enhance the performance of model fusion for transformer-based models?
RQ2: How does rotation symmetry contribute to parameter matching for self-attention layers?
RQ3: As a plugin module, does our algorithm introduce significant additional computational overhead?
RQ4: Is matching all transformer layers equally important? Can we improve efficiency without compromising utility by matching only a subset of layers?

\subsection{Experimental Settings}\label{sec:exp_setting}
\paragraph{Platform.}
% Our code is implemented based on \texttt{Python 3.10} and \texttt{Pytorch 1.13}. We fine-tune, match, and merge all the models on a cluster equipped with \texttt{Nvidia A100 80GB} GPUs.
Our implementation is based on \texttt{Python 3.10} and \texttt{Pytorch 1.13}. All fine-tuning, parameter matching, and model merging processes are conducted on a cluster equipped with \texttt{Nvidia A100 80GB} GPUs.

\paragraph{Models.}
% Two widely used transformer architecture models are employed to evaluate the effectiveness of our approach: Roberta~\citep{liu2019roberta} and Deberta~\citep{he2021deberta}. 
% We utilize the HuggingFace library to obtain pretrained models for both Roberta (\texttt{roberta-base} with 12 attention layers) and Deberta (\texttt{microsoft/deberta-v3-large} with 24 attention layers).
% In terms of vision transformers~\citep{dosovitskiy2021imageworth16x16words}, we directly deploy two pretrained models from~\citep{imfeld2024transformer}.
% By selecting these three architectures, we aim to demonstrate the performance of permutation across different scales and downstream tasks.

To evaluate the effectiveness of our approach, we use two widely adopted transformer models: RoBERTa~\citep{liu2019roberta} and DeBERTa~\citep{he2021deberta}.
We obtain pretrained models of RoBERTa (\texttt{roberta-base} with 12 attention layers) and DeBERTa-Large (\texttt{microsoft/deberta-v3-large} with 24 attention layers) from the Hugging Face library. 
For vision transformers (ViTs)~\citep{dosovitskiy2021imageworth16x16words}, we directly use the pretrained models following~\citep{imfeld2024transformer}. 
By selecting these three type of models, we assess the effectiveness of our method across different model scales and downstream tasks.

\paragraph{Finetuning and Matching.}
% We performed finetuning on each pretrained language model for 20 epochs with a learning rate of 1e-5 on each dataset individually. Additionally, we used a batch size of 16 and a warmup ratio of 0.06.
% Afterward, we matched and merged the models fine-tuned on different datasets in pairs (for in-domain experiments) or in groups (for out-of-domain experiments). It is noteworthy that we only matched the parameters within the attention layers, while for the classifier module, we directly copied the parameters from the fine-tuned model corresponding to the dataset.
% For more information regarding the datasets and baselines, please refer to the appendix.

In the experiments, each pretrained language model is fine-tuned for 20 epochs on each dataset individually.
We set the learning rate at 1e-5, the batch size at 16, and the warmup ratio at 0.06 for each model. 
After fine-tuning, we perform parameter matching and model merging, considering both in-domain (pairwise fine-tuned models) and out-of-domain (grouped models) experiments. 
Notably, we match only the parameters within the attention layers, while the classifier module is directly copied from the model fine-tuned on the corresponding downstream task.
For additional details on datasets and baseline methods, please refer to~\Cref{app:b}.

\begin{figure}[t]
\centering
% \vspace{-1mm}
\includegraphics[width=0.9\linewidth]{images/matching_distance.pdf}
\vspace{-3mm}
\caption{The Euclidean Distance of end ViT models after different parameter matching algorithms.}\label{tab:matching_distance}
\vspace{-3mm}
\end{figure}

\begin{figure}[t]
\centering
\subfigure[Single-Layer Matching]{
\includegraphics[width=0.48\linewidth]{images/deberta_single_regmean_exp3.pdf}\label{fig:single}}
\subfigure[Tail-Layers Matching]{
\includegraphics[width=0.48\linewidth]{images/deberta_regmean_exp3.pdf}\label{fig:tail}}
\vspace{-3mm}
\caption{Evaluation loss of matching a subset of layers.}
\label{fig:subset_matching}
\vspace{-2mm}
\end{figure}

\subsection{Performance of Model Fusion}
To answer RQ1, we compare the performance of three model fusion baselines with and without our proposed parameter matching algorithm.

% \paragraph{In-Domain Settings.}
% We conducted experiments on model fusion for Emotion Classification and Named Entity Recognition (NER) tasks, where our approach (\textbf{+match}) serves as a plugin module for three model fusion baselines. 
% For each task, we first fine-tune the corresponding language models on each in-domain dataset (5 for Emotion, 6 for NER) separately and then merge them pairwise. 
% We use different initializations for new classification heads.
% The first two columns in~\Cref{tab:glue1} present the average macro F1-score (for Emotion) and F1-score (for NER) of merged models.

% \paragraph{Out-of-Domain Settings.}
% In this part of the experiment, we merged the models trained on all in-domain datasets on NER tasks and evaluated their performance on CoNLL datasets (which serve the out-of-domain datasets in our setting). The third column in \Cref{tab:glue1} presents the OOD performance. 

% \paragraph{ViT Settings.}
% We employ \texttt{OT-acts-EMD} from~\citep{imfeld2024transformer} as an additional baseline. 
% Two pretrained models are merged, and we evaluate the final performance on the test set of CIFAR-10~\citep{krizhevsky2009learning} dataset. \Cref{fig:vit} presents the classification accuracy of merged models. 
% To improve the flexibility of parameter matching, we separately match each layer and conduct model fusion. 
% We then choose the merged model with the best performance over the validation set as the test model.

\paragraph{In-Domain Settings.}
We evaluate model fusion performance on Emotion Classification and Named Entity Recognition (NER) tasks, where our approach (\textbf{+match}) is integrated as a plugin module for three model fusion baselines.
For each task, we fine-tune the language models on each in-domain dataset (5 for Emotion, 6 for NER) respectively and merge them pairwise.
% The classification heads of merged models are initialized separately.
The first two columns in~\Cref{tab:glue1} present the average macro F1-score (for Emotion) and micro F1-score (for NER) of the merged models following~\citep{jin2023dataless}.

\paragraph{Out-of-Domain Settings.}
To assess the generalization ability, we merge models trained on all in-domain NER datasets and evaluate their performance on CoNLL datasets, which serve as out-of-domain (OOD) test sets.
The third column in~\Cref{tab:glue1} reports the OOD performance of the merged models.
\vspace{-3mm}

\paragraph{ViT Settings.}
We employ \texttt{OT-acts-EMD} from~\citep{imfeld2024transformer} as an additional baseline.
Two pretrained models are merged, and we evaluate their performance on the CIFAR-10~\citep{krizhevsky2009learning} dataset.
\Cref{tab:vit} presents the classification accuracy of the merged models.
To improve the flexibility of parameter matching, we match each layer separately before conducting model fusion.
The merged model achieving the highest validation performance is selected as the final test model.

% From the results shown in \Cref{tab:glue1}, we have the following observations:
% In most cases, our proposed parameter matching algorithms can help improve the performance of different model fusion approaches. We attribute this to parameter matching's capability of aligning distant models located at different local minimums, so the end models are closer and have a better chance of approaching a minimum point after model fusion.
% % 2) Compared with using the same initialization for classification heads, our proposed parameter matching algorithm brings a larger improvement for model fusion using different initialization.
% % We attribute this to the capability of parameter matching in aligning distant models located at different local minimums.
% Compared with smaller-sized models (Roberta-base), our proposed parameter matching algorithm benefits merging larger-sized models (Deberta-large) better.
% From the results in \Cref{fig:vit}, we can obtain that our proposed parameter matching strategy consistently improves the performances of model fusion methods.
% Among the adopted fusion methods, the performance of simple fusion increases the most. 
% We attribute this to the difference in objectives.
% Simple fusion directly averages the model weights in the parameter space which can explicitly benefit from the reduced distance brought by parameter matching. 
% Advanced fusion methods aim to align the output of the merged model with end models, which is equivalent to a weighted averaging of end models by input data.
% Compared with direct averaging, weighted averaging can only implicitly benefit from the distance reduction.

From the results in~\Cref{tab:glue1} and~\Cref{tab:vit}, we make the following observations:
% \begin{itemize}[leftmargin=*]
% \item Across most cases, our parameter matching algorithm consistently improves the performance of various model fusion approaches.
% The reason is that distant end models are aligned by parameter matching to be closer, making the merged model more likely to approach an overall minimum point.
% \item Compared to RoBERTa-base, our method yields greater improvements for DeBERTa-large, suggesting that larger models benefit more from parameter matching.
% \item Among the fusion baselines, simple fusion shows the largest performance increase. 
% We attribute this to differences in fusion objectives: simple fusion directly averages model weights, benefiting explicitly from the reduced distance between model parameters after matching; while advanced fusion methods attempt to align the outputs of the end models based on input data, which is equivalent to weighted averaging of end models.
% Unlike direct averaging, advanced fusion implicitly benefits from parameter matching for distance reduction.
% \end{itemize}
(1). Our parameter matching algorithm consistently improves the performance of different model fusion methods. 
The reason is that parameter matching helps align distant models, bringing them closer in parameter space. 
As a result, the merged model is more likely to approach an overall minimum.
(2). Compared to RoBERTa-base, our method brings larger improvement for DeBERTa-large, suggesting that larger models benefit more from parameter matching. 
We explain this as larger models have more parameters that can be aligned, and in high-dimensional spaces, models tend to be more spread out. Subsequently, parameter matching helps bridge this gap more effectively.
(3). Among all the fusion methods, simple fusion shows the largest improvement after parameter matching. 
This is likely because simple fusion directly averages model weights, so reducing the distance between model parameters brings a clear benefit. 
In contrast, advanced fusion methods work by aligning outputs of the end models based on input data, which is equivalent to weighted averaging. Unlike direct averaging, these methods benefit from parameter matching in a more implicit way.

\subsection{Ablation Study}

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.75\linewidth]{AnonymousSubmission/LaTeX/images/ablation_study.pdf}
%     \vspace{-3mm}
%     \caption{The Euclidean Distance of end ViT models after different parameter matching algorithms.}
%     \label{fig:matching_distance}
% \end{figure}

% \begin{table}[t]
% \centering
% \small
% \renewcommand{\arraystretch}{1.1}
% \tabcolsep = 4pt
% % \vspace{-6mm}
% \caption{The Euclidean Distance of end ViT models after different parameter matching algorithms.}\label{tab:matching_distance}
% % \vspace{-3mm}
% \aboverulesep = 0pt
% \belowrulesep = 0pt
% \begin{tabular}{l|c}
% \toprule
% Matching Algorithms & Distance $\downarrow$ \\
% \midrule
% Original & 651.57  \\
% Git-Rebasin & 625.73  \\
% OT-Fusion (FFN-only) & 626.16 \\
% Ours (ATTN only) & 640.01 \\
% Ours (ATTN only, w/ scaling) & 639.96  \\
% Ours & \underline{613.68}  \\
% Ours (w/ scaling) & \textbf{613.63}  \\
% \bottomrule
% \end{tabular}
% % \vspace{-3mm}
% \end{table}

% To answer RQ2, we conduct ablation studies targeting the rotation symmetry of self-attention layers. 
% We match the two pretrained vision transformer models utilized in the previous section.
% To assess rotation symmetry, we use the Euclidean distances between the parameters of all modules from the matched model and the target model. Rather than aggregating the distances of individual modules and layers, we analyze the entire model to provide a more comprehensive distance assessment.
% Here we also employ Git-Rebasin~\citep{ainsworth2023git} and OT-Fusion (FFN-only)~\citep{jin2023dataless} as external baselines. 
% The experimental results are shown in \Cref{tab:matching_distance}.
% Our proposed parameter matching algorithm, based on rotation symmetry, reduces the distance between end models more than previous algorithms based on permutation symmetry.
% % Comparing the two versions of our method, we can observe that only matching attention layers can yield a smaller distance reduction than matching full layers.
% % As the parameter matching of feedforward networks is based on discrete permutation matrices, it can hardly work well when end models are close to each other.
% The parameter matching of self-attention layers based on the continuous space of rotation matrices can always smoothly reduce the distance of end models.
% Additionally, the rescaling symmetry can help further reduce the distance of parameters.

To answer RQ2, we conduct ablation studies to evaluate the impact of rotation symmetry in parameter matching. 
Specifically, match the two pretrained ViT models utilized in the previous section.
To assess the impact of rotation symmetry, we measure the Euclidean distance between the parameters in the matched model and the anchor model. 
Instead of aggregating distances at the layer or module level, we analyze the entire model holistically to provide a more comprehensive assessment of alignment.
For comparison, we also include two external baselines: Git-Rebasin~\citep{ainsworth2023git} and OT-Fusion~\citep{imfeld2024transformer}. 
Following the original paper, we apply OT-Fusion exclusively to the feed-forward networks.
The results in \Cref{tab:matching_distance} show that our rotation symmetry-based parameter matching algorithm consistently reduces the distance between end models more effectively than permutation symmetry-based methods.
The advantage of rotation symmetry lies in its ability to operate in a continuous space, allowing for smoother and more effective parameter alignment in self-attention layers. 
Additionally, incorporating rescaling symmetry further refines parameter alignment, leading to a greater distance reduction.

\begin{table}[t]
\centering
\tabcolsep = 3pt
\renewcommand{\arraystretch}{1.1}
\caption{The average runtime in seconds of the fine-tuning (top), matching (middle), and merging (bottom) stage. }
\label{tab:time}
\aboverulesep = 0pt
\belowrulesep = 0pt
% \vspace{-3mm}
\begin{tabular}{l|cccc}
\toprule
 & Deberta & Roberta & Vit  \\
\midrule
Fine-tuning & 12983.71 & 3071.49 & - \\
\hline
Matching & 1.59 & 1.71 & 3.48 \\
% OT-Fusion & - & - & 44.39 \\
\hline
Simple Merging & 0.13 & 0.09 & 0.22  \\
Fisher Merging & 197.47 & 69.57 & 83.67  \\
Regmean Merging & 137.67 & 36.44 & 71.02  \\
\bottomrule
\end{tabular}
\vspace{-2mm}
\end{table}

\subsection{Complexity Study}
% To answer RQ3, We calculated the average time overhead of fine-tuning (by each dataset), matching (by model-pair), and three merging methods (by model-pair) in the main experiments as shown in \cref{tab:time}. 
% For the Vit model, we only used the pre-trained model and also recorded the time for OT-Fusion.
% We found that our module's runtime is under 5\% of Fisher/Regmean merging across all models, with negligible impact on overall complexity.

To answer RQ3, we evaluate the computational overhead introduced by our parameter matching algorithm. 
Specifically, we measure the average runtime for fine-tuning (per dataset), matching (per model pair), and merging methods (per model pair) from the main experiments, as shown in \Cref{tab:time}.
For ViTs, we only use pre-trained models following~\cite{imfeld2024transformer} without fine-tuning.
The results show that our parameter matching module incurs an overhead of less than 5\% compared to Fisher / RegMean merging across all models, with negligible impact on overall complexity.

\subsection{Matching a Subset of Layers}
% To answer RQ4, we choose CoLA and STS-B from GLUE to fine-tune the end models (Deberta) and evaluate the matched models of different subsets of layers. 
% Specifically, we cross-validate the importance of each attention layer in matching through the following two experimental settings. 

% \paragraph{Single-Layer Matching.} 
% Here, we only match the specified index of the attention layer and leave the others unchanged. The evaluation loss regarding the matched layer index is shown in \Cref{fig:single}. 

% \paragraph{Tail-Layers Matching.}
% In this setting, we match a certain number of the trailing attention layers (\textit{e.g.}, if matching three attention layers, they would be the last three attention layers). 
% The evaluation loss regarding the matched layer numbers is shown in \Cref{fig:tail}. 

% The experimental results are shown in \Cref{fig:subset_matching}.
% The results in both figures demonstrate that matching head layers brings more benefits to the model utility than matching tail layers. 
% The loss value has a sharp decrease when matching the beginning 5 layers.
% Hence, we can further improve the efficiency of our method by dropping tail layers without distinctly compromising efficacy.



To answer RQ4, we investigate the effect of matching different subsets of layers. 
We fine-tune DeBERTa models on CoLA and STS-B from the GLUE benchmark and evaluate the performance of matched models under different subsets of layers. 
Specifically, we cross-validate the importance of each attention layer in matching through two settings:

\paragraph{Single-Layer Matching.}
In this setting, we match only a single attention layer while leaving all other layers unchanged. The evaluation loss corresponding to different matched layer indices is shown in \Cref{fig:single}.

\paragraph{Tail-Layers Matching.}
Here, we match a certain number of trailing attention layers (\textit{e.g.}, when matching three attention layers, we align only the last three layers). The evaluation loss for different numbers of matched layers is presented in \Cref{fig:tail}.

The experimental results in \Cref{fig:subset_matching} demonstrate that matching head layers yields greater improvements in model utility compared to tail layers. 
Notably, the loss value drops sharply when the first 5 layers are matched. 
Based on this observation, we can further improve efficiency by dropping tail layers without significantly compromising utility.

\section{Related Work}

\paragraph{Parameter Space Symmetry.}
\nop{The parameter space symmetry of deep neural networks has been studied for a long time.
Generally speaking, the parameter space symmetry is a set of models with different parameters but functionally equivalent. There are multiple ways to identify parameter space symmetries. Parameter space symmetries such as rescaling symmetry~\citep{neyshabur2015path,badrinarayanan2015symmetry,du2018algorithmic,meng2019g}, scaling symmetry~\citep{kunin2021neural}, and translation symmetry~\citep{kunin2021neural} had been identified in conventional deep neural networks to gain an in-depth understanding of the training dynamics and also accelerate the optimization process~\citep{zhao2022symmetry,zhao2023symmetries,zhao2024improving}. Another type of parameter space symmetry, \textit{i.e.}, permutation symmetry, was found to be closely related to the manifold of the global minimum and critical points~\citep{fukumizu2000local,brea2019weight,simsek2021geometry,benton2021loss,entezari2022role,ainsworth2023git}.}
Parameter space symmetry refers to a set of models with different parameter values but functionally equivalent. This concept has been extensively studied in the context of deep neural networks, as it plays a crucial role in understanding model behavior and training dynamics. %Various methods have been developed to identify parameter space symmetries. 
Examples of parameter space symmetries include rescaling symmetry~\citep{neyshabur2015path,badrinarayanan2015symmetry,du2018algorithmic,meng2019g}, scaling symmetry~\citep{kunin2021neural}, and translation symmetry~\citep{kunin2021neural}. These symmetries have been identified in conventional deep neural networks to provide deeper insights into training dynamics and to accelerate the optimization process~\citep{zhao2022symmetry,zhao2023symmetries,zhao2024improving}. Another important type of parameter space symmetry is permutation symmetry, which has been shown to closely relate to the manifold of global minima and critical points~\citep{fukumizu2000local,brea2019weight,simsek2021geometry,benton2021loss,entezari2022role,ainsworth2023git}. The permutation symmetry can also be used to align (match) the outputs or model parameters of different end models with the same architecture~\citep{singh2020model,wang2020federated,ainsworth2023git,imfeld2024transformer}.
Some concurrent works~\citep{ziyin2024symmetry,tran2024equivariant} investigate similar forms of rotation symmetry in neural networks.
\citet{ziyin2024symmetry} shows that the mirror symmetry leads to low-rankness. 
Meanwhile, \citet{tran2024equivariant} leverages the rotation symmetry to construct transformer-based neural functional networks.
In comparison, our study focuses on the role of rotation symmetry in model fusion and proposes a theoretically optimal parameter matching approach based on the properties of rotation symmetries.

\paragraph{Model Fusion.}
The goal of model fusion~\citep{li2023deep} is to merge multiple available end models (with the same architecture) to obtain a stronger model.
The scenarios of model fusion can be flexible.
When training on the same dataset, model fusion can be used to improve the model utility or generalization by merging models trained with different configurations or in different stages~\citep{izmailov2018averaging,gupta2020stochastic,cha2021swad,wortsman2022model,rame2022diverse,arpit2022ensemble}.
As a representative method in this setting, ModelSoup~\citep{wortsman2022model} greedily averages the models fine-tuned with different hyperparameter configurations to improve the utility and robustness of the model.
In addition, when training on different datasets or tasks, model fusion can be used to improve out-of-domain generalization or multitasking of the model~\citep{matena2022merging,choshen2022fusing,li2022branch,jin2023dataless}, especially for language models.
A state-of-the-art merging algorithm, RegMean~\citep{jin2023dataless}, successfully merges language models fine-tuned over different tasks and improves the model's out-of-distribution generalization.
Moreover, model fusion plays a pivotal role in federated learning~\citep{konevcny2016federated,mcmahan2017communication,wang2020federated} when the local updates are collected to make a global update.
FedAvg~\citep{mcmahan2017communication} is a classical merging algorithm that directly computes the average of the local models as the updated global model.
Recent studies propose to incorporate the permutation symmetry to align the neurons of different end models~\citep{wang2020federated,singh2020model,ainsworth2023git}.
However, these methods fail to achieve a desirable performance when tackling transformer-based models~\citep{jin2023dataless}.

\section{Conclusion}
In this paper, we introduced rotation symmetry as a novel type of parameter space symmetry for transformers, extending the concept of symmetry to continuous spaces. 
Building on this foundation, we proposed a theoretically optimal parameter matching algorithm to enhance the fusion of transformer models in a plug-and-play manner.
To validate our approach, we conducted extensive experiments on real-world NLP and vision benchmarks. 
The results demonstrated that incorporating rotation symmetry effectively and efficiently facilitates transformer model fusion, showcasing our method's practical utility.

\section*{Impact Statement}
Our study provides novel insights into the parameter space symmetry of transformers and establishes a practical framework for advancing model fusion techniques. 
Bridging theoretical innovations with practical applications, this work reveals the potential for leveraging parameter space symmetries in deep learning research.
There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here.

\bibliography{icml2025}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Proof}
\paragraph{\Cref{thm:solution}.}
\textit{The following optimization problem has a closed-form solution.}
\begin{equation}
% \small
\begin{aligned}
\min_{\bm{R}_1,\bm{R}_2\in\mathcal{R}}&\left\|\left[
\begin{array}{cc}
\bm{W}_{Q_1}^\top & \bm{W}_{Q_2}^\top \\
\bm{b}_{Q_1} & \bm{b}_{Q_2} \\
\end{array}\right]\left[
\begin{array}{c}
\bm{R}_1 \\
-\bm{R}_2 \\
\end{array}\right]\right\|_F^2+\left\|\left[
\begin{array}{cc}
\bm{W}_{K_1}^\top & \bm{W}_{K_2}^\top \\
\bm{b}_{K_1} & \bm{b}_{K_2} \\
\end{array}\right]\left[
\begin{array}{c}
\bm{R}_1 \\
-\bm{R}_2 \\
\end{array}\right]\right\|_F^2.
\end{aligned}
\end{equation}
\textit{The solution is given by}
\begin{equation}
\bm{R}_1=\bm{U}\bm{V}^\top,\bm{R}_2=\bm{I},
\end{equation}
\textit{where $\bm{I}$ is the identity matrix and $\bm{U}\bm{\Sigma}\bm{V}^\top=\bm{W}_{Q_1}\bm{W}_{Q_2}^\top+\bm{W}_{K_1}\bm{W}_{K_2}^\top+\bm{b}_{Q_1}^\top\bm{b}_{Q_2}+\bm{b}_{K_1}^\top\bm{b}_{K_2}$ is the result of eigendecomposition.}

\begin{proof}
We first show the process of converting the optimization problem to an Orthogonal Procrustes problem~\citep{schonemann1966generalized,gower2004procrustes}.
We can obtain that the optimization problem is equivalent to the following one.
\begin{equation}\label{eq:objective}
% \scriptsize
\begin{aligned}
\min_{\bm{R}_1,\bm{R}_2\in\mathcal{R}}&\left\|\bm{W}_{Q_1}^\top\bm{R}_1-\bm{W}_{Q_2}^\top\bm{R}_2\right\|_F^2+\left\|\bm{b}_{Q_1}\bm{R}_1-\bm{b}_{Q_2}\bm{R}_2\right\|_F^2+\left\|\bm{W}_{K_1}^\top\bm{R}_1-\bm{W}_{K_2}^\top\bm{R}_2\right\|_F^2+\left\|\bm{b}_{K_1}\bm{R}_1-\bm{b}_{K_2}\bm{R}_2\right\|_F^2.
\end{aligned}
\end{equation}
Considering that $\bm{R}_1,\bm{R}_2\in\mathcal{R}$ and $\bm{R}_1,\bm{R}_2$ are nonsingular matrices, we let $\bm{R}=\bm{R}_1\bm{R}_2^{-1}$ and convert \Cref{eq:objective} to the following one:
\begin{equation}\label{eq:objective1}
% \scriptsize
\begin{aligned}
\min_{\bm{R},\bm{R}_2\in\mathcal{R}}&\left\|\left(\bm{W}_{Q_1}^\top\bm{R}-\bm{W}_{Q_2}^\top\right)\bm{R}_2\right\|_F^2+\left\|\left(\bm{b}_{Q_1}\bm{R}-\bm{b}_{Q_2}\right)\bm{R}_2\right\|_F^2+\left\|\left(\bm{W}_{K_1}^\top\bm{R}-\bm{W}_{K_2}^\top\right)\bm{R}_2\right\|_F^2+\left\|\left(\bm{b}_{K_1}\bm{R}-\bm{b}_{K_2}\right)\bm{R}_2\right\|_F^2.
\end{aligned}
\end{equation}
As multiplying $\bm{R}_2$ preserves the Frobenius norm, we can remove the $\bm{R}_2$ terms from the objective and obtain an Orthogonal Procrustes problem as follows:
\begin{equation}\label{eq:objective2}
% \small
\begin{aligned}
\min_{\bm{R}\in\mathcal{R}}&\left\|\bm{W}_{Q_1}^\top\bm{R}-\bm{W}_{Q_2}^\top\right\|_F^2+\left\|\bm{b}_{Q_1}\bm{R}-\bm{b}_{Q_2}\right\|_F^2+\left\|\bm{W}_{K_1}^\top\bm{R}-\bm{W}_{K_2}^\top\right\|_F^2+\left\|\bm{b}_{K_1}\bm{R}-\bm{b}_{K_2}\right\|_F^2.
\end{aligned}
\end{equation}
We take a look at the first term $\left\|\bm{W}_{Q_1}^\top\bm{R}-\bm{W}_{Q_2}^\top\right\|_F^2$ and have:
\begin{equation}
% \small
\begin{aligned}
&\min_{\bm{R}\in\mathcal{R}}\left\|\bm{W}_{Q_1}^\top\bm{R}-\bm{W}_{Q_2}^\top\right\|_F^2 \\
=&\min_{\bm{R}\in\mathcal{R}}\left<\bm{W}_{Q_1}^\top\bm{R}-\bm{W}_{Q_2}^\top,\bm{W}_{Q_1}^\top\bm{R}-\bm{W}_{Q_2}^\top\right>_F \\
=&\max_{\bm{R}\in\mathcal{R}}\left<\bm{W}_{Q_1}^\top\bm{R},\bm{W}_{Q_2}^\top\right>_F \\
=&\max_{\bm{R}\in\mathcal{R}}\mathrm{tr}\left(\bm{R}^\top\bm{W}_{Q_1}\bm{W}_{Q_2}^\top\right) \\
=&\max_{\bm{R}\in\mathcal{R}}\left<\bm{R},\bm{W}_{Q_1}\bm{W}_{Q_2}^\top\right>_F. \\
\end{aligned}
\end{equation}
Similarly, we can convert \Cref{eq:objective2} to the following one:
\begin{equation}
% \small
\max_{\bm{R}\in\mathcal{R}}\left<\bm{R},\bm{W}_{Q_1}\bm{W}_{Q_2}^\top+\bm{W}_{K_1}\bm{W}_{K_2}^\top+\bm{b}_{Q_1}^\top\bm{b}_{Q_2}+\bm{b}_{K_1}^\top\bm{b}_{K_2}\right>_F.
\end{equation}
We then conduct the singular value decomposition to the matrix $\bm{W}_{Q_1}\bm{W}_{Q_2}^\top+\bm{W}_{K_1}\bm{W}_{K_2}^\top+\bm{b}_{Q_1}^\top\bm{b}_{Q_2}+\bm{b}_{K_1}^\top\bm{b}_{K_2}=\bm{U}\bm{\Sigma}\bm{V}^\top$ and have:
\begin{equation}
% \small
\begin{aligned}
&\max_{\bm{R}\in\mathcal{R}}\left<\bm{R},\bm{W}_{Q_1}\bm{W}_{Q_2}^\top+\bm{W}_{K_1}\bm{W}_{K_2}^\top+\bm{b}_{Q_1}^\top\bm{b}_{Q_2}+\bm{b}_{K_1}^\top\bm{b}_{K_2}\right>_F \\
=&\max_{\bm{R}\in\mathcal{R}}\mathrm{tr}\left(\bm{V}\bm{\Sigma}\bm{U}^\top\bm{R}\right) \\
=&\max_{\bm{R}\in\mathcal{R}}\mathrm{tr}\left(\bm{\Sigma}\bm{U}^\top\bm{R}\bm{V}\right) \\
=&\mathrm{tr}(\bm{\Sigma}),
\end{aligned}
\end{equation}
where $\bm{U}^\top\bm{R}\bm{V}=\bm{I}$, \textit{i.e.}, $\bm{R}=\bm{U}\bm{V}^\top$.
Finally, we incorporate $\bm{R}=\bm{U}\bm{V}^\top$ into $\bm{R}=\bm{R}_1\bm{R}_2^{-1}$ and let $\bm{R}_2=\bm{I}$ for simplicity.
Subsequently, we have $\bm{R}_1=\bm{U}\bm{V}^\top$ and $\bm{R}_2=\bm{I}$ as a closed-form solution of the optimization problem in \Cref{thm:solution}.

\end{proof}

From the proof, we can observe that the optimization problem in \Cref{thm:solution} has infinite pairs of solutions $\{\bm{R}_1,\bm{R}_2\}$ regarding the value of $\bm{R}_2$.
In this paper, we let $\bm{R}_2=\bm{I}$ for simplicity, which makes model 2 an anchor model.
However, the value of $\bm{R}_2$ can be further adjusted to benefit some data-dependent model fusion techniques~\citep{singh2020model,jin2023dataless}.
We leave this part in future works.

\begin{algorithm}[t]
\caption{Matching two self-attention layers.}\label{alg:attn_matching}
\textbf{Input:} Model parameters of the attention layers \{$\bm{W}_{Q_k}^h$,$\bm{b}_{Q_k}^h$,$\bm{W}_{K_k}^h$,$\bm{b}_{K_k}^h$,$\bm{W}_{V_k}^h$,$\bm{b}_{V_k}^h$,$\bm{W}_{O_k}^h$,$\bm{b}_{O_k}^h$\}$_{k=1,2;h=1,\dots,H}$. \\
\textbf{Output:} The optimally matched parameters from source parameters ($k=1$) to anchor parameters ($k=2$, maintain unchanged).
\begin{algorithmic}[1]
\STATE QK solution: $\bm{R}_{qk1}^h=\bm{U}_{qk}^h(\bm{V}_{qk}^h)^\top,\bm{R}_{qk2}^h=\bm{I}$, where $\bm{U}_{qk}^h\bm{\Sigma}_{qk}^h(\bm{V}_{qk}^h)^\top=\bm{W}_{Q_1}^h(\bm{W}_{Q_2}^h)^\top+\bm{W}_{K_1}^h(\bm{W}_{K_2}^h)^\top+(\bm{b}_{Q_1}^h)^\top\bm{b}_{Q_2}^h+(\bm{b}_{K_1}^h)^\top\bm{b}_{K_2}^h$.
\STATE QK matching: $\bm{W}_{Q_1}^h\rightarrow(\bm{R}_{qk1}^h)^\top\bm{W}_{Q_1}^h$, $\bm{W}_{K_1}^h\rightarrow(\bm{R}_{qk1}^h)^\top\bm{W}_{K_1}^h$, $\bm{b}_{Q_1}^h\rightarrow\bm{b}_{Q_1}^h\bm{R}_{qk1}^h$, $\bm{b}_{K_1}^h\rightarrow\bm{b}_{K_1}^h\bm{R}_{qk1}^h$.
\STATE VO solution: $\bm{R}_{vo1}^h=\bm{U}_{vo}^h(\bm{V}_{vo}^h)^\top,\bm{R}_{vo2}^h=\bm{I}$, where $\bm{U}_{vo}^h\bm{\Sigma}_{vo}^h(\bm{V}_{vo}^h)^\top=\bm{W}_{V_1}^h(\bm{W}_{V_2}^h)^\top+(\bm{W}_{O_1}^h)^\top\bm{W}_{O_2}^h+(\bm{b}_{V_1}^h)^\top\bm{b}_{V_2}^h$.
\STATE VO matching: $\bm{W}_{V_1}^h\rightarrow(\bm{R}_{vo1}^h)^\top\bm{W}_{V_1}^h$, $\bm{W}_{O_1}^h\rightarrow\bm{W}_{O_1}^h\bm{R}_{vo1}^h$, $\bm{b}_{V_1}^h\rightarrow\bm{b}_{V_1}^h\bm{R}_{vo1}^h$.
\end{algorithmic}
\end{algorithm}

\section{Algorithm Pseudo Code}
We provide the pseudo code of the parameter matching algorithm for self-attention layers in~\Cref{alg:attn_matching}.

\section{Comparison with Related Work}
% Git-Rebasin~\citep{ainsworth2023git} designs three model fusion algorithms for weight matching, activation matching, and straight-through matching based on permutation symmetry.
% Although Git-Rebasin works well for CNNs and MLPs, their method cannot be easily adapted to transformers.
% Different from permutation symmetry, rotation symmetry extends the symmetry space to a continuous space, consequently yielding a closed-form solution in the application of parameter matching (in a discrete space, the parameter matching problem is equivalent to a sum of bilinear assignment problem which is NP hard~\citep{ainsworth2023git}).
% Additionally, our study highlights the advantage of weight matching in serving as a plug-and-play module to enhance the model fusion methods because of its capability to preserve the model functionality after matching.
% Mentioning this point, OT-ACTS~\citep{imfeld2024transformer} provides a model fusion method based on optimal transport. 
% Although OT-ACTS includes a parameter matching step as well, the matching module is limited to permutation operations, and they fix the fusion method as simple merging.
% In contrast, our method extends parameter matching to rotation operations and is incorporated with advanced merging methods, showcasing the potential of parameter matching to enhance model fusion.
% \citeauthor{tran2024equivariant} proposes a concurrent work with a similar form of rotation symmetry. 
% However, this symmetry is only used to construct functional networks for transformers. 
% In comparison, we focus on the role of rotation symmetry in model fusion, proposing a theoretically optimal parameter matching algorithm. 

Git-Rebasin~\citep{ainsworth2023git} introduces three model fusion algorithms: weight matching, activation matching, and straight-through matching, based on permutation symmetry. 
While these methods are effective for CNNs and MLPs, they do not easily extend to transformers due to the discrete nature of permutation symmetry. 
In a discrete space, the parameter matching problem is equivalent to solving a sum of bilinear assignment problems, which is NP-hard~\citep{ainsworth2023git}.
To address this limitation, we introduce rotation symmetry, which extends the symmetry space to a continuous domain, allowing for a closed-form solution to parameter matching. 
Unlike permutation symmetry, rotation symmetry provides a more flexible and efficient solution for aligning parameters in transformers. 

Additionally, our study emphasizes the advantage of weight matching as a plug-and-play module for model fusion, as it preserves model functionality while reducing the inner distance of end models after matching. 
OT-ACTS~\citep{imfeld2024transformer} also includes a parameter matching step, leveraging optimal transport for model fusion. 
However, its matching module is restricted to permutation operations, and its fusion method is limited to simple merging. 
In contrast, our approach generalizes parameter matching to rotation operations and integrates it with more advanced merging strategies, demonstrating the potential of parameter matching to enhance model fusion.

A concurrent work by~\citet{tran2024equivariant} explores a similar form of rotation symmetry. However, their focus is on constructing functionally equivalent networks for transformers, whereas we focus on leveraging rotation symmetry for model fusion, proposing a theoretically optimal parameter matching algorithm specifically designed to improve fusion performance.



\section{Experimental Details}\label{app:b}

\paragraph{Datasets.}
Similar with~\citep{jin2023dataless}, We employ emotion classification and named entity recognition (NER) as the tasks for the main experiments.
The emotion classification datasets are extracted from~\citep{bostan-klinger-2018-analysis}. 
Five of them are selected as in-domain datasets (emoint, ssec, electoraltweets, grounded\_emotions, affectivetext), and five others are good datasets (dailydialog, crowdflower, tec, tales-emotion, isear). 
For the NER task, we choose OntoNotes~\citep{hovy-etal-2006-ontonotes} for model finetuning and CoNLL~\citep{tjong-kim-sang-de-meulder-2003-introduction} for out-of-domain evaluation.
Additionally, in the ablation study and subset layers matching, we employ datasets (STS-B, SST-2, CoLA) from GLUE~\citep{wang2019glue} to analyze the module performance. 

\paragraph{Baselines.}
To address RQ1, we selected three merging methods as baselines: Simple~\citep{wortsman2022model}, Fisher~\citep{matena2022merging}, and RegMean~\citep{jin2023dataless}. Meanwhile, to address RQ2, we further compared the performance of our method with two matching method~\citep{singh2020model, ainsworth2023git}. The code for the baselines are based on~\citep{jin2023dataless}.
% TODO: domain specific
% TODO: ensemble
% TODO: Average

\section{Limitations}

Similar to previous parameter matching algorithms, our proposed method can only handle the end models with exactly the same architecture.
Considering that different pre-trained transformers can have different architectures (most the same, but with slightly different modules), we leave merging different pre-trained transformers as future works.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
