\section{Related Work}
\stitle{Guardrails}
Recent research on guardrail models has mainly explored two key approaches: rule-based filtering and LLM-based safety classifiers.

Early guardrail models relied on rule-based filtering~\cite{welbl-etal-2021-challenges-detoxifying, clarke-etal-2023-rule,DBLP:conf/fat/GomezMPC24}, which uses predefined keyword lists and heuristic constraints to identify harmful content. While these methods are transparent and efficient, they suffer from rigidity and poor adaptability, leading to often issues of false positives (overblocking benign content;~\cite{DBLP:conf/chi/SongLLKK23}) and false negatives (failing to detect nuanced harms;~\cite{DBLP:journals/corr/abs-2212-05926}). Due to these limitations, research has shifted towards LLM-based classifiers, which provide greater flexibility by leveraging large models for content assessment.

LLM-based safety classifiers, such as LLaMA Guard~\cite{DBLP:journals/corr/abs-2312-06674, DBLP:journals/corr/abs-2411-17713, DBLP:journals/corr/abs-2411-10414} and Aegis Guard~\cite{DBLP:journals/corr/abs-2404-05993}, improve moderation by fine-tuning models on safety datasets, enabling them to classify inputs according to predefined safety guidelines. Unlike rule-based approaches, these classifiers leverage large models to capture different categories of harmful content, making them more adaptable to diverse regulatory requirements and shifting safety standards. However, most LLM-based guardrails operate in a single-pass classification manner, resulting in their providing binary moderation decisions without deeper analysis. This approach makes them vulnerable to adversarial prompts and subtle harmful content that requires nuanced reasoning~\cite{DBLP:journals/corr/abs-2309-02705, DBLP:conf/lamps/Zhu0ZW0WY000024}. Recent research, such as ShieldGemma~\cite{DBLP:journals/corr/abs-2407-21772} and WildGuard~\cite{DBLP:conf/nips/HanREJL00D24}, attempts to improve guardrail models through larger training datasets and fine-tuning on adversarial examples, but challenges in interpretability and robustness persist.

\stitle{LLM Inference with Slow Thinking}
The concept of ``slow thinking'' in Large Language Models is inspired by the psychological Dual-Process Theory~\cite{thompson2009dual}, which differentiates between fast, intuitive thought processes (System 1) and slow, deliberate reasoning (System 2). Integrating slow thinking into LLMs aims to enhance their reasoning capabilities by enabling more deliberate and structured problem-solving approaches.

Recent research has explored various methods for implementing ``slow thinking'' in LLMs' decision making. For instance, \citet{hagendorff2022thinking} evaluated the performance of proprietary models such as GPT-3 on cognitive reflection tests (CRT) and found that these models exhibited behavior similar to human intuition, as well as the cognitive errors that come with it. In a related effort, OpenAI introduced o1~\cite{openai2024learning}, a model trained to deliberate longer before responding, thereby improving performance on complex tasks. Unlike earlier models that often required explicit chain-of-thought prompting, o1 naturally engages in step-by-step reasoning and dynamically adapts its responses to context. In addition, \citet{DBLP:journals/corr/abs-2412-09413} proposes a framework for imitation, exploration, and self-improvement that significantly improves the model's performance on complex reasoning tasks through a reward-guided tree search algorithm. Another notable work is by~\citet{DBLP:journals/corr/abs-2310-18075}, which adopts a dual-mind mechanism, using two generative LLMs to handle fast and slow thinking respectively. The fast thinking model is responsible for initial response generation, while the slow thinking model carries out detailed planning, reasoning, and tool use to provide a well-considered answer.