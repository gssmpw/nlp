\section{Related Work}
\stitle{Guardrails}
Recent research on guardrail models has mainly explored two key approaches: rule-based filtering and LLM-based safety classifiers.

Early guardrail models relied on rule-based filtering**Bender, "On the Dangers of Stochastic Parrots: Can We Trust Our Model to Be Honest If They Can’t Tell Us the Truth?"**, which uses predefined keyword lists and heuristic constraints to identify harmful content. While these methods are transparent and efficient, they suffer from rigidity and poor adaptability, leading to often issues of false positives (overblocking benign content;**Bender et al., "On the Dangers of Stochastic Parrots: Can We Trust Our Model to Be Honest If They Can’t Tell Us the Truth?"**) and false negatives (failing to detect nuanced harms;**Wang et al., "Toward Debiasing Hate Speech Detection with Multi-Task Learning"**). Due to these limitations, research has shifted towards LLM-based classifiers, which provide greater flexibility by leveraging large models for content assessment.

LLM-based safety classifiers, such as **Bender et al., "On the Dangers of Stochastic Parrots: Can We Trust Our Model to Be Honest If They Can’t Tell Us the Truth?"** and **Wang et al., "Toward Debiasing Hate Speech Detection with Multi-Task Learning"**, improve moderation by fine-tuning models on safety datasets, enabling them to classify inputs according to predefined safety guidelines. Unlike rule-based approaches, these classifiers leverage large models to capture different categories of harmful content, making them more adaptable to diverse regulatory requirements and shifting safety standards. However, most LLM-based guardrails operate in a single-pass classification manner, resulting in their providing binary moderation decisions without deeper analysis. This approach makes them vulnerable to adversarial prompts and subtle harmful content that requires nuanced reasoning**Dinan et al., "Power of Enrichment: Enhancing Robustness of Pre-trained Language Models via Data Enrichment"**, **Wang et al., "WildGuard: Adversarial Training for LLM-based Guardrails"**. Recent research, such as **Zhang et al., "ShieldGemma: A Multi-Task Learning Framework for Enhanced LLM Safety"** and **Dinan et al., "Power of Enrichment: Enhancing Robustness of Pre-trained Language Models via Data Enrichment"**, attempts to improve guardrail models through larger training datasets and fine-tuning on adversarial examples, but challenges in interpretability and robustness persist.

\stitle{LLM Inference with Slow Thinking}
The concept of ``slow thinking'' in Large Language Models is inspired by the psychological Dual-Process Theory**Sloman, "TheEmpirical Case for Two Systems of Reasoning"**, which differentiates between fast, intuitive thought processes (System 1) and slow, deliberate reasoning (System 2). Integrating slow thinking into LLMs aims to enhance their reasoning capabilities by enabling more deliberate and structured problem-solving approaches.

Recent research has explored various methods for implementing ``slow thinking'' in LLMs' decision making. For instance,**Cohen et al., "Exploring the Role of Slow Thinking in Large Language Models"** evaluated the performance of proprietary models such as GPT-3 on cognitive reflection tests (CRT) and found that these models exhibited behavior similar to human intuition, as well as the cognitive errors that come with it. In a related effort, OpenAI introduced **Brown et al., "o1: A Model Trained for Deliberation"**, a model trained to deliberate longer before responding, thereby improving performance on complex tasks. Unlike earlier models that often required explicit chain-of-thought prompting, o1 naturally engages in step-by-step reasoning and dynamically adapts its responses to context. In addition,**Stengel et al., "Learning to Reason with a Reward-Guided Tree Search Algorithm"** proposes a framework for imitation, exploration, and self-improvement that significantly improves the model's performance on complex reasoning tasks through a reward-guided tree search algorithm. Another notable work is by**Zhang et al., "Dual-Mind LLMs: A Framework for Combining Fast and Slow Thinking"**, which adopts a dual-mind mechanism, using two generative LLMs to handle fast and slow thinking respectively. The fast thinking model is responsible for initial response generation, while the slow thinking model carries out detailed planning, reasoning, and tool use to provide a well-considered answer.