\section{Experiments}

% \subsection{Model}

% We utilize the PyTorch Lightning machine learning framework to finetune a ResNet18 model pre-trained on ImageNet. We use a learning rate of 1e-5. In the training loop we use binary cross entropy loss and predict the two logits (right and left counts) separately. On the last layer we implement a rectified linear unit as the activation function since only non-negative counts are possible.

Our best-performing model achieves an overall nMAE on KL-val of 23\% and 30.7\% on KR (\cref{tab:datasets}).%, demonstrating the feasibility of our echogram-based approach. 
The count error on downstream-moving fish is especially high (\cref{tab:datasets}), due partly to an extreme class imbalance between downstream- and upstream-moving fish in all training sets. In addition, the model systematically predicts lower counts than ground truth in clips with large numbers of fish (\cref{fig:preds_gt}), where separate tracks on an echogram may overlap and become difficult to distinguish.

These error rates are higher than state-of-the-art detector-tracker pipelines for salmon counting: Kay et al.~\cite{kay2022caltechfishcountingdataset} achieved 4.9\% error on KL-val and 11.8\% on KR using a YOLOv5m detector, and reduced these errors to 3.3\% error on KL-val and 3.7\% error on KR using a more complex input representation. However, our results are comparable to initial results published by the same team~\cite{kulits2020automated} that reported counting error rates of 19.3\%, indicating the potential to improve our results in future work.
% reduced these errors to between 3.7\%  In comparison, the detector-tracker pipeline developed by \cite{kay2022caltechfishcountingdataset} achieves 3.3\% error on KL-val and 3.7\% error on KR. 

Our experiments in \cref{tab:datasets}, \cref{tab:echogram-params}, and \cref{tab:dataaug} demonstrate that our model's performance is improved by incorporating both weak and strong labels during training, tuning the echogram generation parameters, and by applying domain-specific data augmentations during training. We ablate these contributions next.

\subsection{Training data}
\vspace{-10pt}
\begin{table}[h]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{lcccccc}
\toprule
 & \multicolumn{3}{c}{\textbf{KL-val nMAE (\%)} $\downarrow$} & \multicolumn{3}{c}{\textbf{KR nMAE (\%)} $\downarrow$} \\
\cmidrule(lr){2-4}\cmidrule(lr){5-7}
\textbf{Training set} & Total & Down & Up & Total & Down & Up \\
\midrule
KL-train {\footnotesize(\cite{kay2022caltechfishcountingdataset}, 481 imgs)} & 42.1 & 100.0 & 39.4 & 61.0 & 102.7 & 57.9 \\
KL-weak {\footnotesize(Ours, 33k imgs)} & 44.3 & 112.5 & 41.1 & 34.8 & 96 & 30.3 \\
\textbf{KL-train + KL-weak} & \textbf{23.0} & \textbf{37.5} & \textbf{22.3} & \textbf{30.7} & \textbf{96.0} & \textbf{25.8} \\
% KL-dt-train + KL-train + KL-dt-test & 30.6 & 62.5 & 29.1 & 30.4 & 92.0 & 25.8 \\
\bottomrule
\end{tabular}
} %\resizebox
\vspace{-5pt}
\caption{\footnotesize 
 Dataset choice vs performance on KL-val and KR, split by downstream (``down'') and upstream (``up'') moving fish.  Training with strong and weak labels improves over both a small dataset of strong labels only, and a large, diverse dataset of weak labels only.}
 \vspace{-5pt}
\label{tab:datasets}
\end{table}

We train the model on three different datasets: one composed of weak labels only; one composed of strong labels only; and one composed of a mixture of all weak labels and strong labels. In \cref{tab:datasets}, the drastic difference between nMAE for the model trained only on strong labels vs the mixture of weak and strong labels (about a 20\% improvement for KL-val and 30\% improvement for KR) indicates that training on a large, diverse dataset improves the model despite the potential inaccuracies present in the weak labels. The inclusion of strong labels, which make up less than 2\% of the total dataset size, also significantly improves model performance compared to the model trained on weak labels only, especially on the in-distribution test set (KL-val). 

\subsection{Echogram generation parameters}

\vspace{-10pt}

\begin{table}[h]
    \centering
    \footnotesize
    \begin{tabular}{ccccc}
        \toprule
        \multicolumn{4}{c}{\textbf{Echogram params}} & \textbf{nMAE (\%) $\downarrow$} \\
        \cmidrule(lr){1-4} \cmidrule(lr){5-5}
        $\alpha_0$ & $\alpha_1$ & $\alpha_2$ & \textit{size\_thresh} & KL-val \\
        \midrule
        0 & 0 & 0 & 0 & 84.7 \\
        20 & 0 & 0 & 0 & 36.6 \\
        \textbf{20} & \textbf{40} & \textbf{60} & \textbf{100} & \textbf{23.0}  \\
        20 & 40 & 100 & 120 & 37.2 \\
        \bottomrule
    \end{tabular}
    \vspace{-5pt}
    \caption{\footnotesize Echogram generation parameters vs performance on KL-val and KR for models trained and validated on a mixture of weak and strong labels. The model performs best at some intermediate setting where a balance is achieved between filtering out background noise and preserving information about fish tracks.}
    \vspace{-5pt}
    \label{tab:echogram-params}
\end{table}

When generating the echogram slices used as input and test data for the model, thresholds for initial background subtraction, secondary background subtraction, and filtering based on size can be tuned. Higher thresholds lead to information loss but also produce a cleaner, less noisy signal for the model. Testing different sets of thresholds as in \cref{tab:echogram-params} shows that an intermediate setting is ideal: the model benefits from some filtering of noise but is negatively affected by cutting background signal too aggressively.

% \begin{table*}[t]
%     \centering
%     \begin{tabular}{cccccccccccc}
%         \toprule
%         \multicolumn{4}{c}{\textbf{Echogram generation parameters}} & \multicolumn{3}{c}{\textbf{KL-val nMAE (\%)}} & \multicolumn{3}{c}{\textbf{KR nMAE (\%)}} \\
%         \cmidrule(lr){1-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10}
%         $\alpha_0$ & $\alpha_1$ & $\alpha_2$ & \textit{size\_thresh} & \textit{Total} & \textit{Left} & \textit{Right} & \textit{Total} & \textit{Left} & \textit{Right} \\
%         \midrule
%         0 & 0 & 0 & 0 & & & & & & & & \\
%         20 & 0 & 0 & 0 & & & & & & & & \\
%         20 & 40 & 60 & 100 & 23.0 & 37.5 & 22.3 & 30.7 & 96.0 & 25.8 \\
%         20 & 40 & 125 & 135 & & & & & & & & \\
%         \bottomrule
%     \end{tabular}
%     \caption{Echogram generation parameters vs performance on KL-val and KR, for models trained on the KL-dt-train + KL-train dataset and validated on the KL-dt-val + KL-val dataset.}
%     \label{tab:echogram-params}
% \end{table*}



\subsection{Data augmentations}

% By testing different combinations of data augmentations, shown in Table \cref{tab:dataaug}, we find a most useful set of augmentations to apply to the weak labels and strong labels. 

We explore various data augmentation strategies, informed by the specifics of the echogram image domain, in \cref{tab:dataaug}.
% also apply a set of transformations to augment the data as it's being loaded in as a batch during training. All available augmentations are listed below in order of application.

\noindent
\textbf{Vertical flip.} Flipping the entire image across the horizontal axis improves nMAE for all model setups.

\noindent
\textbf{Naive horizontal flip.} Regardless of the set of labels the model is trained on, nMAE worsens when a horizontal flip augmentation is applied, which flips the entire image. In all training and validation sets, a class imbalance between upstream- and downstream-traveling fish exists: during spawn season, many more fish are swimming upstream than downstream. In KL-val, 175 fish are swimming upstream while only 8 are swimming downstream. In addition, the upstream and downstream motion patterns of fish are different due to the direction of the river current. This naive horizontal flip augmentation thus both obscures the true distribution of upstream vs. downstream counts and does not accurately capture the motion of the fish in the opposite direction, suggesting that a different method is needed to robustly classify downstream-swimming fish.

\noindent
\textbf{Realistic horizontal flip.} This transformation reflects the image across the horizontal axis and then inverts the lateral position channel to match the original, pre-reflection direction of fish motion. This improves nMAE across all model setups but does not improve the left-right class imbalance.

\noindent
\textbf{Superposition.} Two echograms are superposed, displaying at each point the intensity and color of the brightest pixel; the target counts are added together. This augmentation has mixed effects on performance, modestly improving training on weak labels while worsening training on strong labels.


% \begin{table*}[h!]
% \centering
% \begin{tabular}{llcccccccccccc}
% \toprule
% \multicolumn{2}{c}{\textbf{Datasets}} & \multicolumn{4}{c}{\textbf{Data Augmentations}} & \multicolumn{3}{c}{\textbf{KL-val nMAE (\%)}} & \multicolumn{3}{c}{\textbf{KR nMAE (\%)}} \\
% \cmidrule(lr){1-2} \cmidrule(lr){3-6} \cmidrule(lr){7-9} \cmidrule(lr){10-12}
% \textbf{Train set} & \textbf{Val set} & V. flip & H. flip & Superpos. & Realistic h. flip & Total & Left & Right & Total & Left & Right \\
% \midrule
% KL-dt-train & KL-dt-val & & & & & 64.5 & & & 48.1 & 106.7 & 43.8 \\
% KL-dt-train & KL-dt-val & \textbullet & & & & 49.2 & 100 & 46.9 & 38 & 94.7 & 33.8 \\
% KL-dt-train & KL-dt-val & \textbullet & \textbullet & & & 53.6 & 137.5 & 49.7 & 43.6 & 86.7 & 40.4 \\
% KL-dt-train & KL-dt-val & \textbullet & \textbullet & \textbullet & & 49.7 & 75 & 48.6 & 40.3 & 84 & 37 \\
% KL-dt-train & KL-dt-val & \textbullet & & \textbullet & \textbullet & 37.2 & 50 & 36.6 & 32.9 & 89.3 & 28.7 \\

% \midrule
% KL-train & KL-val & & & & & 48.1 & 100 & 45.7 & 80.2 & 101.3 & 78.6 \\
% KL-train & KL-val & \textbullet & & & & 43.2 & 100 & 46 & 76.9 & 101.3 & 75.0 \\
% KL-train & KL-val & \textbullet & \textbullet & & & 111.5 & 950 & 73.1 & 126.9 & 650.7 & 157 \\
% KL-train & KL-val & \textbullet & \textbullet & \textbullet & & 129 & 1100 & 84.6 & 146.8 & 794.7 & 98.7 \\
% KL-train & KL-val & \textbullet & & & \textbullet & 39.3 & 100 & 36.6 & 67.7 & 101.3 & 65.2 \\
% KL-train & KL-val & \textbullet & & \textbullet & \textbullet &  &  &  &  &  &  \\
% \bottomrule
% \end{tabular}
% \label{tab:dataaug}
% \caption{Performance of model vs data augmentations applied to purely detector-tracker annotated dataset and purely manually annotated dataset, using cleanest echogram generation settings.}
% \end{table*}


\begin{table}[h]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{cccccc}
\toprule
\multicolumn{4}{c}{\textbf{Data augmentations}}  & \multicolumn{2}{c}{\textbf{Train set}} \\
\cmidrule(lr){1-4}\cmidrule(lr){5-6}
V. flip & H. flip & Superpos. & Realistic h. flip & KL-train & KL-weak \\
\midrule
& & & & 48.1 & 64.5 \\
\textbullet & & & & 43.2 & 49.2 \\
\textbullet & \textbullet & & & 111.5 & 53.6 \\
\textbullet & \textbullet & \textbullet & & 129 & 49.7 \\
\textbullet & & \textbullet & \textbullet & 74.3 & \textbf{37.2} \\
\textbullet & & & \textbullet & \textbf{39.3} & 44.3 \\
\bottomrule
\end{tabular}
} % /resizebox
\vspace{-5pt}
\caption{\footnotesize 
 KL-val nMAE using either KL-train or KL-weak and ablating data augmentations, using cleanest echogram generation settings. A naive horizontal flip augmentation worsens the performance of the model trained on either dataset, while vertical flip and our domain-specific realistic horizontal flip improve results.}
 \vspace{-12pt}
\label{tab:dataaug}
\end{table}

\input{fig/preds_vs_gt}