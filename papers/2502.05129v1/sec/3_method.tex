\section{Method}

\subsection{Echogram generation}
\label{sec:echogram}

We begin with sonar video files in the ARIS format~\cite{soundmetrics_ARIS}. Each file represents 10-20 minutes of continuous sonar footage which may or may not contain fish. At each range (radial distance from the sonar camera) a certain angular span is sampled, % by 48 or 96 sonar beams radiating from the camera, 
outputting a pixel intensity corresponding to the strength of the echo received. 

To generate the echogram, we apply successive iterations of background subtraction to each frame of the ARIS file, as in \cref{fig:echogram}. In each application of background subtraction, after taking the mean frame across the ARIS file, only the pixels of each frame exceeding a threshold value $\alpha$ above the corresponding pixel of the mean frame are kept. 

First we apply background subtraction to the raw frame with a low threshold value $\alpha_0$. Then OpenCV's ConnectedComponentsWithStats function obtains all connected components in the new image which are larger than a size threshold scaled by range. Finally, we apply background subtraction once more with a threshold value $\alpha_1$ within these components and $\alpha_2$ outside these components such that $\alpha_0 < \alpha_1 < \alpha_2$.
The values of each threshold are tuned by trial and error until a qualitatively acceptable echogram is produced, and we use those same parameters for all data.
% then the same thresholds are applied with good generalization to the rest of the clips from that channel. 

% table or list of echogram generation parameters? like what the thresholds all mean, and the values we tested?

This version of the clip, with each frame cleaned of background noise, is used for echogram generation. Each frame of shape (number of samples along range) $\times$ (number of beams) is collapsed into a column of height (number of samples along range), each pixel of the column corresponding to the maximum intensity at that range of the various beams. A second image channel stores the lateral position of that maximum intensity point, normalized between 0 and 1. Concatenating these columns together gives the full 2D echogram, of shape (number of samples along range) $\times$ (number of frames in video).

\subsection{Computer vision model}

We train a computer vision model in the PyTorch Lightning machine learning framework to predict left and right counts for echogram images. We finetune a ResNet18 model pre-trained on ImageNet with a final fully connected layer that contains two outputs corresponding to left and right counts. We use a ReLU activation function after the final layer (since counts must be non-negative) and optimize for mean squared error. % loss and predict the two logits (right and left counts) separately. On the last layer we implement a rectified linear unit as the activation function since only non-negative counts are possible. 
We use an input size of 200px by 800px, learning rate of 1e-5 using Adam optimization, batch size of 256, and train for a maximum of 100 epochs on a single NVIDIA A100 GPU with early stopping based on KL-val (see \cref{sec:data}) performance.