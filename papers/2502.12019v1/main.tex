%Version 2.1 April 2023
% See section 11 of the User Manual for version history
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                 %%
%% Please do not use \input{...} to include other tex files.       %%
%% Submit your LaTeX manuscript as one .tex document.              %%
%%                                                                 %%
%% All additional figures and files should be attached             %%
%% separately and not embedded in the \TeX\ document itself.       %%
%%                                                                 %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%\documentclass[referee,sn-basic]{sn-jnl}% referee option is meant for double line spacing

%%=======================================================%%
%% to print line numbers in the margin use lineno option %%
%%=======================================================%%

%%\documentclass[lineno,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style

%%======================================================%%
%% to compile with pdflatex/xelatex use pdflatex option %%
%%======================================================%%

%%\documentclass[pdflatex,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style


%%Note: the following reference styles support Namedate and Numbered referencing. By default the style follows the most common style. To switch between the options you can add or remove �Numbered� in the optional parenthesis. 
%%The option is available for: sn-basic.bst, sn-vancouver.bst, sn-chicago.bst, sn-mathphys.bst. %  
 
%%\documentclass[sn-nature]{sn-jnl}% Style for submissions to Nature Portfolio journals
%%\documentclass[sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style
\documentclass[sn-mathphys,Numbered]{sn-jnl}% Math and Physical Sciences Reference Style
%%\documentclass[sn-aps]{sn-jnl}% American Physical Society (APS) Reference Style
%%\documentclass[sn-vancouver,Numbered]{sn-jnl}% Vancouver Reference Style
%%\documentclass[sn-apa]{sn-jnl}% APA Reference Style 
%%\documentclass[sn-chicago]{sn-jnl}% Chicago-based Humanities Reference Style
%%\documentclass[default]{sn-jnl}% Default
%%\documentclass[default,iicol]{sn-jnl}% Default with double column layout


%%%% Standard Packages
%%<additional latex packages if required can be included here>

\usepackage{graphicx}%
\usepackage{multirow}%
\usepackage{amsmath,amssymb,amsfonts}%
\usepackage{amsthm}%
\usepackage{mathrsfs}%
\usepackage[title]{appendix}%
\usepackage{xcolor}%
\usepackage{textcomp}%
\usepackage{manyfoot}%
\usepackage{booktabs}%
\usepackage{algorithm}%
\usepackage{subcaption}%
\usepackage{algorithmicx}%
\usepackage{algpseudocode}%
\usepackage{listings}%
\usepackage{amssymb}
\usepackage{latexsym}
\usepackage{url}
\usepackage{amsmath,amssymb,amsfonts,bm}
\usepackage{booktabs}
\usepackage{arydshln}
\usepackage{tabulary}
\usepackage{graphicx,epstopdf}
\usepackage{textcomp}
\usepackage{subcaption}
\usepackage{booktabs}
%%%%
%%%%%=============================================================================%%%%
%%%%  Remarks: This template is provided to aid authors with the preparation
%%%%  of original research articles intended for submission to journals published 
%%%%  by Springer Nature. The guidance has been prepared in partnership with 
%%%%  production teams to conform to Springer Nature technical requirements. 
%%%%  Editorial and presentation requirements differ among journal portfolios and 
%%%%  research disciplines. You may find sections in this template are irrelevant 
%%%%  to your work and are empowered to omit any such section if allowed by the 
%%%%  journal you intend to submit to. The submission guidelines and policies 
%%%%  of the journal take precedence. A detailed User Manual is available in the 
%%%%  template package for technical guidance.
%%%%%=============================================================================%%%%
\newcommand{\revision}[1]{\textcolor{black}{#1}} %for displaying red texts

% \jyear{2023}%

% %% as per the requirement new theorem styles can be included as shown below
% \theoremstyle{thmstyleone}%
% \newtheorem{theorem}{Theorem}%  meant for continuous numbers
% %%\newtheorem{theorem}{Theorem}[section]% meant for sectionwise numbers
% %% optional argument [theorem] produces theorem numbering sequence instead of independent numbers for Proposition
% \newtheorem{proposition}[theorem]{Proposition}% 
% %%\newtheorem{proposition}{Proposition}% to get separate numbers for theorem and proposition etc.

% \theoremstyle{thmstyletwo}%
% \newtheorem{example}{Example}%
% \newtheorem{remark}{Remark}%

% \theoremstyle{thmstylethree}%
% \newtheorem{definition}{Definition}%

% \raggedbottom
% %%\unnumbered% uncomment this for unnumbered level heads

\begin{document}

\title{Robotic CBCT Meets Robotic Ultrasound}

% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
%%=============================================================%%
%% Prefix	-> \pfx{Dr}
%% GivenName	-> \fnm{Joergen W.}
%% Particle	-> \spfx{van der} -> surname prefix
%% FamilyName	-> \sur{Ploeg}
%% Suffix	-> \sfx{IV}
%% NatureName	-> \tanm{Poet Laureate} -> Title after name
%% Degrees	-> \dgr{MSc, PhD}
%% \author*[1,2]{\pfx{Dr} \fnm{Joergen W.} \spfx{van der} \sur{Ploeg} \sfx{IV} \tanm{Poet Laureate} 
%%                 \dgr{MSc, PhD}}\email{iauthor@gmail.com}
%%=============================================================%%

\author{\fnm{Feng} 
\sur{Li$^1$}}
\email{feng.li@tum.de}

\author{\fnm{Yuan} 
\sur{Bi$^1$}}
\email{yuan.bi@tum.de}

\author{\fnm{Dianye} 
\sur{Huang}}
\email{dianye.huang@tum.de}

\author*{\fnm{Zhongliang} 
\sur{Jiang}}
\email{zl.jiang@tum.de}

\author{\fnm{Nassir} 
\sur{Navab}}
\email{nassir.navab@tum.de}

% \author[2]{\fnm{Jo\"el L.} 
% \sur{Lavanchy}}
% % \email{joel.lavanchy@ihu-strasbourg.eu}

% \author[1,2]{\fnm{Vinkle} 
% \sur{Srivastav}}
% % \email{srivastav@unistra.fr}

% \author[1,2]{\fnm{Nicolas} 
% \sur{Padoy}}
% % \email{npadoy@unistra.fr}

\affil{CAMP, Technical University of Munich, Munich, Germany}
\affil{Munich Center of Machine Learning, Munich, Germany}
\affil[1]{Equal Contribution}
% \affil[1] {con}
% \affil[2]{IHU Strasbourg, France}
%\affil[4]{University Digestive Health Care Center Basel – Clarunis, Switzerland}
%%==================================%%
%% sample for unstructured abstract %%
%%==================================%%
\abstract{
% Cone beam computed tomography (CBCT) and ultrasound (US) are widely used imaging techniques in clinical practice. Robotic CBCT integrates not only the imaging advantage of the CBCT but also provides robotic flexible mobility to achieve optimal imaging views. However, CBCT still has imaging limitations, particularly in low contrast of soft tissue and the inability to visualize blood vessels. On the other hand, robotic ultrasound, particularly with Doppler capabilities, can precisely localize and deliver detailed imaging of soft tissues and vascular flow. The complementary strengths of these two imaging modalities are mutually enhancing, particularly when integrated with automated robotic systems, offering enhanced precision and versatility. In this paper, we presented a novel clinical setup in which robotic CBCT and robotic ultrasound are pre-calibrated and dynamically co-registered, which enables novel clinical applications in which a registration-free rigid registration allows for multi-modal guided procedures in the absence of deformation, as well as solid automatic initialization for deformable registration in case of soft tissue manipulation. Additionally, we developed a visualized robotic interactive system to simplify its application across multiple procedures. Several experiments were conducted using a liver-simulated phantom, which demonstrated a CBCT-US mapping error of 1.72 mm, providing strong evidence of the system's accuracy. These results underscore the potential of our system for clinical application.



% Cone beam computed tomography (CBCT) and ultrasound (US) are widely used imaging techniques in clinical practice. Robotic CBCT integrates not only the imaging advantage of the CBCT but also provides robotic flexible mobility to achieve optimal imaging views. However, CBCT still has imaging limitations, particularly in low contrast of soft tissue and the inability to visualize blood vessels. On the other hand, robotic ultrasound, particularly with Doppler capabilities, can precisely localize and deliver detailed imaging of soft tissues and vascular flow. The complementary strengths of these two imaging modalities are mutually enhancing, particularly when integrated with automated robotic systems, offering enhanced precision and versatility. In this paper, we presented a novel clinical setup in which robotic CBCT and robotic ultrasound are pre-calibrated and dynamically co-registered, which enables novel clinical applications in which a registration-free rigid registration allows for multi-modal guided procedures in the absence of deformation, as well as solid automatic initialization for deformable registration in case of soft tissue manipulation. Additionally, we developed a visualized robotic interactive system to simplify its application across multiple procedures. Several experiments were conducted using a liver-simulated phantom, which demonstrated a CBCT-US mapping error of 0.00 mm, providing strong evidence of the system's accuracy. These results underscore the potential of our system for clinical application.


\textbf{Purpose:}
The multi-modality imaging system offers optimal fused images for safe and precise interventions in modern clinical practices, such as \revision{computed tomography - ultrasound (CT-US)} guidance for needle insertion. However, the limited dexterity and mobility of current imaging devices hinder their integration into standardized workflows and the advancement toward fully autonomous intervention systems. In this paper, we present a novel clinical setup where robotic \revision{cone beam computed tomography (CBCT)} and robotic US are pre-calibrated and dynamically co-registered, enabling new clinical applications. This setup allows registration-free rigid registration, facilitating multi-modal guided procedures in the absence of tissue deformation.

\textbf{Methods:}
First, a one-time pre-calibration is performed between the systems. To ensure a safe insertion path by highlighting critical vasculature on the 3D CBCT, SAM2 segments vessels from B-mode images, using the Doppler signal as an autonomously generated prompt. Based on the registration, the Doppler image or segmented vessel masks are then mapped onto the CBCT, creating an optimally fused image with comprehensive detail. To validate the system, we used a specially designed phantom, featuring lesions covered by ribs and multiple vessels with simulated moving flow.

\textbf{Results:}
The mapping error between US and CBCT resulted in an average deviation of $1.72\pm0.62$ mm. A user study demonstrated the effectiveness of CBCT-US fusion for needle insertion guidance, showing significant improvements in time efficiency, accuracy, and success rate. Needle intervention performance improved by approximately 50\% compared to the conventional US-guided workflow.

\textbf{Conclusion:}
We present the first robotic dual-modality imaging system designed to guide clinical applications. The results show significant performance improvements compared to traditional manual interventions.
}
% \keywords{Visual question answering, Multi-modality learning, Surgical Data Science.}
\keywords{robotic ultrasound, multimodality fusion, visualization}
% The modern operating room requires innovative intra-operative support due to its evolving complexity. While surgical data science has mainly focused on video analysis, the future lies in integrating surgical computer vision with natural language. Hence, Visual Question Answering (VQA) is emerging in surgical data science, enabling surgeons to communicate with intelligent systems using language. In this work, we address two main challenges of current surgical VQA system: question-condition bias in datasets and the lack of scene-aware reasoning ability in models. Current benchmarks in surgical VQA simplify the task evaluation by providing question-condition biased question-answer pairs, in which the answers can be derived from the questions themselves. Our solution, the diagnostic \textbf{S}urgical \textbf{S}cene \textbf{G}raph-based dataset (SSG-QA), offers a more intricate, unbiased, and action-focused dataset. It reduces the bias and highlights VQA model's visual reasoning ability in complex surgical scenes. Additionally, we introduce SSG-QA-Net, a scene-aware network that enhances multi-modal representations using a proposed lightweight Scene-embedded Interaction Module (SIM). We use a trained object detector to extract scene embeddings, which are refined by the textual embeddings from the questions. This introduces the scene knowledge in the network before performing the multi-modal fusion. Our careful analysis on SSG-QA dataset provides fine-grained results for different question types and complexities, showing our model's superiority over existing methods and indicating that VQA's main limitation is the visual representation learning, guiding the future surgical VQA model design.}



%%\pacs[JEL Classification]{D8, H51}

%%\pacs[MSC Classification]{35A01, 65L10, 65L12, 65L20, 65L70}

\maketitle

\section{Introduction}
\par
Traditional medical interventions often rely on a single-image modality such as ultrasound (US), X-ray, or magnetic resonance (MRI). These single modalities have their own advantages and favorable use cases in reality. However, it is also commonly known that they often suffer from limitations such as sub-optimal contrast, limited image view, sensitivity to tissue properties, and also real-time performance. To tackle this problem, the dual-modality or multimodal imaging fused imaging system has been seen as a promising solution for further enhancing the safety, precision, and stability of future image-guided interventions. Pioneering examples are interventional single photon emissions tomographie-computed tomography (SPECT-CT) imaging system~\cite{gardiazabal2014towards} and CT-US system~\cite{monfardini2018ultrasound}. Such dual-modality systems can provide complementary information in terms of field of view or optimal visualizations of different tissues to have an ideal fused image. For example, the CT-US or cone beam computed tomography-US (CBCT-US) fusion is desired for guiding and tracking the needle insertion procedure, where the CBCT provides high-quality 3D images of hard tissues while US excels in soft tissue visualization and provides real-time, radiation-free guidance. This fused modality allows clinicians to leverage both static pre-operative 3D views and dynamic intraoperative imaging. The combination of US and CBCT has proven effective in applications such as liver ablation~\cite{monfardini2018ultrasound}, renal ablation~\cite{monfardini2021real}, and radiation therapy for uterine cervix cancer~\cite{mason2019combined}. 


\par
However, deploying two imaging devices in the operation room has practical challenges over the limited working space, altering the standard intervention workflow. To tackle this problem, state-of-the-art (SOTA) is considering motorizing image devices such as mobile robotic CTs~\cite{kim2019design, weir2015dosimetric, ebinger2015mobile} and robotic C-Arms~\cite{tsang2015real,tanaka2024low}. Recently, a mobile cone-beam computed tomography (CBCT) scanner called LoopX (medPhoton, Austria) was designed with great dexterity in both translational and rotational directions to meet the requirement of mobility~\cite{karius2024first}. By automating workflows, Loop-X is advancing mobile imaging with functionality that will be desirable for intervention procedures, particularly in combination with intraoperative US imaging modality. 

\par
To further eliminate the down facts of traditional free-hand US scanning in terms of reproducibility and standardization, the US probe is desired to be maneuvered by a robotic arm, particularly when we need to position the probe precisely to guide the needle insertion process to a target, such as the tumor in ablation procedure~\cite{li2024invisible, jiang2024needle}. Recent advances in the field of robotic US have demonstrated the superior performance of robots over humans in terms of precision, stability, and reproducibility~\cite{jiang2023robotic, bi2024machine, von2021medical, huang2024robot_qin, li12autonomous}. The integration of robotic CBCT and robotic US systems holds significant potential for future surgical applications, while this combination has yet to be explored in the research community.  

\par
In this work, we present the first robotic CBCT-US dual-modality imaging system, where the robotic CBCT and robotic US images are pre-calibrated and dynamically co-registered, which enables novel clinical applications in which a registration-free rigid registration allows for multi-modal guided procedures in the absence of deformation. Unlike previous approaches, our system leverages robotic precision to automatically determine and maintain the optimal probe positioning, ensuring accurate needle insertion without the need for manual adjustments. This capability is particularly valuable in procedures like liver ablation, where precise targeting of targets and avoiding critical structures such as blood vessels are crucial. To prove the fused image with ample information, we also map the Doppler images into the 3D CBCT volume or slice based on the registration. The effectiveness of the proposed system was validated on a carefully designed phantom with lesions covered by ribs and multiple vessels, including mimicked moving flow. \revision{Although deformation was not considered, the system primarily focuses on offering a robust initial registration framework that can be further optimized for future research on deformable registration methods.}




% \par
% Furthermore, the fusion of US and CBCT also has the potential to facilitate the development of autonomous robotic intra-operative assistive systems. Leveraging the 3D information and high-quality bone imaging provided by CBCT, it is possible to automatically determine the optimal acquisition pose for the selected region of interest while avoiding the occlusion of bones~\cite{gobl2017acoustic,al2021autonomous}. In procedures like liver ablation, for instance, the system can autonomously position the US probe and guide the needle to the precise location, ensuring that the path avoids any interaction with blood vessels and accurately reaches the intended lesion.



% Robotic systems have attracted increasing attention in modern clinical procedures due to their precision, reproducibility, and stability. However, practical issues such as shared working space between humans and devices still remain. Moreover, the reduced operation space may affect surgery operations and their final performance. To tackle this problem, this paper presents a novel concept for motorizing the imaging devices and keeping them dynamically registered to each other. In this way, the multimodal images can compensate for each other to achieve the best imaging performance while mitigating the effect on the standard clinical procedure. As an example, we employ robotic ultrasound and motorized CBCT for better liver ablations based on the dynamically fused visualization procedure to demonstrate its potential for future surgery. 



% ---------------------------------jzl----------------------

% Cone Beam Computed Tomography (CBCT) is widely used in clinical practice due to its advantages, such as lower radiation exposure, higher accessibility, and cost-effectiveness compared to conventional Computed Tomography (CT). It provides high-quality 3D images of hard tissues like bones, making it ideal for dental and orthopedic surgery. However, its limited ability to differentiate soft tissues has restricted its use primarily to these fields, where soft tissue contrast is less critical. Additionally, CBCT serves primarily as a pre-operative imaging tool and lacks the capability to provide real-time intra-operative guidance, limiting its usefulness during surgical procedures where continuous monitoring is needed.

% \par
% In contrast, ultrasound (US) is an efficient imaging modality primarily used for visualizing soft tissues and blood vessels. It introduces no radiation to patients or surgeons, making it a safer option for repeated use. The real-time imaging capability of US is particularly valuable for intra-operative guidance. Its high flexibility and manipulability have contributed to its widespread use in daily clinical practice. However, these same characteristics can reduce the reproducibility and standardization of US imaging compared to more structured modalities like CT or CBCT. Moreover, unlike CBCT, which directly provides 3D images, most real-time US remains a 2D imaging modality, requiring operators to have advanced skills in navigation and mapping between 2D and 3D structures. This places a higher demand on the operator’s expertise. In light of these limitations, robotic US has gained increasing interest due to its potential to overcome these challenges.

% \par
% Recent advances in the field of robotic US have demonstrated the superior performance of robots over humans in terms of precision, stability, and reproducibility. With the implementation of various force control algorithms~\cite{gilbertson2015force,huang2024robot,pierrot1999hippocrate}, robotic US probes are able to maintain proper contact with the scanning surface, reducing the force-induced deformation during scanning~\cite{virga2018use,jiang2021deformation}. By applying orientation optimization methods, robotic US systems can automatically determine the optimal acquisition angle for improved image quality~\cite{jiang2020automatic1}. This optimization can be based on measured contact force~\cite{jiang2020automatic2} or the use of external cameras~\cite{wang2022full,huang2018robotic}. Benefiting from the high stability and reproducibility provided by the robotic systems, high-fidelity 3D US reconstruction can be achieved, offering more accurate and consistent reconstruction results compared to freehand reconstruction~\cite{velikova2024implicit,nadeau2013intensity}.

% % The quality of US images are dependent on several acquisition parameters, including proper contact with the scanning surface, contact force, orientation angles, etc. To standardize and increase the reproducibility of US imaging, researchers have implemented various force control algorithms~\cite{pierrot1999hippocrate,huang2024robot,gilbertson2015force} to stabilize the contact force during scanning, reducing the force induced deformation. Orientation optimization is also a crucial enabling technology for robotic US. Previous researchers have 

% \par
% Nonetheless, due to the inherent limitations of US in imaging hard tissues and its limited field of view, it falls short in areas where CBCT excels. CBCT provides visualization of hard tissues and full 3D imaging, making it a complementary technology to US. Combining CBCT with US offers significant advantages by merging their strengths: CBCT delivers 3D imaging of hard tissues, while US excels in soft tissue visualization and provides real-time, radiation-free guidance. This fused modality allows clinicians to leverage both static pre-operative 3D views and dynamic intraoperative imaging. The combination of US and CBCT has proven effective in applications such as liver ablation~\cite{monfardini2018ultrasound}, renal ablation~\cite{monfardini2021real}, and radiation therapy for uterine cervix cancer~\cite{mason2019combined}.

% \par
% Traditional registration between 2D US and 3D imaging modalities like CT, MRI, or CBCT poses significant challenges due to differences in imaging scales and the lack of distinct anatomical landmarks. To address these issues, multiple 2D US slices~\cite{sun2007image} or even 3D US reconstruction volumes~\cite{barratt2006self} are typically required for accurate registration. Rigid~\cite{wein2008automatic} or non-rigid~\cite{fuerst2014automatic} registration can be achieved by minimizing defined metrics that measure the correlation between the two modalities. However, the accuracy of the registration heavily depends on the quality of the initialization~\cite{che2017ultrasound}. Commonly, initialization relies on markers placed on the patient~\cite{kaspersen2003three} or identifiable anatomical landmarks~\cite{zeng2022learning}, often requiring manual alignment~\cite{huang2009dynamic} or additional segmentation models~\cite{jiang2024class}. Therefore, a system that provides built-in initialization would be highly valuable to simplify and improve the registration process. 

% \par
% Furthermore, the fusion of US and CBCT also has the potential to facilitate the development of autonomous robotic intra-operative assistive systems. Leveraging the 3D information and high-quality bone imaging provided by CBCT, it is possible to automatically determine the optimal acquisition pose for the selected region of interest while avoiding the occlusion of bones~\cite{gobl2017acoustic,al2021autonomous}. In procedures like liver ablation, for instance, the system can autonomously position the US probe and guide the needle to the precise location, ensuring that the path avoids any interaction with blood vessels and accurately reaches the intended lesion.

% % Such similarity metrics can be either intensity-based or feature-based.
% % Wein~\emph{et al.} proposed an effective intensity-based registration metric, LC2, which calculates a similarity measurement between the US image and the simulated US from CT volume~\cite{wein2008automatic}. Its applications have been found in both rigid~\cite{mazzucchi2023automatic} or non-rigid registrations~\cite{zeng2022learning, fuerst2014automatic}. Feature-based registration metrics have also shown their advantages in terms of high accuracy in critical structures.
% % The registration between the pre-operative 3D imaging and the intra-operative US is an essential step towards the utilization of strengths from both imaging modalities. Previous researches have shown the effectiveness of registering US with pre-operative images in neurosurgery~\cite{gobbi2000ultrasound}, abdominal interventions~\cite{kaspersen2003three}, and orthopedic surgery~\cite{barratt2006self}. Compared to 

% \par
% % In light of these advantages, we introduce a robotic CBCT-US system that provides registration-free rigid alignment between CBCT and US images, enabling fused visualization of both modalities. The system leverages US-specific information, such as Doppler and soft tissue visualization, to complement CBCT’s robust bone imaging and 3D perceptions. This fusion sets the groundwork for future non-rigid registrations. Furthermore, the robotic system is able to provide precise probe positioning for needle insertion guidance. We further validate the effectiveness of the proposed system in the challenging scenario of intra-operative liver ablation needle insertion guidance. Additionally, we develop an automatic needle insertion planning algorithm and implemented robotic control to provide automated needle guidance, serving as a corner stone for future translational studies.
% In light of these advantages, we introduce a robotic CBCT-US system that not only provides registration-free rigid alignment between CBCT and US images for fused visualization but also integrates advanced robotic control to directly guide and position the US probe for needle insertion. Unlike previous approaches, our system leverages robotic precision to automatically determine and maintain the optimal probe positioning, ensuring accurate needle insertion without the need for manual adjustments. This capability is particularly valuable in procedures like liver ablation, where precise targeting and avoiding critical structures such as blood vessels are crucial. We validate the effectiveness of this system in the challenging scenario of intra-operative liver ablation, where the robotic system’s automated needle insertion planning and control significantly enhance procedural accuracy and safety.

\section{Methods}
An overview of the proposed robotic CBCT-US system is shown in Fig.~\ref{fig:overview}(a). It consists of two main parts, a robotic US system and a robotic CBCT system. The robotic US system is composed of a KUKA arm (LBR iiwa 14 R820, KUKA GmbH, Germany) and a Siemens US machine (ACUSON Juniper, Siemens Healthineers, Germany), where the US probe (5C1, Siemens Healthineers, Germany) is rigidly attached to the end-effector of the robot using 3D printed probe holder. The US images are captured using a frame grabber (Epiphan Video, Canada), so that the images are accessible from the controlling computer. \revision{Each frame has a resolution of $880 \times 660$. During the whole experiment, US acquisition parameters are fixed.} The robotic CBCT system utilizes a commercial mobile ``ImagingRing'' (LoopX, medPhoton, Austria), with six degree of freedoms (DoFs), three for translational and rotational movements of the device on the ground, two for rotations of the X-ray source and detector, and one for rotation of the ``ImagingRing''. Additionally, LoopX is equipped with an integrated optical tracking camera mounted on the upper part of the ``ImagingRing'', as shown in Fig.~\ref{fig:overview}(a). In order to co-register the two robotic systems, a hand-eye calibration is performed between the robot arm and the optical tracking camera of LoopX. Once calibrated, the two systems are able to communicate and interact with each other.

\par
\revision{The initial setup we showed in this work can be treated as a foundational system proposal for the future robotized intelligent imaging system}, where multi-modality imaging technologies can work together, complementing each other to enhance the precision and flexibility of medical procedures. With such a system, the radiologists would no longer be limited to viewing a static image but could perform real-time acquisition and measurements in the areas of interest, e.g., Doppler imaging, which is mapped on top of the CT/CBCT image. This approach has the potential to streamline the current surgical workflow.
We simulated a challenging needle insertion scenario for system validation. The current surgical workflow of abdominal ablation often involves both CT and US, where CT is primarily used for pre-operative planning and occasionally for intra-operative guidance through CT fluoroscopy, while US is mostly utilized as real-time continuous imaging feedback during needle insertion. In the proposed robotized setup, the entire needle insertion process can be fully or semi-automated upon surgeons' demands, largely reducing radiation exposure while enhancing the efficiency and accuracy of needle placement. This automation streamlines the workflow and minimizes the need for continuous manual adjustments during the procedure.

\par
A phantom that simulates the complex anatomical structures of liver is built as shown in Fig.~\ref{fig:overview}(b). Three tubes with inner diameters of 7mm, 8mm, and 16mm, respectively, with a wall thickness of 1mm, \revision{reflecting typical abdominal blood vessel sizes, such as the renal vein (5–8 mm), hepatic vein (6–10 mm), and abdominal aorta (15–25 mm)}, are placed inside the phantom with a water pump circulating water through them to simulate vasculatures. Below the three tubes, a glass ball with a diameter of 10mm is positioned to represent a lesion inside the liver, while two 3D-printed long strip parts are placed near the surface to mimic the ribs. For simplification, all the mimicked ribs are put into a water tank. The procedure begins by acquiring a CBCT scan of the target volume, in this case, the phantom. Then the lesion can be located inside the volume. By applying the registered transformation between the CBCT volume captured by LoopX and the robot base, the robotic US system can localize both the lesion and the positions of the ribs. The robot is then controlled to place the US probe to a proper position that can visualize the lesion while avoiding any occlusions caused by ribs. Next, in order to localize the distribution of vessels near the lesion, the probe is controlled to perform a fan motion to reconstruct the vasculatures with the help of Doppler imaging. The reconstructed vessels are then mapped to the CBCT volume to create a multi-modality imaging volume. 

\par
In the semi-automated setup, the surgeons can define the needle insertion trajectory based on the fused CBCT-US volume, ensuring it avoids any intersections with vessels. In contrast, the fully automated approach can directly provide an optimal needle insertion trajectory to the center of the lesion while maximizing the distance from the vessels. Once the trajectory is defined, the robotic arm positions the US probe to the correct pose where the needle holder attached besides the US probe can guide the needle insertion to follow the planned trajectory. 

% In this section, we introduce the overall architecture of our robotic CBCT-US system. We detail and focus on the integration of the robotic CBCT scanner and the robotic US, the extraction of Doppler-localized vessels from continuous US images, which enhanced CBCT volume reconstruction, and the needle trajectory estimation. Further details are provided in the subsequent subsections.
\begin{figure}[h]
\centering
\includegraphics[width=0.98\textwidth]{pictures/overview.jpg}
\caption{Overview of the robotic CBCT-US system and fusion results.}\label{fig:overview}
\end{figure}


\subsection{System Calibration}\label{sec:calibration}



% ----------------------------------------------
% hello ha---jzl
% --------------------------------

The registration between the robotic CBCT and the robotic US is performed via a hand-eye calibration process. An optical marker is rigidly attached to the 3D-printed US probe holder. The final goal is to determine the transformation between US imaging coordinate system $\{u\}$ and CBCT coordinate system $\{c\}$ as depicted in Fig.~\ref{fig:overview}(c).

\par
Let $ T^{e}_{b} $ denote the transformation from the robot base $\{b\}$ to the robot end-effector $\{e\}$, which can be determined by forward kinematics of the robotic arm, and let $ T^{o}_{m} $ denote the transformation from the optical marker $\{m\}$ to the optical tracking camera of LoopX $\{o\}$, which can be given by the camera directly. Then the hand-eye calibration process can be formulated as follows:
% In our system, shown in the scenario in Figure 1, The probe holder, which can be mounted on the robot end-effector, holds an ultrasound probe, a 4 super-spherical mocap marker, and a needle holder. We utilize a robotic cone-beam computed tomography (CBCT) device equipped with an optical tracking system. This system captures the six-degree-of-freedom (6D) pose of the marker attached to the robot's end-effector. To facilitate accurate interaction between the robot and the optical tracking device, a hand-eye calibration is performed. The transformation $ T^{b}_{o} $ from the optical tracking device to the robot base is computed by,

\begin{equation}
\begin{split}
T^{e(1)}_{b} \, T^{b}_{o} \, T^{o(1)}_{m} & = T^{e(2)}_{b} \, T^{b}_{o} \, T^{o(2)}_{m} \\
(T^{e(2)}_{b})^{-1} \, T^{e(1)}_{b} \, T^{b}_{o} & = T^{b}_{o} \, T^{o(2)}_{m} \, (T^{o(1)}_{m})^{-1} \\
AX & = XB
\end{split}
\label{eq1}
\end{equation}
where $ T^{b}_{o} $ represents the transformation between the robot base coordinate system and the camera coordinate system of LoopX. The transformation $ T^{o}_{c} $ is determined by a camera-CBCT registration as described in~\cite{karius2024first}, while US calibration is also performed following~\cite{jiang2021autonomous} to specify $ T^{u}_{b} $, which depicts the transformation between US imaging coordinate and robot end-effector. Then transformation $ T^{u}_{c} $ can be expressed as :

% where $ T^{e}_{b} $ represents the transformation from the robot base to the robot end-effector, and $ T^{o}_{m} $ is the transformation from the marker to the optical tracking system. These transformations are provided by the robot and optical tracking system, respectively. The transformation between the optical tracking system and the CBCT device, denoted as $ T^{c}_{o} $, is a fixed value. By computing the transformation between the optical tracking system and the CBCT machine, we can establish the spatial relationship between the CBCT machine and the robot. This allows us to effectively register the ultrasound images with corresponding slices from the CBCT scan when the robot’s end-effector is equipped with the ultrasound probe. When the transformation from the ultrasound probe to the robot end-effector is $ T^{e}_{u} $, the transformation from the ultrasound probe to the robot base is $ T^{b}_{u} $, the transformation from the optical tracking system to the CBCT machine is $ T^{c}_{o} $, then, the transformation $ T^{c}_{u} $ from the ultrasound probe to the CBCT device can be expressed as:

\begin{equation}
\begin{split}
% T^{b}_{u} & = T^{b}_{e} \, T^{e}_{u} \\
T^{u}_{c} & =  T^{u}_{b} \, T^{b}_{o} \, T^{o}_{c}
\end{split}
\label{eq2}
\end{equation}

% Based on the above calculations, we establish an association between the robotic CBCT and robotic ultrasound systems. This integration allows for synchronized imaging and enhanced coordination between the two modalities, facilitating more accurate procedures and enabling the registration-free rigid registration ultrasound images with CBCT data.
\par
\revision{An automatic hand-eye calibration system was developed for quick and efficient setup. The process includes three steps: defining the robotic end-effector's movement range, automatically sampling paired poses, and solving the calibration matrix using the "Tsai-Lenz" method ~\cite{tsai1989new}. First, 4-6 poses along the workspace borders are manually sampled to define the range. Then, the end-effector moves randomly within this range, recording 30 paired poses for calibration.}
\par
Notably, this registration can be maintained even if the LoopX system moves, as the integrated tracking sensors continuously monitor its movements. The corresponding registration matrix can be updated by applying the transformation measured by the LoopX system, ensuring that the alignment between the CBCT and US imaging coordinate systems remains accurate despite any repositioning of the LoopX during the procedure. This capability ensures consistent imaging fusion and precise navigation, even in dynamic environments.

% \subsection{Doppler-based vessel extraction, enhanced CBCT volume, and visualization}
\subsection{Vasculature Mapping Based on Doppler Imaging}
Doppler imaging is a commonly utilized US-specific imaging technique in clinical practice for vessel localization and measurements~\cite{jiang2023dopus}. In this section we present a simple framework that is able to automatically localize and map the extracted vasculatures in US to the CBCT image utilizing Doppler images as guiding signal. The Doppler signal is treated as a strong indicator for the presence of vessels. As shown in Fig.~\ref{fig:overview}(d) The vessel localization process begins with the extraction of the colored regions from color Doppler images. 
% In order to filter out the noise signal, only the connected components that are larger than 10 mm$^2$ are kept. 
\revision{To minimize noise in the water tank, only the Doppler signals larger than 10 mm$^2$ are kept, ensuring compatibility with the blood vessel dimensions in the targeted application scenario}. The centerpoint of each extracted components are then served as the prompt for the Segment Anything Model 2 (SAM2) to initiate segmentation process. 
% \revision{ Given that segmentation performance for the blood vessel is sufficient, we used the model directly without finetuning.} 
Then when the robot is performing a fan-motion, the SAM2 continuously segments the vessels and reconstruct it in 3D.
\revision{Fine-tuning of SAM2 was not conducted, as the vessel borders in the water phantom were clearly visualized in US.}

% To localize the blood vessels, we first utilize Doppler ultrasound to identify the vascular structures and capture the corresponding images. From the Doppler US images, we extract the colored regions representing blood vessels, identify the connected components, and remove smaller irrelevant areas. The remaining regions are then sorted by size, and we calculate the center of each major component to accurately localize the vessels. This process ensures precise identification of the vascular structures, laying the groundwork for further imaging and procedural guidance.

% Blood vessel segmentation was performed using the Segment Anything Model 2 (SAM2), a promptable visual segmentation model. In ultrasound images, the center point of the blood vessel serves as the prompt for the model to initiate segmentation. The process begins by segmenting the vessel in the first ultrasound image and then extends the segmentation across the entire video sequence. To scan the video, the robot performs a fan motion around the probe’s center point, which is between two ribs, ensuring thorough coverage of the blood vessels surrounding the lesion.

\par
After scanning and segmentation, each US frame can be mapped to the CBCT volume using the known transformation $ T^{u}_{c} $ from US to CBCT. Based on the position of the segmentation mask in the US images, the vascular pixels are transferred into corresponding voxels within the CBCT volume. Once all pixel-to-voxel mappings are completed, the vascular structure is reconstructed within the CBCT volume, resulting in an enhanced CBCT volume as shown in Fig.~\ref{fig:overview}(e) (bottom right). As shown in Figure 2, the US image, CBCT-US fusion slice, and the enhanced CBCT volume can be shown in our visualized robotic interactive system in real time. It is evident that all critical anatomical structures relevant to liver ablation are clearly visualized, providing a solid foundation for precise needle insertion planning. This detailed presentation has the potential to enhance the accuracy and safety of the procedure by ensuring key structures, such as blood vessels and the lesion, are accurately identified and considered in the planning process.

\par
Apart from mapping the segmented vessel masks to the CBCT volume, it is also possible to directly map the original US images to the CBCT as shown in Fig.~\ref{fig:overview}(e) (top left). Furthermore, the Doppler image, containing the flow information, can also be fused with the acquired CBCT, allowing the surgeons to visualize blood flow patterns alongside anatomical structures, as shown in Fig.~\ref{fig:overview}(e) (top right). Based on the enhanced visualization results, anatomical information from both modalities are well conserved, providing a comprehensive view for more accurate surgical planning.

% \begin{figure}[h]
% \centering
% \includegraphics[width=1\textwidth]{pictures/gui.jpg}
% \caption{Visualized robotic interactive system. We showed the US image, the CBCT-US fusion image, and also the enhanced CBCT volume in real-time.}\label{fig2}
% \end{figure}

\subsection{Needle Trajectory Planning}\label{sec:needle_planning}
With the fused visualization of CBCT and US, we also present a simple yet effective automatic needle trajectory planning pipeline. The whole planning process is divided into two steps: out-of-plane localization and in-plane localization. The plane is referred as the US imaging plane. Since the needle is inserted through a needle holder which is rigidly attached to the US probe, therefore, the correct placement of needle can be to finding the accurate pose of the US probe. 

\par
In the out-of-plane localization phase, the axial slice in the CBCT volume that passes through the center of the lesion is automatically allocated based on the lesion segmentation results from CBCT. Then at the in-plane localization phase, based on the calibration matrix between LoopX and robotic US, the US probe is controlled to overlap the US imaging plane with the selected CBCT slice while the lesion is placed in the middle of the US image, ensuring the probe to be orthogonal to the scanning surface, which in the presented setup is the water surface.
Then in the in-plane localization phase, utilizing the calibration matrix between the LoopX system and the robotic US, the US probe is controlled to align the US imaging plane with the selected CBCT slice. Meanwhile the lesion is centered in the middle of the US image. Additionally, the probe is positioned to be orthogonal to the scanning surface, which in the presented setup is the water surface. The needle planning problem is then simplified to an optimization problem of finding a line ($y=kx+b$), which represents the needle insertion trajectory on a 2D plane. The objective is to maximize the distances from this line to the center of each vessel while satisfying specific constraints. These constraints ensure that the planned trajectory passes through the center of the lesion, and the in-plane rotation angle of the US probe is limited to $\pm15^{\circ}$, 

\begin{equation}
\begin{array}{rll}
% \begin{align*}
\mathop{max} \limits_{k,b}~~~~  & \sum_{i=1}^{N} \dfrac{|k \cdot x_{i} - y_{i} + b|}{\sqrt{1 + k^{2}}} &, (x_i,y_i)\in \{\text{vessel centers}\} \\
\text{subject to}~~~~ & k \cdot x_{l} + b - y_{l} = 0 &, (x_l,y_l)\in \{\text{center of lesion}\}\\
& k_{\text{max}} \geq k \geq k_{\text{min}}
\label{eq:optimization}
% \end{align*}
\end{array}
\end{equation}
where $N$ is the total number of the vessel in the 2D US slice.
\revision{The probe rotation was limited to $\pm15^{\circ}$ to prevent the needle from becoming nearly parallel to the surface, due to the fixed insertion angle of the needle holder, and to maintain proper contact between the convex probe and the scanning surface.} After solving this optimization problem with sequential quadratic programming, the robot is controlled to move the probe to align the insertion trajectory of the needle holder with the planned insertion trajectory.

\revision{
The needle holder can secure a needle at an insertion angle of approximately $39^{\circ}$, as shown in Fig. 2(a), with the corresponding US image displayed in Fig. 2(b). Regardless of probe movement, the needle's position remains fixed in the captured US image. Once the system predicts the insertion angle, the robotic arm automatically adjusts to the appropriate position, ensuring that the needle reaches the target lesion, as shown in Fig. 2(c) and Fig. 2(d).
}

\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth]{pictures/needle.png}
\caption{\revision{(a) US probe positioned perpendicular to the phantom, showing the initial needle setup. (b) The US image showing the needle inserted into the phantom with the US probe perpendicular to the phantom. (c) Probe and needle positions after the robotic arm's movement. (d) US image after movement, with the needle indicating the predicted path toward the target lesion.}}\label{fig:sketch}
\end{figure}

\par
The designed pipeline is a tailored strategy specifically developed for this particular use case. More generalized, 3D-based trajectory planning could also be implemented to handle a wider range of scenarios. However, the primary goal here is to demonstrate the precision of the proposed robotic CBCT-US system in a straightforward automatic needle placement task, highlighting its potential for accurate interventions.

% Based on our fused CBCT-US volume, we can readily acquire slices that clearly depict both blood vessels and lesions. To determine the optimal needle trajectory within these slices, we have developed an optimization algorithm. The needle path should pass through the lesion area but far from every vessel, and based on our real scenario, the needle is mounted on the left side of the probe holder, which is from the upper left to the lower right in the US image. We defined the needle path as a standard line equation that passes through the center of the lesion:

% \begin{equation}
% y = k \cdot x + b,
% \label{eq3}
% \end{equation}

% where the $ k $ and $ b $ are the slope and the intercept to be optimized. Each vessel center is defined as $ (x_{i}, y_{i}) $. The objective function is to maximize the minimum distance from a straight line to each blood vessels,

% \begin{equation}
% min\, f(k, b) = -\mathop{min} \limits_{x_{i}, y_{i}}\frac{|k \cdot x_{i} - y_{i} + b|}{\sqrt{1 + k^{2}}}, 
% \label{eq4}
% \end{equation}

% where N is the number of the vessel. To ensure the needle passes through the lesion center $ (x_{l}, y_{l}) $ and the needle direction is coordinated with our scenario, we built three constraints, 

% \begin{equation}
% \begin{split}
% c(k, b) =  & k \cdot x_{l} + b - y_{l} = 0 \\
% & k - k_{min} \geq 0 \\
% & k_{max} - k \geq 0,
% \label{eq5}
% \end{split}
% \end{equation}

% where the c(k, b) is an equality constraint, and the other two are inequality constraints. Then, we can build a Lagrangian function,

% \begin{equation}
% L(k, b, \lambda, \mu_{1}, \mu_{2}) = f(k, b) + \lambda \cdot c(k, b) + \mu_{1}(k - k_{min}) + \mu_{2}(k_{max} - k),
% \label{eq6}
% \end{equation}

% where the $ \lambda $ is the Lagrange multipliers for the equality constraints, the $ \mu_{1} $, $ \mu_{2} $ are the Lagrange multipliers for the inequality constraints. At this point, we can transform it into a sequential quadratic programming (SQP) problem to optimize the $ k $ and $ b $. At each iteration $ m $, SQP generates search directions $ \Delta k $ and $ \Delta b $ via the following quadratic programming problem:

% \begin{equation}
% \mathop{min} \limits_{\Delta k, \Delta b} \frac{1}{2}[\Delta k, \Delta b]^{T} H_{m} [\Delta k, \Delta b] + \nabla f(k_{m}, b_{m})^T [\Delta k, \Delta b], 
% \label{eq7}
% \end{equation}

% where $ H_{m} $ is the Hessian matrix of the $ L(k, b, \lambda, \mu_{1}, \mu_{2}) $, $ \nabla f(k_{m}, b_{m}) $ is the gradient of the objective function. The constraint equations also satisfy SQP, 

% \begin{equation}
% \begin{split}
% \nabla c(k_{m}&,  b_{m})^T [\Delta k, \Delta b] + c(k_{m}, b_{m}) = 0 \\
% &\mu_{1} \cdot (k_{m} + \Delta k - k_{min}) \geq 0 \\
% &\mu_{1} \cdot (k_{max} - (k_{m} + \Delta k) ) \geq 0,
% \label{eq8}
% \end{split}
% \end{equation}

% In summary, we got the predicted needle path equation $ y = k \cdot x + b $. Due to the needle insertion trajectory being fixed with the probe in the real case, we can set the needle equation as $ y = k_{n} \cdot x + b_{n} $. Based on these line equations, we can compute the angle and distance to move the robot to the correct needle insertion position.

\section{Results}\label{sec3}
% \subsection{Experimental Setup}\label{subsec1}
% The experimental setup consists of a robotic CBCT device (LoopX, Brainlab, Germany), an ACUSON Juniper ultrasound machine (Siemens Healthineers, Germany), and a robotic manipulator (LBR iiwa 14 R820, KUKA GmbH, Germany). A convex ultrasound probe (5C1, Siemens Healthineers, Germany) was mounted on the robot's end-effector. CBCT scanning is performed by 360-degree scanning with a voxel spacing around 0.6. US images were captured at a frequency of 20 Hz using a frame grabber (Epiphan Video, Canada) via OpenCV, in which the imaging depth was set to 100 mm, with a pixel spacing of around 0.166.

% All of the experiments were performed in a liver-simulated phantom, shown in the bottom left of Fig. 1. There are 3 plastic tube simulated blood vessels, 1 glass ball simulated lesion, and 3D printed ribs in a water tank. We used a pump to circulate water through each tube to simulate blood flow. The calibration is performed before the experiments, we developed an automatic calibration system and precision validation system ensuring the quick positioning after changing scenes.

% \subsection{\revision{Calibration Performance}}\label{subsec1}
% % \revision{
% % The calibration process consists of two parts: hand-eye calibration and robot-US calibration. The hand-eye experiments and calibration performance are detailed in this section. In Section 3.2, the robot-US calibration error is implicitly assessed through the mapping error, assuming the precision of the robot-US calibration has been ensured.
% % }

% % \revision{
% % An automatic hand-eye calibration system was developed for quick and efficient setup. The process includes three steps: defining the robotic end-effector's movement range, automatically sampling paired poses, and computing the calibration using the "Tsai-Lenz" method ~\cite{tsai1989new}. First, 4-6 poses along the workspace borders are manually sampled to define the range. Then, the end-effector moves randomly within this range, recording 30 paired poses with the optical tracking system.
% % % An automatic hand-eye calibration system was developed to enable a quick and efficient setup. The process consists of three main steps: defining the robotic end-effector's movement range, automatically sampling paired poses, and computing the calibration result. Initially, the robotic end-effector is manually moved to the workspace, where 4-6 robotic end-effector's poses along the workspace borders are sampled to define the range of movement. Next, the robotic end-effector navigates randomly within the specified spatial range. At each brief pause, both the robotic end-effector and the optical tracking system simultaneously record their poses, generating paired data points. To ensure high calibration precision, 30 poses are randomly sampled during each session. Finally, the calibration result is computed using the "Tsai-Lenz" method~\cite{tsai1989new}.
% % }

% \revision{
% % Several methods can be used to validate the precision of hand-eye calibration. In this work, we developed a validation system by calculating the transformation between the marker and the robotic end-effector, denoted as $ T^{e}_{m} $. As shown in Equation (1), $ T^{e}_{m} $ appears on both sides of the equation and should remain constant after calibration, regardless of the robot's movements. To validate this, the robot was moved within the workspace to randomly sample paired poses. The transformation $ T^{e}_{m} $ was then computed at different robotic positions. The results showed a translation error of less than 1 mm and an angular error of less than 0.5 degrees in different positions, confirming the high precision of the hand-eye calibration.
% Several methods can be used to validate the precision of hand-eye calibration. In this work, we developed a validation system based on calculating the reprojection error. After obtaining the calibration matrix $ T^{b}_{o} $ as defined in Equation (1), the transformation $ T^{e}_{m} $, representing the transformation between the end-effector and marker, is computed. The transformation between the end-effector and the robotic base is then calculated as $ T^{b}_{o} T^{o}_{m} (T^{e}_{m})^{-1} $, yielding the reprojected end-effector matrix. This matrix is compared with the robot's actual matrix to determine the reprojection error. In our experiment, we validated the errors using ten pairs of poses, resulting in a translation error of $0.87\pm0.33$ mm and a rotation error of $0.45^{\circ}\pm0.16^{\circ}$.
% }


% \subsection{Robotic CBCT performance}
\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth]{pictures/mapping_error.jpg}
\caption{(a) Mapping error of blood vessels shown in color gradient. Green means the error is low, while yellow represents the error is high. (b) Mapping errors over frames.}\label{fig:mapping_error}
\end{figure}

\subsection{CBCT-US mapping performance}\label{subsec2}
The mapping accuracy between the US and CBCT modalities is evaluated by measuring the matching errors of the three vessels \revision{, and the robot-US calibration error is also implicitly assessed through the mapping error, assuming the precision of the robot-US calibration has been ensured}. The vessels are first segmented from the tracked US images, and the centerlines of the vessels are extracted. These extracted centerlines are then mapped to the CBCT coordinate system using the calibration matrix. Since the mimicked vessels are made from rubber, they are clearly visible within the CBCT volume, which enables the direct localization of the vessel positions in CBCT. The mapping error is determined by calculating the distance between the vessel centerlines extracted from US and those extracted from CBCT, providing an assessment of the alignment accuracy between the two modalities. As shown in Fig.~\ref{fig:mapping_error}(a), the color gradient on the vessels represents the mapping error between the US-extracted centerlines and the corresponding vessel centerlines extracted from the CBCT volume. The error is visualized along the length of each vessel, with lighter colors indicating higher errors and darker shades indicating lower errors. It can be observed that the error distribution is relatively consistent along each vessel. Fig.~\ref{fig:mapping_error}(b) shows the mapping error over multiple frames for each of the three vessels. The mapping errors remain within a reasonable range ($1.72\pm0.62$), demonstrating that the proposed system achieves good alignment between US and CBCT.
\revision{The achieved precision is sufficient for liver ablation procedures, as study indicated that a needle placement accuracy below 5 mm ensures effective ablation while minimizing damage to healthy tissue ~\cite{nicolau2009augmented}.
These errors arise from multiple sources, including calibration errors between the camera and robot coordinate systems, the camera and CBCT coordinate systems, and the US and robot arm coordinate systems. 
As reported in \cite{sun2018robot}, using a similar hardware setup and the same calibration method, the error was measured at $0.573$ mm. 
Since the current mapping between CBCT and US is solely based on calibration processes and does not account for the alignment of anatomical features across modalities, future work could further minimize the mapping error by incorporating feature-based registration methods.}

% \revision{
% The registration of vessels from the US to CBCT volumes remains a challenging issue, with research reporting image-based registration accuracies ranging from 6.3 to 16.6 mm~\cite{montana2021vessel}. Our pre-calibration approach provides an efficient and reliable solution for establishing registration initialization between the two modalities.
% } 
% To assess the performance of the mapping between CBCT volume and US, we calculate the distance between the vessel centerline extracted from US images and the vessel centerline extracted from CBCT volume. It is important to note that plastic tubes were used to simulate blood vessels in this experiment; while the tubes are visible in CBCT, real blood vessels are not typically visible without a contrast agent. Due to the visibility of the tubes, we treated the blood vessels from the original CBCT slices as the ground truth. We then compared the center of each detected blood vessel in the enhanced slices with the ground truth to evaluate accuracy.



% \subsection{Needle insertion performance}\label{subsec3}
% To assess the reliability and precision of the system, we designed a needle insertion experiment. Using the CBCT data, the lesion was clearly visualized and localized within the 3D volume. Based on the lesion position, first, the robot, holding the ultrasound probe, moved to the surface between the ribs and quickly identified the lesion plane. Next, the robot performed a fan motion around the surface point to capture a series of continuous ultrasound images. Utilizing Doppler prompts, blood vessels were localized and segmented using the SAM2 model. Based on the segmented mask, the system reconstructed the vascular structure around the lesion.
% Following this, the system calculated the optimal needle trajectory using our estimation algorithm and guided the robot to the desired position for assisted needle insertion. In this experiment, we demonstrated the precision of needle placement by measuring the distance between the needle and the blood vessel contours, as well as the distance between the needle trajectory and the lesion center. Two different phantoms were used: a pure water tank and a pig liver water tank. The results are shown in Tab. 1.

\subsection{User study}\label{subsec3}

\revision{
Five volunteers participated in the experiment. Three were familiar with both needle insertion and robotic US systems, while the remaining two had experience with the US system but lacked familiarity with needle insertion and robotic systems. Prior to the experiment, all participants underwent training to familiarize themselves with the proposed system.
}

To compare the conventional US-guided needle insertion workflow with the workflow guided by the proposed CBCT-US fusion, two test scenarios are designed. In the first scenario, needle insertion is performed using only US guidance. The robotic arm is set to hand-guide mode, allowing users to manually manipulate the US probe into the correct position. The objective is for the needle, inserted through the needle holder, to successfully reach the lesion center while avoiding nearby vessels. Once the optimal US imaging plane is identified, the robotic arm is fixed in place, so that the users can free both hands for the needle insertion procedure. In the second scenario, the needle insertion is guided by the robotic CBCT-US system, where the localization process is devided into out-of-plane and in-plane localization as described in Sec.~\ref{sec:needle_planning}. The user is asked to determine the proper CT slice that passes through the lesion center while avoiding the occlusion of ribs. On the resulting US images, two points are manually selected by the users to allocate the needle insertion trajectory. The two approaches are compared in terms of time efficiency, accuracy, and success rates.

\begin{table}[ht!]
\centering
\caption{User Study Statistics: Comparison of Freehand and Robotic CBCT-US Methods}\label{tab:user_study}
\begin{tabular}{c|ccc}
\toprule
                &Searching time (s) &Lesion center deviation (mm) &Success rate \\
\midrule
Freehand        &57.11 $\pm$ 30.28 &7.89 $\pm$ 4.09  &65\%  \\
Robotic CBCT-US  &23.32 $\pm$ 5.90 &2.89 $\pm$ 1.68  &95\%  \\
\bottomrule
\end{tabular}
\end{table}

\par
The results are shown in Tab.~\ref{tab:user_study}. The searching time is defined as the time the volunteers spent locating the appropriate US imaging plane for needle insertion. Since the needle itself is not tracked, the deviation from the lesion center is estimated by calculating the distance between the expected needle trajectory and the lesion center. The expected needle trajectory is defined as the ideal path the needle would follow when precisely guided by the needle holder, which is rigidly mounted beside the US probe. To determine the success rate, an insertion attempt is considered unsuccessful if it involved contact with vessels or ribs, or if more than two retrials were necessary to achieve a proper insertion. With the guidance from the proposed robotic CBCT-US system, the searching time is reduced by more than $50\%$ compared to the free-hand US guidance. Additionally, the deviation from the lesion center is significantly improved by 5 mm with the assistance of the proposed system. The higher success rate further highlights the superiority of the CBCT-US fusion system, demonstrating its enhanced precision and efficiency in needle insertion tasks.
% To compare the conventional needle insertion method with the guided by the fusion CBCT-US method, we designed two test scenarios. These included freehand localization, which is commonly used in clinical practice, and semi-automatic robot-assisted localization. In both the freehand and guided by the fusion CBCT-US approaches, volunteers used either a handheld ultrasound probe or our graphical user interface system to locate the lesion plane and determine the optimal needle entry point. For the freehand localization method, volunteers first examined the CBCT volume to understand the spatial structure of the phantom, then used the ultrasound screen to search for the lesion plane. Once the plane was identified, the robot was locked to insert the needle. In the fusion CBCT-US guided method, volunteers used the GUI system to control the robot and move the slice to the correct plane. Throughout the experiments, we recorded key metrics such as the time taken to locate the lesion plane, the number of retries, and the number of blood vessel contacts.

% In our study, we invited five volunteers to participate in the experiment, with each case tested five times. We calculated the average time required to locate the lesion plane and the mean deviation of the lesion center during needle insertion. Insertion attempts were considered unsuccessful if they involved contact with vessels or ribs, or if two retrials were required. After the needle was inserted, its trajectory was mapped from the ultrasound image to the CBCT volume using our system. We then calculated and compared the deviations of the lesion center and the needle trajectory. Table 1 presents the statistical data, demonstrating how our system simplifies operational complexity and enhances the efficiency of needle intervention.

\subsection{Registration Accuracy After Repositioning}\label{subsec4}

In this section we evaluate the accuracy of the registation after repositioning of the CBCT device without system re-calibration. Based on the tracking information provided by the robotic CBCT about its movements, the transformation between the original CBCT coordinate system and the new CBCT coordinate system ($T^{c_{old}}_{c_{new}}$) can be determined. Then the calibration matrix after repositioning ($T^{u}_{c_{new}}$) can be updated as:
$T^{u}_{c_{new}}  =  T^{u}_{b} \, T^{b}_{o} \, T^{o}_{c_{old}} \, T^{c_{old}}_{c_{new}}$. The ground truth calibration is determined by re-performing the calibration process, as described in Sec.~\ref{sec:calibration} at the new position of the CBCT device. The system was evaluated by moving the LoopX device to five random positions from the initial position in two orthogonal directions in $\pm30~mm$ and $\pm10~mm$, respectively. The registration error after repositioning is $2.59\pm0.76$ mm and $0.75^{\circ}\pm0.42^{\circ}$ on average in translation and rotation, respectively. These results indicate that the system maintains a reasonable level of accuracy without requiring re-calibration after repositioning.  

% \begin{equation}
% \begin{split}
% % T^{b}_{u} & = T^{b}_{e} \, T^{e}_{u} \\
% T^{u}_{c_{new}} & =  T^{u}_{b} \, T^{b}_{o} \, T^{o}_{c_{old}} \, T^{c_{old}}_{c_{new}}
% \end{split}
% \label{eq2}
% \end{equation}


\section{\revision{Discussion}}

\par
\revision{
% In future work, we plan to focus on several key areas to enhance our method. 
Our current approach provides a practical solution for achieving registration initialization between the two modalities, with needle insertion showcasing the system’s potential for clinical application. The primary aim has been to demonstrate its feasibility in this scenario, while the needle insertion task itself is not our main focus. Compared to single-modality robotic needle insertion methods, our system combines the real-time soft tissue detail of US with the 3D anatomical context of CBCT, offering the potential for improved visualization.}
\par
\revision{Currently, the system operates in a semi-automated mode, where the robotic US system positions itself based on fused CBCT and US modalities to guide manual needle insertion through a needle holder. In the future, the system could be fully automated by incorporating a robotic needle insertion mechanism, which may improve accuracy by ensuring consistent and precise movements, compared to the variability introduced by human manipulation. The semi-automated mode serves as a proof of concept, laying the groundwork for advancements in fully automated approaches to enhance clinical outcomes.}
\par
\revision{Moving forward, we will prioritize integrating the image-based registration~\cite{jiang2024class} to enhance both accuracy and practicality. Additionally, addressing and compensating for registration errors caused by internal anatomy motion~\cite{jiang2022precise} and deformation~\cite{jiang2023defcor} due to breathing will be crucial for adapting the system to more complex and realistic scenarios. To facilitate adoption in clinical settings, we will evaluate the system’s safety and usability and seek feedback from both surgeons and patients. Future directions could also include exploring further automation of surgical tasks and integrating artificial intelligence to optimize procedural decision-making~\cite{bi2022vesnet, jiang2024intelligent}, ultimately making the system more intelligent and reliable.}

% jiang2021deformation, jiang2021motion, jiang2023thoracic


\section{Conclusion}
%1. Initial attempt 2. Addressing deformation or breathing effects not our main focus of this paper 3. But this work can set a good common ground for the deformable registration in the future.

In this work, we presented a robotic dual-modality imaging system that integrates robotic CBCT and robotic US to provide enhanced fusion guidance for clinical procedures. The system allows for pre-calibrated and dynamically co-registered CBCT and US images, enabling registration-free, multi-modality image fusion. By leveraging both modalities, our system combines CBCT with the real-time soft tissue visualization and Doppler flow information from US. 

% \revision{
% Our study has some limitations that should be acknowledged. In current surgical procedures, such as liver ablation, patients are typically instructed to hold their breath during CBCT imaging and US diagnosis. For this reason, we did not account for the effects of breathing motion and tissue deformation, as they were considered less significant in our experiments. However, in real clinical scenarios, the abdominal blood vessel system is far more complex than our phantom design, featuring numerous branches and intersections. While creating a phantom that fully replicates human anatomy is challenging, we aim to address this complexity in future research. Another limitation lies in the potential acceptance of our system by surgeons and patients. To facilitate its adoption in clinical settings, we plan to evaluate the system's safety and usability and gather feedback from both surgeons and patients regarding its implementation. These efforts will guide the refinement of our approach to meet practical and ethical standards.
% }limited scope in the user study but in the future we will include more experts and optimize the system based on their clinical feedback.

The effectiveness of this approach was validated in a needle insertion scenario simulating complex anatomical structures. The results demonstrate that the fused imaging system is able to enhance the accuracy and safety of needle insertion procedures. Our user study \revision{indicated an} improvement in time efficiency, lesion targeting accuracy, and overall success rate compared to traditional freehand US-guided workflow. Moreover, the system is able to maintain registration accuracy even after repositioning the CBCT device, showing the capability of co-registration for mobile imaging systems. 

In conclusion, our proposed robotic CBCT-US system paves the way for advanced and automated interventions in a clinical setting, with potential beyond just needle insertions. \revision{The integration of two robotic systems provides a promising platform for various clinical applications. }
This dual-modality system could be applied to more complex interventions, where precise navigation and multi-modal imaging are essential. Morworeover, the system’s ability to maintain registration accuracy after repositioning opens the door for more flexible and adaptive intra-operative imaging workflows. 

\section*{Declarations}
\subsection*{Funding} 
Partial financial support was received from Brainlab Vaskuläre Chirurgie.

\subsection*{Conflict of interest} 
The authors declare no conflict of interest.

\subsection*{Informed consent} 
Informed consent was obtained from all individual participants included in the study.

%Our study has certain aspects to improve. In current liver ablation procedures, patients typically hold their breath during CBCT imaging and US diagnosis, so we did not focus on breathing motion or tissue deformation. While our phantom design simplifies the complex human abdominal blood vessel system, replicating such intricacy is challenging and will be explored in future work. Lastly, to ensure clinical adoption, we plan to evaluate the system’s safety and usability and incorporate feedback from surgeons and patients to refine our approach.


%The needle insertion highlights our system's potential application, with the primary goal of demonstrating its feasibility in this clinical scenario. Future work will focus on refining and optimizing this aspect for improved practical performance.

% \bibliographystyle{sn-basic.bst}
\bibliography{sn-bibliography}% common bib file
%% if required, the content of .bbl file can be included here once bbl is generated
%%\input sn-article.bbl


\end{document}
