
The interaction springs between computational degrees of freedom $\vec{\psi}_i$ and $\vec{\psi}_j$ are determined by the long term memory (weights) of the model, encoded in the spin texture of the $\sigma$ field. In our model, this dependence arises through a nonlinear interaction $H_I$ between each pair of nearest-neighbor sites $i$ and $j$, with the form:
\begin{equation}
H_I=\lambda(t)\left[c_0 + (\sigma_i-\sigma_j)^2 \right]\vec{\psi}_i^T \vec{\psi}_j,
\label{eqn:interactionpotential}
\end{equation}

This term induces an effective coupling spring $c_{eff}$ between modes $\psi_{x,i}$ and $\psi_{x,j}$, as well as between modes $\psi_{y,i}$ and $\psi_{y,j}$; with values $c_{eff}=\lambda(t)c_0$ when the spins $\sigma_i$ and $\sigma_j$ are aligned, and $c_{eff}=\lambda(t)(c_0+2A_{\sigma}^2)$ when $\sigma_i$ and $\sigma_j$ are anti-aligned (See Fig.~\ref{fig:epsart} for a visual representation of this effect). Here, $A_{\sigma}$ is the amplitude of oscillation of the Ising mode. During inference, the coupling strength $\lambda(t)$ is set to a fixed value $\lambda_0$; while at each training iteration it is momentarily set to $0$ so that sites exactly follow the single-site learning dynamics described earlier. The term $c_0$ ensures that there is some coupling even when the spins $\sigma_i$ and $\sigma_j$ are aligned---thus preventing a large number of spin configurations from collapsing to zero transmissivity. Although this work corresponds to a tight-binding model, in an experimental setting, the potential could be realized by a set of appropriately placed cubic springs (see Appendix \ref{Appendix:NonlinearSprings}).


\begin{figure}[b!]
\includegraphics[width = \columnwidth]{Fig4.pdf}
\caption{Iris flower classification using a multifield coherent Ising machine. \textbf{(a)} The Iris dataset consists of the petal length $l_p$, petal width $w_p$, sepal length $l_s$ and sepal width $w_s$ for 150 flowers belonging to three classes (iris setosa, iris virginica, iris versicolor). \textbf{(b)} The features are injected into the multifield coherent Ising machine, by encoding them in the amplitude of harmonic excitations at the computational frequency $\omega_c$. The output is taken at the central site. Positive and negative copies of the signals are applied, as the lattice cannot perform subtractions. The full model consists of three multifield Coherent Ising Machines, corresponding to each of the model classes. The machines learn to produce a high amplitude when excited by a sample of their corresponding class. \textbf{(c)} Evolution of the mean classification accuracy during training. The shaded area corresponds to one standard deviation. \textbf{(d)} Histograms of the classification accuracy computed on an untrained lattice (blue), and after 20 (orange) and 98 (green) training iterations. The training times corresponding to the histograms are indicated as solid dots in panel (c).}
\label{fig:iris} 
\vspace{-15  pt}
\end{figure}

Due to the interaction term $H_I$, during inference ($\lambda(t)=\lambda_0$ and $\mu(t)=0$), the dynamics of the $\vec{\psi}$ field can be understood as a spin-dependent effective spring network, whose steady-state amplitude response is given by the linear system $K(s)A_{\psi_{x/y}} = f_{\psi_{x/y}}$.  Here, $K(s)$ is the spin-dependent stiffness matrix, whose diagonal components are $K_{i,i}=i\omega_c^2/Q_c$ and the off-diagonal components are given by $K_{i,j}=-c_{eff}(s_i, s_j)$ when $i$ and $j$ are nearest neighbors, and zero otherwise. We choose $\lambda_0$ and $c_0$ so that the coupling stiffness is $10$ times the damping rate $\omega^2_c/Q$ for anti-aligned spins, and $2.5$ times the damping rate for the aligned case. Here, we have introduced the variable $s_i\in\{+1,-1\}$ to label the two possible phase oscillation states of $\sigma_i$. Gauge freedom means that the specific labeling is irrelevant as long as it is consistent between sites. We use this linearized model to study the expressivity of a square lattice with $4x4$ sites (Fig.~\ref{fig:lattice}a)---which output amplitude responses $A_o$ can be encoded as a spin configuration $s$. We do so by computing the transmitted amplitude associated with every spin configuration and calculating an effective density of states (Fig.~\ref{fig:lattice}b,c). We observe that the lattice can encode a range of mostly positive transmissivities, with two small gap regions (Fig.~\ref{fig:lattice}b).

Our lattice model can learn to approximate a given transmission amplitude (Fig.~\ref{fig:lattice}d, e), as long as the target amplitude can be expressed by a spin configuration. The learning protocol is as follows: We apply a harmonic force with magnitude $F_i$ and frequency $\omega_i$ at both clamped $\psi_x$ and free $\psi_y$ degrees of freedom of the the top-left site, while prescribing the displacement of the clamped mode $\psi_y$ at bottom-right site to a target value $A_T$. During this procedure, we keep $\lambda(t)=\lambda_0$ and $\mu(t)=0$, waiting for a sufficient time ($\Delta T\gg Q_c/\omega_c$) so that the $\vec{\psi}$ field equilibrates. Then, we set $\lambda(t)=0$ and apply a Gaussian learning pulse $\mu(t)$ to every site---during this protocol, the contrast $\psi_{x-y}$ remains large owing to the long lifetime of the computational modes.  For every target output amplitude $A_T$, we repeat this procedure $n$ times, starting from a random spin configuration.

%We model this with a master equation, computing the amplitudes using the linearized model and the bit-flip probability from the 
%Coupling the linearized model with the sigmoidal bit-flip probability allows us to model the system as a master equation. At every learning iteration

%The linearized model allows us to formulate a master equation for the whole system that does not require integrating a stochastic differential equation. To construct the master equation, we replace the physical field $\sigma_i$ by the logical field $s_i$, that can take values $s=\{+1, -1\}$ depending on the phase of the underlying physical degree of freedom. In this model, the effective. 

%We also augment our model --- the absence of this term causes many configruations to have zero conductivity, reducing the number of available states for learning. consider cases where there is a spin-independent linear hopping 




%In this section we introduce a master-equation-based model, which captures the relevant behavior of the system, while reducing the computational burden of the simulations. We then use this simplified model to study  the multi-neuron dynamics and show that the system can converge on a simple training example. Ultimately, this allows us to train the lattice on a non-trivial task in the next section. 



%The first step in simplifying the model, is to observe that, assuming the deviations in the computational modes are small compared to the Ising mode, neighboring sites are approximately linearly coupled, with a spring constant that depends on the configuration of the spins associated to each site. To wit, the force of the $j$th resonator on mode $\psi_i$ is:
%\begin{align*}
%F_{ji} &= -\frac{\partial E_{ij}}{\partial\psi_i} = -4\gamma(\Delta\sigma_{ij} + \Delta\psi_{ij})^3\\
%&\approx-2\gamma\Delta\sigma_{ij}^2\Delta\psi_{ij}.
%\end{align*}
%This prompts us to model the network as two separate lattices of linearly coupled resonators (the clamped and free states) whose coupling stiffnesses are parametrized by the spin configuration. Since everything happens on resonance, we use linear algebra in the Fourier domain to find phase and amplitude of each resonator. We can then compare the resulting free and clamped states to extract the average amplitude of the contrast mode $\psi_ {X-Y}$ at every site. Next, we invoke the flip probability distribution discussed in section \ref{SysDesc} and apply the learning rule of Eq. \ref{eq:abstract_rule}, resulting in an updated spin configuration (bypassing the need to simulate the full learning dynamics). To test whether this model accurately captures the multi-neuron dynamics, we consider the concrete example of a $4 \times4$ lattice with one input and one output, as shown in Fig. \ref{fig:lattice}a. For each spin configuration, we measure the output amplitude $A_o$ in response to a fixed harmonic input, resulting in the density of states plotted in Fig. \ref{fig:lattice}b. We single out four spin configurations (Fig. \ref{fig:lattice}c) for which we integrate the full equations of motion (Eqs. \ref{eq:local} \& \ref{eq:inter}). Turning off the local potentials ($\mu(t) = 0$) and applying minimal noise, ensures that the spins are preserved from their initialization throughout the integration. The colored dots in Fig. \ref{fig:lattice}b show the resulting transmitted amplitudes. Because of noise and interference from higher harmonics, some disagreement is expected, but overall the results show that the simplified model agrees with the original, proving that the proposed model adequately captures the multi-neuron dynamics. The question remains whether the flipping dynamics of a single site is significantly altered by being in a lattice compared to being isolated. (some comment on separation of timescales?). However, we can always decouple sites during each learning protocol by turning off $\lambda(t)$, removing any effect of the lattice on the learning dynamics. 

%The master-equation-based model speeds up the simulations, enabling us to probe the network's learning capabilities. Indeed, over many training iterations, the network converges to the target amplitude within some margin of error (Fig. \ref{fig:lattice}d), which can be attributed to the detuning threshold discussed earlier (do we need to show this?): learning stops when free and clamped differ less than this threshold. Furthermore, the network can be trained to achieve any feasible transmission amplitude (Fig. \ref{fig:lattice}e), with better performance seen for those with a high density of states.