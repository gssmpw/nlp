Our learning rule boils down to a conditional spin forgetting mechanism: the phase of a spin $\sigma_i$ is randomized with a probability conditional on the difference between the clamped and free configurations at the site, thereby inducing spin flips in sites where free and clamped are misaligned. This mechanism arises from the following nonlinear, local interaction term acting on each site $i$,
\begin{equation}
H_{L,i}=\mu(t)\left( \psi_{x,i}-\psi_{y,i} \right)^2\sigma^2_i =\mu(t)\psi_{x-y,i}^2\sigma_i^2.
\label{eqn:localterm}
\end{equation}

Since $\psi_x$ and $\psi_y$ are degenerate modes, the contrast between clamped and free configurations $\psi_{x-y}$ is merely a rotation of the local basis $(\psi_{x+y}, \psi_{x-y})=(\hat{\sigma}_x+\hat{\sigma}_z)(\psi_x, \psi_y)$, where $\hat{\sigma}_{x/y/z}$ is the corresponding Pauli matrix. This allows us to interpret $H_L$ as a cross-Kerr interaction between the Ising field $\sigma$ and the rotated computational field $\psi_{x-y}$. Thus, the term introduces a change in the resonance frequency of the mode $\sigma_i$ that increases quadratically with the contrast $\psi_{x-y} =\psi_x-\psi_y$ between clamped and free configurations. If $\psi_{x-y}$ is sufficiently large, the system will be detuned outside the parametric resonance region (Fig.~\ref{fig:flip}a), causing its oscillation to decay, thereby forgetting its state (Fig.~\ref{fig:flip}b,c).  At low temperatures, the detuning results in a sharp, step-like transition: If $\psi_{x-y}$ is above a certain threshold, the system forgets its state, resulting in a probability of flip of $1/2$, while nothing occurs below the threshold. For higher temperatures, the probability of a spin flip becomes a smooth function of the contrast $\psi_{x-y}$ (Fig.~\ref{fig:flip}b). 
\begin{figure}[t!]
\includegraphics[width = \columnwidth]{Fig3.pdf}
\caption{Learning in a 4x4 lattice \textbf{(a)} System under consideration. A harmonic force with frequency $\omega_c$ and amplitude $F_i$ is applied at the top-left site, to both clamped and free degrees of freedom. The output amplitude $A_o$ is measured at the bottom-right corner.  \textbf{(b)} Cumulative density of states (orange) and density of states (blue) as a function of the transmissivity, computed via linearized model. The dots correspond to simulations of the transmissivity  using the full nonlinear ODE with an excitation force of $10^{-4}$.  \textbf{(c)} Spin textures corresponding to the dots in (b) \textbf{(d)} Evolution of the transmissivity as a function of training iteration for target values $A_T$ of $0$, $1$ and $2$ (dashed line). The line represents the mean of N simulations, while the shaded area represents a standard deviation. \textbf{(e)} Output amplitude $A_o$ as a  function of the clamping amplitude, after 200 learning iterations, starting from a random configuration. The shaded area represents one standard deviation. The dashed orange line corresponds to an ideal learning response. We observe that, for conductivity values for which spin state exists, the transmitted amplitude approximately converges to the clamping amplitude. Panels (d) and (e) are calculated with a sigmoidal flip probability ($p_0=0$, $\psi_T=0.16$ and $\beta=1500$) and an input harmonic force $F_i=10^{-3}$.}
\label{fig:lattice} 
\vspace{-15  pt}
\end{figure}
An unwanted side effect of the potential in Eq.~\ref{eqn:localterm} is that it couples the free and clamped sub-lattices. This is exemplified by the presence of Rabi oscillations between $\psi_x$ and $\psi_y$ when the potential coefficient $\mu(t)$ is nonzero (Fig.~\ref{fig:flip}c). However, the learning rule requires comparing two independent lattices. We mitigate this effect by keeping the interaction strength to zero, $\mu(t)=0$, during equilibration of the field $\vec{\psi}$ in response to a change in the input. At every learning iteration, we apply the learning potential according to a Gaussian pulse, characterized by a peak value $\mu_m$ and a width $\Delta_t$, $\mu(t)=\mu_m \exp{\left(-\frac{(t-t_0)^2}{2\Delta_t^2}\right)}$. This potential causes the spins $\sigma$ to be updated in response to the difference between clamped and free configurations. Since the decay rate of the the Ising mode $\sigma_i$ is much faster than the decay rate associated with the computational modes $\vec{\psi}_i$, the contrast $\psi_{x-y}$ remains significant during the learning iteration (Fig.~\ref{fig:flip}c). Even if Rabi swaps occur between $\psi_x$ and $\psi_y$, the contrast $\psi_{x-y}$ is unaffected as $H_L$ does not mix the $\psi_{x+y}$ and $\psi_{x-y}$ sectors. 

Intuitively, we would expect the probability of spin-flip to follow a step-like change as a function of $\psi_{x-y}$: Below a critical detuning, the system remains in resonance and remembers its state, while above the threshold $\psi_T$ the system falls out of resonance and forgets its state (Fig.~\ref{fig:flip}a).  At finite temperatures, this step-like response becomes a smooth sigmoidal function (Fig.~\ref{fig:flip}b,d):
\begin{equation}
p(\psi_x-\psi_y)=p_0+\frac{1}{4}\left[1+\text{tanh}\left(\beta(\psi_{x-y}^2-\psi_T)\right)\right],
\label{eqn:switchprobability}
\end{equation}
in which the threshold $\psi_T$ and the slope $\beta$ are fitted empirically to a numerical simulation (Fig.~\ref{fig:flip}d). Remarkably, the sigmoidal description breaks down for shorter pulse widths $\Delta_t$. In these cases, the oscillation of $\sigma$ does not have time to decay below the thermal noise, thus forgetting its phase state (spin). In this short-pulse regime, the system presents coherent bit flipping---the spin changes deterministically for specific values of $\psi_{x-y}$---as opposed to conditional forgetting (Fig.~\ref{fig:flip}d), which is the intended response.

%This can be thought of as a cross-Kerr interaction between the degree of freedom $\sigma$ and the rotated mode $\psi_{x-y}$. Although the present work is concerned with a tight-binding model, it is worth noting that cross-Kerr interactions arise frequently among mechanical, optical and optomechanical modes.

%And thus introduces Rabi swaps between the modes $\psi_x$ and $\psi_y$

%In the contrastive learning rule, the long-term memory --- encoded in the flow resistivities of the network, 



%\textcolor{red}{I suggest the title 'Single-site dynamics' and start with something that conveys (not necessarily in these words) (A) The WHY: 'A crucial ingredient in contrastive learning is the capability of changing conductivity when clamped and free solutions are different. In this work... then (B) The what: We accomplish thus through nonlinear... and then (C) the HOW This can be modeled with a master equation. But don't make it mostly about the modelling.} 
%In this section, we focus on a single neuron and analyze the long-timescale learning protocol. Ultimately, the dynamics induced by the local potentials is encapsulated by the following abstract learning rule: 
%\begin{equation}
%\sigma_i \mapsto  B(p)\cdot\sigma_i,  \label{eq:abstract_rule}
%\end{equation}
%where $B(p)$ is some Bernoulli random variable that takes the value $-1$ with %\textit{flip probability} $p$, which depends---among other things---on the displacement in the contrast mode $\psi_{X-Y}$ such that the Ising mode is generally more likely to flip when the free and clamped modes are less in agreement.

%How exactly this flipping behavior arises, is outlined in Fig. \ref{fig:flip}. It starts from the fact that the Ising mode is parametrically driven with a certain driving amplitude $h$ and frequency $\omega_d$. Suitable values of these parameters place the mode---indicated with a red dot in Fig. \ref{fig:flip}a---inside the resonance region marked by the red curve in the same figure. We leave the derivation of this curve to Appendix \ref{Appendix:PR}. The cross-Kerr coupling between $\sigma$ and $\psi_{X-Y}$ detunes the Ising mode, increasing its effective stiffness with:
%\[2\mu(t)\langle\psi_{X-Y}^2\rangle.\]
%As detailed in Appendix \ref{Appendix:PR}, this detuning shifts the resonance tongue along the $\omega_d$ axis, thereby moving the Ising mode closer to the boundary, as indicated by the purple and brown curves in Fig. \ref{fig:flip}a. By modulating the interaction strength $\mu(t)$ as depicted in the top panel of Fig. \ref{fig:flip}c, the Ising mode can thus be (temporarily) pushed out of resonance, causing its amplitude to decrease as shown in the middle panel of Fig. \ref{fig:flip}c. During this time, the mode approaches the noise-dominated regime indicated by the red band in the same panel. In an unintended side effect, the local potential also couples the computational modes with each other (bottom panel of Fig. \ref{fig:flip}c). For this reason, the coupling strength is eventually turned off again, marking the end of the learning protocol. This also allows the Ising mode to move back into resonance, possibly with a different phase --- i.e. spin --- than it started out with. The probability $p$ for such a spin-flip to occur depends on the amount of detuning that is present in a very specific way: whenever the detuning is great enough to kick the Ising mode out of resonance, there is an equal probability of ending up with either spin, due to the randomness induced by the thermal noise, resulting in a sharp step-function situated at a certain threshold detuning (Fig. \ref{fig:flip}b). Increasing the amount of thermal noise has the effect of smoothing out this step-function and increasing the flip probability for the below-threshold detuning. For shorter pulses however, there is a different mechanism at play. Above threshold, there is not enough time for the Ising mode to die out, meaning that thermal noise is not dominant in that regime. Instead, the detuning causes the Ising mode to change frequency, which leads to a more deterministic phase drift. This results in alternating high and low probabilities of spin flip for increasing detuning (as seen in Fig. \ref{fig:epsart}d). This effect disappears as the duration of the pulse increases. 


%In the noise dominated flipping regime, the flip probability can be approximated as a sigmoidal function:
%\begin{equation}
%p(\psi_x-\psi_y)=p_0+\frac{1}{4}\left[1+\text{tanh}\left(\beta((\psi_x-\psi_y)^2-%\Delta^2)\right)\right],
%\label{eqn:switchprobability}
%\end{equation}
%where $p_0$ is the flip probability during a learning protocol in the absence of any contrast between clamped and free solutions, $\Delta$ determines the contrast threshold for a the transition to occur and $\beta$ determines the sharpness of the transition.

%By iteratively switching on and off the local potentials, and giving the system time to reach steady state in between the learning protocols, the learning parameters are constantly updated until the free and clamped states match. 

%\textcolor{red}{The separation of time scales must be somehow explained, potentially together with the time dependence}