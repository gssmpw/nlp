We illustrate the capabilities of the proposed system on the Iris classification test~\cite{fisher1936use}. The dataset consists of measurements of the petal length $l_p$, petal width $w_p$, sepal length $l_s$ and sepal width $s_w$ (Fig.~\ref{fig:iris}a) for a set of 150 flowers belonging to the species \emph{iris setosa}, \emph{iris virginica}, and \emph{iris versicolor}. The classifier consists of a system of three 13x13 lattices with one-hot encoding; the lattice with the highest output amplitude, measured at the central site, will be the inferred flower species. We apply the geometric features ($l_p$, $w_p$, $l_s$, $w_s$) by encoding them in the amplitudes of a set  harmonic excitation forces at the frequency of the computational  modes. These signals are applied at sites through the boundary of the lattice (Fig.~\ref{fig:iris}b), acting on both free $\psi_x$ and clamped $\psi_y$ computational modes. Since the lattices cannot produce large negative transmissivities (Fig.~\ref{fig:lattice}b), we apply both positive and negative copies of each feature signal. 

The training starts from three lattices with random spin configurations. Then, we apply the forces encoding the features corresponding to a randomly chosen flower to each of the three lattices. We set $\lambda=\lambda_0$ and $\mu=0$, and clamp the output site $\psi_{y,center}$ to $55$ for the lattice corresponding to the current type of flower, and to $-35$ for the lattices corresponding to the other flowers. Then, we apply a learning protocol to all lattices, consisting of setting $\lambda=0$ and $\mu(t)$ to a Gaussian pulse, inducing bit flips when clamped and free sectors disagree. We simulate the training protocol by computing the steady-state amplitudes using the linearized model, and then flipping the bits with probabilities given by Eq.~\ref{eqn:switchprobability} with $\alpha=10$ and $\psi_T=1.5$. We repeat this training for 2000 iterations, observing that the system starts from an accuracy of $33.2 \pm 22.1\,\%$ , as one would expect from random choice. As more training iterations are performed---each consisting of a single, randomly-selected flower sample, the accuracy increases to $71.9\pm9.8\,\%$, more than halving the error rate, and significantly outperforming random choice. While this accuracy is lower than prior works on physical learning, we use a much simpler training method and network architecture. We expect that complex hierarchical networks trained with complex protocols should be able to reach performances comparable with the state-of-the-art.