\section{Conclusion}
\label{sec:conclusion}



\textbf{Summary.}  
We introduced \sysname{}, a novel framework designed to address the challenges of large-scale LLM inference. \sysname{} leverages serverless features, including LLM specific autoscaling, cold start optimization, and high-density deployment, to significantly reduce inference costs at the system level. Beyond cost efficiency, \sysname{} introduces distributed and disaggregated serving features, enabling scalable and flexible LLM inference across diverse workloads. Innovations such as the distributed KV cache pool, hybrid orchestration, and heterogeneous serving optimizer further push the boundaries of performance optimization, ensuring efficient resource utilization while maintaining low operational costs. These features collectively establish AIBrix as a highly adaptable and cost-effective solution for large-scale LLM deployment. By integrating deep inference engine optimizations with cloud-native infrastructure, \sysname{} provides a scalable, high-performance serving platform that bridges the gap between efficiency and flexibility in AI inference workloads.

\textbf{Limitations and future work.}
Some of our experiments do not fully evaluate routing strategies, heterogeneous serving under non-ideal workloads, limiting the ability to generalize these features across different workload characteristics. Additionally, profiling-based autoscaling and heterogeneous GPU scheduling currently rely on offline model profiling, which introduces an additional step that may not always be practical for dynamic workloads. To mitigate this, a potential solution is to streamline the profiling process by adopting roofline model analysis (\cite{imai2024predicting}), which can provide a more structured and lightweight approach to profiling heterogeneous inference performance.
Furthermore, we aim to expand evaluations to cover diverse real-world workload scenarios, refining routing strategies, GPU allocation mechanisms, and distributed orchestration to further enhance \sysname{}â€™s adaptability across various LLM deployment environments.  

Moving forward, we will continue to explore deeper co-design between inference engines and system architecture, ensuring that \sysname{} remains a highly optimized, scalable, and production-ready solution for LLM inference at scale.


