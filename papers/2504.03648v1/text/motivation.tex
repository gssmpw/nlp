\section{Related Works}
\label{sec:motivation}

Existing cloud-native and machine learning (ML) serving frameworks provide fundamental infrastructure for model inference but lack key optimizations necessary for large-scale LLM inference. We categorize prior works into two main areas: microservice-based serverless frameworks and traditional ML model-serving frameworks, highlighting their limitations when applied to LLM workloads.

\textbf{Microservice-Based Systems}
Microservice-based frameworks like Knative (\cite{knative}) and Istio (\cite{istio}) offer powerful solutions for managing stateless services, emphasizing advanced traffic control mechanisms such as request rate limiting, request interception, authentication (AuthN), authorization (AuthZ), and autoscaling based on QPS and concurrency metrics. However, these solutions are not designed for GPU-based inference workloads and fail to address the fundamental changes that LLM applications introduce. Unlike traditional microservices, LLM inference does not require complex service meshes or extensive request routing features. Instead, it introduces new challenges such as token-based rate limiting, KV cache-aware autoscaling, and model-specific scheduling constraints. For example, circuit-breaker-based rate limiting in Knative is incompatible with LLM’s token-based inference constraints, and QPS-based autoscaling cannot accurately capture GPU-bound resource usage patterns such as KV cache memory pressure. Furthermore, the overhead of Istio’s service mesh makes it unsuitable for LLM applications, where lightweight, inference-specific optimizations are more effective.

\textbf{Traditional ML Model-Serving Systems}
Traditional ML model-serving frameworks like KServe (\cite{kserve}) and RayServe (\cite{rayserve}) provide solutions for model deployment, request handling, and autoscaling, making them well-suited for conventional deep learning inference. These frameworks support features such as model URI management, dynamic scaling, and model versioning. However, they lack deep integration with LLM inference engines and fail to address key challenges specific to LLM workloads. LLM inference introduces unique characteristics such as highly variable input-output lengths, massive model sizes, and stateful execution (e.g., KV cache management), which require custom routing, model distribution strategies, and GPU-aware scheduling. While KServe and RayServe can deploy LLM models, they do not provide specialized optimizations for efficient batch scheduling, KV cache coordination, or heterogeneous GPU utilization. As a result, they cannot fully leverage the performance potential of modern LLM inference engines such as vLLM and TensorRT-LLM (\cite{tensorrtllm}).