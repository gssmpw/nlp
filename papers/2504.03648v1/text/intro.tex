\section{Introduction}
\label{sec:intro}
 
Large language models (LLMs) have revolutionized AI applications, powering innovations in areas like chatbots, automated content generation, and advanced recommendation engines.
While API services based on proprietary models like OpenAI and Anthropic have gained widespread adoption, many enterprises often seek open-source alternatives due to data security concerns, customizability, or the cost of of proprietary solutions.
The growing demand for hosting open-source models like LLaMA, Deepseek, Qwen and Mistral and offer production-grade APIs presents new challenges -- namely, how to efficiently deploy them at scale while maintaining low inference latency and cost efficiency.

%Yet, turning open-source models into cost-efficient, production-grade APIs remains a significant challenge. 
%Achieving low-latency, scalable inference requires more than just an optimized modelâ€”it demands a holistic system approach that spans multiple layers, from the model itself to the inference engine and the surrounding infrastructure. Building a performant and cost-effective LLM API requires joint efforts across three key layers:

Deploying LLMs in production requires more than just an optimized model; it demands a holistic system approach spanning multiple layers:

\begin{itemize}
    \item \textbf{Open-Source Models}: The foundation of AI applications, where model optimizations such as model architecture, Multi-Head Latent Attention (MLA) (\cite{liu2024deepseek}), distillation, and adapter fine-tuning enhance performance and adaptability.

    \item \textbf{Inference Engines}: Inference engines like vLLM(\cite{kwon2023efficient}) or TensorRT-LLM improve model serving efficiency via KV cache management, model parallelism, attention optimization, and other optimizations.

    \item \textbf{System-Level Orchestration (\sysname{})}: The critical but often overlooked layer that determines real-world cost efficiency and scalability by governing resource scheduling, autoscaling, request routing, heterogeneity management, and multi-cluster or multi-region resource optimization.
\end{itemize}


While model and engine optimizations are crucial, system-level orchestration is the key to unblocking true cost efficiency.
Without a well-designed infrastructure, even the most advanced model and inference engines struggle with real-world deployment challenges, such as autoscaling, cache-aware routing, heterogeneity resource management. 

To address these challenges, we introduce \sysname{}, a novel cloud-native framework designed to simplify and optimize LLM inference infrastructure, providing users with a \textbf{one-click deployment experience} while ensuring best-in-class performance and cost-efficiency.
%and easily manage LLM serving instructure with best performance and lowest cost. 
Our key contributions include:

\begin{itemize}
    \item \textbf{High-Density LoRA Management:} \sysname{} 
    enables dynamic LoRA registration and lineage support (\cite{loraproposal}) in vLLM, streamlining LoRA adapter management and reducing the cost of managing fine-tuned models.

    \item \textbf{LLM-Specific Autoscaling:} \sysname{} supports various scenario-driven LLM autoscaling policies. It also features optimizations such as sliding window metric aggregation to reduce the propagation delay of real-time metrics. 
    
    \item \textbf{Advanced LLM Gateway and Routing Strategies:} \sysname{} introduces an LLM-aware API gateway, extending Envoy Gateway to optimize instance routing and support various routing strategies.
    Unlike traditional gateways that distribute requests blindly, \sysname{} analyzes token patterns, prefill cache availability, and compute overhead to enhance routing efficiency in diverse deployment scenarios. 
    
    \item \textbf{Unified AI Runtime with GPU Streaming Loader:} \sysname{} serves as a unified runtime layer, managing interactions between inference engine pods and the control plane. It automates models artifact handling, configures inference engines, and provides real-time observability, ensuring vendor-agnostic compatibility.
    Additionally, \sysname{} features a GPU streaming loader that bypasses disk I/O bottlenecks to accelerate model loading and execution.
        
    \item \textbf{Distributed and disaggregated KV Cache pool:} \sysname{} introduces a distributed KV cache that enables high-capacity, cross-engine KV reuse while optimizing network and memory efficiency. 
    Key innovations include a scan-resistant eviction policy, reduced redundant data transfers, asynchronous metadata updates, and shared-memory-based data exchange, enhancing inference throughput and efficiency.
    
    \item \textbf{Mixed-Grain Multi-Node Inference Orchestration:} \sysname{} introduces a hybrid approach to multi-node inference by integrating Ray (\cite{ray}) for fine-grained application orchestration with Kubernetes for coarse-grained resource management.
    Compared to inference engine's native supports in a distributed environment (\cite{distributed-serving}), which emphasize parallelism over service-oriented needs, AIBrix balances distributed execution with production-grade orchestration, achieving scalability, rolling upgrades, and efficient resource allocation. 
    
    \item \textbf{Cost efficient and SLO-driven Heterogeneous Serving:} \sysname{} introduces a GPU optimizer that balances cost efficiency with SLO adherence, dynamically selecting the optimal GPU configuration based on workload characteristics and hardware availability, ensuring cost-effective heterogeneous GPU utilization.
    
    \item \textbf{Accelerator Diagnostic and Failure Mockup Tools:} \sysname{} introduces a diagnostic tool that leverages AI accelerators' built-in capabilities to detect and diagnose hardware failures.
    Also, a failure mockup tool simulates hardware failures, enabling rigorous fault tolerance and recovery testing.
    
\end{itemize}

\sysname{} is a cloud-native, open-source framework that simplifies and optimizes LLM deployment in production environments, offering AI practitioners across industry and academia with a flexible, scalable, and cost-effective serving solution.
By deeply integrating model and engine optimizations with system-level orchestration, \sysname{} bridges the gap between efficiency and flexibility, setting a new standard for large-scale LLM inference workloads.

% Here's where \sysname{} steps in. \sysname{} is a cloud-native, open-source framework designed to simplify and optimize LLM deployment, offering flexibility and cost savings without sacrificing performance.

% Key Challenges in AI Infrastructure: 1. Efficient Heterogeneous Resource Management: Managing GPU resources across clouds is crucial for balancing cost and performance. This involves autoscaling, high-density deployments, and efficiently handling mixed GPU types to reduce expenses and support peak loads without over-provisioning. 2. Next-Gen Disaggregation Architectures: Cutting-edge architectures, like prefill and decoding disaggregating or employing a remote KV cache, enable more granular resource control and reduce processing costs. However, they demand significant R\&D investment to develop reliable, scalable implementations. 3. Operating LLM Services at Scale: Ensuring reliable, scalable LLM services on the cloud requires complex service discovery, multi-tenant scheduling, and robust fault-tolerant mechanisms to handle failures and ensure fair resource allocation.



