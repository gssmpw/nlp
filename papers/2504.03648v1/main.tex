\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2024


% ready for submission
% \usepackage{neurips_2024}
\usepackage[preprint]{neurips_2024}
\usepackage{natbib}
% \usepackage[backend=bibtex]{biblatex}
% \usepackage[backend=biber]{biblatex}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2024}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2024}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2024}
  


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage{todonotes}
\usepackage{array}
% \usepackage{amsmath}
\usepackage{booktabs}
\usepackage{amssymb}% http://ctan.org/pkg/amssymb
\usepackage{pifont}% http://ctan.org/pkg/pifont
\usepackage{tablefootnote}
\usepackage{tabularx}


\newcommand{\sysname}{AIBrix}
% \renewcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\renewcommand{\arraystretch}{1.3} 

% \addbibresource{references.bib} % Link the .bib file

\title{\sysname{}: Towards Scalable, Cost-Effective Large Language Model Inference Infrastructure}
%\title{\sysname{}: Towards Scalable, Cost-Effective AI Infrastructure for Generative AI}
 

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  The AIBrix Team\\%\thanks{Use footnote for providing further information
    % about author (webpage, alternative address)---\emph{not} for acknowledging
    % funding agencies.} \\
  \texttt{maintainers@aibrix.ai} \\
}

% \author{%
%   David S.~Hippocampus\\%\thanks{Use footnote for providing further information
%     % about author (webpage, alternative address)---\emph{not} for acknowledging
%     % funding agencies.} \\
%   Department of Computer Science\\
%   Cranberry-Lemon University\\
%   Pittsburgh, PA 15213 \\
%   \texttt{hippo@cs.cranberry-lemon.edu} \\
%   % examples of more authors
%   % \And
%   % Coauthor \\
%   % Affiliation \\
%   % Address \\
%   % \texttt{email} \\
%   % \AND
%   % Coauthor \\
%   % Affiliation \\
%   % Address \\
%   % \texttt{email} \\
%   % \And
%   % Coauthor \\
%   % Affiliation \\
%   % Address \\
%   % \texttt{email} \\
%   % \And
%   % Coauthor \\
%   % Affiliation \\
%   % Address \\
%   % \texttt{email} \\
% }


\begin{document}


\maketitle


\begin{abstract}
  We introduce \sysname{}, a cloud-native, open-source framework designed to optimize and simplify large-scale LLM deployment in cloud environments.
  Unlike traditional cloud-native stacks, \sysname{} follows a co-design philosophy, ensuring every layer of the infrastructure is purpose-built for seamless integration with inference engines like vLLM.

  \sysname{} introduces several key innovations to reduce inference costs and enhance performance including high-density LoRA management for dynamic adapter scheduling, LLM-specific autoscalers, and prefix-aware, load-aware routing.
  To further improve efficiency, \sysname{} incorporates a distributed KV cache, boosting token reuse across nodes, leading to a 50\% increase in throughput and a 70\% reduction in inference latency.
  \sysname{} also supports unified AI runtime which streamlines model management while maintaining vendor-agnostic engine compatibility.

  
  For large-scale multi-node inference, \sysname{} employs hybrid orchestration—leveraging Kubernetes for coarse-grained scheduling and Ray for fine-grained execution—to balance efficiency and flexibility.
  Additionally, an SLO-driven GPU optimizer dynamically adjusts resource allocations, optimizing heterogeneous serving to maximize cost efficiency while maintaining service guarantees.
  Finally, \sysname{} enhances system reliability with AI accelerator diagnostic tools, enabling automated failure detection and mock-up testing to improve fault resilience.
  \sysname{} is available at \textcolor{red}{https://github.com/vllm-project/aibrix}.

\end{abstract}




\input{text/intro}
\input{text/motivation}
\input{text/architecture}
% \input{text/evaluation}
\input{text/conclusion}




% \printbibliography

% \bibliographystyle{plainnat}
% \bibliography{references.bib}

\bibliographystyle{plainnat}
% \bibliographystyle{unsrt}
\bibliography{main.bib}

% \appendix

% \section{Appendix / supplemental material}

\appendix
\section{List of Authors}

\sysname{} was made possible by the following team members:


\noindent
\renewcommand{\arraystretch}{1.5} % Adjust row height for better spacing
\begin{tabularx}{\textwidth}{@{}l>{\hspace{1.5cm}}X X r@{}}  
    Jiaxin Shan\textsuperscript{1} & Varun Gupta\textsuperscript{1} & Le Xu\textsuperscript{1} & Haiyang Shi\textsuperscript{1} \\
    Jingyuan Zhang\textsuperscript{1} & Ning Wang\textsuperscript{1} & Linhui Xu\textsuperscript{1} & Rong Kang\textsuperscript{1} \\
    Tongping Liu\textsuperscript{1} & Yifei Zhang\textsuperscript{1} & Yiqing Zhu\textsuperscript{1} & Shuowei Jin\textsuperscript{1,2} \\
    Gangmuk Lim\textsuperscript{1,3} & Binbin Chen\textsuperscript{1} & Zuzhi Chen\textsuperscript{1} & Xiao Liu\textsuperscript{1} \\
    Xin Chen\textsuperscript{1} & Kante Yin\textsuperscript{4} & Chak-Pong Chung\textsuperscript{1} & Chenyu Jiang\textsuperscript{1} \\
    Yicheng Lu\textsuperscript{1} & Jianjun Chen\textsuperscript{1} & Caixue Lin\textsuperscript{1} & Wu Xiang\textsuperscript{1} \\
    Rui Shi\textsuperscript{1} & Liguang Xie\textsuperscript{1} & & \\
\end{tabularx}

\footnotetext[1]{ByteDance.}
\footnotetext[2]{University of Michigan. Work done as part of the internship. }
\footnotetext[3]{University of Illinois at Urbana Champaign. Work done as part of the internship.}
\footnotetext[4]{DaoCloud}



Each contributor played a vital role in ensuring the success of AIBrix through technical development, performance evaluation, and strategic business guidance.







% Optionally include supplemental material (complete proofs, additional experiments and plots) in appendix.
% All such materials \textbf{SHOULD be included in the main submission.}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}