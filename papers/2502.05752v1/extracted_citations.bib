@inproceedings{behley2018rss,
  author    = {J. Behley and C. Stachniss},
  title     = {{Efficient Surfel-Based SLAM using 3D Laser Range Data in Urban Environments}},
  booktitle = rss,
  year      = 2018,
  videourl  = {https://www.youtube.com/watch?v=-AEX203rXkE},
  url       = {http://www.roboticsproceedings.org/rss14/p16.pdf}
}

@article{hornung2013ar,
  author  = {A. Hornung and K.M. Wurm and M. Bennewitz and C. Stachniss and W. Burgard},
  title   = {{OctoMap: An Efficient Probabilistic 3D Mapping Framework Based on Octrees}},
  journal = ar,
  volume  = 34,
  number  = 3,
  pages   = {189--206},
  year    = 2013,
  url     = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/hornung13auro.pdf}
}

@article{kazhdan2013acmgraphics,
  author   = {M. Kazhdan and H. Hoppe},
  journal  = acmgraphics,
  title    = {Screened poisson surface reconstruction},
  year     = {2013},
  number   = {3},
  pages    = {1--13},
  volume   = {32},
  keywords = {3D Surface Reconstruction},
  url      = {https://www.cs.jhu.edu/~misha/MyPapers/ToG13.pdf}
}

@inproceedings{klingensmith2015rss,
  author    = {M. Klingensmith and I. Dryanovski and S. Srinivasa and J. Xiao},
  booktitle = rss,
  title     = {Chisel: Real Time Large Scale 3D Reconstruction Onboard a Mobile Device using Spatially Hashed Signed Distance Fields.},
  year      = {2015},
  keywords  = {3D Surface Reconstruction},
  url       = {http://www.roboticsproceedings.org/rss11/p40.pdf}
}

@inproceedings{mescheder2019cvpr,
  title     = {{Occupancy networks: Learning 3d reconstruction in function space}},
  author    = {Mescheder, Lars and Oechsle, Michael and Niemeyer, Michael and Nowozin, Sebastian and Geiger, Andreas},
  booktitle = cvpr,
  year      = {2019},
  url       = {https://openaccess.thecvf.com/content_CVPR_2019/papers/Mescheder_Occupancy_Networks_Learning_3D_Reconstruction_in_Function_Space_CVPR_2019_paper.pdf}
}

@inproceedings{mildenhall2020eccv,
  title     = {{NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis}},
  author    = {B. Mildenhall and P.P. Srinivasan and M. Tancik and J.T. Barron and R. Ramamoorthi and R. Ng},
  booktitle = eccv,
  year      = {2020},
  url       = {https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123460392.pdf},
  abstract  = {We  present  a  method  that  achieves  state-of-the-art  resultsfor synthesizing novel views of complex scenes by optimizing an under-lying  continuous  volumetric  scene  function  using  a  sparse  set  of  inputviews.  Our  algorithm  represents  a  scene  using  a  fully-connected  (non-convolutional) deep network, whose input is a single continuous 5D coor-dinate (spatial location $(x,y,z)$ and viewing direction $(\theta,\phi)$) and whoseoutput  is  the  volume  density  and  view-dependent  emitted  radiance  atthat spatial location. We synthesize views by querying 5D coordinatesalong camera rays and use classic volume rendering techniques to projectthe output colors and densities into an image. Because volume renderingis naturally differentiable, the only input required to optimize our repre-sentation is a set of images with known camera poses. We describe how toeffectively optimize neural radiance fields to render photorealistic novelviews of scenes with complicated geometry and appearance, and demon-strate results that outperform prior work on neural rendering and viewsynthesis. View synthesis results are best viewed as videos, so we urgereaders to view our supplementary video for convincing comparisons.}
}

@article{mueller2022acmgraphics,
  title   = {Instant Neural Graphics Primitives with a Multiresolution Hash Encoding},
  author  = {Thomas M\"uller and Alex Evans and Christoph Schied and Alexander Keller},
  journal = acmgraphics,
  volume  = {41},
  number  = {4},
  year    = {2022},
  pages   = {102:1--102:15},
  url     = {https://dl.acm.org/doi/pdf/10.1145/3528223.3530127}
}

@inproceedings{newcombe2011ismar,
  author    = {R. A. Newcombe and S. Izadi and O. Hilliges and D. Molyneaux and D. Kim and A. J. Davison and P. Kohli and J. Shotton and S. Hodges and A. Fitzgibbon},
  title     = {{KinectFusion: Real-Time Dense Surface Mapping and Tracking}},
  booktitle = ismar,
  year      = 2011,
  keywords  = {RGB-D, SLAM, Mapping},
  abstract  = {We present a system for accurate real-time mapping of complex and arbitrary indoor scenes in variable lighting conditions, using only a moving low-cost depth camera and commodity graphics hardware. We fuse all of the depth data streamed from a Kinect sensor into a single global implicit surface model of the observed scene in real-time. The current sensor pose is simultaneously obtained by tracking the live depth frame relative to the global model using a coarse-to-fine iterative closest point (ICP) algorithm, which uses all of the observed depth data available. We demonstrate the advantages of tracking against the growing full surface model compared with frame-to-frame tracking, obtaining tracking and mapping results in constant time within room sized scenes with limited drift and high accuracy. We also show both qualitative and quantitative results relating to various aspects of our tracking and mapping system. Modelling of natural scenes, in real-time with only commodity sensor and GPU hardware, promises an exciting step forward in augmented reality (AR), in particular, it allows dense surfaces to be reconstructed in real-time, with a level of detail and robustness beyond any solution yet presented using passive computer vision.},
  url       = {https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/ismar2011.pdf}
}

@inproceedings{oleynikova2017iros,
  author    = {H. Oleynikova and Z. Taylor and M. Fehr and R. Siegwar and J. Nieto},
  booktitle = iros,
  title     = {Voxblox: Incremental 3d euclidean signed distance fields for on-board mav planning},
  year      = {2017},
  keywords  = {3D Surface Reconstruction},
  url       = {https://helenol.github.io/publications/iros_2017_voxblox.pdf}
}

@inproceedings{ortiz2022rss,
  title     = {iSDF: Real-Time Neural Signed Distance Fields for Robot Perception},
  author    = {Ortiz, Joseph and Clegg, Alexander and Dong, Jing and Sucar, Edgar and Novotny, David and Zollhoefer, Michael and Mukadam, Mustafa},
  booktitle = rss,
  year      = {2022},
  url       = {https://arxiv.org/pdf/2204.02296.pdf}
}

@inproceedings{park2019cvpr,
  author    = {Park, Jeong Joon and Florence, Peter and Straub, Julian and Newcombe, Richard and Lovegrove, Steven},
  title     = {{DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation}},
  booktitle = cvpr,
  year      = {2019},
  url       = {https://openaccess.thecvf.com/content_CVPR_2019/papers/Park_DeepSDF_Learning_Continuous_Signed_Distance_Functions_for_Shape_Representation_CVPR_2019_paper.pdf}
}

@inproceedings{rematas2022cvpr,
  title     = {Urban Radiance Fields},
  author    = {Rematas, Konstantinos and Liu, Andrew and Srinivasan, Pratul P and Barron, Jonathan T and Tagliasacchi, Andrea and Funkhouser, Thomas and Ferrari, Vittorio},
  booktitle = cvpr,
  year      = {2022},
  url       = {https://urban-radiance-fields.github.io/images/go_urf.pdf}
}

@inproceedings{stachniss2005rss,
  title     = {{Information Gain-based Exploration Using Rao-Blackwellized Particle Filters}},
  author    = {C. Stachniss and G. Grisetti and W. Burgard},
  booktitle = rss,
  year      = 2005,
  url       = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/stachniss05rss.pdf}
}

@inproceedings{sucar2021iccv,
  title     = {imap: Implicit mapping and positioning in real-time},
  author    = {Sucar, Edgar and Liu, Shikun and Ortiz, Joseph and Davison, Andrew J},
  booktitle = iccv,
  year      = {2021},
  url       = {proceedings:sucar2021iccv.pdf}
}

@article{thrun2001ai,
  title   = {{Robust Monte Carlo Localization for Mobile Robots}},
  author  = {S. Thrun and D. Fox and W. Burgard and F. Dellaert},
  journal = ai,
  volume  = {128},
  number  = {1-2},
  year    = {2001},
  url     = {https://www.sciencedirect.com/science/article/pii/S0004370201000698}
}

@inproceedings{vizzo2021icra,
  author    = {I. Vizzo and X. Chen and N. Chebrolu and J. Behley and C. Stachniss},
  title     = {{Poisson Surface Reconstruction for LiDAR Odometry and Mapping}},
  booktitle = icra,
  year      = 2021,
  url       = {http://www.ipb.uni-bonn.de/pdfs/vizzo2021icra.pdf}
}

@article{vizzo2022sensors,
  author   = {Vizzo, Ignacio and Guadagnino, Tiziano and Behley, Jens and Stachniss, Cyrill},
  title    = {{VDBFusion: Flexible and Efficient TSDF Integration of Range Sensor Data}},
  journal  = sensors,
  url      = {https://www.mdpi.com/1424-8220/22/3/1296/pdf},
  volume   = {22},
  year     = {2022},
  number   = {3},
  pages    = {1296},
  abstract = {Mapping is a crucial task in robotics and a fundamental building block of most mobile systems deployed in the real world. Robots use different environment representations depending on their task and sensor setup. This paper showcases a practical approach to volumetric surface reconstruction based on truncated signed distance functions, also called TSDFs. We revisit the basics of this mapping technique and offer an approach for building effective and efficient real-world mapping systems. In contrast to most state-of-the-art SLAM and mapping approaches, we are making no assumptions on the size of the environment nor the employed range sensor. Unlike most other approaches, we introduce an effective system that works in multiple domains using different sensors. To achieve this, we build upon the Academy-Award-winning OpenVDB library used in filmmaking to realize an effective 3D map representation. Based on this, our proposed system is flexible and highly effective and, in the end, capable of integrating point clouds from a 64-beam LiDAR sensor at 20 frames per second using a single-core CPU. Along with this publication comes an easy-to-use C++ and Python library to quickly and efficiently solve volumetric mapping problems with TSDFs.}
}

@inproceedings{wang2021neurips,
  title     = {NeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction},
  author    = {Wang, Peng and Liu, Lingjie and Liu, Yuan and Theobalt, Christian and Komura, Taku and Wang, Wenping},
  booktitle = neurips,
  year      = {2021},
  url       = {https://arxiv.org/pdf/2106.10689.pdf}
}

@inproceedings{whelan2015rss,
  author    = {T. Whelan and S. Leutenegger and R. S. Moreno and B. Glocker and A. Davison},
  title     = {{ElasticFusion: Dense SLAM Without A Pose Graph}},
  booktitle = rss,
  year      = {2015},
  abstract  = {We present a novel approach to real-time dense visual SLAM. Our system is capable of capturing comprehensive dense globally consistent surfel-based maps of room scale environments explored using an RGB-D camera in an incremental online fashion, without pose graph optimisation or any post-processing steps. This is accomplished by using dense frame-to-model camera tracking and windowed surfel-based fusion coupled with frequent model refinement through non-rigid surface deformations. Our approach applies local model-to-model surface loop closure optimisations as often as possible to stay close to the mode of the map distribution, while utilising global loop closure to recover from arbitrary drift and maintain global consistency.},
  keywords  = {RGB-D Perception, SLAM, Mapping},
  url       = {proceedings:whelan2015rss.pdf}
}

@inproceedings{zhang2014rss,
  author    = {J. Zhang and S. Singh},
  title     = {{LOAM: Lidar Odometry and Mapping in Real-time}},
  booktitle = rss,
  year      = {2014},
  abstract  = {We propose a real-time method for odometry and mapping using range measurements from a 2-axis lidar moving in 6-DOF. The problem is hard because the range measurements are received at different times, and errors in motion estimation can cause mis-registration of the resulting point cloud. To date, coherent 3D maps can be built by off-line batch methods, often using loop closure to correct for drift over time. Our method achieves both low-drift and low-computational complexity without the need for high accuracy ranging or inertial measurements. The key idea in obtaining this level of performance is the division of the complex problem of simultaneous localization and mapping, which seeks to optimize a large number of variables simultaneously, by two algorithms. One algorithm performs odometry at a high frequency but low fidelity to estimate velocity of the lidar. Another algorithm runs at a frequency of an order of magnitude lower for fine matching and registration of the point cloud. Combination of the two algorithms allows the method to map in real-time. The method has been evaluated by a large set of experiments as well as on the KITTI odometry benchmark. The results indicate that the method can achieve accuracy at the level of state of the art offline batch methods.},
  url       = {proceedings:zhang2014rss.pdf}
}

@inproceedings{zhong2023icra,
  author    = {Zhong, Xingguang and Pan, Yue and Behley, Jens and Stachniss, Cyrill},
  title     = {{SHINE-Mapping: Large-Scale 3D Mapping Using Sparse Hierarchical Implicit Neural Representations}},
  booktitle = icra,
  year      = 2023,
  codeurl   = {https://github.com/PRBonn/SHINE_mapping},
  url       = {https://www.ipb.uni-bonn.de/wp-content/papercite-data/pdf/zhong2023icra.pdf}
}

