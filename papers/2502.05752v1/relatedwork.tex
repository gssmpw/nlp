\section{Related Work}
\label{sec:related}

% Discuss the main related work and cite around key papers. The number
% of cites depend on the page limit. Never below 15, 20-30 is often fine
% for conference papers.
% The related work section should be approx. 1 column long, assuming 
% a 6-page paper, up to one page for 8-10 page papers.  You can structure 
% the  section in paragraphs, grouping the  papers, and describing the key 
% approaches with 1-2 sentences. Avoid dull enumerations of papers. If 
% applicable, describe the key difference to your approach at the end 
% of each paragraph briefly. Avoid adding subsections, al least for a 
% conference paper.


\subsection{Point-based Implicit Neural Representation}


Robotics has long relied on explicit map representations with discrete primitives like point clouds~\cite{zhang2014rss}, surfels~\cite{whelan2015rss, behley2018rss}, meshes~\cite{vizzo2021icra}, or voxel grids~\cite{hornung2013ar,newcombe2011ismar} for core tasks like localization~\cite{thrun2001ai} and planning~\cite{stachniss2005rss}.
%

Recently, implicit neural representations have been proposed to model radiance fields~\cite{mildenhall2020eccv} and geometric (occupancy or distance) fields~\cite{mescheder2019cvpr, park2019cvpr, ortiz2022rss} using \mbox{multi-layer perceptrons (MLP)}. 
%
These continuous representations offer advantages like compact storage, and better handling of regions with sparse observations or occlusions, while supporting conversion to explicit representations for downstream tasks.
%

Instead of using a single MLP for the entire scene, recent methods use hybrid representations that combine local feature vectors with a shared shallow MLP.
% Explain neural point here
Point-based implicit neural representations~\cite{xu2022cvpr-pointnerf, pan2024tro-pin} store optimizable features in a neural point cloud, which has advantages over grid-based alternatives through its flexible spatial layout and inherent elasticity under transformations for example caused by loop closures.
%

%
Point-based implicit neural representations have been used for modeling either radiance fields or distance fields for various applications including differentiable rendering~\cite{xu2022cvpr-pointnerf, chen2023iccv-neurbf}, dynamic scene modeling~\cite{abou-chakra2024wacv}, surface reconstruction~\cite{li2022cvpr-dccdif}, visual odometry~\cite{sandstrom2023iccv-pointslam,zhang2024eccv-glorie}, and globally consistent mapping~\cite{pan2024tro-pin}. 
%
In particular, PIN-SLAM~\cite{pan2024tro-pin} demonstrates the effectiveness of representing local distance fields for odometry estimation and using its elasticity during loop closure correction.
%

In this work, we propose a novel LiDAR-visual SLAM system inspired by PIN-SLAM~\cite{pan2024tro-pin} and Scaffold-GS~\cite{lu2024cvpr-scaffoldgs} encoding a Gaussian splatting radiance field within the neural points, and jointly optimizing it with the distance field.
%
Compared to NeRF-based approaches~\cite{xu2022cvpr-pointnerf, chen2023iccv-neurbf}, this also offers much faster novel view rendering suitable for robotics applications.



\subsection{Gaussian Splatting Radiance Field}

NeRF~\cite{mildenhall2020eccv} pioneered the use of MLPs to map 3D positions and view directions to color and volume density, encoding radiance fields through volume rendering-based training with posed RGB images.
%
% The rendering photorealism, training and inference efficiency as well as the scalability have been later improved to a certain extent by using either the proper anti-aliasing~\cite{barron2021iccv-mipnerf, barron2022cvpr-mipnerf360} or the hybrid representations~\cite{sun2022cvpr-dvgo, mueller2022acmgraphics, xu2022cvpr-pointnerf, chen2022eccv-tensorf} combining optimizable local features and a shallow global MLP.
% Add more citations, such as TensorRF
More recently, 3DGS~\cite{kerbl2023tog-3dgs} introduced explicit 3D Gaussian primitives to represent the radiance fields, achieving high-quality novel view synthesis.
%
Compared to NeRF-based methods, 3DGS is more efficient by using primitive-based differentiable rasterization~\cite{wang2019tog-dss} instead of ray-wise volume rendering.
%
The explicit primitives also enables editing and manipulation of the radiance field.
%
These properties make 3DGS promising for robotics applications~\cite{jin2024arxiv-activegs, li2024arxiv-activesplat, matsuki2024cvpr-monogs, wildersmith2024iros, abou-chakra2024corl}.
%
However, two main challenges limit its usage: geometric accuracy and scalability for incremental mapping.
% 
We discuss the related works addressing geometric accuracy in the following and addressing scalable mapping in \secref{subsec:large_scale_3d_reconstruction}.



While 3DGS achieves high-fidelity photorealistic rendering, it often lacks the geometric accuracy. 
% needed for robotics applications.
%
To tackle this limitation, SuGaR~\cite{guedon2024cvpr-sugar} uses a hybrid representation to extract meshes from 3DGS and align the Gaussian primitives with the surface meshes.
%
To address the ambiguity in surface description, another solution is to flatten the 3D Gaussian ellipses to 2D disks~\cite{huang2024siggraph-2dgs, dai2024siggraph-gaussian-surfels, zhang2024arxiv-radegs, jiang2024arxiv-ligs}.
%
The 2D disks gradually align with surfaces during training, enabling more accurate depth and normal rendering.
%
However, extracting surface meshes from these discrete primitives still requires either TSDF fusion~\cite{newcombe2011ismar} with rendered depth or Poisson surface reconstruction~\cite{kazhdan2013acmgraphics}. 
% with post-processing.
%

Another line of works~\cite{yu2024tog-gof, song2024neurips-gvkf} model discrete Gaussian opacity as a continuous field, similar to NeRF-based surface reconstruction~\cite{wang2021neurips}.
%
Several works~\cite{chen2023arxiv-neusg, lyu2024tog-3dgsr, yu2024neurips-gsdf} jointly train a distance field with 3DGS and align the Gaussian primitives with the zero-level set of the distance field to achieve accurate surface reconstruction.
%
However, these methods rely solely on image rendering supervision for both 3DGS and neural SDF training without direct 3D geometric constraints, leading to ambiguities in textureless or specular regions.
%
The volume rendering-based SDF training also impacts efficiency.
%

While 3DGS originally uses structure-from-motion point clouds, robotic platforms with LiDAR can initialize primitives directly from LiDAR measurements~\cite{cui2024tog-letsgo, hong2024ral-livgaussmap, xie2024arxiv-gslivm}.
%
Direct depth measurements can further supervise depth rendering to improve geometric accuracy and convergence speed~\cite{matsuki2024cvpr-monogs, jiang2024arxiv-ligs}.

% Different from aforementioned works, our approach adopts the geometrically consistent 2D Gaussian primitives and train them together with a neural distance field supervised by direct depth measurements from LiDAR.
%
% We unify the Gaussian splatting radiance field and SDF in one single implicit neural map representation and enforce the consistency of the primitives and the distance field to improve the geometric accuracy of both fields.
%
% Recent work GSFusion~\cite{wei2024ral-gsfusion} maintains a voxel-based distance field and a Gaussian splatting radiance field simultaneously. However, the two fields are decoupled without mutual supervision.
%
Our approach uniquely combines geometrically consistent 2D Gaussian disks with a neural distance field supervised by direct LiDAR measurements, enforcing mutual geometric consistency between the representations.
%
This differs from GSFusion~\cite{wei2024ral-gsfusion}, which maintains decoupled distance and radiance fields without mutual supervision.
%

\subsection{Large-Scale 3D Reconstruction}
\label{subsec:large_scale_3d_reconstruction}

This paper focuses on online large-scale 3D reconstruction.
%, particularly for block and city-scale scenes.
% at ground level.
%
There have been numerous works for the scalable occupancy or distance field mapping in the past decade, using efficient data structures such as Octree~\cite{hornung2013ar, zhong2023icra}, voxel hashing~\cite{klingensmith2015rss, oleynikova2017iros, zhong2024cvpr},  VDB~\cite{vizzo2022sensors, wu2024ral-vdbgpdf}, or wavelets~\cite{reijgwart2023rss-wavemap}.
%

Scalable radiance field mapping has also made significant progress recently.
%
For large scale scenes captured by aerial images, recent works~\cite{lu2024cvpr-scaffoldgs, ren2024arxiv-octreegs, liu2024eccv-citygaussian} demonstrate promising results using level-of-detail rendering and neural Gaussian compression.
%
For driving scenes with short sequences containing hundreds of images, both NeRF-based~\cite{rematas2022cvpr, yang2023cvpr-unisim} and 3DGS-based~\cite{zhou2024cvpr-drivinggs, yan2024eccv-streetgs, zhao2024eccv-tclcgs, fischer2024neurips-dgf, chen2024arxiv-omnire, hess2024arxiv-splatad} approaches have demonstrated high-fidelity offline radiance field reconstruction, enabling closed-loop autonomous driving simulation~\cite{yang2023cvpr-unisim, chen2024arxiv-omnire}.
%

%
However, radiance field mapping for even larger scenes at ground level with thousands of images remains challenging due to scene complexity and memory constraints.
%
\mbox{BlockNeRF}~\cite{tancik2022cvpr-blocknerf} addresses this by dividing scenes into overlapping blocks, training separate NeRFs per block, and consolidating them during rendering. 
%
Similarly, SiLVR~\cite{tao2024icra-silvr} employs a submap strategy for scalable NeRF mapping.
%
For 3DGS, hierarchical 3DGS~\cite{kerbl2024tog-hierarchical3dgs} introduces a level-of-detail hierarchy that enables real-time rendering of city-scale scenes. 
% with tens of thousands of images.
% Firstly, the large scale geo-referencing of images is a non-trivial task. 
% For in a driving scene, an offline NeRF or 3DGS-based radiance field construction have achieved great success in the past few years.
%
The aforementioned methods require time-consuming structure-from-motion preprocessing and offline divide-and-conquer processing, limiting their applicability for online missions. 
%

While there are several works on incremental mapping and SLAM with NeRF~\cite{sucar2021iccv, ortiz2022rss, sandstrom2023iccv-pointslam} or 3DGS~\cite{matsuki2024cvpr-monogs,keetha2024cvpr-splatam,zhu2025threedv-loopsplat,wei2024ral-gsfusion}, they primarily focus on bounded indoor scenes and struggle with our target scenarios.
%
Our proposed system enables incremental radiance and distance field mapping at the scale of previous offline methods~\cite{tancik2022cvpr-blocknerf, kerbl2024tog-hierarchical3dgs}, while achieving globally consistent 3D reconstruction through loop closure correction.
% with the proposed neural point map.
%
% Additionally, with the proposed point-based implicit neural map, we can achieve a globally consistent 3D reconstruction when incorporated into a SLAM system with the loop closure correction capability.


% NeuralSceneGraph, DrivingGaussians~\cite{zhou2024cvpr-drivinggs}, StreetGaussians~\cite{yan2024eccv-streetgs}, HUGS~\cite{zhou2024cvpr-hugs}, Dynamic Gaussian Field~\cite{fischer2024neurips-dgf}, OmniRe~\cite{chen2024arxiv-omnire}, SplatAD~\cite{hess2024arxiv-splatad}
% BungeeNeRF~\cite{xiangli2022eccv-bungeenerf}, Scaffold-GS~\cite{lu2024cvpr-scaffoldgs} and Octree-GS~\cite{ren2024arxiv-octreegs}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%