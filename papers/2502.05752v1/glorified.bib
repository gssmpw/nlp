@STRING{aaai    = {Proc.~of the Conf.~on Advancements of Artificial Intelligence (AAAI)} }
@STRING{aaaiold = {Proc.~of the National Conf.~on Artificial Intelligence (AAAI)} }
@STRING{aamas   = {Proc.~of the International Conf.~on Autonomous Agents and Multi-Agent Systems (AAMAS)}}
@STRING{ac      = {IEEE Trans. on Automatic Control} }
@STRING{acc     = {Proc.~of the IEEE American Control Conference (ACC)} }
@STRING{accv    = {Proc.~of the Asian Conf.~on Computer Vision (ACCV)} }
@STRING{acmcs   = {ACM Computing Surveys} }
@STRING{acmgraphics={ACM Trans.~on Graphics} }
@STRING{acmsuist={ACM Symposium on User Interface Software and Technology} }
@STRING{acra    = {Proc.~of the Australasian Conf.~on Robotics and Automation (ACRA)} }
@STRING{addison = {Addison-Wesley Publishing Inc.} }
@STRING{advancedrobotics={Advanced Robotics} }
@STRING{agsys    = {Agricultural Systems} }
@STRING{ai      = {Artificial Intelligence} }
@STRING{ams     = {Proc.~of Autonome Mobile Systeme} }
@STRING{annurevcontrol = {Annual Review of Control, Robotics, and Autonomous Systems} }
@STRING{ar      = {Autonomous Robots} }
@STRING{arpb = {Annual review of plant biology}}
@STRING{arxiv   = {arXiv preprint} }
@STRING{biosyseng={Biosystems Engineering} }
@STRING{bmcbio = {BMC Bioinformatics} }
@STRING{bmvc    = {Proc.~of British Machine Vision Conference (BMVC)} }
@STRING{case    = {Proc.~of the International Conf.~on Automation Science and Engineering (CASE)} }
@STRING{cacm    = {Communications of the ACM} }
@STRING{ccvw    = {Proc.~of the Croation Computer Vision Workshop (CCVW)} }
@STRING{cdc     = {Proceedings of the Conference on Decision Making and Control (CDC)}}
@STRING{cea     = {Computers and Electronics in Agriculture} }
@STRING{cigr    = {Proc.~of the International Conf.~of Agricultural Engineering (CIGR)} }
@STRING{cira    = {Proc.~of the IEEE Intl.~Symp. on Computer Intelligence in Robotics and Automation (CIRA)} }
@STRING{cob = {Current opinion in biotechnology}}
@STRING{cogsys  = {Proc.~of the Intl.~Conf.~on Cognitive Systems (CogSys)} }
@STRING{corl   = {Proc.~of the Conf.~on Robot Learning (CoRL)} }
@STRING{crews   = {Proc.~of the SPIE Conf.~on Reconnaissance and Electronic Warfare System} }
@STRING{cvgip   = {Computer vision, graphics, and image processing} }
@STRING{cviu    = {Journal of Computer Vision and Image Understanding (CVIU)} }
@STRING{cvpr    = {Proc.~of the IEEE/CVF Conf.~on Computer Vision and Pattern Recognition (CVPR)} }
@STRING{cvprold = {Proc.~of the IEEE Conf.~on Computer Vision and Pattern Recognition (CVPR)} }
@STRING{cvprws    = {Proc.~of the IEEE/CVF Conf. on Computer Vision and Pattern Recognition Workshops} }
@STRING{cvvt    = {Proc.~of the Intl.~Workshop on Computer Vision in Vehicle Technology (CVVT)} }
@STRING{crv     = {Proc.~of the Conf.~on Computer and Robot Vision (CRV)} }
@STRING{dagm    = {Proc.~of the Symposium of the German Association for Pattern Recognition (DAGM)} }
@STRING{dagstuhl= {Proc.~of the Dagstuhl Seminar} }
@STRING{dars    = {Distributed Autonomous Robotic Systems}}
@STRING{dgpf    = {Proc.~of the Conf.~of the German Society for Photogrammetry, Remote Sensing and Geoinformation (DGPF)} }
@STRING{eccv    = {Proc.~of the Europ.~Conf.~on Computer Vision (ECCV)} }
@STRING{eccvws    = {Proc.~of the Europ.~Conf.~on Computer Vision Workshops}}
@STRING{ecmr    = {Proc.~of the Europ.~Conf.~on Mobile Robotics (ECMR)} }
@STRING{ecml = {Proc.~of the Europ.~Conf.~on Machine Learning (ECML)}}
@STRING{emav    = {Proc.~of the European Micro Aerial Vehicle Conference} }
@STRING{esa     = {Expert Systems with Applications} }
@STRING{etfa    = {Proc.~of the IEEE Int.~Conf.~on Emerging Technologies Factory Automation (ETFA)} }
@STRING{euros   = {Proc.~of the Europ.~Robotics Symp. (EUROS)} }
@STRING{fntr    = {Foundations and Trends in Robotics} }
@STRING{fps = {Frontiers in plant science} }
@STRING{gcpr    = {Proc.~of the German Conf.~on Pattern Recognition (GCPR)} }
@STRING{grsl    = {IEEE Geoscience and Remote Sensing Letters} }
@STRING{humanoids={Proc.~of the IEEE Intl.~Conf.~on Humanoid Robots} }
@STRING{ias     = {Proc. of Int.~Conf.~on Intelligent Autonomous Systems (IAS)} }
@STRING{icar    = {Proc.~of the Int.~Conf.~on Advanced Robotics (ICAR)} }
@STRING{icces   = {Proc.~of the Intl.~Conf.~on Computer Engineering Systems (ICCES)} }
@STRING{iccv    = {Proc.~of the IEEE/CVF Intl.~Conf.~on Computer Vision (ICCV)} }
@STRING{iccvold = {Proc.~of the IEEE Intl.~Conf.~on Computer Vision (ICCV)} }
@STRING{iccvws  = {Proc.~of the Int.~Conf.~on Computer Vision Workshops} }
@STRING{iciap   = {Proc.~of the Intl.~Conf.~on Image Analysis and Processing (ICIAP)} }
@STRING{icif    = {Proc.~of the Int.~Conf.~on Information Fusion} }
@STRING{icip    = {Proc.~of the IEEE Intl.~Conf.~on Image Processing (ICIP)} }
@STRING{iclr    = {Proc.~of the Int.~Conf.~on Learning Representations (ICLR)}}
@STRING{icme    = {Proc.~of the IEEE Intl.~Conf.~on Multimedia and Expo}}
@STRING{icml    = {Proc.~of the Intl.~Conf.~on Machine Learning (ICML)} }
@STRING{icpr    = {Proc.~of the Intl.~Conf.~on Pattern Recognition (ICPR)} }
@STRING{icra    = {Proc.~of the IEEE Intl.~Conf.~on Robotics \& Automation (ICRA)} }
@STRING{icuas   = {Proc.~of the Intl.~Conf.~on Unmanned Aircraft Systems (ICUAS)} }
@STRING{ieeepress={IEEE Computer Society Press} }
@STRING{ifac    = {IFAC-PapersOnLine}}
@STRING{ijabe     = {Int.~J.~Agric \& Biol.~Eng.} }
@STRING{ijars   = {Intl.~Journal of Advanced Robotic Systems} }
@STRING{ijcai   = {Proc.~of the Intl.~Conf.~on Artificial Intelligence (IJCAI)} }
@STRING{ijcv    = {Intl.~Journal~of Computer Vision (IJCV)} }
@STRING{ijf    = {Intl.~Journal~of Forecasting (IJF)} }
@STRING{ijgi    = {Intl.~Journal of Geo-Information} }
@STRING{ijhr    = {The Int.~Journal of Humanoid Robotics (IJHR)} }
@STRING{ijrr    = {Intl.~Journal~of Robotics Research (IJRR)} }
@STRING{ijrs = {International Journal of Remote Sensing}}
@STRING{ijsip   = {Int. Journal of Signal Processing, Image Processing and Pattern Recognition} }
@STRING{imvip   = {Proc.~of the Irish Machine Vision and Image Processing Conference (IMVIP)} }
@STRING{iros    = {Proc.~of the IEEE/RSJ Intl.~Conf.~on Intelligent Robots and Systems (IROS)} }
@STRING{iser    = {Proc.~of the Intl.~Sym.~on Experimental Robotics (ISER)} }
@STRING{isic    = {Proc.~of the Intl.~Symp. on Intelligent Control (ISIC)}}
@STRING{ismar   = {Proc.~of the Intl.~Symposium~on Mixed and Augmented Reality (ISMAR)} }
@STRING{isprsannals={ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences} }
@STRING{isprsarchives={ISPRS Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences} }
@STRING{isrr    = {Proc.~of the Intl.~Symposium~on Robotic Research (ISRR)} }
@STRING{itsc    = {Proc.~of the IEEE Intl.~Conf.~on Intelligent Transportation Systems (ITSC)} }
@STRING{iv      = {Proc.~of the IEEE Vehicles Symposium (IV)} }
@STRING{ivc     = {Journal on Image and Vision Computing (IVC)} }
@STRING{jacm    = {Journal of the ACM} }
@STRING{jaer    = {Journal of Agricultural Engineering Research} }
@STRING{jair    = {Journal of Artificial Intelligence Research (JAIR)} }
@STRING{jbe     = {ASME Journal of Basic Engineering} }
@STRING{jfr     = {Journal of Field Robotics (JFR)} }
@STRING{jirs    = {Journal of Intelligent and Robotic Systems (JIRS)} }
@STRING{jmiv    = {Journal of Mathematical Imaging and Vision} }
@STRING{jmlr = {Journal on Machine Learning Research~(JMLR)}}
@STRING{jmp     = {Mathematical Programming}}
@STRING{joe     = {IEEE Journal of Oceanic Engineering} }
@STRING{jprs    = {ISPRS Journal of Photogrammetry and Remote Sensing (JPRS)} }
@STRING{jra     = {IEEE Journal of Robotics and Automation} }
@STRING{jras    = {Journal on Robotics and Autonomous Systems (RAS)} }
@STRING{jvcir   = {Journal of Visual Communication and Image Representation~(JVCIR)} }
@STRING{lsno    = {Large-Scale Nonlinear Optimization}}
@STRING{m2rsm = {Proc.~of the IEEE Intl.~Workshop on Multi-Platform/Multi-Sensor Remote Sensing and Mapping}}
@STRING{mcg     = {Proc.~of the Intl.~Conf.~on Machine Control and Guidance (MCG)} }
@STRING{mirage  = {Proc.~of the Intl.~Conf.~on Computer Vision/Computer Graphics Collaboration Techniques and Applications (MIRAGE)} }
@STRING{mitpress= {MIT Press} }
@STRING{mg    = {Mathematical Geosciences} }
@STRING{ml      = {Machine Learning} }
@STRING{mobicom = {Proc.~of the {ACM} Intl.~Conf.~on Mobile Computing and Networking (MobiCom)} }
@STRING{mor     = {Mathematics of Operations Research} }
@STRING{mpc     = {Mathematical Programming Computation}}
@STRING{mva     = {Proc.~of the IAPR Conf.~on Machine Vision Applications (MVA)} }
@STRING{nas     = {National Academy of Sciences}}
@STRING{neurips = {Proc.~of the Conf. on Neural Information Processing Systems (NeurIPS)} }
@STRING{neuroc = {Neurocomputing}}
@STRING{nips    = {Proc.~of the Advances in Neural Information Processing Systems (NIPS)} }
@STRING{nipsws  = {Proc.~of the Advances in Neural Information Processing Systems Workshops} }
@STRING{npl = {Neural processing letters} }
@STRING{nrlq = {Naval research logistics quarterly} }
@STRING{oceans  = {Proc.~of OCEANS MTS/IEEE Conference and Exhibition} }
@STRING{paa     = {Pattern Analysis and Applications} }
@STRING{pagri   = {Precision Agriculture} }
@STRING{pami    = {IEEE Trans.~on Pattern Analysis and Machine Intelligence (TPAMI)} }
@STRING{pcv     = {Proc.~of the ISPRS Conference on Photogrammeric Computer Vision (PCV)} }
@STRING{pers    = {Photogrammetric Engineering and Remote Sensing (PE\&RS)} }
@STRING{pfg     = {Photogrammetrie -- Fernerkundung -- Geoinformation (PFG)} }
@STRING{phowo   = {Proc.~of the Photogrammetric Week (PhoWo)} }
@STRING{pia     = {Proc.~of the ISPRS Conference on Photogrammeric Image Analysis (PIA)} }
@STRING{pieee   = {Proc.~of the IEEE} }
@STRING{plantphysiology = {Plant Physiology} }
@STRING{plosone = {PLOS ONE} }
@STRING{pnas    = {Proc.~of the National Academy of Sciences}}
@STRING{pr      = {Pattern Recognition} }
@STRING{prl     = {Pattern Recognition Letters} }
@STRING{ral     = {IEEE Robotics and Automation Letters (RA-L)} }
@STRING{ram     = {IEEE Robotics and Automation Magazine (RAM)} }
@STRING{ras     = {Journal on Robotics and Autonomous Systems (RAS)} }
@STRING{rasmag  = {IEEE Robotics and Automation Magazine} }
@STRING{rs      = {Remote Sensing} }
@STRING{rss     = {Proc.~of Robotics: Science and Systems (RSS)} }
@STRING{rssbook = {Robotics: Science and Systems} }
@STRING{scia    = {Proc.~of the Scandinavian Conference on Image Analysis} }
@STRING{sensors = {Sensors} }
@STRING{siam    = {Society for Industrial and Applied Mathematics (SIAM)}}
@STRING{sice    = {Proc.~of the Annual Conference of the Society of Instrument and Control Engineers (SICE)} }
@string{sii     = {Proc.~of the Intl.~Symp.~on System Integration (SII)} }
@STRING{siggraph    = {Proc.~of the Intl.~Conf.~on Computer Graphics and Interactive Techniques (SIGGRAPH)} }
@STRING{sirev   = {SIAM Review (SIREV)}}
@STRING{smc     = {Proc.~of the IEEE Intl.~Conf.~on Systems, Man, and Cybernetics (SMC)} }
@STRING{snowbird= {Proc.~of the Learning Workshop (Snowbird)} }
@STRING{soave   = {Proc.~of the Workshop on Self-Organization of AdaptiVE behavior (SOAVE)} }
@STRING{spiesdvrs={Proc.~of SPIE Stereoscopic Displays and Virtual Reality Systems} }
@STRING{spiev   = {Proc.~of SPIE Videometrics} }
@STRING{springer= {Springer Verlag} }
@STRING{springerstaradvanced={STAR Springer Tracts in Advanced Robotics} }
@STRING{ssns = {Sensors, Systems, and Next-Generation Satellites}}
@STRING{ssrr = {Proc.~of the IEEE Intl.~Sym.~on Safety, Security, and Rescue Robotics (SSRR)} }
@STRING{talg    = {ACM Transactions on Algorithms (TALG)}}
@STRING{tarj    = {The Australian Rangeland Journal} }
@STRING{tgrs = {IEEE Trans.~on Geoscience and Remote Sensing}}
@STRING{threedv = {Proc.~of the Intl.~Conf.~on 3D Vision (3DV)} }
@STRING{tip     = {IEEE Trans.~on Image Processing} }
@STRING{tits    = {IEEE Trans.~on Intelligent Transportation Systems (T-ITS)} }
@STRING{titsmag = {IEEE Trans.~on Intelligent Transportation Systems Magazine} }
@STRING{tiv     = {IEEE Trans.~on Intelligent Vehicles} }
@STRING{tmlr    = {Trans.~on Machine Learning Research (TMLR)} }
@STRING{tog     = {ACM Trans.~on Graphics (TOG)} }
@STRING{tpami   = {IEEE Trans.~on Pattern Analysis and Machine Intelligence (TPAMI)} }
@STRING{tra     = {IEEE Trans.~on Robotics and Automation} }
@STRING{tro     = {IEEE Trans.~on Robotics (TRO)} }
@STRING{tsmc   = {IEEE Trans.~on Systems, Man, and Cybernetics} }
@STRING{tvcg     = {IEEE Trans.~on Visualization and Computer Graphics} }
@STRING{uai     = {Proc.~of the Conf.~on Uncertainty in Artificial Intelligence (UAI)} }
@STRING{uavg    = {Proc.~of the Intl.~Conf.~on Unmanned Aerial Vehicles in Geomatics} }
@STRING{uust    = {Proc.~of the Intl.~Symp.~on Unmanned Untethered Submersible Technology} }
@STRING{vc      = {The Visual Computer (VC)} }
@STRING{vldb    = {Proc.~of the Intl. Conf. on Very Large Database (VLDB)} }
@STRING{visapp    = {Proc.~of the Intl.~Conf.~of Computer Vision Theory and Applications (VISAPP)} }
@STRING{vmv     = {Proc.~of Vision, Modeling, Visualization (VMV)} }
@STRING{wacv    = {Proc.~of the IEEE Winter Conf.~on Applications of Computer Vision (WACV)} }
@STRING{wafr    = {Intl.~Workshop on the Algorithmic Foundations of Robotics (WAFR)} }
@STRING{wcica     = {Proc.~of the World Congress on Intelligent Control and Automation} }

@article{driess2022arxiv,
  title   = {{Reinforcement Learning with Neural Radiance Fields}},
  author  = {Driess, Danny and Schubert, Ingmar and Florence, Pete and Li, Yunzhu and Toussaint, Marc},
  journal = arxiv,
  volume  = {arXiv:1908.07906},
  year    = {2022},
  url     = {https://arxiv.org/pdf/2206.01634.pdf}
}

@inproceedings{lin2022icra,
  title     = {{NeRF-Supervision: Learning Dense Object Descriptors from Neural Radiance Fields}},
  author    = {Yen-Chen, Lin and Florence, Pete and Barron, Jonathan T},
  booktitle = icra,
  year      = {2022},
  url       = {https://arxiv.org/pdf/2203.01913.pdf}
}

@article{vallicrosa2018sensors,
  title   = {{H-slam: Rao-blackwellized particle filter slam using hilbert maps}},
  author  = {Vallicrosa, Guillem and Ridao, Pere},
  journal = sensors,
  volume  = {18},
  number  = {5},
  pages   = {1386},
  year    = {2018},
  url     = {https://dugi-doc.udg.edu/bitstream/handle/10256/15517/HSLAM.pdf}
}

@inproceedings{senanayake2017corl,
  title     = {{Bayesian Hilbert Maps for Dynamic Continuous Occupancy Mapping}},
  author    = {Senanayake, Ransalu and Ramos, Fabio},
  booktitle = corl,
  year      = {2017},
  url       = {http://proceedings.mlr.press/v78/senanayake17a/senanayake17a.pdf}
}

@inproceedings{min2022cvpr,
  title     = {{LASER: LAtent SpacE Rendering for 2D Visual Localization}},
  author    = {Min, Zhixiang and Khosravan, Naji and Bessinger, Zachary and Narayana, Manjunath and Kang, Sing Bing and Dunn, Enrique and Boyadzhiev, Ivaylo},
  booktitle = cvpr,
  year      = {2022},
  url       = {https://arxiv.org/pdf/2204.00157.pdf}
}

@article{yilmaz2019ras,
  title   = {{Self-adaptive Monte Carlo method for indoor localization of smart AGVs using LIDAR data}},
  author  = {Yilmaz, Abdurrahman and Temeltas, Hakan},
  journal = ras,
  volume  = {122},
  pages   = {103285},
  year    = {2019}
}

@inproceedings{yan2021iccv,
  title     = {Continual neural mapping: Learning an implicit scene representation from sequential observations},
  author    = {Yan, Zike and Tian, Yuxin and Shi, Xuesong and Guo, Ping and Wang, Peng and Zha, Hongbin},
  booktitle = iccv,
  year      = {2021},
  url       = {https://arxiv.org/pdf/2108.05851.pdf}
}

@inproceedings{hedman2021iccv,
  title     = {Baking neural radiance fields for real-time view synthesis},
  author    = {Hedman, Peter and Srinivasan, Pratul P and Mildenhall, Ben and Barron, Jonathan T and Debevec, Paul},
  booktitle = iccv,
  year      = {2021},
  url       = {https://arxiv.org/pdf/2103.14645.pdf}
}

@misc{grupp2017github,
  title        = {{evo: Python package for the evaluation of odometry and SLAM}},
  author       = {Grupp, Michael},
  howpublished = {\url{https://github.com/MichaelGrupp/evo}},
  year         = {2017}
}

@inproceedings{moreau2023wacv,
  title     = {{ImPosIng: Implicit Pose Encoding for Efficient Camera Pose Estimation}},
  author    = {Moreau, Arthur and Gilles, Thomas and Piasco, Nathan and Tsishkou, Dzmitry and Stanciulescu, Bogdan and de La Fortelle, Arnaud},
  booktitle = wacv,
  year      = {2023},
  url       = {https://arxiv.org/pdf/2205.02638.pdf}
}

@inproceedings{moreau2021corl,
  title     = {{LENS: Localization enhanced by NeRF synthesis}},
  author    = {Moreau, Arthur and Piasco, Nathan and Tsishkou, Dzmitry and Stanciulescu, Bogdan and de La Fortelle, Arnaud},
  booktitle = corl,
  year      = {2021},
  url       = {https://arxiv.org/pdf/2110.06558.pdf}
}

@inproceedings{garbin2021iccv,
  title     = {{Fastnerf: High-fidelity neural rendering at 200fps}},
  author    = {Garbin, Stephan J and Kowalski, Marek and Johnson, Matthew and Shotton, Jamie and Valentin, Julien},
  booktitle = iccv,
  year      = {2021},
  url       = {https://arxiv.org/pdf/2103.10380.pdf}
}

@article{zhao2020ral,
  title   = {{2D laser SLAM with general features represented by implicit functions}},
  author  = {Zhao, Jiaheng and Zhao, Liang and Huang, Shoudong and Wang, Yue},
  journal = ral,
  volume  = {5},
  number  = {3},
  pages   = {4329--4336},
  year    = {2020},
  url     = {http://ras.papercept.net/images/temp/IROS/files/2460.pdf}
}

@inproceedings{deng2022cvpr,
  title     = {{Depth-supervised nerf: Fewer views and faster training for free}},
  author    = {Deng, Kangle and Liu, Andrew and Zhu, Jun-Yan and Ramanan, Deva},
  booktitle = cvpr,
  year      = {2022},
  url       = {https://arxiv.org/pdf/2107.02791.pdf}
}

@inproceedings{li2021iccv-mtcd,
  title     = {Mine: Towards continuous depth mpi with nerf for novel view synthesis},
  author    = {Li, Jiaxin and Feng, Zijian and She, Qi and Ding, Henghui and Wang, Changhu and Lee, Gim Hee},
  booktitle = iccv,
  year      = {2021},
  url       = {https://arxiv.org/pdf/2103.14910.pdf}
}

@inproceedings{yuan2018icarcv,
  title     = {Fast gaussian process occupancy maps},
  author    = {Yuan, Yijun and Kuang, Haofei and Schwertfeger, S{\"o}ren},
  booktitle = {Proc.~of the Intl. Conf. on Control, Automation, Robotics and Vision~(ICARCV)},
  year      = {2018},
  url       = {https://arxiv.org/pdf/1811.10156.pdf}
}

@inproceedings{chen2022eccv,
  title     = {{MobileNeRF: Exploiting the Polygon Rasterization Pipeline for Efficient Neural Field Rendering on Mobile Architectures}},
  author    = {Chen, Zhiqin and Funkhouser, Thomas and Hedman, Peter and Tagliasacchi, Andrea},
  booktitle = eccv,
  year      = {2022},
  url       = {https://arxiv.org/pdf/2208.00277.pdf}
}

@inproceedings{meng2021iccv,
  title     = {Gnerf: Gan-based neural radiance field without posed camera},
  author    = {Meng, Quan and Chen, Anpei and Luo, Haimin and Wu, Minye and Su, Hao and Xu, Lan and He, Xuming and Yu, Jingyi},
  booktitle = iccv,
  year      = {2021},
  url       = {https://arxiv.org/pdf/2103.15606.pdf}
}

@inproceedings{pumarola2021cvpr,
  title     = {D-nerf: Neural radiance fields for dynamic scenes},
  author    = {Pumarola, Albert and Corona, Enric and Pons-Moll, Gerard and Moreno-Noguer, Francesc},
  booktitle = cvpr,
  year      = {2021},
  url       = {https://arxiv.org/pdf/2011.13961.pdf}
}

@inproceedings{li2021cvpr,
  title     = {Neural scene flow fields for space-time view synthesis of dynamic scenes},
  author    = {Li, Zhengqi and Niklaus, Simon and Snavely, Noah and Wang, Oliver},
  booktitle = cvpr,
  year      = {2021},
  url       = {https://arxiv.org/pdf/2011.13084.pdf}
}

@inproceedings{ost2021cvpr,
  title     = {Neural scene graphs for dynamic scenes},
  author    = {Ost, Julian and Mannan, Fahim and Thuerey, Nils and Knodt, Julian and Heide, Felix},
  booktitle = cvpr,
  year      = {2021},
  url       = {https://arxiv.org/pdf/2011.10379.pdf}
}

@inproceedings{chang2021icra,
  title     = {{HyperMap: Compressed 3D Map for Monocular Camera Registration}},
  author    = {Chang, Ming-Fang and Mangelson, Joshua and Kaess, Michael and Lucey, Simon},
  booktitle = icra,
  year      = {2021},
  url       = {https://www.cs.cmu.edu/~kaess/pub/Chang21icra.pdf}
}

@article{yin2020tits,
  title   = {{3D LiDAR Map Compression for Efficient Localization on Resource Constrained Vehicles}},
  author  = {Yin, Huan and Wang, Yue and Tang, Li and Ding, Xiaqing and Huang, Shoudong and Xiong, Rong},
  journal = tits,
  volume  = {22},
  number  = {2},
  pages   = {837--852},
  year    = {2020},
  url     = {https://opus.lib.uts.edu.au/bitstream/10453/147656/2/Huan_Yin_IEEE_ITS_Map_Compression_2019_December.pdf}
}

@article{yang2015pami,
  title   = {{Go-ICP: A Globally Optimal Solution to 3D ICP Point-Set Registration}},
  author  = {Yang, Jiaolong and Li, Hongdong and Campbell, Dylan and Jia, Yunde},
  journal = pami,
  year    = {2015},
  volume  = {38},
  number  = {11},
  pages   = {2241--2254},
  url     = {https://arxiv.org/pdf/1605.03344.pdf}
}

@inproceedings{xiao2021neurips,
  title     = {{Early Convolutions help Transformers see Better}},
  author    = {Xiao, Tete and Dollar, Piotr and Singh, Mannat and Mintun, Eric and Darrell, Trevor and Girshick, Ross},
  booktitle = neurips,
  year      = {2021},
  url       = {https://proceedings.neurips.cc/paper/2021/file/ff1418e8cc993fe8abcfe3ce2003e5c5-Paper.pdf}
}

@inproceedings{wang2021cvpr-pwclo,
  title     = {{PWCLO-Net: Deep LiDAR Odometry in 3D Point Clouds Using Hierarchical Embedding Mask Optimization}},
  author    = {Guangming Wang and Xinrui Wu and Zhe Liu and Hesheng Wang},
  booktitle = cvpr,
  year      = {2021},
  url       = {https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_PWCLO-Net_Deep_LiDAR_Odometry_in_3D_Point_Clouds_Using_Hierarchical_CVPR_2021_paper.pdf}
}

@inproceedings{yew2020cvpr,
  title     = {{RPM-Net: Robust Point Matching using Learned Features}},
  author    = {Yew, Zi Jian and Lee, Gim Hee},
  booktitle = cvpr,
  year      = {2020},
  url       = {https://openaccess.thecvf.com/content_CVPR_2020/papers/Yew_RPM-Net_Robust_Point_Matching_Using_Learned_Features_CVPR_2020_paper.pdf}
}

@inproceedings{zeng2016cvpr,
  title     = {{3DMatch: Learning Local Geometric Descriptors from RGB-D Reconstructions}},
  author    = {Zeng, Andy and Song, Shuran and Nie{\ss}ner, Matthias and Fisher, Matthew and Xiao, Jianxiong and Funkhouser, Thomas},
  booktitle = cvpr,
  year      = {2017},
  url       = {https://openaccess.thecvf.com/content_cvpr_2017/papers/Zeng_3DMatch_Learning_Local_CVPR_2017_paper.pdf}
} 

@inproceedings{yew2018eccv,
  author    = {Yew, Zi Jian and Lee, Gim Hee},
  title     = {{3DFeat-Net: Weakly Supervised Local 3D Features for Point Cloud Registration}},
  booktitle = eccv,
  year      = {2018},
  url       = {https://openaccess.thecvf.com/content_ECCV_2018/papers/Zi_Jian_Yew_3DFeat-Net_Weakly_Supervised_ECCV_2018_paper.pdf}
}

@inproceedings{wang2019neurips,
  title     = {{PRNet: Self-Supervised Learning for Partial-to-Partial Registration}},
  author    = {Wang, Yue and Solomon, Justin M.},
  booktitle = neurips,
  year      = {2019},
  url       = {https://proceedings.neurips.cc/paper/2019/file/ebad33b3c9fa1d10327bb55f9e79e2f3-Paper.pdf}
}

@inproceedings{aoki2019cvpr,
  author    = {Aoki, Yasuhiro and Goforth, Hunter and Rangaprasad, Arun Srivatsan and Lucey, Simon},
  year      = {2019},
  booktitle = cvpr,
  title     = {{PointNetLK: Robust \& Efficient Point Cloud Registration Using PointNet}},
  url       = {https://openaccess.thecvf.com/content_CVPR_2019/papers/Aoki_PointNetLK_Robust__Efficient_Point_Cloud_Registration_Using_PointNet_CVPR_2019_paper.pdf}
}

@inproceedings{lu2019iccv,
  author    = {Lu, Weixin and Wan, Guowei and Zhou, Yao and Fu, Xiangyu and Yuan, Pengfei and Song, Shiyu},
  booktitle = iccv,
  title     = {{DeepVCP: An End-to-End Deep Neural Network for Point Cloud Registration}},
  year      = {2019},
  url       = {https://openaccess.thecvf.com/content_ICCV_2019/papers/Lu_DeepVCP_An_End-to-End_Deep_Neural_Network_for_Point_Cloud_Registration_ICCV_2019_paper.pdf}
}

@inproceedings{wang2019iccv,
  title     = {{Deep Closest Point: Learning Representations for Point Cloud Registration}},
  author    = {Wang, Yue and Solomon, Justin M.},
  booktitle = iccv,
  year      = {2019},
  url       = {https://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Deep_Closest_Point_Learning_Representations_for_Point_Cloud_Registration_ICCV_2019_paper.pdf}
}

@article{johnson1999ivc,
  title   = {{Registration and Integration of Textured 3D Data}},
  author  = {Johnson, Andrew Edie and Kang, Sing Bing},
  journal = ivc,
  volume  = {17},
  number  = {2},
  pages   = {135--147},
  year    = {1999},
  url     = {https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=e0b0e198cdafd580714017d5f58063a3f7c9f1d1}
}

@inproceedings{gelfand2005sgp,
  title     = {{Robust Global Registration}},
  author    = {Gelfand, Natasha and Mitra, Niloy J and Guibas, Leonidas J and Pottmann, Helmut},
  booktitle = {Proc.~of the Symp. on Geometry Processing},
  year      = {2005},
  url       = {http://vecg.cs.ucl.ac.uk/Projects/SmartGeometry/global_registration/paper_docs/global_registration_sgp_05.pdf}
}

@article{bouzaiz2013cgf,
  author   = {Bouaziz, Sofien and Tagliasacchi, Andrea and Pauly, Mark},
  title    = {{Sparse Iterative Closest Point}},
  journal  = {Computer Graphics Forum},
  volume   = {32},
  number   = {5},
  pages    = {113--123},
  url      = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.12178},
  abstract = {Abstract Rigid registration of two geometric data sets is essential in many applications, including robot navigation, surface reconstruction, and shape matching. Most commonly, variants of the Iterative Closest Point (ICP) algorithm are employed for this task. These methods alternate between closest point computations to establish correspondences between two data sets, and solving for the optimal transformation that brings these correspondences into alignment. A major difficulty for this approach is the sensitivity to outliers and missing data often observed in 3D scans. Most practical implementations of the ICP algorithm address this issue with a number of heuristics to prune or reweight correspondences. However, these heuristics can be unreliable and difficult to tune, which often requires substantial manual assistance. We propose a new formulation of the ICP algorithm that avoids these difficulties by formulating the registration optimization using sparsity inducing norms. Our new algorithm retains the simple structure of the ICP algorithm, while achieving superior registration results when dealing with outliers and incomplete data. The complete source code of our implementation is provided at http://lgg.epfl.ch/sparseicp.},
  year     = {2013}
}

@inproceedings{lu2021iccv,
  title     = {HRegNet: A Hierarchical Network for Large-scale Outdoor LiDAR Point Cloud Registration},
  author    = {Lu, Fan and Chen, Guang and Liu, Yinlong and Zhang, Lijun and Qu, Sanqing and Liu, Shu and Gu, Rongqi},
  booktitle = iccv,
  year      = {2021},
  url       = {https://openaccess.thecvf.com/content/ICCV2021/papers/Lu_HRegNet_A_Hierarchical_Network_for_Large-Scale_Outdoor_LiDAR_Point_Cloud_ICCV_2021_paper.pdf}
}

@article{sarode2019arxiv,
  title   = {{PCRNet: Point Cloud Registration Network using PointNet Encoding}},
  author  = {Sarode, Vinit and Li, Xueqian and Goforth, Hunter and Aoki, Yasuhiro and Srivatsan, Rangaprasad Arun and Lucey, Simon and Choset, Howie},
  journal = arxiv,
  volume  = {arXiv:1908.07906},
  year    = {2019},
  url     = {https://arxiv.org/pdf/1908.07906.pdf}
}

@article{ba2016arxiv,
  title   = {{Layer Normalization}},
  author  = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
  journal = arxiv,
  volume  = {arXiv:1607.06450},
  year    = {2016},
  url     = {https://arxiv.org/pdf/1607.06450.pdf}
}

@article{kabsch1976acs,
  title   = {{A solution for the best rotation to relate two sets of vectors}},
  author  = {Kabsch, Wolfgang},
  journal = {Acta Crystallographica Section A: Crystal Physics, Diffraction, Theoretical and General Crystallography},
  volume  = {32},
  number  = {5},
  pages   = {922--923},
  year    = {1976}
}

@inproceedings{xiong2020icml,
  title     = {{On Layer Normalization in the Transformer Architecture}},
  author    = {Xiong, Ruibin and Yang, Yunchang and He, Di and Zheng, Kai and Zheng, Shuxin and Xing, Chen and Zhang, Huishuai and Lan, Yanyan and Wang, Liwei and Liu, Tieyan},
  booktitle = icml,
  year      = {2020},
  url       = {http://proceedings.mlr.press/v119/xiong20b/xiong20b.pdf}
}

@inproceedings{sun2022iros,
  author    = {Jiadai Sun and Yuchao Dai and Xianjing Zhang and Jintao Xu and Rui Ai and Weihao Gu and Xieyuanli Chen},
  title     = {{Efficient Spatial-Temporal Information Fusion for LiDAR-Based 3D Moving Object Segmentation}},
  booktitle = iros,
  year      = {2022},
  url       = {https://arxiv.org/pdf/2207.02201.pdf}
}

@inproceedings{kreutz2023wacv,
  author    = {Thomas Kreutz and Max M\"uhlh\"auser and Alejandro Sanchez Guinea},
  title     = {{Unsupervised 4D LiDAR Moving Object Segmentation in Stationary Settings with Multivariate Occupancy Time Series}},
  booktitle = wacv,
  year      = {2023},
  url       = {https://openaccess.thecvf.com/content/WACV2023/papers/Kreutz_Unsupervised_4D_LiDAR_Moving_Object_Segmentation_in_Stationary_Settings_With_WACV_2023_paper.pdf}
}

@inproceedings{huang2022eccv,
  author    = {Shengyu Huang and Zan Gojcic and Jiahui Huang and Andreas Wieser and Konrad Schindler},
  title     = {{Dynamic 3D Scene Analysis by Point Cloud Accumulation}},
  booktitle = eccv,
  year      = {2022},
  url       = {https://arxiv.org/pdf/2207.12394.pdf}
}

@article{kim2022ral,
  author  = {Jaeyeul Kim and Jungwan Woo and Sunghoon},
  title   = {{RVMOS: Range-View Moving Object Segmentation Leveraged by Semantic and Motion Features}},
  journal = ral,
  volume  = {7},
  number  = {3},
  pages   = {8044--8051},
  year    = {2022}
}

@article{arora2023jras,
  author  = {Mehul Arora and Louis Wiesmann and Xieyuanli Chen and Cyrill Stachniss},
  title   = {{Static Map Generation from 3D LiDAR Point Clouds Exploiting Ground Segmentation}},
  journal = jras,
  volume  = {159},
  pages   = {104287},
  year    = {2023},
  url     = {https://www.ipb.uni-bonn.de/wp-content/papercite-data/pdf/arora2023jras.pdf}
}

@article{nuss2018ijrr,
  author  = {Dominik Nuss and Stephan Reuter and Markus Thom and Ting Yuan and Gunther Krehl and Michael Maile and Axel Gern and Klaus Dietmayer},
  title   = {{A Random Finite Set Approach for Dynamic Occupancy Grid Maps with Real-Time Application}},
  journal = ijrr,
  volume  = {37},
  number  = {8},
  pages   = {841--866},
  year    = {2018},
  url     = {https://arxiv.org/pdf/1605.02406.pdf}
}

@inproceedings{wellhausen2017ssrr,
  author    = {Lorenz Wellhausen and Renaud Dub{\'e} and Abel Gawel and Roland Siegwart and Cesar Cadena},
  title     = {{Reliable Real-time Change Detection and Mapping for 3D LiDARs}},
  booktitle = ssrr,
  year      = {2017},
  url       = {https://n.ethz.ch/~cesarc/files/SSRR2017_lwellhausen.pdf}
}

@article{behley2021ijrr,
  author  = {J. Behley and M. Garbade and A. Milioto and J. Quenzel and S. Behnke and J. Gall and C. Stachniss},
  title   = {{Towards 3D LiDAR-based Semantic Scene Understanding of 3D Point Cloud Sequences: The SemanticKITTI Dataset}},
  journal = ijrr,
  volume  = {40},
  number  = {8--9},
  pages   = {959--967},
  year    = {2021},
  url     = {http://www.ipb.uni-bonn.de/pdfs/behley2021ijrr.pdf}
}

@article{chen2021ral,
  title   = {{Moving Object Segmentation in 3D LiDAR Data: A Learning-based Approach Exploiting Sequential Data}},
  author  = {X. Chen and S. Li and B. Mersch and L. Wiesmann and J. Gall and J. Behley and C. Stachniss},
  year    = {2021},
  volume  = {6},
  number  = {4},
  pages   = {6529--6536},
  journal = ral,
  url     = {http://www.ipb.uni-bonn.de/pdfs/chen2021ral-iros.pdf}
}


@article{gehrung2017isprsannals,
  title   = {{An Approach to Extract Moving Objects From MLS Data Using a Volumetric Background Representation}},
  author  = {Gehrung, Joachim and Hebel, Marcus and Arens, Michael and Stilla, Uwe},
  journal = isprsannals,
  volume  = {IV-1/W1},
  year    = {2017},
  pages   = {107--114},
  url     = {https://www.isprs-ann-photogramm-remote-sens-spatial-inf-sci.net/IV-1-W1/107/2017/isprs-annals-IV-1-W1-107-2017.pdf}
}


@article{henein2018arxiv,
  author   = {M. Henein and G. Kennedy and V. Ila and R. Mahony},
  title    = {{Simultaneous Localization and Mapping with Dynamic Rigid Objects}},
  journal  = arxiv,
  volume   = {arXiv:1805.03800},
  year     = 2018,
  url      = {http://arxiv.org/pdf/1805.03800.pdf},
  abstract = {Accurate estimation of the environment structure simultaneously with the robot pose is a key capability of autonomous robotic vehicles. Classical simultaneous localization and mapping (SLAM) algorithms rely on the static world assumption to formulate the estimation problem, however, the real world has a significant amount of dynamics that can be exploited for a more accurate localization and versatile representation of the environment. In this paper we propose a technique to integrate the motion of dynamic objects into the SLAM estimation problem, without the necessity of estimating the pose or the geometry of the objects. To this end, we introduce a novel representation of the pose change of rigid bodies in motion and show the benefits of integrating such information when performing SLAM in dynamic environments. Our experiments show consistent improvement in robot localization and mapping accuracy when using a simple constant motion assumption, even for objects whose motion slightly violates this assumption.}
}


@inproceedings{kuemmerle2013icra,
  author    = {R. K\"ummerle and M. Ruhnke and B. Steder and C. Stachniss and W. Burgard},
  title     = {{A Navigation System for Robots Operating in Crowded Urban Environments}},
  booktitle = icra,
  year      = 2013,
  url       = {http://ais.informatik.uni-freiburg.de/publications/papers/kuemmerle13icra.pdf},
  abstract  = {Over the past years, there has been a tremendous progress in the area of robot navigation. Most of the systems developed thus far, however, are restricted to indoor scenarios, non-urban outdoor environments, or road usage with cars. Urban areas introduce numerous challenges to autonomous mobile robots as they are highly complex and in addition to that dynamic. In this paper, we present a navigation system for pedestrian-like autonomous navigation with mobile robots in city environments. We describe different components including a SLAM system for dealing with huge maps of city centers, a planning approach for inferring feasible paths taking also into account the traversability and type of terrain, and a method for accurate localization in dynamic environments. The navigation system has been implemented and tested in several large-scale field tests in which the robot Obelix managed to autonomously navigate from our university campus over a 3.3 km long route to the city center of Freiburg.}
}

@incollection{museth2013siggraph,
  title     = {{OpenVDB: An Open-source Data Structure and Toolkit for High-resolution Volumes}},
  author    = {Museth, Ken and Lait, Jeff and Johanson, John and Budsberg, Jeff and Henderson, Ron and Alden, Mihai and Cucka, Peter and Hill, David and Pearce, Andrew},
  booktitle = {ACM SIGGRAPH 2013 courses},
  year      = {2013}
}

@inproceedings{stachniss2005aaai,
  title     = {{Mobile Robot Mapping and Localization in Non-Static Environments}},
  author    = {C. Stachniss and W. Burgard},
  booktitle = aaaiold,
  year      = 2005,
  url       = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/stachniss05aaai.pdf}
}

@article{schauer2018ral,
  title   = {{The Peopleremover -- Removing Dynamic Objects From 3-D Point Cloud Data by Traversing a Voxel Occupancy Grid}},
  author  = {Schauer, Johannes and N{\"u}chter, Andreas},
  journal = ral,
  volume  = {3},
  number  = {3},
  pages   = {1679--1686},
  year    = {2018},
  url     = {https://robotik.informatik.uni-wuerzburg.de/telematics/download/icra2018.pdf}
}

@article{vizzo2022sensors,
  author   = {Vizzo, Ignacio and Guadagnino, Tiziano and Behley, Jens and Stachniss, Cyrill},
  title    = {{VDBFusion: Flexible and Efficient TSDF Integration of Range Sensor Data}},
  journal  = sensors,
  url      = {https://www.mdpi.com/1424-8220/22/3/1296/pdf},
  volume   = {22},
  year     = {2022},
  number   = {3},
  pages    = {1296},
  abstract = {Mapping is a crucial task in robotics and a fundamental building block of most mobile systems deployed in the real world. Robots use different environment representations depending on their task and sensor setup. This paper showcases a practical approach to volumetric surface reconstruction based on truncated signed distance functions, also called TSDFs. We revisit the basics of this mapping technique and offer an approach for building effective and efficient real-world mapping systems. In contrast to most state-of-the-art SLAM and mapping approaches, we are making no assumptions on the size of the environment nor the employed range sensor. Unlike most other approaches, we introduce an effective system that works in multiple domains using different sensors. To achieve this, we build upon the Academy-Award-winning OpenVDB library used in filmmaking to realize an effective 3D map representation. Based on this, our proposed system is flexible and highly effective and, in the end, capable of integrating point clouds from a 64-beam LiDAR sensor at 20 frames per second using a single-core CPU. Along with this publication comes an easy-to-use C++ and Python library to quickly and efficiently solve volumetric mapping problems with TSDFs.}
}

@inproceedings{wurm2010icraws,
  author    = {K.M. Wurm and A. Hornung and M. Bennewitz and C. Stachniss and W. Burgard},
  title     = {{OctoMap: A Probabilistic, Flexible, and Compact 3D Map Representation for Robotic Systems}},
  booktitle = {Workshop on Best Practice in 3D Perception and Modeling for Mobile Manipulation, IEEE Int. Conf. on Robotics \& Automation (ICRA)},
  year      = 2010,
  url       = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/wurm10icraws.pdf}
}

@inproceedings{yoon2019crv,
  title     = {{Mapless Online Detection of Dynamic Objects in 3D Lidar}},
  author    = {Yoon, David and Tang, Tim and Barfoot, Timothy},
  booktitle = crv,
  year      = {2019},
  url       = {https://arxiv.org/pdf/1809.06972.pdf}
}

@inproceedings{biber2005rss,
  title     = {{Dynamic Maps for Long-Term Operation of Mobile Service Robots}},
  author    = {P. Biber and T. Duckett},
  booktitle = rss,
  year      = 2005,
  url       = {http://www.roboticsproceedings.org/rss01/p03.pdf}
}

@article{hornung2013ar,
  author  = {A. Hornung and K.M. Wurm and M. Bennewitz and C. Stachniss and W. Burgard},
  title   = {{OctoMap: An Efficient Probabilistic 3D Mapping Framework Based on Octrees}},
  journal = ar,
  volume  = 34,
  number  = 3,
  pages   = {189--206},
  year    = 2013,
  url     = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/hornung13auro.pdf}
}

@inproceedings{milioto2019iros,
  author    = {A. Milioto and I. Vizzo and J. Behley and C. Stachniss},
  title     = {{RangeNet++: Fast and Accurate LiDAR Semantic Segmentation}},
  booktitle = iros,
  year      = 2019,
  codeurl   = {https://github.com/PRBonn/lidar-bonnetal},
  videourl  = {https://youtu.be/wuokg7MFZyU},
  url       = {https://www.ipb.uni-bonn.de/wp-content/papercite-data/pdf/milioto2019iros.pdf}
}

@article{vizzo2023ral,
  author  = {Vizzo, Ignacio and Guadagnino, Tiziano and Mersch, Benedikt and Wiesmann, Louis and Behley, Jens and Stachniss, Cyrill},
  title   = {{KISS-ICP: In Defense of Point-to-Point ICP -- Simple, Accurate, and Robust Registration If Done the Right Way}},
  journal = ral,
  pages   = {1029--1036},
  volume  = {8},
  number  = {2},
  year    = {2023},
  codeurl = {https://github.com/PRBonn/kiss-icp},
  url     = {https://arxiv.org/pdf/2209.15397.pdf}
}

@inproceedings{wang2019icra-rsdsm,
  author    = {K. Wang and F. Gao and S. Shen},
  title     = {{Real-Time Scalable Dense Surfel Mapping}},
  booktitle = icra,
  year      = 2019,
  keywords  = {Mapping, Sensor Fusion, Aerial Systems: Perception and Autonomy},
  abstract  = {In this paper, we propose a novel dense surfel mapping system that scales well in different environments with only CPU computation. Using a sparse SLAM system to estimate camera poses, the proposed mapping system can fuse intensity images and depth images into a globally consistent model. The system is carefully designed so that it can build from room-scale environments to urban-scale environments using depth images from RGB-D cameras, stereo cameras or even a monocular camera. First, superpixels extracted from both intensity and depth images are used to model surfels in the system. superpixel-based surfels make our method both runtime efficient and memory efficient. Second, surfels are further organized according to the pose graph of the SLAM system to achieve O(1) fusion time regardless of the scale of reconstructed models. Third, a fast map deformation using the optimized pose graph enables the map to achieve global consistency in real-time. The proposed surfel mapping system is compared with other state-of-the-art methods on synthetic datasets. The performances of urban-scale and room-scale reconstruction are demonstrated using the KITTI dataset [1] and autonomous aggressive flights, respectively. The code is available for the benefit of the community.},
  url       = {https://arxiv.org/pdf/1909.04250.pdf}
}
@inproceedings{stoyanov2013iros,
  author    = {T. Stoyanov and J.P. Saarinen and H. Andreasson and A.J. Lilienthal},
  title     = {{Normal Distributions Transform Occupancy Map Fusion: Simultaneous Mapping and Tracking in Large Scale Dynamic Environments}},
  booktitle = iros,
  year      = 2013,
  keywords  = {Mapping, SLAM},
  abstract  = {Autonomous vehicles operating in real-world industrial environments have to overcome numerous challenges, chief among which are the creation of consistent 3D world models and the simultaneous tracking of the vehicle pose with respect to the created maps. In this paper we integrate two recently proposed algorithms in an online, near-realtime mapping and tracking system. Using the Normal Distributions Transform (NDT), a sparse Gaussian Mixture Model, for representation of 3D range scan data, we propose a frame-to-model registration and data fusion algorithm NDT Fusion. The proposed approach uses a submap indexing system to achieve operation in arbitrarily-sized environments. The approach is evaluated on a publicly available city-block sized data set, achieving accuracy and runtime performance significantly better than current state of the art. In addition, the system is evaluated on a data set covering ten hours of operation and a trajectory of 7.2km in a real-world industrial environment, achieving centimeter accuracy at update rates of 5-10 Hz.},
  url       = {http://vigir.missouri.edu/~gdesouza/Research/Conference_CDs/IEEE_IROS_2013/media/files/0778.pdf}
}

@inproceedings{behley2015icra,
  title     = {{Efficient Radius Neighbor Search in Three-dimensional Point Clouds}},
  author    = {Behley, Jens and Steinhage, Volker and Cremers, Armin B},
  booktitle = icra,
  year      = {2015},
  url       = {https://jbehley.github.io/papers/behley2015icra.pdf}
}

@article{rosenfeld1966jacm,
  title   = {{Sequential operations in digital picture processing}},
  author  = {Rosenfeld, Azriel and Pfaltz, John L},
  journal = jacm,
  volume  = {13},
  number  = {4},
  pages   = {471--494},
  year    = {1966},
  url     = {https://www.cs.virginia.edu/~jlp/66.sequential.op.pdf}
}

@inproceedings{williams2021cvpr,
  title     = {{Neural Splines: Fitting 3D Surfaces with Infinitely-Wide Neural Networks}},
  author    = {Williams, Francis and Trager, Matthew and Bruna, Joan and Zorin, Denis},
  booktitle = cvpr,
  year      = {2021},
  url       = {https://openaccess.thecvf.com/content/CVPR2021/papers/Williams_Neural_Splines_Fitting_3D_Surfaces_With_Infinitely-Wide_Neural_Networks_CVPR_2021_paper.pdf}
}

@inproceedings{ueda2022eccv,
  title     = {{Neural Density-Distance Fields}},
  author    = {Ueda, Itsuki and Fukuhara, Yoshihiro and Kataoka, Hirokatsu and Aizawa, Hiroaki and Shishido, Hidehiko and Kitahara, Itaru},
  booktitle = eccv,
  year      = {2022},
  url       = {https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136920053.pdf}
}

@inproceedings{zhong2023icra,
  author    = {Zhong, Xingguang and Pan, Yue and Behley, Jens and Stachniss, Cyrill},
  title     = {{SHINE-Mapping: Large-Scale 3D Mapping Using Sparse Hierarchical Implicit Neural Representations}},
  booktitle = icra,
  year      = 2023,
  codeurl   = {https://github.com/PRBonn/SHINE_mapping},
  url       = {https://www.ipb.uni-bonn.de/wp-content/papercite-data/pdf/zhong2023icra.pdf}
}

@inproceedings{azinovic2022cvpr,
  author    = {Azinovi\'c, Dejan and Martin-Brualla, Ricardo and Goldman, Dan B and Nie{\ss}ner, Matthias and Thies, Justus},
  title     = {{Neural RGB-D Surface Reconstruction}},
  booktitle = cvpr,
  year      = {2022},
  url       = {https://openaccess.thecvf.com/content/CVPR2022/papers/Azinovic_Neural_RGB-D_Surface_Reconstruction_CVPR_2022_paper.pdf}
}

@inproceedings{chibane2020nips,
  title     = {{Neural Unsigned Distance Fields for Implicit Function Learning}},
  author    = {Chibane, Julian and Mir, Aymen and Pons-Moll, Gerard},
  booktitle = neurips,
  year      = {2020},
  url       = {https://proceedings.neurips.cc/paper/2020/file/f69e505b08403ad2298b9f262659929a-Paper.pdf}
}

@article{kuang2023ral,
  author  = {Kuang, Haofei and Chen, Xieyuanli and Guadagnino, Tiziano and Zimmerman, Nicky and Behley, Jens and Stachniss, Cyrill},
  title   = {{IR-MCL: Implicit Representation-Based Online Global Localization}},
  journal = ral,
  volume  = {8},
  number  = {3},
  pages   = {1627--1634},
  year    = {2023},
  codeurl = {https://github.com/PRBonn/ir-mcl},
  url     = {https://www.ipb.uni-bonn.de/wp-content/papercite-data/pdf/kuang2023ral.pdf}
}

@inproceedings{reiser2021iccv,
  author    = {C. Reiser and S. Peng and Y. Liao and A. Geiger},
  title     = {{KiloNeRF: Speeding Up Neural Radiance Fields With Thousands of Tiny MLPs}},
  booktitle = iccv,
  year      = 2021,
  abstract  = {NeRF synthesizes novel views of a scene with unprecedented quality by fitting a neural radiance field to RGB images. However, NeRF requires querying a deep Multi-Layer Perceptron (MLP) millions of times, leading to slow rendering times, even on modern GPUs. In this paper, we demonstrate that real-time rendering is possible by utilizing thousands of tiny MLPs instead of one single large MLP. In our setting, each individual MLP only needs to represent parts of the scene, thus smaller and faster-to-evaluate MLPs can be used. By combining this divide-and-conquer strategy with further optimizations, rendering is accelerated by three orders of magnitude compared to the original NeRF model without incurring high storage costs. Further, using teacher-student distillation for training, we show that this speed-up can be achieved without sacrificing visual quality.},
  url       = {proceedings: reiser2021iccv.pdf}
}

@inproceedings{itzhaky2018bmvc,
  title     = {{Leaf Counting: Multiple Scale Regression and Detection Using Deep CNNs.}},
  author    = {Yotam Itzhaky and Guy Farjon and Faina Khoroshevsky and Alon Shpigler and Aharon Bar-Hillel},
  booktitle = bmvc,
  year      = {2018},
  url       = {http://bmvc2018.org/contents/workshops/cvppp2018/0031.pdf}
}


@inproceedings{sitzmann2020neurips,
  title     = {{Implicit Neural Representations with Periodic Activation Functions}},
  author    = {Sitzmann, Vincent and Martel, Julien and Bergman, Alexander and Lindell, David and Wetzstein, Gordon},
  booktitle = neurips,
  year      = {2020}
}

@article{gropp2020arxiv,
  title   = {{Implicit Geometric Regularization for Learning Shapes}},
  author  = {Gropp, Amos and Yariv, Lior and Haim, Niv and Atzmon, Matan and Lipman, Yaron},
  journal = arxiv,
  volume  = {arXiv:2002.10099},
  year    = {2020}
}

@inproceedings{zimmerman2022iros,
  title     = {{Robust Onboard Localization in Changing Environments Exploiting Text Spotting}},
  author    = {N. Zimmerman and L. Wiesmann and T. Guadagnino and T. LÃ¤be and J. Behley and C. Stachniss},
  booktitle = iros,
  year      = {2022},
  codeurl   = {https://github.com/PRBonn/tmcl},
  url       = {https://www.ipb.uni-bonn.de/wp-content/papercite-data/pdf/zimmerman2022iros.pdf}
}

@misc{grisetti2018github,
  author       = {G. Grisetti},
  title        = {srrg-localizer2d (1.6.0)},
  howpublished = {\url{https://gitlab.com/srrg-software/srrg_localizer2d}},
  year         = {2018}
}

@article{zimmerman2023ral,
  author  = {N. Zimmerman and T. Guadagnino and X. Chen and J. Behley and C. Stachniss},
  title   = {{Long-Term Localization using Semantic Cues in Floor Plan Maps}},
  journal = ral,
  year    = {2023},
  volume  = {8},
  number  = {1},
  pages   = {176--183},
  codeurl = {https://github.com/PRBonn/hsmcl},
  url     = {https://www.ipb.uni-bonn.de/wp-content/papercite-data/pdf/zimmerman2023ral.pdf}
}

@inproceedings{boniardi2017iros,
  title     = {{Robust LiDAR-based localization in architectural floor plans}},
  author    = {Boniardi, Federico and Caselitz, Tim and K{\"u}mmerle, Rainer and Burgard, Wolfram},
  booktitle = iros,
  year      = {2017},
  url       = {http://ais.informatik.uni-freiburg.de/publications/papers/boniardi17iros.pdf}
}

@inproceedings{boniardi2019iros,
  title     = {{Robot localization in floor plans using a room layout edge extraction network}},
  author    = {Boniardi, Federico and Valada, Abhinav and Mohan, Rohit and Caselitz, Tim and Burgard, Wolfram},
  booktitle = iros,
  year      = {2019},
  url       = {http://ais.informatik.uni-freiburg.de/publications/papers/boniardi19iros.pdf}
}

@inproceedings{geman1985asa,
  title     = {{Bayesian image analysis: An application to single photon emission tomography}},
  author    = {Geman, Stuart and McClure, D},
  booktitle = {Proc. of the American Statistical Association},
  year      = {1985},
  url       = {https://www.dam.brown.edu/people/geman/Homepage/Image%20processing,%20image%20analysis,%20Markov%20random%20fields,%20and%20MCMC/1985GemanMcClureASA.pdf}
}

@article{wiesmann2022ral-iros,
  author   = {L. Wiesmann and T. Guadagnino and I. Vizzo and G. Grisetti and J. Behley and C. Stachniss},
  title    = {{DCPCR: Deep Compressed Point Cloud Registration in Large-Scale Outdoor Environments}},
  journal  = ral,
  year     = 2022,
  volume   = {7},
  number   = {3},
  pages    = {6327--6334},
  codeurl  = {https://github.com/PRBonn/DCPCR},
  videourl = {https://youtu.be/RqLr2RTGy1s}
}

@article{maggio2022arxiv,
  title   = {{Loc-NeRF: Monte Carlo Localization using Neural Radiance Fields}},
  author  = {Maggio, Dominic and Abate, Marcus and Shi, Jingnan and Mario, Courtney and Carlone, Luca},
  journal = arxiv,
  volume  = {arXiv:2209.09050},
  year    = {2022}
}


@inproceedings{fox2001neurips,
  title     = {{KLD-sampling: Adaptive particle filters}},
  author    = {Fox, Dieter},
  booktitle = neurips,
  year      = {2001},
  url       = {https://proceedings.neurips.cc/paper/2001/file/c5b2cebf15b205503560c4e8e6d1ea78-Paper.pdf}
}



@article{chen2022ral,
  author  = {X. Chen and B. Mersch and L. Nunes and R. Marcuzzi and I. Vizzo and J. Behley and C. Stachniss},
  title   = {{Automatic Labeling to Generate Training Data for Online LiDAR-Based Moving Object Segmentation}},
  journal = ral,
  year    = 2022,
  volume  = {7},
  number  = {3},
  pages   = {6107--6114},
  url     = {http://arxiv.org/pdf/2201.04501}
}

@inproceedings{mersch2021iros,
  author    = {B. Mersch and T. H\"ollen and K. Zhao and C. Stachniss and R. Roscher},
  title     = {{Maneuver-based Trajectory Prediction for Self-driving Cars Using Spatio-temporal Convolutional Networks}},
  booktitle = iros,
  year      = {2021},
  url       = {http://www.ipb.uni-bonn.de/pdfs/mersch2021iros.pdf}
}

@inproceedings{mersch2021corl,
  author    = {B. Mersch and X. Chen and J. Behley and C. Stachniss},
  title     = {{Self-supervised Point Cloud Prediction Using 3D Spatio-temporal Convolutional Networks}},
  booktitle = corl,
  year      = {2021},
  url       = {http://www.ipb.uni-bonn.de/pdfs/mersch2021corl.pdf}
}

@article{mersch2022ral,
  author  = {B. Mersch and X. Chen and I. Vizzo and L. Nunes and J. Behley and C. Stachniss},
  title   = {{Receding Moving Object Segmentation in 3D LiDAR Data Using Sparse 4D Convolutions}},
  journal = ral,
  year    = {2022},
  url     = {https://www.ipb.uni-bonn.de/wp-content/papercite-data/pdf/mersch2022ral.pdf},
  volume  = {7},
  number  = {3},
  pages   = {7503--7510}
}

@article{andersson2019mpc,
  title   = {{CasADi}: a software framework for nonlinear optimization and optimal control},
  author  = {Andersson, Joel A. E. and Gillis, Joris and Horn, Greg and Rawlings, James B. and Diehl, Moritz},
  journal = mpc,
  volume  = {11},
  number  = {1},
  pages   = {1--36},
  year    = {2019},
  url     = {http://mpc.zib.de/archive/2019/1/Andersson2019_Article_CasADiASoftwareFrameworkForNon.pdf}
}

@inproceedings{lofberg2004icra,
  author    = {L{\"{o}}fberg, Johan},
  booktitle = icra,
  title     = {{YALMIP : A Toolbox for Modeling and Optimization in MATLAB}},
  year      = {2004},
  url       = {http://s3.amazonaws.com/researchcompendia_prod/articles/d9ae105043e693a4bbcece63173e425d-2014-02-21-05-26-08/01393890.pdf}
}

@article{gill2005sirev,
  author  = {Gill, Philip E. and Murray, Walter and Saunders, Michael A.},
  title   = {{SNOPT}: An {SQP} algorithm for large-scale constrained optimization},
  journal = sirev,
  pages   = {99--131},
  volume  = {47},
  year    = {2005},
  url     = {https://ccom.ucsd.edu/~optimizers/static/pdfs/siam_44609.pdf}
}

@inbook{byrd2006lsno,
  title     = {{Knitro: An integrated package for nonlinear optimization}},
  author    = {Byrd, Richard H. and Nocedal, Jorge and Waltz, Richard A.},
  booktitle = lsno,
  publisher = {Springer},
  pages     = {35--59},
  year      = {2006}
}

@article{nubert2020arxiv,
  author   = {Julian Nubert and Shehryar Khattak and Marco Hutter},
  title    = {{Self-supervised  Learning  of  LiDAR  Odometry  for  Robotic  Applications}},
  journal  = arxiv,
  volume   = {arXiv:2011.05418},
  year     = 2020,
  abstract = {Reliable robot pose estimation is a key building block of many robot autonomy pipelines,
              with LiDAR localization being an active research domain. In this work, aversatile self-supervised
              LiDAR odometry estimation methodis presented, in order to enable the efficient utilization of
              all available LiDAR data while maintaining real-time performance.The proposed approach  selectively
              applies geometric losses during training, being cognizant of the amount of information that can be extracted
              from  scan points. In addition, no labeled or ground-truth data is required, hence making the presented approach
              suitable for pose estimation in applications where accurate ground-truth is difficult to obtain.
              Furthermore, thepresented network architecture is applicable to a wide rangeof environments and
              sensor modalities without requiring anynetwork or loss function adjustments. The proposed approach is thoroughly
              tested for both indoor and outdoor real-world appli-cations through a variety of experiments using legged, tracked
              and wheeled robots, demonstrating the suitability of learning-based LiDAR odometry for complex robotic applications.},
  url      = {https://arxiv.org/pdf/2011.05418.pdf},
  keywords = {LiDAR, SLAM, Odometry, }
}

@inproceedings{nubert2021icra,
  author    = {Julian Nubert and Shehryar Khattak and Marco Hutter},
  title     = {{Self-supervised  Learning  of  LiDAR  Odometry  for  Robotic  Applications}},
  booktitle = icra,
  year      = 2021,
  abstract  = {Reliable robot pose estimation is a key building block of many robot autonomy pipelines,
               with LiDAR localization being an active research domain. In this work, aversatile self-supervised
               LiDAR odometry estimation methodis presented, in order to enable the efficient utilization of
               all available LiDAR data while maintaining real-time performance.The proposed approach  selectively
               applies geometric losses during training, being cognizant of the amount of information that can be extracted
               from  scan points. In addition, no labeled or ground-truth data is required, hence making the presented approach
               suitable for pose estimation in applications where accurate ground-truth is difficult to obtain.
               Furthermore, thepresented network architecture is applicable to a wide rangeof environments and
               sensor modalities without requiring anynetwork or loss function adjustments. The proposed approach is thoroughly
               tested for both indoor and outdoor real-world appli-cations through a variety of experiments using legged, tracked
               and wheeled robots, demonstrating the suitability of learning-based LiDAR odometry for complex robotic applications.},
  url       = {https://arxiv.org/pdf/2011.05418.pdf},
  keywords  = {LiDAR, SLAM, Odometry, }
}

@article{shen2020arxiv,
  author   = {X. Shen and I. Batkovic and V. Govindarajan and P. Falcone and T. Darrell and F. Borrelli},
  title    = {{ParkPredict: Motion and Intent Prediction of Vehicles in Parking Lots}},
  journal  = arxiv,
  year     = 2020,
  volume   = {arXiv:2004.10293},
  url      = {https://arxiv.org/pdf/2004.10293v1.pdf},
  abstract = {We investigate the problem of predicting driver behavior in parking lots, an environment which is less structured than typical road networks and features complex, interactive maneuvers in a compact space. Using the CARLA simulator, we develop a parking lot environment and collect a dataset of human parking maneuvers. We then study the impact of model complexity and feature information by comparing a multi-modal Long Short-Term Memory (LSTM) prediction model and a Convolution Neural Network LSTM (CNN-LSTM) to a physics-based Extended Kalman Filter (EKF) baseline. Our results show that 1) intent can be estimated well (roughly 85\% top-1 accuracy and nearly 100\% top-3 accuracy with the LSTM and CNN-LSTM model); 2) knowledge of the human driver's intended parking spot has a major impact on predicting parking trajectory; and 3) the semantic representation of the environment improves long term predictions.}
}

@article{burnett2020arxiv,
  author   = {K. Burnett and J. Qian and X. Du and L. Liu and D.J. Yoon and T. Shen and S. Sun and S. Samavi and M.J. Sorocky and M. Bianchi and K. Zhang and A. Arkhangorodsky and Q. Sykora and S. Lu and Y. Huang and A.P. Schoellig and T.D. Barfoot},
  title    = {{Zeus: A System Description of the Two-Time Winner of the Collegiate SAE AutoDrive Competition}},
  journal  = arxiv,
  year     = 2020,
  volume   = {arXiv:2004.08752},
  url      = {http://arxiv.org/pdf/2004.08752v1},
  abstract = {The SAE AutoDrive Challenge is a three-year collegiate competition to develop a self-driving car by 2020. The second year of the competition was held in June 2019 at MCity, a mock town built for self-driving car testing at the University of Michigan. Teams were required to autonomously navigate a series of intersections while handling pedestrians, traffic lights, and traffic signs. Zeus is aUToronto's winning entry in the AutoDrive Challenge. This article describes the system design and development of Zeus as well as many of the lessons learned along the way. This includes details on the team's organizational structure, sensor suite, software components, and performance at the Year 2 competition. With a team of mostly undergraduates and minimal resources, aUToronto has made progress towards a functioning self-driving vehicle, in just two years. This article may prove valuable to researchers looking to develop their own self-driving platform.}
}

@misc{stachnis2020prof,
  author   = {C. Stachniss},
  title    = {{A Baseline of Achievements for Becoming a Professor within the German Academic System}},
  year     = {2020},
  url      = {http://www.ipb.uni-bonn.de/pdfs/stachniss2020prof.pdf},
  abstract = {What is needed to become a professor? This document summarizes what is often regarded as a minimum baseline in terms of achievement within selection committees at German universities. My goal is to give a brief guideline for early career researchers on their way towards becoming a faculty member.}
}

@article{vougioukas2019annurevcontrol,
  author   = {Vougioukas, S.G.},
  title    = {{Agricultural Robotics}},
  journal  = annurevcontrol,
  volume   = {2},
  number   = {1},
  pages    = {365--392},
  year     = {2019},
  abstract = { A key goal of contemporary agriculture is to dramatically increase production of food, feed, fiber, and biofuel products in a sustainable fashion, facing the pressure of diminishing farm labor supply. Agricultural robots can accelerate plant breeding and advance data-driven precision farming with significantly reduced labor inputs by providing task-appropriate sensing and actuation at fine spatiotemporal resolutions. This article highlights the distinctive challenges imposed on ground robots by agricultural environments, which are characterized by wide variations in environmental conditions, diversity and complexity of plant canopy structures, and intraspecies biological variation of physical and chemical characteristics and responses to the environment. Existing approaches to address these challenges are presented, along with their limitations; possible future directions are also discussed. Two key observations are that biology (breeding) and horticultural practices can reduce variabilities at the source and that publicly available benchmark data sets are needed to increase perception robustness and performance despite variability. }
}

@article{radi2019arxiv,
  author   = {H. Radi and W. Ali},
  title    = {{VolMap: A Real-time Model for Semantic Segmentation of a LiDAR surrounding view}},
  journal  = arxiv,
  year     = 2019,
  volume   = {arXiv:1906.11873},
  url      = {http://arxiv.org/pdf/1906.11873v1},
  abstract = {This paper introduces VolMap, a real-time approach for the semantic segmentation of a 3D LiDAR surrounding view system in autonomous vehicles. We designed an optimized deep convolution neural network that can accurately segment the point cloud produced by a 360\degree{} LiDAR setup, where the input consists of a volumetric bird-eye view with LiDAR height layers used as input channels. We further investigated the usage of multi-LiDAR setup and its effect on the performance of the semantic segmentation task. Our evaluations are carried out on a large scale 3D object detection benchmark containing a LiDAR cocoon setup, along with KITTI dataset, where the per-point segmentation labels are derived from 3D bounding boxes. We show that VolMap achieved an excellent balance between high accuracy and real-time running on CPU.}
}

@article{yu2018arxiv,
  author   = {F. Yu and W. Xian and Y. Chen and F. Liu and M. Liao and V. Madhavan and T. Darrell},
  title    = {{BDD100K: A Diverse Driving Video Database with Scalable Annotation Tooling}},
  journal  = arxiv,
  year     = 2018,
  volume   = {arXiv:1805.04687},
  url      = {http://arxiv.org/pdf/1805.04687v1},
  abstract = {Datasets drive vision progress and autonomous driving is a critical vision application, yet existing driving datasets are impoverished in terms of visual content. Driving imagery is becoming plentiful, but annotation is slow and expensive, as annotation tools have not kept pace with the flood of data. Our first contribution is the design and implementation of a scalable annotation system that can provide a comprehensive set of image labels for large-scale driving datasets. Our second contribution is a new driving dataset, facilitated by our tooling, which is an order of magnitude larger than previous efforts, and is comprised of over 100K videos with diverse kinds of annotations including image level tagging, object bounding boxes, drivable areas, lane markings, and full-frame instance segmentation. The dataset possesses geographic, environmental, and weather diversity, which is useful for training models so that they are less likely to be surprised by new conditions. The dataset can be requested at http://bdd-data.berkeley.edu.}
}

@article{shamshiri2018ijabe,
  author   = {R.R. Shamshiri and C. Weltzien and I.A. Hameed and I.J. Yule and T.E. Grift and S.K. Balasundram and L. Pitonakova and D. Ahmad and G. Chowdhary},
  title    = {{Research and development in agricultural robotics: A perspective of digital farming}},
  journal  = ijabe,
  year     = 2018,
  volume   = {11},
  number   = {4},
  pages    = {1--14},
  abstract = {Digital farming is the practice of modern technologies such as sensors, robotics, and data analysis for shifting from tedious operations to continuously automated processes. This paper reviews some of the latest achievements in agricultural robotics, specifically those that are used for autonomous weed control, field scouting, and harvesting. Object identification, task planning algorithms, digitalization and optimization of sensors are highlighted as some of the facing challenges in the context of digital farming. The concepts of multi-robots, human-robot collaboration, and environment reconstruction from aerial images and ground-based sensors for the creation of virtual farms were highlighted as some of the gateways of digital farming. It was shown that one of the trends and research focuses in agricultural field robotics is towards building a swarm of small scale robots and drones that collaborate together to optimize farming inputs and reveal denied or concealed information. For the case of robotic harvesting, an autonomous framework with several simple axis manipulators can be faster and more efficient than the currently adapted professional expensive manipulators. While robots are becoming the inseparable parts of the modern farms, our conclusion is that it is not realistic to expect an entirely automated farming system in the future.},
  url      = {https://ntnuopen.ntnu.no/ntnu-xmlui/bitstream/handle/11250/2595468/4278-13703-4-PB.pdf?sequence=1&isAllowed=y}
}

@article{caesar2019arxiv,
  author   = {H. Caesar and V. Bankiti and A.H. Lang and S. Vora and V.E. Liong and Q. Xu and A. Krishnan and Y. Pan and G. Baldan and O. Beijbom},
  title    = {{nuScenes: A multimodal dataset for autonomous driving}},
  journal  = arxiv,
  year     = 2019,
  volume   = {arXiv:1903.11027},
  url      = {http://arxiv.org/pdf/1903.11027},
  abstract = {Robust detection and tracking of objects is crucial for the deployment of autonomous vehicle technology. Image-based benchmark datasets have driven the development in computer vision tasks such as object detection, tracking and segmentation of agents in the environment. Most autonomous vehicles, however, carry a combination of cameras and range sensors such as lidar and radar. As machine learning based methods for detection and tracking become more prevalent, there is a need to train and evaluate such methods on datasets containing range sensor data along with images. In this work we present nuTonomy scenes (nuScenes), the first dataset to carry the full autonomous vehicle sensor suite: 6 cameras, 5 radars and 1 lidar, all with full 360 degree field of view. nuScenes comprises 1000 scenes, each 20s long and fully annotated with 3D bounding boxes for 23 classes and 8 attributes. It has 7x as many annotations and 100x as many images as the pioneering KITTI dataset. We also define a new metric for 3D detection which consolidates the multiple aspects of the detection task: classification, localization, size, orientation, velocity and attribute estimation. We provide careful dataset analysis as well as baseline performance for lidar and image based detection methods. Data, development kit, and more information are available at www.nuscenes.org.}
}

@article{deng2019arxiv,
  author   = {X. Deng and A. Mousavian and Y. Xiang and F. Xia and T. Bretl and D. Fox},
  title    = {{PoseRBPF: A Rao-Blackwellized Particle Filter for 6D Object Pose Tracking}},
  journal  = arxiv,
  year     = 2019,
  volume   = {arXiv:1905.09304},
  url      = {http://arxiv.org/pdf/1905.09304},
  abstract = {Tracking 6D poses of objects from videos provides rich information to a robot in performing different tasks such as manipulation and navigation. In this work, we formulate the 6D object pose tracking problem in the Rao-Blackwellized particle filtering framework, where the 3D rotation and the 3D translation of an object are decoupled. This factorization allows our approach, called PoseRBPF, to efficiently estimate the 3D translation of an object along with the full distribution over the 3D rotation. This is achieved by discretizing the rotation space in a fine-grained manner, and training an auto-encoder network to construct a codebook of feature embeddings for the discretized rotations. As a result, PoseRBPF can track objects with arbitrary symmetries while still maintaining adequate posterior distributions. Our approach achieves state-of-the-art results on two 6D pose estimation benchmarks. A video showing the experiments can be found at https://youtu.be/lE5gjzRKWuA}
}

@article{mei2019arxiv,
  author   = {J. Mei and H. Zhao},
  title    = {{Incorporating Human Domain Knowledge in 3D LiDAR-based Semantic Segmentation}},
  journal  = arxiv,
  year     = 2019,
  volume   = {arXiv:1905.09533},
  url      = {http://arxiv.org/pdf/1905.09533},
  abstract = {This work studies semantic segmentation using 3D LiDAR data. Popular deep learning methods applied for this task require a large number of manual annotations to train the parameters. We propose a new method that makes full use of the advantages of traditional methods and deep learning methods via incorporating human domain knowledge into the neural network model to reduce the demand for large numbers of manual annotations and improve the training efficiency. We first pretrain a model with autogenerated samples from a rule-based classifier so that human knowledge can be propagated into the network. Based on the pretrained model, only a small set of annotations is required for further fine-tuning. Quantitative experiments show that the pretrained model achieves better performance than random initialization in almost all cases; furthermore, our method can achieve similar performance with fewer manual annotations.}
}

@article{pfeuffer2019arxiv,
  author   = {A. Pfeuffer and K. Dietmayer},
  title    = {{Robust Semantic Segmentation in Adverse Weather Conditions by means of Sensor Data Fusion}},
  journal  = arxiv,
  year     = 2019,
  volume   = {arXiv:1905.10117},
  url      = {http://arxiv.org/pdf/1905.10117},
  abstract = {A robust and reliable semantic segmentation in adverse weather conditions is very important for autonomous cars, but most state-of-the-art approaches only achieve high accuracy rates in optimal weather conditions. The reason is that they are only optimized for good weather conditions and given noise models. However, most of them fail, if data with unknown disturbances occur, and their performance decrease enormously. One possibility to still obtain reliable results is to observe the environment with different sensor types, such as camera and lidar, and to fuse the sensor data by means of neural networks, since different sensors behave differently in diverse weather conditions. Hence, the sensors can complement each other by means of an appropriate sensor data fusion. Nevertheless, the fusion-based approaches are still susceptible to disturbances and fail to classify disturbed image areas correctly. This problem can be solved by means of a special training method, the so called Robust Learning Method (RLM), a method by which the neural network learns to handle unknown noise. In this work, two different sensor fusion architectures for semantic segmentation are compared and evaluated on several datasets. Furthermore, it is shown that the RLM increases the robustness in adverse weather conditions enormously, and achieve good results although no disturbance model has been learned by the neural network.}
}

@article{chadwick2019arxiv,
  author   = {S. Chadwick and P. Newman},
  title    = {{Training Object Detectors With Noisy Data}},
  journal  = arxiv,
  year     = 2019,
  volume   = {arXiv:1905.07202},
  url      = {http://arxiv.org/pdf/1905.07202v1},
  abstract = {The availability of a large quantity of labelled training data is crucial for the training of modern object detectors. Hand labelling training data is time consuming and expensive while automatic labelling methods inevitably add unwanted noise to the labels. We examine the effect of different types of label noise on the performance of an object detector. We then show how co-teaching, a method developed for handling noisy labels and previously demonstrated on a classification problem, can be improved to mitigate the effects of label noise in an object detection setting. We illustrate our results using simulated noise on the KITTI dataset and on a vehicle detection task using automatically labelled data.}
}

@article{burnett2019arxiv,
  author   = {K. Burnett and S. Samavi and S.L. Waslander and T.D. Barfoot and A.P. Schoellig},
  title    = {{aUToTrack: A Lightweight Object Detection and Tracking System for the SAE AutoDrive Challenge}},
  journal  = arxiv,
  year     = 2019,
  volume   = {arXiv:1905.08758},
  url      = {http://arxiv.org/pdf/1905.08758v1},
  abstract = {The University of Toronto is one of eight teams competing in the SAE AutoDrive Challenge -- a competition to develop a self-driving car by 2020. After placing first at the Year 1 challenge, we are headed to MCity in June 2019 for the second challenge. There, we will interact with pedestrians, cyclists, and cars. For safe operation, it is critical to have an accurate estimate of the position of all objects surrounding the vehicle. The contributions of this work are twofold: First, we present a new object detection and tracking dataset (UofTPed50), which uses GPS to ground truth the position and velocity of a pedestrian. To our knowledge, a dataset of this type for pedestrians has not been shown in the literature before. Second, we present a lightweight object detection and tracking system (aUToTrack) that uses vision, LIDAR, and GPS/IMU positioning to achieve state-of-the-art performance on the KITTI Object Tracking benchmark. We show that aUToTrack accurately estimates the position and velocity of pedestrians, in real-time, using CPUs only. aUToTrack has been tested in closed-loop experiments on a real self-driving car, and we demonstrate its performance on our dataset.}
}



@phdthesis{merfels2018phd,
  author = {C. Merfels},
  title  = {{Sensor fusion for localization of automated vehicles}},
  school = {Rheinische Friedrich-Wilhelms University of Bonn},
  year   = 2018,
  url    = {http://hss.ulb.uni-bonn.de/2018/5276/5276.pdf}
}

@phdthesis{bogoslavskyi2018phd,
  author = {I. Bogoslavskyi},
  title  = {{Robot Mapping and Navigation in Real-World Environments}},
  school = {Rheinische Friedrich-Wilhelms University of Bonn},
  year   = 2018,
  url    = {http://www.ipb.uni-bonn.de/pdfs/bogoslavskyi2018phd.pdf}
}

@article{sa2018rs,
  author   = {I. Sa and M. Popovic and R. Khanna and Z. Chen and P. Lottes and F. Liebisch and J. Nieto and C. Stachniss and R. Siegwart},
  title    = {{WeedMap: A Large-Scale Semantic Weed Mapping Framework Using Aerial Multispectral Imaging and Deep Neural Network for Precision Farming}},
  journal  = rs,
  year     = 2018,
  volume   = {10},
  number   = {9},
  url      = {http://www.mdpi.com/2072-4292/10/9/1423/pdf},
  abstract = {The ability to automatically monitor agricultural fields is an important capability in precision farming, enabling steps towards more sustainable agriculture. Precise, high-resolution monitoring is a key prerequisite for targeted intervention and the selective application of agro-chemicals. The main goal of this paper is developing a novel crop/weed segmentation and mapping framework that processes multispectral images obtained from an unmanned aerial vehicle (UAV) using a deep neural network (DNN).  Most studies on crop/weed semantic segmentation only consider single images for processing and classification. Images taken by UAVs often cover only a few hundred square meters with either color only or color and near-infrared (NIR) channels. Although a map can be generated by processing single segmented images incrementally, this requires additional complex information fusion techniques which struggle to handle high fidelity maps due to their computational costs and problems in ensuring global consistency. Moreover, computing a single large and accurate vegetation map (e.g., crop/weed) using a DNN is non-trivial due to difficulties arising from: (1) limited ground sample distances (GSDs) in high-altitude datasets, (2) sacrificed resolution resulting from downsampling high-fidelity images, and (3) multispectral image alignment. To address these issues, we adopt a stand sliding window approach that operates on only small portions of multispectral orthomosaic maps (tiles), which are channel-wise aligned and calibrated radiometrically across the entire map. We define the tile size to be the same as that of the DNN input to avoid resolution loss. Compared to our baseline model (i.e., SegNet with 3 channel RGB inputs) yielding an area under the curve (AUC) of [background=0.607, crop=0.681, weed=0.576], our proposed model with 9 input channels achieves [0.839, 0.863, 0.782]. Additionally, we provide an extensive analysis of 20 trained models, both qualitatively and quantitatively, in order to evaluate the effects of varying input channels and tunable network hyperparameters. Furthermore, we release a large sugar beet/weed aerial dataset with expertly guided annotations for further research in the fields of remote sensing, precision agriculture, and agricultural robotics.}
}

@article{dodorico2018revgeophysics,
  author   = {P. D'Odorico and K.F. Davis and L. Rosa and J.A. Carr and D. Chiarelli and J. Dell'Angelo and J. Gephart and G.K. MacDonald and D.A. Seekell and S. Suweis and M.C. Rulli},
  title    = {{The Global Food-Energy-Water Nexus}},
  journal  = {Reviews of Geophysics},
  year     = {2018},
  volume   = {56},
  pages    = {456--531},
  abstract = {Water availability is a major factor constraining humanityâs ability to meet the future food and energy needs of a growing and increasingly affluent human population. Water plays an important role in the production of energy, including renewable energy sources and the extraction of unconventional fossil fuels that are expected to become important players in future energy security. The emergent competition for water between the food and energy systems is increasingly recognized in the concept of the âfood-energy-water nexus.â The nexus between food and water is made even more complex by the globalization of agriculture and rapid growth in food trade, which results in a massive virtual transfer of water among regions and plays an important role in the food and water security of some regions. This review explores multiple components of the food-energy-water nexus and highlights possible approaches that could be used to meet food and energy security with the limited renewable water resources of the planet. Despite clear tensions inherent in meeting the growing and changing demand for food and energy in the 21st century, the inherent linkages among food, water, and energy systems can offer an opportunity for synergistic strategies aimed at resilient food, water, and energy security, such as the circular economy.}
}

@inproceedings{forster2014icra,
  author    = {C. Forster and M. Pizzoli and D. Scaramuzza},
  title     = {{SVO: Fast semi-direct monocular visual odometry}},
  booktitle = icra,
  year      = {2014},
  abstract  = {We propose a semi-direct monocular visual odometry algorithm that is precise, robust, and faster than current state-of-the-art methods. The semi-direct approach eliminates the need of costly feature extraction and robust matching techniques for motion estimation. Our algorithm operates directly on pixel intensities, which results in subpixel precision at high frame-rates. A probabilistic mapping method that explicitly models outlier measurements is used to estimate 3D points, which results in fewer outliers and more reliable points. Precise and high frame-rate motion estimation brings increased robustness in scenes of little, repetitive, and high-frequency texture. The algorithm is applied to micro-aerial-vehicle state-estimation in GPS-denied environments and runs at 55 frames per second on the onboard embedded computer and at more than 300 frames per second on a consumer laptop. We call our approach SVO (Semi-direct Visual Odometry) and release our implementation as open-source software.},
  url       = {https://rpg.ifi.uzh.ch/docs/ICRA14_Forster.pdf}
}

@inproceedings{newcombe2011iccv,
  author    = {R.A. Newcombe and S.J. Lovegrove and A.J. Davison},
  title     = {{DTAM: Dense tracking and mapping in real-time}},
  booktitle = iccvold,
  year      = {2011},
  abstract  = {DTAM is a system for real-time camera tracking and reconstruction which relies not on feature extraction but dense, every pixel methods. As a single hand-held RGB camera flies over a static scene, we estimate detailed textured depth maps at selected keyframes to produce a surface patchwork with millions of vertices. We use the hundreds of images available in a video stream to improve the quality of a simple photometric data term, and minimise a global spatially regularised energy functional in a novel non-convex optimisation framework. Interleaved, we track the camera's 6DOF motion precisely by frame-rate whole image alignment against the entire dense model. Our algorithms are highly parallelisable throughout and DTAM achieves real-time performance using current commodity GPU hardware. We demonstrate that a dense model permits superior tracking performance under rapid motion compared to a state of the art method using features; and also show the additional usefulness of the dense model for real-time scene interaction in a physics-enhanced augmented reality application.},
  url       = {https://www.doc.ic.ac.uk/~ajd/Publications/newcombe_etal_iccv2011.pdf}
}


@inproceedings{steinbrucker2011iccv,
  author    = {F. Steinbr{\"u}cker and J. Sturm and D. Cremers},
  title     = {{Real-Time Visual Odometry from Dense RGB-D Images}},
  booktitle = iccvold,
  year      = {2011},
  abstract  = {We present an energy-based approach to visual odometry from RGB-D images of a Microsoft Kinect camera. To this end we propose an energy function which aims at finding the best rigid body motion to map one RGB-D image into another one, assuming a static scene filmed by a moving camera. We then propose a linearization of the energy function which leads to a 6Ã6 normal equation for the twist coordinates representing the rigid body motion. To allow for larger motions, we solve this equation in a coarse-to-fine scheme. Extensive quantitative analysis on recently proposed benchmark datasets shows that the proposed solution is faster than a state-of-the-art implementation of the iterative closest point (ICP) algorithm by two orders of magnitude. While ICP is more robust to large camera motion, the proposed method gives better results in the regime of small displacements which are often the case in camera tracking applications.},
  url       = {https://jsturm.de/publications/data/steinbruecker_sturm_cremers_iccv11.pdf}
}

@inproceedings{weingarten2013icar,
  author    = {J. Weingarten and G. Gruener and R. Siegwart},
  title     = {A fast and robust 3d feature extraction algorithm for structured environment reconstruction},
  booktitle = icar,
  year      = {2003},
  abstract  = {This paper describes an algorithm for generating compact feature-based 3D models of indoor environments with a mobile robot. The emphasis lies on the high performance of the algorithm, its possible incremental use, as well as its wide applicability to a variety of sensors as it does not assume any structure in the raw data at all. It recovers planar surfaces of physical environments based on a set of unorganized points and generates a compact, real-time renderable 3D model.},
  url       = {https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/82554/eth-8218-01.pdf}
}

@article{arun1987pami,
  author  = {K.S. Arun and T.S. Huang and S.D. Blostein},
  title   = {{Least-Squares Fitting of Two 3-D Point Sets}},
  journal = pami,
  year    = {1987},
  volume  = {9},
  number  = {5},
  pages   = {698--700},
  url     = {https://www.math.pku.edu.cn/teachers/yaoy/Fall2011/arun.pdf}
}

@inproceedings{carrera2011icra,
  author    = {G. Carrera and A. Angeli and A.J. Davison},
  title     = {{SLAM-Based Automatic Extrinsic Calibration of a Multi-Camera Rig}},
  booktitle = icra,
  year      = {2011},
  abstract  = {Cameras are often a good choice as the primary outward-looking sensor for mobile robots, and a wide field of view is usually desirable for responsive and accurate navigation, SLAM and relocalisation. While this can potentially be provided by a single omnidirectional camera, it can also be flexibly achieved by multiple cameras with standard optics mounted around the robot. However, such setups are difficult to calibrate. Here we present a general method for fully automatic extrinsic auto-calibration of a fixed multi camera rig, with no requirement for calibration patterns or other infrastructure, which works even in the case where the cameras have completely non-overlapping views. The robot is placed in a natural environment and makes a set of programmed movements including a full horizontal rotation and captures a synchronized image sequence from each camera. These sequences are processed individually with a monocular visual SLAM algorithm. The resulting maps are matched and fused robustly based on corresponding invariant features, and then all estimates are optimised full joint bundle adjustment, where we constrain the relative poses of the cameras to be fixed. We present results showing accurate performance of the method for various two and four camera configurations.},
  keywords  = {Computer Vision for Robotics and Automation, SLAM, Omnidirectional Vision},
  url       = {https://www.doc.ic.ac.uk/~ajd/Publications/carrera_etal_icra2011.pdf}
}

@article{condurache2016mmt,
  author   = {D. Condurache and A. Burlacu},
  title    = {{Orthogonal dual tensor method for solving the AX=XB sensor calibration problem}},
  journal  = {Mechanism and Machine Theory},
  year     = {2016},
  pages    = {382--404},
  volume   = {104},
  abstract = {The AX=XB sensor calibration problem is very important in multiple fields such as robotics, computer vision or machine vision. In this type of problem, A and B are parameterizations of rigid body motions, while X is an unknown transformation. This research proposes a new approach for solving the AX=XB sensor calibration problem by mapping the SE3 classic formulation into an orthogonal dual tensors set SOÌ²3 representation. Using an isomorphism between SE3 and SOÌ²3, together with a set of properties from SOÌ²3, we formulate closed-form simultaneous solutions for data recorded from any number of displacements. These solutions are free of coordinates and can easily be put into practice. Implementation algorithms are presented and numerical results are discussed in order to evaluate the performance of the proposed solutions.}
}

@inproceedings{faugueras1989tcpf,
  author    = {O.D. Faugueras and G. Toscani},
  title     = {{The Calibration Problem for Stereoscopic Vision}},
  booktitle = {Sensor Devices and Systems for Robotics},
  year      = {1989},
  abstract  = {The problem of calibrating a stereo system is extremely important in practical applications. We describe in this paper our approach for coming up with an efficient and accurate solution. We first review the pinhole camera model that is used and analyze its relationship with respect to the internal camera parameters and its position in space. We then study its behavior with respect to changes of coordinate systems.}
}

@inproceedings{fernndez-moral2015icra,
  author    = {E. Fernndez-Moral and V. Arevalo and J. Gonzlez-Jimnez},
  title     = {{Extrinsic Calibration of a Set of 2D Laser Rangefinders}},
  booktitle = icra,
  year      = {2015},
  abstract  = {The integration of several 2D laser rangefinders in a vehicle is a common resource employed for 3D mapping, obstacle detection and navigation. The extrinsic calibration between such sensors (i.e. finding their relative poses) is required to exploit effectively the sensor measurements and to perform data fusion. The approaches found in the literature to obtain such calibration either rely on the approximated parameters from the rig construction or propose ad-hoc solutions for specific LRF rigs. In this paper we present a novel solution for the extrinsic calibration of a set of at least three laser scanners from the information provided by the sensor measurements. This method only requires the lasers to observe a common planar surface from different orientations, thus there is no need of any specific calibration pattern. This calibration technique can be used with almost any geometric sensor configuration (except for sensors scanning parallel planes), and constitutes a versatile solution that is accurate, fast and easy to apply. This approach is validated with both simulated and real data.},
  keywords  = {Calibration and Identification, Range Sensing},
  url       = {http://mapir.isa.uma.es/varevalo/drafts/fernandez2015icra.pdf}
}

@inproceedings{geiger2012icra,
  author    = {A. Geiger and F. Moosmann and m. Car and B. Schuster},
  title     = {{Automatic Camera and Range Sensor Calibration using a single Shot}},
  booktitle = icra,
  year      = {2012},
  abstract  = {As a core robotic and vision problem, camera and range sensor calibration have been researched intensely over the last decades. However, robotic research efforts still often get heavily delayed by the requirement of setting up a calibrated system consisting of multiple cameras and range measurement units. With regard to removing this burden, we present a toolbox with web interface for fully automatic camerato-camera and camera-to-range calibration. Our system is easy to setup and recovers intrinsic and extrinsic camera parameters as well as the transformation between cameras and range sensors within one minute. In contrast to existing calibration approaches, which often require user intervention, the proposed method is robust to varying imaging conditions, fully automatic, and easy to use since a single image and range scan proves sufficient for most calibration scenarios. Experimentally, we demonstrate that the proposed checkerboard corner detector significantly outperforms current state-of-the-art. Furthermore, the proposed camera-to-range registration method is able to discover multiple solutions in the case of ambiguities. Experiments using a variety of sensors such as grayscale and color cameras, the Kinect 3D sensor and the Velodyne HDL-64 laser scanner show the robustness of our method in different indoor and outdoor settings and under various lighting conditions.},
  keywords  = {Calibration and Identification, Sensor Fusion, Range Sensing},
  url       = {https://www.cvlibs.net/publications/Geiger2012ICRA.pdf}
}

@inproceedings{gomez-ojeda2015icra,
  author    = {R. Gomez-Ojeda and J. Briales and E. Fernndez-Moral and J. Gonzlez-Jimnez},
  title     = {{Extrinsic Calibration of a 2D Laser-Rangefinder and a Camera Based on Scene Corners}},
  booktitle = icra,
  year      = {2015},
  abstract  = {Robots are often equipped with 2D laserrangefinders (LRFs) and cameras since they complement well to each other. In order to correctly combine measurements from both sensors, it is required to know their relative pose, that is, to solve their extrinsic calibration. In this paper we present a new approach to such problem which relies on the observations of orthogonal trihedrons which are profusely found as corners in human-made scenarios. Thus, the method does not require any specific pattern, which turns the calibration process fast and simpler to perform. The estimated relative pose has proven to be also very precise since it uses two different types of constraints, line-to-plane and point-to-plane, as a result of a richer configuration than previous proposals that relies on plane or V-shaped patterns. Our approach is validated with synthetic and real experiments, showing better performance than the state-of-art methods.},
  keywords  = {Visual Tracking, Object detection, segmentation, categorization},
  url       = {http://mapir.isa.uma.es/rgomez/publications/icra15extrinsic.pdf}
}

@book{gower2004book,
  author    = {J.C. Gower and G.B. Dijksterhuis and others},
  title     = {Procrustes problems},
  publisher = {Oxford University Press on Demand},
  year      = {2004},
  volume    = {30}
}

@inproceedings{heng2014icra,
  title     = {{Infrastructure-Based Calibration of a Multi-Camera Rig}},
  author    = {L. Heng and M. Brki and G.H. Lee and P.T. Furgale and R. Siegwart and M. Pollefeys},
  booktitle = icra,
  year      = {2014},
  abstract  = {The online recalibration of multi-sensor systems is a fundamental problem that must be solved before complex automated systems are deployed in situations such as automated driving. In such situations, accurate knowledge of calibration parameters is critical for the safe operation of automated systems. However, most existing calibration methods for multisensor systems are computationally expensive, use installations of known fiducial patterns, and require expert supervision. We propose an alternative approach called infrastructure-based calibration that is efficient, requires no modification of the infrastructure, and is completely unsupervised. In a survey phase, a computationally expensive simultaneous localization and mapping (SLAM) method is used to build a highly accurate map of a calibration area. Once the map is built, many other vehicles are able to use it for calibration as if it were a known fiducial pattern. We demonstrate the effectiveness of this method to calibrate the extrinsic parameters of a multi-camera system. The method does not assume that the cameras have an overlapping field of view and it does not require an initial guess. As the camera rig moves through the previously mapped area, we match features between each set of synchronized camera images and the map. Subsequently, we find the camera poses and inlier 2D-3D correspondences. From the camera poses, we obtain an initial estimate of the camera extrinsics and rig poses, and optimize these extrinsics and rig poses via non-linear refinement. The calibration code is publicly available as a standalone C++ package.},
  keywords  = {Calibration and Identification, Computer Vision for Robotics and Automation},
  url       = {https://arxiv.org/pdf/2007.15330.pdf}
}

@inproceedings{li2016icra-shar,
  author    = {H. Li and Q. Ma and T. Wang and G. Chirikjian},
  title     = {{Simultaneous Hand-Eye and Robot-World Calibration by Solving the AX=YB Problem without Correspondence}},
  booktitle = icra,
  year      = {2016},
  abstract  = {Calibration is often an important and necessary step in the use of image-guided systems. In the case of the AX = Y B problem, the relative hand-eye (X) and robotworld (Y ) transformations must be determined to provide accurate data for use in control. As an added difficulty, the exact correspondence between the streams of sensor data (As and Bs) is typically unknown due to asynchrony in sampling rates and processing time. One common scenario is a constant shift between the two data streams. Therefore, in this paper we present a probabilistic method to simultaneously solve for X and Y without a priori knowledge of the correspondence between the streams of As and Bs. We begin by discussing probability density functions on SE(3) and then use Euclideangroup invariants to obtain an exact solution for X and Y . We then present a method to simultaneously recover X and Y and the correspondence between temporally shifted data sets using a correlation method. Following this, we show how to solve the problem in the case when the data are completely scrambled, corresponding to a complete loss of temporal information. Finally, we numerically simulated the proposed method with asynchronous data and noise added to the stream of Bs to verify its efficiency and robustness.},
  keywords  = {Calibration and Identification, Probability and Statistical Methods, Computer Vision for Medical Robotics},
  url       = {https://rpk.lcsr.jhu.edu/wp-content/uploads/2016/08/Li16_Simultaneous-Hand-Eye-and-Robot-World-Calibration.pdf}
}

@article{pluim2003tmi,
  author   = {J.P.W. Pluim and J.B.A. Maintz and M.A. Viergever},
  title    = {Mutual-information-based registration of medical images: a survey},
  journal  = {IEEE Trans.~on Medical Imaging},
  year     = {2003},
  number   = {8},
  pages    = {986--1004},
  volume   = {22},
  abstract = {An overview is presented of the medical image processing literature on mutual-information-based registration. The aim of the survey is threefold: an introduction for those new to the field, an overview for those working in the field, and a reference for those searching for literature on a specific application. Methods are classified according to the different aspects of mutual-information-based registration. The main division is in aspects of the methodology and of the application. The part on methodology describes choices made on facets such as preprocessing of images, gray value interpolation, optimization, adaptations to the mutual information measure, and different types of geometrical transformations. The part on applications is a reference of the literature available on different modalities, on interpatient registration and on different anatomical objects. Comparison studies including mutual information are also considered. The paper starts with a description of entropy and mutual information and it closes with a discussion on past achievements and some future challenges.},
  keywords = {reviews;image registration;medical image processing;entropy;optimisation;interpolation;image matching;mutual-information-based registration;interpatient registration;anatomical objects;past achievements;images preprocessing;future challenges;medical diagnostic imaging;medical image processing literature;Biomedical imaging;Mutual information;Entropy;Image registration;Biomedical image processing;Interpolation;Optimization methods;Springs;Medical services;Biomedical equipment;Algorithms;Anatomy, Cross-Sectional;Humans;Image Enhancement;Image Interpretation, Computer-Assisted;Imaging, Three-Dimensional;Pattern Recognition, Automated;Subtraction Technique}
}

@article{rodriguez1840jmpa,
  author  = {O. Rodriguez},
  title   = {Des lois gÃ©ometriques qui regissent les deplacements d'un systÃ¨me solide independament des causes qui peuvent les produire},
  journal = {Journal de mathÃ©matiques pures et appliquÃ©es},
  year    = {1840},
  pages   = {380--440},
  volume  = {5},
  url     = {https://arxiv.org/pdf/2211.07787.pdf}
}

@inproceedings{rwekmper2015icra,
  author    = {J. R{\"}owek{\"a}mper and M. Ruhnke and B. Steder and W. Burgard and G.D. Tipaldi},
  title     = {{Automatic Extrinsic Calibration of Multiple Laser Range Sensors with Little Overlap}},
  booktitle = icra,
  year      = {2015},
  abstract  = {Networks of laser range finders are a popular tool for monitoring large cluttered areas and to track people. Whenever multiple scanners are used for this purpose, one major problem is how to determine the relative positions of all the scanners. In this paper, we present a novel approach to calibrate a network of multiple planar laser range finders scanning horizontally. To robustly deal with the potentially restricted overlap between the fields of view, our approach only requires a dynamic object, e.g., a person, moving through the observed area. We employ a RANSAC-like algorithm to find the correspondences between the measurements of the different laser range finders. Based on these correspondences we formulate a graph-based optimization problem to determine the maximum likelihood extrinsic parameters of the sensor network. Furthermore, we present a method to evaluate the consistency of the resulting calibration based on visibility constraints. Experiments on real and simulated data show that the proposed approach yields better results than techniques that only perform pairwise calibration.},
  keywords  = {Calibration and Identification, Sensor Networks},
  url       = {http://ais.informatik.uni-freiburg.de/publications/papers/roewekaemper15icra.pdf}
}

@inproceedings{scaramuzza2007iros,
  author    = {D. Scaramuzza and A. Harati and R. Siegwart},
  title     = {{Extrinsic Self Calibration of a Camera and a 3D Laser Range Finder from Natural Scenes}},
  booktitle = iros,
  year      = {2007},
  abstract  = {In this paper, we describe a new approach for the extrinsic calibration of a camera with a 3D laser range finder, that can be done on the fly. This approach does not require any calibration object. Only few point correspondences are used, which are manually selected by the user from a scene viewed by the two sensors. The proposed method relies on a novel technique to visualize the range information obtained from a 3D laser scanner. This technique converts the visually ambiguous 3D range information into a 2D map where natural features of a scene are highlighted. We show that by enhancing the features the user can easily find the corresponding points of the camera image points. Therefore, visually identifying lasercamera correspondences becomes as easy as image pairing. Once point correspondences are given, extrinsic calibration is done using the well-known PnP algorithm followed by a nonlinear refinement process. We show the performance of our approach through experimental results. In these experiments, we will use an omnidirectional camera. The implication of this method is important because it brings 3D computer vision systems out of the laboratory and into practical use.},
  keywords  = {Sensor Fusion, Range Sensing, Mapping},
  url       = {https://rpg.ifi.uzh.ch/docs/IROS07_scaramuzza.pdf}
}

@inproceedings{schenk2012iros,
  author    = {K. Schenk and A. Kolarow and M. Eisenbach and K. Debes and H. Gross},
  title     = {{Automatic Calibration of a Stationary Network of Laser Range Finders by Matching Movement Trajectories}},
  booktitle = iros,
  year      = {2012},
  abstract  = {Laser based detection and tracking of persons can be used for numerous tasks. While a single laser range finder (LRF) is sufficient for detecting and tracking persons on a mobile robot platform, a network of multiple LRF is required to observe persons in larger spaces. Calibrating multiple LRF into a global coordinate system is usually done by hand in a time consuming procedure. An automatic calibration mechanism for such a sensor network is introduced in this paper. Without the need of prior knowledge about the environment, this mechanism is able to obtain the positions and orientations of all LRF in a global coordinate system. By comparing person tracks, determined for each individual LRF unit and matching them, constrains between the LRF units can be calculated. We are able to estimate the poses of all LRF by resolving these constrains. We evaluate and compare our method to the current state of the art approach methodically and experimentally. Experiments show that our calibration approach outperforms this approach.},
  keywords  = {Calibration and Identification, Sensor Networks, Human detection tracking},
  url       = {https://www.tu-ilmenau.de/fileadmin/Bereiche/IA/neurob/Publikationen/conferences_int/2012/Schenk-IROS-2012.pdf}
}

@article{shannon1948mi,
  author  = {C.E. Shannon},
  title   = {{A mathematical theory of communication}},
  journal = {Bell System Technical Journal},
  year    = {1948},
  number  = {3},
  pages   = {379--423},
  volume  = {27},
  url     = {https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf}
}

@inproceedings{tabb2015iros,
  author    = {A. Tabb and K.A. Yousef},
  title     = {{Parameterizations for Reducing Camera Reprojection Error for Robot-World Hand-Eye Calibration}},
  booktitle = iros,
  year      = {2015},
  abstract  = {Accurate robot-world, hand-eye calibration is crucial to automation tasks. In this paper, we discuss the robot-world, hand-eye calibration problem which has been modeled as the linear relationship AX = ZB, where X and Z are the unknown calibration matrices composed of rotation and translation components. While there are many different approaches to determining X and Z, including linear and iterative methods, we parameterize the rotation components using Euler angles and find a solution using Levenberg-Marquadt iterative approach. We also offer a method to determine A, X, and Z, by formulating the robot-world, hand-eye calibration problem in terms of camera reprojection error. We compare both of these approaches to the state-ofthe-art and conclude that our approaches yield lower values of camera reprojection error. In addition, we demonstrate the improved reconstruction accuracy when using the robot-world, hand-eye calibrations produced from our methods.},
  keywords  = {Calibration and Identification, Computer Vision, Robotics in Agriculture and Forestry}
}

@inproceedings{taylor2013iros,
  author    = {Z. Taylor and J. Nieto and D. Johnson},
  title     = {{Automatic Calibration of Multi-Modal Sensor Systems Using a Gradient Orientation Measure}},
  booktitle = iros,
  year      = {2013},
  abstract  = {A novel technique for calibrating a multi-modal sensor system has been developed. Our calibration method is based on the comparative alignment of output gradients from two candidate sensors. The algorithm is applied to the calibration of the extrinsic parameters of several camera-lidar systems. In this calibration the lidar scan is projected onto the cameras image using a camera model. Particle swarm optimization is used to find the optimal parameters for this model. This method requires no markers to be placed in the scene. While the system can use a set of scans, unlike many existing techniques it can also automatically calibrate the system reliably using a single scan. The method presented is successfully validated on a variety of cameras, lidars and locations. It is also compared to three existing techniques and shown to give comparable or superior results on the datasets tested.},
  keywords  = {Calibration and Identification, Field Robots, Sensor Fusion}
}

@article{umeyama1991pami,
  author   = {S. Umeyama},
  title    = {Least-squares estimation of transformation parameters between two point patterns},
  journal  = pami,
  year     = {1991},
  number   = {4},
  pages    = {376--380},
  volume   = {13},
  abstract = {In many applications of computer vision, the following problem is encountered. Two point patterns (sets of points) (x/sub i/) and (x/sub i/); i=1, 2, . . ., n are given in m-dimensional space, and the similarity transformation parameters (rotation, translation, and scaling) that give the least mean squared error between these point patterns are needed. Recently, K.S. Arun et al. (1987) and B.K.P. Horn et al. (1987) presented a solution of this problem. Their solution, however, sometimes fails to give a correct rotation matrix and gives a reflection instead when the data is severely corrupted. The proposed theorem is a strict solution of the problem, and it always gives the correct transformation parameters even when the data is corrupted.}
}

@article{wang1992tra,
  author   = {C.C. Wang},
  title    = {Extrinsic calibration of a vision sensor mounted on a robot},
  journal  = tra,
  year     = {1992},
  number   = {2},
  pages    = {161--175},
  volume   = {8},
  abstract = {A vision sensor mounted on a robot to detect surrounding objects is discussed. Its mounting position and orientation must be identified, resulting in an extrinsic calibration problem. The author presents three classes of extrinsic calibration procedures. All use closed-form solutions. The class A calibration procedure requires a reference object at a recalibrated location. The class B calibration procedure takes advantage of robot mobility. It requires a reference frame, but not precalibration. The class C procedure, by taking full advantage of both robot mobility and dexterity, requires no reference object but the simplest one-a visible point. In simulation studies, the class A, B, and C calibration procedures have produced estimates successfully converging to true extrinsic parameter values. Results of field experiments carried out for class B and C calibration procedures are presented. For comparison, two existing B-type calibration methods have also been tested with real data.},
  url      = {https://people.csail.mit.edu/tieu/stuff/Wang1992.pdf}
}

@article{zhuang1994tra,
  author   = {H.Q. Zhuang and Z.S. Roth and R. Sudhakar},
  title    = {{Simultaneous robot/world and tool/flange calibration by solving homogeneous transformation equations of the form AX=YB}},
  journal  = tra,
  year     = {1994},
  number   = {4},
  pages    = {549--554},
  volume   = {10},
  abstract = {The paper presents a linear solution that allows a simultaneous computation of the transformations from robot world to robot base and from robot tool to robot flange coordinate frames. The flange frame is defined on the mounting surface of the end-effector. It is assumed that the robot geometry, i.e., the transformation from the robot base frame to the robot flange frame, is known with sufficient accuracy, and that robot end-effector poses are measured. The solution has applications to accurately locating a robot with respect to a reference frame, and a robot sensor with respect to a robot end-effector. The identification problem is cast as solving a system of homogeneous transformation equations of the form A/sub i/X=YB/sub i/,i=1, 2, ..., m. Quaternion algebra is applied to derive explicit linear solutions for X and Y provided that three robot pose measurements are available. Necessary and sufficient conditions for the uniqueness of the solution are stated. Computationally, the resulting solution algorithm is noniterative, fast and robust.}
}

@article{corsini2009cgf,
  author   = {M. Corsini and M. Dellepiane and F. Ponchio and R. Scopigno},
  title    = {{Image-to-Geometry Registration: a Mutual Information Method exploiting Illumination-related Geometric Properties}},
  journal  = {Computer Graphics Forum},
  year     = {2009},
  volume   = {28},
  number   = {7},
  pages    = {1755--1764},
  abstract = {Abstract This work concerns a novel study in the field of image-to-geometry registration. Our approach takes inspiration from medical imaging, in particular from multi-modal image registration. Most of the algorithms developed in this domain, where the images to register come from different sensors (CT, X-ray, PET), are based on Mutual Information, a statistical measure of non-linear correlation between two data sources. The main idea is to use mutual information as a similarity measure between the image to be registered and renderings of the model geometry, in order to drive the registration in an iterative optimization framework. We demonstrate that some illumination-related geometric properties, such as surface normals, ambient occlusion and reflection directions can be used for this purpose. After a comprehensive analysis of such properties we propose a way to combine these sources of information in order to improve the performance of our automatic registration algorithm. The proposed approach can robustly cover a wide range of real cases and can be easily extended.}
}

@article{bentley1975cacm,
  author  = {J.L. Bentley},
  title   = {{Multidimensional Binary Search Trees Used for Associative Searching}},
  journal = cacm,
  volume  = {18},
  number  = {9},
  year    = {1975},
  pages   = {509--517},
  url     = {https://dl.acm.org/doi/pdf/10.1145/361002.361007}
}

@inproceedings{xiao2011icma,
  author    = {J.H. Xiao and J.H. Zhang and J.W. Zhang and H.X. Zhang and H.P. Hildre},
  title     = {Fast plane detection for SLAM from noisy range images in both structured and unstructured environments},
  booktitle = {Proc.~of the Intl.~Conf.~on Mechatronics and Automation (ICMA)},
  year      = {2011},
  abstract  = {This paper focuses on fast plane detection in noisy range images. First, two improvements to the state-of-the-art region growing algorithm are presented to make it faster without losing precision for unstructured environments. One is to add the seed selection procedure based on local shape information to avoid blind growth. The other is to simplify the plane fitting mean square error computation complex. Second, a novel algorithm called grid-based region growing is presented for structured environments. The point cloud is divided into small patches based on neighborhood information when it is viewed as a range image. The small patch is called grid. Then the grids are classified into different categories according to their local appearance, including sparse, planar, spherical and linear. Finally, the planar grids are clustered into big patches by region growing. The plane parameters are incrementally computed whenever a new grid is added. The resulting planes can be used for 3D plane simultaneous localization and mapping (SLAM). Experimental results show promising plane detecting speed for both structured and unstructured environments.},
  url       = {https://tams.informatik.uni-hamburg.de/publications/2011/Junhao_ICMA.pdf}
}

@article{zhang2017ar,
  author   = {J. Zhang and M. Kaess and S. Singh},
  title    = {A real-time method for depth enhanced visual odometry},
  journal  = ar,
  year     = {2017},
  pages    = {31--43},
  volume   = {41},
  number   = {1},
  abstract = {Visual odometry can be augmented by depth information such as provided by RGB-D cameras, or from lidars associated with cameras. However, such depth information can be limited by the sensors, leaving large areas in the visual images where depth is unavailable. Here, we propose a method to utilize the depth, even if sparsely available, in recovery of camera motion. In addition, the method utilizes depth by structure from motion using the previously estimated motion, and salient visual features for which depth is unavailable. Therefore, the method is able to extend RGB-D visual odometry to large scale, open environments where depth often cannot be sufficiently acquired. The core of our method is a bundle adjustment step that refines the motion estimates in parallel by processing a sequence of images, in a batch optimization. We have evaluated our method in three sensor setups, one using an RGB-D camera, and two using combinations of a camera and a 3D lidar. Our method is rated #4 on the KITTI odometry benchmark irrespective of sensing modalityâcompared to stereo visual odometry methods which retrieve depth by triangulation. The resulting average position error is 1.14\% of the distance traveled.},
  url      = {https://www.ri.cmu.edu/app/uploads/2017/08/Zhang17auro.pdf}
}

@misc{agarwal2010ceres,
  author       = {S. Agarwal and K. Mierle and Others},
  title        = {{Ceres Solver}},
  year         = {2010},
  howpublished = {http://ceres-solver.org},
  abstract     = {Ceres Solver is an open source C++ library for modeling and solving large, complicated optimization problems. It can be used to solve Non-linear Least Squares problems with bounds constraints and general unconstrained optimization problems. It is a mature, feature rich, and performant library that has been used in production at Google since 2010.}
}

@phdthesis{amiri2007phd,
  author   = {A. Amiri-Simkooei},
  title    = {{Least-squares variance component estimation: theory and GPS applications}},
  year     = {2007},
  school   = {TU Delft, Delft University of Technology},
  abstract = {In this thesis we study the method of least-squares variance component estimation (LS-VCE) and elaborate on theoretical and practical aspects of the method. We show that LS-VCE is a simple, flexible, and attractive VCE-method. The LS-VCE method is simple because it is based on the well-known principle of least-squares. With this method the estimation of the (co)variance components is based on a linear model of observation equations. The method is flexible since it works with a user-defined weight matrix. Different weight matrix classes can be defined which all automatically lead to unbiased estimators of (co)variance components. LS-VCE is attractive since it allows one to apply the existing body of knowledge of least-squares theory to the problem of (co)variance component estimation. With this method, one can 1) obtain measures of discrepancies in the stochastic model, 2) determine the covariance matrix of the (co)variance components, 3) obtain the minimum variance estimator of (co)variance components by choosing the weight matrix as the inverse of the covariance matrix, 4) take the a-priori information on the (co)variance component into account, 5) solve for a nonlinear (co)variance component model, 6) apply the idea of robust estimation to (co)variance components, 7) evaluate the estimability of the (co)variance components, and 8) avoid the problem of obtaining negative variance components.}
}

@article{chang1993pr,
  author   = {Y. Chang and X. Lebegue and J.K. Aggarwal},
  title    = {{Calibrating a mobile camera's parameters}},
  journal  = pr,
  year     = {1993},
  number   = {1},
  pages    = {75--88},
  volume   = {26},
  abstract = {Hand-eye calibration is a very important task in robotics. Many algorithms have been proposed for it, but they almost apply the L2 optimization, which is usually in the form of nonlinear optimization. In this paper, we propose new handeye calibration algorithms using convex optimization, and it can be solved in the form of a global linear optimization without starting values. Experiments with both simulated and real data are performed to test our algorithms. The experimental results show the robustness and validity of our algorithms. Considering both the computing errors and the time consuming, our algorithm based on quaternions is a good option for real applications.}
}

@article{dornaika1998tra,
  author   = {F. Dornaika and R. Horaud},
  title    = {{Simultaneous robot-world and hand-eye calibration}},
  journal  = tra,
  year     = {1998},
  number   = {4},
  pages    = {617--622},
  volume   = {14},
  abstract = {Zhuang et al. (1994) proposed a method that allows simultaneous computation of the rigid transformations from world frame to robot base frame and from hand frame to camera frame. Their method attempts to solve a homogeneous matrix equation of the for, AX=ZB. They use quaternions to derive explicit linear solution for X and Z. In this paper, we present two new solutions that attempt to solve the homogeneous matrix equation mentioned above: 1) a closed-form method which uses quaternion algebra and a positive quadratic error function associated with this representation; 2) a method based on nonlinear constrained minimization and which simultaneously solves for rotations and translations. These results may be useful to other problems that can be formulated in the same mathematical form. We perform a sensitivity analysis for both our two methods and the linear method developed by Zhuang et al. This analysis allows the comparison of the three methods. In the light of this comparison, the nonlinear optimization method, which solves for rotations and translations simultaneously, seems to be the most stable one with respect to noise and to measurement errors.}
}

@article{neitzel2016jcam,
  author   = {F. Neitzel and B. Schaffrin},
  title    = {{On the Gauss--Helmert model with a singular dispersion matrix where BQ is of smaller rank than B}},
  journal  = {Journal of computational and applied mathematics},
  year     = {2016},
  pages    = {458--467},
  volume   = {291},
  abstract = {The case of a singular dispersion matrix within the GaussâHelmert Model has been considered before, usually assuming a sufficiently small rank deficiency in order to guarantee a unique solution for both the residual vector as well as the estimated parameter vector of type Best Linear Uniformly Unbiased Estimate (BLUUE). In this contribution the emphasis is shifted towards establishing necessary and sufficient conditions for a unique residual vector, along with a unique estimate of type Best Linear Uniformly Minimum Bias Estimate (BLUMBE) for the parameter vector. Should uniformly unbiased estimates exist, the BLUMBE obviously becomes the BLUUE.}
}

@inproceedings{blum2018iros,
  author    = {H. Blum and A.R. Gawel and R. Siegwart and C. Cadena},
  title     = {{Modular Sensor Fusion for Semantic Segmentation}},
  booktitle = iros,
  year      = 2018,
  keywords  = {Sensor Fusion, Object detection, segmentation, categorization, Deep Learning in Robotics and Automation},
  abstract  = {Sensor fusion is a fundamental process in robotic systems as it extends the perceptual range and increases robustness in real-world operations. Current multi-sensor deep learning based semantic segmentation approaches do not provide robustness to under-performing classes in one modality, or require a specific architecture with access to the full aligned multi-sensor training data. In this work, we analyze statistical fusion approaches for semantic segmentation that overcome these drawbacks while keeping a competitive performance. The studied approaches are modular by construction, allowing to have different training sets per modality and only a much smaller subset is needed to calibrate the statistical models. We evaluate a range of statistical fusion approaches and report their performance against state-of-the-art baselines on both realworld and simulated data. In our experiments, the approach improves performance in IoU over the best single modality segmentation results by up to 5\%. We make all implementations and configurations publicly available.},
  url       = {proceedings:blum2018iros.pdf}
}

@inproceedings{tsai1988icra,
  author    = {R.Y. Tsai and R.K. Lenz},
  title     = {{Real time versatile robotics hand/eye calibration using 3D machine vision}},
  booktitle = icra,
  year      = {1988},
  abstract  = {A technique is described for computing 3-D position and orientation of a camera relative to the last joint of a robot manipulator in an eye-on-hand configuration. The calibration can be done within a fraction of a millisecond after the robot finishes the movement. The setup is simple (a planar set of calibration points arbitrarily placed on the work table, in addition to robot and camera) and is the same as that for a common camera calibration. This method is claimed to be faster, simpler, and more accurate than any existing technique for hand/eye calibration. Generic geometric properties of lemmas are presented, leading to the derivation of the final algorithms, which are aimed at simplicity, efficiency, and accuracy while giving ample geometric and algebraic insights. Besides describing the technique, critical factors influencing the accuracy are analysed, and procedures for improving accuracy are introduced. Tests results of both simulation and real experiments on an IBM Cartesian robot are reported.},
  url       = {https://people.csail.mit.edu/tieu/stuff/Tsai.pdf}
}

@article{wolf1978ghm,
  author  = {H. Wolf},
  title   = {{Das geod{\"a}tische Gau{\ss}-Helmert-Modell und seine Eigenschaften}},
  year    = {1978},
  volume  = {103},
  pages   = {103:41-43},
  journal = {Zeitschrift for Vermessungswesen (103)}
}

@inproceedings{zhao2011icra,
  author    = {Z. Zhao},
  title     = {{Hand-Eye Calibration Using Convex Optimization}},
  booktitle = icra,
  year      = 2011,
  keywords  = {Computer Vision for Robotics and Automation, Medical Robots and Systems},
  abstract  = {Hand-eye calibration is a very important task in robotics. Many algorithms have been proposed for it, but they almost apply the L2 optimization, which is usually in the form of nonlinear optimization. In this paper, we propose new handeye calibration algorithms using convex optimization, and it can be solved in the form of a global linear optimization without starting values. Experiments with both simulated and real data are performed to test our algorithms. The experimental results show the robustness and validity of our algorithms. Considering both the computing errors and the time consuming, our algorithm based on quaternions is a good option for real applications.},
  url       = {proceedings:zhao2011icra-hcuc.pdf}
}


@inproceedings{guo2012icra,
  author    = {C.X. Guo and F.M. Mirzaei and S.I. Roumeliotis},
  title     = {{An analytical least-squares solution to the odometer-camera extrinsic calibration problem}},
  booktitle = icra,
  year      = {2012},
  abstract  = {In order to fuse camera and odometer measurements, we first need to estimate their relative transformation through the so-called odometer-camera extrinsic calibration. In this paper, we present a two-step analytical least-squares solution for the extrinsic odometer-camera calibration that (i) is not iterative and finds the least-squares optimal solution without any initialization, and (ii) does not require any special hardware or the presence of known landmarks in the scene. Specifically, in the first step, we estimate a subset of the 3D relative rotation parameters by analytically minimizing a least-squares cost function. We then back-substitute these estimates in the measurement constraints, and determine the rest of the 3D transformation parameters by analytically minimizing a second least-squares cost function. Simulation and experimental results are presented that validate the efficiency and accuracy of the proposed algorithm.},
  url       = {https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=08858e5c03b6f7f8d843c2371fc2be2fb4bea945}
}

@inproceedings{kummerle2011icra,
  author    = {R. K{\"u}mmerle and G. Grisetti and H. Strasdat and K. Konolige and W. Burgard},
  title     = {{g2o: A general framework for graph optimization}},
  booktitle = icra,
  year      = {2011},
  abstract  = {Many popular problems in robotics and computer vision including various types of simultaneous localization and mapping (SLAM) or bundle adjustment (BA) can be phrased as least squares optimization of an error function that can be represented by a graph. This paper describes the general structure of such problems and presents g2o, an open-source C++ framework for optimizing graph-based nonlinear error functions. Our system has been designed to be easily extensible to a wide range of problems and a new problem typically can be specified in a few lines of code. The current implementation provides solutions to several variants of SLAM and BA. We provide evaluations on a wide range of real-world and simulated datasets. The results demonstrate that while being general g2o offers a performance comparable to implementations of state of-the-art approaches for the specific problems.},
  url       = {http://ais.informatik.uni-freiburg.de/publications/papers/kuemmerle11icra.pdf}
}

@inproceedings{schneider2013iros,
  author    = {S. Schneider and T. Luettel and H. Wuensche},
  title     = {{Odometry-based online extrinsic sensor calibration}},
  booktitle = iros,
  year      = {2013},
  abstract  = {In recent years vehicles have been equipped with more and more sensors for environment perception. Among these sensors are cameras, RADAR, single-layer and multi-layer LiDAR. One key challenge for the fusion of these sensors is sensor calibration. In this paper we present a novel extrinsic calibration algorithm based on sensor odometry. Given the time-synchronized delta poses of two sensors our technique recursively estimates the relative pose between these sensors. The method is generic in that it can be used to estimate complete 6DOF poses, given the sensors provide a 6DOF odometry, as well as 3DOF poses (planar offset and yaw angle) for sensors providing a 3DOF odometry, like a single-beam LiDAR. We show that the proposed method is robust against motion degeneracy and present results on both simulated and real world data using an inertial navigation system (INS) and a stereo camera system.},
  url       = {http://vigir.missouri.edu/~gdesouza/Research/Conference_CDs/IEEE_IROS_2013/media/files/0325.pdf}
}

@inproceedings{taylor2015icra,
  author    = {Z. Taylor and J. Nieto},
  title     = {{Motion-based calibration of multimodal sensor arrays}},
  booktitle = icra,
  year      = {2015},
  abstract  = {This paper formulates a new pipeline for automated extrinsic calibration of multi-sensor mobile platforms. The new method can operate on any combination of cameras, navigation sensors and 3D lidars. Current methods for extrinsic calibration are either based on special markers and/or chequerboards, or they require a precise parameters initialisation for the calibration to converge. These two limitations prevent them from being fully automatic. The method presented in this paper removes these restrictions. By combining information extracted from both, platform's motion estimates and external observations, our approach eliminates the need for special markers and also removes the need for manual initialisation. A third advantage is that the motion-based automatic initialisation does not require overlapping field of view between sensors. The paper also provides a method to estimate the accuracy of the resulting calibration. We illustrate the generalisation of our approach and validate its performance by showing results with two contrasting datasets. The first dataset was collected in a city with a car platform, and the second one was collected in a tree-crop farm with a Segway platform.}
}

@inproceedings{suenderhauf2017iros,
  author    = {N. S{\"u}nderhauf and T. Pham and Y. Latif and M.J. Milford and I. Reid},
  title     = {{Meaningful Maps with Object-Oriented Semantic Mapping}},
  booktitle = iros,
  year      = 2017,
  keywords  = {Object detection, segmentation, categorization, Mapping, Semantic Scene Understanding},
  abstract  = {For intelligent robots to interact in meaningful ways with their environment, they must understand both the geometric and semantic properties of the scene surrounding them. The majority of research to date has addressed these mapping challenges separately, focusing on either geometric or semantic mapping. In this paper we address the problem of building environmental maps that include both semantically meaningful, object-level entities and point- or mesh-based geometrical representations. We simultaneously build geometric point cloud models of previously unseen instances of known object classes and create a map that contains these object models as central entities. Our system leverages sparse, feature-based RGB-D SLAM, image-based deep-learning object detection and 3D unsupervised segmentation.},
  url       = {proceedings:snderhauf2017iros.pdf}
}

@article{panphattarasap2018arxiv,
  author   = {P. Panphattarasap and A. Calway},
  title    = {{Automated Map Reading: Image Based Localisation in 2-D Maps Using Binary Semantic Descriptors}},
  journal  = arxiv,
  year     = 2018,
  volume   = {arXiv:1803.00788},
  url      = {http://arxiv.org/pdf/1803.00788v1},
  abstract = {We describe a novel approach to image based localisation in urban environments using semantic matching between images and a 2-D map. It contrasts with the vast majority of existing approaches which use image to image database matching. We use highly compact binary descriptors to represent semantic features at locations, significantly increasing scalability compared with existing methods and having the potential for greater invariance to variable imaging conditions. The approach is also more akin to human map reading, making it more suited to human-system interaction. The binary descriptors indicate the presence or not of semantic features relating to buildings and road junctions in discrete viewing directions. We use CNN classifiers to detect the features in images and match descriptor estimates with a database of location tagged descriptors derived from the 2-D map. In isolation, the descriptors are not sufficiently discriminative, but when concatenated sequentially along a route, their combination becomes highly distinctive and allows localisation even when using non-perfect classifiers. Performance is further improved by taking into account left or right turns over a route. Experimental results obtained using Google StreetView and OpenStreetMap data show that the approach has considerable potential, achieving localisation accuracy of around 85\% using routes corresponding to approximately 200 meters.}
}

@article{yang2018arxiv-dvso,
  author   = {N. Yang and R. Wang and J. StÃ¼ckler and D. Cremers},
  title    = {{Deep Virtual Stereo Odometry: Leveraging Deep Depth Prediction for Monocular Direct Sparse Odometry}},
  journal  = arxiv,
  year     = 2018,
  volume   = {arXiv:1807.02570},
  url      = {http://arxiv.org/pdf/1807.02570v1},
  abstract = {Monocular visual odometry approaches that purely rely on geometric cues are prone to scale drift and require sufficient motion parallax in successive frames for motion estimation and 3D reconstruction. In this paper, we propose to leverage deep monocular depth prediction to overcome limitations of geometry-based monocular visual odometry. To this end, we incorporate deep depth predictions into Direct Sparse Odometry as direct virtual stereo measurements. For depth prediction, we design a novel deep network that refines predicted depth from a single image in a two-stage process. We train our network in a semi-supervised way on photoconsistency in stereo images and on consistency with accurate sparse depth reconstructions from Stereo DSO. Our deep predictions excel state-of-the-art approaches for monocular depth on the KITTI benchmark. Moreover, our Deep Virtual Stereo Odometry clearly exceeds previous monocular and deep-learning based methods in accuracy. It even achieves comparable performance to the state-of-the-art stereo methods, while only relying on a single camera.}
}

@inproceedings{huang2018iros,
  author    = {K.H. Huang and C. Stachniss},
  title     = {{Joint Ego-motion Estimation Using a Laser Scanner and a Monocular Camera Through Relative Orientation Estimation and 1-DoF ICP}},
  booktitle = iros,
  year      = 2018,
  abstract  = {Pose estimation and mapping are key capabilities of most autonomous vehicles and thus a number of localization and SLAM algorithms have been developed in the past. Autonomous robots and cars are typically equipped with multiple sensors. Often, the sensor suite includes a camera and a laser range finder. In this paper, we consider the problem of incremental ego-motion estimation, using both, a monocular camera and a laser range finder jointly. We propose a new algorithm, that exploits the advantages of both sensors---the ability of cameras to determine orientations well and the ability of laser range finders to estimate the scale and to directly obtain 3D point clouds. Our approach estimates the five degree of freedom relative orientation from image pairs through feature point correspondences and formulates the remaining scale estimation as a new variant of the iterative closet point problem with only one degree of freedom. We furthermore exploit the camera information in a new way to constrain the data association between laser point clouds. The experiments presented in this paper suggest that our approach is able to accurately estimate the ego-motion of a vehicle and that we obtain more accurate frame-to-frame alignments than with one sensor modality alone.},
  url       = {https://www.ipb.uni-bonn.de/wp-content/papercite-data/pdf/huang2018iros.pdf}
}

@inproceedings{lottes2018iros,
  author    = {P. Lottes and J. Behley and N. Chebrolu and A. Milioto and C. Stachniss},
  title     = {{Joint Stem Detection and Crop-Weed Classification for Plant-specific Treatment in Precision Farming}},
  booktitle = iros,
  year      = 2018,
  abstract  = {Applying agrochemicals is the default procedure for conventional weed control in crop production, but has negative impacts on the environment. Robots have the potential to treat every plant in the field individually and thus can reduce the required use of such chemicals. To achieve that, robots need the ability to identify crops and weeds in the field and must additionally select effective treatments. While certain types of weed can be treated mechanically, other types need to be treated by (selective) spraying. In this paper, we present an approach that provides the necessary information for effective plant-specific treatment. It outputs the stem location for weeds, which allows for mechanical treatments, and the covered area of the weed for selective spraying. Our approach uses an end-to- end trainable fully convolutional network that simultaneously estimates stem positions as well as the covered area of crops and weeds. It jointly learns the class-wise stem detection and the pixel-wise semantic segmentation. Experimental evaluations on different real-world datasets show that our approach is able to reliably solve this problem. Compared to state-of-the-art approaches, our approach not only substantially improves the stem detection accuracy, i.e., distinguishing crop and weed stems, but also provides an improvement in the semantic segmentation performance.},
  url       = {https://www.ipb.uni-bonn.de/pdfs/lottes18iros.pdf}
}

@article{lottes2018ral,
  author   = {P. Lottes and J. Behley and A. Milioto and C. Stachniss},
  title    = {{Fully Convolutional Networks with Sequential Information for Robust Crop and Weed Detection in Precision Farming}},
  journal  = ral,
  year     = 2018,
  volume   = {3},
  number   = {4},
  pages    = {3097--3104},
  abstract = {Reducing the use of agrochemicals is an important component towards sustainable agriculture. Robots that can perform targeted weed control offer the potential to contribute to this goal, for example, through specialized weeding actions such as selective spraying or mechanical weed removal. A prerequisite of such systems is a reliable and robust plant classification system that is able to distinguish crop and weed in the field. A major challenge in this context is the fact that different fields show a large variability. Thus, classification systems have to robustly cope with substantial environmental changes with respect to weed pressure and weed types, growth stages of the crop, visual appearance, and soil conditions. In this paper, we propose a novel crop-weed classification system that relies on a fully convolutional network with an encoder-decoder structure and incorporates spatial information by considering image sequences. Exploiting the crop arrangement information that is observable from the image sequences enables our system to robustly estimate a pixel-wise labeling of the images into crop and weed, i.e., a semantic segmentation. We provide a thorough experimental evaluation, which shows that our system generalizes well to previously unseen fields under varying environmental conditions --- a key capability to actually use such systems in precision framing. We provide comparisons to other state-of-the-art approaches and show that our system substantially improves the accuracy of crop-weed classification without requiring a retraining of the model.},
  url      = {https://www.ipb.uni-bonn.de/wp-content/papercite-data/pdf/lottes2018ral.pdf}
}

@inproceedings{behley2018rss,
  author    = {J. Behley and C. Stachniss},
  title     = {{Efficient Surfel-Based SLAM using 3D Laser Range Data in Urban Environments}},
  booktitle = rss,
  year      = 2018,
  videourl  = {https://www.youtube.com/watch?v=-AEX203rXkE},
  url       = {http://www.roboticsproceedings.org/rss14/p16.pdf}
}

@article{yang2018arxiv,
  author   = {S. Yang and S. Scherer},
  title    = {{CubeSLAM: Monocular 3D Object Detection and SLAM without Prior Models}},
  journal  = arxiv,
  year     = 2018,
  volume   = {arXiv:1806.00557},
  url      = {http://arxiv.org/pdf/1806.00557v1},
  abstract = {We present a method for single image 3D cuboid object detection and multi-view object SLAM without prior object model, and demonstrate that the two aspects can benefit each other. For 3D detection, we generate high quality cuboid proposals from 2D bounding boxes and vanishing points sampling. The proposals are further scored and selected to align with image edges. Experiments on SUN RGBD and KITTI shows the efficiency and accuracy over existing approaches. Then in the second part, multi-view bundle adjustment with novel measurement functions is proposed to jointly optimize camera poses, objects and points, utilizing single view detection results. Objects can provide more geometric constraints and scale consistency compared to points. On the collected and public TUM and KITTI odometry datasets, we achieve better pose estimation accuracy over the state-of-the-art monocular SLAM while also improve the 3D object detection accuracy at the same time.}
}

@article{goldfain2018arxiv,
  author   = {B. Goldfain and P. Drews and C. You and M. Barulic and O. Velev and P. Tsiotras and J.M. Rehg},
  title    = {{AutoRally An open platform for aggressive autonomous driving}},
  journal  = arxiv,
  year     = 2018,
  volume   = {arXiv:1806.00678},
  url      = {http://arxiv.org/pdf/1806.00678v1},
  abstract = {This article presents AutoRally, a 1$:$5 scale robotics testbed for autonomous vehicle research. AutoRally is designed for robustness, ease of use, and reproducibility, so that a team of two people with limited knowledge of mechanical engineering, electrical engineering, and computer science can construct and then operate the testbed to collect real world autonomous driving data in whatever domain they wish to study. Complete documentation to construct and operate the platform is available online along with tutorials, example controllers, and a driving dataset collected at the Georgia Tech Autonomous Racing Facility. Offline estimation algorithms are used to determine parameters for physics-based dynamics models using an adaptive limited memory joint state unscented Kalman filter. Online vehicle state estimation using a factor graph optimization scheme and a convolutional neural network for semantic segmentation of drivable surface are presented. All algorithms are tested with real world data from the fleet of six AutoRally robots at the Georgia Tech Autonomous Racing Facility tracks, and serve as a demonstration of the robot$'$s capabilities.}
}

@article{jirak2018arxiv,
  author   = {D. Jirak and S. Wermter},
  title    = {{Potentials and Limitations of Deep Neural Networks for Cognitive Robots}},
  journal  = arxiv,
  year     = 2018,
  volume   = {arXiv:1805.00777},
  url      = {http://arxiv.org/pdf/1805.00777v1},
  abstract = {Although Deep Neural Networks reached remarkable performance on several benchmarks and even gained scientific publicity, they are not able to address the concept of cognition as a whole. In this paper, we argue that those architectures are potentially interesting for cognitive robots regarding their perceptual representation power for audio and vision data. Then, we identify crucial settings for cognitive robotics where deep neural networks have as yet only contributed little compared to the challenges in cognitive robotics. Finally, we argue that the rather unexplored area of Reservoir Computing qualifies to be an integral part of sequential learning in this context.}
}

@article{shakhatreh2018arxiv,
  author   = {H. Shakhatreh and A. Sawalmeh and A. Al-Fuqaha and Z. Dou and E. Almaita and I. Khalil and N.S. Othman and A. Khreishah and M. Guizani},
  title    = {{Unmanned Aerial Vehicles: A Survey on Civil Applications and Key Research Challenges}},
  journal  = arxiv,
  year     = 2018,
  volume   = {arXiv:1805.00881},
  url      = {http://arxiv.org/pdf/1805.00881v1},
  abstract = {The use of unmanned aerial vehicles (UAVs) is growing rapidly across many civil application domains including real-time monitoring, providing wireless coverage, remote sensing, search and rescue, delivery of goods, security and surveillance, precision agriculture, and civil infrastructure inspection. Smart UAVs are the next big revolution in UAV technology promising to provide new opportunities in different applications, especially in civil infrastructure in terms of reduced risks and lower cost. Civil infrastructure is expected to dominate the more that $45 Billion market value of UAV usage. In this survey, we present UAV civil applications and their challenges. We also discuss current research trends and provide future insights for potential UAV uses. Furthermore, we present the key challenges for UAV civil applications, including: charging challenges, collision avoidance and swarming challenges, and networking and security related challenges. Based on our review of the recent literature, we discuss open research challenges and draw high-level insights on how these challenges might be approached.}
}

@article{valada2018arxiv,
  author   = {A. Valada and W. Burgard},
  title    = {{Deep Spatiotemporal Models for Robust Proprioceptive Terrain Classification}},
  journal  = arxiv,
  year     = 2018,
  volume   = {arXiv:1804.00736},
  url      = {http://arxiv.org/pdf/1804.00736v1},
  abstract = {Terrain classification is a critical component of any autonomous mobile robot system operating in unknown real-world environments. Over the years, several proprioceptive terrain classification techniques have been introduced to increase robustness or act as a fallback for traditional vision based approaches. However, they lack widespread adaptation due to various factors that include inadequate accuracy, robustness and slow run-times. In this paper, we use vehicle-terrain interaction sounds as a proprioceptive modality and propose a deep Long-Short Term Memory (LSTM) based recurrent model that captures both the spatial and temporal dynamics of such a problem, thereby overcoming these past limitations. Our model consists of a new Convolution Neural Network (CNN) architecture that learns deep spatial features, complemented with LSTM units that learn complex temporal dynamics. Experiments on two extensive datasets collected with different microphones on various indoor and outdoor terrains demonstrate state-of-the-art performance compared to existing techniques. We additionally evaluate the performance in adverse acoustic conditions with high ambient noise and propose a noise-aware training scheme that enables learning of more generalizable models that are essential for robust real-world deployments.}
}

@article{valada2017ijrr,
  author   = {A. Valada and W. Burgard},
  title    = {{Deep Spatiotemporal Models for Robust Proprioceptive Terrain Classification}},
  journal  = ijrr,
  volume   = {36},
  number   = {13--14},
  pages    = {1521--1539},
  year     = 2017,
  url      = {http://arxiv.org/pdf/1804.00736v1},
  abstract = {Terrain classification is a critical component of any autonomous mobile robot system operating in unknown real-world environments. Over the years, several proprioceptive terrain classification techniques have been introduced to increase robustness or act as a fallback for traditional vision based approaches. However, they lack widespread adaptation due to various factors that include inadequate accuracy, robustness and slow run-times. In this paper, we use vehicle-terrain interaction sounds as a proprioceptive modality and propose a deep Long-Short Term Memory (LSTM) based recurrent model that captures both the spatial and temporal dynamics of such a problem, thereby overcoming these past limitations. Our model consists of a new Convolution Neural Network (CNN) architecture that learns deep spatial features, complemented with LSTM units that learn complex temporal dynamics. Experiments on two extensive datasets collected with different microphones on various indoor and outdoor terrains demonstrate state-of-the-art performance compared to existing techniques. We additionally evaluate the performance in adverse acoustic conditions with high ambient noise and propose a noise-aware training scheme that enables learning of more generalizable models that are essential for robust real-world deployments.}
}

@article{yin2018arxiv,
  author   = {P. Yin and Y. He and L. Xu and Y. Peng and J. Han and W. Xu},
  title    = {{Synchronous Adversarial Feature Learning for LiDAR based Loop Closure Detection}},
  journal  = arxiv,
  year     = 2018,
  volume   = {arXiv:1804.01945},
  url      = {http://arxiv.org/pdf/1804.01945v1},
  abstract = {Loop Closure Detection (LCD) is the essential module in the simultaneous localization and mapping (SLAM) task. In the current appearance-based SLAM methods, the visual inputs are usually affected by illumination, appearance and viewpoints changes. Comparing to the visual inputs, with the active property, light detection and ranging (LiDAR) based point-cloud inputs are invariant to the illumination and appearance changes. In this paper, we extract 3D voxel maps and 2D top view maps from LiDAR inputs, and the former could capture the local geometry into a simplified 3D voxel format, the later could capture the local road structure into a 2D image format. However, the most challenge problem is to obtain efficient features from 3D and 2D maps to against the viewpoints difference. In this paper, we proposed a synchronous adversarial feature learning method for the LCD task, which could learn the higher level abstract features from different domains without any label data. To the best of our knowledge, this work is the first to extract multi-domain adversarial features for the LCD task in real time. To investigate the performance, we test the proposed method on the KITTI odometry dataset. The extensive experiments results show that, the proposed method could largely improve LCD accuracy even under huge viewpoints differences.}
}

@article{mcfadyen2018arxiv,
  author   = {A. McFadyen and F. Dayoub and S. Martin and J. Ford and P. Corke},
  title    = {{Assisted Control for Semi-Autonomous Power Infrastructure Inspection using Aerial Vehicles}},
  journal  = arxiv,
  year     = 2018,
  volume   = {arXiv:1804.02154v1},
  url      = {http://arxiv.org/pdf/1804.02154v1},
  abstract = {This paper presents the design and implementation of an assisted control technology for a small multirotor platform for aerial inspection of fixed energy infrastructure. Sensor placement is supported by a theoretical analysis of expected sensor performance and constrained platform behaviour to speed up implementation. The optical sensors provide relative position information between the platform and the asset, which enables human operator inputs to be autonomously adjusted to ensure safe separation. The assisted control approach is designed to reduced operator workload during close proximity inspection tasks, with collision avoidance and safe separation managed autonomously. The energy infrastructure includes single vertical wooden poles and crossarm with attached overhead wires. Simulated and real experimental results are provided.}
}

@article{beyer2018arxiv,
  author   = {L. Beyer and A. Hermans and T. Linder and K.O. Arras and B. Leibe},
  title    = {{Deep Person Detection in 2D Range Data}},
  journal  = arxiv,
  year     = 2018,
  volume   = {arXiv:1804.02463},
  url      = {http://arxiv.org/pdf/1804.02463v1},
  abstract = {Detecting humans is a key skill for mobile robots and intelligent vehicles in a large variety of applications. While the problem is well studied for certain sensory modalities such as image data, few works exist that address this detection task using 2D range data. However, a widespread sensory setup for many mobile robots in service and domestic applications contains a horizontally mounted 2D laser scanner. Detecting people from 2D range data is challenging due to the speed and dynamics of human leg motion and the high levels of occlusion and self-occlusion particularly in crowds of people. While previous approaches mostly relied on handcrafted features, we recently developed the deep learning based wheelchair and walker detector DROW. In this paper, we show the generalization to people, including small modifications that significantly boost DROW's performance. Additionally, by providing a small, fully online temporal window in our network, we further boost our score. We extend the DROW dataset with person annotations, making this the largest dataset of person annotations in 2D range data, recorded during several days in a real-world environment with high diversity. Extensive experiments with three current baseline methods indicate it is a challenging dataset, on which our improved DROW detector beats the current state-of-the-art.}
}

@article{hallmann2017plosone,
  author   = {Hallmann, C.A. and Sorg, M. and Jongejans, E. and Siepel, H. and Hofland, N. and Schwan, H. and Stenmans, W. and Mueller, A. and Sumser, H. and Hoerren, T. and Goulson, D. and de Kroon, H.},
  title    = {More than 75 percent decline over 27 years in total flying insect biomass in protected areas},
  journal  = plosone,
  year     = {2017},
  volume   = {12},
  pages    = {1--21},
  number   = {10},
  abstract = {Global declines in insects have sparked wide interest among scientists, politicians, and the general public. Loss of insect diversity and abundance is expected to provoke cascading effects on food webs and to jeopardize ecosystem services. Our understanding of the extent and underlying causes of this decline is based on the abundance of single species or taxonomic groups only, rather than changes in insect biomass which is more relevant for ecological functioning. Here, we used a standardized protocol to measure total insect biomass using Malaise traps, deployed over 27 years in 63 nature protection areas in Germany (96 unique location-year combinations) to infer on the status and trend of local entomofauna. Our analysis estimates a seasonal decline of 76\%, and mid-summer decline of 82\% in flying insect biomass over the 27 years of study. We show that this decline is apparent regardless of habitat type, while changes in weather, land use, and habitat characteristics cannot explain this overall decline. This yet unrecognized loss of insect biomass must be taken into account in evaluating declines in abundance of species depending on insects as a food source, and ecosystem functioning in the European landscape.},
  url      = {http://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0185809&type=printable}
}

@article{pound2014plantphysiology,
  author   = {Pound, M.P. and French, A.P. and Murchie, E.H. and Pridmore, T.P.},
  title    = {{Automated Recovery of Three-Dimensional Models of Plant Shoots from Multiple Color Images}},
  volume   = {166},
  number   = {4},
  pages    = {1688--1698},
  year     = {2014},
  abstract = {Increased adoption of the systems approach to biological research has focused attention on the use of quantitative models of biological objects. This includes a need for realistic three-dimensional (3D) representations of plant shoots for quantification and modeling. Previous limitations in single-view or multiple-view stereo algorithms have led to a reliance on volumetric methods or expensive hardware to record plant structure. We present a fully automatic approach to image-based 3D plant reconstruction that can be achieved using a single low-cost camera. The reconstructed plants are represented as a series of small planar sections that together model the more complex architecture of the leaf surfaces. The boundary of each leaf patch is refined using the level-set method, optimizing the model based on image information, curvature constraints, and the position of neighboring surfaces. The reconstruction process makes few assumptions about the nature of the plant material being reconstructed and, as such, is applicable to a wide variety of plant species and topologies and can be extended to canopy-scale imaging. We demonstrate the effectiveness of our approach on data sets of wheat (Triticum aestivum) and rice (Oryza sativa) plants as well as a unique virtual data set that allows us to compute quantitative measures of reconstruction accuracy. The output is a 3D mesh structure that is suitable for modeling applications in a format that can be imported in the majority of 3D graphics and software packages.Glossary3Dthree-dimensionalLIDARlight detection and ranging laserPMVSpatch-based multiple-view stereo2Dtwo-dimensional},
  url      = {http://www.plantphysiol.org/content/166/4/1688.full.pdf},
  journal  = plantphysiology
}


@article{wolfert2017agsys,
  author   = {S. Wolfert and L. Ge and C. Verdouw and M.-J. Bogaardt},
  title    = {{Big Data in Smart Farming - A review}},
  journal  = agsys,
  volume   = {153},
  pages    = {69--80},
  year     = 2017,
  url      = {https://reader.elsevier.com/reader/sd/294A9648AD59BE627EAC447C858CB0D2484A27DB2F266D1A57F88939390130C0A523F4042C266D8AFB2ABCD1E3BDAEB3},
  keywords = {Agriculture, Big Data},
  abstract = {Smart Farming is a development that emphasizes the use of information and communication technology in the cyber-physical farm management cycle. New technologies such as the Internet of Things and Cloud Computing are expected to leverage this development and introduce more robots and artificial intelligence in farming. This is encompassed by the phenomenon of Big Data, massive volumes of data with a wide variety that can be captured, analysed and used for decision-making. This review aims to gain insight into the state-of-the-art of Big Data applications in Smart Farming and identify the related socio-economic challenges to be addressed. Following a structured approach, a conceptual framework for analysis was developed that can also be used for future studies on this topic. The review shows that the scope of Big Data applications in Smart Farming goes beyond primary production; it is influencing the entire food supply chain. Big data are being used to provide predictive insights in farming operations, drive real-time operational decisions, and redesign business processes for game-changing business models. Several authors therefore suggest that Big Data will cause major shifts in roles and power relations among different players in current food supply chain networks. The landscape of stakeholders exhibits an interesting game between powerful tech companies, venture capitalists and often small start-ups and new entrants. At the same time there are several public institutions that publish open data, under the condition that the privacy of persons must be guaranteed. The future of Smart Farming may unravel in a continuum of two extreme scenarios: 1) closed, proprietary systems in which the farmer is part of a highly integrated food supply chain or 2) open, collaborative systems in which the farmer and every other stakeholder in the chain network is flexible in choosing business partners as well for the technology as for the food production side. The further development of data and application infrastructures (platforms and standards) and their institutional embedment will play a crucial role in the battle between these scenarios. From a socio-economic perspective, the authors propose to give research priority to organizational issues concerning governance issues and suitable business models for data sharing in different supply chain scenarios.}
}

@article{carlevaris-bianco2016ijrr,
  author   = {N. Carlevaris-Bianco and A.K. Ushani and R.M. Eustice},
  title    = {{University of Michigan North Campus long-term vision and lidar dataset}},
  journal  = ijrr,
  volume   = {35},
  number   = {9},
  pages    = {1023--1035},
  year     = 2016,
  abstract = {This paper documents a large scale, long-term autonomy dataset for robotics research collected on the University of Michiganâs North Campus. The dataset consists of omnidirectional imagery, 3D lidar, planar lidar, GPS, and proprioceptive sensors for odometry collected using a Segway robot. The dataset was collected to facilitate research focusing on long-term autonomous operation in changing environments. The dataset is composed of 27 sessions spaced approximately biweekly over the course of 15 months. The sessions repeatedly explore the campus, both indoors and outdoors, on varying trajectories, and at different times of the day across all four seasons. This allows the dataset to capture many challenging elements including: moving obstacles (e.g. pedestrians, bicyclists and cars), changing lighting, varying viewpoint, seasonal and weather changes (e.g. falling leaves and snow), and long-term structural changes caused by construction projects. To further facilitate research, we also provide ground-truth pose for all sessions in a single frame of reference. },
  url      = {https://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=16141E792B9CB614DCCDA835A2424C20?doi=10.1.1.703.5766&rep=rep1&type=pdf}
}


@inproceedings{amayo2018icra,
  author    = {P. Amayo and P. Pinies and L.M. Paz and P. Newman},
  title     = {{Fast Global Labelling for Depth-Map Improvement Via Architectural Priors}},
  booktitle = icra,
  year      = 2018,
  keywords  = {Mapping},
  abstract  = {Depth map estimation techniques from cameras often struggle to accurately estimate the depth of large textureless regions. In this work we present a vision-only method that accurately extracts planar priors from a viewed scene without making any assumptions of the underlying scene layout. Through a fast global labelling, these planar priors can be associated to the individual pixels leading to more complete depth-maps specifically over large, plain and planar regions that tend to dominate the urban environment. When these depth-maps are deployed to the creation of a vision only dense reconstruction over large scales, we demonstrate reconstructions that yield significantly better results in terms of coverage while still maintaining high accuracy.},
  url       = {https://ori.ox.ac.uk/media/5761/2018icra_amayo.pdf}
}

@article{bai2018ral,
  author   = {F. Bai and T.A. Vidal-Calleja and S. Huang},
  title    = {{Robust Incremental SLAM under Constrained Optimization Formulation}},
  journal  = ral,
  volume   = {3},
  number   = {2},
  pages    = {1207--1214},
  year     = 2018,
  keywords = {SLAM, Localization, Mapping},
  abstract = {In this paper, we propose a constrained optimization formulation and a robust incremental framework for the simultaneous localization and mapping problem (SLAM). The new SLAM formulation is derived from the nonlinear least squares (NLS) formulation by mathematically formulating loopclosure cycles as constraints. Under the constrained SLAM formulation, we study the robustness of an incremental SLAM algorithm against local minima and outliers as a constraint/loopclosure cycle selection problem. We find a constraint metric that can predict the objective function growth after including the constraint. By the virtue of the constraint metric, we select constraints into the incremental SLAM according to a least objective function growth principle to increase robustness against local minima, and perform 2 difference test on the constraint metric to increase robustness against outliers. Finally, using sequential quadratic programming (SQP) as the solver, an incremental SLAM algorithm (iSQP) is proposed. Experimental validations are provided to illustrate the accuracy of the constraint metric, and the robustness of the proposed incremental SLAM algorithm. Nonetheless, the proposed approach is currently confined to datasets with sparse loop-closures due to its computational cost.},
  url      = {https://opus.lib.uts.edu.au/bitstream/10453/125853/1/ICRA2018_Fang.pdf}
}

@inproceedings{barnes2018icra,
  author    = {D. Barnes and W. Maddern and G.M. Pascoe and I. Posner},
  title     = {{Driven to Distraction: Self-Supervised Distractor Learning for Robust Monocular Visual Odometry in Urban Environments}},
  booktitle = icra,
  year      = 2018,
  keywords  = {Semantic Scene Understanding, Autonomous Vehicle Navigation, Visual Tracking},
  abstract  = {We present a self-supervised approach to ignoring distractors in camera images for the purposes of robustly estimating vehicle motion in cluttered urban environments. We leverage offline multi-session mapping approaches to automatically generate a per-pixel ephemerality mask and depth map for each input image, which we use to train a deep convolutional network. At run-time we use the predicted ephemerality and depth as an input to a monocular visual odometry (VO) pipeline, using either sparse features or dense photometric matching. Our approach yields metric-scale VO using only a single camera and can recover the correct egomotion even when 90\% of the image is obscured by dynamic, independently moving objects. We evaluate our robust VO methods on more than 400km of driving from the Oxford RobotCar Dataset and demonstrate reduced odometry drift and significantly improved egomotion estimation in the presence of large moving vehicles in urban traffic.},
  url       = {https://arxiv.org/pdf/1711.06623.pdf}
}

@article{bergmann2018ral,
  author   = {P. Bergmann and R. Wang and D. Cremers},
  title    = {{Online Photometric Calibration of Auto Exposure Video for Realtime Visual Odometry and SLAM}},
  journal  = ral,
  volume   = {3},
  number   = {2},
  pages    = {627--634},
  year     = 2018,
  keywords = {SLAM, Computer Vision for Other Robotic Applications},
  abstract = {Recent direct visual odometry and SLAM algorithms have demonstrated impressive levels of precision. However, they require a photometric camera calibration in order to achieve competitive results. Hence, the respective algorithm cannot be directly applied to an off-the-shelf-camera or to a video sequence acquired with an unknown camera. In this work we propose a method for online photometric calibration which enables to process auto exposure videos with visual odometry precisions that are on par with those of photometrically calibrated videos. Our algorithm recovers the exposure times of consecutive frames, the camera response function, and the attenuation factors of the sensor irradiance due to vignetting. Gain robust KLT feature tracks are used to obtain scene point correspondences as input to a nonlinear optimization framework. We show that our approach can reliably calibrate arbitrary video sequences by evaluating it on datasets for which full photometric ground truth is available. We further show that our calibration can improve the performance of a state-of-theart direct visual odometry method that works solely on pixel intensities, calibrating for photometric parameters in an online fashion in realtime.},
  url      = {https://arxiv.org/pdf/1710.02081.pdf}
}

@inproceedings{bloechliger2018icra,
  author    = {F. Bl{\"o}chliger and M. Fehr and M.T. Dymczyk and T. Schneider and R. Siegwart},
  title     = {{Topomap: Topological Mapping and Navigation Based on Visual SLAM Maps}},
  booktitle = icra,
  year      = 2018,
  keywords  = {Localization, Mapping, Motion and Path Planning},
  abstract  = {Visual robot navigation within large-scale, semistructured environments deals with various challenges such as computation intensive path planning algorithms or insufficient knowledge about traversable spaces. Moreover, many stateof-the-art navigation approaches only operate locally instead of gaining a more conceptual understanding of the planning objective. This limits the complexity of tasks a robot can accomplish and makes it harder to deal with uncertainties that are present in the context of real-time robotics applications. In this work, we present Topomap, a framework which simplifies the navigation task by providing a map to the robot which is tailored for path planning use. This novel approach transforms a sparse feature-based map from a visual Simultaneous Localization And Mapping (SLAM) system into a three-dimensional topological map. This is done in two steps. First, we extract occupancy information directly from the noisy sparse point cloud. Then, we create a set of convex free-space clusters, which are the vertices of the topological map. We show that this representation improves the efficiency of global planning, and we provide a complete derivation of our algorithm. Planning experiments on real world datasets demonstrate that we achieve similar performance as RRT* with significantly lower computation times and storage requirements. Finally, we test our algorithm on a mobile robotic platform to prove its advantages.}
}

@article{bloesch2018ral,
  author   = {M. Bloesch and M. Burri and H. Sommer and R. Siegwart and M. Hutter},
  title    = {{The Two-State Implicit Filter - Recursive Estimation for Mobile Robots}},
  journal  = ral,
  volume   = {3},
  number   = {1},
  pages    = {573--580},
  year     = 2018,
  keywords = {Sensor Fusion, Probability and Statistical Methods, Field Robots},
  abstract = {This paper deals with recursive filtering for dynamic systems where an explicit process model is not easily devisable. Most Bayesian filters assume the availability of such an explicit process model and thus may require additional assumptions or fail to properly leverage all available information. In contrast, we propose a filter which employs a purely residual based modeling of the available information and thus achieves higher modeling flexibility. While this work is related to the descriptor Kalman filter, it also represents a step towards batch optimization and allows the integration of further techniques such as robust weighting for outlier rejection. We derive recursive filter equations which exhibit similar computational complexity when compared to their Kalman filter counterpart the extended information filter. The applicability of the proposed approach is experimentally confirmed on two different real mobile robotic state estimation problems.},
  url      = {https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/242919/bloesch2018ral.pdf?sequence=1&isAllowed=y}
}

@inproceedings{bodin2018icra,
  author    = {B. Bodin and H. Wagstaff and S. Saeedi and L. Nardi and E. Vespa and J. Mawer and A. Nisbet and M. Lujn and S. Furber and A.J. Davison and P.H.J. Kelly and M.F.P. OBoyle},
  title     = {{SLAMBench2: Multi-Objective Head-To-Head Benchmarking for Visual SLAM}},
  booktitle = icra,
  year      = 2018,
  keywords  = {Performance Evaluation and Benchmarking, SLAM},
  abstract  = {SLAM is becoming a key component of robotics and augmented reality (AR) systems. While a large number of SLAM algorithms have been presented, there has been little effort to unify the interface of such algorithms, or to perform a holistic comparison of their capabilities. This is a problem since different SLAM applications can have different functional and non-functional requirements. For example, a mobile phonebased AR application has a tight energy budget, while a UAV navigation system usually requires high accuracy. SLAMBench2 is a benchmarking framework to evaluate existing and future SLAM systems, both open and close source, over an extensible list of datasets, while using a comparable and clearly specified list of performance metrics. A wide variety of existing SLAM algorithms and datasets is supported, e.g. ElasticFusion, InfiniTAM, ORB-SLAM2, OKVIS, and integrating new ones is straightforward and clearly specified by the framework. SLAMBench2 is a publicly-available software framework which represents a starting point for quantitative, comparable and validatable experimental research to investigate trade-offs across SLAM systems.},
  url       = {https://arxiv.org/pdf/1808.06820.pdf}
}

@inproceedings{border2018icra,
  author    = {R. Border and J.D. Gammell and P. Newman},
  title     = {{Surface Edge Explorer (SEE): Planning Next Best Views Directly from 3D Observations}},
  booktitle = icra,
  year      = 2018,
  keywords  = {Computer Vision for Automation, Computer Vision for Other Robotic Applications},
  abstract  = {Surveying 3D scenes is a common task in robotics. Systems can do so autonomously by iteratively obtaining measurements. This process of planning observations to improve the model of a scene is called Next Best View (NBV) planning. NBV planning approaches often use either volumetric (e.g., voxel grids) or surface (e.g., triangulated meshes) representations. Volumetric approaches generalise well between scenes as they do not depend on surface geometry but do not scale to high-resolution models of large scenes. Surface representations can obtain high-resolution models at any scale but often require tuning of unintuitive parameters or multiple survey stages. This paper presents a scene-model-free NBV planning approach with a density representation. The Surface Edge Explorer (SEE) uses the density of current measurements to detect and explore observed surface boundaries. This approach is shown experimentally to provide better surface coverage in lower computation time than the evaluated state-of-the-art volumetric approaches while moving equivalent distances.},
  url       = {https://arxiv.org/pdf/1802.08617.pdf}
}

@inproceedings{brucker2018icra,
  author    = {M. Brucker and M. Durner and R. Ambrus and Z. Marton and A.J. Wendt and P. Jensfelt and K.O. Arras and R. Triebel},
  title     = {{Semantic Labeling of Indoor Environments from 3D RGB Maps}},
  booktitle = icra,
  year      = 2018,
  keywords  = {Semantic Scene Understanding, Mapping, Deep Learning in Robotics and Automation},
  abstract  = {We present an approach to automatically assign semantic labels to rooms reconstructed from 3D RGB maps of apartments. Evidence for the room types is generated using state-of-the-art deep-learning techniques for scene classification and object detection based on automatically generated virtual RGB views, as well as from a geometric analysis of the maps 3D structure. The evidence is merged in a conditional random field, using statistics mined from different datasets of indoor environments. We evaluate our approach qualitatively and quantitatively and compare it to related methods.}
}

@inproceedings{bruls2018icra,
  author    = {T. Bruls and W. Maddern and A. Morye and P. Newman},
  title     = {{Mark Yourself: Road Marking Segmentation via Weakly-Supervised Annotations from Multimodal Data}},
  booktitle = icra,
  year      = 2018,
  keywords  = {Semantic Scene Understanding, Deep Learning in Robotics and Automation, Autonomous Vehicle Navigation},
  abstract  = {This paper presents a weakly-supervised learning system for real-time road marking detection using images of complex urban environments obtained from a monocular camera. We avoid expensive manual labelling by exploiting additional sensor modalities to generate large quantities of annotated images in a weakly-supervised way, which are then used to train a deep semantic segmentation network. At run time, the road markings in the scene are detected in real time in a variety of traffic situations and under different lighting and weather conditions without relying on any preprocessing steps or predefined models. We achieve reliable qualitative performance on the Oxford RobotCar dataset, and demonstrate quantitatively on the CamVid dataset that exploiting these annotations significantly reduces the required labelling effort and improves performance.},
  url       = {https://www.ori.ox.ac.uk/media/5670/2018icra_bruls.pdf}
}

@article{carlone2018ral,
  author   = {L. Carlone and G. Calafiore},
  title    = {{Convex Relaxations for Pose Graph Optimization with Outliers}},
  journal  = ral,
  volume   = {3},
  number   = {2},
  pages    = {1160--1167},
  year     = 2018,
  keywords = {SLAM, Localization, Sensor Fusion},
  abstract = {Pose Graph Optimization consists in the estimation of a set of poses from pairwise measurements and provides a formalization for many problems arising in mobile robotics and geometric computer vision. In this paper, we consider 2D pose estimation problems in which a subset of the measurements is spurious. Our first contribution is to develop robust estimators that can cope with heavy-tailed measurement noise, hence increasing robustness to the presence of outliers. Since the resulting estimators require solving nonconvex optimization problems, we further develop convex relaxations that approximately solve those problems via semidefinite programming. We then provide conditions for testing the exactness of the proposed relaxations. Contrary to existing approaches, our convex relaxations do not rely on the availability of an initial guess for the unknown poses, hence they are more suitable for setups in which such guess is not available (e.g., multi robot localization, recovery after localization failure). We tested the proposed techniques in extensive simulations, and we show that some of the proposed relaxations are indeed tight (i.e., they solve the original nonconvex problem exactly) and ensure accurate estimation in the face of a large number of outliers.},
  url      = {https://arxiv.org/pdf/1801.02112.pdf}
}

@inproceedings{cen2018icra,
  author    = {S.H. Cen and P. Newman},
  title     = {{Precise Ego-Motion Estimation with Millimeter-Wave Radar under Diverse and Challenging Conditions}},
  booktitle = icra,
  year      = 2018,
  keywords  = {Intelligent Transportation Systems, Range Sensing, Autonomous Vehicle Navigation},
  abstract  = {In contrast to cameras, lidars, GPS, and proprioceptive sensors, radars are affordable and efficient systems that operate well under variable weather and lighting conditions, require no external infrastructure, and detect long-range objects. In this paper, we present a reliable and accurate radar-only motion estimation algorithm for mobile autonomous systems. Using a frequency-modulated continuous-wave (FMCW) scanning radar, we first extract landmarks with an algorithm that accounts for unwanted effects in radar returns. To estimate relative motion, we then perform scan matching by greedily adding point correspondences based on unary descriptors and pairwise compatibility scores. Our radar odometry results are robust under a variety of conditions, including those under which visual odometry and GPS/INS fail.},
  url       = {https://www.ori.ox.ac.uk/media/5535/2018icra_cen.pdf}
}

@inproceedings{cieslewski2018icra,
  author    = {T. Cieslewski and S. Choudhary and D. Scaramuzza},
  title     = {{Data-Efficient Decentralized Visual SLAM}},
  booktitle = icra,
  year      = 2018,
  keywords  = {SLAM, Multi-Robot Systems, Visual-Based Navigation},
  abstract  = {Decentralized visual simultaneous localization and mapping (SLAM) is a powerful tool for multi-robot applications in environments where absolute positioning is not available. Being visual, it relies on cheap, lightweight and versatile cameras, and, being decentralized, it does not rely on communication to a central entity. In this work, we integrate state-of-theart decentralized SLAM components into a new, complete decentralized visual SLAM system. To allow for data association and optimization, existing decentralized visual SLAM systems exchange the full map data among all robots, incurring large data transfers at a complexity that scales quadratically with the robot count. In contrast, our method performs efficient data association in two stages: first, a compact full-image descriptor is deterministically sent to only one robot. Then, only if the first stage succeeded, the data required for relative pose estimation is sent, again to only one robot. Thus, data association scales linearly with the robot count and uses highly compact place representations. For optimization, a state-of-theart decentralized pose-graph optimization method is used. It exchanges a minimum amount of data which is linear with trajectory overlap. We characterize the resulting system and identify bottlenecks in its components. The system is evaluated on publicly available datasets and we provide open access to the code.},
  url       = {https://arxiv.org/pdf/1710.05772.pdf}
}

@article{clement2018ral,
  author   = {L. Clement and J. Kelly},
  title    = {{How to Train a CAT: Learning Canonical Appearance Transformations for Direct Visual Localization Under Illumination Change}},
  journal  = ral,
  volume   = {3},
  number   = {2},
  pages    = {2447--2454},
  year     = 2018,
  keywords = {Visual Learning, Visual-Based Navigation, Localization},
  abstract = {Direct visual localization has recently enjoyed a resurgence in popularity with the increasing availability of cheap mobile computing power. The competitive accuracy and robustness of these algorithms compared to state-of-the-art feature-based methods, as well as their natural ability to yield dense maps, makes them an appealing choice for a variety of mobile robotics applications. However, direct methods remain brittle in the face of appearance change due to their underlying assumption of photometric consistency, which is commonly violated in practice. In this paper, we propose to mitigate this problem by training deep convolutional encoder-decoder models to transform images of a scene such that they correspond to a previously-seen canonical appearance. We validate our method in multiple environments and illumination conditions using high-fidelity synthetic RGB-D datasets, and integrate the trained models into a direct visual localization pipeline, yielding improvements in visual odometry (VO) accuracy through timevarying illumination conditions, as well as improved metric relocalization performance under illumination change, where conventional methods normally fail. We further provide a preliminary investigation of transfer learning from synthetic to real environments in a localization context.},
  url      = {https://arxiv.org/pdf/1709.03009.pdf}
}

@inproceedings{cop2018icra,
  author    = {K.P. Cop and P.V.K. Borges and R. Dub},
  title     = {{DELIGHT: An Efficient Descriptor for Global Localisation Using LiDAR Intensities}},
  booktitle = icra,
  year      = 2018,
  keywords  = {Localization, Field Robots, Autonomous Vehicle Navigation},
  abstract  = {Place recognition is a key element of mobile robotics. It can assist with the wake-up and kidnapped robot problems, where the robot position needs to be estimated without prior information. Among the different sensors that can be used for the task (e.g., camera, GPS, LiDAR), LiDAR has the advantage of operating in the dark and in GPSdenied areas. We propose a new method that uses solely the LiDAR data and that can be performed without robot motion. In contrast to other methods, our system leverages intensity information (as opposed to only range information) which is encoded into a novel descriptor of LiDAR intensities as a group of histograms, named DELIGHT. The descriptor encodes the distributed histograms of intensity of the surroundings which are compared using chi-squared tests. Our pipeline is a twostage solution consisting of an intensity-based prior estimation and a geometry-based verification. For a map of 220k square meters, the method achieves localisation in around 3s with a success rate of 97\%, illustrating the applicability of the method in real environments.},
  url       = {https://research.csiro.au/robotics/wp-content/uploads/sites/96/2018/04/DELIGHT-8-002.pdf}
}

@article{costante2018ral,
  author   = {G. Costante and T.A. Ciarfuglia},
  title    = {{LS-VO: Learning Dense Optical Subspace for Robust Visual Odometry Estimation}},
  journal  = ral,
  volume   = {3},
  number   = {3},
  pages    = {1735--1742},
  year     = 2018,
  keywords = {Visual Learning, Deep Learning in Robotics and Automation},
  abstract = {This work proposes a novel deep network architecture to solve the camera Ego-Motion estimation problem. A motion estimation network generally learns features similar to Optical Flow (OF) fields starting from sequences of images. This OF can be described by a lower dimensional latent space. Previous research has shown how to find linear approximations of this space. We propose to use an Auto-Encoder network to find a non-linear representation of the OF manifold. In addition, we propose to learn the latent space jointly with the estimation task, so that the learned OF features become a more robust description of the OF input. We call this novel architecture Latent Space Visual Odometry (LS-VO). The experiments show that LS-VO achieves a considerable increase in performances with respect to baselines, while the number of parameters of the estimation network only slightly increases.},
  url      = {https://arxiv.org/pdf/1709.06019.pdf}
}

@inproceedings{delmerico2018icra,
  author    = {J. Delmerico and D. Scaramuzza},
  title     = {{A Benchmark Comparison of Monocular Visual-Inertial Odometry Algorithms for Flying Robots}},
  booktitle = icra,
  year      = 2018,
  keywords  = {SLAM, Visual-Based Navigation, Performance Evaluation and Benchmarking},
  abstract  = {Flying robots require a combination of accuracy and low latency in their state estimation in order to achieve stable and robust flight. However, due to the power and payload constraints of aerial platforms, state estimation algorithms must provide these qualities under the computational constraints of embedded hardware. Cameras and inertial measurement units (IMUs) satisfy these power and payload constraints, so visualinertial odometry (VIO) algorithms are popular choices for state estimation in these scenarios, in addition to their ability to operate without external localization from motion capture or global positioning systems. It is not clear from existing results in the literature, however, which VIO algorithms perform well under the accuracy, latency, and computational constraints of a flying robot with onboard state estimation. This paper evaluates an array of publicly-available VIO pipelines (MSCKF, OKVIS, ROVIO, VINS-Mono, SVO+MSF, and SVO+GTSAM) on different hardware configurations, including several singleboard computer systems that are typically found on flying robots. The evaluation considers the pose estimation accuracy, per-frame processing time, and CPU and memory load while processing the EuRoC datasets, which contain six degree of freedom (6DoF) trajectories typical of flying robots. We present our complete results as a benchmark for the research community.},
  url       = {https://rpg.ifi.uzh.ch/docs/ICRA18_Delmerico.pdf}
}

@inproceedings{deschaud2018icra,
  author    = {J. Deschaud},
  title     = {{IMLS-SLAM: scan-to-model matching based on 3D data}},
  booktitle = icra,
  year      = 2018,
  keywords  = {SLAM, Range Sensing, Localization},
  abstract  = {The Simultaneous Localization And Mapping (SLAM) problem has been well studied in the robotics community, especially using mono, stereo cameras or depth sensors. 3D depth sensors, such as Velodyne LiDAR, have proved in the last 10 years to be very useful to perceive the environment in autonomous driving, but few methods exist that directly use these 3D data for odometry. We present a new low-drift SLAM algorithm based only on 3D LiDAR data. Our method relies on a scan-to-model matching framework. We first have a specific sampling strategy based on the LiDAR scans. We then define our model as the previous localized LiDAR sweeps and use the Implicit Moving Least Squares (IMLS) surface representation. We show experiments with the Velodyne HDL32 with only 0.40\% drift over a 4 km acquisition without any loop closure (i.e., 16 m drift after 4 km). We tested our solution on the KITTI benchmark with a Velodyne HDL64 and ranked among the best methods (against mono, stereo and LiDAR methods) with a global drift of only 0.69\%.},
  url       = {https://arxiv.org/pdf/1802.08633.pdf}
}

@inproceedings{dong2018icra,
  author    = {W. Dong and J. Shi and W. Tang and X. Wang and H. Zha},
  title     = {{An Efficient Volumetric Mesh Representation for Real-Time Scene Reconstruction using Spatial Hashing}},
  booktitle = icra,
  year      = 2018,
  keywords  = {Mapping, Computational Geometry, RGB-D Perception},
  abstract  = {Mesh plays an indispensable role in dense realtime reconstruction essential in robotics. Efforts have been made to maintain flexible data structures for 3D data fusion, yet an efficient incremental framework specifically designed for online mesh storage and manipulation is missing. We propose a novel framework to compactly generate, update, and refine mesh for scene reconstruction upon a volumetric representation. Maintaining a spatial-hashed field of cubes, we distribute vertices with continuous value on discrete edges that support O(1) vertex accessing and forbid memory redundancy. By introducing Hamming distance in mesh refinement, we further improve the mesh quality regarding the triangle type consistency with a low cost. Lock-based and lock-free operations were applied to avoid thread conflicts in GPU parallel computation. Experiments demonstrate that the mesh memory consumption is significantly reduced while the running speed is kept in the online reconstruction process.},
  url       = {https://arxiv.org/pdf/1803.03949.pdf}
}

@inproceedings{dong2018icra-sgpo,
  author    = {J. Dong and M. Mukadam and B. Boots and F. Dellaert},
  title     = {{Sparse Gaussian Processes on Matrix Lie Groups: A Unified Framework for Optimizing Continuous-Time Trajectories}},
  booktitle = icra,
  year      = 2018,
  keywords  = {SLAM, Probability and Statistical Methods, Motion and Path Planning},
  abstract  = {Continuous-time trajectories are useful for reasoning about robot motion in a wide range of tasks. Sparse Gaussian processes (GPs) can be used as a non-parametric representation for trajectory distributions that enables fast trajectory optimization by sparse GP regression. However, most previous approaches that utilize sparse GPs for trajectory optimization are limited by the fact that the robot state is represented in vector space. In this paper, we first extend previous works to consider the state on general matrix Lie groups by applying a constant-velocity prior and defining locally linear GPs. Next, we discuss how sparse GPs on Lie groups provide a unified continuous-time framework for trajectory optimization for solving a number of robotics problems including state estimation and motion planning. Finally, we demonstrate and evaluate our approach on several different estimation and motion planning tasks with both synthetic and real-world experiments.},
  url       = {https://homes.cs.washington.edu/~bboots/files/Sparse_GP_Lie_Group_SLAM.pdf}
}

@inproceedings{droeschel2018icra,
  author    = {D. Droeschel and S. Behnke},
  title     = {{Efficient Continuous-Time SLAM for 3D Lidar-Based Online Mapping}},
  booktitle = icra,
  year      = 2018,
  keywords  = {Mapping, SLAM},
  abstract  = {Modern 3D laser-range scanners have a high data rate, making online simultaneous localization and mapping (SLAM) computationally challenging. Recursive state estimation techniques are efficient but commit to a state estimate immediately after a new scan is made, which may lead to misalignments of measurements. We present a 3D SLAM approach that allows for refining alignments during online mapping. Our method is based on efficient local mapping and a hierarchical optimization back-end. Measurements of a 3D laser scanner are aggregated in local multiresolution maps by means of surfel-based registration. The local maps are used in a multi-level graph for allocentric mapping and localization. In order to incorporate corrections when refining the alignment, the individual 3D scans in the local map are modeled as a sub-graph and graph optimization is performed to account for drift and misalignments in the local maps. Furthermore, in each sub-graph, a continuous-time representation of the sensor trajectory allows to correct measurements between scan poses. We evaluate our approach in multiple experiments by showing qualitative results. Furthermore, we quantify the map quality by an entropy-based measure.},
  url       = {https://arxiv.org/pdf/1810.06802.pdf}
}

@inproceedings{du2018icra,
  author    = {X. DU and M.H.A. Jr and S. Karaman and D. Rus},
  title     = {{A General Pipeline for 3D Detection of Vehicles}},
  booktitle = icra,
  year      = 2018,
  keywords  = {Deep Learning in Robotics and Automation, Object Detection, Segmentation and Categorization, Intelligent Transportation Systems},
  abstract  = {Autonomous driving requires 3D perception of vehicles and other objects in the in environment. Much of the current methods support 2D vehicle detection. This paper proposes a flexible pipeline to adopt any 2D detection network and fuse it with a 3D point cloud to generate 3D information with minimum changes of the 2D detection networks. To identify the 3D box, an effective model fitting algorithm is developed based on generalised car models and score maps. A two-stage convolutional neural network (CNN) is proposed to refine the detected 3D box. This pipeline is tested on the KITTI dataset using two different 2D detection networks. The 3D detection results based on these two networks are similar, demonstrating the flexibility of the proposed pipeline. The results rank second among the 3D detection algorithms, indicating its competencies in 3D detection.},
  url       = {https://arxiv.org/pdf/1803.00387.pdf}
}

@inproceedings{kundu2018icra,
  author    = {T. Kundu and I. Saha},
  title     = {{Charging Station Placement for Indoor Robotic Applcations}},
  booktitle = icra,
  year      = 2018,
  keywords  = {Motion and Path Planning, Formal Methods in Robotics and Automation, Factory Automation},
  abstract  = {For an autonomous mobile robot, when the available power goes below a certain threshold, the robot needs to abort its current task and move towards a charging station to recharge its battery. The efficiency of an autonomous mobile robot depends significantly on the location of the charging stations. In this paper, we address the charging station placement problem for mobile robots in a controlled workspace. We propose two algorithms to place a number of charging stations so that a robot is always capable of reaching one of the charging stations from any obstacle-free location in the workspace without aborting its task too early. We reduce the charging-station placement problem to a series of Satisfiability Modulo Theory (SMT) problems and use the off-the-shelf SMT solver Z3 to implement our algorithm. The algorithm produces as output the locations of the charging stations in the workspace and the trajectories from any obstacle-free locations to one of the charging stations. Our experimental results show how our algorithm can efficiently find the locations of the charging stations and robot trajectories to reach the charging stations. We demonstrate through simulation how the generated trajectories can be effectively used by a robot to reach a charging stations autonomously without getting depleted with power.},
  url       = {https://www.cse.iitk.ac.in/users/isaha/Publications/Conferences/ICRA18.pdf/}
}

@article{dube2018ral,
  author   = {R. Dube and M.G. Gollub and H. Sommer and I. Gilitschenski and R. Siegwart and C.Cadena and J. Nieto},
  title    = {{Incremental Segment-Based Localization in 3D Point Clouds}},
  journal  = ral,
  volume   = {3},
  number   = {2},
  pages    = {1832--1839},
  year     = 2018,
  keywords = {Localization, Recognition, SLAM},
  abstract = {Localization in 3D point clouds is a highly challenging task due to the complexity associated with extracting information from 3D data. This paper proposes an incremental approach addressing this problem efficiently. The presented method first accumulates the measurements in a dynamic voxel grid and selectively updates the point normals affected by the insertion. An incremental segmentation algorithm, based on region growing, tracks the evolution of single segments which enables an efficient recognition strategy using partitioning and caching of geometric consistencies. We show that the incremental method can perform global localization at 10Hz in a urban driving environment, a speedup of x7.1 over the compared batch solution. The efficiency of the method makes it suitable for applications where realtime localization is required and enables its usage on cheaper, lowenergy systems. Our implementation is available open source along with instructions for running the system.},
  url      = {https://n.ethz.ch/~cesarc/files/RAL2018_rdube.pdf}
}

@article{faigl2018ral,
  author   = {J. Faigl and P. Vana},
  title    = {{Surveillance Planning with Bezier Curves}},
  journal  = ral,
  volume   = {3},
  number   = {2},
  pages    = {750--757},
  year     = 2018,
  keywords = {Motion and Path Planning, Nonholonomic Motion Planning, Aerial Systems: Applications},
  abstract = {This paper concerns surveillance planning for an Unmanned Aerial Vehicle (UAV) that is requested to periodically take snapshots of areas of interest by visiting a given set of waypoint locations in the shortest time possible. The studied problem can be considered as a variant of the combinatorial traveling salesman problem in which trajectories between the waypoints respect the kinematic constraints of the UAV. Contrary to the existing formulation for curvature-constrained vehicles known as the Dubins traveling salesman problem, the herein addressed problem is motivated by planning for multirotor UAVs which are not limited by the minimal required forward velocity and minimal turning radius as the Dubins vehicle, but rather by the maximal speed and acceleration. Moreover, the waypoints to be visited can be at different altitudes, and the addressed problem is to find a fast and smooth trajectory in 3D space from which all the areas of interest can be captured. The proposed solution is based on unsupervised learning in which the requested 3D smooth trajectory is determined as a sequence of Bezier curves in a finite number of learning epochs. The reported results support feasibility of the proposed solution which has also been experimentally verified with a real UAV.},
  url      = {https://comrob.fel.cvut.cz/papers/ral18bezier.pdf}
}

@inproceedings{florence2018icra,
  author    = {P. Florence and J. Carter and J. Ware and R. Tedrake},
  title     = {{NanoMap: Fast, Uncertainty-Aware Proximity Queries with Lazy Search Over Local 3D Data}},
  booktitle = icra,
  year      = 2018,
  keywords  = {Collision Avoidance, Reactive and Sensor-Based Planning, Aerial Systems: Perception and Autonomy},
  abstract  = {We would like robots to be able to safely navigate at high speed, efficiently use local 3D information, and robustly plan motions that consider pose uncertainty of measurements in a local map structure. This is hard to do with previously existing mapping approaches, like occupancy grids, that are focused on incrementally fusing 3D data into a common world frame. In particular, both their fragile sensitivity to state estimation errors and computational cost can be limiting. We develop an alternative framework, NanoMap, which alleviates the need for global map fusion and enables a motion planner to efficiently query pose-uncertainty-aware local 3D geometric information. The key idea of NanoMap is to store a history of noisy relative pose transforms and search over a corresponding set of depth sensor measurements for the minimum-uncertainty view of a queried point in space. This approach affords a variety of capabilities not offered by traditional mapping techniques: (a) the pose uncertainty associated with 3D data can be incorporated in motion planning, (b) poses can be updated (i.e., from loop closures) with minimal computational effort, and (c) 3D data can be fused lazily for the purpose of planning. We provide an open-source implementation of NanoMap, and analyze its capabilities and computational efficiency in simulation experiments. Finally, we demonstrate in hardware its effectiveness for fast 3D obstacle avoidance onboard a quadrotor flying up to 10 m/s.},
  url       = {https://arxiv.org/pdf/1802.09076.pdf}
}

@inproceedings{garg2018icra,
  author    = {S. Garg and N. Snderhauf and M.J. Milford},
  title     = {{Don't Look Back: Robustifying Place Categorization for Viewpoint and Condition-Invariant Place Recognition}},
  booktitle = icra,
  year      = 2018,
  keywords  = {Localization, Semantic Scene Understanding, Mapping},
  abstract  = {When a human drives a car along a road for the first time, they later recognize where they are on the return journey typically without needing to look in their rear view mirror or turn around to look back, despite significant viewpoint and appearance change. Such navigation capabilities are typically attributed to our semantic visual understanding of the environment [1] beyond geometry to recognizing the types of places we are passing through such as passing a shop on the left or moving through a forested area. Humans are in effect using place categorization [2] to perform specific place recognition even when the viewpoint is 180 degrees reversed. Recent advances in deep neural networks have enabled high performance semantic understanding of visual places and scenes, opening up the possibility of emulating what humans do. In this work, we develop a novel methodology for using the semantics-aware higher-order layers of deep neural networks for recognizing specific places from within a reference database. To further improve the robustness to appearance change, we develop a descriptor normalization scheme that builds on the success of normalization schemes for pure appearance-based techniques such as SeqSLAM [3]. Using two different datasets one road-based, one pedestrian-based, we evaluate the performance of the system in performing place recognition on reverse traversals of a route with a limited field of view camera and no turn-back-and-look behaviours, and compare to existing stateof-the-art techniques and vanilla off-the-shelf features. The results demonstrate significant improvements over the existing state of the art, especially for extreme perceptual challenges that involve both great viewpoint change and environmental appearance change. We also provide experimental analyses of the contributions of the various system components: the use of spatio-temporal sequences, place categorization and placecentric characteristics as opposed to object-centric semantics.},
  url       = {proceedings: garg2018icra.pdf}
}

@inproceedings{gentil2018icra,
  author    = {C.L. Gentil and T.A. Vidal-Calleja and S. Huang},
  title     = {{3D Lidar-IMU Calibration Based on Upsampled Preintegrated Measurements for Motion Distortion Correction}},
  booktitle = icra,
  year      = 2018,
  keywords  = {Calibration and Identification, Localization, Autonomous Vehicle Navigation},
  abstract  = {In this paper, we present a probabilistic framework to recover the extrinsic calibration parameters of a lidarIMU sensing system. Unlike global-shutter cameras, lidars do not take single snapshots of the environment. Instead, lidars collect a succession of 3D-points generally grouped in scans. If these points are assumed to be expressed in a common frame, this becomes an issue when the sensor moves rapidly in the environment causing motion distortion. The fundamental idea of our proposed framework is to use preintegration over interpolated inertial measurements to characterise the motion distortion in each lidar scan. Moreover, by using a set of planes as a calibration target, the proposed method makes use of lidar point-to-plane distances to jointly calibrate and localise the system using on-manifold optimisation. The calibration does not rely on a predefined target as arbitrary planes are detected and modelled in the first lidar scan. Simulated and real data are used to show the effectiveness of the proposed method.}
}

@inproceedings{gomez-ojeda2018icra,
  author    = {R. Gomez-Ojeda and Z. Zhang and J. Gonzlez-Jimnez and D. Scaramuzza},
  title     = {{Learning-Based Image Enhancement for Visual Odometry in Challenging HDR Environments}},
  booktitle = icra,
  year      = 2018,
  keywords  = {Visual Learning},
  abstract  = {One of the main open challenges in visual odometry (VO) is the robustness to difficult illumination conditions or high dynamic range (HDR) environments. The main difficulties in these situations come from both the limitations of the sensors and the inability to perform a successful tracking of interest points because of the bold assumptions in VO, such as brightness constancy. We address this problem from a deep learning perspective, for which we first fine-tune a deep neural network with the purpose of obtaining enhanced representations of the sequences for VO. Then, we demonstrate how the insertion of long short term memory allows us to obtain temporally consistent sequences, as the estimation depends on previous states. However, the use of very deep networks enlarges the computational burden of the VO framework; therefore, we also propose a convolutional neural network of reduced size capable of performing faster. Finally, we validate the enhanced representations by evaluating the sequences produced by the two architectures in several state-of-art VO algorithms, such as ORB-SLAM and DSO.},
  url       = {https://arxiv.org/pdf/1707.01274.pdf}
}

@article{gordon2018ral,
  author   = {D. Gordon and A. Farhadi and D. Fox},
  title    = {{Re3: Real-Time Recurrent Regression Networks for Visual Tracking of Generic Objects}},
  journal  = ral,
  volume   = {3},
  number   = {2},
  pages    = {788--795},
  year     = 2018,
  keywords = {Visual Tracking, Deep Learning in Robotics and Automation, Visual Learning},
  abstract = {Robust object tracking requires knowledge and understanding of the object being tracked: its appearance, its motion, and how it changes over time. A tracker must be able to modify its underlying model and adapt to new observations. We present Re3 , a real-time deep object tracker capable of incorporating temporal information into its model. Rather than focusing on a limited set of objects or training a model at test-time to track a specific instance, we pretrain our generic tracker on a large variety of objects and efficiently update on the fly; Re3 simultaneously tracks and updates the appearance model with a single forward pass. This lightweight model is capable of tracking objects at 150 FPS, while attaining competitive results on challenging benchmarks. We also show that our method handles temporary occlusion better than other comparable trackers using experiments that directly measure performance on sequences with occlusion.},
  url      = {https://arxiv.org/pdf/1705.06368.pdf}
}

@inproceedings{heiden2018icra,
  author    = {E. Heiden and L. Palmieri and S. Koenig and K.O. Arras and G. Sukhatme},
  title     = {{Gradient-Informed Path Smoothing for Wheeled Mobile Robots}},
  booktitle = icra,
  year      = 2018,
  keywords  = {Motion and Path Planning, Nonholonomic Motion Planning, AI-Based Methods},
  abstract  = {Planning smooth trajectories is important for the safe, efficient and comfortable operation of mobile robots, such as wheeled robots moving in crowded environments or cars moving at high speed. Asymptotically optimal sampling-based motion planners can be used to generate such trajectories. However, to achieve the necessary efficiency for the realtime operation of robots, one often uses their initial feasible trajectories or the trajectories of non-optimal motion planners instead, typically after a post-smoothing step. We propose a gradient-informed post-smoothing algorithm, called GRIPS, that deforms given trajectories by locally optimizing the placement of vertices while satisfying the systems kinodynamic constraints. We show experimentally that GRIPS typically produces trajectories of significantly smaller length and higher smoothness than several existing post-smoothing algorithms.},
  url       = {http://iliad-project.eu/wp-content/uploads/2018/03/heidenPalmieriICRA2018.pdf}
}

@inproceedings{hoermann2018icra,
  author    = {S. Hoermann and M. Bach and K. Dietmayer},
  title     = {{Dynamic Occupancy Grid Prediction for Urban Autonomous Driving: A Deep Learning Approach with Fully Automatic Labeling}},
  booktitle = icra,
  year      = 2018,
  keywords  = {Autonomous Vehicle Navigation, Deep Learning in Robotics and Automation, Sensor Fusion},
  abstract  = {Long-term situation prediction plays a crucial role for intelligent vehicles. A major challenge still to overcome is the prediction of complex downtown scenarios with multiple road users, e.g., pedestrians, bikes, and motor vehicles, interacting with each other. This contribution tackles this challenge by combining a Bayesian filtering technique for environment representation, and machine learning as long-term predictor. More specifically, a dynamic occupancy grid map is utilized as input to a deep convolutional neural network. This yields the advantage of using spatially distributed velocity estimates from a single time step for prediction, rather than a raw data sequence, alleviating common problems dealing with input time series of multiple sensors. Furthermore, convolutional neural networks have the inherent characteristic of using context information, enabling the implicit modeling of road user interaction. Pixelwise balancing is applied in the loss function counteracting the extreme imbalance between static and dynamic cells. One of the major advantages is the unsupervised learning character due to fully automatic label generation. The presented algorithm is trained and evaluated on multiple hours of recorded sensor data and compared to Monte-Carlo simulation. Experiments show the ability to model complex interactions.},
  url       = {https://arxiv.org/pdf/1705.08781.pdf}
}

@inproceedings{hsiao2018icra,
  author    = {M. Hsiao and E. Westman and M. Kaess},
  title     = {{Dense Planar-Inertial SLAM with Structural Constraints}},
  booktitle = icra,
  year      = 2018,
  keywords  = {SLAM, Mapping, Sensor Fusion},
  abstract  = {In this work, we develop a novel dense planarinertial SLAM (DPI-SLAM) system to reconstruct dense 3D models of large indoor environments using a hand-held RGB-D sensor and an inertial measurement unit (IMU). The preintegrated IMU measurements are loosely-coupled with the dense visual odometry (VO) estimation and tightly-coupled with the planar measurements in a full SLAM framework. The poses, velocities, and IMU biases are optimized together with the planar landmarks in a global factor graph using incremental smoothing and mapping with the Bayes Tree (iSAM2). With odometry estimation using both RGB-D and IMU data, our system can keep track of the poses of the sensors even without sufficient planes or visual information (e.g. textureless walls) temporarily. Modeling planes and IMU states in the fully probabilistic global optimization reduces the drift that distorts the reconstruction results of other SLAM algorithms. Moreover, structural constraints between nearby planes (e.g. right angles) are added into the DPI-SLAM system, which further recovers the drift and distortion. We test our DPI-SLAM on large indoor datasets and demonstrate its state-of-the-art performance as the first planar-inertial SLAM system.},
  url       = {https://www.cs.cmu.edu/~kaess/pub/Hsiao18icra.pdf}
}

@inproceedings{huang2018icra-ocfv,
  author    = {H. Huang and T. Wang and J. Lin and C. Hu and K. Zeng and M. Sun},
  title     = {{Omnidirectional CNN for Visual Place Recognition and Navigation}},
  booktitle = icra,
  year      = 2018,
  keywords  = {Localization, Deep Learning in Robotics and Automation, Omnidirectional Vision},
  abstract  = {Visual place recognition is challenging, especially when only a few place exemplars are given. To mitigate the challenge, we consider place recognition method using omnidirectional cameras and propose a novel Omnidirectional Convolutional Neural Network (O-CNN) to handle severe camera pose variation. Given a visual input, the task of the O-CNN is not to retrieve the matched place exemplar, but to retrieve the closest place exemplar and estimate the relative distance between the input and the closest place. With the ability to estimate relative distance, a heuristic policy is proposed to navigate a robot to the retrieved closest place. Note that the network is designed to take advantage of the omnidirectional view by incorporating circular padding and rotation invariance. To train a powerful OCNN, we build a virtual world for training on a large scale. We also propose a continuous lifted structured feature embedding loss to learn the concept of distance efficiently. Finally, our experimental results confirm that our method achieves stateof-the-art accuracy and speed with both the virtual world and real-world datasets.},
  url       = {https://arxiv.org/pdf/1803.04228.pdf}
}

@article{jeon2018flis,
  title   = {Semantic segmentation using trade-off and internal ensemble},
  author  = {Jeon, Wang-Su and Cielniak, Grzegorz and Rhee, Sang-Yong},
  journal = {Intl.~Journal of Fuzzy Logic and Intelligent Systems},
  volume  = {18},
  number  = {3},
  pages   = {196--203},
  year    = {2018},
  url     = {https://www.ijfis.org/journal/download_pdf.php?doi=10.5391/IJFIS.2018.18.3.196}
}

@inproceedings{jeong2018icra,
  author    = {J. Jeong and Y. Cho and Y. Shin and H. Roh and A. Kim},
  title     = {{Complex Urban LiDAR Data Set}},
  booktitle = icra,
  year      = 2018,
  keywords  = {Mapping, Range Sensing, Sensor Fusion},
  abstract  = {This paper presents a Light Detection and Ranging (LiDAR) data set that targets complex urban environments. Urban environments with high-rise buildings and congested traffic pose a significant challenge for many robotics applications. The presented data set is unique in the sense it is able to capture the genuine features of an urban environment (e.g. metropolitan areas, large building complexes and underground parking lots). Data of two-dimensional (2D) and threedimensional (3D) LiDAR, which are typical types of LiDAR sensors, are provided in the data set. The two 16-ray 3D LiDARs are tilted on both sides for maximal coverage. One 2D LiDAR faces backward while the other faces forwards to collect data of roads and buildings, respectively. Raw sensor data from Fiber Optic Gyro (FOG), Inertial Measurement Unit (IMU), and the Global Positioning System (GPS) are presented in a file format for vehicle pose estimation. The pose information of the vehicle estimated at 100 Hz is also presented after applying the graph simultaneous localization and mapping (SLAM) algorithm. For the convenience of development, the file player and data viewer in Robot Operating System (ROS) environment were also released via the web page. The full data sets are available at: http://irap.kaist.ac.kr/dataset. In this website, 3D preview of each data set is provided using WebGL.},
  url       = {https://arxiv.org/pdf/1803.06121.pdf}
}

@inproceedings{jund2018icra,
  author    = {P. Jund and A. Eitel and N. Abdo and W. Burgard},
  title     = {{Optimization Beyond the Convolution: Generalizing Spatial Relations with End-To-End Metric Learning}},
  booktitle = icra,
  year      = 2018,
  keywords  = {Deep Learning in Robotics and Automation, Visual Learning},
  abstract  = {To operate intelligently in domestic environments, robots require the ability to understand arbitrary spatial relations between objects and to generalize them to objects of varying sizes and shapes. In this work, we present a novel end-to-end approach to generalize spatial relations based on distance metric learning. We train a neural network to transform 3D point clouds of objects to a metric space that captures the similarity of the depicted spatial relations, using only geometric models of the objects. Our approach employs gradient-based optimization to compute object poses in order to imitate an arbitrary target relation by reducing the distance to it under the learned metric. Our results based on simulated and real-world experiments show that the proposed method enables robots to generalize spatial relations to unknown objects over a continuous spectrum.},
  url       = {https://arxiv.org/pdf/1707.00893.pdf}
}

@inproceedings{kim2018icra-ecub,
  author    = {J. Kim and Y. Cho and A. Kim},
  title     = {{Exposure Control Using Bayesian Optimization Based on Entropy Weighted Image Gradient}},
  booktitle = icra,
  year      = 2018,
  keywords  = {Computer Vision for Other Robotic Applications, SLAM, Visual-Based Navigation},
  abstract  = {Under- and oversaturation can cause severe image degradation in many vision-based robotic applications. To control camera exposure in dynamic lighting conditions, we introduce a novel metric for image information measure. Measuring an image gradient is typical when evaluating its level of image detail. However, emphasizing more informative pixels substantially improves the measure within an image. By using this entropy weighted image gradient, we introduce an optimal exposure value for vision-based approaches. Using this newly invented metric, we also propose an effective exposure control scheme that covers a wide range of light conditions. When evaluating the function (e.g., image frame grab) is expensive, the next best estimation needs to be carefully considered. Through Bayesian optimization, the algorithm can estimate the optimal exposure value with minimal cost. We validated the proposed image information measure and exposure control scheme via a series of thorough experiments using various exposure conditions.},
  url       = {proceedings:kim2018icra-ecub.pdf}
}

@inproceedings{kim2018icra-lvoi,
  author    = {P. Kim and B. Coltin and H.J. Kim},
  title     = {{Low-Drift Visual Odometry in Structured Environments by Decoupling Rotational and Translational Motion}},
  booktitle = icra,
  year      = 2018,
  keywords  = {Visual-Based Navigation, Computer Vision for Transportation, Autonomous Vehicle Navigation},
  abstract  = {We present a low-drift visual odometry algorithm that separately estimates rotational and translational motion from lines, planes, and points found in RGB-D images. Previous methods estimate drift-free rotational motion from structural regularities to reduce drift in the rotation estimate, which is the primary source of positioning inaccuracy in visual odometry. However, multiple orthogonal planes are required to be visible throughout the entire motion estimation process; otherwise, these VO approaches fail. We propose a new approach to estimate drift-free rotational motion jointly from both lines and planes by exploiting environmental regularities. We track the spatial regularities with an efficient SO(3)-manifold constrained mean shift algorithm. Once the drift-free rotation is found, we recover the translational motion from all tracked points with and without depth by minimizing the de-rotated reprojection error. We compare the proposed algorithm to other stateof-the-art visual odometry methods on a variety of RGB-D datasets (including especially challenging pure rotations) and demonstrate improved accuracy and lower drift error.}
}

@inproceedings{kim2018icra-rlom,
  author    = {J. KIM and W. Chung},
  title     = {{Robust Localization of Mobile Robots Considering Reliability of LiDAR Measurements}},
  booktitle = icra,
  year      = 2018,
  keywords  = {Localization, Range Sensing},
  abstract  = {In this study, we propose a novel Light Detection and Ranging (LiDAR) sensor-based localization method for localization of a mobile robot. In localization using the LiDAR sensor, localization errors occur when real range measurements differ from reference distances computed from a map. This study focuses on three factors that cause differences between real range measurements and reference distances. The first factor corresponds to optical characteristics of the LiDAR sensor for objects such as glass walls and mirrors. The second factor corresponds to occlusions by dynamic obstacles. The third factor corresponds to static changes in the environment. In practical applications, three factors often simultaneously occur. Although there have been many previous works, robust localization to overcome these three difficulties is still a challenging problem. This study proposes a novel robust localization scheme that exploits only reliable range measurements. A LiDAR sensor-based localization scheme can be successfully executed by utilizing only a few reliable range measurements. Therefore, the computation of reliability plays a significant role. The computation of reliability is divided into two steps. The first step considers characteristics of optical sensors. The second step mainly deals with the effects of obstacles. The observation likelihood model exploits computed reliability for pose estimation. The proposed scheme was successfully verified through various experiments under challenging situations.}
}

@inproceedings{latif2018icra,
  author    = {Y. Latif and R. Garg and M.J. Milford and I. Reid},
  title     = {{Addressing Challenging Place Recognition Tasks Using Generative Adversarial Networks}},
  booktitle = icra,
  year      = 2018,
  keywords  = {Localization, Mapping, SLAM},
  abstract  = {Place recognition is an essential component of Simultaneous Localization And Mapping (SLAM). Under severe appearance change, reliable place recognition is a difficult perception task since the same place is perceptually very different in the morning, at night, or over different seasons. This work addresses place recognition as a domain translation task. Using a pair of coupled Generative Adversarial Networks (GANs), we show that it is possible to generate the appearance of one domain (such as summer) from another (such as winter) without requiring image-to-image correspondences across the domains. Mapping between domains is learned from sets of images in each domain without knowing the instanceto-instance correspondence by enforcing a cyclic consistency constraint. In the process, meaningful feature spaces are learned for each domain, the distances in which can be used for the task of place recognition. Experiments show that learned features correspond to visual similarity and can be effectively used for place recognition across seasons.}
}

@inproceedings{liu2018icra-difc,
  author    = {K. Liu and K. Ok and W. Vega-Brown and N. Roy},
  title     = {{Deep Inference for Covariance Estimation: Learning Gaussian Noise Models for State Estimation}},
  booktitle = icra,
  year      = 2018,
  keywords  = {Sensor Fusion, Deep Learning in Robotics and Automation, SLAM},
  abstract  = {We present a novel method of measurement covariance estimation that models measurement uncertainty as a function of the measurement itself. Existing work in predictive sensor modeling outperforms conventional fixed models, but requires domain knowledge of the sensors that heavily influences the accuracy and the computational cost of the models. In this work, we introduce Deep Inference for Covariance Estimation (DICE), which utilizes a deep neural network to predict the covariance of a sensor measurement from raw sensor data. We show that given pairs of raw sensor measurement and groundtruth measurement error, we can learn a representation of the measurement model via supervised regression on the prediction performance of the model, eliminating the need for handcoded features and parametric forms. Our approach is sensoragnostic, and we demonstrate improved covariance prediction on both simulated and real data.}
}

@article{loghmani2019ral,
  author   = {M.R. Loghmani and M. Planamente and B. Caputo and M. Vincze},
  title    = {{Recurrent Convolutional Fusion for RGB-D Object Recognition}},
  journal  = ral,
  year     = 2019,
  volume   = {4},
  number   = {3},
  pages    = {2878--2885},
  keywords = {RGB-D Perception, Recognition, Visual Learning},
  abstract = { Providing robots with the ability to recognize objects like humans has always been one of the primary goals of robot vision. The introduction of RGB-D cameras has paved the way for a significant leap forward in this direction thanks to the rich information provided by these sensors. However, the robot vision community still lacks an effective method to synergically use the RGB and depth data to improve object recognition. In order to take a step in this direction, we introduce a novel end-to-end architecture for RGB-D object recognition called recurrent convolutional fusion (RCFusion). Our method generates compact and highly discriminative multi-modal features by combining RGB and depth information representing different levels of abstraction. Extensive experiments on two popular datasets, RGB-D Object Dataset and JHUIT-50, show that RCFusion significantly outperforms state-of-the-art approaches in both the object categorization and instance recognition tasks. In addition, experiments on the more challenging Object Clutter Indoor Dataset confirm the validity of our method in the presence of clutter and occlusion. The code is publicly available at: https://github.com/MRLoghmani/rcfusion.},
  url      = {proceedings: loghmani2019ral.pdf}
}

@inproceedings{loghmani2018icra,
  author    = {M.R. Loghmani and B. Caputo and M. Vincze},
  title     = {{Recognizing Objects In-The-Wild: Where Do We Stand?}},
  booktitle = icra,
  year      = 2018,
  keywords  = {Computer Vision for Other Robotic Applications, Recognition, RGB-D Perception},
  abstract  = {The ability to recognize objects is an essential skill for a robotic system acting in human-populated environments. Despite decades of effort from the robotic and vision research communities, robots are still missing good visual perceptual systems, preventing the use of autonomous agents for realworld applications. The progress is slowed down by the lack of a testbed able to accurately represent the world perceived by the robot in-the-wild. In order to fill this gap, we introduce a large-scale, multi-view object dataset collected with an RGBD camera mounted on a mobile robot. The dataset embeds the challenges faced by a robot in a real-life application and provides a useful tool for validating object recognition algorithms. Besides describing the characteristics of the dataset, the paper evaluates the performance of a collection of wellestablished deep convolutional networks on the new dataset and analyzes the transferability of deep representations from Web images to robotic data. Despite the promising results obtained with such representations, the experiments demonstrate that object classification with real-life robotic data is far from being solved. Finally, we provide a comparative study to analyze and highlight the open challenges in robot vision, explaining the discrepancies in the performance.}
}


@inproceedings{lowry2018icra,
  author    = {S. Lowry and H. Andreasson},
  title     = {{LOGOS: Local Geometric Support for High-Outlier Spatial Verification}},
  booktitle = icra,
  year      = 2018,
  keywords  = {Localization, Computer Vision for Transportation, Visual-Based Navigation},
  abstract  = {This paper presents LOGOS, a method of spatial verification for visual localization that is robust in the presence of a high proportion of outliers. LOGOS uses scale and orientation information from local neighbourhoods of features to determine which points are likely to be inliers. The inlier points can be used for secondary localization verification and pose estimation. LOGOS is demonstrated on a number of benchmark localization datasets and outperforms RANSAC as a method of outlier removal and localization verification in scenarios that require robustness to many outliers.}
}

@article{luft2018ral,
  author   = {L. Luft and A. Schaefer and T. Schubert and W. Burgard},
  title    = {{Detecting Changes in the Environment Based on Full Posterior Distributions Over Real-Valued Grid Maps}},
  journal  = ral,
  volume   = {3},
  number   = {2},
  pages    = {1299--1305},
  year     = 2018,
  keywords = {Mapping, Range Sensing, Probability and Statistical Methods},
  abstract = {To detect changes in an environment, one has to decide whether a set of recent observations is incompatible with a set of previous observations. For binary, lidar-based grid maps, this is essentially the case when the laser beam traverses a voxel which has been observed as occupied, or when the beam is reflected by a voxel which has been observed as empty. However, in real-world environments, some voxels are neither completely occupied nor completely free. These voxels have to be modeled by real-valued variables, whose estimation is an inherently statistical process. Thus, it is nontrivial to decide whether two sets of observations emerge from the same underlying true map values, and hence from an unchanged environment. Our main idea is to account for the statistical nature of the estimation by leveraging the full map posteriors instead of only the most likely maps. Closed form solutions of posteriors over realvalued grid maps have been introduced recently. We leverage a similarity measure on these posteriors to score each point in time according to the probability that it constitutes a change in the hidden map value. While the proposed approach works for any type of real-valued grid map that allows the computation of the full posterior, we provide all formulas for the wellknown reflection maps and the recently introduced decay-rate maps. We introduce and compare different similarity measures and show that our method significantly outperforms baseline approaches in simulated and real world experiments.}
}

@article{loshchilov2017arxiv,
  title   = {{Decoupled weight decay regularization}},
  author  = {Loshchilov, Ilya and Hutter, Frank},
  journal = arxiv,
  volume  = {arXiv:1711.05101},
  year    = 2017,
  url     = {https://arxiv.org/pdf/1711.05101.pdf}
}

@inproceedings{mai2018icra,
  author    = {X. Mai and H. Zhang and M.Q. Meng},
  title     = {{Faster R-CNN with Classifier Fusion for Small Fruit Detection}},
  booktitle = icra,
  year      = 2018,
  keywords  = {Deep Learning in Robotics and Automation, Agricultural Automation, Robotics in Agriculture and Forestry},
  abstract  = {The-state-of-the-art of fruit detection with Faster R-CNN shows lack of detection advantage on small fruits. One of reasons is only single level features is used for localization of proposal candidates. In this paper, we propose to incorporate a multiple classifier fusion strategy into a Faster R-CNN network for small fruit detection. We utilize features from three different levels to learn three classifiers for objectness classification in the stage of proposal localization. Probabilities from classifiers are combined by a simple convolutional layer to generate final objectness classification for proposal candidates. In order to keep diversity of multiple classifiers, a novel loss term of classifier correlation is introduced into original loss function. Experimental results show that our model is feasible for detecting small fruits.}
}

@inproceedings{mangelson2018icra,
  author    = {J. Mangelson and D. Dominic and R. Eustice and R. Vasudevan},
  title     = {{Pairwise Consistent Measurement Set Maximization for Robust Multi-Robot Map Merging}},
  booktitle = icra,
  year      = 2018,
  keywords  = {Multi-Robot Systems, SLAM, Mapping},
  abstract  = {This paper reports on a method for robust selection of inter-map loop closures in multi-robot simultaneous localization and mapping (SLAM). Existing robust SLAM methods assume a good initialization or an odometry backbone to classify inlier and outlier loop closures. In the multi-robot case, these assumptions do not always hold. This paper presents an algorithm called Pairwise Consistency Maximization (PCM) that estimates the largest pairwise internally consistent set of measurements. Finding the largest pairwise internally consistent set can be transformed into an instance of the maximum clique problem from graph theory, and by leveraging the associated literature it can be solved in realtime. This paper evaluates how well PCM approximates the combinatorial gold standard using simulated data. It also evaluates the performance of PCM on synthetic and real-world data sets in comparison with DCS, SCGP, and RANSAC, and shows that PCM significantly outperforms these methods.}
}

@inproceedings{marion2018icra,
  author    = {P. Marion and P. Florence and L. Manuelli and R. Tedrake},
  title     = {{LabelFusion: A Pipeline for Generating Ground Truth Labels for Real RGBD Data of Cluttered Scenes}},
  booktitle = icra,
  year      = 2018,
  keywords  = {Object Detection, Segmentation and Categorization, RGB-D Perception, Deep Learning in Robotics and Automation},
  abstract  = {Deep neural network (DNN) architectures have been shown to outperform traditional pipelines for object segmentation and pose estimation using RGBD data, but the performance of these DNN pipelines is directly tied to how representative the training data is of the true data. Hence a key requirement for employing these methods in practice is to have a large set of labeled data for your specific robotic manipulation task, a requirement that is not generally satisfied by existing datasets. In this paper we develop a pipeline to rapidly generate high quality RGBD data with pixelwise labels and object poses. We use an RGBD camera to collect video of a scene from multiple viewpoints and leverage existing reconstruction techniques to produce a 3D dense reconstruction. We label the 3D reconstruction using a human assisted ICPfitting of object meshes. By reprojecting the results of labeling the 3D scene we can produce labels for each RGBD image of the scene. This pipeline enabled us to collect over 1,000,000 labeled object instances in just a few days. We use this dataset to answer questions related to how much training data is required, and of what quality the data must be, to achieve high performance from a DNN architecture. Our dataset and annotation pipeline are available at labelfusion.csail.mit.edu.}
}

@inproceedings{weyler2022wacv,
  author    = {J. Weyler and and F. Magistri and P. Seitz and J. Behley and C. Stachniss},
  booktitle = wacv,
  title     = {{In-Field Phenotyping Based on Crop Leaf and Plant Instance Segmentation}},
  year      = {2022},
  url       = {https://openaccess.thecvf.com/content/WACV2022/papers/Weyler_In-Field_Phenotyping_Based_on_Crop_Leaf_and_Plant_Instance_Segmentation_WACV_2022_paper.pdf}
}

@article{weyler2022ral,
  title   = {Joint Plant and Leaf Instance Segmentation on Field-Scale UAV Imagery},
  author  = {Weyler, Jan and Quakernack, Jan and Lottes, Philipp and Behley, Jens and Stachniss, Cyrill},
  journal = ral,
  volume  = {7},
  number  = {2},
  pages   = {3787--3794},
  year    = {2022},
  url     = {https://www.ipb.uni-bonn.de/wp-content/papercite-data/pdf/weyler2022ral.pdf}
}

@article{halstead2018ral,
  author  = {Halstead, Michael and McCool, Christopher and
             Denman, Simon and Perez, Tristan and Fookes, Clinton},
  journal = ral,
  number  = {4},
  pages   = {2995--3002},
  title   = {Fruit quantity and ripeness estimation using a
             robotic vision system},
  volume  = {3},
  year    = {2018},
  url     = {proceedings: halstead2018ral.pdf}
}

@inproceedings{wagner2021icra,
  author    = {Wagner, Nikolaus and Kirk, Raymond and Hanheide, Marc and Cielniak, Grzegorz and others},
  booktitle = icra,
  title     = {{Efficient and Robust Orientation Estimation of Strawberries for Fruit Picking Applications}},
  year      = {2021},
  url       = {proceedings: wagner2021icra.pdf}
}

@article{rose2016sensors,
  author  = {Rose, Johann Christian and Kicherer, Anna and  Wieland, Markus and Klingbeil, Lasse and T{\"o}pfer, Reinhard and Kuhlmann, Heiner},
  journal = sensors,
  number  = {12},
  pages   = {2136},
  title   = {Towards automated large-scale 3D phenotyping ofvineyards under field conditions},
  volume  = {16},
  year    = {2016}
}

@article{barth2016biosyseng,
  author  = {Barth, Ruud and Hemming, Jochen and van Henten, Eldert J},
  journal = biosyseng,
  pages   = {71--84},
  title   = {Design of an eye-in-hand sensing and servo control
             framework for harvesting robotics in dense
             vegetation},
  volume  = {146},
  year    = {2016}
}

@article{arad2020jfr,
  author  = {Arad, Boaz and Balendonck, Jos and Barth, Ruud and
             Ben-Shahar, Ohad and Edan, Yael and
             Hellstr{\"o}m, Thomas and Hemming, Jochen and
             Kurtser, Polina and Ringdahl, Ola and Tielen, Toon and
             others},
  journal = jfr,
  number  = {6},
  pages   = {1027--1039},
  title   = {Development of a sweet pepper harvesting robot},
  volume  = {37},
  year    = {2020}
}

@inproceedings{montes2020iros,
  author    = {Montes, Hector A and Le Louedec, Justin and
               Cielniak, Grzegorz and Duckett, Tom},
  booktitle = iros,
  title     = {Real-time detection of broccoli crops in 3D point
               clouds for autonomous robotic harvesting},
  year      = {2020},
  url       = {proceedings: montes2020iros.pdf}
}

@article{goerlich2021drones,
  author   = {GÃ¶rlich, Florian and Marks, Elias and
              Mahlein, Anne-Katrin and KÃ¶nig, Kathrin and
              Lottes, Philipp and Stachniss, Cyrill},
  journal  = {Drones},
  number   = {2},
  title    = {{UAV-Based Classification of Cercospora Leaf Spot
              Using RGB Images}},
  volume   = {5},
  pages    = {34},
  year     = {2021},
  abstract = {Plant diseases can impact crop yield. Thus, the
              detection of plant diseases using sensors that can be
              mounted on aerial vehicles is in the interest of
              farmers to support decision-making in integrated pest
              management and to breeders for selecting tolerant or
              resistant genotypes. This paper investigated the
              detection of Cercospora leaf spot (CLS), caused by
              Cercospora beticola in sugar beet using RGB imagery.
              We proposed an approach to tackle the CLS detection
              problem using fully convolutional neural networks,
              which operate directly on RGB images captured by a
              UAV. This efficient approach does not require complex
              multi- or hyper-spectral sensors, but provides
              reliable results and high sensitivity. We provided a
              detection pipeline for pixel-wise semantic
              segmentation of CLS symptoms, healthy vegetation, and
              background so that our approach can automatically
              quantify the grade of infestation. We thoroughly
              evaluated our system using multiple UAV datasets
              recorded from different sugar beet trial fields. The
              dataset consisted of a training and a test dataset
              and originated from different fields. We used it to
              evaluate our approach under realistic conditions and
              analyzed its generalization capabilities to unseen
              environments. The obtained results correlated to
              visual estimation by human experts significantly. The
              presented study underlined the potential of
              high-resolution RGB imaging and convolutional neural
              networks for plant disease detection under field
              conditions. The demonstrated procedure is
              particularly interesting for applications under
              practical conditions, as no complex and
              cost-intensive measuring system is required.},
  url      = {https://www.mdpi.com/2504-446X/5/2/34/pdf}
}

@article{mccool2018ral,
  author   = {C.S. McCool and J. Beattie and J. Firn and C. Lehnert and J. Kulk and R. Russell and T. Perez and O. Bawden},
  title    = {{Efficacy of Mechanical Weeding Tools: A Study into Alternative Weed Management Strategies Enabled by Robotics (2)}},
  journal  = ral,
  year     = 2018,
  keywords = {Agricultural Automation, Robotics in Agriculture and Forestry, Field Robots},
  volume   = {3},
  number   = {2},
  pages    = {1184--1190},
  abstract = {The rise of herbicide resistant weed species has re-invigorated research in non-chemical methods for weed management. Robots, such as AgBot II, which can detect and classify weeds as they traverse a field are a key enabling factor for individualised treatment of weed species. Integral to the invidualised treatment of weed species are the the non-herbicide methods through which the weeds are managed. This paper explores mechanical methods as an alternative to weed management. Three implements are considered: belowsurface tilling (arrow hoe), above-surface tilling (tines) and a cutting mechanism. These mechanisms were evaluated in a controlled field with varying rates of application to herbicide resistant weeds of interest for Queensland, Australia. Statistical analysis demonstrated the efficacy of these implements and highlighted the importance of early intervention. It was found that a tine, deployed automatically on AgBot II, was effective for all of the weeds considered in this study, leading to an overall survival probability of 0.28 0.15. Further analysis demonstrated the significance of treatment time with late intervention commencing at week 6 resulting in a survival probability of 0.54 0.08 vs 0.24 0.18 for earlier intervention at week 4.}
}

@inproceedings{mehta2018icra,
  author    = {D. Mehta and G. Ferrer and E. Olson},
  title     = {{Backprop-MPDM: Faster Risk-Aware Policy Evaluation through Efficient Gradient Optimization}},
  booktitle = icra,
  year      = 2018,
  keywords  = {Motion and Path Planning, Intelligent Transportation Systems, Field Robots},
  abstract  = {In Multi-Policy Decision-Making (MPDM), many computationally-expensive forward simulations are performed in order to predict the performance of a set of candidate policies. In risk-aware formulations of MPDM, only the worst outcomes affect the decision making process, and efficiently finding these influential outcomes becomes the core challenge. Recently, stochastic gradient optimization algorithms, using a heuristic function, were shown to be significantly superior to random sampling. In this paper, we show that accurate gradients can be computed even through a complex forward simulation using approaches similar to those in deep networks. We show that our proposed approach finds influential outcomes more reliably, and is faster than earlier methods, allowing us to evaluate more policies while simultaneously eliminating the need to design an easily-differentiable heuristic function. We demonstrate significant performance improvements in simulation as well as on a real robot platform navigating a highly dynamic environment.}
}

@inproceedings{mehta2018icra-raf3,
  author    = {N. Mehta and J. McBride and G. Pandey},
  title     = {{Robust and Fast 3D Scan Alignment Using Mutual Information}},
  booktitle = icra,
  year      = 2018,
  keywords  = {Autonomous Vehicle Navigation, Localization, Wheeled Robots},
  abstract  = {This paper presents a mutual information (MI) based algorithm for the estimation of full 6-degree-of-freedom (DOF) rigid body transformation between two overlapping point clouds. We first divide the scene into a 3D voxel grid and define simple to compute features for each voxel in the scan. The two scans that need to be aligned are considered as a collection of these features and the MI between these voxelized features is maximized to obtain the correct alignment of scans. We have implemented our method with various simple point cloud features (such as number of points in voxel, variance of z-height in voxel) and compared the performance of the proposed method with existing point-to-point and point-todistribution registration methods. We show that our approach has an efficient and fast parallel implementation on GPU, and evaluate the robustness and speed of the proposed algorithm on two real-world datasets which have variety of dynamic scenes from different environments.}
}

@inproceedings{mielle2018icra,
  author    = {M. Mielle and M. Magnusson and A.J. Lilienthal},
  title     = {{A method to segment maps from different modalities using free space layout MAORIS: map of ripples segmentation}},
  booktitle = icra,
  year      = 2018,
  keywords  = {Mapping, Object Detection, Segmentation and Categorization, Semantic Scene Understanding},
  abstract  = {How to divide floor plans or navigation maps into semantic representations, such as rooms and corridors, is an important research question in fields such as human-robot interaction, place categorization, or semantic mapping. While most works focus on segmenting robot built maps, those are not the only types of map a robot, or its user, can use. We present a method for segmenting maps from different modalities, focusing on robot built maps and hand-drawn sketch maps, and show better results than state of the art for both types. Our method segments the map by doing a convolution between the distance image of the map and a circular kernel, and grouping pixels of the same value. Segmentation is done by detecting ripple-like patterns where pixel values vary quickly, and merging neighboring regions with similar values. We identify a flaw in the segmentation evaluation metric used in recent works and propose a metric based on Matthews correlation coefficient (MCC). We compare our results to groundtruth segmentations of maps from a publicly available dataset, on which we obtain a better MCC than the state of the art with 0.98 compared to 0.65 for a recent Voronoi-based segmentation method and 0.70 for the DuDe segmentation method. We also provide a dataset of sketches of an indoor environment, with two possible sets of ground truth segmentations, on which our method obtains an MCC of 0.56 against 0.28 for the Voronoibased segmentation method and 0.30 for DuDe.}
}

@inproceedings{milan2018icra,
  author    = {A. Milan and T. Pham and V.K.B. G and D. Morrison and A.M.W. Tow and L. Liu and J. Erskine and R. Grinover and A. Gurman and T. Hunn and N. Kelly-Boxall and D.Q. Lee and M. McTaggart and G.M. Rallos and A. Razjigaev and T.J. Rowntree and T. Shen and R. Smith and S. Wade-McCue and Z. Zhuang and C. Lehnert and G. Lin and I. Reid and P. Corke and J. Leitner},
  title     = {{Semantic Segmentation from Limited Training Data}},
  booktitle = icra,
  year      = 2018,
  keywords  = {Semantic Scene Understanding, RGB-D Perception, Object Detection, Segmentation and Categorization},
  abstract  = {We present our approach for robotic perception in cluttered scenes that led to winning the recent Amazon Robotics Challenge (ARC) 2017. Next to small objects with shiny and transparent surfaces, the biggest challenge of the 2017 competition was the introduction of unseen categories. In contrast to traditional approaches which require large collections of annotated data and many hours of training, the task here was to obtain a robust perception pipeline with only few minutes of data acquisition and training time. To that end, we present two strategies that we explored. One is a deep metric learning approach that works in three separate steps: semantic-agnostic boundary detection, patch classification and pixel-wise voting. The other is a fully-supervised semantic segmentation approach with efficient dataset collection. We conduct an extensive analysis of the two methods on our ARC 2017 dataset. Interestingly, only few examples of each class are sufficient to fine-tune even very deep convolutional neural networks for this specific task.}
}

@inproceedings{miller2018icra,
  author    = {D. Miller and L.J. Nicholson and F. Dayoub and N. Snderhauf},
  title     = {{Dropout Sampling for Robust Object Detection in Open-Set Conditions}},
  booktitle = icra,
  year      = 2018,
  keywords  = {Deep Learning in Robotics and Automation, Object Detection, Segmentation and Categorization},
  abstract  = {Dropout Variational Inference, or Dropout Sampling, has been recently proposed as an approximation technique for Bayesian Deep Learning and evaluated for image classification and regression tasks. This paper investigates the utility of Dropout Sampling for object detection for the first time. We demonstrate how label uncertainty can be extracted from a state-of-the-art object detection system via Dropout Sampling. We evaluate this approach on a large synthetic dataset of 30,000 images, and a real-world dataset captured by a mobile robot in a versatile campus environment. We show that this uncertainty can be utilized to increase object detection performance under the open-set conditions that are typically encountered in robotic vision. A Dropout Sampling network is shown to achieve a 12.3\% increase in recall (for the same precision score as a standard network) and a 15.1\% increase in precision (for the same recall score as the standard network).}
}

@inproceedings{nasiri2018icra,
  author    = {S.M. Nasiri and H. Moradi and R. Hosseini},
  title     = {{A Linear Least Square Initialization Method for 3D Pose Graph Optimization Problem}},
  booktitle = icra,
  year      = 2018,
  keywords  = {SLAM},
  abstract  = {Pose Graph Optimization (PGO) is an important optimization problem arising in robotics and machine vision applications like 3D reconstruction and 3D SLAM. Each node of pose graph corresponds to an orientation and a location. The PGO problem finds orientations and locations of the nodes from relative noisy observation between nodes. Recent investigations show that well-known iterative PGO solvers need good initialization to converge to good solutions. However, we observed that state-of-the-art initialization methods obtain good initialization only in low noise problems, and they fail in challenging problems having more measurement noise. Consequently, iterative methods may converge to bad solutions in high noise problems. In this paper, a new method for obtaining orientations in the PGO optimization problem is presented. Like other well-known methods the initial locations are obtained from the result of a least-squares problem. The proposed method iteratively approximates the problem around current estimation and converts it to a least-squares problem. Therefore, the method can be seen as an iterative least-squares method which is computationally efficient. Simulation results show that the proposed initialization method helps the most well-known iterative solver to obtain better optima and significantly outperform other solvers in some cases. Index TermsPose Graph Optimization, Least square, 3D SLAM, Initialization method}
}

@article{oleynikova2018ral,
  author   = {H. Oleynikova and Z.J. Taylor and R. Siegwart and J. Nieto},
  title    = {{Safe Local Exploration for Replanning in Cluttered Unknown Environments for Micro-Aerial Vehicles}},
  journal  = ral,
  volume   = {3},
  number   = {3},
  pages    = {1474--1481},
  year     = 2018,
  keywords = {Motion and Path Planning, Aerial Systems: Perception and Autonomy, Visual-Based Navigation},
  abstract = {In order to enable Micro-Aerial Vehicles (MAVs) to assist in complex, unknown, unstructured environments, they must be able to navigate with guaranteed safety, even when faced with a cluttered environment they have no prior knowledge of. While trajectory optimization-based local planners have been shown to perform well in these cases, prior work either does not address how to deal with local minima in the optimization problem, or solves it by using an optimistic global planner. We present a conservative trajectory optimization-based local planner, coupled with a local exploration strategy that selects intermediate goals. We perform extensive simulations to show that this system performs better than the standard approach of using an optimistic global planner, and also outperforms doing a single exploration step when the local planner is stuck. The method is validated through experiments in a variety of highly cluttered environments including a dense forest. These experiments show the complete system running in real time fully onboard an MAV, mapping and replanning at 4 Hz.}
}

@inproceedings{oliveira2018icra,
  author    = {G. Oliveira and W. Burgard and T. Brox},
  title     = {{DPDB-Net: Exploiting Dense Connections for Convolutional Encoders}},
  booktitle = icra,
  year      = 2018,
  keywords  = {Semantic Scene Understanding, Recognition},
  abstract  = {Densely connected networks for classification enable feature exploration and result in state-of-the-art performance on multiple classification tasks. The alternative to dense networks is the residual network which enables feature reusage. In this work, we combine these orthogonal concepts for encoder-decoder architectures, which we call Dual-Path DenseBlock Network (DPDB-Net). We}
}

@inproceedings{oliveira2018icra-ltrt,
  author    = {R. Oliveira and F.H.M.d. Rocha and L. Ott and V. Guizilini and F. Ramos and V.G. Junior},
  title     = {{Learning to Race through Coordinate Descent Bayesian Optimisation}},
  booktitle = icra,
  year      = 2018,
  keywords  = {Learning and Adaptive Systems},
  abstract  = {In the automation of many kinds of processes, the observable outcome can often be described as the combined effect of an entire sequence of actions, or controls, applied throughout the process execution. In these cases, strategies to optimise control policies for individual stages of the process are not applicable, and instead the whole policy needs to be optimised at once. On the other hand, the cost to evaluate the policys performance might also be high, being desirable that a solution can be found with as few interactions as possible with the real system. We consider the problem of optimising control policies to allow a robot to complete a given race track within a minimum amount of time. We assume that the robot has no prior information about the track or its own dynamical model, just an initial valid driving example. Localisation is only applied to monitor the robot and to provide an indication of its position along the tracks centre axis. With that in mind, we propose a method for finding a policy that minimises the time per lap while keeping the vehicle on the track using a Bayesian optimisation (BO) approach over a reproducing kernel Hilbert space. We apply an algorithm to search more efficiently over high-dimensional policy-parameter spaces with BO, by iterating over each dimension individually, in a sequential coordinate descent-like scheme. Experiments demonstrate the performance of the algorithm against other methods in a simulated car racing environment.}
}

@inproceedings{ort2018icra,
  author    = {T. Ort and L. Paull and D. Rus},
  title     = {{Autonomous Vehicle Navigation in Rural Environments without Detailed Prior Maps}},
  booktitle = icra,
  year      = 2018,
  keywords  = {Autonomous Vehicle Navigation, Localization, Intelligent Transportation Systems},
  abstract  = {State-of-the-art autonomous driving systems rely heavily on detailed and highly accurate prior maps. However, outside of small urban areas, it is very challenging to build, store, and transmit detailed maps since the spatial scales are so large. Furthermore, maintaining detailed maps of large rural areas can be impracticable due to the rapid rate at which these environments can change. This is a significant limitation for the widespread applicability of autonomous driving technology, which has the potential for an incredibly positive societal impact. In this paper, we address the problem of autonomous navigation in rural environments through a novel mapless driving framework that combines sparse topological maps for global navigation with a sensor-based perception system for local navigation. First, a local navigation goal within the sensor view of the vehicle is chosen as a waypoint leading towards the global goal. Next, the local perception system generates a feasible trajectory in the vehicle frame to reach the waypoint while abiding by the rules of the road for the segment being traversed. These trajectories are updated to remain in the local frame using the vehicles odometry and the associated uncertainty based on the least-squares residual and a recursive filtering approach, which allows the vehicle to navigate road networks reliably, and at high speed, without detailed prior maps. We demonstrate the performance of the system on a full-scale autonomous vehicle navigating in a challenging rural environment and benchmark the system on a large amount of collected data.}
}

@inproceedings{osep2018icra,
  author    = {A. Osep and W. Mehner and P. Voigtlaender and B. Leibe},
  title     = {{Track, Then Decide: Category-Agnostic Vision-Based Multi-Object Tracking}},
  booktitle = icra,
  year      = 2018,
  keywords  = {Visual Tracking, Computer Vision for Transportation, Autonomous Agents},
  abstract  = {The most common paradigm for vision-based multi-object tracking is tracking-by-detection, due to the availability of reliable detectors for several important object categories such as cars and pedestrians. However, future mobile systems will need a capability to cope with rich humanmade environments, in which obtaining detectors for every possible object category would be infeasible. In this paper, we address the problem of class-agnostic multi-object tracking using generic object proposals. We present an efficient segmentation mask-based tracker which associates pixel-precise masks reported by the segmentation. Our approach can utilize semantic information whenever it is available for classifying objects at the track level, while retaining the capability to track generic unknown objects in the absence of such information. We demonstrate experimentally that our approach achieves performance comparable to state-of-the-art trackingby-detection methods for popular object categories such as cars and pedestrians. Additionally, we show that the proposed method can discover and robustly track a large variety of other objects.}
}

@inproceedings{park2018icra-elfd,
  author    = {C. Park and P. Moghadam and S. Kim and A. Elfes and C. Fookes and S. Sridharan},
  title     = {{Elastic LiDAR Fusion: Dense Map-Centric Continuous-Time SLAM}},
  booktitle = icra,
  year      = 2018,
  keywords  = {SLAM, Mapping, Sensor Fusion},
  abstract  = {The concept of continuous-time trajectory representation has brought increased accuracy and efficiency to multi-modal sensor fusion in modern SLAM. However, regardless of these advantages, its offline property caused by the requirement of global batch optimization is critically hindering its relevance for real-time and life-long applications. In this paper, we present a dense map-centric SLAM method based on a continuous-time trajectory to cope with this problem. The proposed system locally functions in a similar fashion to conventional Continuous-Time SLAM (CT-SLAM). However, it removes the need for global trajectory optimization by introducing map deformation. The computational complexity of the proposed approach for loop closure does not depend on the operation time, but only on the size of the space it explored before the loop closure. It is therefore more suitable for long term operation compared to the conventional CTSLAM. Furthermore, the proposed method reduces uncertainty in the reconstructed dense map by using probabilistic surface element (surfel) fusion. We demonstrate that the proposed method produces globally consistent maps without global batch trajectory optimization, and effectively reduces LiDAR noise by surfel fusion.}
}

@inproceedings{park2018icra-hdew,
  author    = {K. Park and S. Kim and K. Sohn},
  title     = {{High-Precision Depth Estimation with the 3D LiDAR and Stereo Fusion}},
  booktitle = icra,
  year      = 2018,
  keywords  = {Sensor Fusion, Deep Learning in Robotics and Automation, RGB-D Perception},
  abstract  = {We present a deep convolutional neural network (CNN) architecture for high-precision depth estimation by jointly utilizing sparse 3D LiDAR and dense stereo depth information. In this network, the complementary characteristics of sparse 3D LiDAR and dense stereo depth are simultaneously encoded in a boosting manner. Tailored to the LiDAR and stereo fusion problem, the proposed network differs from previous CNNs in the incorporation of a compact convolution module, which can be deployed with the constraints of mobile devices. As training data for the LiDAR and stereo fusion is rather limited, we introduce a simple yet effective approach for reproducing the raw KITTI dataset. The raw LiDAR scans are augmented by adapting an off-the-shelf stereo algorithm and a confidence measure. We evaluate the proposed network on the KITTI benchmark and data collected by our multi-sensor acquisition system. Experiments demonstrate that the proposed network generalizes across datasets and is significantly more accurate than various baseline approaches.}
}

@inproceedings{pham2018icra-sjga,
  author    = {T. Pham and T. Do and N. Snderhauf and I. Reid},
  title     = {{SceneCut: Joint Geometric and Object Segmentation for Indoor Scenes}},
  booktitle = icra,
  year      = 2018,
  keywords  = {RGB-D Perception, Object Detection, Segmentation and Categorization, Semantic Scene Understanding},
  abstract  = {This paper presents SceneCut, a novel approach to jointly discover previously unseen objects and non-object surfaces using a single RGB-D image. SceneCuts joint reasoning over scene semantics and geometry allows a robot to detect and segment object instances in complex scenes where modern deep learning-based methods either fail to separate object instances, or fail to detect objects that were not seen during training. SceneCut automatically decomposes a scene into meaningful regions which either represent objects or scene surfaces. The decomposition is qualified by an unified energy function over objectness and geometric fitting. We show how this energy function can be optimized efficiently by utilizing hierarchical segmentation trees. Moreover, we leverage a pretrained convolutional oriented boundary network to predict accurate boundaries from images, which are used to construct high-quality region hierarchies. We evaluate SceneCut on several different indoor environments, and the results show that SceneCut significantly outperforms all the existing methods.}
}

@article{piazza2018ral,
  author   = {E. Piazza and A. Romanoni and M. Matteucci},
  title    = {{Real-Time CPU-Based Large-Scale 3D Mesh Reconstruction}},
  journal  = ral,
  volume   = {3},
  number   = {3},
  pages    = {1584--1591},
  year     = 2018,
  keywords = {Mapping, Computer Vision for Transportation},
  abstract = {In Robotics, especially in this era of autonomous driving, mapping is one key ability of a robot to be able to navigate through an environment, localize on it and analyze its traversability. To allow for real-time execution on constrained hardware, the map usually estimated by feature-based or semidense SLAM algorithms is a sparse point cloud; a richer and more complete representation of the environment is desirable. Existing dense mapping algorithms require extensive use of GPU computing and they hardly scale to large environments; incremental algorithms from sparse points still represent an effective solution when light computational effort is needed and big sequences have to be processed in real-time. In this paper we improved and extended the state of the art incremental manifold mesh algorithm proposed in [1] and extended in [2]. While these algorithms do not achieve real-time and they embed points from SLAM or Structure from Motion only when their position is fixed, in this paper we propose the first incremental algorithm able to reconstruct a manifold mesh in real-time through single core CPU processing which is also able to modify the mesh according to 3D points updates from the underlying SLAM algorithm. We tested our algorithm against two state of the art incremental mesh mapping systems on the KITTI dataset, and we showed that, while accuracy is comparable, our approach is able to reach real-time performances thanks to an order of magnitude speed-up.}
}

@inproceedings{porav2018icra,
  author    = {H. Porav and W. Maddern and P. Newman},
  title     = {{Adversarial Training for Adverse Conditions: Robust Metric Localisation Using Appearance Transfer}},
  booktitle = icra,
  year      = 2018,
  keywords  = {Localization, Visual Learning, Autonomous Vehicle Navigation},
  abstract  = {We present a method of improving visual place recognition and metric localisation under very strong appearance change. We learn an invertable generator that can transform the conditions of images, e.g. from day to night, summer to winter etc. This image transforming filter is explicitly designed to aid and abet feature-matching using a new loss based on SURF detector and dense descriptor maps. A network is trained to output synthetic images optimised for feature matching given only an input RGB image, and these generated images are used to localize the robot against a previously built map using traditional sparse matching approaches. We benchmark our results using multiple traversals of the Oxford RobotCar Dataset over a year-long period, using one traversal as a map and the other to localise. We show that this method significantly improves place recognition and localisation under changing and adverse conditions, while reducing the number of mapping runs needed to successfully achieve reliable localisation.}
}

@inproceedings{riggio2018icra,
  author    = {G. Riggio and C. Fantuzzi and C. Secchi},
  title     = {{A Low-Cost Navigation Strategy for Yield Estimation in Vineyards}},
  booktitle = icra,
  year      = 2018,
  keywords  = {Robotics in Agriculture and Forestry, Autonomous Vehicle Navigation},
  abstract  = {Accurate yield estimation is very important for improving the vineyard management, the quality of the grapes and the health of the vines. The most common systems use RGB image processing for achieving a good estimation. In order to collect images, robots or farming vehicles can be equipped with a RGB camera. In this paper, we propose a low-cost autonomous system which can navigate through a vineyard while collecting grape pictures in order to provide a yield estimation. Our system uses only a laser scanner to detect the row and follows it until its end, then it navigates towards the next one, exploiting the knowledge of the vineyard. The navigation algorithm was tested both in simulation and in a real environment with good results. Furthermore, a yield estimation of two different grape varieties is presented.}
}

@inproceedings{romero2018icra,
  author    = {A.R. Romero and P.V.K. Borges and A. Pfrunder and A. Elfes},
  title     = {{Map-Aware Particle Filter for Localization}},
  booktitle = icra,
  year      = 2018,
  keywords  = {Sensor Fusion, Localization, Wheeled Robots},
  abstract  = {This work presents a method to improve vehicle localization by using the information from a prior occupancy grid to bound the possible poses. The method, named MapAware Particle Filter, uses a nonlinear approach to mapmatching that can be integrated into a particle filter framework for localization. Each particle is re-weighted based on the validity of its current position in the map. In addition, we buffer the trajectory followed by the vehicle and then append it to each particles pose. We then quantify the overlap between the trajectory and the maps free space. This serves as a measure of each particles validity given the trajectory and the shape of the map. We evaluated the method by performing experiments with different types of localization sensors: First, (i) we significantly reduced the drift inherent to dead reckoning. By only using wheel odometry and map information we achieved loop closure over a distance of approximately 3 km. We also (ii) increased the accuracy of GPS localization. Finally, (iii) we fused a fragile 2D LiDAR localization with the map information. The resulting system had a higher robustness and managed to close the loop in an outdated map where it had failed before.}
}

@inproceedings{ruchti2018icra,
  author    = {P. Ruchti and W. Burgard},
  title     = {{Mapping with Dynamic-Object Probabilities Calculated from Single 3D Range Scans}},
  booktitle = icra,
  year      = 2018,
  keywords  = {Mapping, Dynamics},
  abstract  = {Various autonomous robotic systems require maps for robust and safe navigation. Particularly when robots are employed in dynamic environments, accurate knowledge about which components of the robot perceptions belong to dynamic and static aspects in the environment can greatly improve navigation functions. In this paper we propose a novel method for building 3D grid maps using laser range data in dynamic environments. Our approach uses a neural network to estimate the pointwise probability of a point belonging to a dynamic object. The output from our network is fed to the mapping module for building a 3D grid map containing only static parts of the environment. We present experimental results obtained by training our neural network using the KITTI dataset and evaluating it in a mapping process using our own dataset. In extensive experiments, we show that maps generated using the proposed probability about dynamic objects increases the accuracy of the resulting maps.}
}

@article{sa2018ral,
  author   = {I. Sa and Z. Chen and M. Popovic and R. Khanna and F. Liebisch and J. Nieto and R. Siegwart},
  title    = {{Weednet: Dense Semantic Weed Classification Using Multispectral Images and MAV for Smart Farming}},
  journal  = ral,
  volume   = {3},
  number   = {1},
  pages    = {588--595},
  year     = 2018,
  keywords = {Robotics in Agriculture and Forestry, Agricultural Automation, Object Detection, Segmentation and Categorization},
  abstract = {Selective weed treatment is a critical step in autonomous crop management as related to crop health and yield. However, a key challenge is reliable, and accurate weed detection to minimize damage to surrounding plants. In this paper, we present an approach for dense semantic weed classification with multispectral images collected by a micro aerial vehicle (MAV). We use the recently developed encoder-decoder cascaded Convolutional Neural Network (CNN), SegNet, that infers dense semantic classes while allowing any number of input image channels and class balancing with our sugar beet and weed datasets. To obtain training datasets, we established an experimental field with varying herbicide levels resulting in field plots containing only either crop or weed, enabling us to use the Normalized Difference Vegetation Index (NDVI) as a distinguishable feature for automatic ground truth generation. We train 6 models with different numbers of input channels and condition (fine-tune) it to achieve 0.8 F1-score and 0.78 Area Under the Curve (AUC) classification metrics. For model deployment, an embedded GPU system (Jetson TX2) is tested for MAV integration. Dataset used in this paper is released to support the community and future work.}
}

@article{schaefer2018ral,
  author   = {A. Schaefer and L. Luft and W. Burgard},
  title    = {{DCT Maps: Compact Differentiable Lidar Maps Based on the Cosine Transform}},
  journal  = ral,
  volume   = {3},
  number   = {2},
  pages    = {1002--1009},
  year     = 2018,
  keywords = {Localization, Mapping, Range Sensing},
  abstract = {Most robot mapping techniques for lidar sensors tessellate the environment into pixels or voxels and assume uniformity of the environment within them. Although intuitive, this representation entails disadvantages: The resulting grid maps exhibit aliasing effects and are not differentiable. In the present paper, we address these drawbacks by introducing a novel mapping technique that does neither rely on tessellation nor on the assumption of piecewise uniformity of the space, without increasing memory requirements. Instead of representing the map in the position domain, we store the map parameters in the discrete frequency domain and leverage the continuous extension of the inverse discrete cosine transform to convert them to a continuously differentiable scalar field in the position domain, which we call DCT map. A DCT map assigns to each point in space a lidar decay rate, which models the local permeability of the space for laser rays. In this way, the map can describe objects of different laser permeabilities, from completely opaque to completely transparent. DCT maps represent lidar measurements significantly more accurate than grid maps, Gaussian process occupancy maps, and Hilbert maps, all with the same memory requirements, as demonstrated in our real-world experiments.},
  url      = {https://arxiv.org/pdf/1910.11147.pdf}
}

@inproceedings{schlegel2018icra,
  author    = {D. Schlegel and M. Colosi and G. Grisetti},
  title     = {{ProSLAM: Graph SLAM from a Programmer's Perspective}},
  booktitle = icra,
  year      = 2018,
  keywords  = {SLAM, Education Robotics},
  abstract  = {In this paper we present ProSLAM, a lightweight open-source stereo visual SLAM system designed with simplicity in mind. This work stems from the experience gathered by the authors while teaching SLAM and aims at providing a highly modular system that can be easily implemented and understood. Rather than focusing on the well known mathematical aspects of stereo visual SLAM, we highlight the data structures and the algorithmic aspects required to realize such a system. We implemented ProSLAM using the C++ programming language in combination with a minimal set of standard libraries. The results of a thorough validation performed on several standard benchmark datasets show that ProSLAM achieves precision comparable to state-of-the-art approaches, while requiring substantially less computation.}
}

@article{schneider2018ral,
  author   = {T. Schneider and M.T. Dymczyk and M. Fehr and K. Egger and S. Lynen and I. Gilitschenski and R. Siegwart},
  title    = {{Maplab: An Open Framework for Research in Visual-Inertial Mapping and Localization}},
  journal  = ral,
  volume   = {3},
  number   = {3},
  pages    = {1418--1425},
  year     = 2018,
  keywords = {Mapping, Localization, Visual-Based Navigation},
  abstract = {Robust and accurate visual-inertial estimation is crucial to many of todays challenges in robotics. Being able to localize against a prior map and obtain accurate and driftfree pose estimates can push the applicability of such systems even further. Most of the currently available solutions, however, either focus on a single session use case, lack localization capabilities, or dont provide an end-to-end pipeline. We believe that only a complete system, combining state-of-the-art algorithms, scalable multi-session mapping tools, and a flexible user interface, can become an efficient research platform. We, therefore, present maplab, an open, research-oriented visual-inertial mapping framework for processing and manipulating multi-session maps, written in C++. On the one hand, maplab can be seen as a ready-to-use visual-inertial mapping and localization system. On the other hand, maplab provides the research community with a collection of multisession mapping tools that include map merging, visual-inertial batch optimization, and loop closure. Furthermore, it includes an online frontend that can create visual-inertial maps and also track a global drift-free pose within a localization map. In this paper, we present the system architecture, five use cases, and evaluations of the system on public datasets. The source code of maplab is freely available for the benefit of the robotics research community.},
  url      = {https://arxiv.org/pdf/1711.10250.pdf}
}

@inproceedings{scona2018icra,
  author    = {R. Scona and M. Jaimez and Y.R. Petillot and M. Fallon and D. Cremers},
  title     = {{StaticFusion: Background Reconstruction for Dense RGB-D SLAM in Dynamic Environments}},
  booktitle = icra,
  year      = 2018,
  keywords  = {SLAM, RGB-D Perception, Visual Tracking},
  abstract  = {Dynamic environments are challenging for visual SLAM as moving objects can impair camera pose tracking and cause corruptions to be integrated into the map. In this paper, we propose a method for robust dense RGB-D SLAM in dynamic environments which detects moving objects and simultaneously reconstructs the background structure. While most methods employ implicit robust penalisers or outlier filtering techniques in order to handle moving objects, our approach is to simultaneously estimate the camera motion as well as a probabilistic static/dynamic segmentation of the current RGBD image pair. This segmentation is then used for weighted dense RGB-D fusion to estimate a 3D model of only the static parts of the environment. By leveraging the 3D model for frameto-model alignment, as well as static/dynamic segmentation, camera motion estimation has reduced overall drift as well as being more robust to the presence of dynamics in the scene. Demonstrations are presented which compare the proposed method to related state-of-the-art approaches using both static and dynamic sequences. The proposed method achieves similar performance in static environments and improved accuracy and robustness in dynamic scenes.}
}

@article{shahbandi2018ral,
  author   = {S.G. Shahbandi and M. Magnusson and K. Iagnemma},
  title    = {{Nonlinear Optimization of Multimodal 2D Map Alignment with Application to Prior Knowledge Transfer}},
  journal  = ral,
  volume   = {3},
  number   = {3},
  pages    = {2040--2047},
  year     = 2018,
  keywords = {Mapping},
  abstract = {We propose a method based on a non-linear transformation for non-rigid alignment of maps of different modalities, exemplified with matching partial and deformed 2D maps to layout maps. For two types of indoor environments, over a data-set of 40 maps, we have compared the method to state-of-the-art map matching and non-rigid image registration methods and demonstrate a success rate of 80.41\% and a mean point-to-point alignment error of 1.78 meters, compared to 31.9\% and 10.7 meters for the best alternative method. We also propose a fitness measure that can quite reliably detect bad alignments. Finally we show a use case of transferring prior knowledge (labels/segmentation), demonstrating that map segmentation is more consistent when transferred from an aligned layout map than when operating directly on partial maps (95.97\% vs. 81.56\%).},
  url      = {https://www.diva-portal.org/smash/get/diva2:1197226/FULLTEXT01.pdf}
}

@inproceedings{sharma2018icra-gaos,
  author    = {S. Sharma and J.A. Ansari and K.M. Jatavallabhula and M. Krishna},
  title     = {Geometry and Object Shape Costs for Accurate Multi-Object Tracking in Road Scenes},
  booktitle = icra,
  year      = 2018,
  keywords  = {Visual Tracking, Computer Vision for Automation},
  abstract  = {This paper introduces geometry and object shape and pose costs for multi-object tracking in urban driving scenarios. Using images from a monocular camera alone, we devise pairwise costs for object tracks, based on several 3D cues such as object pose, shape, and motion. The proposed costs are agnostic to the data association method and can be incorporated into any optimization framework to output the pairwise data associations. These costs are easy to implement, can be computed in real-time, and complement each other to account for possible errors in a tracking-by-detection framework. We perform an extensive analysis of the designed costs and empirically demonstrate consistent improvement over the state-of-the-art under varying conditions that employ a range of object detectors, exhibit a variety in camera and object motions, and, more importantly, are not reliant on the choice of the association framework. We also show that, by using the simplest of associations frameworks (two-frame Hungarian assignment), we surpass the state-of-the-art in multi-objecttracking on road scenes. More qualitative and quantitative results can be found at https://junaidcs032.github. io/Geometry_ObjectShape_MOT/.}
}

@inproceedings{shin2018icra,
  author    = {Y. Shin and Y.S. Park and A. Kim},
  title     = {Direct Visual SLAM Using Sparse Depth for Camera-LiDAR System},
  booktitle = icra,
  year      = 2018,
  keywords  = {SLAM, Mapping, Sensor Fusion},
  abstract  = {This paper describes a framework for direct visual simultaneous localization and mapping (SLAM) combining a monocular camera with sparse depth information from Light Detection and Ranging (LiDAR). To ensure realtime performance while maintaining high accuracy in motion estimation, we present (i) a sliding window-based tracking method, (ii) strict pose marginalization for accurate pose-graph SLAM and (iii) depth-integrated frame matching for largescale mapping. Unlike conventional feature-based visual and LiDAR mapping, the proposed approach is direct, eliminating the visual feature in the objective function. We evaluated results using our portable camera-LiDAR system as well as KITTI odometry benchmark datasets. The experimental results prove that the characteristics of two complementary sensors are very effective in improving real-time performance and accuracy. Via validation, we achieved low drift error of 0.98\% in the KITTI benchmark including various environments such as a highway and residential areas.}
}

@inproceedings{siva2018icra,
  author    = {S. Siva and H. Zhang},
  title     = {Omnidirectional Multisensory Perception Fusion for Long-Term Place Recognition},
  booktitle = icra,
  year      = 2018,
  keywords  = {SLAM, Omnidirectional Vision, Visual Learning},
  abstract  = {Over the recent years, long-term place recognition has attracted an increasing attention to detect loops for largescale Simultaneous Localization and Mapping (SLAM) in loopy environments during long-term autonomy. Almost all existing methods are designed to work with traditional cameras with a limited field of view. Recent advances in omnidirectional sensors offer a robot an opportunity to perceive the entire surrounding environment. However, no work has existed thus far to research how omnidirectional sensors can help long-term place recognition, especially when multiple types of omnidirectional sensory data are available. In this paper, we propose a novel approach to integrate observations obtained from multiple sensors from different viewing angles in the omnidirectional observation in order to perform multi-directional place recognition in longterm autonomy. Our approach also answers two new questions when omnidirectional multisensory data is available for place recognition, including whether it is possible to recognize a place with long-term appearance variations when robots approach it from various directions, and whether observations from various viewing angles are the same informative. To evaluate our approach and hypothesis, we have collected the first largescale dataset that consists of omnidirectional multisensory (intensity and depth) data collected in urban and suburban environments across a year. Experimental results have shown that our approach is able to achieve multi-directional long-term place recognition, and identifies the most discriminative viewing angles from the omnidirectional observation.}
}

@inproceedings{berenstein2018icra,
  author    = {R. Berenstein and R. Fox and S. McKinley and S. Carpin and K. Goldberg},
  title     = {Robustly Adjusting Indoor Drip Irrigation Emitters with the Toyota HSR Robot},
  booktitle = icra,
  year      = 2018,
  keywords  = {Agricultural Automation, Robotics in Agriculture and Forestry, Mobile Manipulation},
  abstract  = {Indoor plants in homes and commercial buildings such as malls, offices, airports, and hotels, can benefit from precision irrigation to maintain healthy growth and reduce water consumption. As active valves are too costly, and ongoing precise manual adjustment of drip emitters is impractical, we explore how the Toyota HSR mobile manipulator robot can autonomously adjust low-cost passive emitters. To provide sufficient accuracy for gripper alignment, we designed a lightweight, modular Emitter Localization Device (ELD) with cameras and LEDs that can be non-invasively mounted on the arm. This paper presents details of the design, algorithms, and experiments with adjusting emitters using a two-phase procedure: 1) aligning the robot base using the build-in hand camera, and 2) aligning the gripper axis with the emitter axis using the ELD. We report success rates and sensitivity analysis to tune computer vision parameters and joint motor gains. Experiments suggest that emitters can be adjusted with 95\% success rate in approximately 20 seconds.}
}

@inproceedings{stein2018icra,
  author    = {G. Stein and N. Roy},
  title     = {GeneSIS-RT: Generating Synthetic Images for training Secondary Real-World Tasks},
  booktitle = icra,
  year      = 2018,
  keywords  = {Deep Learning in Robotics and Automation, Collision Avoidance, Semantic Scene Understanding},
  abstract  = {We propose a novel approach for generating highquality, synthetic data for domain-specific learning tasks, for which training data may not be readily available. We leverage recent progress in image-to-image translation to bridge the gap between simulated and real images, allowing us to generate realistic training data for real-world tasks using only unlabeled real-world images and a simulation. GeneSIS-RT ameliorates the burden of having to collect labeled real-world images and is a promising candidate for generating high-quality, domainspecific, synthetic data. To show the effectiveness of using GeneSIS-RT to create training data, we study two tasks: semantic segmentation and reactive obstacle avoidance. We demonstrate that learning algorithms trained using data generated by GeneSIS-RT make high-accuracy predictions and outperform systems trained on raw simulated data alone, and as well or better than those trained on real data. Finally, we use our data to train a quadcopter to fly 60 meters at speeds up to 3.4 m/s through a cluttered environment, demonstrating that our GeneSIS-RT images can be used to learn to perform mission-critical tasks.}
}

@inproceedings{stenborg2018icra,
  author    = {E. Stenborg and C. Toft and L. Hammarstrand},
  title     = {Long-Term Visual Localization Using Semantically Segmented Images},
  booktitle = icra,
  year      = 2018,
  keywords  = {Localization, Semantic Scene Understanding, Autonomous Vehicle Navigation},
  abstract  = {Robust cross-seasonal localization is one of the major challenges in long-term visual navigation of autonomous vehicles. In this paper, we exploit recent advances in semantic segmentation of images, i.e., where each pixel is assigned a label related to the type of object it represents, to attack the problem of long-term visual localization. We show that semantically labeled 3D point maps of the environment, together with semantically segmented images, can be efficiently used for vehicle localization without the need for detailed feature descriptors (SIFT, SURF, etc.). Thus, instead of depending on hand-crafted feature descriptors, we rely on the training of an image segmenter. The resulting map takes up much less storage space compared to a traditional descriptor based map. A particle filter based semantic localization solution is compared to one based on SIFT-features, and even with large seasonal variations over the year we perform on par with the larger and more descriptive SIFT-features, and are able to localize with an error below 1 m most of the time.}
}

@inproceedings{stumberg2018icra,
  author    = {L.v. Stumberg and V. Usenko and D. Cremers},
  title     = {Direct Sparse Visual-Inertial Odometry Using Dynamic Marginalization},
  booktitle = icra,
  year      = 2018,
  keywords  = {SLAM, Sensor Fusion, Localization},
  abstract  = {We present VI-DSO, a novel approach for visualinertial odometry, which jointly estimates camera poses and sparse scene geometry by minimizing photometric and IMU measurement errors in a combined energy functional. The visual part of the system performs a bundle-adjustment like optimization on a sparse set of points, but unlike key-point based systems it directly minimizes a photometric error. This makes it possible for the system to track not only corners, but any pixels with large enough intensity gradients. IMU information is accumulated between several frames using measurement preintegration, and is inserted into the optimization as an additional constraint between keyframes. We explicitly include scale and gravity direction into our model and jointly optimize them together with other variables such as poses. As the scale is often not immediately observable using IMU data this allows us to initialize our visual-inertial system with an arbitrary scale instead of having to delay the initialization until everything is observable. We perform partial marginalization of old variables so that updates can be computed in a reasonable time. In order to keep the system consistent we propose a novel strategy which we call dynamic marginalization. This technique allows us to use partial marginalization even in cases where the initial scale estimate is far from the optimum. We evaluate our method on the challenging EuRoC dataset, showing that VI-DSO outperforms the state of the art.}
}

@inproceedings{ta2018icra-fnao,
  author    = {D. Ta and N. Banerjee and S. Eick and S. Lenser and M.E. Munich},
  title     = {Fast Nonlinear Approximation of Pose Graph Node Marginalization},
  booktitle = icra,
  year      = 2018,
  keywords  = {SLAM, Mapping, Localization},
  abstract  = {We present a fast nonlinear approximation method for marginalizing out nodes on pose graphs for longterm simultaneous localization, mapping, and navigation. Our approximation preserves the pose graph structure to leverage the rich literature of pose graphs and optimization schemes. By re-parameterizing from absolute- to relative-pose spaces, our method does not suffer from the choice of linearization points as in previous works. We then join our approximation process with a scaled version of the recently-demoted pose-composition approach. Our approach eschews the expenses of many stateof-the-art convex optimization schemes through our efficient and simple O(N 2 ) implementation for a given known topology of the approximate subgraph. We demonstrate its speed and near optimality in practice by comparing against state-of-theart techniques on popular datasets.}
}

@inproceedings{foroutan2018icra,
  author    = {V. Foroutan and F. Farzami and D. Erricolo and R. Majumdar and I. Paprotny},
  title     = {SAT-C: An Efficient Control Strategy for Assembly of Heterogeneous Stress-Engineered MEMS Microrobots},
  booktitle = icra,
  year      = 2018,
  keywords  = {Micro/Nano Robots, Assembly, Multi-Robot Systems},
  abstract  = {We present a new efficient control framework for controlling groups of heterogeneous stress-engineered MEMS microrobots for accomplishing micro-assembly. The objective is to maximize the number of controllable microrobots in the system while keeping the number of external global signals as low as possible. This work proposes a theoretical control strategy that could complete multiple-shapes microassembly from arbitrary initial configuration where all the control primitives can be accompanied with a constant number (O(1)) of control pulses of the power delivery waveform. We focus on microrobotic systems that can be modeled as nonholonomic unicycles. We validate the control policy with hardware experiments for implementing planar assembly using multiple macroscale robots with direct drive wheels. These results lay the foundation for developing new methods to control of a large number of MEMS microrobots.}
}

@inproceedings{tan2018icra,
  author    = {Y.T. Tan and A. Kunapareddy and M. Kobilarov},
  title     = {Gaussian Process Adaptive Sampling using the Cross-Entropy Method for Environmental Sensing and Monitoring},
  booktitle = icra,
  year      = 2018,
  keywords  = {Marine Robotics, Learning and Adaptive Systems, Autonomous Vehicle Navigation},
  abstract  = {In this paper, we focus on adaptive sampling on a Gaussian Processes (GP) using the receding-horizon Cross-Entropy (CE) trajectory optimization. Specifically, we employ the GP upper confidence bound (GP-UCB) as the optimization criteria to adaptively plan sampling paths that balance the exploitation-exploration trade-off. Path planning at the initial stage focuses on exploring and learning a model of the environment, and later, on exploiting the learned model to focus sampling around regions that exhibit extreme sensory measurements and much higher spatial variability, denoted as the Region of Interest (ROI). The integration of the CE trajectory optimization allows the sampling density to be dynamically adjusted based on the latest sensory measurements, thus providing an efficient sampling strategy for sensing and localizing the ROI. We demonstrate the effectiveness of the proposed method in exploring simulated scalar fields with single or multiple ROIs. Field experiments with an Unmanned Surface Vehicle (USV) in a coastal bathymetry mapping mission validate the approachs capability in quickly exploring and mapping the given area, and then focusing and increasing the sampling density around the deepest region, as a surrogate for e.g. the extremal concentration of a pollutant in the environment.}
}

@inproceedings{tan2018icra-hdiu,
  author    = {W.C. Tan and C. Weng and Y. Zhou and K.H. Chua and I. Chen},
  title     = {{Historical Data Is Useful for Navigation Planning: Data Driven Route Generation for Autonomous Ship}},
  booktitle = icra,
  year      = 2018,
  keywords  = {Intelligent Transportation Systems, Marine Robotics, Robot Safety},
  abstract  = {This work presents a method for automated generation of navigation plan for autonomous or robotic surface vessel. Historical Automatic Identification System (AIS) data is of significant value to this problem. The method joins AIS locations of a same vessel at different time and locations in a region into a route. Next, it automatically computes navigation plans using nearest neighbour based path retrieval relying on two representations, Ship Feature and Navigation Feature. Before starting service, existing AIS records in the form of ship properties and corresponding route are preprocessed and stored in the form of Ship and Navigation Feature. During online retrieval, given input constraints in vector form, nearest neighbour of this query vector in the same space is found and corresponding path of the neighbour is returned as recommended path. Analysis was done in four and two dimensional spaces for Ship and Navigation Feature respectively. Application of the method is demonstrated in two regions of Australian, covering Bass Strait and Great Australian Bight.}
}

@inproceedings{mendez2018icra,
  title     = {{Sedar-semantic detection and ranging: Humans can localise without lidar, can robots?}},
  author    = {Mendez, Oscar and Hadfield, Simon and Pugeault, Nicolas and Bowden, Richard},
  booktitle = icra,
  year      = {2018},
  url       = {proceedings: mendez2018icra.pdf}
}

@article{tang2018ral-gcnf,
  author   = {J. Tang and J. Folkesson and P. Jensfelt},
  title    = {{Geometric Correspondence Network for Camera Motion Estimation}},
  journal  = ral,
  volume   = {3},
  number   = {2},
  pages    = {1010--1017},
  year     = 2018,
  keywords = {SLAM, Visual-Based Navigation, Visual Learning},
  abstract = {In this paper, we propose a new learning scheme for generating geometric correspondences to be used for visual odometry. A convolutional neural network (CNN) combined with a recurrent neural network (RNN) are trained together to detect the location of keypoints as well as to generate corresponding descriptors in one unified structure. The network is optimized by warping points from source frame to reference frame, with a rigid body transform. Essentially, learning from warping. The overall training is focused on movements of the camera rather than movements within the image, which leads to better consistency in the matching and ultimately better motion estimation. Experimental results show that the proposed method achieves better results than both related deep learning and hand crafted methods. Furthermore, as a demonstration of the promise of our method we use a naive SLAM implementation based on these keypoints and get a performance on par with ORB-SLAM.}
}

@inproceedings{tanner2018icra,
  author    = {M. Tanner and S. Saftescu and A. Bewley and P. Newman},
  title     = {Meshed Up: Learnt Error Correction in 3D Reconstructions},
  booktitle = icra,
  year      = 2018,
  keywords  = {Deep Learning in Robotics and Automation, Mapping, Computer Vision for Other Robotic Applications},
  abstract  = {Dense reconstructions often contain errors that prior work has so far minimised using high quality sensors and regularising the output. Nevertheless, errors still persist. This paper proposes a machine learning technique to identify errors in three dimensional (3D) meshes. Beyond simply identifying errors, our method quantifies both the magnitude and the direction of depth estimate errors when viewing the scene. This enables us to improve the reconstruction accuracy. We train a suitably deep network architecture with two 3D meshes: a high-quality laser reconstruction, and a lower quality stereo image reconstruction. The network predicts the amount of error in the lower quality reconstruction with respect to the high-quality one, having only view the former through its input. We evaluate our approach by correcting two dimensional (2D) inverse-depth images extracted from the 3D model, and show that our method improves the quality of these depth reconstructions by up to a relative 10\% RMSE.}
}

@inproceedings{thayer2018icra,
  author    = {T.C. Thayer and S. Vougioukas and K. Goldberg and S. Carpin},
  title     = {Routing Algorithms for Robot Assisted Precision Irrigation},
  booktitle = icra,
  year      = 2018,
  keywords  = {Robotics in Agriculture and Forestry, Agricultural Automation, Motion and Path Planning},
  abstract  = {When robots navigate through vineyards to perform irrigation adjustments, an optimization problem emerges whereby robots are tasked with performing adjustments having the highest cumulative outcome within a given temporal budget due to limited battery charge. To this end, the robot needs to reach a set of spatially distributed sites, and the specific structure of the vineyard imposes various constraints on possible motions. In this paper we first demonstrate that this type of orienteering problem remains NP-hard even for the restricted class of graphs associated with precision irrigation. Then, we devise and analyze two greedy heuristics informed by the problem we consider. Finally, these algorithms are evaluated on settings associated with a commercial vineyard and we show that our methods favorably compare to solutions proposed in the past.}
}

@inproceedings{valada2018icra,
  author    = {A. Valada and N. Radwan and W. Burgard},
  title     = {Deep Auxiliary Learning for Visual Localization and Odometry},
  booktitle = icra,
  year      = 2018,
  keywords  = {Deep Learning in Robotics and Automation, Visual Learning, Localization},
  abstract  = {Localization is an indispensable component of a robots autonomy stack that enables it to determine where it is in the environment, essentially making it a precursor for any action execution or planning. Although convolutional neural networks have shown promising results for visual localization, they are still grossly outperformed by state-of-the-art local feature-based techniques. In this work, we propose VLocNet, a new convolutional neural network architecture for 6-DoF global pose regression and odometry estimation from consecutive monocular images. Our multitask model incorporates hard parameter sharing, thus being compact and enabling real-time inference, in addition to being end-to-end trainable. We propose a novel loss function that utilizes auxiliary learning to leverage relative pose information during training, thereby constraining the search space to obtain consistent pose estimates. We evaluate our proposed VLocNet on indoor as well as outdoor datasets and show that even our single task model exceeds the performance of state-of-the-art deep architectures for global localization, while achieving competitive performance for visual odometry estimation. Furthermore, we present extensive experimental evaluations utilizing our proposed Geometric Consistency Loss that show the effectiveness of multitask learning and demonstrate that our model is the first deep learning technique to be on par with, and in some cases outperforms state-of-theart SIFT-based approaches.}
}

@inproceedings{valls2018icra,
  author    = {M.d.l.I. Valls and H.F.C. Hendrikx and V. Reijgwart and F.V. Meier and I. Sa and R. Dub and A.R. Gawel and M. Brki and R. Siegwart},
  title     = {Design of an Autonomous Racecar: Perception, State Estimation and System Integration},
  booktitle = icra,
  year      = 2018,
  keywords  = {Autonomous Vehicle Navigation, Sensor Fusion, SLAM},
  abstract  = {This paper introduces fluela driverless: the first autonomous racecar to win a Formula Student Driverless competition. In this competition, among other challenges, an autonomous racecar is tasked to complete 10 laps of a previously unknown racetrack as fast as possible and using only onboard sensing and computing. The key components of fluelas design are its modular redundant subsystems that allow robust performance despite challenging perceptual conditions or partial system failures. The paper presents the integration of key components of our autonomous racecar, i.e., system design, EKFbased state estimation, LiDARbased perception, and particle filter-based SLAM. We perform an extensive experimental evaluation on realworld data, demonstrating the systems effectiveness by outperforming the nextbest ranking team by almost half the time required to finish a lap. The autonomous racecar reaches lateral and longitudinal accelerations comparable to those achieved by experienced human drivers.}
}

@inproceedings{vaquero2018icra,
  author    = {V. Vaquero and A. Sanfeliu and F. Moreno-Noguer},
  title     = {Deep Lidar CNN to Understand the Dynamics of Moving Vehicles},
  booktitle = icra,
  year      = 2018,
  keywords  = {Semantic Scene Understanding, Object Detection, Segmentation and Categorization, Deep Learning in Robotics and Automation},
  abstract  = {Perception technologies in Autonomous Driving are experiencing their golden age due to the advances in Deep Learning. Yet, most of these systems rely on the semantically rich information of RGB images. Deep Learning solutions applied to the data of other sensors typically mounted on autonomous cars (e.g. lidars or radars) are not explored much. In this paper we propose a novel solution to understand the dynamics of moving vehicles of the scene from only lidar information. The main challenge of this problem stems from the fact that we need to disambiguate the proprio-motion of the observer vehicle from that of the external observed vehicles. For this purpose, we devise a CNN architecture which at testing time is fed with pairs of consecutive lidar scans. However, in order to properly learn the parameters of this network, during training we introduce a series of so-called pretext tasks which also leverage on image data. These tasks include semantic information about vehicleness and a novel lidar-flow feature which combines standard image-based optical flow with lidar scans. We obtain very promising results and show that including distilled image information only during training, allows improving the inference results of the network at test time, even when image data is no longer used.}
}

@article{vespa2018ral,
  author   = {E. Vespa and N. Nikolov and M. Grimm and L. Nardi and P.H.J. Kelly and S. Leutenegger},
  title    = {Efficient Octree-Based Volumetric SLAM Supporting Signed-Distance and Occupancy Mapping},
  journal  = ral,
  year     = 2018,
  keywords = {Mapping, SLAM, Visual-Based Navigation},
  volume   = {3},
  number   = {2},
  pages    = {1144--1151},
  abstract = {We present a dense volumetric SLAM framework that uses an octree representation for efficient fusion and rendering of either a truncated signed distance field (TSDF) or occupancy map. The primary aim of this work is to use one single representation of the environment that can be used not only for robot pose tracking, and high-resolution mapping, but seamlessly for planning. We show that our highly efficient octree representation of space fits SLAM and planning purposes in a real-time control loop. In a comprehensive evaluation, we demonstrate dense SLAM accuracy and runtime performance on-par with flat hashing approaches when using TSDF-based maps, and considerable speed-ups when using occupancy mapping compared to standard occupancy maps frameworks. Our SLAM system can run at 10-40 Hz on a modern quadcore CPU, without the need for massive parallelisation on a GPU. We furthermore demonstrate a probabilistic occupancy mapping as an alternative to TSDF mapping in dense SLAM and show its direct applicability to online motion planning, using the example of Informed RRT*.}
}

@article{vidal2018ral,
  author   = {A.R. Vidal and H. Rebecq and T. Horstschfer and D. Scaramuzza},
  title    = {{Ultimate SLAM? Combining Events, Images, and IMU for Robust Visual SLAM in HDR and High Speed Scenarios}},
  journal  = ral,
  volume   = {3},
  number   = {2},
  pages    = {994--1001},
  year     = 2018,
  keywords = {Visual-Based Navigation, Aerial Systems: Perception and Autonomy},
  abstract = {Event cameras are bio-inspired vision sensors that output pixel-level brightness changes instead of standard intensity frames. These cameras do not suffer from motion blur and have a very high dynamic range, which enables them to provide reliable visual information during high speed motions or in scenes characterized by high dynamic range. However, event cameras output only little information when the amount of motion is limited, such as in the case of almost still motion. Conversely, standard cameras provide instant and rich information about the environment most of the time (in low-speed and good lighting scenarios), but they fail severely in case of fast motions, or difficult lighting such as high dynamic range or low light scenes. In this paper, we present the first state estimation pipeline that leverages the complementary advantages of these two sensors by fusing in a tightly-coupled manner events, standard frames, and inertial measurements. We show on the publicly available Event Camera Dataset that our hybrid pipeline leads to an accuracy improvement of 130\% over event-only pipelines, and 85\% over standard-framesonly visual-inertial systems, while still being computationally tractable. Furthermore, we use our pipeline to demonstrate to the best of our knowledgethe first autonomous quadrotor flight using an event camera for state estimation, unlocking flight scenarios that were not reachable with traditional visualinertial odometry, such as low-light environments and highdynamic range scenes.}
}

@inproceedings{wang2018icra-arsa,
  author    = {X. Wang and R. Marcotte and G. Ferrer and E. Olson},
  title     = {AprilSAM: Real-Time Smoothing and Mapping},
  booktitle = icra,
  year      = 2018,
  keywords  = {SLAM, Field Robots},
  abstract  = {For online robots, incremental SLAM algorithms offer huge potential computational savings over batch algorithms. The dominant incremental algorithms are iSAM and iSAM2 which offer radically different approaches to computing incremental updates, balancing issues like 1) the need to relinearize, 2) changes in the desirable variable marginalization order, and 3) the underlying conceptual approach (i.e. the matrix story versus the factor graph story). In this paper, we propose a new incremental algorithm that computes solutions with lower absolute error and generally provides lower error solutions for a fixed computational budget than either iSAM or iSAM2. Key to AprilSAMs performance are a new dynamic variable reordering algorithm for fast incremental Cholesky factorizations, a method for reducing the work involved in backsubstitutions, and a new algorithm for deciding between incremental and batch updates.}
}

@inproceedings{wang2018icra-emre,
  author    = {C. Wang and T. Li and C.d. Silva and M.Q. Meng},
  title     = {Efficient Mobile Robot Exploration with Gaussian Markov Random Fields in 3D Environments},
  booktitle = icra,
  year      = 2018,
  keywords  = {Autonomous Agents, Mapping, Planning, Scheduling and Coordination},
  abstract  = {In this paper, we study the problem of autonomous exploration in unknown indoor environments using mobile robot. We use mutual information (MI) to evaluate the information the robot would get at a certain location. In order to get the most informative sensing location, we first propose a sampling method that can get random sensing patches in free space. Each sensing patch is extended to informative locations to collect information with true values. Then we use Gaussian Markov Random Fields (GMRF) to model the distribution of MI in environment. Compared with the traditional methods that employ Gaussian Process (GP) model, GMRF is more efficient. MI of every sensing location can be estimated using the training sample patches and the established GMRF model. We utilize an efficient computation algorithm to estimate the GMRF model hyperparameters so as to speed up the computation. Besides the information gain of the candidates regions, the path cost is also considered in this work. We propose a utility function that can balance the path cost and the information gain the robot would collect. We tested our algorithm in both simulated and real experiment. The experiment results demonstrate that our proposed method can explore the environment efficiently with relatively shorter path length.}
}

@inproceedings{wang2018icra-mvos,
  author    = {X. Wang and H. Zhang and X. Yin and M. Du and Q. Chen},
  title     = {Monocular Visual Odometry Scale Recovery Using Geometrical Constraint},
  booktitle = icra,
  year      = 2018,
  keywords  = {Localization, SLAM},
  abstract  = {Scale recovery is one of the essential problems for monocular visual odometry. The camera height is usually used as an absolute reference to recover the scale. In this case, the precision of scale recovery depends on the accuracy of the road region detection and road geometrical model calculation. In previous works, road detection and road geometrical model calculation are solved sequentially: the road geometrical model calculation is based on the road detection and the road region detection is based on the color information. However, the color information of a road is not stable enough. In the proposed method, the estimated road geometrical model is taken into consideration to detect the road region as a feedback. Therefore, the road region detection and road geometrical model estimation can benefit each other. Delaunay Triangulation method is used to segment an input image to many triangles with the matched feature points as vertices. Every triangle region is classified as a road region or not by comparing their geometrical model with that of the road and the road geometrical model is updated online. We evaluate our visual odometry scale recovery method on the KITTI dataset and the results show that our method is achieving the best performance among all existing monocular visual odometry scale recovery methods without additional sensors.}
}

@inproceedings{weerasekera2018icra,
  author    = {C.S. Weerasekera and T. Dharmasiri and R. Garg and T. Drummond and I. Reid},
  title     = {Just-In-Time Reconstruction: Inpainting Sparse Maps Using Single View Depth Predictors As Priors},
  booktitle = icra,
  year      = 2018,
  keywords  = {Mapping, Sensor Fusion, Computer Vision for Other Robotic Applications},
  abstract  = {We present just-in-time reconstruction as realtime image-guided inpainting of a map with arbitrary scale and sparsity to generate a fully dense depth map for the image. In particular, our goal is to inpaint a sparse map obtained from either a monocular visual SLAM system or a sparse sensor using a single-view depth prediction network as a virtual depth sensor. We adopt a fairly standard approach to data fusion, to produce a fused depth map by performing inference over a novel fully-connected Conditional Random Field (CRF) which is parameterized by the input depth maps and their pixel-wise confidence weights. Crucially, we obtain the confidence weights that parameterize the CRF model in a data-dependent manner via Convolutional Neural Networks (CNNs) which are trained to model the conditional depth error distributions given each source of input depth map and the associated RGB image. Our CRF model penalises absolute depth error in its nodes and pairwise scale-invariant depth error in its edges, and the confidence-based fusion minimizes the impact of outlier input depth values on the fused result. We demonstrate the flexibility of our method by real-time inpainting of ORB-SLAM, Kinect, and LIDAR depth maps acquired both indoors and outdoors at arbitrary scale and varied amount of irregular sparsity.}
}

@inproceedings{wei2018icra,
  author    = {M. Wei and V. Isler},
  title     = {Coverage Path Planning under the Energy Constraint},
  booktitle = icra,
  year      = 2018,
  keywords  = {Motion and Path Planning, Planning, Scheduling and Coordination, Energy and Environment-Aware Automation},
  abstract  = {In the coverage path planning problem, a common assumption is that the robot can fully cover the environment without recharging. However, in reality most mobile robot systems operate under battery limitations. To incorporate this constraint, we consider the problem when the working environment is large and the robot needs to recharge multiple times to fully cover the environment. We focus on a geometric version where the environment is represented as a polygonal grid with a single charging station. Energy consumption throughout the environment is assumed to be uniform and proportional to the distance traveled. We first present a constant-factor approximation algorithm for contour-connected environments. We then extend the algorithm for general environments. We also validate the results in experiments performed with an aerial robot.}
}

@inproceedings{wei2018icra-iaar,
  author    = {B. Wei and N. Trigoni and A. Markham},
  title     = {Imag: Accurate and Rapidly Deployable Inertial Magneto-Inductive Localisation},
  booktitle = icra,
  year      = 2018,
  keywords  = {SLAM, Sensor Fusion},
  abstract  = {Localisation is of importance for many applications. Our motivating scenarios are short-term construction work and emergency rescue. Not only is accuracy necessary, these scenarios also require rapid setup and robustness to environmental conditions. These requirements preclude the use of many traditional methods e.g. vision-based, laser-based, Ultrawide band (UWB) and Global Positioning System (GPS)-based localisation systems. To solve these challenges, we introduce iMag, an accurate and rapidly deployable inertial magnetoinductive (MI) localisation system. It localises monitored workers using a single MI transmitter and inertial measurement units with minimal setup effort. However, MI location estimates can be distorted and ambiguous. To solve this problem, we suggest a novel method to use MI devices for sensing environmental distortions, and use these to correctly close inertial loops. By applying robust simultaneous localisation and mapping (SLAM), our proposed localisation method achieves excellent tracking accuracy, and can improve performance significantly compared with only using an inertial measurement unit (IMU) and MI device for localisation. Keywords: Magneto-inductive device; Inertial measurements; Localisation; SLAM}
}

@inproceedings{smitt2021icra,
  author    = {Smitt, Claus and Halstead, Michael and
               Zaenker, Tobias and Bennewitz, Maren and
               McCool, Chris},
  booktitle = icra,
  title     = {{PATHoBot}: {A} Robot for Glasshouse Crop Phenotyping and Intervention},
  year      = {2021},
  url       = {proceedings: smitt2021icra.pdf}
}


@inproceedings{wulfmeier2018icra,
  author    = {M. Wulfmeier and A. Bewley and I. Posner},
  title     = {Incremental Adversarial Domain Adaptation for Continually Changing Environments},
  booktitle = icra,
  year      = 2018,
  keywords  = {Deep Learning in Robotics and Automation, Field Robots, Computer Vision for Transportation},
  abstract  = {Continuous appearance shifts such as changes in weather and lighting conditions can impact the performance of deployed machine learning models. While unsupervised domain adaptation aims to address this challenge, current approaches do not utilise the continuity of the occurring shifts. In particular, many robotics applications exhibit these conditions and thus facilitate the potential to incrementally adapt a learnt model over minor shifts which integrate to massive differences over time. Our work presents an adversarial approach for lifelong, incremental domain adaptation which benefits from unsupervised alignment to a series of intermediate domains which successively diverge from the labelled source domain. We empirically demonstrate that our incremental approach improves handling of large appearance changes, e.g. day to night, on a traversable-path segmentation task compared with a direct, single alignment step approach. Furthermore, by approximating the feature distribution for the source domain with a generative adversarial network, the deployment module can be rendered fully independent of retaining potentially large amounts of the related source training data for only a minor reduction in performance.}
}

@inproceedings{zhang2018icra-lpbd,
  author    = {N. Zhang and M. Warren and T. Barfoot},
  title     = {Learning Place-And-Time-Dependent Binary Descriptors for Long-Term Visual Localization},
  booktitle = icra,
  year      = 2018,
  keywords  = {Visual Learning, Localization, Autonomous Vehicle Navigation},
  abstract  = {Vision-based navigation is extremely susceptible to natural scene changes. This can result in localization failures in less than a few hours after map creation. To combat short-term illumination changes as well as long-term seasonal variations, we propose using a place-and-time-dependent binary descriptor that adapts to different scenarios in an online fashion. This is achieved by extending the GRIEF [6] evolution algorithm in two ways: correspondence generation using a known pose change and the inclusion of LATCH triplets in addition to BRIEF comparisons for descriptor generation. We show the adaptive descriptor outperforms a single descriptor scheme for localization within a single-experience Visual Teach and Repeat (VT\&R) system while maintaining the efficiency of binary descriptors. By adapting the description function to different environmental conditions, it allows the system to operate for a longer period before a new experience is required. In the presence of extreme illumination changes from day to night, we obtain 40\% more inlier matches compared to SURF. In the case of seasonal variations, a 70\% increase is demonstrated. The increased correspondences result in more localizable sections along the paths, amounting to a 25\% and 150\% increase in the lighting and seasonal cases, respectively.}
}

@inproceedings{zhang2018icra-prhn,
  author    = {Z. Zhang and D. Scaramuzza},
  title     = {Perception-aware Receding Horizon Navigation for MAVs},
  booktitle = icra,
  year      = 2018,
  keywords  = {Aerial Systems: Perception and Autonomy, Visual-Based Navigation},
  abstract  = {To reach a given destination safely and accurately, a micro aerial vehicle needs to be able to avoid obstacles and minimize its state estimation uncertainty at the same time. To achieve this goal, we propose a perception-aware receding horizon approach. In our method, a single forwardlooking camera is used for state estimation and mapping. Using the information from the monocular state estimation and mapping system, we generate a library of candidate trajectories and evaluate them in terms of perception quality, collision probability, and distance to the goal. The best trajectory to execute is then selected as the one that maximizes a reward function based on these three metrics. To the best of our knowledge, this is the first work that integrates active vision within a receding horizon navigation framework for a goal reaching task. We demonstrate by simulation and real-world experiments on an actual quadrotor that our active approach leads to improved state estimation accuracy in a goal-reaching task when compared to a purely-reactive navigation system, especially in difficult scenes (e.g., weak texture).}
}

@inproceedings{zhou2018icra,
  author    = {W. Zhou and S. Worrall and A. Zyner and E. Nebot},
  title     = {Automated Process for Incorporating Drivable Path into Real-Time Semantic Segmentation},
  booktitle = icra,
  year      = 2018,
  keywords  = {Computer Vision for Transportation, Sensor Fusion},
  abstract  = {Vision systems are widely used in autonomous vehicle systems due to the rich information that camera sensors provide of the surrounding environment. This paper presents an automatic algorithm to obtain the drivable path of a vehicle operating in urban roads with or without clear lane markings. The developed system projects trajectories obtained during human operation of the vehicle and utilizes these to generate automatic labels for training a semantic based path prediction model. The system segments an urban scenario into 13 categories including vehicles, pedestrian, undrivable road, other categories relevant to urban roads, and a new class for a path proposal. The drivable path information is essential particularly in unstructured scenarios, and is critical for an intelligent vehicle system to make sound driving decisions. The path proposal category is a car-width drivable lane estimated to be safe to drive for the vehicle under consideration. The data collection, model training and inference process requires only images from a monocular camera and odometry from a low-cost IMU combined with a wheel encoder. The algorithm has been successfully demonstrated on the Sydney University campus, which is a challenging environment without clear road markings. The algorithm was demonstrated to run in real-time, proving its applicability for intelligent vehicles.}
}

@inproceedings{zhou2018icra-etpw,
  author    = {T. Zhou and J. Wachs},
  title     = {Early Turn-Taking Prediction with Spiking Neural Networks for Human Robot Collaboration},
  booktitle = icra,
  year      = 2018,
  keywords  = {Physical Human-Robot Interaction, Deep Learning in Robotics and Automation, Medical Robots and Systems},
  abstract  = {Turn-taking is essential to the structure of human teamwork. Humans are typically aware of team members intention to keep or relinquish their turn before a turn switch, where the responsibility of working on a shared task is shifted. Future co-robots are also expected to provide such competence. To that end, this paper proposes the Cognitive Turn-taking Model (CTTM), which leverages cognitive models (i.e., Spiking Neural Network) to achieve early turn-taking prediction. The CTTM framework can process multimodal human communication cues (both implicit and explicit) and predict human turntaking intentions in an early stage. The proposed framework is tested on a simulated surgical procedure, where a robotic scrub nurse predicts the surgeons turn-taking intention. It was found that the proposed CTTM framework outperforms the state-of-the-art turn-taking prediction algorithms by a large margin. It also outperforms humans when presented with partial observations of communication cues (i.e., less than 40\% of full actions). This early prediction capability enables robots to initiate turn-taking actions at an early stage, which facilitates collaboration and increases overall efficiency.}
}

@inproceedings{zhou2018icra-hmcu,
  author    = {X. Zhou and S. Liu and G. Pavlakos and V. Kumar and K. Daniilidis},
  title     = {Human Motion Capture Using a Drone},
  booktitle = icra,
  year      = 2018,
  keywords  = {Human Detection and Tracking, Computer Vision for Other Robotic Applications, Aerial Systems: Applications},
  abstract  = {Current motion capture (MoCap) systems generally require markers and multiple calibrated cameras, which can be used only in constrained environments. In this work we introduce a drone-based system for 3D human MoCap. The system only needs an autonomously flying drone with an on-board RGB camera and is usable in various indoor and outdoor environments. A reconstruction algorithm is developed to recover full-body motion from the video recorded by the drone. We argue that, besides the capability of tracking a moving subject, a flying drone also provides fast varying viewpoints, which is beneficial for motion reconstruction. We evaluate the accuracy of the proposed system using our new DroCap dataset and also demonstrate its applicability for MoCap in the wild using a consumer drone.}
}

@inproceedings{zhou2018icra-jmbe,
  author    = {B. Zhou and W. Schwarting and D. Rus and J. Alonso-Mora},
  title     = {Joint Multi-Policy Behavior Estimation and Receding-Horizon Trajectory Planning for Automated Urban Driving},
  booktitle = icra,
  year      = 2018,
  keywords  = {Path Planning for Multiple Mobile Robots or Agents, Motion and Path Planning, Nonholonomic Motion Planning},
  abstract  = {When driving in urban environments, an autonomous vehicle must account for the interaction with other traffic participants. It must reason about their future behavior, how its actions affect their future behavior, and potentially consider multiple motion hypothesis. In this paper we introduce a method for joint behavior estimation and trajectory planning that models interaction and multi-policy decisionmaking. The method leverages Partially Observable Markov Decision Processes to estimate the behavior of other traffic participants given the planned trajectory for the ego-vehicle, and Receding-Horizon Control for generating safe trajectories for the ego-vehicle. To achieve safe navigation we introduce chance constraints over multiple motion policies in the recedinghorizon planner. These constraints account for uncertainty over the behavior of other traffic participants. The method is capable of running in real-time and we show its performance and good scalability in simulated multi-vehicle intersection scenarios.}
}

@inproceedings{zhou2018icra-pntl,
  author    = {M. Zhou and K. Huang and A. Eslami and H. Roodaki and D. Zapp and M. Maier and C.P. Lohmann and A. Knoll and M.A. Nasseri},
  title     = {Precision Needle Tip Localization Using Optical Coherence Tomography Images for Subretinal Injection},
  booktitle = icra,
  year      = 2018,
  keywords  = {Computer Vision for Medical Robotics, Medical Robots and Systems},
  abstract  = {Subretinal injection is a delicate and complex microsurgery, which requires surgeons to inject the therapeutic substance in a pre-operatively defined and intra-operatively updated subretinal target area. Due to the lack of subretinal visual feedback, it is hard to sense the insertion depth during the procedure, thus affecting the results of surgical outcome and hindering the widespread use of this treatment. This paper presents a novel approach to estimate the 3D position of the needle under the retina using the information from microscope-integrated Intraoperative Optical Coherence Tomography (iOCT). We evaluated our approach on both tissue phantom and ex-vivo porcine eyes. Evaluation results show that the average error in distance measurement is 4.7 m (maximum of 16.5 m). We furthermore, verified the feasibility of the proposed method to track the insertion depth of needle in robotassisted subretinal injection.}
}

@inproceedings{zhou2018icra-hmra,
  author    = {X. Zhou and H. Wang and B. Ding},
  title     = {How Many Robots Are Enough: A Multi-Objective Genetic Algorithm for the Single-Objective Time-Limited Complete Coverage Problem},
  booktitle = icra,
  year      = 2018,
  keywords  = {Path Planning for Multiple Mobile Robots or Agents, Task Planning, Optimization and Optimal Control},
  abstract  = {Complete coverage, which is the foundation of many robotic applications, aims to cover an area as quickly as possible. This study investigates the time-limited version of multi-robot complete coverage problem, that is, to find the least number of robots and allocate tasks properly to them such that they can finish a known mission within the time limit. This version of problem can be tackled straightforwardly based on optimizing the task-allocation to a fixed number of robots and enumerating the number. However, the number-fixed problem is NP-hard and the existing algorithm for the numberfixed problem allows intersecting tasks (possibly causing robots interference) and endures high approximation factor. In this study, the time-limited complete coverage problem is tackled with a multi-objective approach, instead of enumerating robots number and optimizing each number-fixed problem one by one. The multi-objective GA, Mofint, at first estimates the lower and upper bounds of the number of robots. It abstracts each task as a weighted node of a graph. Then, Mofint evolves individuals, each individual being a forest containing a certain number (within the bounds) of non-intersecting trees. Mofint can finally obtain higher precision than existing work with less time: the approximation factor for Mofint is 1.1 to 1.5 times the ideal allocation when robots number is fixed, while for existing work is 1.5 to 2. Due to its higher precision, the least number of robots obtained in the experiments by Mofint is 0.6 times of existing work.}
}


@article{christiansen2018arxiv,
  author   = {M.P. Christiansen and M.S. Laursen and R.N. Jorgensen and S. Skovsen and R. Gislum},
  title    = {{Ground vehicle mapping of fields using LiDAR to enable prediction of crop biomass}},
  journal  = arxiv,
  year     = 2018,
  volume   = {arXiv:1805.01426},
  url      = {http://arxiv.org/pdf/1805.01426v1},
  abstract = {Mapping field environments into point clouds using a 3D LIDAR has the ability to become a new approach for online estimation of crop biomass in the field. The estimation of crop biomass in agriculture is expected to be closely correlated to canopy heights. The work presented in this paper contributes to the mapping and textual analysis of agricultural fields. Crop and environmental state information can be used to tailor treatments to the specific site. This paper presents the current results with our ground vehicle LiDAR mapping systems for broad acre crop fields. The proposed vehicle system and method facilitates LiDAR recordings in an experimental winter wheat field. LiDAR data are combined with data from Global Navigation Satellite System (GNSS) and Inertial Measurement Unit (IMU) sensors to conduct environment mapping for point clouds. The sensory data from the vehicle are recorded, mapped, and analyzed using the functionalities of the Robot Operating System (ROS) and the Point Cloud Library (PCL). In this experiment winter wheat (Triticum aestivum L.) in field plots, was mapped using 3D point clouds with a point density on the centimeter level. The purpose of the experiment was to create 3D LiDAR point-clouds of the field plots enabling canopy volume and textural analysis to discriminate different crop treatments. Estimated crop volumes ranging from 3500-6200 (m3) per hectare are correlated to the manually collected samples of cut biomass extracted from the experimental field.}
}

@article{koppel2018arxiv,
  author   = {L. Koppel and S.L. Waslander},
  title    = {{Manifold Geometry with Fast Automatic Derivatives and Coordinate Frame Semantics Checking in C++}},
  journal  = arxiv,
  year     = 2018,
  volume   = {arXiv:1805.01810},
  url      = {http://arxiv.org/pdf/1805.01810v1},
  abstract = {Computer vision and robotics problems often require representation and estimation of poses on the SE(3) manifold. Developers of algorithms that must run in real time face several time-consuming programming tasks, including deriving and computing analytic derivatives and avoiding mathematical errors when handling poses in multiple coordinate frames. To support rapid and error-free development, we present wave_geometry, a C++ manifold geometry library with two key contributions: expression template-based automatic differentiation and compile-time enforcement of coordinate frame semantics. We contrast the library with existing open source packages and show that it can evaluate Jacobians in forward and reverse mode with little to no runtime overhead compared to hand-coded derivatives. The library is available at https://github.com/wavelab/wave_geometry .}
}

@article{forechi2018arxiv,
  author   = {A. Forechi and T. Oliveira-Santos and C. Badue and A.F.D. Souza},
  title    = {{Visual Global Localization with a Hybrid WNN-CNN Approach}},
  journal  = arxiv,
  year     = 2018,
  volume   = {arXiv:1805.03183},
  url      = {http://arxiv.org/pdf/1805.03183v2},
  abstract = {Currently, self-driving cars rely greatly on the Global Positioning System (GPS) infrastructure, albeit there is an increasing demand for alternative methods for GPS-denied environments. One of them is known as place recognition, which associates images of places with their corresponding positions. We previously proposed systems based on Weightless Neural Networks (WNN) to address this problem as a classification task. This encompasses solely one part of the global localization, which is not precise enough for driverless cars. Instead of just recognizing past places and outputting their poses, it is desired that a global localization system estimates the pose of current place images. In this paper, we propose to tackle this problem as follows. Firstly, given a live image, the place recognition system returns the most similar image and its pose. Then, given live and recollected images, a visual localization system outputs the relative camera pose represented by those images. To estimate the relative camera pose between the recollected and the current images, a Convolutional Neural Network (CNN) is trained with the two images as input and a relative pose vector as output. Together, these systems solve the global localization problem using the topological and metric information to approximate the current vehicle pose. The full approach is compared to a Real- Time Kinematic GPS system and a Simultaneous Localization and Mapping (SLAM) system. Experimental results show that the proposed approach correctly localizes a vehicle 90\% of the time with a mean error of 1.20m compared to 1.12m of the SLAM system and 0.37m of the GPS, 89\% of the time.}
}


@article{maini2018arxiv,
  author   = {P. Maini and K. Sundar and S. Rathinam and P. Sujit},
  title    = {{Cooperative Planning for Fuel-constrained Aerial Vehicles and Ground-based Refueling Vehicles for Large-Scale Coverage}},
  journal  = arxiv,
  year     = 2018,
  volume   = {arXiv:1805.04417},
  url      = {http://arxiv.org/pdf/1805.04417v1},
  abstract = {Low cost Unmanned Aerial Vehicles (UAVs) need multiple refuels to accomplish large area coverage. The number of refueling stations and their placement plays a vital role in determining coverage efficiency. In this paper, we propose the use of a ground-based refueling vehicle (RV) to increase the operational range of a UAV in both spatial and temporal domains. Determining optimal routes for the UAV and RV, and selecting optimized locations for refueling to aid in minimizing coverage time is a challenging problem due to different vehicle speeds, coupling between refueling location placement, and the coverage area at each location. We develop a two-stage strategy for coupled route planning for UAV and RV to perform a coverage mission. The first stage computes a minimal set of refueling sites that permit a feasible UAV route. In the second stage, multiple Mixed-Integer Linear Programming (MILP) formulations are developed to plan optimal routes for the UAV and the refueling vehicle taking into account the feasible set of refueling sites generated in stage one. The performance of different formulations is compared empirically. In addition, computationally efficient heuristics are developed to solve the routing problem. Extensive simulations are conducted to corroborate the effectiveness of proposed approaches.}
}

@article{xiao2018arxiv,
  author   = {Z. Xiao and K. Jiang and S. Xie and T. Wen and C. Yu and D. Yang},
  title    = {{Monocular Vehicle Self-localization method based on Compact Semantic Map}},
  journal  = arxiv,
  year     = 2018,
  volume   = {arXiv:1805.06155},
  url      = {http://arxiv.org/pdf/1805.06155v1},
  abstract = {High precision localization is a crucial requirement for the autonomous driving system. Traditional positioning methods have some limitations in providing stable and accurate vehicle poses, especially in an urban environment. Herein, we propose a novel self-localizing method using a monocular camera and a 3D compact semantic map. Pre-collected information of the road landmarks is stored in a self-defined map with a minimal amount of data. We recognize landmarks using a deep neural network, followed with a geometric feature extraction process which promotes the measurement accuracy. The vehicle location and posture are estimated by minimizing a self-defined re-projection residual error to evaluate the map-to-image registration, together with a robust association method. We validate the effectiveness of our approach by applying this method to localize a vehicle in an open dataset, achieving the RMS accuracy of 0.345 meter with reduced sensor setup and map storage compared to the state of art approaches. We also evaluate some key steps and discuss the contribution of the subsystems.}
}

@article{morreale2018arxiv,
  author   = {L. Morreale and A. Romanoni and M. Matteucci},
  title    = {{Predicting the Next Best View for 3D Mesh Refinement}},
  journal  = arxiv,
  year     = 2018,
  volume   = {arXiv:1805.06207},
  url      = {http://arxiv.org/pdf/1805.06207v1},
  abstract = {3D reconstruction is a core task in many applications such as robot navigation or sites inspections. Finding the best poses to capture part of the scene is one of the most challenging topic that goes under the name of Next Best View. Recently, many volumetric methods have been proposed; they choose the Next Best View by reasoning over a 3D voxelized space and by finding which pose minimizes the uncertainty decoded into the voxels. Such methods are effective, but they do not scale well since the underlaying representation requires a huge amount of memory. In this paper we propose a novel mesh-based approach which focuses on the worst reconstructed region of the environment mesh. We define a photo-consistent index to evaluate the 3D mesh accuracy, and an energy function over the worst regions of the mesh which takes into account the mutual parallax with respect to the previous cameras, the angle of incidence of the viewing ray to the surface and the visibility of the region. We test our approach over a well known dataset and achieve state-of-the-art results.}
}

@article{ruenz2018arxiv,
  author   = {M. R{\"u}nz and L. Agapito},
  title    = {{MaskFusion: Real-Time Recognition, Tracking and Reconstruction of Multiple Moving Objects}},
  journal  = arxiv,
  year     = 2018,
  volume   = {arXiv:1804.09194},
  url      = {http://arxiv.org/pdf/1804.09194v1},
  abstract = {We present MaskFusion, a real-time, object-aware, semantic and dynamic RGB-D SLAM system that goes beyond traditional systems that output a geometry-only map -- MaskFusion recognizes, segments and assigns semantic class labels to different objects in the scene, while tracking and reconstructing them even when they move independently from the camera.   As an RGB-D camera scans a cluttered scene, image-based instance-level semantic segmentation creates semantic object masks that enable real-time object recognition and the creation of an object-level representation for the world map. Unlike previous recognition-based SLAM systems, MaskFusion does not require prior knowledge or known models of the objects it can recognize and can deal with multiple independent motions. Unlike recent semantics enabled SLAM systems that perform voxel-level semantic segmentation MaskFusion takes full advantage of using instance-level semantic segmentation to enable semantic labels to be fused into an object-aware map. We show augmented-reality applications, that demonstrate the unique features of the map output by MaskFusion: instance-aware, semantic and dynamic.}
}

@article{younes2018arxiv,
  author   = {G. Younes and D. Asmar and J. Zelek},
  title    = {{FDMO: Feature Assisted Direct Monocular Odometry}},
  journal  = arxiv,
  year     = 2018,
  volume   = {arXiv:1804.05422},
  url      = {http://arxiv.org/pdf/1804.05422v1},
  abstract = {Visual Odometry (VO) can be categorized as being either direct or feature based. When the system is calibrated photometrically, and images are captured at high rates, direct methods have shown to outperform feature-based ones in terms of accuracy and processing time; they are also more robust to failure in feature-deprived environments. On the downside, Direct methods rely on heuristic motion models to seed the estimation of camera motion between frames; in the event that these models are violated (e.g., erratic motion), Direct methods easily fail. This paper proposes a novel system entitled FDMO (Feature assisted Direct Monocular Odometry), which complements the advantages of both direct and featured based techniques. FDMO bootstraps indirect feature tracking upon the sub-pixel accurate localized direct keyframes only when failure modes (e.g., large baselines) of direct tracking occur. Control returns back to direct odometry when these conditions are no longer violated. Efficiencies are introduced to help FDMO perform in real time. FDMO shows significant drift (alignment, rotation & scale) reduction when compared to DSO & ORB SLAM when evaluated using the TumMono and EuroC datasets.}
}

@article{stumberg2018arxiv,
  author   = {L.v. Stumberg and V. Usenko and D. Cremers},
  title    = {{Direct Sparse Visual-Inertial Odometry using Dynamic Marginalization}},
  journal  = arxiv,
  year     = 2018,
  volume   = {arXiv:1804.05625},
  url      = {http://arxiv.org/pdf/1804.05625v1},
  abstract = {We present VI-DSO, a novel approach for visual-inertial odometry, which jointly estimates camera poses and sparse scene geometry by minimizing photometric and IMU measurement errors in a combined energy functional. The visual part of the system performs a bundle-adjustment like optimization on a sparse set of points, but unlike key-point based systems it directly minimizes a photometric error. This makes it possible for the system to track not only corners, but any pixels with large enough intensity gradients. IMU information is accumulated between several frames using measurement preintegration, and is inserted into the optimization as an additional constraint between keyframes. We explicitly include scale and gravity direction into our model and jointly optimize them together with other variables such as poses. As the scale is often not immediately observable using IMU data this allows us to initialize our visual-inertial system with an arbitrary scale instead of having to delay the initialization until everything is observable. We perform partial marginalization of old variables so that updates can be computed in a reasonable time. In order to keep the system consistent we propose a novel strategy which we call "dynamic marginalization". This technique allows us to use partial marginalization even in cases where the initial scale estimate is far from the optimum. We evaluate our method on the challenging EuRoC dataset, showing that VI-DSO outperforms the state of the art.}
}

@article{nicholson2018arxiv,
  author   = {L. Nicholson and M. Milford and N. S{\"u}nderhauf},
  title    = {{QuadricSLAM: Constrained Dual Quadrics from Object Detections as Landmarks in Semantic SLAM}},
  journal  = arxiv,
  year     = 2018,
  volume   = {arXiv:1804.04011},
  url      = {http://arxiv.org/pdf/1804.04011v1},
  abstract = {Research in Simultaneous Localization And Mapping (SLAM) is increasingly moving towards richer world representations involving objects and high level features that enable a semantic model of the world for robots, potentially leading to a more meaningful set of robot-world interactions. Many of these advances are grounded in state-of-the-art computer vision techniques primarily developed in the context of image-based benchmark datasets, leaving several challenges to be addressed in adapting them for use in robotics. In this paper, we derive a SLAM formulation that uses dual quadrics as 3D landmark representations, exploiting their ability to compactly represent the size, position and orientation of an object, and show how 2D bounding boxes (such as those typically obtained from visual object detection systems) can directly constrain the quadric parameters via a novel geometric error formulation. We develop a sensor model for deep-learned object detectors that addresses the challenge of partial object detections often encountered in robotics applications, and demonstrate how to jointly estimate the camera pose and constrained dual quadric parameters in factor graph based SLAM with a general perspective camera.}
}

@article{shao2018arxiv,
  author   = {L. Shao and P. Shah and V. Dwaracherla and J. Bohg},
  title    = {{Motion-based Object Segmentation based on Dense RGB-D Scene Flow}},
  journal  = arxiv,
  year     = 2018,
  volume   = {arXiv:1804.05195},
  url      = {http://arxiv.org/pdf/1804.05195v1},
  abstract = {Given two consecutive RGB-D images, we propose a model that estimates a dense 3D motion field, also known as scene flow. We take advantage of the fact that in robot manipulation scenarios, scenes often consist of a set of rigidly moving objects. Our model jointly estimates (i) the segmentation of the scene into an unknown but finite number of objects, (ii) the motion trajectories of these objects and (iii) the object scene flow. We employ an hourglass, deep neural network architecture. In the encoding stage, the RGB and depth images undergo spatial compression and correlation. In the decoding stage, the model outputs three images containing a per-pixel estimate of the corresponding object center as well as object translation and rotation. This forms the basis for inferring the object segmentation and final object scene flow. To evaluate our model, we generated a new and challenging, large-scale, synthetic dataset that is specifically targeted at robotic manipulation: It contains a large number of scenes with a very diverse set of simultaneously moving 3D objects and is recorded with a commonly-used RGB-D camera. In quantitative experiments, we show that we significantly outperform state-of-the-art scene flow and motion-segmentation methods. In qualitative experiments, we show how our learned model transfers to challenging real-world scenes, visually generating significantly better results than existing methods.}
}

@article{cramariuc2018arxiv,
  author   = {A. Cramariuc and R. Dube and H. Sommer and R. Siegwart and I. Gilitschenski},
  title    = {{Learning 3D Segment Descriptors for Place Recognition}},
  journal  = arxiv,
  year     = 2018,
  volume   = {arXiv:1804.09270},
  url      = {http://arxiv.org/pdf/1804.09270v1},
  abstract = {In the absence of global positioning information, place recognition is a key capability for enabling localization, mapping and navigation in any environment. Most place recognition methods rely on images, point clouds, or a combination of both. In this work we leverage a segment extraction and matching approach to achieve place recognition in Light Detection and Ranging (LiDAR) based 3D point cloud maps. One challenge related to this approach is the recognition of segments despite changes in point of view or occlusion. We propose using a learning based method in order to reach a higher recall accuracy then previously proposed methods. Using Convolutional Neural Networks (CNNs), which are state-of-the-art classifiers, we propose a new approach to segment recognition based on learned descriptors. In this paper we compare the effectiveness of three different structures and training methods for CNNs. We demonstrate through several experiments on real-world data collected in an urban driving scenario that the proposed learning based methods outperform hand-crafted descriptors.}
}

@article{li2018arxiv,
  author   = {Y. Li and G. Wang and X. Ji and Y. Xiang and D. Fox},
  title    = {{DeepIM: Deep Iterative Matching for 6D Pose Estimation}},
  journal  = arxiv,
  year     = 2018,
  volume   = {arXiv:1804.00175},
  url      = {http://arxiv.org/pdf/1804.00175v2},
  abstract = {Estimating the 6D pose of objects from images is an important problem in various applications such as robot manipulation and virtual reality. While direct regression of images to object poses has limited accuracy, matching rendered images of an object against the observed image can produce accurate results. In this work, we propose a novel deep neural network for 6D pose matching named DeepIM. Given an initial pose estimation, our network is able to iteratively refine the pose by matching the rendered image against the observed image. The network is trained to predict a relative pose transformation using an untangled representation of 3D location and 3D orientation and an iterative training process. Experiments on two commonly used benchmarks for 6D pose estimation demonstrate that DeepIM achieves large improvements over state-of-the-art methods. We furthermore show that DeepIM is able to match previously unseen objects.}
}

@inproceedings{brazil2020eccv,
  title     = {Kinematic 3d object detection in monocular video},
  author    = {Garrick Brazil and Gerard Pons-Moll and Xiaoming Liu and Bernt Schiele},
  booktitle = eccv,
  year      = {2020},
  url       = {proceedings: brazil2020eccv.pdf}
}

@article{brazil2022arxiv,
  title   = {{Omni3D: A Large Benchmark and Model for 3D Object Detection in the Wild}},
  author  = {Garrick Brazil and Julian Straub and Nikhila Ravi and Justin Johnson and Georgia Gkioxari},
  journal = arxiv,
  volume  = {arXiv:2207.10660},
  year    = {2022},
  url     = {https://arxiv.org/pdf/2207.10660.pdf}
}

@article{gasperini2021ral,
  author   = {S. Gasperini and M. Nikouei Mahani and A. Marcos-Ramiro and N. Navab and F. Tombari},
  title    = {{Panoster: End-To-End Panoptic Segmentation of LiDAR Point Clouds}},
  journal  = ral,
  year     = 2021,
  volume   = {6},
  number   = {2},
  pages    = {3216--3223},
  abstract = {Panoptic segmentation has recently unified semantic and instance segmentation, previously addressed separately, thus taking a step further towards creating more comprehensive and efficient perception systems. In this paper, we present Panoster, a novel proposal-free panoptic segmentation method for LiDAR point clouds. Unlike previous approaches relying on several steps to group pixels or points into objects, Panoster proposes a simplified framework incorporating a learning-based clustering solution to identify instances. At inference time, this acts as a class-agnostic segmentation, allowing Panoster to be fast, while outperforming prior methods in terms of accuracy. Without any post-processing, Panoster reached state-of-theart results among published approaches on the challenging SemanticKITTI benchmark, and further increased its lead by exploiting heuristic techniques. Additionally, we showcase how our method can be flexibly and effectively applied on diverse existing semantic architectures to deliver panoptic predictions.},
  url      = {proceedings: gasperini2021ral.pdf}
}

@article{dube2018arxiv,
  author   = {R. Dube and A. Cramariuc and D. Dugas and J. Nieto and R. Siegwart and C. Cadena},
  title    = {{SegMap: 3D Segment Mapping using Data-Driven Descriptors}},
  journal  = arxiv,
  year     = 2018,
  volume   = {arXiv:1804.09557},
  url      = {http://arxiv.org/pdf/1804.09557v1},
  abstract = {When performing localization and mapping, working at the level of structure can be advantageous in terms of robustness to environmental changes and differences in illumination. This paper presents SegMap: a map representation solution to the localization and mapping problem based on the extraction of segments in 3D point clouds. In addition to facilitating the computationally intensive task of processing 3D point clouds, working at the level of segments addresses the data compression requirements of real-time single- and multi-robot systems. While current methods extract descriptors for the single task of localization, SegMap leverages a data-driven descriptor in order to extract meaningful features that can also be used for reconstructing a dense 3D map of the environment and for extracting semantic information. This is particularly interesting for navigation tasks and for providing visual feedback to end-users such as robot operators, for example in search and rescue scenarios. These capabilities are demonstrated in multiple urban driving and search and rescue experiments. Our method leads to an increase of area under the ROC curve of 28.3\% over current state of the art using eigenvalue based features. We also obtain very similar reconstruction capabilities to a model specifically trained for this task. The SegMap implementation will be made available open-source along with easy to run demonstrations at www.github.com/ethz-asl/segmap. A video demonstration is available at https://youtu.be/CMk4w4eRobg.}
}

@article{hosseinzadeh2018arxiv,
  author   = {M. Hosseinzadeh and Y. Latif and T. Pham and N. S{\"u}nderhauf and I. Reid},
  title    = {{Towards Semantic SLAM: Points, Planes and Objects}},
  journal  = arxiv,
  year     = 2018,
  volume   = {arXiv:1804.09111},
  url      = {http://arxiv.org/pdf/1804.09111v1},
  abstract = {Simultaneous Localization And Mapping (SLAM) is a fundamental problem in mobile robotics. Semantic SLAM is an effort to build meaningful map representations that not only provide rich information about the environment but also aid in camera localization. This work proposes a method for representing generic objects using quadrics that allows seamless integration in a SLAM framework, with additional dominant planar structure modeled as infinite planes. Experiments show the proposed points-planes-quadrics representation can easily incorporate Manhattan and object affordance constraints, greatly improving camera localization and leading to semantically meaningful maps.}
}

@article{radwan2018arxiv,
  author   = {N. Radwan and A. Valada and W. Burgard},
  title    = {{VLocNet++: Deep Multitask Learning for Semantic Visual Localization and Odometry}},
  journal  = arxiv,
  year     = 2018,
  volume   = {arXiv:1804.08366},
  url      = {http://arxiv.org/pdf/1804.08366v3},
  abstract = {Semantic understanding and localization are fundamental enablers of robot autonomy that have for the most part been tackled as disjoint problems. While deep learning has enabled recent breakthroughs across a wide spectrum of scene understanding tasks, its applicability to state estimation tasks has been limited due to the direct formulation that renders it incapable of encoding scene-specific constrains. In this work, we propose the VLocNet++ architecture that employs a multitask learning approach to exploit the inter-task relationship between learning semantics, regressing 6-DoF global pose and odometry, for the mutual benefit of each of these tasks. Our network overcomes the aforementioned limitation by simultaneously embedding geometric and semantic knowledge of the world into the pose regression network. We propose a novel adaptive weighted fusion layer to aggregate motion-specific temporal information and to fuse semantic features into the localization stream based on region activations. Furthermore, we propose a self-supervised warping technique that uses the relative motion to warp intermediate network representations in the segmentation stream for learning consistent semantics. Finally, we introduce a first-of-a-kind urban outdoor localization dataset with pixel-level semantic labels and multiple loops for training deep networks. Extensive experiments on the challenging Microsoft 7-Scenes benchmark and our DeepLoc dataset demonstrate that our approach exceeds the state-of-the-art outperforming local feature-based methods while simultaneously performing multiple tasks and exhibiting substantial robustness in challenging scenarios.}
}

@article{watson2018arxiv,
  author   = {R.M. Watson and J.N. Gross},
  title    = {{Evaluation of Kinematic Precise Point Positioning Convergence with an Incremental Graph Optimizer}},
  journal  = arxiv,
  year     = 2018,
  volume   = {arXiv:1804.04197},
  url      = {http://arxiv.org/pdf/1804.04197v1},
  abstract = {Estimation techniques to precisely localize a kinematic platform with GNSS observables can be broadly partitioned into two categories: differential, or undifferenced. The differential techniques (e.g., real-time kinematic (RTK)) have several attractive properties, such as correlated error mitigation and fast convergence; however, to support a differential processing scheme, an infrastructure of reference stations within a proximity of the platform must be in place to construct observation corrections. This infrastructure requirement makes differential processing techniques infeasible in many locations. To mitigate the need for additional receivers within proximity of the platform, the precise point positioning (PPP) method utilizes accurate orbit and clock models to localize the platform. The autonomy of PPP from local reference stations make it an attractive processing scheme for several applications; however, a current disadvantage of PPP is the slow positioning convergence when compared to differential techniques. In this paper, we evaluate the convergence properties of PPP with an incremental graph optimization scheme (Incremental Smoothing and Mapping (iSAM2)), which allows for real-time filtering and smoothing. The characterization is first conducted through a Monte Carlo analysis within a simulation environment, which allows for the variations of parameters, such as atmospheric conditions, satellite geometry, and intensity of multipath. Then, an example collected data set is utilized to validate the trends presented in the simulation study.}
}

@article{iyer2018arxiv,
  author   = {G. Iyer and J.K. Murthy and G. Gupta and K.M. Krishna and L. Paull},
  title    = {{Geometric Consistency for Self-Supervised End-to-End Visual Odometry}},
  journal  = arxiv,
  year     = 2018,
  volume   = {arXiv:1804.03789},
  url      = {http://arxiv.org/pdf/1804.03789v1},
  abstract = {With the success of deep learning based approaches in tackling challenging problems in computer vision, a wide range of deep architectures have recently been proposed for the task of visual odometry (VO) estimation. Most of these proposed solutions rely on supervision, which requires the acquisition of precise ground-truth camera pose information, collected using expensive motion capture systems or high-precision IMU/GPS sensor rigs. In this work, we propose an unsupervised paradigm for deep visual odometry learning. We show that using a noisy teacher, which could be a standard VO pipeline, and by designing a loss term that enforces geometric consistency of the trajectory, we can train accurate deep models for VO that do not require ground-truth labels. We leverage geometry as a self-supervisory signal and propose "Composite Transformation Constraints (CTCs)", that automatically generate supervisory signals for training and enforce geometric consistency in the VO estimate. We also present a method of characterizing the uncertainty in VO estimates thus obtained. To evaluate our VO pipeline, we present exhaustive ablation studies that demonstrate the efficacy of end-to-end, self-supervised methodologies to train deep models for monocular VO. We show that leveraging concepts from geometry and incorporating them into the training of a recurrent neural network results in performance competitive to supervised deep VO methods.}
}

@article{garg2018arxiv,
  author   = {S. Garg and N. S{\"u}nderhauf and M. Milford},
  title    = {{LoST? Appearance-Invariant Place Recognition for Opposite Viewpoints using Visual Semantics}},
  journal  = arxiv,
  year     = 2018,
  volume   = {arXiv:1804.05526},
  url      = {http://arxiv.org/pdf/1804.05526v2},
  abstract = {Human visual scene understanding is so remarkable that we are able to recognize a revisited place when entering it from the opposite direction it was first visited, even in the presence of extreme variations in appearance. This capability is especially apparent during driving: a human driver can recognize where they are when travelling in the reverse direction along a route for the first time, without having to turn back and look. The difficulty of this problem exceeds any addressed in past appearance- and viewpoint-invariant visual place recognition (VPR) research, in part because large parts of the scene are not commonly observable from opposite directions. Consequently, as shown in this paper, the precision-recall performance of current state-of-the-art viewpoint- and appearance-invariant VPR techniques is orders of magnitude below what would be usable in a closed-loop system. Current engineered solutions predominantly rely on panoramic camera or LIDAR sensing setups; an eminently suitable engineering solution but one that is clearly very different to how humans navigate, which also has implications for how naturally humans could interact and communicate with the navigation system. In this paper we develop a suite of novel semantic- and appearance-based techniques to enable for the first time high performance place recognition in this challenging scenario. We first propose a novel Local Semantic Tensor (LoST) descriptor of images using the convolutional feature maps from a state-of-the-art dense semantic segmentation network. Then, to verify the spatial semantic arrangement of the top matching candidates, we develop a novel approach for mining semantically-salient keypoint correspondences.}
}

@article{suenderhauf2018arxiv,
  author   = {N. S{\"u}nderhauf and O. Brock and W. Scheirer and R. Hadsell and D. Fox and J. Leitner and B. Upcroft and P. Abbeel and W. Burgard and M. Milford and P. Corke},
  title    = {{The Limits and Potentials of Deep Learning for Robotics}},
  journal  = arxiv,
  year     = 2018,
  volume   = {arXiv:1804.06557},
  url      = {http://arxiv.org/pdf/1804.06557v1},
  abstract = {The application of deep learning in robotics leads to very specific problems and research questions that are typically not addressed by the computer vision and machine learning communities. In this paper we discuss a number of robotics-specific learning, reasoning, and embodiment challenges for deep learning. We explain the need for better evaluation metrics, highlight the importance and unique challenges for deep robotic learning in simulation, and explore the spectrum between purely data-driven and model-driven approaches. We hope this paper provides a motivating overview of important research directions to overcome the current limitations, and help fulfill the promising potentials of deep learning in robotics.}
}

@article{talbot2018arxiv,
  author   = {B. Talbot and S. Garg and M. Milford},
  title    = {{OpenSeqSLAM2.0: An Open Source Toolbox for Visual Place Recognition Under Changing Conditions}},
  journal  = arxiv,
  year     = 2018,
  volume   = {arXiv:1804.02156v2},
  url      = {http://arxiv.org/pdf/1804.02156v2},
  abstract = {Visually recognising a traversed route - regardless of whether seen during the day or night, in clear or inclement conditions, or in summer or winter - is an important capability for navigating robots. Since SeqSLAM was introduced in 2012, a large body of work has followed exploring how robotic systems can use the algorithm to meet the challenges posed by navigation in changing environmental conditions. The following paper describes OpenSeqSLAM2.0, a fully open source toolbox for visual place recognition under changing conditions. Beyond the benefits of open access to the source code, OpenSeqSLAM2.0 provides a number of tools to facilitate exploration of the visual place recognition problem and interactive parameter tuning. Using the new open source platform, it is shown for the first time how comprehensive parameter characterisations provide new insights into many of the system components previously presented in ad hoc ways and provide users with a guide to what system component options should be used under what circumstances and why.}
}

@inproceedings{hazirbas2016accv,
  author    = {Hazirbas, Caner and Ma, Lingni and Domokos, Csaba and Cremers, Daniel},
  title     = {{Fusenet: Incorporating depth into semantic segmentation via fusion-based cnn architecture}},
  booktitle = accv,
  year      = 2016,
  url       = {https://www.researchgate.net/profile/Caner-Hazirbas/publication/308311897_FuseNet_Incorporating_Depth_into_Semantic_Segmentation_via_Fusion-Based_CNN_Architecture/links/5a436d7d458515f6b050cb7f/FuseNet-Incorporating-Depth-into-Semantic-Segmentation-via-Fusion-Based-CNN-Architecture.pdf}
}

@article{denniston2018arxiv,
  author   = {C. Denniston and T.R. Krogstad and S. Kemna and G.S. Sukhatme},
  title    = {{Planning Safe Paths through Hazardous Environments}},
  journal  = arxiv,
  year     = 2018,
  volume   = {arXiv:1803.00664},
  url      = {http://arxiv.org/pdf/1803.00664v2},
  abstract = {Autonomous underwater vehicles (AUVs) are robotic platforms that are commonly used to map the sea floor, for example for benthic surveys or for naval mine countermeasures (MCM) operations. AUVs create an acoustic image of the survey area, such that objects on the seabed can be identified and, in the case of MCM, mines can be found and disposed of. The common method for creating such seabed maps is to run a lawnmower survey, which is a standard method in coverage path planning. We are interested in exploring alternate techniques for surveying areas of interest, in order to reduce mission time or assess feasible actions, such as finding a safe path through a hazardous region. In this paper, we use Gaussian Process regression to build models of seabed complexity data, obtained through lawnmower surveys. We evaluate several commonly used kernels to assess their modeling performance, which includes modeling discontinuities in the data. Our results show that an additive Mat\'ern kernel is most suitable for modeling seabed complexity data. On top of the GP model, we use adaptations of two standard path planning methods, A* and RRT*, to find safe paths through the modeled areas. We evaluate the planned paths and also run a vehicle dynamics simulator to assess potential performance by a marine vessel.}
}

@article{imperoli2018arxiv,
  author   = {M. Imperoli and C. Potena and D. Nardi and G. Grisetti and A. Pretto},
  title    = {{An Effective Multi-Cue Positioning System for Agricultural Robotics}},
  journal  = arxiv,
  year     = 2018,
  volume   = {arXiv:1803.00954},
  url      = {http://arxiv.org/pdf/1803.00954v1},
  abstract = {The self-localization capability is a crucial component for Unmanned Ground Vehicles (UGV) in farming applications. Approaches based solely on visual information or on low-cost GPS are easily prone to fail in such scenarios. In this paper, we present a robust and accurate 3D global pose estimation framework, designed to take full advantage of heterogeneous sensory data. By modeling the pose estimation problem as a pose graph optimization, our approach simultaneously mitigates the cumulative drift introduced by motion estimation systems (wheel odometry, visual odometry, ...), and the noise introduced by the raw GPS readings. Along with a suitable motion model, our system also integrates two additional types of constraints: (i) a Digital Elevation Model (DEM) and (ii) a Markov Random Field (MRF) assumption. We demonstrate how using these additional cues substantially reduces the error along the altitude axis, and this benefit spreads to the other components of the state. We report exhaustive experiments combining several sensor setups, showing improvements from 37\% to 76\% of the localization accuracy with respect to using only the GPS. We also show that our approach provides accurate results even if the GPS temporarily change positioning mode. We released our C++ open-source implementation and two challenging datasets with their relative ground truth.}
}

@article{dong2018arxiv,
  author   = {W. Dong and J. Shi and W. Tang and X. Wang and H. Zha},
  title    = {{An Efficient Volumetric Mesh Representation for Real-time Scene Reconstruction using Spatial Hashing}},
  journal  = arxiv,
  year     = 2018,
  volume   = {arXiv:1803.03949},
  url      = {http://arxiv.org/pdf/1803.03949v1},
  abstract = {Mesh plays an indispensable role in dense real-time reconstruction essential in robotics. Efforts have been made to maintain flexible data structures for 3D data fusion, yet an efficient incremental framework specifically designed for online mesh storage and manipulation is missing. We propose a novel framework to compactly generate, update, and refine mesh for scene reconstruction upon a volumetric representation. Maintaining a spatial-hashed field of cubes, we distribute vertices with continuous value on discrete edges that support O(1) vertex accessing and forbid memory redundancy. By introducing Hamming distance in mesh refinement, we further improve the mesh quality regarding the triangle type consistency with a low cost. Lock-based and lock-free operations were applied to avoid thread conflicts in GPU parallel computation. Experiments demonstrate that the mesh memory consumption is significantly reduced while the running speed is kept in the online reconstruction process.}
}

@article{oleynikova2018arxiv,
  author   = {H. Oleynikova and Z. Taylor and R. Siegwart and J. Nieto},
  title    = {{Sparse 3D Topological Graphs for Micro-Aerial Vehicle Planning}},
  journal  = arxiv,
  year     = 2018,
  volume   = {arXiv:1803.04345},
  url      = {http://arxiv.org/pdf/1803.04345v1},
  abstract = {Micro-Aerial Vehicles (MAVs) have the advantage of moving freely in 3D space. However, creating compact and sparse map representations that can be efficiently used for planning for such robots is still an open problem. In this paper, we take maps built from noisy sensor data and construct a sparse graph containing topological information that can be used for 3D planning. We use a Euclidean Signed Distance Field, extract a 3D Generalized Voronoi Diagram (GVD), and obtain a thin skeleton diagram representing the topological structure of the environment. We then convert this skeleton diagram into a sparse graph, which we show is resistant to noise and changes in resolution. We demonstrate global planning over this graph, and the orders of magnitude speed-up it offers over other common planning methods. We validate our planning algorithm in real maps built onboard an MAV, using RGB-D sensing.}
}

@article{jeong2018arxiv,
  author   = {J. Jeong and Y. Cho and Y. Shin and H. Roh and A. Kim},
  title    = {{Complex Urban LiDAR Data Set}},
  journal  = arxiv,
  year     = 2018,
  volume   = {arXiv:1803.06121},
  url      = {http://arxiv.org/pdf/1803.06121v1},
  abstract = {This paper presents a Light Detection and Ranging (LiDAR) data set that targets complex urban environments. Urban environments with high-rise buildings and congested traffic pose a significant challenge for many robotics applications. The presented data set is unique in the sense it is able to capture the genuine features of an urban environment (e.g. metropolitan areas, large building complexes and underground parking lots). Data of two-dimensional (2D) and threedimensional (3D) LiDAR, which are typical types of LiDAR sensors, are provided in the data set. The two 16-ray 3D LiDARs are tilted on both sides for maximal coverage. One 2D LiDAR faces backward while the other faces forwards to collect data of roads and buildings, respectively. Raw sensor data from Fiber Optic Gyro (FOG), Inertial Measurement Unit (IMU), and the Global Positioning System (GPS) are presented in a file format for vehicle pose estimation. The pose information of the vehicle estimated at 100 Hz is also presented after applying the graph simultaneous localization and mapping (SLAM) algorithm. For the convenience of development, the file player and data viewer in Robot Operating System (ROS) environment were also released via the web page. The full data sets are available at: http://irap.kaist.ac.kr/dataset. In this website, 3D preview of each data set is provided using WebGL.}
}

@article{fentanes2018arxiv,
  author   = {J.P. Fentanes and I. Gould and T. Duckett and S. Pearson and G. Cielniak},
  title    = {{3D Soil Compaction Mapping through Kriging-based Exploration with a Mobile Robot}},
  journal  = arxiv,
  year     = 2018,
  volume   = {arXiv:1803.08069},
  url      = {http://arxiv.org/pdf/1803.08069v1},
  abstract = {This paper presents an automated method for creating spatial maps of soil condition with an outdoor mobile robot. Effective soil mapping on farms can enhance yields, reduce inputs and help protect the environment. Traditionally, data are collected manually at an arbitrary set of locations, then soil maps are constructed offline using Kriging, a form of Gaussian process regression. This process is laborious and costly, limiting the quality and resolution of the resulting information. Instead, we propose to use an outdoor mobile robot for automatic collection of soil condition data, building soil maps online and also adapting the robot's exploration strategy on-the-fly based on the current quality of the map. We show how using Kriging variance as a reward function for robotic exploration allows for both more efficient data collection and better soil models. This work presents the theoretical foundations for our proposal and an experimental comparison of exploration strategies using soil compaction data from a field generated with a mobile robot.}
}

@article{fehr2018arxiv,
  author   = {M. Fehr and T. Schneider and M. Dymczyk and J. Sturm and R. Siegwart},
  title    = {{Visual-Inertial Teach and Repeat for Aerial Inspection}},
  journal  = arxiv,
  year     = 2018,
  volume   = {arXiv:1803.09650},
  url      = {http://arxiv.org/pdf/1803.09650v1},
  abstract = {Industrial facilities often require periodic visual inspections of key installations. Examining these points of interest is time consuming, potentially hazardous or require special equipment to reach. MAVs are ideal platforms to automate this expensive and tedious task. In this work we present a novel system that enables a human operator to teach a visual inspection task to an autonomous aerial vehicle by simply demonstrating the task using a handheld device. To enable robust operation in confined, GPS-denied environments, the system employs the Google Tango visual-inertial mapping framework as the only source of pose estimates. In a first step the operator records the desired inspection path and defines the inspection points. The mapping framework then computes a feature-based localization map, which is shared with the robot. After take-off, the robot estimates its pose based on this map and plans a smooth trajectory through the way points defined by the operator. Furthermore, the system is able to track the poses of other robots or the operator, localized in the same map, and follow them in real-time while keeping a safe distance.}
}

@article{witting2018arxiv,
  author   = {C. Witting and M. Fehr and R. BÃ¤hnemann and H. Oleynikova and R. Siegwart},
  title    = {{History-aware Autonomous Exploration in Confined Environments using MAVs}},
  journal  = arxiv,
  year     = 2018,
  volume   = {arXiv:1803.10558},
  url      = {http://arxiv.org/pdf/1803.10558v1},
  abstract = {Many scenarios require a robot to be able to explore its 3D environment online without human supervision. This is especially relevant for inspection tasks and search and rescue missions. To solve this high-dimensional path planning problem, sampling-based exploration algorithms have proven successful. However, these do not necessarily scale well to larger environments or spaces with narrow openings. This paper presents a 3D exploration planner based on the principles of Next-Best Views (NBVs). In this approach, a Micro-Aerial Vehicle (MAV) equipped with a limited field-of-view depth sensor randomly samples its configuration space to find promising future viewpoints. In order to obtain high sampling efficiency, our planner maintains and uses a history of visited places, and locally optimizes the robot's orientation with respect to unobserved space. We evaluate our method in several simulated scenarios, and compare it against a state-of-the-art exploration algorithm. The experiments show substantial improvements in exploration time ($2\times$ faster), computation time, and path length, and advantages in handling difficult situations such as escaping dead-ends (up to $20\times$ faster). Finally, we validate the on-line capability of our algorithm on a computational constrained real world MAV.}
}

@article{katyal2018arxiv,
  author   = {K. Katyal and K. Popek and C. Paxton and J. Moore and K. Wolfe and P. Burlina and G.D. Hager},
  title    = {{Occupancy Map Prediction Using Generative and Fully Convolutional Networks for Vehicle Navigation}},
  journal  = arxiv,
  year     = 2018,
  volume   = {arXiv:1803.02007},
  url      = {http://arxiv.org/pdf/1803.02007v1},
  abstract = {Fast, collision-free motion through unknown environments remains a challenging problem for robotic systems. In these situations, the robot's ability to reason about its future motion is often severely limited by sensor field of view (FOV). By contrast, biological systems routinely make decisions by taking into consideration what might exist beyond their FOV based on prior experience. In this paper, we present an approach for predicting occupancy map representations of sensor data for future robot motions using deep neural networks. We evaluate several deep network architectures, including purely generative and adversarial models. Testing on both simulated and real environments we demonstrated performance both qualitatively and quantitatively, with SSIM similarity measure up to 0.899. We showed that it is possible to make predictions about occupied space beyond the physical robot's FOV from simulated training data. In the future, this method will allow robots to navigate through unknown environments in a faster, safer manner.}
}

@article{davison2018arxiv,
  author   = {A.J. Davison},
  title    = {{FutureMapping: The Computational Structure of Spatial AI Systems}},
  journal  = arxiv,
  year     = 2018,
  volume   = {arXiv:1803.11288},
  url      = {http://arxiv.org/pdf/1803.11288v1},
  abstract = {We discuss and predict the evolution of Simultaneous Localisation and Mapping (SLAM) into a general geometric and semantic `Spatial AI' perception capability for intelligent embodied devices. A big gap remains between the visual perception performance that devices such as augmented reality eyewear or comsumer robots will require and what is possible within the constraints imposed by real products. Co-design of algorithms, processors and sensors will be needed. We explore the computational structure of current and future Spatial AI algorithms and consider this within the landscape of ongoing hardware developments.}
}


@article{garcia-fidalgo2018arxiv,
  author   = {E. Garcia-Fidalgo and A. Ortiz},
  title    = {{iBoW-LCD: An Appearance-based Loop Closure Detection Approach using Incremental Bags of Binary Words}},
  journal  = arxiv,
  year     = 2018,
  volume   = {arXiv:1802.05909},
  url      = {http://arxiv.org/pdf/1802.05909v1},
  abstract = {In this paper, we introduce iBoW-LCD, a novel appearance-based loop closure detection method. The presented approach makes use of an incremental Bag-of-Words (BoW) scheme based on binary descriptors to retrieve previously seen similar images, avoiding any vocabulary training stage usually required by classic BoW models. In addition, to detect loop closures, iBoW-LCD builds on the concept of dynamic islands, a simple but effective mechanism to group similar images close in time, which reduces the computational times typically associated to Bayesian frameworks. Our approach is validated using several indoor and outdoor public datasets, taken under different environmental conditions, achieving a high accuracy and outperforming other state-of-the-art solutions.}
}


@article{zollhoefer2018eg,
  author   = {Zollh{\"o}fer, M. and Stotko, P. and G{\"o}rlitz, A. and Theobalt, C. and Nie{\ss}ner, M. and Klein, R. and Kolb, A.},
  title    = {{State of the Art on 3D Reconstruction with RGB-D Cameras}},
  journal  = {Eurographics - State-of-the-Art Reports (STARs)},
  volume   = {37},
  number   = {2},
  year     = 2018,
  url      = {https://web.stanford.edu/~zollhoef/papers/EG18_RecoSTAR/paper.pdf},
  abstract = {The advent of affordable consumer grade RGB-D cameras has brought about a profound advancement of visual scene reconstruction methods. Both computer graphics and computer vision researchers spend significant effort to develop entirely new algorithms to capture comprehensive shape models of static and dynamic scenes with RGB-D cameras. This led to significant advances of the state of the art along several dimensions. Some methods achieve very high reconstruction detail, despite limited sensor resolution. Others even achieve real-time performance, yet possibly at lower quality. New concepts were developed to capture scenes at larger spatial and temporal extent. Other recent algorithms flank shape reconstruction with concurrent material and lighting estimation, even in general scenes and unconstrained conditions. In this state-of-the-art report, we analyze these recent developments in RGB-D scene reconstruction in detail and review essential related work. We explain, compare, and critically analyze the common underlying algorithmic concepts that enabled these recent advancements. Furthermore, we show how algorithms are designed to best exploit the benefits of RGB-D data while suppressing their often non-trivial data distortions. In addition, this report identifies and discusses important open research questions and suggests relevant directions for future work.}
}

@inproceedings{della-corte2018icra,
  author    = {Della Corte, B. and I. Bogoslavskyi and C. Stachniss and G. Grisetti},
  title     = {{A General Framework for Flexible Multi-Cue Photometric Point Cloud Registration}},
  year      = 2018,
  booktitle = icra,
  codeurl   = {https://gitlab.com/srrg-software/srrg_mpr},
  videourl  = {https://www.youtube.com/watch?v=_z98guJTqfk},
  url       = {http://www.ipb.uni-bonn.de/wp-content/papercite-data/pdf/della-corte2018icra.pdf}
}

@inproceedings{milioto2018icra,
  author    = {A. Milioto and P. Lottes and C. Stachniss},
  title     = {{Real-time Semantic Segmentation of Crop and Weed for Precision Agriculture Robots Leveraging Background Knowledge in CNNs}},
  year      = {2018},
  booktitle = icra,
  abstract  = {Precision farming robots, which target to reduce the amount of herbicides that need to be brought out in the fields, must have the ability to identify crops and weeds in real time to trigger weeding actions. In this paper, we address the problem of CNN-based semantic segmentation of crop fields separating sugar beet plants, weeds, and background solely based on RGB data. We propose a CNN that exploits existing vegetation indexes and provides a classification in real time. Furthermore, it can be effectively re-trained to so far unseen fields with a comparably small amount of training data. We implemented and thoroughly evaluated our system on a real agricultural robot operating in different fields in Germany and Switzerland. The results show that our system generalizes well, can operate at around 20Hz, and is suitable for online operation in the fields.},
  url       = {https://arxiv.org/pdf/1709.06764},
  videourl  = {https://youtu.be/DXcTkJmdWFQ}
}

@inproceedings{milioto2018icraws,
  author    = {A. Milioto and C. Stachniss},
  title     = {{Bonnet: An Open-Source Training and Deployment Framework for Semantic Segmentation in Robotics using CNNs}},
  booktitle = {Proc. of the Worshop on Perception, Inference, and Learning for Joint Semantic, Geometric, and Physical Understanding, IEEE Int. Conf. on Robotics \& Automation (ICRA)},
  keywords  = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition},
  year      = 2018,
  url       = {https://arxiv.org/abs/1802.08960},
  codeurl   = {https://github.com/Photogrammetry-Robotics-Bonn/bonnet},
  videourl  = {https://www.youtube.com/watch?v=tfeFHCq6YJs}
}

@article{palazzolo2018drones,
  author   = {E. Palazzolo and C. Stachniss},
  title    = {{Effective Exploration for MAVs Based on the Expected Information Gain}},
  journal  = {Drones},
  volume   = {2},
  year     = {2018},
  number   = {1},
  url      = {http://www.ipb.uni-bonn.de/pdfs/palazzolo2018drones.pdf},
  abstract = {Micro aerial vehicles (MAVs) are an excellent platform for autonomous exploration. Most MAVs rely mainly on cameras for buliding a map of the 3D environment. Therefore, vision-based MAVs require an efficient exploration algorithm to select viewpoints that provide informative measurements. In this paper, we propose an exploration approach that selects in real time the next-best-view that maximizes the expected information gain of new measurements. In addition, we take into account the cost of reaching a new viewpoint in terms of distance and predictability of the flight path for a human observer. Finally, our approach selects a path that reduces the risk of crashes when the expected battery life comes to an end, while still maximizing the information gain in the process. We implemented and thoroughly tested our approach and the experiments show that it offers an improved performance compared to other state-of-the-art algorithms in terms of precision of the reconstruction, execution time, and smoothness of the path.}
}

@inproceedings{palazzolo2018icra,
  title     = {{Fast Image-Based Geometric Change Detection Given a 3D Model}},
  author    = {E. Palazzolo and C. Stachniss},
  booktitle = icra,
  year      = {2018},
  url       = {http://www.ipb.uni-bonn.de/pdfs/palazzolo2018icra.pdf},
  codeurl   = {https://github.com/Photogrammetry-Robotics-Bonn/fast_change_detection},
  videourl  = {https://youtu.be/DEkOYf4Zzh4}
}

@inproceedings{huang2018icra,
  author    = {K.H. Huang and C. Stachniss},
  title     = {{On Geometric Models and Their Accuracy for Extrinsic Sensor Calibration}},
  booktitle = icra,
  year      = 2018,
  url       = {http://www.ipb.uni-bonn.de/pdfs/huang2018icra.pdf}
}

@article{naseer2018tro,
  author   = {T. Naseer and W. Burgard and C. Stachniss},
  title    = {{Robust Visual Localization Across Seasons}},
  journal  = tro,
  volume   = {34},
  number   = {2},
  pages    = {289--302},
  year     = {2018},
  keywords = {Place Recognition, Localization},
  abstract = {Localization is an integral part of reliable robot navigation and long-term autonomy requires robustness against perceptional changes in the environment during localization. In the context of vision-based localization, such changes can be caused by illumination variations, occlusion, structural development, different weather conditions and seasons. In this paper, we present a novel approach for localizing a robot over longer periods of time using only monocular image data. We propose a novel data association approach for matching streams of incoming images to an image sequence stored in a database. Our method exploits network flows to leverage sequential information to improve the localization performance and to maintain several possible trajectories hypotheses in parallel. To compare images, we consider a semi-dense image description based on HOG features as well as global descriptors from Deep Convolutional Neural Networks trained on ImageNet for robust localization. We perform extensive evaluations on a variety of datasets and show that our approach outperforms existing state-of-the-art approaches.}
}

@article{border2018arxiv,
  author   = {R. Border and J.D. Gammell and P. Newman},
  title    = {{Surface Edge Explorer (SEE): Planning Next Best Views Directly from 3D Observations}},
  journal  = arxiv,
  year     = {2018},
  volume   = {arXiv:1802.08617},
  url      = {http://arxiv.org/pdf/1802.08617v1},
  abstract = {Surveying 3D scenes is a common task in robotics. Systems can do so autonomously by iteratively obtaining measurements. This process of planning observations to improve the model of a scene is called Next Best View (NBV) planning. NBV planning approaches often use either volumetric (e.g., voxel grids) or surface (e.g., triangulated meshes) representations. Volumetric approaches generalise well between scenes as they do not depend on surface geometry but do not scale to high-resolution models of large scenes. Surface representations can obtain high-resolution models at any scale but often require tuning of unintuitive parameters or multiple survey stages. This paper presents a scene-model-free NBV planning approach with a density representation. The Surface Edge Explorer (SEE) uses the density of current measurements to detect and explore observed surface boundaries. This approach is shown experimentally to provide better surface coverage in lower computation time than the evaluated state-of-the-art volumetric approaches while moving equivalent distances.}
}

@article{schlegel2018arxiv,
  author   = {D. Schlegel and G. Grisetti},
  title    = {{HBST: A Hamming Distance embedding Binary Search Tree for Visual Place Recognition}},
  journal  = arxiv,
  year     = {2018},
  volume   = {arXiv:1802.09261},
  url      = {http://arxiv.org/pdf/1802.09261v2},
  abstract = {Reliable and efficient Visual Place Recognition is a major building block of modern SLAM systems. Leveraging on our prior work, in this paper we present a Hamming Distance embedding Binary Search Tree (HBST) approach for binary Descriptor Matching and Image Retrieval. HBST allows for descriptor Search and Insertion in logarithmic time by exploiting particular properties of binary Feature descriptors. We support the idea behind our search structure with a thorough analysis on the exploited descriptor properties and their effects on completeness and complexity of search and insertion. To validate our claims we conducted comparative experiments for HBST and several state-of-the-art methods on a broad range of publicly available datasets. HBST is available as a compact open-source C++ header-only library.}
}

@article{florence2018arxiv,
  author   = {P.R. Florence and J. Carter and J. Ware and R. Tedrake},
  title    = {{NanoMap: Fast, Uncertainty-Aware Proximity Queries with Lazy Search over Local 3D Data}},
  journal  = arxiv,
  year     = {2018},
  volume   = {arXiv:1802.09076},
  url      = {http://arxiv.org/pdf/1802.09076v1},
  abstract = {We would like robots to be able to safely navigate at high speed, efficiently use local 3D information, and robustly plan motions that consider pose uncertainty of measurements in a local map structure. This is hard to do with previously existing mapping approaches, like occupancy grids, that are focused on incrementally fusing 3D data into a common world frame. In particular, both their fragile sensitivity to state estimation errors and computational cost can be limiting. We develop an alternative framework, NanoMap, which alleviates the need for global map fusion and enables a motion planner to efficiently query pose-uncertainty-aware local 3D geometric information. The key idea of NanoMap is to store a history of noisy relative pose transforms and search over a corresponding set of depth sensor measurements for the minimum-uncertainty view of a queried point in space. This approach affords a variety of capabilities not offered by traditional mapping techniques: (a) the pose uncertainty associated with 3D data can be incorporated in motion planning, (b) poses can be updated (i.e., from loop closures) with minimal computational effort, and (c) 3D data can be fused lazily for the purpose of planning. We provide an open-source implementation of NanoMap, and analyze its capabilities and computational efficiency in simulation experiments. Finally, we demonstrate in hardware its effectiveness for fast 3D obstacle avoidance onboard a quadrotor flying up to 10 m/s.}
}

@inproceedings{zhang2017icra,
  author    = {J. Zhang and S. Singh},
  title     = {{Enabling Aggressive Motion Estimation at Low-Drift and Accurate Mapping in Real-Time}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Range Sensing, Visual-Based Navigation, Mapping},
  abstract  = {We present a data processing pipeline to online estimate ego-motion and build a map of the traversed environment, leveraging data from a 3D laser, a camera, and an IMU. Different from traditional methods that use a Kalman filter or factor-graph optimization, the proposed method employs a sequential, multi-layer processing pipeline, solving for motion from coarse to fine. The resulting system enables highfrequency, low-latency ego-motion estimation, along with dense, accurate 3D map registration. Further, the system is capable of handling sensor degradation by automatic reconfiguration bypassing failure modules. Therefore, it can operate in the presence of highly dynamic motion as well as in dark, textureless, and structure-less environments. During experiments, the system demonstrates 0.22\% of relative position drift over 9.3km of navigation and robustness w.r.t aggressive motion such as highway speed driving (up to 33m/s).},
  url       = {http://www.frc.ri.cmu.edu/~jizhang03/Publications/ICRA_2017.pdf}
}

@inproceedings{tung2017icra,
  author    = {F. Tung and J.J. Little},
  title     = {{MF3D: Model-Free 3D Semantic Scene Parsing}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Semantic Scene Understanding, Recognition, Object detection, Segmentation, Categorization},
  abstract  = {We present a novel model-free method for online 3D semantic scene parsing from video sequences. MF3D (Model-Free 3D) is different from conventional methods for 3D scene parsing in that voxel labelling is approached via searchbased label transfer instead of discriminative classification. This non-parametric approach makes MF3D easy to scale with an online growth in the database, as no model re-training is required with the addition of new examples or categories. Experimental results on the KITTI benchmark demonstrate that our model-free approach enables accurate online 3D scene parsing while retaining extensibility to new categories. In addition, we show that unsupervised binary encoding (hashing) techniques can be easily incorporated into our framework for scalability to larger databases. Building Vegetation Car Wall/fence Pavement},
  url       = {http://www.sfu.ca/~ftung/papers/mf3d_icra17.pdf}
}

@inproceedings{weerasekera2017icra,
  author    = {C.S. Weerasekera and Y. Latif and R. Garg and I. Reid},
  title     = {{Dense Monocular Reconstruction Using Surface Normals}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Mapping, Semantic Scene Understanding, Visual-Based Navigation, CNN},
  abstract  = {This paper presents an efficient framework for dense 3D scene reconstruction using input from a moving monocular camera. Visual SLAM (Simultaneous Localisation and Mapping) approaches based solely on geometric methods have proven to be quite capable of accurately tracking the pose of a moving camera and simultaneously building a map of the environment in real-time. However, most of them suffer from the 3D map being too sparse for practical use. The missing points in the generated map correspond mainly to areas lacking texture in the input images, and dense mapping systems often rely on hand-crafted priors like piecewise-planarity or piecewise-smooth depth. These priors do not always provide the required level of scene understanding to accurately fill the map. On the other hand, Convolutional Neural Networks (CNNs) have had great success in extracting high-level information from images and regressing pixel-wise surface normals, semantics, and even depth. In this work we leverage this high-level scene context learned by a deep CNN in the form of a surface normal prior. We show, in particular, that using the surface normal prior leads to better reconstructions than the weaker smoothness prior.}
}

@inproceedings{barnes2017icra,
  author    = {D. Barnes and W. Maddern and I. Posner},
  title     = {{Find Your Own Way: Weakly-Supervised Segmentation of Path Proposals for Urban Autonomy}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Autonomous Vehicle Navigation, Computer Vision for Transportation, Motion and Path Planning},
  abstract  = {We present a weakly-supervised approach to segmenting proposed drivable paths in images with the goal of autonomous driving in complex urban environments. Using recorded routes from a data collection vehicle, our proposed method generates vast quantities of labelled images containing proposed paths and obstacles without requiring manual annotation, which we then use to train a deep semantic segmentation network. With the trained network we can segment proposed paths and obstacles at run-time using a vehicle equipped with only a monocular camera without relying on explicit modelling of road or lane markings. We evaluate our method on the largescale KITTI and Oxford RobotCar datasets and demonstrate reliable path proposal and obstacle segmentation in a wide variety of environments under a range of lighting, weather and traffic conditions. We illustrate how the method can generalise to multiple path proposals at intersections and outline plans to incorporate the system into a framework for autonomous urban driving.},
  url       = {https://arxiv.org/pdf/1610.01238.pdf}
}

@inproceedings{jellal2017icra,
  author    = {R.A. Jellal and M. Lange and B. Wassermann and A. Schilling and A. Zell},
  title     = {{LS-ELAS: Line Segment Based Efficient Large Scale Stereo Matching}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Computer Vision for Other Robotic Applications, Mapping, RGB-D Perception},
  abstract  = {We present LS-ELAS, a line segment extension to the ELAS algorithm, which increases the performance and robustness. LS-ELAS is a binocular dense stereo matching algorithm, which computes the disparities in constant time for most of the pixels in the image and in linear time for a small subset of the pixels (support points). Our approach is based on line segments to determine the support points instead of uniformly selecting them over the image range. This way we find very informative support points which preserve the depth discontinuity. The prior of our Bayesian stereo matching method is based on a set of line segments and a set of support points. Both sets are given to a constrained Delaunay triangulation to generate a triangulation mesh which is aware of possible depth discontinuities. We further increased the accuracy by using an adaptive method to sample candidate points along edge segments. We evaluated our algorithm on the Middlebury benchmark.},
  url       = {http://www.cogsys.cs.uni-tuebingen.de/publikationen/2017/JellalICRA17.pdf}
}

@inproceedings{dzitsiuk2017icra,
  author    = {M. Dzitsiuk and J. Sturm and R. Maier and L. Ma and D. Cremers},
  title     = {{De-Noising, Stabilizing and Completing 3D Reconstructions On-The-Go Using Plane Priors}},
  booktitle = icra,
  year      = 2017,
  keywords  = {RGB-D Perception, Object detection, Segmentation, Categorization, Semantic Scene Understanding},
  abstract  = {Creating 3D maps on robots and other mobile devices has become a reality in recent years. Online 3D reconstruction enables many exciting applications in robotics and AR/VR gaming. However, the reconstructions are noisy and generally incomplete. Moreover, during online reconstruction, the surface changes with every newly integrated depth image which poses a significant challenge for physics engines and path planning algorithms. This paper presents a novel, fast and robust method for obtaining and using information about planar surfaces, such as walls, floors, and ceilings as a stage in 3D reconstruction based on Signed Distance Fields (SDFs). Our algorithm recovers clean and accurate surfaces, reduces the movement of individual mesh vertices caused by noise during online reconstruction and fills in the occluded and unobserved regions. We implemented and evaluated two different strategies to generate plane candidates and two strategies for merging them. Our implementation is optimized to run in real-time on mobile devices such as the Tango tablet. In an extensive set of experiments, we validated that our approach works well in a large number of natural environments despite the presence of significant amount of occlusion, clutter and noise, which occur frequently. We further show that plane fitting enables in many cases a meaningful semantic segmentation of real-world scenes.},
  url       = {https://vision.in.tum.de/_media/spezial/bib/dzitsiuk2017icra.pdf}
}

@inproceedings{rehder2017icra,
  author    = {J. Rehder and J. Nikolic and T. Schneider and R. Siegwart},
  title     = {{A Direct Formulation for Camera Calibration}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Calibration and Identification, Sensor Fusion},
  abstract  = {Conventional camera calibration techniques rely on discrete reference points extracted from a set of input images. While these approaches have been applied successfully for a long time, omitting all image information apart from reference point positions at the initial stage of the calibration pipeline renders correct treatment of uncertainties difficult and gives rise to complications in timestamping measurements in applications where exposure time cannot be neglected. Drawing inspiration from visual state estimation, we employ a direct formulation of the camera measurement model. To this end, we render a view of the target given all calibration parameters, enabling a maximum likelihood estimator formulated on image intensities as measurements. We demonstrate the advantages of avoiding abstraction from image measurements for determining the line delay of a rolling shutter camera and by estimating camera exposure time from motion blur.}
}

@inproceedings{chen2017icra,
  author    = {J. Chen and S. Shen},
  title     = {{Improving Octree-Based Occupancy Maps Using Environment Sparsity with Application to Aerial Robot Navigation}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Mapping, Aerial Robotics, Autonomous Vehicle Navigation},
  abstract  = {In this paper, we present an improved octreebased mapping framework for autonomous navigation of mobile robots. Octree is best known for its memory efficiency for representing large-scale environments. However, existing implementations, including the state-of-the-art OctoMap [1], are computationally too expensive for online applications that require frequent map updates and inquiries. Utilizing the sparse nature of the environment, we propose a ray tracing method with early termination for efficient probabilistic map update. We also propose a divide-and-conquer volume occupancy inquiry method which serves as the core operation for generation of free-space configurations for optimization-based trajectory generation. We experimentally demonstrate that our method maintains the same storage advantage of the original OctoMap, but being computationally more efficient for map update and occupancy inquiry. Finally, by integrating the proposed map structure in a complete navigation pipeline, we show autonomous quadrotor flight through complex environments.}
}

@inproceedings{pfrommer2017icra,
  author    = {B. Pfrommer and N.J. Sanket and K. Daniilidis and J. Cleveland},
  title     = {{PennCOSYVIO: A Challenging Visual Inertial Odometry Benchmark}},
  booktitle = icra,
  year      = 2017,
  keywords  = {SLAM, Localization, Mapping},
  abstract  = {We present PennCOSYVIO, a new challenging Visual Inertial Odometry (VIO) benchmark with synchronized data from a VI-sensor (stereo camera and IMU), two Project Tango hand-held devices, and three GoPro Hero 4 cameras. Recorded at UPenns Singh center, the 150m long path of the hand-held rig crosses from outdoors to indoors and includes rapid rotations, thereby testing the abilities of VIO and Simultaneous Localization and Mapping (SLAM) algorithms to handle changes in lighting, different textures, repetitive structures, and large glass surfaces. All sensors are synchronized and intrinsically and extrinsically calibrated. We demonstrate the accuracy with which ground-truth poses can be obtained via optic localization off of fiducial markers. The data set can be found at https://daniilidis-group.github.io/penncosyvio/. Fig. 1.}
}

@inproceedings{zhang2017icra-rsmw,
  author    = {S. Zhang and W. Xie and G. Zhang and H. Bao and M. Kaess},
  title     = {{Robust Stereo Matching with Surface Normal Prediction}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Mapping, Range Sensing, Computer Vision for Other Robotic Applications},
  abstract  = {Traditional stereo matching approaches generally have problems in handling textureless regions, strong occlusions and reflective regions that do not satisfy a Lambertian surface assumption. In this paper, we propose to combine the predicted surface normal by deep learning to overcome these inherent difficulties in stereo matching. With the selected reliable disparities from stereo matching method and effective edge fusion strategy, we can faithfully convert the predicted surface normal map to a disparity map by solving a least squares system which maintains discontinuity on object boundaries and continuity on other regions. Then we refine the disparity map iteratively by bilateral filtering-based completion and edge feature refinement. Experimental results on the Middlebury dataset and our own captured stereo sequences demonstrate the effectiveness of the proposed approach.},
  url       = {http://frc.ri.cmu.edu/~kaess/pub/Zhang17icra.pdf}
}

@inproceedings{chan2017icra,
  author    = {S. Chan and X. Zhou and Z. Zhang and S. Chen},
  title     = {{Compressive Tracking with Locality Sensitive Histograms Features}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Visual Tracking, Computer Vision for Automation, Surveillance Systems},
  abstract  = {Currently, Compressive Tracking (CT) method has drawn great attention because of its high efficiency. However, it cannot well deal with some appearance variations due to its limitations of feature expression and it only uses a fixed parameter to update the appearance model. In order to handle such matters, we propose an adaptive CT method that combines the predicted target position with CT based on Locality Sensitive Histograms (LSH) features. Our method significantly improves CT in four aspects. First, the efficient illumination invariant features extracted based on LSH are used to represent an effective appearance model that is robust to illumination changes. Second, the color attributes tracker is adopted to predict the target position for re-building the new weighted discriminant function which brings in the color information to make up for the inadequacy of Haarlike characteristics. Third, a new model update mechanism is proposed to preserve the stable features while avoid the noisy appearance variations during tracking. Fourth, a trajectory rectification method is employed to refine the tracking location when possible inaccurate tracking occurs. Finally, we show that our tracker achieves state-of-the-art performance in a comprehensive evaluation over 47 challenging color sequences.}
}

@inproceedings{hall2017icra,
  author    = {D. Hall and F. Dayoub and J. Kulk and C.S. McCool},
  title     = {{Towards Unsupervised Weed Scouting for Agricultural Robotics}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Object detection, Segmentation, Categorization, Agricultural Automation},
  abstract  = {Weed scouting is an important part of modern integrated weed management but can be time consuming and sparse when performed manually. Automated weed scouting and weed destruction has typically been performed using classification systems able to classify a set group of species known a priori. This greatly limits deployability as classification systems must be retrained for any field with a different set of weed species present within them. In order to overcome this limitation, this paper works towards developing a clustering approach to weed scouting which can be utilized in any field without the need for prior species knowledge. We demonstrate our system using challenging data collected in the field from an agricultural robotics platform. We show that considerable improvements can be made by (i) learning low-dimensional (bottleneck) features using a deep convolutional neural network to represent plants in general and (ii) tying views of the same area (plant) together. Deploying this algorithm on in-field data collected by AgBotII, we are able to successfully cluster cotton plants from grasses without prior knowledge or training for the specific plants in the field.},
  url       = {https://arxiv.org/pdf/1702.01247.pdf}
}

@inproceedings{peretroukhin2017icra,
  author    = {V. Peretroukhin and L. Clement and J. Kelly},
  title     = {{Reducing Drift in Visual Odometry by Inferring Sun Direction Using a Bayesian Convolutional Neural Network}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Visual-Based Navigation, Visual Learning, Localization, CNN},
  abstract  = {We present a method to incorporate global orientation information from the sun into a visual odometry pipeline using only the existing image stream, where the sun is typically not visible. We leverage recent advances in Bayesian Convolutional Neural Networks to train and implement a sun detection model that infers a three-dimensional sun direction vector from a single RGB image. Crucially, our method also computes a principled uncertainty associated with each prediction, using a Monte Carlo dropout scheme. We incorporate this uncertainty into a sliding window stereo visual odometry pipeline where accurate uncertainty estimates are critical for optimal data fusion. Our Bayesian sun detection model achieves a median error of approximately 12 degrees on the KITTI odometry benchmark training set, and yields improvements of up to 42\% in translational ARMSE and 32\% in rotational ARMSE compared to standard VO. An open source implementation of our Bayesian CNN sun estimator (Sun-BCNN) using Caffe is available at https://github. com/utiasSTARS/sun-bcnn-vo.}
}

@inproceedings{sun2017icra,
  author    = {W. Sun and N. Sood and D. Dey and G. Ranade and S. Prakash and A. Kapoor},
  title     = {{No-Regret Replanning under Uncertainty}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Probability and Statistical Methods, AI Reasoning Methods, Reactive and Sensor-Based Planning},
  abstract  = {This paper explores the problem of path planning under uncertainty. Specifically, we consider online receding horizon based planners that need to operate in a latent environment where the latent information can be modelled via Gaussian Processes. Online path planning in latent environments is challenging since the robot needs to explore the environment to get a more accurate model of latent information for better planning later and also achieves the task as quick as possible. We propose UCB style algorithms that are popular in the bandit settings and show how those analyses can be adapted to the online robotic path planning problems. The proposed algorithm trades-off exploration and exploitation in near-optimal manner and has appealing no-regret properties. We demonstrate the efficacy of the framework on the application of aircraft flight path planning when the winds are partially observed.}
}

@inproceedings{kim2017icra,
  author    = {P. Kim and B. Coltin and O. Alexandrov and H.J. Kim},
  title     = {{Robust Visual Localization in Changing Lighting Conditions}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Space Robotics, Localization, Visual-Based Navigation},
  abstract  = {We present an illumination-robust visual localization algorithm for Astrobee, a free-flying robot designed to autonomously navigate on the International Space Station (ISS). Astrobee localizes with a monocular camera and a prebuilt sparse map composed of natural visual features. Astrobee must perform tasks not only during the day, but also at night when the ISS lights are dimmed. However, the localization performance degrades when the observed lighting conditions differ from the conditions when the sparse map was built. We investigate and quantify the effect of lighting variations on visual feature-based localization systems, and discover that maps built in darker conditions can also be effective in bright conditions, but the reverse is not true. We extend Astrobees localization algorithm to make it more robust to changinglight environments on the ISS by automatically recognizing the current illumination level, and selecting an appropriate map and camera exposure time. We extensively evaluate the proposed algorithm through experiments on Astrobee.}
}

@inproceedings{zermas2017icra,
  author    = {D. Zermas and I. Izzat and N. Papanikolopoulos},
  title     = {{Fast Segmentation of 3D Point Clouds: A Paradigm on Lidar Data for Autonomous Vehicle Applications}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Intelligent Transportation Systems, Computer Vision for Transportation},
  abstract  = {The recent activity in the area of autonomous vehicle navigation has initiated a series of reactions that stirred the automobile industry, pushing for the fast commercialization of this technology which, until recently, seemed futuristic. The LiDAR sensor is able to provide a detailed understanding of the environment surrounding the vehicle making it useful in a plethora of autonomous driving scenarios. Segmenting the 3D point cloud that is provided by modern LiDAR sensors, is the first important step towards the situational assessment pipeline that aims for the safety of the passengers. This step needs to provide accurate segmentation of the ground surface and the obstacles in the vehicles path, and to process each point cloud in real time. The proposed pipeline aims to solve the problem of 3D point cloud segmentation for data received from a LiDAR in a fast and low complexity manner that targets real world applications. The two-step algorithm first extracts the ground surface in an iterative fashion using deterministically assigned seed points, and then clusters the remaining non-ground points taking advantage of the structure of the LiDAR point cloud. Our proposed algorithms outperform similar approaches in running time, while producing similar results and support the validity of this pipeline as a segmentation tool for real world applications.}
}

@inproceedings{engelcke2017icra,
  author    = {M. Engelcke and D. Rao and D.Z. Wang and C.H. Tong and I. Posner},
  title     = {{Vote3Deep: Fast Object Detection in 3D Point Clouds Using Efficient Convolutional Neural Networks}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Object detection, Segmentation, Categorization, Learning and Adaptive Systems, CNN, Range Sensing},
  abstract  = {This paper proposes a computationally efficient approach to detecting objects natively in 3D point clouds using convolutional neural networks (CNNs). In particular, this is achieved by leveraging a feature-centric voting scheme to implement novel convolutional layers which explicitly exploit the sparsity encountered in the input. To this end, we examine the trade-off between accuracy and speed for different architectures and additionally propose to use an L1 penalty on the filter activations to further encourage sparsity in the intermediate representations. To the best of our knowledge, this is the first work to propose sparse convolutional layers and L1 regularisation for efficient large-scale processing of 3D data. We demonstrate the efficacy of our approach on the KITTI object detection benchmark and show that Vote3Deep models with as few as three layers outperform the previous state of the art in both laser and laser-vision based approaches by margins of up to 40\% while remaining highly competitive in terms of processing time.}
}

@inproceedings{ma2017icra,
  author    = {W. Ma and S. Wang and M.A. Brubaker and S. Fidler and R. Urtasun},
  title     = {{Find Your Way by Observing the Sun and Other Semantic Cues}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Localization, Autonomous Vehicle Navigation, Visual-Based Navigation},
  abstract  = {In this paper we present a robust, efficient and affordable approach to self-localization which requires neither GPS nor knowledge about the appearance of the world. Towards this goal, we utilize freely available cartographic maps and derive a probabilistic model that exploits semantic cues in the form of sun direction, presence of an intersection, road type, speed limit and ego-car trajectory to produce very reliable localization results. Our experimental evaluation shows that our approach can localize much faster (in terms of driving time) with less computation and more robustly than competing approaches, which ignore semantic information.}
}

@inproceedings{yang2017icra,
  author    = {S. Yang and S. Scherer},
  title     = {{Direct Monocular Odometry Using Points and Lines}},
  booktitle = icra,
  year      = 2017,
  keywords  = {SLAM, Mapping, Localization},
  abstract  = {Most visual odometry algorithm for a monocular camera focuses on points, either by feature matching, or direct alignment of pixel intensity, while ignoring a common but important geometry entity: edges. In this paper, we propose an odometry algorithm that combines points and edges to benefit from the advantages of both direct and feature based methods. It works better in texture-less environments and is also more robust to lighting changes and fast motion by increasing the convergence basin. We maintain a depth map for the keyframe then in the tracking part, the camera pose is recovered by minimizing both the photometric error and geometric error to the matched edge in a probabilistic framework. In the mapping part, edge is used to speed up and increase stereo matching accuracy. On various public datasets, our algorithm achieves better or comparable performance than state-of-theart monocular odometry methods. In some challenging textureless environments, our algorithm reduces the state estimation error over 50\%.}
}

@inproceedings{fehr2017icra,
  author    = {M. Fehr and F. Furrer and I. Dryanovski and J. Sturm and I. Gilitschenski and R. Siegwart and C.C. Lerma},
  title     = {{TSDF-Based Change Detection for Consistent Long-Term Dense Reconstruction and Dynamic Object Discovery}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Object detection, Segmentation, Categorization, RGB-D Perception, Mapping},
  abstract  = {Robots that are operating for extended periods of time need to be able to deal with changes in their environment and represent them adequately in their maps. In this paper, we present a novel 3D reconstruction algorithm based on an extended Truncated Signed Distance Function (TSDF) that enables to continuously refine the static map while simultaneously obtaining 3D reconstructions of dynamic objects in the scene. This is a challenging problem because map updates happen incrementally and are often incomplete. Previous work typically performs change detection on point clouds, surfels or maps, which are not able to distinguish between unexplored and empty space. In contrast, our TSDF-based representation naturally contains this information and thus allows us to more robustly solve the scene differencing problem. We demonstrate the algorithms performance as part of a system for unsupervised object discovery and class recognition. We evaluated our algorithm on challenging datasets that we recorded over several days with RGB-D enabled tablets. To stimulate further research in this area, all of our datasets are publicly available3 .}
}

@inproceedings{dong2017icra,
  author    = {J. Dong and J.G. Burnham and B. Boots and G. Rains and F. Dellaert},
  title     = {{4D Crop Monitoring: Spatio-Temporal Reconstruction for Agriculture}},
  booktitle = icra,
  year      = 2017,
  keywords  = {SLAM, Agricultural Automation, Field Robots},
  abstract  = {Autonomous crop monitoring at high spatial and temporal resolution is a critical problem in precision agriculture. While Structure from Motion and Multi-View Stereo algorithms can finely reconstruct the 3D structure of a field with low-cost image sensors, these algorithms fail to capture the dynamic nature of continuously growing crops. In this paper we propose a 4D reconstruction approach to crop monitoring, which employs a spatio-temporal model of dynamic scenes that is useful for precision agriculture applications. Additionally, we provide a robust data association algorithm to address the problem of large appearance changes due to scenes being viewed from different angles at different points in time, which is critical to achieving 4D reconstruction. Finally, we collected a high-quality dataset with ground-truth statistics to evaluate the performance of our method. We demonstrate that our 4D reconstruction approach provides models that are qualitatively correct with respect to visual appearance and quantitatively accurate when measured against the ground truth geometric properties of the monitored crops.}
}

@inproceedings{leonardos2017icra,
  author    = {S. Leonardos and X. Zhou and K. Daniilidis},
  title     = {{Distributed Consistent Data Association Via Permutation Synchronization}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Sensor Networks, Agent-Based Systems, Autonomous Agents},
  abstract  = {Data association is one of the fundamental problems in multi-sensor systems. Most current techniques rely on pairwise data associations which can be spurious even after the employment of outlier rejection schemes. Considering multiple pairwise associations at once significantly increases accuracy and leads to consistency. In this work, we propose a fully decentralized method for globally consistent data association from pairwise data associations based on a distributed averaging scheme on the set of doubly stochastic matrices. We demonstrate the effectiveness of the proposed method using theoretical analysis and experimental evaluation.}
}

@inproceedings{wang2017icra,
  author    = {S. Wang and R. Clark and H. Wen and N. Trigoni},
  title     = {{DeepVO: Towards End-To-End Visual Odometry with Deep Recurrent Convolutional Neural Networks}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Visual-Based Navigation, Localization, CNN},
  abstract  = {This paper studies monocular visual odometry (VO) problem. Most of existing VO algorithms are developed under a standard pipeline including feature extraction, feature matching, motion estimation, local optimisation, etc. Although some of them have demonstrated superior performance, they usually need to be carefully designed and specifically fine-tuned to work well in different environments. Some prior knowledge is also required to recover an absolute scale for monocular VO. This paper presents a novel end-to-end framework for monocular VO by using deep Recurrent Convolutional Neural Networks (RCNNs). Since it is trained and deployed in an end-to-end manner, it infers poses directly from a sequence of raw RGB images (videos) without adopting any module in the conventional VO pipeline. Based on the RCNNs, it not only automatically learns effective feature representation for the VO problem through Convolutional Neural Networks, but also implicitly models sequential dynamics and relations using deep Recurrent Neural Networks. Extensive experiments on the KITTI VO dataset show competitive performance to state-ofthe-art methods, verifying that the end-to-end Deep Learning technique can be a viable complement to the traditional VO systems.}
}

@inproceedings{loquercio2017icra,
  author    = {A. Loquercio and M.T. Dymczyk and B. Zeisl and S. Lynen and R. Siegwart and I. Gilitschenski},
  title     = {{Efficient Descriptor Learning for Large Scale Localization}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Localization, Mapping, Recognition},
  abstract  = {Many robotics and Augmented Reality (AR) systems that use sparse keypoint-based visual maps operate in large and highly repetitive environments, where pose tracking and localization are challenging tasks. Additionally, these systems usually face further challenges, such as limited computational power, or insufficient memory for storing large maps of the entire environment. Thus, developing compact map representations and improving retrieval is of considerable interest for enabling large-scale visual place recognition and loop-closure. In this paper, we propose a novel approach to compress descriptors while increasing their discriminability and matchability, based on recent advances in neural networks. At the same time, we target resource-constrained robotics applications in our design choices. The main contributions of this work are twofold. First, we propose a linear projection from descriptor space to a lower-dimensional Euclidean space, based on a novel supervised learning strategy employing a triplet loss. Second, we show the importance of including contextual appearance information to the visual feature in order to improve matching under strong viewpoint, illumination and scene changes. Through detailed experiments on three challenging datasets, we demonstrate significant gains in performance over state-ofthe-art methods.}
}

@inproceedings{wang2017icra-fflb,
  author    = {X. Wang and S. Vozar and E. Olson},
  title     = {{FLAG: Feature-based Localization between Air and Ground}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Localization, Mapping, Computer Vision for Other Robotic Applications},
  abstract  = {In GPS-denied environments, robot systems typically revert to navigating with dead-reckoning and relative mapping, accumulating error in their global pose estimate. In this paper, we propose Feature-based Localization between Air and Ground (FLAG), a method for computing global position updates by matching features observed from ground to features in an aerial image. Our method uses stable, descriptorless features associated with vertical structure in the environment around a ground robot in previously unmapped areas, referencing only overhead imagery, without GPS. Multiple-hypothesis data association with a particle filter enables efficient recovery from data association error and odometry uncertainty. We implement a stereo system to demonstrate our vertical feature based global positioning approach in both indoor and outdoor scenarios, and show comparable performance to laser-scanmatching results in both environments.}
}

@inproceedings{xu2017icra,
  author    = {D. Xu and X. He and H. Zhao and J. Cui and H. Zha and F. Guillemard and S. Geronimi and F. Aioun},
  title     = {{Ego-Centric Traffic Behavior Understanding through Multi-Level Vehicle Trajectory Analysis}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Intelligent Transportation Systems},
  abstract  = {This study proposes a multi-level trajectory analysis method for modeling traffic behavior from an ego-centric view, where on-road vehicle trajectories are collected based on the authors previous studies of an on-board system consisting of multiple 2D lidar sensors. From an input set of trajectories, a set of hot regions (topics) that trajectory points most frequently present are first discovered using a sticky HDP-HMM; then, the major paths of the trajectories transitions across different hot regions are extracted by recursively mining frequent subsequences of topics; and finally, paths are modeled using a hierarchical hidden Markov model (HHMM), where the intra-path dynamics is represented using an HMM, in which each state corresponds to a hot region, while the inter-path transition is assumed to be Markovian. The model could be used for behavior prediction, i.e. whenever a vehicle is detected in a scene, predicting which route it will probably follow and how its trajectory will probably develop over time, which is essential to interpreting the potential risks for longer time horizons. Experiments are conducted using a large set of vehicle trajectories collected from motorways in Beijing, and promising results are presented.}
}

@inproceedings{hsiao2017icra,
  author    = {M. Hsiao and E. Westman and G. Zhang and M. Kaess},
  title     = {{Keyframe-Based Dense Planar SLAM}},
  booktitle = icra,
  year      = 2017,
  keywords  = {SLAM, Mapping, Localization},
  abstract  = {In this work, we develop a novel keyframe-based dense planar SLAM (KDP-SLAM) system, based on CPU only, to reconstruct large indoor environments in real-time using a hand-held RGB-D sensor. Our keyframe-based approach applies a fast dense method to estimate odometry, fuses depth measurements from small baseline images, extracts planes from the fused depth map, and optimizes the poses of the keyframes and landmark planes in a global factor graph using incremental smoothing and mapping (iSAM). Using the fast odometry estimation, correct plane correspondences may be found projectively, and the pose of each frame can be estimated accurately even without sufficient planes to fully constrain the 6 degree-of-freedom transformation. The depth map generated from the local fusion process generates higher quality reconstructions and plane segmentations by eliminating noise. Moreover, explicitly modeling plane landmarks in the fully probabilistic global optimization significantly reduces the drift that plagues other dense SLAM algorithms. We test our system on standard RGB-D benchmarks as well as additional indoor environments, demonstrating its state-of-the-art performance as a real-time dense 3D SLAM algorithm, without the use of GPU.}
}

@inproceedings{salaris2017icra,
  author    = {P. Salaris and R. Spica and P.R. Giordano and P. Rives},
  title     = {{Online Optimal Active Sensing Control}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Motion and Path Planning, Optimization and Optimal Control, Localization},
  abstract  = {This paper deals with the problem of active sensing control for nonlinear differentially flat systems. The objective is to improve the estimation accuracy of an observer by determining the inputs of the system that maximise the amount of information gathered by the outputs over a time horizon. In particular, we use the Observability Gramian (OG) to quantify the richness of the acquired information. First, we define a trajectory for the flat outputs of the system by using B-Spline curves. Then, we exploit an online gradient descent strategy to move the control points of the B-Spline in order to actively maximise the smallest eigenvalue of the OG over the whole planning horizon. While the system travels along its planned (optimized) trajectory, an Extended Kalman Filter (EKF) is used to estimate the system state. In order to keep memory of the past acquired sensory data for online re-planning, the OG is also computed on the past estimated state trajectories. This is then used for an online replanning of the optimal trajectory during the robot motion which is continuously refined by exploiting the state estimation obtained by the EKF. In order to show the effectiveness of our method we consider a simple but significant case of a planar robot with a single range measurement. The simulation results show that, along the optimal path, the EKF converges faster and provides a more accurate estimate than along other possible (non-optimal) paths.}
}


@inproceedings{bargoti2017icra,
  author    = {S. Bargoti and J.P. Underwood},
  title     = {{Deep Fruit Detection in Orchards}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Robotics in Agriculture and Forestry, Computer Vision for Other Robotic Applications, Object Detection, Segmentation, Categorization, CNN},
  abstract  = {An accurate and reliable image based fruit detection system is critical for supporting higher level agriculture tasks such as yield mapping and robotic harvesting. This paper presents the use of a state-of-the-art object detection framework, Faster R-CNN, in the context of fruit detection in orchards, including mangoes, almonds and apples. Ablation studies are presented to better understand the practical deployment of the detection network, including how much training data is required to capture variability in the dataset. Data augmentation techniques are shown to yield significant performance gains, resulting in a greater than two-fold reduction in the number of training images required. In contrast, transferring knowledge between orchards contributed to negligible performance gain over initialising the Deep Convolutional Neural Network directly from ImageNet features. Finally, to operate over orchard data containing between 100-1000 fruit per image, a tiling approach is introduced for the Faster R-CNN framework. The study has resulted in the best yet detection performance for these orchards relative to previous works, with an F1-score of > 0.9 achieved for apples and mangoes.}
}

@inproceedings{carlucci2017icra,
  author    = {F.M. Carlucci and P. Russo and B. Caputo},
  title     = {{A Deep Representation for Depth Images from Synthetic Data}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Object detection, Segmentation, Categorization, RGB-D Perception, Computer Vision for Other Robotic Applications, CNN},
  abstract  = {Convolutional Neural Networks (CNNs) trained on large scale RGB databases have become the secret sauce in the majority of recent approaches for object categorization from RGB-D data. Thanks to colorization techniques, these methods exploit the filters learned from 2D images to extract meaningful representations in 2.5D. Still, the perceptual signature of these two kind of images is very different, with the first usually strongly characterized by textures, and the second mostly by silhouettes of objects. Ideally, one would like to have two CNNs, one for RGB and one for depth, each trained on a suitable data collection, able to capture the perceptual properties of each channel for the task at hand. This has not been possible so far, due to the lack of a suitable depth database. This paper addresses this issue, proposing to opt for synthetically generated images rather than collecting by hand a 2.5D large scale database. While being clearly a proxy for real data, synthetic images allow to trade quality for quantity, making it possible to generate a virtually infinite amount of data. We show that the filters learned from such data collection, using the very same architecture typically used on visual data, learns very different filters, resulting in depth features (a) able to better characterize the different facets of depth images, and (b) complementary with respect to those derived from CNNs pretrained on 2D datasets. Experiments on two publicly available databases show the power of our approach.}
}

@inproceedings{suger2017icra,
  author    = {B. Suger and W. Burgard},
  title     = {{Global Outer-Urban Navigation with OpenStreetMap}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Autonomous Vehicle Navigation, Field Robots, Localization},
  abstract  = {Publicly available map services are widely used by humans for navigation and nowadays provide almost complete road network data. When utilizing such maps for autonomous navigation with mobile robots one is faced with the problem of inaccuracies of the map and the uncertainty about the position of the robot relative to the map. In this paper, we present a probabilistic approach to autonomous robot navigation using data from OpenStreetMap that associates tracks from OpenStreeetMap with the trails detected by the robot based on its 3DLiDAR data. It combines semantic terrain information, derived from the 3D-LiDAR data, with a Markov-Chain Monte-Carlo technique to match the tracks from OpenStreetMap with the sensor data. This enables our robot to utilize OpenStreetMap for navigation planning and to still stay on the trails during the execution of these plans. We present the results of extensive experiments carried out in real world settings that demonstrate the robustness of our system regarding the alignment of the vehicle pose relative to the OpenStreetMap data.}
}

@inproceedings{naseer2017icra,
  author    = {T. Naseer and G. Oliveira and T. Brox and W. Burgard},
  title     = {{Semantics-Aware Visual Localization under Challenging Perceptual Conditions}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Localization, Visual Learning, Semantic Scene Understanding},
  abstract  = {Visual place recognition under difficult perceptual conditions remains a challenging problem due to changing weather conditions, illumination and seasons. Long-term visual navigation approaches for robot localization should be robust to these dynamics of the environment. Existing methods typically leverage feature descriptions of whole images or image regions from Deep Convolutional Neural Networks. Some approaches also exploit sequential information to alleviate the problem of spatially inconsistent and non-perfect image matches. In this paper, we propose a novel approach for learning a discriminative holistic image representation which exploits the image content to create a dense and salient scene description. These salient descriptions are learnt over a variety of datasets under large perceptual changes. Such an approach enables us to precisely segment the regions of an image which are geometrically stable over large time lags. We combine features from these salient regions and an off-the-shelf holistic representation to form a more robust scene descriptor. We also introduce a semantically labeled dataset which captures extreme perceptual and structural scene dynamics over the course of 3 years. We evaluated our approach with extensive experiments on data collected over several kilometers in Freiburg and show that our learnt image representation outperforms off-the-shelf features from the deep networks and hand-crafted features.}
}

@inproceedings{lottes2017icra,
  author    = {P. Lottes and R. Khanna and J. Pfeifer and R. Siegwart and C. Stachniss},
  title     = {{UAV-Based Crop and Weed Classification for Smart Farming}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Robotics in Agriculture and Forestry, Mapping},
  abstract  = {Unmanned aerial vehicles (UAVs) and other robots in smart farming applications offer the potential to monitor farm land on a per-plant basis, which in turn can reduce the amount of herbicides and pesticides that must be applied. A central information for the farmer as well as for autonomous agriculture robots is the knowledge about the type and distribution of the weeds in the field. In this regard, UAVs offer excellent survey capabilities at low cost. In this paper, we address the problem of detecting value crops such as sugar beets as well as typical weeds using a camera installed on a light-weight UAV. We propose a system that performs vegetation detection, plant-tailored feature extraction, and classification to obtain an estimate of the distribution of crops and weeds in the field. We implemented and evaluated our system using UAVs on two farms, one in Germany and one in Switzerland and demonstrate that our approach allows for analyzing the field and classifying individual plants.},
  url       = {http://www.ipb.uni-bonn.de/wp-content/papercite-data/pdf/lottes17icra.pdf}
}

@inproceedings{roelofsen2017icra,
  author    = {S. Roelofsen and D. Gillet and A. Martinoli},
  title     = {{Collision Avoidance with Limited Field of View Sensing: A Velocity Obstacle Approach}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Collision Avoidance, Distributed Robot Systems, Sensor-based Control},
  abstract  = {Collision avoidance, in particular between robots, is an important component for autonomous robots. It is a necessary component in numerous applications such as humanrobot interaction, automotive or unmanned aerial vehicles. While many collision avoidance algorithms take into account actuation constraints, only a few consider sensing limitations. In this paper, we present a reciprocal collision avoidance algorithm based on the velocity obstacle approach that guarantees collision-free maneuvers even when the robots are only capable to sense their environment within a limited Field Of View (FOV). We also present the challenges associated to sensors with limited FOV, show the conditions under which maneuvering can be safely done, and the modifications that a velocity obstacle approach requires to satisfy such conditions. We provide simulations and real robot experiments to validate our approach.}
}

@inproceedings{fukui2017icra,
  author    = {R. Fukui and J. Schneider and T. Nishioka and S. Warisawa and I. Yamada},
  title     = {{Growth Measurement of Tomato Fruit Based on Whole Image Processing}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Computer Vision for Other Robotic Applications, Robotics in Agriculture and Forestry, Object detection, Segmentation, Categorization},
  abstract  = {- Crop grow measurement technologies are important to increase the farm productivity. Detection and measurement of fruit volume are useful for forecasting and harvesting applications. Some environmental challenges such as lighting conditions or occlusions make the fruit detection difficult. Our approach is based on features extraction from images through a sub-image clustering technique. Then images being described as a number of pixel in various labels are used in a regression model to estimate the fruit volume. The validity of the proposed method in experimental condition is successfully verified. The method is evaluated also in a field condition but results were inferior to the expectation. This paper tries to elucidate the reasons of the insufficient performance and tries to improve the proposed method in terms of illumination condition, precision and calculation time. Key Words: Agriculture, Field Robotics Growth measurement, Image processing, Tomato volume, Regression}
}

@inproceedings{yang2017icra-rmdm,
  author    = {Z. Yang and F. Gao and S. Shen},
  title     = {{Real-Time Monocular Dense Mapping on Aerial Robots Using Visual-Inertial Fusion}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Aerial Robotics, Mapping, Autonomous Vehicle Navigation},
  abstract  = {In this work, we present a solution to real-time monocular dense mapping. A tightly-coupled visual-inertial localization module is designed to provide metric and highaccuracy odometry. A motion stereo algorithm is proposed to take the video input from one camera to produce local depth measurements with semi-global regularization. The local measurements are then integrated into a global map for noise filtering and map refinement. The global map obtained is able to support navigation and obstacle avoidance for aerial robots through our indoor and outdoor experimental verification. Our system runs at 10Hz on an Nvidia Jetson TX1 by properly distributing computation to CPU and GPU. Through onboard experiments, we demonstrate its ability to close the perceptionaction loop for autonomous aerial robots. We release our implementation as open-source software1 .}
}

@inproceedings{gehrig2017icra,
  author    = {M. Gehrig and E. Stumm and T. Hinzmann and R. Siegwart},
  title     = {{Visual Place Recognition with Probabilistic Voting}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Localization, SLAM, Recognition},
  abstract  = {We propose a novel scoring concept for visual place recognition based on nearest neighbor descriptor voting and demonstrate how the algorithm naturally emerges from the problem formulation. Based on the observation that the number of votes for matching places can be evaluated using a binomial distribution model, loop closures can be detected with high precision. By casting the problem into a probabilistic framework, we not only remove the need for commonly employed heuristic parameters but also provide a powerful score to classify matching and non-matching places. We present methods for both a 2D-2D image matching and a 2D-3D landmark matching based on the above scoring. The approach maintains accuracy while being efficient enough for online application through the use of compact (low-dimensional) descriptors and fast nearest neighbor retrieval techniques. The proposed methods are evaluated on several challenging datasets in varied environments, showing state-of-the-art results with high precision and high recall.}
}

@inproceedings{jatavallabhula2017icra,
  author    = {K.M. Jatavallabhula and S.K. Gottipati and F. Chhaya and M. Krishna},
  title     = {{Reconstructing Vehicles from a Single Image: Shape Priors for Road Scene Understanding}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Object detection, Segmentation, Categorization, Visual Learning, Localization, CNN},
  abstract  = {We present an approach for reconstructing vehicles from a single (RGB) image, in the context of autonomous driving. Though the problem appears to be ill-posed, we demonstrate that prior knowledge about how 3D shapes of vehicles project to an image can be used to reason about the reverse process, i.e., how shapes (back-)project from 2D to 3D. We encode this knowledge in shape priors, which are learnt over a small keypoint-annotated dataset. We then formulate a shapeaware adjustment problem that uses the learnt shape priors to recover the 3D pose and shape of a query object from an image. For shape representation and inference, we leverage recent successes of Convolutional Neural Networks (CNNs) for the task of object and keypoint localization, and train a novel cascaded fully-convolutional architecture to localize vehicle keypoints in images. The shape-aware adjustment then robustly recovers shape (3D locations of the detected keypoints) while simultaneously filling in occluded keypoints. To tackle estimation errors incurred due to erroneously detected keypoints, we use an Iteratively Re-weighted Least Squares (IRLS) scheme for robust optimization, and as a by-product characterize noise models for each predicted keypoint. We evaluate our approach on autonomous driving benchmarks, and present superior results to existing monocular, as well as stereo approaches.}
}

@inproceedings{schneider2017icra,
  author    = {T. Schneider and M. Li and M. Burri and J. Nieto and R. Siegwart and I. Gilitschenski},
  title     = {{Visual-Inertial Self-Calibration on Informative Motion Segments}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Calibration and Identification, Visual-Based Navigation, SLAM},
  abstract  = {Environmental conditions and external effects, such as shocks, have a significant impact on the calibration parameters of visual-inertial sensor systems. Thus longterm operation of these systems cannot fully rely on factory calibration. Since the observability of certain parameters is highly dependent on the motion of the device, using short data segments at device initialization may yield poor results. When such systems are additionally subject to energy constraints, it is also infeasible to use full-batch approaches on a big dataset and careful selection of the data is of high importance. In this paper, we present a novel approach for resource efficient self-calibration of visual-inertial sensor systems. This is achieved by casting the calibration as a segment-based optimization problem that can be run on a small subset of informative segments. Consequently, the computational burden is limited as only a predefined number of segments is used. We also propose an efficient information-theoretic selection to identify such informative motion segments. In evaluations on a challenging dataset, we show our approach to significantly outperform state-of-the-art in terms of computational burden while maintaining a comparable accuracy.}
}

@inproceedings{honegger2017icra,
  author    = {D. Honegger and T. Sattler and M. Pollefeys},
  title     = {{Embedded Real-Time Multi-Baseline Stereo}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Range Sensing, Autonomous Vehicle Navigation, Object detection, Segmentation, Categorization},
  abstract  = {Dense depth map estimation from stereo cameras has many applications in robotic vision, e.g., obstacle detection, especially when performed in real-time. The range in which depth values can be accurately estimated is usually limited for two-camera stereo setups due to the fixed baseline between the cameras. In addition, two-camera setups suffer from wrong depth estimates caused by local minima in the matching cost functions. Both problems can be alleviated by adding more cameras as this creates multiple baselines of different lengths and since multi-image matching leads to unique minima. However, using more cameras usually comes at an increase in run-time. In this paper, we present a novel embedded system for multibaseline stereo. By exploiting the parallelization capabilities within FPGAs, we are able to estimate a depth map from multiple cameras in real-time. We show that our approach requires only little more power and weight compared to a twocamera stereo system. At the same time, we show that our system produces significantly better depth maps and is able to handle occlusion of some cameras, resulting in the redundancy typically desired for autonomous vehicles. Our system is small in size and leight-weight and can be employed even on a MAV platform with very strict power, weight, and size requirements.}
}

@inproceedings{khosravian2017icra,
  author    = {A. Khosravian and T. Chin and I. Reid and R. Mahony},
  title     = {A Discrete-Time Attitude Observer on SO(3) for Vision and GPS Fusion},
  booktitle = icra,
  year      = 2017,
  keywords  = {Localization, Sensor Fusion, Visual-Based Navigation},
  abstract  = {This paper proposes a discrete-time geometric attitude observer for fusing monocular vision with GPS velocity measurements. The observer takes the relative transformations obtained from processing monocular images with any visual odometry algorithm and fuses them with GPS velocity measurements. The objectives of this sensor fusion are twofold; first to mitigate the inherent drift of the attitude estimates of the visual odometry, and second, to estimate the orientation directly with respect to the North-East-Down frame. A key contribution of the paper is to present a rigorous stability analysis showing that the attitude estimates of the observer converge exponentially to the true attitude and to provide a lower bound for the convergence rate of the observer. Through experimental studies, we demonstrate that the observer effectively compensates for the inherent drift of the pure monocular vision based attitude estimation and is able to recover the North-East-Down orientation even if it is initialized with a very large attitude error.}
}

@inproceedings{dube2017icra,
  author    = {R. Dub{\'e} and D. Dugas and E. Stumm and J. Nieto and R. Siegwart and C.C. Lerma},
  title     = {{SegMatch: Segment Based Place Recognition in 3D Point Clouds}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Localization, Object detection, Segmentation, Categorization, SLAM},
  abstract  = {Place recognition in 3D data is a challenging task that has been commonly approached by adapting imagebased solutions. Methods based on local features suffer from ambiguity and from robustness to environment changes while methods based on global features are viewpoint dependent. We propose SegMatch, a reliable place recognition algorithm based on the matching of 3D segments. Segments provide a good compromise between local and global descriptions, incorporating their strengths while reducing their individual drawbacks. SegMatch does not rely on assumptions of perfect segmentation, or on the existence of objects in the environment, which allows for reliable execution on large scale, unstructured environments. We quantitatively demonstrate that SegMatch can achieve accurate localization at a frequency of 1Hz on the largest sequence of the KITTI odometry dataset. We furthermore show how this algorithm can reliably detect and close loops in real-time, during online operation. In addition, the source code for the SegMatch algorithm is made publicly available.}
}

@inproceedings{pfeiffer2017icra,
  author    = {M. Pfeiffer and M. Schaeuble and J. Nieto and R. Siegwart and C.C. Lerma},
  title     = {{From Perception to Decision: A Data-Driven Approach to End-To-End Motion Planning for Autonomous Ground Robots}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Learning and Adaptive Systems, Motion and Path Planning},
  abstract  = {Learning from demonstration for motion planning is an ongoing research topic. In this paper we present a model that is able to learn the complex mapping from raw 2D-laser range findings and a target position to the required steering commands for the robot. To our best knowledge, this work presents the first approach that learns a target-oriented endto-end navigation model for a robotic platform. The supervised model training is based on expert demonstrations generated in simulation with an existing motion planner. We demonstrate that the learned navigation model is directly transferable to previously unseen virtual and, more interestingly, real-world environments. It can safely navigate the robot through obstaclecluttered environments to reach the provided targets. We present an extensive qualitative and quantitative evaluation of the neural network-based motion planner, and compare it to a grid-based global approach, both in simulation and in realworld experiments.}
}

@inproceedings{gomez-ojeda2017icra,
  author    = {R. Gomez-Ojeda and F. Moreno and J. Gonzlez-Jimnez},
  title     = {{Accurate Stereo Visual Odometry with Gamma Distributions}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Autonomous Vehicle Navigation, Localization, Visual-Based Navigation},
  abstract  = {Point-based stereo visual odometry systems typically estimate the camera motion by minimizing a cost function of the projection residuals between consecutive frames. Under some mild assumptions, such minimization is equivalent to maximizing the probability of the measured residuals given a certain pose change, for which a suitable model of the error distribution (sensor model) becomes of capital importance in order to obtain accurate results. This paper proposes a robust probabilistic model for projection errors, based on real world data. For that, we argue that projection distances follow Gamma distributions, and hence, the introduction of these models in a probabilistic formulation of the motion estimation process increases both precision and accuracy. Our approach has been validated through a series of experiments with both synthetic and real data, revealing an improvement in accuracy while not increasing the computational burden.}
}

@inproceedings{merzic2017icra,
  author    = {H. Merzic and E. Stumm and M.T. Dymczyk and R. Siegwart and I. Gilitschenski},
  title     = {{Map Quality Evaluation for Visual Localization}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Localization, Mapping, Visual-Based Navigation},
  abstract  = {A variety of end-user devices involving keypointbased mapping systems are about to hit the market e.g. as part of smartphones, cars, robotic platforms, or virtual and augmented reality applications. Thus, the generated map data requires automated evaluation procedures that do not require experienced personnel or ground truth knowledge of the underlying environment. A particularly important question enabling commercial applications is whether a given map is of sufficient quality for localization. This paper proposes a framework for predicting localization performance in the context of visual landmark-based mapping. Specifically, we propose an algorithm for predicting performance of vision-based localization systems from different poses within the map. To achieve this, a metric is defined that assigns a score to a given query pose based on the underlying map structure. The algorithm is evaluated on two challenging datasets involving indoor data generated using a handheld device and outdoor data from an autonomous fixedwing unmanned aerial vehicle (UAV). Using these, we are able to show that the score provided by our method is highly correlated to the true localization performance. Furthermore, we demonstrate how the predicted map quality can be used within a belief based path planning framework in order to provide reliable trajectories through high-quality areas of the map.}
}

@inproceedings{pathak2017icra,
  author    = {S. Pathak and A. Thomas and V. Indelman},
  title     = {{Nonmyopic Data Association Aware Belief Space Planning for Robust Active Perception}},
  booktitle = icra,
  year      = 2017,
  keywords  = {SLAM, Autonomous Vehicle Navigation, Localization},
  abstract  = {One key assumption of Belief Space Planning (BSP) is that the data association is known perfectly. In this paper, we relax this assumption in the context of non-myopic planning as well as belief being a Gaussian Mixture Model (GMM). Interestingly, explicit reasoning about the data association within the belief enables our framework to have parsimonious data association, thereby resulting in a scalable solution compared with nave permutational approaches. Unlike in some of the recent approaches where the number of components in a GMM belief can only be reduced, in our approach this can also go up such as due to perceptual aliasing present in the environment. Furthermore, our approach naturally integrates with inference, providing a unified framework for robust passive and active perception. We demonstrate key aspects of our approach and its comparison with the state of the art on a general abstract domain as well as in a real robot setup.}
}

@inproceedings{hietanen2017icra,
  author    = {A.E. Hietanen and J. Halme and A.G. Buch and J.M. Latokartano and J. Kamarainen},
  title     = {{Robustifying Correspondence Based 6D Object Pose Estimation}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Computer Vision for Other Robotic Applications, Recognition, RGB-D Perception},
  abstract  = {We propose two methods to robustify point correspondence based 6D object pose estimation. The first method, curvature filtering, is based on the assumption that low curvature regions provide false matches, and removing points in these regions improves robustness. The second method, region pruning, is more general by making no assumptions about local surface properties. Our region pruning segments a model point cloud into cluster regions and searches good region combinations using a validation set. The robustifying methods are general and can be used with any correspondence based method. For the experiments, we evaluated three correspondence selection methods, Geometric Consistency (GC) [1], Hough Grouping (HG) [2] and Search of Inliers (SI) [3] and report systematic improvements for their robustified versions with two distinct datasets.}
}

@inproceedings{fermn-len2017icra,
  author    = {L. Fermn-Len and J. Neira and J.A. Castellanos},
  title     = {{Incremental Contour-Based Topological Segmentation for Robot Exploration}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Mapping, Semantic Scene Understanding, SLAM},
  abstract  = {We propose an alternative to the common approaches to the topological segmentation in structured or unstructured environments, Contour-Based Segmentation. It is faster and equally accurate, without the need of fine tuning parameters or heuristics. During robotic exploration, we propose an incremental version that reduces the processing time by reusing the previous segmentation. Tests demonstrate the velocity and quality in room segmentation in both batch and incremental mode. Tests also demonstrate the incremental version outperforms the state of the art in incremental topological segmentation.}
}

@inproceedings{popovic2017icra,
  author    = {M. Popovic and G. Hitz and J. Nieto and I. Sa and R. Siegwart and E. Galceran},
  title     = {{Online Informative Path Planning for Active Classification Using UAVs}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Motion and Path Planning, Robotics in Agriculture and Forestry, Planning, Scheduling and Coordination},
  abstract  = {In this paper, we introduce an informative path planning (IPP) framework for active classification using unmanned aerial vehicles (UAVs). Our algorithm uses a combination of global viewpoint selection and evolutionary optimization to refine the planned trajectory in continuous 3D space while satisfying dynamic constraints. Our approach is evaluated on the application of weed detection for precision agriculture. We model the presence of weeds on farmland using an occupancy grid and generate adaptive plans according to informationtheoretic objectives, enabling the UAV to gather data efficiently. We validate our approach in simulation by comparing against existing methods, and study the effects of different planning strategies. Our results show that the proposed algorithm builds maps with over 50\% lower entropy compared to traditional lawnmower coverage in the same amount of time. We demonstrate the planning scheme on a multirotor platform with different artificial farmland set-ups.}
}

@inproceedings{schaffernicht2017icra,
  author    = {E. Schaffernicht and V.M.H. Bennetts and A.J. Lilienthal},
  title     = {{Mobile Robots for Learning Spatio-temporal Interpolation Models in Sensor Networks - the Echo State Map Approach}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Sensor Networks, Field Robots},
  abstract  = {Sensor networks have limited capabilities to model complex phenomena occuring between sensing nodes. Mobile robots can be used to close this gap and learn local interpolation models. In this paper, we utilize Echo State Networks in order to learn the calibration and interpolation model between sensor nodes using measurements collected by a mobile robot. The use of Echo State Networks allows to deal with temporal dependencies implicitly, while the spatial mapping with a Gaussian Process estimator exploits the fact that Echo State Networks learn linear combinations of complex temporal dynamics. The resulting Echo State Map elegantly combines spatial and temporal cues into a single representation. We showcase the method in the exposure modeling task of building dust distribution maps for foundries, a challenge which is of great interest to occupational health researchers. Results from simulated data and real world experiments highlight the potential of Echo State Maps. While we focus on particulate matter measurements, the method can be applied for any other environmental variables like temperature or gas concentration.}
}

@inproceedings{withers2017icra,
  author    = {D. Withers and P. Newman},
  title     = {{Modelling Scene Change in Large Scale Long Term Laser Localisation}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Localization, Mapping, Autonomous Vehicle Navigation},
  abstract  = {This paper addresses a difficulty in large-scale long term laser localisation - how to deal with scene change. We pose this as a distraction suppression problem. Urban driving environments are frequently subject to large dynamic outliers, such as buses, trucks etc. These objects can mask the static elements of the prior map that we rely on for localisation. At the same time some objects change shape in a way that is less dramatic but equally pernicious during localisation for example trees over seasons and in wind, shop fronts and doorways. In this paper, we show how we can learn in high resolution, the areas of our map that are subject to such distractions (low value data) in a place-dependent approach. We demonstrate how to utilise this model to select individual laser measurements for localisation. Specifically, by leveraging repeated operation over weeks and months, for each point in our map pointcloud we build distributions of the errors associated with that point for multiple localisation passes. These distributions are then used to determine the legitimacy of laser measurements prior to their use in localisation. We demonstrate distraction suppression as a front-end process to large scale localiser by incrementally adding 50km of error data to our base map and show that robustness is improved over the base system with a further 10km of urban driving.}
}

@inproceedings{mactavish2017icra,
  author    = {K. MacTavish and M. Paton and T. Barfoot},
  title     = {{Visual Triage: A Bag-Of-Words Experience Selector for Long-Term Visual Route Following}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Visual-Based Navigation, Autonomous Vehicle Navigation},
  abstract  = {Our work builds upon Visual Teach & Repeat 2 (VT&R2): a vision-in-the-loop autonomous navigation system that enables the rapid construction of route networks, safely built through operator-controlled driving. Added routes can be followed autonomously using visual localization. To enable long-term operation that is robust to appearance change, its Multi-Experience Localization (MEL) leverages many previously driven experiences when localizing to the manually taught network. While this multi-experience method is effective across appearance change, the computation becomes intractable as the number of experiences grows into the tens and hundreds. This paper introduces an algorithm that prioritizes experiences most relevant to live operation, limiting the number of experiences required for localization. The proposed algorithm uses a visual Bag-of-Words description of the live view to select relevant experiences based on what the vehicle is seeing right now, without having to factor in all possible environmental influences on scene appearance. This system runs in the loop, in real time, does not require bootstrapping, can be applied to any pointfeature MEL paradigm, and eliminates the need for visual training using an online, local visual vocabulary. By picking a subset of visually similar experiences to the live view, we demonstrate safe, vision-in-the-loop route following over a 31 hour period, despite appearance as different as night and day.}
}

@inproceedings{yousif2017icra,
  author    = {K. Yousif and Y. Taguchi and S. Ramalingam},
  title     = {{MonoRGBD-SLAM: Simultaneous Localization and Mapping Using Both Monocular and RGBD Cameras}},
  booktitle = icra,
  year      = 2017,
  keywords  = {SLAM, RGB-D Perception, Sensor Fusion},
  abstract  = {RGBD SLAM systems have shown impressive results, but the limited field of view (FOV) and depth range of typical RGBD cameras still cause problems for registering distant frames. Monocular SLAM systems, in contrast, can exploit wide-angle cameras and do not have the depth range limitation, but are unstable for textureless scenes. We present a SLAM system that uses both an RGBD camera and a wide-angle monocular camera for combining the advantages of the two sensors. Our system extracts 3D point features from RGBD frames and 2D point features from monocular frames, which are used to perform both RGBD-to-RGBD and RGBDto-monocular registration. To compensate for different FOV and resolution of the cameras, we generate multiple virtual images for each wide-angle monocular image and use the feature descriptors computed on the virtual images to perform the RGBD-to-monocular matches. To compute the poses of the frames, we construct a graph where nodes represent RGBD and monocular frames and edges denote the pairwise registration results between the nodes. We compute the global poses of the nodes by first finding the minimum spanning trees (MSTs) of the graph and then pruning edges that have inconsistent poses due to possible mismatches using the MST result. We finally run bundle adjustment on the graph using all the consistent edges. Experimental results show that our system registers a larger number of frames than using only an RGBD camera, leading to larger-scale 3D reconstruction.}
}

@inproceedings{palmieri2017icra,
  author    = {L. Palmieri and T.P. Kucner and M. Magnusson and A.J. Lilienthal and K.O. Arras},
  title     = {{Kinodynamic Motion Planning on Gaussian Mixture Fields}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Nonholonomic Motion Planning, Motion and Path Planning, Reactive and Sensor-Based Planning},
  abstract  = {We present a mobile robot motion planning approach under kinodynamic constraints that exploits learned perception priors in the form of continuous Gaussian mixture fields. Our Gaussian mixture fields are statistical multi-modal motion models of discrete objects or continuous media in the environment that encode e.g. the dynamics of air or pedestrian flows. We approach this task using a recently proposed circular linear flow field map based on semi-wrapped GMMs whose mixture components guide sampling and rewiring in an RRT* algorithm using a steer function for non-holonomic mobile robots. In our experiments with three alternative baselines, we show that this combination allows the planner to very efficiently generate high-quality solutions in terms of path smoothness, path length as well as natural yet minimum control effort motions through multi-modal representations of Gaussian mixture fields.}
}

@inproceedings{teixeira2017icra,
  author    = {L. Teixeira and M. Chli},
  title     = {{Real-Time Local 3D Reconstruction for Aerial Inspection using Superpixel Expansion}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Aerial Robotics, Visual-Based Navigation, Computer Vision for Automation},
  abstract  = {On the quest of automating the navigation of challenging and promising Robotics platforms such as small Unmanned Aerial Vehicles (UAVs), the community has been increasingly active in developing perception capabilities able to run onboard such platforms in real-time. Despite that visionbased techniques have been at the heart of recent advancements, the realistic employment onboard UAVs is still in its infancy. Inspired by some of the most recent breakthroughs in online dense scene estimation and borrowing fundamental concepts from Computer Vision, in this work we propose a new pipeline for real-time, local scene reconstruction using a single camera for aerial navigation. Aiming for denser scene estimation than traditional feature-based maps with the ability to run onboard a small UAV in real-time, the proposed approach is demonstrated to achieve unprecedented performance producing rich maps of the cameras workspace, timely enough to serve in obstacle avoidance and real-time interaction of a robot with its direct surroundings. Evaluation on benchmarking datasets and on challenging aerial footage captured with a UAV featuring a conventional camera, reveals dramatic speed-ups, as well as denser and more accurate local reconstructions with respect to the state of the art.}
}

@inproceedings{pumarola2017icra,
  author    = {A. Pumarola and A. Vakhitov and A. Agudo and A. Sanfeliu and F. Moreno-Noguer},
  title     = {{PL-SLAM: Real-Time Monocular Visual SLAM with Points and Lines}},
  booktitle = icra,
  year      = 2017,
  keywords  = {SLAM, Visual Tracking},
  abstract  = {Low textured scenes are well known to be one of the main Achilles heels of geometric computer vision algorithms relying on point correspondences, and in particular for visual SLAM. Yet, there are many environments in which, despite being low textured, one can still reliably estimate line-based geometric primitives, for instance in city and indoor scenes, or in the so-called Manhattan worlds, where structured edges are predominant. In this paper we propose a solution to handle these situations. Specifically, we build upon ORBSLAM, presumably the current state-of-the-art solution both in terms of accuracy as efficiency, and extend its formulation to simultaneously handle both point and line correspondences. We propose a solution that can even work when most of the points are vanished out from the input images, and, interestingly it can be initialized from solely the detection of line correspondences in three consecutive frames. We thoroughly evaluate our approach and the new initialization strategy on the TUM RGB-D benchmark and demonstrate that the use of lines does not only improve the performance of the original ORB-SLAM solution in poorly textured frames, but also systematically improves it in sequence frames combining points and lines, without compromising the efficiency.}
}

@inproceedings{contreras-toledo2017icra,
  author    = {L.A. Contreras-Toledo and W. Mayol},
  title     = {{O-POCO: Online POint Cloud COmpression Mapping for Visual Odometry and SLAM}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Localization, SLAM, Mapping},
  abstract  = {This paper presents O-POCO, a visual odometry and SLAM system that makes online decisions regarding what to map and what to ignore. It takes a point cloud from classical SfM and aims to sample it on-line by selecting map features useful for future 6D relocalisation. We use the cameras traveled trajectory to compartamentalize the point cloud, along with visual and spatial information to sample and compress the map. We propose and evaluate a number of different information layers such as the descriptor informations relative entropy, map-feature occupancy grid, and the point clouds geometry error. We compare our proposed system against both SfM, and online and offline ORB-SLAM using publicly available datasets in addition to our own. Results show that our online compression strategy is capable of outperforming the baseline even for conditions when the number of features per key-frame used for mapping is four times less.}
}

@inproceedings{jafari2017icra,
  author    = {O.H. Jafari and O. Groth and A. Kirillov and M.Y. Yang and C. Rother},
  title     = {{Analyzing Modular CNN Architectures for Joint Depth Prediction and Semantic Segmentation}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Semantic Scene Understanding, CNN},
  abstract  = {This paper addresses the task of designing a modular neural network architecture that jointly solves different tasks. As an example we use the tasks of depth estimation and semantic segmentation given a single RGB image. The main focus of this work is to analyze the cross-modality influence between depth and semantic prediction maps on their joint refinement. While most of the previous works solely focus on measuring improvements in accuracy, we propose a way to quantify the cross-modality influence. We show that there is a relationship between final accuracy and cross-modality influence, although not a simple linear one. Hence a larger cross-modality influence does not necessarily translate into an improved accuracy. We find that a beneficial balance between the cross-modality influences can be achieved by network architecture and conjecture that this relationship can be utilized to understand different network design choices. Towards this end we propose a Convolutional Neural Network (CNN) architecture that fuses the state-of-the-art results for depth estimation and semantic labeling. By balancing the crossmodality influences between depth and semantic prediction, we achieve improved results for both tasks using the NYU-Depth v2 benchmark.}
}

@inproceedings{agarwal2017icra,
  author    = {S. Agarwal and V. Shree and S. Chakravorty},
  title     = {{RFM-SLAM: Exploiting Relative Feature Measurements to Separate Orientation and Position Estimation in SLAM}},
  booktitle = icra,
  year      = 2017,
  keywords  = {SLAM},
  abstract  = {The SLAM problem is known to have a special property that when robot orientation is known, estimating the history of robot poses and feature locations can be posed as a standard linear least squares problem. In this work, we develop a SLAM framework that uses relative featureto-feature measurements to exploit this structural property of SLAM. Relative feature measurements are used to pose a linear estimation problem for pose-to-pose orientation constraints. This is followed by solving an iterative non-linear on-manifold optimization problem to compute the maximum likelihood estimate for robot orientation given relative rotation constraints. Once the robot orientation is computed, we solve a linear problem for robot position and map estimation. Our approach reduces the computational complexity of non-linear optimization by posing a smaller optimization problem as compared to standard graph-based methods for feature-based SLAM. Further, empirical results show our method avoids catastrophic failures that arise in existing methods due to using odometery as an initial guess for non-linear optimization, while its accuracy degrades gracefully as sensor noise is increased. We demonstrate our method through extensive simulations and comparisons with an existing state-of-the-art solver. Keywords: SLAM, graph-based SLAM, non-linear optimization, relative measurements}
}

@inproceedings{alzugaray2017icra,
  author    = {I. Alzugaray and L. Teixeira and M. Chli},
  title     = {{Short-Term UAV Path-Planning with Monocular-Inertial SLAM in the Loop}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Aerial Robotics, Visual-Based Navigation, SLAM},
  abstract  = {Small Unmanned Aerial Vehicles (UAVs) are some of the most promising robotic platforms in a variety of applications due to their high mobility. Their restricted computational and payload capabilities, however, translate into significant challenges in automating their navigation. With Simultaneous Localization And Mapping (SLAM) systems recently demonstrated to be employable onboard UAVs, the focus fall on path-planning on the quest of achieving autonomous navigation. With the vast body of path-planning literature often assuming perfect maps or maps known a priori, the biggest challenge lies in dealing with the robustness and accuracy limitations of onboard SLAM in real missions. In this spirit, this paper proposes a path-planning algorithm designed to work in the loop of the SLAM estimation of a monocular-inertial system. This point-to-point planner is demonstrated to navigate in an unknown environment using the incrementally generated SLAM map, while dictating the navigation strategy for preferable acquisition of sensor data for better estimations within SLAM. A thorough evaluation testbed of both simulated and real data is presented, demonstrating the robustness of the proposed pipeline against the state-of-the-art and its dramatically lower computational complexity, revealing its suitability to UAV navigation. Video: https://youtu.be/Izn_vVb_M-E}
}

@inproceedings{schwarting2017icra,
  author    = {W. Schwarting and J. Alonso-Mora and L. Paull and S. Karaman and D. Rus},
  title     = {{Parallel Autonomy in Automated Vehicles: Safe Motion Generation with Minimal Intervention}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Collision Avoidance, Motion and Path Planning, Autonomous Vehicle Navigation},
  abstract  = {Current state-of-the-art vehicle safety systems, such as assistive braking or automatic lane following, are still only able to help in relatively simple driving situations. We introduce a Parallel Autonomy shared-control framework that produces safe trajectories based on human inputs even in much more complex driving scenarios, such as those commonly encountered in an urban setting. We minimize the deviation from the human inputs while ensuring safety via a set of collision avoidance constraints. We develop a receding horizon planner formulated as a Non-linear Model Predictive Control (NMPC) including analytic descriptions of road boundaries, and the configurations and future uncertainties of other traffic participants, and directly supplying them to the optimizer without linearization. The NMPC operates over both steering and acceleration simultaneously. Furthermore, the proposed receding horizon planner also applies to fully autonomous vehicles. We validate the proposed approach through simulations in a wide variety of complex driving scenarios such as leftturns across traffic, passing on busy streets, and under dynamic constraints in sharp turns on a race track.}
}

@inproceedings{massiceti2017icra,
  author    = {D. Massiceti and A. Krull and E. Brachmann and C. Rother and P. Torr},
  title     = {Random Forests versus Neural Networks - What's Best for Camera Localization?},
  booktitle = icra,
  year      = 2017,
  keywords  = {Localization, Visual-Based Navigation, Computer Vision for Other Robotic Applications},
  abstract  = {This work addresses the task of camera localization in a known 3D scene given a single input RGB image. State-of-the-art approaches accomplish this in two steps: firstly, regressing for every pixel in the image its 3D scene coordinate and subsequently, using these coordinates to estimate the final 6D camera pose via RANSAC. To solve the first step, Random Forests (RFs) are typically used. On the other hand, Neural Networks (NNs) reign in many dense regression tasks, but are not test-time efficient. We ask the question: which of the two is best for camera localization? To address this, we make two method contributions: (1) a test-time efficient NN architecture which we term a ForestNet that is derived and initialized from a RF, and (2) a new fully-differentiable robust averaging technique for regression ensembles which can be trained endto-end with a NN. Our experimental findings show that for scene coordinate regression, traditional NN architectures are superior to test-time efficient RFs and ForestNets, however, this does not translate to final 6D camera pose accuracy where RFs and ForestNets perform slightly better. To summarize, our best method, a ForestNet with a robust average, which has an equivalent fast and lightweight RF, improves over the stateof-the-art for camera localization on the 7-Scenes dataset [1]. While this work focuses on scene coordinate regression for camera localization, our innovations may also be applied to other continuous regression tasks.}
}

@inproceedings{scheel2017icra,
  author    = {A. Scheel and S. Reuter and K. Dietmayer},
  title     = {{Vehicle Tracking Using Extended Object Methods: An Approach for Fusing Radar and Laser}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Intelligent Transportation Systems, Sensor Fusion, Range Sensing},
  abstract  = {Combining data from heterogeneous sensors allows to enhance tracking systems by increasing the field of view, incorporating redundancy, and improving the performance by exploiting complementary sensor characteristics. This paper proposes a new vehicle tracking approach for vehicle environment perception that fuses radar and laser data. A Random-Finite-Set-based tracking filter, which permits a clear mathematical formulation of the multi-object problem, is used as fusion center. In combination with extended object measurement models that work on the raw sensor data directly, the filter uses all available information without the need for further preprocessing routines, considers object interdependencies, and works in ambiguous situations. The results are evaluated using experimental data from a test vehicle.}
}

@inproceedings{park2017icra,
  author    = {S. Park and T. Sch{\"o}ps and M. Pollefeys},
  title     = {{Illumination Change Robustness in Direct Visual SLAM}},
  booktitle = icra,
  year      = 2017,
  keywords  = {SLAM, Visual Tracking, RGB-D Perception},
  abstract  = {Direct visual odometry and Simultaneous Localization and Mapping (SLAM) methods determine camera poses by means of direct image alignment. This optimizes a photometric cost term based on the Lucas-Kanade method. Many recent works use the brightness constancy assumption in the alignment cost formulation and therefore cannot cope with significant illumination changes. Such changes are especially likely to occur for loop closures in SLAM. Alternatives exist which attempt to match images more robustly. In our paper, we perform a systematic evaluation of real-time capable methods. We determine their accuracy and robustness in the context of odometry and of loop closures, both on real images as well as synthetic datasets with simulated lighting changes. We find that for real images, a Census-based method outperforms the others. We make our new datasets available online3 .}
}

@inproceedings{lukierski2017icra,
  author    = {R. Lukierski and S. Leutenegger and A.J. Davison},
  title     = {{Room Layout Estimation from Rapid Omnidirectional Exploration}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Omnidirectional Vision, SLAM, Domestic Robots},
  abstract  = {A new generation of practical, low-cost indoor robots is now using wide-angle cameras to aid navigation, but usually this is limited to position estimation via sparse featurebased SLAM. Such robots usually have little global sense of the dimensions, demarcation or identities of the rooms they are in, information which would be very useful to enable behaviour with much more high level intelligence. In this paper we show that we can augment an omnidirectional SLAM pipeline with straightforward dense stereo estimation and simple and robust room model fitting to obtain rapid and reliable estimation of the global shape of typical rooms from short robot motions. We have tested our method extensively in real homes, offices and on synthetic data. We also give examples of how our method can extend to making composite maps of larger rooms, and detecting room transitions.}
}

@inproceedings{jaimez2017icra,
  author    = {M. Jaimez and C. Kerl and J. Gonzalez and D. Cremers},
  title     = {{Fast Odometry and Scene Flow from RGB-D Cameras Based on Geometric Clustering}},
  booktitle = icra,
  year      = 2017,
  keywords  = {RGB-D Perception, Visual Tracking},
  abstract  = {In this paper we propose an efficient solution to jointly estimate the camera motion and a piecewise-rigid scene flow from an RGB-D sequence. The key idea is to perform a two-fold segmentation of the scene, dividing it into geometric clusters that are, in turn, classified as static or moving elements. Representing the dynamic scene as a set of rigid clusters drastically accelerates the motion estimation, while segmenting it into static and dynamic parts allows us to separate the camera motion (odometry) from the rest of motions observed in the scene. The resulting method robustly and accurately determines the motion of an RGB-D camera in dynamic environments with an average runtime of 80 milliseconds on a multi-core CPU. The code is available for public use/test.}
}

@inproceedings{schenck2017icra,
  author    = {C. Schenck and D. Fox},
  title     = {{Visual Closed-Loop Control for Pouring Liquids}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Learning and Adaptive Systems, Domestic Robots},
  abstract  = {Pouring a specific amount of liquid is a challenging task. In this paper we develop methods for robots to use visual feedback to perform closed-loop control for pouring liquids. We propose both a model-based and a model-free method utilizing deep learning for estimating the volume of liquid in a container. Our results show that the model-free method is better able to estimate the volume. We combine this with a simple PID controller to pour specific amounts of liquid, and show that the robot is able to achieve an average 38ml deviation from the target amount. To our knowledge, this is the first use of raw visual feedback to pour liquids in robotics.}
}

@inproceedings{siam2017icra,
  author    = {S.M. Siam and H. Zhang},
  title     = {{Fast-SeqSLAM: A Fast Appearance Based Place Recognition Algorithm}},
  booktitle = icra,
  year      = 2017,
  keywords  = {SLAM, Localization, Mapping},
  abstract  = {Loop closure detection or place recognition is a fundamental problem in robot simultaneous localization and mapping (SLAM). SeqSLAM is considered to be one of the most successful algorithms for loop closure detection as it has been demonstrated to be able to handle significant environmental condition changes including those due to illumination, weather, and time of the day. However, SeqSLAM relies heavily on exhaustive sequence matching, a computationally expensive process that prevents the algorithm from being used in dealing with large maps. In this paper, we propose Fast-SeqSLAM, an efficient version of SeqSLAM. Fast-SeqSLAM has a much reduced time complexity without degrading the accuracy, and this is achieved by using an approximate nearest neighbor (ANN) algorithm to match the current image with those in the robot map and extending the idea of SeqSLAM to greedily search a sequence of images that best match with the current sequence. We demonstrate the effectiveness of our Fast-SeqSLAM algorithm in appearance based loop closure detection.}
}

@inproceedings{meier2017icra,
  author    = {L. Meier and D. Honegger and V. Vilhjalmsson and M. Pollefeys},
  title     = {{Real-Time Stereo Matching Failure Prediction and Resolution Using Orthogonal Stereo Setups}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Range Sensing, Computer Vision for Automation, Failure Detection and Recovery},
  abstract  = {Estimating the depth from two images with a baseline has a well-known regular problem: When a line is parallel to the epipolar geometry it is not possible to estimate the depth from pixels on this line. Moreover, the classic measure for the certainty of the depth estimate fails as well: The matching score between the template and any pixel on the epipolar line is perfect. This results for common scenes in incorrect matches with very high confidence, some even resistant to leftright image checks. It is straightforward to try to address this by adding a second stereo head in a perpendicular direction. However, it is nontrivial to identify the failure and fuse the two depth maps in a real-time system. A simple weighted average will alleviate the problem but still result in a very large error in the depth map. Our contributions are: 1) We derive a model to predict the failure of stereo by leveraging the matching scores and 2) we propose a combined cost function to fuse two depth maps from orthogonal stereo heads using the failure prediction, matching score and consistency. We show the resulting system in real-time operation on a low-latency system in indoor, urban and natural environments.}
}

@inproceedings{alahi2017icra,
  author    = {A. Alahi and J. Wilson and L. Fei-Fei and S. Savarese},
  title     = {{Unsupervised Camera Localization in Crowded Spaces}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Sensor Networks, Surveillance Systems, Human detection and tracking},
  abstract  = {Existing camera networks in public spaces such as train terminals or malls can help social robots to navigate crowded scenes. However, the localization of the cameras is required, i.e., the positions and poses of all cameras in a unique reference. In this work, we estimate the relative location of any pair of cameras by solely using noisy trajectories observed from each camera. We propose a fully unsupervised learning technique using unlabelled pedestrians motion patterns captured in crowded scenes. We first estimate the pairwise camera parameters by optimally matching single-view pedestrian tracks using social awareness. Then, we show the impact of jointly estimating the network parameters. This is done by formulating a nonlinear least square optimization problem, leveraging a continuous approximation of the matching function. We evaluate our approach in real-world environments such as train terminals, where several hundreds of individuals need to be tracked across dozens of cameras every second.}
}

@inproceedings{kuhlman2017icra,
  author    = {M.J. Kuhlman and M.W. Otte and D. Sofge and S.K. Gupta},
  title     = {{Maximizing Mutual Information for Multipass Target Search in Changing Environments}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Motion and Path Planning, Search and Rescue Robots},
  abstract  = {Motion planning for multi-target autonomous search requires efciently gathering as much information over an area as possible with an imperfect sensor. In disaster scenarios and contested environments the spatial connectivity may unexpectedly change (due to aftershock, avalanche, ood, building collapse, adversary movements, etc.) and the ight envelope may evolve as a known function of time to ensure rescue worker safety or to facilitate other mission goals. Algorithms designed to handle both expected and unexpected changes must: (1) reason over a sufciently long time horizon to respect expected changes, and (2) replan quickly in response to unexpected changes. These ambitions are hindered by the submodularity property of mutual information, which makes optimal solutions NP-hard to compute. We present an algorithm for autonomous search in changing environments that uses a variety of techniques to improve both the speed and time horizon, including using admissible heuristics to speed up the search.}
}

@inproceedings{ballardini2017icra,
  author    = {A.L. Ballardini and D. Cattaneo and S. Fontana and D.G. Sorrenti},
  title     = {{An Online Probabilistic Road Intersection Detector}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Computer Vision for Transportation, Intelligent Transportation Systems, Localization},
  abstract  = {In this paper we propose a probabilistic approach for detecting and classifying urban road intersections from a moving vehicle. The approach is based on images from an onboard stereo rig; it relies on the detection of the road ground plane on one side, and on a pixel-level classification of the road on the other. The two processing pipelines are then integrated and the parameters of the road components, i.e., the intersection geometry, are inferred. As opposed to other state-of-the-art offline methods, which require processing of the whole video sequence, our approach integrates the image data by means of an online procedure. The experiments have been performed on well-known KITTI datasets, allowing for future comparisons.}
}

@inproceedings{gregorio2017icra,
  author    = {D.D. Gregorio and L.D. Stefano},
  title     = {{SkiMap: An Efficient Mapping Framework for Robot Navigation}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Mapping, SLAM, RGB-D Perception},
  abstract  = {We present a novel mapping framework for robot navigation which features a multi-level querying system capable to obtain rapidly representations as diverse as a 3D voxel grid, a 2.5D height map and a 2D occupancy grid. These are inherently embedded into a memory and time efficient core data structure organized as a Tree of SkipLists. Compared to the wellknown Octree representation, our approach exhibits a better time efficiency, thanks to its simple and highly parallelizable computational structure, and a similar memory footprint when mapping large workspaces. Peculiarly within the realm of mapping for robot navigation, our framework supports realtime erosion and re-integration of measurements upon reception of optimized poses from the sensor tracker, so as to improve continuously the accuracy of the map.}
}

@inproceedings{fridovich-keil2017icra,
  author    = {D. Fridovich-Keil and E. Nelson and A. Zakhor},
  title     = {{AtomMap: A Probabilistic Amorphous 3D Map Representation for Robotics and Surface Reconstruction}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Mapping, Autonomous Vehicle Navigation, SLAM},
  abstract  = {We present a new 3D probabilistic occupancy map representation for robotics applications by relaxing the commonly-assumed constraint that space must be perfectly tessellated. We replace the regular structure of 3D grids with an unstructured collection of non-overlapping, equally-sized spheres, which we call atoms. Abandoning the grid structure allows a more accurate representation of space directly tangent to surfaces, which facilitates a number of applications such as high fidelity surface reconstruction and surface-guided path planning. Maps composed of atoms can distinguish between free, occupied, and unknown space, support computationally efficient insertions and collision queries, provide free space planning guarantees, and achieve state-of-the-art memory efficiency over large volumes. This is achieved while simultaneously reducing quantization effects in the vicinity of surfaces and defining a useful implicit surface representation.}
}

@inproceedings{khanna2017icra,
  author    = {R. Khanna and I. Sa and J. Nieto and R. Siegwart},
  title     = {{On Field Radiometric Calibration for Multispectral Cameras}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Calibration and Identification, Aerial Robotics, Range Sensing},
  abstract  = {Perception systems for outdoor robotics have to deal with varying environmental conditions. Variations in illumination in particular, are currently the biggest challenge for vision-based perception. In this paper we present an approach for radiometric characterisation of multispectral cameras. To enable spatio-temporal mapping we also present a procedure for in-situ illumination estimation, resulting in radiometric calibration of the collected images. In contrast to current approaches, we present a purely data driven, parameter free approach, based on maximum likelihood estimation which can be performed entirely on the field, without requiring specialised laboratory equipment. Our routine requires three simple datasets which are easily acquired using most modern multispectral cameras. We evaluate the framework with a cost-effective snapshot multispectral camera. The results show that our method enables the creation of quatitatively accurate relative reflectance images with challenging on field calibration datasets under a variety of ambient conditions.}
}

@inproceedings{saxena2017icra,
  author    = {D.M. Saxena and V. Kurtz and M. Hebert},
  title     = {{Learning Robust Failure Response for Autonomous Vision Based Flight}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Failure Detection and Recovery, Visual-Based Navigation, Aerial Robotics},
  abstract  = {The ability of autonomous mobile robots to react to and recover from potential failures of on-board systems is an important area of ongoing robotics research. With increasing emphasis on robust systems and long-term autonomy, mobile robots must be able to respond safely and intelligently to dangerous situations. Recent developments in computer vision have made autonomous vision based navigation possible. However, vision systems are known to be imperfect and prone to failure due to variable lighting, terrain changes, and other environmental variables. We describe a system for learning simple failure recovery maneuvers based on experience. This involves both recognizing when the vision system is prone to failure, and associating failures with appropriate responses that will most likely help the robot recover. We implement this system on an autonomous quadrotor and demonstrate that behaviors learned with our system are effective in recovering from situational perception failure, thereby improving reliability in cluttered and uncertain environments.}
}

@inproceedings{morere2017icra,
  author    = {P. Morere and R. Marchant and F. Ramos},
  title     = {{Sequential Bayesian Optimisation as a POMDP for Environment Monitoring with UAVs}},
  booktitle = icra,
  year      = 2017,
  keywords  = {AI Reasoning Methods, Learning and Adaptive Systems, Motion and Path Planning},
  abstract  = {Bayesian Optimisation has gained much popularity lately, as a global optimisation technique for functions that are expensive to evaluate or unknown a priori. While classical BO focuses on where to gather an observation next, it does not take into account practical constraints for a robotic system such as where it is physically possible to gather samples from, nor the sequential nature of the problem while executing a trajectory. In field robotics and other real-life situations, physical and trajectory constraints are inherent problems. This paper addresses these issues by formulating Bayesian Optimisation for continuous trajectories within a Partially Observable Markov Decision Process (POMDP) framework. The resulting POMDP is solved using Monte-Carlo Tree Search (MCTS), which we adapt to using a reward function balancing exploration and exploitation. Experiments on monitoring a spatial phenomenon with a UAV illustrate how our BO-POMDP algorithm outperforms competing techniques.}
}

@inproceedings{mueller-sim2017icra,
  author    = {T. Mueller-Sim and M. Jenkins and J. Abel and G. Kantor},
  title     = {{The Robotanist: A Ground-Based Agricultural Robot for High-Throughput Crop Phenotyping}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Robotics in Agriculture and Forestry, Agricultural Automation, Field Robots},
  abstract  = {The established processes for measuring physiological and morphological traits (phenotypes) of crops in outdoor test plots are labor intensive and error-prone. Low-cost, reliable, field-based robotic phenotyping will enable geneticists to more easily map genotypes to phenotypes, which in turn will improve crop yields. In this paper, we present a novel robotic ground-based platform capable of autonomously navigating below the canopy of row crops such as sorghum or corn. The robot is also capable of deploying a manipulator to measure plant stalk strength and gathering phenotypic data with a modular array of non-contact sensors. We present data obtained from deployments to Sorghum bicolor test plots at various sites in South Carolina, USA.}
}

@inproceedings{platinsky2017icra,
  author    = {L. Platinsky and A.J. Davison and S. Leutenegger},
  title     = {{Monocular Visual Odometry: Sparse Joint Optimisation or Dense Alternation?}},
  booktitle = icra,
  year      = 2017,
  keywords  = {SLAM, Visual-Based Navigation},
  abstract  = {Real-time monocular SLAM is increasingly mature and entering commercial products. However, there is a divide between two techniques providing similar performance. Despite the rise of dense and semi-dense methods which use large proportions of the pixels in a video stream to estimate motion and structure via alternating estimation, they have not eradicated feature-based methods which use a significantly smaller amount of image information from keypoints and retain a more rigorous joint estimation framework. Dense methods provide more complete scene information, but in this paper we focus on how the amount of information and different optimisation methods affect the accuracy of local motion estimation (monocular visual odometry). This topic becomes particularly relevant after the recent results from a direct sparse system. We propose a new method for fairly comparing the accuracy of SLAM frontends in a common setting. We suggest computational cost models for an overall comparison which indicates that there is relative parity between the approaches at the settings allowed by current serial processors when evaluated under equal conditions.}
}

@inproceedings{osep2017icra,
  author    = {A. Osep and W. Mehner and M. Mathias and B. Leibe},
  title     = {{Combined Image and World-Space Tracking in Traffic Scenes}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Visual Tracking, Computer Vision for Automation, Computer Vision for Transportation},
  abstract  = {Tracking in urban street scenes plays a central role in autonomous systems such as self-driving cars. Most of the current vision-based tracking methods perform tracking in the image domain. Other approaches, e.g. based on LIDAR and radar, track purely in 3D. While some vision-based tracking methods invoke 3D information in parts of their pipeline, and some 3D-based methods utilize image-based information in components of their approach, we propose to use image- and world-space information jointly throughout our method. We present our tracking pipeline as a 3D extension of image-based tracking. From enhancing the detections with 3D measurements to the reported positions of every tracked object, we use worldspace 3D information at every stage of processing. We accomplish this by our novel coupled 2D-3D Kalman filter, combined with a conceptually clean and extendable hypothesize-andselect framework. Our approach matches the current stateof-the-art on the official KITTI benchmark, which performs evaluation in the 2D image domain only. Further experiments show significant improvements in 3D localization precision by enabling our coupled 2D-3D tracking.}
}

@inproceedings{raposo2017icra,
  author    = {C. Raposo and J.P. Barreto},
  title     = {Using 2 Point+Normal Sets for Fast Registration of Point Clouds with Small Overlap},
  booktitle = icra,
  year      = 2017,
  keywords  = {Computational Geometry},
  abstract  = {Global 3D point cloud registration has been solved by finding putative matches between the point clouds for establishing alignment hypotheses. A naive approach would try to perform exhaustive search of triplets with a cubic runtime complexity in the number of data points. Super4PCS reduces this complexity to linear by making use of sets of 4 coplanar points. This paper proposes 2-Point-Normal Sets (2PNS), a new global 3D registration approach that advances Super4PCS by using 2 points and their normals for generating alignment hypotheses. The dramatic improvement in the complexity of 2PNS when compared to Super4PCS is demonstrated by the experiments that show speed-ups of two orders of magnitude in noise-free datasets and up to 5.2 in Kinect scans, while improving robustness and alignment accuracy, even in datasets with overlaps as low as 5\%.}
}

@inproceedings{doherty2017icra,
  author    = {K. Doherty and J. Wang and B. Englot},
  title     = {{Bayesian Generalized Kernel Inference for Occupancy Map Prediction}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Mapping, Range Sensing, Learning and Adaptive Systems},
  abstract  = {We consider the problem of building accurate and descriptive 3D occupancy maps of an environment from sparse and noisy range sensor data. We seek to accomplish this task by constructing a predictive model online and inferring the occupancy probability of regions we have not directly observed. We propose a novel algorithm leveraging recent advances in data structures for mapping, sparse kernels, and Bayesian nonparametric inference. The resulting inference model has several desirable properties in comparison to existing methods, including speed of computation, the ability to be recursively updated without approximation, and consistency between batch and online inference. The method also reverts to the use of a specified prior state when insufficient relevant training data exist to predict the occupancy probability of a query point, a property which is attractive for motion planning and exploration applications with mobile robots.}
}

@inproceedings{williams2017icra,
  author    = {G. Williams and N. Wagener and B. Goldfain and P. Drews and J. Rehg and B. Boots and E. Theodorou},
  title     = {{Information Theoretic MPC for Model-Based Reinforcement Learning}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Learning and Adaptive Systems, Optimization and Optimal Control, Autonomous Vehicle Navigation},
  abstract  = {We introduce an information theoretic model predictive control (MPC) algorithm capable of handling complex cost criteria and general nonlinear dynamics. The generality of the approach makes it possible to use multi-layer neural networks as dynamics models, which we incorporate into our MPC algorithm in order to solve model-based reinforcement learning tasks. We test the algorithm in simulation on a cartpole swing up and quadrotor navigation task, as well as on actual hardware in an aggressive driving task. Empirical results demonstrate that the algorithm is capable of achieving a high level of performance and does so only utilizing data collected from the system. Fig. 1. Aggressive driving with model predictive path integral control and neural network dynamics.}
}

@inproceedings{garimella2017icra,
  author    = {G. Garimella and M. Sheckells and M. Kobilarov},
  title     = {{Robust Obstacle Avoidance for Aerial Platforms Using Adaptive Model Predictive Control}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Robust/Adaptive Control of Robotic Systems, Aerial Robotics, Learning and Adaptive Systems},
  abstract  = {This work addresses the problem of motion planning among obstacles for quadrotor platforms under external disturbances and with model uncertainty. A novel Nonlinear Model Predictive Control (NMPC) optimization technique is proposed which incorporates specified uncertainties into the planned trajectories. At the core of the procedure lies the propagation of model parameter uncertainty and initial state uncertainty as high-confidence ellipsoids in pose space. The quadrotor trajectories are then computed to avoid obstacles by a required safety margin, expressed as ellipsoid penetration while minimizing control effort and achieving a user-specified goal location. Combining this technique with online model identification results in robust obstacle avoidance behavior. Experiments in outdoor scenarios with virtual obstacles show that the quadrotor can avoid obstacles robustly, even under the influence of external disturbances.}
}

@inproceedings{bowman2017icra,
  author    = {S. Bowman and N. Atanasov and K. Daniilidis and G.J. Pappas},
  title     = {{Probabilistic Data Association for Semantic SLAM}},
  booktitle = icra,
  year      = 2017,
  keywords  = {SLAM, Localization, Recognition},
  abstract  = {Traditional approaches to simultaneous localization and mapping (SLAM) rely on low-level geometric features such as points, lines, and planes. They are unable to assign semantic labels to landmarks observed in the environment. Furthermore, loop closure recognition based on low-level features is often viewpoint-dependent and subject to failure in ambiguous or repetitive environments. On the other hand, object recognition methods can infer landmark classes and scales, resulting in a small set of easily recognizable landmarks, ideal for view-independent unambiguous loop closure. In a map with several objects of the same class, however, a crucial data association problem exists. While data association and recognition are discrete problems usually solved using discrete inference, classical SLAM is a continuous optimization over metric information. In this paper, we formulate an optimization problem over sensor states and semantic landmark positions that integrates metric information, semantic information, and data associations, and decompose it into two interconnected problems: an estimation of discrete data association and landmark class probabilities, and a continuous optimization over the metric states. The estimated landmark and robot poses affect the association and class distributions, which in turn affect the robot-landmark pose optimization. The performance of our algorithm is demonstrated on indoor and outdoor datasets.}
}

@inproceedings{mccormac2017icra,
  author    = {J. McCormac and A. Handa and A.J. Davison and S. Leutenegger},
  title     = {{SemanticFusion: Dense 3D Semantic Mapping with Convolutional Neural Networks}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Semantic Scene Understanding, SLAM, Object detection, Segmentation, Categorization, CNN},
  abstract  = {Ever more robust, accurate and detailed mapping using visual sensing has proven to be an enabling factor for mobile robots across a wide variety of applications. For the next level of robot intelligence and intuitive user interaction, maps need to extend beyond geometry and appearance they need to contain semantics. We address this challenge by combining Convolutional Neural Networks (CNNs) and a state-of-the-art dense Simultaneous Localisation and Mapping (SLAM) system, ElasticFusion, which provides long-term dense correspondences between frames of indoor RGB-D video even during loopy scanning trajectories. These correspondences allow the CNNs semantic predictions from multiple view points to be probabilistically fused into a map. This not only produces a useful semantic 3D map, but we also show on the NYUv2 dataset that fusing multiple predictions leads to an improvement even in the 2D semantic labelling over baseline single frame predictions. We also show that for a smaller reconstruction dataset with larger variation in prediction viewpoint, the improvement over single frame segmentation increases. Our system is efficient enough to allow real-time interactive use at frame-rates of 25Hz.}
}

@inproceedings{zhou2017icra,
  author    = {Y. Zhou and K. Hauser},
  title     = {{Incorporating Side-Channel Information into Convolutional Neural Networks for Robotic Tasks}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Learning and Adaptive Systems, Collision Avoidance, Robust/Adaptive Control of Robotic Systems, CNN},
  abstract  = {Convolutional neural networks (CNN) are a deep learning technique that has achieved state-of-the-art prediction performance in computer vision and robotics, but assume the input data can be formatted as an image or video (e.g. predicting a robot grasping location given RGB-D image input). This paper considers the problem of augmenting a traditional CNN for handling image-like input (called main-channel input) with additional, highly predictive, non-image-like input (called side-channel input). An example of such a task would be to predict whether a robot path is collision-free given an occupancy grid of the environment and the paths start and goal configurations; the occupancy grid is the main-channel and the start and goal are the side-channel. This paper presents several candidate network architectures for doing so. Empirical tests on robot collision prediction and control problems compare the the proposed architectures in terms of learning speed, memory usage, learning capacity, and susceptibility to overfitting.}
}

@inproceedings{briales2017icra,
  author    = {J. Briales and J. Gonzalez},
  title     = {{Initialization of 3D Pose Graph Optimization Using Lagrangian Duality}},
  booktitle = icra,
  year      = 2017,
  keywords  = {SLAM, Optimization and Optimal Control, Localization},
  abstract  = {Pose Graph Optimization (PGO) is the de facto choice to solve the trajectory of an agent in Simultaneous Localization and Mapping (SLAM). The Maximum Likelihood Estimation (MLE) for PGO is a non-convex problem for which no known technique is able to guarantee a globally optimal solution under general conditions. In recent years, Lagrangian duality has proved suitable to provide good, frequently tight relaxations of the hard PGO problem through convex Semidefinite Programming (SDP). In this work, we build from the state-ofthe-art Lagrangian relaxation [1] and contribute a complete recovery procedure that, given the (tractable) optimal solution of the relaxation, provides either the optimal MLE solution if the relaxation is tight, or a remarkably good feasible guess if the relaxation is non-tight, which occurs in specially challenging PGO problems (very noisy observations, low graph connectivity, etc.). In the latter case, when used for initialization of local iterative methods, our approach outperforms other state-ofthe-art approaches converging to better solutions. We support our claims with extensive experiments.}
}

@inproceedings{zhen2017icra,
  author    = {W. Zhen and S. Zeng and S. Scherer},
  title     = {{Robust Localization and Localizability Estimation with a Rotating Laser Scanner}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Localization, Aerial Robotics, Range Sensing},
  abstract  = {This paper presents a robust localization approach that fuses measurements from inertial measurement unit (IMU) and a rotating laser scanner. An Error State Kalman Filter (ESKF) is used for sensor fusion and is combined with a Gaussian Particle Filter (GPF) for measurements update. We experimentally demonstrated the robustness of this implementation in various challenging situations such as kidnapped robot situation, laser range reduction and various environment scales and characteristics. Additionally, we propose a new method to evaluate localizability of a given 3D map and show that the computed localizability can precisely predict localization errors, thus helps to find safe routes during flight.}
}

@inproceedings{lee2017icra,
  author    = {S. Lee and S. Seo},
  title     = {{A Learning-Based Framework for Handling Dilemmas in Urban Automated Driving}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Autonomous Vehicle Navigation, Motion and Path Planning, Intelligent Transportation Systems},
  abstract  = {Over the last decade, automated vehicles have been widely researched and their massive potential has been verified through several milestone demonstrations. However, there are still many challenges ahead. One of the biggest challenges is integrating them into urban environments in which dilemmas occur frequently. Conventional automated driving strategies make automated vehicles foolish in dilemmas such as making lane-changes in heavy traffic, handling a yellow traffic light, and crossing a double-yellow line to pass an illegally parked car. In this paper, we introduce a novel automated driving strategy that allows automated vehicles to tackle these dilemmas. The key insight behind our automated driving strategy is that expert drivers understand human interactions on the road and comply with mutually-accepted rules, which are learned from countless experiences. In order to teach the driving strategy of expert drivers to automated vehicles, we propose a general learning framework based on maximum entropy inverse reinforcement learning and Gaussian process. Experiments are conducted on a 5.2 km-long campus road at Seoul National University and demonstrate that our framework performs comparably to expert drivers in planning trajectories to handle various dilemmas.}
}

@inproceedings{papachristos2017icra,
  author    = {C. Papachristos and S. Khattak and K. Alexis},
  title     = {{Uncertainty-aware Receding Horizon Exploration and Mapping Using Aerial Robots}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Aerial Robotics},
  abstract  = {This paper presents a novel path planning algorithm for autonomous, uncertaintyaware exploration and mapping of unknown environments using aerial robots. The proposed planner follows a twostep, receding horizon, belief spacebased approach. At first, in an online computed tree the algorithm finds the branch that optimizes the amount of space expected to be explored. The first viewpoint configuration of this branch is selected, but the path towards it is decided through a second planning step. Within that, a new tree is sampled, admissible branches arriving at the reference viewpoint are found and the robot belief about its state and the tracked landmarks of the environment is propagated. The branch that minimizes the expected localization and mapping uncertainty is selected, the corresponding path is executed by the robot and the whole process is iteratively repeated. The proposed planner is capable of running online onboard a small aerial robot and its performance is evaluated using experimental studies in a challenging environment.}
}

@inproceedings{kim2017icra-silv,
  author    = {D. Kim and M. Walter},
  title     = {{Satellite Image-Based Localization Via Learned Embeddings}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Visual-Based Navigation, Localization},
  abstract  = {We propose a vision-based method that localizes a ground vehicle using publicly available satellite imagery as the only prior knowledge of the environment. Our approach takes as input a sequence of ground-level images acquired by the vehicle as it navigates, and outputs an estimate of the vehicles pose relative to a georeferenced satellite image. We overcome the significant viewpoint and appearance variations between the images through a neural multi-view model that learns locationdiscriminative embeddings in which ground-level images are matched with their corresponding satellite view of the scene. We use this learned function as an observation model in a filtering framework to maintain a distribution over the vehicles pose. We evaluate our method on different benchmark datasets and demonstrate its ability localize ground-level images in environments novel relative to training, despite the challenges of significant viewpoint and appearance variations.}
}

@inproceedings{arora2017icra,
  author    = {S. Arora and S. Scherer},
  title     = {{Randomized Algorithm for Informative Path Planning with Budget Constraints}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Motion and Path Planning, Aerial Robotics, Surveillance Systems},
  abstract  = {Maximizing information gathered within a budget is a relevant problem for information gathering tasks for robots with cost or operating time constraints. This problem is also known as the informative path planning (IPP) problem or correlated orienteering. It can be formalized as that of finding budgeted routes in a graph such that the reward collected by the route is maximized, where the reward at nodes can be dependent. Unfortunately, the problem is NP-Hard and the state of the art methods are too slow to even present an approximate solution online. Here we present Randomized Anytime Orienteering (RAOr) algorithm that provides near optimal solutions while demonstrably converging to an efficient solution in runtimes that allows the solver to be run online. The key idea of our approach is to pose orienteering as a combination of a Constraint Satisfaction Problem and a Traveling Salesman Problem. This formulation allows us to restrict the search space to routes that incur minimum distance to visit a set of selected nodes, and rapidly search this space using random sampling. The paper provides the analysis of asymptotic nearoptimality, convergence rates for RAOr algorithms, and present strategies to improve anytime performance of the algorithm. Our experimental results suggest an improvement by an order of magnitude over the state of the art methods in relevant simulation and in real world scenarios.}
}

@inproceedings{ushani2017icra,
  author    = {A. Ushani and R. Wolcott and J. Walls and R. Eustice},
  title     = {{A Learning Approach for Real-Time Temporal Scene Flow Estimation from LIDAR Data}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Field Robots},
  abstract  = {Many autonomous systems require the ability to perceive and understand motion in a dynamic environment. We present a novel algorithm that estimates this motion from raw LIDAR data in real-time without the need for segmentation or model-based tracking. The sensor data is first used to construct an occupancy grid. The foreground is then extracted via a learned background filter. Using the filtered occupancy grid, raw scene flow between successive scans is computed. Finally, we incorporate these measurements in a filtering framework to estimate temporal scene flow. We evaluate our method on the KITTI dataset.}
}

@inproceedings{desaraju2017icra,
  author    = {V. Desaraju and N. Michael},
  title     = {{Leveraging Experience for Computationally Efficient Adaptive Nonlinear Model Predictive Control}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Robust/Adaptive Control of Robotic Systems, Learning and Adaptive Systems, Optimization and Optimal Control},
  abstract  = {This work presents Experience-driven Predictive Control (EPC) as a fast technique for solving nonlinear model predictive control (NMPC) problems with uncertain system dynamics. EPC leverages an affine dynamics model that is updated online via Locally Weighted Projection Regression (LWPR) to capture nonlinearities, uncertainty, and changes in the system dynamics. This model enables the NMPC problem to be re-cast as a quadratic program (QP). The QP can then be solved via multi-parametric techniques to generate a mapping from state, reference, and dynamics model to a locally optimal, affine feedback control law. These mappings, in conjunction with the basis functions learned via LWPR, define a notion of experience for the controller as they capture the full inputoutput relationship for previous actions the controller has taken. The resulting experience database allows EPC to avoid solving redundant optimization problems, and as it is constructed online, enables the system to operate more efficiently over time. We demonstrate the performance of EPC through a set of hardware-in-the-loop simulation studies of a quadrotor micro air vehicle that is subjected to unmodeled exogenous perturbations.}
}

@inproceedings{zheng2017icra,
  author    = {X. Zheng and Z. Moratto and M. Li and A. Mourikis},
  title     = {{Photometric Patch-Based Visual-Inertial Odometry}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Sensor Fusion, Localization, SLAM},
  abstract  = {In this paper we present a novel direct visualinertial odometry algorithm, for estimating motion in unknown environments. The algorithm utilizes image patches extracted around image features, and formulates measurement residuals in the image intensity space directly. One key characteristic of the proposed method is that it models the true irradiance at each pixel as a random variable to be estimated and marginalized out. The formulation of the photometric residual explicitly accounts for the camera response function and lens vignetting (which can be calibrated in advance), as well as unknown illumination gains and biases, which are estimated on a per-feature or per-image basis. We present a detailed evaluation of our algorithm on 50 datasets with high-precision ground truth, which amount to approximately 1.5 hours of localization data. Through a direct comparison with a pointfeature based method, we demonstrate that the use of photometric residuals results in increased pose estimation accuracy, with approximately 23\% lower estimation errors, on average.}
}

@inproceedings{ma2017icra-ipao,
  author    = {K. Ma and L. Liu and G. Sukhatme},
  title     = {{Informative Planning and Online Learning with Sparse Gaussian Processes}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Marine Robotics, Motion and Path Planning, Learning and Adaptive Systems},
  abstract  = {A big challenge in environmental monitoring is the spatiotemporal variation of the phenomena to be observed. To enable persistent sensing and estimation in such a setting, it is beneficial to have a time-varying underlying environmental model. Here we present a planning and learning method that enables an autonomous marine vehicle to perform persistent ocean monitoring tasks by learning and refining an environmental model. To alleviate the computational bottleneck caused by large-scale data accumulated, we propose a framework that iterates between a planning component aimed at collecting the most information-rich data, and a sparse Gaussian Process learning component where the environmental model and hyperparameters are learned online by taking advantage of only a subset of data that provides the greatest contribution. Our simulations with ground-truth ocean data shows that the proposed method is both accurate and efficient.}
}

@inproceedings{fourie2017icra,
  author    = {D. Fourie and S.D. Claassens and S. Pillai and R. Mata and J. Leonard},
  title     = {{SLAMinDB: Centralized Graph Databases for Mobile Robotics}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Autonomous Vehicle Navigation, SLAM, Distributed Robot Systems},
  abstract  = {Robotic systems typically require memory recall mechanisms for a variety of tasks including localization, mapping, planning, visualization etc. We argue for a novel memory recall framework that enables more complex inference schemas by separating the computation from its associated data. In this work we propose a shared, centralized data persistence layer that maintains an ensemble of online, situationally-aware robot states. This is realized through a queryable graph-database with an accompanying key-value store for larger data. In turn, this approach is scalable and enables a multitude of capabilities such as experience-based learning and long-term autonomy. Using multi-modal simultaneous localization and mapping and a few example use-cases, we demonstrate the versatility and extensible nature that centralized persistence and SLAMinDB can provide. In order to support the notion of life-long autonomy, we envision robots to be endowed with such a persistence model, enabling them to revisit previous experiences and improve upon their existing task-specific capabilities.}
}

@inproceedings{wu2017icra,
  author    = {K. WU and X. Li and R. Ranasinghe and G. Dissanayake and Y. Liu},
  title     = {{RISAS: A Novel Rotation, Illumination, Scale Invariant Appearance and Shape Feature}},
  booktitle = icra,
  year      = 2017,
  keywords  = {RGB-D Perception},
  abstract  = {This paper presents a novel appearance and shape feature, RISAS, which is robust to viewpoint, illumination, scale and rotation variations. RISAS consists of a keypoint detector and a feature descriptor both of which utilise texture and geometric information present in the appearance and shape channels. A novel response function based on the surface normals is used in combination with the Harris corner detector for selecting keypoints in the scene. A strategy that uses the depth information for scale estimation and background elimination is proposed to select the neighbourhood around the keypoints in order to build precise invariant descriptors. Proposed descriptor relies on the ordering of both grayscale intensity and shape information in the neighbourhood. Comprehensive experiments which confirm the effectiveness of the proposed RGB-D feature when compared with CSHOT [1] and LOIND[2] are presented. Furthermore, we highlight the utility of incorporating texture and shape information in the design of both the detector and the descriptor by demonstrating the enhanced performance of CSHOT and LOIND when combined with RISAS detector.}
}

@inproceedings{kim2017icra-rrrd,
  author    = {J. Kim and Y. Latif and I. Reid},
  title     = {{RRD-SLAM: Radial-Distorted Rolling-Shutter Direct SLAM}},
  booktitle = icra,
  year      = 2017,
  keywords  = {SLAM},
  abstract  = {In this paper, we present a monocular direct semi-dense SLAM (Simultaneous Localization And Mapping) method that can handle both radial distortion and rollingshutter distortion. Such distortions are common in, but not restricted to, situations when an inexpensive wide-angle lens and a CMOS sensor are used, and leads to significant inaccuracy in the map and trajectory estimates if not modeled correctly. The apparent naive solution of simply undistorting the images using pre-calibrated parameters does not apply to this case since rows in the undistorted image are no longer captured at the same time. To address this we develop an algorithm that incorporates radial distortion into an existing state-of-theart direct semi-dense SLAM system that takes rolling-shutters into account. We propose a method for finding the generalized epipolar curve for each rolling-shutter radially distorted image. Our experiments demonstrate the efficacy of our approach and compare it favorably with the state-of-the-art in direct semidense rolling-shutter SLAM.}
}

@inproceedings{dutoit2017icra,
  author    = {R. DuToit and J.A. Hesch and E. Nerurkar and S. Roumeliotis},
  title     = {{Consistent Map-Based 3D Localization on Mobile Devices}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Localization},
  abstract  = {In this paper, we seek to provide consistent, realtime 3D localization capabilities to mobile devices navigating within previously mapped areas. To this end, we introduce the Cholesky-Schmidt-Kalman filter (C-SKF), which explicitly considers the uncertainty of the prior map, by employing the sparse Cholesky factor of the maps Hessian, instead of its dense covarianceas is the case for the Schmidt-Kalman filter. By doing so, the C-SKF has memory requirements typically linear in the size of the map, as opposed to quadratic for storing the maps covariance. Moreover, and in order to bound the processing needs of the C-SKF (between linear and quadratic in the size of the map), we introduce two relaxations of the CSKF algorithm: (i) The sC-SKF, which operates on the Cholesky factors of independent sub-maps resulting from dividing the map into overlapping segments. (ii) We formulate an efficient method for sparsifying the Cholesky factor by selecting and processing a subset of loop-closure measurements based on their temporal distribution. Lastly, we assess the processing and memory requirements of the proposed algorithms, and compare their positioning accuracy against other inconsistent mapbased localization approaches that employ measurement-noisecovariance inflation to compensate for the maps uncertainty.}
}

@inproceedings{ammirato2017icra,
  author    = {P. Ammirato and P. Poirson and E. Park and J. Kosecka and A. Berg},
  title     = {{A Dataset for Developing and Benchmarking Active Vision}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Object detection, Segmentation, Categorization, Computer Vision for Other Robotic Applications, Recognition},
  abstract  = {We present a new public dataset with a focus on simulating robotic vision tasks in everyday indoor environments using real imagery. The dataset includes 20,000+ RGB-D images and 50,000+ 2D bounding boxes of object instances densely captured in 9 unique scenes. We train a fast object category detector for instance detection on our data. Using the dataset we show that, although increasingly accurate and fast, the state of the art for object detection is still severely impacted by object scale, occlusion, and viewing direction all of which matter for robotics applications. We next validate the dataset for simulating active vision, and use the dataset to develop and evaluate a deep-network-based system for next best move prediction for object classification using reinforcement learning. Our dataset is available for download at cs.unc. edu/ammirato/active_vision_dataset_website/.}
}

@inproceedings{valada2017icra,
  author    = {A. Valada and J. Vertens and A. Dhall and W. Burgard},
  title     = {{AdapNet: Adaptive Semantic Segmentation in Adverse Environmental Conditions}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Semantic Scene Understanding, Learning and Adaptive Systems, Object detection, Segmentation, Categorization},
  abstract  = {Robust scene understanding of outdoor environments using passive optical sensors is a onerous and essential task for autonomous navigation. The problem is heavily characterized by changing environmental conditions throughout the day and across seasons. Robots should be equipped with models that are impervious to these factors in order to be operable and more importantly to ensure safety in the real-world. In this paper, we propose a novel semantic segmentation architecture and the convoluted mixture of deep experts (CMoDE) fusion technique that enables a multi-stream deep neural network to learn features from complementary modalities and spectra, each of which are specialized in a subset of the input space. Our model adaptively weighs class-specific features of expert networks based on the scene condition and further learns fused representations to yield robust segmentation. We present results from experimentation on three publicly available datasets that contain diverse conditions including rain, summer, winter, dusk, fall, night and sunset, and show that our approach exceeds the state-of-the-art. In addition, we evaluate the performance of autonomously traversing several kilometres of a forested environment using only the segmentation for perception.}
}

@inproceedings{pavlakos2017icra,
  author    = {G. Pavlakos and X. Zhou and A. Chan and K. Derpanis and K. Daniilidis},
  title     = {{6-DoF Object Pose from Semantic Keypoints}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Visual Tracking, Computer Vision for Automation, Computer Vision for Other Robotic Applications},
  abstract  = {This paper presents a novel approach to estimating the continuous six degree of freedom (6-DoF) pose (3D translation and rotation) of an object from a single RGB image. The approach combines semantic keypoints predicted by a convolutional network (convnet) with a deformable shape model. Unlike prior work, we are agnostic to whether the object is textured or textureless, as the convnet learns the optimal representation from the available training image data. Furthermore, the approach can be applied to instance- and class-based pose recovery. Empirically, we show that the proposed approach can accurately recover the 6-DoF object pose for both instanceand class-based scenarios with a cluttered background. For class-based object pose estimation, state-of-the-art accuracy is shown on the large-scale PASCAL3D+ dataset.}
}

@inproceedings{mehta2017icra,
  author    = {D. Mehta and G. Ferrer and E. Olson},
  title     = {{Fast Discovery of Influential Outcomes for Risk-Aware MPDM}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Motion and Path Planning, Intelligent Transportation Systems, Field Robots},
  abstract  = {In the Multi-Policy Decision Making (MPDM) framework, a robots policy is elected by sampling from the distribution of current states, predicting future outcomes through forward simulation, and selecting the policy with the best expected performance. Electing the best plan depends on sampling initial conditions with influential (very high costs) outcomes. Discovering these configurations through random sampling may require drawing many samples, which becomes a performance bottleneck. In this paper, we describe a risk-aware approach which augments this sampling with an optimization process that helps discover those influential outcomes. We describe how we overcome several practical difficulties with this approach, and demonstrate significant performance improvements on a real robot platform navigating a semi-crowded, highly dynamic environment.}
}

@inproceedings{huang2017icra,
  author    = {E. Huang and M. Mukadam and Z. Liu and B. Boots},
  title     = {{Motion Planning with Graph-Based Trajectories and Gaussian Process Inference}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Motion and Path Planning},
  abstract  = {Motion planning as trajectory optimization requires generating trajectories that minimize a desired objective function or performance metric. Finding a globally optimal solution is often intractable in practice: despite the existence of fast motion planning algorithms, most are prone to local minima, which may require re-solving the problem multiple times with different initializations. In this work we provide a novel motion planning algorithm, GPMP-GRAPH, that considers a graph-based initialization that simultaneously explores multiple homotopy classes, helping to contend with the local minima problem. Drawing on previous work to represent continuoustime trajectories as samples from a Gaussian process (GP) and formulating the motion planning problem as inference on a factor graph, we construct a graph of interconnected states such that each path through the graph is a valid trajectory and efficient inference can be performed on the collective factor graph. We perform a variety of benchmarks and show that our approach allows the evaluation of an exponential number of trajectories within a fraction of the computational time required to evaluate them one at a time, yielding a more thorough exploration of the solution space and a higher success rate.}
}

@inproceedings{paul2017icra,
  author    = {M.K. Paul and K. Wu and J.A. Hesch and E. Nerurkar and S. Roumeliotis},
  title     = {{A Comparative Analysis of Tightly-Coupled Monocular, Binocular, and Stereo VINS}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Localization, SLAM, Visual-Based Navigation},
  abstract  = {In this paper, a sliding-window two-camera visionaided inertial navigation system (VINS) is presented in the square-root inverse domain. The performance of the system is assessed for the cases where feature matches across the two-camera images are processed with or without any stereo constraints (i.e., stereo vs. binocular). To support the comparison results, a theoretical analysis on the information gain when transitioning from binocular to stereo is also presented. Additionally, the advantage of using a two-camera (both stereo and binocular) system over a monocular VINS is assessed. Furthermore, the impact on the achieved accuracy of different image-processing frontends and estimator design choices is quantified. Finally, a thorough evaluation of the algorithms processing requirements, which runs in real-time on a mobile processor, as well as its achieved accuracy as compared to alternative approaches is provided, for various scenes and motion profiles.}
}

@inproceedings{wang2017icra-sctl,
  author    = {W. Wang and N. Wang and X. Wu and S. You and U. Neumann},
  title     = {{Self-Paced Cross-Modality Transfer Learning for Efficient Road Segmentation}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Object detection, Segmentation, Categorization, Computer Vision for Transportation, Computer Vision for Other Robotic Applications, CNN},
  abstract  = {Accurate road segmentation is a prerequisite for autonomous driving. Current state-of-the-art methods are mostly based on convolutional neural networks (CNNs). Nevertheless, their good performance is at expense of abundant annotated data and high computational cost. In this work, we address these two issues by a self-paced cross-modality transfer learning framework with efficient projection CNN. To be specific, with the help of stereo images, we first tackle a relevant but easier task, i.e. free-space detection with well developed unsupervised methods. Then, we transfer these useful but noisy knowledge in depth modality to single RGB modality with self-paced CNN learning. Finally, we only need to finetune the CNN with a few annotated images to get good performance. In addition, we propose an efficient projection CNN, which can improve the fine-grained segmentation results with little additional cost. At last, we test our method on KITTI road benchmark. Our proposed method surpasses all published methods at a speed of 15fps.}
}

@inproceedings{runz2020cvpr,
  author    = {M. Runz and K. Li and M. Tang and L. Ma and C. Kong and T. Schmidt and I. Reid and L. Agapito and J. Straub and S. Lovegrove and R. Newcombe},
  title     = {{FroDO: From Detections to 3D Objects}},
  booktitle = cvpr,
  year      = 2020,
  abstract  = {Object-oriented maps are important for scene understanding since they jointly capture geometry and semantics, allow individual instantiation and meaningful reasoning about objects. We introduce FroDO, a method for accurate 3D reconstruction of object instances from RGB video that infers their location, pose and shape in a coarse to fine manner. Key to FroDO is to embed object shapes in a novel learnt shape space that allows seamless switching between sparse point cloud and dense DeepSDF decoding. Given an input sequence of localized RGB frames, FroDO first aggregates 2D detections to instantiate a 3D bounding box per object. A shape code is regressed using an encoder network before optimizing shape and pose further under the learnt shape priors using sparse or dense shape representations. The optimization uses multi-view geometric, photometric and silhouette losses. We evaluate on real-world datasets, including Pix3D, Redwood-OS, and ScanNet, for single-view, multi-view, and multi-object reconstruction.},
  url       = {proceedings: runz2020cvpr.pdf}
}

@inproceedings{runz2017icra,
  author    = {M. Runz and L. Agapito},
  title     = {{Co-Fusion: Real-Time Segmentation, Tracking and Fusion of Multiple Objects}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Visual Tracking, Sensor Fusion},
  abstract  = {In this paper we introduce Co-Fusion, a dense SLAM system that takes a live stream of RGB-D images as input and segments the scene into different objects (using either motion or semantic cues) while simultaneously tracking and reconstructing their 3D shape in real time. We use a multiple model fitting approach where each object can move independently from the background and still be effectively tracked and its shape fused over time using only the information from pixels associated with that object label. Previous attempts to deal with dynamic scenes have typically considered moving regions as outliers, and consequently do not model their shape or track their motion over time. In contrast, we enable the robot to maintain 3D models for each of the segmented objects and to improve them over time through fusion. As a result, our system can enable a robot to maintain a scene description at the object level which has the potential to allow interactions with its working environment; even in the case of dynamic scenes.}
}

@inproceedings{byravan2017icra,
  author    = {A. Byravan and D. Fox},
  title     = {{SE3-Nets: Learning Rigid Body Motion Using Deep Neural Networks}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Learning and Adaptive Systems, RGB-D Perception, Object detection, Segmentation, Categorization},
  abstract  = {We introduce SE3-N ETS which are deep neural networks designed to model and learn rigid body motion from raw point cloud data. Based only on sequences of depth images along with action vectors and point wise data associations, SE3-N ETS learn to segment effected object parts and predict their motion resulting from the applied force. Rather than learning point wise flow vectors, SE3-N ETS predict SE(3) transformations for different parts of the scene. Using simulated depth data of a table top scene and a robot manipulator, we show that the structure underlying SE3-N ETS enables them to generate a far more consistent prediction of object motion than traditional flow based networks. Additional experiments with a depth camera observing a Baxter robot pushing objects on a table show that SE3-N ETS also work well on real data.}
}

@inproceedings{song2017icra,
  author    = {S. Song and S. Jo},
  title     = {{Online Inspection Path Planning for Autonomous 3D Modeling Using a Micro-Aerial Vehicle}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Motion and Path Planning, Aerial Robotics, Visual-Based Navigation},
  abstract  = {In this paper, we propose a novel algorithm for planning exploration paths to generate 3D models of unknown environments by using a micro-aerial vehicle (MAV). Our algorithm initially determines a next-best-view (NBV) that maximizes information gain and plans a collision-free path to reach the NBV. Along the path, the MAV explores the greatest unknown area although it sometimes misses minor unreconstructed region, such as a hole or a sparse surface. To cover such a region, we propose an online inspection algorithm that consistently provides an optimal coverage path toward the NBV in real time. The algorithm iteratively refines an inspection path according to the acquired information until the modeling of a specific local area is complete. We evaluated the proposed algorithm by comparing it with other state-of-the-art approaches through simulated experiments. The results show that our algorithm outperforms the other approaches in both exploration and 3D modeling scenarios.}
}

@inproceedings{delmerico2017icra,
  author    = {J. Delmerico and E. Mueggler and J. Nitsch and D. Scaramuzza},
  title     = {{Active Autonomous Aerial Exploration for Ground Robot Path Planning}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Search and Rescue Robots, Motion and Path Planning, Visual-Based Navigation},
  abstract  = {We address the problem of planning a path for a ground robot through unknown terrain, using observations from a flying robot. In search and rescue missions, which are our target scenarios, the time from arrival at the disaster site to the delivery of aid is critically important. Previous works required exhaustive exploration before path planning, which is time-consuming but eventually leads to an optimal path for the ground robot. Instead, we propose active exploration of the environment, where the flying robot chooses regions to map in a way that optimizes the overall response time of the system, which is the combined time for the air and ground robots to execute their missions. In our approach, we estimate terrain classes throughout our terrain map, and we also add elevation information in areas where the active exploration algorithm has chosen to perform 3D reconstruction. This terrain information is used to estimate feasible and efficient paths for the ground robot. By exploring the environment actively, we achieve superior response times compared to both exhaustive and greedy exploration strategies. We demonstrate the performance and capabilities of the proposed system in simulated and real-world outdoor experiments. To the best of our knowledge, this is the first work to address ground robot path planning using active aerial exploration.}
}

@inproceedings{dong2017icra-anmf,
  author    = {W. Dong and V. Isler},
  title     = {{A Novel Method for the Extrinsic Calibration of a 2-D Laser-Rangefinder and a Camera}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Calibration and Identification, Range Sensing, RGB-D Perception},
  abstract  = {We present a novel method for extrinsically calibrating a camera and a 2-D Laser Rangefinder (LRF) whose beams are invisible from the camera image. We show that pointto-plane constraints from a single observation of a V-shaped calibration pattern composed of two non-coplanar triangles suffice to uniquely constrain the relative pose between two sensors. Next, we present an approach to obtain solutions using point-to-plane constraints from single or multiple observations. Along the way, we also show that previous solutions, in contrast to our method, have inherent ambiguities and therefore must rely on a good initial estimate. Real and synthetic experiments validate our method and show that it achieves better accuracy than previous methods.}
}

@inproceedings{ambrus2017icra,
  author    = {R. Ambrus and S. Claici and A.J. Wendt},
  title     = {{Automatic Room Segmentation from Unstructured 3D Data of Indoor Environments}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Mapping, Semantic Scene Understanding, RGB-D Perception},
  abstract  = {We present an automatic approach for the task of reconstructing a 2D floor plan from unstructured point clouds of building interiors. Our approach emphasizes accurate and robust detection of building structural elements, and unlike previous approaches does not require prior knowledge of scanning device poses. The reconstruction task is formulated as a multiclass labeling problem that we approach using energy minimization. We use intuitive priors to define the costs for the energy minimization problem, and rely on accurate wall and opening detection algorithms to ensure robustness. We provide detailed experimental evaluation results, both qualitative and quantitative, against state of the art methods and labeled ground truth data.}
}

@inproceedings{jadidi2017icra,
  author    = {M.G. Jadidi and J.V. Miro and G. Dissanayake},
  title     = {{Warped Gaussian Processes Occupancy Mapping with Uncertain Inputs}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Mapping},
  abstract  = {In this paper, we study extensions to the Gaussian Processes (GPs) continuous occupancy mapping problem. There are two classes of occupancy mapping problems that we particularly investigate. The first problem is related to mapping under pose uncertainty and how to propagate pose estimation uncertainty into the map inference. We develop expected kernel and expected sub-map notions to deal with uncertain inputs. In the second problem, we account for the complication of the robots perception noise using Warped Gaussian Processes (WGPs). This approach allows for non-Gaussian noise in the observation space and captures the possible nonlinearity in that space better than standard GPs. The developed techniques can be applied separately or concurrently to a standard GP occupancy mapping problem. According to our experimental results, although taking into account pose uncertainty leads, as expected, to more uncertain maps, by modeling the nonlinearities present in the observation space WGPs can improve the map quality.}
}

@inproceedings{zhou2017icra-svof,
  author    = {Y. Zhou and L. Kneip and H. Li},
  title     = {{Semi-Dense Visual Odometry for RGB-D Cameras Using Approximate Nearest Neighbour Fields}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Localization, Mapping, SLAM},
  abstract  = {This paper presents a robust and efficient semidense visual odometry solution for RGB-D cameras. The core of our method is a 2D-3D ICP pipeline which estimates the pose of the sensor by registering the projection of a 3D semidense map of a reference frame with the 2D semi-dense region extracted in the current frame. The processing is speeded up by efficiently implemented approximate nearest neighbour fields under the Euclidean distance criterion, which permits the use of compact Gauss-Newton updates in the optimization. The registration is formulated as a maximum a posterior problem to deal with outliers and sensor noise, and the equivalent weighted least squares problem is consequently solved by iteratively reweighted least squares method. A variety of robust weight functions are tested and the optimum is determined based on the probabilistic characteristics of the sensor model. Extensive evaluation on publicly available RGB-D datasets shows that the proposed method predominantly outperforms existing state-ofthe-art methods.}
}

@inproceedings{mur-artal2017icra,
  author    = {R. Mur-Artal and J.D. Tardos},
  title     = {{Visual-Inertial Monocular SLAM with Map Reuse}},
  booktitle = icra,
  year      = 2017,
  keywords  = {SLAM, Sensor Fusion, Visual-Based Navigation},
  abstract  = {In recent years there have been excellent results in Visual-Inertial Odometry techniques, which aim to compute the incremental motion of the sensor with high accuracy and robustness. However these approaches lack the capability to close loops, and trajectory estimation accumulates drift even if the sensor is continually revisiting the same place. In this work we present a novel tightly-coupled Visual-Inertial Simultaneous Localization and Mapping system that is able to close loops and reuse its map to achieve zero-drift localization in already mapped areas. While our approach can be applied to any camera configuration, we address here the most general problem of a monocular camera, with its well-known scale ambiguity. We also propose a novel IMU initialization method, which computes the scale, the gravity direction, the velocity, and gyroscope and accelerometer biases, in a few seconds with high accuracy. We test our system in the 11 sequences of a recent micro-aerial vehicle public dataset achieving a typical scale factor error of 1\% and centimeter precision. We compare to the state-of-the-art in visual-inertial odometry in sequences with revisiting, proving the better accuracy of our method due to map reuse and no drift accumulation. Index Terms SLAM, Sensor Fusion, Visual-Based Navigation}
}

@inproceedings{sa2017icra,
  author    = {I. Sa and C. Lehnert and A. English and C.S. McCool and F. Dayoub and B. Upcroft and T. Perez},
  title     = {{Peduncle Detection of Sweet Pepper for Autonomous Crop Harvesting - Combined Colour and 3D Information}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Agricultural Automation, Robotics in Agriculture and Forestry, RGB-D Perception},
  abstract  = {This paper presents a 3D visual detection method for the challenging task of detecting peduncles of sweet peppers (Capsicum annuum) in the field. Cutting the peduncle cleanly is one of the most difficult stages of the harvesting process, where the peduncle is the part of the crop that attaches it to the main stem of the plant. Accurate peduncle detection in 3D space is therefore a vital step in reliable autonomous harvesting of sweet peppers, as this can lead to precise cutting while avoiding damage to the surrounding plant. This paper makes use of both colour and geometry information acquired from an RGB-D sensor and utilises a supervised-learning approach for the peduncle detection task. The performance of the proposed method is demonstrated and evaluated using qualitative and quantitative results (the Area-Under-the-Curve (AUC) of the detection precision-recall curve). We are able to achieve an AUC of 0.71 for peduncle detection on field-grown sweet peppers. We release a set of manually annotated 3D sweet pepper and peduncle images to assist the research community in performing further research on this topic.}
}

@inproceedings{daudelin2017icra,
  author    = {J.J. Daudelin and M. Campbell},
  title     = {{An Adaptable, Probabilistic, Next Best View Algorithm for Reconstruction of Unknown 3D Objects}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Autonomous Agents, Probability and Statistical Methods, Motion and Path Planning},
  abstract  = {Autonomous mobile robots perform many tasks, such as grasping and inspection, that may require complete models of 3D objects in the environment. If little or no knowledge about an object is known a priori, the robot must take sensor measurements from strategically determined viewpoints in order to reconstruct a 3D model of the object. We propose an autonomous object reconstruction approach for mobile robots that is very general, with no assumptions about object shape or size, such as a bounding box or predetermined set of candidate viewpoints. A probabilistic, volumetric method for determining the optimal next best view is developed based on a partial model of a 3D object of unknown shape and size. The proposed method integrates an object probability characteristic to determine sensor views that incrementally reconstruct a 3D model of the object. Experiments in simulation and on a real world robot validate the work and compare it to the state of the art.}
}

@inproceedings{lehnert2017icra,
  author    = {C. Lehnert and A. English and C.S. McCool and A.M.W. Tow and T. Perez},
  title     = {{Autonomous Sweet Pepper Harvesting for Protected Cropping Systems}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Agricultural Automation, Dexterous Manipulation, Mechanism Design of Manipulators},
  abstract  = {In this paper we present a new robotic harvester (Harvey) that can autonomously harvest sweet pepper in protected cropping environments. Our approach combines effective vision algorithms with a novel end-effector design to enable successful harvesting of sweet peppers. Initial field trials in protected cropping environments, with two cultivar, demonstrate the efficacy of this approach achieving a 46\% success rate for unmodified crop, and 58\% for modified crop. Furthermore, for the more favourable cultivar we were also able to detach 90\% of sweet peppers, indicating that improvements in the grasping success rate would result in greatly improved harvesting performance.}
}

@inproceedings{fleckenstein2017icra,
  author    = {F.V. Fleckenstein and C. Dornhege and W. Burgard},
  title     = {{Efficient Path Planning for Mobile Robots with Adjustable Wheel Positions}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Agricultural Automation, Motion and Path Planning},
  abstract  = {Efficient navigation planning for mobile robots in complex environments is a challenging problem. In this paper we consider the path planning problem for mobile robots with adjustable relative wheel positions, which further increase the navigation capabilities. In particular we account for changes of these relative wheel positions during planning time, thus fully leveraging the capabilities of the robot. Whereas these additional degrees of freedom increase flexibility, they introduce a more challenging planning problem. The approach proposed in this paper is built upon a search-based planner. We describe how to flexibly integrate joint angle changes in the path planning process and furthermore propose a representation of the robot configuration that substantially reduces the computational burden. In addition, we introduce search guidance heuristics that are particularly useful in environments in which a robot is required to pass over obstacles, such as on agricultural fields. An extensive evaluation on simulated and real-world data with our BoniRob agricultural robot demonstrates the efficiency of our approach.}
}

@inproceedings{chen2017icra-caao,
  author    = {S.W. Chen and S. Skandan and S. Dcunha and J. Das and C. Qu and C.J. Taylor and V. Kumar},
  title     = {{Counting Apples and Oranges with Deep Learning: A Data Driven Approach}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Agricultural Automation, Object detection, Segmentation, Categorization, Visual Learning},
  abstract  = {This paper describes a fruit counting pipeline based on deep learning that accurately counts fruit in unstructured environments. Obtaining reliable fruit counts is challenging because of variations in appearance due to illumination changes and occlusions from foliage and neighboring fruits. We propose a novel approach that uses deep learning to map from input images to total fruit counts. The pipeline utilizes a custom crowd-sourcing platform to quickly label large data sets. A blob detector based on a fully convolutional network extracts candidate regions in the images. A counting algorithm based on a second convolutional network then estimates the number of fruit in each region. Finally, a linear regression model maps that fruit count estimate to a final fruit count. We analyze the performance of the pipeline on two distinct data sets of oranges in daylight, and green apples at night, utilizing human generated labels as ground truth. We also show that the pipeline has a short training time and performs well with a limited data set size. Our method generalizes across both data sets and is able to perform well even on highly occluded fruits that are challenging for human labelers to annotate.}
}

@inproceedings{deray2017icra,
  author    = {J. Deray and J. Sol and J. Andrade-Cetto},
  title     = {{Word Ordering and Document Adjacency for Large Loop Closure Detection in 2D Laser Maps}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Localization, Recognition, SLAM},
  abstract  = {We address in this paper the problem of loop closure detection for laser-based simultaneous localization and mapping (SLAM) of very large areas. Consistent with the state of the art, the map is encoded as a graph of poses, and to cope with very large mapping capabilities, loop closures are asserted by comparing the features extracted from a query laser scan against a previously acquired corpus of scan features using a bag-of-words (BoW) scheme. Two contributions are here presented. First, to benefit from the graph topology, feature frequency scores in the BoW are computed not only for each individual scan but also from neighboring scans in the SLAM graph. This has the effect of enforcing neighbor relational information during document matching. Secondly, a weak geometric check that takes into account feature ordering and occlusions is introduced that substantially improves loop closure detection performance. The two contributions are evaluated both separately and jointly on four common SLAM datasets, and are shown to improve the state-of-the-art performance both in terms of precision and recall in most of the cases. Moreover, our current implementation is designed to work at nearly frame rate, allowing loop closure query resolution at nearly 22 Hz for the best case scenario and 2 Hz for the worst case scenario.}
}

@inproceedings{chen2017icra-dlfa,
  author    = {Z. Chen and A. Jacobson and N. S{\"u}nderhauf and B. Upcroft and L. Liu and C. Shen and I. Reid and M.J. Milford},
  title     = {{Deep Learning Features at Scale for Visual Place Recognition}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Visual Learning, Localization, Computer Vision for Other Robotic Applications, CNN},
  abstract  = {The success of deep learning techniques in the computer vision domain has triggered a range of initial investigations into their utility for visual place recognition, all using generic features from networks that were trained for other types of recognition tasks. In this paper, we train, at large scale, two CNN architectures for the specific place recognition task and employ a multi-scale feature encoding method to generate condition- and viewpoint-invariant features. To enable this training to occur, we have developed a massive Specific PlacEs Dataset (SPED) with hundreds of examples of place appearance change at thousands of different places, as opposed to the semantic place type datasets currently available. This new dataset enables us to set up a training regime that interprets place recognition as a classification problem. We comprehensively evaluate our trained networks on several challenging benchmark place recognition datasets and demonstrate that they achieve an average 10\% increase in performance over other place recognition algorithms and pre-trained CNNs. By analyzing the network responses and their differences from pre-trained networks, we provide insights into what a network learns when training for place recognition, and what these results signify for future research in this area.}
}

@inproceedings{mccool2017icra,
  author    = {C.S. McCool and T. Perez and B. Upcroft},
  title     = {{Mixtures of Lightweight Deep Convolutional Neural Networks: Applied to Agricultural Robotics}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Computer Vision for Automation, Recognition, Agricultural Automation, CNN},
  abstract  = {We propose a novel approach for training deep convolutional neural networks (DCNNs) that allows us to tradeoff complexity and accuracy to learn lightweight models suitable for robotic platforms such as AgBot II (which performs automated weed management). Our approach consists of three stages, the first is to adapt a pre-trained model to the task at hand. This provides state-of-the-art performance but at the cost of high computational complexity resulting in a low frame rate of just 0.12 frames per second (fps). Second, we use the adapted model and employ model compression techniques to learn a lightweight DCNN that is less accurate but has two orders of magnitude fewer parameters. Third, K lightweight models are combined as a mixture model to further enhance the performance of the lightweight models. Applied to the challenging task of weed segmentation, we improve accuracy from 85.9\%, using a traditional approach, to 93.9\% by adapting a complicated pre-trained DCNN with 25M parameters (Inception-v3). The downside to this adapted model, Adapted-IV3, is that it can only process 0.12fps. To make this approach fast while still retaining accuracy, we learn lightweight DCNNs which when combined can achieve accuracy greater than 90\% while using considerably fewer parameters capable of processing between 1.07 and 1.83 frames per second, up to an order of magnitude faster and up to an order of magnitude fewer parameters.}
}

@inproceedings{alismail2017icra,
  author    = {H. Alismail and M. Kaess and B. Browning and S. Lucey},
  title     = {{Direct Visual Odometry in Low Light Using Binary Descriptors}},
  booktitle = icra,
  year      = 2017,
  keywords  = {Visual-Based Navigation, SLAM, Mining Robotics},
  abstract  = {Feature descriptors are powerful tools for photometrically and geometrically invariant image matching. To date, however, their use has been tied to sparse interest point detection, which is susceptible to noise under adverse imaging conditions. In this work, we propose to use binary feature descriptors in a direct tracking framework without relying on sparse interest points. This novel combination of feature descriptors and direct tracking is shown to achieve robust and efficient visual odometry with applications to poorly lit subterranean environments.}
}

@article{delmerico2017ral,
  author   = {J. Delmerico and E. Mueggler and J. Nitsch and D. Scaramuzza},
  title    = {{Active Autonomous Aerial Exploration for Ground Robot Path Planning}},
  journal  = ral,
  volume   = {2},
  number   = {2},
  pages    = {664--671},
  year     = 2017,
  keywords = {Search and Rescue Robots, Motion and Path Planning, Visual-Based Navigation},
  abstract = {We address the problem of planning a path for a ground robot through unknown terrain, using observations from a flying robot. In search and rescue missions, which are our target scenarios, the time from arrival at the disaster site to the delivery of aid is critically important. Previous works required exhaustive exploration before path planning, which is time-consuming but eventually leads to an optimal path for the ground robot. Instead, we propose active exploration of the environment, where the flying robot chooses regions to map in a way that optimizes the overall response time of the system, which is the combined time for the air and ground robots to execute their missions. In our approach, we estimate terrain classes throughout our terrain map, and we also add elevation information in areas where the active exploration algorithm has chosen to perform 3D reconstruction. This terrain information is used to estimate feasible and efficient paths for the ground robot. By exploring the environment actively, we achieve superior response times compared to both exhaustive and greedy exploration strategies. We demonstrate the performance and capabilities of the proposed system in simulated and real-world outdoor experiments. To the best of our knowledge, this is the first work to address ground robot path planning using active aerial exploration.}
}

@article{mur-artal2017tro,
  author   = {R. Mur-Artal and J.D. Tard{\'o}s},
  title    = {{ORB-SLAM2: An Open-Source SLAM System for Monocular, Stereo, and RGB-D Cameras}},
  journal  = tro,
  volume   = {33},
  number   = {5},
  pages    = {1255--1262},
  year     = 2017,
  keywords = {SLAM},
  abstract = {We present ORB-SLAM2, a complete simultaneous localiza- tion and mapping (SLAM) system for monocular, stereo and RGB-D cam- eras, including map reuse, loop closing, and relocalization capabilities. The system works in real time on standard central processing units in a wide variety of environments from small hand-held indoors sequences, to drones flying in industrial environments and cars driving around a city. Our back-end, based on bundle adjustment with monocular and stereo observations, allows for accurate trajectory estimation with metric scale. Our system includes a lightweight localization mode that leverages visual odometry tracks for unmapped regions and matches with map points that allow for zero-drift localization. The evaluation on 29 popular public se- quences shows that our method achieves state-of-the-art accuracy, being in most cases the most accurate SLAM solution. We publish the source code, not only for the benefit of the SLAM community, but with the aim of being an out-of-the-box SLAM solution for researchers in other fields.}
}


@article{ambrus2017ral,
  author   = {R. Ambrus and S. Claici and A.J. Wendt},
  title    = {{Automatic Room Segmentation from Unstructured 3D Data of Indoor Environments}},
  journal  = ral,
  volume   = {2},
  number   = {2},
  pages    = {749--756},
  year     = 2017,
  keywords = {Mapping, Semantic Scene Understanding, RGB-D Perception},
  abstract = {We present an automatic approach for the task of reconstructing a 2D floor plan from unstructured point clouds of building interiors. Our approach emphasizes accurate and robust detection of building structural elements, and unlike previous approaches does not require prior knowledge of scanning device poses. The reconstruction task is formulated as a multiclass labeling problem that we approach using energy minimization. We use intuitive priors to define the costs for the energy minimization problem, and rely on accurate wall and opening detection algorithms to ensure robustness. We provide detailed experimental evaluation results, both qualitative and quantitative, against state of the art methods and labeled ground truth data.}
}

@article{jadidi2017ral,
  author   = {M.G. Jadidi and J.V. Miro and G. Dissanayake},
  title    = {{Warped Gaussian Processes Occupancy Mapping with Uncertain Inputs}},
  journal  = ral,
  volume   = {2},
  number   = {2},
  pages    = {680--687},
  year     = 2017,
  keywords = {Mapping},
  abstract = {In this paper, we study extensions to the Gaussian Processes (GPs) continuous occupancy mapping problem. There are two classes of occupancy mapping problems that we particularly investigate. The first problem is related to mapping under pose uncertainty and how to propagate pose estimation uncertainty into the map inference. We develop expected kernel and expected sub-map notions to deal with uncertain inputs. In the second problem, we account for the complication of the robots perception noise using Warped Gaussian Processes (WGPs). This approach allows for non-Gaussian noise in the observation space and captures the possible nonlinearity in that space better than standard GPs. The developed techniques can be applied separately or concurrently to a standard GP occupancy mapping problem. According to our experimental results, although taking into account pose uncertainty leads, as expected, to more uncertain maps, by modeling the nonlinearities present in the observation space WGPs can improve the map quality.}
}

@article{mur-artal2017ral,
  author   = {R. Mur-Artal and J.D. Tardos},
  title    = {{Visual-Inertial Monocular SLAM with Map Reuse}},
  journal  = ral,
  volume   = {2},
  number   = {2},
  pages    = {796--803},
  year     = 2017,
  keywords = {SLAM, Sensor Fusion, Visual-Based Navigation},
  abstract = {In recent years there have been excellent results in Visual-Inertial Odometry techniques, which aim to compute the incremental motion of the sensor with high accuracy and robustness. However these approaches lack the capability to close loops, and trajectory estimation accumulates drift even if the sensor is continually revisiting the same place. In this work we present a novel tightly-coupled Visual-Inertial Simultaneous Localization and Mapping system that is able to close loops and reuse its map to achieve zero-drift localization in already mapped areas. While our approach can be applied to any camera configuration, we address here the most general problem of a monocular camera, with its well-known scale ambiguity. We also propose a novel IMU initialization method, which computes the scale, the gravity direction, the velocity, and gyroscope and accelerometer biases, in a few seconds with high accuracy. We test our system in the 11 sequences of a recent micro-aerial vehicle public dataset achieving a typical scale factor error of 1\% and centimeter precision. We compare to the state-of-the-art in visual-inertial odometry in sequences with revisiting, proving the better accuracy of our method due to map reuse and no drift accumulation. Index Terms SLAM, Sensor Fusion, Visual-Based Navigation}
}

@article{sa2017ral,
  author   = {I. Sa and C. Lehnert and A. English and C.S. McCool and F. Dayoub and B. Upcroft and T. Perez},
  title    = {{Peduncle Detection of Sweet Pepper for Autonomous Crop Harvesting - Combined Colour and 3D Information}},
  journal  = ral,
  year     = 2017,
  volume   = {2},
  number   = {2},
  pages    = {765--772},
  keywords = {Agricultural Automation, Robotics in Agriculture and Forestry, RGB-D Perception},
  abstract = {This paper presents a 3D visual detection method for the challenging task of detecting peduncles of sweet peppers (Capsicum annuum) in the field. Cutting the peduncle cleanly is one of the most difficult stages of the harvesting process, where the peduncle is the part of the crop that attaches it to the main stem of the plant. Accurate peduncle detection in 3D space is therefore a vital step in reliable autonomous harvesting of sweet peppers, as this can lead to precise cutting while avoiding damage to the surrounding plant. This paper makes use of both colour and geometry information acquired from an RGB-D sensor and utilises a supervised-learning approach for the peduncle detection task. The performance of the proposed method is demonstrated and evaluated using qualitative and quantitative results (the Area-Under-the-Curve (AUC) of the detection precision-recall curve). We are able to achieve an AUC of 0.71 for peduncle detection on field-grown sweet peppers. We release a set of manually annotated 3D sweet pepper and peduncle images to assist the research community in performing further research on this topic.}
}

@article{daudelin2017ral,
  author   = {J.J. Daudelin and M. Campbell},
  title    = {{An Adaptable, Probabilistic, Next Best View Algorithm for Reconstruction of Unknown 3D Objects}},
  journal  = ral,
  volume   = {2},
  number   = {3},
  pages    = {1540--1547},
  year     = 2017,
  keywords = {Autonomous Agents, Probability and Statistical Methods, Motion and Path Planning},
  abstract = {Autonomous mobile robots perform many tasks, such as grasping and inspection, that may require complete models of 3D objects in the environment. If little or no knowledge about an object is known a priori, the robot must take sensor measurements from strategically determined viewpoints in order to reconstruct a 3D model of the object. We propose an autonomous object reconstruction approach for mobile robots that is very general, with no assumptions about object shape or size, such as a bounding box or predetermined set of candidate viewpoints. A probabilistic, volumetric method for determining the optimal next best view is developed based on a partial model of a 3D object of unknown shape and size. The proposed method integrates an object probability characteristic to determine sensor views that incrementally reconstruct a 3D model of the object. Experiments in simulation and on a real world robot validate the work and compare it to the state of the art.},
  url      = {https://ieeexplore.ieee.org/ielaam/7083369/7875382/7835670-aam.pdf}
}

@article{lehnert2017ral,
  author   = {C. Lehnert and A. English and C.S. McCool and A.M.W. Tow and T. Perez},
  title    = {{Autonomous Sweet Pepper Harvesting for Protected Cropping Systems}},
  journal  = ral,
  volume   = {2},
  number   = {2},
  pages    = {872--879},
  year     = 2017,
  keywords = {Agricultural Automation, Dexterous Manipulation, Mechanism Design of Manipulators},
  abstract = {In this paper we present a new robotic harvester (Harvey) that can autonomously harvest sweet pepper in protected cropping environments. Our approach combines effective vision algorithms with a novel end-effector design to enable successful harvesting of sweet peppers. Initial field trials in protected cropping environments, with two cultivar, demonstrate the efficacy of this approach achieving a 46\% success rate for unmodified crop, and 58\% for modified crop. Furthermore, for the more favourable cultivar we were also able to detach 90\% of sweet peppers, indicating that improvements in the grasping success rate would result in greatly improved harvesting performance.}
}

@article{chen2017ral,
  author   = {S.W. Chen and S. Skandan and S. Dcunha and J. Das and C. Qu and C.J. Taylor and V. Kumar},
  title    = {{Counting Apples and Oranges with Deep Learning: A Data Driven Approach}},
  journal  = ral,
  volume   = {2},
  number   = {2},
  pages    = {781--788},
  year     = 2017,
  keywords = {Agricultural Automation, Object detection, Segmentation, Categorization, Visual Learning},
  abstract = {This paper describes a fruit counting pipeline based on deep learning that accurately counts fruit in unstructured environments. Obtaining reliable fruit counts is challenging because of variations in appearance due to illumination changes and occlusions from foliage and neighboring fruits. We propose a novel approach that uses deep learning to map from input images to total fruit counts. The pipeline utilizes a custom crowd-sourcing platform to quickly label large data sets. A blob detector based on a fully convolutional network extracts candidate regions in the images. A counting algorithm based on a second convolutional network then estimates the number of fruit in each region. Finally, a linear regression model maps that fruit count estimate to a final fruit count. We analyze the performance of the pipeline on two distinct data sets of oranges in daylight, and green apples at night, utilizing human generated labels as ground truth. We also show that the pipeline has a short training time and performs well with a limited data set size. Our method generalizes across both data sets and is able to perform well even on highly occluded fruits that are challenging for human labelers to annotate.}
}

@article{bonanni2017ral,
  author   = {T.M. Bonanni and B.D. Corte and G. Grisetti},
  title    = {{3D Map Merging on Pose Graphs}},
  journal  = ral,
  volume   = {2},
  number   = {2},
  pages    = {1031--1038},
  year     = 2017,
  keywords = {Mapping, SLAM, Localization},
  abstract = {In this paper, we propose an approach for merging 3D maps represented as pose graphs of point clouds. Our method can effectively deal with typical distortions affecting SLAM-generated maps. Traditional map merging techniques that use a single rigid body transformation to relate the reference frames of different maps. Instead, our approach achieves more accurate results by eliminating the inconsistencies resulting from distortions affecting the inputs, and can succeed in those situations where traditional approaches fail for substantial deformations. The core idea behind our solution is to localize the robot in a reference map by using the data from another map as observations. We validated our approach on publicly available datasets, and provide quantitative results that confirm its effectiveness on challenging instances of the merging problem.}
}

@article{deray2017ral,
  author   = {J. Deray and J. Sol and J. Andrade-Cetto},
  title    = {{Word Ordering and Document Adjacency for Large Loop Closure Detection in 2D Laser Maps}},
  journal  = ral,
  volume   = {2},
  number   = {3},
  pages    = {1532--1539},
  year     = 2017,
  keywords = {Localization, Recognition, SLAM},
  abstract = {We address in this paper the problem of loop closure detection for laser-based simultaneous localization and mapping (SLAM) of very large areas. Consistent with the state of the art, the map is encoded as a graph of poses, and to cope with very large mapping capabilities, loop closures are asserted by comparing the features extracted from a query laser scan against a previously acquired corpus of scan features using a bag-of-words (BoW) scheme. Two contributions are here presented. First, to benefit from the graph topology, feature frequency scores in the BoW are computed not only for each individual scan but also from neighboring scans in the SLAM graph. This has the effect of enforcing neighbor relational information during document matching. Secondly, a weak geometric check that takes into account feature ordering and occlusions is introduced that substantially improves loop closure detection performance. The two contributions are evaluated both separately and jointly on four common SLAM datasets, and are shown to improve the state-of-the-art performance both in terms of precision and recall in most of the cases. Moreover, our current implementation is designed to work at nearly frame rate, allowing loop closure query resolution at nearly 22 Hz for the best case scenario and 2 Hz for the worst case scenario.}
}

@article{schmidt2017ral,
  author   = {T. Schmidt and R. Newcombe and D. Fox},
  title    = {{Self-Supervised Learning of Dense Visual Descriptors}},
  journal  = ral,
  volume   = {2},
  number   = {1},
  pages    = {420--427},
  year     = 2017,
  keywords = {Visual Learning, RGB-D Perception, Recognition},
  abstract = {Robust estimation of correspondences between image pixels is an important problem in robotics, with applications in tracking, mapping, and recognition of objects, environments, and other agents. Correspondence estimation has long been the domain of hand-engineered features, but more recently deep learning techniques have provided powerful tools for learning features from raw data. The drawback of the latter approach is that a vast amount of (labelled, typically) training data is required for learning. This paper advocates a new approach to learning visual descriptors for dense correspondence estimation in which we harness the power of a strong 3D generative model to automatically label correspondences in RGB-D video data. A fully-convolutional network is trained using a contrastive loss to produce viewpoint- and lighting-invariant descriptors. As a proof of concept, we collected two datasets: the first depicts the upper torso and head of the same person in widely varied settings, and the second depicts an office as seen on multiple days with objects re-arranged within. Our datasets focus on re-visitation of the same objects and environments, and we show that by training the CNN only from local tracking data, our learned visual descriptor generalizes towards identifying non-labelled correspondences across videos. We furthermore show that our approach to descriptor learning can be used to achieve state-of-the-art single-frame localization results on the MSR 7-scenes dataset without using any labels identifying correspondences between separate videos of the same scenes at training time.}
}

@article{mccool2017ral,
  author   = {C.S. McCool and T. Perez and B. Upcroft},
  title    = {{Mixtures of Lightweight Deep Convolutional Neural Networks: Applied to Agricultural Robotics}},
  journal  = ral,
  volume   = {2},
  number   = {3},
  pages    = {1344--1351},
  year     = 2017,
  keywords = {Computer Vision for Automation, Recognition, Agricultural Automation, CNN},
  abstract = {We propose a novel approach for training deep convolutional neural networks (DCNNs) that allows us to tradeoff complexity and accuracy to learn lightweight models suitable for robotic platforms such as AgBot II (which performs automated weed management). Our approach consists of three stages, the first is to adapt a pre-trained model to the task at hand. This provides state-of-the-art performance but at the cost of high computational complexity resulting in a low frame rate of just 0.12 frames per second (fps). Second, we use the adapted model and employ model compression techniques to learn a lightweight DCNN that is less accurate but has two orders of magnitude fewer parameters. Third, K lightweight models are combined as a mixture model to further enhance the performance of the lightweight models. Applied to the challenging task of weed segmentation, we improve accuracy from 85.9\%, using a traditional approach, to 93.9\% by adapting a complicated pre-trained DCNN with 25M parameters (Inception-v3). The downside to this adapted model, Adapted-IV3, is that it can only process 0.12fps. To make this approach fast while still retaining accuracy, we learn lightweight DCNNs which when combined can achieve accuracy greater than 90\% while using considerably fewer parameters capable of processing between 1.07 and 1.83 frames per second, up to an order of magnitude faster and up to an order of magnitude fewer parameters.}
}

@article{alismail2017ral,
  author   = {H. Alismail and M. Kaess and B. Browning and S. Lucey},
  title    = {{Direct Visual Odometry in Low Light Using Binary Descriptors}},
  journal  = ral,
  volume   = {2},
  number   = {2},
  pages    = {444--451},
  year     = 2017,
  keywords = {Visual-Based Navigation, SLAM, Mining Robotics},
  abstract = {Feature descriptors are powerful tools for photometrically and geometrically invariant image matching. To date, however, their use has been tied to sparse interest point detection, which is susceptible to noise under adverse imaging conditions. In this work, we propose to use binary feature descriptors in a direct tracking framework without relying on sparse interest points. This novel combination of feature descriptors and direct tracking is shown to achieve robust and efficient visual odometry with applications to poorly lit subterranean environments.}
}

@article{nardi2017jras,
  author   = {L. Nardi and C. Stachniss},
  title    = {{User Preferred Behaviors for Robot Navigation Exploiting Previous Experiences}},
  journal  = jras,
  year     = 2017,
  volume   = 97,
  number   = {C},
  keywords = {Navigation; Path Planning},
  abstract = {Recently, various industries started asking for flexible robots that are able to accomplish different tasks at different locations such as navigation and mobile manipulation. Mobile robots operating on factory floors are usually requested to follow definite and predictable behaviors. This becomes essential when a robot shares the workspace with other moving entities to avoid unexpected behaviors and collisions. In this paper, we present a system for robot navigation that is able to exploit the previous experiences of the robot for providing predictable behaviors that meet user preferences. User preferences are implicitly extracted from robot experiences without requiring experts to hard-code rules and automatically considered to plan paths for the successive tasks. We introduce a probabilistic approach to model the highly uncertain trajectories of the moving individuals that populate factory floors that allows to avoid them behaving according to user preferences. We thoroughly tested our system both in simulation and on an real KUKA mobile robot. An extensive set of experiments presented in this paper demonstrates that using our approach a robot can successfully navigate meeting user preferences.}
}

@article{manikandasriram2017arxiv,
  author   = {S.R. Manikandasriram and C. Anderson and R. Vasudevan and M. Johnson-Roberson},
  title    = {{Failing to Learn: Autonomously Identifying Perception Failures for Self-driving Cars}},
  journal  = arxiv,
  volume   = {arXiv:1707.00051},
  year     = 2017,
  keywords = {Autonomous Driving, Classification, CNN},
  url      = {http://arxiv.org/pdf/1707.00051},
  abstract = {One of the major open challenges in self-driving cars is the ability to detect cars and pedestrians to safely navigate in the world. Deep learning-based object detector approaches have enabled great advances in using camera imagery to detect and classify objects. But for a safety critical application such as autonomous driving, the error rates of the current state-of-the-art are still too high to enable safe operation. Moreover, our characterization of object detector performance is primarily limited to testing on prerecorded datasets. Errors that occur on novel data go undetected without additional human labels. In this paper, we propose an automated method to identify mistakes made by object detectors without ground truth labels. We show that inconsistencies in object detector output between a pair of similar images can be used to identify false negatives(e.g. missed detections). In particular, we study two distinct cues - temporal and stereo inconsistencies - using data that is readily available on most autonomous vehicles. Our method can be used with any camera-based object detector and we evaluate the technique on several sets of real world data. The proposed method achieves over 97\% precision in automatically identifying missed detections produced by one of the leading state-of-the-art object detectors in the literature. We also release a new tracking dataset with over 100 sequences totaling more than 80,000 labeled images from a game engine to facilitate further research. }
}

@inproceedings{beekmans2017egu,
  title     = {3D-Cloud Morphology and Motion from Dense Stereo for Fisheye Cameras},
  author    = {Ch. Beekmans and J. Schneider and T. Laebe and M. Lennefer and C. Stachniss and C. Simmer},
  booktitle = {In Proc.~of the European Geosciences Union General Assembly (EGU)},
  year      = {2017}
}

@inproceedings{bogoslavskyi2017iros,
  title     = {Analyzing the Quality of Matched 3D Point Clouds of Objects},
  author    = {I. Bogoslavskyi and C. Stachniss},
  booktitle = iros,
  year      = {2017},
  url       = {http://www.ipb.uni-bonn.de/pdfs/bogoslavskyi17iros.pdf}
}

@article{bogoslavskyi2017pfg,
  title    = {{Efficient Online Segmentation for Sparse 3D Laser Scans}},
  author   = {Bogoslavskyi, Igor and Stachniss, Cyrill},
  journal  = pfg,
  volume   = {85},
  number   = {1},
  year     = {2017},
  pages    = {41--52},
  abstract = {The ability to extract individual objects in the scene is key for a large number of autonomous navigation systems such as mobile robots or autonomous cars. Such systems navigating in dynamic environments need to be aware of objects that may change or move. In most perception cues, a pre-segmentation of the current image or laser scan into individual objects is the first processing step before a further analysis is performed. In this paper, we present an effective method that first removes the ground from the scan and then segments the 3D data in a range image representation into different objects. A key focus of our work is a fast execution with several hundred Hertz. Our implementation has small computational demands so that it can run online on most mobile systems. We explicitly avoid the computation of the 3D point cloud and operate directly on a 2.5D range image, which enables a fast segmentation for each 3D scan. This approach can furthermore handle sparse 3D data well, which is important for scanners such as the new Velodyne VLP-16 scanner. We implemented our approach in C++ and ROS, thoroughly tested it using different 3D scanners, and will release the source code of our implementation. Our method can operate at frame rates that are substantially higher than those of the sensors while using only a single core of a mobile CPU and producing high-quality segmentation results.},
  url      = {http://www.ipb.uni-bonn.de/pdfs/bogoslavskyi16pfg.pdf},
  codeurl  = {https://github.com/Photogrammetry-Robotics-Bonn/depth_clustering},
  videourl = {https://www.youtube.com/watch?v=6WqsOlHGTLA}
}

@article{lottes2017jfr,
  title   = {{Effective Vision-based Classification for Separating Sugar Beets and Weeds for Precision Farming}},
  author  = {Lottes, P. and H\"oferlin, M. and Sander, S. and Stachniss, C.},
  journal = jfr,
  year    = {2017},
  volume  = {34},
  number  = {6},
  pages   = {1160--1178},
  url     = {http://www.ipb.uni-bonn.de/wp-content/papercite-data/pdf/lottes16jfr.pdf}
}

@article{chebrolu2017ijrr,
  title   = {{Agricultural Robot Dataset for Plant Classification, Localization and Mapping on Sugar Beet Fields}},
  author  = {N. Chebrolu and P. Lottes and A. Schaefer and W. Winterhalter and W. Burgard and C. Stachniss},
  journal = ijrr,
  volume  = {36},
  number  = {10},
  pages   = {1045--1052},
  year    = {2017},
  url     = {http://www.ipb.uni-bonn.de/wp-content/papercite-data/pdf/chebrolu2017ijrr.pdf}
}

@inproceedings{lottes2017iros,
  title     = {Semi-Supervised Online Visual Crop and Weed Classification in Precision Farming Exploiting Plant Arrangement},
  author    = {P. Lottes and C. Stachniss},
  booktitle = iros,
  year      = {2017},
  url       = {http://www.ipb.uni-bonn.de/wp-content/papercite-data/pdf/lottes17iros.pdf}
}

@article{merfels2017pfg,
  title   = {{Sensor Fusion for Self-Localisation of Automated Vehicles}},
  author  = {Merfels, C. and Stachniss, C.},
  journal = pfg,
  volume  = {85},
  number  = {2},
  pages   = {113--126},
  year    = {2017},
  url     = {http://link.springer.com/article/10.1007/s41064-017-0008-1}
}

@inproceedings{milioto2017isprsannals,
  title     = {Real-time Blob-wise Sugar Beets vs Weeds Classification for Monitoring Fields using Convolutional Neural Networks},
  author    = {A. Milioto and P. Lottes and C. Stachniss},
  booktitle = isprsannals,
  year      = {2017},
  abstract  = {UAVs are becoming an important tool for field monitoring and precision farming. A prerequisite for observing and analyzing fields is the ability to identify crops and weeds from image data. In this paper, we address the problem of detecting the sugar beet plants and weeds in the field based solely on image data. We propose a system that combines vegetation detection and deep learning to obtain a high-quality classification of the vegetation in the field into value crops and weeds. We implemented and thoroughly evaluated our system on image data collected from different sugar beet fields and illustrate that our approach allows for accurately identifying the weeds on the field.},
  url       = {http://www.ipb.uni-bonn.de/pdfs/milioto17uavg.pdf}
}

@inproceedings{palazzolo2017isprsannals,
  title     = {{Information-Driven Autonomous Exploration for a Vision-Based MAV}},
  author    = {E. Palazzolo and C. Stachniss},
  booktitle = isprsannals,
  year      = {2017},
  url       = {http://www.ipb.uni-bonn.de/pdfs/palazzolo2017uavg.pdf}
}

@inproceedings{schneider2017isprsannals,
  title     = {On the Quality and Efficiency of Approximate Solutions to Bundle Adjustment with Epipolar and Trifocal Constraints},
  author    = {J. Schneider and C. Stachniss and W. F\"orstner},
  booktitle = isprsannals,
  year      = {2017},
  abstract  = {Bundle adjustment is a central part of most visual SLAM and Structure from Motion systems and thus a relevant component of UAVs equipped with cameras. This paper makes two contributions to bundle adjustment. First, we present a novel approach which exploits trifocal constraints, i.e., constraints resulting from corresponding points observed in three camera images, which allows to estimate the camera pose parameters without 3D point estimation. Second, we analyze the quality loss compared to the optimal bundle adjustment solution when applying different types of approximations to the constrained optimization problem to increase efficiency. We implemented and thoroughly evaluated our approach using a UAV performing mapping tasks in outdoor environments. Our results indicate that the complexity of the constraint bundle adjustment can be decreased without loosing too much accuracy.},
  url       = {https://www.isprs-ann-photogramm-remote-sens-spatial-inf-sci.net/IV-2-W3/81/2017/isprs-annals-IV-2-W3-81-2017.pdf}
}

@article{vysotska2017pfg,
  title   = {Improving SLAM by Exploiting Building Information from Publicly Available Maps and Localization Priors},
  author  = {Vysotska, O. and Stachniss, C.},
  journal = pfg,
  year    = {2017},
  number  = {1},
  pages   = {53--65},
  volume  = {85},
  url     = {http://link.springer.com/article/10.1007/s41064-017-0006-3}
}

@article{jung2017ijgi,
  author  = {J. Jung and C. Stachniss and C. Kim},
  title   = {Automatic room segmentation of 3D laser data using morphological processing},
  journal = ijgi,
  volume  = {6},
  number  = {7},
  pages   = {206},
  year    = {2017},
  url     = {http://www.mdpi.com/2220-9964/6/7/206}
}

@inproceedings{schirmer2017iros,
  author    = {R. Schirmer and P. Biber and C. Stachniss},
  title     = {Efficient Path Planning in Belief Space for Safe Navigation},
  booktitle = iros,
  year      = {2017},
  abstract  = {Robotic lawn-mowers are required to stay within a predefined working area, otherwise they may drive into a pond or on the street. This turns navigation and path planning into safety critical components. If we consider using SLAM techniques in that context, we must be able to provide safety guarantees in the presence of sensor/actuator noise and featureless areas in the environment. In this paper, we tackle the problem of planning a path that maximizes robot safety while navigating inside the working area and under the constraints of limited computing resources and cheap sensors. Our approach uses a map of the environment to estimate localizability at all locations, and it uses these estimates to search for a path from start to goal in belief space using an extended heuristic search algorithm. We implemented our approach using C++ and ROS and thoroughly tested it on simulation data recorded on eight different gardens, as well as on a real robot. The experiments presented in this paper show that our approach leads to short computation times and short paths while maximizing robot safety under certain assumptions.},
  url       = {http://www.ipb.uni-bonn.de/pdfs/schirmer17iros.pdf}
}

@inproceedings{huang2017iros,
  author    = {K.H. Huang and C. Stachniss},
  title     = {{Extrinsic Multi-Sensor Calibration For Mobile Robots Using the Gauss-Helmert Model}},
  booktitle = iros,
  year      = 2017,
  url       = {http://www.ipb.uni-bonn.de/pdfs/huang2017iros.pdf}
}

@article{ampatzidis2017sustainability,
  author   = {Y. Ampatzidis and L. De Bellis and A. Luvisi},
  title    = {{iPathology: Robotic Applications and Management of Plants and Plant Diseases}},
  journal  = {Sustainability},
  year     = 2017,
  volume   = 9,
  number   = 6,
  keywords = {Sustainability, Agriculture Robotics},
  url      = {http://www.mdpi.com/2071-1050/9/6/1010/pdf},
  abstract = {The rapid development of new technologies and the changing landscape of the online world (e.g., Internet of Things (IoT), Internet of All, cloud-based solutions) provide a unique opportunity for developing automated and robotic systems for urban farming, agriculture, and forestry. Technological advances in machine vision, global positioning systems, laser technologies, actuators, and mechatronics have enabled the development and implementation of robotic systems and intelligent technologies for precision agriculture. Herein, we present and review robotic applications on plant pathology and management, and emerging agricultural technologies for intra-urban agriculture. Greenhouse advanced management systems and technologies have been greatly developed in the last years, integrating IoT and WSN (Wireless Sensor Network). Machine learning, machine vision, and AI (Artificial Intelligence) have been utilized and applied in agriculture for automated and robotic farming. Intelligence technologies, using machine vision/learning, have been developed not only for planting, irrigation, weeding (to some extent), pruning, and harvesting, but also for plant disease detection and identification. However, plant disease detection still represents an intriguing challenge, for both abiotic and biotic stress. Many recognition methods and technologies for identifying plant disease symptoms have been successfully developed; still, the majority of them require a controlled environment for data acquisition to avoid false positives. Machine learning methods (e.g., deep and transfer learning) present promising results for improving image processing and plant symptom identification. Nevertheless, diagnostic specificity is a challenge for microorganism control and should drive the development of mechatronics and robotic solutions for disease management.}
}

@inproceedings{vongkulbhisal2017cvpr,
  author    = {J. Vongkulbhisal and F. D. l. Torre and J. P. Costeira},
  title     = {{Discriminative Optimization: Theory and Applications to Point Cloud Registration}},
  booktitle = cvprold,
  year      = 2017,
  abstract  = {Many computer vision problems are formulated as the optimization of a cost function. This approach faces two main challenges: (1) designing a cost function with a local optimum at an acceptable solution, and (2) developing an efficient numerical method to search for one (or multiple) of these local optima. While designing such functions is feasible in the noiseless case, the stability and location of local optima are mostly unknown under noise, occlusion, or missing data. In practice, this can result in undesirable local optima or not having a local optimum in the expected place. On the other hand, numerical optimization algorithms in high-dimensional spaces are typically local and often rely on expensive first or second order information to guide the search. To overcome these limitations, this paper proposes Discriminative Optimization (DO), a method that learns search directions from data without the need of a cost function. Specifically, DO explicitly learns a sequence of updates in the search space that leads to stationary points that correspond to desired solutions. We provide a formal analysis of DO and illustrate its benefits in the problem of 2D and 3D point cloud registration both in synthetic and range-scan data. We show that DO outperforms state-of-the-art algorithms by a large margin in terms of accuracy, robustness to perturbations, and computational efficiency.},
  url       = {proceedings:vongkulbhisal2017cvpr.pdf}
}

@article{zhang2017auro,
  author   = {J. Zhang and S. Singh},
  title    = {{Low-drift and real-time lidar odometry and mapping}},
  journal  = ar,
  year     = {2017},
  volume   = {41},
  pages    = {401--416},
  keywords = {SLAM, Mapping, Velodyne},
  url      = {http://www.frc.ri.cmu.edu/~jizhang03/Publications/AURO_2017_2.pdf}
}

@article{kruesi2017jfr,
  author   = {P. Kr\"usi and P. Furgale and M. Bosse and R. Siegwart},
  title    = {{Driving on Point Clouds: Motion Planning, Trajectory Optimization, and Terrain Assessment in Generic Nonplanar Environments}},
  journal  = jfr,
  year     = {2017},
  volume   = {34},
  number   = {5},
  pages    = {940--984},
  keywords = {Navigation, Point Cloud, Motion Planning, Traversability},
  abstract = {We present a practical approach to global motion planning and terrain assessment for ground robots in generic three-dimensional (3D) environments, including rough outdoor terrain, multilevel facilities, and more complex geometries. Our method computes optimized six-dimensional trajectories compliant with curvature and continuity constraints directly on unordered point cloud maps, omitting any kind of explicit surface reconstruction, discretization, or topology extraction. We assess terrain geometry and traversability on demand during motion planning, by fitting robot-sized planar patches to the map and analyzing the local distribution of map points. Our motion planning approach consists of sampling-based initial trajectory generation, followed by precise local optimization according to a custom cost measure, using a novel, constraint-aware trajectory optimization paradigm. We embed these methods in a complete autonomous navigation system based on localization and mapping by means of a 3D laser scanner and iterative closest point matching, suitable for both static and dynamic environments. The performance of the planning and terrain assessment algorithms is evaluated in offline experiments using recorded and simulated sensor data. Finally, we present the results of navigation experiments in three different environmentsârough outdoor terrain, a two-level parking garage, and a dynamic environment, demonstrating how the proposed methods enable autonomous navigation in complex 3D terrain.}
}

@article{tao2017jfr,
  author   = {Z. Tao and P. Bonnifait and V. Fr\'emont and J. Ibanez-Guzman and S. Bonnet},
  title    = {{Road-Centered Map-Aided Localization for Driverless Cars Using Single-Frequency GNSS Receivers}},
  journal  = jfr,
  year     = {2017},
  volume   = {34},
  number   = {5},
  pages    = {1010--1033},
  keywords = {Autonomous Driving, Localization, EKF},
  abstract = {Accurate localization with high availability is a key requirement for autonomous vehicles. It remains a major challenge when using automotive sensors such as single-frequency Global Navigation Satellite System (GNSS) receivers, a lane detection camera, and proprioceptive sensors. This paper describes a method that enables the estimation of stand-alone single-frequency GNSS errors by integrating the measurements from a forward-looking camera matched with lane markings stored in a digital map. It includes a parameter identification method for a shaping model, which is evaluated using experimental data. An algebraic observability study is then conducted to prove that the proposed state vector is fully observable in a road-oriented frame. This observability property is the basis to develop a road-centered Extended Kalman filter (EKF) that can maintain the observability of every component of the state vector on any road, whatever its orientation. To accomplish this, the filter needs to handle road changes, which it does using bijective transformations. The filter was implemented and tested intensely on an experimental vehicle for driverless valet parking services. Field results have shown that the performance of the estimation process is better than solutions based on EKF implemented in a fixed working frame. The proposed filter guarantees that the drift along the road direction remains bounded. This is very important when the vehicle navigates autonomously. Furthermore, the road-centered modeling improves the accuracy, consistency, and robustness of the localization solver.}
}

@article{ball2017ram,
  author   = {D. Ball and P. Ross and A. English and P. Milani and D. Richards and A. Bate and B. Upcroft and G. Wyeth and P. Corke},
  journal  = ram,
  title    = {{Farm Workers of the Future: Vision-Based Robotics for Broad-Acre Agriculture}},
  year     = {2017},
  volume   = {24},
  number   = {3},
  pages    = {97--107},
  abstract = {Farmers are under growing pressure to intensify production to feed a growing population while managing environmental impact. Robotics has the potential to address these challenges by replacing large complex farm machinery with fleets of small autonomous robots. This article presents our research toward the goal of developing teams of autonomous robots that perform typical farm coverage operations. Making a large fleet of autonomous robots economical requires the use of inexpensive sensors, such as cameras for localization and obstacle avoidance. To this end, we describe a vision-based obstacle detection system that continually adapts to environmental and illumination variations and a visionassisted localization system that can guide a robot along crop rows with a complex appearance. Large fleets of robots will become time-consuming to monitor, control, and resupply. To reduce this burden, we describe a vision-based docking system for autonomously refilling liquid supplies and an interface for controlling multiple robots.},
  keywords = {Agriculture Robotics}
}

@inproceedings{dai2017cvpr,
  title     = {{ScanNet: Richly-Annotated 3D Reconstructions of Indoor Scenes}},
  author    = {Dai, A. and Chang, A.X. and Savva, M. and Halber, M. and Funkhouser, T. and Nie{\ss}ner, M.},
  booktitle = cvprold,
  year      = {2017},
  abstract  = {A key requirement for leveraging supervised deep learning methods is
               the availability of large, labeled datasets. Unfortunately, in the context of RGB-D
               scene understanding, very little data is available -- current datasets cover a small
               range of scene views and have limited semantic annotations. To address this issue,
               we introduce ScanNet, an RGB-D video dataset containing 2.5M views in 1513 scenes
               annotated with 3D camera poses, surface reconstructions, and semantic segmentations.
               To collect this data, we designed an easy-to-use and scalable RGB-D capture system
               that includes automated surface reconstruction and crowd sourced semantic annotation.
               We show that using this data helps achieve state-of-the-art performance on several 3D
               scene understanding tasks, including 3D object classification, semantic voxel labeling,
               and CAD model retrieval.},
  url       = {proceedings: dai2017cvpr-srro.pdf}
}

@inproceedings{vysotska2017irosws,
  title     = {Relocalization under Substantial Appearance Changes using Hashing},
  author    = {O. Vysotska and C. Stachniss},
  booktitle = {Proc.~of the IROS Workshop on Planning, Perception and Navigation for Intelligent Vehicles},
  year      = {2017}
}

@inproceedings{khosravian2017icra-abaf,
  author    = {A. Khosravian and T. Chin and I. Reid},
  title     = {A Branch-And-Bound Algorithm for Checkerboard Extraction in Camera-Laser Calibration},
  booktitle = icra,
  year      = 2017,
  url       = {proceedings:khosravian2017icra-abaf.pdf},
  abstract  = {We address the problem of camera-to-laserscanner calibration using a checkerboard and multiple imagelaser scan pairs. Distinguishing which laser points measure the checkerboard and which lie on the background is essential to any such system. We formulate the checkerboard extraction as a combinatorial optimization problem with a clear cut objective function. We propose a branch-and-bound technique that deterministically and globally optimizes the objective. Unlike what is available in the literature, the proposed method is not heuristic and does not require assumptions such as constraints on the background or relying on discontinuity of the range measurements to partition the data into line segments. The proposed approach is generic and can be applied to both 3D or 2D laser scanners as well as the cases where multiple checkerboards are present. We demonstrate the effectiveness of the proposed approach by providing numerical simulations as well as experimental results.}
}

@article{badrinarayanan2015pami,
  title   = {{SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation}},
  author  = {V. Badrinarayanan and A. Kendall and R. Cipolla},
  journal = pami,
  volume  = {39},
  number  = {12},
  pages   = {2481--2495},
  year    = {2017},
  url     = {https://arxiv.org/pdf/1511.00561.pdf}
}

@article{he2017arxiv,
  author  = {K. He and G. Gkioxari and P. Doll\'{a}r and R. Girshick},
  title   = {{Mask R-CNN}},
  journal = arxiv,
  volume  = {arXiv:1703.06870},
  year    = {2017},
  url     = {https://arxiv.org/pdf/1703.06870.pdf}
}

@article{roy2017natureplants,
  title   = {European infrastructures for sustainable agriculture},
  author  = {J. Roy and F. Tardieu and M. Tixier-Boichard and U. Schurr},
  journal = {Nature Plants},
  volume  = 3,
  pages   = {756--758},
  year    = 2017
}

@inproceedings{wu2017nips,
  title     = {{MarrNet: 3D Shape Reconstruction via 2.5D Sketches}},
  author    = {Wu, J. and Wang, Y. and Xue, T. and Sun, X. and Freeman, W.T. and Tenenbaum, J.B.},
  booktitle = nips,
  year      = 2017,
  url       = {http://marrnet.csail.mit.edu/papers/marrnet_nips.pdf},
  abstract  = {3D object reconstruction from a single image is a highly under-determined prob- lem, requiring strong prior knowledge of plausible 3D shapes. This introduces challenges for learning-based approaches, as 3D object annotations are scarce in real images. Previous work chose to train on synthetic data with ground truth 3D information, but suffered from domain adaptation when tested on real data. In this work, we propose MarrNet, an end-to-end trainable model that sequentially estimates 2.5D sketches and 3D object shape. Our disentangled, two-step formula- tion has three advantages. First, compared to full 3D shape, 2.5D sketches are much easier to be recovered from a 2D image; models that recover 2.5D sketches are also more likely to transfer from synthetic to real data. Second, for 3D reconstruction from 2.5D sketches, systems can learn purely from synthetic data. This is because we can easily render realistic 2.5D sketches without modeling object appearance variations in real images, including lighting, texture, etc. This further relieves the domain adaptation problem. Third, we derive differentiable projective functions from 3D shape to 2.5D sketches; the framework is therefore end-to-end trainable on real images, requiring no human annotations. Our model achieves state-of-the-art performance on 3D shape reconstruction.}
}

@article{bohg2017tro,
  author   = {J. Bohg and K. Hausman and B. Sankaran and O. Brock and D. Kragic and S. Schaal and G. S. Sukhatme},
  journal  = tro,
  title    = {{Interactive Perception: Leveraging Action in Perception and Perception in Action}},
  year     = 2017,
  volume   = 33,
  number   = 6,
  pages    = {1273--1291},
  abstract = {Recent approaches in robot perception follow the insight that perception is facilitated by interaction with the environment. These approaches are subsumed under the term Interactive Perception (IP). This view of perception provides the following benefits. First, interaction with the environment creates a rich sensory signal that would otherwise not be present. Second, knowledge of the regularity in the combined space of sensory data and action parameters facilitates the prediction and interpretation of the sensory signal. In this survey, we postulate this as a principle for robot perception and collect evidence in its support by analyzing and categorizing existing work in this area. We also provide an overview of the most important applications of IP. We close this survey by discussing remaining open questions. With this survey, we hope to help define the field of Interactive Perception and to provide a valuable resource for future research.},
  keywords = {Robot learning, Robot vision systems,interactive perception}
}

@inproceedings{qi2017iccv-gnnf,
  author    = {X. Qi and R. Liao and J. Jia and S. Fidler and R. Urtasun},
  title     = {{3D Graph Neural Networks for RGBD Semantic Segmentation}},
  booktitle = iccv,
  year      = 2017,
  abstract  = {RGBD semantic segmentation requires joint reasoning about 2D appearance and 3D geometric information. In this paper we propose a 3D graph neural network (3DGNN) that builds a k-nearest neighbor graph on top of 3D point cloud. Each node in the graph corresponds to a set of points and is associated with a hidden representation vector initialized with an appearance feature extracted by a unary CNN from 2D images. Relying on recurrent functions, every node dynamically updates its hidden representation based on the current status and incoming messages from its neighbors. This propagation model is unrolled for a certain number of time steps and the final per-node representation is used for predicting the semantic class of each pixel. We use back-propagation through time to train the model. Extensive experiments on NYUD2 and SUN-RGBD datasets demonstrate the effectiveness of our approach.},
  url       = {proceedings: qi2017iccv-gnnf.pdf}
}

@inproceedings{qi2017cvpr,
  author    = {C. R. Qi and H. Su and K. Mo and L. J. Guibas},
  title     = {{PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation}},
  booktitle = cvprold,
  year      = 2017,
  keywords  = {CNN, Classification, Point Clouds},
  abstract  = {Point cloud is an important type of geometric data structure. Due to its irregular format, most researchers transform such data to regular 3D voxel grids or collections of images. This, however, renders data unnecessarily voluminous and causes issues. In this paper, we design a novel type of neural network that directly consumes point clouds, which well respects the permutation invariance of points in the input. Our network, named PointNet, provides a unified architecture for applications ranging from object classification, part segmentation, to scene semantic parsing. Though simple, PointNet is highly efficient and effective. Empirically, it shows strong performance on par or even better than state of the art. Theoretically, we provide analysis towards understanding of what the network has learnt and why the network is robust with respect to input perturbation and corruption.},
  url       = {https://arxiv.org/pdf/1612.00593.pdf}
}

@article{zou2017arxiv,
  author   = {Y. Zhou and O. Tuzel},
  title    = {{VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection}},
  journal  = arxiv,
  volume   = {arXiv:1711.06396},
  year     = 2017,
  keywords = {CNN, Classification, Point Clouds},
  abstract = {Accurate detection of objects in 3D point clouds is a central problem in many applications, such as autonomous navigation, housekeeping robots, and augmented/virtual reality. To interface a highly sparse LiDAR point cloud with a region proposal network (RPN), most existing efforts have focused on hand-crafted feature representations, for example, a bird's eye view projection. In this work, we remove the need of manual feature engineering for 3D point clouds and propose VoxelNet, a generic 3D detection network that unifies feature extraction and bounding box prediction into a single stage, end-to-end trainable deep network. Specifically, VoxelNet divides a point cloud into equally spaced 3D voxels and transforms a group of points within each voxel into a unified feature representation through the newly introduced voxel feature encoding (VFE) layer. In this way, the point cloud is encoded as a descriptive volumetric representation, which is then connected to a RPN to generate detections. Experiments on the KITTI car detection benchmark show that VoxelNet outperforms the state-of-the-art LiDAR based 3D detection methods by a large margin. Furthermore, our network learns an effective discriminative representation of objects with various geometries, leading to encouraging results in 3D detection of pedestrians and cyclists, based on only LiDAR.},
  url      = {https://arxiv.org/pdf/1711.06396.pdf}
}

@article{qi2017arxiv,
  author   = {C.R. Qi and W. Liu and C. Wu and H. Su and L.J. Guibas},
  title    = {{Frustum PointNets for 3D Object Detection from RGB-D Data}},
  journal  = arxiv,
  volume   = {arXiv:1711.08488},
  year     = 2017,
  keywords = {CNN, Classification, Point Clouds},
  abstract = {While object recognition on 2D images is getting more and more mature, 3D understanding is eagerly in demand yet largely underexplored. In this paper, we study the 3D object detection problem from RGB-D data captured by depth sensors in both indoor and outdoor environments. Different from previous deep learning methods that work on 2D RGB-D images or 3D voxels, which often obscure natural 3D patterns and invariances of 3D data, we directly operate on raw point clouds by popping up RGB-D scans. Although recent works such as PointNet performs well for segmentation in small-scale point clouds, one key challenge is how to efficiently detect objects in large-scale scenes. Leveraging the wisdom of dimension reduction and mature 2D object detectors, we develop a Frustum PointNet framework that addresses the challenge. Evaluated on KITTI and SUN RGB-D 3D detection benchmarks, our method outperforms state of the arts by remarkable margins with high efficiency (running at 5 fps).},
  url      = {https://arxiv.org/pdf/1711.08488.pdf}
}

@article{ku2017arxiv,
  author   = {J. Ku and M. Mozifian and J. Lee and A. Harakeh and S. Waslander},
  title    = {{Joint 3D Proposal Generation and Object Detection from View Aggregation}},
  journal  = arxiv,
  volume   = {arXiv:1712.02294},
  year     = 2017,
  keywords = {CNN, Classification, Point Clouds},
  abstract = {We present AVOD, an Aggregate View Object Detection network for autonomous driving scenarios. The proposed neural network architecture uses LIDAR point clouds and RGB images to generate features that are shared by two subnetworks: a region proposal network (RPN) and a second stage detector network. The proposed RPN uses a novel architecture capable of performing multimodal feature fusion to generate reliable 3D object proposals for multiple object classes in road scenes. Using these proposals, the second stage detection network performs accurate oriented 3D bounding box regression and category classification to predict the extents, orientation, and classification of objects in 3D space. Our proposed architecture is shown to produce state of the art results on the KITTI 3D object detection benchmark while running in real time with a low memory footprint, making it a suitable candidate for deployment on autonomous vehicles. },
  url      = {https://arxiv.org/pdf/1712.02294.pdf}
}

@inproceedings{qi2017nips,
  author    = {C.R. Qi and K. Yi and H. Su and L. J. Guibas},
  title     = {{PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space}},
  booktitle = neurips,
  year      = 2017,
  keywords  = {CNN, Classification, Point Clouds},
  abstract  = {Few prior works study deep learning on point sets. PointNet is a pioneer in this direction. However, by design PointNet does not capture local structures induced by the metric space points live in, limiting its ability to recognize fine-grained patterns and generalizability to complex scenes. In this work, we introduce a hierarchical neural network that applies PointNet recursively on a nested partitioning of the input point set. By exploiting metric space distances, our network is able to learn local features with increasing contextual scales. With further observation that point sets are usually sampled with varying densities, which results in greatly decreased performance for networks trained on uniform densities, we propose novel set learning layers to adaptively combine features from multiple scales. Experiments show that our network called PointNet++ is able to learn deep point set features efficiently and robustly. In particular, results significantly better than state-of-the-art have been obtained on challenging benchmarks of 3D point clouds.},
  url       = {https://arxiv.org/pdf/1706.02413.pdf}
}

@article{palazzolo2017uavg,
  author  = {Palazzolo, E. and Stachniss, C.},
  title   = {{Information-Driven Autonomous Exploration for a Vision-Based MAV}},
  journal = {ISPRS Annals of Photogrammetry, Remote Sensing and Spatial Information Sciences},
  volume  = {IV-2/W3},
  year    = {2017},
  pages   = {59--66},
  url     = {http://www.ipb.uni-bonn.de/pdfs/palazzolo2017uavg.pdf}
}

@inproceedings{cieslewski2017iros,
  title     = {{Rapid Exploration with Multi-Rotors: A Frontier Selection Method for High Speed Flight}},
  author    = {Cieslewski, T. and Kaufmann, E. and Scaramuzza, D.},
  booktitle = iros,
  year      = {2017},
  url       = {http://rpg.ifi.uzh.ch/docs/IROS17_Cieslewski.pdf}
}

@inproceedings{bai2017iros,
  title     = {{Toward Autonomous Mapping and Exploration for Mobile Robots through Deep Supervised Learning}},
  author    = {Bai, S. and Chen, F. and Englot, B.},
  booktitle = iros,
  year      = {2017},
  url       = {http://personal.stevens.edu/~benglot/Bai_Chen_Englot_IROS_2017.pdf}
}

@article{delmerico2018ar,
  title   = {{A Comparison of Volumetric Information Gain Metrics for Active 3D Object Reconstruction}},
  author  = {Delmerico, J. and Isler, S. and Sabzevari, R. and Scaramuzza, D.},
  journal = ar,
  volume  = {42},
  pages   = {197--208},
  year    = {2018},
  url     = {http://rpg.ifi.uzh.ch/docs/AURO17_Delmerico.pdf}
}

@article{cvisic2017jfr,
  title    = {{SOFT-SLAM: Computationally Efficient Stereo Visual SLAM for Autonomous UAVs}},
  author   = {I. Cvisic and J. Cesic and I. Markovic and I. Petrovic},
  journal  = jfr,
  volume   = {35},
  number   = {4},
  pages    = {578--595},
  year     = {2017},
  keywords = {SLAM, Mapping, Visual Odometry, Visual-Based Navigation},
  url      = {https://lamor.fer.hr/images/50020776/Cvisic2017.pdf}
}

@article{park2017arxiv,
  title    = {{Elastic LiDAR Fusion: Dense Map-Centric Continuous-Time SLAM}},
  author   = {C. Park and P. Moghadam and S. Kim and A. Elfes and C. Fookes and S. Sridharan},
  journal  = arxiv,
  year     = {2017},
  volume   = {arXiv:1711.01691},
  keywords = {Mapping, SLAM, Range Sensing, Laser-based Navigation},
  url      = {https://arxiv.org/pdf/1711.01691.pdf}
}

@inproceedings{park2017iccv,
  author    = {J. Park and Q. Zhou and V. Koltun},
  title     = {{Colored Point Cloud Registration Revisited}},
  booktitle = iccvold,
  year      = 2017,
  abstract  = {We present an algorithm for tightly aligning two colored point clouds. The key idea is to optimize a joint photometric and geometric objective that locks the alignment along both the normal direction and the tangent plane. We extend a photometric objective for aligning RGB-D images to point clouds, by locally parameterizing the point cloud with a virtual camera. Experiments demonstrate that our algorithm is more accurate and more robust than prior point cloud registration algorithms, including those that utilize color information. We use the presented algorithms to enhance a state-of-the-art scene reconstruction system. The accuracy of the resulting system is demonstrated on real-world scenes with accurate ground-truth models.},
  url       = {proceedings:park2017iccv.pdf}
}

@inproceedings{park2017iccv-rrmr,
  author    = {S. Park and K. Hong and S. Lee},
  title     = {{RDFNet: RGB-D Multi-Level Residual Feature Fusion for Indoor Semantic Segmentation}},
  booktitle = iccvold,
  year      = 2017,
  abstract  = {In multi-class indoor semantic segmentation using RGB-D data, it has been shown that incorporating depth feature into RGB feature is helpful to improve segmentation accuracy. However, previous studies have not fully exploited the potentials of multi-modal feature fusion, e.g., simply concatenating RGB and depth features or averaging RGB and depth score maps. To learn the optimal fusion of multi-modal features, this paper presents a novel network that extends the core idea of residual learning to RGB-D semantic segmentation. Our network effectively captures multi-level RGB-D CNN features by including multi-modal feature fusion blocks and multi-level feature refinement blocks. Feature fusion blocks learn residual RGB and depth features and their combinations to fully exploit the complementary characteristics of RGB and depth data. Feature refinement blocks learn the combination of fused features from multiple levels to enable high-resolution prediction. Our network can efficiently train discriminative multi-level features from each modality end-to-end by taking full advantage of skip-connections. Our comprehensive experiments demonstrate that the proposed architecture achieves the state-of-the-art accuracy on two challenging RGB-D indoor datasets, NYUDv2 and SUN RGB-D.},
  url       = {proceedings: park2017iccv-rrmr.pdf}
}

@inproceedings{palazzolo2017irosws,
  title     = {{Change Detection in 3D Models Based on Camera Images}},
  author    = {E. Palazzolo and C. Stachniss},
  booktitle = {9th Workshop on Planning, Perception and Navigation for Intelligent Vehicles at the IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (IROS)},
  year      = {2017},
  url       = {http://www.ipb.uni-bonn.de/pdfs/palazzolo2017irosws}
}

@article{kok2017arxiv,
  author   = {M. Kok and J.D. Hol and T.B. Sch{\"o}n},
  title    = {{Using Inertial Sensors for Position and Orientation Estimation}},
  journal  = arxiv,
  year     = {2017},
  volume   = {arXiv:1704.06053},
  url      = {http://arxiv.org/pdf/1704.06053v1},
  keywords = {cs.RO},
  abstract = {In recent years, MEMS inertial sensors (3D accelerometers and 3D gyroscopes) have become widely available due to their small size and low cost. Inertial sensor measurements are obtained at high sampling rates and can be integrated to obtain position and orientation information. These estimates are accurate on a short time scale, but suffer from integration drift over longer time scales. To overcome this issue, inertial sensors are typically combined with additional sensors and models. In this tutorial we focus on the signal processing aspects of position and orientation estimation using inertial sensors. We discuss different modeling choices and a selected number of important algorithms. The algorithms include optimization-based smoothing and filtering as well as computationally cheaper extended Kalman filter and complementary filter implementations. The quality of their estimates is illustrated using both experimental and simulated data.}
}

@inproceedings{ok2016icra,
  author    = {K. Ok and W.N. Greene and N. Roy},
  title     = {{Simultaneous Tracking and Rendering: Real-Time Monocular Localization for MAVs}},
  booktitle = icra,
  year      = 2016,
  keywords  = {Localization},
  abstract  = {We propose a method of real-time monocular camera-based localization in known environments. With the goal of controlling high-speed micro air vehicles (MAVs), we localize with respect to a mesh map of the environment that can support both pose estimation and trajectory planning. Using only limited hardware that can be carried on a MAV, we achieve accurate pose estimation at rates above 50 Hz, an order of magnitude faster than the current state-of-the-art meshbased localization algorithms. In our simultaneous tracking and rendering (STAR) approach, we render virtual images of the environment and track camera images with respect to them using a robust semi-direct image alignment technique. Our main contribution is the decoupling of camera tracking from virtual image rendering, which drastically reduces the number of rendered images and enables accurate full camerarate tracking without needing a high-end GPU. We demonstrate our approach in GPS-denied indoor environments.}
}

@inproceedings{bogoslavskyi2016icra,
  author    = {I. Bogoslavskyi and M. Mazuran and C. Stachniss},
  title     = {{Robust Homing for Autonomous Robots}},
  booktitle = icra,
  year      = 2016,
  keywords  = {Autonomous Vehicle Navigation, Search and Rescue Robots, Mapping},
  abstract  = {In autonomous exploration tasks, robots usually rely on a SLAM system to build a map of the environment online and then use it for navigation purposes. Although there has been substantial progress in robustly building accurate maps, these systems cannot guarantee the consistency of the resulting environment model. In this paper, we address the problem of robustly guiding a robot back to its starting location after exploring an unknown environmenteven if the mapping system fails to produce a consistent map. To tackle this problem, we propose a two-step procedure. First, we check if the current map is consistent using a statistical test. If the map is consistent, we navigate the robot back to its starting location using a standard navigation system. In case of an inconsistent map, however, we propose to rewind the trajectory from the current location to the start without relying on a map. We implemented the proposed system in ROS and showcase its effectiveness on an autonomous exploration robot in real underground and office environments.},
  url       = {http://www.ipb.uni-bonn.de/wp-content/papercite-data/pdf/bogoslavskyi16icra.pdf},
  videourl  = {https://www.youtube.com/watch?v=sUvDvq91Vpw}
}

@inproceedings{zhang2016icra,
  author    = {J. Zhang and M. Kaess and S. Singh},
  title     = {{On Degeneracy of Optimization-Based State Estimation Problems}},
  booktitle = icra,
  year      = 2016,
  keywords  = {Visual-Based Navigation, Range Sensing, Mapping},
  abstract  = {Positioning and mapping can be conducted accurately by state-of-the-art state estimation methods. However, reliability of these methods is largely based on avoiding degeneracy that can arise from cases such as scarcity of texture features for vision sensors and lack of geometrical structures for range sensors. Since the problems are inevitably solved in uncontrived environments where sensors cannot function with their highest quality, it is important for the estimation methods to be robust to degeneracy. This paper proposes an online method to mitigate for degeneracy in optimizationbased problems, through analysis of geometric structure of the problem constraints. The method determines and separates degenerate directions in the state space, and only partially solves the problem in well-conditioned directions. We demonstrate utility of this method with data from a camera and lidar sensor pack to estimate 6-DOF ego-motion. Experimental results show that the system is able to improve estimation in environmentally degenerate cases, resulting in enhanced robustness for online positioning and mapping.}
}

@inproceedings{rosen2016icra,
  author    = {D. Rosen and J. Mason and J. Leonard},
  title     = {{Towards Lifelong Feature-Based Mapping in Semi-Static Environments}},
  booktitle = icra,
  year      = 2016,
  keywords  = {Mapping, SLAM, Probability and Statistical Methods},
  abstract  = {The feature-based graphical approach to robotic mapping provides a representationally rich and computationally efficient framework for an autonomous agent to learn a model of its environment. However, this formulation does not naturally support long-term autonomy because it lacks a notion of environmental change; in reality, everything changes and nothing stands still, and any mapping and localization system that aims to support truly persistent autonomy must be similarly adaptive. To that end, in this paper we propose a novel feature-based model of environmental evolution over time. Our approach is based upon the development of an expressive probabilistic generative feature persistence model that describes the survival of abstract semi-static environmental features over time. We show that this model admits a recursive Bayesian estimator, the persistence filter, that provides an exact online method for computing, at each moment in time, an explicit Bayesian belief over the persistence of each feature in the environment. By incorporating this feature persistence estimation into current state-of-the-art graphical mapping techniques, we obtain a flexible, computationally efficient, and information-theoretically rigorous framework for lifelong environmental modeling in an ever-changing world.}
}

@inproceedings{usenko2016icra,
  author    = {V. Usenko and J. Engel and J. St{\"u}ckler and D. Cremers},
  title     = {{Direct Visual-Inertial Odometry with Stereo Cameras}},
  booktitle = icra,
  year      = 2016,
  keywords  = {Sensor Fusion, Visual Tracking, Mapping},
  abstract  = {We propose a novel direct visual-inertial odometry method for stereo cameras. Camera pose, velocity and IMU biases are simultaneously estimated by minimizing a combined photometric and inertial energy functional. This allows us to exploit the complementary nature of vision and inertial data. At the same time, and in contrast to all existing visual-inertial methods, our approach is fully direct: geometry is estimated in the form of semi-dense depth maps instead of manually designed sparse keypoints. Depth information is obtained both from static stereo relating the fixed-baseline images of the stereo camera and temporal stereo relating images from the same camera, taken at different points in time. We show that our method outperforms not only vision-only or loosely coupled approaches, but also can achieve more accurate results than state-of-the-art keypoint-based methods on different datasets, including rapid motion and significant illumination changes. In addition, our method provides high-fidelity semi-dense, metric reconstructions of the environment, and runs in real-time on a CPU.}
}

@inproceedings{peretroukhin2016icra,
  author    = {V. Peretroukhin and W. Vega-Brown and N. Roy and J. Kelly},
  title     = {{PROBE-GK: Predictive Robust Estimation Using Generalized Kernels}},
  booktitle = icra,
  year      = 2016,
  keywords  = {Visual-Based Navigation, Visual Learning, Localization},
  abstract  = {Many algorithms in computer vision and robotics make strong assumptions about uncertainty, and rely on the validity of these assumptions to produce accurate and consistent state estimates. In practice, dynamic environments may degrade sensor performance in predictable ways that cannot be captured with static uncertainty parameters. In this paper, we employ fast nonparametric Bayesian inference techniques to more accurately model sensor uncertainty. By setting a prior on observation uncertainty, we derive a predictive robust estimator, and show how our model can be learned from sample images, both with and without knowledge of the motion used to generate the data. We validate our approach through Monte Carlo simulations, and report significant improvements in localization accuracy relative to a fixed noise model in several settings, including on synthetic data, the KITTI dataset, and our own experimental platform.}
}

@inproceedings{tanner2016icra,
  author    = {M. Tanner and P. Pinies and L.M. Paz and P. Newman},
  title     = {{What Lies Behind: Recovering Hidden Shape in Dense Mapping}},
  booktitle = icra,
  year      = 2016,
  keywords  = {Mapping, Range Sensing, Field Robots},
  abstract  = {In mobile robotics applications, generation of accurate static maps is encumbered by the presence of ephemeral objects such as vehicles, pedestrians, or bicycles. We propose a method to process a sequence of laser point clouds and back-fill dense surfaces into gaps caused by removing objects from the scene a valuable tool in scenarios where resource constraints permit only one mapping pass in a particular region. Our method processes laser scans in a three-dimensional voxel grid using the Truncated Signed Distance Function (TSDF) and then uses a Total Variation (TV) regulariser with a Kernel Conditional Density Estimation (KCDE) soft data term to interpolate missing surfaces. Using four scenarios captured with a push-broom 2D laser, our technique infills approximately 20 m2 of missing surface area for each removed object. Our reconstructions median error ranges between 5.64 cm - 9.24 cm with standard deviations between 4.57 cm - 6.08 cm.}
}

@inproceedings{rao2016icra,
  author    = {D. Rao and A. Bender and S.B. Williams and O. Pizarro},
  title     = {{Multimodal information-theoretic measures for autonomous exploration}},
  booktitle = icra,
  year      = 2016,
  keywords  = {Visual Learning, Reactive and Sensor-Based Planning, Marine Robotics},
  abstract  = {Autonomous underwater vehicles (AUVs) are widely used to perform information gathering missions in unseen environments. Given the sheer size of the ocean environment, and the time and energy constraints of an AUV, it is important to consider the potential utility of candidate missions when performing survey planning. In this paper, we utilise a multimodal learning approach to capture the relationship between in-situ visual observations, and shipborne bathymetry (ocean depth) data that are freely available a priori. We then derive information-theoretic measures under this model that predict the amount of visual information gain at an unobserved location based on the bathymetric features. Unlike previous approaches, these measures consider the value of additional visual features, rather than just the habitat labels obtained. Experimental results with a toy dataset and real marine data demonstrate that the approach can be used to predict the true utility of unexplored areas.}
}

@inproceedings{held2016icra,
  author    = {D. Held and S. Thrun and S. Savarese},
  title     = {{Robust Single-View Instance Recognition}},
  booktitle = icra,
  year      = 2016,
  keywords  = {Visual Learning, Computer Vision for Other Robotic Applications, Recognition},
  abstract  = {Some robots must repeatedly interact with a fixed set of objects in their environment. To operate correctly, it is helpful for the robot to be able to recognize the object instances that it repeatedly encounters. However, current methods for recognizing object instances require that, during training, many pictures are taken of each object from a large number of viewing angles. This procedure is slow and requires much manual effort before the robot can begin to operate in a new environment. We have developed a novel procedure for training a neural network to recognize a set of objects from just a single training image per object. To obtain robustness to changes in viewpoint, we take advantage of a supplementary dataset in which we observe a separate (non-overlapping) set of objects from multiple viewpoints. After pre-training the network in a novel multi-stage fashion, the network can robustly recognize new object instances given just a single training image of each object. If more images of each object are available, the performance improves. We perform a thorough analysis comparing our novel training procedure to traditional neural network pre-training techniques as well as previous stateof-the-art approaches including keypoint-matching, templatematching, and sparse coding, and we demonstrate that our method significantly outperforms these previous approaches. Our method can thus be used to easily teach a robot to recognize a novel set of object instances from unknown viewpoints.}
}

@inproceedings{velas2016icra,
  author    = {M. Velas and M. Spanel and A. Herout},
  title     = {{Collar Line Segments for Fast Odometry Estimation from Velodyne Point Clouds}},
  booktitle = icra,
  year      = 2016,
  keywords  = {Range Sensing, Mapping, SLAM},
  abstract  = {We present a novel way of odometry estimation from Velodyne LiDAR point cloud scans. The aim of our work is to overcome the most painful issues of Velodyne data the sparsity and the quantity of data points in an efficient way, enabling more precise registration. Alignment of the point clouds which yields the final odometry is based on random sampling of the clouds using Collar Line Segments (CLS). The closest line segment pairs are identified in two sets of line segments obtained from two consequent Velodyne scans. From each pair of correspondences, a transformation aligning the matched line segments into a 3D plane is estimated. By this, significant planes (ground, walls, . . . ) are preserved among aligned point clouds. Evaluation using the KITTI dataset shows that our method outperforms publicly available and commonly used state-of-the-art method GICP for point cloud registration in both accuracy and speed, especially in cases where the scene lacks significant landmarks or in typical urban elements. For such environments, the registration error of our method is reduced by 75\% compared to the original GICP error.}
}

@inproceedings{greene2016icra,
  author    = {W.N. Greene and K. Ok and P. Lommel and N. Roy},
  title     = {{Multi-Level Mapping: Real-Time Dense Monocular SLAM}},
  booktitle = icra,
  year      = 2016,
  keywords  = {SLAM, Visual-Based Navigation, RGB-D Perception},
  abstract  = {We present a method for Simultaneous Localization and Mapping (SLAM) using a monocular camera that is capable of reconstructing dense 3D geometry online without the aid of a graphics processing unit (GPU). Our key contribution is a multi-resolution depth estimation and spatial smoothing process that exploits the correlation between low-texture image regions and simple planar structure to adaptively scale the complexity of the generated keyframe depthmaps to the texture of the input imagery. High-texture image regions are represented at higher resolutions to capture fine detail, while low-texture regions are represented at coarser resolutions for smooth surfaces. The computational savings enabled by this approach allow for significantly increased reconstruction density and quality when compared to the state-ofthe-art. The increased depthmap density also improves tracking performance as more constraints can contribute to the pose estimation. A video of experimental results is available at http:// groups.csail.mit.edu/rrg/multi_level_mapping.}
}

@inproceedings{mentges2016icra,
  author    = {G. Mentges and R. Grigat},
  title     = {{Surface Reconstruction from Image Space Adjacency of Lines Using Breadth-First Plane Search}},
  booktitle = icra,
  year      = 2016,
  keywords  = {Mapping, Computer Vision for Automation},
  abstract  = {In this paper, we propose a novel multi-view method for surface reconstruction from matched line segments with applications to robotic mapping and image-based rendering. Starting from 3D line segments and poses obtained via Line-SLAM, we project segments from multiple frames into keyframes for image-space analysis. For each keyframe, a grid of image faces is created by optimized intersection of the segment projection lines. These faces define a segment adjacency graph, wherein we perform our Breadth-First Plane Search (BFPS). The found plane hypotheses are merged maximally with respect to a structure-preserving criterion by growing coplanar regions across the graph. Hypotheses violating the visibility constraint are discarded based on fast per-face and mostly nongeometrical evaluation of the scene and image graph. Finally, each image face gets back-projected onto an optimal plane to obtain a 3D surface model. The presented system is a complete and automatic solution suitable for mapping an environment in real-time scenarios like robotic exploration. We demonstrate the performance of our algorithm on several indoor scenes of varying complexity. Compared to a pure 3D analysis of segments, we see a speed-up by one to almost two orders of magnitude, while still improving on reconstruction accuracy.}
}

@inproceedings{chen2016icra,
  author    = {Y. Chen and M. Liu and J.P. How},
  title     = {{Augmented Dictionary Learning for Sparse Representations of Trajectories with Application to Motion Prediction}},
  booktitle = icra,
  year      = 2016,
  keywords  = {Autonomous Vehicle Navigation, Learning and Adaptive Systems, Robot Safety},
  abstract  = {Developing accurate models and efficient representations of multivariate trajectories is important for understanding the behavior patterns of mobile agents. This work presents a dictionary learning algorithm for developing a partbased trajectory representation, which combines merits of the existing Markovian-based and clustering-based approaches. In particular, this work presents the augmented semi-nonnegative sparse coding (ASNSC) algorithm for solving a constrained dictionary learning problem, and shows that the proposed method would converge to a local optimum given a convexity condition. We consider a trajectory modeling application, in which the learned dictionary atoms correspond to local motion patterns. Classical semi-nonnegative sparse coding approaches would add dictionary atoms with opposite signs to reduce the representational error, which can lead to learning noisy dictionary atoms that correspond poorly to local motion patterns. ASNSC addresses this problem and learns a concise set of intuitive motion patterns. ASNSC shows significant improvement over existing trajectory modeling methods in both prediction accuracy and computational time, as revealed by extensive numerical analysis on real datasets.}
}

@inproceedings{lee2016icra,
  author    = {S.U. Lee and R. GONZALEZ and K. Iagnemma},
  title     = {{Robust Sampling-Based Motion Planning for Autonomous Tracked Vehicles in Deformable High Slip Terrain}},
  booktitle = icra,
  year      = 2016,
  keywords  = {Motion and Path Planning, Autonomous Vehicle Navigation, Field Robots},
  abstract  = {This paper presents an optimal global planner for autonomous tracked vehicles navigating in off-road terrain with uncertain slip, which affects the vehicle as a process noise. This paper incorporates two fields of study: slip estimation and motion planning. For slip estimation, an experimental result from [9] is used to model the effect of the slip on the vehicle in various soil types. For motion planning, a robust incremental sampling based motion planning algorithm (CC-RRT*) is combined with the LQG-MP algorithm. CC-RRT* yields the optimal and probabilistically feasible trajectory by using a chance constrained approach under the RRT* framework. LQG-MP provides the capability of considering the role of compensator in the motion planning phase and bounds the degree of uncertainty to appropriate size. In simulation, the planner successfully finds the optimal and robust solution. In addition, the planner is compared with an RRT* algorithm with dilated obstacles to show that it avoids being overly conservative.}
}

@inproceedings{beksi2016icra,
  author    = {W. Beksi and N. Papanikolopoulos},
  title     = {{3D Point Cloud Segmentation Using Topological Persistence}},
  booktitle = icra,
  year      = 2016,
  keywords  = {RGB-D Perception, Object detection, Segmentation, Categorization, Computational Geometry},
  abstract  = {In this paper, we present an approach to segment 3D point cloud data using ideas from persistent homology theory. The proposed algorithms first generate a simplicial complex representation of the point cloud dataset. Next, we compute the zeroth homology group of the complex which corresponds to the number of connected components. Finally, we extract the clusters of each connected component in the dataset. We show that this technique has several advantages over state of the art methods such as the ability to provide a stable segmentation of point cloud data under noisy or poor sampling conditions and its independence of a fixed distance metric.}
}

@inproceedings{lottes2016icra,
  author    = {P. Lottes and M. H\"oferlin and S. Sander and M. M\"uter and P. Schulze-Lammers and C. Stachniss},
  title     = {{An Effective Classification System for Separating Sugar Beets and Weeds for Precision Farming Applications}},
  booktitle = icra,
  year      = 2016,
  keywords  = {Robotics in Agriculture and Forestry, Semantic Scene Understanding},
  abstract  = {Robots for precision farming have the potential to reduce the reliance on herbicides and pesticides through selectively spraying individual plants or through manual weed removal. To achieve this, the value crops and the weeds must be identified by the robots perception system to trigger the actuators for spraying or removal. In this paper, we address the problem of detecting the sugar beet plants as well as weeds using a camera installed on a mobile robot operating on a field. We propose a system that performs vegetation detection, feature extraction, random forest classification, and smoothing through a Markov random field to obtain an accurate estimate of the crops and weeds. We implemented and thoroughly evaluated our system on a real farm robot on different sugar beet fields and illustrate that our approach allows for accurately identifying the weed on the field.},
  url       = {http://flourish-project.eu/fileadmin/user_upload/publications/lottes16icra.pdf}
}

@inproceedings{corcoran2016icra,
  author    = {P. Corcoran and G. Huang and P. Mooney},
  title     = {{Unsupervised Trajectory Compression}},
  booktitle = icra,
  year      = 2016,
  keywords  = {Intelligent Transportation Systems, Motion and Path Planning},
  abstract  = {We present a method for compressing trajectories in an unsupervised manner. Given a set of trajectories sampled from a space we construct a basis for compression whose elements correspond to paths in the space which are topologically distinct. This is achieved by computing a canonical representative for each element in a generating set for the first homology group and decomposing these representatives into a set of distinct paths. Trajectory compression is subsequently accomplished through representation in terms of this basis. Robustness with respect to outliers is achieved by only considering those elements of the first homology group which exist in the super-level sets of the Kernel Density Estimation (KDE) above a threshold. Robustness with respect to small scale topological artifacts is achieved by only considering those elements of the first homology group which exist for a sufficient range in the super-level sets. We demonstrate this approach to trajectory compression in the context of a large set of crowd-sourced GPS trajectories captured in the city of Chicago. On this set, the compression method achieves a mean geometrical accuracy of 108 meters with a compression ratio of over 12.}
}

@inproceedings{martn-martn2016icra,
  author    = {R. Mart{\'i}n-Mart{\'i}n and S. H{\"o}fer and O. Brock},
  title     = {{An Integrated Approach to Visual Perception of Articulated Objects}},
  booktitle = icra,
  year      = 2016,
  keywords  = {RGB-D Perception, Visual Tracking, Object detection, Segmentation, Categorization},
  abstract  = {We present an integrated approach for perception of unknown articulated objects. To robustly perceive objects and understand interactions, our method tightly integrates pose tracking, shape reconstruction, and the estimation of their kinematic structure. The key insight of our method is that these sub-problems complement each other: for example, tracking is greatly facilitated by knowing the shape of the object, whereas the shape and the kinematic structure can be more easily reconstructed if the motion of the object is known. Our combined method leverages these synergies to improve the performance of perception. We analyze the proposed method in average cases and difficult scenarios using a variety of rigid and articulated objects. The results show that our integrated solution achieves better results than solutions for the individual problems. This demonstrates the benefits of approaching robot perception problems in an integrated manner.}
}

@inproceedings{duggal2016icra,
  author    = {V. Duggal and M. Sukhwani and K. Bipin and M. Krishna and S.R. Gunnamreddy},
  title     = {{Plantation Monitoring and Yield Estimation Using Autonomous Quadcopter for Precision Agriculture}},
  booktitle = icra,
  year      = 2016,
  keywords  = {Robotics in Agriculture and Forestry, Agricultural Automation},
  abstract  = {Recently, quadcopters with their advance sensors and imaging capabilities have become an imperative part of the precision agriculture. In this work, we have described a framework which performs plantation monitoring and yield estimation using the supervised learning approach, while autonomously navigating through an inter-row path of the plantation. The proposed navigation framework assists the quadcopter to follow a sequence of collision-free GPS way points and has been integrated with ROS (Robot Operating System). The trajectory planning and control module of the navigation framework employ convex programming techniques to generate minimum time trajectory between way-points and produces appropriate control inputs for the quadcopter. A new pomegranate dataset comprising of plantation surveillance video and annotated frames capturing the varied stages of pomegranate growth along with the navigation framework are being delivered as a part of this work.}
}

@inproceedings{narr2016icra,
  author    = {A. Narr and R. Triebel and D. Cremers},
  title     = {{Stream-Based Active Learning for Efficient and Adaptive Classification of 3D Objects}},
  booktitle = icra,
  year      = 2016,
  keywords  = {Learning and Adaptive Systems, Object detection, Segmentation, Categorization, Semantic Scene Understanding},
  abstract  = {We present a new Active Learning approach for classifying objects from streams of 3D point cloud data. The major problems here are the non-uniform occurence of class instances and the unbalanced numbers of samples per class. We show that standard online learning methods based on decision trees perform comparably bad for such data streams, which are however particularly relevant for mobile robots that need to learn semantics persistently. To address this, we use Mondrian forests (MF), a recent online learning algorithm that is independent on the data order. We present an extension of that algorithm and show that MF are less overconfident than standard Random Forests. In experiments on the KITTI benchmark, we show that this leads to a substantially improved classification performance for data streams, rendering our approach very attractive for lifelong robot learning applications. Fig. 1. Example situation from the KITTI benchmark data set with a car, a cyclist and some pedestrians. The 3D point cloud data of this frame was classified after actively learning semantics from a stream of 10 000 previous samples. The bottom left image shows the result from actively learning an online Random Forest classifier, the right one shows the result using a modified Mondrian forest instead. A green bounding box refers to a correct classification, while red boxes are wrong predictions. As we show in this paper, the better performance of Mondrian forests comes from their higher capability to learn from streams of data.}
}

@inproceedings{viswanathan2016icra,
  author    = {A. Viswanathan and B. Pires and D. Huber},
  title     = {{Vision-Based Robot Localization across Seasons and in Remote Locations}},
  booktitle = icra,
  year      = 2016,
  keywords  = {Localization, Field Robots, Computer Vision for Other Robotic Applications},
  abstract  = {This paper studies the problem of GPS-denied unmanned ground vehicle (UGV) localization by matching ground images to a satellite map. We examine the realistic, but particularly challenging problem of navigation in remote areas using maps that may correspond to a different season of the year. The problem is difficult due to the limited UGV sensor horizon, the drastic shift in perspective between ground and aerial views, the absence of discriminative features in the environment due to the remote location, and the high variation in appearance of the satellite map caused by the change in seasons. We present an approach to image matching using semantic information that is invariant to seasonal change. This semantics-based matching is incorporated into a particle filter framework and successful localization of the ground vehicle is demonstrated for satellite maps captured in summer, spring, and winter.}
}

@inproceedings{fehr2016icra,
  author    = {M. Fehr and M.T. Dymczyk and S. Lynen and R. Siegwart},
  title     = {{Reshaping Our Model of the World Over Time}},
  booktitle = icra,
  year      = 2016,
  keywords  = {Computer Vision for Other Robotic Applications, Mapping, Visual-Based Navigation},
  abstract  = {An accurate estimate of the 3D-structure in the environment is key to robotic applications such as autonomous inspection, obstacle avoidance and manipulation. Recent years have seen substantial algorithmic advances towards creating highly accurate models of small objects as well as large scale architectural structures. Most commonly a rich set of images covering a static scene are used to jointly estimate the pose of the cameras and the observed 3D-structure. For many practical application however the assumption of static scenes and sufficient coverage by images does not hold. In fact for industrial inspection the change in the scene is of most interest and the limited resources on mobile platforms dont allow for extensive data captures. In this paper we investigate the potential of combining multiple independent captures of a place to selectively reconstruct a scene over time. We propose an incremental reconstruction algorithm which identifies and fuses novel data into a joint model of the scene. Being able to identify changing parts of the scene is particularly interesting for mobile applications where bandwidth, storage and processing power are limited. Through detailed experiments, we show the potential of our approach to use multiple mobile devices to reconstruct and update a model of the static part of the environment over time.}
}

@inproceedings{cieslewski2016icra,
  author    = {T. Cieslewski and E. Stumm and A.R. Gawel and M. Bosse and S. Lynen and R. Siegwart},
  title     = {{Point Cloud Descriptors for Place Recognition Using Sparse Visual Information}},
  booktitle = icra,
  year      = 2016,
  keywords  = {Localization, Visual-Based Navigation, SLAM},
  abstract  = {Place recognition is a core component in simultaneous localization and mapping (SLAM), limiting positional drift over space and time to unlock precise robot navigation. Determining which previously visited places belong together continues to be a highly active area of research as robotic applications demand increasingly higher accuracies. A large number of place recognition algorithms have been proposed, capable of consuming a variety of sensor data including laser, sonar and depth readings. The best performing solutions, however, have utilized visual information by either matching entire images or parts thereof. Most commonly, vision based approaches are inspired by information retrieval and utilize 3D-geometry information about the observed scene as a postverification step. In this paper we propose to use the 3D-scene information from sparse-visual feature maps directly at the core of the place recognition pipeline. We propose a novel structural descriptor which aggregates sparse triangulated landmarks from SLAM into a compact signature. The resulting 3Dfeatures provide a discriminative fingerprint to recognize places over seasonal and viewpoint changes which are particularly challenging for approaches based on sparse visual descriptors. We evaluate our system on publicly available datasets and show how its complementary nature can provide an improvement over visual place recognition.}
}

@inproceedings{handa2016icra,
  author    = {A. Handa and V. Patraucean and S. Stent and R. Cipolla},
  title     = {{SceneNet: An Annotated Model Generator for Indoor Scene Understanding}},
  booktitle = icra,
  year      = 2016,
  keywords  = {Semantic Scene Understanding, RGB-D Perception, Simulation and Animation},
  abstract  = {We introduce SceneNet, a framework for generating high-quality annotated 3D scenes to aid indoor scene understanding. SceneNet leverages manually-annotated datasets of real world scenes such as NYUv2 to learn statistics about object co-occurrences and their spatial relationships. Using a hierarchical simulated annealing optimisation, these statistics are exploited to generate a potentially unlimited number of new annotated scenes, by sampling objects from various existing databases of 3D objects such as ModelNet, and textures such as OpenSurfaces and ArchiveTextures. Depending on the task, SceneNet can be used directly in the form of annotated 3D models for supervised training and 3D reconstruction benchmarking, or in the form of rendered annotated sequences of RGB-D frames or videos.}
}

@inproceedings{mukadam2016icra,
  author    = {M. Mukadam and X. Yan and B. Boots},
  title     = {{Gaussian Process Motion Planning}},
  booktitle = icra,
  year      = 2016,
  keywords  = {Motion and Path Planning, Probability and Statistical Methods},
  abstract  = {Motion planning is a fundamental tool in robotics, used to generate collision-free, smooth, trajectories, while satisfying task-dependent constraints. In this paper, we present a novel approach to motion planning using Gaussian processes. In contrast to most existing trajectory optimization algorithms, which rely on a discrete state parameterization in practice, we represent the continuous-time trajectory as a sample from a Gaussian process (GP) generated by a linear time-varying stochastic differential equation. We then provide a gradientbased optimization technique that optimizes continuous-time trajectories with respect to a cost functional. By exploiting GP interpolation, we develop the Gaussian Process Motion Planner (GPMP), that finds optimal trajectories parameterized by a small number of states. We benchmark our algorithm against recent trajectory optimization algorithms by solving 7-DOF robotic arm planning problems in simulation and validate our approach on a real 7-DOF WAM arm.}
}

@inproceedings{pokorny2016icra,
  author    = {F.T. Pokorny and K. Goldberg and D. Kragic},
  title     = {{Topological Trajectory Clustering with Relative Persistent Homology}},
  booktitle = icra,
  year      = 2016,
  keywords  = {Computational Geometry, Motion and Path Planning, Learning and Adaptive Systems},
  abstract  = {Cloud Robotics techniques based on Learning from Demonstrations suggest promising alternatives to manual programming of robots and autonomous vehicles. One challenge is that demonstrated trajectories may vary dramatically: it can be very difficult, if not impossible, for a system to learn control policies unless the trajectories are clustered into meaningful consistent subsets. Metric clustering methods, based on a distance measure, require quadratic time to compute a pairwise distance matrix and do not naturally distinguish topologically distinct trajectories. This paper presents an algorithm for topological clustering based on relative persistent homology, which, for a fixed underlying simplicial representation and discretization of trajectories, requires only linear time in the number of trajectories. The algorithm incorporates global constraints formalized in terms of the topology of sublevel or superlevel sets of a function and can be extended to incorporate probabilistic motion models. In experiments with real automobile and ship GPS trajectories as well as pedestrian trajectories extracted from video, the algorithm clusters trajectories into meaningful consistent subsets and, as we show in an experiment with ship trajectories, results in a faster and more efficient clustering than a metric clustering by Frechet distance.}
}

@inproceedings{wang2016icra,
  author    = {R. Wang and M. Veloso and S. Seshan},
  title     = {{Active Sensing Data Collection with Autonomous Mobile Robots}},
  booktitle = icra,
  year      = 2016,
  keywords  = {Reactive and Sensor-Based Planning, Automation Technologies for Smart Cities, Surveillance Systems},
  abstract  = {With the introduction of autonomous robots that help perform various tasks in our environments, we can opportunistically use them for collecting fine-grain sensor measurements about our surroundings. Use of mobile robots for data collection scales much better than static sensors in terms of number of measurement locations and provide more fine-grain accuracy and reliability than alternate human crowd-sourcing efforts. One of the unique features of mobile robots is the ability to control and direct where and when measurements should be collected. In this paper, we present a system to compute paths for the robot to follow that incorporates the robots limited expected deployment time, expected measurement value at each location, and a history of when each location was last visited.}
}

@inproceedings{zhang2016icra-lbfo,
  author    = {G. Zhang and J.M. Lilly and P. Vela},
  title     = {{Learning Binary Features Online from Motion Dynamics for Incremental Loop-Closure Detection and Place Recognition}},
  booktitle = icra,
  year      = 2016,
  keywords  = {Visual-Based Navigation, Recognition, SLAM},
  abstract  = {This paper proposes a simple yet effective approach to learn visual features online for improving loopclosure detection and place recognition, based on bag-of-words frameworks. The approach learns a codeword in the bagof-words model from a pair of matched features from two consecutive frames, such that the codeword has temporallyderived perspective invariance to camera motion. The learning algorithm is efficient: the binary descriptor is generated from the mean image patch, and the mask is learned based on discriminative projection by minimizing the intra-class distances among the learned feature and the two original features. A codeword is generated by packaging the learned descriptor and mask, with a masked Hamming distance defined to measure the distance between two codewords. The geometric properties of the learned codewords are then mathematically justified. In addition, hypothesis constraints are imposed through temporal consistency in matched codewords, which improves precision. The approach, integrated in an incremental bag-of-words system, is validated on multiple benchmark data sets and compared to state-of-the-art methods. Experiments demonstrate improved precision/recall outperforming state of the art with little loss in runtime.}
}

@inproceedings{dewan2016icra,
  author    = {A. Dewan and T. Caselitz and G.D. Tipaldi and W. Burgard},
  title     = {{Motion-Based Detection and Tracking in 3D LiDAR Scans}},
  booktitle = icra,
  year      = 2016,
  keywords  = {Range Sensing, Object detection, Segmentation, Categorization, Autonomous Vehicle Navigation},
  abstract  = {Robots are expected to operate autonomously in increasingly complex scenarios such as crowded streets or heavy traffic situations. Perceiving the dynamics of moving objects in the environment is crucial for safe and smart navigation and therefore a key enabler for autonomous driving. In this paper we present a novel model-free approach for detecting and tracking dynamic objects in 3D LiDAR scans obtained by a moving sensor. Our method only relies on motion cues and does not require any prior information about the objects. We sequentially detect multiple motions in the scene and segment objects using a Bayesian approach. For robustly tracking objects, we utilize their estimated motion models. We present extensive quantitative results based on publicly available datasets and show that our approach outperforms the state of the art.}
}

@inproceedings{gomez-ojeda2016icra,
  author    = {R. Gomez-Ojeda and J. Gonzlez-Jimnez},
  title     = {{Robust Stereo Visual Odometry through a Probabilistic Combination of Points and Line Segments}},
  booktitle = icra,
  year      = 2016,
  keywords  = {Autonomous Vehicle Navigation, Computer Vision for Other Robotic Applications, Localization},
  abstract  = {Most approaches to stereo visual odometry reconstruct the motion based on the tracking of point features along a sequence of images. However, in low-textured scenes it is often difficult to encounter a large set of point features, or it may happen that they are not well distributed over the image, so that the behavior of these algorithms deteriorates. This paper proposes a probabilistic approach to stereo visual odometry based on the combination of both point and line segment that works robustly in a wide variety of scenarios. The camera motion is recovered through non-linear minimization of the projection errors of both point and line segment features. In order to effectively combine both types of features, their associated errors are weighted according to their covariance matrices, computed from the propagation of Gaussian distribution errors in the sensor measurements. The method, of course, is computationally more expensive that using only one type of feature, but still can run in real-time on a standard computer and provides interesting advantages, including a straightforward integration into any probabilistic framework commonly employed in mobile robotics.}
}

@inproceedings{bose2016icra,
  author    = {L. Bose and A. Richards},
  title     = {{Fast Depth Edge Detection and Edge Based RGB-D SLAM}},
  booktitle = icra,
  year      = 2016,
  keywords  = {SLAM, Mapping, Visual Tracking},
  abstract  = {This paper presents a method of occluding depth edge-detection targeted towards RGB-D video streams and explores the use of these and other edge features in RGB-D SLAM. The proposed depth edge-detection approach uses prior information obtained from the previous RGB-D video frame to determine which areas of the current depth image are likely to contain edges due to image similarity. By limiting the search for edges to these areas a significant amount of computation time is saved compared to searching the entire image. Pixels belonging to both the depth and colour edges of an RGB-D image can be back projected using the depth component to form 3D point clouds of edge points. Registration between such edge point clouds is achieved using ICP and we present a realtime RGB-D SLAM system utilizing such back projected edge features. Experimental results are presented demonstrating the performance of both the proposed depth edge-detection and SLAM system using publicly available datasets.}
}

@inproceedings{jaimez2016icra,
  author    = {M. Jaimez and J.G. Monroy and J. Gonzlez-Jimnez},
  title     = {{Planar Odometry from a Radial Laser Scanner. A Range Flow-Based Approach}},
  booktitle = icra,
  year      = 2016,
  keywords  = {Range Sensing, Localization},
  abstract  = {In this paper we present a fast and precise method to estimate the planar motion of a lidar from consecutive range scans. For every scanned point we formulate the range flow constraint equation in terms of the sensor velocity, and minimize a robust function of the resulting geometric constraints to obtain the motion estimate. Conversely to traditional approaches, this method does not search for correspondences but performs dense scan alignment based on the scan gradients, in the fashion of dense 3D visual odometry. The minimization problem is solved in a coarse-to-fine scheme to cope with large displacements, and a smooth filter based on the covariance of the estimate is employed to handle uncertainty in unconstraint scenarios (e.g. corridors). Simulated and real experiments have been performed to compare our approach with two prominent scan matchers and with wheel odometry. Quantitative and qualitative results demonstrate the superior performance of our approach which, along with its very low computational cost (0.9 milliseconds on a single CPU core), makes it suitable for those robotic applications that require planar odometry. For this purpose, we also provide the code so that the robotics community can benefit from it.}
}

@inproceedings{ma2016icra,
  author    = {L. Ma and C. Kerl and J. St{\"u}ckler and D. Cremers},
  title     = {{CPA SLAM: Consistent Plane-Model Alignment for Direct RGB-D SLAM}},
  booktitle = icra,
  year      = 2016,
  keywords  = {SLAM, RGB-D Perception, Semantic Scene Understanding},
  abstract  = {Planes are predominant features of man-made environments which have been exploited in many mapping approaches. In this paper, we propose a real-time capable RGB-D SLAM system that consistently integrates frame-tokeyframe and frame-to-plane alignment. Our method models the environment with a global plane model and besides direct image alignment it uses the planes for tracking and global graph optimization. This way, our method makes use of the dense image information available in keyframes for accurate short-term tracking. At the same time it uses a global model to reduce drift. Both components are integrated consistently in an expectation-maximization framework. In experiments, we demonstrate the benefits our approach and its state-of-the-art accuracy on challenging benchmarks.}
}

@inproceedings{mount2016icra,
  author    = {J. Mount and M.J. Milford},
  title     = {{2D Visual Place Recognition for Domestic Service Robots at Night}},
  booktitle = icra,
  year      = 2016,
  keywords  = {Localization, Service Robots, Visual-Based Navigation},
  abstract  = {Domestic service robots such as lawn mowing and vacuum cleaning robots are the most numerous consumer robots in existence today. While early versions employed random exploration, recent systems fielded by most of the major manufacturers have utilized range-based and visual sensors and user-placed beacons to enable robots to map and localize. However, active range and visual sensing solutions have the disadvantages of being intrusive, expensive, or only providing a 1D scan of the environment, while the requirement for beacon placement imposes other practical limitations. In this paper we present a passive and potentially cheap vision-based solution to 2D localization at night that combines easily obtainable day-time maps with low resolution contrast-normalized image matching algorithms, image sequence-based matching in two-dimensions, place match interpolation and recent advances in conventional low light camera technology. In a range of experiments over a domestic lawn and in a lounge room, we demonstrate that the proposed approach enables 2D localization at night, and analyse the effect on performance of varying odometry noise levels, place match interpolation and sequence matching length. Finally we benchmark the new low light camera technology and show how it can enable robust place recognition even in an environment lit only by a moonless sky, raising the tantalizing possibility of being able to apply all conventional vision algorithms, even in the darkest of nights.}
}

@inproceedings{radwan2016icra,
  author    = {N. Radwan and G.D. Tipaldi and L. Spinello and W. Burgard},
  title     = {{Do You See the Bakery? Leveraging Geo-Referenced Texts for Global Localization in Public Maps}},
  booktitle = icra,
  year      = 2016,
  keywords  = {Localization, Visual-Based Navigation, Recognition},
  abstract  = {Text is one of the richest sources of information in an urban environment. Although textual information is heavily relied on by humans for a majority of the daily tasks, its usage has not been completely exploited in the field of robotics. In this work, we propose a localization approach utilizing textual features in urban environments. Starting at an unknown location, equipped with an RGB-camera and a compass, our approach uses off-the-shelf text extraction methods to identify text labels in the vicinity. We then apply a probabilistic localization approach with specific sensor models to integrate multiple observations. An extensive evaluation with real-world data gathered in different cities reveals an improvement over GPS-based localization when using our method.}
}

@inproceedings{wojke2016icra,
  author    = {N. Wojke and D. Paulus},
  title     = {{Global Data Association for the Probability Hypothesis Density Filter Using Network Flows}},
  booktitle = icra,
  year      = 2016,
  keywords  = {Visual Tracking, Human detection & tracking, RGB-D Perception},
  abstract  = {The Probability Hypothesis Density (PHD) filter is an efficient formulation of multi-target state estimation that circumvents the combinatorial explosion of the multitarget posterior by operating on single-target space without maintaining target identities. In this paper, we propose a multi-target tracker based on the PHD filter that provides instantaneous state estimation and delayed decision on data association. For this purpose, we reformulate the PHD recursion in terms of single-target track hypotheses and solve a mincost flow network for trajectory estimation where measurement likelihoods and transition probabilities are based on multitarget state estimates. In this manner, the presented approach combines global data association with efficient multi-target filtering. We evaluate the approach on a publicly available pedestrian tracking dataset to present state estimation and data association capabilities.}
}

@inproceedings{chen2016icra-pfrr,
  author    = {M. Chen and E. Frazzoli and D. Hsu and W.S. Lee},
  title     = {{POMDP-lite for Robust Robot Planning under Uncertainty}},
  booktitle = icra,
  year      = 2016,
  keywords  = {Motion and Path Planning, AI Reasoning Methods},
  abstract  = {The partially observable Markov decision process (POMDP) provides a principled general model for planning under uncertainty. However, solving a general POMDP is computationally intractable in the worst case. This paper introduces POMDP-lite, a subclass of POMDPs in which the hidden state variables are constant or only change deterministically. We show that a POMDP-lite is equivalent to a set of fully observable Markov decision processes indexed by a hidden parameter and is useful for modeling a variety of interesting robotic tasks. We develop a simple model-based Bayesian reinforcement learning algorithm to solve POMDP-lite models. The algorithm performs well on large-scale POMDP-lite models with up to 1020 states and outperforms the state-of-the-art general-purpose POMDP algorithms. We further show that the algorithm is near-Bayesian-optimal under suitable conditions.}
}

@inproceedings{mendes2016icra,
  author    = {C.C.T. Mendes and V. Fremont and D.F. Wolf},
  title     = {{Exploiting Fully Convolutional Neural Networks for Fast Road Detection}},
  booktitle = icra,
  year      = 2016,
  keywords  = {Semantic Scene Understanding, Computer Vision for Transportation, Visual-Based Navigation},
  abstract  = {Road detection is a crucial task in autonomous navigation systems. It is responsible for delimiting the road area and hence the free and valid space for maneuvers. In this paper, we consider the visual road detection problem where, given an image, the objective is to classify every of its pixels into road or non-road. We address this task by proposing a convolutional neural network architecture. We are especially interested in a model that takes advantage of a large contextual window while maintaining a fast inference. We achieve this by using a Network-in-Network (NiN) architecture and by converting the model into a fully convolutional network after training. Experiments have been conducted to evaluate the effects of different contextual window sizes (the amount of contextual information) and also to evaluate the NiN aspect of the proposed architecture. Finally, we evaluated our approach using the KITTI road detection benchmark achieving results in line with other state-of-the-art methods while maintaining real-time inference. The benchmark results also reveal that the inference time of our approach is unique at this level of accuracy, being two orders of magnitude faster than other methods with similar performance.}
}

@inproceedings{yang2016icra,
  author    = {S. Yang and D. Maturana and S. Scherer},
  title     = {{Real-Time 3D Scene Layout from a Single Image Using Convolutional Neural Networks}},
  booktitle = icra,
  year      = 2016,
  keywords  = {Semantic Scene Understanding, Visual Learning, Autonomous Vehicle Navigation, CNN},
  abstract  = {We consider the problem of understanding the 3D layout of indoor corridor scenes from a single image in real time. Identifying obstacles such as walls is essential for robot navigation, but also challenging due to the diversity in structure, appearance and illumination of real-world corridor scenes. Many current single-image methods make Manhattanworld assumptions, and break down in environments that do not meet this mold. They also may require complicated handdesigned features for image segmentation or clear boundaries to form certain building models. In addition, most cannot run in real time. In this paper, we propose to combine machine learning with geometric modelling to build a simplified 3D model from a single image. We first employ a supervised Convolutional Neural Network (CNN) to provide a dense, but coarse, geometric class labelling of the scene. We then refine this labelling with a fully connected Conditional Random Field (CRF). Finally, we fit line segments along wall-ground boundaries and pop up a 3D model using geometric constraints. We assemble a dataset of 967 labelled corridor images. Our experiments on this dataset and another publicly available dataset show our method outperforms other single image scene understanding methods in pixelwise accuracy while labelling images at over 15 Hz.}
}

@inproceedings{sun2016icra,
  author    = {Y. Sun and D. Fox},
  title     = {{NEOL: Toward Never-Ending Object Learning for Robots}},
  booktitle = icra,
  year      = 2016,
  keywords  = {Recognition, Cognitive Human-Robot Interaction, Learning and Adaptive Systems},
  abstract  = {Learning to recognize objects based on names is a crucial capability for personal robots. Recent recognition methods successfully learn to recognize objects in a train-oncethen-test setting. Yet, these methods do not apply readily to robotic settings, where a robot might continuously encounter new objects and new names. In this work, we present a framework for Never-Ending Object Learning (NEOL). Our framework automatically learns to organize object names into a semantic hierarchy using crowdsourcing and background knowledge bases. It then uses the hierarchy to improve the consistency and efficiency of annotating objects. It also adapts information from additional image datasets to learn object classifiers from a very small number of training examples. We present experiments to test the performance of the adaptation method and demonstrate the full system in a never-ending object learning experiment.}
}

@inproceedings{ghafarianzadeh2016icra,
  author    = {M. Ghafarianzadeh and M. Blaschko and G. Sibley},
  title     = {{Efficient, Dense, Object-Based Segmentation from RGBD Video}},
  booktitle = icra,
  year      = 2016,
  keywords  = {Object detection, Segmentation, Categorization},
  abstract  = {Spatio-temporal cues offer a rich source of information for inferring structural and semantic scene properties. A particularly useful representation in computer vision is a spatiotemporal video segmentation. Together with motion, knowledge of depth can substantially improve superpixel segmentation. In this work we present a novel framework for spatio-temporal segmentation from RGBD video. The method employs both low-level (intensity, color) and high-level (deformable parts model) appearance features. Motion is incorporated through the use of optical flow to construct the temporal connections in the graph Laplacian. Depth cues are incorporated in the similarity metric to provide an informative cue for object boundaries at depth disparities. Nave application of spectral clustering to dense spatio-temporal graphs leads to a high computational cost that is typically addressed through the use of GPUs or computer clusters. By contrast, we build upon a recently proposed Nystrom approximation strategy for spatiotemporal clustering that enables computation on a single core. We further explore structured local connectivity patterns to give high performance at low computational cost. Also we propose a novel context-aware aggregation method that uses a deformable parts model to group the detected parts of the object as a single segment with an accurate boundary. Detailed experiments on the NYU Depth Dataset and TUM RGBD Dataset is performed to compare against previous large-scale graph-based spatiotemporal segmentation techniques which shows the substantial performance advantages of our framework.}
}

@inproceedings{tourani2016icra,
  author    = {S. Tourani and S. Mittal and A. Nagariya and V. Chari and M. Krishna},
  title     = {{Rolling Shutter and Motion Blur Removal for Depth Cameras}},
  booktitle = icra,
  year      = 2016,
  keywords  = {RGB-D Perception},
  abstract  = {Structured light range sensors (SLRS) like the Microsoft Kinect have electronic rolling shutters (ERS). The output of such a sensor while in motion is subject to significant motion blur (MB) and rolling shutter (RS) distortion. Most robotic literature still does not explicitly model this distortion, resulting in inaccurate camera motion estimation. In RGBD cameras, we show via experimentation that the distortion undergone by depth images is different from that of color images and provide a mathematical model for it. We propose an algorithm that rectifies for these RS and MB distortions. To assess the performance of the algorithm we conduct an extensive set of experiments for each step of the pipeline. We assess the performance of our algorithm by comparing the performance of the rectified images on scene-flow and camera pose estimation, and show that with our proposed rectification, the performance improvement is significant.}
}

@inproceedings{schlosser2016icra,
  author    = {J. Schlosser and C. Chow and Z. Kira},
  title     = {{Fusing LIDAR and Images for Pedestrian Detection Using Convolutional Neural Networks}},
  booktitle = icra,
  year      = 2016,
  keywords  = {Visual Learning, Sensor Fusion, Computer Vision for Other Robotic Applications, CNN},
  abstract  = {In this paper, we explore various aspects of fusing LIDAR and color imagery for pedestrian detection in the context of convolutional neural networks (CNNs), which have recently become state-of-art for many vision problems. We incorporate LIDAR by up-sampling the point cloud to a dense depth map and then extracting three features representing different aspects of the 3D scene. We then use those features as extra image channels. Specifically, we leverage recent work on HHA [9] (horizontal disparity, height above ground, and angle) representations, adapting the code to work on up-sampled LIDAR rather than Microsoft Kinect depth maps. We show, for the first time, that such a representation is applicable to up-sampled LIDAR data, despite its sparsity. Since CNNs learn a deep hierarchy of feature representations, we then explore the question: At what level of representation should we fuse this additional information with the original RGB image channels? We use the KITTI pedestrian detection dataset for our exploration. We first replicate the finding that region-CNNs (R-CNNs) [8] can outperform the original proposal mechanism using only RGB images, but only if fine-tuning is employed. Then, we show that: 1) using HHA features and RGB images performs better than RGB-only, even without any fine-tuning using large RGB web data, 2) fusing RGB and HHA achieves the strongest results if done late, but, under a parameter or computational budget, is best done at the early to middle layers of the hierarchical representation, which tend to represent midlevel features rather than low (e.g. edges) or high (e.g. object class decision) level features, 3) some of the less successful methods have the most parameters, indicating that increased classification accuracy is not simply a function of increased capacity in the neural network.}
}

@inproceedings{choudhury2016icra,
  author    = {S. Choudhury and J.D. Gammell and T. Barfoot and S. Srinivasa and S. Scherer},
  title     = {Regionally Accelerated Batch Informed Trees (RABIT*): A Framework to Integrate Local Information into Optimal Path Planning},
  booktitle = icra,
  year      = 2016,
  keywords  = {Motion and Path Planning, Optimization and Optimal Control, Aerial Robotics},
  abstract  = {Sampling-based optimal planners, such as RRT*, almost-surely converge asymptotically to the optimal solution, but have provably slow convergence rates in high dimensions. This is because their commitment to finding the global optimum compels them to prioritize exploration of the entire problem domain even as its size grows exponentially. Optimization techniques, such as CHOMP, have fast convergence on these problems but only to local optima. This is because they are exploitative, prioritizing the immediate improvement of a path even though this may not find the global optimum of nonconvex cost functions. In this paper, we present a hybrid technique that integrates the benefits of both methods into a single search. A key insight is that applying local optimization to a subset of edges likely to improve the solution avoids the prohibitive cost of optimizing every edge in a global search. This is made possible by Batch Informed Trees (BIT*), an informed global technique that orders its search by potential solution quality. In our algorithm, Regionally Accelerated BIT* (RABIT*), we extend BIT* by using optimization to exploit local domain information and find alternative connections for edges in collision and accelerate the search. This improves search performance in problems with difficult-to-sample homotopy classes (e.g., narrow passages) while maintaining almost-sure asymptotic convergence to the global optimum. Our experiments on simulated random worlds and real data from an autonomous helicopter show that on certain difficult problems, RABIT* converges 1.8 times faster than BIT*. Qualitatively, in problems with difficult-to-sample homotopy classes, we show that RABIT* is able to efficiently transform paths to avoid obstacles.}
}

@inproceedings{concha2016icra,
  author    = {A. Concha and G. Loianno and V. Kumar and J. Civera},
  title     = {{Visual-Inertial Direct SLAM}},
  booktitle = icra,
  year      = 2016,
  keywords  = {SLAM, Mapping, Sensor Fusion},
  abstract  = {The so-called direct visual SLAM methods have shown a great potential in estimating a semidense or fully dense reconstruction of the scene, in contrast to the sparse reconstructions of the traditional feature-based algorithms. In this paper, we propose for the first time a direct, tightly-coupled formulation for the combination of visual and inertial data. Our algorithm runs in real-time on a standard CPU. The processing is split in three threads. The first thread runs at frame rate and estimates the camera motion by a joint non-linear optimization from visual and inertial data given a semidense map. The second one creates a semidense map of high-gradient areas only for camera tracking purposes. Finally, the third thread estimates a fully dense reconstruction of the scene at a lower frame rate. We have evaluated our algorithm in several real sequences with ground truth trajectory data, showing a state-of-the-art performance.}
}

@inproceedings{kendall2016icra,
  author    = {A. Kendall and R. Cipolla},
  title     = {{Modelling Uncertainty in Deep Learning for Camera Relocalization}},
  booktitle = icra,
  year      = 2016,
  keywords  = {Localization, SLAM, Probability and Statistical Methods},
  abstract  = {We present a robust and real-time monocular six degree of freedom visual relocalization system. We use a Bayesian convolutional neural network to regress the 6-DOF camera pose from a single RGB image. It is trained in an end-to-end manner with no need of additional engineering or graph optimisation. The algorithm can operate indoors and outdoors in real time, taking under 6ms to compute. It obtains approximately 2m and 6 accuracy for very large scale outdoor scenes and 0.5m and 10 accuracy indoors. Using a Bayesian convolutional neural network implementation we obtain an estimate of the models relocalization uncertainty and improve state of the art localization accuracy on a large scale outdoor dataset. We leverage the uncertainty measure to estimate metric relocalization error and to detect the presence or absence of the scene in the input image. We show that the models uncertainty is caused by images being dissimilar to the training dataset in either pose or appearance.}
}

@inproceedings{vivaldini2016icra,
  author    = {K.C.T. Vivaldini and V. Guizilini and M.D.C. Oliveira and T.H. Martinelli and F. Ramos and D.F. Wolf},
  title     = {{Route Planning for Active Classification with UAVs}},
  booktitle = icra,
  year      = 2016,
  keywords  = {Motion and Path Planning, Agricultural Automation},
  abstract  = {The mapping of agricultural crops by capturing images obtained with UAVs enables fast environmental monitoring and diagnosis in large areas. Airborne monitoring in agriculture can a substantially impacts on the identification of diseases and produce accurate information on affected areas. The problem can be formulated as a classification task on aerial images with significant opportunities to impact other fields. This paper presents an active learning method through route planning for improvements in the knowledge on visited areas and minimization uncertainties about the classification of diseases in crops. Binary Logistic Regression and Gaussian Process were used for the detection of pathologies and map interpolation, respectively. A Bayesian optimization strategy is also proposed for the planning of an informative trajectory, which resulted in a maximized search for affected areas in an initially unknown environment.}
}

@inproceedings{dietrich2016icra,
  author    = {V. Dietrich and D. Chen and K.M. Wurm and G.v. Wichert and P. Ennen},
  title     = {{Probabilistic Multi-Sensor Fusion Based on Signed Distance Functions}},
  booktitle = icra,
  year      = 2016,
  keywords  = {Sensor Fusion, RGB-D Perception, Mapping},
  abstract  = {In this paper, we present an approach for the probabilistic fusion of 3D sensor measurements. Our fusion algorithm is based on truncated signed distance functions. It explicitly considers the measurement noise by modeling the surface using random variables. Furthermore, our proposed surface model provides an explicit estimation of the spatial uncertainty. The approach can be implemented on a GPU to achieve a high update performance and enable online updates of the model. The approach was evaluated in simulation and using real sensor data. In our experiments, we confirmed that it accurately estimates surfaces from noisy sensor data and that it provides a corresponding estimate of the uncertainty. We could also show that the approach is able to fuse measurements from sensors with different noise characteristics.}
}

@inproceedings{hostettler2016icra,
  author    = {L.O. Hostettler and A. {\"O}zg{\"u}r and S. Lemaignan and P. Dillenbourg and F. Mondada},
  title     = {{Real-Time High-Accuracy 2D Localization with Structured Patterns}},
  booktitle = icra,
  year      = 2016,
  keywords  = {Localization},
  abstract  = {Building over algorithms previously developed for digital pens, this article introduces a novel 2D localization technique for mobile robots, based on simple printed patterns. This method combines high absolute accuracy (below 0.3mm), unlimited scalability, low computational requirements (the presented open-source implementation runs at above 45Hz on a low-cost microcontroller) and low cost (below 30 per device at prototype stage). The article first presents the underlying algorithms and localization pipeline. It then describes our reference hardware and software implementations, and finally evaluates the performance of this technique for mobile robots.}
}

@inproceedings{linegar2016icra,
  author    = {C. Linegar and W. Churchill and P. Newman},
  title     = {{Made to Measure: Bespoke Landmarks for 24-Hour, All-Weather Localisation with a Camera}},
  booktitle = icra,
  year      = 2016,
  keywords  = {Visual-Based Navigation, Localization, Computer Vision for Transportation},
  abstract  = {This paper is about camera-only localisation in challenging outdoor environments, where changes in lighting, weather and season cause traditional localisation systems to fail. Conventional approaches to the localisation problem rely on point-features such as SIFT, SURF or BRIEF to associate landmark observations in the live image with landmarks stored in the map; however, these features are brittle to the severe appearance change routinely encountered in outdoor environments. In this paper, we propose an alternative to traditional point-features: we train place-specific linear SVM classifiers to recognise distinctive elements in the environment. The core contribution of this paper is an unsupervised mining algorithm which operates on a single mapping dataset to extract distinct elements from the environment for localisation. We evaluate our system on 205 km of data collected from central Oxford over a period of six months in bright sun, night, rain, snow and at all times of the day. Our experiment consists of a comprehensive N-vs-N analysis on 22 laps of the approximately 10 km route in central Oxford. With our proposed system, the portion of the route where localisation fails is reduced by a factor of 6, from 33.3\% to 5.5\%.}
}

@inproceedings{chhaya2016icra,
  author    = {F. Chhaya and D.R. Narapureddy and S. Upadhyay and V. Chari and M.Z. Zia and M. Krishna},
  title     = {{Monocular Reconstruction of Vehicles: Combining SLAM with Shape Priors}},
  booktitle = icra,
  year      = 2016,
  keywords  = {Semantic Scene Understanding, SLAM, Visual Tracking},
  abstract  = {Reasoning about objects in images and videos using 3D representations is re-emerging as a popular paradigm in computer vision. Specifically, in the context of scene understanding for roads, 3D vehicle detection and tracking from monocular videos still needs a lot of attention to enable practical applications. Current approaches leverage two kinds of information to deal with the vehicle detection and tracking problem: (1) 3D representations (eg. wireframe models or voxel based or CAD models) for diverse vehicle skeletal structures learnt from data, and (2) classifiers trained to detect vehicles or vehicle parts in single images built on top of a basic feature extraction step. In this paper, we propose to extend current approaches in two ways. First, we extend detection to a multiple view setting. We show that leveraging information given by feature or part detectors in multiple images can lead to more accurate detection results than single image detection. Secondly, we show that given multiple images of a vehicle, we can also leverage 3D information from the scene generated using a unique structure from motion algorithm. This helps us localize the vehicle in 3D, and constrain the parameters of optimization for fitting the 3D model to image data. We show results on the KITTI dataset, and demonstrate superior results compared with recent state-of-theart methods, with upto 14.64 \% improvement in localization error.}
}

@inproceedings{bircher2016icra,
  author    = {A. Bircher and M. Kamel and K. Alexis and H. Oleynikova and R. Siegwart},
  title     = {{Receding Horizon "Next-Best-View" Planner for 3D Exploration}},
  booktitle = icra,
  year      = 2016,
  keywords  = {Aerial Robotics, Autonomous Vehicle Navigation, Motion and Path Planning},
  abstract  = {This paper presents a novel path planning algorithm for the autonomous exploration of unknown space using aerial robotic platforms. The proposed planner employs a receding horizon nextbestview scheme: In an online computed random tree it finds the best branch, the quality of which is determined by the amount of unmapped space that can be explored. Only the first edge of this branch is executed at every planning step, while repetition of this procedure leads to complete exploration results. The proposed planner is capable of running online, onboard a robot with limited resources. Its high performance is evaluated in detailed simulation studies as well as in a challenging real world experiment using a rotorcraft micro aerial vehicle. Analysis on the computational complexity of the algorithm is provided and its good scaling properties enable the handling of large scale and complex problem setups.}
}

@inproceedings{alcantarilla2016icra,
  author    = {P.F. Alcantarilla and B. Stenger},
  title     = {{How Many Bits Do I Need for Matching Local Binary Descriptors?}},
  booktitle = icra,
  year      = 2016,
  keywords  = {Visual Learning, Recognition, SLAM},
  abstract  = {In this paper we provide novel insights about the performance and design of popular pairwise tests-based local binary descriptors with the aim of answering the question: How many bits are needed for matching local binary descriptors? We use the interpretation of binary descriptors as a Locality Sensitive Hashing (LSH) scheme for approximating Kendalls tau rank distance between image patches. Based on this understanding we compare local binary descriptors in terms of the number of bits that are required to achieve a certain performance in feature-based matching problems. Furthermore, we introduce a calibration method to automatically determine a suitable number of bits required in an image matching scenario. We provide a performance analysis in image matching and structure from motion benchmarks, showing calibration results in visual odometry and object recognition problems. Our results show that excellent performance can be achieved using a small fraction of the total number of bits from the whole descriptor, speeding-up matching and reducing storage requirements.}
}

@inproceedings{ursic2016icra,
  author    = {P. Ursic and A. Leonardis and D. Skocaj and M. Kristan},
  title     = {{Hierarchical Spatial Model for 2D Range Data Based Room Categorization}},
  booktitle = icra,
  year      = 2016,
  keywords  = {Range Sensing, Service Robots},
  abstract  = {The next generation service robots are expected to co-exist with humans in their homes. Such a mobile robot requires an efficient representation of space, which should be compact and expressive, for effective operation in real-world environments. In this paper we present a novel approach for 2D ground-plan-like laser-range-data-based room categorization that builds on a compositional hierarchical representation of space, and show how an additional abstraction layer, whose parts are formed by merging partial views of the environment followed by graph extraction, can achieve improved categorization performance. A new algorithm is presented that finds a dictionary of exemplar elements from a multi-category set, based on the affinity measure defined among pairs of elements. This algorithm is used for part selection in new layer construction. Room categorization experiments have been performed on a challenging publicly available dataset, which has been extended in this work. State-of-the-art results were obtained by achieving the most balanced performance over all categories.}
}

@inproceedings{otte2016icra,
  author    = {M.W. Otte and W. Silva and E.W. Frew},
  title     = {Any-Time Path-Planning: Time-Varying Wind Field + Moving Obstacles},
  booktitle = icra,
  year      = 2016,
  keywords  = {Motion and Path Planning, Autonomous Vehicle Navigation, Aerial Robotics},
  abstract  = {We consider the problem of real-time pathplanning in a spatiotemporally varying wind-field with moving obstacles. We are provided with changing wind and obstacle predictions along a (D + 1)-dimensional space-time lattice. We present an Any-Time algorithm that quickly finds an -suboptimal solution (a path that is not longer than times the optimal time-length), and then improves and while planning time remains or until new wind/obstacle predictions trigger a restart. The factor comes from an -overestimate of the A*-like cost heuristic. is proportional to motion modeling error. Any-Time performance is achieved by: (1) improving the connectivity model of the environment from a discrete graph to a continuous cost-field (decreasing ); (2) using the established method of incrementally deflating . Our method was deployed as the global planner on a fixed-wing unmanned aircraft system that uses Doppler radar and atmospheric models for online realtime wind sensing and prediction. We compare its performance vs. other state-of-the-art methods in simulated environments.}
}

@inproceedings{zhang2016icra-bolf,
  author    = {Z. Zhang and H. Rebecq and C. Forster and D. Scaramuzza},
  title     = {{Benefit of Large Field-Of-View Cameras for Visual Odometry}},
  booktitle = icra,
  year      = 2016,
  keywords  = {Visual-Based Navigation, Omnidirectional Vision, SLAM},
  abstract  = {The transition of visual-odometry technology from research demonstrators to commercial applications naturally raises the question: what is the optimal camera for visionbased motion estimation? This question is crucial as the choice of camera has a tremendous impact on the robustness and accuracy of the employed visual odometry algorithm. While many properties of a camera (e.g. resolution, frame-rate, globalshutter/rolling-shutter) could be considered, in this work we focus on evaluating the impact of the camera field-of-view (FoV) and optics (i.e., fisheye or catadioptric) on the quality of the motion estimate. Since the motion-estimation performance depends highly on the geometry of the scene and the motion of the camera, we analyze two common operational environments in mobile robotics: an urban environment and an indoor scene. To confirm the theoretical observations, we implement a stateof-the-art VO pipeline that works with large FoV fisheye and catadioptric cameras. We evaluate the proposed VO pipeline in both synthetic and real experiments. The experiments point out that it is advantageous to use a large FoV camera (e.g., fisheye or catadioptric) for indoor scenes and a smaller FoV for urban canyon environments.}
}

@inproceedings{zeisl2016icra,
  author    = {B. Zeisl and M. Pollefeys},
  title     = {{Structure-Based Auto-Calibration of RGB-D Sensors}},
  booktitle = icra,
  year      = 2016,
  keywords  = {RGB-D Perception, Sensor Fusion, Mapping},
  abstract  = {The readily available image and depth data from commodity RGB-D sensors has had tremendous impact in the robotics and computer vision community recently. To jointly leverage both modalities, the depth and image measurements need to be registered. Typical calibration approaches make use of artificial landmarks and special calibration targets. However, this is not feasible if on-line (re-)calibration is necessary or the sensor setup is inaccessible, e.g., for already captured datasets. Instead of using specific calibration patterns, we propose to leverage a sparse environment model as geometric prior for the calibration. Structure-from-motion or SLAM can provide such a sparse 3D scene model, and hence our approach allows for self-calibration without the need for any manual interaction. We validate our hypothesis by introducing an optimization that jointly minimizes the alignment error between the sparse map and all recorded depth maps. Since the accuracy of depth measurements is known to degrade considerably with scene depth, we account for this distortion via a spatially varying correction term. The evaluation of our approach demonstrates that we are able to compute an accurate extrinsic and intrinsic calibration, which for example allows dense 3D modeling at improved precision.}
}

@inproceedings{scott2016icra,
  author    = {T. Scott and A. Morye and P. Pinies and L.M. Paz and I. Posner and P. Newman},
  title     = {{Choosing a Time and Place for Calibration of Lidar-Camera Systems}},
  booktitle = icra,
  year      = 2016,
  keywords  = {Calibration and Identification, Localization, Sensor Networks},
  abstract  = {We propose a calibration method that automatically estimates the extrinsic calibration between a sensor posegraph from natural scenes. The sensor pose-graph represents a system of sensors comprising of lidars and cameras, without sensor co-visibility constraints. The method addresses the fact that each scene contributes differently to the calibration problem by introducing a diligent scene selection scheme. The algorithm searches over all scenes to extract a subset of exemplars, whose joint optimisation yields progressively better calibration estimates. This non-parametric method requires no knowledge of the physical world, and continuously finds scenes that better constrain the optimisation parameters. We explain the theory, implement the method, and provide detailed performance analyses with experiments on real-world data.}
}

@inproceedings{wendel2016icra,
  author    = {A. Wendel and J.P. Underwood},
  title     = {{Self-Supervised Weed Detection in Vegetable Crops Using Ground Based Hyperspectral Imaging}},
  booktitle = icra,
  year      = 2016,
  keywords  = {Robotics in Agriculture and Forestry, Agricultural Automation, Field Robots},
  abstract  = {A critical step in treating or eradicating weed infestations amongst vegetable crops is the ability to accurately and reliably discriminate weeds from crops. In recent times, high spatial resolution hyperspectral imaging data from ground based platforms have shown particular promise in this application. Using spectral vegetation signatures to discriminate between crop and weed species has been demonstrated on several occasions in the literature over the past 15 years. A number of authors demonstrated successful per-pixel classification with accuracies of over 80\%. However, the vast majority of the related literature uses supervised methods, where training datasets have been manually compiled. In practice, static training data can be particularly susceptible to temporal variability due to physiological or environmental change. A self-supervised training method that leverages prior knowledge about seeding patterns in vegetable fields has recently been introduced in the context of RGB imaging, allowing the classifier to continually update weed appearance models as conditions change. This paper combines and extends these methods to provide a selfsupervised framework for hyperspectral crop/weed discrimination with prior knowledge of seeding patterns using an autonomous mobile ground vehicle. Experimental results in corn crop rows demonstrate the systems performance and limitations.}
}

@inproceedings{osep2016icra,
  author    = {A. Osep and A. Hermans and F. Engelmann and D. Klostermann and M. Mathias and B. Leibe},
  title     = {{Multi-Scale Object Candidates for Generic Object Tracking in Street Scenes}},
  booktitle = icra,
  year      = 2016,
  keywords  = {Visual Tracking, Computer Vision for Transportation},
  abstract  = {Most vision based systems for object tracking in urban environments focus on a limited number of important object categories such as cars or pedestrians, for which powerful detectors are available. However, practical driving scenarios contain many additional objects of interest, for which suitable detectors either do not yet exist or would be cumbersome to obtain. In this paper we propose a more general tracking approach which does not follow the often used tracking-bydetection principle. Instead, we investigate how far we can get by tracking unknown, generic objects in challenging street scenes. As such, we do not restrict ourselves to only tracking the most common categories, but are able to handle a large variety of static and moving objects. We evaluate our approach on the KITTI dataset and show competitive results for the annotated classes, even though we are not restricted to them.}
}

@inproceedings{pillai2016icra,
  author    = {S. Pillai and S. Ramalingam and J. Leonard},
  title     = {{High-Performance and Tunable Stereo Reconstruction}},
  booktitle = icra,
  year      = 2016,
  keywords  = {Computer Vision for Transportation, Mapping, RGB-D Perception},
  abstract  = {Traditional stereo algorithms have focused their efforts on reconstruction quality and have largely avoided prioritizing for run time performance. Robots, on the other hand, require quick maneuverability and effective computation to observe its immediate environment and perform tasks within it. In this work, we propose a high-performance and tunable stereo disparity estimation method, with a peak frame-rate of 120Hz (VGA resolution, on a single CPU-thread), that can potentially enable robots to quickly reconstruct their immediate surroundings and maneuver at high-speeds. Our key contribution is a disparity estimation algorithm that iteratively approximates the scene depth via a piece-wise planar mesh from stereo imagery, with a fast depth validation step for semidense reconstruction. The mesh is initially seeded with sparsely matched keypoints, and is recursively tessellated and refined as needed (via a resampling stage), to provide the desired stereo disparity accuracy. The inherent simplicity and speed of our approach, with the ability to tune it to a desired reconstruction quality and runtime performance makes it a compelling solution for applications in high-speed vehicles.}
}

@inproceedings{guo2016icra,
  author    = {C. Guo and K. Sartipi and R. DuToit and G. Georgiou and R. Li and J. O'Leary and E. Nerurkar and J. Hesch and S. Roumeliotis},
  title     = {{Large-Scale Cooperative 3D Visual-Inertial Mapping in a Manhattan World}},
  booktitle = icra,
  year      = 2016,
  keywords  = {Mapping, SLAM, Sensor Fusion},
  abstract  = {In this paper, we address the problem of cooperative mapping (CM) using datasets collected by multiple users at different times, when the transformation between the users starting poses is unknown. Specifically, we formulate CM as a constrained optimization problem, where each users independently estimated trajectory and map are combined in a single map by imposing geometric constraints between commonly-observed point and line features. Furthermore, our formulation allows for modularity since new/old maps (or parts of them) can be easily added/removed with no impact on the remaining ones. Additionally, the proposed CM algorithm lends itself, for the most part, to parallel implementations, hence gaining in speed. Experimental results based on visual and inertial measurements collected from four users within two large buildings are used to assess the performance of the proposed CM algorithm.}
}

@inproceedings{liu2016icra,
  author    = {S. Liu and M. Watterson and S. Tang and V. Kumar},
  title     = {{High Speed Navigation for Quadrotors with Limited Onboard Sensing}},
  booktitle = icra,
  year      = 2016,
  keywords  = {Aerial Robotics, Collision Avoidance, Autonomous Vehicle Navigation},
  abstract  = {We address the problem of high speed autonomous navigation of quadrotor micro aerial vehicles with limited onboard sensing and computation. In particular, we propose a dual range planning horizon method to safely and quickly navigate quadrotors to specified goal locations in previously unknown and unstructured environments. In each planning epoch, a short-range planner uses a local map to generate a new trajectory. At the same time, a safe stopping policy is found. This allows the robot to come to an emergency halt when necessary. Our algorithm guarantees collision avoidance and demonstrates important advances in real-time planning. First, our novel short range planning method allows us to generate and re-plan trajectories that are dynamically feasible, comply with state and input constraints, and avoid obstacles in real-time. Further, previous planning algorithms abstract away the obstacle detection problem by assuming the instantaneous availability of geometric information about the environment. In contrast, our method addresses the challenge of using the raw sensor data to form a map and navigate in real-time. Finally, in addition to simulation examples, we provide physical experiments that demonstrate the entire algorithmic pipeline from obstacle detection to trajectory execution.}
}

@inproceedings{dequaire2016icra,
  author    = {J.M.M. Dequaire and C.H. Tong and W. Churchill and I. Posner},
  title     = {{Off the Beaten Track: Predicting Localisation Performance in Visual Teach and Repeat}},
  booktitle = icra,
  year      = 2016,
  keywords  = {Visual-Based Navigation, Localization, Autonomous Vehicle Navigation},
  abstract  = {This paper proposes an appearance-based approach to estimating localisation performance in the context of visual teach and repeat. Specifically, it aims to estimate the likely corridor around a taught trajectory within which a visionbased localisation system is still able to localise itself. In contrast to prior art, our system is able to predict this localisation envelope for trajectories in similar, yet geographically distant locations where no repeat runs have yet been performed. Thus, by characterising the localisation performance in one region, we are able to predict performance in another. To achieve this, we leverage a Gaussian Process regressor to estimate the likely number of feature matches for any keyframe in the teach run, based on a combination of trajectory properties such as curvature and an appearance model of the keyframe. Using data from real traversals, we demonstrate that our approach performs as well as prior art when it comes to interpolating localisation performance based on a number of repeat runs, while also performing well at generalising performance estimation to freshly taught trajectories.}
}

@inproceedings{lee2016icra-aeps,
  author    = {D. Lee and M. Campbell},
  title     = {{An Efficient Probabilistic Surface Normal Estimator}},
  booktitle = icra,
  year      = 2016,
  keywords  = {Object detection, Segmentation, Categorization, Recognition, Mapping},
  abstract  = {An efficient surface normal estimation method is presented. The new algorithm estimates surface normal direction for each cell in a grid based on the occupancy information (both occupied and empty) of the neighboring cells. This grid representation allows user-defined sizes and scaling with the environment, not the number of measurements. Recursive and batch formulations to obtain the posterior estimate are presented, and compared. A computationally efficient implementation is derived which provides consistent and accurate estimates as measurements become available. Both simulation and experimental results are shown, demonstrating comparable estimation performance to that of using Point Cloud Library, but with significantly reduced computation time.}
}

@inproceedings{amayo2016icra,
  author    = {P. Amayo and P. Pinies and L.M. Paz and P. Newman},
  title     = {{A Unified Representation for Application of Architectural Constraints in Large-Scale Mapping}},
  booktitle = icra,
  year      = 2016,
  keywords  = {SLAM, Mapping, Optimization and Optimal Control},
  abstract  = {This paper is about discovering and leveraging architectural constraints in large scale 3D reconstructions using laser. Our contribution is to offer a formulation of the problem which naturally and in a unified way, captures the variety of architectural constraints that can be discovered and applied in urban reconstructions. We focus in particular on the case of survey construction with a push broom laser + VO system. Here visual odometry is combined with vertical 2D scans to create a 3D picture of the environment. A key characteristic here is that the sensors pass/sweep swiftly through the environment such that elements of the scene are seen only briefly by cameras and scanned just once by the laser. These qualities make for a an ill-constrained optimisation problem which is greatly aided if architectural constraints can be discovered and appropriately applied. We demonstrate our approach in an end-to-end implementation which discovers salient architectural constraints and rejects false loop closures before invoking an optimisation to return a 3D model of the workspace. We evaluate the precision of this model by comparison to a ground truth provided by a 3rd party professional survey using highend (static) 3D laser scanners.}
}

@inproceedings{dube2016icra,
  author    = {R. Dub{\'e} and H. Sommer and A.R. Gawel and M. Bosse and R. Siegwart},
  title     = {{Non-Uniform Sampling Strategies for Continuous Correction Based Trajectory Estimation}},
  booktitle = icra,
  year      = 2016,
  keywords  = {SLAM, Localization, Field Robots},
  abstract  = {Sliding window estimation is widely used for online simultaneous localization and mapping. While increasing the sliding window size generally yields improved accuracy, it also comes at an increase in computational cost. In order to reduce this cost, we propose smarter non-uniform sampling of the trajectory representation over the sliding window. This nonuniform temporal resolution is possible with continuous-time representations that allow freely adjustable knots location. Four strategies for selecting the knots location are presented and evaluated based on a real data laser-odometry SLAM problem. The results clearly show that non-uniform distributions of knots can be superior to uniform distribution in terms of accuracy per computation time.}
}

@inproceedings{ivanov2016icra,
  author    = {A. Ivanov and M. Campbell},
  title     = {{An Efficient Robotic Exploration Planner with Probabilistic Guarantees}},
  booktitle = icra,
  year      = 2016,
  keywords  = {Motion and Path Planning, Optimization and Optimal Control, Autonomous Vehicle Navigation},
  abstract  = {Efficient robotic exploration of an unknown, sensor limited, global-information-deficient environment poses a unique challenge to path planning algorithms because no deterministic guarantees on path completion and mission success can be made. Integrated Exploration (IE), which strives to combine localization and exploration, must be solved in order to create an autonomous robotic system capable of long term operation in new and challenging environments. This paper formulates a probabilistic framework which allows the creation of exploration algorithms providing probabilistic guarantees of success. A novel connection is made between the Hamiltonian Path Problem and exploration. The Guaranteed Probabilistic Information Explorer (G-PIE) is developed for the IE problem, providing a probabilistic guarantee on path completion, and asymptotic optimality of exploration.}
}

@inproceedings{hoffman2016icra,
  author    = {J. Hoffman and S. Gupta and J. Leong and S. Guadarrama and T. Darrell},
  title     = {{Cross-Modal Adaptation for RGB-D Detection}},
  booktitle = icra,
  year      = 2016,
  keywords  = {RGB-D Perception, Computer Vision for Other Robotic Applications, Object detection, Segmentation, Categorization, CNN},
  abstract  = {In this paper owing paper, we present a framework for we propose a technique to adapt convolutional neural Our network (CNN) based object detectors ject detectors for robotic perception. trained on images to effectively leverage depth images at y robotics practitioners to RGB quickly (under test time to boost detection performance. Given labeled depth build a large-scale real-time perception images a detectors handful of we show how to createfor new oncategories we adapt an RGB object detector for thus a new category such that it can now use depth e internet image databases, allowing in addition to RGB images at test time to produce ng thousands images of available categories to more detections. em suitable for theaccurate particular robotic Our approach is built upon the observation layers of a CNN are largely task and ore, we show how to adaptthat theselower models category domain specific while higher layers are nment with just a fewagnostic in-situ and images. largelyevaluate task andthe category ng 2D benchmarks speed,specific while being domain agnostic. We operationalize this observation by proposing a mid-level y of our system. fusion of RGB and depth CNNs. Experimental evaluation on the challenging NYUD2 dataset shows that our proposed adaptation .}
}

@inproceedings{kaelbling2016icra,
  author    = {L. Kaelbling and T. Lozano-Perez},
  title     = {{Implicit Belief-Space Pre-Images for Hierarchical Planning and Execution}},
  booktitle = icra,
  year      = 2016,
  keywords  = {Manipulation Planning, AI Reasoning Methods, Mobile Manipulation},
  abstract  = {We present a method for planning and execution in very high-dimensional mixed discrete and continuous spaces in the presence of uncertainty using an implicit, factored approximation representation of pre-images and extend it to planning in belief space. We demonstrate the approach in a mobile-manipulation domain combining pushing with pick-andplace manipulation with error in sensing and manipulation. We show empirically that execution monitoring using pre-images improves computational efficiency over continual replanning, and that the hierarchical planning method it enables provides further efficiency improvements.}
}

@inproceedings{frost2016icra,
  author    = {D. Frost and O. K{\"a}hler and D. Murray},
  title     = {{Object-Aware Bundle Adjustment for Correcting Monocular Scale Drift}},
  booktitle = icra,
  year      = 2016,
  keywords  = {SLAM, Computer Vision for Other Robotic Applications, Visual Tracking},
  abstract  = {Without knowledge of the absolute baseline between images, the scale of a map from single-camera simultaneous localization and mapping system is subject to calamitous drift over time. We describe a monocular approach that in addition to point measurements also considers object detections to resolve this scale ambiguity and drift. By placing a prior on the size of the objects, the scale estimation can be seamlessly integrated into a bundle adjustment. When object observations are available, the local scale of the map is then determined jointly with the camera pose in local adjustments. Unlike many previous visual odometry methods, our approach does not impose restrictions such as approximately constant camera height or planar roadways, and is therefore applicable to a much wider range of applications. We evaluate our approach on the KITTI dataset and show that it reduces scale drift over long-range outdoor sequences with a total length of 40 km. Qualitative evaluation is also performed on video footage from a hand-held camera.}
}

@inproceedings{mccool2016icra,
  author    = {C.S. McCool and I. Sa and F. Dayoub and C. Lehnert and T. Perez and B. Upcroft},
  title     = {{Visual Detection of Occluded Crop: For Automated Harvesting}},
  booktitle = icra,
  year      = 2016,
  keywords  = {Computer Vision for Other Robotic Applications},
  abstract  = {This paper presents a novel crop detection system applied to the challenging task of field sweet pepper (capsicum) detection. The field-grown sweet pepper crop presents several challenges for robotic systems such as the high degree of occlusion and the fact that the crop can have a similar colour to the background (green on green). To overcome these issues, we propose a two-stage system that performs per-pixel segmentation followed by region detection. The output of the segmentation is used to search for highly probable regions and declares these to be sweet pepper. We propose the novel use of the local binary pattern (LBP) to perform crop segmentation. This feature improves the accuracy of crop segmentation from an AUC of 0.10, for previously proposed features, to 0.56. Using the LBP feature as the basis for our two-stage algorithm, we are able to detect 69.2\% of field grown sweet peppers in three sites. This is an impressive result given that the average detection accuracy of people viewing the same colour imagery is 66.8\%.}
}

@inproceedings{pothen2016icra,
  author    = {Z. Pothen and S. Nuske},
  title     = {{Texture-Based Fruit Detection Via Images Using the Smooth Patterns on the Fruit}},
  booktitle = icra,
  year      = 2016,
  keywords  = {Field Robots, Robotics in Agriculture and Forestry, Object detection, Segmentation, Categorization},
  abstract  = {This paper describes a keypoint detection algorithm to accurately detect round fruits in high resolution imagery. The significant challenge associated with round fruits such as grapes and apples is that the surface is smooth and lacks definition and contrasting features, the contours of the fruit may be partially occluded, and the color of the fruit often blends with background foliage. We propose a fruit detection algorithm that utilizes the gradual variation of intensity and gradient orientation on the surface of the fruit. Candidate fruit locations, or seed points are tested for both monotonically decreasing intensity and gradient orientation profiles. Candidate fruit locations that pass the initial filter are classified using modified histogram of oriented gradients combined with a pairwise intensity comparison texture descriptor and random forest classifier. We analyse the performance of the fruit detection algorithm on image datasets of grapes and apples using human labeled images as ground truth. Our method to detect candidate fruit locations is scale invariant, robust to partial occlusions and more accurate than existing methods. We achieve overall F1 accuracy score of 0.82 for grapes and 0.80 for apples. We demonstrate our method is more accurate than existing methods.}
}

@inproceedings{ataer-cansizoglu2016icra,
  author    = {E. Ataer-Cansizoglu and Y. Taguchi and S. Ramalingam},
  title     = {{Pinpoint SLAM: A Hybrid of 2D and 3D Simultaneous Localization and Mapping for RGB-D Sensors}},
  booktitle = icra,
  year      = 2016,
  keywords  = {SLAM},
  abstract  = {Conventional SLAM systems with an RGB-D sensor use depth measurements only in a limited depth range due to hardware limitation and noise of the sensor, ignoring regions that are too far or too close from the sensor. Such systems introduce registration errors especially in scenes with large depth variations. In this paper, we present a novel RGB-D SLAM system that makes use of both 2D and 3D measurements. Our system first extracts keypoints from RGB images and generates 2D and 3D point features from the keypoints with invalid and valid depth values, respectively. It then establishes 3D-to-3D, 2D-to-3D, and 2D-to-2D point correspondences among frames. For the 2D-to-3D point correspondences, we use the rays defined by the 2D point features to pinpoint the corresponding 3D point features, generating longer-range constraints than using only 3D-to-3D correspondences. For the 2D-to-2D point correspondences, we triangulate the rays to generate 3D points that are used as 3D point features in the subsequent process. We use the hybrid correspondences in both online SLAM and offline postprocessing: the online SLAM focuses more on the speed by computing correspondences among consecutive frames for real-time operations, while the offline postprocessing generates more correspondences among all the frames for higher accuracy. The results on RGB-D SLAM benchmarks show that the online SLAM provides higher accuracy than conventional SLAM systems, while the postprocessing further improves the accuracy.}
}

@inproceedings{suenderhauf2016icra,
  author    = {N. S{\"u}nderhauf and F. Dayoub and S.M. McMahon and B. Talbot and R. Schulz and G. Wyeth and P. Corke and B. Upcroft and M.J. Milford},
  title     = {{Place Categorization and Semantic Mapping on a Mobile Robot}},
  booktitle = icra,
  year      = 2016,
  keywords  = {Semantic Scene Understanding, Mapping},
  abstract  = {In this paper we focus on the challenging problem of place categorization and semantic mapping on a robot without environment-specific training. Motivated by their ongoing success in various visual recognition tasks, we build our system upon a state-of-the-art convolutional network. We overcome its closed-set limitations by complementing the network with a series of one-vs-all classifiers that can learn to recognize new semantic classes online. Prior domain knowledge is incorporated by embedding the classification system into a Bayesian filter framework that also ensures temporal coherence. We evaluate the classification accuracy of the system on a robot that maps a variety of places on our campus in real-time. We show how semantic information can boost robotic object detection performance and how the semantic map can be used to modulate the robots behaviour during navigation tasks. The system is made available to the community as a ROS module.}
}

@inproceedings{phillips2016icra,
  author    = {S. Phillips and A. Jaegle and K. Daniilidis},
  title     = {{Fast, Robust, Continuous Monocular Egomotion Computation}},
  booktitle = icra,
  year      = 2016,
  keywords  = {Visual-Based Navigation, Computer Vision for Other Robotic Applications},
  abstract  = {We propose robust methods for estimating camera egomotion in noisy, real-world monocular image sequences in the general case of unknown observer rotation and translation with two views and a small baseline. This is a difficult problem because of the nonconvex cost function of the perspective camera motion equation and because of non-Gaussian noise arising from noisy optical flow estimates and scene non-rigidity. To address this problem, we introduce the expected residual likelihood method (ERL), which estimates confidence weights for noisy optical flow data using likelihood distributions of the residuals of the flow field under a range of counterfactual model parameters. We show that ERL is effective at identifying outliers and recovering appropriate confidence weights in many settings. We compare ERL to a novel formulation of the perspective camera motion equation using a lifted kernel, a recently proposed optimization framework for joint parameter and confidence weight estimation with good empirical properties. We incorporate these strategies into a motion estimation pipeline that avoids falling into local minima. We find that ERL outperforms the lifted kernel method and baseline monocular egomotion estimation strategies on the challenging KITTI dataset, while adding almost no runtime cost over baseline egomotion methods.}
}

@inproceedings{bargoti2016icra,
  author    = {S. Bargoti and J.P. Underwood},
  title     = {{Image Classification with Orchard Metadata}},
  booktitle = icra,
  year      = 2016,
  keywords  = {Robotics in Agriculture and Forestry, Semantic Scene Understanding, Object detection, Segmentation, Categorization},
  abstract  = {Low cost and easy to use monocular vision systems are able to capture large scale, dense data in orchards, to facilitate precision agriculture applications. Accurate image parsing is required for this purpose, however, operating in natural outdoor conditions makes this a complex task due to the undesirable intra-class variations caused by changes in illumination, pose and tree types, etc. Typically these variations are difficult to explicitly model and discriminative classifiers strive to be invariant to them. However, given the presence of structure, in both the orchard and how the data was obtained, a subset of these factors of variations can correlate with readily available metadata, including extrinsic experimental information such as the sun incidence angle, position within farm, etc. This paper presents a method to incorporate such metadata to aid scene parsing based on a multi-scale Multi-Layered Perceptron (MLP) architecture. Experimental results are shown for pixel segmentation over data collected at an apple orchard, leading to fruit detection and yield estimation. The results show a consistent improvement in segmentation accuracy with the inclusion of metadata under different network complexities, training configurations and evaluation metrics.}
}

@inproceedings{hayne2016icra,
  author    = {R. Hayne and R. Luo and D. Berenson},
  title     = {{Considering Avoidance and Consistency in Motion Planning for Human-Robot Manipulation in a Shared Workspace}},
  booktitle = icra,
  year      = 2016,
  keywords  = {Manipulation Planning, Physical Human-Robot Interaction},
  abstract  = {This paper presents an approach to formulating the cost function for a motion planner intended for human-robot collaboration on manipulation tasks in a shared workspace. To be effective for human-robot collaboration a robot should plan its motion so that it is both safe and efficient. To achieve this, we propose two factors to consider in the cost function for the robots motion planner: (1) Avoidance of the workspace previously-occupied by the human, so that the motion is as safe as possible, and (2) Consistency of the robots motion, so that the motion is as predictable as possible for the human and they can perform their task without focusing undue attention on the robot. Our experiments in simulation and a human-robot workspace sharing study compare a cost function that uses only the first factor and a combined cost that uses both factors vs. a baseline method that is perfectly consistent but does not account for the humans previous motion. We find that using either cost function we outperform the baseline method in terms of task success rate without degrading the task completion time. The best task success rate is achieved with the cost function that includes both the avoidance and consistency terms.}
}

@inproceedings{cicco2016icra,
  author    = {Di Cicco, M. and Della Corte, B. and G. Grisetti},
  title     = {{Unsupervised Calibration of Wheeled Mobile Platforms}},
  booktitle = icra,
  year      = 2016,
  keywords  = {Calibration and Identification, Kinematics},
  abstract  = {This paper describes an unsupervised approach to retrieve the kinematic parameters of a wheeled mobile robot. The robot chooses which action to take in order to minimize the uncertainty in the parameter estimate and to fully explore the parameter space. Our method explores the effects of a set of elementary motion on the platform to dynamically select the best action and to stop the process when the estimate can be no further improved. We tested our approach both in simulation and with real robots. Our method is reported to obtain in shorter time parameter estimates that are statistically more accurate than the ones obtained by steering the robot on predefined patterns.}
}

@inproceedings{li2016icra-ltg3,
  author    = {J. Li and D.P. Meger and G. Dudek},
  title     = {{Learning to Generalize 3D Spatial Relationships}},
  booktitle = icra,
  year      = 2016,
  keywords  = {Semantic Scene Understanding, RGB-D Perception, Visual Learning},
  abstract  = {This paper presents an approach to learn meaningful spatial relationships in an unsupervised fashion from the distribution of 3D object poses in the real world. Our approach begins by extracting an over-complete set of features to describe the relative geometry of two objects. Each relationship type is modeled using a relevance-weighted distance over this feature space. This effectively ignores irrelevant feature dimensions. Our algorithm RANSEM for determining subsets of data that share a relationship as well as the model to describe each relationship is based on robust sample-based clustering. This approach combines the search for consistent groups of data with the extraction of models that precisely capture the geometry of those groups. An iterative refinement scheme has shown to be an effective approach for finding concepts of differing degrees of geometric specificity. Our results show that the models learned by our approach correlate strongly with the English labels that have been given by a human annotator to a set of validation data drawn from the NYUv2 real-world Kinect dataset, demonstrating that these concepts can be automatically acquired given sufficient experience. Additionally, the results of our method significantly out-perform K-means, a standard baseline for unsupervised cluster extraction.}
}

@inproceedings{kim2016icra,
  author    = {J. Kim and C.D.C. Lerma and I. Reid},
  title     = {{Direct Semi-dense SLAM for Rolling Shutter Cameras}},
  booktitle = icra,
  year      = 2016,
  keywords  = {SLAM},
  abstract  = {In this paper, we present a monocular Direct and Semi-dense SLAM (Simultaneous Localization And Mapping) system for rolling shutter cameras. In a rolling shutter camera, the pose is different for each row of each image, and this yields poor pose estimates and poor structure estimates when using a state-of-the-art semi-dense direct method designed for global shutter cameras. To address this issue in tracking, we model the smooth and continuous camera trajectory using a B-spline curve of degree k1 for poses in the Lie algebra, se(3). We solve for the camera poses at each row-time by a direct optimisation of photometric error as a function of the control points of the spline. Likewise for mapping, we develop generalised epipolar geometry for the rolling shutter case and solve for point depths using photometric error. Although each of these issues has been previously tackled, to the best of our knowledge ours is the first full solution to monocular, direct (feature-less) SLAM. We benchmark our method for pose accuracy and map accuracy against the state-of-the-art semi-dense SLAM system, LSDSLAM, demonstrating the improved efficacy of our approach when using rolling shutter cameras via synthetic sequences with known ground-truth and real sequences.}
}

@inproceedings{paull2016icra,
  author    = {L. Paull and G. Huang and J. Leonard},
  title     = {{A Unified Resource-Constrained Framework for Graph SLAM}},
  booktitle = icra,
  year      = 2016,
  keywords  = {SLAM, Mapping, Localization},
  abstract  = {Graphical methods have proven an extremely useful tool employed by the mobile robotics community to frame estimation problems. Incremental solvers are able to process incoming sensor data and produce maximum a posteriori (MAP) estimates in realtime by exploiting the natural sparsity within the graph for reasonable-sized problems. However, to enable truly longterm operation in prior unknown environments requires algorithms whose computation, memory, and bandwidth (in the case of distributed systems) requirements scale constantly with time and environment size. Some recent approaches have addressed this problem through a two-step process - first the variables selected for removal are marginalized which induces density, and then the result is sparsified to maintain computational efficiency. Previous literature generally addresses only one of these two components. In this work, we attempt to explicitly connect all of the aforementioned resource constraint requirements by considering the node removal and sparsification pipeline in its entirety. We formulate the node selection problem as a minimization problem over the penalty to be paid in the resulting sparsification. As a result, we produce node subset selection strategies that are optimal in terms of minimizing the impact, in terms of Kullback-Liebler divergence (KLD), of approximating the dense distribution by a sparse one. We then show that one instantiation of this problem yields a computationally tractable formulation. Finally, we evaluate the method on standard datasets and show that the KLD is minimized as compared to other commonly-used heuristic node selection techniques.}
}

@inproceedings{kim2016icra-pfgs,
  author    = {S. Kim and M. Likhachev},
  title     = {{Planning for Grasp Selection of Partially Occluded Objects}},
  booktitle = icra,
  year      = 2016,
  keywords  = {Motion and Path Planning, Manipulation Planning, Grasping},
  abstract  = {In a cluttered scene, an object is often occluded by other objects, and a robot cannot figure out what the object is and perceive its pose exactly. We assume that the robot is equipped with a depth sensor and given a database of 3D object models and their grasping poses, but yet there is uncertainty about objects class and pose. In this paper, we study the problem of how to predict the class and pose of an occluded object by carefully taking a sequence of observations. To find the best sequence of viewpoints by the robot, we construct hypotheses of the states of the target and occluding objects, and update our belief state as new observations come in. Every time selecting the next robot pose, we greedily choose the one that is expected to reduce the uncertainty the most. Based on the theoretical analysis of adaptive submodular maximization problems, this process is guaranteed to find a near-optimal sequence of robot poses in terms of observation and traverse costs. To validate the proposed method, we present simulation and robot experiments using a PR2.}
}

@inproceedings{karasev2016icra,
  author    = {V. Karasev and A. Ayvaci and B. Heisele and S. Soatto},
  title     = {{Intent-Aware Long-Term Prediction of Pedestrian Motion}},
  booktitle = icra,
  year      = 2016,
  keywords  = {Autonomous Vehicle Navigation, Motion and Path Planning},
  abstract  = {We present a method to predict long-term motion of pedestrians, modeling their behavior as jump-Markov processes with their goal a hidden variable. Assuming approximately rational behavior, and incorporating environmental constraints and biases, including time-varying ones imposed by traffic lights, we model intent as a policy in a Markov decision process framework. We infer pedestrian state using a Rao-Blackwellized filter, and intent by planning according to a stochastic policy, reflecting individual preferences in aiming at the same goal.}
}

@inproceedings{khosoussi2016icra,
  author    = {K. Khosoussi and S. Huang and G. Dissanayake},
  title     = {{Tree-Connectivity: Evaluating the Graphical Structure of SLAM}},
  booktitle = icra,
  year      = 2016,
  keywords  = {SLAM},
  abstract  = {Simultaneous localization and mapping (SLAM) in robotics, and a number of related problems that arise in sensor networks are instances of estimation problems over weighted graphs. This paper studies the relation between the graphical representation of such problems and estimationtheoretic concepts such as the Cramer-Rao lower bound (CRLB) and D-optimality. We prove that the weighted number of spanning trees, as a graph connectivity metric, is closely related to the determinant of CRLB. This metric can be efficiently computed for large graphs by exploiting the sparse structure of underlying estimation problems. Our analysis is validated using experiments with publicly available pose-graph SLAM datasets.}
}

@inproceedings{lehnert2016icra,
  author    = {C. Lehnert and I. Sa and C.S. McCool and B. Upcroft and T. Perez},
  title     = {{Sweet Pepper Pose Detection and Grasping for Automated Crop Harvesting}},
  booktitle = icra,
  year      = 2016,
  keywords  = {Robotics in Agriculture and Forestry, Computer Vision for Other Robotic Applications},
  abstract  = {This paper presents a method for estimating the 6DOF pose of sweet-pepper (capsicum) crops for autonomous harvesting via a robotic manipulator. The method uses the Kinect Fusion algorithm to robustly fuse RGB-D data from an eye-in-hand camera combined with a colour segmentation and clustering step to extract an accurate representation of the crop. The 6DOF pose of the sweet peppers is then estimated via a nonlinear least squares optimisation by fitting a superellipsoid to the segmented sweet pepper. The performance of the method is demonstrated on a real 6DOF manipulator with a custom gripper. The method is shown to estimate the 6DOF pose successfully enabling the manipulator to grasp sweet peppers for a range of different orientations. The results obtained improve largely on the performance of grasping when compared to a naive approach, which does not estimate the orientation of the crop.}
}

@inproceedings{isler2016icra,
  author    = {S.R. Isler and R. Sabzevari and J. Delmerico and D. Scaramuzza},
  title     = {{An Information Gain Formulation for Active Volumetric 3D Reconstruction}},
  booktitle = icra,
  year      = 2016,
  keywords  = {Autonomous Agents, Computational Geometry, Computer Vision for Automation},
  abstract  = {We consider the problem of next-best view selection for volumetric reconstruction of an object by a mobile robot equipped with a camera. Based on a probabilistic volumetric map that is built in real time, the robot can quantify the expected information gain from a set of discrete candidate views. We propose and evaluate several formulations to quantify this information gain for the volumetric reconstruction task, including visibility likelihood and the likelihood of seeing new parts of the object. These metrics are combined with the cost of robot movement in utility functions. The next best view is selected by optimizing these functions, aiming to maximize the likelihood of discovering new parts of the object. We evaluate the functions with simulated and real world experiments within a modular software system that is adaptable to other robotic platforms and reconstruction problems. We release our implementation open source.}
}

@inproceedings{bormann2016icra,
  author    = {R. Bormann and F. Jordan and W. LI and J. Hampp and M. Haegele},
  title     = {{Room Segmentation: Survey, Implementation, and Analysis}},
  booktitle = icra,
  year      = 2016,
  keywords  = {Mapping, Localization},
  abstract  = {The division of floor plans or navigation maps into single rooms or similarly meaningful semantic units is central to numerous tasks in robotics such as topological mapping, semantic mapping, place categorization, human-robotinteraction, or automatized professional cleaning. Although many map partitioning algorithms have been proposed for various applications there is a lack of comparative studies on these different algorithms. This paper surveys the literature on room segmentation and provides four publicly available implementations of popular methods, which target the semantic mapping domain and are tuned to yield segmentations into complete rooms. In an attempt to provide new users of such technologies guidance in the choice of map segmentation algorithm, those methods are compared qualitatively and quantitatively using several criteria. The evaluation is based on a novel compilation of 20 challenging floor plans.}
}

@inproceedings{li2016icra-lsim,
  author    = {Z. Li and V. Isler},
  title     = {{Large Scale Image Mosaic Construction for Agricultural Applications}},
  booktitle = icra,
  year      = 2016,
  keywords  = {Computer Vision for Other Robotic Applications, Robotics in Agriculture and Forestry},
  abstract  = {We present a novel technique for stitching images including those obtained from aerial vehicles flying at low altitudes. Existing image stitching/mosaicking methods rely on inter-image homography computation based on a planar scene assumption. This assumption holds when images are taken from high-altitudes (hence the depth variation is negligible). It is often violated when flying at low altitudes. Further, to avoid scale and resolution changes, existing methods rely on primarily translational motion at fixed altitudes. Our method removes these limitations and performs well even when aerial images are taken from low altitudes by an aerial vehicle performing complex motions. It starts by extracting the ground geometry from a sparse reconstruction of the scene obtained from a small fraction of the input images. Next, it selects the best image (from the entire sequence) for each location on the ground using a novel camera selection criterion. This image is then independently rectified to obtain the corresponding portion of the mosaic. Therefore, the technique avoids performing costly joint-optimization over the entire sequence. It is validated using challenging input sequences motivated by agricultural applications.}
}

@inproceedings{dymczyk2016iros,
  author    = {M.T. Dymczyk and T. Schneider and I. Gilitschenski and R. Siegwart and E. Stumm},
  title     = {{Erasing Bad Memories: Agent-Side Summarization for Long-Term Mapping}},
  booktitle = iros,
  year      = 2016,
  keywords  = {Mapping, Visual Navigation, Localization},
  abstract  = {Precisely estimating the pose of an agent in a global reference frame is a crucial goal that unlocks a multitude of robotic applications, including autonomous navigation and collaboration. In order to achieve this, current state-ofthe-art localization approaches collect data provided by one or more agents and create a single, consistent localization map, maintained over time. However, with the introduction of lengthier sorties and the growing size of the environments, data transfers between the backend server where the global map is stored and the agents are becoming prohibitively large. While some existing methods partially address this issue by building compact summary maps, the data transfer from the agents to the backend can still easily become unmanageable. In this paper, we propose a method that is designed to reduce the amount of data that needs to be transferred from the agent to the backend, functioning in large-scale, multisession mapping scenarios. Our approach is based upon a landmark selection method that exploits information coming from multiple, possibly weak and correlated, landmark utility predictors; fused using learned feature coefficients. Such a selection yields a drastic reduction in data transfer while maintaining localization performance and the ability to efficiently summarize environments over time. We evaluate our approach on a data set that was autonomously collected in a dynamic indoor environment over a period of several months.}
}

@inproceedings{buerki2016iros,
  author    = {M. B{\"u}rki and I. Gilitschenski and E. Stumm and R. Siegwart and J. Nieto},
  title     = {{Appearance-Based Landmark Selection for Efficient Long-Term Visual Localization}},
  booktitle = iros,
  year      = 2016,
  keywords  = {Visual Navigation, Localization, Distributed Robot Systems},
  abstract  = {In this paper, we present an online landmark selection method for distributed long-term visual localization systems in bandwidth-constrained environments. Sharing a common map for online localization provides a fleet of autonomous vehicles with the possibility to maintain and access a consistent map source, and therefore reduce redundancy while increasing efficiency. However, connectivity over a mobile network imposes strict bandwidth constraints and thus the need to minimize the amount of exchanged data. The wide range of varying appearance conditions encountered during long-term visual localization offers the potential to reduce data usage by extracting only those visual cues which are relevant at the given time. Motivated by this, we propose an unsupervised method of adaptively selecting landmarks according to how likely these landmarks are to be observable under the prevailing appearance condition. The ranking function this selection is based upon exploits landmark co-observability statistics collected in past traversals through the mapped area. Evaluation is performed over different outdoor environments, large time-scales and varying appearance conditions, including the extreme transition from day-time to night-time, demonstrating that with our appearance-dependent selection method, we can significantly reduce the amount of landmarks used for localization while maintaining or even improving the localization performance.}
}

@inproceedings{shakeri2016iros,
  author    = {M. Shakeri and H. Zhang},
  title     = {{Illumination Invariant Representation of Natural Images for Visual Place Recognition}},
  booktitle = iros,
  year      = 2016,
  keywords  = {Recognition, Localization, Visual Navigation},
  abstract  = {Illumination changes are a typical problem for many outdoor long-term applications such as visual place recognition. Keypoints may fail to match between images taken at the same location but different times of the day. Although recently some methods are presented for creating shadowfree image representations, all of them have the limitation in terms of dealing with night images and non-Planckian source of lighting. In this paper we present a new method for creating illumination invariant image representation using a combination of two existing methods based on natural image statistics that address the issue of illumination invariance. Unlike previous attempts at solving the problem of illumination invariant representation, the proposed method does not assume the ideal narrow-band color camera nor a calibration step for each environment. We evaluate our method on real datasets to establish its accuracy and efficiency. Experimental results show that our method outperforms competing methods for illumination invariant image representation.}
}

@inproceedings{caselitz2016iros,
  author    = {T. Caselitz and B. Steder and M. Ruhnke and W. Burgard},
  title     = {{Monocular Camera Localization in 3D LiDAR Maps}},
  booktitle = iros,
  year      = 2016,
  keywords  = {Localization, Visual Navigation, Range Sensing},
  abstract  = {Localizing a camera in a given map is essential for vision-based navigation. In contrast to common methods for visual localization that use maps acquired with cameras, we propose a novel approach, which tracks the pose of monocular camera with respect to a given 3D LiDAR map. We employ a visual odometry system based on local bundle adjustment to reconstruct a sparse set of 3D points from image features. These points are continuously matched against the map to track the camera pose in an online fashion. Our approach to visual localization has several advantages. Since it only relies on matching geometry, it is robust to changes in the photometric appearance of the environment. Utilizing panoramic LiDAR maps additionally provides viewpoint invariance. Yet lowcost and lightweight camera sensors are used for tracking. We present real-world experiments demonstrating that our method accurately estimates the 6-DoF camera pose over long trajectories and under varying conditions.}
}

@inproceedings{paton2016iros,
  author    = {M. Paton and K.A. MacTavish and M. Warren and T. Barfoot},
  title     = {Bridging the Appearance Gap: Multi-Experience Localization for Long-Term Visual Teach & Repeat},
  booktitle = iros,
  year      = 2016,
  keywords  = {Localization, Mapping, Field Robots},
  abstract  = {Vision-based, route-following algorithms enable autonomous robots to repeat manually taught paths over long distances using inexpensive vision sensors. However, these methods struggle with long-term, outdoor operation due to the challenges of environmental appearance change caused by lighting, weather, and seasons. While techniques exist to address appearance change by using multiple experiences over different environmental conditions, they either provide topological-only localization, require several manually taught experiences in different conditions, or require extensive offline mapping to produce metric localization. For real-world use, we would like to localize metrically to a single manually taught route and gather additional visual experiences during autonomous operations. Accordingly, we propose a novel multi-experience localization (MEL) algorithm developed specifically for routefollowing applications; it provides continuous, six-degree-offreedom (6DoF) localization with relative uncertainty to a privileged (manually taught) path using several experiences simultaneously. We validate our algorithm through two experiments: i) an offline performance analysis on a 9km subset of a challenging 27km route-traversal dataset and ii) an online field trial where we demonstrate autonomy on a small 250m loop over the course of a sunny day. Both exhibit significant appearance change due to lighting variation. Through these experiments we show that safe localization can be achieved by bridging the appearance gap.}
}

@inproceedings{tanaka2016iros,
  author    = {K. Tanaka},
  title     = {{Self-Localization from Images with Small Overlap}},
  booktitle = iros,
  year      = 2016,
  keywords  = {Localization, Robot Vision, Performance Evaluation and Benchmarking, CNN},
  abstract  = {With the recent success of visual features from deep convolutional neural networks (DCNN) in visual robot selflocalization, it has become important and practical to address more general self-localization scenarios. In this paper, we address the scenario of self-localization from images with small overlap. We explicitly introduce a localization difficulty index as a decreasing function of view overlap between query and relevant database images and investigate performance versus difficulty for challenging cross-view self-localization tasks. We then reformulate the self-localization as a scalable bag-ofvisual-features (BoVF) scene retrieval and present an efficient solution called PCA-NBNN, aiming to facilitate fast and yet discriminative correspondence between partially overlapping images. The proposed approach adopts recent findings in discriminativity preserving encoding of DCNN features using principal component analysis (PCA) and cross-domain scene matching using naive Bayes nearest neighbor distance metric (NBNN). We experimentally demonstrate that the proposed PCA-NBNN framework frequently achieves comparable results to previous DCNN features and that the BoVF model is significantly more efficient. We further address an important alternative scenario of self-localization from images with NO overlap and report the result.}
}

@inproceedings{fontana2016iros,
  author    = {S. Fontana and G. Agamennoni and R. Siegwart and D.G. Sorrenti},
  title     = {{Point Clouds Registration with Probabilistic Data Association}},
  booktitle = iros,
  year      = 2016,
  keywords  = {Mapping, Field Robots},
  abstract  = {Although Point Clouds Registration is a very well studied problem, with many different solutions, most of the approaches in the literature aims at aligning two dense point clouds. Instead, we tackle the problem of aligning a dense point cloud with a sparse one: a problem that has to be solved, for example, to merge maps produced by different sensors, such as a vision-based sensor and laser scanner or two different laser-based sensors. The most used approach to point clouds registration, Iterative Closest Point (ICP), is also applicable to this sub-problem. We propose an improvement over the standard ICP data association policy and we called it Probabilistic Data Association. It was derived applying statistical inference techniques on a fully probabilistic model. In our proposal, each point in the source point cloud is associated with a set of points in the target point cloud; each association is then weighted so that the weights form a probability distribution. The result is an algorithm similar to ICP but more robust w.r.t. noise and outliers. While we designed our approach to deal with the problem of dense-sparse registration, it can be successfully applied also to standard point clouds registration.}
}

@inproceedings{schlegel2016iros,
  author    = {D. Schlegel and G. Grisetti},
  title     = {{Visual Localization and Loop Closing Using Decision Trees and Binary Features}},
  booktitle = iros,
  year      = 2016,
  keywords  = {Recognition, SLAM, Visual Navigation},
  abstract  = {In this paper we present an approach for efficiently retrieving the most similar image, based on pointto-point correspondences, within a sequence that has been acquired through continuous camera movement. Our approach is entailed to the use of standardized binary feature descriptors and exploits the temporal form of the input data to dynamically adapt the search structure. While being straightforward to implement, our method exhibits very fast response times and its Precision/Recall rates compete with state of the art approaches. Our claims are supported by multiple large scale experiments on publicly available datasets. Index Terms Place recognition, Localization, Robot Vision}
}

@inproceedings{arroyo2016iros,
  author    = {R. Arroyo and P.F. Alcantarilla and L.M. Bergasa and E. Romera},
  title     = {{Fusion and Binarization of CNN Features for Robust Topological Localization across Seasons}},
  booktitle = iros,
  year      = 2016,
  keywords  = {Localization, Visual Navigation, Robot Vision, CNN},
  abstract  = {The extreme variability in the appearance of a place across the four seasons of the year is one of the most challenging problems in life-long visual topological localization for mobile robotic systems and intelligent vehicles. Traditional solutions to this problem are based on the description of images using hand-crafted features, which have been shown to offer moderate invariance against seasonal changes. In this paper, we present a new proposal focused on automatically learned descriptors, which are processed by means of a technique recently popularized in the computer vision community: Convolutional Neural Networks (CNNs). The novelty of our approach relies on fusing the image information from multiple convolutional layers at several levels and granularities. In addition, we compress the redundant data of CNN features into a tractable number of bits for efficient and robust place recognition. The final descriptor is reduced by applying simple compression and binarization techniques for fast matching using the Hamming distance. An exhaustive experimental evaluation confirms the improved performance of our proposal (CNN-VTL) with respect to state-of-the-art methods over varied long-term datasets recorded across seasons.}
}

@inproceedings{serafin2016iros,
  author    = {J. Serafin and E. Olson and G. Grisetti},
  title     = {{Fast and Robust 3D Feature Extraction from Sparse Point Clouds}},
  booktitle = iros,
  year      = 2016,
  keywords  = {Localization, SLAM, Navigation},
  abstract  = {Matching 3D point clouds, a critical operation in map building and localization, is difficult with Velodyne-type sensors due to the sparse and non-uniform point clouds that they produce. Standard methods from dense 3D point clouds are generally not effective. In this paper, we describe a featurebased approach using Principal Components Analysis (PCA) of neighborhoods of points, which results in mathematically principled line and plane features. The key contribution in this work is to show how this type of feature extraction can be done efficiently and robustly even on non-uniformly sampled point clouds. The resulting detector runs in real-time and can be easily tuned to have a low false positive rate, simplifying data association. We evaluate the performance of our algorithm on an autonomous car at the MCity Test Facility using a Velodyne HDL-32E, and we compare our results against the state-of-theart NARF keypoint detector.}
}

@inproceedings{he2016iros,
  author    = {L. He and X. Wang and H. Zhang},
  title     = {{M2DP: A Novel 3D Point Cloud Descriptor and Its Application in Loop Closure Detection}},
  booktitle = iros,
  year      = 2016,
  keywords  = {Visual Navigation, SLAM},
  abstract  = {In this paper, we present a novel global descriptor M2DP for 3D point clouds, and apply it to the problem of loop closure detection. In M2DP, we project a 3D point cloud to multiple 2D planes and generate a density signature for points for each of the planes. We then use the left and right singular vectors of these signatures as the descriptor of the 3D point cloud. Our experimental results show that the proposed algorithm outperforms state-of-the-art global 3D descriptors in both accuracy and efficiency.}
}

@inproceedings{kallasi2016iros,
  author    = {F. Kallasi and D.L. Rizzini},
  title     = {{Efficient Loop Closure Based on FALKO LIDAR Features for Online Robot Localization and Mapping}},
  booktitle = iros,
  year      = 2016,
  keywords  = {Range Sensing, Localization, Mapping},
  abstract  = {Keypoint features detection from measurements enables efficient localization and map estimation through the compact representation and recognition of locations. The keypoint detector FALKO has been proposed to detect stable points in laser scans for localization and mapping tasks. In this paper, we present novel loop closure methods based on FALKO keypoints and compare their performance in online localization and mapping problems. The pose graph formulation is adopted, where each pose is associated to a local map of keypoints extracted from the corresponding laser scan. Loops in the graph are detected by matching local maps in two steps. First, the candidate matching scans are selected by comparing the scan signatures obtained from the keypoints of each scan. Second, the transformation between two scans is obtained by pairing and aligning the respective keypoint sets. Experiments with standard benchmark datasets assess the performance of FALKO and of the proposed loop closure algorithms in both offline and online localization and map estimation.}
}

@inproceedings{spangenberg2016iros,
  author    = {R. Spangenberg and D. Goehring and R. Rojas},
  title     = {{Pole-Based Localization for Autonomous Vehicles in Urban Scenarios}},
  booktitle = iros,
  year      = 2016,
  keywords  = {Localization, Robot Vision, Mapping},
  abstract  = {Localization is a key capability for autonomous vehicles especially in urban scenarios. We propose the use of pole-like landmarks as primary features in these environments, as they are distinct, long-term stable and can be detected reliably with a stereo camera system. Furthermore, the resulting map representation is memory efficient, allowing for easy storage and on-line updates. The localization is performed in real-time by a stereo camera system as a main sensor, using vehicle odometry and an off-the-shelf GPS as secondary information sources. Localization is performed by a particle filter approach, coupled with an Kalman filter for robustness and sensor fusion. This leads to a lateral accuracy below 20 cm in various urban test areas. The system has been included in our autonomous test vehicle and successfully demonstrated the full loop from mapping to autonomous driving.}
}

@inproceedings{skinner2016iros,
  author    = {J.R. Skinner and S. Garg and N. S{\"u}nderhauf and P. Corke and B. Upcroft and M.J. Milford},
  title     = {{High-Fidelity Simulation for Evaluating Robotic Vision Performance}},
  booktitle = iros,
  year      = 2016,
  keywords  = {Animation and Simulation, Robot Vision, Recognition},
  abstract  = {Robotic vision, unlike computer vision, typically involves processing a stream of images from a camera with time varying pose operating in an environment with time varying lighting conditions and moving objects. Repeating robotic vision experiments under identical conditions is often impossible, making it difficult to compare different algorithms. For machine learning applications a critical bottleneck is the limited amount of real world image data that can be captured and labelled for both training and testing purposes. In this paper we investigate the use of a photo-realistic simulation tool to address these challenges, in three specific domains: robust place recognition, visual SLAM and object recognition. For the first two problems we generate images from a complex 3D environment with systematically varying camera paths, camera viewpoints and lighting conditions. For the first time we are able to systematically characterise the performance of these algorithms as paths and lighting conditions change. In particular, we are able to systematically generate varying camera viewpoint datasets that would be difficult or impossible to generate in the real world. We also compare algorithm results for a camera in a real environment and a simulated camera in a simulation model of that real environment. Finally, for the object recognition domain, we generate labelled image data and characterise the viewpoint dependency of a current convolution neural network in performing object recognition. Together these results provide a multi-domain demonstration of the beneficial properties of using simulation to characterise and analyse a wide range of robotic vision algorithms.}
}

@article{canelhas2016ral,
  author   = {D.R. Canelhas and T. Stoyanov and A.J. Lilienthal},
  title    = {{From Feature Detection in Truncated Signed Distance Fields to Sparse Stable Scene Graphs}},
  journal  = ral,
  volume   = {1},
  number   = {2},
  pages    = {1148--1155},
  year     = 2016,
  keywords = {Mapping, Recognition, RGB-D Perception},
  abstract = {With the increased availability of GPUs and multicore CPUs, volumetric map representations are an increasingly viable option for robotic applications. A particularly important representation is the truncated signed distance field (TSDF) that is at the core of recent advances in dense 3D mapping. However, there is relatively little literature exploring the characteristics of 3D feature detection in volumetric representations. In this paper we evaluate the performance of features extracted directly from a 3D TSDF representation. We compare the repeatability of Integral invariant features, specifically designed for volumetric images, to the 3D extensions of Harris and Shi & Tomasi corners. We also study the impact of different methods for obtaining gradients for their computation. We motivate our study with an example application for building sparse stable scene graphs, and present an efficient GPU-parallel algorithm to obtain the graphs, made possible by the combination of TSDF and 3D feature points. Our findings show that while the 3D extensions of 2D corner-detection perform as expected, integral invariants have shortcomings when applied to discrete TSDFs. We conclude with a discussion of the cause for these points of failure that sheds light on possible mitigation strategies.}
}

@article{patten2016ral,
  author   = {T. Patten and M. Zillich and R. Fitch and M. Vincze and S. Sukkarieh},
  title    = {{Viewpoint Evaluation for Online 3D Active Object Classification}},
  journal  = ral,
  volume   = {1},
  number   = {1},
  pages    = {73--81},
  year     = 2016,
  keywords = {Object detection, Segmentation, Categorization, Semantic Scene Understanding, RGB-D Perception},
  abstract = {We present an end-to-end method for active object classification in cluttered scenes from RGB-D data. Our algorithms predict the quality of future viewpoints in the form of entropy using both class and pose. Occlusions are explicitly modelled in predicting the visible regions of objects, which modulates the corresponding discriminatory value of a given view. We implement a one-step greedy planner and demonstrate our method online using a mobile robot. We also analyse the performance of our method compared to similar strategies in simulated execution using the Willow Garage dataset. Results show that our active method usefully reduces the number of views required to accurately classify objects in clutter as compared to traditional passive perception.}
}

@article{neubert2016ral,
  author   = {P. Neubert and P. Protzel},
  title    = {{Beyond Holistic Descriptors, Keypoints and Fixed Patches: Multiscale Superpixel Grids for Place Recognition in Changing Environments}},
  journal  = ral,
  volume   = {1},
  number   = {1},
  pages    = {484--491},
  year     = 2016,
  keywords = {Localization, Visual-Based Navigation, Recognition},
  abstract = {Vision-based place recognition in environments subject to severe appearance changes due to day-night cycles, changing weather or seasons is a challenging task. Existing methods typically exploit image sequences, holistic descriptors and/or training data. Each of these approaches limits the practical applicability, e.g. to constant viewpoints for usage of holistic image descriptors. Recently, the combination of local region detectors and descriptors based on Convolutional Neural Networks showed to be a promising approach to overcome these limitations. However, established region detectors, for example keypoint detectors, showed severe problems to provide repetitive landmarks despite dramatically changed appearance of the environment. Thus, they are typically replaced by holistic image descriptors or fixedly arranged patches - both known to be sensitive towards viewpoint changes. In this paper, we present a novel local region detector, SP-Grid, that is particularly suited for the combination of severe appearance and viewpoint changes. It is based on multi-scale image oversegmentations and is designed to combine the advantages of keypoints and fixed image patches by starting from an initial grid-like arrangement and subsequently adapting to the image content. The gridlike arrangement showed to be beneficial in the presence of severe appearance changes and the adaptation to the image content increases the robustness towards viewpoint changes. The experimental evaluation will show the benefit compared to existing local region detectors and holistic image descriptors.}
}

@article{jiang2016ral,
  author   = {C. Jiang and D.P. Paudel and Y. Fougerolle and D. Fofi and C. Demonceaux},
  title    = {{Static-Map and Dynamic Object Reconstruction in Outdoor Scenes Using 3D Motion Segmentation}},
  journal  = ral,
  volume   = {1},
  number   = {1},
  pages    = {324--331},
  year     = 2016,
  keywords = {Mapping, Motion and Path Planning, SLAM},
  abstract = {This paper aims to build the static-map of a dynamic scene using a mobile robot equipped with 3D sensors. The sought static-map consists of only the static scene parts, which has a vital role in scene understanding and landmark based navigation. Building static-map requires the categorization of moving and static objects. In this work, we propose a Sparse Subspace Clustering-based Motion Segmentation method that categories the static scene parts and the multiple moving objects using their 3D motion trajectories. Our motion segmentation method uses the raw trajectory data, allowing the objects to move in direct 3D space, without any projection model assumption or whatsoever. We also propose a complete pipeline for static-map building which estimates the inter-frame motion parameters by exploiting the minimal 3-point Random Sample Consensus algorithm on the feature correspondences only from the static scene parts. The proposed method has been especially designed and tested for large scene in real outdoor environments. On one hand, our 3D Motion Segmentation approach outperforms its 2D based counterparts, for extensive experiments on KITTI dataset. On the other hand, separately reconstructed static-maps and moving objects for various dynamic scenes are very satisfactory.}
}

@article{wolf2016ral,
  author   = {D. Wolf and J. Prankl and M. Vincze},
  title    = {{Enhancing Semantic Segmentation for Robotics: The Power of 3D Entangled Forests}},
  journal  = ral,
  volume   = {1},
  number   = {1},
  pages    = {49--56},
  year     = 2016,
  keywords = {Semantic Scene Understanding, RGB-D Perception, Computer Vision for Automation},
  abstract = {We present a novel, fast and compact method to improve semantic segmentation of 3D point clouds, which is able to learn and exploit common contextual relations between observed structures and objects. Introducing 3D Entangled Forests (3DEF), we extend the concept of entangled features for decision trees to 3D point clouds, enabling the classifier not only to learn which labels are likely to occur close to each other, but also in which specific geometric configuration. Operating on a plane-based representation of a point cloud, our method does not require a final smoothing step and achieves state-of-the-art results on the NYU Depth Dataset in a single inference step. This compactness in turn allows for fast processing times, a crucial factor to consider for online applications on robotic platforms. In a thorough evaluation, we demonstrate the expressiveness of our new 3D entangled feature set and the importance of spatial context in the scope of semantic segmentation.}
}


@article{vitzrabin2016ral,
  author   = {E. Vitzrabin and Y. Edan},
  title    = {{Changing Task Objectives for Improved Sweet Pepper Detection for Robotic Harvesting}},
  journal  = ral,
  volume   = {1},
  number   = {1},
  pages    = {578--584},
  year     = 2016,
  keywords = {Robotics in Agriculture and Forestry, Object detection, Segmentation, Categorization, Sensor Fusion},
  abstract = {This paper presents a method to improve detection for robotic harvesting by changing the task objective in real time in an adaptive thresholding algorithm. The adaptive thresholding algorithm includes three main parts: 3D adaptive thresholding, object detection, and fusion. Optimal local 3D thresholds that were previously determined according to changing illumination conditions were expanded in this research to include also changing task objectives. The task objectives describe the relationships between False Positive Rate, True Positive Rate, and accuracy in the location. The first task objective aims to maximize detection and minimize false alarms so as to ensure the arm is directed only towards real fruits. The second task objective focuses on high accuracy in the detection. Intensive evaluations were conducted on databases which contained 240 images acquired in the field with various artificial illumination setups. The difference between the two tasks objectives was on average 0.09 in detection rates and 0.66 cm in the accuracy. Robotic experiments resulted in 26.6\% difference in pepper grasping success rate with two different task objectives indicating the importance of changing the task objectives for the fruit detection task.}
}

@article{schor2016ral,
  author   = {N. Schor and A. Bechar and T. Ignat and A. Dombrovsky and Y. Elad and S. Berman},
  title    = {{Robotic Disease Detection in Greenhouses: Combined Detection of Powdery Mildew and Tomato Spotted Wilt Virus}},
  journal  = ral,
  volume   = {1},
  number   = {1},
  pages    = {354--360},
  year     = 2016,
  keywords = {Agricultural Automation, Computer Vision for Automation},
  abstract = {Robotic systems for disease detection in greenhouses are expected to improve disease control, increase yield, and reduce pesticide application. We present a robotic detection system for combined detection of two major threats of greenhouse bell peppers: Powdery mildew (PM) and Tomato spotted wilt virus (TSWV). The system is based on a manipulator which facilitates reaching multiple detection poses. Several detection algorithms are developed based on principal component analysis (PCA) and the coefficient of variation (CV). Tests ascertain the system can successfully detect the plant and reach the detection pose required for PM (along the side of the plant), yet it has difficulties in reaching the TSWV detection pose (above the plant). Increasing manipulator workvolume is expected to solve this issue. For TSWV, PCA-based classification with leaf vein removal, achieved the highest classification accuracy (90\%) while the accuracy of the CV methods was also high (85\%, 87\%). For PM, PCA-based pixellevel classification was high (95.2\%) while leaf condition classification accuracy was low (64.3\%) since it was determined based on the upper side of the leaf while disease symptoms start on its lower side. Exposure of the lower side of the leaf during detection is expected to improve PM condition detection.}
}

@article{santos2016ral,
  author   = {J.M. Santos and T. Krajn{\'i}k and J.P. Fentanes and T. Duckett},
  title    = {{Lifelong Information-Driven Exploration to Complete and Refine 4D Spatio-Temporal Maps}},
  journal  = ral,
  volume   = {1},
  number   = {2},
  pages    = {684--691},
  year     = 2016,
  keywords = {Mapping, Service Robots},
  abstract = {This paper presents an exploration method that allows mobile robots to build and maintain spatio-temporal models of changing environments. The assumption of a perpetuallychanging world adds a temporal dimension to the exploration problem, making spatio-temporal exploration a never-ending, life-long learning process. We address the problem by application of information-theoretic exploration methods to spatiotemporal models that represent the uncertainty of environment states as probabilistic functions of time. This allows to predict the potential information gain to be obtained by observing a particular area at a given time, and consequently, to decide which locations to visit and the best times to go there. To validate the approach, a mobile robot was deployed continuously over 5 consecutive business days in a busy office environment. The results indicate that the robots ability to spot environmental changes improved as it refined its knowledge of the world dynamics. Index Terms mobile robotics, spatio-temporal exploration}
}

@article{kaiser2016ral,
  author   = {J. Kaiser and A. Martinelli and F. Fontana and D. Scaramuzza},
  title    = {{Simultaneous State Initialization and Gyroscope Bias Calibration in Visual Inertial Aided Navigation}},
  journal  = ral,
  volume   = {2},
  number   = {1},
  pages    = {18--25},
  year     = 2016,
  keywords = {Sensor Fusion, Localization, Visual-Based Navigation},
  abstract = {State of the art approaches for visual-inertial sensor fusion use filter-based or optimization-based algorithms. Due to the nonlinearity of the system, a poor initialization can have a dramatic impact on the performance of these estimation methods. Recently, a closed-form solution providing such an initialization was derived in [1]. That solution determines the velocity (angular and linear) of a monocular camera in metric units by only using inertial measurements and image features acquired in a short time interval. In this paper, we study the impact of noisy sensors on the performance of this closed-form solution. We show that the gyroscope bias, not accounted for in [1], significantly affects the performance of the method. Therefore, we introduce a new method to automatically estimate this bias. Compared to the original method, the new approach now models the gyroscope bias and is robust to it. The performance of the proposed approach is successfully demonstrated on real data from a quadrotor MAV. Index Terms Sensor Fusion, Localization, Visual-Based Navigation}
}

@article{kallasi2016ral,
  author   = {F. Kallasi and D.L. Rizzini and S. Caselli},
  title    = {{Fast Keypoint Features from Laser Scanner for Robot Localization and Mapping}},
  journal  = ral,
  volume   = {1},
  number   = {1},
  pages    = {176--183},
  year     = 2016,
  keywords = {Range Sensing, Mapping},
  abstract = {Detecting features in sensor measurements and distinguishing among them is an important capability for robot localization and navigation. Despite the wide diffusion of range finders, there are few works on keypoint features for 2D LIDAR and there is potential for improvement over the existing methods. This paper proposes two novel keypoint detectors for the stable detection of interest points in laser measurements and two descriptors for robust associations. The features defined by combining keypoints and descriptors allow stable and efficient place recognition. Experiments with standard benchmark datasets assess the performance of the detectors and descriptors investigated. One of the proposed features, termed FALKO-BSC, achieves higher repeatability score and similar descriptor performance compared with the FLIRT state-of-the-art feature. FALKO-BSC is also shown to enable effective localization.}
}

@article{li2016ral,
  author   = {Z. Li and V. Isler},
  title    = {{Large Scale Image Mosaic Construction for Agricultural Applications}},
  journal  = ral,
  volume   = {1},
  number   = {1},
  pages    = {295--302},
  year     = 2016,
  keywords = {Computer Vision for Other Robotic Applications, Robotics in Agriculture and Forestry},
  abstract = {We present a novel technique for stitching images including those obtained from aerial vehicles flying at low altitudes. Existing image stitching/mosaicking methods rely on inter-image homography computation based on a planar scene assumption. This assumption holds when images are taken from high-altitudes (hence the depth variation is negligible). It is often violated when flying at low altitudes. Further, to avoid scale and resolution changes, existing methods rely on primarily translational motion at fixed altitudes. Our method removes these limitations and performs well even when aerial images are taken from low altitudes by an aerial vehicle performing complex motions. It starts by extracting the ground geometry from a sparse reconstruction of the scene obtained from a small fraction of the input images. Next, it selects the best image (from the entire sequence) for each location on the ground using a novel camera selection criterion. This image is then independently rectified to obtain the corresponding portion of the mosaic. Therefore, the technique avoids performing costly joint-optimization over the entire sequence. It is validated using challenging input sequences motivated by agricultural applications.}
}

@article{khomutenko2016ral,
  author   = {B. Khomutenko and G. Garcia and P. Martinet},
  title    = {{An Enhanced Unified Camera Model}},
  journal  = ral,
  volume   = {1},
  number   = {1},
  pages    = {137--144},
  year     = 2016,
  keywords = {Calibration and Identification, Omnidirectional Vision, Computer Vision for Automation},
  abstract = {This paper describes a novel projection model based on the so-called unified projection model. The new model applies to catadioptric systems and wide-angle fish-eye cameras, it does not require additional mapping to model distortions, and it takes just two projection parameters more than a simple pinhole model to represent radial distortion (one parameter more than the unified model). Here we provide a study of different mathematical aspects of the model, its application limits, and explicit closed-form inversion. The latter allows to apply all the notions of epipolar geometry with no difficulties. Also we introduce a concept of projection surface, which is a useful notion to study and compare different projection models with radial distortion. Using developed software, several different lenses were calibrated using the proposed model, and in all cases sub-pixel precision was achieved.}
}

@article{davis2016ral,
  author   = {B. Davis and I. Karamouzas and S.J. Guy},
  title    = {{C-OPT: Coverage-Aware Trajectory Optimization under Uncertainty}},
  journal  = ral,
  volume   = {1},
  number   = {2},
  pages    = {1020--1027},
  year     = 2016,
  keywords = {Motion and Path Planning, Collision Avoidance, Reactive and Sensor-Based Planning},
  abstract = {We introduce a new problem of continuous, coverage-aware trajectory optimization under localization and sensing uncertainty. In this problem, the goal is to plan a path from a start state to a goal state that maximizes the coverage of a user-specified region while minimizing the control costs of the robot and the probability of collision with the environment. We present a principled method for quantifying the coverage sensing uncertainty of the robot. We use this sensing uncertainty along with the uncertainty in robot localization to develop C-OPT, a coverage-optimization algorithm which optimizes trajectories over belief-space to find locally optimal coverage paths. We highlight the applicability of our approach in multiple simulated scenarios inspired by surveillance, UAV crop analysis, and search-and-rescue tasks. We also present a case study on a physical, differential-drive robot. We also provide quantitative and qualitative analysis of the paths generated by our approach.}
}

@article{johnson2016ral,
  author   = {A. Johnson and J. King and S. Srinivasa},
  title    = {{Convergent Planning}},
  journal  = ral,
  volume   = {1},
  number   = {2},
  pages    = {1044--1051},
  year     = 2016,
  keywords = {Motion and Path Planning, Manipulation Planning, Field Robots},
  abstract = {We propose a number of divergence metrics to quantify the robustness of a trajectory to state uncertainty for under-actuated or under-sensed systems. These metrics are inspired by contraction analysis and we demonstrate their use to guide randomized planners towards more convergent trajectories through three extensions to the kinodynamic RRT. The first strictly thresholds action selection based on these metrics, forcing the planner to find a solution that lies within a contraction region over which all initial conditions converge exponentially to a single trajectory. However, finding such a monotonically contracting plan is not always possible. Thus, we propose a second method that relaxes these strict requirements to find convergent (i.e. low-divergence) plans. The third algorithm uses these metrics for post-planning path selection. Two examples test the ability of these metrics to lead the planners to more robust trajectories: a mobile robot climbing a hill and a manipulator rearranging objects on a table.}
}

@article{osswald2016ral,
  author   = {S. Osswald and M. Bennewitz and W. Burgard and C. Stachniss},
  title    = {{Speeding-Up Robot Exploration by Exploiting Background Information}},
  journal  = ral,
  volume   = {1},
  number   = {2},
  pages    = {716--723},
  year     = 2016,
  keywords = {Mapping, Motion and Path Planning},
  url      = {http://www.ipb.uni-bonn.de/wp-content/papercite-data/pdf/osswald16ral.pdf},
  abstract = {The ability to autonomously learn a model of an environment is an important capability of a mobile robot. In this paper, we investigate the problem of exploring a scene given background information in form of a topo-metric graph of the environment. Our method is relevant for several real-world applications in which the rough structure of the environment is known beforehand. We present an approach that exploits such background information and enables a robot to cover the environment with its sensors faster compared to a greedy exploration system without this information. We implemented our exploration system in ROS and evaluated it in different environments. As the experimental results demonstrate, our proposed method significantly reduces the overall trajectory length needed to cover the environment with the robots sensors and thus yields a more efficient exploration strategy compared to state-of-the-art greedy exploration, if the additional information is available.}
}

@article{fulhammer2016ral,
  author   = {T. Fulhammer and R. Ambrus and C. Burbridge and M. Zillich and J. Folkesson and N. Hawes and P. Jensfelt and M. Vincze},
  title    = {{Autonomous Learning of Object Models on a Mobile Robot}},
  journal  = ral,
  volume   = {2},
  number   = {1},
  pages    = {26--33},
  year     = 2016,
  keywords = {Visual Learning, Object detection, Segmentation, Categorization, Autonomous Agents},
  abstract = {In this article we present and evaluate a system which allows a mobile robot to autonomously detect, model and re-recognize objects in everyday environments. Whilst other systems have demonstrated one of these elements, to our knowledge we present the first system which is capable of doing all of these things, all without human interaction, in normal indoor scenes. Our system detects objects to learn by modelling the static part of the environment and extracting dynamic elements. It then creates and executes a view plan around a dynamic element to gather additional views for learning. Finally these views are fused to create an object model. The performance of the system is evaluated on publicly available datasets as well as on data collected by the robot in both controlled and uncontrolled scenarios.}
}

@article{adarve2016ral,
  author   = {J.D. Adarve and R. Mahony},
  title    = {{A Filter Formulation for Computing Real Time Optical Flow}},
  journal  = ral,
  volume   = {1},
  number   = {2},
  pages    = {1192--1199},
  year     = 2016,
  keywords = {Computer Vision for Transportation, Visual Tracking, Computer Vision for Other Robotic Applications},
  abstract = {Dense optical flow is a crucial visual cue for obstacle avoidance and motion control for robotic systems functioning in complex unstructured environments. In order to compute image motion for modern highly dynamic robots, it is necessary to use high speed vision systems. To perceive small and thin objects such as tree branches, fence poles, and other similar objects, high resolution vision systems must be used. The data stream from a high-resolution, high-speed vision system will saturate the onboard data bus and overload the onboard processor of almost all existing robotic systems. To implement such a vision system, we believe it is necessary to process the data in situ at the camera using dedicated processing hardware, a vision processing paradigm that we term Rapid Embedded Vision (REV) systems. Even then, the task of processing multiple hundred hertz of dense high-resolution images is not possible using classical algorithms and classical computing hardware. A new processing paradigm that actively exploits the high frame rate of the image sequences, and high performance computing hardware such as GPU or FPGA must be employed. This paper proposes a filtering algorithm for the computation of dense optical flow fields in real time. The filter is designed as a pyramidal structure of update and propagation loops, where an optical flow state is constantly refined with new image data from the camera. The computational properties of the proposed algorithm makes it well suited for implementation in GPU and FPGA systems. We present results from a GPU implementation achieving frame rates in the order of 800 Hz at VGA resolution. Experimental validation and comparison is provided for both synthetic and real life high speed video sequences at frame rates of 300 Hz and 1016 544 pixel resolution.}
}

@article{costante2016ral,
  author   = {G. Costante and M. Mancini and P. Valigi and T.A. Ciarfuglia},
  title    = {{Exploring Representation Learning with CNNs for Frame to Frame Ego-Motion Estimation}},
  journal  = ral,
  volume   = {1},
  number   = {1},
  pages    = {18--25},
  year     = 2016,
  keywords = {Visual Learning, Visual-Based Navigation, CNN},
  abstract = {Visual Ego-Motion Estimation, or briefly Visual Odometry (VO), is one of the key building blocks of modern SLAM systems. In the last decade, impressive results have been demonstrated in the context of visual navigation, reaching very high localization performance. However, all ego-motion estimation systems require careful parameter tuning procedures for the specific environment they have to work in. Furthermore, even in ideal scenarios, most state-of-the-art approaches fail to handle image anomalies and imperfections, which results in less robust estimates. VO systems that rely on geometrical approaches extract sparse or dense features and match them to perform Frame to Frame (F2F) motion estimation. However, images contain much more information that can be used to further improve the F2F estimation. To learn new feature representation a very successful approach is to use deep Convolutional Neural Networks. Inspired by recent advances in Deep Networks and by previous work on learning methods applied to VO, we explore the use of Convolutional Neural Networks to learn both the best visual features and the best estimator for the task of visual Ego-Motion Estimation. With experiments on publicly available datasets we show that our approach is robust with respect to blur, luminance and contrast anomalies and outperforms most state-of-the-art approaches even in nominal conditions.}
}

@article{cadena2016tro,
  author   = {C. Cadena and L. Carlone and H. Carrillo and Y. Latif and D. Scaramuzza and J. Neira and I. Reid and J.J. Leonard},
  title    = {{Past, Present, and Future of Simultaneous Localization And Mapping: Towards the Robust-Perception Age}},
  journal  = tro,
  volume   = 32,
  number   = 6,
  year     = 2016,
  pages    = {1309--1332},
  keywords = {SLAM, Survey},
  url      = {http://arxiv.org/pdf/1606.05830v4}
}

@article{abdo2016ijrr,
  title   = {{Organizing Objects by Predicting User Preferences Through Collaborative Filtering}},
  author  = {N. Abdo and C. Stachniss and L. Spinello and W. Burgard},
  journal = ijrr,
  volume  = {35},
  number  = {13},
  pages   = {1587--1608},
  year    = {2016},
  url     = {http://arxiv.org/pdf/1512.06362}
}

@article{beekmans2016acp,
  title    = {{Cloud Photogrammetry with Dense Stereo for Fisheye Cameras}},
  author   = {C. Beekmans and J. Schneider and T. L\"abe and M. Lennefer and C. Stachniss and C. Simmer},
  journal  = {Atmospheric Chemistry and Physics (ACP)},
  year     = {2016},
  number   = {22},
  pages    = {14231--14248},
  volume   = {16},
  abstract = {We present a novel approach for dense 3-D cloud reconstruction above an area of 10â¯Ãâ10â¯km2 using two hemispheric sky imagers with fisheye lenses in a stereo setup. We examine an epipolar rectification model designed for fisheye cameras, which allows the use of efficient out-of-the-box dense matching algorithms designed for classical pinhole-type cameras to search for correspondence information at every pixel. The resulting dense point cloud allows to recover a detailed and more complete cloud morphology compared to previous approaches that employed sparse feature-based stereo or assumed geometric constraints on the cloud field. Our approach is very efficient and can be fully automated. From the obtained 3-D shapes, cloud dynamics, size, motion, type and spacing can be derived, and used for radiation closure under cloudy conditions, for example. Fisheye lenses follow a different projection function than classical pinhole-type cameras and provide a large field of view with a single image. However, the computation of dense 3-D information is more complicated and standard implementations for dense 3-D stereo reconstruction cannot be easily applied. Together with an appropriate camera calibration, which includes internal camera geometry, global position and orientation of the stereo camera pair, we use the correspondence information from the stereo matching for dense 3-D stereo reconstruction of clouds located around the cameras. We implement and evaluate the proposed approach using real world data and present two case studies. In the first case, we validate the quality and accuracy of the method by comparing the stereo reconstruction of a stratocumulus layer with reflectivity observations measured by a cloud radar and the cloud-base height estimated from a Lidar-ceilometer. The second case analyzes a rapid cumulus evolution in the presence of strong wind shear.},
  url      = {http://www.ipb.uni-bonn.de/pdfs/beekmans16acp.pdf}
}

@inproceedings{bogoslavskyi2016iros,
  title     = {Fast Range Image-Based Segmentation of Sparse 3D Laser Scans for Online Operation},
  author    = {I. Bogoslavskyi and C. Stachniss},
  booktitle = iros,
  year      = {2016},
  url       = {http://www.ipb.uni-bonn.de/pdfs/bogoslavskyi16iros.pdf},
  codeurl   = {https://github.com/Photogrammetry-Robotics-Bonn/depth_clustering},
  videourl  = {https://www.youtube.com/watch?v=6WqsOlHGTLA}
}

@inproceedings{foerstner2016icpr,
  title     = {{A Future for Learning Semantic Models of Man-Made Environments}},
  author    = {W. F{\"o}rstner},
  booktitle = icpr,
  year      = {2016},
  abstract  = {Deriving semantic 3D models of man-made environments hitherto has not reached the desired maturity which makes human interaction obsolete. Man-made environments play a central role in navigation, city planning, building management systems, disaster management or augmented reality. They are characterised by rich geometric and semantic structures. These cause conceptual problems when learning generic models or when developing automatic acquisition systems. The problems appear to be caused by (1) the incoherence of the models for signal analysis, (2) the type of interplay between discrete and continuous geometric representations, (3) the inefficiency of the interaction between crisp models, such as partonomies and taxonomies, and soft models, mostly having a probabilistic nature, and (4) the vagueness of the used notions in the envisaged application domains. The paper wants to encourage the development and learning of generative models, specifically for man-made objects, to be able to understand, reason about, and explain interpretations.},
  url       = {http://www.ipb.uni-bonn.de/pdfs/foerstner16Future.pdf}
}

@inproceedings{merfels2016iros,
  title     = {Pose Fusion with Chain Pose Graphs for Automated Driving},
  author    = {Ch. Merfels and C. Stachniss},
  booktitle = iros,
  year      = {2016},
  url       = {http://www.ipb.uni-bonn.de/pdfs/merfels16iros.pdf}
}

@inproceedings{nardi2016iros,
  title     = {{Experience-Based Path Planning for Mobile Robots Exploiting User Preferences}},
  author    = {L. Nardi and C. Stachniss},
  booktitle = iros,
  year      = {2016},
  keywords  = {Planning, Navigation, Mobile Robots},
  abstract  = {The demand for flexible industrial robotic solutions that are able to accomplish tasks at different locations in a factory is growing more and more. When deploying mobile robots in a factory environment, the predictability and reproducibility of their behaviors become important and are often requested. In this paper, we propose an easy-to-use motion planning scheme that can take into account user preferences for robot navigation. The preferences are extracted implicitly from the previous experiences or from demonstrations and are automatically considered in the subsequent planning steps. This leads to reproducible and thus better to predict navigation behaviors of the robot, without requiring experts to hard-coding control strategies or cost functions within a planner. Our system has been implemented and evaluated on a simulated KUKA mobile robot in different environments.},
  url       = {http://www.ipb.uni-bonn.de/pdfs/nardi16iros.pdf}
}

@inproceedings{perea2016jras,
  title     = {{Robust Exploration and Homing for Autonomous Robots}},
  author    = {D. Perea-Str{\"o}m and I. Bogoslavskyi and C. Stachniss},
  booktitle = jras,
  year      = {2016},
  url       = {http://www.ipb.uni-bonn.de/wp-content/papercite-data/pdf/perea16jras.pdf}
}

@inproceedings{roscher2016isprsannals,
  title     = {Detection of Disease Symptoms on Hyperspectral {3D} Plant Models},
  author    = {Roscher, R. and Behmann, J. and Mahlein, A.-K. and Dupuis, J. and Kuhlmann, H. and Pl{\"u}mer, L.},
  booktitle = isprsannals,
  year      = {2016},
  abstract  = {We analyze the benefit of combining hyperspectral images information with 3D geometry information for the detection of Cercospora leaf spot disease symptoms on sugar beet plants. Besides commonly used one-class Support Vector Machines, we utilize an unsupervised sparse representation-based approach with group sparsity prior. Geometry information is incorporated by representing each sample of interest with an inclination-sorted dictionary, which can be seen as an 1D topographic dictionary. We compare this approach with a sparse representation based approach without geometry information and One-Class Support Vector Machines. One-Class Support Vector Machines are applied to hyperspectral data without geometry information as well as to hyperspectral images with additional pixelwise inclination information. Our results show a gain in accuracy when using geometry information beside spectral information regardless of the used approach. However, both methods have different demands on the data when applied to new test data sets. One-Class Support Vector Machines require full inclination information on test and training data whereas the topographic dictionary approach only need spectral information for reconstruction of test data once the dictionary is build by spectra with inclination.}
}

@inproceedings{roscher2016prrs,
  title     = {Discriminative Archetypal Self-taught Learning for Multispectral Landcover Classification},
  author    = {Roscher, R. and Wenzel, S. and Waske, B.},
  booktitle = {Proc. of Pattern Recogniton in Remote Sensing 2016 (PRRS)},
  year      = {2016},
  abstract  = {Self-taught learning (STL) has become a promising paradigm to exploit unlabeled data for classification. The most commonly used approach to self-taught learning is sparse representation, in which it is assumed that each sample can be represented by a weighted linear combination of elements of a unlabeled dictionary. This paper proposes discriminative archetypal self-taught learning for the application of landcover classification, in which unlabeled discriminative archetypal samples are selected to build a powerful dictionary. Our main contribution is to present an approach which utilizes reversible jump Markov chain Monte Carlo method to jointly determine the best set of archetypes and the number of elements to build the dictionary. Experiments are conducted using synthetic data, a multi-spectral Landsat 7 image of a study area in the Ukraine and the Zurich benchmark data set comprising 20 multispectral Quickbird images. Our results confirm that the proposed approach can learn discriminative features for classification and show better classification results compared to self-taught learning with the original feature representation and compared to randomly initialized archetypal dictionaries.},
  url       = {http://www.ipb.uni-bonn.de/pdfs/Roscher2016Discriminative.pdf}
}

@inproceedings{schneider2016icra,
  title     = {{Fast and Effective Online Pose Estimation and Mapping for UAVs}},
  author    = {J. Schneider and C. Eling and L. Klingbeil and H. Kuhlmann and W. F\"orstner and C. Stachniss},
  booktitle = icra,
  year      = {2016},
  keywords  = {SLAM, Localization, Aerial Robotics},
  abstract  = {Online pose estimation and mapping in unknown environments is essential for most mobile robots. Especially autonomous unmanned aerial vehicles require good pose estimates at comparably high frequencies. In this paper, we propose an effective system for online pose and simultaneous map estimation designed for light-weight UAVs. Our system consists of two components: (1) real-time pose estimation combining RTK-GPS and IMU at 100 Hz and (2) an effective SLAM solution running at 10 Hz using image data from an omnidirectional multi-fisheye-camera system. The SLAM procedure combines spatial resection computed based on the map that is incrementally refined through bundle adjustment and combines the image data with raw GPS observations and IMU data on keyframes. The overall system yields a real-time, georeferenced pose at 100 Hz in GPS-friendly situations. Additionally, we obtain a precise pose and feature map at 10 Hz even in cases where the GPS is not observable or underconstrained. Our system has been implemented and thoroughly tested on a 5 kg copter and yields accurate and reliable pose estimation at high frequencies. We compare the point cloud obtained by our method with a model generated from georeferenced terrestrial laser scanner.},
  url       = {http://www.ipb.uni-bonn.de/pdfs/schneider16icra.pdf}
}

@article{schneider2016ral,
  title    = {{On the Accuracy of Dense Fisheye Stereo}},
  author   = {J. Schneider and C. Stachniss and W. F\"orstner},
  journal  = ral,
  year     = {2016},
  number   = {1},
  pages    = {227--234},
  volume   = {1},
  keywords = {Range Sensing, Mapping, Computer Vision for Other Robotic Applications},
  abstract = {Fisheye cameras offer a large field of view, which is important for several robotics applications as a larger field of view allows for covering a large area with a single image. In contrast to classical cameras, however, fisheye cameras cannot be approximated well using the pinhole camera model and this renders the computation of depth information from fisheye stereo image pairs more complicated. In this work, we analyze the combination of an epipolar rectification model for fisheye stereo cameras with existing dense methods. This has the advantage that existing dense stereo systems can be applied as a black-box even with cameras that have field of view of more than 180 deg to obtain dense disparity information. We thoroughly investigate the accuracy potential of such fisheye stereo systems using image data from our UAV. The empirical analysis is based on image pairs of a calibrated fisheye stereo camera system and two state-of-the-art algorithms for dense stereo applied to adequately rectified image pairs from fisheye stereo cameras. The canonical stochastic model for sensor points assumes homogeneous uncertainty and we generalize this model based on an empirical analysis using a test scene consisting of mutually orthogonal planes. We show (1) that the combination of adequately rectified fisheye image pairs and dense methods provides dense 3D point clouds at 6-7 Hz on our autonomous multi-copter UAV, (2) that the uncertainty of points depends on their angular distance from the optical axis, (3) how to estimate the variance component as a function of that distance, and (4) how the improved stochastic model improves the accuracy of the scene points.},
  url      = {http://www.ipb.uni-bonn.de/pdfs/schneider16ral.pdf}
}

@inproceedings{schubert2016isprsannals,
  title     = {{Investigation of Latent Traces Using Infrared Reflectance Hyperspectral Imaging}},
  author    = {Schubert, Till and Wenzel, Susanne and Roscher, Ribana and Stachniss, Cyrill},
  booktitle = isprsannals,
  year      = {2016},
  abstract  = {The detection of traces is a main task of forensic science. A potential method is hyperspectral imaging (HSI) from which we expect to capture more fluorescence effects than with common Forensic Light Sources (FLS). Specimen of blood, semen and saliva traces in several dilution steps are prepared on cardboard substrate. As our key result we successfully make latent traces visible up to highest available dilution (1:8000). We can attribute most of the detectability to interference of electromagnetic light with the water content of the traces in the Shortwave Infrared region of the spectrum. In a classification task we use several dimensionality reduction methods (PCA and LDA) in combination with a Maximum Likelihood (ML) classifier assuming normally distributed data. Random Forest builds a competitive approach. The classifiers retrieve the exact positions of labeled trace preparation up to highest dilution and determine posterior probabilities. By modeling the classification with a Markov Random Field we obtain smoothed results.},
  url       = {http://www.ipb.uni-bonn.de/pdfs/Schubert2016Investigation.pdf}
}

@inbook{siedentop2016lnib,
  author  = {C. Siedentop and V. Laukhart and B. Krastev and D. Kasper and A. Wenden and G. Breuel and C. Stachniss},
  chapter = {Autonomous Parking Using Previous Paths},
  editor  = {T. Schulze and B. M{\"u}ller and G. Meyer},
  title   = {Advanced Microsystems for Automotive Applications 2015: Smart Systems for Green and Automated Driving. Lecture Notes in Mobility.},
  year    = 2016,
  pages   = {3--14}
}

@inproceedings{liebisch2016wslw,
  author    = {F. Liebisch and J. Pfeifer and R. Khanna and P. Lottes and C. Stachniss and T. Falck and S. Sander and R. Siegwart and A. Walter and E. Galceran},
  title     = {{Flourish -- A robotic approach for automation in crop management}},
  booktitle = {In Proc.~of the Workshop f\"ur Computer-Bildanalyse und unbemannte autonom fliegende Systeme in der Landwirtschaft},
  year      = 2016
}

@inbook{stachniss2016handbookphoto,
  author    = {C. Stachniss},
  alteditor = {C. Heipke},
  title     = {Springer Handbuch der Geod{\"a}sie},
  chapter   = {Simultaneous Localization and Mapping},
  publisher = springer,
  note      = {In German, invited.},
  doi       = {10.1007/978-3-662-46900-2_49-2},
  year      = 2016,
  abstract  = {Dieses Kapitel gibt eine EinfÃ¼hrung in die Kartenerstellung und gleichzeitige Lokalisierung mobiler Sensorplattformen. Die gemeinsame LÃ¶sung dieser beiden Probleme ist eine Voraussetzung fÃ¼r die Realisierung vieler technischer Systeme von leichten FluggerÃ¤ten Ã¼ber autonome Roboter bis hin zu mobilen Kameras. Als Simultaneous Localization and Mapping bezeichnet man die Aufgabe, die Trajektorie samt Orientierungsinformation einer sich bewegenden Plattform aus Beobachtungen zu schÃ¤tzen und gleichzeitig eine Karte der Umgebung zu erstellen. Diese Aufgabe ist in vielen realen Systemen von entscheidender Bedeutung: einerseits stellen hochgenaue Karten mitunter einen Wert an sich fÃ¼r den Benutzer oder eine spezielle Anwendung dar, andererseits benÃ¶tigen beispielsweise autonome Roboter ein solches Modell, um zielgerichtet selbststÃ¤ndig navigieren zu kÃ¶nnen. Das Simultaneous Localization and Mapping Problem, beziehungsweise Teilprobleme davon, werden, je nach verwendeter Sensorik, auch als BÃ¼ndelausgleichung, Structure from Motion oder SLAM bezeichnet. In diesem Kapitel werden wir verschiedene AnsÃ¤tze vorstellen, mit denen man das SLAM Problem adressieren kann. Dies beinhaltet neben dem klassischen Verfahren mittels Ausgleichung, welches offline auf allen Daten operiert, auch Filtertechniken wie den Kalman-Filter und den Partikel-Filter, die zu den Onlineverfahren zÃ¤hlen. Bei der Verwendung der Kleinsten-Quadrate Methode sowie beim Kalman-Filter wird meist eine Normalverteilung beziehungsweise eine unimodale Verteilung Ã¼ber die Positionen der 3D-Punkte in der Umgebung und die Orientierung des Sensors geschÃ¤tzt. Im Gegensatz dazu arbeitet der Partikel-Filter nichtparametrisch und kann multiple Hypothesen Ã¼ber mÃ¶gliche Datenassoziationen parallel schÃ¤tzen. Neben den einzusetzenden SchÃ¤tzverfahren wird auch skizziert, wie SLAM Systeme mit unterschiedlichen Sensoren realisiert werden kÃ¶nnen.}
}

@inproceedings{seiskari2022wacv,
  title     = {{Hybvio: Pushing the limits of real-time visual-inertial odometry}},
  author    = {Seiskari, Otto and Rantalankila, Pekka and Kannala, Juho and Ylilammi, Jerry and Rahtu, Esa and Solin, Arno},
  booktitle = wacv,
  year      = {2022},
  url       = {https://openaccess.thecvf.com/content/WACV2022/papers/Seiskari_HybVIO_Pushing_the_Limits_of_Real-Time_Visual-Inertial_Odometry_WACV_2022_paper.pdf}
}

@inbook{stachniss2016handbook-slamchapter,
  author    = {C. Stachniss and J. Leonard and S. Thrun},
  editor    = {B. Siciliano and O. Khatib},
  title     = {{Springer Handbook of Robotics, 2nd edition}},
  chapter   = {Chapt.~46: Simultaneous Localization and Mapping},
  publisher = springer,
  year      = 2016,
  abstract  = {This chapter provides a comprehensive introduction into one of the key enabling technologies of mobile robot navigation: simultaneous localization and mapping, or in short SLAM. SLAM addresses the problem of acquiring a spatial map of an environment while simultaneously localizing the robot relative to this model. The SLAM problem is generally regarded as one of the most important problems in the pursuit of building truly autonomous mobile robots. It is of great practical importance; if a robust, general-purpose solution to SLAM can be found, then many new applications of mobile robotics will become possible.}
}

@article{sprunk2017auro,
  author  = {C. Sprunk and B. Lau and P. Pfaff and W. Burgard},
  title   = {{An Accurate and Efficient Navigation System for Omnidirectional Robots in Industrial Environments}},
  journal = ar,
  volume  = {41},
  pages   = {473--493},
  year    = 2017,
  url     = {http://ais.informatik.uni-freiburg.de/publications/papers/sprunk16auro.pdf}
}

@article{kretzschmar2016ijrr,
  author  = {H. Kretzschmar and M. Spies and C. Sprunk and W. Burgard},
  title   = {{Socially Compliant Mobile Robot Navigation via Inverse Reinforcement Learning}},
  journal = ijrr,
  volume  = {35},
  number  = {11},
  pages   = {1289--1307},
  year    = 2016,
  url     = {http://ais.informatik.uni-freiburg.de/publications/papers/kretzschmar16ijrr.pdf}
}

@book{foerstner2016pcvbook,
  author    = {W. F{\"o}rstner and B. Wrobel},
  title     = {{Photogrammetric Computer Vision -- Statistics, Geometry, Orientation and Reconstruction}},
  publisher = springer,
  year      = 2016,
  keywords  = {Book, Computer Vision, Photogrammetry, Geometry, State Estimation},
  abstract  = {This textbook on Photogrammetric Computer Vision â Statistics, Geometry, Orientation and Reconstruction provides a statistical treatment of the geometry of multiple view anal- ysis useful for camera calibration, orientation, and geometric scene reconstruction. The book is the first to offer a joint view of photogrammetry and computer vision, two fields that have converged in recent decades. It is motivated by the need for a conceptually consistent theory aiming at generic solutions for orientation and reconstruction problems. Large parts of the book result from teaching bachelorâs and masterâs courses for students of geodesy within their education in photogrammetry. Most of these courses were simultaneously offered as subjects in the computer science faculty.The book provides algorithms for various problems in geometric computation and in vision metrology, together with mathematical justification and statistical analysis allowing thorough evaluation. The book aims at enabling researchers, software developers, and practitioners in the photogrammetric and GIS industry to design, write, and test their own algorithms and application software using statistically founded concepts to obtain optimal solutions and to realize self-diagnostics within algorithms. This is essential when applying vision tech- niques in practice. The material of the book can serve as a source for different levels of undergraduate and graduate courses in photogrammetry, computer vision, and computer graphics, and for research and development in statistically based geometric computer vision methods. The sixteen chapters of the book are self-contained, are illustrated with numerous fig- ures, have exercises, and are supported by an appendix and an index. Many of the examples and exercises can be verified or solved using the Matlab routines available on the home page of the book, which also contains solutions to some of the exercises.}
}

@article{dou2016acmgraphics,
  author   = {M. Dou and S. Khamis and Y. Degtyarev and P. Davidson and S.R. Fanello and A. Kowdle and S.O. Escolano and C. Rhemann and D. Kim and J. Taylor and P. Kohli and V. Tankovich and S. Izadi},
  title    = {{Fusion4D: Real-time Performance Capture of Challenging Scenes}},
  journal  = acmgraphics,
  year     = 2016,
  volume   = 35,
  number   = 4,
  keywords = {Kinect, Mapping, Dynamic},
  url      = {http://dl.acm.org/ft_gateway.cfm?id=2925969&ftid=1755905&dwn=1&CFID=967720923&CFTOKEN=61468293},
  abstract = {We contribute a new pipeline for live multi-view performance capture, generating temporally coherent high-quality reconstructions in real-time. Our algorithm supports both incremental reconstruction, improving the surface estimation over time, as well as parameterizing the nonrigid scene motion. Our approach is highly robust to both large frame-to-frame motion and topology changes, allowing us to reconstruct extremely challenging scenes. We demonstrate advantages over related real-time techniques that either deform an online generated template or continually fuse depth data nonrigidly into a single reference model. Finally, we show geometric reconstruction results on par with offline methods which require orders of magnitude more processing time and many more RGBD cameras.}
}

@inproceedings{hess2016icra,
  author    = {W. Hess and D. Kohler and H. Rapp and D. Andor},
  title     = {{Real-Time Loop Closure in 2D LIDAR SLAM}},
  booktitle = icra,
  year      = 2016,
  keywords  = {SLAM, Range Sensing},
  abstract  = {Portable laser range-finders, further referred to as LIDAR, and simultaneous localization and mapping (SLAM) are an efficient method of acquiring as-built floor plans. Generating and visualizing floor plans in real-time helps the operator assess the quality and coverage of capture data. Building a portable capture platform necessitates operating under limited computational resources. We present the approach used in our backpack mapping platform which achieves real-time mapping and loop closure at a 5 cm resolution. To achieve realtime loop closure, we use a branch-and-bound approach for computing scan-to-submap matches as constraints. We provide experimental results and comparisons to other well known approaches which show that, in terms of quality, our approach is competitive with established techniques.},
  url       = {proceedings:hess2016icra.pdf}
}

@article{dai2017tog,
  author   = {A. Dai and M. Nie{\ss}ner and M. Zollh{\"o}fer and S. Izadi and C. Theobalt},
  title    = {{BundleFusion: Real-time Globally Consistent 3D Reconstruction using Online Surface Re-integration}},
  journal  = tog,
  volume   = {36},
  number   = {3},
  pages    = {1--18},
  year     = {2017},
  keywords = {RGB-D Perception, SLAM, Mapping},
  url      = {https://arxiv.org/pdf/1604.01093v1.pdf}
}


@article{qin2016jprs,
  author  = {Qin, R. and Tian, J. and Reinartz, P.},
  journal = jprs,
  pages   = {41--56},
  title   = {{3D Change Detection -- Approaches and Applications}},
  volume  = {122},
  year    = {2016},
  url     = {http://elib.dlr.de/107163/1/Qin_3D-change-detection-approaches-and-applications-ueqeb5%20(2).pdf}
}

@inproceedings{alcantarilla2016rss,
  title     = {{Street-View Change Detection with Deconvolutional Networks}},
  author    = {Alcantarilla, P.F. and Stent, S. and Ros, G. and Arroyo, R. and Gherardi, R.},
  booktitle = rss,
  year      = {2016},
  url       = {https://www.researchgate.net/profile/Roberto_Arroyo2/publication/304533064_Street-View_Change_Detection_with_Deconvolutional_Networks/links/5773858508ae6f328f6c29e4.pdf}
}

@inproceedings{roggiolani2022icra,
  title     = {{Hierarchical Approach for Joint Semantic, Plant Instance, and Leaf Instance Segmentation in the Agricultural Domain}},
  author    = {Roggiolani, Gianmarco and Sodano, Matteo and Guadagnino, Tiziano and Magistri, Federico and Behley, Jens and Stachniss, Cyrill},
  booktitle = icra,
  year      = {2023},
  url       = {https://www.ipb.uni-bonn.de/wp-content/papercite-data/pdf/roggiolani2023icra-hajs.pdf}
}

@inproceedings{roberts2021iccv-haps,
  author    = {M. Roberts and J. Ramapuram and A. Ranjan and A. Kumar and M. A. Bautista and N. Paczan and R. Webb and J. M. Susskind},
  title     = {{Hypersim: A Photorealistic Synthetic Dataset for Holistic Indoor Scene Understanding}},
  booktitle = iccv,
  year      = 2021,
  abstract  = {For many fundamental scene understanding tasks, it is difficult or impossible to obtain per-pixel ground truth labels from real images. We address this challenge by introducing Hypersim, a photorealistic synthetic dataset for holistic indoor scene understanding. To create our dataset, we leverage a large repository of synthetic scenes created by professional artists, and we generate 77,400 images of 461 indoor scenes with detailed per-pixel labels and corresponding ground truth geometry. Our dataset: (1) relies exclusively on publicly available 3D assets; (2) includes complete scene geometry, material information, and lighting information for every scene; (3) includes dense per-pixel semantic instance segmentations and complete camera information for every image; and (4) factors every image into diffuse reflectance, diffuse illumination, and a non-diffuse residual term that captures view-dependent lighting effects. We analyze our dataset at the level of scenes, objects, and pixels, and we analyze costs in terms of money, computation time, and annotation effort. Remarkably, we find that it is possible to generate our entire dataset from scratch, for roughly half the cost of training a popular open-source natural language processing model. We also evaluate sim-to-real transfer performance on two real-world scene understanding tasks - semantic segmentation and 3D shape prediction - where we find that pre-training on our dataset significantly improves performance on both tasks, and achieves state-of-the-art performance on the most challenging Pix3D test set. All of our rendered image data, as well as all the code we used to generate our dataset and perform our experiments, is available online.},
  url       = {proceedings: roberts2021iccv-haps.pdf}
}

@inproceedings{griffith2016bmvc,
  author    = {S. Griffith and C. Pradalier},
  title     = {Reprojection Flow for Image Registration Across Seasons},
  booktitle = bmvc,
  year      = {2016},
  url       = {https://pdfs.semanticscholar.org/2e1d/97e9d54c075aea2015643281024f91f94f45.pdf}
}

@article{lowry2016tro,
  author  = {S. Lowry and N. Sunderhauf and P. Newman and J.J Leonard and D. Cox and P. Corke and M.J. Milford},
  title   = {{Visual Place Recognition: A Survey}},
  journal = tro,
  volume  = {32},
  number  = {1},
  year    = {2016},
  pages   = {1--19},
  url     = {https://eprints.qut.edu.au/105651/1/visual_place_recognition_a_survey_revised_final.pdf}
}

@inproceedings{pfeifer2016cigr,
  title     = {Towards automatic UAV data interpretation for precision farming},
  author    = {J. Pfeifer and R. Khanna and C. Dragos and M. Popovic and E. Galceran and N. Kirchgessner and A. Walter and R. Siegwart and F. Liebisch},
  booktitle = {Proc. of the Conf. of Agricultural Engineering (CIGR)},
  year      = {2016},
  url       = {http://flourish-project.eu/fileadmin/user_upload/publications/2016_Pfeifer-UAV_data_interpretation.pdf}
}

@article{maye2016ijrr,
  author   = {J. Maye and H. Sommer and G. Agamennoni and R. Siegwart and P. Furgale},
  title    = {Online self-calibration for robotic systems},
  journal  = ijrr,
  year     = 2016,
  pages    = {357--380},
  volume   = 35,
  number   = {4},
  abstract = {We present a generic algorithm for self-calibration of robotic systems that utilizes two key innovations. First, it uses an information-theoretic measure to automatically identify and store novel measurement sequences. This keeps the computation tractable by discarding redundant information and allows the system to build a sparse but complete calibration dataset from data collected at different times. Second, as the full observability of the calibration parameters may not be guaranteed for an arbitrary measurement sequence, the algorithm detects and locks unobservable directions in parameter space using a combination of rank-revealing QR and singular value decompositions of the Fisher information matrix. The result is an algorithm that listens to an incoming sensor stream, builds a minimal set of data for estimating the calibration parameters, and updates parameters as they become observable, leaving the others locked at their initial guess. We validate our approach through an extensive set of simulated and real-world experiments. }
}

@article{hamuda2016cea,
  author  = {Hamuda, E. and Glavin, M. and Jones, E.},
  title   = {A Survey of Image Processing Techniques for Plant Extraction and Segmentation in the Field},
  journal = cea,
  volume  = {125},
  year    = {2016},
  pages   = {184--199}
}

@inproceedings{mortensen2016cigr,
  title     = {{Semantic Segmentation of Mixed Crops using Deep Convolutional Neural Network}},
  author    = {A.K. Mortensen and M. Dyrmann and H. Karstoft and R. N. J{\"o}rgensen and R. Gislum},
  booktitle = cigr,
  year      = 2016
}

@inproceedings{potena2016ias,
  author    = {Potena, C. and Nardi, D. and Pretto, A.},
  title     = {Fast and Accurate Crop and Weed Identification with Summarized Train Sets for Precision Agriculture},
  booktitle = ias,
  year      = {2016},
  url       = {http://www.dis.uniroma1.it/~pretto/papers/pnp_ias2016.pdf}
}

@article{cicco2016arxiv,
  author   = {Di Cicco, M. and C. Potena and G. Grisetti and A. Pretto},
  title    = {{Automatic Model Based Dataset Generation for Fast and Accurate Crop and Weeds Detection}},
  journal  = arxiv,
  year     = {2016},
  volume   = {arXiv:1612.03019},
  url      = {http://arxiv.org/pdf/1612.03019v3},
  abstract = {Selective weeding is one of the key challenges in the field of agriculture robotics. To accomplish this task, a farm robot should be able to accurately detect plants and to distinguish them between crop and weeds. Most of the promising state-of-the-art approaches make use of appearance-based models trained on large annotated datasets. Unfortunately, creating large agricultural datasets with pixel-level annotations is an extremely time consuming task, actually penalizing the usage of data-driven techniques. In this paper, we face this problem by proposing a novel and effective approach that aims to dramatically minimize the human intervention needed to train the detection and classification algorithms. The idea is to procedurally generate large synthetic training datasets randomizing the key features of the target environment (i.e., crop and weed species, type of soil, light conditions). More specifically, by tuning these model parameters, and exploiting a few real-world textures, it is possible to render a large amount of realistic views of an artificial agricultural scenario with no effort. The generated data can be directly used to train the model or to supplement real-world images. We validate the proposed methodology by using as testbed a modern deep learning based image segmentation architecture. We compare the classification results obtained using both real and synthetic images as training data. The reported results confirm the effectiveness and the potentiality of our approach.}
}

@inproceedings{he2016cvpr,
  title     = {{Deep Residual Learning for Image Recognition}},
  booktitle = cvprold,
  author    = {K. He and X. Zhang and S. Ren and J. Sun},
  year      = 2016,
  url       = {https://arxiv.org/pdf/1512.03385},
  abstract  = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8 deeper than VGG nets [41] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet local- ization, COCO detection, and COCO segmentation.}
}

@article{tokekar2016tro,
  title   = {{Sensor Planning for a Symbiotic UAV and UGV System for Precision Agriculture}},
  author  = {P. Tokekar and J.V. Hook and D. Mulla and V. Isler},
  journal = tro,
  volume  = 32,
  number  = 6,
  pages   = {1498--1511},
  year    = 2016
}

@article{cornick2016jfr,
  author   = {M. Cornick and J. Koechling and B. Stanley and B. Zhang},
  journal  = jfr,
  title    = {{Localizing Ground Penetrating RADAR: A Step Toward Robust Autonomous Ground Vehicle Localization}},
  year     = 2016,
  volume   = 33,
  number   = 1,
  pages    = {82--102},
  abstract = {Autonomous ground vehicles navigating on road networks require robust and accurate localization over long-term operation and in a wide range of adverse weather and environmental conditions. GPS/INS (inertial navigation system) solutions, which are insufficient alone to maintain a vehicle within a lane, can fail because of significant radio frequency noise or jamming, tall buildings, trees, and other blockage or multipath scenarios. LIDAR and camera map-based vehicle localization can fail when optical features become obscured, such as with snow or dust, or with changes to gravel or dirt road surfaces. Localizing ground penetrating radar (LGPR) is a new mode of a priori map-based vehicle localization designed to complement existing approaches with a low sensitivity to failure modes of LIDAR, camera, and GPS/INS sensors due to its low-frequency RF energy, which couples deep into the ground. Most subsurface features detected are inherently stable over time. Significant research, discussed herein, remains to prove general utility. We have developed a novel low-profile ultra-low power LGPR system and demonstrated real-time operation underneath a passenger vehicle. A correlation maximizing optimization technique was developed to allow real-time localization at 126 Hz. Here we present the detailed design and results from highway testing, which uses a simple heuristic for fusing LGPR estimates with a GPS/INS system. Cross-track localization accuracies of 4.3 cm RMS relative to a âtruthâ RTK GPS/INS unit at speeds up to 100 km/h (60 mph) are demonstrated. These results, if generalizable, introduce a widely scalable real-time localization method with cross-track accuracy as good as or better than current localization methods},
  keywords = {Localization, Sensing, RADAR}
}

@incollection{nieuwenhuisen2016ias,
  title     = {{Layered Mission and Path Planning for MAV Navigation with Partial Environment Knowledge}},
  author    = {Nieuwenhuisen, M. and Behnke, S.},
  booktitle = {Intelligent Autonomous Systems 13},
  pages     = {307--319},
  year      = {2016},
  publisher = {Springer},
  url       = {https://s3.amazonaws.com/academia.edu.documents/38099200/IAS_2014_Nieuwenhuisen_Layered_Planning.pdf?AWSAccessKeyId=AKIAIWOWYYGZ2Y53UL3A&Expires=1515512575&Signature=rUGJ0m54MelAy9NAea%2BYSsrpcAs%3D&response-content-disposition=inline%3B%20filename%3DLayered_Mission_and_Path_Planning_for_MA.pdf}
}

@article{paustian2016pagri,
  author   = {M. Paustian and L. Theuvsen},
  title    = {Adoption of precision agriculture technologies by German crop farmers},
  journal  = pagri,
  year     = {2016},
  volume   = {18},
  number   = {5},
  pages    = {701--716},
  keywords = {crop farming},
  abstract = {In recent years, precision farming has been receiving more attention from researchers. Precision farming, which provides a holistic system approach, helps farmers to manage the spatial and temporal crop and soil variability within a field in order to increase profitability, optimize yield and quality, and reduce costs. There has been considerable research in farmersâ adoption of precision agriculture technologies. However, most recent studies have considered only a few aspects, whereas in this study a wide range of farm characteristics and farmer demographics are tested to gain insight into the relevant aspects of adoption of precision farming in German crop farming. The results of a logistic regression analysis show that predictors with positive influence on the adoption of precision farming are agricultural contractor services such as an additional farming business, having under 5 yearsâ experience in crop farming, having between 16 and 20 yearsâ experience in crop farming, and having more than 500 ha of arable land. However, having a farm of less than 100 ha and producing barley are factors that exert a negative influence on the adoption of precision farming. The results of this study provide manifold starting points for the further proliferation of precision agriculture technologies and future research directions.}
}

@article{bechar2016biosyseng,
  author   = {A. Bechar and C. Vigneault},
  title    = {{Agricultural robots for field operations: Concepts and components}},
  journal  = biosyseng,
  volume   = 149,
  pages    = {94--111},
  year     = 2016,
  keywords = {Agricultural robots, Robotics, Field operations, Autonomous},
  abstract = {This review investigates the research effort, developments and innovation in agricultural robots for field operations, and the associated concepts, principles, limitations and gaps. Robots are highly complex, consisting of different sub-systems that need to be integrated and correctly synchronised to perform tasks perfectly as a whole and successfully transfer the required information. Extensive research has been conducted on the application of robots and automation to a variety of field operations, and technical feasibility has been widely demonstrated. Agricultural robots for field operations must be able to operate in unstructured agricultural environments with the same quality of work achieved by current methods and means. To assimilate robotic systems, technologies must be developed to overcome continuously changing conditions and variability in produce and environments. Intelligent systems are needed for successful task performance in such environments. The robotic system must be cost-effective, while being inherently safe and reliable human safety, and preservation of the environment, the crop and the machinery are mandatory. Despite much progress in recent years, in most cases the technology is not yet commercially available. Information-acquisition systems, including sensors, fusion algorithms and data analysis, need to be adjusted to the dynamic conditions of unstructured agricultural environments. Intensive research is needed on integrating human operators into the system control loop for increased system performance and reliability. System sizes should be reduced while improving the integration of all parts and components. For robots to perform in agricultural environments and execute agricultural tasks, research must focus on: fusing complementary sensors for adequate localisation and sensing abilities, developing simple manipulators for each agricultural task, developing path planning, navigation and guidance algorithms suited to environments besides open fields and known a-priori, and integrating human operators in this complex and highly dynamic situation.}
}

@article{riegler2016arxiv,
  author   = {G. Riegler and A.O. Ulusoy and A. Geiger},
  title    = {{OctNet: Learning Deep 3D Representations at High Resolutions}},
  journal  = arxiv,
  year     = 2016,
  volume   = {arXiv:1611.05009},
  url      = {http://arxiv.org/pdf/1611.05009v4},
  keywords = {cs.CV},
  abstract = {We present OctNet, a representation for deep learning with sparse 3D data. In contrast to existing models, our representation enables 3D convolutional networks which are both deep and high resolution. Towards this goal, we exploit the sparsity in the input data to hierarchically partition the space using a set of unbalanced octrees where each leaf node stores a pooled feature representation. This allows to focus memory allocation and computation to the relevant dense regions and enables deeper networks without compromising resolution. We demonstrate the utility of our OctNet representation by analyzing the impact of resolution on several 3D tasks including 3D object classification, orientation estimation and point cloud labeling.}
}

@inproceedings{cieslewski2015icra,
  author    = {T. Cieslewski and S. Lynen and M.T. Dymczyk and S. Magnenat and R. Siegwart},
  title     = {{Map API - Scalable Decentralized Map Building for Robots}},
  booktitle = icra,
  year      = 2015,
  keywords  = {Robotic Software, Middleware and Programming Environments, Networked Robots, Mapping},
  abstract  = {Large scale, long-term, distributed mapping is a core challenge to modern field robotics. Using the sensory output of multiple robots and fusing it in an efficient way enables the creation of globally accurate and consistent metric maps. To combine data from multiple agents into a global map, most existing approaches use a central entity that collects and manages the information from all agents. Often, the raw sensor data of one robot needs to be made available to processing algorithms on other agents due to the lack of computational resources on that robot. Unfortunately, network latency and low bandwidth in the field limit the generality of such an approach and make multi-robot map building a tedious task. In this paper, we present a distributed and decentralized back-end for concurrent and consistent robotic mapping. We propose a set of novel approaches that reduce the bandwidth usage and increase the effectiveness of inter-robot communication for distributed mapping. Instead of locking access to the map during operations, we define a version control system which allows concurrent and consistent access to the map data. Updates to the map are then shared asynchronously with agents which previously registered notifications. A technique for data lookup is provided by state-of-the-art algorithms from distributed computing. We validate our approach on real-world datasets and demonstrate the effectiveness of the proposed algorithms.}
}

@inproceedings{dymczyk2015icra,
  author    = {M.T. Dymczyk and S. Lynen and T. Cieslewski and M. Bosse and R. Siegwart and P.T. Furgale},
  title     = {{The Gist of Maps - Summarizing Experience for Lifelong Localization}},
  booktitle = icra,
  year      = 2015,
  keywords  = {Localization, Visual-Based Control and/or Navigation, Mapping},
  abstract  = {Robust, scalable place recognition is a core competency for many robotic applications. However, when revisiting places over and over, many state-of-the-art approaches exhibit reduced performance in terms of computation and memory complexity and in terms of accuracy. For successful deployment of robots over long time scales, we must develop algorithms that get better with repeated visits to the same environment, while still working within a fixed computational budget. This paper presents and evaluates an algorithm that alternates between online place recognition and offline map maintenance with the goal of producing the best performance with a fixed map size. At the core of the algorithm is the concept of a Summary Map, a reduced map representation that includes only the landmarks that are deemed most useful for place recognition. To assign landmarks to the map, we use a scoring function that ranks the utility of each landmark and a sampling policy that selects the landmarks for each place. The Summary Map can then be used by any descriptor-based inference method for constant-complexity online place recognition. We evaluate a number of scoring functions and sampling policies and show that it is possible to build and maintain maps of a constant size and that place-recognition performance improves over multiple visits.}
}

@inproceedings{khan2015icra,
  author    = {S. Khan and D. Wollherr},
  title     = {{IBuILD: Incremental Bag of Binary Words for Appearance Based Loop Closure Detection}},
  booktitle = icra,
  year      = 2015,
  keywords  = {Visual Place Recognition},
  abstract  = {In robotics applications such as SLAM (Simultaneous Localization and Mapping), loop closure detection is an integral component required to build a consistent topological or metric map. This paper presents an appearance based loop closure detection mechanism titled IBuILD (Incremental bag of BInary words for Appearance based Loop closure Detection). The presented approach focuses on an online, incremental formulation of binary vocabulary generation for loop closure detection. The proposed approach does not require a prior vocabulary learning phase and relies purely on the appearance of the scene for loop closure detection without the need of odometry or GPS estimates. The vocabulary generation process is based on feature tracking between consecutive images to incorporate pose invariance. In addition, this process is coupled with a simple likelihood function to generate the most suitable loop closure candidate and a temporal consistency constraint to filter out inconsistent loop closures. Evaluation on different publicly available outdoor urban and indoor datasets shows that the presented approach is capable of generating higher recall at 100\% precision in comparison to the state of the art.}
}

@inproceedings{zhang2015icra,
  author    = {J. Zhang and S. Singh},
  title     = {{Visual-Lidar Odometry and Mapping: Low-Drift, Robust, and Fast}},
  booktitle = icra,
  year      = 2015,
  keywords  = {Visual-Based Control and/or Navigation, Range Sensing, Mapping},
  abstract  = {Here, we present a general framework for combining visual odometry and lidar odometry in a fundamental and first principle method. The method shows improvements in performance over the state of the art, particularly in robustness to aggressive motion and temporary lack of visual features. The proposed on-line method starts with visual odometry to estimate the ego-motion and to register point clouds from a scanning lidar at a high frequency but low fidelity. Then, scan matching based lidar odometry refines the motion estimation and point cloud registration simultaneously. We show results with datasets collected in our own experiments as well as using the KITTI odometry benchmark. Our proposed method is ranked #1 on the benchmark in terms of average translation and rotation errors, with a 0.75\% of relative position drift. In addition to comparison of the motion estimation accuracy, we evaluate robustness of the method when the sensor suite moves at a high speed and is subject to significant ambient lighting changes.}
}

@inproceedings{maddern2015icra,
  author    = {W. Maddern and G.M. Pascoe and P. Newman},
  title     = {{Leveraging Experience for Large-Scale LIDAR Localisation in Changing Cities}},
  booktitle = icra,
  year      = 2015,
  keywords  = {Localization, Intelligent Transportation Systems, Mapping},
  abstract  = {Recent successful approaches to autonomous vehicle localisation and navigation typically involve 3D LIDAR scanners and a static, curated 3D map, both of which are expensive to acquire and maintain. In this paper we propose an experience-based approach to matching a local 3D swathe built using a push-broom 2D LIDAR to a number of prior 3D maps, each of which has been collected during normal driving in different conditions. Local swathes are converted to a combined 2D height and reflectance representation, and we exploit the GPU rendering pipeline to densely sample the localisation cost function to provide robustness and a wide basin of convergence. Prior maps are incrementally built into an experience-based framework from multiple traversals of the same environment, capturing changes in environment structure and appearance over time. The LIDAR localisation solutions from each prior map are fused with vehicle odometry in a probabilistic framework to provide a single pose solution suitable for automated driving. Using this framework we demonstrate realtime centimetre-level localisation using LIDAR data collected in a dynamic city environment over a period of a year.}
}

@inproceedings{pascoe2015icra,
  author    = {G.M. Pascoe and W. Maddern and A. Stewart and P. Newman},
  title     = {{FARLAP: Fast Robust Localisation Using Appearance Priors}},
  booktitle = icra,
  year      = 2015,
  keywords  = {Localization, Visual Tracking, Intelligent Transportation Systems},
  abstract  = {This paper is concerned with large-scale localisation at city scales with monocular cameras. Our primary motivation lies with the development of autonomous road vehicles an application domain in which low-cost sensing is particularly important. Here we present a method for localising against a textured 3-dimensional prior mesh using a monocular camera. We first present a system for generating and texturing the prior using a LIDAR scanner and camera. We then describe how we can localise against that prior with a single camera, using an information-theoretic measure of image similarity. This process requires dealing with the distortions induced by a wide-angle camera. We present and justify an interesting approach to this issue in which we distort the prior map into the image rather than vice-versa. Finally we explain how the general purpose computation functionality of a modern GPU is particularly apt for our task, allowing us to run the system in real time. We present results showing centimetre-level localisation accuracy through a city over six kilometres.}
}

@inproceedings{paton2015icra,
  author    = {M. Paton and K.A. MacTavish and C.J. Ostafew and T. Barfoot},
  title     = {It's Not Easy Seeing Green: Lighting-Resistant Stereo Visual Teach & Repeat Using Color-Constant Images},
  booktitle = icra,
  year      = 2015,
  keywords  = {Computer Vision for Robotics and Automation, Field Robots, Visual-Based Control and/or Navigation},
  abstract  = {Stereo Visual Teach & Repeat (VT&R) is a system for long-range, autonomous route following in unstructured 3D environments. As this system relies on a passive sensor to localize, it is highly susceptible to changes in lighting conditions. Recent work in the optics community has provided a method to transform images collected from a three-channel passive sensor into color-constant images that are resistant to changes in outdoor lighting conditions. This paper presents a lightingresistant VT&R system that uses experimentally trained colorconstant images to autonomously navigate difficult outdoor terrain despite changes in lighting. We show through an extensive field trial that our algorithm is capable of autonomously following a 1km outdoor route spanning sandy/rocky terrain, grassland, and wooded areas. Using a single visual map created at midday, the route was autonomously repeated 26 times over a period of four days, from sunrise to sunset with an autonomy rate (by distance) of over 99.9\%. These experiments show that a simple image transformation can extend the operation of VT&R from a few hours to multiple days.}
}

@inproceedings{carlone2015icra,
  author    = {L. Carlone and F. Dellaert},
  title     = {{Duality-Based Verification Techniques for 2D SLAM}},
  booktitle = icra,
  year      = 2015,
  keywords  = {SLAM, Localization, Autonomous Navigation},
  abstract  = {While iterative optimization techniques for Simultaneous Localization and Mapping (SLAM) are now very efficient and widely used, none of them can guarantee global convergence to the maximum likelihood estimate. Local convergence usually implies artifacts in map reconstruction and large localization errors, hence it is very undesirable for applications in which accuracy and safety are of paramount importance. We provide a technique to verify if a given 2D SLAM solution is globally optimal. The insight is that, while computing the optimal solution is hard in general, duality theory provides tools to compute tight bounds on the optimal cost, via convex programming. These bounds can be used to evaluate the quality of a SLAM solution, hence providing a sanity check for stateof-the-art incremental and batch solvers. Experimental results show that our technique successfully identifies wrong estimates (i.e., local minima) in large-scale SLAM scenarios. This work, together with [1], represents a step towards the objective of having SLAM techniques with guaranteed performance, that can be used in safety-critical applications.}
}

@inproceedings{carlone2015icra-itf3,
  author    = {L. Carlone and R. Tron and K. Daniilidis and F. Dellaert},
  title     = {{Initialization Techniques for 3D SLAM: A Survey on Rotation Estimation and Its Use in Pose Graph Optimization}},
  booktitle = icra,
  year      = 2015,
  keywords  = {SLAM, Localization, Mapping},
  abstract  = {Pose graph optimization is the non-convex optimization problem underlying pose-based Simultaneous Localization and Mapping (SLAM). If robot orientations were known, pose graph optimization would be a linear leastsquares problem, whose solution can be computed efficiently and reliably. Since rotations are the actual reason why SLAM is a difficult problem, in this work we survey techniques for 3D rotation estimation. Rotation estimation has a rich history in three scientific communities: robotics, computer vision, and control theory. We review relevant contributions across these communities, assess their practical use in the SLAM domain, and benchmark their performance on representative SLAM problems (Fig. 1). We show that the use of rotation estimation to bootstrap iterative pose graph solvers entails significant boost in convergence speed and robustness.}
}

@inproceedings{kaess2015icra,
  author    = {M. Kaess},
  title     = {{Simultaneous Localization and Mapping with Infinite Planes}},
  booktitle = icra,
  year      = 2015,
  keywords  = {SLAM, Mapping, RGB-D Perception},
  abstract  = {Simultaneous localization and mapping with infinite planes is attractive because of the reduced complexity with respect to both sparse point-based and dense volumetric methods. We show how to include infinite planes into a leastsquares formulation for mapping, using a homogeneous plane parametrization with a corresponding minimal representation for the optimization. Because it is a minimal representation, it is suitable for use with Gauss-Newton, Powells Dog Leg and incremental solvers such as iSAM. We also introduce a relative plane formulation that improves convergence. We evaluate our proposed approach on simulated data to show its advantages over alternative solutions. We also introduce a simple mapping system and present experimental results, showing real-time mapping of select indoor environments with a hand-held RGBD sensor.}
}

@inproceedings{pepperell2015icra,
  author    = {E. Pepperell and P. Corke and M.J. Milford},
  title     = {{Automatic Image Scaling for Place Recognition in Changing Environments}},
  booktitle = icra,
  year      = 2015,
  keywords  = {Visual Place Recognition, Visual-Based Control and/or Navigation},
  abstract  = {Robustness to variations in environmental conditions and camera viewpoint is essential for long-term place recognition, navigation and SLAM. Existing systems typically solve either of these problems, but invariance to both remains a challenge. This paper presents a training-free approach to lateral viewpoint- and condition-invariant, vision-based place recognition. Our successive frame patch-tracking technique infers average scene depth along traverses and automatically rescales views of the same place at different depths to increase their similarity. We combine our system with the condition-invariant SMART algorithm and demonstrate place recognition between day and night, across entire 4-lane-plus-median-strip roads, where current algorithms fail.}
}

@inproceedings{choudhary2015icra,
  author    = {S. Choudhary and V. Indelman and H.I. Christensen and F. Dellaert},
  title     = {{Information Based Reduced Landmark SLAM}},
  booktitle = icra,
  year      = 2015,
  keywords  = {SLAM, Mapping, Localization},
  abstract  = {In this paper, we present an information-based approach to select a reduced number of landmarks and poses for a robot to localize itself and simultaneously build an accurate map. We develop an information theoretic algorithm to efficiently reduce the number of landmarks and poses in a SLAM estimate without compromising the accuracy of the estimated trajectory. We also propose an incremental version of the reduction algorithm which can be used in SLAM framework resulting in information based reduced landmark SLAM. The results of reduced landmark based SLAM algorithm are shown on Victoria park dataset and a Synthetic dataset and are compared with standard graph SLAM (SAM [6]) algorithm. We demonstrate a reduction of 40-50\% in the number of landmarks and around 55\% in the number of poses with minimal estimation error as compared to standard SLAM algorithm.}
}

@inproceedings{english2015icra,
  author    = {A. English and P. Ross and D. Ball and B. Upcroft and P. Corke},
  title     = {{TriggerSync: A Time Synchronisation Tool}},
  booktitle = icra,
  year      = 2015,
  keywords  = {Robotic Software, Middleware and Programming Environments, Calibration and Identification, Sensor Fusion},
  abstract  = {This paper presents a framework for synchronising multiple triggered sensors with respect to a local clock using standard computing hardware. Providing sensor measurements with accurate and meaningful timestamps is important for many sensor fusion, state estimation and control applications. Accurately synchronising sensor timestamps can be performed with specialised hardware, however, performing sensor synchronisation using standard computing hardware and non-real-time operating systems is difficult due to inaccurate and temperature sensitive clocks, variable communication delays and operating system scheduling delays. Results show the ability of our framework to estimate time offsets to sub-millisecond accuracy. We also demonstrate how synchronising timestamps with our framework results in a tenfold reduction in image stabilisation error for a vehicle driving on rough terrain. The source code will be released as an open source tool for time synchronisation in ROS.}
}

@inproceedings{vineet2015icra,
  author    = {V. Vineet and O. Miksik and M. Lidegaard and M. Niessner and S. Golodetz and V. Prisacariu and O. Kahler and D. Murray and S. Izadi and P. Perez and P. Torr},
  title     = {{Incremental Dense Semantic Stereo Fusion for Large-Scale Semantic Scene Reconstruction}},
  booktitle = icra,
  year      = 2015,
  keywords  = {Semantic Scene Understanding, Computer Vision for Robotics and Automation},
  abstract  = {Our abilities in scene understanding, which allow us to perceive the 3D structure of our surroundings and intuitively recognise the objects we see, are things that we largely take for granted, but for robots, the task of understanding large scenes quickly remains extremely challenging. Recently, scene understanding approaches based on 3D reconstruction and semantic segmentation have become popular, but existing methods either do not scale, fail outdoors, provide only sparse reconstructions or are rather slow. In this paper, we build on a recent hash-based technique for large-scale fusion and an efficient mean-field inference algorithm for densely-connected CRFs to present what to our knowledge is the first system that can perform dense, large-scale, outdoor semantic reconstruction of a scene in (near) real time. We also present a semantic fusion approach that allows us to handle dynamic objects more effectively than previous approaches. We demonstrate the effectiveness of our approach on the KITTI dataset, and provide qualitative and quantitative results showing high-quality dense reconstruction and labelling of a number of scenes.}
}

@inproceedings{agamennoni2015icra,
  author    = {G. Agamennoni and P.T. Furgale and R. Siegwart},
  title     = {{Self-Tuning M-Estimators}},
  booktitle = icra,
  year      = 2015,
  keywords  = {SLAM},
  abstract  = {M-estimators are the de-facto standard method of robust estimation in robotics. They are easily incorporated into iterative non-linear least-squares estimation and provide seamless and effective handling of outliers in data. However, every M-estimators robust loss function has one or more tuning parameters that control the influence of different data. The choice of M-estimator and the manual tuning of these parameters is always a source of uncertainty when applying the technique to new data or a new problem. In this paper we develop the concept of self-tuning Mestimators. We first make the connection between many common M-estimators and elliptical probability distributions. This connection shows that the choice of M-estimator is an assumption that the residuals belong to a well-defined elliptical distribution. We exploit this implication in two ways. First, we develop an algorithm for tuning the M-estimator parameters during iterative optimization. Second, we show how to choose the correct M-estimator for your data by examining the likelihood of the data given the model. We fully derive these algorithms and show their behavior on a representative example of visual simultaneous localization and mapping.}
}

@inproceedings{stumm2015icra,
  author    = {E. Stumm and C. Mei and S. Lacroix and M. Chli},
  title     = {{Location Graphs for Visual Place Recognition}},
  booktitle = icra,
  year      = 2015,
  keywords  = {Visual Place Recognition, Autonomous Navigation, Mapping},
  abstract  = {With the growing demand for deployment of robots in real scenarios, robustness in the perception capabilities for navigation lies at the forefront of research interest, as this forms the backbone of robotic autonomy. Existing place recognition approaches traditionally follow the feature-based bag-of-words paradigm in order to cut down on the richness of information in images. As structural information is typically ignored, such methods suffer from perceptual aliasing and reduced recall, due to the ambiguity of observations. In a bid to boost the robustness of appearance-based place recognition, we consider the world as a continuous constellation of visual words, while keeping track of their covisibility in a graph structure. Locations are queried based on their appearance, and modelled by their corresponding cluster of landmarks from the global covisibility graph, which retains important relational information about landmarks. Complexity is reduced by comparing locations by their graphs of visual words in a simplified manner. Test results show increased recall performance and robustness to noisy observations, compared to state-of-the-art methods.}
}

@inproceedings{suger2015icra,
  author    = {B. Suger and B. Steder and W. Burgard},
  title     = {{Traversability Analysis for Mobile Robots in Outdoor Environments: A Semi-Supervised Learning Approach Based on 3D-Lidar Data}},
  booktitle = icra,
  year      = 2015,
  keywords  = {Field Robots, Learning and Adaptive Systems},
  abstract  = {The ability to safely navigate is a crucial prerequisite for truly autonomous systems. A robot has to distinguish obstacles from traversable ground. Failing on this task can cause great damage or restrict the robots movement unnecessarily. Due to the security relevance of this problem, great effort is typically spent to design models for individual robots and sensors, and the complexity of such models is correlated to the complexity of the environment and the capabilities of the robot. We present a semi supervised learning approach, where the robot learns its traversability capabilities from a human operating it. From this partially and only positive labeled training data, our approach infers a model for the traversability analysis, thereby requiring very little manual effort for the human. In practical experiments we show that our method can be used for robots that need to reliably navigate on dirt roads as well as for robots that have very restricted traversability capabilities. Fig. 1. Different mobile robot outdoor platforms with different capabilities and different fields of applications.}
}

@inproceedings{meier2015icra,
  author    = {L. Meier and D. Honegger and M. Pollefeys},
  title     = {{PX4: A Node-Based Multithreaded Open Source Robotics Framework for Deeply Embedded Platforms}},
  booktitle = icra,
  year      = 2015,
  keywords  = {Robotic Software, Middleware and Programming Environments},
  abstract  = {We present a novel, deeply embedded robotics middleware and programming environment. It uses a multithreaded, publish-subscribe design pattern and provides a Unixlike software interface for micro controller applications. We improve over the state of the art in deeply embedded open source systems by providing a modular and standards-oriented platform. Our system architecture is centered around a publishsubscribe object request broker on top of a POSIX application programming interface. This allows to reuse common Unix knowledge and experience, including a bash-like shell. We demonstrate with a vertical takeoff and landing (VTOL) use case that the system modularity is well suited for novel and experimental vehicle platforms. We also show how the system architecture allows a direct interface to ROS and to run individual processes either as native ROS nodes on Linux or nodes on the micro controller, maximizing interoperability. Our microcontroller-based execution environment has substantially lower latency and better hardware connectivity than a typical Robotics Linux system and is therefore well suited for fast, high rate control tasks.}
}

@inproceedings{linegar2015icra,
  author    = {C. Linegar and W. Churchill and P. Newman},
  title     = {{Work Smart, Not Hard: Recalling Relevant Experiences for Vast-Scale but Time-Constrained Localisation}},
  booktitle = icra,
  year      = 2015,
  keywords  = {Localization, Mapping, Computer Vision for Robotics and Automation},
  abstract  = {This paper is about life-long vast-scale localisation in spite of changes in weather, lighting and scene structure. Building upon our previous work in Experience-based Navigation [1], we continually grow and curate a visual map of the world that explicitly supports multiple representations of the same place. We refer to these representations as experiences, where a single experience captures the appearance of an environment under certain conditions. Pedagogically, an experience can be thought of as a visual memory. By accumulating experiences we are able to handle cyclic appearance change (diurnal lighting, seasonal changes, and extreme weather conditions) and also adapt to slow structural change. This strategy, although elegant and effective, poses a new challenge: In a region with many stored representations which one(s) should we try to localise against given finite computational resources? By learning from our previous use of the experience-map, we can make predictions about which memories we should consider next, conditioned on how the robot is currently localised in the experience-map. During localisation, we prioritise the loading of past experiences in order to minimise the expected computation required. We do this in a probabilistic way and show that this memory policy significantly improves localisation efficiency, enabling long-term autonomy on robots with limited computational resources. We demonstrate and evaluate our system over three challenging datasets, totalling 206km of outdoor travel. We demonstrate the system in a diverse range of lighting and weather conditions, scene clutter, camera occlusions, and permanent structural change in the environment.}
}

@inproceedings{steiner2015icra,
  author    = {T. Steiner and G. Huang and J. Leonard},
  title     = {{Location Utility-Based Map Reduction}},
  booktitle = icra,
  year      = 2015,
  keywords  = {Autonomous Navigation, SLAM, Localization},
  abstract  = {Maps used for navigation often include a database of location descriptions for place recognition (loop closing), which permits bounded-error performance. A standard posegraph SLAM system adds a new entry for every new pose into the location database, which grows linearly and unbounded in time and thus becomes unsustainable. To address this issue, in this paper we propose a new map-reduction approach that pre-constructs a fixed-size place-recognition database amenable to the limited storage and processing resources of the vehicle by exploiting the high-level structure of the environment as well as the vehicle motion. In particular, we introduce the concept of location utility which encapsulates the visitation probability of a location and its spatial distribution relative to nearby locations in the database as a measure of the value of potential loop-closure events to occur at that location. While finding the optimal reduced location database is NPhard, we develop an efficient greedy algorithm to sort all the locations in a map based on their relative utility without access to sensor measurements or the vehicle trajectory. This enables pre-determination of a generic, limited-size place-recognition database containing the N best locations in the environment. To validate the proposed approach, we develop an open-source street-map simulator using real city-map data and show that an accurate map (pose-graph) can be attained even when using a place-recognition database with only 1\% of the entries of the corresponding full database.}
}

@inproceedings{carrillo2015icra,
  author    = {H. Carrillo and P. Dames and V. Kumar and J.A. Castellanos},
  title     = {{Autonomous Robotic Exploration Using Occupancy Grid Maps and Graph SLAM Based on Shannon and Rnyi Entropy}},
  booktitle = icra,
  year      = 2015,
  keywords  = {Autonomous Navigation, SLAM, Mapping},
  abstract  = {In this paper we examine the problem of autonomously exploring and mapping an environment using a mobile robot. The robot uses a graph-based SLAM system to perform mapping and represents the map as an occupancy grid. In this setting, the robot must trade-off between exploring new area to complete the task and exploiting the existing information to maintain good localization. Selecting actions that decrease the map uncertainty while not significantly increasing the robots localization uncertainty is challenging. We present a novel information-theoretic utility function that uses both Shannons and Renyis definitions of entropy to jointly consider the uncertainty of the robot and the map. This allows us to fuse both uncertainties without the use of manual tuning. We present simulations and experiments comparing the proposed utility function to state-of-the-art utility functions, which only use Shannons entropy. We show that by using the proposed utility function, the robot and map uncertainties are smaller than using other existing methods.}
}

@inproceedings{merriaux2015icra,
  author    = {P. Merriaux and Y. Dupuis and P. Vasseur and X. Savatier},
  title     = {{Fast and Robust Vehicle Positioning on Graph-Based Representation of Drivable Maps}},
  booktitle = icra,
  year      = 2015,
  keywords  = {Localization, Intelligent Transportation Systems},
  abstract  = {In this paper, we propose a car positioning approach that does not rely on GPS. We propose to use car wheel speeds and road maps in order to achieve robust positioning of the vehicle. The vehicle positioning is achieved by applying particle filtering on a graph-based representation of a road map. We show that the vehicle positioning is feasible and robust with these two inputs at a really low computational cost. We achieve car positioning with an averaged 5 m accuracy within a 100 km drivable road map on a 12 km sequence.}
}

@inproceedings{heise2015icra,
  author    = {P. Heise and B. Jensen and S. Klose and A. Knoll},
  title     = {{Fast Dense Stereo Correspondences by Binary Locality Sensitive Hashing}},
  booktitle = icra,
  year      = 2015,
  keywords  = {Computer Vision for Robotics and Automation},
  abstract  = {The stereo correspondence problem is still a highly active topic of research with many applications in the robotic domain. Still many state of the art algorithms proposed to date are unable to reasonably handle high resolution images due to their run time complexities or memory requirements. In this work we propose a novel stereo correspondence estimation algorithm that employs binary locality sensitive hashing and is well suited to implementation on the GPU. Our proposed method is capable of processing very high-resolution stereo images at near real-time rates. An evaluation on the new Middlebury and Disney high-resolution stereo benchmarks demonstrates that our proposed method performs well compared to existing state of the art algorithms.}
}

@inproceedings{forster2015icra,
  author    = {C. Forster and M. Faessler and F. Fontana and M. Werlberger and D. Scaramuzza},
  title     = {{Continuous On-Board Monocular-Vision-based Elevation Mapping Applied to Autonomous Landing of Micro Aerial Vehicles}},
  booktitle = icra,
  year      = 2015,
  keywords  = {Computer Vision for Robotics and Automation, Aerial Robotics},
  abstract  = {In this paper, we propose a resource-efficient system for real-time 3D terrain reconstruction and landingspot detection for micro aerial vehicles. The system runs on an on-board smartphone processor and requires only the input of a single downlooking camera and an inertial measurement unit. We generate a two-dimensional elevation map that is probabilistic, of fixed size, and robot-centric, thus, always covering the area immediately underneath the robot. The elevation map is continuously updated at a rate of 1 Hz with depth maps that are triangulated from multiple views using recursive Bayesian estimation. To highlight the usefulness of the proposed mapping framework for autonomous navigation of micro aerial vehicles, we successfully demonstrate fully autonomous landing including landing-spot detection in real-world experiments.}
}

@inproceedings{nelson2015icra,
  author    = {P. Nelson and W. Churchill and I. Posner and P. Newman},
  title     = {{From Dusk Till Dawn: Localisation at Night Using Artificial Light Sources}},
  booktitle = icra,
  year      = 2015,
  keywords  = {Localization, Mapping, Visual-Based Control and/or Navigation},
  abstract  = {This paper is about localising at night in urban environments using vision. Despite it being dark exactly half of the time, surprisingly little attention has been given to this problem. A defining aspect of night-time urban scenes is the presence and effect of artificial lighting be that in the form of street or interior lighting through windows. By building a model of the environment which includes a representation of the spatial location of every light source, localisation becomes possible using monocular cameras. One of the challenges we face is the gross change in light appearance as a function of distance due to flare, saturation and bleeding city lights certainly do not appear as point features. To overcome this, we model the appearance of each light as a function of vehicle location, using this to inform our data-association decisions and to regularise the cost function which is used to infer vehicle pose. In this way we develop a place-dependent but stable sensor model which is customised for the particular environment in which we are operating. We demonstrate that our system is able to localise successfully at night over 12 km in situations where a traditional point feature based system fails.}
}

@inproceedings{mitzel2015icra,
  author    = {D. Mitzel and J. Diesel and A. Osep and U. Rafi and B. Leibe},
  title     = {{A Fixed-Dimensional 3D Shape Representation for Matching Partially Observed Objects in Street Scenes}},
  booktitle = icra,
  year      = 2015,
  keywords  = {Object detection, Segmentation, Categorization, RGB-D Perception, Human detection & tracking},
  abstract  = {In this paper, we present an object-centric, fixeddimensional 3D shape representation for robust matching of partially observed object shapes, which is an important component for object categorization from 3D data. A main problem when working with RGB-D data from stereo, Kinect, or laser sensors is that the 3D information is typically quite noisy. For that reason, we accumulate shape information over time and register it in a common reference frame. Matching the resulting shapes requires a strategy for dealing with partial observations. We therefore investigate several distance functions and kernels that implement different such strategies and compare their matching performance in quantitative experiments. We show that the resulting representation achieves good results for a large variety of vision tasks, such as multi-class classification, person orientation estimation, and articulated body pose estimation, where robust 3D shape matching is essential.}
}

@inproceedings{coleman2015icra,
  author    = {D. Coleman and I.A. Sucan and M. Moll and K. Okada and N. Correll},
  title     = {{Experience-Based Planning with Sparse Roadmap Spanners}},
  booktitle = icra,
  year      = 2015,
  keywords  = {Motion and Path Planning, Humanoid Robots, Learning and Adaptive Systems},
  abstract  = {We present an experience-based planning framework called Thunder that learns to reduce computation time required to solve high-dimensional planning problems in varying environments. The approach is especially suited for large configuration spaces that include many invariant constraints, such as those found with whole body humanoid motion planning. Experiences are generated using probabilistic sampling and stored in a sparse roadmap spanner (SPARS), which provides asymptotically near-optimal coverage of the configuration space, making storing, retrieving, and repairing past experiences very efficient with respect to memory and time. The Thunder framework improves upon past experience-based planners by storing experiences in a graph rather than in individual paths, eliminating redundant information, providing more opportunities for path reuse, and providing a theoretical limit to the size of the experience graph. These properties also lead to improved handling of dynamically changing environments, reasoning about optimal paths, and reducing query resolution time. The approach is demonstrated on a 30 degrees of freedom humanoid robot and compared with the Lightning framework, an experience-based planner that uses individual paths to store past experiences. In environments with variable obstacles and stability constraints, experiments show that Thunder is on average an order of magnitude faster than Lightning and planning from scratch. Thunder also uses 98.8\% less memory to store its experiences after 10,000 trials when compared to Lightning. Our framework is implemented and freely available in the Open Motion Planning Library.}
}

@inproceedings{churchill2015icra,
  author    = {W. Churchill and C.H. Tong and C. Gurau and I. Posner and P. Newman},
  title     = {{Know Your Limits: Embedding Localiser Performance Models in Teach and Repeat Maps}},
  booktitle = icra,
  year      = 2015,
  keywords  = {Localization, Mapping, Computer Vision for Robotics and Automation},
  abstract  = {This paper is about building maps which not only contain the traditional information useful for localising such as point features but also embeds a spatial model of expected localiser performance. This often overlooked secondorder information provides vital context when it comes to map use and planning. Our motivation here is to improve the performance of the popular Teach and Repeat paradigm [1] which has been shown to enable truly large-scale field operation. When using the taught route for localisation, it is often assumed the robot is following exactly, or is sufficiently close to, the original path, enabling successful localisation. However, what happens if it is not possible, or not desirable to exactly follow the mapped path? How far off the beaten track can the robot travel before it gets lost? We present an approach for assessing this localisation area around a taught route, which we refer to as the localisation envelope. Using a combination of physical sampling and a Gaussian Process model, we are able to accurately predict the localisation performance at unseen points.}
}

@inproceedings{paz2015icra,
  author    = {L.M. Paz and P. Pinies and P. Newman},
  title     = {{A Variational Approach to Online Road and Path Segmentation with Monocular Vision}},
  booktitle = icra,
  year      = 2015,
  keywords  = {Visual Learning, Intelligent Transportation Systems, Computer Vision for Robotics and Automation},
  abstract  = {In this paper we present an online approach to segmenting roads on large scale trajectories using only a monocular camera mounted on a car. We differ from popular 2D segmentation solutions which use single colour images and machine learning algorithms that require supervised training on huge image databases. Instead, we propose a novel approach that fuses 3D geometric data with appearance-based segmentation of 2D information in an automatic system. Our contribution is twofold: first, we propagate labels from frame to frame using depth priors of the segmented road avoiding user interaction most of the time; second, we transfer the segmented road labels to 3D laser point clouds. This reduces the complexity of stateof-the-art segmentation algorithms running on 3D Lidar data. Segmentation fails is in only 3\% of the cases over a sequence of 13,600 monocular images spanning an urban trajectory of more than 10km.}
}

@inproceedings{rhinehart2015icra,
  author    = {N. Rhinehart and J. Zhou and M. Hebert and J. Bagnell},
  title     = {{Visual Chunking: A List Prediction Framework for Region-Based Object Detection}},
  booktitle = icra,
  year      = 2015,
  keywords  = {Computer Vision for Robotics and Automation},
  abstract  = {We consider detecting objects in an image by iteratively selecting from a set of arbitrarily shaped candidate regions. Our generic approach, which we term visual chunking, reasons about the locations of multiple object instances in an image while expressively describing object boundaries. We design an optimization criterion for measuring the performance of a list of such detections as a natural extension to a common per-instance metric. We present an efficient algorithm with provable performance for building a highquality list of detections from any candidate set of region-based proposals. We also develop a simple class-specific algorithm to generate a candidate region instance in near-linear time in the number of low-level superpixels that outperforms other region generating methods. In order to make predictions on novel images at testing time without access to ground truth, we develop learning approaches to emulate these algorithms behaviors. We demonstrate that our new approach outperforms sophisticated baselines on benchmark datasets.}
}

@inproceedings{berczi2015icra,
  author    = {L. Berczi and I. Posner and T. Barfoot},
  title     = {{Learning to Assess Terrain from Human Demonstration Using an Introspective Gaussian-Process Classifier}},
  booktitle = icra,
  year      = 2015,
  keywords  = {Learning and Adaptive Systems, Autonomous Navigation, Robot Safety},
  abstract  = {This paper presents an approach to learning robot terrain assessment from human demonstration. An operator drives a robot for a short period of time, supervising the gathering of traversable and untraversable terrain data. After this initial training period, the robot can then predict the traversability of new terrain based on its experiences. We improve on current methods in two ways: first, we maintain a richer (higher-dimensional) representation of the terrain that is better able to distinguish between different training examples. Second, we use a Gaussian-process classifier for terrain assessment due to its superior introspective abilities (leading to better uncertainty estimates) when compared to other classifier methods in the literature. Our method is tested on real data and shown to outperform current methods both in classification accuracy and uncertainty estimation.}
}

@inproceedings{pinies2015icra,
  author    = {P. Pinies and L.M. Paz and P. Newman},
  title     = {{Dense Mono Reconstruction: Living with the Pain of the Plain Plane}},
  booktitle = icra,
  year      = 2015,
  keywords  = {Computer Vision for Robotics and Automation, Mapping},
  abstract  = {This paper is about dense depthmap estimation using a monocular camera in workspaces with extensive textureless surfaces. Current state of the art techniques have been shown to work in real time with an admirable performance in desktop-size environments. Unfortunately, as we show in this paper, when applied to larger indoor environments, performance often degrades. A common cause is the presence of large affine texture-less areas like by walls, floors, ceilings and drab objects such as chairs and tables. These produce noisy and worse still, grossly erroneous initial seeds for the depthmap that greatly impede successful optimisation. We solve this problem via the introduction of a new nonlocal higher-order regularisation term that enforces piecewise affine constraints between image pixels that are far apart in the image. This property leverages the observation that the depth at the edges of bland regions are often well estimated whereas their inner pixels are deeply problematic. A welcome by-product of our proposed technique is an estimate of the surface normals at each pixel. We will show that in terms of implementation, our algorithm is a natural extension of the often used variational approaches. We evaluate the proposed technique using real datasets for which we have ground truth models.}
}

@inproceedings{jalobeanu2015icra,
  author    = {M. Jalobeanu and G. Shirakyan and G. Parent and H. Kikkeri and B. Peasley and A. Feniello},
  title     = {{Reliable Kinect-Based Navigation in Large Indoor Environments}},
  booktitle = icra,
  year      = 2015,
  keywords  = {Autonomous Navigation, Localization, SLAM},
  abstract  = {Practical mapping and navigation solutions for large indoor environments continue to rely on relatively expensive range scanners, because of their accuracy, range and field of view. Microsoft Kinect on the other hand is inexpensive, is easy to use and has high resolution, but suffers from high noise, shorter range and a limiting field of view. We present a mapping and navigation system that uses the Microsoft Kinect sensor as the sole source of range data and achieves performance comparable to state-of-theart LIDAR-based systems. We show how we circumvent the main limitations of Kinect to generate usable 2D maps of relatively large spaces and to enable robust navigation in changing and dynamic environments. We use the Benchmark for Robotic Indoor Navigation (BRIN) to quantify and validate the performance of our system.}
}

@inproceedings{pinies2015icra-tmti,
  author    = {P. Pinies and L.M. Paz and P. Newman},
  title     = {{Too Much TV Is Bad: Dense Reconstruction from Sparse Laser with Non-Convex Regularisation}},
  booktitle = icra,
  year      = 2015,
  keywords  = {Mapping, Computer Vision for Robotics and Automation},
  abstract  = {In this paper we address the problem of dense depth map estimation from sparse noisy range data to reconstruct large heterogeneous outdoor scenes. We propose a surface inpainting solution through energy minimisation with an adaptive selection of surface regularisers among a set of well known convex and non-convex regularisers. In fact, the selection of norm is pivotal with respect to the intrinsic surface characteristics. Our goal is to show how dense interpolation of sparse range data can be leveraged of more exotic and non-convex regularisers such as the log and logTGV [1] which can better capture the scene geometry. In contrast to state of the art solutions, we do not restrict ourselves to this set of norms, instead we search for the most apt norm for each semantically segmented part of the scene. Our energy model selection use Bayesian optimisation to learn the best choice of free parameters. This results in an adaptive model selection and the generalisation of well studied regularisation norms. We conclude with a detailed experimental analysis of our approach using a basis of four norms over a set of challenging outdoor scenes.}
}

@inproceedings{cunningham2015icra,
  author    = {A.G. Cunningham and E. Galceran and R. Eustice and E. Olson},
  title     = {{MPDM: Multipolicy Decision Making in Dynamic, Uncertain Environments for Autonomous Driving}},
  booktitle = icra,
  year      = 2015,
  keywords  = {AI Reasoning Methods, Planning, Scheduling and Coordination, Intelligent Transportation Systems},
  abstract  = {Real-world autonomous driving in city traffic must cope with dynamic environments including other agents with uncertain intentions. This poses a challenging decisionmaking problem, e.g., deciding when to perform a passing maneuver or how to safely merge into traffic. Previous work in the literature has typically approached the problem using adhoc solutions that do not consider the possible future states of other agents, and thus have difficulty scaling to complex traffic scenarios where the actions of participating agents are tightly conditioned on one another. In this paper we present multipolicy decision-making (MPDM), a decision-making algorithm that exploits knowledge from the autonomous driving domain to make decisions online for an autonomous vehicle navigating in traffic. By assuming the controlled vehicle and other traffic participants execute a policy from a set of plausible closedloop policies at every timestep, the algorithm selects the best available policy for the controlled vehicle to execute. We perform policy election using forward simulation of both the controlled vehicle and other agents, efficiently sampling from the high-likelihood outcomes of their interactions. We then score the resulting outcomes using a user-defined cost function to accommodate different driving preferences, and select the policy with the highest score. We demonstrate the algorithm on a realworld autonomous vehicle performing passing maneuvers and in a simulated merging scenario.}
}

@inproceedings{sunderhauf2015rss,
  author    = {N. S{\"u}nderhauf and S. Shirazi and A. Jacobson and F. Dayoub and E. Pepperell and B. Upcroft and M. Milford},
  keywords  = {Place Recognition, CNN, Image Features},
  title     = {{Place Recognition with ConvNet Landmarks : Viewpoint-Robust, Condition-Robust, Training-Free}},
  booktitle = rss,
  url       = {https://eprints.qut.edu.au/84931/1/rss15_placeRec.pdf},
  year      = 2015
}

@inproceedings{siedentop2015fas,
  author    = {C. Siedentop and R. Heinze and D. Kasper and G. Breuel and C. Stachniss},
  title     = {{Path-Planning for Autonomous Parking with Dubins Curves}},
  booktitle = {Proc.~of the Workshop Fahrerassistenzsysteme},
  year      = {2015}
}

@article{abdo2015arxiv,
  author  = {N. Abdo and C. Stachniss and L. Spinello and W. Burgard},
  title   = {{Collaborative Filtering for Predicting User Preferences for Organizing Objects}},
  journal = arxiv,
  volume  = {arXiv:1512.06362},
  year    = 2015,
  url     = {http://arxiv.org/pdf/1512.06362}
}

@inproceedings{naseer2015iros,
  author    = {T. Naseer and M. Ruhnke and L. Spinello and C. Stachniss and W. Burgard},
  title     = {{Robust Visual SLAM Across Seasons}},
  booktitle = iros,
  year      = 2015
}

@inproceedings{perea2015icra,
  author    = {D. Perea Str{\"o}m and F. Nenci and C. Stachniss},
  title     = {{Predictive Exploration Considering Previously Mapped Environments}},
  booktitle = icra,
  year      = 2015
}

@inproceedings{abdo2015icra,
  author    = {N. Abdo and C. Stachniss and L. Spinello and W.Burgard},
  title     = {Robot, Organize my Shelves! Tidying up Objects by Predicting User Preferences},
  booktitle = icra,
  year      = 2015
}

@inproceedings{bogoslavskyi2015icra,
  author    = {I. Bogoslavskyi and L. Spinello and W. Burgard and C. Stachniss},
  title     = {{Where to Park? Minimizing the Expected Time to Find a Parking Space}},
  booktitle = icra,
  year      = 2015
}

@inproceedings{vysotska2015icra,
  author    = {O. Vysotska and T. Naseer and L. Spinello and W. Burgard and C. Stachniss},
  title     = {{Efficient and Effective Matching of Image Sequences Under Substantial Appearance Changes Exploiting GPS Prior}},
  booktitle = icra,
  year      = 2015,
  url       = {http://ais.informatik.uni-freiburg.de/publications/papers/vysotska15icra.pdf}
}

@inproceedings{vysotska2015icraws,
  author    = {O. Vysotska and C. Stachniss},
  title     = {{Lazy Sequences Matching Under Substantial Appearance Changes}},
  booktitle = {Workshop on Visual Place Recognition in Changing Environments at the IEEE Int.~Conf.~on Robotics \& Automation},
  year      = 2015
}

@phdthesis{sprunk2015phd,
  author = {C. Sprunk},
  title  = {{Highly Accurate Mobile Robot Navigation}},
  school = {Albert-Ludwigs-University of Freiburg, Department of Computer Science},
  year   = 2015,
  url    = {http://ais.informatik.uni-freiburg.de/publications/papers/sprunk15phd.pdf}
}

@phdthesis{agarwal2015phd,
  author = {P. Agarwal},
  title  = {{Robust Graph-Based Localization and Mapping}},
  school = {Albert-Ludwigs-University of Freiburg, Department of Computer Science},
  year   = 2015,
  url    = {http://ais.informatik.uni-freiburg.de/publications/papers/agarwal15phd.pdf}
}

@inproceedings{agarwal2015iros,
  author    = {P. Agarwal and W. Burgard and L. Spinello},
  title     = {{Metric Localization using Google Street View}},
  booktitle = iros,
  year      = 2015,
  url       = {http://ais.informatik.uni-freiburg.de/publications/papers/agarwal15iros.pdf}
}

@inproceedings{naseer2015ecmr,
  author    = {T. Naseer and B. Suger and M. Ruhnke and W. Burgard},
  title     = {{Vision-Based Markov Localization Across Large Perceptual Changes}},
  booktitle = ecmr,
  year      = 2015,
  url       = {http://ais.informatik.uni-freiburg.de/publications/papers/naseer15ecmr.pdf}
}

@inproceedings{mazuran2015icra,
  author    = {M. Mazuran and C. Sprunk and W. Burgard and G.D. Tipaldi},
  title     = {Lex{TOR}: Lexicographic Teach Optimize and Repeat Based on User Preferences},
  booktitle = icra,
  year      = 2015,
  url       = {http://ais.informatik.uni-freiburg.de/publications/papers/mazuran15icra.pdf}
}

@inbook{carrillo2015springer,
  author    = {H. Carrillo and J.A. Castellanos},
  title     = {{Navigation Under Uncertainty Based on Active SLAM Concepts}},
  booktitle = {Studies in Systems, Decision and Control},
  year      = 2015,
  publisher = {Springer International Publishing},
  pages     = {209--235},
  abstract  = {This chapter addresses the problem of path planning considering uncertainty criteria over the belief space. We propose a path planning algorithm that uses a determinant-based measure of uncertainty and a reduced representation of the environment, in order to obtain the minimum uncertainty path from a roadmap. The determinant-based measure of uncertainty is borrowed from the active SLAM literature. We also present in this chapter an overview of the active SLAM problem. Our path planning proposal does not require a priori knowledge of the environment due to the construction of the roadmap via a graph-based SLAM algorithm. We report experimental results of our proposal in four datasets that show its feasibility to obtain the minimum uncertainty path towards an autonomous navigation framework. We also show an improvement in the computation time with respect to the state of the art.}
}

@inproceedings{engel2015iros,
  author    = {J. Engel and J. St{\"u}ckler and D. Cremers},
  title     = {Large-Scale Direct SLAM with Stereo Cameras},
  booktitle = iros,
  year      = {2015},
  keywords  = {SLAM, Computer Vision, Visual Navigation},
  abstract  = {We propose a novel Large-Scale Direct SLAM algorithm for stereo cameras (Stereo LSD-SLAM) that runs in real-time at high frame rate on standard CPUs. In contrast to sparse interest-point based methods, our approach aligns images directly based on the photoconsistency of all highcontrast pixels, including corners, edges and high texture areas. It concurrently estimates the depth at these pixels from two types of stereo cues: Static stereo through the fixed-baseline stereo camera setup as well as temporal multi-view stereo exploiting the camera motion. By incorporating both disparity sources, our algorithm can even estimate depth of pixels that are under-constrained when only using fixed-baseline stereo. Using a fixed baseline, on the other hand, avoids scale-drift that typically occurs in pure monocular SLAM. We furthermore propose a robust approach to enforce illumination invariance, capable of handling aggressive brightness changes between frames greatly improving the performance in realistic settings. In experiments, we demonstrate state-of-the-art results on stereo SLAM benchmarks such as Kitti or challenging datasets from the EuRoC Challenge 3 for micro aerial vehicles.},
  url       = {proceedings:engel2015iros.pdf}
}

@inproceedings{roehling2015iros,
  author    = {T. R{\"o}hling and J. Mack and D. Schulz},
  title     = {{A Fast Histogram-Based Similarity Measure for Detecting Loop Closures in 3-D LIDAR Data}},
  booktitle = iros,
  year      = {2015},
  abstract  = {We present a fast method of detecting loop closure opportunities through the use of similarity measures on histograms extracted from 3-D LIDAR data. We avoid computationally expensive features and compute histograms over simple global statistics of the LIDAR scans. The resulting histograms encode sufficient information to detect spatially close scans with high precision and recall and can be computed at rates faster than data acquisition on modest consumer-grade hardware. Our approach is able to match previously established results in LIDAR loop closure detection with less computational overhead.},
  keywords  = {SLAM, point cloud registration, Velodyne}
}

@inproceedings{serafin2015iros,
  author    = {J. Serafin and G. Grisetti},
  title     = {{NICP: Dense Normal Based Point Cloud Registration}},
  booktitle = iros,
  year      = {2015},
  abstract  = {In this paper we present a novel on-line method to recursively align point clouds. By considering each point together with the local features of the surface (normal and curvature), our method takes advantage of the 3D structure around the points for the determination of the data association between two clouds. The algorithm relies on a least squares formulation of the alignment problem, that minimizes an error metric depending on these surface characteristics. We named the approach Normal Iterative Closest Point (NICP in short). Extensive experiments on publicly available benchmark data show that NICP outperforms other state-of-the-art approaches.},
  keywords  = {SLAM, Mapping, Computer Vision, RGB-D Perception},
  url       = {proceedings:serafin2015iros.pdf}
}

@inproceedings{whelan2015rss,
  author    = {T. Whelan and S. Leutenegger and R. S. Moreno and B. Glocker and A. Davison},
  title     = {{ElasticFusion: Dense SLAM Without A Pose Graph}},
  booktitle = rss,
  year      = {2015},
  abstract  = {We present a novel approach to real-time dense visual SLAM. Our system is capable of capturing comprehensive dense globally consistent surfel-based maps of room scale environments explored using an RGB-D camera in an incremental online fashion, without pose graph optimisation or any post-processing steps. This is accomplished by using dense frame-to-model camera tracking and windowed surfel-based fusion coupled with frequent model refinement through non-rigid surface deformations. Our approach applies local model-to-model surface loop closure optimisations as often as possible to stay close to the mode of the map distribution, while utilising global loop closure to recover from arbitrary drift and maintain global consistency.},
  keywords  = {RGB-D Perception, SLAM, Mapping},
  url       = {proceedings:whelan2015rss.pdf}
}

@inproceedings{seichter2021icra,
  author    = {D. Seichter and M. K{\"o}hler and B. Lewandowski and T. Wengefeld and H. Gross},
  title     = {{Efficient RGB-D Semantic Segmentation for Indoor Scene Analysis}},
  booktitle = icra,
  year      = 2021,
  abstract  = {Analyzing scenes thoroughly is crucial for mobile robots acting in different environments. Semantic segmentation can enhance various subsequent tasks, such as (semantically assisted) person perception, (semantic) free space detection, (semantic) mapping, and (semantic) navigation. In this paper, we propose an efficient and robust RGB-D segmentation approach that can be optimized to a high degree using NVIDIA TensorRT and, thus, is well suited as a common initial processing step in a complex system for scene analysis on mobile robots. We show that RGB-D segmentation is superior to processing RGB images solely and that it can still be performed in real time if the network architecture is carefully designed. We evaluate our proposed Efficient Scene Analysis Network (ESANet) on the common indoor datasets NYUv2 and SUNRGB-D and show that we reach state-of-the-art performance while enabling faster inference. Furthermore, our evaluation on the outdoor dataset Cityscapes shows that our approach is suitable for other areas of application as well. Finally, instead of presenting benchmark results only, we also show qualitative results in one of our indoor application scenarios.},
  url       = {proceedings: seichter2021icra.pdf}
}

@article{martin-braualla2015tog,
  author   = {R. Martin-Brualla and D. Gallup and S.M. Seitz},
  title    = {Time-lapse Mining from Internet Photos},
  journal  = tog,
  volume   = {34},
  number   = {4},
  year     = {2015},
  pages    = {62:1--62:8},
  url      = {https://grail.cs.washington.edu/projects/timelapse/TimelapseMiningSIGGRAPH15.pdf},
  keywords = {computational photography, image-based rendering, time-lapse}
}

@inproceedings{martin-brualla2015iccv,
  title     = {3d time-lapse reconstruction from internet photos},
  author    = {R. Martin-Brualla and D. Gallup and S.M. Seitz},
  booktitle = iccvold,
  year      = {2015},
  url       = {https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Martin-Brualla_3D_Time-Lapse_Reconstruction_ICCV_2015_paper.pdf}
}

@article{taneja2015pami,
  title   = {{Geometric Change Detection in Urban Environments Using Images}},
  author  = {A. Taneja and L. Ballan and M. Pollefeys},
  journal = pami,
  volume  = {37},
  number  = {11},
  pages   = {2193--2206},
  year    = {2015},
  url     = {https://www.inf.ethz.ch/personal/pomarc/pubs/TanejaPAMI15.pdf}
}

@inproceedings{carlone2015icra-ws,
  title     = {Towards 4D crop analysis in precision agriculture: Estimating plant height and crown radius over time via expectation-maximization},
  author    = {L. Carlone and J. Dong and S. Fenu and G.G. Rains and F. Dellaert},
  booktitle = {ICRA Workshop on Robotics in Agriculture},
  year      = {2015},
  url       = {https://pdfs.semanticscholar.org/5053/33647d4fe7eb80c6c390f79ae601dd1955bd.pdf}
}

@inproceedings{neubert2015cvprws,
  title     = {Exploiting intra Database Similarities for Selection of Place Recognition Candidates in Changing Environments},
  author    = {Neubert, P. and Schubert, S. and Protzel, P.},
  booktitle = {Proc. of the CVPR Workshop on Visual Place Recognition in Changing Environments},
  year      = {2015}
}

@article{mcmanus2015ar,
  year    = {2015},
  volume  = {39},
  number  = {3},
  pages   = {363--387},
  author  = {C. McManus and B. Upcroft and P. Newman},
  title   = {Learning place-dependant features for long-term vision-based localisation},
  journal = ar
}

@article{pandey2015jfr,
  author  = {G. Pandey and J.R. McBride and S. Savarese and R.M. Eustice},
  title   = {{Automatic Extrinsic Calibration of Vision and Lidar by Maximizing Mutual Information}},
  journal = jfr,
  year    = 2015,
  pages   = {696--722},
  volume  = {32},
  number  = {5}
}

@article{taylor2015jfr,
  author  = {Z. Taylor and J. Nieto and D. Johnson},
  title   = {{Multi-Modal Sensor Calibration Using a Gradient Orientation Measure}},
  journal = jfr,
  year    = 2015,
  pages   = {675--695},
  volume  = {32},
  number  = {5}
}

@article{torressanchez2015cea,
  title   = {An automatic object-based method for optimal thresholding in UAV images: Application for vegetation detection in herbaceous crops},
  author  = {J. Torres-Sanchez and F. L{\'o}pez-Granados and J.M. Pe{\~n}a},
  journal = cea,
  volume  = {114},
  pages   = {43--52},
  year    = {2015}
}

@inproceedings{hall2015wacv,
  title     = {Evaluation of Features for Leaf Classification in Challenging Conditions},
  author    = {Hall, D. and McCool, C.S. and Dayoub, F. and Sunderhauf, N. and Upcroft, B.},
  booktitle = wacv,
  year      = {2015},
  url       = {https://eprints.qut.edu.au/78723/1/174.pdf}
}

@inproceedings{zheng2015iccv,
  author    = {S. Zheng and S. Jayasumana and B. Romera-Paredes and V. Vineet and Z. Su and D. Du and C. Huang and P. Torr},
  title     = {Conditional Random Fields as Recurrent Neural Networks},
  booktitle = iccvold,
  year      = {2015},
  url       = {http://www.robots.ox.ac.uk/~szheng/papers/CRFasRNN.pdf},
  abstract  = {Pixel-level labelling tasks, such as semantic segmentation, play a central role in image understanding. Recent approaches have attempted to harness the capabilities of deep learning techniques for image recognition to tackle pixel-level labelling tasks. One central issue in this methodology is the limited capacity of deep learning techniques to delineate visual objects. To solve this problem, we introduce a new form of convolutional neural network that combines the strengths of Convolutional Neural Networks (CNNs) and Conditional Random Fields (CRFs)-based probabilistic graphical modelling. To this end, we formulate mean-field approximate inference for the Conditional Random Fields with Gaussian pairwise potentials as Recurrent Neural Networks. This network, called CRF-RNN, is then plugged in as a part of a CNN to obtain a deep network that has desirable properties of both CNNs and CRFs. Importantly, our system fully integrates CRF modelling with CNNs, making it possible to train the whole deep network end-to-end with the usual back-propagation algorithm, avoiding offlinempost-processing methods for object delineation. We apply the proposed method to the problem of semantic image segmentation, obtaining top results on the challenging Pascal VOC 2012 segmentation benchmark.}
}

@inproceedings{charrow2015rss,
  title     = {{Information-Theoretic Planning with Trajectory Optimization for Dense 3D Mapping}},
  author    = {Charrow, B. and Kahn, G. and Patil, S. and Liu, S. and Goldberg, K. and Abbeel, P. and Michael, N. and Kumar, V.},
  booktitle = rss,
  year      = {2015},
  url       = {http://goldberg.berkeley.edu/pubs/RSS2015_InfoTheoretic_Mapping.pdf}
}

@inproceedings{atanasov2015icra,
  title     = {{Decentralized Active Information Acquisition: Theory and Application to Multi-Robot SLAM}},
  author    = {Atanasov, N. and Le Ny, J. and Daniilidis, K. and Pappas, G.J.},
  booktitle = icra,
  year      = {2015},
  url       = {http://www.professeurs.polymtl.ca/jerome.le-ny/docs/proceedings/Atanasov_etal_infoAcqSLAM_ICRA15.pdf}
}

@inproceedings{bircher2015icra,
  title     = {{Structural Inspection Path Planning via Iterative Viewpoint Resampling with Application to Aerial Robotics}},
  author    = {Bircher, A. and Alexis, K. and Burri, M. and Oettershagen, P. and Omari, S. and Mantel, T. and Siegwart, R.},
  booktitle = icra,
  year      = {2015},
  url       = {https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/101881/eth-47824-01.pdf}
}

@inproceedings{stent2015bmvc,
  title     = {{Detecting Change for Multi-View, Long-Term Surface Inspection.}},
  author    = {Stent, S. and Gherardi, R. and Stenger, B. and Cipolla, R.},
  booktitle = bmvc,
  year      = {2015},
  url       = {http://www.bmva.org/bmvc/2015/papers/paper127/paper127.pdf}
}

@inproceedings{sakurada2015bmvc,
  title     = {{Change Detection from a Street Image Pair using CNN Features and Superpixel Segmentation.}},
  author    = {Sakurada, K. and Okatani, T.},
  booktitle = bmvc,
  year      = {2015},
  url       = {http://www.bmva.org/bmvc/2015/papers/paper061/paper061.pdf}
}

@article{xie2015arxiv,
  author   = {J. Xie and M. Kiefel and M. Sun and A. Geiger},
  title    = {{Semantic Instance Annotation of Street Scenes by 3D to 2D Label Transfer}},
  journal  = arxiv,
  year     = 2015,
  volume   = {arXiv:1511.03240v2},
  url      = {http://arxiv.org/pdf/1511.03240v2},
  abstract = {Semantic annotations are vital for training models for object recognition, semantic segmentation or scene understanding. Unfortunately, pixelwise annotation of images at very large scale is labor-intensive and only little labeled data is available, particularly at instance level and for street scenes. In this paper, we propose to tackle this problem by lifting the semantic instance labeling task from 2D into 3D. Given reconstructions from stereo or laser data, we annotate static 3D scene elements with rough bounding primitives and develop a model which transfers this information into the image domain. We leverage our method to obtain 2D labels for a novel suburban video dataset which we have collected, resulting in 400k semantic and instance image annotations. A comparison of our method to state-of-the-art label transfer baselines reveals that 3D information enables more efficient annotation while at the same time resulting in improved accuracy and time-coherent labels.}
}

@article{wang2014arxiv,
  title    = {{Hashing for similarity search: A survey}},
  author   = {J. Wang, and H.T. Shen and J. Song and J. Ji},
  journal  = arxiv,
  year     = {2014},
  volume   = {arXiv:1408.2927},
  keywords = {Hashing, Locality Sensitive Hashing},
  url      = {https://arxiv.org/pdf/1408.2927}
}

@inproceedings{naseer2014aaai,
  title     = {{Robust Visual Robot Localization Across Seasons using Network Flows}},
  author    = {T. Naseer and L. Spinello and W. Burgard and C. Stachniss},
  booktitle = aaai,
  year      = 2014,
  url       = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/naseer14aaai.pdf},
  keywords  = {Localization, Visual Navigation, Place Recognition}
}

@inproceedings{ziparo2014icomosga,
  author    = {V.A. Ziparo and G. Castelli and L. Van Gool and G. Grisetti and B. Leibe and M. Proesmans and C. Stachniss},
  title     = {{The ROVINA Project. Robots for Exploration, Digital Preservation and Visualization of Archeological sites}},
  booktitle = {Proc.~of the 18th ICOMOS General Assembly and Scientific Symposium ``Heritage and Landscape as Human Values''},
  year      = 2014
}

@inproceedings{nenci2014iros,
  title     = {{Effective Compression of Range Data Streams for Remote Robot Operations using H.264}},
  author    = {F. Nenci and L. Spinello and C. Stachniss},
  booktitle = iros,
  year      = 2014,
  url       = {http://www2.informatik.uni-freiburg.de/~stachnis/pdf/nenci14iros.pdf}
}

@inproceedings{vysotska2014iros,
  title     = {{Automatic Channel Selection and Neural Signal Estimation across Channels of Neural Probes}},
  author    = {O. Vysotska and B. Frank and I. Ulbert and O. Paul and P. Ruther and C. Stachniss and W. Burgard},
  booktitle = iros,
  year      = 2014,
  url       = {http://ais.informatik.uni-freiburg.de/publications/papers/vysotska14iros.pdf},
  keywords  = {Neurorobotics, Brain Machine Interfaces}
}

@article{agarwal2014ram,
  author  = {Pratik Agarwal and Wolfram Burgard and Cyrill Stachniss},
  title   = {{A Survey of Geodetic Approaches to Mapping and the Relationship to Graph-Based SLAM}},
  journal = ram,
  volume  = {21},
  number  = {3},
  pages   = {63--80},
  year    = 2014,
  url     = {http://ais.informatik.uni-freiburg.de/publications/papers/agarwal-geodetic.pdf}
}

@article{frank2014ras,
  author  = {B. Frank and C. Stachniss and R. Schmedding and M. Teschner and W. Burgard},
  title   = {{Learning object deformation models for robot motion planning}},
  journal = jras,
  year    = 2014,
  volume  = {62},
  number  = {8},
  pages   = {1153--1174},
  url     = {http://ais.informatik.uni-freiburg.de/publications/papers/frank14ras.pdf}
}

@inproceedings{abdo2014icra,
  title     = {{Inferring What to Imitate in Manipulation Actions by Using a Recommender System}},
  author    = {N. Abdo and L. Spinello and W. Burgard and C. Stachniss},
  booktitle = icra,
  year      = 2014
}

@inproceedings{agarwal2014icra-eaod,
  author    = {P. Agarwal and G. Grisetti and G.D. Tipaldi and L. Spinello and W. Burgard and C. Stachniss},
  title     = {{Experimental Analysis of Dynamic Covariance Scaling for Robust Map Optimization Under Bad Initial Estimates}},
  booktitle = icra,
  year      = 2014,
  url       = {http://ais.informatik.uni-freiburg.de/publications/papers/agarwal2014bicra.pdf}
}

@inproceedings{agarwal2014icra-habg,
  author    = {P. Agarwal and W. Burgard and C. Stachniss},
  title     = {Helmert's and Bowie's Geodetic Mapping Methods and Their Relation to Graph-Based SLAM},
  booktitle = icra,
  year      = 2014,
  url       = {http://ais.informatik.uni-freiburg.de/publications/papers/agarwal2014aicra.pdf}
}

@inproceedings{mazuran2014icra,
  author    = {M. Mazuran and G.D. Tipaldi and L. Spinello and W. Burgard and C. Stachniss},
  title     = {{A Statistical Measure for Map Consistency in SLAM}},
  booktitle = icra,
  year      = 2014,
  url       = {http://ais.informatik.uni-freiburg.de/publications/papers/mazuran14icra1.pdf}
}

@inproceedings{osswald2014icra,
  author    = {S. O{\ss}wald and H. Kretzschmar and W. Burgard and C. Stachniss},
  title     = {{Learning to Give Route Directions from Human Demonstrations}},
  booktitle = icra,
  year      = 2014,
  url       = {http://ais.informatik.uni-freiburg.de/publications/papers/osswald14icra.pdf},
  abstract  = {For several applications, robots and other computer systems must provide route descriptions to humans. These descriptions should be natural and intuitive for the human users. In this paper, we present an algorithm that learns how to provide good route descriptions from a corpus of human-written directions. Using inverse reinforcement learning, our algorithm learns how to select the information for the description depending on the context of the route segment. The algorithm then uses the learned policy to generate directions that imitate the style of the descriptions provided by humans, thus taking into account personal as well as cultural preferences and special requirements of the particular user group providing the learning demonstrations. We evaluate our approach in a user study and show that the directions generated by our policy sound similar to human-given directions and substantially more natural than directions provided by commercial web services.}
}

@inproceedings{ito2014icra,
  author    = {S. Ito and F. Endres and M. Kuderer and G.D. Tipaldi and C. Stachniss and W. Burgard},
  title     = {{W-RGB-D: Floor-Plan-Based Indoor Global Localization Using a Depth Camera and WiFi}},
  booktitle = icra,
  year      = 2014,
  url       = {http://ais.informatik.uni-freiburg.de/publications/papers/ito14icra.pdf}
}

@article{kuemmerle2014jfr,
  author  = {K{\"u}mmerle, R. and Ruhnke, M. and Steder, B. and Stachniss, C. and Burgard, W.},
  title   = {{Autonomous Robot Navigation in Highly Populated Pedestrian Zones}},
  journal = jfr,
  volume  = {34},
  number  = {4},
  pages   = {565--589},
  year    = 2014,
  url     = {http://ais.informatik.uni-freiburg.de/publications/papers/kuemmerle14jfr.pdf}
}

@article{endres2014tro,
  author  = {F. Endres and J. Hess and J. Sturm and D. Cremers and W. Burgard},
  title   = {{3D Mapping with an RGB-D Camera}},
  journal = tro,
  volume  = {30},
  number  = {1},
  pages   = {177--187},
  year    = 2014,
  url     = {http://ais.informatik.uni-freiburg.de/publications/papers/endres14tro.pdf}
}

@techreport{handa2014tr,
  author      = {A. Handa},
  title       = {{Simplified Jacobians in 6-DoF Camera Tracking}},
  institution = {University of Cambridge},
  year        = {2014},
  keywords    = {SLAM, Mapping},
  url         = {http://www.doc.ic.ac.uk/~ahanda/simjacob.pdf}
}

@inproceedings{salas-moreno2014ismar,
  author    = {R. F. Salas-Moreno and B. Glocker and P. H. J. Kelly and A. J. Davison},
  title     = {{Dense Planar SLAM}},
  booktitle = ismar,
  year      = {2014},
  keywords  = {SLAM, RGB-D Perception},
  url       = {https://www.doc.ic.ac.uk/~rfs09/docs/salas-moreno_ismar2014.pdf}
}

@article{stueckler2014vcir,
  author   = {J. St{\"u}ckler and S. Behnke},
  title    = {{Multi-Resolution Surfel Maps for Efficient Dense 3D Modeling and Tracking}},
  journal  = jvcir,
  year     = {2014},
  volume   = {25},
  pages    = {137--147},
  number   = {1},
  keywords = {Mapping, SLAM, RGB-D Perception},
  url      = {https://www.ais.uni-bonn.de/papers/JVCI_13_RGB-D-SLAM.pdf}
}

@article{whelan2014ijrr,
  author   = {T. Whelan and M. Kaess and H. Johannsson and M. Fallon and J. J. Leonard and J. McDonald},
  title    = {{Real-time large scale dense RGB-D SLAM with volumetric fusion}},
  journal  = ijrr,
  year     = {2014},
  volume   = {34},
  pages    = {598--626},
  number   = {4-5},
  abstract = {We present a new simultaneous localization and mapping SLAM system capable of producing high-quality globally consistent surface reconstructions over hundreds of meters in real time with only a low-cost commodity RGB-D sensor. By using a fused volumetric surface reconstruction we achieve a much higher quality map over what would be achieved using raw RGB-D point clouds. In this paper we highlight three key techniques associated with applying a volumetric fusion-based mapping system to the SLAM problem in real time. First, the use of a GPU-based 3D cyclical buffer trick to efficiently extend dense every-frame volumetric fusion of depth maps to function over an unbounded spatial region. Second, overcoming camera pose estimation limitations in a wide variety of environments by combining both dense geometric and photometric camera pose constraints. Third, efficiently updating the dense map according to place recognition and subsequent loop closure constraints by the use of an 'as-rigid-as-possible' space deformation. We present results on a wide variety of aspects of the system and show through evaluation on de facto standard RGB-D benchmarks that our system performs strongly in terms of trajectory estimation, map quality and computational performance in comparison to other state-of-the-art systems.},
  keywords = {SLAM, Mapping, RGB-D Perception},
  url      = {http://thomaswhelan.ie/Whelan14ijrr.pdf}
}

@inproceedings{tancik2020neurips,
  title     = {Fourier features let networks learn high frequency functions in low dimensional domains},
  author    = {Tancik, Matthew and Srinivasan, Pratul and Mildenhall, Ben and Fridovich-Keil, Sara and Raghavan, Nithin and Singhal, Utkarsh and Ramamoorthi, Ravi and Barron, Jonathan and Ng, Ren},
  booktitle = neurips,
  year      = {2020},
  url       = {https://arxiv.org/pdf/2006.10739.pdf}
}

@inproceedings{zhang2014rss,
  author    = {J. Zhang and S. Singh},
  title     = {{LOAM: Lidar Odometry and Mapping in Real-time}},
  booktitle = rss,
  year      = {2014},
  abstract  = {We propose a real-time method for odometry and mapping using range measurements from a 2-axis lidar moving in 6-DOF. The problem is hard because the range measurements are received at different times, and errors in motion estimation can cause mis-registration of the resulting point cloud. To date, coherent 3D maps can be built by off-line batch methods, often using loop closure to correct for drift over time. Our method achieves both low-drift and low-computational complexity without the need for high accuracy ranging or inertial measurements. The key idea in obtaining this level of performance is the division of the complex problem of simultaneous localization and mapping, which seeks to optimize a large number of variables simultaneously, by two algorithms. One algorithm performs odometry at a high frequency but low fidelity to estimate velocity of the lidar. Another algorithm runs at a frequency of an order of magnitude lower for fine matching and registration of the point cloud. Combination of the two algorithms allows the method to map in real-time. The method has been evaluated by a large set of experiments as well as on the KITTI odometry benchmark. The results indicate that the method can achieve accuracy at the level of state of the art offline batch methods.},
  url       = {proceedings:zhang2014rss.pdf}
}

@article{qin2014jprs,
  author  = {Qin, R. and Gruen, A.},
  journal = jprs,
  pages   = {23--35},
  title   = {{3D Change Detection at Street Level Using Mobile Laser Scanning Point Clouds and Terrestrial Images}},
  volume  = {90},
  year    = {2014},
  url     = {https://www.researchgate.net/profile/Rongjun_Qin/publication/260265360_3D_change_detection_at_street_level_using_mobile_laser_scanning_point_clouds_and_terrestrial_images/links/00b495306a0c9f2b44000000.pdf}
}

@inproceedings{ulusoy2014eccv,
  title     = {{Image-Based 4D Reconstruction Using 3D Change Detection}},
  author    = {Ulusoy, A.O. and Mundy, J.L.},
  booktitle = eccv,
  year      = {2014},
  url       = {http://ai2-s2-pdfs.s3.amazonaws.com/4f64/0fcdfb8773191786c8c17de1bb73aeec31e4.pdf}
}

@inproceedings{indelman2014icra,
  title     = {Planning under uncertainty in the continuous domain: a generalized belief space approach},
  author    = {V. Indelman and L. Carlone and F. Dellaert},
  booktitle = icra,
  year      = {2014},
  url       = {https://smartech.gatech.edu/bitstream/handle/1853/53725/Indelman14icra_a.pdf}
}

@inproceedings{anthony2014iros,
  title     = {On crop height estimation with UAVs},
  author    = {D. Anthony and S. Elbaum and A.Lorenz and C. Detweiler},
  booktitle = iros,
  year      = {2014}
}

@inproceedings{chen2014acra,
  title     = {Convolutional Neural Network-based Place Recognition},
  author    = {Z. Chen and O. Lam and A. Jacobson and M.Milford},
  booktitle = acra,
  year      = {2014}
}

@inproceedings{sermanet2014iclr,
  title     = {{OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks}},
  author    = {Sermanet, P. and Eigen, D. and Zhang, Z. and Mathieu, M. and Fergus, R. and LeCun, Y.},
  booktitle = iclr,
  year      = {2014},
  url       = {http://openreview.net/document/d332e77d-459a-4af8-b3ed-55ba}
}

@conference{haug2014wacv,
  title     = {Plant Classification System for Crop / Weed Discrimination without Segmentation},
  author    = {S. Haug and A. Michaels and P. Biber and J. Ostermann},
  booktitle = wacv,
  year      = {2014}
}

@article{rainville2012paa,
  author  = { F.M. De Rainville and A. Durand and F.A. Fortin and K. Tanguy and X. Maldague and B. Panneton and M.J. Simard},
  title   = { Bayesian Classification and Unsupervised Learning for Isolating Weeds in Row Crops },
  volume  = { 17 },
  number  = { 2 },
  pages   = { 401--414 },
  year    = { 2014 },
  journal = paa
}

@inproceedings{goodfellow2014nips,
  author    = {Goodfellow, I.J. and Pouget-Abadie, J. and Mirza, M. and Xu, B. and Warde-Farley, D. and Ozair, S. and Courville, A. and Bengio, Y.},
  title     = {{Generative Adversarial Networks}},
  booktitle = nips,
  year      = 2014,
  url       = {https://arxiv.org/pdf/1406.2661},
  abstract  = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.}
}

@inproceedings{pizzoli2014icra,
  title     = {{REMODE: Probabilistic, Monocular Dense Reconstruction in Real Time}},
  author    = {Pizzoli, M. and Forster, C. and Scaramuzza, D.},
  booktitle = icra,
  year      = {2014},
  url       = {https://infoscience.epfl.ch/record/199737/files/ICRA14_Pizzoli.pdf}
}

@article{vasquez2014ijars,
  title   = {{Volumetric Next-Best-View Planning for 3D Object Reconstruction with Positioning Error}},
  author  = {Vasquez-Gomez, J.I. and Sucar, L.E. and Murrieta-Cid, R. and Lopez-Damian, E.},
  journal = ijars,
  volume  = {11},
  number  = {10},
  year    = {2014},
  url     = {https://s3.amazonaws.com/academia.edu.documents/42748905/Volumetric_Next-Best-View_Planning_for_320160216-11239-1tpjehh.pdf?AWSAccessKeyId=AKIAIWOWYYGZ2Y53UL3A&Expires=1515513155&Signature=jxMf7jn%2BqY%2BSu9UIvCaoFCsBHVQ%3D&response-content-disposition=inline%3B%20filename%3DVolumetric_Next-best-view_Planning_for_3.pdf}
}

@inproceedings{sucar2021iccv,
  title     = {imap: Implicit mapping and positioning in real-time},
  author    = {Sucar, Edgar and Liu, Shikun and Ortiz, Joseph and Davison, Andrew J},
  booktitle = iccv,
  year      = {2021},
  url       = {proceedings:sucar2021iccv.pdf}
}

@article{campos2021tro,
  title   = {Orb-slam3: An accurate open-source library for visual, visual--inertial, and multimap slam},
  author  = {Campos, Carlos and Elvira, Richard and Rodr{\'\i}guez, Juan J G{\'o}mez and Montiel, Jos{\'e} MM and Tard{\'o}s, Juan D},
  journal = tro,
  volume  = {37},
  number  = {6},
  pages   = {1874--1890},
  year    = {2021},
  url     = {https://arxiv.org/pdf/2007.11898.pdf}
}

@article{usenko2019ral,
  title   = {Visual-inertial mapping with non-linear factor recovery},
  author  = {Usenko, Vladyslav and Demmel, Nikolaus and Schubert, David and St{\"u}ckler, J{\"o}rg and Cremers, Daniel},
  journal = ral,
  volume  = {5},
  number  = {2},
  pages   = {422--429},
  year    = {2019},
  url     = {https://cvg.cit.tum.de/_media/spezial/bib/usenko19nfr.pdf}
}

@article{qin2018tro,
  title   = {Vins-mono: A robust and versatile monocular visual-inertial state estimator},
  author  = {Qin, Tong and Li, Peiliang and Shen, Shaojie},
  journal = tro,
  volume  = {34},
  number  = {4},
  pages   = {1004--1020},
  year    = {2018},
  url     = {https://arxiv.org/pdf/1708.03852.pdf}
}

@article{bloesch2017ijrr,
  title   = {Iterated extended Kalman filter based visual-inertial odometry using direct photometric feedback},
  author  = {Bloesch, Michael and Burri, Michael and Omari, Sammy and Hutter, Marco and Siegwart, Roland},
  journal = ijrr,
  volume  = {36},
  number  = {10},
  pages   = {1053--1072},
  year    = {2017},
  url     = {https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/263423/1/ROVIO.pdf}
}

@inproceedings{geneva2020icra,
  title     = {Openvins: A research platform for visual-inertial estimation},
  author    = {Geneva, Patrick and Eckenhoff, Kevin and Lee, Woosik and Yang, Yulin and Huang, Guoquan},
  booktitle = icra,
  year      = {2020},
  url       = {proceedings:geneva2020icra.pdf}
}

@inproceedings{kerl2013iros,
  title     = {Dense visual SLAM for RGB-D cameras},
  author    = {Kerl, Christian and Sturm, J{\"u}rgen and Cremers, Daniel},
  booktitle = iros,
  year      = {2013},
  url       = {proceedings:kerl2013iros.pdf}
}

@inproceedings{mostegel2014icra,
  title     = {{Active Monocular Localization: Towards Autonomous Monocular Exploration for Multirotor MAVs}},
  author    = {Mostegel, C. and Wendel, A. and Bischof, H.},
  booktitle = icra,
  year      = {2014},
  url       = {https://www.researchgate.net/profile/Christian_Mostegel/publication/261476118_Active_Monocular_Localization_Towards_Autonomous_Monocular_Exploration_for_Multirotor_MAVs/links/0f3175346418523fbe000000.pdf}
}

@inproceedings{lin2021iccv,
  title     = {Barf: Bundle-adjusting neural radiance fields},
  author    = {Lin, Chen-Hsuan and Ma, Wei-Chiu and Torralba, Antonio and Lucey, Simon},
  booktitle = iccv,
  year      = {2021},
  url       = {proceedings:lin2021iccv.pdf}
}

@inproceedings{martin2021cvpr,
  title     = {Nerf in the wild: Neural radiance fields for unconstrained photo collections},
  author    = {Martin-Brualla, Ricardo and Radwan, Noha and Sajjadi, Mehdi SM and Barron, Jonathan T and Dosovitskiy, Alexey and Duckworth, Daniel},
  booktitle = cvpr,
  year      = {2021},
  url       = {https://arxiv.org/pdf/2008.02268.pdf}
}

@inproceedings{sadat2014icra,
  title     = {{Feature-Rich Path Planning for Robust Navigation of MAVs with Mono-SLAM}},
  author    = {Sadat, S.A. and Chutskoff, K. and Jungic, D. and Wawerla, J. and Vaughan, R.},
  booktitle = icra,
  year      = {2014},
  url       = {http://autonomy.cs.sfu.ca/doc/sadat_icra14.pdf}
}

@inproceedings{forster2014rss,
  title     = {{Appearance-based Active, Monocular, Dense Reconstruction for Micro Aerial Vehicles}},
  author    = {Forster, C. and Pizzoli, M. and Scaramuzza, D.},
  booktitle = rss,
  year      = {2014},
  url       = {https://infoscience.epfl.ch/record/203672/files/RSS14_Forster.pdf}
}

@article{potthast2014jvcir,
  title   = {{A Probabilistic Framework for Next Best View Estimation in a Cluttered Environment}},
  author  = {Potthast, C. and Sukhatme, G.S.},
  journal = jvcir,
  volume  = {25},
  number  = {1},
  pages   = {148--164},
  year    = {2014},
  url     = {http://robotics.usc.edu/~potthast/potthast_vua2014.pdf}
}

@inproceedings{neubert2013ecmr,
  author    = {P. Neubert and N. Sunderhauf and P. Protzel},
  title     = {{Appearance Change Prediction for Long-Term Navigation Across Seasons}},
  booktitle = ecmr,
  keywords  = {Localization, Visual Navigation, Place Recognition},
  year      = {2013}
}

@article{churchill2013ijrr,
  author   = {W. Churchill and P. Newman},
  journal  = ijrr,
  keywords = {Localization, Visual Navigation},
  url      = {http://www.robots.ox.ac.uk/~mobile/Papers/2013IJRR_Churchill.pdf},
  title    = {{Experience-Based Navigation for Long-Term Localisation}},
  volume   = {32},
  number   = {14},
  pages    = {1645--1661},
  year     = 2013
}

@inproceedings{johns2013icra,
  author    = {E. Johns and G.-Z. Yang},
  booktitle = icra,
  title     = {{Feature Co-occurrence Maps: Appearance-Based Localisation Throughout the Day}},
  keywords  = {Localization},
  year      = 2013
}

@article{zeiler2013arxiv,
  author   = {M.D. Zeiler and R. Fergus},
  title    = {{Visualizing and Understanding Convolutional Networks}},
  journal  = arxiv,
  volume   = {arXiv:1311.2901},
  year     = 2013,
  keywords = {CNN},
  url      = {http://arxiv.org/pdf/1311.2901},
  abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. We also perform an ablation study to discover the performance contribution from different model layers. This enables us to find model architectures that outperform Krizhevsky \etal on the ImageNet classification benchmark. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets. }
}

@inproceedings{bogoslavskyi2013ecmr,
  author    = {I. Bogoslavskyi and O. Vysotska and J. Serafin and G. Grisetti and C. Stachniss},
  title     = {{Efficient Traversability Analysis for Mobile Robots using the Kinect Sensor}},
  booktitle = ecmr,
  year      = 2013,
  url       = {http://ais.informatik.uni-freiburg.de/publications/papers/bogoslavskyi13ecmr.pdf}
}

@article{burgard2013forschung,
  author  = {W. Burgard and C. Stachniss},
  title   = {{G}estatten, {O}belix!},
  journal = {Forschung -- Das {M}agazin der {D}eutschen {F}orschungsgemeinschaft},
  volume  = 1,
  year    = 2013,
  url     = {http://ais.informatik.uni-freiburg.de/publications/papers/forschung_2013_01-pg4-9.pdf}
}

@article{maier2013ijhr,
  author  = {D. Maier and C. Stachniss and M. Bennewitz},
  title   = {{Vision-Based Humanoid Navigation Using Self-Supervised Obstacle Detection}},
  journal = ijhr,
  volume  = {10},
  number  = {2},
  year    = 2013
}

@article{wurm2013ar,
  author  = {K.M. Wurm and C. Dornhege and B. Nebel and W. Burgard and C. Stachniss},
  title   = {{Coordinating Heterogeneous Teams of Robots using Temporal Symbolic Planning}},
  journal = ar,
  year    = 2013,
  url     = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/wurm13auro.pdf},
  volume  = {34},
  pages   = {277--294}
}

@article{wurm2013ras,
  author  = {K.M. Wurm and H. Kretzschmar and R. K{\"u}mmerle and C. Stachniss and W. Burgard},
  title   = {{Identifying Vegetation from Laser Data in Structured Outdoor Environments}},
  journal = jras,
  volume  = {62},
  number  = {5},
  pages   = {675--684},
  year    = 2013,
  url     = {http://ais.informatik.uni-freiburg.de/publications/papers/wurm13ras.pdf}
}

@inproceedings{abdo2013icra,
  author    = {N. Abdo and H. Kretzschmar and L. Spinello and C. Stachniss},
  title     = {{Learning Manipulation Actions from a Few Demonstrations}},
  booktitle = icra,
  year      = 2013,
  url       = {http://ais.informatik.uni-freiburg.de/publications/papers/abdo13icra.pdf}
}

@inproceedings{agarwal2013icra,
  author    = {P. Agarwal and G.D. Tipaldi and L. Spinello and C. Stachniss and W. Burgard},
  title     = {{Robust Map Optimization using Dynamic Covariance Scaling}},
  booktitle = icra,
  year      = 2013,
  url       = {http://ais.informatik.uni-freiburg.de/publications/papers/agarwalicra13_DCS.pdf}
}

@inproceedings{agarwal2013icraws,
  title     = {{Dynamic Covariance Scaling for Robust Robotic Mapping}},
  author    = {P. Agarwal and G.D. Tipaldi and L. Spinello and C. Stachniss and W. Burgard},
  booktitle = {{ICRA Workshop on robust and Multimodal Inference in Factor Graphs}},
  year      = 2013,
  url       = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/agarwal13icraws.pdf}
}

@article{deng2013fntsp,
  title    = {{Deep Learning: Methods and Applications}},
  author   = {L. Deng and D. Yu},
  year     = 2013,
  volume   = 7,
  number   = {3--4},
  pages    = {197--387},
  journal  = {Foundations and Trends in Signal Processing},
  keywords = {Deep Learning},
  abstract = {This monograph provides an overview of general deep learning methodology and its applications to a variety of signal and information processing tasks. The application areas are chosen with the following three criteria in mind: (1) expertise or knowledge of the authors; (2) the application areas that have already been transformed by the successful use of deep learning technology, such as speech recognition and computer vision; and (3) the application areas that have the potential to be impacted significantly by deep learning and that have been experiencing research growth, including natural language and text processing, information retrieval, and multimodal information processing empowered by multi-task deep learning.}
}

@inproceedings{keller2013threedv,
  author    = {M. Keller and D. Lefloch and M. Lambers and S. Izadi},
  title     = {{Real-time 3D Reconstruction in Dynamic Scenes using Point-based Fusion}},
  booktitle = threedv,
  year      = {2013},
  keywords  = {SLAM, Mapping, RGB-D Perception},
  url       = {http://reality.cs.ucl.ac.uk/projects/kinect/keller13realtime.pdf}
}

@inproceedings{niessner2013siggraph,
  author    = {M. Nie{\ss}ner and M. Zollh{\"o}fer and S. Izadi and M. Stamminger},
  title     = {{Real-time 3D Reconstruction at Scale using Voxel Hashing}},
  booktitle = {Proc.~of the SIGGRAPH Asia},
  year      = {2013},
  keywords  = {SLAM, RGB-D Perception, Mapping},
  url       = {https://graphics.stanford.edu/~niessner/papers/2013/4hashing/niessner2013hashing.pdf}
}


@inproceedings{moosmann2013icra,
  author    = {F. Moosmann and C. Stiller},
  title     = {{Joint Self-Localization and Tracking of Generic Objects in 3D Range Data}},
  booktitle = icra,
  year      = {2013},
  abstract  = {Both, the estimation of the trajectory of a sensor and the detection and tracking of moving objects are essential tasks for autonomous robots. This work proposes a new algorithm that treats both problems jointly. The sole input is a sequence of dense 3D measurements as returned by multi-layer laser scanners or time-of-flight cameras. A major characteristic of the proposed approach is its applicability to any type of environment since specific object models are not used at any algorithm stage. More specifically, precise localization in non-flat environments is possible as well as the detection and tracking of e.g. trams or recumbent bicycles. Moreover, 3D shape estimation of moving objects is inherent to the proposed method. Thorough evaluation is conducted on a vehicular platform with a mounted Velodyne HDL-64E laser scanner.},
  keywords  = {Visual Tracking, Localization, Range Sensing},
  url       = {proceedings:moosmann2013icra.pdf}
}

@inproceedings{saarinen2013iros-f3mi,
  author    = {J.P. Saarinen and T. Stoyanov and H. Andreasson and A.J. Lilienthal},
  title     = {{Fast 3D Mapping in Highly Dynamic Environments Using Normal Distributions Transform Occupancy Maps}},
  booktitle = iros,
  year      = 2013,
  keywords  = {Mapping, SLAM},
  abstract  = {Autonomous vehicles operating in real-world industrial environments have to overcome numerous challenges, chief among which is the creation and maintenance of consistent 3D world models. This paper focuses on a particularly important challenge: mapping in dynamic environments. We introduce several improvements to the recently proposed Normal Distributions Transform Occupancy Map (NDT-OM) aimed for efficient mapping in dynamic environments. A careful consistency analysis is given based on convergence and similarity metrics specifically designed for evaluation of NDT maps in dynamic environments. We show that in the context of mapping with known poses the proposed method results in improved consistency and in superior runtime performance, when compared against 3D occupancy grids at the same size and resolution. Additionally, we demonstrate that NDT-OM features real-time performance in a highly dynamic 3D mapping and tracking scenario with centimeter accuracy over a 1.5km trajectory.},
  url       = {proceedings:saarinen2013iros-f3mi.pdf}
}

@inproceedings{das2013icra-3sru,
  author    = {A. Das and J. Servos and S.L. Waslander},
  title     = {{3D Scan Registration Using the Normal Distributions Transform with Ground Segmentation and Point Cloud Clustering}},
  booktitle = icra,
  year      = 2013,
  keywords  = {Mapping, SLAM, Autonomous Navigation},
  abstract  = {The Normal Distributions Transform (NDT) scan registration algorithm models the environment as a set of Gaussian distributions and generates the Gaussians by discretizing the environment into voxels. With the standard approach, the NDT algorithm has a tendency to have poor convergence performance for even modest initial transformation error. In this work, a segmented greedy cluster NDT (SGC-NDT) variant is proposed, which uses natural features in the environment to generate Gaussian clusters for the NDT algorithm. By segmenting the ground plane and clustering the remaining features, the SGC-NDT approach results in a smooth and continuous cost function which guarantees that the optimization will converge. Experiments show that the SGC-NDT algorithm results in scan registrations with higher accuracy and better convergence properties when compared against other state-ofthe-art methods for both urban and forested environments.},
  url       = {proceedings:das2013icra-3sru.pdf}
}

@inproceedings{taneja2013cvpr,
  title     = {{City-Scale Change Detection in Cadastral 3D Models Using Images}},
  author    = {Taneja, A. and Ballan, L. and Pollefeys, M.},
  booktitle = cvprold,
  year      = {2013},
  url       = {https://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Taneja_City-Scale_Change_Detection_2013_CVPR_paper.pdf}
}

@inproceedings{sakurada2013cvpr,
  title     = {{Detecting Changes in 3D Structure of a Scene from Multi-View Images Captured by a Vehicle-Mounted Camera}},
  author    = {Sakurada, K. and Okatani, T. and Deguchi, K.},
  booktitle = cvprold,
  year      = {2013},
  url       = {https://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Sakurada_Detecting_Changes_in_2013_CVPR_paper.pdf}
}

@inproceedings{floros2013icra,
  title     = {Openstreetslam: Global vehicle localization using openstreetmaps},
  author    = {G. Floros and B. van der Zander and B Leibe},
  booktitle = icra,
  year      = {2013},
  url       = {http://www.vision.rwth-aachen.de/media/papers/florosicra13.pdf}
}

@article{li2013acmgraphics,
  title   = {Analyzing growing plants from 4D point cloud data},
  author  = {Y. Li and X. Fan and N.J. Mitra and D. Chamovitz and D. Cohen-Or and B. Chen},
  journal = acmgraphics,
  volume  = {32},
  number  = {6},
  pages   = {157},
  year    = {2013},
  url     = {https://www.cs.tau.ac.il/~dcor/articles/2013/Analyzing_Growing_Plants_from_4D.pdf}
}

@article{milford2013ijrr,
  title   = {Vision-based place recognition: how low can you go?},
  author  = {Milford, M.},
  journal = ijrr,
  volume  = {32},
  number  = {7},
  pages   = {766--789},
  year    = {2013}
}

@article{hartley2013ijcv,
  author  = {R. Hartley and J. Trumpf and Y. Dai and H. Li},
  title   = {Rotation averaging},
  journal = ijcv,
  year    = 2013,
  pages   = {267--305},
  volume  = 103
}

@inproceedings{heng2013iros,
  author    = {L. Heng and B. Li and M. Pollefeys},
  title     = {Camodocal: Automatic intrinsic and extrinsic calibration of a rig with multiple generic cameras and odometry},
  booktitle = iros,
  year      = 2013
}

@article{guo2013cea,
  author  = {W. Guo and U.K. Rage and S. Ninomiya},
  title   = {{Illumination Invariant Segmentation of Vegetation for Time Series Wheat Images Based on Decision Tree Model}},
  journal = cea,
  volume  = {96},
  year    = {2013},
  pages   = {58--66}
}

@article{moakher2002siam,
  title   = {{Means and averaging in the group of rotations}},
  author  = {Moakher, Maher},
  journal = {SIAM Journal on Matrix Analysis and Applications (SIMAX)},
  volume  = {24},
  number  = {1},
  pages   = {1--16},
  year    = {2002},
  url     = {https://lcvmwww.epfl.ch/publications/data/articles/63/Means_and_Averaging_in_the_Group_of_Rotations.pdf}
}

@article{miyagusuku2019ral,
  author  = {Miyagusuku, Renato and Yamashita, Atsushi and Asama, Hajime},
  journal = ral,
  title   = {{Data Information Fusion From Multiple Access Points for WiFi-Based Self-localization}},
  year    = {2019},
  volume  = {4},
  number  = {2},
  pages   = {269--276},
  url     = {proceedings: miyagusuku2019ral.pdf}
}

@article{ponti2013grsl,
  author   = {M.P. Ponti},
  journal  = grsl,
  title    = {Segmentation of Low-Cost Remote Sensing Images Combining Vegetation Indices and Mean Shift},
  year     = 2013,
  volume   = {10},
  number   = {1},
  pages    = {67--70},
  keywords = {Agriculture, Image segmentation, Vegetation mapping, Precision Agriculture},
  url      = {http://conteudo.icmc.usp.br/pessoas/moacir/papers/Ponti_GRSL2013.pdf}
}

@inproceedings{freundlich2013icra,
  title     = {{A Hybrid Control Approach to the Next-Best-View Problem using Stereo Vision}},
  author    = {Freundlich, Charles and Mordohai, Philippos and Zavlanos, Michael M},
  booktitle = icra,
  year      = {2013},
  url       = {https://pdfs.semanticscholar.org/07dc/f756e50c8b8b48613b02a40fa6bd69b56aff.pdf}
}

@inproceedings{badino2013iccvw,
  title     = {{Visual Odometry by Multi-frame Feature Integration}},
  author    = {H. Badino and A. Yamamoto and T. Kanade},
  booktitle = {Proc.~of IEEE International Conference on Computer Vision Workshops (ICCVW)},
  year      = {2013},
  keywords  = {Mapping, SLAM, Visual-based Navigation, Visual Odometry},
  url       = {http://www.lelaps.de/papers/badino_cvad13.pdf}
}

@inproceedings{quin2013icra,
  title     = {{Efficient Neighbourhood-Based Information Gain Approach for Exploration of Complex 3D Environments}},
  author    = {Quin, P. and Paul, G. and Alempijevic, A. and Liu, D. and Dissanayake, G.},
  booktitle = icra,
  year      = {2013},
  url       = {https://opus.lib.uts.edu.au/bitstream/10453/27588/4/Quin%2C%20Paul%2C%20Alempijevic%2C%20Liu%2C%20Dissanayak%20-%20Ef%EF%AC%81cient%20Neighbourhood-Based%20Information%20Gain%20Approach%20for.pdf}
}

@inproceedings{milford2012icra,
  author    = {M. Milford and G.F. Wyeth},
  booktitle = icra,
  title     = {{SeqSLAM: Visual route-based navigation for sunny summer days and stormy winter nights.}},
  year      = {2012},
  keywords  = {Localization, Visual Navigation, Place Recognition}
}

@inproceedings{churchill2012icra,
  author    = {W. Churchill and P. Newman},
  booktitle = icra,
  title     = {{Practice Makes Perfect? Managing and Leveraging Visual Experiences for Lifelong Navigation}},
  keywords  = {Localization, Visual Navigation},
  year      = 2012
}

@article{stachniss2012fnt,
  author  = {C. Stachniss and W. Burgard},
  title   = {{Particle Filters for Robot Navigation}},
  journal = fntr,
  year    = 2012,
  volume  = 3,
  number  = 4,
  pages   = {211--282}
}

@inproceedings{grisetti2012rich,
  author    = {G. Grisetti and L. Iocchi and B. Leibe and V.A. Ziparo and C. Stachniss},
  title     = {{Digitization of Inaccessible Archeological Sites with Autonomous Mobile Robots}},
  booktitle = {Conf.~on Robotics Innovation for Cultural Heritage},
  year      = 2012
}

@inproceedings{abdo2012tampra,
  author    = {N. Abdo and H. Kretzschmar and C. Stachniss},
  title     = {{From Low-Level Trajectory Demonstrations to Symbolic Actions for Planning}},
  booktitle = {Proc.~of the ICAPS Workshop on Combining Task and Motion Planning for Real-World Applications (TAMPRA)},
  year      = 2012,
  url       = {http://ais.informatik.uni-freiburg.de/publications/papers/abdo12tampra.pdf}
}

@inproceedings{joho2012rss,
  author    = {D. Joho and G.D. Tipaldi and N. Engelhard and C. Stachniss and W. Burgard},
  title     = {Nonparametric {B}ayesian Models for Unsupervised Scene Analysis and Reconstruction},
  booktitle = rss,
  year      = 2012,
  url       = {http://ais.informatik.uni-freiburg.de/publications/papers/joho12rss.pdf}
}

@article{kretzschmar2012ijrr,
  author  = {H. Kretzschmar and C. Stachniss},
  title   = {Information-Theoretic Pose Graph Compression for Laser-based {SLAM}},
  journal = ijrr,
  year    = 2012,
  volume  = 31,
  number  = 11,
  pages   = {1219--1230}
}

@inproceedings{roewekaemper2012iros,
  author    = {J. Roewekaemper and C. Sprunk and G.D. Tipaldi and C. Stachniss and P. Pfaff and W. Burgard},
  title     = {{On the Position Accuracy of Mobile Robot Localization based on Particle Filters combined with Scan Matching}},
  booktitle = iros,
  year      = 2012,
  url       = {http://ais.informatik.uni-freiburg.de/publications/papers/roewekaemper12iros.pdf}
}

@inproceedings{spinello2012rssws,
  author    = {L. Spinello and C. Stachniss and W. Burgard},
  title     = {{Scene in the Loop: Towards Adaptation-by-Tracking in RGB-D Data}},
  booktitle = {Proc.~of the RSS Workshop RGB-D: Advanced Reasoning with Depth Cameras},
  year      = 2012,
  url       = {http://ais.informatik.uni-freiburg.de/publications/papers/spinello12rssws.pdf}
}

@book{murphy2012mlbook,
  author    = {K.P. Murphy},
  title     = {{Machine Learning -- A Probabilistic Perspective}},
  publisher = mitpress,
  year      = 2012,
  keywords  = {Book, Machine Learning},
  abstract  = {With the ever increasing amounts of data in electronic form, the need for automated methods for data analysis continues to grow. The goal of machine learning is to develop methods that can automatically detect patterns in data, and then to use the uncovered patterns to predict future data or other outcomes of interest. Machine learning is thus closely related to the fields of statistics and data mining, but di ers slightly in terms of its emphasis and terminology. This book provides a detailed introduction to the field, and includes worked examples drawn from application domains such as molecular biology, text processing, computer vision, and robotics.}
}

@inproceedings{geiger2012cvpr,
  author    = {A. Geiger and P. Lenz and R. Urtasun},
  title     = {{Are we ready for Autonomous Driving? The KITTI Vision Benchmark Suite}},
  booktitle = cvprold,
  year      = {2012},
  keywords  = {Autonomous Driving, Dataset, Classification, SLAM, Visual Odometry},
  url       = {http://www.cvlibs.net/publications/Geiger2012CVPR.pdf}
}


@article{stoyanov2012ijrr,
  author   = {T. Stoyanov and M. Magnusson and H. Andreasson and A. J. Lilienthal},
  title    = {{Fast and accurate scan registration through minimization of the distance between compact 3D NDT representations}},
  journal  = ijrr,
  year     = {2012},
  volume   = {31},
  pages    = {1377--1393},
  number   = {12},
  keywords = {Velodyne, Mapping, SLAM}
}

@inproceedings{brookshire2012rss,
  author    = {J. Brookshire and S. Teller},
  title     = {Extrinsic Calibration from Per-Sensor Egomotion},
  booktitle = rss,
  year      = 2012,
  url       = {proceedings:brookshire2012rss.pdf},
  abstract  = {We show how to recover the 6-DOF transform between two sensors mounted rigidly on a moving body, a form of extrinsic calibration useful for data fusion. Our algorithm takes noisy, per-sensor incremental egomotion observations (i.e., incremental poses) as input and produces as output an estimate of the maximum-likelihood 6-DOF calibration relating the sensors and accompanying uncertainty. The 6-DOF transformation sought can be represented effectively as a unit dual quaternion with 8 parameters subject to two constraints. Noise is explicitly modeled (via the Lie algebra), yielding a constrained Fisher Information Matrix and Cramer-Rao Lower Bound. The result is an analysis of motion degeneracy and a singularity-free optimization procedure. The method requires only that the sensors travel together along a motion path that is non-degenerate. It does not require that the sensors be synchronized, have overlapping fields of view, or observe common features. It does not require construction of a global reference frame or solving SLAM. In practice, from hand-held motion of RGB-D cameras, the method recovered inter-camera calibrations accurate to within 0.014 m and 0.022 radians (about 1 cm and 1 degree).}
}

@inproceedings{reinhardt2012icif,
  title     = {{Closed-Form Optimization of Covariance Intersection for Low-Dimensional Matrices}},
  author    = {Reinhardt, M. and Noack, B. and Hanebeck, U.D.},
  booktitle = icif,
  year      = {2012},
  url       = {http://isas.uni-karlsruhe.de/Publikationen/Fusion12_Reinhardt-FastCI.pdf}
}

@inproceedings{fraundorfer2012iros,
  title     = {{Vision-Based Autonomous Mapping and Exploration Using a Quadrotor MAV}},
  author    = {Fraundorfer, Friedrich and Heng, Lionel and Honegger, Dominik and Lee, Gim Hee and Meier, Lorenz and Tanskanen, Petri and Pollefeys, Marc},
  booktitle = iros,
  year      = {2012},
  url       = {https://www.inf.ethz.ch/personal/pomarc/pubs/FraundorferIROS12.pdf}
}

@inproceedings{hoppe2012bmvc,
  title     = {{Online Feedback for Structure-from-Motion Image Acquisition}},
  author    = {Christof H. and Manfred K. and Markus R. and Andreas W. and Stefan K. and Horst B. and Gerhard R.},
  year      = {2012},
  booktitle = bmvc,
  url       = {https://www.tugraz.at/fileadmin/user_upload/Institute/ICG/Images/team_fraundorfer/personal_pages/markus_rumpler/bmvc_2012_hoppe.pdf}
}

@article{schmid2012jirs,
  title   = {{View Planning for Multi-View Stereo 3D Reconstruction using an Autonomous Multicopter}},
  author  = {Schmid, K. and Hirschm{\"u}ller, H. and D{\"o}mel, A. and Grixa, I. and Suppa, M. and Hirzinger, G.},
  journal = jirs,
  volume  = {65},
  number  = {1--4},
  pages   = {309--323},
  year    = {2012},
  url     = {https://link.springer.com/content/pdf/10.1007%2Fs10846-011-9576-2.pdf}
}

@inproceedings{shen2012icra,
  title     = {{Autonomous Indoor 3D Exploration with a Micro-Aerial Vehicle}},
  author    = {Shen, S. and Michael, N. and Kumar, V.},
  booktitle = icra,
  year      = {2012}
}

@inproceedings{bennewitz2011humanoids,
  author    = {M. Bennewitz and D. Maier and A. Hornung and C. Stachniss},
  title     = {{Integrated Perception and Navigation in Complex Indoor Environments}},
  booktitle = {Proc.~of the IEEE-RAS Int.~Conf.~on Humanoid Robots (HUMANOIDS)},
  year      = 2011
}

@article{sturm2011jair,
  author  = {J. Sturm and C. Stachniss and W. Burgard},
  title   = {{A Probabilistic Framework for Learning Kinematic Models of Articulated Objects}},
  journal = jair,
  pages   = {477--526},
  volume  = 41,
  year    = 2011,
  url     = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/sturm11jair.pdf}
}

@inproceedings{becker2011irosws,
  author    = {J. Becker and C. Bersch and D. Pangercic and B. Pitzer and T. R\"uhr and B. Sankaran and J. Sturm and C. Stachniss and M. Beetz and W. Burgard},
  title     = {{Mobile Manipulation of Kitchen Containers}},
  booktitle = {Proc.~of the IROS'11 Workshop on Results, Challenges and Lessons Learned in Advancing Robots with a Common Platform},
  year      = 2011,
  url       = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/becker11irosws.pdf}
}

@inproceedings{kuemmerle2011arso,
  author    = {R. K\"ummerle and G. Grisetti and C. Stachniss and W. Burgard},
  title     = {{Simultaneous Parameter Calibration, Localization, and Mapping for Robust Service Robotics}},
  booktitle = {Proc.~of the IEEE Workshop on Advanced Robotics and its Social Impacts},
  year      = 2011,
  url       = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/kuemmerle11arso.pdf},
  abstract  = {Modern service robots are designed to be deployed by end-users and not to be monitored by experts during operation. Most service robotics applications require reliable navigation capabilities of the robot. The calibration parameters of a mobile robot play a substantial role in navigation tasks. Often these parameters are subject to variations that depend either on environmental changes or on the wear of the devices. In this paper, we propose an approach to simultaneously estimate a map of the environment, the position of the on-board sensors of the robot, and its kinematic parameters. Our method requires no prior knowledge about the environment and relies only on a rough initial guess of the platform parameters. The proposed approach performs on-line estimation of the parameters and it is able to adapt to non-stationary changes of the configuration. Our approach has been implemented and is used on the EUROPA robot, a service robot operating in urban environments. In addition to that, we tested our approach in simulated environments and on a wide range of real world data using different types of robotic platforms.}
}

@inproceedings{stachniss2011isrr,
  author    = {C. Stachniss and H. Kretzschmar},
  title     = {Pose Graph Compression for Laser-based {SLAM}},
  booktitle = isrr,
  year      = 2011
}

@inproceedings{kretzschmar2011iros,
  author    = {H. Kretzschmar and C. Stachniss and G. Grisetti},
  title     = {Efficient Information-Theoretic Graph Pruning for Graph-based {SLAM} with Laser Range Finders},
  booktitle = iros,
  year      = 2011
}

@inproceedings{wurm2011iros,
  title     = {{Hierarchies of Octrees for Efficient 3D Mapping}},
  author    = {K.M. Wurm and D. Hennes and D. Holz and R.B. Rusu and C. Stachniss and K. Konolige and W. Burgard},
  booktitle = iros,
  year      = 2011
}

@inproceedings{ziegler2011iros,
  author    = {J. Ziegler and H. Kretzschmar and C. Stachniss and G. Grisetti and W. Burgard},
  title     = {{Accurate Human Motion Capture in Large Areas by Combining IMU- and Laser-based People Tracking}},
  booktitle = iros,
  year      = 2011
}

@inproceedings{frank2011iros,
  author    = {B. Frank and C. Stachniss and N. Abdo and W. Burgard},
  title     = {{Efficient Motion Planning for Manipulation Robots in Environments with Deformable Objects}},
  booktitle = iros,
  year      = 2011
}

@inproceedings{frank2011pamr,
  author    = {B. Frank and C. Stachniss and N. Abdo and W. Burgard},
  title     = {{Using Gaussian Process Regression for Efficient Motion Planning in Environments with Deformable Objects}},
  booktitle = {Proc. of the AAAI-11 Workshop on Automated Action Planning for Autonomous Mobile Robots (PAMR)},
  year      = 2011
}

@inproceedings{maier2011icra,
  author    = {D. Maier and M. Bennewitz and C. Stachniss},
  title     = {{Self-supervised Obstacle Detection for Humanoid Navigation Using Monocular Vision and Sparse Laser Data}},
  booktitle = icra,
  year      = 2011
}

@inbook{asadi2011isfmo,
  author    = {S. Asadi and M. Reggente and C. Stachniss and C. Plagemann and A.J. Lilienthal},
  title     = {{Intelligent Systems for Machine Olfaction: Tools and Methodologies}},
  editor    = {E.L. Hines and M.S. Leeson},
  chapter   = {Statistical Gas Distribution Modelling using Kernel Methods},
  publisher = {{IGI} {G}lobal},
  pages     = {153--179},
  year      = 2011
}

@inproceedings{newcombe2011ismar,
  author    = {R. A. Newcombe and S. Izadi and O. Hilliges and D. Molyneaux and D. Kim and A. J. Davison and P. Kohli and J. Shotton and S. Hodges and A. Fitzgibbon},
  title     = {{KinectFusion: Real-Time Dense Surface Mapping and Tracking}},
  booktitle = ismar,
  year      = 2011,
  keywords  = {RGB-D, SLAM, Mapping},
  abstract  = {We present a system for accurate real-time mapping of complex and arbitrary indoor scenes in variable lighting conditions, using only a moving low-cost depth camera and commodity graphics hardware. We fuse all of the depth data streamed from a Kinect sensor into a single global implicit surface model of the observed scene in real-time. The current sensor pose is simultaneously obtained by tracking the live depth frame relative to the global model using a coarse-to-fine iterative closest point (ICP) algorithm, which uses all of the observed depth data available. We demonstrate the advantages of tracking against the growing full surface model compared with frame-to-frame tracking, obtaining tracking and mapping results in constant time within room sized scenes with limited drift and high accuracy. We also show both qualitative and quantitative results relating to various aspects of our tracking and mapping system. Modelling of natural scenes, in real-time with only commodity sensor and GPU hardware, promises an exciting step forward in augmented reality (AR), in particular, it allows dense surfaces to be reconstructed in real-time, with a level of detail and robustness beyond any solution yet presented using passive computer vision.},
  url       = {https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/ismar2011.pdf}
}

@inproceedings{moosmann2011iv,
  author    = {F. Moosmann and C. Stiller},
  title     = {{Velodyne SLAM}},
  booktitle = iv,
  year      = {2011},
  keywords  = {Mapping, Velodyne},
  url       = {http://www.mrt.kit.edu/z/publ/download/Moosmann_IV11.pdf}
}

@article{weise2011cviu,
  author   = {T. Weise and T. Wismer and B. Leibe and L. Van Gool},
  title    = {{Online loop closure for real-time interactive 3D scanning}},
  journal  = cviu,
  year     = {2011},
  volume   = {115},
  pages    = {635--648},
  keywords = {SLAM, Visual Odometry, Mapping},
  url      = {http://web-info8.informatik.rwth-aachen.de/media/papers/weise11CVIU.pdf}
}

@inproceedings{steder2011iros,
  author    = {B. Steder and M. Ruhnke and S. Grzonka and W. Burgard},
  title     = {{Place Recognition in 3D Scans Using a Combination of Bag of Words and Point Feature Based Relative Pose Estimation}},
  booktitle = iros,
  year      = 2011,
  keywords  = {Recognition, Range Sensing, SLAM},
  abstract  = {Place recognition, i.e., the ability to recognize previously seen parts of the environment, is one of the fundamental tasks in mobile robotics. The wide range of applications of place recognition includes localization (determine the initial pose), SLAM (detect loop closures), and change detection in dynamic environments. In the past, only relatively little work has been carried out to attack this problem using 3D range data and the majority of approaches focuses on detecting similar structures without estimating relative poses. In this paper, we present an algorithm based on 3D range data that is able to reliably detect previously seen parts of the environment and at the same time calculates an accurate transformation between the corresponding scan-pairs. Our system uses the estimated transformation to evaluate a candidate and in this way to more robustly reject false positives for place recognition. We present an extensive set of experiments using publicly available datasets in which we compare our system to other state-of-theart approaches.},
  url       = {proceedings:steder2011iros.pdf}
}

@inproceedings{taneja2011iccv,
  author    = {Taneja, A. and Ballan, L. and Pollefeys, M.},
  booktitle = iccvold,
  title     = {{Image Based Detection of Geometric Changes in Urban Environments}},
  year      = {2011},
  url       = {https://www.inf.ethz.ch/personal/marc.pollefeys/pubs/TanejaICCV11.pdf}
}

@inproceedings{golparvar2011iccvw,
  title     = {{Monitoring Changes of 3D Building Elements from Unordered Photo Collections}},
  author    = {Golparvar-Fard, M. and Pena-Mora, F. and Savarese, S.},
  booktitle = iccvws,
  year      = {2011},
  url       = {http://vhosts.eecs.umich.edu/vision/papers/Golparvar-Fard-cvrse2011.pdf}
}

@inproceedings{konolige2011icra,
  title     = {Navigation in hybrid metric-topological maps},
  author    = {K. Konolige and E. Marder-Eppstein and B. Marthi},
  booktitle = icra,
  year      = {2011},
  url       = {https://www.willowgarage.com/sites/default/files/icra-top-nav.pdf}
}

@inproceedings{candido2011icra,
  title     = {Minimum uncertainty robot navigation using information-guided POMDP planning},
  author    = {S. Candido and S. Hutchinson},
  booktitle = icra,
  year      = {2011},
  url       = {https://pdfs.semanticscholar.org/6d92/e83a550e137f0550269952856288f19f2417.pdf}
}

@inproceedings{kawano2011aim,
  title     = {Study of path planning method for under-actuated blimp-type UAV in stochastic wind disturbance via augmented-MDP},
  author    = {H. Kawano},
  booktitle = {IEEE/ASME International Conference on Advanced Intelligent Mechatronics (AIM)},
  year      = {2011},
  url       = {https://www.ipb.uni-bonn.de:5555/pdfs/kawano2011aim.pdf}
}

@article{agarwal2011cacm,
  author  = {S. Agarwal and Y. Furukawa and N. Snavely and I. Simon and B. Curless and S.M. Seitz and R. Szeliski},
  title   = {Building Rome in a Day},
  journal = cacm,
  volume  = {54},
  number  = {10},
  year    = {2011},
  pages   = {105--112},
  url     = {https://grail.cs.washington.edu/rome/rome_paper.pdf}
}

@inproceedings{brookshire2011rss,
  author    = {J. Brookshire and S. Teller},
  title     = {Automatic calibration of multiple coplanar sensors},
  booktitle = rss,
  year      = 2011
}

@inproceedings{kummerle2011iros,
  author    = {R. K{\"u}mmerle and G. Grisetti and W. Burgard},
  title     = {Simultaneous calibration, localization, and mapping},
  booktitle = iros,
  year      = 2011
}

@inproceedings{lebraly2011icra,
  author    = {P. L{\'e}braly and E. Royer and O. Ait-Aider and C. Deymier and M. Dhome},
  title     = {Fast calibration of embedded non-overlapping cameras},
  booktitle = icra,
  year      = 2011,
  abstract  = {This article deals with a simple and flexible extrinsic calibration method, for non-overlapping camera rig. The cameras do not see the same area at the same time. They are rigidly linked and can be moved. The most representative application is the mobile robotics domain. The calibration procedure consists in maneuvering the system while each camera observes a static scene. A linear solution derived from hand eye calibration scheme is proposed to compute an initial estimate of the extrinsic parameters. The main contribution is a specific bundle adjustment which refines both the scene geometry and the extrinsic parameters. Finally, an efficient implementation of the specific bundle adjustment step is defined for online calibration purpose. The proposed approach is validated with both synthetic and real data.}
}

@inproceedings{olson2011icra-aara,
  author    = {E. Olson},
  title     = {{AprilTag: A Robust and Flexible Visual Fiducial System}},
  booktitle = icra,
  year      = 2011,
  url       = {proceedings:olson2011icra-aara.pdf},
  abstract  = {While the use of naturally-occurring features is a central focus of machine perception, artificial features (fiducials) play an important role in creating controllable experiments, ground truthing, and in simplifying the development of systems where perception is not the central objective. We describe a new visual fiducial system that uses a 2D bar code style tag, allowing full 6 DOF localization of features from a single image. Our system improves upon previous systems, incorporating a fast and robust line detection system, a stronger digital coding system, and greater robustness to occlusion, warping, and lens distortion. While similar in concept to the ARTag system, our method is fully open and the algorithms are documented in detail.}
}

@inproceedings{tong2011icra,
  author    = {C.H. Tong and T.D. Barfoot},
  title     = {A self-calibrating 3D ground-truth localization system using retroreflective landmarks},
  booktitle = icra,
  year      = 2011
}

@inproceedings{kriegel2011icra,
  title     = {{A Surface-Based Next-Best-View Approach for Automated 3D Model Completion of Unknown Objects}},
  author    = {Kriegel, S. and Bodenm{\"u}ller, T. and Suppa, M. and Hirzinger, G.},
  booktitle = icra,
  year      = {2011},
  url       = {http://elib.dlr.de/73427/1/kriegel11_icra_final_110207.pdf}
}

@inproceedings{haner2011scia,
  title     = {{Optimal View Path Planning for Visual SLAM}},
  author    = {Haner, S. and Heyden, A.},
  booktitle = scia,
  year      = {2011},
  url       = {https://pdfs.semanticscholar.org/3d27/0c833e89e23304e8bccdc2d2b6e4458c4174.pdf}
}

@inproceedings{shade2011icra,
  title     = {{Choosing Where to Go: Complete 3D Exploration with Stereo}},
  author    = {Shade, R. and Newman, P.},
  booktitle = icra,
  year      = {2011},
  url       = {http://rjshade.com/work/files/papers/pdf/shade_newman_icra2011_choosing.pdf}
}

@inproceedings{krainin2011icra,
  title     = {{Autonomous Generation of Complete 3D Object Models Using Next Best View Manipulation Planning}},
  author    = {Krainin, M. and Curless, B. and Fox, D.},
  booktitle = icra,
  year      = {2011},
  url       = {https://rse-lab.cs.washington.edu/postscripts/nbv-icra-11-final.pdf}
}

@inproceedings{glover2010icra,
  author    = {A.J. Glover and W.P. Maddern and M. Milford and G.F. Wyeth},
  booktitle = icra,
  title     = {{FAB-MAP} + {RatSLAM}: Appearance-based SLAM for multiple times of day.},
  year      = 2010,
  keywords  = {SLAM, Localization, Visual Navigation, Place Recognition}
}

@article{furgale2010jfr,
  author   = {P.T. Furgale and T.D. Barfoot},
  journal  = jfr,
  pages    = {534--560},
  title    = {{Visual teach and repeat for long-range rover autonomy}},
  keywords = {Localization, Visual Navigation, Place Recognition},
  volume   = {27},
  year     = {2010}
}

@article{valgren2010jras,
  author   = {C. Valgren and A.J. Lilienthal },
  title    = {{SIFT, SURF \& Seasons: Appearance-Based Long-Term Localization in Outdoor Environments}},
  journal  = jras,
  number   = 2,
  year     = 2010,
  volume   = {85},
  pages    = {149--156},
  keywords = {Localization, Image Features}
}

@inproceedings{burgard2010irosws,
  author    = {W. Burgard and K.M. Wurm and M. Bennewitz and C. Stachniss and A. Hornung and R.B. Rusu and K. Konolige},
  title     = {{Modeling the World Around Us: An Efficient 3D Representation for Personal Robotics}},
  booktitle = {Workshop on Defining and Solving Realistic Perception Problems in Personal Robotics at the IEEE/RSJ Int.~Conf.~on Intelligent Robots and Systems},
  year      = 2010
}

@inproceedings{hornung2010erlars,
  author    = {A. Hornung and M.Bennewitz and C. Stachniss and H. Strasdat and S. O\ss{}wald and W. Burgard},
  title     = {{Learning Adaptive Navigation Strategies for Resource-Constrained Systems}},
  booktitle = {Proc.~of the Int.~Workshop on Evolutionary and Reinforcement Learning for Autonomous Robot Systems},
  year      = 2010
}

@article{grisetti2010titsmag,
  author  = {G. Grisetti and R. K{\"u}mmerle and C. Stachniss and W. Burgard},
  title   = {A Tutorial on Graph-based {SLAM}},
  journal = titsmag,
  year    = 2010,
  pages   = {31--43},
  volume  = 2,
  number  = 4
}

@inproceedings{sturm2010rssws,
  author    = {J. Sturm and K. Konolige and C. Stachniss and W. Burgard},
  title     = {{3D Pose Estimation, Tracking and Model Learning of Articulated Objects from Dense Depth Video using Projected Texture Stereo}},
  booktitle = {Proc.~of the Workshop RGB-D: Advanced Reasoning with Depth Cameras at Robotics: Science and Systems (RSS)},
  year      = 2010
}

@inproceedings{frank2010rssws,
  author    = {B. Frank and R. Schmedding and C. Stachniss and M. Teschner and W. Burgard},
  title     = {{Learning Deformable Object Models for Mobile Robot Path Planning using Depth Cameras and a Manipulation Robot}},
  booktitle = {Proc.~of the Workshop RGB-D: Advanced Reasoning with Depth Cameras at Robotics: Science and Systems (RSS)},
  year      = 2010
}

@inproceedings{wurm2010iros,
  title     = {{Coordinated Exploration with Marsupial Teams of Robots using Temporal Symbolic Planning}},
  author    = {K.M. Wurm and C. Dornhege and P. Eyerich and C. Stachniss and B. Nebel and W. Burgard},
  booktitle = iros,
  year      = 2010
}

@inproceedings{sturm2010iros,
  title     = {{Robustly Operating Articulated Objects based on Experience }},
  author    = {J. Sturm and A. Jain and C. Stachniss and C.C. Kemp and W. Burgard},
  booktitle = iros,
  year      = 2010
}

@inproceedings{frank2010iros,
  title     = {{Learning the Elasticity Parameters of Deformable Objects with a Manipulation Robot}},
  author    = {B. Frank and R. Schmedding and C. Stachniss and M. Teschner and W. Burgard},
  booktitle = iros,
  year      = 2010
}

@inproceedings{grisetti2010icra,
  author    = {G. Grisetti and R. K{\"u}mmerle and C. Stachniss and U. Frese and C. Hertzberg},
  title     = {{Hierarchical Optimization on Manifolds for Online 2D and 3D Mapping}},
  booktitle = icra,
  year      = 2010,
  url       = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/grisetti10icra.pdf}
}

@inproceedings{karg2010icra,
  author    = {M. Karg and K.M. Wurm and C. Stachniss and K. Dietmayer and W. Burgard},
  title     = {Consistent Mapping of Multistory Buildings by Introducing Global Constraints to Graph-based {SLAM}},
  booktitle = icra,
  year      = 2010
}

@inproceedings{sturm2010icra,
  author    = {J. Sturm and K. Konolige and C. Stachniss and W. Burgard},
  title     = {{Vision-based Detection for Learning Articulation Models of Cabinet Doors and Drawers in Household Environments}},
  booktitle = icra,
  year      = 2010
}

@inbook{mueller2010cogsysbook,
  author    = {J. M\"{u}ller and C. Stachniss and K.O. Arras and W. Burgard},
  chapter   = {Socially Inspired Motion Planning for Mobile Robots in Populated Environments},
  title     = {{Cognitive Systems}},
  series    = {Cognitive Systems Monographs},
  publisher = springer,
  year      = 2010
}

@article{plagemann2010ras,
  title   = {{A Nonparametric Learning Approach to Range Sensing from Omnidirectional Vision}},
  author  = {C. Plagemann and C. Stachniss and J. Hess and F. Endres and N. Franklin},
  journal = jras,
  volume  = 58,
  number  = 6,
  pages   = {762--772},
  year    = 2010
}

@article{kretzschmar2010ki,
  title   = {Lifelong Map Learning for Graph-based {SLAM} in Static Environments},
  author  = {H. Kretzschmar and G. Grisetti and C. Stachniss},
  journal = {{KI} -- {K}\"unstliche {I}ntelligenz (German {AI} Magazine)},
  volume  = 24,
  number  = 3,
  pages   = {199--206},
  year    = 2010
}

@article{wurm2010ras,
  title   = {{Bridging the Gap Between Feature- and Grid-based SLAM}},
  author  = {K.M. Wurm and C. Stachniss and G. Grisetti},
  journal = jras,
  volume  = {58},
  number  = {2},
  pages   = {140 - 148},
  year    = {2010},
  url     = {http://ais.informatik.uni-freiburg.de/publications/papers/wurm10ras.pdf}
}

@book{szeliski2010cvbook,
  author    = {R. Szeliski},
  title     = {{Computer Vision: Algorithms and Applications}},
  publisher = springer,
  keywords  = {Book, Computer Vision},
  year      = 2010,
  abstract  = {This book also reflects my 20 yearsâ experience doing computer vision research in corpo- rate research labs, mostly at Digital Equipment Corporationâs Cambridge Research Lab and at Microsoft Research. In pursuing my work, I have mostly focused on problems and solution techniques (algorithms) that have practical real-world applications and that work well in practice. Thus, this book has more emphasis on basic techniques that work under real-world conditions and less on more esoteric mathematics that has intrinsic elegance but less practical applicability. This book is suitable for teaching a senior-level undergraduate course in computer vision to students in both computer science and electrical engineering. I prefer students to have either an image processing or a computer graphics course as a prerequisite so that they can spend less time learning general background mathematics and more time studying computer vision techniques. The book is also suitable for teaching graduate-level courses in computer vision (by delving into the more demanding application and algorithmic areas) and as a general reference to fundamental techniques and the recent research literature. To this end, I have attempted wherever possible to at least cite the newest research in each sub-field, even if technical details are too complex to cover in the book itself.}
}

@book{alpaydin2010mlbook,
  author    = {E. Alpaydin},
  title     = {{Introdiction to Machine Learning}},
  publisher = mitpress,
  keywords  = {Book, Machine Learning},
  year      = 2010,
  abstract  = {Machine learning is programming computers to optimize a performance criterion using example data or past experience. We need learning in cases where we cannot directly write a computer program to solve a given problem, but need example data or experience. One case where learning is necessary is when human expertise does not exist, or when humans are unable to explain their expertise. Consider the recognition of spoken speechâthat is, converting the acoustic speech signal to an ASCII text; we can do this task seemingly without any difficulty, but we are unable to explain how we do it. Different people utter the same word differently due to differences in age, gender, or accent. In machine learning, the approach is to collect a large collection of sample utterances from different people and learn to map these to words. The book discusses many learning methods that have their bases in different fields: statistics, pattern recognition, neural networks, artificial intelligence, signal processing, control, and data mining. In the past, research in these different communities followed different paths with different emphases. In this book, the aim is to incorporate them together to give a unified treatment of the problems and the proposed solutions to them. This is an introductory textbook, intended for senior undergraduate and graduate-level courses on machine learning, as well as engineers working in the industry who are interested in the application of these methods. The prerequisites are courses on computer programming, probability, calculus, and linear algebra. The aim is to have all learning algorithms sufficiently explained so it will be a small step from the equations given in the book to a computer program. For some cases, pseudocode of algorithms are also included to make this task easier.}
}

@inproceedings{levinson2010icra,
  author    = {J. Levinson and S. Thrun},
  title     = {{Robust Vehicle Localization in Urban Environments Using Probabilistic Maps}},
  booktitle = icra,
  year      = {2010},
  abstract  = {Autonomous vehicle navigation in dynamic urban environments requires localization accuracy exceeding that available from GPS-based inertial guidance systems. We have shown previously that GPS, IMU, and LIDAR data can be used to generate a high-resolution infrared remittance ground map that can be subsequently used for localization [4]. We now propose an extension to this approach that yields substantial improvements over previous work in vehicle localization, including higher precision, the ability to learn and improve maps over time, and increased robustness to environment changes and dynamic obstacles. Specifically, we model the environment, instead of as a spatial grid of fixed infrared remittance values, as a probabilistic grid whereby every cell is represented as its own gaussian distribution over remittance values. Subsequently, Bayesian inference is able to preferentially weight parts of the map most likely to be stationary and of consistent angular reflectivity, thereby reducing uncertainty and catastrophic errors. Furthermore, by using offline SLAM to align multiple passes of the same environment, possibly separated in time by days or even months, it is possible to build an increasingly robust understanding of the world that can be then exploited for localization. We validate the effectiveness of our approach by using these algorithms to localize our vehicle against probabilistic maps in various dynamic environments, achieving RMS accuracy in the 10cm-range and thus outperforming previous work. Importantly, this approach has enabled us to autonomously drive our vehicle for hundreds of miles in dense traffic on narrow urban roads which were formerly unnavigable with previous localization methods.},
  keywords  = {Localization, Mapping, Autonomous Navigation},
  url       = {proceedings:levinson2010icra.pdf}
}

@inproceedings{steder2010icra,
  author    = {B. Steder and G. Grisetti and W. Burgard},
  title     = {{Robust Place Recognition for 3D Range Data Based on Point Features}},
  booktitle = icra,
  year      = 2010,
  keywords  = {SLAM, Recognition, Range Sensing, Localization},
  abstract  = {The problem of place recognition appears in different mobile robot navigation problems including localization, SLAM, or change detection in dynamic environments. Whereas this problem has been studied intensively in the context of robot vision, relatively few approaches are available for threedimensional range data. In this paper, we present a novel and robust method for place recognition based on range images. Our algorithm matches a given 3D scan against a database using point features and scores potential transformations by comparing significant points in the scans. A further advantage of our approach is that the features allow for a computation of the relative transformations between scans which is relevant for registration processes. Our approach has been implemented and tested on different 3D data sets obtained outdoors. In several experiments we demonstrate the advantages of our approach also in comparison to existing techniques.},
  url       = {proceedings:steder2010icra.pdf}
}

@article{everingham2010ijcv,
  title   = {{The Pascal Visual Object Classes (VOC) Challenge}},
  author  = {Everingham, M. and Van Gool, L. and Williams, C.K. and Winn, J. and Zisserman, A.},
  journal = ijcv,
  volume  = {88},
  number  = {2},
  pages   = {303--338},
  year    = {2010},
  url     = {https://pdfs.semanticscholar.org/0ee1/916a0cb2dc7d3add086b5f1092c3d4beb38a.pdf}
}

@inproceedings{hentschel2010itsc,
  title     = {Autonomous robot navigation based on openstreetmap geodata},
  author    = {M. Hentschel and B. Wagner},
  booktitle = itsc,
  year      = {2010},
  url       = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.651.3479&rep=rep1&type=pdf}
}

@article{furukawa2010pami,
  author   = {Y. Furukawa and J. Ponce},
  title    = {Accurate, Dense, and Robust Multiview Stereopsis},
  journal  = pami,
  volume   = {32},
  number   = {8},
  year     = {2010},
  pages    = {1362--1376},
  url      = {https://www.cse.wustl.edu/~furukawa/papers/pami08a.pdf},
  keywords = {3D/stereo scene analysis, Computer vision, Computer vision, 3D/stereo scene analysis, modeling and recovery of physical attributes, motion, shape., modeling and recovery of physical attributes, motion, shape.}
}

@article{bryson2010jfr,
  author  = {M. Bryson and A. Reid and F.T. Ramos and S. Sukkarieh},
  title   = {Airborne vision-based mapping and classification of large farmland environments},
  journal = jfr,
  volume  = {27},
  number  = {5},
  pages   = {632--655},
  year    = {2010},
  url     = {https://doi.org/10.1002/rob.20343}
}

@inproceedings{trummer2010icpr,
  title     = {{Online Next-Best-View Planning for Accuracy Optimization Using an Extended E-Criterion}},
  author    = {Trummer, M. and Munkelt, C. and Denzler, J.},
  booktitle = icpr,
  year      = {2010},
  url       = {http://olymp.inf-cv.uni-jena.de:6680/pdf/Trummer10:ONP.pdf}
}

@inproceedings{newman2009rss,
  author    = {M. Cummins AND P. Newman},
  title     = {Highly scalable appearance-only {SLAM} - {FAB-MAP} 2.0},
  booktitle = rss,
  year      = 2009,
  keywords  = {SLAM, Localization, Visual Navigation, Place Recognition}
}

@phdthesis{stachniss2009habil,
  author = {C. Stachniss},
  title  = {{Spatial Modeling and Robot Navigation}},
  year   = 2009,
  school = {University of Freiburg, Department of Computer Science},
  url    = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/stachniss-habil.pdf}
}

@book{stachniss2009springerbook,
  author    = {C. Stachniss},
  title     = {{Robotic Mapping and Exploration}},
  publisher = springer,
  year      = 2009
}

@inproceedings{sturm2009ijcai,
  author    = {J. Sturm and V. Predeap and C. Stachniss and C. Plagemann and K. Konolige and W. Burgard},
  title     = {{Learning Kinematic Models for Articulated Objects}},
  booktitle = ijcai,
  year      = 2009
}

@inproceedings{sturm2009snowbird,
  author    = {J. Sturm and C. Stachniss and V. Predeap and C. Plagemann and K. Konolige and W. Burgard},
  title     = {{Learning Kinematic Models for Articulated Objects}},
  booktitle = {Online Proc. of the Learning Workshop (Snowbird)},
  year      = 2009
}

@inproceedings{endres2009rss,
  author    = {F. Endres and C. Plagemann and C. Stachniss and W. Burgard},
  title     = {{Scene Analysis using Latent Dirichlet Allocation}},
  booktitle = rss,
  year      = 2009
}

@inproceedings{endres2009rssws,
  author    = {F. Endres and J. Hess and N. Franklin and C. Plagemann and C. Stachniss and W. Burgard},
  title     = {{Estimating Range Information from Monocular Vision}},
  booktitle = {Workshop Regression in Robotics - Approaches and Applications at Robotics: Science and Systems (RSS)},
  year      = 2009
}

@inproceedings{sturm2009rssws,
  author    = {J. Sturm and C. Stachniss and V. Predeap and C. Plagemann and K. Konolige and W. Burgard},
  title     = {{Towards Understanding Articulated Objects}},
  booktitle = {Workshop Integrating Mobility and Manipulation at Robotics: Science and Systems (RSS)},
  year      = 2009
}

@article{kuemmerle2009ar,
  title   = {On measuring the accuracy of {SLAM} algorithms},
  author  = {R. K{\"u}mmerle and B. Steder and C. Dornhege and M. Ruhnke and G. Grisetti and C. Stachniss and A. Kleiner},
  journal = ar,
  year    = 2009,
  volume  = 27,
  number  = 4,
  pages   = {387ff}
}

@inproceedings{burgard2009iros,
  title     = {Trajectory-based Comparison of {SLAM} Algorithms},
  author    = {W. Burgard and C. Stachniss and G. Grisetti and B. Steder and R. K\"ummerle and C. Dornhege and M. Ruhnke and A. Kleiner and J.D. Tard\'os},
  booktitle = iros,
  year      = 2009
}

@inproceedings{wurm2009iros,
  author    = {K.M. Wurm and R. K{\"u}mmerle and Stachniss, C. and Burgard, W.},
  title     = {{Improving Robot Navigation in Structured Outdoor Environments by Identifying Vegetation from Laser Data}},
  booktitle = iros,
  year      = 2009
}

@inproceedings{schneider2009iros,
  author    = {A. Schneider and J. Sturm C. Stachniss and M. Reisert and H. Burkhardt and W. Burgard},
  title     = {{Object Identification with Tactile Sensors Using Bag-of-Features}},
  booktitle = iros,
  year      = 2009
}

@article{stachniss2009ar,
  title   = {{Gas Distribution Modeling using Sparse Gaussian Process Mixtures}},
  author  = {C. Stachniss and C. Plagemann and A.J. Lilienthal},
  journal = ar,
  year    = 2009,
  volume  = 26,
  number  = 2,
  pages   = {187ff}
}

@inproceedings{bennewitz2009icra,
  author    = {M. Bennewitz and C. Stachniss and S. Behnke and W. Burgard},
  title     = {{Utilizing Reflection Properties of Surfaces to Improve Mobile Robot Localization}},
  booktitle = icra,
  year      = 2009
}

@inproceedings{strasdat2009icra,
  author    = {H. Strasdat and C. Stachniss and W. Burgard},
  title     = {{Which Landmark is Useful? Learning Selection Policies for Navigation in Unknown Environments}},
  booktitle = icra,
  year      = 2009
}

@inproceedings{frank2009icra,
  author    = {B. Frank and C. Stachniss and R. Schmedding and W. Burgard and M. Teschner},
  title     = {{Real-world Robot Navigation amongst Deformable Obstacles}},
  booktitle = icra,
  year      = 2009
}

@inproceedings{eppner2009icra,
  author    = {C. Eppner and J. Sturm and M. Bennewitz and C. Stachniss and W. Burgard},
  title     = {{Imitation Learning with Generalized Task Descriptions}},
  booktitle = icra,
  year      = 2009
}

@article{grisetti2009tits,
  author  = {G. Grisetti and C. Stachniss and W. Burgard},
  title   = {{Non-linear Constraint Network Optimization for Efficient Map Learning}},
  journal = tits,
  year    = 2009,
  volume  = 10,
  number  = 3,
  pages   = {428--439},
  url     = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/grisetti09its.pdf}
}

@article{stachniss2009amai,
  title   = {{Efficient Exploration of Unknown Indoor Environments using a Team of Mobile Robots}},
  author  = {C. Stachniss and O. Martinez Mozos and W. Burgard},
  journal = {Annals of Mathematics and Artificial Intelligence},
  year    = 2009,
  volume  = 52,
  number  = 2,
  pages   = {205ff},
  url     = {http://ais.informatik.uni-freiburg.de/publications/papers/stachniss09amai.pdf}
}

@book{heineman2009algorithmsbook,
  author    = {G.T. Heineman and G. Pollice and S. Selkow},
  title     = {{Algorithms in a Nutshell}},
  publisher = {O{\' }Reiley},
  year      = 2009,
  keywords  = {Book, Programming, Algorithms},
  abstract  = {As authors of this book, we answer the question that has led you here: Can I use algorithm X to solve my problem? If so, how do I implement it?}
}

@inproceedings{magnusson2009icra-aldf,
  author    = {M. Magnusson and H. Andreasson and A. Nuechter and Achim and J. Lilienthal},
  title     = {{Appearance-Based Loop Detection from 3D Laser Data Using the Normal Distributions Transform}},
  booktitle = icra,
  year      = 2009,
  keywords  = {SLAM, Recognition, Mining Robotics},
  abstract  = {We propose a new approach to appearance based loop detection from metric 3D maps, exploiting the NDT surface representation. Locations are described with feature histograms based on surface orientation and smoothness, and loop closure can be detected by matching feature histograms. We also present a quantitative performance evaluation using two realworld data sets, showing that the proposed method works well in different environments.},
  url       = {proceedings:magnusson2009icra-aldf.pdf}
}

@inproceedings{lidoris2009icra,
  title     = {The Autonomous City Explorer (ACE) project -- mobile robot navigation in highly populated urban environments},
  author    = {G. Lidoris and F. Rohrmuller and D. Wollherr and M. Buss},
  booktitle = icra,
  year      = {2009},
  url       = {https://pdfs.semanticscholar.org/910a/89585b09425cb0de108c82a8c541e59127ad.pdf}
}

@inproceedings{hornung2009iros,
  title     = {Learning efficient policies for vision-based navigation},
  author    = {A. Hornung and H. Strasdat and M. Bennewitz and W. Burgard},
  booktitle = iros,
  year      = {2009},
  url       = {https://pdfs.semanticscholar.org/2fa0/03efb608aae01f5fe9605463fb0e67b10ef9.pdf}
}

@article{prentice2009ijrr,
  title   = {The belief roadmap: Efficient planning in belief space by factoring the covariance},
  author  = {S. Prentice and N. Roy},
  journal = ijrr,
  volume  = {28},
  number  = {11-12},
  pages   = {1448--1465},
  year    = {2009},
  url     = {http://people.csail.mit.edu/prentice/papers/ijrr09-brm.pdf}
}

@inproceedings{weiss2009nips,
  title     = {Spectral hashing},
  author    = {Y. Weiss and A. Torralba and R. Fergus},
  booktitle = nips,
  year      = {2009}
}

@inproceedings{dunn2009bmvc,
  title     = {{Next Best View Planning for Active Model Improvement}},
  author    = {Dunn, E. and Frahm, J.},
  booktitle = bmvc,
  year      = {2009},
  url       = {http://ai2-s2-pdfs.s3.amazonaws.com/14f2/2f38bb939393b61fefd3fbc8f713c81b1ac7.pdf}
}

@inproceedings{weiss2008nips,
  title     = {{Spectral Hashing}},
  author    = {Y. Weiss and A. Torralba and R. Fergus},
  booktitle = nips,
  keywords  = {Hashing, Spectral Clustering},
  url       = {https://people.csail.mit.edu/torralba/publications/spectralhashing.pdf},
  year      = 2008
}

@article{agrawal2008tro,
  title    = {{FrameSLAM: From Bundle Adjustment to Real-Time Visual Mapping}},
  journal  = tro,
  volume   = {24},
  number   = {5},
  pages    = {1066--1077},
  year     = 2008,
  author   = {M. Agrawal and K. Konolige},
  keywords = {SLAM, Visual SLAM}
}

@article{bay2008cviu,
  author   = {H. Bay and A. Ess and T. Tuytelaars and L. Van Gool},
  title    = {Speeded-Up Robust Features ({SURF})},
  journal  = cviu,
  volume   = {110},
  number   = {3},
  year     = 2008,
  pages    = {346--359},
  keywords = {Image Features}
}

@inproceedings{kretzschmar2008iros,
  author    = {H. Kretzschmar and C. Stachniss and C. Plagemann and W. Burgard},
  title     = {{Estimating Landmark Locations from Geo-Referenced Photographs}},
  booktitle = iros,
  year      = 2008
}

@inproceedings{pfaff2008iros,
  author    = {P. Pfaff and C. Stachniss and C. Plagemann and W. Burgard},
  title     = {{Efficiently Learning High-dimensional Observation Models for Monte-Carlo Localization using Gaussian Mixtures}},
  booktitle = iros,
  year      = 2008,
  url       = {http://www2.informatik.uni-freiburg.de/~stachnis/pdf/pfaff08iros.pdf},
  abstract  = {Whereas probabilistic approaches are a powerful tool for mobile robot localization, they heavily rely on the proper definition of the so-called observation model which defines the likelihood of an observation given the position and orientation of the robot and the map of the environment. Most of the sensor models for range sensors proposed in the past either consider the individual beam measurements independently or apply uni-modal models to represent the likelihood function. In this paper we present an approach that learns place-dependent sensor models for entire range scans using Gaussian mixture models. To deal with the high dimensionality of the measurement space, we utilize principle component analysis for dimensionality reduction. In practical experiments carried out with data obtained from a real robot we demonstrate that our model substantially outperforms existing and popular sensor models.}
}

@inproceedings{wurm2008iros,
  author    = {K.M. Wurm and C. Stachniss and W. Burgard},
  title     = {{Coordinated Multi-Robot Exploration using a Segmentation of the Environment}},
  booktitle = iros,
  year      = 2008,
  url       = {http://ais.informatik.uni-freiburg.de/publications/papers/wurm08iros.pdf}
}

@inproceedings{stachniss2008rss,
  author    = {C. Stachniss and C. Plagemann and A.J. Lilienthal and W. Burgard},
  title     = {{Gas Distribution Modeling using Sparse Gaussian Process Mixture Models}},
  booktitle = rss,
  year      = 2008,
  url       = {http://ais.informatik.uni-freiburg.de/publications/papers/stachniss08rss.pdf}
}

@inproceedings{mueller2008cogsys,
  author    = {M\"uller, J. and Stachniss, C. and Arras, K.O. and Burgard, W.},
  title     = {{Socially Inspired Motion Planning for Mobile Robots in Populated Environments}},
  booktitle = cogsys,
  year      = 2008,
  url       = {http://ais.informatik.uni-freiburg.de/publications/papers/mueller08cogsys.pdf}
}

@article{steder2008tro,
  author  = {B. Steder and G. Grisetti and C. Stachniss and W. Burgard},
  title   = {Visual {SLAM} for Flying Vehicles},
  journal = tro,
  year    = 2008,
  volume  = {24},
  number  = {8},
  pages   = {1088--1093},
  url     = {http://ais.informatik.uni-freiburg.de/publications/papers/steder08tro.pdf}
}

@inproceedings{stachniss2008icra,
  author    = {C. Stachniss and M. Bennewitz and G. Grisetti and S. Behnke and W. Burgard},
  title     = {{How to Learn Accurate Grid Maps with a Humanoid}},
  booktitle = icra,
  year      = 2008,
  url       = {http://ais.informatik.uni-freiburg.de/publications/papers/stachniss08icra.pdf}
}

@inproceedings{plagemann2008icra,
  author    = {C. Plagemann and F. Endres and J. Hess and C. Stachniss and W. Burgard},
  title     = {{Monocular Range Sensing: A Non-Parametric Learning Approach}},
  booktitle = icra,
  year      = 2008,
  url       = {http://www2.informatik.uni-freiburg.de/~stachnis/pdf/plagemann08icra.pdf},
  abstract  = {For many applications, mobile robots need to estimate the geometry of their local surrounding area. To do so, proximity sensor such as laser range finders or sonars are typically employed. Cameras are a cheap and lightweight alternative to such sensors, but do not offer proximity information directly. In this paper, we present a novel approach to learning the relationship between range measurements and visual features extracted from a single monocular camera image. As the learning engine, we apply Gaussian processes, a non- parametric learning technique that not only yields the most likely range prediction corresponding to a certain visual input but also the predictive uncertainty. This information, in turn, can be utilized in an extended grid-based mapping scheme to update a model of the environment more gently where the predictions are unreliable. In practical experiments carried out with a mobile robot equipped with an omnidirectional camera system in different environments, we show that our system is able to predict range scans accurate enough to construct maps of the environment.}
}

@inproceedings{grisetti2008icra,
  author    = {G. Grisetti and D. Lordi Rizzini and C. Stachniss and E. Olson and W. Burgard},
  title     = {{Online Constraint Network Optimization for Efficient Maximum Likelihood Map Learning}},
  booktitle = icra,
  year      = 2008,
  url       = {http://ais.informatik.uni-freiburg.de/publications/papers/grisetti08icra.pdf}
}

@inproceedings{frank2008icra,
  author    = {B. Frank and M. Becker and C. Stachniss and M. Teschner and W. Burgard},
  title     = {{Efficient Path Planning for Mobile Robots in Environments with Deformable Objects}},
  booktitle = icra,
  year      = 2008,
  url       = {http://ais.informatik.uni-freiburg.de/publications/papers/frank08icra.pdf}
}

@inproceedings{frank2008icraws,
  author    = {B. Frank and M. Becker and C. Stachniss and M. Teschner and W. Burgard},
  title     = {{Learning Cost Functions for Mobile Robot Navigation in Environments with Deformable Objects}},
  booktitle = {Workshop on Path Planning on Cost Maps at the IEEE Int.~Conf.~on Robotics \& Automation},
  year      = 2008,
  url       = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/frank08icraws.pdf}
}

@inproceedings{steder2008visappws,
  author    = {B. Steder and G. Grisetti and S. Grzonka and C. Stachniss and W. Burgard},
  title     = {{Estimating Consistent Elevation Maps using Down-Looking Cameras and Inertial Sensors}},
  booktitle = {Workshop on Robotic Perception, International Conference on Computer Vision Theory and Applications},
  year      = 2008,
  url       = {http://ais.informatik.uni-freiburg.de/publications/papers/steder08visapp.pdf}
}

@inproceedings{eden2008eccv,
  title     = {{Using 3D Line Segments for Robust and Efficient Change Detection from Multiple Noisy Images}},
  author    = {Eden, I. and Cooper, D.B.},
  booktitle = eccv,
  year      = {2008},
  url       = {https://pdfs.semanticscholar.org/f81b/760af0f878a6beb9b6af10de8e03679bc20b.pdf}
}

@article{meyer2008cea,
  title   = {Verification of color vegetation indices for automated crop imaging applications},
  author  = {G.E. Meyer and J. Camargo-Neto},
  journal = cea,
  volume  = {63},
  number  = {2},
  pages   = {282--293},
  year    = {2008},
  url     = {http://www.sciencedirect.com/science/article/pii/S0168169908001063}
}

@article{davison2007pami,
  author   = {A.J. Davison and I.D. Reid and N.D. Molton and O. Stasse},
  title    = {{MonoSLAM: Real-time single camera SLAM}},
  journal  = pami,
  year     = 2007,
  volume   = {29},
  number   = {6},
  pages    = {1052--1067},
  keywords = {SLAM, Visual SLAM}
}

@inbook{burgard2007starbook,
  author    = {W. Burgard and C. Stachniss and D. Haehnel},
  editor    = {Laugier, C. and Chatila, R.},
  title     = {{Autonomous Navigation in Dynamic Environments}},
  chapter   = {Mobile Robot Map Learning from Range Data in Dynamic Environments},
  publisher = springer,
  year      = 2007,
  url       = {http://ais.informatik.uni-freiburg.de/publications/papers/burgard07starbook.pdf}
}

@inproceedings{wurm2007ecmr,
  author    = {K.M. Wurm and C. Stachniss and G. Grisetti and W. Burgard},
  title     = {{Improved Simultaneous Localization and Mapping using a Dual Representation of the Environment}},
  booktitle = ecmr,
  year      = 2007,
  url       = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/wurm07ecmr.pdf}
}

@inproceedings{joho2007ams,
  author    = {D. Joho and C. Stachniss and P. Pfaff and W. Burgard},
  title     = {{Autonomous Exploration for 3D Map Learning}},
  booktitle = ams,
  year      = 2007,
  url       = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/joho07ams.pdf}
}

@inproceedings{joho2009icra,
  title     = {{Modeling RFID signal strength and tag detection for localization and mapping}},
  author    = {Joho, Dominik and Plagemann, Christian and Burgard, Wolfram},
  booktitle = icra,
  year      = {2009},
  url       = {proceedings: joho2009icra.pdf}
}

@inproceedings{strasdat2007ams,
  author    = {H. Strasdat and C. Stachniss and M. Bennewitz and W. Burgard},
  title     = {{Visual Bearing-Only Simultaneous Localization and Mapping with Improved Feature Matching}},
  booktitle = ams,
  year      = 2007,
  url       = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/strasdat07ams.pdf}
}

@article{stachniss2007it,
  title   = {{Efficiently Learning Metric and Topological Maps with Autonomous Service Robots}},
  author  = {Stachniss, C. and Grisetti, G. and Mart\'{i}nez-Mozos, O. and Burgard, W.},
  journal = {it -- Information Technology},
  year    = 2007,
  volume  = {49},
  number  = {4},
  pages   = {232--238},
  url     = {http://ais.informatik.uni-freiburg.de/publications/papers/stachniss2007it.pdf}
}

@inproceedings{pfaff2007irosws,
  author    = {P. Pfaff and R. K{\"u}mmerle and D. Joho and C. Stachniss and R. Triebel and W. Burgard},
  title     = {{Navigation in Combined Outdoor and Indoor Environments using M ulti-Level Surface Maps}},
  booktitle = {Workshop on Safe Navigation in Open and Dynamic Environments a t the IEEE/RSJ Int.~Conf.~on Intelligent Robots and Systems (IROS)},
  year      = 2007,
  url       = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/pfaff07irosws.pdf}
}

@inproceedings{stachniss2007iros,
  author    = {C. Stachniss and G. Grisetti and N. Roy and W. Burgard},
  title     = {{Evaluation of Gaussian Proposal Distributions for Mapping with Rao-Blackwellized Particle Filters}},
  booktitle = iros,
  year      = 2007,
  url       = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/stachniss07iros.pdf}
}

@inproceedings{steder2007iros,
  author    = {B. Steder and G. Grisetti and S. Grzonka and C. Stachniss and A. Rottmann and W. Burgard},
  title     = {{Learning Maps in 3D using Attitude and Noisy Vision Sensors}},
  booktitle = iros,
  year      = 2007,
  url       = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/steder07iros.pdf}
}

@inproceedings{steder2007irosws,
  author    = {B. Steder and A. Rottmann and G. Grisetti and C. Stachniss and W. Burgard},
  title     = {{Autonomous Navigation for Small Flying Vehicles}},
  booktitle = {Workshop on Micro Aerial Vehicles at the IEEE/RSJ Int.~Conf.~on Intelligent Robots and Systems},
  year      = 2007,
  url       = {http://ais.informatik.uni-freiburg.de/publications/papers/steder07irosws.pdf}
}

@inproceedings{grisetti2007iros,
  author    = {G. Grisetti and S. Grzonka and C. Stachniss and P. Pfaff and W. Burgard},
  title     = {{Efficient Estimation of Accurate Maximum Likelihood Maps in 3D}},
  booktitle = iros,
  year      = 2007,
  url       = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/grisetti07iros.pdf}
}

@inproceedings{grisetti2007rss,
  author    = {G. Grisetti and C. Stachniss and S. Grzonka and W. Burgard},
  title     = {{A Tree Parameterization for Efficiently Computing Maximum Likelihood Maps using Gradient Descent}},
  booktitle = rss,
  year      = 2007,
  url       = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/grisetti07rss.pdf}
}

@inproceedings{pfaff2007smart,
  author    = {P. Pfaff and R. Triebel and C. Stachniss and P. Lamon and W. Burgard and R. Siegwart},
  title     = {{Towards Mapping of Cities}},
  booktitle = icra,
  year      = 2007,
  url       = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/pfaff07icra.pdf}
}

@inbook{martinez2007starbook,
  author    = {O. Mart\'{i}nez-Mozos and C. Stachniss and A. Rottmann and W. Burgard},
  editor    = {Thrun, S. and Brooks, R. and Durrant-Whyte, H.},
  title     = {{Robotics Research}},
  chapter   = {Using AdaBoost for Place Labelling and Topological Map Building},
  publisher = springer,
  year      = 2007,
  volume    = {28},
  series    = springerstaradvanced,
  isbn      = {978-3-540-48110-2},
  url       = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/martinez07springer.pdf}
}

@article{grisetti2007tro,
  title   = {{Improved Techniques for Grid Mapping with Rao-Blackwellized Particle Filters}},
  author  = {G. Grisetti and C. Stachniss and W. Burgard},
  journal = tro,
  year    = 2007,
  pages   = {34--46},
  volume  = {23},
  number  = {1},
  url     = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/grisetti07tro.pdf}
}

@article{grisetti2007jras,
  title   = {{Fast and Accurate SLAM with Rao-Blackwellized Particle Filters}},
  author  = {G. Grisetti and G.D. Tipaldi and C. Stachniss and W. Burgard and D. Nardi},
  journal = jras,
  year    = {2007},
  pages   = {30--38},
  volume  = {55},
  number  = {1},
  url     = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/grisetti07jras.pdf}
}

@inproceedings{levinson2007rss,
  author    = {J. Levinson and M. Montemerlo and S. Thrun},
  title     = {{Map-Based Precision Vehicle Localization in Urban Environments}},
  booktitle = rss,
  year      = {2007},
  abstract  = {Many urban navigation applications (e.g., autonomous navigation, driver assistance systems) can benefit greatly from localization with centimeter accuracy. Yet such accuracy cannot be achieved reliably with GPS-based inertial guidance systems, specifically in urban settings. We propose a technique for high-accuracy localization of moving vehicles that utilizes maps of urban environments. Our approach integrates GPS, IMU, wheel odometry, and LIDAR data acquired by an instrumented vehicle, to generate high-resolution environment maps. Offline relaxation techniques similar to recent SLAM methods [2, 10, 13, 14, 21, 30] are employed to bring the map into alignment at intersections and other regions of self-overlap. By reducing the final map to the flat road surface, imprints of other vehicles are removed. The result is a 2-D surface image of ground reflectivity in the infrared spectrum with 5cm pixel resolution. To localize a moving vehicle relative to these maps, we present a particle filter method for correlating LIDAR measurements with this map. As we show by experimentation, the resulting relative accuracies exceed that of conventional GPS-IMU-odometry-based methods by more than an order of magnitude. Specifically, we show that our algorithm is effective in urban environments, achieving reliable real-time localization with accuracy in the 10-centimeter range. Experimental results are provided for localization in GPS-denied environments, during bad weather, and in dense traffic.},
  url       = {proceedings:levinson2007rss.pdf}
}

@inproceedings{pollard2007cvpr,
  title     = {{Change Detection in a 3D World}},
  author    = {Pollard, T. and Mundy, J.L.},
  booktitle = cvprold,
  year      = {2007},
  url       = {https://pdfs.semanticscholar.org/40b0/330e57838c110b9735625acfd78b03dcf292.pdf},
  abstract  = {This paper examines the problem of detecting changes in a 3-d scene from a sequence of images, taken by cameras with arbitrary but known pose. No prior knowledge of the state of normal appearance and geometry of object surfaces is assumed, and abnormal changes can occur in any image of the sequence. To the authorsâ knowledge, this paper is the first to address the change detection problem in such a general framework. Existing change detection algorithms that exploit multiple image viewpoints typically can detect only motion changes or assume a planar world geometry which cannot cope effectively with appearance changes due to occlusion and un-modeled 3-d scene geometry (egomotion parallax). The approach presented here can manage the complications of unknown and sometimes changing world surfaces by maintaining a 3-d voxel-based model, where probability distributions for surface occupancy and image appearance are stored in each voxel. The probability distributions at each voxel are continuously updated as new images are received. The key question of convergence of this joint estimation problem is answered by a formal proof based on realistic assumptions about the nature of real world scenes. A series of experiments are presented that evaluate change detection accuracy under laboratory controlled conditions as well as aerial reconnaissance scenarios.}
}

@inproceedings{lv2007vldb,
  title     = {Multi-probe LSH: efficient indexing for high-dimensional similarity search},
  author    = {Q. Lv and W. Josephson and Z. Wang and M. Charikar and K. Li},
  booktitle = vldb,
  year      = {2007}
}

@inproceedings{meier2006sensor,
  author    = {D. Meier and C. Stachniss and W. Burgard},
  title     = {{Cooperative Exploration With Multiple Robots Using Low Bandwidth Communication}},
  booktitle = {{Informationsfusion in der Mess- und Sensortechnik}},
  year      = 2006,
  url       = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/meier06sensor.pdf}
}

@inproceedings{lamon2006irosws,
  author    = {P. Lamon and C. Stachniss and R. Triebel and P. Pfaff and C. Plagemann and G. Grisetti and S. Kolski and W. Burgard and R. Siegwart},
  title     = {{Mapping with an Autonomous Car}},
  booktitle = {Workshop on Safe Navigation in Open and Dynamic Environments at the IEEE/RSJ Int.~Conf.~on Intelligent Robots and Systems},
  year      = 2006,
  url       = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/lamon06iros.pdf}
}

@inproceedings{kolski2006irosws,
  author    = {S. Kolski and D. Furgeson and C. Stachniss and R. Siegwart},
  title     = {{Autonomous Driving in Dynamic Environments}},
  booktitle = {Workshop on Safe Navigation in Open and Dynamic Environments at the IEEE/RSJ Int.~Conf.~on Intelligent Robots and Systems},
  year      = 2006
}

@inproceedings{gil2006iros,
  author    = {A. Gil and O. Reinoso and O. Mart\'{i}nez-Mozos and C. Stachniss and W. Burgard},
  title     = {{Improving Data Association in Vision-based SLAM}},
  booktitle = iros,
  year      = 2006,
  url       = {https://pdfs.semanticscholar.org/ea1d/829217e1059bb77aae5b5e2605c981a58063.pdf}
}

@article{sonntag2006austendodj,
  title   = {Determination of Root Canal Curvatures before and after Canal Preparation (Part {II}): A Method based on Numeric Calculus},
  author  = {D. Sonntag and S. Stachniss-Carp and C. Stachniss and V. Stachniss},
  journal = {Aust Endod J},
  volume  = {32},
  pages   = {16--25},
  year    = 2006,
  url     = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/sonntag06endod.pdf}
}

@phdthesis{stachniss2006phd,
  author = {C. Stachniss},
  title  = {{Exploration and Mapping with Mobile Robots}},
  school = {University of Freiburg, Department of Computer Science},
  year   = 2006,
  url    = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/stachniss06phd.pdf}
}

@inproceedings{stachniss2006icra,
  author    = {Stachniss, C. and Mart\'{i}nez-Mozos, O. and Burgard, W.},
  title     = {{Speeding-Up Multi-Robot Exploration by Considering Semantic Place Information}},
  booktitle = icra,
  year      = 2006,
  url       = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/stachniss06icra.pdf}
}

@inproceedings{grisetti2006icra,
  author    = {G. Grisetti and G.D. Tipaldi and C. Stachniss and W. Burgard and D. Nardi},
  title     = {{Speeding-Up Rao-Blackwellized {SLAM}}},
  booktitle = icra,
  year      = 2006,
  url       = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/grisetti06icra.pdf}
}

@inproceedings{plagemann2006euros,
  author    = {C. Plagemann and C. Stachniss and W. Burgard},
  title     = {{Efficient Failure Detection for Mobile Robots using Mixed-Abstraction Particle Filters}},
  booktitle = {European Robotics Symposium},
  year      = 2006,
  url       = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/plagemann06euros.pdf}
}

@inproceedings{bennewitz2006euros,
  author    = {M. Bennewitz and C. Stachniss and W. Burgard and S. Behnke},
  booktitle = {European Robotics Symposium 2006},
  title     = {{Metric Localization with Scale-Invariant Visual Features using a Single Perspective Camera}},
  year      = 2006,
  url       = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/bennewitz06euros.pdf},
  keywords  = {Localization}
}

@book{lavalle2006planningbook,
  author    = {S.M. LaValle},
  title     = {{Planning Algorithms}},
  publisher = {Cambridge University Press},
  year      = 2006,
  keywords  = {Book, Motion Planning},
  abstract  = {Due to many exciting developments in the fields of robotics, artificial intelligence, and control theory, three topics that were once quite distinct are presently on a collision course. In robotics, motion planning was originally concerned with problems such as how to move a piano from one room to another in a house without hitting anything. The field has grown, however, to include complications such as uncertainties, multiple bodies, and dynamics. In artificial intelligence, planning originally meant a search for a sequence of logical operators or actions that transform an initial world state into a desired goal state. Presently, planning extends beyond this to include many decision-theoretic ideas such as Markov decision processes, imperfect state information, and game-theoretic equilibria. Although control theory has traditionally been concerned with issues such as stability, feedback, and optimality, there has been a growing interest in designing algorithms that find feasible open-loop trajectories for nonlinear systems. In some of this work, the term motion planning has been applied, with a different interpretation of its use in robotics. Thus, even though each originally considered different problems, the fields of robotics, artificial intelligence, and control theory have expanded their scope to share an interesting common ground. In this text, I use the term planning in a broad sense that encompasses this common ground. This does not, however, imply that the term is meant to cover everything important in the fields of robotics, artificial intelligence, and control theory. The presentation focuses on algorithm issues relating to planning. Within robotics, the focus is on designing algorithms that generate useful motions by processing complicated geometric models. Within artificial intelligence, the focus is on designing systems that use decision-theoretic models compute appropriate actions. Within control theory, the focus is on algorithms that compute feasible trajectories for systems, with some additional coverage of feedback and optimality. Analytical techniques, which account for the majority of control theory literature, are not the main focus here. The phrase planning and control is often used to identify complementary issues in developing a system. Planning is often considered as a higher-level process than control. In this text, I make no such distinctions. Ignoring historical connotations that come with the terms, planning or control can be used in terchangeably. Either refers to some kind of decision making in this text, with no associated notion of high or low level. A hierarchical approach can be developed, and either level could be called planning or control without any difference in meaning.}
}

@book{rasmussen2006gpbook,
  author    = {C.E. Rasmussen and C.K.I. Williams},
  title     = {{Gaussian Processes for Machine Learning}},
  publisher = mitpress,
  year      = 2006,
  keywords  = {Book, Machine Learning, Gaussian Process},
  url       = {http://www.gaussianprocess.org/gpml/chapters/RW.pdf},
  abstract  = {Over the last decade there has been an explosion of work in the kernel machines area of machine learning. Probably the best known example of this is work on support vector machines, but during this period there has also been much activity concerning the application of Gaussian process models to machine learning tasks. The goal of this book is to provide a systematic and unified treatment of this area. Gaussian processes provide a principled, practical, probabilistic approach to learning in kernel machines. This gives advantages with respect to the interpretation of model predictions and provides a wellfounded framework for learning and model selection. Theoretical and practical developments of over the last decade have made Gaussian processes a serious competitor for real supervised learning applications.}
}

@inproceedings{laebe2006tggd,
  title     = {Automatic Relative Orientation of Images},
  author    = {L\"abe, T. and F\"orstner, W.},
  booktitle = {Proceedings of the 5th Turkish-German Joint Geodetic Days},
  year      = {2006},
  abstract  = {This paper presents a new full automatic approach for the relative orientation of several digital images taken with a calibrated camera. This approach uses new algorithms for feature extraction and relative orientation developed in the last few years. There is no need for special markers in the scene nor for approximate values of the orientation data. We use the point operator developed by D. G. Lowe (2004), which extracts points with scale- and rotation-invariant descriptors (SIFT-features). These descriptors allow a successful matching of image points even when dealing with highly convergent or rotated images. The approach consists of the following steps: After extracting image points on all images a matching between every image pair is calculated using the SIFT parameters only. No prior information about the pose of the images or the overlapping parts of the images is used. For every image pair a relative orientation is computed with the help of a RANSAC procedure. Here we use the new 5-point algorithm from D. Nister (2004). Out of this set of orientations approximate values for the orientation parameters and the object coordinates are calculated by computing the relative scales and transforming the models into a common coordinate system. Several tests are made in order to get a reliable input for the currently final step: a bundle block adjustment. The paper discusses the practical impacts of the used algorithms. Examples of different indoor- and outdoor-scenes including a data set of oblique images taken from a helicopter are presented and the results of the approach applied to these data sets are evaluated. These results show that the approach can be used for a wide range of scenes with different types of the image geometry and taken with different types of cameras including inexpensive consumer cameras. In particular we investigate in the robustness of the algorithms, e. g. in geometric tests on image triplets. Further developments like the use of image pyramids with a modified matching are discussed in the outlook. Literature: David G. Lowe, Distinctive image features from scale-invariant keypoints, International Journal of Computer Vision, 60, 2 (2004), pp. 91-110. D. Nister, An efficient solution to the five-point relative pose problem, IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 26(6):756-770, June 2004.},
  url       = {http://www.ipb.uni-bonn.de/pdfs/Labe2006Automatic.pdf}
}

@inproceedings{strobl2006iros,
  author    = {K.H. Strobl and G. Hirzinger},
  title     = {Optimal Hand-Eye Calibration},
  booktitle = iros,
  year      = 2006,
  abstract  = {This paper presents a calibration method for eye-in-hand systems in order to estimate the hand-eye and the robot-world transformations. The estimation takes place in terms of a parametrization of a stochastic model. In order to perform optimally, a metric on the group of the rigid transformations SE(3) and the corresponding error model are proposed for nonlinear optimization. This novel metric works well with both common formulations AX=XB and AX=ZB, and makes use of them in accordance with the nature of the problem. The metric also adapts itself to the system precision characteristics. The method is compared in performance to earlier approaches}
}

@article{stachniss2005advancedrobotics,
  author  = {C. Stachniss and D. H\"{a}hnel and W. Burgard and G. Grisetti},
  title   = {{On Actively Closing Loops in Grid-based FastSLAM}},
  journal = advancedrobotics,
  year    = 2005,
  volume  = {19},
  number  = {10},
  pages   = {1059--1080},
  url     = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/stachniss05ar.pdf}
}

@inproceedings{stachniss2005isrr,
  author    = {Stachniss, C. and Mart\'{i}nez-Mozos, O. and Rottmann, A. and Burgard, W.},
  title     = {{Semantic Labeling of Places}},
  booktitle = isrr,
  year      = 2005,
  url       = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/stachniss05isrr.pdf}
}

@inproceedings{stachniss2005rss,
  title     = {{Information Gain-based Exploration Using Rao-Blackwellized Particle Filters}},
  author    = {C. Stachniss and G. Grisetti and W. Burgard},
  booktitle = rss,
  year      = 2005,
  url       = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/stachniss05rss.pdf}
}

@inproceedings{meier2005ecmr,
  author    = {D. Meier and C. Stachniss and W. Burgard},
  title     = {{Coordinating Multiple Robots During Exploration Under Communication With Limited Bandwidth}},
  booktitle = ecmr,
  year      = 2005,
  url       = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/meier05ecmr.pdf}
}

@inproceedings{rottmann2005aaai,
  title     = {{Place Classification of Indoor Environments with Mobile Robots using Boosting}},
  author    = {A. Rottmann and O. Mart\'{i}nez-Mozos and C. Stachniss and W. Burgard},
  booktitle = aaaiold,
  year      = 2005,
  url       = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/rottmann05aaai.pdf}
}

@article{trahanias2005ram,
  author   = {P. Trahanias and W. Burgard and A. Argyros and D. H\"{a}hnel and H. Baltzakis and P. Pfaff and C. Stachniss},
  title    = {{TOURBOT and WebFAIR: Web-Operated Mobile Robots for Tele-Presence in Populated Exhibitions}},
  journal  = ram,
  year     = 2005,
  volume   = {12},
  number   = {2},
  pages    = {77--89},
  url      = {http://www2.informatik.uni-freiburg.de/~stachnis/pdf/trahanias05webfair-ramprint.pdf},
  abstract = {This paper presents a number of techniques that are needed for realizing Web-operated mobile robots. These techniques include effective map-building capabilities, a method for obstacle avoidance based on a combination of range and visual information, and advanced Web and onboard robot interfaces. In addition to video streams, the system provides high-resolution virtual reality visualizations that also include the people in the vicinity of the robot. This increases the flexibility of the interface and simultaneously allows a user to understand the navigation actions of the robot. The techniques described in this article have been successfully deployed within the EU-funded projects TOURBOT and WebFAIR, which aimed to develop interactive tour-guided robots able to serve Web as well as on-site visitors. Technical developments in the framework of these projects have resulted in robust and reliable systems that have been demonstrated and validated in real-world conditions. Equally important, the system setup time has been drastically reduced, facilitating its porting to new environments.}
}

@article{burgard2005tro,
  author  = {W. Burgard and M. Moors and C. Stachniss and F. Schneider},
  title   = {{Coordinated Multi-Robot Exploration}},
  journal = tro,
  year    = 2005,
  volume  = {21},
  number  = {3},
  pages   = {376--378},
  url     = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/burgard05tro.pdf}
}

@inproceedings{burgard2005snowbird,
  title     = {{Information Gain-based Exploration Using Rao-Blackwellized Particle Filters}},
  author    = {W. Burgard and C. Stachniss and G. Grisetti},
  booktitle = {Proc. of the Learning Workshop (Snowbird)},
  year      = 2005,
  url       = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/burgard05snowbird.pdf}
}

@inproceedings{stachniss2005icra,
  title     = {Recovering Particle Diversity in a Rao-Blackwellized Particle Filter for {SLAM} after Actively Closing Loops},
  author    = {C. Stachniss and G. Grisetti and W. Burgard},
  booktitle = icra,
  year      = 2005,
  url       = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/stachniss05icra.pdf}
}

@inproceedings{martinez2005icra,
  title     = {{Supervised Learning of Places from Range Data using Adaboost}},
  author    = {O. Mart\'{i}nez-Mozos and C. Stachniss and W. Burgard},
  booktitle = icra,
  year      = 2005,
  url       = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/martinez05icra.pdf}
}

@inproceedings{grisetti2005icra,
  title     = {{Improving Grid-based SLAM with Rao-Blackwellized Particle Filters by Adaptive Proposals and Selective Resampling}},
  author    = {G. Grisetti and C. Stachniss and W. Burgard},
  booktitle = icra,
  year      = 2005,
  url       = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/grisetti05icra.pdf}
}

@book{thrun2005probrobbook,
  author    = {S. Thrun and W. Burgard and D. Fox},
  title     = {{Probabilistic Robotics}},
  publisher = mitpress,
  year      = 2005,
  keywords  = {Book, Probabilistic Robotics},
  abstract  = {This book provides a comprehensive introduction into the emerging field of probabilistic robotics. Probabilistic robotics is a subfield of robotics concerned with perception and control. It relies on statistical techniques for representing information and making decisions. By doing so, it accommodates the uncertainty that arises in most contemporary robotics applications. In recent years, probabilistic techniques have become one of the dominant paradigms for algorithm design in robotics. This monograph provides a first comprehensive introduction into some of the major techniques in this field. This book has a strong focus on algorithms. All algorithms in this book are based on a single overarching mathematical foundation: Bayes rule, and its temporal extension known as Bayes filters. This unifying mathematical framework is the core commonality of probabilistic algorithms.}
}

@article{radke2005tip,
  author  = {Radke, R.J. and Andra, S. and Al-Kofahi, O. and Roysam, B.},
  journal = tip,
  number  = {3},
  pages   = {294--307},
  title   = {{Image Change Detection Algorithms: a Systematic Survey}},
  volume  = {14},
  year    = {2005},
  url     = {http://msol.people.uic.edu/ECE531/papers/Image%20Change%20Detection%20Algorithms-%20Survey.pdf}
}

@article{fassi2005jrs,
  author   = {I. Fassi and G. Legnani},
  title    = {Hand to sensor calibration: A geometrical interpretation of the matrix equation AX=XB},
  journal  = {Journal of Robotic Systems},
  year     = 2005,
  volume   = 22,
  pages    = {497--506},
  abstract = {In this paper, the matrix equation AX=XB used for hand to sensor calibration of robot-mounted sensors is analyzed using a geometrical approach. The analysis leads to an original way to describe the properties of the equation and to find all of its solutions. It will also be highlighted why, when multiple instances AiX=XBi (i=1,2,...) of the equation are to be solved simultaneously, the system is overconstrained. Finally, singular cases are also discussed. Â© 2005 Wiley Periodicals, Inc.}
}

@article{lowe2004ijcv,
  author   = {D.G. Lowe},
  title    = {{Distinctive Image Features from Scale-Invariant Keypoints}},
  journal  = ijcv,
  volume   = {60},
  number   = {2},
  year     = 2004,
  pages    = {91--110},
  keywords = {Localization, Image Features}
}

@inproceedings{stachniss2004soave,
  author    = {C. Stachniss and G. Grisetti and D. H\"{a}hnel and W. Burgard},
  title     = {{Improved Rao-Blackwellized Mapping by Adaptive Sampling and Active Loop-Closure}},
  booktitle = soave,
  year      = 2004,
  url       = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/stachniss04soave.pdf}
}

@inproceedings{stachniss2004iros,
  author    = {C. Stachniss and D. H\"{a}hnel and W. Burgard},
  title     = {{Exploration with Active Loop-Closing for FastSLAM}},
  booktitle = iros,
  year      = 2004,
  url       = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/stachniss04iros.pdf}
}

@book{mackay2004informationbook,
  author    = {D. MacKay},
  title     = {{Information Theory, Inference, and Learning Algorithms}},
  publisher = {Cambridge University Press},
  year      = 2004,
  keywords  = {Book, Machine Learning, Information Theory},
  abstract  = {This book is aimed at senior undergraduates and graduate students in Engineering, Science, Mathematics, and Computing. It expects familiarity with calculus, probability theory, and linear algebra as taught in a first- or second- year undergraduate course on mathematics for scientists and engineers. Conventional courses on information theory cover not only the beautiful theoretical ideas of Shannon, but also practical solutions to communication problems. This book goes further, bringing in Bayesian data modelling, Monte Carlo methods, variational methods, clustering algorithms, and neural networks. Why unify information theory and machine learning? Because they are two sides of the same coin. In the 1960s, a single field, cybernetics, was populated by information theorists, computer scientists, and neuroscientists, all studying common problems. Information theory and machine learning still belong together. Brains are the ultimate compression and communication systems. And the state-of-the-art algorithms for both data compression and error-correcting codes use the same tools as machine learning.}
}

@inproceedings{stachniss2003dagstuhl,
  author    = {C. Stachniss and D. H\"{a}hnel and W. Burgard},
  title     = {{Grid-based FastSLAM and Exploration with Active Loop Closing}},
  booktitle = {Online Proc.~of the Dagstuhl Seminar on Robot Navigation (Dagstuhl Seminar~03501)},
  year      = 2003
}

@inproceedings{stachniss2003iros,
  author    = {C. Stachniss and W. Burgard},
  title     = {{Mapping and Exploration with Mobile Robots using Coverage Maps}},
  booktitle = iros,
  year      = 2003,
  url       = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/stachniss03iros.pdf}
}

@inproceedings{stachniss2003ecmr,
  author    = {C. Stachniss and W. Burgard},
  title     = {{Using Coverage Maps to Represent the Environment of Mobile Robots}},
  booktitle = ecmr,
  year      = 2003,
  url       = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/stachniss03ecmr.pdf}
}

@inproceedings{stachniss2003ijcai,
  author    = {C. Stachniss and W. Burgard},
  title     = {{Exploring Unknown Environments with Mobile Robots using Coverage Maps}},
  booktitle = ijcai,
  year      = 2003,
  url       = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/stachniss03ijcai.pdf}
}

@book{lischner2003cppbook,
  author    = {R. Lischner},
  title     = {{C++ in a Nutshell}},
  publisher = {O{\' }Reiley},
  year      = 2003,
  keywords  = {Book, Programming, CPP},
  abstract  = {C++ in a Nutshell is a reference to the C++ language and library. Being a Nutshell guide, it is not a comprehensive manual, but it is complete enough to cover everything a working professional needs to know. Nonetheless, C++ is such a large and complex language that even this Nutshell guide is a large book. This book covers the C++ standard, the international standard published as ISO/IEC 14882:1998(E), Programming LanguagesâC++, plus Technical Corrigendum 1. Many implementations of C++ extend the language and standard library. Except for brief mentions of language and library extensions in the appendixes, this book covers only the standard. The standard library is largeâit includes strings, containers, common algorithms, and much moreâbut it omits much that is commonplace in computing today: concurrency, network protocols, database access, graphics, windows, and so on. See Appendix B for information about nonstandard libraries that provide additional functionality. This book is a reference. It is not a tutorial. Newcomers to C++ might find portions of this book difficult to understand. Although each section contains some advice on idioms and the proper use of certain language constructs, the main focus is on the reference material.}
}

@book{schildt2003cppbook,
  author    = {H. Schildt},
  title     = {{C++ From the Ground Up}},
  publisher = {McGraw-Hill/Osborne},
  year      = 2003,
  keywords  = {Book, Programming, CPP},
  abstract  = {This book teaches you how to program in C++, the most powerful computer language in use today. No previous programming experience is required. The book starts with the basics, covers the fundamentals, moves on to the core of the language, and concludes with its more advanced features. By the time you finish, you will be an accomplished C++ programmer.}
}

@article{bengtsson2003jras,
  title    = {{Robot Localization Based on Scan-Matching -- Estimating the Covariance Matrix for the IDC Algorithm}},
  author   = {O. Bengtsson and A. Baerveldt},
  journal  = jras,
  volume   = {44},
  number   = {1},
  pages    = {29--40},
  year     = {2003},
  abstract = {We have previously presented a new scan-matching algorithm based on the IDC (iterative dual correspondence) algorithm, which showed a good localization performance even in environments with severe changes. The problem of the IDC algorithm is that there is no good way to estimate a covariance matrix of the position estimate, which prohibits an effective fusion with other position estimates of other sensors. This paper presents two new ways to estimate the covariance matrix. The first estimates the covariance matrix from the Hessian matrix of the error function minimized by the scan-matching algorithm. The second one, which is an off-line method, estimates the covariance matrix of a specific scan, from a specific position by simulating and matching scans around the position. Simulation results show that the covariance matrix provided by the off-line method fully corresponds with the real one. Some preliminary tests on real data indicate that the off-line method gives a good quality value of a specific scan position, which is of great value in map building.}
}

@phdthesis{roy2003thesis,
  title    = {Finding Approximate POMDP solutions Through Belief Compression},
  author   = {N. Roy},
  keywords = {POMDPs, Robotics, Planning, Machine Learning},
  school   = {Carnegie Mellon Univ.},
  year     = {2003},
  url      = {http://www.ri.cmu.edu/pub_files/pub4/roy_nicholas_2003_1/roy_nicholas_2003_1.pdf}
}

@article{scott2003acmcs,
  title   = {{View Planning for Automated 3D Object Reconstruction Inspection}},
  author  = {Scott, W. and Roth, G. and Rivest, J.},
  journal = acmcs,
  volume  = {35},
  number  = {1},
  year    = {2003},
  url     = {https://www.researchgate.net/profile/Gerhard_Roth/publication/44053230_View_Planning_for_Automated_3D_Object_Reconstruction_Inspection/links/544baca70cf2bcc9b1d6be1f.pdf}
}

@inproceedings{newman2003icra,
  title     = {Autonomous feature-based exploration},
  author    = {Newman, P. and Bosse, M. and Leonard, J.},
  booktitle = icra,
  year      = {2003},
  url       = {http://neuro.bstu.by/ai/To-dom/My_research/Papers-2.0/shortest%20route%20loop%20navigation%20gridworld/ICRA03_explore.pdf}
}

@inproceedings{stachniss2002iros,
  author    = {C. Stachniss and W. Burgard},
  title     = {{An Integrated Approach to Goal-directed Obstacle Avoidance under Dynamic Constraints for Dynamic Environments}},
  booktitle = iros,
  year      = 2002,
  url       = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/stachniss02iros.pdf}
}

@mastersthesis{stachniss2002diplom,
  author = {C. Stachniss},
  title  = {{Z}ielgerichtete {K}ollisionsvermeidung f{\"u}r mobile {R}oboter in dynamischen {U}mgebungen},
  school = {University of Freiburg, Department of Computer Science},
  year   = 2002,
  note   = {In German},
  url    = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/stachniss02diplom.pdf}
}

@inproceedings{julier2002acc,
  title     = {{The Scaled Unscented Transformation}},
  author    = {S.J. Julier},
  year      = 2002,
  booktitle = acc,
  keywords  = {Kalman Filter, Non-Linear Estimation, Unscented Filtering, State Estimation},
  abstract  = {This paper describes a generalisation of the unscented transformation (UT) which allows sigma points to be scaled to an arbitrary dimension. The UT is a method for predicting means and covariances in nonlinear systems. A set of samples are deterministically chosen which match the mean and covariance of a (not necessarily Gaussian-distributed) probability distribution. These samples can be scaled by an arbitrary constant. The method guarantees that the mean and covariance second order accuracy in mean and covariance, giving the same performance as a second order truncated filter but without the need to calculate any Jacobians or Hessians. The impacts of scaling issues are illustrated by considering conversions from polarto Cartesian coordinates with large angular uncertainties.}
}

@inproceedings{bourgault2002iros,
  title     = {{Information Based Adaptive Robotic Exploration}},
  author    = {Bourgault, F. and Makarenko, A.A. and Williams, S.B. and Grocholsky, B. and Durrant-Whyte, H.F.},
  booktitle = iros,
  year      = {2002},
  url       = {https://s3.amazonaws.com/academia.edu.documents/42943278/Information_based_adaptive_robotic_explo20160222-29566-we5vs7.pdf?AWSAccessKeyId=AKIAIWOWYYGZ2Y53UL3A&Expires=1515513407&Signature=TF5Jp5sXUmfJiVAEkHmBancAY44%3D&response-content-disposition=inline%3B%20filename%3DInformation_based_adaptive_robotic_explo.pdf}
}

@misc{welch2001tutorial,
  author       = {G. Welch and G. Bischop},
  title        = {{An Introduction to the Kalman Filter}},
  howpublished = {SIGHGRAPH 2001 Tutorial Course},
  year         = 2001,
  keywords     = {Tutorial, Kalman Filter},
  abstract     = {The Kalman filter is a mathematical power tool that is playing an increasingly important role in computer graphics as we include sensing of the real world in our systems. The good news is you do not have to be a mathematical genius to understand and effectively use Kalman filters. This tutorial is designed to provide developers of graphical systems with a basic understanding of this important mathematical tool.While the Kalman filter has been around for about 30 years, it (and related optimal estimators) have recently started popping up in a wide variety of computer graphics applications. These applications span from simulating musical instruments in VR, to head tracking, to extracting lip motion from video sequences of speakers, to fitting spline surfaces over collections of points. The Kalman filter is the best possible (optimal) estimator for a large class of problems and a very effective and useful estimator for an even larger class. With a few conceptual tools, the Kalman filter is actually very easy to use. We will present an intuitive approach to this topic that will enable developers to approach the extensive literature with confidence.},
  url          = {https://courses.cs.washington.edu/courses/cse571/03wi/notes/welch-bishop-tutorial.pdf}
}

@article{hemming2001jaer,
  author  = {Hemming, J. and Rath, T.},
  journal = jaer,
  number  = 3,
  pages   = {233--243},
  title   = {{Computer-Vision-based Weed Identification under Field Conditions using Controlled Lighting}},
  volume  = 78,
  year    = {2001},
  url     = {http://www.hemming.nl/jh/bib/jaer2000.pdf}
}

@article{horaud2000pami,
  author  = {R. Horaud and G. Csurka and D. Demirdijian},
  title   = {Stereo calibration from rigid motions},
  journal = pami,
  year    = 2000,
  pages   = {1446--1452},
  volume  = {22},
  number  = {12}
}

@inproceedings{roy1999icra,
  title     = {{Coastal navigation: Mobile robot navigation with uncertainty in dynamic environments}},
  author    = {N. Roy and W. Burgard and D. Fox and S. Thrun},
  booktitle = icra,
  keywords  = {Navigation, Planning, Coastal Navigation, POMDP},
  year      = 1999,
  url       = {http://www.ipb.uni-bonn.de/pdfs/roy1999icra.pdf}
}

@inproceedings{roy1999nips,
  title     = {{Coastal navigation with mobile robots}},
  author    = {N. Roy and S. Thrun},
  booktitle = nips,
  keywords  = {Navigation, Planning, Coastal Navigation, POMDP},
  year      = 1999,
  url       = {http://web.mit.edu/nickroy/www/papers/nips99.pdf},
  abstract  = {The problem that we address in this paper is how a mobile robot can plan in order to arrive at its goal with minimum uncertainty. Traditional motion planning algorithms often assume that a mobile robot can track its position reliably, however, in real world situations, reliable localization may not always be feasible. Partially Observable Markov Decision Processes (POMDPs) provide one way to maximize the certainty of reaching the goal state, but at the cost of computational intractability for large state spaces. The method we propose explicitly models the uncertainty of the robotâs position as a state variable, and generates trajectories through the augmented pose-uncertainty space. By minimizing the positional uncertainty at the goal, the robot reduces the likelihood it becomes lost. We demonstrate experimentally that coastal navigation reduces the uncertainty at the goal, especially with degraded localization}
}

@inproceedings{lowe1999iccv,
  title     = {{Object recognition from local scale-invariant features}},
  author    = {Lowe, D.G.},
  booktitle = iccvold,
  year      = {1999},
  url       = {http://www.cs.ubc.ca/~lowe/papers/iccv99.pdf}
}

@inproceedings{gionis1999vldb,
  title     = {Similarity search in high dimensions via hashing},
  author    = {A. Gionis and P. Indyk and R. Motwani},
  booktitle = vldb,
  year      = {1999}
}

@article{daniilidis1999ijrr,
  author  = {K. Daniilidis},
  title   = {Hand-eye calibration using dual quaternions},
  journal = ijrr,
  year    = 1999,
  pages   = {286--298},
  volume  = {18},
  number  = {3}
}

@article{pito1999pami,
  title   = {{A Solution to the Next Best View Problem for Automated Surface Acquisition}},
  author  = {Pito, R.},
  journal = pami,
  volume  = {21},
  number  = {10},
  pages   = {1016--1030},
  year    = {1999},
  url     = {https://www.computer.org/csdl/trans/tp/1999/10/i1016.pdf}
}

@article{fox1998jras,
  title   = {Active markov localization for mobile robots},
  author  = {D. Fox and W. Burgard and S. Thrun},
  journal = jras,
  volume  = {25},
  number  = {3--4},
  pages   = {195--207},
  year    = {1998},
  url     = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.1.8418&rep=rep1&type=pdf}
}

@book{stroustrup1997cppbook,
  author    = {B. Stroustrup},
  title     = {{The C++ Programming Language}},
  publisher = {AT\&T},
  keywords  = {Book, Programming, CPP},
  year      = 1997,
  abstract  = {This book introduces standard C++ and the key programming and design techniques supported by C++. Standard C++ is a far more powerful and polished language than the version of C++ intro- duced by the first edition of this book. New language features such as namespaces, exceptions, templates, and run-time type identification allow many techniques to be applied more directly than was possible before, and the standard library allows the programmer to start from a much higher level than the bare language. The primary aim of this book is to help the reader understand how the facilities offered by C++ support key programming techniques. The aim is to take the reader far beyond the point where he or she gets code running primarily by copying examples and emulating programming styles from other languages. Only a good understanding of the ideas behind the language facilities leads to mastery. Supplemented by implementation documentation, the information provided is sufficient for completing significant real-world projects. The hope is that this book will help the reader gain new insights and become a better programmer and designer.}
}

@book{mitchell1997mlbook,
  author    = {T. Mitchell},
  title     = {{Machine Learning}},
  publisher = {McGraw-Hill Science},
  year      = 1997,
  keywords  = {Book, Machine Learning},
  abstract  = {This book covers the field of machine learning, which is the study of algorithms that allow computer programs to automatically improve through experience. The book is intended to support upper level undergraduate and introductory level graduate courses in machine learning}
}

@article{julier1997crews,
  author  = {Julier, S.J. and Uhlmann, J.K.},
  journal = crews,
  volume  = {3068},
  pages   = {182--193},
  title   = {{A New Extension of the Kalman Filter to Nonlinear Systems}},
  year    = {1997},
  url     = {http://jimbeck.caltech.edu/summerlectures/references/Unscented%20Kalman%20filter.pdf}
}

@inproceedings{yamauchi1997cira,
  title     = {{A Frontier-Based Approach for Autonomous Exploration}},
  author    = {Yamauchi, B.},
  booktitle = cira,
  year      = {1997},
  url       = {http://www.robotfrontier.com/papers/cira97.pdf}
}

@inproceedings{zisserman1995iwrvs,
  author    = {A. Zisserman and P.A. Beardsley and I.D. Reid},
  title     = {Metric calibration of a stereo rig},
  booktitle = {Proc.~of the IEEE Workshop on Representation of Visual Scenes},
  year      = 1995,
  abstract  = {Describes a method to determine affine and metric calibration for a stereo rig. The method does not involve the use of calibration objects or special motions, but simply a single general motion of the rig with fixed parameters (i.e. camera parameters and relative orientation of the camera pair). The novel aspects of this work are: first, relating the distinguished objects of Euclidean geometry to fixed entities of a Euclidean transformation matrix; second, showing that these fixed entities are accessible from the conjugate Euclidean transformation arising from the projective transformation of the structure under a motion of the fixed stereo rig; and third, a robust and automatic implementation of the method. Results are included of affine and metric calibration and structure recovery using images of real scenes}
}

@article{park1994tra,
  author   = {F.C. Park and B.J. Martin},
  title    = {{Robot sensor calibration: solving AX=XB on the Euclidean group}},
  journal  = tra,
  year     = 1994,
  pages    = {717--721},
  volume   = {10},
  number   = {5},
  url      = {http://robotics.snu.ac.kr/fcp/files/_pdf_files_publications/7_c/robot_sensor_calibration.pdf},
  abstract = {The equation AX=XB on the Euclidean group arises in the problem of calibrating wrist-mounted robotic sensors. In this article the authors derive, using methods of Lie theory, a closed-form exact solution that can be visualized geometrically, and a closed-form least squares solution when A and B are measured in the presence of noise}
}

@article{shiu1989tra,
  author  = {Y.C. Shiu and S. Ahmad},
  title   = {Calibration of wrist-mounted robotic sensors by solving homogeneous transform equations of the form AX=XB},
  journal = tra,
  year    = 1989,
  pages   = {16--29},
  volume  = {5},
  number  = {1}
}

@article{aloimonos1988ijcv,
  title   = {{Active Vision}},
  author  = {Aloimonos, J. and Weiss, I. and Bandyopadhyay, A.},
  journal = ijcv,
  volume  = {1},
  number  = {4},
  pages   = {333--356},
  year    = {1988},
  url     = {https://link.springer.com/content/pdf/10.1007%2FBF00133571.pdf}
}

@article{bajcsy1988pieee,
  title   = {{Active Perception}},
  author  = {Bajcsy, R.},
  journal = pieee,
  volume  = {76},
  number  = {8},
  pages   = {966--1005},
  year    = {1988},
  url     = {https://www.researchgate.net/profile/Yiannis_Aloimonos/publication/228083826_Active_Perception/links/00b49528e6f94c813e000000.pdf}
}

@article{suzuki1985cvgip,
  title    = {{Topological Structural Analysis of Digitized Binary Images by Border Following}},
  author   = {Suzuki, S. and Abe, K.},
  journal  = cvgip,
  volume   = {30},
  number   = {1},
  pages    = {32--46},
  year     = {1985},
  url      = {https://s3.amazonaws.com/academia.edu.documents/38698235/suzuki1985.pdf?AWSAccessKeyId=AKIAIWOWYYGZ2Y53UL3A&Expires=1505921107&Signature=zJn8q0RKxCG7UahiOOoaxD6E2ss%3D&response-content-disposition=inline%3B%20filename%3DTopological_Structural_Analysis_of_Digit.pdf},
  abstract = {Two border following algorithms are proposed for the topological analysis of digitized binary images. The first one determines the surroundness relations among the borders of a binary image. Since the outer borders and the hole borders have a one-to-one correspondence to the connected components of l-pixels and to the holes, respectively, the proposed algorithm yields a representation of a binary image, from which one can extract some sort of features without reconstructing the image. The second algorithm, which is a modified version of the first, follows only the outermost borders (i.e., the outer borders which are not surrounded by holes). These algorithms can be effectively used in component counting, shrinking, and topological structural analysis of binary images, when a sequential digital computer is used.}
}

@book{huber1981book,
  title     = {{Robust Statistics}},
  publisher = {Wiley},
  year      = {1981},
  author    = {P. J. Huber}
}

@book{strunk1997stylebook,
  author    = {W. Strunk and E.B. White},
  title     = {{The Elements of Style}},
  publisher = {Allyn and Bacon},
  keywords  = {Book, Writing},
  year      = 1979,
  abstract  = {No book in shorter space, with fewer words, will help any writer more than this persistent little volume.}
}

@article{kalman1960tasme,
  title    = {{A New Approach to Linear Filtering and Prediction Problems}},
  author   = {R.E. Kalman},
  journal  = {Transactions of the ASME -- Journal of Basic Engineering},
  volume   = {82},
  year     = 1960,
  pages    = {35--45},
  abstract = {The classical filtering and prediction problem is re-examined using the Bode- Shannon representation of random processes and the state transition method of analysis of dynamic systems. New results are: (1) The formulation and methods of solution of the problem apply without modifica- tion to stationary and nonstationary statistics and to growing-memory and infinite-memory filters. (2) A nonlinear difference (or differential) equation is derived for the covariance matrix of the optimal estimation error. From the solution of this equation the coefficients of the difference (or differential) equation of the optimal linear filter are ob- tained without further calculations. (3) The filtering problem is shown to be the dual of the noise-free regulator problem. The new method developed here is applied to two well-known problems, confirming and extending earlier results. The discussion is largely self-contained and proceeds from first principles; basic concepts of the theory of random processes are reviewed in the Appendix.},
  keywords = {Kalman Filter, State Estimation}
}


@article{andreasson2010ras,
  author  = {H. Andreasson and A.J Lilienthal},
  title   = {6D scan registration using depth-interpolated local image features},
  journal = ras,
  year    = {2010},
  number  = {2},
  pages   = {157--165},
  volume  = {58}
}

@article{besl1992pami,
  author   = {P.J. Besl and N.D. McKay},
  title    = {{A Method for Registration of 3D Shapes}},
  journal  = pami,
  year     = {1992},
  volume   = {14},
  number   = {2},
  pages    = {239--256},
  url      = {http://www-evasion.inrialpes.fr/people/Franck.Hetroy/Teaching/ProjetsImage/2007/Bib/besl_mckay-pami1992.pdf},
  keywords = {Registration},
  abstract = {The authors describe a general-purpose, representation-independent method for the accurate and computationally efficient registration of 3-D shapes including free-form curves and surfaces. The method handles the full six degrees of freedom and is based on the iterative closest point (ICP) algorithm, which requires only a procedure to find the closest point on a geometric entity to a given point. The ICP algorithm always converges monotonically to the nearest local minimum of a mean-square distance metric, and the rate of convergence is rapid during the first few iterations. Therefore, given an adequate set of initial rotations and translations for a particular class of objects with a certain level of 'shape complexity', one can globally minimize the mean-square distance metric over all six degrees of freedom by testing each initial registration. One important application of this method is to register sensed data from unfixtured rigid objects with an ideal geometric model, prior to shape inspection. Experimental results show the capabilities of the registration algorithm on point sets, curves, and surfaces.}
}

@article{engel2018pami,
  author   = {J. Engel and V. Koltun and D. Cremers},
  title    = {{Direct Sparse Odometry}},
  journal  = pami,
  year     = 2018,
  pages    = {611--625},
  volume   = {40},
  number   = {3},
  url      = {http://vladlen.info/papers/DSO.pdf},
  abstract = {Direct Sparse Odometry (DSO) is a visual odometry method based on a novel, highly accurate sparse and direct structure and motion formulation. It combines a fully direct probabilistic model (minimizing a photometric error) with consistent, joint optimization of all model parameters, including geometry-represented as inverse depth in a reference frame-and camera motion. This is achieved in real time by omitting the smoothness prior used in other direct methods and instead sampling pixels evenly throughout the images. Since our method does not depend on keypoint detectors or descriptors, it can naturally sample pixels from across all image regions that have intensity gradient, including edges or smooth intensity variations on essentially featureless walls. The proposed model integrates a full photometric calibration, accounting for exposure time, lens vignetting, and non-linear response functions. We thoroughly evaluate our method on three different datasets comprising several hours of video. The experiments show that the presented approach significantly outperforms state-of-the-art direct and indirect methods in a variety of real-world settings, both in terms of tracking accuracy and robustness.},
  keywords = {Visual Odometry}
}

@inproceedings{engel2014eccv,
  author    = {J. Engel and T. Sch{\"o}ps and D. Cremers},
  title     = {{LSD-SLAM: Large-scale direct monocular SLAM}},
  booktitle = eccv,
  year      = {2014},
  keywords  = {SLAM, Visual Odometry, Computer Vision}
}

@inproceedings{harrison2010fsr,
  author    = {A. Harrison and P. Newman},
  title     = {{Image and Sparse Laser Fusion for Dense Scene Reconstruction}},
  booktitle = {Field and Service Robotics},
  year      = {2010}
}

@book{hartley2004book,
  author    = {R. Hartley and A. Zisserman},
  title     = {{Multiple View Geometry in Computer Vision}},
  publisher = {Cambridge University Press},
  year      = {2004},
  edition   = {Second}
}

@inproceedings{joung2009iros,
  author    = {J.H. Joung and K.H. An and J.W. Kang and M.J. Chung and W. Yu},
  title     = {{3D environment reconstruction using modified color ICP algorithm by fusion of a camera and a 3D laser range finder}},
  booktitle = iros,
  year      = {2009},
  url       = {proceedings:joung2009iros.pdf},
  abstract  = {In this paper, we propose a system which reconstructs the environment with both color and 3D information. We perform extrinsic calibration of a camera and a LRF (laser range finder) to fuse 3D information and color information of objects. We also formularize an equation to measure the result of the calibration. Moreover, we acquire 3D data by rotating 2D LRF with camera, and use ICP (iterative closest point) algorithm to combine data acquired in other places. We use the SIFT (scale invariant feature transform) matching for the initial estimation of ICP algorithm. It offers accurate and stable initial estimation robust to motion change compare to odometry. We also modify the ICP algorithm using color information. Computation time of ICP algorithm can be reduced by using color information.}
}

@inproceedings{nickels2003icar,
  author    = {K.M. Nickels and A. Casta{\~n}o, and C. Cianci},
  title     = {{Fusion of Lidar and Stereo Range for Mobile Robots}},
  booktitle = icar,
  year      = {2003}
}

@inproceedings{kerl2013icra,
  author    = {C. Kerl and J. Sturm and D. Cremers},
  title     = {{Robust Odometry Estimation for RGB-D Cameras}},
  booktitle = icra,
  year      = {2013},
  url       = {proceedings:kerl2013icra.pdf},
  abstract  = {The goal of our work is to provide a fast and accurate method to estimate the camera motion from RGB-D images. Our approach registers two consecutive RGB-D frames directly upon each other by minimizing the photometric error. We estimate the camera motion using non-linear minimization in combination with a coarse-to-fine scheme. To allow for noise and outliers in the image data, we propose to use a robust error function that reduces the influence of large residuals. Furthermore, our formulation allows for the inclusion of a motion model which can be based on prior knowledge, temporal filtering, or additional sensors like an IMU. Our method is attractive for robots with limited computational resources as it runs in real-time on a single CPU core and has a small, constant memory footprint. In an extensive set of experiments carried out both on a benchmark dataset and synthetic data, we demonstrate that our approach is more accurate and robust than previous methods. We provide our software under an open source license.}
}

@inproceedings{klein2007ismar,
  author    = {G. Klein and D. Murray},
  title     = {{Parallel Tracking and Mapping for Small AR Workspaces}},
  booktitle = ismar,
  year      = 2007,
  abstract  = {This paper presents a method of estimating camera pose in an unknown scene. While this has previously been attempted by adapting SLAM algorithms developed for robotic exploration, we propose a system specifically designed to track a hand-held camera in a small AR workspace. We propose to split tracking and mapping into two separate tasks,  processed  in parallel threads  on a dual-core  computer:  one  thread deals with the task  of  robustly tracking  erratic hand-held motion, while the other produces a 3D map of point features from previously observed video frames. This allows the use of computationally expensive batch optimisation techniques not usually associated with real-time operation: The result is a system that produces detailed maps with thousands of landmarks which can be tracked at frame-rate, with an accuracy and robustness rivalling that of state-of-the-art model-based systems.}
}

@inproceedings{maddern2016iros,
  author    = {W. Maddern and P. Newman},
  title     = {Real-time probabilistic fusion of sparse 3D LIDAR and dense stereo},
  booktitle = iros,
  year      = {2016},
  url       = {proceedings:maddern2016iros.pdf},
  abstract  = {Real-time 3D perception is critical for localisation, mapping, path planning and obstacle avoidance for mobile robots and autonomous vehicles. For outdoor operation in real-world environments, 3D perception is often provided by sparse 3D LIDAR scanners, which provide accurate but low-density depth maps, and dense stereo approaches, which require significant computational resources for accurate results. Here, taking advantage of the complementary error characteristics of LIDAR range sensing and dense stereo, we present a probabilistic method for fusing sparse 3D LIDAR data with stereo images to provide accurate dense depth maps and uncertainty estimates in real-time. We evaluate the method on data collected from a small urban autonomous vehicle and the KITTI dataset, providing accuracy results competitive with state-of-the-art stereo approaches and credible uncertainty estimates that do not misrepresent the true errors, and demonstrate real-time operation on a range of low-power GPU systems.}
}

@inproceedings{men2011icra,
  author    = {H. Men and B. Gebre and K. Pochiraju},
  title     = {{Color Point Cloud Registration with 4D ICP Algorithm}},
  booktitle = icra,
  year      = {2011},
  url       = {proceedings:men2011icra.pdf}
}

@article{mur-artal2015tro,
  author  = {R. Mur-Artal and J.M.M. Montiel and J. D Tardos},
  title   = {{ORB-SLAM: a versatile and accurate monocular SLAM system}},
  journal = tro,
  year    = {2015},
  number  = {5},
  pages   = {1147--1163},
  volume  = {31}
}

@inproceedings{naikal2009iros,
  author    = {N. Naikal and J. Kua and G. Chen and A. Zakhor},
  title     = {Image augmented laser scan matching for indoor dead reckoning},
  booktitle = iros,
  year      = {2009},
  url       = {proceedings:naikal2009iros.pdf},
  abstract  = {Most existing approaches to indoor localization focus on using either cameras or laser scanners as the primary sensor for pose estimation. In scan matching based localization, finding scan point correspondences across scans is challenging as individual scan points lack unique attributes. In camera based localization, one has to deal with images with few or no visual features as well as scale factor ambiguities to recover absolute distances. In this paper, we develop multimodal approaches for two indoor localization problems by fusing a camera and laser scanners in order to alleviate the drawbacks of each individual modality. For our first problem we recover 3 Degrees of Freedom (DoF) of a camera-laser rig on a rolling cart in a 2D plane, by using visual odometry to facilitate scan correspondence estimation. We demonstrate this approach to result in a 0.3\% loop closure error for a 60m loop around the interior corridor of a building. In our second problem, we recover 6 DoF of a human operator carrying a backpack system mounted with sensors in 3D, by merging rotation estimates from scan matching and translation estimates from visual odometry, resulting in a 1\% loop closure error.}
}

@inproceedings{newman2006icra,
  author    = {P. Newman and D. Cole and K. Ho},
  title     = {Outdoor SLAM using visual appearance and laser ranging},
  booktitle = icra,
  year      = {2006}
}

@article{nister2004pami,
  author  = {D. Nist{\'e}r},
  title   = {An efficient solution to the five-point relative pose problem},
  journal = pami,
  year    = {2004},
  number  = {6},
  pages   = {756--770},
  volume  = {26}
}

@inproceedings{olson2009icra,
  author    = {E.B. Olson},
  title     = {{Real-Time Correlative Scan Matching}},
  booktitle = icra,
  year      = {2009},
  url       = {proceedings:olson2009icra.pdf}
}

@inproceedings{pandey2011icra,
  author    = {G. Pandey and J. McBride and S. Savarese and R. Eustice},
  title     = {{Visually Bootstrapped Generalized ICP}},
  booktitle = icra,
  year      = {2011},
  abstract  = {This paper reports a novel algorithm for bootstrapping the automatic registration of unstructured 3D point clouds collected using co-registered 3D lidar and omnidirectional camera imagery. Here, we exploit the co-registration of the 3D point cloud with the available camera imagery to associate high dimensional feature descriptors such as scale invariant feature transform (SIFT) or speeded up robust features (SURF) to the 3D points. We first establish putative point correspondence in the high dimensional feature space and then use these correspondences in a random sample consensus (RANSAC) framework to obtain an initial rigid body transformation that aligns the two scans. This initial transformation is then refined in a generalized iterative closest point (ICP) framework. The proposed method is completely data driven and does not require any initial guess on the transformation. We present results from a real world dataset collected by a vehicle equipped with a 3D laser scanner and an omnidirectional camera.},
  keywords  = {Computer Vision for Robotics and Automation, Sensor Fusion, SLAM},
  url       = {proceedings:pandey2011icra.pdf}
}

@article{pomerleau2015ftr,
  author  = {F. Pomerleau and F. Colas and R. Siegwart},
  title   = {{A Review of Point Cloud Registration Algorithms for Mobile Robotics}},
  year    = {2015},
  pages   = {1--104},
  volume  = {4},
  journal = fntr
}

@inproceedings{rusinkiewicz2001dim,
  author    = {S. Rusinkiewicz and M. Levoy},
  title     = {{Efficient variants of the ICP algorithm}},
  booktitle = {Proc.~of Int. Conf. on 3-D Digital Imaging and Modeling},
  year      = {2001}
}

@inproceedings{segal2009rss,
  author    = {A. Segal and D. Haehnel and S. Thrun},
  title     = {{Generalized-ICP}},
  booktitle = rss,
  year      = {2009},
  abstract  = {In this paper we combine the Iterative Closest Point (ICP') and 'point-to-plane ICP' algorithms into a single probabilistic framework. We then use this framework to model locally planar surface structure from both scans instead of just the "model" scan as is typically done with the point-to-plane method. This can be thought of as 'plane-to-plane'. The new approach is tested with both simulated and real-world data and is shown to outperform both standard ICP and point-to-plane. Furthermore, the new approach is shown to be more robust to incorrect correspondences, and thus makes it easier to tune the maximum match distance parameter present in most variants of ICP. In addition to the demonstrated performance improvement, the proposed model allows for more expressive probabilistic models to be incorporated into the ICP framework. While maintaining the speed and simplicity of ICP, the Generalized-ICP could also allow for the addition of outlier terms, measurement noise, and other probabilistic techniques to increase robustness.}
}

@inproceedings{zhang2015icra-voam,
  author    = {J. Zhang and S. Singh},
  title     = {{Visual-lidar odometry and mapping: Low-drift, robust, and fast}},
  booktitle = icra,
  year      = {2015},
  url       = {proceedings:zhang2015icra-voam.pdf},
  abstract  = {Here, we present a general framework for combining visual odometry and lidar odometry in a fundamental and first principle method. The method shows improvements in performance over the state of the art, particularly in robustness to aggressive motion and temporary lack of visual features. The proposed on-line method starts with visual odometry to estimate the ego-motion and to register point clouds from a scanning lidar at a high frequency but low fidelity. Then, scan matching based lidar odometry refines the motion estimation and point cloud registration simultaneously.We show results with datasets collected in our own experiments as well as using the KITTI odometry benchmark. Our proposed method is ranked #1 on the benchmark in terms of average translation and rotation errors, with a 0.75\% of relative position drift. In addition to comparison of the motion estimation accuracy, we evaluate robustness of the method when the sensor suite moves at a high speed and is subject to significant ambient lighting changes.}
}

@article{zhang1994ijcv,
  author  = {Z. Zhang},
  title   = {{Iterative point matching for registration of free-form curves and surfaces}},
  journal = ijcv,
  year    = {1994},
  number  = {2},
  pages   = {119--152},
  volume  = {13}
}

@inproceedings{chollet2017cvpr,
  author    = {F. Chollet},
  title     = {{Xception: Deep Learning With Depthwise Separable Convolutions}},
  booktitle = cvpr,
  year      = 2017,
  abstract  = {We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters.},
  url       = {proceedings: chollet2017cvpr.pdf}
}


@book{chollet2017keras,
  author    = {F. Chollet},
  title     = {{Deep Learning with Python}},
  publisher = {Manning},
  year      = 2017,
  keywords  = {Book, Deep Learning, CNN, Tensorflow, Keras},
  abstract  = {Deep Learning with Python introduces the field of deep learning using the Python language and the powerful Keras library.
               Written by Keras creator and Google AI researcher Francois Chollet, this book builds your understanding through intuitive explanations and practical examples. Youâll explore challenging concepts and practice with applications in computer vision, natural-language processing, and generative models. By the time you finish, youâll have the knowledge and hands-on skills to apply deep learning in your own projects.
               
               Note: A watermarked PDF is available. Ask Jens if your are interested.}
}

@inproceedings{andreasson2005iros,
  author    = {Andreasson, H. and Triebel, R. and Burgard, W.},
  title     = {{Improving Plane Extraction from 3D Data by Fusing Laser Data and Vision}},
  booktitle = iros,
  year      = 2005
}

@article{chen2018pami,
  title   = {{DeepLab: Semantic Image Segmentation withDeep Convolutional Nets, Atrous Convolution,and Fully Connected CRFs}},
  author  = {L.C. Chen and G. Papandreou and I. Kokkinos and K. Murphy and A.L. Yuille},
  journal = pami,
  volume  = {40},
  number  = {4},
  pages   = {834--848},
  year    = {2018},
  url     = {http://arxiv.org/pdf/1606.00915}
}

@article{zhao2016arxiv,
  author  = {H. Zhao and J. Shi and X. Qi and X. Wang and J. Jia},
  title   = {{Pyramid Scene Parsing Network}},
  journal = arxiv,
  volume  = {arXiv:1612.01105},
  year    = {2016},
  url     = {http://arxiv.org/pdf/1612.01105}
}

@inproceedings{glorot2010aistats,
  title     = {Understanding the difficulty of training deep feedforward neural networks},
  author    = {Glorot, Xavier and Bengio, Yoshua},
  booktitle = {Proc.~of the Intl.~Conf.~on Artificial Intelligence and Statistics},
  year      = {2010},
  url       = {https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf}
}

@inproceedings{zhao2017cvpr-pspn,
  author    = {H. Zhao and J. Shi and X. Qi and X. Wang and J. Jia},
  title     = {{Pyramid Scene Parsing Network}},
  booktitle = cvpr,
  year      = 2017,
  abstract  = {Scene parsing is challenging for unrestricted open vocabulary and diverse scenes. In this paper, we exploit the capability of global context information by different-region-based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet). Our global prior representation is effective to produce good quality results on the scene parsing task, while PSPNet provides a superior framework for pixel-level prediction. The proposed approach achieves state-of-the-art performance on various datasets. It came first in ImageNet scene parsing challenge 2016, PASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields the new record of mIoU accuracy 85.4\% on PASCAL VOC 2012 and accuracy 80.2\% on Cityscapes.},
  url       = {proceedings: zhao2017cvpr-pspn.pdf}
}

@article{romera2018tits,
  author   = {E. Romera and J. M. Alvarez and L. M. Bergasa and R. Arroyo},
  journal  = tits,
  title    = {{ERFNet: Efficient Residual Factorized ConvNet for Real-Time Semantic Segmentation}},
  year     = 2018,
  volume   = {19},
  number   = {1},
  pages    = {263--272},
  keywords = {Computer architecture;Image segmentation;Kernel;Real-time systems;Semantics;Two dimensional displays;Intelligent vehicles;deep learning;real-time;residual layers;scene understanding;semantic segmentation},
  url      = {http://www.robesafe.uah.es/personal/eduardo.romera/pdfs/Romera17tits.pdf}
}

@inproceedings{sandler2018cvpr,
  title     = {{MobileNetV2: Inverted residuals and linear bottlenecks}},
  author    = {Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
  booktitle = cvpr,
  year      = {2018},
  url       = {https://openaccess.thecvf.com/content_cvpr_2018/papers/Sandler_MobileNetV2_Inverted_Residuals_CVPR_2018_paper.pdf}
}

@article{sandler2018arxiv,
  author   = {M. Sandler and A. Howard and M. Zhu and A. Zhmoginov and L. Chen},
  title    = {{MobileNetV2: Inverted Residuals and Linear Bottlenecks}},
  journal  = arxiv,
  year     = {2018},
  volume   = {arXiv:1801.04381},
  url      = {http://arxiv.org/pdf/1801.04381v3},
  abstract = {In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3.   The MobileNetV2 architecture is based on an inverted residual structure where the input and output of the residual block are thin bottleneck layers opposite to traditional residual models which use expanded representations in the input an MobileNetV2 uses lightweight depthwise convolutions to filter features in the intermediate expansion layer. Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demonstrate that this improves performance and provide an intuition that led to this design. Finally, our approach allows decoupling of the input/output domains from the expressiveness of the transformation, which provides a convenient framework for further analysis. We measure our performance on Imagenet classification, COCO object detection, VOC image segmentation. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as the number of parameters}
}

@article{simonyan2014arxiv,
  author  = {K. Simonyan and A. Zisserman},
  title   = {{Very Deep Convolutional Networks for Large-Scale Image Recognition}},
  journal = arxiv,
  volume  = {arXiv:1409.1556},
  year    = 2014,
  url     = {http://arxiv.org/pdf/1409.1556}
}

@article{chen2016arxivb,
  author  = {T. Chen and B. Xu and C. Zhang and C. Guestrin},
  title   = {{Training Deep Nets with Sublinear Memory Cost}},
  journal = arxiv,
  volume  = {arXiv:1604.06174},
  year    = 2016,
  url     = {http://arxiv.org/pdf/1604.06174}
}

@article{ioffe2015arxiv,
  author  = {S. Ioffe and C. Szegedy},
  title   = {{Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}},
  journal = arxiv,
  volume  = {arXiv:1502.03167},
  year    = 2015,
  url     = {http://arxiv.org/pdf/1502.03167}
}

@article{lin2017arxiv,
  author  = {T.Y. Lin and P. Goyal and R.B. Girshick and K. He and P. Doll{\'{a}}r},
  title   = {{Focal Loss for Dense Object Detection}},
  journal = arxiv,
  volume  = {arXiv:1708.02002},
  year    = 2017,
  url     = {http://arxiv.org/pdf/1708.02002}
}

@article{kingma2014arxiv,
  author  = {D.P. Kingma and J.Ba},
  title   = {{Adam: {A} Method for Stochastic Optimization}},
  journal = arxiv,
  volume  = {arXiv:1412.6980},
  year    = 2014,
  url     = {http://arxiv.org/pdf/1412.6980}
}

@article{hinton2012arxiv,
  author  = {G.E. Hinton and N. Srivastava and A. Krizhevsky and I. Sutskever and R. Salakhutdinov},
  title   = {Improving neural networks by preventing co-adaptation of feature detectors},
  journal = arxiv,
  volume  = {arXiv:1207.0580},
  year    = 2012,
  url     = {http://arxiv.org/pdf/1207.0580}
}

@article{zhao2017arxiv,
  author  = {H. Zhao and X. Qi and X. Shen and J. Shi and J. Jia},
  title   = {{ICNet for Real-Time Semantic Segmentation on High-Resolution Images}},
  journal = arxiv,
  volume  = {arXiv:1704.08545},
  year    = 2017,
  url     = {http://arxiv.org/pdf/1704.08545}
}

@inproceedings{ros2016cvpr,
  author    = {G. Ros and L. Sellart and J. Materzynska and D. Vazquez and A. Lopez},
  title     = {The SYNTHIA Dataset: A Large Collection of Synthetic Images for Semantic Segmentation of Urban Scenes},
  booktitle = cvprold,
  year      = {2016},
  url       = {https://www.cv-foundation.org/openaccess/content_cvpr_2016/app/S14-11.pdf}
}

@inproceedings{cordts2016cvpr,
  author    = {M. Cordts and Omran, S. Mohamed and Ramos and T. Rehfeld and M. Enzweiler and R. Benenson and U. Franke and S. Roth and B. Schiele},
  title     = {The Cityscapes Dataset for Semantic Urban Scene Understanding},
  booktitle = cvprold,
  year      = {2016},
  url       = {http://openaccess.thecvf.com/content_cvpr_2016/papers/Cordts_The_Cityscapes_Dataset_CVPR_2016_paper.pdf}
}

@article{abadi2016arxiv,
  author   = {M. Abadi and A. Agarwal and P. Barham and E. Brevdo and Z. Chen and C. Citro and G.S. Corrado and A. Davis and J. Dean and M. Devin and S. Ghemawat and I. Goodfellow and A. Harp and G. Irving and M. Isard and Y. Jia and R. Jozefowicz and L. Kaiser and M. Kudlur and J. Levenberg and D. Mane and R. Monga and S. Moore and D. Murray and C. Olah and M. Schuster and J. Shlens and B. Steiner and I. Sutskever and K. Talwar and P. Tucker and V. Vanhoucke and V. Vasudevan and F. Viegas and O. Vinyals and P. Warden and M. Wattenberg and M. Wicke and Y. Yu and X. Zheng},
  title    = {{TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems}},
  journal  = arxiv,
  year     = {2016},
  volume   = {arXiv:1603.04467},
  url      = {http://arxiv.org/pdf/1603.04467v2},
  abstract = {TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.}
}

@article{jia2014arxiv,
  author   = {Y. Jia and E. Shelhamer and J. Donahue and S. Karayev and J. Long and R. Girshick and S. Guadarrama and T. Darrell},
  title    = {{Caffe: Convolutional Architecture for Fast Feature Embedding}},
  journal  = arxiv,
  year     = {2014},
  volume   = {arXiv:1408.5093},
  url      = {http://arxiv.org/pdf/1408.5093v1},
  abstract = {Caffe provides multimedia scientists and practitioners with a clean and modifiable framework for state-of-the-art deep learning algorithms and a collection of reference models. The framework is a BSD-licensed C++ library with Python and MATLAB bindings for training and deploying general-purpose convolutional neural networks and other deep models efficiently on commodity architectures. Caffe fits industry and internet-scale media needs by CUDA GPU computation, processing over 40 million images a day on a single K40 or Titan GPU ($\approx$ 2.5 ms per image). By separating model representation from actual implementation, Caffe allows experimentation and seamless switching among platforms for ease of development and deployment from prototyping machines to cloud environments. Caffe is maintained and developed by the Berkeley Vision and Learning Center (BVLC) with the help of an active community of contributors on GitHub. It powers ongoing research projects, large-scale industrial applications, and startup prototypes in vision, speech, and multimedia.}
}

@inproceedings{armagan2017jurse,
  author    = {A. Armagan and M. Hirzer and V. Lepetit},
  booktitle = {Proc. of the Joint Urban Remote Sensing Event (JURSE)},
  title     = {Semantic segmentation for 3D localization in urban environments},
  year      = {2017},
  url       = {https://pdfs.semanticscholar.org/0529/44a740699e8b4b2f8e52510afadbd417610f.pdf}
}

@inproceedings{poschmann2017ecmr,
  author    = {J. P{\"{o}}schmann and P. Neubert and S. Schubert and P. Protzel},
  booktitle = ecmr,
  title     = {Synthesized semantic views for mobile robot localization},
  year      = 2017
}

@article{atanasov2016ijrr,
  author   = {N. Atanasov and M. Zhu and K. Daniilidis and G.J. Pappas},
  title    = {Localization from semantic observations via the matrix permanent},
  journal  = ijrr,
  volume   = {35},
  number   = {1--3},
  pages    = {73--99},
  year     = {2016},
  abstract = { Most approaches to robot localization rely on low-level geometric features such as points, lines, and planes. In this paper, we use object recognition to obtain semantic information from the robotâs sensors and consider the task of localizing the robot within a prior map of landmarks, which are annotated with semantic labels. As object recognition algorithms miss detections and produce false alarms, correct data association between the detections and the landmarks on the map is central to the semantic localization problem. Instead of the traditional vector-based representation, we propose a sensor model, which encodes the semantic observations via random finite sets and enables a unified treatment of missed detections, false alarms, and data association. Our second contribution is to reduce the problem of computing the likelihood of a set-valued observation to the problem of computing a matrix permanent. It is this crucial transformation that allows us to solve the semantic localization problem with a polynomial-time approximation to the set-based Bayes filter. Finally, we address the active semantic localization problem, in which the observerâs trajectory is planned in order to improve the accuracy and efficiency of the localization process. The performance of our approach is demonstrated in simulation and in real environments using deformable-part-model-based object detectors. Robust global localization from semantic observations is demonstrated for a mobile robot, for the Project Tango phone, and on the KITTI visual odometry dataset. Comparisons are made with the traditional lidar-based geometric Monte Carlo localization. }
}

@article{sunderhauf2015arxiv,
  author  = {N. S{\"{u}}nderhauf and F. Dayoub and S. McMahon and B. Talbot and R. Schulz and P.I. Corke and G. Wyeth and B. Upcroft and M. Milford},
  title   = {{Place Categorization and Semantic Mapping on a Mobile Robot}},
  journal = arxiv,
  volume  = {arXiv:1507.02428},
  year    = {2015},
  url     = {http://arxiv.org/pdf/1507.02428}
}

@inproceedings{Khanna2015etfa,
  author    = {R. Khanna and M. M{\"{o}}ller and J. Pfeifer and F. Liebisch and A. Walter and R. Siegwart},
  title     = {Beyond point clouds - 3D mapping and field parameter measurements using UAVs},
  booktitle = {Proc.~of the IEEE Conf.~on Emerging Technologies Factory Automation (ETFA)},
  year      = {2015},
  keywords  = {computerised monitoring,crops,Precision agriculture,UAVs,crop height,point cloud processing},
  url       = {http://flourish-project.eu/fileadmin/user_upload/publications/2016_Pfeifer-UAV_data_interpretation.pdf}
}

@inproceedings{drouilly2015icra,
  author    = {R. Drouilly and P. Rives and B. Morisset},
  booktitle = icra,
  title     = {Semantic representation for navigation in large-scale environments},
  year      = {2015},
  keywords  = {mobile robots,path planning,mobile robots,object-based representation,semantic navigation representation,Buildings,Navigation,Observability,Planning,Robot sensing systems,Semantics},
  url       = {https://hal.inria.fr/hal-01122196/document}
}

@inproceedings{zhao2015icac,
  author    = {C. Zhao and H. Hu and D. Gu},
  booktitle = {Proc.~of the Intl.~Conf.~on Automation and Computing (ICAC)},
  title     = {Building a grid-point cloud-semantic map based on graph for the navigation of intelligent wheelchair},
  year      = {2015},
  keywords  = {human-robot interaction,wheelchairs,semantic labelling},
  url       = {http://cswww.sx.ac.uk/staff/hhu/Papers/ICAC-2015-222-228.pdf}
}


@inproceedings{blodow2011iros,
  author    = {N. Blodow and L. C. Goron and Z. C. Marton and D. Pangercic and T. R{\"{u}}hr and M. Tenorth and M. Beetz},
  booktitle = iros,
  title     = {Autonomous semantic mapping for robots performing everyday manipulation tasks in kitchen environments},
  year      = {2011},
  url       = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.391.4010&rep=rep1&type=pdf}
}

@article{schwarz2017ijrr,
  author   = {M. Schwarz and A. Milan and A.S. Periyasamy and S. Behnke},
  title    = {{RGB-D object detection and semantic segmentation for autonomous manipulation in clutter}},
  journal  = ijrr,
  volume   = {37},
  number   = {4--5},
  pages    = {437--451},
  year     = {2017},
  url      = {https://www.researchgate.net/profile/Sven_Behnke/publication/317183054_RGB-D_Object_Detection_and_Semantic_Segmentation_for_Autonomous_Manipulation_in_Clutter/links/59eb6315aca272cddddef67d/RGB-D-Object-Detection-and-Semantic-Segmentation-for-Autonomous-Manipulation-in-Clutter.pdf},
  abstract = { Autonomous robotic manipulation in clutter is challenging. A large variety of objects must be perceived in complex scenes, where they are partially occluded and embedded among many distractors, often in restricted spaces. To tackle these challenges, we developed a deep-learning approach that combines object detection and semantic segmentation. The manipulation scenes are captured with RGB-D cameras, for which we developed a depth fusion method. Employing pretrained features makes learning from small annotated robotic datasets possible. We evaluate our approach on two challenging datasets: one captured for the Amazon Picking Challenge 2016, where our team NimbRo came in second in the Stowing and third in the Picking task; and one captured in disaster-response scenarios. The experiments show that object detection and semantic segmentation complement each other and can be combined to yield reliable object perception. }
}

@article{goyal2017arxiv,
  author   = {P. Goyal and P. Dollar and R. Girshick and P. Noordhuis and L. Wesolowski and A. Kyrola and A. Tulloch and Y. Jia and K. He},
  title    = {{Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour}},
  journal  = arxiv,
  year     = {2017},
  volume   = {arXiv:1706.02677v1},
  url      = {http://arxiv.org/pdf/1706.02677v1},
  abstract = {Deep learning thrives with large neural networks and large datasets. However, larger networks and larger datasets result in longer training times that impede research and development progress. Distributed synchronous SGD offers a potential solution to this problem by dividing SGD minibatches over a pool of parallel workers. Yet to make this scheme efficient, the per-worker workload must be large, which implies nontrivial growth in the SGD minibatch size. In this paper, we empirically show that on the ImageNet dataset large minibatches cause optimization difficulties, but when these are addressed the trained networks exhibit good generalization. Specifically, we show no loss of accuracy when training with large minibatch sizes up to 8192 images. To achieve this result, we adopt a linear scaling rule for adjusting learning rates as a function of minibatch size and develop a new warmup scheme that overcomes optimization challenges early in training. With these simple techniques, our Caffe2-based system trains ResNet-50 with a minibatch size of 8192 on 256 GPUs in one hour, while matching small minibatch accuracy. Using commodity hardware, our implementation achieves ~90\% scaling efficiency when moving from 8 to 256 GPUs. This system enables us to train visual recognition models on internet-scale data with high efficiency.}
}

@inproceedings{balntas2017cvpr,
  author    = {V. Balntas and K. Lenc and A. Vedaldi and K. Mikolajczyk},
  title     = {{HPatches: A Benchmark and Evaluation of Handcrafted and Learned Local Descriptors}},
  booktitle = cvprold,
  year      = 2017,
  abstract  = {In this paper, we propose a novel benchmark for evaluating local image descriptors. We demonstrate that the existing datasets and evaluation protocols do not specify unambiguously all aspects of evaluation, leading to ambiguities and inconsistencies in results reported in the literature. Furthermore, these datasets are nearly saturated due to the recent improvements in local descriptors obtained by learning them from large annotated datasets. Therefore, we introduce a new large dataset suitable for training and testing modern descriptors, together with strictly defined evaluation protocols in several tasks such as matching, retrieval and classification. This allows for more realistic, and thus more reliable comparisons in different application scenarios. We evaluate the performance of several state-of-the-art descriptors and analyse their properties. We show that a simple normalisation of traditional hand-crafted descriptors can boost their performance to the level of deep learning based descriptors within a realistic benchmarks evaluation.},
  url       = {proceedings:balntas2017cvpr.pdf}
}

@inproceedings{arandjelovic2016cvpr-ncaf,
  author    = {R. Arandjelovic and P. Gronat and A. Torii and T. Pajdla and J. Sivic},
  title     = {{NetVLAD: CNN Architecture for Weakly Supervised Place Recognition}},
  booktitle = cvprold,
  year      = 2016,
  abstract  = {We tackle the problem of large scale visual place recognition, where the task is to quickly and accurately recognize the location of a given query photograph. We present the following three principal contributions. First, we develop a convolutional neural network (CNN) architecture that is trainable in an end-to-end manner directly for the place recognition task. The main component of this architecture, NetVLAD, is a new generalized VLAD layer, inspired by the "Vector of Locally Aggregated Descriptors" image representation commonly used in image retrieval. The layer is readily pluggable into any CNN architecture and amenable to training via backpropagation. Second, we develop a training procedure, based on a new weakly supervised ranking loss, to learn parameters of the architecture in an end-to-end manner from images depicting the same places over time downloaded from Google Street View Time Machine. Finally, we show that the proposed architecture significantly outperforms non-learnt image representations and off-the-shelf CNN descriptors on two challenging place recognition benchmarks, and improves over current state-of-the-art compact image representations on standard image retrieval benchmarks.},
  url       = {proceedings:arandjelovic2016cvpr-ncaf.pdf}
}

@inproceedings{canelhas2013iros,
  title     = {{SDF Tracker: A Parallel Algorithm for On-Line Pose Estimation and Scene Reconstruction from Depth Images}},
  author    = {D. Canelhas and T. Stoyanov and A. Lilienthal},
  booktitle = iros,
  year      = {2013},
  url       = {http://130.243.105.49/Research/mro/publications/2013/Canelhas_etal_2013-IROS2013-SDF-Tracker.pdf}
}

@article{kahler2015tvcg,
  title   = {{Very High Frame Rate Volumetric Integration of Depth Images on Mobile Devices}},
  author  = {O. K{\"a}hler and V.A. Prisacariu and C.Y. Ren and X. Sun and P. Torr and D. Murray},
  journal = tvcg,
  volume  = {21},
  number  = {11},
  pages   = {1241--1250},
  year    = {2015},
  url     = {http://www.robots.ox.ac.uk/~victor/infinitam/files/ismar15infinitam.pdf}
}

@article{slavcheva2018ijcv,
  title   = {{SDF-2-SDF Registration for Real-Time 3D Reconstruction from RGB-D Data}},
  author  = {M. Slavcheva and W. Kehl and N. Navab and S. Ilic},
  journal = ijcv,
  pages   = {615--636},
  volume  = {126},
  year    = {2018},
  url     = {https://link.springer.com/content/pdf/10.1007/s11263-017-1057-z.pdf}
}

@inproceedings{curless1996siggraph,
  title     = {{A Volumetric Method for Building Complex Models from Range Images}},
  author    = {B. Curless and M. Levoy},
  booktitle = siggraph,
  year      = {1996},
  url       = {http://papers.cumincad.org/data/works/att/2ca3.content.pdf}
}

@inproceedings{bylow2013rss,
  title     = {{Real-Time Camera Tracking and 3D Reconstruction Using Signed Distance Functions.}},
  author    = {E. Bylow and J. Sturm and C. Kerl and F. Kahl and D. Cremers},
  booktitle = rss,
  year      = {2013},
  url       = {http://portal.research.lu.se/portal/files/3674772/3917437.pdf}
}

@inproceedings{teschner2003vmv,
  title     = {{Optimized Spatial Hashing for Collision Detection of Deformable Objects.}},
  author    = {M. Teschner and B. Heidelberger and M. M{\"u}ller and D. Pomerantes and M.H. Gross},
  booktitle = vmv,
  year      = {2003},
  url       = {http://www.matthias-mueller-fischer.ch/publications/tetraederCollision.pdf}
}

@inproceedings{lorensen1987siggraph,
  title     = {{Marching Cubes: a High Resolution 3D Surface Construction Algorithm}},
  author    = {W.E. Lorensen and H.E. Cline},
  booktitle = siggraph,
  year      = {1987},
  url       = {http://fab.cba.mit.edu/classes/S62.12/docs/Lorensen_marching_cubes.pdf}
}

@inproceedings{sturm2012iros,
  author    = {J. Sturm and N. Engelhard and F. Endres and W. Burgard and D. Cremers},
  title     = {{A Benchmark for the Evaluation of RGB-D SLAM Systems}},
  booktitle = iros,
  year      = {2012},
  url       = {https://vision.in.tum.de/_media/spezial/bib/sturm12iros.pdf}
}


@inproceedings{vempati2017iros,
  author    = {A.S. Vempati and I. Gilitschenski and J. Nieto and P. Beardsley and R. Siegwart},
  title     = {{Onboard Real-time Dense Reconstruction of Large-scale Environments for UAV}},
  booktitle = iros,
  year      = 2017,
  keywords  = {Aerial Systems: Perception and Autonomy, RGB-D Perception, SLAM},
  abstract  = {In this paper, we propose a GPU parallelized SLAM system capable of using photometric and inertial data together with depth data from an active RGB-D sensor to build accurate dense 3D maps of indoor environments. We describe several extensions to existing dense SLAM techniques that allow us to operate in real-time onboard memory constrained robotic platforms. Our primary contribution is a memory management algorithm that scales to large scenes without being limited by GPU memory resources. Moreover, by integrating a visualinertial odometry system, we robustly track the camera pose even on an agile platform such as a quadrotor UAV. Our robust camera tracking framework can deal with fast camera motions and varying environments by relying on depth, color and inertial motion cues. Global consistency is achieved via regular checking for loop closures in conjunction with a pose graph, as a basis for corrective deformation of the 3D map. Our efficient SLAM system is capable of producing highly dense meshes up to 5mm resolution at rates close to 60Hz fully onboard a UAV. Experimental validations both in simulation and on a real-world platform, show that our approach is fast, more robust and more memory efficient than state-of-the-art techniques, while obtaining better or comparable accuracy.}
}

@inproceedings{kaehler2016icra,
  author    = {O. K{\"a}hler and V.A. Prisacariu and J. Valentin and D. Murray},
  title     = {{Hierarchical Voxel Block Hashing for Efficient Integration of Depth Images}},
  booktitle = icra,
  year      = 2016,
  keywords  = {SLAM, Mapping, RGB-D Perception},
  abstract  = {Many modern 3D reconstruction methods accumulate information volumetrically using truncated signed distance functions. While this usually imposes a regular grid with fixed voxel size, not all parts of a scene necessarily need to be represented at the same level of detail. For example, a flat table needs less detail than a highly structured keyboard on it. We introduce a novel representation for the volumetric 3D data that uses hash functions rather than trees for accessing individual blocks of the scene, but which still provides different resolution levels. We show that our data structure provides efficient access and manipulation functions that can be very well parallelised, and also describe an automatic way of choosing appropriate resolutions for different parts of the scene. We embed the novel representation in a system for simultaneous localisation and mapping from RGB-D imagery and also investigate the implications of the irregular grid on interpolation routines. Finally we evaluate our system in experiments, demonstrating state-of-the-art representation accuracy at typical framerates around 100 Hz, along with 40\% memory savings.}
}

@inproceedings{steinbruecker2014icra,
  author    = {F. Steinbr{\"u}cker and J. Sturm and D. Cremers},
  title     = {{Volumetric 3D Mapping in Real-Time on a CPU}},
  booktitle = icra,
  year      = {2014},
  keywords  = {RGB-D, SLAM, point cloud}
}

@inproceedings{pinheiro2016eccv,
  author    = {P.O. Pinheiro and T. Lin and R. Collobert and P. Doll\'ar},
  title     = {{Learning to Refine Object Segments}},
  booktitle = eccv,
  year      = {2016}
}

@inproceedings{lin2014eccv,
  author    = {T. Lin and M. Maire and S. Belongie and J. Hays and P. Perona and D. Ramanan and P. Doll\'ar and C. Lawrence Zitnick},
  title     = {{Microsoft COCO: Common Objects in Context}},
  booktitle = eccv,
  year      = {2014},
  abstract  = {We present a new dataset with the goal of advancing the state-of-the-art in
               object recognition by placing the question of object recognition in the context of the
               broader question of scene understanding. This is achieved by gathering images of complex
               everyday scenes containing common objects in their natural context. Objects are labeled
               using per-instance segmentations to aid in precise object localization. Our dataset contains
               photos of 91 objects types that would be easily recognizable by a 4 year old. With a total
               of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon
               extensive crowd worker involvement via novel user interfaces for category detection, instance
               spotting and instance segmentation. We present a detailed statistical analysis of the dataset
               in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis
               for bounding box and segmentation detection results using a Deformable Parts Model. },
  url       = {https://arxiv.org/pdf/1405.0312}
}

@inproceedings{szegedy2015cvpr,
  author    = {C. Szegedy and W. Liu and Y. Jia and P. Sermanet and S. Reed and D. Anguelov and D. Erhan and V. Vanhoucke and A. Rabinovich},
  title     = {{Going Deeper With Convolutions}},
  booktitle = cvprold,
  year      = 2015,
  abstract  = {We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation of this architecture, GoogLeNet, a 22 layers deep network, was used to assess its  quality in the context of object detection and classification.},
  url       = {proceedings: szegedy2015cvpr.pdf}
}

@inproceedings{paszke2017neurips,
  title     = {{Automatic differentiation in PyTorch}},
  author    = {A. Paszke and S. Gross and S. Chintala and G. Chanan and E. Yang and Z. DeVito and Z. Lin and A. Desmaison and L. Antiga and A. Lerer},
  booktitle = neurips,
  year      = {2017}
}

@article{szegedy2015arxiv,
  author   = {C. Szegedy and V. Vanhoucke and S. Ioffe and J. Shlens and Z. Wojna},
  title    = {{Rethinking the Inception Architecture for Computer Vision}},
  journal  = arxiv,
  year     = 2015,
  volume   = {arXiv:1512.00567},
  url      = {http://arxiv.org/pdf/1512.00567v3},
  abstract = {Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we explore ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21.2\% top-1 and 5.6\% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5\% top-5 error on the validation set (3.6\% error on the test set) and 17.3\% top-1 error on the validation set.}
}


@techreport{forstner2009etrims,
  author    = {F. Kor{\v c} and W. F{\" o}rstner},
  title     = {{eTRIMS} {I}mage {D}atabase for Interpreting Images of Man-Made Scenes},
  year      = {2009},
  institute = {Dept. of Photogrammetry, University of Bonn},
  url       = {http://www.ipb.uni-bonn.de/projects/etrims_db/}
}

@inproceedings{bai2017cvpr,
  author    = {M. Bai and R. Urtasun},
  title     = {{Deep Watershed Transform for Instance Segmentation}},
  booktitle = cvprold,
  year      = 2017,
  url       = {https://arxiv.org/pdf/1611.08303.pdf}
}

@inproceedings{debrabandere2017cvprws,
  author    = {B. De Brabandere and D. Neven and L. Van Gool},
  title     = {{Semantic Instance Segmentation with a Discriminative Loss Function}},
  booktitle = {Deep Learning for Robotic Vision workshop, IEEE Conf.~on Computer Vision and Pattern Recognition (CVPR)},
  year      = 2017,
  url       = {https://arxiv.org/pdf/1708.02551.pdf}
}

@inproceedings{ren2015nips,
  title     = {{Faster R-CNN: Towards real-time object detection with region proposal networks}},
  author    = {Shaoqing Ren and Kaiming He and Ross Girshick and Jian Sun},
  booktitle = nips,
  year      = {2015},
  url       = {https://papers.nips.cc/paper/2015/file/14bfa6bb14875e45bba028a21ed38046-Paper.pdf}
}

@article{ren2015arxiv,
  author   = {C.Y. Ren and V.A. Prisacariu and I.D. Reid},
  title    = {{gSLICr: SLIC superpixels at over 250Hz}},
  journal  = arxiv,
  year     = 2015,
  volume   = {arXiv:1509.04232},
  url      = {http://arxiv.org/pdf/1509.04232v1},
  abstract = {We introduce a parallel GPU implementation of the Simple Linear Iterative Clustering (SLIC) superpixel segmentation. Using a single graphic card, our implementation achieves speedups of up to $83\times$ from the standard sequential implementation. Our implementation is fully compatible with the standard sequential implementation and the software is now available online and is open source.}
}

@article{schunck2021plosone,
  author   = {D. Schunck and F. Magistri and R.A. Rosu and
              A. Corneli{\ss}en and N. Chebrolu and S. Paulus and
              J. L\'eon and S. Behnke and C. Stachniss and
              H. Kuhlmann and L. Klingbeil},
  journal  = plosone,
  number   = {8},
  pages    = {1--18},
  title    = {{Pheno4D: A spatio-temporal dataset of maize and
              tomato plant point clouds for phenotyping and
              advanced plant analysis }},
  volume   = {16},
  year     = {2021},
  abstract = {Understanding the growth and development of
              individual plants is of central importance in modern
              agriculture, crop breeding, and crop science. To this
              end, using 3D data for plant analysis has gained
              attention over the last years. High-resolution point
              clouds offer the potential to derive a variety of
              plant traits, such as plant height, biomass, as well
              as the number and size of relevant plant organs.
              Periodically scanning the plants even allows for
              performing spatio-temporal growth analysis. However,
              highly accurate 3D point clouds from plants recorded
              at different growth stages are rare, and acquiring
              this kind of data is costly. Besides, advanced plant
              analysis methods from machine learning require
              annotated training data and thus generate intense
              manual labor before being able to perform an
              analysis. To address these issues, we present with
              this dataset paper a multi-temporal dataset featuring
              high-resolution registered point clouds of maize and
              tomato plants, which we manually labeled for computer
              vision tasks, such as for instance segmentation and
              3D reconstruction, providing approximately 260
              million labeled 3D points. To highlight the usability
              of the data and to provide baselines for other
              researchers, we show a variety of applications
              ranging from point cloud segmentation to non-rigid
              registration and surface reconstruction. We believe
              that our dataset will help to develop new algorithms
              to advance the research for plant phenotyping, 3D
              reconstruction, non-rigid registration, and deep
              learning on raw point clouds. The dataset is freely
              accessible at
              https://www.ipb.uni-bonn.de/data/pheno4d/.},
  url      = {https://journals.plos.org/plosone/article/file?id=10.1371/
              journal.pone.0256340&type=printable}
}



@article{neven2017arxiv,
  author   = {D. Neven and B.D. Brabandere and S. Georgoulis and M. Proesmans and L.V. Gool},
  title    = {{Fast Scene Understanding for Autonomous Driving}},
  journal  = arxiv,
  year     = 2017,
  volume   = {arXiv:1708.02550},
  url      = {http://arxiv.org/pdf/1708.02550v1},
  abstract = {Most approaches for instance-aware semantic labeling traditionally focus on accuracy. Other aspects like runtime and memory footprint are arguably as important for real-time applications such as autonomous driving. Motivated by this observation and inspired by recent works that tackle multiple tasks with a single integrated architecture, in this paper we present a real-time efficient implementation based on ENet that solves three autonomous driving related tasks at once: semantic scene segmentation, instance segmentation and monocular depth estimation. Our approach builds upon a branched ENet architecture with a shared encoder but different decoder branches for each of the three tasks. The presented method can run at 21 fps at a resolution of 1024x512 on the Cityscapes dataset without sacrificing accuracy compared to running each task separately.}
}

@inproceedings{liu2018cvpr,
  author    = {S. Liu and L. Qi and H. Qin and J. Shi and J. Jia},
  title     = {{Path Aggregation Network for Instance Segmentation}},
  booktitle = cvprold,
  year      = 2018,
  url       = {http://arxiv.org/abs/1803.01534}
}

@article{fathi2017arxiv,
  author   = {A. Fathi and Z. Wojna and V. Rathod and P. Wang and H.O. Song and S. Guadarrama and K.P. Murphy},
  title    = {{Semantic Instance Segmentation via Deep Metric Learning}},
  journal  = arxiv,
  year     = 2017,
  volume   = {arXiv:1703.10277},
  url      = {http://arxiv.org/pdf/1703.10277v1},
  abstract = {We propose a new method for semantic instance segmentation, by first computing how likely two pixels are to belong to the same object, and then by grouping similar pixels together. Our similarity metric is based on a deep, fully convolutional embedding model. Our grouping method is based on selecting all points that are sufficiently similar to a set of "seed points", chosen from a deep, fully convolutional scoring model. We show competitive results on the Pascal VOC instance segmentation benchmark.}
}


@article{payer2018arxiv,
  author   = {C. Payer and D. Å tern and T. Neff and H. Bischof and M. Urschler},
  title    = {{Instance Segmentation and Tracking with Cosine Embeddings and Recurrent Hourglass Networks}},
  journal  = arxiv,
  year     = 2018,
  volume   = {arXiv:1806.02070},
  url      = {http://arxiv.org/pdf/1806.02070v3},
  abstract = {Different to semantic segmentation, instance segmentation assigns unique labels to each individual instance of the same class. In this work, we propose a novel recurrent fully convolutional network architecture for tracking such instance segmentations over time. The network architecture incorporates convolutional gated recurrent units (ConvGRU) into a stacked hourglass network to utilize temporal video information. Furthermore, we train the network with a novel embedding loss based on cosine similarities, such that the network predicts unique embeddings for every instance throughout videos. Afterwards, these embeddings are clustered among subsequent video frames to create the final tracked instance segmentations. We evaluate the recurrent hourglass network by segmenting left ventricles in MR videos of the heart, where it outperforms a network that does not incorporate video information. Furthermore, we show applicability of the cosine embedding loss for segmenting leaf instances on still images of plants. Finally, we evaluate the framework for instance segmentation and tracking on six datasets of the ISBI celltracking challenge, where it shows state-of-the-art performance.}
}


@inproceedings{ren2017cvpr,
  author    = {M. Ren and R.S. Zemel},
  title     = {{End-to-End Instance Segmentation and Counting with Recurrent Attention}},
  booktitle = cvprold,
  year      = 2017
}

@inproceedings{romera2016eccv,
  author    = {B. Romera{-}Paredes and P.H.S. Torr},
  title     = {{Recurrent Instance Segmentation}},
  booktitle = eccv,
  year      = 2016
}

@article{redmon2018arxiv,
  author   = {J. Redmon and A. Farhadi},
  title    = {{YOLOv3: An Incremental Improvement}},
  journal  = arxiv,
  year     = 2018,
  volume   = {arXiv:1804.02767},
  url      = {http://arxiv.org/pdf/1804.02767v1},
  abstract = {We present some updates to YOLO! We made a bunch of little design changes to make it better. We also trained this new network that's pretty swell. It's a little bigger than last time but more accurate. It's still fast though, don't worry. At 320x320 YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD but three times faster. When we look at the old .5 IOU mAP detection metric YOLOv3 is quite good. It achieves 57.9 mAP@50 in 51 ms on a Titan X, compared to 57.5 mAP@50 in 198 ms by RetinaNet, similar performance but 3.8x faster. As always, all the code is online at https://pjreddie.com/yolo/}
}

@article{magistri2022ral-iros,
  author  = {Federico Magistri and Elias Marks and Sumanth Nagulavancha and Ignacio Vizzo and Thomas L{\"a}be and Jens Behley and Michael Halstead and Chris McCool and Cyrill Stachniss},
  title   = {{Contrastive 3D Shape Completion and Reconstruction for Agricultural Robots using RGB-D Frames}},
  journal = ral,
  year    = {2022},
  volume  = {7},
  number  = {4},
  pages   = {10120--10127}
}

@article{achanta2012pami,
  author  = {R. Achanta and A. Shaji and K. Smith and A. Lucchi and P. Fua and S. S\"usstrunk},
  title   = {{SLIC Superpixels Compared to State-of-the-art Superpixel Methods}},
  journal = pami,
  volume  = {34},
  number  = {11},
  pages   = {2274--2282},
  year    = 2012
}

@inproceedings{deng2009cvpr,
  author    = {J. Deng and W. Dong and R. Socher and L. Li and Kai Li and Li Fei-Fei},
  booktitle = cvprold,
  title     = {ImageNet: A large-scale hierarchical image database},
  year      = {2009},
  keywords  = {computer vision;image resolution;image retrieval;Internet;multimedia computing;ontologies (artificial intelligence);trees (mathematics);very large databases;visual databases;ImageNet database;large-scale hierarchical image database;Internet;image retrieval;multimedia data;large-scale ontology;wordNet structure;image resolution;subtree;computer vision;Large-scale systems;Image databases;Explosions;Internet;Robustness;Information retrieval;Image retrieval;Multimedia databases;Ontologies;Spine},
  url       = {http://www.image-net.org/papers/imagenet_cvpr09.pdf}
}

@article{chen2017arxiv,
  author   = {L. Chen and G. Papandreou and F. Schroff and H. Adam},
  title    = {{Rethinking Atrous Convolution for Semantic Image Segmentation}},
  journal  = arxiv,
  year     = 2017,
  volume   = {arXiv:1706.05587},
  url      = {http://arxiv.org/pdf/1706.05587v3},
  abstract = {In this work, we revisit atrous convolution, a powerful tool to explicitly adjust filter's field-of-view as well as control the resolution of feature responses computed by Deep Convolutional Neural Networks, in the application of semantic image segmentation. To handle the problem of segmenting objects at multiple scales, we design modules which employ atrous convolution in cascade or in parallel to capture multi-scale context by adopting multiple atrous rates. Furthermore, we propose to augment our previously proposed Atrous Spatial Pyramid Pooling module, which probes convolutional features at multiple scales, with image-level features encoding global context and further boost performance. We also elaborate on implementation details and share our experience on training our system. The proposed `DeepLabv3' system significantly improves over our previous DeepLab versions without DenseCRF post-processing and attains comparable performance with other state-of-art models on the PASCAL VOC 2012 semantic image segmentation benchmark.}
}

@article{mccormac2018arxiv,
  author   = {J. McCormac and R. Clark and M. Bloesch and A.J. Davison and S. Leutenegger},
  title    = {{Fusion++: Volumetric Object-Level SLAM}},
  journal  = arxiv,
  year     = 2018,
  volume   = {arXiv:1808.08378},
  url      = {http://arxiv.org/pdf/1808.08378v2},
  abstract = {We propose an online object-level SLAM system which builds a persistent and accurate 3D graph map of arbitrary reconstructed objects. As an RGB-D camera browses a cluttered indoor scene, Mask-RCNN instance segmentations are used to initialise compact per-object Truncated Signed Distance Function (TSDF) reconstructions with object size-dependent resolutions and a novel 3D foreground mask. Reconstructed objects are stored in an optimisable 6DoF pose graph which is our only persistent map representation. Objects are incrementally refined via depth fusion, and are used for tracking, relocalisation and loop closure detection. Loop closures cause adjustments in the relative pose estimates of object instances, but no intra-object warping. Each object also carries semantic information which is refined over time and an existence probability to account for spurious instance predictions. We demonstrate our approach on a hand-held RGB-D sequence from a cluttered office scene with a large number and variety of object instances, highlighting how the system closes loops and makes good use of existing objects on repeated loops. We quantitatively evaluate the trajectory error of our system against a baseline approach on the RGB-D SLAM benchmark, and qualitatively compare reconstruction quality of discovered objects on the YCB video dataset. Performance evaluation shows our approach is highly memory efficient and runs online at 4-8Hz (excluding relocalisation) despite not being optimised at the software level.}
}

@inproceedings{lang2007rss,
  title     = {{Adaptive Non-Stationary Kernel Regression for Terrain Modeling}},
  author    = {T. Lang and C. Plagemann and W. Burgard},
  booktitle = rss,
  year      = {2007}
}

@inproceedings{viseras2017ros,
  title     = {{Online Information Gathering Using Sampling-based Planners and GPs: an Information Theoretic Approach}},
  author    = {A. Viseras and D. Shutin and L. Merino},
  year      = 2017,
  booktitle = iros
}

@inproceedings{marchant2014icra,
  title     = {{Bayesian Optimisation For Informative Continuous Path Planning}},
  author    = {R. Marchant and F. Ramos},
  booktitle = icra,
  year      = {2014}
}

@inproceedings{cunningham2017icra,
  title     = {{Locally-Adaptive Slip Prediction for Planetary Rovers Using Gaussian Processes}},
  author    = {C. Cunningham and M. Ono and I. Nesnas and J. Yen and W. Whittaker},
  booktitle = icra,
  year      = {2017}
}

@inproceedings{tresp2001nips,
  title     = {{Mixtures of Gaussian processes}},
  author    = {V. Tresp},
  booktitle = nips,
  year      = 2001
}

@inproceedings{rasmussen2002nips,
  title     = {{Infinite Mixtures of Gaussian Process Experts}},
  author    = {C. E. Rasmussen and Z. Ghahramani},
  booktitle = nips,
  year      = 2002
}

@inproceedings{souza2014icra,
  title     = {{Bayesian Optimisation for Active Perception and Smooth Navigation}},
  author    = {J. R. Souza and R. Marchant and L. Ott and D. F. Wolf and F. Ramos},
  booktitle = icra,
  year      = 2014
}

@inproceedings{wolf2005icra,
  title     = {{Autonomous Terrain Mapping and Classification Using Hidden Markov Models}},
  author    = {D. Wolf and G. Sukhatme and D. Fox and W. Burgard},
  booktitle = icra,
  year      = 2005
}

@inproceedings{suger2016iros,
  title     = {{Terrain-Adaptive Obstacle Detection}},
  author    = {B. Suger and B. Steder and W. Burgard},
  booktitle = iros,
  year      = 2016
}

@inproceedings{snelson2006nips,
  title     = {Sparse Gaussian Processes Using Pseudo-Inputs},
  author    = {E. Snelson and Z. Ghahramani},
  booktitle = nips,
  year      = 2006
}

@inproceedings{senanayake2017icra,
  title     = {{Learning Highly Dynamic Environments with Stochastic Variational Inference}},
  author    = {R. Senanayake and S. O'Callaghan and F. Ramos},
  booktitle = icra,
  year      = 2017
}

@article{ocallaghan2012ijrr,
  title   = {{Gaussian Process Occupancy Maps}},
  author  = {S. O'Callaghan and F. Ramos},
  journal = ijrr,
  volume  = {31},
  number  = {1},
  pages   = {42--62},
  year    = 2012
}

@inproceedings{ocallaghan2016iser,
  title     = {{Gaussian Process Occupancy Maps for Dynamic Environments}},
  author    = {S. O'Callaghan and F. Ramos},
  booktitle = iser,
  year      = 2016
}

@article{krause2008jmlr,
  title   = {{Near-Optimal Sensor Placements in Gaussian Processes: Theory, Efficient Algorithms and Empirical Studies}},
  author  = {A. Krause and A. Singh and C. Guestrin},
  journal = jmlr,
  volume  = {9},
  pages   = {235--284},
  year    = 2008
}

@inproceedings{srinivas2010icml,
  title     = {Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design},
  author    = {N. Srinivas and A. Krause and S. Kakade and M. Seeger},
  booktitle = icml,
  year      = 2010
}


@inproceedings{dugas2001nips,
  title     = {Incorporating Second-Order Functional Knowledge for Better Option Pricing},
  author    = {C. Dugas and Y. Bengio and F. B{\'e}lisle and C. Nadeau and R. Garcia},
  booktitle = nips,
  year      = 2001
}

@inproceedings{cox1992smc,
  title     = {A Statistical Method for Global Optimization},
  author    = {D. Cox and S. John},
  booktitle = smc,
  year      = 1992
}

@inproceedings{nair2010icml,
  title     = {Rectified Linear Units Improve Restricted Boltzmann Machines},
  author    = {V. Nair and G. Hinton},
  booktitle = icml,
  year      = 2010
}

@inproceedings{ondruvska2015icra,
  title     = {Scheduled Perception for Energy-Efficient Path Following},
  author    = {P. Ondr{\'u}{\v{s}}ka and C. Gur{\u{a}}u and L. Marchegiani and C. Tong and I. Posner},
  booktitle = icra,
  year      = 2015
}

@inproceedings{bartlett2016iros,
  title     = {Enabling Intelligent Energy Management for Robots Using Publicly Available Maps},
  author    = {O. Bartlett and C. Gurau and L. Marchegiani and I. Posner},
  booktitle = iros,
  year      = 2016
}

@inproceedings{weiss2012icra,
  title     = {Real-Time Onboard Visual-Inertial State Estimation and Self-Calibration of MAVs in Unknown Environments},
  author    = {S. Weiss and M. Achtelik and S. Lynen and M. Chli and R. Siegwart},
  booktitle = icra,
  year      = 2012
}

@book{farrell2007gnssbook,
  title     = {GNSS Aided Navigation \& Tracking: Inertially Augmented Or Autonomous},
  author    = {J. Farrell},
  year      = {2007},
  publisher = {American Literary Press Baltimore, MD}
}

@inproceedings{murphy2010icra,
  title     = {Planning Most-Likely Paths from Overhead Imagery},
  author    = {E. Murphy and P. Newman},
  booktitle = icra,
  year      = 2010
}

@inproceedings{silver2008rss,
  title     = {{High Performance Outdoor Navigation from Overhead Data Using Imitation Learning}},
  author    = {D. Silver and J. Bagnell and A. Stentz},
  booktitle = rss,
  year      = 2008
}

@article{fukunaga1975it,
  title   = {The Estimation of the Gradient of a Density Function, With Applications in Pattern Recognition},
  author  = {K. Fukunaga and L. Hostetler},
  journal = {IEEE Trans. on Information Theory},
  volume  = {21},
  number  = {1},
  pages   = {32--40},
  year    = 1975
}

@book{neal2012bayesianlearning,
  title     = {Bayesian Learning for Neural Networks},
  author    = {R.M. Neal},
  year      = 2012,
  publisher = {Springer}
}

@inproceedings{arandjelovic2016cvpr,
  title     = {NetVLAD: CNN architecture for weakly supervised place recognition},
  author    = {Arandjelovic, R. and Gronat, P. and Torii, A. and Pajdla, T. and Sivic, J.},
  booktitle = cvprold,
  year      = {2016}
}

@inproceedings{glover2012icra,
  author    = {Glover, A. and Maddern, W. and Warren, M. and Reid, S. and Milford, M. and Wyeth, G.},
  booktitle = icra,
  title     = {OpenFABMAP: An open source toolbox for appearance-based loop closure detection},
  year      = {2012}
}

@inproceedings{majdik2013iros,
  author    = {Majdik, A.L. and Albers-Schoenberg, Y. and Scaramuzza, D.},
  booktitle = iros,
  title     = {MAV urban localization from Google street view data},
  year      = {2013}
}

@inproceedings{badino2011ivs,
  author    = {Badino, H. and Huber, D. and Kanade, T.},
  booktitle = iv,
  title     = {Visual topometric localization},
  year      = {2011}
}

@article{slaughter2008cea,
  title   = {Autonomous robotic weed control systems: A review },
  journal = cea,
  volume  = {61},
  number  = {1},
  pages   = {63--78},
  year    = {2008},
  author  = {D.C. Slaughter and D.K. Giles and D. Downey}
}

@phdthesis{nieuwenhuizen2009phd,
  title  = {Automated detection and control of volunteer potato plants},
  author = {A.T. Nieuwenhuizen},
  school = {Wageningen University},
  year   = 2009
}


@inproceedings{huang2017cvpr,
  author    = {G. Huang and Z. Liu and L.v.d. Maaten and K. Q. Weinberger},
  title     = {{Densely Connected Convolutional Networks}},
  booktitle = cvprold,
  year      = 2017,
  abstract  = {Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections--one between each layer and its subsequent layer--our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR- 10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less memory and computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet.}
}

@article{jegou2016arxiv,
  author   = {S. JÃ©gou and M. Drozdzal and D. Vazquez and A. Romero and Y. Bengio},
  title    = {{The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for Semantic Segmentation}},
  journal  = arxiv,
  year     = 2016,
  volume   = {arXiv:1611.09326},
  url      = {http://arxiv.org/pdf/1611.09326v3},
  abstract = {State-of-the-art approaches for semantic image segmentation are built on Convolutional Neural Networks (CNNs). The typical segmentation architecture is composed of (a) a downsampling path responsible for extracting coarse semantic features, followed by (b) an upsampling path trained to recover the input image resolution at the output of the model and, optionally, (c) a post-processing module (e.g. Conditional Random Fields) to refine the model predictions.   Recently, a new CNN architecture, Densely Connected Convolutional Networks (DenseNets), has shown excellent results on image classification tasks. The idea of DenseNets is based on the observation that if each layer is directly connected to every other layer in a feed-forward fashion then the network will be more accurate and easier to train.   In this paper, we extend DenseNets to deal with the problem of semantic segmentation. We achieve state-of-the-art results on urban scene benchmark datasets such as CamVid and Gatech, without any further post-processing module nor pretraining. Moreover, due to smart construction of the model, our approach has much less parameters than currently published best entries for these datasets.   Code to reproduce the experiments is available here : https://github.com/SimJeg/FC-DenseNet/blob/master/train.py}
}

@inproceedings{ronneberger2015micc,
  author    = {O. Ronneberger and P.Fischer and T. Brox},
  title     = {{U-Net: Convolutional Networks for Biomedical Image Segmentation}},
  booktitle = {Proc. of the Medical Image Computing and Computer-Assisted Intervention (MICCAI)},
  year      = 2015,
  url       = {https://arxiv.org/pdf/1505.04597.pdf}
}

@article{paszke2016arxiv,
  author   = {A. Paszke and A. Chaurasia and S. Kim and E. Culurciello},
  title    = {{ENet: A Deep Neural Network Architecture for Real-Time Semantic Segmentation}},
  journal  = arxiv,
  year     = 2016,
  volume   = {arXiv:1606.02147v1},
  url      = {http://arxiv.org/pdf/1606.02147v1},
  abstract = {The ability to perform pixel-wise semantic segmentation in real-time is of paramount importance in mobile applications. Recent deep neural networks aimed at this task have the disadvantage of requiring a large number of floating point operations and have long run-times that hinder their usability. In this paper, we propose a novel deep neural network architecture named ENet (efficient neural network), created specifically for tasks requiring low latency operation. ENet is up to 18$\times$ faster, requires 75$\times$ less FLOPs, has 79$\times$ less parameters, and provides similar or better accuracy to existing models. We have tested it on CamVid, Cityscapes and SUN datasets and report on comparisons with existing state-of-the-art methods, and the trade-offs between accuracy and processing time of a network. We present performance measurements of the proposed architecture on embedded systems and suggest possible software improvements that could make ENet even faster.}
}

@article{badrinarayanan2017pami,
  title   = {{SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation}},
  author  = {V. Badrinarayanan and A. Kendall and R. Cipolla},
  journal = pami,
  volume  = {39},
  number  = {12},
  pages   = {2481--2495},
  year    = {2017}
}

@inproceedings{szegedy2016cvpr,
  author    = {C. Szegedy and V. Vanhoucke and S. Ioffe and J. Shlens and Z. Wojna},
  title     = {{Rethinking the Inception Architecture for Computer Vision}},
  booktitle = cvprold,
  year      = 2016,
  abstract  = {Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible. We benchmark our methods on the ILSVRC 2012 classification challenge validation set and demonstrate substantial gains over the state of the art via to carefully factorized convolutions and aggressive regularization: 21.2\% top-1 and 5.6\% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters.}
}


@article{dumoulin2016arxiv,
  author   = {V. Dumoulin and F. Visin},
  title    = {{A guide to convolution arithmetic for deep learning}},
  journal  = arxiv,
  year     = 2016,
  volume   = {arXiv:1603.07285},
  url      = {http://arxiv.org/pdf/1603.07285v2},
  abstract = {We introduce a guide to help deep learning practitioners understand and manipulate convolutional neural network architectures. The guide clarifies the relationship between various properties (input shape, kernel shape, zero padding, strides and output shape) of convolutional, pooling and transposed convolutional layers, as well as the relationship between convolutional and transposed convolutional layers. Relationships are derived for various cases, and are illustrated in order to make them intuitive.}
}

@article{li2017arxiv,
  author   = {X. Li and H. Chen and X. Qi and Q. Dou and C. Fu and P.A. Heng},
  title    = {{H-DenseUNet: Hybrid Densely Connected UNet for Liver and Tumor Segmentation from CT Volumes}},
  journal  = arxiv,
  year     = 2017,
  volume   = {arXiv:1709.07330},
  url      = {http://arxiv.org/pdf/1709.07330v3},
  abstract = {Liver cancer is one of the leading causes of cancer death. To assist doctors in hepatocellular carcinoma diagnosis and treatment planning, an accurate and automatic liver and tumor segmentation method is highly demanded in the clinical practice. Recently, fully convolutional neural networks (FCNs), including 2D and 3D FCNs, serve as the back-bone in many volumetric image segmentation. However, 2D convolutions can not fully leverage the spatial information along the third dimension while 3D convolutions suffer from high computational cost and GPU memory consumption. To address these issues, we propose a novel hybrid densely connected UNet (H-DenseUNet), which consists of a 2D DenseUNet for efficiently extracting intra-slice features and a 3D counterpart for hierarchically aggregating volumetric contexts under the spirit of the auto-context algorithm for liver and tumor segmentation. We formulate the learning process of H-DenseUNet in an end-to-end manner, where the intra-slice representations and inter-slice features can be jointly optimized through a hybrid feature fusion (HFF) layer. We extensively evaluated our method on the dataset of MICCAI 2017 Liver Tumor Segmentation (LiTS) Challenge and 3DIRCADb Dataset. Our method outperformed other state-of-the-arts on the segmentation results of tumors and achieved very competitive performance for liver segmentation even with a single model.}
}

@inproceedings{cicco2017iros,
  author    = {M.D. Cicco and C. Potena and G. Grisetti and A. Pretto},
  title     = {{Automatic Model Based Dataset Generation for Fast and Accurate Crop and Weeds Detection}},
  booktitle = iros,
  year      = 2017,
  keywords  = {Robotics in Agriculture and Forestry, Agricultural Automation, Computer Vision for Other Robotic Applications},
  abstract  = {Selective weeding is one of the key challenges in the field of agriculture robotics. To accomplish this task, a farm robot should be able to accurately detect plants and to distinguish them between crop and weeds. Most of the promising state-of-the-art approaches make use of appearance-based models trained on large annotated datasets. Unfortunately, creating large agricultural datasets with pixel-level annotations is an extremely time consuming task, actually penalizing the usage of data-driven techniques. In this paper, we face this problem by proposing a novel and effective approach that aims to dramatically minimize the human intervention needed to train the detection and classification algorithms. The idea is to procedurally generate large synthetic training datasets randomizing the key features of the target environment (i.e., crop and weed species, type of soil, light conditions). More specifically, by tuning these model parameters, and exploiting a few real-world textures, it is possible to render a large amount of realistic views of an artificial agricultural scenario with no effort. The generated data can be directly used to train the model or to supplement real-world images. We validate the proposed methodology by using as testbed a modern deep learning based image segmentation architecture. We compare the classification results obtained using both real and synthetic images as training data. The reported results confirm the effectiveness and the potentiality of our approach.},
  url       = {proceedings:cicco2017iros.pdf}
}

@inproceedings{hall2017iros,
  author    = {D. Hall and F. Dayoub and T. Perez and C.S. McCool},
  title     = {{A Transplantable System for Weed Classification by Agricultural Robotics}},
  booktitle = iros,
  year      = 2017,
  keywords  = {Computer Vision for Automation, Robotics in Agriculture and Forestry},
  abstract  = {This work presents a rapidly deployable system for automated precision weeding with minimal human labeling time. This overcomes a limiting factor in robotic precision weeding related to the use of vision-based classification systems trained for species that may not be relevant to specific farms. We present a novel approach to overcome this problem by employing unsupervised weed scouting, weed-group labeling, and finally, weed classification that is trained on the labeled scouting data. This work demonstrates a novel labeling approach designed to maximize labeling accuracy whilst needing to label as few images as possible. The labeling approach is able to provide the best classification results of any of the examined exemplar-based labeling approaches whilst needing to label over seven times fewer images than full data labeling.},
  url       = {proceedings:hall2017iros.pdf}
}

@inproceedings{long2015cvpr-fcnf,
  author    = {J. Long and E. Shelhamer and T. Darrell},
  title     = {{Fully Convolutional Networks for Semantic Segmentation}},
  booktitle = cvprold,
  year      = 2015,
  abstract  = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation.  Our key insight is to build "fully convolutional" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning.  We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models.  We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task.  We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations.  Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20\% relative improvement to 62.2\% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes less than one fifth of a second for a typical image.},
  url       = {proceedings:long2015cvpr-fcnf.pdf}
}

@article{srivastava2014jmlr,
  author  = {N. Srivastava and G. Hinton and A. Krizhevsky and I. Sutskever and R. Salakhutdinov},
  title   = {{Dropout: A Simple Way to Prevent Neural Networks from Overfitting}},
  journal = jmlr,
  year    = 2014,
  volume  = {15},
  pages   = {1929--1958},
  url     = {http://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf}
}

@article{pont-tuset2017arxiv,
  author   = {J. Pont-Tuset and F. Perazzi and S. Caelles and P. ArbelÃ¡ez and A. Sorkine-Hornung and L.V. Gool},
  title    = {{The 2017 DAVIS Challenge on Video Object Segmentation}},
  journal  = arxiv,
  year     = 2017,
  volume   = {arXiv:1704.00675},
  url      = {http://arxiv.org/pdf/1704.00675v3},
  abstract = {We present the 2017 DAVIS Challenge on Video Object Segmentation, a public dataset, benchmark, and competition specifically designed for the task of video object segmentation. Following the footsteps of other successful initiatives, such as ILSVRC and PASCAL VOC, which established the avenue of research in the fields of scene classification and semantic segmentation, the DAVIS Challenge comprises a dataset, an evaluation methodology, and a public competition with a dedicated workshop co-located with CVPR 2017. The DAVIS Challenge follows up on the recent publication of DAVIS (Densely-Annotated VIdeo Segmentation), which has fostered the development of several novel state-of-the-art video object segmentation techniques. In this paper we describe the scope of the benchmark, highlight the main characteristics of the dataset, define the evaluation metrics of the competition, and present a detailed analysis of the results of the participants to the challenge.}
}

@inproceedings{lin2014iclr,
  author    = {M. Lin and Q. Chen and S. Yan},
  title     = {{Network In Network}},
  booktitle = iclr,
  year      = 2014
}

@inproceedings{he2015iccv,
  author    = {K. He and X. Zhang and S. Ren and J. Sun},
  title     = {{Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification}},
  booktitle = iccvold,
  year      = {2015}
}

@inproceedings{kraemer2017iros,
  author    = {F. Kraemer and A. Schaefer and A. Eitel and J. Vertens and W. Burgard},
  title     = {{From Plants to Landmarks: Time-invariant Plant Localization that uses Deep Pose Regression in Agricultural Fields}},
  booktitle = {IROS Workshop on Agri-Food Robotics},
  year      = 2017,
  url       = {http://ais.informatik.uni-freiburg.de/publications/papers/kraemer17iros.pdf}
}

@inproceedings{haug2014ias,
  author    = {S. Haug and  P. Biber and A. Michaels and J. Ostermann},
  title     = {{Plant  stem detection and position estimation using machine vision}},
  booktitle = {Workshop Proc.~of Conf. on Intelligent Autonomous Systems~(IAS)},
  year      = 2014,
  url       = {http://agribotics.blogs.lincoln.ac.uk/files/2014/07/paper_3.pdf}
}

@inproceedings{michaels2015iros,
  author    = {A. Michaels and S. Haug and A. Albert},
  title     = {{Vision-Based High-Speed Manipulation for Robotic Ultra-Precise Weed Control}},
  booktitle = iros,
  year      = 2015,
  keywords  = {Visual Servoing, Mobile Manipulation, Robotics in Agriculture and Forestry},
  abstract  = {In this system paper we present a novel approach for autonomous, mobile manipulation for agricultural robots. Our target application is mechanical weed control which for example is needed in organic farming. Today, this task is often performed by field workers, whose availability is declining and the quality of their work differs greatly. In addition, manual weeding is a very tedious task with adverse health effects. Hence, mechanical weed control is considered as a prime example for automation. One of the challenges in order to perform the task efficiently is the need for fast plant detection and precise treatment of single plants with a mobile manipulator. The unstructured environment and rough field conditions raise additional challenges. We introduce a first system concept and control design known to handle these issues. In particular, high speed image processing and visual servoing are applied to precisely position a specially designed weeding tool. To evaluate the performance of the mobile manipulator we run experiments with a mobile test vehicle in field-like conditions and demonstrate that our system can treat single plants in less than 1s as it is required to be competitive.},
  url       = {proceedings:michaels2015iros.pdf}
}

@article{haff2011aea,
  author  = {Haff, R.P. and Slaughter, David and S Jackson, E},
  year    = {2011},
  pages   = {803--810},
  title   = {{X-Ray Based Stem Detection in an Automatic Tomato Weeding System}},
  volume  = {27},
  journal = {Applied Engineering in Agriculture}
}

@article{kiani2012jast,
  author  = {S. Kiani and A. Jafari},
  year    = {2012},
  pages   = {755--765},
  title   = {Crop detection and positioning in the field using discriminant analysis and neural networks based on shape features},
  volume  = {14},
  journal = {Journal of Agricultural Science and Technology}
}

@article{cheein2011cea,
  title    = {Optimized EIF-SLAM algorithm for precision agriculture mapping based on stems detection},
  journal  = cea,
  volume   = {78},
  number   = {2},
  pages    = {195--207},
  year     = {2011},
  author   = {F. Auat Cheein and G. Steiner and G. Perez Paina and R. Carelli},
  keywords = {Agricultural mapping, SLAM, Mobile robot}
}

@article{midtiby2012be,
  author  = {H.S. Midtiby and T.M. Giselsson and R.N. Joergensen},
  title   = {Estimating the plant stem emerging points (PSEPs) of sugar beets at early growth stages},
  journal = biosyseng,
  volume  = {111},
  number  = {1},
  pages   = {83--90},
  year    = {2012},
  url     = {http://www.sciencedirect.com/science/article/pii/S1537511011001954}
}

@article{weiss2011ras,
  author   = {U. Weiss and P. Biber},
  title    = {Plant detection and mapping for agricultural robots using a 3D LIDAR sensor},
  journal  = ras,
  volume   = {59},
  number   = {5},
  pages    = {265--273},
  year     = {2011},
  url      = {http://www.sciencedirect.com/science/article/pii/S0921889011000315},
  keywords = {Individual plant detection, Plant mapping, 3D LIDAR sensor, Agricultural robotics}
}

@inproceedings{gai2016abe,
  author    = {J. Gai and L. Tang and and B. Steward},
  title     = {{Plant Localization and Discrimination using 2D+3D Computer Vision for Robotic Intra-row Weed Control}},
  booktitle = {Agricultural and Biosystems Engineering Conference Proceedings and Presentations},
  year      = 2016
}

@inproceedings{maas2013icml-ws,
  author    = {A. L. Maas and A. Y. Hannun and A. Y. Ng},
  title     = {{Rectifier nonlinearities improve neural network acoustic models}},
  booktitle = {ICML Workshop on Deep Learning for Audio, Speech and Language Processing},
  year      = {2013}
}

@inproceedings{rahman2016icsv,
  title     = {{Optimizing Intersection-Over-Union in Deep Neural Networks for Image Segmentation}},
  author    = {M. A. Rahman and Y. Wang},
  booktitle = {Intl. Symp. on Visual Computing},
  year      = {2016}
}

@article{gold1998pr,
  author  = {S. Gold and
             A. Rangarajan and
             C.P. Lu and
             S. Pappu and
             E. Mjolsness},
  title   = {New algorithms for 2D and 3D point matching: pose estimation and correspondence},
  journal = pr,
  volume  = {31},
  number  = {8},
  pages   = {1019--1031},
  year    = {1998}
}

@article{yang2015tvcg,
  author  = {L. Yang and
             J.M. Normand and
             G. Moreau},
  title   = {Local Geometric Consensus: {A} General Purpose Point Pattern-Based
             Tracking Algorithm},
  journal = {{IEEE} Trans. on Visualization and Computer Graphics},
  volume  = {21},
  number  = {11},
  pages   = {1299--1308},
  year    = {2015},
  url     = {https://doi.org/10.1109/TVCG.2015.2459897}
}

@article{carcassoni2003pr,
  author  = {M. Carcassoni and
             E.R. Hancock},
  title   = {Spectral correspondence for point pattern matching},
  journal = pr,
  volume  = {36},
  number  = {1},
  pages   = {193--204},
  year    = {2003}
}

@article{wolfson1997icse,
  author  = {H.J. Wolfson and I. Rigoutsos},
  journal = {IEEE Computational Science and Engineering},
  title   = {Geometric hashing: an overview},
  year    = {1997},
  volume  = {4},
  number  = {4},
  pages   = {10--21}
}

@article{munkres1957siam,
  author  = {J. Munkres},
  title   = {Algorithms for the Assignment and Transportation Problems},
  journal = {Journal of the Society for Industrial and Applied Mathematics},
  year    = {1957},
  volume  = {5},
  number  = {1},
  pages   = {32--38}
}

@article{otsu1979tsmc,
  author  = {N. Otsu},
  journal = {IEEE Trans. on Systems, Man, and Cybernetics},
  title   = {A Threshold Selection Method from Gray-Level Histograms},
  year    = {1979},
  volume  = {9},
  number  = {1},
  pages   = {62--66}
}

@inproceedings{kusumam2016iros,
  author    = {K. Kusumam and T. KrajnÃ­k and S. Pearson and G. Cielniak and T. Duckett},
  booktitle = iros,
  title     = {Can you pick a broccoli? 3D-vision based detection and localisation of broccoli heads in the field},
  year      = {2016}
}

@article{christie2016arxiv,
  author   = {G. Christie and G. Warnell and K. Kochersberger},
  title    = {{Semantics for UGV Registration in GPS-denied Environments}},
  journal  = arxiv,
  year     = 2016,
  volume   = {arXiv:1609.04794},
  url      = {http://arxiv.org/pdf/1609.04794v2},
  abstract = {Localization in a global map is critical to success in many autonomous robot missions. This is particularly challenging for multi-robot operations in unknown and adverse environments. Here, we are concerned with providing a small unmanned ground vehicle (UGV) the ability to localize itself within a 2.5D aerial map generated from imagery captured by a low-flying unmanned aerial vehicle (UAV). We consider the scenario where GPS is unavailable and appearance-based scene changes may have occurred between the UAV's flight and the start of the UGV's mission. We present a GPS-free solution to this localization problem that is robust to appearance shifts by exploiting high-level, semantic representations of image and depth data. Using data gathered at an urban test site, we empirically demonstrate that our technique yields results within five meters of a GPS-based approach.}
}

@inproceedings{forster2013iros,
  author    = {C. Forster and M. Pizzoli and D. Scaramuzza},
  booktitle = iros,
  title     = {Air-ground localization and map augmentation using monocular dense reconstruction},
  year      = {2013},
  keywords  = {image reconstruction;mobile robots;position measurement;robot vision;visual feature matching;vantage points;sensors;position estimation;iterative pose refinement;Monte Carlo localization;live dense reconstruction;depth sensor;MAV monocular camera;3D reconstruction;3D map registration;ground robot;micro aerial vehicle;monocular dense reconstruction;map augmentation;air-ground localization;Robot kinematics;Three-dimensional displays;Cameras;Simultaneous localization and mapping}
}

@article{kwon2010jfr,
  author   = {T.B. Kwon and J.B. Song},
  title    = {A new feature commonly observed from air and ground for outdoor localization with elevation map built by aerial mapping system},
  journal  = jfr,
  year     = {2010},
  volume   = {28},
  number   = {2},
  pages    = {227--240},
  url      = {https://onlinelibrary.wiley.com/doi/abs/10.1002/rob.20373},
  abstract = {Abstract Monte Carlo localization (MCL) uses a reference map to estimate a pose of a ground robot in outdoor environments. However, MCL shows low performance when it uses an elevation map built by an aerial mapping system because three-dimensional (3D) environments are observed differently from the air and the ground and such an elevation map cannot represent outdoor environments in detail. Although other types of maps have been proposed to improve localization performance, an elevation map is still used as the main reference map in some applications. Therefore, we propose a new feature to improve localization performance with an elevation map. This feature is extracted from 3D range data and represents the part of an object that can be commonly observed from both the air and the ground. Therefore, this feature is likely to be accurately matched with an elevation map, and the average error of this feature is much smaller than that of unclassified sensing data. Experimental results in real environments show that the success rate of global localization increased and the error of local tracking decreased. Thus, the proposed feature can be very useful for localization of an outdoor ground robot when an elevation map is used as a reference map. Â© 2010 Wiley Periodicals, Inc.}
}


@article{nathan2012jfr,
  author   = {Michael, Nathan and Shen, Shaojie and Mohta, Kartik and Mulgaonkar, Yash and Kumar, Vijay and Nagatani, Keiji and Okada, Yoshito and Kiribayashi, Seiga and Otake, Kazuki and Yoshida, Kazuya and Ohno, Kazunori and Takeuchi, Eijiro and Tadokoro, Satoshi},
  title    = {Collaborative mapping of an earthquake-damaged building via ground and aerial robots},
  journal  = {Journal of Field Robotics},
  year     = {2012},
  volume   = {29},
  number   = {5},
  pages    = {832--841},
  url      = {https://onlinelibrary.wiley.com/doi/abs/10.1002/rob.21436},
  abstract = {Abstract We report recent results from field experiments conducted with a team of ground and aerial robots engaged in the collaborative mapping of an earthquake-damaged building. The goal of the experimental exercise is the generation of three-dimensional maps that capture the layout of a multifloor environment. The experiments took place in the top three floors of a structurally compromised building at Tohoku University in Sendai, Japan that was damaged during the 2011 Tohoku earthquake. We provide details of the approach to the collaborative mapping and report results from the experiments in the form of maps generated by the individual robots and as a team. We conclude by discussing observations from the experiments and future research topics. Â© 2012 Wiley Periodicals, Inc.}
}

@article{calleja2011ras,
  author   = {T. A. Vidal-Calleja and C. Berger and J. SolÃ  and S. Lacroix},
  title    = {Large scale multiple robot visual mapping with heterogeneous landmarks in semi-structured terrain},
  journal  = ras,
  volume   = {59},
  number   = {9},
  pages    = {654--674},
  year     = {2011},
  url      = {http://www.sciencedirect.com/science/article/pii/S0921889011000923},
  keywords = {Multi-robots cooperation, Visual SLAM}
}

@inproceedings{ding2008cvpr,
  author    = {M. Ding and K. Lyngbaek and A. Zakhor},
  booktitle = cvprold,
  title     = {Automatic registration of aerial imagery with untextured 3D LiDAR models},
  year      = {2008},
  keywords  = {geophysical signal processing;Hough transforms;image reconstruction;image registration;image texture;optical radar;pose estimation;radar imaging;remote sensing by laser beam;terrain mapping;aerial imagery registration;untextured 3D LiDAR models;3D model reconstruction methodology;texture mapping;light detection and ranging;global positioning system;inertial system readings;calibrated camera;Hough transform;generalized M-estimator;pose recovery;Berkeley;Laser radar;Cameras;Clouds;Image reconstruction;Urban planning;Geometry;Solid modeling;Image generation;Application software;Computational modeling}
}

@inproceedings{leung2008icra,
  author    = {K. Y. K. Leung and C. M. Clark and J. P. Huissoon},
  booktitle = icra,
  title     = {Localization in urban environments by matching ground level video images with an aerial image},
  year      = {2008},
  keywords  = {cameras;Global Positioning System;image matching;particle filtering (numerical methods);video signal processing;urban environments;ground level video image matching;aerial image;monocular vision;particle filter localization system;aerial orthoimagery;reference map;global positioning system;GPS;image processing techniques;camera images;Cameras;Robot vision systems;Particle filters;Global Positioning System;Image databases;Degradation;Image processing;Navigation;Buildings;Robot sensing systems}
}

@article{majdik2015jfr,
  author   = {A. Majdik and D. Verda and Y. Albers-Schoenberg and D. Scaramuzza},
  title    = {Air-ground Matching: Appearance-based GPS-denied Urban Localization of Micro Aerial Vehicles},
  journal  = jfr,
  year     = {2015},
  volume   = {32},
  number   = {7},
  pages    = {1015--1039},
  url      = {https://onlinelibrary.wiley.com/doi/abs/10.1002/rob.21585},
  abstract = {In this paper, we address the problem of globally localizing and tracking the pose of a camera-equipped micro aerial vehicle (MAV) flying in urban streets at low altitudes without GPS. An image-based global positioning system is introduced to localize the MAV with respect to the surrounding buildings. We propose a novel air-ground image-matching algorithm to search the airborne image of the MAV within a ground-level, geotagged image database. Based on the detected matching image features, we infer the global position of the MAV by back-projecting the corresponding image points onto a cadastral three-dimensional city model. Furthermore, we describe an algorithm to track the position of the flying vehicle over several frames and to correct the accumulated drift of the visual odometry whenever a good match is detected between the airborne and the ground-level images. The proposed approach is tested on a 2 km trajectory with a small quadrocopter flying in the streets of Zurich. Our vision-based global localization can robustly handle extreme changes in viewpoint, illumination, perceptual aliasing, and over-season variations, thus outperforming conventional visual place-recognition approaches. The dataset is made publicly available to the research community. To the best of our knowledge, this is the first work that studies and demonstrates global localization and position tracking of a drone in urban streets with a single onboard camera.}
}

@inproceedings{ruchti2015icra,
  author    = {P. Ruchti and B. Steder and M. Ruhnke and W. Burgard},
  title     = {{Localization on OpenStreetMap Data Using a 3D Laser Scanner}},
  booktitle = icra,
  year      = 2015,
  keywords  = {Localization, Range Sensing},
  abstract  = {To determine the pose of a vehicle is a fundamental problem in mobile robotics. Most approaches relate the current sensor observations to a map generated with previously acquired data of the same system or by another system with a similar sensor setup. Unfortunately, previously acquired data is not always available. In outdoor settings, GPS is a very useful tool to determine a global estimate of the vehicles pose. Unfortunately, GPS tends to be unreliable in situations in which a clear view to the sky is restricted. Yet, one can make use of publicly available map material as prior information. In this paper, we describe an approach to localize a robot equipped with a 3D range scanner with respect to a road network created from OpenStreetMap data. To successfully localize a mobile robot we propose a road classification scheme for 3D range data together with a novel sensor model, which relates the classification results to a road network. Compared to other approaches, our system does not require the robot to actually travel on the road network. We evaluate our approach in extensive experiments on simulated and real data and compare favorably to two state-of-the-art methods on those data.}
}

@article{kummerle2011ar,
  author   = {R. KÃ¼mmerle and B. Steder and C. Dornhege and A. Kleiner and G. Grisetti and W. Burgard},
  title    = {Large scale graph-based SLAM using aerial images as prior information},
  journal  = ar,
  year     = {2011},
  volume   = {30},
  number   = {1},
  pages    = {25--39},
  abstract = {The problem of learning a map with a mobile robot has been intensively studied in the past and is usually referred to as the simultaneous localization and mapping (SLAM) problem. However, most existing solutions to the SLAM problem learn the maps from scratch and have no means for incorporating prior information. In this paper, we present a novel SLAM approach that achieves global consistency by utilizing publicly accessible aerial photographs as prior information. It inserts correspondences found between stereo and three-dimensional range data and the aerial images as constraints into a graph-based formulation of the SLAM problem. We evaluate our algorithm based on large real-world datasets acquired even in mixed in- and outdoor environments by comparing the global accuracy with state-of-the-art SLAM approaches and GPS. The experimental results demonstrate that the maps acquired with our method show increased global consistency.}
}

@article{chebrolu2018ral,
  author   = {N. Chebrolu and T. L\"abe and C. Stachniss},
  journal  = ral,
  title    = {{Robust Long-Term Registration of UAV Images of Crop Fields for Precision Agriculture}},
  year     = {2018},
  volume   = {3},
  number   = {4},
  pages    = {3097--3104},
  keywords = {Agriculture;Cameras;Geometry;Monitoring;Robustness;Three-dimensional displays;Visualization;Robotics in agriculture and forestry;SLAM},
  url      = {http://www.ipb.uni-bonn.de/pdfs/chebrolu2018ral.pdf}
}

@inproceedings{dellaert1999icra,
  author    = {F. Dellaert and D. Fox and W. Burgard and S. Thrun},
  title     = {Monte Carlo Localization for Mobile Robots},
  booktitle = icra,
  year      = {1999}
}

@inproceedings{zaenker2021ecmr,
  author    = {Zaenker, Tobias and Lehnert, Chris and McCool, Chris and
               Bennewitz, Maren},
  booktitle = ecmr,
  title     = {Combining Local and Global Viewpoint Planning for
               Fruit Coverage},
  year      = {2021},
  url       = {https://arxiv.org/pdf/2108.08114.pdf}
}

@inproceedings{zaenker2021iros,
  title     = {Viewpoint planning for fruit size and position estimation},
  author    = {Zaenker, Tobias and Smitt, Claus and McCool, Chris and Bennewitz, Maren},
  booktitle = iros,
  url       = {proceedings:zaenker2021iros.pdf},
  year      = {2021}
}


@inproceedings{nuske2011iros,
  author    = {S. Nuske and S. Achar and T. Bates and S. Narasimhan and S. Singh},
  title     = {{Yield Estimation in Vineyards by Visual Grape Detection}},
  booktitle = iros,
  year      = 2011,
  keywords  = {Robotics in Agriculture and Forestry, Field Robots},
  abstract  = {The harvest yield in vineyards can vary significantly from year to year and also spatially within plots due to variations in climate, soil conditions and pests. Fine grained knowledge of crop yields can allow viticulturists to better manage their vineyards. The current industry practice for yield prediction is destructive, expensive and spatially sparse during the growing season sparse samples are taken and extrapolated to determine overall yield. We present an automated method that uses computer vision to detect and count grape berries. The method could potentially be deployed across large vineyards taking measurements at every vine in a non-destructive manner. Our berry detection uses both shape and visual texture and we can demonstrate detection of green berries against a green leaf background. Berry detections are counted and the eventual harvest yield is predicted. Results are presented for 224 vines (over 450 meters) of two different grape varieties and compared against the actual harvest yield as groundtruth. We calibrate our berry count to yield and find that we can predict yield of individual vineyard rows to within 9.8\% of actual crop weight.},
  url       = {proceedings: nuske2011iros.pdf}
}

@article{underwood2016cea,
  title   = {Mapping Almond Orchard Canopy Volume, flowers, fruit and yield using Lidar and vision sensors},
  volume  = {130},
  journal = cea,
  author  = {Underwood, James P. and Hung, Calvin and Whelan, Brett and Sukkarieh, Salah},
  year    = {2016},
  pages   = {83--96}
}


@inproceedings{rublee2011iccv,
  author    = {E. Rublee and V. Rabaud and K. Konolige and G. Bradski},
  title     = {ORB: an efficient alternative to SIFT or SURF},
  booktitle = iccvold,
  year      = {2011}
}

@article{liu2019ral,
  title   = {Monocular camera based fruit counting and mapping with Semantic Data Association},
  volume  = {4},
  number  = {3},
  journal = ral,
  author  = {Liu, Xu and Chen, Steven W. and Liu, Chenhao and Shivakumar, Shreyas S. and Das, Jnaneshwar and Taylor, Camillo J. and Underwood, James and Kumar, Vijay},
  year    = {2019},
  pages   = {2296--2303},
  url     = {https://ieeexplore.ieee.org/ielaam/7083369/8668830/8653965-aam.pdf}
} 

@article{winterhalter2018ral,
  title   = {{Crop Row Detection on Tiny Plants With the Pattern Hough Transform}},
  author  = {W. Winterhalter and F. V. Fleckenstein and C. Dornhege and W. Burgard},
  journal = ral,
  year    = {2018},
  volume  = {3},
  number  = {4},
  pages   = {3394--3401}
}

@article{halstead2021fps,
  author  = {Halstead, Michael and Ahmadi, Alireza and
             Smitt, Claus and Schmittmann, Oliver and
             McCool, Chris},
  journal = fps,
  title   = {Crop Agnostic Monitoring Driven by Deep Learning},
  volume  = {12},
  year    = {2021},
  url     = {https://www.frontiersin.org/article/10.3389/
             fpls.2021.786702}
}

@inproceedings{magistri2021icra,
  author    = {F. Magistri and N. Chebrolu and J. Behley and
               C. Stachniss},
  booktitle = icra,
  title     = {{Towards In-Field Phenotyping Exploiting
               Differentiable Rendering with Self-Consistency Loss}},
  year      = {2021},
  url       = {proceedings: magistri2021icra.pdf}
}

@inproceedings{marks2022icra,
  author    = {E. Marks and F. Magistri and C. Stachniss},
  booktitle = icra,
  title     = {{Precise 3D Reconstruction of Plants from UAV Imagery
               Combining Bundle Adjustment and Template Matching}},
  year      = {2022}
}

@inproceedings{stutz2018cvpr,
  author    = {Stutz, David and Geiger, Andreas},
  booktitle = cvpr,
  title     = {Learning 3D Shape Completion from Laser Scan Data
               with Weak Supervision},
  year      = {2018},
  url       = {proceedings: stutz2018cvpr.pdf}
}

@inproceedings{hadsell2006cvpr,
  author    = {Hadsell, Raia and Chopra, Sumit and LeCun, Yann},
  booktitle = cvpr,
  title     = {Dimensionality reduction by learning an invariant
               mapping},
  year      = {2006},
  url       = {http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf}
}


@article{blok2021biosyseng,
  author  = {Blok, Pieter M and van Henten, Eldert J and
             van Evert, Frits K and Kootstra, Gert},
  journal = biosyseng,
  pages   = {213--233},
  title   = {Image-based size estimation of broccoli heads under
             varying degrees of occlusion},
  volume  = {208},
  year    = {2021}
}

@article{stein2016sensors,
  title   = {Image based mango fruit detection, localisation and yield estimation using multiple view geometry},
  volume  = {16},
  number  = {11},
  journal = sensors,
  author  = {Stein, Madeleine and Bargoti, Suchet and Underwood, James},
  year    = {2016},
  pages   = {1915}
} 


@article{guo2014pami,
  title   = {{3D Object Recognition in Cluttered Scenes with Local Surface Features: A Survey}},
  author  = {Y. Guo and M. Bennamoun and F. Sohel and M. Lu and J. Wan},
  journal = pami,
  year    = 2014,
  volume  = 36,
  number  = 1,
  pages   = {2270--2287},
  url     = {http://www.romisatriawahono.net/lecture/rm/survey/computer%20vision/Guo%20-%203D%20Object%20Recognition%20in%20Cluttered%20Scenes%20-%202014.pdf}
}

@inproceedings{drost2010cvpr,
  author    = {B. Drost and M. Ulrich and N. Navab and S. Ilic},
  booktitle = cvpr,
  title     = {{Model globally, match locally: Efficient and robust 3D object recognition}},
  year      = {2010}
}

@inproceedings{akiba2019kddm,
  title     = {Optuna: A Next-generation Hyperparameter Optimization Framework},
  author    = {Akiba, Takuya and Sano, Shotaro and Yanase, Toshihiko and Ohta, Takeru and Koyama, Masanori},
  booktitle = {Proc.~of the Intl.~Conf.~on Knowledge Discovery and Data Mining},
  year      = {2019},
  url       = {https://arxiv.org/pdf/1907.10902.pdf}
}

@inproceedings{das2015case,
  title     = {Devices, systems, and methods for automated monitoring enabling precision agriculture},
  author    = {Das, Jnaneshwar and Cross, Gareth and Qu, Chao and Makineni, Anurag and Tokekar, Pratap and Mulgaonkar, Yash and Kumar, Vijay},
  booktitle = case,
  year      = {2015}
}

@inproceedings{montemerlo2002aaai,
  author    = {Michael Montemerlo and Sebastian Thrun and Daphne Koller and Ben Wegbreit},
  title     = {{FastSLAM: A Factored Solution to the Simultaneous Localization and Mapping Problem}},
  booktitle = aaai,
  year      = {2002}
}

@inproceedings{maffra2018icra,
  author    = {F. Maffra and Z. Chen and M. Chli},
  title     = {{Viewpoint-tolerant Place Recognition combining 2D and 3D information for UAV navigation}},
  booktitle = icra,
  year      = 2018,
  keywords  = {Aerial Systems: Perception and Autonomy, SLAM, Computer Vision for Automation},
  abstract  = {The booming interest in Unmanned Aerial Vehicles (UAVs) is fed by their potentially great impact, however progress is hindered by their limited perception capabilities. While vision-based odometry was shown to run successfully onboard UAVs, loop-closure detection to correct for drift or to recover from tracking failures, has so far, proven particularly challenging for UAVs. At the heart of this is the problem of viewpoint-tolerant place recognition; in stark difference to ground robots, UAVs can revisit a scene from very different viewpoints. As a result, existing approaches struggle greatly as the task at hand violates underlying assumptions in assessing scene similarity. In this paper, we propose a place recognition framework, which exploits both efficient binary features and noisy estimates of the local 3D geometry, which are anyway computed for visual-inertial odometry onboard the UAV. Attaching both an appearance and a geometry signature to each location, the proposed approach demonstrates unprecedented recall for perfect precision as well as high quality loopclosing transformations on both flying and hand-held datasets exhibiting large viewpoint and appearance changes as well as perceptual aliasing.}
}

@article{papadimitriou1987mor,
  title   = {The complexity of Markov decision processes},
  author  = {C. Papadimitriou and  J. Tsitsiklis},
  journal = mor,
  volume  = {12},
  number  = {3},
  pages   = {441--450},
  year    = {1987}
}

@inproceedings{littman1995uai,
  title     = {On the complexity of solving Markov decision problems},
  author    = {M. Littman and T. Dean and L Kaelbling},
  booktitle = uai,
  year      = {1995}
}

@inproceedings{somani2013nips,
  title     = {DESPOT: Online POMDP planning with regularization},
  author    = {A. Somani and N. Ye and D. Hsu and W. S. Lee},
  booktitle = nips,
  year      = {2013}
}

@inproceedings{silver2010nips,
  title     = {Monte-Carlo planning in large POMDPs},
  author    = {D. Silver and J. Veness},
  booktitle = nips,
  year      = {2010}
}

@inproceedings{agha2011iros,
  title     = {FIRM: Feedback controller-based Information-state RoadMap-a framework for motion planning under uncertainty},
  author    = {A. Agha-Mohammadi and S. Chakravorty and N. M. Amato},
  booktitle = iros,
  year      = {2011}
}

@article{vandenberg2011ijrr,
  title   = {LQG-MP: Optimized path planning for robots with motion uncertainty and imperfect state information},
  author  = {J. Van Den Berg and P. Abbeel and K. Goldberg},
  journal = ijrr,
  volume  = {30},
  number  = {7},
  pages   = {895--913},
  year    = {2011}
}

@inproceedings{platt2010rss,
  title     = {Belief space planning assuming maximum likelihood observations},
  author    = {R. Platt and R. Tedrake and L. Kaelbling and T. Lozano-Perez},
  booktitle = rss,
  year      = {2010}
}

@inproceedings{bopardikar2014icra,
  title     = {Robust belief roadmap: Planning under Uncertain and intermittent sensing},
  author    = {S. Bopardikar and B. Englot and A. Speranzon},
  booktitle = icra,
  year      = {2014}
}

@inproceedings{kuipers1988aaai,
  title     = {A Robust, Qualitative Method for Robot Spatial Learning},
  author    = {B. Kuipers and Y. Byun},
  booktitle = aaai,
  year      = {1988}
}

@article{choi2003pr,
  title   = {Feature extraction based on the Bhattacharyya distance},
  author  = {E. Choi and C. Lee},
  journal = pr,
  volume  = {36},
  number  = {8},
  pages   = {1703--1709},
  year    = {2003}
}

@article{bhattacharyya1943bhatdist,
  title   = {On a Measure of Divergence Between Two Statistical Populations Defined by Their Probability Distributions},
  author  = {A. Bhattacharyya},
  journal = {Bull. Calcutta Math. Soc.},
  volume  = {35},
  pages   = {99--109},
  year    = 1943
}

@article{kullback1951kldiv,
  title   = {On Information and Sufficiency},
  author  = {S. Kullback and R. Leibler},
  journal = {Annals of Mathematical Statistics},
  volume  = {22},
  number  = {1},
  pages   = {79--86},
  year    = 1951
}

@book{howard1964dpbook,
  title     = {Dynamic Programming and Markov Processes},
  author    = {R. Howard},
  year      = 1964,
  publisher = mitpress
}

@inproceedings{hackel2017isprs,
  title     = {{SEMANTIC3D.NET: A new large-scale point cloud classification benchmark}},
  author    = {T. Hackel and N. Savinov and L. Ladicky and Jan D. Wegner and K. Schindler and M. Pollefeys},
  booktitle = isprsannals,
  year      = {2017}
}

@inproceedings{behley2012icra,
  author    = {J. Behley and V. Steinhage and A.B. Cremers},
  title     = {{Performance of Histogram Descriptors for the Classification of 3D Laser Range Data in Urban Environments}},
  booktitle = icra,
  year      = {2012},
  url       = {http://jbehley.github.io/papers/behley2012icra.pdf}
}

@inproceedings{munoz2009cvpr,
  author    = {D. Munoz and J. A. Bagnell and N. Vandapel and M. Hebert},
  title     = {{Contextual Classification with Functional Max-Margin Markov Networks}},
  booktitle = cvprold,
  year      = {2009},
  abstract  = {We address the problem of label assignment in computer
               vision: given a novel 3-D or 2-D scene, we wish to assign a
               unique label to every site (voxel, pixel, superpixel, etc.). To this end, the Markov Random Field framework has proven
               to be a model of choice as it uses contextual information to yield improved classification results over locally independent classifiers.
               In this work we adapt a functional gradient approach for learning high-dimensional parameters of
               random fields in order to perform discrete, multi-label classification.  With this approach we can learn robust models
               involving high-order interactions better than the previously used learning method. We validate
               the approach in the context of point cloud classification and improve the state of
               the art. In addition, we successfully demonstrate the generality of the approach on the challenging vision problem of
               recovering 3-D geometric surfaces from images.},
  url       = {https://www.ri.cmu.edu/pub_files/2009/6/munoz_cvpr_09.pdf}
}

@inproceedings{neuhold2017iccv,
  author    = {G. Neuhold and T. Ollmann and S. Rota Bulo and P. Kontschieder},
  title     = {{The Mapillary Vistas Dataset for Semantic Understanding of Street Scenes}},
  booktitle = iccvold,
  year      = 2017,
  url       = {https://research.mapillary.com/img/publications/ICCV17a.pdf},
  abstract  = {The   Mapillary   Vistas   Dataset   is   a   novel,    large-scale
               street-level  image  dataset  containing  25 000  high-resolution images annotated
               into 66 object categories with additional, instance-specific labels for 37 classes.
               Annotation is performed in a dense and fine-grained style by using
               polygons  for  delineating  individual  objects.   Our  dataset is 5x larger than the
               total amount of fine annotations for Cityscapes and contains images from all around the world,
               captured  at  various  conditions  regarding  weather,  season and daytime.  Images come
               from different imaging devices (mobile phones, tablets, action cameras, professional capturing
               rigs) and differently experienced photographers.  In such a way,  our dataset has been
               designed and compiled to cover diversity, richness of detail and geographic extent.
               As default benchmark tasks, we define semantic image segmentation and instance-specific image segmentation,
               aiming to significantly further the development of state-of-the-art methods for visual
               road-scene understanding}
}

@inproceedings{xie2016cvpr,
  author    = {J. Xie and M. Kiefel and M. Sun and A. Geiger},
  title     = {{Semantic Instance Annotation of Street Scenes by 3D to 2D Label Transfer}},
  booktitle = cvprold,
  year      = 2016,
  url       = {http://www.cvlibs.net/publications/Xie2016CVPR.pdf},
  abstract  = {Semantic  annotations  are  vital  for  training  models  for
               object recognition, semantic segmentation or scene understanding.  Unfortunately,
               pixelwise annotation of images at very  large  scale  is  labor-intensive  and
               only  little  labeled data  is  available,  particularly  at  instance  level  and  for
               street scenes.  In this paper, we propose to tackle this problem by lifting the
               semantic instance labeling task from 2D into 3D. Given reconstructions from stereo
               or laser data, we annotate static 3D scene elements with rough bounding
               primitives and develop a model which transfers this information into the image domain.
               We leverage our method to obtain 2D labels for a novel suburban video dataset which
               we have collected, resulting in 400k semantic and instance image annotations. A
               comparison of our method to state-of-the-art label transfer baselines reveals
               that 3D information enables more efficient annotation while at the same time resulting
               in improved accuracy and time-coherent labels.}
}

@article{roynard2018ijrr,
  author   = {X. Roynard and J. Deschaud and F. Goulette},
  title    = {{Paris-Lille-3D: A large and high-quality ground-truth urban point cloud dataset for automatic segmentation and classification}},
  volume   = {37},
  number   = {6},
  journal  = ijrr,
  year     = 2018,
  pages    = {545--557},
  abstract = {This paper introduces a new urban point cloud dataset for automatic segmentation
              and classification acquired by mobile laser scanning (MLS). We describe how the dataset is
              obtained from acquisition to post-processing and labeling. This dataset can be used to train
              pointwise classification algorithms; however, given that a great attention has been paid to
              the split between the different objects, this dataset can also be used to train the detection
              and segmentation of objects. The dataset consists of around 2km of MLS point cloud acquired in
              two cities. The number of points and range of classes mean that it can be used to train
              deep-learning methods. In addition, we show some results of automatic segmentation and
              classification. The dataset is available at: http://caor-mines-paristech.fr/fr/paris-lille-3d-dataset/.},
  url      = {https://arxiv.org/pdf/1712.00032}
}

@inproceedings{landrieu2018cvpr,
  author    = {L. Landrieu and M. Simonovsky},
  title     = {{Large-scale Point Cloud Semantic Segmentation with Superpoint Graphs}},
  booktitle = cvprold,
  abstract  = {We propose a novel deep learning-based framework to tackle the challenge of
               semantic segmentation of large-scale point clouds of millions of points. We argue that the
               organization of 3D point clouds can be efficiently captured by a structure called superpoint
               graph (SPG), derived from a partition of the scanned scene into geometrically homogeneous
               elements. SPGs offer a compact yet rich representation of contextual relationships between
               object parts, which is then exploited by a graph convolutional network. Our framework sets
               a new state of the art for segmenting outdoor LiDAR scans (+11.9 and +8.8 mIoU points for
               both Semantic3D test sets), as well as indoor scans (+12.4 mIoU points for the S3DIS dataset).},
  year      = 2018,
  url       = {https://arxiv.org/pdf/1711.09869}
}

@inproceedings{su2018cvpr,
  author    = {H. Su and V. Jampani and D. Sun and S. Maji and E. Kalogerakis and M-H. Yang and J. Kautz},
  title     = {{SPLATNet: Sparse Lattice Networks for Point Cloud Processing}},
  booktitle = cvprold,
  abstract  = {We present a network architecture for processing point clouds that directly
               operates on a collection of points represented as a sparse set of samples in a high-dimensional
               lattice. Naively applying convolutions on this lattice scales poorly, both in terms of memory
               and computational cost, as the size of the lattice increases. Instead, our network uses sparse
               bilateral convolutional layers as building blocks. These layers maintain efficiency by using
               indexing structures to apply convolutions only on occupied parts of the lattice, and allow flexible
               specifications of the lattice structure enabling hierarchical and spatially-aware feature learning,
               as well as joint 2D-3D reasoning. Both point-based and image-based representations can be easily
               incorporated in a network with such layers and the resulting model can be trained in an end-to-end
               manner. We present results on 3D segmentation tasks where our approach outperforms existing
               state-of-the-art techniques. },
  url       = {http://openaccess.thecvf.com/content_cvpr_2018/html/../../content_cvpr_2018/papers/Su_SPLATNet_Sparse_Lattice_CVPR_2018_paper.pdf},
  year      = 2018
}

@inproceedings{tatarchenko2018cvpr,
  author    = {M. Tatarchenko and J. Park and V. Koltun and Q-Y. Zhou},
  title     = {{Tangent Convolutions for Dense Prediction in 3D}},
  booktitle = cvprold,
  abstract  = {We present an approach to semantic scene analysis using deep convolutional networks.
               Our approach is based on tangent convolutions â a new construction for convolutional networks on
               3D data. In contrast to volumetric approaches, our method operates directly on surface geometry.
               Crucially, the construction is applicable to unstructured point clouds and other noisy real-world
               data. We show that tangent convolutions can be evaluated efficiently on large-scale point clouds
               with millions of points. Using tangent convolutions, we design a deep fully-convolutional network
               for semantic segmentation of 3D point clouds, and apply it to challenging real-world datasets of
               indoor and outdoor 3D environments. Experimental results show that the presented approach outperforms
               other recent deep network constructions in detailed analysis of large 3D scenes.},
  url       = {proceedings: tatarchenko2018cvpr.pdf},
  year      = 2018
}


@inproceedings{wu2018icra-scnn,
  author    = {B. Wu and A. Wan and X. Yue and K. Keutzer},
  title     = {{SqueezeSeg: Convolutional Neural Nets with Recurrent CRF for Real-Time Road-Object Segmentation from 3D LiDAR Point Cloud}},
  booktitle = icra,
  year      = 2018,
  keywords  = {Object Detection, Segmentation and Categorization, Deep Learning in Robotics and Automation, Autonomous Vehicle Navigation},
  abstract  = {We address semantic segmentation of road-objects from 3D LiDAR point clouds. In
               particular, we wish to detect and categorize instances of interest, such as cars, pedestrians and
               cyclists. We formulate this problem as a point-wise classification problem, and propose an
               end-to-end pipeline called SqueezeSeg based on convolutional neural networks (CNN): the CNN takes
               a transformed LiDAR point cloud as input and directly outputs a point-wise label map, which is then
               refined by a conditional random field (CRF) implemented as a recurrent layer. Instance-level labels
               are then obtained by conventional clustering algorithms. Our CNN model is trained on LiDAR point
               clouds from the KITTI [1] dataset, and our point-wise segmentation labels are derived from 3D
               bounding boxes from KITTI. To obtain extra training data, we built a LiDAR simulator into Grand Theft
               Auto V (GTA-V), a popular video game, to synthesize large amounts of realistic training data. Our
               experiments show that SqueezeSeg achieves high accuracy with astonishingly fast and stable runtime
               (8.7  0.5 ms per frame), highly desirable for autonomous driving. Furthermore, additionally training on
               synthesized data boosts validation accuracy on real-world data. Our source code is open-source released1 .
               The paper is accompanied by a video2 containing a high level introduction and demonstrations of this work.},
  url       = {proceedings: wu2018icra-scnn.pdf}
}

@inproceedings{berman2018cvpr,
  author    = {M. Berman and A. R. Triki and M. B. Blaschko},
  title     = {{The LovÃ¡sz-Softmax Loss: A Tractable Surrogate for the Optimization of the Intersection-Over-Union Measure in Neural Networks}},
  booktitle = cvpr,
  year      = 2018,
  abstract  = {The Jaccard index, also referred to as the intersection-over-union score, is commonly employed in the evaluation of image segmentation results given its perceptual qualities, scale invariance - which lends appropriate relevance to small objects, and appropriate counting of false negatives, in comparison to per-pixel losses. We present a method for direct optimization of the mean intersection-over-union loss in neural networks, in the context of semantic image segmentation, based on the convex LovÃ¡sz extension of submodular losses. The loss is shown to perform better with respect to the Jaccard index measure than the traditionally used cross-entropy loss. We show quantitative and qualitative differences between optimizing the Jaccard index per image versus optimizing the Jaccard index taken over an entire dataset. We evaluate the impact of our method in a semantic segmentation pipeline and show substantially improved intersection-over-union segmentation scores on the Pascal VOC and Cityscapes datasets using state-of-the-art deep learning segmentation architectures.},
  url       = {proceedings: berman2018cvpr.pdf}
}


@inproceedings{silberman2012eccv,
  author    = {N. Silberman and D. Hoiem and P. Kohli and Rob Fergus},
  title     = {{Indoor Segmentation and Support Inference from RGBD Images}},
  booktitle = eccv,
  year      = 2012,
  url       = {https://cs.nyu.edu/~silberman/papers/indoor_seg_support.pdf}
}

@techreport{shapenet2015tr,
  title       = {{ShapeNet: An Information-Rich 3D Model Repository}},
  author      = {A. Chang and T. Funkhouser and L. Guibas and P. Hanrahan and Q. Huang and Z. Li and S. Savarese and M. Savva and S. Song and H. Su and J. Xiao and L. Yi and F. Yu},
  number      = {arXiv:1512.03012 [cs.GR]},
  institution = {Stanford University --- Princeton University --- Toyota Technological Institute at Chicago},
  year        = {2015},
  url         = {https://arxiv.org/pdf/1512.03012.pdf}
}


@inproceedings{firman2016cvprw,
  author    = {Michael Firman},
  title     = {{RGBD Datasets: Past, Present and Future}},
  booktitle = {CVPR Workshop on Large Scale 3D Data: Acquisition, Modelling and Analysis},
  year      = {2016},
  url       = {http://openaccess.thecvf.com/content_cvpr_2016_workshops/w17/papers/Firman_RGBD_Datasets_Past_CVPR_2016_paper.pdf}
}

@inproceedings{wang2018cvpr-dpcc,
  author    = {S. Wang and S. Suo and W. Ma and A. Pokrovsky and R. Urtasun},
  title     = {{Deep Parametric Continuous Convolutional Neural Networks}},
  booktitle = cvprold,
  year      = 2018,
  abstract  = {Standard convolutional neural networks assume a grid structured input
               is available and exploit discrete convolutions as their fundamental building blocks.
               This limits their applicability to many real-world applications. In this paper we propose
               Parametric Continuous Convolution, a new learnable operator that operates over non-grid
               structured data. The key idea is to exploit parameterized kernel functions that span
               the full continuous vector space. This generalization allows us to learn over arbitrary
               data structures as long as their support relationship is computable. Our experiments show
               significant improvement over the state-of-the-art in point cloud segmentation of indoor
               and outdoor scenes, and lidar motion estimation of driving scenes.},
  url       = {proceedings: wang2018cvpr-dpcc.pdf}
}

@article{everingham2015ijcv,
  title    = {{The Pascal Visual Object Classes Challenge â a Retrospective}},
  author   = {M. Everingham and S. Eslami and L. van Gool and C. Williams and J. Winn and A. Zisserman},
  volume   = 111,
  number   = 1,
  pages    = {98--136},
  journal  = ijcv,
  year     = 2015,
  abstract = {The Pascal Visual Object Classes (VOC) challenge consists of two components: (i) a
              publicly available dataset of images together with ground truth annotation and standardised
              evaluation software; and (ii) an annual competition and workshop. There are five challenges:
              classification, detection, segmentation, action classification, and person layout. In this paper
              we provide a review of the challenge from 2008â2012. The paper is intended for two audiences:
              algorithm designers, researchers who want to see what the state of the art is, as measured by
              performance on the VOC datasets, along with the limitations and weak points of the current generation
              of algorithms; and, challenge designers, who want to see what we as organisers have learnt from the
              process and our recommendations for the organisation of future challenges. To analyse the performance
              of submitted algorithms on the VOC datasets we introduce a number of novel evaluation methods: a
              bootstrapping method for determining whether differences in the performance of two algorithms are
              significant or not; a normalised average precision so that performance can be compared across classes
              with different proportions of positive instances; a clustering method for visualising the performance
              across multiple algorithms so that the hard and easy images can be identified; and the use of a joint
              classifier over the submitted algorithms in order to measure their complementarity and combined
              performance. We also analyse the communityâs progress through time using the methods of Hoiem
              et al. (Proceedings of European Conference on Computer Vision, 2012) to identify the types of
              occurring errors. We conclude the paper with an appraisal of the aspects of the challenge that
              worked well, and those that could be improved in future challenges.}
}

@inproceedings{anguelov2005cvpr,
  author    = {D. Anguelov and B. Taskar and V. Chatalbashev and D.  Koller and D. Gupta and G. Heitz and A. Ng},
  title     = {{Discriminative Learning of Markov Random Fields for Segmentation
               of 3D Scan Data}},
  booktitle = cvprold,
  year      = {2005},
  keywords  = {CRF, classification, point-wise, robotics},
  url       = {http://ai.stanford.edu/~vasco/pubs/cvpr05.pdf}
}

@inproceedings{triebel2006icra,
  author    = {R. Triebel and K. Kersting and W. Burgard},
  title     = {{Robust 3D Scan Point Classification using Associative Markov Networks}},
  booktitle = icra,
  year      = {2006},
  keywords  = {classification, point-wise},
  url       = {https://vision.in.tum.de/_media/spezial/bib/triebel06robust.pdf}
}

@inproceedings{munoz20083dpvt,
  author    = {D. Munoz and N.s Vandapel and M. Hebert},
  title     = {{Directional Associative Markov Network for 3-D Point Cloud Classification}},
  booktitle = {Proc.~of the Intl. Symp. on 3D Data Processing, Visualization and Transmission (3DPVT)},
  year      = {2008},
  keywords  = {classification, point-wise},
  url       = {https://www.cc.gatech.edu/conferences/3DPVT08/Program/Papers/paper200.pdf}
}

@inproceedings{agrawal2009icra,
  author    = {A. Agrawal and A. Nakazawa and H. Takemura},
  title     = {{MMM-classification of 3D Range Data}},
  booktitle = icra,
  year      = {2009},
  keywords  = {3D laser-based, CRF, classification, robotics, point-wise},
  url       = {https://www.cs.princeton.edu/courses/archive/spring11/cos598A/pdfs/Agrawal09.pdf}
}

@inproceedings{munoz2009icra,
  author    = {D. Munoz and N. Vandapel and M. Hebert},
  title     = {{Onboard Contextual Classification of 3-D Point Clouds with Learned
               High-order Markov Random Fields}},
  booktitle = icra,
  year      = {2009},
  keywords  = {classification, point-wise},
  url       = {https://ri.cmu.edu/pub_files/2009/5/munoz_icra_09.pdf}
}

@inproceedings{behley2010iros,
  author    = {J. Behley and K. Kersting and D. Schulz and V. Steinhage
               and Armin B. Cremers},
  title     = {{Learning to Hash Logistic Regression for Fast 3D Scan Point Classification}},
  booktitle = iros,
  year      = {2010},
  keywords  = {classification, 3D laser-based, point-wise, robotics},
  url       = {http://jbehley.github.io/papers/behley2010iros.pdf}
}



@inproceedings{tchapmi2017threedv,
  author    = {L. Tchapmi and C. Choy and I. Armeni and J. Gwak and S. Savarese},
  title     = {{SEGCloud: Semantic Segmentation of 3D Point Clouds}},
  booktitle = threedv,
  year      = {2017},
  url       = {http://segcloud.stanford.edu/segcloud_2017.pdf}
}

@inproceedings{huang2016icpr,
  author    = {Jing Huang and Suya You},
  title     = {{Point Cloud Labeling using 3D Convolutional Neural Network}},
  booktitle = {Proc.~of the Internation Conference on Pattern Recognition (ICPR)},
  year      = 2016,
  url       = {http://www.cvlibs.net/projects/autonomous_vision_survey/literature/Huang2016ICPR.pdf}
}

@inproceedings{maturana2015iros,
  author    = {D. Maturana and S. Scherer},
  title     = {{VoxNet: A 3D Convolutional Neural Network for Real-Time Object Recognition}},
  booktitle = iros,
  year      = 2015,
  url       = {https://www.ri.cmu.edu/pub_files/2015/9/voxnet_maturana_scherer_iros15.pdf}
}

@inproceedings{riegler2017cvpr,
  author    = {G. Riegler and A. Ulusoy and A. Geiger},
  title     = {{OctNet: Learning Deep 3D Representations at High Resolutions}},
  booktitle = cvprold,
  year      = 2017,
  url       = {https://arxiv.org/pdf/1611.05009.pdf}
}

@inproceedings{klokov2017iccv,
  author    = {R. Klukov and V. Lempitsky},
  title     = {{Escape from Cells: Deep Kd-Networks for the Recognition of 3D Point Cloud Models}},
  booktitle = iccvold,
  year      = 2017,
  url       = {http://openaccess.thecvf.com/content_ICCV_2017/papers/Klokov_Escape_From_Cells_ICCV_2017_paper.pdf}
}


@article{johnson1999pami,
  author   = {Andrew Johnson and Martial Hebert},
  title    = {{Using spin images for effcient object recognition in cluttered 3D
              scenes}},
  journal  = pami,
  year     = {1999},
  volume   = {21},
  pages    = {433--449},
  number   = {5},
  keywords = {descriptor},
  url      = {https://pdfs.semanticscholar.org/30c3/e410f689516983efcd780b9bea02531c387d.pdf}
}

@inproceedings{gaidon2016cvpr,
  author    = {A. Gaidon and Q. Wang and Y. Cabon and E. Vig},
  title     = {{Virtual Worlds as Proxy for Multi-Object Tracking Analysis}},
  booktitle = cvprold,
  year      = 2016,
  abstract  = {Modern computer vision algorithms typically require expensive data acquisition
               and accurate manual labeling. In this work, we instead leverage the recent progress in
               computer graphics to generate fully labeled, dynamic, and photo-realistic proxy virtual
               worlds. We propose an efficient real-to-virtual world cloning method, and validate our
               approach by building and publicly releasing a new video dataset, called Virtual KITTI (see
               this http URL), automatically labeled with accurate ground truth for object detection,
               tracking, scene and instance segmentation, depth, and optical flow. We provide quantitative
               experimental evidence suggesting that (i) modern deep learning algorithms pre-trained on
               real data behave similarly in real and virtual worlds, and (ii) pre-training on virtual
               data improves performance. As the gap between real and virtual worlds is small, virtual
               worlds enable measuring the impact of various weather and imaging conditions on recognition
               performance, all other things being equal. We show these factors may affect drastically
               otherwise high-performing deep models for tracking. },
  url       = {https://elib.dlr.de/105154/1/Gaidon_Virtual_Worlds_as_CVPR_2016_paper.pdf}
}


@inproceedings{jampani2016cvpr,
  author    = {V. Jampani and M. Kiefel and P.V. Gehler},
  title     = {{Learning Sparse High Dimensional Filters: Image Filtering, Dense CRFs and Bilateral Neural Networks}},
  booktitle = cvprold,
  year      = 2016,
  url       = {https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Jampani_Learning_Sparse_High_CVPR_2016_paper.pdf}
}


@inproceedings{hua2016threedv,
  author    = {B. Hua and Q. Pham and D. Nguyen and M. Tran and L. Yu and S. Yeung},
  title     = {{SceneNN: A Scene Meshes Dataset with aNNotations}},
  booktitle = threedv,
  year      = {2016},
  url       = {http://www.scenenn.net/pdf/dataset_3dv16.pdf}
}

@inproceedings{mccormac2017iccv,
  author    = {J. McCormac and A. Handa and S. Leutenegger and A.J.Davison},
  title     = {{SceneNet RGB-D: Can 5M Synthetic Images Beat Generic ImageNet Pre-training on Indoor Segmentation?}},
  booktitle = iccvold,
  year      = {2017},
  url       = {https://www.imperial.ac.uk/media/imperial-college/research-centres-and-groups/dyson-robotics-lab/jmccormac_etal_iccv2017.pdf}
}

@inproceedings{li2018bmvc,
  author    = {W. Li and S. Saeedi and J. McCormac and R. Clark and D. Tzoumanikas and Q. Ye and Y. Huang and R. Tang and S. Leutenegger },
  title     = {{InteriorNet: Mega-scale Multi-sensor Photo-realistic Indoor Scenes Dataset}},
  booktitle = bmvc,
  year      = {2018},
  url       = {https://arxiv.org/pdf/1809.00716.pdf}
}

@article{armeni2017arxiv,
  author  = {I. Armeni and A. Sax and A. Zamir and S. Savarese},
  title   = {{Joint 2D-3D-Semantic Data for Indoor Scene Understanding}},
  journal = arxiv,
  volume  = {arXiv:1702.01105},
  year    = 2017,
  url     = {https://arxiv.org/pdf/1702.01105}
}


@inproceedings{hua2018cvpr,
  title     = {{Pointwise Convolutional Neural Networks}},
  author    = {B. Hua and M. Tran and S. Yeung},
  booktitle = cvprold,
  year      = {2018},
  url       = {proceedings:hua2018cvpr.pdf}
}

@inproceedings{groh2018accv,
  author    = {F. Groh and P. Wieschollek and H. Lensch},
  title     = {{Flex-Convolution (Million-Scale Pointcloud Learning Beyond Grid-Worlds)}},
  booktitle = accv,
  year      = {2018},
  url       = {https://arxiv.org/pdf/1803.07289.pdf}
}

@article{boulch2017cg,
  title   = {{SnapNet: 3D point cloud semantic labeling with 2D deep segmentation networks}},
  author  = {Boulch, A. and Guerry, J. and Le Saux, B. and Audebert, N.},
  journal = {Computers \& Graphics},
  volume  = {71},
  pages   = {189--198},
  year    = {2017},
  url     = {http://isiarticles.com/bundles/Article/pre/pdf/148557.pdf}
}

@inproceedings{rethage2018eccv,
  title     = {{Fully-Convolutional Point Networks for Large-Scale Point Clouds}},
  author    = {D. Rethage and J. Wald and J. Sturm and N. Navab and F. Tombari},
  booktitle = eccv,
  year      = {2018},
  url       = {https://eccv2018.org/openaccess/content_ECCV_2018/papers/Dario_Rethage_Fully-Convolutional_Point_Networks_ECCV_2018_paper.pdf}
}

@article{jiang2018arxiv,
  title   = {{PointSIFT: A SIFT-like Network Module for 3D Point Cloud Semantic Segmentation}},
  author  = {M. Jiang and Y. Wu and C. Lu},
  journal = arxiv,
  year    = {2018},
  volume  = {arXiv:1807.00652},
  url     = {https://arxiv.org/pdf/1807.00652}
}

@article{wang2018arxiv,
  author  = {Zo. Wang and F. Lu},
  title   = {{VoxSegNet: Volumetric CNNs for Semantic Part Segmentation of 3D Shapes}},
  journal = arxiv,
  year    = {2018},
  volume  = {arXiv:1809.00226},
  url     = {https://arxiv.org/pdf/1809.00226}
}

@article{mei2018arxiv,
  title   = {{Semantic Segmentation of 3D LiDAR Data in Dynamic Scene Using Semi-supervised Learning}},
  author  = {J. Mei and B. Gao and D. Xu and W. Yao and X. Zhao and H. Zhao},
  journal = arxiv,
  year    = {2018},
  volume  = {arXiv:1809.00426},
  url     = {https://arxiv.org/pdf/1809.00426}
}

@article{engelmann2018arxiv,
  title   = {{Know What Your Neighbors Do: 3D Semantic Segmentation of Point Clouds}},
  author  = {F. Engelmann and T. Kontogianni and J. Schult and B. Leibe},
  journal = arxiv,
  year    = {2018},
  volume  = {arXiv:1810.01151},
  url     = {https://arxiv.org/pdf/1810.01151}
}

@inproceedings{engelmann2020cvpr,
  author    = {F. Engelmann and M. Bokeloh and A. Fathi and B. Leibe and M. Niessner},
  title     = {{3D-MPA: Multi-Proposal Aggregation for 3D Semantic Instance Segmentation}},
  booktitle = cvpr,
  year      = 2020,
  abstract  = {We present 3D-MPA, a method for instance segmentation on 3D point clouds. Given an input point cloud, we propose an object-centric approach where each point votes for its object center. We sample object proposals from the predicted object centers. Then, we learn proposal features from grouped point features that voted for the same object center. A graph convolutional network introduces inter-proposal relations, providing higher-level feature learning in addition to the lower-level point features. Each proposal comprises a semantic label, a set of associated points over which we define a foreground-background mask, an objectness score and aggregation features. Previous works usually perform non-maximum-suppression (NMS) over proposals to obtain the final object detections or semantic instances. However, NMS can discard potentially correct predictions. Instead, our approach keeps all proposals and groups them together based on the learned aggregation features. We show that grouping proposals improves over NMS and outperforms previous state-of-the-art methods on the tasks of 3D object detection and semantic instance segmentation on the ScanNetV2 benchmark and the S3DIS dataset.},
  url       = {proceedings: engelmann2020cvpr.pdf}
}

@inproceedings{graham2018cvpr,
  title     = {{3D Semantic Segmentation with Submanifold Sparse Convolutional Networks}},
  author    = {B. Graham and M. Engelcke and L. van der Maaten},
  booktitle = cvprold,
  year      = {2018},
  url       = {http://openaccess.thecvf.com/content_cvpr_2018/papers/Graham_3D_Semantic_Segmentation_CVPR_2018_paper.pdf}
}

@article{zeng2017arxiv,
  title   = {{3DContextNet: K-d Tree Guided Hierarchical Learning of Point Clouds Using Local and Global Contextual Cues}},
  author  = {W. Zeng and T. Gevers},
  journal = arxiv,
  year    = {2017},
  volume  = {arXiv:1711.11379},
  url     = {https://arxiv.org/pdf/1711.11379}
}

@article{sung2017arxiv,
  title   = {{Deep Functional Dictionaries: Learning Consistent Semantic Structures on 3D Models from Functions}},
  author  = {M. Sung and H. Su and R. Yu and L. Guibas},
  journal = arxiv,
  year    = {2018},
  volume  = {arXiv:1805.09957},
  url     = {https://arxiv.org/pdf/1805.09957}
}

@article{te2018arxiv,
  title   = {{RGCNN: Regularized Graph CNN for Point Cloud Segmentation}},
  author  = {G. Te and W. Hu and Z. Guo and A. Zheng},
  journal = arxiv,
  year    = {2018},
  volume  = {arXiv:1806.02952},
  url     = {https://arxiv.org/pdf/1806.02952}
}

@book{pearl2014book,
  title     = {{Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference}},
  author    = {J. Pearl},
  year      = {2014},
  publisher = {Elsevier}
}

@inproceedings{furtlehner2007tits,
  title     = {{A Belief Propagation Approach to Traffic Prediction using Probe Vehicles}},
  author    = {C. Furtlehner and J.M. Lasgouttes and D. de La Fortelle},
  booktitle = tits,
  year      = {2007}
}

@inproceedings{yedidia2001nips,
  title     = {Generalized Belief Propagation},
  author    = {J. Yedidia and W. Freeman and Y. Weiss},
  booktitle = nips,
  year      = {2001}
}

@article{fried2013tcs,
  title   = {Complexity of Canadian Traveler Problem Variants},
  author  = {D. Fried and S. Shimony and A. Benbassat and C. Wenner},
  journal = {Theoretical Computer Science},
  volume  = {487},
  pages   = {1--16},
  year    = {2013}
}

@article{papadimitriou1991tcs,
  title   = {Shortest Paths Without a Map},
  author  = {C. Papadimitriou and M. Yannakakis},
  journal = {Theoretical Computer Science},
  volume  = {84},
  number  = {1},
  pages   = {127--150},
  year    = {1991}
}

@inproceedings{kocsis2006ecml,
  title     = {Bandit based Monte-Carlo Planning},
  author    = {L. Kocsis and C. Szepesv{\'a}ri},
  booktitle = ecml,
  year      = {2006}
}

@inproceedings{pereira2011iros,
  title     = {Toward Risk Aware Mission Planning for Autonomous Underwater Vehicles},
  author    = {A. Pereira and J. Binney and B. Jones and M. Ragan and G. Sukhatme},
  booktitle = iros,
  year      = {2011}
}

@inproceedings{krause2006ipsn,
  title     = {Near-optimal Sensor Placements: Maximizing Information while Minimizing Communication Cost},
  author    = {A. Krause and C. Guestrin and A. Gupta and J. Kleinberg},
  booktitle = {Proc.~of the Intl.~Conf.~on Information Processing in Sensor Networks},
  year      = {2006}
}

@inproceedings{meliou2007aaai,
  title     = {Nonmyopic Informative Path Planning in Spatio-temporal Models},
  author    = {A. Meliou and A. Krause and C. Guestrin and J. Hellerstein},
  booktitle = aaai,
  year      = {2007}
}

@article{fox1997jra,
  title   = {The Dynamic Window Approach to Collision Avoidance},
  author  = {D. Fox and W. Burgard and S. Thrun},
  journal = jra,
  volume  = {4},
  number  = {1},
  pages   = {23--33},
  year    = {1997}
}

@inproceedings{fentanes2015icra,
  title     = {Now or later? Predicting and Maximising Success of Navigation Actions from Long-Term Experience},
  author    = {J. Fentanes and B. Lacerda and T. Krajn{\'\i}k and N. Hawes and M. Hanheide},
  booktitle = icra,
  year      = {2015}
}

@article{cummins2008ijrr,
  title   = {{FAB-MAP}: Probabilistic Localization and Mapping in the Space of Appearance},
  author  = {M. Cummins and P. Newman},
  journal = ijrr,
  volume  = {27},
  number  = {6},
  pages   = {647--665},
  year    = {2008}
}

@article{chow1968tit,
  title   = {Approximating Discrete Probability Distributions with Dependence Trees},
  author  = {C. Chow and C. Liu},
  journal = {IEEE Trans.~on Information Theory},
  volume  = {14},
  number  = {3},
  pages   = {462--467},
  year    = {1968}
}

@inproceedings{krajnik2014icra,
  title     = {Spectral analysis for long-term robotic mapping},
  author    = {T. Krajnik and J. Fentanes and G. Cielniak and C. Dondrup and T. Duckett},
  booktitle = icra,
  year      = {2014}
}

@article{chung2018ijrr,
  title   = {{Risk-aware Graph Search with Dynamic Edge Cost Discovery}},
  author  = {J.J. Chungn and A. Smith and R. Skeele and G. Hollinger},
  journal = ijrr,
  volume  = {38},
  number  = {2--3},
  pages   = {182--195},
  year    = {2018}
}

@article{murphy2013tra,
  title   = {Risky Planning on Probabilistic Costmaps for Path Planning in Outdoor Environments},
  author  = {L. Murphy and P. Newman},
  journal = tra,
  volume  = {29},
  number  = {2},
  pages   = {445--457},
  year    = {2013}
}

@inproceedings{lim2017uai,
  title     = {Shortest Path under Uncertainty: Exploration versus Exploitation.},
  author    = {Z.W. Lim and D. Hsu and W.S. Lee and W. Sun},
  booktitle = uai,
  year      = {2017}
}

@inproceedings{eyerich2010aaai,
  title     = {High-quality Policies for the Canadian Traveler's Problem},
  author    = {P. Eyerich and T. Keller and M. Helmert},
  booktitle = aaai,
  year      = {2010}
}

@article{mooij2010jmlr,
  author  = {J. Mooij},
  title   = {lib{DAI}: A Free and Open Source {C++} Library for Discrete Approximate Inference in Graphical Models},
  journal = {Journal of Machine Learning Research},
  year    = {2010},
  volume  = {11},
  url     = {http://www.jmlr.org/papers/volume11/mooij10a/mooij10a.pdf}
}

@inproceedings{kucner2013iros,
  title     = {Conditional Transition Maps: Learning Motion Patterns in Dynamic Environments},
  author    = {T. Kucner and J. Saarinen and M. Magnusson and A. Lilienthal},
  booktitle = iros,
  year      = {2013}
}

@article{haigh1999jras,
  title   = {Learning Situation-dependent Costs: Improving Planning from Probabilistic Robot Execution},
  author  = {K. Haigh and M. Veloso},
  journal = jras,
  volume  = {29},
  number  = {2--3},
  pages   = {145--174},
  year    = {1999}
}

@article{binney2013ijrr,
  title   = {Optimizing Waypoints for Monitoring Spatiotemporal Phenomena},
  author  = {J. Binney and A. Krause and G. Sukhatme},
  journal = ijrr,
  volume  = {32},
  number  = {8},
  pages   = {873--888},
  year    = {2013}
}

@inproceedings{nardi2019icra-uapp,
  author    = {L. Nardi and C. Stachniss},
  title     = {{Uncertainty-Aware Path Planning for Navigation on Road Networks Using Augmented MDPs}},
  booktitle = icra,
  year      = 2019
}

@inproceedings{nardi2019icra-airn,
  author    = {L. Nardi and C. Stachniss},
  title     = {{Actively Improving Robot Navigation On Different Terrains Using Gaussian Process Mixture Models}},
  booktitle = icra,
  year      = 2019
}

@inproceedings{nardi2018irosws,
  title     = {Towards Uncertainty-Aware Path Planning for Navigation on Road Networks Using Augmented MDPs},
  author    = {L. Nardi and C. Stachniss},
  booktitle = {Workshop on Planning, Perception and Navigation for Intelligent Vehicles at the IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (IROS)},
  year      = {2018},
  videourl  = {https://youtu.be/SLp5YVplJAQ}
}


@inproceedings{furgale2014icraws,
  title     = {There and Back Again--Dealing with highly-dynamic scenes and long-term change during topological/metric route following},
  author    = {P. Furgale and P. Kr{\"u}si and F. Pomerleau and U. Schwesinger and F. Colas and R. Siegwart},
  booktitle = {Workshop~on Modelling, Estimation, Perception, and Control of All Terrain Mobile Robots},
  year      = {2014}
}

@inproceedings{jiang2007iros,
  title     = {Learning humanoid reaching tasks in dynamic environments},
  author    = {X. Jiang and M. Kallmann},
  booktitle = iros,
  year      = {2007}
}

@inproceedings{jetchev2009icml,
  title     = {Trajectory prediction: learning to map situations to robot trajectories},
  author    = {N. Jetchev and M. Toussaint},
  booktitle = icml,
  year      = {2009}
}

@inproceedings{jetchev2010icra,
  title     = {Trajectory Prediction in Cluttered Voxel Environments},
  author    = {N. Jetchev and M. Toussaint},
  booktitle = icra,
  year      = {2010}
}

@article{sucan2012ram,
  title   = {The {O}pen {M}otion {P}lanning {L}ibrary},
  author  = {I. {\c{S}}ucan and M. Moll and L. Kavraki},
  journal = ram,
  number  = {4},
  pages   = {72--82},
  volume  = {19},
  year    = {2012}
}

@inproceedings{phillips2012rss,
  title     = {E-Graphs: Bootstrapping Planning with Experience Graphs},
  author    = {M. Phillips and B. Cohen and S. Chitta and M. Likhachev},
  booktitle = rss,
  year      = {2012}
}

@inproceedings{berenson2012icra,
  title     = {A robot path planning framework that learns from experience},
  author    = {D. Berenson and P. Abbeel and K. Goldberg},
  booktitle = icra,
  year      = {2012}
}

@inproceedings{kuffner2000icra,
  title     = {RRT-connect: An efficient approach to single-query path planning},
  author    = {J. Kuffner and S. LaValle},
  booktitle = icra,
  year      = {2000}
}

@inproceedings{lien2009rss,
  title     = {Planning motion in environments with similar obstacles},
  author    = {J. Lien and Y. Lu},
  booktitle = rss,
  year      = {2009}
}

@inproceedings{sprunk2013iros,
  title     = {Lidar-based teach-and-repeat of mobile robot trajectories},
  author    = {C. Sprunk and G. D. Tipaldi and A. Cherubini and W. Burgard},
  booktitle = iros,
  year      = {2013}
}

@article{delsart2009cit,
  title   = {Navigating dynamic environments with trajectory deformation},
  author  = {V. Delsart and T. Fraichard},
  journal = {Journal of Computing and Information Technology},
  volume  = {17},
  number  = {1},
  pages   = {27--36},
  year    = {2009}
}

@inproceedings{bruce2002iros,
  title     = {Real-Time Randomized Path Planning for Robot Navigation},
  author    = {J. Bruce and M. Veloso},
  booktitle = iros,
  year      = {2002}
}

@inproceedings{zucker2008icra,
  title     = {Adaptive Workspace Biasing for Sampling-based Planners},
  author    = {M. Zucker and J. Kuffner and A. Bagnell},
  booktitle = icra,
  year      = {2008}
}

@article{mericcli2015jirs,
  title   = {A Case-Based Approach to Mobile Push-Manipulation},
  author  = {T. Meri{\c{c}}li and M. Veloso and H. L. Ak{\i}n},
  journal = jirs,
  volume  = {80},
  number  = {1},
  pages   = {189--203},
  year    = {2015}
}

@inproceedings{ros2007iccbr,
  title     = {Team Playing Behavior in Robot Soccer: A Case-based Reasoning Approach},
  author    = {R. Ros and R. L. De M{\`a}ntaras and J. L. Arcos and M. Veloso},
  booktitle = {Proc.~of the Intl. Conf.~on Case-Based Reasoning},
  year      = {2007}
}

@inproceedings{quinlan1993icra,
  title     = {Elastic Bands: Connecting Path Planning and Control},
  author    = {S. Quinlan and O. Khatib},
  booktitle = icra,
  year      = {1993}
}

@inproceedings{rosmann2012robotik,
  title     = {Trajectory Modification Considering Dynamic Constraints of Autonomous Robots},
  author    = {C. R{\"o}smann and W. Feiten and T. W{\"o}sch and F. Hoffmann and T. Bertram},
  booktitle = {Proc.~of German Conf.~on Robotics (ROBOTIK)},
  year      = {2012}
}

@article{keller2014ifac,
  title   = {Planning of Optimal Collision Avoidance Trajectories with Timed Elastic Bands},
  author  = {M. Keller and F. Hoffmann and C. Hass and T. Bertram and A. Seewald},
  journal = {IFAC Proceedings Volumes},
  volume  = {47},
  number  = {3},
  pages   = {9822--9827},
  year    = {2014}
}

@article{lavalle2011ram,
  title   = {Motion Planning},
  author  = {S. LaValle},
  journal = ram,
  volume  = {18},
  number  = {2},
  pages   = {108--118},
  year    = {2011}
}

@techreport{lavalle1998rrt,
  title       = {Rapidly-exploring Random Trees: A New Tool for Path Planning},
  author      = {S. LaValle},
  institution = {Iowa State University},
  year        = {1998}
}

@inproceedings{kuderer2013iros,
  title     = {Teaching Mobile Robots to Cooperatively Navigate in Populated Environments},
  author    = {M. Kuderer and H. Kretzschmar and W. Burgard},
  booktitle = iros,
  year      = {2013}
}

@inproceedings{pfeiffer2016iros,
  title     = {Predicting Actions to Act Predictably: Cooperative Partial Motion Planning with Maximum Entropy Models},
  author    = {M. Pfeiffer and U. Schwesinger and H. Sommer and E. Galceran and R. Siegwart},
  booktitle = iros,
  year      = {2016}
}

@article{bennewitz2005ijrr,
  title   = {Learning Motion Patterns of People for Compliant Robot Motion},
  author  = {M. Bennewitz and W. Burgard and G. Cielniak and S. Thrun},
  journal = ijrr,
  volume  = {24},
  number  = {1},
  pages   = {31--48},
  year    = {2005}
}

@article{kruse2013jras,
  title   = {Human-aware Robot Navigation: A Survey},
  author  = {T. Kruse and A. K. Pandey and R. Alami and A. Kirsch},
  journal = jras,
  volume  = {61},
  number  = {12},
  pages   = {1726--1743},
  year    = {2013}
}

@inproceedings{ziebart2009iros,
  title     = {Planning-based Prediction for Pedestrians},
  author    = {B. Ziebart and N. Ratliff and G. Gallagher and C. Mertz and K. Peterson and A. Bagnell and M. Hebert and A. Dey and S. Srinivasa},
  booktitle = iros,
  year      = {2009}
}

@inproceedings{trautman2013icra,
  title     = {Robot Navigation in Dense Human Crowds: The Case for Cooperation},
  author    = {P. Trautman and J. Ma and R. Murray and A. Krause},
  booktitle = icra,
  year      = {2013}
}

@inproceedings{fulgenzi2008iros,
  title     = {Probabilistic Navigation in Dynamic Environment Using Rapidly-exploring Random Trees and Gaussian Processes},
  author    = {C. Fulgenzi and C. Tay and A. Spalanzani and C. Laugier},
  booktitle = iros,
  year      = {2008}
}

@inproceedings{ellis2009iccvws,
  title     = {Modelling Pedestrian Trajectory Patterns with Gaussian Processes},
  author    = {D. Ellis and E. Sommerlade and I. Reid},
  booktitle = iccvws,
  year      = {2009}
}

@article{bescos2018ral,
  author  = {Bescos, B. and FÃ¡cil, J.M. and Civera, J. and Neira, J.},
  journal = ral,
  title   = {{DynaSLAM: Tracking, Mapping, and Inpainting in Dynamic Scenes}},
  year    = {2018},
  volume  = {3},
  number  = {4},
  pages   = {4076--4083},
  url     = {https://arxiv.org/pdf/1806.05620.pdf}
}

@inproceedings{ruenz2018ismar,
  author    = {R{\"u}nz, M. and Buffier, M. and Agapito, L.},
  booktitle = ismar,
  title     = {{MaskFusion: Real-Time Recognition, Tracking and Reconstruction of Multiple Moving Objects}},
  year      = {2018},
  url       = {https://arxiv.org/pdf/1804.09194.pdf}
}

@inproceedings{mishkin2015cvpr,
  title     = {Place Recognition with WxBS Retrieval},
  author    = {Mishkin, D. and Perdoch, M. and Matas, J.},
  booktitle = cvprold,
  year      = 2015
}

@article{gomez-ojeda2015arxiv,
  author   = {R. Gomez-Ojeda and M. Lopez-Antequera and N. Petkov and J. Gonzalez-Jimenez},
  title    = {{Training a Convolutional Neural Network for Appearance-Invariant Place Recognition}},
  journal  = arxiv,
  year     = 2015,
  volume   = {arXiv:1505.07428},
  url      = {http://arxiv.org/pdf/1505.07428v1},
  abstract = {Place recognition is one of the most challenging problems in computer vision, and has become a key part in mobile robotics and autonomous driving applications for performing loop closure in visual SLAM systems. Moreover, the difficulty of recognizing a revisited location increases with appearance changes caused, for instance, by weather or illumination variations, which hinders the long-term application of such algorithms in real environments. In this paper we present a convolutional neural network (CNN), trained for the first time with the purpose of recognizing revisited locations under severe appearance changes, which maps images to a low dimensional space where Euclidean distances represent place dissimilarity. In order for the network to learn the desired invariances, we train it with triplets of images selected from datasets which present a challenging variability in visual appearance. The triplets are selected in such way that two samples are from the same location and the third one is taken from a different place. We validate our system through extensive experimentation, where we demonstrate better performance than state-of-art algorithms in a number of popular datasets.}
}
@inproceedings{haehnel2003isrr,
  author    = {H{\"a}hnel, D. and Burgard, W. and Wegbreit, B. and Thrun., S.},
  booktitle = isrr,
  title     = {Towards lazy data association in SLAM},
  year      = 2003
}

@article{galvez2012tro,
  author  = {D. Galvez-Lopez and J. D. Tardos},
  journal = tro,
  title   = {{Bags of Binary Words for Fast Place Recognition in Image Sequences}},
  volume  = {28},
  number  = {5},
  pages   = {1188--1197},
  year    = {2012}
}


@article{pacheco2012ai,
  author  = {Fuentes-Pacheco, J. and Ruiz-Ascencio, J. and Rend\'on-Mancha, J.M.},
  title   = {Visual simultaneous localization and mapping: a survey},
  journal = ai,
  volume  = {43},
  pages   = {1--27},
  year    = {2012}
}

@inproceedings{kendall2015iccv,
  author    = {A. Kendall and M. Grimes and R. Cipolla},
  title     = {{PoseNet: A Convolutional Network for Real-Time 6-DOF Camera Relocalization}},
  booktitle = iccvold,
  year      = 2015,
  url       = {proceedings:kendall2015iccv.pdf}
}

@article{unger2016isprs,
  author  = {Unger, J. and Rottensteiner, F. and Heipke, C.},
  title   = {INTEGRATION OF A GENERALISED BUILDING MODEL INTO THE POSE ESTIMATION OF UAS IMAGES},
  journal = isprsarchives,
  volume  = {XLI-B1},
  year    = {2016},
  pages   = {1057--1064},
  url     = {http://www.int-arch-photogramm-remote-sens-spatial-inf-sci.net/XLI-B1/1057/2016/}
}


@article{gerke2011isprs,
  title   = {Using horizontal and vertical building structure to constrain indirect sensor orientation},
  author  = {Gerke, Markus},
  journal = isprsannals,
  volume  = {66},
  number  = {3},
  pages   = {307--316},
  year    = {2011}
}

@inbook{forstner2016springer,
  author    = {F{\"o}rstner, W. and Wrobel, B.},
  title     = {Photogrammetric Computer Vision},
  chapter   = {Robust estimation and outlier detection},
  publisher = springer,
  pages     = {141--159},
  year      = {2016}
}

@article{douglas1973ijgig,
  title   = {Algorithms for the reduction of the number of points required to represent a digitized line or its caricature},
  author  = {Douglas, David H and Peucker, Thomas K},
  journal = {Cartographica: The International Journal for Geographic Information and Geovisualization},
  volume  = {10},
  number  = {2},
  pages   = {112--122},
  year    = {1973}
}

@article{fischer1998cviu,
  title   = {Extracting buildings from aerial images using hierarchical aggregation in 2D and 3D},
  author  = {Fischer, Andr{\'e} and Kolbe, Thomas H and Lang, Felicitas and Cremers, Armin B and F{\"o}rstner, Wolfgang and Pl{\"u}mer, Lutz and Steinhage, Volker},
  journal = {Computer Vision and Image Understanding},
  volume  = {72},
  number  = {2},
  pages   = {185--203},
  year    = {1998}
}

@inproceedings{huber2003rsdfua,
  author    = {M. Huber and W. Schickler and S. Hinz and A. Baumgartner},
  booktitle = {GRSS/ISPRS Joint Workshop on Remote Sensing and Data Fusion over Urban Areas},
  title     = {{Fusion of LIDAR data and aerial imagery for automatic reconstruction of building surfaces}},
  year      = {2003}
}

@inproceedings{wulf2004icra,
  title     = {{2D mapping of cluttered indoor environments by means of 3D perception}},
  author    = {Wulf, O. and Arras, K. O and Christensen, H. I. and Wagner, B.},
  booktitle = icra,
  year      = {2004}
}

@inproceedings{hentschel2008iros,
  title     = {{A GPS and laser-based localization for urban and non-urban outdoor environments}},
  author    = {Hentschel, M. and Wulf, O. and Wagner, B.},
  booktitle = iros,
  year      = {2008}
}


@article{lu1997ar,
  author  = {Lu, F. and Milios, E.},
  title   = {{Globally Consistent Range Scan Alignment for Environment Mapping}},
  journal = ar,
  year    = {1997},
  volume  = {4},
  pages   = {333--349}
}

@inproceedings{gutmann2000cira,
  author    = {J.-S. Gutmann and K. Konolige},
  title     = {Incremental Mapping of Large Cyclic Environments},
  booktitle = cira,
  year      = {2000}
}

@article{bailey2006ram1,
  author   = {T. Bailey and H.F. Durrant-Whyte},
  title    = {Simultaneous Localisation and Mapping ({SLAM}): Part {I}},
  journal  = ram,
  year     = 2006,
  volume   = {13},
  number   = {2},
  pages    = {99--110},
  keywords = {computational complexity;data
              association;environment representation;mobile
              robot;probability distributions;recursive Bayesian
              formulation;simultaneous localization and
              mapping;vehicle pose;Bayes methods;computational
              complexity;mobile robots;path planning;statistical
              distributions;}
}

@article{bailey2006ram2,
  author   = {T. Bailey and H.F. Durrant-Whyte},
  title    = {Simultaneous Localisation and Mapping ({SLAM}): Part {II}},
  journal  = ram,
  year     = 2006,
  volume   = {13},
  number   = {3},
  pages    = {108--117},
  keywords = {computational complexity;data
              association;environment representation;mobile
              robot;probability distributions;recursive Bayesian
              formulation;simultaneous localization and
              mapping;vehicle pose;Bayes methods;computational
              complexity;mobile robots;path planning;statistical
              distributions;}
}

@article{furbank2019np,
  author   = {R.T. Furbank and J.A. Jimenez-Berni and B. George-Jaeggli and A.B. Potgieter and D.M.   Deery},
  title    = {Field crop phenomics: enabling breeding for radiation use efficiency and biomass in cereal crops},
  journal  = {New Phytologist},
  volume   = {223},
  number   = {4},
  pages    = {1714--1727},
  year     = 2019,
  keywords = {big data, canopy temperature, crop breeding, crop physiology, photosynthesis, sorghum,  stomatal conductance, wheat},
  url      = {https://nph.onlinelibrary.wiley.com/doi/abs/10.1111/nph.15817}
}

@inproceedings{izadi2011acmsuist,
  title     = {{KinectFusion: Real-time 3D Reconstruction and Interaction using a Moving Depth Camera}},
  author    = {Izadi, S. and Kim, D. and Hilliges, O. and Molyneaux, D. and Newcombe, R. and Kohli, P. and Shotton, J. and Hodges, S. and Freeman, D. and Davison, A. and Fitzgibbon, A.},
  booktitle = acmsuist,
  year      = {2011},
  url       = {https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/kinectfusion-uist-comp.pdf}
}

@inproceedings{whelan2012rssws,
  title     = {{Kintinuous: Spatially Extended KinectFusion}},
  author    = {Whelan, T. and Kaess, M. and Fallon, M. and Johannsson, H. and Leonard, J. and McDonald, J.},
  year      = {2012},
  booktitle = {Proc. RSS Workshop on RGB-D: Advanced Reasoning with Depth Cameras},
  url       = {http://www.thomaswhelan.ie/Whelan12rssw.pdf}
}

@article{chen2013acmgraphics,
  title   = {{Scalable Real-Time Volumetric Surface Reconstruction}},
  author  = {Chen, J. and Bautembach, D. and Izadi, S.},
  journal = acmgraphics,
  volume  = {32},
  number  = {4},
  pages   = {113},
  year    = {2013},
  url     = {http://delivery.acm.org/10.1145/2470000/2461940/a113-chen.pdf?ip=131.220.233.117&id=2461940&acc=ACTIVE%20SERVICE&key=2BA2C432AB83DA15%2E56D2680C9BA0337E%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&__acm__=1540296726_9f3a3c6ef254b213e1c5eb2cd8dd2a1a}
}

@article{dai2017acmgraphics,
  title   = {{BundleFusion: Real-Time Globally Consistent 3D Reconstruction using On-the-fly Surface Reintegration}},
  author  = {Dai, A. and Nie{\ss}ner, M. and Zollh{\"o}fer, M. and Izadi, S. and Theobalt, C.},
  journal = acmgraphics,
  volume  = {36},
  number  = {4},
  pages   = {76a},
  year    = {2017},
  url     = {https://arxiv.org/pdf/1604.01093.pdf}
}

@inproceedings{steinbruecker2013iccv,
  title     = {{Large-Scale Multi-Resolution Surface Reconstruction from RGB-D Sequences}},
  author    = {Steinbrucker, F. and Kerl, C. and Cremers, D.},
  booktitle = iccvold,
  year      = {2013},
  url       = {https://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Steinbrucker_Large-Scale_Multi-resolution_Surface_2013_ICCV_paper.pdf}
}

@article{sarbolandi2015cviu,
  title   = {{Kinect Range Sensing: Structured-Light versus Time-of-Flight Kinect}},
  author  = {Sarbolandi, H. and Lefloch, D. and Kolb, A.},
  journal = cviu,
  volume  = {139},
  pages   = {1--20},
  year    = {2015},
  url     = {https://arxiv.org/pdf/1505.05459.pdf}
}

@article{zhou2013acmgraphics,
  title   = {{Dense Scene Reconstruction with Points of Interest}},
  author  = {Zhou, Q. and Koltun, V.},
  journal = acmgraphics,
  volume  = {32},
  number  = {4},
  pages   = {112},
  year    = {2013},
  url     = {https://www.researchgate.net/profile/Vladlen_Koltun/publication/256663515_Dense_Scene_Reconstruction_with_Points_of_Interest/links/00463523921580a3f2000000.pdf}
}

@inproceedings{zhou2013iccv,
  title     = {{Elastic Fragments for Dense Scene Reconstruction}},
  author    = {Zhou, Q. and Miller, S. and Koltun, V.},
  booktitle = iccvold,
  year      = {2013},
  url       = {https://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Zhou_Elastic_Fragments_for_2013_ICCV_paper.pdf}
}

@inproceedings{choi2015cvpr,
  title     = {{Robust Reconstruction of Indoor Scenes}},
  author    = {Choi, S. and Zhou, Q. and Koltun, V.},
  booktitle = cvprold,
  year      = {2015},
  url       = {https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Choi_Robust_Reconstruction_of_2015_CVPR_paper.pdf}
}

@inproceedings{slavcheva2016eccv,
  title     = {{SDF-2-SDF: Highly Accurate 3D Object Reconstruction}},
  author    = {Slavcheva, M. and Kehl, W. and Navab, N. and Ilic, S.},
  booktitle = eccv,
  year      = {2016},
  url       = {http://campar.in.tum.de/pub/slavcheva2016eccv/slavcheva2016eccv.pdf}
}

@inproceedings{slavcheva2016bmvc,
  title     = {{SDF-TAR: Parallel Tracking and Refinement in RGB-D Data using Volumetric Registration.}},
  author    = {Slavcheva, M. and Ilic, S.},
  booktitle = bmvc,
  year      = {2016},
  url       = {http://campar.in.tum.de/pub/slavcheva2016bmvc/slavcheva2016bmvc.pdf}
}

@inproceedings{mccormac2018threedv,
  title     = {{Fusion++: Volumetric Object-Level SLAM}},
  author    = {McCormac, J. and Clark, R. and Bloesch, M. and Davison, A. and Leutenegger, S.},
  booktitle = threedv,
  year      = {2018},
  url       = {https://arxiv.org/pdf/1808.08378.pdf}
}

@inproceedings{millane2018iros,
  title     = {{C-Blox: A Scalable and Consistent TSDF-based Dense Mapping Approach}},
  author    = {Millane, A. and Taylor, Z. and Oleynikova, H. and Nieto, J. and Siegwart, R. and Cadena, C.},
  booktitle = iros,
  year      = {2018},
  url       = {https://www.researchgate.net/profile/Alexander_Millane/publication/320516974_C-blox_A_Scalable_and_Consistent_TSDF-based_Dense_Mapping_Approach/links/5afaef75aca272e7302a973c/C-blox-A-Scalable-and-Consistent-TSDF-based-Dense-Mapping-Approach.pdf}
}

@inproceedings{andreasson2007iros,
  title     = {{Has Somethong Changed Here? Autonomous Difference Detection for Security Patrol Robots}},
  author    = {Andreasson, H. and Magnusson, M. and Lilienthal, A.},
  booktitle = iros,
  year      = {2007},
  url       = {https://www.researchgate.net/profile/Henrik_Andreasson/publication/224296623_Has_Something_Changed_Here_Autonomous_Difference_Detection_for_Security_Patrol_Robots/links/02bfe50c9b74a32647000000/Has-Something-Changed-Here-Autonomous-Difference-Detection-for-Security-Patrol-Robots.pdf}
}

@inproceedings{finman2013ecmr,
  title     = {{Toward Lifelong Object Segmentation from Change Detection in Dense RGB-D Maps}},
  author    = {Finman, R. and Whelan, T. and Kaess, M. and Leonard, J.J.},
  booktitle = ecmr,
  year      = {2013},
  url       = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.640.6828&rep=rep1&type=pdf}
}

@inproceedings{ambrucs2014iros,
  title     = {{Meta-Rooms: Building and Maintaining Long Term Spatial Models in a Dynamic World}},
  author    = {Ambru{\c{s}}, R. and Bore, N. and Folkesson, J. and Jensfelt, P.},
  booktitle = iros,
  year      = {2014},
  url       = {http://www.diva-portal.org/smash/get/diva2:767282/FULLTEXT01.pdf}
}

@article{chen2016neuroc,
  title   = {{Building Change Detection with RGB-D Map Generated from UAV Images}},
  author  = {Chen, B. and Chen, Z. and Deng, L. and Duan, Y. and Zhou, J.},
  journal = neuroc,
  volume  = {208},
  pages   = {350--364},
  year    = {2016}
}

@article{xiao2015jprs,
  title   = {{Street Environment Change Detection from Mobile Laser Scanning Point Clouds}},
  author  = {Xiao, W. and Vallet, B. and Br{\'e}dif, M. and Paparoditis, N.},
  journal = jprs,
  volume  = {107},
  pages   = {38--49},
  year    = {2015}
}

@article{crispell2012tgrs,
  title   = {{A Variable-Resolution Probabilistic Three-Dimensional Model for Change Detection}},
  author  = {Crispell, D. and Mundy, J. and Taubin, G.},
  journal = tgrs,
  volume  = {50},
  number  = {2},
  pages   = {489--500},
  year    = {2012},
  url     = {https://www.researchgate.net/profile/Gabriel_Taubin/publication/224248818_A_Variable-Resolution_Probabilistic_Three-Dimensional_Model_for_Change_Detection/links/02bfe5105fdf685796000000/A-Variable-Resolution-Probabilistic-Three-Dimensional-Model-for-Change-Detection.pdf}
}

@article{choi2009isprsarchives,
  title   = {{A Feature Based Approach to Automatic Change Detection from LiDAR Data in Urban Areas}},
  author  = {Choi, K. and Lee, I. and Kim, S.},
  journal = isprsarchives,
  volume  = {18},
  pages   = {259--264},
  year    = {2009},
  url     = {http://www.isprs.org/proceedings/Xxxviii/3-W8/papers/259_laserscanning09.pdf}
}

@article{dini2012isprsarchives,
  title   = {{3D Building Change Detection using High Resolution Stereo Images and a GIS Database}},
  author  = {Dini, G.R. and Jacobsen, K. and Rottensteiner, F. and Al Rajhi, M. and Heipke, C.},
  journal = isprsarchives,
  volume  = {39},
  pages   = {299--304},
  year    = {2012},
  url     = {https://www.repo.uni-hannover.de/bitstream/handle/123456789/1362/isprsarchives-XXXIX-B7-299-2012.pdf}
}

@article{girardeau2005isprsarchives,
  title   = {{Change Detection on Points Cloud Data Acquired with a Ground Laser Scanner}},
  author  = {Girardeau-Montaut, D. and Roux, M. and Marc, R. and Thibault, G.},
  journal = isprsarchives,
  volume  = {36},
  number  = {part 3},
  pages   = {W19},
  year    = {2005},
  url     = {https://www.researchgate.net/profile/Michel_Roux2/publication/228684497_Change_detection_on_point_cloud_data_acquired_with_a_ground_laser_scanner/links/02e7e5293d07101912000000/Change-detection-on-point-cloud-data-acquired-with-a-ground-laser-scanner.pdf}
}

@inproceedings{heller2001ssns,
  title     = {{Framework for Robust 3D Change Detection}},
  author    = {Heller, A.J. and Leclerc, Y.G. and Luong, Q.},
  booktitle = ssns,
  year      = {2001},
  url       = {https://pdfs.semanticscholar.org/efe1/375d1dad0eb3b720ca113933f6e88ec2da1c.pdf}
}

@article{hussain2013jprs,
  title   = {{Change Detection from Remotely Sensed Images: From Pixel-Based to Object-Based Approaches}},
  author  = {Hussain, M. and Chen, D. and Cheng, A. and Wei, H. and Stanley, D.},
  journal = jprs,
  volume  = {80},
  pages   = {91--106},
  year    = {2013},
  url     = {https://www.ec.gc.ca/glaces-ice/0F9E03B9-1146-4AD4-A13B-AE9E07C9F72B/1.change%20detection.pdf}
}

@inproceedings{kang2011m2rsm,
  title     = {{The Change Detection of Building Models Using Epochs of Terrestrial Point Clouds}},
  author    = {Kang, Z. and Lu, Z.},
  booktitle = m2rsm,
  year      = {2011},
  url       = {http://www.isprs.org/proceedings/xxxviii/part8/pdf/W01OD2_20100307041951.pdf}
}

@inproceedings{kovsecka2012accv,
  title     = {{Detecting Changes in Images of Street Scenes}},
  author    = {Ko{\v{s}}ecka, J.},
  booktitle = accv,
  year      = {2012},
  url       = {https://pdfs.semanticscholar.org/fce5/ae47a8914534ebb3a4d70c0f47a0864d5787.pdf}
}

@article{malpica2013ijrs,
  title   = {{Change Detection of Buildings from Satellite Imagery and LiDAR Data}},
  author  = {Malpica, J.A. and Alonso, M.C. and Pap{\'\i}, F. and Arozarena, A. and Mart{\'\i}nez De Agirre, A.},
  journal = ijrs,
  volume  = {34},
  number  = {5},
  pages   = {1652--1675},
  year    = {2013},
  url     = {https://ebuah.uah.es/dspace/bitstream/handle/10017/32223/change_alonso_IJRS_2012.pdf}
}

@inproceedings{nunez2010iros,
  title     = {{Change Detection in 3D Environments Based on Gaussian Mixture Model and Robust Structural Matching for Autonomous Robotic Applications}},
  author    = {N{\'u}{\~n}ez, P. and Drews, P. and Bandera, A. and Rocha, R. and Campos, M. and Dias, J.},
  booktitle = iros,
  year      = {2010},
  url       = {https://www.researchgate.net/profile/Paulo_Drews-Jr/publication/224199840_Change_detection_in_3D_environments_based_on_Gaussian_Mixture_Model_and_robust_structural_matching_for_autonomous_robotic_applications/links/02e7e51fa85a6a1e9b000000/Change-detection-in-3D-environments-based-on-Gaussian-Mixture-Model-and-robust-structural-matching-for-autonomous-robotic-applications.pdf}
}

@article{meagher1980techreport,
  author  = {Meagher, D.},
  year    = {1980},
  journal = {Technical Report},
  title   = {{Octree Encoding: A New Technique for the Representation, Manipulation and Display of Arbitrary 3-D Objects by Computer}},
  number  = {IPL-TR-80-111},
  volume  = {Image Processing Laboratory, Rensselaer Polytechnic Institute{ } }
}

@article{fitzgibbon2003ivc,
  title   = {{Robust Registration of 2D and 3D Point Sets}},
  author  = {Fitzgibbon, A.W.},
  journal = ivc,
  volume  = {21},
  number  = {13-14},
  pages   = {1145--1153},
  year    = {2003},
  url     = {http://luthuli.cs.uiuc.edu/~daf/courses/Opt-2019/Papers/sdarticle.pdf}
}

@inproceedings{alcantarilla2012icra,
  title     = {{On Combining Visual SLAM and Dense Scene Flow to Increase the Robustness of Localization and Mapping in Dynamic Environments}},
  author    = {Alcantarilla, P.F. and Yebes, J.J. and Almaz{\'a}n, J. and Bergasa, L.M.},
  booktitle = icra,
  year      = {2012},
  url       = {https://www.researchgate.net/profile/Luis_Bergasa/publication/261503596_On_combining_visual_SLAM_and_dense_scene_flow_to_increase_the_robustness_of_localization_and_mapping_in_dynamic_environments/links/0a85e5320ca3b61609000000.pdf}
}

@inproceedings{wang2014wcica,
  title     = {{Motion Segmentation Based Robust RGB-D SLAM}},
  author    = {Wang, Y. and Huang, S.},
  booktitle = wcica,
  year      = {2014}
}

@article{kim2016tro,
  title   = {{Effective Background Model-Based RGB-D Dense Visual Odometry in a Dynamic Environment}},
  author  = {Kim, D.H. and Kim, J.H.},
  journal = tro,
  volume  = {32},
  number  = {6},
  pages   = {1565--1573},
  year    = {2016}
}

@article{sun2017ras,
  title   = {{Improving RGB-D SLAM in Dynamic Environments: a Motion Removal Approach}},
  author  = {Sun, Y. and Liu, M. and Meng, M.Q.H.},
  journal = ras,
  volume  = {89},
  pages   = {110--122},
  year    = {2017},
  url     = {https://ram-lab.com/papers/2018/ras_2018_sun2.pdf}
}

@inproceedings{tan2013ismar,
  title     = {{Robust Monocular SLAM in Dynamic Environments}},
  author    = {Tan, W. and Liu, H. and Dong, Z. and Zhang, G. and Bao, H.},
  booktitle = ismar,
  year      = {2013},
  url       = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.431.8137&rep=rep1&type=pdf}
}

@inproceedings{wangsiripitak2009icra,
  title     = {{Avoiding Moving Outliers in Visual SLAM by Tracking Moving Objects}},
  author    = {Wangsiripitak, S. and Murray, D.W.},
  booktitle = icra,
  year      = {2009},
  url       = {https://www.robots.ox.ac.uk/~dwm/Publications/wangsiripitak_murray_icra2009/wangsiripitak_murray_icra2009.pdf}
}

@inproceedings{riazuelo2017ecmr,
  title     = {{Semantic Visual SLAM in Populated Environments}},
  author    = {Riazuelo, L. and Montano, L. and Montiel, J.M.M.},
  booktitle = ecmr,
  year      = {2017}
}

@article{li2017ral,
  title   = {{RGB-D SLAM in Dynamic Environments using Static Point Weighting}},
  author  = {Li, S. and Lee, D.},
  journal = ral,
  volume  = {2},
  number  = {4},
  pages   = {2263--2270},
  year    = {2017},
  url     = {https://mediatum.ub.tum.de/doc/1375854/file.pdf}
}

@inproceedings{newcombe2015cvpr,
  title     = {{DynamicFusion: Reconstruction and Tracking of Non-Rigid Scenes in Real-Time}},
  author    = {Newcombe, R.A. and Fox, D. and Seitz, S.M.},
  booktitle = cvprold,
  year      = {2015},
  url       = {https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Newcombe_DynamicFusion_Reconstruction_and_2015_CVPR_paper.pdf}
}

@article{dou2016tog,
  title   = {{Fusion4D: Real-time Performance Capture of Challenging Scenes}},
  author  = {Dou, M. and Khamis, S. and Degtyarev, Y. and Davidson, P. and Fanello, S.R. and Kowdle, A. and Escolano, S.O. and Rhemann, C. and Kim, D. and Taylor, J. and Pushmeet, K. and Tankovich, V. and Izadi, S.},
  journal = tog,
  volume  = {35},
  number  = {4},
  pages   = {114},
  year    = {2016},
  url     = {http://rua.ua.es/dspace/bitstream/10045/62531/5/2016_Dou_etal_ACMTransGraph_preprint.pdf}
}

@inproceedings{slavcheva2018cvpr,
  title     = {{SobolevFusion: 3D Reconstruction of Scenes Undergoing Free Non-Rigid Motion}},
  author    = {Slavcheva, M. and Baust, M. and Ilic, S.},
  booktitle = cvprold,
  year      = {2018},
  url       = {http://openaccess.thecvf.com/content_cvpr_2018/papers/Slavcheva_SobolevFusion_3D_Reconstruction_CVPR_2018_paper.pdf}
}

@inproceedings{slavcheva2017cvpr,
  title     = {{KillingFusion: Non-Rigid 3D Reconstruction without Correspondences}},
  author    = {Slavcheva, M. and Baust, M. and Cremers, D. and Ilic, S.},
  booktitle = cvprold,
  year      = {2017},
  url       = {http://openaccess.thecvf.com/content_cvpr_2017/papers/Slavcheva_KillingFusion_Non-Rigid_3D_CVPR_2017_paper.pdf}
}

@article{zhang2018tvcg,
  title   = {{MixedFusion: Real-Time Reconstruction of an Indoor Scene with Dynamic Objects}},
  author  = {Zhang, H. and Xu, F.},
  journal = tvcg,
  volume  = {24},
  number  = {12},
  pages   = {3137--3146},
  year    = {2018}
}

@article{zollhofer2014tog,
  title   = {{Real-Time Non-Rigid Reconstruction using an RGB-D Camera}},
  author  = {Zollh{\"o}fer, M. and Nie{\ss}ner, M. and Izadi, S. and Rehmann, C. and Zach, C. and Fisher, M. and Wu, C. and Fitzgibbon, A. and Loop, C. and Theobalt, C. and Stamminger, M.},
  journal = tog,
  volume  = {33},
  number  = {4},
  pages   = {156},
  year    = {2014},
  url     = {http://www.charlesloop.com/zollhoefer2014deformable.pdf}
}

@inproceedings{innmann2016eccv,
  title     = {{VolumeDeform: Real-time Volumetric Non-rigid Reconstruction}},
  author    = {Innmann, M. and Zollh{\"o}fer, M. and Nie{\ss}ner, M. and Theobalt, C. and Stamminger, M.},
  booktitle = eccv,
  year      = {2016},
  url       = {https://arxiv.org/pdf/1603.08161.pdf}
}

@article{guo2017tog,
  title   = {{Real-Time Geometry, Albedo, and Motion Reconstruction using a Single RGB-D Camera}},
  author  = {Guo, K. and Xu, F. and Yu, T. and Liu, X. and Dai, Q. and Liu, Y.},
  journal = tog,
  volume  = {36},
  number  = {3},
  pages   = {32},
  year    = {2017}
}

@inproceedings{elgammal2000eccv,
  title     = {{Non-parametric Model for Background Subtraction}},
  author    = {Elgammal, A. and Harwood, D. and Davis, L.},
  booktitle = eccv,
  year      = {2000},
  url       = {https://www.cs.rutgers.edu/~elgammal/pub/bgmodel_ECCV00_postfinal.pdf}
}

@inproceedings{jafari2014icra,
  title     = {{Real-Time RGB-D Based People Detection and Tracking for Mobile Robots and Head-Worn Cameras}},
  author    = {Jafari, O.H. and Mitzel, D. and Leibe, B.},
  booktitle = icra,
  year      = {2014},
  url       = {https://ohosseini.github.io/publications/pdf/jafari_etal_ICRA14.pdf}
}

@inproceedings{salas2013cvpr,
  title     = {Slam++: Simultaneous localisation and mapping at the level of objects},
  author    = {Salas-Moreno, R. F. and Newcombe, R. A. and Strasdat, H. and Kelly, P. HJ and Davison, A. J.},
  booktitle = cvprold,
  year      = {2013}
}

@inproceedings{konolige2010iros,
  author    = {K. Konolige and G. Grisetti and R. K{\"u}mmerle and W. Burgard and B. Limketkai and R. Vincent},
  title     = {Sparse Pose Adjustment for 2D Mapping},
  booktitle = iros,
  year      = {2010}
}

@inproceedings{olson2006icra,
  author    = {E. Olson and J. Leonard and S. Teller},
  title     = {Fast Iterative Optimization of Pose Graphs with Poor
               Initial Estimates},
  booktitle = icra,
  year      = {2006}
}

@article{thrun2006ijrr,
  title   = {{The graph SLAM algorithm with applications to large-scale mapping of urban structures}},
  author  = {Thrun, S. and Montemerlo, M.},
  journal = ijrr,
  volume  = {25},
  number  = {5-6},
  pages   = {403},
  year    = {2006}
}

@inproceedings{sunderhauf2012iros,
  title     = {Switchable Constraints for Robust Pose Graph SLAM},
  author    = {S{\"u}nderhauf, N. and Protzel, P.},
  booktitle = iros,
  year      = {2012}
}

@article{haklay2008osm,
  title   = {Openstreetmap: User-generated street maps},
  author  = {Haklay, M. and Weber, P.},
  journal = {IEEE Pervasive Computing},
  volume  = {7},
  number  = {4},
  pages   = {12--18},
  year    = {2008}
}

@inproceedings{pink2009iv,
  title     = {Visual features for vehicle localization and ego-motion estimation},
  author    = {Pink, O. and Moosmann, F. and Bachmann, A.},
  booktitle = iv,
  year      = {2009}
}

@article{brubaker2016pami,
  title   = {Map-based probabilistic visual self-localization},
  author  = {Brubaker, M. and Geiger, A. and Urtasun, R.},
  journal = pami,
  volume  = {38},
  number  = {4},
  pages   = {652--665},
  year    = {2016}
}


@article{chen2011ijrr,
  title   = {Active vision in robotic systems: A survey of recent developments},
  author  = {Chen, S. and Li, Y. and Kwok, N. M.},
  journal = ijrr,
  volume  = {30},
  number  = {11},
  pages   = {1343--1377},
  year    = {2011}
}

@article{kim2014ijrr,
  author  = {Kim, Ayoung and Eustice, Ryan M.},
  title   = {Active Visual SLAM for Robotic Area Coverage: Theory and Experiment},
  journal = ijrr,
  volume  = {34},
  number  = {4-5},
  year    = {2015},
  pages   = {457--475}
}

@inproceedings{velez2011iciap,
  title     = {Planning to Perceive: Exploiting Mobility for Robust Object Detection.},
  author    = {Velez, J. and Hemann, G. and Huang, A. S and Posner, I. and Roy, N.},
  booktitle = iciap,
  year      = {2011}
}

@inproceedings{roy1998aaai,
  author    = {N. Roy and W. Burgard and D. Fox and S. Thrun},
  title     = {Coastal Navigation -- Robot Motion with Uncertainty},
  booktitle = {Proceedings of the AAAI Fall Symposium: Planning
               with POMDPs},
  year      = {1998}
}

@inproceedings{latif2012rss,
  title     = {Robust loop closing over time},
  author    = {Latif, Y. and Cadena, C. and Neira, J.},
  booktitle = rss,
  year      = {2012}
}

@article{pacheco2012air,
  author   = {Fuentes-Pacheco, J. and Ruiz-Ascencio, J. and Rend\'on-Mancha, J.M.},
  year     = {2012},
  journal  = {Artificial Intelligence Review},
  title    = {Visual simultaneous localization and mapping: a survey},
  volume   = {45},
  pages    = {55--81},
  keywords = {Visual SLAM; Salient feature selection; Image matching; Data association; Topological and metric maps}
}

@inproceedings{matsumoto1996icra,
  author    = {Matsumoto, Y. and Inaba, M. and Inoue, H.},
  booktitle = icra,
  title     = {Visual navigation using view-sequenced route representation},
  year      = {1996}
}

@article{mikolajczyk2005pami,
  author  = {K. Mikolajczyk and C. Schmid},
  journal = pami,
  title   = {A performance evaluation of local descriptors},
  year    = {2005},
  volume  = {27},
  number  = {10},
  pages   = {1615--1630}
}

@inproceedings{ke2004cvpr,
  title     = {{PCA-SIFT: More Distinctive Representation for Local Image Descriptors}},
  author    = {Ke, Y. and Sukthankar, R.},
  booktitle = cvprold,
  year      = {2004},
  url       = {http://www.cs.cmu.edu/~rahuls/pub/cvpr2004-keypoint-rahuls.pdf}
}

@article{se2002ijrr,
  title   = {{Mobile RobotLocalization and Mapping withUncertainty using Scale-Invariant Visual Landmarks}},
  author  = {Se, S. and Lowe, D. and Little, J.},
  journal = ijrr,
  volume  = {21},
  number  = {8},
  pages   = {735--758},
  year    = {2002},
  url     = {https://www.cs.ubc.ca/~lowe/papers/ijrr02.pdf}
}

@article{andreasson2004top,
  title   = {{Topological localization for mobile robots using omni-directional vision and local features}},
  author  = {Andreasson, H. and Duckett, T.},
  journal = {IFAC Proceedings Volumes},
  volume  = {37},
  number  = {8},
  pages   = {36--41},
  year    = {2004},
  url     = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.9.4697&rep=rep1&type=pdf}
}

@inproceedings{stumm2013iros,
  title     = {{Probabilistic Place Recognition with Covisibility Maps}},
  author    = {Stumm, E. and Mei, C. and Lacroix, S.},
  booktitle = iros,
  year      = {2013},
  url       = {https://hal.archives-ouvertes.fr/hal-00940830/document}
}

@inproceedings{angeli2008iros,
  title     = {{Incremental vision-based topological SLAM}},
  author    = {Angeli, A. and Doncieux, S. and Meyer, J.A.  and Filliat, D.},
  booktitle = iros,
  year      = {2008},
  url       = {https://spiral.imperial.ac.uk/bitstream/10044/1/1387/1/angeli_iros08.pdf}
}

@inproceedings{calonder2010eccv,
  title     = {{BRIEF: Binary Robust IndependentElementary Features}},
  author    = {Calonder, M. and Lepetit, V. and Strecha, C. and Fua, P.},
  booktitle = eccv,
  year      = {2010},
  url       = {https://www.cs.ubc.ca/~lowe/525/papers/calonder_eccv10.pdf}
}

@inproceedings{rosten2005iccv,
  title     = {{Fusing Points and Lines for High Performance Tracking}},
  author    = {Rosten, E. and Drummond, T.},
  year      = {2005},
  booktitle = iccvold,
  url       = {http://www.edwardrosten.com/work/rosten_2005_tracking.pdf}
}

@inproceedings{rosten2006eccv,
  title     = {Machine learning for high-speed corner detection},
  author    = {Rosten, E. and Drummond, T.},
  year      = {2006},
  booktitle = eccv,
  url       = {http://www.edwardrosten.com/work/rosten_2006_machine.pdf}
}

@inproceedings{forstner1987isprs,
  title     = {{A Fast Operator for Detection and Precise Location of Distinct Points, Corners and Centres of Circular Features}},
  author    = {F{\"o}rstner, W. and G{\"u}lch, E.},
  booktitle = isprsannals,
  year      = {1987},
  url       = {http://www.ipb.uni-bonn.de/pdfs/Forstner1987Fast.pdf}
}

@inproceedings{harris1988avc,
  title     = {A combined corner and edge detector},
  author    = {Harris, C. and Stephens, M.},
  booktitle = {Alvey vision conference},
  year      = {1988},
  url       = {http://www.bmva.org/bmvc/1988/avc-88-023.pdf}
}

@techreport{shi1993cornel,
  title       = {{Good Features to Track}},
  author      = {Shi, J. and Tomasi, C.},
  year        = {1993},
  institution = {Cornell University},
  url         = {http://www.ai.mit.edu/courses/6.891/handouts/shi94good.pdf}
}

@inproceedings{mei2009bmvc,
  title     = {{A Constant-Time Efficient Stereo SLAM System}},
  author    = {Mei, C. and Sibley, G. and Cummins, M. and Newman, P. and Reid, I.},
  booktitle = bmvc,
  year      = {2009},
  url       = {http://www.robots.ox.ac.uk/~mobile/Papers/AConstantTimeEfficientStereoSLAMSystem_mei_bmvc_09.pdf}
}

@inproceedings{leutenegger2011iccv,
  title     = {{BRISK: Binary robust invariant scalable keypoints}},
  author    = {Leutenegger, S. and Chli, M. and Siegwart, R.},
  booktitle = iccvold,
  year      = {2011},
  url       = {http://www.margaritachli.com/papers/ICCV2011paper.pdf}
}

@inproceedings{alahi2012cvpr,
  title     = {{FREAK: Fast Retina Keypoint}},
  author    = {Alahi, A. and Ortiz, R. and Vandergheynst, P.},
  booktitle = cvprold,
  year      = {2012}
}

@inproceedings{badino2012icra,
  title     = {{Real-Time  Topometric  Localization}},
  author    = {Badino, H. and Huber, D. and Kanade, T.},
  booktitle = icra,
  year      = {2012},
  url       = {https://ri.cmu.edu/pub_files/2012/5/badino_icra12.pdf}
}

@inproceedings{sunderhauf2011iros,
  title     = {{BRIEF-Gist -- Closing the Loop by Simple Means}},
  author    = {S{\"u}nderhauf, N. and Protzel, P.},
  booktitle = iros,
  year      = {2011},
  url       = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.413.6184&rep=rep1&type=pdf}
}

@article{oliva2006pbr,
  title   = {{Building the gist of a scene: The role of global image features in recognition}},
  author  = {Oliva, A. and Torralba, A.},
  journal = {Progress in brain research},
  volume  = {155},
  pages   = {23--36},
  year    = {2006},
  url     = {http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=0EE62ACA3A03999CC8D42247A3436CA6?doi=10.1.1.304.2750&rep=rep1&type=pdf}
}

@inproceedings{murillo2009iccv,
  title     = {{Experiments in Place Recognition using Gist Panoramas}},
  author    = {Murillo, A. and Kosecka, J.},
  booktitle = iccvold,
  year      = {2009},
  url       = {http://webdiis.unizar.es/~anacris/papers/09OMNIVISplaceRecognition.pdf}
}

@inproceedings{dalal2005cvpr,
  title     = {{Histograms o fOriented Gradients for Human Detection}},
  author    = {Dalal, N. and Triggs, B.},
  booktitle = cvprold,
  year      = {2005},
  url       = {https://lear.inrialpes.fr/people/triggs/pubs/Dalal-cvpr05.pdf}
}

@inproceedings{zitnick2014eccv,
  title     = {{Edge Boxes: LocatingObject Proposals from Edges}},
  author    = {Zitnick, C. and Doll{\'a}r, P.},
  booktitle = eccv,
  year      = {2014},
  url       = {https://pdollar.github.io/files/papers/ZitnickDollarECCV14edgeBoxes.pdf}
}

@inproceedings{sivic2003iccv,
  author    = {Sivic, J. and Zisserman, A.},
  booktitle = iccvold,
  title     = {{Video Google: A Text Retrieval Approach to Object Matching in Videos}},
  year      = {2003},
  url       = {http://www.robots.ox.ac.uk/~vgg/publications/papers/sivic03.pdf}
}

@inproceedings{krizhevsky2012nips,
  title     = {{ImageNet Classification with Deep Convolutional Neural Networks}},
  author    = {Krizhevsky, A. and Sutskever, I. and Hinton, G.},
  booktitle = nips,
  year      = {2012},
  url       = {https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf}
}

@inproceedings{sunderhauf2015iros,
  title     = {On the performance of convnet features for place recognition},
  author    = {S{\"u}nderhauf, N. and Shirazi, S. and Dayoub, F. and Upcroft, B. and Milford, M.},
  booktitle = iros,
  year      = {2015},
  url       = {https://arxiv.org/pdf/1501.04158.pdf}
}

@article{radwan2018ral,
  author  = {Radwan, N. and Valada, A. and Burgard, W.},
  title   = {{VLocNet++: Deep Multitask Learning for Semantic Visual Localization and Odometry}},
  journal = ral,
  volume  = {3},
  number  = {4},
  pages   = {4407--4414},
  year    = {2018},
  url     = {https://arxiv.org/pdf/1804.08366.pdf}
}

 @inproceedings{arandjelovic2014accv,
  title     = {{DisLocation: Scalable descriptor distinctiveness for location recognition}},
  author    = {Arandjelovi{\'c}, R. and Zisserman, A.},
  booktitle = accv,
  year      = {2014},
  url       = {https://www.robots.ox.ac.uk/~vgg/publications/2014/arandjelovic14a/arandjelovic14a.pdf}
}

@inproceedings{cao2013cvpr,
  title     = {{Graph-Based Discriminative Learning for Location Recognition}},
  author    = {Cao, S. and Snavely, N.},
  booktitle = cvprold,
  year      = {2013},
  url       = {http://www.cs.cornell.edu/projects/graphlocation/docs/location_cvpr13.pdf}
}

@inproceedings{chen2011cvpr,
  title     = {{City-Scale Landmark Identification on Mobile Devices}},
  author    = {Chen, D. and Baatz, G. and K{\"o}ser, K. and Tsai, S. and Vedantham, R. and Pylv{\"a}n{\"a}inen, T. and Roimela, K. and Chen, X. and Bach, J. and Pollefeys, M. and Girod, B. and Grzezczuk, R.},
  booktitle = cvprold,
  year      = {2011},
  url       = {https://velodynelidar.com/lidar/hdlpressroom/pdf/Articles/City-Scale%20Landmark%20Identification%20on%20Mobile%20Devices.pdf}
}

@inproceedings{knopp2010eccv,
  title     = {{Avoiding confusing features in place recognition}},
  author    = {Knopp, J. and Sivic, J. and Pajdla, T.},
  booktitle = eccv,
  year      = {2010},
  url       = {https://www.di.ens.fr/~josef/publications/knopp10.pdf}
}

@inproceedings{sattler2015iccv,
  title     = {{Hyperpoints and Fine Vocabularies for Large-Scale Location Recognition}},
  author    = {Sattler, T. and Havlena, M. and Radenovic, F. and Schindler, K. and Pollefeys, M.},
  booktitle = iccvold,
  year      = {2015},
  url       = {https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Sattler_Hyperpoints_and_Fine_ICCV_2015_paper.pdf}
}

@inproceedings{sattler2011iccv,
  title     = {{Fast Image-Based Localization using Direct 2D-to-3D Matching}},
  author    = {Sattler, T. and Leibe, B. and Kobbelt, L.},
  booktitle = iccvold,
  year      = {2011},
  url       = {https://www.graphics.rwth-aachen.de/media/papers/sattler_iccv11_preprint_011.pdf}
}

@inproceedings{schindler2007cvpr,
  title     = {{City-scale Location Recognition}},
  author    = {Schindler, G. and Brown, M. and Szeliski, R.},
  booktitle = cvprold,
  year      = {2007},
  url       = {https://www.cc.gatech.edu/~phlosoft/files/schindler07cvpr2.pdf}
}

@inproceedings{torii2015cvpr,
  title     = {24/7 place recognition by view synthesis},
  author    = {Torii, A. and Arandjelovic, R. and Sivic, J. and Okutomi, M. and Pajdla, T.},
  booktitle = cvprold,
  year      = {2015},
  url       = {http://www.ok.ctrl.titech.ac.jp/~torii/project/247/download/Torii-CVPR-2015-final.pdf}
}

@inproceedings{torii2013cvpr,
  title     = {Visual place recognition with repetitive structures},
  author    = {Torii, A. and Sivic, J. and Pajdla, T. and Okutomi, M.},
  booktitle = cvprold,
  year      = {2013},
  url       = {https://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Torii_Visual_Place_Recognition_2013_CVPR_paper.pdf}
}

@inproceedings{mcmanus2014icra,
  title     = {{Shady dealings: Robust, long-term visual localisation using illumination invariance}},
  author    = {McManus, C. and Churchill, W. and Maddern, W. and Stewart, A. and Newman, P.},
  booktitle = icra,
  year      = {2014},
  url       = {http://www.robots.ox.ac.uk/~mobile/Papers/2014ICRA_McManus.pdf}
}

@inproceedings{middelberg2014eccv,
  title     = {Scalable 6-dof localization on mobile devices},
  author    = {Middelberg, S.and Sattler, T. and Untzelmann, O. and Kobbelt, L.},
  booktitle = eccv,
  year      = {2014},
  url       = {https://www.graphics.rwth-aachen.de/media/papers/ECCV14_preprint.pdf}
}

@article{aubry2014tog,
  title   = {Painting-to-3D model alignment via discriminative visual elements},
  author  = {Aubry, M. and Russell, B. and Sivic, J.},
  journal = tog,
  volume  = {33},
  number  = {2},
  pages   = {14},
  year    = {2014},
  url     = {https://hal.inria.fr/hal-00863615v1/document}
}

@inproceedings{arandjelovic2013cvpr,
  author    = {Arandjelovic, R. and Zisserman, A.},
  title     = {{All About VLAD}},
  booktitle = cvprold,
  year      = {2013},
  url       = {https://www.robots.ox.ac.uk/~vgg/publications/2013/arandjelovic13/arandjelovic13.pdf}
}


@incollection{fox2001springer,
  title     = {Particle filters for mobile robot localization},
  author    = {Fox, D. and Thrun, S. and Burgard, W. and Dellaert, F.},
  booktitle = {Sequential Monte Carlo methods in practice},
  pages     = {401--428},
  year      = {2001},
  url       = {http://users.umiacs.umd.edu/~fer/cmsc828/classes/fox.mcmc-book.pdf}
}

@inproceedings{bampis2016iros,
  title     = {{Encoding the description of image sequences: A two-layered pipeline for loop closure detection}},
  author    = {Bampis, L. and Amanatiadis, A. and Gasteratos, A.},
  booktitle = iros,
  year      = {2016},
  url       = {http://robotics.pme.duth.gr/bampis/papers/Encoding%20the%20Description%20of%20Image%20Sequences.%20A%20Two-Layered%20Pipeline%20for%20Loop%20Closure%20Detection.pdf}
}

@techreport{barrow1977parametric,
  title       = {{Parametric correspondence and chamfer matching: Two new techniques for image matching}},
  author      = {Barrow, H. and Tenenbaum, J. and Bolles, R. and Wolf, H.},
  year        = {1977},
  institution = {SRI INTERNATIONAL MENLO PARK CA ARTIFICIAL INTELLIGENCE CENTER},
  url         = {https://www.ijcai.org/Proceedings/77-2/Papers/024.pdf}
}

@article{vysotska2019ral,
  author  = {O. Vysotska and C. Stachniss},
  title   = {{Effective Visual Place Recognition Using Multi-Sequence Maps}},
  journal = ral,
  volume  = {4},
  number  = {2},
  pages   = {1730--1736},
  year    = 2019,
  url     = {http://www.ipb.uni-bonn.de/pdfs/vysotska2019ral.pdf}
}

@article{vysotska2016ral,
  author   = {O. Vysotska and C. Stachniss},
  title    = {{Lazy Data Association For Image Sequences Matching Under Substantial Appearance Changes}},
  journal  = ral,
  year     = 2016,
  volume   = {1},
  number   = {1},
  pages    = {213--220},
  url      = {http://www.ipb.uni-bonn.de/pdfs/vysotska16ral-icra.pdf},
  keywords = {Localization},
  abstract = {Localization is an essential capability for mobile robots and the ability to localize in changing environments is key to robust outdoor navigation. Robots operating over extended periods of time should be able to handle substantial appearance changes such as those occurring over seasons or under different weather conditions. In this paper, we investigate the problem of efficiently coping with seasonal appearance changes in online localization. We propose a lazy data association approach for matching streams of incoming images to a reference image sequence in an online fashion. We present a search heuristic to quickly find matches between the current image sequence and a database using a data association graph. Our experiments conducted under substantial seasonal changes suggest that our approach can efficiently match image sequences while requiring a comparably small number of image to image comparisons.}
}

@inproceedings{vysotska2016iros,
  title     = {{Exploiting Building Information from Publicly Available Maps in Graph-Based SLAM}},
  author    = {O. Vysotska and C. Stachniss},
  booktitle = iros,
  year      = {2016},
  url       = {http://www.ipb.uni-bonn.de/pdfs/vysotska16iros.pdf},
  videourl  = {https://www.youtube.com/watch?v=5RfRAEP-baM}
}

@book{russell2016artificial,
  title     = {{Artificial intelligence: a modern approach}},
  author    = {Russell, S. and Norvig, P.},
  year      = {2016},
  publisher = {Malaysia; Pearson Education Limited,}
}

@article{muja2014pami,
  title   = {{Scalable Nearest Neighbor Algorithms for High Dimensional Data}},
  author  = {M. Muja and D. G. Lowe},
  journal = pami,
  volume  = {36},
  number  = {11},
  pages   = {2227--2240},
  year    = {2014},
  url     = {https://www.cs.ubc.ca/research/flann/uploads/FLANN/flann_pami2014.pdf}
}

@inproceedings{behley2019iccv,
  author    = {J. Behley and M. Garbade and A. Milioto and J. Quenzel and S. Behnke and C. Stachniss and J. Gall},
  title     = {{SemanticKITTI: A Dataset for Semantic Scene Understanding of LiDAR Sequences}},
  booktitle = iccv,
  year      = {2019},
  url       = {https://arxiv.org/pdf/1904.01416.pdf}
}

@article{brostow2008prl,
  author  = {G.J. Brostow and J. Fauqueur and R. Cipolla},
  title   = {{Semantic Object Classes in Video: A High-Definition Ground Truth Database}},
  journal = prl,
  volume  = {30},
  number  = {2},
  pages   = {88--97},
  year    = {2008},
  url     = {http://www0.cs.ucl.ac.uk/staff/G.Brostow/papers/Brostow_2009-PRL.pdf}
}

@inproceedings{milioto2019icra,
  author    = {A. Milioto and C. Stachniss},
  title     = {{Bonnet: An Open-Source Training and Deployment Framework for Semantic Segmentation in Robotics using CNNs}},
  booktitle = icra,
  year      = {2019},
  url       = {https://arxiv.org/pdf/1802.08960.pdf},
  codeurl   = {https://github.com/Photogrammetry-Robotics-Bonn/bonnet},
  videourl  = {https://www.youtube.com/watch?v=tfeFHCq6YJs}
}

@inproceedings{wu2019icra,
  author    = {B. Wu and X. Zhou and S. Zhao and X. Yue and	K. Keutzer},
  title     = {{SqueezeSegV2: Improved Model Structure and Unsupervised Domain Adaptation
               for Road-Object Segmentation from a LiDAR Point Cloud}},
  booktitle = icra,
  year      = {2019},
  url       = {https://arxiv.org/pdf/1809.08495.pdf}
}

@inproceedings{xiong2011icra,
  author    = {X. Xiong and D. Munoz and J. A. Bagnell and M. Hebert},
  title     = {{3-D Scene Analysis via Sequenced Predictions over Points and Regions}},
  booktitle = icra,
  year      = {2011},
  keywords  = {classification, point-wise},
  url       = {http://www.cs.princeton.edu/courses/archive/spr11/cos598A/pdfs/Xiong11.pdf}
}

@inproceedings{rock2015cvpr,
  title     = {{Completing 3D Object Shape from One Depth Image}},
  author    = {J. Rock and T. Gupta and J. Thorsen and J. Gwak and D. Shin and D. Hoiem},
  booktitle = cvprold,
  year      = {2015},
  url       = {http://dhoiem.cs.illinois.edu/publications/rock_shape_cvpr2015.pdf}
}

@inproceedings{song2017cvpr,
  title     = {{Semantic Scene Completion from a Single Depth Image}},
  author    = {S. Song and F. Yu and A. Zeng and A.X. Chang and M. Savva and T. Funkhouser},
  booktitle = cvprold,
  year      = {2017},
  url       = {http://openaccess.thecvf.com/content_cvpr_2017/papers/Song_Semantic_Scene_Completion_CVPR_2017_paper.pdf}
}

@inproceedings{garbade2019cvpr-ws,
  title     = {{Two Stream 3D Semantic Scene Completion}},
  author    = {M. Garbade and Y.-T. Chen and J. Sawatzky and J. Gall},
  booktitle = cvprws,
  year      = {2019},
  url       = {http://openaccess.thecvf.com/content_CVPRW_2019/papers/MULA/Garbade_Two_Stream_3D_Semantic_Scene_Completion_CVPRW_2019_paper.pdf}
}

@inproceedings{torralba2011cvpr,
  title     = {{Unbiased Look at Dataset Bias}},
  author    = {A. Torralba and A.A. Efros},
  booktitle = cvprold,
  year      = {2011},
  url       = {http://people.csail.mit.edu/torralba/publications/datasets_cvpr11.pdf}
}

@inproceedings{zhang2015icra-sffs,
  title     = {{Sensor Fusion for Semantic Segmentation of Urban Scenes}},
  author    = {R. Zhang and S.A. Candra and K. Vetter and A. Zakhor},
  booktitle = icra,
  year      = {2015},
  url       = {https://richzhang.github.io/index_files/icra2015.pdf}
}

@inproceedings{dai2018cvpr,
  author    = {A. Dai and D. Ritchie and M. Bokeloh and S. Reed and J. Sturm and M. Nie{\ss}ner},
  booktitle = cvprold,
  title     = {{ScanComplete: Large-Scale Scene Completion and Semantic Segmentation for 3D Scans}},
  year      = {2018},
  url       = {https://arxiv.org/pdf/1712.10215.pdf}
}

@inproceedings{dai2018eccv-jpfs,
  author    = {A. Dai and M. Niessner},
  title     = {{3DMV: Joint 3D-Multi-View Prediction for 3D Semantic Scene Segmentation}},
  booktitle = eccv,
  year      = 2018,
  abstract  = {We present 3DMV, a novel method for 3D semantic scene segmentation of RGB-D scans using a joint 3D-multi-view prediction network. In contrast to existing methods that either use geometry or RGB data as input for this task, we combine both data modalities in a joint, end-to-end network architecture. Rather than simply projecting color data into a volumetric grid and operating solely in 3D -- which would result in insufficient detail -- we first extract feature maps from associated RGB images. These features are then directly projected into the volumetric feature grid of a 3D network using a differentiable backprojection layer. Since our target is 3D scanning scenarios with possibly many frames, we use a multi-view pooling approach in order to handle a varying number of RGB input views. This learned combination of RGB and geometric features with our joint 2D-3D architecture achieves significantly better results than existing baselines. For instance, our final result on the ScanNet 3D segmentation benchmark increases from 52.8\% to 75\% accuracy compared to existing volumetric architectures.},
  url       = {proceedings: dai2018eccv-jpfs.pdf}
}


@inproceedings{liu2018nips,
  author    = {S. Liu and Y. Hu and Y. Zeng and Q. Tang and B. Jin and Y. Han and X. Li},
  booktitle = nips,
  title     = {{See and Think: Disentangling Semantic Scene Completion}},
  year      = {2018},
  url       = {https://papers.nips.cc/paper/7310-see-and-think-disentangling-semantic-scene-completion.pdf}
}

@inproceedings{zhang2018eccv,
  title     = {{Efficient Semantic Scene Completion Network with Spatial Group Convolution}},
  author    = {J. Zhang and H. Zhao and A. Yao and Y. Chen and L. Zhang and H. Liao},
  booktitle = eccv,
  year      = {2018},
  url       = {http://openaccess.thecvf.com/content_ECCV_2018/papers/Jiahui_Zhang_Efficient_Semantic_Scene_ECCV_2018_paper.pdf}
}

@inproceedings{geiger2015gcpr,
  author    = {A. Geiger and C. Wang},
  booktitle = {Proc.~of the German Conf. on Pattern Recognition~(GCPR)},
  title     = {{Joint 3d Object and Layout Inference from a single RGB-D Image}},
  year      = {2015},
  url       = {http://www.cvlibs.net/publications/Geiger2015GCPR.pdf}
}

@inproceedings{firman2016cvpr,
  author    = {M. Firman and  O. Mac Aodha and S. Julier and G.J. Brostow},
  title     = {{Structured Prediction of Unobserved Voxels From a Single Depth Image}},
  booktitle = cvprold,
  year      = {2016},
  url       = {http://visual.cs.ucl.ac.uk/pubs/depthPrediction/depth_prediction_main.pdf}
}

@article{yang2018pami,
  title   = {{Dense 3D object reconstruction from a single depth view}},
  author  = {Bo Yang and Stefano Rosa and Andrew Markham and  Niki Trigoni and Hongkai Wen},
  journal = pami,
  volume  = {41},
  number  = {12},
  pages   = {2820--2834},
  year    = {2018},
  url     = {https://arxiv.org/pdf/1802.00411.pdf}
}


@article{walter2015plantmethods,
  title   = {Plant phenotyping: from bean weighing to image analysis},
  author  = {A. Walter and F. Liebisch and A. Hund},
  journal = {Plant Methods},
  volume  = {11},
  number  = {1},
  year    = {2015}
}

@article{sumner2007tog,
  title   = {Embedded deformation for shape manipulation},
  author  = {Sumner, R. W. and Schmid, J. and Pauly, M.},
  journal = tog,
  volume  = {26},
  number  = {3},
  pages   = {80},
  year    = {2007},
  url     = {https://people.inf.ethz.ch/~sumnerb/research/embdef/Sumner2007EDF.pdf}
}


@article{huang2013tog,
  title   = {L1-medial skeleton of point cloud.},
  author  = {Huang, H. and Wu, S. and Cohen-Or, D. and Gong, M. and Zhang, H. and Li, G. and Chen, B.},
  journal = tog,
  volume  = {32},
  number  = {4},
  pages   = {65--1},
  year    = {2013},
  url     = {https://vcc.tech/file/upload_file//image/research/att201801101049/1515552582208.pdf}
}

@inproceedings{wei2010spie,
  author    = { Y. Wei and G. E. Christensen and J.H. Song and D. Rudrauf and J. Bruss and J.G. Kuhl and T.J. Grabowski},
  title     = {Evaluation of five non-rigid image registration algorithms using the NIREP framework},
  booktitle = {Proc.~of Society of Photo-Optical Instrumentation Engineers (SPIE)},
  year      = {2010}
}

@article{paulus2014biosyseng,
  title    = {{High-precision laser scanning system for capturing 3D plant architecture and analysing growth of cereal plants}},
  author   = {S. Paulus and H. Schumann and H. Kuhlmann and J. L{\'{e}}on},
  journal  = biosyseng,
  keywords = {3D plant architecture, Barley, Non-invasive, Phenotyping, Plant growth,Laser scanning},
  pages    = {1--11},
  volume   = {121},
  year     = {2014}
}

@inproceedings{isokane2018cvpr,
  title     = {{Probabilistic Plant Modeling via Multi-View Image-to-Image Translation}},
  author    = {T. Isokane and F. Okura and A. Ide and Y. Matsushita and Y. Yagi},
  booktitle = cvprold,
  year      = {2018},
  url       = {http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/1443.pdf}
}

@article{tagliasacchi2016cgf,
  title   = {3d skeletons: A state-of-the-art report},
  author  = {A. Tagliasacchi and T. Delame and M. Spagnuolo and N. Amenta and A. Telea},
  journal = {Computer Graphics Forum},
  volume  = {35},
  number  = {2},
  pages   = {573--597},
  year    = {2016},
  url     = {https://hal.archives-ouvertes.fr/hal-01300281/document}
}

@article{tagliasacchi2009tog,
  title   = {Curve skeleton extraction from incomplete point cloud},
  author  = {A. Tagliasacchi and H. Zhang and D. Cohen-Or},
  journal = tog,
  volume  = {28},
  number  = {3},
  pages   = {71},
  year    = {2009},
  url     = {https://www.cs.sfu.ca/~haoz/pubs/sig09_rosa.pdf}
}


@article{zheng2010cgf,
  title   = {Consensus skeleton for non-rigid space-time registration},
  author  = {Q. Zheng and A. Sharf and A. Tagliasacchi and B. Chen and H. Zhang and A. Sheffer and D. Cohen-Or},
  journal = {Computer Graphics Forum},
  volume  = {29},
  number  = {2},
  pages   = {635--644},
  year    = {2010},
  url     = {https://www.cs.princeton.edu/courses/archive/spring11/cos598A/pdfs/Qian10.pdf}
}

@article{li2008global,
  title   = {Global correspondence optimization for non-rigid registration of depth scans},
  author  = {H. Li and R.W. Sumner and Pauly, Mark},
  journal = {Computer Graphics Forum},
  volume  = {27},
  number  = {5},
  pages   = {1421--1430},
  year    = {2008},
  url     = {https://lgg.epfl.ch/publications/2008/sgp2008GCO.pdf}
}

@inproceedings{shoemake1992gi,
  title     = {Matrix animation and polar decomposition},
  author    = {Shoemake, K. and Duff, T.},
  booktitle = {Proc.~of the Conf. on Graphics Interface},
  year      = {1992},
  url       = {https://research.cs.wisc.edu/graphics/Courses/838-s2002/Papers/polar-decomp.pdf}
}

@article{wand2009tog,
  title   = {Efficient reconstruction of nonrigid shape and motion from real-time 3D scanner data},
  author  = {Wand, M. and Adams, B. and Ovsjanikov, M. and Berner, A. and Bokeloh, M. and Jenke, P. and Guibas, L. and Seidel, H.P. and Schilling, A.},
  journal = tog,
  volume  = {28},
  number  = {2},
  pages   = {15},
  year    = {2009},
  url     = {https://www.cs.jhu.edu/~misha/Fall13b/Papers/Wand08.pdf}
}

@inproceedings{alenya2011icra,
  title     = {3D modelling of leaves from color and ToF data for robotized plant measuring},
  author    = {Alenya, G. and Dellen, B. and Torras, C.},
  year      = {2011},
  booktitle = icra,
  url       = {proceedings:aleny2011icra.pdf}
}

@article{klose2009bab,
  title   = {Usability study of 3D time-of-flight cameras for automatic plant phenotyping},
  author  = {Klose, R. and Penlington, J. and Ruckelshausen, A.},
  journal = {Bornimer Agrartechnische Berichte},
  volume  = {69},
  number  = {93--105},
  pages   = {12},
  year    = {2009},
  url     = {https://www2.atb-potsdam.de/cigr-imageanalysis/images/09_110_Klose.pdf}
}


@article{furbank2011ps,
  title   = {Phenomics -- technologies to relieve the phenotyping bottleneck},
  author  = {R. T. Furbank and M. Tester},
  journal = {Trends in Plant Science},
  volume  = {16},
  number  = {12},
  pages   = {635--644},
  year    = {2011},
  url     = {https://www.cell.com/trends/plant-science/pdf/S1360-1385(11)00209-3.pdf}
}

@article{paproki2012bmc,
  title   = {A novel mesh processing based technique for 3D plant analysis},
  author  = {Paproki, A. and Sirault, X. and Berry, S. and Furbank, R. and Fripp, J.},
  journal = {BMC Plant Biology},
  volume  = {12},
  number  = {1},
  pages   = {63},
  year    = {2012},
  url     = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3464618/pdf/1471-2229-12-63.pdf}
}

@inproceedings{sorkine2007sgm,
  title     = {As-rigid-as-possible surface modeling},
  author    = {Sorkine, O. and Alexa, M.},
  booktitle = {Symposium on Geometry Processing},
  year      = {2007},
  url       = {https://igl.ethz.ch/projects/ARAP/arap_web.pdf}
}

@article{viterbi1967tit,
  title   = {Error bounds for convolutional codes and an asymptotically optimum decoding algorithm},
  author  = {Viterbi, A.},
  journal = {IEEE Trans. on Information Theory},
  volume  = {13},
  number  = {2},
  pages   = {260--269},
  year    = {1967},
  url     = {http://www.essrl.wustl.edu/~jao/itrg/viterbi.pdf}
}

@book{heath1996computing,
  author    = {Heath, M. T.},
  title     = {{Scientific Computing: An Introductory Survey}},
  year      = {1996},
  edition   = {2nd},
  publisher = {McGraw-Hill Higher Education}
}

@inproceedings{shoemake1985siggraph,
  author    = {Shoemake, K.},
  title     = {{Animating Rotation with Quaternion Curves}},
  booktitle = siggraph,
  year      = {1985},
  url       = {http://run.usc.edu/cs520-s15/assign2/p245-shoemake.pdf}
}

@inproceedings{herda2000ca,
  author    = {L. Herda and P. Fua and R. Plankers and R. Boulic and D. Thalmann},
  booktitle = {Proc. on Computer Animation},
  title     = {Skeleton-based motion capture for robust reconstruction of human motion},
  year      = {2000},
  url       = {https://infoscience.epfl.ch/record/98916/files/HerdaFPBT00.pdf}
}

@inproceedings{gall2009cvpr,
  title     = {{Motion capture using joint skeleton tracking and surface estimation}},
  author    = {Gall, J. and Stoll, C. and De Aguiar, E. and Theobalt, C. and Rosenhahn, B. and Seidel, H.P.},
  booktitle = cvprold,
  year      = {2009},
  url       = {https://www.tnt.uni-hannover.de/papers/data/773/773_1.pdf}
}

@article{schwarz2012ivc,
  author  = {L.A. Schwarz and A. Mkhitaryan and D. Mateus and N. Navab},
  title   = {Human skeleton tracking from depth data using geodesic distances and optical flow},
  journal = {Image and Vision Computing},
  volume  = {30},
  number  = {3},
  pages   = {217--226},
  year    = {2012}
}

@inproceedings{bouaziz2016siggraph,
  author    = {Bouaziz, S. and Tagliasacchi, A. and Li, H. and Pauly, M.},
  title     = {Modern Techniques and Applications for Real-time Non-rigid Registration},
  booktitle = {SIGGRAPH ASIA 2016 Courses},
  year      = {2016},
  url       = {https://gfx.uvic.ca/pubs/2016/regcourse_siga16/paper.pdf}
}

@inproceedings{mactavish2015crv,
  title     = {At all costs: A comparison of robust cost functions for camera correspondence outliers},
  author    = {MacTavish, K. and Barfoot, T. D.},
  booktitle = crv,
  year      = {2015},
  url       = {http://ncfrn.mcgill.ca/members/pubs/AtAllCosts_mactavish_crv15.pdf}
}

@article{hess1997wr,
  title    = {Use of the extended BBCH scaleâgeneral for the descriptions of the growth stages of mono; and dicotyledonous weed species},
  author   = {Hess, M and Barralis, G and Bleiholder, H and Buhr, L and Eggers, TH and Hack, H and Stauss, R},
  journal  = {Weed Research},
  volume   = {37},
  number   = {6},
  pages    = {433--441},
  year     = {1997},
  abstract = {The extended BBCH scale is a system for a uniform coding of phenologicaliy similar growth stages of all monoâ and dicotyledonous plant species, based on the well known cereal code of Zadoks et al. (1974). The BBCH key is it decimal system, with 10 principal growth stages and up to 10 secondary ones, starting with seed germination, sprouting of perennials, progressing through leaf production and extension growth to flowering and senescence. Therefore, it can also be a suitable tool to define the growth stages of different weed species. To encourage further use of the BBCH scale in weed research, definitions of the codes have been more closely adapted to weeds. Possible problems are discussed and guidelines for correct use are given.}
}

@inproceedings{palazzolo2019iros,
  author    = {E. Palazzolo and J. Behley and P. Lottes and P. Giguere and C. Stachniss},
  title     = {{ReFusion: 3D Reconstruction in Dynamic Environments for RGB-D Cameras Exploiting Residuals}},
  year      = 2019,
  booktitle = iros,
  url       = {proceedings: palazzolo2019iros.pdf}
}

@inproceedings{barron2019cvpr,
  author    = {J. T. Barron},
  title     = {{A General and Adaptive Robust Loss Function}},
  booktitle = cvpr,
  year      = 2019,
  abstract  = {We present a generalization of the Cauchy/Lorentzian, Geman-McClure, Welsch/Leclerc, generalized Charbonnier, Charbonnier/pseudo-Huber/L1-L2, and L2 loss functions. By introducing robustness as a continuous parameter, our loss function allows algorithms built around robust loss minimization to be generalized, which improves performance on basic vision tasks such as registration and clustering. Interpreting our loss as the negative log of a univariate density yields a general probability distribution that includes normal and Cauchy distributions as special cases. This probabilistic interpretation enables the training of neural networks in which the robustness of the loss automatically adapts itself during training, which improves performance on learning-based tasks such as generative image synthesis and unsupervised monocular depth estimation, without requiring any manual parameter tuning.},
  url       = {proceedings: barron2019cvpr.pdf}
}

@inproceedings{brasch2018iros,
  author    = {N. Brasch and A. Bozic and J. Lallemand and F. Tombari},
  title     = {{Semantic Monocular SLAM for Highly Dynamic Environments}},
  booktitle = iros,
  year      = 2018
}

@inproceedings{tateno2017cvpr,
  author    = {K. Tateno and F. Tombari and I. Laina and N. Navab},
  title     = {{CNN-SLAM: Real-time dense monocular SLAM with learned depth prediction}},
  booktitle = cvprold,
  year      = 2017,
  keywords  = {image reconstruction;image texture;learning (artificial intelligence);mobile robots;neural nets;robot vision;SLAM (robots);stereo image processing;dense monocular reconstruction;depth measurements;direct monocular SLAM;CNN-SLAM;real-time dense monocular SLAM;deep neural network;convolutional neural networks;depth maps prediction learning;Simultaneous localization and mapping;Image reconstruction;Cameras;Semantics;Pose estimation;Three-dimensional displays}
}

@inproceedings{yu2018iros,
  author    = {C. Yu and Z. Liu and X. Liu and F. Xie and Y. Yang and Q. Wei and Q. Fei},
  title     = {{DS-SLAM: A Semantic Visual SLAM towards Dynamic Environments}},
  booktitle = iros,
  year      = 2018,
  keywords  = {Semantics;Simultaneous localization and mapping;Image segmentation;Feature extraction;Heuristic algorithms;Three-dimensional displays;Optical flow}
}

@inproceedings{li2018eccv,
  author    = {P. Li and T. Qin and S. Shen},
  title     = {{Stereo Vision-based Semantic 3D Object and Ego-motion Tracking for Autonomous Driving}},
  booktitle = eccv,
  year      = {2018}
}

@inproceedings{yang2017iros,
  author    = {S. Yang and Y. Huang and S. Scherer},
  title     = {{Semantic 3D occupancy mapping through efficient high order CRFs}},
  booktitle = iros,
  year      = {2017}
}

@article{jeong2018sensors,
  author   = {J. Jeong and T.S. Yoon and J.B. Park},
  title    = {{Towards a Meaningful 3D Map Using a 3D Lidar and a Camera}},
  journal  = sensors,
  year     = {2018},
  volume   = {18},
  number   = {8},
  pages    = {2571},
  url      = {http://www.mdpi.com/1424-8220/18/8/2571},
  abstract = {Semantic 3D maps are required for various applications including robot navigation and surveying, and their importance has significantly increased. Generally, existing studies on semantic mapping were camera-based approaches that could not be operated in large-scale environments owing to their computational burden. Recently, a method of combining a 3D Lidar with a camera was introduced to address this problem, and a 3D Lidar and a camera were also utilized for semantic 3D mapping. In this study, our algorithm consists of semantic mapping and map refinement. In the semantic mapping, a GPS and an IMU are integrated to estimate the odometry of the system, and subsequently, the point clouds measured from a 3D Lidar are registered by using this information. Furthermore, we use the latest CNN-based semantic segmentation to obtain semantic information on the surrounding environment. To integrate the point cloud with semantic information, we developed incremental semantic labeling including coordinate alignment, error minimization, and semantic information fusion. Additionally, to improve the quality of the generated semantic map, the map refinement is processed in a batch. It enhances the spatial distribution of labels and removes traces produced by moving vehicles effectively. We conduct experiments on challenging sequences to demonstrate that our algorithm outperforms state-of-the-art methods in terms of accuracy and intersection over union.}
}

@article{sun2018ral,
  author   = {L. Sun and Z. Yan and A. Zaganidis and C. Zhao and T. Duckett},
  title    = {{Recurrent-OctoMap: Learning State-Based Map Refinement for Long-Term Semantic Mapping With 3-D-Lidar Data}},
  journal  = ral,
  year     = {2018},
  volume   = {3},
  number   = {4},
  pages    = {3749--3756},
  keywords = {Bayes methods;image motion analysis;image representation;image sequences;learning (artificial intelligence);Markov processes;optical radar;recurrent neural nets;3-D semantic map refinement;consecutive predictive probabilities;learning approach;semantic features;recurrent neural network;Recurrent-OctoMap;semantic mapping process;mapping system;state-based map refinement;long-term semantic mapping;3-D-Lidar data;semantic understanding;3-D refinement;semantic observations;Bayes update;sequence-to-sequence encoding-decoding;robust 3-D localization and mapping system;dynamic environment;arbitrary memory length;ETH long-term 3-D Lidar dataset;Semantics;Three-dimensional displays;Laser radar;Two dimensional displays;Simultaneous localization and mapping;Feature extraction;Recurrent neural networks;Mapping;simultaneous localization and mapping (SLAM);deep learning in robotics and automation;object detection;segmentation and categorization}
}


@inproceedings{wang2017iros,
  author    = {J. Wang and J. Kim},
  title     = {{Semantic segmentation of urban scenes with a location prior map using lidar measurements}},
  booktitle = iros,
  year      = {2017},
  keywords  = {cartography;geophysical image processing;image segmentation;optical radar;random processes;semantic segmentation;lidar measurements;urban environments;road-normal coordinates;sensor measurements;conditional random field model;urban scenes segmentation;location prior map;road layout estimation;Roads;Layout;Three-dimensional displays;Image segmentation;Semantics;Buildings;Cameras}
}


@article{zaganidis2018ral,
  author   = {A. Zaganidis and L. Sun and T. Duckett and G. Cielniak},
  title    = {{Integrating Deep Semantic Segmentation Into 3-D Point Cloud Registration}},
  journal  = ral,
  year     = {2018},
  volume   = {3},
  number   = {4},
  pages    = {2942--2949},
  keywords = {image registration;image segmentation;iterative methods;learning (artificial intelligence);neural nets;normal distribution;transforms;3D point cloud registration;iterative closest point equivalent;point cloud partitioning;semantic-assisted normal distribution transform;registration errors;semantic labels;deep semantic segmentation;Semantic3d.net;publicly available classification data set;deep neural network;NDT registration pipeline;registration algorithm;SE-NDT;Semantics;Three-dimensional displays;Transforms;Gaussian distribution;Iterative closest point algorithm;Image color analysis;Partitioning algorithms;Localization;SLAM;robotics in agriculture and forestry}
}


@inproceedings{parkison2018bmvc,
  author    = {S.A. Parkison and L. Gan and M.G. Jadidi and  R.M. Eustice},
  title     = {{Semantic Iterative Closest Point through Expectation-Maximization}},
  booktitle = bmvc,
  year      = {2018},
  url       = {http://bmvc2018.org/contents/papers/1073.pdf}
}


@inproceedings{yan2014threedv,
  author    = {J. Yan and D. Chen and H. Myeong and T. Shiratori and Y. Ma},
  title     = {{Automatic Extraction of Moving Objects from Image and LIDAR Sequences}},
  booktitle = threedv,
  year      = {2014},
  keywords  = {computer vision;image segmentation;image sequences;object detection;optical radar;moving object automatic extraction;image sequences;LIDAR sequences;image sequence;computer vision applications;image segmention;image-based moving object detection;modern street-side imagery;3D point clouds;sparse measurements;2D images;object removal;3D reconstruction;rendering;Three-dimensional displays;Laser radar;Image color analysis;Image segmentation;Shape;Object detection;Image sequences}
}


@inproceedings{dube2018rss,
  author    = {R. Dub\'e and A. Cramariuc and D. Dugas and J. Nieto and R. Siegwart and C. Cadena},
  title     = {{SegMap: 3D Segment Mapping using Data-Driven Descriptors}},
  booktitle = rss,
  year      = {2018}
}


@inproceedings{li2017cvpr,
  author    = {X. Li and Z. Liu and P. Luo and C.C. Loy and X. Tang},
  title     = {{Not All Pixels Are Equal: Difficulty-Aware Semantic Segmentation via Deep Layer Cascade}},
  booktitle = cvprold,
  year      = {2017}
}


@inproceedings{liang2018eccv,
  author    = {M. Liang and B. Yang and S. Wang and R. Urtasun},
  title     = {Deep continuous fusion for multi-sensor 3d object detection},
  booktitle = eccv,
  year      = {2018}
}


@inproceedings{kim2015iccv,
  author    = {C. Kim and F. Li and A. Ciptadi and J.M. Rehg},
  title     = {{Multiple hypothesis tracking revisited}},
  booktitle = iccvold,
  year      = {2015}
}


@inproceedings{lianos2018eccv,
  author    = {K.N. Lianos and J.L. Schonberger and M. Pollefeys and T. Sattler},
  title     = {{VSO: Visual Semantic Odometry}},
  booktitle = eccv,
  year      = {2018}
}


@article{ganti2018arxiv,
  author   = {P. Ganti and S.L. Waslander},
  title    = {{Network Uncertainty Informed Semantic Feature Selection for Visual SLAM}},
  journal  = arxiv,
  year     = 2018,
  volume   = {arXiv:1811.11946},
  url      = {http://arxiv.org/pdf/1811.11946v2},
  abstract = {In order to facilitate long-term localization using a visual simultaneous localization and mapping (SLAM) algorithm, careful feature selection can help ensure that reference points persist over long durations and the runtime and storage complexity of the algorithm remain consistent. We present SIVO (Semantically Informed Visual Odometry and Mapping), a novel information-theoretic feature selection method for visual SLAM which incorporates semantic segmentation and neural network uncertainty into the feature selection pipeline. Our algorithm selects points which provide the highest reduction in Shannon entropy between the entropy of the current state and the joint entropy of the state, given the addition of the new feature with the classification entropy of the feature from a Bayesian neural network. Each selected feature significantly reduces the uncertainty of the vehicle state and has been detected to be a static object (building, traffic sign, etc.) repeatedly with a high confidence. This selection strategy generates a sparse map which can facilitate long-term localization. The KITTI odometry dataset is used to evaluate our method, and we also compare our results against ORB_SLAM2. Overall, SIVO performs comparably to the baseline method while reducing the map size by almost 70\%.}
}

@inproceedings{pomerleau2014icra,
  author    = {F. Pomerleau and P. Kr{\"u}siand and F. Colas and P. Furgale and R. Siegwart},
  title     = {{Long-term 3D Map Maintenance in Dynamic Environments}},
  booktitle = icra,
  year      = {2014}
}

@article{bresson2017tiv,
  author  = {G. Bresson and Z. Alsayed and L. Yu and S. Glaser},
  title   = {{Simultaneous Localization and Mapping: A Survey of Current Trends in Autonomous Driving}},
  journal = tiv,
  year    = 2017,
  volume  = {2},
  number  = {3},
  pages   = {194--220}
}

@article{jeong2018esa,
  author   = {J. Jeong and T. S. Yoon and J. B. Park},
  title    = {{Multimodal Sensor-Based Semantic 3D Mapping for a Large-Scale Environment}},
  journal  = esa,
  year     = {2018},
  volume   = {105},
  pages    = {1--10},
  keywords = {Semantic mapping, Semantic reconstruction, 3D mapping, Semantic segmentation, 3D refinement}
}


@inproceedings{haehnel2003ijcai,
  title     = {An extension of the ICP algorithm for modeling nonrigid objects with mobile robots},
  author    = {Haehnel, D. and Thrun, S. and Burgard, W.},
  booktitle = ijcai,
  year      = {2003},
  url       = {https://www.ijcai.org/Proceedings/03/Papers/132.pdf},
  keywords  = {ICP, non-rigid, registration},
  abstract  = {The iterative closest point (ICP) algorithm is a popular method for modeling 3D objects from range data. The classical ICP algorithm rests on a rigid surface assumption. Building on recent work on nonrigid object models, this paper presents an ICP algorithm capable of modeling nonrigid objects, where individual scans may be subject to local deformations. We describe an integrated mathematical framework for simultaneously registering scans and recovering the surface configuration. To tackle the resulting high-dimensional optimization problems, we introduce a hierarchical method that first matches a coarse skeleton of scan points, then adapts local scan patches. The approach is implemented for a mobile robot capable of acquiring 3D models of objects.}
}



@inproceedings{wang2018threedv,
  author    = {Y. Wang and D. Tan and N. Navab and F. Tombari},
  title     = {{Adversarial Semantic Scene Completion from a Single Depth Image}},
  booktitle = threedv,
  year      = {2018},
  url       = {https://arxiv.org/pdf/1810.10901.pdf}
}

@inproceedings{milioto2019icra-fiass,
  author    = {A. Milioto and L. Mandtler and C. Stachniss},
  title     = {{Fast Instance and Semantic Segmentation Exploiting Local Connectivity, Metric Learning, and One-Shot Detection for Robotics}},
  booktitle = icra,
  year      = 2019,
  url       = {http://www.ipb.uni-bonn.de/wp-content/papercite-data/pdf/milioto2019icra-fiass.pdf}
}

@inproceedings{zhou2018cvpr-velf,
  author    = {Yin Zhou and Oncel Tuzel},
  title     = {{VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection}},
  booktitle = cvprold,
  year      = 2018,
  abstract  = {Accurate detection of objects in 3D point clouds is a central problem in many applications, such as autonomous navigation, housekeeping robots, and augmented/virtual reality. To interface a highly sparse LiDAR point cloud with a region proposal network (RPN), most existing efforts have focused on hand-crafted feature representations, for example, a bird's eye view projection. In this work, we remove the need of manual feature engineering for 3D point clouds and propose VoxelNet, a generic 3D detection network that unifies feature extraction and bounding box prediction into a single stage, end-to-end trainable deep network. Specifically, VoxelNet divides a point cloud into equally spaced 3D voxels and transforms a group of points within each voxel into a unified feature representation through the newly introduced voxel feature encoding (VFE) layer. In this way, the point cloud is encoded as a descriptive volumetric representation, which is then connected to a RPN to generate detections. Experiments on the KITTI car detection benchmark show that VoxelNet outperforms  the state-of-the-art LiDAR based 3D detection methods by a large margin. Furthermore, our network learns an effective discriminative  representation of objects with various geometries, leading to encouraging results in 3D detection of pedestrians and cyclists, based on only LiDAR.},
  url       = {http://openaccess.thecvf.com/content_cvpr_2018/html/../../content_cvpr_2018/papers/Zhou_VoxelNet_End-to-End_Learning_CVPR_2018_paper.pdf}
}

@inproceedings{luo2018cvpr-fafr,
  author    = {W. Luo and B. Yang and R. Urtasun},
  title     = {{Fast and Furious: Real Time End-to-End 3D Detection, Tracking and Motion Forecasting With a Single Convolutional Net}},
  booktitle = cvprold,
  year      = 2018,
  abstract  = {In this paper we propose a novel  deep neural network that is able to jointly reason about 3D detection, tracking and motion forecasting  given data captured by a 3D sensor. By jointly reasoning about these tasks, our  holistic approach is  more robust to occlusion as well as sparse data at range. Our approach performs 3D convolutions across space and time over a bird's eye view representation of the 3D world, which  is very efficient in terms of both  memory and computation. Our experiments on a new very large scale dataset captured  in several north american cities,   show that we can outperform the state-of-the-art by a large margin. Importantly, by sharing computation we can perform all  tasks in as little as 30 ms.},
  url       = {proceedings: luo2018cvpr-fafr.pdf}
}

@inproceedings{qi2020cvpr,
  title     = {Imvotenet: Boosting 3d object detection in point clouds with image votes},
  author    = {Charles R. Qi and Xinlei Chen and Or Litany and Leonidas J. Guibas},
  booktitle = cvpr,
  year      = {2020},
  url       = {proceedings: qi2020cvpr.pdf}
}

@inproceedings{qi2018cvpr-fpfo,
  author    = {C.R. Qi and W. Liu and C. Wu and H. Su and L.J. Guibas},
  title     = {{Frustum PointNets for 3D Object Detection From RGB-D Data}},
  booktitle = cvprold,
  year      = 2018,
  abstract  = {In this work, we study 3D object detection from RGB-D data in both indoor and outdoor scenes. While previous methods focus on images or 3D voxels, often obscuring natural 3D patterns and invariances of 3D data, we directly operate on raw point clouds by popping up RGB-D scans. However, a key challenge of this approach is how to efficiently localize objects in point clouds of large-scale scenes (region proposal). Instead of solely relying on 3D proposals, our method leverages both mature 2D object detectors and advanced 3D deep learning for object localization, achieving efficiency as well as high recall for even small objects. Benefited from learning directly in raw point clouds, our method is also able to precisely estimate 3D bounding boxes even under strong occlusion or with very sparse points. Evaluated on KITTI and SUN RGB-D 3D detection benchmarks, our method outperforms the state of the art by remarkable margins while having real-time capability.},
  url       = {http://openaccess.thecvf.com/content_cvpr_2018/html/../../content_cvpr_2018/papers/Qi_Frustum_PointNets_for_CVPR_2018_paper.pdf}
}

@inproceedings{yang2019iccv,
  author    = {Z. Yang and Y. Sun and S. Liu and X. Shen and J. Jia},
  title     = {{STD: Sparse-to-Dense 3D Object Detector for Point Cloud}},
  booktitle = iccv,
  year      = 2019,
  url       = {http://jiaya.me/papers/std_iccv19.pdf}
}

@article{yang2019tro,
  title   = {{Cubeslam: Monocular 3-D object SLAM}},
  author  = {Yang, Shichao and Scherer, Sebastian},
  journal = tro,
  volume  = {35},
  number  = {4},
  pages   = {925--938},
  year    = {2019},
  url     = {https://arxiv.org/pdf/1806.00557.pdf}
}

@article{zeng2018ral,
  author   = {Y. Zeng and Y. Hu and S. Liu and J. Ye and Y. Han and X. Li and N. Sun},
  title    = {{RT3D: Real-Time 3D Vehicle Detection in LiDAR Point Cloud for Autonomous Driving}},
  journal  = ral,
  volume   = {3},
  number   = {4},
  pages    = {3434--3440},
  year     = 2018,
  keywords = {Deep Learning in Robotics and Automation, Intelligent Transportation Systems},
  abstract = {For autonomous driving, vehicle detection is the prerequisite for many tasks like collision avoidance and path planning. In this paper, we present a real-time 3D vehicle detection method (RT3D) that utilizes pure LiDAR point cloud to predict the location, orientation, and size of vehicles. In contrast to previous 3D object detection methods, we used a pre-RoIpooling convolution technique that moves a majority of the convolution operations to ahead of the RoI pooling, leaving just a small part behind, so that significantly boosts the computation efficiency. We also propose a pose-sensitive feature map design which can be strongly activated by the relative poses of vehicles, leading to a high regression accuracy on the location, orientation, and size of vehicles. Experiments on the KITTI benchmark dataset show that the RT3D is not only able to deliver competitive detection accuracy against state-of-the-art methods, but also the first LiDAR-based 3D vehicle detection work that completes detection within 0.09 seconds which is even shorter than the scan period of mainstream LiDAR sensors.}
}


@article{shi2019arxiv,
  author   = {S. Shi and Z. Wang and J. Shi and X. Wang and H. Li},
  title    = {{From Points to Parts: 3D Object Detection from Point Cloud with Part-aware and Part-aggregation Network}},
  journal  = arxiv,
  year     = 2019,
  volume   = {arXiv:1907.03670},
  url      = {http://arxiv.org/pdf/1907.03670v2},
  abstract = {3D object detection from LiDAR point cloud is a challenging problem in 3D scene understanding and has many practical applications. In this paper, we extend our preliminary work PointRCNN to a novel and strong point-cloud-based 3D object detection framework, the part-aware and aggregation neural network (Part-$A^2$ net). The whole framework consists of the part-aware stage and the part-aggregation stage. Firstly, the part-aware stage for the first time fully utilizes free-of-charge part supervisions derived from 3D ground-truth boxes to simultaneously predict high quality 3D proposals and accurate intra-object part locations. The predicted intra-object part locations within the same proposal are grouped by our new-designed RoI-aware point cloud pooling module, which results in an effective representation to encode the geometry-specific features of each 3D proposal. Then the part-aggregation stage learns to re-score the box and refine the box location by exploring the spatial relationship of the pooled intra-object part locations. Extensive experiments are conducted to demonstrate the performance improvements from each component of our proposed framework. Our Part-$A^2$ net outperforms all existing 3D detection methods and achieves new state-of-the-art on KITTI 3D object detection dataset by utilizing only the LiDAR point cloud data.}
}


@inproceedings{lang2019cvpr,
  author    = {A.H. Lang and S. Vora and H. Caesar and L. Zhou and J. Yang and O. Beijbom},
  title     = {{PointPillars: Fast Encoders for Object Detection From Point Clouds}},
  booktitle = cvpr,
  year      = 2019,
  abstract  = {Object detection in point clouds is an important aspect of many robotics applications such as autonomous driving. In this paper, we consider the problem of encoding a point cloud into a format appropriate for a downstream detection pipeline. Recent literature suggests two types of encoders; fixed encoders tend to be fast but sacrifice accuracy, while encoders that are learned from data are more accurate, but slower. In this work, we propose PointPillars, a novel encoder which utilizes PointNets to learn a representation of point clouds organized in vertical columns (pillars). While the encoded features can be used with any standard 2D convolutional detection architecture, we further propose a lean downstream network. Extensive experimentation shows that PointPillars outperforms previous encoders with respect to both speed and accuracy by a large margin. Despite only using lidar, our full detection pipeline significantly outperforms the state of the art, even among fusion methods, with respect to both the 3D and bird's eye view KITTI benchmarks. This detection performance is achieved while running at 62 Hz: a 2 - 4 fold runtime improvement. A faster version of our method matches the state of the art at 105 Hz. These benchmarks suggest that PointPillars is an appropriate encoding for object detection in point clouds.},
  url       = {proceedings: lang2019cvpr.pdf}
}

@inproceedings{wang2019cvpr-asia,
  author    = {X. Wang and S. Liu and X. Shen and C. Shen and J. Jia},
  title     = {{Associatively Segmenting Instances and Semantics in Point Clouds}},
  booktitle = cvpr,
  year      = 2019,
  abstract  = {A 3D point cloud describes the real scene precisely and intuitively. To date how to segment diversified elements in such an informative 3D scene is rarely discussed. In this paper, we first introduce a simple and flexible framework to segment instances and semantics in point clouds simultaneously. Then, we propose two approaches which make the two tasks take advantage of each other, leading to a win-win situation. Specifically, we make instance segmentation benefit from semantic segmentation through learning semantic-aware point-level instance embedding. Meanwhile, semantic features of the points belonging to the same instance are fused together to make more accurate per-point semantic predictions. Our method largely outperforms the state-of-the-art method in 3D instance segmentation along with a significant improvement in 3D semantic segmentation. Code has been made available at: https://github.com/WXinlong/ASIS.},
  url       = {proceedings: wang2019cvpr-asia.pdf}
}

@inproceedings{shi2019cvpr,
  author    = {S. Shi and X. Wang and H. Li},
  title     = {{PointRCNN: 3D Object Proposal Generation and Detection From Point Cloud}},
  booktitle = cvpr,
  year      = 2019,
  abstract  = {In this paper, we propose PointRCNN for 3D object detection from raw point cloud. The whole framework is composed of two stages: stage-1 for the bottom-up 3D proposal generation and stage-2 for refining proposals in the canonical coordinates to obtain the final detection results. Instead of generating proposals from RGB image or projecting point cloud to bird's view or voxels as previous methods do, our stage-1 sub-network directly generates a small number of high-quality 3D proposals from point cloud in a bottom-up manner via segmenting the point cloud of the whole scene into foreground points and background. The stage-2 sub-network transforms the pooled points of each proposal to canonical coordinates to learn better local spatial features, which is combined with global semantic features of each point learned in stage-1 for accurate box refinement and confidence prediction. Extensive experiments on the 3D detection benchmark of KITTI dataset show that our proposed architecture outperforms state-of-the-art methods with remarkable margins by using only point cloud as input. The code is available at https://github.com/sshaoshuai/PointRCNN.},
  url       = {proceedings: shi2019cvpr.pdf}
}

@inproceedings{chen2019iccv,
  author    = {Y. Chen and S. Liu and X. Shen and J. Jia},
  title     = {{Fast Point R-CNN}},
  booktitle = iccv,
  year      = 2019,
  url       = {http://jiaya.me/papers/fprcnn_iccv19.pdf}
}

@inproceedings{pham2019cvpr,
  author    = {Q.H. Pham and D.T. Nguyen and B.S. Hua and G. Roig and S.K. Yeung},
  title     = {{JSIS3D: Joint Semantic-Instance Segmentation of 3D Point Clouds With Multi-Task Pointwise Networks and Multi-Value Conditional Random Fields}},
  booktitle = cvpr,
  year      = 2019,
  abstract  = {Deep learning techniques have become the to-go models for most vision-related tasks on 2D images. However, their power has not been fully realised on several tasks in 3D space, e.g., 3D scene understanding. In this work, we jointly address the problems of semantic and instance segmentation of 3D point clouds. Specifically, we develop a multi-task pointwise network that simultaneously performs two tasks: predicting the semantic classes of 3D points and embedding the points into high-dimensional vectors so that points of the same object instance are represented by similar embeddings. We then propose a multi-value conditional random field model to incorporate the semantic and instance labels and formulate the problem of semantic and instance segmentation as jointly optimising labels in the field model. The proposed method is thoroughly evaluated and compared with existing methods on different indoor scene datasets including S3DIS and SceneNN. Experimental results showed the robustness of the proposed joint semantic-instance segmentation scheme over its single components. Our method also achieved state-of-the-art performance on semantic segmentation.},
  url       = {proceedings: pham2019cvpr.pdf}
}

@inproceedings{yang2019neurips,
  author    = {B. Yang and J. Wang and R. Clark and Q. Hu and S. Wang and A. Markham and N. Trigoni},
  title     = {{Learning Object Bounding Boxes for 3D Instance Segmentation on Point Clouds}},
  booktitle = neurips,
  year      = 2019,
  url       = {https://arxiv.org/pdf/1906.01140.pdf}
}

@article{chen2019jmlr,
  title   = {{SimpleDet: A Simple and Versatile Distributed Framework for Object Detection and Instance Recognition}},
  author  = {Y. Chen and C. Han and Y. Li and Z. Huang and Y. Jiang and N. Wang and Z. Zhang},
  journal = {Journal of Machine Learning Research(156):1â8, 2019},
  volume  = 156,
  pages   = {1--8},
  year    = {2019},
  url     = {https://arxiv.org/pdf/1903.05831.pdf}
}

@inproceedings{kirillov2019cvpr,
  author    = {A. Kirillov and R. Girshick and K. He and P. Dollar},
  title     = {{Panoptic Feature Pyramid Networks}},
  booktitle = cvpr,
  year      = 2019,
  abstract  = {The recently introduced panoptic segmentation task has renewed our community's interest in unifying the tasks of instance segmentation (for thing classes) and semantic segmentation (for stuff classes). However, current state-of-the-art methods for this joint task use separate and dissimilar networks for instance and semantic segmentation, without performing any shared computation. In this work, we aim to unify these methods at the architectural level, designing a single network for both tasks. Our approach is to endow Mask R-CNN, a popular instance segmentation method, with a semantic segmentation branch using a shared Feature Pyramid Network (FPN) backbone. Surprisingly, this simple baseline not only remains effective for instance segmentation, but also yields a lightweight, top-performing method for semantic segmentation. In this work, we perform a detailed study of this minimally extended version of Mask R-CNN with FPN, which we refer to as Panoptic FPN, and show it is a robust and accurate baseline for both tasks. Given its effectiveness and conceptual simplicity, we hope our method can serve as a strong baseline and aid future research in panoptic segmentation.},
  url       = {proceedings: kirillov2019cvpr.pdf}
}

@inproceedings{kirillov2019cvpr-ps,
  author    = {A. Kirillov and K. He and R. Girshick and C. Rother and P. Doll{\'a}r},
  title     = {{Panoptic Segmentation}},
  booktitle = cvpr,
  year      = 2019,
  abstract  = {We propose and study a task we name panoptic segmentation (PS). Panoptic segmentation unifies the typically distinct tasks of semantic segmentation (assign a class label to each pixel) and instance segmentation (detect and segment each object instance). The proposed task requires generating a coherent scene segmentation that is rich and complete, an important step toward real-world vision systems. While early work in computer vision addressed related image/scene parsing tasks, these are not currently popular, possibly due to lack of appropriate metrics or associated recognition challenges. To address this, we propose a novel panoptic quality (PQ) metric that captures performance for all classes (stuff and things) in an interpretable and unified manner. Using the proposed metric, we perform a rigorous study of both human and machine performance for PS on three existing datasets, revealing interesting insights about the task. The aim of our work is to revive the interest of the community in a more unified view of image segmentation. For more analysis and up-to-date results, please check the arXiv version of the paper:  \smallhttps://arxiv.org/abs/1801.00868 .},
  url       = {proceedings: kirillov2019cvpr-ps.pdf}
}

@inproceedings{hou2019cvpr-siso,
  author    = {J. Hou and A. Dai and M. Niessner},
  title     = {{3D-SIS: 3D Semantic Instance Segmentation of RGB-D Scans}},
  booktitle = cvpr,
  year      = 2019,
  abstract  = {We introduce 3D-SIS, a novel neural network architecture for 3D semantic instance segmentation in commodity RGB-D scans. The core idea of our method to jointly learn from both geometric and color signal, thus enabling accurate instance predictions. Rather than operate solely on 2D frames, we observe that most computer vision applications have multi-view RGB-D input available, which we leverage to construct an approach for 3D instance segmentation that effectively fuses together these multi-modal inputs. Our network leverages high-resolution RGB input by associating 2D images with the volumetric grid based on the pose alignment of the 3D reconstruction. For each image, we first extract 2D features for each pixel with a series of 2D convolutions; we then backproject the resulting feature vector to the associated voxel in the 3D grid. This combination of 2D and 3D feature learning allows significantly higher accuracy object detection and instance segmentation than state-of-the-art alternatives. We show results on both synthetic and real-world public benchmarks, achieving an improvement in mAP of over 13 on real-world data.},
  url       = {proceedings: hou2019cvpr-siso.pdf}
}

@inproceedings{yi2019cvpr,
  author    = {L. Yi and W. Zhao and H. Wang and M. Sung and L. Guibas},
  title     = {{GSPN: Generative Shape Proposal Network for 3D Instance Segmentation in Point Cloud}},
  booktitle = cvpr,
  year      = 2019,
  abstract  = {We introduce a novel 3D object proposal approach named Generative Shape Proposal Network (GSPN) for instance segmentation in point cloud data. Instead of treating object proposal as a direct bounding box regression problem, we take an analysis-by-synthesis strategy and generate proposals by reconstructing shapes from noisy observations in a scene. We incorporate GSPN into a novel 3D instance segmentation framework named Region-based PointNet (R-PointNet) which allows flexible proposal refinement and instance segmentation generation. We achieve state-of-the-art performance on several 3D instance segmentation tasks. The success of GSPN largely comes from its emphasis on geometric understandings during object proposal, greatly reducing proposals with low objectness.},
  url       = {proceedings: yi2019cvpr.pdf}
}

@inproceedings{yi2009iros,
  author    = {Yi, Chuho and Suh, Il Hong and Lim, Gi Hyun and Choi, Byung-Uk},
  title     = {{Bayesian robot localization using spatial object contexts}},
  booktitle = iros,
  year      = {2009},
  url       = {proceedings: yi2009iros.pdf}
}

@inproceedings{thomas2019iccv,
  author    = {H. Thomas and C.R. Qi and J. Deschaud and B. Marcotegui and F. Goulette and L.J. Guibas},
  title     = {{KPConv: Flexible and Deformable Convolution for Point Clouds}},
  booktitle = iccv,
  year      = 2019,
  abstract  = {We present Kernel Point Convolution (KPConv), a new design of point convolution, i.e. that operates on point clouds without any intermediate representation. The convolution weights of KPConv are located in Euclidean space by kernel points, and applied to the input points close to them. Its capacity to use any number of kernel points gives KPConv more flexibility than fixed grid convolutions. Furthermore, these locations are continuous in space and can be learned by the network. Therefore, KPConv can be extended to deformable convolutions that learn to adapt kernel points to local geometry. Thanks to a regular subsampling strategy, KPConv is also efficient and robust to varying densities. Whether they use deformable KPConv for complex tasks, or rigid KPconv for simpler tasks, our networks outperform state-of-the-art classification and segmentation approaches on several datasets. We also offer ablation studies and visualizations to provide understanding of what has been learned by KPConv and to validate the descriptive power of deformable KPConv.},
  url       = {proceedings: thomas2019iccv.pdf}
}


@inproceedings{porzi2019cvpr,
  author    = {L. Porzi and S. Rota Bulo and A. Colovic and P. Kontschieder},
  title     = {{Seamless Scene Segmentation}},
  booktitle = cvpr,
  year      = 2019,
  abstract  = {In this work we introduce a novel, CNN-based architecture that can be trained end-to-end to deliver seamless scene segmentation results. Our goal is to predict consistent semantic segmentation and detection results by means of a panoptic output format, going beyond the simple combination of independently trained segmentation and detection models. The proposed architecture takes advantage of a novel segmentation head that seamlessly integrates multi-scale features generated by a Feature Pyramid Network with contextual information conveyed by a light-weight DeepLab-like module. As additional contribution we review the panoptic metric and propose an alternative that overcomes its limitations when evaluating non-instance categories. Our proposed network architecture yields state-of-the-art results on three challenging street-level datasets, i.e. Cityscapes, Indian Driving Dataset and Mapillary Vistas.},
  url       = {proceedings: porzi2019cvpr.pdf}
}


@inproceedings{danielczuk2019icra-su3ofr,
  author    = {M. Danielczuk and M. Matl and S. Gupta and A. Lee and A. Li and J. Mahler and K. Goldberg},
  title     = {{Segmenting Unknown 3D Objects from Real Depth Images Using Mask R-CNN Trained on Synthetic Point Clouds}},
  booktitle = icra,
  year      = 2019,
  keywords  = {Object Detection, Segmentation and Categorization, Computer Vision for Automation},
  abstract  = {The ability to segment unknown objects in depth images has potential to enhance robot skills in grasping and object tracking. Recent computer vision research has demonstrated that Mask R-CNN can be trained to segment specific categories of objects in RGB images when massive handlabeled datasets are available. As generating these datasets is time-consuming, we instead train with synthetic depth images. Many robots now use depth sensors, and recent results suggest training on synthetic depth data can transfer successfully to the real world. We present a method for automated dataset generation and rapidly generate a synthetic training dataset of 50,000 depth images and 320,000 object masks using simulated heaps of 3D CAD models. We train a variant of Mask R-CNN with domain randomization on the generated dataset to perform category-agnostic instance segmentation without any handlabeled data and we evaluate the trained network, which we refer to as Synthetic Depth (SD) Mask R-CNN, on a set of real, high-resolution depth images of challenging, densely-cluttered bins containing objects with highly-varied geometry. SD Mask R-CNN outperforms point cloud clustering baselines by an absolute 15\% in Average Precision and 20\% in Average Recall on COCO benchmarks, and achieves performance levels similar to a Mask R-CNN trained on a massive, hand-labeled RGB dataset and fine-tuned on real images from the experimental setup. We deploy the model in an instance-specific grasping pipeline to demonstrate its usefulness in a robotics application. Code, the synthetic training dataset, and supplementary material are available at https://bit.ly/2letCuE.}
}

@inproceedings{he2017iccv-mr,
  author    = {K. He and G. Gkioxari and P. Doll{\'a}r and R. Girshick},
  title     = {{Mask R-CNN}},
  booktitle = iccvold,
  year      = 2017,
  abstract  = {We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without tricks, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code will be made available.},
  url       = {proceedings: he2017iccv-mr.pdf}
}

@article{lecun2015nature,
  author  = {Y. LeCun and Y. Bengio and G. Hinton},
  title   = {{Deep Learning}},
  journal = {Nature},
  volume  = 521,
  pages   = {436--444},
  year    = 2015,
  url     = {https://www.nature.com/articles/nature14539.pdf}
}

@inproceedings{xiong2019cvpr-uaup,
  author    = {Y. Xiong and R. Liao and H. Zhao and R. Hu and M. Bai and E. Yumer and R. Urtasun},
  title     = {{UPSNet: A Unified Panoptic Segmentation Network}},
  booktitle = cvpr,
  year      = 2019,
  abstract  = {In this paper, we propose a unified panoptic segmentation network (UPSNet) for tackling the newly proposed panoptic segmentation task. On top of a single backbone residual network, we first design a deformable convolution based semantic segmentation head and a Mask R-CNN style instance segmentation head which solve these two subtasks simultaneously. More importantly, we introduce a parameter-free panoptic head which solves the panoptic segmentation via pixel-wise classification. It first leverages the logits from the previous two heads and then innovatively expands the representation for enabling prediction of an extra unknown class which helps better resolving the conflicts between semantic and instance segmentation. Besides, it handles the challenge caused by the varying number of instances and permits back propagation to the bottom modules in an end-to-end manner. Extensive experimental results on Cityscapes, COCO and our internal dataset demonstrate that our UPSNet achieves state-of-the-art performance with much faster inference. Code has been made available at: https://github.com/uber-research/UPSNet},
  url       = {proceedings: xiong2019cvpr-uaup.pdf}
}

@inproceedings{liu2019cvpr-aenf,
  author    = {H. Liu and C. Peng and C. Yu and J. Wang and X. Liu and G. Yu and W. Jiang},
  title     = {{An End-To-End Network for Panoptic Segmentation}},
  booktitle = cvpr,
  year      = 2019,
  abstract  = {Panoptic segmentation, which needs to assign a category label to each pixel and segment each object instance simultaneously, is a challenging topic. Traditionally, the existing approaches utilize two independent models without sharing features, which makes the pipeline inefficient to implement. In addition, a heuristic method is usually employed to merge the results. However, the overlapping relationship between object instances is difficult to determine without sufficient context information during the merging process. To address the problems, we propose a novel end-to-end Occlusion Aware Network (OANet) for panoptic segmentation, which can efficiently and effectively predict both the instance and stuff segmentation in a single network. Moreover, we introduce a novel spatial ranking module to deal with the occlusion problem between the predicted instances. Extensive experiments have been done to validate the performance of our proposed method and promising results have been achieved on the COCO Panoptic benchmark.},
  url       = {proceedings: liu2019cvpr-aenf.pdf}
}


@inproceedings{li2019cvpr-aunf,
  author    = {Y. Li and X. Chen and Z. Zhu and L. Xie and G. Huang and D. Du and X. Wang},
  title     = {{Attention-Guided Unified Network for Panoptic Segmentation}},
  booktitle = cvpr,
  year      = 2019,
  abstract  = {This paper studies panoptic segmentation, a recently proposed task which segments foreground (FG) objects at the instance level as well as background (BG) contents at the semantic level. Existing methods mostly dealt with these two problems separately, but in this paper, we reveal the underlying relationship between them, in particular, FG objects provide complementary cues to assist BG understanding. Our approach, named the Attention-guided Unified Network (AUNet), is a unified framework with two branches for FG and BG segmentation simultaneously. Two sources of attentions are added to the BG branch, namely, RPN and FG segmentation mask to provide object-level and pixel-level attentions, respectively. Our approach is generalized to different backbones with consistent accuracy gain in both FG and BG segmentation, and also sets new state-of-the-arts both in the MS-COCO (46.5\% PQ) and Cityscapes (59.0\% PQ)  benchmarks.},
  url       = {proceedings: li2019cvpr-aunf.pdf}
}

@inproceedings{redmon2016cvpr,
  author    = {J. Redmon and S. Divvala and R. Girshick and A. Farhadi},
  title     = {{You Only Look Once: Unified, Real-Time Object Detection}},
  booktitle = cvprold,
  year      = 2016,
  abstract  = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance.  Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is less likely to predict false positives on background. Finally, YOLO learns very general representations of objects. It outperforms other detection methods, including DPM and R-CNN, when generalizing from natural images to other domains like artwork.},
  url       = {proceedings: redmon2016cvpr.pdf}
}

@inproceedings{liu2016eccv,
  author    = {W. Liu and D. Anguelov and D. Erhan and C. Szegedy and S. Reed and C.Y. Fu and A.C. Berg},
  title     = {{SSD: Single Shot MultiBox Detector}},
  booktitle = eccv,
  year      = 2016,
  url       = {https://arxiv.org/pdf/1512.02325.pdf}
}

@inproceedings{girshick2014cvpr,
  author    = {R. Girshick and J. Donahue and T. Darrell and J. Malik},
  title     = {{Rich feature hierarchies for accurate object detection and semantic segmentation}},
  booktitle = cvprold,
  year      = 2014,
  url       = {https://arxiv.org/pdf/1311.2524.pdf}
}


@article{zhou2019arxiv,
  author  = {X. Zhou and D. Wang and P. KrÃ¤henbÃ¼hl},
  title   = {{Objects as Points}},
  journal = arxiv,
  year    = 2019,
  volume  = {arXiv:1904.07850},
  url     = {http://arxiv.org/pdf/1904.07850v2}
}


@inproceedings{lin2017iccv-flfd,
  author    = {T.Y. Lin and P. Goyal and R. Girshick and K. He and P. Doll{\'a}r},
  title     = {{Focal Loss for Dense Object Detection}},
  booktitle = iccvold,
  year      = 2017,
  abstract  = {The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call RetinaNet. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors.},
  url       = {proceedings: lin2017iccv-flfd.pdf}
}


@article{yun2019ral,
  author   = {P. Yun and L. Tai and Y. Wang and C. Liu and M. Liu},
  title    = {{Focal Loss in 3D Object Detection}},
  journal  = ral,
  volume   = {4},
  number   = {2},
  pages    = {1263--1270},
  year     = 2019,
  keywords = {Deep Learning in Robotics and Automation, Object Detection, Segmentation and Categorization, Recognition},
  abstract = {3D object detection is still an open problem in autonomous driving scenes. When recognizing and localizing key objects from sparse 3D inputs, autonomous vehicles suffer from a larger continuous searching space and higher forebackground imbalance compared to image-based object detection. In this paper, we aim to solve this fore-background imbalance in 3D object detection. Inspired by the recent use of focal loss in image-based object detection, we extend this hard-mining improvement of binary cross entropy to pointcloud-based object detection and conduct experiments to show its performance based on two different 3D detectors: 3D-FCN and VoxelNet. The evaluation results show up to 11.2AP gains through the focal loss in a wide range of hyperparameters for 3D object detection.},
  url      = {https://arxiv.org/pdf/1809.06065.pdf}
}


@inproceedings{wang2015rss,
  author    = {D. Zeng Wang and I. Posner},
  title     = {{Voting for Voting in Online Point Cloud Object Detection}},
  booktitle = rss,
  year      = 2015,
  abstract  = {This paper proposes an efficient and effective scheme to applying the sliding window approach popular in computer vision to 3D data. Specifically, the sparse nature of the problem is exploited via a voting scheme to enable a search through all putative object locations at any orientation. We prove that this voting scheme is mathematically equivalent to a convolution on a sparse feature grid and thus enables the processing, in full 3D, of any point cloud irrespective of the number of vantage points required to construct it. As such it is versatile enough to operate on data from popular 3D laser scanners such as a Velodyne as well as on 3D data obtained from increasingly popular push-broom configurations. Our approach is "embarrassingly parallelisable" and capable of processing a point cloud containing over 100K points at eight orientations in less than 0.5s. For the object classes car, pedestrian and bicyclist the resulting detector achieves best-in-class detection and timing performance relative to prior art on the KITTI dataset as well as compared to another existing 3D object detection approach.},
  url       = {proceedings: wang2015rss.pdf}
}

@inproceedings{yang2018cvpr-prod,
  author    = {B. Yang and W. Luo and R. Urtasun},
  title     = {{PIXOR: Real-Time 3D Object Detection From Point Clouds}},
  booktitle = cvprold,
  year      = 2018,
  abstract  = {We address the problem of real-time 3D object detection from point clouds in the context of autonomous driving. Speed is critical as detection is a necessary component for safety. Existing approaches are, however, expensive in computation due to high dimensionality of point clouds. We utilize the 3D data more efficiently by representing the scene from the Bird's Eye View (BEV), and propose PIXOR, a proposal-free, single-stage detector that outputs oriented 3D object estimates decoded from pixel-wise neural network predictions. The input representation, network architecture, and model optimization are specially designed to balance high accuracy and real-time efficiency. We validate PIXOR on two datasets: the KITTI BEV object detection benchmark, and a large-scale 3D vehicle detection benchmark. In both datasets we show that the proposed detector surpasses  other state-of-the-art methods notably in terms of Average Precision (AP), while still runs at 10 FPS.},
  url       = {proceedings: yang2018cvpr-prod.pdf}
}

@inproceedings{gao2019iccv,
  author    = {N. Gao and Y. Shan and Y. Wang and X. Zhao and Y. Yu and M. Yang and K. Huang},
  title     = {{SSAP: Single-Shot Instance Segmentation With Affinity Pyramid}},
  booktitle = iccv,
  year      = 2019,
  abstract  = {Recently, proposal-free instance segmentation has received increasing attention due to its concise and efficient pipeline. Generally, proposal-free methods generate instance-agnostic semantic segmentation labels and instance-aware features to group pixels into different object instances. However, previous methods mostly employ separate modules for these two sub-tasks and require multiple passes for inference. We argue that treating these two sub-tasks separately is suboptimal. In fact, employing multiple separate modules significantly reduces the potential for application. The mutual benefits between the two complementary sub-tasks are also unexplored. To this end, this work proposes a single-shot proposal-free instance segmentation method that requires only one single pass for prediction. Our method is based on a pixel-pair affinity pyramid, which computes the probability that two pixels belong to the same instance in a hierarchical manner. The affinity pyramid can also be jointly learned with the semantic class labeling and achieve mutual benefits. Moreover, incorporating with the learned affinity pyramid, a novel cascaded graph partition module is presented to sequentially generate instances from coarse to fine. Unlike previous time-consuming graph partition methods, this module achieves 5x speedup and 9\% relative improvement on Average-Precision (AP). Our approach achieves new state of the art on the challenging Cityscapes dataset.},
  url       = {proceedings: gao2019iccv.pdf}
}

@inproceedings{ku2018iros-j3pgao,
  author    = {J. Ku and M. Mozifian and J. Lee and A. Harakeh and S. Waslander},
  title     = {{Joint 3D Proposal Generation and Object Detection from View Aggregation}},
  booktitle = iros,
  year      = 2017,
  url       = {http://arxiv.org/pdf/1712.02294v4},
  abstract  = {We present AVOD, an Aggregate View Object Detection network for autonomous driving scenarios. The proposed neural network architecture uses LIDAR point clouds and RGB images to generate features that are shared by two subnetworks: a region proposal network (RPN) and a second stage detector network. The proposed RPN uses a novel architecture capable of performing multimodal feature fusion on high resolution feature maps to generate reliable 3D object proposals for multiple object classes in road scenes. Using these proposals, the second stage detection network performs accurate oriented 3D bounding box regression and category classification to predict the extents, orientation, and classification of objects in 3D space. Our proposed architecture is shown to produce state of the art results on the KITTI 3D object detection benchmark while running in real time with a low memory footprint, making it a suitable candidate for deployment on autonomous vehicles. Code is at: https://github.com/kujason/avod}
}



@inproceedings{neven2019cvpr,
  author    = {D. Neven and B. De Brabandere and M. Proesmans and L. Van Gool},
  title     = {{Instance Segmentation by Jointly Optimizing Spatial Embeddings and Clustering Bandwidth}},
  booktitle = cvpr,
  year      = 2019,
  abstract  = {Current state-of-the-art instance segmentation methods are not suited for real-time applications like autonomous driving, which require fast execution times at high accuracy. Although the currently dominant proposal-based methods have high accuracy, they are slow and generate masks at a fixed and low resolution. Proposal-free methods, by contrast, can generate masks at high resolution and are often faster, but fail to reach the same accuracy as the proposal-based methods. In this work we propose a new clustering loss function for proposal-free instance segmentation. The loss function pulls the spatial embeddings of pixels belonging to the same instance together and jointly learns an instance-specific clustering bandwidth, maximizing the intersection-over-union of the resulting instance mask. When combined with a fast architecture, the network can perform instance segmentation in real-time while maintaining a high accuracy. We evaluate our method on the challenging Cityscapes benchmark and achieve top results (5\% improvement over Mask R-CNN) at more than 10 fps on 2MP images.},
  url       = {proceedings: neven2019cvpr.pdf}
}

@inproceedings{chen2017cvpr-modn,
  author    = {X. Chen and H. Ma and J. Wan and B. Li and T. Xia},
  title     = {{Multi-View 3D Object Detection Network for Autonomous Driving}},
  booktitle = cvprold,
  year      = 2017,
  abstract  = {This paper aims at high-accuracy 3D object detection in autonomous driving scenario. We propose Multi-View 3D networks (MV3D), a sensory-fusion framework that takes both LIDAR point cloud and RGB images as input and predicts oriented 3D bounding boxes. We encode the sparse 3D point cloud with a compact multi-view representation. The network is composed of two subnetworks: one for 3D object proposal generation and another for multi-view feature fusion. The proposal network generates 3D candidate boxes efficiently from the bird's eye view representation of 3D point cloud. We design a deep fusion scheme to combine region-wise features from multiple views and enable interactions between intermediate layers of different paths. Experiments on the challenging KITTI benchmark show that our approach outperforms the state-of-the-art by around 25\% and 30\% AP on the tasks of 3D localization and 3D detection. In addition, for 2D detection, our approach obtains 14.9\% higher AP than the state-of-the-art on the hard data among the LIDAR-based methods.},
  url       = {proceedings: chen2017cvpr-modn.pdf}
}

@article{cheng2019arxiv,
  author  = {B. Cheng and M.D. Collins and Y. Zhu and T. Liu and T.S. Huang and H. Adam and L. Chen},
  title   = {{Panoptic-DeepLab}},
  journal = arxiv,
  year    = 2019,
  volume  = {arXiv:1910.04751},
  url     = {http://arxiv.org/pdf/1910.04751v3}
}


@inproceedings{cheng2019iccvw,
  author    = {B. Cheng and M.D. Collins and Y. Zhu and T. Liu and T.S. Huang and H. Adam and L. Chen},
  title     = {{Panoptic-DeepLab}},
  booktitle = {Proc. of the ICCV Workshop: Joint COCO and Mapillary Recognition Challlenge Workshop},
  year      = 2019,
  url       = {http://arxiv.org/pdf/1910.04751v3}
}



@inproceedings{qi2019iccv-dhvf,
  author    = {C. R. Qi and O. Litany and K. He and L. J. Guibas},
  title     = {{Deep Hough Voting for 3D Object Detection in Point Clouds}},
  booktitle = iccv,
  year      = 2019,
  abstract  = {Current 3D object detection methods are heavily influenced by 2D detectors. In order to leverage architectures in 2D detectors, they often convert 3D point clouds to regular grids (i.e., to voxel grids or to bird's eye view images), or rely on detection in 2D images to propose 3D boxes. Few works have attempted to directly detect objects in point clouds. In this work, we return to first principles to construct a 3D detection pipeline for point cloud data and as generic as possible. However, due to the sparse nature of the data -- samples from 2D manifolds in 3D space -- we face a major challenge when directly predicting bounding box parameters from scene points: a 3D object centroid can be far from any surface point thus hard to regress accurately in one step. To address the challenge, we propose VoteNet, an end-to-end 3D object detection network based on a synergy of deep point set networks and Hough voting. Our model achieves state-of-the-art 3D detection on two large datasets of real 3D scans, ScanNet and SUN RGB-D with a simple design, compact model size and high efficiency. Remarkably, VoteNet outperforms previous methods by using purely geometric information without relying on color images.},
  url       = {proceedings: qi2019iccv-dhvf.pdf}
}


@inproceedings{law2018eccv,
  author    = {H. Law and J. Deng},
  title     = {{CornerNet: Detecting Objects as Paired Keypoints}},
  booktitle = eccv,
  year      = 2018,
  abstract  = {We propose CornerNet, a new approach to object detection where we detect an object bounding box as a pair of keypoints, the top-left corner and the bottom-right corner, using a single convolution neural network. By detecting objects as paired keypoints, we eliminate the need for designing a set of anchor boxes commonly used in prior single-stage detectors. In addition to our novel formulation, we introduce corner pooling, a new type of pooling layer that helps the network better localize the corners. Experiments show that CornerNet achieves a 42.1\% AP on MS COCO, outperforming all existing one-stage detectors.},
  url       = {proceedings: law2018eccv.pdf}
}

@inproceedings{wang2018cvpr-ssgp,
  author    = {W. Wang and R. Yu and Q. Huang and U. Neumann},
  title     = {{SGPN: Similarity Group Proposal Network for 3D Point Cloud Instance Segmentation}},
  booktitle = cvprold,
  year      = 2018,
  abstract  = {We introduce Similarity Group Proposal Network (SGPN), a simple and intuitive deep learning framework for 3D object instance segmentation on point clouds. SGPN uses a single network to  predict point grouping proposals and a corresponding semantic class for each proposal, from which we can directly extract instance segmentation results. Important to the effectiveness of SGPN is its novel representation of 3D instance segmentation results in the form of a similarity matrix that indicates the similarity between each pair of points in embedded feature space, thus producing an accurate grouping proposal for each point.  To the best of our knowledge, SGPN is the first framework to learn 3D instance-aware semantic segmentation on point clouds. Experimental results on various 3D scenes show the effectiveness of our method on 3D instance segmentation, and we also evaluate the capability of SGPN to improve 3D object detection and semantic segmentation results. We also demonstrate its flexibility by seamlessly incorporating 2D CNN features into the framework to boost performance.},
  url       = {http://openaccess.thecvf.com/content_cvpr_2018/html/../../content_cvpr_2018/papers/Wang_SGPN_Similarity_Group_CVPR_2018_paper.pdf}
}

@inproceedings{chen2017arxiv-gradnorm,
  author    = {Z. Chen and V. Badrinarayanan and C.Y. Lee and A. Rabinovich},
  title     = {{GradNorm: Gradient Normalization for Adaptive Loss Balancing in Deep Multitask Networks}},
  booktitle = icml,
  year      = 2018,
  url       = {https://arxiv.org/pdf/1711.02257.pdf},
  abstract  = {Deep multitask networks, in which one neural network produces multiple predictive outputs, can offer better speed and performance than their single-task counterparts but are challenging to train properly. We present a gradient normalization (GradNorm) algorithm that automatically balances training in deep multitask models by dynamically tuning gradient magnitudes. We show that for various network architectures, for both regression and classification tasks, and on both synthetic and real datasets, GradNorm improves accuracy and reduces overfitting across multiple tasks when compared to single-task networks, static baselines, and other adaptive multitask loss balancing techniques. GradNorm also matches or surpasses the performance of exhaustive grid search methods, despite only involving a single asymmetry hyperparameter $\alpha$. Thus, what was once a tedious search process that incurred exponentially more compute for each task added can now be accomplished within a few training runs, irrespective of the number of tasks. Ultimately, we will demonstrate that gradient manipulation affords us great control over the training dynamics of multitask networks and may be one of the keys to unlocking the potential of multitask learning.}
}

@inproceedings{behley2013iros,
  author    = {J. Behley and V. Steinhage and A.B. Cremers},
  title     = {{Laser-based Segment Classification Using a Mixture of Bag-of-Words}},
  booktitle = iros,
  year      = 2013,
  url       = {http://jbehley.github.io/papers/behley2013iros.pdf}
}

@article{bosse2016fntr,
  author  = {M. Bosse and G. Agamennoni and I. Gilitschenski},
  year    = {2016},
  volume  = {4},
  journal = fntr,
  title   = {{Robust Estimation and Applications in Robotics}},
  number  = {4},
  pages   = {225--269},
  url     = {https://www.gilitschenski.org/igor/publications/201612-fntrob-robust_statistics/fntrob16-robust_statistics.pdf}
}

@inproceedings{chebrolu2020icra,
  title     = {{Spatio-Temporal Non-Rigid Registration of 3D Point Clouds of Plants}},
  author    = {N. Chebrolu and T. L\"abe and C. Stachniss},
  booktitle = icra,
  year      = {2020},
  url       = {http://www.ipb.uni-bonn.de/wp-content/papercite-data/pdf/chebrolu2020icra.pdf}
}

@inproceedings{chen2019iros,
  author    = {X. Chen and A. Milioto and E. Palazzolo and P. GiguÃ¨re and J. Behley and C. Stachniss},
  title     = {{SuMa++: Efficient LiDAR-based Semantic SLAM}},
  booktitle = iros,
  year      = 2019,
  url       = {http://www.ipb.uni-bonn.de/wp-content/papercite-data/pdf/chen2019iros.pdf},
  codeurl   = {https://github.com/PRBonn/semantic_suma/},
  videourl  = {https://youtu.be/uo3ZuLuFAzk}
}

@inproceedings{chen2020rss,
  author    = {X. Chen and T. L\"abe and A. Milioto and T. R\"ohling and O. Vysotska and A. Haag and J. Behley and C. Stachniss},
  title     = {{OverlapNet: Loop Closing for LiDAR-based SLAM}},
  booktitle = rss,
  year      = {2020},
  url       = {https://www.ipb.uni-bonn.de/wp-content/papercite-data/pdf/chen2020rss.pdf},
  codeurl   = {https://github.com/PRBonn/OverlapNet/},
  videourl  = {https://youtu.be/YTfliBco6aw}
}

@inproceedings{steder2010irosws,
  author    = {B. Steder and R.B. Rusu and K. Konolige and W. Burgard},
  booktitle = {Workshop on Defining and Solving Realistic Perception Problems
               in Personal Robotics at the IEEE/RSJ Int. Conf. on
               Intelligent Robots and Systems (IROS)},
  year      = {2010},
  title     = {{NARF}: {3D} Range Image Features for Object Recognition},
  url       = {http://ais.informatik.uni-freiburg.de/publications/papers/steder10irosws.pdf}
}


@article{guo2019ral,
  title   = {{Local descriptor for robust place recognition using LiDAR intensity}},
  author  = {J. Guo and P.V.K. Borges and C. Park and A. Gawel},
  journal = ral,
  volume  = {4},
  number  = {2},
  pages   = {1470--1477},
  year    = {2019},
  url     = {https://arxiv.org/pdf/1811.12646.pdf}
}

@inproceedings{barsan2018corl,
  title     = {{Learning to Localize Using a LiDAR Intensity Map}},
  author    = {I.A. Barsan, I. Andrei and S. Wang and A. Pokrovsky and R. Urtasun},
  booktitle = corl,
  year      = {2018},
  url       = {http://proceedings.mlr.press/v87/barsan18a/barsan18a.pdf}
}

@inproceedings{schaupp2019iros,
  title     = {{OREOS: Oriented Recognition of 3D Point Clouds in Outdoor Scenarios}},
  author    = {L. Schaupp and M. B{\"u}rki and R. Dub{\'e} and R. Siegwart and C. Cadena},
  booktitle = iros,
  year      = {2019},
  url       = {proceedings: schaupp2019iros.pdf}
}

@article{yin2019tits,
  title   = {{3D LiDAR-Based Global Localization Using Siamese Neural Network}},
  author  = {H. Yin and Y. Wang and X. Ding and L. Tang and S. Huang and R. Xiong},
  journal = tits,
  volume  = {21},
  number  = {4},
  pages   = {1380--1392},
  year    = {2019}
}

@inproceedings{lu2019cvpr,
  author    = {W. Lu and Y. Zhou and G. Wan and S. Hou and S. Song},
  title     = {{L3-Net: Towards Learning Based LiDAR Localization for Autonomous Driving}},
  booktitle = cvpr,
  year      = 2019,
  abstract  = {We present L3-Net - a novel learning-based LiDAR localization system that achieves centimeter-level localization accuracy, comparable to prior state-of-the-art systems with hand-crafted pipelines. Rather than relying on these hand-crafted modules, we innovatively implement the use of various deep neural network structures to establish a learning-based approach. L3-Net learns local descriptors specifically optimized for matching in different real-world driving scenarios. 3D convolutions over a cost volume built in the solution space significantly boosts the localization accuracy. RNNs are demonstrated to be effective in modeling the vehicle's dynamics, yielding better temporal smoothness and accuracy. We comprehensively validate the effectiveness of our approach using freshly collected datasets. Multiple trials of repetitive data collection over the same road and areas make our dataset ideal for testing localization systems. The  SunnyvaleBigLoop sequences, with a year's time interval between the collected mapping and testing data, made it quite challenging, but the low localization error of our method in these datasets demonstrates its maturity for real industrial implementation.},
  url       = {proceedings: lu2019cvpr-ltlb.pdf}
}

@article{pandey2011ijrr,
  title   = {{Ford campus vision and lidar data set}},
  author  = {G. Pandey and J.R. McBride and R.M. Eustice},
  journal = ijrr,
  volume  = {30},
  number  = {13},
  pages   = {1543--1552},
  year    = {2011},
  url     = {http://robots.engin.umich.edu/publications/gpandey-2010b.pdf}
}

@inproceedings{angelina2018cvpr,
  title     = {{PointNetVLAD: Deep point cloud based retrieval for large-scale place recognition}},
  author    = {A.M. Uy and  G.H. Lee},
  booktitle = cvprold,
  year      = {2018},
  url       = {proceedings: angelina2018cvpr.pdf}
}

@article{kim2019ral,
  title   = {{1-day learning, 1-year localization: Long-term LiDAR localization using scan context image}},
  author  = {G. Kim and B. Park and A. Kim},
  journal = ral,
  volume  = {4},
  number  = {2},
  pages   = {1948--1955},
  year    = {2019},
  url     = {https://irap.kaist.ac.kr/publications/gkim-2019-ral.pdf}
}

@inproceedings{nagashima2007iciar,
  title     = {{A high-accuracy rotation estimation algorithm based on 1D phase-only correlation}},
  author    = {S. Nagashima and K. Ito and T. Aoki and H. Ishii and K. Kobayashi},
  booktitle = {Proc.~of the Intl.~Conf. on Image Analysis and Recognition},
  year      = {2007},
  url       = {http://www.aoki.ecei.tohoku.ac.jp/~ito/2007-08-ICIAR.pdf}
}

@inproceedings{bromley1994nips,
  author    = {J. Bromley and I. Guyon and Y. LeCun and E. S{\"a}ckinger and R. Shah},
  title     = {{Signature Verification using a ``Siamese'' Time Delayed Neural Network}},
  booktitle = nips,
  year      = 1994,
  url       = {https://papers.nips.cc/paper/769-signature-verification-using-a-siamese-time-delay-neural-network.pdf}
}

@inproceedings{zaganidis2019iros,
  author    = {A. Zaganidis and A. Zerntev and T. Duckett and G. Cielniak},
  title     = {{Semantically Assisted Loop Closure in SLAM Using NDT Histograms}},
  booktitle = iros,
  year      = 2019,
  keywords  = {Localization, Mapping, SLAM},
  abstract  = { Precise knowledge of pose is of great importance for reliable operation of mobile robots in outdoor environments.
               Simultaneous localization and mapping (SLAM) is the online construction of a map during exploration of an environment. One of
               the components of SLAM is loop closure detection, identifying that the same location has been visited and is present on the
               existing map, and localizing against it. We have shown in previous work that using semantics from a deep segmentation network
               in conjunction with the Normal Distributions Transform (NDT) point cloud registration improves the robustness, speed and accuracy
               of lidar odometry. In this work we extend the method for loop closure detection, using the labels already available from local
               registration into NDT Histograms, and we present a SLAM pipeline based on Semantic assisted NDT and PointNet++. We experimentally
               demonstrate on sequences from the KITTI benchmark that the map descriptor we propose outperforms NDT Histograms without
               semantics, and we validate its use on a SLAM task.},
  url       = {proceedings: zaganidis2019iros.pdf}
}

@inproceedings{sun2020icra,
  title     = {{Localising Faster: Efficient and precise lidar-based robot localisation in large-scale environments}},
  author    = {L. Sun and D. Adolfsson and M. Magnusson and H. Andreasson and I. Posner and T. Duckett},
  booktitle = icra,
  year      = {2020},
  url       = {https://arxiv.org/pdf/2003.01875.pdf}
}

@inproceedings{censi2009icra,
  title     = {{HSM3D: feature-less global 6DOF scan-matching in the Hough/Radon domain}},
  author    = {A. Censi and S. Carpin},
  booktitle = icra,
  year      = {2009},
  url       = {https://censi.science/pub/research/2009-icra-hsm3d.pdf}
}

@inproceedings{zhou2016eccv,
  title     = {{Fast Global Registration}},
  author    = {Q. Zhou and J. Park and V. Koltun},
  booktitle = eccv,
  year      = {2016},
  url       = {http://vladlen.info/papers/fast-global-registration.pdf}
}

@article{teasdale1998jpa,
  title   = {Influence of herbicide application timing on corn production in a hairy vetch cover crop},
  author  = {Teasdale, John R and Shirley, Daniel W},
  journal = {Journal of Production Agriculture},
  volume  = {11},
  number  = {1},
  pages   = {121--125},
  year    = {1998},
  url     = {https://link.springer.com/content/pdf/10.1007/s13593-022-00815-2.pdf?pdf=button}
}

@article{yang2020tro,
  title   = {{TEASER: Fast and Certifiable Point Cloud Registration}},
  author  = {Yang, Heng and Shi, Jingnan and Carlone, Luca},
  journal = tro,
  volume  = {37},
  number  = {2},
  pages   = {314--333},
  year    = {2020},
  codeurl = {https://github.com/MIT-SPARK/TEASER-plusplus},
  url     = {https://arxiv.org/pdf/2001.07715.pdf}
}

@article{bromley1994ijprai,
  author   = {J. Bromley and I. Guyon and Y. LeCun and E. S{\"a}ckinger and R. Shah},
  title    = {{Signature Verification using a ``Siamese'' Time Delayed Neural Network}},
  journal  = pami,
  volume   = {7},
  number   = {4},
  pages    = {669--688},
  year     = {1993},
  abstract = { This paper describes the development of an algorithm for verification of signatures written on a touch-sensitive pad. The signature verification algorithm is based on an artificial neural network. The novel network presented here, called a âSiameseâ time delay neural network, consists of two identical networks joined at their output. During training the network learns to measure the similarity between pairs of signatures. When used for verification, only one half of the Siamese network is evaluated. The output of this half network is the feature vector for the input signature. Verification consists of comparing this feature vector with a stored feature vector for the signer. Signatures closer than a chosen threshold to this stored representation are accepted, all other signatures are rejected as forgeries. System performance is illustrated with experiments performed in the laboratory. }
}

@article{krizhevsky2012cacm,
  author  = {A. Krizhevsky and I. Sutskever and G.E. Hinton},
  title   = {{ImageNet Classification with Deep Convolutional Neural Networks}},
  year    = {2017},
  volume  = {60},
  number  = {6},
  url     = {https://dl.acm.org/doi/pdf/10.1145/3065386},
  journal = cacm,
  pages   = {84--90}
}

@article{paulus2013bmcbio,
  title    = {Surface feature based classification of plant organs from 3D laserscanned point clouds for plant phenotyping},
  author   = {S. Paulus and J. Dupuisand A. Mahlein and H. Kuhlmann},
  journal  = bmcbio,
  volume   = {14},
  number   = {1},
  pages    = {238},
  year     = {2013},
  abstract = {Laserscanning recently has become a powerful and common method for plant parameterization andplant growth observation on nearly every scale range. However, 3D measurements with high accuracy, spatialresolution and speed result in a multitude of points that require processing and analysis. The primary objective of thisresearch has been to establish a reliable and fast technique for high throughput phenotyping using differentiation,segmentation and classification of single plants by a fully automated system. In this report, we introduce a techniquefor automated classification of point clouds of plants and present the applicability for plant parameterization.}
}

@article{shi2019be,
  title    = {Plant-part segmentation using deep learning and multi-view vision},
  author   = {W. Shi and R. van de Zedde and H. Jiang and G. Kootstra},
  journal  = biosyseng,
  volume   = {187},
  pages    = {81--95},
  year     = {2019},
  abstract = {To accelerate the understanding of the relationship between genotype and phenotype,plant scientists and plant breeders are looking for more advanced phenotyping systemsthat provide more detailed phenotypic information about plants. Most current systemsprovide information on the whole-plant level and not on the level of specific plant partssuch as leaves, nodes and stems. Computer vision provides possibilities to extract infor-mation from plant parts from images. However, the segmentation of plant parts is achallenging problem, due to the inherent variation in appearance and shape of naturalobjects. In this paper, deep-learning methods are proposed to deal with this variation.Moreover, a multi-view approach is taken that allows the integration of information fromthe two-dimensional (2D) images into a three-dimensional (3D) point-cloud model of theplant. Specifically, a fully convolutional network (FCN) and a masked R-CNN (region-basedconvolutional neural network) were used for semantic and instance segmentation on the2D images. The different viewpoints were then combined to segment the 3D point cloud.The performance of the 2D and multi-view approaches was evaluated on tomato seedlingplants. Our results show that the integration of information in 3D outperforms the 2Dapproach, because errors in 2D are not persistent for the different viewpoints and cantherefore be overcome in 3D}
}

@inproceedings{zermas2018iros,
  title     = {Extracting Phenotypic Characteristics of Corn Crops Through the Use of Reconstructed 3D Models},
  author    = {D. Zermas and V. Morellas and D. Mulla and N. Papanikolopoulos},
  booktitle = iros,
  year      = {2018},
  url       = {proceedings: zermas2018iros.pdf},
  abstract  = {Financial and social elements of modern societiesare  closely  connected  to  the  cultivation  of  corn.  Due  to  itsmassive  production,  deficiencies  during  the  cultivation  processdirectly translate to major financial losses. Since proper surveil-lance  in  a  large  scale  is  still  very  challenging,  the  companiesthat specialize in optimizing crop yield are trying to address theproblem at its root by developing hybrid plants able to resist theharsh conditions of the field. The selection of the best hybrid isnot easy and every year hundreds of test plants with differentphenotypic characteristics are planted while their performanceis quantified by inconsistent and rough measurements gatheredby humans. We propose a pipeline that takes advantage of thestructure from motion technology to create a detailed 3D pointcloud  of  a  few  plants  and  segment  it  into  the  basic  elementsof  the  scene;  the  ground,  the  plants,  the  plant  stems,  and  theplant leaves. The focus is on the segmentation process throughwhich  several  phenotypic  characteristics  of  individual  plantscan  be  extracted.  As  an  example,  we  show  the  results  for  theplant counting and plant height estimation processes where weachieve  an  accuracy  of  88.1\%  and  89.2\%.}
}

@inproceedings{zermas2017iros,
  title     = {Estimating the leaf area index of crops through the evaluation of 3D models},
  author    = {D. Zermas and V. Morellas and D. Mulla and N. Papanikolopoulos},
  booktitle = iros,
  year      = {2017},
  url       = {proceedings: zermas2017iros.pdf},
  abstract  = {inancial and social elements of modern societiesare  closely  connected  to  the  cultivation  of  corn.  Due  to  themassive  production  of  corn,  deficiencies  during  the  cultivationprocess  directly  translate  to  major  financial  losses.  The  earlydetection  and  treatment  of  crops  deficiencies  is  thus  a  taskof  great  significance.  Towards  an  automated  health  conditionassessment, this study introduces a scheme for the computationof plant health indices. Based on the 3D reconstruction of smallbatches  of  corn  plants,  an  alternative  to  existing  cumbersomeLeaf  Area  Index  (LAI)  estimation  methodologies  is  presented.The use of 3D models provides an elevated information content,when compared to planar methods, mainly due to the reducedloss  attributed  to  leaf  occlusions.  High  resolution  images  ofcorn  stalks  are  collected  and  used  to  obtain  3D  models  ofplants  of  interest.  Based  on  the  extracted  3D  point  clouds,  anaccurate calculation of the Leaf Area Index (LAI) of the plantsis  performed.  An  experimental  validation  (using  artificiallymade corn plants used as ground truth of the LAI estimation),emulating  real  world  scenarios,  supports  the  efficacy  of  theproposed methodology. The conclusions of this work, suggest afully  automated  scheme  for  information  gathering  in  modernfarms capable of replacing current labor intensive procedures,thus greatly impacting the timely detection of crop deficiencies.}
}

@inproceedings{sodhi2017iros,
  title     = {In-field segmentation and identification of plant structures using 3D imaging},
  author    = {P. Sodhi and S. Vijayarangan and D. Wettergreen},
  booktitle = iros,
  year      = {2017},
  url       = {proceedings: sodhi2017iros.pdf},
  abstract  = {Automatically  correlating  plant  observable  char-acteristics to their underlying genetics will streamline selectionmethods  in  plant  breeding.  Measurement  of  plant  observablecharacteristics  is  called  phenotyping,  and  knowing  plant  phe-notypes  accurately  and  throughout  a  plantâs  growth  is  centralto  making  breeding  decisions.  In-field  plant  phenotyping  inan  automated  and  noninvasive  manner  is  hence  crucial  toaccelerating  plant  breeding  methods.  However,  most  of  theexisting  methods  on  plant  phenotyping  using  visual  imagingare  confined  to  controlled  greenhouse  environments.This  paper  presents  an  automated  method  of  mapping  2Dimages collected in an outdoor sorghum field to segmented 3Dplant  units  that  are  of  interest  for  phenotyping.  This  methodleverages   multiple   horizontal   and   vertical   viewpoints   whilecapturing 2D images from a robotic platform so as to generatein-field  3D  reconstructions  of  the  sorghum  plant.  We  developand  quantitatively  evaluate  segmentation  methods  on  these3D  reconstructions  and  also  compare  against  reconstructionsobtained from a controlled greenhouse environment. We presentanalysis that contrasts the role of purely local geometric featuresand  the  effect  of  addition  of  global  context  in  both  datasets.This  work  furthers  capabilities  of  in-field  phenotyping  whichpaves the way forward for plant biologists to study the coupledeffect  of  genetics  and  environment  on  improving  crop  yields}
}

@inproceedings{su2015iccv,
  title     = {Multi-view convolutional neural networks for 3d shape recognition},
  author    = {H. Su and S. Maji and E. Kalogerakis and E. Learned-Miller},
  booktitle = iccvold,
  year      = {2015},
  url       = {proceedings: su2015iccv.pdf},
  abstract  = {A longstanding question in computer vision concerns therepresentation  of  3D  shapes  for  recognition:  should  3Dshapes be represented with descriptors operating on theirnative 3D formats, such as voxel grid or polygon mesh, orcan they be effectively represented with view-based descrip-tors?  We address this question in the context of learningto recognize 3D shapes from a collection of their renderedviews on 2D images.  We first present a standard CNN ar-chitecture trained to recognize the shapesâ rendered viewsindependently  of  each  other,  and  show  that  a  3D  shapecan be recognized even from a single view at an accuracyfar higher than using state-of-the-art 3D shape descriptors.Recognition rates further increase when multiple views ofthe shapes are provided.  In addition, we present a novelCNN architecture that combines information from multipleviews of a 3D shape into a single and compact shape de-scriptor offering even better recognition performance. Thesame architecture can be applied to accurately recognizehuman hand-drawn sketches of shapes.  We conclude thata collection of 2D views can be highly informative for 3Dshape recognition and is amenable to emerging CNN archi-tectures and their derivatives.}
}

@article{suykens1999npl,
  title    = {Least squares support vector machine classifiers},
  author   = {J. AK. Suykens and J. Vandewalle},
  journal  = npl,
  volume   = {9},
  number   = {3},
  pages    = {293--300},
  year     = {1999},
  url      = {https://link.springer.com/content/pdf/10.1023/A:1018628609742.pdf},
  abstract = {In this letter we discuss a least squares version for support vector machine (SVM) classi-fiers. Due to equality type constraints in the formulation, the solution follows from solving a set oflinear equations, instead of quadratic programming for classical SVMâs. The approach is illustratedon a two-spiral benchmark classification problem}
}

@inproceedings{kohonen1990pieee,
  title     = {The self-organizing map},
  author    = {T. Kohonen},
  booktitle = {Proc. of the IEEE},
  year      = {1990},
  url       = {https://sci2s.ugr.es/keel/pdf/algorithm/articulo/1990-Kohonen-PIEEE.pdf},
  abstract  = {Among the architectures and algorithms suggested for artificial neural networks, the Self-Organizing Map has the special property of effectively creating spatially organized "internal representation" of various features of input signals and their abstractions. One novel result is that the self-organization process can also discover semantic relationships in sentence. In this respect the resulting maps very closely resemble the topographically organized maps found in the cortices of the more developed animal brains. After supervised fine tuning of its weight vectors, the Self-Organizing Map has been particularly succesfull in various pattern recognition tasks involving very noisy signals. In particular, these maps have been used in practical speech recognition, and work is in progress on their application to robotics, process control, teleccommunications, etc. This paper contains a survey of several basic facts and results.}
}

@article{huang2013acm,
  title   = {L1-medial skeleton of point cloud.},
  author  = {H. Huang and S. Wu and D. Cohen-Or and M. Gong and H. Zhang and G. Li and B. Chen},
  journal = acmgraphics,
  volume  = {32},
  number  = {4},
  pages   = {65--1},
  year    = {2013},
  url     = {http://www.math.tau.ac.il/~dcor/articles/2013/L1-Medial-Skeleton-of-Point.pdf}
}

@article{wu2019fps,
  title    = {An accurate skeleton extraction approach from 3D point clouds of maize plants},
  author   = {S. Wu and W. Wen  and B. Xiao and X. Guo and J. Du and C. Wang and Y. Wang},
  journal  = fps,
  volume   = {10},
  year     = {2019},
  abstract = {ccurate and high-throughput determination of plant morphological traits is essentialfor  phenotyping  studies.  Nowadays,  there  are  many  approaches  to  acquire  high-quality three-dimensional (3D) point clouds of plants. However, it is difficult to estimatephenotyping parameters accurately of the whole growth stages of maize plants usingthese  3D  point  clouds.  In  this  paper,  an  accurate  skeleton  extraction  approach  wasproposed to bridge the gap between 3D point cloud and phenotyping traits estimationof  maize  plants.  The  algorithm  first  uses  point  cloud  clustering  and  color  differencedenoising to reduce the noise of the input point clouds. Next, the Laplacian contractionalgorithm is applied to shrink the points. Then the key points representing the skeleton ofthe plant are selected through adaptive sampling, and neighboring points are connectedto form a plant skeleton composed of semantic organs. Finally, deviation skeleton pointsto  the  input  point  cloud  are  calibrated  by  building  a  step  forward  local  coordinatealong the tangent direction of the original points. The proposed approach successfullygenerates  accurately  extracted  skeleton  from  3D  point  cloud  and  helps  to  estimatephenotyping parameters with high precision of maize plants. Experimental verification ofthe skeleton extraction process, tested using three cultivars and different growth stagesmaize, demonstrates that the extracted matches the input point cloud well. Comparedwith 3D digitizing data-derived morphological parameters, the NRMSE of leaf length,leaf inclination angle, leaf top length, leaf azimuthal angle, leaf growth height, and plantheight, estimated using the extracted plant skeleton, are 5.27, 8.37, 5.12, 4.42, 1.53,and 0.83\%, respectively, which could meet the needs of phenotyping analysis. The timerequired to process a single maize plant is below 100 s. The proposed approach mayplay an important role in further maize research and applications, such as genotype-to-phenotype study, geometric reconstruction, functional structural maize modeling, anddynamic growth animation.}
}

@inproceedings{ester1996kdd,
  title     = {A density-based algorithm for discovering clusters in large spatial databases with noise.},
  author    = {M. Ester and H. Kriegel and J. Sander and X.Xu},
  booktitle = {Proc. of the Conf. on Knowledge Discovery and Data Mining (KDD)},
  year      = {1996},
  url       = {https://www.aaai.org/Papers/KDD/1996/KDD96-037.pdf},
  abstract  = {Clustering algorithms are attractive for the task of class iden-tification in spatial databases. However, the application tolarge spatial databases rises the following requirements forclustering algorithms: minimal requirements of domainknowledge to determine the input parameters, discovery ofclusters with arbitrary shape and good efficiency on large da-tabases. The well-known clustering algorithms offer no solu-tion to the combination of these requirements. In this paper,we present the new clustering algorithm DBSCAN relying ona density-based notion of clusters which is designed to dis-cover clusters of arbitrary shape. DBSCAN requires only oneinput parameter and supports the user in determining an ap-propriate value for it. We performed an experimental evalua-tion of the effectiveness and efficiency of DBSCAN usingsynthetic data and real data of the SEQUOIA 2000 bench-mark. The results of our experiments demonstrate that (1)DBSCAN is significantly more effective in discovering clus-ters of arbitrary shape than the well-known algorithm CLAR-ANS, and that (2) DBSCAN outperforms CLARANS by factor of more than 100 in terms of efficiency}
}

@inproceedings{rusu2009icra,
  title     = {Fast point feature histograms (FPFH) for 3D registration},
  author    = {R.B. Rusu and N. Blodow and M. Beetz},
  booktitle = icra,
  year      = {2009},
  url       = {proceedings: rusu2009icra.pdf},
  abstract  = {In  our  recent  work  [1],  [2],  we  proposed  PointFeature Histograms (PFH) as robust multi-dimensional featureswhich describe the local geometry around a pointpfor 3D pointcloud  datasets.  In  this  paper,  we  modify  their  mathematicalexpressions and perform a rigorous analysis on their robustnessand complexity for the problem of 3D registration for overlap-ping  point  cloud  views.  More  concretely,  we  present  severaloptimizations  that  reduce  their  computation  times  drasticallyby  either  caching  previously  computed  values  or  by  revisingtheir theoretical formulations. The latter results in a new typeof local features, called Fast Point Feature Histograms (FPFH),which  retain  most  of  the  discriminative  power  of  the  PFH.Moreover, we propose an algorithm for the online computationof  FPFH  features  for  realtime  applications.  To  validate  ourresults we demonstrate their efficiency for 3D registration andpropose  a  new  sample  consensus  based  method  for  bringingtwo  datasets  into  the  convergence  basin  of  a  local  non-linearoptimizer:  SAC-IA  (SAmple  Consensus  Initial  Alignment)}
}

@article{kuhn1955nrlq,
  title    = {The Hungarian method for the assignment problem},
  author   = {H.W. Kuhn},
  journal  = nrlq,
  volume   = {2},
  number   = {1-2},
  pages    = {83--97},
  year     = {1955},
  url      = {https://web.eecs.umich.edu/~pettie/matching/Kuhn-hungarian-assignment.pdf},
  abstract = {ssuming that numerical scores are available for the perform- ance of each of n  persons on each of n  jobs, the "assignment  problem" is the quest for  an assignment of persons to jobs so that the sum of the n  scores so obtained is as large as possible. It is shown that ideas latent in the work of two Hungarian mathematicians  may be exploited  to yield a new method of solving this problem.}
}

@inproceedings{ahmadi2020icra,
  title     = {{Visual Servoing-based Navigation for Monitoring Row-Crop Fields}},
  author    = {A. Ahmadi and L. Nardi and N. Chebrolu and C. Stachniss},
  booktitle = icra,
  year      = {2020},
  url       = {proceedings: ahmadi2020icra.pdf},
  codeurl   = {https://github.com/PRBonn/visual-crop-row-navigation},
  videourl  = {https://youtu.be/0qg6n4sshHk},
  abstract  = {Autonomous navigation is a pre-requisite for fieldrobots  to  carry  out  precision  agriculture  tasks.  Typically,  arobot has to navigate through a whole crop field several timesduring  a  season  for  monitoring  the  plants,  for  applying  agro-chemicals,  or  for  performing  targeted  intervention  actions.  Inthis  paper,  we  propose  a  framework  tailored  for  navigation  inrow-crop  fields  by  exploiting  the  regular  crop-row  structurepresent in the fields. Our approach uses only the images fromon-board  cameras  without  the  need  for  performing  explicitlocalization  or  maintaining  a  map  of  the  field  and  thus  canoperate  without  expensive  RTK-GPS  solutions  often  used  inagriculture  automation  systems.  Our  navigation  approach  al-lows  the  robot  to  follow  the  crop-rows  accurately  and  handlesthe  switch  to  the  next  row  seamlessly  within  the  same  frame-work. We implemented our approach using C++ and ROS andthoroughly  tested  it  in  several  simulated  environments  withdifferent  shapes  and  sizes  of  field.  We  also  demonstrated  thesystem running at frame-rate on an actual robot operating ona  test  row-crop  field.  The  code  and  data  have  been  published.}
}

@article{fiorani2012cob,
  title    = {Imaging plants dynamics in heterogenic environments},
  author   = {F. Fiorani and U. Rascher and S. Jahnke and U. Schurr},
  journal  = cob,
  volume   = {23},
  number   = {2},
  pages    = {227--235},
  year     = {2012},
  abstract = {Noninvasive imaging sensors and computer vision approaches are key technologies to quantify plant structure, physiological status, and performance. Today, imaging sensors exploit a wide range of the electromagnetic spectrum, and they can be deployed to measure a growing number of traits, also in heterogenic environments. Recent advances include the possibility to acquire high-resolution spectra by imaging spectroscopy and classify signatures that might be informative of plant development, nutrition, health, and disease. Three-dimensional (3D) reconstruction of surfaces and volume is of particular interest, enabling functional and mechanistic analyses. While taking pictures is relatively easy, quantitative interpretation often remains challenging and requires integrating knowledge of sensor physics, image analysis, and complex traits characterizing plant phenotypes.}
}

@article{fiorani2013arpb,
  title    = {Future scenarios for plant phenotyping},
  author   = {F. Fiorani and U. Schurr},
  journal  = arpb,
  volume   = {64},
  pages    = {267--291},
  year     = {2013},
  abstract = {With increasing demand to support and accelerate progress in breeding for novel traits, the plant research community faces the need to accurately measure increasingly large numbers of plants and plant parameters. The goal is to provide quantitative analyses of plant structure and function relevant for traits that help plants better adapt to low-input agriculture and resource-limited environments. We provide an overview of the inherently multidisciplinary research in plant phenotyping, focusing on traits that will assist in selecting genotypes with increased resource use efficiency. We highlight opportunities and challenges for integrating noninvasive or minimally invasive technologies into screening protocols to characterize plant responses to environmental challenges for both controlled and field experimentation. Although technology evolves rapidly, parallel efforts are still required because large-scale phenotyping demands accurate reporting of at least a minimum set of information concerning experimental protocols, data management schemas, and integration with modeling. The journey toward systematic plant phenotyping has only just begun. }
}

@article{bosse2008ijrr,
  title   = {{Map matching and data association for large-scale two-dimensional laser scan-based slam}},
  author  = {M. Bosse and R. Zlot},
  journal = ijrr,
  volume  = {27},
  number  = {6},
  pages   = {667--691},
  year    = {2008},
  url     = {https://journals.sagepub.com/doi/pdf/10.1177/0278364908091366}
}

@inproceedings{kohlbrecher2011ssrr,
  title     = {{A flexible and scalable slam system with full 3d motion estimation}},
  author    = {S. Kohlbrecher and S. O. Von and J. Meyer and U. Klingauf},
  booktitle = ssrr,
  year      = {2011},
  url       = {https://www.sim.informatik.tu-darmstadt.de/publ/download/2011_SSRR_KohlbrecherMeyerStrykKlingauf_Flexible_SLAM_System.pdf}
}


@inproceedings{chen2017ssrr,
  title     = {{Robust SLAM system based on monocular vision and LiDAR for robotic urban search and rescue}},
  author    = {X. Chen and H. Zhang and H. Lu and J. Xiao and Q. Qiu and Y. Li},
  booktitle = ssrr,
  year      = {2017},
  url       = {https://ieeexplore.ieee.org/document/8088138}
}

@article{dube2019ijrr,
  title   = {{SegMap: Segment-based mapping and localization using data-driven descriptors}},
  author  = {R. Dub'e and A. Cramariuc and D. Dugas and H. Sommer and M. Dymczyk and J. Nieto and R. Siegwart and C. Cadena},
  journal = ijrr,
  volume  = {39},
  number  = {2--3},
  pages   = {339--355},
  year    = {2019},
  url     = {https://journals.sagepub.com/doi/10.1177/0278364919863090}
}

@inproceedings{wolcott2015icra,
  title     = {{Fast lidar localization using multiresolution gaussian mixture maps}},
  author    = {R.W. Wolcott and R.M. Eustice},
  booktitle = icra,
  year      = {2015},
  url       = {https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.710.5073&rep=rep1&type=pdf}
}

@inproceedings{ma2019iros,
  title     = {{Exploiting Sparse Semantic HD Maps for Self-Driving Vehicle Localization}},
  author    = {W. Ma and I. Tartavull and I. A. B{\^a}rsan and S. Wang and M. Bai and G. Mattyus and N. Homayounfar and S. K. Lakshmikanth and A. Pokrovsky and R. Urtasun},
  booktitle = iros,
  year      = {2019},
  url       = {https://arxiv.org/pdf/1908.03274.pdf}
}


@inproceedings{wei2019cvpr,
  title     = {{Learning to Localize Through Compressed Binary Maps}},
  author    = {X. Wei and I. A. B{\^a}rsan and S. Wang and J. Martinez and R. Urtasun},
  booktitle = cvpr,
  year      = {2019},
  url       = {https://openaccess.thecvf.com/content_CVPR_2019/papers/Wei_Learning_to_Localize_Through_Compressed_Binary_Maps_CVPR_2019_paper.pdf}
}


@inproceedings{wilbers2019icra,
  author    = {D. Wilbers and Ch. Merfels and C. Stachniss},
  title     = {{Localization with Sliding Window Factor Graphs on Third-Party Maps for Automated Driving}},
  booktitle = icra,
  year      = 2019,
  url       = {https://www.ipb.uni-bonn.de/wp-content/papercite-data/pdf/wilbers2019icra.pdf}
}


@inproceedings{zhang2018iros,
  title     = {{Robust lidar localization for autonomous driving in rain}},
  author    = {C. Zhang and M. H. Ang and D. Rus},
  booktitle = iros,
  year      = {2018},
  url       = {https://ieeexplore.ieee.org/document/8593703}
}


@inproceedings{schaefer2019ecmr,
  title     = {{Long-term urban vehicle localization using pole landmarks extracted from 3-D lidar scans}},
  author    = {A. Schaefer and D. B{\"u}scher and J. Vertens and L. Luft and W. Burgard},
  booktitle = ecmr,
  year      = {2019},
  url       = {http://ais.informatik.uni-freiburg.de/publications/papers/schaefer19ecmr.pdf}
}


@inproceedings{yan2019ecmr,
  author    = {F. Yan and O. Vysotska and C. Stachniss},
  title     = {{Global Localization on OpenStreetMap Using 4-bit Semantic Descriptors}},
  booktitle = ecmr,
  year      = {2019},
  url       = {https://www.ipb.uni-bonn.de/wp-content/papercite-data/pdf/yan2019ecmr.pdf}
}


@article{tinchev2019ral,
  title   = {{Learning to see the wood for the trees: Deep laser localization in urban and natural environments on a CPU}},
  author  = {G. Tinchev and A. Penate-Sanchez and M. Fallon},
  journal = ral,
  volume  = {4},
  number  = {2},
  pages   = {1327--1334},
  year    = {2019},
  url     = {https://arxiv.org/pdf/1902.10194.pdf}
}

@article{thrun2001ai,
  title   = {{Robust Monte Carlo Localization for Mobile Robots}},
  author  = {S. Thrun and D. Fox and W. Burgard and F. Dellaert},
  journal = ai,
  volume  = {128},
  number  = {1-2},
  year    = {2001},
  url     = {https://www.sciencedirect.com/science/article/pii/S0004370201000698}
}

@inproceedings{fox1999aaai,
  title     = {{Monte Carlo Localization: Efficient Position Estimation for Mobile Robots}},
  author    = {D. Fox and W. Burgard and F. Dellaert and S. Thrun},
  booktitle = aaaiold,
  year      = {1999},
  url       = {http://robots.stanford.edu/papers/fox.aaai99.pdf}
}


@inproceedings{chong2013icra,
  title     = {{Synthetic 2D Lidar for Precise Vehicle Localization in 3D Urban Environment}},
  author    = {Z.J. Chong and B. Qin and T. Bandyopadhyay and M. Ang and E. Frazzoli and D. Rus},
  booktitle = icra,
  year      = {2013},
  url       = {http://docshare04.docshare.tips/files/28872/288728033.pdf}
}


@inproceedings{schiotka2017iros,
  title     = {{Robot Localization with Sparse Scan-based Maps}},
  author    = {A. Schiotka and B. Suger and W. Burgard},
  booktitle = iros,
  year      = {2017},
  url       = {http://ais.informatik.uni-freiburg.de/publications/papers/schiotka17iros.pdf}
}

@article{cox1991tra,
  title   = {{Blanche-an experiment in guidance and navigation of an autonomous robot vehicle}},
  author  = {Cox, I.J},
  journal = tra,
  volume  = {7},
  number  = {2},
  pages   = {193--204},
  year    = {1991},
  url     = {https://ieeexplore.ieee.org/document/75902}
}


@incollection{fox2001particle,
  title     = {{Particle filters for mobile robot localization}},
  author    = {Fox, D. and Thrun, S. and Burgard, W. and Dellaert, F.},
  booktitle = {Sequential Monte Carlo methods in practice},
  pages     = {401--428},
  year      = {2001},
  url       = {http://users.umiacs.umd.edu/~fer/cmsc828/classes/fox.mcmc-book.pdf}
}


@article{smith1986ijrr,
  title   = {{On the representation and estimation of spatial uncertainty}},
  author  = {R.C. Smith and P. Cheeseman},
  journal = ijrr,
  volume  = {5},
  number  = {4},
  pages   = {56--68},
  year    = {1986},
  url     = {https://journals.sagepub.com/doi/pdf/10.1177/027836498600500404}
}

@inproceedings{milioto2020iros,
  author    = {A. Milioto and J. Behley and C.S. McCool and C. Stachniss},
  title     = {{LiDAR Panoptic Segmentation for Autonomous Driving}},
  booktitle = iros,
  year      = 2020,
  codeurl   = {https://github.com/PRBonn/lidar-bonnetal},
  url       = {http://www.ipb.uni-bonn.de/pdfs/milioto2020iros.pdf}
}


@inproceedings{langer2020iros,
  author    = {F. Langer and A. Milioto and A. Haag and J. Behley and C. Stachniss},
  title     = {{Domain Transfer for Semantic Segmentation of LiDAR Data using Deep Neural Networks}},
  booktitle = iros,
  year      = 2020,
  url       = {http://www.ipb.uni-bonn.de/pdfs/langer2020iros.pdf}
}

@article{behley2020arxiv,
  author   = {J. Behley and A. Milioto and C. Stachniss},
  title    = {{A Benchmark for LiDAR-based Panoptic Segmentation based on KITTI}},
  journal  = arxiv,
  year     = 2020,
  volume   = {arXiv:2003.02371},
  url      = {http://arxiv.org/pdf/2003.02371v1},
  abstract = {Panoptic segmentation is the recently introduced task that tackles semantic segmentation and instance segmentation jointly. In this paper, we present an extension of SemanticKITTI, which is a large-scale dataset providing dense point-wise semantic labels for all sequences of the KITTI Odometry Benchmark, for training and evaluation of laser-based panoptic segmentation. We provide the data and discuss the processing steps needed to enrich a given semantic annotation with temporally consistent instance information, i.e., instance information that supplements the semantic labels and identifies the same instance over sequences of LiDAR point clouds. Additionally, we present two strong baselines that combine state-of-the-art LiDAR-based semantic segmentation approaches with a state-of-the-art detector enriching the segmentation with instance information and that allow other researchers to compare their approaches against. We hope that our extension of SemanticKITTI with strong baselines enables the creation of novel algorithms for LiDAR-based panoptic segmentation as much as it has for the original semantic segmentation and semantic scene completion tasks. Data, code, and an online evaluation using a hidden test set will be published on http://semantic-kitti.org.}
}

@article{aitkenhead2003cea,
  title   = {Weed and crop discrimination using image analysis and artificial intelligence methods},
  author  = {Aitkenhead, M. J. and Dalgetty, I. A. and Mullins, C. E. and McDonald, A. J. S. and Strachan, N. J. C.},
  journal = cea,
  year    = {2003},
  number  = {3},
  pages   = {157--171},
  volume  = {39},
  url     = {https://www.sciencedirect.com/science/article/abs/pii/S0168169903000760}
}

@book{alpaydin2004ml,
  title     = {Introduction to Machine Learning},
  author    = {Alaydin, E.},
  publisher = mitpress,
  year      = {2004},
  url       = {https://kkpatel7.files.wordpress.com/2015/04/alppaydin_machinelearning_2010.pdf}
}

@inproceedings{arnold2007comparative,
  title     = {A comparative study of methods for transductive transfer learning},
  author    = {Arnold, Andrew and Nallapati, Ramesh and Cohen, William W},
  booktitle = {Proc.~of the IEEE Int.~Conf.~on on Data Mining Workshops (ICDMW)},
  year      = {2007},
  url       = {https://ieeexplore.ieee.org/abstract/document/4476649}
}

@article{serrano2015sensors,
  author  = {Borra-Serrano, I. and Pe{\~n}a, J. M. and Torres-S{\'a}nchez, J. and Mesas-Carrascosa, F. J. and L{\'o}pez-Granados, F.},
  title   = {Spatial Quality Evaluation of Resampled Unmanned Aerial Vehicle-Imagery for Weed Mapping},
  journal = sensors,
  volume  = {15},
  year    = {2015},
  number  = {8},
  pages   = {19688},
  url     = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4570392/pdf/sensors-15-19688.pdf}
}


@article{borregaard2000jaer,
  title   = {Crop-weed discrimination by line imaging spectroscopy},
  author  = {Borregaard, T. and Nielsen, H. and Norgaard, L. and Have, H.},
  journal = jaer,
  year    = {2000},
  number  = {4},
  pages   = {389--400},
  volume  = {72},
  url     = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6928640/pdf/sensors-19-05154.pdf}
}

@article{breiman2001ml,
  title   = {Random Forests},
  author  = {L. Breiman},
  journal = ml,
  year    = {2001},
  number  = {1},
  pages   = {5--32},
  volume  = {45},
  url     = {https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf}
}

@article{burks2000tasae,
  title   = {Backpropagation neural network design and evaluation for classifying weed species using color image texture},
  author  = {Burks, T. F. and Shearer, S. A. and Gates, R. S. and Donohue, K. D.},
  journal = {Transactions of the American Society of Agricultural Engineers},
  year    = {2000},
  number  = {4},
  pages   = {1029--1037},
  volume  = {43},
  url     = {https://abe.ufl.edu/faculty/tburks/Publication%20PDF/Burks%20BP%20Weed%20Classifier%202000.pdf}
}

@inproceedings{liris2013icip,
  title     = {A model-based approach for compound leaves understanding and identification},
  author    = {G. Cerutti and L. Tougne and J. Mille and A. Vacavant and D. Coquin},
  booktitle = icip,
  year      = {2013},
  url       = {https://hal.archives-ouvertes.fr/hal-00872889/document}
}


@inproceedings{corke2013iros,
  title     = {Dealing with Shadows: Capturing Intrinsic Scene Appearance for Image-based Outdoor Localisation},
  author    = {P. Corke and R. Paul and W. Churchill and P. Newman},
  booktitle = iros,
  year      = {2013},
  url       = {http://www.robots.ox.ac.uk/~mobile/Papers/2013IROS_corke.pdf}
}

@inproceedings{farajidavar2011iccv,
  title     = {Transductive transfer learning for action recognition in tennis games},
  author    = {N. FarajiDavar and T. De Campos and J Kittler and F. Yan},
  booktitle = iccvold,
  year      = {2011},
  url       = {https://ieeexplore.ieee.org/document/6130434}
}


@article{felzenszwalb2006ijcv,
  title   = {{Efficient Belief Propagation for Early Vision}},
  author  = {P. F. Felzenszwalb and D. P. Huttenlocher},
  journal = ijcv,
  year    = {2006},
  volume  = {70},
  pages   = {41--54},
  url     = {http://cs.brown.edu/people/pfelzens/papers/bp-cvpr.pdf}
}

@article{felzenszwalb2013acm,
  author  = {Felzenszwalb, Pedro and Girshick, Ross and McAllester, David and Ramanan, Deva},
  title   = {{Visual Object Detection with Deformable Part Models}},
  year    = {2013},
  journal = cacm,
  volume  = {56},
  pages   = {97--105},
  number  = {9},
  url     = {https://dl.acm.org/doi/pdf/10.1145/2494532}
}

@article{feyaerts2001pr,
  title   = {Multi-spectral vision system for weed detection},
  author  = {Feyaerts, F. and van Gool, L.},
  journal = prl,
  year    = {2001},
  pages   = {667--674},
  volume  = {22},
  url     = {https://www.sciencedirect.com/science/article/abs/pii/S016786550100006X}
}


@article{garcia2015biosyseng,
  title   = {{Sugar beet (Beta vulgaris L.) and thistle (Cirsium arvensis L.) discrimination based on field spectral data}},
  author  = {F.J. Garcia-Ruiz and D. Wulfsohn and J. Rasmussen},
  year    = {2015},
  volume  = {139},
  pages   = {1--15},
  journal = biosyseng,
  url     = {https://www.sciencedirect.com/science/article/abs/pii/S1537511015001270}
}

@article{geipel2014rs,
  title   = {{Combined Spectral and Spatial Modeling of Corn Yield Based on Aerial Images and Crop Surface Models Acquired with an Unmanned Aircraft System}},
  author  = {J. Geipel and J. Link and W. Claupein},
  journal = rs,
  year    = {2014},
  number  = {11},
  pages   = {10335},
  volume  = {6},
  url     = {https://www.precisionag.no/wp-content/uploads/2018/02/geipel_et_al_2014.pdf}
}


@article{guerrero2012esa,
  title   = {Support Vector Machines for crop/weeds identification in maize fields },
  author  = {J. M. Guerrero and G. Pajares and M. Montalvo and J. Romeo and M. Guijarro},
  journal = esa,
  volume  = {39},
  number  = {12},
  pages   = {11149--11155},
  year    = {2012},
  url     = {https://www.sciencedirect.com/science/article/abs/pii/S0957417412005635}
}

@article{horrigan2002ehp,
  title   = {How sustainable agriculture can address the environmental and human health harms of industrial agriculture},
  journal = {Environ Health Perspect},
  volume  = {110},
  year    = {2002},
  pages   = {445--56},
  author  = {Horrigan, L. and Lawrence, R. S. and Walker, P.},
  url     = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1240832/pdf/ehp0110-000445.pdf}
}

@inproceedings{kodagoda2008ipms,
  title     = {Weed detection and classification for autonomous farming},
  author    = {Kodagoda, S and Zhang, Z and Ruiz, D and Dissanayake, G},
  booktitle = {Proc. of the Intelligent Production Machines and Systems},
  year      = {2008},
  url       = {https://pdfs.semanticscholar.org/e2b7/94627dac28e81f24b510c6e93df06535a1c2.pdf}
}

@inproceedings{kumar2012eccv,
  title     = {{Leafsnap: A Computer Vision System for Automatic Plant Species Identification}},
  author    = {N. Kumar and P. N. Belhumeur and A. Biswas and D. W.Jacobs and W. J. Kress and I. L{\'o}pez and J. V. B. Soares},
  booktitle = eccv,
  year      = {2012},
  url       = {https://link.springer.com/content/pdf/10.1007%2F978-3-642-33709-3.pdf}
}

@article{latte2015ijsip,
  title   = {{A Combined Color and Texture Features Based Methodology for Recognition of Crop Field Image}},
  author  = {Latte, M. V. and Anami, B. S. and Kuligod V. B.},
  journal = ijsip,
  year    = {2015},
  number  = {2},
  pages   = {287--302},
  volume  = {8},
  url     = {http://article.nadiapub.com/IJSIP/vol8_no2/28.pdf}
}

@inproceedings{kulikov2020cvpr,
  title     = {{Instance Segmentation of Biological Images using Harmonic Embeddings}},
  author    = {Victor Kulikov and Victor Lempitsky},
  booktitle = cvpr,
  year      = {2020},
  url       = {proceedings: kulikov2020cvpr.pdf}
}

@article{wang2016ei,
  author  = {L. Wang and N. Zheng and C. Hanyue and L. Dong and W. Mingquan and Z. Wei},
  journal = {Ecological Indicators},
  pages   = {637--648},
  title   = {Remote estimation of canopy height and aboveground biomass of maize using high-resolution stereo images from a low-cost unmanned aerial vehicle system},
  volume  = {67},
  year    = {2016},
  url     = {http://agri.ckcest.cn/ass/NK006-20160711004.pdf}
}


@inproceedings{liebisch2017earsel,
  author    = {F. Liebisch and M. Popovic and J. Pfeifer and R. Khanna and P. Lottes and C. Stachniss and A. Pretto and S. In Kyu and J. Nieto and R. Siegwart and A. Walter},
  title     = {Automatic UAV-based field inspection campaigns for weeding in row crops},
  booktitle = {Proc. of the 10th EARSeL SIG Imaging Spectroscopy Workshop},
  year      = {2017}
}

@inproceedings{mcmanus2016icra,
  title     = {Shady Dealings: Robust, Long- Term Visual Localisation using Illumination Invariance},
  author    = {C. McManus and W. Churchill and W. Maddern and A. Stewart and P. Newman},
  booktitle = icra,
  year      = {2014},
  url       = {http://www.robots.ox.ac.uk/~mobile/Papers/2014ICRA_McManus.pdf}
}

@inproceedings{midtiby2014njf,
  title     = {Automatic Location of Crop Rows in UAV Images},
  author    = {H. S. Midtiby and J. Rasmussen},
  booktitle = {NJF Seminar 477. Future arable farming and agricultural engineering },
  year      = {2014},
  url       = {https://portal.findresearcher.sdu.dk/en/publications/automatic-crop-row-detection-from-uav-images}
}


@article{montalvo2012esa,
  author  = {Montalvo, M. and Pajares, G. and Guerrero, J. M. and Romeo, J. and Guijarro, M. and Ribeiro, A. and Ruz, J. J. and Cruz, J. M.},
  title   = {Automatic Detection of Crop Rows in Maize Fields with High Weeds Pressure},
  journal = esa,
  volume  = {39},
  number  = {15},
  year    = {2012},
  pages   = {11889--11897},
  url     = {https://www.sciencedirect.com/science/article/abs/pii/S0957417412003806}
}

@article{ojala1999pr,
  title   = {Unsupervised texture segmentation using feature distributions},
  author  = {T. Ojala and M. PietikÃ¤Ã¤inen},
  journal = pr,
  year    = {1999},
  volume  = {32},
  number  = {3},
  pages   = {477--486}
}

@article{pan2010tkde,
  title   = {A Survey on Transfer Learning},
  author  = {Pan, Sinno Jialin and Yang, Qiang},
  journal = {IEEE Trans. on Knowl. and Data Eng.},
  year    = {2010},
  number  = {10},
  pages   = {1345--1359},
  volume  = {22}
}

@article{pena2013pls,
  author  = {J. M. Pe{\~n}a and J. Torres-S{\'a}nchez and A. I. de Castro and M. Kelly and F. L{\'o}pez-Granados},
  journal = plosone,
  title   = {Weed Mapping in Early-Season Maize Fields Using Object-Based Analysis of Unmanned Aerial Vehicle UAV Images},
  year    = {2013},
  volume  = {8}
}

@article{pena2015sensors,
  author  = {J. M. Pe{\~n}a and J. Torres-S{\'a}nchez and A. Serrano-Perez  and A. I. de Castro and F. L{\'o}pez-Granados},
  title   = {Quantifying Efficacy and Limits of Unmanned Aerial Vehicle UAV Technology for Weed Seedling Detection as Affected by Sensor Resolution},
  journal = sensors,
  volume  = {15},
  year    = {2015},
  number  = {3}
}


@article{perezortiz2016esa,
  title   = {Selecting patterns and features for between- and within- crop-row weed mapping using UAV-imagery },
  author  = {M. Perez-Ortiz and J. M. Pe{\~n}a and P. A. Gutierrez and J. Torres-S{\'a}nchez and C. Herv{\'a}s-Mart{\'i}nez and F. L{\'o}pez-Granados},
  journal = esa,
  volume  = {47},
  pages   = {85--94},
  year    = {2016}
}

@article{perezortiz2015asc,
  title   = {A semi-supervised system for weed mapping in sunflower crops using unmanned aerial vehicles and a crop row detection method },
  journal = {Applied Soft Computing},
  volume  = {37},
  pages   = {533--544},
  year    = {2015},
  author  = {M. Perez-Ortiz and J. M. Pe{\~n}a and P. A. Gutierrez and J. Torres-S{\'a}nchez and C. Herv{\'a}s-Mart{\'i}nez and F. L{\'o}pez-Granados}
}

@inproceedings{raina2007icml,
  title     = {Self-taught Learning: Transfer Learning from Unlabeled Data},
  author    = {R. Raina and A. Battle  and H. Lee  and B. Packer  and A. Y. Ng},
  booktitle = icml,
  year      = {2007}
}

@inproceedings{ranganathan2007rss,
  title     = {Semantic Modeling of Places using Objects},
  author    = {Ranganathan, A. and Dellaert, F.},
  booktitle = rss,
  year      = {2007}
}

@article{rainville2012ppa,
  author  = { F. M. De Rainville and A. Durand and F. A. Fortin and K. Tanguy and X. Maldague and B. Panneton and M. J. Simard},
  title   = { Bayesian Classification and Unsupervised Learning for Isolating Weeds in Row Crops },
  volume  = { 17 },
  number  = { 2 },
  pages   = { 401--414 },
  year    = { 2014 },
  journal = paa
}

@article{schirrmann2016cea,
  author  = {Schirrmann, M. and Hamdorf, A. and Garz, A. and Ustyuzhanin, A. and Dammer, K.},
  journal = cea,
  pages   = {374--384},
  title   = {Estimating wheat biomass by combining image clustering with crop height.},
  volume  = {121},
  year    = {2016}
}

@article{shearer1990tasae,
  title   = {Plant identification using color co-occurrence matrices},
  author  = {Shearer, S.A. and Holmes, R.G.},
  journal = {Transactions of the American Society of Agricultural Engineers},
  year    = {1990},
  number  = {6},
  pages   = {2037--2044},
  volume  = {33}
}


@article{slaughter1980cea,
  title   = {Autonomous robotic weed control systems: A review },
  journal = cea,
  volume  = {61},
  number  = {1},
  pages   = {63--78},
  year    = {2008},
  author  = {D.C. Slaughter and D.K. Giles and D. Downey}
}

@article{strothmann2017cea,
  title   = {Plant classification with In-Field-Labeling for crop/weed discrimination using spectral features and 3D surface features from a multi-wavelength laser line profile system },
  journal = cea,
  volume  = {134},
  pages   = {79--93},
  year    = {2017},
  author  = {W. Strothmann and A. Ruckelshausen and J. Hertzberg and C. Scholz and F. Langsenkamp}
}

@article{tax2000pr,
  author  = {Tax, D. M. and van Breukelen, M. and Duin, R. P. and Kittler, J.},
  journal = pr,
  number  = {9},
  pages   = {1475--1485},
  title   = {Combining multiple classifiers by averaging or by multiplying?},
  volume  = {33},
  year    = {2000}
}

@article{tellaeche2008pr,
  title   = {A vision-based method for weeds identification through the Bayesian decision theory},
  author  = {Tellaeche, A. and Burgos-Artizzu, X. P. and Pajares, G. and Ribeiro, A.},
  journal = pr,
  year    = {2008},
  number  = {2},
  pages   = {521--530},
  volume  = {41}
}

@inbook{tokekar2013iros,
  title     = {Sensor planning for a symbiotic UAV and UGV system for precision agriculture},
  author    = {Pratap Tokekar and Hook, {Joshua Vander} and David Mulla and Volkan Isler},
  year      = {2013},
  booktitle = iros
}


@article{wang2008amc,
  title   = {Classification of plant leaf images with complicated background},
  author  = {X.F. Wang and D. Huang and J. Du and H. Xu and L. Heutte},
  journal = {Applied Mathematics and Computation},
  year    = {2008},
  pages   = {916--926},
  volume  = {205},
  number  = {2}
}


@article{zeng2016arxiv,
  author   = {A. Zeng and S. Song and M. NieÃner and M. Fisher and J. Xiao and T. Funkhouser},
  title    = {{3DMatch: Learning Local Geometric Descriptors from RGB-D Reconstructions}},
  journal  = arxiv,
  year     = 2016,
  volume   = {arXiv:1603.08182},
  url      = {http://arxiv.org/pdf/1603.08182v3},
  abstract = {Matching local geometric features on real-world depth images is a challenging task due to the noisy, low-resolution, and incomplete nature of 3D scan data. These difficulties limit the performance of current state-of-art methods, which are typically based on histograms over geometric properties. In this paper, we present 3DMatch, a data-driven model that learns a local volumetric patch descriptor for establishing correspondences between partial 3D data. To amass training data for our model, we propose a self-supervised feature learning method that leverages the millions of correspondence labels found in existing RGB-D reconstructions. Experiments show that our descriptor is not only able to match local geometry in new scenes for reconstruction, but also generalize to different tasks and spatial scales (e.g. instance-level object model alignment for the Amazon Picking Challenge, and mesh surface correspondence). Results show that 3DMatch consistently outperforms other state-of-the-art approaches by a significant margin. Code, data, benchmarks, and pre-trained models are available online at http://3dmatch.cs.princeton.edu}
}

@article{szegedy2016arxiv,
  author   = {C. Szegedy and S. Ioffe and V. Vanhoucke and A. Alemi},
  title    = {{Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning}},
  journal  = arxiv,
  year     = 2016,
  volume   = {arXiv:1602.07261},
  url      = {http://arxiv.org/pdf/1602.07261v2},
  abstract = {Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question of whether there are any benefit in combining the Inception architecture with residual connections. Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4, we achieve 3.08 percent top-5 error on the test set of the ImageNet classification (CLS) challenge}
}

@article{he2015arxiv,
  author   = {K. He and X. Zhang and S. Ren and J. Sun},
  title    = {{Deep Residual Learning for Image Recognition}},
  journal  = arxiv,
  year     = 2015,
  volume   = {arXiv:1512.03385},
  url      = {http://arxiv.org/pdf/1512.03385v1},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.   The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.}
}


@article{lecun1989nc,
  author  = {LeCun, Y. and Boser, B.  and Denker, J. S. and Henderson, D.
             and Howard, R. E. and Hubbard, W. and Jackel,  L. D.},
  title   = {Backpropagation Applied to Handwritten Zip Code Recognition},
  journal = {Neural Computation},
  year    = {1989},
  volume  = {1},
  number  = {4},
  pages   = {541--551}
}

@inproceedings{lecun1998gbl,
  author    = {Yann LeCun and LÃ©on Bottou and Yoshua Bengio and Patrick Haffner},
  title     = {Gradient-based learning applied to document recognition},
  booktitle = {Proc. of the IEEE},
  year      = {1998}
}


@inproceedings{lafferty2001icml,
  author    = {J.D. Lafferty and A. McCallum and F.C.N. Pereira},
  title     = {{Conditional Random Fields: Probabilistic Models for Segmentating and Labeling Sequence Data}},
  booktitle = icml,
  year      = 2001
}


@inproceedings{regier2018humanoids,
  author    = {P. Regier and A. Milioto and P. Karkowski and C. Stachniss and M. Bennewitz},
  title     = {{Classifying Obstacles and Exploiting Knowledge about Classes for Efficient Humanoid Navigation}},
  booktitle = humanoids,
  year      = 2018
}

@article{howard2019arxiv,
  author   = {A. Howard and M. Sandler and G. Chu and L. Chen and B. Chen and M. Tan and W. Wang and Y. Zhu and R. Pang and V. Vasudevan and Q.V. Le and H. Adam},
  title    = {{Searching for MobileNetV3}},
  journal  = arxiv,
  year     = 2019,
  volume   = {arXiv:1905.02244v5},
  url      = {http://arxiv.org/pdf/1905.02244v5},
  abstract = {We present the next generation of MobileNets based on a combination of complementary search techniques as well as a novel architecture design. MobileNetV3 is tuned to mobile phone CPUs through a combination of hardware-aware network architecture search (NAS) complemented by the NetAdapt algorithm and then subsequently improved through novel architecture advances. This paper starts the exploration of how automated search algorithms and network design can work together to harness complementary approaches improving the overall state of the art. Through this process we create two new MobileNet models for release: MobileNetV3-Large and MobileNetV3-Small which are targeted for high and low resource use cases. These models are then adapted and applied to the tasks of object detection and semantic segmentation. For the task of semantic segmentation (or any dense pixel prediction), we propose a new efficient segmentation decoder Lite Reduced Atrous Spatial Pyramid Pooling (LR-ASPP). We achieve new state of the art results for mobile classification, detection and segmentation. MobileNetV3-Large is 3.2\% more accurate on ImageNet classification while reducing latency by 15\% compared to MobileNetV2. MobileNetV3-Small is 4.6\% more accurate while reducing latency by 5\% compared to MobileNetV2. MobileNetV3-Large detection is 25\% faster at roughly the same accuracy as MobileNetV2 on COCO detection. MobileNetV3-Large LR-ASPP is 30\% faster than MobileNetV2 R-ASPP at similar accuracy for Cityscapes segmentation.}
}


@article{geyer2020arxiv,
  title   = {{A2D2: Audi Autonomous Driving Dataset}},
  author  = {J. Geyer and Y. Kassahun and M. Mahmudi and X. Ricou and R. Durgesh and A. S. Chung and L. Hauswald and V. Hoang Pham and M. M{\"u}hlegg and S. Dorn and T. Fernandez and M. J{\"a}nicke and S. Mirashi and C. Savani and M. Sturm and O. Vorobiov and M. Oelker and S. Garreis and P. Schuberth},
  journal = arxiv,
  year    = {2020},
  volume  = {arXiv:2004.06320},
  url     = {https://arxiv.org/pdf/2004.06320.pdf}
}

@article{hoang2020ral,
  title   = {{Panoptic 3D mapping and object pose estimation using adaptively weighted semantic information}},
  author  = {Hoang, Dinh-Cuong and Lilienthal, Achim J and Stoyanov, Todor},
  journal = ral,
  volume  = {5},
  number  = {2},
  pages   = {1962--1969},
  year    = {2020},
  url     = {proceedings: hoang2020ral.pdf}
}

@article{sun2019arxiv,
  author   = {P. Sun and H. Kretzschmar and X. Dotiwalla and A. Chouard and V. Patnaik and P. Tsui and J. Guo and Y. Zhou and Y. Chai and B. Caine and V. Vasudevan and W. Han and J. Ngiam and H. Zhao and A. Timofeev and S. Ettinger and M. Krivokon and A. Gao and A. Joshi and S. Zhao and S. Cheng and Y. Zhang and J. Shlens and Z. Chen and D. Anguelov},
  title    = {{Scalability in Perception for Autonomous Driving: Waymo Open Dataset}},
  journal  = arxiv,
  year     = 2019,
  volume   = {arXiv:1912.04838v7},
  url      = {http://arxiv.org/pdf/1912.04838v7},
  abstract = {The research community has increasing interest in autonomous driving research, despite the resource intensity of obtaining representative real world data. Existing self-driving datasets are limited in the scale and variation of the environments they capture, even though generalization within and between operating regions is crucial to the overall viability of the technology. In an effort to help align the research community's contributions with real-world self-driving problems, we introduce a new large scale, high quality, diverse dataset. Our new dataset consists of 1150 scenes that each span 20 seconds, consisting of well synchronized and calibrated high quality LiDAR and camera data captured across a range of urban and suburban geographies. It is 15x more diverse than the largest camera+LiDAR dataset available based on our proposed diversity metric. We exhaustively annotated this data with 2D (camera image) and 3D (LiDAR) bounding boxes, with consistent identifiers across frames. Finally, we provide strong baselines for 2D as well as 3D detection and tracking tasks. We further study the effects of dataset size and generalization across geographies on 3D detection methods. Find data, code and more up-to-date information at http://www.waymo.com/open.}
}

@misc{kesten2019misc,
  title        = {{Lyft Level 5 AV Dataset 2019}},
  author       = {Kesten, R. and Usman, M. and Houston, J. and Pandya, T. and Nadhamuni, K. and Ferreira, A. and Yuan, M. and Low, B. and Jain, A. and Ondruska, P. and Omari, S. and Shah, S. and Kulkarni, A. and Kazakova, A. and Tao, C. and Platinsky, L. and Jiang, W. and Shet, V.},
  year         = 2019,
  howpublished = {{https://level5.lyft.com/dataset/}}
}

@inproceedings{chang2019cvpr,
  author    = {M.F. Chang and J.W. Lambert and P. Sangkloy and J. Singh and S. Bak and A. Hartnett and D. Wang and P. Carr and S. Lucey and D. Ramanan and J. Hays},
  title     = {{Argoverse: 3D Tracking and Forecasting with Rich Maps}},
  booktitle = cvpr,
  year      = {2019}
}

@article{hu2019arxiv,
  author   = {Q. Hu and B. Yang and L. Xie and S. Rosa and Y. Guo and Z. Wang and N. Trigoni and A. Markham},
  title    = {{RandLA-Net: Efficient Semantic Segmentation of Large-Scale Point Clouds}},
  journal  = arxiv,
  year     = 2019,
  volume   = {arXiv:1911.11236},
  url      = {http://arxiv.org/pdf/1911.11236v3},
  abstract = {We study the problem of efficient semantic segmentation for large-scale 3D point clouds. By relying on expensive sampling techniques or computationally heavy pre/post-processing steps, most existing approaches are only able to be trained and operate over small-scale point clouds. In this paper, we introduce RandLA-Net, an efficient and lightweight neural architecture to directly infer per-point semantics for large-scale point clouds. The key to our approach is to use random point sampling instead of more complex point selection approaches. Although remarkably computation and memory efficient, random sampling can discard key features by chance. To overcome this, we introduce a novel local feature aggregation module to progressively increase the receptive field for each 3D point, thereby effectively preserving geometric details. Extensive experiments show that our RandLA-Net can process 1 million points in a single pass with up to 200X faster than existing approaches. Moreover, our RandLA-Net clearly surpasses state-of-the-art approaches for semantic segmentation on two large-scale benchmarks Semantic3D and SemanticKITTI.}
}

@article{mohan2020arxiv,
  author   = {R. Mohan and A. Valada},
  title    = {{EfficientPS: Efficient Panoptic Segmentation}},
  journal  = arxiv,
  year     = 2020,
  volume   = {arXiv:2004.02307},
  url      = {http://arxiv.org/pdf/2004.02307v2},
  abstract = {Understanding the scene in which an autonomous robot operates is critical for its competent functioning. Such scene comprehension necessitates recognizing instances of traffic participants along with general scene semantics which can be effectively addressed by the panoptic segmentation task. In this paper, we introduce the Efficient Panoptic Segmentation (EfficientPS) architecture that consists of a shared backbone which efficiently encodes and fuses semantically rich multi-scale features. We incorporate a new semantic head that aggregates fine and contextual features coherently and a new variant of Mask R-CNN as the instance head. We also propose a novel panoptic fusion module that congruously integrates the output logits from both the heads of our EfficientPS architecture to yield the final panoptic segmentation output. Additionally, we introduce the KITTI panoptic segmentation dataset that contains panoptic annotations for the popularly challenging KITTI benchmark. Extensive evaluations on Cityscapes, KITTI, Mapillary Vistas and Indian Driving Dataset demonstrate that our proposed architecture consistently sets the new state-of-the-art on all these four benchmarks while being the most efficient and fast panoptic segmentation architecture to date.}
}

@article{rosu2019arxiv,
  author   = {R.A. Rosu and P. SchÃ¼tt and J. Quenzel and S. Behnke},
  title    = {{LatticeNet: Fast Point Cloud Segmentation Using Permutohedral Lattices}},
  journal  = arxiv,
  year     = 2019,
  volume   = {arXiv:1912.05905},
  url      = {http://arxiv.org/pdf/1912.05905v2},
  abstract = {Deep convolutional neural networks (CNNs) have shown outstanding performance in the task of semantically segmenting images. However, applying the same methods on 3D data still poses challenges due to the heavy memory requirements and the lack of structured data. Here, we propose LatticeNet, a novel approach for 3D semantic segmentation, which takes as input raw point clouds. A PointNet describes the local geometry which we embed into a sparse permutohedral lattice. The lattice allows for fast convolutions while keeping a low memory footprint. Further, we introduce DeformSlice, a novel learned data-dependent interpolation for projecting lattice features back onto the point cloud. We present results of 3D segmentation on various datasets where our method achieves state-of-the-art performance.}
}

@article{zhang2020arxiv,
  author   = {Y. Zhang and Z. Zhou and P. David and X. Yue and Z. Xi and B. Gong and H. Foroosh},
  title    = {{PolarNet: An Improved Grid Representation for Online LiDAR Point Clouds Semantic Segmentation}},
  journal  = arxiv,
  year     = 2020,
  volume   = {arXiv:2003.14032},
  url      = {http://arxiv.org/pdf/2003.14032v2},
  abstract = {The need for fine-grained perception in autonomous driving systems has resulted in recently increased research on online semantic segmentation of single-scan LiDAR. Despite the emerging datasets and technological advancements, it remains challenging due to three reasons: (1) the need for near-real-time latency with limited hardware; (2) uneven or even long-tailed distribution of LiDAR points across space; and (3) an increasing number of extremely fine-grained semantic classes. In an attempt to jointly tackle all the aforementioned challenges, we propose a new LiDAR-specific, nearest-neighbor-free segmentation algorithm - PolarNet. Instead of using common spherical or bird's-eye-view projection, our polar bird's-eye-view representation balances the points across grid cells in a polar coordinate system, indirectly aligning a segmentation network's attention with the long-tailed distribution of the points along the radial axis. We find that our encoding scheme greatly increases the mIoU in three drastically different segmentation datasets of real urban LiDAR single scans while retaining near real-time throughput.}
}

@article{xu2020arxiv,
  author   = {C. Xu and B. Wu and Z. Wang and W. Zhan and P. Vajda and K. Keutzer and M. Tomizuka},
  title    = {{SqueezeSegV3: Spatially-Adaptive Convolution for Efficient Point-Cloud Segmentation}},
  journal  = arxiv,
  year     = 2020,
  volume   = {arXiv:2004.01803},
  url      = {http://arxiv.org/pdf/2004.01803v1},
  abstract = {LiDAR point-cloud segmentation is an important problem for many applications. For large-scale point cloud segmentation, the \textit{de facto} method is to project a 3D point cloud to get a 2D LiDAR image and use convolutions to process it. Despite the similarity between regular RGB and LiDAR images, we discover that the feature distribution of LiDAR images changes drastically at different image locations. Using standard convolutions to process such LiDAR images is problematic, as convolution filters pick up local features that are only active in specific regions in the image. As a result, the capacity of the network is under-utilized and the segmentation performance decreases. To fix this, we propose Spatially-Adaptive Convolution (SAC) to adopt different filters for different locations according to the input image. SAC can be computed efficiently since it can be implemented as a series of element-wise multiplications, im2col, and standard convolution. It is a general framework such that several previous methods can be seen as special cases of SAC. Using SAC, we build SqueezeSegV3 for LiDAR point-cloud segmentation and outperform all previous published methods by at least 3.7\% mIoU on the SemanticKITTI benchmark with comparable inference speed.}
}

@article{cortinhal2020arxiv,
  author   = {T. Cortinhal and G. Tzelepis and E.E. Aksoy},
  title    = {{SalsaNext: Fast Semantic Segmentation of LiDAR Point Clouds for Autonomous Driving}},
  journal  = arxiv,
  year     = 2020,
  volume   = {arXiv:2003.03653},
  url      = {http://arxiv.org/pdf/2003.03653v1},
  abstract = {In this paper, we introduce SalsaNext for the semantic segmentation of a full 3D LiDAR point cloud in real-time. SalsaNext is the next version of SalsaNet [1] which has an encoder-decoder architecture where the encoder unit has a set of ResNet blocks and the decoder part combines upsampled features from the residual blocks. In contrast to SalsaNet, we have an additional layer in the encoder and decoder, introduce the context module, switch from stride convolution to average pooling and also apply central dropout treatment. To directly optimize the Jaccard index, we further combine the weighted cross-entropy loss with Lovasz-Softmax loss [2]. We provide a thorough quantitative evaluation on the Semantic-KITTI dataset [3], which demonstrates that the proposed SalsaNext outperforms other state-of-the-art semantic segmentation networks in terms of accuracy and computation time. We also release our source code https://github.com/TiagoCortinhal/SalsaNext.}
}

@article{alonso2020arxiv,
  author   = {I. Alonso and L. Riazuelo and L. Montesano and A.C. Murillo},
  title    = {{3D-MiniNet: Learning a 2D Representation from Point Clouds for Fast and Efficient 3D LIDAR Semantic Segmentation}},
  journal  = arxiv,
  year     = 2020,
  volume   = {arXiv:2002.10893v2},
  url      = {http://arxiv.org/pdf/2002.10893v2},
  abstract = {LIDAR semantic segmentation, which assigns a semantic label to each 3D point measured by the LIDAR, is becoming an essential task for many robotic applications such as autonomous driving. Fast and efficient semantic segmentation methods are needed to match the strong computational and temporal restrictions of many of these real-world applications.   This work presents 3D-MiniNet, a novel approach for LIDAR semantic segmentation that combines 3D and 2D learning layers. It first learns a 2D representation from the raw points through a novel projection which extracts local and global information from the 3D data. This representation is fed to an efficient 2D Fully Convolutional Neural Network (FCNN) that produces a 2D semantic segmentation. These 2D semantic labels are re-projected back to the 3D space and enhanced through a post-processing module. The main novelty in our strategy relies on the projection learning module. Our detailed ablation study shows how each component contributes to the final performance of 3D-MiniNet. We validate our approach on well known public benchmarks (SemanticKITTI and KITTI), where 3D-MiniNet gets state-of-the-art results while being faster and more parameter-efficient than previous methods.}
}

@article{razavian2014arxiv,
  author   = {A.S. Razavian and H. Azizpour and J. Sullivan and S. Carlsson},
  title    = {{CNN Features off-the-shelf: an Astounding Baseline for Recognition}},
  journal  = arxiv,
  year     = 2014,
  volume   = {arXiv:1403.6382},
  url      = {http://arxiv.org/pdf/1403.6382v3},
  abstract = {Recent results indicate that the generic descriptors extracted from the convolutional neural networks are very powerful. This paper adds to the mounting evidence that this is indeed the case. We report on a series of experiments conducted for different recognition tasks using the publicly available code and model of the \overfeat network which was trained to perform object classification on ILSVRC13. We use features extracted from the \overfeat network as a generic image representation to tackle the diverse range of recognition tasks of object image classification, scene recognition, fine grained recognition, attribute detection and image retrieval applied to a diverse set of datasets. We selected these tasks and datasets as they gradually move further away from the original task and data the \overfeat network was trained to solve. Astonishingly, we report consistent superior results compared to the highly tuned state-of-the-art systems in all the visual classification tasks on various datasets. For instance retrieval it consistently outperforms low memory footprint methods except for sculptures dataset. The results are achieved using a linear SVM classifier (or $L2$ distance in case of retrieval) applied to a feature representation of size 4096 extracted from a layer in the net. The representations are further modified using simple augmentation techniques e.g., jittering. The results strongly suggest that features obtained from deep learning with convolutional nets should be the primary candidate in most visual recognition tasks.}
}

@inproceedings{zabawa2019cvpr-ws,
  author    = {L. Zabawa and A. Kicherer and L. Klingbeil and A. Milioto and R. Topfer and H. Kuhlmann and R. Roscher},
  title     = {Detection of Single Grapevine Berries in Images Using Fully Convolutional Neural Networks},
  booktitle = cvprws,
  year      = {2019},
  url       = {https://openaccess.thecvf.com/content_CVPRW_2019/papers/CVPPP/Zabawa_Detection_of_Single_Grapevine_Berries_in_Images_Using_Fully_Convolutional_CVPRW_2019_paper.pdf},
  abstract  = {Yield estimation and forecasting are of special interest inthe field of grapevine breeding and viticulture. The numberof harvested berries per plant is strongly correlated withthe resulting quality. Therefore, early yield forecasting canenable a focused thinning of berries to ensure a high qual-ity end product. Traditionally yield estimation is done byextrapolating from a small sample size and by utilizing his-toric data. Moreover, it needs to be carried out by skilled ex-perts with much experience in this field. Berry detection inimages offers a cheap, fast and non-invasive alternative tothe otherwise time-consuming and subjective on-site anal-ysis by experts. We apply fully convolutional neural net-works on images acquired with the Phenoliner, a field phe-notyping platform. We count single berries in images toavoid the error-prone detection of grapevine clusters. Clus-ters are often overlapping and can vary a lot in the sizewhich makes the reliable detection of them difficult. We ad-dress especially the detection of white grapes directly in thevineyard. The detection of single berries is formulated as aclassification task with three classes, namely âberryâ, âedgeâand âbackgroundâ. A connected component algorithm is ap-plied to determine the number of berries in one image. Wecompare the automatically counted number of berries withthe manually detected berries in 60 images showing Ries-ling plants in vertical shoot positioned trellis (VSP) andsemi minimal pruned hedges (SMPH). We are able to detectberries correctly within the VSP system with an accuracy of 94.0\% and for the SMPH system with 85.6\%.}
}


@article{lottes2020jfr,
  title   = {{Robust Joint Stem Detection and Crop-Weed Classification using Image Sequences for Plant-Specific Treatment in Precision Farming}},
  author  = {Lottes, P. and Behley, J. and Chebrolu, N. and Milioto, A. and Stachniss, C.},
  journal = jfr,
  volume  = {37},
  number  = {1},
  pages   = {20--34},
  year    = {2020},
  url     = {http://www.ipb.uni-bonn.de/pdfs/lottes2019jfr.pdf}
}

@inproceedings{sheikh2020icra,
  title     = {Gradient and Log-based Active Learning for Semantic Segmentation of Crop and Weed for Agricultural Robots},
  author    = {R. Sheikh and A. Milioto and P. Lottes and C. Stachniss and M. Bennewitz and T. Schultz},
  booktitle = icra,
  year      = {2020},
  url       = {http://www.ipb.uni-bonn.de/pdfs/sheikh2020icra.pdf}
}

@article{regier2020ijhr,
  author   = {Regier, P. and Milioto, A. and Stachniss, C. and Bennewitz, M.},
  title    = {{Classifying Obstacles and Exploiting Class Information for Humanoid Navigation Through Cluttered Environments}},
  journal  = ijhr,
  volume   = {17},
  number   = {02},
  pages    = {2050013},
  year     = {2020},
  abstract = {Humanoid robots are often supposed to share their workspace with humans and thus have to deal with objects used by humans in their everyday life. In this article, we present our novel approach to humanoid navigation through cluttered environments, which exploits knowledge about different obstacle classes to decide how to deal with obstacles and select appropriate robot actions. To classify objects from RGB images and decide whether an obstacle can be overcome by the robot with a corresponding action, e.g., by pushing or carrying it aside or stepping over or onto it, we train and exploit a convolutional neural network (CNN). Based on associated action costs, we compute a cost grid containing newly observed objects in addition to static obstacles on which a 2D path can be efficiently planned. This path encodes the necessary actions that need to be carried out by the robot to reach the goal. We implemented our framework in the Robot Operating System (ROS) and tested it in various scenarios with a Nao robot as well as in simulation with the REEM-C robot. As the experiments demonstrate, using our CNN, the robot can robustly classify the observed obstacles into the different classes and decide on suitable actions to find efficient solution paths. Our system finds paths also through regions where traditional motion planning methods are not able to calculate a solution or require substantially more time. }
}


@article{pretto2019arxiv,
  author   = {A. Pretto and S. Aravecchia and W. Burgard and N. Chebrolu and C. Dornhege and T. Falck and F. Fleckenstein and A. Fontenla and M. Imperoli and R. Khanna and F. Liebisch and P. Lottes and A. Milioto and D. Nardi and S. Nardi and J. Pfeifer and M. PopoviÄ and C. Potena and C. Pradalier and E. Rothacker-Feder and I. Sa and A. Schaefer and R. Siegwart and C. Stachniss and A. Walter and W. Winterhalter and X. Wu and J. Nieto},
  title    = {{Building an Aerial-Ground Robotics System for Precision Farming}},
  journal  = arxiv,
  year     = 2019,
  volume   = {arXiv:1911.03098},
  url      = {http://arxiv.org/pdf/1911.03098v2},
  abstract = {The application of autonomous robots in agriculture is gaining increasing popularity thanks to the high impact it may have on food security, sustainability, resource use efficiency, reduction of chemical treatments, and the optimization of human effort and yield. With this vision, the Flourish research project aimed to develop an adaptable robotic solution for precision farming that combines the aerial survey capabilities of small autonomous unmanned aerial vehicles (UAVs) with targeted intervention performed by multi-purpose unmanned ground vehicles (UGVs). This paper presents an overview of the scientific and technological advances and outcomes obtained in the project. We introduce multi-spectral perception algorithms and aerial and ground-based systems developed for monitoring crop density, weed pressure, crop nitrogen nutrition status, and to accurately classify and locate weeds. We then introduce the navigation and mapping systems tailored to our robots in the agricultural environment, as well as the modules for collaborative mapping. We finally present the ground intervention hardware, software solutions, and interfaces we implemented and tested in different field conditions and with different crops. We describe a real use case in which a UAV collaborates with a UGV to monitor the field and to perform selective spraying without human intervention.}
}

@article{pretto2020ram,
  author  = {A. Pretto and S. Aravecchia and W. Burgard and N. Chebrolu and C. Dornhege and T. Falck and F. Fleckenstein and A. Fontenla and M. Imperoli and R. Khanna and F. Liebisch and P. Lottes and A. Milioto and D. Nardi and S. Nardi and J. Pfeifer and M. PopoviÄ and C. Potena and C. Pradalier and E. Rothacker-Feder and I. Sa and A. Schaefer and R. Siegwart and C. Stachniss and A. Walter and W. Winterhalter and X. Wu and J. Nieto},
  title   = {{Building an Aerial-Ground Robotics System for Precision Farming}},
  journal = ram,
  volume  = {28},
  number  = {3},
  pages   = {29--49},
  year    = 2020
}

@inproceedings{franz2018ocean,
  author    = {Franz, K. and Roscher, R. and Milioto, A. and Wenzel, S. and Kusche, J.},
  title     = {Ocean Eddy Identification and Tracking using Neural Networks},
  booktitle = {Proc. of the IEEE Int. Geoscience and Remote Sensing Symposium (IGARSS)},
  year      = {2018},
  url       = {https://arxiv.org/abs/arXiv:1803.07436}
}

@article{hurtado2020arxiv,
  author   = {J.V. Hurtado and R. Mohan and W. Burgard and A. Valada},
  title    = {{MOPT: Multi-Object Panoptic Tracking}},
  journal  = arxiv,
  volume   = {arXiv:2004.08189},
  year     = 2020,
  url      = {https://arxiv.org/pdf/2004.08189v2.pdf},
  abstract = {Comprehensive understanding of dynamic scenes is a critical prerequisite for intelligent robots to autonomously operate in their environment. Research in this domain, which encompasses diverse perception problems, has primarily been focused on addressing specific tasks individually rather than modeling the ability to understand dynamic scenes holistically. In this paper, we introduce a novel perception task denoted as multi-object panoptic tracking (MOPT), which unifies the conventionally disjoint tasks of semantic segmentation, instance segmentation, and multi-object tracking. MOPT allows for exploiting pixel-level semantic information of 'thing' and 'stuff' classes, temporal coherence, and pixel-level associations over time, for the mutual benefit of each of the individual sub-problems. To facilitate quantitative evaluations of MOPT in a unified manner, we propose the soft panoptic tracking quality (sPTQ) metric. As a first step towards addressing this task, we propose the novel PanopticTrackNet architecture that builds upon the state-of-the-art top-down panoptic segmentation network EfficientPS by adding a new tracking head to simultaneously learn all sub-tasks in an end-to-end manner. Additionally, we present several strong baselines that combine predictions from state-of-the-art panoptic segmentation and multi-object tracking models for comparison. We present extensive quantitative and qualitative evaluations of both vision-based and LiDAR-based MOPT that demonstrate encouraging results.}
}

@phdthesis{palazzolo2019phd,
  author = {Palazzolo, E.},
  title  = {Active 3D Reconstruction for Mobile Robots},
  year   = 2019,
  school = {University of Bonn},
  url    = {http://www.ipb.uni-bonn.de/pdfs/palazzolo2019phd.pdf}
}

@article{hornik1991nn,
  title    = {{Approximation capabilities of multilayer feedforward networks}},
  journal  = {Neural Networks},
  volume   = {4},
  number   = {2},
  pages    = {251 - 257},
  year     = {1991},
  url      = {http://www.sciencedirect.com/science/article/pii/089360809190009T},
  author   = {K. Hornik},
  keywords = {Multilayer feedforward networks, Activation function, Universal approximation capabilities, Input environment measure, () approximation, Uniform approximation, Sobolev spaces, Smooth approximation},
  abstract = {We show that standard multilayer feedforward networks with as few as a single hidden layer and arbitrary bounded and nonconstant activation function are universal approximators with respect to Lp(Î¼) performance criteria, for arbitrary finite input environment measures Î¼, provided only that sufficiently many hidden units are available. If the activation function is continuous, bounded and nonconstant, then continuous mappings can be learned uniformly over compact input sets. We also give very general conditions ensuring that networks with sufficiently smooth activation functions are capable of arbitrarily accurate approximation to a function and its derivatives.}
}

@inbook{rumelhart1988mit,
  author    = {D.E. Rumelhart and G.E. Hinton and R.J. Williams},
  title     = {Learning Representations by Back-Propagating Errors},
  year      = {1988},
  publisher = {MIT press},
  booktitle = {Neurocomputing: Foundations of Research},
  pages     = {696--699}
}

@article{weise2011tog,
  title    = {Realtime performance-based facial animation},
  author   = {T. Weise and S. Bouaziz and H. Li and M. Pauly},
  journal  = {ACM transactions on graphics (TOG)},
  volume   = {30},
  number   = {4},
  year     = {2011},
  url      = {https://lgg.epfl.ch/publications/2011/RPBFA/paper.pdf},
  abstract = {This paper presents a system for performance-based character ani-mation that enables any user to control the facial expressions of adigital avatar in realtime.  The user is recorded in a natural envi-ronment using a non-intrusive, commercially available 3D sensor.The simplicity of this acquisition device comes at the cost of highnoise levels in the acquired data. To effectively map low-quality 2Dimages and 3D depth maps to realistic facial expressions, we intro-duce a novel face tracking algorithm that combines geometry andtexture registration with pre-recorded animation priors in a singleoptimization.  Formulated as a maximum a posteriori estimation ina reduced parameter space, our method implicitly exploits temporalcoherence to stabilize the tracking. We demonstrate that compelling3D facial dynamics can be reconstructed in realtime without the useof face markers, intrusive lighting, or complex scanning hardware.This makes our system easy to deploy and facilitates a range of newapplications, e.g. in digital gameplay or social interactions.}
}

@inproceedings{magistri2020iros,
  author    = {F. Magistri and N. Chebrolu and C. Stachniss},
  title     = {{Segmentation-Based 4D Registration of Plants Point Clouds for Phenotyping}},
  booktitle = iros,
  year      = {2020},
  abstract  = {Plant   phenotyping,   i.e.,   the   task   of   measuringplant  traits  to  describe  the  anatomy  and  physiology  of  plants,is  a  central  task  in  crop  science  and  plant  breeding.  Standardmethods  often  require  intrusive  or  time-consuming  operationsinvolving  a  lot  of  manual  labor.  Cameras  or  range  sensors,paired  with  3D  reconstructions  methods,  can  support  pheno-typing but the task yields several challenges in practice such asplant growth over time. In this paper, we address the problemof finding correspondences between plants recorded at differentpoints  in  time  to  track  phenotypic  traits  in  an  automatedfashion.  Our  approach  makes  use  of  semantic  segmentationand  unsupervised  clustering  to  compute  keypoints  from  plantpoint   clouds.   We   extract   a   compact   representation   of   theconsidered   scan   that   encodes   both,   topology   and   semanticinformation.  Through  our  approach,  we  are  able  to  tackle  thedata  association  problem  for  4D  point  cloud  data  of  plantseffectively.  We  tested  our  approach  on  different  3D  plus  time,i.e.,  4D,  sequences  of  plant  point  clouds  of  different  plantspecies.  The  experiments  presented  in  this  paper  suggest  thatour  4D  matching  approach  allows  for  non-rigid  registrationof  the  plants  that  change  over  time.  Moreover,  we  show  thatour  method  allows  for  tracking  different  phenotyping  traitsat  an  organ  level,  forming  a  basis  for  automated  temporalphenotyping.},
  url       = {http://www.ipb.uni-bonn.de/pdfs/magistri2020iros.pdf}
}


@incollection{ye2013tof,
  title     = {A survey on human motion analysis from depth data},
  author    = {M. Ye and Q. Zhang and L. Wang and J. Zhu and R. Yang and J. Gall},
  booktitle = {Time-of-flight and depth imaging. sensors, algorithms, and applications},
  year      = {2013},
  url       = {http://files.is.tue.mpg.de/jgall/tutorials/slides/motionanalysis_DRAFT.pdf},
  abstract  = {Human pose estimation has been actively studied for decades.While traditional approaches rely on 2d data like images or videos, thedevelopment of Time-of-Flight cameras and other depth sensors creatednew opportunities to advance the field. We give an overview of recentapproaches that perform human motion analysis which includes depth-based and skeleton-based activity recognition, head pose estimation, fa-cial feature detection, facial performance capture, hand pose estimationand  hand  gesture  recognition.  While  the  focus  is  on  approaches  usingdepth data, we also discuss traditional image based methods to providea broad overview of recent developments in these areas}
}


@inproceedings{lehnert2019iros,
  title     = {3D Move to See: Multi-perspective visual servoing towards the next best view within unstructured and occluded environments},
  author    = {C. Lehnert and D. Tsai and A. Eriksson and C. McCool},
  booktitle = iros,
  year      = {2019},
  url       = {proceedings: lehnert2019iros.pdf}
}

@article{kusumam2017jfr,
  title    = {3D-vision based detection, localization, and sizing of broccoli heads in the field},
  author   = {K. Kusumam and T. Krajn{\'\i}k and S. Pearson and T. Duckett and G. Cielniak},
  journal  = jfr,
  volume   = {34},
  number   = {8},
  year     = {2017},
  url      = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/rob.21726},
  abstract = {This paper describes a 3D vision system for robotic harvesting of broccoli using lowâcost RGBâD sensors, which was developed and evaluated using sensory data collected under realâworld field conditions in both the UK and Spain. The presented method addresses the tasks of detecting mature broccoli heads in the field and providing their 3D locations relative to the vehicle. The paper evaluates different 3D features, machine learning, and temporal filtering methods for detection of broccoli heads. Our experiments show that a combination of Viewpoint Feature Histograms, Support Vector Machine classifier, and a temporal filter to track the detected heads results in a system that detects broccoli heads with high precision. We also show that the temporal filtering can be used to generate a 3D map of the broccoli head positions in the field. Additionally, we present methods for automatically estimating the size of the broccoli heads, to determine when a head is ready for harvest. All of the methods were evaluated using groundâtruth data from both the UK and Spain, which we also make available to the research community for subsequent algorithm development and result comparison. Crossâvalidation of the system trained on the UK dataset on the Spanish dataset, and vice versa, indicated good generalization capabilities of the system, confirming the strong potential of lowâcost 3D imaging for commercial broccoli harvesting.}
}


@inproceedings{binney2009icra,
  title     = {3D tree reconstruction from laser range data},
  author    = {J. Binney and G.S. Sukhatme},
  booktitle = icra,
  year      = {2009},
  url       = {proceedings: binney2009icra.pdf},
  abstract  = {We present a method for reconstructing 3D modelsof tree branch structure from laser range data. Our approachis probabilistic, and uses a generative model of a tree to guidean  iterative  reconstruction  process.  Our  goal  is  to  recoverparameters such as branch locations, angles, radii, and lengths,as  well  as  connectivity  information  between  branches.  Theseparameters  can  then  be  fed  into  functional-structural  plantmodels  to  study  the  relationships  between  the  structure  of  aplant, its environment, and its internal biology. In this paper wepresent an algorithm for finding these parameters, and resultson  both  simulated  and  real  datasets}
}

@article{myronenko2010pami,
  title    = {Point set registration: Coherent point drift},
  author   = {A. Myronenko and X. Song},
  journal  = pami,
  volume   = {32},
  number   = {12},
  pages    = {2262--2275},
  year     = {2010},
  url      = {https://arxiv.org/pdf/0905.2635.pdf},
  abstract = {Point set registration is a key component in many computer vision tasks. The goal of point set registration is to assign correspondences between two sets of points and to recover the transformation that maps one point set to the other. Multiple factors, including an unknown nonrigid spatial transformation, large dimensionality of point set, noise, and outliers, make the point set registration a challenging problem. We introduce a probabilistic method, called the Coherent Point Drift (CPD) algorithm, for both rigid and nonrigid point set registration. We consider the alignment of two point sets as a probability density estimation problem. We fit the Gaussian mixture model (GMM) centroids (representing the first point set) to the data (the second point set) by maximizing the likelihood. We force the GMM centroids to move coherently as a group to preserve the topological structure of the point sets. In the rigid case, we impose the coherence constraint by reparameterization of GMM centroid locations with rigid parameters and derive a closed form solution of the maximization step of the EM algorithm in arbitrary dimensions. In the nonrigid case, we impose the coherence constraint by regularizing the displacement field and using the variational calculus to derive the optimal transformation. We also introduce a fast algorithm that reduces the method computation complexity to linear. We test the CPD algorithm for both rigid and nonrigid transformations in the presence of noise, outliers, and missing points, where CPD shows accurate results and outperforms current state-of-the-art methods.}
}


@inproceedings{min2019iros,
  author    = {{Min, Zhe and Pan, Jin and Zhang, Ang and Meng, Max Q.-H.}},
  title     = {Robust Non-Rigid Point Set Registration Algorithm Considering Anisotropic Uncertainties Based on Coherent Point Drift},
  booktitle = iros,
  year      = {2019},
  url       = {proceedings: min2019iros.pdf}
}


@inproceedings{loper2014eccv,
  title     = {{OpenDR: An approximate differentiable renderer}},
  author    = {M.M. Loper and M.J. Black},
  booktitle = eccv,
  year      = {2014},
  url       = {https://files.is.tue.mpg.de/black/papers/OpenDR.pdf},
  abstract  = {Inverse graphics attempts to take sensor data and infer 3Dgeometry, illumination, materials, and motions such that a graphics ren-derer could realistically reproduce the observed scene. Renderers, how-ever, are designed to solve the forward process of image synthesis. Togo in the other direction, we propose an approximatedifferentiable ren-derer (DR)that explicitly models the relationship between changes inmodel parameters and image observations. We describe a publicly avail-ableOpenDRframework that makes it easy to express a forward graph-ics model and then automatically obtain derivatives with respect to themodel parameters and to optimize over them. Built on a new auto-differentiation package and OpenGL, OpenDR provides a local optimiza-tion method that can be incorporated into probabilistic programmingframeworks. We demonstrate the power and simplicity of programmingwith OpenDR by using it to solve the problem of estimating human bodyshape from Kinect depth and RGB data.}
}

@inproceedings{mildenhall2020eccv,
  title     = {{NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis}},
  author    = {B. Mildenhall and P.P. Srinivasan and M. Tancik and J.T. Barron and R. Ramamoorthi and R. Ng},
  booktitle = eccv,
  year      = {2020},
  url       = {https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123460392.pdf},
  abstract  = {We  present  a  method  that  achieves  state-of-the-art  resultsfor synthesizing novel views of complex scenes by optimizing an under-lying  continuous  volumetric  scene  function  using  a  sparse  set  of  inputviews.  Our  algorithm  represents  a  scene  using  a  fully-connected  (non-convolutional) deep network, whose input is a single continuous 5D coor-dinate (spatial location $(x,y,z)$ and viewing direction $(\theta,\phi)$) and whoseoutput  is  the  volume  density  and  view-dependent  emitted  radiance  atthat spatial location. We synthesize views by querying 5D coordinatesalong camera rays and use classic volume rendering techniques to projectthe output colors and densities into an image. Because volume renderingis naturally differentiable, the only input required to optimize our repre-sentation is a set of images with known camera poses. We describe how toeffectively optimize neural radiance fields to render photorealistic novelviews of scenes with complicated geometry and appearance, and demon-strate results that outperform prior work on neural rendering and viewsynthesis. View synthesis results are best viewed as videos, so we urgereaders to view our supplementary video for convincing comparisons.}
}

@inproceedings{meshry2019cvpr,
  title     = {Neural rerendering in the wild},
  author    = {M. Meshry and D.B. Goldman and S. Khamis and H. Hoppe and R. Pandey and N. Snavely and R. Martin-Brualla},
  booktitle = cvpr,
  year      = {2019},
  url       = {proceedings: meshry2019cvpr.pdf},
  abstract  = {We explore total scene capture â recording, modeling,and rerendering a scene under varying appearance suchas season and time of day. Starting from internet photosof a tourist landmark, we apply traditional 3D reconstruc-tion to register the photos and approximate the scene as apoint cloud. For each photo, we render the scene pointsinto a deep framebuffer, and train a neural network to learnthe mapping of these initial renderings to the actual pho-tos. This rerendering network also takes as input a latentappearance vector and a semantic mask indicating the lo-cation of transient objects like pedestrians. The model isevaluated on several datasets of publicly available imagesspanning a broad range of illumination conditions. We cre-ate short videos demonstrating realistic manipulation of theimage viewpoint, appearance, and semantic labeling. Wealso compare results with prior work on scene reconstruc-tion from internet photos}
}


@inproceedings{kato2018cvpr,
  title     = {Neural 3D Mesh Renderer},
  author    = {H. Kato and Y. Ushiku and T. Harada},
  booktitle = cvpr,
  year      = {2018},
  url       = {proceedings: kato2018cvpr.pdf},
  abstract  = {For  modeling  the  3D  world  behind  2D  images,  which3D  representation  is  most  appropriate?   A  polygon  meshis a promising candidate for its compactness and geometricproperties.   However,  it is not straightforward to model apolygon mesh from 2D images using neural networks be-cause  the  conversion  from  a  mesh  to  an  image,  or  ren-dering,  involves  a  discrete  operation  called  rasterization,which prevents back-propagation.  Therefore, in this work,we propose an approximate gradient for rasterization thatenables the integration of rendering into neural networks.Using this renderer, we perform single-image 3D mesh re-construction with silhouette image supervision and our sys-tem outperforms the existing voxel-based approach.  Addi-tionally, we perform gradient-based 3D mesh editing opera-tions, such as 2D-to-3D style transfer and 3D DeepDream,with 2D supervision for the first time.  These applicationsdemonstrate the potential of the integration of a mesh ren-derer into neural networks and the effectiveness of our pro-posed renderer.}
}

@inproceedings{gupta2014eccv,
  author    = {Gupta, Saurabh and Girshick, Ross and Arbel{\'a}ez, Pablo and Malik, Jitendra},
  title     = {{Learning rich features from RGB-D images for object detection and segmentation}},
  booktitle = eccv,
  year      = 2014,
  url       = {https://arxiv.org/pdf/1407.5736.pdf}
}

@inproceedings{orsic2019cvpr,
  author    = {Orsic, Marin and Kreso, Ivan and Bevandic, Petra and Segvic, Sinisa},
  title     = {{In Defense of Pre-Trained ImageNet Architectures for Real-Time Semantic Segmentation of Road-Driving Images}},
  booktitle = cvpr,
  year      = {2019},
  url       = {proceedings: orsic2019cvpr.pdf}
} 

@inproceedings{sengupta2019iccv,
  title     = {Neural inverse rendering of an indoor scene from a single image},
  author    = {S. Sengupta and J. Gu and K. Kim and G. Liu and D.W. Jacobs and J. Kautz},
  booktitle = iccv,
  year      = {2019},
  url       = {proceedings: sengupta2019iccv.pdf},
  abstract  = {Inverse rendering aims to estimate physical attributes ofa scene,e.g., reflectance, geometry, and lighting, from im-age(s). Inverse rendering has been studied primarily forsingle objects or with methods that solve for only one of thescene attributes. We propose the first learning based ap-proach that jointly estimates albedo, normals, and lightingof an indoor scene from a single image. Our key contri-bution is the Residual Appearance Renderer (RAR), whichcan be trained to synthesize complex appearance effects(e.g., inter-reflection, cast shadows, near-field illumination,and realistic shading), which would be neglected otherwise.This enables us to perform self-supervised learning on realdata using a reconstruction loss, based on re-synthesizingthe input image from the estimated components. We finetunewith real data after pretraining with synthetic data. To thisend we use physically-based rendering to create a large-scale synthetic dataset, which is a significant improvementover prior datasets. Experimental results show that our ap-proach outperforms state-of-the-art methods that estimateone or more scene attributes}
}


@article{tewari2020eg,
  title    = {{State of the Art on Neural Rendering}},
  author   = {A. Tewari and O. Fried and J. Thies and V. Sitzmann and S. Lombardi and K. Sunkavalli and R. Martin-Brualla and T. Simon and J. Saragih and M. Nie{\ss}ner and others},
  journal  = {Eurographics - State-of-the-Art Reports (STARs)},
  volume   = {39},
  number   = {2},
  pages    = {701--727},
  year     = {2020},
  url      = {https://arxiv.org/pdf/2004.03805.pdf},
  abstract = {Efficient rendering of photo-realistic virtual worlds is a long standing effort of computer graphics. Modern graphics techniqueshave succeeded in synthesizing photo-realistic images from hand-crafted scene representations. However, the automatic gen-eration of shape, materials, lighting, and other aspects of scenes remains a challenging problem that, if solved, would makephoto-realistic computer graphics more widely accessible. Concurrently, progress in computer vision and machine learninghave  given  rise  to  a  new  approach  to  image  synthesis  and  editing,  namely  deep  generative  models.  Neural  rendering  is  anew and rapidly emerging field that combines generative machine learning techniques with physical knowledge from computergraphics, e.g., by the integration of differentiable rendering into network training. With a plethora of applications in computergraphics and vision, neural rendering is poised to become a new area in the graphics community, yet no survey of this emerg-ing field exists. This state-of-the-art report summarizes the recent trends and applications of neural rendering. We focus onapproaches that combine classic computer graphics techniques with deep generative models to obtain controllable and photo-realistic outputs. Starting with an overview of the underlying computer graphics and machine learning concepts, we discusscritical aspects of neural rendering approaches. Specifically, our emphasis is on the type of control, i.e., how the control isprovided, which parts of the pipeline are learned, explicit vs. implicit control, generalization, and stochastic vs. deterministicsynthesis. The second half of this state-of-the-art report is focused on the many important use cases for the described algorithmssuch as novel view synthesis, semantic photo manipulation, facial and body reenactment, relighting, free-viewpoint video, andthe creation of photo-realistic avatars for virtual and augmented reality telepresence. Finally, we conclude with a discussion ofthe social implications of such technology and investigate open research problems.}
}


@article{knapitsch2017tog,
  title    = {Tanks and temples: Benchmarking large-scale scene reconstruction},
  author   = {A. Knapitsch and J. Park and Q. Zhou and V. Koltun},
  journal  = acmgraphics,
  volume   = {36},
  number   = {4},
  pages    = {1--13},
  year     = {2017},
  url      = {http://vladlen.info/papers/tanks-and-temples.pdf},
  abstract = {We present a benchmark for image-based 3D reconstruction. Thebenchmark sequences were acquired outside the lab, in realistic con-ditions. Ground-truth data was captured using an industrial laserscanner. The benchmark includes both outdoor scenes and indoorenvironments. High-resolution video sequences are provided asinput, supporting the development of novel pipelines that take ad-vantage of video input to increase reconstruction fidelity. We reportthe performance of many image-based 3D reconstruction pipelineson the new benchmark. The results point to exciting challenges andopportunities for future work.}
}

@inproceedings{kazhdan2006eg,
  title     = {Poisson surface reconstruction},
  author    = {M. Kazhdan and M. Bolitho and H. Hoppe},
  booktitle = {Proc. of the Eurographics Symposium on Geometry Processing},
  year      = {2006},
  url       = {https://people.engr.tamu.edu/schaefer/teaching/689_Fall2006/poissonrecon.pdf},
  abstract  = {We show that surface reconstruction from oriented points can be cast as a spatial Poisson problem. This Poissonformulation considers all the points at once, without resorting to heuristic spatial partitioning or blending, andis therefore highly resilient to data noise. Unlike radial basis function schemes, our Poisson approach allows ahierarchy of locally supported basis functions, and therefore the solution reduces to a well conditioned sparselinear system. We describe a spatially adaptive multiscale algorithm whose time and space complexities are pro-portional to the size of the reconstructed model. Experimenting with publicly available scan data, we demonstratereconstruction of surfaces with greater detail than previously achievable.}
}

@article{kierdorf2022jfr,
  title   = {GrowliFlower: An image time series dataset for GROWth analysis of cauLIFLOWER},
  author  = {Kierdorf, Jana and Junker-Frohn, Laura Verena and Delaney, Mike and Olave, Mariele Donoso and Burkart, Andreas and Jaenicke, Hannah and Muller, Onno and Rascher, Uwe and Roscher, Ribana},
  journal = jfr,
  volume  = {40},
  number  = {2},
  pages   = {173--192},
  year    = {2022},
  url     = {http://rs.ipb.uni-bonn.de/wp-content/papercite-data/pdf/kierdorf2022growliflower.pdf}
}

@article{kierdorf2022fai,
  title   = {Behind the leaves: Estimation of occluded grapevine berries with conditional generative adversarial networks},
  author  = {Kierdorf, Jana and Weber, Immanuel and Kicherer, Anna and Zabawa, Laura and Drees, Lukas and Roscher, Ribana},
  journal = {Frontiers in Artificial Intelligence},
  volume  = {5},
  year    = {2022},
  pages   = {830026}
}

@article{gibbs2017plantbio,
  title    = {Approaches to three-dimensional reconstruction of plant shoot topology and geometry},
  author   = {J.A. Gibbs and M. Poundl and A.P. French and D.M. Wells and E. Murchie and T. Pridmore},
  journal  = {Functional Plant Biology},
  volume   = {44},
  number   = {1},
  pages    = {62--75},
  year     = {2017},
  abstract = {There are currently 805 million people classified as chronically undernourished,and yetthe Worldâs 10population is still increasing. At the same time,global warming is causing more frequent and severe 11flooding and drought,thus destroying crops and reducing the amount of land available for agriculture. 12Recent studies show thatwithout crop climate adaption, crop productivity will deteriorate. With access 13to 3D models of real plants it is possible to acquire detailed morphological and gross developmental 14data that can be used to study their ecophysiology,leading to an increase in crop yield and stability 15across hostile and changing environments. Here we review approaches to the reconstruction of 3D 16models of plant shoots from image data, consider current applications in plant and crop science,and 17identify remaining challenges. We conclude that although phenotyping is receiving an increasing 18amount of attentionâparticularly from computer vision researchersâand numerous vision approaches 19have been proposed, it still remains a highly interactive process. An automated system capable of 20producing 3D models of plants would significantly aid phenotyping practice, increasing accuracy and 21repeatability of measurements},
  url      = {https://nottingham-repository.worktribe.com/preview/799435/FPB.pdf}
}

@article{sirohi2021tro,
  title   = {{EfficientLPS: Efficient LiDAR Panoptic Segmentation}},
  author  = {Sirohi, Kshitij and Mohan, Rohit and B{\"u}scher, Daniel and Burgard, Wolfram and Valada, Abhinav},
  journal = tro,
  year    = 2021,
  volume  = {38},
  number  = {3},
  pages   = {1894--1914},
  url     = {proceedings: sirohi2021tro}
}

@article{golbach2016mva,
  title    = {Validation of plant part measurements using a 3D reconstruction method suitable for high-throughput seedling phenotyping},
  author   = {F. Golbach and G. Kootstra and S. Damjanovic and G. Otten and R, van de Zedde},
  journal  = {Machine Vision and Applications},
  volume   = {27},
  number   = {5},
  pages    = {663--680},
  year     = {2016},
  abstract = {In plant phenotyping, there is a demand for high-throughput, non-destructive systems that can accurately analyse various plant traits by measuring features such as plant volume, leaf area, and stem length. Existing vision-based systems either focus on speed using 2D imaging, which is consequently inaccurate, or on accuracy using time-consuming 3D methods. In this paper, we present a computer-vision system for seedling phenotyping that combines best of both approaches by utilizing a fast three-dimensional (3D) reconstruction method. We developed image processing methods for the identification and segmentation of plant organs (stem and leaf) from the 3D plant model. Various measurements of plant features such as plant volume, leaf area, and stem length are estimated based on these plant segments. We evaluate the accuracy of our system by comparing the measurements of our methods with ground truth measurements obtained destructively by hand. The results indicate that the proposed system is very promising.}
}

@inproceedings{xia2012view,
  title     = {View invariant human action recognition using histograms of 3d joints},
  author    = {L. Xia and C. Chen and J. Aggarwal},
  booktitle = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
  year      = {2012},
  url       = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.469.9489&rep=rep1&type=pdf},
  abstract  = {in  this  paper,  we  present  a  novel  approach  for  human action  recognition  with  histograms  of  3D  joint  locations (HOJ3D)  as  a  compact  representation  of  postures.  We extract  the  3D  skeletal  joint  locations  from  Kinect  depth maps  using  Shotton  et  al.âs  method  [6]. The HOJ3Dcomputed from the action depth sequences are reprojected using LDA and then clustered into k posture visual words, whichrepresent  the  prototypical  poses  of  actions.  The temporal evolutions of those visualwordsare modeled by discrete hidden Markov models (HMMs). In addition, dueto  the  design  of  our spherical coordinate  system  and  the robust  3D  skeleton  estimation  from  Kinect,  our  method demonstrates significant view invariance on our3D action dataset. Our dataset is composed of200 3D sequences of 10 indoor  activities  performed  by  10  individuals  in  varied views.  Our  method  is  real-time  and  achieves superiorresults on the challenging 3D action dataset. We also tested our  algorithm  on  the  MSR  Action3D  dataset  and  our algorithm outperforms Li et al.[25] on most ofthe cases.}
}


@inproceedings{campbell1995iccv,
  title     = {Recognition of human body motion using phase space constraints},
  author    = {L.W. Campbell and A.F. Bobick},
  booktitle = iccv,
  url       = {https://www.researchgate.net/profile/Aaron_Bobick/publication/3613062_Recognition_of_human_body_motion_using_phase_space_constraints/links/55f37b3108ae7a10cf88cea6.pdf},
  year      = {1995},
  abstract  = {A new method for representing and recognizing humanbody movements is presented. Assuming the availabil-ity of Cartesian tracking data, we develop techniquesfor representation of movements based on space curvesin subspaces of a âphase space.â  The phase space hasaxes of joint angles and torso location and attitude, andthe axes of the subspaces are subsets of the axes of thephase space.  Using this representation we develop asystem for learning new movements from ground truthdata by searching for constraints which are in  effectduring the movement to be learned, and not in effectduring other movements. We then use the learned rep-resentation for recognizing movements in data.Prior  approaches  by  other  researchers  used  a  smallnumber of classification categories, which demandedless attention to representation.  We train and test thesystem on nine fundamental movements from classicalballet performed by two dancers.  The system learnsand accurately recognizes the nine movements in  anunsegmented stream of motion.}
}

@article{zabawa2020jprs,
  title    = {Counting of grapevine berries in images via semantic segmentation using convolutional neural networks},
  author   = {L. Zabawa and A. Kicherer and L. Klingbeil and R. T{\"o}pfer and H. Kuhlmann and R. Roscher},
  journal  = jprs,
  volume   = {164},
  pages    = {73--83},
  year     = {2020},
  url      = {https://arxiv.org/pdf/2004.14010.pdf},
  abstract = {The extraction of phenotypic traits is often very time and labour intensive.  Especially the investigation in viticulture is restrictedto an on-site analysis due to the perennial nature of grapevine. Traditionally skilled experts examine small samples and extrapolatethe results to a whole plot.  Thereby different grapevine varieties and training systems, e.g.  vertical shoot positioning (VSP) andsemi minimal pruned hedges (SMPH) pose different challenges.In  this  paper  we  present  an  objective  framework  based  on  automatic  image  analysis  which  works  on  two  different  trainingsystems. The images are collected semi automatic by a camera system which is installed in a modified grape harvester. The systemproduces overlapping images from the sides of the plants.  Our framework uses a convolutional neural network to detect singleberries in images by performing a semantic segmentation. Each berry is then counted with a connected component algorithm. Wecompare our results with the Mask-RCNN, a state-of-the-art network for instance segmentation and with a regression approachfor counting.  The experiments presented in this paper show that we are able to detect green berries in images despite of differenttraining systems. We achieve an accuracy for the berry detection of 94.0\% in the VSP and 85.6\% in the SMPH.}
}

@article{tagliasacchi2015cgf,
  title    = {Robust articulated-ICP for real-time hand tracking},
  author   = {A. Tagliasacchi and M. Schr{\"o}der and A. Tkach and S. Bouaziz and M. Botsch and M. Pauly},
  journal  = {Computer Graphics Forum},
  volume   = {34},
  number   = {5},
  pages    = {101--114},
  year     = {2015},
  url      = {https://lgg.epfl.ch/publications/2015/Htrack_ICP/paper.pdf},
  abstract = {We present a robust method for capturing articulated hand motions in realtime using a single depth camera. Oursystem is based on a realtime registration process that accurately reconstructs hand poses by fitting a 3D articulatedhand model to depth images. We register the hand model using depth, silhouette, and temporal information. Toeffectively map low-quality depth maps to realistic hand poses, we regularize the registration with kinematicand temporal priors, as well as a data-driven prior built from a database of realistic hand poses. We presenta principled way of integrating such priors into our registration optimization to enable robust tracking withoutseverely restricting the freedom of motion. A core technical contribution is a new method for computing trackingcorrespondences that directly models occlusions typical of single-camera setups. To ensure reproducibility of ourresults and facilitate future research, we fully disclose the source code of our implementation.}
}
@inproceedings{langsenkamp2014cigr,
  author    = {F. Langsenkamp and F. Sellmann and M. Kohlbrecher and A. Kielhorn  and W. Strothmann and A. Michaels and A. Ruckelshausen and D. Trautz},
  booktitle = cigr,
  year      = {2014},
  title     = {Tube Stamp for mechanical intra-row individual Plant Weed Control}
}


@inproceedings{walter2018icpa,
  title     = {Flourish - A robotic approach for automation in crop management},
  author    = {A. Walter and R. Khanna and P. Lottes and C. Stachniss and R. Siegwart and J. Nieto and F. Liebisch},
  booktitle = {Proc.~of the International Conf.~on Precision Agriculture},
  year      = 2018,
  abstract  = {The Flourish project aims to bridge the gap between current and desired capabilities of agricultural robots by developing an adaptable robotic solution for precision farming. Combining the aerial survey capabilities of a small autonomous multi-copter Unmanned Aerial Vehicle (UAV) with a multi-purpose agricultural Unmanned Ground Vehicle (UGV), the system will be able to survey a field from the air, perform targeted intervention on the ground, and provide detailed information for decision support, all with minimal user intervention. The system can be adapted to a wide range of farm management activities and to different crops by choosing different sensors, status indicators and ground treatment packages. The research project thereby touches a selection of topics addressed by ICPA such as sensor application in managing in-season crop variability, precision nutrient management and crop protection as well as remote sensing applications in precision agriculture and engineering technologies and advances. This contribution will introduce the Flourish consortium and concept using the results of three years of active development, testing, and measuring in field campaigns. Two key parts of the project will be shown in more detail: First, mapping of the field by drones for detection of sugar beet nitrogen status variation and weed pressure in the field and second the perception of the UGV as related to weed classification and subsequent precision weed management. The field mapping by means of an UAV will be shown for crop nitrogen status estimation and weed pressure with examples for subsequent crop management decision support. For nitrogen status, the results indicate that drones are up to the task to deliver crop nitrogen variability maps utilized for variable rate application that are of comparable quality to current on-tractor systems. The weed pressure mapping is viable as basis for the UGV showcase of precision weed management. For this, we show the automated image acquisition by the UGV and a subsequent plant classification with a four-step pipeline, differentiating crop from weed in real time. Advantages and disadvantages as well as future prospects of such approaches will be discussed.}
}


@article{rouse1974nasa,
  title   = {{Monitoring Vegetation Systems in the Great Plains with Erts}},
  author  = {J. W. Rouse and R. H. Haas and J.~A. Schell and D. W. Deering},
  journal = {NASA Special Publication},
  year    = {1974},
  pages   = {309},
  volume  = {351},
  url     = {http://adsabs.harvard.edu/abs/1974NASSP.351..309R}
}


@inproceedings{cerutti2013icip,
  title     = {A model-based approach for compound leaves understanding and identification},
  author    = {G. Cerutti and L. Tougne and J. Mille and A. Vacavant and D. Coquin},
  booktitle = icip,
  year      = {2013}
}

@inproceedings{elhariri2014icces,
  author    = {E. Elhariri and N. El-Bendary and A. E. Hassanien},
  booktitle = icces,
  title     = {Plant classification system based on leaf features},
  year      = {2014}
}

@article{feyaerts2001prl,
  title   = {Multi-spectral vision system for weed detection},
  author  = {Feyaerts, F. and van Gool, L.},
  journal = prl,
  year    = {2001},
  pages   = {667--674},
  volume  = {22}
}

@inproceedings{mueter2013aec,
  title     = {Development of an intra-row weeding system using electric servo drives and machine vision for plant detection},
  author    = {M. M{\"u}ter and P. Schulze Lammers and L. Damerow},
  booktitle = {Proc.~of the Agricultural Engineering Conference},
  year      = {2013}
}

@article{garcia2015be,
  title   = {{Sugar beet (Beta vulgaris L.) and thistle (Cirsium arvensis L.) discrimination based on field spectral data}},
  author  = {G.~R.~F. Jose and D. Wulfsohn and J. Rasmussen},
  year    = {2015},
  volume  = {139},
  pages   = {1--15},
  journal = biosyseng
}

@article{perez2015asc,
  title   = {A semi-supervised system for weed mapping in sunflower crops using unmanned aerial vehicles and a crop row detection method},
  journal = {Applied Soft Computing},
  volume  = {37},
  pages   = {533--544},
  year    = {2015},
  author  = {M. Perez-Ortiz and J.~M. Pe{\~n}a and P.~A. Gutierrez and J. Torres-S{\'a}nchez and C. Herv{\'a}s-Mart{\'i}nez and F. L{\'o}pez-Granados}
}

@article{perez2016esa,
  title   = {Selecting patterns and features for between- and within- crop-row weed mapping using UAV-imagery},
  author  = {M. Perez-Ortiz and J.~M. Pe{\~n}a and P.~A. Gutierrez and J. Torres-S{\'a}nchez and C. Herv{\'a}s-Mart{\'i}nez and F. L{\'o}pez-Granados},
  journal = esa,
  volume  = {47},
  pages   = {85--94},
  year    = {2016}
}


@article{hamud2016cea,
  author  = {E. Hamuda and M. Glavin and E. Jones},
  title   = {A Survey of Image Processing Techniques for Plant Extraction and Segmentation in the Field},
  journal = cea,
  volume  = {125},
  year    = {2016},
  pages   = {184--199}
}

@article{pena2013po,
  author  = {J.~M. Pe{\~n}a and J. Torres-S{\'a}nchez and A.~I. de Castro and M. Kelly and F. L{\'o}pez-Granados},
  journal = plosone,
  title   = {{Weed Mapping in Early-Season Maize Fields Using Object-Based Analysis of Unmanned Aerial Vehicle UAV Images}},
  year    = {2013},
  volume  = {8}
}


@article{rumelhart1986nature,
  author   = {D.E. Rumelhart and G.E. Hinton and R.J. Williams},
  title    = {{Learning representations by back-propagating errors}},
  journal  = {Nature},
  volume   = 323,
  pages    = {533--536},
  abstract = {We describe a new learning procedure, back-propagation, for networks
              of neurone-like units. The procedure repeatedly adjusts the weights of the connections
              in the network so as to minimize a measure of the difference between the actual
              output vector of the net and the desired output vector. As a result of the weight
              adjustments, internal 'hidden' units which are not part of the input or output come to
              represent important features of the task domain, and the regularities in the task are
              captured by the interactions of these units. The ability to create useful new features
              distinguishes back-propagation from earlier, simpler methods such as the
              perceptron-convergence procedure},
  year     = 1986
}

@article{viola2001ijcv,
  author  = {P. Viola and M. Jones},
  title   = {Robust Real-time Object Detection},
  journal = ijcv,
  volume  = {57},
  pages   = {137--154},
  year    = {2001}
}

@article{leibe2008ijcv,
  author  = {B. Leibe and A. Leonardis and B. Schiele},
  title   = {Robust object detection with interleaved categorization and segmentation},
  journal = ijcv,
  volume  = {77},
  pages   = {259--289},
  year    = 2008
}

@inproceedings{leibe2005cvpr,
  author    = {B. Leibe and E. Seemann and B. Schiele},
  title     = {Pedestrian detection in crowded scenes},
  booktitle = cvpr,
  year      = 2005
}

@inproceedings{boser1992colt,
  author    = {B.E Boser and I.M. Guyon and V. Vapnik},
  title     = {{A training algorithm for optimal margin classifiers}},
  booktitle = {Proc.~of the workshop on computational learning theory (COLT)},
  year      = 1992
}

@article{freund1997jcss,
  author  = {Y. Freund and R.E. Schapire},
  title   = {{A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting}},
  journal = {Journal of Computer and System Sciences},
  volume  = 55,
  pages   = {119--139},
  year    = 1997
}

  @inproceedings{papageorgiou1998iccv,
  author    = {C.P. Papageorgiou and M. Oren and T. Poggio},
  title     = {A general framework for object detection},
  booktitle = iccv,
  year      = {1988}
}

@article{taghavi2017pm,
  author  = {S.~T. Taghavi and M.~N. Esmaeilzadeh and T.~B. Brown, Tim B. and J.~O. Borevitz},
  title   = {Deep Phenotyping: Deep Learning For Temporal Phenotype/Genotype Classification},
  year    = {2018},
  pages   = {66},
  volume  = {14},
  url     = {https://www.biorxiv.org/content/early/2017/05/04/134205},
  journal = {Plant Methods}
}

@inproceedings{oca2018icuas,
  title     = {Low-cost multispectral imaging system for crop monitoring},
  author    = {A. Montes de Oca and L. Arreola and A. Flores and J. Sanchez and G.~R. Flores},
  booktitle = {Proc.~of the International Conf.~on Unmanned Aircraft Systems (ICUAS)},
  year      = {2018}
}


@article{araus2014tps,
  title    = {Field high-throughput phenotyping: the new crop breeding frontier},
  journal  = {Trends in Plant Science},
  volume   = {19},
  number   = {1},
  pages    = {52--61},
  year     = {2014},
  url      = {http://www.sciencedirect.com/science/article/pii/S1360138513001994},
  author   = {J.~L. Araus and J.E. Cairns},
  keywords = {crop breeding, phenotyping, remote sensing, spatial variability, stress tolerance}
}

@article{rascher2011fpb,
  author  = {U. Rascher and S. Blossfeld and F. Fiorani and S. Jahnke and  M. Jansen and A.J. Kuhn and S. Matsubara and L.L.A. M{\"u}rtin and A. Merchant and R. Metzner and M. M{\"u}ller-Linow and K.A. Nagel and R. Pieruschka  and F. Pinto and C.M. Schreiber and V.M. Temperton and M.R. Thorpe and D. Van DusschoteN and E. Van Volkenburgh and C.W. Windt and U. Schurr},
  title   = {{N}on-invasive approaches for phenotyping of enhanced performance traits in bean},
  journal = {Functional plant biology},
  volume  = {38},
  pages   = {968--983},
  year    = {2011},
  url     = {http://juser.fz-juelich.de/record/17624}
}


@article{fiorani2012gcb,
  title   = {Sun-induced fluorescence - a new probe of photosynthesis: First maps from the imaging spectrometer
             HyPlant.},
  journal = {Global Change Biology},
  volume  = {21},
  number  = {12},
  pages   = {4673--4684},
  year    = {2015},
  author  = {U. Rascher, L. Alonso, A. Burkart, C. Cilia, S. Cogliati, R. Colombo, A. Damm, M. Drusch, L.
             Guanter, J. Hanus, T. HyvÃ¤rinen, T. Julitta, J. Jussila, K. Kataja, P. Kokkalis, S. Kraft, T. Kraska,
             M. Matveeva, J. Moreno, O. Muller, C. Panigada, M. Pikl, F. Pinto, L. Prey, R. Pude, M. Rossini,A. Schickling, U. Schurr, D. SchÃ¼ttemeyer, J. Verrelst, and F. Zemek }
}

@article{pinto2016pce,
  volume  = {39},
  number  = {7},
  author  = {F. Pinto and A. Damm and A. Schickling and C. Panigada and S. Cogliati and M. M{\"u}ller-Linow and A. Balvora and U. Rascher},
  title   = {Sun-induced chlorophyll fluorescence from high-resolution imaging spectroscopy data to quantify spatio-temporal patterns of photosynthetic function in crop canopies},
  journal = {Plant, Cell \& Environment},
  pages   = {1500--1512},
  year    = {2016},
  url     = {https://doi.org/10.5167/uzh-127022}
}


@article{pedersen2006pagri,
  title   = {Agricultural robotsâsystem analysis and economic feasibility},
  author  = {S.~M. Pedersen and S. Fountas and H. Have and B.~S. Blackmore},
  journal = pagri,
  year    = {2006},
  volume  = {7},
  pages   = {295--308}
}

@article{vleugels2017fcr,
  title   = {Seed yield response to N fertilization and potential of proximal sensing in Italian ryegrass seed crops},
  author  = {T. Vleugels and G. Rijckaert and R. Gislum},
  year    = {2017},
  volume  = {211},
  pages   = {37--47},
  journal = {Field Crops Research}
}

@article{walter2018nas,
  author  = {A. Walter and R. Finger and R. Huber and N. Buchmann},
  title   = {Opinion: Smart farming is key to developing sustainable agriculture},
  volume  = {114},
  number  = {24},
  pages   = {6148--6150},
  year    = {2017},
  url     = {https://www.pnas.org/content/114/24/6148.full.pdf},
  journal = {Proceedings of the National Academy of Sciences}
}

@article{bechar2017be,
  title    = {Agricultural robots for field operations. Part 2: Operations and systems},
  journal  = biosyseng,
  volume   = {153},
  pages    = {110--128},
  year     = {2017},
  author   = {A. Bechar and C. Vigneault},
  keywords = {Agricultural robots, Robotics, Field operations, Autonomous}
}

@article{duckett2018arxiv,
  author   = {T. Duckett and S. Pearson and S. Blackmore and B. Grieve and W. Chen and G. Cielniak and J. Cleaversmith and J. Dai and S. Davis and C. Fox and P. From and I. Georgilas and R. Gill and I. Gould and M. Hanheide and A. Hunter and F. Iida and L. Mihalyova and S. Nefti-Meziani and G. Neumann and P. Paoletti and T. Pridmore and D. Ross and M. Smith and M. Stoelen and M. Swainson and S. Wane and P. Wilson and I. Wright and G. Yang},
  title    = {{Agricultural Robotics: The Future of Robotic Agriculture}},
  journal  = arxiv,
  volume   = {arXiv:1806.06762},
  year     = 2018,
  url      = {http://arxiv.org/pdf/1806.06762v2},
  abstract = {Agri-Food is the largest manufacturing sector in the UK. It supports a food chain that generates over
              {\pounds}108bn p.a., with 3.9m employees in a truly international industry and exports {\pounds}20bn of
              UK manufactured goods. However, the global food chain is under pressure from population growth, climate change,
              political pressures affecting migration, population drift from rural to urban regions and the demographics of an
              aging global population. These challenges are recognised in the UK Industrial Strategy white paper and backed by
              significant investment via a Wave 2 Industrial Challenge Fund Investment
              ("Transforming Food Production: from Farm to Fork"). Robotics and Autonomous Systems (RAS) and associated digital
              technologies are now seen as enablers of this critical food chain transformation. To meet these challenges, this
              white paper reviews the state of the art in the application of RAS in Agri-Food production and explores research
              and innovation needs to ensure these technologies reach their full potential and deliver the necessary impacts
              in the Agri-Food sector.}
}

@article{roscher2016grsl,
  author  = {R. Roscher and B. Waske},
  journal = {IEEE Transactions on Geoscience and Remote Sensing},
  title   = {Shapelet-Based Sparse Representation for Landcover Classification of Hyperspectral Images},
  year    = {2016},
  volume  = {54},
  number  = {3},
  pages   = {1623--1634}
}


@article{wahabzada2016sr,
  author   = {M. Wahabzada and A. Mahlein and C. Bauckhage and U. Steiner E. Oerke and K. Kersting},
  journal  = {Scientific Reports},
  keywords = {Artificial Intelligence and Image Processing and Plant Biology},
  number   = {1},
  pages    = {22482},
  title    = {Plant Phenotyping using Probabilistic Topic Models: Uncovering the Hyperspectral Language of Plants},
  url      = {https://app.dimensions.ai/details/publication/pub.1047930706 and https://www.nature.com/articles/srep22482.pdf},
  volume   = {6},
  year     = {2016}
}

@inproceedings{tilman2011nas,
  author    = {D. Tilman and C. Balzer and J. Hill and B.~L. Befort},
  year      = {2011},
  title     = {Global food demand and the sustainable intensification of agriculture},
  booktitle = {Proc. of the National Academy of Sciences}
}


@article{godfray2010science,
  title   = {Food security: the challenge of feeding 9 billion people.},
  author  = {H.C.J. Godfray and J.R. Beddington and R.R. Crute and L. Haddad and D. Lawrence and J.F. Muir and J.N. Pretty and S. Robinson and S.M. Thomas and C. Toulmin},
  journal = {Science},
  year    = {2010},
  volume  = {327 5967},
  pages   = {812--8}
}

@inproceedings{lottes2019cba,
  title     = {{{UAV}-based Field Monitoring for Precision Farming}},
  author    = {P. Lottes and N. Chebrolu and F. Liebisch and C. Stachniss},
  booktitle = {25. Workshop Computer-Bildanalyse in der Landwirtschaft},
  year      = {2019}
}


@inproceedings{chebrolu2019icra,
  author    = {N. Chebrolu and P. Lottes and T. Laebe and C. Stachniss},
  title     = {{Robot Localization Based on Aerial Images for Precision Agriculture Tasks in Crop Fields}},
  booktitle = icra,
  year      = 2019,
  url       = {http://www.ipb.uni-bonn.de/pdfs/chebrolu2019icra.pdf},
  videourl  = {https://youtu.be/TlijLgoRLbc},
  abstract  = {Localization is a prerequisite for most autonomous robots. For example, to carry out precision agriculture tasks
               effectively, a robot must be able to localize itself accurately in crop fields. The crop field environment
               presentsunique challenges such as the highly repetitive structure ofthe crops leading to visual aliasing as
               well as the continuously changing appearance of the field, which makes it difficult tolocalize over time.
               In this paper, we present a localization system, which uses an aerial map of the field and exploits the
               semantic information of the crops, weeds, and their stem positions to resolve the visual ambiguity problem and to
               enable robot localization over extended periods of time. We evaluate our approach on a real field over
               multiple sessions spanning several weeks. Experiments suggest that our approach provides the necessary accuracy
               required by precision agriculture applications and works in cases where current techniques using
               typical visual features tend to fail.}
}

@inproceedings{albani2017iros,
  author    = {D. Albani and D. Nardi and V. Trianni},
  booktitle = iros,
  title     = {Field coverage and weed mapping by UAV swarms},
  year      = {2017}
}

@incollection{tang2018nips,
  title     = {When do random forests fail?},
  author    = {C. Tang and D. Garreau and U. von Luxburg},
  booktitle = {Advances in Neural Information Processing Systems 31},
  editor    = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
  pages     = {2983--2993},
  year      = {2018},
  url       = {http://papers.nips.cc/paper/7562-when-do-random-forests-fail.pdf}
}

@inproceedings{haug2014eccv-ws,
  title     = {A Crop/Weed Field Image Dataset for the Evaluation of Computer Vision Based Precision Agriculture Tasks},
  author    = {S. Haug and J. Ostermann},
  booktitle = {ECCV Workshops},
  year      = {2014}
}

@inproceedings{scharr2014eccv,
  title     = {{Annotated Image Datasets of Rosette Plants}},
  author    = {Hanno Scharr and Massimo Minervini and Andreas Fischbach and Sotiriois A. Tsaftaris},
  booktitle = eccv,
  year      = {2014},
  url       = {https://juser.fz-juelich.de/record/154525/files/FZJ-2014-03837.pdf}
}

@misc{plantphenotypingdataset2015ppd,
  title  = {Plant Phenotyping Datasets},
  year   = {2015},
  url    = {http://www.plant-phenotyping.org/datasets},
  author = {M. Minervini and A. Fischbach and H. Scharr and S.A. Tsaftaris}
}

@article{wu2007arxiv,
  author   = {S.G. Wu and F.S. Bao and E.Y. Xu and Y. Wang and Y. Chang and Q. Xiang},
  title    = {{A Leaf Recognition Algorithm for Plant Classification Using Probabilistic Neural Network}},
  journal  = arxiv,
  year     = 2007,
  volume   = {arXiv:0707.4289},
  url      = {http://arxiv.org/pdf/0707.4289v1},
  abstract = {In this paper, we employ Probabilistic Neural Network (PNN) with image and data processing techniques to implement a general purpose automated leaf recognition algorithm. 12 leaf features are extracted and orthogonalized into 5 principal variables which consist the input vector of the PNN. The PNN is trained by 1800 leaves to classify 32 kinds of plants with an accuracy greater than 90\%. Compared with other approaches, our algorithm is an accurate artificial intelligence approach which is fast in execution and easy in implementation.}
}

@article{pire2018arxiv,
  author   = {T. Pire and M. Mujica and J. Civera and E. Kofman},
  title    = {{The Rosario Dataset: Multisensor Data for Localization and Mapping in Agricultural Environments}},
  journal  = arxiv,
  year     = 2018,
  volume   = {arXiv:1809.06413v2},
  url      = {http://arxiv.org/pdf/1809.06413v2},
  abstract = {In this paper we present The Rosario Dataset, a collection of sensor data for autonomous mobile robotics in agricultural scenes. The dataset is motivated by the lack of realistic sensor readings gathered by a mobile robot in such environments. It consists of 6 sequences recorded in soybean fields showing real and challenging cases: highly repetitive scenes, reflection and burned images caused by direct sunlight and rough terrain among others. The dataset was conceived in order to provide a benchmark and contribute to the agricultural SLAM/odometry and sensor fusion research. It contains synchronized readings of several sensors: wheel odometry, IMU, stereo camera and a GPS-RTK system. The dataset is publicly available in http://www.cifasis-conicet.gov.ar/robot/.}
}

@article{fischer1936ae,
  title   = {The Use of Multiple Measurements in Taxonomic Problems},
  author  = {R.A. Fisher},
  journal = {Annals of Eugenics},
  year    = {1936},
  volume  = {7},
  pages   = {179--188}
}

@book{cook2013cuda,
  author    = {S. Cook},
  title     = {CUDA Programming: A Developer's Guide to Parallel Computing with GPUs},
  year      = {2013},
  edition   = {1st},
  publisher = {Morgan Kaufmann Publishers Inc.}
}

@inproceedings{Ng2009icra-ws,
  title     = {ROS: an open-source Robot Operating System},
  booktitle = {ICRA Workshop on Open Source Software},
  year      = {2009},
  url       = {http://www.willowgarage.com/sites/default/files/icraoss09-ROS.pdf},
  author    = {M. Quigley and K. Conley and B.P. Gerkey and J. Faust and T. Foote and J. Leibs and R. Wheeler and A.Y. Ng}
}

@article{chen2018arxiv,
  author   = {L. Chen and Y. Zhu and G. Papandreou and F. Schroff and H. Adam},
  title    = {{Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation}},
  journal  = arxiv,
  year     = 2018,
  volume   = {arXiv:1802.02611},
  url      = {http://arxiv.org/pdf/1802.02611v3},
  abstract = {Spatial pyramid pooling module or encode-decoder structure are used in deep neural networks for semantic segmentation task. The former networks are able to encode multi-scale contextual information by probing the incoming features with filters or pooling operations at multiple rates and multiple effective fields-of-view, while the latter networks can capture sharper object boundaries by gradually recovering the spatial information. In this work, we propose to combine the advantages from both methods. Specifically, our proposed model, DeepLabv3+, extends DeepLabv3 by adding a simple yet effective decoder module to refine the segmentation results especially along object boundaries. We further explore the Xception model and apply the depthwise separable convolution to both Atrous Spatial Pyramid Pooling and decoder modules, resulting in a faster and stronger encoder-decoder network. We demonstrate the effectiveness of the proposed model on PASCAL VOC 2012 and Cityscapes datasets, achieving the test set performance of 89.0\% and 82.1\% without any post-processing. Our paper is accompanied with a publicly available reference implementation of the proposed models in Tensorflow at \url{https://github.com/tensorflow/models/tree/master/research/deeplab}.}
}

@article{bengio2012arxiv,
  author   = {Y. Bengio},
  title    = {{Practical recommendations for gradient-based training of deep architectures}},
  journal  = arxiv,
  year     = 2012,
  volume   = {arXiv:1206.5533},
  url      = {http://arxiv.org/pdf/1206.5533v2},
  abstract = {Learning algorithms related to artificial neural networks and in particular for Deep Learning may seem to involve many bells and whistles, called hyper-parameters. This chapter is meant as a practical guide with recommendations for some of the most commonly used hyper-parameters, in particular in the context of learning algorithms based on back-propagated gradient and gradient-based optimization. It also discusses how to deal with the fact that more interesting results can be obtained when allowing one to adjust many hyper-parameters. Overall, it describes elements of the practice used to successfully and efficiently train and debug large-scale and often deep multi-layer neural networks. It closes with open questions about the training difficulties observed with deeper architectures.}
}

@inproceedings{kampen2019dgpf,
  title     = {UAV-Based Multispectral Data for Tree Species Classification and Tree Vitality Analysis},
  author    = {M. Kampen and S. Lederbauer and J. P. Mund and M. Immitzer},
  booktitle = {Dreil{\"a}ndertagung der DGPF der OVG und der SGPF},
  year      = {2019},
  url       = {https://www.dgpf.de/src/tagung/jt2019/proceedings/proceedings/papers/42_3LT2019_Kampen_et_al.pdf}
}

@article{zhao2019sensors,
  title   = {Use of Unmanned Aerial Vehicle Imagery and Deep Learning UNet to Extract Rice Lodging},
  author  = {X. Zhao and Y. Yuan and M. Song and Y. Ding and F. Lin and D. Liang and Dongyan Zhang},
  journal = sensors,
  volume  = {19},
  year    = {2019},
  number  = {18},
  url     = {https://www.mdpi.com/1424-8220/19/18/3859}
}

@phdthesis{dyrmann2017phd,
  author = {M. Dyrmann},
  title  = {Automatic Detection and Classification of Weed Seedlings under Natural Light Conditions},
  school = {University of Southern Denmark, Department of Computer Science},
  year   = 2017,
  url    = {https://pure.au.dk/portal/files/114969776/MadsDyrmannAfhandlingMedOmslag.pdf}
}


@article{bah2018arxiv,
  author   = {M.D. Bah and A. Hafiane and R. Canals},
  title    = {{Deep Learning with unsupervised data labeling for weeds detection on UAV images}},
  journal  = arxiv,
  year     = 2018,
  volume   = {arXiv:1805.12395},
  url      = {http://arxiv.org/pdf/1805.12395v1},
  abstract = {In modern agriculture, usually weeds control consists in spraying herbicides all over the agricultural field. This practice involves significant waste and cost of herbicide for farmers and environmental pollution. One way to reduce the cost and environmental impact is to allocate the right doses of herbicide at the right place and at the right time (Precision Agriculture). Nowadays, Unmanned Aerial Vehicle (UAV) is becoming an interesting acquisition system for weeds localization and management due to its ability to obtain the images of the entire agricultural field with a very high spatial resolution and at low cost. Despite the important advances in UAV acquisition systems, automatic weeds detection remains a challenging problem because of its strong similarity with the crops. Recently Deep Learning approach has shown impressive results in different complex classification problem. However, this approach needs a certain amount of training data but, creating large agricultural datasets with pixel-level annotations by expert is an extremely time consuming task. In this paper, we propose a novel fully automatic learning method using Convolutional Neuronal Networks (CNNs) with unsupervised training dataset collection for weeds detection from UAV images. The proposed method consists in three main phases. First we automatically detect the crop lines and using them to identify the interline weeds. In the second phase, interline weeds are used to constitute the training dataset. Finally, we performed CNNs on this dataset to build a model able to detect the crop and weeds in the images. The results obtained are comparable to the traditional supervised training data labeling. The accuracy gaps are 1.5\% in the spinach field and 6\% in the bean field.}
}


@article{guo2018fps,
  author  = {W. Guo and B. Zheng and A. B. Potgieter and J. Diot K. Watanabe and K. Noshita and D. R. Jordan and X. Wang and J. Watson and S. Ninomiya and S. C. Chapman},
  title   = {Aerial Imagery Analysis â Quantifying Appearance and Number of Sorghum Heads for Applications in Breeding and Agronomy},
  journal = fps,
  volume  = {9},
  pages   = {1544},
  year    = {2018},
  url     = {https://www.frontiersin.org/article/10.3389/fpls.2018.01544}
}

@article{ghosal2019pp,
  author  = {S. Ghosal and B. Zheng and S. C. Chapman and A. B. Potgieter and D. R. Jordan and X. Wang and A. K. Singh and A. Singh and M. Hirafuji and S. Ninomiya and B. Ganapathysubramanian and S. Sarkar and W.Guo},
  title   = {A Weakly Supervised Deep Learning Framework for Sorghum Head Detection and Counting},
  journal = {Plant Phenomics},
  volume  = {9},
  pages   = {1544},
  year    = {2019},
  url     = {https://www.frontiersin.org/article/10.3389/fpls.2018.01544}
}

@article{hosang2015pami,
  title   = {{What makes for effective detection proposals?}},
  author  = {Hosang, Jan and Benenson, Rodrigo and Doll{\'a}r, Piotr and Schiele, Bernt},
  journal = pami,
  year    = 2015,
  volume  = {38},
  number  = {4},
  pages   = {814--830},
  url     = {https://arxiv.org/pdf/1502.05082.pdf}
}

@inproceedings{lee2015icip,
  author    = {S. H. {Lee} and C. S. {Chan} and P. {Wilkin} and P. {Remagnino}},
  booktitle = icip,
  title     = {Deep-plant: Plant identification with convolutional neural networks},
  year      = {2015}
}

@article{yu2019fps,
  author   = {J. Yu and A. Schumann and Z. Cao and S. M. Sharpe and  N. S. Boyd},
  title    = {Weed Detection in Perennial Ryegrass With Deep Learning Convolutional Neural Network},
  journal  = fps,
  volume   = {10},
  pages    = {1422},
  year     = {2019},
  url      = {https://www.frontiersin.org/article/10.3389/fpls.2019.01422},
  abstract = {Precision herbicide application can substantially reduce herbicide input and weed control cost in turfgrass management systems. Intelligent spot-spraying system predominantly relies on machine vision-based detectors for autonomous weed control. In this work, several deep convolutional neural networks (DCNN) were constructed for detection of dandelion (Taraxacum officinale Web.), ground ivy (Glechoma hederacea L.), and spotted spurge (Euphorbia maculata L.) growing in perennial ryegrass. When the networks were trained using a dataset containing a total of 15,486 negative (images contained perennial ryegrass with no target weeds) and 17,600 positive images (images contained target weeds), VGGNet achieved high F<sub>1</sub> scores (â¥0.9278), with high recall values (â¥0.9952) for detection of E. maculata, G. hederacea, and T. officinale growing in perennial ryegrass. The F<sub>1</sub> scores of AlexNet ranged from 0.8437 to 0.9418 and were generally lower than VGGNet at detecting E. maculata, G. hederacea, and T. officinale. GoogleNet is not an effective DCNN at detecting these weed species mainly due to the low precision values. DetectNet is an effective DCNN and achieved high F<sub>1</sub> scores (â¥0.9843) in the testing datasets for detection of T. officinale growing in perennial ryegrass. Moreover, VGGNet had the highest Matthews correlation coefficient (MCC) values, while GoogleNet had the lowest MCC values. Overall, the approach of training DCNN, particularly VGGNet and DetectNet, presents a clear path toward developing a machine vision-based decision system in smart sprayers for precision weed control in perennial ryegrass.}
}


@article{boulent2019fps,
  author   = {J. Boulent and S. Foucher and J. ThÃ©au and P. St-Charles},
  title    = {Convolutional Neural Networks for the Automatic Identification of Plant Diseases},
  journal  = fps,
  volume   = {10},
  pages    = {941},
  year     = {2019},
  url      = {https://www.frontiersin.org/article/10.3389/fpls.2019.00941},
  abstract = {Deep learning techniques, and in particular Convolutional Neural Networks (CNNs), have led to significant progress in image processing. Since 2016, many applications for the automatic identification of crop diseases have been developed. These applications could serve as a basis for the development of expertise assistance or automatic screening tools. Such tools could contribute to more sustainable agricultural practices and greater food production security. To assess the potential of these networks for such applications, we survey 19 studies that relied on CNNs to automatically identify crop diseases. We describe their profiles, their main implementation aspects and their performance. Our survey allows us to identify the major issues and shortcomings of works in this research area. We also provide guidelines to improve the use of CNNs in operational contexts as well as some directions for future research.}
}


@article{olsen2019sr,
  author  = {A. Olsen and D. A. Konovalov and B. Philippa and P. Ridd and J. C. Wood and J. Johns and W. Banks and B. Girgenti and O. Kenny and  J. Whinney and B. Calvert and M. Rahimi Azghadi and R. D. White},
  title   = {{DeepWeeds: A Multiclass Weed Species Image Dataset for Deep Learning}},
  journal = {Scientific Reports},
  year    = 2019,
  volume  = 9,
  number  = 1,
  url     = {https://doi.org/10.1038/s41598-018-38343-3}
}


@article{fuentes-pacheco2019rs,
  author   = {J. Fuentes-Pacheco and J. Torres-Olivares and  E. Roman-Rangel and S. Cervantes P. and Juarez-Lopezo and J. Hermosillo-Valadez and J. M. RendÃ³n-Mancha},
  title    = {Fig Plant Segmentation from Aerial Images Using a Deep Convolutional Encoder-Decoder Network},
  journal  = rs,
  volume   = {11},
  year     = {2019},
  number   = {10},
  url      = {https://www.mdpi.com/2072-4292/11/10/1157},
  abstract = {Crop segmentation is an important task in Precision Agriculture, where the use of aerial robots with an on-board camera has contributed to the development of new solution alternatives. We address the problem of fig plant segmentation in top-view RGB (Red-Green-Blue) images of a crop grown under open-field difficult circumstances of complex lighting conditions and non-ideal crop maintenance practices defined by local farmers. We present a Convolutional Neural Network (CNN) with an encoder-decoder architecture that classifies each pixel as crop or non-crop using only raw colour images as input. Our approach achieves a mean accuracy of 93.85\% despite the complexity of the background and a highly variable visual appearance of the leaves. We make available our CNN code to the research community, as well as the aerial image data set and a hand-made ground truth segmentation with pixel precision to facilitate the comparison among different algorithms.}
}

@inproceedings{pound2017iccvws,
  author    = {M. P. {Pound} and J. A. {Atkinson} and D. M. {Wells} and T. P. {Pridmore} and A. P. {French}},
  booktitle = iccvws,
  title     = {Deep Learning for Multi-task Plant Phenotyping},
  year      = {2017},
  keywords  = {computer vision;crops;image annotation;image classification;learning (artificial intelligence);multitask plant phenotyping;computer vision;natural variability;machine learning approaches;accurately annotated images;wheat spikes;image level class annotation;deep learning approach;multitask deep architectures;crop phenotyping;ACID;Agriculture;Ear;Machine learning;Training;Image resolution;Image segmentation;Computer vision}
}

@inproceedings{newell2016eccv,
  title     = {Stacked Hourglass Networks for Human Pose Estimation},
  author    = {A. Newell and K. Yang and J. Deng},
  booktitle = eccv,
  year      = {2016},
  url       = {https://arxiv.org/pdf/1603.06937.pdf}
}


@article{lottes2019jfr,
  title    = {Robust Joint Crop-Weed Classification and Stem Detection Using Image Sequences},
  author   = {P. Lottes and J. Behley and N. Chebrolu and A. Milioto and C. Stachniss},
  journal  = jfr,
  volume   = {37},
  number   = {1},
  pages    = {20--34},
  keywords = {agricultural robotics, machine learning, plant classification, precision farming},
  url      = {https://onlinelibrary.wiley.com/doi/abs/10.1002/rob.21901},
  abstract = {Abstract Conventional farming still relies on large quantities of agrochemicals for weed management which have several negative side-effects on the environment. Autonomous robots offer the potential to reduce the amount of chemicals applied, as robots can monitor and treat each plant in the field individually and thereby circumventing the uniform chemical treatment of the whole field. Such agricultural robots need the ability to identify individual crops and weeds in the field using sensor data and must additionally select effective treatment methods based on the type of weed. For example, certain types of weeds can only be effectively treated mechanically due to their resistance to herbicides, whereas other types can be treated trough selective spraying. In this article, we present a novel system that provides the necessary information for effective plant-specific treatment. It estimates the stem location for weeds, which enables the robots to perform precise mechanical treatment, and at the same time provides the pixel-accurate area covered by weeds for treatment through selective spraying. The major challenge in developing such a system is the large variability in the visual appearance that occurs in different fields. Thus, an effective classification system has to robustly handle substantial environmental changes including varying weed pressure, various weed types, different growth stages, changing visual appearance of the plants and the soil. Our approach uses an end-to-end trainable fully convolutional network that simultaneously estimates plant stem positions as well as the spatial extent of crop plants and weeds. It jointly learns how to detect the stems and the pixel-wise semantic segmentation and incorporates spatial information by considering image sequences of local field strips. The jointly learned feature representation for both tasks furthermore exploits the crop arrangement information that is often present in crop fields. This information is considered even if it is only observable from the image sequences and not a single image. Such image sequences, as typically provided by robots navigating over the field along crop rows, enable our approach to robustly estimate the semantic segmentation and stem positions despite the large variations encountered in different fields. We implemented and thoroughly tested our approach on images from multiple farms in different countries. The experiments show that our system generalizes well to previously unseen fields under varying environmental conditionsâa key capability to deploy such systems in the real world. Compared to state-of-the-art approaches, our approach generalizes well to unseen fields and not only substantially improves the stem detection accuracy, that is, distinguishing crop and weed stems, but also improves the semantic segmentation performance.},
  year     = {2020}
}

@article{bosilj2020jfr,
  author   = {P. Bosilj and E. Aptoula and T. Duckett and G. Cielniak},
  title    = {Transfer learning between crop types for semantic segmentation of crops versus weeds in precision agriculture},
  journal  = jfr,
  volume   = {37},
  number   = {1},
  pages    = {7--19},
  keywords = {agricultural robotics, deep learning, precision agriculture, vegetation classification},
  url      = {https://onlinelibrary.wiley.com/doi/abs/10.1002/rob.21869},
  abstract = {Abstract Agricultural robots rely on semantic segmentation for distinguishing between crops and weeds to perform selective treatments and increase yield and crop health while reducing the amount of chemicals used. Deep-learning approaches have recently achieved both excellent classification performance and real-time execution. However, these techniques also rely on a large amount of training data, requiring a substantial labeling effort, both of which are scarce in precision agriculture. Additional design efforts are required to achieve commercially viable performance levels under varying environmental conditions and crop growth stages. In this paper, we explore the role of knowledge transfer between deep-learning-based classifiers for different crop types, with the goal of reducing the retraining time and labeling efforts required for a new crop. We examine the classification performance on three datasets with different crop types and containing a variety of weeds and compare the performance and retraining efforts required when using data labeled at pixel level with partially labeled data obtained through a less time-consuming procedure of annotating the segmentation output. We show that transfer learning between different crop types is possible and reduces training times for up to 80\%. Furthermore, we show that even when the data used for retraining are imperfectly annotated, the classification performance is within 2\% of that of networks trained with laboriously annotated pixel-precision data.},
  year     = {2020}
}

@article{chechlinski2019sensors,
  title    = {A System for Weeds and Crops IdentificationâReaching over 10 FPS on Raspberry Pi with the Usage of MobileNets, DenseNet and Custom Modifications},
  author   = {L. ChechliÅski and B. SiemiÄtkowska and M. Majewski},
  journal  = sensors,
  volume   = {19},
  year     = {2019},
  number   = {17},
  abstract = {Automated weeding is an important research area in agrorobotics. Weeds can be removed mechanically or with the precise usage of herbicides. Deep Learning techniques achieved state of the art results in many computer vision tasks, however their deployment on low-cost mobile computers is still challenging. The described system contains several novelties, compared both with its previous version and related work. It is a part of a project of the automatic weeding machine, developed by the Warsaw University of Technology and MCMS Warka Ltd. Obtained models reach satisfying accuracy (detecting 47--67\% of weed area, misclasifing as weed 0.1&ndash;0.9\% of crop area) at over 10 FPS on the Raspberry Pi 3B+ computer. It was tested for four different plant species at different growth stadiums and lighting conditions. The system performing semantic segmentation is based on Convolutional Neural Networks. Its custom architecture combines U-Net, MobileNets, DenseNet and ResNet concepts. Amount of needed manual ground truth labels was significantly decreased by the usage of the knowledge distillation process, learning final model which mimics an ensemble of complex models on a large database of unlabeled data. Further decrease of the inference time was obtained by two custom modifications: in the usage of separable convolutions in DenseNet block and in the number of channels in each layer. In the authors&rsquo; opinion, the described novelties can be easily transferred to other agrorobotics tasks.}
}


@article{teimouri2018sensors,
  author   = {N. Teimouri and M. Dyrmann and P. R. Nielsen and S. K. Mathiassen and G. J. Somerville and R. N. JÃ¸rgensen},
  title    = {Weed Growth Stage Estimator Using Deep Convolutional Neural Networks},
  journal  = sensors,
  volume   = {18},
  year     = {2018},
  number   = {5},
  url      = {https://www.mdpi.com/1424-8220/18/5/1580},
  abstract = {This study outlines a new method of automatically estimating weed species and growth stages (from cotyledon until eight leaves are visible) of in situ images covering 18 weed species or families. Images of weeds growing within a variety of crops were gathered across variable environmental conditions with regards to soil types, resolution and light settings. Then, 9649 of these images were used for training the computer, which automatically divided the weeds into nine growth classes. The performance of this proposed convolutional neural network approach was evaluated on a further set of 2516 images, which also varied in term of crop, soil type, image resolution and light conditions. The overall performance of this approach achieved a maximum accuracy of 78\% for identifying Polygonum spp. and a minimum accuracy of 46\% for blackgrass. In addition, it achieved an average 70\% accuracy rate in estimating the number of leaves and 96\% accuracy when accepting a deviation of two leaves. These results show that this new method of using deep convolutional neural networks has a relatively high ability to estimate early growth stages across a wide variety of weed species.}
}


@article{bullock2019arxiv,
  author   = {D. Bullock and A. Mangeni and T. Wiesner-Hanks and C. DeChant and E.L. Stewart and N. Kaczmar and J.M. Kolkman and R.J. Nelson and M.A. Gore and H. Lipson},
  title    = {{Automated Weed Detection in Aerial Imagery with Context}},
  journal  = arxiv,
  year     = 2019,
  volume   = {arXiv:1910.00652},
  url      = {http://arxiv.org/pdf/1910.00652v3},
  abstract = {In this paper, we demonstrate the ability to discriminate between cultivated maize plant and grass or grass-like weed image segments using the context surrounding the image segments. While convolutional neural networks have brought state of the art accuracies within object detection, errors arise when objects in different classes share similar features. This scenario often occurs when objects in images are viewed at too small of a scale to discern distinct differences in features, causing images to be incorrectly classified or localized. To solve this problem, we will explore using context when classifying image segments. This technique involves feeding a convolutional neural network a central square image along with a border of its direct surroundings at train and test times. This means that although images are labelled at a smaller scale to preserve accurate localization, the network classifies the images and learns features that include the wider context. We demonstrate the benefits of this context technique in the object detection task through a case study of grass (foxtail) and grass-like (yellow nutsedge) weed detection in maize fields. In this standard situation, adding context alone nearly halved the error of the neural network from 7.1\% to 4.3\%. After only one epoch with context, the network also achieved a higher accuracy than the network without context did after 50 epochs. The benefits of using the context technique are likely to particularly evident in agricultural contexts in which parts (such as leaves) of several plants may appear similar when not taking into account the context in which those parts appear.}
}

@article{mccool2018ee,
  author   = {C. McCool and J. Beattie and M. Milford and J. D. Bakker and J. L. Moore and  J. Firn},
  title    = {Automating analysis of vegetation with computer vision: Cover estimates and classification},
  journal  = {Ecology and Evolution},
  volume   = {8},
  number   = {12},
  pages    = {6005--6015},
  keywords = {automation, computer vision, image analysis, visual cover estimate},
  url      = {https://onlinelibrary.wiley.com/doi/abs/10.1002/ece3.4135},
  abstract = {Abstract This study develops an approach to automating the process of vegetation cover estimates using computer vision and pattern recognition algorithms. Visual cover estimation is a key tool for many ecological studies, yet quadrat-based analyses are known to suffer from issues of consistency between people as well as across sites (spatially) and time (temporally). Previous efforts to estimate cover from photograps require considerable manual work. We demonstrate that an automated system can be used to estimate vegetation cover and the type of vegetation cover present using topâdown photographs of 1Â m by 1Â m quadrats. Vegetation cover is estimated by modelling the distribution of color using a multivariate Gaussian. The type of vegetation cover is then classified, using illumination robust local binary pattern features, into two broad groups: graminoids (grasses) and forbs. This system is evaluated on two datasets from the globally distributed experiment, the Nutrient Network (NutNet). These NutNet sites were selected for analyses because repeat photographs were taken over time and these sites are representative of very different grassland ecosystemsâa low stature subalpine grassland in an alpine region of Australia and a higher stature and more productive lowland grassland in the Pacific Northwest of the USA. We find that estimates of treatment effects on grass and forb cover did not differ between field and automated estimates for eight of nine experimental treatments. Conclusions about total vegetation cover did not correspond quite as strongly, particularly at the more productive site. A limitation with this automated system is that the total vegetation cover is given as a percentage of pixels considered to contain vegetation, but ecologists can distinguish species with overlapping coverage and thus can estimate total coverage to exceed 100\%. Automated approaches such as this offer techniques for estimating vegetation cover that are repeatable, cheaper to use, and likely more reliable for quantifying changes in vegetation over the long-term. These approaches would also enable ecologists to increase the spatial and temporal depth of their coverage estimates with methods that allow for vegetation sampling over large spatial scales quickly.},
  year     = {2018}
}

@article{langer2018arxiv,
  author  = {F. Langer and L. Mandtler and A. Milioto and E. Palazzolo and C. Stachniss},
  title   = {{Geometrical Stem Detection from Image Data for Precision Agriculture}},
  journal = arxiv,
  year    = 2018,
  volume  = {arXiv:1812.05415},
  url     = {http://arxiv.org/pdf/1812.05415v1}
}

@phdthesis{karpathy2015phd,
  author = {A. Karpathy},
  title  = {{Connecting images and natural language}},
  school = {University of Stanford},
  year   = 2016,
  url    = {https://cs.stanford.edu/people/karpathy/main.pdf}
}

@article{wu2020jfr,
  title   = {Robotic Weed Control Using Automated Weed and Crop Classification},
  author  = {X. Wu and S. Aravecchia and P. Lottes and C. Stachniss and C. Pradalier},
  journal = jfr,
  year    = {2020},
  volume  = {37},
  number  = {2},
  pages   = {322--340},
  url     = {http://www.ipb.uni-bonn.de/pdfs/wu2020jfr.pdf}
}


@inproceedings{gogoll2020iros,
  author    = {D. Gogoll and P. Lottes and J. Weyler and N. Petrinic and C. Stachniss},
  title     = {{Unsupervised Domain Adaptation for Transferring Plant Classification Systems to New Field Environments, Crops, and Robots}},
  booktitle = iros,
  year      = {2020},
  url       = {http://www.ipb.uni-bonn.de/pdfs/gogoll2020iros.pdf}
}

@inproceedings{zhu2017iccv,
  author    = {Jun-Yan Zhu and Taesung Park and Phillip Isola and Alexei A. Efros},
  title     = {{Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks}},
  booktitle = iccv,
  year      = 2017,
  url       = {https://arxiv.org/pdf/1703.10593.pdf}
}

@article{brown2020arxiv,
  title   = {{Modeling and Prediction of Human Driver Behavior: A Survey}},
  author  = {K. Brown and K. Driggs-Campbell and M.J. Kochenderfer},
  year    = {2020},
  journal = arxiv,
  url     = {https://arxiv.org/pdf/2006.08832},
  volume  = {arXiv:2006.08832}
}

@inproceedings{yu2016iclr,
  title     = {{Multi-Scale Context Aggregation by Dilated Convolutions}},
  author    = {F. Yu and V. Koltun},
  booktitle = iclr,
  year      = {2016},
  url       = {https://arxiv.org/pdf/1511.07122.pdf}
}

@inproceedings{schulz2018iros,
  abstract  = {Planning for autonomous driving in complex, urban scenarios requires accurate prediction of the trajectories of surrounding traffic participants. Their future behavior depends on their route intentions, the road-geometry, traffic rules and mutual interaction, resulting in interdependencies between their trajectories. We present a probabilistic prediction framework based on a dynamic Bayesian network, which represents the state of the complete scene including all agents and respects the aforementioned dependencies. We propose Markovian, context-dependent motion models to define the interaction-aware behavior of drivers. At first, the state of the dynamic Bayesian network is estimated over time by tracking the single agents via sequential Monte Carlo inference. Secondly, we perform a probabilistic forward simulation of the network's estimated belief state to generate the different combinatorial scene developments. This provides the corresponding trajectories for the set of possible, future scenes. Our framework can handle various road layouts and number of traffic participants. We evaluate the approach in online simulations and real-world scenarios. It is shown that our interaction-aware prediction outperforms interaction-unaware physics- and map-based approaches.},
  author    = {J. Schulz and C. Hubmann and J. Lochner and D. Burschka},
  booktitle = iros,
  url       = {https://arxiv.org/pdf/1804.10467.pdf},
  title     = {{Interaction-Aware Probabilistic Behavior Prediction in Urban Environments}},
  year      = {2018}
}

@inproceedings{sun2019iv,
  abstract  = {Autonomous cars have to navigate in dynamic environment which can be full of uncertainties. The uncertainties can come either from sensor limitations such as occlusions and limited sensor range, or from probabilistic prediction of other road participants, or from unknown social behavior in a new area. To safely and efficiently drive in the presence of these uncertainties, the decision-making and planning modules of autonomous cars should intelligently utilize all available information and appropriately tackle the uncertainties so that proper driving strategies can be generated. In this paper, we propose a social perception scheme which treats all road participants as distributed sensors in a sensor network. By observing the individual behaviors as well as the group behaviors, uncertainties of the three types can be updated uniformly in a belief space. The updated beliefs from the social perception are then explicitly incorporated into a probabilistic planning framework based on Model Predictive Control (MPC). The cost function of the MPC is learned via inverse reinforcement learning (IRL). Such an integrated probabilistic planning module with socially enhanced perception enables the autonomous vehicles to generate behaviors which are defensive but not overly conservative, and socially compatible. The effectiveness of the proposed framework is verified in simulation on an representative scenario with sensor occlusions.},
  author    = {L. Sun and W. Zhan and C.Y. Chan and M. Tomizuka},
  booktitle = iv,
  url       = {https://arxiv.org/pdf/1905.00988.pdf},
  title     = {{Behavior Planning of Autonomous Cars with Social Perception}},
  year      = {2019}
}

@article{lefevre2014robo,
  abstract = {With the objective to improve road safety, the automotive industry is moving toward more âintelligentâ vehicles. One of the major challenges is to detect dangerous situations and react accordingly in order to avoid or mitigate accidents. This requires predicting the likely evolution of the current traffic situation, and assessing how dangerous that future situation might be. This paper is a survey of existing methods for motion prediction and risk assessment for intelligent vehicles. The proposed classification is based on the semantics used to define motion and risk. We point out the tradeoff between model completeness and real-time constraints, and the fact that the choice of a risk assessment method is influenced by the selected motion model.},
  author   = {S. Lef{\`{e}}vre and D. Vasquez and C. Laugier},
  journal  = {Journal of Robotics and Mechanical Engineering Research},
  volume   = {1},
  pages    = {1--14},
  url      = {https://hal.inria.fr/hal-01053736/document},
  title    = {{A survey on motion prediction and risk assessment for intelligent vehicles}},
  year     = {2014}
}

@inproceedings{tang2019neurips,
  title     = {{Multiple Futures Prediction}},
  author    = {Y. Tang and R. Salakhutdinov},
  booktitle = neurips,
  year      = {2019},
  url       = {https://papers.nips.cc/paper/2019/file/86a1fa88adb5c33bd7a68ac2f9f3f96b-Paper.pdf}
}

@inproceedings{kingma2015iclr,
  author    = {D.P. Kingma and J. Ba},
  title     = {{Adam: {A} Method for Stochastic Optimization}},
  booktitle = iclr,
  url       = {https://arxiv.org/pdf/1412.6980.pdf},
  year      = {2015}
}

@article{mozaffari2020arxiv,
  title   = {{Deep Learning-Based Vehicle Behavior Prediction for Autonomous Driving Applications: A Review}},
  journal = arxiv,
  author  = {S. Mozaffari and O.Y. Al-Jarrah and M. Dianati and P. Jennings and A. Mouzakitis},
  year    = {2019},
  url     = {https://arxiv.org/pdf/1912.11676.pdf},
  volume  = {arXiv:1912.11676}
}

@inproceedings{xin2018itsc,
  author    = {L. Xin and P. Wang and C.Y. Chan and J. Chen and S.E. Li and B. Cheng},
  booktitle = itsc,
  title     = {{Intention-aware Long Horizon Trajectory Prediction of Surrounding Vehicles using Dual LSTM Networks}},
  year      = {2018},
  url       = {https://arxiv.org/pdf/1906.02815.pdf},
  abstract  = {As autonomous vehicles (AVs) need to interact with other road users, it is of importance to comprehensively understand the dynamic traffic environment, especially the future possible trajectories of surrounding vehicles. This paper presents an algorithm for long-horizon trajectory prediction of surrounding vehicles using a dual long short term memory (LSTM) network, which is capable of effectively improving prediction accuracy in strongly interactive driving environments. In contrast to traditional approaches which require trajectory matching and manual feature selection, this method can automatically learn high-level spatial-temporal features of driver behaviors from naturalistic driving data through sequence learning. By employing two blocks of LSTMs, the proposed method feeds the sequential trajectory to the first LSTM for driver intention recognition as an intermediate indicator, which is immediately followed by a second LSTM for future trajectory prediction. Test results from real-world highway driving data show that the proposed method can, in comparison to state-of-art methods, output more accurate and reasonable estimate of different future trajectories over 5s time horizon with root mean square error (RMSE) for longitudinal and lateral prediction less than 5.77m and 0.49m, respectively.}
}

@inproceedings{park2018iv,
  abstract  = {In this paper, we propose a deep learning based vehicle trajectory prediction technique which can generate the future trajectory sequence of surrounding vehicles in real time. We employ the encoder-decoder architecture which analyzes the pattern underlying in the past trajectory using the long short-term memory (LSTM) based encoder and generates the future trajectory sequence using the LSTM based decoder. This structure produces the K most likely trajectory candidates over occupancy grid map by employing the beam search technique which keeps the K locally best candidates from the decoder output. The experiments conducted on highway traffic scenarios show that the prediction accuracy of the proposed method is significantly higher than the conventional trajectory prediction techniques.},
  author    = {S.H. Park and B. Kim and C.M. Kang and C.C. Chung and J.W. Choi},
  booktitle = iv,
  title     = {{Sequence-to-Sequence Prediction of Vehicle Trajectory via LSTM Encoder-Decoder Architecture}},
  year      = {2018},
  url       = {https://arxiv.org/pdf/1802.06338.pdf}
}

@inproceedings{deo2018iv,
  abstract  = {To safely and efficiently navigate through complex traffic scenarios, autonomous vehicles need to have the ability to predict the future motion of surrounding vehicles. Multiple interacting agents, the multi-modal nature of driver behavior, and the inherent uncertainty involved in the task make motion prediction of surrounding vehicles a challenging problem. In this paper, we present an LSTM model for interaction aware motion prediction of surrounding vehicles on freeways. Our model assigns confidence values to maneuvers being performed by vehicles and outputs a multi-modal distribution over future motion based on them. We compare our approach with the prior art for vehicle motion prediction on the publicly available NGSIM US-101 and I-80 datasets. Our results show an improvement in terms of RMS values of prediction error. We also present an ablative analysis of the components of our proposed model and analyze the predictions made by the model in complex traffic scenarios.},
  author    = {N. Deo and M.M. Trivedi},
  booktitle = iv,
  title     = {{Multi-Modal Trajectory Prediction of Surrounding Vehicles with Maneuver based LSTMs}},
  url       = {https://arxiv.org/pdf/1805.05499.pdf},
  year      = {2018}
}

@article{dai2019acce,
  abstract = {The long short-term memory (LSTM) model is one of the most commonly used vehicle trajectory predicting models. In this paper, we study two problems of the existing LSTM models for long-term trajectory prediction in dense traffic. First, the existing LSTM models cannot simultaneously describe the spatial interactions between different vehicles and the temporal relations between the trajectory time series. Thus, the existing models cannot accurately estimate the influence of the interactions in dense traffic. Second, the basic LSTM models often suffer from vanishing gradient problem and are, thus, hard to train for long time series. These two problems sometimes lead to large prediction errors in vehicle trajectory predicting. In this paper, we proposed a spatiooral LSTM-based trajectory prediction model (ST-LSTM) which includes two modifications. We embed spatial interactions into LSTM models to implicitly measure the interactions between neighboring vehicles. We also introduce shortcut connections between the inputs and the outputs of two consecutive LSTM layers to handle gradient vanishment. The proposed new model is evaluated on the I-80 and US-101 datasets. Results show that our new model has a higher trajectory predicting accuracy than one state-of-the-art model [maneuver-LSTM (M-LSTM)].},
  author   = {S. Dai and L. Li and Z. Li},
  journal  = {IEEE Access},
  pages    = {38287--38296},
  volume   = {7},
  title    = {{Modeling Vehicle Interactions via Modified LSTM Models for Trajectory Prediction}},
  url      = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8672889},
  year     = {2019}
}

@inproceedings{cui2021iros,
  author    = {L. Cui and C. Rong and J. Huang and A. Rosendo and L. Kneip},
  title     = {{Monte-Carlo Localization in Underground Parking Lots Using Parking Slot Numbers}},
  booktitle = iros,
  year      = 2021,
  url       = {proceedings: cui2021iros.pdf}
}

@inproceedings{cui2019icra,
  title     = {{Multimodal Trajectory Predictions for Autonomous Driving using Deep Convolutional Networks}},
  author    = {H. Cui and V. Radosavljevic and F.C. Chou and T.H. Lin and T. Nguyen and T.K. Huang and J. Schneider and N. Djuric},
  booktitle = icra,
  year      = {2019},
  url       = {https://arxiv.org/pdf/1809.10732.pdf}
}

@inproceedings{casas2018corl,
  author    = {S. Casas and W. Luo and R. Urtasun},
  booktitle = corl,
  url       = {http://www.cs.toronto.edu/~wenjie/papers/intentnet_corl18.pdf},
  title     = {{IntentNet: Learning to Predict Intention from Raw Sensor Data}},
  year      = {2018}
}

@inproceedings{li2020iros,
  author    = {L.L. Li and B. Yang and M. Liang and W. Zeng and M. Ren},
  booktitle = iros,
  title     = {{End-to-end Contextual Perception and Prediction with Interaction Transformer}},
  year      = {2020},
  url       = {https://arxiv.org/pdf/2008.05927}
}

@inproceedings{deo2018cvprws,
  author    = {N. Deo and M.M. Trivedi},
  booktitle = cvprws,
  title     = {{Convolutional Social Pooling for Vehicle Trajectory Prediction}},
  year      = {2018},
  url       = {https://arxiv.org/pdf/1805.06771}
}

@article{zyner2018tits,
  abstract = {Understanding the intentions of drivers at intersections is a critical component for autonomous vehicles. Urban intersections that do not have traffic signals are a common epicenter of highly variable vehicle movement and interactions. We present a method for predicting driver intent at urban intersections through multi-modal trajectory prediction with uncertainty. Our method is based on recurrent neural networks combined with a mixture density network output layer. To consolidate the multi-modal nature of the output probability distribution, we introduce a clustering algorithm that extracts the set of possible paths that exist in the prediction output and ranks them according to probability. To verify the method's performance and generalizability, we present a real-world dataset that consists of over 23 000 vehicles traversing five different intersections, collected using a vehicle-mounted lidar-based tracking system. An array of metrics is used to demonstrate the performance of the model against several baselines.},
  author   = {A. Zyner and S. Worrall and E. Nebot},
  journal  = tits,
  volume   = {21},
  number   = {4},
  url      = {https://arxiv.org/pdf/1807.09995.pdf},
  pages    = {1584--1594},
  title    = {{Naturalistic Driver Intention and Path Prediction Using Recurrent Neural Networks}},
  year     = {2018}
}

@inproceedings{ding2019icra,
  abstract  = {Anticipating possible behaviors of traffic participants is an essential capability of autonomous vehicles. Many behavior detection and maneuver recognition methods only have a very limited prediction horizon that leaves inadequate time and space for planning. To avoid unsatisfactory reactive decisions, it is essential to count long-term future rewards in planning, which requires extending the prediction horizon. In this paper, we uncover that clues to vehicle behaviors over an extended horizon can be found in vehicle interaction, which makes it possible to anticipate the likelihood of a certain behavior, even in the absence of any clear maneuver pattern. We adopt a recurrent neural network (RNN) for observation encoding, and based on that, we propose a novel vehicle behavior interaction network (VBIN) to capture the vehicle interaction from the hidden states and connection feature of each interaction pair. The output of our method is a probabilistic likelihood of multiple behavior classes, which matches the multimodal and uncertain nature of the distant future. A systematic comparison of our method against two state-of-the-art methods and another two baseline methods on a publicly available real highway dataset is provided, showing that our method has superior accuracy and advanced capability for interaction modeling.},
  author    = {W. Ding and J. Chen and S. Shen},
  booktitle = icra,
  url       = {https://arxiv.org/pdf/1903.00848.pdf},
  title     = {{Predicting Vehicle Behaviors Over An Extended Horizon Using Behavior Interaction Network}},
  year      = {2019}
}

@inproceedings{luo2018cvpr,
  author    = {W. Luo and B. Yang and R. Urtasun},
  booktitle = cvprold,
  title     = {{Fast and Furious: Real Time End-to-End 3D Detection, Tracking and Motion Forecasting with a Single Convolutional Net}},
  year      = {2018},
  url       = {https://openaccess.thecvf.com/content_cvpr_2018/papers/Luo_Fast_and_Furious_CVPR_2018_paper.pdf}
}

@inproceedings{zhao2019cvpr,
  abstract  = {Accurate prediction of others' trajectories is essential for autonomous driving. Trajectory prediction is challenging because it requires reasoning about agents' past movements, social interactions among varying numbers and kinds of agents, constraints from the scene context, and the stochasticity of human behavior. Our approach models these interactions and constraints jointly within a novel Multi-Agent Tensor Fusion (MATF) network. Specifically, the model encodes multiple agents' past trajectories and the scene context into a Multi-Agent Tensor, then applies convolutional fusion to capture multiagent interactions while retaining the spatial structure of agents and the scene context. The model decodes recurrently to multiple agents' future trajectories, using adversarial loss to learn stochastic predictions. Experiments on both highway driving and pedestrian crowd datasets show that the model achieves state-of-the-art prediction accuracy.},
  author    = {T. Zhao and Y. Xu and M. Monfort and W. Choi and C. Baker and Y. Zhao and Y. Wang and Y.N. Wu},
  booktitle = cvpr,
  url       = {https://arxiv.org/pdf/1904.04776.pdf},
  title     = {{Multi-Agent Tensor Fusion for Contextual Trajectory Prediction}},
  year      = {2019}
}

@inproceedings{gupta2018cvpr,
  abstract  = {Understanding human motion behavior is critical for autonomous moving platforms (like self-driving cars and social robots) if they are to navigate human-centric environments. This is challenging because human motion is inherently multimodal: given a history of human motion paths, there are many socially plausible ways that people could move in the future. We tackle this problem by combining tools from sequence prediction and generative adversarial networks: a recurrent sequence-to-sequence model observes motion histories and predicts future behavior, using a novel pooling mechanism to aggregate information across people. We predict socially plausible futures by training adversarially against a recurrent discriminator, and encourage diverse predictions with a novel variety loss. Through experiments on several datasets we demonstrate that our approach outperforms prior work in terms of accuracy, variety, collision avoidance, and computational complexity.},
  author    = {A. Gupta and J. Johnson and L. Fei-Fei and S. Savarese and A. Alahi},
  booktitle = cvprold,
  url       = {https://arxiv.org/pdf/1803.10892.pdf},
  title     = {{Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks}},
  year      = {2018}
}

@inproceedings{driggs-campbell2015itsc,
  abstract  = {In light of growing attention of intelligent vehicle systems, we propose developing a driver model that uses a hybrid system formulation to capture the intent of the driver. This model hopes to capture human driving behavior in a way that can be utilized by semi-and fully autonomous systems in heterogeneous environments. We consider a discrete set of high level goals or intent modes, that is designed to encompass the decision making process of the human. A driver model is derived using a dataset of lane changes collected in a realistic driving simulator, in which the driver actively labels data to give us insight into her intent. By building the labeled dataset, we are able to utilize classification tools to build the driver model using features of based on her perception of the environment, and achieve high accuracy in identifying driver intent. Multiple algorithms are presented and compared on the dataset, and a comparison of the varying behaviors between drivers is drawn. Using this modeling methodology, we present a model that can be used to assess driver behaviors and to develop human-inspired safety metrics that can be utilized in intelligent vehicular systems.},
  author    = {K. Driggs-Campbell and R. Bajcsy},
  booktitle = itsc,
  url       = {https://arxiv.org/pdf/1505.05921.pdf},
  title     = {{Identifying Modes of Intent from Driver Behaviors in Dynamic Environments}},
  year      = {2015}
}

@inproceedings{chung2014nipsws,
  title     = {{Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling}},
  author    = {J. Chung and C. Gulcehre and K. Cho and Y. Bengio},
  year      = {2014},
  url       = {https://arxiv.org/pdf/1412.3555.pdf},
  booktitle = nipsws
}

@article{hochreiter1997neuralcomputation,
  title   = {{Long Short-Term Memory}},
  author  = {S. Hochreiter and J. Schmidhuber},
  journal = {Neural Computation},
  volume  = {9},
  number  = {8},
  url     = {https://www.bioinf.jku.at/publications/older/2604.pdf},
  year    = {1997},
  pages   = {1735--1780}
}

@inproceedings{alahi2016cvpr,
  author    = {A. Alahi and K. Goel and V. Ramanathan and A. Robicquet and L. Fei-Fei and S. Savarese},
  booktitle = cvprold,
  title     = {{Social LSTM: Human Trajectory Prediction in Crowded Spaces}},
  year      = {2016},
  url       = {https://cvgl.stanford.edu/papers/CVPR16_Social_LSTM.pdf}
}

 @inproceedings{kim2017itsc,
  title     = {{Probabilistic Vehicle Trajectory Prediction over Occupancy Grid Map via Recurrent Neural Network}},
  author    = {B. Kim and C. M. Kan and J. Kim and S. H. Lee and C. C. Chung and J. W. Choi},
  url       = {https://arxiv.org/pdf/1704.07049.pdf},
  booktitle = itsc,
  year      = {2017}
}

@inproceedings{amirian2019cvprws,
  author    = {J. Amirian and J.B. Hayet and J. Pettre},
  title     = {{Social Ways: Learning Multi-Modal Distributions of Pedestrian Trajectories With GANs}},
  url       = {https://arxiv.org/pdf/1904.09507.pdf},
  booktitle = cvprws,
  year      = {2019}
}

@inproceedings{hong2019cvpr,
  title     = {{Rules of the Road: Predicting Driving Behavior with a Convolutional Model of Semantic Interactions}},
  author    = {J. Hong and B. Sapp and J. Philbin},
  booktitle = cvpr,
  url       = {https://arxiv.org/pdf/1906.08945.pdf},
  year      = {2019}
}

@inproceedings{hu2018cvpr-sn,
  author    = {J. Hu and L. Shen and G. Sun},
  title     = {{Squeeze-and-Excitation Networks}},
  booktitle = cvprold,
  year      = 2018,
  abstract  = {Convolutional neural networks are built upon the convolution operation, which extracts informative features by fusing spatial and channel-wise information together within local receptive fields. In order to boost the representational power of a network, several recent approaches have shown the benefit of enhancing spatial encoding. In this work, we focus on the channel relationship and propose a novel architectural unit, which we term the âSqueeze-and-Excitationâ (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We demonstrate that by stacking these blocks together, we can construct SENet architectures that generalise extremely well across challenging datasets. Crucially, we find that SE blocks produce significant performance improvements for existing state-of-the-art deep architectures at minimal additional computational cost. SENets formed the foundation of our ILSVRC 2017 classification submission which won first place and significantly reduced the top-5 error to 2.251\%, achieving a â¼25\% relative improvement over the winning entry of 2016. Code and models are available at https: //github.com/hujie-frank/SENet.},
  url       = {proceedings: hu2018cvpr-sn.pdf}
}

@inproceedings{hu2018iv,
  title     = {{Probabilistic Prediction of Vehicle Semantic Intention and Motion}},
  author    = {Y. Hu and W. Zhan and M. Tomizuka},
  booktitle = iv,
  url       = {https://arxiv.org/pdf/1804.03629.pdf},
  year      = {2018}
}

@inproceedings{diehl2019iv,
  title     = {{Graph Neural Networks for Modelling Traffic Participant Interaction}},
  author    = {F. Diehl and T. Brunner and M. Truong-Le and A. Knoll},
  booktitle = iv,
  year      = {2019},
  url       = {https://arxiv.org/pdf/1903.01254.pdf}
}

@inproceedings{sadeghian2019cvpr,
  title     = {{SoPhie: An Attentive GAN for Predicting Paths Compliant to Social and Physical Constraints}},
  author    = {A. Sadeghian and V. Kosaraju and A. Sadeghian and N. Hirose and S. H. Rezatofighi and S. Savarese},
  booktitle = cvpr,
  url       = {https://arxiv.org/pdf/1806.01482.pdf},
  year      = {2019}
}

@inproceedings{bansal2019rss,
  title     = {{ChauffeurNet: Learning to Drive by Imitating the Best and Synthesizing the Worst}},
  author    = {M. Bansal and A. Krizhevsky and A. Ogale},
  booktitle = rss,
  url       = {https://arxiv.org/pdf/1812.03079.pdf},
  year      = {2019}
}

@inproceedings{chandra2019cvpr,
  title     = {{TraPHic: Trajectory Prediction in Dense and Heterogeneous Traffic Using Weighted Interactions}},
  author    = {R. Chandra and U. Bhattacharya, A. Bera and D. Manocha},
  booktitle = cvpr,
  year      = {2019},
  url       = {https://arxiv.org/pdf/1812.04767.pdf}
}

@inproceedings{kayhan2020cvpr,
  title     = {{On Translation Invariance in CNNs: Convolutional Layers can Exploit Absolute Spatial Location}},
  booktitle = cvpr,
  author    = {O.S. Kayhan and J.C. van Gemert},
  url       = {https://arxiv.org/pdf/2003.07064.pdf},
  year      = {2020}
}

@article{bai2018arxiv,
  abstract = {For most deep learning practitioners, sequence modeling is synonymous with recurrent networks. Yet recent results indicate that convolutional architectures can outperform recurrent networks on tasks such as audio synthesis and machine translation. Given a new sequence modeling task or dataset, which architecture should one use? We conduct a systematic evaluation of generic convolutional and recurrent architectures for sequence modeling. The models are evaluated across a broad range of standard tasks that are commonly used to benchmark recurrent networks. Our results indicate that a simple convolutional architecture outperforms canonical recurrent networks such as LSTMs across a diverse range of tasks and datasets, while demonstrating longer effective memory. We conclude that the common association between sequence modeling and recurrent networks should be reconsidered, and convolutional networks should be regarded as a natural starting point for sequence modeling tasks. To assist related work, we have made code available at http://github.com/locuslab/TCN .},
  author   = {S. Bai and J.Z. Kolter and V. Koltun},
  journal  = arxiv,
  title    = {{An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling}},
  year     = {2018},
  url      = {https://arxiv.org/pdf/1803.01271},
  volume   = {arXiv:1803.01271}
}

@inproceedings{maas2013icml,
  title     = {{Rectifier Nonlinearities Improve Neural Network Acoustic Models}},
  author    = {A. Maas and A.Y. Hannun and A.Y. Ng},
  url       = {https://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf},
  booktitle = icml,
  year      = {2013}
}

@inproceedings{krajewski2018itsc,
  title     = {{The highD Dataset: A Drone Dataset of Naturalistic Vehicle Trajectories on German Highways for Validation of Highly Automated Driving Systems}},
  author    = {R. Krajewski and J. Bock and L. Kloeker and L. Eckstein},
  booktitle = itsc,
  url       = {https://arxiv.org/pdf/1810.05642.pdf},
  year      = {2018}
}

@techreport{halkias2006ngsim,
  title       = {{Next Generation SIMulation Fact Sheet}},
  author      = {J. Halkias and J. Colyar},
  year        = {2006},
  url         = {https://www.fhwa.dot.gov/publications/research/operations/its/06135/06135.pdf},
  institution = {Federal Highway Administration}
}

@article{song2020arxiv,
  title   = {{PiP: Planning-informed Trajectory Prediction for Autonomous Driving}},
  author  = {H. Song and W. Ding and Y. Chen and S. Shen and M. Wang and Q. Chen},
  journal = arxiv,
  year    = {2020},
  url     = {https://arxiv.org/pdf/2003.11476},
  volume  = {arXiv:2003.11476}
}

@inproceedings{kuefler2017iv,
  title     = {{Imitating Driver Behavior with Generative Adversarial Networks}},
  author    = {A. Kuefler and J. Morton and T. A. Wheeler and M.J. Kochenderfer},
  booktitle = iv,
  url       = {https://arxiv.org/pdf/1701.06699.pdf},
  year      = {2017}
}

@inproceedings{eitel2019iros,
  author    = {A. Eitel and N. Hauff and W. Burgard},
  title     = {{Self-supervised Transfer Learning for Instance Segmentation through Physical Interaction}},
  booktitle = iros,
  year      = 2019,
  url       = {http://ais.informatik.uni-freiburg.de/publications/papers/eitel19iros.pdf}
}

@article{valada2020ijcv,
  author  = {Valada, A. and Mohan, R. and Burgard, W.},
  title   = {{Self-Supervised Model Adaptation for Multimodal Semantic Segmentation}},
  journal = ijcv,
  volume  = {128},
  pages   = {1239--1285},
  year    = 2020,
  url     = {http://ais.informatik.uni-freiburg.de/publications/papers/valada19ijcv.pdf}
}


@inproceedings{caesar2020cvpr,
  author    = {H. Caesar and V. Bankiti and A.H. Lang and S. Vora and V.E. Liong and Q. Xu and A. Krishnan and Y. Pan and G. Baldan and O. Beijbom},
  title     = {{nuScenes: A Multimodal Dataset for Autonomous Driving}},
  booktitle = cvpr,
  year      = {2020}
}

@inproceedings{hu2020cvpr-ress,
  author    = {Q. Hu and B. Yang and L. Xie and S. Rosa and Y. Guo and Z. Wang and N. Trigoni and A. Markham},
  title     = {{RandLA-Net: Efficient Semantic Segmentation of Large-Scale Point Clouds}},
  booktitle = cvpr,
  year      = 2020,
  abstract  = {We study the problem of efficient semantic segmentation for large-scale 3D point clouds. By relying on expensive sampling techniques or computationally heavy pre/post-processing steps, most existing approaches are only able to be trained and operate over small-scale point clouds. In this paper, we introduce RandLA-Net, an efficient and lightweight neural architecture to directly infer per-point semantics for large-scale point clouds. The key to our approach is to use random point sampling instead of more complex point selection approaches. Although remarkably computation and memory efficient, random sampling can discard key features by chance. To overcome this, we introduce a novel local feature aggregation module to progressively increase the receptive field for each 3D point, thereby effectively preserving geometric details. Extensive experiments show that our RandLA-Net can process 1 million points in a single pass with up to 200x faster than existing approaches. Moreover, our RandLA-Net clearly surpasses state-of-the-art approaches for semantic segmentation on two large-scale benchmarks Semantic3D and SemanticKITTI.},
  url       = {proceedings: hu2020cvpr-ress.pdf}
}

@inproceedings{zhang2020cvpr-paig,
  author    = {Y. Zhang and Z. Zhou and P. David and X. Yue and Z. Xi and B. Gong and H. Foroosh},
  title     = {{PolarNet: An Improved Grid Representation for Online LiDAR Point Clouds Semantic Segmentation}},
  booktitle = cvpr,
  year      = 2020,
  abstract  = {The requirement of fine-grained perception by autonomous driving systems has resulted in recently increased research in the online semantic segmentation of single-scan LiDAR. Emerging datasets and technological advancements have enabled researchers to benchmark this problem and improve the applicable semantic segmentation algorithms. Still, online semantic segmentation of LiDAR scans in autonomous driving applications remains challenging due to three reasons: (1) the need for near-real-time latency with limited hardware, (2) points are distributed unevenly across space, and (3) an increasing number of more fine-grained semantic classes. The combination of the aforementioned challenges motivates us to propose a new LiDAR-specific, KNN-free segmentation algorithm - PolarNet. Instead of using common spherical or bird's-eye-view projection, our polar bird's-eye-view representation balances the points per grid and thus indirectly redistributes the network's attention over the long-tailed points distribution over the radial axis in polar coordination. We find that our encoding scheme greatly increases the mIoU in three drastically different real urban LiDAR single-scan segmentation datasets while retaining ultra low latency and near real-time throughput.},
  url       = {proceedings: zhang2020cvpr-paig.pdf}
}

 @inproceedings{rosu2020rss,
  author    = {R.A. Rosu and P. Sch\"utt and J. Quenzel and S. Behnke},
  title     = {{LatticeNet: Fast Point Cloud SegmentationUsing Permutohedral Lattices}},
  booktitle = rss,
  year      = 2020,
  url       = {http://www.roboticsproceedings.org/rss16/p006.pdf}
}

@article{gan2020ral,
  author  = {L. Gan and R. Zhang and J.W. Grizzle and R.M. Eustice and M. Ghaffari},
  title   = {{Bayesian spatial kernel smoothing for scalable dense semantic mapping}},
  journal = ral,
  volume  = 5,
  number  = 2,
  pages   = {790--797},
  year    = 2020,
  url     = {https://arxiv.org/pdf/1909.04631.pdf}
}

@article{patel2015spm,
  author   = {V.M. {Patel} and R. {Gopalan} and R. {Li} and R. {Chellappa}},
  journal  = {IEEE Signal Processing Magazine},
  title    = {{Visual Domain Adaptation: A survey of recent advances}},
  year     = {2015},
  volume   = {32},
  number   = {3},
  pages    = {53--69},
  abstract = {In pattern recognition and computer vision, one is often faced with scenarios where the training data used to learn a model have different distribution from the data on which the model is applied. Regardless of the cause, any distributional change that occurs after learning a classifier can degrade its performance at test time. Domain adaptation tries to mitigate this degradation. In this article, we provide a survey of domain adaptation methods for visual recognition. We discuss the merits and drawbacks of existing domain adaptation approaches and identify promising avenues for research in this rapidly evolving field.},
  url      = {https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.445.2867&rep=rep1&type=pdf}
}

@article{lewiner2003jgt,
  author  = {T. Lewiner and H. Lopes and A. W. Vieira and G. Tavares},
  title   = {{Efficient implementation of marching cubes cases with topological guarantees}},
  year    = {2003},
  journal = {Journal of Graphics Tools},
  volume  = {8},
  number  = {2},
  pages   = {1--15},
  url     = {http://thomas.lewiner.org/pdfs/marching_cubes_jgt.pdf}
}

@book{pharr2018book,
  title     = {{Physically Based Rendering: From Theory To Implementation}},
  author    = {M. Pharr and W. Jakob and G. Humphreys},
  year      = 2016,
  edition   = 3,
  publisher = {Morgan Kaufmann},
  url       = {http://www.pbr-book.org/}
}

@article{yi2020arxiv,
  author  = {Li Yi and Boqing Gong and Thomas Funkhouser},
  title   = {{Complete \& Label: A Domain Adaptation Approach to Semantic Segmentation of LiDAR Point Clouds}},
  journal = arxiv,
  volume  = {arXiv:2007.08488},
  year    = 2020,
  url     = {https://arxiv.org/pdf/2007.08488.pdf}
}

@article{jiang2020arxiv,
  author  = {P. Jiang and S. Saripalli},
  title   = {{LiDARNet:  A  Boundary-Aware  Domain  Adaptation  Model  for  LidarPoint  Cloud  Semantic  Segmentation}},
  journal = arxiv,
  volume  = {arXiv:2003.01174},
  year    = 2020,
  url     = {https://arxiv.org/pdf/2003.01174.pdf}
}

@inproceedings{jaritz2020cvpr,
  author    = {M. Jaritz and T. Vu and R. d. Charette and E. Wirbel and P. Perez},
  title     = {{xMUDA: Cross-Modal Unsupervised Domain Adaptation for 3D Semantic Segmentation}},
  booktitle = cvpr,
  year      = 2020,
  abstract  = {Unsupervised Domain Adaptation (UDA) is crucial to tackle the lack of annotations in a new domain. There are many multi-modal datasets, but most UDA approaches are uni-modal. In this work, we explore how to learn from multi-modality and propose cross-modal UDA (xMUDA) where we assume the presence of 2D images and 3D point clouds for 3D semantic segmentation. This is challenging as the two input spaces are heterogeneous and can be impacted differently by domain shift. In xMUDA, modalities learn from each other through mutual mimicking, disentangled from the segmentation objective, to prevent the stronger modality from adopting false predictions from the weaker one. We evaluate on new UDA scenarios including day-to-night, country-to-country and dataset-to-dataset, leveraging recent autonomous driving datasets. xMUDA brings large improvements over uni-modal UDA on all tested scenarios, and is complementary to state-of-the-art UDA techniques. Code is available at https://github.com/valeoai/xmuda.},
  url       = {proceedings: jaritz2020cvpr.pdf}
}

@article{sun2016arxiv,
  author   = {B. Sun and J. Feng and K. Saenko},
  title    = {{Correlation Alignment for Unsupervised Domain Adaptation}},
  journal  = arxiv,
  volume   = {arXiv:1612.01939},
  year     = {2016},
  url      = {https://arxiv.org/pdf/1612.01939.pdf},
  abstract = {In this chapter, we present CORrelation ALignment (CORAL), a simple yet effective method for unsupervised domain adaptation. CORAL minimizes domain shift by aligning the second-order statistics of source and target distributions, without requiring any target labels. In contrast to subspace manifold methods, it aligns the original feature distributions of the source and target domains, rather than the bases of lower-dimensional subspaces. It is also much simpler than other distribution matching methods. CORAL performs remarkably well in extensive evaluations on standard benchmark datasets. We first describe a solution that applies a linear transformation to source features to align them with target features before classifier training. For linear classifiers, we propose to equivalently apply CORAL to the classifier weights, leading to added efficiency when the number of classifiers is small but the number and dimensionality of target examples are very high. The resulting CORAL Linear Discriminant Analysis (CORAL-LDA) outperforms LDA by a large margin on standard domain adaptation benchmarks. Finally, we extend CORAL to learn a nonlinear transformation that aligns correlations of layer activations in deep neural networks (DNNs). The resulting Deep CORAL approach works seamlessly with DNNs and achieves state-of-the-art performance on standard benchmark datasets. Our code is available at:~\url{https://github.com/VisionLearningGroup/CORAL}}
}

@article{morerio2017arxiv,
  author   = {Pietro Morerio and Jacopo Cavazza and Vittorio Murino},
  title    = {{Minimal-Entropy Correlation Alignment for Unsupervised Deep Domain Adaptation}},
  journal  = arxiv,
  volume   = {arXiv:1711.10288},
  year     = {2017},
  url      = {https://arxiv.org/pdf/1711.10288.pdf},
  abstract = {In this work, we face the problem of unsupervised domain adaptation with a novel deep learning approach which leverages on our finding that entropy minimization is induced by the optimal alignment of second order statistics between source and target domains. We formally demonstrate this hypothesis and, aiming at achieving an optimal alignment in practical cases, we adopt a more principled strategy which, differently from the current Euclidean approaches, deploys alignment along geodesics. Our pipeline can be implemented by adding to the standard classification loss (on the labeled source domain), a source-to-target regularizer that is weighted in an unsupervised and data-driven fashion. We provide extensive experiments to assess the superiority of our framework on standard domain and modality adaptation benchmarks.}
}

@inproceedings{fernando2013iccv,
  author    = {B. {Fernando} and A. {Habrard} and M. {Sebban} and T. {Tuytelaars}},
  booktitle = iccv,
  title     = {{Unsupervised Visual Domain Adaptation Using Subspace Alignment}},
  year      = {2013},
  url       = {https://hal.archives-ouvertes.fr/hal-00869417/document}
}

@inproceedings{shrivastava2017cvpr,
  author    = {A. Shrivastava and T. Pfister and O. Tuzel and J. Susskind and W. Wang and R. Webb},
  title     = {{Learning from Simulated and Unsupervised Images through Adversarial Training}},
  year      = 2017,
  booktitle = cvpr,
  url       = {http://openaccess.thecvf.com/content_cvpr_2017/papers/Shrivastava_Learning_From_Simulated_CVPR_2017_paper.pdf}
}

@inproceedings{hoffman2018icml,
  author    = {J. Hoffman and E. Tzeng and T. Park and T.{-}Y. Zhu and P. Isola and K. Saenko and A. A. Efros and T. Darrell},
  title     = {{CyCADA: Cycle-Consistent Adversarial Domain Adaptation}},
  year      = 2018,
  booktitle = icml,
  url       = {http://proceedings.mlr.press/v80/hoffman18a/hoffman18a.pdf}
}

@inproceedings{dosovitskiy2017corl,
  title     = {{CARLA: An Open Urban Driving Simulator}},
  author    = {A. Dosovitskiy and G. Ros and F. Codevilla and A. Lopez and V. Koltun},
  booktitle = corl,
  year      = {2017},
  url       = {http://proceedings.mlr.press/v78/dosovitskiy17a/dosovitskiy17a.pdf}
}

@inproceedings{johnson-roberson2017icra,
  author    = {Johnson-Roberson, M. and Barto, C. and Mehta, R. and Sridhar, S. N. and Rosaen, K. and Vasudevan, R.},
  booktitle = icra,
  title     = {{Driving in the Matrix: Can virtual worlds replace human-generated annotations for real world tasks?}},
  year      = {2017},
  url       = {https://arxiv.org/pdf/1610.01983.pdf}
}

@inproceedings{qi2019iccv,
  author    = {Charles R. Qi and Or Litany and Kaiming He and Leonidas J. Guibas},
  title     = {{Deep Hough Voting for 3D Object Detection in Point Clouds}},
  booktitle = iccv,
  year      = {2019},
  url       = {https://openaccess.thecvf.com/content_ICCV_2019/papers/Qi_Deep_Hough_Voting_for_3D_Object_Detection_in_Point_Clouds_ICCV_2019_paper.pdf}
}

@inproceedings{wang2020cvpr,
  author    = {Wang, Haochen and Luo, Ruotian and Maire, Michael and Shakhnarovich, Greg},
  title     = {{Pixel Consensus Voting for Panoptic Segmentation}},
  booktitle = cvpr,
  year      = {2020},
  url       = {https://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Pixel_Consensus_Voting_for_Panoptic_Segmentation_CVPR_2020_paper.pdf}
}

@inproceedings{cheng2020cvpr,
  author    = {Cheng, Bowen and Collins, Maxwell D. and Zhu, Yukun and Liu, Ting and Huang, Thomas S. and Adam, Hartwig and Chen, Liang-Chieh},
  title     = {{Panoptic-DeepLab: A Simple, Strong, and Fast Baseline for Bottom-Up Panoptic Segmentation}},
  booktitle = cvpr,
  year      = {2020},
  url       = {https://openaccess.thecvf.com/content_CVPR_2020/papers/Cheng_Panoptic-DeepLab_A_Simple_Strong_and_Fast_Baseline_for_Bottom-Up_Panoptic_CVPR_2020_paper.pdf}
}

@inproceedings{jiang2020cvpr,
  author    = {H. Jiang and F. Yan and J. Cai and J. Zheng and J. Xiao},
  title     = {{End-to-end 3D Point Cloud Instance Segmentation without Detection}},
  booktitle = cvpr,
  year      = {2020},
  url       = {https://openaccess.thecvf.com/content_CVPR_2020/papers/Jiang_End-to-End_3D_Point_Cloud_Instance_Segmentation_Without_Detection_CVPR_2020_paper.pdf}
}

@inproceedings{du2020icra,
  author    = {L. Du and J. Tan and X. Xue and L. Chen and H. Wen and J. Feng and J. Li and X. Zhang},
  title     = {{3DCFS: Fast and Robust Joint 3D Semantic-Instance Segmentation via Coupled Feature Selection}},
  booktitle = icra,
  year      = {2020},
  url       = {https://arxiv.org/pdf/2003.00535.pdf}
}

@inproceedings{zhao2020aaai,
  author    = {L. Zhao and W. Tao},
  title     = {{JSNet: Joint Instance and Semantic Segmentation of 3D Point Clouds}},
  booktitle = aaai,
  year      = {2020},
  url       = {https://arxiv.org/pdf/1912.09654.pdf}
}

@inproceedings{wang2019cvpr,
  author    = {X. Wang and S. Liu and X. Shen and C. Shen and J. Jia},
  title     = {{Associatively Segmenting Instances and Semantics in Point Clouds}},
  booktitle = cvpr,
  year      = {2019},
  url       = {https://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Associatively_Segmenting_Instances_and_Semantics_in_Point_Clouds_CVPR_2019_paper.pdf}
}

@inproceedings{narita2019iros,
  author    = {Gaku Narita and Takashi Seno and Tomoya Ishikawa and Yohsuke Kaji},
  title     = {{PanopticFusion: Online Volumetric Semantic Mapping at the Level of Stuff and Things}},
  booktitle = iros,
  year      = {2019},
  url       = {https://arxiv.org/pdf/1903.01177.pdf}
}

@inproceedings{han2020cvpr,
  author    = {L. Han and T. Zheng and L. Xu and L. Fang},
  title     = {{OccuSeg: Occupancy-aware 3D Instance Segmentation}},
  booktitle = cvpr,
  year      = {2020},
  url       = {https://openaccess.thecvf.com/content_CVPR_2020/papers/Han_OccuSeg_Occupancy-Aware_3D_Instance_Segmentation_CVPR_2020_paper.pdf}
}

@article{pitropov2020arxiv,
  author  = {M. Pitropov and G. Danson and J. Rebello and M. Smart and C. Wang and K. Czarnecki and S. Waslander},
  title   = {{Canadian  Adverse  Driving  Conditions  Dataset}},
  journal = arxiv,
  volume  = {arXiv:2001.10117},
  year    = 2020,
  url     = {https://arxiv.org/pdf/2001.10117.pdf}
}

@inproceedings{sun2020cvpr-sipf,
  author    = {P. Sun and H. Kretzschmar and X. Dotiwalla and A. Chouard and V. Patnaik and P. Tsui and J. Guo and Y. Zhou and Y. Chai and B. Caine and V. Vasudevan and W. Han and J. Ngiam and H. Zhao and A. Timofeev and S. Ettinger and M. Krivokon and A. Gao and A. Joshi and Y. Zhang and J. Shlens and Z. Chen and D. Anguelov},
  title     = {{Scalability in Perception for Autonomous Driving: Waymo Open Dataset}},
  booktitle = cvpr,
  year      = 2020,
  abstract  = {The research community has increasing interest in autonomous driving research, despite the resource intensity of obtaining representative real world data. Existing self-driving datasets are limited in the scale and variation of the environments they capture, even though generalization within and between operating regions is crucial to the over-all viability of the technology. In an effort to help align the research community's contributions with real-world self-driving problems, we introduce a new large scale, high quality, diverse dataset. Our new dataset consists of 1150 scenes that each span 20 seconds, consisting of well synchronized and calibrated high quality LiDAR and camera data captured across a range of urban and suburban geographies. It is 15x more diverse than the largest camera+LiDAR dataset available based on our proposed diversity metric. We exhaustively annotated this data with 2D (camera image) and 3D (LiDAR) bounding boxes, with consistent identifiers across frames. Finally, we provide strong baselines for 2D as well as 3D detection and tracking tasks. We further study the effects of dataset size and generalization across geographies on 3D detection methods. Find data, code and more up-to-date information at http://www.waymo.com/open.},
  url       = {proceedings: sun2020cvpr-sipf.pdf}
}

@inproceedings{patil2019icra,
  author    = {A.  Patil and  S.  Malla and  H.  Gang  and  Y.-T.  Chen},
  title     = {{The  H3D  dataset  forfull-surround 3D multi-object detection and tracking in crowded urban scenes}},
  booktitle = icra,
  year      = {2019},
  url       = {https://arxiv.org/pdf/1903.01568.pdf}
}

@article{pan2020arxiv,
  title   = {{SemanticPOSS: A Point Cloud Dataset with Large Quantity of Dynamic Instances}},
  author  = {Yancheng Pan and Biao Gao and Jilin Mei and Sibo Geng and Chengkun Li and Huijing Zhao},
  journal = arxiv,
  volume  = {arXiv:2002.09147},
  year    = 2020,
  url     = {https://arxiv.org/pdf/2002.09147.pdf}
}

@misc{pandas2020misc,
  author = {scale and Hesai},
  title  = {{PandaSet Dataset (Available at \href{https://scale.com/open-datasets/pandaset}{https://scale.com/open-datasets/pandaset})}},
  year   = 2020
}

@inproceedings{paszke2019neurips,
  title     = {{PyTorch: An Imperative Style, High-Performance Deep Learning Library}},
  author    = {A. Paszke and S. Gross and F. Massa and A. Lerer and J. Bradbury and G. Chanan and T. Killeen and Z. Lin and N. Gimelshein, Natalia and L. Antiga and A. Desmaison and A. Kopf and E. Yang and Z. DeVito and M. Raison and A. Tejani and S. Chilamkurthy and B. Steiner and L. Fang and J. Bai and S. Chintala},
  booktitle = neurips,
  year      = {2019},
  url       = {http://papers.nips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
}

@article{geiger2013ijrr,
  author  = {Andreas Geiger and Philip Lenz and Christoph Stiller and Raquel Urtasun},
  journal = ijrr,
  title   = {{Vision meets Robotics: The KITTI Dataset}},
  year    = {2013},
  volume  = {32},
  number  = {11},
  pages   = {1231--1237},
  url     = {https://www.mrt.kit.edu/z/publ/download/2013/GeigerAl2013IJRR.pdf}
}

@article{kilpatrick1953pr,
  author  = {F.P. Kilpatrick and W.H. Ittelson},
  title   = {{The size-distance invariance hypothesis}},
  journal = {Psychological Review},
  volume  = {60},
  number  = {4},
  pages   = {223--231},
  year    = {1953}
}

@article{schiffman1967ajp,
  author  = {H.R. Schiffman},
  title   = {{Size estimation of familiar objects under informative and reduced conditions of viewing}},
  journal = {American Journal of Psychology},
  volume  = {80},
  number  = {2},
  pages   = {229--235},
  year    = {1967}
}

@book{eysenck2000book,
  author    = {Michael W. Eysenck and Mark T. Keane},
  title     = {{Cognitive Pyschology: A Student's Handbook}},
  edition   = {4th},
  publisher = {Psychology Press},
  year      = {2000}
}

@article{wang2004tip,
  author  = {Wang, Zhou and Bovik, Alan C and Sheikh, Hamid R and Simoncelli, Eero P},
  journal = {IEEE Transactions on Image Processing},
  title   = {Image quality assessment: from error visibility to structural similarity},
  year    = {2004},
  number  = {4},
  pages   = {600--612},
  volume  = {13},
  url     = {https://www.cns.nyu.edu/pub/lcv/wang03-preprint.pdf}
}

@inproceedings{mahjourian2018cvpr,
  author    = {Mahjourian, Reza and Wicke, Martin and Angelova, Anelia},
  booktitle = cvpr,
  title     = {Unsupervised learning of depth and ego-motion from monocular video using 3d geometric constraints},
  year      = {2018},
  url       = {https://openaccess.thecvf.com/content_cvpr_2018/papers/Mahjourian_Unsupervised_Learning_of_CVPR_2018_paper.pdf}
}

@inproceedings{horn1990sam,
  author    = {Horn, Roger A},
  booktitle = {Proc.~of the Symp. on Appl. Mathematics},
  title     = {The hadamard product},
  year      = {1990}
}

@inproceedings{he2020cvpr-mcfu,
  author    = {K. He and H. Fan and Y. Wu and S. Xie and R. Girshick},
  title     = {{Momentum Contrast for Unsupervised Visual Representation Learning}},
  booktitle = cvpr,
  year      = 2020,
  abstract  = {We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a 
               perspective on contrastive learning as dictionary look-up, we build a dynamic dictionary with a queue and a 
               moving-averaged encoder. This enables building a large and consistent dictionary on-the-fly that facilitates 
               contrastive unsupervised learning. MoCo provides competitive results under the common linear protocol on ImageNet 
               classification. More importantly, the representations learned by MoCo transfer well to downstream tasks. MoCo can 
               outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on PASCAL VOC, COCO, and other
               datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised 
               representation learning has been largely closed in many vision tasks.},
  url       = {proceedings: he2020cvpr-mcfu.pdf}
}


@article{khosla2020arxiv,
  title   = {{Supervised Contrastive Learning}},
  author  = {Prannay Khosla and Piotr Teterwak and Chen Wang and Aaron Sarna and Yonglong Tian and Phillip Isola and Aaron Maschinot and Ce Liu and Dilip Krishnan},
  journal = arxiv,
  volume  = {arXiv:2004.11362},
  year    = {2020},
  url     = {https://arxiv.org/pdf/2004.11362.pdf}
}

@inproceedings{chen2020icml,
  author    = {Ting Chen and Simon Kornblith and Mohammad Norouzi and Geoffrey Hinton},
  title     = {{A Simple Framework for Contrastive Learning of Visual Representations}},
  booktitle = icml,
  year      = {2020},
  url       = {https://arxiv.org/pdf/2002.05709.pdf},
  abstract  = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We 
               simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures 
               or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, 
               we systematically study the major components of our framework. We show that (1) composition of data augmentations 
               plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation 
               between the representation and the contrastive loss substantially improves the quality of the learned representations, 
               and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. 
               By combining these findings, we are able to considerably outperform previous methods for self-supervised and 
               semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by 
               SimCLR achieves 76.5\% top-1 accuracy, which is a 7\% relative improvement over previous state-of-the-art, matching the 
               performance of a supervised ResNet-50. When fine-tuned on only 1\% of the labels, we achieve 85.8\% top-5 accuracy, 
               outperforming AlexNet with 100X fewer labels. }
}

@inproceedings{kolesnikov2019cvpr,
  author    = {Alexander Kolesnikov and Xiaohua Zhai and Lucas Beyer},
  title     = {{Revisiting Self-Supervised Visual Representation Learning}},
  booktitle = cvpr,
  year      = {2019},
  url       = {https://openaccess.thecvf.com/content_CVPR_2019/papers/Kolesnikov_Revisiting_Self-Supervised_Visual_Representation_Learning_CVPR_2019_paper.pdf},
  abstract  = {Unsupervised visual representation learning remainsa largely unsolved problem in computer vision 
               research. Among a big body of recently proposed approaches for unsupervised learning of visual representations, a 
               class of self-supervised techniques achieves superior performanceon many challenging benchmarks. A large number of the 
               pretext tasks for self-supervised learning have been studied, but other important aspects, such as the choice of 
               convolutional neural networks (CNN), has not received equalattention. Therefore, we revisit numerous previously 
               proposed self-supervised models, conduct a thorough largescale study and, as a result, uncover multiple crucial 
               insights. We challenge a number of common practices in self-supervised visual representation learning and observe 
               that standard recipes for CNN design do not always translateto self-supervised representation learning. As part of 
               our study, we drastically boost the performance of previouslyproposed techniques and outperform previously published 
               state-of-the-art results by a large margin.}
}

@inproceedings{he2019iccv,
  author    = {Kaiming He and Ross Girshick and Piotr Dollar},
  title     = {{Rethinking ImageNet Pre-training}},
  booktitle = iccv,
  year      = {2019},
  url       = {https://arxiv.org/pdf/1811.08883.pdf}
}

@inproceedings{razavian2014cvpr,
  author    = {Ali Sharif Razavian and Hossein Azizpour and Josephine Sullivan and Stefan Carlsson},
  title     = {{CNN Features off-the-shelf: an Astounding Baseline for Recognition}},
  booktitle = {Proc.~of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) Workshops},
  year      = {2014},
  url       = {https://arxiv.org/pdf/1403.6382.pdf}
}

@inproceedings{wang2018cvpr,
  author    = {Wang, Chaoyang and Miguel Buenaposada, Jos{\'e} and Zhu, Rui and Lucey, Simon},
  booktitle = cvpr,
  title     = {Learning depth from monocular videos using direct methods},
  year      = {2018},
  url       = {https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Learning_Depth_From_CVPR_2018_paper.pdf}
}

@inproceedings{eigen2014nips,
  author    = {Eigen, David and Puhrsch, Christian and Fergus, Rob},
  booktitle = nips,
  title     = {{Depth Map Prediction from a Single Image using a Multi-Scale Deep Network}},
  year      = {2014},
  url       = {http://papers.nips.cc/paper/5539-depth-map-prediction-from-a-single-image-using-a-multi-scale-deep-network.pdf}
}

@article{saxena2008pami,
  author  = {Saxena, Ashutosh and Sun, Min and Ng, Andrew Y},
  journal = pami,
  title   = {{Make3d: Learning 3d scene structure from a single still image}},
  year    = {2008},
  number  = {5},
  pages   = {824--840},
  volume  = {31},
  url     = {http://www.cs.cornell.edu/~asaxena/reconstruction3d/saxena_make3d_learning3dstructure.pdf}
}

@inproceedings{saxena2006nips,
  author    = {Saxena, Ashutosh and Chung, Sung H and Ng, Andrew Y},
  booktitle = nips,
  title     = {Learning depth from single monocular images},
  year      = {2006},
  url       = {http://www.cs.cornell.edu/~asaxena/learningdepth/NIPS_LearningDepth.pdf}
}

@inproceedings{geiger2010accv,
  author    = {Andreas Geiger and Martin Roser and Raquel Urtasun},
  booktitle = accv,
  title     = {{Efficient Large-Scale Stereo Matching}},
  year      = {2010},
  url       = {http://www.cvlibs.net/publications/Geiger2010ACCV.pdf}
}

@article{scharstein2002ijcv,
  author  = {Scharstein, Daniel and Szeliski, Richard},
  journal = ijcv,
  title   = {A taxonomy and evaluation of dense two-frame stereo correspondence algorithms},
  year    = {2002},
  number  = {1-3},
  pages   = {7--42},
  volume  = {47}
}

@inproceedings{ladicky2014cvpr,
  author    = {Ladicky, Lubor and Shi, Jianbo and Pollefeys, Marc},
  booktitle = cvpr,
  title     = {Pulling things out of perspective},
  year      = {2014},
  url       = {https://people.inf.ethz.ch/ladickyl/depth_cvpr14.pdf}
}

@inproceedings{godard2017cvpr,
  author    = {Godard, Clement and Mac Aodha, Oisin and Brostow, Gabriel J.},
  booktitle = cvpr,
  title     = {{Unsupervised Monocular Depth Estimation With Left-Right Consistency}},
  year      = {2017},
  url       = {https://openaccess.thecvf.com/content_cvpr_2017/papers/Godard_Unsupervised_Monocular_Depth_CVPR_2017_paper.pdf}
}

@article{konda2013arxiv,
  author  = {Konda, Kishore and Memisevic, Roland},
  journal = arxiv,
  volume  = {arXiv:1312.3429},
  title   = {Unsupervised learning of depth and motion},
  year    = {2013},
  url     = {https://arxiv.org/pdf/1312.3429.pdf}
}

@inproceedings{garg2016eccv,
  author    = {Garg, Ravi and Vijay Kumar and Carneiro, Gustavo and Reid, Ian},
  booktitle = eccv,
  title     = {{Unsupervised CNN for Single View Depth Estimation: Geometry to the rescue}},
  year      = {2016},
  url       = {https://arxiv.org/pdf/1603.04992.pdf}
}

@inproceedings{klinger2020eccv,
  author    = {Marvin Klinger and Jan-Alke Term\"ohlen and Jonas Mikolayczyk and Tim Fingscheidt},
  title     = {{Self-Supervised Monocular Depth Estimation: Solving the Dynamic Object Problem by Semantic Guidance}},
  booktitle = eccv,
  year      = {2020},
  url       = {https://arxiv.org/pdf/2007.06936.pdf}
}

@inproceedings{kumar2021wacv,
  author    = {Varun Ravi Kumar and Marvin Klingner and Senthil Yogamani and Stefan Milz and Tim Fingscheidt and Patrick Maeder},
  title     = {{SynDistNet: Self-Supervised Monocular Fisheye Camera Distance Estimation Synergized with Semantic Segmentation for Autonomous Driving}},
  booktitle = wacv,
  year      = 2021,
  url       = {https://arxiv.org/pdf/2008.04017.pdf}
}

@inproceedings{kumar2020iros,
  author    = {Varun Ravi Kumar and Senthil Yogamani and Markus Bach and Christian Witt and Stefan Milz and Patrick Mader},
  title     = {{UnRectDepthNet:  Self-Supervised  Monocular  Depth  Estimation  using  a Generic  Framework  for  Handling  Common  Camera  Distortion  Models}},
  booktitle = iros,
  year      = {2020},
  url       = {https://arxiv.org/pdf/2007.06676.pdf}
}

@inproceedings{ramirez2018accv,
  author    = {Ramirez, Pierluigi Zama and Poggi, Matteo and Tosi, Fabio and Mattoccia, Stefano and Di Stefano, Luigi},
  booktitle = accv,
  title     = {Geometry meets semantics for semi-supervised monocular depth estimation},
  year      = {2018},
  url       = {https://arxiv.org/pdf/1810.04093.pdf}
}

@inproceedings{guizilini2020cvpr,
  author    = {Guizilini, Vitor and Ambrus, Rares and Pillai, Sudeep and Gaidon, Adrien},
  booktitle = cvpr,
  title     = {{3D Packing for Self-Supervised Monocular Depth Estimation}},
  year      = {2020},
  url       = {https://arxiv.org/pdf/1905.02693.pdf}
}

@inproceedings{guizilini2020iclr,
  author    = {Guizilini, Vitor and Hou, Rui and Li, Jie and Ambrus, Rares and Gaidon, Adrien},
  title     = {{Semantically-Guided Representation Learning for Self-Supervised Monocular Depth}},
  booktitle = iclr,
  year      = {2020},
  url       = {https://arxiv.org/pdf/2002.12319.pdf}
}


@inproceedings{zhou2017cvpr,
  author    = {Zhou, Tinghui and Brown, Matthew and Snavely, Noah and Lowe, David G},
  booktitle = cvpr,
  title     = {{Unsupervised Learning of Depth and Ego-Motion From Video}},
  year      = {2017},
  url       = {https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhou_Unsupervised_Learning_of_CVPR_2017_paper.pdf}
}

@inproceedings{godard2019iccv,
  author    = {Godard, Cl{\'e}ment and Mac Aodha, Oisin and Firman, Michael and Brostow, Gabriel J},
  booktitle = iccv,
  title     = {{Digging into self-supervised monocular depth estimation}},
  year      = {2019},
  url       = {https://openaccess.thecvf.com/content_ICCV_2019/papers/Godard_Digging_Into_Self-Supervised_Monocular_Depth_Estimation_ICCV_2019_paper.pdf}
}

@inproceedings{chen2020iros,
  author    = {X. Chen and T. L\"abe and L. Nardi and J. Behley and C. Stachniss},
  title     = {{Learning an Overlap-based Observation Model for 3D LiDAR Localization}},
  booktitle = iros,
  year      = {2020},
  url       = {https://www.ipb.uni-bonn.de/pdfs/chen2020iros.pdf}
}


@article{zhou2018arxiv,
  author  = {Q.Y. Zhou and J. Park and V. Koltun},
  title   = {{Open3D}: {A} Modern Library for {3D} Data Processing},
  journal = arxiv,
  volume  = {arXiv:1801.09847},
  year    = {2018},
  url     = {https://arxiv.org/pdf/1801.09847.pdf}
}


@inproceedings{kim2020icra,
  title     = {Mulran: Multimodal range dataset for urban place recognition},
  author    = {G. Kim and Y.S. Park and Y. Cho and J. Jeong and A. Kim},
  booktitle = icra,
  year      = {2020},
  url       = {proceedings:kim2020icra}
}

@inproceedings{akai2020iv,
  author    = {N. Akai and T. Hirayama and H. Murase},
  title     = {{3D Monte Carlo Localization with Efficient Distance FieldRepresentation for Automated Driving in Dynamic Environments}},
  booktitle = iv,
  year      = {2020},
  url       = {https://www.ipb.uni-bonn.de/pdfs/akai2020iv}
}

@inproceedings{carr2001siggraph,
  title     = {Reconstruction and representation of 3D objects with radial basis functions},
  author    = {Carr, J.C. and Beatson, R.K. and Cherrie, J.B. and Mitchell, T.J. and Fright, W.R. and McCallum, B.C. and Evans, T.R.},
  booktitle = siggraph,
  year      = {2001},
  url       = {http://www.cs.jhu.edu/~misha/Fall05/Papers/carr01.pdf}
}

@inproceedings{alexa2001vis,
  title     = {Point set surfaces},
  author    = {M. Alexa and J. Behr and D. Cohen-Or and S. Fleishman and D. Levin and C.T. Silva},
  booktitle = {Proc.~of the IEEE Visualization (VIS)},
  year      = {2001},
  url       = {http://www.cs.tau.ac.il/~dcor/online_papers/papers/points_set_vis01.pdf}
}

@inproceedings{shen2004siggraph,
  title     = {Interpolating and approximating implicit surfaces from polygon soup},
  author    = {C. Shen and J.F.  O'Brien and J.R. Shewchuk},
  booktitle = siggraph,
  year      = {2004},
  url       = {https://people.engr.tamu.edu/schaefer/teaching/689_Fall2006/Shen-2004-IAI.pdf}
}

@article{berger2017cgf,
  author  = {M. Berger and A.Tagliasacchi and L. Seversky and P. Alliez and G. Guennebaud and J. Levine and A. Sharf and C. Silva},
  journal = {Computer Graphics Forum},
  title   = {A survey of surface reconstruction from point clouds},
  year    = {2017},
  number  = {1},
  pages   = {301--329},
  volume  = {36},
  url     = {https://hal.inria.fr/hal-01348404v2/document}
}

@inproceedings{goyal2019iccv,
  author    = {P. Goyal and D. Mahajan and A. Gupta and I. Misra},
  title     = {{Scaling and Benchmarking Self-Supervised Visual Representation Learning}},
  booktitle = iccv,
  year      = 2019,
  abstract  = {Self-supervised learning aims to learn representations from the data itself without explicit manual supervision. Existing efforts ignore a crucial aspect of self-supervised learning - the ability to scale to large amount of data because self-supervision requires no manual labels. In this work, we revisit this principle and scale two popular self-supervised approaches to 100 million images. We show that by scaling on various axes (including data size and problem 'hardness'), one can largely match or even exceed the performance of supervised pre-training on a variety of tasks such as object detection, surface normal estimation (3D) and visual navigation using reinforcement learning. Scaling these methods also provides many interesting insights into the limitations of current self-supervised techniques and evaluations. We conclude that current self-supervised methods are not 'hard' enough to take full advantage of large scale data and do not seem to learn effective high level semantic representations. We also introduce an extensive benchmark across 9 different datasets and tasks. We believe that such a benchmark along with comparable evaluation settings is necessary to make meaningful progress. Code is at: https://github.com/facebookresearch/fair_self_supervision_benchmark.},
  url       = {proceedings: goyal2019iccv.pdf}
}

% vizzo2021icra references =====================================================
@article{kazhdan2013acmgraphics,
  author   = {M. Kazhdan and H. Hoppe},
  journal  = acmgraphics,
  title    = {Screened poisson surface reconstruction},
  year     = {2013},
  number   = {3},
  pages    = {1--13},
  volume   = {32},
  keywords = {3D Surface Reconstruction},
  url      = {https://www.cs.jhu.edu/~misha/MyPapers/ToG13.pdf}
}

@inproceedings{roldao2019itsc,
  author    = {L. Roldao and R. Charette and A. Verroust-Blondet},
  title     = {3D Surface Reconstruction from Voxel-based Lidar Data},
  booktitle = {Proc.~of the IEEE Intell. Transp. Sys. Conf. (ITSC)},
  year      = {2019},
  abstract  = {To achieve fully autonomous navigation, vehicles need to compute an accurate model of their direct surrounding. In this paper, a 3D surface reconstruction algorithm from heterogeneous density 3D data is presented. The proposed method is based on a TSDF voxel-based representation, where an adaptive neighborhood kernel sourced on a Gaussian confidence evaluation is introduced. This enables to keep a good trade-off between the density of the reconstructed mesh and its accuracy. Experimental evaluations carried on both synthetic (CARLA) and real (KITTI) 3D data show a good performance compared to a state of the art method used for surface reconstruction.},
  keywords  = {3D Surface Reconstruction},
  url       = {https://arxiv.org/pdf/1906.10515.pdf}
}

@article{kolluri2008talg,
  author   = {R. Kolluri},
  journal  = talg,
  title    = {Provably good moving least squares},
  year     = {2008},
  number   = {2},
  pages    = {18},
  volume   = {4},
  keywords = {3D Surface Reconstruction},
  url      = {http://graphics.berkeley.edu/papers/Kolluri-PGM-2005-08/Kolluri-PGM-2005-08.pdf}
}


@inproceedings{hoppe1992siggraph,
  author    = {H. Hoppe and T. DeRose and T. Duchamp and J. McDonald and W. Stuetzle},
  booktitle = siggraph,
  title     = {Surface reconstruction from unorganized points},
  year      = {1992},
  keywords  = {3D Surface Reconstruction},
  url       = {https://graphics.pixar.com/library/Reconstruction/paper.pdf}
}

@inproceedings{oleynikova2017iros,
  author    = {H. Oleynikova and Z. Taylor and M. Fehr and R. Siegwar and J. Nieto},
  booktitle = iros,
  title     = {Voxblox: Incremental 3d euclidean signed distance fields for on-board mav planning},
  year      = {2017},
  keywords  = {3D Surface Reconstruction},
  url       = {https://helenol.github.io/publications/iros_2017_voxblox.pdf}
}

@article{champ2020aps,
  title   = {Instance segmentation for the fine detection of crop and weed plants by precision agricultural robots},
  author  = {Champ, Julien and Mora-Fallas, Adan and Go{\"e}au, Herv{\'e} and Mata-Montero, Erick and Bonnet, Pierre and Joly, Alexis},
  journal = {Applications in Plant Sciences},
  volume  = {8},
  number  = {7},
  pages   = {e11373},
  year    = {2020},
  url     = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7394709/pdf/APS3-8-e11373.pdf}
}

@article{reijgwart2019ral,
  author  = {V. Reijgwart and A. Millane and H. Oleynikova and R. Siegwart and C. Cadena and J. Nieto},
  journal = ral,
  title   = {Voxgraph: Globally Consistent, Volumetric Mapping Using Signed Distance Function Submaps},
  year    = {2019},
  number  = {1},
  pages   = {227--234},
  volume  = {5},
  url     = {https://arxiv.org/pdf/2004.13154.pdf}
}

@inproceedings{klingensmith2015rss,
  author    = {M. Klingensmith and I. Dryanovski and S. Srinivasa and J. Xiao},
  booktitle = rss,
  title     = {Chisel: Real Time Large Scale 3D Reconstruction Onboard a Mobile Device using Spatially Hashed Signed Distance Fields.},
  year      = {2015},
  keywords  = {3D Surface Reconstruction},
  url       = {http://www.roboticsproceedings.org/rss11/p40.pdf}
}

@article{smith2019aiml,
  title   = {Super-convergence: Very fast training of neural networks using large learning rates},
  author  = {Smith, Leslie N and Topin, Nicholay},
  journal = {Artificial Intelligence and Machine Learning for Multi-Domain Operations Applications},
  volume  = {11006},
  pages   = {369--386},
  year    = {2019},
  url     = {https://arxiv.org/pdf/1708.07120.pdf}
}

@inproceedings{badino2011icra,
  title     = {Fast and accurate computation of surface normals from range images},
  author    = {H. Badino and D. Huber and Y. Park and T. Kanade},
  booktitle = icra,
  year      = {2011},
  url       = {https://www.ri.cmu.edu/pub_files/2011/5/ICRA2011.pdf}
}

@inproceedings{aspert2002icme,
  author    = {N. Aspert, D. Santa-Cruz and E. Touradj},
  booktitle = icme,
  title     = {Mesh: Measuring errors between surfaces using the hausdorff distance},
  year      = {2002},
  url       = {http://www.cmap.polytechnique.fr/~peyre/cours/x2005signal/mesh_mesh.pdf}
}

@inproceedings{kuehner2020icra,
  author    = {T. K{\"u}hner and J. K{\"u}mmerle},
  title     = {{Large-Scale Volumetric Scene Reconstruction using LiDAR}},
  booktitle = icra,
  year      = 2020,
  keywords  = {TSDF, Range Sensing, Mapping, SLAM},
  abstract  = {Large-scale 3D scene reconstruction is an important task in autonomous driving and other 
               robotics applications as having an accurate representation of the environment is necessary to safely 
               interact with it. Reconstructions are used for numerous tasks ranging from localization and mapping to 
               planning. In robotics, volumetric depth fusion is the method of choice for indoor applications since the 
               emergence of commodity RGB-D cameras due to its robustness and high reconstruction quality. In this work 
               we present an approach for volumetric depth fusion using LiDAR sensors as they are common on most autonomous 
               cars. We present a framework for large-scale mapping of urban areas considering loop closures. Our method 
               creates a meshed representation of an urban area from recordings over a distance of 3.7 km with a high level 
               of detail on consumer graphics hardware in several minutes. The whole process is fully automated and does not 
               need any user interference. We quantitatively evaluate our results from a real world application. Also, we 
               investigate the effects of the sensor model that we assume on reconstruction quality by using synthetic data.},
  url       = {proceedings: kuehner2020icra.pdf}
}

@article{rong2020lgsvl,
  title   = {LGSVL Simulator: A High Fidelity Simulator for Autonomous Driving},
  author  = {G. Rong and B.H. Shin and H. Tabatabaee and Q. Lu and S. Lemke and M. Mo{\v{z}}eiko and E. Boise and G. Uhm and M. Gerow and S. Mehta and others},
  journal = arxiv,
  volume  = {arXiv:2005.03778},
  year    = {2020},
  url     = {https://arxiv.org/pdf/2005.03778.pdf}
}

@inproceedings{rosinol2020icra,
  title     = {{Kimera: an open-source library for real-time metric-semantic localization and mapping}},
  author    = {Rosinol, Antoni and Abate, Marcus and Chang, Yun and Carlone, Luca},
  booktitle = icra,
  year      = {2020},
  url       = {proceedings: rosinol2020icra.pdf}
}

@inproceedings{rosinol2019icra,
  title     = {Incremental visual-inertial 3d mesh generation with structural regularities},
  author    = {A. Rosinol and T. Sattler and M. Pollefeys and L. Carlone},
  booktitle = icra,
  year      = {2019},
  url       = {https://arxiv.org/pdf/1903.01067.pdf}
}

@inproceedings{ruetz2019icra,
  title     = {OVPC Mesh: 3D Free-space Representation for Local Ground Vehicle Navigation},
  author    = {F. Ruetz and E. Hern{\'a}ndez and M. Pfeiffer and H. Oleynikova and M. Cox and T. Lowe and P. Borges},
  booktitle = icra,
  year      = {2019},
  url       = {https://arxiv.org/pdf/1811.10266.pdf}
}

@article{piazza2018arxiv,
  title   = {Real-time CPU-based large-scale 3D mesh reconstruction},
  author  = {E. Piazza and A. Romanoni and M. Matteucci},
  journal = arxiv,
  volume  = {arXiv:1801.05230},
  year    = {2018},
  url     = {https://arxiv.org/pdf/1801.05230.pdf}
}

@inproceedings{marton2009icra,
  title     = {On fast surface reconstruction methods for large and noisy point clouds},
  author    = {Z.C. Marton and R.B. Rusu and M. Beetz},
  booktitle = icra,
  year      = {2009},
  url       = {https://ias.informatik.tu-muenchen.de/_media/spezial/bib/marton09icra.pdf}
}

@article{wald2014acmgraphics,
  title   = {Embree: a kernel framework for efficient CPU ray tracing},
  author  = {I. Wald and S. Woop and C. Benthin and G.S. Johnson and M. Ernst},
  journal = acmgraphics,
  volume  = {33},
  number  = {4},
  pages   = {1--8},
  year    = {2014},
  url     = {https://www.embree.org/papers/2014-Siggraph-Embree.pdf}
}

@inproceedings{woo2018eccv,
  author    = {Woo, Sanghyun and Park, Jongchan and Lee, Joon-Young and Kweon, In So},
  title     = {CBAM: Convolutional block attention module},
  booktitle = eccv,
  year      = 2018,
  url       = {proceedings: woo2018eccv}
}

@inproceedings{fan2017cvpr,
  title     = {A point set generation network for 3d object reconstruction from a single image},
  author    = {H. Fan and H. Su and L.J. Guibas},
  booktitle = cvpr,
  year      = {2017},
  url       = {https://arxiv.org/pdf/1612.00603.pdf}
}

@inproceedings{whelan2013iros,
  author    = {T. Whelan and M Kaess and J. Leonard and J. McDonald},
  booktitle = iros,
  title     = {Deformation-based loop closure for large scale dense RGB-D SLAM},
  year      = {2013},
  abstract  = {In this paper we present a system for capturing large scale dense maps in an online setting with a low cost RGB-D sensor. Central to this work is the use of an âas-rigid-as-possibleâ space deformation for efficient dense map correction in a pose graph optimisation framework. By combining pose graph optimisation with non-rigid deformation of a dense map we are able to obtain highly accurate dense maps over large scale trajectories that are both locally and globally consistent. With low latency in mind we derive an incremental method for deformation graph construction, allowing multi-million point maps to be captured over hundreds of metres in real-time. We provide benchmark results on a well established RGB-D SLAM dataset demonstrating the accuracy of the system and also provide a number of our own datasets which cover a wide range of environments, both indoors, outdoors and across multiple floors.},
  url       = {http://thomaswhelan.ie/Whelan13iros.pdf}
}
% end vizzo2021icra references =================================================

@book{basar1998gametheorybook,
  title     = {Dynamic noncooperative game theory},
  author    = {Ba{\c{s}}ar, Tamer and Olsder, Geert Jan},
  year      = {1999},
  volume    = {23},
  publisher = siam
}

@book{nocedal2006optimizationbook,
  title     = {Numerical optimization},
  author    = {Nocedal, Jorge and Wright, Stephen},
  year      = {2006},
  publisher = springer
}

@article{wachter2006jmp,
  title   = {On the implementation of an interior-point filter line-search algorithm for large-scale nonlinear programming},
  author  = {W{\"a}chter, Andreas and Biegler, Lorenz T},
  journal = jmp,
  volume  = {106},
  number  = {1},
  pages   = {25--57},
  year    = {2006}
}

@article{mombaur2010ar,
  title   = {From human to humanoid locomotionâan inverse optimal control approach},
  author  = {Mombaur, Katja and Truong, Anh and Laumond, Jean-Paul},
  journal = ar,
  volume  = {28},
  number  = {3},
  pages   = {369--383},
  year    = {2010}
}

@inproceedings{keshavarz2011isic,
  title     = {Imputing a convex objective function},
  author    = {Keshavarz, Arezou and Wang, Yang and Boyd, Stephen},
  booktitle = isic,
  year      = {2011}
}

@inproceedings{albrecht2011humanoids,
  title     = {Imitating human reaching motions using physically inspired optimization principles},
  author    = {Albrecht, Sebastian and Ramirez-Amaro, Karinne and Ruiz-Ugalde, Federico and Weikersdorfer, David and Leibold, Marion and Ulbrich, Michael and Beetz, Michael},
  booktitle = humanoids,
  year      = {2011}
}

@inproceedings{levine2012icml,
  title     = {Continuous inverse optimal control with locally optimal examples},
  author    = {Levine, Sergey and Koltun, Vladlen},
  booktitle = icml,
  year      = {2012}
}

@article{dunning2017sirev,
  title   = {{JuMP}: A modeling language for mathematical optimization},
  author  = {Dunning, Iain and Huchette, Joey and Lubin, Miles},
  journal = sirev,
  volume  = {59},
  number  = {2},
  pages   = {295--320},
  year    = {2017}
}

@article{kopf2017ifac,
  title   = {Inverse reinforcement learning for identification in linear-quadratic dynamic games},
  author  = {K{\"o}pf, Florian and Inga, Jairo and Rothfu{\ss}, Simon and Flad, Michael and Hohmann, S{\"o}ren},
  journal = ifac,
  volume  = {50},
  number  = {1},
  pages   = {14902--14908},
  year    = {2017}
}

@article{rothfuss2017ifac,
  title   = {Inverse optimal control for identification in non-cooperative differential games},
  author  = {Rothfu{\ss}, Simon and Inga, Jairo and K{\"o}pf, Florian and Flad, Michael and Hohmann, S{\"o}ren},
  journal = ifac,
  volume  = {50},
  number  = {1},
  pages   = {14909--14915},
  year    = {2017}
}

@article{bezanson2017sirev,
  title   = {{J}ulia: A fresh approach to numerical computing},
  author  = {Bezanson, Jeff and Edelman, Alan and Karpinski, Stefan and Shah, Viral B.},
  journal = sirev,
  volume  = {59},
  number  = {1},
  pages   = {65--98},
  year    = {2017}
}

@inproceedings{ng2000icml,
  title     = {Algorithms for inverse reinforcement learning},
  author    = {Ng, Andrew Y. and Russell, Stuart J.},
  booktitle = icml,
  year      = {2000}
}

@inproceedings{wang2019dars,
  title     = {{Game Theoretic Motion Planning for Multi-Robot Racing}},
  author    = {Wang, Zijian and Spica, Riccardo and Schwager, Mac},
  booktitle = dars,
  year      = {2019}
}

@inproceedings{ziebart2008aaai,
  title     = {Maximum entropy inverse reinforcement learning.},
  author    = {Ziebart, Brian D. and Maas, Andrew L. and Bagnell, J. Andrew and Dey, Anind K.},
  booktitle = aaai,
  year      = {2008}
}

@inproceedings{ziebart2011aamas,
  title     = {Maximum Causal Entropy Correlated Equilibria for Markov Games.},
  author    = {Ziebart, Brian D and Bagnell, J Andrew and Dey, Anind K},
  booktitle = aamas,
  year      = {2011}
}

@article{kalman1964jbe,
  author  = {Kalman, Rudolf E.},
  title   = {{When Is a Linear Control System Optimal?}},
  journal = jbe,
  volume  = {86},
  number  = {1},
  pages   = {51--60},
  year    = {1964}
}

@article{englert2018ijrr,
  title   = {Inverse {KKT}: Learning cost functions of manipulation tasks from demonstrations},
  author  = {Englert, Peter and Toussaint, Marc},
  journal = ijrr,
  volume  = {36},
  number  = {13--14},
  pages   = {57--72},
  year    = {2018}
}

@article{schwarting2019pnas,
  title   = {Social behavior for autonomous vehicles},
  author  = {Schwarting, Wilko and Pierson, Alyssa and Alonso-Mora, Javier and Karaman, Sertac and Rus, Daniela},
  journal = pnas,
  volume  = {116},
  number  = {50},
  pages   = {24972--24978},
  year    = {2019}
}

@inproceedings{di2019cdc,
  title     = {{Newton's Method and Differential Dynamic Programming for Unconstrained Nonlinear Dynamic Games}},
  author    = {Di, Bolei and Lamperski, Andrew},
  booktitle = cdc,
  year      = {2019}
}

@inproceedings{cleac2020rss,
  title     = {{ALGAMES}: A fast solver for constrained dynamic games},
  author    = {Le Cleac'h, Simon and Schwager, Mac and Manchester, Zachary},
  booktitle = rss,
  year      = {2020}
}

@mastersthesis{awasthi2019master,
  title  = {Forward and Inverse Methods in Optimal Control and Dynamic Game Theory},
  author = {Awasthi, Chaitanya},
  school = {University of Minnesota},
  year   = {2019}
}

@article{inga2019arxiv,
  title   = {Inverse Dynamic Games Based on Maximum Entropy Inverse Reinforcement Learning},
  author  = {Inga, Jairo and Bischoff, Esther and K{\"o}pf, Florian and Hohmann, S{\"o}ren},
  journal = arxiv,
  volume  = {arXiv:1911.07503},
  year    = {2019}
}

@inproceedings{fridovich2020icra,
  title     = {Efficient iterative linear-quadratic approximations for nonlinear multi-player general-sum differential games},
  author    = {Fridovich-Keil, David and Ratner, Ellis and Peters, Lasse and Dragan, Anca D. and Tomlin, Claire J.},
  booktitle = icra,
  year      = {2020}
}

@inproceedings{fridovich2020icra-aiqm,
  title     = {An iterative quadratic method for general-sum differential games with feedback linearizable dynamics},
  author    = {Fridovich-Keil, David and Rubies-Royo, Vicenc and Tomlin, Claire J.},
  booktitle = icra,
  year      = {2020}
}

@article{menner2020arxiv,
  title   = {Maximum Likelihood Methods for Inverse Learning of Optimal Controllers},
  author  = {Menner, Marcel and Zeilinger, Melanie N.},
  journal = arxiv,
  volume  = {arXiv:2005.02767},
  year    = {2020}
}

@mastersthesis{peters2020master,
  author = {Peters, Lasse},
  title  = {Accommodating Intention Uncertainty in General-Sum Games for Human-Robot Interaction},
  year   = {2020},
  school = {{Hamburg University of Technology}}
}

@article{cleac2020arxiv,
  title   = {{LUCIDGames}: Online Unscented Inverse Dynamic Games for Adaptive Trajectory Prediction and Planning},
  author  = {Le Cleac'h, Simon and Schwager, Mac and Manchester, Zachary},
  journal = ral,
  volume  = {6},
  number  = {3},
  pages   = {5485--5492},
  year    = {2021}
}

@inproceedings{awasthi2020acc,
  title     = {Inverse Differential Games With Mixed Inequality Constraints},
  author    = {Awasthi, Chaitanya and Lamperski, Andrew},
  booktitle = acc,
  year      = {2020}
}

@article{laine2021arxiv,
  title   = {The Computation of Approximate Generalized Feedback Nash Equilibria},
  author  = {Laine, Forrest and Fridovich-Keil, David and Chiu, Chih-Yuan and Tomlin, Claire},
  journal = arxiv,
  volume  = {arXiv:2101.02900},
  year    = {2021}
}

@inproceedings{chen2021icra,
  title     = {{Range Image-based LiDAR Localization for Autonomous Vehicles}},
  author    = {X. Chen and I. Vizzo and T. L\"abe and J. Behley and C. Stachniss},
  booktitle = icra,
  year      = {2021},
  url       = {http://www.ipb.uni-bonn.de/pdfs/chen2021icra.pdf}
}

@inproceedings{reinke2021icra,
  title     = {{Simple But Effective Redundant Odometry for Autonomous Vehicles}},
  author    = {A. Reinke and X. Chen and C. Stachniss},
  booktitle = icra,
  year      = {2021}
}

@inproceedings{nikhil2018eccvws,
  author    = {N. Nikhil and B.T. Morris},
  title     = {{Convolutional Neural Network for Trajectory Prediction}},
  booktitle = eccvws,
  year      = {2018},
  url       = {https://openaccess.thecvf.com/content_ECCVW_2018/papers/11131/Nikhil_Convolutional_Neural_Network_for_Trajectory_Prediction_ECCVW_2018_paper.pdf}
}

@inproceedings{sodhi2018iros,
  title     = {Robust Plant Phenotyping via Model-Based Optimization},
  author    = {P. Sodhi and H. Sun and B. P{\'o}czos and D. Wettergreen},
  booktitle = iros,
  year      = {2018},
  url       = {https://www.ri.cmu.edu/wp-content/uploads/2018/10/Sodhi18iros.pdf},
  abstract  = {Plant phenotyping is the measurement of observ-able plant traits. Current methods for phenotyping in the fieldare  labour  intensive  and  error  prone.  High  throughput  plantphenotyping in an automated and noninvasive manner is crucialto accelerating plant breeding methods.Occlusions and non-ideal sensing conditions is a major prob-lem for high throughput plant phenotyping with most state-of-the-art 3D phenotyping algorithms relying heavily on heuristicsor hand-tuned parameters. To address this problem, we presenta novel model-based optimization approach for estimating plantphysical traits from plant units called phytomers. The proposedapproach  involves  sampling  parameterized  3D  plant  modelsfrom an underlying probability distribution. It then optimizes,making the mass of this probability distribution approach trueparameters of the model.Reformulating   the   phenotyping   objective   as   a   search   inthe   space   of   plant   models   lets   us   reason   about   the   plantstructure   in   a   holistic   manner   without   having   to   rely   onhand-tuned  parameters.  This  makes  our  approach  robust  tonoise  and  occlusions  as  frequently  encountered  in  real  worldenvironments. We evaluate our approach for plant units takenacross simulated, greenhouse and field environments. This workfurthers field-based robotic phenotyping capabilities paving theway for plant biologists to study the coupled effect of geneticsand environment on improving crop yields.}
}

@article{weyler2021ral,
  author  = {J. Weyler and A. Milioto and T. Falck and J. Behley and C. Stachniss},
  title   = {{Joint Plant Instance Detection and Leaf Count Estimation for In-Field Plant Phenotyping}},
  journal = ral,
  volume  = 6,
  number  = 2,
  pages   = {3599--3606},
  year    = {2021}
}

@article{chebrolu2021ral,
  author  = {N. Chebrolu and T. L\"{a}be and O. Vysotska and J. Behley and C. Stachniss},
  title   = {{Adaptive Robust Kernels for Non-Linear Least Squares Problems}},
  journal = ral,
  volume  = 6,
  number  = 2,
  pages   = {2240--2247},
  year    = 2021
}

@article{chebrolu2021plosone,
  author  = {N. Chebrolu and F. Magistri and T. L{\"a}be and C. Stachniss},
  title   = {{Registration of Spatio-Temporal Point Clouds of Plants for Phenotyping}},
  journal = plosone,
  year    = 2021,
  number  = {2},
  volume  = {16},
  url     = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0247243}
}

@article{potena2019ral,
  author  = {C. Potena and R. Khanna and J. Nieto and R. Siegwart and D. Nardi and A. Pretto},
  journal = ral,
  title   = {AgriColMap: Aerial-Ground Collaborative 3D Mapping for Precision Farming},
  year    = {2019},
  volume  = {4},
  number  = {2},
  pages   = {1085--1092}
}

@inproceedings{triggs1999iccv,
  author    = {B. Triggs and P. F. McLauchlan and R. I. Hartley and A. W. Fitzgibbon},
  title     = {Bundle Adjustment - A Modern Synthesis},
  year      = {1999},
  booktitle = {Proc. of the International Workshop on Vision Algorithms: Theory and Practice}
}

@article{maurer2003pami,
  author  = {C. R. {Maurer} and  {Rensheng Qi} and V. {Raghavan}},
  journal = pami,
  title   = {A linear time algorithm for computing exact Euclidean distance transforms of binary images in arbitrary dimensions},
  year    = {2003},
  volume  = {25},
  number  = {2},
  pages   = {265--270}
}


@article{paulus2013bmc,
  title   = {Surface feature based classification of plant organs from 3D laserscanned point clouds for plant phenotyping},
  author  = {S. Paulus and J. Dupuisand A. Mahlein and H. Kuhlmann},
  journal = bmcbio,
  volume  = {14},
  number  = {1},
  pages   = {238},
  year    = {2013}
}

@article{shi2019bioeng,
  title   = {Plant-part segmentation using deep learning and multi-view vision},
  author  = {W. Shi and R. van de Zedde and H. Jiang and G. Kootstra},
  journal = biosyseng,
  volume  = {187},
  pages   = {81--95},
  year    = {2019}
}

@article{rabiner1989ieee,
  author  = {L. R. {Rabiner}},
  journal = {Proc. of the IEEE},
  title   = {A tutorial on hidden Markov models and selected applications in speech recognition},
  year    = {1989},
  volume  = {77},
  number  = {2},
  pages   = {257--286}
}

@inproceedings{zach2014eccv,
  title     = {Robust bundle adjustment revisited},
  author    = {Zach, C.},
  booktitle = eccv,
  year      = {2014}
}

@inproceedings{babin2019icra,
  title     = {Analysis of Robust Functions for Registration Algorithms},
  author    = {Babin, P. and Giguere, P. and Pomerleau, F.},
  booktitle = icra,
  year      = {2019}
}

@article{huber1964kernel,
  author  = {Huber, P. J.},
  title   = {Robust estimation of a location parameter},
  journal = {Annals of Mathematical Statistics},
  number  = 1,
  pages   = {73--101},
  volume  = 35,
  year    = 1964
}

@article{zhang1997ivc,
  author  = {Zhang, Z.},
  title   = {Parameter estimation techniques: A tutorial with application to conic fitting},
  year    = {1997},
  pages   = {59--76},
  journal = {Image and Vision Computing},
  volume  = {15}
}

@inproceedings{aravkin2012icip,
  author    = {A. {Aravkin} and M. {Styer} and Z. {Moratto} and A. {Nefian} and M. {Broxton}},
  booktitle = icip,
  title     = {Student's t robust bundle adjustment algorithm},
  year      = {2012}
}

@article{dellaert2006ijrr,
  author  = {F. Dellaert and M. Kaess},
  title   = {Square Root SAM: Simultaneous Localization and Mapping via Square Root Information Smoothing},
  journal = ijrr,
  volume  = {25},
  number  = {12},
  pages   = {1181--1203},
  year    = {2006}
}


@article{kaess2008tro,
  author  = {M. Kaess and A. Ranganathan and F. Dellaert},
  title   = {{iSAM}: Incremental Smoothing and Mapping},
  journal = tro,
  volume  = {24},
  number  = {6},
  pages   = {1365--1378},
  year    = 2008
}

@inproceedings{dickscheid2008isprs,
  author    = {Dickscheid, T. and L\"abe, T. and F\"orstner, W.},
  title     = {{Benchmarking Automatic Bundle Adjustment Results}},
  booktitle = {Cong.~of the Intl.~Society for Photogrammetry and Remote Sensing (ISPRS)},
  year      = {2008},
  url       = {http://www.ipb.uni-bonn.de/pdfs/Dickscheid2008Benchmarking.pdf}
}

@book{koch1988book,
  author    = {Koch, K. R.},
  title     = {{Parameter Estimation and Hypothesis Testing in Linear Models}},
  year      = {1988},
  publisher = {Springer-Verlag}
}

@inproceedings{schneider2012isprs,
  title     = {Bundle Adjustment for Multi-camera Systems with Points at Infinity},
  author    = {J. Schneider and F. Schindler and T. L\"abe and W. F\"orstner},
  booktitle = {ISPRS Annals of Photogrammetry, Remote Sensing and Spatial Information Sciences},
  year      = {2012},
  url       = {http://www.isprs-ann-photogramm-remote-sens-spatial-inf-sci.net/I-3/75/2012/isprsannals-I-3-75-2012.pdf}
}

@article{yang2020ral,
  author  = {Heng Yang, Pasquale Antonante, Vasileios Tzoumas, Luca Carlone},
  title   = {Graduated Non-Convexity for Robust Spatial Perception: From Non-Minimal Solvers to Global Outlier Rejection},
  volume  = 5,
  number  = 2,
  pages   = {1127--1134},
  url     = {https://arxiv.org/pdf/1909.08605.pdf},
  journal = ral,
  year    = {2020}
}

@article{lajoie2019ral,
  author  = {P. Lajoie and S. Hu and G. Beltrame and L. Carlone},
  title   = {{Modeling Perceptual Aliasing in {SLAM} via Discrete-Continuous Graphical Models}},
  journal = ral,
  volume  = {4},
  number  = {2},
  pages   = {1232--1239},
  url     = {https://arxiv.org/pdf/1810.11692.pdf},
  year    = 2019
}


@book{hampel1986book,
  author    = {F.R. Hampel and E.M. Ronchetti and P. J. Rousseeuw and W. A. Stahel},
  title     = {{Robust Statistics: The Approach Based on Influence Functions}},
  publisher = {John Wiley and Sons},
  year      = 1986
}

@article{black1996ijcv,
  author  = {M. J. Black  and A. Rangarajan},
  title   = {On the unification of line processes, outlier rejection, and robust statistics with applications in early vision},
  journal = ijcv,
  volume  = {19},
  number  = {1},
  pages   = {57--91},
  year    = {1996}
}

@inproceedings{zhou2020eccv-apollo,
  title     = {DA4AD: End-to-end Deep Attention-based Visual Localization for Autonomous Driving},
  author    = {Y. Zhou and G. Wan and S. Hou and L. Yu and G. Wang and X. Rui and S. Song},
  booktitle = eccv,
  year      = {2020},
  url       = {https://arxiv.org/pdf/2003.03026.pdf}
}

           
@inproceedings{vizzo2021icra,
  author    = {I. Vizzo and X. Chen and N. Chebrolu and J. Behley and C. Stachniss},
  title     = {{Poisson Surface Reconstruction for LiDAR Odometry and Mapping}},
  booktitle = icra,
  year      = 2021,
  url       = {http://www.ipb.uni-bonn.de/pdfs/vizzo2021icra.pdf}
}

@inproceedings{lu2021aaai,
  title     = {{PointINet: Point Cloud Frame Interpolation Network}},
  author    = {Lu, Fan and Chen, Guang and Qu, Sanqing and Li, Zhijun and Liu, Yinlong and Knoll, Alois},
  booktitle = aaai,
  year      = {2021},
  url       = {https://arxiv.org/pdf/2012.10066.pdf}
}

@article{lu2020arxiv,
  title   = {{MoNet: Motion-based Point Cloud Prediction Network}},
  author  = {Lu, Fan and Chen, Guang and Liu, Yinlong and Li, Zhijun and Qu, Sanqing and Zou, Tianpei},
  journal = arxiv,
  volume  = {arXiv:2011.10812},
  year    = {2020},
  url     = {https://arxiv.org/pdf/2011.10812.pdf}
}

@inproceedings{zhang2021cvpr,
  title     = {{We are More than Our Joints: Predicting how 3D Bodies Move}},
  author    = {Zhang, Yan and Black, Michael J and Tang, Siyu},
  booktitle = cvpr,
  year      = {2021},
  abstract  = {A key step towards understanding human behavior is the
               prediction of 3D human motion. Successful solutions have many applications
               in human tracking, HCI, and graphics. Most previous work focuses on
               predicting a time series of future 3D joint locations given a sequence 3D
               joints from the past. This Euclidean formulation generally works better
               than predicting pose in terms of joint rotations. Body joint locations,
               however, do not fully constrain 3D human pose, leaving degrees of freedom
               (like rotation about a limb) undefined. Note that 3D joints can be viewed
               as a sparse point cloud. Thus the problem of human motion prediction can
               be seen as a problem of point cloud prediction. With this observation, we
               instead predict a sparse set of locations on the body surface that
               correspond to motion capture markers. Given such markers, we fit a
               parametric body model to recover the 3D body of the person. These sparse
               surface markers also carry detailed information about human movement that
               is not present in the joints, increasing the naturalness of the predicted
               motions. Using the AMASS dataset, we train MOJO (More than Our JOints),
               which is a novel variational autoencoder with a latent DCT space that
               generates motions from latent frequencies. MOJO preserves the full
               temporal resolution of the input motion, and sampling from the latent
               frequencies explicitly introduces high-frequency components into the
               generated motion. We note that motion prediction methods accumulate errors
               over time, resulting in joints or markers that diverge from true human
               bodies. To address this, we fit the SMPL-X body model to the predictions
               at each time step, projecting the solution back onto the space of valid
               bodies, before propagating the new markers in time. Quantitative and
               qualitative experiments show that our approach produces state-of-the-art
               results and realistic 3D body animations. The code is available for
               research purposes at  https://yz-cnsdqz.github.io/MOJO/MOJO.html .},
  url       = {proceedings: zhang2021cvpr-wamt.pdf}
}

@inproceedings{weng2020corl,
  title     = {{Inverting the Pose Forecasting Pipeline with SPF2: Sequential Pointcloud Forecasting for Sequential Pose Forecasting}},
  author    = {Weng, Xinshuo and Wang, Jianren and Levine, Sergey and Kitani, Kris and Rhinehart, Nick},
  booktitle = corl,
  year      = {2020},
  url       = {https://www.xinshuoweng.com/papers/SPF2/camera_ready.pdf}
}

@inproceedings{weng2020eccvws,
  title     = {{4D Forecasting: Sequential Forecasting of 100,000 Points}},
  author    = {Xinshuo Weng and Jianren Wang and Sergey Levine and Kris Kitani and Nicholas Rhinehart},
  booktitle = eccvws,
  year      = {2020},
  url       = {https://www.xinshuoweng.com/papers/SPF2_eccvw/camera_ready.pdf}
}


@inproceedings{caccia2019iros,
  title     = {{Deep Generative Modeling of Lidar Data}},
  author    = {Caccia, Lucas and Van Hoof, Herke and Courville, Aaron and Pineau, Joelle},
  booktitle = iros,
  year      = {2019},
  url       = {https://arxiv.org/pdf/1812.01180.pdf}
}

@article{eskandar2021arxiv,
  title   = {{SLPC: a VRNN-based Approach for Stochastic Lidar prediction and Completion in Autonomous Driving}},
  author  = {George Eskandar and Alexander Braun and M. Meinke and Karim Armanious and Bin Yang},
  journal = arxiv,
  year    = {2021},
  volume  = {arXiv:2102.09883},
  url     = {https://arxiv.org/pdf/2102.09883.pdf}
}

@article{fan2019arxiv,
  title   = {{PointRNN: Point Recurrent Neural Network for Moving Point Cloud Processing}},
  author  = {Hehe Fan and Yezhou Yang},
  journal = arxiv,
  year    = {2019},
  volume  = {arXiv:1910.08287},
  url     = {https://arxiv.org/pdf/1910.08287.pdf}
}

@article{zhang2019arxiv,
  title   = {{CloudLSTM: A Recurrent Neural Model for Spatiotemporal Point-cloud Stream Forecasting}},
  author  = {Chaoyun Zhang and M. Fiore and Iain Murray and P. Patras},
  journal = arxiv,
  year    = {2019},
  volume  = {arXiv:1907.12410},
  url     = {https://arxiv.org/pdf/1907.12410.pdf}
}

@inproceedings{deng2020threedv,
  title     = {{Temporal LiDAR Frame Prediction for Autonomous Driving}},
  author    = {David Deng and Avideh Zakhor},
  booktitle = threedv,
  year      = {2020},
  url       = {https://arxiv.org/pdf/2012.09409.pdf}
}

@inproceedings{wu2020cvpr,
  title     = {{MotionNet: Joint Perception and Motion Prediction for Autonomous Driving Based on Birdâs Eye View Maps}},
  author    = {Pengxiang Wu and Siheng Chen and Dimitris N. Metaxas},
  booktitle = cvpr,
  year      = {2020},
  abstract  = {The ability to reliably perceive the environmental states,
               particularly the existence of objects and their motion behavior, is crucial
               for autonomous driving. In this work, we propose an efficient deep model,
               called MotionNet, to jointly perform perception and motion prediction from
               3D point clouds. MotionNet takes a sequence of LiDAR sweeps as input and
               outputs a bird's eye view (BEV) map, which encodes the object category and
               motion information in each grid cell. The backbone of MotionNet is a novel
               spatio-temporal pyramid network, which extracts deep spatial and temporal
               features in a hierarchical fashion. To enforce the smoothness of predictions
               over both space and time, the training of MotionNet is further regularized
               with novel spatial and temporal consistency losses. Extensive experiments
               show that the proposed method overall outperforms the state-of-the-arts,
               including the latest scene-flow- and 3D-object-detection-based methods. This
               indicates the potential value of the proposed method serving as a backup to
               the bounding-box-based system, and providing complementary information to
               the motion planner in autonomous driving. Code is available at
               https://www.merl.com/research/license#MotionNet.},
  url       = {proceedings: wu2020cvpr-mjpa.pdf}
}

@inproceedings{wu2021cvpr,
  author    = {Wu, Shun-Cheng and Wald, Johanna and Tateno, Keisuke and Navab, Nassir and Tombari, Federico},
  title     = {{SceneGraphFusion: Incremental 3D Scene Graph Prediction From RGB-D Sequences}},
  booktitle = cvpr,
  year      = {2021},
  url       = {proceedings: wu2021cvpr}
}

@inproceedings{aigner2018cvpr,
  title     = {{FutureGAN: Anticipating the Future Frames of Video Sequences using Spatio-Temporal 3d Convolutions in Progressively Growing GANs}},
  author    = {Sandra Aigner and Marco Korner},
  booktitle = cvpr,
  year      = {2018},
  url       = {https://arxiv.org/pdf/1810.01325.pdf}
}

@inproceedings{toyungyernsub2021icra,
  title     = {{Double-Prong ConvLSTM for Spatiotemporal Occupancy Prediction in Dynamic Environments}},
  author    = {Maneekwan Toyungyernsub and M. Itkina and Ransalu Senanayake and Mykel J. Kochenderfer},
  booktitle = icra,
  year      = {2021},
  url       = {https://arxiv.org/pdf/2011.09045.pdf}
}

@inproceedings{meyer2021icra,
  title     = {{LaserFlow: Efficient and Probabilistic Object Detection and Motion Forecasting}},
  author    = {Gregory P. Meyer and Jake Charland and Shreyash Pandey and A. Laddha and Shivam Gautam and Carlos Vallespi-Gonzalez and Carl K. Wellington},
  booktitle = icra,
  year      = {2021},
  url       = {proceedings:meyer2021ral.pdf}
}

@article{laddha2020arxiv,
  title   = {{RV-FuseNet: Range View based Fusion of Time-Series LiDAR Data for Joint 3D Object Detection and Motion Forecasting}},
  author  = {A. Laddha and Shivam Gautam and Gregory P. Meyer and Carlos Vallespi-Gonzalez},
  journal = arxiv,
  year    = {2020},
  volume  = {arXiv:2005.10863},
  url     = {https://arxiv.org/pdf/2005.10863.pdf}
}

@inproceedings{liang2020cvpr,
  author    = {Liang, Ming and Yang, Bin and Zeng, Wenyuan and Chen, Yun and Hu, Rui and Casas, Sergio and Urtasun, Raquel},
  title     = {{PnPNet: End-to-End Perception and Prediction With Tracking in the Loop}},
  booktitle = cvpr,
  year      = {2020},
  abstract  = {We tackle the problem of joint perception and motion forecasting
               in the context of self-driving vehicles. Towards this goal we propose PnPNet,
               an end-to-end model that takes as input sequential sensor data, and outputs at
               each time step object tracks and their future trajectories. The key component
               is a novel tracking module that generates object tracks online from detections
               and exploits trajectory level features for motion forecasting. Specifically,
               the object tracks get updated at each time step by solving both the data
               association problem and the trajectory estimation problem. Importantly, the
               whole model is end-to-end trainable and benefits from joint optimization of
               all tasks. We validate PnPNet on two large-scale driving datasets, and show
               significant improvements over the state-of-the-art with better occlusion
               recovery and more accurate future prediction.},
  url       = {proceedings:liang2020cvpr-pepa.pdf}
}

@inproceedings{shi2015nips,
  title     = {{Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting}},
  author    = {Xingjian Shi and Zhourong Chen and Hao Wang and Dit-Yan Yeung and Wai-Kin Wong and Wang-Chun Woo},
  booktitle = nips,
  year      = {2015},
  url       = {https://arxiv.org/pdf/1506.04214.pdf}
}

@inproceedings{srivastava2015icml,
  title     = {{Unsupervised Learning of Video Representations using LSTMs}},
  author    = {Nitish Srivastava and Elman Mansimov and Ruslan Salakhutdinov},
  booktitle = icml,
  year      = {2015},
  url       = {https://www.cs.toronto.edu/~nitish/unsup_video.pdf}
}

@article{wei2018arxiv,
  title   = {{Novel Video Prediction for Large-scale Scene using Optical Flow}},
  author  = {Henglai Wei and Xiaochuan Yin and Penghong Lin},
  journal = arxiv,
  year    = {2018},
  volume  = {arXiv:1805.12243},
  url     = {https://arxiv.org/pdf/1805.12243.pdf}
}

@inproceedings{gu2019cvpr,
  title     = {{HPLFlowNet: Hierarchical Permutohedral Lattice FlowNet for Scene Flow Estimation on Large-Scale Point Clouds}},
  author    = {Xiuye Gu and Y. Wang and Chongruo Wu and Y. Lee and Panqu Wang},
  booktitle = cvpr,
  year      = {2019},
  abstract  = {We present a novel deep neural network architecture for end-to-end scene flow estimation that directly 
               operates on large-scale 3D point clouds. Inspired by Bilateral Convolutional Layers (BCL), we propose novel DownBCL, 
               UpBCL, and CorrBCL operations that restore structural information from unstructured point clouds, and fuse information 
               from two consecutive point clouds. Operating on discrete and sparse permutohedral lattice points, our architectural 
               design is parsimonious in computational cost. Our model can efficiently process a pair of point cloud frames at once 
               with a maximum of 86K points per frame. Our approach achieves state-of-the-art performance on the FlyingThings3D and 
               KITTI Scene Flow 2015 datasets. Moreover, trained on synthetic data, our approach shows great generalization ability on 
               real-world data and on different point densities without fine-tuning.},
  url       = {proceedings: gu2019cvpr-hhpl.pdf}
}

@inproceedings{liu2022cvpr,
  title     = {A convnet for the 2020s},
  author    = {Liu, Zhuang and Mao, Hanzi and Wu, Chao-Yuan and Feichtenhofer, Christoph and Darrell, Trevor and Xie, Saining},
  booktitle = cvpr,
  year      = {2022},
  url       = {https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_A_ConvNet_for_the_2020s_CVPR_2022_paper.pdf}
}

@inproceedings{liu2019iccv,
  title     = {{MeteorNet: Deep Learning on Dynamic 3D Point Cloud Sequences}},
  author    = {Xingyu Liu and Mengyuan Yan and Jeannette Bohg},
  booktitle = iccv,
  year      = {2019},
  abstract  = {Understanding dynamic 3D environment is crucial for robotic agents and many other applications. We propose 
               a novel neural network architecture called MeteorNet for learning representations for dynamic 3D point cloud sequences. 
               Different from previous work that adopts a grid-based representation and applies 3D or 4D convolutions, our network 
               directly processes point clouds. We propose two ways to construct spatiotemporal neighborhoods for each point in the 
               point cloud sequence. Information from these neighborhoods is aggregated to learn features per point. We benchmark our 
               network on a variety of 3D recognition tasks including action recognition, semantic segmentation and scene flow estimation. 
               MeteorNet shows stronger performance than previous grid-based methods while achieving state-of-the-art performance on 
               Synthia. MeteorNet also outperforms previous baseline methods that are able to process at most two consecutive point clouds. 
               To the best of our knowledge, this is the first work on deep learning for dynamic raw point cloud sequences.},
  url       = {proceedings: liu2019iccv-mdlo.pdf}
}

@inproceedings{song2019icra,
  title     = {{2D LiDAR Map Prediction via Estimating Motion Flow with GRU}},
  author    = {Yafei Song and Yonghong Tian and G. Wang and Mingyang Li},
  booktitle = icra,
  year      = {2019},
  keywords  = {Visual Tracking, Deep Learning in Robotics and Automation, Object Detection, Segmentation and Categorization},
  abstract  = {It is a significant problem to predict the 2D LiDAR map at next moment for robotics navigation and path-planning. To tackle this problem, we resort to the motion flow between adjacent maps, as motion flow is a powerful tool to process and analyze the dynamic data, which is named optical flow in video processing. However, unlike video, which contains abundant visual features in each frame, a 2D LiDAR map lacks distinctive local features. To alleviate this challenge, we propose to estimate the motion flow based on deep neural networks inspired by its powerful representation learning ability in estimating the optical flow of the video. To this end, we design a recurrent neural network based on gated recurrent unit, which is named LiDAR-FlowNet. As a recurrent neural network can encode the temporal dynamic information, our LiDAR-FlowNet can estimate motion flow between the current map and the unknown next map only from the current frame and previous frames. A self-supervised strategy is further designed to train the LiDARFlowNet model effectively, while no training data need to be manually annotated. With the estimated motion flow, it is straightforward to predict the 2D LiDAR map at the next moment. Experimental results verify the effectiveness of our LiDAR-FlowNet as well as the proposed training strategy. The results of the predicted LiDAR map also show the advantages of our motion flow based method.},
  url       = {proceedings: song2019icra-2lmpve.pdf}
}

@inproceedings{song2019icra-2lmpve,
  author    = {Y. Song and Y. Tian and G. Wang and M. Li},
  title     = {{2D LiDAR Map Prediction Via Estimating Motion Flow with GRU}},
  booktitle = icra,
  year      = 2019,
  keywords  = {Visual Tracking, Deep Learning in Robotics and Automation, Object Detection, Segmentation and Categorization},
  abstract  = {It is a significant problem to predict the 2D LiDAR map at next moment for robotics navigation and path-planning. To tackle this problem, we resort to the motion flow between adjacent maps, as motion flow is a powerful tool to process and analyze the dynamic data, which is named optical flow in video processing. However, unlike video, which contains abundant visual features in each frame, a 2D LiDAR map lacks distinctive local features. To alleviate this challenge, we propose to estimate the motion flow based on deep neural networks inspired by its powerful representation learning ability in estimating the optical flow of the video. To this end, we design a recurrent neural network based on gated recurrent unit, which is named LiDAR-FlowNet. As a recurrent neural network can encode the temporal dynamic information, our LiDAR-FlowNet can estimate motion flow between the current map and the unknown next map only from the current frame and previous frames. A self-supervised strategy is further designed to train the LiDARFlowNet model effectively, while no training data need to be manually annotated. With the estimated motion flow, it is straightforward to predict the 2D LiDAR map at the next moment. Experimental results verify the effectiveness of our LiDAR-FlowNet as well as the proposed training strategy. The results of the predicted LiDAR map also show the advantages of our motion flow based method.}
}

@inproceedings{liu2019cvpr,
  title     = {{FlowNet3D: Learning Scene Flow in 3D Point Clouds}},
  author    = {Xingyu Liu and Charles R Qi and Leonidas J Guibas},
  booktitle = cvpr,
  year      = {2019},
  abstract  = {Many applications in robotics and human-computer interaction can benefit from understanding 3D motion of points in a dynamic environment, widely noted as scene flow. While most previous methods focus on stereo and RGB-D images as input, few try to estimate scene flow directly from point clouds. In this work, we propose a novel deep neural network named FlowNet3D that learns scene flow from point clouds in an end-to-end fashion. Our network simultaneously learns deep hierarchical features of point clouds and flow embeddings that represent point motions, supported by two newly proposed learning layers for point sets. We evaluate the network on both challenging synthetic data from FlyingThings3D and real Lidar scans from KITTI. Trained on synthetic data only, our network successfully generalizes to real scans, outperforming various baselines and showing competitive results to the prior art. We also demonstrate two applications of our scene flow output (scan registration and motion segmentation) to show its potential wide use cases.},
  url       = {proceedings: liu2019cvpr-flsf.pdf}
}

@inproceedings{wang2019iclr,
  title     = {{Eidetic 3D LSTM: A Model for Video Prediction and Beyond}},
  author    = {Yunbo Wang and Lu Jiang and Ming-Hsuan Yang and Li-Jia Li and Mingsheng Long and Li Fei-Fei},
  booktitle = iclr,
  year      = {2019},
  url       = {https://openreview.net/pdf?id=B1lKS2AqtX}
}

@inproceedings{ushani2018corl,
  title     = {{Feature Learning for Scene Flow Estimation from LIDAR}},
  author    = {Arash K. Ushani and Ryan M. Eustice},
  booktitle = corl,
  year      = {2018},
  url       = {http://proceedings.mlr.press/v87/ushani18a/ushani18a.pdf}
}

@article{chen2021arxiv,
  title   = {{Moving Object Segmentation in 3D LiDAR Data: A Learning-based Approach Exploiting Sequential Data}},
  author  = {Xieyuanli Chen and Shijie Li and Benedikt Mersch and Louis Wiesmann and JÃ¼rgen Gall and Jens Behley and Cyrill Stachniss},
  year    = {2021},
  journal = arxiv,
  volume  = {arXiv:2105.08971},
  url     = {http://www.ipb.uni-bonn.de/pdfs/chen2021ral-iros.pdf}
}
@inproceedings{yin2021cvpr,
  author    = {T. Yin and X. Zhou and P. Kr\"ahenb\"uhl},
  title     = {{Center-Based 3D Object Detection and Tracking}},
  booktitle = cvpr,
  year      = 2021,
  abstract  = {Three-dimensional objects are commonly represented as 3D boxes in a point-cloud. 
               This representation mimics the well-studied image-based 2D bounding-box detection but comes with additional 
               challenges. Objects in a 3D world do not follow any particular orientation, and box-based detectors have 
               difficulties enumerating all orientations or fitting an axis-aligned bounding box to rotated objects. In this 
               paper, we instead propose to represent, detect, and track 3D objects as points. Our framework, CenterPoint, 
               first detects centers of objects using a keypoint detector and regresses to other attributes, including 3D size, 
               3D orientation, and velocity. In a second stage, it refines these estimates using additional point features on 
               the object. In CenterPoint, 3D object tracking simplifies to greedy closest-point matching. The resulting 
               detection and tracking algorithm is simple, efficient, and effective. On the nuScenes and Waymo datasets, 
               CenterPoint surpasses prior methods by a large margin. On the Waymo Open Dataset, CenterPoint improves previous 
               tate-of-the-art by 10-20\% while running at 13FPS. The code and pretrained models are available 
               at https://github.com/tianweiy/CenterPoint.},
  url       = {proceedings: yin2021cvpr-coda.pdf}
}

@inproceedings{xu2020corl,
  title     = {{SelfVoxeLO: Self-supervised LiDAR Odometry with Voxel-based Deep Neural Networks}},
  author    = {Yan Xu and Zhaoyang Huang and Kwan-Yee Lin and Xinge Zhu and Jianping Shi and Hujun Bao and Guofeng Zhang and Hongsheng Li},
  booktitle = corl,
  year      = {2020},
  url       = {https://arxiv.org/pdf/2010.09343.pdf}
}

@article{zhai2021pr,
  title   = {{Optical Flow and Scene Flow Estimation: A Survey}},
  author  = {Mingliang Zhai and Xiang Xuezhi and Lv Ning and Xiangdong Kong},
  year    = {2021},
  pages   = {107861},
  volume  = {114},
  journal = pr
}

@inproceedings{triess2020iv,
  title     = {{Scan-based Semantic Segmentation of LiDAR Point Clouds: An Experimental Study}},
  author    = {Larissa T. Triess and David Peter and Christoph B. Rist and Johann Marius Z{\"o}llner},
  booktitle = iv,
  year      = {2020},
  url       = {https://arxiv.org/pdf/2004.11803.pdf},
  abstract  = {Autonomous vehicles need to have a semantic understanding of the three-dimensional world around them in 
               order to reason about their environment. State of the art methods use deep neural networks to predict semantic classes 
               for each point in a LiDAR scan. A powerful and efficient way to process LiDAR measurements is to use two-dimensional, 
               image-like projections. In this work, we perform a comprehensive experimental study of image-based semantic segmentation 
               architectures for LiDAR point clouds. We demonstrate various techniques to boost the performance and to improve runtime 
               as well as memory constraints. First, we examine the effect of network size and suggest that much faster inference times 
               can be achieved at a very low cost to accuracy. Next, we introduce an improved point cloud projection technique that does 
               not suffer from systematic occlusions. We use a cyclic padding mechanism that provides context at the horizontal 
               field-of-view boundaries. In a third part, we perform experiments with a soft Dice loss function that directly 
               optimizes for the intersection-over-union metric. Finally, we propose a new kind of convolution layer with a reduced 
               amount of weight-sharing along one of the two spatial dimensions, addressing the large difference in appearance 
               along the vertical axis of a LiDAR scan. We propose a final set of the above methods with which the model achieves
               an increase of 3.2\% in mIoU segmentation performance over the baseline while requiring only 42\% of the original inference time.}
}

@article{rubner2004ijcv,
  title   = {{The Earth Mover's Distance as a Metric for Image Retrieval}},
  author  = {Y. Rubner and Carlo Tomasi and L. Guibas},
  journal = ijcv,
  year    = {2004},
  volume  = {40},
  pages   = {99--121},
  url     = {https://www.cs.cmu.edu/~efros/courses/LBMV07/Papers/rubner-jcviu-00.pdf}
}

@inproceedings{laina2016threedv,
  title     = {{Deeper Depth Prediction with Fully Convolutional Residual Networks}},
  author    = {Laina, Iro and Rupprecht, Christian and Belagiannis, Vasileios and Tombari, Federico and Navab, Nassir},
  booktitle = threedv,
  year      = {2016},
  url       = {https://arxiv.org/pdf/1606.00373.pdf}
}

@article{liu2016pami,
  author  = {Fayao Liu and Chunhua Shen and Guosheng Lin and Ian Reid},
  title   = {{Learning Depth from Single Monocular Images Using Deep Convolutional Neural Fields}},
  journal = pami,
  volume  = {38},
  pages   = {2024--2039},
  number  = {10},
  url     = {https://arxiv.org/pdf/1502.07411.pdf},
  year    = {2016}
}

@inproceedings{kendall2017nips,
  author    = {Kendall, Alex and Gal, Yarin},
  booktitle = nips,
  title     = {{What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?}},
  url       = {https://proceedings.neurips.cc/paper/2017/file/2650d6089a6d640c5e85b2b88265dc2b-Paper.pdf},
  year      = {2017}
}

@inproceedings{wang2017iccv,
  title     = {{Stereo DSO: Large-Scale Direct Sparse Visual Odometry with Stereo Cameras}},
  author    = {R. Wang and M. SchwÃ¶rer and D. Cremers},
  booktitle = iccv,
  year      = {2017},
  keywords  = {dso, visual odometry, stereo, 3D reconstruction, stereodso},
  url       = {https://vision.in.tum.de/_media/spezial/bib/wang2017stereodso.pdf}
}

@inproceedings{yang2018eccv,
  author    = {N. Yang and R. Wang and J. Stueckler and D. Cremers},
  title     = {{Deep Virtual Stereo Odometry: Leveraging Deep Depth Prediction for Monocular Direct Sparse Odometry}},
  booktitle = eccv,
  year      = {2018},
  keywords  = {dso, dvso, deep learning, monocular depth estimation, semi-supervised learning, slam, visual odometry, vslam},
  url       = {https://arxiv.org/pdf/1807.02570.pdf}
}

@inproceedings{yang2020cvpr,
  author    = {N. Yang and L. von Stumberg and R. Wang and D. Cremers},
  title     = {{D3VO: Deep Depth, Deep Pose and Deep Uncertainty for Monocular Visual Odometry}},
  booktitle = cvpr,
  year      = {2020},
  keywords  = {dso,dvso, deep learning, monocular depth estimation, semi-supervised learning, slam, visual odometry,d3vo, vslam},
  url       = {https://vision.in.tum.de/_media/spezial/bib/yang20d3vo.pdf}
}

@inproceedings{wimbauer2021cvpr,
  title     = {{MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera}},
  author    = {F. Wimbauer and N. Yang and L. von Stumberg and N. Zeller and D. Cremers},
  booktitle = cvpr,
  year      = {2021},
  keywords  = {monorec, dvso, d3vo, mvs, deep learning, SLAM, vslam, reconstruction},
  url       = {https://arxiv.org/pdf/2011.11814.pdf}
}

@inproceedings{chen2019cvpr-cpdt,
  author    = {Y. Chen and Y. Lin and M. Yang and J. Huang},
  title     = {{CrDoCo: Pixel-Level Domain Transfer With Cross-Domain Consistency}},
  booktitle = cvpr,
  year      = {2019},
  url       = {proceedings: chen2019cvpr-cpdt.pdf},
  abstract  = {Unsupervised domain adaptation algorithms aim to transfer the knowledge learned from one domain to another (e.g., synthetic to real images). The adapted representations often do not capture pixel-level domain shifts that are crucial for dense prediction tasks (e.g., semantic segmentation). In this paper, we present a novel pixel-wise adversarial domain adaptation algorithm. By leveraging image-to-image translation methods for data augmentation, our key insight is that while the translated images between domains may differ in styles, their predictions for the task should be consistent. We exploit this property and introduce a cross-domain consistency loss that enforces our adapted model to produce consistent predictions. Through extensive experimental results, we show that our method compares favorably against the state-of-the-art on a wide variety of unsupervised domain adaptation tasks.}
}


@article{jumpasut2008amam,
  author  = {A. Jumpasut and N. Petrinic and B. Elliott and C. Siviour and M. Arthington},
  year    = {2008},
  pages   = {203--210},
  title   = {An Error Analysis into the Use of Regular Targets and Target Detection in Image Analysis for Impact Engineering},
  volume  = {13-14},
  journal = {Journal Applied Mechanics and Materials},
  url     = {https://www.scientific.net/AMM.13-14.203.pdf}
}

@inproceedings{dai2020cvpr,
  title     = {SG-NN: Sparse Generative Neural Networks for Self-Supervised Scene Completion of RGB-D Scans},
  author    = {Dai, Angela and Diller, Christian and Nie{\ss}ner, Matthias},
  booktitle = cvpr,
  year      = {2020}
}

@inproceedings{weder2020cvpr,
  author    = {Silvan Weder and Johannes L. SchÃ¶nberger and Marc Pollefeys and Martin R. Oswald},
  booktitle = cvpr,
  title     = {RoutedFusion: Learning Real-time Depth Map Fusion},
  year      = {2020},
  keywords  = {3D Surface Reconstruction, Learning}
}

@inproceedings{rodriguez2018icra,
  author    = {Rodriguez, Diego and Cogswell, Corbin and Koo, Seongyong and Behnke, Sven},
  booktitle = icra,
  title     = {Transferring grasping skills to novel instances by latent space non-rigid registration},
  year      = {2018},
  url       = {proceedings: rodriguez2018icra.pdf}
}

@inproceedings{murez2020eecv,
  title     = {Atlas: End-to-End 3D Scene Reconstruction from Posed Images},
  author    = {Zak Murez and Tarrence van As and James Bartolozzi and Ayan Sinha and Vijay Badrinarayanan and Andrew Rabinovich},
  booktitle = eccv,
  year      = {2020},
  url       = {https://arxiv.org/abs/2003.10432}
}

@inproceedings{weder2021cvpr,
  title     = {NeuralFusion: Online Depth Fusion in Latent Space},
  author    = {Weder, Silvan and Schonberger, Johannes L and Pollefeys, Marc and Oswald, Martin R},
  booktitle = cvpr,
  year      = {2021}
}

@inproceedings{mihajlovic2021cvpr,
  title     = {DeepSurfels: Learning online appearance fusion},
  author    = {Mihajlovic, Marko and Weder, Silvan and Pollefeys, Marc and Oswald, Martin R},
  booktitle = cvpr,
  year      = {2021}
}

@inproceedings{wang2021icra,
  author    = {Y. Wang and N. Funk and M. Ramezani and S. Papatheodorou and M. Popovic and M. Camurri and S. Leutenegger and M. Fallon},
  title     = {{Elastic and Efficient LiDAR Reconstruction for Large-Scale Exploration Tasks}},
  booktitle = icra,
  year      = 2021,
  abstract  = {We present an efficient, elastic 3D LiDAR reconstruction framework which can reconstruct up to maximum LiDAR ranges (60 m) at multiple frames per second, thus enabling robot exploration in large-scale environments. Our approach only requires a CPU. We focus on three main challenges of large-scale reconstruction: integration of long-range LiDAR scans at high frequency, the capacity to deform the reconstruction after loop closures are detected, and scalability for longduration exploration. Our system extends upon a state-of-theart efficient RGB-D volumetric reconstruction technique, called supereight, to support LiDAR scans and a newly developed submapping technique to allow for dynamic correction of the 3D reconstruction. We then introduce a novel pose graph clustering and submap fusion feature to make the proposed system more scalable for large environments. We evaluate the performance using two public datasets including outdoor exploration with a handheld device and a drone, and with a mobile robot exploring an underground room network. Experimental results demonstrate that our system can reconstruct at 3 Hz with 60 m sensor range and 5 cm resolution, while state-of-the-art approaches can only reconstruct to 25 cm resolution or 20 m range at the same frequency.}
}

@article{funk2021ral,
  title   = {Multi-resolution 3D mapping with explicit free space representation for fast and accurate mobile robot motion planning},
  author  = {Funk, Nils and Tarrio, Juan and Papatheodorou, Sotiris and Popovi{\'c}, Marija and Alcantarilla, Pablo F and Leutenegger, Stefan},
  journal = ral,
  volume  = {6},
  number  = {2},
  pages   = {3553--3560},
  year    = {2021},
  url     = {https://arxiv.org/pdf/2010.07929.pdf}
}

@article{popovic2021ral,
  author  = {PopoviÄ, Marija and Thomas, Florian and Papatheodorou, Sotiris and Funk, Nils and Vidal-Calleja, Teresa and Leutenegger, Stefan},
  journal = ral,
  title   = {{Volumetric Occupancy Mapping With Probabilistic Depth Completion for Robotic Navigation}},
  year    = {2021},
  volume  = {6},
  number  = {3},
  pages   = {5072--5079},
  url     = {https://arxiv.org/pdf/2012.03023.pdf}
}

@inproceedings{riegler2017cvpr-oldf,
  title     = {{OctNetFusion: Learning Depth Fusion from Data}},
  author    = {Riegler, Gernot and Ulusoy, Ali Osman and Bischof, Horst and Geiger, Andreas},
  booktitle = cvpr,
  year      = {2017},
  url       = {https://arxiv.org/pdf/1704.01047.pdf}
}
@inproceedings{park2019cvpr,
  author    = {Park, Jeong Joon and Florence, Peter and Straub, Julian and Newcombe, Richard and Lovegrove, Steven},
  title     = {{DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation}},
  booktitle = cvpr,
  year      = {2019},
  url       = {https://openaccess.thecvf.com/content_CVPR_2019/papers/Park_DeepSDF_Learning_Continuous_Signed_Distance_Functions_for_Shape_Representation_CVPR_2019_paper.pdf}
}

@inproceedings{mescheder2019cvpr,
  title     = {{Occupancy networks: Learning 3d reconstruction in function space}},
  author    = {Mescheder, Lars and Oechsle, Michael and Niemeyer, Michael and Nowozin, Sebastian and Geiger, Andreas},
  booktitle = cvpr,
  year      = {2019},
  url       = {https://openaccess.thecvf.com/content_CVPR_2019/papers/Mescheder_Occupancy_Networks_Learning_3D_Reconstruction_in_Function_Space_CVPR_2019_paper.pdf}
}

@inproceedings{peng2020eecv,
  title     = {Convolutional occupancy networks},
  author    = {Peng, Songyou and Niemeyer, Michael and Mescheder, Lars and Pollefeys, Marc and Geiger, Andreas},
  booktitle = eccv,
  year      = {2020},
  url       = {https://arxiv.org/pdf/2003.04618.pdf}
}

@article{rist2021pmai,
  title   = {Semantic scene completion using local deep implicit functions on LiDAR data},
  author  = {Rist, Christoph and Emmerichs, David and Enzweiler, Markus and Gavrila, Dariu},
  journal = pami,
  year    = {2021},
  volume  = {44},
  number  = {10},
  pages   = {7205--7218},
  url     = {https://arxiv.org/pdf/2011.09141.pdf}
}

@inproceedings{roldao2020threedv,
  title     = {{LMSCNet: Lightweight Multiscale 3D Semantic Completion}},
  author    = {Rold{\~a}o, Luis and de Charette, Raoul and Verroust-Blondet, Anne},
  booktitle = threedv,
  year      = {2020}
}

@article{jones2006tvcg,
  title   = {{3D distance fields: A survey of techniques and applications}},
  author  = {Jones, Mark W and Baerentzen, J Andreas and Sramek, Milos},
  journal = tvcg,
  volume  = {12},
  number  = {4},
  pages   = {581--599},
  year    = {2006}
}

@inproceedings{dai2017cvpr-scuc,
  author    = {A. Dai and C. R. Qi and M. Niessner},
  title     = {{Shape Completion Using 3D-Encoder-Predictor CNNs and Shape Synthesis}},
  booktitle = cvpr,
  year      = 2017,
  abstract  = {We introduce a data-driven approach to complete partial 3D shapes through a combination of volumetric deep neural networks and 3D shape synthesis. From a partially-scanned input shape, our method first infers a low-resolution -- but complete -- output. To this end, we introduce a 3D-Encoder-Predictor Network (3D-EPN) which is composed of 3D convolutional layers. The network is trained to predict and fill in missing data, and operates on an implicit surface representation that encodes both known and unknown space. This allows us to predict global structure in unknown areas at high accuracy. We then correlate these intermediary results with 3D geometry from a shape database at test time. In a final pass, we propose a patch-based 3D shape synthesis method that imposes the 3D geometry from these retrieved shapes as constraints on the coarsely-completed mesh.  This synthesis process enables us to reconstruct fine-scale detail and generate high-resolution output while respecting the global mesh structure obtained by the 3D-EPN. Although our 3D-EPN outperforms state-of-the-art completion method, the main contribution in our work lies in the combination of a data-driven shape predictor and analytic 3D shape synthesis. In our results, we show extensive evaluations on a newly-introduced shape completion benchmark for both real-world and synthetic data.},
  url       = {proceedings: dai2017cvpr-scuc.pdf}
}

@inproceedings{li2019iccv,
  title     = {PU-GAN: a Point Cloud Upsampling Adversarial Network},
  author    = {Li, Ruihui and Li, Xianzhi and Fu, Chi-Wing and Cohen-Or, Daniel and Heng, Pheng-Ann},
  booktitle = iccv,
  year      = {2019}
}

@inproceedings{ioffe2015icml,
  author    = {S. Ioffe and C. Szegedy},
  title     = {{Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}},
  booktitle = icml,
  year      = {2015},
  url       = {http://arxiv.org/pdf/1502.03167}
}

@article{wiesmann2021ral,
  author   = {L. Wiesmann and A. Milioto and X. Chen and C. Stachniss and J. Behley},
  title    = {{Deep Compression for Dense Point Cloud Maps}},
  journal  = ral,
  volume   = 6,
  number   = 2,
  pages    = {2060--2067},
  year     = 2021,
  url      = {http://www.ipb.uni-bonn.de/pdfs/wiesmann2021ral.pdf},
  codeurl  = {https://github.com/PRBonn/deep-point-map-compression},
  videourl = {https://youtu.be/fLl9lTlZrI0}
}


@article{dobrushin1970tpia,
  author  = {R.L. Dobrushin},
  title   = {Prescribing a System of Random Variables by Conditional Distributions},
  journal = {Theory of Probability \& Its Applications},
  volume  = {15},
  number  = {3},
  pages   = {458--486},
  year    = {1970},
  url     = {https://epubs.siam.org/doi/pdf/10.1137/1115049}
}

@inproceedings{cortinhal2020iv,
  title     = {{SalsaNext: Fast, Uncertainty-Aware Semantic Segmentation of LiDAR Point Clouds}},
  author    = {Cortinhal, Tiago and Tzelepis, George and Aksoy, Eren Erdal},
  booktitle = iv,
  year      = {2020},
  url       = {https://arxiv.org/pdf/2003.03653.pdf}
}

@article{li2022ral,
  author  = {S. Li and X. Chen and Y. Liu and D. Dai and C. Stachniss and J. Gall},
  title   = {{Multi-scale Interaction for Real-time LiDAR Data Segmentation on an Embedded Platform}},
  journal = ral,
  year    = 2022,
  volume  = {7},
  number  = {2},
  pages   = {738--745},
  url     = {https://www.ipb.uni-bonn.de/wp-content/papercite-data/pdf/li2022ral.pdf}
}


@inproceedings{mcmanus2013icra,
  title     = {{Distraction suppression for vision-based pose estimation at city scales}},
  author    = {McManus, Colin and Churchill, Winston and Napier, Ashley and Davis, Ben and Newman, Paul},
  booktitle = icra,
  year      = {2013},
  url       = {https://www.robots.ox.ac.uk/~mobile/Papers/2013ICRA_cm.pdf}
}

@inproceedings{patil2020cvpr,
  title     = {An end-to-end edge aggregation network for moving object segmentation},
  author    = {P.W. Patil and K.M. Biradar and A. Dudhane and S. Murala},
  booktitle = cvpr,
  year      = {2020},
  url       = {https://openaccess.thecvf.com/content_CVPR_2020/papers/Patil_An_End-to-End_Edge_Aggregation_Network_for_Moving_Object_Segmentation_CVPR_2020_paper.pdf}
}

@inproceedings{aldera2019icra,
  title     = {Fast radar motion estimation with a learnt focus of attention using weak supervision},
  author    = {Aldera, Roberto and De Martini, Daniele and Gadd, Matthew and Newman, Paul},
  booktitle = icra,
  year      = {2019},
  url       = {https://ori.ox.ac.uk/media/5537/2019icra_aldera.pdf}
}


@inproceedings{pagad2020icra,
  title     = {{Robust Method for Removing Dynamic Objects from Point Clouds}},
  author    = {Pagad, Shishir and Agarwal, Divya and Narayanan, Sathya and Rangan, Kasturi and Kim, Hyungjin and Yalla, Ganesh},
  booktitle = icra,
  year      = {2020},
  url       = {https://www.ipb.uni-bonn.de/pdfs/pagad2020icra.pdf}
}


@inproceedings{kim2020iros,
  title     = {{Remove, Then Revert: Static Point Cloud Map Construction Using Multiresolution Range Images}},
  author    = {Kim, Giseop and Kim, Ayoung},
  booktitle = iros,
  year      = {2020},
  url       = {http://ras.papercept.net/images/temp/IROS/files/0855.pdf}
}


@inproceedings{wang2012icra,
  title     = {{What Could Move? Finding Cars, Pedestrians and Bicyclists in 3D Laser Data}},
  author    = {Wang, Dominic Zeng and Posner, Ingmar and Newman, Paul},
  booktitle = icra,
  year      = {2012},
  url       = {https://ori.ox.ac.uk/media/5737/2012icra_wang.pdf}
}


@inproceedings{dewan2016iros,
  title     = {{Rigid scene flow for 3d lidar scans}},
  author    = {Dewan, Ayush and Caselitz, Tim and Tipaldi, Gian Diego and Burgard, Wolfram},
  booktitle = iros,
  year      = {2016},
  url       = {https://europa2.informatik.uni-freiburg.de/files/dewan-16iros.pdf}
}


@inproceedings{dewan2017iros,
  title     = {{Deep semantic classification for 3d lidar data}},
  author    = {Dewan, Ayush and Oliveira, Gabriel L and Burgard, Wolfram},
  booktitle = iros,
  year      = {2017},
  url       = {https://arxiv.org/pdf/1706.08355.pdf}
}

@inproceedings{dewan2020icra,
  title     = {{DeepTemporalSeg: Temporally Consistent Semantic Segmentation of 3D LiDAR Scans}},
  author    = {Dewan, Ayush and Burgard, Wolfram},
  booktitle = icra,
  year      = {2020},
  url       = {https://arxiv.org/pdf/1906.06962.pdf}
}

@inproceedings{tang2020eccv,
  title     = {{Searching Efficient 3D Architectures with Sparse Point-Voxel Convolution}},
  author    = {Tang, Haotian and Liu, Zhijian and Zhao, Shengyu and Lin, Yujun and Lin, Ji and Wang, Hanrui and Han, Song},
  booktitle = eccv,
  year      = {2020},
  url       = {https://arxiv.org/pdf/2007.16100.pdf}
}


@inproceedings{shi2020cvpr,
  title     = {{SpSequenceNet: Semantic Segmentation Network on 4D Point Clouds}},
  author    = {Shi, Hanyu and Lin, Guosheng and Wang, Hao and Hung, Tzu-Yi and Wang, Zhenhua},
  booktitle = cvpr,
  year      = {2020},
  url       = {https://openaccess.thecvf.com/content_CVPR_2020/papers/Shi_SpSequenceNet_Semantic_Segmentation_Network_on_4D_Point_Clouds_CVPR_2020_paper.pdf}
}

@article{wang2018tpami,
  title   = {{Temporal segment networks for action recognition in videos}},
  author  = {Wang, Limin and Xiong, Yuanjun and Wang, Zhe and Qiao, Yu and Lin, Dahua and Tang, Xiaoou and Van Gool, Luc},
  journal = tpami,
  volume  = {41},
  number  = {11},
  pages   = {2740--2755},
  year    = {2018},
  url     = {https://arxiv.org/pdf/1705.02953.pdf}
}



@inproceedings{peters2021rss,
  title     = {Inferring Objectives in Continuous Dynamic Games from Noise-Corrupted Partial State Observations},
  author    = {Peters, Lasse and Fridovich-Keil, David and Rubies-Royo, Vicenc and Tomlin, Claire J. and Stachniss, Cyrill},
  booktitle = rss,
  year      = {2021},
  url       = {https://arxiv.org/abs/2106.03611}
}

@article{kummerle2014jfr,
  author  = {K{\"u}mmerle, Rainer and Ruhnke, Michael and
             Steder, Bastian and Stachniss, Cyrill and Burgard,
             Wolfram},
  title   = {{Autonomous Robot Navigation in Highly Populated Pedestrian Zones}},
  journal = jfr,
  volume  = {32},
  number  = {4},
  pages   = {565--589},
  year    = 2014
}

@article{lim2021ral,
  title   = {{ERASOR: Egocentric Ratio of Pseudo Occupancy-Based Dynamic Object Removal for Static 3D Point Cloud Map Building}},
  author  = {Lim, Hyungtae and Hwang, Sungwon and Myung, Hyun},
  journal = ral,
  volume  = {6},
  number  = {2},
  pages   = {2272--2279},
  year    = {2021},
  url     = {https://arxiv.org/pdf/2103.04316.pdf}
}

@inproceedings{arora2021ecmr,
  author    = {M. Arora and L. Wiesmann and X. Chen and C. Stachniss},
  title     = {{Mapping the Static Parts of Dynamic Scenes from 3D LiDAR Point Clouds Exploiting Ground Segmentation}},
  booktitle = ecmr,
  url       = {https://www.ipb.uni-bonn.de/wp-content/papercite-data/pdf/arora2021ecmr.pdf},
  year      = {2021}
}

@article{hendrycks2016arxiv,
  title   = {{Gaussian Error Linear Units (GELUs)}},
  author  = {Dan Hendrycks and Kevin Gimpel},
  journal = arxiv,
  volume  = {arXiv:1606.08415},
  year    = {2016},
  url     = {https://arxiv.org/pdf/1606.08415.pdf}
}

@inproceedings{pfreundschuh2021icra,
  title     = {{Dynamic Object Aware LiDAR SLAM based on Automatic Generation of Training Data}},
  author    = {Pfreundschuh, Patrick and Hendrikx, Hubertus Franciscus Cornelis and Reijgwart, Victor and Dub{\'e}, Renaud and Siegwart, Roland and Cramariuc, Andrei},
  booktitle = icra,
  year      = {2021},
  url       = {https://arxiv.org/pdf/2104.03657.pdf}
}

@article{chen2022auro,
  author  = {X. Chen and T. L\"abe and A. Milioto and T. R\"ohling and J. Behley and C. Stachniss},
  title   = {{OverlapNet: A Siamese Network for Computing LiDAR Scan Similarity with Applications to Loop Closing and Localization}},
  journal = ar,
  volume  = {46},
  pages   = {61--81},
  year    = {2022},
  url     = {http://www.ipb.uni-bonn.de/pdfs/chen2021auro.pdf}
}

@inproceedings{sefati2017iv,
  title     = {Improving vehicle localization using semantic and pole-like landmarks},
  author    = {Sefati, Mohsen and Daum, M and Sondermann, B and Kreisk{\"o}ther, Kai D and Kampker, Achim},
  booktitle = iv,
  year      = {2017},
  url       = {http://www.ipb.uni-bonn.de/pdfs/sefati2017iv.pdf}
}

@inproceedings{weng2018rcar,
  title     = {Pole-based real-time localization for autonomous driving in congested urban scenarios},
  author    = {Weng, Lihong and Yang, Ming and Guo, Lindong and Wang, Bing and Wang, Chunxiang},
  booktitle = {Proc.~of the Intl.~Conf.~on Real-time Computing and Robotics (RCAR)},
  year      = {2018},
  url       = {http://www.ipb.uni-bonn.de/pdfs/weng2018rcar.pdf}
}

@inproceedings{kummerle2019icra,
  title     = {Accurate and efficient self-localization on roads using basic geometric primitives},
  author    = {K{\"u}mmerle, Julius and Sons, Marc and Poggenhans, Fabian and K{\"u}hner, Tilman and Lauer, Martin and Stiller, Christoph},
  booktitle = icra,
  year      = {2019},
  url       = {https://www.mrt.kit.edu/z/publ/download/2019/Accurate_and_Efficient_Self-Localization_on_Roads_using_Basic_Geometric_Primitives.pdf}
}

@inproceedings{kong2020iros,
  title     = {{Semantic Graph based Place Recognition for Point Clouds}},
  author    = {Kong, Xin and Yang, Xuemeng and Zhai, Guangyao and Zhao, Xiangrui and Zeng, Xianfang and Wang, Mengmeng and Liu, Yong and Li, Wanlong and Wen, Feng},
  booktitle = iros,
  year      = {2020},
  url       = {https://arxiv.org/pdf/2008.11459.pdf}
}

@inproceedings{silver2011iros,
  author    = {D. Silver and A. Stentz},
  title     = {{Monte Carlo Localization and Registration to Prior Data for Outdoor Navigation}},
  booktitle = iros,
  year      = {2011},
  url       = {https://www.ri.cmu.edu/pub_files/2011/9/iros2011.pdf}
}

@article{campello2015tkdd,
  title   = {{Hierarchical density estimates for data clustering, visualization, and outlier detection}},
  author  = {Campello, Ricardo JGB and Moulavi, Davoud and Zimek, Arthur and Sander, J{\"o}rg},
  journal = {ACM Transactions on Knowledge Discovery from Data (TKDD)},
  volume  = {10},
  number  = {1},
  pages   = {1--51},
  year    = {2015},
  url     = {https://dl.acm.org/doi/pdf/10.1145/2733381}
}

@inproceedings{campello2013dbscan,
  title     = {Density-based clustering based on hierarchical density estimates},
  author    = {Campello, Ricardo JGB and Moulavi, Davoud and Sander, J{\"o}rg},
  booktitle = {Proc.~of Pacific-Asia.~Conf.~on Knowledge Discovery and Data Mining},
  year      = {2013},
  url       = {http://pdf.xuebalib.com:1262/2ac1mJln8ATx.pdf}
}

@inproceedings{weng2020iros,
  title     = {3d multi-object tracking: A baseline and new evaluation metrics},
  author    = {Weng, Xinshuo and Wang, Jianren and Held, David and Kitani, Kris},
  booktitle = iros,
  year      = {2020},
  url       = {https://arxiv.org/pdf/1907.03961.pdf}
}

@inproceedings{dong2021ecmr,
  author    = {H. Dong$^*$ and X. Chen$^*$ and C. Stachniss},
  title     = {{Online Range Image-based Pole Extractor for Long-term LiDAR Localization in Urban Environments}},
  booktitle = ecmr,
  year      = {2021},
  url       = {https://www.ipb.uni-bonn.de/pdfs/dong2021ecmr.pdf}
}


@incollection{hussain2004mp,
  author    = {M. Hussain and J. Bethel},
  title     = {Project and mission planing},
  editor    = {McGlone, Chris and Mikhail, Edward and Bethel, James and Mullen, Roy},
  booktitle = {Manual of Photogrammetry},
  year      = 2004,
  pages     = {1109--1111},
  chapter   = {15.1.2.6}
}

@inproceedings{abadi2016tensorflow,
  title     = {Tensorflow: A system for large-scale machine learning},
  author    = {Abadi, Mart{\'\i}n and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and others},
  booktitle = {Proc. of the USENIX symposium on operating systems design and implementation (OSDI)},
  year      = {2016},
  url       = {http://arxiv.org/pdf/1603.04467v2}
}

@article{li2020arxiv,
  title   = {Multi-scale Interaction for Real-time LiDAR Data Segmentation on an Embedded Platform},
  author  = {Li, Shijie and Chen, Xieyuanli and Liu, Yun and Dai, Dengxin and Stachniss, Cyrill and Gall, Juergen},
  journal = arxiv,
  volume  = {arXiv:2008.09162},
  year    = {2020},
  url     = {https://arxiv.org/pdf/2008.09162.pdf}
}

@article{li2020arxiv-psan,
  title   = {Projected-point-based segmentation: A new paradigm for lidar point cloud segmentation},
  author  = {Li, Shijie and Liu, Yun and Gall, Juergen},
  journal = arxiv,
  volume  = {arXiv:2008.03928},
  year    = {2020},
  url     = {https://arxiv.org/pdf/2008.03928.pdf}
}


@article{giraldo2020pami,
  title   = {{Graph Moving Object Segmentation}},
  author  = {Giraldo, Jhony H and Javed, Sajid and Bouwmans, Thierry},
  journal = pami,
  year    = {2020},
  volume  = {44},
  number  = {5},
  pages   = {2485--2503},
  url     = {https://drive.google.com/file/d/1Pk7y6tp5fO2qUTISJyHlqVD67NxXrU3h/view}
}

@inproceedings{baur2021iccv,
  title     = {{SLIM: Self-Supervised LiDAR Scene Flow and Motion Segmentation}},
  author    = {Baur, Stefan Andreas and Emmerichs, David Josef and Moosmann, Frank and Pinggera, Peter and Ommer, Bjorn and Geiger, Andreas},
  booktitle = iccv,
  year      = {2021},
  url       = {http://cvlibs.net/publications/Baur2021ICCV.pdf}
}

@inproceedings{gojcic2021cvpr,
  title     = {{Weakly Supervised Learning of Rigid 3D Scene Flow}},
  author    = {Gojcic, Zan and Litany, Or and Wieser, Andreas and Guibas, Leonidas J and Birdal, Tolga},
  booktitle = cvpr,
  year      = {2021},
  url       = {https://arxiv.org/pdf/2102.08945.pdf}
}



@article{kostavelis2015jras,
  title   = {Semantic mapping for mobile robotics tasks: A survey},
  author  = {Kostavelis, Ioannis and Gasteratos, Antonios},
  journal = jras,
  volume  = {66},
  pages   = {86--103},
  year    = {2015},
  url     = {https://reader.elsevier.com/reader/sd/pii/S0921889014003030?token=E6D99677EB542C21F350B40F25EACAD60C9FE7A22EB52E33C29E85024B676118E8E4B812DD0E9347C89C93DA9A03073B&originRegion=eu-west-1&originCreation=20220107093105}
}

@article{xia2020ijars,
  title   = {A survey of image semantics-based visual simultaneous localization and mapping: Application-oriented solutions to autonomous navigation of mobile robots},
  author  = {Xia, Linlin and Cui, Jiashuo and Shen, Ran and Xu, Xun and Gao, Yiping and Li, Xinying},
  journal = ijars,
  volume  = {17},
  number  = {3},
  year    = {2020},
  url     = {https://journals.sagepub.com/doi/pdf/10.1177/1729881420919185}
}

@inproceedings{morris2018crv,
  title     = {{A Pyramid CNN for Dense-Leaves Segmentation}},
  author    = {Daniel Morris},
  booktitle = crv,
  year      = {2018},
  url       = {https://arxiv.org/pdf/1804.01646.pdf}
}

@article{garg2021arxiv,
  title   = {Semantics for robotic mapping, perception and interaction: A survey},
  author  = {Garg, Sourav and S{\"u}nderhauf, Niko and Dayoub, Feras and Morrison, Douglas and Cosgun, Akansel and Carneiro, Gustavo and Wu, Qi and Chin, Tat-Jun and Reid, Ian and Gould, Stephen and others},
  journal = arxiv,
  volume  = {arXiv:2101.00443},
  year    = {2021},
  url     = {https://arxiv.org/pdf/2101.00443.pdf}
}


@article{sualeh2019ijcas,
  title   = {Simultaneous localization and mapping in the epoch of semantics: a survey},
  author  = {Sualeh, Muhammad and Kim, Gon-Woo},
  journal = {Intl.~Journal~of Control, Automation and Systems},
  volume  = {17},
  number  = {3},
  pages   = {729--742},
  year    = {2019},
  url     = {https://link.springer.com/content/pdf/10.1007/s12555-018-0130-x.pdf}
}

@article{piasco2018pr,
  title   = {A survey on visual-based localization: On the benefit of heterogeneous data},
  author  = {Piasco, Nathan and Sidib{\'e}, D{\'e}sir{\'e} and Demonceaux, C{\'e}dric and Gouet-Brunet, Val{\'e}rie},
  journal = pr,
  volume  = {74},
  pages   = {90--109},
  year    = {2018},
  url     = {https://hal.archives-ouvertes.fr/hal-01744680/file/journalpaper-5.pdf?.pdf}
}

@incollection{kaymak2019survey,
  title     = {A brief survey and an application of semantic image segmentation for autonomous driving},
  author    = {Kaymak, {\c{C}}a{\u{g}}r{\i} and U{\c{c}}ar, Ay{\c{s}}eg{\"u}l},
  booktitle = {Handbook of Deep Learning Applications},
  pages     = {161--200},
  year      = {2019},
  url       = {https://arxiv.org/pdf/1808.08413.pdf}
}

@inproceedings{shan2018iros,
  title     = {{LeGO-LOAM: Lightweight and Ground-Optimized Lidar Odometry and Mapping on Variable Terrain}},
  author    = {Shan, Tixiao and Englot, Brendan},
  booktitle = iros,
  year      = {2018},
  url       = {https://www.researchgate.net/profile/Tixiao-Shan/publication/330592017_LeGO-LOAM_Lightweight_and_Ground-Optimized_Lidar_Odometry_and_Mapping_on_Variable_Terrain/links/5d7c44a9a6fdcc2f0f6dc9e9/LeGO-LOAM-Lightweight-and-Ground-Optimized-Lidar-Odometry-and-Mapping-on-Variable-Terrain.pdf}
}


@inproceedings{li2021iros,
  title     = {{SSC: Semantic scan context for large-scale place recognition}},
  author    = {Li, Lin and Kong, Xin and Zhao, Xiangrui and Huang, Tianxin and Li, Wanlong and Wen, Feng and Zhang, Hongbo and Liu, Yong},
  booktitle = iros,
  year      = {2021},
  url       = {https://arxiv.org/pdf/2107.00382.pdf}
}

@inproceedings{li2021iccv,
  title     = {{ODAM: Object Detection, Association, and Mapping using Posed RGB Video}},
  author    = {Keije Li and Daniel DeTone and Yu F. S. Chen and Minh Vo and Ian Reid and Hamid Rezatofighi and Chris Sweeney and Julian Straub and Richard Newcombe},
  booktitle = iccv,
  year      = {2021},
  url       = {proceedings: li2021iccv.pdf}
}

@article{li2021ral,
  title   = {{MOLTR: Multiple Object Localization, Tracking and Reconstruction From Monocular RGB Videos}},
  author  = {Kejie Li and Hamid Rezatofighi and Ian Reid},
  journal = ral,
  volume  = {6},
  number  = {2},
  pages   = {3341--3348},
  year    = {2021},
  url     = {proceedings: li2021ral.pdf}
}

@inproceedings{kim2018iros,
  author    = {Kim, Giseop and Kim, Ayoung},
  title     = {{Scan Context: Egocentric Spatial Descriptor for Place Recognition within {3D} Point Cloud Map}},
  booktitle = iros,
  year      = {2018},
  url       = {https://gisbi-kim.github.io/publications/gkim-2018-iros.pdf}
}

@inproceedings{postica2016iros,
  title     = {Robust moving objects detection in lidar data exploiting visual cues},
  author    = {Postica, Gheorghii and Romanoni, Andrea and Matteucci, Matteo},
  booktitle = iros,
  year      = {2016},
  url       = {https://arxiv.org/pdf/1609.09267.pdf}
}

@article{toft2020pami,
  author  = {Toft, Carl and Maddern, Will and Torii, Akihiko and Hammarstrand, Lars and Stenborg, Erik and Safari, Daniel and Okutomi, Masatoshi and Pollefeys, Marc and Sivic, Josef and Pajdla, Tomas and Kahl, Fredrik and Sattler, Torsten},
  journal = pami,
  title   = {{Long-Term Visual Localization Revisited}},
  year    = {2020},
  volume  = {44},
  number  = {4},
  pages   = {2074--2088},
  url     = {https://hal.inria.fr/hal-03140805/file/Toft2020PAMI.pdf}
}

@article{maddern2017ijrr,
  author  = {Will Maddern and Geoffrey Pascoe and Chris Linegar and Paul Newman},
  title   = {1 year, 1000 km: The Oxford RobotCar dataset},
  journal = ijrr,
  volume  = {36},
  number  = {1},
  pages   = {3--15},
  year    = {2017},
  url     = {https://doi.org/10.1177/0278364916679498}
}
  
  
@inproceedings{mohapatra2022visapp,
  author    = {S. Mohapatra and M. Hodaei and S. Yogamani and S. Milz and P. M{\"a}der and H. Gotzig and M. Simon and H. Rashed},
  title     = {{LiMoSeg: Real-time Bird's Eye View based LiDAR Motion Segmentation}},
  booktitle = visapp,
  year      = 2022,
  url       = {https://arxiv.org/pdf/2111.04875.pdf}
}

@inproceedings{thomas2021icra,
  title     = {{Self-Supervised Learning of Lidar Segmentation for Autonomous Indoor Navigation}},
  author    = {Thomas, Hugues and Agro, Ben and Gridseth, Mona and Zhang, Jian and Barfoot, Timothy D},
  booktitle = icra,
  year      = {2021},
  url       = {https://arxiv.org/pdf/2012.05897.pdf}
}

@article{shi2019rs,
  author  = {Shi, Zhenwei and Kang, Zhizhong and Lin, Yi and Liu, Yu and Chen, Wei},
  title   = {Automatic Recognition of Pole-Like Objects from Mobile Laser Scanning Point Clouds},
  journal = rs,
  volume  = {10},
  year    = {2018},
  url     = {https://www.mdpi.com/2072-4292/10/12/1891}
}


@inproceedings{choy2019cvpr,
  author    = {C. Choy and J. Gwak and S. Savarese},
  title     = {{4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural Networks}},
  booktitle = cvpr,
  year      = 2019,
  url       = {https://arxiv.org/pdf/1904.08755.pdf}
}

@inproceedings{tishchenko2020threedv,
  author    = {I. Tishchenko and S. Lombardi and M. R. Oswald and M. Pollefeys},
  title     = {{Self-Supervised Learning of Non-Rigid Residual Flow and Ego-Motion}},
  booktitle = threedv,
  url       = {https://arxiv.org/pdf/2009.10467.pdf},
  year      = 2020
}

@inproceedings{duerr2020threedv,
  author    = {F. Duerr, M. Pfaller, H. Weigel and J. Beyerer},
  title     = {{LiDAR-based Recurrent 3D Semantic Segmentation with Temporal Memory Alignment}},
  booktitle = threedv,
  url       = {https://arxiv.org/pdf/2103.02263.pdf},
  year      = 2020
}

@inproceedings{fan2021iclr,
  title     = {{PSTNet: Point Spatio-Temporal Convolution on Point Cloud Sequences}},
  author    = {H. Fan and X. Yu and Y. Ding and Y. Yang and M. Kankanhalli},
  booktitle = iclr,
  year      = 2021
}

@inproceedings{shan2020iros,
  title     = {{LIO-SAM: Tightly-coupled Lidar Inertial Odometry via Smoothing and Mapping}},
  author    = {T. Shan and B. Englot and D. Meyers and W. Wang and C. Ratti and D. Rus},
  booktitle = iros,
  year      = 2020,
  url       = {https://arxiv.org/pdf/2007.00258.pdf}
}

@article{dellenbach2021arxiv,
  title   = {{CT-ICP: Real-time Elastic LiDAR Odometry with Loop Closure}},
  author  = {P. Dellenbach and J.-E. Deschaud and B. Jacquet and F. Goulette},
  journal = arxiv,
  year    = {2021},
  volume  = {arXiv:2109.12979},
  url     = {https://arxiv.org/pdf/2109.12979.pdf}
}

@inproceedings{levinson2014unsupervised,
  title     = {Unsupervised calibration for multi-beam lasers},
  author    = {Levinson, Jesse and Thrun, Sebastian},
  booktitle = {Experimental Robotics},
  year      = {2014}
}

@inproceedings{weng2016iccvrs,
  author    = {Shengxia Weng and Jonathan Li and Yiping Chen and Cheng Wang},
  title     = {{Road traffic sign detection and classification from mobile LiDAR point clouds}},
  booktitle = {Proc.~of the Intl.~Conf.~on Computer Vision in Remote Sensing},
  year      = {2016}
}

@article{chen2022itvt,
  author  = {Chen, Guang and Lu, Fan and Li, Zhijun and Liu, Yinlong and Dong, Jinhu and Zhao, Junqiao and Yu, Junwei and Knoll, Alois},
  journal = {IEEE Transactions on Vehicular Technology},
  title   = {{Pole-Curb Fusion Based Robust and Efficient Autonomous Vehicle Localization System With Branch-and-Bound Global Optimization and Local Grid Map Method}},
  year    = {2021},
  volume  = {70},
  number  = {11},
  pages   = {11283--11294}
}

@inproceedings{plachetka2021itsc,
  author    = {Plachetka, Christopher and Fricke, Jenny and Klingner, Marvin and Fingscheidt, Tim},
  booktitle = itsc,
  title     = {{DNN-Based Recognition of Pole-Like Objects in LiDAR Point Clouds}},
  year      = {2021}
}

@article{wang2021arxiv,
  author  = {Zhihao Wang and Silin Li and Ming Cao and Haoyao Chen and Yunhui Liu},
  title   = {{Pole-like Objects Mapping and Long-Term Robot Localization in Dynamic Urban Scenarios}},
  year    = {2021},
  journal = arxiv,
  volume  = {arXiv:2103.13224},
  url     = {https://arxiv.org/pdf/2103.13224.pdf}
}



@article{nunes2022ral,
  author  = {L. Nunes and R. Marcuzzi and X. Chen and J. Behley and C. Stachniss},
  title   = {{SegContrast: 3D Point Cloud Feature Representation Learning through Self-supervised Segment Discrimination}},
  journal = ral,
  volume  = {7},
  number  = {2},
  pages   = {2116--2123},
  year    = 2022,
  url     = {http://www.ipb.uni-bonn.de/pdfs/nunes2022ral-icra.pdf}
}

@article{deng2020access,
  author  = {Deng, Wenbang and Huang, Kaihong and Chen, Xieyuanli and Zhou, Zhiqian and Shi, Chenghao and Guo, Ruibin and Zhang, Hui},
  journal = {IEEE Access},
  title   = {Semantic RGB-D SLAM for Rescue Robot Navigation},
  year    = {2020},
  volume  = {8},
  pages   = {221320--221329}
}


@inproceedings{zhou2021iros,
  title     = {Efficient Localisation Using Images and OpenStreetMaps},
  author    = {Zhou, Mengjie and Chen, Xieyuanli and Samano, Noe and Stachniss, Cyrill and Calway, Andrew},
  booktitle = iros,
  year      = {2021},
  url       = {http://www.ipb.uni-bonn.de/pdfs/zhou2021iros.pdf}
}


@article{shi2021ral,
  title   = {{Keypoint Matching for Point Cloud Registration using Multiplex Dynamic Graph Attention Networks}},
  author  = {C. Shi and X. Chen and K. Huang and J. Xiao and H. Lu and C. Stachniss},
  year    = {2021},
  journal = ral,
  volume  = 6,
  number  = 4,
  pages   = {8221--8228},
  url     = {http://www.ipb.uni-bonn.de/pdfs/shi2021ral-iros.pdf},
  codeurl = {https://github.com/chenghao-shi/MDGAT-matcher}
}

@incollection{museth2021siggraph,
  title     = {NanoVDB: A GPU-Friendly and Portable VDB Data Structure For Real-Time Rendering And Simulation},
  author    = {Museth, Ken},
  booktitle = {ACM SIGGRAPH 2021 Talks},
  year      = {2021}
}

@incollection{museth2014siggraph,
  title     = {Hierarchical digital differential analyzer for efficient ray-marching in openvdb},
  author    = {Museth, Ken},
  booktitle = {ACM SIGGRAPH 2014 Talks},
  year      = {2014}
}

@inproceedings{ramezani2020iros,
  title     = {The Newer College Dataset: Handheld LiDAR, Inertial and Vision with Ground Truth},
  author    = {M. {Ramezani} and Y. {Wang} and M. {Camurri} and D. {Wisth} and M. {Mattamala} and M. {Fallon}},
  booktitle = iros,
  year      = {2020}
}

@article{zeng2013gmodels,
  title   = {Octree-based fusion for realtime 3D reconstruction},
  author  = {Zeng, Ming and Zhao, Fukai and Zheng, Jiaxiang and Liu, Xinguo},
  journal = {Graphical Models},
  volume  = {75},
  number  = {3},
  pages   = {126--136},
  year    = {2013}
}

@inproceedings{triebel2006iros,
  author    = {Rudolph Triebel and Patrick Pfaff and Wolfram Burgard},
  booktitle = iros,
  title     = {{Multi-Level Surface Maps for Outdoor Terrain Mapping and Loop Closing}},
  year      = 2006
}

@article{grinvald2019ral,
  author  = {M. {Grinvald} and F. {Furrer} and T. {Novkovic} and J. J. {Chung} and C. {Cadena} and R. {Siegwart} and J. {Nieto}},
  journal = ral,
  title   = {{Volumetric Instance-Aware Semantic Mapping and 3D Object Discovery}},
  year    = {2019},
  volume  = {4},
  number  = {3},
  pages   = {3037--3044}
}

@inproceedings{handa2014icra,
  author    = {A. Handa and T. Whelan and J.B. McDonald and A.J. Davison},
  title     = {A Benchmark for {RGB-D} Visual Odometry, {3D} Reconstruction and {SLAM}},
  booktitle = icra,
  year      = {2014}
}

@inproceedings{weixin2019cvpr,
  title     = {L3-net: Towards learning based lidar localization for autonomous driving},
  author    = {Lu, Weixin and Zhou, Yao and Wan, Guowei and Hou, Shenhua and Song, Shiyu},
  booktitle = cvpr,
  year      = {2019}
}

@article{grisetti2020robotics,
  author   = {Grisetti, Giorgio and Guadagnino, Tiziano and Aloise, Irvin and Colosi, Mirco and Della Corte, Bartolomeo and Schlegel, Dominik},
  title    = {{Least Squares Optimization: From Theory to Practice}},
  journal  = {Robotics},
  volume   = {9},
  year     = 2020,
  number   = 3,
  abstract = {Nowadays, Nonlinear Least-Squares embodies the foundation of many Robotics and Computer Vision systems. The research community deeply investigated this topic in the last few years, and this resulted in the development of several open-source solvers to approach constantly increasing classes of problems. In this work, we propose a unified methodology to design and develop efficient Least-Squares Optimization algorithms, focusing on the structures and patterns of each specific domain. Furthermore, we present a novel open-source optimization system that addresses problems transparently with a different structure and designed to be easy to extend. The system is written in modern C++ and runs efficiently on embedded systemsWe validated our approach by conducting comparative experiments on several problems using standard datasets. The results show that our system achieves state-of-the-art performances in all tested scenarios.}
}

@inproceedings{colosi2020iros,
  author    = {M. Colosi and I. Aloise and T. Guadagnino and D. Schlegel and B.D. Corte and K.O. Arras and G. Grisetti},
  title     = {{Plug-And-Play SLAM A Unified SLAM Architecture for Modularity and Ease of Use}},
  booktitle = iros,
  year      = 2020,
  keywords  = {SLAM, Mapping},
  abstract  = {Simultaneous Localization and Mapping (SLAM) is considered a mature research field with numerous applications and publicly available open-source systems. Despite this maturity, existing SLAM systems often rely on ad-hoc implementations or are tailored to predefined sensor setups. In this work, we tackle these issues, proposing a novel unified SLAM architecture specifically designed to standardize the SLAM problem and to address heterogeneous sensor configurations. Thanks to its modularity and design patterns, the presented framework is easy to extend, maximizes code reuse and improves computational efficiency. We show in our experiments with a variety of typical sensor configurations that these advantages come without compromising state-of-the-art SLAM performance. The result demonstrates the architectures relevance for facilitating further research in (multisensor) SLAM and its transfer into practical applications.}
}

@article{dalmedico2019sensors,
  author   = {Dalmedico, Nicolas and SimÃµes Teixeira, Marco AntÃ´nio and Barbosa Santos, Higor and Nogueira, Rafael de Castro Martins and Ramos de Arruda, LÃºcia ValÃ©ria and Neves, FlÃ¡vio and Rodrigues Pipa, Daniel and Endress Ramos, JÃºlio and Schneider de Oliveira, AndrÃ©},
  title    = {Sliding Window Mapping for Omnidirectional RGB-D Sensors},
  journal  = sensors,
  volume   = {19},
  year     = {2019},
  number   = {23},
  abstract = {This paper presents an omnidirectional RGB-D (RGB + Distance fusion) sensor prototype using an actuated LIDAR (Light Detection and Ranging) and an RGB camera. Besides the sensor, a novel mapping strategy is developed considering sensor scanning characteristics. The sensor can gather RGB and 3D data from any direction by toppling in 90 degrees a laser scan sensor and rotating it about its central axis. The mapping strategy is based on two environment maps, a local map for instantaneous perception, and a global map for perception memory. The 2D local map represents the surface in front of the robot and may contain RGB data, allowing environment reconstruction and human detection, similar to a sliding window that moves with a robot and stores surface data.}
}

@article{peng2020sensors,
  author   = {Peng, Cheng-Wei and Hsu, Chen-Chien and Wang, Wei-Yen},
  title    = {Cost Effective Mobile Mapping System for Color Point Cloud Reconstruction},
  journal  = sensors,
  volume   = {20},
  year     = {2020},
  number   = {22},
  abstract = {Survey-grade Lidar brands have commercialized Lidar-based mobile mapping systems (MMSs) for several years now. With this high-end equipment, the high-level accuracy quality of point clouds can be ensured, but unfortunately, their high cost has prevented practical implementation in autonomous driving from being affordable. As an attempt to solve this problem, we present a cost-effective MMS to generate an accurate 3D color point cloud for autonomous vehicles. Among the major processes for color point cloud reconstruction, we first synchronize the timestamps of each sensor. The calibration process between camera and Lidar is developed to obtain the translation and rotation matrices, based on which color attributes can be composed into the corresponding Lidar points. We also employ control points to adjust the point cloud for fine tuning the absolute position. To overcome the limitation of Global Navigation Satellite System/Inertial Measurement Unit (GNSS/IMU) positioning system, we utilize Normal Distribution Transform (NDT) localization to refine the trajectory to solve the multi-scan dispersion issue. Experimental results show that the color point cloud reconstructed by the proposed MMS has a position error in centimeter-level accuracy, meeting the requirement of high definition (HD) maps for autonomous driving usage.}
}

@article{macenski2020ijars,
  title   = {Spatio-temporal voxel layer: A view on robot perception for the dynamic world},
  author  = {Macenski, Steve and Tsai, David and Feinberg, Max},
  journal = ijars,
  volume  = {17},
  number  = {2},
  year    = {2020}
}

@inproceedings{besselmann2021case,
  author    = {Besselmann, M. Grosse and Puck, L. and Steffen, L. and Roennau, A. and Dillmann, R.},
  booktitle = case,
  title     = {{VDB-Mapping: A High Resolution and Real-Time Capable 3D Mapping Framework for Versatile Mobile Robots}},
  year      = {2021}
}

// Introduced in vizzo2022ral-iros:
@article{vizzo2022ral,
  author  = {I. Vizzo and B. Mersch and R. Marcuzzi and L. Wiesmann and and J. Behley and C. Stachniss},
  title   = {Make it Dense: Self-Supervised Geometric Scan Completion of Sparse 3D LiDAR Scans in Large Outdoor Environments},
  journal = ral,
  url     = {https://www.ipb.uni-bonn.de/wp-content/papercite-data/pdf/vizzo2022ral-iros.pdf},
  codeurl = {https://github.com/PRBonn/make_it_dense},
  year    = {2022},
  volume  = {7},
  number  = {3},
  pages   = {8534--8541}
}

@inproceedings{wang2021iros-l3op,
  author    = {L. Wang and H. Ye and Q. Wang and Y. Gao and C. Xu and F. Gao},
  title     = {{Learning-Based 3D Occupancy Prediction for Autonomous Navigation in Occluded Environments}},
  booktitle = iros,
  year      = 2021,
  keywords  = {Autonomous Vehicle Navigation, Deep Learning Methods, Collision Avoidance},
  url       = {https://arxiv.org/pdf/2011.03981.pdf}
}

@article{yan2020arxiv,
  author  = {Yan, Xu and Gao, Jiantao and Li, Jie and Zhang, Ruimao and Li, Zhen and Huang, Rui and Cui, Shuguang},
  title   = {Sparse single sweep lidar point cloud segmentation via learning contextual shape priors from scene completion},
  journal = arxiv,
  year    = {2020},
  volume  = {arXiv:2012.03762},
  url     = {https://arxiv.org/pdf/2012.03762.pdf}
}

@article{cheng2020arxiv,
  author  = {Cheng, Ran and Agia, Christopher and Ren, Yuan and Li, Xinhai and Bingbing, Liu},
  title   = {S3cnet: A sparse semantic scene completion network for lidar point clouds},
  journal = arxiv,
  year    = {2020},
  volume  = {arXiv:2012.09242},
  url     = {https://arxiv.org/pdf/2012.09242.pdf}
}

@article{shea2015arxiv,
  title   = {An introduction to convolutional neural networks},
  author  = {O'Shea, Keiron and Nash, Ryan},
  journal = arxiv,
  year    = {2015},
  volume  = {arXiv:1511.08458},
  url     = {https://arxiv.org/pdf/1511.08458.pdf}
}

@article{zhang2021arxiv,
  title   = {{Dive into Deep Learning}},
  author  = {Zhang, Aston and Lipton, Zachary C. and Li, Mu and Smola, Alexander J.},
  journal = arxiv,
  volume  = {arXiv:2106.11342},
  year    = {2021},
  url     = {https://arxiv.org/pdf/2106.11342.pdf}
}

@inproceedings{kumar2021aaai,
  author    = {Prashant Kumar and Sabyasachi Sahoo and Vanshil Shah and Vineetha Kondameedi and Abhinav Jain and Akshaj Verma and Chiranjib Bhattacharyya and Vinay V},
  title     = {{DSLR: Dynamic to Static LiDAR Scan Reconstruction Using Adversarially Trained Autoencoder}},
  booktitle = aaai,
  year      = {2021},
  url       = {https://arxiv.org/pdf/2105.12774.pdf}
}

@inproceedings{cicek2016micc,
  title     = {3D U-Net: learning dense volumetric segmentation from sparse annotation},
  author    = {{\c{C}}i{\c{c}}ek, {\"O}zg{\"u}n and Abdulkadir, Ahmed and Lienkamp, Soeren S and Brox, Thomas and Ronneberger, Olaf},
  booktitle = {Medical Image Computing and Computer-Assisted Intervention},
  year      = {2016},
  url       = {https://arxiv.org/pdf/1606.06650.pdf}
}

@inproceedings{sun2021cvpr-nrcr,
  author    = {J. Sun and Y. Xie and L. Chen and X. Zhou and H. Bao},
  title     = {{NeuralRecon: Real-Time Coherent 3D Reconstruction From Monocular Video}},
  booktitle = cvpr,
  year      = 2021,
  abstract  = {We present a novel framework named NeuralRecon for real-time 3D scene reconstruction from a monocular video. Unlike previous methods that estimate single-view depth maps separately on each key-frame and fuse them later, we propose to directly reconstruct local surfaces represented as sparse TSDF volumes for each video fragment sequentially by a neural network. A learning-based TSDF fusion module based on gated recurrent units is used to guide the network to fuse features from previous fragments. This design allows the network to capture local smoothness prior and global shape prior of 3D surfaces when sequentially reconstructing the surfaces, resulting in accurate, coherent, and real-time surface reconstruction. The experiments on ScanNet and 7-Scenes datasets show that our system outperforms state-of-the-art methods in terms of both accuracy and speed. To the best of our knowledge, this is the first learning-based system that is able to reconstruct dense coherent 3D geometry in real-time. Code is available at the project page: https://zju3dv.github.io/neuralrecon/.},
  url       = {proceedings: sun2021cvpr-nrcr.pdf}
}

// vizzo2023ral(KISS-ICP)
@inproceedings{chen1991iros,
  title     = {{Object Modelling by Registration of Multiple Range Images}},
  author    = {Chen, Yang and Medioni, G{\'e}rard},
  booktitle = iros,
  year      = {1991},
  url       = {http://graphics.stanford.edu/courses/cs348a-17-winter/Handouts/chen-medioni-align-rob91.pdf}
}

@article{thrun1998ar,
  title   = {A probabilistic approach to concurrent mapping and localization for mobile robots},
  author  = {Thrun, Sebastian and Burgard, Wolfram and Fox, Dieter},
  journal = ar,
  volume  = {5},
  number  = {3},
  pages   = {253--271},
  year    = {1998}
}

@inproceedings{yokozuka2020iros,
  author    = {M. Yokozuka and K. Koide and S. Oishi and A. Banno},
  title     = {{LiTAMIN LiDAR Based Tracking and MappINg by Stabilized ICP for Geometry Approximation with Normal Distributions}},
  booktitle = iros,
  year      = 2020,
  keywords  = {SLAM, Mapping, Localization},
  abstract  = {This paper proposes a 3D LiDAR simultaneous localization and mapping (SLAM) method that improves accuracy, robustness, and computational efciency for an iterative closest point (ICP) algorithm employing a locally approximated geometry with clusters of normal distributions. In comparison with previous normal distribution-based ICP methods, such as normal distribution transformation and generalized ICP, our ICP method is simply stabilized with normalization of the cost function by the Frobenius norm and a regularized covariance matrix. The previous methods are stabilized with principal component analysis, whose computational cost is higher than that of our method. Moreover, our SLAM method can reduce the effect of incorrect loop closure constraints. The experimental results show that our SLAM method has advantages over open source state-of-the-art methods, including LOAM, LeGO-LOAM, and hdl graph slam.}
}

@inproceedings{dellenbach2022icra,
  author    = {P. Dellenbach and J. Deschaud and B. Jacquet and F. Goulette},
  title     = {{CT-ICP Real-Time Elastic LiDAR Odometry with Loop Closure}},
  booktitle = icra,
  year      = 2022,
  keywords  = {SLAM, Data Sets for SLAM},
  abstract  = { Multi-beam LiDAR sensors are increasingly used in robotics, particularly with autonomous cars for localization and perception tasks, both relying on the ability to build a precise map of the environment. For this, we propose a new real-time LiDAR-only odometry method called CT-ICP (for Continuous-Time ICP), completed into a full SLAM with a novel loop detection procedure. The core of this method, is the introduction of the combined continuity in the scan matching, and discontinuity between scans. It allows both the elastic distortion of the scan during the registration for increased precision, and the increased robustness to high frequency motions from the discontinuity. We build a complete SLAM on top of this odometry, using a fast pure LiDAR loop detection based on elevation image 2D matching, providing a pose graph with loop constraints. To show the robustness of the method, we tested it on seven datasets: KITTI, KITTI-raw, KITTI-360, KITTICARLA, ParisLuco, Newer College, and NCLT in driving and high-frequency motion scenarios. Both the CT-ICP odometry and the loop detection are made available online. CT-ICP is currently first, among those giving access to a public code, on the KITTI odometry leaderboard, with an average Relative Translation Error (RTE) of 0.59\% and an average time per scan of 60ms on a CPU with a single thread.}
}

@inproceedings{pan2021icra-mvls,
  author    = {Y. Pan and P. Xiao and Y. He and Z. Shao and Z. Li},
  title     = {{MULLS: Versatile LiDAR SLAM Via Multi-Metric Linear Least Square}},
  booktitle = icra,
  year      = 2021,
  abstract  = {The rapid development of autonomous driving and mobile mapping calls for off-the-shelf LiDAR SLAM solutions that are adaptive to LiDARs of different specifications on various complex scenarios. To this end, we propose MULLS, an efficient, low-drift, and versatile 3D LiDAR SLAM system. For the front-end, roughly classified feature points (ground, facade, pillar, beam, etc.) are extracted from each frame using dualthreshold ground filtering and principal components analysis. Then the registration between the current frame and the local submap is accomplished efficiently by the proposed multimetric linear least square iterative closest point algorithm. Point-to-point (plane, line) error metrics within each point class are jointly optimized with a linear approximation to estimate the ego-motion. Static feature points of the registered frame are appended into the local map to keep it updated. For the backend, hierarchical pose graph optimization is conducted among regularly stored history submaps to reduce the drift resulting from dead reckoning. Extensive experiments are carried out on three datasets with more than 100,000 frames collected by seven types of LiDAR on various outdoor and indoor scenarios. On the KITTI benchmark, MULLS ranks among the top LiDARonly SLAM systems with real-time performance.}
}

@inproceedings{xu2021iccv-rada,
  author    = {J. Xu and R. Zhang and J. Dou and Y. Zhu and J. Sun and S. Pu},
  title     = {{RPVNet: A Deep and Efficient Range-Point-Voxel Fusion Network for LiDAR Point Cloud Segmentation}},
  booktitle = iccv,
  year      = 2021,
  abstract  = {Point clouds can be represented in many forms (views), typically, point-based sets, voxel-based cells or range-based images(i.e., panoramic view). The point-based view is geometrically accurate, but it is disordered, which makes it difficult to find local neighbors efficiently. The voxel-based view is regular, but sparse, and computation grows cubicly when voxel resolution increases. The range-based view is regular and generally dense, however spherical projection makes physical dimensions distorted. Both voxel- and range-based views suffer from quantization loss, especially for voxels when facing large-scale scenes. In order to utilize different view's advantages and alleviate their own shortcomings in fine-grained segmentation task, we propose a novel range-point-voxel fusion network, namely RPVNet. In this network, we devise a deep fusion framework with multiple and mutual information interactions among these three views, and propose a gated fusion module (termed as GFM), which can adaptively merge the three features based on concurrent inputs. Moreover, the proposed RPV interaction mechanism is highly efficient, and we summarize it to a more general formulation. By leveraging this efficient interaction and relatively lower voxel resolution, our method is also proved to be more efficient. Finally, we evaluated the proposed model on two large-scale datasets, i.e., SemanticKITTI and nuScenes, and it shows state-of-the-art performance on both of them. Note that, our method currently ranks 1st on SemanticKITTI leaderboard without any extra tricks.},
  url       = {proceedings: xu2021iccv-rada.pdf}
}

@article{xu2021ral-fafr,
  author   = {W. Xu and F. Zhang},
  title    = {{FAST-LIO: A Fast, Robust LiDAR-Inertial Odometry Package by Tightly-Coupled Iterated Kalman Filter}},
  journal  = ral,
  year     = 2021,
  volume   = {6},
  number   = {2},
  pages    = {3317--3324},
  abstract = {This paper presents a computationally efficient and robust LiDAR-inertial odometry framework. We fuse LiDAR feature points with IMU data using a tightly-coupled iterated extended Kalman filter to allow robust navigation in fastmotion, noisy or cluttered environments where degeneration occurs. To lower the computation load in the presence of a large number of measurements, we present a new formula to compute the Kalman gain. The new formula has computation load depending on the state dimension instead of the measurement dimension. The proposed method and its implementation are tested in various indoor and outdoor environments. In all tests, our method produces reliable navigation results in realtime: running on a quadrotor onboard computer, it fuses more than 1,200 effective feature points in a scan and completes all iterations of an iEKF step within 25 ms. Our codes are opensourced on Github2 .}
}

@inproceedings{biber2003iros,
  title     = {The normal distributions transform: A new approach to laser scan matching},
  author    = {Biber, Peter and Stra{\ss}er, Wolfgang},
  booktitle = iros,
  year      = {2003}
}


@inproceedings{wang2021iros-fflo,
  author    = {H. Wang and C. Wang and C. Chen and L. Xie},
  title     = {{F-LOAM: Fast LiDAR Odometry and Mapping}},
  booktitle = iros,
  year      = 2021,
  keywords  = {SLAM, Localization, Industrial Robots},
  abstract  = { Simultaneous Localization and Mapping (SLAM) has wide robotic applications such as autonomous driving and unmanned aerial vehicles. Both computational efficiency and localization accuracy are of great importance towards a good SLAM system. Existing works on LiDAR based SLAM often formulate the problem as two modules: scan-to-scan match and scan-to-map refinement. Both modules are solved by iterative calculation which are computationally expensive. In this paper, we propose a general solution that aims to provide a computationally efficient and accurate framework for LiDAR based SLAM. Specifically, we adopt a non-iterative two-stage distortion compensation method to reduce the computational cost. For each scan input, the edge and planar features are extracted and matched to a local edge map and a local plane map separately, where the local smoothness is also considered for iterative pose optimization. Thorough experiments are performed to evaluate its performance in challenging scenarios, including localization for a warehouse Automated Guided Vehicle (AGV) and a public dataset on autonomous driving. The proposed method achieves a competitive localization accuracy with a processing rate of more than 10 Hz in the public dataset evaluation, which provides a good trade-off between performance and computational cost for practical applications. It is one of the most accurate and fastest open-sourced SLAM systems1 in KITTI dataset ranking.}
}

@article{cook1986tog,
  title   = {Stochastic sampling in computer graphics},
  author  = {Cook, Robert L},
  journal = tog,
  volume  = {5},
  number  = {1},
  pages   = {51--72},
  year    = {1986}
}

@inproceedings{dippe1985siggraph,
  title     = {Antialiasing through stochastic sampling},
  author    = {Dipp{\'e}, Mark AZ and Wold, Erling Henry},
  booktitle = siggraph,
  year      = {1985}
}

@inproceedings{neuhaus2018gcpr,
  title     = {Mc2slam: Real-time inertial lidar odometry using two-scan motion compensation},
  author    = {Neuhaus, Frank and Ko{\ss}, Tilman and Kohnen, Robert and Paulus, Dietrich},
  booktitle = gcpr,
  year      = {2018}
}

@inproceedings{lin2019iros-larl,
  author    = {J. Lin and F. Zhang},
  title     = {{Loam\_livox A Robust LiDAR Odemetry and Mapping LOAM Package for Livox LiDAR}},
  booktitle = iros,
  year      = 2019,
  keywords  = {SLAM, Range Sensing},
  abstract  = { Lidar Odometry And Mapping (LOAM) plays an important role in autonomous robotics navigation. LOAM can not only estimate the 6-Degree-of-Freedom (6-DOF) pose (including 3-DOF of transition and 3-DOF of rotation) relative to its initial state, but also build a high-precision, highresolution map of its surrounds environment. In this OnePage abstract, we present an overview of our open-source rospackage: the LOAM-Livox1 , a robust LiDAR Odometry and mapping (LOAM) package for Livox LiDAR.}
}

@article{zhou2021ar,
  title   = {S4-SLAM: A real-time 3D LIDAR SLAM system for ground/watersurface multi-scene outdoor applications},
  author  = {Zhou, Bo and He, Yi and Qian, Kun and Ma, Xudong and Li, Xiaomao},
  journal = ar,
  volume  = {45},
  number  = {1},
  pages   = {77--98},
  year    = {2021}
}

@article{guadagnino2022ral,
  author  = {T. Guadagnino and X. Chen and M. Sodano and J. Behley and G. Grisetti and C. Stachniss},
  title   = {{Fast Sparse LiDAR Odometry Using Self-Supervised Feature Selection on Intensity Images}},
  journal = ral,
  year    = 2022,
  volume  = {7},
  number  = {3},
  pages   = {7597--7604},
  url     = {https://www.ipb.uni-bonn.de/wp-content/papercite-data/pdf/guadagnino2022ral-iros.pdf}
}

@inproceedings{sodano2023icra,
  title     = {{Robust Double-Encoder Network for RGB-D Panoptic Segmentation}},
  author    = {Sodano, Matteo and Magistri, Federico and Guadagnino, Tiziano and Behley, Jens and Stachniss, Cyrill},
  booktitle = icra,
  year      = {2023},
  url       = {https://www.ipb.uni-bonn.de/wp-content/papercite-data/pdf/sodano2023icra.pdf}
}

@inproceedings{stueckler2012iros,
  author    = {St{\"u}ckler, Jorg and Biresev, Nenad and Behnke, Sven},
  booktitle = iros,
  title     = {{Semantic mapping using object-class segmentation of RGB-D images}},
  year      = {2012},
  url       = {proceedings: stuckler2012iros.pdf}
}

@inproceedings{rusu2011icra,
  title     = {3d is here: Point cloud library (pcl)},
  author    = {Rusu, Radu Bogdan and Cousins, Steve},
  booktitle = icra,
  year      = {2011}
}

@inproceedings{pomerleau2010fsr,
  title     = {Relative motion threshold for rejection in ICP registration},
  author    = {Pomerleau, Fran{\c{c}}ois and Colas, Francis and Ferland, Fran{\c{c}}ois and Michaud, Fran{\c{c}}ois},
  booktitle = {Field and Service Robotics},
  year      = {2010}
}

@misc{blanco2014mrpt,
  title        = {Mobile robot programming toolkit (MRPT)},
  author       = {Blanco-Claraco, Jose-Luis},
  howpublished = {URL: http://www. mrpt. org/},
  year         = {2014}
}

@inproceedings{choy2019iccv,
  title     = {Fully convolutional geometric features},
  author    = {Choy, Christopher and Park, Jaesik and Koltun, Vladlen},
  booktitle = iccv,
  url       = {https://openaccess.thecvf.com/content_ICCV_2019/papers/Choy_Fully_Convolutional_Geometric_Features_ICCV_2019_paper.pdf},
  year      = {2019}
}

@inproceedings{bai2020cvpr,
  title     = {D3feat: Joint learning of dense detection and description of 3d local features},
  author    = {Bai, Xuyang and Luo, Zixin and Zhou, Lei and Fu, Hongbo and Quan, Long and Tai, Chiew-Lan},
  booktitle = cvpr,
  url       = {https://openaccess.thecvf.com/content_CVPR_2020/papers/Bai_D3Feat_Joint_Learning_of_Dense_Detection_and_Description_of_3D_CVPR_2020_paper.pdf},
  year      = {2020}
}

@inproceedings{streiff2021icra,
  title     = {3D3L: Deep Learned 3D Keypoint Detection and Description for LiDARs},
  author    = {Streiff, Dominc and Bernreiter, Lukas and Tschopp, Florian and Fehr, Marius and Siegwart, Roland},
  booktitle = icra,
  url       = {https://arxiv.org/pdf/2103.13808.pdf},
  year      = {2021}
}

@inproceedings{deng2019cvpr,
  title     = {3d local features for direct pairwise registration},
  author    = {Deng, Haowen and Birdal, Tolga and Ilic, Slobodan},
  booktitle = cvpr,
  url       = {https://openaccess.thecvf.com/content_CVPR_2019/papers/Deng_3D_Local_Features_for_Direct_Pairwise_Registration_CVPR_2019_paper.pdf},
  year      = {2019}
}

// Introduced in marcuzzi2022ral-icra:
@inproceedings{khosla2020neurips,
  title     = {{Supervised Contrastive Learning}},
  author    = {Khosla, Prannay and Teterwak, Piotr and Wang, Chen and Sarna, Aaron and Tian, Yonglong and Isola, Phillip and Maschinot, Aaron and Liu, Ce and Krishnan, Dilip},
  year      = 2020,
  booktitle = neurips,
  url       = {https://proceedings.neurips.cc/paper/2020/file/d89a66c7c80a29b1bdbab0f2a1a94af8-Paper.pdf}
}

@article{vandenoord2019arxiv,
  title   = {{Representation Learning with Contrastive Predictive Coding}},
  author  = {Aaron van den Oord and Yazhe Li and Oriol Vinyals},
  journal = arxiv,
  volume  = {arXiv:1807.03748},
  year    = 2019,
  url     = {https://arxiv.org/pdf/1807.03748}
}

@inproceedings{voigtlaender2019cvpr,
  author    = {P. Voigtlaender and M. Krause and A. Osep and J. Luiten and B. B. G. Sekar and A. Geiger and B. Leibe},
  title     = {{MOTS: Multi-Object Tracking and Segmentation}},
  booktitle = cvpr,
  year      = 2019,
  abstract  = {This paper extends the popular task of multi-object tracking to multi-object tracking and segmentation (MOTS). Towards this goal, we create dense pixel-level annotations for two existing tracking datasets using a semi-automatic annotation procedure. Our new annotations comprise 65,213 pixel masks for 977 distinct objects (cars and pedestrians) in 10,870 video frames. For evaluation, we extend existing multi-object tracking metrics to this new task. Moreover, we propose a new baseline method which jointly addresses detection, tracking, and segmentation with a single convolutional network. We demonstrate the value of our datasets by achieving improvements in performance when training on MOTS annotations. We believe that our datasets, metrics and baseline will become a valuable resource towards developing multi-object tracking approaches that go beyond 2D bounding boxes. We make our annotations, code, and models available at https://www.vision.rwth-aachen.de/page/mots.},
  url       = {proceedings: voigtlaender2019cvpr.pdf}
}

@article{comaniciu2002pami,
  title   = {{Mean Shift: A Robust Approach Toward Feature Space Analysis}},
  author  = {D. Comaniciu and P. Meer},
  journal = pami,
  year    = 2002,
  volume  = {24},
  number  = {5},
  pages   = {603--619},
  url     = {https://www.cs.cmu.edu/~efros/courses/AP06/Papers/comaniciu-pami-02.pdf}
}

@inproceedings{ye2020cvpr,
  author    = {M. Ye and S. Xu and T. Cao},
  title     = {{HVNet: Hybrid Voxel Network for LiDAR Based 3D Object Detection}},
  booktitle = cvpr,
  year      = 2020,
  url       = {https://openaccess.thecvf.com/content_CVPR_2020/papers/Ye_HVNet_Hybrid_Voxel_Network_for_LiDAR_Based_3D_Object_Detection_CVPR_2020_paper.pdf}
}

@article{shi2021pami,
  title   = {{From Points to Parts: 3D Object Detection from Point Cloud with Part-aware and Part-aggregation Network}},
  author  = {{Shaoshuai Shi, Zhe Wang, Jianping Shi, Xiaogang Wang, and Hongsheng Li}},
  journal = pami,
  year    = {2021},
  volume  = {43},
  number  = {8},
  pages   = {2647--2664},
  url     = {https://arxiv.org/pdf/1907.03670.pdf}
}

@inproceedings{chen2020neurips,
  author    = {Chen, Ting and Kornblith, Simon and Swersky, Kevin and Norouzi, Mohammad and Hinton, Geoffrey E},
  booktitle = neurips,
  title     = {{Big Self-Supervised Models are Strong Semi-Supervised Learners}},
  url       = {https://proceedings.neurips.cc/paper/2020/file/fcbc95ccdd551da181207c0c1400c655-Paper.pdf},
  year      = 2020
}

@inproceedings{hong2021cvpr-lpsv,
  author    = {F. Hong and H. Zhou and X. Zhu and H. Li and Z. Liu},
  title     = {{LiDAR-Based Panoptic Segmentation via Dynamic Shifting Network}},
  booktitle = cvpr,
  year      = 2021,
  abstract  = {With the rapid advances of autonomous driving, it becomes critical to equip its sensing system with more holistic 3D perception. However, existing works focus on parsing either the objects (e.g. cars and pedestrians) or scenes (e.g. trees and buildings) from the LiDAR sensor. In this work, we address the task of LiDAR-based panoptic segmentation, which aims to parse both objects and scenes in a unified manner. As one of the first endeavors towards this new challenging task, we propose the Dynamic Shifting Network (DS-Net), which serves as an effective panoptic segmentation framework in the point cloud realm. In particular, DS-Net has three appealing properties: 1) strong backbone design. DS-Net adopts the cylinder convolution that is specifically designed for LiDAR point clouds. The extracted features are shared by the semantic branch and the instance branch which operates in a bottom-up clustering style. 2) Dynamic Shifting for complex point distributions. We observe that commonly-used clustering algorithms like BFS or DBSCAN are incapable of handling complex autonomous driving scenes with non-uniform point cloud distributions and varying instance sizes. Thus, we present an efficient learnable clustering module, dynamic shifting, which adapts kernel functions on-the-fly for different instances. 3) Consensus-driven Fusion. Finally, consensus-driven fusion is used to deal with the disagreement between semantic and instance predictions. To comprehensively evaluate the performance of LiDAR-based panoptic segmentation, we construct and curate benchmarks from two large-scale autonomous driving LiDAR datasets, SemanticKITTI and nuScenes. Extensive experiments demonstrate that our proposed DS-Net achieves superior accuracies over current state-of-the-art methods. Notably, we achieve 1st place on the public leaderboard of SemanticKITTI, outperforming 2nd place by 2.6\% in terms of the PQ metric.},
  url       = {proceedings: hong2021cvpr-lpsv.pdf}
}

@inproceedings{zhou2021cvpr-pplp,
  author    = {Z. Zhou and Y. Zhang and H. Foroosh},
  title     = {{Panoptic-PolarNet: Proposal-Free LiDAR Point Cloud Panoptic Segmentation}},
  booktitle = cvpr,
  year      = 2021,
  abstract  = {Panoptic segmentation presents a new challenge in exploiting the merits of both detection and segmentation, with the aim of unifying instance segmentation and semantic segmentation in a single framework. However, an efficient solution for panoptic segmentation in the emerging domain of LiDAR point cloud is still an open research problem and is very much under-explored. In this paper, we present a fast and robust LiDAR point cloud panoptic segmentation framework, referred to as Panoptic-PolarNet. We learn both semantic segmentation and class-agnostic instance clustering in a single inference network using a polar Bird's Eye View (BEV) representation, enabling us to circumvent the issue of occlusion among instances in urban street scenes. To improve our network's learnability, we also propose an adapted instance augmentation technique and a novel adversarial point cloud pruning method. Our experiments show that Panoptic-PolarNet outperforms the baseline methods on SemanticKITTI and nuScenes datasets with an almost real-time inference speed. Panoptic-PolarNet achieved 54.1\% PQ in the public SemanticKITTI panoptic segmentation leaderboard and leading performance for the validation set of nuScenes.},
  url       = {proceedings: zhou2021cvpr-pplp.pdf}
}

@inproceedings{bergmann2019iccv,
  author    = {P. Bergmann and T. Meinhardt and L. Leal-Taixe},
  title     = {{Tracking Without Bells and Whistles}},
  booktitle = iccv,
  year      = 2019,
  abstract  = {The problem of tracking multiple objects in a video sequence poses several challenging tasks. For tracking-by-detection, these include object re-identification, motion prediction and dealing with occlusions. We present a tracker (without bells and whistles) that accomplishes tracking without specifically targeting any of these tasks, in particular, we perform no training or optimization on tracking data. To this end, we exploit the bounding box regression of an object detector to predict the position of an object in the next frame, thereby converting a detector into a Tracktor. We demonstrate the potential of Tracktor and provide a new state-of-the-art on three multi-object tracking benchmarks by extending it with a straightforward re-identification and camera motion compensation. We then perform an analysis on the performance and failure cases of several state-of-the-art tracking methods in comparison to our Tracktor. Surprisingly, none of the dedicated tracking methods are considerably better in dealing with complex tracking scenarios, namely, small and occluded objects or missing detections. However, our approach tackles most of the easy tracking scenarios. Therefore, we motivate our approach as a new tracking paradigm and point out promising future research directions. Overall, Tracktor yields superior tracking performance than any current tracking method and our analysis exposes remaining and unsolved tracking challenges to inspire future research directions.},
  url       = {proceedings: bergmann2019iccv.pdf}
}

@inproceedings{kim2020cvpr-vps,
  author    = {D. Kim and S. Woo and J. Lee and I. S. Kweon},
  title     = {{Video Panoptic Segmentation}},
  booktitle = cvpr,
  year      = 2020,
  abstract  = {Panoptic segmentation has become a new standard of visual recognition task by unifying previous semantic segmentation and instance segmentation tasks in concert. In this paper, we propose and explore a new video extension of this task, called video panoptic segmentation. The task requires generating consistent panoptic segmentation as well as an association of instance ids across video frames. To invigorate research on this new task, we present two types of video panoptic datasets. The first is a re-organization of the synthetic VIPER dataset into the video panoptic format to exploit its large-scale pixel annotations. The second is a temporal extension on the Cityscapes val. set, by providing new video panoptic annotations (Cityscapes-VPS). Moreover, we propose a novel video panoptic segmentation network (VPSNet) which jointly predicts object classes, bounding boxes, masks, instance id tracking, and semantic segmentation in video frames. To provide appropriate metrics for this task, we propose a video panoptic quality (VPQ) metric and evaluate our method and several other baselines. Experimental results demonstrate the effectiveness of the presented two datasets. We achieve state-of-the-art results in image PQ on Cityscapes and also in VPQ on Cityscapes-VPS and VIPER datasets.},
  url       = {proceedings: kim2020cvpr-vps.pdf}
}

@inproceedings{aygun2021cvpr,
  author    = {M. Aygun and A. Osep and M. Weber and M. Maximov and C. Stachniss and J. Behley and L. Leal-Taixe},
  title     = {{4D Panoptic LiDAR Segmentation}},
  booktitle = cvpr,
  year      = 2021,
  abstract  = {Temporal semantic scene understanding is critical for self-driving cars or robots operating in dynamic environments. In this paper, we propose 4D panoptic LiDAR segmentation to assign a semantic class and a temporally-consistent instance ID to a sequence of 3D points. To this end, we present an approach and a novel evaluation metric. Our approach determines a semantic class for every point while modeling object instances as probability distributions in the 4D spatio-temporal domain. We process multiple point clouds in parallel and resolve point-to-instance associations, effectively alleviating the need for explicit temporal data association. Inspired by recent advances in benchmarking of multi-object tracking, we propose to adopt a new evaluation metric that separates the semantic and point-to-instance association aspects of the task. With this work, we aim at paving the road for future developments aiming at temporal LiDAR panoptic perception.},
  url       = {proceedings: aygun2021cvpr.pdf}
}

@article{hermans2017arxiv,
  author  = {Alexander Hermans and Lucas Beyer and Bastian Leibe},
  title   = {In Defense of the Triplet Loss for Person Re-Identification},
  journal = arxiv,
  volume  = {arXiv:1703.07737},
  year    = {2017},
  url     = {https://arxiv.org/pdf/1703.07737.pdf}
}

@inproceedings{son2017cvpr,
  author    = {Jeany Son and Mooyeol Baek and Minsu Cho and Bohyung Han},
  title     = {{Multi-Object Tracking With Quadruplet Convolutional Neural Networks}},
  booktitle = cvpr,
  year      = {2017},
  url       = {https://openaccess.thecvf.com/content_cvpr_2017/papers/Son_Multi-Object_Tracking_With_CVPR_2017_paper.pdf}
}

@inproceedings{weng2020cvpr,
  author    = {X. Weng and Y. Wang and Y. Man and K. M. Kitani},
  title     = {{GNN3DMOT: Graph Neural Network for 3D Multi-Object Tracking With 2D-3D Multi-Feature Learning}},
  booktitle = cvpr,
  year      = 2020,
  abstract  = {3D Multi-object tracking (MOT) is crucial to autonomous systems. Recent work uses a standard tracking-by-detection pipeline, where feature extraction is first performed independently for each object in order to compute an affinity matrix. Then the affinity matrix is passed to the Hungarian algorithm for data association. A key process of this standard pipeline is to learn discriminative features for different objects in order to reduce confusion during data association. In this work, we propose two techniques to improve the discriminative feature learning for MOT: (1) instead of obtaining features for each object independently, we propose a novel feature interaction mechanism by introducing the Graph Neural Network. As a result, the feature of one object is informed of the features of other objects so that the object feature can lean towards the object with similar feature (i.e., object probably with a same ID) and deviate from objects with dissimilar features (i.e., object probably with different IDs), leading to a more discriminative feature for each object; (2) instead of obtaining the feature from either 2D or 3D space in prior work, we propose a novel joint feature extractor to learn appearance and motion features from 2D and 3D space simultaneously. As features from different modalities often have complementary information, the joint feature can be more discriminate than feature from each individual modality. To ensure that the joint feature extractor does not heavily rely on one modality, we also propose an ensemble training paradigm. Through extensive evaluation, our proposed method achieves state-of-the-art performance on KITTI and nuScenes 3D MOT benchmarks. Our code will be made available at https://github.com/xinshuoweng/GNN3DMOT},
  url       = {proceedings: weng2020cvpr.pdf}
}

@inproceedings{dong2018eccv,
  author    = {X. Dong and J. Shen},
  title     = {{Triplet Loss in Siamese Network for Object Tracking}},
  booktitle = eccv,
  year      = 2018,
  abstract  = {Object tracking is still a critical and challenging problem with many applications in computer vision. For this challenge, more and more researchers pay attention to applying deep learning to get powerful feature for better tracking accuracy. In this paper, a novel triplet loss is proposed to extract expressive deep feature for object tracking by adding it into Siamese network framework instead of pairwise loss for training. Without adding any inputs, our approach is able to utilize more elements for training to achieve more powerful feature via the combination of original samples. Furthermore, we propose a theoretical analysis by combining comparison of gradients and back-propagation, to prove the effectiveness of our method. In experiments, we apply the proposed triplet loss for three real-time trackers based on Siamese network. And the results on several popular tracking benchmarks show our variants operate at almost the same frame-rate with baseline trackers and achieve superior tracking performance than them, as well as the comparable accuracy with recent state-of-the-art real-time trackers.},
  url       = {proceedings: dong2018eccv.pdf}
}

@inproceedings{frossard2018icra,
  author    = {D. Frossard and R. Urtasun},
  title     = {{End-To-End Learning of Multi-Sensor 3D Tracking by Detection}},
  booktitle = icra,
  year      = 2018,
  keywords  = {Visual Tracking, Deep Learning in Robotics and Automation, Autonomous Vehicle Navigation},
  abstract  = {In this paper we propose a novel approach to tracking by detection that can exploit both cameras as well as LIDAR data to produce very accurate 3D trajectories. Towards this goal, we formulate the problem as a linear program that can be solved exactly, and learn convolutional networks for detection as well as matching in an end-to-end manner. We evaluate our model in the challenging KITTI dataset and show very competitive results.}
}

@article{reid1979ac,
  title   = {An Algorithm for Tracking Multiple Targets},
  author  = {{Reid, Donald B}},
  journal = ac,
  year    = 1979,
  volume  = {24},
  number  = {6},
  pages   = {843--854},
  url     = {http://www.cse.psu.edu/~rtc12/CSE598G/papers/ReidMHT.pdf}
}

@article{guo2020tpami,
  author  = {Yulan Guo and Hanyun Wang and Qingyong Hu and Hao Liu and Li Liu and Mohammed Bennamoun},
  title   = {{Deep Learning for 3D Point Clouds: A Survey}},
  journal = tpami,
  volume  = {43},
  pages   = {4338--4364},
  year    = 2020,
  url     = {https://arxiv.org/pdf/1912.12033.pdf}
}

@inproceedings{andriyenko2011cvpr,
  author    = {Andriyenko, Anton and Schindler, Konrad},
  title     = {{Multi-target tracking by continuous energy minimization}},
  booktitle = cvpr,
  year      = 2011,
  url       = {http://www.milanton.de/files/cvpr2011/cvpr2011-anton.pdf}
}

@inproceedings{choi2010eccv,
  author    = {Wongun Choi and Silvio Savarese},
  title     = {{Multiple target tracking in world coordinate with single, minimally calibrated camera}},
  booktitle = eccv,
  year      = 2010,
  url       = {cvgl.stanford.edu/papers/mtt_wg_eccv2010.pdf}
}

@inproceedings{bewley2016icip,
  author    = {Bewley, Alex and Ge, Zongyuan and Ott, Lionel and Ramos, Fabio and Upcroft, Ben},
  booktitle = icip,
  title     = {{Simple online and realtime tracking}},
  year      = 2016,
  url       = {https://arxiv.org/pdf/1602.00763.pdf}
}

@inproceedings{behley2021icra,
  author    = {J. Behley and A. Milioto and C. Stachniss},
  title     = {{A Benchmark for LiDAR-Based Panoptic Segmentation Based on KITTI}},
  booktitle = icra,
  year      = 2021,
  abstract  = {Panoptic segmentation is the recently introduced task that tackles semantic segmentation and instance segmentation jointly [18]. In this paper, we present an extension of SemanticKITTI [1], a large-scale dataset providing dense point-wise semantic labels for all sequences of the KITTI Odometry Benchmark [10]. This extension enables training and evaluation of LiDAR-based panoptic segmentation. We provide the data and discuss the processing steps needed to enrich a given semantic annotation with temporally consistent instance information, i.e., instance information that supplements the semantic labels and identifies the same instance over sequences of LiDAR point clouds. Additionally, we present two strong baselines that combine state-of-the-art LiDAR-based semantic segmentation approaches with a state-of-the-art detector enriching the segmentation with instance information and that allow other researchers to compare their approaches against. We believe that our extension of SemanticKITTI with strong baselines enables the creation of novel algorithms for LiDARbased panoptic segmentation as much as it has for the original semantic segmentation and semantic scene completion tasks. Data, code, and an online evaluation service using a hidden test set are publicly available at http://semantic-kitti.org.}
}

@inproceedings{mittal2020cvpr-jgwt,
  author    = {H. Mittal and B. Okorn and D. Held},
  title     = {{Just Go With the Flow: Self-Supervised Scene Flow Estimation}},
  booktitle = cvpr,
  year      = 2020,
  abstract  = {When interacting with highly dynamic environments, scene flow allows autonomous systems to reason about the non-rigid motion of multiple independent objects. This is of particular interest in the field of autonomous driving, in which many cars, people, bicycles, and other objects need to be accurately tracked. Current state-of-the-art methods require annotated scene flow data from autonomous driving scenes to train scene flow networks with supervised learning. As an alternative, we present a method of training scene flow that uses two self-supervised losses, based on nearest neighbors and cycle consistency. These self-supervised losses allow us to train our method on large unlabeled autonomous driving datasets; the resulting method matches current state-of-the-art supervised performance using no real world annotations and exceeds state-of-the-art performance when combining our self-supervised approach with supervised learning on a smaller labeled dataset.},
  url       = {proceedings: mittal2020cvpr-jgwt.pdf}
}

@inproceedings{jaegle2021icml,
  title     = {{Perceiver: General Perception with Iterative Attention}},
  author    = {Andrew Jaegle and Felix Gimeno and Andrew Brock and Andrew Zisserman and Oriol Vinyals and Joao Carreira},
  booktitle = icml,
  year      = 2021,
  url       = {https://arxiv.org/pdf/2103.03206}
}

@inproceedings{vaswani2017neurips,
  title     = {{Attention Is All You Need}},
  author    = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  booktitle = neurips,
  year      = 2017,
  url       = {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf}
}

@inproceedings{dosovitskiy2021iclr,
  title     = {{An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale}},
  author    = {Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
  booktitle = iclr,
  year      = 2021,
  url       = {https://openreview.net/pdf?id=YicbFdNTTy}
}

@article{vandermaaten2008jmlr,
  author  = {Laurens van der Maaten and Geoffrey Hinton},
  title   = {{Visualizing Data using t-SNE}},
  journal = jmlr,
  year    = 2008,
  volume  = {9},
  number  = {86},
  pages   = {2579--2605},
  url     = {http://jmlr.org/papers/v9/vandermaaten08a.html}
}

@inproceedings{caron2020neurips,
  title     = {{Unsupervised Learning of Visual Features by Contrasting Cluster Assignments}},
  author    = {Caron, Mathilde and Misra, Ishan and Mairal, Julien and Goyal, Priya and Bojanowski, Piotr and Joulin, Armand},
  booktitle = neurips,
  year      = 2020,
  url       = {https://arxiv.org/pdf/2006.09882}
}

@inproceedings{grill2020neurips,
  author    = {Grill, Jean-Bastien and Strub, Florian and Altch\'{e}, Florent and Tallec, Corentin and Richemond, Pierre and Buchatskaya, Elena and Doersch, Carl and Avila Pires, Bernardo and Guo, Zhaohan and Gheshlaghi Azar, Mohammad and Piot, Bilal and kavukcuoglu, koray and Munos, Remi and Valko, Michal},
  title     = {{Bootstrap Your Own Latent - A New Approach to Self-Supervised Learning}},
  booktitle = neurips,
  url       = {https://proceedings.neurips.cc/paper/2020/file/f3ada80d5c4ee70142b17b8192b2958e-Paper.pdf},
  year      = 2020
}

@inproceedings{chen2021cvpr-essr,
  author    = {X. Chen and K. He},
  title     = {{Exploring Simple Siamese Representation Learning}},
  booktitle = cvpr,
  year      = 2021,
  abstract  = {Siamese networks have become a common structure in various recent models for unsupervised visual representation learning. These models maximize the similarity between two augmentations of one image, subject to certain conditions for avoiding collapsing solutions. In this paper, we report surprising empirical results that simple Siamese networks can learn meaningful representations even using none of the following: (i) negative sample pairs, (ii) large batches, (iii) momentum encoders. Our experiments show that collapsing solutions do exist for the loss and structure, but a stop-gradient operation plays an essential role in preventing collapsing. We provide a hypothesis on the implication of stop-gradient, and further show proof-of-concept experiments verifying it. Our "SimSiam" method achieves competitive results on ImageNet and downstream tasks. We hope this simple baseline will motivate people to rethink the roles of Siamese architectures for unsupervised representation learning. Code is made available. (https://github.com/facebookresearch/simsiam)},
  url       = {proceedings: chen2021cvpr-essr.pdf}
}

@inproceedings{wang2021iccv,
  title     = {{Solving Inefficiency of Self-supervised Representation Learning}},
  author    = {Guangrun Wang and Keze Wang and Guangcong Wang and P. Torr and Liang Lin},
  booktitle = iccv,
  year      = 2021,
  url       = {https://arxiv.org/pdf/2104.08760.pdf}
}

@inproceedings{wang2022corl,
  title     = {{DETR3D: 3D object detection from multi-view images via 3D-to-2D queries}},
  author    = {Yue Wang and Vitor C. Guizilini and Tianyuan Zhang and Yilun Wang and Hang Zhao and Justin Solomon},
  booktitle = corl,
  year      = {2022},
  url       = {https://openreview.net/pdf?id=xHnJS2GYFDz}
}

@inproceedings{schroff2015cvpr,
  author    = {F. Schroff and D. Kalenichenko and J. Philbin},
  title     = {{FaceNet: A Unified Embedding for Face Recognition and Clustering}},
  booktitle = cvpr,
  year      = 2015,
  abstract  = {Despite significant recent advances in the field of face recognition [DeepFace, DeepId2], implementing face verification and recognition efficiently at scale presents serious challenges to current approaches. In this paper we present a system, called FaceNet, that directly learns a mapping from face images to a compact Euclidean space where distances directly correspond to a measure of face similarity. Once this space has been produced, tasks such as face recognition, verification and clustering can be easily implemented using standard techniques with FaceNet embeddings as feature vectors.  Our method uses a deep convolutional network trained to directly optimize the embedding itself, rather than an intermediate bottleneck layer as in previous approaches. To train, we use triplets of roughly aligned matching / non-matching face patches generated using a novel online triplet mining method. The benefit of our approach is much greater representational efficiency: we achieve state-of-the-art face recognition performance using only 128 bytes per face.  On the widely used Labeled Faces in the Wild (LFW) dataset, our system achieves a new record accuracy of 99.63\%. On YouTube Faces DB it achieves 95.12\%. Our system cuts the error rate in comparison to the best published result [DeepId2+] by 30\% on both datasets.},
  url       = {proceedings: schroff2015cvpr.pdf}
}

@inproceedings{marcuzzi2022icra,
  author    = {R. Marcuzzi and L. Nunes and L. Wiesmann and I. Vizzo and J. Behley and C. Stachniss},
  title     = {{Contrastive Instance Association for 4D Panoptic Segmentation for Sequences of 3D LiDAR Scans}},
  booktitle = icra,
  year      = 2022,
  keywords  = {Semantic Scene Understanding, Deep Learning Methods},
  abstract  = {Scene understanding is critical for autonomous navigation in dynamic environments. Perception tasks in this domain like segmentation and tracking are usually tackled individually. In this paper, we address the problem of 4D panoptic segmentation using LiDAR scans, which requires to assign to each 3D point in a temporal sequence of scans a semantic class and for each object a temporally consistent instance ID. We propose a novel approach that builds on top of an arbitrary single-scan panoptic segmentation network and extends it to the temporal domain by associating instances across time. We propose a contrastive aggregation network that leverages the point-wise features from the panoptic network. It generates an embedding space in which encodings of the same instance at different timesteps lie close together and far from encodings belonging to other instances. The training is inspired by contrastive learning techniques for self-supervised metric learning. Our association module combines appearance and motion cues to associate instances across scans, allowing us to perform temporal perception. We evaluate our proposed method on the SemanticKITTI benchmark and achieve state-of-the-art results even without relying on pose information.},
  url       = {https://www.ipb.uni-bonn.de/wp-content/papercite-data/pdf/marcuzzi2022ral.pdf}
}

@article{marquardt1963siam,
  author  = {Donald W. Marquardt},
  title   = {{An Algorithm for Least-Squares Estimation of Nonlinear Parameters}},
  journal = {Journal of the Society for Industrial and Applied Mathematics},
  volume  = {11},
  number  = {2},
  pages   = {431--441},
  year    = {1963}
}

@article{lehnert2020jfr,
  author  = {C. Lehnert and C. McCool and Y. Sa and T. Perez},
  title   = {Performance improvements of a sweet pepper harvesting robot in protected cropping environments},
  journal = jfr,
  volume  = {37},
  number  = {7},
  pages   = {1197--1223},
  year    = {2020},
  url     = {https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/rob.21973?download=true}
}

@article{brown2021jfr,
  title   = {Design and evaluation of a modular robotic plum harvesting system utilizing soft components},
  author  = {J. Brown and S. Sukkarieh},
  journal = jfr,
  volume  = {38},
  number  = {2},
  pages   = {289--306},
  year    = {2021},
  url     = {https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/rob.21987?download=true}
}

@article{maiseli2017jvcir,
  author  = {Maiseli, Baraka and Gu, Yanfeng and Gao, Huijun},
  year    = {2017},
  pages   = {95--106},
  title   = {Recent developments and trends in point set registration methods},
  volume  = {46},
  journal = jvcir
}

@article{ando2021pp,
  author  = {Ando, Ryuhei and Ozasa, Yuko and Guo, Wei},
  year    = {2021},
  pages   = {1--15},
  title   = {Robust Surface Reconstruction of Plant Leaves from 3D Point Clouds},
  volume  = {2021},
  journal = {Plant Phenomics}
}

@inproceedings{moosmann2009ivs,
  author    = {Moosmann, Frank and Pink, Oliver and Stiller, Christoph},
  year      = {2009},
  title     = {Segmentation of 3D Lidar Data in non-flat Urban Environments using a Local Convexity Criterion},
  booktitle = iv,
  url       = {https://www.researchgate.net/profile/Frank-Moosmann/publication/224562434_Segmentation_of_3D_Lidar_Data_in_non-flat_Urban_Environments_using_a_Local_Convexity_Criterion/links/5821e75508ae1e06ad249fb7/Segmentation-of-3D-Lidar-Data-in-non-flat-Urban-Environments-using-a-Local-Convexity-Criterion.pdf?origin=publicationDetail&_sg%5B0%5D=Y9DH-7DaOdrI32T_9guKV6-bJP4EUeaDbvBHCFOFPWPI6OWUjAJo0b02eLeQP2OvnjCVzpELAxiOS4X5M7-aoA.dwPiRM-DmFDU-Lc8xaSf3-1RD5w01ZQcJ6XvQDVQdCg40vwPNGCubPFfOPsUrWYhf14E9jNqnn0fXw4AIgNtdg&_sg%5B1%5D=X8YaHfLjLREbDU-docQ_igsH4etrr45oQfJuBmeFPT_-s4fjB28fX5DBbGLZfSSMgIyioqCYSfNTdSzDOrB0wTvUXSBOhhbPELwRJFtH3L04.dwPiRM-DmFDU-Lc8xaSf3-1RD5w01ZQcJ6XvQDVQdCg40vwPNGCubPFfOPsUrWYhf14E9jNqnn0fXw4AIgNtdg&_iepl=&_rtd=eyJjb250ZW50SW50ZW50IjoibWFpbkl0ZW0ifQ%3D%3D}
}

@inproceedings{douillard2011icra,
  author    = {B. Douillard and J.P. Underwood and N. Kuntz and V. Vlaskine and A.J. Quadros and P. Morton and A. Frenkel},
  title     = {{On the Segmentation of 3D LIDAR Point Clouds}},
  booktitle = icra,
  year      = 2011,
  url       = {https://www.ipb.uni-bonn.de:5555/pdfs/douillard2011icra.pdf}
}

@inproceedings{yang2018corl,
  title     = {Hdnet: Exploiting hd maps for 3d object detection},
  author    = {Yang, Bin and Liang, Ming and Urtasun, Raquel},
  booktitle = corl,
  year      = {2018},
  url       = {http://proceedings.mlr.press/v87/yang18b/yang18b.pdf}
}

@inproceedings{wong2020corl,
  title     = {Identifying unknown instances for autonomous driving},
  author    = {Wong, Kelvin and Wang, Shenlong and Ren, Mengye and Liang, Ming and Urtasun, Raquel},
  booktitle = corl,
  year      = {2020},
  url       = {http://www.cs.toronto.edu/~slwang/osis.pdf}
}

@inproceedings{sun2020cvpr,
  title     = {Scalability in perception for autonomous driving: Waymo open dataset},
  author    = {Sun, Pei and Kretzschmar, Henrik and Dotiwalla, Xerxes and Chouard, Aurelien and Patnaik, Vijaysai and Tsui, Paul and Guo, James and Zhou, Yin and Chai, Yuning and Caine, Benjamin and others},
  booktitle = cvpr,
  year      = {2020},
  url       = {https://openaccess.thecvf.com/content_CVPR_2020/papers/Sun_Scalability_in_Perception_for_Autonomous_Driving_Waymo_Open_Dataset_CVPR_2020_paper.pdf}
}

@inproceedings{dube2018icra,
  title     = {SegMap: 3d segment mapping using data-driven descriptors},
  author    = {Dub{\'e}, Renaud and Cramariuc, Andrei and Dugas, Daniel and Nieto, Juan and Siegwart, Roland and Cadena, Cesar},
  booktitle = icra,
  year      = {2018},
  url       = {http://www.roboticsproceedings.org/rss14/p03.pdf}
}

@inproceedings{weber2021neurips,
  title     = {STEP: Segmenting and Tracking Every Pixel},
  author    = {Weber, Mark and Xie, Jun and Collins, Maxwell and Zhu, Yukun and Voigtlaender, Paul and Adam, Hartwig and Green, Bradley and Geiger, Andreas and Leibe, Bastian and Cremers, Daniel and Osep, Aljosa and Leal-Taix{\'e}, Laura and Chen, Liang-Chieh},
  booktitle = neurips,
  year      = {2021},
  url       = {https://openreview.net/pdf?id=0uQIr4XA77f}
}

@article{hu2020ral,
  title   = {Learning to Optimally Segment Point Clouds},
  author  = {Hu, Peiyun and Held, David and Ramanan, Deva},
  journal = ral,
  volume  = {5},
  number  = {2},
  pages   = {875--882},
  year    = {2020},
  url     = {https://arxiv.org/pdf/1912.04976.pdf}
}

@inproceedings{teichman2011icra,
  title     = {Towards {3D} Object Recognition via Classification of Arbitrary Object Tracks},
  author    = {Teichman, Alex and Levinson, Jesse and Thrun, Sebastian},
  booktitle = icra,
  year      = {2011},
  url       = {https://cs.stanford.edu/people/teichman/papers/icra2011.pdf}
}

@inproceedings{himmelsbach2010iv,
  title     = {Fast segmentation of 3D point clouds for ground vehicles},
  author    = {Himmelsbach, Michael and Hundelshausen, Felix V and Wuensche, H-J},
  booktitle = iv,
  year      = {2010}
}

@inproceedings{gidaris2018iclr,
  title     = {{Unsupervised Representation Learning by Predicting Image Rotations}},
  author    = {Gidaris, Spyros and Singh, Praveer and Komodakis, Nikos},
  booktitle = iclr,
  year      = {2018},
  url       = {https://hal-enpc.archives-ouvertes.fr/hal-01864755/file/iclr2018.pdf}
}

@inproceedings{chen2021iccv,
  author    = {{Xinlei Chen, Saining Xie, and Kaiming He}},
  title     = {An Empirical Study of Training Self-Supervised Vision Transformers},
  booktitle = iccv,
  year      = {2021},
  url       = {https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_An_Empirical_Study_of_Training_Self-Supervised_Vision_Transformers_ICCV_2021_paper.pdf}
}

@article{simonyan2014arxiv-dicn,
  title   = {Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps},
  author  = {Karen Simonyan and Andrea Vedaldi and Andrew Zisserman},
  journal = arxiv,
  year    = {2014},
  volume  = {arXiv:1312.6034},
  url     = {https://arxiv.org/pdf/1312.6034.pdf}
}

@inproceedings{zeiler2014eccv,
  title     = {Visualizing and Understanding Convolutional Networks},
  author    = {Matthew D. Zeiler and Rob Fergus},
  booktitle = eccv,
  year      = {2014},
  url       = {https://arxiv.org/pdf/1311.2901.pdf}
}

@article{shi2000pami,
  author  = {Jianbo Shi and Malik, J.},
  journal = pami,
  title   = {Normalized cuts and image segmentation},
  year    = {2000},
  volume  = {22},
  number  = {8},
  pages   = {888--905},
  url     = {https://people.eecs.berkeley.edu/~malik/papers/SM-ncut.pdf}
}

@article{boykov2006ijcv,
  author  = {Boykov, Yuri and Funka-Lea, Gareth},
  title   = {Graph Cuts and Efficient N-D Image Segmentation},
  journal = ijcv,
  year    = {2006},
  volume  = {70},
  number  = {2},
  pages   = {109--131},
  url     = {https://www.csd.uwo.ca/~yboykov/Papers/ijcv06.pdf}
}

@inproceedings{boykov2001iccv,
  author    = {Boykov, Y.Y. and Jolly, M.-P.},
  booktitle = iccv,
  title     = {Interactive graph cuts for optimal boundary  amp; region segmentation of objects in N-D images},
  year      = {2001},
  url       = {https://www.csd.uwo.ca/~yboykov/Papers/iccv01.pdf}
}

@inproceedings{noroozi2016eccv,
  title     = {Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles},
  author    = {Mehdi Noroozi and Paolo Favaro},
  booktitle = eccv,
  year      = {2016},
  url       = {https://arxiv.org/pdf/1603.09246.pdf}
}

@inproceedings{tolstikhin2021neurips,
  title     = {MLP-Mixer: An all-MLP Architecture for Vision},
  author    = {Tolstikhin, Ilya and Houlsby, Neil and Kolesnikov, Alexander and Beyer, Lucas and Zhai, Xiaohua and Unterthiner, Thomas and Yung, Jessica and Steiner, Andreas and Keysers, Daniel and Uszkoreit, Jakob and Lucic, Mario and Dosovitskiy, Alexey},
  booktitle = neurips,
  year      = {2021},
  url       = {https://proceedings.neurips.cc/paper/2021/file/cba0a4ee5ccd02fda0fe3f9a3e7b89fe-Paper.pdf}
}

@article{steiner2022tmlr,
  title   = {{How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers}},
  author  = {Steiner, Andreas and Kolesnikov, Alexander and and Zhai, Xiaohua and Wightman, Ross and Uszkoreit, Jakob and Beyer, Lucas},
  journal = tmlr,
  year    = {2022},
  url     = {https://openreview.net/pdf?id=4nPswr1KcP}
}

@inproceedings{chen2022iclr,
  title     = {When Vision Transformers Outperform ResNets without Pretraining or Strong Data Augmentations},
  author    = {Chen, Xiangning and Hsieh, Cho-Jui and Gong, Boqing},
  booktitle = iclr,
  year      = {2022},
  url       = {https://openreview.net/pdf?id=LtKcMgGOeLt}
}

@inproceedings{wang2021neurips,
  title     = {NeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction},
  author    = {Wang, Peng and Liu, Lingjie and Liu, Yuan and Theobalt, Christian and Komura, Taku and Wang, Wenping},
  booktitle = neurips,
  year      = {2021},
  url       = {https://arxiv.org/pdf/2106.10689.pdf}
}

@inproceedings{ortiz2022rss,
  title     = {iSDF: Real-Time Neural Signed Distance Fields for Robot Perception},
  author    = {Ortiz, Joseph and Clegg, Alexander and Dong, Jing and Sucar, Edgar and Novotny, David and Zollhoefer, Michael and Mukadam, Mustafa},
  booktitle = rss,
  year      = {2022},
  url       = {https://arxiv.org/pdf/2204.02296.pdf}
}

@inproceedings{takikawa2021cvpr,
  title     = {Neural geometric level of detail: Real-time rendering with implicit 3D shapes},
  author    = {Takikawa, Towaki and Litalien, Joey and Yin, Kangxue and Kreis, Karsten and Loop, Charles and Nowrouzezahrai, Derek and Jacobson, Alec and McGuire, Morgan and Fidler, Sanja},
  booktitle = cvpr,
  year      = {2021},
  url       = {https://arxiv.org/pdf/2101.10994.pdf}
}

@inproceedings{wang2022threedv,
  title     = {GO-Surf: Neural Feature Grid Optimization for Fast, High-Fidelity RGB-D Surface
               Reconstruction},
  author    = {Wang, Jingwen and Bleja, Tymoteusz and Agapito, Lourdes},
  booktitle = threedv,
  year      = {2022},
  url       = {https://arxiv.org/pdf/2206.14735.pdf}
}

@article{mueller2022acmgraphics,
  title   = {Instant Neural Graphics Primitives with a Multiresolution Hash Encoding},
  author  = {Thomas M\"uller and Alex Evans and Christoph Schied and Alexander Keller},
  journal = acmgraphics,
  volume  = {41},
  number  = {4},
  year    = {2022},
  pages   = {102:1--102:15},
  url     = {https://dl.acm.org/doi/pdf/10.1145/3528223.3530127}
}

@inproceedings{huang2021cvpr,
  title     = {DI-Fusion: Online Implicit 3D Reconstruction with Deep Priors},
  author    = {Huang, Jiahui and Huang, Shi-Sheng and Song, Haoxuan and Hu, Shi-Min},
  booktitle = cvpr,
  year      = {2021},
  url       = {https://arxiv.org/pdf/2012.05551.pdf}
}

@inproceedings{lionar2021threedv,
  title     = {NeuralBlox: Real-Time Neural Representation Fusion for Robust Volumetric Mapping},
  author    = {Stefan Lionar and Lukas Schmid and Cesar Cadena and Roland Siegwart and Andrei Cramariuc},
  booktitle = threedv,
  year      = {2021},
  url       = {https://arxiv.org/pdf/2110.09415.pdf}
}

@inproceedings{sun2022siggraph,
  title     = {Neural {3D} Reconstruction in the Wild},
  author    = {Sun, Jiaming and Chen, Xi and Wang, Qianqian and Li, Zhengqi and Averbuch-Elor, Hadar and Zhou, Xiaowei and Snavely, Noah},
  booktitle = siggraph,
  year      = {2022},
  url       = {https://arxiv.org/pdf/2205.12955.pdf}
}

@inproceedings{sun2021cvpr,
  title     = {{NeuralRecon}: Real-Time Coherent {3D} Reconstruction from Monocular Video},
  author    = {Sun, Jiaming and Xie, Yiming and Chen, Linghao and Zhou, Xiaowei and Bao, Hujun},
  booktitle = cvpr,
  year      = {2021},
  url       = {https://arxiv.org/pdf/2104.00681.pdf}
}

@inproceedings{pan2022iros,
  title     = {Voxfield: Non-Projective Signed Distance Fields for Online Planning and 3D Reconstruction},
  author    = {Yue Pan and Yves Kompis and Luca Bartolomei and Ruben Mascaro and Cyrill Stachniss and Margarita Chli},
  booktitle = iros,
  year      = {2022},
  url       = {https://www.ipb.uni-bonn.de/wp-content/papercite-data/pdf/pan2022iros.pdf}
}


@inproceedings{martin-braualla2021cvpr,
  title     = {Nerf in the wild: Neural radiance fields for unconstrained photo collections},
  author    = {Martin-Brualla, Ricardo and Radwan, Noha and Sajjadi, Mehdi SM and Barron, Jonathan T and Dosovitskiy, Alexey and Duckworth, Daniel},
  booktitle = cvpr,
  year      = {2021},
  url       = {https://arxiv.org/pdf/2008.02268.pdf}
}

@inproceedings{xu2021neurips,
  title     = {Generative occupancy fields for 3d surface-aware image synthesis},
  author    = {Xu, Xudong and Pan, Xingang and Lin, Dahua and Dai, Bo},
  booktitle = neurips,
  year      = {2021},
  url       = {https://arxiv.org/pdf/2111.00969.pdf}
}

@inproceedings{oechsle2021iccv,
  title     = {Unisurf: Unifying neural implicit surfaces and radiance fields for multi-view reconstruction},
  author    = {Oechsle, Michael and Peng, Songyou and Geiger, Andreas},
  booktitle = iccv,
  year      = {2021},
  url       = {https://arxiv.org/pdf/2104.10078.pdf}
}

@inproceedings{zhu2022cvpr,
  title     = {NICE-SLAM: Neural Implicit Scalable Encoding for SLAM},
  author    = {Zhu, Zihan and Peng, Songyou and Larsson, Viktor and Xu, Weiwei and Bao, Hujun and Cui, Zhaopeng and Oswald, Martin R and Pollefeys, Marc},
  booktitle = cvpr,
  year      = {2022},
  url       = {https://arxiv.org/pdf/2112.12130.pdf}
}

@article{ramos2016ijrr,
  title   = {Hilbert maps: Scalable continuous occupancy mapping with stochastic gradient descent},
  author  = {Ramos, Fabio and Ott, Lionel},
  journal = ijrr,
  volume  = {35},
  number  = {14},
  pages   = {1717--1730},
  year    = {2016},
  url     = {https://journals.sagepub.com/doi/pdf/10.1177/0278364916684382?casa_token=AJ8x0RB7BS8AAAAA:7XrL00Ij7lvK4Yqwv8T4wnPkR8UyZABtwMSLtjNAWLIBWNCky9PHnfd0ppcazsNuI4DJ1iEhhrC_hQ}
}

@inproceedings{rematas2022cvpr,
  title     = {Urban Radiance Fields},
  author    = {Rematas, Konstantinos and Liu, Andrew and Srinivasan, Pratul P and Barron, Jonathan T and Tagliasacchi, Andrea and Funkhouser, Thomas and Ferrari, Vittorio},
  booktitle = cvpr,
  year      = {2022},
  url       = {https://urban-radiance-fields.github.io/images/go_urf.pdf}
}

@inproceedings{barron2022cvpr,
  title     = {Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields},
  author    = {Barron, Jonathan T and Mildenhall, Ben and Verbin, Dor and Srinivasan, Pratul P and Hedman, Peter},
  booktitle = cvpr,
  year      = {2022},
  url       = {https://arxiv.org/pdf/2111.12077.pdf}
}

@inproceedings{roessle2022cvpr,
  title     = {Dense depth priors for neural radiance fields from sparse input views},
  author    = {Roessle, Barbara and Barron, Jonathan T and Mildenhall, Ben and Srinivasan, Pratul P and Nie{\ss}ner, Matthias},
  booktitle = cvpr,
  year      = {2022},
  url       = {https://arxiv.org/pdf/2112.03288.pdf}
}

@inproceedings{lin2021iros,
  title     = {inerf: Inverting neural radiance fields for pose estimation},
  author    = {Yen-Chen, Lin and Florence, Pete and Barron, Jonathan T and Rodriguez, Alberto and Isola, Phillip and Lin, Tsung-Yi},
  booktitle = iros,
  year      = {2021},
  url       = {https://arxiv.org/pdf/2012.05877.pdf}
}


@article{adamkiewicz2022ral,
  title   = {Vision-only robot navigation in a neural radiance world},
  author  = {Adamkiewicz, Michal and Chen, Timothy and Caccavale, Adam and Gardner, Rachel and Culbertson, Preston and Bohg, Jeannette and Schwager, Mac},
  journal = ral,
  volume  = {7},
  number  = {2},
  pages   = {4606--4613},
  year    = {2022},
  url     = {https://arxiv.org/pdf/2110.00168.pdf}
}

@article{mccloskey1989plm,
  author  = {Micahel McCloskey and Neal J. Cohen},
  title   = {{Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem}},
  journal = {Psychology of Learning and Motivation},
  volume  = {24},
  year    = {1989},
  pages   = {109--165},
  url     = {https://www.andywills.info/hbab/mccloskeycohen.pdf}
}

@article{kirkpatrick2017pnas,
  title   = {Overcoming catastrophic forgetting in neural networks},
  author  = {Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and others},
  journal = pnas,
  volume  = {114},
  number  = {13},
  pages   = {3521--3526},
  year    = {2017},
  url     = {https://arxiv.org/pdf/1612.00796.pdf}
}

@inproceedings{aljundi2018eccv,
  title     = {Memory aware synapses: Learning what (not) to forget},
  author    = {Aljundi, Rahaf and Babiloni, Francesca and Elhoseiny, Mohamed and Rohrbach, Marcus and Tuytelaars, Tinne},
  booktitle = eccv,
  year      = {2018},
  url       = {https://arxiv.org/pdf/1711.09601.pdf}
}

@inproceedings{zenke2017icml,
  title     = {Continual learning through synaptic intelligence},
  author    = {Zenke, Friedemann and Poole, Ben and Ganguli, Surya},
  booktitle = icml,
  year      = {2017},
  url       = {http://proceedings.mlr.press/v70/zenke17a/zenke17a.pdf}
}

@inproceedings{wang2021threedv,
  author    = {Jingwen Wang and Martin RÃ¼nz and Lourdes Agapito},
  title     = {DSP-SLAM: Object Oriented SLAM with Deep Shape Priors},
  booktitle = threedv,
  year      = {2021},
  url       = {https://arxiv.org/pdf/2108.09481.pdf}
}

@inproceedings{schmid2022icra,
  title     = {Panoptic Multi-TSDFs: a Flexible Representation for Online Multi-resolution Volumetric Mapping and Long-term Dynamic Scene Consistency},
  author    = {Schmid, Lukas and Delmerico, Jeffrey and Sch{\"o}nberger, Johannes and Nieto, Juan and Pollefeys, Marc and Siegwart, Roland and Cadena, Cesar},
  booktitle = icra,
  year      = {2022},
  url       = {https://arxiv.org/pdf/2109.10165.pdf}
}

@inproceedings{zakharov2020cvpr,
  title     = {Autolabeling 3D Objects with Differentiable Rendering of SDF Shape Priors},
  author    = {Sergey Zakharov and Wadim Kehl and Arjun Bhargava and Adrien Gaidon},
  booktitle = cvpr,
  year      = {2020},
  url       = {https://arxiv.org/pdf/1911.11288.pdf}
}

@inproceedings{yuan2018threedv,
  title     = {Pcn: Point completion network},
  author    = {Yuan, Wentao and Khot, Tejas and Held, David and Mertz, Christoph and Hebert, Martial},
  booktitle = threedv,
  year      = {2018},
  url       = {https://arxiv.org/pdf/1808.00671.pdf}
}

@inproceedings{irshad2022eccv,
  title     = {ShAPO: Implicit Representations for Multi Object Shape Appearance and Pose Optimization},
  author    = {Muhammad Zubair Irshad and Sergey Zakharov and Rares Ambrus and Thomas Kollar and Zsolt Kira and Adrien Gaidon},
  booktitle = eccv,
  year      = {2022},
  url       = {https://arxiv.org/pdf/2207.13691.pdf}
}

@inproceedings{marangoz2022case,
  title     = {Fruit mapping with shape completion for autonomous crop monitoring},
  author    = {Marangoz, Salih and Zaenker, Tobias and Menon, Rohit and Bennewitz, Maren},
  booktitle = case,
  year      = {2022},
  url       = {https://arxiv.org/pdf/2203.15489.pdf}
}

@article{sa2016sensors,
  title   = {Deepfruits: A fruit detection system using deep neural networks},
  author  = {Sa, Inkyu and Ge, Zongyuan and Dayoub, Feras and Upcroft, Ben and Perez, Tristan and McCool, Chris},
  journal = sensors,
  volume  = {16},
  number  = {8},
  pages   = {1222},
  year    = {2016},
  url     = {https://researchmgt.monash.edu/ws/portalfiles/portal/250858891/250858645_oa.pdf}
}

@inproceedings{lehnert2018iros,
  title     = {3d move to see: Multi-perspective visual servoing for improving object views with semantic segmentation},
  author    = {Lehnert, Chris and Tsai, Dorian and Eriksson, Anders and McCool, Chris},
  booktitle = iros,
  year      = {2018},
  url       = {https://arxiv.org/pdf/1809.07896.pdf}
}

@article{botterill2017jfr,
  title   = {A robot system for pruning grape vines},
  author  = {Botterill, Tom and Paulin, Scott and Green, Richard and Williams, Samuel and Lin, Jessica and Saxton, Valerie and Mills, Steven and Chen, XiaoQi and Corbett-Davies, Sam},
  journal = jfr,
  volume  = {34},
  number  = {6},
  pages   = {1100--1122},
  year    = {2017},
  url     = {http://hilandtom.com/tombotterill/pruner-preprint.pdf}
}

@article{yi2020sensors,
  title   = {Deep learning for non-invasive diagnosis of nutrient deficiencies in sugar beet using RGB images},
  author  = {Yi, Jinhui and Krusenbaum, Lukas and Unger, Paula and H{\"u}ging, Hubert and Seidel, Sabine J and Schaaf, Gabriel and Gall, Juergen},
  journal = sensors,
  volume  = {20},
  number  = {20},
  pages   = {5893},
  year    = {2020},
  url     = {https://pdfs.semanticscholar.org/0af8/78d6058fdf711962710e8b1622a9d6905eb2.pdf}
}

@article{zermas2020cea,
  title   = {3D model processing for high throughput phenotype extraction--the case of corn},
  author  = {Zermas, Dimitris and Morellas, Vassilios and Mulla, David and Papanikolopoulos, Nikos},
  journal = cea,
  volume  = {172},
  pages   = {105047},
  year    = {2020},
  url     = {https://www.researchgate.net/profile/Dimitris-Zermas-2/publication/337052401_3D_model_processing_for_high_throughput_phenotype_extraction_-_the_case_of_corn/links/5e220537299bf1e1fab9e85e/3D-model-processing-for-high-throughput-phenotype-extraction-the-case-of-corn.pdf}
}

@inproceedings{ahmadi2022iros,
  title     = {{BonnBot-I: A Precise Weed Management and Crop Monitoring Platform}},
  author    = {Ahmadi, Alireza and Halstead, Michael and McCool, Chris},
  booktitle = iros,
  year      = {2022},
  url       = {proceedings: ahmadi2022iros.pdf}
}

@inproceedings{halstead2020dicta,
  title     = {Fruit detection in the wild: The impact of varying conditions and cultivar},
  author    = {Halstead, Michael and Denman, Simon and Fookes, Clinton and McCool, Chris},
  booktitle = {Proc. of Digital Image Comp.: Techniques and Applications (DICTA)},
  year      = {2020},
  url       = {https://www.conferences.com.au/wp-content/uploads/2020/09/21_CameraReady.pdf}
}

// Introduced by marcuzzi2023ral-iros:
@inproceedings{cheng2021neurips,
  title     = {Per-Pixel Classification is Not All You Need for Semantic Segmentation},
  author    = {Bowen Cheng and Alexander G. Schwing and Alexander Kirillov},
  booktitle = neurips,
  year      = {2021},
  url       = {https://arxiv.org/pdf/2107.06278.pdf}
}

@inproceedings{cheng2021cvpr,
  title     = {Masked-attention Mask Transformer for Universal Image Segmentation},
  author    = {Bowen Cheng and Ishan Misra and Alexander G. Schwing and Alexander Kirillov and Rohit Girdhar},
  booktitle = cvpr,
  year      = {2022},
  url       = {https://arxiv.org/pdf/2112.01527.pdf}
}

@inproceedings{carion2020eccv,
  author    = {Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
  title     = {End-to-End Object Detection with Transformers},
  booktitle = eccv,
  year      = 2020,
  url       = {https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123460205.pdf}
}

@inproceedings{misra2021iccv,
  title     = {{An End-to-End Transformer Model for 3D Object Detection}},
  author    = {Misra, Ishan and Girdhar, Rohit and Joulin, Armand},
  booktitle = iccv,
  year      = 2021,
  url       = {https://arxiv.org/pdf/2109.08141.pdf}
}

@inproceedings{zhu2021cvpr-caac,
  author    = {X. Zhu and H. Zhou and T. Wang and F. Hong and Y. Ma and W. Li and H. Li and D. Lin},
  title     = {{Cylindrical and Asymmetrical 3D Convolution Networks for LiDAR Segmentation}},
  booktitle = cvpr,
  year      = 2021,
  abstract  = {State-of-the-art methods for large-scale driving-scene LiDAR segmentation often project the point clouds to 2D space and then process them via 2D convolution. Although this corporation shows the competitiveness in the point cloud, it inevitably alters and abandons the 3D topology and geometric relations. A natural remedy is to utilize the 3D voxelization and 3D convolution network. However, we found that in the outdoor point cloud, the improvement obtained in this way is quite limited. An important reason is the property of the outdoor point cloud, namely sparsity and varying density. Motivated by this investigation, we propose a new framework for the outdoor LiDAR segmentation, where cylindrical partition and asymmetrical 3D convolution networks are designed to explore the 3D geometric pattern while mainaining these inhernt properties. Moreover, a point-wise refinement module is introduced to alleviate the interference of lossy voxel-based label encoding. We evaluate the proposed model on two large-scale datasets , i.e., SemanticKITTI and nuScenes. Our method achieves the 1st place in the leaderboard of SemanticKITTI and outperforms existing methods on nuScenes with a noticeable margin. Furthermore, the proposed 3D framework also generalizes well to LiDAR panoptic segmentation and LiDAR 3D detection.},
  url       = {proceedings: zhu2021cvpr-caac.pdf}
}

@inproceedings{fan2021iccv-rido,
  author    = {L. Fan and X. Xiong and F. Wang and N. Wang and Z. Zhang},
  title     = {{RangeDet: In Defense of Range View for LiDAR-Based 3D Object Detection}},
  booktitle = iccv,
  year      = 2021,
  abstract  = {In this paper, we propose an anchor-free single-stage LiDAR-based 3D object detector -- RangeDet. The most notable difference with previous works is that our method is purely based on the range view representation. Compared with the commonly used voxelized or Bird's Eye View (BEV) representations, the range view representation is more compact and without quantization error. Although there are works adopting it for semantic segmentation, its performance in object detection is largely behind voxelized or BEV counterparts. We first analyze the existing range-view-based methods and find two issues overlooked by previous works: 1) the scale variation between nearby and far away objects; 2) the inconsistency between the 2D range image coordinates used in feature extraction and the 3D Cartesian coordinates used in output. Then we deliberately design three components to address these issues in our RangeDet. We test our RangeDet in the large-scale Waymo Open Dataset (WOD). Our best model achieves 72.9/75.9/65.8 3D AP on vehicle/pedestrian/cyclist. These results outperform other range-view-based methods by a large margin, and are overall comparable with the state-of-the-art multi-view-based methods. Codes will be released at https://github.com/TuSimple/RangeDet.},
  url       = {proceedings: fan2021iccv-rido.pdf}
}

@inproceedings{zhang2020icra-isol,
  author    = {F. Zhang and C. Guan and J. Fang and S. Bai and R. Yang and P. Torr and V. Prisacariu},
  title     = {{Instance Segmentation of LiDAR Point Clouds}},
  booktitle = icra,
  year      = 2020,
  keywords  = {Object Detection, Segmentation and Categorization, Deep Learning in Robotics and Automation, Computer Vision for Transportation},
  abstract  = {We propose a robust baseline method for instance segmentation which are specially designed for large-scale outdoor LiDAR point clouds. Our method includes a novel dense feature encoding technique, allowing the localization and segmentation of small, far-away objects, a simple but effective solution for single-shot instance prediction and effective strategies for handling severe class imbalances. Since there is no public dataset for the study of LiDAR instance segmentation, we also build a new publicly available LiDAR point cloud dataset to include both precise 3D bounding box and point-wise labels for instance segmentation, while still being about 320 times as large as other existing LiDAR datasets. The dataset will be published at https://github.com/feihuzhang/LiDARSeg.},
  url       = {proceedings: zhang2020icra-sol.pdf}
}

@article{yan2018sensors,
  author   = {Yan, Yan and Mao, Yuxing and Li, Bo},
  title    = {SECOND: Sparsely Embedded Convolutional Detection},
  journal  = sensors,
  volume   = {18},
  year     = {2018},
  number   = {10},
  pages    = {3337},
  abstract = {LiDAR-based or RGB-D-based object detection is used in numerous applications, ranging from autonomous driving to robot vision. Voxel-based 3D convolutional networks have been used for some time to enhance the retention of information when processing point cloud LiDAR data. However, problems remain, including a slow inference speed and low orientation estimation performance. We therefore investigate an improved sparse convolution method for such networks, which significantly increases the speed of both training and inference. We also introduce a new form of angle loss regression to improve the orientation estimation performance and a new data augmentation approach that can enhance the convergence speed and performance. The proposed network produces state-of-the-art results on the KITTI 3D object detection benchmarks while maintaining a fast inference speed.},
  url      = {https://mdpi-res.com/d_attachment/sensors/sensors-18-03337/article_deploy/sensors-18-03337.pdf?version=1538798176}
}

@inproceedings{wang2021cvpr,
  author    = {Huiyu Wang and Yukun Zhu and Hartwig Adam and Alan Yuille and Liang-Chieh Chen},
  title     = {{MaX-DeepLab}: End-to-End Panoptic Segmentation with Mask Transformers},
  booktitle = cvpr,
  year      = 2021,
  url       = {https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_MaX-DeepLab_End-to-End_Panoptic_Segmentation_With_Mask_Transformers_CVPR_2021_paper.pdf}
}

@inproceedings{sirohi2022icra,
  author    = {K. Sirohi and R. Mohan and D. B\"uscher and W. Burgard and A. Valada},
  title     = {{EfficientLPS Efficient LiDAR Panoptic Segmentation}},
  booktitle = icra,
  year      = 2022,
  keywords  = {Object Detection, Segmentation and Categorization, Computer Vision for Transportation, Semantic Scene Understanding},
  abstract  = {Panoptic segmentation of point clouds is a crucial task that enables autonomous vehicles to comprehend their vicinity using their highly accurate and reliable LiDAR sensors. Existing topdown approaches tackle this problem by either combining independent task-specific networks or translating methods from the image domain ignoring the intricacies of LiDAR data and thus often resulting in suboptimal performance. In this article, we present the novel topdown efficient LiDAR panoptic segmentation (EfficientLPS) architecture that addresses multiple challenges in segmenting LiDAR point clouds, including distance-dependent sparsity, severe occlusions, large scale-variations, and reprojection errors. EfficientLPS comprises of a novel shared backbone that encodes with strengthened geometric transformation modeling capacity and aggregates semantically rich range-aware multiscale features. It incorporates new scale-invariant semantic and instance segmentation heads along with the panoptic fusion module which is supervised by our proposed panoptic periphery loss function. Additionally, we formulate a regularized pseudolabeling framework to further improve the performance of EfficientLPS by training on unlabeled data. We benchmark our proposed model on two largescale LiDAR datasets: nuScenes, for which we also provide ground truth annotations, and SemanticKITTI. Notably, EfficientLPS sets the new state-of-the-art on both these datasets. Index TermsInstance segmentation, panoptic segmentation, scene understanding, semantic segmentation.},
  url       = {proceedings: sirohi2022icra.pdf}
}

@inproceedings{cai2018cvpr-crdi,
  author    = {Z. Cai and N. Vasconcelos},
  title     = {{Cascade R-CNN: Delving Into High Quality Object Detection}},
  booktitle = cvpr,
  year      = 2018,
  abstract  = {In object detection, an intersection over union (IoU) threshold is required to define positives and negatives. An object detector, trained with low IoU threshold, e.g. 0.5, usually produces noisy detections. However, detection performance tends to degrade with increasing the IoU thresholds. Two main factors are responsible for this: 1) overfitting during training, due to exponentially vanishing positive samples, and 2) inference-time mismatch between the IoUs for which the detector is optimal and those of the input hypotheses. A multi-stage object detection architecture, the Cascade R-CNN, is proposed to address these problems. It consists of a sequence of detectors trained with increasing IoU thresholds, to be sequentially more selective against close false positives. The detectors are trained stage by stage, leveraging the observation that the output of a detector is a good distribution for training the next higher quality detector. The resampling of progressively improved hypotheses guarantees that all detectors have a positive set of examples of equivalent size, reducing the overfitting problem. The same cascade procedure is applied at inference, enabling a closer match between the hypotheses and the detector quality of each stage. A simple implementation of the Cascade R-CNN is shown to surpass all single-model object detectors on the challenging COCO dataset. Experiments also show that the Cascade R-CNN is widely applicable across detector architectures, achieving consistent gains independently of the baseline detector strength. The code is available at https://github.com/zhaoweicai/cascade-rcnn.},
  url       = {http://openaccess.thecvf.com/content_cvpr_2018/html/../../content_cvpr_2018/papers/Cai_Cascade_R-CNN_Delving_CVPR_2018_paper.pdf}
}

@inproceedings{vu2022cvpr,
  author    = {T. Vu and K. Kim and T. M. Luu and T. Nguyen and C. D. Yoo},
  title     = {{SoftGroup for 3D Instance Segmentation on Point Clouds}},
  booktitle = cvpr,
  year      = 2022,
  abstract  = {Existing state-of-the-art 3D instance segmentation methods perform semantic segmentation followed by grouping. The hard predictions are made when performing semantic segmentation such that each point is associated with a single class. However, the errors stemming from hard decision propagate into grouping that results in (1) low overlaps between the predicted instance with the ground truth and (2) substantial false positives. To address the aforementioned problems, this paper proposes a 3D instance segmentation method referred to as SoftGroup by performing bottom-up soft grouping followed by top-down refinement. SoftGroup allows each point to be associated with multiple classes to mitigate the problems stemming from semantic prediction errors and suppresses false positive instances by learning to categorize them as background. Experimental results on different datasets and multiple evaluation metrics demonstrate the efficacy of SoftGroup. Its performance surpasses the strongest prior method by a significant margin of +6.2\% on the ScanNet v2 hidden test set and +6.8\% on S3DIS Area 5 in terms of AP50. SoftGroup is also fast, running at 345ms per scan with a single Titan X on ScanNet v2 dataset. The source code and trained models for both datasets are available at https://github.com/thangvubk/SoftGroup.git.},
  url       = {proceedings: vu2022cvpr.pdf}
}

@inproceedings{yin2021cvpr-coda,
  author    = {T. Yin and X. Zhou and P. Krahenbuhl},
  title     = {{Center-Based 3D Object Detection and Tracking}},
  booktitle = cvpr,
  year      = 2021,
  abstract  = {Three-dimensional objects are commonly represented as 3D boxes in a point-cloud. This representation mimics the well-studied image-based 2D bounding-box detection but comes with additional challenges. Objects in a 3D world do not follow any particular orientation, and box-based detectors have difficulties enumerating all orientations or fitting an axis-aligned bounding box to rotated objects. In this paper, we instead propose to represent, detect, and track 3D objects as points. Our framework, CenterPoint, first detects centers of objects using a keypoint detector and regresses to other attributes, including 3D size, 3D orientation, and velocity. In a second stage, it refines these estimates using additional point features on the object. In CenterPoint, 3D object tracking simplifies to greedy closest-point matching. The resulting detection and tracking algorithm is simple, efficient, and effective. On the nuScenes and Waymo datasets, CenterPoint surpasses prior methods by a large margin. On the Waymo Open Dataset, CenterPoint improves previous state-of-the-art by 10-20\% while running at 13FPS. The code and pretrained models are available at https://github.com/tianweiy/CenterPoint.},
  url       = {proceedings: yin2021cvpr-coda.pdf}
}

@inproceedings{jiang2020cvpr-pdpg,
  author    = {L. Jiang and H. Zhao and S. Shi and S. Liu and C. Fu and J. Jia},
  title     = {{PointGroup: Dual-Set Point Grouping for 3D Instance Segmentation}},
  booktitle = cvpr,
  year      = 2020,
  abstract  = {Instance segmentation is an important task for scene understanding. Compared to the fully-developed 2D, 3D instance segmentation for point clouds have much room to improve. In this paper, we present PointGroup, a new end-to-end bottom-up architecture, specifically focused on better grouping the points by exploring the void space between objects. We design a two-branch network to extract point features and predict semantic labels and offsets, for shifting each point towards its respective instance centroid. A clustering component is followed to utilize both the original and offset-shifted point coordinate sets, taking advantage of their complementary strength. Further, we formulate the ScoreNet to evaluate the candidate instances, followed by the Non-Maximum Suppression (NMS) to remove duplicates. We conduct extensive experiments on two challenging datasets, ScanNet v2 and S3DIS, on which our method achieves the highest performance, 63.6\% and 64.0\%, compared to 54.9\% and 54.4\% achieved by former best solutions in terms of mAP with IoU threshold 0.5.},
  url       = {proceedings: jiang2020cvpr-pdpg.pdf}
}

@inproceedings{razani2021iccv,
  author    = {R. Razani and R. Cheng and E. Li and E. Taghavi and Y. Ren and L. Bingbing},
  title     = {{GP-S3Net: Graph-Based Panoptic Sparse Semantic Segmentation Network}},
  booktitle = iccv,
  year      = 2021,
  abstract  = {Panoptic segmentation as an integrated task of both static environmental understanding and dynamic object identification, has recently begun to receive broad research interest. In this paper, we propose a new computationally efficient LiDAR based panoptic segmentation framework, called GP-S3Net. GP-S3Net is a proposal-free approach in which no object proposals are needed to identify the objects in contrast to conventional two-stage panoptic systems, where a detection network is incorporated for capturing instance information.  Our new design consists of a novel instance-level network to process the semantic results by constructing a graph convolutional network to identify objects (foreground), which later on are fused with the background classes. Through the fine-grained clusters of the foreground objects from the semantic segmentation backbone, over-segmentation priors are generated and subsequently processed by 3D sparse convolution to embed each cluster. Each cluster is treated as a node in the graph and its corresponding embedding is used as its node feature. Then a GCNN predicts whether edges exist between each cluster pair. We utilize the instance label to generate ground truth edge labels for each constructed graph in order to supervise the learning. Extensive experiments demonstrate that GP-S3Net outperforms the current state-of-the-art approaches, by a significant margin across available datasets such as, nuScenes and SemanticPOSS, ranking first on the competitive public SemanticKITTI leaderboard upon publication.},
  url       = {proceedings: razani2021iccv.pdf}
}

@inproceedings{hurtado2020cvpr,
  title     = {MOPT: Multi-Object Panoptic Tracking},
  author    = {Hurtado, Juana Valeria and Mohan, Rohit and Burgard, Wolfram and Valada, Abhinav},
  booktitle = cvpr,
  year      = 2020,
  url       = {https://arxiv.org/pdf/2004.08189.pdf}
}

@inproceedings{meng2021iccv-cdff,
  author    = {D. Meng and X. Chen and Z. Fan and G. Zeng and H. Li and Y. Yuan and L. Sun and J. Wang},
  title     = {{Conditional DETR for Fast Training Convergence}},
  booktitle = iccv,
  year      = 2021,
  abstract  = {The recently-developed DETR approach applies the transformer encoder and decoder architecture to object detection and achieves promising performance. In this paper, we handle the critical issue, slow training convergence, and present a conditional cross-attention mechanism for fast DETR training. Our approach is motivated by that the cross-attention in DETR relies highly on the content embeddings for localizing the four extremities and predicting the box, which increases the need for high-quality content embeddings and thus the training difficulty. Our approach, named conditional DETR, learns a conditional spatial query from the decoder embedding for decoder multi-head cross-attention. The benefit is that through the conditional spatial query, each cross-attention head is able to attend to a band containing a distinct region, e.g., one object extremity or a region inside the object box. This narrows down the spatial range for localizing the distinct regions for object classification and box regression, thus relaxing the dependence on the content embeddings and easing the training. Empirical results show that conditional DETR converges 6.7x faster for the backbones R50 and R101 and 10x faster for stronger backbones DC5-R50 and DC5-R101. Code is available at https://github.com/Atten4Vis/ConditionalDETR.},
  url       = {proceedings: meng2021iccv-cdff.pdf}
}

@inproceedings{zhu2021iclr,
  title     = {Deformable DETR: Deformable Transformers for End-to-End Object Detection},
  author    = {Xizhou Zhu and Weijie Su and Lewei Lu and Bin Li and Xiaogang Wang and Jifeng Dai},
  booktitle = iclr,
  year      = 2021,
  url       = {https://openreview.net/pdf?id=gZ9hCDWe6ke}
}

@inproceedings{kirillov2020cvpr,
  author    = {A. Kirillov and Y. Wu and K. He and R. Girshick},
  title     = {{PointRend: Image Segmentation As Rendering}},
  booktitle = cvpr,
  year      = 2020,
  abstract  = {We present a new method for efficient high-quality image segmentation of objects and scenes. By analogizing classical computer graphics methods for efficient rendering with over- and undersampling challenges faced in pixel labeling tasks, we develop a unique perspective of image segmentation as a rendering problem. From this vantage, we present the PointRend (Point-based Rendering) neural network module: a module that performs point-based segmentation predictions at adaptively selected locations based on an iterative subdivision algorithm. PointRend can be flexibly applied to both instance and semantic segmentation tasks by building on top of existing state-of-the-art models. While many concrete implementations of the general idea are possible, we show that a simple design already achieves excellent results. Qualitatively, PointRend outputs crisp object boundaries in regions that are over-smoothed by previous methods. Quantitatively, PointRend yields significant gains on COCO and Cityscapes, for both instance and semantic segmentation. PointRend's efficiency enables output resolutions that are otherwise impractical in terms of memory or computation compared to existing approaches. Code has been made available at https://github.com/facebookresearch/detectron2/tree/master/projects/PointRend.},
  url       = {proceedings: kirillov2020cvpr.pdf}
}

@inproceedings{loshchilov2018iclr,
  title     = {Decoupled Weight Decay Regularization},
  author    = {Ilya Loshchilov and Frank Hutter},
  booktitle = iclr,
  year      = 2019,
  url       = {https://openreview.net/pdf?id=Bkg6RiCqY7}
}

@article{alonso2020ral,
  author   = {I. Alonso and L. Riazuelo and L. Montesano and A.C. Murillo},
  title    = {{3D-MiniNet Learning a 2D Representation from Point Clouds for Fast and Efficient 3D LIDAR Semantic Segmentation}},
  journal  = ral,
  year     = 2020,
  volume   = {5},
  number   = {4},
  pages    = {5432--5439},
  keywords = {Semantic Scene Understanding, Object Detection, Segmentation and Categorization, Deep Learning for Visual Perception},
  abstract = {LIDAR semantic segmentation is an essential task that provides 3D semantic information about the environment to robots. Fast and efficient semantic segmentation methods are needed to match the strong computational and temporal restrictions of many real-world robotic applications. This work presents 3D-MiniNet, a novel approach for LIDAR semantic segmentation that combines 3D and 2D learning layers. It first learns a 2D representation from the raw points through a novel projection which extracts local and global information from the 3D data. This representation is fed to an efficient 2D Fully Convolutional Neural Network (FCNN) that produces a 2D semantic segmentation. These 2D semantic labels are reprojected back to the 3D space and enhanced through a postprocessing module. The main novelty in our strategy relies on the projection learning module. Our detailed ablation study shows how each component contributes to the final performance of 3D-MiniNet. We validate our approach on well known public benchmarks (SemanticKITTI and KITTI), where 3DMiniNet gets state-of-the-art results while being faster and more parameter-efficient than previous methods.},
  url      = {proceedings: alonso2020ral.pdf}
}

@article{hu2020ral-ltos,
  author   = {P. Hu and D. Held and D. Ramanan},
  title    = {{Learning to Optimally Segment Point Clouds}},
  journal  = ral,
  year     = 2020,
  volume   = {5},
  number   = {2},
  pages    = {875--882},
  keywords = {Deep Learning in Robotics and Automation, Object Detection, Segmentation and Categorization, Autonomous Vehicle Navigation},
  abstract = {We focus on the problem of class-agnostic instance segmentation of LiDAR point clouds. We propose an approach that combines graph-theoretic search with data-driven learning: it searches over a set of candidate segmentations and returns one where individual segments score well according to a datadriven point-based model of objectness. We prove that if we score a segmentation by the worst objectness among its individual segments, there is an efficient algorithm that finds the optimal worst-case segmentation among an exponentially large number of candidate segmentations. We also present an efficient algorithm for the average-case. For evaluation, we repurpose KITTI 3D detection as a segmentation benchmark and empirically demonstrate that our algorithms significantly outperform past bottom-up segmentation approaches and topdown object-based algorithms on segmenting point clouds.},
  url      = {proceedings: hu2020ral-ltos.pdf}
}

@inproceedings{fong2022icra,
  author    = {W.K. Fong and R. Mohan and H.J. Valeria and L. Zhou and H. Caesar and O. Beijbom and A. Valada},
  title     = {{Panoptic nuScenes A Large-Scale Benchmark for LiDAR Panoptic Segmentation and Tracking}},
  booktitle = icra,
  year      = 2022,
  keywords  = {Data Sets for Robotic Vision, Semantic Scene Understanding, Object Detection, Segmentation and Categorization},
  abstract  = {Panoptic scene understanding and tracking of dynamic agents are essential for robots and automated vehicles to navigate in urban environments. As LiDARs provide accurate illumination-independent geometric depictions of the scene, performing these tasks using LiDAR point clouds provides reliable predictions. However, existing datasets lack diversity in the type of urban scenes and have a limited number of dynamic object instances which hinders both learning of these tasks as well as credible benchmarking of the developed methods. In this paper, we introduce the large-scale Panoptic nuScenes benchmark dataset that extends our popular nuScenes dataset with point-wise groundtruth annotations for semantic segmentation, panoptic segmentation, and panoptic tracking tasks. To facilitate comparison, we provide several strong baselines for each of these tasks on our proposed dataset. Moreover, we analyze the drawbacks of the existing metrics for panoptic tracking and propose the novel instance-centric PAT metric that addresses the concerns. We present exhaustive experiments that demonstrate the utility of Panoptic nuScenes compared to existing datasets and make the online evaluation server available at nuScenes.org. We believe that this extension will accelerate the research of novel methods for scene understanding of dynamic urban environments. Index TermsList of keywords (from the RA Letters keyword list)},
  url       = {proceedings: fong2022icra}
}

@inproceedings{li2022cvpr,
  author    = {Li, Jinke and He, Xiao and Wen, Yang and Gao, Yuan and Cheng, Xiaoqiang and Zhang, Dan},
  title     = {Panoptic-PHNet: Towards Real-Time and High-Precision LiDAR Panoptic Segmentation via Clustering Pseudo Heatmap},
  booktitle = cvpr,
  year      = {2022},
  url       = {https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Panoptic-PHNet_Towards_Real-Time_and_High-Precision_LiDAR_Panoptic_Segmentation_via_Clustering_CVPR_2022_paper.pdf}
}

@inproceedings{li2022icra,
  author    = {Li, Enxu and Razani, Ryan and Xu, Yixuan and Liu, Bingbing},
  booktitle = icra,
  title     = {SMAC-Seg: LiDAR Panoptic Segmentation via Sparse Multi-directional Attention Clustering},
  year      = {2022},
  url       = {https://arxiv.org/pdf/2108.13588.pdf}
}

@article{li2021arxiv,
  author   = {Li, Enxu and Razani, Ryan and Xu, Yixuan and Liu, Bingbing},
  title    = {CPSeg: Cluster-free Panoptic Segmentation of 3D LiDAR Point Clouds},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  journal  = arxiv,
  volume   = {arXiv:2111.01723},
  year     = 2021,
  url      = {https://arxiv.org/abs/2111.01723}
}

@inproceedings{liu2022icra-pclf,
  author    = {M. Liu and Z. Qiang and H. Zhao and J. Li and Y. Du and K. Keutzer and L. DU and S. Zhang},
  title     = {{Prototype-Voxel Contrastive Learning for LiDAR Point Cloud Panoptic Segmentation}},
  booktitle = icra,
  year      = 2022,
  keywords  = {Semantic Scene Understanding, Representation Learning, Object Detection, Segmentation and Categorization},
  abstract  = { LiDAR point cloud panoptic segmentation, including both semantic and instance segmentation, plays a critical role in meticulous scene understanding for autonomous driving. Existing 3D voxelized approaches either utilize 3D sparse convolution that only focuses on local scene understanding, or add extra and time-consuming PointNet branch to capture global feature structures. To address these limitations, we propose an end-to-end Prototype-Voxel Contrastive Learning (PVCL) framework for learning stable and discriminative semantic representations, which includes voxel-level and prototype-level contrastive learning (CL). The voxel-level CL decreases intraclass distance and increases inter-class distance among sample representations, while the prototype-level CL further reduces the dependence of CL on negative sampling and avoids the influence of outliers from the same class, enabling PVCL to be more effective for outdoor point cloud panoptic segmentation. Extensive experiments are conducted on the public point cloud panoptic segmentation datasets, Semantic-KITTI and nuScenes, where evaluations and ablation studies demonstrate PVCL achieves superior performance compared with the state-of-theart. Our approach ranks the top on the public leaderboard of Semantic-KITTI at the time of submission, and surpasses the published 2nd rank, EfficientLPS, by 1.7\% in PQ.},
  url       = {proceedings: liu2022icra-pclf.pdf}
}

@inproceedings{xu2022aaai,
  author    = {Shuangjie Xu and Rui Wan and Maosheng Ye and Xiaoyi Zou and Tongyi Cao},
  title     = {Sparse Cross-Scale Attention Network for Efficient LiDAR Panoptic Segmentation},
  booktitle = aaai,
  year      = {2022},
  url       = {https://ojs.aaai.org/index.php/AAAI/article/view/20197/19956}
}

@inproceedings{detone2017cvpr,
  author    = {DeTone, Daniel and Malisiewicz, Tomasz and Rabinovich, Andrew},
  title     = {{SuperPoint: Self-Supervised Interest Point Detection and Description}},
  booktitle = cvpr,
  year      = {2017},
  abstract  = {This paper presents a self-supervised framework for training interest point
               detectors and descriptors suitable for a large number of multiple-view geometry
               problems in computer vision. As opposed to patch-based neural networks, our
               fully-convolutional model operates on full-sized images and jointly computes
               pixel-level interest point locations and associated descriptors in one forward
               pass. We introduce Homographic Adaptation, a multi-scale, multi-homography
               approach for boosting interest point detection repeatability and performing
               cross-domain adaptation (e.g., synthetic-to-real). Our model, when trained on
               the MS-COCO generic image dataset using Homographic Adaptation, is able to
               repeatedly detect a much richer set of interest points than the initial
               pre-adapted deep model and any other traditional corner detector. The final
               system gives rise to state-of-the-art homography estimation results on HPatches
               when compared to LIFT, SIFT and ORB.},
  url       = {https://arxiv.org/pdf/1712.07629.pdf}
}

@article{fischler1981cacm,
  author  = {M.A. Fischler and R.C. Bolles},
  title   = {{Random Sample Consensus: A Paradigm for Model Fitting with Applications to Image Analysis and Automated Cartography}},
  journal = cacm,
  volume  = {24},
  number  = {6},
  pages   = {381--395},
  year    = {1981},
  url     = {https://dl.acm.org/doi/pdf/10.1145/358669.358692}
}

@article{huang2021arxiv,
  author  = {Xiaoshui Huang and Guofeng Mei and Jian Zhang and Rana Abbas},
  title   = {{A Comprehensive Survey on Point Cloud Registration}},
  journal = arxiv,
  year    = {2021},
  volume  = {arXiv:2103.02690},
  url     = {https://arxiv.org/pdf/2103.02690.pdf}
}

@article{masone2021acce,
  author  = {Masone, Carlo and Caputo, Barbara},
  title   = {A Survey on Deep Visual Place Recognition},
  journal = {IEEE Access},
  volume  = {9},
  pages   = {19516--19547},
  year    = {2021},
  url     = {https://ieeexplore.ieee.org/ielx7/6287639/9312710/09336674.pdf}
}

@inproceedings{riccardi2023icra,
  author    = {A. Riccardi and S. Kelly and E. Marks and F. Magistri and T. Guadagnino and J. Behley and M. Bennewitz and C. Stachniss},
  title     = {{Fruit Tracking Over Time Using High-Precision Point Clouds}},
  booktitle = icra,
  year      = {2023},
  url       = {https://www.ipb.uni-bonn.de/wp-content/papercite-data/pdf/riccardi2023icra.pdf}
}

@article{felzenszwalb2004ijcv,
  author  = {Felzenszwalb, Pedro F. and Huttenlocher, Daniel P},
  title   = {Efficient Graph-Based Image Segmentation},
  journal = ijcv,
  volume  = {59},
  number  = {2},
  pages   = {167--181},
  year    = {2004},
  url     = {https://cs.brown.edu/people/pfelzens/papers/seg-ijcv.pdf}
}

@inproceedings{ozaki2020gecco,
  author    = {Ozaki, Yoshihiko and Tanigaki, Yuki and Watanabe, Shuhei and Onishi, Masaki},
  title     = {Multiobjective Tree-Structured Parzen Estimator for Computationally Expensive Optimization Problems},
  year      = {2020},
  booktitle = {Proc.~of Genetic and Evolutionary Computation Conference (GECCO)}
}

@article{zhang2009pami,
  title   = {Image segmentation with a unified graphical model},
  author  = {Zhang, Lei and Ji, Qiang},
  journal = pami,
  volume  = {32},
  number  = {8},
  pages   = {1406--1425},
  year    = {2009},
  url     = {https://sites.ecse.rpi.edu/~cvrl/Publication/pdf/Zhang2010.pdf}
}

@article{yin2021ac,
  title   = {Automated semantic segmentation of industrial point clouds using ResPointNet++},
  journal = {Automation in Construction},
  volume  = {130},
  pages   = {103874},
  year    = {2021},
  author  = {Chao Yin and Boyu Wang and Vincent J.L. Gan and Mingzhu Wang and Jack C.P. Cheng}
}

@inproceedings{xie2021neurips,
  title     = {SegFormer: Simple and efficient design for semantic segmentation with transformers},
  author    = {Xie, Enze and Wang, Wenhai and Yu, Zhiding and Anandkumar, Anima and Alvarez, Jose M and Luo, Ping},
  booktitle = neurips,
  year      = {2021},
  url       = {https://proceedings.neurips.cc/paper/2021/file/64f1f27bf1b4ec22924fd0acb550c235-Paper.pdf}
}

@article{nunes2022ral-3duis,
  author  = {Lucas Nunes and Xieyuanli Chen and Rodrigo Marcuzzi and Aljosa Osep and Laura Leal-TaixÃ© and Cyrill Stachniss and Jens Behley},
  title   = {{Unsupervised Class-Agnostic Instance Segmentation of 3D LiDAR Data for Autonomous Vehicles}},
  journal = ral,
  url     = {https://www.ipb.uni-bonn.de/pdfs/nunes2022ral-iros.pdf},
  codeurl = {https://github.com/PRBonn/3DUIS},
  volume  = 7,
  number  = 4,
  pages   = {8713--8720},
  year    = 2022
}

@article{marcuzzi2023ral,
  author  = {R. Marcuzzi and L. Nunes and L. Wiesmann and J. Behley and C. Stachniss},
  title   = {{Mask-Based Panoptic LiDAR Segmentation for Autonomous Driving}},
  journal = ral,
  volume  = {8},
  number  = {2},
  pages   = {1141--1148},
  year    = 2023,
  url     = {https://www.ipb.uni-bonn.de/wp-content/papercite-data/pdf/marcuzzi2023ral.pdf}
}

@article{woebbecke1995asae,
  title   = {Color indices for weed identification under various soil, residue, and lighting conditions},
  author  = {Woebbecke, David M and Meyer, George E and Von Bargen, Kenneth and Mortensen, David A},
  journal = {Trans. of the American Society of Agricultural Engineers},
  volume  = {38},
  number  = {1},
  pages   = {259--269},
  year    = {1995}
}

@article{hassanein2018sensors,
  author  = {Hassanein, Mohamed and Lari, Zahra and El-Sheimy, Naser},
  title   = {A New Vegetation Segmentation Approach for Cropped Fields Based on Threshold Detection from Hue Histograms},
  journal = sensors,
  volume  = {18},
  pages   = {1253},
  year    = {2018},
  number  = {4}
}

@article{bradski2000jst,
  author  = {Bradski, G.},
  journal = {Dr. Dobb's Journal of Software Tools},
  title   = {{The OpenCV Library}},
  volume  = {120},
  pages   = {122--125},
  year    = {2000}
}

@techreport{lawrence1998tr,
  title       = {What size neural network gives optimal generalization? Convergence properties of backpropagation},
  author      = {Lawrence, Steve and Giles, C Lee and Tsoi, Ah Chung},
  institution = {University of Maryland Institute for Advanced Computer Studies},
  number      = {3617},
  year        = {1998},
  url         = {https://clgiles.ist.psu.edu/papers/UMD-CS-TR-3617.what.size.neural.net.to.use.pdf}
}

@article{brice1970ai,
  title   = {Scene analysis using regions},
  author  = {Brice, Claude R and Fennema, Claude L},
  journal = ai,
  volume  = {1},
  number  = {3-4},
  pages   = {205--226},
  year    = {1970}
}


@article{adelson1983rca,
  author  = {Adelson, Edward and Anderson, Charles and Bergen, James and Burt, Peter and Ogden, Joan},
  year    = {1983},
  pages   = {33--41},
  title   = {Pyramid Methods in Image Processing},
  volume  = {29},
  journal = {RCA Engineer},
  url     = {http://persci.mit.edu/pub_pdfs/RCA84.pdf}
}

@article{kotropoulos2003prl,
  title   = {Segmentation of ultrasonic images using Support Vector Machines},
  journal = prl,
  volume  = {24},
  number  = {4},
  pages   = {715--727},
  year    = {2003},
  author  = {Constantine Kotropoulos and Ioannis Pitas},
  url     = {http://poseidon.csd.auth.gr/papers/PUBLISHED/JOURNAL/pdf/kotropoulos2002a.pdf}
}

@article{wang2011pr,
  title   = {Color image segmentation using pixel wise support vector machine classification},
  journal = pr,
  volume  = {44},
  number  = {4},
  pages   = {777--787},
  year    = {2011},
  author  = {Xiang-Yang Wang and Ting Wang and Juan Bu}
}

@article{burney2014ijca,
  title   = {K-means cluster analysis for image segmentation},
  author  = {Burney, S.M. Aqil and Tariq, Humera},
  journal = {Intl. Journal of Computer Applications},
  volume  = {96},
  number  = {4},
  pages   = {1--8},
  year    = {2014}
}

@article{dhanachandra2015pcs,
  title   = {Image Segmentation Using K -means Clustering Algorithm and Subtractive Clustering Algorithm},
  journal = {Procedia Computer Science},
  volume  = {54},
  pages   = {764--771},
  year    = {2015},
  author  = {Nameirakpam Dhanachandra and Khumanthem Manglem and Yambem Jina Chanu}
}

@article{liu2020mp,
  author  = {Liu, Caixia and Zhao, Ruibin and Pang, Mingyong},
  title   = {A fully automatic segmentation algorithm for CT lung images based on random forest},
  journal = {Medical Physics},
  volume  = {47},
  number  = {2},
  pages   = {518--529},
  year    = {2020}
}

@inproceedings{schroff2008bmvc,
  title     = {Object Class Segmentation using Random Forests},
  author    = {Schroff, Florian and Criminisi, Antonio and Zisserman, Andrew},
  booktitle = bmvc,
  year      = {2008},
  url       = {https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/Criminisi_bmvc2008.pdf}
}

@article{pong1984cvgip,
  title   = {Experiments in segmentation using a facet model region grower},
  journal = cvgip,
  volume  = {25},
  number  = {1},
  pages   = {1--23},
  year    = {1984},
  author  = {Ting-Chuen Pong and Linda G Shapiro and Layne T Watson and Robert M Haralick},
  url     = {https://haralick.org/journals/Experiments_Segmentation.pdf}
}

@article{reddi1984tsmc,
  author  = {Reddi, S. S. and Rudin, S. F. and Keshavan, H. R.},
  journal = tsmc,
  title   = {An optimal multiple threshold scheme for image segmentation},
  year    = {1984},
  volume  = {14},
  number  = {4},
  pages   = {661--665}
}


@article{pun1981cgip,
  title   = {Entropic thresholding, a new approach},
  journal = {Computer Graphics and Image Processing},
  volume  = {16},
  number  = {3},
  pages   = {210--239},
  year    = {1981},
  url     = { https://pdf.sciencedirectassets.com/273220/1-s2.0-S0146664X00X00214/1-s2.0-0146664X81900381/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEGUaCXVzLWVhc3QtMSJHMEUCIQCOH%2FfCl10oqgc%2BxFC7DZaX6tSWlek63Y%2Bmv1rncEXhDwIgfTFbg3wdRE3KckdiL0B55xSEuVcw806wnwFxhEFtzgQqsgUILRAFGgwwNTkwMDM1NDY4NjUiDFqtQ%2BF89GG6GlxXnSqPBVcmBMB2p%2BHhWURcQCAPRR7a6XKQSM1QiILAoDky6eMceI7cCrUVmZDN%2B18WT46aG3oXM20WovIFwqFSVzWlIwhGzN8ux4xO9iNWKaL8CzRr1KOkTMp2woKfEA41XR%2Bw%2BFySMhh1ClhZgx1VqbiXFzx6jEJlMbvUAGXZpGk%2B40CyGTX1nFznDzvxckVsfqrpPUGG9If%2Bd%2FYriJcDO34MijPPP4yohxqSS0wZsDkmICweIFvFRh76pxik1RPElkv1AJV13dac9c9JOSrPsjW0buAA2NAU824cpKVpaAwiEB1vPIMnF4q%2BuiUCJylRkMvKyYwxdqq%2BkpRyYDFoDdbex2tBhMalEjwHgQ6l4xja49%2B9cpgcsquxEerIm%2FyngjK2MZ7HLb2uz4bWig5pt8bMG63E%2Fp1iNzASOFP8eSuvTtqJ5xapZivW3M6nF%2F5LyEt1%2B3lZdXZvVST8ifEmOaxjLDGQIjkMb87b8wZvRlCHm41pqR%2FZNUSYd%2FK%2FMbsjDffDGQTEWOpB%2Ft%2FC3tMLHcVTx39ViToT7DonduYnnaASzzh8Fv0hTRCcfC%2BgAoVx3OiZUqAmvCXxX4SL4pIGe1o%2Fb5CUdsnM63Ms%2F%2B3VvxJhINocl3MUMWIJapQTfUZH%2BXxfPDUyyWs50dKBilDDKLImVgb9lJ5yoS4N6e%2Fpmg5PEIBApAUS3HSGN2kry1U53vja4KZZM2iswY2mMHvVY7xeIgtENE0TR8pyUbrlllStS5spKTXoYL7S45USEMk5JtMK3hFdUbPBwI8PpIflbasgZj%2BfnLyNFdB6oLtmfZIRg0b%2Bv58rl3Pdl6esqz7iBbE7fumXzR5znDGk%2Bbb6NoaDvbdkK%2BkA50yPNKSowtNWeXow9rDRoAY6sQHqXIB0Eo9WaTBLMnEP6UhM7JUNjgYwHJ3xaeX0AauGLS0U10KW5mHmK7BJ0jwbw27rkOoU8B6avbUpAF74jLmeRhVGB6nRo6xVU%2Ff96TzPYrXJ8Kf0YMHovJP65uYsX0OOE6QOMzAJXg1q1dh3hMQ7tj7CiO1YRSfIa05Kus8MbKsZIO%2Bxhot0N5zkixJAB6kKpKngeBkslSt4IMWsUqquWHrbpKIq%2FmwEz5KxEkpiARE%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20230317T125555Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTYT3RFO3N3%2F20230317%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=d8be4d73fd951842141cce496178dc0119975be77a4787f33b3ffa4b6c742b2b&hash=843c603e3662f98fa29c52bea515cc0d9d395a250a2d122359270700061f1177&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=0146664X81900381&tid=spdf-f452f711-20df-47bc-b5a8-ca99cce19ea4&sid=9c6211959c24784cb408aa46ea286b47824agxrqb&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=1e035905585759525905&rr=7a95649c5b07bb83&cc=de},
  author  = {T. Pun}
}

@article{wang1984cvgip,
  title   = {Automatic multithreshold selection},
  journal = cvgip,
  volume  = {25},
  number  = {1},
  pages   = {46--67},
  year    = {1984},
  url     = {https://pdf.sciencedirectassets.com/273221/1-s2.0-S0734189X00X00096/1-s2.0-0734189X84900483/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEGUaCXVzLWVhc3QtMSJIMEYCIQCZBVTf0corDDwxacIQfwWWLkkjRPJbgzOE8x%2FNajK09wIhAMUFbi2Rd1jXM17vU63lp7e5lOiiQDj9Oly88%2FeSnRtuKrIFCC0QBRoMMDU5MDAzNTQ2ODY1IgyxzI%2F7PbQvEOun%2B7wqjwXL23S4ZJTOyP7twdFhMlIQVFdaRaoVXrHgjqZLUXTqKK2BnSdd%2FgUQ5vUbBypBKx08FhaivzXSWu9zlPKpnccsFTDJ30kHsLw1SlWQVZCGWhOn0JkMmDI7WKbkWHKKafFqS3p6LfXjhQP8AzNLX2T9gxP%2FLaodWZCha%2FDMuLSMWJICPq5K29hUIxShTNJUfuag9Ldw%2BUx0r0aEd1DXuTfre5B8rbQkb6%2FAq%2FzrbOJnWCuffdS5IiyO16eDAkM6sPZk7TmPRKkj4FhrLaFIIUMJvS%2FkzbdoqI7fVvlbPpTaVrbsop%2BSbHDu%2F0w1IJxZMfbN1mlMLOmusH72YQ%2BZrX3zf4lC%2BFyKLZcYGcrwGd1BsVTTX8M7mAkSrp%2FX4KmhQF7Yqwbgcu5jGlVf3XAYc8GAhTTuxx5MR%2BQdWGEohTj5DbVQiXblLBjrMO3X5ycDMFtGgeleWrwzBh47qu7CdPRXmv2Hmtut%2FfuLzgszMus%2FeM%2BpevnxSmgjvGRcMoBOhdPsGX7Aoc8oI3%2F5o0d8wRKbgT5oJ5OXpiCkfrbPPvTD%2FbuFZ9IP83HII1pIME0pIKzujidL1Oyyv836cbQS82ETwn6BkUKp69OGOCEW4p00rKQS%2BZpUApTzm0HG9ao8Sz%2B3sp9Xp5Qfs15%2FsyzPA%2BjWdtg71LoltOiU56WOpKjVRWS26HkuRXpvGIvDEch8g4K2rnq4LIrKEPxmeYncLgsCXEODaWCMBiSqQrqS3pPcLqjHSK7K6oZ8ww9Oot5E4u8%2FVYTuZepGZGqp5B9w5aVY4khF%2FpGx0kTdwWic7L1skEfIiMJeTBNchxGEiQrnIe4%2BqbhwqRaeZWLCpaXfq6eOy5yoV%2BlAUopyTPKpz9NjMNux0aAGOrABsZIJtj89csig8Weetx099wTodYbZiIxPDC09m5jvJ%2FUjYelLVYtgeymZAcGJ6EYvFMpbERjNRuvZyyagFIa9b33zJcS0Mhw%2F9fbnK3gVauGgICKh6fj7xx2BDLET09%2BhEm7ETddk22eTQFpMQ9OKe6k%2FENeERADRNPRsM2dwALEVZV%2BvYWeIrmBQ25bXSu2NGQO3HbG9%2Fg69hPFy%2BUhxip5j2jsmVGmBlcESXNKJvBg%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20230317T125835Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTY4NAPRB53%2F20230317%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=cc9aeccf609b73aebca9f5b21db6d97cb1043c79f234134d852caadf77594687&hash=6f29ec28c6e4a28a90a03f3cf56fbb1f650f9696b8d71cbabd80c955468276fd&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=0734189X84900483&tid=spdf-c7c7bfa2-3868-401a-a912-b751e6ea6bc9&sid=9c6211959c24784cb408aa46ea286b47824agxrqb&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=1e0359055857595e5700&rr=7a95687fa864bb83&cc=de},
  author  = {Shyuan Wang and Robert M. Haralick}
}

@article{kittler1985tsmc,
  author  = {Kittler, J. and Illingworth, J.},
  journal = tsmc,
  title   = {On threshold selection using clustering criteria},
  year    = {1985},
  volume  = {15},
  number  = {5},
  pages   = {652--655}
}

@article{nobuyuki1979tsmc,
  author  = {Otsu, Nobuyuki},
  journal = tsmc,
  title   = {A Threshold Selection Method from Gray-Level Histograms},
  year    = {1979},
  volume  = {9},
  number  = {1},
  pages   = {62--66}
}


@article{deravi1983prl,
  title   = {Grey level thresholding using second-order statistics},
  journal = prl,
  volume  = {1},
  number  = {5},
  pages   = {417--422},
  year    = {1983},
  author  = {F. Deravi and S.K. Pal}
}

@article{yanowitz1989cvgip,
  title   = {A new method for image segmentation},
  journal = cvgip,
  volume  = {46},
  number  = {1},
  pages   = {82--95},
  year    = {1989},
  author  = {S.D. Yanowitz and A.M. Bruckstein}
}

@inproceedings{tomita1973ijcai,
  title     = {Detection of Homogeneous Regions by Structural Analysis.},
  author    = {Tomita, Fumiaki and Yachida, Masahiko and Tsuji, Saburo},
  booktitle = ijcai,
  year      = {1973}
}


@article{sietsma1991nn,
  title   = {Creating artificial neural networks that generalize},
  author  = {Sietsma, Jocelyn and Dow, Robert JF},
  journal = {Neural Networks},
  volume  = {4},
  number  = {1},
  pages   = {67--79},
  year    = {1991}
}

@inproceedings{yoshimoto1990ijcnn,
  author    = {Yoshimoto, S.},
  booktitle = {Proc. of the Intl. Joint Conf. on Neural Networks~(IJCNN)},
  title     = {A study on artificial neural network generalization capability},
  year      = {1990}
}

@inproceedings{yang2023wacv,
  title     = {Tvt: Transferable vision transformer for unsupervised domain adaptation},
  author    = {Yang, Jinyu and Liu, Jingjing and Xu, Ning and Huang, Junzhou},
  booktitle = wacv,
  year      = {2023},
  url       = {https://openaccess.thecvf.com/content/WACV2023/papers/Yang_TVT_Transferable_Vision_Transformer_for_Unsupervised_Domain_Adaptation_WACV_2023_paper.pdf}
}

@article{ge2023pr,
  title   = {Unsupervised Domain Adaptation via Deep Conditional Adaptation Network},
  author  = {Ge, Pengfei and Ren, Chuan-Xian and Xu, Xiao-Lin and Yan, Hong},
  journal = pr,
  volume  = {134},
  pages   = {109088},
  year    = {2023}
}

@inproceedings{kraehenbuehl2011nips,
  title     = {{Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials}},
  author    = {P. Kr\"ahenb\"uhl and Vladen Koltun},
  booktitle = nips,
  year      = 2011,
  url       = {https://arxiv.org/pdf/1210.5644.pdf}
}


@inproceedings{ledig2017cvpr,
  author    = {Christian Ledig and  Lucas Theis and Ferenc Huszar and Jose Caballero and Andrew Cunningham and Alejandro Acosta and Andrew Aitken and Alykhan Tejani and Johannes Totz and Zehan Wang and Wenzhe Shi},
  title     = {{Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network}},
  booktitle = cvpr,
  year      = 2017,
  abstract  = {Despite the breakthroughs in accuracy and speed of single image super-resolution using faster and deeper convolutional neural networks, one central problem remains largely unsolved: how do we recover the finer texture details when we super-resolve at large upscaling factors? The behavior of optimization-based super-resolution methods is principally driven by the choice of the objective function. Recent work has largely focused on minimizing the mean squared reconstruction error. The resulting estimates have high peak signal-to-noise ratios, but they are often lacking high-frequency details and are perceptually unsatisfying in the sense that they fail to match the fidelity expected at the higher resolution. In this paper, we present SRGAN, a generative adversarial network (GAN) for image super-resolution (SR). To our knowledge, it is the first framework capable of inferring photo-realistic natural images for 4x upscaling factors. To achieve this, we propose a perceptual loss function which consists of an adversarial loss and a content loss. The adversarial loss pushes our solution to the natural image manifold using a discriminator network that is trained to differentiate between the super-resolved images and original photo-realistic images. In addition, we use a content loss motivated by perceptual similarity instead of similarity in pixel space. Our deep residual network is able to recover photo-realistic textures from heavily downsampled images on public benchmarks. An extensive mean-opinion-score (MOS) test shows hugely significant gains in perceptual quality using SRGAN. The MOS scores obtained with SRGAN are closer to those of the original high-resolution images than to those obtained with any state-of-the-art method.},
  url       = {proceedings: ledig2017cvpr.pdf}
}

@inproceedings{xu2018eccv-sgis,
  author    = {Wenqiang Xu and Yonglu Li and Cewu Lu},
  title     = {{SRDA: Generating Instance Segmentation Annotation via Scanning, Reasoning and Domain Adaptation}},
  booktitle = eccv,
  year      = 2018,
  abstract  = {Instance segmentation is a problem of significance in computer vision. However, preparing annotated data for this task is extremely time-consuming and costly. By combining the advantages of 3D scanning, reasoning, and GAN-based domain adaptation techniques, we introduce a novel pipeline named SRDA to obtain large quantities of training samples with very minor effort. Our pipeline is well-suited to scenes that can be scanned, i.e. most indoor and some outdoor scenarios. To evaluate our performance, we build three representative scenes and a new dataset, with 3D models of various common objects categories and annotated real-world scene images. Extensive experiments show that our pipeline can achieve decent instance segmentation performance given very low human labor cost.},
  url       = {proceedings: xu2018eccv-sgis.pdf}
}

@inproceedings{wang2021iccv-ursa,
  author    = {Wei Wang and Haochen Zhang and Zehuan Yuan and Changhu Wang},
  title     = {{Unsupervised Real-World Super-Resolution: A Domain Adaptation Perspective}},
  booktitle = iccv,
  year      = 2021,
  abstract  = {Most existing convolution neural network (CNN) based super-resolution (SR) methods generate their paired training dataset by artificially synthesizing low-resolution (LR) images from the high-resolution (HR) ones. However, this dataset preparation strategy harms the application of these CNNs in real-world scenarios due to the inherent domain gap between the training and testing data. A popular attempts towards the challenge is unpaired generative adversarial networks, which generate "real" LR counterparts from real HR images using image-to-image translation and then perform super-resolution from "real" LR->SR. Despite great progress, it is still difficult to synthesize perfect "real" LR images for super-resolution. In this paper, we firstly consider the real-world SR problem from the traditional domain adaptation perspective. We propose a novel unpaired SR training framework based on feature distribution alignment, with which we can obtain degradation-indistinguishable feature maps and then map them to HR images. In order to generate better SR images for target LR domain, we introduce several regularization losses to force the aligned feature to locate around the target domain. Our experiments indicate that our SR network obtains the state-of-the-art performance over both blind and unpaired SR methods on diverse datasets.},
  url       = {proceedings: wang2021iccv-ursa.pdf}
}

@inproceedings{yuan2018cvprws,
  author    = {Yuan Yuan and Siyuan Liu and Jiawei Zhang and Yongbing Zhang and Chao Dong and Liang Lin},
  title     = {{Unsupervised Image Super-Resolution using Cycle-in-Cycle Generative Adversarial Networks}},
  booktitle = cvprws,
  year      = 2018,
  url       = {https://arxiv.org/pdf/1809.00437.pdf}
}

@inproceedings{guo2022cvpr-lala,
  author    = {Baisong Guo and Xiaoyun Zhang and Haoning Wu and Yu Wang and Ya Zhang and Yan-Feng Wang},
  title     = {{LAR-SR: A Local Autoregressive Model for Image Super-Resolution}},
  booktitle = cvpr,
  year      = 2022,
  abstract  = {Previous super-resolution (SR) approaches often formulate SR as a regression problem and pixel wise restoration, which leads to a blurry and unreal SR output. Recent works combine adversarial loss with pixel-wise loss to train a GAN-based model or introduce normalizing flows into SR problems to generate more realistic images. As another powerful generative approach, autoregressive (AR) model has not been noticed in low level tasks due to its limitation. Based on the fact that given the structural information, the textural details in the natural images are locally related without long term dependency, in this paper we propose a novel autoregressive model-based SR approach, namely LAR-SR, which can efficiently generate realistic SR images using a novel local autoregressive (LAR) module. The proposed LAR module can sample all the patches of textural components in parallel, which greatly reduces the time consumption. In addition to high time efficiency, it is also able to leverage contextual information of pixels and can be optimized with a consistent loss. Experimental results on the widely-used datasets show that the proposed LAR-SR approach achieves superior performance on the visual quality and quantitative metrics compared with other generative models such as GAN, Flow, and is competitive with the mixture generative model.},
  url       = {proceedings: guo2022cvpr-lala.pdf}
}

@inproceedings{zhang2021iccv-dapd,
  author    = {Kai Zhang and Jingyun Liang and Luc Van Gool and Radu Timofte},
  title     = {{Designing a Practical Degradation Model for Deep Blind Image Super-Resolution}},
  booktitle = iccv,
  year      = 2021,
  abstract  = {It is widely acknowledged that single image super-resolution (SISR) methods would not perform well if the assumed degradation model deviates from those in real images. Although several degradation models take additional factors into consideration, such as blur, they are still not effective enough to cover the diverse degradations of real images. To address this issue, this paper proposes to design a more complex but practical degradation model that consists of randomly shuffled blur, downsampling and noise degradations. Specifically, the blur is approximated by two convolutions with isotropic and anisotropic Gaussian kernels; the downsampling is randomly chosen from nearest, bilinear and bicubic interpolations; the noise is synthesized by adding Gaussian noise with different noise levels, adopting JPEG compression with different quality factors, and generating processed camera sensor noise via reverse-forward camera image signal processing (ISP) pipeline model and RAW image noise model. To verify the effectiveness of the new degradation model, we have trained a deep blind ESRGAN super-resolver and then applied it to super-resolve both synthetic and real images with diverse degradations. The experimental results demonstrate that the new degradation model can help to significantly improve the practicability of deep super-resolvers, thus providing a powerful alternative solution for real SISR applications.},
  url       = {proceedings: zhang2021iccv-dapd.pdf}
}

@inproceedings{zhang2019iccv-rgan,
  author    = {Wenlong Zhang and Yihao Liu and Chao Dong and Yu Qiao},
  title     = {{RankSRGAN: Generative Adversarial Networks With Ranker for Image Super-Resolution}},
  booktitle = iccv,
  year      = 2019,
  abstract  = {Generative Adversarial Networks (GAN) have demonstrated the potential to recover realistic details for single image super-resolution (SISR). To further improve the visual quality of super-resolved results, PIRM2018-SR Challenge employed perceptual metrics to assess the perceptual quality, such as PI, NIQE, and Ma. However, existing methods cannot directly optimize these indifferentiable perceptual metrics, which are shown to be highly correlated with human ratings. To address the problem, we propose Super-Resolution Generative Adversarial Networks with Ranker (RankSRGAN) to optimize generator in the direction of perceptual metrics. Specifically, we first train a Ranker which can learn the behavior of perceptual metrics and then introduce a novel rank-content loss to optimize the perceptual quality. The most appealing part is that the proposed method can combine the strengths of different SR methods to generate better results. Extensive experiments show that RankSRGAN achieves visually pleasing results and reaches state-of-the-art performance in perceptual metrics. Project page: https://wenlongzhang0724.github.io/Projects/RankSRGAN},
  url       = {proceedings: zhang2019iccv-rgan.pdf}
}

@inproceedings{wang2018eccvws,
  author    = {Wang, Xintao and Yu, Ke and Wu, Shixiang and Gu, Jinjin and Liu, Yihao and Dong, Chao and Qiao, Yu and Change Loy, Chen},
  title     = {{ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks}},
  booktitle = eccvws,
  year      = 2018,
  url       = {https://openaccess.thecvf.com/content_ECCVW_2018/papers/11133/Wang_ESRGAN_Enhanced_Super-Resolution_Generative_Adversarial_Networks_ECCVW_2018_paper.pdf}
}

@inproceedings{wei2021cvpr-uris,
  author    = {Yunxuan Wei and Shuhang Gu and Yawei Li and Radu Timofte and Longcun Jin and Hengjie Song},
  title     = {{Unsupervised Real-World Image Super Resolution via Domain-Distance Aware Training}},
  booktitle = cvpr,
  year      = 2021,
  abstract  = {These days, unsupervised super-resolution (SR) is soaring due to its practical and promising potential in real scenarios. The philosophy of off-the-shelf approaches lies in the augmentation of unpaired data, i.e. first generating synthetic low-resolution (LR) images Y^g corresponding to real-world high-resolution (HR) images X^r in the real-world LR domain Y^r, and then utilizing the pseudo pairs Y^g, X^r for training in a supervised manner. Unfortunately, since image translation itself is an extremely challenging task, the SR performance of these approaches is severely limited by the domain gap between generated synthetic LR images and real LR images. In this paper, we propose a novel domain-distance aware super-resolution (DASR) approach for unsupervised real-world image SR. The domain gap between training data (e.g. Y^g) and testing data (e.g. Y^r) is addressed with our domain-gap aware training and domain-distance weighted supervision strategies. Domain-gap aware training takes additional benefit from real data in the target domain while domain-distance weighted supervision brings forward the more rational use of labeled source domain data. The proposed method is validated on synthetic and real datasets and the experimental results show that DASR consistently outperforms state-of-the-art unsupervised SR approaches in generating SR outputs with more realistic and natural textures. Codes are available at https://github.com/ShuhangGu/DASR.},
  url       = {proceedings: wei2021cvpr-uris.pdf}
}

@inproceedings{richardson2021cvpr,
  author    = {Elad Richardson and Yuval Alaluf and Or Patashnik and Yotam Nitzan and Yaniv Azar and Stav Shapiro and Daniel Cohen-Or},
  title     = {{Encoding in Style: A StyleGAN Encoder for Image-to-Image Translation}},
  booktitle = cvpr,
  year      = 2021,
  abstract  = {We present a generic image-to-image translation framework, pixel2style2pixel (pSp). Our pSp framework is based on a novel encoder network that directly generates a series of style vectors which are fed into a pretrained StyleGAN generator, forming the extended W+ latent space. We first show that our encoder can directly embed real images into W+, with no additional optimization. Next, we propose utilizing our encoder to directly solve image-to-image translation tasks, defining them as encoding problems from some input domain into the latent domain. By deviating from the standard invert first, edit later methodology used with previous StyleGAN encoders, our approach can handle a variety of tasks even when the input image is not represented in the StyleGAN domain. We show that solving translation tasks through StyleGAN significantly simplifies the training process, as no adversary is required, has better support for solving tasks without pixel-to-pixel correspondence, and inherently supports multi-modal synthesis via the resampling of styles. Finally, we demonstrate the potential of our framework on a variety of facial image-to-image translation tasks, even when compared to state-of-the-art solutions designed specifically for a single task, and further show that it can be extended beyond the human facial domain. Code is available at https://github.com/eladrich/pixel2style2pixel.},
  url       = {proceedings: richardson2021cvpr.pdf}
}

@inproceedings{kynkaanniemi2019neurips,
  author    = {Kynk{\"a}{\"a}nniemi, Tuomas and Karras, Tero and Laine, Samuli and Lehtinen, Jaakko and Aila, Timo},
  title     = {{Improved precision and recall metric for assessing generative models}},
  booktitle = neurips,
  year      = 2019,
  url       = {https://proceedings.neurips.cc/paper/2019/file/0234c510bc6d908b28c70ff313743079-Paper.pdf}
}

@inproceedings{karras2020cvpr,
  author    = {Tero Karras and Samuli Laine and Miika Aittala and Janne Hellsten and Jaakko Lehtinen and Timo Aila},
  title     = {{Analyzing and Improving the Image Quality of StyleGAN}},
  booktitle = cvpr,
  year      = 2020,
  abstract  = {The style-based GAN architecture (StyleGAN) yields state-of-the-art results in data-driven unconditional generative image modeling. We expose and analyze several of its characteristic artifacts, and propose changes in both model architecture and training methods to address them. In particular, we redesign the generator normalization, revisit progressive growing, and regularize the generator to encourage good conditioning in the mapping from latent codes to images. In addition to improving image quality, this path length regularizer yields the additional benefit that the generator becomes significantly easier to invert. This makes it possible to reliably attribute a generated image to a particular network. We furthermore visualize how well the generator utilizes its output resolution, and identify a capacity problem, motivating us to train larger models for additional quality improvements. Overall, our improved model redefines the state of the art in unconditional image modeling, both in terms of existing distribution quality metrics as well as perceived image quality.},
  url       = {proceedings: karras2020cvpr.pdf}
}

@article{kang2022arxiv,
  author  = {Kang, Minguk and Shin, Joonghyuk and Park, Jaesik},
  title   = {{StudioGAN: A Taxonomy and Benchmark of GANs for Image Synthesis}},
  journal = arxiv,
  volume  = {arXiv:2206.09479},
  year    = 2022,
  url     = {https://arxiv.org/pdf/2206.09479.pdf}
}

@inproceedings{tewari2020cvpr,
  author    = {Ayush Tewari and Mohamed Elgharib and Gaurav Bharaj and Florian Bernard and Hans-Peter Seidel and Patrick Perez and Michael Zollhofer and Christian Theobalt},
  title     = {{StyleRig: Rigging StyleGAN for 3D Control Over Portrait Images}},
  booktitle = cvpr,
  year      = 2020,
  abstract  = {StyleGAN generates photorealistic portrait images of faces with eyes, teeth, hair and context (neck, shoulders, background), but lacks a rig-like control over semantic face parameters that are interpretable in 3D, such as face pose, expressions, and scene illumination. Three-dimensional morphable face models (3DMMs) on the other hand offer control over the semantic parameters, but lack photorealism when rendered and only model the face interior, not other parts of a portrait image (hair, mouth interior, background). We present the first method to provide a face rig-like control over a pretrained and fixed StyleGAN via a 3DMM. A new rigging network, RigNet is trained between the 3DMM's semantic parameters and StyleGAN's input. The network is trained in a self-supervised manner, without the need for manual annotations. At test time, our method generates portrait images with the photorealism of StyleGAN and provides explicit control over the 3D semantic parameters of the face.},
  url       = {proceedings: tewari2020cvpr.pdf}
}

@inproceedings{karras2019cvpr,
  author    = {Tero Karras and Samuli Laine and Timo Aila},
  title     = {{A Style-Based Generator Architecture for Generative Adversarial Networks}},
  booktitle = cvpr,
  year      = 2019,
  abstract  = {We propose an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature. The new architecture leads to an automatically learned, unsupervised separation of high-level attributes (e.g., pose and identity when trained on human faces) and stochastic variation in the generated images (e.g., freckles, hair), and it enables intuitive, scale-specific control of the synthesis. The new generator improves the state-of-the-art in terms of traditional distribution quality metrics, leads to demonstrably better interpolation properties, and also better disentangles the latent factors of variation. To quantify interpolation quality and disentanglement, we propose two new, automated methods that are applicable to any generator architecture. Finally, we introduce a new, highly varied and high-quality dataset of human faces.},
  url       = {proceedings: karras2019cvpr.pdf}
}

@inproceedings{abdal2020cvpr,
  author    = {Rameen Abdal and Yipeng Qin and Peter Wonka},
  title     = {{Image2StyleGAN++: How to Edit the Embedded Images?}},
  booktitle = cvpr,
  year      = 2020,
  abstract  = {We propose Image2StyleGAN++, a flexible image editing framework with many applications. Our framework extends the recent Image2StyleGAN in three ways. First, we introduce noise optimization as a complement to the W+ latent space embedding. Our noise optimization can restore high frequency features in images and thus significantly improves the quality of reconstructed images, e.g. a big increase of PSNR from 20 dB to 45 dB. Second, we extend the global W+ latent space embedding to enable local embeddings. Third, we combine embedding with activation tensor manipulation to perform high quality local edits along with global semantic edits on images. Such edits motivate various high quality image editing applications, e.g. image reconstruction, image inpainting, image crossover, local style transfer, image editing using scribbles, and attribute level feature transfer. Examples of the edited images are shown across the paper for visual inspection.},
  url       = {proceedings: abdal2020cvpr.pdf}
}

@inproceedings{chong2022eccv,
  author    = {Min Jin Chong and David Forsyth},
  title     = {{JoJoGAN: One Shot Face Stylization}},
  booktitle = eccv,
  year      = 2022,
  url       = {https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136760124.pdf}
}

@inproceedings{abdal2021iccv,
  author    = {Rameen Abdal and Peihao Zhu and Niloy J. Mitra and Peter Wonka},
  title     = {{Labels4Free: Unsupervised Segmentation Using StyleGAN}},
  booktitle = iccv,
  year      = 2021,
  abstract  = {We propose an unsupervised segmentation framework for StyleGAN generated objects. We build on two main observations. First, the features generated by StyleGAN hold valuable information that can be utilized towards training segmentation networks. Second, the foreground and background can often be treated to be largely independent and be swapped across images to produce plausible composited images. For our solution, we propose to augment the Style-GAN2 generator architecture with a segmentation branch and to split the generator into a foreground and background network. This enables us to generate soft segmentation masks for the foreground object in an unsupervised fashion. On multiple object classes, we report comparable results against state-of-the-art supervised segmentation networks, while against the best unsupervised segmentation approach we demonstrate a clear improvement, both in qualitative and quantitative metrics. Project Page : https:/rameenabdal.github.io/Labels4Free},
  url       = {proceedings: abdal2021iccv.pdf}
}

@inproceedings{oeldorf2019icmla,
  author    = {Oeldorf, Cedric and Spanakis, Gerasimos},
  title     = {{LoGANv2: Conditional Style-Based Logo Generation with Generative Adversarial Networks}},
  booktitle = {Proc.~of the IEEE Intl.~Conf.~on Machine Learning And Applications (ICMLA)},
  year      = 2019,
  url       = {https://arxiv.org/pdf/1909.09974.pdf}
}

@inproceedings{xu2022cvpr-daaf,
  author    = {Xiaoqian Xu and Pengxu Wei and Weikai Chen and Yang Liu and Mingzhi Mao and Liang Lin and Guanbin Li},
  title     = {{Dual Adversarial Adaptation for Cross-Device Real-World Image Super-Resolution}},
  booktitle = cvpr,
  year      = 2022,
  abstract  = {Due to the sophisticated imaging process, an identical scene captured by different cameras could exhibit distinct imaging patterns, introducing distinct proficiency among the super-resolution (SR) models trained on images from different devices. In this paper, we investigate a novel and practical task coded cross-device SR, which strives to adapt a real-world SR model trained on the paired images captured by one camera to low-resolution (LR) images captured by arbitrary target devices. The proposed task is highly challenging due to the absence of paired data from various imaging devices. To address this issue, we propose an unsupervised domain adaptation mechanism for real-world SR, named Dual ADversarial Adaptation (DADA), which only requires LR images in the target domain with available real paired data from a source camera. DADA employs the Domain-Invariant Attention (DIA) module to establish the basis of target model training even without HR supervision. Furthermore, the dual framework of DADA facilitates an Inter-domain Adversarial Adaptation (InterAA) in one branch for two LR input images from two domains, and an Intra-domain Adversarial Adaptation (IntraAA) in two branches for an LR input image. InterAA and IntraAA together improve the model transferability from the source domain to the target. We empirically conduct experiments under six Real to Real adaptation settings among three different cameras, and achieve superior performance compared with existing state-of-the-art approaches. We also evaluate the proposed DADA to address the adaptation to the video camera, which presents a promising research topic to promote the wide applications of real-world super-resolution. Our source code is publicly available at https://github.com/lonelyhope/DADA.git.},
  url       = {proceedings: xu2022cvpr-daaf.pdf}
}

@article{chen2022tog,
  author  = {Anpei Chen and Ruiyang Liu and Ling Xie and Zhang Chen and Hao Su and Jingyi Yu},
  title   = {{SofGAN: A Portrait Image Generator with Dynamic Styling}},
  journal = tog,
  volume  = {41},
  number  = {1},
  pages   = {1--26},
  year    = 2022,
  url     = {https://dl.acm.org/doi/pdf/10.1145/3470848}
}

@inproceedings{eskandar2022iros,
  author    = {George Eskandar and Robert A. Marsden and Pavithran Pandiyan and Mario Doebler and Karim Guirguis and Bin Yang},
  title     = {{An Unsupervised Domain Adaptive Approach for Multimodal 2D Object Detection in Adverse Weather Conditions}},
  booktitle = iros,
  year      = 2022,
  abstract  = {Integrating different representations from complementary sensing modalities is crucial for robust scene interpretation in autonomous driving. While deep learning architectures that fuse vision and range data for 2D object detection have thrived in recent years, the corresponding modalities can degrade in adverse weather or lighting conditions, ultimately leading to a drop in performance. Although domain adaptation methods attempt to bridge the domain gap between source and target domains, they do not readily extend to heterogeneous data distributions. In this work, we propose an unsupervised domain adaptation framework, which adapts a 2D object detector for RGB and LiDAR sensors to one or more target domains featuring adverse weather conditions. Our proposed approach consists of three components. First, a data augmentation scheme that simulates weather distortions is devised to add domain confusion and prevent overfitting on the source data. Second, to promote cross-domain foreground object alignment, we leverage the complementary features of multiple modalities through a multi-scale entropy-weighted domain discriminator. Finally, we use carefully designed pretext tasks to learn a more robust representation of the target domain data. Experiments performed on the DENSE dataset show that our method can substantially alleviate the domain gap under the single-target domain adaptation setting and the less explored yet more general multi-target domain adaptation setting.},
  url       = {proceedings: eskandar2022iros.pdf}
}

@inproceedings{wang2018multimedia,
  author    = {Wang, Jindong and Feng, Wenjie and Chen, Yiqiang and Yu, Han and Huang, Meiyu and Yu, Philip S},
  title     = {{Visual Domain Adaptation with Manifold Embedded Distribution Alignment}},
  booktitle = {Proc.~of the ACM Intl.~Conf.~on Multimedia},
  year      = 2018,
  url       = {https://arxiv.org/pdf/1807.07258.pdf}
}

@article{kafri2022acmgraphics,
  author   = {Kafri, Omer and Patashnik, Or and Alaluf, Yuval and Cohen-Or, Daniel},
  title    = {{StyleFusion: Disentangling Spatial Segments in StyleGAN-Generated Images}},
  journal  = acmgraphics,
  volume   = {41},
  number   = {5},
  pages    = {1--15},
  year     = 2022,
  url      = {https://dl.acm.org/doi/pdf/10.1145/3527168},
  abstract = {We present StyleFusion, a new mapping architecture for StyleGAN, which takes as input a number of latent codes and fuses them into a single style code. Inserting the resulting style code into a pre-trained StyleGAN generator results in a single harmonized image in which each semantic region is controlled by one of the input latent codes. Effectively, StyleFusion yields a disentangled representation of the image, providing fine-grained control over each region of the generated image. Moreover, to help facilitate global control over the generated image, a special input latent code is incorporated into the fused representation. StyleFusion operates in a hierarchical manner, where each level is tasked with learning to disentangle a pair of image regions (e.g., the car body and wheels). The resulting learned disentanglement allows one to modify both local, fine-grained semantics (e.g., facial features) as well as more global features (e.g., pose and background), providing improved flexibility in the synthesis process. As a natural extension, StyleFusion allows one to perform semantically-aware cross-image mixing of regions that are not necessarily aligned. Finally, we demonstrate how StyleFusion can be paired with existing editing techniques to more faithfully constrain the edit to the userâs region of interest. Code is available at: .}
}

@inproceedings{heusel2017nips,
  author    = {Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
  title     = {{GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium}},
  booktitle = nips,
  year      = 2017,
  url       = {https://proceedings.neurips.cc/paper/2017/file/8a1d694707eb0fefe65871369074926d-Paper.pdf}
}

@book{lanczos1988applied,
  title     = {{Applied Analysis}},
  author    = {Lanczos, Cornelius},
  year      = {1988},
  publisher = {Courier Corporation}
}

@inproceedings{dischinger2021iros,
  author    = {Lisa M. Dischinger and Miranda Cravetz and Jacob Dawes and Callen Votzke and Chelse VanAtter and Matthew L. Johnston and Cindy Grimm and Joseph R. Davidson},
  title     = {{Towards Intelligent Fruit Picking with In-Hand Sensing}},
  booktitle = iros,
  year      = 2021,
  abstract  = { Studies have shown that picking techniques play an important role in determining fruit quality at harvest (e.g. bruising, stem retention, etc). When picking fruit such as apples and pears, professional pickers use active perception, incorporating both visual and tactile input about fruit orientation, stem location, and the fruits immediate surroundings. This combination of tactile, visual, and force feedback is what enables human workers to execute dynamic movements that quickly and efficiently remove fruit from the tree without damage. However, much of the prior work on robotic fruit picking has formulated the harvesting problem as a positioncontrol problem, using visual feedback for closed-loop endeffector placement while disregarding feedback on physical contact. As a first step towards more intelligent fruit picking  combining proprioception, localized sensing, and observed forces  we have developed a custom end-effector with multiple in-hand sensors, including tactile sensors on the fingertips. This paper presents the mechatronic design of the device as well as results from multiple outdoor picking trials with a Honeycrisp apple tree. Preliminary results show that, with multi-modal sensing, fruit slip, fruit separation from the tree, and fruit release from the hand can be detected.},
  url       = {https://www.ipb.uni-bonn.de:5555/pdfs/dischinger2021iros.pdf}
}

@inproceedings{naeem2020icml,
  author    = {Naeem, Muhammad Ferjad and Oh, Seong Joon and Uh, Youngjung and Choi, Yunjey and Yoo, Jaejun},
  title     = {{Reliable fidelity and diversity metrics for generative models}},
  booktitle = icml,
  year      = 2020,
  url       = {https://arxiv.org/pdf/2002.09797.pdf}
}

@inproceedings{aljundi2015cvpr,
  author    = {Rahaf Aljundi and  R{\'e}mi Emonet and Damien Muselet and Marc Sebban},
  title     = {{Landmarks-Based Kernelized Subspace Alignment for Unsupervised Domain Adaptation}},
  booktitle = cvpr,
  year      = 2015,
  abstract  = {Domain adaptation (DA) has gained a lot of success in the recent years in computer vision to deal with situations where the learning process has to transfer knowledge from a source to a target domain. In this paper, we introduce a novel unsupervised DA approach based on both subspace alignment and selection of landmarks similarly distributed between the two domains. Those landmarks are selected so as to reduce the discrepancy between the domains and then are used to non linearly project the data in the same space where an efficient subspace alignment (in closed-form) is performed. We carry out a large experimental comparison in visual domain adaptation showing that our new method outperforms the most recent unsupervised DA approaches.},
  url       = {proceedings: aljundi2015cvpr.pdf}
}

@inproceedings{lo2022iros,
  author    = {Shao-Yuan Lo and Wei Wang and Jim Thomas and Jingjing Zheng and Vishal M. Patel and Cheng-Hao Kuo},
  title     = {{Learning Feature Decomposition for Domain Adaptive Monocular Depth Estimation}},
  booktitle = iros,
  year      = 2022,
  abstract  = {Monocular depth estimation (MDE) has attracted intense study due to its low cost and critical functions for robotic tasks such as localization, mapping and obstacle detection. Supervised approaches have led to great success with the advance of deep learning, but they rely on large quantities of ground-truth depth annotations that are expensive to acquire. Unsupervised domain adaptation (UDA) transfers knowledge from labeled source data to unlabeled target data, so as to relax the constraint of supervised learning. However, existing UDA approaches may not completely align the domain gap across different datasets because of the domain shift problem. We believe better domain alignment can be achieved via welldesigned feature decomposition. In this paper, we propose a novel UDA method for MDE, referred to as Learning Feature Decomposition for Adaptation (LFDA), which learns to decompose the feature space into content and style components. LFDA only attempts to align the content component since it has a smaller domain gap. Meanwhile, it excludes the style component which is specific to the source domain from training the primary task. Furthermore, LFDA uses separate feature distribution estimations to further bridge the domain gap. Extensive experiments on three domain adaptative MDE scenarios show that the proposed method achieves superior accuracy and lower computational cost compared to the stateof-the-art approaches.},
  url       = {proceedings: lo2022iros.pdf}
}

@article{stavros2019cras,
  author  = {Vougioukas, Stavros G.},
  title   = {{Agricultural Robotics}},
  journal = annurevcontrol,
  volume  = {2},
  number  = {1},
  pages   = {365--392},
  year    = 2019,
  url     = {https://www.annualreviews.org/doi/pdf/10.1146/annurev-control-053018-023617}
}

@article{russakovsky2015ijcv,
  author  = {Olga Russakovsky and Jia Deng and Hao Su and Jonathan Krause and Sanjeev Satheesh and Sean Ma and Zhiheng Huang and Andrej Karpathy and Aditya Khosla and Michael Bernstein and Alexander C. Berg and Li Fei-Fei},
  title   = {{ImageNet Large Scale Visual Recognition Challenge}},
  journal = ijcv,
  volume  = {115},
  number  = {3},
  pages   = {211--252},
  year    = 2015,
  url     = {https://arxiv.org/pdf/1409.0575.pdf}
}

@phdthesis{lottes2021phd,
  author = {Philipp Lottes},
  title  = {{Plant Classification Systems for Agricultural Robots}},
  school = {University of Bonn},
  year   = 2021
}

@inproceedings{hendrikx2021icra,
  title     = {{Connecting Semantic Building Information Models and Robotics: An application to 2D LiDAR-based localization}},
  author    = {Hendrikx, RWM and Pauwels, Pieter and Torta, Elena and Bruyninckx, HPJ and van de Molengraft, MJG},
  booktitle = icra,
  year      = {2021},
  url       = {proceedings: hendrikx2021icra.pdf}
}

@article{bochkovskiy2020arxiv,
  author  = {Bochkovskiy, Alexey and Wang, Chien-Yao and Liao, Hong-Yuan Mark},
  title   = {{YOLOv4: Optimal Speed and Accuracy of Object Detection}},
  journal = arxiv,
  volume  = {arXiv:2004.10934},
  year    = {2020},
  url     = {https://arxiv.org/pdf/2004.10934.pdf}
}

@article{liao2019arxiv,
  title   = {{Real-time Scene Text Detection with Differentiable Binarization}},
  author  = {Minghui Liao and Zhaoyi Wan and Cong Yao and Kai Chen and Xiang Bai},
  year    = {2019},
  journal = arxiv,
  volume  = {arXiv:1911.08947},
  url     = {https://arxiv.org/pdf/1911.08947.pdf}
}

@article{shi2015arxiv,
  title   = {{An End-to-End Trainable Neural Network for Image-based Sequence Recognition and Its Application to Scene Text Recognition}},
  author  = {Baoguang Shi and Xiang Bai and Cong Yao},
  year    = {2015},
  journal = arxiv,
  volume  = {arXiv:1507.05717},
  url     = {https://arxiv.org/pdf/1507.05717.pdf}
}

@inproceedings{moravec1985icra,
  author    = {Moravec, H. and Elfes, A.},
  booktitle = icra,
  title     = {{High resolution maps from wide angle sonar}},
  year      = {1985},
  url       = {proceedings: moravec1985icra.pdf}
}

@inproceedings{moravec1989sdsr,
  author    = {Moravec, H. P.},
  title     = {{Sensor Fusion in Certainty Grids for Mobile Robots}},
  booktitle = {Sensor Devices and Systems for Robotics (SDSR)},
  year      = {1989}
}

@inbook{mucherino2009springer,
  author    = {Mucherino, Antonio and Papajorgji, Petraq J. and Pardalos, Panos M.},
  title     = {{k-Nearest Neighbor Classification}},
  booktitle = {Data Mining in Agriculture},
  year      = {2009}
}


@article{felzenszwalb2012toc,
  author  = {Felzenszwalb, Pedro F. and Huttenlocher, Daniel P.},
  journal = {Theory of Computing},
  number  = {1},
  pages   = {415--428},
  title   = {{Distance Transforms of Sampled Functions}},
  volume  = {8},
  year    = 2012,
  url     = {https://cs.brown.edu/people/pfelzens/papers/dt-final.pdf}
}

@inproceedings{zhou2014nips,
  author    = {Zhou, Bolei and Lapedriza, Agata and Xiao, Jianxiong and Torralba, Antonio and Oliva, Aude},
  booktitle = nips,
  title     = {{Learning Deep Features for Scene Recognition using Places Database}},
  year      = {2014},
  url       = {https://papers.nips.cc/paper_files/paper/2014/file/3fe94a002317b5f9259f82690aeea4cd-Paper.pdf}
}

@article{leonard1991tro,
  author  = {Leonard, J.J. and Durrant-Whyte, H.F.},
  journal = tro,
  title   = {{Mobile robot localization by tracking geometric beacons}},
  year    = {1991},
  volume  = {7},
  number  = {3},
  pages   = {376--382},
  url     = {https://www.cs.cmu.edu/~motionplanning/papers/sbp_papers/l/leonard_ieeetroa19.pdf}
}


@inproceedings{pfaff2006ers,
  author    = {Pfaff, Patrick and Burgard, Wolfram and Fox, Dieter},
  year      = {2006},
  title     = {{Robust Monte-Carlo Localization Using Adaptive Likelihood Models}},
  booktitle = {Proc. of the European Robotics Symposium},
  url       = {http://ais.informatik.uni-freiburg.de/publications/papers/pfaff06euros.pdf}
}

@inproceedings{wada2020cvpr,
  title     = {{{MoreFusion}: Multi-object Reasoning for {6D} Pose Estimation from Volumetric Fusion}},
  author    = {Kentaro Wada and Edgar Sucar and Stephen James and Daniel Lenton and Andrew J. Davison},
  booktitle = cvpr,
  year      = {2020},
  url       = {proceedings: wada2020cvpr.pdf}
}

@article{binbin2018arxiv,
  author  = {Binbin Xu and
             Wenbin Li and
             Dimos Tzoumanikas and
             Michael Bloesch and
             Andrew J. Davison and
             Stefan Leutenegger},
  title   = {{MID-Fusion: Octree-based Object-Level Multi-Instance Dynamic {SLAM}}},
  journal = arxiv,
  year    = {2018},
  volume  = {arXiv:1812.07976},
  url     = {https://arxiv.org/pdf/1812.07976.pdf}
}

@article{zafari2019cst,
  author  = {Zafari, Faheem and Gkelias, Athanasios and Leung, Kin K.},
  journal = {IEEE Communications Surveys Tutorials (CST)},
  title   = {{A Survey of Indoor Localization Systems and Technologies}},
  year    = {2019},
  volume  = {21},
  number  = {3},
  pages   = {2568--2599},
  url     = {https://arxiv.org/pdf/1709.01015.pdf}
}


  @article{zhang2022arxiv,
  author  = {Zhang, Hao and Li, Feng and Liu, Shilong and Zhang, Lei and Su, Hang and Zhu, Jun and Ni, Lionel M. and Shum, Heung-Yeung},
  title   = {{DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection}},
  journal = arxiv,
  volume  = {arXiv:2203.03605},
  year    = {2022},
  url     = {https://arxiv.org/pdf/2203.03605.pdf}
}

@inproceedings{krajnik2016iros,
  title     = {{Persistent localization and life-long mapping in changing environments using the frequency map enhancement}},
  author    = {Krajn{\'\i}k, Tom{\'a}{\v{s}} and Fentanes, Jaime Pulido and Hanheide, Marc and Duckett, Tom},
  booktitle = iros,
  year      = {2016},
  url       = {proceedings: krajnik2016iros.pdf}
}

@inproceedings{sun2016iros,
  author    = {Sun, Dali and GeiÃer, Florian and Nebel, Bernhard},
  booktitle = iros,
  title     = {{Towards effective localization in dynamic environments}},
  year      = {2016},
  url       = {proceedings: sun2016iros.pdf}
}

  @article{tipaldi2013ijrr,
  title   = {{Lifelong localization in changing environments}},
  author  = {Tipaldi, Gian Diego and Meyer-Delius, Daniel and Burgard, Wolfram},
  journal = ijrr,
  volume  = {32},
  number  = {14},
  pages   = {1662--1678},
  year    = {2013},
  url     = {http://ais.informatik.uni-freiburg.de/publications/papers/tipaldi13ijrr.pdf}
}

@inproceedings{valencia2014iros,
  title     = {{Localization in highly dynamic environments using dual-timescale NDT-MCL}},
  author    = {Valencia, Rafael and Saarinen, Jari and Andreasson, Henrik and Vallv{\'e}, Joan and Andrade-Cetto, Juan and Lilienthal, Achim J},
  booktitle = iros,
  year      = {2014},
  url       = {proceedings: valencia2014iros.pdf}
}

@inproceedings{li2020iros-olwi,
  title     = {{Online Localization with Imprecise Floor Space Maps using Stochastic Gradient Descent}},
  author    = {Li, Zhikai and Ang, Marcelo H and Rus, Daniela},
  booktitle = iros,
  year      = {2020},
  url       = {proceedings: li2020iros.pdf}
} 

@misc{jocher2020zenodo,
  author       = {Glenn Jocher},
  title        = {{ultralytics/yolov5: v3.1}},
  year         = {2020},
  publisher    = {Zenodo},
  version      = {v3.1},
  doi          = {10.5281/zenodo.4154370},
  url          = {https://doi.org/10.5281/zenodo.4154370},
  howpublished = {\url{https://github.com/ultralytics/yolov5}}
}

@inproceedings{choi2021cvpr,
  title     = {{Robustnet: Improving domain generalization in urban-scene segmentation via instance selective whitening}},
  author    = {Choi, Sungha and Jung, Sanghun and Yun, Huiwon and Kim, Joanne T and Kim, Seungryong and Choo, Jaegul},
  booktitle = cvpr,
  url       = {https://arxiv.org/pdf/2103.15597.pdf},
  year      = {2021}
}

@inproceedings{hendrycks2019iclr,
  title     = {{Augmix: A simple data processing method to improve robustness and uncertainty}},
  author    = {Hendrycks, Dan and Mu, Norman and Cubuk, Ekin D and Zoph, Barret and Gilmer, Justin and Lakshminarayanan, Balaji},
  booktitle = iclr,
  url       = {https://arxiv.org/pdf/1912.02781.pdf},
  year      = {2019}
}

@inproceedings{lee2022cvpr,
  title     = {{WildNet: Learning Domain Generalized Semantic Segmentation from the Wild}},
  author    = {Lee, Suhyeon and Seong, Hongje and Lee, Seongwon and Kim, Euntai},
  booktitle = cvpr,
  year      = {2022},
  url       = {https://openaccess.thecvf.com/content/CVPR2022/papers/Lee_WildNet_Learning_Domain_Generalized_Semantic_Segmentation_From_the_Wild_CVPR_2022_paper.pdf}
}

@inproceedings{chiu2019cvpr,
  title     = {{Understanding generalized whitening and coloring transform for universal style transfer}},
  author    = {Chiu, Tai-Yin},
  booktitle = cvpr,
  year      = {2019},
  url       = {https://openaccess.thecvf.com/content_ICCV_2019/papers/Chiu_Understanding_Generalized_Whitening_and_Coloring_Transform_for_Universal_Style_Transfer_ICCV_2019_paper.pdf}
}

// vizzo2023itsc
@inproceedings{eidson2002ptp,
  title     = {IEEE-1588\texttrademark{} Standard for a precision clock synchronization protocol for networked measurement and control systems},
  author    = {Eidson, John C and Fischer, Mike and White, Joe},
  booktitle = {Proc.~of the Annual Precise Time and Time Interval Systems and Applications Meeting},
  year      = {2002}
}

@article{gahlert2020arxiv,
  author  = {Nils G{\"{a}}hlert and Nicolas Jourdan and Marius Cordts and Uwe Franke and Joachim Denzler},
  title   = {{Cityscapes 3D: Dataset and Benchmark for 9 DoF Vehicle Detection}},
  volume  = {arXiv:2006.07864},
  journal = arxiv,
  year    = {2020},
  url     = {https://arxiv.org/pdf/2006.07864.pdf}
}

@inproceedings{yu2020cvpr-badd,
  author    = {F. Yu and H. Chen and X. Wang and W. Xian and Y. Chen and F. Liu and V. Madhavan and T. Darrell},
  title     = {{BDD100K: A Diverse Driving Dataset for Heterogeneous Multitask Learning}},
  booktitle = cvpr,
  year      = 2020,
  url       = {proceedings: yu2020cvpr-badd.pdf}
}

@inproceedings{huang2018cvprws,
  author    = {Huang, Xinyu and Cheng, Xinjing and Geng, Qichuan and Cao, Binbin and Zhou, Dingfu and Wang, Peng and Lin, Yuanqing and Yang, Ruigang},
  booktitle = cvprws,
  title     = {{The ApolloScape Dataset for Autonomous Driving}},
  year      = {2018},
  url       = {https://arxiv.org/pdf/1803.06184.pdf}
}

@inproceedings{wilson2021neurips,
  author    = {Benjamin Wilson and William Qi and Tanmay Agarwal and John Lambert and Jagjeet Singh and Siddhesh Khandelwal and Bowen Pan and Ratnesh Kumar and Andrew Hartnett and Jhony Kaesemodel Pontes and Deva Ramanan and Peter Carr and James Hays},
  title     = {{Argoverse 2: Next Generation Datasets for Self-driving Perception and Forecasting}},
  booktitle = neurips,
  year      = {2021},
  url       = {https://arxiv.org/pdf/2301.00493.pdf}
}

@article{cervera2019ral,
  author  = {Cervera, Enric},
  journal = ral,
  title   = {{Try to Start It! The Challenge of Reusing Code in Robotics Research}},
  year    = {2019},
  volume  = {4},
  number  = {1},
  pages   = {49--56},
  url     = {https://repositori.uji.es/xmlui/bitstream/handle/10234/182251/IEEE_ROBOTICS_AND_AUTOMATION_LETTERS.pdf?sequence=1&isAllowed=y}
}

@article{cervera2019ram,
  title   = {Roslab: sharing ros code interactively with docker and jupyterlab},
  author  = {Cervera, Enric and Del Pobil, Angel P},
  journal = ram,
  volume  = {26},
  number  = {3},
  pages   = {64--69},
  year    = {2019},
  url     = {https://repositori.uji.es/xmlui/bitstream/handle/10234/184828/IEEE%20ROBOTICS%20&%20AUTOMATION%20MAGAZINE.pdf?sequence=1}
}

@inproceedings{wendt2022sii,
  title     = {{Proxying ROS communications--enabling containerized ROS deployments in distributed multi-host environments}},
  author    = {Wendt, Arne and Sch{\"u}ppstuhl, Thorsten},
  booktitle = sii,
  year      = {2022}
}

@incollection{white2017ros,
  title     = {ROS and Docker},
  author    = {White, Ruffin and Christensen, Henrik},
  booktitle = {Robot Operating System (ROS) The Complete Reference (Volume 2)},
  pages     = {285--307},
  year      = {2017}
}

@article{triantafyllou2021ram,
  title   = {{A Methodology for Approaching the Integration of Complex Robotics Systems: Illustration Through a Bimanual Manipulation Case Study}},
  author  = {Triantafyllou, Pavlos and Afonso Rodrigues, Rafael and Chaikunsaeng, Sirapoab and Almeida, Diogo and Deacon, Graham and Konstantinova, Jelizaveta and Cotugno, Giuseppe},
  journal = ram,
  year    = {2021},
  volume  = {28},
  number  = {2},
  pages   = {88--100},
  url     = {https://arxiv.org/pdf/2103.10242.pdf}
}

@article{fischer2021ram,
  title   = {{A RoboStack Tutorial: Using the Robot Operating System Alongside the Conda and Jupyter Data Science Ecosystems}},
  author  = {Fischer, Tobias and Vollprecht, Wolf and Traversaro, Silvio and Yen, Sean and Herrero, Carlos and Milford, Michael},
  journal = ram,
  volume  = {29},
  number  = {2},
  pages   = {65--74},
  year    = {2021},
  url     = {https://arxiv.org/pdf/2104.12910.pdf}
}

@misc{newman2008moos,
  title  = {MOOS - Mission Orientated Operating Suite},
  author = {Paul Newman},
  year   = {2008}
}

@inproceedings{montemerlo2003iros,
  author    = {Montemerlo, M. and Roy, N. and Thrun, S.},
  booktitle = iros,
  title     = {{Perspectives on standardization in mobile robot programming: the Carnegie Mellon Navigation (CARMEN) Toolkit}},
  year      = {2003}
}

@article{macenski2022sr,
  author  = {Steven Macenski  and Tully Foote  and Brian Gerkey  and Chris Lalancette  and William Woodall},
  title   = {{Robot Operating System 2: Design, architecture, and uses in the wild}},
  journal = {Science Robotics},
  volume  = {7},
  number  = {66},
  pages   = {eabm6074},
  year    = {2022},
  url     = {https://arxiv.org/pdf/2211.07752.pdf}
}
