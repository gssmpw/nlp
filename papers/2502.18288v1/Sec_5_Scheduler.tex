%forked from the original scheduler

\section{Scheduler} \label{sec:scheduler}
In this section, we introduce the \emph{scheduler}, responsible for 
coordinating the executions for subcircuits based on a scalable pipeline model. 

\vspace{-3pt}
\subsection{Subcircuit Construction \& Execution}
Given a partition $\mathcal{P}=\{V_1, \ldots, V_k\}$, the scheduler should construct a subcircuit $F_i$ for each partition $V_i$, which is actually executable as shown in the \cref{fig:circuit_partition}. If the scale of the original circuit $\mathcal{F}$ is large, the resources, e.g., the memory, equipped may not afford its execution. With smaller subcircuits, the execution for which consumes much fewer resources, \system can first execute them separately and then combine the executions to produce the final proof. 
Essentially, it must ensure the combination of these subcircuits' executions is equivalent to the full circuit. 



\paragraph{Subcircuit construction} Given a \zk circuit $\mathcal{F}$, the subcircuit $F_i$ corresponding to $V_i$ is defined as:
$$
F_i\left(I_p^i, I_s^i, \bigcup_{t=1}^{i-1}{S_{ti}}\right) = \left(O_p^i, O_s^i, \bigcup_{t=i+1}^{k}{S_{it}}\right)
$$
\noindent The inputs of $F_i$ encompasses three parts. The $I_p^i\subseteq I_p$ and $I_s^i\subseteq I_s$ are the subsets of public and secret inputs involved in $F_i$, respectively. Each $S_{ti}$ indicates the intermediate results to be transmitted from a previous subcircuit $F_t$ to $F_i$. In fact, $S_{ti}$ corresponds to the cross-partition edges in CDG $G$ between $V_t$ and $V_i$. 
Then,  $\bigcup_{t=1}^{i-1}{S_{ti}}$ merges all the inputs received from previous subcircuits, termed as the \emph{shared inputs}. Similarly,  $O_p^i$ and $O_s^i$ denote the public and secret outputs of $F_i$, maintaining the output structure of the original circuit, while $\bigcup_{t=i+1}^{k}{S_{it}}$, the \emph{shared outputs}, are going to be passed to succeeding subcircuits. In \system, public outputs can be exposed to the verifier, while secret outputs are only for internal use to the prover. 




\paragraph{Subcircuit-wise execution} 
Given the set of $\{F_1, \ldots, F_k\}$ regarding $\{V_1, \ldots, V_k\}$, \system can execute them sequentially to generate the final proof. Some outputs propagate onward as shared variables during this process, connecting the subcircuits. The serializability of partition ensures that the current subcircuit $F_i$ can continue computation based on the shared variables from every proceeding subcircuit $F_t$ ($t<i$). Obviously, the combined logic of these subcircuits is equivalent to the full circuit.

However, this sequential execution model cannot utilize the advantage of modern multi-core chips. Actually, we can parallelize the execution of subcircuits if they have no dependency. For example, the subcircuit $F_1$ and $F_2$ in \cref{fig:circuit_partition} can be executed in parallel. \system adopts a \emph{execution DAG} (ExDAG) to capture dependencies between subcircuits. In an ExDAG, each node represents a subcircuit $F_i$, and $F_i$ points to $F_j$ iff %= if and only if
the counterpart partition $V_i$ is depended on by $V_j$. Initially, the subcircuits can be executed in parallel, not depending on others. As the execution proceeds, the edges between nodes (subcircuits) are moved and new subcircuits are available for parallel execution until all are finished.  


Unfortunately, such an execution model still falls in the low CPU utilization as illustrated in \cref{fig:solve_vs_prove}. Therefore, \system proposes a scalable pipeline execution framework, achieving high resource utilization. 

\begin{figure}[t]
    \centering
    \includegraphics[width=0.47\textwidth]{figures/zk-pipeline.pdf}
    \caption{Pipelined and parallel execution of the Scheduler.}
    \label{fig:scheduler}
\end{figure}





% 5.2节，就是解耦了witness generation和proof computation，前者cpu利用率差，很慢；后者经过优化以后可以很好的利用cpu（具体看电路规模，大的时候可以有70~80%甚至100%，可以参考图1）；同一个子电路的p_c一定要等它的w_g做完；但不同子电路的完全独立；然后这里我们更在意的是源源不断的证明任务，每个任务task的电路是一样的（也就是子电路一样），只是输入不一样；这些任务之间w_g和p_c完全独立，想怎么并行怎么并行；

% 可配置的原因是，比如一个w_g要20s，p_c要4s，那么单纯的1:1的pipeline其实还是受限于w_g，所以要根据实际情况配置两者的比例，比如上面那个里子，就是5:1比较好;但这样会带来内存的问题，5个w_g带来的内存会很大，而通过partition，比如5-partition，把这5个w_g的内存变成相当于1个，这样双赢；

% 一个可能的想法是自适应的配置，这个和电路相关，目前不确定所以没做，可以作为一个讨论

% 下面的逻辑就是前面在说pipeline，中间说怎么做，最后举了个简单的例子，这个例子可以和figure5绑定，我专门弄了F5里的三个子电路的顺序和这个例子是一样的，就是1和2子电路可并行，3依赖于1,2

% 第4节的“可串行”其实是包含一定的并行可能的，所以其实某种意义上下面的pipeline里的调度算法是可以有一些深层考虑的，也是一个讨论的方向；现在就是单纯的没依赖了，拿出来准备调度，例子里只是一个可能的调度；目前没细想

%Upon obtaining the subcircuits and their corresponding execution order, the \textbf{Scheduler} orchestrates a \textit{configurable pipeline} to manage the execution of these subcircuits in response to the continuous stream of proof requests.



\subsection{Scalable Pipeline Execution}


\paragraph{Pipeline} 
To increase resource utilization with continuous proof generation requests, termed \emph{tasks}, \system introduces a pipeline design to handle tasks simultaneously. Each task can naturally be divided into two phases: witness generation (WG) and proof computation (PC) as described in \cref{subsec:zk}.  The execution for each subcircuit also goes through the two phases. A straightforward way is to decouple the WG and PC and allow two parallel phases to process different tasks. Once WG for the former task ends, the next task's WG is scheduled in parallel to the PC of the former task. 
For the pipeline to optimize resource utilization effectively, the precondition is that the latencies of the WG and PC phases are approximately equal. However, \cref{fig:solve_vs_prove} demonstrates that WG requires much more time than the PC in the current optimized {\zk}. Consequently, overall resource utilization still remains low. 

 
\paragraph{Scalable execution} If the computational ability of each phase is scalable, \system can balance the latencies by delicately assigning resources to both phases. In this setting, the resource utilization can be maximized. To this end, the scheduler employs two groups of workers: \emph{solve workers} for WG and \emph{prove workers} for PC. With more workers devoted, the performance of the WG phase can be raised to a level comparable to the PC phase, and the number of both kinds of workers relies on concrete cases. 

With continuous tasks received, the scheduler obtains their ExDAGs and assigns the subcircuits that depend on no others to multiple solve works for parallel witness generation. As this phase for some subcircuits finishes, other subcircuits, depending on them as per the ExDAGs, become ready and are gradually scheduled for witness generation. Then, each subcircuit is delivered to a prove worker for proof computation. The PC for different subcircuits can proceed in parallel, as there are no further dependencies after the witness generation.


\begin{example}   
\Cref{fig:scheduler} illustrates how witness generation and proof computation phases can operate in parallel for three tasks: $F$, $T$, and $H$. Each of them is partitioned into three subcircuits again. Here, the number of solve and prove workers is 3 and 2, respectively.  Initially, the subcircuits $F_1$, $F_2$, and $T_2$ are assigned to three solve workers for witness generation, achieving an intra- and inter-task concurrency. This is because they do not depend on other subcircuits. 
Once WG for $F_2$ or $T_2$ ends, it is delivered to PC phase, handled by a prove worker. In the PC, all subcircuits can be executed concurrently in any arbitrary order, as the proof computation for each is fully independent. As we can see, all workers stay busy during the process, maximizing resource utilization. If the performance of the witness generation phase cannot catch up with the proof computation phase, \system can devote more skilled workers to achieve a new balance in \system's scalable framework.
\end{example}



Different hardware environments and circuit structures exhibit varying resource utilization and execution times for both phases. In some cases, WG may take significantly longer than PC, while in others, they may have close running time.   We should configure the number of both kinds of workers catering to the real workloads.  


















\begin{algorithm}[t]
\caption{Preprocessing for execution}
\label{alg:Subcircuit Construction}  % 添加label
\require{ \rm CDG $G = (V, E)$, partition $\mathcal{P} = \{V_1, \ldots, V_k\}$}

% 这里大写的F用在上面对原电路的定义里了
\ensure {\rm Subcircuits $F = \{F_1, \ldots, F_k\}$, public inputs $I_p$, secret inputs $I_s$, ExDAG $D = (\mathcal{V}, \mathcal{E})$  }

Initialize subcircuits set $F \gets \emptyset$ \\
Initialize constraints set $C_p \gets \emptyset$, input set $I \gets \emptyset$\\
Initialize shared variable sets $S_{ij} \gets \emptyset, \forall 1\le i\ne j \le k$  \\

Initialize an graph $D \gets (\mathcal{V}=\{F_1,\ldots, F_k\}, \mathcal{E}=\emptyset)$ \\

\For{\rm each partition  $V_i \in \mathcal{P}$} {
    \For{\rm each node $u \in V_i$} {
        $C_p \gets C_p \cup \{u\}$ \label{stmt:constraint_add}\\
        \For{\rm each $u$'s child node $v$ \label{stmt:process_child_start}} {
            \If{\( v \in V_j \) and \( i \neq j \)} {
            $S_{ij} \gets S_{ij} \cup \{u\}$  \\   
                $\mathcal{E}\gets \mathcal{E}\cup\{(F_i, F_j)\}$ \label{stmt:process_child_end}
            }
        }
    }
    $I \gets$ Get\_Input $(C_p, I_p, I_s)$ \label{stmt:construct_subcircuit_start}\\
    $F_i \gets$Create\_Subcircuit$(I, C_p, \cup_{t=1}^{i-1}S_{ti}, \cup_{t=i+1}^{k}S_{it})$ \label{stmt:construct_subcircuit_end}\\
    $F\gets F\cup F_i$ \\
    $C_p, I \gets \emptyset$\\
}
  

\Return{$F, D$}
\end{algorithm}



\subsection{Algorithm}



\Cref{alg:Subcircuit Construction} outlines the preprocessing for the scalable pipeline execution, including the construction of subcircuits and the ExDAG. Each partition  $V_i$ identifies the necessary inputs, constraints, and shared variables to form the corresponding subcircuit $F_i$. Specifically, the constraints associated with the nodes in the partition $V_i$ are added to the constraint set (Line \ref{stmt:constraint_add}). When a node $u\in V_i$ points to a node $u$ in a different partition $V_j$ ($j > i$), this establishes a cross-partition dependency. The output of the node $v$ is recorded as a shared variable between $F_i$ and $F_j$, and then a edge is added into $D$
(Lines \ref{stmt:process_child_start}-\ref{stmt:process_child_end}). Once all nodes in partition $V_i$ are processed, the algorithm gathers the necessary inputs, including any shared variables from prior partitions, and constructs the subcircuit $F_i$ (Lines \ref{stmt:construct_subcircuit_start}-\ref{stmt:construct_subcircuit_end}).




\begin{algorithm}[t]
\caption{Scalable Pipeline Execution}
\label{alg:pipeline}

Initialize an empty solve queue \( Q_{\text{solve}} \) and an empty prove queue \( Q_{\text{prove}} \) \label{stmt:pipeline_init_start}\\
Initialize task pool $\mathcal{P}_{task}\gets \emptyset$ \label{stmt:pipeline_init_end}

\upon{\rm receive task $R$ \label{stmt:task_receive_start}}{
    $D\gets Get\_ExDAG(R)$\\
    Add $D$ to  $\mathcal{P}_{task}$ \label{stmt:task_receive_end}\\
}


\vspace{3pt}

\tcp{Run on a separate thread} 
\While{\rm true \label{stmt:schedule_start}} {
    $F_i\gets Next\_Ready\_Subcircuit(\mathcal{P}_{task})$  \tcp{All dependencies are resolved}
    $Q_{solve}.Push(F_i)$ \label{stmt:schedule_end}\\ 
}
\vspace{3pt}

\tcp{Run on each solve worker in parallel}  
\While{\rm true \label{stmt:solve_state_start}} {
    $F_i\gets Q_{solve}.Pop()$\\
    $Witness\_Gen(F_i)$\\
    
    $C \gets Get\_Dependent\_Circuits(\mathcal{P}_{task}, F_i)$ \tcp{Get all circuits depending on $F_i$}
    \For{each subcircuit \( F_j \in C\)}{
        $Share\_Variables(F_i, F_j)$ \tcp{Share the output with dependent subcircuits}
    }

    $Q_{prove}.Push(F_i)$ \label{stmt:solve_state_end}
}
\vspace{3pt}

\tcp{Run on each prove worker in parallel}
\While{\rm true \label{stmt:prove_stage_start}} {
    $F_i\gets Q_{prove}.Pop()$\\
    $Proof\_Com(F_i)$\\
    $Finish(F_i, \mathcal{P}_{task})$\label{stmt:prove_stage_end}
}
\end{algorithm}

As the subcircuits and execution DAG are constructed, we can present the framework of our scalable pipeline model as elaborated in \Cref{alg:pipeline}. Specifically, it goes through the following stages:
\begin{itemize}[leftmargin=*]
    \item \textbf{Initialization (Lines \ref{stmt:pipeline_init_start}-\ref{stmt:pipeline_init_end}):}  
    Initially, the algorithm prepares two empty queues: $Q_{\text{solve}}$ for subcircuits ready for witness generation, and $Q_{\text{prove}}$ for subcircuits awaiting proof generation. Besides, a task pool $\mathcal{P}_{task}$ is used to store all tasks' subcircuits. 

    \item \textbf{Task receiving (Lines \ref{stmt:task_receive_start}-\ref{stmt:task_receive_end}):} Upon receiving a task to generate a proof, the execution DAG for this task is retrieved and added to the task pool $\mathcal{P}_{task}$.

    \item \textbf{Scheduling (Lines \ref{stmt:schedule_start}-\ref{stmt:schedule_end}):}  
    The scheduler continuously checks for any subcircuit $F_i$ ready for witness generation and adds it to queue $Q_{\text{solve}}$ to be consumed by available solve workers. A subcircuit is ready if all its dependencies in the DAG are resolved. 
    
    \item \textbf{Solve phase (Lines \ref{stmt:solve_state_start}-\ref{stmt:solve_state_end}):}  
    Each solve worker continuously acquires subcircuit $F_i$ from $Q_{\text{solve}}$ and performs witness generation. After that, the worker passes the shared outputs of $F_i$ to other subcircuits that depend on it to trigger succeeding execution. Then, $F_i$ is inserted into the prove queue $Q_{\text{prove}}$ for proof computation.

    \item \textbf{Prove phase (Lines \ref{stmt:prove_stage_start}-\ref{stmt:prove_stage_end}):}  
    Each prove worker separately performs proof computation for subcircuits retrieved from $Q_{\text{prove}}$ and then marks $F_i$ as completed.
\end{itemize}

% 和partition结合的好处和理由




\subsection{Discussion}


The scalable pipeline increases overall memory usage as more subcircuits are executed on different workers simultaneously. Therefore, the partitioning approach should break the circuit into suitable and manageable subcircuits, promising the memory required by all subcircuits to be executed in parallel is under limit. By tuning the factor $k$, the number of partitions, it allows \system to better control memory consumption while effectively utilizing resources. Building on the circuit partition and scalable pipeline, \system can maximize the throughput and be able to handle larger \zk workloads efficiently.

\system currently relies on manual configuration of the ratio between solve and prove workers to adapt to different scenarios. While this approach provides flexibility, a more promising direction is to tune these parameters in an adaptive manner. Specifically, as tasks are processed, \system should adjust the allocation of solve and prove workers based on real-time statistics of witness generation and proof computation phases. \system can collect this information by continuously evaluating the latencies and resource utilization during execution. This dynamic adjustment would optimize performance and resource utilization, especially in environments with varying workloads. 

% \paragraph{Parameters Determination}
% \system currently relies on manual adjustment of the ratio between solve and prove workers to adapt to different scenarios. While this approach provides flexibility, a more promising direction involves dynamically adjusting these parameters in an adaptive manner. Specifically, as the system processes tasks, it can adjust the allocation of solve and prove workers based on real-time monitoring of the witness generation (WG) and proof computation (PC) phases. By continuously evaluating the latencies and resource utilization of WG and PC during execution, the system could reconfigure the number of workers to better balance the pipeline. This dynamic adjustment would further optimize performance and resource utilization, especially in environments with varying workloads. 

% \paragraph{Memory limitations of proving keys}
% Each subcircuit requires its own proving key($pk$), and as multiple subcircuits are processed in parallel, the $pks$ for these subcircuits must coexist in memory. In cases where circuit size is large and subcircuit PKs are also sizable, memory consumption due to PKs can become a bottleneck. While partitioning reduces the size of individual subcircuits, the cumulative effect of multiple PKs stored in memory at once is not something our system optimizes. Even when adopting techniques such as PK persistence, where PKs are stored on disk and loaded as needed, the process can introduce significant overhead, potentially extending proof generation by several minutes. Therefore, while our partitioning strategy and pipeline model effectively reduce memory usage for WG and PC, PK memory remains a limitation that requires careful consideration when scaling to larger circuits.


