% todo 这里要修改一下，就是写我们对yomiya做了测试，测试内容主要针对我们设计的两个不同的模块：Split和pipeline
\section{Evaluation} \label{sec:evaluation}
In this section, we evaluate the efficiency of  \system and compare it with the current baseline.


 


\subsection{Experimental setup}\label{6.1}

\paragraph{Implementation} We built Yoimiya based on the Gnark library\footnote{\url{https://github.com/ConsenSys/gnark}}, a high-performance \zk framework written in Go, which provides a high-level API for designing cryptographic circuits. In our implementation, we adopt the Rank-1 Constraint System (R1CS) \cite{parno2016pinocchio} to represent the constraints. The bilinear map used in Gnark is instantiated using the BN254 curve\footnote{\url{https://github.com/Consensys/gnark-crypto}}, which offers approximately 100 bits of security. This curve's pairing operations are also supported in Solidity, the programming language used for Ethereum smart contracts.

\paragraph{Workloads} Our circuit is constructed using Gnark and represents a zero-knowledge proof for a simple linear recursive sequence, defined as $F_n = \alpha F_{n-1} + \beta F_{n-2}$. This allows us to prove that the $n$-th term in the sequence is equal to a specified value. We can vary the constraints in the corresponding constraint system by adjusting the number of iterations. The constraint number of the circuit used in our experiment is up to 60 million.

\paragraph{Metrics}
We assess the effectiveness and efficiency of \system from the following  metrics:
\begin{itemize}[leftmargin=*]
    \item Prove Memory: the memory required during the proof generation, including the memory consumed by witness generation and proof computation.

    \item Total Memory: the overall memory usage, counting the prove memory and additional memory used by other components, i.e., the constraint system and the key management system.
    \item Prove Time: the total time to generate all proofs, excluding the one-time setup of the circuit and the verification phases on the client sides.
    \item CPU Usage: the CPU utilization over time during the proof generation process.

\end{itemize}


\paragraph{Testbed}  We performed experiments on a server equipped with an Intel Xeon Gold-6330 2.00GHz CPU with 56 cores, 112 threads, and 500GB RAM, running Ubuntu 20.04.2 LTS. Building on the resources available and the real workloads, we set the number of solve and prove workers to 4 and 1, respectively, which appropriately balances the latency for both phases in our experiments. We run each set of experiments multiple times and take the average.


%To simplify the experimental setup, we set the number of prove workers to 1, treating multiple instances of proof computation as independent tasks. The number of solve workers in our experiments is up to 4.

\subsection{Performance of Circuit Partitioning}
\label{6.2}
 We evaluate the effectiveness of circuit partitioning on memory usage and proof generation time.

% \begin{figure}[t]
% \centering
% \includegraphics[width=0.48\textwidth]{figures/graph_size_performace.pdf}
% \caption{Performance of the normal and 2-partition approaches under different loop sizes.}
% \label{fig:graph_size_performance}
% \end{figure}
% 第一部分实验就是针对Partition
% 1. 改变电路规模（电路是循环，所以调整循环数即可），固定partition数为2，说明划分算法在电路规模上的可扩展性
% 2. 改变分区数为2~5，固定电路，说明划分算法在分区数上的可扩展性

% 实验1：circuit(graph)_size_partition，结果就是内存要下去，这里pk没堆叠，所以total和prove都是明显下降，然后时间要尽可能一样，就是划分后跑2个和原来跑一个的时间差不多
\paragraph{Circuit size}
We first assessed the performance of our partitioning approach with varying circuit sizes, which is controlled by the loop count, as shown in \cref{fig:performance_comparison}. Meanwhile, the number of partitions is fixed to 2, and the loop count is up to 10 million. \crefrange{fig:graph_size_performance_a}{fig:graph_size_performance_b} showcases that the circuit partitioning significantly reduces total and prove memory consumption. Moreover,  the reduction becomes more pronounced as the circuit size increases. For example, the prove memory consumption with 2-partition is 59\% of the normal case when loop count is 100K while it decreases to 51\% with 10 million loops. 
Importantly, \cref{fig:graph_size_performance_c} confirms that the gap of total proof generation between the two approaches remains slight (up to 13\%), not sacrificing too much latency. This is critical for the scalable pipeline design, as the partitioning approach will not significantly raise the latency of circuit execution.


\begin{figure}[t]
    \centering
    \captionsetup[subfigure]{skip=0pt}
    \begin{subfigure}[b]{0.15\textwidth}
        \centering
        \includegraphics[width=\textwidth]{experiments/graph_size_performace_a.pdf}
        \captionsetup{justification=centering}  % Center the caption
        \caption{Total Memory}
        \label{fig:graph_size_performance_a}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.15\textwidth}
        \centering
        \includegraphics[width=\textwidth]{experiments/graph_size_performace_b.pdf}
        \captionsetup{justification=centering}  % Center the caption
        \caption{Prove Memory}
        \label{fig:graph_size_performance_b}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.15\textwidth}
        \centering
        \includegraphics[width=\textwidth]{experiments/graph_size_performace_c.pdf}
        \captionsetup{justification=centering}  % Center the caption
        \caption{Prove Time}
        \label{fig:graph_size_performance_c}
    \end{subfigure}
    \caption{Performance of the normal and 2-partition approaches under different loop sizes.}
    \label{fig:performance_comparison}
\end{figure}

%the scalability of our partitioning approach by varying the size of the circuits, controlled through the loop count, while keeping the number of partitions fixed at 2. 

% First, we evaluate the impact of circuit partitioning on memory usage and proof generation time for different circuit sizes(by adjusting the loop number). To control the variables, we set the number of partitions to 2. We mainly tested three indicators: \textbf{Total Memory}, \textbf{Prove Memory}, \textbf{Prove Time}. \textbf{Total Memory} refers to the circuit-specific memory required for each subcircuit, such as the proving key and constraint system, as well as the memory used during the actual proof generation process(denoted as \textbf{Prove Memory}). \textbf{Prove time} refers to the time required to generate the zero knowledge proof corresponding to the generated circuit (or all sub circuits).

  
 


\begin{figure}[t]
    \centering
    \captionsetup[subfigure]{skip=0pt}
    \begin{subfigure}[b]{0.22\textwidth}  % Adjust width to balance in 2x2 layout
        \centering
        \includegraphics[width=\textwidth]{experiments/k_partition_performance_a.pdf}
        \captionsetup{justification=centering}  % Center the caption
        \caption{Total Memory}
        \label{fig:k_partition_performance_a}
    \end{subfigure}
    \hspace{0.01\textwidth}  % Adjust horizontal spacing between subfigures
    \begin{subfigure}[b]{0.22\textwidth}  % Adjust width to balance in 2x2 layout
        \centering
        \includegraphics[width=\textwidth]{experiments/k_partition_performance_b.pdf}
        \captionsetup{justification=centering}  % Center the caption
        \caption{Prove Memory}
        \label{fig:k_partition_performance_b}
    \end{subfigure}

    \vspace{0.1em}

    \begin{subfigure}[b]{0.22\textwidth}  % Adjust width to balance in 2x2 layout
        \centering
        \includegraphics[width=\textwidth]{experiments/k_partition_performance_c.pdf}
        \captionsetup{justification=centering}  % Center the caption
        \caption{Memory Reduction}
        \label{fig:k_partition_performance_c}
    \end{subfigure}
    \hspace{0.01\textwidth}  % Adjust horizontal spacing between subfigures
    \begin{subfigure}[b]{0.22\textwidth}  % Adjust width to balance in 2x2 layout
        \centering
        \includegraphics[width=\textwidth]{experiments/k_partition_performance_d.pdf}
        \captionsetup{justification=centering}  % Center the caption
        \caption{Prove Time}
        \label{fig:k_partition_performance_d}
    \end{subfigure}

    \caption{Performance for the normal and partitioned approaches across different partition numbers.}
    \label{fig:performance_comparison}
\end{figure}


% \begin{figure}[t]
% \centering
% \includegraphics[width=0.48\textwidth, trim={0 0 0 0}, clip]{figures/k_partition_performance.pdf}
% \caption{Performance for the normal and partitioned approaches across different partition numbers.}
% \label{fig:k_partition_performance}
% \end{figure}

% 实验二，partition_number_performance，就是改变分区数，内存和时间很直观，就是时间不怎么变，内存能下来；给了一个内存下降的比例（尽可能趋近反比例函数），凑图的；然后给出了一个不同分区占用的时间；
% 我这里还有setup的实验数据，也是时间和占比;
\paragraph{Partition number}
Next, we tested the partitioning approach with varying numbers of partitions, ranging from 1 to 5, while the constraint number is 60 million. A partition count of 1 stands for the normal case in previous tests. \crefrange{fig:k_partition_performance_a}{fig:k_partition_performance_b} show a near-linear reduction in both total memory and prove memory with a growth of partition number. In particular, the total memory consumption is reduced from 66.7 GB to 20.4 GB when the partition number reaches 5.  This trend is further confirmed in \cref{fig:k_partition_performance_c}, which presents the reduction in memory usage over partition number. Lastly, \cref{fig:k_partition_performance_d} breaks down the proof generation time for each partition. As expected, the time required for each partition goes from 27s to 6s as the number of partitions increases. This demonstrates that our partitioning strategy effectively reduces memory usage while maintaining stable execution time, establishing the foundation of the scalable pipeline execution.

% Next, we evaluate the effect of increasing the number of partitions on memory usage and proof generation time. In this experiment, we tested multiple partition configurations ranging from 1 to 5 subcircuits, where a partition count of 1 represents the standard (unpartitioned) execution. As seen in \Cref{fig:k_partition_performance_a} and \Cref{fig:k_partition_performance_b}, increasing the number of partitions leads to a nearly linear reduction in both total memory and prove memory. \Cref{fig:k_partition_performance_c} provides a more detailed view of this reduction, showing the percentage decrease in memory usage, which grows progressively with partitioning, illustrating the scalability of the partitioning approach. Lastly, \Cref{fig:k_partition_performance_d} breaks down the total proof generation time for each subcircuit, where individual circuits are denoted as $Part_i$. The time for each part decreases as the number of partitions increases, ensuring that the overall proof generation remains efficient while memory usage is significantly reduced.

% 第二部分实验，就是Pipeline以及pipeline+partition的效果
\subsection{Performance of Scalable Pipeline Execution}
\label{6.3}
% 一个电路，电路是实验一中最大的电路6000w约束，然后m:n的pipeline设置简化为了m:1，因为每个m：1看成一个独立的个体；然后测内存的时间多个子电路的pk是堆叠的，所以在这个实验里total的效果会比prove差一点；然后用20个证明请求来说明多任务
In this experiment, we evaluated the performance of our scalable pipeline framework towards continuous tasks.   To simulate such continuous tasks, we produce 20 tasks with approximately 60 million constraints and feed them into \system gradually. 
We want to test \system with varying rates between the number of solve and prove workers. Therefore, we fix one prove worker and vary the number of solve workers, as the witness generation requires much more time than proof computation. 


\begin{figure}[t]
    \centering
    \captionsetup[subfigure]{skip=0pt}
    \begin{subfigure}[b]{0.15\textwidth}
        \centering
        \includegraphics[width=\textwidth]{experiments/pipeline_solve_number_a.pdf}
        \captionsetup{justification=centering}  % Center the caption
        \caption{Time Cost}
        \label{fig:pipeline_solve_performance_a}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.15\textwidth}
        \centering
        \includegraphics[width=\textwidth]{experiments/pipeline_solve_number_b.pdf}
        \captionsetup{justification=centering}  % Center the caption
        \caption{Total Memory}
        \label{fig:pipeline_solve_performance_b}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.15\textwidth}
        \centering
        \includegraphics[width=\textwidth]{experiments/pipeline_solve_number_c.pdf}
        \captionsetup{justification=centering}  % Center the caption
        \caption{Prove Memory}
        \label{fig:pipeline_solve_performance_c}
    \end{subfigure}
    \caption{Performance for different numbers of solve workers across the normal and 2-partition approaches. }
    \label{fig:pipeline_solve_performance}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.46\textwidth]{experiments/pipeline_solve_cpu_usage.pdf}
    \caption{CPU usage distribution for different execution modes. Normal:  serial execution of all tasks; $(s,p)$: the number of solve workers is set to $s$, and the number of partitions is set to $p$. If $p=1$, it means no partitioning is applied. } 
    \label{fig:pipeline_solve_cpu_performance}
    
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.48\textwidth]{experiments/cpu_pipeline_track.pdf}
    \caption{CPU usage with continuous tasks over time for different execution modes. } 
    \label{fig:pipeline_solve_cpu_performance}
\end{figure}

%To simplify the experimental setup, we set the number of prove workers to 1, treating multiple instances of proof computation as independent tasks. We fixed the number of tasks to 20, each involving circuits with approximately 60 million constraints, which is same as the largest circuit size used in the previous experiment.

% todo 没有说明pk的堆叠，但是否有必要说明？

% 首先测试的是pipeline，然后给出了pipeline+2-partition，因为要控制变量，这里的变量是witness generation的并行数0-4，要说明的就是1. 内存越来越大；2. 利用率越来越高；3. 时间越来越快；4. 用了2-partition的会比不用的内存小，时间、利用率差不多



\paragraph{Solve worker number}
First, we test the impact of solve worker number on the performance with 2 partitions for each circuit as reported in \cref{fig:pipeline_solve_performance}.
As illustrated in \cref{fig:pipeline_solve_performance_a}, increasing the number of solve workers significantly reduces the total time required to complete all tasks from 464s to 164s. This is because a prove worker outperforms a solve worker, so more solve workers must be devoted to increasing the parallelism, balancing their latency.  Note that when the performance of witness generation has aligned with proof computation, there is no need to scale solve workers. 
\crefrange{fig:pipeline_solve_performance_b}{fig:pipeline_solve_performance_c} reveals that as the number of solve workers increases, so does memory consumption. This because running multiple witness generation instances simultaneously is memory-intensive. \system partitions each circuit into more subcircuits, reducing the memory consumption while maintaining the parallelism between solve workers. In \cref{fig:pipeline_solve_performance_c}, the prove memory consumption for the 2-partition case is only 56.1\% of the single-partition case, lowering the memory barrier of the system. 




 %However, \crefrange{fig:pipeline_solve_performance_b}{fig:pipeline_solve_performance_c} reveal a tradeoff with memory usage to be considered in practice.
%As shown in \Cref{fig:pipeline_solve_performance}. This combination reduced memory usage significantly while maintaining nearly identical proof generation times compared to the non-partitioned pipeline. This demonstrates that partitioning is an effective strategy to alleviate memory overhead, ensuring that memory consumption remains within manageable limits without compromising execution speed. 



\paragraph{Varying combination}
In addition, \cref{fig:pipeline_solve_cpu_performance} shows the CPU utilization of \system with various parameters. As the parallelism of the witness generation phase increases (with more solve workers), the CPU engagement improves significantly up to 52.9\% for the parameter (4,1) while the (1,1) case only exhibits 21.8\% CPU utilization. Additionally, the growth of partition number has slightly impacted the overall CPU utilization, which is 52.9\% and 56.7\% for (4,1) and (4,2). However, the partition approach can help to decrease the memory requirement significantly in our scalable pipeline framework as explained above. \cref{fig:pipeline_solve_cpu_performance} further elaborates the CPU usage of one typical case (4,2) in \cref{fig:pipeline_solve_cpu_performance} and the normal case over time. As we can see, the CPU usage for case (4,2) maintains over 60\%, mostly due to the scalable pipeline execution. The CPU usage for (4,2) also periodically falls low due to the sequential preparation phases during proof computation, which temporarily limits resource utilization. However, this period of case (4,2) is much shorter than the normal case. 






% Higher parallelism in witness generation allows the system to better utilize available CPU resources, enhancing overall efficiency.



%In \system, increasing the number of solve workers in witness generation is aimed at improving the overall throughput of the system by leveraging the pipeline's ability to decouple witness generation (WG) from proof computation (PC). This allows the system to handle continuous ZKP requests more efficiently, as parallel WG ensures that the system can process multiple tasks concurrently. By running multiple WG instances simultaneously, the system seeks to maximize the utilization of CPU resources and speed up the generation of ZKPs.












% First, we examine how different levels of witness generation parallelism affect performance. As shown in \Cref{fig:pipeline_solve_performance_a}, the total time required to complete all tasks decreases as the level of witness generation parallelism increases. This illustrates the benefits of parallelism achieved through our pipeline approach. However, in \Cref{fig:pipeline_solve_performance_b} and \Cref{fig:pipeline_solve_performance_c}, we also observe that both total memory and prove memory increase with the rise in parallelism granularity. This is because witness generation itself requires significant memory resources. Addressing this challenge is a key reason for combining partitioning with the pipeline method. To illustrate this, we also present the results of combining the 2-partition approach with the pipeline method. As seen in \Cref{fig:pipeline_solve_performance}, memory usage decreases significantly while maintaining nearly the same proof generation time compared to the non-partitioned approach. This demonstrates that partitioning helps to mitigate memory consumption without compromising overall performance, particularly for larger parallelism levels. \Cref{fig:pipeline_solve_cpu_performance} reflects the distribution of CPU usage over time for different execution methods. The results show a clear increase in overall CPU utilization when the pipeline method is applied, with witness generation parallelism leading to higher CPU engagement. Additionally, we provide the average CPU utilization for each method in the graph, showing that CPU efficiency improves significantly as the parallelism level increases. 
% \begin{figure}[t]
% \centering
% \includegraphics[width=0.48\textwidth]{figures/pipeline_solve_number.pdf}
% \caption{Performance for different levels of witness generation parallelism across the normal and 2-partition approaches. }
% \label{fig:pipeline_solve_performance}
% \end{figure}






\begin{figure}[t]
    \centering
    \captionsetup[subfigure]{skip=0pt}
    \begin{subfigure}[b]{0.22\textwidth}  % Adjust width to balance in 2x2 layout
        \centering
        \includegraphics[width=\textwidth]{experiments/pipeline_partition_performance_a.pdf}
        \captionsetup{justification=centering}  % Center the caption
        \caption{Prove Time}
        \label{fig:pipeline_partition_performance_a}
    \end{subfigure}
    \hspace{0.01\textwidth}  % Adjust horizontal spacing between subfigures
    \begin{subfigure}[b]{0.22\textwidth}  % Adjust width to balance in 2x2 layout
        \centering
        \includegraphics[width=\textwidth]{experiments/pipeline_partition_performance_b.pdf}
        \captionsetup{justification=centering}  % Center the caption
        \caption{CPU Usage Distribution}
        \label{fig:pipeline_partition_performance_b}
    \end{subfigure}

    \vspace{0.1em}

    \begin{subfigure}[b]{0.22\textwidth}  % Adjust width to balance in 2x2 layout
        \centering
        \includegraphics[width=\textwidth]{experiments/pipeline_partition_performance_c.pdf}
        \captionsetup{justification=centering}  % Center the caption
        \caption{Prove Memory}
        \label{fig:pipeline_partition_performance_c}
    \end{subfigure}
    \hspace{0.01\textwidth}  % Adjust horizontal spacing between subfigures
    \begin{subfigure}[b]{0.22\textwidth}  % Adjust width to balance in 2x2 layout
        \centering
        \includegraphics[width=\textwidth]{experiments/pipeline_partition_performance_d.pdf}
        \captionsetup{justification=centering}  % Center the caption
        \caption{Total Memory}
        \label{fig:pipeline_partition_performance_d}
    \end{subfigure}

    \caption{Comparison of performance across different partition numbers in the pipeline approach.}
    \label{fig:pipeline_partition_performance}
\end{figure}
% \begin{figure}[t]
% \centering
% \includegraphics[width=0.46\textwidth]{figures/pipeline_partition_performance.pdf}
% \caption{Comparison of performance across different partition numbers in the pipeline approach.} \label{fig:pipeline_partition_performance}
% \end{figure}

% 这里就是控制witness generation的数量后，控制为4个，就是上面实验里最好的一个；然后改变partition数量，得到结果是1. 时间差不多，因为时间和partition无关，且在实验一里得到了分区和不分区时间差不多的结论；2. cpu利用率差不多，其实这里箱型图出来后面3个柱子很短，很难解释，可以解释为电路小了以后wg和pc彼此之间的影响更小；3. 内存明显减少，甚至比如p=4的时候，用了pipeline和baseline基本内存一样，内存可控，速度提升 
% Next, to further demonstrate the benefits of combining partitioning with the pipeline approach, we conducted an additional experiment in which the parallelism level was fixed at 4, the most effective configuration from the previous evaluation. In this experiment, we varied the number of partitions to evaluate its impact, as shown in \Cref{fig:pipeline_partition_performance}. \Cref{fig:pipeline_partition_performance_a} shows that the total time required to complete all proof tasks remains nearly constant as the number of partitions increases, consistent with the results from the non-partitioned pipeline execution, which is significantly faster than serial execution. \Cref{fig:pipeline_partition_performance_b} highlights the CPU utilization distribution, demonstrating that high CPU usage is sustained across different partition counts when employing the pipeline approach. Additionally, \Cref{fig:pipeline_partition_performance_c} and \Cref{fig:pipeline_partition_performance_d} present the total memory and prove memory, respectively, showing that increasing the number of partitions effectively reduces the memory overhead introduced by the pipeline. This allows the proof generation process to remain efficient while keeping memory usage within manageable limits.
\paragraph{Partition number} Last, we evaluate the scalable pipeline framework with fix 4 solve workers against varying partition numbers as shown in \cref{fig:pipeline_partition_performance}. The normal case means no partition and pipeline design.
The results, shown in \cref{fig:pipeline_partition_performance_a}, reveal that the total proof generation time is irrespective of the number of partitions if the memory is affordable, remaining around 160s in all cases with the pipeline. This indicates that partitioning does not introduce significant overhead to the system in terms of time. \cref{fig:pipeline_partition_performance_b} shows that CPU utilization stays consistently high (around 55\%) across different partition configurations, confirming that partitioning does not affect the performance again. Finally, \crefrange{fig:pipeline_partition_performance_c}{fig:pipeline_partition_performance_d} show that increasing the number of partitions leads to substantial reductions in both total memory and prove memory in the scalable pipeline framework. This reduction enables \system to handle multiple larger circuits concurrently without the risk of overwhelming the memory. 




