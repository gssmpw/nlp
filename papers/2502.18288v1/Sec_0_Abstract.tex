% Zero-knowledge proofs (ZKPs) are increasingly employed in privacy-preserving technologies, blockchain applications, and secure authentication. However, several challenges remain in their current implementation. The high memory footprint not only limits the scalability of ZKP systems but also hinders their applicability on resource-constrained platforms. Furthermore, in most implementations, the witness generation and proof computation stages are tightly coupled. While recent optimizations have focused on parallelizing the proof phase, this coupling has caused witness generation to become a bottleneck, leading to inefficiencies in resource utilization and prolonged proof generation time.

% We present \textbf{Yoimiya}, a flexible ZKP framework that addresses these challenges. It introduces an automatic circuit partitioning mechanism that allows users to split ZKP circuits into sequential sub-circuits, reducing memory consumption during proof generation. Furthermore, Yoimiya decouples the witness generation and proof computation stages, organizing them into a pipeline with configurable parallelism. This design improves resource utilization and speeds up proof generation, particularly when generating multiple proofs. By combining partitioning with pipelining, Yoimiya efficiently manages memory in parallelized environments. Extensive experiments demonstrate its effectiveness in improving proof generation efficiency and reducing memory overhead.
\begin{abstract}
%Zero-Knowledge Proof systems, particularly {\zk}s, are widely used in verifiable computation outsourcing applications. Despite the efficiency of ZK-SNARKs in proof verification, generating proofs remains a computationally intensive task, particularly in systems handling continuous streams of proof requests. Existing optimization efforts have primarily focused on accelerating proof computation, often overlooking the inefficiencies in the witness generation phase, which has now become a critical bottleneck.
With the widespread adoption of Zero-Knowledge Proof systems, particularly {\zk}, the efficiency of proof generation, encompassing both the witness generation and proof computation phases, has become a significant concern. While substantial efforts have successfully accelerated proof computation, progress in optimizing witness generation remains limited, which inevitably hampers overall efficiency.
%Existing optimization efforts have primarily focused on accelerating proof computation, often overlooking the inefficiencies in the witness generation phase, which has now become a critical bottleneck.
In this paper, we propose \system, a scalable framework with pipeline, to optimize the efficiency in \zk systems. First,  \system introduces an automatic circuit partitioning algorithm that divides large circuits of {\zk} into smaller subcircuits, the minimal computing units with smaller memory requirement, allowing parallel processing on multiple units. 
%effectively reducing memory consumption while minimizing resource contention.
Second, \system decouples witness generation from proof computation, and achieves simultaneous executions over units from multiple circuits. Moreover, \system enables each phase scalable separately by configuring the resource distribution to make the time costs of the two phases aligned, maximizing the resource utilization. 
%a configurable pipeline framework that decouples witness generation from proof computation, allowing flexible control over parallelism, thereby improving CPU utilization and proof generation speed. However, the pipeline introduces additional memory overhead due to concurrent witness generation and proof computation. 
%To address this, we introduce an automated circuit partitioning algorithm that divides large circuits into smaller subcircuits, effectively reducing memory consumption while minimizing resource contention. 
%Our approach tackles both computational and memory efficiency challenges, providing a more adaptable solution for ZK-SNARK systems. 
Experimental results confirmed that our framework effectively improves the resource utilization and proof generation speed.
\end{abstract}