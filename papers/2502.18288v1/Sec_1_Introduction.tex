Zero-knowledge proof, a prevailing cryptographic technique, allows one party to convince another of the correctness of a statement while not leaking any valuable secret of it. \zk (Zero-Knowledge Succinct Non-Interactive Argument of Knowledge), the most popular protocol, is widely used in industry to protect privacy and secure data, as it additionally avoids the interaction between prover and verifier and generates proof with constant size. For instance, \zk  
is used in outsourced computing platforms to force the service providers to accomplish the target task delegated by users as expected. The succinct feature of \zk's proof enables fast verification on the user side. To pursue better service, the provider's mission is to respond to all requests by generating corresponding proofs as quickly as possible. However, the inherent performance bottleneck in proof generation significantly stops the throughput improvement. 


%the proof generation process remains a major bottleneck due to its intensive computational demands



%When a user delegates a heavy computation task to a remote computing platform, ZK-SNARK can be used to generate a proof that the computation task was performed truthfully and correctly, and send it along with the computation results to the client, who can verify the correctness of the results.  As a computing service provider, it is very demanding on the resources of the computing servers to generate proofs for all the computation results quickly while completing all kinds of computation tasks received in a steady stream. While the succinctness of ZK-SNARKs enables fast verification, the proof generation process remains a major bottleneck due to its intensive computational demands. 


%For example, ZK-SNARK  builds trust between service providers and service requesters in an outsourced computing platform. When a user delegates a heavy computation task to a remote computing platform, ZK-SNARK can be used to generate a proof that the computation task was performed truthfully and correctly, and send it along with the computation results to the client, who can verify the correctness of the results.  As a computing service provider, it is very demanding on the resources of the computing servers to generate proofs for all the computation results quickly while completing all kinds of computation tasks received in a steady stream. While the succinctness of ZK-SNARKs enables fast verification, the proof generation process remains a major bottleneck due to its intensive computational demands. 



%is correct without having to provide any information other than its correctness, which is crucial for protecting privacy and data security.
%Particularly noteworthy is the fact that ZK-SNARK(Zero-Knowledge Succinct Non-Interactive Argument of Knowledge)is widely used in industry due to the advantages that its proof process does not require interaction and the small size of the generated proofs.


% circuit: ZK-SNARKs typically use \emph{circuit} to represent a computational process, where each step of the computation is modeled as a logical operation. sec 2.1，这里

%Suggestion:
%Concepts about circuit and the functionalities of two phases are not used in upcoming discussion, no need of including them here. Just get rid of them. Directly go ahead to  analyze the performance bottleneck of PC, do not say too much regarding the optimization for WG (summarized within one sentence), because it's not our focus. 


%Typically, ZK-SNARKs  use \emph{circuit}s to represent the computation logic, and the generation of proofs for ZK-SNARKs consists of two phases:  \textit{witness generation(WG)} and \textit{proof computation(PC)}. Meanwhile, WG is the process where the prover constructs the necessary inputs, known as the witness, that satisfy the predicate $F$ for a given instance. And PC is the process that the prover produces a succinct cryptographic proof from the witness based on the proving key and the circuit constraints.


Typically, the proof generation for \zk consists of two phases:  \textit{witness generation(WG)} and \textit{proof computation(PC)}. The WG  constructs the necessary inputs, known as the witness, satisfying a specified predication and the PC produces a succinct cryptographic proof based on the witness and some other data. Recently, the optimization for the PC phase has made a great achievement, successfully accelerating the execution through various parallel solutions \cite{botrel2023faster, chen2024load, zhang2021pipezk, ma2023gzkp, ji2024accelerating}. Conversely, the development of PC optimization is deeply under our expectation, because the WG phase is closely associated with the specific logic of the task to be proved, which prevents general optimization in advance. 
Consequently, the WG naturally becomes a new performance bottleneck for proof generation.



%Optimization of PCs, whether through the underlying implementation of algorithms \cite{botrel2023faster, chen2024load} or based on hardware acceleration \cite{zhang2021pipezk, ma2023gzkp, ji2024accelerating}, is relatively easy as they are both general-purpose optimisations that are not related to specific circuit logic.

%However, optimising the PC phase alone is not sufficient for a process that incorporates both WG and PC for proof generation. In the process of proof generation, after the PC is optimised fast enough, the WG naturally becomes a new performance bottleneck for the system. At this point, even if more physical resources are allocated, the system performance cannot be further improved. And the WG is closely related to the specific circuit logic, it is difficult to do general optimisation in advance.


\begin{figure}[t]
    \centering
    \includegraphics[width=0.45\textwidth]{figures/cpu_track.pdf}
    \caption{CPU usage with continuous tasks over time.}
    \label{fig:CPU_track}
\end{figure}
% 这里一定要强调proof computation经过了优化，这样在多核的场景下,witness generation才会是瓶颈
%In practical outsourced computation scenarios, especially when third-party platforms are continuously tasked with generating proofs, ensuring computational efficiency is paramount. These platforms must maintain high throughput and optimal resource utilization to meet performance goals, such as minimizing latency and maximizing processing capacity. While the succinctness of ZK-SNARKs enables fast verification, the proof generation process remains a major bottleneck due to its intensive computational demands. This process can be broadly divided into two stages: \textit{witness generation} and \textit{proof computation}. 
%%需要顺一下circuit是怎么出来的



%Optimization of PCs, whether through the underlying implementation of algorithms \cite{botrel2023faster, chen2024load} or based on hardware acceleration \cite{zhang2021pipezk, ma2023gzkp, ji2024accelerating}, is relatively easy as they are both general-purpose optimisations that are not related to specific circuit logic. However, optimising the PC phase alone is not sufficient for a process that incorporates both WG and PC for proof generation. In the process of proof generation, after the PC is optimised fast enough, the WG naturally becomes a new performance bottleneck for the system. At this point, even if more physical resources are allocated, the system performance cannot be further improved. And the WG is closely related to the specific circuit logic, it is difficult to do general optimisation in advance.


%However, the witness generation phase still faces substantial challenges. Many circuit structures lack sufficient parallelism, which limits the potential for concurrent execution and results in a waste of resources during witness generation. Even in multi-core environments, this leads to inefficient CPU usage. Consequently, when platforms are flooded with continuous proof requests, the lack of efficient parallelism in the witness generation stage results in low throughput and wasted resources, ultimately hindering the platform's scalability and overall efficiency. 
%In fact, due to the lack of parallelism in the WG phase, the resources of the system have been underutilised.

To make a better understanding of this issue, we tested a real example of the \zk, where the logic is expressed by a \emph{circuit}. Basically, we monitored the CPU usage over time against a stream of continuous tasks of proof generation. 
These tasks all rely on identical circuits—a linear recurrence circuit with 60 million constraints (refer to \cref{sec:evaluation} for details). As shown in \cref{fig:CPU_track}, the CPU utilization 
exhibits intense fluctuations over time
and commonly remains at an extremely low level. Despite there are sufficient tasks to run,  CPU utilization still exhibits a consistent cyclical pattern, indicating inefficiencies in resource utilization. 
In light of this, we conducted further tests focusing on the two phases separately.  \cref{fig:solve_vs_prove} showcases that increasing CPU resources significantly accelerates proof computation, while the latency of witness generation remains largely unaffected regardless of the CPU number. This inefficiency highlights that witness generation has become the bottleneck in the {\zk} system, preventing the optimal utilization of available computational resources.





% \begin{figure}[t]
% \centering
% \includegraphics[width=0.48\textwidth]{asplos25-templates/figures/solve_vs_prove.pdf}
% \caption{Performance for witness generation and proof computation across different numbers of CPUs.}
% \label{fig:solve_vs_prove}
% \end{figure}
% cpu利用率要上去不需要partition，partition对内存的减少需要无pk（有pk就是单纯减少prove memory），就是一次性，cpu那边需要堆pk，然后partition针对一个电路，目前后续pipeline针对证明流
%%%之前讨论的分别对CPU和内存的要求不一样还存在吗？

%Fortunately, ...


\paragraph{Our solution} In this paper, we propose \system, a scalable framework for optimal resource utilization in \zk systems.
To increase the CPU utilization rate, \system leverages the \emph{pipeline} to make the proof generation for different tasks interleave. To this end, \system decouples the witness generation phase and proof computation phase and assigns them for adjacent tasks to distinct computing units. Ideally, all computing units should be occupied by tasks simultaneously, maximizing CPU utilization. However, this pipeline design comes up with two severe issues to be solved: \emph{increased memory consumption} and \emph{imbalanced execution costs}.  

\paragraph{Increased memory consumption} As more tasks, especially the witness generation, run concurrently, memory consumption grows at an incredible speed—which becomes not affordable to systems. To mitigate this, \system adopts a topological sort-based greedy partitioning algorithm that breaks down large circuits into smaller subcircuits, which can be executed sequentially and separately.  \system guarantees that by executing these subcircuits in a certain order, the final result is equivalent to that of directly running the full circuit. This makes executing large-scale circuits with limited memory resources possible in practice. 


%reducing the overall memory required to complete the entire computation, while ensuring that the total proof generation time remains nearly unchanged.

%By combining partitioning with the pipeline model, we enable faster proof generation while keeping memory usage within acceptable limits, offering a scalable and efficient solution for verifiable computation involving large-scale zero-knowledge proof circuits.

\paragraph{Imbalanced execution costs}
The benefit of pipeline is maximized when the costs of witness generation and proof computation for each subcircuit are approximately equal. However, the performance of witness generation is influenced by the circuit and resource configuration and is commonly not comparable with proof computation, resulting in a skewed workload for both phases. Therefore, we propose a scalable framework in \system, where the ability of each phase is scalable and configurable. By tuning the resources assigned to both phases, \system can achieve a balanced workload across phases. 

%the cost of witness generation to proof computation varies depending on the circuit and resource configuration, meaning that the benefits of a simple pipeline model may be limited if the concurrency is not configurable. As a result, we propose a configurable pipeline model that allows multiple witness generation tasks to be executed concurrently with a single proof computation task.

%Although WGs and PCs cannot be brought up in parallel for the same circuit, for a batch of circuits, full consideration can be given to further enhancing the resource utilisation of the system and improving the throughput capability of the system by pipelining the WG and PC phases between multiple circuits.
%A natural solution is to use \textbf{pipeline} techniques to address this challenge. Since proof computation and witness generation are sequential but independent processes, they inherently fit into a pipeline model.  However, the cost of witness generation to proof computation varies depending on the circuit and resource configuration, meaning that the benefits of a simple pipeline model may be limited if the concurrency is not configurable. As a result, we propose a configurable pipeline model that allows multiple witness generation tasks to be executed concurrently with a single proof computation task.

%However, the pipeline model introduces a new issue: \textbf{Increased Memory Overhead}. 


\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.22\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/solve_vs_prove_a.pdf}
        \captionsetup{justification=centering}  % Center the caption
        \caption{Time Cost}
        \label{fig:solve_vs_prove_a}
    \end{subfigure}
    % \hfill
        \hspace{0.01\textwidth}
    \begin{subfigure}[b]{0.22\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/solve_vs_prove_b.pdf}
        \captionsetup{justification=centering}  % Center the caption
        \caption{CPU Utilization}
        \label{fig:solve_vs_prove_b}
    \end{subfigure}
    \caption{Performance for witness generation and proof computation across different numbers of CPUs. }
    \label{fig:solve_vs_prove}
\end{figure}

In summary, our contribution to this paper includes:
\begin{itemize}[leftmargin=*]
    \item[1)]  We propose the pipeline technique to increase the CPU utilization in \system tasks. Moreover, we introduce an automatic circuit partitioning algorithm that divides large circuits into sequential subcircuits, which decreases the memory usage to execute large-scale circuits. 
    \item[2)]    We decouple the witness generation phase and proof computation phase for each subcircuit and enable them scalable separately. This model balances the costs of both phases,  optimizing parallelism granularity for faster proof generation.
    \item[3)] We implement \emph{\system}, which integrates the proposed technologies and conducts extensive experimental evaluations. The results demonstrate that \system significantly improves resource utilization and proof generation speed under controllable memory usage.
\end{itemize}
%\raggedbottom



The remainder of the paper is structured as follows. \cref{sec:background} covers the background on \zk and the motivation for this work. \cref{sec:overview} overviews {\system}'s design, and \crefrange{sec:partitioner}{sec:scheduler} detail its specific components.  \cref{sec:evaluation} evaluates \system's performance  while \cref{sec:related_work} reviews related literature. Finally, \cref{sec:conclusion} concludes the paper.