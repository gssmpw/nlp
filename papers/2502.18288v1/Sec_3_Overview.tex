\section{Overview}\label{sec:overview}
In this section, we provide an overview of \system, covering its system architecture as well as the foundational model on which it operates. 

\subsection{System Model}

\system operates based on the following three fundamental assumptions.

\paragraph{Continuous requests} The advancement of \system relies on maximizing the overall throughput for proof generation of \zk towards a stream of requests rather than optimizing the performance of a single run. Thus, \system aims to increase the peak throughput when dealing with continuous requests of proof generation from users, e.g., in the outsourcing platform. 

\paragraph{CPU-based optimization} \system focuses on optimizing the performance of a CPU-based  {\zk} implementation. The works that leverage GPUs to enhance \zk systems \cite{chen2017big,dai2016cuhe,goey2021accelerating,kim2020accelerating},  are orthogonal to our work and \system still can adapt to these works after applying some necessary modifications. 

\paragraph{Public variables} The intermediate computed results of circuits
can be safely exposed as public variables. It means that the execution outcome itself may be revealed to the public.  \system suits for scenarios where the primary concern is to protect the private inputs. 
For example, in most ZK-Rollup implementations, the focus is on leveraging the succinctness of SNARKs to verify off-chain computations rather than on ensuring zero-knowledge property.

%Even in scenarios where zero-knowledge properties are required, such as when protecting user privacy, the primary concern is to keep the private inputs (e.g., user transaction details) confidential. The intermediate computation results are often not sensitive and can be safely exposed without compromising privacy.



\subsection{System Architecture}



\Cref{fig:overview} depicts the system architecture of \system, encompassing three core components: \emph{Compiler}, \emph{Partitioner}, and \emph{Scheduler}, which collectively facilitate the generation of \zk proofs.

%These components collectively facilitate the generation of ZK-SNARK proofs by breaking down the process into distinct, manageable tasks.

\paragraph{Compiler}  
The compiler is responsible for compiling the ZKP circuits, converted from the original programs written in high-level languages, into constraint systems, such as R1CS \cite{parno2016pinocchio}, Plonkish \cite{gabizon2019plonk} and AIR \cite{ben2019scalable}. \system is irrespective of the concrete implementation of the compiler. A variety of existing tools, e.g., Circom \cite{circom}, Gnark \cite{gnark}, and Libsnark \cite{libsnark},  can play the role of the compiler in \system. 

\paragraph{Partitioner}  
The partitioner divides each large \zk circuit into multiple subcircuits. Executing these smaller sub-circuits sequentially allows resource-limited machines to execute the large-scale circuit. Moreover, each subcircuit is a basic execution unit scheduled by the scheduler. \system, inherently,  models a circuit as a Directed Acyclic Graph (DAG) and transforms circuit division into a DAG partition problem with two goals.  First, the partition must be serializable, meaning the results of executing the subcircuits in a topological order are equivalent to the original full circuit. Second, the load of each circuit, in terms of the number of nodes and across-partition dependencies, should be approximately equal, ensuring balanced workloads. We refer to this process as a \emph{balanced serializable circuit partition} in this work, completed via a greedy algorithm based on topological sorting.


\paragraph{Scheduler}  
The scheduler coordinates the execution of the partitioned subcircuits, ensuring they follow the correct order in accordance with the circuit’s dependencies. With continuous proof generation requests submitted to \system, the scheduler decouples the witness generation and proof computation for each subcircuit from different requests and applies the pipeline design to process execution across them. Ideally, this will increase the parallelism between processes of different requests.
However, the costs for witness generation and proof computation are not balanced, offsetting the benefits brought by the pipeline. To address this issue,  \system enables the capability of both configurable by a scalable framework. In this scenario, we can dynamically assign computational resources to both phases, ensuring their latencies are close to maximize the pipeline's potential. 




We will elaborate on the designs of the partitioner and scheduler in the following two sections. 


%Once obtaining sub-DAGs from the Partitioner, the Scheduler reconstructs the constraint systems for each sub-circuit and generates an execution DAG, \qi{is each node in such DAG a sub-circuit?}
%which dictates the sequence in which sub-circuits are processed. 


%Subsequently, the Scheduler establishes a \emph{Configurable Pipeline} that decouples the \emph{witness generation} and \emph{proof computation} phases. This parallelism optimizes resource usage and minimizes idle time, particularly in scenarios where witness generation typically underutilizes resources. With adjustable parallelism, the Scheduler allows flexible control over how many subcircuits are processed simultaneously, adapting to different hardware environments and sub-circuit complexity. 


%We consider a system designed to handle continuous requests for generating \zk proofs, where the internal computational results are not sensitive in terms of zero-knowledge requirements.
%The primary motivation for partitioning is to address the significant memory usage during the proof generation process, which becomes more critical when introducing pipelining for parallel execution. By splitting the circuit into smaller, serializable components, we can better manage memory consumption and prepare the system for the subsequent scheduling and execution phases handled by the Scheduler.


% todo 这里考虑补上一段说明源源不断的证明流

% This approach increases system throughput and ensures scalability in environments where large volumes of ZKP requests need to be continuously processed.


% 因为有一个地方需要解释为什么要减少public input的overlap，所以这里单独起一节作为上面parititioner的补充，同样下面的Config Pipeline也是一样，但要说的内容不多
%因为已经在system model说过不考虑计算结果暴露了，所以这边就说这些电路是用public input连接的就行，但要说明为什么要减少overlap，因为验证成本和内存代价
% todo: Explain the purpose and significance of partition, emphasizing that single machine refers to serial, and public input does not need to be discussed, but overlap should be explained. This way, Section 4.1 will directly provide the definition
% \subsection{Balanced Serializable Circuit Partition}
% After partitioning the circuit’s corresponding DAG into several sub-DAGs, these sub-DAGs often have overlapping nodes. These overlaps represent dependencies between the subcircuits—specifically, the outputs of one subcircuit that are required as inputs for the next. We define these overlapping variables as \textbf{Shared Variables}. These variables are exposed as public outputs in the preceding subcircuit and passed as public inputs to the subsequent subcircuit.  This approach effectively restructures the original circuit logic by making the necessary connections between subcircuits through these shared variables. It is important to note that we enforce acyclic dependencies between subcircuits, as circular dependencies would violate the serializability required in a single-machine environment.

% However, introducing shared variables between subcircuits presents its own set of challenges. Excessive shared variables increase memory usage during the witness generation phase, counteracting the memory reduction achieved by partitioning. Additionally, each shared variable adds complexity to the verification process, especially in blockchain scenarios where public inputs contribute to storage and computation overhead, leading to higher gas fees. Thus, limiting the number of shared variables is essential for ensuring both memory efficiency and reducing verification overhead.

% % todo: Explain the reason and significance of the pipeline, emphasize its configurability, and then explain the combined effect with partitioning
% % 这里解释可配置的原因，以及自适应配置的可能性
% % 并说明pipeline和partition结合的效果

% \subsection{Configurable Pipeline}
% The need for a configurable pipeline arises from the fact that different hardware environments and circuit structures exhibit varying levels of resource utilization and execution times for witness generation and proof computation. In some cases, witness generation may take significantly longer than proof computation, while in others, the two phases may have more balanced runtimes. A static pipeline configuration would be suboptimal, as it would not fully leverage the available resources across different systems and ZKP workloads. Therefore, the configurable pipeline allows for adjustments in the degree of parallelism between the two phases, optimizing resource usage to suit specific hardware setups and circuit characteristics. Exploring adaptive configuration adjustments based on real-time workload monitoring and system performance could be a promising direction, and we leave this as a potential avenue for future work.


% While the pipeline increases overall memory usage, our partitioning approach mitigates this by breaking the circuit into smaller, manageable subcircuits. This allows the system to better control memory consumption while effectively utilizing system resources. By combining partitioning with the configurable pipeline, the system maximizes throughput and ensures it can handle larger ZKP workloads with greater scalability and efficiency.






% \subsection{System Architecture}
% \Cref{fig:overview} shows an overview of our system architecture. Our system is designed with modular components that handle different aspects of the ZKP process. The three main components are the \emph{Compiler}, \emph{Partitioner}, and \emph{Scheduler}.

% \paragraph{Compiler} ZKP circuits are typically derived from programs written in high-level languages, which are compiled into constraint systems (such as R1CS \cite{parno2016pinocchio}, Plonkish \cite{gabizon2019plonk}, AIR \cite{ben2019scalable}, etc.). In our system, we do not delve into the specifics of this compilation process, but treat it as a foundational part of the architecture. This step can be accomplished by a variety of existing frameworks, such as Circom \cite{circom}, Gnark \cite{gnark}, Libsnark \cite{libsnark}, among others. We collectively refer to these frameworks as the \textbf{Compiler}. 

% The primary function of the \textbf{Compiler} is to translate the high-level circuit logic into a constraint system that the \textbf{Partitioner} can process. This transformation is essential, as it breaks down complex logic into constraints that define the relationships between inputs and outputs, ensuring that the circuit is ready for partitioning.

% \paragraph{Partitioner} The \textbf{Partitioner} is responsible for dividing the large constraint system into multiple subcircuits, which can be executed sequentially. The partitioning process ensures that the subcircuits respect the dependencies within the original circuit and that they are optimized for execution in a serializable order. 

% % 这里是重点对public input的讨论，如果这一点被质疑，整个partition全部都有问题

% % 事实上就是中间计算的witness(private input + private output)能否公开一部分的问题，也就是用户是否只想要private input不泄露
% % 但事实上零知识证明本身是不允许witness公开的，所以只能写成一种trade-off.

% An important design decision is the use of \textbf{public inputs/outputs} to pass intermediate variables between subcircuits. While some prior works use private inputs combined with hash functions to ensure variable consistency across subcircuits, this approach introduces significant overhead due to the high cost of hash functions in ZK circuits. As an alternative, we transform some intermediate private results into public results that are passed between subcircuit. Since the transformed public values only reflect non-sensitive intermediate computations, we avoid exposing any sensitive private inputs. Therefore, these shared public outputs serve as connections between subcircuits, ensuring the correctness of the entire circuit execution while keeping the original private inputs hidden from the verifier. This approach effectively reorganizes parts of the circuit, converting certain intermediate values into public inputs to facilitate the sequential execution of subcircuits without revealing privacy data that users are concerned about. We believe this is a reasonable balance between efficient execution and maintaining sensitive data privacy.

% However, the use of public inputs/outputs to pass shared variables introduces a new challenge: managing the number of public inputs. First, during the witness generation phase, all variables, including public variables, must be stored in memory. This can directly conflict with our goal of reducing memory usage through partitioning. A higher number of public inputs increases the memory footprint, which undermines the efficiency gained from partitioning the circuit into subcircuits. Additionally, public inputs/outputs require extra verification by the verifier, as their consistency across subcircuits needs to be checked. Excessive public inputs/outputs increase the verification complexity, especially in blockchain-based ZK-SNARK applications. In these scenarios, every public input/output adds to the computation and storage overhead, leading to increased verification costs and higher gas fees in smart contract execution. Minimizing the number of public inputs/outputs is therefore essential for optimizing both memory usage and verification efficiency.



% \paragraph{Scheduler} The \textbf{Scheduler} is responsible for orchestrating the execution of the partitioned subcircuits. It transforms the partitions into independent subcircuit constraint systems, ensuring they are executed in the correct order based on their dependencies. The \textbf{Scheduler} manages the \textit{pipeline} for witness generation (denoted as \textbf{solve}) and proof computation (denoted as \textbf{prove}), allowing these two phases to run in parallel.

% A key feature of the \textbf{Scheduler} is the user-defined parallelism that allows flexibility in the allocation of resources for witness generation and proof computation. The pipeline processes multiple subcircuits in parallel as long as their dependencies, defined by the execution DAG, are respected. This maximizes resource utilization and ensures that the system can handle large-scale ZKP circuits efficiently.

% Additionally, the \textbf{Scheduler} supports adaptive execution, where the degree of parallelism can be dynamically adjusted based on the characteristics of the subcircuits and available computational resources. This allows for a more efficient use of system resources, especially in scenarios where certain subcircuits are more complex and require more intensive computation.

