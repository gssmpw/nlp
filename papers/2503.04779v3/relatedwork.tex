\section{Related Works}
\textbf{Reasoning Evaluation of LLMs.} Numerous datasets have been curated to evaluate the reasoning capabilities of LLMs across diverse domains, including mathematical~\cite{cobbe2021training,hendrycks2021measuring}, logical~\cite{liu2021logiqa, yang2022language}, and causal reasoning~\cite{jin2024cladder, jin2023can}. Recent work explores code reasoning, evaluating LLMs' ability on program semantic inference.
~\cite{hu2018deep, jain2024livecodebench, chen2024reasoning}. Early studies focus on code summarization~\cite{husain2019codesearchnet, hu2018deep}, capturing high-level understanding rather than fine-grained semantic reasoning. Recent studies examine code reasoning in detail through output prediction~\cite{jain2024livecodebench}, execution trace simulation~\cite{chen2024reasoning}, and invariant inference~\cite{pei2023can}, yet they still address only partial program semantics. In contrast, FormalBench targets formal specification inference, demanding exhaustive reasoning that produces precise, verifiable specifications for every possible execution.

\textbf{Formal Specification Inference.}  Traditional dynamic analysis methods, such as Daikon~\cite{ernst2007daikon}, Houdini~\cite{flanagan2001houdini}, and DIG~\cite{nguyen2014dig}, infer likely invariants from observed behaviors using predefined templates. However, these tools often yield trivial invariants (e.g., nums != null) and struggle with complex functional properties~\cite{ma2024specgen}. Recent work leverages LLMs to address these limitations. Early approaches~\cite{pei2023can, chakraborty2023ranking} fine-tuned LLMs for invariant inference but focused on specific cases, such as loop invariants or unverified likely invariants. Nilizadeh et al.\cite{nilizadeh2021exploring} manually crafted complete program specifications to assess automated repair effectiveness. Building on this, newer methods such as SpecGen\cite{ma2024specgen} and AutoSpec~\cite{wen2024enchanting} automatically generate full formal specifications via iterative refinement and static analysis. However, as discussed in Section~\ref{sec:intro}, their evaluations remain limited, highlighting the need for FormalBench and more comprehensive assessments of LLM effectiveness.