% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
% \usepackage[review]{acl}
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{enumitem}
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{hyperref}
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}
\usepackage[most]{tcolorbox}
\usepackage{listings}


%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{amsthm}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.


\usepackage{tablefootnote}
\usepackage[normalem]{ulem}\usepackage{listings}
\usepackage{xcolor}
\usepackage{colortbl}
%New colors defined below
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
  label=code:sample,
  frame=single,
  backgroundcolor=\color{backcolour}, commentstyle=\color{codegreen},
  keywordstyle=\color{magenta},
  numberstyle=\tiny\color{codegray},
  stringstyle=\color{codepurple},
  basicstyle=\ttfamily\footnotesize,
  breakatwhitespace=false,         
  breaklines=true,                 
  captionpos=t,                    
  keepspaces=true,                 
  numbers=left,                    
  numbersep=5pt,                  
  showspaces=false,                
  showstringspaces=false,
  showtabs=false,                  
  tabsize=1
}

\lstset{
    language=Java,
    basicstyle=\ttfamily\footnotesize,
    keywordstyle=\color{blue},
    commentstyle=\color{red},
    breaklines=true,
    frame=single, 
    numbers=left, 
    numbersep=5pt, 
    numberstyle=\tiny\color{gray},
    showstringspaces=false,
    tabsize=4,
}

% Define a new environment that takes an optional argument for additional styling
% and a mandatory argument for the title.
\newtcolorbox{llmpromptbox}[2][]{%
  colback=blue!5,         % Background color
  colframe=blue!75,        % Frame color
  fonttitle=\bfseries,     % Bold title
  title={#2},             % Title provided as mandatory argument
  fontupper=\small,
  sharp corners,          % Use sharp corners (optional)
  #1                      % Additional styling can be passed in the optional argument
}


%"mystyle" code listing set
\lstset{style=mystyle}

% \title{FormalBench: A Comprehensive Evaluation of Large Language Models for Synthesizing Formal Program Specifications}
\title{\textit{Can LLMs Reason About Program Semantics?}\par A Comprehensive Evaluation of LLMs on Formal Specification Inference}

% Author information can be set in various styles:
% For several authors from the same institution:
\author{Thanh Le-Cong \and Bach Le \and Toby Murray \\
        School of Computing and Information Systems \\
        The University of Melbourne \\
        \texttt{\{t.lecong, bach.le, toby.murray\}@unimelb.edu.au}}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

% \author{Thanh Le-Cong \\
%   The Unversity of Melbourne \\  \texttt{tlecong@student.unimelb.edu.au} \\\And
%   Bach Le \\
%   The Unversity of Melbourne \\
%   \texttt{bach.le@unimelb.edu.au} \\\And
%   Toby Murray \\
%   The Unversity of Melbourne \\
%   \texttt{toby.murray@unimelb.edu.au}}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}
\newboolean{showcomments}
\setboolean{showcomments}{true}
\ifthenelse{\boolean{showcomments}}
{ \newcommand{\dnote}[2]{\textcolor{red}{
			\fbox{\bfseries\sffamily\scriptsize#1}
			{\small$\blacktriangleright$\textsf{\emph{#2}}$\blacktriangleleft$}}}}
            
\newcommand{\bachle}[1]{\dnote{Bach}{#1}}
\newcommand{\thanh}[1]{\dnote{Thanh}{#1}}

\begin{document}
\maketitle

\begin{abstract}
Large Language Models (LLMs) are increasingly being used to automate programming tasks.
Yet, LLMs' capabilities in reasoning about program semantics are still inadequately studied, leaving significant potential for further exploration.
This paper introduces FormalBench, a comprehensive benchmark designed to evaluate LLMs' reasoning abilities on program semantics, particularly via the task of synthesizing formal program specifications to assist verifying program correctness. This task requires both comprehensive reasoning over all possible program executions and the generation of precise, syntactically correct expressions that adhere to formal syntax and semantics. Using this benchmark, we evaluated the ability of LLMs in synthesizing consistent and complete specifications. Our findings show that LLMs perform well with simple control flows but struggle with more complex structures, especially loops, even with advanced prompting. Additionally, LLMs exhibit limited robustness against semantic-preserving transformations. We also highlight common failure patterns and design self-repair prompts, improving success rates by 25\%.
% FormalBench is packaged as a Pip library and will be released upon publication. An early access version can be found at \href{link}{https://github.com/thanhlecongg/FormalBench/}.
FormalBench is packaged as a Pip library and released at \href{link}{https://github.com/thanhlecongg/FormalBench/}.
\end{abstract}

\section{Introduction}~\label{sec:intro}

Recent advances in Large Language Models (LLMs) have demonstrated significant potential for code understanding and generation~\cite{hou2024large, chen2021evaluating}. However, as adoption grows, critical concerns emerge about their reliability in programming tasks, particularly their capacity to reason about program semantics~\cite{liu2024your, yang2024robustness, liu2024refining}. A fundamental question remains: \textit{Can LLMs reason about program semantics?} 
Pioneering studies~\cite{pei2023can, chen2024reasoning} have tackled this challenge by evaluating LLMs on \textit{partial} semantic properties, such as predicting execution traces or inferring likely program invariants. Although these efforts provide valuable insights, they examine narrow aspects of program behavior rather than a comprehensive semantic understanding. For example, execution-based evaluations~\cite{chen2024reasoning, jain2024livecodebench} are limited to specific execution paths and inputs, offering an incomplete view of LLM's semantic reasoning.


\begin{figure}[t]
  \centering
  % Set the above/below skips of the listing to 0pt to reduce extra space
  \begin{lstlisting}[aboveskip=0pt, belowskip=0pt, basicstyle=\scriptsize\ttfamily]
//@ requires num >= 0 && t >= 0;
//@ requires num + 2*t <= Integer.MAX_VALUE;
//@ requires num + 2*t >= Integer.MIN_VALUE;
//@ ensures \result == num + 2*t;
public int theMaximumAchievableX(int num, int t) {
  int res = num;
  //@ maintaining res == num + 2*(i-1);
  //@ maintaining i >= 1 && i <= t+1;
  //@ decreasing t-i+1;
  for(int i = 1; i <= t; i++) {
    res = res + 2;
  }
  return res;
}
  \end{lstlisting}
  \caption{Illustration of a Java program annotated with JML specifications (highlighted in \color{green!50!black}{green}).}
  \label{fig:example}
  \vspace{-5mm}
\end{figure}

% \begin{figure}[t]
%   \centering
%   % Set the above/below skips of the listing to 0pt to reduce extra space
%   \begin{lstlisting}[aboveskip=0pt, belowskip=0pt, basicstyle=\scriptsize\ttfamily]
% public int theMaximumAchievableX(int num, int t) {
%   int res = num;
%   for(int i = 1; i <= t; i++) {
%     res = res + 2;
%   }
%   return res;
% }
%   \end{lstlisting}
%   \caption{Illustration of a Java program annotated with JML specifications (highlighted in \color{green!50!black}{green}).}
%   \label{fig:example}
%   \vspace{-5mm}
% \end{figure}


Recent studies~\cite{wen2024enchanting, ma2024specgen} have evaluated LLMs in the synthesis of program specifications expressed in formal languages such as JML~\cite{leavens2006preliminary} and ACSL~\cite{baudin2008acsl}. The synthesized specifications can then be used to assist in automated software verification~\cite{d2008survey} and bug finding~\cite{le2022finding}. This task challenges LLMs to (1) reason exhaustively over all possible program executions and (2) generate logically precise expressions that comply with the formal syntax and semantics of the specification language.

While initial results are promising, current evaluation methodologies for LLM reasoning through formal specification inference face three key limitations. 
First, the evaluation datasets are \textit{small and lack diversity}. For example, SpecGenBench~\cite{ma2024specgen} and the Frama-C problems~\cite{kirchner2015frama} contain only 120 and 57 programs, respectively. 
Second, evaluation metrics focus \textit{narrowly on consistency}, i.e., alignment between specifications and programs, while neglecting completeness, i.e., coverage of all semantic behaviors. Completeness is particularly important for assessing LLM's ability to reason comprehensively about \textit{complete} program behaviors.
Finally, current studies primarily aim to develop new LLM-based techniques for formal specification inference rather than \textit{evaluating the LLM reasoning capabilities} themselves. Consequently, evaluations are often ad hoc, relying on specific prompting techniques or LLMs, leading to a lack of comprehensive insights across a wide range of models and prompts.

To address the above challenges, we introduce FormalBench, a comprehensive benchmark for evaluating the reasoning capabilities of LLMs through formal specification inference. FormalBench improves the existing dataset with two notable features: (1) a large-scale dataset of 700 manually validated Java programs and 6,219 augmented programs, covering various control flow structures, and (2) a Python library with a robust suite of evaluation metrics to measure both consistency (via deductive verification) and completeness (through mutation analysis). We then leverage FormalBench to conduct a comprehensive study evaluating eight state-of-the-art LLMs across four critical dimensions:  (1) their effectiveness in synthesizing complete and consistent specifications, (2) robustness against semantic-preserving code transformations, (3) impact of advanced prompting techniques, and (4) the root causes of failures and their self-repair ability. 
 
 Our findings reveal several key insights. LLMs demonstrate limited effectiveness, achieving only about 10\% verification success with over 50\% failures, particularly struggling with complex control-flow structures such as nested loops. Advanced prompting techniques, such as few-shot and least-to-most prompting, improve success rates to 16.6\% and reduce failures, yet overall performance remains suboptimal. Robustness issues also arise, with LLMs exhibiting flip rates between 27.2\% and 39.2\% under semantic-preserving transformations, negatively impacting their performance. Common failures of LLMs include syntax errors, flawed inductive reasoning, incorrect postconditions, faulty loop invariants, and misjudged arithmetic bounds. However, error-specific prompts enhance LLM self-repair capabilities, improving verifiable specifications by approximately 25\% and reducing failures by around 40\%, although these improvements converged after a few iterations.


In summary, our main contributions include:
\begin{itemize}[noitemsep, left=1pt]
    \item We introduce FormalBench, a comprehensive dataset specifically designed for evaluating formal reasoning of LLMs about program semantics.
    \item We propose a robust set of evaluation metrics to assess the effectiveness and robustness of LLMs on synthesizing consistent and complete formal specifications.
    \item We conduct an extensive empirical study of popular LLMs using FormalBench, highlighting their limitations. We also identify common failure patterns and design customized prompts to assist LLMs in self-repairing these failures.
    \item We advance research in formal specification inference by releasing FormalBench as an installable Python library under Apache 2.0 License, lowering barriers for academic research and establishing foundational benchmarks and metrics for future work.
\end{itemize}

\section{Problem Statement}~\label{sec:statement}
Given an input program, the formal specification inference task is to annotate the program with a set of formal specifications, i.e., Boolean expressions written in a formal specification language.
A good formal specification should be adequate, consistent, unambiguous, complete, satisfied, and minimal~\cite{lamsweerde2000formal}. In this work, we particularly focus on two key properties of specifications, including completeness and consistency, which present the correctness of generated specifications. Inspired by ~\cite{lamsweerde2000formal}, we define these properties in our problem as follows:

\begin{definition}
(Consistency) A formal specification is considered \textbf{consistent} to a given input program if all specified properties are well-formed and true with respect to that program.
\end{definition}


\begin{definition}
(Completeness) A formal specification is considered \textbf{complete} if all function properties that hold with respect to a given input program are specified in the specification.
\end{definition}

To determine the consistency of LLM-generated specifications, i.e., specifications that hold for an input program, we use deductive verification tools that transform the annotated program into logical proof obligations and verify them with theorem provers. These tools ensure software correctness by systematically analyzing all possible execution paths, making our consistency checking reliable.  

Measuring the completeness of formal specifications is inherently challenging because of the complex behaviors exhibited by software programs. 
Inspired by the success of mutation testing~\cite{andrews2005mutation} in evaluating the completeness of test suites, we propose to use mutation analysis as a proxy to assess the completeness of formal specifications. Mutation analysis generates non-equivalent mutant variants of input programs by introducing artificial faults~\cite{andrews2005mutation}. Ideally, a complete specification should be able to detect all such faults. Therefore, we measure the proportion of mutants that violate the specification as a proxy of its completeness.

% We acknowledge that determining the consistency and completeness of program specifications is, in general, undecidable. We discuss results with regard to undecidability via our experiments in Section 4.
% Existing studies on code reasoning tasks, such as code summarization~\cite{hu2018deep} and invariant prediction~\cite{pei2023can}, typically assess the effectiveness of LLM by comparing generated outputs with the human-written ground truth. To do so, they frequently rely on syntax-based similarity metrics, such as BLEU or Jaccard distances. However, these methods have two notable limitations: (1) they are inadequate for capturing the semantic similarity between generated outputs and ground truth, and (2) they do not soundly confirm that the generated outputs are semantically consistent with the corresponding input programs.



\section{Dataset Construction and Evaluation}


\subsection{FormalBench Construction}~\label{sec:benchmark}

% FormalBench is a new benchmark dataset for formal specification inference, designed to address the limitations of existing benchmarks. It consists of 6,919 Java programs that have been rigorously validated for correctness by the authors. These programs encompass a diverse range of control flow structures, including sequential execution, branching, single-path loops, multi-path loops, and nested loops, ensuring a comprehensive evaluation framework for specification inference tasks.

% Existing benchmarks for evaluating LLM-based specification inference are often limited in size. For example, SV-Comp~\cite{beyer2023competition}, SpecGenBench~\cite{ma2024specgen} and the Frama-C problem~\cite{kirchner2015frama} contain only 285, 120 and 57 programs, respectively. Additionally, these benchmarks are also susceptible to data contamination. For example, the Frama-C problem consists of manually written specifications and was published in 2015. Similarly, SpecGenBench includes 20 manually-written specifications from Nilizadeh et al.\cite{nilizadeh2021exploring}, published in 2021.

% To address these issues, we introduce FormalBench, a new benchmark dataset for formal specification inference. 

FormalBench is constructed in three phases to ensure its reliability and diversity: (1) curating reference Java programs paired with natural language descriptions, (2) manually verifying program correctness with respect to natural language descriptions to establish FormalBench-Base, which comprises 700 programs, and (3) augmenting FormalBench-Base using semantic-preserving transformations to create FormalBench-Diverse, which comprises 6,219 programs.

Specifically, we begin with an initial pool of 966 Java programs generated by the MBXP model~\cite{athiwaratkun2022multi} for the MBJP benchmark. This dataset is released under the Apache License 2.0, which permits modification and redistribution.
To filter incorrect candidates, we execute these programs against the MBJP test suite, retaining 824 programs that pass all the provided test cases. However, as MBJP’s test suite is generally weak, we conduct manual validation to ensure alignment between program behavior and natural-language intents. This involves a multi-step review process: we carefully inspect each program, augment the test suite with adversarial inputs, and validate correctness against the specified intents. 

The result is FormalBench-Base, a rigorously validated dataset of 700 programs with provably correct implementations. To ensure diversity, FormalBench-Base covers a wide range of control flow types, including sequential, branching, single-path loops, multi-path loops, and nested loop structures, as illustrated in Figure~\ref{fig:data_category}.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.7\columnwidth]{img/category.png}
  \caption{Distribution of our datasets and SpecGenBench over different control flow types.}
  \label{fig:data_category}
  \vspace{-5mm}
\end{figure}

Finally, we apply 18 semantic preservation transformations from the literature~\cite{lecong2024reliableevaluationneuralprogram, rabin2021generalizability, zhang2023challenging} to FormalBench-Base, generating FormalBench-Diverse, a dataset of 6,219 program variants designed to evaluate the robustness of LLMs against syntactic variations. These transformations, detailed in Appendix~\ref{appx:transformations}, span multiple levels of code structure, including naming (e.g., variable renaming), expression (e.g., switching equal expressions) and statement (e.g., transforming switch statements to if-statements). 

\subsection{Evaluation Metrics}~\label{sec:metrics}

\textbf{Consistency Metrics.} As mentioned in Section~\ref{sec:statement}, we utilize deductive verification tools to determine the consistency of LLM-generated specifications. 
The verification process produces three outcomes: 
(1) \textit{Verification Success}, where the implementation satisfies the specification; 
(2) \textit{Verification Failure}, where the implementation violates the specification; and 
(3) \textit{Unknown}, where the tool cannot definitively determine the result (e.g. due to timeouts or undecidability). 
While prior works~\cite{wen2024enchanting, ma2024specgen} often merge \textit{unknown} cases with failures, we observed that specifications that lead to timeouts are qualitatively distinct from those that cause failures. Thus, we treat \textit{Unknown} as a separate category. Based on these categories, we define two consistency metrics as follows: (1) \textit{Success Rate (SR)}, the proportion of specifications that pass verification; and (2) \textit{Failure Rate (FR)}, the proportion of specifications that fail verification.

\textbf{Completeness Metrics.} 
To evaluate the completeness of a specification, we first apply mutation testing~\cite{andrews2005mutation} to generate a set of mutants, i.e., non-equivalent variants of the input programs created by deliberately injecting artificial faults. We then compute the fraction of these mutants that fail to satisfy the specification. This fraction is defined as the Completeness Rate (CR) of the specification.

\textbf{Robustness Metrics.} 
To evaluate the robustness of LLMs in synthesizing specifications under semantic-preserving transformations, we measure the \textit{Flip Rate (FlR)}, which captures cases where LLMs generate verifiable specifications for the original program $p$ but fail for its transformed versions. Moreover, we also measure the impact of unrobust behaviors on the performance of LLMs by measuring the success rate and failure rates on \textit{FormalBench-Diverse} (Section~\ref{sec:benchmark}). Since transformations may not apply universally, we normalize metrics over applicable transformations.

\textbf{Details.} We use OpenJML version 21.0 as our deductive verification tool and Major 3.0.1 as our mutation analysis tool. Full implementation details and formal formulations of these evaluation metrics are provided in Appendix~\ref{appx:metrics}. 

\section{Experiments}

In this section, we present our empirical results on LLMs using FormalBench, guided by the following research questions:

\begin{itemize}[noitemsep, topsep=0pt, left=0pt]
    \item \textbf{RQ}$_1$: \textit{How effective are LLMs in synthesizing formal specifications?}
    \item \textbf{RQ}$_2$: \textit{Can advanced prompting techniques improve the effectiveness of LLM?}
    \item \textbf{RQ}$_3$: \textit{How robust are LLMs in synthesizing formal specifications?}
    \item \textbf{RQ}$_4$: \textit{What are the common mistakes made by LLMs, and can they self-repair these errors?} 
\end{itemize}

Following prior works~\cite{ma2024specgen, flanagan2001houdini}, we focus on Java and its specification language, JML~\cite{leavens2006preliminary}, using OpenJML~\cite{cok2011openjml} as the verifier. LLMs are evaluated using their official chat templates and the same query prompts, as detailed in Appendix~\ref{appx:prompts}. To ensure fairness, we use sampling with a temperature setting of 0.7 across all LLMs. For open-source LLMs, the maximum number of tokens generated is limited to 2048 due to GPU constraints. A full description of the experimental setup is provided in Appendix~\ref{appx:exp_settings}.

\textbf{Cost Analysis.}Our experiments for closed-source LLMs cost approximately 250 USD, while those for open-source LLMs required around 100 GPU hours.

\begin{table*}[t]
  \centering
  \resizebox{0.75\textwidth}{!}{
  \begin{tabular}{llccc}
    \hline
    \textbf{}         & \textbf{Models}                 & \textbf{Success Rate (\%)} & \textbf{Failure Rate (\%)} & \textbf{Completeness (\%)} \\
    \midrule
    \multirow{12}{*}{\rotatebox[origin=c]{90}{\textbf{Open-Source LLMs}}} 
    & CodeQwen-1.5-7B         & 1.1   & 97.4  & 79.1  \\
    & + Few-shot prompt          & 3.9   & 85.6  & 74.1  \\
    \cline{2-5}
    & CodeQwen-2.5-32B          & 7.6   & 77.1  & 83.3  \\
    & + Few-shot prompt           & 11.4  & 66.8  & 82.1  \\
    & + COT                      & 9.2   & 69.4  & 86.8  \\
    & + LTM                      & 12.0  & 68.2  & 89.1  \\
    \cline{2-5}
    & DeepSeek-V2-236B          & 2.7   & 88.8  & 77.0  \\
    & + Few-shot prompt            & 6.9   & 78.4  & 78.4  \\
    & + COT                      & 7.9   & 76.4  & 77.1  \\
    & + LTM                      & 8.9   & 70.4  & 81.6  \\
    \cline{2-5}
    & CodeLLaM-34B            & 0.1   & 99.6  & 100.0 \\
    & + Few-shot prompt             & 5.3   & 82.7  & 60.3  \\
    \midrule
    \multirow{12}{*}{\rotatebox[origin=c]{90}{\textbf{Proprietary LLMs}}} 
    & DeepSeek-V3-671B           & 8.4   & 65.2  & 89.6  \\
    & + Few-shot prompt            & 15.5  & 56.2  & 85.2  \\
    & + COT                      & 16.2  & 55.9  & 85.3  \\
    & + LTM                      & 16.6  & 56.8  & 89.6  \\
    \cline{2-5}
    & GPT-3.5               & 6.9   & 75.8  & 62.3  \\
    & + Few-shot prompt              & 12.6  & 59.8  & 59.0  \\
    \cline{2-5}
    & o3-mini               & 10.0   & 66.2  & 83.5  \\
    & + Few-shot prompt              & 11.7  & 59.7  & 88.7  \\
    \cline{2-5}
    & GPT-4o               & 11.2  & 56.4  & 80.4  \\
    & + Few-shot prompt                & 13.4  & 56.4  & 77.6  \\
    & + COT                      & 13.4  & 61.5  & 81.2  \\
    & + LTM                      & 15.0  & 57.7  & 86.4  \\
    \cline{2-5}
    & Claude-3.5-Sonnet     & 10.5  & 64.5  & 91.2  \\
    & + Few-shot prompt   & 14.7  & 53.1  & 82.6  \\
    & + COT                      & 14.2  & 59.1  & 83.6  \\
    & + LTM                      & 15.4  & 51.1  & 86.4  \\
    \bottomrule
  \end{tabular}
  }
  \caption{\label{tab:rq1}
    Performance comparison of Open-Source and Commercial LLMs under zero-shot, in-context learning with few-shot prompt, chain-of-thought (COT), and least-to-most (LTM) prompting settings.
  }
\end{table*}

\subsection{\textbf{RQ}$_1$: Effectiveness of LLMs}

To answer the RQ$_1$, we evaluate the effectiveness of LLMs in synthesizing complete and consistent specifications by measuring their success rates, failure rates, and completeness rates. We assess eight popular open-source and proprietary LLMs on FormalBench-Base, as detailed in Appendix~\ref{appx:llms}. 
Detailed experimental results are presented in Table ~\ref{tab:rq1}.

\textbf{LLMs with zero-shot prompts.} From the results in Table~\ref{tab:rq1}, we observe that LLMs with zero-shot prompts perform poorly in synthesizing formal specifications, achieving a success rate of around 10\% and failure rates ranging from 56.4\% to 99.6\%. Most open source LLMs, except CodeQwen-2.5, exhibit particularly poor performance, with success rates below 3\%. Upon closer analysis, we found that this poor performance of open-source LLMs is due to a lack of familiarity with the JML syntax, resulting in a significant number of invalid responses, up to 77\%. For example, these models often generate natural language descriptions instead of formal JML specifications, highlighting their inability to produce follow formal grammar of JML without explicit guidance. 

\textbf{LLMs with few-shot prompts.}  To address these limitations, we enhanced the ability of LLMs to generate formal specifications by incorporating additional instructions on JML syntax and providing two demonstration examples in the few-shot prompts, as shown in the Appendix~\ref{appx:prompts}. From the results in Table~\ref{tab:rq1}, we can see that this approach significantly improves LLM performance, with increases of up to 7.1 percentage points in success rates and reductions of up to 16.9 percentage points in failure rates. Furthermore, we observe a slight decline in completeness, although the overall completeness of the generated specifications remains high. This suggests a reasonable trade-off between correctness and consistency.
However, despite these improvements, the success rates remain relatively low at less than 16\%, highlighting the inherent challenge of synthesizing formal specifications for LLMs.

\textbf{Open-source vs. Proprietary LLMs.} Additionally, we observe that proprietary LLMs such as DeepSeek-V3 and GPT-4o are significantly more effective than open-source LLMs. However, the best-performing open-source LLM, CodeQwen-2.5, shows considerable promise with a success rate of 12.0\%, only 3 percentage points lower than GPT-4o. This performance is particularly impressive given CodeQwen-2.5's compact size of 32B parameters. These findings suggest that open-source LLMs still have significant potential for further advancements in this domain due to their flexibility and cost-effectiveness compared to proprietary LLMs.

\textbf{Distribution over different Control-flow types.} Finally, we analyze verification success and failure distributions across different control flow types (see detailed visualizations in Appendix~\ref{appx:distribution}).
From this analysis, we observe that LLMs are primarily capable of generating verifiable specifications for programs with simple control-flow structures, such as sequential or branched programs. 
However, they often struggle to synthesize formal specifications for programs that contain loops, where the complexity of control flow increases significantly, with a success rate of less than 10\% and failure rates of more than 50\%.
These findings highlight the limitations of LLMs in reasoning about complex control-flow structures, particularly loops, which require more advanced logical and inductive reasoning capabilities.

\subsection{\textbf{RQ}$_2$: Impact of Advanced Prompting Techniques}

To answer the RQ$_2$, we evaluate the top LLMs from RQ$_1$ (CodeQwen-2.5, DeepSeek-V2, DeepSeek-V3, GPT-4, and Claude 3.5 Sonnet) using two prompting techniques: chain-of-thought (CoT)\cite{kojima2022large} and least-to-most (LTM)\cite{zhou2022least}. Detailed prompt designs are in Appendix~\ref{appx:prompts}.

\textbf{Least-to-Most Prompts.} As shown in Table~\ref{tab:rq1}, LTM consistently improves the effectiveness of these LLMs, improving both consistency and completeness metrics. For example, the success rate of DeepSeek-V3 increases from 15.5\% with few-shot prompts to 16. 6\% with LTM (a 7\% improvement), while the completeness rate increases from 85. 2\% to 89. 6\% (a 5\% improvement). These improvements are observed not only in proprietary LLMs, but also in open-source LLMs. Specifically, the success rates of CodeQwen-2.5 and DeepSeek-V2 improve significantly, from 11.4\% and 6.9\% to 12.0\% and 8.9\%, respectively. Overall, these findings suggest that LTM prompting, when combined with few-shot demonstrations, should be used to optimize the effectiveness of LLMs in synthesizing program specifications.

\textbf{Chain-of-Thought Prompts.} In contrast, the impact of CoT on LLMs is mixed, with both positive and negative outcomes. For example, CoT improves the success rate of DeepSeek-V3 from 15.5\% to 16.2\%. However, it has no effect on the success rate of GPT-4o and even decreases the performance of the Claude 3.5 Sonnet. CoT even significantly increases the failure rates of GPT-4o and Claude-3.5-Sonnet from 56.4\% and 53.1\% to 61.5\% and 59.1\%.
We suspect that this is because CoT relies on the model's ability to self-reason, while LTM provides human-instructed reasoning steps and explicit demonstrations, which guide the models toward better reasoning.


\begin{table*}[t]
  \centering
  \resizebox{0.75\textwidth}{!}{
  \begin{tabular}{lccccc}
    \hline
    \textbf{Model Name} & \multicolumn{2}{c}{\textbf{FormalBench-Base}} & \multicolumn{2}{c}{\textbf{FormalBench-Diverse-N}} & \textbf{Flip Rate (\%)} \\
    \cline{2-5}
    & \textbf{SR (\%)} & \textbf{FR (\%)} & \textbf{SR (\%)} & \textbf{FR (\%)} & \\
    \midrule
    DeepSeek-V3 & 9.3 & 62.1 & 7.8 & 65.0 & 27.2 \\
    Claude-3.5-Sonnet & 10.0 & 64.1 & 8.3 & 64.3 & 39.20 \\
    GPT-4o & 11.9 & 60.1 & 10.9 & 64.1 & 29.2 \\
    \bottomrule
  \end{tabular}
  }
  \caption{\label{tab:rq3}
    Robustness evaluation of LLMs on FormalBench-Base and FormalBench-Diverse-N benchmarks using different metrics Success Rate (SR), Failure Rate (FR), and Flip Rate.
  }
\end{table*}

\textbf{Effectiveness on Complex Control-Flow Programs.} While LTM prompting can further improve the performance of LLMs, these improvements are primarily observed in reasoning programs involving branching and sequential control flow. In contrast, the impact of LTM prompting on improving reasoning for programs with complex control flow, such as those containing loops, remains unclear. As a result, the performance of LLMs in loop-containing programs remains low, with success rates of less than 10\%. This further underscores the limitations of LLMs in reasoning about programs with loops, which require more advanced inductive reasoning capabilities.

\subsection{\textbf{RQ}$_3$: Robustness of LLMs}

To answer the RQ$_{3}$, we assess LLM robustness by evaluating their performance on semantically equivalent but syntactically diverse programs using FormalBench-Diverse-N, a subset of 1,794 \textit{natural} program transformations~\cite{lecong2024reliableevaluationneuralprogram} from FormalBench-Diverse (see Appendix~\ref{appx:diversenatural}). Due to resource constraints, we focus on the top three LLMs: GPT-4, Claude 3.5 Sonnet, and DeepSeek-V3.

Our experimental results, presented in Table~\ref{tab:rq3}, reveal significant robustness challenges for all evaluated LLMs. Specifically, we observe flip rates, the proportion of semantically equivalent programs for which the model does not generate verifiable specifications, ranging from 27. 2\% to 39. 2\%. Among the models, Claude-3.5-Sonnet is the most severely impacted, with a flip rate of 39.2\%, indicating that it does not generate verifiable specifications for nearly 40\% of the transformations when the original programs had verifiable specifications.

More critically, this lack of robustness leads to a notable decrease in success rates and an increase in failure rates. For instance, the success rate of DeepSeek-V3 drops from 9.3\% to 7.8\%, a 16\% reduction, while GPT-4o and Claude-3.5-Sonnet experience reductions of 9.5\% and 17\%, respectively. Similarly, failure rates increase by up to 6.6\%, further underscoring the sensitivity of LLMs to the syntactic variation created by semantic-preserving transformations.

\begin{figure*}[t]
    \centering
    % First subfigure
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/failure_distribution_zero-shot_models.png}
        \caption{LLMs with Zero-shot prompts}
    \end{subfigure}
    \hfill
    % Second subfigure
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/failure_distribution_few-shot_models.png}
        \caption{LLMs with Few-shot prompts}
    \end{subfigure}
    \hfill
    % Third subfigure
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/failure_distribution_ltm_models.png}
        \caption{LLMs with LTM prompts}
    \end{subfigure}
    \caption{Top-10 failure category of LLMs with various prompts}
    \label{fig:failure_category}
\end{figure*}

These findings highlight the limited robustness of LLMs against semantic-preserving transformations, which expose a critical dependence on syntactic patterns rather than underlying semantic properties. This indicates that current LLMs still lack the deep semantic reasoning capabilities necessary to generalize across functionally equivalent but syntactically varied programs.  


\subsection{\textbf{RQ}$_4$: Common Failures and Self-Repair Ability of LLMs}

To answer the RQ$_4$, we begin by conducting a semi-automated analysis, as outlined in Appendix~\ref{appx:failure_analysis}, to categorize the failures of LLMs. For each type of failure, we sample a subset of instances and investigate their root causes. Building on these insights, we design customized prompts that include failure descriptions, additional guidance, and illustrative examples to enable LLMs to self-repair these errors. Additional details on repair prompts are provided in the Appendix~\ref{appx:repair_prompts}.

\subsubsection{Common Failures}

In total, we identified 32 failures of LLMs based on their error messages.  Figure~\ref{fig:failure_category} illustrates the 10 most common failure categories encountered across llm when using zero-shot, few-shot, and least-to-most (LTM) prompts. 

\begin{figure*}[t]
    \centering
    % First subfigure
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/Claude35_Sonnet_performance.png}
        \caption{Claude-3.5-Sonnet}
    \end{subfigure}
    \hfill
    % Second subfigure
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/GPT4o_performance.png}
        \caption{GPT-4o}
    \end{subfigure}
    \hfill
    % Third subfigure
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/DeepSeekV3_performance.png}
        \caption{DeepSeekV3}
    \end{subfigure}
    \caption{Effectiveness and self-repair rates of LLMs across iterations: ``Iter $i$'' represents self-repair with feedback, while   ``mutation'' represents results from mutation-based repair in the final iteration.}
    \label{fig:repair}
\end{figure*}

\textbf{Syntax Errors.} Among failure types, ``SyntaxError'' is the most frequent, and LLMs often generate specifications that violate the JML or Java syntax. This issue persists in most models. A common example is the error message “Unexpected or misspelled JML token,” which occurs when an LLM produces incorrect JML grammar. This highlights the challenge of expressing implicit program intent in formal languages, a key distinction from natural language specifications such as code comments. More critically, LLMs with zero-shot prompts yield about 25\% invalid responses (e.g., producing Javadoc comments instead of JML). Fortunately, this rate drops to 5\% with few-shot prompts and further to 1\% with LTM prompts.

\textbf{Reasoning Errors.} LLMs often encounter reasoning errors, particularly with quantifiers, postconditions, loop invariants, and arithmetic bounds. The most frequent are ``UnsupportedQuantifier'' errors.
In these cases, LLMs rely on inductive quantifiers such as \textbackslash sum or \textbackslash product, which are not supported by deductive verification for reasoning about program behaviors. In practice, formal experts must supplement these quantifiers with auxiliary mathematical functions and lemmas to enable inductive reasoning. For improved formal specification synthesis, LLMs must adopt similar human-like strategies rather than relying solely on unsupported quantifiers.

Following ``UnsupportedQuantifier'', the next most common failure categories, ``PostconditionFailure'', ``LoopInvariantFailure'', and ``ArithmeticOperationRange'', account for nearly 30\% of total failures. These errors occur when the verification tool cannot prove postconditions, loop invariants, or arithmetic bounds (e.g., to prevent overflow). Our analysis identifies three main root causes: (1) incorrect specifications, (2) weak or incorrect preconditions that render specifications unprovable, and (3) incomplete reasoning about program behavior, leaving verifiers with insufficient information. These findings underscore the need for LLMs to enhance their reasoning capabilities for more effective formal specification synthesis.

\subsubsection{Self-Repair}


To evaluate LLM's self-repair ability, we (1) develop a simple failure classifier using pattern matching and (2) design customized prompts with failure descriptions, guidance, and examples (see Appendix~\ref{appx:prompts}). Additionally, we assess SpecGen’s mutation-based repair for verification failures. Due to resource constraints, we evaluate only the top three LLMs: Claude-3.5-Sonnet, GPT-4o, and DeepSeek-V3, and present the results in Figure~\ref{fig:repair}.

The results show that LLMs effectively repair errors using our custom prompts, improving success rates by 25\%, from 16\% to 20\%, and reducing failure rates from over 50\% to under 30\%. Mutation-based repair further increases success by 0.5 percentage points and reduces failure by 1 to 2 percentage points. Additionally, LLMs can also self-repair across various error categories, such as fixing 53.7\% of ``SyntaxErrors'', 79\% of ``LoopInvariantFailures'', and 65\% of ``PostconditionFailures'' in the first iterations. This pattern persists across subsequent iterations, highlighting the flexibility of self-repair. In contrast, mutation-based repair is limited to specific errors, such as not addressing ``ArithmeticOperationRange'' errors. Notably, both methods preserve the completeness of generated specifications, improving the number of verifiable specifications without sacrificing quality. However, we identify two limitations of self-repair approaches. First, self-repair rates decrease with each iteration, leading to saturation in success and failure rates. Second, mutation-based repair is computationally expensive and requires frequent calls to verification tools, so it should be used sparingly, ideally as a final step, to minimize costs.

\section{Related Works}

\textbf{Reasoning Evaluation of LLMs.} Numerous datasets have been curated to evaluate the reasoning capabilities of LLMs across diverse domains, including mathematical~\cite{cobbe2021training,hendrycks2021measuring}, logical~\cite{liu2021logiqa, yang2022language}, and causal reasoning~\cite{jin2024cladder, jin2023can}. Recent work explores code reasoning, evaluating LLMs' ability on program semantic inference.
~\cite{hu2018deep, jain2024livecodebench, chen2024reasoning}. Early studies focus on code summarization~\cite{husain2019codesearchnet, hu2018deep}, capturing high-level understanding rather than fine-grained semantic reasoning. Recent studies examine code reasoning in detail through output prediction~\cite{jain2024livecodebench}, execution trace simulation~\cite{chen2024reasoning}, and invariant inference~\cite{pei2023can}, yet they still address only partial program semantics. In contrast, FormalBench targets formal specification inference, demanding exhaustive reasoning that produces precise, verifiable specifications for every possible execution.

\textbf{Formal Specification Inference.}  Traditional dynamic analysis methods, such as Daikon~\cite{ernst2007daikon}, Houdini~\cite{flanagan2001houdini}, and DIG~\cite{nguyen2014dig}, infer likely invariants from observed behaviors using predefined templates. However, these tools often yield trivial invariants (e.g., nums != null) and struggle with complex functional properties~\cite{ma2024specgen}. Recent work leverages LLMs to address these limitations. Early approaches~\cite{pei2023can, chakraborty2023ranking} fine-tuned LLMs for invariant inference but focused on specific cases, such as loop invariants or unverified likely invariants. Nilizadeh et al.\cite{nilizadeh2021exploring} manually crafted complete program specifications to assess automated repair effectiveness. Building on this, newer methods such as SpecGen\cite{ma2024specgen} and AutoSpec~\cite{wen2024enchanting} automatically generate full formal specifications via iterative refinement and static analysis. However, as discussed in Section~\ref{sec:intro}, their evaluations remain limited, highlighting the need for FormalBench and more comprehensive assessments of LLM effectiveness.

\section{Conclusion}

In this work, we introduce FormalBench, a comprehensive and large-scale benchmark for specification inference. FormalBench integrates robust evaluation metrics to assess the consistency, completeness, and robustness of LLMs on this task. Using FormalBench, we conduct an extensive evaluation of eight popular LLMs, revealing their limited effectiveness and lack of robustness in synthesizing formal specifications, even with advanced prompting techniques. We further analyze their common failure patterns and propose a set of customized prompts, leveraging LLMs' self-repair capabilities to enhance their performance. Overall, FormalBench aims to enable a thorough evaluation and deeper understanding of LLMs in formal program reasoning. The dataset and evaluation infrastructure will be publicly released upon publication.

\section{Limitations.}
The limitations of this work are as follows:

First, mutation analysis, which sits at the core of our completeness metric, relies on predefined rules to systematically break program behavior, assuming that the generated mutants will exhibit semantic differences from the original program, thereby triggering detectable errors. However, the presence of equivalent mutants, i.e., semantically identical variants of the original program, presents a challenge, as they evade detection, leading to false positives and undermining the accuracy of completeness metrics. To mitigate this issue, we incorporate Equivalent Mutant Suppression (EMS)~\cite{kushigian2024equivalent}, a state-of-the-art technique for filtering out equivalent mutants. While EMS reduces their prevalence, some undetected equivalents may still affect the validity of our results. Future research should focus on the development of metrics that more effectively measure the completeness of generated specifications, thereby reducing reliance on mutation analysis as an isolated proxy.

Second, our experiments produced a significant number of "unknown" results from the program verification tools. Manual inspection suggests that these cases are generally associated with higher-quality specifications compared to those with verification failures; yet, they introduce ambiguity due to inherent limitations in deductive verification tools. Future work should prioritize techniques for interpreting or eliminating these ambiguous results, possibly through enhanced symbolic execution or dynamic verification methods.

Finally, our experiments did not include OpenAI-o1 and DeepSeekR1, the latest LLMs at the time of writing. For OpenAI-o1, the associated costs were prohibitively high, so we could not incorporate it into our experiments due to resource constraints. As an alternative, we conducted experiments on o3-mini, the latest reasoning model from OpenAI, with reasonable cost. 
For DeepSeekR1, access was unavailable at the time of writing due to a security breach affecting the service~\footnote{https://www.scmp.com/tech/tech-war/article/3296975/deepseek-outage-adds-growing-pains-amid-political-scrutiny-some-see-opportunity}.

\bibliography{main}

\clearpage

\appendix

\section{Semantic-preserving Transformations}
\label{appx:transformations}

In this study, we curate a set of 18 semantic-preserving transformations from recent studies~\cite{lecong2024reliableevaluationneuralprogram, zhang2023challenging, rabin2021generalizability} including:

\begin{itemize}
    \item \textbf{VariableRenaming-1} replaces a variable name by its first characters;
    \item \textbf{VariableRenaming-1} replaces a variable name by substitutions derived from CodeBERT~\cite{feng2020codebert};
    \item \textbf{SwitchRelation} transforms relational expressions by swapping the operands. For example, the expression \texttt{a < b} is transformed into \texttt{b > a}.
    \item \textbf{Unary2Add} modifies unary operations or increments by converting them into normal assignment statements. For instance, \texttt{i++;} is transformed into \texttt{i = i + 1;}.
    \item \textbf{Add2Equal} converts add/subtract assignments into equal assignments. For example, \texttt{a += 9;} is transformed into \texttt{a = a + 9;}, and \texttt{b -= 10;} is transformed into \texttt{b = b - 10;}.
    \item \textbf{MergeVarDecl} merges multiple variable declarations into a single statement. For instance, \texttt{int a;} and \texttt{int b;} are merged into \texttt{int a, b;}.
    \item \textbf{InfixDividing} divides an in/pre/post-fix expression into two separate expressions, storing intermediate results in a temporary variable. For example, \texttt{x = a + b * c} is transformed into \texttt{temp = b * c;} followed by \texttt{x = a + temp}.
    \item \textbf{SwitchEqualExp} switches the two expressions on both sides of an infix expression where the operator is \texttt{=}. For instance, \texttt{a == b} is transformed into \texttt{b == a}.
    \item \textbf{SwitchStringEqual} switches the order of string equality checks. For example, \texttt{a.equals(b)} is transformed into \texttt{b.equals(a)}.
    \item \textbf{For2While} transforms a for-loop into a while-loop, restructuring the loop for different control flow requirements.
    \item \textbf{While2For} transforms a while-loop into a for-loop;
    \item \textbf{ElseIf2If} transforms an If...Else if... structure into a nested If...Else structure;
    \item \textbf{Switch2If} transforms a Switch-Case structure into an If-Else structure, converting switch-based logic into a series of conditional checks.
    \item \textbf{SwapStatement} swaps two statements that have no control or data dependency;
    \item \textbf{ReverseIf} switches the code blocks in the if statement and the corresponding else statement, inverting the condition and its associated logic.
    \item \textbf{If2CondExp} changes a single if statement into a conditional expression statement, simplifying the code into a more concise form. For example, \texttt{if (condition) \{ StatementA \} else \{ StatementB \}} becomes \texttt{condition ? StatementA : StatementB}.
    \item \textbf{CondExp2If} changes a conditional expression statement into a single if statement. For example, \texttt{condition ? StatementA : StatementB} becomes \texttt{if (condition) \{ StatementA \} else \{ StatementB \}}.
    \item \textbf{DividingComposedIf} divides an if statement with a compound condition \((\wedge, \vee, -)\) into two nested if-statements, breaking down complex conditions into simpler, more manageable parts.
\end{itemize}

\section{Evaluation Metrics}
\label{appx:metrics}

In this appendix, we present the formal definition and implementation details of our evaluation metrics, presented in Section~\ref{sec:metrics} 

\subsection{Consistency Metrics}
Given a benchmark dataset $\mathcal{D}$, an LLM $\mathcal{L}$, and a verification tool $\mathcal{V}$, success rate (SR) and failure rate (FR) are formally defined as follows:

\begin{equation*}
    SR(\mathcal{L}) = \dfrac{\big|\{r \in \mathcal{D} \mid g = \mathcal{L}(r) \wedge \mathcal{V}(g, r) = \text{ok}\}\big|}{|\mathcal{D}|},
\end{equation*}

\begin{equation*}
    FR(\mathcal{L}) = \dfrac{\big|\{r \in \mathcal{D} \mid g = \mathcal{L}(r) \wedge \mathcal{V}(g, r) = \text{fail}\}\big|}{|\mathcal{D}|},
\end{equation*}

where $g = \mathcal{L}(p)$ is the specification generated by $\mathcal{L}$ for the reference program $r$, and $\mathcal{V}(g, p)$ denotes the verification result of $g$ on $p$ using $\mathcal{V}$. To ensure the consistency between the generated specification and the reference program, we employ OpenJML~\cite{cok2011openjml}, a widely used program verification tool. Specifically, we utilize its latest version (21.0) in the \texttt{esc} mode (Extended Static Checker~\cite{flanagan2002extended}) with the CVC4 SMT solver~\cite{barrett2011cvc4}. Additionally, we enable arithmetic mode and assume that pointers are nullable by default.

\subsection{Completeness Metrics}
For a generated specification $g$ and a reference program $r$, the completeness rate (CR) is formally defined as follows:

\begin{equation*}
    CR(g, r) = \dfrac{\big|\{p \in \mathcal{P}(r) \mid \mathcal{V}(g, p) \neq \text{ok}\}\big|}{|\mathcal{P}(r)|},
\end{equation*}

where $\mathcal{P}(r)$ is the set of mutants for $r$, and $\mathcal{V}(g, p)$ is the verification result of $g$ on mutant $p$. Higher CR indicates greater completeness, as $g$ detects more faults. To generate s$\mathcal{P}(r)$, we utilize Major~\cite{just2014major}, a widely recognized mutation testing framework, using its latest version (3.0.1). To mitigate the generation of equivalent mutants, we further employ EMS~\cite{kushigian2024equivalent}, a state-of-the-art equivalent mutant suppression technique.

\subsection{Robustness Metrics}

To evaluate the robustness of LLMs, we leverage a set of 18 semantic-preserving transformations, presented in Section~\ref{appx:transformations}. Given $p$, its set of transformed programs $\mathcal{T}$, LLM $\mathcal{L}$, and verification tool $\mathcal{V}$, with $g = \mathcal{L}(p)$ verified as correct ($\mathcal{V}(g, p) = \text{ok}$), Flip Rate (FlR) is defined as follows:

\begin{equation*}
    FlR(p, \mathcal{T}) = \dfrac{\big|\{t \in \mathcal{T} \mid g' = \mathcal{L}(t) \wedge \mathcal{V}(g', t) \neq \text{ok}\}\big|}{|\mathcal{T}|}.
\end{equation*}

Moreover, we also measure the consistency and completeness metrics of LLMs on our transformed dataset. Since transformations may not apply universally, we normalize metrics over applicable transformations. For a reference program $r$, its set of transformed programs $\mathcal{T}$, and metric $\mathcal{M}$, the normalized metric $\mathcal{M'}$ is defined as follows:
\begin{equation*}
    \mathcal{M'}(\mathcal{T}) = \dfrac{\sum_{t \in \mathcal{T}} \mathcal{M}(t)}{|\mathcal{T}|},
\end{equation*}
where $\mathcal{M}$ can be $SR$, $FR$, or $CR$. 

\section{Evaluated Large Language Models}
\label{appx:llms}

We evaluate the following models with our FormalBench benchmark:
\begin{itemize}
    \item \textbf{Open-source LLMs:}
    \begin{itemize}
        \item CodeQwen-1.5: CodeQwen-1.5-7B~\cite{qwen}
        \item CodeQwen-2.5: Qwen2.5-Coder-32B-Instruct~\cite{qwen2, hui2024qwen2}
        \item CodeLLama: CodeLlama-34b-Instruct-hf~\cite{roziere2023code}
        \item DeepSeek-V2~\cite{liu2024deepseekv2}
    \end{itemize}
    \item \textbf{Proprietary LLMs: }
    \begin{itemize}
        \item DeepSeek-V3-671B~\cite{liu2024deepseekv3}
        \item GPT3.5: GPT-3.5-turbo~\cite{openai_gpt35_turbo}
        \item GPT-4o~\cite{openai_gpt4o}
        \item Claude: Claude-3.5-Sonnet~\cite{claude}
    \end{itemize}
\end{itemize}

\section{Experimental Settings}
\label{appx:exp_settings}

To query LLMs, we implemented our framework using LangChain, an open-source framework designed to streamline the development of applications leveraging llm. For running open-source LLMs, we use an NVIDIA A100 GPU with 80GB of VRAM and an Intel® Xeon® Gold 6326 CPU operating at 2.90 GHz. For running the verification tool, we leverage an Intel® Xeon® Platinum 8358 CPU operating at 2.90 GHz with 28 CPU cores and 1953GB of RAM.

\section{Constructions of FormalBench-Diverse-Natural}
\label{appx:diversenatural}

To construct FormalBench-Diverse-Natural, we first assess the naturalness of semantic-preserving transformations by measuring the relative change in cross-entropy, following established methods in previous studies~\cite{lecong2024reliableevaluationneuralprogram,ray2016naturalness,hindle2016naturalness}. We then select 50\% of the transformations, sorted by naturalness score. Finally, we choose programs with at least three transformations to avoid bias in the calculation of normalized metrics.

\section{Distribution of Verification success and failures over different control-flow types.}
\label{appx:distribution}

In this section, we present the distribution of verification success and failures over different control-flow types for three evaluated LLMs.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{img/claude_fail.png}
    \caption{Verification Failures of Claude-3.5-Sonnet}
    \label{fig:enter-label}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{img/claude_success.png}
    \caption{Verification Successes of Claude-3.5-Sonnet}
    \label{fig:enter-label}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{img/gpt4o_fail.png}
    \caption{Verification Failures of GPT-4o}
    \label{fig:enter-label}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{img/gpt4o_success.png}
    \caption{Verification Successes of GPT-4o}
    \label{fig:enter-label}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{img/deepseekv3_fail.png}
    \caption{Verification Failures of DeepSeek-V3}
    \label{fig:enter-label}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{img/deepseekv3_success.png}
    \caption{Verification Successes of DeepSeek-V3}
    \label{fig:enter-label}
\end{figure}

\section{Failure Analysis}
\label{appx:failure_analysis}

\begin{lstlisting}[float, language=java, caption=A example of error messages for a postcondition failures, escapechar=!, label=lst:error_example]
/tmp/PairOrSum.java:77: verify: The prover cannot establish an assertion (Postcondition: /tmp/PairOrSum.java:69:) in method spec_partialOrSum
        return sum;

/tmp/PairOrSum.java:69: verify: Associated declaration: /tmp/PairOrSum.java:77:
      @ ensures \result >= 0;
\end{lstlisting}

Since each specification can contain failures at multiple locations, our analysis begins by separating these errors into atomic errors. We then conduct a manual analysis of each error to identify common patterns in their error messages. For example, failures related to postconditions consistently include the following string in their messages: "The prover cannot establish an assertion (Postcondition)", as illustrated in Listing~\ref{lst:error_example}. Based on these patterns, we build a simple pattern matching to classify failures. This process is repeated until no remaining unknown patterns are found.

\section{Prompts}
\label{appx:prompts}

In this section, we present prompt templates used in this study for specification generation, including zero-shot, few-shot, chain-of-thought, and least-to-most prompts.

\begin{llmpromptbox}{Zero-shot prompt}
(System) You are an expert in Java Modeling Language (JML). You will be provided with Java code snippets and their task descriptions. Your task is to generate JML specifications for the given Java code. The specifications should be written as annotations within the Java code and must be compatible with the OpenJML tool for verification. Ensure the specifications include detailed preconditions, postconditions, necessary loop invariants, invariants, assertions, and any relevant assumptions.

(User) Please generate JML specifications for the provided Java code.

\#\#\# CODE

\{code\}

\end{llmpromptbox}

\begin{llmpromptbox}{Few-shot prompt}
(System) You are an expert in Java Modeling Language (JML). 
You will be provided with Java code snippets. 
Your task is to generate JML specifications for the given Java code. 
The specifications should be written as annotations within the Java code and must be compatible with the OpenJML tool for verification. 
Ensure the specifications include detailed preconditions, postconditions, necessary loop invariants, invariants, assertions, and any relevant assumptions.

Please also adhere to the following syntax guidelines for JML:

JML text is written in comments that either:

a) begin with //@ and end with the end of the line, or

b) begin with /*@ and end with */. Lines within such a block comment may 
have the first non-whitespace characters be a series of @ symbols.

\{examples\}

(User) Please generate JML specifications for the provided Java code.

\#\#\# CODE

\{code\}
\end{llmpromptbox}

\begin{llmpromptbox}{Chain-of-thought prompt}
(System) You are an expert in Java Modeling Language (JML). 
You will be provided with Java code snippets. 
Your task is to generate JML specifications for the given Java code. 
The specifications should be written as annotations within the Java code and must be compatible with the OpenJML tool for verification. 
Ensure the specifications include detailed preconditions, postconditions, necessary loop invariants, invariants, assertions, and any relevant assumptions.

Please also adhere to the following syntax guidelines for JML:

JML text is written in comments that either:

a) begin with //@ and end with the end of the line, or

b) begin with /*@ and end with */. Lines within such a block comment may 
have the first non-whitespace characters be a series of @ symbols.

\{examples\}

(User) Please generate JML specifications for the provided Java code.

\#\#\# CODE

\{code\}

Let's think step by step!
\end{llmpromptbox}

\begin{llmpromptbox}{Least-to-Most prompt}
(System) You are an expert in Java Modeling Language (JML). 
You will be provided with Java code snippets. 
Your task is to generate JML specifications for the given Java code. 
The specifications should be written as annotations within the Java code and must be compatible with the OpenJML tool for verification. 
Ensure the specifications include detailed preconditions, postconditions, necessary loop invariants, invariants, assertions, and any relevant assumptions.

Please also adhere to the following syntax guidelines for JML:

JML text is written in comments that either:

a) begin with //@ and end with the end of the line, or

b) begin with /*@ and end with */. Lines within such a block comment may 
have the first non-whitespace characters be a series of @ symbols.

\{examples\}

(User) Please generate JML specifications for the provided Java code.

\#\#\# CODE

\{code\}

Let's break down this problem:

1. What are the weakest preconditions for the code? Be sure to include preconditions related to nullness and arithmetic bounds.

2. What are the strongest postconditions for the code?

3. What necessary specifications are required to prove the above post-conditions? This includes loop invariants, assertions, assumptions, and ranking functions.

After answering these questions, let's generate the specifications for the code and provide solution after `\#\#\# SPECIFCIATION'

\end{llmpromptbox}

\section{Self-Repair Prompts}
\label{appx:repair_prompts}

In this section, we illustrate several repair prompts designed to address the most common errors. For a complete list of all repair prompts, please refer to our repository.

\begin{llmpromptbox}{Fixing prompt for Syntax Errors}
(System) You are an experts on Java Modeling Language (JML). Your task is to fix the JML specifications annotated in the target Java code. You will be provided the error messages from the OpenJML tool and you need to fix the specifications accordingly.

(User) The following Java code is annotated with JML specifications:

\{current specification\}

OpenJML Verification tool failed to verify the specifications given above, with error information as follows:

\#\#\# ERROR MESSAGE:

\{error messages\}

\#\#\# ERROR TYPES: Syntax Error

To resolve the syntax error, you should consider the following steps:

1. Identify whether the error is due to a Java syntax issue or a JML syntax issue.

2. Review the code to identify the specific location and nature of the syntax error.

3. Correct the syntax error based on the language rules and conventions.

Please refine the specifications so that they can pass verification. Provide the specifications for the code and include the solution written between triple backticks, after `\#\#\# FIXED SPECIFICATION`.
\end{llmpromptbox}

\begin{llmpromptbox}{Fixing prompt for Unsupported Sum/NumOf/Product Quantifier Expressions}
(System) You are an experts on Java Modeling Language (JML). Your task is to fix the JML specifications annotated in the target Java code. You will be provided the error messages from the OpenJML tool and you need to fix the specifications accordingly.

(User) The following Java code is annotated with JML specifications:

\{current specification\}

OpenJML Verification tool failed to verify the specifications given above, with error information as follows:

\#\#\# ERROR MESSAGE:

\{error messages\}

\#\#\# ERROR TYPES: Unsupported Sum/NumOf/Product Quantifier Expressions

OpenJML does not fully support JML's inductive quantifiers like \textbackslash num\_of, \textbackslash sum, and \textbackslash product in specifications. These operators require inductive reasoning (e.g., counting elements, summing values over a range, or computing products), which is difficult for SMT solvers (the engines behind OpenJML and most of deductive verification tools) to handle.

To avoid the use of \textbackslash sum, \textbackslash num\_of, and \textbackslash product quantifiers in your JML specifications, you can express your specifications using induction steps to help OpenJML's verifiers to reason about your code. You can do this by define mathematical functions and lemmas through model methods. 
For example, you can should not use \textbackslash product quantifier in the following specifications:

\{Examples with reasoning\}

Please refine the specifications so that they can pass verification. Provide the specifications for the code and include the solution written between triple backticks, after `\#\#\# FIXED SPECIFICATION`.
\end{llmpromptbox}

\begin{llmpromptbox}{Fixing prompt for Unsupported Min/Max Quantifier Expressions}
(System) You are an experts on Java Modeling Language (JML). Your task is to fix the JML specifications annotated in the target Java code. You will be provided the error messages from the OpenJML tool and you need to fix the specifications accordingly.

(User) The following Java code is annotated with JML specifications:

\{current specification\}

OpenJML Verification tool failed to verify the specifications given above, with error information as follows:

\#\#\# ERROR MESSAGE:

\{error messages\}

\#\#\# ERROR TYPES: Unsupported Min/Max Quantifier Expressions

OpenJML does not fully support JML's inductive quantifiers like \textbackslash min, \textbackslash max in specifications. These operators require inductive reasonings, which is difficult for SMT solvers (the engines behind OpenJML and most of deductive verification tools) to handle.

To avoid the use of \textbackslash min and \textbackslash max quantifiers in your JML specifications, you can use the \textbackslash forall quantifier to express your specifications. 
For example, you should not use \textbackslash max quantifier in the following specifications:

\{Examples with reasoning\}

Please refine the specifications so that they can pass verification. Provide the specifications for the code and include the solution written between triple backticks, after `\#\#\# FIXED SPECIFICATION`.
\end{llmpromptbox}

\begin{llmpromptbox}{Fixing prompt for Loop Invariant Failures}
(System) You are an experts on Java Modeling Language (JML). Your task is to fix the JML specifications annotated in the target Java code. You will be provided the error messages from the OpenJML tool and you need to fix the specifications accordingly.

(User) The following Java code is annotated with JML specifications:

\{current specification\}

OpenJML Verification tool failed to verify the specifications given above, with error information as follows:

\#\#\# ERROR MESSAGE:

\{error messages\}

\#\#\# ERROR TYPES: Loop Invariant Failures

This error occurs when the loop invariant, a condition that must hold true before the loop begins and remain true after each iteration, is not properly established or maintained. This semantic error typically arises when verifiers fail to confirm the correctness of the synthesized loop invariant. The causes of this error include: (1) an incorrect loop invariant, (2) wrong/weak preconditions that prevent the invariant from holding at the start of the loop, or (3) incomplete reasoning about the loop, leading to insufficient information for the verifier to verify the invariant.

To resolve the error, please consider the following steps:

1. Carefully review the loop invariant to ensure it correctly captures the necessary conditions that hold true before and after each iteration of the loop.

2. Carefully examine preconditions to ensure they are strong enough to establish the loop invariant at the beginning of the loop.

3. Add additional assertions or assumptions within the loop to help the verifier reason about the loop invariant.

For example, consider the following code snippet with a loop invariant failure:

\{Examples with reasoning\}

Please refine the specifications so that they can pass verification. Provide the specifications for the code and include the solution written between triple backticks, after `\#\#\# FIXED SPECIFICATION`.
\end{llmpromptbox}

\begin{llmpromptbox}{Fixing prompt for Post-Condition Failures}
(System) You are an experts on Java Modeling Language (JML). Your task is to fix the JML specifications annotated in the target Java code. You will be provided the error messages from the OpenJML tool and you need to fix the specifications accordingly.

(User) The following Java code is annotated with JML specifications:

\{current specification\}

OpenJML Verification tool failed to verify the specifications given above, with error information as follows:

\#\#\# ERROR MESSAGE:

\{error messages\}

\#\#\# ERROR TYPES: Post-condition Failures

This error occurs when the postcondition, a condition that must hold true after the execution of a program or function, is not satisfied. This type of semantic error typically arises when verifiers are unable to confirm that the program’s logic guarantees the postcondition under all valid inputs and scenarios. The causes of this error include: (1) an incorrect or incomplete postcondition, (2) wrong/weak preconditions that prevent the program from reaching a state where the postcondition holds, or (3) incomplete reasoning about the programs, leading to insufficient information for the verifier to verify the postcondition.

To resolve the error, please consider the following steps:

1. Review the postcondition to ensure it correctly captures the expected behavior of the program or function.

2. Check the preconditions to ensure they are strong enough to reach a state where the postcondition holds.

3. Add additional assertions or assumptions within the program or function to help the verifier reason about the postcondition.

For example, consider the following code snippet with a postcondition failure:

\{Examples with reasoning\}

Please refine the specifications so that they can pass verification. Provide the specifications for the code and include the solution written between triple backticks, after `\#\#\# FIXED SPECIFICATION`.
\end{llmpromptbox}

\end{document}
