\section{Related Works}
\textbf{Reasoning Evaluation of LLMs.} Numerous datasets have been curated to evaluate the reasoning capabilities of LLMs across diverse domains, including mathematical____, logical____, and causal reasoning____. Recent work explores code reasoning, evaluating LLMs' ability on program semantic inference.
____. Early studies focus on code summarization____, capturing high-level understanding rather than fine-grained semantic reasoning. Recent studies examine code reasoning in detail through output prediction____, execution trace simulation____, and invariant inference____, yet they still address only partial program semantics. In contrast, FormalBench targets formal specification inference, demanding exhaustive reasoning that produces precise, verifiable specifications for every possible execution.

\textbf{Formal Specification Inference.}  Traditional dynamic analysis methods, such as Daikon____, Houdini____, and DIG____, infer likely invariants from observed behaviors using predefined templates. However, these tools often yield trivial invariants (e.g., nums != null) and struggle with complex functional properties____. Recent work leverages LLMs to address these limitations. Early approaches____ fine-tuned LLMs for invariant inference but focused on specific cases, such as loop invariants or unverified likely invariants. Nilizadeh et al.____ manually crafted complete program specifications to assess automated repair effectiveness. Building on this, newer methods such as SpecGen____ and AutoSpec____ automatically generate full formal specifications via iterative refinement and static analysis. However, as discussed in Section~\ref{sec:intro}, their evaluations remain limited, highlighting the need for FormalBench and more comprehensive assessments of LLM effectiveness.