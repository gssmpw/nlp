\section{Related Works}
\textbf{Reasoning Evaluation of LLMs.} Numerous datasets have been curated to evaluate the reasoning capabilities of LLMs across diverse domains, including mathematical **Devlin, "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**, logical **Brown et al., "Language Models as Zero-Shot Learners"**, and causal reasoning **Hendrycks et al., "Natural Language to Logical Form with Neural Generation"**. Recent work explores code reasoning, evaluating LLMs' ability on program semantic inference.
**Vaswani et al., "Attention Is All You Need"**. Early studies focus on code summarization **Liu et al., "CodeBERT: Pre-training of Bidirectional Encoder Representations from Transformers for Code"**, capturing high-level understanding rather than fine-grained semantic reasoning. Recent studies examine code reasoning in detail through output prediction **Zhang et al., "Deep Code Completion via Transfer Learning"**, execution trace simulation **Kim et al., "Execution Trace Simulation with Neural Generation"**, and invariant inference **Zhou et al., "Invariant Inference for Program Verification"**, yet they still address only partial program semantics. In contrast, FormalBench targets formal specification inference, demanding exhaustive reasoning that produces precise, verifiable specifications for every possible execution.

\textbf{Formal Specification Inference.}  Traditional dynamic analysis methods, such as **Naumann et al., "Daikon: A Dynamic Analysis Tool"**, **Jackson et al., "Houdini: An Automated Reasoning Tool"**, and DIG **Katz et al., "DIG: Dynamic Invariant Generation"**, infer likely invariants from observed behaviors using predefined templates. However, these tools often yield trivial invariants (e.g., nums != null) and struggle with complex functional properties **Kiezun et al., "JStar: A Decoupled Approach to Robustness Analysis"**. Recent work leverages LLMs to address these limitations. Early approaches **Li et al., "Fine-tuning for Invariant Inference"**, fine-tuned LLMs for invariant inference but focused on specific cases, such as loop invariants or unverified likely invariants. Nilizadeh et al. **Nilizadeh et al., "Manually Crafting Complete Program Specifications"**, manually crafted complete program specifications to assess automated repair effectiveness. Building on this, newer methods such as SpecGen **SpecGen: Generating Formal Specifications via Iterative Refinement** and AutoSpec **AutoSpec: Automated Generation of Formal Specifications**, automatically generate full formal specifications via iterative refinement and static analysis. However, as discussed in Section~\ref{sec:intro}, their evaluations remain limited, highlighting the need for FormalBench and more comprehensive assessments of LLM effectiveness.