%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% LaTeX Template for AAMAS-2025 (based on sample-sigconf.tex)
%%% Prepared by the AAMAS-2025 Program Chairs based on the version from AAMAS-2025. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Start your document with the \documentclass command.


%%% == IMPORTANT ==
%%% Use the first variant below for the final paper (including auithor information).
%%% Use the second variant below to anonymize your submission (no authoir information shown).
%%% For further information on anonymity and double-blind reviewing, 
%%% please consult the call for paper information
%%% https://aamas2025.org/index.php/conference/calls/submission-instructions-main-technical-track/

%%%% For anonymized submission, use this
%\documentclass[sigconf,anonymous]{aamas} 

%%%% For camera-ready, use this
\documentclass[sigconf]{aamas} 


%%% Load required packages here (note that many are included already).
\usepackage{amsfonts}
\usepackage{balance} % for balancing columns on the final page
\usepackage{booktabs}
\usepackage[capitalize,noabbrev]{cleveref}
\usepackage[inline]{enumitem}
\usepackage{float}
\usepackage{flushend}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{microtype}
\usepackage{placeins}
\usepackage{siunitx}
\usepackage{subcaption}
\usepackage{verbatim}
\usepackage{xcolor}
\usepackage{xspace}

\makeatletter
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}
\def\ie{\emph{i.e}\onedot} \def\Ie{\emph{I.e}\onedot}

\newcommand{\longcell}[2][t]{%
  \begin{tabular}[#1]{@{}c@{}}#2\end{tabular}}

\newcommand{\vast}{\bBigg@{4}}
\newcommand{\Vast}{\bBigg@{5}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% AAMAS-2025 copyright block (do not change!)

\makeatletter
\gdef\@copyrightpermission{
  \begin{minipage}{0.2\columnwidth}
   \href{https://creativecommons.org/licenses/by/4.0/}{\includegraphics[width=0.90\textwidth]{by}}
  \end{minipage}\hfill
  \begin{minipage}{0.8\columnwidth}
   \href{https://creativecommons.org/licenses/by/4.0/}{This work is licensed under a Creative Commons Attribution International 4.0 License.}
  \end{minipage}
  \vspace{5pt}
}
\makeatother

\setcopyright{ifaamas}
\acmConference[AAMAS '25]{Proc.\@ of the 24th International Conference
on Autonomous Agents and Multiagent Systems (AAMAS 2025)}{May 19 -- 23, 2025}
{Detroit, Michigan, USA}{Y.~Vorobeychik, S.~Das, A.~Now√©  (eds.)}
\copyrightyear{2025}
\acmYear{2025}
\acmDOI{}
\acmPrice{}
\acmISBN{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% == IMPORTANT ==
%%% Use this command to specify your OpenReview submission number.
%%% In anonymous mode, it will be printed on the first page.

\acmSubmissionID{1307}

%%% Use this command to specify the title of your paper.

\title[Implicit Repair with Reinforcement Learning in Emergent Communication]{Implicit Repair with Reinforcement Learning in Emergent Communication}

% Add the subtitle below for an extended abstract
%\subtitle{Extended Abstract}

%%% Provide names, affiliations, and email addresses for all authors.

\author{F\'abio Vital}
\affiliation{
  \institution{INESC-ID \& Instituto Superior T\'ecnico}
  \city{Lisboa}
  \country{Portugal}}
%\email{fabiovital@tecnico.ulisboa.pt}

\author{Alberto Sardinha}
\affiliation{
  \institution{INESC-ID \& PUC-Rio}
  \city{Rio de Janeiro}
  \country{Brazil}}


\author{Francisco S. Melo}
\affiliation{
  \institution{INESC-ID \& Instituto Superior T\'ecnico}
  \city{Lisboa}
  \country{Portugal}}

%%% math commands  
\input{math_commands.tex}

\DeclareMathOperator{\sg}{sg}
\DeclareMathOperator{\score}{score}
%\DeclareMathOperator{\mess}{\(m\)}
%\DeclareMathOperator{\noise}{\(n\)}
\DeclareMathOperator{\choice}{choice}
  
%%% Use this environment to specify a short abstract for your paper.
\begin{abstract}
\emph{Conversational repair} is a mechanism used to detect and resolve miscommunication and misinformation problems when two or more agents interact. One particular and underexplored form of repair in emergent communication is the implicit repair mechanism, where the interlocutor purposely conveys the desired information in such a way as to prevent misinformation from any other interlocutor. This work explores how redundancy can modify the emergent communication protocol to continue conveying the necessary information to complete the underlying task, even with additional external environmental pressures such as noise. We focus on extending the signaling game, called the Lewis Game, by adding noise in the communication channel and inputs received by the agents. Our analysis shows that agents add redundancy to the transmitted messages as an outcome to prevent the negative impact of noise on the task success. Additionally, we observe that the emerging communication protocol's generalization capabilities remain equivalent to architectures employed in simpler games that are entirely deterministic. Additionally, our method is the only one suitable for producing robust communication protocols that can handle cases with and without noise while maintaining increased generalization performance levels. Our code and appendix are available at \url{https://fgmv.me/projects/noisy-emcom}. First author correspondence: \href{mailto:fabiovital@tecnico.ulisboa.pt}{fabiovital@tecnico.ulisboa.pt}.
\end{abstract}

%%% The code below was generated by the tool at http://dl.acm.org/ccs.cfm.
%%% Please replace this example with code appropriate for your own paper.


%%% Use this command to specify a few keywords describing your work.
%%% Keywords should be separated by commas.

\keywords{emergent communication; representation learning; reinforcement learning; multi-agent reinforcement learning}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Include any author-defined commands here.
         
\newcommand{\BibTeX}{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em\TeX}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

%%% The following commands remove the headers in your paper. For final 
%%% papers, these will be inserted during the pagination process.

\pagestyle{fancy}
\fancyhead{}

%%% The next command prints the information defined in the preamble.

\maketitle 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}

\emph{Emergent Communication} (EC) is a field that recently gained attention in  Machine Learning (ML) research. The progress of language evolution research~\citep{wagner2003progress,choicompositional,Ren2020Compositional,chaabouni-etal-2020-compositionality,chaabouni2019anti,ueda-washio-2021-relationship} and the conceptualization of artificial languages for robot and human-robot communication~\citep{bogin2018emergence,mordatch2018emergence,havrylov2017emergence,jorge2016learning} are some of the fundamental motivations behind the recent rise in interest. Mainly, EC focuses on developing experiments where a group of agents must learn how to communicate without prior knowledge to achieve a common goal, where coordination and cooperation are essential for the group's success~\citep{winograd1972understanding,Lewis1979,nowak1999evolution}. This approach differs from the current state of the art in natural language processing (NLP), where large language models (LLMs) dominate the field. LLMs are supervised statistical models that optimize the prediction of the next token given a context (textual input)~\citep{openai2023gpt4,jiang2023mistral,dubey2024llama,dao2024transformers}. It is still an open question whether working only in the language space (as LLMs do) is enough to create agents with an intrinsic and deeper meaning about the world that are capable of adapting to novel circumstances effectively~\citep{bisk2020experience}. As such, we argue that exploring different approaches, like EC, is crucial to continue advancing the field of NLP.

The main focus of this work comprises the study of a specific topic in language evolution called \emph{conversational repair}~\citep{schegloff1977preference}. In linguistics, conversational repair is already a known topic that plays an important role in establishing complex and efficient communication protocols~\citep{albert2018repair}. In short, conversational repair aggregates any communication mechanism employed by any interlocutor to initiate a process to detect and clarify some information being transferred by any other interlocutor. To give more context on how our work relates to previous literature, we divide repair mechanisms into two broad categories: implicit and explicit, a coarser partitioning of the one introduced by~\citet{lemon2022conversational}. Explicit repair mechanisms happen when an interlocutor distinctly starts a follow-up interaction (communication) in order to clarify some conveyed past information, e.g., ``Is it the blue one?'' (confirmation); ``Is it the first or the second one?'' (clarification). On the other hand, implicit mechanisms happen in a subtle way where the interlocutor, conveying the original information, intentionally expresses it in such a way as to minimize misinformation and preemptively avoid posterior conversational repair phases altogether. The implicit conversational repair mechanism also has connections to the concept of redundancy in linguistics and communication analysis~\citep{cherry1966human}. Redundancy appears in every human language, at the semantic and syntax level, and appears in the form of repetition or extra content to send. Additionally, the sending interlocutor may apply different levels of redundancy that she/he finds necessary to convey the information given the target audience and medium used for communication.

Most previous works in EC employ variations of a signaling game called the Lewis Game (LG)~\citep{Lewis1979}, with the primary purpose of analyzing how a communication protocol emerges as the result of achieving cooperation to solve the game~\citep{choicompositional,jorge2016learning,guo2019emergence}. In the LG, the Speaker describes an object to the Listener, who then has to discriminate it against a set of distractor objects. We call the union of the assigned object and distractors the \emph{candidates}. Regarding this study, we extend a variation introduced by~\citet{chaabouni2022emergent}, where real images are used as the objects to discriminate instead of categorical inputs, and the number of candidates given to the Listener increases in several orders of magnitude, conveying a more complex game than in previous works~\citep{ueda-washio-2021-relationship,mordatch2018emergence,lazaridou2018emergence,rita2022on,chaabouni-etal-2020-compositionality,chaabouni2019anti}. In the original work, the authors propose a supervised training routine where the Listener receives the correct answer after each game. However, this implementation diverges from human communication, where there is usually no direct supervision on how effective a particular dialogue can be~\citep{hayashi2013conversational}. We propose modeling both agents as RL agents. As such, the only (semi) supervised information given to the agents is the outcome of the game. Similar to previous works~\citep{havrylov2017emergence,lazaridou2018emergence,li2019ease,rita2022on}, we model both agents using Reinforce~\citep{williams1992simple}.

Furthermore, as a means to study implicit repair mechanisms, we define a new game variation with faulty communication channels that can introduce noise into the messages. This new game setup has the necessary conditions to study if agents can detect and overcome miscommunication/misinformation to solve the game cooperatively. Our analysis shows that the emerging communication protocols have redundancy built in to prevent the adverse effects of noise, where even partial messages have enough information for the Listener to select the correct candidate. Additionally, we show that the training in the noisy game produces communication protocols that are highly robust to noise, being effective in different noise levels, even without noise, at test time.

Previous literature has already addressed the problem of explicit conversational repair. \citet{lemon2022conversational} propose new research directions on how to embed conversational repair into EC tasks, where the repair mechanism acts as a catalyst to fix misalignments, for example, in the language learned by each agent for a specific cooperative task. Moreover, another recent work develops an extended version of the Lewis Game to enable a feedback mechanism from the Listener to the Speaker, mimicking the initialization of an external repair mechanism phase~\citep{nikolaus2023emergent}. However, some limitations compromise the co-relation to human languages. First, the feedback sent by the Listener contains minimal information (single binary token), and such feedback is sent after every token in the message, breaking the turn-taking nature of the dialogue. Compared to our work, we designed a more challenging game where we prevent cyclic feedback (from the Listener to the Speaker), meaning the Speaker does not receive direct feedback about how noise affects the messages being transferred. In our case, the Speaker only knows the result of the game. Consequently, the Speaker needs to understand through trial and error how to convey information to facilitate the Listener's job, inducing an implicit repair mechanism, as explained previously. %{\color{red}Additionally, the work of uead and washio also introduces a noisy game design where message tokens can be substituted by other tokens present in the vocabulary. This approach creates an adversarial setting by giving other plausible messages to the Listener (different from what the Speaker sent) in an attempt to develop a protocol that enforces concise messages. This setup allows for the authors to analyze a specific language property: Zipf's law of abbreviation. On the other hand, our work focuses on simulating the loss of information to guide the creating of robust communication protocols by taking advantage of adding redundant information in order to actively create repair mechanisms to prevent misinformation.}

To summarize, our contributions are 3-fold. First, although previous works introduce game designs featuring noisy channels, we contribute with a rigorous mathematical derivation on how to aggregate noise into the LG. We found such inference lacking in the literature. Furthermore, we define a new noisy game variant where the input objects to discriminate are injected with noise. We use this new variant as an out-of-distribution game to evaluate how the trained protocols react to new forms of noise. Secondly, we demonstrate the effectiveness of employing RL agents on complex LG variants featuring the discrimination of natural images and noisy communication channels. Additionally, we showcase that the RL variant achieves better results than the original architecture with a supervised Listener. We emphasize that our objective is not to benchmark different RL algorithms but to show that implementing the Listener as an RL agent can bring advantages against the Supervised counterpart, where even a straightforward implementation of Reinforce is enough to observe substantial gains already. Third, we analyze and show how more complex game designs, such as introducing noise to the LG, guide the agents to resort to redundancy measures to complete the game efficiently, mimicking implicit conversational repair mechanisms. We show how the protocols emerging to communicate through noisy channels have better generalization capabilities and robustness to different noise levels at test time. Additionally, we illustrate that these improvements are a side-effect of resorting to redundancy in the messages sent.

% Left 14 lines on the second's page first column

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Methodology} \label{sec:meth}

We start this section by defining a noisy variation of the LG ,called the Noisy Lewis Game (NLG). The main change in the NLG incorporates a faulty communication channel where noise interferes with the transmitted messages by masking a subset of the tokens. This game variation is more complex than the LG, where the (RL) game environment becomes stochastic. We further note that the original LG is a simplification of the NLG where we fix the noise level at zero. Afterward, we detail how the Speaker converts the received input into a message, a sequence of discrete tokens, and how the Listener processes the message and candidates to make decisions. We impose a RIAL setting~\citep{foerster2016learning}, where agents are independent and perceive the other as part of the environment. Hence, we describe the learning strategy for both agents independently, explaining the loss composition and the importance of each loss term to guide training where functional communications protocols can emerge.

\subsection{Noisy Lewis Game (NLG)} \label{sec:meth-nlg}
The Noisy Lewis Game (NLG) is a discrimination game in which one of the agents, the \emph{Speaker}, must describe an object by sending a message to the other agent, the \emph{Listener}. When the game starts, the Speaker receives a target image \(\vx\) retrieved from a fixed dataset \(\vx\in\sX\) and describes it by generating a message, \(m: \sX\times\sR^K\rightarrow \sW^N\), where \(\sW\) is a finite vocabulary, and \(\vtheta\in\sR^K\) parametrizes \(m\). The message comprises \(N\) discrete tokens, \(m\left(\vx;\vtheta\right)=\left(m_t\left(\vx;\vtheta\right)\right)_{t=1}^{N}\), where \(m_t(\vx;\vtheta)\in\sW\). Due to the noisy nature of the communication channel, the Listener can receive a message with unexpected modifications. We model this perturbation with the function \(n:\sW^N\rightarrow\sW'^N\), where the function processes each token independently and converts it into a default unknown token with a given probability. As such, \(\sW'\) is the union of the original vocabulary plus the unknown token, \(\sW'=\sW\cup\{\mathtt{unk}\}\). We describe introduce \(n\) as :
%
\begin{equation}
\begin{split}
    n&\left(m\left(\vx;\vtheta\right)\right) = \left( n_t\left(m_t\left(\vx;\vtheta\right)\right) \right)_{t=1}^{N}\\
    & \text{s.t.}\;\;n_t\left(m_t\left(\vx;\vtheta\right)\right) = 
    \begin{cases}
        m_t(\vx;\vtheta), & \mbox{if } \rp > \lambda \\
        \mathtt{unk}, & \mbox{otherwise},
    \end{cases}
\end{split}
\label{eq:lg-noise}
\end{equation}
%
where \(\rp\) is sampled from a uniform distribution, \(\rp\sim \mathcal{U}\left(0,1\right)\), and \(\lambda\in\left[0,1\right)\) is a fixed threshold, indicating the noise level present in the communication channel. By definition, the Speaker is agnostic to this process and will never know if the message was modified. For simplicity, we define \(\vm\) to describe a (noisy) message given some input, \(\vm=m\left(\vx,\vtheta\right)\) or \(\vm=n\left(m\left(\vx,\vtheta\right)\right)\). We also refer each message token as \(m_t\) instead of \(m_t(\vx;\vtheta)\), omitting the domain.

Subsequently, the Listener receives the message along with a set of candidate images, \(\sC\in \left[\sX\right]^C\), where \(\left[\sX\right]^C\) defines the set of all subsets with \(C\) elements from \(\sX\). With both inputs, the Listener tries to identify the image the Speaker received, \(\vx\). We define \(\choice:\sW'^N\times\left[\sX\right]^C\times\sR^{k'}\rightarrow\sJ\) to specify the Listener's discrimination process, where \(\sJ\subset\sN\) is a particular enumeration of \(\sC\), such that \(\sC=\bigcup_{j\in\sJ}\sC_j\), \(|\sJ|=C\), and \(\vphi\in\sR^{k'}\) parametrizes \(\choice\). Therefore, the index outputted by the Listener, \(j=\choice\left(\vm,\sC;\vphi\right)\), is the \(j\)-th element in \(\sC\), denoting the final guess \(\hat{\vx}=\sC_j\).

Both agents receive a positive reward if the Listener correctly identifies the target image \(\vx\) and a negative reward otherwise:
%
\begin{equation}
    R(\vx,\hat{\vx}) = 
    \begin{cases}
        1, & \mbox{if } \hat{\vx}=\vx \\
        -1, & \mbox{if } \text{otherwise}.
    \end{cases}
    \label{eq:lg-rwd}
\end{equation}
%
Lastly, note that the original LG is a specification of the NLG where we set \(\lambda=0\). \Cref{fig:emcom-nlg} depicts a visual representation of NLG. For completeness, the LG is depicted in \Cref{app:lg}.

\begin{figure}[!t]
  \centering
  \includegraphics[width=0.81\linewidth,keepaspectratio]{figures/nlg-new.pdf}
  \caption{Visual Representation of the Noisy Lewis Game (NLG). In this illustration, the message, \(\vm\), contains three tokens (\(N=3\)), where the last one is masked.}
  \label{fig:emcom-nlg}
\end{figure}

%%%

\subsection{Agent Architectures} \label{sec:meth-arch}
We now describe the architectures implemented for both agents, the Speaker and the Listener (\Cref{fig:speaker-list-archs}). We design both architectures to be able to model policy gradient RL algorithms~\citep{sutton2018reinforcement}.

As an overview, the Speaker's objective is to encode a discrete message, \(\vm\), describing an input image, \(\vx\), see \Cref{sec:meth-nlg}. First, we encode the image using a pre-trained image encoder~\citep{grill2020bootstrap}, \(f\), to reduce its dimensionality and extract valuable features, \(\vx'=f\left(\vx\right)\). Subsequently, a trainable encoder \(g\) processes the new sequence of features, outputting the initial hidden and cell values, \((\vz_{0,\vtheta},\vc_{0,\vtheta})=g\left(\vx';\vtheta\right)\), used by the recurrent module \(h\), in this case, an LSTM~\citep{hochreiter1997long}.

Subsequently, the Speaker will select each token \(m_t\) to add to the message iteratively, using \(h\). On this account, we define a complementary embedding module, \(e\), to convert the previous discrete token \(m_{t-1}\) into a dense vector \(\vd_{t,\vtheta}=e\left(m_{t-1};\vtheta\right)\). Then, the recurrent module, \(h\), consumes the new dense vector and previous internal states to produce the new ones, \((\vz_{t,\vtheta},\vc_{t,\vtheta})=h\left(\vd_{t,\vtheta},\vz_{t-1,\vtheta},\vc_{t-1,\vtheta};\vtheta\right)\). We then pass \(\vz_{t,\vtheta}\) through two concurrent heads:
%
\begin{enumerate*}[label=(\roman*)]
    \item The actor head yields the probability of choosing each token as the next one, \(m_t\sim\pi_S\left(\cdot|\vz_{t,\vtheta};\vtheta\right)\);
    \item The critic head estimates the expected reward \(V\left(\vx\right) := v\left(\vz_{t,\theta};\theta\right)\).
\end{enumerate*}
%
After the new token is sampled, we feed it back to \(e(\cdot\,;\theta)\), and the process repeats itself until we generate \(N\) tokens. The first token \(m_0\) is a predefined \emph{start-of-string} token and is not included in the message. Following~\cite{chaabouni2022emergent}, we maintain the original vocabulary and message sizes, where \(|\sW|=20\), and \(N=10\), making the set of all possible message much larger than the size of the dataset used (\(|\sX|\approx10^6\) for ImageNet~\cite{ILSVRC15}). We depict the Speaker's architecture in \Cref{fig:speaker-arch}.

\begin{figure*}[!t]
    \begin{center}
    \begin{subfigure}{.49\textwidth}
      \centering
      \centerline{\includegraphics[width=0.4\linewidth]{figures/emcom-speaker.pdf}}
      \caption{}
      \label{fig:speaker-arch}
    \end{subfigure}
    \hfill
    \begin{subfigure}{.49\textwidth}
      \centering
      \centerline{\includegraphics[width=0.52\linewidth]{figures/emcom-listener.pdf}}
      \caption{}
      \label{fig:list-arch}
    \end{subfigure}
    \end{center}
    \caption{Graphical representation of Speaker, \Cref{sub@fig:speaker-arch}, and Listener, \Cref{sub@fig:list-arch}, architectures for the NLG. In this illustration, the message, \(\vm\), contains only two tokens, \(N=2\).}
    \label{fig:speaker-list-archs}
    \vspace{-2ex}
\end{figure*}

The Listener architecture has two different modules to process the message, sent from the Speaker, \(\vm\) and the images obtained as candidates \(\sC\). Additionally, a third module combines the output of both input components and provides it to the actor and critic heads. We now describe each component.

To process the candidate images \(\sC\), the Listener uses the same pre-trained encoder \(f\) combined with the network \(c\) to embed the candidate images, \(\vl_{j}=c(\vx_j';\vphi)\), where \(\vx_j'=f(\vx_j)\) and \(\vx_j\in\sC\).

Concerning the message received, the Listener uses the recurrent model \(h\) (an LSTM) to handle each token, \(m_t\), iteratively. Similarly to the Speaker, there is an embedding layer, \(e(\cdot\,;\vphi)\), to convert the discrete token into a dense vector before giving it to \(h\), where we have \((\vz_{t,\vphi}, \vc_{t,\vphi}) = h(e(m_t;\vphi), \vz_{t-1,\vphi}, \vc_{t-1};\vphi)\). The initial internal states of \(h\) are initialized as \(\vz_{0,\vphi}=\vzero\) and \(\vc_{0,\vphi}=\vzero\). After processing all message tokens, the final hidden state, \(\vz_{N,\vphi}\), goes through a final network \(g\) to output the message's hidden value \(\vl_\text{m}=g(\vz_{N,\vphi};\vphi)\). Finally, the generated hidden values for the message and all candidates flow through to the head module.

The first operation in the head module executes an attention mechanism to combine the message features with each candidate's counterpart. The output includes a value per candidate which we concatenate into a vector \(\vs=\left[\begin{matrix}\vl_\text{m}\cdot \vl_{1} & \ldots & \vl_\text{m}\cdot \vl_{{|\sC|}}\end{matrix}\right]^T\), called the candidates' score. We define the actor head as \(\pi_L(\cdot|\vs;\vphi)\) to output the Listener's policy \(\hat{\vx}\sim\pi_L(\cdot|\vm,\sC)\), which is a valid approximation since \(\vs\) holds information from the message and candidates. Parallelly, the critic head \(v(\cdot\,;\vphi)\) receives the same scores \(\vs\) and estimates the expected cumulative reward, as detailed in \Cref{sec:meth-learn}.

%%%

\subsection{Learning Strategy} \label{sec:meth-learn}
As described at the start of \cref{sec:meth-nlg}, the agents can only transmit information via the communication channel, which has only one direction: from the Speaker to the Listener. Additionally, agents learn how to communicate following the RIAL protocol, where agents are independent and treat others as part of the environment. As such, we have a decentralized training scheme where the agents improve their own parameters solely by maximizing the game's reward, see~\eqref{eq:lg-rwd}.

To perform well and consistently when playing the NLG, the Speaker must learn how to utilize the vocabulary to distinctively encode each image into a message to obtain the highest expected reward possible. We use Reinforce~\citep{williams1992simple}, a policy gradient algorithm, to train the Speaker. Given a target image \(\vx\) and the corresponding Listener's action \(\hat{\vx}\), we have a loss, \(L_\text{S,A}\), to fit the actor's head and another one, \(L_\text{S,C}\), for the critic's head. We define,
%
\begin{equation*}
L_\text{S,A}(\vtheta) = -\sum_{t=1}^N\sg\left(R(\vx,\hat{\vx})-v\left(\vz_{t,\vtheta};\vtheta\right)\right)\cdot\log\pi_S\left(m_t|\vz_{t,\vtheta};\vtheta\right),
\end{equation*}
%
where \(\sg\left(\cdot\right)\) is the \textit{stop-gradient} function, in order to optimize the policy. Note that the Speaker is in a sparse reward setting~\citep{10.5555/3042573.3042686}, where the sum of returns is the same as the game reward \(R\left(\vx,\hat{\vx}\right)\). Further, we subtract a baseline (critic head's value \(v\left(\vz_{t,\vtheta}\right)\)) from the returns to reduce variance. Regarding the critic loss, we devise
%
\begin{equation*}
L_\text{S,C}(\vtheta)=\sum_{t=1}^{N}\left(R(\vx, \hat{\vx}) - v(\vz_{t,\vtheta};\vtheta)\right)^2,
\end{equation*}
%
to approximate the state-value function \(V(\vx)=\E_{\pi_S}\left[R(\vx,\hat{\vx})\right]\).

We also use an additional entropy regularization term, \(L_{\text{S},\mathcal{H}}\), to make sure the language learned by the Speaker will not entirely stagnate by encouraging new combinations of tokens that increase entropy, further incentivizing exploration. Moreover, we define a target policy for the Speaker to minimize an additional KL divergent term, \(L_\text{S,KL}\), between the online and target policies, \(\vtheta\) and \(\bar{\vtheta}\), respectively. We update \(\bar{\vtheta}\) using an exponential moving average (EMA) over \(\vtheta\), \ie \(\bar{\vtheta}\leftarrow (1-\eta)\vtheta + \eta\bar{\vtheta}\) where \(\eta\) is the EMA weight parameter. With \(L_\text{S,KL}\), we prevent steep changes in the parameter space, which helps stabilize training~\citep{rawlik2012stochastic,chane2021goal}. We refer to~\citet{chaabouni2022emergent} for a complete analysis on the impact of \(L_\text{S,KL}\). Finally, we weigh each loss term and average the resulting sum given a batch of input images, \(\sX'\subset\sX\), to obtain the overall Speaker loss:
%
\begin{equation*}
\begin{split}
L_\text{S}(\vtheta) = \frac{1}{|\sX'|} \sum_{\vx\in\sX'} &\alpha_\text{S,A} L_\text{S,A}(\vtheta) + \alpha_\text{S,C} L_\text{S,C}(\vtheta) \\
& + \alpha_{\text{S},\mathcal{H}} L_{\text{S},\mathcal{H}}(\vtheta) + \alpha_\text{S,KL} L_\text{S,KL}(\vtheta),
\end{split}
\end{equation*}
%\(L(\theta) = \frac{1}{|\sX'|} \sum_{\hat{x}\in\sX'} \frac{1}{I}\sum_{i=1}^{I} \alpha_{S,A} L_A^{(i)}\left(\hat{x},x^{(i)},\theta\right) + \alpha_{S,C} L_C^{(i)}\left(\hat{x},x^{(i)},\theta\right) + \alpha_{S,\mathcal{H}} L_\mathcal{H}^{(i)}\left(\theta\right) + \alpha_{S,\text{KL}} L_\text{KL}^{(i)}\left(\theta\right)\),
%
where \(\alpha_\text{S,A}\), \(\alpha_\text{S,C}\), \(\alpha_{\text{S},\mathcal{H}}\), \(\alpha_\text{S,KL}\) are constants.

We also use Reinforce~\citep{williams1992simple} to train the Listener. We define the loss \(L_\text{L,A}\) to train the Listener's policy:
%
\begin{equation*}
L_\text{L,A}(\vphi)=-\sg\left(R(\vx,\hat{\vx})-v(\vs;\vphi)\right)\cdot\log\pi_L(\hat{\vx}|\vs;\vphi),
\end{equation*}
%
where cumulative returns is again the game reward \(R(\vx,\hat{\vx})\) since the Listener is in a single-step episode format where the game ends after choosing a candidate, \(\hat{\vx}\in\sC\). Identically to the Speaker, we subtract the Listener critic's value \(v(s;\vphi)\) from the game reward. The critic sub-network optimizes
%
\begin{equation*}
L_\text{L,C}(\vphi)=\left(R(\vx,\hat{\vx})-v(s;\vphi)\right)^2.
\end{equation*}
%
Similarly to the Speaker loss, we add an entropy loss term \(L_{\text{L},\mathcal{H}}(\vphi)\) to encourage exploration. The final Listener loss for a batch of images \(\sX'\) is:
%
\begin{equation*}
L_\text{L}(\vphi) = \frac{1}{|\sX'|} \sum_{x\in\sX'} \alpha_\text{L,A} L_\text{L,A}(\vphi) + \alpha_\text{L,C} L_\text{L,C}(\vphi) + \alpha_{\text{L},\mathcal{H}} L_{L,\mathcal{H}}(\vphi),
\end{equation*}
%\(L(\phi) = \frac{1}{|\sX'|} \sum_{\hat{x}\in\sX'} \frac{1}{I}\sum_{i=1}^{I} \alpha_{L,A} L_A^{(i)}\left(x,\phi\right) + \alpha_{L,C} L_C^{(i)}\left(x,\phi\right) + \alpha_{L,\mathcal{H}} L_\mathcal{H}^{(i)}\left(\phi\right)\),
%
where \(\alpha_\text{L,A}\), \(\alpha_\text{L,C}\), and \(\alpha_{\text{L},\mathcal{H}}\) are constants.

A detailed analysis of the learning strategy, for both agents, can be found in \Cref{app:learn-strat}. Additionally, due to the complexity and non-stationarity of NLG, we define a scheduler for the noise level in the communication channel, during training. Namely, we linearly increase the noise level from \(0\) to \(\lambda\) at the beginning of training. This phase is optional and only helps with data efficiency (we refer to \Cref{app:noise-schedule} for more details).

%Due to the complexity and non-stationarity of the environment, we found several requirements to be necessary to guide the training of both agents towards regions in the parameter space where viable communication protocols emerge, instead of being degenerate.
%Namely, having different learning rates for both agents is essential for convergence, where, in particular, we set the Listener's learning rate to a lower value than the Speaker's counterpart. Thus, we simulate a configuration where the Listener slowly adapts to message changes.
%Namely, we add a pre-train phase to NLG, where we linearly increase the noise level in the communication channel from \(0\) to \(\lambda\). This phase is optional and only helps with data efficiency (we refer to \Cref{app:noise-schedule} for more details).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Evaluation} \label{sec:eval}

We provide an extensive evaluation of NLG and variants. For completeness, we also consider the original architecture proposed by \citet{chaabouni2022emergent} and our novel agent architecure to play the original LG (without message noise) as baselines. In this game variant, our model surpasses the original architecture at a slight cost of data efficiency. This trade-off is expected and fully explained in \cref{sec:eval-comm}. At a glance, this happens because the baseline version can retrieve more information than our implementation, during training. Having a progressive sequence of LG variants enables us to assess how each modification influences the emergent communication protocol learned by the agents.

We continue this section by introducing all LG variants, giving a broader view of each game, agent architectures, and learning strategy. Next, we evaluate the generality of the emerging language for each game variant when providing new and unseen images. We compare LG and NLG variants when testing with and without noise in the communication channel. Additionally, we investigate how the candidate set, \(\sC\), impacts the generalization capabilities of the message protocols. Moreover, we investigate the internal message structure to understand how robust communication protocols emerge when agents play in the NLG. Finally, we perform an out-of-distribution evaluation to discern how each variant reacts to novel forms of noise. We always report results using the average (plus SD) over 10 different seeds.

Due to space constraints, \Cref{app:add-results} contains the results obtained in all experiments, for both ImageNet~\citep{ILSVRC15} and CelebA~\citep{liu2015faceattributes} datastes, used to devise the analyses detailed in this section. Additionally, we report a supplementary evaluation to assess the capacity of each game variant to adapt to new tasks in a transfer learning manner, called \emph{ease and transfer learning} (ETL)~\citep{chaabouni2022emergent} (see \Cref{app:etl}). This supplementary evaluation gives yet another frame of reference to evaluate the generality and robustness of the learned languages. Finally, we also refer to the appendix for further details regarding related work (\Cref{app:rw}), every game variant (\Cref{app:lg}), model architectures (\Cref{app:arch}), and datasets used (\Cref{app:data}).

%the results obtained evaluate all game variants in several transfer learning tasks, named \emph{ETL}.
%Due to space constraints, we provide additional results in \Cref{app:add-results,app:etl}, where we present the results obtained evaluate all game variants in several transfer learning tasks, named \emph{ETL}. This supplementary evaluation gives yet another frame of reference to evaluate the generality and robustness of the learned languages. Finally, we also refer to the appendix for further implementation details regarding every game variant (\Cref{app:lg}), model architectures (\Cref{app:arch}), and datasets used (\Cref{app:data}).

\subsection{Lewis Game Variants} \label{sec:eval-variants}
We briefly report essential aspects of each game variant, while referring to supplementary information when necessary. We consider three variants of the LG, all of which share the same Speaker architecture. The Listener architecture differs in all games. We refer to \Cref{app:arch} for a detailed description of the implementation of these architectures. Additionally, all variants except for LG (S) are a contribution of this work. The LG variants considered are:

\begin{itemize}
    \item LG (S): Original LG variant introduced in~\citet{chaabouni2022emergent}. The Listener trains under supervised data by using InfoNCE loss~\citep{oord2018representation,dessi2021interpretable} to find similarities between the message and the correct candidate, see \Cref{app:list-arch}.
    \item LG (RL): LG with a deterministic communication channel (no noise) where both agents implement RL architectures. The only semi-supervised information given to both agents is whether the game ended successfully or not. We refer to \Cref{sec:meth-arch,sec:meth-learn} for a comprehensive description of the agents' architectures and learning strategies, respectively.
    \item NLG: LG variant introduced in \Cref{sec:meth-nlg}, where we apply an external environmental pressure by adding noise to the message during transmission. Both agents function as RL agents, as in LG (RL). Agents' architectures and learning strategies appear in \Cref{sec:meth-arch,sec:meth-learn}, respectively. For an overall understanding of NLG, we define 3 different versions: NLG (0.25), NLG (0.5), and NLG (0.75); where NLG (\(x\)) means that, during training, we fix the noise threshold at \(\lambda=x\).
    %\item ILG: ILG has the same game environment as the SLG counterpart. We modify the Listener architecture, particularly the actor and value heads, to be able to model the idk action. Note that ILG continues to be a one-round game.
    %\item MRILG: The most general and the target game introduced in \Cref{sec:meth-mrilg}. Note that this is the only game where the \emph{idk} action has an extended impact on the game itself since it allows the agents to play another round.
\end{itemize}

\subsection{Robust Communication Protocols} \label{sec:eval-comm}

This section analyzes the performance of all LG variants described above. Since there is the possibility to apply different hyper-parameters depending on the current phase (training or testing phase), we define two extra variables, \(\lambda_\text{test}\) and \(\sC_\text{test}\), to define the noise threshold and candidate set applied during the test phase, respectively.

Starting by comparing LG (S) with LG (RL), we can see an apparent performance boost for the LG when the Listener is an RL agent. \Cref{fig:compare-lg} shows that, during training, the RL version performs better than the supervised version. Equivalent results occur in the testing phase. From \Cref{fig:compare-lg1,fig:compare-lg2}, and focusing on the results obtained with a deterministic communication channel, \(\lambda_\text{test}=0\), the RL version surpasses the accuracy achieved by the supervised counterpart. This performance gap becomes more predominant as we increase game complexity, as seen in \Cref{sec:eval-cand}. From \Cref{fig:compare-lg}, we also observe a trade-off between performance and sample efficiency, where the RL version is less sample efficient. We can trace these differences back to the loss function employed by each version. For instance, the supervised version employs the InfoNCE loss (\Cref{app:arch-list-ss}), which we can see as a Reinforce variant with only a policy to optimize and, particularly, with access to an oracle giving information about which action (candidate) is the right one for each received message. As such, the Listener (S) can efficiently learn how to map messages to the correct candidates. On the other hand, the RL version has no access to such oracle and needs to interact with the environment to build this knowledge. The decrease in sample efficiency from supervised to RL is, therefore, a natural phenomenon. Nonetheless, the RL version introduces a critic loss term whose synergy with the policy loss term helps to improve the final performance when compared to the supervised version.

One disadvantage of employing, at inference time, the communication protocols learned by playing default LG variants (LG (S) and LG (RL)) is that they are not robust to deal with message perturbations. Since agents train only with perfect communication, they never experience noisy communication. When testing the performance of LG (S) and LG (RL) with noisy communication channels \(\lambda_\text{test}>0\),  we observe a noticeable dominance of RL against S. Nonetheless, there is a massive drop in performance for both variants compared to the noiseless case \(\lambda_\text{test}=0\), see \Cref{fig:compare-lg1,fig:compare-lg2}.

Conversely, NLG puts agents in a more complex environment where only random fractions of the message are visible during training time. Despite such modifications, the pair of agents can still adapt to the environment and learn robust communication protocols that handle both types of messages (with and without noise). We notice equivalent accuracy performance for NLG and LG (RL) when testing with deterministic communication channels, see \Cref{fig:compare-lg1,fig:compare-lg2}. Notably, every NLG version only suffers a negligible performance loss when testing with \(\lambda_\text{test}=0.25\). This loss starts to be more noticeable at higher noise levels, where the accuracy drops to around 80\%, and further to the interval between 30\%-40\% when \(\lambda_\text{test}\) is \(0.5\) and \(0.75\), respectively. Still, NLG is considerably more effective than LG (S) and LG (RL) when communicating in noisy environments, as seen in the considerable performance gap visible in each tested noise level \(\lambda_\text{test}\). This increased performance suggests that, in NLG, agents can encode redundant information where communication is still functional when random parts of the message are hidden. Additionally, the performance obtained for each test threshold \(\lambda_\text{test}\) is similar for every NLG version. As such, each version displays similar capacities to handle noise in the communication channel, independent of the noise threshold \(\lambda\) applied during training. Please refer to \Cref{app:add-results} for additional results on the ImageNet and CelebA datasets.

\subsubsection{Comparing Different Noise Levels}
Comparing the different variants of NLG, we observe that the mean accuracy obtained for NLG (0.75) is slightly lower than NLG (0.25) and NLG (0.5) when we set \(\lambda_\text{test}\) to \(0\) or \(0.25\), see \Cref{fig:compare-lg1,fig:compare-lg2}. When \(\lambda_\text{test}=0.5\), NLG (0.5) performs slightly better than its counterparts. Finally, NLG (0.5) and NLG (0.75) seem to perform slightly better than NLG (0.25) is the extreme noise case (\(\lambda_\text{test}\) is \(0.75\)).

Henceforth, having a threshold of \(\lambda=0.5\) during training appears to give a good balance for the pair of agents to develop a communication protocol that can effectively act in a broad range of noise levels, even when there is no noise during communication.

\begin{figure}[!t]
  \centering
  \includegraphics[width=0.71\linewidth]{figures/compare_lg}
  \caption{Training accuracy for LG (S) and LG (RL). Trained on ImageNet dataset and  \(|\sC|=1024\).}
  \label{fig:compare-lg}
  %\vspace{-2ex}
\end{figure}

\begin{figure*}[!t]
  \begin{minipage}[!t]{1\textwidth}
  \centering
  \includegraphics[width=0.55\linewidth]{figures/legend}
  \caption*{}
  %\vspace{-8ex}
  \end{minipage}
  %\vspace{-4ex}
  \begin{minipage}[!t]{0.49\textwidth}
  \vspace{-7ex}
  \centering
  \includegraphics[width=0.81\linewidth]{figures/acc_test_imagenet_1024_1024}
  \caption{Test accuracy for all variants with \(|\sC_\text{test}|=1024\). Trained on ImageNet and \(|\sC|=1024\). The \(x\)-axis denotes \(\lambda_\text{test}\).}
  \label{fig:compare-lg1}
  %\vspace{-2ex}
  \end{minipage}
  \hfill
  \begin{minipage}[!t]{0.49\textwidth}
  \vspace{-7ex}
  \centering
  \includegraphics[width=0.81\linewidth]{figures/acc_test_imagenet_1024_4096}
  \caption{Test accuracy for all variants with \(|\sC_\text{test}|=4096\). Trained on ImageNet and \(|\sC|=1024\). The \(x\)-axis denotes \(\lambda_\text{test}\).}
  \label{fig:compare-lg2}
  %\vspace{-2ex}
  \end{minipage}
\end{figure*}

%\subsubsection{Comparing Different Levels of Noise} \label{sec:eval_noise}
%Examining the results obtained from NLG against LG (RL) when evaluating using a noiseless channel, we observe no loss in inference capabilities for any noise level applied during training, as shown in \cref{table:lg_imagenet_eval_noise}. Additionally, every NLG version (trained with different noise level thresholds) obtained similar performance at test time for every test threshold \(\lambda_\text{test}\) analyzed. As such, each version displays similar capacities to handle noise in the communication channel, independent of the noise threshold applied during training.

%Additionally, since we force the evaluation game to have at most one round, we observe no clear advantage when training agents in the multi-round game against NLG.
%From \cref{table:lg_imagenet_noise_eval_noise}, we observe that the test accuracy decreases as the noise increases. Additionally, when training with \(\lambda=0.25\), the performance loss in test accuracy is almost negligible compared to the results in \cref{table:lg_imagenet_eval_noise}. The agents can effectively sustain this noise threshold without losing performance. One possible reason for this behavior is that messages are overly lengthy, where even having just \(75\%\) of the tokens is still enough to encode useful information regarding the Speaker's input.

%Based on such a description, we now focus our analysis on the noise level \(\lambda=0.75\). For this \(\lambda\), the agents are in a mismatched condition where the Speaker assumes all tokens are valuable to give meaning to the message conveyed. However, the Listener effectively uses only a few of them when interpreting the message. In this case, agreeing on a communication protocol becomes hard. Looking at \cref{table:lg_imagenet_noise_eval_noise}, we see this discrepancy between performance obtained with \(\lambda=0.75\) compared to \(\lambda=0.25\) or \(\lambda=0.5\) for all games. Nonetheless, we see some improvements when training agents in MRILG compared to NLG, indicating that the Listener can extract more knowledge from multiple messages that map to the same image.

%\subsubsection{Comparing Different Rewards for the IDK Action} \label{eval:idk}

%Focusing on the \emph{idk} reward's impact, \(\nu\), we observe a clear advantage of using a lower value for the \emph{idk} reward, see \Cref{table:lg_imagenet_0_eval_idk,table:lg_imagenet_noise_eval_idk}. We observe similar results when the \(\nu=\{-0.5,-0.2\}\). On the other hand, having \(\nu\) close to \(0\) decreases test accuracy considerably. In addition, the training is unstable for this \emph{idk} reward level since a considerably high variance appears in all results. As such, we discern that adding more cost to the \emph{idk} action is a clear strategy to ensure the Listener does not exploit this action and only uses it when there is high uncertainty about the correct candidate.

\begin{comment}
    
\begin{table}[t]
\linespread{0.6}\selectfont\centering
\begin{minipage}[t]{0.48\linewidth}
    
\caption{Test accuracy with SD for different game variants, using ImageNet dataset and over 10 seeds. During test \(\lambda_{\text{test}}\) is set to \(0\).}
\label{table:lg_imagenet_0_eval_idk}
\begin{tabular}[t]{lrrrr}%{LRLRRR}
\toprule
Game & \(\lambda\) & \(\nu\) & \multicolumn{2}{c}{\(|\sC|\) (test)} \\[1ex]\cmidrule(r){4-5}
 &  &  & \multicolumn{1}{c}{\(1024\)} & \multicolumn{1}{c}{\(4096\)} \\
\midrule
%ILG & 0.5 & -0.5 & \longcell{0.99\\{\tiny(0.00)}} & \longcell{0.96\\{\tiny(0.01)}} \\[2.2ex]
%ILG & 0.5 & 0.0 & \longcell{0.99\\{\tiny(0.00)}} & \longcell{0.96\\{\tiny(0.00)}} \\[2.2ex]
%ILG & 0.5 & 0.5 & \longcell{0.99\\{\tiny(0.00)}} & \longcell{0.97 \\ {\tiny(0.01)}} \\[2.2ex]
MRILG & 0.5 & -0.5 & \longcell{0.99 \\ {\tiny(0.00)}} & \longcell{0.96 \\ {\tiny(0.00)}} \\[2.2ex]
MRILG & 0.5 & -0.2 & \longcell{0.99 \\ {\tiny(0.00)}} & \longcell{0.97 \\ {\tiny(0.00)}} \\[2.2ex]
MRILG & 0.5 & -0.05 & \longcell{0.79 \\ {\tiny(0.42)}} & \longcell{0.72 \\ {\tiny(0.38)}} \\
\bottomrule
\end{tabular}
\end{minipage}
\hfill
\begin{minipage}[t]{0.48\linewidth}
    
%\end{table}

%\begin{table}[t]
\centering
\caption{Test accuracy with SD for different game variants, using ImageNet dataset and over 10 seeds. During test \(\lambda_{\text{test}}\) is set to \(0.5\).}
\label{table:lg_imagenet_noise_eval_idk}
\begin{tabular}[t]{lrrrr}
\toprule
Game & \(\lambda\) & \(\nu\) & \multicolumn{2}{c}{\(|\sC|\) (test)} \\[1ex]\cmidrule(r){4-5}
 &  &  & \multicolumn{1}{c}{\(1024\)} & \multicolumn{1}{c}{\(4096\)} \\
\midrule
%ILG & 0.5 & -0.5 & \longcell{0.82 \\ {\tiny(0.01)}} & \longcell{0.68 \\ {\tiny(0.02)}} \\[2.2ex]
%ILG & 0.5 & 0.0 & \longcell{0.83 \\ {\tiny(0.01)}} & \longcell{0.68 \\ {\tiny(0.02)}} \\[2.2ex]
%ILG & 0.5 & 0.5 & \longcell{0.81 \\ {\tiny(0.02)}} & \longcell{0.67 \\ {\tiny(0.03)}} \\[2.2ex]
MRILG & 0.5 & -0.5 & \longcell{0.81 \\ {\tiny(0.01)}} & \longcell{0.66 \\ {\tiny(0.02)}} \\[2.2ex]
MRILG & 0.5 & -0.2 & \longcell{0.80 \\ {\tiny(0.01)}} & \longcell{0.65 \\ {\tiny(0.02)}} \\[2.2ex]
MRILG & 0.5 & -0.05 & \longcell{0.62 \\ {\tiny(0.33)}} & \longcell{0.50 \\ {\tiny(0.26)}} \\
\bottomrule
\end{tabular}
\end{minipage}
\vspace{-2ex}
\end{table}
\end{comment}

\subsubsection{Scaling the Number of Candidates} \label{sec:eval-cand}

We train every game variant with different candidate sizes \(|\sC|\). We scale \(|\sC|\) from 16 to 1024, using a ratio of 4. At test time, we evaluate all experiments using larger sizes, for the candidates set, to inspect generalization capabilities. In our case, we use \(|\sC_\text{test}|=1024\) and \(|\sC_\text{test}|=4096\). Looking at \cref{table:lg_imagenet_0_test_cand,table:lg_imagenet_noise_test_cand}, we can see an evident generalization boost when the number of candidates increases for every game. We posit that increasing the game's difficulty (increasing the number of candidates) helps the agents to generalize. As the candidates' set gets additional images, the input diversity increases, which affects how agents encode and interpret more information to distinguish the correct image from all others. Even when adding noise, the agents can quickly adapt to such changes conveying information in such a way as to repress the noise mechanism. We argue that agents have two correlated ways to achieve such adaptation:
%
\begin{enumerate*}
  \item send redundant information regarding specific features of the image; and
  \item create a spatial mapping from the image to the message space.
\end{enumerate*}

We note that as \(|\sC|\) increases, the test performance also increases, but at a smaller scale. As an example, consider LG (RL) variant when \(\lambda_\text{test}=0\) and \(|\sC_\text{test}|=4096\). In this case, the test performance gap between \(|\sC|=16\) and \(|\sC|=64\) is \(0.4\), and only \(0.03\) between \(|\sC|=256\) and \(|\sC|=1024\), see \Cref{table:lg_imagenet_0_test_cand}. Regarding NLG, the accuracy starts lower for smaller candidate sizes, e.g., \(0.27\) when \(|\sC|=16\), against \(0.67\) for the LG (RL) counterpart. Nonetheless, as the candidate set size increases, the noise effect becomes less predominant and the NLG's performance reaches the same level as in LG (RL), both achieving an accuracy of \(0.97\) when \(|\sC|=1024\).

% We note that as \(|\sC|\) increases, the test performance also increases, but at a smaller scale, e.g., the test gap between LG (RL) with \(|\sC|=16\) and \(|\sC|=64\) is \(0.4\) and only \(0.03\) between \(|\sC|=256\) and \(|\sC|=1024\), for the noiseless case and \(|\sC_\text{test}|=4096\), see \Cref{table:lg_imagenet_0_test_cand}. Regarding NLG, the accuracy starts lower for smaller candidate sizes, e.g., \(0.27\) when \(|\sC|=16\), against \(0.67\) for the LG (RL). Nonetheless, as the candidate set size increases, the noise effect becomes less predominant and the NLG's performance reaches the same level as in LG (RL), both achieving an accuracy of \(0.97\) when \(|\sC|=1024\).

Equivalently to the results introduced in the beginning of \Cref{sec:eval-comm}, we observe low accuracy for LG (RL) when testing with noisy communication channels (\Cref{table:lg_imagenet_noise_test_cand}), regardless of the candidate size. In respect to NLG, there is an apparent increase in performance as \(|\sC|\) increases during training. Additionally, the performance gap between consecutive candidate set sizes is approximately the same (between \(0.14\) and \(0.21\), when \(|\sC_\text{test}|=4096\)).

%Looking in detail, we note that the SS version of LG has the most significant gap in test performance when increasing the number of candidates. The LG (RL) and the other noisy games also exhibit the same behavior on a smaller scale since the test accuracy is already considerably higher than LG (S). The noisy games have comparable results to LG (RL) despite the increased complexity in the environments (addition of the noisy communication channel and multi-round settings). %Having a diverse set of images seems to be a key factor in creating languages that generalize better. Even when adding noise to the messages, the agents quickly adapt to such changes, conveying information that overcomes the noise mechanism. We argue that agents have two interrelated ways to achieve such adaptation: send redundant information regarding specific features of the image and create a spatial mapping from the image input to the message space.

\begin{table*}[t]
%\vspace{-2ex}
\linespread{0.6}\selectfont\centering

\begin{minipage}[t]{0.48\linewidth}
\centering
\caption{Test accuracy including SD, when \(\lambda_{\text{test}}=0\), for LG (RL) and NLG, on ImageNet dataset.}
\label{table:lg_imagenet_0_test_cand}
\begin{tabular}{lrrrr}
\toprule
Game & \(\lambda\) & \(|\sC|\) & \multicolumn{2}{c}{\(|\sC|\) (test)} \\[1ex]\cmidrule(r){4-5}
 &  &  & \multicolumn{1}{c}{\(1024\)} & \multicolumn{1}{c}{\(4096\)} \\
\midrule
%LG {\scriptsize(S)} & 0 & \multicolumn{1}{c}{-} & 16 & \longcell{0.61\\{\tiny(0.04)}} & \longcell{0.35\\{\tiny(0.04)}} \\
%LG {\scriptsize(S)} & 0 & \multicolumn{1}{c}{-} & 64 & \longcell{0.76\\{\tiny(0.06)}} & \longcell{0.52\\{\tiny(0.08)}} \\
%LG {\scriptsize(S)} & 0 & \multicolumn{1}{c}{-} & 256 & \longcell{0.88\\{\tiny(0.03)}} & \longcell{0.71\\{\tiny(0.06)}} \\
%LG {\scriptsize(S)} & 0 & \multicolumn{1}{c}{-} & 1024 & \longcell{0.96\\{\tiny(0.02)}} & \longcell{0.88\\{\tiny(0.04)}} \\
LG {\scriptsize(RL)} & 0 & 16 & \longcell{0.67\\{\tiny(0.04)}} & \longcell{0.39\\{\tiny(0.04)}} \\[2.2ex]
LG {\scriptsize(RL)} & 0 & 64 & \longcell{0.93\\{\tiny(0.01)}} & \longcell{0.79\\{\tiny(0.03)}} \\[2.2ex]
LG {\scriptsize(RL)} & 0 & 256 & \longcell{0.98\\{\tiny(0.00)}} & \longcell{0.94\\{\tiny(0.01)}} \\[2.2ex]
LG {\scriptsize(RL)} & 0 & 1024 & \longcell{0.99\\{\tiny(0.00)}} & \longcell{0.97\\{\tiny(0.00)}} \\[2.2ex]
NLG & 0.5 & 16 & \longcell{0.55\\{\tiny(0.03)}} & \longcell{0.27\\{\tiny(0.02)}} \\[2.2ex]
NLG & 0.5 & 64 & \longcell{0.87\\{\tiny(0.01)}} & \longcell{0.67\\{\tiny(0.03)}} \\[2.2ex]
NLG & 0.5 & 256 & \longcell{0.98\\{\tiny(0.00)}} & \longcell{0.91\\{\tiny(0.01)}} \\[2.2ex]
NLG & 0.5 & 1024 & \longcell{0.99\\{\tiny(0.00)}} & \longcell{0.97\\{\tiny(0.00)}} \\[2.2ex]
%MRILG & 0.5 & -0.5 & 16 & \longcell{0.56\\{\tiny(0.04)}} & \longcell{0.29\\{\tiny(0.04)}} \\[2.2ex]
%MRILG & 0.5 & -0.5 & 64 & \longcell{0.90\\{\tiny(0.01)}} & \longcell{0.72\\{\tiny(0.03)}} \\[2.2ex]
%MRILG & 0.5 & -0.5 & 256 & \longcell{0.98\\{\tiny(0.00)}} & \longcell{0.92\\{\tiny(0.01)}} \\[2.2ex]
%MRILG & 0.5 & -0.5 & 1024 & \longcell{0.99\\{\tiny(0.00)}} & \longcell{0.96\\{\tiny(0.00)}} \\
\bottomrule
\end{tabular}
\end{minipage}
%\end{table}
\hfill
%
\begin{minipage}[t]{0.48\linewidth}
    
%\begin{table}[t]
\centering
\caption{Test accuracy including SD, when \(\lambda_{\text{test}}=0.5\), for LG (RL) and NLG, on ImageNet dataset.}
\label{table:lg_imagenet_noise_test_cand}
\begin{tabular}{lrrrr}
\toprule
Game & \(\lambda\) & \(|\sC|\) & \multicolumn{2}{c}{\(|\sC|\) (test)} \\[1ex]\cmidrule(r){4-5}
 &  &  & \multicolumn{1}{c}{\(1024\)} & \multicolumn{1}{c}{\(4096\)} \\
\midrule
%LG {\scriptsize(S)} & 0 & \multicolumn{1}{c}{-} & 16 & \longcell{0.03\\{\tiny(0.01)}} & \longcell{0.01\\{\tiny(0.00)}} \\
%LG {\scriptsize(S)} & 0 & \multicolumn{1}{c}{-} & 64 & \longcell{0.04\\{\tiny(0.01)}} & \longcell{0.02\\{\tiny(0.01)}} \\
%LG {\scriptsize(S)} & 0 & \multicolumn{1}{c}{-} & 256 & \longcell{0.06\\{\tiny(0.02)}} & \longcell{0.03\\{\tiny(0.01)}} \\
%LG {\scriptsize(S)} & 0 & \multicolumn{1}{c}{-} & 1024 & \longcell{0.05\\{\tiny(0.01)}} & \longcell{0.03\\{\tiny(0.01)}} \\
LG {\scriptsize(RL)} & 0 & 16 & \longcell{0.03\\{\tiny(0.01)}} & \longcell{0.01\\{\tiny(0.01)}} \\[2.2ex]
LG {\scriptsize(RL)} & 0 & 64 & \longcell{0.09\\{\tiny(0.07)}} & \longcell{0.05\\{\tiny(0.03)}} \\[2.2ex]
LG {\scriptsize(RL)} & 0 & 256 & \longcell{0.11\\{\tiny(0.06)}} & \longcell{0.06\\{\tiny(0.04)}} \\[2.2ex]
LG {\scriptsize(RL)} & 0 & 1024 & \longcell{0.11\\{\tiny(0.02)}} & \longcell{0.06\\{\tiny(0.01)}} \\[2.2ex]
NLG & 0.5 & 16 & \longcell{0.32\\{\tiny(0.02)}} & \longcell{0.14\\{\tiny(0.01)}} \\[2.2ex]
NLG & 0.5 & 64 & \longcell{0.57\\{\tiny(0.02)}} & \longcell{0.33\\{\tiny(0.02)}} \\[2.2ex]
NLG & 0.5 & 256 & \longcell{0.73\\{\tiny(0.01)}} & \longcell{0.54\\{\tiny(0.02)}} \\[2.2ex]
NLG & 0.5 & 1024 & \longcell{0.82\\{\tiny(0.01)}} & \longcell{0.68\\{\tiny(0.02)}} \\[2.2ex]
%MRILG & 0.5 & -0.5 & 16 & \longcell{0.33\\{\tiny(0.03)}} & \longcell{0.14\\{\tiny(0.02)}} \\[2.2ex]
%MRILG & 0.5 & -0.5 & 64 & \longcell{0.59\\{\tiny(0.02)}} & \longcell{0.35\\{\tiny(0.02)}} \\[2.2ex]
%MRILG & 0.5 & -0.5 & 256 & \longcell{0.75\\{\tiny(0.02)}} & \longcell{0.55\\{\tiny(0.03)}} \\[2.2ex]
%MRILG & 0.5 & -0.5 & 1024 & \longcell{0.81\\{\tiny(0.01)}} & \longcell{0.66\\{\tiny(0.02)}} \\
\bottomrule
\end{tabular}
\end{minipage}
\vspace{-2ex}
\end{table*}

\begin{figure*}[!t]
  \begin{minipage}[!t]{1\textwidth}
  \centering
  \includegraphics[width=0.55\linewidth]{figures/legend}
  \caption*{}
  %\vspace{-8ex}
  \end{minipage}
  \begin{minipage}[!t]{0.49\textwidth}
  \vspace{-7ex}
  \centering
  \includegraphics[width=0.81\linewidth]{figures/acc_testmessage_imagenet_1024_1024}
  \caption{Mean test accuracy for all LG variants, trained with \(|\sC|=1024\) and on ImageNet dataset. At test time, \(|\sC_\text{test}|=1024\). We report 10 different combinations of masked tokens.}
  \label{fig:compare-lg-mess-1024}
  %\vspace{-2ex}
  \end{minipage}
  \hfill
  \begin{minipage}[!t]{0.49\textwidth}
    \vspace{-7ex}
  \centering
  \includegraphics[width=0.81\linewidth]{figures/acc_testmessage_imagenet_1024_4096}
  \caption{Mean test accuracy for all LG variants, trained with \(|\sC|=1024\) and on ImageNet dataset. At test time, \(|\sC_\text{test}|=4096\). We report 10 different combinations of masked tokens.}
  \label{fig:compare-lg-mess-4096}
  %\vspace{-2ex}
  \end{minipage}
\end{figure*}

\begin{figure*}[!t]
  \begin{minipage}[!t]{1\textwidth}
  \centering
  \includegraphics[width=0.55\linewidth]{figures/legend}
  \caption*{}
  %\vspace{-8ex}
  \end{minipage}
  \begin{minipage}{0.49\textwidth}
  \vspace{-7ex}
  \centering
  \includegraphics[width=0.81\linewidth]{figures/acc_testinput_imagenet_1024_1024}
  \caption{Mean test accuracy for all variants with \(|\sC_\text{test}|=1024\). Trained on ImageNet and \(|\sC|=1024\). During test noise is added to the inputs (\Cref{sec:eval-ext-noise}). The \(x\)-axis denotes \(\lambda_\text{test}\).}
  \label{fig:compare-lg-noisy-input-1024}
  %\vspace{-2ex}
  \end{minipage}
  \hfill
  \begin{minipage}{0.49\textwidth}
  \vspace{-7ex}
  \centering
  \includegraphics[width=0.81\linewidth]{figures/acc_testinput_imagenet_1024_4096}
  \caption{Mean test accuracy for all variants with \(|\sC_\text{test}|=4096\). Trained on ImageNet and \(|\sC|=1024\). During test noise is added to the inputs (\Cref{sec:eval-ext-noise}). The \(x\)-axis denotes \(\lambda_\text{test}\).}
  \label{fig:compare-lg-noisy-input-4096}
  %\vspace{-2ex}
  \end{minipage}
\end{figure*}

\subsection{Message Structure Analysis} \label{sec:eval-mess-struct}

With the aim of addressing how NLG variations develop robust communication protocols, we propose to analyze the message structure of the language protocols. The NLG includes additional adverse pressures where the communication channel seems unreliable. In order to overcome the noise introduced by the faulty channel, the pair of agents must find alternative ways to coordinate how to send information. Moreover, since the only feedback received by the Speaker is the game's outcome, the new coordination mechanism becomes essential to complete the game with a high success rate. The most intuitive behavior for the Speaker focuses on developing implicit repair mechanisms where messages incorporate redundant information. As such, even if a subset of the message tokens become masked, the Listener can still parse the remaining message and select the correct candidate, making the communication robust to the noise being introduced by the faulty channel.

To test our assumption, we indirectly analyze the internal structure of the message being transmitted in all games. Since we want to examine the existence of redundancy in the communication protocol, we propose an evaluation where we iteratively increase the number of masked tokens and retrieve the performance obtained. The range of masked tokens spreads from \(0\) to \(N/2=5\) (half of the tokens). Additionally, when selecting the number of tokens to conceal, we evaluate the performance over ten different combinations of masked tokens (except when no tokens are masked), ensuring the results represent the average case. As such, this analysis allows us to examine, in greater detail, how the accuracy changes as we increase the number of masked tokens iteratively. As shown in \Cref{fig:compare-lg-mess-1024,fig:compare-lg-mess-4096}, the NLG variations only decrease slightly in accuracy as the number of masked tokens increases, conveying that our assumption is accurate and messages contain redundant information. Consequently, the Listener still contains enough information to select the right candidate, even with partial messages. On the other hand, for the deterministic variations, LG (S) and LG (RL), we observe a faster decrease in performance as more tokens are masked, conveying that every token is essential for the Listener to infer the right candidate. Furthermore, we highlight that the average accuracy obtained by the NLG (0.5) variant, with five tokens masked, is similar to the performance obtained by LG (RL) when masking a single token, which is around 75\% (\Cref{fig:compare-lg-mess-4096}). This particular result illustrates how much noise both variants can manage for a particular and similar performance level. As such, NLG (0.5) can deal with \texttt{5x} more noise than LG (RL) when discriminating the most amount of images, \(|\sC_\text{test}|=4096\). Please refer to \Cref{app:add-results} for additional results on the ImageNet and CelebA datasets.

In addition to the results presented above, we observe an interesting occurrence when communication protocols emerge with deterministic channels, as in the case of LG (S) and LG (RL). Our analysis shows that when masking a single token, the first token of the message seems to carry more information than the others, or, at least, is crucial to derive meaning from the rest of the message. We show these results in the \Cref{app:add-results}, where the drop in performance can, in some cases, drop down around 10\% when the first token is masked in opposition to mask any other token. The drop in performance is more noticeable as the number of candidates given to the Listener increases, where it seems the first token plays a crucial role to reduce the number of final choices to consider.

\subsection{External Noise Interference} \label{sec:eval-ext-noise}

In this experiment, we test the capability of the emergent communication protocols trained in the games presented above to adapt to new game variations where we focus on adding noise to conceal information in other components of the input. In this regard, we model a new noise dynamic by adding random information to the objects to discriminate:
%
\begin{enumerate*}
  \item the target image provided to the Speaker, and
  \item the candidates given to the Listener.
\end{enumerate*}
%
We sample noise from a Gaussian distribution and add it to the output of the frozen image encoder. As such, in this game adaptation, we modify the target image to \(\vx'=f(\vx)+\vvarepsilon\) and the candidates to \(\vx_i=f(\vx_i)+\vvarepsilon\), where \(\vvarepsilon\sim\mathcal{N}(\vzero,\sigma^2\mI)\) and \(\vx_i\in\sC\). Unless otherwise noted, we set \(\sigma=1\).

We can view this new modification as an out-of-distribution task since, during training, agents only deal with noise in the communication channel, where the obstruction happens by masking some of the message tokens. \Cref{fig:compare-lg-noisy-input-1024,fig:compare-lg-noisy-input-4096} depict the results obtained for all game variants at inference time when we introduce noise to the input images. When testing with a deterministic channel (\(\lambda_\text{test}=0\)), LG (RL) and all NLG versions have similar performance, where the mean accuracy is around \(0.7\) and \(0.5\) for 1024 and 4096 candidates, respectively. We can see a slight degradation in performance across all variants compared to the previous experiment setting in \Cref{sec:eval-comm} (images without any perturbations). Nonetheless, these results indicate a positive transfer capability where the language protocols are general enough, allowing effective agent reasoning even with partially hidden input distributions. Additionally, we notice that adding noise in the communication channel during training (NLG variant) does not provide improved benefits to the communication protocol to deal with other noise types, as there is no gain in performance when \(\lambda_\text{test}=0\), comparing against LG (RL).

For the experiments where noise is also present in the communication channel, \(\lambda_\text{test}>0\), we observe similar results as in \Cref{sec:eval-comm}, where NLG versions vastly surpass both LG (RL) and LG (S) since the former emergent protocols can efficiently retrieve applicable information to reason about, from the noisy messages. Finally, the results obtained by LG (S) were considerably lower than all other variants, again claiming the superiority of having a Listener as an RL agent, see \Cref{sec:eval-comm}. Please refer to \Cref{app:add-results} for additional results on the ImageNet and CelebA datasets.

%We test the communication protocol's ability to act in the presence of noise in other parts of the input. In this regard, we model a new noise dynamic by adding random information to the objects to discriminate: the target image provided to the Speaker and the candidates given to the Listener. We sample noise from a Gaussian distribution and add it to the output of the frozen image encoder. As such, in this game adaptation, we modify the target image to \(\vx'=f(\vx)+\vvarepsilon\) and the candidates to \(\vx_i=f(\vx_i)+\vvarepsilon\), where \(\vvarepsilon\sim\mathcal{N}(\vzero,\sigma^2\mI)\) and \(\vx_i\in\sC\). Unless otherwise noted, we set \(\sigma=1\) in our experiments.

%We test agents' capacity to deal with other kinds of noise in a transfer learning capacity, in which this type of noise is never seen by the agents during training. We model this new noise dynamic by adding noise to the inputs of both agents, this means the target image provided to the Speaker and candidates. This means that the noise types are independent since LG (RL) has the same accuracy as NLG (0.25) and NLG (0.5) when noise channel is 0, which means training with noise does not provide any benefit to provide generality on another noise types.



\section{Conclusion \& Future Work}

In this work, we focus on designing agent systems that can learn language protocols without prior knowledge, where communication evolves grounded on experience to solve the task at hand. We explore EC from a language evolution perspective to analyze a particular linguistic concept called conversational repair. Conversational repair appears in human languages as a mechanism to detect and resolve miscommunication and misinformation during social interactions. Mainly, we focus on the implicit repair mechanism, where the interlocutor sending the information deliberately communicates in such a way as to prevent misinformation and avoid future interactions on correcting it. Our analysis shows that implicit conversational repair can also emerge in artificial designs when there is enough disruptive environmental pressure, where sending redundant information facilitates solving the task more effectively.

For future work, several ideas can be explored. One possible research idea passes to merge explicit and implicit repair mechanisms. In this scenario, agents would have to coordinate between using implicit mechanisms to prevent misinformation or starting a posterior dialogue (an explicit mechanism) when the Listener cannot extract useful information from the message sent. We argue that this coordination mechanism can emerge naturally in sufficiently complex environments, where the Listener develops the capacity to leverage the likelihood of success and the cost of playing more communication rounds. Another exciting research direction focuses on developing a universal repair mechanism system with the objective of merging various language protocols, grounded in different tasks, into a universal and general language protocol, focusing on the capability to be used by new agents and in out-of-distribution tasks.

%In this work, we focus on creating agent systems that can develop emergent languages without any prior knowledge. We additionally pay particular attention to how to safely employ such systems in the real world by adding additional robustness to uncertain perturbations in the communication channel, emulating noise in most physical (real-world) channels. Following this motivation, we extend the LG, arriving at more complex variations to add robustness to the communication protocol. Such modifications include introducing a novel Listener architecture based on Reinforce~\citep{hochreiter1997long} and including a noisy communication channel. The communication protocols that emerged from the new LG variants perform similarly to the original LG using deterministic channels. Additionally, when communication happens over noisy channels, the former protocols show resilience to such random perturbation, surpassing the performance of the original LG counterparts.

%As future work, we could evaluate different types of noise induced in the communication channel, like changing tokens for others of the same vocabulary, or even using an additional agent to control what tokens to hide. One could also employ the agents' architectures in other, more complex, environments with long-horizon rewards, where the Listener intentionally asks for another message from the speaker when evaluating which action to take. Additionally, as we emphasize in the last paragraph of \Cref{sec:meth-learn}, a deeper study is encouraged to understand the conditions necessary for the agents learn how to communicate and agree on a communication protocol.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% The acknowledgments section is defined using the "acks" environment
%%% (rather than an unnumbered section). The use of this environment 
%%% ensures the proper identification of the section in the article 
%%% metadata as well as the consistent spelling of the heading.

\begin{acks}
This work was supported by national funds through Funda√ß√£o para a Ci√™ncia e a Tecnologia (FCT) with reference UIDB/50021/2020 - DOI: 10.54499/UIDB/50021/2020, project Center for Responsible AI with reference C628696807-00454142, and project RELEvaNT PTDC/CCI-COM/5060/2021. The first author acknowledges the FCT PhD grant 2022.14163.BD.
\end{acks}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% The next two lines define, first, the bibliography style to be 
%%% applied, and, second, the bibliography file to be used.

\bibliographystyle{ACM-Reference-Format} 
\balance
\bibliography{refs}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage

\appendix

\input{appendix.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

