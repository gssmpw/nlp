% SIAM Article Template
\documentclass[final,hidelinks,onefignum,onetabnum]{siamart220329}

% Information that is shared between the article and the supplement
% (title and author information, macros, packages, etc.) goes into
% ex_shared.tex. If there is no supplement, this file can be included
% directly.

\input{ex_shared}

% Optional PDF information
\ifpdf
\hypersetup{
  pdftitle={Convergence Guarantees of Model-free Policy Gradient Methods for LQR with Stochastic Data},
  pdfauthor={Bowen Song, Andrea Iannelli}
}
\fi

% The next statement enables references to information in the
% supplement. See the xr-hyperref package for details.

%\externaldocument[][nocite]{ex_supplement}

% FundRef data to be entered by SIAM
%<funding-group specific-use="FundRef">
%<award-group>
%<funding-source>
%<named-content content-type="funder-name"> 
%</named-content> 
%<named-content content-type="funder-identifier"> 
%</named-content>
%</funding-source>
%<award-id> </award-id>
%</award-group>
%</funding-group>

\begin{document}

\maketitle

% REQUIRED
\begin{abstract}
Policy gradient (PG) methods are the backbone of many reinforcement learning algorithms due to their good performance in policy optimization problems. As a gradient-based approach, PG methods typically rely on knowledge of the system dynamics. However, this information is not always available, and in such cases, trajectory data can be utilized to approximate first-order information. When the data are noisy, gradient estimates become inaccurate and a formal investigation that encompasses uncertainty estimation and the analysis of its propagation through the algorithm is currently missing. To address this, our work focuses on the Linear Quadratic Regulator (LQR) problem for systems subject to additive stochastic noise. 
After briefly summarizing the state of the art for cases with a known model, we focus on scenarios where the system dynamics are unknown, and approximate gradient information is obtained using zeroth-order optimization techniques. We analyze the theoretical properties by computing the error in the estimated gradient and examining how this error affects the convergence of PG algorithms. Additionally, we provide global convergence guarantees for various versions of PG methods, including those employing adaptive step sizes and variance reduction techniques, which help increase the convergence rate and reduce sample complexity. One contribution of this work is the study of the robustness of model-free PG methods, aiming to identify their limitations in the presence of noise and propose improvements to enhance their applicability. Numerical simulations show that these theoretical analyses provide valuable guidance in tuning the algorithm parameters, thereby making these methods more reliable in practically relevant scenarios.
\end{abstract}

% REQUIRED
\begin{keywords}
Policy Gradient Methods, Sample Complexity, Linear Optimal Control, Zeroth-order Optimization
\end{keywords}

% REQUIRED
\begin{MSCcodes}
93E20, 90C56, 49N10
\end{MSCcodes}

\section{Introduction}
Reinforcement learning \cite{Annaswamy_23_ARCRAS,bertsekas2019reinforcement,Sutton1998} has demonstrated a significant influence across a wide range of applications. A key concept within reinforcement learning is policy optimization, where the policy is parameterized and directly optimized over based on a predefined performance metric \cite{annurev:/content/journals/10.1146/annurev-control-042920-020021}. Several successful policy optimization methods have been developed, including policy gradient \cite{10.5555/3009657.3009806}, actor-critic \cite{NIPS1999_6449f44a}, proximal policy optimization \cite{schulman2017proximalpolicyoptimizationalgorithms}. This work focuses on policy gradient (PG) methods, which are based on the simple idea of minimizing a cost function over the parameterized policy by improving performance through a gradient descent-type update. Studying the convergence properties of PG methods, particularly their global convergence to the optimal policy, is an active area of research \cite{cen2023globalconvergencepolicygradient,pmlr-v80-fazel18a,9254115,doi:10.1137/20M1347942,doi:10.1137/19M1288012,10005813,9992612}. For instance, PG methods have been applied in \cite{doi:10.1137/20M1347942} to solve $\mathcal{H}_2$ cost function subject to $\mathcal{H}_\infty$ constraints, and they have been applied in \cite{doi:10.1137/19M1288012} to the Markov decision processes. 

The application of reinforcement learning to the linear quadratic regulator (LQR) problem has been extensively explored due to its value as an analytically tractable benchmark, making it ideal for studying reinforcement learning in environments with continuous state and action spaces \cite{pmlr-v54-abeille17b,articlesimulation,pmlr-v80-fazel18a,8558117,10384256,annurev:/content/journals/10.1146/annurev-control-053018-023825,pmlr-v80-tu18a}. Policy gradient methods have gained attention in recent studies focusing on the LQR problem. Interest in applying these methods to the LQR setting increased significantly after the work in \cite{pmlr-v80-fazel18a} demonstrated global convergence properties of policy gradient methods applied to the deterministic LQR problem. In \cite{9992612}, the performance limitations of PG applied to LQR problem are studied from a control-theoretic perspective. The work \cite{10005813} explores a primal-dual policy gradient method to solve the constrained LQR problem.

Solving the LQR problem requires the availability of a model of the system, as it is the case in \cite{doi:10.1137/20M1347942,doi:10.1137/19M1288012,10005813,9992612}, where the analysis assumes knowledge of the exact model. However, in real-world applications, a complete system description is often unavailable, making it necessary to combine policy optimization schemes with data-driven techniques. To address this challenge, various approaches have been developed. For instance, methods like those in \cite{pmlr-v19-abbasi-yadkori11a,pmlr-v119-cassel20a,pmlr-v119-simchowitz20a} combine system identification with model-based LQR design and use regret as a performance metric of the learning process. It is also possible to use the same two-step approach to solve the LQR problem with gradient-based schemes. The work \cite{https://doi.org/10.1002/rnc.7475} integrates recursive least squares with policy iteration optimization, while \cite{10383604} combines recursive least squares with the policy gradient method. These approaches are termed indirect data-driven methods, because first data is used to estimate a model, which is then integrated into a model-based certainty equivalence design. An alternative category is direct data-driven control, which directly uses data to design the controller bypassing the intermediate model estimation process. For instance, a model-free policy iteration algorithm is proposed in \cite{9691800}. Several model-free policy gradient methods have also been introduced and theoretically analyzed. In \cite{pmlr-v80-fazel18a}, model-free PG methods are proposed and formulated using zeroth-order optimization to estimate gradients for the LQR cost from finite but noise-free system trajectories. Similarly, in \cite{10091214}, PG methods are applied to a continuous-time output feedback system using zeroth-order gradient estimation alongside variance reduction techniques. In the aforementioned model-free works \cite{pmlr-v80-fazel18a,10383604,https://doi.org/10.1002/rnc.7475,10091214,9691800}, only the case where the system's trajectories are noise-free is studied. However, it is essential to analyze what happens when gradients and other objects involved in the policy update are estimated using noisy data. In \cite{9254115}, Model-free PG methods are applied to the linear system with multiplicative noise, addressing uncertainty in the system matrix. To date, there is no holistic analysis including the uncertainty associated with the estimation process and the analysis of its propagation through the algorithm, including probabilistic guarantees on convergence and its sample complexity

In this work, we analyze the sample complexity and robustness to noise of PG methods for the model-free LQR with additive stochastic noise. Due to the presence of noise with unbounded support, we consider an average infinite-horizon cost. We first characterize the properties of this cost function and analyze model-based versions of a few PG algorithms. For the gradient descent (GD) and natural policy gradient (NPG) methods we propose an adaptive stepsize scheme that has a provable faster convergence rate compared to the fixed step size methods discussed in \cite{pmlr-v80-fazel18a, 9254115, 10091214}. Then, for both the model-free gradient descent and natural policy gradient methods, we employ zeroth-order optimization to approximate the gradient and covariance from noisy data. We provide convergence guarantees for these model-free PG methods with a sample complexity that takes into account the noisy source of estimation error. Furthermore, we introduce and analyze a variance reduction technique within the zeroth-order optimization framework, and prove that this achieves a guaranteed improvement in the sample complexity. Even though variance reduction techniques for model-free LQR were already presented in \cite{10091214}, the contribution here represents a significant extension as we work in the noisy scenario and we are able to provide a provable improvement in the number of required samples. Finally, we provide a comparative analysis of model-free PG methods with and without noisy data, to demonstrate the importance of the analysis considering noise, offer guidance of parameter tuning in applying policy gradient methods to applications, and also illustrate the improvement of introducing adaptive step sizes and variance reduction. Our analysis shows that model-free PG with stochastic data enjoy qualitatively similar convergence guarantees to those in the noise-free scenario, but it points out limitations in terms of the number of samples necessaries, stepsize ranges and convergence rate that are caused by noise. 
%The main contributions of this work are listed as follows:
%\begin{enumerate}
%    \item investigating the properties of the cost function and gradient in the LQR problem with stochastic noise, which are fundamental for providing the global convergence guarantees of model-based and also model-free policy gradient methods.
%    \item Utilizing a time-adaptive step size to accelerate the convergence rate of model-based and also model-free gradient descent and natural policy gradient methods.
%    \item Introducing variance reduction technique in the zeroth-order optimization framework and provides the guarantee of the performance improvement.
%    \item Comparing the application of the gradient descent and natural policy gradient methods to noise-free and stochastic noise scenarios to understand the impact of noise on the policy gradient methods, which offer guidance of parameter tuning in applying policy gradient method to applications
%\end{enumerate}

The paper is organized as follows. Section \ref{sec:Preliminaries} introduces the problem setting and provides some preliminaries. Section \ref{sec:MBGD} studies convergence properties of a few representative PG methods for the average cost case and proposes their extension to adaptive stepsizes. Section \ref{sec:MFGD} investigates convergence and sample complexity of model-free versions of gradient descent and natural policy gradient methods by placing particular emphasis on the effect of noisy data on the algorithms, together with adaptive step sizes and variance reduction techniques. Section \ref{sec:Simulation} exemplifies the main findings of the work by observing the behavior of the analyzed PG methods under different noise levels and algorithmic options. Section \ref{sec:conclusions} serves as a concluding summary of the work. 
\subsection*{Notations}
We denote by $A\succeq 0$ and $A\succ0$ a positive semidefinite and positive definite matrix $A$, respectively. The symbols $\lambda_1(A)$ denote the smallest eigenvalue of the matrix $A$. For matrices, $\lVert \cdot\rVert_F$, $\lVert \cdot\rVert$ denote the Frobenius norm and induced $2$-norm, respectively. For vectors, $\lVert \cdot\rVert$ denotes the Euclidean norm. $I$ represents the identity matrix. $\mathcal{N}(0,\Sigma)$ denotes a Gaussian distribution with $0$ mean and covariance $\Sigma \succ 0$.
\section{Preliminaries}
\label{sec:Preliminaries}
In this work, we consider the following averaged infinite horizon optimal control problem,  where the plant is subject to additive stochastic noise:
\begin{subequations}\label{2}
\begin{align}
        \min_{u_t}& \lim_{T\rightarrow +\infty}\frac{1}{T}\mathop{\mathbb{E}}_{x_0, w_t} \sum_{t=0}^{T-1}\left( x_t^\top Q x_t +u_t ^\top R u_t\right),\label{Cost}\\ 
        \mathrm{s.\,t.} ~ &x_{t+1}=Ax_t+Bu_t+w_t, x_0 \sim \mathcal{D}, \label{LTI}
\end{align}
\end{subequations}
where $x_t\in \mathbb{R}^{n_x}$ is the system state and $u_t\in \mathbb{R}^{n_u}$ is the system input; $A\in \mathbb{R}^{n_x \times n_x}$, $B\in \mathbb{R}^{n_x \times n_u}$, $(A,B)$ is unknown but stabilizable, which is a standard assumption in data-driven control \cite{pmlr-v99-tu19a,9691800}; $w_t \sim \mathcal{N}(0,\Sigma_w)$; $Q,R\succ 0$ are the weighting matrix. We define the state covariance at the initial time as $\Sigma_0:=\mathop{\mathbb{E}}\limits_{x_0}\left[x_0x_0^\top\right]$ and then $\mathcal{D}:=\mathcal{N}(0,\Sigma_0)$.  %Because the dynamics (\ref{LTI}) is linear and the cost (\ref{Cost}) is quadratic, this optimal control problem is referred to as an infinite horizon linear quadratic regulator (LQR).

The input $u_t$ is parameterized as a linear state feedback control with gain $K$, i.e. $u_t=Kx_t$. The optimal control cost only depends on $K$ and is denoted by $C$:
\begin{equation}\label{CostFunction0}
    C(K):=\lim_{T\rightarrow +\infty}\frac{1}{T}\mathop{\mathbb{E}}_{x_0,w_t} \sum_{t=0}^{T-1} x_t^\top \underbrace{\left(Q+K^\top R K\right)}_{=:Q_K}x_t.
\end{equation}

\subsection{Properties of $C$}\label{section2.2}
In this section, we introduce basic properties of problem \eqref{2} that are particularly relevant when policy gradient methods are used to solve it. While these results are mostly well-known, they form the foundation for the main results of this work in Sections \ref{sec:MBGD} and \ref{sec:MFGD}.  The state response $x_t$ for $t\geq 1$ associated with any gain $K$ is given by:
\begin{equation} \label{stateresponce}
    x_t=(A_K)^tx_0+\sum_{k=0}^{t-1} (A_K)^{t-k}w_{k-1}.
\end{equation}
We define the set $\mathcal{S}$ as the set of matrices $K\in \mathbb{R}^{n_x \times n_u}$ that stabilizes the system $(A,B)$, meaning that the matrix $A_K:=A+BK$ is Schur stable:
\begin{equation}
    \mathcal{S}:=\left\{K \in \mathbb{R}^{n_x \times n_u} | \lVert A_K \rVert <1 \right\}.
\end{equation}

Now, we introduce some important definitions associated with $K \in \mathcal{S}$. The covariance matrix at time $t$ is defined as $\Sigma_t:=\mathop{\mathbb{E}}\limits_{x_0,w_t}[x_t x_t^\top]$. From this definition and the state response given in \eqref{stateresponce}, we obtain:
\begin{equation}\label{covariance}
           \Sigma_{t+1}=A_K\Sigma_{t}A_K^\top+\Sigma_w, \quad t\in \mathbb{Z}_{+}.\\
\end{equation}
The average covariance matrix associated with $K\in \mathcal{S}$ is defined as:
\begin{equation}\label{definedAverage}
    \Sigma_K:=\lim_{T\rightarrow +\infty}\frac{1}{T}\sum_{t=0}^{T-1} \Sigma_t.
\end{equation}
Then from \eqref{covariance} and \eqref{definedAverage}, we have:
\begin{align}
    \Sigma_K=\Sigma_w+A_K \Sigma_K A_K^\top=\sum_{t=0}^{\infty}A_K^t \Sigma_w A_K^{t~\top}.
\end{align}
The cost function $C(K)$ associated with $K\in \mathcal{S}$ can be equivalently expressed as:
\begin{equation}\label{CostFunction}
    C(K)=\mathrm{Tr}\left(P_K\Sigma_w\right)=\mathrm{Tr}\left(Q_K\Sigma_K\right),
\end{equation}
where $P_K$ is defined as the solution of the following Lyapunov equation:
\begin{align}
    P_K=Q_K+A_K^\top P_K A_K=\sum_{t=0}^{\infty}A_K^{t~\top} Q_K A_K^{t}.
\end{align}
From the expression of the cost function \eqref{CostFunction} and well-known results \cite{lewis2012optimal}, the optimal $K^*$, which minimizes the cost function $C$, is given by:
\begin{subequations}
    \begin{align}
K^*&=-(R+B^\top P_{K^*}B)^{-1}B^\top P_{K^*}A, \label{Kpolicyimprovement} \\
P_{K^*}&=Q+A^\top P_{K^*}A-A^\top P_{K^*}B(R+B^\top P_{K^*}B)^{-1}B^\top P_{K^*}A. 
    \end{align}
\end{subequations}
The average covariance matrix associated with the optimal $K^*$ is denoted as $\Sigma_{K^*}$. Using \eqref{CostFunction}, the gradient of $C(K)$ with $K\in \mathcal{S}$ can be expressed as follows \cite{9992612}:
    \begin{equation}\label{Gradient}    
    \nabla C(K)=2E_K \Sigma_K,
\end{equation}
where $E_K:=\left(R+B^\top P_K B\right) K+B^\top P_K A$.

It is known that the cost function $C$ defined in \eqref{CostFunction0} is generally non-convex. In the context of non-convex optimization, the convergence to the optimal solution of gradient descent cannot usually be guaranteed. However, the cost function $C(K)$ satisfies a special condition known as \emph{gradient domination}.
\begin{Lemma}[Gradient Domination]\label{LemmaGD}
    The function $C$ on the set $\mathcal{S}$ is gradient dominated. That is, for any $K \in \mathcal{S}$, the following inequality holds:
    \begin{equation}\label{GDMu}
        C(K)-C(K^*)\leq \mu\lVert \nabla C(K)\rVert_F^2,
    \end{equation}
    with $\mu:=\frac{1}{4}\lVert \Sigma_{K^*} \rVert\lVert \Sigma_w ^{-2}\rVert\lVert R ^{-1}\rVert$.
\end{Lemma}
From \eqref{GDMu} and the fact that $\Sigma_w \succ 0$, it follows that the cost function $C$ has a unique minimizer and no other stationary points. The proof of Lemma \ref{LemmaGD} is provided in Appendix \ref{ProofLemmaGD}, and is a straightforward extension of \cite[Lemma 3]{pmlr-v80-fazel18a} to the averaged infinite horizon setting considered here \eqref{CostFunction0}.
\begin{Lemma}[Almost Smoothness on $\mathcal{S}$]\label{AlmostSmoothness}
For any $K,K' \in \mathbb{S}$, the following inequality holds: 
\begin{equation}\label{AlmostSmoothness1}
        \begin{split}
            \left\lvert C(K')-C(K) -  2\mathrm{Tr}\left(({K'}-{K})^\top E_K \Sigma_{K'} \right) \right\rvert \leq \lVert \Sigma_{K'}\rVert \left\lVert R+B^\top P_{K} B\right\rVert \left\lVert K'-{K}\right\rVert_F^2,
        \end{split}
\end{equation}
\end{Lemma}
The proof of Lemma \ref{AlmostSmoothness} is provided in Appendix \ref{ProofSmoothness}, which is an extension of \cite[Lemma 6]{pmlr-v80-fazel18a}. To understand why \eqref{AlmostSmoothness1} is referred to as almost smoothness, assume additionally that: we can approximate $\Sigma_K$ with $\Sigma_{K'}$, i.e. $\Sigma_K \approx \Sigma_{K'}$, and that we can upper bound the term $\lVert \Sigma_{K}\rVert \left\lVert R+B^\top P_{K} B\right\rVert$ by $L$. Then, recalling \eqref{AlmostSmoothness1}, we see that \eqref{Gradient} is equivalent to:
$$            \left\lvert C(K')-C(K) -  \mathrm{Tr}\left(({K'}-{K})^\top \nabla C(K) \right) \right\rvert \leq L\left\lVert K'-{K}\right\rVert_F^2, \quad \forall K,~K' \in \mathcal{S.}$$
which is the classic descent lemma for a smooth matrix function $C$. This approximation allows us to interpret the cost function as nearly smooth when the aforementioned assumptions are satisfied. Since we aim to leverage \eqref{AlmostSmoothness1} to develop a gradient descent algorithm, it is crucial to quantify the relationship between $\Sigma_K$ and $\Sigma_{K'}$ and find the upper bound $L$, which will be discussed in the following section.
\subsection{Perturbation of \texorpdfstring{{\boldmath$\Sigma_K,C,\nabla C$}}{Covariance, Cost, Gradient}}\label{section2.3}
In this subsection, we demonstrate that the average covariance matrix $\Sigma_K$, the cost function $C(K)$, and its gradient $\nabla C(K)$ are locally Lipschitz continuous with respect to the policy $K$. This property is crucial for the analysis of model-free policy optimization. 
\begin{Lemma}[$\Sigma_{K}$ Perturbation]\label{PerturbationSigmaK}
    Suppose $K',K \in \mathcal{S}$ are such that:
    \begin{equation*}\label{SigmaK1}
    \lVert K-K' \rVert \leq \frac{\lambda_1(\Sigma_w)\lambda_1(Q)}{8C(K)\lVert B \rVert}=: h(C(K)),
\end{equation*}
it holds that:
\begin{equation*}\label{SigmaK2}
    \lVert \Sigma_K-\Sigma_{K'} \rVert \leq \underbrace{8 \left(\frac{C(K)}{\lambda_1(Q)} \right)^2 \frac{\lVert B \rVert}{\lambda_1(\Sigma_w)}}_{=:h_{\Sigma}(C(K))}\lVert K-K' \rVert.
\end{equation*}
\end{Lemma}
The proof of Lemma \ref{PerturbationSigmaK} is given in Appendix \ref{ProofPerturbationSigmaK}, which follows the proof of \cite[Lemma 16]{pmlr-v80-fazel18a} and uses the definition of $\Sigma_K$. 
%Then the "almost" smoothness property stated in Lemma \ref{AlmostSmoothness} can be expressed as local smoothness. That is, when $\lVert K-K' \rVert \leq h(C(K))$, then:
%\begin{equation*}\label{SigmaK3}
%      \begin{split}
%            \left\lvert C(K')-C(K) -  \mathrm{Tr}\left(({K'}-{K})^\top\nabla C(K) \right) \right\rvert \leq 2h_{\Sigma}(C(K))\mathrm{Tr}(E_K)\left\lVert K'-{K}\right\rVert_F^2\\ 
%            +\left[\lVert \Sigma_{K}\rVert+h_{K}(C(K)) h_{\Sigma}(C(K))\right]\left\lVert R+B^\top P_{K} B\right\rVert \left\lVert K'-{K}\right\rVert_F^2.
%        \end{split}
%\end{equation*}

\begin{Lemma}[$C$ Perturbation]\label{PerturbationCK}
    Suppose $K',K \in \mathcal{S}$ are such that:
    \begin{equation*}
        \lVert K-K' \rVert \leq\min \{h\left(C(K)\right), \lVert K \rVert\},
    \end{equation*}
    it holds that:
    \begin{equation*}
        \lVert C(K')-C(K)\rVert \leq h_{C}(C(K))\lVert K-K' \rVert,
\end{equation*}
where $h_{C}(C(K))$ is defined in \eqref{Pertubation1} in the proof.
\end{Lemma}
\begin{Lemma}[$\nabla C$ Perturbation]\label{PerturbationnablaCK} Suppose $K'$ is such that:
   \begin{equation*}
        \lVert K-K' \rVert \leq \min \{h(C(K)), \lVert K \rVert\},
    \end{equation*}
    then there exists a polynomial $h_{\nabla}(C(K))$ such that:
    \begin{equation*}
        \lVert \nabla C(K)-\nabla C(K') \rVert \leq h_{\nabla}(C(K)) \lVert K-K' \rVert,
    \end{equation*}
where $h_{\nabla}(C(K))$ is defined in \eqref{ErrorGradient} in the proof.
\end{Lemma}
The proofs of Lemma \ref{PerturbationCK} and Lemma \ref{PerturbationnablaCK} are provided in Appendix \ref{ProofPerturbationCK} and \ref{ProofPerturbationnablaCK}, respectively. These proofs follow the approach used in \cite[Lemma 24, Lemma 25]{pmlr-v80-fazel18a} by adapting the expressions of $C$ and $\nabla C$. In summary, Lemma \ref{PerturbationSigmaK}, Lemma \ref{PerturbationCK} and Lemma \ref{PerturbationnablaCK} establish that $\Sigma_K$, $C$ and $\nabla C$ are locally Lipschitz continuous. 

%his observation aligns with our theoretical analysis, where the step size in Theorem \ref{MFPGV} is a function of the covariance of the noise. This aspect is not addressed in the analysis provided in \cite{pmlr-v80-fazel18a}.
\section{Convergence of Model-based Policy Gradient}\label{sec:MBGD}
In this section, we show the global convergence properties of various model-based policy gradient methods, that is, algorithms that operate with perfect knowledge of the system matrices $(A, B)$. We consider three variants of policy gradient algorithms: 
\begin{itemize}
    \item Gradient Descent:  $K_{i+1}=K_i-\eta \nabla C(K_i)$;
    \item Natural Policy Gradient: $K_{i+1}=K_i-\eta \nabla C(K_i) \Sigma_{K_i}^{-1}$;
    \item Gauss-Newton: $K_{i+1}=K_i-\eta (R+B^\top P_{K_i} B)^{-1} \nabla C(K_i) \Sigma_{K_i}^{-1}$,
\end{itemize}
where $i$ is the iteration index and $i\in \mathbb{Z}_+$.
The Gauss-Newton method, when implemented with a step size $\eta=\frac{1}{2}$, becomes equivalent to the policy iteration algorithm, which is well-known for its quadratic convergence rate \cite{1099755}. To implement the Gauss-Newton method, three terms, $(R+B^\top P_{K_i} B),\nabla C(K_i),\Sigma_{K_i}$ are necessary. In contrast, gradient descent only requires the gradient $\nabla C(K_i)$, but exhibits only a linear convergence rate. The natural policy gradient method strikes a balance between the two, requiring less information than the Gauss-Newton method while providing faster convergence properties than the gradient descent method. 
\subsection{Gradient Descent}
In this subsection, we present the theorem guaranteeing the convergence of the model-based gradient descent method, based on the properties introduced in Sections \ref{section2.2} and \ref{section2.3}.
\begin{Theorem}[Gradient Descent with Adaptive Step Size]\label{PGTheorem}
    Suppose the initial $K_0 \in \mathcal{S}$, and consider the gradient descent iteration
     \begin{equation}
    K_{i+1}=K_i-\eta_i \nabla C(K_i), \quad \forall i\in \mathbb{Z}_+,
\end{equation}
where the step size satisfies $\eta_i \leq h_{\mathrm{GD}}(C(K_i))$, and $h_{\mathrm{GD}}(C(K_i))$ is defined in \eqref{pG2} in Appendix \ref{ProofTheorem3}. Then, the following relationship holds: 
\begin{equation}\label{GD}
    C(K_{i+1})-C(K^*) \leq \left( 1-\frac{2\eta_i \lambda_1{(R)} \lambda_1^2(\Sigma_w)}{\lVert \Sigma_{K^*}\rVert}\right) (C(K_{i})-C(K^*)).
\end{equation}
For $\eta_i=h_{\mathrm{GD}}(C(K_i)), i\in \mathbb{Z}_+$ and given any accuracy gap $\epsilon >0$, if the number of iterations $N$ satisfies:
\begin{equation*}
    N \geq  \frac{\lVert \Sigma_{K^*}\rVert}{2{\eta_0} \lambda_1(R)\lambda^2_1(\Sigma_w)}\log \frac{C(K_{0})-C(K^*)}{\epsilon},
\end{equation*}
%where $\underline{\eta}:=\min\limits_{i\in [0,N]}\eta_i$, 
then 
\begin{equation*}
    C(K_N)-C(K^*) \leq \epsilon.
\end{equation*}


\end{Theorem}
The proof of the Theorem \ref{PGTheorem} can be found in Appendix \ref{ProofTheorem3}. From Theorem \ref{PGTheorem}, we see that the gradient descent method can achieve any desired accuracy $\epsilon$ within a finite number of iterations $N$ and converges linearly. According to Theorem \ref{PGTheorem}, the step size is adaptive according to the cost $C(K_i)$ at the current iterate $K_i$. %Since the cost $C(K_i)$ decreases as the iteration index $i$ increases, the adaptive step size increases with the index $i$.

% \begin{Theorem}[Gradient Descent with Adaptive Step Size]\label{PGV}
%     Suppose the initial $K_0 \in \mathcal{S}$, using gradient descent with adaptive step size:
%      \begin{equation}
%     K_{i+1}=K_i-\eta_i \nabla C(K_i), \quad \forall i\in \mathbb{Z}_+
% \end{equation}
% with $\eta_i = h_{\mathrm{GD}}(C(K_i))$ defined in Appendix \ref{ProofTheorem3}, the cost function satisfies the following inequality:  
% \begin{equation}
%     C(K_{i+1})-C(K^*) \leq \left( 1-\frac{2\eta_i \lambda_1{(R)} \lambda_1^2(\Sigma_w)}{\lVert \Sigma_{K^*}\rVert}\right) (C(K_{i})-C(K^*)).
% \end{equation}
% \end{Theorem}
% The proof of Theorem \ref{PGV} is a direct consequence of Theorem \ref{PGTheorem}. 
\begin{Remark}[Advantage of Adaptive Step Size over Fixed Step Size] \label{RemarkGD}
 In \cite{pmlr-v80-fazel18a}, a fixed step size is introduced. From Theorem \ref{PGTheorem}, a fixed step size version can also be derived, where the fixed step size is given by $\eta = h_\mathrm{GD}(C(K_0))$. For both fixed and adaptive step sizes, we can prove that $C(K_{i+1})\leq C(K_i)$ from Theorem \ref{PGTheorem}. Given the same initial $K_0$ and together with the expression of $h_\mathrm{GD}(C(K))$, it follows that $\eta_0 \leq \eta_{i}$ for all $i$. This leads to the following inequality: $$0< \left( 1-\frac{2\eta_i \lambda_1{(R)} \lambda_1^2(\Sigma_w)}{\lVert \Sigma_{K^*}\rVert} \right) \leq \left( 1-\frac{2\eta_0 \lambda_1{(R)} \lambda_1^2(\Sigma_w)}{\lVert \Sigma_{K^*}\rVert} \right)<1, \forall i\in \mathbb{Z}_+.$$ Thus, it can be concluded that using an adaptive step size improves the convergence rate compared to a fixed step size.
\end{Remark}

\subsection{Natural Policy Gradient}
The natural policy gradient method adjusts the standard policy gradient by considering the geometry of the parameter space through the Fisher information matrix \cite{NIPS2001_4b86abe4}. The update rule for the natural policy gradient is given by:
\begin{equation}\label{NPGtheta}
    \theta \leftarrow \theta-\eta G_{\theta}^{-1} \nabla C(\theta),
\end{equation}
where $\theta$ is a vector of optimization variables and $G_{\theta}$ is the Fisher information matrix associated with $\theta$, and $\eta$ is the step size. An important special case is optimizing over a linear policy with additive Gaussian noise, expressed as:
\begin{equation}\label{parametrization}
    u_t=\pi_K(u_t|x_t)=\mathcal{N}(Kx_t, \alpha^2I),
\end{equation}
where $K\in \mathbb{R}^{n_x \times n_u}$ represents the optimization variables and $\alpha^2$ is the fixed noise variance, which can be set arbitrarily. Without loss of generality, we set $\alpha=1$. Since the optimization variable $\theta$ in \eqref{NPGtheta} is a vector, and $K$ in \eqref{parametrization} is a matrix, we define $\bar{K}$ as the vectorized form of $K$, such that $\theta = \bar{K}$:
\begin{equation*}
    K\in \mathbb{R}^{n_x\times n_u}=\left[\begin{array}{cc}
         k_1  \\
         k_2  \\
         ... \\
         k_{n_x}
    \end{array}\right] \rightarrow [k_1,k_2,...,k_{n_x}]^\top=:\bar{K}\in \mathbb{R}^{n_xn_u}.
\end{equation*} 
In this work, we define the Fisher information matrix $G_{\bar{K}}$ as:
\begin{equation}
    \hat{G}_{\bar{K}}:=\lim_{T\rightarrow +\infty}\frac{1}{T}\sum_{t=0}^T~\mathbb{E} \left[ \nabla \mathop{log} \pi_{\bar{K}}(u_t| x_t)\nabla \mathop{log} \pi_{\bar{K}}(u_t| x_t)^\top  \right].
\end{equation}
Given this definition, the natural policy gradient update, incorporating the Fisher information matrix $G_{\bar{K}}$, is given by:
\begin{equation}\label{updateNPG}
    K_{i+1}=K_i-\eta \nabla C(K_i) \Sigma_{K_i}^{-1}
\end{equation}
The explicit derivation of \eqref{updateNPG} starting from \eqref{NPGtheta} and using the particular policy parametrization \eqref{parametrization} is given in Appendix \ref{UpdateNPG}.
We now present the theorem that establishes the convergence guarantee of the natural policy gradient method
\begin{Theorem}[Natural Policy Gradient with Adaptive Step Size]\label{NPGTheorem}
     Suppose the initial $K_0 \in \mathcal{S}$, and consider the natural policy gradient iteration
     \begin{equation}
    K_{i+1}=K_i-\eta_i \nabla C(K_i) \Sigma_{K_i}^{-1}, \quad \forall i\in \mathbb{Z}_+,
\end{equation}
where the step size satisfies 
\begin{equation}
    \eta_i \leq \frac{1}{2\lVert R \rVert+\frac{2\lVert B \rVert^2 C(K_i)}{\lambda_1(\Sigma_w)}}.
\end{equation}
Then the following relationship holds: 
\begin{equation}\label{PGConvergence}
    C(K_{i+1})-C(K^*) \leq \left( 1-\frac{2\eta_i \lambda_1{(R)} \lambda_1(\Sigma_w)}{\lVert \Sigma_{K^*}\rVert}\right) (C(K_{i})-C(K^*)).
\end{equation}
For 
\begin{equation}
    \eta_i = \frac{1}{2\lVert R \rVert+\frac{2\lVert B \rVert^2 C(K_i)}{\lambda_1(\Sigma_w)}},\quad \forall i\in \mathbb{Z}_+,
\end{equation}
and given any accuracy gap $\epsilon >0$, if the number of iterations $N$ satisfies:
\begin{equation*}
    N \geq \frac{\lVert \Sigma_{K^*}\rVert}{2\lambda_1(\Sigma_w)} \left( \frac{\lVert R \rVert}{\lambda_1(R)} +\frac{\lVert B \rVert^2 C(K_0)}{\lambda_1{(R)} \lambda_1(\Sigma_w)}\right)\log \frac{C(K_{0})-C(K^*)}{\epsilon},
\end{equation*}
then 
\begin{equation*}
    C(K_N)-C(K^*) \leq \epsilon.
\end{equation*}

\end{Theorem}
The proof of Theorem \ref{NPGTheorem} relies on Lemma \ref{AlmostSmoothness} and follows the procedure outlined in \cite[Lemma 15]{pmlr-v80-fazel18a}. Theorem \ref{NPGTheorem} establishes the global convergence properties of the natural policy gradient method, when an appropriate step size $\eta_i$ is selected. Similar to the observation in Remark \ref{RemarkGD}, the introduction of adaptive step sizes improves the convergence rate, as the step sizes increase adaptively with the decreasing of the cost.

% \begin{Theorem}[Natural Policy Gradient with Adaptive Step Size]\label{NPGV}
%      Suppose the initial $K_0 \in \mathcal{S}$, using the natural policy gradient with adaptive step size:
%      \begin{equation}
%     K_{i+1}=K_i-\eta_i \nabla C(K_i) \Sigma_{K_i}^{-1},
% \end{equation}
% where the step size $\eta_i$ is chosen as:
% \begin{equation}\label{Step}
%     \eta_i = \frac{1}{2\lVert R \rVert+\frac{2\lVert B \rVert^2 C(K_i)}{\lambda_1(\Sigma_w)}},
% \end{equation}
% the cost function satisfies the following inequality: 
% \begin{equation}
% \begin{split}
%         C(K_{i+1})-C(K^*) \leq {\left( 1-\frac{2\eta_i \lambda_1{(R)} \lambda_1(\Sigma_w)}{\lVert \Sigma_{K^*}\rVert}\right)} (C(K_{i})-C(K^*)),\forall i\in \mathbb{Z}_+.
% \end{split}
% \end{equation}
% \end{Theorem}
%The proof of Theorem \ref{NPGV} follows directly from Theorem \ref{NPGTheorem}. Similar to the observation in Remark \ref{RemarkGD}, the introduction of adaptive step sizes enhances the convergence rate, as the step sizes increase adaptively with the decreasing of the cost.
% \begin{Remark}[Comparison between Natural Policy Gradient with Fixed and adaptive Step Size] \label{R1}
% Similar to Remark \ref{RemarkGD} and based on Theorem \ref{NPGV}, the cost function is non-increasing with natural policy gradient method with adaptive step size, thus, we have $C(K_i)\geq C(K_{i+1})$ for all $i$. Together with \eqref{Step}, for any iteration $i$, it holds that $\eta=\eta_0 \leq \eta_{i+1}$. This relationship leads to the following inequality:$$ 0 < \left( 1-\frac{2\eta_i \lambda_1{(R)} \lambda_1(\Sigma_w)}{\lVert \Sigma_{K^*}\rVert} \right) \leq \left( 1-\frac{2\eta_0 \lambda_1{(R)} \lambda_1(\Sigma_w)}{\lVert \Sigma_{K^*}\rVert} \right) < 1, \forall i\in \mathbb{Z}_+i.$$ As a result, the convergence rate is improved when using a adaptive step size, as the step size can adapt to the current value of $C(K_i)$, enabling a faster convergence rate to the optimal policy $K^*$. 
% \end{Remark}



\subsection{Gauss-Newton Method}
We now present the theorem that provides the convergence guarantee for the Gauss-Newton method.
\begin{Theorem}[Gauss-Newton Method]\label{GNMTheorem}
    Suppose the initial $K_0 \in \mathcal{S}$, and consider the Gauss-Newton iteration
    \begin{equation}
        K_{i+1}=K_i-\eta (R+B^\top P_{K_i} B)^{-1} \nabla C(K_i) \Sigma_{K_i}^{-1}, \quad \forall i\in \mathbb{Z}_+, 
    \end{equation}
    with $\eta \leq \frac{1}{2}$. Then, the following relationship holds: 
    \begin{equation}
    C(K_{i+1})-C(K^*) \leq \left( 1-\frac{2\eta \lambda_1(\Sigma_w)}{\lVert \Sigma_K^*\rVert}\right) (C(K_{i})-C(K^*)).
\end{equation}
    For the maximum fixed step size $\eta=\frac{1}{2}$ and any accuracy gap $\epsilon >0$, if the number of iterations $N$ satisfies: 
    \begin{equation*}
    N \geq \frac{\lVert \Sigma_K^*\rVert}{\lambda_1(\Sigma_w)} \log \frac{C(K_{0})-C(K^*)}{\epsilon},
\end{equation*}
then 
\begin{equation*}
    C(K_N)-C(K^*) \leq \epsilon.
\end{equation*} 
\end{Theorem}
The proof of the Theorem \ref{GNMTheorem} builds on the property shown in Lemma \ref{AlmostSmoothness} and follows similar steps to the steps in \cite[Lemma 14]{pmlr-v80-fazel18a}. It is important to note that the choice of step size here is independent of the specific system parameters. To achieve the fastest convergence rate, one can always select the step size $\eta=\frac{1}{2}$, which is equivalent to the policy iteration algorithm.

The Gauss-Newton method requires knowledge of term $(R+B^\top P_KB)$, making it incompatible within the zeroth-order optimization frameworks in the model-free setting discussed in the following section. For implementation details of a model-free policy iteration method, please refer to \cite{9691800}, which employs least squares to estimate the necessary matrices from online data. However, it is important to note that \cite{9691800} does not provide any convergence guarantees under noisy data conditions. Developing a sample-based estimator for the term $(R+B^\top P_KB)$ and establishing convergence guarantees for the Gauss-Newton method under noisy data conditions remains a key area for future research.

%\begin{Remark} [Comparison between PG for Noise and Noise-free Case]\label{RMBPG}
%     In this work, we focus on linear systems with stochastic noise. Due to the presence of the noise, it is not possible to define the same infinite horizon cost as in \cite{pmlr-v80-fazel18a}. Instead, we adopt the average cost defined in \cite{pmlr-v19-abbasi-yadkori11a}, which is commonly used in online LQR learning framework with stochastic noise \cite{pmlr-v119-simchowitz20a,articlesimulation}. In the noise-free case, the initial distribution of $x_0$ plays an important role in the cost and also the associated gradient, from the forms given in \cite{pmlr-v80-fazel18a}. However, in the presence of noise, the averaging over the infinite horizon diminishes the impact of the initial state, making instead the noise distribution the key factor. Mathematically, if the distributions of the initial state $\Sigma_0$ in noise-free case and the noise $\Sigma_w$ in noise case are identical, i.e. $\Sigma_0=\Sigma_w$, the statements in Theorem \ref{GNMTheorem}, Theorem \ref{NPGTheorem} and \ref{PGTheorem} are same for both the noise-free and the noise cases. However, the nature of the problem is meaningfully different and it is important, already in the model-based version of the algorithms, to distinguish the source of stochasticity. The effect of the noise will have a much bigger impact when model-related quantities need to be estimated from data.  
%\end{Remark}

\subsection{Lack of robustness of model-based PG methods}\label{simu24}

The guarantees on PG methods reviewed in the previous sections rely on the availability of an exact system model. When this is not available, gradients and covariances are estimated from data trajectories, and for plants such as \eqref{LTI}, they will be subject to stochastic errors. We exemplify with a simple toy example the performance of a policy GD method when noisy gradients are used. We consider a linear time-invariant (LTI) system described by the following dynamics \cite{articlesimulation,9691800}:
\begin{equation}\label{LTIsimulation}
  x_{t+1}=\underbrace{\left[\begin{array}{ccc}
            1.01 & 0.01 & 0 \\
            0.01 & 1.01 & 0.01 \\
            0 & 0.01 & 1.01 
          \end{array}\right]}_A x_t+\underbrace{\left[\begin{array}{ccc}
            1 & 0 & 0 \\
            0 & 1 & 0 \\
            0 & 0 & 1 
          \end{array}\right]}_B u_t+w_t.
\end{equation}
The weighting matrices $Q$ and $R$ are set to
\begin{equation}
    Q=0.001I_3,\quad R=I_3
\end{equation}
The initial condition $\hat{K}_0$ is chosen as the optimal gain for the LQR problem with $(A,B,50Q,R)$. To model gradient estimation errors due to noisy data, we define the gradient estimate as follows:
\begin{equation}
    \hat{\nabla}C(K):=\nabla C(K)+ \Delta, 
\end{equation}
where $\Delta \in \mathbb{R}^{3\times 3}$ is a matrix with entries $\Delta_{ij}$ sampled according to the distribution $\mathcal{N}(0,\sigma^2)$, where the values of $\sigma\geq 0$ will be discussed later. The gradient descent algorithm updates the gain $\hat{K}_i$ as follows:
\begin{equation}
    \hat{K}_{i+1}=\hat{K}_i-\eta \hat{\nabla}C(\hat{K}_i),\quad i\in \mathbb{Z}_+.
\end{equation}
The simulation results are illustrated in Figure \ref{fig:MB}, where the y-axis represents the average obtained from a Monte Carlo simulation over 10,000 data samples.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\linewidth]{Figure1_Intro.pdf}
    \caption{Performance of Gradient Descent with Inexact Noisy Gradient for Different Values of Step Size and Noise}
    \label{fig:MB}
\end{figure}
When the gradient is exactly known (i.e. $\sigma=0$, black dotted line), we can use theoretical bounds on the step size to guarantee convergence of the gradient descent algorithm. In this case, the converging behavior of $\eta=0.12$ to the optimal policy is shown as an example. 
When the gradient estimation is affected by a small amount of noise ($\sigma=0.03$, blue dashed line), the same step size yields convergence but to a suboptimal value of gain $K$. 
As the noise level increases ($\sigma=0.6$, green dash-dotted line), the algorithm starts showing non-converging behavior. In this case, reducing the step size addresses the issue, as shown by the red solid line where the step size is empirically decreased to $0.03$. It is worth observing that this reduction in step size results in a slower convergence rate. 

These results exemplify that when PG algorithms are run using finite noisy data trajectories to compute gradients (or equivalently covariances), the convergence guarantees obtained in the model-based scenario no longer hold. Specifically, the algorithm might converge to suboptimal solutions or even diverge. It is also shown that, in the latter case, by properly modifying the algorithms' parameters (such as the step size), converging behaviors can be achieved. 
 
To characterize the fundamental limitations of such algorithms and improve their performance it is then paramount to: quantify the uncertainty on gradients and covariances due to the use of finite noisy data; and analyze how this uncertainty propagates in the policy optimization algorithm. To the best of the authors' knowledge, there is no holistic analysis that captures all these aspects and provides guarantees for model-free PG algorithms in the presence of noisy trajectory data. The main technical contribution of this work is to bridge this gap by providing: probabilistic guarantees of convergence and suboptimality bounds; closed-form expressions for the algorithm's parameters (e.g. step size, variance reduction) that guarantee converging behavior



\section{Convergence of Model-free Policy Gradient}\label{sec:MFGD}
In this section, we analyze two variants of model-free policy gradient methods: gradient descent and natural policy gradient. Unlike model-based approaches, the system dynamics $(A,B)$ are unknown in the model-free setting. Instead of relying on system identification, we use zeroth-order optimization, where gradients and covariance estimates are estimated directly from trajectory data.
%Using zeroth-order optimization, gradients and covariances can be estimated directly from trajectory data. 
In Section \ref{secGCE}, we begin by analyzing estimation algorithms for two fundamental components utilized in the proposed methods: the cost function gradient and the covariance. We derive novel finite-sample error bounds that explicitly account for the effects of noisy data. Building on these quantitative bounds, Section \ref{secGEVR} introduces a variance reduction technique to provably reduce the required number of rollouts. Leveraging these results, we establish probabilistic convergence guarantees for GD (Section \ref{secMFGD}) and NPG (Section \ref{secMFNPG}), emphasizing the more general scenario with time-adaptive step sizes.

\subsection{Model-free Gradient and Covariance Estimation}\label{secGCE}
 In the model-free setting, zeroth-order optimization techniques \cite{doi:10.1137/1.9780898718768,9186148} can be utilized to approximate the gradient and average covariance matrix using only function evaluations. The procedures for gradient and covariance estimation are outlined in Algorithm \ref{Algo1}.
\begin{algorithm}[H]
  \caption{Gradient and covariance estimation}\label{Algo1}
  \begin{algorithmic}
      \REQUIRE{Gain matrix $K\in \mathcal{S}$, number of rollouts $n$, rollout length $l$, exploration radius $r$, an upper bound of the initial state $L_0$ }
      \FOR{$k=1,...,n$}
        \STATE{ 1. Generate a sample gain matrix $\hat{K}_k=K+U_k$, where $U_k$ is drawn uniformly at random over matrices of compatible dimensions with Frobenius norm $r$; }
        \STATE{ 2. Generate a sample initial state $x_0^{(k)}$ with $\lVert x_0^{(k)} \rVert\leq L_0$;}
        \STATE{ 3. Excite the closed-loop system with $u_t^{(k)}=\hat{K}_kx_t^{(k)}$ for $l$-steps starting from $x_0^{(k)}$, yielding the state sequence $\left\{ x_t^{(k)} \right\}_{t=0}^{l-1}$ originating from \eqref{LTI};}
        \STATE{ 4. Collect the empirical cost estimate $\hat{C}_k:=\frac{1}{l}\sum_{t=0}^{l-1} x_t^{(k)~\top}(Q+\hat{K}^{\top}_kR\hat{K}_k)x_t^{(k)}$ and the empirical covariance matrix $\hat{\Sigma}_i=\frac{1}{l}\sum_{t=0}^{l-1}~x_t^{(k)}x_t^{(k)~\top} $;}
      \ENDFOR
      \RETURN{Gradient estimate $\hat{\nabla}C(K):=\frac{1}{n}\sum^{n}_{k=1}\frac{n_xn_u}{r^2}\hat{C}_kU_k$ and covariance estimate $\hat{\Sigma}_K:=\frac{1}{n}\sum^{n}_{k=1}\hat{\Sigma}_k$.}
  \end{algorithmic}
\end{algorithm}
% \begin{algorithm}[h]
%   \caption{Gradient and covariance estimation with online data}\label{Algo5}
%   \begin{algorithmic}
%       \REQUIRE{Gain matrix $K\in \mathcal{S}$, number of rollouts $n$, rollout length $l$, exploration radius $r$, an upper bound $L_0$, current state $x_t$ }
%       \FOR{$k=1,...,n$}
%         \STATE{ 1. Excite the closed-loop system with $u_t=Kx_t$ until $\lVert x_{t} \rVert\leq L_0$ and record the current index as $t=t_L$}
%         \STATE{ 2. Generate a sample gain matrix $\hat{K}_k=K+U_k$, where $U_k$ is drawn uniformly at random over matrices of compatible dimensions with Frobenius norm $r$; }
%         \STATE{ 3. Excite the closed-loop system with $u_t=\hat{K}_kx_t$ for $l$-steps starting from $x_{t_L}$, yielding the state sequence $\left\{ x_t \right\}_{t=t_L}^{t_L+l-1}$;}
%         \STATE{ 4. Collect the empirical finite-horizon cost estimate $\hat{C}_k:=\frac{1}{l}\sum_{t=t_L}^{t_L+l-1} x_t^{\top}(Q+\hat{K}^{\top}_kR\hat{K}_k)x_t$ and the empirical covariance matrix $\hat{\Sigma}_i=\frac{1}{l}\sum_{t=t_L}^{t_L+l-1}~x_tx_t^{\top} $;}
%       \ENDFOR
%       \RETURN{Gradient estimate $\hat{\nabla}C(K):=\frac{1}{n}\sum^{n}_{k=1}\frac{n_xn_u}{r^2}\hat{C}_kU_k$ and covariance estimate $\hat{\Sigma}_K:=\frac{1}{n}\sum^{n}_{k=1}\hat{\Sigma}_k$.}
%   \end{algorithmic}
% \end{algorithm}

We present two theorems to analyze the estimation errors of both $\hat{\nabla} C(K)$ and $\hat{\Sigma}_K$, obtained from Algorithm \ref{Algo1}.
\begin{Theorem}[Error Bound of $\hat{\nabla}C(K)$]\label{Theorem4}
    Given an arbitrary tolerance $\epsilon>0$, which can be decomposed as  $\epsilon=\epsilon_d+\epsilon_l+\epsilon_n+\epsilon_r$, and an arbitrary probability $\delta\in(0,1)$, which can be decomposed as $\delta=(1-\delta_d)(1-\delta_n)\delta_x$, for a given $K\in \mathcal{S}$,
    the estimated gradient $\hat{\nabla}C(K)$ from Algorithm \ref{Algo1} enjoys the following bound:
    \begin{equation}
    \mathbb{P}\left\{\lVert \hat{\nabla}C(K)- {\nabla}C(K)\rVert\leq \epsilon \right\}\geq \delta,
\end{equation}
if the parameters $r,l,n$ in Algorithm \ref{Algo1} satisfy:
\begin{subequations}
    \begin{align}
    r\leq r_{\max}(C(K),\epsilon_r),&\quad l\geq l_{\min}(C(K),\epsilon_l),\\
    n\geq \max(N_1(C(K),\epsilon_r,\epsilon_l,\epsilon_n,\delta_n,\delta_d),&N_2(C(K),\epsilon_r,\epsilon_l,\epsilon_n,\epsilon_d,\delta_x,\delta_n,\delta_d)),\label{N2}
\end{align}
\end{subequations}
where the detailed expressions of functions $r_{\max}$, $l_{\min}$,  $N_1$ and $N_2$ are given in \eqref{rexpresion}, \eqref{lexpression}, \eqref{Nexpression1}, and \eqref{Nexpression2} in Appendix \ref{ProofTheorem4}, respectively.
\end{Theorem}
The proof of Theorem \ref{Theorem4} is given in Appendix \ref{ProofTheorem4}.
\begin{Remark}[Interpretation of Theorem \ref{Theorem4}]\label{Remark42}
    In Theorem \ref{Theorem4}, the error tolerance $\epsilon$ and the probability $\delta$ are decomposed to provide a clearer and more structured statement. The error $\epsilon$ is broken down into four components: 
    \begin{itemize}
            \item $\epsilon_d$: the error caused by the noisy data;
        \item $\epsilon_l$: the error due to the finite rollout length compared to the infinite horizon cost;
        \item $\epsilon_n$: the error from using a finite number of rollouts in zeroth-order optimization;
        \item $\epsilon_r$: the difference between the gradient of smoothing function $C_r(K)$ defined in \eqref{C3eq} in Appendix \ref{AppSmooth} and the original function.
    \end{itemize}
     The probability $\delta$ is similarly decomposed: 
     \begin{itemize}
         \item $\delta_d$: the concentration probabilities for gradient estimation of the smoothing function, linked to the required rollouts $N_1$ defined in Theorem \ref{Theorem4};
         \item $\delta_n$: the concentration probabilities for the noisy data, linked to the required rollouts $N_2$ defined in Theorem \ref{Theorem4};
         \item $\delta_x$: the probability that the state remains bounded.
     \end{itemize}
\end{Remark}
\begin{Theorem}[Error bound of $\hat{\Sigma}_K$]\label{Theorem5}
    Given an arbitrary tolerance $\epsilon'>0$, which can be decomposed as $\epsilon'=\epsilon'_l+\epsilon'_n+\epsilon'_r$, and an arbitrary probability $\delta'\in(0,1)$, which can be decomposed as $\delta'=(1-\delta'_n)\delta'_x$, for a given $K\in \mathcal{S}$, the estimated average covariance $\hat{\Sigma}_K$ from Algorithm \ref{Algo1} enjoys the following bound:
    \begin{equation}
    \mathbb{P}\left\{\lVert \hat{\Sigma}_K- \Sigma_K \rVert\leq \epsilon' \right\}\geq \delta',
\end{equation}
    if the parameters $r,l,n$ in Algorithm \ref{Algo1} satisfy:
    \begin{equation}
        r\leq r'_{\max}(C(K),\epsilon'_r),l\geq l'_{\min}(C(K),\epsilon'_l),n\geq n'_{\min}(C(K),\epsilon'_r,\epsilon'_l,\epsilon'_n,\delta'_x,\delta'_n),
    \end{equation}
    where the detailed expressions of functions $r'_{\max}$, $l'_{\min}$ and $n'_{\min}$ are given in \eqref{radius2} \eqref{l2} and \eqref{n2} in \ref{ProofTheorem5}, respectively.
\end{Theorem}
The proof of Theorem \ref{Theorem5} is given in Appendix \ref{ProofTheorem5}. 
\begin{Remark}[Interpretation of Theorem \ref{Theorem5}]
    In Theorem \ref{Theorem5}, similar to Theorem \ref{Theorem4}, the error $\epsilon'$ is decomposed into three components: 
    \begin{itemize}
        \item $\epsilon'_l$: the error due to the finite rollout length compared to the infinite horizon average covariance;
        \item $\epsilon'_n$: the error from noise in the data;
        \item $\epsilon'_r$: the difference between average covariance associated with $K$ and $\hat{K}_k$.
    \end{itemize}  
 The probability $\delta$ is similarly decomposed: 
 \begin{itemize}
     \item $\delta'_n$: the concentration probabilities for the noisy data;
     \item $\delta'_x$: the probability that the state remains bounded.
 \end{itemize}
\end{Remark}
We analyze the effect of estimation errors caused by stochastic noise acting on \eqref{LTI}. This analysis requires the introduction of new tools and a significant extension of existing results, where previous works \cite{pmlr-v80-fazel18a,9254115,10091214} assumed that the trajectory has bounded support. Moreover, while \cite{pmlr-v80-fazel18a} assumes that all error terms are equal, i.e., $\epsilon_r = \epsilon_n = \epsilon_l$ and $\epsilon_r' = \epsilon_n' = \epsilon_l'$, we present a more structured approach to the error decomposition. By selecting different values for $l$, $r$, and $n$ (or $l'$, $r'$, $n'$), the same accuracy $\epsilon$ (or $\epsilon'$) can be achieved, but with a more refined breakdown of the contributing error terms.

Based on the results of Theorem \ref{Theorem4} and Theorem \ref{Theorem5}, we conclude that, with appropriate choices of $l$, $r$, and $n$, the estimates $\hat{\Sigma}_K$ and $\hat{\nabla} C(K)$ from Algorithm \ref{Algo1} can achieve any desired accuracy with a corresponding desired probability. These estimates then replace their model-based counterparts in model-free policy gradient methods. The error bounds established in this section serve as a foundation for the convergence guarantees of both model-free gradient descent and natural policy gradient methods, which are discussed in the next sections.
\subsection{Model-free Gradient Estimation with Variance Reduction}\label{secGEVR}
In the previous section, we employed zeroth-order optimization to estimate the gradient of the cost at the current value of the policy. However, this approach often suffers from high variance, resulting in a slow learning process \cite{6392457}. Using a baseline is a common variance reduction technique in policy gradient methods \cite{6315022,pmlr-v48-mniha16}. In this section, we propose employing the finite-horizon cost function as a baseline and show its performance improvement. 
For a state-dependent baseline function $b(x)$, the estimated gradient with variance reduction $\hat{\nabla} C_v(K)$ is expressed as
\begin{equation}
    \hat{\nabla} C_v(K):=\frac{1}{n}\sum^{n}_{k=1}\frac{n_xn_u}{r^2}(\hat{C}_k-b(x_0^{(k)}))U_k,
\end{equation}
where $U_k,x_0^{(k)},\hat{C}_k,r$ are the same as those defined in Algorithm \ref{Algo1}. The estimated gradient $\hat{\nabla} C_v(K)$ satisfies:
\begin{equation}\label{VR1}
    \begin{split}
        \mathbb{E}_{x_0^{(k)},U_k}[\hat{\nabla} C_v(K)]=\mathbb{E}_{x_0^{(k)},U_k}[\hat{\nabla} C(K)]-\frac{1}{n}\sum^{n}_{k=1}\frac{n_xn_u}{r^2}\mathbb{E}_{x_0^{(k)},U_k}[\hat{\nabla} b(x_0^{(k)})U_k].
    \end{split}
\end{equation}
Since $U_k$ and $x_0^{(k)}$ are independent (as described in Algorithm \ref{Algo1}), then $$\mathbb{E}_{x_0^{(k)},U_k}[\hat{\nabla} C_v(K)]=\mathbb{E}_{x_0^{(k)},U_k}[\hat{\nabla} C(K)].$$ Next, we determine the optimal function $b^*$, which minimizes the variance of the sample $Z_k^V:=(\hat{C}_k-b(x_0^{(k)}))U_k$. The norm of the variance of $Z_k^V$ is given by:
\begin{equation}\label{VR2}
    \lVert Var[Z_k^V] \rVert_F=\mathbb{E}_{x_0^{(k)},U_k}\left[\left\lVert (\hat{C}_k -b(x_0^{(k)}))U_k\right\rVert_F^2 \right]-\left\lVert\mathbb{E}_{x_0^{(k)},U_k}\left[ (\hat{C}_k -b(x_0^{(k)}))U_k \right]\right\rVert_F^2.
\end{equation}
For any state-dependent baseline function $b$, Since $U_k$ is i.i.d and zero mean, we have: $$\mathbb{E}_{x_0^{(k)},U_k}\left[ b(x_0^{(k)})U_k\right]=0.$$ Consequently, the second term in \eqref{VR2} becomes independent of the choice of $b$, leaving only the first term in \eqref{VR2} to be minimized. To minimize the first term, the baseline function should be chosen to reduce the mean square error of $\hat{C}_k$. Thus, the optimal baseline $b^*$ is given by:
\begin{equation}\label{eqbaseline}
    b^*(x_0)=\frac{1}{l}\sum_{t=0}^{l-1} \mathbb{E}_{w_t,U_k} \left[ x_t^{~\top}(Q+(\hat{K}_i+U_k)^{\top}R(\hat{K}_i+U_k))x_t \right].
\end{equation}
Since $U_k$, which was also already defined in Algorithm \ref{Algo1}, enters nonlinearly in \eqref{eqbaseline}, it is hard to compute the expectation and thus to obtain a closed-form expression for $b^*(x_0)$. Instead, a suboptimal solution $b_s(x_0)$ is used:
\begin{equation}
    b_s(x_0)=\frac{1}{l}\sum_{t=0}^{l-1} \mathbb{E}_{w_t} \left[ x_t^{~\top}(Q+\hat{K}^{\top}_iR\hat{K}_i)x_t \right].
\end{equation}
While the proposed approximation is simply driven by the need to simplify the computation of the expected value in \eqref{eqbaseline}, we will show later (Theorem \ref{TheoremVariancereduction}) that using $b_s$ yields a provable reduction in the required number of rollouts compared to the standard gradient computation from the previous section.

Algorithm \ref{AlgoV} summarizes the procedure used to compute the baseline function from data.
\begin{algorithm}[H]
  \caption{Baseline function estimation $b_s(x_0)$ for variance reduction of $\hat\nabla C(K)$}\label{AlgoV}
  \begin{algorithmic}
      \REQUIRE{Gain matrix $K\in \mathcal{S}$, number of rollouts to estimate the baseline function $n_v$, rollout length $l$, initial state $x_0$ }
      \FOR{$k=1,...,n_v$}
        \STATE{ 1. Excite the closed-loop system with $u_t=\hat{K}_ix_t$ for $l$-steps starting from $x_0$, yielding the state sequence $\left\{ x_t^{(k)} \right\}_{t=0}^{l-1}$;}
        \STATE{ 2. Collect the empirical finite-horizon cost estimate $\hat{C}^V_k:=\frac{1}{l}\sum_{t=0}^{l-1} x_t^{(k)~\top}(Q+\hat{K}^{\top}_iR\hat{K}_i)x_t^{(k)}$;}
      \ENDFOR
      \RETURN{Baseline function estimate $\hat{b}_s(x_0):=\frac{1}{n_v}\sum^{n_v}_{k=1}\hat{C}^V_k$.}
  \end{algorithmic}
\end{algorithm}

Using the estimated baseline function $\hat{b}_s$, the gradient estimation algorithm can be formulated as follows:
\begin{algorithm}[H]
  \caption{Gradient estimation with variance reduction}\label{AlgoVV}
  \begin{algorithmic}
      \REQUIRE{Gain matrix $K\in \mathcal{S}$, number of rollouts $n_b$, rollout length $l$, exploration radius $r$, an upper bound of the initial state $L_0$ }
      \FOR{$k=1,...,n_b$}
             \STATE{ 1. Generate a sample initial state $x_0^{(k)}$ with $\lVert x_0^{(k)} \rVert\leq L_0$;}
             \STATE{2. Estimate the baseline function $\hat{b}_s(x_0^{(k)})$ using Algorithm \ref{AlgoV}}
        \STATE{ 3. Generate a sample gain matrix $\hat{K}_k=K+U_k$, where $U_k$ is drawn uniformly at random over matrices of compatible dimensions with Frobenius norm $r$; }
 
        \STATE{ 4. Excite the closed-loop system with $u_t^{(k)}=\hat{K}_kx_t^{(k)}$ for $l$-steps starting from $x_0^{(k)}$, yielding the state sequence $\left\{ x_t^{(k)} \right\}_{t=0}^{l-1}$;}
        \STATE{ 5. Collect the empirical finite-horizon cost estimate $\hat{C}_k:=\frac{1}{l}\sum_{t=0}^{l-1} x_t^{(k)~\top}(Q+\hat{K}^{\top}_kR\hat{K}_k)x_t^{(k)}$}
      \ENDFOR
      \RETURN{Gradient estimate $\hat{\nabla}C(K):=\frac{1}{n_b}\sum^{n_b}_{k=1}\frac{n_xn_u}{r^2}(\hat{C}_k-\hat{b}_s(x_0^{(k)}))U_k$}
  \end{algorithmic}
\end{algorithm}

Compared with Algorithm \ref{Algo1}, Algorithm \ref{AlgoVV} introduces an additional step involving the baseline function to reduce the variance of the estimated gradient. The following theorem shows that this modification of the algorithm brings a provable improvement in the sample complexity required to estimate the gradient with a given accuracy:
\begin{Theorem}[Performance Improvement]\label{TheoremVariancereduction}
    Given the same tolerance $\epsilon=\epsilon_d+\epsilon_l+\epsilon_n+\epsilon_r$ and probability $\delta=(1-\delta_d)(1-\delta_n)\delta_x$ introduced in Theorem \ref{Theorem4}, for a given $K\in \mathcal{S}$,
    then the estimated gradient $\hat{\nabla}C(K)$ from Algorithm \ref{AlgoVV} enjoys the following bound:
    \begin{equation}
    \mathbb{P}\left\{\lVert \hat{\nabla}C(K)- {\nabla}C(K)\rVert\leq \epsilon \right\}\geq \delta,
\end{equation}
    if $n_b,l$ and $r$ in Algorithm \ref{AlgoVV} satisfy: 
    \begin{subequations}
        \begin{align}
            r&\leq r_{\max}(C(K),\epsilon_r),l\geq l_{\min}(C(K),\epsilon_l),\label{1aaa}\\
            n_b&\geq \max(N_1,N_3(\hat{b}_s(x_0^{(k)}))\label{1bbb}
        \end{align}
    \end{subequations}
    where the detailed expression of $N_3$ is given in \eqref{n3expression} in Appendix \ref{ProofVariance} and $r_{\max}$, $l_{\min}$ and $N_1$ were introduced in Theorem \ref{Theorem4}.
    
    Moreover, let the desired probability $\delta_v\in(0,1)$ be decomposed as $\delta_v=(1-\tilde{\delta}_v)\tilde{\delta}_x$. If the probability $\delta_x$ satisfies $\delta_x\geq \underline{\delta}_x(\Sigma_w)$, and the number of rollouts $n_v$ to estimate the baseline function (as defined in Algorithm \ref{AlgoV}) satisfies 
    \begin{equation}
        n_v\geq \tilde{n}_{\min}(C(K),l,\tilde{\delta}_v,\tilde{\delta}_x),
    \end{equation}
    then:
    \begin{equation}\label{1ccc}
        \mathbb{P}\left\{N_2 \geq  N_3 \right\}\geq \delta_v,
    \end{equation}
    where $N_2$ was first introduced in \eqref{N2} in Theorem \ref{Theorem4} and the detailed expressions of $\underline{\delta}_x,\tilde{n}_{\min}$ are provided in \eqref{deltax} and \eqref{tilden} in Appendix \ref{ProofVariance}. 
\end{Theorem}
The proof of Theorem \ref{TheoremVariancereduction} is given in Appendix \ref{ProofVariance}. The probability decompositions in Theorem \ref{TheoremVariancereduction} include: 
\begin{itemize}
    \item $\tilde{\delta}_x$: the probability that the sample remains bounded.
    \item $\tilde{\delta}_v$: the probability associated with the concentration of the sample.
\end{itemize}

Based on Theorem \ref{TheoremVariancereduction}, we can compare the performance of Algorithm \ref{Algo1} without variance reduction and Algorithm \ref{AlgoVV} with variance reduction. The exploration radius $r_{\max}$ and rollout length $l_{\min}$ \eqref{1aaa} remain the same for both algorithms. The number of rollouts \eqref{1bbb} is determined by the maximum of two arguments. $N_1$ remains unchanged across the two algorithms, as it depends on the smoothing function (see \eqref{C3eq})and the original function. From the second part of Theorem \ref{TheoremVariancereduction} (\eqref{1ccc}), it then follows that the required number of rollouts is at least non-increasing when the baseline function estimates are sufficiently accurate. As noted in Remark \ref{Remark42}, $\delta_x$ represents the probability that the data samples remain bounded. Increasing $\delta_x$ leads to a larger bound for the data samples. This upper bound is utilized in the matrix concentration inequality to determine the required number of rollouts. From \eqref{Nexpression1} and \eqref{Nexpression2}, we observe that, for large $\Sigma_w$ and thus large noise, the required number of rollouts is determined by $N_2$ (in Algorithm \ref{Algo1}) and $N_3$ (in Algorithm \ref{AlgoVV}). %By choosing $\delta_x$ to be sufficiently large, the benefits of introducing variance reduction can be demonstrated. 
In other words, the proposed variance reduction gives a provable reduction of the number of rollouts (and thus of samples) for scenarios with large noise, which is intuitive and expected.
\subsection{Gradient Descent with Adaptive Step Size}\label{secMFGD}
Building on the gradient uncertainty quantified in the previous section, we now proceed to study the convergence of the model-free gradient descent algorithm.
\begin{Theorem}[Model-free Gradient Descent with Adaptive Step Size] \label{MFPGV}
    Suppose the initial $\hat{K}_0\in \mathcal{S}$, and consider the gradient descent with adaptive step size:
    \begin{equation}\label{GDS}
    \hat{K}_{i+1}=\hat{K}_i-\eta_i \hat{\nabla} C(\hat{K}_i), \quad \forall i \in \mathbb{Z}_+,
\end{equation}
where $\hat{\nabla} C(\hat{K}_i)$ is the gradient estimate from Algorithm \ref{Algo1} and $0 <\eta_i \leq h_{\mathrm{GD}}(C(\hat{K}_i))$, where the function $h_{\mathrm{GD}}$ was introduced in Theorem \ref{PGTheorem}. Given any accuracy $\epsilon >0 $, probability $\delta \in (0,1)$ and $\sigma\in (0,1)$, if the estimation error of the gradient $\hat{\nabla} C(\hat{K}_i)$ satisfies:
\begin{equation*}
    \mathbb{P}\left\{\lVert \hat{\nabla}C(\hat{K}_i)- {\nabla}C(\hat{K}_i)\rVert\leq \frac{\sigma\epsilon\lambda_1{(R)} \lambda_1^2(\Sigma_w)}{h_{C}(C(\hat{K}_i))\lVert \Sigma_{K^*}\rVert} \right\}\geq \delta,
\end{equation*}
where $h_C$ was introduced in Lemma \ref{PerturbationCK}, then for any $C(\hat{K}_i)\geq C(K^*)+\epsilon$, the following inequality holds:
\begin{equation*}
        \mathbb{P}\left\{C(\hat{K}_{i+1})-C(K^*) \leq  \gamma_\mathrm{GD} (C(\hat{K}_{i})-C(K^*))\Big|C(\hat{K}_i)\right\}\geq \delta,
\end{equation*}
where $\gamma_i:= 1-(1-\sigma)\frac{2\eta_i \lambda_1{(R)} \lambda_1^2(\Sigma_w)}{\lVert \Sigma_{K^*}\rVert}$ and $\gamma_i < 1, \forall i \in \mathbb{Z}_+$. 

As a result, the gradient descent method enjoys the following performance bound:
\begin{equation*}
    \mathbb{P}\left\{ \min_{i\in [0,n_{\mathrm{GD}}]}C(\hat{K}_i)-C(K^*)  \leq \epsilon \right\}\geq \delta^{n_{\mathrm{GD}}} ,
\end{equation*}
if the number of iterations $n_{\mathrm{GD}}$ satisfies:
\begin{equation*}
    n_{\mathrm{GD}} \geq  \frac{\lVert \Sigma_{K^*}\rVert}{2(1-\sigma)\eta_\mathrm{GD} \lambda_1(R)\lambda^2_1(\Sigma_w)}\log \frac{C(\hat{K}_{0})-C(K^*)}{\epsilon},
\end{equation*}
where ${\eta_\mathrm{GD}}:=\min\limits_{i\in [0,n_{\mathrm{GD}}]}\eta_i$.
\end{Theorem}
%\textcolor{red}{
%Then gradient descent method enjoys the following performance bound:
%\begin{equation*}
%    \mathbb{P}\left\{ C(\hat{K}_N)-C(K^*)  \leq \epsilon \right\}\geq \delta^N ,
%\end{equation*}
%if the number of iterations $N$ satisfies:
%\begin{equation*}
%    N \geq  \frac{\lambda_1(\Sigma_w)}{2(1-\sigma)\eta \lambda_1(R)\lambda^2_1(\Sigma_w)}\log \frac{C(\hat{K}_{0})-C(K^*)}{\epsilon}.
%\end{equation*}}
\begin{algorithm}[H]
  \caption{Model-free gradient descent with adaptive step size}\label{Algo2}
  \begin{algorithmic}
      \REQUIRE{An initial stabilizing gain matrix $\hat{K}_0$, desired accuracy $\epsilon$ and probability $\delta$}.
      \FOR{$i=1,...,\infty$}
      \STATE {1. Compute the required rollouts $n_i$, exploration radius $r_i$ and rollout length $l_i$ based on Theorem \ref{Theorem4} or \ref{TheoremVariancereduction} to achieve the accuracy stated in Theorem \ref{MFPGV}.}
    \STATE {2. Use Algorithm \ref{Algo1} or \ref{AlgoVV} to estimate the gradient $\hat{\nabla} C(\hat{K}_i)$.}
      \STATE{ 3. Update the gradient as $\hat{K}_{i+1}=\hat{K}_i-\eta_i \hat{\nabla} C(\hat{K}_i)$, with $\eta_i\leq h_\mathrm{GD}(C(\hat{K}_i))$.}
      \ENDFOR 
  \end{algorithmic}
\end{algorithm}
To achieve the desired accuracy of the estimates stated in Theorem \ref{MFPGV}, the corresponding values of $l_i, r_i, n_i$, as defined in Algorithm \ref{Algo2}, should be selected based on Theorem \ref{Theorem4}. Theorem \ref{MFPGV} ensures that the cost will converge to the optimal solution with a predefined accuracy gap $\epsilon$, meaning $C(K^*)+\epsilon$ will be reached with a certain probability. However, due to the estimation error in the gradient, further improvements in cost stop when the realized cost $C(\hat{K}_i) \leq C(K^*)+\epsilon$. To achieve higher accuracy, it is necessary to minimize the gradient estimation error. Nevertheless, the algorithm can only converge to the optimal when the gradient estimation is exact, which is not possible with a finite number of rollouts. The interpretation of the $\sigma$ is given in Remark \ref{mbmfsigma}. Regarding the step size $\eta_i$ stated in Theorem \ref{MFPGV}, from the expression of $h_\mathrm{GD}$ given in \eqref{pG2}, we observe that as the noise level $\Sigma_w$ increases, the step size $\eta_i$ decreases, reflecting the observation analyzed in Section \ref{simu24}.

In Algorithm \ref{Algo2}, the required rollouts $n_i$, exploration radius $r_i$ and rollout length $l_i$ are determined adaptively to the cost $C(\hat{K}_i)$, i.e., they are computed online inside the for-loop. This design is aligned with Theorem \ref{MFPGV}, which shows that the cost function $C(\hat{K}_i)$ decreases probabilistically with each iteration. As the cost $C(\hat{K}_i)$ decreases, this leads to a reduction in the required rollouts $n_i$ and rollout length $l_i$, while the exploration radius $r_i$ increases. Alternatively, the algorithm can be modified to compute these parameters offline, where for all iterations $i$, $n_i,l_i,r_i$ are calculated once based on the initial cost $C(\hat{K}_0)$.


\subsection{Natural Policy Gradient with Adaptive Step Size}\label{secMFNPG}
Analogous to the model-free gradient descent method, we now turn to analyzing the convergence of the natural policy gradient method.
\begin{Theorem}[Model-free Natural Policy Gradient with Adaptive Step Size] \label{MFNPGV}
    Suppose the initial $\hat{K}_0\in \mathcal{S}$, and consider natural policy gradient with adaptive step size:
    \begin{equation}\label{theorem4.7update}
    \hat{K}_{i+1}=\hat{K}_i-\eta_i \hat{\nabla}C(\hat{K}_i)\hat{\Sigma}_{\hat{K}_i}^{-1},\quad \forall i \in \mathbb{Z}_+,
\end{equation}
where $\hat{\nabla} C(\hat{K}_i)$ and $\hat{\Sigma}_{\hat{K}_i}$ are the gradient and covariance estimates from Algorithm \ref{Algo1} and $0<\eta_i\leq \frac{1}{2\lVert R\rVert+\frac{2\lVert B \rVert C(\hat{K}_i)}{\lambda_1(\Sigma_w)}}$. Given any accuracy $\epsilon> 0$, probability $\delta\in (0,1)$ and $\sigma\in (0,1)$, if the estimation error of the gradient $\hat{\nabla} C(\hat{K}_i)$ and the covariance $\hat{\Sigma}_{\hat{K}_i}$ satisfy:
\begin{subequations}
\begin{align}
    \mathbb{P}&\left\{\lVert \hat{\nabla} C(\hat{K}_i) -{\nabla} C(\hat{K}_i)\rVert \leq \frac{\sigma\epsilon\lambda_1{(R)} \lambda_1^2(\Sigma_w)}{4h_{\nabla}(C(\hat{K}_i))\lVert \Sigma_K^*\rVert} \right\}\geq \sqrt{\delta},\label{1a}\\
    \mathbb{P}&\left\{\lVert\hat{\Sigma}_{\hat{K}_i} -{\Sigma}_{\hat{K}_i} \rVert \leq \frac{\sigma\epsilon \lambda_1{(R)} \lambda_1^3(\Sigma_w)}{4h_{\nabla}(C(\hat{K}_i))\lVert \Sigma_K^*\rVert\sqrt{b_{\nabla}(C(\hat{K}_i))}}\right\}\geq \sqrt{\delta},\label{1b}
\end{align}
\end{subequations}
where $h_{\nabla}$ was introduced in Lemma \ref{PerturbationnablaCK} and $b_{\nabla}$ was defined in \eqref{boundedgradienteq} in Appendix \ref{ProofPerturbationnablaCK}, then for any $C(\hat{K}_i)\geq C(K^*)+\epsilon$, the following inequality holds:
\begin{equation*}
        \mathbb{P}\left\{    C(\hat{K}_{i+1})-C(K^*) \leq  \kappa_i (C(\hat{K}_{i})-C(K^*))\Big|C(\hat{K}_i) \right\}\geq \delta,
\end{equation*}
where $\kappa_i:=\left( 1-(1-\sigma)\frac{2\eta_i \lambda_1{(R)} \lambda_1(\Sigma_w)}{\lVert \Sigma_K^*\rVert}\right)$ and $\kappa_i<1,\forall i \in \mathbb{Z}_+$. 

As a result, the natural policy gradient method enjoys the following performance bound:
\begin{equation*}
    \mathbb{P}\left\{ \min_{i\in[0,n_\mathrm{NPG}]}C(\hat{K}_i)-C(K^*)  \leq \epsilon \right\}\geq \delta^{n_\mathrm{NPG}} ,
\end{equation*}
if the number of iterations $n_\mathrm{NPG}$ satisfies:
\begin{equation*}
    n_\mathrm{NPG} \geq \frac{\lVert \Sigma_{K^*}\rVert}{2(1-\sigma)\eta_{\mathrm{NPG}}\lambda_1(\Sigma_w)}\log \frac{C(K_{0})-C(K^*)}{\epsilon},
\end{equation*}
where ${\eta_\mathrm{NPG}}:=\min\limits_{i\in [0,n_{\mathrm{NPG}}]}\eta_i$.
\end{Theorem}
    %n_\mathrm{NPG} \geq \frac{\lVert \Sigma_{K^*}\rVert}{2(1-\sigma)\lambda_1(\Sigma_w)} \left( \frac{\lVert R \rVert}{\lambda_1(R)} +\frac{\lVert B \rVert^2 C(K_0)}{\lambda_1{(R)} \lambda_1(\Sigma_w)}\right)\log \frac{C(K_{0})-C(K^*)}{\epsilon}.
From Theorem \ref{Theorem4}, we can select the values of $l^{\nabla}_i,r^{\nabla}_i,n^{\nabla}_i$ to satisfy the requirement for the gradient estimation error, as indicated in \eqref{1a}. Similarly, we can select $l^{\Sigma}_i,r^{\Sigma}_i,n^{\Sigma}_i$ to satisfy the requirement for the covariance estimation error, as described in \eqref{1b}. To ensure both requirements are met, we can then set: $n_i \geq\max\{n^{\nabla}_i,n^{\Sigma}_i\},l_i\geq \max\{l^{\nabla}_i,l^{\Sigma}_i\}, r_i\leq \min\{r^{\nabla}_i,r^{\Sigma}_i\}$. These choices guarantee that the estimates of both the gradient and the covariance matrix satisfy their respective accuracy requirements. This combined selection of parameters is implemented in Algorithm \ref{Algo3} to ensure convergence with the desired accuracy and probability.
\begin{algorithm}[H]
  \caption{Model-free natural policy gradient with adaptive step size}\label{Algo3}
  \begin{algorithmic}
      \REQUIRE{An initial stabilizing gain matrix $\hat{K}_0$, desired accuracy $\epsilon$ and probability $\delta$}.
      \FOR{$i=0,...,\infty$}
    \STATE {1. Compute the required rollouts $n_i$, exploration radius $r_i$ and rollout length $l_i$ based on Theorem \ref{Theorem4} to achieve the accuracy stated in Theorem \ref{MFNPGV}.} 
    \STATE {2. Use Algorithm \ref{Algo1} to estimate the gradient $\hat{\nabla}C(\hat{K}_i)$ and covariance $\hat{\Sigma}_{\hat{K}_i}$.}
      \STATE{ 3. Update the gradient as $\hat{K}_{i+1}=\hat{K}_i-\eta_i \hat{\nabla} C(\hat{K}_i)\hat{\Sigma}^{-1}_{\hat{K}_i}$ with $\eta_i\leq \frac{1}{2\lVert R\rVert+\frac{2\lVert B \rVert C(\hat{K}_i)}{\lambda_1(\Sigma_w)}}$.}
      \ENDFOR 
  \end{algorithmic}
\end{algorithm}

\begin{Remark}[Qualitative effect of gradient and covariance errors on convergence] \label{mbmfsigma}
Model-free algorithms are extensions of model-based policy gradient methods. However, in the model-free setting, the convergence rate is typically slower, reduced by a factor of ($1-\sigma$) compared to the model-based case. This reduction is due to estimation errors in the gradient and covariance matrices, which are inherent in model-free methods as they rely on data-driven estimates rather than exact model information. When the estimated gradient closely matches the true gradient, the model-free policy gradient approaches the same convergence rate as the model-based policy gradient, corresponding to $\sigma \to0$. However, if the estimation error becomes large, such that $\sigma\to 1$, further policy improvements become impossible, and the algorithm can no longer guarantee convergence to the optimal with the desired accuracy. This effect of gradient estimation error on the convergence rate and suboptimality gap aligns with our observations in Section \ref{simu24}, as illustrated by the black dotted line, blue dashed line, and green dash-dotted line in Figure \ref{fig:MB}. 
\end{Remark}
\subsection{Comparison of Model-free PG for Noise and Noise-free Case}
In Algorithm \ref{Algo1}, three key quantities, exploration radius $r$, rollout length $l$, and the required rollouts $n$, play a central role in determining the estimation errors on gradient and covariance estimation. The presence of stochastic noise acting on the system \eqref{LTI} during data collection influences  these quantities in the following ways:
\begin{itemize}
    \item \textbf{Exploration radius:} The selection of the exploration radius is fully determined by the desired accuracy and the specific model parameters. The choice of $r$ directly influences the estimation error by affecting the bias term $U_i$, which represents the difference between quantities $\lVert \nabla C(K) \rVert$ and $\lVert \nabla C(K+U_k) \rVert$, as well as$\lVert \Sigma_K \rVert$ and $\lVert \Sigma_{K+U_k} \rVert$. These differences are crucial in the error analysis, as detailed in Appendix \ref{MC} and \ref{ProofTheorem5}, respectively. The estimation error of the gradient is discussed in detail in \eqref{radius} for the noise case and in \cite[Lemma 27]{pmlr-v80-fazel18a} for the noise-free case. Similarly, the estimation error of the covariance is analyzed in \eqref{radius2} for the noise case and in \cite[Theorem 30]{pmlr-v80-fazel18a} for the noise-free case. Due to the differences in cost functions, the noise-free case considers the infinite-horizon cost, while the noisy case considers the average infinite-horizon cost. Consequently, for the noisy case, $r$ is determined by $\Sigma_w$ whereas for the noise-free case, it is determined by $\Sigma_0$.
    \item \textbf{Rollout length:} The rollout length $l$ is different for the noise-free case and the noise case. In both scenarios, the rollout length plays a crucial role in estimating the gradient and covariance, whose true values are defined over an infinite horizon. However, finite approximations must be used. In the noise-free case, as described in \cite[Lemma 23]{pmlr-v80-fazel18a}, the rollout length is determined by the desired accuracy, model parameters, and initial covariance $\Sigma_0$. In the presence of noise, as discussed in Lemma \ref{Lemmafinitness}, the rollout length is influenced not only by $\Sigma_0$, but also by the noise covariance $\Sigma_w$. Therefore, the rollout length in the noisy case must account for both the system's initial conditions and noise disturbances. These relationships are quantitatively explored in Lemma \ref{Lemmafinitness} through \eqref{ll1} and \eqref{ll2}.
    
    \item \textbf{Required rollouts:} 
    The most significant difference between the noisy and noise-free cases comes from using the matrix concentration inequality for the trajectories consisting of noisy data. In the noise-free case, the boundedness of the data is guaranteed by an upper bound on the initial state, i.e., $\lVert x_{0}^{(k)} \rVert\leq L_0$ (see \cite[Lemmas 27, 29]{pmlr-v80-fazel18a}). In contrast, in the noisy case, particularly with unbounded Gaussian noise, the state remains bounded only with a certain probability (Appendix \ref{Proofboundedness}). This boundedness in probability is then used to apply matrix concentration inequalities, providing an upper bound on the covariance of the samples(Appendix \ref{ProofTheorem4}). 
    
    
    
   % In the proof of Theorem \ref{Theorem4}, Bernstein's matrix concentration inequality is applied twice: once for the function smoothing and once for the noisy data. In the noise-free case, as discussed in \cite[Lemma 27]{pmlr-v80-fazel18a}, the required number of rollouts to guarantee the concentration of the function smoothing also ensures the concentration of the noise-free data. In the noisy case, while the concentration of function smoothing remains similar (Lemma \ref{MC}), differences arise due to the rollout length, affecting the upper bounds on the state $\lVert x_{l-1}^{(k)} \rVert$. These variations in the state affect the concentration of the trajectory data, as discussed in Lemma \ref{ProofTheorem4}. The presence of noise increases the magnitude of the state variables, leading to different upper bounds compared to the noise-free case. Similarly, in the proof of Theorem \ref{Theorem5}, the concentration of noisy data (Lemma \ref{ProofTheorem5}) differs from the noise-free case \cite[Lemma 29]{pmlr-v80-fazel18a}, resulting in variations in the number of required rollouts.
\end{itemize}

\section{Simulation Results}
\label{sec:Simulation}
In this subsection, we use numerical simulations \footnote{The Matlab codes used to generate these results (in Section \ref{simu24} and Section \ref{sec:Simulation}) are accessible from the repository: \url{https://github.com/col-tasas/2025-PGforLQRwithStochastic}} to demonstrate the benefits of incorporating the variance reduction technique and adaptive step sizes in model-free policy gradient algorithms and the overall effect of noise on them. The matrices $(A,B,Q,R)$ and $\hat{K}_0$ are the same as described in Section \ref{simu24}. The rollout length, exploration radius, and number of rollouts are set to $l=100,~r=0.05,~n=1000$, respectively.
\subsection{Model-free GD with Variance Reduction and Adaptive Step Sizes}
  The update of the policy is according to the gradient descent method, whose update law was defined in \eqref{GDS}. The step size $\eta$ is chosen based on the noise level: for $\Sigma_w=10^{-4}I$ is set to $100$, while for $\Sigma_w=10^{-2}I$, it is reduced to $1$, qualitatively in accordance with the bounds established in Theorem \ref{MFPGV}. For the variance reduction technique, the number of rollouts to estimate the baseline function, $n_b$, is set as $20$. We consider a time-varying step size to illustrate the behavior of the algorithm with an adaptive step size. For noise $\Sigma_w=10^{-2}I$, the adaptive stepsize is set as $\eta_i=1.08^i$, increasing over iterations. The step size continues to grow until the cost no longer decreases significantly (until $i=15$).
\begin{figure}[H] 
    \centering
    \includegraphics[width=0.85\linewidth]{Figure2_GD.pdf}
    \caption{Model-free GD with/out Variance Reduction and with/out Adaptive Step Size}
    \label{fig:VRVS}
\end{figure}
Figure \ref{fig:VRVS} shows the relative suboptimality gap of the iterated policy $\hat{K}_i$ as a function of the iteration counter $i$. From it, we can draw a number of observations:

\begin{itemize} 
\item Suboptimal gap: As analyzed in Theorem \ref{MFPGV}, due to the estimation error in the gradient, the model-free gradient descent algorithm does not converge to the optimal solution but finds suboptimal policies.
\item Step size selection: When the noise level is higher, the step size must be decreased, consistent with the expression of $h_\mathrm{GD}$ (see \eqref{pG2}) introduced in Theorem \ref{MFPGV}. For adaptive step sizes, the algorithm exhibits a faster convergence rate compared to using fixed step sizes, highlighting the advantages of adaptive strategies. 
\item Variance reduction impact: For the policy gradient method with variance reduction (green solid line and black dashed line), the gradient estimation error is significantly reduced compared to the method without variance reduction. This leads to a faster convergence rate, as evidenced by comparing the green solid line with the red dotted line and the black dashed line with the blue dot-dashed line. The performance improvement from variance reduction is more pronounced in high-noise scenarios, consistent with the analysis in Theorem \ref{TheoremVariancereduction}. \end{itemize}


\subsection{Model-free NPG with Adaptive Step Sizes}\label{Simu2}
The natural policy gradient algorithm updates the gain $\hat{K}_i$ according to the natural policy gradient method, whose update law was defined in \eqref{theorem4.7update}.  The step size is chosen as $\eta=0.1$ across all noise levels $\Sigma_w=10^{-4}I,10^{-2}I,10^{0}I$. Similar to Model-free GD, the time varying step size is set as $\eta_i=1.01^i\eta$, increasing over iterations. The step size continues to grow until the cost no longer decreases significantly (until $i=15$).
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{Figure3_NPG.pdf}
    \caption{Model-free NPG with/out Adaptive Step Size}
    \label{fig:NPG}
\end{figure}
As shown in Figure \ref{fig:NPG}, the model-free NPG algorithm achieves nearly the same accuracy even as the noise $\Sigma_w$ increases, provided that $l,r,n$ remain unchanged. With higher noise levels, the estimation errors of $\hat{\nabla}C(\hat{K}_i)$ and $\hat{\Sigma}_{\hat{K}_i}$ increase. In our analysis in Theorems \ref{Theorem4} and \ref{Theorem5}, these estimation errors are explicitly captured. Due to the update rule in the natural policy gradient method (see \eqref{theorem4.7update}), the estimated gradient is multiplied by the inverse of the estimated covariance. Interestingly, the errors in these two terms cancel out due to this inversion. If the error could be analyzed directly for the product $\hat{\nabla}C(\hat{K}_i)\hat{\Sigma}_{\hat{K}_i}^{-1}$ rather than treating them separately (as done in Proof \ref{ProofMFNPG}), significantly stronger theoretical guarantees could be obtained. This observation highlights an important direction for future research.
For adaptive step sizes, the algorithm demonstrates a faster convergence rate compared to using fixed step sizes, further supporting the potential benefit of adaptive strategies.


%from the analysis in Algorithm \ref{Algo3}, the step size $$\eta= \frac{1}{2\lVert R\rVert+\frac{2\lVert B \rVert C(\hat{K}_0)}{\lambda_1(\Sigma_w)}}=\frac{1}{2\lVert R\rVert+2\lVert B \rVert Tr(P_{\hat{K}_0})}$$.
% \subsection{Model-based Policy Gradient}
% In this subsection, we demonstrate the benefits of employing a time-adaptive step size for model-based policy gradient methods. We consider a linear time-invariant (LTI) system previously studied in the literature \cite{9691800, articlesimulation, AbbasiYadkori2018ModelFreeLQ}, described by the following dynamics:
% \begin{equation}\label{LTIsimulation}
%   x_{t+1}=\underbrace{\left[\begin{array}{ccc}
%             1.01 & 0.01 & 0 \\
%             0.01 & 1.01 & 0.01 \\
%             0 & 0.01 & 1.01 
%           \end{array}\right]}_A x_t+\underbrace{\left[\begin{array}{ccc}
%             1 & 0 & 0 \\
%             0 & 1 & 0 \\
%             0 & 0 & 1 
%           \end{array}\right]}_B u_t+w_t.
% \end{equation}
% The weighting matrices $Q$ and $R$ are set to $100I_3$ and $I_3$, respectively. The noise $w_t$ follows a Gaussian distribution $\mathcal{N}(0,\Sigma_w)$ with $\Sigma_w=I_3$. The initial gain $K_0$ is initialized to the optimal gain, using weighting matrix $Q_0=0.001Q$ and $R_0=R$.\\

% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.6\linewidth]{Modelbased.eps}
%     \caption{Model-based PG}
%     \label{fig:MB}
% \end{figure}

% As shown in Figure \ref{fig:MB}, we know that the Gauss-Newton (GN) method achieves the fastest convergence to the optimal $C(K^*)$ among the proposed methods. The step size $\eta$ in Gauss-Newton method is fixed at $\frac{1}{2}$. For the natural policy gradient method, the fixed step size (NPG Fixed) and adaptive step size (NPG adaptive) are strictly following the results in Theorem \ref{NPGTheorem} and \ref{NPGV}. For the gradient descent method, due to the conservative nature of the theoretical results, we choose the fixed step size $\eta=0.00005$ (GD Fixed) and the adaptive step size $\eta_i=1.005^i\eta$ (GD adaptive), which are both larger than our theoretical results. By comparing the fixed step size with the adaptive step size for both the gradient descent and the natural policy gradient methods, we observe that the adaptive step size enhances the convergence rate, allowing for more efficient optimization and faster achievement of the optimal cost.
% \subsection{Model-free Policy Gradient}
% In this subsection, we demonstrate the model-free gradient descent and the natural policy gradient methods by considering the following system:
% \begin{equation}\label{LTIsimulation2}
%   x_{t+1}=\underbrace{\left[\begin{array}{ccc}
%             0.5 & 0.01 & 0 \\
%             0.01 & 0.5 & 0.01 \\
%             0 & 0.01 & 0.5 
%           \end{array}\right]}_A x_t+\underbrace{\left[\begin{array}{ccc}
%             0.1 & 0 & 0 \\
%             0 & 0.1 & 0 \\
%             0 & 0 & 0.1 
%           \end{array}\right]}_B u_t+w_t.
% \end{equation}
% Here, the weighting matrices are set to $Q=I_3$ and $R=0.1I_3$. The noise $w_t$ follows a Gaussian distribution $\mathcal{N}(0,\Sigma_w)$ with $\Sigma_w=0.0001I_3$. The initial covariance $\Sigma_0$ is set to $0.0001I_3$. The initial gain $K_0$ is set to the optimal gain, calculated using a weighting matrix $Q_1=1000Q$ and $R_1=R$. For both our algorithm and the baseline method from \cite{pmlr-v80-fazel18a}, the following setting are used: the rollout length $l=500$, exploration radius $r=0.1$ and the number of rollouts $n=600$. The fixed step size for NPG and GD from \cite{pmlr-v80-fazel18a} are set to $0.0001$ and $0.0005$, respectively. For our work, the adaptive step size for NPG and GD are set to: $\eta_i=1.002^i \times 0.001$ for NPG and $\eta_i=1.003^i \times 0.5$ for GD. All four lines shown in the figure below represent the mean value over three simulations. \textcolor{red}{(In the next version this figure will be replaced by more simulations.)}\\

% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.6\linewidth]{Modelfree.eps}
%     \caption{Model-free PG}
%     \label{fig:MF}
% \end{figure}

% From the figure, we can have the following observations: 
% \begin{itemize}
%     \item Accuracy Gap in Model-free NPG and GD with adaptive Step Sizes: as discussed in Theorems \ref{MFPGV} and \ref{MFNPGV}, the model-free natural policy gradient (NPG) and gradient descent (GD) algorithms with adaptive step sizes can only converge to the optimal solution with a certain accuracy gap. This gap arises due to the estimation errors in both the gradient and covariance estimation. Despite the convergence, the algorithms are constrained by these inherent errors when estimating the gradient and covariance in a model-free setting.
%     \item Impact of Noise on Convergence Rate and Accuracy Gap: By observing the dashed lines representing the NPG and GD methods from \cite{pmlr-v80-fazel18a}, we can see that the noise in the system results in a larger accuracy gap and a slower convergence rate.
% \end{itemize}
\section{Conclusions}
\label{sec:conclusions}
In this work, we applied the policy gradient method to the Linear Quadratic Regulator problem with stochastic noise, analyzing both model-based and model-free approaches. Convergence guarantees were established for the model-based gradient descent and natural policy gradient methods with adaptive step sizes, demonstrating their ability to converge to the global optimum. For the model-free gradient descent and natural policy gradient methods, we showed that they can converge to the optimal solution within any desired accuracy. However, it is crucial that the algorithm's parameters are tuned based on factors such as the noise magnitude affecting the data trajectories. To improve convergence rates and reduce sample complexity, in addition to adaptive step sizes, we introduced a variance reduction technique. We also provided a qualitative discussion on the impact of noise on model-free policy gradient methods, offering insights into their robustness and providing useful guidelines for their tuning under practical scenarios.

Several open problems remain to be addressed. As previously mentioned, implementing the model-free Gauss-Newton method presents an interesting opportunity for future research. While this work focused primarily on direct data-driven policy gradient approaches, it would also be valuable to explore and compare indirect data-driven policy gradient methods, which integrate online parameter estimation with model-based gradient descent. A thorough comparison of direct and indirect PG methods in the presence of stochastic noise is an important topic for future study.


\section*{Acknowledgments}
We would like to thank Nicolas Chatzikiriakos for his constructive discussion and valuable feedback.
Bowen Song acknowledges the support of the International Max Planck Research School for Intelligent Systems (IMPRS-IS). Andrea Iannelli acknowledges the German Research Foundation (DFG) for support of this work under Germanys Excellence Strategy - EXC 2075  390740016.
\appendix
\section{Proof of Results in Section \ref{sec:Preliminaries}}
\subsection{Proof of Lemma \ref{LemmaGD}}\label{ProofLemmaGD}
\begin{Proof}
    To prove Lemma \ref{LemmaGD}, we introduce the kernel defined as $A(K',K):=P^{K'}_K-P_K$, where $P^{K'}_K:=Q_{K'}+(A+BK')^\top P_{K} (A+BK')$ with $K',K \in \mathcal{S}$, which works as a bridge to connect $P_K$ and $P_{K'}$. Then we have:
    \begin{equation}\label{A}
\begin{split}
    A(K',K)&=Q+K'^\top R K'+(A+BK')^\top P_{K} (A+BK')-P_K\\
    &=(K'-K)^\top(R+B^\top P_K B)(K'-K)\\
    &~~~~+(K'-K)^\top\left( (R+B^\top P_K B)K+B^\top P_KA\right)\\
    &~~~~+\left( (R+B^\top P_K B)K+B^\top P_KA\right)^\top(K'-K)\\
    &=(K'-K)^\top(R+B^\top P_K B)(K'-K)\\
    &=+(K'-K)^\top E_K+E_K^\top(K'-K).
\end{split}
\end{equation}
A lower bound of \eqref{A} can be derived as:
\begin{equation}\label{ADKernel}
    \begin{split}
        A(K',K)&=(K'-K)^\top(R+B^\top P_K B)(K'-K)\\
        &~~~~+(K'-K)^\top E_K+E_K^\top(K'-K)\\
        &=(\underbrace{K'-K+(R+B^\top P_K B)^{-1} E_K}_{=:\mathcal{A}})^\top(R+B^\top P_K B)\mathcal{A}\\
        &~~~~-E_K^\top (R+B^\top P_K B)^{-1} E_K\\
        &\succeq -E_K^\top (R+B^\top P_K B)^{-1} E_K,
    \end{split}
\end{equation}
with equality when $K'=K-(R+B^\top P_K B)^{-1} E_K$.
Then for $P_{K}$ and $P_{K'}$, we have:
\begin{equation}\label{B}
       P_{K}-P_{K'}=\sum_{t=0}^{\infty}(A_{K}^t)^\top A(K,K') A_{K}^t.
\end{equation}
From \eqref{CostFunction} and \eqref{B}, we set $K'=K^*$ and then have:
\begin{equation}\label{gd}
\begin{split}
    C(K)-C(K^*)&=\mathrm{Tr}\left((P_K-P^*)\Sigma_w\right)\\
    &=\mathrm{Tr}\left(-\left(\sum_{t=0}^{\infty}(A_{K^*}^t)^\top A(K^*,K) A_{K^*}^t\right)\Sigma_w\right)\\
    &\leq\mathrm{Tr}\left(\left(\sum_{t=0}^{\infty}(A_{K^*}^t)^\top E_K^\top (R+B^\top P_K B)^{-1} E_K A_{K^*}^t\right)\Sigma_w\right) \\
    &=\mathrm{Tr}\left( E_K^\top (R+B^\top P_K B)^{-1} E_K \Sigma_{K^*}\right). \\
\end{split}
\end{equation}
From \eqref{Gradient} and $\Sigma_K \succeq \Sigma_w \succ 0$, we can get the expression of $E_K$ as:
\begin{equation*}
    E_K=\frac{1}{2}\nabla C(K)\Sigma_K^{-1}
\end{equation*}
Then \eqref{gd} can be further simplified as:
\begin{equation*}
    \begin{split}
    C(K)-C(K^*)&=\mathrm{Tr}\left( E_K^\top (R+B^\top P_K B)^{-1} E_K \Sigma_{K^*}\right) \\
    &=\frac{1}{4}\mathrm{Tr}\left(\left(\Sigma_K\right)^{-1~\top} \nabla C(K)^\top (R+B^\top P_K B)^{-1} \nabla C(K) \left(\Sigma_K\right)^{-1}\Sigma_{K^*}\right) \\
    &\leq\frac{1}{4}\lVert \Sigma_{K^*} \rVert\mathrm{Tr}\left(\nabla C(K)^\top (R+B^\top P_K B)^{-1} \nabla C(K)\right) \\
    &=\frac{1}{4}\lVert \Sigma_{K^*} \rVert\lVert \Sigma_w ^{-2}\rVert\lVert R ^{-1}\rVert\lVert \nabla C(K)\rVert_F^2=\mu \lVert \nabla C(K)\rVert_F^2
\end{split}
\end{equation*}
This concludes the proof.
\end{Proof}
\subsection{Proof of Lemma \ref{AlmostSmoothness}} \label{ProofSmoothness}
\begin{Proof}
From \eqref{B},
    \begin{equation}\label{smooth1}
\begin{split}
    &~~~~~C(K')-C(K)\\
    &=\mathrm{Tr}\left((P_{K'}-P_K)\Sigma_w\right)\\
    &=\mathrm{Tr}\left(\left(\sum_{t=0}^{\infty}(A_{K'}^t)^\top A({K'},K) A_{K'}^t\right)\Sigma_w\right)\\
    &=\mathrm{Tr}\left(\left[({K'}-{K})^\top(R+B^\top P_{K} B)({K'}-{K})+({K'}-{K})^\top E_{K}+E_{K}^\top({K'}-{K})\right]\Sigma_{K'}\right) 
\end{split}
\end{equation}
The third equality is obtained by using \eqref{A}. Then from \eqref{smooth1}, we can get:
\begin{equation*}
    \left\lvert C(K')-C(K) -  2\mathrm{Tr}\left(({K'}-{K})^\top E_K\Sigma_{K'} \right) \right\rvert \leq \lVert \Sigma_{K'}\rVert \left\lVert R+B^\top P_{K} B\right\rVert \left\lVert K'-{K}\right\rVert_F
\end{equation*}
This concludes the proof.
\end{Proof}
\subsection{Proof of Lemma \ref{PerturbationSigmaK}}\label{ProofPerturbationSigmaK}
\begin{Proof}
From \eqref{CostFunction}, we can derive the upper bound on the following matrices: 
\begin{equation}\label{Bound1}
    \lVert P_K \rVert \leq \frac{C(K)}{\lambda_1(\Sigma_w)}, ~~~\lVert \Sigma_{K} \rVert \leq \frac{C(K)}{\lambda_1(Q)}.
\end{equation} 
    Define a linear operator $\mathcal{T}_K$ as:
\begin{equation*}
    \mathcal{T}_K (X):= \sum_{t=0}^{\infty} (A+BK)^t (X) (A+BK)^{t~\top}.
\end{equation*}
Define the induced norm of $\mathcal{T}_K$ as:
\begin{equation*}
    \lVert \mathcal{T}_K \rVert:=\mathop{sup}_X \frac{\lVert \mathcal{T}_K (X) \rVert}{\lVert X \rVert}.
\end{equation*}
From \cite[Lemma 17]{pmlr-v80-fazel18a}, we can derive the upper bound of $\lVert \mathcal{T}_K \rVert$ as
\begin{equation*}
    \lVert \mathcal{T}_K \rVert\leq \frac{C(K)}{\lambda_1(\Sigma_w)\lambda_1(Q)}
\end{equation*}
Define another linear operator $\mathcal{F}_K$ as:
\begin{equation*}
    \mathcal{F}_K(X):=(A+BK)X(A+BK)^\top.
\end{equation*}
Define the identity operator $\mathcal{I}(X):=X$
Then we have the following results from \cite[Lemma 18]{pmlr-v80-fazel18a}:
\begin{equation*}
    \mathcal{T}_K=(\mathcal{I}-\mathcal{F}_K)^{-1}
\end{equation*}
By the definition of $\Sigma_K$
\begin{equation*}
    \Sigma_K=\mathcal{T}_K(\Sigma_w)=(\mathcal{I}-\mathcal{F}_K)^{-1}(\Sigma_w)
\end{equation*}
From \cite[Lemma 19]{pmlr-v80-fazel18a}:
\begin{equation*}
    \lVert \mathcal{F}_K-\mathcal{F}_{K'} \rVert \leq 2\lVert A+BK \rVert \lVert B \rVert \lVert K-K' \rVert + \lVert B \rVert^2 \lVert K-K' \rVert^2
\end{equation*}
From \cite[Lemma 20]{pmlr-v80-fazel18a},
if \begin{equation*}
    \lVert \mathcal{T}_K \rVert\lVert \mathcal{F}_K-\mathcal{F}_{K'} \rVert\leq \frac{1}{2},
\end{equation*}
then:
\begin{equation*}
    \lVert (\mathcal{T}_K - \mathcal{T}_{K'}) (\Sigma_w) \rVert \leq 2\lVert \mathcal{T}_K \rVert^2\lVert \mathcal{F}_K-\mathcal{F}_{K'} \rVert\lVert \Sigma_w \rVert
\end{equation*}
Then from \cite[Lemma 16]{pmlr-v80-fazel18a}, we can prove the result of perturbation of $\Sigma_K$, as stated in Lemma \ref{PerturbationSigmaK}

\end{Proof}

\subsection{Proof of Lemma \ref{PerturbationCK}}\label{ProofPerturbationCK}
\begin{Proof}
    When $\lVert K-K' \rVert= \min \{h(C(K)), \lVert K \rVert\}$, by \cite[Lemma 24]{pmlr-v80-fazel18a}, an upper bound on  $\lVert P_{K'}-P_K\rVert$ can be derived as:
\begin{equation}\label{PertubationPK}
    \begin{split}
        \lVert P_{K'}-P_K\rVert \leq {6 \left( \frac{C(K)}{\lambda_1(\Sigma_w)\lambda_1(Q)}\right)^2\left( 2\lVert K \rVert^2\lVert R \rVert\lVert B \rVert+\lVert K \rVert\lVert R\rVert\right)}\lVert K-K' \rVert
    \end{split}
\end{equation}
We aim to derive an upper bound on $\lVert K \rVert$ as a function of $C(K)$. First, we find the upper bound of the $C(K)-C(K^*)$, we choose $K'=K-(R+B^\top P_K B)^{-1} E_K$, from \eqref{ADKernel}, we can get:
\begin{equation}\label{Upper}
    \begin{split}
    C(K)-C(K^*)&\geq  C(K)-C(K') \\
    &=\mathrm{Tr}\left((P_K-P_{K'})\Sigma_w\right)\\
    &=\mathrm{Tr}\left(-\left(\sum_{t=0}^{\infty}(A_{K'}^t)^\top A(K',K) A_{K'}^t\right)\Sigma_w\right)\\
    &=\mathrm{Tr}\left(\left(\sum_{t=0}^{\infty}(A_{K'}^t)^\top E_K^\top (R+B^\top P_K B)^{-1} E_K A_{K'}^t\right)\Sigma_w\right) \\
    &=\mathrm{Tr}\left( E_K^\top (R+B^\top P_K B)^{-1} E_K P_{K'}^w\right) \\
    &\geq\frac{\sigma_1(\Sigma_w)}{\lVert R+B^\top P_K B \rVert}\mathrm{Tr}\left( E_K^\top  E_K \right) \\
\end{split}
\end{equation}
From \eqref{Upper}, we can also derive the upper bound of $\lVert K \rVert$:
\begin{equation}\label{boundK}
    \begin{split}
            \lVert K \rVert &\leq \frac{1}{\lambda_1(R) } \left( \sqrt{\frac{(C(K)-C(K^*))\lVert R+B^\top P_K B \rVert}{\lambda_1(\Sigma_w)}}+\lVert B^\top P_K A \rVert \right)\\
            &\leq \underbrace{\frac{1}{\lambda_1(R) } \left( \sqrt{\frac{(C(K)-C(K^*))\left(\lVert R\rVert+\lVert B \rVert^2 \frac{C(K)}{\lambda_1(\Sigma_w)}\right)}{\lambda_1(\Sigma_w)}}+\lVert B \rVert \lVert A\rVert\frac{C(K)}{\lambda_1(\Sigma_w)}\right)}_{=:b_{K}(C(K))}\\
    \end{split}
\end{equation} 
From \eqref{CostFunction} and \eqref{PertubationPK}, we can obtain
\begin{equation}\label{Pertubation1}
    \begin{split}
       &~~~ |C(K')-C(K)| =\mathrm{Tr}\left((P_{K'}-P_K) \Sigma_w\right) \leq \lVert P_{K'}-P_K\rVert \mathrm{Tr}(\Sigma_w)\\
        &\leq  \underbrace{6 \left( \frac{C(K)}{\lambda_1(\Sigma_w)\lambda_1(Q)}\right)^2\left( 2b^2_{K}(C(K))\lVert R \rVert\lVert B \rVert+b_{K}(C(K))\lVert R\rVert\right)\mathrm{Tr}(\Sigma_w)}_{=:h_{C}(C(K))} \lVert K-K' \rVert.
    \end{split}
\end{equation}
\end{Proof}

\subsection{Proof of Lemma \ref{PerturbationnablaCK}}\label{ProofPerturbationnablaCK}
\begin{Proof}
    From \eqref{Gradient}, we know that: $\nabla C(K)=2E_K\Sigma_K$, therefore:
\begin{equation*}
    \begin{split}
        |\nabla C(K')-\nabla C(K)|=2E_{K'}\Sigma_{K'}-2E_K\Sigma_K=2(E_{K'}-E_K)\Sigma_{K'}+2E_K(\Sigma_{K'}-\Sigma_K)
    \end{split}
\end{equation*}
For the first term, we aim to investigate the upper bound of $E_K$. 
From \eqref{Upper}, we know that:
\begin{equation*}
\begin{split}
            \mathrm{Tr}\left( E_K^\top  E_K \right) \leq\frac{(C(K)-C(K^*))\lVert R+B^\top P_K B \rVert}{\sigma_1(\Sigma_w)} \\
            \leq \frac{(C(K)-C(K^*))}{\sigma_1(\Sigma_w)}\left(  \lVert R \rVert+ \frac{\lVert B \rVert^2 C(K)}{\lambda_1{(\Sigma_w)}}\right).
\end{split}
\end{equation*}
Then, an upper-bound on the gradient is given as:
\begin{equation}\label{boundedgradienteq}
\begin{split}
         \lVert \nabla C(K) \rVert& \leq \sqrt{4\lVert \Sigma_K \rVert^2 \mathrm{Tr}\left( E_K^\top  E_K \right)}\\
         &\leq\underbrace{\sqrt{4 \left(\frac{C(K)}{\lambda_1(Q)}\right)^2\frac{(C(K)-C(K^*))}{\sigma_1(\Sigma_w)}\left(  \lVert R \rVert+ \frac{\lVert B \rVert^2 C(K)}{\lambda_1{(\Sigma_w)}}\right)}}_{=:b_{\nabla}(C(K))}.
\end{split}
\end{equation}
Under the condition on $K'$: $\lVert K-K' \rVert \leq h(C(K))$, then the second term is bounded by:
\begin{equation}\label{Plambda1}
    \begin{split}
            &~~~~\lVert 2E_K (\Sigma_K-\Sigma_{K'} )\rVert\\
            &\leq \underbrace{2\sqrt{\frac{(C(K)-C(K^*))}{\sigma_1(\Sigma_w)}\left(  \lVert R \rVert+ \frac{\lVert B \rVert^2 C(K)}{\lambda_1{(\Sigma_w)}}\right)}h_{\Sigma}{(C(K))} }_{=:\alpha_1(C(K))}\lVert K-K' \rVert.
    \end{split}
\end{equation}
For the term $E_{K'}-E_K$, we need to find the upper bound of $P_{K'}-P_{K}$: by \eqref{PertubationPK}, for $\lVert K-K' \rVert= \min \{h\left(C(K),K\right), \lVert K \rVert\}$, we have
\begin{equation*}
    \begin{split}
        \lVert P_{K'}-P_K\rVert \leq \underbrace{6 \left( \frac{C(K)}{\lambda_1(\Sigma_w)\lambda_1(Q)}\right)^2\left( 2b^2_{K}(C(K))\lVert R \rVert\lVert B \rVert+b_{K}(C(K))\lVert R\rVert\right)}_{=:\alpha_2(C(K))}\lVert K-K' \rVert.
    \end{split}
\end{equation*}
From Lemma \ref{PerturbationSigmaK}:
\begin{equation*}
    \lVert \Sigma_K-\Sigma_{K'} \rVert \leq \frac{C(K)}{\lambda_1(Q)}.
\end{equation*}
Then we know:
\begin{equation*}
    \lVert \Sigma_{K'} \rVert \leq \lVert \Sigma_K \rVert+\lVert \Sigma_K-\Sigma_{K'} \rVert \leq \lVert \Sigma_K \rVert+\frac{C(K)}{\lambda_1(Q)}.
\end{equation*}
Therefore
\begin{equation*}
    \begin{split}
        E_{K'}-E_K&=R(K'-K)+B^\top (P_{K'}-P_K) A\\
        &~~+B^\top (P_{K'}-P_K) BK'+B^\top P_KB(K'-K).
    \end{split}
\end{equation*}
With assumption $\lVert K'-K \rVert \leq \lVert K \rVert$, we can get $\lVert K' \rVert \leq 2\lVert K \rVert$. Combining all the equations, an upper bound on $2(E_{K'}-E_K)\Sigma_{K'}$ can be derived as:
\begin{equation}\label{Plambda2}
    \begin{split}
        &\lVert 2(E_{K'}-E_K)\Sigma_{K'}\rVert \leq 2\lVert E_{K'}-E_K \rVert\left( \lVert \Sigma_K \rVert+\frac{C(K)}{\lambda_1(Q)}\right)\\
        &\leq 2\left( \lVert \Sigma_K \rVert+\frac{C(K)}{\lambda_1(Q)}\right)\left(  \lVert R \rVert+ \frac{\lVert B \rVert^2 C(K)}{\lambda_1{(\Sigma_0)}}\right)\lVert K-K' \rVert\\
        &~~~+ 2\alpha_2(C(K))\left( \lVert \Sigma_K \rVert+\frac{C(K)}{\lambda_1(Q)}\right)\left(  \lVert B \rVert\lVert A \rVert+ \lVert B \rVert^2\lVert K \rVert \right)\lVert K-K' \rVert\\
        &\leq \alpha_3(C(K))\lVert K-K' \rVert
    \end{split}
\end{equation}
with $$\alpha_3(C(K)):=\left[ \lVert R \rVert+ \frac{\lVert B \rVert^2 C(K)}{\lambda_1{(\Sigma_0)}}+\alpha_2(C(K))\left(  \lVert B \rVert\lVert A \rVert+b_{K}(C(K)) \lVert B \rVert^2 \right)\right].$$
Combing \eqref{Plambda1} and \eqref{Plambda2}, we have:
\begin{equation}\label{ErrorGradient}
    \lVert\nabla C(K')-\nabla C(K)\rVert\leq \underbrace{(\alpha_1(C(K))+\alpha_3(C(K)))}_{=:h_{\nabla}(C(K))} \lVert K-K' \rVert
\end{equation}
when $\lVert K-K' \rVert= \min \{h(C(K)), \lVert K \rVert\}$. 
\end{Proof}
\section{Proof of Results in Section \ref{sec:MBGD}}
\subsection{Proof of Theorem \ref{PGTheorem}}\label{ProofTheorem3}
\begin{Proof}
    %The proof of the Theorem \ref{PGTheorem} is by 
    Using Lemma \ref{AlmostSmoothness} and following the step by \cite[Lemma 21]{pmlr-v80-fazel18a}, if 
    \begin{equation}\label{pG1}
    \begin{split}
        \eta_i \leq \frac{1}{32}\min \left\{ \left( \frac{\lambda_1(Q)\lambda_1(\Sigma_w)}{C(K_i)} \right)^2\frac{1}{\lVert B \rVert\lVert \nabla C(K_i) \rVert(1+\lVert A+BK_i \rVert)},\right.\\
        \left. \frac{\lambda_1(Q)}{2C(K_i)\lVert R+B^\top P_{K_i} B \rVert}\right\},
    \end{split}
    \end{equation}
    then,
\begin{equation}\label{discout}
    C(K_{i+1})-C(K^*) \leq \left( 1-\frac{2\eta_i \lambda_1{(R)} \lambda_1^2(\Sigma_w)}{\lVert \Sigma_K^*\rVert}\right) (C(K_{i})-C(K^*)).
\end{equation}
Based on the upper bounds on $\lVert \nabla C(K)\rVert$, we can derive the following from \eqref{pG1}:
    \begin{equation}\label{pG2}
        \eta_i \leq \underbrace{\frac{1}{32}\min \left\{ \left( \frac{\lambda_1(Q)\lambda_1(\Sigma_w)}{C(K_i)} \right)^2\frac{1}{2\lVert B \rVert b_{\nabla}(C(K_i))}, \frac{\lambda_1(Q)}{2C(K_i)\left(  \lVert R \rVert+ \frac{\lVert B \rVert^2 C(K_i)}{\lambda_1{(\Sigma_w)}}\right)}\right\}}_{=:h_{\mathrm{GD}}(C(K))},
    \end{equation}
    where $b_{\nabla}$ is defined in \eqref{boundedgradienteq}.
From \eqref{discout}, we know that $C(K_{i+1})\leq C(K_i)$. Then we have $h_{\mathrm{GD}}(C(K_{i+1}))\geq h_{\mathrm{GD}}(C(K_{i}))$. The remainder of the proof proceeds by setting $\eta_i=\eta_0, \forall i\in \mathbb{Z}_+$, which represents the lower bound of the convergence rate, and then following the steps outlined in \cite[Lemma 22]{pmlr-v80-fazel18a}.
\end{Proof}
\subsection{Update of Natural Policy Gradient}\label{UpdateNPG}
\begin{Proof}
    Vectorizing the gain $K$ as $\bar{K}$:
\begin{equation*}
    K\in \mathbb{R}^{n_x\times n_u}=\left[\begin{array}{cc}
         k_1  \\
         k_2  \\
         ... \\
         k_{n_x}
    \end{array}\right] \rightarrow [k_1,k_2,...,k_{n_x}]^\top=:\bar{K}\in \mathbb{R}^{n_xn_u}
\end{equation*}
Then the input $u_t=Kx_t$ can be expressed as:
\begin{equation}\label{mt}
    u_t=\underbrace{\left[\begin{array}{cccc}
        x_t^\top & 0 &  ...& 0\\
        0 & x_t^\top & ... &  ...\\
        ... & ... & ...& 0\\
        0,& ... & 0 & x_t^\top
    \end{array}\right]}_{=: X_t\in \mathbb{R}^{n_u\times n_xn_u}}\left[\begin{array}{cc}
         k_1^ \top \\
         k_2 ^\top \\
         ... \\
         k_{n_x}^\top
    \end{array}\right]=X_t\bar{K}=Kx_t
\end{equation}


We parameterize the linear policy with additive Gaussian noise with
\begin{equation*}
    u_t=\pi_{\bar{K}}(x_t,u_t)=\mathcal{N}(X_t K, \alpha^2I)
\end{equation*}
Based on the PDF of Normal distribution:
\begin{equation*}
    \pi_{\bar{K}}(u_t| x_t)=\frac{1}{\sqrt{(2\pi)^{n_xn_u}\alpha^2}} \mathop{exp}\left[ -\frac{1}{2\alpha^2}(u_t-X_t\bar{K})^\top(u_t-X_t\bar{K})\right]
\end{equation*}
The derivative with respect to $\bar{K}$:
\begin{equation*}
\begin{split}
        \mathbb{E} \left[ \nabla \mathop{log} \pi_{\bar{K}}(u_t| x_t)\nabla \mathop{log} \pi_{\bar{K}}(u_t| x_t)^\top  \right]&=\mathbb{E} \left[\frac{X_t^\top (u_t-X_t \bar{K}) (u_t-X_t \bar{K})^\top X_t}{\alpha^4}\right]\\
       &=\left[\begin{array}{cccc}
        \Sigma_K & 0 &  ...& 0\\
        0 & \Sigma_K & ... &  ...\\
        ... & ... & ...& 0\\
        0,& ... & 0 & \Sigma_K
    \end{array}\right]\in \mathbb{R}^{n_xn_u\times n_xn_u}
\end{split}
\end{equation*}
Then $G_{\bar{K}}^{-1}$ are given as:
\begin{equation*}
    G_{\bar{K}}^{-1}=\left[\begin{array}{cccc}
        \Sigma_K^{-1} & 0 &  ...& 0\\
        0 & \Sigma_K^{-1} & ... &  ...\\
        ... & ... & ...& 0\\
        0,& ... & 0 & \Sigma_K^{-1}
    \end{array}\right].
\end{equation*}
With the inverse matrix transformation as \eqref{mt}, we can get: 
\begin{equation*}
    K_{t+1}=K_t-\eta \nabla C(K) \Sigma_K^{-1}
\end{equation*}
\end{Proof}

\section{Proof of Results in Section \ref{sec:MFGD}}\
\subsection{Approximating \texorpdfstring{{\boldmath$C(K),\Sigma_K$}}{Cost and Covariance} with finite horizon}
We aim to show that, for $K \in \mathcal{S}$, it is possible to approximate both $C(K)$ and $\Sigma_K$ with any desired accuracy:
\begin{Lemma}[Approximating $C(K)$ and $\Sigma_K$ with finite horizon] \label{Lemmafinitness}
For any $K \in \mathcal{S}$, let $\Sigma_K^{(l)}:=\frac{1}{l}\sum_{t=0}^{l-1}\mathbb{E}_{x_0,w_t}\left[x_tx_t^{\top} \right]$ and $C^{(l)}(K):=\frac{1}{l}\sum_{t=0}^{l-1}\mathbb{E}_{x_0,w_t}\left[x_t^{\top}Q_Kx_t \right]$. If 
\begin{equation*}
    l \geq \frac{2C(K)}{\epsilon_{\Sigma}}{\left(\frac{\lVert \Sigma_0\rVert}{\lambda_1(Q)\lambda_1(\Sigma_w)}+\frac{C(K)}{\lambda_1(Q)\lambda_1^2(\Sigma_w)}+\frac{1}{\lambda_1(Q)}\right)},
\end{equation*}
then 
\begin{equation*}
    \lVert \Sigma_K^{(l)}-\Sigma_K \rVert \leq \epsilon_{\Sigma}.
\end{equation*}
If \begin{equation}\label{ll1}
    l \geq \frac{2C(K)}{\epsilon_{C}\lambda_1(\Sigma_w)}\left(\frac{C(K)\lVert \Sigma_0\rVert}{\lambda_1(Q)\lambda_1(\Sigma_w)}+\frac{C^2(K)}{\lambda_1(Q)\lambda_1^2(\Sigma_w)}+\frac{C(K)}{\lambda_1(Q)}\right),
\end{equation}
then
\begin{equation*}
    \lVert C^{(l)}(K)-C(K) \rVert \leq \epsilon_{C}.
\end{equation*}
\end{Lemma}
\begin{Proof}
We define $\Tilde{P}_K^w$ as the solution to the following equation:
\begin{equation*}
    \Tilde{P}_K^w=A_K P_K^w A_K^\top +A_K \Tilde{P}_K^w A_K^\top 
\end{equation*}
and the operator $\mathcal{C}_Y(X):=\sum_{t=0}^{+\infty}Y^{t} X Y^{t~\top}$ with $X \succ 0$ and $\lVert Y \rVert\leq 1$ .
From the definition of $\Sigma_K^{(l)}$:
    \begin{equation*}
    \begin{split}
        \Sigma_K^{(l)}&=\frac{1}{l}\left[\Sigma_0+\sum_{t=1}^{l-1} \left[(A_K)^t\Sigma_0(A_K)^{t~\top}+\sum_{k=0}^{t-1} (A_K)^{t-k}\Sigma_w(A_K)^{t-k~\top}\right]\right]\\
                &=\frac{1}{l}\sum_{t=0}^{l-1} \left[(A_K)^t\Sigma_0(A_K)^{t~\top}\right]+\frac{1}{l}\sum_{t=1}^{l-1}\left[\sum_{k=0}^{t-1} (A_K)^{t-k}\Sigma_w(A_K)^{t-k~\top}\right]\\
                &=\Sigma_K+\frac{(\mathcal{C}_{A_K}(\Sigma_0-P_K))-A_K^l(\mathcal{C}_{A_K}(\Sigma_0)-\Tilde{P}_K^w)A_K^{l~\top}-2\Sigma_K}{l}\\
    \end{split}
\end{equation*}
We take the norm for both sides:
\begin{equation*}
    \begin{split}
       \lVert \Sigma_K^{(l)}-\Sigma_K \rVert       &\leq \frac{2\lVert\mathcal{C}_{A_K}(\Sigma_0-P_K)\rVert+2 \lVert \Sigma_K\rVert}{l}\\
       &\leq \frac{2}{l}{\left(\frac{C(K)\lVert \Sigma_0\rVert}{\lambda_1(Q)\lambda_1(\Sigma_w)}+\frac{C^2(K)}{\lambda_1(Q)\lambda_1^2(\Sigma_w)}+\frac{C(K)}{\lambda_1(Q)}\right)}=:\epsilon_{\Sigma}
    \end{split}
\end{equation*}
Then for any desired accuracy $\lVert \Sigma_K^{(l)}-\Sigma_K \rVert \leq \epsilon_{\Sigma}$, we can choose a rollout length $$l \geq \frac{2C(K)}{\epsilon_{\Sigma}}{\left(\frac{\lVert \Sigma_0\rVert}{\lambda_1(Q)\lambda_1(\Sigma_w)}+\frac{C(K)}{\lambda_1(Q)\lambda_1^2(\Sigma_w)}+\frac{1}{\lambda_1(Q)}\right)}$$ to satisfy the accuracy.  
\begin{equation*}
    \begin{split}
        C^{(l)}(K):&=\frac{1}{l}\sum_{t=0}^{l-1}\mathbb{E}\left[x_t^{\top}Q_Kx_t \right]=\frac{1}{l}\sum_{t=0}^{l-1}\mathrm{Tr}\left[Q_K\Sigma_t \right]=\mathrm{Tr}\left[Q_K\frac{1}{l}\sum_{t=0}^{l-1}\Sigma_t \right]\\
        &=\mathrm{Tr}\left[Q_K(\Sigma_K+\frac{(\mathcal{C}_{A_K}(\Sigma_0)-\Tilde{P}_K^w)-A_K^l(\mathcal{C}_{A_K}(\Sigma_0)-\Tilde{P}_K^w)A_K^{l~\top}-2\Sigma_K}{l}) \right]\\
        &=C(K)+\frac{1}{l}\mathrm{Tr}\left[{Q_K[(\mathcal{C}_{A_K}(\Sigma_0)-\Tilde{P}_K^w)-A_K^l(\mathcal{C}_{A_K}(\Sigma_0)-\Tilde{P}_K^w)A_K^{l~\top}-2\Sigma_K]} \right]\\
    \end{split}
\end{equation*}
We take the absolute value for both sides:
\begin{equation*}\label{ckfiniteness}
    \begin{split}
        |C^{(l)}(K)-C(K)|        &\leq\frac{2C(K)}{l\lambda_1(\Sigma_w)}\left(\frac{C(K)\lVert \Sigma_0\rVert}{\lambda_1(Q)\lambda_1(\Sigma_w)}+\frac{C^2(K)}{\lambda_1(Q)\lambda_1^2(\Sigma_w)}+\frac{C(K)}{\lambda_1(Q)}\right)=:\epsilon_{C}\\
    \end{split}
\end{equation*}
Then for any desired accuracy $\lVert C^{(l)}(K)-C(K) \rVert \leq \epsilon_{C}$, we can choose a rollout length:
\begin{equation}\label{ll2}
    l\geq \frac{2C(K)}{\epsilon_{C}\lambda_1(\Sigma_w)}\left(\frac{C(K)\lVert \Sigma_0\rVert}{\lambda_1(Q)\lambda_1(\Sigma_w)}+\frac{C^2(K)}{\lambda_1(Q)\lambda_1^2(\Sigma_w)}+\frac{C(K)}{\lambda_1(Q)}\right).
\end{equation} 
\end{Proof}

\subsection{Smoothing and the gradient descent analysis}\label{AppSmooth}
The algorithm performs gradient descent on the following smoothing function:
\begin{equation}\label{C3eq}
    C_r(K):=\mathbb{E}_{U\sim\mathbb{B}_r}\left[C(K+U)U\right],
\end{equation}
where $\mathbb{B}_r$ is the uniform distribution over all matrices with Frobenius norm less than $r$ (the entire ball)
\begin{Lemma}[Gradient of smoothing function]\label{LemmaSmooth}
\begin{equation}
    \nabla C_r(K)=\frac{n_xn_u}{r^2}\mathbb{E}_{U\sim\mathbb{S}_r}\left[C(K+U)U\right],
\end{equation}
where $\mathbb{S}_r$ is the uniform distribution over all matrices with Frobenius norm $r$ (the sphere)
\end{Lemma}
The proof of Lemma \ref{LemmaSmooth} is given in \cite[Lemma 26]{pmlr-v80-fazel18a}.

\subsection{Estimating \texorpdfstring{{\boldmath$\nabla C(K)$}}{Gradient} with finitely many infinite-horizon Rollouts}\label{MC}
In this section, we aim to bound the difference between $\nabla C(K)-\bar{\nabla} C(K)$, where 
\begin{equation}
    \bar{\nabla} C(K):=\frac{1}{n}\sum^{n}_{k=1}\frac{n_xn_u}{r^2}C(K+U_k)U_k,
\end{equation}
and $U_i$ is uniformly distributed and $\lVert U_i \rVert_F=r$.
\begin{Lemma}\label{Lemma12}
    Given an accuracy $\epsilon$ and probability $\delta$, which can be decomposed by $\epsilon_1$ and $\epsilon_2$ ($\epsilon=\epsilon_1+\epsilon_2$), there exist an exploration radius $r_{\max}$ which is a function of $\epsilon_1$ and the number of rollouts $n$ which is a function of $\epsilon_1,\epsilon_2$ such that when $r \leq r_{\max}$ and $n \geq N$, with high probability (at least $1-\delta$),
    \begin{equation}
        \lVert \bar{\nabla} C(K)-  \nabla C(K)\rVert \leq \epsilon
    \end{equation}
 \end{Lemma}
 \begin{Proof}
     First, we bridge $\bar{\nabla} C(K)$ and $\nabla C(K)$ with $\nabla C_r(K)$,
     \begin{equation*}
    \bar{\nabla} C(K)-\nabla C(K)=(\nabla C_r(K)-\nabla C(K))+(\bar{\nabla} C(K)-\nabla C_r(K))
\end{equation*}
For the first term, let $r_{\max} \leq \min\{h(C(K)), \lVert K \rVert\}$,we have $C(K+U_k)-C(K)\leq h_{C}(C(K)) \lVert U_k \rVert_F$, Then we can get $|C(K+U_k)|\leq C(K)+h_{C}(C(K))\lVert U_k \rVert_F$. 
From Lemma \ref{PerturbationnablaCK}, if 
\begin{equation}\label{radius}
    r_{\max} \leq \min\{h(C(K)), \lVert K \rVert, \frac{\epsilon_1}{h_{\nabla}(C(K))}\},
\end{equation}
then
\begin{equation*}
    \lVert \nabla C(K+U_k)-{\nabla} C(K) \rVert\leq {\epsilon_1}.
\end{equation*}
Since $\nabla C_r(K)$ is the expectation of $\nabla C(K+U_k)$, i.e.
\begin{equation*}
    \nabla C_r(K) =\mathbb{E}_{U\sim\mathbb{S}_r}\left[\nabla C(K+U)\right],
\end{equation*}
by the triangle inequality, we have
\begin{equation} \label{12}
    \lVert \nabla  C_r(K)-\nabla C(K) \rVert\leq {\epsilon_1}.
\end{equation}
For the second term $\lVert \bar{\nabla} C(K)-\nabla C_r(K) \rVert$, we define the sample $Z_k:=\frac{n_xn_u}{r^2}C(K+U_k)U_k$
\begin{equation*}
    \lVert Z_k \rVert\leq \frac{n_xn_u}{r^2}C(K+U_k)\lVert U_k \rVert \leq \frac{n_xn_uC(K+U_k)}{r}\leq \frac{n_xn_u\left(C(K)+rh_{C}(C(K))\right)}{r} 
\end{equation*}
We define $Z:=\nabla  C_r(K)$, together with \eqref{12}, an upper bounded can be derived as:
\begin{equation*}
    \lVert Z\rVert=\lVert \nabla  C_r(K)\rVert\leq \lVert \nabla C(K)\rVert+{\epsilon_1}\leq {\epsilon_1} +b_{\nabla}(C(K)).
\end{equation*}
Then an upper bound on $\lVert Z_k-Z\rVert$ as:
\begin{equation}\label{alpha4}
    \lVert Z_k-Z\rVert \leq \lVert Z_k\rVert+\lVert Z\rVert \leq \underbrace{\frac{n_xn_u\left(C(K)+rh_{C}(C(K))\right)}{r} +\epsilon_1+b_{\nabla}(C(K)) }_{=:\alpha_4(C(K),\epsilon_1)}
\end{equation}
Further, we have:
\begin{equation}\label{alpha5}
    \begin{split}
        &~~~~\lVert \mathbb{E}(Z_kZ_k^\top)-ZZ^\top\rVert=\lVert \mathbb{E}(Z_k^\top Z_k)-Z^\top Z\rVert\\
        &\leq \lVert \mathbb{E}(Z_k^\top Z_k) \rVert_F +\lVert Z^\top Z\rVert_F \leq \max_{Z_k}(\lVert Z_k \rVert_F)^2+\lVert Z\rVert^2_F\\
        &\leq \underbrace{\max\{n_x,n_u\}^2 \left(\frac{n_xn_u\left(C(K)+rh_{C}(C(K))\right)}{r}\right)^2+ \left( \epsilon_1+b_{\nabla}(C(K))\right)^2}_{=:\alpha_5(C(K),\epsilon_1)}
    \end{split}
\end{equation}
We aim to bound $\lVert \bar{\nabla} C(K)-\nabla C_r(K) \rVert\leq \epsilon_2$, so the number of rollouts much be greater and equal than $N$ (Matrix Bernstein Inequality, \cite[Lemma B.5]{9254115})
\begin{equation}\label{Numberofsamples}
    N:=\frac{2\min\{n_x,n_u\}}{\epsilon_2^2} \left( \alpha_4^2(C(K),\epsilon_1) +\frac{\alpha_5(C(K),\epsilon_1)\epsilon_2}{3\sqrt{\min(n_x,n_u)}}\right)\log \left[ \frac{n_x+n_u}{\delta}\right],
\end{equation}
where $\alpha_4$ and $\alpha_5$ are defined in \eqref{alpha4} and \eqref{alpha5}, respectively. 
Then we have $$\mathbb{P}\left[ \lVert \bar{\nabla} C(K)-\nabla C_r(K)  \rVert_F \leq \epsilon_2 \right]\geq 1-\delta.$$
We sum up all the terms and get the following if $r\geq r_{\max}$ and ${n}\geq N$, where $r_{min}$ and $N$ are defined in \eqref{radius} and \eqref{Numberofsamples}, we have
\begin{equation}
    \mathbb{P}\left[ \lVert \bar{\nabla} C(K)-\nabla C(K)  \rVert_F \leq \epsilon \right]\geq 1-\delta.
\end{equation}
 \end{Proof}
\subsection{boundedness of state \texorpdfstring{{\boldmath$\lVert x_t \rVert$}}{}}\label{Proofboundedness}
The noise $w_t$ is a Gaussian distributed vector with zero mean and covariance $\Sigma_w$. Let $\Sigma_w=V\Lambda V^\top$ be the eigenvalue decomposition, where $V$ is a diagonal matrix with diagonal entries $\sigma_1,...,\sigma_{n_x}$. Then we have the vector $W_t=V^{-1}w_t$, with zero mean and variance $\Lambda$.
By the Chebyshev's inequality \cite[Corollary 1.2.5]{Vershynin_2018}, we have 
\begin{equation}\label{Noise}
    \mathbb{P}\left\{\lVert w_t \rVert \leq \bar{w} \right\} =\mathbb{P}\left\{\lVert W_t \rVert \leq \bar{w}\right\} \geq \left\{1-\frac{n_x\lambda_1(\Sigma_w)}{\bar{w}^{2}}\right\}^{n_x}=:\delta_w({\bar{w}})
\end{equation}
Based on \eqref{Noise}, we can derive the following Lemma:
\begin{Lemma}[Bound of state $\lVert x_t \rVert$]\label{boundofstate}
    For any initial state $\lVert x_0 \rVert \leq L_0$ and $K\in \mathcal{S}$, given a desired probability $\delta$, the state response $\lVert x_t\rVert$ expressed as \eqref{stateresponce} is bounded by $\bar{L}:=L_0+t\left(\frac{\lambda_1(\Sigma_w)^{n_x}}{1-\delta^{\frac{1}{t}}}\right)^{\frac{1}{2n_x}}$ with probability at least $\delta$.
\end{Lemma}
\begin{Proof}
    From \eqref{stateresponce}, we can get:
    \begin{equation*}
      \begin{split}
            \lVert x_t \rVert &\leq \lVert  (A_K)^t x_0^{(i)}\rVert +\left\lVert\sum_{k=1}^{t} (A_K)^{t-k}w_{k-1}\right\rVert\\
            &\leq \lVert  x_0\rVert +\sum_{k=1}^{t}\lVert  (A_K)^{t-k}\rVert \left\lVert w_{k-1}\right\rVert \leq L_0 +\sum_{k=0}^{t-1}\left\lVert w_k\right\rVert
      \end{split}
    \end{equation*}
    If for all $t \in [0,t-1]$,$\lVert w_t \rVert\leq \bar{w}$, then $\lVert x_t \rVert \leq L_0+t\bar{w}=\bar{L}$. Then the probability of $\lVert x_t \rVert \leq \bar{L}(\bar{w})$ is at least $\delta:=\delta_w(\bar{w})^t$. Together with \eqref{Noise}, we can then derive $\bar{w}=\sqrt{\frac{n_x\lambda_1(\Sigma_w)}{1-\delta^{\frac{1}{tn_x}}}} $. We conclude the proof. 
\end{Proof}
\subsection{Proof of Theorem \ref{Theorem4}}\label{ProofTheorem4}
We aim to bound the difference between the estimates from the Algorithm \ref{Algo1}, expressed by
\begin{equation*}
    \hat{\nabla}C(K)=\frac{1}{n}\sum^{n}_{k=1}\frac{n_xn_u}{r^2}\hat{C}_kU_k
\end{equation*}
and the true gradient $\nabla C(K)$. 
\begin{Proof}
We bridge this two terms with $\nabla' C(K)$ and $\bar{\nabla}C(K))$, where $\bar{\nabla}C(K))$ is defined in Lemma \ref{Lemma12} and $\nabla' C(K)$ is defined as:
\begin{equation*}
    \nabla' C(K):=\frac{1}{n}\sum^{n}_{k=1}\frac{n_xn_u}{r^2}C^{(l)}(K+U_k)U_k.
\end{equation*}
The relation between the estimates and the true gradient is given as:
\begin{equation}
    \hat{\nabla}C(K)-\nabla C(K)=(\hat{\nabla}C(K)-\nabla' C(K))+(\nabla' C(K)-\bar{\nabla} C(K))+(\bar{\nabla} C(K)-\nabla C(K))
\end{equation}
The third term is analyzed in Lemma \ref{Lemma12}. We choose $\epsilon_{nr}:=\epsilon_{n}+\epsilon_{r}$ and $\delta_{n}$, then we can find the exploration radius $r_{\max}$ and $N$ as:
\begin{subequations}
    \begin{align}
        r_{\max} &= \min\{h(C(K)), \lVert K \rVert, \frac{\epsilon_r}{h_{\nabla}(C(K))}\}\\
            N&=\frac{2\min\{n_x,n_u\}}{\epsilon_n^2} \left( \alpha_4^2(C(K),\epsilon_r) +\frac{\alpha_5(C(K),\epsilon_r)\epsilon_n}{3\sqrt{\min(n_x,n_u)}}\right)\log \left[ \frac{n_x+n_u}{\delta_n}\right]
    \end{align}
\end{subequations}
Then we have 
\begin{equation}\label{teil1}
     \mathbb{P}\left[ \lVert \bar{\nabla} C(K)-\nabla C(K)  \rVert_F \leq \epsilon_{nr} \right]\geq 1-\delta_n.
\end{equation}
For the second term, as stated in Lemma \ref{Lemmafinitness}, for any desired accuracy $\epsilon_l$, we can choose $l$ such that:
\begin{equation}
    l \geq \frac{2(C(K)+rh_{C}C(K))^2}{\epsilon_l\lambda_1(\Sigma_w)}\left(\frac{\lVert \Sigma_0\rVert}{\lambda_1(Q)\lambda_1(\Sigma_w)}+\frac{(C(K)+rh_{C}C(K))}{\lambda_1(Q)\lambda_1^2(\Sigma_w)}+\frac{1}{\lambda_1(Q)}\right),
\end{equation}
then,
\begin{equation}
    |C^{(l)}(\hat{K}_k)-C(\hat{K}_k)|\leq \frac{r\epsilon_l}{n_xn_u},\quad \forall k.
\end{equation}
By the triangle inequality and $\lVert U_k \rVert_F = r$, we can get:
\begin{equation} \label{teil2}
    \lVert \nabla' C(K)-\bar{\nabla} C(K) \rVert \leq \left \lVert \frac{1}{n} \sum_{k=1}^{n} \frac{n_xn_u }{r^2}\left[ C^{(l)}(\hat{K}_k)-C(\hat{K}_k)\right]U_k \right \rVert =  \epsilon_l
\end{equation}
For the first term, from Lemma \ref{boundofstate}, we know that for any selected accuracy $\delta_x$, we have 
\begin{equation}
    \lVert x_t \rVert \leq \bar{L}=L_0+t\bar{w}=L_0+t\sqrt{\frac{n_x\lambda_1(\Sigma_w)}{1-\delta_x^{\frac{1}{tn_x}}}} 
\end{equation}
with probability at least $\delta_x$. Given the probability $\delta_x$, the upper bound of the sample $\hat{C}_k$ is given by:
\begin{equation}\label{VRtest1}
    \begin{split}
        &~~~~ \frac{1}{l}\sum_{t=0}^{l-1} x_t^{(k)~\top}(Q+\hat{K}^{\top}_kR\hat{K}_k)x_t^{(k)} \\
        &\leq \frac{1}{l}\sum_{t=0}^{l-1} \lVert Q_{\hat{K}_k}\rVert \lVert      x_t^{(k)} \rVert^2 \leq (\lVert Q_{\hat{K}_k}\rVert \max_{t \in [0,l-1]}\lVert      x_t^{(k)} \rVert^2 \leq \frac{C(\hat{K}_k)}{\lambda_1(\Sigma_w)} \left(L_0+(l-1)L_w\right)^2 \\
        &\leq \frac{C(K)+rh_{C}(C(K))}{\lambda_1(\Sigma_w)}\left(L_0+(l-1)\sqrt{\frac{n_x\lambda_1(\Sigma_w)}{1-\delta_x^{\frac{1}{ln_x}}}} \right)^2=:\bar{C}
    \end{split}
\end{equation}
Each sample $Z_k:=\frac{n_xn_u}{r^2}\hat{C}_kU_k$ has bounded norm as:
\begin{equation}
    \lVert Z_k \rVert \leq \frac{n_xn_u}{r} \bar{C}=:\alpha_{6}(C(K),\epsilon_x).
\end{equation}
Then we have for $Z:= \mathbb{E} [\hat{\nabla} C(K)]=\nabla C'(K)$, with \eqref{teil1} and \eqref{teil2},
\begin{equation}
    \lVert Z \rVert  \leq \epsilon_l+\lVert \bar{\nabla}C(K) \rVert \leq \epsilon_l+\epsilon_{nr}+\lVert \nabla{C}(K) \rVert \leq \epsilon_l+\epsilon_{nr}+b_{\nabla}(C(K))
\end{equation}
Then we can derive the bound on $\lVert Z_k-Z \rVert$:
\begin{equation}\label{alpha_7}
    \lVert Z_k-Z \rVert\leq \lVert Z \rVert+\lVert Z_k \rVert \leq \underbrace{\epsilon_l+\epsilon_{nr}+b_{\nabla}(C(K))+\alpha_{6}(C(K),\delta_x)}_{=:\alpha_{7}(C(K),\delta_x)}
\end{equation}
Similarly, the bound on $\lVert \mathbb{E}(Z_kZ_k^\top)-ZZ^\top\rVert$ can be derived as:
\begin{equation}\label{alpha_8}
    \begin{split}
        \lVert \mathbb{E}(Z_kZ_k^\top)-ZZ^\top\rVert&\leq \lVert \mathbb{E}(Z_k^\top Z_k) \rVert_F +\lVert Z^\top Z\rVert_F\leq \max_{Z_k}(\lVert Z_k \rVert_F)^2+\lVert Z\rVert^2_F\\
        &\leq \underbrace{\max\{n_x,n_u\}^2 \alpha^2_{6}(C(K),\epsilon_x)+ \left( \epsilon_l+\epsilon_{nr}+b_{\nabla}(C(K))\right)^2}_{=:\alpha_{8}(C(K),\delta_x)}
    \end{split}
\end{equation}
Given an accuracy $\epsilon_d$ and $\delta_d$, we can use the matrix concentration inequality \cite[Lemma B2.5]{9254115},
\begin{equation}
    N=\frac{2\min\{n_x,n_u\}}{\epsilon_d^2} \left( \alpha_{7}^2(C(K),\delta_x) +\frac{\alpha_{8}(C(K),\delta_x)\epsilon_d}{3\sqrt{\min(n_x,n_u)}}\right)\log \left[ \frac{n_x+n_u}{\delta_d}\right]
\end{equation}
Then we have 
\begin{equation}
    \mathbb{P}\left[ \lVert \hat{\nabla}C(K)-\nabla' C(K) \rVert_F \leq \epsilon_d \right]\geq 1-\delta_d.
\end{equation}

Now, we can summarize all the terms. Given an arbitrary tolerance $\epsilon$, which can be decomposed by $\epsilon_d,\epsilon_l,\epsilon_n,\epsilon_r$, with $\epsilon=\epsilon_d+\epsilon_l+\epsilon_n+\epsilon_r$, and an arbitrary probability $\delta$, which can be decomposed by $\delta_x,\delta_d,\delta_n$ with $\delta=(1-\delta_d)(1-\delta_n)\delta_x$, suppose the exploration radius is chosen as $r\leq r_{\max}$ with 
\begin{equation}\label{rexpresion}
    r_{\max} = \min\{h(C(K)), \lVert K \rVert, \frac{\epsilon_r}{h_{\nabla}(C(K))}\},
\end{equation}
where $h_\nabla$ is defined in \eqref{ErrorGradient},
the rollout length is chosen as: $l\geq l_{\min}$ 
\begin{equation}\label{lexpression}
        l_{\min} = \frac{2(C(K)+rh_{C}C(K))^2}{\epsilon_l\lambda_1(\Sigma_w)}\left(\frac{\lVert \Sigma_0\rVert\lambda_1(\Sigma_w)+C(K)+rh_{C}C(K)}{\lambda_1(Q)\lambda_1^2(\Sigma_w)}+\frac{1}{\lambda_1(Q)}\right),
\end{equation}

and the number of rollouts is chosen as: $n\geq N$, where $N:=\max \{N_1,N_2\}$ with
\begin{subequations}
\begin{align}
    N_1&:=\frac{2\min\{n_x,n_u\}}{\epsilon_n^2} \left( \alpha_4^2(C(K),\epsilon_r) +\frac{\alpha_5(C(K),\epsilon_r)\epsilon_n}{3\sqrt{\min(n_x,n_u)}}\right)\log \left[ \frac{n_x+n_u}{\delta_n}\right]\label{Nexpression1},\\
        N_2&:=\frac{2\min\{n_x,n_u\}}{\epsilon_d^2} \left( \alpha_{7}^2(C(K),\delta_x) +\frac{\alpha_{8}(C(K),\delta_x)\epsilon_d}{3\sqrt{\min(n_x,n_u)}}\right)\log \left[ \frac{n_x+n_u}{\delta_d}\right]\label{Nexpression2},
\end{align}
\end{subequations}
where $\alpha_4,\alpha_5,\alpha_7$ and $\alpha_8$ are defined in \eqref{alpha4}, \eqref{alpha5}, \eqref{alpha_7} and \eqref{alpha_8} respectively. 
Then, we have 
\begin{equation*}
    \mathbb{P}\left\{\lVert \hat{\nabla}C(K)- {\nabla}C(K)\rVert\leq \epsilon \right\}\geq \delta.
\end{equation*}
\end{Proof}




\subsection{Proof of Theorem \ref{Theorem5}}\label{ProofTheorem5}
\begin{Proof}
    First, we also bridge the estimates $\hat{\Sigma}_K$ and $\Sigma_K$ with the following equation:
    \begin{equation}
    \hat{\Sigma}_K-\Sigma_K=(\hat{\Sigma}_K-\tilde{\Sigma}^{(l)}_K)+(\tilde{\Sigma}^{(l)}_K-\tilde{\Sigma}_K)+(\tilde{\Sigma}_K-\Sigma_K),
\end{equation}
with $\tilde{\Sigma}^{(l)}_K:=\frac{1}{n}\sum_{i=1}^n \Sigma^{(l)}_{K+U_i}$ and $\tilde{\Sigma}_K:=\frac{1}{n}\sum_{i=1}^n \Sigma_{K+U_i}$.\\
For the last term, we aim to bound $ \lVert  \tilde{\Sigma}_K-\Sigma_K \rVert$. From Lemma \ref{PerturbationSigmaK}, if we choose $r\leq r'_{\max}$ as $r'_{\max}:=\frac{\epsilon'_r}{b_{\nabla}(C(K))}$, where $b_{\nabla}$ is defined in \eqref{boundedgradienteq}, then $ \lVert  \tilde{\Sigma}_K-\Sigma_K \rVert \leq {\epsilon'_r}$. \\
For the second term, from Lemma \ref{Lemmafinitness}, if we choose $l\geq l'_{\min}$ with $$l'_{\min}:=\frac{2C(K)}{\epsilon_{C}\lambda_1(\Sigma_w)}\left(\frac{C(K)\lVert \Sigma_0\rVert}{\lambda_1(Q)\lambda_1(\Sigma_w)}+\frac{C^2(K)}{\lambda_1(Q)\lambda_1^2(\Sigma_w)}+\frac{C(K)}{\lambda_1(Q)}\right),$$ then $ \lVert \tilde{\Sigma}^{(l)}_K-\tilde{\Sigma}_K  \rVert \leq \epsilon'_l$.\\
Let us turn to the first term, from Lemma \ref{boundofstate}, 
\begin{equation*}
    \lVert \underbrace{\frac{1}{l}\sum_{t=0}^{l-1} x_t^{(i)}x_t^{(i)~\top}}_{=:\tilde{Z}_k} \rVert \leq  \max_{i\in [0,l-1]} \lVert x_t^{(i)} \rVert^2
\end{equation*}
For any desired probability $\delta'_x$, $\lVert x_t^{(k)} \rVert$ is upper-bounded by $\bar{L}'$ which is defined as $\bar{L}':=L_0+(l-1)\sqrt{\frac{n_x\lambda_1(\Sigma_w)}{1-(\delta_x')^{\frac{1}{ln_x}}}}$(see Lemma \ref{boundofstate}). Because of the definition of $\tilde{\Sigma}^{(l)}_K$, we have:
\begin{equation*} 
    \lVert \tilde{\Sigma}^{(l)}_K \rVert \leq\frac{1}{n}  \sum_{k=1}^n\frac{C(K+U_k)}{\lambda_1(Q)}
\end{equation*}
For  $r \leq \min\{h(C(K)), \lVert K \rVert\}$, we have $C(K+U_i)\leq C(K)+rh_{C}(C(K))$, then we can get:
\begin{equation*}
    \lVert \tilde{\Sigma}^{(l)}_K \rVert \leq \frac{1}{n} \sum_{k=1}^n\frac{C(K+U_k)}{\lambda_1(Q)} \leq \frac{C(K)+rh_{C}(C(K))}{\lambda_1(Q)}
\end{equation*}
Now an upper bound on $\lVert \tilde{Z}_k-\tilde{Z} \rVert$ is given as:
\begin{equation}\label{alpha_9}
    \lVert \tilde{Z}_k-\tilde{Z} \rVert \leq \lVert \tilde{Z}_k \rVert +\lVert \tilde{Z} \rVert \leq \frac{C(K)+rh_{C}(C(K))}{\lambda_1(Q)}+(\bar{L}'(\delta'_x))^2=: \alpha_{9}(C(K),\delta'_x)
\end{equation}
Then an upper bound on $\lVert \mathbb{E}(\tilde{Z}_k\tilde{Z}_k^\top)-\tilde{Z}\tilde{Z}^\top\rVert$ is given as:
\begin{equation}\label{alpha_10}
    \begin{split}
        \lVert \mathbb{E}(\tilde{Z}_k\tilde{Z}_k^\top)-\tilde{Z}\tilde{Z}^\top\rVert&\leq \lVert \mathbb{E}(\tilde{Z}_k^\top \tilde{Z}_k) \rVert_F +\lVert \tilde{Z}^\top \tilde{Z}\rVert_F\\
        &\leq \max_{\tilde{Z}_k}(\lVert \tilde{Z}_k \rVert_F)^2+\lVert \tilde{Z}\rVert^2_F\\
        &\leq \underbrace{n_x^2\left[ (\bar{L}'(\delta'_x))^2+\left(\frac{C(K)+rh_{C}(C(K))}{\lambda_1(Q)} \right)^2 \right]}_{=: \alpha_{10}(C(K),\delta'_x)}
    \end{split}
\end{equation}
With Matrix Bernstein Inequality \cite[Lemma B2.5]{9254115}, given a small tolerance ${\epsilon'_n}$ and small probability $\delta'_n$, if the number of rollouts $n\geq n_{\min}'$ with
\begin{equation*}
    n_{\min}':= \frac{2n_x}{(\epsilon'_n)^2} \left( \alpha_{10}(C(K),\delta'_x)+\frac{\alpha_{9}(C(K),\delta'_x)\epsilon'_n}{3 \sqrt{n_x}}\right) \log \left[ \frac{2n_x}{\delta'_n}\right]
\end{equation*}
where $\alpha_9$ and $\alpha_{10}$ are defined in \eqref{alpha_9} and \eqref{alpha_10}.
Then we have $$\mathbb{P}\left[ \lVert \hat{\Sigma}_K-\tilde{\Sigma}^{(l)}_K  \rVert \leq \epsilon'_n \right]\geq 1-\delta'_n$$
We sum all terms, we can derive that given an arbitrary tolerance $\epsilon'$, which can be decomposed by $\epsilon'_n,\epsilon'_l,\epsilon'_r$, with $\epsilon=\epsilon'_n+\epsilon'_l+\epsilon'_r$, and an arbitrary probability $\delta'$, which can be decomposed by $\delta'_x,\delta'_n$ with $\delta=(1-\delta_n')\delta_x$, suppose the exploration radius is chosen as $r\leq r'_{\max}$ with 
\begin{equation}\label{radius2}
    r'_{\max}:=\min\left\{h(C(K)), \lVert K \rVert , \frac{\epsilon'_r}{b_{\nabla}(C(K))}\right\}
\end{equation}
the rollout length is chosen as $l\geq l'_{\min}$ with
\begin{equation}\label{l2}
    l'_{\min}:=\frac{2C(K)}{\epsilon'_l\lambda_1(\Sigma_w)}\left(\frac{C(K)\lVert \Sigma_0\rVert}{\lambda_1(Q)\lambda_1(\Sigma_w)}+\frac{C^2(K)}{\lambda_1(Q)\lambda_1^2(\Sigma_w)}+\frac{C(K)}{\lambda_1(Q)}\right)
\end{equation}
and the number of rollouts is chosen as $n\geq n_{\min}'$ with
\begin{equation}\label{n2}
    n_{\min}':= \frac{2n_x}{(\epsilon'_n)^2} \left( \alpha_{10}(C(K),\delta'_x)+\frac{\alpha_{9}(C(K),\delta'_x)\epsilon'_n}{3 \sqrt{n_x}}\right) \log \left[ \frac{2n_x}{\delta'_n}\right]
\end{equation}
then we have,
\begin{equation*}
    \mathbb{P}\left\{\lVert \hat{\Sigma}_K- \Sigma_K \rVert\leq \epsilon' \right\}\geq \delta'.
\end{equation*}
\end{Proof}

\subsection{Proof of Theorem \ref{TheoremVariancereduction}}\label{ProofVariance}
\begin{Proof}
    In this subsection, we aim to prove that the introduction of the baseline function reduces the sample complexity.
    From the proof of Theorem \ref{Theorem4}, the required sample $n_{\min}$ is determined by the upper bound of the sample $\lVert Z_i \rVert$. Using \eqref{VRtest1}, we know that this upper bound is probabilistically bounded as follows:
    \begin{equation}\label{VR30}
    \begin{split}
                \lVert \hat{C}_k-\hat{b}_s(x_0)\rVert&=\lVert \hat{C}_k-b_s(x_0)+b_s(x_0)-\hat{b}_s(x_0)\rVert\\
                &\leq \lVert \hat{C}_k-b_s(x_0)\rVert+\lVert b_s(x_0)-\hat{b}_s(x_0)\rVert
    \end{split}
    \end{equation}
    Since $\hat{b}_s(x_0)> 0$, the term $\lVert \hat{C}_k-b_s(x_0)\rVert$ is upper bounded by $\bar{C}-{b}_s^*(x_0)$, if ${b}_s^*(x_0)\leq \hat{C}_k$, and by ${b}_s^*(x_0)$, if ${b}_s^*(x_0)> \hat{C}_k$. Therefore, we have:
    \begin{equation}
    \begin{split}
                \lVert \hat{C}_k-b_s(x_0)\rVert \leq \max\{\hat{C}_k-{b}_s^*(x_0),{b}_s^*(x_0)\} \leq \max\{\bar{C}-{b}_s^*(x_0),{b}_s^*(x_0)\}
    \end{split}
    \end{equation}
    From the definition of $b_s(x_0)$, we know:
    \begin{equation}
        b_s(x_0)\leq \frac{C(K)}{\lambda_1(\Sigma_w)}(\lVert x_0 \rVert+(l-1)\lVert \Sigma_w\rVert)^2.
    \end{equation}
    If 
    \begin{equation}\label{deltax}
        \delta_x\geq \underline{\delta}_x:=\left(1-\frac{n_x\lambda_1(\Sigma_w)}{\lVert \Sigma_w \rVert^{2}}\right)^{ln_x},
    \end{equation} then $b_s(x_0) \leq \bar{C}$, and further $\lVert \hat{C}_k-b_s(x_0)\rVert \leq \bar{C}$.  
    From \eqref{VR30}, if
    \begin{equation}
        \lVert b_s(x_0)-\hat{b}_s(x_0)\rVert \leq \min\{b_s(x_0), \bar{C}-b_s(x_0)\}=:\epsilon_v,
    \end{equation}
    then $\lVert \hat{C}_k-b_s(x_0)\rVert \leq \bar{C}$. Therefore, the required sample for variance reduction is at least not larger than the case without variance reduction.
Following the proof of Theorem \ref{Theorem4}, we can obtain the expression of $N_3$:
\begin{equation}\label{n3expression}
    N_3:=\frac{2\min\{n_x,n_u\}}{\epsilon_d^2} \left( \alpha_{7}^2(C(K),\delta_x) +\frac{\alpha_{11}(C(K),\delta_x,\hat{b}_s(x_0))\epsilon_d}{3\sqrt{\min(n_x,n_u)}}\right)\log \left[ \frac{n_x+n_u}{\delta_d}\right],
\end{equation}
where $$\alpha_{11}(C(K),\delta_x,\hat{b}_s(x_0)):=\max\{n_x,n_u\}^2 \alpha^2_{12}(\hat{b}_s(x_0))+ \left( \epsilon_l+\epsilon_{nr}+b_{\nabla}(C(K))\right)^2.$$
and $\alpha_{12}(\hat{b}_s(x_0)):=\frac{n_xn_u}{r}\{ \max\{\hat{C}_k-{b}_s^*(x_0),{b}_s^*(x_0)\}+ \lVert b_s(x_0)-\hat{b}_s(x_0)\rVert\}$.

  From Algorithm \ref{AlgoV} and Lemma \ref{boundofstate}, similar to the proof of Theorem \ref{Theorem4}, we have:
    \begin{equation}
        \hat{C}_k^V \leq \frac{C(K)}{\lambda_1(\Sigma_w)}\left(L_0+(l-1)\sqrt{\frac{n_x\lambda_1(\Sigma_w)}{1-(\tilde{\delta}_x)^{\frac{1}{tn_x}}}} \right)^2=: \bar{C}^V(\tilde{\delta}_x)
    \end{equation}
    with predefined probability $\delta_x^V$. The expectation of $\hat{C}_k^V$ is upper bounded as:
        \begin{equation}
        \mathbb{E}[\hat{C}_k^V]\leq \frac{C(K)}{\lambda_1(\Sigma_w)}\left(L_0+(l-1)\lVert \Sigma_w\rVert\right)^2=: \bar{C}_E^V.
    \end{equation}
    %if $\lVert x_0 \rVert < \frac{L_0}{2}$ and $\lVert \Sigma_w \rVert \leq \frac{}{}$
The bound on $\lVert C^V_k-\mathbb{E}[\hat{C}_k^V] \rVert$ is given by:
\begin{equation}
    \lVert C^V_k-\mathbb{E}[\hat{C}_k^V] \rVert\leq \bar{C}_E^V+\bar{C}^V(\tilde{\delta}_x)
\end{equation}
Similarly, the bound on $\lVert \mathbb{E}(C^V_kC^{V~\top}_k)-{C}_k^V{C}_k^{V~\top}\rVert$ is given by:
\begin{equation}
    \begin{split}
        &\lVert \mathbb{E}(C^V_kC^{V~\top}_k)-{C}_k^V{C}_k^{V~\top}\rVert\leq \max_{C^V_k}(\lVert C^V_k \rVert_F)^2+\lVert \mathbb{E}[C^V_k]\rVert^2_F\leq (\bar{C}_E^V)^2+(\bar{C}^V(\tilde{\delta}_x))^2.
    \end{split}
\end{equation}
Using the matrix concentration inequality \cite[Lemma B2.5]{9254115}, we get:
\begin{equation}\label{tilden}
    \tilde{n}_{\min}=\frac{2}{(\epsilon'_v)^2} \left( (\bar{C}_E^V+\bar{C}^V)^2 +\frac{((\bar{C}_E^V)^2+(\bar{C}^V(\tilde{\delta}_x))^2)\epsilon_v}{3}\right)\log \left[ \frac{2}{(\tilde{\delta}_v)}\right].
\end{equation}
Thus, if the number of rollouts used to estimate the baseline function satisfies $n_v \geq \tilde{n}_{\min}(\tilde{\delta}_x,\delta_v)$, the sample size required to estimate the gradient with the baseline function is at least not larger than the case without the baseline function, with probability $\delta_v=(1-\tilde{\delta}_v)\tilde{\delta}_x$.
\end{Proof}

\subsection{Proof of Theorem \ref{MFPGV}}\label{ProofMFPG}
\begin{Proof}
    Let $K_{i+1}:=\hat{K}_i-\eta_i \nabla C(\hat{K}_i)$, where $\nabla C(\hat{K}_i)$ is the exact gradient. Our goal is to have
    \begin{equation*}
    |C(\hat{K}_{i+1})-C(K_{i+1})|\leq \frac{\eta_i \sigma\lambda_1{(R)} \lambda_1^2(\Sigma_w)}{2\lVert \Sigma_K^*\rVert} \epsilon
\end{equation*}
Then when $C(\hat{K}_i)-C(K^*) \geq \epsilon$ and with \eqref{GD}, we have
\begin{equation*}
    C(\hat{K}_{i+1})-C(K^*) \leq \left( 1-(1-\sigma)\frac{\eta_i \lambda_1{(R)} \lambda_1^2(\Sigma_w)}{2\lVert \Sigma_K^*\rVert}\right) (C(\hat{K}_{i})-C(K^*))
\end{equation*}
Then convergence is guaranteed. Observing that: $\hat{K}_{i+1}-K_{i+1}=\eta_i (\hat{\nabla}C(\hat{K}_i)-\nabla C(K_i) )$, together with Theorem \ref{PGTheorem}:
\begin{equation*}
\begin{split}
        |C(\hat{K}_{i+1})-C(K_{i+1})|&\leq h_{\nabla}(C(\hat{K}_i)) \lVert \hat{K}_{i+1}-K_{i+1} \rVert\\
        &=\eta_i h_{\nabla}(C(\hat{K}_i)) \lVert \hat{\nabla}C(\hat{K}_i)-\nabla C(K_i)  \rVert.
\end{split}
\end{equation*}
Then we can derive the requirement on the estimation accuracy $\epsilon_\mathrm{GD}$ on $\hat{\nabla} C(\hat{K}_i)$
\begin{equation*}
    \epsilon_\mathrm{GD}:=\frac{\epsilon\sigma\lambda_1{(R)} \lambda_1^2(\Sigma_w)}{2h_{\nabla}(C(\hat{K}_i))\lVert \Sigma_K^*\rVert}. 
\end{equation*}
Then we should choose the $l,n,r$ as discussed in Theorem \ref{Theorem4} to satisfy the accuracy and probability $\delta$ above. The gradient descent algorithm can reach the desired accuracy $\epsilon$ with probability $\delta$.
The remainder of the proof follows Theorem \ref{PGTheorem}, which assumes that the gradient estimate meets the required accuracy at each iteration, guaranteed by the union bound. 
\end{Proof}



\subsection{Proof of Theorem \ref{MFNPGV}}\label{ProofMFNPG}
\begin{Proof}
Let ${K}_{i+1}:=\hat{K}_i-\eta_i {\nabla} C(\hat{K}_i){\Sigma}_{\hat{K}_i}^{-1}$. From the model-based natural policy gradient in Theorem \ref{NPGTheorem}:   
\begin{equation*}
    C(K_{i+1})-C(K^*) \leq \left( 1-\frac{\eta_i \lambda_1{(R)} \lambda_1(\Sigma_w)}{\lVert \Sigma_K^*\rVert}\right) (C(\hat{K}_{i})-C(K^*))
\end{equation*}
we aim to bound $\lVert C(\hat{K}_{i+1})-C({K}_{i+1})\rVert \leq \frac{\epsilon\sigma\eta_i \lambda_1{(R)} \lambda_1(\Sigma_w)}{2\lVert \Sigma_K^*\rVert}$, then when $C(K)-C(K^*)\geq \epsilon$, we have 
\begin{equation*}
    C(\hat{K}_{i+1})-C(K^*) \leq \left( 1-(1-\sigma)\frac{\eta_i \lambda_1{(R)} \lambda_1(\Sigma_w)}{2\lVert \Sigma_K^*\rVert}\right) (C(\hat{K}_{i})-C(K^*))
\end{equation*}
We aim to have the following, for any $\hat{K}_i$:
\begin{equation*}
    \lVert \hat{\nabla} C(\hat{K}_i)\hat{\Sigma}_{\hat{K}_i}^{-1} -{\nabla} C(\hat{K}_i){\Sigma}_{\hat{K}_i}^{-1} \rVert \leq \frac{\epsilon\sigma \lambda_1{(R)} \lambda_1(\Sigma_w)}{2h_{\nabla}(C(\hat{K}_i))\lVert \Sigma_K^*\rVert}.
\end{equation*}
This is broken into two terms:
\begin{equation*}
    \lVert \hat{\nabla} C(\hat{K}_i)\hat{\Sigma}_{\hat{K}_i}^{-1} -{\nabla} C(\hat{K}_i){\Sigma}_{\hat{K}_i}^{-1}\rVert   \leq \lVert \hat{\nabla} C(\hat{K}_i) -{\nabla} C(\hat{K}_i)\rVert\lVert\hat{\Sigma}_{\hat{K}_i}^{-1} \rVert +\lVert {\nabla} C(\hat{K}_i)\rVert \lVert\hat{\Sigma}_{\hat{K}_i}^{-1} -{\Sigma}_{\hat{K}_i}^{-1} \rVert 
\end{equation*}
From Weyl's Theorem \cite[Theorem 31]{pmlr-v80-fazel18a},  if $\epsilon \leq \frac{\lambda_1(\Sigma_w)}{2}$, then it holds that $\lambda_1(\hat{\Sigma}_{\hat{K}_i}) \leq \lambda_1({\Sigma}_w)-\lVert \hat{\Sigma}_{\hat{K}_i}-\Sigma_{\hat{K}_i} \rVert=\frac{\lambda_1({\Sigma}_w)}{2}$. Because $\lVert\hat{\Sigma}_{\hat{K}_i}^{-1} \rVert \leq \frac{2}{\lambda_1(\Sigma_w)}$, we aim to make sure that 
\begin{equation*}
    \lVert \hat{\nabla} C(\hat{K}_i) -{\nabla} C(\hat{K}_i)\rVert \leq \frac{\epsilon\sigma \lambda_1{(R)} \lambda_1^2(\Sigma_w)}{8h_{\nabla}(C(\hat{K}_i))\lVert \Sigma_K^*\rVert}=:\epsilon_{NPG}^C,
\end{equation*}so that $\lVert \hat{\nabla} C(\hat{K}_i) -{\nabla} C(\hat{K}_i)\rVert\lVert\hat{\Sigma}_{\hat{K}_i}^{-1} \rVert \leq \frac{\epsilon\sigma \lambda_1{(R)} \lambda_1(\Sigma_w)}{4h_{\nabla}(C(\hat{K}_i))\lVert \Sigma_K^*\rVert}$. Then we should choose the $l_1,n_1,r_1$ in Algorithm \ref{Algo1} according to Theorem \ref{Theorem4} to satisfy the accuracy on $\hat{\nabla} C(\hat{K}_i)$ 
and the probability $\delta^{\frac{1}{2}}$ above.\\
For the second term, $\lVert\hat{\Sigma}_{\hat{K}_i}^{-1} -{\Sigma}_{\hat{K}_i}^{-1} \rVert$ must be bounded by: 
\begin{equation*}
    \lVert\hat{\Sigma}_{\hat{K}_i}^{-1} -{\Sigma}_{\hat{K}_i}^{-1} \rVert \leq \frac{\epsilon\sigma \lambda_1{(R)} \lambda_1(\Sigma_w)}{4h_{\nabla}(C(\hat{K}_i))\lVert \Sigma_{K^*}\rVert\lVert {\nabla} C(\hat{K}_i)\rVert}.
\end{equation*}
where $h_\nabla$ is defined in \eqref{ErrorGradient}.
By matrix perturbation, if $\lambda_1(\Sigma_{\hat{K}_i})\geq \lambda_1(\Sigma_w)$ and $\lVert \hat{\Sigma}_{\hat{K}_i} -{\Sigma}_{\hat{K}_i} \rVert \leq \frac{\lambda_1(\Sigma_w)}{2}$, then $\lVert\hat{\Sigma}_{\hat{K}_i}^{-1} -{\Sigma}_{\hat{K}_i}^{-1} \rVert \leq \frac{2 \lVert\hat{\Sigma}_{\hat{K}_i}^{-1} -{\Sigma}_{\hat{K}_i}^{-1} \rVert}{\lambda_1^2(\Sigma_w)}$. The desired accuracy on $\lVert\hat{\Sigma}_{\hat{K}_i}^{-1} -{\Sigma}_{\hat{K}_i}^{-1} \rVert$ can be derived as:
\begin{equation}
\begin{split}
        \lVert\hat{\Sigma}_{\hat{K}_i} -{\Sigma}_{\hat{K}_i} \rVert &\leq \frac{\epsilon\sigma \lambda_1{(R)} \lambda_1^3(\Sigma_w)}{8h_{\nabla}(C(\hat{K}_i))\lVert \Sigma_K^*\rVert\lVert {\nabla} C(\hat{K}_i)\rVert}\\
        &\leq \frac{\epsilon\sigma \lambda_1{(R)} \lambda_1^3(\Sigma_w)}{8h_{\nabla}(C(\hat{K}_i))\lVert \Sigma_K^*\rVert\sqrt{b_{\nabla}(C(\hat{K}_i))}}=:\epsilon_{NPG}^\Sigma
\end{split}
\end{equation}
Then we should choose the $l_2,n_2,r_2$ according to Theorem \ref{Theorem5} to satisfy the accuracy on $\hat{\Sigma}_K$ and probability $\delta^{\frac{1}{2}}$ above. If we choose $n \geq\max\{n_1,n_2\},l\geq \max\{l_1,l_2\}, R\leq \min\{r_1,r_2\}$, then we have 
\begin{equation*}
    \mathbb{P}\left\{    \lVert C(\hat{K}_{i+1})-C({K}_{i+1})\rVert \leq \frac{\epsilon\sigma \lambda_1{(R)} \lambda_1(\Sigma_w)}{2\lVert \Sigma_K^*\rVert} \right\} \geq \delta
\end{equation*}
Then natural policy gradient can reach the desired accuracy $\epsilon$ with probability $\delta$. The remainder of the proof follows Theorem \ref{NPGTheorem}, which assumes that the gradient estimate and covariance matrix meet the required accuracy at each iteration, guaranteed by the union bound. 
\end{Proof}


\bibliographystyle{siamplain}
\bibliography{references}
\end{document}
