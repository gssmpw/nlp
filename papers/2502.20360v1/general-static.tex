\section{Selfish mining with static rewards}\label{sec:generalstatic}

\Cref{sec:prelims,subsec:properties} presented our model of general stochastic rewards and created a structure around these reward functions. The subsequent sections study a specific set of miner strategies to analyze their profitability and feasibility under general static rewards (\Cref{def:static}). We examine $\beta-$cutoff selfish mining strategies \cite{carlsten2016instability}, in which the attacker determines whether or not to hide their blocks based on the amount of reward realized during the mining process.

\input{strategies}

Given a static reward function, we want to determine the per-unit-time expected attacker rewards from following the $\beta$-cutoff strategy as in \Cref{def:betacutoff}. We develop a new technique based on a Markov Chain similar to Figure~13 in \citet{carlsten2016instability} and Figure~1 in \citet{eyal2013majority}. 

\begin{definition}[$\beta$-cutoff Markov Chain]\label{def:markovchain}
    Consider the NCG where the $1-\alpha$ of the mining power follows the honest strategy and $\alpha$ follows the $\beta$-cutoff strategy. Then define \statei for $i \geq 1$ where the attacker has a hidden chain $i$ blocks longer than the public chain. Let \statezero denote the attacker having no hidden blocks and \texttt{State 0'} denote the race state between the honest and attacker forks each of length 1. Let \texttt{State 0''} denote the state immediately after the attacker publishes their private chain. 
\end{definition}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{arxiv/img/final/mc.png}
    \caption{The Markov Chain capturing the $\beta-$cutoff strategy for miners deciding whether to publish blocks depending on the size of the static reward. $F_t(\beta)$ is the CDF of the rewards given time $t$ since the parent block, $\Pr[R(t) \leq \beta]$. The rate of the chain is $1/(1-\lambda)$, which explicitly captures the difficulty adjustment that results from a specific $\beta$-cutoff strategy.}
    \label{fig:markovgeneral}
\end{figure}

\Cref{fig:markovgeneral} depicts this Markov Chain. We now derive the transition probabilities using a general, static reward function. When considering static reward sources, notice that $R$ is only a function of the time since the parent block was mined; we hereafter denote this static reward source as $R(t)$, where $t$ is the time since the parent block. This simplification allows us to compute the probability of transitioning from \texttt{State 0} $\rightarrow$ \texttt{State 1} by comparing the expected amount of rewards earned in \texttt{State 0} conditioned on those rewards being less than $\beta$ (the cutoff threshold for publishing the block in \statezeronosp).

\begin{definition}[Static Reward CDF]\label{def:cdf}
    For a static reward source $R$ and randomness $r$, let $F_t(x)$ denote the CDF of the reward function indexed by time $t$,
    \begin{align*}
        F_t(x) = {\Pr}_r[R(t) \leq x].
    \end{align*}
\end{definition}
To calculate the probability of withholding the block, we integrate the probability distribution of the time until the next block multiplied by the CDF of the rewards at each time.
\begin{align}\label{eq:zerotoone}
	\Pr[\texttt{State 0} \rightarrow \texttt{State 1}] &= \underbrace{\alpha}_{\text{attacker block}} \int_0^\infty 
	\underbrace{\frac{e^{-t/(1-\lambda)}}{(1-\lambda)}}_{\shortstack{\scriptsize density of time}}
	\cdot \underbrace{F_t(\beta)}_{\shortstack{\scriptsize rewards $ <\beta$\\ \scriptsize by time $t$}} dt 
\end{align}
Intuitively, given a reward source $R$, this value tells us how likely it is that the rewards within an attacker block are less than $\beta$. Notice that the density function of the exponential depends on a rate parameter $1/(1-\lambda)$ (as discussed in \Cref{subsec:NCG}), where $\lambda$ the explicitly calculated orphan block rate calculated as a function of $\beta$ to account for difficulty adjustment. See \Cref{lem:lambda} for its derivation. 
Conversely, given an attacker block we can also calculate the probability that the attacker publishes the block immediately if the block rewards are be greater than $\beta$,
\begin{align}\label{eq:zerotozero}
	\Pr[\texttt{State 0} \rightarrow \texttt{State 0} \land \text{attacker block}] &= \underbrace{\alpha}_{\text{attacker block}} \int_0^\infty 
	\underbrace{\frac{e^{-t/(1-\lambda)}}{(1-\lambda)}}_{\shortstack{\scriptsize density of time}} 
	\cdot \underbrace{(1-F_t(\beta))}_{\shortstack{\scriptsize rewards $ \geq \beta$\\ \scriptsize by time $t$}} dt 
\end{align}
With \Cref{eq:zerotoone,eq:zerotozero}, we construct the entire Markov chain in \Cref{fig:markovgeneral}. Note that it differs from Figure~1 in \citet{eyal2013majority} and Figure~13 in \citet{carlsten2016instability}, only in the transition probabilities from \statezero calculated above for general static reward sources (\Cref{eq:zerotoone,eq:zerotozero}). 
As in previous work, $\gamma$ is the tie-breaking rate dictating the fraction of honest miners who mine on the attacker block after it is published, and there is a race of length-1 forks (in \texttt{State 1}). This parameter doesn't impact the $\beta$-cutoff itself and only affects the probability that the attacker fork wins the tie. 
Using this Markov Chain, we calculate the stationary distribution using the same technique conducted in Appendix~E.2 in \citet{carlsten2016instability}.
\begin{definition}[Stationary distribution, $p_i$]\label{def:stationary}
    Let $p_i$ denote the stationary distribution of the Markov Chain for \texttt{State i}.
    We start by calculating all probabilities relative to $p_1$,
    \begin{alignat*}{2}
    	&p_0 &&= \frac{p_1}{\alpha \int_0^\infty \frac{e^{-t/(1-\lambda)}}{(1-\lambda)}F_t(\beta)dt} \\ 
    	&p_{0'} &&= p_1(1-\alpha) \\
    	&p_{0''} &&= p_1 \alpha \\
    	&p_i &&= p_1\left(\frac{\alpha}{1-\alpha}\right)^{i-1}, \; \text{for } i \geq 1.
    \end{alignat*}
    Using the simplex constraint, we solve for $p_1$ explicitly,
    \begin{align*}
    	p_0 &+ p_{0'} + p_{0''} + \sum_{i=1}^\infty p_i =1 \implies  p_1 = \left(\frac{1}{\alpha \int_0^\infty \frac{e^{-t/(1-\lambda)}}{(1-\lambda)}F_t(\beta)dt} + 1 + \frac{1-\alpha}{1-2\alpha}\right)^{-1}.
    \end{align*}
\end{definition}

With the stationary distribution, we can explicitly solve for the proportion of orphan blocks, $\lambda \in [0,1]$, which in turn gives us the difficulty-adjusted rate of the Poisson process of the transitions in the Markov Chain as $1/(1-\lambda)$. This rate is \textit{faster} than the rate of canonical blocks (normalized to 1) because the orphaning process causes a reduction in difficulty.  

\begin{lemma}[Calculating $\lambda$]\label{lem:lambda}
    Let $\lambda$ measure the probability that a block produced in the Markov Chain is orphaned. Then,
    \begin{align*}
        \lambda = p_1 (1-\alpha) \left(1+\frac{\alpha}{1-2\alpha}\right).
    \end{align*}
\end{lemma}
\begin{proof}
	Every time the Markov Chain enters \texttt{State 0'}, a block is orphaned. Additionally, for all \statei where $i \geq 2$, a block is orphaned with probability $1-a$ as any honest block will be abandoned when \texttt{State 0''} is reached. Thus,
	\begin{align*}
		\lambda &= p_{0'} + (1-a)\sum_{i=2}^\infty p_i \\ 
        &= p_1 (1-\alpha)\left(1 + \sum_{i=2}^\infty \left(\frac{\alpha}{1-\alpha}\right)^{i-1}\right) \\
        &= p_1 (1-\alpha) \left(1+\frac{\alpha}{1-2\alpha}\right).
	\end{align*}
\end{proof}
With $\lambda$, the new block production rate is $1/(1-\lambda)$. This is the rate at which blocks are found by any miner (i.e., the rate of transitioning between states in the Markov Chain; \Cref{fig:markovgeneral}) assuming a constant hash rate and results in the canonical chain blocks being produced at a rate of $1$.


\subsection{Expected attacker rewards}\label{subsec:perstateattacker}
The stationary distribution alone is incomplete. To determine the attacker profit for a given cutoff strategy, we calculate their expected profit from each state and multiply those values by the stationary distribution of the Markov Chain to determine the expected profit per unit of time.
\begin{definition}[Per-state attacker rewards, $f_i$]\label{def:fis}
    Let $f_i$ denote the expected reward of a canonicalized attacker block mined in \texttt{State i}.     
\end{definition}
To calculate this value, we need to find the expected value of the reward function by integrating the time distribution over the possible paths that include an attacker block claiming rewards arriving during \texttt{State i}. 
We first enumerate all possible paths that result in a canonical attacker block from \stateinosp; we then integrate the reward function over each path. The following example demonstrates this technique, and we generalize it in \Cref{thm:attackgeq2}.

\begin{example}[\texttt{State 3} paths]\label{ex:state3paths}
	Consider the rewards arriving after the attacker has a lead of length three. These rewards can be canonicalized in four different ways:
	\begin{enumerate}
		\item the attacker finds the next block, extending their lead to four,
		\item the honest parties find the next block, then the attacker finds the subsequent,
		\item the honest parties find the next two blocks, causing the attacker to publish their hidden chain, and then the attacker finds the first block after publishing,
		\item the honest parties find the next two blocks, causing the attacker to publish their hidden chain, and then the honest parties find the first block after that.
	\end{enumerate}
\end{example}
We can succinctly represent these four outcomes using the strings, \texttt{A, HA, HHA, HHH}, where \texttt{H} \& \texttt{A} denote honest and attacker blocks, respectively.
This example prompts the definition of attacker paths.
\begin{definition}[Attacker paths]
	Given \statei for all $i \geq 2$, there are $i$ distinct paths resulting in the attacker capturing rewards accrued in that state. The paths are enumerated as the string \texttt{(H$^*$)A}, where \texttt{H} \& \texttt{A} denote honest and attacker blocks respectively and \texttt{H} is repeated $0,1,\ldots i-1$ times.
\end{definition}

Continuing our \texttt{State 3} example, we now calculate the expected reward from each attacker path; adding these together is precisely the value of interest, $f_3$.

\begin{example}[$f_3$ continued]
	Consider the three attacker paths of \texttt{State 3}: \texttt{A, HA, HHA}. These paths have lengths 1,2,3 and occur with probabilities $\alpha, (1-\alpha)\alpha, (1-\alpha)^2\alpha$, respectively. Thus, we calculate the expected reward as,
	\begin{align*}
		f_3 =& 
		\underbrace{\alpha\int_{0}^\infty \frac{e^{-t/(1-\lambda)}}{(1-\lambda)} \mathbb{E}_{r}[R(t)]dt}_{\texttt{A}} +
		\underbrace{(1-\alpha)\alpha \int_0^\infty \frac{te^{-t/(1-\lambda)}}{(1-\lambda)^2}\mathbb{E}_{r}[R(t)] dt}_{\texttt{HA}} \\
		&+ 
		\underbrace{(1-\alpha)^2\alpha \int_0^\infty \frac{t^2 e^{-t/(1-\lambda)}}{2(1-\lambda)^3} \mathbb{E}_{r}[R(t)]dt}_{\texttt{HHA}}
	\end{align*}
\end{example}
Each of these expressions can be viewed as the product of three independent sources of randomness. The coefficients of the integrals are the probabilities of each path determined by the winning miner, which depends on $\vec{m}$. The first expression in the integrand is the PDF of the Erlang Distribution, which measures the sum of i.i.d. exponential random variables (all with rate $1/(1-\lambda)$) to determine the amount of time of the path, which depends on $\vec{t}$. The second expression in the integrand is the expected value over all remaining randomness, $r$, of the reward function at time $t$.
We now generalize for \statei where $i \geq 2$.

\begin{lemma}[$f_{i\geq 2}$]\label{thm:attackgeq2}
	For all states $i\geq 2$, the expected attacker rewards collected in \texttt{State i},
	\begin{align*}
		f_i &= \sum_{j=0}^{i-1} \left[\alpha (1-\alpha)^j \int_{0}^\infty \frac{t^j e^{-t/(1-\lambda)}}{j!(1-\lambda)^{j+1}}\mathbb{E}_{r}[R(t)]dt\right]
	\end{align*}
\end{lemma}

\begin{proof}
	In the set of \statei attacker paths, there is exactly one path for each length $j=1, 2, \ldots i$, and the paths are $j-1$ copies of \texttt{H} before a single \texttt{A} (\texttt{A, HA, HHA, \ldots}). Each path occurs with probability $\alpha(1-\alpha)^j$, and the distribution of time for the length of the path is Erlang$(j,1/(1-\lambda))$. We integrate over the density of these path timings and multiply by the expectation of $R(t)$ over all remaining randomness, $r$. 
\end{proof}

\paragraph{Calculating $f_0$.}
\statezero requires deriving the expected reward for an attacker, given they may or may not hide a block they find. For this, we need the PDF of a static reward function.
\begin{definition}[Static Reward PDF]\label{def:pdf}
	For a static reward source $R$, let $f_t(x)$ denote the PDF of the reward function over randomness $r$ indexed by time $t$,
	\begin{align}
		f_t(x) = {\Pr}_r[R(t) = x].
	\end{align}
\end{definition}
From \texttt{State 0}, rewards are canonicalized by an attacker block in three ways:
\begin{description}[leftmargin=!,labelwidth=2.5cm]
	\item[Case i] the block has more rewards than $\beta$ (the attacker publishes),
	\item[Case ii] the block has less rewards than $\beta$ (the attacker hides) \textit{and} the attacker finds the next block,
	\item[Case iii] the block has less rewards than $\beta$ \textit{and} honest finds the next block (transitioning to \texttt{State 0'}) \textit{and} the attacker fork wins the race.
\end{description}
We treat each case individually. For \textbf{Case i}, the attacker publishes the block and thus realizes those rewards immediately on the canonical chain.
\begin{align*}
	f_{0,(i)} &= \underbrace{\alpha}_{\text{attacker block}} \int_{t=0}^\infty 
	\underbrace{\frac{e^{-t/(1-\lambda)}}{(1-\lambda)}}_{\shortstack{\scriptsize density of time}}
	\underbrace{\int_{x=\beta}^\infty x f_t(x)  dx}_{\shortstack{\scriptsize expected reward $\geq \beta$\\ \scriptsize at time $t$}} dt.
\end{align*}
This is exactly the expected attacker value of the state transition \texttt{State 0} $\rightarrow$ \texttt{State 0}. The inner integral bounds are $\beta \to \infty$ to capture the expected rewards given they are greater than $\beta$. For \textbf{Case ii}, the attacker block mined in \texttt{State 0} will become canonicalized for certain once they mine the second block. Thus, their rewards are realized when they transition to \texttt{State 2}.
\begin{align*}
	f_{0,(ii)} &= \underbrace{\alpha^2}_{\shortstack{\scriptsize two attacker \\ \scriptsize blocks}}
	\int_0^\infty 
	\underbrace{\frac{e^{-t/(1-\lambda)}}{(1-\lambda)}}_{\shortstack{\scriptsize density of time}}
	\underbrace{\int_{x=0}^\beta x f_t(x)  dx}_{\shortstack{\scriptsize expected reward $< \beta$\\ \scriptsize at time $t$}} dt.
\end{align*}
This is the contribution to the attacker's expected rewards of the state transition \texttt{State 0} $\rightarrow$ \texttt{State 1} given a second attacker block in a row. Here, the integral is evaluated from $0 \to \beta$ to account for the expected value of rewards conditioned on the block remaining unpublished. For \textbf{Case iii}, the attacker block mined in \texttt{State 0} will become canonicalized if they win the race out of \texttt{State 0'} (e.g., either themselves or the $\gamma (1-\alpha)$ portion of the honest network that contributes to their chain mining the subsequent block and breaking the tie). Thus, their rewards are realized when they transition back to \texttt{State 0}.
\begin{align*}
	f_{0,(iii)} &= \underbrace{\alpha}_{\shortstack{\scriptsize attacker block \\ \scriptsize in \texttt{State 0}}}\underbrace{(1-\alpha)}_{\shortstack{\scriptsize honest block \\ \scriptsize in \texttt{State 1}}}
	\underbrace{(\alpha+\gamma(1-\alpha)}_{\shortstack{\scriptsize attacker fork \\ \scriptsize wins tie-break}}
	\int_0^\infty 
	\underbrace{\frac{e^{-t/(1-\lambda)}}{(1-\lambda)}}_{\shortstack{\scriptsize density of time}}
	\underbrace{\int_{x=0}^\beta x f_t(x)  dx}_{\shortstack{\scriptsize expected reward $< \beta$\\ \scriptsize at time $t$}} dt.
\end{align*}
Thus $f_0 = f_{0,(i)}+f_{0,(ii)}+f_{0,(iii)}$.


\paragraph{Calculating $f_1$.}
For \texttt{State 1}, rewards arriving in that state will be canonicalized by the attacker under two paths: (i) the attacker finds the next block (transitioning into \texttt{State 2}) or (ii) the honest party finds the next block (transitioning into \texttt{State 0'}) \textit{and} the attacker finds the subsequent. This is the same as the for the \texttt{State 2} attacker paths \texttt{A,HA}, so we use \Cref{thm:attackgeq2} with $i=2$,
\begin{align*}
	f_1 = \alpha \int_0^{\infty} \frac{e^{-t/(1-\lambda)}}{(1-\lambda)} \mathbb{E}_{r}[R(t)]dt + \alpha(1-\alpha) \int_{0}^\infty \frac{te^{-t/(1-\lambda)}}{(1-\lambda)^2}  \mathbb{E}_{r}[R(t)] dt.
\end{align*}
Note that for \texttt{States 0', 0''}, the rewards accrued are already accounted for in $f_1$ and $f_2$ calculations, respectively. With $\lambda$ derived in \Cref{lem:lambda}, the stationary distribution calculated in \Cref{def:stationary} (the $p_i$ values), and the per-state attacker expected rewards calculated in \Cref{def:fis} (the $f_i$ values), we can calculate the full rewards for the attacker following the $\beta-$cutoff strategy.
\begin{definition}\label{def:fullreward}
    The attacker's reward is,
    \begin{align*}
        \text{ATTACKER REWARD} =f_0p_0 + f_1 p_1 + \alpha \sum_{i=2}^\infty f_i p_{i-1}.
    \end{align*}
\end{definition}
For \texttt{States 0, 1}, we multiply the stationary distribution probability by the expected per-state attacker reward to calculate the contribution to the full attacker reward. For \stateinosp, $i\geq 2$, we need to avoid double counting the contributions from each state (e.g., you can transition to \texttt{State 3} from either \texttt{State 2} \textit{or} \texttt{State 4}). To account for this we only consider the probability of arriving in each state from the $i-1$ state, which occurs with probability $\alpha p_{i-1}$. Thus, for each state, we add the contribution to the total attacker reward as $\alpha f_i p_{i-1}$. The resulting value tells us the expected attacker reward per unit time of following a $\beta$-cutoff strategy under the static reward function and as a function of $\alpha,\beta,\gamma.$ 
