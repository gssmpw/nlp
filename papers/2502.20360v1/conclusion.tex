\section{Conclusion and future work}\label{sec:conclusion}
We hope this work serves as a starting point for a more complete picture of participants' incentives in permissionless consensus mechanisms. The model of the NCG under general stochastic rewards developed in \Cref{sec:prelims} can serve as the basis for modeling all consensus games. Similarly, the properties and examples we develop in \Cref{subsec:properties} focus on Proof-of-Work, but we believe they naturally extend to all blockchain protocols under slight modifications. The path-counting technique presented in \Cref{sec:generalstatic}, which allows for the explicit instantiation of the aggregate reward function $\hat{R}$ in \Cref{sec:multiplerewards}, highlights the importance of tracking difficulty adjustment explicitly when considering rewards that may be random functions of time since the parent block was mined. The methodology is specific to $\beta$-cutoff selfish mining, but other strategies should be similarly tractable with the tools used here. 

More broadly, we hope this work inspires a more thorough understanding of how MEV and application-layer generated revenue can warp protocol-prescribed rewards and lead to safety and liveness faults in blockchain consensus mechanisms. To that end, we outline many potential future research directions.

\paragraph{Applying our methodology more broadly.} We believe our reward instantiation in \Cref{sec:multiplerewards} is a reasonably realistic model of reward sources in the Bitcoin blockchain today.
The methodology and instantiation represent a significant step in understanding the risk of selfish mining in the presence of multi-faceted rewards, especially since prior work generally considered one reward source at a time. 
However, empirical analysis may strengthen our results by forming a more nuanced understanding of these rewards in practice (e.g., measuring the relative size and probability of different MEV events). Note that our methodology still applies to any static reward sources that can be analytically calculated using the path-counting technique presented in \Cref{sec:generalstatic}. Beyond explicitly using our methodology, there are relatively straightforward extensions to our technique that can reach beyond static rewards and $\beta-$cutoff strategies.

\paragraph{Extending our methodology.} There are several natural extensions to our methods. For example, considering the profitability of $\beta$-cutoff selfish mining under non-static reward functions is feasible. Such reward functions depend on additional information not captured in the states of the Markov Chain (\Cref{def:markovchain}). However, suppose the additional information is exogenous to the chain and independent of views. In that case, it is possible to augment the state space of the Markov Chain to include this information.
To capture non-local LVR (\Cref{rem:non-local-lvr}), which depends on the price of an asset on a CEX, augmenting each state with that price level allows explicit modeling of non-local LVR. The transitions for this new Markov Chain would now also depend on the starting price level on the CEX and would enable the attacker to condition their cutoff threshold on the price level.

Another extension is to study MDP-based optimal strategies as in \citet{sapirshtein2017optimal} rather than $\beta$-cutoff selfish mining. \citet{ZurAET23} demonstrate the impact of changing the reward function on optimal selfish mining profits when considering the combination of block rewards and occasional ``whale'' (high fee-paying) transactions, and they note that the resulting large state spaces were untractable with traditional MDP solving tooling and thus required Deep Reinforcement Learning. Considering how to more succinctly represent multi-reward state spaces or using the Deep RL approach with more combinatorial rewards are promising directions. While the strategies in the current paper only make broadcasting decisions based on the realization of rewards in the \emph{current} block, the broader MDP strategy space can be future-looking; for example, an attacker may want to start creating a hidden chain of several blocks in advance of an anticipated large reward (e.g., from an NFT drop occurring at a specific block height). Expanding the strategy space could better capture realistic mining strategies during the launch of Babylon, where the height of the highly contentious blocks was publicly known in advance.

Finally, our model of reward functions can be used to understand selfish attacks in other consensus protocols beyond Proof-of-Work. A particularly relevant example is timing games in leader-based protocols (e.g., Proof-of-Stake), where a myopic validator delays creating a block in the hopes of collecting more time-accruing rewards (\Cref{ex:timinggames}). The distinction between leader-based and leaderless also leads to interesting implications since advanced knowledge of the ensuing block producers enables more strategies than would be possible if block contents are committed to before block creation. See the discrete vs. continuous LVR discussion in \Cref{sec:examples} for more details.

\paragraph{A complete picture of consensus incentives.}
As demonstrated in \Cref{ex:babylon,ex:lowcarb,ex:timinggames}, modern blockchains have faced and will continue to face distortion of consensus incentives from the application layer handling larger amounts of economic activity. \Cref{subsec:properties} is a first step at modeling properties of general reward functions, but applying these properties to MEV beyond the transaction fee and LVR case studies in \Cref{sec:examples} remains as vital open work. A taxonomy of MEV types and the corresponding properties would need more thorough treatment to be complete. Additionally, a clear demonstration that a set of properties is sufficient (e.g., fully covers all possible properties of various MEV types) would be invaluable. 

Beyond characterizing MEV, how this value is distributed among participants is another key open question. Modeling the relationship between wallet providers, block builders, mining/staking pools, and the other actors who partake in the consensus process, especially as it relates to the model of rewards and reward properties described above, remains a vital step to understanding how participants in permissionless crypto-economic systems can and will behave strategically. Studying heterogeneity of reward sources (e.g., non-miner-independent \Cref{def:minerindependent}) was out of the scope of this work but remains a critical reality of the current MEV landscape. Expanding and exploring the properties of reward functions in \Cref{subsec:properties} when block producers may have highly different realizations of the rewards available for mining a block is another key element of reality that should be modeled explicitly.