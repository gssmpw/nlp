\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/intro.png} \hfill

    \caption{Comparison of feedback approaches for data synthesis. Left: Traditional sample-level feedback generates and applies feedback individually for each sample. Right: Our \textsc{Reference-Level Feedback} approach collects feedback once from a high-quality reference sample and applies it to synthesize and improve multiple new samples.
    }
    \label{fig:intro}
\end{figure}

\section{Introduction}

Large Language Models (LLMs) demonstrate remarkable capabilities in following natural language instructions and performing real-world tasks \citep{openai2024openaio1card, dubey2024llama3herdmodels}. This can be largely attributed to instruction-tuning, which refers to supervised finetuning (SFT) on instruction-response pairs \citep{wei2022finetunedlanguagemodelszeroshot, bai2022traininghelpfulharmlessassistant, NEURIPS2022_b1efde53}. Recent advancements in instruction-tuning emphasize the importance of high-quality datasets in enhancing model performance \citep{chen2024alpagasus, lima}.

Traditionally, high-quality instruction-tuning datasets are created by repurposing existing datasets or using human annotators \citep{wei2022finetunedlanguagemodelszeroshot, wang-etal-2022-super, NEURIPS2022_b1efde53,lima,chen-etal-2024-minprompt}. However, these methods present challenges that prevent the creation of large-scale datasets, such as data scarcity and the considerable cost and time required for human annotation \citep{liu2024bestpracticeslessonslearned, long-etal-2024-llms, singh2024humandatascalingselftraining}. The use of synthetic data for the creation of instruction-tuning datasets has emerged as a reliable alternative that overcomes such challenges \citep{wang-etal-2023-self-instruct, alpaca, xu2023wizardlmempoweringlargelanguage, peng2023instructiontuninggpt4}. 

To further improve the quality of the synthesized data, recent approaches incorporate natural language feedback \citep{chen2024iteralign,chen2024learning, sun2024principle, bai2022constitutionalaiharmlessnessai}. In these approaches, an LLM generates a response to an existing instruction, then collects feedback on their response either through self-reflection, from a stronger LLM or a human annotator. This resulting feedback is provided to the LLM to refine its initial response. Such uses of feedback has proven effective in improving LLM performance on alignment benchmarks as well as reinforcing specific principles such as helpfulness and truthfulness \citep{chen2024learning, sun2024principle, bai2022constitutionalaiharmlessnessai}. The current feedback-driven approaches operate at the sample-level, which means that feedback is generated for and applied to each response individually.

Rather than collecting sample-level feedback, we propose \textsc{Reference-Level Feedback}, a novel approach that uses feedback collected from the high-quality reference samples in a seed dataset, as shown in Figure \ref{fig:intro}. Many data synthesis approaches carefully curate reference samples to use as seed data, which are used as in-context examples for synthesis \citep{wang-etal-2023-self-instruct, alpaca}. We extend this and collect feedback based on these reference samples, since they serve as exemplars for training data. These samples are of higher quality than model generated samples, so the captured feedback provides a richer signal towards the desirable characteristics (i.e. clarity, relevance) for a data sample.

Our framework is presented in Figure \ref{fig:pipeline}. For each reference sample, we identify the desirable characteristics of both the instruction and response components and use it to create instruction and response feedback, respectively. The instruction-specific feedback is used to guide the synthesis of new instructions, and response-specific feedback is used to refine the corresponding responses. Since synthesized instructions share key characteristics of their reference counterparts, response-specific feedback remains relevant and is used to improve the quality of synthesized responses. This framework enables us to systematically propagate the desirable qualities of reference samples to newly generated samples, establishing overall higher quality standards for data synthesis.

We demonstrate the effectiveness of our approach through \textbf{RE}ference-Level \textbf{F}eedback \textbf{E}nhanced \textbf{D}ata (\textsc{REFED}), a dataset synthesized using our framework. Models fine-tuned on \textsc{REFED} achieve state-of-the-art performance on instruction-following benchmarks AlpacaEval 2.0 \citep{dubois2024lengthcontrolledalpacaevalsimpleway} and Arena-Hard \citep{arenahard2024}. Through comprehensive experiments and analyses, we demonstrate that using \textsc{Reference-Level Feedback} is more effective for synthetic data generation compared to existing approaches.

The main contributions of this work are \footnotemark[1]:
\begin{itemize}
    \item We introduce \textsc{Reference-Level Feedback} for data synthesis, a novel method that leverages feedback collected from reference samples to capture and propagate desirable characteristics to newly synthesized data. Using our framework, and the LIMA \citep{lima} training dataset as seed data, we synthesize \textsc{REFED}, a dataset of 10K instruction-response pairs.
    \item We demonstrate the effectiveness of our approach by presenting Llama-3.1-8B-Instruct-\textsc{REFED}, a Llama-3.1-8B-Instruct model finetuned on \textsc{REFED}. The resulting model achieves state-of-the-art performance among similarly sized models trained only with SFT on the AlpacaEval 2.0 leaderboard \citep{dubois2024lengthcontrolledalpacaevalsimpleway}, with a 21.06\% improvement in length-controlled win rate. It also shows strong performance on Arena-Hard, outperforming larger models like GPT-3.5-Turbo \citep{arenahard2024}.
    \item Through comprehensive experiments, we demonstrate that our approach: (1) outperforms models trained on other synthetic instruction-tuning datasets, (2) consistently improves base and instruct variants of different model architectures, and (3) provides more effective quality improvements compared to traditional sample-level feedback approaches, while also being more efficient.
\end{itemize}

\footnotetext[1]{Our code and data are available at \url{https://github.com/Shuhaibm/refed}}