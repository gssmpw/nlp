\section{Related Work}

\paragraph{Synthetic Data for Instruction Tuning.}

Data synthesis has emerged as an effective and scalable approach to creating instruction-tuning datasets. One line of approaches use instruction-response pairs from a seed dataset as in-context examples to guide synthesis \citep{wang-etal-2023-self-instruct, alpaca, peng2023instructiontuninggpt4}. While \citet{wang-etal-2023-self-instruct} use models to self-generate their training data, subsequent works leverage more capable proprietary models to generate higher quality data \citep{alpaca, peng2023instructiontuninggpt4}.

Other approaches have explored alternative synthesis strategies. There are works that use structured guidance through manually curated taxonomies or LLM-generated skill sets \citep{li2024syntheticdataalmostscratch, kaur2024instruct}. \citet{xu2024magpiealignmentdatasynthesis} uses pre-query templates to sample instructions from aligned LLMs and generate instructions that reflect the LLM's existing knowledge.

Many works have explored methods for enhancing the quality of synthesized data. \citet{xu2023wizardlmempoweringlargelanguage} proposes Evol-Instruct, which generates increasingly complex versions of existing instructions. Other approaches include using multi-agent simulation \citep{tang2024synthesizingposttrainingdatallms} or incorporating natural language feedback \citep{bai2022constitutionalaiharmlessnessai, chen2024learning, NEURIPS2023_0764db11}.


\paragraph{Natural Language Feedback.}

Natural language serves as a rich medium for providing feedback to LLMs, effective in conveying detailed and nuanced information. Recent work has demonstrated the effectiveness of using LLMs to generate this feedback. \citet{NEURIPS2023_91edff07} introduce Self-Refine, which has LLMs generate feedback and refine their own responses. Following this, several works have shown that using various feedback methods and fine-tuned critic models can yield further improvements \citep{jin2023dataefficient, wang2023shepherdcriticlanguagemodel, gou2024criticlargelanguagemodels, wu2024meta}.

Another application of feedback is at the dataset level, focusing on creating higher-quality training data. Constitutional AI \citep{bai2022constitutionalaiharmlessnessai} generates self-critiques and revisions to create training data aligned with specific principles. In a similar manner, Self-Align \citep{NEURIPS2023_0764db11} uses natural language descriptions of desirable qualities to guide LLMs towards producing more aligned outputs, IterAlign \citep{chen2024iteralign} uses an iterative process to discover constitutions and self-correct, and \citet{chen2024learning} demonstrate the effectiveness of feedback-based refinement in code generation tasks.

In order to more effectively incorporate feedback for data synthesis, we introduce \textsc{Reference-Level Feedback}. It fundamentally differs from existing feedback-based methods in three key aspects. First, while previous work collects feedback at the sample-level, we collect feedback from high-quality reference samples in the seed data. This enables us to identify and propagate desirable qualities from reference samples and establish higher quality standards for the data synthesis.

Secondly, our approach more effectively leverages seed datasets. Rather than just using seed data samples as in-context examples for synthesizing similar samples, we systematically analyze and capture the specific qualities that make the reference sample effective.

Lastly, we expand the role of feedback beyond response refinement and guide the entire data synthesis process: our method uses feedback to synthesize new instructions and to refine the corresponding responses.