\input{tables/table1}
\input{tables/table2}
\input{tables/table3}

\section{Experiments}


\subsection{Experimental Setup}
\paragraph{Data Synthesis.} 

We use the LIMA \citep{lima} training dataset as our seed dataset, which comprises of one thousand carefully curated instruction-response pairs. The samples were either manually written or selected from community forums, and were selected based on quality as well as diversity. This dataset was chosen because it is concise enough to serve as a seed dataset, while being well-designed and has demonstrated effectiveness for instruction tuning \citep{lima}.

In our experiments, we use GPT-4o mini \citep{OpenAIGpt4oMini} with our data synthesis framework to create \textsc{REFED}, an instruction tuning dataset with 10K data samples.

\paragraph{Training Setup.}

We finetune the base and instruct variants of Llama-3.1-8B \citep{grattafiori2024llama3herdmodels} and Mistral-7B \citep{jiang2023mistral7b} on \textsc{REFED}. We use a learning rate of $1 \times 10^{-6}$ for instruct variants, and $2 \times 10^{-5}$ for base variants. All other hyperparameters remain consistent across models: linear warmup ratio of 0.03, cosine decay, batch size of 128, and maximum sequence length of 2048. The models are trained for 15 epochs, with checkpoint selection based on length-controlled win-rate \citep{dubois2024lengthcontrolledalpacaevalsimpleway} on a held-out validation set of 100 synthesized instruction-response pairs that were synthesized with GPT-4o \citep{openai2024gpt4ocard}. 

When training on larger datasets like Evol Instruct \citep{xu2023wizardlmempoweringlargelanguage} and UltraChat \citep{ding-etal-2023-enhancing}, we follow prior works and modify our training setup as follows: 100 warmup steps, batch size of 32, and train for 2 epochs \citep{xu2024magpiealignmentdatasynthesis}.


\paragraph{Evaluation.}

To evaluate our model's instruction-following abilities, we use two benchmarks: AlpacaEval 2.0 \citep{dubois2024lengthcontrolledalpacaevalsimpleway} and Arena-Hard \citep{arenahard2024}. These benchmarks are automatic evaluators of language models' instruction-following abilities and have demonstrated the highest correlations with human preferences from Chatbot Arena \citep{li2024crowdsourced, dubois2024lengthcontrolledalpacaevalsimpleway}. 

Both benchmarks compute win rates by using a powerful LLM as a judge to compare model responses against established responses from a reference model. To further improve correlation with human preferences, AlpacaEval 2.0 additionally computes a length-controlled win rate that mitigates biases towards longer responses by comparing responses of similar length \citep{dubois2024lengthcontrolledalpacaevalsimpleway}.

In our experiments, we follow standard evaluation protocols and use GPT-4-Turbo (1106) as a judge. For AlpacaEval 2.0, we use GPT-4-Turbo (1106) as the reference model, and GPT-4-Turbo (0314) for the reference model in Arena-Hard.


\subsection{Experimental Results}

\subsubsection{Experiment 1: How Effective is \textsc{Reference-Level Feedback} for Data Synthesis?}

The first set of experiments evaluate the effectiveness of \textsc{Reference-Level Feedback} for data synthesis by comparing it against traditional sample-level feedback, and also systematically analyzing different components in our framework. For each approach, we synthesize datasets with 10K samples, finetune Llama-3.1-8B-Instruct on that data, then evaluate.

We conduct an ablation study by progressively introducing the different components of the reference-level feedback in our framework, instruction and response feedback. Starting with a baseline of no feedback, we finetune on just our initial seed dataset. Next, we train on a dataset with 10K samples that was created by incorporating instruction feedback from \textsc{Reference-Level Feedback} and generate the corresponding response. Lastly, we evaluate our complete approach by synthesizing a dataset that also incorporates the response feedback to improve the generated response (\textsc{REFED}).

Additionally, we compare against sample-level feedback, where feedback is generated and applied individually for each response. Here, the synthesis pipeline remains consistent, with minimal prompt modifications to accommodate different feedback types. With this, we can effectively isolate the impact of different feedback strategies on response quality.

\paragraph{Results.}

The results in Table \ref{tab:table1} demonstrate improvements in performance as each component of our framework is introduced. On both benchmarks, we see a clear improvement as we introduce using instruction feedback, and response feedback to synthesize data.

On AlpacaEval 2.0, using the complete \textsc{Reference-Level Feedback} for data synthesis achieves a length-controlled win rate of 43.96\% and win rate of 42.25\%, showing that it is superior to sample-level feedback (LC: 42.92\%, WR: 41.74\%). Results on Arena-Hard are similar, where it achieves a win rate of 35.9\%, substantially outperforming sample-level feedback (WR: 30.8\%).

The consistent performance gains across both benchmarks demonstrate that \textsc{Reference-Level Feedback} is more effective for improving responses and generating high-quality data compared to alternative feedback types.

\subsubsection{Experiment 2: How Does Our Method Compare Against Other Baselines?} \label{sec:4.2.2}

We evaluate the performance of our synthetic data by comparing a Llama-3.1-8B-Instruct model finetuned on our dataset against several baselines.

\paragraph{Baselines.}
For the first set of baselines, we finetune Llama-3.1-8B-Instruct on various well-known synthetic datasets: Alpaca \citep{alpaca}, Evol Instruct \citep{xu2023wizardlmempoweringlargelanguage}, UltraChat 200K \citep{ding-etal-2023-enhancing}, and Instruct-Skillmix-SDA \citep{kaur2024instruct}. We use an identical training setup to the one we use for our models.

We also compare against leading models from the AlpacaEval 2.0 leaderboard that use SFT to train 8B-parameter models: Llama-3-8B-Instruct-Skillmix, which trains Llama-3-8B on the Instruct-Skillmix dataset \citep{kaur2024instruct}, and Infinity-Instruct-7M-Gen-Llama3.1-8B model, trained on Infinity-Instruct-7M and Infinity-Instruct-Gen \citep{InfinityInstruct2024}. Additionally, we consider some larger and more powerful models such as GPT-3.5, Llama-3.1-405B-Instruct \citep{dubey2024llama3herdmodels} and Claude 3 Opus \citep{anthropic_claude3}.

\paragraph{Results.}

Our results are presented in Table \ref{tab:table2}. The Llama-3.1-8B-Instruct model finetuned on \textsc{REFED} achieves state-of-the-art performance among similar sized models trained with SFT, across both evaluation benchmarks. On AlpacaEval 2.0, it achieves a length-controlled win rate of 43.96\%. This not only scores higher than our selected baselines, but also outperforms significantly larger models including LLama-3.1-405B-Instruct and Claude 3 Opus. On Arena-Hard, we get a win-rate of 35.9\%, outperforming both our baseline models and established models like GPT-3.5 Turbo. These results demonstrate that our data synthesis approach can enable strong model performance on established benchmarks, highlighting the effectiveness of \textsc{Reference-Level Feedback}.

\subsubsection{Experiment 3: Does \textsc{REFED} Generalize To Different Model Architectures?}

In this section, we evaluate the effectiveness of \textsc{REFED} across different models by finetuning both base and instruct variants of Llama-3.1-8B \citep{grattafiori2024llama3herdmodels} and Mistral-7B \citep{jiang2023mistral7b}. This analysis validates the robustness of our approach by demonstrating consistent benefits across different cases.

\paragraph{Results.}

Our results are presented in Table \ref{tab:table3}. Training on \textsc{REFED} yields improvements across all model variants. In particular, the instruct models show very strong performance. Llama-3.1-8B-Instruct-\textsc{REFED} achieves the strongest performance, with a length-controlled win rate of 43.96\% on AlpacaEval 2.0 and 35.9\% on Arena-Hard. Mistral-7B-Instruct-\textsc{REFED} shows impressive results, with 41.0\% and 25.0\% respectively. 

The base models also demonstrate notable improvements. Llama-3.1-8B-\textsc{REFED} achieves a length-controlled win rate of 29.63\% on AlpacaEval 2.0 and 12.7\% on Arena-Hard, outperforming Llama-3.1-8B-Instruct (20.9\%) on AlpacaEval 2. Similarly, Mistral-7B-\textsc{REFED} achieves 16.97\% on AlpacaEval 2.0 and 3.6\% on Arena-Hard, getting close performance to Mistral-7B-Instruct (20.7\%). 


These results demonstrate that \textsc{REFED} effectively improves instruction-following capabilities across different models and model variants. The strong performance gains, particularly in base models surpassing their instruct variants, highlight how effective our dataset is in developing LLM instruction-following abilities. This observation matches the model-agnostic design of our method.


\subsubsection{Experiment 4: Does Filtering Enhance the Effectiveness?}

We explore how different filtering approaches affect model performance by finetuning Llama-3.1-8B-Instruct on various subsets of filtered data. We compare three strategies: random sampling, LLM-judge filtering, and ROUGE-L similarity filtering.
\paragraph{Random Sampling.}
As our baseline, we randomly sample subsets of size 1K, 2K, 4K, and 8K from \textsc{REFED}. 
% This approach shows the lowest performance across all subset sizes.
\paragraph{LLM-Judge Filtering.}
We use GPT-4o-mini as a judge to evaluate pairs of initial and refined responses. We only keep samples where refined responses are rated higher than initial responses, and obtain approximately 5K instruction-response pairs. From these, we sample subsets of size 1K, 2K, and 4K.
% , and achieve the best performance amongst all strategies.
\paragraph{ROUGE-L Similarity Filtering.}
Following \citet{wang-etal-2023-self-instruct}, we use ROUGE-L similarity scoring to maximize instruction diversity. Starting with a randomly selected sample, we iteratively add candidates where the instruction's maximum similarity score with existing instructions is below a specific threshold. We use thresholds of 0.10, 0.11, 0.12, and 0.145 to get subsets of sizes 1K, 2K, 4K, and 8K respectively.
% This approach consistently outperforms random sampling but falls slightly short of LLM-judge Filtering


\paragraph{Analysis.} 
Figure \ref{fig:filtering} shows the effects of data filtering. The results demonstrate clear benefits of filtering strategies. LLM-Judge filtering proves most effective, achieving 42.64\% performance with just 4K samples -- comparable to the full dataset's performance with less than half the data. ROUGE-L similarity filtering performs slightly worse, but still outperforms random sampling, achieving 42.43\% with 8K samples. Although neither filtered dataset leads to higher results than the full dataset, they give comparable results while requiring less training time and computational cost. The results suggest that these filtering strategies successfully identify high-quality samples, though the slight drop in performance indicates that filtered-out responses may still contain valuable training signal.


\begin{figure}[t]
    \centering
    \includegraphics[width=.95\linewidth]{figures/filter.png} \hfill

    \caption{Length Controlled Win-Rate on AlpacaEval 2.0 for Llama-3.1-8B-Instruct finetuned on various subsets of \textsc{REFED}, based on different filtering strategies.
    }
    \label{fig:filtering}
\end{figure}

\subsection{Empirical Efficiency Analysis}
Our method demonstrates significant efficiency advantages in both computational and cost requirements. Using \textsc{Reference-Level Feedback}, we collect feedback from 1K reference samples to synthesize 10K new samples. This means that we collect feedback only 1K times. In contrast, using sample-level feedback would require 11K feedback collections -- 1K for instruction synthesis and 10K for response improvement. The reduction in feedback collection, combined with the strong performance metrics, highlights the advantages of our reference-level approach.

Furthermore, we achieve state-of-the-art results without requiring the most expensive language models. While approaches like \citet{kaur2024instruct} report costs of \$600 to synthesize 4K samples using GPT-4, our experiments synthesize 10K samples for less than \$20 using GPT-4o-mini. Having such a more cost efficient approach, while also achieving better performance, demonstrates that high-quality data synthesis is possible with more economical models.
