\subsection{Study 1: Training}
\label{sec: data collection}
The first study focused on the training phase of \coach to collect training data and evaluate \textbf{Q1}. Forty participants (20 females, 20 males, mean age: 28.5$\pm$4.9 years) completed the \movers and \rescue tasks with a robot teammate, while also providing annotations of their task-relevant intent $(x \in X)$. For \movers, intent is defined as the box a team member plans to pick up or drop next. For \rescue, intent refers to the site a team member plans to approach next.

\subsubsection{Materials and Setup}
We developed a website using the Flask framework~\cite{grinberg2018flask} that included the two tasks, complete with a user interface for task execution and intent labeling (\cref{fig. ui}). This platform enabled participants to perform the experiment remotely. Each participant was paired with a robot teammate, forming a dyadic human-robot team. Following \cref{sec. teamwork model}, behavior of each teammate was modeled as  $\mathcal{H}_j = (X, \pi_j, \zeta_j; \mathcal{M})$. The robot (denoted as $R$) had its policy $\pi_R$ pre-trained using value iteration, and its intent dynamics $\zeta_R$ were manually specified. The experiment aimed to collect data on the human teammate's (denoted as $H$) behavior in order to learn their policy $\pi_H$ and intent dynamics $\zeta_H$. Both teammates had to make decisions under partial observability and infer the intent of their teammate to complete the task successfully.

\subsubsection{Procedure}
\label{sec: data collection procedures}
Upon providing informed consent, participants were introduced to the experiment and completed a demographic survey. They were then instructed to complete the dyadic tasks with the robot, following the same process for both \movers and \rescue. This process included an interactive tutorial and four task trials. The tutorial introduced participants to the task and trained them on how to navigate the user interface (UI). The tutorial featured a guided scenario that mirrored the actual task.
For each domain, participants proceeded through four task trials after completing the tutorial. Each trial was followed by a simplified \textit{after-action review}~\cite{morrison1999foundations, taberski2021visualizing, qian2024measuring}. 
During each trial, the website displayed a task scene and a task control UI, allowing participants to control their character to complete the task (\cref{fig. ui for task}). The experiment collected data on task states $(s)$ and team actions $(a)$ while generating human intent annotations $(x)$. Intent annotations were generated during the task and refined via the after-action reviews, as described in \cref{sec: annotation}. After completing four trials for both the \movers and \rescue tasks, the experiment concluded with a post-experiment survey, where participants provided open-ended feedback about their experience.

\subsubsection{Annotation}
\label{sec: annotation}
Training \coach requires both observable $(s,a)$-trajectories and time series data of team members' intents $(x)$, which are latent and must be manually annotated. In this study, we collected intent data through participant reports, supported by user-centered annotation mechanisms to ensure reliable data collection.
Recall that in both domains, intent is tied to a physical location in the task scene, such as a box or a rescue site. To streamline reporting, we developed a ``Destination Selection'' UI, allowing participants to report their intended destination during task execution (\cref{fig. ui for intent selection}). Potential destinations are highlighted, and participants select their intended location with a mouse click. Participants are encouraged to update their intent when it changes and are prompted if five time steps pass without a report. Selected intents are visually indicated with a flashing red circle. Additionally, key actions like ``Pick Up,'' ``Drop,'' or ``Rescue'' are restricted to the selected destination, ensuring alignment between reported intents and actions. After each task trial, participants use the "after-action review" UI to verify and, if needed, correct their annotations (\cref{fig. ui for post-session review}). This interface replays the task execution, displaying both team actions and selected intents, allowing participants to confirm their reports. If discrepancies are found, participants can adjust incorrect intents using the ``Fix Destination'' button, improving the accuracy of the dataset used to train and validate \coach.

\subsubsection{Data Analysis}
\label{sec. learning team behavior model}
We collected 160 demonstrations per domain and trained \coach using a semi-supervised approach. Recognizing that intent annotation is resource-intensive, we used only 30\% of the intent labels for training and reserving the rest for validation. This approach enables evaluating \coach in a more realistic setting, where only partial intent annotations are available.