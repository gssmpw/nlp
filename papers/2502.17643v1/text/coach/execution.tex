\subsection{Execution Phase}
\label{sec. tic execution}

\subsubsection{Intent Detection}
\coach monitors the team during task execution, identifying potential misalignments in team members' intents and computing timely interventions. This capability is enabled by \tic, a framework that has been experimentally shown to generate task-time interventions that enhance teamwork among AI agents~\cite{seo2023automated}. We extend this framework to develop an AI-enabled coaching system for teams that include human members.
During task execution, \coach can observe team members' states and actions, but their intents (a latent variable) remain unobservable. While \coach leverages a human annotator to obtain partial intent annotations during the training phase, involving a human in the loop during task execution is impractical. Therefore, to infer team members' intents, \coach frames the problem as one of Bayesian filtering. Specifically, given the learned model of team behavior $(\mathcal{H}_j \forall j = 1:n)$ and the partial $(s,a)$-trajectory of the team's task execution, \coach employs the forward-backward algorithm to infer each team member's current intent $\hat{x}$. 

\subsubsection{Intervention Generation}
\coach next uses the inferred intents to assess whether the team is aligned. If the intended plans of the team members are likely to lead to suboptimal task performance, \coach intervenes by weighing the costs and benefits of the intervention. Under the \tic framework, determining this balance requires an intervention strategy, which can be hand-crafted or learned. For \coach, we opt for a learned, value-based strategy to minimize human effort in intervention generation. Specifically,\footnote{Since this computation relies on observations, task model, and the learned model of team behavior, it requires no additional human input or domain-specific knowledge.}
 \begin{itemize}
    \item \coach first computes the expected return $(g)$ conditioned on the inferred intent: $g(\hat{x}|s) = E_{\mathcal{H}}[\sum_t \gamma^t r_t |s, \hat{x}]$.
    \item Next, \coach computes the intent values and return for a hypothetical fully aligned team as $x^* = \arg\max_{x} g(x|s)$ and $g(x^*|s) = E_{\pi, \zeta}[\sum_t \gamma^t r_t | s, x^*]$, respectively. We define the benefit of an intervention as the difference between the optimal and estimated return: $g(x^*|s) - g(\hat{x}|s)$.
    \item Finally, if the benefit of an intervention exceeds its cost $c$ by a pre-defined threshold (i.e., $g(x^*|s) - g(\hat{x}|s) > c + \delta$), then \coach prompts the team to pause, reflect on their plans, and recommends the optimal plan corresponding to $x^*$.
\end{itemize}
Choosing an appropriate cost $(c)$ and threshold $(\delta)$ for interventions is crucial, as unnecessary or incorrect interventions could impair team performance and reduce human trust in, and adoption of, \coach. 
\cref{sec. validation} outlines the approach for selecting these hyperparameters for our implementation and evaluation of \coach.

\subsubsection{Intervention Delivery}
To assist human team members, in addition to generating interventions, Socratic requires effective mechanisms for delivering these instructions. In this work, we utilize an interactive user interface for delivering interventions, as illustrated in \cref{fig. aicoach ui} and detailed in \cref{sec: data collection}. Since human team members can choose whether to accept the AI-generated recommendations, Socratic incorporates a hyperparameter $p_a$, which models the probability of a human accepting its recommendation.
