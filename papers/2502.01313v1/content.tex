\begin{abstract}
    We consider the problem of strategic classification, where a \emph{learner} must build a model to classify \emph{agents} based on features that have been strategically modified.
    Previous work in this area has concentrated on the case when the learner is restricted to deterministic classifiers. In contrast, we perform a theoretical analysis of an extension to this setting that allows the learner to produce a randomised classifier. We show that, under certain conditions, the optimal randomised classifier can achieve better accuracy than the optimal deterministic classifier, but under no conditions can it be worse. When a finite set of training data is available, we show that the excess risk of Strategic Empirical Risk Minimisation over the class of randomised classifiers is bounded in a similar manner as the deterministic case. In both the deterministic and randomised cases, the risk of the classifier produced by the learner converges to that of the corresponding optimal classifier as the volume of available training data grows. Moreover, this convergence happens at the same rate as in the i.i.d. case. Our findings are compared with previous theoretical work analysing the problem of strategic classification. We conclude that randomisation has the potential to alleviate some issues that could be faced in practice without introducing any substantial downsides.
\end{abstract}

\section{Introduction}
Classifiers built with machine learning can play a significant role in a number of resource allocation scenarios; universities determining what students to enrol for the coming year and banks deciding whether or not to give a customer a loan will rely on classification methods to determine the eligibility of candidates \citep{citron2014,milli2019}. In these settings, it is known that candidates can use information about the classifier to strategically alter how they represent themselves to the system, incurring some cost, with the aim of improving their classification. This is known as ``gaming" the classifier. The problem of learning classifiers in the presence of such gaming behaviour, known as Strategic Classification, is a growing area of research.

Strategic Classification models an interaction between a \emph{Leaner}, who chooses and publicly discloses a classifier, and \emph{Agents} who are subject to classification \citep{hardt2016}.\footnote{In the literature the Learner and Agent roles are also referred to as ``Jury" and ``Contestant", respectively \citep{hardt2016}.} The Agents are each independently motivated to be positively classified and, knowing the publicly disclosed classifier, are empowered to alter their representations in order to be classified favourably. The Learner's goal is to choose a classifier that achieves the highest classification accuracy possible, conditioned on this gaming behaviour. Existing work in this area is restricted to the setting where the Learner must select a single classifier from a specified family of classifiers. This puts a heavy constraint on the Learner's options, and limits their ability to counteract the Agents' strategic behaviour.

We argue that, from the modelling point of view, the Learner should instead construct a classifier that incorporates randomness. That is, instead of identifying a single classifier, the Learner should optimise a distribution over classifiers. Under our proposed framework, each Agent would be classified by first sampling a classifier according to the distribution and then using it to make a prediction. Recent work has provided strong evidence indicating that randomisation could improve robustness to strategic behaviours \cite{heredia2023, pinot2020}. A key component of our argument is that the optimal randomised classifier can outperform the optimal deterministic classifier in some cases, but the reverse is never true. The intuition behind this is that when a Learner uses a randomised classifier, the Agents will not know which classifier they should game and therefore what strategy should be employed. Moreover, we show that one does not pay a penalty (in terms of sample complexity) when training randomised classifiers.

In summary, our perspective on the problem and the theoretical analysis provides the following contributions:
\begin{itemize}
    \item We provide a novel formulation for the Strategic Classification problem that allows for the Learner to select a randomised classifier, in the form of a probability distribution over deterministic classifiers.
    \item We identify a small set of sufficient conditions that characterise when one should expect the optimal randomised classifier to outperform the optimal deterministic classifier, as measured by the risk on perturbed data points.
    \item We derive bounds on the excess risk of the Strategic Empirical Risk Minimisation (SERM) introduced by \citet{levanon2021} in the case where it is used on the class of randomised classifiers. The resulting bound demonstrates that the performance of randomised classifiers trained with SERM converges towards the optimal risk at the same rate as the conventional SERM that returns a deterministic classifier.
\end{itemize}

\section{Related Work}
\label{related_work}
Strategic Classification literature primarily builds upon the problem structure and nomenclature established by \cite{hardt2016}. However, earlier works such as \cite{dalvi2004} and \cite{bruckner2011} show that efforts to address the problem predate this. In their work, \citeauthor{hardt2016} established the convention of the Agent with state $\vx \in \mathcal{X}$, for some feature space $\mathcal{X}$, changing their state to $\Delta(\vx)$ defined as:
\begin{equation*}
    \Delta(\vx) := \argmax_{\vz \in \mathcal{X}} \{f(\vz) - c(\vx,\vz)\},
\end{equation*}
where $f: \mathcal{X} \rightarrow \{-1,1\}$, the known classifier, and $c: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}$ the cost the Agent incurs to change their state from $\rvx$ to $\rvz$, are specified by modelling assumptions. In the same work, \citeauthor{hardt2016} proposed an algorithm that could solve this problem, under the assumption of a separable cost function. Subsequent literature has proposed solutions that weaken this assumption (e.g., \cite{miller2020, eilat2022}). Other works propose an alternative formulation which does not explicitly rely on the cost, $c$, but instead introduces the concept of a manipulation graph to define the set of feasible states \cite{zhang2021, lechner2022, lechner2023}. In contrast with these works, this paper generalises the definition of the classifier, $f$, in the model.

\cite{ghalme2021} and \cite{cohen2024} explore variants on the conventional Strategic Classification formulation where the classifier, $f$, is presumed to be unknown to the Agents, and must be inferred. Both instances use distributions to capture the Agents' beliefs about the ``true" classifier; \citeauthor{cohen2024} model the Agents as maintaining a belief over possible classifier definitions. The Analyst can then shape the information they reveal about the classifier to the Agents in order to control their ability to game, with the goal of maximising accuracy. \citeauthor{ghalme2021} instead explore the case where the classifier is not revealed to the Agents, and so they have to approximate it from observation data about the classifier's behaviour. They define a measure, the Price of Opacity (POP), that measures the accuracy cost to the Learner for having the Agents estimate the classifier instead of just revealing it. The authors demonstrate that, under certain assumptions, not revealing classifier definition can result in considerable accuracy losses for the Learner. Unlike in \cite{ghalme2021} and \cite{cohen2024}, where distributions are only used to capture the Agents' uncertainty over the classifier chosen by the Learner, in this work we model the problem such that the distribution is what is chosen by the Learner. Further, in those works all Agents are ultimately classified by the same classifier, whereas in this work each User is classified by a classifier sampled from the chosen distribution.

Both \cite{braverman2020} and \cite{sundaram2023} explore the role of randomisation in improving robustness in Strategic Classification, although it is not the main focus of the latter work. As in this work, the authors consider a case where a distribution over classifiers is constructed (which they treat as a probabilistic classifier), and demonstrate conditions under which a randomised strategy could outperform an optimal deterministic strategy. However, as part of their investigations, the authors make assumptions that limit the degree to which the results transfer to more general settings (specifically they restrict themselves to linear classifiers in at most 2 dimensions). One of our results can be seen as a substantial generalisation of the claims about randomisation made in these previous works. In particular, Section \ref{sec:comparing_optimal_classifiers} of this paper extends the claims made by these previous works such that they can be applied to arbitrary hypothesis classes operating on features of any dimensionality. In contrast to \citet{braverman2020} and \citet{sundaram2023}, we also do not make a specific assumption about the data distribution. Instead, we provide a small set of sufficient conditions that must hold.

PAC Learning methods \citep{valiant1984} can be used to produce bounds on how well a classifier trained on a fixed dataset would be expected to generalise to the whole population distribution from which the dataset was sampled. \citet{zhang2021, sundaram2023, cullina2018} are examples of just a few Strategic Classification papers that have used PAC Learning methods to establish such bounds. The key difference between our work and these prior works is that we focus on the novel setting where the Learner selects a distribution over classifiers, rather than a single deterministic classifier. In the case when a distribution over hypotheses is being learned, these conventional PAC-Learning tools can't be applied. As a consequence, we derive new results that allow us to quantify the rate at which the performance of models trained using SERM converge towards the optimal risk.

\section{Strategic Classification with Randomisation}
\label{sec:strategic_classification}
Throughout this paper we will use $\gP(\gA)$ to denote the set of probability measures over some measurable space, $\gA$. Given a data distribution $\gD \in \gP(\gX\times \gY)$, where $\gX$ is a feature space and $\gY = \{-1,1\}$, and a family of classifiers, $\gF$, that map from $\gX$ to $\gY$, the goal in the i.i.d. learning setting is to identify a function $f \in \gF$ that minimises the risk,
\begin{equation}
    R(f) = \E_{(\rvx, \ry) \sim \gD}\left[l(f(\rvx), \ry)\right],
    \label{eqn:risk}
\end{equation}
induced by some loss function $l: \R \times \{-1,1\} \rightarrow \mathbb{R}^+$. The distribution, $\gD$, is typically assumed to be unknown, so the choice of classifier, $f$, is determined through the use of a training set $S = \{(\rvx_{i}, \ry_{i})\}_{i=1}^{n}$, where $(\rvx_{i}, \ry_{i})$ are i.i.d. samples from $\gD$. This set is used to define the empirical risk,
\begin{equation}
    r(f) := \frac{1}{n}\sum_{i=1}^{n}l(f(\rvx_{i}), \ry_{i}).
    \label{eqn:empirical_risk}
\end{equation}
Unless stated otherwise, in this work we choose $l$ to be the zero--one error,
\begin{equation*}
    l(\hat{y}, y) = \ind[\hat{y} \neq y],
\end{equation*}
where $\ind$ is the indicator function that evaluates to one if the argument is true and zero otherwise.

The strategic classification problem \citep{hardt2016} differs from the conventional i.i.d. learning setting in that the distribution of data observed when training the classifiers in $\mathcal{F}$ is different from the distribution encountered at test time. In particular, it is assumed that associated with each data point is an agent that will strategically modify features according to some cost model in order to obtain a positive classification. This interaction is modelled as a Stackelberg Game between a Learner player and an unknown number of Agent players, with the Learner as the leader \citep{stackelberg1934}. The Learner player chooses a classifier, $f$, to classify the Agents. The Agents observe $f$ and, in response, attempt to ``game'' the classifier by independently perturbing their features, $\Delta_{f}(x)$, with the aim of being classified as the positive class. Concretely, the Agents optimise a utility,
\begin{equation}
    \Delta_{f}(\vx) \in \text{BR}(f) := \argmax_{\vz \in \mathcal{X}} f(\vz) - c(\vx,\vz),
    \label{eqn:strategic_response}
\end{equation}
where $\text{BR}(f)$ denotes the set of best responses to $f$ that the Agent might play, and $c: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}^{+}$ is a non-negative function quantifying the cost incurred by the Agent to alter their features. As is typical in the literature, we assume the positive classification is the desired outcome for all Agents and that all Agents use the same cost function, which is also typically assumed to be known to the Learner. Agents are modelled as being rational, so if the Agent is already positively classified ($f(\vx)=1$), then $\Delta_{f}(\vx) = \vx$.

As in the standard i.i.d learning problem, the goal is to identify a classifier, $f \in \gF$, that minimises the strategic risk over an unknown data distribution, $\gD$. Given the Agents' gaming strategy, $\Delta$, the strategic risk is defined as
\begin{equation}
    R_{\Delta}(f) = \E_{(\rvx,\ry) \sim \mathcal{D}}[l(f(\Delta(\rvx)), \ry)],
    \label{eqn:strategic_risk}
\end{equation}
and the empirical strategic risk on the training set, $S$, is given by
\begin{equation}
    r_{\Delta}(f) = \frac{1}{n}\sum_{i=1}^{n}l(f(\Delta(\rvx_{i})),\ry_{i}).
    \label{eqn:empirical_strategic_risk}
\end{equation}
The idealised objective for the Learner is therefore to solve a bi-level optimisation problem,
\begin{equation}
    f^\ast = \argmin_{f \in \gF} R_{\Delta_f} (f),
\end{equation}
where the lower level of the problem arises from the definition of $\Delta_f$. Conventional approaches to this problem approximate the solution of this via a variant of empirical risk minimisation that takes into account the bi-level structure of the optimisation problem \citep{hardt2016, levanon2021, levanon2022}. This idea has become known as Strategic Empirical Risk Minimisation (SERM) \citep{levanon2021}, and we denote the model obtained via this method by
\begin{equation}
    \hat{f} = \argmin_{f \in \gF} r_{\Delta_f}(f).
\end{equation}

\subsection{Generalising to Randomised Classifiers}
\label{sec:mixed_strategies_and_cogameability}
In the conventional strategic classification problem formulation, the Learner commits to using a single classifier from $\gF$ to make all predictions at test time. We propose that the Learner instead commit to a distribution over classifiers, $Q \in \gP(\gF)$. When classifying each Agent's features at test time, the Learner samples a classifier according to this distribution and then uses this classifier to make a prediction. Crucially, a new classifier will be sampled each time a prediction is to be made. This type of randomised classifier is sometimes known as a Gibbs classifier in the machine learning community (e.g., \citet{ng2001convergence}). We note that $Q$ can be chosen to be a point mass on an individual $f$ in order to select a deterministic classifier. In this sense, our problem formulation is a strict generalisation of the conventional strategic learning problem.

As a result of the uncertainty in the classification outcome introduced by the randomisation in this formulation, the Agents' objective is revised to optimise the expected utility\footnote{See, e.g., \citet{berger2013} or \citet{maschler2020} for discussions on why this is justified.},
\begin{equation}
    \Delta_Q(\vx) = \argmax_{\vz \in \gX} \E_{f \sim Q}[f(\vz)] - c(\vx, \vz).
\end{equation}
The strategic risk and its empirical counterpart are therefore generalised to
\begin{equation}
    R_\Delta(Q) = \E_{f \sim Q} \E_{(\rvx, \ry) \sim D}[l(f(\Delta(\rvx)), \ry)]
    \label{eqn:randomised_strategic_risk}
\end{equation}
and
\begin{equation}
    r_\Delta(Q) = \E_{f \sim Q} \left[ \frac{1}{n} \sum_{i=1}^n l(f(\Delta(\rvx_i)), \ry_i) \right],
\end{equation}
respectively, and the optimal randomised classifier, $Q^{\ast}$ solves
\begin{equation}
    Q^{\ast} = \argmin_{Q \in \gP(\gF)} R_{\Delta_{Q}}(Q).
\end{equation}
Similar to the deterministic case, we can also define the SERM solution for the randomised classifier setting,
\begin{equation}
    \hat{Q} = \argmin_{Q \in \gP(\gF)} r_{\Delta_Q}(Q).
\end{equation}
We note here that the optimal randomised classifier, as we have defined it, can assign all of the probability mass to a single element of $\gF$---including the optimal deterministic classifier. This means that the optimal randomised classifier can never perform worse than the optimal deterministic classifier.

\section{Comparing Optimal Classifiers}
\label{sec:comparing_optimal_classifiers}
We begin by analysing the simplified problem of determining when the optimal randomised classifier could outperform the optimal deterministic classifier. This allows us to avoid additional complications that can arise from the imperfect information situation encountered when learning from a finite dataset. Our goal is to identify a small number of sufficient conditions that could plausibly arise in a real problem and that lead to the optimal randomised classifier provably outperforming the optimal deterministic classifier. 

\subsection{Sufficient Conditions}
The standard strategic classification setting assumes that there exists some classifier, $h \in \mathcal{F}$, according to which labels are generated using the clean data points $\vx \in \mathcal{X}$ \citep{hardt2016}. If $h$ is also incentive compatible (i.e, $\forall \vx \in \supp(\gD), h(\Delta_h(\vx)) = h(\vx)$), then $h = f^\ast$. In this situation it is possible that a learning rule mapping training sets to deterministic classifiers in $\gF$ can be optimal, because $f^\ast$ is in the codomain and achieves a strategic risk of zero. As such, the first condition we identify for the optimal randomised classifier to strictly improve upon $f^\ast$ is that $f^\ast$ must have non-zero strategic risk.

The second condition we identify relates to the non-uniqueness of $f^\ast$. We therefore define $\mathcal{F}^{\ast}$ to be the set of $f \in \mathcal{F}$ that are optimal with respect to the strategic risk,
\begin{equation}
    \mathcal{F}^{\ast} = \argmin_{f \in \mathcal{F}} R_{\Delta_{f}}(f).
\end{equation}
For convenience, we will refer to the optimal strategic risk as $R_{\Delta}^{\ast}$, rather than selecting a specific element of $f^\ast \in \gF^\ast$ and computing $R_{\Delta_{f^\ast}}(f^\ast)$.

We are now able to state the second condition for the optimal randomised classifier to outperform $f^{\ast}$; there must exist at least two deterministic classifiers, $f, f^\prime \in \gF^\ast$, that achieve optimal strategic risk, but whose corresponding sets of best responses are disjoint, $BR(f) \cap BR(f^\prime) = \emptyset$. The intuition underlying this condition is that there must be a set of Agents (with non-zero measure) who cannot simultaneously game both classifiers. This is made more clear by observing that some $\Delta_f \in BR(f)$ would only be absent from $BR(f^\prime)$ in the case when
\begin{equation}
    R_{\Delta_f}(f^\prime) < R_{\Delta_{f^\prime}}(f^\prime),
\end{equation}
where $\Delta_{f^\prime} \in BR(f^\prime)$.

With these sufficient conditions identified, we now provide the first of our main results below.

\begin{theorem}
    If $R_\Delta^\ast > 0$ and there exists $f, f' \in \mathcal{F}^{\ast}$ such that $\text{BR}(f)$ and $\text{BR}(f')$ are disjoint, then we have that
    \begin{equation*}
        R_{\Delta_{Q^\ast}}(Q^\ast) < R_{\Delta}^\ast.
    \end{equation*}
    \label{thm:sufficient-conditions}
\end{theorem}

Before providing the proof for our theorem, it is useful to consider at an intuitive level why randomisation could be useful given the sufficient conditions we have identified; in essence, it allows the Learner to deter gaming behaviour by utilising different classifiers that force some subset of the Agents to have to choose which ones to game. If the Learner randomly selects which classifier to use to make each prediction, this means the Agents that cannot simultaneously game all classifiers will either commit to game only a subset of them, or decide that the cost of gaming only a subset outweighs the risk of failing to game the right one.

\begin{proof}[Proof of Theorem \ref{thm:sufficient-conditions}]
    It suffices to show that
    \begin{equation}
        \exists Q \in \mathcal{P}(\mathcal{F}), R_{\Delta_{Q}}(Q) < R_{\Delta_{f^{\ast}}}(f^{\ast}).
    \end{equation}
    Then, by the optimality of $Q^{\ast}$, the desired condition holds.
    
    Our proof strategy is to explicitly construct such a $Q$; we choose the uniform distribution over $\{f, f'\}$,
    \begin{equation}
        Q = U(\{f, f^\prime\}),
    \end{equation}
    
    Begin by considering $\Delta_{Q}$, a best response to $Q$. From the assumption that $\text{BR}(f)$ and $\text{BR}(f^\prime)$ are disjoint, it is not possible for $\Delta_{Q}$ to simultaneously be a member of $BR(f)$ and $BR(f')$. This means $\Delta_{Q}$ is either in one of these sets and not the other, or in neither set. We consider these two cases separately.
    
    We begin with the case that $\Delta_Q$ appears in exactly one of $\text{BR}(f)$ and $\text{BR}(f^\prime)$. Without loss of generality, assume $\Delta_{Q} \in \text{BR}(f)$ (and $\Delta_{Q} \notin \text{BR}(f^\prime)$). Therefore, from the definition of the strategic risk and our choice of $Q$, we have that
    \begin{equation}
        \begin{split}
            R_{\Delta_{Q}}(Q) &= \dfrac{1}{2}(R_{\Delta_{Q}}(f) + R_{\Delta_{Q}}(f'))\\
            &= \dfrac{1}{2}(R_{\Delta_{f}}(f) + R_{\Delta_{Q}}(f')),
        \end{split}
    \end{equation}
    since $\Delta_{Q} \in \text{BR}(f)$. However, by the definition of the best response, $R_{\Delta_{f'}}(f') = \max_{f \in \mathcal{F}}R_{\Delta_{f}}(f') = R_{\Delta}^{\ast}$. Therefore it follows that $R_{\Delta_{Q}}(f') < R_{\Delta_{f'}}(f')$ and
    \begin{equation}
        \begin{split}
            R_{\Delta_{Q}} &< \dfrac{1}{2}(R_{\Delta_{f}}(f) + R_{\Delta_{f'}}(f'))\\
            &= \dfrac{1}{2}(R_{\Delta}^{\ast} + R_{\Delta}^{\ast})\\
            &= R_{\Delta}^{\ast}.
        \end{split}
    \end{equation}
    We note that if $\Delta_Q$ was actually in $BR(f^\prime)$, one can simply repeat this argument with the roles of $f$ and $f^\prime$ reversed.
        
    Next we consider the case where $\Delta_{Q}$ is not in either of the best response sets; i.e., $\Delta_{Q} \notin \text{BR}(f)$ and $\Delta_{Q} \notin \text{BR}(f')$). In this case we have
    \begin{equation}
        \begin{split}
            R_{\Delta_{Q}}(Q) &= \dfrac{1}{2}(R_{\Delta_{Q}}(f) + R_{\Delta_{Q}}(f'))\\
            &< \dfrac{1}{2}(R_{\Delta_{f}}(f) + R_{\Delta_{f'}}(f'))\\
            &= R_{\Delta}^{*}.
        \end{split}
    \end{equation}
    
    Therefore, in all possible cases, $R_{\Delta_{Q}}(Q)<R_{\Delta}^{\ast}$.
\end{proof}

\subsection{When are the Sufficient Conditions Satisfied?}
\label{sec:when_sufficient}
Previous works exploring randomised classifiers in the context of Strategic Classification relied on overly conservative conditions that constrained the applicability of their results \citep{braverman2020, sundaram2023}. Namely, they have constructed specific problem instances for linear classifiers in $1$ and $2$ dimensions, respectively. In contrast, our analysis has shown that an optimal randomised classifier can outperform an optimal deterministic classifier under a minimal set of sufficient conditions. In particular, we make no assumption on the type of decision employed by the classifier or the topology of the space the features lie in. This broadens the space of problems randomised classifiers could potentially be applied to significantly compared to the conditions explored in prior work.

%Randomisation as a social good? Fairness? 
It is well known that Strategic Classification can motivate the development of classifiers that disadvantage people who do not want to game, or whose circumstances do not allow them to \cite{milli2019, hu2019}. This can arise where a Learner must choose between deploying a zero-risk classifier which is not incentive compatible (and so is vulnerable to gaming), and a classifier that has non-zero risk but is incentive compatible. Deploying the latter would result in Agents having no incentive to game, but the Learner would also be knowingly misclassifying some Agents in order to prevent the gaming behaviour. However, deploying the former effectively obliges Agents to consider gaming. In the case where the classifiers have disjoint best responses, Theorem \ref{thm:sufficient-conditions} suggests that randomisation over the these classifiers could effectively disincentivise gaming without sacrificing performance.

It is worth considering when the two sufficient conditions outlined previously might hold in practice. The first condition---the optimal risk being non-zero---is a common occurrence even for the standard i.i.d. learning setting. There are two main causes for this: (i) the chosen hypothesis class does not contain decision boundaries of the correct shape (e.g., linear classifiers require linearly separable data); and (ii) the information in the features does not fully determine the label. We argue that the second condition---where there are multiple classifiers that achieve the optimal strategic risk---is not unrealistic. If there is redundancy in the feature space, one might expect that different optimal classifiers will leverage different subsets of features. In this case, modifying features in one subset will game one classifier but not the other. Modifying features in both subsets would result in the Agent incurring a higher cost.

\section{Generalisation of Randomised Classifiers}
In the previous section we have shown that the optimal randomised classifier solution to a strategic classification problem can outperform the optimal deterministic solution. However, the question of whether it is practical to learn a randomised classifier from a finite amount of data remains unanswered.

To address this question we will demonstrate that the gap in performance between the randomised classifier solution realised by SERM, $\hat{Q}$, and the optimal randomised classifier, $Q^{\ast}$, can be upper bounded in a similar manner to the deterministic case. This implies that the risk of a randomised classifier converges to that of the optimal randomised classifier as the data volume grows, making learning over this space viable from a statistical point of view.

\subsection{Notation}
Before proceeding with our arguments we will need to introduce the following notation; in a minor abuse of notation, let us define the set of classifiers in $\mathcal{F}$ composed with the loss function, $l$, as
\begin{equation}
    \gF^l = l \circ \gF = \{(\vx, y) \mapsto l(f(\vx),y): f \in \gF\}.
\end{equation}
We can further extend this definition to be composed with a response function, $\Delta$, as
\begin{equation}
    \gF^l_{\Delta} = \gF^l \circ \Delta = \{(\vx,y) \mapsto f^l(\Delta(\vx), y): f^l \in \gF^l \}.
\end{equation}

We denote the loss class of randomised classifiers defined in terms of distributions over $\gF$ as
\begin{equation}
    \tilde{\gF}^l = \left\{ (\vx, y) \mapsto \E_{f \sim Q}[l(f(\vx), y)] : Q \in \gP(\gF) \right\}.
\end{equation}

Finally, we introduce a standard measure used in the literature when bounding generalisation; the Rademacher Complexity.

\begin{definition}[Rademacher Complexity \citep{bartlett2002rademacher, shalev-shwartz2014}]
The Rademacher Complexity of a class $\gG$ on a sample of $n$ independent random variables distributed according to $\gD$ is defined as
\begin{equation*}
    \Rad_n(\gG) = \E_{\rvz_{1:n} \sim \gD^n} \E_{\rvsigma}\left [\sup_{g \in \gG} \frac{1}{n} \sum_{i=1}^{n} \ervsigma_{i} g(\rvz_i) \right],
\end{equation*}
where $\rvsigma$ is a vector of independent Rademacher random variables, $\Pr(\ervsigma_i = 1) = \Pr(\ervsigma_i = -1) = \frac{1}{2}$.
\end{definition}
When $\gG$ is a loss class, such as $\gF^l$, then each $\rvz_i$ will be a tuple, $(\rvx_i, \ry_i)$. Whereas, when $\gG$ represents only a hypothesis class, such as $\gF$, then one should understand that $\rvz_i = \rvx_i$.

We will also make use of the standard Rademacher complexity-based bound on the generalisation gap, that was also proposed by \citet{bartlett2002rademacher, shalev-shwartz2014}.

\begin{theorem}
    For a loss class, $\gF^l$, the expected worst-case difference between the empirical risk and population risk is bounded as
    \begin{equation*}
        \E_{S \sim \gD^n} \left[ \sup_{f \in \mathcal{F}^l}R(f) - r(f)\right] \leq 2\Rad_n(\gF^l).
    \end{equation*}
    Moreover, with probability at least $1-\delta$, we have
    \begin{equation*}
        \sup_{f \in \mathcal{F}^l}R(f) - r(f) \leq 2\Rad_n(\gF^l) + \sqrt{\frac{\ln(1/\delta)}{2n}}
    \end{equation*}
    \label{thm:rademacher-iid}
\end{theorem}
We note that this theorem also holds for randomised classes and classes composed with a response function, $\Delta$.

\newcommand{\qhs}{\hat{Q}}
\newcommand{\qhsbr}{\Delta_{\hat{Q}}}
\newcommand{\qa}{Q^\ast}
\newcommand{\qabr}{\Delta_{Q^\ast}}
\newcommand{\qbr}{\Delta_{Q}}

\subsection{Excess Risk of SERM for Randomised Classifiers}
Our main result demonstrating how fast the strategic risk of SERM on the randomised class converges towards the optimum value is given below.

\begin{theorem}
    If $\hat{Q} \in \mathcal{P}(\mathcal{F})$ minimises $r_{\Delta_{\hat{Q}}}(\hat{Q})$, and $Q^{\ast} \in \mathcal{P}(\mathcal{F})$ minimises $R_{\Delta_{Q^{\ast}}}(Q^{\ast})$. Then we have
    \begin{equation*}
        \E_{S \sim \mathcal{D}^{n}} [R_{\Delta_{\hat{Q}}}(\hat{Q}) - R_{\Delta_{Q^\ast}}(Q^\ast)] \leq \sup_{Q \in \gP(\gF)} 2 \Rad_{n}(\gF^{l}_{\Delta_Q}).
    \end{equation*}
    Moreover, with probability at least $1-\delta$, we also have
    \begin{equation}
        R_{\Delta_{\hat{Q}}}(\hat{Q}) - R_{\Delta_{Q^\ast}}(Q^\ast) \leq \sup_{Q \in \gP(\gF)} 2 \Rad_{n}(\gF^{l}_{\Delta_Q}) + \sqrt{\frac{\ln(1/\delta)}{2n}}.
    \end{equation}
    \label{thm:excess-risk}
\end{theorem}

There are several interesting observations we make about this result. The first is that the excess risk of \emph{randomised} classifiers can be bounded in terms of Rademacher complexity of the corresponding class of \emph{deterministic} classifiers. This allows existing analysis of classes of deterministic classifiers to be reused without modification. The second is that the leading constant factor of $2$ is the same for this setting as in the deterministic i.i.d. setting. This is despite the additional complexity of the strategic classification problem and the inclusion of randomisation.

We provide two lemmas that will be useful in the course of proving Theorem \ref{thm:excess-risk}.


The first lemma we make use of allows us to take advantage of our specific conditions to exchange an expectation and supremum.

\begin{lemma}
   For a fixed $Q^\prime \in \mathcal{P}(\mathcal{F})$
   \begin{equation}
       \begin{split}
           \E_{S \sim \mathcal{D}^{n}} \left[ \sup_{Q \in \gP(\gF)} R_{\qbr}(Q^\prime) - r_{\qbr}(Q^\prime) \right] =\\
           \sup_{Q \in \gP(\gF)} \E_{S \sim \mathcal{D}^{n}} \left[ R_{\qbr}(Q^\prime) - r_{\qbr}(Q^\prime) \right].
       \end{split}
   \end{equation}
   \label{lemma:sup_exp_swap}
\end{lemma}

\begin{proof}[Proof of Lemma \ref{lemma:sup_exp_swap}]
   For fixed $Q^\prime$, let $g(Q,S) = R_{\qbr}(Q^\prime) - r_{\qbr}(Q^\prime)$. From the definition of $R_{\Delta_{Q}}$ and $r_{\Delta_{Q}}$, it can be concluded that $g$ is a bounded and measurable function.
   It is already known that
   \begin{equation}
    \sup_{Q \in \gP(\gF)} \E_{S \sim \mathcal{D}^{n}} \left[ g(Q,S) \right] \leq \E_{S \sim \mathcal{D}^{n}} \left[ \sup_{Q \in \gP(\gF)} g(Q,S) \right].
    \end{equation}
    We will prove equality by demonstrating that the opposite inequality is also true. That is,
   \begin{equation}
       \E_{S \sim \mathcal{D}^{n}} \left[ \sup_{Q \in \gP(\gF)} g(Q,S) \right] \leq \sup_{Q \in \gP(\gF)} \E_{S \sim \mathcal{D}^{n}} \left[ g(Q,S) \right]
   \end{equation}
   By the definition of the best response, for fixed $Q^\prime \in \mathcal{P}(\mathcal{F})$ there exists $Q^{\ast} \in \mathcal{P}(\mathcal{F})$ such that $g(Q,S) \leq g(Q^{\ast},S), \; \forall S \subset (\mathcal{X} \times \mathcal{Y})^n, \;\forall Q \in \mathcal{P}(\mathcal{F})$. Therefore,
   \begin{equation}
       \sup_{Q \in \gP(\gF)} g(Q,S) = g(Q^{\ast},S) 
   \end{equation}
   and, as a result of this it follows that
   \begin{equation}
       \begin{split}
       \sup_{Q \in \gP(\gF)} \E_{S \sim \mathcal{D}^{n}} \left[ g(Q,S) \right] &\geq \E_{S \sim \mathcal{D}^{n}} \left[ g(Q^{\ast},S) \right] \\
       &= \E_{S \sim \mathcal{D}^{n}} \left[ \sup_{Q \in \gP(\gF)} g(Q,S) \right]
       \end{split}
   \end{equation}
   as required.
\end{proof}

 The second lemma allows us to reason about the Rademacher complexity of the class of deterministic classifiers rather than the class of randomised classifiers.

\begin{lemma}
    For a fixed $\Delta : \gX \to \gX$, we have that
    \begin{equation*}
        \Rad_n(\tilde{\gF}^l_\Delta) = \Rad_n(\gF^l_\Delta).
    \end{equation*}
    \label{lemma:convex_hull_complexity_bound}
\end{lemma}

\begin{proof}[Proof of Lemma \ref{lemma:convex_hull_complexity_bound}]
We prove the equality by showing that both
\begin{equation}
    \Rad_n(\tilde{\gF}^l_\Delta) \leq \Rad_n(\gF^l_\Delta)
\end{equation}
and
\begin{equation}
    \Rad_n(\gF^l_\Delta) \leq \Rad_n(\tilde{\gF}^l_\Delta)
\end{equation}
are true.

We obtain the first inequality via
\begin{equation}
    \begin{split}
        &n\Rad_{n}(\tilde{\gF}^{l}_{\Delta}) \\ &=\E_{\rvz_{1:n}}\E_{\rvsigma} \left[ \sup_{Q \in \mathcal{P}(\mathcal{F})} \sum_{i=1}^{n} \sigma_{i} \E_{f \sim Q}\left[ l(f(\Delta(\rvz_{i})) \right]\right]\\
        &= \E_{\rvz_{1:n}}\E_{\sigma} \left[ \sup_{Q} \E_{f \sim Q} \left[ \sum_{i=1}^{n} \sigma_{i} l(f(\Delta(\rvx_{i}), \ry_i) \right] \right]\\
        &\leq \E_{\rvz_{1:n}}\E_{\sigma} \left[ \sup_{Q} \E_{f \sim Q} \left[ \sup_{f^\prime \in \gF} \sum_{i=1}^{n} \sigma_{i} l(f^\prime(\Delta(\rvx_{i}), \ry_i) \right] \right]\\
        &= \E_{\rvz_{1:n}}\E_{\sigma} \left[ \sup_{f \in \mathcal{F}} \sum_{i=1}^{n}\sigma_{i} l(f(\Delta(\rvx_{i}), \ry_i) \right]\\
        &= n\Rad_{n}(\mathcal{F}^{l}_{\Delta}).
    \end{split}
\end{equation}
The second inequality follows from $\gF^l_\Delta \subseteq \tilde{\gF}^l_\Delta$, because the latter contains a point mass distribution associated with each element of $\gF^l_\Delta$, and $A \subseteq B \implies \Rad_{n}(A) \leq \Rad_{n}(B)$ \citep{bartlett2002rademacher}.
\end{proof}

We now prove Theorem \ref{thm:excess-risk}.

\begin{proof}[Proof of Theorem \ref{thm:excess-risk}]
    We begin by expanding out the excess risk term by introducing $r_{\Delta_{\hat{Q}}}(\hat{Q})$ and using the independence of $Q^{\ast}$ from $S$, and to rewrite it as
    \begin{equation}
        \begin{split}
            &\E_{S \sim \mathcal{D}^{n}} \left[ R_{\qhsbr}(\qhs) - R_{\qabr}(\qa) \right] \\
            &= \E_{S \sim \mathcal{D}^{n}} \left[ R_{\qhsbr}(\qhs) - r_{\qhsbr}(\qhs) + r_{\qhsbr}(\qhs) - R_{\qabr}(\qa) \right] \\
            &= \E_{S \sim \mathcal{D}^{n}} \left[ R_{\qhsbr}(\qhs) - r_{\qhsbr}(\qhs) + r_{\qhsbr}(\qhs) - r_{\qabr}(\qa) \right].
        \end{split}
    \end{equation}
    Next we observe that, since $\hat{Q}$ is a minimiser for the empirical strategic risk, we have that
    \begin{equation}
        \forall Q \in \gP(\gF), \;r_{\Delta_{\hat{Q}}}(\hat{Q}) \leq r_{\Delta_{Q}}(Q).
    \end{equation}
    This tells us that $r_{\qhsbr}(\qhs) - r_{\qabr}(\qa) \leq 0$. We can upper bound the remaining terms with a response, $\Delta_{Q}$, that induces the largest generalisation gap,
    \begin{equation}
        \begin{split}
            &\E_{S \sim \mathcal{D}^{n}} \left[ R_{\qhsbr}(\qhs) - r_{\qhsbr}(\qhs) \right] \\
            &\leq \E_{S \sim \mathcal{D}^{n}} \left[ \sup_{Q \in \gP(\gF)} R_{\qbr}(\qhs) - r_{\qbr}(\qhs) \right] \\
            &= \sup_{Q \in \gP(\gF)} \E_{S \sim \mathcal{D}^{n}} \left[ R_{\qbr}(\qhs) - r_{\qbr}(\qhs) \right] \\
            &\leq \sup_{Q \in \gP(\gF)} 2\Rad_n(\tilde{\gF}^l_{\Delta_Q}) \\
            &= \sup_{Q \in \gP(\gF)} 2\Rad_n(\gF^l_{\Delta_Q}),
        \end{split}
    \end{equation}
    where the first equality is due to Lemma \ref{lemma:sup_exp_swap}, the second inequality is due to Theorem \ref{thm:rademacher-iid}, and the final equality is due to Lemma \ref{lemma:convex_hull_complexity_bound}.
\end{proof}

We note that using similar argumentation as this proof (without Lemma \ref{lemma:convex_hull_complexity_bound}) gives an analogous result for the deterministic case.

\begin{theorem}
    If $\hat{f} \in \mathcal{F}$ minimises $r_{\Delta_{\hat{f}}}(\hat{f})$, and $f^{\ast} \in \mathcal{F}$ minimises $R_{\Delta_{f^{\ast}}}(f^{\ast})$. Then we have
    \begin{equation*}
        \E_{S \sim \mathcal{D}^{n}} [R_{\Delta_{\hat{f}}}(\hat{f}) - R_{\Delta_{f^\ast}}(f^\ast)] \leq \sup_{f \in \gF} 2 \Rad_{n}(\gF^{l}_{\Delta_f}).
    \end{equation*}
    Moreover, with probability at least $1-\delta$, we also have
    \begin{equation}
        R_{\Delta_{\hat{f}}}(\hat{f}) - R_{\Delta_{f^\ast}}(f^\ast) \leq \sup_{f \in \gF} 2 \Rad_{n}(\gF^{l}_{\Delta_f}) + \sqrt{\frac{\ln(1/\delta)}{2n}}.
    \end{equation}
    \label{thm:excess-risk-deterministic}
\end{theorem}

\subsection{Comparison with Prior Work}
We compare our results with two other works analysing the strategic classification problem. The work of \citet{sundaram2023} provides a generalisation of the VC dimension that can be used to bound the excess risk of SERM on a deterministic class of classifiers. We restate their result below in a form that is amenable to comparison with our Theorem \ref{thm:excess-risk}.

\begin{theorem}[\citet{sundaram2023}]
With probability at least $1-\delta$, the solution of SERM on $\gF$ satisfies
\begin{equation*}
    R_{\Delta_{\hat{f}}}(\hat{f}) - r_{\Delta_{\hat{f}}}(\hat{f}) \leq C \sqrt{\frac{d + \ln(1/\delta)}{n}},
\end{equation*}
where $d$ is the Strategic VC dimension of the class, $\gF$, and $C$ is an absolute constant.
\end{theorem}
They note that, in the case of linear classifiers applied in the classic strategic learning setting, the original VC dimension is an upper bound for the Strategic VC dimension. Consider the right-hand side of the first part of Theorem \ref{thm:excess-risk},
\begin{equation}
    \sup_{Q} \Rad_n(\gF_{\Delta_Q}^l).
\end{equation}
We can interpret the composition of $\gF$ with $\Delta_Q$ applied to data from $\gD$ as applying some $f \in \gF$ to some new distribution defined as the pushforward of $\gD$ by $\Delta_Q$. This implies that the above complexity is actually just a Rademacher complexity defined on a different data distribution. This allows us to use a fairly standard argument (see, e.g., Corollary 3.8 then Corollary 3.19 of \citet{mohri2018foundations}) to say that the above quantity is bounded by
\begin{equation}
    \sqrt{\frac{2d \ln(en/d)}{n}},
\end{equation}
where $d$ is the VC dimension.

The other work we compare with is the (corrected) strategic hinge loss bound for linear classifiers, originally proposed by \citet{levanon2022} and then fixed by \citet{rosenfeld2023}. For a class of linear classifiers parameterised by $B$,
\begin{equation*}
    \gG_B = \{\vx \mapsto \vw^T \vx : \|\vw\| \leq B \},
\end{equation*}
they provide the guarantee below.
\begin{theorem}[\citet{rosenfeld2023}]
    \label{thm:rosenfeld}
    With probability at least $1-\delta$, for all $g \in \gG$ we have
    \begin{equation*}
        R_{\Delta_{g}}(g) \leq r_{s-hinge}^c(g) + \frac{B(4X + u_\ast) + 3 \sqrt{\ln(1/\delta)}}{\sqrt{n}},
    \end{equation*}
    where $\forall \vx \in \gX, \|\vx\| \leq X$ and $u_\ast$ is a non-negative quantity derived from the Agents' cost function.
\end{theorem}
\citet{rosenfeld2023} also show that the strategic hinge loss upper bounds the zero-one loss. By way of comparison, we provide the following corollary of our result for deterministic classifiers (Theorem \ref{thm:excess-risk-deterministic}).
\begin{corollary}
    If $\hat{g}$ is the SERM solution for $\gG$, then we have with probability at least $1-\delta$ that
    \begin{equation*}
        R_{\Delta_{\hat{g}}}(\hat{g}) \leq r_{s-hinge}^c(\hat{g}) + \frac{4XB + \sqrt{\ln(1/\delta)}}{2\sqrt{n}}.
    \end{equation*}
\end{corollary}
\begin{proof}
    The result follow from applying Theorem \ref{thm:excess-risk-deterministic}, upper bounding the Rademacher complexity with the usual bound for linear classes (see, e.g., \citet{shalev-shwartz2014}), moving the empirical strategic risk to the right-hand side, and finally upper bounding it by the strategic hinge loss.
\end{proof}
The main improvement compared to Theorem \ref{thm:rosenfeld} is that we lack the dependence on $Bu_\ast$. The other differences are due to using slightly different variants of the standard Rademacher complexity tools.

\section{Conclusions}
Randomised classifiers can be more robust to gaming than deterministic approaches, and have the potential to achieve lower strategic risk. In this work we proposed a novel formulation of the strategic classification problem that admits randomised classifier solutions, and identified a minimal set of conditions which are sufficient to for optimal randomised classifier solutions to outperform optimal deterministic solutions. We investigated this problem setting from a statistical point of view and determined that the data requirements for reliably fitting models are comparable to learning a deterministic model in the i.i.d. setting.

We highlight that, while a motivation of this work was to promote the adoption of randomised classifier-based solutions in settings that are vulnerable to gaming behaviours, existing regulations in certain areas of application may limit the degree to which this adoptions can occur. While the evolution and development of such regulations is beyond the scope of this work, our results provide evidence that would it may be required to rethink these regulations in order find an acceptable compromise between those using machine learning models and those that must interact with them.

\section*{Acknowledgements}
%Acknowledgements here: NWG and RAEng
This work was funded by NatWest Group via the Centre for Purpose-Driven Innovation in Banking. This project was supported by the Royal Academy of Engineering under the Research Fellowship programme.

\section*{Impact Statement}
The theoretical nature of our work means there is unlikely to be any direct societal impact from our work. However, the topic we investigate does relate to the interface between society and machine learning models. As mentioned in Section \ref{sec:when_sufficient}, a practical realisation of the ideas we discuss could lead to fairer machine learning systems; they would be more resilient to gaming behaviour without having to sacrifice accuracy.

\bibliography{bibliography}