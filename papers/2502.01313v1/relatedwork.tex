\section{Related Work}
\label{related_work}
Strategic Classification literature primarily builds upon the problem structure and nomenclature established by \cite{hardt2016}. However, earlier works such as \cite{dalvi2004} and \cite{bruckner2011} show that efforts to address the problem predate this. In their work, \citeauthor{hardt2016} established the convention of the Agent with state $\vx \in \mathcal{X}$, for some feature space $\mathcal{X}$, changing their state to $\Delta(\vx)$ defined as:
\begin{equation*}
    \Delta(\vx) := \argmax_{\vz \in \mathcal{X}} \{f(\vz) - c(\vx,\vz)\},
\end{equation*}
where $f: \mathcal{X} \rightarrow \{-1,1\}$, the known classifier, and $c: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}$ the cost the Agent incurs to change their state from $\rvx$ to $\rvz$, are specified by modelling assumptions. In the same work, \citeauthor{hardt2016} proposed an algorithm that could solve this problem, under the assumption of a separable cost function. Subsequent literature has proposed solutions that weaken this assumption (e.g., \cite{miller2020, eilat2022}). Other works propose an alternative formulation which does not explicitly rely on the cost, $c$, but instead introduces the concept of a manipulation graph to define the set of feasible states \cite{zhang2021, lechner2022, lechner2023}. In contrast with these works, this paper generalises the definition of the classifier, $f$, in the model.

\cite{ghalme2021} and \cite{cohen2024} explore variants on the conventional Strategic Classification formulation where the classifier, $f$, is presumed to be unknown to the Agents, and must be inferred. Both instances use distributions to capture the Agents' beliefs about the ``true" classifier; \citeauthor{cohen2024} model the Agents as maintaining a belief over possible classifier definitions. The Analyst can then shape the information they reveal about the classifier to the Agents in order to control their ability to game, with the goal of maximising accuracy. \citeauthor{ghalme2021} instead explore the case where the classifier is not revealed to the Agents, and so they have to approximate it from observation data about the classifier's behaviour. They define a measure, the Price of Opacity (POP), that measures the accuracy cost to the Learner for having the Agents estimate the classifier instead of just revealing it. The authors demonstrate that, under certain assumptions, not revealing classifier definition can result in considerable accuracy losses for the Learner. Unlike in \cite{ghalme2021} and \cite{cohen2024}, where distributions are only used to capture the Agents' uncertainty over the classifier chosen by the Learner, in this work we model the problem such that the distribution is what is chosen by the Learner. Further, in those works all Agents are ultimately classified by the same classifier, whereas in this work each User is classified by a classifier sampled from the chosen distribution.

Both \cite{braverman2020} and \cite{sundaram2023} explore the role of randomisation in improving robustness in Strategic Classification, although it is not the main focus of the latter work. As in this work, the authors consider a case where a distribution over classifiers is constructed (which they treat as a probabilistic classifier), and demonstrate conditions under which a randomised strategy could outperform an optimal deterministic strategy. However, as part of their investigations, the authors make assumptions that limit the degree to which the results transfer to more general settings (specifically they restrict themselves to linear classifiers in at most 2 dimensions). One of our results can be seen as a substantial generalisation of the claims about randomisation made in these previous works. In particular, Section \ref{sec:comparing_optimal_classifiers} of this paper extends the claims made by these previous works such that they can be applied to arbitrary hypothesis classes operating on features of any dimensionality. In contrast to \citet{braverman2020} and \citet{sundaram2023}, we also do not make a specific assumption about the data distribution. Instead, we provide a small set of sufficient conditions that must hold.

PAC Learning methods \citep{valiant1984} can be used to produce bounds on how well a classifier trained on a fixed dataset would be expected to generalise to the whole population distribution from which the dataset was sampled. \citet{zhang2021, sundaram2023, cullina2018} are examples of just a few Strategic Classification papers that have used PAC Learning methods to establish such bounds. The key difference between our work and these prior works is that we focus on the novel setting where the Learner selects a distribution over classifiers, rather than a single deterministic classifier. In the case when a distribution over hypotheses is being learned, these conventional PAC-Learning tools can't be applied. As a consequence, we derive new results that allow us to quantify the rate at which the performance of models trained using SERM converge towards the optimal risk.