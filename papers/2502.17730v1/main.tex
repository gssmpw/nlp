\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage[numbers]{natbib}  % natbib with numbered citations
\usepackage{hyperref}         % for clickable links, load after natbib
\usepackage{amsmath} 
%\bibliographystyle{plainnat}  % or another numbered style
\usepackage{authblk} % Package for author affiliations
\usepackage{xcolor}
\usepackage{url}
\usepackage{booktabs}
\usepackage[a4paper, margin=1in]{geometry}
\renewcommand{\Affilfont}{\small}  % Adjust affiliation font size


\title{Gender Bias in Perception of Human Managers Extends to AI Managers} %{Gender discrimination by humans generalizes even to AI managers
%Perceptions of gendered AI managers in collaborative  teams/ Female human manager is perceived more negatively than male AI in collaborative  teams/Even male AI managers receive a fairer evaluation than female human managers

\author[1, 2, 3]{Hao Cui}
\author[1, 4, 5, *]{Taha Yasseri}
\affil[1]{School of Social Sciences and Philosophy, Trinity College Dublin, Dublin, Ireland}
\affil[2]{School of Sociology, University College Dublin, Dublin, Ireland}
\affil[3]{Geary Institute for Public Policy, University College Dublin, Dublin, Ireland}
\affil[4]{Faculty of Arts and Humanities, Technological University Dublin, Dublin, Ireland}
\affil[5]{School of Mathematics and Statistics, University College Dublin, Dublin, Ireland} 
\affil[*]{Corresponding author: taha.yasseri@tcd.ie}

\begin{document}

\maketitle

\begin{abstract}
As AI becomes more embedded in workplaces, it is shifting from a tool for efficiency to an active force in organizational decision-making. Whether due to anthropomorphism or intentional design choices, people often assign human-like qualities -- including gender -- to AI systems. However, how AI managers are perceived in comparison to human managers and how gender influences these perceptions remains uncertain. To investigate this, we conducted randomized controlled trials (RCTs) where teams of three participants worked together under a randomly assigned manager -- either human or AI -- who was presented as male, female, or gender-neutral. The manager’s role was to select the best-performing team member for an additional award. Our findings reveal that while participants initially showed no strong preference based on manager type or gender, their perceptions changed significantly after experiencing the award process. As expected, those who received awards rated their managers as more fair, competent, and trustworthy, while those who were not selected viewed them less favorably. However, male managers -- both human and AI -- were more positively received by awarded participants, whereas female managers, especially female AI managers, faced greater skepticism and negative judgments when they denied rewards. These results suggest that gender bias in leadership extends beyond human managers and towards AI-driven decision-makers. As AI takes on greater managerial roles, understanding and addressing these biases will be crucial for designing fair and effective AI management systems.
\end{abstract}

\noindent \textbf{Keywords:} Gender bias, AI management,  AI and Workplace, Anthropomorphism in AI

\section*{Introduction}

The increasing integration of artificial intelligence (AI) in workplaces is transforming its function from merely enhancing efficiency to actively shaping organizational decision-making, sparking discussions on the evolving dynamics between humans and machines~\cite{tsvetkova2024new}. 
AI algorithms are increasingly taking on roles traditionally performed by humans, such as assisting with decisions related to promotion and hiring~\cite{kelan2024algorithmic, mainka2019algorithm}. While human managers often retain final authority, some companies are experimenting with AI in leadership-adjacent roles, such as managing repetitive tasks or optimizing team operations~\cite{kiron2023workforce}. This growing influence of AI raises questions about workplace dynamics, particularly how AI decision-makers are perceived relative to human managers. Issues such as fairness, biases, organizational justice~\cite{colquitt2001justice}, trust, morale, and willingness to collaborate within teams are becoming increasingly important as AI reshapes the future of leadership and management. %, ***???with potential to enhance collective intelligence~\cite{cui2024ai}***.

Studies indicate that people may have different expectations and biases toward AI versus human managers~\cite{logg2019algorithm, logg2024simple}. 
Research reveals that AI-driven decisions are often seen as objective and consistent~\cite{jones2023people}.
AI's data-centric methods can reduce certain biases inherent in human decision-making~\cite{hofeditz2022applying}. 
However, AI may lack the empathy~\cite{montemayor2022principle} and contextual awareness~\cite{mitchell2021abstraction} expected from human managers, which can impact trust and acceptance. 
Research shows people may perceive AI algorithms as more unfair because they fail to incorporate certain qualitative information and context, which can undermine the belief of procedural fairness~\cite{newman2020eliminating}.  
Other research shows participants may be more lenient in their evaluations of AI systems than human experts when they perceive the AI as having less control over unfavorable outcomes~\cite{jones2023people}.

Gender bias often influences how people assess a manager's fairness, competence, and leadership qualities, and it has been widely studied. 
Studies consistently reveal that female managers are often perceived less favorably than their male counterparts~\cite{heilman1995sex, heilman2001description, koch2015meta, braun2017think}, potentially due to the discrepancy between the traditional female gender role and the leadership role~\cite{elsesser2011does, eagly2002role}.
Women in managerial positions are often held to higher standards of warmth and likability~\cite{ridgeway2001gender}, while also being scrutinized more harshly for assertiveness or decisiveness—traits typically valued in male managers~\cite{rudman2008backlash}. Male managers are frequently perceived as more competent and authoritative by default~\cite{schein1996think, powell2002gender}, which can enhance perceptions of their fairness and leadership capability. Conversely, female managers, despite displaying the same level of competence, may face skepticism regarding their authority or be seen as less credible~\cite{eagly2002role}. 
Past research indicates that this disparity is especially pronounced in workplace evaluations by male peers~\cite{szymanska2018gender}, where female managers are rated significantly lower than their male counterparts, while female peers provide similar evaluations for both male and female managers~\cite{szymanska2018gender}. 

Gender stereotypes and biases that affect human managers are likely to also influence perceptions of AI managers, as AI systems are often anthropomorphized and assigned gender markers such as voice, name, or avatar by designers and users~\cite{craiut2022technology, wong2023chatgpt}.
Female AI systems are often perceived as warmer and human-like, making them more acceptable in healthcare contexts~\cite{borau2021most}.
Research in human-robot interaction shows that female bots are frequently associated with communal qualities such as warmth, friendliness, and emotional sensitivity~\cite{gustavsson2005virtual, eyssel2012s, otterbacher2017s, stroessner2019social, borau2021most} -- traits traditionally attributed to women~\cite{eagly1984gender, ebert2014warm}.
Studies further show that female service robots elicit greater satisfaction and positive experiences compared with male robots~\cite{seo2022female}, reflecting the persistence of traditional gender stereotypes in shaping perceptions of AI systems. 
However, gendered biases in AI do not always confer advantages.  
The assigned gender of an AI system affects human-AI cooperation, with research showing that female-labeled AI agents are more likely to be exploited than their human counterparts~\cite{bazazi2024ai}. 
This gendered bias extends to error mitigation scenarios, where male users preferred apologetic feminine voice assistants over masculine ones~\cite{mahmood2024gender}.  
These patterns suggest that bots and voice assistants not only reflect but may also reinforce and amplify existing gender biases~\cite{kiron2023workforce}.  

Even when AI lacks explicit gender markers, individuals tend to subconsciously assign gender based on factors like language style, voice, and the perceived roles of AI. 
Research shows that minimal gender cues, such as vocal cues embedded in a machine, can evoke gender-based stereotypical responses~\cite{nass1997machines}.
A study on ChatGPT shows it is more likely to be perceived as male than female due to its competence-oriented attributes, such as providing information or summarizing text~\cite{wong2023chatgpt}. 
AI systems designed for caregiving or support, such as digital assistants like Siri and Alexa, tend to be feminized through their anthropomorphization~\cite{costa2018conversing}, reflecting societal norms that associate caregiving and service roles with women~\cite{costa2019ai}. 
The subconscious gendering of AI can influence fairness perceptions and team dynamics, making it important to understand these biases to promote positive interactions in human-AI collaborations.

In addition to external factors such as manager type and perceived gender, internal factors like self-perceived contribution can also shape perceptions of fairness in decision-making~\cite{guo2014neural}.
According to equity theory, individuals evaluate fairness by assessing the balance between their contributions and rewards in comparison with those of their peers~\cite{adams1963towards}.
Research demonstrates that individuals who view themselves as high contributors but are not chosen for recognition often report lower fairness ratings, driven by unmet expectations and perceived inequity~\cite{adams1965inequity}. This dynamic highlights the role of internal self-assessments in fairness evaluations. 

The interplay between manager type (human or AI) and perceived gender in influencing fairness perceptions, particularly in competitive scenarios where only a subset of individuals are awarded, remains underexplored.
This study aims to examine how the combined effects of manager type, manager gender, self-perceived contribution, and award status influence the perception of managers' fairness, trustworthiness, competence, and willingness to work with similar managers in future tasks. Fairness is crucial for morale and acceptance; trustworthiness relates to reliability and ethical standing; competence underpins a manager’s authority and efficacy; and willingness to work in future endeavors reflects the sustainability of the collaboration. These dimensions are essential for fostering positive and effective human-AI team interactions.

To explore these dynamics, we conducted online experiments where participants worked in teams under managerial scenarios varying by manager type and gender. This study aims to provide insights into the nuanced ways that both AI and gendered presentations impact human acceptance of AI managers, ultimately contributing to the growing understanding of effective AI integration in workplaces. 


\section*{Results}

Participants based in the United States were recruited for the online experiment. In the pre-treatment survey, participants were asked to express their perception of managers across different types and genders on four dimensions, using a scale of 0 to 10. Here we present results for perceived fairness; results for other dimensions are available in the SI. 

The treatment phase consisted of two rounds: an individual problem-solving task and a team-based task. In the team-based round, participants worked in teams of three, with a randomly assigned manager whose identity labels varied in type and gender. The manager was introduced as responsible for selecting the best player to receive an additional award, whereas in reality, one player was selected to receive an extra award at random. In the next step, the participants were asked similar questions regarding their manager, and their responses to the post-treatment survey were compared with their responses to the pre-treatment survey. 

In the pre-treatment survey, participants perceived fairness across manager types and genders averaged around 7, with slight variations (Fig.~\ref{fig:fig1}A, D). 
Female participants gave slightly higher ratings to female managers, while male participants viewed male managers as fairer, 
though the differences between groups were relatively small.
The perceived fairness of managerial decisions shows a weak positive correlation between human and AI managers ($r$ = 0.097) but a strong positive correlation between female and male managers ($r$ = 0.653) (See SI).
This suggests that fairness perceptions in the pre-treatment phase may be more influenced by manager type, whereas gender differences appear to have a weaker effect in favor of female managers.
Similar patterns are observed across other dimensions as detailed in the SI.

Post-treatment comparisons (Fig.~\ref{fig:fig1}B, E) show that receiving an award increased perceived fairness across all manager types. Among female participants, the differences were pronounced, with male-AI managers showing the largest increase and female-human managers the smallest. For male participants, the differences were less pronounced, with male-human managers showing the highest increase and female-human managers the lowest.
Conversely, not receiving an award (Fig.~\ref{fig:fig1}C, F) generally reduced perceived fairness, with the steepest declines observed for female-AI managers across both female and male participants. 
Female-human managers experienced greater declines in fairness ratings from female participants than male participants,
suggesting that unmet pre-existing expectations may negatively shape post-treatment perceptions.
The dynamic shifts observed in fairness perceptions before and after the treatment emphasizes the influence of manager type, gender, and award outcomes. 

\begin{figure}[htbp] 
    \centering
    \includegraphics[width=\textwidth]{Figure1_Jan20.png} %0.47
    \caption{Comparison of the average perceived fairness of different groups by manager type and gender, with standard error. 
    The upper row represents results for female participants, and the lower row represents results for male participants. (A) and (D) show the average pre-treatment perceived fairness. (B) and (E) depict the average change in post-treatment perceived fairness for awarded participants, while (C) and (F) illustrate the change for non-awarded participants.
} 
    \label{fig:fig1} 
\end{figure}


Figure~\ref{fig:fig2} illustrates the factors influencing post-treatment fairness perception $Fairness_{post}$ in the linear regression model with interaction terms (Eq.~\ref{eq:post_fairness}) for female and male participants seperately. 
$Fairness_{post}$ is shaped by different persistent factors.
For female participants, pre-treatment perceptions of the manager’s gender fairness significantly predict $Fairness_{post}$ ($\beta = 0.389, p < 0.001$), whereas for male participants, pre-treatment perceptions of the manager’s type play a similar role ($\beta = 0.294, p < 0.001$). 

%*** These two sentences are saying the same thing about male an female, why not merge them?***
%For female participants, their pre-treatment perception of the manager's gender fairness emerges as a significant predictor of $Fairness_{post}$ ($\beta = 0.389, p < 0.001$). 
%For male participants, their pre-treatment perception of the manager's type  is a significant predictor ($\beta = 0.294, p < 0.001$). 

Demographic variables such as age and education generally show insignificance effects on participants' perception of their manager's decision, except for male participants where a postgraduate degree predicts higher $Fairness_{post}$ ($\beta = 2.329, p < 0.05$). 
%Receiving an award reduces negative impacts on fairness perceptions across manager types and genders. 
%However, 
Relative to the reference category (awarded by a human-male manager), not receiving an award significantly lowers perceived fairness for female-AI managers ($\beta = -3.467, p < 0.001$ for female participants; $\beta = -2.871, p < 0.001$ for male participants), female-human managers ($\beta = -2.209, p < 0.01$ for female participants; $\beta = -2.574, p < 0.01$ for male participants), and AI managers of unspecified gender ($\beta = -3.599, p < 0.001$ for female participants; $\beta = -2.005, p < 0.05$ for male participants). 

\begin{figure}[htbp] 
    \centering
    \includegraphics[width=\textwidth]{Figure2_Jan23.png} %0.47
    \caption{
    Coefficient estimates ($\beta$) for post-treatment perceived fairness $Fairness_{post}$ among female and male participants, based on pre-treatment perceptions, demographic factors, and interactions (see Eq.~\ref{eq:post_fairness}).
    Interaction terms capture the influence of manager type, manager gender, and participants' self-perceived contributions on perceived fairness. Statistical significance is denoted by $p<0.05$. Error bars represent standard errors.
%Due to perfect multicollinearity, the predictor Awarded $\times$ Human $\times$ Male was automatically removed by R's regression algorithm, as its effect was fully accounted for by other interaction terms.
    %NA values are due to collinearity among interaction terms, where certain predictors are redundant or linearly dependent on others.
    } 
    \label{fig:fig2} 
\end{figure} 

Interaction effects between participants' self-perceived contribution, award receipt, and manager type reveal that fairness perceptions are closely tied to an expectation-reward dynamic. Not receiving an award, especially under high self-perceived contribution, significantly reduces fairness perceptions for both human managers ($\beta = -0.749, p < 0.001$ for female participants; $\beta = -0.376, p < 0.001$ for male participants) and AI managers ($\beta = -0.437, p < 0.001$ for female participants; $\beta = -0.436, p < 0.001$ for male participants). 
The reduction in fairness perception for human managers among female participants is greater than that for AI managers and exceeds the reduction observed for either human or AI managers among male participants.
This may stem from higher expectations of fairness and empathy in human leadership from female participants. 
The results we report here are specifically for perceived fairness. Findings for the other dimensions are generally consistent; see Supporting Information for additional details.


\section*{Discussion}

Being selected for an award increased participants' positive perceptions of the manager across all manager types. However, the magnitude of these benefits was moderated by manager gender, with male managers experiencing more pronounced gains than female managers.
This aligns with pervasive societal stereotypes that link competence and leadership effectiveness with male figures~\cite{eagly1984gender, ridgeway2001gender, eagly2002role}. 
The reinforcement of positive recognition for male managers, and especially male-AI managers (Fig.~\ref{fig:fig1}), could reflect societal tendencies to perceive male-led successes as more legitimate or expected, a bias that some have argued is deeply ingrained in patriarchal organizational structures~\cite{connell2020social}. Furthermore, algorithmic neutrality and objectivity~\cite{green2020algorithmic} may intersect here, as male-AI managers might benefit from a dual layer of presumed objectivity: male leadership and AI precision.

On the other hand, negative outcomes notably harmed perceptions of female managers, particularly female-AI managers, highlighting a compounded vulnerability, where skepticism toward AI decision-making intersects with gender biases to yield harsher judgments. Female human managers also faced a notable decline in the assessment of participants, though to a lesser extent, possibly due to the perception that human managers demonstrate greater accountability and empathy~\cite{montemayor2022principle}.
By contrast, male managers exhibited greater resilience to negative outcomes, likely due to societal leniency afforded to male figures, consistent with status expectations theory~\cite{ridgeway2001gender}. 
This underscores the moderating role of gender in shaping perceptions of managerial decisions, illustrating a deeply embedded double standard where cultural narratives shield male leaders with presumed competence and external locus of control, while female leaders face greater blame for unfavorable outcomes. 
These findings resonate with broader feminist critiques that women’s roles and societal positions are often disproportionately scrutinized within a male-dominated societal framework~\cite{butler1990feminism, hooks2000feminist}.

The amplified negative impact on female-AI managers emphasizes the need to address intersectional biases in workplace dynamics, where gender and technology intersect to create compounded disadvantages. Organizations aiming to integrate AI in leadership roles should consider strategies to mitigate such biases, such as transparency in AI decision-making processes and fostering a culture that values fairness and equity. Furthermore, the results point to the importance of aligning performance outcomes with fairness perceptions, as these evaluations can significantly influence trust, collaboration, and organizational morale.

Our study has limitations that highlight avenues for future research. Conducted in a single setting with one specific task, our findings may not generalize to more complex scenarios involving diverse tasks, larger groups, or mixed-agent interactions between humans and AI in the same environment. Future studies could explore the mechanisms driving these biases, such as cognitive biases, stereotypes, and trust dynamics, to better understand how they shape various dimensions of perceptions, including trustworthiness, competence, fairness, and willingness to collaborate. Additionally, broader contextual factors like cultural norms, team dynamics, and prior experiences with AI or human managers, could also be examined to uncover the nuanced drivers of these perceptions. These insights will be essential for designing systems across a range of settings that foster equitable, trustworthy, unbiased, and effective integration of AI in managerial and collaborative tasks.

The emergence of AI in managerial and societal roles presents an opportunity to move beyond outdated frameworks and mindsets, yet it also risks perpetuating or even amplifying existing gender and sexist stereotypes if left unchecked. While significant scrutiny is placed on ensuring AI is human-centric, fair, and explainable, we must also hold ourselves accountable as humans to embody these principles in our behaviors and societal norms. History has shown how prejudice and discrimination linger in our actions and institutions, and these biases could be magnified as AI becomes more embedded in daily life. To prevent this, it is imperative to address and dismantle these stereotypes now—before they shape the interactions between humans and AI in ways that are counterproductive or harmful. Policymakers currently shaping AI regulation must consider these risks and explore bold measures, such as developing AI systems that are gender-neutral, to break free from reinforcing stereotypes. By confronting these issues today, we set the stage for a more equitable future where both humans and AI contribute to a society free from discrimination.


\section*{Materials and Methods}

{The survey questions used in the pre-treatment and post-treatment surveys, along with screenshots from the experiment, are provided in the Supporting Information.
This research complies with University College Dublin (UCD) Human Research Ethics Regulations and Human Research Ethics Committee (HREC) guidelines for research involving human participants. The research study protocol has been approved by UCD Human Research Ethics
(HS-LR-24-10-Cui-Yasseri). Informed consent was obtained from all human participants prior to the experiment.


\subsection*{Participants}
Participants were recruited from the Prolific platform (\url{https://www.prolific.com/}), selecting U.S.-based adults aged 18 and older for an online experiment exploring perceptions of decision-making in scenarios involving human and AI managers. The final sample included N = 556 participants, with an average age of 37.5 years ($\pm$ 12.1). The sample consisted of approximately 48.7\% female, 48.4\% male, and 2.9\% identifying as non-binary or other genders. Participants reported various educational backgrounds, ranging from secondary education to postgraduate degrees. (See details in Supporting Information.) 

\subsection*{Design and Measures}

\begin{figure}[htbp] 
    \centering
    \includegraphics[width=0.6\textwidth]{game.png} 
    \caption{The methodological framework of the experiment. A team of three human subjects engage collaboratively in an online environment. A manager is assigned randomly who is either a human or an AI agent, and either female, male, or gender unspecified. 
    Participants are informed that the manager will select the best player based on multiple factors evaluated during the team collaboration round, including problem-solving ability and communication skills. The chosen player will receive an extra award of \pounds0.5. 
    }
    \label{fig:fig_game} 
\end{figure}

Figure~\ref{fig:fig_game} illustrates a methodological framework of the experiment.
This study employed a mixed experimental design to examine how participants’ perceptions of trustworthiness, competence, fairness, and willingness to work with a manager were influenced by primary factors: manager type, manager gender, award outcome, and self-perceived contribution. Participants were randomly assigned to three-person groups, with each group placed in a different manager condition. In these conditions, participants viewed scenarios in which either a human or an AI manager made a selection decision. Within each manager type, the gender of the manager was also manipulated, with the manager identified as either female, male, or gender unspecified to explore potential biases or differential expectations based on perceived gender. 

A key aspect of the design was the participants' award outcome. In each group, the assigned manager randomly chose a “best player” to receive an extra award, and participants received an announcement about which participant had been selected as the ``best player". This manipulation allowed the study to investigate how participants’ selection status interacted with both the manager type and manager gender, potentially shaping perceptions and attitudes toward the manager.

In addition to the award outcome, participants rated their own contributions to the teamwork as well as the contributions of their teammates on an 11-point scale ranging from 0 to 10. 
We define the contribution measure as

\begin{align*}
\text{Contribution} &= \text{Rating}_{\text{self}} - \frac{1}{2} \big( \text{Rating}_{\text{teammate 1}}  \\
&\quad + \text{Rating}_{\text{teammate 2}} \big),
\end{align*}
where $\text{Rating}_{\text{self}}$  is the rating an individual gives to themselves 
and $\text{Rating}_{\text{teammate i}}$  is the rating an individual gives to teammate i.
This self-assessed contribution measure offers insight into how individuals’ perceptions of their own contributions, relative to their peers, influence their judgments of the manager.

The dependent variables in this study were the participants' perceptions of trustworthiness, competence, fairness of their assigned manager, and the willingness to collaborate with a similar manager in future tasks. Each of these perceptions was measured using a Likert-type scale from 0 to 10, with items such as “How fair do you think the manager was in making the award decision in the game?”.
Take fairness as an example; we employed the following linear regression model to examine the factors influencing post-treatment fairness perceptions:  
\begin{equation}
\begin{split}
\text{PF} = &\ \beta_0 + \beta_1 \text{PTF} + \beta_2 \text{PGF} + \beta_3 \text{A} + \beta_4 \text{E} + \beta_5 \text{G} + \notag \\
&\beta_6 (\text{AW} \cdot \text{MT} \cdot \text{MG}) +
\beta_7 (\text{AW} \cdot \text{MT} \cdot \text{C}) + \epsilon,
\label{eq:post_fairness}\\
\text{where}\\
\text{PF} & : \text{post-fairness (response)} \\
\text{PTF} & : \text{pre-type-fairness} \\
\text{PGF} & : \text{pre-gender-fairness} \\
\text{A}, \text{E}, \text{G} & : \text{Age, Education, and Gender} \\
\text{AW} &: \text{Awarded, } \\
\text{MT}, \text{MG} &: \text{Manager Type}, \text{Manager Gender}\\
\text{C} &: \text{Contribution}\\ 
\beta_0 & : \text{Intercept term} \\
\beta_1, \beta_2, \ldots, \beta_7 & : \text{Coefficients for predictors and interactions} \\
\epsilon & : \text{Error term}
\end{split}
\end{equation}

The model includes pre-treatment fairness perceptions based on manager type (pre-type-fairness) and manager gender (pre-gender-fairness), along with demographic variables such as age, education, and gender. Interaction terms were included to capture the combined effects of awards, manager type, manager gender, and contribution on post-treatment fairness. Specifically, the model accounts for the three-way interaction between awards, manager type, and manager gender, as well as the three-way interaction between awards, manager type, and contribution.
The model was applied separately to female and male participants.


Overall, this design and these measures were intended to provide a detailed understanding of how variations in manager characteristics and selection outcomes influence key perceptions in decision-making contexts.

\subsection*{Procedure} 

\textbf{Experimental platform and infrastructure.} The experiments were conducted using Empirica (\url{https://empirica.ly/}), a platform designed for building and deploying interactive, real-time experiments~\cite{almaatouq2021empirica}. Empirica enables the creation of dynamic multiplayer tasks and facilitates participant interaction in controlled online environments. 
The experiments were run on the DigitalOcean droplet (\url{https://www.digitalocean.com/}). 
\vspace{0.3cm}

\noindent\textbf{Participant onboarding and pre-treatment survey.}
The experiment took place entirely online and was designed to simulate a decision-making scenario involving teamwork and managerial evaluation. 
Participants recruited via Prolific began by clicking a provided link to enter the Empirica platform, where the experiment was hosted.
Upon joining, participants first read an information sheet outlining the study's goals and procedures, followed by providing their consent. They then completed a brief survey collecting demographic information, such as age, gender, education level, and their initial perceptions of trustworthiness, competence, fairness, and willingness to work with both AI and human managers, as well as male and female managers. This initial survey helped establish baseline perspectives toward different manager types. 
\vspace{0.3cm}

\noindent\textbf{Individual problem-solving task.}
Following the preliminary survey, participants engaged in two rounds of a game designed to foster both individual and team-based problem-solving. 
The game involved solving a brain teaser by observing pictures and identifying the ``robber".
In the first round, they completed the task individually, allowing them to form an initial self-assessment of their performance. 
\vspace{0.3cm}

\noindent\textbf{Manager assignment.}
After completing the task, participants were informed they would join a team with two teammates for the next round. Each team was randomly assigned a manager, either a human or an AI, who would evaluate the team's performance. The manager's gender was also manipulated within each manager type, with the manager identified as male, female, or gender unspecified. The manager would ultimately select the ``best player" in the team to receive an extra cash award of \pounds0.5.
\vspace{0.3cm}

\noindent\textbf{Team-based task and collaboration.}
In the second round, participants collaborated in a team-based setting through an online chat interface to complete the same task as in the first round. They could view their own and their teammates' responses from the previous round on the slider bars, providing context for discussion. Through the chat, participants worked collectively to arrive at a final team response, simulating real-world team dynamics. All teams completed identical tasks, ensuring a consistent basis for comparing perceptions of the managers across groups. 
\vspace{0.3cm}

\noindent\textbf{Contribution assessment.}
After completing the team task, participants rated their own and their teammates’ contributions on a scale from 0 to 10. The chat history remained visible during this rating phase but was disabled for further input, ensuring that participants could not influence each other’s ratings. This contribution assessment allowed for an additional evaluation of how individuals perceived their own and their peers' contributions within the team.
\vspace{0.3cm}

\noindent\textbf{Manager decision and exit survey.}
After a waiting time of 20 seconds, participants received the announcements of the ``best player" selected by the manager.  
Finally, participants completed an exit survey that included the study’s primary measures: perceptions of trustworthiness, competence, and fairness regarding the manager’s decision, as well as their willingness to work with a similar manager in the future. 
\vspace{0.3cm}

\noindent\textbf{Debriefing and payment.} 
After completing the survey, participants were informed during the debriefing that there was no actual manager involved in the experiment and that the selection was random. The entire experiment took approximately 12 minutes, providing an experience of teamwork, evaluation, and managerial decision-making. Participants were compensated at a rate of \pounds9 per hour, prorated for the duration of the study. The chosen ``best player" received an additional \pounds0.5 bonus. 

}


\section*{Data availability}

Experimental results data have been deposited in Osf.io (\href{https://osf.io/253wq/}{https://osf.io/253wq/})~\cite{database}. 

\section*{ACKNOWLEDGMENTS}
The research conducted in this publication was funded by the Irish Research Council under grant number IRCLA/2022/3217, ANNETTE (Artificial Intelligence Enhanced Collective Intelligence).
We thank Bahador Bahrami and Nico Mutzner for their valuable discussions and comments. 

\bibliographystyle{unsrt}

\bibliography{references}

\clearpage
\appendix
\clearpage
\begin{center}
    \textbf{\LARGE SUPPLEMENTARY INFORMATION}
\end{center}
\bigskip\input{main_SI}

\end{document}
