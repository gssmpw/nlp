\section{Related Work}
\textbf{Spatio-Temporal Video Grounding.} Spatio-temporal video grounding (STVG) **Lin, "Temporal Language Network for Temporally Localizing Actions"**____ aims to spatially and temporally localize the target of interest, given its free-form of textual description, from an untrimmed sequence. Early STVG approaches **Duan, "Visual Grounding of Text Phrases in Videos by Deep Model Learning"**____ mainly consists of two stages, \emph{first} generating candidate proposals from the video with a pre-trained detector and \emph{then} selecting correct proposals based on the textual expression. To eliminate the heavy dependency on the pre-trained detection model, recent methods **Yang, "End-to-End Video Captioning via Multimodal Transformers"**____, inspired by Transformer, switch to the one-stage pipeline that directly generates a spatio-temporal tube for target localization, without relying on any external detectors. Owing to the compact end-to-end training pipeline, such a one-stage framework demonstrates superior performance compared to previous two-stage algorithms. Our TA-STVG also belongs to the one-stage Transformer-based type. However, \emph{\textbf{different from}} the aforementioned Transformer-based approaches that simply follows **Carion, "End-to-End Object Detection with Transformers"**____ to leverage zero-initialized object queries for target localization, TA-STVG innovatively exploits target-specific cues from the video-text pair for object query generation, making it adaptive to various scenarios and better interact with multimodal features in the decoder for more accurate localization. 

\textbf{Temporal Grounding.} Temporal grounding focuses on localizing specific targets or events from the video given the textual expression. Being relevant to STVG, it requires temporally localizing the target of interest, but the difference is that temporal grounding does not require to perform the spatial bounding box localization.
In recent years, many methods **Li, "Video Temporal Grounding with Multi-Task Learning"**____ have been proposed for temporal grounding. For instance, the work of **Wang, "Temporal Language Network for Temporally Localizing Actions"**____ introduces a pre-training approach for improving video temporal grounding. The method in **Chen, "A Unified Framework for Video Temporal Grounding Tasks"**____ presents a unified framework for various video temporal grounding tasks. The approach in **Xu, "Learning Complementary Features for Temporal Grounding"**____ proposes to learn complementary features from different modalities including images, flow, and depth for temporal grounding. \textbf{\emph{Different than}} these works, we focus on the more challenging STVG involved with spatial and temporal localization of the target.

\textbf{Transformer-based Detection.} Detection is a fundamental component in computer vision. Recently, the seminal work **Carion, "End-to-End Object Detection with Transformers"**____ has applied Transformer **Lin, "Temporal Language Network for Temporally Localizing Actions"**____ for detection with impressive performance, and later been further improved in numerous extensions **Zhao, "Improved End-to-End Video Captioning via Multimodal Transformers"**____. Similar to other Transformer-based STVG works, our method employs the DETR-similar architecture for STVG. The \emph{\textbf{difference}} is that we propose to learn adaptive object queries from the video-text pair, instead of utilizing zero object queries following **Carion, "End-to-End Object Detection with Transformers"**____  as in existing STVG methods, for better target localization. 

\textbf{Vision-Language Modeling.} Vision-language modeling (VLM) aims to process both visual content and language for multimodal understanding. In recent years, it has attracted great attention from the researchers and been studied in various tasks including **Miech, "End-to-End Object Detection with Transformers"**, **Gao, "Visual Captioning via Multimodal Transformers"**, navigation **Wang, "Navigating with Language and Vision"**, text-to-image generation **Hendricks, "Text-to-Image Synthesis for Visually Grounded Tasks"**, referring expression segmentation **Chen, "Referring Expression Segmentation using Image-Text Features"**, vision-language tracking **Zhang, "Vision-Language Tracking via Multimodal Transformers"**. \emph{\textbf{Different}} from these tasks, we focus on vision-language modeling for spatio-temporal video grounding.