\section{Related Work}
\textbf{Spatio-Temporal Video Grounding.} Spatio-temporal video grounding (STVG)~\citep{STGRN} aims to spatially and temporally localize the target of interest, given its free-form of textual description, from an untrimmed sequence. Early STVG approaches~\citep{STGRN, OAMBRN, hcstvg} mainly consists of two stages, \emph{first} generating candidate proposals from the video with a pre-trained detector and \emph{then} selecting correct proposals based on the textual expression. To eliminate the heavy dependency on the pre-trained detection model, recent methods~\citep{STVGBert, TubeDETR,STCAT,csdvl,talal2023video,cgstvg}, inspired by Transformer, switch to the one-stage pipeline that directly generates a spatio-temporal tube for target localization, without relying on any external detectors. Owing to the compact end-to-end training pipeline, such a one-stage framework demonstrates superior performance compared to previous two-stage algorithms. Our TA-STVG also belongs to the one-stage Transformer-based type. However, \emph{\textbf{different from}} the aforementioned Transformer-based approaches that simply follows~\citep{dert} to leverage zero-initialized object queries for target localization, TA-STVG innovatively exploits target-specific cues from the video-text pair for object query generation, making it adaptive to various scenarios and better interact with multimodal features in the decoder for more accurate localization. 


\textbf{Temporal Grounding.} Temporal grounding focuses on localizing specific targets or events from the video given the textual expression. Being relevant to STVG, it requires temporally localizing the target of interest, but the difference is that temporal grounding does not require to perform the spatial bounding box localization.
In recent years, many methods~\citep{locvtp,drft,mun2020local,hao2022can,wang2023protege,zhang2023text,lmmg,lin2023univtg} have been proposed for temporal grounding. For instance, the work of~\citep{wang2023protege} introduces a pre-training approach for improving video temporal grounding. The method in~\citep{lin2023univtg} presents a unified framework for various video temporal grounding tasks. The approach in~\citep{drft} proposes to learn complementary features from different modalities including images, flow, and depth for temporal grounding. \textbf{\emph{Different than}} these works, we focus on the more challenging STVG involved with spatial and temporal localization of the target.

\textbf{Transformer-based Detection.} Detection is a fundamental component in computer vision. Recently, the seminal work DETR~\citep{dert} has applied Transformer~\citep{vaswani2017attention} for detection with impressive performance, and later been further improved in numerous extensions~\citep{sun2021rethinking,zhu2020deformable,zheng2023less,ye2023cascade}. Similar to other Transformer-based STVG works, our method employs the DETR-similar architecture for STVG. The \emph{\textbf{difference}} is that we propose to learn adaptive object queries from the video-text pair, instead of utilizing zero object queries following~\citep{dert}  as in existing STVG methods, for better target localization. 

\textbf{Vision-Language Modeling.} Vision-language modeling (VLM) aims to process both visual content and language for multimodal understanding. In recent years, it has attracted great attention from the researchers and been studied in various tasks including visual question answering~\citep{antol2015vqa,jiang2020defense,han2023shot2story20k,shao2023prompting, wang2024reconstructive,weng2024longvlm}, visual captioning~\citep{you2016image,gu2023text,gu2022dual,wang2024droppos,shen2023accurate, ren2024pixellm, wang2023hard}, navigation~\citep{zhu2020vision,li2023improving}, text-to-image generation~\citep{MReward,ramesh2021zero}, referring expression segmentation~\citep{yang2022lavt,liu2023gres}, vision-language tracking~\citep{guo2022divert,zhou2023joint}, etc. \emph{\textbf{Different}} from these tasks, we focus on vision-language modeling for spatio-temporal video grounding.