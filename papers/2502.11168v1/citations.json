[
  {
    "index": 0,
    "papers": [
      {
        "key": "STGRN",
        "author": "Zhu Zhang and\nZhou Zhao and\nYang Zhao and\nQi Wang and\nHuasheng Liu and\nLianli Gao",
        "title": "Where Does It Exist: Spatio-Temporal Video Grounding for Multi-Form\nSentences"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "STGRN",
        "author": "Zhu Zhang and\nZhou Zhao and\nYang Zhao and\nQi Wang and\nHuasheng Liu and\nLianli Gao",
        "title": "Where Does It Exist: Spatio-Temporal Video Grounding for Multi-Form\nSentences"
      },
      {
        "key": "OAMBRN",
        "author": "Zhu Zhang and\nZhou Zhao and\nZhijie Lin and\nBaoxing Huai and\nJing Yuan",
        "title": "Object-Aware Multi-Branch Relation Networks for Spatio-Temporal Video\nGrounding"
      },
      {
        "key": "hcstvg",
        "author": "Tang, Zongheng and Liao, Yue and Liu, Si and Li, Guanbin and Jin, Xiaojie and Jiang, Hongxu and Yu, Qian and Xu, Dong",
        "title": "Human-centric spatio-temporal video grounding with visual transformers"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "STVGBert",
        "author": "Rui Su and\nQian Yu and\nDong Xu",
        "title": "STVGBert: {A} Visual-linguistic Transformer based Framework for Spatio-temporal\nVideo Grounding"
      },
      {
        "key": "TubeDETR",
        "author": "Antoine Yang and\nAntoine Miech and\nJosef Sivic and\nIvan Laptev and\nCordelia Schmid",
        "title": "TubeDETR: Spatio-Temporal Video Grounding with Transformers"
      },
      {
        "key": "STCAT",
        "author": "Yang Jin and\nYongzhi Li and\nZehuan Yuan and\nYadong Mu",
        "title": "Embracing Consistency: {A} One-Stage Approach for Spatio-Temporal\nVideo Grounding"
      },
      {
        "key": "csdvl",
        "author": "Lin, Zihang and Tan, Chaolei and Hu, Jian-Fang and Jin, Zhi and Ye, Tiancai and Zheng, Wei-Shi",
        "title": "Collaborative Static and Dynamic Vision-Language Streams for Spatio-Temporal Video Grounding"
      },
      {
        "key": "talal2023video",
        "author": "Talal Wasim, Syed and Naseer, Muzammal and Khan, Salman and Yang, Ming-Hsuan and Shahbaz Khan, Fahad",
        "title": "Video-GroundingDINO: Towards Open-Vocabulary Spatio-Temporal Video Grounding"
      },
      {
        "key": "cgstvg",
        "author": "Gu, Xin and Fan, Heng and Huang, Yan and Luo, Tiejian and Zhang, Libo",
        "title": "Context-Guided Spatio-Temporal Video Grounding"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "dert",
        "author": "Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey",
        "title": "End-to-end object detection with transformers"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "locvtp",
        "author": "Cao, Meng and Yang, Tianyu and Weng, Junwu and Zhang, Can and Wang, Jue and Zou, Yuexian",
        "title": "Locvtp: Video-text pre-training for temporal localization"
      },
      {
        "key": "drft",
        "author": "Chen, Yi-Wen and Tsai, Yi-Hsuan and Yang, Ming-Hsuan",
        "title": "End-to-end multi-modal video temporal grounding"
      },
      {
        "key": "mun2020local",
        "author": "Mun, Jonghwan and Cho, Minsu and Han, Bohyung",
        "title": "Local-global video-text interactions for temporal grounding"
      },
      {
        "key": "hao2022can",
        "author": "Hao, Jiachang and Sun, Haifeng and Ren, Pengfei and Wang, Jingyu and Qi, Qi and Liao, Jianxin",
        "title": "Can shuffling video benefit temporal bias problem: A novel training framework for temporal grounding"
      },
      {
        "key": "wang2023protege",
        "author": "Wang, Lan and Mittal, Gaurav and Sajeev, Sandra and Yu, Ye and Hall, Matthew and Boddeti, Vishnu Naresh and Chen, Mei",
        "title": "ProTeGe: Untrimmed Pretraining for Video Temporal Grounding by Video Temporal Grounding"
      },
      {
        "key": "zhang2023text",
        "author": "Zhang, Yimeng and Chen, Xin and Jia, Jinghan and Liu, Sijia and Ding, Ke",
        "title": "Text-visual prompting for efficient 2d temporal video grounding"
      },
      {
        "key": "lmmg",
        "author": "Barrios, Wayner and Soldan, Mattia and Heilbron, Fabian Caba and Ceballos-Arroyo, Alberto Mario and Ghanem, Bernard",
        "title": "Localizing moments in long video via multimodal guidance"
      },
      {
        "key": "lin2023univtg",
        "author": "Lin, Kevin Qinghong and Zhang, Pengchuan and Chen, Joya and Pramanick, Shraman and Gao, Difei and Wang, Alex Jinpeng and Yan, Rui and Shou, Mike Zheng",
        "title": "Univtg: Towards unified video-language temporal grounding"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "wang2023protege",
        "author": "Wang, Lan and Mittal, Gaurav and Sajeev, Sandra and Yu, Ye and Hall, Matthew and Boddeti, Vishnu Naresh and Chen, Mei",
        "title": "ProTeGe: Untrimmed Pretraining for Video Temporal Grounding by Video Temporal Grounding"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "lin2023univtg",
        "author": "Lin, Kevin Qinghong and Zhang, Pengchuan and Chen, Joya and Pramanick, Shraman and Gao, Difei and Wang, Alex Jinpeng and Yan, Rui and Shou, Mike Zheng",
        "title": "Univtg: Towards unified video-language temporal grounding"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "drft",
        "author": "Chen, Yi-Wen and Tsai, Yi-Hsuan and Yang, Ming-Hsuan",
        "title": "End-to-end multi-modal video temporal grounding"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "dert",
        "author": "Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey",
        "title": "End-to-end object detection with transformers"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "vaswani2017attention",
        "author": "Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\\L}ukasz and Polosukhin, Illia",
        "title": "Attention is all you need"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "sun2021rethinking",
        "author": "Sun, Zhiqing and Cao, Shengcao and Yang, Yiming and Kitani, Kris M",
        "title": "Rethinking transformer-based set prediction for object detection"
      },
      {
        "key": "zhu2020deformable",
        "author": "Zhu, Xizhou and Su, Weijie and Lu, Lewei and Li, Bin and Wang, Xiaogang and Dai, Jifeng",
        "title": "Deformable detr: Deformable transformers for end-to-end object detection"
      },
      {
        "key": "zheng2023less",
        "author": "Zheng, Dehua and Dong, Wenhui and Hu, Hailin and Chen, Xinghao and Wang, Yunhe",
        "title": "Less is more: Focus attention for efficient detr"
      },
      {
        "key": "ye2023cascade",
        "author": "Ye, Mingqiao and Ke, Lei and Li, Siyuan and Tai, Yu-Wing and Tang, Chi-Keung and Danelljan, Martin and Yu, Fisher",
        "title": "Cascade-DETR: Delving into High-Quality Universal Object Detection"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "dert",
        "author": "Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey",
        "title": "End-to-end object detection with transformers"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "antol2015vqa",
        "author": "Antol, Stanislaw and Agrawal, Aishwarya and Lu, Jiasen and Mitchell, Margaret and Batra, Dhruv and Zitnick, C Lawrence and Parikh, Devi",
        "title": "Vqa: Visual question answering"
      },
      {
        "key": "jiang2020defense",
        "author": "Jiang, Huaizu and Misra, Ishan and Rohrbach, Marcus and Learned-Miller, Erik and Chen, Xinlei",
        "title": "In defense of grid features for visual question answering"
      },
      {
        "key": "han2023shot2story20k",
        "author": "Han, Mingfei and Yang, Linjie and Chang, Xiaojun and Wang, Heng",
        "title": "Shot2story20k: A new benchmark for comprehensive understanding of multi-shot videos"
      },
      {
        "key": "shao2023prompting",
        "author": "Shao, Zhenwei and Yu, Zhou and Wang, Meng and Yu, Jun",
        "title": "Prompting large language models with answer heuristics for knowledge-based visual question answering"
      },
      {
        "key": "wang2024reconstructive",
        "author": "Wang, Haochen and Zheng, Anlin and Zhao, Yucheng and Wang, Tiancai and Ge, Zheng and Zhang, Xiangyu and Zhang, Zhaoxiang",
        "title": "Reconstructive visual instruction tuning"
      },
      {
        "key": "weng2024longvlm",
        "author": "Weng, Yuetian and Han, Mingfei and He, Haoyu and Chang, Xiaojun and Zhuang, Bohan",
        "title": "Longvlm: Efficient long video understanding via large language models"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "you2016image",
        "author": "You, Quanzeng and Jin, Hailin and Wang, Zhaowen and Fang, Chen and Luo, Jiebo",
        "title": "Image captioning with semantic attention"
      },
      {
        "key": "gu2023text",
        "author": "Gu, Xin and Chen, Guang and Wang, Yufei and Zhang, Libo and Luo, Tiejian and Wen, Longyin",
        "title": "Text with knowledge graph augmented transformer for video captioning"
      },
      {
        "key": "gu2022dual",
        "author": "Gu, Xin and Ye, Hanhua and Chen, Guang and Wang, Yufei and Zhang, Libo and Wen, Longyin",
        "title": "Dual-stream transformer for generic event boundary captioning"
      },
      {
        "key": "wang2024droppos",
        "author": "Wang, Haochen and Fan, Junsong and Wang, Yuxi and Song, Kaiyou and Wang, Tong and ZHANG, ZHAO-XIANG",
        "title": "Droppos: Pre-training vision transformers by reconstructing dropped positions"
      },
      {
        "key": "shen2023accurate",
        "author": "Shen, Yaojie and Gu, Xin and Xu, Kai and Fan, Heng and Wen, Longyin and Zhang, Libo",
        "title": "Accurate and Fast Compressed Video Captioning"
      },
      {
        "key": "ren2024pixellm",
        "author": "Ren, Zhongwei and Huang, Zhicheng and Wei, Yunchao and Zhao, Yao and Fu, Dongmei and Feng, Jiashi and Jin, Xiaojie",
        "title": "Pixellm: Pixel reasoning with large multimodal model"
      },
      {
        "key": "wang2023hard",
        "author": "Wang, Haochen and Song, Kaiyou and Fan, Junsong and Wang, Yuxi and Xie, Jin and Zhang, Zhaoxiang",
        "title": "Hard patches mining for masked image modeling"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "zhu2020vision",
        "author": "Zhu, Fengda and Zhu, Yi and Chang, Xiaojun and Liang, Xiaodan",
        "title": "Vision-language navigation with self-supervised auxiliary reasoning tasks"
      },
      {
        "key": "li2023improving",
        "author": "Li, Jialu and Bansal, Mohit",
        "title": "Improving vision-and-language navigation by generating future-view image semantics"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "MReward",
        "author": "Xin Gu and\nMing Li and\nLibo Zhang and\nFan Chen and\nLongyin Wen and\nTiejian Luo and\nSijie Zhu",
        "title": "Multi-Reward as Condition for Instruction-based Image Editing"
      },
      {
        "key": "ramesh2021zero",
        "author": "Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya",
        "title": "Zero-shot text-to-image generation"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "yang2022lavt",
        "author": "Yang, Zhao and Wang, Jiaqi and Tang, Yansong and Chen, Kai and Zhao, Hengshuang and Torr, Philip HS",
        "title": "Lavt: Language-aware vision transformer for referring image segmentation"
      },
      {
        "key": "liu2023gres",
        "author": "Liu, Chang and Ding, Henghui and Jiang, Xudong",
        "title": "Gres: Generalized referring expression segmentation"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "guo2022divert",
        "author": "Guo, Mingzhe and Zhang, Zhipeng and Fan, Heng and Jing, Liping",
        "title": "Divert more attention to vision-language tracking"
      },
      {
        "key": "zhou2023joint",
        "author": "Zhou, Li and Zhou, Zikun and Mao, Kaige and He, Zhenyu",
        "title": "Joint Visual Grounding and Tracking with Natural Language Specification"
      }
    ]
  }
]