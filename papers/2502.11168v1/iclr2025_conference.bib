@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})



@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})



@misc{Authors14,
 author = {FirstName LastName},
 title = {The frobnicatable foo filter},
 note = {Face and Gesture submission ID 324. Supplied as supplemental material {\tt fg324.pdf}},
 year = 2014
}

@misc{Authors14b,
 author = {FirstName LastName},
 title = {Frobnication tutorial},
 note = {Supplied as supplemental material {\tt tr.pdf}},
 year = 2014
}

@article{Alpher02,
author = {FirstName Alpher},
title = {Frobnication},
journal = PAMI,
volume = 12,
number = 1,
pages = {234--778},
year = 2002
}

@article{Alpher03,
author = {FirstName Alpher and  FirstName Fotheringham-Smythe},
title = {Frobnication revisited},
journal = {Journal of Foo},
volume = 13,
number = 1,
pages = {234--778},
year = 2003
}

@article{Alpher04,
author = {FirstName Alpher and FirstName Fotheringham-Smythe and FirstName Gamow},
title = {Can a machine frobnicate?},
journal = {Journal of Foo},
volume = 14,
number = 1,
pages = {234--778},
year = 2004
}

@inproceedings{Alpher05,
author = {FirstName Alpher and FirstName Gamow},
title = {Can a computer frobnicate?},
booktitle = CVPR,
pages = {234--778},
year = 2005
}

@inproceedings{OAMBRN,
  author       = {Zhu Zhang and
                  Zhou Zhao and
                  Zhijie Lin and
                  Baoxing Huai and
                  Jing Yuan},
  title        = {Object-Aware Multi-Branch Relation Networks for Spatio-Temporal Video
                  Grounding},
  booktitle    = {IJCAI},
  year         = {2020}
}

@inproceedings{talal2023video,
  title={Video-GroundingDINO: Towards Open-Vocabulary Spatio-Temporal Video Grounding},
  author={Talal Wasim, Syed and Naseer, Muzammal and Khan, Salman and Yang, Ming-Hsuan and Shahbaz Khan, Fahad},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{zhou2022conditional,
  title={Conditional prompt learning for vision-language models},
  author={Zhou, Kaiyang and Yang, Jingkang and Loy, Chen Change and Liu, Ziwei},
  booktitle={CVPR},
  year={2022}
}

@article{li2019controllable,
  title={Controllable text-to-image generation},
  author={Li, Bowen and Qi, Xiaojuan and Lukasiewicz, Thomas and Torr, Philip},
  journal={NeurIPS},
  year={2019}
}

@inproceedings{ramesh2021zero,
  title={Zero-shot text-to-image generation},
  author={Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
  booktitle={ICML},
  year={2021}
}

@inproceedings{zhou2018end,
  title={End-to-end dense video captioning with masked transformer},
  author={Zhou, Luowei and Zhou, Yingbo and Corso, Jason J and Socher, Richard and Xiong, Caiming},
  booktitle={CVPR},
  year={2018}
}

@inproceedings{shao2023prompting,
  title={Prompting large language models with answer heuristics for knowledge-based visual question answering},
  author={Shao, Zhenwei and Yu, Zhou and Wang, Meng and Yu, Jun},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{huang2019attention,
  title={Attention on attention for image captioning},
  author={Huang, Lun and Wang, Wenmin and Chen, Jie and Wei, Xiao-Yong},
  booktitle={ICCV},
  year={2019}
}

@inproceedings{le2020hierarchical,
  title={Hierarchical conditional relation networks for video question answering},
  author={Le, Thao Minh and Le, Vuong and Venkatesh, Svetha and Tran, Truyen},
  booktitle={CVPR},
  year={2020}
}

@inproceedings{aneja2018convolutional,
  title={Convolutional image captioning},
  author={Aneja, Jyoti and Deshpande, Aditya and Schwing, Alexander G},
  booktitle={CVPR},
  year={2018}
}

@inproceedings{you2016image,
  title={Image captioning with semantic attention},
  author={You, Quanzeng and Jin, Hailin and Wang, Zhaowen and Fang, Chen and Luo, Jiebo},
  booktitle={CVPR},
  year={2016}
}

@inproceedings{chen2022rethinking,
  title={Rethinking data augmentation for robust visual question answering},
  author={Chen, Long and Zheng, Yuhang and Xiao, Jun},
  booktitle={ECCV},
  year={2022}
}

@inproceedings{yu2019deep,
  title={Deep modular co-attention networks for visual question answering},
  author={Yu, Zhou and Yu, Jun and Cui, Yuhao and Tao, Dacheng and Tian, Qi},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{jiang2020defense,
  title={In defense of grid features for visual question answering},
  author={Jiang, Huaizu and Misra, Ishan and Rohrbach, Marcus and Learned-Miller, Erik and Chen, Xinlei},
  booktitle={CVPR},
  year={2020}
}

@inproceedings{antol2015vqa,
  title={Vqa: Visual question answering},
  author={Antol, Stanislaw and Agrawal, Aishwarya and Lu, Jiasen and Mitchell, Margaret and Batra, Dhruv and Zitnick, C Lawrence and Parikh, Devi},
  booktitle={ICCV},
  year={2015}
}

@inproceedings{yang2023vid2seq,
  title={Vid2seq: Large-scale pretraining of a visual language model for dense video captioning},
  author={Yang, Antoine and Nagrani, Arsha and Seo, Paul Hongsuck and Miech, Antoine and Pont-Tuset, Jordi and Laptev, Ivan and Sivic, Josef and Schmid, Cordelia},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{seo2022end,
  title={End-to-end generative pretraining for multimodal video captioning},
  author={Seo, Paul Hongsuck and Nagrani, Arsha and Arnab, Anurag and Schmid, Cordelia},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{iashin2020multi,
  title={Multi-modal dense video captioning},
  author={Iashin, Vladimir and Rahtu, Esa},
  booktitle={CVPR},
  year={2020}
}

@inproceedings{shen2023accurate,
  title={Accurate and Fast Compressed Video Captioning},
  author={Shen, Yaojie and Gu, Xin and Xu, Kai and Fan, Heng and Wen, Longyin and Zhang, Libo},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{zhou2023joint,
  title={Joint Visual Grounding and Tracking with Natural Language Specification},
  author={Zhou, Li and Zhou, Zikun and Mao, Kaige and He, Zhenyu},
  booktitle={CVPR},
  year={2023}
}

@article{guo2022divert,
  title={Divert more attention to vision-language tracking},
  author={Guo, Mingzhe and Zhang, Zhipeng and Fan, Heng and Jing, Liping},
  journal={NeurIPS},
  year={2022}
}

@inproceedings{mu2022slip,
  title={Slip: Self-supervision meets language-image pre-training},
  author={Mu, Norman and Kirillov, Alexander and Wagner, David and Xie, Saining},
  booktitle={ECCV},
  year={2022}
}

@article{alayrac2022flamingo,
  title={Flamingo: a visual language model for few-shot learning},
  author={Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and others},
  journal={NeurIPS},
  year={2022}
}

@inproceedings{li2022grounded,
  title={Grounded language-image pre-training},
  author={Li, Liunian Harold and Zhang, Pengchuan and Zhang, Haotian and Yang, Jianwei and Li, Chunyuan and Zhong, Yiwu and Wang, Lijuan and Yuan, Lu and Zhang, Lei and Hwang, Jenq-Neng and others},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{kim2021vilt,
  title={Vilt: Vision-and-language transformer without convolution or region supervision},
  author={Kim, Wonjae and Son, Bokyung and Kim, Ildoo},
  booktitle={ICML},
  year={2021}
}

@inproceedings{desai2021virtex,
  title={Virtex: Learning visual representations from textual annotations},
  author={Desai, Karan and Johnson, Justin},
  booktitle={CVPR},
  year={2021}
}

@inproceedings{chen2020uniter,
  title={Uniter: Universal image-text representation learning},
  author={Chen, Yen-Chun and Li, Linjie and Yu, Licheng and El Kholy, Ahmed and Ahmed, Faisal and Gan, Zhe and Cheng, Yu and Liu, Jingjing},
  booktitle={ECCV},
  year={2020}
}

@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={ICML},
  year={2021}
}

@inproceedings{STGRN,
  author       = {Zhu Zhang and
                  Zhou Zhao and
                  Yang Zhao and
                  Qi Wang and
                  Huasheng Liu and
                  Lianli Gao},
  title        = {Where Does It Exist: Spatio-Temporal Video Grounding for Multi-Form
                  Sentences},
  booktitle    = {CVPR},
  year         = {2020}
}

@inproceedings{STVGBert,
  author       = {Rui Su and
                  Qian Yu and
                  Dong Xu},
  title        = {STVGBert: {A} Visual-linguistic Transformer based Framework for Spatio-temporal
                  Video Grounding},
  booktitle    = {ICCV},
  year         = {2021}
}

@inproceedings{TubeDETR,
  author       = {Antoine Yang and
                  Antoine Miech and
                  Josef Sivic and
                  Ivan Laptev and
                  Cordelia Schmid},
  title        = {TubeDETR: Spatio-Temporal Video Grounding with Transformers},
  booktitle    = {CVPR},
  year         = {2022}
}

@article{WSSTG,
  title={Weakly-supervised spatio-temporally grounding natural sentence in video},
  author={Chen, Zhenfang and Ma, Lin and Luo, Wenhan and Wong, Kwan-Yee K},
  year={2019}
}

@inproceedings{CLIP,
  author    = {Alec Radford and
               Jong Wook Kim and
               Chris Hallacy and
               Aditya Ramesh and
               Gabriel Goh and
               Sandhini Agarwal and
               Girish Sastry and
               Amanda Askell and
               Pamela Mishkin and
               Jack Clark and
               Gretchen Krueger and
               Ilya Sutskever},
  title     = {Learning Transferable Visual Models From Natural Language Supervision},
  booktitle = {ICML},
  pages     = {8748--8763},
  year      = {2021},
}

@inproceedings{wheredoseit,
  author       = {Zhu Zhang and
                  Zhou Zhao and
                  Yang Zhao and
                  Qi Wang and
                  Huasheng Liu and
                  Lianli Gao},
  title        = {Where Does It Exist: Spatio-Temporal Video Grounding for Multi-Form
                  Sentences},
  booktitle    = {CVPR},
  year         = {2020}
}

@inproceedings{STCAT,
  author       = {Yang Jin and
                  Yongzhi Li and
                  Zehuan Yuan and
                  Yadong Mu},
  title        = {Embracing Consistency: {A} One-Stage Approach for Spatio-Temporal
                  Video Grounding},
  booktitle    = {NeurIPS},
  year         = {2022},
  timestamp    = {Fri, 05 May 2023 16:00:57 +0200},
}

@inproceedings{csdvl,
  title={Collaborative Static and Dynamic Vision-Language Streams for Spatio-Temporal Video Grounding},
  author={Lin, Zihang and Tan, Chaolei and Hu, Jian-Fang and Jin, Zhi and Ye, Tiancai and Zheng, Wei-Shi},
  booktitle={CVPR},
  year={2023}
}

@article{hcstvg,
  title={Human-centric spatio-temporal video grounding with visual transformers},
  author={Tang, Zongheng and Liao, Yue and Liu, Si and Li, Guanbin and Jin, Xiaojie and Jiang, Hongxu and Yu, Qian and Xu, Dong},
  journal={IEEE TCSVT},
  volume={32},
  number={12},
  pages={8238--8249},
  year={2021},
  publisher={IEEE}
}

@inproceedings{resnet,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={CVPR},
  year={2016}
}

@inproceedings{vidswin,
  title={Video swin transformer},
  author={Liu, Ze and Ning, Jia and Cao, Yue and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Hu, Han},
  booktitle={CVPR},
  year={2022}
}

@article{roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@inproceedings{mdetr,
  title={Mdetr-modulated detection for end-to-end multi-modal understanding},
  author={Kamath, Aishwarya and Singh, Mannat and LeCun, Yann and Synnaeve, Gabriel and Misra, Ishan and Carion, Nicolas},
  booktitle={ICCV},
  year={2021}
}

@inproceedings{faster,
  title={Faster r-cnn: Towards real-time object detection with region proposal networks},
  author={Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  booktitle={NIPS},
  year={2015}
}

@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={NIPS},
  year={2017}
}

@inproceedings{zhang2023text,
  title={Text-visual prompting for efficient 2d temporal video grounding},
  author={Zhang, Yimeng and Chen, Xin and Jia, Jinghan and Liu, Sijia and Ding, Ke},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{wang2023protege,
  title={ProTeGe: Untrimmed Pretraining for Video Temporal Grounding by Video Temporal Grounding},
  author={Wang, Lan and Mittal, Gaurav and Sajeev, Sandra and Yu, Ye and Hall, Matthew and Boddeti, Vishnu Naresh and Chen, Mei},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{hao2022can,
  title={Can shuffling video benefit temporal bias problem: A novel training framework for temporal grounding},
  author={Hao, Jiachang and Sun, Haifeng and Ren, Pengfei and Wang, Jingyu and Qi, Qi and Liao, Jianxin},
  booktitle={ECCV},
  year={2022}
}

@inproceedings{mun2020local,
  title={Local-global video-text interactions for temporal grounding},
  author={Mun, Jonghwan and Cho, Minsu and Han, Bohyung},
  booktitle={CVPR},
  year={2020}
}

@inproceedings{lmmg,
  title={Localizing moments in long video via multimodal guidance},
  author={Barrios, Wayner and Soldan, Mattia and Heilbron, Fabian Caba and Ceballos-Arroyo, Alberto Mario and Ghanem, Bernard},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{nsm,
  title={Negative sample matters: A renaissance of metric learning for temporal grounding},
  author={Wang, Zhenzhi and Wang, Limin and Wu, Tao and Li, Tianhao and Wu, Gangshan},
  booktitle={AAAI},
  year={2022}
}

@inproceedings{locvtp,
  title={Locvtp: Video-text pre-training for temporal localization},
  author={Cao, Meng and Yang, Tianyu and Weng, Junwu and Zhang, Can and Wang, Jue and Zou, Yuexian},
  booktitle={ECCV},
  year={2022}
}

@inproceedings{zhu2020vision,
  title={Vision-language navigation with self-supervised auxiliary reasoning tasks},
  author={Zhu, Fengda and Zhu, Yi and Chang, Xiaojun and Liang, Xiaodan},
  booktitle={CVPR},
  year={2020}
}

@inproceedings{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  booktitle={ICLR},
  year={2015}
}

@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={NeurIPS},
  year={2019}
}

@inproceedings{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  booktitle={ICLR},
  year={2021}
}

@inproceedings{yang2022lavt,
  title={Lavt: Language-aware vision transformer for referring image segmentation},
  author={Yang, Zhao and Wang, Jiaqi and Tang, Yansong and Chen, Kai and Zhao, Hengshuang and Torr, Philip HS},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{liu2023gres,
  title={Gres: Generalized referring expression segmentation},
  author={Liu, Chang and Ding, Henghui and Jiang, Xudong},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{li2023improving,
  title={Improving vision-and-language navigation by generating future-view image semantics},
  author={Li, Jialu and Bansal, Mohit},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{zheng2023less,
  title={Less is more: Focus attention for efficient detr},
  author={Zheng, Dehua and Dong, Wenhui and Hu, Hailin and Chen, Xinghao and Wang, Yunhe},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{ye2023cascade,
  title={Cascade-DETR: Delving into High-Quality Universal Object Detection},
  author={Ye, Mingqiao and Ke, Lei and Li, Siyuan and Tai, Yu-Wing and Tang, Chi-Keung and Danelljan, Martin and Yu, Fisher},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{zhu2020deformable,
  title={Deformable detr: Deformable transformers for end-to-end object detection},
  author={Zhu, Xizhou and Su, Weijie and Lu, Lewei and Li, Bin and Wang, Xiaogang and Dai, Jifeng},
  booktitle={ICLR},
  year={2021}
}

@inproceedings{sun2021rethinking,
  title={Rethinking transformer-based set prediction for object detection},
  author={Sun, Zhiqing and Cao, Shengcao and Yang, Yiming and Kitani, Kris M},
  booktitle={ICCV},
  year={2021}
}

@inproceedings{lin2023univtg,
  title={Univtg: Towards unified video-language temporal grounding},
  author={Lin, Kevin Qinghong and Zhang, Pengchuan and Chen, Joya and Pramanick, Shraman and Gao, Difei and Wang, Alex Jinpeng and Yan, Rui and Shou, Mike Zheng},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{drft,
  title={End-to-end multi-modal video temporal grounding},
  author={Chen, Yi-Wen and Tsai, Yi-Hsuan and Yang, Ming-Hsuan},
  booktitle={NeurIPS},
  year={2021}
}

@inproceedings{how2100m,
  author    = {Antoine Miech and
               Dimitri Zhukov and
               Jean{-}Baptiste Alayrac and
               Makarand Tapaswi and
               Ivan Laptev and
               Josef Sivic},
  title     = {How{T}o100{M}: Learning a Text-Video Embedding by Watching Hundred Million
               Narrated Video Clips},
  booktitle = {ICCV},
  year      = {2019}
}

@inproceedings{webvid,
  author    = {Max Bain and
               Arsha Nagrani and
               G{\"{u}}l Varol and
               Andrew Zisserman},
  title     = {Frozen in Time: {A} Joint Video and Image Encoder for End-to-End Retrieval},
  booktitle = {ICCV},
  year      = {2021}
}

@inproceedings{mvgpt,
  title={End-to-end generative pretraining for multimodal video captioning},
  author={Seo, Paul Hongsuck and Nagrani, Arsha and Arnab, Anurag and Schmid, Cordelia},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{blip,
  title={Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation},
  author={Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},
  booktitle={ICML},
  year={2022}
}

@article{coot,
  title={Coot: Cooperative hierarchical transformer for video-text representation learning},
  author={Ging, Simon and Zolfaghari, Mohammadreza and Pirsiavash, Hamed and Brox, Thomas},
  journal={NeurIPS},
  year={2020}
}

@inproceedings{hmn,
  title={Hierarchical modular network for video captioning},
  author={Ye, Hanhua and Li, Guorong and Qi, Yuankai and Wang, Shuhui and Huang, Qingming and Yang, Ming-Hsuan},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{cc12m,
  title={Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts},
  author={Changpinyo, Soravit and Sharma, Piyush and Ding, Nan and Soricut, Radu},
  booktitle={CVPR},
  year={2021}
}

@inproceedings{lavender,
  title={Lavender: Unifying video-language understanding as masked language modeling},
  author={Li, Linjie and Gan, Zhe and Lin, Kevin and Lin, Chung-Ching and Liu, Zicheng and Liu, Ce and Wang, Lijuan},
  booktitle={CVPR},
  year={2023}
}

@article{git,
  title={Git: A generative image-to-text transformer for vision and language},
  author={Wang, Jianfeng and Yang, Zhengyuan and Hu, Xiaowei and Li, Linjie and Lin, Kevin and Gan, Zhe and Liu, Zicheng and Liu, Ce and Wang, Lijuan},
  journal={arXiv},
  year={2022}
}

@article{mplug,
  title={mplug: Effective and efficient vision-language learning by cross-modal skip-connections},
  author={Li, Chenliang and Xu, Haiyang and Tian, Junfeng and Wang, Wei and Yan, Ming and Bi, Bin and Ye, Jiabo and Chen, Hehong and Xu, Guohai and Cao, Zheng and others},
  journal={arXiv},
  year={2022}
}

@article{pali,
  title={Pali: A jointly-scaled multilingual language-image model},
  author={Chen, Xi and Wang, Xiao and Changpinyo, Soravit and Piergiovanni, AJ and Padlewski, Piotr and Salz, Daniel and Goodman, Sebastian and Grycner, Adam and Mustafa, Basil and Beyer, Lucas and others},
  journal={arXiv},
  year={2022}
}

@inproceedings{dert,
  title={End-to-end object detection with transformers},
  author={Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
  booktitle={ECCV},
  year={2020}
}

@inproceedings{tan2024siamese,
  title={Siamese Learning with Joint Alignment and Regression for Weakly-Supervised Video Paragraph Grounding},
  author={Tan, Chaolei and Lai, Jianhuang and Zheng, Wei-Shi and Hu, Jian-Fang},
  booktitle={CVPR},
  year={2024}
}

@article{pcc,
  title={2rd Place Solutions in the HC-STVG track of Person in Context Challenge 2021},
  author={Yi Yu and Xinying Wang and Wei Hu and Xun Luo and Cheng Li},
  journal={arXiv},
  year={2021}
}

@inproceedings{mmn,
  title={Negative sample matters: A renaissance of metric learning for temporal grounding},
  author={Wang, Zhenzhi and Wang, Limin and Wu, Tao and Li, Tianhao and Wu, Gangshan},
  booktitle={AAAI},
  year={2022}
}

@article{2d-tan,
  title={Augmented 2d-tan: A two-stage approach for human-centric spatio-temporal video grounding},
  author={Tan, Chaolei and Lin, Zihang and Hu, Jian-Fang and Li, Xiang and Zheng, Wei-Shi},
  journal={arXiv},
  year={2021}
}

@inproceedings{carion2020end,
	title={End-to-end object detection with transformers},
	author={Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
	booktitle={ECCV},
	year={2020}
}

@inproceedings{he2017mask,
	title={Mask r-cnn},
	author={He, Kaiming and Gkioxari, Georgia and Doll{\'a}r, Piotr and Girshick, Ross},
	booktitle={ICCV},
	year={2017}
}

@inproceedings{jiang2018acquisition,
	title={Acquisition of localization confidence for accurate object detection},
	author={Jiang, Borui and Luo, Ruixuan and Mao, Jiayuan and Xiao, Tete and Jiang, Yuning},
	booktitle={ECCV},
	year={2018}
}

@inproceedings{tall,
  title={Tall: Temporal activity localization via language query},
  author={Gao, Jiyang and Sun, Chen and Yang, Zhenheng and Nevatia, Ram},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={5267--5275},
  year={2017}
}

@inproceedings{lnet,
  title={Localizing natural language in videos},
  author={Chen, Jingyuan and Ma, Lin and Chen, Xinpeng and Jie, Zequn and Luo, Jiebo},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  number={01},
  pages={8175--8182},
  year={2019}
}

@inproceedings{cgstvg,
  title={Context-Guided Spatio-Temporal Video Grounding},
  author={Gu, Xin and Fan, Heng and Huang, Yan and Luo, Tiejian and Zhang, Libo},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{ts1,
  title={Multilevel language and vision integration for text-to-clip retrieval},
  author={Xu, Huijuan and He, Kun and Plummer, Bryan A and Sigal, Leonid and Sclaroff, Stan and Saenko, Kate},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  pages={9062--9069},
  year={2019}
}

@inproceedings{ts2,
  title={Semantic proposal for activity localization in videos via sentence query},
  author={Chen, Shaoxiang and Jiang, Yu-Gang},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  pages={8199--8206},
  year={2019}
}

@inproceedings{ts3,
  title={Language-driven temporal activity localization: A semantic matching reinforcement learning model},
  author={Wang, Weining and Huang, Yan and Wang, Liang},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={334--343},
  year={2019}
}

@article{os1,
  title={End-to-end multi-modal video temporal grounding},
  author={Chen, Yi-Wen and Tsai, Yi-Hsuan and Yang, Ming-Hsuan},
  journal={Advances in Neural Information Processing Systems},
  pages={28442--28453},
  year={2021}
}

@inproceedings{os2,
  title={Local-global video-text interactions for temporal grounding},
  author={Mun, Jonghwan and Cho, Minsu and Han, Bohyung},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10810--10819},
  year={2020}
}

@inproceedings{os3,
  title={Dense regression network for video grounding},
  author={Zeng, Runhao and Xu, Haoming and Huang, Wenbing and Chen, Peihao and Tan, Mingkui and Gan, Chuang},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10287--10296},
  year={2020}
}

@article{deformable,
  title={Deformable detr: Deformable transformers for end-to-end object detection},
  author={Zhu, Xizhou and Su, Weijie and Lu, Lewei and Li, Bin and Wang, Xiaogang and Dai, Jifeng},
  year={2020}
}

@inproceedings{anchor,
  title={Anchor detr: Query design for transformer-based detector},
  author={Wang, Yingming and Zhang, Xiangyu and Yang, Tong and Sun, Jian},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  pages={2567--2575},
  year={2022}
}

@article{dino,
  title={Dino: Detr with improved denoising anchor boxes for end-to-end object detection},
  author={Zhang, Hao and Li, Feng and Liu, Shilong and Zhang, Lei and Su, Hang and Zhu, Jun and Ni, Lionel M and Shum, Heung-Yeung},
  journal={arXiv preprint arXiv:2203.03605},
  year={2022}
}

@inproceedings{nms,
  title={Efficient non-maximum suppression},
  author={Neubeck, Alexander and Van Gool, Luc},
  booktitle={18th international conference on pattern recognition (ICPR'06)},
  pages={850--855},
  year={2006},
  organization={IEEE}
}

@article{dabdetr,
  title={Dab-detr: Dynamic anchor boxes are better queries for detr},
  author={Liu, Shilong and Li, Feng and Zhang, Hao and Yang, Xiao and Qi, Xianbiao and Su, Hang and Zhu, Jun and Zhang, Lei},
  journal={arXiv preprint arXiv:2201.12329},
  year={2022}
}

@inproceedings{wang2023efficient,
  title={Efficient Spatio-Temporal Video Grounding with Semantic-Guided Feature Decomposition},
  author={Wang, Weikang and Liu, Jing and Su, Yuting and Nie, Weizhi},
  booktitle={Proceedings of the 31st ACM International Conference on Multimedia},
  pages={4867--4876},
  year={2023}
}

@inproceedings{wang2023deconfounded,
  title={Deconfounded Multimodal Learning for Spatio-temporal Video Grounding},
  author={Wang, Jiawei and Ma, Zhanchang and Cao, Da and Le, Yuquan and Xiao, Junbin and Chua, Tat-Seng},
  booktitle={Proceedings of the 31st ACM International Conference on Multimedia},
  pages={7521--7529},
  year={2023}
}

@inproceedings{ren2024pixellm,
  title={Pixellm: Pixel reasoning with large multimodal model},
  author={Ren, Zhongwei and Huang, Zhicheng and Wei, Yunchao and Zhao, Yao and Fu, Dongmei and Feng, Jiashi and Jin, Xiaojie},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={26374--26383},
  year={2024}
}

@article{wang2024reconstructive,
  title={Reconstructive visual instruction tuning},
  author={Wang, Haochen and Zheng, Anlin and Zhao, Yucheng and Wang, Tiancai and Ge, Zheng and Zhang, Xiangyu and Zhang, Zhaoxiang},
  journal={arXiv preprint arXiv:2410.09575},
  year={2024}
}

@article{wang2024droppos,
  title={Droppos: Pre-training vision transformers by reconstructing dropped positions},
  author={Wang, Haochen and Fan, Junsong and Wang, Yuxi and Song, Kaiyou and Wang, Tong and ZHANG, ZHAO-XIANG},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{wang2023hard,
  title={Hard patches mining for masked image modeling},
  author={Wang, Haochen and Song, Kaiyou and Fan, Junsong and Wang, Yuxi and Xie, Jin and Zhang, Zhaoxiang},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10375--10385},
  year={2023}
}

@article{MReward,
  author       = {Xin Gu and
                  Ming Li and
                  Libo Zhang and
                  Fan Chen and
                  Longyin Wen and
                  Tiejian Luo and
                  Sijie Zhu},
  title        = {Multi-Reward as Condition for Instruction-based Image Editing},
  journal      = {CoRR},
  volume       = {abs/2411.04713},
  year         = {2024},
}

@inproceedings{gu2023text,
  title={Text with knowledge graph augmented transformer for video captioning},
  author={Gu, Xin and Chen, Guang and Wang, Yufei and Zhang, Libo and Luo, Tiejian and Wen, Longyin},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={18941--18951},
  year={2023}
}

@article{gu2022dual,
  title={Dual-stream transformer for generic event boundary captioning},
  author={Gu, Xin and Ye, Hanhua and Chen, Guang and Wang, Yufei and Zhang, Libo and Wen, Longyin},
  journal={arXiv preprint arXiv:2207.03038},
  year={2022}
}

@article{han2023shot2story20k,
  title={Shot2story20k: A new benchmark for comprehensive understanding of multi-shot videos},
  author={Han, Mingfei and Yang, Linjie and Chang, Xiaojun and Wang, Heng},
  journal={arXiv preprint arXiv:2312.10300},
  year={2023}
}

@inproceedings{weng2024longvlm,
  title={Longvlm: Efficient long video understanding via large language models},
  author={Weng, Yuetian and Han, Mingfei and He, Haoyu and Chang, Xiaojun and Zhuang, Bohan},
  booktitle={European Conference on Computer Vision},
  pages={453--470},
  year={2024},
  organization={Springer}
}