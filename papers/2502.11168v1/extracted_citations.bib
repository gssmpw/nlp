@article{MReward,
  author       = {Xin Gu and
                  Ming Li and
                  Libo Zhang and
                  Fan Chen and
                  Longyin Wen and
                  Tiejian Luo and
                  Sijie Zhu},
  title        = {Multi-Reward as Condition for Instruction-based Image Editing},
  journal      = {CoRR},
  volume       = {abs/2411.04713},
  year         = {2024},
}

@inproceedings{OAMBRN,
  author       = {Zhu Zhang and
                  Zhou Zhao and
                  Zhijie Lin and
                  Baoxing Huai and
                  Jing Yuan},
  title        = {Object-Aware Multi-Branch Relation Networks for Spatio-Temporal Video
                  Grounding},
  booktitle    = {IJCAI},
  year         = {2020}
}

@inproceedings{STCAT,
  author       = {Yang Jin and
                  Yongzhi Li and
                  Zehuan Yuan and
                  Yadong Mu},
  title        = {Embracing Consistency: {A} One-Stage Approach for Spatio-Temporal
                  Video Grounding},
  booktitle    = {NeurIPS},
  year         = {2022},
  timestamp    = {Fri, 05 May 2023 16:00:57 +0200},
}

@inproceedings{STGRN,
  author       = {Zhu Zhang and
                  Zhou Zhao and
                  Yang Zhao and
                  Qi Wang and
                  Huasheng Liu and
                  Lianli Gao},
  title        = {Where Does It Exist: Spatio-Temporal Video Grounding for Multi-Form
                  Sentences},
  booktitle    = {CVPR},
  year         = {2020}
}

@inproceedings{STVGBert,
  author       = {Rui Su and
                  Qian Yu and
                  Dong Xu},
  title        = {STVGBert: {A} Visual-linguistic Transformer based Framework for Spatio-temporal
                  Video Grounding},
  booktitle    = {ICCV},
  year         = {2021}
}

@inproceedings{TubeDETR,
  author       = {Antoine Yang and
                  Antoine Miech and
                  Josef Sivic and
                  Ivan Laptev and
                  Cordelia Schmid},
  title        = {TubeDETR: Spatio-Temporal Video Grounding with Transformers},
  booktitle    = {CVPR},
  year         = {2022}
}

@inproceedings{antol2015vqa,
  title={Vqa: Visual question answering},
  author={Antol, Stanislaw and Agrawal, Aishwarya and Lu, Jiasen and Mitchell, Margaret and Batra, Dhruv and Zitnick, C Lawrence and Parikh, Devi},
  booktitle={ICCV},
  year={2015}
}

@inproceedings{cgstvg,
  title={Context-Guided Spatio-Temporal Video Grounding},
  author={Gu, Xin and Fan, Heng and Huang, Yan and Luo, Tiejian and Zhang, Libo},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{csdvl,
  title={Collaborative Static and Dynamic Vision-Language Streams for Spatio-Temporal Video Grounding},
  author={Lin, Zihang and Tan, Chaolei and Hu, Jian-Fang and Jin, Zhi and Ye, Tiancai and Zheng, Wei-Shi},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{dert,
  title={End-to-end object detection with transformers},
  author={Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
  booktitle={ECCV},
  year={2020}
}

@inproceedings{drft,
  title={End-to-end multi-modal video temporal grounding},
  author={Chen, Yi-Wen and Tsai, Yi-Hsuan and Yang, Ming-Hsuan},
  booktitle={NeurIPS},
  year={2021}
}

@article{gu2022dual,
  title={Dual-stream transformer for generic event boundary captioning},
  author={Gu, Xin and Ye, Hanhua and Chen, Guang and Wang, Yufei and Zhang, Libo and Wen, Longyin},
  journal={arXiv preprint arXiv:2207.03038},
  year={2022}
}

@inproceedings{gu2023text,
  title={Text with knowledge graph augmented transformer for video captioning},
  author={Gu, Xin and Chen, Guang and Wang, Yufei and Zhang, Libo and Luo, Tiejian and Wen, Longyin},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={18941--18951},
  year={2023}
}

@article{guo2022divert,
  title={Divert more attention to vision-language tracking},
  author={Guo, Mingzhe and Zhang, Zhipeng and Fan, Heng and Jing, Liping},
  journal={NeurIPS},
  year={2022}
}

@article{han2023shot2story20k,
  title={Shot2story20k: A new benchmark for comprehensive understanding of multi-shot videos},
  author={Han, Mingfei and Yang, Linjie and Chang, Xiaojun and Wang, Heng},
  journal={arXiv preprint arXiv:2312.10300},
  year={2023}
}

@inproceedings{hao2022can,
  title={Can shuffling video benefit temporal bias problem: A novel training framework for temporal grounding},
  author={Hao, Jiachang and Sun, Haifeng and Ren, Pengfei and Wang, Jingyu and Qi, Qi and Liao, Jianxin},
  booktitle={ECCV},
  year={2022}
}

@article{hcstvg,
  title={Human-centric spatio-temporal video grounding with visual transformers},
  author={Tang, Zongheng and Liao, Yue and Liu, Si and Li, Guanbin and Jin, Xiaojie and Jiang, Hongxu and Yu, Qian and Xu, Dong},
  journal={IEEE TCSVT},
  volume={32},
  number={12},
  pages={8238--8249},
  year={2021},
  publisher={IEEE}
}

@inproceedings{jiang2020defense,
  title={In defense of grid features for visual question answering},
  author={Jiang, Huaizu and Misra, Ishan and Rohrbach, Marcus and Learned-Miller, Erik and Chen, Xinlei},
  booktitle={CVPR},
  year={2020}
}

@inproceedings{li2023improving,
  title={Improving vision-and-language navigation by generating future-view image semantics},
  author={Li, Jialu and Bansal, Mohit},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{lin2023univtg,
  title={Univtg: Towards unified video-language temporal grounding},
  author={Lin, Kevin Qinghong and Zhang, Pengchuan and Chen, Joya and Pramanick, Shraman and Gao, Difei and Wang, Alex Jinpeng and Yan, Rui and Shou, Mike Zheng},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{liu2023gres,
  title={Gres: Generalized referring expression segmentation},
  author={Liu, Chang and Ding, Henghui and Jiang, Xudong},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{lmmg,
  title={Localizing moments in long video via multimodal guidance},
  author={Barrios, Wayner and Soldan, Mattia and Heilbron, Fabian Caba and Ceballos-Arroyo, Alberto Mario and Ghanem, Bernard},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{locvtp,
  title={Locvtp: Video-text pre-training for temporal localization},
  author={Cao, Meng and Yang, Tianyu and Weng, Junwu and Zhang, Can and Wang, Jue and Zou, Yuexian},
  booktitle={ECCV},
  year={2022}
}

@inproceedings{mun2020local,
  title={Local-global video-text interactions for temporal grounding},
  author={Mun, Jonghwan and Cho, Minsu and Han, Bohyung},
  booktitle={CVPR},
  year={2020}
}

@inproceedings{ramesh2021zero,
  title={Zero-shot text-to-image generation},
  author={Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
  booktitle={ICML},
  year={2021}
}

@inproceedings{ren2024pixellm,
  title={Pixellm: Pixel reasoning with large multimodal model},
  author={Ren, Zhongwei and Huang, Zhicheng and Wei, Yunchao and Zhao, Yao and Fu, Dongmei and Feng, Jiashi and Jin, Xiaojie},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={26374--26383},
  year={2024}
}

@inproceedings{shao2023prompting,
  title={Prompting large language models with answer heuristics for knowledge-based visual question answering},
  author={Shao, Zhenwei and Yu, Zhou and Wang, Meng and Yu, Jun},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{shen2023accurate,
  title={Accurate and Fast Compressed Video Captioning},
  author={Shen, Yaojie and Gu, Xin and Xu, Kai and Fan, Heng and Wen, Longyin and Zhang, Libo},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{sun2021rethinking,
  title={Rethinking transformer-based set prediction for object detection},
  author={Sun, Zhiqing and Cao, Shengcao and Yang, Yiming and Kitani, Kris M},
  booktitle={ICCV},
  year={2021}
}

@inproceedings{talal2023video,
  title={Video-GroundingDINO: Towards Open-Vocabulary Spatio-Temporal Video Grounding},
  author={Talal Wasim, Syed and Naseer, Muzammal and Khan, Salman and Yang, Ming-Hsuan and Shahbaz Khan, Fahad},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={NIPS},
  year={2017}
}

@inproceedings{wang2023hard,
  title={Hard patches mining for masked image modeling},
  author={Wang, Haochen and Song, Kaiyou and Fan, Junsong and Wang, Yuxi and Xie, Jin and Zhang, Zhaoxiang},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10375--10385},
  year={2023}
}

@inproceedings{wang2023protege,
  title={ProTeGe: Untrimmed Pretraining for Video Temporal Grounding by Video Temporal Grounding},
  author={Wang, Lan and Mittal, Gaurav and Sajeev, Sandra and Yu, Ye and Hall, Matthew and Boddeti, Vishnu Naresh and Chen, Mei},
  booktitle={CVPR},
  year={2023}
}

@article{wang2024droppos,
  title={Droppos: Pre-training vision transformers by reconstructing dropped positions},
  author={Wang, Haochen and Fan, Junsong and Wang, Yuxi and Song, Kaiyou and Wang, Tong and ZHANG, ZHAO-XIANG},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{wang2024reconstructive,
  title={Reconstructive visual instruction tuning},
  author={Wang, Haochen and Zheng, Anlin and Zhao, Yucheng and Wang, Tiancai and Ge, Zheng and Zhang, Xiangyu and Zhang, Zhaoxiang},
  journal={arXiv preprint arXiv:2410.09575},
  year={2024}
}

@inproceedings{weng2024longvlm,
  title={Longvlm: Efficient long video understanding via large language models},
  author={Weng, Yuetian and Han, Mingfei and He, Haoyu and Chang, Xiaojun and Zhuang, Bohan},
  booktitle={European Conference on Computer Vision},
  pages={453--470},
  year={2024},
  organization={Springer}
}

@inproceedings{yang2022lavt,
  title={Lavt: Language-aware vision transformer for referring image segmentation},
  author={Yang, Zhao and Wang, Jiaqi and Tang, Yansong and Chen, Kai and Zhao, Hengshuang and Torr, Philip HS},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{ye2023cascade,
  title={Cascade-DETR: Delving into High-Quality Universal Object Detection},
  author={Ye, Mingqiao and Ke, Lei and Li, Siyuan and Tai, Yu-Wing and Tang, Chi-Keung and Danelljan, Martin and Yu, Fisher},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{you2016image,
  title={Image captioning with semantic attention},
  author={You, Quanzeng and Jin, Hailin and Wang, Zhaowen and Fang, Chen and Luo, Jiebo},
  booktitle={CVPR},
  year={2016}
}

@inproceedings{zhang2023text,
  title={Text-visual prompting for efficient 2d temporal video grounding},
  author={Zhang, Yimeng and Chen, Xin and Jia, Jinghan and Liu, Sijia and Ding, Ke},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{zheng2023less,
  title={Less is more: Focus attention for efficient detr},
  author={Zheng, Dehua and Dong, Wenhui and Hu, Hailin and Chen, Xinghao and Wang, Yunhe},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{zhou2023joint,
  title={Joint Visual Grounding and Tracking with Natural Language Specification},
  author={Zhou, Li and Zhou, Zikun and Mao, Kaige and He, Zhenyu},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{zhu2020deformable,
  title={Deformable detr: Deformable transformers for end-to-end object detection},
  author={Zhu, Xizhou and Su, Weijie and Lu, Lewei and Li, Bin and Wang, Xiaogang and Dai, Jifeng},
  booktitle={ICLR},
  year={2021}
}

@inproceedings{zhu2020vision,
  title={Vision-language navigation with self-supervised auxiliary reasoning tasks},
  author={Zhu, Fengda and Zhu, Yi and Chang, Xiaojun and Liang, Xiaodan},
  booktitle={CVPR},
  year={2020}
}

