\PassOptionsToPackage{table}{xcolor}
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

% \usepackage[pagebackref,breaklinks,colorlinks,citecolor=eccvblue]{hyperref}
\definecolor{eccvblue}{rgb}{0.12,0.49,0.85}
\usepackage[breaklinks,colorlinks,citecolor=eccvblue]{hyperref}
\usepackage{url}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
% \usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{verbatim}
\usepackage{arydshln}
\usepackage{array}
\usepackage{amsmath}
\usepackage{wrapfig}
\usepackage{amssymb}
\usepackage{pifont}


\def\eg{{\em e.g.}}
\def\ie{{\em i.e.}}
\def\etal{{\em et al.}}
\definecolor{mygray}{gray}{0.9}
\definecolor{highlight}{RGB}{238,250,215}
\newcommand{\improve}[1]{\scriptsize{\textcolor[RGB]{61,145,64}{~(#1)}}}
\newcommand{\HF}[1]{{\textcolor[rgb]{1,0,0}{#1}}}
\newcommand{\REV}[1]{{\textcolor[rgb]{0,0,1}{#1}}}

\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}


\title{Knowing Your Target \raisebox{-0.15em}{\includegraphics[height=1em]{target.pdf}}: Target-Aware\\ Transformer Makes Better Spatio-Temporal Video Grounding}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Xin Gu$^{1}$~~~~ Yaojie Shen$^{2,1}$~~~~ Chenxi Luo$^{3}$~~~~ Tiejian Luo$^{1,*}$~~~~ Yan Huang$^{4}$~~~~ Yuewei Lin$^{5}$ \\ \textbf{Heng Fan}$^{4,\dagger}$~~~~ \textbf{Libo Zhang}$^{2,1,\dagger,*}$\\
$^{1}$University of Chinese Academy of Sciences \\ $^{2}$Institute of Software Chinese Academy of Sciences ~~~~~
$^{3}$La Trobe University \\$^{4}$University of North Texas ~~~~~$^{5}$Brookhaven National Laboratory\\
% Institution1 address\\
\tt\small guxin21@mails.ucas.edu.cn, 
{\tt\small libo@iscas.ac.cn}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle
\let\thefootnote\relax\footnotetext{
$^\dagger$Equal advising and co-last authors; $^*$Corresponding author}

\begin{abstract}
Transformer has attracted increasing interest in spatio-temporal video grounding, or STVG, owing to its end-to-end pipeline and promising result. Existing Transformer-based STVG approaches often leverage a set of object queries, which are initialized simply using zeros and then gradually learn target position information via iterative interactions with multimodal features, for spatial and temporal localization. Despite simplicity, these zero object queries, due to lacking target-specific cues, are hard to learn discriminative target information from interactions with multimodal features in complicated scenarios (\eg, with distractors or occlusion), resulting in degradation. Addressing this, we introduce a novel \emph{\textbf{T}arget-\textbf{A}ware Transformer for \textbf{STVG}} (\textbf{TA-STVG}), which seeks to adaptively generate object queries via exploring target-specific cues from the given video-text pair, for improving STVG. The key lies in two simple yet effective modules, comprising \emph{text-guided temporal sampling} (TTS) and \emph{attribute-aware spatial activation} (ASA), working in a cascade. The former focuses on selecting target-relevant temporal cues from a video utilizing holistic text information, while the latter aims at further exploiting the fine-grained visual attribute information of the object from previous target-aware temporal cues, which is applied for object query initialization. Compared to existing methods leveraging zero-initialized queries, object queries in our TA-STVG, directly generated from a given video-text pair, naturally carry target-specific cues, making them adaptive and better interact with multimodal features for learning more discriminative information to improve STVG. In our experiments on three benchmarks, including HCSTVG-v1/-v2 and VidSTG, TA-STVG achieves state-of-the-art performance and significantly outperforms the baseline, validating its efficacy. Moreover, TTS and ASA are designed for general purpose. When applied to existing methods such as TubeDETR and STCAT, we show substantial performance gains, verifying its generality. Code is released at \url{https://github.com/HengLan/TA-STVG}.
\end{abstract}


\section{Introduction}

Spatio-temporal video grounding (or \textbf{\emph{STVG}}) aims at \emph{spatially} and \emph{temporally} localizing the target of interest from an untrimmed video given its textual description~\citep{STGRN}. As a multimodal task, it requires to understand the spatio-temporal content from videos and accurately connect it to the corresponding textual expression. Owing to its importance in multimodal video understanding and key applications such as content-based video retrieval, robotics, etc, STVG has attracted increasing interest with many models proposed~\citep{OAMBRN,STGRN,hcstvg,STVGBert}.

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.945\linewidth]{fig1.pdf}
    \caption{Comparison between existing Transformer-based STVG methods applying zero-initialized queries for STVG in (a) and our proposed Target-Aware Transformer-based STVG generating queries with target-aware cues from video and text for STVG in (b). \emph{Best viewed in color for all figures}.}
    \vspace{-5mm}
    \label{fig:framework_comparison}
\end{figure}

Recently, inspired by the compact end-to-end pipeline of Transformer-based detection~\citep{dert}, researchers have introduced the Transformer~\citep{vaswani2017attention} for STVG given that they both are localization task, and achieved previously unattainable result~\citep{TubeDETR,STCAT,csdvl,cgstvg}. Similar to Transformer-based detection~\citep{dert}, existing Transformer-based STVG methods often adopt a set of spatial and temporal object queries, and then leverage them to learn target position information by iterative interactions with multimodal features (generated by the encoder) from video and text for spatio-temporal target localization. In these approaches~\citep{TubeDETR,STCAT,csdvl,cgstvg}, the object queries are usually simply initialized using \emph{zeros} (see Fig.~\ref{fig:framework_comparison} (a)). Despite promising results, such zero-initialized object queries, due to the lack of effective target-specific semantic cues, are difficult to learn discriminative target position information from the interactions with multimodal features in complicated scenarios, \eg, with distractors or occlusion, leading to performance degradation. 

\setlength{\columnsep}{10pt}%
\setlength\intextsep{0pt}
\begin{wrapfigure}{r}{0.47\textwidth}
\centering
%\vspace{-5mm}
\includegraphics[width=0.47\textwidth]{fig2.pdf}
\caption{Comparison of the zero-initialized queries and groundtruth-generated queries for STVG. We see the target-specific information in groundtruth largely enhances results.}
%\vspace{3mm}
\label{fig:2}
\end{wrapfigure}
In the decoder procedure, object queries expect to learn target position information from multimodal features. If \emph{object queries know the target from the very beginning}, or in other words, \emph{they know what to learn}, they can employ target-specific cues as a prior to guide themselves for better interaction with the multimodal features, which benefits learning more discriminative features for better localization. In order to validate this, we conduct the \emph{oracle} experiment by generating spatial and temporal object queries from groundtruth, and compare the performance using such groundtruth-generated queries and zero-initialized queries. The architecture in this oracle experiment simply consists of  feature extraction, an encoder and a decoder, and please refer to the supplementary material for details due to limited space. Fig.~\ref{fig:2} shows the comparison results on HCSTVG-v1~\citep{hcstvg}, and more comparison on other benchmarks can be found in Sec.~\ref{sup_oracle} in the supplementary material. From Fig.~\ref{fig:2}, we can clearly observe that, when introducing the target-specific information from groundtruth to initialize object queries, the STVG performance can be significantly improved, \eg, from 49.9\% to 68.9\% in m\_IoU (19.0\% absolute gains) and 36.4\% to 49.9\% in m\_vIoU (13.5\% absolute gains), fully verifying effectiveness of target-specific information in initializing queries for accurate STVG.

Thus motivated by the above, we introduce a novel \emph{\textbf{T}arget-\textbf{A}ware Transformer for \textbf{STVG}} (TA-STVG), which seeks to adaptively initialize object queries by exploring target-specific cues from videos for spatio-temporal target localization (see Fig.~\ref{fig:framework_comparison} (b)). The crux of our TA-STVG lies in two simple yet effective  modules, including \emph{text-guided temporal sampling} (TTS) and \emph{attribute-aware spatial activation} (ASA). These two modules work in a cascade in the TA-STVG, from coarse to fine, to generate the target-aware queries for subsequent localization. Specifically, TTS, from the temporal perspective, firstly aims to select target-relevant frames from the video using holistic information from the textual expression. To enhance the temporal selection, both appearance and motion cues are jointly considered in TTS. Then, from a spatial perspective, ASA focuses on exploring fine-grained visual semantic information of the object from the previous target-aware temporal cues learned by TTS. Particularly, attributes such as color and action will be exploited in the ASA to activate spatial features most relevant to the target object. Considering the specificity of object queries for spatial and temporal decoders, different activation features based on the specific attributes will be generated. Finally, features from ASA are used to initialize queries, which are fed to decoders, interacting with multimodal features from the encoder, to learn target position information for localization. 

In this work, we employ the DETR-similar architecture for TA-STVG, similar to existing Transformer-based STVG methods~\citep{TubeDETR,STCAT,cgstvg}, but \emph{\textbf{difference}} is that, our object queries, for both spatial and temporal localization, are directly generated from the given video-text pair, and thus naturally carry the target-specific cues, making them adaptive
and better interact with multimodal features for learning more discriminative information to improve STVG. {Our attribute-aware activation is related to~\cite{csdvl} that activates the spatially attended target using intra-frame visual cues and ~\cite{tan2024siamese} that jointly learns feature alignment and regression using weak supervision signals. The \textbf{\emph{difference}} is that, our ASA mines target-relevant spatial cues from features by TTS for learning target-aware queries to enhance target localization.} To our best knowledge, TA-STVG is the first to incorporate target-specific cues for object query generation to improve STVG. To validate its efficacy, we conduct experiments on HCSTVG-v1/-v2~\citep{hcstvg} and VidSTG~\citep{STGRN}, and results show that TA-STVG outperforms previous STVG methods by achieving new state-of-the-arts, evidencing effectiveness of our solution. Moreover, our TTS and ASA are designed for general purpose and thus applicable to other architectures. We apply TTS and ASA on top of two approaches, including the seminal Transformer-based TubeDETR~\citep{TubeDETR} and STCAT~\citep{STCAT}, significantly improving their performance, as shown in our experiments, further highlighting the generality of our approach.

In summary, the major contributions of this work are as follows: \ding{171} We present the TA-STVG, a novel Target-Aware Transformer for improving STVG by exploring target-specific cues for object queries; \ding{170} We propose text-guided temporal sampling (TTS) for selecting target-relevant temporal cues from the videos; \ding{168} We present attribute-aware spatial activation (ASA) to exploit fine-grained visual semantic attribute information for object query generation; and \ding{169} TA-STVG in extensive experiments achieves new state-of-the-art performance and shows good generality, demonstrating its efficacy. 

\section{Related Work}

\textbf{Spatio-Temporal Video Grounding.} Spatio-temporal video grounding (STVG)~\citep{STGRN} aims to spatially and temporally localize the target of interest, given its free-form of textual description, from an untrimmed sequence. Early STVG approaches~\citep{STGRN, OAMBRN, hcstvg} mainly consists of two stages, \emph{first} generating candidate proposals from the video with a pre-trained detector and \emph{then} selecting correct proposals based on the textual expression. To eliminate the heavy dependency on the pre-trained detection model, recent methods~\citep{STVGBert, TubeDETR,STCAT,csdvl,talal2023video,cgstvg}, inspired by Transformer, switch to the one-stage pipeline that directly generates a spatio-temporal tube for target localization, without relying on any external detectors. Owing to the compact end-to-end training pipeline, such a one-stage framework demonstrates superior performance compared to previous two-stage algorithms. Our TA-STVG also belongs to the one-stage Transformer-based type. However, \emph{\textbf{different from}} the aforementioned Transformer-based approaches that simply follows~\citep{dert} to leverage zero-initialized object queries for target localization, TA-STVG innovatively exploits target-specific cues from the video-text pair for object query generation, making it adaptive to various scenarios and better interact with multimodal features in the decoder for more accurate localization. 


\textbf{Temporal Grounding.} Temporal grounding focuses on localizing specific targets or events from the video given the textual expression. Being relevant to STVG, it requires temporally localizing the target of interest, but the difference is that temporal grounding does not require to perform the spatial bounding box localization.
In recent years, many methods~\citep{locvtp,drft,mun2020local,hao2022can,wang2023protege,zhang2023text,lmmg,lin2023univtg} have been proposed for temporal grounding. For instance, the work of~\citep{wang2023protege} introduces a pre-training approach for improving video temporal grounding. The method in~\citep{lin2023univtg} presents a unified framework for various video temporal grounding tasks. The approach in~\citep{drft} proposes to learn complementary features from different modalities including images, flow, and depth for temporal grounding. \textbf{\emph{Different than}} these works, we focus on the more challenging STVG involved with spatial and temporal localization of the target.

\textbf{Transformer-based Detection.} Detection is a fundamental component in computer vision. Recently, the seminal work DETR~\citep{dert} has applied Transformer~\citep{vaswani2017attention} for detection with impressive performance, and later been further improved in numerous extensions~\citep{sun2021rethinking,zhu2020deformable,zheng2023less,ye2023cascade}. Similar to other Transformer-based STVG works, our method employs the DETR-similar architecture for STVG. The \emph{\textbf{difference}} is that we propose to learn adaptive object queries from the video-text pair, instead of utilizing zero object queries following~\citep{dert}  as in existing STVG methods, for better target localization. 

\textbf{Vision-Language Modeling.} Vision-language modeling (VLM) aims to process both visual content and language for multimodal understanding. In recent years, it has attracted great attention from the researchers and been studied in various tasks including visual question answering~\citep{antol2015vqa,jiang2020defense,han2023shot2story20k,shao2023prompting, wang2024reconstructive,weng2024longvlm}, visual captioning~\citep{you2016image,gu2023text,gu2022dual,wang2024droppos,shen2023accurate, ren2024pixellm, wang2023hard}, navigation~\citep{zhu2020vision,li2023improving}, text-to-image generation~\citep{MReward,ramesh2021zero}, referring expression segmentation~\citep{yang2022lavt,liu2023gres}, vision-language tracking~\citep{guo2022divert,zhou2023joint}, etc. \emph{\textbf{Different}} from these tasks, we focus on vision-language modeling for spatio-temporal video grounding.

\section{Our Approach}

\textbf{Overview.} In this work, we propose TA-STVG, which aims to generate target-aware object queries for improving STVG. Motivated by DETR~\citep{dert}, TA-STVG utilizes the encoder-decoder framework which consists of a multimodal encoder and a spatio-temporal grounding decoder. As shown in Fig.~\ref{fig:mainnetwork}, the multimodal encoder ($\S$\ref{encoder}) extracts and interacts visual and textual features, while the decoder ($\S$\ref{decoder}) learns target position information using target-aware object queries generated with TTS ($\S$\ref{tss}) and ASA ($\S$\ref{asa}) and their interaction with the multimodal feature for target localization.

\begin{figure}[!t]
	\centering
	\includegraphics[width=0.96\linewidth]{framework.pdf}
	\caption{Overview of TA-STVG, which exploits target-specific information from the video and text (\ie, features from multimodal encoder) for generating spatial and temporal object queries for STVG.}
 \vspace{-10pt}
	\label{fig:mainnetwork}
\end{figure}


\subsection{Multimodal Encoder}
\label{encoder}
Given a video and the textual expression, the multimodal encoder aims at obtaining vision-language features for target-aware query generation and the subsequent decoder for localization. It comprises visual and textual feature extraction and multimodal feature fusion as described in the following.

\textbf{Visual and Textual Feature Extraction.} For the video, we extract its both appearance and motion features to leverage rich static and dynamic information. Specifically, we first sample $N_v$ frames $\mathcal{I}=\{I_i\}_{i=1}^{N_v}$ from the video. Afterwards, ResNet-101~\citep{resnet} and VidSwin~\citep{vidswin} are respectively utilized for appearance and motion feature extraction. The appearance feature is denoted as $\mathcal{F}_{a}=\{f^{a}_i\}_{i=1}^{N_v}$, where $f_i^{a} \in \mathbb{R}^{H \times W \times C_a}$ with $H$, $W$ and $C_a$ the height, width and channel dimensions, and the motion feature is represented using $\mathcal{F}_{m}=\{f^{m}_i\}_{i=1}^{N_v}$, where $f_i^{m} \in \mathbb{R}^{H \times W \times C_m}$ with $C_m$ the channel dimension.

For the textual expression, we use RoBERTa~\citep{roberta} to extract its feature. We first tokenize it to obtain a word sequence $\mathcal{W}=\{w_{i}\}_{i=1}^{N_t}$ containing $N_t$ words. Then, we apply RoBERTa on $\mathcal{W}$ to generate the textual feature $\mathcal{F}_{t} = \{f^t_i\}_{i=1}^{N_t}$, where $f^t_i \in \mathbb{R}^{C_t}$ with $C_t$ the textual feature channel.

\textbf{Multimodal Feature Fusion.} For enhancing feature representation for STVG, we fuse multimodal features of appearance feature $\mathcal{F}_{a}$, motion feature $\mathcal{F}_{m}$, and text feature $\mathcal{F}_{t}$, similar to~\citep{cgstvg}. Specifically, we first project them to the same channel number $D$ and then concatenate corresponding features to produce the multimodal feature $\mathcal{F}=\{f_i\}_{i=1}^{N_v}$ as follows,
\begin{equation}
	f_i = [\underbrace{f^a_{i_1}, f^a_{i_2}, ..., f^a_{i_{H \times W}}}_{\text{appearance features $f^a_i$}},\underbrace{f^m_{i_1}, f^m_{i_2}, ..., f^m_{i_{H \times W}}}_{\text{motion features $f^m_i$}}, \underbrace{f_{1}^{t}, f_{2}^{t}, ..., f_{N_t}^{t}}_{\text{textual features $\mathcal{F}_{t}$}}]
\end{equation}
where $f_i$ is the multimodal feature in frame $i$. Then, we perform feature fusion using the self-attention encoder~\citep{vaswani2017attention} to obtain the multimodal feature $\mathcal{\tilde{F}}$ as follows,
\begin{equation}
    \mathcal{\tilde{F}} = \mathtt{SelfAttEncoder}(\mathcal{F}+\mathcal{E}_{pos}+\mathcal{E}_{typ})
\end{equation}
where $\mathcal{E}_{pos}$ and $\mathcal{E}_{typ}$ denote position and type embeddings imposed on $\mathcal{F}$, and $\mathtt{SelfAttEncoder}(\cdot)$ is the self-attention encoder with $L$ ($L$=6) standard self-attention encoder blocks. Please refer to Sec.~\ref{network} in supplementary material for architecture of $\mathtt{SelfAttEncoder}(\cdot)$.

\begin{figure}[!t]
	\centering
	\includegraphics[width=0.98\linewidth]{module.pdf}
	\caption{Illustration of the architecture for TTS in (a) and ASA in (b).}
 \vspace{-10pt}
	\label{fig:module}
\end{figure}

\subsection{Text-Guided Temporal Sampling (TTS)}
\label{tss}

To generate target-specific object queries, we first develop a \emph{text-guided temporal sampling} (TTS) module to identify and sample frames relevant to the target guided by holistic textual features. Specifically, it will predict a relevance score between each frame and text and then sample video frames with their relevance scores above a predefined threshold. To enhance the frame selection in TTS, both appearance and motion are considered in computing the relevance, as shown in Fig.~\ref{fig:module} (a).

More concretely, given multimodal feature $\mathcal{\tilde{F}}$ from the multimodal encoder ($\S$\ref{encoder}), we first extract the appearance, motion and textual features, respectively denoted by $\mathcal{\tilde{F}}_{a}$, $\mathcal{\tilde{F}}_{m}$, and $\mathcal{\tilde{F}}_{t}$, from $\mathcal{\tilde{F}}$ via deconcatenation $[\mathcal{\tilde{F}}_{a}, \mathcal{\tilde{F}}_{m},\mathcal{\tilde{F}}_{t}] = \mathtt{DeConcat}(\mathcal{\tilde{F}})$.
Afterwards, we perform average pooling on $\mathcal{\tilde{F}}_{a}$ and $\mathcal{\tilde{F}}_{m}$ and channel-average operation on $\mathcal{\tilde{F}}_{t}$ to obtain frame-level appearance and motion features and the holistic textual feature,
\begin{equation}
        \mathcal{G}_{a} = \mathtt{AvgPooling}(\mathcal{\tilde{F}}_{a})\;\;\;\;\;\;\;\;\;\;
        \mathcal{G}_{m} = \mathtt{AvgPooling}(\mathcal{\tilde{F}}_{m}) \;\;\;\;\;\;\;\;\;\;
        \mathcal{G}_{t} = \mathtt{Avg}(\mathcal{\tilde{F}}_{t})
\end{equation}
where $\mathcal{G}_{a}$, $\mathcal{G}_{m}$ (both $\in \mathbb{R}^{N_{v}\times D}$) and $\mathcal{G}_{t}$ ($\in \mathbb{R}^{1\times D}$) are averaged appearance, motion, and text features. 

To measure the relevance of each frame with the text, we fuse the textual feature into appearance and motion features via two cross-attention blocks, respectively. Specifically, the appearance feature $\mathcal{G}_{a}$ or motion feature $\mathcal{G}_{m}$ serves as query, while the textual feature $\mathcal{G}_{t}$ as key/value. After this, MLP is performed on the appearance and motion features to predict relevance scores for each frame in terms of appearance and motion. As shown in Fig.~\ref{fig:module} (a), this process can be expressed as follows,
\begin{equation}
    s_a = \mathtt{MLP}(\mathtt{CA}(\mathtt{CA}(\mathcal{G}_{a}, \mathcal{G}_{t}),\mathcal{G}_{t})) \;\;\;\;\;\;\;\;\;\;\;\;\;\;\; s_m = \mathtt{MLP}(\mathtt{CA}(\mathtt{CA}(\mathcal{G}_{m}, \mathcal{G}_{t}),\mathcal{G}_{t}))
\end{equation}
where $s_a$ and $s_m$ are the relevance scores regarding appearance and motion, $\mathtt{CA}(\textbf{z},\textbf{u})$ denotes the cross-attention block (CAB)~\citep{vaswani2017attention} with $\textbf{z}$ generating query and $\textbf{u}$ key/value, and $\mathtt{MLP}(\cdot)$ is the MLP. 

Considering both appearance and motion are critical when determining target-relevant frames, we combine $s_a$ and $s_m$, controlled by $\delta$, to compute the final relevance score $s$ for each frame, as follows,
\begin{equation}
    s = \delta \times s_{a} + (1-\delta) \times s_m
\end{equation}
With $s$, we leverage it to sample target-relevant appearance and motion features. Specifically, we only keep appearance and motion features in frames whose relevance scores are above the pre-defined threshold $\theta$, and this process can be written as follows,
\begin{equation}\label{sample}
    \begin{split}
        \mathcal{R}_{a}&=\mathtt{sample}(\mathcal{\tilde{F}}_{a}, s, \theta)=\{\mathcal{\tilde{F}}_{a}(i) | s(i)>\theta\} \\ \mathcal{R}_{m}&=\mathtt{sample}(\mathcal{\tilde{F}}_{m}, s, \theta)=\{\mathcal{\tilde{F}}_{m}(i) | s(i)>\theta\}
    \end{split}
\end{equation}
where $i\in[1,N_{v}]$ is the frame index. $\mathcal{R}_{a}$ and $\mathcal{R}_{m}$ are sampled target-relevant temporal appearance and motion features, which will be used for extracting target-specific fine-grained attribute information. Fig.~\ref{fig:example} (a) shows an example of the temporal relevance score and selected temporal frames by TTS. {Please note, for $\mathcal{R}_a$ and $\mathcal{R}_m$, their quantities are the same, because we set the same threshold $\theta$ for both appearance and motion features, and sample $\mathcal{R}_a$ and $\mathcal{R}_m$ from $\mathcal{\tilde{F}}_{a}$ and $\mathcal{\tilde{F}}_{m}$ ($\mathcal{\tilde{F}}_{a}$ and $\mathcal{\tilde{F}}_{m}$ have the same quantities) both based on the relevance score $s$ generated by TTS (see Eq.~(\ref{sample})). Thus, their quantities are the same, and consistent with the number of frames sampled by TTS.}

\subsection{Attribute-aware Spatial Activation (ASA)} 
\label{asa}

To explore more fine-grained information of the target, we design an \emph{attribute-aware spatial activation} (ASA) module, which mines fine-grained visual semantic information from previous coarse-grained temporal features $\mathcal{R}_{a}$ and $\mathcal{R}_{m}$ for object queries. Particularly, ASA considers appearance attribute like color (\eg, ``\emph{green}'', ``\emph{red}'', etc) and shape (\eg, ``\emph{small}'', ``\emph{tall}'', etc) and motion attribute such as  action (\eg, ``\emph{walk}'', ``\emph{ride}'', etc) to generate fine-grained spatial features for the target. The \textbf{\emph{intuition}} is that, appearance attribute is important for spatial target localization, while motion attribute is crucial in temporal localization, \ie, locating the start and end of the tube. In certain cases, even if spatial localization of target is good enough in each frame, the accuracy is low due to the incapability of temporally recognizing target action. Considering the specificity of queries for spatial and temporal decoders, appearance and motion activation features based on specific attributes are generated.

\begin{figure}[!t]
	\centering
	\includegraphics[width=\linewidth]{example.pdf}
	\caption{Illustration of temporal relevance score $s$ by TTS in (a) and attribution-aware spatial activation (in partial selected frames) including appearance and motion activation in (b) and (c). We can see from (a) that TTS can accurately select target-relevant frames, and from (b) and (c) that ASA  precisely localizes attributes, \eg, color ``\emph{yellow}'' and action ``\emph{walks in}, \emph{stops}'', related to the target.}
 \vspace{-3mm}
	\label{fig:example}
\end{figure}

Specifically, as shown in Fig.~\ref{fig:module} (b), given the textual feature $\mathcal{\tilde{F}}_t$, we first extract the feature $\mathcal{\tilde{T}}_t$ of the subject (usually the target to localize) in the textual sentence from it. Due to limited space, please refer to the Sec.~\ref{subject} in supplementary material for subject extraction. Then, we repeat the subject feature $\mathcal{\tilde{T}}_t$ to match the number of frames in $\mathcal{R}_{a}$ and $\mathcal{R}_{m}$, and the resulted feature is denoted as $\mathcal{T}_t$. After that, we learn the fine-grained spatial features from $\mathcal{R}_{a}$ and $\mathcal{R}_{m}$ by interacting them with $\mathcal{T}_t$ respectively using two cross-attention blocks. In specific, the subject feature $\mathcal{T}_t$ serves as the query, while $\mathcal{R}_{a}$ and $\mathcal{R}_{m}$ as the key/value. By doing so, we can allow the subject feature to learn relevant spatial features from appearance and motion. To enforce the learning of attribute-aware spatial features, we propose to supervise ASA with \emph{explicit} weak attribute labels generated from the textual expression using multi-label classification losses for appearance and motion. The appearance and motion are multi-class labels. Due to space limitation, we refer readers to our supplementary material for details regrading the attribute label construction. The attribute classification is performed with a linear projection layer followed with softmax. As in Fig.~\ref{fig:module} (b), this can be written as follows,
\begin{equation}
    \begin{split}
        c_a &= \mathtt{Softmax}(\mathtt{Linear}(\mathtt{CA}(\mathtt{CA}(\mathcal{T}_{t}, \mathcal{R}_{a}),\mathcal{R}_{a}))) \\ c_m &= \mathtt{Softmax}(\mathtt{Linear}(\mathtt{CA}(\mathtt{CA}(\mathcal{T}_{t}, \mathcal{R}_{m}),\mathcal{R}_{m})))
    \end{split}
\end{equation}
where $c_a$ and $c_m$ represent classification scores for appearance and motion attributes, respectively, and $\mathtt{Linear}(\cdot)$ and $\mathtt{Softmax}(\cdot)$ denote linear projection and softmax classification. 

By supervising $c_a$ and $c_m$ with appearance and motion attribute labels, the subject feature $\mathcal{T}_{t}$, serving as query in cross-attention blocks, is able to adaptively learn fine-grained features relevant to attributes from $\mathcal{R}_{a}$ and $\mathcal{R}_{m}$ for attribute classification. Therefore, the attention maps of $\mathcal{T}_{t}$ with $\mathcal{R}_{a}$ and $\mathcal{R}_{m}$ can be explained as attribute-specific spatial activation in appearance and motion~\citep{dosovitskiy2020image}, and adopted to generate attribute features from $\mathcal{R}_{a}$ and $\mathcal{R}_{m}$ as follows,
\begin{equation}
    \mathcal{A}_a = \mathcal{M}_a \otimes \mathcal{R}_a \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;
    \mathcal{A}_m = \mathcal{M}_m \otimes \mathcal{R}_m
\end{equation}
where $\mathcal{A}_a$ and $\mathcal{A}_m$ are the target-specific attribute features and $\mathcal{M}_a$ and $\mathcal{M}_m$ the attribute-aware spatial activation in appearance and motion. Fig.~\ref{fig:example} (b) and (c) show the learned attribute activation. 

\textbf{\emph{Discussion.}} Please note that, besides our method, another way is to directly learn spatial activation of the target (\ie, instance-level activation, \emph{not} the attribute-specific activation) supervised by binary masks generated from the groundtruth box. In this method, results after softmax in ASA will be directly employed as the appearance and motion spatial activation for feature extraction (see details in Sec.~\ref{strategy} in supplementary material). Despite simplicity, this will \emph{not} enable the learning of fine-grained unique attribute features, which are discriminative to distinguish the target from background. As shown in an experiment later, our strategy of ASA shows better performance.

\textbf{Generating Object Queries.} After the TTS and ASA, the final fine-grained target-specific features $\mathcal{A}_a$ and $\mathcal{A}_m$ are used to generate the spatial and temporal object queries. Specifically, we simply use average pooling on $\mathcal{A}_a$ and $\mathcal{A}_m$ (note, reshaping is needed) and then apply the repeat operation to obtain the initial spatial and temporal object queries $\mathcal{Q}_0^{s}$ and $\mathcal{Q}_0^{t}$ as follows,
\begin{equation}
    \mathcal{Q}_0^{s} = \mathtt{repeat}(\mathtt{AvgPooling}(\mathcal{A}_a), N_v) \;\;\;\;\;\;\;\;\;\;\; \mathcal{Q}_0^{t} = \mathtt{repeat}(\mathtt{AvgPooling}(\mathcal{A}_m), N_v)
\end{equation}
where $\mathtt{repeat}(\textbf{v}, k)$ denotes repeating $\textbf{v}$ by $k$ times. Afterwards, $\mathcal{Q}_0^{s}$ and $\mathcal{Q}_0^{t}$ are sent to the decoder for spatial-temporal target localization as described later.


\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{example_att.pdf}
    \caption{Comparison of attention maps for zero-initialized object queries and the proposed target-aware object queries in video frames in the spatial decoder. The red boxes indicate the foreground target to localize. From this figure, we can clearly observe that the attention maps of our target-aware object queries by TTS and ASA can better focus on the target object for localization.}\vspace{-3mm}
    \label{fig:ex_att_decoder}
\end{figure}

\subsection{Decoder for Spatio-Temporal Localization }
\label{decoder}

To learn target position information, we employ spatial and temporal decoders, similar to existing methods~\citep{STCAT,cgstvg}. In both decoders, object queries $\mathcal{Q}_0^{s}$ and $\mathcal{Q}_0^{t}$ will iteratively interact with the multimodal feature $\mathcal{\tilde{F}}$ from the encoder for target information learning, as follows,
\begin{equation}
    \mathcal{Q}_K^{s} = \mathtt{SpatialDecoder}(\mathcal{Q}_0^{s}, \mathcal{\tilde{F}}_a, \mathcal{\tilde{F}}_t)\;\;\;\;\;\;\;\;\;
    \mathcal{Q}_K^{t} = \mathtt{TemporalDecoder}(\mathcal{Q}_0^{t}, \mathcal{\tilde{F}}_m, \mathcal{\tilde{F}}_t)
\end{equation}
where $\mathcal{Q}_K^{s}$ and $\mathcal{Q}_K^{t}$ are learned spatial and temporal queries after decoders, and $\mathtt{SpatialDecoder}(\cdot,\cdot)$ and $\mathtt{TemporalDecoder}(\cdot,\cdot)$ denote spatial and temporal decoders with each containing $K$ ($K=6$) self-attention blocks and cross-attention blocks.
Fig.~\ref{fig:ex_att_decoder} shows the attention maps of our target-aware queries for localization. 
Due to limited space, please refer to the Sec.~\ref{network} in supplementary material for decoder architectures and  additional attention maps of our target-aware queries for localization. 

Once generating $\mathcal{Q}_K^{s}$ and $\mathcal{Q}_K^{t}$, we employ two heads to predict the final object boxes $\mathcal{B}=\{b_i\}_{i=1}^{N_v}$ where $b_i\in R^{4}$ represents central position, width and height of predicted box in frame $i$, and start and end probabilities of each frame $\mathcal{H} =\{(h_i^s, h_i^e)\}_{i=1}^{N_v}$, where start and end times are determined by the maximum joint start and end probability.

\subsection{Optimization}

In TA-STVG, we predict: (1) the temporal relevance scores $s_a$ and $s_m$ in terms of appearance and motion in TTS, (2) the appearance and motion attribution classification scores $c_a$ and $c_m$ in ASA, and (3) the spatial boxes $\mathcal{B}=\{b_{i}\}_{i=1}^{N_v}$ in the spatial decoder, start timestamps $\mathcal{H}_s=\{h_{i}^{s}\}_{i=1}^{N_v}$ and end timestamps $\mathcal{H}_e=\{h_{i}^{e}\}_{i=1}^{N_v}$ in the temporal decoder. During training, with the groundtruth of motion attribute $c_m^*$, appearance attribute $c_a^*$, start timestamps $\mathcal{H}_s^*$, end timestamps $\mathcal{H}_e^*$, and the bounding box $\mathcal{B}^*$, we can calculate the total loss $\mathcal{L}$ as 
\begin{equation}\small
    \begin{split}
        \mathcal{L} =& \underbrace{\lambda_{\text{TTS}} (\mathcal{L}_{\text{BCE}}(s_a, (\mathcal{H}_s^*, \mathcal{H}_e^*))+ \mathcal{L}_{\text{BCE}}(s_m, (\mathcal{H}_s^*, \mathcal{H}_e^*)))}_{\text{loss of TTS}}) + \underbrace{\lambda_{\text{ASA}}( \mathcal{L}_{{\text{BCE}}}(c_m^*,c_m) +  \mathcal{L}_{{\text{BCE}}}(c_a^*,c_a)}_{\text{loss of ASA}}) \\
        &+\underbrace{\lambda_k (\mathcal{L}_{{\text{KL}}}(\mathcal{H}_s^*,\mathcal{H}_s) + \mathcal{L}_{{\text{KL}}}(\mathcal{H}_e^*,\mathcal{H}_e)}_{\text{loss of temporal decoder}}) + \underbrace{\lambda_l \mathcal{L}_{1}(\mathcal{B}^*,\mathcal{B}) + \lambda_u \mathcal{L}_{{\text{IoU}}}(\mathcal{B}^*,\mathcal{B})}_{\text{ loss of spatial decoder}}
    \end{split}
\end{equation}
where $\mathcal{L}_{{\text{KL}}}$, $\mathcal{L}_{1}$, $\mathcal{L}_{{\text{IoU}}}$ and $\mathcal{L}_{{\text{BCE}}}$ are KL divergence, smooth L1, IoU, and binary cross-entropy losses.



\section{Experiments}

\textbf{Implementation.} TA-STVG is implemented in Python using PyTorch~\citep{paszke2019pytorch}. Similar to~\citep{cgstvg}, we use pre-trained ResNet-101~\citep{resnet} and RoBERTa-base~\citep{roberta} from MDETR~\citep{mdetr} as 2D and text backbones, and VidSwin-tiny~\citep{vidswin} as 3D backbone.
The number of attention heads is $8$, and the hidden dimension of the encoder and decoder is 256.
The channel dimensions $C_a$, $C_m$, $C_t$, $D$ are 2,048, 768, 768 and 256. 
$\delta$ and $\theta$ are empirically set to $0.5$ and 0.7.
% Data
We use random resized cropping as augmentation method, producing  an output with a short side of 420.
The video frame length $N_v$ is 64, and the text sequence length $N_t$ is 30.
% Optim
During training, we use Adam~\citep{kingma2014adam} with an initial learning rate of 2e-5 for the pre-trained backbone and 3e-4 for the remaining modules (note that the 3D backbone is frozen).
The loss weight parameters $\lambda_{\text{TTS}}$, $\lambda_{\text{ASA}}$, $\lambda_k$, $\lambda_l$, and $\lambda_u$ are set to 1, 1, 10, 5, and 3, respectively.

\textbf{Datasets and Metrics.} We use three datasets, \ie, HCSTVG-v1/v2~\citep{hcstvg} and VidSTG~\citep{STGRN}, for experiments. HCSTVG-v1 contains 5,660 untrimmed videos, with 4,500 and 1,160 video-text pairs in training and testing sets. HCSTVG-v2 expands upon HCSTVG-v1, and comprises 10,131 training, 2,000 validation, and 4,413 testing samples. As test set annotations are not available, results are reported based on the validation set on HCSTVG-v2 as in other methods~\citep{STCAT,TubeDETR,csdvl}. VidSTG contains 99,943 video-text pairs. It includes 44,808 declarative and 55,135 interrogative sentences, with training, validation, and test sets containing 80,684, 8,956, and 10,303  sentences, and 5,436, 602, and 732 videos, respectively.

Following~\citep{TubeDETR}, we employ m\_tIoU, m\_vIoU, and vIoU@R as the evaluation metrics. m\_tIoU measures the ability of temporal grounding by computing the average tIoU score across all the test set. m\_vIoU compares the spatial grounding performance by calculating the average of vIoU scores, and vIoU@R measures the performance using ratios of samples with vIoU greater than R in test sets. For detailed metrics, please kindly refer to~\citep{TubeDETR}.



\subsection{State-of-the-art Comparison}

\textbf{HCSTVG Datasets.}  
To validate the efficacy of our method, we compare it against other approaches on HCSTVG-v1/v2 datasets. Tab.~\ref{tab:hcstvgv1} shows the comparison results on the HCSTVG-v1 test set. As shown in Tab.~\ref{tab:hcstvgv1}, TA-STVG achieves state-of-the-art performance on all four metrics, especially on the vIOU@0.3 metric, where it improves the score by 1.6\% absolute gains compared to CG-STVG~\citep{cgstvg}. 
Compared to our baseline that does not employ TTS and ASA modules for target-specific query generation and instead uses zero-initialized object queries, TA-STVG shows absolute performance gains of 3.1\%, 2.7\%, 5.5\%, and 4.7\% on the four metrics, which evidences the effectiveness of our method. 
Tab.~\ref{tab:hcstvgv2} shows the result on HCSTVG-v2 validation set. As shown, TA-STVG achieves SOTA results on all four metrics. 
Compared to the baseline, TA-STVG improves it with 2.1\%, 2.3\%, 3.2\%, and 3.7\% absolute gains on four metrics, validating its efficacy again.

\begin{table}[!t]
\setlength{\tabcolsep}{2.5pt}
	\centering
	\begin{minipage}{.5\textwidth}
		\centering
            \caption{Comparison on HCSTVG-v1 (\%).}
            \vspace{-5pt}
		\renewcommand{\arraystretch}{1.1}
		\scalebox{0.50}{
			\begin{tabular}{rcccc}
				\rowcolor{mygray} 
				\specialrule{1.5pt}{0pt}{0pt}
				Methods & m\_tIoU & m\_vIoU & vIoU@0.3 &  vIoU@0.5  \\ 
				\hline\hline
				STVGBert ~\citep{STVGBert} & - & 20.4 & 29.4 &  11.3  \\
				TubeDETR ~\citep{TubeDETR} & 43.7 & 32.4 & 49.8 & 23.5 \\
				STCAT ~\citep{STCAT} & 49.4 & 35.1 & 57.7 & 30.1 \\
                    SGFDN ~\citep{wang2023efficient} & 46.9 & 35.8 & 56.3 & 37.1 \\
				STVGFormer ~\citep{csdvl} & - & 36.9 & 62.2 & 34.8 \\
                CG-STVG ~\citep{cgstvg} & 52.8 & 38.4 & 61.5 & 36.3 \\ \hline
				Baseline (ours) & 49.9 & 36.4 & 57.6 & 32.1  \\ 
				\rowcolor{highlight} TA-STVG (ours) & \hspace{2em}\textbf{53.0}\improve{~+3.1} & \hspace{2em}\textbf{39.1}\improve{~+2.7} & \hspace{2em}\textbf{63.1}\improve{~+5.5} & \hspace{2em}\textbf{36.8}\improve{~+4.7} \\ \specialrule{1.5pt}{0pt}{0pt}
		\end{tabular}}
		
		\label{tab:hcstvgv1}
	\end{minipage}%
	\hfill
	\begin{minipage}{.5\textwidth}
		\centering
            \caption{Comparison on HCSTVG-v2 (\%).}
            \vspace{-5pt}
		\renewcommand{\arraystretch}{1.1}
		\scalebox{0.49}{
			\begin{tabular}{rcccc}
				\specialrule{1.5pt}{0pt}{0pt}
				\rowcolor{mygray} 
				Methods & m\_tIoU & m\_vIoU & vIoU@0.3 &  vIoU@0.5  \\ \hline\hline
				PCC ~\citep{pcc} & - &  30.0 & - & -  \\ 
				2D-Tan ~\citep{2d-tan}  & - & 30.4 &  50.4 & 18.8  \\
				MMN  ~\citep{mmn} & - & 30.3 & 49.0 & 25.6 \\
				TubeDETR ~\citep{TubeDETR} & 53.9 & 36.4 & 58.8 & 30.6 \\
				STVGFormer ~\citep{csdvl} & 58.1 & 38.7 & 65.5 & 33.8 \\
                    CG-STVG ~\citep{cgstvg} & 60.0 & 39.5 & 64.5 & 36.3 \\ \hline
				Baseline (ours) & 58.3 & 37.9 & 62.6 & 33.0  \\
				 \rowcolor{highlight} TA-STVG (ours) & \hspace{2em}\textbf{60.4}\improve{~+2.1} & \hspace{2em}\textbf{40.2}\improve{~+2.3} & \hspace{2em}\textbf{65.8}\improve{~+3.2} & \hspace{2em}\textbf{36.7}\improve{~+3.7} \\
				\specialrule{1.5pt}{0pt}{0pt}
		\end{tabular}}
		\label{tab:hcstvgv2}
	\end{minipage}
 \vspace{-5pt}
\end{table}

\begin{table}[!t]
\setlength{\tabcolsep}{5pt}
	\centering
        \caption{Comparison with existing state-of-the-art methods on VidSTG (\%).}
        \vspace{-5pt}
	\renewcommand{\arraystretch}{1.1}
	\scalebox{0.57}{
		\begin{tabular}{rcccccccc}
			\specialrule{1.5pt}{0pt}{0pt}
			\rowcolor{mygray} 
			\cellcolor{mygray} & \multicolumn{4}{c}{ \cellcolor{mygray} Declarative Sentences} & \multicolumn{4}{c}{ \cellcolor{mygray}Interrogative Sentences} \\ 
			\rowcolor{mygray} 
			\multirow{-2}{*}{\cellcolor{mygray} Methods} & m\_tIoU & m\_vIoU & vIoU@0.3 &  vIoU@0.5  & m\_tIoU & m\_vIoU & vIoU@0.3 &  vIoU@0.5  \\
			\hline
			\hline

			STGRN ~\citep{STGRN}  &  48.5 &  19.8 & 25.8 & 14.6 &  47.0 & 18.3 & 21.1 & 12.8 \\
			OMRN ~\citep{OAMBRN}  &  50.7 &  23.1 & 32.6 & 16.4 &  49.2 & 20.6 & 28.4 & 14.1 \\
			STGVT ~\citep{hcstvg} & - &  21.6 & 29.8 & 18.9 &  - & - & -  & - \\
			STVGBert ~\citep{STVGBert}  & - &  24.0 & 30.9 & 18.4 & - & 22.5 & 26.0 & 16.0 \\
			TubeDETR ~\citep{TubeDETR} & 48.1 &  30.4 & 42.5 & 28.2 & 46.9 & 25.7 & 35.7 & 23.2 \\
			STCAT~\citep{STCAT} & 50.8 & 33.1 & 46.2 & 32.6 & 49.7 & 28.2 & 39.2 & 26.6  \\
               SGFDN~\citep{wang2023efficient} & 45.1 & 28.3 & 41.7 & 29.1 & 44.8 & 25.8 & 36.9 & 23.9  \\
			STVGFormer~\citep{csdvl} & - & 33.7 & 47.2 & 32.8 & - & 28.5 & 39.9 & 26.2  \\ 
            CG-STVG~\citep{cgstvg} & 51.4 & 34.0 & 47.7 & 33.1 & 49.9 & 29.0 & 40.5 & 27.5 \\ \hline
		Baseline (ours)  & 49.5 & 32.3 & 44.9 & 31.6 & 48.6 & 27.5 & 38.5 & 25.6 \\
		\rowcolor{highlight} TA-STVG (ours) & \hspace{2em}\textbf{51.7}\improve{~+2.2} & \hspace{2em}\textbf{34.4}\improve{~+2.1} & \hspace{2em}\textbf{48.2}\improve{~+3.3} & \hspace{2em}\textbf{33.5}\improve{~+1.9} & \hspace{2em}\textbf{50.2}\improve{~+1.6} & \hspace{2em}\textbf{29.5}\improve{~+2.0} & \hspace{2em}\textbf{41.5}\improve{~+3.0} & \hspace{2em}\textbf{28.0}\improve{~+2.4} \\
			\specialrule{1.5pt}{0pt}{0pt}
	\end{tabular}}
	\label{tab:vidstg}
	\vspace{-10pt}
\end{table}

\textbf{VidSTG Dataset.} We evaluate TA-STVG on VidSTG in Tab.~\ref{tab:vidstg}. Different from HCSTVG, VidSTG includes both declarative and interrogative sentences. As in Tab.~\ref{tab:vidstg}, TA-STVG achieves state-of-the-art results on all 8 metrics. 
using target-aware queries, TA-STVG significantly outperforms the baseline, improving m\_tIoU and m\_vIoU scores by 2.2\% and 2.1\% for declarative sentences, and 1.6\% and 2.0\% for interrogative sentences, respectively, once again validating the effectiveness of TA-STVG.




\subsection{Ablation Study}

\begin{wraptable}{r}{7.2cm}
\setlength{\tabcolsep}{2.pt}
	\centering
	\renewcommand{\arraystretch}{1.1}
        \caption{Ablations of TTS and ASA.}
        \vspace{-5pt}
	\scalebox{0.9}{
		\begin{tabular}{cccccccc}
			\specialrule{1.5pt}{0pt}{0pt}
			\rowcolor{mygray} 
			& TTS & ASA & m\_tIoU & m\_vIoU &  vIoU@0.3 &  vIoU@0.5 \\ \hline\hline
			\ding{182} & - & - & 49.9 & 36.4 & 57.6 & 32.1 \\
			 \ding{183} & \checkmark & - & 52.2 & 38.4 & 61.7 & 36.2 \\
               \ding{184} & - & \checkmark & 51.4 & 38.0 & 60.4 & 34.1\\
			\ding{185} & \checkmark & \checkmark & \textbf{53.0} & \textbf{39.1} & \textbf{63.1} & \textbf{36.8} \\
		      \specialrule{1.5pt}{0pt}{0pt}
	\end{tabular}}
	\label{tab:ttsasa}
 \vspace{5pt}
\end{wraptable}


\textbf{Impact of TTS and ASA.} The 
TTS and ASA are key components of TA-STVG. To validate their effectiveness, we conducted ablation experiments on HCSTVG-v1. As shown in Tab.~\ref{tab:ttsasa}, without TTS and ASA (\ding{182}), our baseline achieves a m\_tIoU score of 49.9\%. When TTS is applied alone, the m\_tIoU score is improved 52.2\% with 2.3\% gains (\ding{182} \emph{v.s.} \ding{183}). When ASA is used alone, m\_tIoU is improved to 51.4\% with 1.5\% gains (\ding{182} \emph{v.s.} \ding{184}). Working together, the best result of 53.0\% is achieved with 3.1\% absolute gains (\ding{182} \emph{v.s.} \ding{184}). All these results clearly validate the efficacy of TTS and ASA, working either alone or together, for improving STVG.



\textbf{Impact of different branches in TTS.} In TTS, we consider both appearance and motion, guided by text, for temporal selection. To further analyze TTS, we conduct experiments on its different choices on HCSTVG-v1. The results are reported in Tab.~\ref{tab:tts1}. As in Tab.~\ref{tab:tts1}, without using TTS (but using ASA), TA-STVG achieves 51.4\% m\_tIoU score (\ding{182}). With the help of text-guided appearance branch, the m\_tIoU score is improved to 51.8\% with 0.4\% gains (\ding{182} \emph{v.s.} \ding{183}). When using text-guided motion branch, the m\_tIoU score is improved to 52.3\% with 0.9\% gains (\ding{182} \emph{v.s.} \ding{184}). When considering both appearance and motion, the m\_tIoU score is boosted to 53.0\% with 1.6\% gains (\ding{182} \emph{v.s.} \ding{185}), showing the best performance. It is worth noting that, if not using text as a guidance, the m\_tIoU score will be decreased from 53.0\% to 51.8\% (\ding{186} \emph{v.s.} \ding{185}), validating the importance of textual guidance in TTS. 

\textbf{Impact of different attributes in ASA.} In ASA, we consider both appearance and motion attributes for fine-grained feature learning. For in-depth analysis, we conduct ablations for ASA on HCSTVG-v1. As in Tab.~\ref{cmm1}, without using ASA (but using TTS), TA-STVG achieves 52.2\% m\_tIoU score (\ding{182}). When considering appearance attribute,  m\_tIoU is improved to 52.3\% (\ding{182} \emph{v.s.} \ding{183}). When using motion attribute, the m\_tIoU score is improved to 52.7\% (\ding{182} \emph{v.s.} \ding{184}). When applying both appearance and motion attributes, best result of 53.0\% m\_tIoU score is achieved with 0.8\% gains (\ding{182} \emph{v.s.} \ding{186}). Please note, the gain by jointly using appearance and motion attributes is the maximum, which shows the necessity of both appearance and motion attributes for improvement. Similarly, when subject is not used, m\_tIoU is decreased from 53.0\% to 52.6\% (\ding{186} \emph{v.s.} \ding{185}), showing the need of subject in ASA.

\begin{table}[!t]
\setlength{\tabcolsep}{2.5pt}
	\centering
	\begin{minipage}{.49\textwidth}

        % \setlength{\tabcolsep}{2.pt}
        \centering
        \renewcommand{\arraystretch}{1.1}
            \caption{Ablations of branches in TTS. ``TG'', ``AB'', and ``MB'' are the text-guided, appearance and motion branches, respectively.}
            \vspace{-5pt}
        \scalebox{0.78}{
            \begin{tabular}{cccccccc}
                \specialrule{1.5pt}{0pt}{0pt}
                \rowcolor{mygray} 
            & \tabincell{c}{TG}	& \tabincell{c}{AB} & \tabincell{c}{MB} & m\_tIoU & m\_vIoU &  vIoU@0.3 &  vIoU@0.5 \\ \hline\hline
                \ding{182} & - & - & - & 51.4 & 38.0 & 60.4 & 34.1 \\
                \ding{183} & \checkmark & \checkmark & - & 51.8 & 38.4 & 61.1 & 36.3  \\
                    \ding{184} & \checkmark & - & \checkmark & 52.3 & 38.3 & 62.0 & 36.5 \\
                    \ding{185} & - & \checkmark & \checkmark & 51.8 & 38.5 & 62.0 & 36.9 \\
                \ding{186} & \checkmark & \checkmark & \checkmark & \textbf{53.0} & \textbf{39.1} & \textbf{63.1} & \textbf{36.8} \\
                  \specialrule{1.5pt}{0pt}{0pt}
        \end{tabular}}
        \label{tab:tts1}
\end{minipage}%
\hfill
\begin{minipage}{.49\textwidth}

        % \setlength{\tabcolsep}{2.pt}
        \centering
        \renewcommand{\arraystretch}{1.1}
            \caption{Ablations of attributes in ASA. ``SG'', ``AA'', and ``MA'' are the subject-guided, appearance and motion attributes, respectively.}
            \vspace{-5pt}
        \scalebox{0.78}{
            \begin{tabular}{cccccccc}
                \specialrule{1.5pt}{0pt}{0pt}
                \rowcolor{mygray} 
                & \tabincell{c}{SG} & \tabincell{c}{AA} & \tabincell{c}{MA} & m\_tIoU & m\_vIoU  &  vIoU@0.3 &  vIoU@0.5\\ \hline\hline
            \ding{182} & - &	- & - & 52.2 & 38.4 & 61.7 & 36.2 \\
                \ding{183} & \checkmark & \checkmark & - & 52.3 & 38.6 & 62.4 & 36.2 \\
                   \ding{184} & \checkmark & - & \checkmark & 52.7 & 38.6 & 61.3 & 36.6 \\
                   \ding{185} & - & \checkmark & \checkmark & 52.6 & 38.8 & 61.9 & \textbf{36.8} \\
                \ding{186} & \checkmark & \checkmark & \checkmark & \textbf{53.0} & \textbf{39.1}  & \textbf{63.1} & \textbf{36.8} \\
                  \specialrule{1.5pt}{0pt}{0pt}
        \end{tabular}}
        \label{cmm1}

	\end{minipage}
 \vspace{-5pt}
\end{table}

\begin{table}[!t]
\setlength{\tabcolsep}{2.5pt}
\centering
\begin{minipage}{.51\textwidth}

        \setlength{\tabcolsep}{1.pt}
	\centering
	\renewcommand{\arraystretch}{1.1}
        \caption{Ablations of activation learning in ASA.} 
        \vspace{-5pt}
	\scalebox{0.85}{
		\begin{tabular}{ccccccc}
			\specialrule{1.5pt}{0pt}{0pt}
			\rowcolor{mygray} 
			 & \tabincell{c}{Activation} & m\_tIoU & m\_vIoU &  vIoU@0.3 &  vIoU@0.5 \\ \hline\hline
                \ding{182} & None & 52.2 & 38.4 & 61.7 & 36.2 \\
			\ding{183} & Instance-act. & 52.8 & 38.8 & 62.0 & 34.7 \\
                \ding{184} & Attribute-act. & \textbf{53.0} & \textbf{39.1} & \textbf{63.1} & \textbf{36.8} \\   
		      \specialrule{1.5pt}{0pt}{0pt}
	\end{tabular}}
        \vspace{-10pt}
	\label{tab:super}
\end{minipage}%
% \hfill
\begin{minipage}{0.245\textwidth}
\centering
	\renewcommand{\arraystretch}{1.1}
        \caption{Ablation of $\delta$.}
        \vspace{-5pt}
	\scalebox{0.85}{
		\begin{tabular}{cccc}
			\specialrule{1.5pt}{0pt}{0pt}
			\rowcolor{mygray} 
			& $\delta$ & m\_tIoU & m\_vIoU \\ \hline\hline
			\ding{182} & 0.4 & 52.7 & 38.8 \\
                \ding{183} & 0.5 & \textbf{53.0} & \textbf{39.1} \\
                \ding{184} & 0.6 & 52.8 & 39.0 \\
		      \specialrule{1.5pt}{0pt}{0pt}
	\end{tabular}}
        \vspace{-10pt}
	\label{tab:delta}
\end{minipage}%
\begin{minipage}{0.245\textwidth}
\centering
	\renewcommand{\arraystretch}{1.1}
        \caption{Ablation of $\theta$.}
        \vspace{-5pt}
	\scalebox{0.85}{
		\begin{tabular}{cccc}
			\specialrule{1.5pt}{0pt}{0pt}
			\rowcolor{mygray} 
			& $\theta$ & m\_tIoU & m\_vIoU \\ \hline\hline
			\ding{182} & 0.6 & 52.3 & 38.5 \\
                \ding{183} &0.7 & \textbf{53.0} & \textbf{39.1} \\
                \ding{184} & 0.8 & 52.6 & 38.6 \\
		      \specialrule{1.5pt}{0pt}{0pt}
	\end{tabular}}
        \vspace{-10pt}
	\label{tab:theta}
\end{minipage}
 \vspace{-5pt}
\end{table}

\textbf{Analysis of activation learning strategy in ASA.} In ASA, we propose learning attribute-aware spatial activation for fine-grained target features. Another way is to learn the naive instance-level activation (not attribute-specific activation) supervised by binary masks generated from the groundtruth boxes as mentioned earlier (see these two strategies in Sec.~\ref{strategy} in supplementary material). We conduct experiments in Tab.~\ref{tab:super}. As shown, the attribute-specific features (\ding{184}) in our strategy shows better result compared to instance-level spatial activation (\ding{183}) or none activation (\ie, removing ASA) (\ding{182}).


\textbf{Impact of parameter $\delta$ and $\theta$ in TTS.} In TTS, $\delta$ controls the fusion of appearance and motion scores, and $\theta$ controls the sampling of video frames. We conduct ablations in Tab.~\ref{tab:delta} and Tab.~\ref{tab:theta}. We can see that the model performs best by setting $\delta$ to 0.5 (\ding{183} in Tab.~\ref{tab:delta}) and $\theta$ to 0.7 (\ding{183} in Tab.~\ref{tab:theta}).


Due to limited space, please refer to our supplementary material for more experiments and analysis.

\subsection{Validation of Generality}

\begin{wraptable}{r}{7.3cm}
\setlength{\tabcolsep}{2.pt}
	\centering
	\renewcommand{\arraystretch}{1.1}
        \caption{Incorporate the TTS and ASA modules into different methods on HCSTVG-v1 (\%). \textcolor{orange}{${\blacklozenge}$}: results from the original paper. \textcolor{eccvblue}{${\blacklozenge}$}: retrained results.}
        \vspace{-5pt}
	\scalebox{0.56}{
		\begin{tabular}{clcccccc}
			\specialrule{1.5pt}{0pt}{0pt}
			\rowcolor{mygray} 
			& Method & TTS + ASA & m\_tIoU & m\_vIoU &  vIoU@0.3 &  vIoU@0.5 \\ \hline\hline
			\ding{182} & \textcolor{gray}{TubeDETR}\textcolor{orange}{$^{\blacklozenge}$} & \textcolor{gray}{-} & \textcolor{gray}{43.7} & \textcolor{gray}{32.4} & \textcolor{gray}{49.8} & \textcolor{gray}{23.5} \\
			 \ding{183} & TubeDETR\textcolor{eccvblue}{$^{\blacklozenge}$} & - & 43.2 & 31.6 & 49.1 & 25.5 \\
               \ding{184} & TubeDETR\textcolor{eccvblue}{$^{\blacklozenge}$} & \checkmark & \hspace{2em}\textbf{45.5}\improve{+2.3} & \hspace{2em}\textbf{33.5}\improve{+1.9} & \hspace{2em}\textbf{53.0}\improve{+3.9} & \hspace{2em}\textbf{27.1}\improve{+1.6}\\ \hline
			\ding{185} & \textcolor{gray}{STCAT}\textcolor{orange}{$^{\blacklozenge}$} & \textcolor{gray}{-} & \textcolor{gray}{49.4} & \textcolor{gray}{35.1} & \textcolor{gray}{57.7} & \textcolor{gray}{30.1} \\
           \ding{186} & STCAT\textcolor{eccvblue}{$^{\blacklozenge}$} & - & {48.3} & {34.9} & {57.2} & {29.8} \\
           \ding{187} & STCAT\textcolor{eccvblue}{$^{\blacklozenge}$} & \checkmark & \hspace{2em}\textbf{50.0}\improve{+1.7} & \hspace{2em}\textbf{36.7}\improve{+1.8} & \hspace{2em}\textbf{59.9}\improve{+2.7} & \hspace{2em}\textbf{31.7}\improve{+1.9} \\
   
		      \specialrule{1.5pt}{0pt}{0pt}
	\end{tabular}}
	\label{tab:general}
 %\vspace{10pt}
\end{wraptable}
To validate the generality of our approach, we conduct experiments by applying TTS and ASA to other Transformer-based methods, including TubeDETR and STCAT. Since these two methods only use appearance features, we retain only the appearance branch to accommodate them. Tab.~\ref{tab:general} reports the results on HCSTVG-v1. Please note, we retrain  TubeDETR and STCAT on our platform for fair comparison. Although we try our best to re-implement these methods with provided code and settings, our results differ slightly from the original paper with small discrepancy. As in Tab.~\ref{tab:general}, we observe our method effectively and consistently improves TubeDETR and STCAT, achieving gains of 2.3\%/1.9\% (\ding{183} \emph{v.s.} \ding{184}) and 1.7\%/1.8\% (\ding{186} \emph{v.s.} \ding{187}) in m\_tIoU/m\_vIoU, evidencing its generality.

\section{Conclusion}

This paper proposes a novel Target-Aware Transformer for STVG (TA-STVG) by exploiting target-specific cues for query generation. The key lies in two effective modules of TTS and ASA. TTS aims to sample target-relevant temporal features using text as a guidance, while ASA explores fine-grained spatial features from coarse-grained temporal features. Working in a cascade, they allow exploiting crucial target-specific cues for generating target-aware object queries, which learn better target position information for STVG. Experiments on three datasets validate the efficacy of TA-STVG. 

\vspace{0.3em}
\textbf{Acknowledgements.} Libo Zhang is supported by National Natural Science Foundation of China (No. 62476266). Yan Huang, Yuewei Lin, and Heng Fan are not supported by any funds for this work.

{\small
\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}
}

\newpage

\appendix

\section*{\textbf{Supplemental Material}}


For better understanding of this work, we offer additional details, analysis, and results as follow:

\begin{itemize}
   \item \textbf{A \;\; Details and More Results for the Oracle Experiments} \\
   In this section, we display more detailed architecture for the Oracle experiment and report results on more datasets.
  
   \item \textbf{B \;\; Architectures for Self-attention Encoder, Spatial and Temporal Decoders} \\
   In this section, we show the details for the architecture of the self-attention encoder used for multimodal feature fusion. In addition, we display the architectures of spatial and temporal decoders. 
   
   \item \textbf{C \;\; Different Architectures of Activation Learning Strategies}\\
   In this section, we present details of architectures for activation learning strategies in ASA.
   
   \item \textbf{D \;\; Text Pre-processing and Attribute Label Construction}\\
   This section will illustrate the necessary text pre-processing for subject extraction. In addition, we will detail the construction of vocabulary for attribute labels.

   \item {\textbf{E \;\; Clarification for Multi-label Classification in ASA}}\\
   {In this section, we offer clarification for the multi-label classification in the ASA module.}

   \item {\textbf{F \;\; Additional Ablation Experiments}}\\
   {In this section, we show additional ablation experiments on different loss weights and the robustness of our method.}

   \item {\textbf{G \;\; Additional Discussion}}\\
   {In this section, we provided additional discussions on our method.}

   \item \textbf{H \;\; Efficiency and Complexity Analysis} \\
   In this section, we provide analysis of efficiency and model complexity for our proposed method and its comparisons with other state-of-the-art approaches.
   
  \item \textbf{I \;\; Qualitative Analysis and Results}\\
  In this section, we display qualitative analysis including relevance score learned by TTS, attribute-specific spatial activation learned by ASA, attention maps of object queries in the decoder, and  qualitative results for spatial-temporal target localization.
   
   \item \textbf{J \;\; Limitation} \\
   We discuss the limitation of our method.
\end{itemize}



\section{Details and More Results for the Oracle Experiments}
\label{sup_oracle}
To verify whether target-aware queries benefit localization, we conduct an Oracle experiment by generating spatial and temporal object queries from the ground-truth boxes. Similar to our TA-STVG, the architecture in this Oracle experiment consists of a multimodal encoder, query generation, and a decoder, as shown in Fig.~\ref{fig:oracle}. The difference with our method is that, in the Oracle experiment, object queries are directly generated from the groundtruth boxes of the target using RoI pooling. In contrast, our method generates object queries from the trained TTS and ASA modules.

\vspace{10pt}
\begin{figure}[hpt]
	\centering
	\includegraphics[width=0.95\linewidth]{oracle.pdf}
	\caption{Detailed architecture for the oracle experiment using groundtruth-generated object queries for STVG. \textcolor{blue}{$\mathcal{B}^*$} denotes the ground truth bounding box used to generate object queries.}
	\label{fig:oracle}
\end{figure}
\vspace{10pt}

We also conduct Oracle experiments on VidSTG~\citep{STGRN} and HCSTVG-v2~\citep{hcstvg} datasets, as shown in Fig.~\ref{fig:oracle_res}. The results reveal that, compared to zero-initialized queries, the groundtruth-generated queries have significantly improve performance on these two datasets, which once again demonstrates the importance of using target-aware queries in enhancing the performance of STVG.

\begin{figure}[hpt]
	\centering
	\includegraphics[width=0.95\linewidth]{oracle_res.pdf}
	\caption{Comparison of performance using zero-initialized object
queries and the groundtruth-generated object queries for STVG
on HCSTVG-v2 in (a) and VidSTG in (b).}
	\label{fig:oracle_res}
\end{figure}

\section{Architectures for Self-attention Encoder, Spatial and Temporal Decoders}
\label{network}

\subsection{Architecture of Self-Attention Encoder}

The self-attention encoder $\mathtt{SelfAttEncoder}(\cdot)$, composed of $L$ ($L=6$) standard self-attention encoder blocks, is used to fuse features from multiple modalities. The structure is shown in Fig.~\ref{fig:selfatt}.

\begin{figure}[!t]
\centering
\begin{minipage}[b]{0.49\textwidth}
\centering
    \includegraphics[width=1.\linewidth]{selfattencoder.pdf}
    \caption{Architecture of self-attention encoder.}
    \label{fig:selfatt}
\end{minipage}
\hfill
\begin{minipage}[b]{0.49\textwidth}
\centering
    \includegraphics[width=1.0\linewidth]{stdecoder.pdf}
    \caption{Architecture of  decoders.}
    \label{fig:stdecoder}
\end{minipage}
\end{figure}



\subsection{Architecture of Spatial and Temporal Decoder}
Similar to previous methods~\citep{cgstvg, STCAT}, we use a spatial decoder to learn spatial locations and a temporal decoder to learn temporal locations. The spatial decoder and the temporal decoder each consist of $K$ ($K=6$) blocks, with each block comprising a self-attention module and a cross-attention module. Fig.~\ref{fig:stdecoder} illustrates the architectures of the spatial and temporal decoders.

\begin{figure}[!t]
	\centering
	\includegraphics[width=0.9\linewidth]{instance_activate.pdf}
	\caption{The architectures of attribute-specific (a) and instance-level (b) spatial activation. }
	\label{fig:activation}
\end{figure}

\section{Different Architectures of Activation Learning Strategies}
\label{strategy}
In ASA, we compare two different strategies for learning spatial activation of the target. One is the proposed attribute-aware spatial activation which learns attribute-specific information by attribute classification, and the other one is to learn the naive instance-level spatial activation through binary classification. Fig.~\ref{fig:activation} (a) and (b) show the detailed architectures of these strategies. Specifically, for the instance-level spatial activation in (b), the classification results can be directly employed as the spatial activation maps $\mathcal{M}_a$ and $\mathcal{M}_m$ to generated the spatially activated features. Compared to the naive instance-level activation method, our attribute-specific spatial activate enables learning more discriminative fine-grained features, which are crucial to distinguish the target from the background.



\section{Text Pre-processing and Attribute Label Construction}
\label{subject}

\textbf{Subject Extraction.} In the ASA module, we guide the learning of activation  using the subject from the text. To this end, we employ the Stanford CoreNLP tool\footnote{stanfordnlp.github.io/CoreNLP/} to conduct syntactic analysis on the text, transforming it into a structured graph. From the generated graph, we are able to extract the subject along with the related adjectives for appearance such as color and shape and verbs for motion such as various actions based on the part-of-speech tagging. The adjectives and verbs extracted are respectively used to produce the appearance and motion attribute labels. {It is worth noticing that, since current STVG task focuses on localizing a single target (and thus there is only one subject in the query), we extract only one subject representation from the textual sentence.}

\textbf{Vocabulary and Attribute Label Construction}. The ASA module is trained by predicting attribute classes. For this purpose, we have constructed two vocabularies for each dataset, including an appearance vocabulary and a motion vocabulary. Please note that, all information is from the dataset itself, without using extra information. Specifically, we first count the frequency of attribute words (adjectives or verbs related to the subject.) in each textual description in the dataset, and then eliminate words that appear less than 50 times. The remaining words constitute the vocabulary. The lengths of the appearance vocabulary and motion vocabulary for HCSTVG-v1/v2 are 25 and 40, respectively, while for VidSTG, they are 36 and 45, respectively. With the vocabulary, we can construct the multi-class attribute label for each dataset based on the words in the textural expression.

 \section{{Clarification for Multi-label Classification in ASA}}
 
 {In this section, we provide more clarifications for the multi-label classification in our ASA module. Specifically, we answer the following questions: \textbf{(a)} Are the extracted subject features for appearance and motion identical? \textbf{(b)} What is the relationship between weak attribute labels and subject features, and \textbf{(c)} do the weak labels update dynamically?}

 {\textbf{For (a)}: The extracted subject features for appearance and motion are identical. Because the subject is usually the target of interest, and we want to extract its related appearance and motion features. Thus, we use the identical subject as the query for learning both appearance and motion information. \textbf{For (b)}: The weak attribute labels and the subject both come from the given textual query (please see Sec. D in our supplementary material on how to obtain these two). For example, in the textual query `The tall girl is walking', `girl' is the subject, while `tall' and `walking' are the weak attribute labels. The subject feature is used as the query to learn relevant attribute information from the visual features for the goal of multi-attribute classification. \textbf{For (c)}: Given the textual query, the weak label for the weak attributes is generated from the query and is fixed (not updated).}

\section{{Additional ablation experiments}}

{\textbf{Impact of different loss weights.} Similarly to the previous approaches, our model is trained with multiple losses, including TTS loss, ASA loss, spatial decoder loss, and temporal decoder loss. The weights for the spatial and temporal decoder losses are kept consistent works such as STCAT~\citep{STCAT} and CG-STVG~\citep{cgstvg}. The weights for TTS and ASA losses are empirically set. We have conducted an ablation on the weights $\lambda_{\text{TTS}}$ and $\lambda_{\text{ASA}}$ for TTS and ASA losses, and the results are shown in Tab.~\ref{tab:loss}. From Tab.~\ref{tab:loss}, we observe that when setting $\lambda_{\text{TTS}}$ and $\lambda_{\text{ASA}}$ to 1 and 1 (see \ding{182}), respectively, we achieve the best performance.}

\begin{table}[!t]
\centering
\caption{{Ablations of different loss weights on HCSTVG-v1.}}
\label{tab:loss}
\scalebox{0.8}{
\begin{tabular}{ccccccc}
\specialrule{1.5pt}{0pt}{0pt}
\rowcolor{mygray} 
& {$\lambda_{\text{TTS}}$} & {$\lambda_{\text{ASA}}$} & {m\_tIoU} & {m\_vIoU} & {vIoU@0.3} & {vIoU@0.5} \\
\hline
\ding{182} &{1} & {1} & {\textbf{53.0}} & {\textbf{39.1}} & {63.1} & {\textbf{36.8}} \\
\ding{183} &{1} & {5} & {52.9} & {39.0} & {\textbf{63.3}} & {36.5} \\
\ding{184} &{5} & {1} & {52.8} & {38.7} & {62.8} & {35.9} \\
\ding{185} &{5} & {5} & {52.6} & {38.8} & {62.7} & {36.3} \\
\specialrule{1.5pt}{0pt}{0pt}
\end{tabular}}
\vspace{-2mm}
\label{tab:performance}
\end{table}

\begin{table}[!t]
\centering
\caption{{Comparison of existing methods in complex scenarios on Hard-HCSTVG-v1.}}
\label{tab:hard}
\scalebox{0.8}{\begin{tabular}{clcccc}
\specialrule{1.5pt}{0pt}{0pt}
\rowcolor{mygray}
& {Method} & {m\_tIoU} & {m\_vIoU} & {vIoU@0.3} & {vIoU@0.5} \\
\hline
\ding{182} & {TubeDETR} & {40.4} & {28.3} & {42.3} & {14.3} \\
\ding{183} & {STCAT} & {42.9} & {28.8} & {47.3} & {19.3} \\
\ding{184} & {CG-STVG} & {45.5} & {31.4} & {50.3} & {20.0} \\ \hline
\ding{185} & {Baseline (ours)} & {43.1} & {29.3} & {48.8} & {18.7} \\
\ding{186} & {TA-STVG (ours)} & {\textbf{45.9}} & {\textbf{32.6}} & {\textbf{54.7}} & {\textbf{22.3}} \\
\specialrule{1.5pt}{0pt}{0pt}
\end{tabular}}
\label{tab:performance}
\end{table}


{\textbf{Ablation on the robustness of TA-STVG.}
In order to show the robustness of our TA-STVG again challenges, we manually select videos from a current benchmark HCSTVG-v1, in which the objects suffer from heavy occlusions, similar distractors, or noisy text descriptions. The resulted subset is called \textbf{\emph{Hard-HCSTVG-v1}} (it will be released together with our source code), containing 300 videos selected from the test set of HCSTVG-v1. We show our performance on this Hard-HCSTVG-v1 and compare it with other state-of-the-art models in Tab.~\ref{tab:hard}. From Tab.~\ref{tab:hard}, we observe that, due to increased complexity, all the models degrade on the more difficult Hard-HCSTVG-v1 (please see Tab.~\ref{tab:hard} here and Tab.~\ref{tab:hcstvgv1} in the paper). Despite this, our proposed TA-STVG still achieves the best performance (45.9/32.6/54.7/22.3 in m\_tIoU/m\_vIoU/vIoU@0.3/vIoU@0.5, see \ding{186}) by outperforming CG-STVG (45.5/31.4/50.3/20.0 in m\_tIoU/m\_vIoU/vIoU@0.3/vIoU@0.5, see \ding{184}), STCAT (42.9/28.8/47.3/19.3 in m\_tIoU/m\_vIoU/vIoU@0.3/vIoU@0.5, see \ding{183}), and TubeDETR (40.4/28.3/42.3/14.3 in m\_tIoU/m\_vIoU/vIoU@0.3/vIoU@0.5, see \ding{182}). Further more, TA-STVG significantly improves its baseline under these challenges (\ding{186} \emph{v.s.} \ding{185}). All of these are attributed to our TTS and ASA modules that mine discriminative information of the target, even in challenging scenarios, for localization, showing their efficacy for improving STVG.}

\section{{Additional Discussion}}

{\textbf{Discussion on construction of frame-specific object queries}.
In this work, the feature output by the ASA module undergoes a pooling and repetition to generate the final object queries. The \textbf{\emph{reasons}} for not constructing frame-specific object queries are two-fold: (1) For STVG, we aim to obtain a global-level target-relevant query by pooling, which ensures the consistency in target prediction across different frames, \ie, localizing the same target instance in all frames using global information; and (2) Since the ASA module only extracts target features from frames selected by the previous TTS module, not every frame has its own target-specific feature. As a result, it is hard to directly construct frame-specific object queries in our work.}


{\textbf{Discussion of computational consumption and future directions}. Our TA-STVG needs a high demand of computational resource. In fact, this is a common issue in the STVG field. The computational complexity of STVG methods is high, because STVG models, particularly current Transformer-based ones (including our work), require to receive all sampled video frames at once to make the prediction, leading to the requirement of significant computational resources. That being said, our proposed modules TTS and ASA are lightweight and only bring about 4M parameters and 0.09T FLOPS compared to the baseline method (please see Tab.~\ref{complex}).
In the future, we will further explore efficient STVG in the future, such as (1) how to develop lightweight STVG architecture by reducing unnecessary network parameters, aiming to improve training and inference efficiency, and (2) how to leverage parameter-efficient fine-tuning (PETR) techniques like adapter or prompt learning for STVG model training, aiming to reduce the training complexity.}


{\textbf{Discussion on handling videos with multiple targets}.
For STVG, the video may comprise multiple targets which are described in a single textual query. Since the target of interest is often the unique subject of the textual query, our model is able to leverage many descriptions such as motion and color attributes via our proposed modules, as well as contextual information like interaction with others by our spatio-temporal modeling in TA-STVG to distinguish the target from other objects.}


{\textbf{Discussion on adapting TTS and ASA to other multimodal data.} In this work, we introduce two modules TTS and ASA for STVG. Besides STVG, 
TTS and ASA (with appropriate adaptations) can be applied to work with other multimodal data. The essence of both TTS and ASA is to align two different correlated modalities (in our work, they are visual and text modalities). Specifically, TTS aims to align the frame-level feature with the global sentence-level feature, while ASA works to align the region-level feature with the local word-level feature. For other multimodal data, such as audio description and video, TTS and ASA can be used in a similar way. For example, TTS can be used to align the video with the global audio description to remove irrelevant video content, and ASA can be applied to align spatial regions in the video to more fine-grained local audio descriptions (like attributes in textual description). For 3D videos, TTS and ASA can be used similarly.}


{\textbf{Analysis on multi-label classification in the ASA}.
In this work, the extracted subject features for appearance and motion are identical. Because the subject is usually the target of interest, and we want to extract its related appearance and motion features. Thus, we use the identical subject as the query for learning both appearance and motion information. we use the identical subject as the query for learning both appearance and motion information. The attribute labels and the subject both come from the given textual query. Given the textual query, the label for the weak attributes is generated from the query, and is fixed.
The subject feature is used as the query to learn relevant attribute information from the visual features for the goal of multi-attribute classification.}

{\textbf{Discussion on sampled target-relevant features on ASA.} In our TA-STVG, the information of sampled frames by TTS is crucial for the subsequent ASA (which extracts attribute-related target information from sample frames for generating queries). We discuss the impact of sampled target-relevant frames on ASA in two situations:
\textbf{(a)} Part of the selected frames does not contain the target object, while the other part contains the target. In this case, since the attention mechanism is utilized in the subsequence ASA, it can still extract useful target-aware information from partial sampled frames that contain the target while decreasing the weight of sampled frames without targets. As a result, we can still generate effective target-aware query for improving STVG. For the worse case in this situation, the subsequent ASA fails, the sampled frames from TTS alone, even only partial frames contain targets, can still generate useful queries to enhance STVG performance, as shown in Tab.~\ref{tab:ttsasa} (see \ding{182} and \ding{183} in Tab.~\ref{tab:ttsasa} in the paper).
\textbf{(b)} All sampled frames from TTS do not contain the target. If unfortunately this happens, then the final generated queries after ASA will not contain effective target-aware information, which is equivalent to using randomness-initialized query for STVG and thus may degrade the final performance. However, we think that this case might be \emph{very occasional}, as from Tab.~\ref{tab:ttsasa} in the paper, we can see that TTS alone can significantly improve baseline performance. This means that TTS can usually generate effective sampled frames that contain the target. We may leave the exploration of this to our future work.}

{\textbf{Discussion on our innovation and contribution.} In this work, we emphasize that, our innovation and contribution is the \textbf{\emph{idea}} of \emph{\textbf{exploring target-aware queries for improving STVG}}, which has \textbf{\emph{never}} been studied in the STVG field. To achieve the target-aware queries for STVG, we proposed two modules, TTS and ASA. The reason why implementing TTS and ASA with attention is because the adopted cross-attention perfectly meets our demand in extracting object query features and is simply enough (the pursuit of \emph{simple but effective} architecture is always our motivation). In experiments, we show our proposed TA-STVG is able to achieve the best performance on all three datasets, showing the significant contribution of our idea of exploring target-aware for STVG. Moreover, our idea is general and when applied to other frameworks such as STCAT and TubeDETR, we demonstrate consistent improvements, evidencing its efficacy. }

\begin{table}[!t]
\centering
\renewcommand{\arraystretch}{1}
\caption{Comparison on model efficacy and complexity on VidSTG. The inference is conducted on a single A100 GPU, and inference time refers to the duration of a single forward propagation.}
\scalebox{0.7}{
    \begin{tabular}{c|cc|cc|cc|c|cc}
    \specialrule{1.5pt}{0pt}{0pt}
    \rowcolor{mygray} 
\cellcolor{mygray} & \multicolumn{2}{c|}{ \cellcolor{mygray} Params} & \multicolumn{2}{c|}{ \cellcolor{mygray} Training} & \multicolumn{2}{c|}{ \cellcolor{mygray}Inference} & \cellcolor{mygray} & \cellcolor{mygray} & \cellcolor{mygray} \\ 
\rowcolor{mygray} 
\multirow{-2}{*}{\cellcolor{mygray} Methods} & Trainable & Total & Time & GPU Num & Time & GPU Mem & \multirow{-2}{*}{\cellcolor{mygray}FLOPS} & \multirow{-2}{*}{\cellcolor{mygray} m\_tIoU} & \multirow{-2}{*}{\cellcolor{mygray} m\_vIoU} \\
			\hline
			\hline
			TubeDETR~\citep{TubeDETR} & 185M & 185M & 48 h & 16 V100 & 0.40 s & 24.4 & 1.45 T & 48.1 & 30.4\\
			STCAT~\citep{STCAT} & 207M & 207M & 12 h & 32 A100 & 0.51 s & 30.4 & 2.85 T & 50.8 & 33.1\\
			STVGFormer~\citep{csdvl} & - & - & $\sim$48 h & 8 A6000 & - & - & - & - & 33.7\\ 
                CG-STVG~\citep{cgstvg} & 203M & 231M & 13.6 h & 32 A100 & 0.61 s & 29.7 & 3.03 T & 51.4 & 34.0\\\hline
			Baseline (ours) & 202M & 230M & 14 h & 32 A100 & 0.53 s & 28.2 & 2.88 T & 49.5 & 32.3\\
                TA-STVG (ours) & 206M & 234M & 14.5 h & 32 A100 & 0.57 s & 28.4 & 2.97 T & \textbf{51.7} & \textbf{34.4}\\
			\specialrule{1.5pt}{0pt}{0pt}
	\end{tabular}}
	\label{complex}
\end{table}


\section{Efficiency and Complexity Analysis}
\label{efficiency}
To analyze the efficacy and complexity of our model, we conduct a comparative analysis involving the number of parameters, training consumption, inference consumption, and FLOPS against other existing methods. {As shown in Tab.~\ref{complex}, we can see that, our proposed maintains similar computational complexity (206/234M trainable/total parameters and 2.97T FLOPS) to existing state-of-the-art models such as STCAT (207M/207M trainable/total parameters and 2.85T FLOPS) and CG-STVG (203M/231M trainable/total parameters and 3.03T FLOPS), while achieving better performance with the best m\_tIoU of 51.7 and m\_vIoU of 34.4 on VidSTG. Moreover, compared to the baseline method, TA-STVG brings negligible parameter and computational increments (Baseline \emph{v.s.} TA-STVG: 202M \emph{v.s.} 206M in trainable parameters, 230M \emph{v.s.} 234M in total parameters, and 2.88T \emph{v.s.} 2.97T in FLOPS), yet significantly enhances the performance (m\_tIoU improved from 49.5 to 51.7 in and m\_vIoU from 32.3 to 34.4), underscoring its superiority.}



\section{Qualitative Analysis and Results}
\label{quality}

\subsection{Visualization of Temporal Relevance Score and Attribute-specific Activation}

In this section, we show more visualizations of the temporal relevance scores learned by TTS and attribute-specific spatial activation learned by ASA in Fig.~\ref{fig:att_ttsasa}. From Fig.~\ref{fig:att_ttsasa}, we can see that the frames, selected by the temporal relevance score $s$, will be within the groundtruth, which shows the selected features are target-relevant. In addition, the attribute-specific spatial attention maps $\mathcal{M}_a$ and $\mathcal{M}_m$ can well localize the related attributes such as color and different actions of the target. F instance, in the first example, the text is ``The \textit{man} in \textit{green} clothes \textit{walks} behind the man in brown''. The highly relevant areas in the appearance attention map are mainly concentrated around the green clothes, while in the motion attention map are focused on the legs. These attribute-specific spatial activation helps with learning more discriminative fine-grained features for improving STVG.




\subsection{Attention Maps of Object Queries in the Spatial Decoder}


To show the role of target-aware queries, we visualize its attention maps in the spatial decoder with comparison to the attention maps of zero-initialized queries, as shown in Fig.~\ref{fig:att_decoder}. As in Fig.~\ref{fig:att_decoder}, the attention maps from zero-initialized queries are scattered, leading to imprecise target localization. In contrast, the attention maps from target-aware queries highly focus on the target, demonstrating that target-aware object queries can better learn target position information for improving STVG.

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.88\linewidth]{att_ttsasa.pdf}
    \caption{Visualization of temporal relevance score predicted by TTS and  attribute-aware activation score predicted by ASA. The red boxes indicate the temporal and spatial foreground object to localize.}
    \label{fig:att_ttsasa}
\end{figure}

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.88\linewidth]{decoderatt_new.pdf}
    \caption{Comparison of attention maps for zero-initialized object queries and the proposed target-aware object queries in video frames in the spatial decoder. The red boxes indicate the foreground target to localize. From this figure, we can clearly observe that the attention maps of our target-aware object queries by TTS and ASA can better focus on the target object for localization.}
    \label{fig:att_decoder}
\end{figure}

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.9\linewidth]{qualitate.pdf}
    \caption{Qualitative results of our TA-STVG (red), the baseline (blue), and ground truth (green).}
    \label{fig:qualitate}
\end{figure}

\subsection{Qualitative Grounding Results}

To further qualitatively validate the effectiveness of our proposed method, we provide its grounding results and comparison with the baseline method (with zero-initialization queries) on HCSTVG-v1/v2, as shown in Fig.~\ref{fig:qualitate}. From Fig.~\ref{fig:qualitate}, it is evident that our method outperforms the baseline in both temporal and spatial localization. This suggests that queries with target-specific fine-grained visual information can better learn the spatio-temporal position of the target, thereby enhancing spatio-temporal grounding capability of the model.

For instance, in the first example, the task is to locate a man wearing green clothes and walking. The baseline model, using zero-initialized queries, fails to locate the target due to incorrect identification. In contrast, our model, through the proposed TTS and ASA modules, extracts target-related visual information such as ``green'' and ``walk'' from the video as initial queries, accurately locating the target object. The same comparison is observed in other examples. These results and comparison further show that initializing queries with fine-grained visual information of target can improve STVG.


\section{Limitation}
\label{limitation}
Despite achieving state-of-the-art performance on multiple datasets, our method has two main limitations. First, although TTS and ASA are able to provide better initialization for object queries, these queries still inevitably contain some noise compared to the desired information, resulting in lower performance than the Oracle experiments with groundtruth-generated object queries. In the future, one direction is to further improve the quality of our target-aware object queries. Second, similar to all other STVG approaches, our method consumes a large amount of computational resources for training, leading to lengthy training period. In future, we will study resource-friendly STVG using techniques from parameter-efficient learning methods.


\end{document}
