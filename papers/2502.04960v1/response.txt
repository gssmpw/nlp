\section{Related Work}
Early humor recognition research predominantly utilized feature engineering and traditional machine learning techniques to identify linguistic and stylistic features of humorous texts**Aue, "Humour Recognition: A Survey"**.
For example, **Kumar et al., "Humor Modeling using Alliteration and Antonyms"** employed features such as alliteration, antonyms, and adult slang to model humor, using classifiers like Naive Bayes (NB) and Support Vector Machines (SVM) **Huang et al., "Support Vector Machine for Humour Recognition"** for humor recognition. **Wang et al., "Feature-Based Humor Recognition with Grammatical, Syntactic, Semantic, and Pragmatic Features"** explored grammatical, syntactic, semantic, and pragmatic features of humor texts, enriching the field of feature-based humor recognition. **Liu et al., "Emotional Analysis for Humour Recognition"** combined emotional and semantic analysis to model the emotional aspects of humor. 
While effective in some scenarios, these methods significantly struggle to capture the deep semantic and contextual nuances vital for understanding humor. 
Additionally, their reliance on manually designed features, which are labor-intensive to create and inflexible in application, further limits their effectiveness.

The advent of deep learning marked a significant shift towards using neural networks for automatic feature extraction. 
Additionally, the introduction of Transformer-based pretrained language models significantly enhanced the performance of humor recognition systems. 
For instance, **Srivastava et al., "Deep Learning-Based Humor Recognition in Speeches and Puns"** developed a CNN-based model to recognize humor in speeches and puns, outperforming traditional methods even without manually designed features. 
**Rajput et al., "Transformer Model for Improved Humour Pattern Analysis"** introduced the Transformer model, which further improved the ability to learn complex humor patterns and exhibited superior performance compared to other deep neural networks. 
**Xu et al., "Using GPT to Evaluate Inconsistency Scores in Jokes"** utilized GPT **to calculate the inconsistency between punchlines in jokes, enhancing humor recognition based on these inconsistency scores. 
**Chen et al., "BERT for Humour Recognition with Sentence Embeddings"** used pretrained BERT **to obtain sentence embeddings for humour recognition. 
**Li et al., "Graph Neural Networks and Social Network Information for Enhanced Humor Recognition"** leveraged social network information and graph neural networks to enhance humor recognition results.

Despite significant progress in humor recognition research, recent approaches tend to focus primarily on single semantic layers, struggling to analyze user language from the diverse common perspectives of humour expression. Additionally, a critical limitation of these methods is their neglect of the speakerâ€™s individual expression styles. 

\begin{figure*}[!ht]
\centering
    \includegraphics[scale=0.75, trim=30 222 0 287, clip]{figures/model.pdf}
\caption{The overall architecture of the proposed CIHR model, which consists of four main module: Humor Commonalities Analysis, Speaker Individuality Extraction, Static Fusion and Dynamic Fusion.}
\label{fig:model}
\end{figure*}