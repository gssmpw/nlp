\section{Related Work}
Early humor recognition research predominantly utilized feature engineering and traditional machine learning techniques to identify linguistic and stylistic features of humorous texts\cite{yang2015humor, liu2018modeling}.
For example, \citet{mihalcea2005making} employed features such as alliteration, antonyms, and adult slang to model humor, using classifiers like Naive Bayes (NB) and Support Vector Machines (SVM) \citep{cortes1995support} for humor recognition. \citet{raz2012automatic} explored grammatical, syntactic, semantic, and pragmatic features of humor texts, enriching the field of feature-based humor recognition. \citet{liu2018modeling} combined emotional and semantic analysis to model the emotional aspects of humor. 
While effective in some scenarios, these methods significantly struggle to capture the deep semantic and contextual nuances vital for understanding humor. 
Additionally, their reliance on manually designed features, which are labor-intensive to create and inflexible in application, further limits their effectiveness.

The advent of deep learning marked a significant shift towards using neural networks for automatic feature extraction. 
Additionally, the introduction of Transformer-based pretrained language models significantly enhanced the performance of humor recognition systems. 
For instance, \citet{chen2017predicting} developed a CNN-based model to recognize humor in speeches and puns, outperforming traditional methods even without manually designed features. 
\citet{weller2019humor} introduced the Transformer model, which further improved the ability to learn complex humor patterns and exhibited superior performance compared to other deep neural networks. 
\citet{xie2021uncertainty} utilized GPT \citep{radford2018improving} to calculate the inconsistency between punchlines in jokes, enhancing humor recognition based on these inconsistency scores. 
\citep{annamoradnejad2024colbert, yang2021choral} used pretrained BERT \citep{kenton2019bert} to obtain sentence embeddings for humor recognition. 
\citep{zeng2024leveraging} leveraged social network information and graph neural networks to enhance humor recognition results.

Despite significant progress in humor recognition research, recent approaches tend to focus primarily on single semantic layers, struggling to analyze user language from the diverse common perspectives of humor expression. Additionally, a critical limitation of these methods is their neglect of the speakerâ€™s individual expression styles. 

\begin{figure*}[!ht]
\centering
    \includegraphics[scale=0.75, trim=30 222 0 287, clip]{figures/model.pdf}
\caption{The overall architecture of the proposed CIHR model, which consists of four main module: Humor Commonalities Analysis, Speaker Individuality Extraction, Static Fusion and Dynamic Fusion.}
\label{fig:model}
\end{figure*}