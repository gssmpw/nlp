

\paragraph{Roadmap.}  
In Section~\ref{sec:app:original_algorithm}, we introduce the Shortcut Model Training and Sampling Algorithm. Section~\ref{sec:app:related_work} discusses related works that inspire our approach. 
Section~\ref{sec:app:main_theorems} states the tools from \cite{fsi+24} used in our analysis. Section~\ref{sec:app:higher_order_flow_matching} explores the theory behind Higher-Order Flow Matching. Section~\ref{sec:app:empirical_ablation_study} investigates the impact of different optimization terms through empirical ablation studies. Section~\ref{sec:app:complex_distribution_experiment} examines model performance on complex distribution experiments. 
Section~\ref{sec:app:3rd_homo} extends HOMO to third-order dynamics and evaluates its effectiveness on complex tasks. Section~\ref{sec:app:computational_cost} quantifies the computational and optimization costs associated with different configurations.

\section{Original Algorithm}\label{sec:app:original_algorithm}
Here we introduce Shortcut Model Training and Sampling Algorithm from Page 5 of~\cite{fhla24}
\begin{algorithm}[!ht]\caption{Shortcut Model Training from page 5 of~\cite{fhla24}}
\begin{algorithmic}[1]
\While{not converged}
\State $x_0 \sim \N (0, I), x_1 \sim D, (d, t) \sim p(d, t)$
\State $x_t \gets (1 - t) x_0 + t x_1$ \Comment{Noise data point}
\For{first $k$ batch elements}
\State $s_{\mathrm{target}} \gets x_1 - x_0$ \Comment{Flow-matching target}
\State $d \gets 0$
\EndFor
\For{other batch elements}
\State $s_t \gets u_1 ( x_t, t, d )$ \Comment{Fitst small step}
\State $x_{t + d} \gets x_t + s_t d$ \Comment{Follow ODE}
\State $s_{t + d} \gets u_1 ( x_{t + d}, t + d, d )$ \Comment{Second small step}
\State $s_{\mathrm{target}} \gets$ stopgrad $( s_t + s_{t + d} ) / 2$ \Comment{Self-consistency target}
\EndFor
\State $\theta \gets \nabla_\theta { \| u_1 ( x_t, t, 2d ) - s_{\mathrm{target}} \|^2 }$
\EndWhile
\end{algorithmic}
\end{algorithm}








\begin{algorithm}[!ht]\caption{Shortcut model. Sampling from page 5 of~\cite{fhla24}}
\begin{algorithmic}[1]
\State $x \sim \N (0, I)$
\State $d \gets 1 / M$
\State $t \gets 0$
\For{$n \in [0, \dots, M - 1]$}
\State $x \gets x + d \cdot u_1 (x, t, d)$
\State $t \gets t + d$
\EndFor
\State \textbf{return} $x$
\end{algorithmic}
\end{algorithm}


\section{More related work}\label{sec:app:related_work}

In this section, we discuss more related work which inspire our work.
\paragraph{Large Language Models.}Neural networks built upon the Transformer architecture~\cite{vsp+17} have swiftly risen to dominate modern machine learning approaches in natural language processing. Extensive Transformer models, trained on wide-ranging and voluminous datasets while encompassing billions of parameters, are often termed large language models (LLM) or foundation models~\cite{bha+21}. Representative instances include BERT~\cite{dclt19}, PaLM~\cite{cnd+22}, Llama~\cite{tli+23}, ChatGPT~\cite{chatgpt}, GPT4~\cite{o23}, among others. These LLMs have showcased striking general intelligence abilities~\cite{bce+23} in various downstream tasks. Numerous adaptation methods have been developed to tailor LLMs for specific applications, such as adapters~\cite{eyp+22,zhz+23,ghz+23,zjk+23}, calibration schemes~\cite{zwf+21,cpp+23}, multitask fine-tuning \cite{gfc+21a,zzj+23a,vnr+23,zzj+23b}, prompt optimization~\cite{gfc+21b,lac+21}, scratchpad approaches \cite{naa+21}, instruction tuning~\cite{ll21,chl+22,mkd+22}, symbol tuning~\cite{jla+23}, black-box tuning~\cite{ssy+22}, and reinforcement learning from human feedback (RLHF)~\cite{owj+22}. Additional lines of research endeavor to boost model efficiency without sacrificing performance across diverse domains, for example in \cite{dswy22_coreset,swy23,gswy23,gsy23_hyper,gsy23_coin,bsy23,dms23_spar,gms23,ssx23_nns,qss23_gnn,cls+24,lsy24,cll+24_icl,lss+24_multi_layer,cll+24_rope,lsss24_dp_ntk,lss+24_relu,lls+24_prune,llss24_sparsegpt,llsz24_nn_tw,cls24_grams,lls+24_io,cll+24_ssm,chl+24_rope_grad,kll+24_fixed,kll+25,kll+25_tc,lls+25_graph,hwg+24}.











