\section{Related Works}
\paragraph{Diffusion Models.} Diffusion models have garnered significant attention for their capability to generate high-fidelity images by incrementally refining noisy samples, as exemplified by DiT____ and U-ViT____. These approaches typically involve a forward process that systematically adds noise to an initial clean image and a corresponding reverse process that learns to remove noise step by step, thereby recovering the underlying data distribution in a probabilistic manner. Early works____ established the theoretical foundations of this denoising strategy, introducing score-matching and continuous-time diffusion frameworks that significantly improved sample quality and diversity. Subsequent research has focused on more efficient training and sampling procedures____, aiming to reduce computational overhead and converge faster without sacrificing image fidelity. Other lines of work leverage latent spaces to learn compressed representations, thereby streamlining both training and inference____. This latent learning approach integrates naturally with modern neural architectures and can be extended to various modalities beyond images, showcasing the versatility of diffusion processes in modeling complex data distributions. In parallel, recent researchers have also explored multi-scale noise scheduling and adaptive step-size strategies to enhance convergence stability and maintain high-resolution detail in generated content in ____. There are more other works also inspire our work____.

\paragraph{Flow Matching. }
Generative models like diffusion ____ and flow-matching ____ operate by learning ordinary differential equations (ODEs) that map noise to data. To simplify, this study leverages the optimal transport flow-matching formulation ____. A linear combination of a noise sample $x_0 \sim \mathcal{N}(0, \mathbb{I})$ and a data point $x_1 \sim \mathcal{D}$ defines $x_t$:
\begin{align*}
x_t = (1-t)x_0 + tx_1,
\qquad v_t = x_1 - x_0,
\end{align*}
with $v_t$ representing the velocity vector directed from $x_0$ to $x_1$. While $v_t$ is uniquely derived from $(x_0, x_1)$, knowledge of only $x_t$ renders it a random variable due to the ambiguity in selecting $(x_0, x_1)$. Neural networks in flow models approximate the expected velocity $\bar{v}_t = \mathbb{E}[v_t \mid x_t]$, calculated as an average over all valid pairings. Training involves minimizing the deviation between predicted and empirical velocities:
\begin{align}\label{eq:flow-matching}
& ~ \notag \bar{v}_\theta(x_t, t) \sim \mathbb{E}_{x_0,x_1 \sim \D} \left[ v_t \mid x_t \right] \\ 
& ~ \mathcal{L}^{\mathrm{F}}(\theta) = \mathbb{E}_{x_0,x_1 \sim \D} \left[ \| \bar{v}_\theta(x_t, t) - (x_1-x_0) \|^2 \right].
\end{align}
Sampling involves first drawing a noise point $x_0 \sim \mathcal{N}(0, I)$ and iteratively transforming it into a data point $x_1$. The denoising ODEs, parameterized by $\bar{v}_\theta(x_t, t)$, governs this transformation, and Eulerâ€™s method approximates it over small, discrete time steps.

\paragraph{High-order ODE Gradient in Diffusion Models. }

Higher-order gradient-based methods like TTMs____ have applications far exceeding DDMs. For instance, solvers____ and regularization frameworks____ for neural ODEs____ frequently utilize higher-order derivatives. Beyond machine learning contexts, the study of higher-order TTMs has been extensively directed toward solving stiff____ and non-stiff____ systems.