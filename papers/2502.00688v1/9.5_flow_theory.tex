\section{Tools from Previous Works}\label{sec:app:main_theorems}

We state the tools in \cite{fsi+24} that we will use to prove our main results.

\subsection{Definitions of Besov Space}

\begin{definition}[Modulus of Smoothness]
\label{def:modulus_smoothness}
Let $\Omega$ be a domain in $\R^d$. For a function 
$f \in L^{p'}(\Omega)$ with $p' \in (0,\infty]$, 
the $r$-th modulus of smoothness of $f$ is defined by
\begin{align*}
    w_{r, p'}(f,t) = \sup_{\|h\|_2 \leq t} \|\Delta_h^r (f) \|_{p'},
\end{align*}
where the finite difference operator $\Delta_h^r (f)(x)$ is given by
\begin{align*}
    \Delta_h^r (f)(x) =
\begin{cases}
\sum_{j=0}^{r} \binom{r}{j} (-1)^{r-j} f(x + jh), & \mathrm{if } x + jh \in \Omega \mathrm{ for all } j, \\
0, & \mathrm{otherwise}.
\end{cases}
\end{align*}
\end{definition}

\begin{definition}[Besov Seminorm]
\label{def:besov_seminorm}
Let $0 < p', q' \leq \infty$, $s > 0$, and set $r := | s | + 1$. The Besov seminorm of $f \in L^{p'}(\Omega)$ is defined as
\begin{align*}
| f |_{B^{s}_{p',q'}} :=
\begin{cases}
\left( \int_0^{\infty} (t^{-s} w_{r, p'}(f,t))^{q'} \frac{dt}{t} \right)^{\frac{1}{q'}}, & q' < \infty, \\
\sup_{t>0} t^{-s} w_{r, p'}(f,t), & q' = \infty.
\end{cases}
\end{align*}
\end{definition}

\begin{definition}[Besov Space]
\label{def:besov_space}
The Besov space $B^{s}_{p',q'}(\Omega)$ is the function space equipped with the norm
\begin{align*}
    \| f \|_{B^{s}_{p',q'}} := \| f \|_{p'} + | f |_{B^{s}_{p',q'}},
\end{align*}
It consists of all functions $f \in L^{p'}(\Omega)$ such that
\begin{align*}
    B^{s}_{p',q'}(\Omega) := \{ f \in L^{p'}(\Omega) \mid \| f \|_{B^{s}_{p',q'}} < \infty \}.
\end{align*}
\end{definition}

\begin{remark}
The parameter $s$ governs the degree of smoothness of functions in $B^{s}_{p',q'}(\Omega)$. In particular, when $p' = q'$ and $s$ is an integer, the Besov space $B^{s}_{p',q'}(\Omega)$ coincides with the standard Sobolev space of order $s$. For further details on the properties and applications of Besov spaces, see \cite{t92}.
\end{remark}

\subsection{B-spline}

\begin{definition}[Indicator Function]\label{def:indicator_func}
Let ${\cal N}(x)$ be the characteristic function defined by
\begin{align*}
    {\cal N}(x) &=
    \begin{cases}
        1, & x \in [0,1],\\
        0, & \mathrm{otherwise}.
    \end{cases}
\end{align*}
\end{definition}

\begin{definition}[Cardinal B-Spline]\label{def:cardinal_b_spline}
For $\ell \in \N$, the cardinal B-spline of order $\ell$ is defined by
\begin{align*}
    {\cal N}_\ell(x) 
    := & 
    \underbrace{{\cal N} * {\cal N} * \cdots * {\cal N}}_{\ell+1 \mathrm{ times}}(x),
\end{align*}
where $*$ denotes the convolution operation. Explicitly, the convolution of two functions $f,g: \R \to \R$ is given by
\begin{align*}
    (f * g)(x) =& \int_{\R} f(x-y)  g(y)  \d y.
\end{align*}
Thus, ${\cal N}_\ell(x)$ is obtained by convolving ${\cal N}$ with itself $(\ell+1)$ times.
\end{definition}

\begin{definition}[Tensor Product B-Spline Basis]\label{def:tensor_product_b_spline}
For a multi-index $k \in \N^d$ and $j \in  \Z^d$, the tensor product B-spline basis in $ \R^d$ of order $\ell$ is defined as
\begin{align*}
    M_{k,j}^d(x) :=&~ \prod_{i=1}^d {\cal N}_\ell (2^{k_i} x_i - j_i).
\end{align*}
This basis is constructed as the product of univariate B-splines, scaled and translated according to the parameters $k$ and $j$.
\end{definition}

\begin{definition}[B-Spline Approximation in Besov Spaces in \cite{s19, oas23}]\label{def:approx_b_spline}
A function $f$ in the Besov space can be approximated using a superposition of tensor product B-splines as
\begin{align*}
    f_N(x) =&~ \sum_{(k,j)} \alpha_{k,j}  M_{k,j}^d(x),
\end{align*}
where the summation is taken over appropriate index sets $(k,j)$, and the coefficients $\alpha_{k,j}$ are real numbers that determine the contribution of each basis function.
\end{definition}

\subsection{Class of Neural Networks}
\begin{definition}[Neural Network Class in \cite{fsi+24}]\label{def:sparse_nn_class}
\label{def:nn_class}
Let $L \in \mathbb{N}$ denote the depth (number of layers), $W = (W_1, W_2, \dots, W_{L+1}) \in \mathbb{N}^{L+1}$ the width configuration of the network, $S \in \mathbb{N}$ a sparsity constraint, and $B > 0$ a norm bound. The class of neural networks ${\cal M}(L,W,S,B)$ is defined as
\begin{align*}
    {\cal M}(L,W,S,B) := 
     \{ &~
    \psi_{A^{(L)},b^{(L)}} 
    \circ \cdots \circ 
    \psi_{A^{(2)},b^{(2)}} (A^{(1)}x + b^{(1)} ) 
    m|  
    A^{(i)} \in \R^{W_{i+1} \times W_i}, 
    b^{(i)} \in \R^{W_{i+1}}, \\ 
    &~
    \sum_{i=1}^{L}  (\|A^{(i)}\|_0 + \|b^{(i)}\|_0 ) \leq S, 
    \quad 
    \max_{1 \leq i \leq L}  \{\|A^{(i)}\|_\infty \vee \|b^{(i)}\|_\infty \} \leq B
     \}.
\end{align*}
Here, the function $\psi_{A,b}: \R^{W_i} \to \R^{W_{i+1}}$ represents the affine transformation with ReLU activation, given by
\begin{align*}
    \psi_{A,b}(z) = A \cdot \mathsf{ReLU}(z) + b, \quad \mathrm{where} \quad \mathsf{ReLU}(z) = \max\{0, z\}.
\end{align*}
The sparsity constraint ensures that the total number of nonzero entries in all weight matrices and bias vectors does not exceed $S$, while the norm constraint limits their maximum absolute values to $B$.
\end{definition}

\subsection{Assumptions}

\begin{remark}\label{rmk:setting_on_assumption}
We introduce a small positive constant $\delta>0$ and denote by $N$ the number of basis functions in the B-spline used to approximate $p_t(x)$. The value of $N$ is determined by the sample size $n$, specifically following the relation $N = n^{\frac{d}{2s+d}}$, which balances the approximation error and the complexity of both the B-spline and the neural network. \end{remark}

\begin{definition}[Stopping Time]\label{def:stop_time}
  As we introduce in Remark~\ref{rmk:setting_on_assumption}, we define the stopping time as $T_0 = N^{-R_0}$, where $R_0$ is a parameter to be specified later, and consider solving the ODE backward in time from $t=1$ down to $t=T_0$.   
\end{definition}

\begin{definition}[Reduced Cube]
Let $I^d = [-1,1]^d$ denote the $d$-dimensional cube. To mitigate boundary effects when $N$ is large, we define the reduced cube as
\begin{align*}
I^d_N := [-1 + N^{-(1-\kappa\delta)}, 1 - N^{-(1-\kappa\delta)}]^d,  
\end{align*}
where the parameter $\kappa > 0$ will be specified later in Assumption~\ref{ass:A3}.
\end{definition}

\begin{assumption}[Smoothness and support of $p_0$]\label{ass:A1}
The target probability $P_0$ has support contained in $I^d$, and its probability density function $p_0$ satisfies
\begin{align*}
    p_0 \in B^s_{p',q'}(I^d)
  \quad\mathrm{and}\quad
  p_0 \in B^{\wt{s}}_{p',q'}(I^d \setminus I^d_N)
  \quad\mathrm{with}\quad
  \wt{s} \geq \max\{6s-1, 1\}.
\end{align*}
\end{assumption}


\begin{assumption}[Boundedness away from $0$ and above]\label{ass:A2}
There exists a constant $C_0>0$ such that
\begin{align*}
    C_0^{-1} \leq  p_0(x) \leq  C_0 \quad\mathrm{for all}\quad x \in I^d.  
\end{align*}
\end{assumption}

\begin{assumption}[Form of $(\alpha_t,\beta_t)$ and their bounds]\label{ass:A3}
There are constants $\kappa \geq \frac{1}{2}$, $b_0>0$, $\wt{\kappa}>0$, and $\wt{b}_0>0$ such that, for sufficiently small $t \geq T_0$,
\begin{align*}
  \alpha_t  =  b_0, t^{\kappa},
  \quad\mathrm{and}\quad
  1 - \beta_t  =  \wt{b}_0, t^{\wt{\kappa}}.
\end{align*}
Moreover, there exist $D_0 > 0$ and $K_0 > 0$ such that $\forall t \in [T_0,1]$, we have
\begin{align*}
  D_0^{-1}  \leq  \alpha_t^2 + \beta_t^2   \leq   D_0,
  \quad
   |\dot{\alpha}_t | +  |\dot{\beta}_t|  \leq   N^{K_0}.
\end{align*}
\end{assumption}

\begin{assumption}[Additional bound in the critical case $\kappa = \frac{1}{2}$]\label{ass:A4}
If $\kappa = \frac{1}{2}$, then there exist $b_1>0$ and $D_1>0$ such that, for all $0 \leq \gamma < R_0$,
\begin{align*}
\int_{T_0}^{N^{-\gamma}} \{ (\dot{\alpha}_t )^2 +  (\dot{\beta}_t )^2 \} \d t \leq  
D_1  (\log N )^{b_1}.
\end{align*}
\end{assumption}

\begin{assumption}[Lipschitz bound on the first moment]\label{ass:A5}
There is a constant $C_L > 0$ such that, for all $t \in [T_0, 1]$,
\begin{align*}
\| \frac{\partial}{\partial x} \int y p_t(y | x) \d y \|_{\mathrm{op}} \leq  C_L.
\end{align*}
\end{assumption}

\subsection{Approximation error for small \texorpdfstring{$t$}{}}
\begin{lemma}[Theorem 7 in \cite{fsi+24}]\label{lem:error_approx_small_t}
    Under Assumptions~\ref{ass:A1}~\ref{ass:A2}~\ref{ass:A3}~\ref{ass:A4} and \ref{ass:A5}, and if the following holds 
    \begin{itemize}
        \item $L = O(\log^4 N )$.
        \item $\|W\|_{\infty} = O(N \log^{6} N)$
        \item $S = O(N \log^{8} N)$
        \item $B = \exp(O (\log N \log \log N ) ).$
    \end{itemize}
    Then there exists a neural network $\phi  \in  {\cal M}(L,W,S,B)$ such that, for sufficiently large $N$, we have
    \begin{align*}
        \int  \|\phi(x,t) - \dot{x}_t^\mathrm{true} \|^{2}_2 p_{t}(x) \d x \lesssim  (\dot{\alpha}_t^{2} \log N  +  \dot{\beta}_t^{2} ) N^{-\frac{2s}{d}},
    \end{align*}
    holds for any $t \in [T_{0}, 3T_{*}]$.
    In addition, $\phi$ can be taken so we have
    \begin{align*}
         \|\phi(\cdot,t) \|_\infty = O(  |\dot{\alpha}_t | \sqrt{\log n} +  |\dot{\beta}_t |) .
    \end{align*}
\end{lemma}

\subsection{Approximation error for large \texorpdfstring{$t$}{}}

\begin{lemma}[Theorem 7 in \cite{fsi+24}]\label{lem:error_approx_large_t}
    Fix $t_{*} \in [T_{*},1]$ and let $\eta>0$ be arbitrary, under Assumptions~\ref{ass:A1}~\ref{ass:A2}~\ref{ass:A3}~\ref{ass:A4} and \ref{ass:A5}, and if the following holds 
    \begin{itemize}
        \item $L = O(\log^4 N )$.
        \item $\|W\|_{\infty} = O(N)$
        \item $S = O(t_{*}^{- d\kappa} N^{\delta\kappa})$
        \item $B = \exp(O (\log N \log \log N ) ).$
    \end{itemize}
  Then there exist a neural network $\phi  \in  {\cal M}(L,W,S,B)$ such that
\begin{align*}
  \int  \| \phi(x,t) - \dot{x}^\mathrm{true}_t \|^2 p_{t} (x) \d x \lesssim (\dot{\alpha}_t^{2} \log N  +   \dot{\beta}_t^{2} ) N^{-\eta}.
 \end{align*}
 holds for any $t \in [2t_{*}, 1]$. In addition, $\phi$ can be taken so we have
 \begin{align*}
       \|\phi(\cdot,t) \|_{\infty} =  O( |\dot{\alpha}_t | \log N +  |\dot{\beta}_t | ).
 \end{align*}
\end{lemma}

\section{Theory of Higher Order Flow Matching}\label{sec:app:higher_order_flow_matching}

We use $\frac{\d^k}{\d t^k} x_t^\mathrm{true}$ to denote the $k$-th order derivative of $x_t^\mathrm{true}$ with respect to $t$. Note that $\dot{x}^\mathrm{true}_t := \frac{\d}{\d t} x^\mathrm{true}_t$, and $\ddot{x}^\mathrm{true}_t := \frac{\d^2}{\d t^2} x^\mathrm{true}_t$.

\subsection{Approximation Error of Second Order Flow Matching for Small \texorpdfstring{$t$}{}}
\begin{theorem}[Approximation error of second order flow matching for small $t$, formal version of Theorem~\ref{thm:secon_order_small_t:informal}]\label{thm:secon_order_small_t:formal}
    Under Assumptions~\ref{ass:A1}~\ref{ass:A2}~\ref{ass:A3}~\ref{ass:A4} and \ref{ass:A5}, and if the following holds 
    \begin{itemize}
        \item $L = O(\log^4 N )$.
        \item $\|W\|_{\infty} = O(N \log^{6} N)$
        \item $S = O(N \log^{8} N)$
        \item $B = \exp(O (\log N \log \log N ) ).$
    \end{itemize}
    Then there exists neural networks $\phi_{1},\phi_2  \in  {\cal M}(L,W,S,B)$ such that, for sufficiently large $N$, we have
\begin{align*}
    &~ \int (\|\phi_1(x, t) - \dot{x}_t^\mathrm{true}\|_2^2 + \|\phi_2(x, t) - \ddot{x}_t^\mathrm{true}\|_2^2) p_t(x) \d x \\ \lesssim &~ (\dot{\alpha}_t^2 \log N + \dot{\beta}_t^2 ) N^{- \frac{2s}{d}} +
    \E_{x \sim P_t}[\|\dot{x}^\mathrm{true}_t - \ddot{x}^\mathrm{true}_t\|_2^2]
\end{align*}
    holds for any $t \in [T_{0}, 3T_{*}]$. In addition, $\phi_1, \phi_2$ can be taken so we have
    \begin{align*}
         \|\phi_1(\cdot,t) \|_\infty = O(  |\dot{\alpha}_t | \sqrt{\log n} +  |\dot{\beta}_t |) \mathrm{~~~and~~~} \|\phi_2(\cdot,t) \|_\infty = O(  |\dot{\alpha}_t | \sqrt{\log n} +  |\dot{\beta}_t |).
    \end{align*}
\end{theorem}
\begin{proof}
    Suppose that $t \in [T_0, 3T_*]$.
    By Lemma~\ref{lem:error_approx_small_t}, there is $\phi_1 \in {\cal M}(L,W,S,B)$ such that
    \begin{align}
        \label{eq:tmp_1}
        \int (\|\phi_1(x, t) - \dot{x}_t^\mathrm{true}\|_2^2 p_t(x) \lesssim  (\dot{\alpha}_t^2 \log N + \dot{\beta}_t^2 ) N^{- \frac{2s}{d}}.
    \end{align}
    
    Next, we can show that there exists some $\phi_2 \in {\cal M}(L,W,S,B)$ such that
    \begin{align}
         \int \|\phi_2(x, t) - \ddot{x}_t^\mathrm{true}\|_2^2 p_t(x) \d x 
         = &~\int \|\phi_2(x, t) - \dot{x}_t^\mathrm{true} +\dot{x}_t^\mathrm{true} - \ddot{x}_t^\mathrm{true}\|_2^2 p_t(x) \d x \notag \\
         \leq &~ \int (\|\phi_2(x, t) - \dot{x}_t^\mathrm{true}\|_2 + \| \dot{x}_t^\mathrm{true}-\ddot{x}_t^\mathrm{true}\|_2)^2 p_t(x) \d x \notag \\
         \leq &~ \int 2(\|\phi_2(x, t) - \dot{x}_t^\mathrm{true}\|_2^2 + \| \dot{x}_t^\mathrm{true}-\ddot{x}_t^\mathrm{true}\|_2^2) p_t(x) \d x \notag \\
         = &~ 2\int \|\phi_2(x, t) - \dot{x}_t^\mathrm{true}\|_2^2 p_t(x) \d x + 2\int \| \dot{x}_t^\mathrm{true}-\ddot{x}_t^\mathrm{true}\|_2^2 p_t(x) \d x \notag \\
         = &~ 2\int \|\phi_2(x, t) - \dot{x}_t^\mathrm{true}\|_2^2p_t(x)\d x + 2\E_{x \sim P_t}[\|\dot{x}^\mathrm{true}_t - \ddot{x}^\mathrm{true}_t\|_2^2 \notag \\
         \lesssim &~ (\dot{\alpha}_t^2 \log N + \dot{\beta}_t^2 ) N^{- \frac{2s}{d}} + \E_{x \sim P_t}[\|\dot{x}^\mathrm{true}_t - \ddot{x}^\mathrm{true}_t\|_2^2] \label{eq:tmp_2}
    \end{align}
    where the first step follows from the basic algebra, the second step follows from the triangle inequality, the third step follows from $(a+b)^2 \leq 2a^2 + 2b^2$, the fourth step follows from basic algebra, the fifth step follows from the definition of expectation, and the last step follows from Lemma~\ref{lem:error_approx_small_t}.

    Finally, by Eq.~\eqref{eq:tmp_1} and Eq.~\eqref{eq:tmp_2}, for any $t \in [T_0, 3T_*]$, we have
    \begin{align*}
    &~ \int (\|\phi_1(x, t) - \dot{x}_t^\mathrm{true}\|_2^2 + \|\phi_2(x, t) - \ddot{x}_t^\mathrm{true}\|_2^2) p_t(x) \d x \\ \lesssim &~ (\dot{\alpha}_t^2 \log N + \dot{\beta}_t^2 ) N^{- \frac{2s}{d}} +
    \E_{x \sim P_t}[\|\dot{x}^\mathrm{true}_t - \ddot{x}^\mathrm{true}_t\|_2^2].
    \end{align*}

    Moreover, by Lemma~\ref{lem:error_approx_small_t}, $\phi_1, \phi_2$ can be taken so we have
    \begin{align*}
         \|\phi_1(\cdot,t) \|_\infty = O(  |\dot{\alpha}_t | \sqrt{\log n} +  |\dot{\beta}_t |) \mathrm{~~~and~~~} \|\phi_2(\cdot,t) \|_\infty = O(  |\dot{\alpha}_t | \sqrt{\log n} +  |\dot{\beta}_t |).
    \end{align*}
    Thus, the proof is complete.
\end{proof}

\subsection{Approximation Error of Higher Order Flow Matching for Small \texorpdfstring{$t$}{}}
\begin{theorem}[Approximation error of higher order flow matching for small $t$]\label{thm:higher_order_small_t:formal}
    Under Assumptions~\ref{ass:A1}~\ref{ass:A2}~\ref{ass:A3}~\ref{ass:A4} and \ref{ass:A5}, and if the following holds 
    \begin{itemize}
        \item $L = O(\log^4 N )$.
        \item $\|W\|_{\infty} = O(N \log^{6} N)$
        \item $S = O(N \log^{8} N)$
        \item $B = \exp(O (\log N \log \log N ) )$
        \item $K = O(1)$
    \end{itemize}
    Then there exists neural networks $\phi_{1},\phi_2, \ldots, \phi_K \in {\cal M}(L,W,S,B)$ such that, for sufficiently large $N$, we have
\begin{align*}
    &~ \int (\sum_{k=1}^K\|\phi_k(x, t) - \frac{\d^k}{\d t^k}x_t^\mathrm{true}\|_2^2) p_t(x) \d x \\ \lesssim &~ (\dot{\alpha}_t^2 \log N + \dot{\beta}_t^2 ) N^{- \frac{2s}{d}} +
    \sum_{k=1}^{K-1}\E_{x \sim P_t}[\|\frac{\d^k}{\d t^k}x_t^\mathrm{true} - \frac{\d^{k+1}}{\d t^{k+1}}x_t^\mathrm{true}\|_2^2] 
\end{align*}
    holds for any $t \in [T_{0}, 3T_{*}]$. In addition, for any $k \in [K]$, $\phi_k$ can be taken so we have
    \begin{align*}
         \|\phi_k(\cdot,t) \|_\infty = O(  |\dot{\alpha}_t | \sqrt{\log n} +  |\dot{\beta}_t |).
    \end{align*}
\end{theorem}
\begin{proof}

    We first show that for any $k \geq 2$, for any $t \in [T_0, 3T_*]$, there exists $\phi \in {\cal M}(L,W,S,B)$ such that
    \begin{align}
    \label{eq:tmp_3}
    &~ \int \|\phi(x, t) - \frac{\d^k}{\d t^k}{x}_t^\mathrm{true}\|^2_2 p_t(x) \d x \notag \\
    \lesssim &~ 
    (\dot{\alpha}_t^2 \log N + \dot{\beta}_t^2 ) N^{- \frac{2s}{d}} +
    \sum_{j=1}^{k}\E_{x \sim P_t}[\|\frac{\d^{j}}{\d t^{j}}x_t^\mathrm{true} - \frac{\d^{j+1}}{\d t^{j+1}}x_t^\mathrm{true}\|_2^2].
    \end{align}

    We prove this by mathematical induction.

    \textbf{Base case.} The statements hold when $k = 2$ because of Lemma~\ref{thm:secon_order_small_t:formal}.

    \textbf{Induction step.} We assume that the statement hold for $k \geq 2$. We would like to show that it holds for $k+1$. We can show that, for any $t \in [T_0, 3T_*]$, there exists $\phi \in {\cal M}(L,S,W, B)$ such that
    \begin{align}
    &~ \int \|\phi(x, t) - \frac{\d^{k+1}}{\d t^{k+1}}{x}_t^\mathrm{true}\|^2_2 p_t(x) \d x \notag
    \\ = &~
    \int \|\phi(x, t) - \frac{\d^{k}}{\d t^{k}}{x}_t^\mathrm{true} + \frac{\d^{k}}{\d t^{k}}{x}_t^\mathrm{true} - \frac{\d^{k+1}}{\d t^{k+1}}{x}_t^\mathrm{true}\|^2_2 p_t(x) \d x \notag \\
    \leq &~ \int (\|\phi(x, t) - \frac{\d^{k}}{\d t^{k}}{x}_t^\mathrm{true}\|_2 + \| \frac{\d^{k}}{\d t^{k}}{x}_t^\mathrm{true} - \frac{\d^{k+1}}{\d t^{k+1}}{x}_t^\mathrm{true}\|_2 )^2 p_t(x) \d x \notag \\
    \leq &~ \int 2(\|\phi(x, t) - \frac{\d^{k}}{\d t^{k}}{x}_t^\mathrm{true}\|_2^2 + \| \frac{\d^{k}}{\d t^{k}}{x}_t^\mathrm{true} - \frac{\d^{k+1}}{\d t^{k+1}}{x}_t^\mathrm{true}\|_2^2) p_t(x) \d x \notag \\
    = &~ 2\int \|\phi(x, t) - \frac{\d^{k}}{\d t^{k}}{x}_t^\mathrm{true}\|_2^2p_t(x) \d x + 2 \int \| \frac{\d^{k}}{\d t^{k}}{x}_t^\mathrm{true} - \frac{\d^{k+1}}{\d t^{k+1}}{x}_t^\mathrm{true}\|_2^2 p_t(x) \d x \notag \\
    = &~ 2\int \|\phi(x, t) - \frac{\d^{k}}{\d t^{k}}{x}_t^\mathrm{true}\|_2^2p_t(x) \d x + 2 \E_{x \sim P_t} [ \| \frac{\d^{k}}{\d t^{k}}{x}_t^\mathrm{true} - \frac{\d^{k+1}}{\d t^{k+1}}{x}_t^\mathrm{true}\|_2^2]\notag \\
    \lesssim &~ (\dot{\alpha}_t^2 \log N + \dot{\beta}_t^2 ) N^{- \frac{2s}{d}} +
    \sum_{j=1}^{k}\E_{x \sim P_t}[\|\frac{\d^{j}}{\d t^{j}}x_t^\mathrm{true} - \frac{\d^{j+1}}{\d t^{j+1}}x_t^\mathrm{true}\|_2^2] + \E_{x \sim P_t} [ \| \frac{\d^{k}}{\d t^{k}}{x}_t^\mathrm{true} - \frac{\d^{k+1}}{\d t^{k+1}}{x}_t^\mathrm{true}\|_2^2]\notag \\
    = &~ (\dot{\alpha}_t^2 \log N + \dot{\beta}_t^2 ) N^{- \frac{2s}{d}} +
    \sum_{j=1}^{k+1}\E_{x \sim P_t}[\|\frac{\d^{j}}{\d t^{j}}x_t^\mathrm{true} - \frac{\d^{j+1}}{\d t^{j+1}}x_t^\mathrm{true}\|_2^2], \label{eq:tmp_4}
    \end{align}
    where the first step follows from basic algebra, the second step follows from triangle inequality, the third step follows from the Cauchy-Schwarz inequality, the fourth step follows from basic algebra, the fifth step follows from the definition of expectation, the six step follows from Eq.~\eqref{eq:tmp_3}.
    
     Hence, there exists $\phi_1, \phi_2, \ldots, \phi_K \in {\cal M}(L,W,S,B)$ such that for $k \in [K]$, for any $t \in [T_0, 3T_*]$, we have
    \begin{align}
    &~ \int \|\phi_k(x, t) - \frac{\d^k}{\d t^k}{x}_t^\mathrm{true}\|^2_2 p_t(x) \d x \notag \\
    \lesssim &~ 
    (\dot{\alpha}_t^2 \log N + \dot{\beta}_t^2 ) N^{- \frac{2s}{d}} +
    \sum_{j=1}^{k}\E_{x \sim P_t}[\|\frac{\d^{j}}{\d t^{j}}x_t^\mathrm{true} - \frac{\d^{j+1}}{\d t^{j+1}}x_t^\mathrm{true}\|_2^2]. \label{eq:tmp_5}
    \end{align}

    Taking the summation over $k \in [K]$, we have for any $t \in [T_0, 3T_*]$,
    \begin{align*}
        &~ \int \sum_{k=1}^K \|\phi_k(x, t) - \frac{\d^k}{\d t^k}{x}_t^\mathrm{true}\|^2_2 p_t(x) \d x \notag \\
    \lesssim &~ 
    K \cdot (\dot{\alpha}_t^2 \log N + \dot{\beta}_t^2 ) N^{- \frac{2s}{d}} +
    \sum_{k=1}^{K} (k \cdot \E_{x \sim P_t}[\|\frac{\d^{j}}{\d t^{j}}x_t^\mathrm{true} - \frac{\d^{j+1}}{\d t^{j+1}}x_t^\mathrm{true}\|_2^2]) \\
    \lesssim &~ ((\dot{\alpha}_t)^2 \log N + (\dot{\beta}_t)^2 ) N^{- \frac{2s}{d}} +
    \sum_{k=1}^{K} \E_{x \sim P_t}[\|\frac{\d^{j}}{\d t^{j}}x_t^\mathrm{true} - \frac{\d^{j+1}}{\d t^{j+1}}x_t^\mathrm{true}\|_2^2]
    \end{align*}
    where the first step follows from Eq.~\eqref{eq:tmp_5}, and the second step uses $K=O(1)$.
    
    Moreover, by Lemma~\ref{lem:error_approx_large_t}, $\phi_1, \phi_2, \ldots, \phi_K$ can be taken so we have for $k \in [K]$,
    \begin{align*}
         \|\phi_k(\cdot,t) \|_\infty = O(  |\dot{\alpha}_t | \log \sqrt{n} +  |\dot{\beta}_t |).
    \end{align*}
    Thus, the proof is complete.
\end{proof}

\subsection{Approximation Error of Second Order Flow Matching for Large \texorpdfstring{$t$}{}}
\begin{theorem}[Approximation error of second order flow matching for large $t$, formal version of Theorem~\ref{thm:secon_order_large_t:informal}]\label{thm:secon_order_large_t:formal}
    Fix $t_{*} \in [T_{*},1]$ and let $\eta>0$ be arbitrary, under Assumptions~\ref{ass:A1}~\ref{ass:A2}~\ref{ass:A3}~\ref{ass:A4} and \ref{ass:A5}, and if the following holds 
    \begin{itemize}
        \item $L = O(\log^4 N )$.
        \item $\|W\|_{\infty} = O(N)$
        \item $S = O(t_{*}^{- d\kappa} N^{\delta\kappa})$
        \item $B = \exp(O (\log N \log \log N ) ).$
    \end{itemize}
  Then there exist neural networks $\phi_{1},\phi_2  \in  {\cal M}(L,W,S,B)$ such that
\begin{align*}
    &~ \int (\|\phi_1(x, t) - \dot{x}_t^\mathrm{true}\|_2^2 + \|\phi_2(x, t) - \ddot{x}_t^\mathrm{true}\|_2^2) p_t(x) \d x \\ \lesssim &~ (\dot{\alpha}_t^{2} \log N  +   \dot{\beta}_t^{2} ) N^{-\eta} +
    \E_{x \sim P_t}[\|\dot{x}^\mathrm{true}_t - \ddot{x}^\mathrm{true}_t\|_2^2]
\end{align*}
    holds for any $t \in [2t_*, 1]$. In addition, $\phi_1, \phi_2$ can be taken so we have
    \begin{align*}
         \|\phi_1(\cdot,t) \|_\infty = O(  |\dot{\alpha}_t | \log N +  |\dot{\beta}_t |) \mathrm{~~~and~~~} \|\phi_2(\cdot,t) \|_\infty = O(  |\dot{\alpha}_t | \log N +  |\dot{\beta}_t |).
    \end{align*}
\end{theorem}
\begin{proof}
    Suppose that $t \in [2t_*, 1]$.
    By Lemma~\ref{lem:error_approx_large_t}, there is $\phi_1 \in {\cal M}(L,W,S,B)$ such that
    \begin{align}
        \label{eq:tmp_1_large}
        \int (\|\phi_1(x, t) - \dot{x}_t^\mathrm{true}\|_2^2p_t(x)\d x \lesssim  (\dot{\alpha}_t^{2} \log N  +   \dot{\beta}_t^{2} ) N^{-\eta}.
    \end{align}
    
    Next, we can show that there exists some $\phi_2 \in {\cal M}(L,W,S,B)$ such that
    \begin{align}
         \int \|\phi_2(x, t) - \ddot{x}_t^\mathrm{true}\|_2^2 p_t(x) \d x 
         = &~\int \|\phi_2(x, t) - \dot{x}_t^\mathrm{true} +\dot{x}_t^\mathrm{true} - \ddot{x}_t^\mathrm{true}\|_2^2 p_t(x) \d x \notag \\
         \leq &~ \int (\|\phi_2(x, t) - \dot{x}_t^\mathrm{true}\|_2 + \| \dot{x}_t^\mathrm{true}-\ddot{x}_t^\mathrm{true}\|_2)^2 p_t(x) \d x \notag \\
         \leq &~ \int 2(\|\phi_2(x, t) - \dot{x}_t^\mathrm{true}\|_2^2 + \| \dot{x}_t^\mathrm{true}-\ddot{x}_t^\mathrm{true}\|_2^2) p_t(x) \d x \notag \\
         = &~ 2\int \|\phi_2(x, t) - \dot{x}_t^\mathrm{true}\|_2^2 p_t(x) \d x + 2\int \|_2 \dot{x}_t^\mathrm{true}-\ddot{x}_t^\mathrm{true}\|_2^2 p_t(x) \d x \notag \\
         = &~ 2\int \|\phi_2(x, t) - \dot{x}_t^\mathrm{true}\|_2^2p_t(x)\d x + 2\E_{x \sim P_t}[\|\dot{x}^\mathrm{true}_t - \ddot{x}^\mathrm{true}_t\|_2^2 \notag \\
         \lesssim &~ (\dot{\alpha}_t^{2} \log N  +   \dot{\beta}_t^{2} ) N^{-\eta} + \E_{x \sim P_t}[\|\dot{x}^\mathrm{true}_t - \ddot{x}^\mathrm{true}_t\|_2^2] \label{eq:tmp_2_large}
    \end{align}
    where the first step follows from the basic algebra, the second step follows from the triangle inequality, the third step follows from $(a+b)^2 \leq 2a^2 + 2b^2$, the fourth step follows from basic algebra, the fifth step follows from the definition of expectation, and the last step follows from Lemma~\ref{lem:error_approx_large_t}.

    Finally, by Eq.~\eqref{eq:tmp_1_large} and Eq.~\eqref{eq:tmp_2_large}, we have
    \begin{align*}
    &~ \int (\|\phi_1(x, t) - \dot{x}_t^\mathrm{true}\|^2 + \|\phi_2(x, t) - \ddot{x}_t^\mathrm{true}\|^2) p_t(x) \d x \\ \lesssim &~ (\dot{\alpha}_t^{2} \log N  +   \dot{\beta}_t^{2} ) N^{-\eta} +
    \E_{x \sim P_t}[\|\dot{x}^\mathrm{true}_t - \ddot{x}^\mathrm{true}_t\|^2].
    \end{align*}

    Moreover, by Lemma~\ref{lem:error_approx_large_t}, $\phi_1, \phi_2$ can be taken so we have
    \begin{align*}
         \|\phi_1(\cdot,t) \|_\infty = O(  |\dot{\alpha}_t | \log N +  |\dot{\beta}_t |) \mathrm{~~~and~~~} \|\phi_2(\cdot,t) \|_\infty = O(  |\dot{\alpha}_t |\log N +  |\dot{\beta}_t |).
    \end{align*}
    Thus, the proof is complete.
\end{proof}

\subsection{Approximation Error of Higher Order Flow Matching for Large \texorpdfstring{$t$}{}}
\begin{theorem}[Approximation error of higher order flow matching for large $t$]\label{thm:higher_order_large_t:formal}
    Fix $t_{*} \in [T_{*},1]$ and let $\eta>0$ be arbitrary, under Assumptions~\ref{ass:A1}~\ref{ass:A2}~\ref{ass:A3}~\ref{ass:A4} and \ref{ass:A5}, and if the following holds 
    \begin{itemize}
        \item $L = O(\log^4 N )$.
        \item $\|W\|_{\infty} = O(N)$
        \item $S = O(t_{*}^{- d\kappa} N^{\delta\kappa})$
        \item $B = \exp(O (\log N \log \log N ) )$ 
        \item $K = O(1)$
    \end{itemize}
  Then there exist neural networks $\phi_{1},\phi_2, \ldots, \phi_K \in {\cal M}(L,W,S,B)$ such that,
\begin{align*}
    &~ \int (\sum_{k=1}^K\|\phi_k(x, t) - \frac{\d^k}{\d t^k}x_t^\mathrm{true}\|^2) p_t(x) \d x \\ \lesssim &~ (\dot{\alpha}_t^{2} \log N  +   \dot{\beta}_t^{2} ) N^{-\eta} +
    \sum_{k=1}^{K-1}\E_{x \sim P_t}[\|\frac{\d^k}{\d t^k}x_t^\mathrm{true} - \frac{\d^{k+1}}{\d t^{k+1}}x_t^\mathrm{true}\|^2] 
\end{align*}
    holds for any $t \in [2t_*, 1]$. In addition, for any $k \in [K]$, $\phi_k$ can be taken so we have
    \begin{align*}
         \|\phi_k(\cdot,t) \|_\infty = O(  |\dot{\alpha}_t | \log N +  |\dot{\beta}_t |).
    \end{align*}
\end{theorem}
\begin{proof}
    We first show that for any $k \geq 2$, for any $t \in [2t_*,1]$, there exists $\phi \in {\cal M}(L,W,S,B)$ such that
    \begin{align}
    \label{eq:tmp_3_large}
    &~ \int \|\phi(x, t) - \frac{\d^k}{\d t^k}{x}_t^\mathrm{true}\|^2_2 p_t(x) \d x \notag \\
    \lesssim &~ 
    (\dot{\alpha}_t^{2} \log N  +   \dot{\beta}_t^{2} ) N^{-\eta} +
    \sum_{j=1}^{k}\E_{x \sim P_t}[\|\frac{\d^{j}}{\d t^{j}}x_t^\mathrm{true} - \frac{\d^{j+1}}{\d t^{j+1}}x_t^\mathrm{true}\|^2].
    \end{align}

    We prove this by mathematical induction.

    \textbf{Base case.} The statements hold when $k = 2$ because of Lemma~\ref{thm:secon_order_large_t:formal}.

    \textbf{Induction step.} We assume that the statement hold for $k \geq 2$. We would like to show that it holds for $k+1$. We can show that, for any $t \in [2t_*, 1]$, there exists $\phi \in {\cal M}(L,S,W, B)$ such that
    \begin{align}
    &~ \int \|\phi(x, t) - \frac{\d^{k+1}}{\d t^{k+1}}{x}_t^\mathrm{true}\|^2_2 p_t(x) \d x \notag
    \\ = &~
    \int \|\phi(x, t) - \frac{\d^{k}}{\d t^{k}}{x}_t^\mathrm{true} + \frac{\d^{k}}{\d t^{k}}{x}_t^\mathrm{true} - \frac{\d^{k+1}}{\d t^{k+1}}{x}_t^\mathrm{true}\|^2_2 p_t(x) \d x \notag \\
    \leq &~ \int (\|\phi(x, t) - \frac{\d^{k}}{\d t^{k}}{x}_t^\mathrm{true}\|_2 + \| \frac{\d^{k}}{\d t^{k}}{x}_t^\mathrm{true} - \frac{\d^{k+1}}{\d t^{k+1}}{x}_t^\mathrm{true}\|_2 )^2 p_t(x) \d x \notag \\
    \leq &~ \int 2(\|\phi(x, t) - \frac{\d^{k}}{\d t^{k}}{x}_t^\mathrm{true}\|_2^2 + \| \frac{\d^{k}}{\d t^{k}}{x}_t^\mathrm{true} - \frac{\d^{k+1}}{\d t^{k+1}}{x}_t^\mathrm{true}\|_2^2) p_t(x) \d x \notag \\
    = &~ 2\int \|\phi(x, t) - \frac{\d^{k}}{\d t^{k}}{x}_t^\mathrm{true}\|_2^2p_t(x) \d x + 2 \int \| \frac{\d^{k}}{\d t^{k}}{x}_t^\mathrm{true} - \frac{\d^{k+1}}{\d t^{k+1}}{x}_t^\mathrm{true}\|_2^2 p_t(x) \d x \notag \\
    = &~ 2\int \|\phi(x, t) - \frac{\d^{k}}{\d t^{k}}{x}_t^\mathrm{true}\|_2^2p_t(x) \d x + 2 \E_{x \sim P_t} [ \| \frac{\d^{k}}{\d t^{k}}{x}_t^\mathrm{true} - \frac{\d^{k+1}}{\d t^{k+1}}{x}_t^\mathrm{true}\|_2^2]\notag \\
    \lesssim &~(\dot{\alpha}_t^{2} \log N  +   \dot{\beta}_t^{2} ) N^{-\eta} +
    \sum_{j=1}^{k}\E_{x \sim P_t}[\|\frac{\d^{j}}{\d t^{j}}x_t^\mathrm{true} - \frac{\d^{j+1}}{\d t^{j+1}}x_t^\mathrm{true}\|_2^2] + \E_{x \sim P_t} [ \| \frac{\d^{k}}{\d t^{k}}{x}_t^\mathrm{true} - \frac{\d^{k+1}}{\d t^{k+1}}{x}_t^\mathrm{true}\|_2^2]\notag \\
    = &~ (\dot{\alpha}_t^{2} \log N  +   \dot{\beta}_t^{2} ) N^{-\eta}+
    \sum_{j=1}^{k+1}\E_{x \sim P_t}[\|\frac{\d^{j}}{\d t^{j}}x_t^\mathrm{true} - \frac{\d^{j+1}}{\d t^{j+1}}x_t^\mathrm{true}\|_2^2], \label{eq:tmp_4_large}
    \end{align}
    where the first step follows from basic algebra, the second step follows from triangle inequality, the third step follows from the Cauchy-Schwarz inequality, the fourth step follows from basic algebra, the fifth step follows from the definition of expectation, the six step follows from Eq.~\eqref{eq:tmp_3_large}.
    
     Hence, there exists $\phi_1, \phi_2, \ldots, \phi_K \in {\cal M}(L,W,S,B)$ such that for $k \in [K]$, for any $t \in [2t_*, 1]$, we have
    \begin{align}
    &~ \int \|\phi_k(x, t) - \frac{\d^k}{\d t^k}{x}_t^\mathrm{true}\|^2_2 p_t(x) \d x \notag \\
    \lesssim &~ 
    (\dot{\alpha}_t^{2} \log N  +   \dot{\beta}_t^{2} ) N^{-\eta} +
    \sum_{j=1}^{k}\E_{x \sim P_t}[\|\frac{\d^{j}}{\d t^{j}}x_t^\mathrm{true} - \frac{\d^{j+1}}{\d t^{j+1}}x_t^\mathrm{true}\|_2^2]. \label{eq:tmp_5_large}
    \end{align}

    Taking the summation over $k \in [K]$, we have for any $t \in [2 t_*, 1]$,
    \begin{align*}
        &~ \int \sum_{k=1}^K \|\phi_k(x, t) - \frac{\d^k}{\d t^k}{x}_t^\mathrm{true}\|^2_2 p_t(x) \d x \notag \\
    \lesssim &~ 
    ((\dot{\alpha}_t )^{2} \log N  +   (\dot{\beta}_t )^{2} ) N^{-\eta} +
    \sum_{k=1}^{K} (k \cdot \E_{x \sim P_t}[\|\frac{\d^{j}}{\d t^{j}}x_t^\mathrm{true} - \frac{\d^{j+1}}{\d t^{j+1}}x_t^\mathrm{true}\|_2^2]) \\
    \lesssim &~ (\dot{\alpha}_t^{2} \log N  +   \dot{\beta}_t^{2} ) N^{-\eta} +
    \sum_{k=1}^{K} \E_{x \sim P_t}[\|\frac{\d^{j}}{\d t^{j}}x_t^\mathrm{true} - \frac{\d^{j+1}}{\d t^{j+1}}x_t^\mathrm{true}\|_2^2]
    \end{align*}
    where the first step follows from Eq.~\eqref{eq:tmp_5_large}, and the second step uses $K=O(1)$.
   Moreover, by Lemma~\ref{lem:error_approx_large_t}, $\phi_1, \phi_2, \ldots, \phi_K$ can be taken so we have for $k \in [K]$,
    \begin{align*}
         \|\phi_k(\cdot,t) \|_\infty = O(  |\dot{\alpha}_t | \log N +  |\dot{\beta}_t |).
    \end{align*}
    Thus, the proof is complete.
\end{proof}