\section{Related Works}
\paragraph{Diffusion Models.} Diffusion models have garnered significant attention for their capability to generate high-fidelity images by incrementally refining noisy samples, as exemplified by DiT~\cite{px23} and U-ViT~\cite{bnx+23}. These approaches typically involve a forward process that systematically adds noise to an initial clean image and a corresponding reverse process that learns to remove noise step by step, thereby recovering the underlying data distribution in a probabilistic manner. Early works~\cite{se19,sme20} established the theoretical foundations of this denoising strategy, introducing score-matching and continuous-time diffusion frameworks that significantly improved sample quality and diversity. Subsequent research has focused on more efficient training and sampling procedures~\cite{lzb+22,ssz+24_dit,ssz+24_pruning}, aiming to reduce computational overhead and converge faster without sacrificing image fidelity. Other lines of work leverage latent spaces to learn compressed representations, thereby streamlining both training and inference~\cite{rbl+22,hwsl24}. This latent learning approach integrates naturally with modern neural architectures and can be extended to various modalities beyond images, showcasing the versatility of diffusion processes in modeling complex data distributions. In parallel, recent researchers have also explored multi-scale noise scheduling and adaptive step-size strategies to enhance convergence stability and maintain high-resolution detail in generated content in \cite{lkw+24,fmzz24,rckc24,jzx+25,lyhz24}. There are more other works also inspire our work~\cite{xzc+22,dwb+23,pbd+23,wsd+23,wcz+23,ssz+24,ssz+24_prun,wxz+24,cl24,kkn24,cll+25,cll+25_deskreject,cxj24,wcy+23,fjl+24,lzw+24,hwl+24}.

\paragraph{Flow Matching. }
Generative models like diffusion \citep{swmg15, hja20, sme20} and flow-matching \citep{lcb+22, lgl22} operate by learning ordinary differential equations (ODEs) that map noise to data. To simplify, this study leverages the optimal transport flow-matching formulation \citep{lgl22}. A linear combination of a noise sample $x_0 \sim \mathcal{N}(0, \mathbb{I})$ and a data point $x_1 \sim \mathcal{D}$ defines $x_t$:
\begin{align*}
x_t = (1-t)x_0 + tx_1,
\qquad v_t = x_1 - x_0,
\end{align*}
with $v_t$ representing the velocity vector directed from $x_0$ to $x_1$. While $v_t$ is uniquely derived from $(x_0, x_1)$, knowledge of only $x_t$ renders it a random variable due to the ambiguity in selecting $(x_0, x_1)$. Neural networks in flow models approximate the expected velocity $\bar{v}_t = \mathbb{E}[v_t \mid x_t]$, calculated as an average over all valid pairings. Training involves minimizing the deviation between predicted and empirical velocities:
\begin{align}\label{eq:flow-matching}
& ~ \notag \bar{v}_\theta(x_t, t) \sim \mathbb{E}_{x_0,x_1 \sim \D} \left[ v_t \mid x_t \right] \\ 
& ~ \mathcal{L}^{\mathrm{F}}(\theta) = \mathbb{E}_{x_0,x_1 \sim \D} \left[ \| \bar{v}_\theta(x_t, t) - (x_1-x_0) \|^2 \right].
\end{align}
Sampling involves first drawing a noise point $x_0 \sim \mathcal{N}(0, I)$ and iteratively transforming it into a data point $x_1$. The denoising ODEs, parameterized by $\bar{v}_\theta(x_t, t)$, governs this transformation, and Eulerâ€™s method approximates it over small, discrete time steps.

\paragraph{High-order ODE Gradient in Diffusion Models. }

Higher-order gradient-based methods like TTMs~\cite{kp92} have applications far exceeding DDMs. For instance, solvers~\cite{dng+22} and regularization frameworks~\cite{kbjd20,fjno20} for neural ODEs~\cite{crbd18,gcb+18} frequently utilize higher-order derivatives. Beyond machine learning contexts, the study of higher-order TTMs has been extensively directed toward solving stiff~\cite{cc94} and non-stiff~\cite{cc94,cc82} systems.