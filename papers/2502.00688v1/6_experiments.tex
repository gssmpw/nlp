\section{Experiments} \label{sec:experiments}

This section presents a series of experiments to evaluate the effectiveness of our HOMO method and assess the impact of each loss component. Our results demonstrate that HOMO significantly improves distribution generation, with the higher-order loss playing a key role in enhancing model performance.

\begin{figure*}[!ht]
\centering
\subfloat[Eight-mode Dataset]{\includegraphics[width=0.24\textwidth]{8_dataset.pdf}}
\subfloat[M1]{\includegraphics[width=0.24\textwidth]{8_1_output.pdf}}
\subfloat[M2]{\includegraphics[width=0.24\textwidth]{8_2_output.pdf}}
\subfloat[SC]{\includegraphics[width=0.24\textwidth]{8_3_output.pdf}}\\
\subfloat[(M1 + M2)]{\includegraphics[width=0.24\textwidth]{8_12_output.pdf}}
\subfloat[(M1 + SC) \cite{fhla24}]{\includegraphics[width=0.24\textwidth]{8_13_output.pdf}}
\subfloat[(M2 + SC)]{\includegraphics[width=0.24\textwidth]{8_23_output.pdf}}
\subfloat[(M1 + M2 + SC) (Ours)]{\includegraphics[width=0.24\textwidth]{8_123_output.pdf}}
\caption{ 
\textbf{HOMO on a mixture of Gaussian datasets.} The first row shows results for the initial eight-mode dataset (a) and HOMO optimized with first-order loss (M1), second-order loss (M2), and self-consistency loss (SC) Figures (b-d). The second row presents combinations of losses: M1+M2 (e), M1+SC \cite{fhla24} (f), M2+SC (g), and M1+M2+SC (Ours) (h). Quantitative results are shown in Table~\ref{tab:euclidean_distance}. 
}
\label{fig:mixture_of_gaussian_experiment}
\end{figure*}


\begin{table}[!ht] 
\centering
\caption{
\textbf{Euclidean distance loss on Gaussian datasets.} Lower values indicate more accurate distribution matching. Optimal values are in \textbf{Bold}, with \underline{Underlined} numbers representing second-best results.
For qualitative results, please refer to Figure~\ref{fig:mixture_of_gaussian_experiment}.
}
\label{tab:euclidean_distance}
\begin{tabular}{l|c|c|c}
\toprule
& \textbf{Four} & \textbf{Five} & \textbf{Eight} \\
\textbf{Losses}  & \textbf{mode} & \textbf{mode} & \textbf{mode} \\
\midrule
M1              & 2.759 & 3.281 & 3.321 \\
M2              & 11.089 & 6.554 & 10.830 \\
SC              & 6.761 & 10.893 & 7.646 \\
M1 + M2          & 0.941 & 1.097 & \underline{0.977} \\
M2 + SC          & 8.708 & 9.212 & 4.801 \\
M1 + SC  \cite{fhla24}        & \underline{0.820} & \underline{1.067} & 1.084 \\
M1 + M2 + SC   (Ours)   & \textbf{0.809} & \textbf{0.917} & \textbf{0.778} \\
\bottomrule
\end{tabular}
\end{table}



\begin{figure*}[!ht]  
\centering
\subfloat[(M1 + M2) / spin]{\includegraphics[width=0.24\textwidth]{12_new_spiral_output.pdf}}
\subfloat[(M1 + SC) / spin ]{\includegraphics[width=0.24\textwidth]{13_new_spiral_output.pdf}}
\subfloat[(M2 + SC) / spin]{\includegraphics[width=0.24\textwidth]{23_new_spiral_output.pdf}}
\subfloat[(M1 + M2 + SC) / spin]{\includegraphics[width=0.24\textwidth]{123_new_spiral_output.pdf}}\\
\caption{
\textbf{HOMO on complex datasets (Spin).} Results show HOMO optimized with various loss combinations: M1+M2 (a), M1+SC \cite{fhla24} (b), M2+SC (c), and M1+M2+SC (Ours) (d). Quantitative results are in Table~\ref{tab:euclidean_distance_other_distributions}.
}
\label{fig:circle_irr_circle_spiral}
\end{figure*}


\begin{table}[!ht]
\centering
\caption{
\textbf{Euclidean distance loss on complex datasets.} Lower values indicate better distribution matching. Optimal results are in \textbf{Bold}, with the second-best marked in \underline{Underlined}.
For qualitative results of complex distribution experiments, please refer to Figure~\ref{fig:circle_irr_circle_spiral} and Figure \ref{fig:m1_m2_appendix}, \ref{fig:m1_sc_appendix}, \ref{fig:m2_sc_appendix}, \ref{fig:m1_m2_sc_appendix}.
}
\label{tab:euclidean_distance_other_distributions}
\begin{tabular}{l|c|c|c|c}
\toprule
\textbf{Losses} & \textbf{Circle} & \textbf{Irregular} & \textbf{Spiral} & \textbf{Spin}\\
\midrule
M1 + M2          & \underline{0.642} & \underline{0.731} & 7.233 & 31.009\\
M1 + SC        & 0.736 & 0.743 & \underline{3.289} & \underline{12.055}\\
M2 + SC          & 7.233 & 0.975 & 10.096 & 50.499\\
M1 + M2 + SC      & \textbf{0.579} & \textbf{0.678} & \textbf{1.840} & \textbf{10.066}\\
\bottomrule
\end{tabular}
\end{table}



\subsection{Experiment setup} \label{sec:exp:experiment_setup}

We evaluate HOMO on a variety of data distributions and different combinations of losses. We would like to restate that the HOMO with first order loss and self-consistency loss is equal to the original One-step Shortcut model \cite{fhla24}, i.e., M1+SC. Furthermore, M1+M2+SC and M1+M2+M3+SC are our proposed methods. 

For the distribution dataset, in the left-most figure of Figure~\ref{fig:mixture_of_gaussian_experiment}, we show an example of an eight-mode Gaussian distribution. The source distribution $\pi_0$ and the target distribution $\pi_1$ are constructed as mixture distributions, each consisting of eight equally weighted Gaussian components. Each Gaussian component has a variance of $0.3$. This setup presents a challenging transportation problem, requiring the flow to handle multiple modes and cross-modal interactions.

We implement HOMO according to the losses defined in Definition~\ref{def:HOMO_loss}, which include the first-order loss $\ell_{2,1,\theta_1}(x_t, \dot{x}_t^{\True})$, the second-order loss $\ell_{2,2,\theta_2, \theta_1}(x_t, \ddot{x}_t^{\True})$, and the self-consistency loss $\ell_{\mathrm{selfc}}(x_t, \dot{x}_t^{\mathrm{target}})$. 
Following Remark~\ref{rem:simplicity_notations}, we denote first-order matching as M1, which implies that HOMO is optimized solely by the first-order loss $\ell_{2,1,\theta_1}(x_t, \dot{x}_t^{\True})$. Second-order matching is denoted as M2, where HOMO is optimized only by the second-order loss $\ell_{2,2,\theta_2, \theta_1}(x_t, \ddot{x}_t^{\True})$. We refer to HOMO optimized solely by the self-consistency loss as SC, denoted by $\ell_{\mathrm{selfc}}(x_t, \dot{x}_t^{\mathrm{target}})$.
For all experiments, we optimize models by the sum of squared error (SSE). 
For the target transport trajectory setting, we follow the VP ODE framework from~\cite{rectified_flow}, which is $x_t = \alpha_t x_0 + \beta_t x_1$. We choose $\alpha_t = \exp(-\frac{1}{4} a(1-t)^2 - \frac{1}{2} b(1-t))$ and $\beta_t = \sqrt{1 - \alpha_t^2}$, with hyperparameters $a = 19.9$ and $b = 0.1$.


\subsection{Mixture of Gaussian experiments} \label{sec:results_and_analysis}

We analyze the performance of HOMO on Gaussian mixture datasets \cite{lssz24_gm} with varying modes (four, five, and eight). The most challenging is the eight-mode distribution, where HOMO with all three losses (M1+M2+SC) produces the best results, achieving the lowest Euclidean distance.

The eight-mode Gaussian mixture distribution dataset (Figure~\ref{fig:mixture_of_gaussian_experiment} (a) ) contains eight Gaussian distributions whose variance is $0.3$. 
Eight source mode (\textbf{brown}) positioned at a distance $D_0 = 6$ from the origin, and eight target mode (\textbf{indigo}) positioned at a distance $D_0 = 13$ from the origin, each mode sample 100 points. 
HOMO optimized with first-order, second-order, and self-consistency losses is the only model that can accurately learn the target eight-mode Gaussian distribution, achieving high precision as evidenced by the lowest Euclidean distance loss among all tested configurations.
We emphasize the importance of the second-order loss. Without it, the model struggles to accurately capture finer distribution details (Figure~\ref{fig:mixture_of_gaussian_experiment} (f)). However, when included, the model better matches the target distribution (Figure~\ref{fig:mixture_of_gaussian_experiment} (h)).

We further analyze how each loss contributes to the final performance of the HOMO. (1) The first-order loss enables HOMO to learn the general structure of the target distribution, but it struggles to capture finer details, as shown in Figure~\ref{fig:mixture_of_gaussian_experiment} (b) and Figure~\ref{fig:mixture_of_gaussian_experiment} (g). (2) The second-order loss can lead to overfitting in the target distribution, as shown in Figure~\ref{fig:mixture_of_gaussian_experiment}(c). When used alone, the second-order loss may cause the model to focus too much on details and lose sight of the broader distribution. (3) The self-consistency loss enhances the concentration of the learned distribution, as shown in Figure~\ref{fig:mixture_of_gaussian_experiment} (d). Without the self-consistency loss, as shown in Figure~\ref{fig:mixture_of_gaussian_experiment} (e), the learned distribution becomes more sparse. 

\subsection{Complex distribution experiments} \label{sec:exp:complex_distribution}
In this section, we conduct experiments on datasets with complex distributions, where we expect our HOMO model to learn the transformation from a regular source distribution to an irregular target distribution.

We first introduce the dataset used in Figure~\ref{fig:circle_irr_circle_spiral}. In the spin dataset, we sample $600$ points from a Gaussian distribution with a variance of $0.3$ for both the source and target distributions.
The second-order loss is essential for accurate fitting, particularly for irregular and spiral distributions. As shown in Figure~\ref{fig:circle_irr_circle_spiral} (b) and (d), incorporating the second-order loss allows the model to better align with the outer boundaries of the target distribution.

We emphasize the critical role of the second order loss in the success of our HOMO model for learning complex distributions. As demonstrated in Figure~\ref{fig:circle_irr_circle_spiral} (b), the original shortcut model, which includes only first-order and self-consistency losses, fails to accurately fit the outer circle distribution. In contrast, the result of our HOMO model, shown in Figure~\ref{fig:circle_irr_circle_spiral} (d), illustrates that adding the second-order loss enables the model to generate points within the outer circle. This highlights the importance of the second-order loss in enabling the model to learn more complex distributions.

We have also performed experiments with HOMO optimized using each loss individually, as well as on other distribution datasets. Due to space limitations, we refer the reader to Section~\ref{sec:app:rectified_flow}, \ref{sec:app:second_order}, and \ref{sec:app:self_consistency} for further details.

Based on the above analysis, we find that each loss plays a crucial role in enhancing HOMO's ability to learn arbitrary target distributions, with the second-order loss further improving its performance.

\subsection{Third-order HOMO}

As discussed in previous sections, second-order HOMO has shown great performance on various distribution datasets. Therefore, in this section, we further investigate the performance of HOMO for adding an additional third-order loss.

We begin by introducing the dataset we used in this section.
We use three kinds of datasets: 2 Round spin, 3 Round spin, and Dot-Circle datasets. In 2 Round spin dataset and 3 Round spin dataset, we both sample 600 points from Gaussian distribution with $0.3$ variance for both source distribution and target distribution. In Dot-Circle datasets, we sample 300 points from the center dot and 300 points from the outermost circle, combine them as source distribution, and then sample 600 points from 2 round spin distribution. 

The qualitative results from the experiments (Figure~\ref{fig:main_paper_3rd_order_homo}) demonstrate that the additional third-order loss enables HOMO to better capture more complex target distributions. For instance, the comparison between Figure~\ref{fig:main_paper_3rd_order_homo} (c) and Figure~\ref{fig:main_paper_3rd_order_homo} (d), as well as between Figure~\ref{fig:main_paper_3rd_order_homo} (g) and Figure~\ref{fig:main_paper_3rd_order_homo} (h), illustrates how the third-order loss enhances the model's ability to fit more intricate distributions. These findings are consistent with the quantitative results presented in Table~\ref{tab:euclidean_distance_complex_datasets_complex}.

The trend of incorporating higher-order loss terms to improve model performance highlights the importance of introducing higher-order supervision in modeling the complex dynamics of distribution transformations between the source and target distributions. By capturing higher-order interactions, the model is better equipped to understand and adapt to the intricate relationships within the data, leading to more accurate and robust learning. This approach underscores the value of enriching the model's training objective to handle the complexities inherent in real-world data distributions.

\begin{figure*}[!ht]
\centering
\subfloat[SC / 2 Round]
{\includegraphics[width=0.24\textwidth]{1_3_2round_spiral_output.pdf}}
\subfloat[(M1 + SC)) / 2 Round]{\includegraphics[width=0.24\textwidth]{1_13_2round_spiral_output.pdf}}
\subfloat[(M1 + M2 + SC)) / 2 Round]{\includegraphics[width=0.24\textwidth]{1_123_2round_spiral_output.pdf}}
\subfloat[(M1+M2+M3+SC))/2 Round]{\includegraphics[width=0.24\textwidth]{1_1234_2round_spiral_output.pdf}} \\
\subfloat[SC / 3 Round]
{\includegraphics[width=0.24\textwidth]{1_3_3round_spiral_output.pdf}}
\subfloat[(M1 + SC)) / 3 Round]{\includegraphics[width=0.24\textwidth]{1_13_3round_spiral_output.pdf}}
\subfloat[(M1 + M2 + SC)) / 3 Round]{\includegraphics[width=0.24\textwidth]{1_123_3round_spiral_output.pdf}}
\subfloat[(M1+M2+M3+SC))/3 Round]{\includegraphics[width=0.24\textwidth]{1_1234_3round_spiral_output.pdf}} \\
\subfloat[SC / DC]
{\includegraphics[width=0.24\textwidth]{1_3_dotpluscircle_output.pdf}}
\subfloat[(M1 + SC)) / DC]{\includegraphics[width=0.24\textwidth]{1_13_dotpluscircle_output.pdf}}
\subfloat[(M1 + M2 + SC)) / DC]{\includegraphics[width=0.24\textwidth]{1_123_dotpluscircle_output.pdf}}
\subfloat[(M1+M2+M3+SC)) / DC]{\includegraphics[width=0.24\textwidth]{1_1234_dotpluscircle_output.pdf}} \\
\caption{
\textbf{Third-Order HOMO results on complex datasets.} We present the third-order HOMO results in three kinds of complex datasets: 2-round spiral (2 Round), 3-round spiral (3 Round), and dot-circle (DC) datasets. From left to right, we present results of HOMO optimized with different kinds of losses. \textbf{Left most, Figure (a), (e), (i), (m):} (SC) HOMO optimized with self-consistency loss; \textbf{Middle left, Figure (b), (f), (j), (n):} (M1+SC \cite{fhla24})  HOMO optimized with first-order and self-consistency losses; \textbf{Middle right, Figure (c), (g), (k), (o):} (M1+M2+SC (Ours)) HOMO optimized with first-order, second-order and self-consistency losses; \textbf{Right most, Figure (d), (h), (l), (p):} (M1+M2+M3+SC (Ours)) HOMO optimized with first-order, second-order, third-order and self-consistency losses. 
A quantitative evaluation of the complex distribution experiments is presented in Table~\ref{tab:euclidean_distance_complex_datasets_complex}.
}
\label{fig:main_paper_3rd_order_homo}
\end{figure*}

\begin{table}[!ht] 
\centering
\caption{
\textbf{Euclidean distance loss of three complex distribution datasets under original trajectory setting.} Lower values indicate more accurate distribution transfer results. Optimal values are highlighted in \textbf{Bold}. And \underline{Underlined} numbers represent the second best (second lowest) loss value for each dataset (row). 
For the qualitative results of a mixture of Gaussian experiments, please refer to Figure~\ref{fig:main_paper_3rd_order_homo}.
}
\label{tab:euclidean_distance_complex_datasets_complex}
\small
\begin{tabular}{l|c|c|c}
\toprule
& \textbf{2 Round} & \textbf{3 Round} & \textbf{Dot-} \\
\textbf{Loss terms}  & \textbf{spin} & \textbf{spin} & \textbf{Circle} \\
\midrule
SC              & 59.490 & 50.981 & 89.974 \\
M1 + SC \cite{fhla24}       & 17.866 & 23.606 & 37.550 \\
M1 + M2 + SC (Ours)     & \underline{9.417} & \underline{13.085} & \underline{30.679} \\
M1 + M2 + SC + M3	(Ours)	& \textbf{7.440} & \textbf{10.679} & \textbf{26.819} \\
\bottomrule
\end{tabular}
\end{table}
