\section{Preliminary}

This section establishes the notations and theoretical foundations for the subsequent analysis. 
Section~\ref{sec:notation} provides a comprehensive list of the primary notations adopted in this work. Section~\ref{sec:pre_flow_matching} elaborates on the flow-matching framework, extending it to the second-order case, with critical definitions underscored. 

\subsection{Notations}\label{sec:notation}

We use $\Pr[\cdot]$ to denote the probability. We use $\E[\cdot]$ to denote the expectation. We use $\var[\cdot]$ to denote the variance.
We use $\|x\|_p$ to denote the $\ell_p$ norm of a vector $x \in \R^n$, i.e. $\|x\|_1 := \sum_{i=1}^n |x_i|$, $\|x\|_2 := (\sum_{i=1}^n x_i^2)^{1/2}$, and $\|x\|_{\infty} := \max_{i \in [n]} |x_i|$. 
We use $f(x) = O(g(x))$ or $f(x) \lesssim g(x)$ to denote that $f(x) \leq C\cdot g(x)$ for some constant $C>0$.
We use $\N(0,I)$ to denote the standard Gaussian distribution.

\subsection{Shortcut model}\label{sec:pre_flow_matching}

Next, we describe the general framework of flow matching and its second-order rectification. These concepts form the basis for our proposed method, as they integrate first and second-order information for trajectory estimation.

\begin{fact}\label{fac:one_second_order}
Let a field $x_t$ be defined as 
\begin{align*}
x_t = \alpha_t x_0 + \beta_t x_1,
\end{align*}
where $\alpha_t$ and $\beta_t$ are functions of $t$, and $x_0, x_1$ are constants. Then, the first-order gradient $\dot{x_t}$ and the second-order gradient $\ddot{x_t}$ can be manually calculated as
\begin{align*}
\dot{x}_t &= \dot{\alpha_t} x_0 + \dot{\beta_t} x_1, \\
\ddot{x}_t &= \ddot{\alpha_t} x_0 + \ddot{\beta_t} x_1.
\end{align*}
\end{fact}

In practice, one often samples $(x_0, x_1)$ from $(\mu_0, \pi_0)$ and parameterizes $x_t$ (e.g., interpolation) at intermediate times to build a training objective that matches the velocity field to the true time derivative $\dot{x}_t$.

\begin{definition}[Shortcut models, implicit definition from page 3 on~\cite{fhla24}]
\label{def:shortcut_models}

Let $\Delta t = 1 / 128$. 
Let $x_t$ be current field. 
Let $t \in \mathbb{N}$ denote time step. 
Let $u_1( x_t, t, d )$ be the network to be trained. 
Let $d \in ( 1 / 128, 1 / 64,\dots, 1 / 2, 1 )$ denote step size. 
Then, we define Shortcut model compute next field $x_{t + d}$ as follow: 
\begin{align*}
x_{t + d} = 
\begin{cases}
x_t + u_1( x_t, t, d ) d & \mathrm{if } d \geq 1 / 128, \\
x_t + u_1( x_t, t, 0 ) \Delta t & \mathrm{if } d < 1 / 128.
\end{cases}
\end{align*}
\end{definition}

