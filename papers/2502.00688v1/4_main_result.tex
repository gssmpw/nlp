
\section{Theoretical Analysis} \label{sec:main_result}


In this section, we will introduce our main result, the approximation error of the second order flow matching. The theory for higher order flow matching is deferred to Section~\ref{sec:app:higher_order_flow_matching}.

\begin{algorithm}[!ht]\caption{HOMO Training}
\begin{algorithmic}[1]
\Procedure{HOMOTraining}{$\theta, D, p, k$}
\State \Comment{Parameter $\theta$ for HOMO model $u_1$ and $u_2$.}
\State \Comment{Training dataset $D$}
\State \Comment{Stepsize and time index distribution $p$}
\State \Comment{Batch size $k$}
\While{not converged}
\State $x_0 \sim \N (0, I), x_1 \sim D, (d, t) \sim p$
\State $\beta_t \gets \sqrt{1-\alpha_t^2}$
\State $x_t \gets \alpha_t \cdot x_0 + \beta_t \cdot x_1$ \Comment{Noise data point}
\For{first $k$ batch elements}
\State $\dot s_t^{\True} \gets \dot{\alpha_t} x_0 + \dot{\beta_t} x_1$ \Comment{First-order target}
\State $\ddot s_t^{\True} \gets \ddot{\alpha_t} x_0 + \ddot{\beta_t} x_1$ \Comment{Second-order target}
\State $d \gets 0$
\EndFor
\For{other batch elements}
\State $s_t \gets u_1 ( x_t, t, d)$ \Comment{First small step of first order}
\State $\dot s_t \gets u_2 (u_1 ( x_t, t, d), x_t, t, d)$ \Comment{First small step of second order}
\State $x_{t + d} \gets x_t + d \cdot s_t + \frac{d^2}{2} \dot s_t $ \Comment{Follow ODE}
\State $s_{t + d} \gets u_1 ( x_{t + d}, t + d, d )$ \Comment{Second small step of first order}
\State $\dot s_t^{\mathrm{target}} \gets$ stopgrad $(s_t + s_{t+d}) / 2$ \Comment{Self-consistency target of first order }
\EndFor
\State $\theta \gets \nabla_\theta ( \| u_1 ( x_t, t, 2d ) - \dot s_t^{\True} \|^2$
\Statex \hspace{4.2em} $ + \| u_2 (u_1 (x_t, t, 2d), x_t, t, 2d) - \ddot s_t^{\True} \|^2$
\Statex \hspace{4.2em} $ + \| u_{1}(x_t, t, 2 d) - \dot{s}_t^{\mathrm{target}}\|^2)$
\EndWhile
\State \Return{$\theta$}
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
[!ht]
\caption{HOMO Sampling}
\begin{algorithmic}[1]
\Procedure{HOMOSampling}{$\theta, M$}
\State \Comment{Parameter $\theta$ for the HOMO model $u_1$ and $u_2$}
\State \Comment{The number of sampling steps $M$}
\State $x \sim \N (0, I)$
\State $d \gets 1 / M$
\State $t \gets 0$
\For{$n \in [0, \dots, M - 1]$}
\State $x \gets x + d \cdot u_1 (x, t, d) + \frac{d^2}{2} \cdot u_2 (u_1 (x, t, d), x, t, d)$
\State $t \gets t + d$
\EndFor
\State \textbf{return} $x$
    \EndProcedure
\end{algorithmic}
\end{algorithm}



We first present the approximation error result for the early stage of the diffusion process. This result establishes theoretical guarantees on how well a neural network can approximate the first and second order flows during the initial phases of the trajectory evolution.

\begin{theorem}[Approximation error of second order flow matching for small $t$, informal version of Theorem~\ref{thm:secon_order_small_t:formal}]\label{thm:secon_order_small_t:informal}
    Let $N$ be a value associated with sample size $n$. Let $T_0 := N^{-R_0}$ and $T_* := N - \frac{\kappa^{-1} - \delta}{d}$ where $R_0, \kappa, \delta$ are some parameters.  Let $s$ be the order of smoothness of the Besov space that the target distribution belongs to.
    Under some mild assumptions, there exist neural networks $\phi_{1},\phi_2$ from a class of neural networks such that, for sufficiently large $N$, we have
\begin{align*}
    &~ \int (\|\phi_1(x, t) - \dot{x}_t^\mathrm{true}\|_2^2 + \|\phi_2(x, t) - \ddot{x}_t^\mathrm{true}\|_2^2) p_t(x) \d x \\ 
    \lesssim &~ (\dot{\alpha}_t^2 \log N + \dot{\beta}_t^2 ) N^{- \frac{2s}{d}} +
    \E_{x \sim P_t}[\|\dot{x}^\mathrm{true}_t - \ddot{x}^\mathrm{true}_t\|_2^2]
\end{align*}
    holds for any $t \in [T_{0}, 3T_{*}]$. In addition, $\phi_1, \phi_2$ can be taken so we have
    \begin{align*}
         \|\phi_1(\cdot,t) \|_\infty = &~ O(  |\dot{\alpha}_t | \sqrt{\log n} +  |\dot{\beta}_t |), \\ \|\phi_2(\cdot,t) \|_\infty = &~ O(  |\dot{\alpha}_t | \sqrt{\log n} +  |\dot{\beta}_t |).
    \end{align*}
\end{theorem}

Next, we present the approximation error result for the later stages, confirming that the second-order flow matching remains effective throughout the generative process.

\begin{theorem}[Approximation error of second order flow matching for large $t$, informal version of Theorem~\ref{thm:secon_order_large_t:formal}]\label{thm:secon_order_large_t:informal}
    Let $N$ be a value associated with sample size $n$. Let $T_0 := N^{-R_0}$ and $T_* := N - \frac{\kappa^{-1} - \delta}{d}$ where $R_0, \kappa, \delta$ are some parameters.  Let $s$ be the order of smoothness of the Besov space that the target distribution belongs to.
    Fix $t_{*} \in [T_{*},1]$ and let $\eta>0$ be arbitrary. Under some mild assumptions, there exists neural networks $\phi_{1},\phi_2$ from a class of neural networks such that
\begin{align*}
    &~ \int (\|\phi_1(x, t) - \dot{x}_t^\mathrm{true}\|_2^2 + \|\phi_2(x, t) - \ddot{x}_t^\mathrm{true}\|_2^2) p_t(x) \d x \\ \lesssim &~ (\dot{\alpha}_t^{2} \log N  +   \dot{\beta}_t^{2} ) N^{-\eta} +
    \E_{x \sim P_t}[\|\dot{x}^\mathrm{true}_t - \ddot{x}^\mathrm{true}_t\|_2^2]
\end{align*}
    holds for any $t \in [2t_*, 1]$. In addition, $\phi_1, \phi_2$ can be taken so we have
    \begin{align*}
         \|\phi_1(\cdot,t) \|_\infty = &~ O(  |\dot{\alpha}_t | \log N +  |\dot{\beta}_t |), \\ \|\phi_2(\cdot,t) \|_\infty = &~ O(  |\dot{\alpha}_t | \log N +  |\dot{\beta}_t |).
    \end{align*}
\end{theorem}

Overall, these two results demonstrate the effectiveness across different phases of the generative process.
