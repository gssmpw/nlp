\subsection{Bi-Directional Skyline Set Generation}
\label{sec-bimodis}
\eat{
Algorithm \apxmodis tends to exhibit enhanced efficiency when users specify higher expectations of the model performance, indicated by smaller thresholds $\boldsymbol{t}$, allowing for earlier termination. 
Yet, in cases involving extensive data discovery with a large $|\D|$, its ``reduction-only'' approach can lead to an 
% exponential
enormous number of valuating. 
To mitigate this, we introduce \bimodis, which implements a bi-directional search strategy, 
aligning with Type-5 systems in Fig~\ref{fig:dds}, 
incorporating a {\em Correlation-Based Pruning} strategy with early detection of dominance.
% Furthermore, it integrates statistical correlation analysis to streamline the search by leveraging relationships among performance metrics.
It covers the space more rapidly and can offer a more balanced view of the solution, especially in high-dimensional datasets where many features may have a minimal impact.
}
%%%%%%%%%%%%
% As the ``reduction-only'' process may lead to an enormous number of evaluating when $|\D|$ is large, we next introduce \bimodis, a bi-directional variant that covers the space more rapidly and can offer a more balanced view of the solution, especially %in %high-dimensional 
% for datasets with many 
% less critical features.

%Algorithm~\apxmodis may perform better when users specify higher 
%expectations to the model performance (with smaller upperbounds $p_u$), hence terminates earlier. 
Given our cost analysis, for skyline data generation 
with larger (more ``tolerate'') ranges $(p_l,p_u)$ 
and 
larger $|\D|$, \apxmodis 
may still need to valuate a large number of 
datasets. To further reduce valuation cost, 
we introduce \bimodis, its bi-directional variant. 
Our idea is to interact both augment and reduct 
operators, with a ``forward'' search from universal dataset, and a ``backward'' counterpart from a single dataset in $\D$. We also introduce a pruning strategy based on an early 
detection of dominance relation. % without valuation. 

\eat{
that covers the space more rapidly and can offer a more balanced view of the solution, especially 
% in high-dimensional 
for datasets with many 
less critical features. 
In addition, it exploits {\em Correlation-Based Pruning} to 
detect dominance early. 
}


\eat{
\stitle{Auxiliary structure}.
Algorithm \bimodis builds a dynamic spawning graph $G_\T$ akin to \apxmodis.
For forward search, it maintains a forward frontier $Q_f$, which initiates from a universal schema $s_U$ and progresses by reduction. 
Meanwhile, a backward frontier $Q_b$ is introduced for backward search, from a state $s_b$, which includes a minimal subset of $s_U$, \eg ensures no classes will be lost if $M$ is a classifier, and advancing step-by-step through augmentations. 
}
% This enables bidirectional exploration of the state space, detecting dominant paths early and pruning the search space to improve efficiency.


\begin{figure}
\centering
\begin{algorithm}[H]
\caption{:\bimodis
}
\begin{algorithmic}[1]
\algtext*{EndFor}
\algtext*{EndIf}
\algtext*{EndWhile}
\algtext*{EndFunction}
\algtext*{EndProcedure}

\State \textbf{Input:} 
    Configuration $C$ = $(s_U, \O, M, T, \E)$, 
    a constant $\epsilon>0$;
\State \textbf{Output:} 
     $\epsilon$-Skyline set $\mathcal{D}_F$.
     \vspace{1ex}
    
\State \textbf{set} $s_b = \text{BackSt}(s_U)$; 
    \textbf{queue} $Q_f := \{(s_U, 0)\}$, 
                   $Q_b := \{(s_b, 0)\}$; 
                   integer $i$ := $0$; 
    \label{bi:ini}

\While{$Q_f \neq \varnothing$, 
       $Q_b \neq \varnothing$ \textbf{and} 
       $Q_f \cap Q_b = \varnothing$}
\label{bi:start}
\State $(s', i)= Q_f$.dequeue();
\Comment{Forward Serach}
\State $(s'', i)= Q_b$.dequeue(); 
\Comment{Backward Serach} 
\State ${\D_F}^{i+1} = {\D_F}^i$;

\For{\textbf{all} $s^f \in$ \opg($s'$) \textbf{and} $s^b \in$ \opg($s''$)}
    \State ${\D_F}^{i + 1}$ = \upi($s^f$, ${\D_F}^{i + 1}$, ${\D_F}^i$, $\epsilon$);
    \State ${\D_F}^{i + 1}$ = \upi($s^b$, ${\D_F}^{i + 1}$, ${\D_F}^i$, $\epsilon$);
    
    \If{canPrune$(s^f,s^b)$} \label{bi:prunes}
        \State \kw{prune}($\C,s^f,s^b$); 
        % \Comment{Prune states between $s^f$ and $s^b$}
    \EndIf \label{a2:pos_prune} \label{bi:prunee}
    
    \State $Q_f$.enqueue(($s^f$, $i+1$)), 
           $Q_b$.enqueue(($s^b$, $i+1$));
    % \State $\upi(\D_F^{d+1},\epsilon)$;
\EndFor \label{a2:fse}
\EndWhile
\label{bi:end}
\State \Return $\D_F$
\end{algorithmic}
\end{algorithm}
\vspace{-3ex}
\caption{\bimodis: Bi-directional Search}
\vspace{-3ex}
\label{alg:bimodis}
\end{figure}


\eetitle{Algorithm}. 
Algorithm \bimodis, as shown in Fig.~\ref{alg:bimodis}, 
has the following steps. (1) {\em Initialization (lines~\ref{bi:ini})}. It first invokes a procedure~\kw{BackSt} to initialize a back-end start state node $s_b$. 
Two queues $Q_f$ and $Q_b$ are initialized, seeded with 
start state $s_U$ for forward search, and a back state $s_b$ 
for backward search, respectively. They serve as the forward and backward frontiers, respectively. 
(2) {\em Bi-directional Search (lines~\ref{bi:start}-\ref{bi:end}).}
\bimodis conducts an exploration from both directions, controlled by $Q_f$ for forward search, and $Q_b$ for backward search.  
Similar to \apxmodis, a Skyline set $\D_F$ is maintained 
in a levelwise manner. The difference is that 
it invokes a revised procedure \opg (with original counterpart in \apxmodis in Fig.~\ref{fig:approx}), which generates reduct operators for the forward search, and augment operators for the backward search.
\eat{
{\em Correlation-Based Pruning} is applied in line~\ref{bi:prunes} to \ref{bi:prunee}. For any two states from opposite directions, if a parameterized $\epsilon$-dominance relation is identified by procedure \kw{canPrune}, the intermediate states are pruned by the \kw{prune} procedure.
}
The search process terminates when both $Q_f$ and $Q_b$ are empty, or when a path is formed, the result $\D_F$ is returned. 

% The bidirectional search is controlled by 
% two queues $Q_f$ and $Q_b$. In the forward 
% stage, \bimodis applies reduction to  
% extends paths and compute their 
% coordinates as in \apxmodis. 
% The backward search, on the other hand, 
% applies augmentation to identify 
% ``promising'' attributes to be 
% augmented. This search is guided by 
% a correlation analysis of 
% features, which exploits a 
% correlation network of features 
% (not shown) to find highly relevant 
% features, via Pearson correlation analysis 
% (see details in~\ref{full}). 

% Whenever a set of states are
% spawned and valuated from either forward stage or 
% backward stage, it goes through 
% a pruning check (by invoking a procedure 
% \kw{canPrune}; line 8) to 
% early determine if an $\epsilon$-dominance relation holds 
% for any pairs of states, 
% one from forward frontier, and 
% the other from the backward. 
% If so, it performs a pruning process 
% to skip the spawning from one of 
% the states. Otherwise, the 
% bidirectional search continues 
% to maintain $\D_F$. 

% The above process continues until 
% no new states can be spawned from 
% either the forward or the backward 
% search, or at most $N$ states are 
% valuated. The result set 
% $\D_F$ is returned.

\eat{
The bidirectional search terminates when there are no pending states or when the two directions meet. We do Forward Search in line~\ref{a2:fss} to \ref{a2:fse}. For each pending state in $Q_f$, we will extend the path to its children generated by \opg($s'$, 'F'), which drops one feature or one \ad each time.  After calculating pos[$\rho_s$] from $\P_s$ using Equation~\ref{eq:pos}, we will prune $\rho_s$ if pos[$\rho_s$] falls within PrunS in line~\ref{a2:pos_prune}. We prune the path falling in \swb and maintain the \swb by {\em BI-Pruning}. We then check the dominance relations to update Skyline set $\D_F$ and maintain the PrunS by \upi. At last, we enqueue the remaining children states of $s'$ into $Q_f$ for the next layer loop in line~\ref{a2:enqueue}. The backward search in line~\ref{a2:bss} to \ref{a2:bse} is similar to the forward search, which spawns children's states by augmenting one feature or one \ad each time and unpruned ones in $Q_b$. 
At the end, we will get a final Skyline set $\D_F$.
}

\eetitle{Procedure~\kw{BackSt}}. This procedure 
initializes a backend dataset $D_b$  for augmentation. 
This procedure can be tailored to the specific task. 
For example, 
for a classifier $M$ with input features  
and a target attribute $A$ to be classified, 
we sample a small (minimal) set of tuples in $D_U$ to 
$D_b$ that covers all 
% the values of $\ad$ of $A$ to ensure that no classes will be ``missed''.
values of the active domain $\ad$ of $A$, to ensure that 
no classes will be ``missed'' in dataset $D_b$. 
Other task-specific strategies can also be applied here. 

\vspace{.5ex}
To reduce the valuation cost, \bimodis leverages correlation analysis 
over historical performance $T$, to 
assert ``non-$\epsilon$-dominance'' early, without a full valuation of their measures $\P$.  

\stitle{Correlation-Based Pruning}. At runtime, 
\bimodis dynamically maintains 
a correlation graph $G_\C$, where 
each node represents a measure in $\P$, and there is an edge $(p_i, p_j)$ in $G_\C$ if $p_i$ and $p_j$ are {\em strongly correlated}, with an associated weight $|\kw{corr}(p_i, p_j)|$~\cite{zheng2019towards}. Here 
we say two measures are strongly correlated, 
if their Spearman correlation coefficient $\kw{corr}(p_i, p_j)\geq \theta$, given their 
value distribution in the current 
set of tests $T$, for a user-defined threshold $\theta$. 
$G_\C$ is dynamically updated, 
as more valuated tests are added to $T$.

\eat{
For each pair $p_i$, $p_j \in \P \cup \{|D|\})$, \bimodis 
maintains the {\em Spearman correlation coefficient} $\kw{corr}(p_i, p_j)$ based on the up-to-date testset $T$. 
A perfect Spearman correlation of $\pm 1$ occurs when both variables are perfect monotone functions of each other~\cite{zheng2019towards}. The pair 
$(p_i, p_j)$ is considered strongly correlated if $|\kw{corr}(p_i, p_j)|$ exceeds a user-defined threshold $\theta$. 
We maintain a correlation graph $G_\C$, where each node represents a measure in $\P$, and there is an edge $(p_i, p_j)$ in $G_\C$ if $p_i$ and $p_j$ are strongly 
correlated, with an associated weight 
$|\kw{corr}(p_i, p_j)|$. 
$G_\C$ is continuously updated to capture the strength of correlations among measures as more valuated tests are added to $T$.
}



\eetitle{Parameterized Dominance}. 
% During the runtime, 
\bimodis also ``parameterize'' any unvaluated measures in the performance vector $s.\P$ 
of a state $s$ with a potential range $[\hat{p_l}, \hat{p_u}]\subseteq [p_l, p_u]$. This range is derived from the 
valuated measures that are most strongly correlated, by consulting $G_\C$ and test sets $T$. 
The entire vector $s.\P$ is incrementally 
updated, for each $p\in \P$, by setting 
(1) $s.\P(p)$ as $t.p$ (valuated), if there is a corresponding test $t=(M, D_s)\in T$ with $t.p$ valuated; or (2) $s.\P(p)$ as a variable with an estimated range $[s.\hat{p_l}, s.\hat{p_u}]$, if no test over $p$ of $D_s$ is valuated.
 
A state $s$ is {\em parameterized $\epsilon$-dominated by} another state $s'$, denoted as $s' \succapprox_{\epsilon} s$,  if for each $p \in \P$, 
\tbi
\item $s'.\P(p)\leq (1+\epsilon) s.\P(p)$, if both 
% $s'.\P(p)$ and $s.\P(p)$ 
are valuated; 
\item $s'.\hat{p_u} \leq (1+\epsilon) s.\hat{p_l}$, if neither 
% $s'.\P(p)$ nor $s.\P(p)$ 
is valuated; or
\item $s'.\P(p) \leq (1+\epsilon) s.\hat{p_l}$ 
(resp. $s'.\hat{p_u} \leq (1+\epsilon) s.\P(p)$), 
if $s'.\P(p)$ (resp. $s.\P(p)$) is valuated but 
$s.\P(p)$ (resp. $s'.\P(p)$) is not. 
\ei

Based on the above construction, \bimodis 
monitors a monotonicity condition as follows. 

\eetitle{Monotonicity Condition}. 
Given the current test set $T$, we say a state $s$ (resp. $s'$) with  
a performance measure $p$ at a path $\rho$ 
has a {\em monotonicity property}, if for 
any state $s''$ reachable from $s$ (resp. can reach $s'$) via $\rho$, 
$s.\hat{p_u}\textless \frac{s''.\hat{p_l}}{1+\epsilon}$ (resp. $s'.\hat{p_u}\textless \frac{s''.\hat{p_l}}{1+\epsilon}$). 

Given two states $s$ and $s'$,  
where $s' \succapprox_{\epsilon} s$, 
a state $s''$ on a path $\rho$ from $s$ or to $s'$ 
{\em can be pruned} by Correlation-based Pruning, if for every $p\in \P$, $s''$ has $p$ 
at $\rho$ with a monotonicity property 
\wrt $s$ (resp. $s'$). 
We present the following pruning rule. 

\begin{lemma}
\label{lm-prune}
Let $s \in Q_f$ and $s' \in Q_b$. 
If $s' \succapprox_{\epsilon} s$, 
then for any state node $s''$ 
on a path from $s$ or to $s'$  
that can be pruned by Correlation-Based Pruning, 
$D_{s''}$ is not in any $\epsilon$-Skyline set  
of the datasets that can be generated from valuated states.
\end{lemma} 

\begin{proofS}
We verify the pruning rule with a case study of 
$s$ and $s'$, subject to 
the monotonicity property.  
\textbf{Case 1: Both $s'.\P(p)$ and $s.\P(p)$ are valuated.}
If $s' \succapprox_{\epsilon} s$, then by definition, $s'.\P(p)\leq (1+\epsilon) s.\P(p)$ for all $p \in \P$. This 
readily leads to $\epsilon$-dominance, \ie $s'\succeq_\epsilon s$. As $s''$ has every performance measures 
$p\in \P$ with a monotonicity property \wrt $s$, 
$s\succeq_\epsilon s''$. Hence $s''$ can be safely pruned without valuation. 
\textbf{Case 2: Neither $s'.\P(p)$ nor $s.\P(p)$ is valuated.} By definition, as $s' \succapprox_{\epsilon} s$, 
then for every $p\in \P$, $s'.\hat{p_u}\leq (1+\epsilon) s.\hat{p_l}$. 
Given that $s''$ has every performance measures 
$p\in \P$ with a monotonicity property \wrt $s$, 
then by definition, for each $p\in \P$, we have 
$s'.p\leq s'.\hat{p_u}\leq (1+\epsilon) s.\hat{p_l}\leq (1+\epsilon)s.\hat{p_u}\textless (1+\epsilon)\frac{s''.\hat{p_l}}{1+\epsilon} \leq s''.p$, for 
every $p\in \P$. By definition of state dominance, $s' \succ s''$, for unevaluated $s''$.  
Following a similar proof, 
one can infer that $s \succ s''$ 
for a state $s$ in the forward front 
of \bimodis. 
Hence $s''$ can be safely pruned. 
\textbf{Case 3: One of $s'.\P(p)$ or $s.\P(p)$ is valuated.}
Given that $s' \succapprox_{\epsilon} s$, we have  
 (a) $s'.\P(p) \leq (1+\epsilon) s.\hat{p_l}$, if only $s'.\P(p)$ is valuated; or  (b) $s'.\hat{p_u} \leq (1+\epsilon) s.\P(p)$, if only $s.\P(p)$ is valuated.
Consider case 3(a). 
As $s$ can reach $s''$ via a path $\rho$, and 
$s''$ satisfiies the pruning condition, 
we can infer that 
$s'.\P(p) \leq (1+\epsilon) s.\hat{p_l} \leq 
(1+\epsilon) s.\hat{p_u} \textless (1+\epsilon) 
\frac{s''.\hat{p_l}}{1+\epsilon}\leq s''.p$, 
hence $s'\succ s''$. Similarly for case 3(b), we can infer that   
$s'.\hat{p_u} \leq (1+\epsilon) s.p \leq 
(1+\epsilon) s.\hat{p_u} \textless (1+\epsilon) 
\frac{s''.\hat{p_l}}{1+\epsilon}\leq s''.p$. 
hence $s'\succ s''$.  
For both cases, $s''$ can be pruned 
without evaluation. 
Lemma~\ref{lm-prune} hence follows. 
\end{proofS}



Procedures~\kw{canPrune} and~\kw{prune} (lines~11-12; omitted) 
asserts the Correlation-Based Pruning condition, 
and perform the maintenance of 
$G_\C$, $T$ and other auxiliary structures, 
respectively. Note that the above rule is checkable in PTIME 
\wrt input size $|\D_S|$. When $|\D_S|$ 
is large, one can generate a path with 
its states unevaluated, 
and check at runtime if the condition holds between 
evaluated states in the forward and backward 
frontier. 

We present the detailed analysis in~\cite{full}.

\eat{
if any state reachable from $S_U$ or 
can reach $s_b$ via the transition path can be 
skipped without valuation, with early detection of 
$\epsilon$-dominance. 
% We consider a subset of measures $\P^k \subseteq \P$ that exhibit {\em monotonic} behaviors throughout the search. 
% Pruning decisions mainly rely on measures that strongly correlate with the size of the result dataset $|D|$, denoted as $\P_{st}$.
We maintain a bounds list that stores pairs of performance vectors with validated $\epsilon$-dominance relationships, where each vector in a pair is associated with states from opposite directions. 
For any feasible states $s$ and $s'$ from opposite directions, we first check if they fall within an existing bound. If they do, they are skipped; otherwise, the bounds list is updated or expanded accordingly. 
}

\eat{
\begin{proofS}
(1) We show that 
the parameterized $\epsilon$-dominance  
includes cases as necessary or sufficient 
conditions for $\epsilon$-dominance. 
%then $s' \succ_{\epsilon} s$. 
This can be verified by contradiction, 
and the definition of 
parameterized dominance. 
(2) We then show that 
for any state $s''$ on a path from $s$ or to $s'$, if $s''$ is pruned according to Correlation-Based Pruning 
, there must exists a state $s_p \in G_\T$ that $\epsilon$-dominates 
$s''$. 
\end{proofS} 
}


\begin{example} 
We illustrate {\em Correlation-Based Pruning} in the figure below. From left to right, it depicts a set of test records $R$, the correlation graph $G_\C$, and part of the running graph $G_\T$.  
$G_\C$ is constructed from $T$ with measures as nodes and Spearman correlations as edge weights. 
For each $s_n \in G_\T$, the associated $p \in \P_{s_n}$ is obtained by test $t_{s_n} = (M, D_{s_n})$. 
\begin{center}
\begin{small}
    \hspace*{5pt}
    \begin{minipage}[c]{0.27\textwidth}
        \centering
        \begin{tabular}{|c|c|c|c|c|}
            \hline
             & Label & $p_1$ & $p_2$ & $p_3$ \\
             \hline
             \tikz[remember picture] \node[anchor=center, inner sep=0] (su){$s_U$}; & (1, 1, 1, 1) & \underline{0.42} & \underline{0.18} & 0.9 \\
             \hline
             $s_1$ & (1, 1, 1, 0) & 0.4 & 0.17 & 0.1 \\
             \hline
             \tikz[remember picture] \node[anchor=center, inner sep=0] (s2){$s_2$};  & (1, 0, 0, 1) & \underline{0.5} & \underline{0.22} & / \\
             \hline
             \tikz[remember picture] \node[anchor=center, inner sep=0] (s3){$s_3$}; & (0, 1, 0, 0) & \textbf{0.45} & \textbf{/} & / \\
             \hline
             $s_b$ & (0, 0, 0, 0) & 0.6 & 0.4 & 0.3 \\
             \hline
        \end{tabular}
    \end{minipage}%
    \begin{minipage}[c]{0.23\textwidth}
        \centering        \includegraphics[height=0.87in]{fig/bimodis}
    \end{minipage}
\end{small}
\end{center}
\begin{tikzpicture}[remember picture, overlay]
  \coordinate (suwest) at (su.west);
  \coordinate (s2west) at (s2.west);
  \coordinate (midpoint) at ($(suwest)!0.5!(s2west)$);

  \draw[line width=0.6pt] 
    ([xshift=-1.73pt]suwest) -- 
    ([xshift=-8.73pt]suwest) -- 
    ([xshift=-10pt]s2west) -- 
    ([xshift=-3pt]s2west);

  \draw[->, line width=0.6pt] 
  ([xshift=-9.7pt]midpoint) -- 
  ([xshift=-14pt]midpoint) -- 
  ([xshift=-14.5pt]s3.west) -- 
  ([xshift=-6.2pt]s3.west);
\end{tikzpicture}

At $\theta=0.8$, $p_1$ and $p_2$ are positively correlated with each other and negatively correlated with $|D|$, so $\P^k = \{p_1, p_2\}$.
% so they must be assigned values prior to pruning validation in $G_\T$.
From $s_U$ and $s_b$, the forward and backward frontiers derive states $s_1$ and $s_3$, respectively.
To estimate $s_3.\P(p_2)$, note that $s_3.\P(p_1) = 0.45$, which lies between $s_U.\P(p_1) = 0.42$ and $s_2.\P(p_1) = 0.5$.
Given the strong correlation between $p_1$ and $p_2$,
% Leveraging the strong correlation between $p_1$ and $p_2$, 
% we set $s_3.\P(\hat{p_{l_2}}) = s_U.\P(p_2)$ and $s_3.\P(\hat{p_{u_2}}) = s_2.\P(p_2)$, so that $s_3.\P(p_2)$ can be inferred to fall 
we infer $s_3.\P(p_2)$ to be within the interval 
% $[t_{s_U}.p_2, t_{s_2}.p_2]$, 
% thus we assign $t_{s_3}.p_2$ 
% which is range 
$[0.18, 0.22]$, with $\hat{p_{l_2}} = s_U.\P(p_2)$ and $\hat{p_{u_2}} = s_2.\P(p_2)$. 
With $\epsilon=0.3$, we find $s_3 \succapprox_{\epsilon} s_1$ because $0.45 \leq (1+0.3)\cdot0.4$ and $0.22 \leq (1+0.3)\cdot0.17$. 
For intermediate states $s_4$ (bitmap entry $(1, 1, 0, 0)$) and $s_5$ (bitmap entry $(0, 1, 1, 0)$), which are not recorded in $T$ and have $|D_{s_4}| = |D_{s_5}| = 2$, a similar inference process shows they fall within the bounds set by $[s_1.\P, s_3.\P]$. As a
result, $s_4$ and $s_5$ can be pruned.
\end{example}

\stitle{Time Cost}. 
% The algorithm 
\bimodis 
takes the same time complexity as \apxmodis. 
The $(N, \epsilon)$-approximation holds 
for \bimodis given that it correctly updates 
the $\epsilon$-Skyline set by definition. 
Our experimental study verifies that it is much faster in practice and particularly suitable for larger $\epsilon$ or search spaces (represented by maximum path length). It also scales more efficiently for large datasets (see Exp-3 in Section~\ref{sec:exp}).

%By BackSt, we add a minimal set of instances into an empty setting $s_E$ to ensure each \ad in $M$'s target feature remains in the start state of backward search. To be specific, no classes will be lost for classification.

%\eetitle{BI-Pruning.}
%\bimodis exploits a bidirectional search  strategy that 
%enables an effective pruning which takes advantage of
%the monotonicity property of performance measures during 
%the bidirectional search process in Lemma~\ref{lm-prune}.

%%% Pruning strategies. In terms of Lemmas. Incorporate Sandwich pruning. 

\eat{
\eetitle{Discussion on Estimators}. 
The lower and upper bound for the performance of a surrogate model~\cite{mousavi2020minimax}.

\begin{lemma} 
\label{theo:surrogate}
If an estimator the performance value, assume $\forall \text{estimator}, P_i(\text{accuracy}) \leq (1+\epsilon)\theta \cdot P$, then the actual approximation ratio would be $(1+f(\epsilon))$
\end{lemma}

\warn{Practically show that it is small and cite some papers.}


Exponential time, with a strong approximate guarantee. The size of the Skyline set is polynomially bounded~\cite{tsaggouris2009multiobjective}.
}


\eat{
Issues:

1. Enumerate the exponential number of cases. It will only apply to small cases where we can enumerate all the possible configurations in the worst case. 

2. Lack of diversity in values.
}

