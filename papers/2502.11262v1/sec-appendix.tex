% \section*{Appendix: Proofs and Algorithms}
\label{sec-appendix}

\vspace{2ex}

\section{Algorithms and Proof}

\subsection{\apxmodis}

\stitle{Proof of Lemma~\ref{lm-approximability}}
{\em For any constant $\epsilon$, \apxmodis correctly computes an $\epsilon$-Skyline set $\Pi$ that approximates a Skyline set defined on the $N$ states it valuated. }

\begin{proof}
We establish the $\epsilon$-approximability of \apxmodis by constructing a reduction from \modis to the multi-objective 
shortest path problem (\mos)~\cite{tsaggouris2009multiobjective}. 

\eetitle{Reduction}.
An instance of \mos consists of an edge-weighted graph $G_w$, where each edge $e_w$ is assigned a $d$-dimensional attribute vector $e_w.c$. The cost of a path $\rho_w$ in $G_w$ is defined as $\rho_w.c$ = $\sum_{e_w\in\rho_w}$ $e_w.c$. 
The dominance relation between two paths $\rho_w$ and $\rho'_w$ is determined by comparing their costs. 
Specifically, $\rho_w$ dominates $\rho'_w$ if $\rho_w$ has equal or lower costs than $\rho'_w$ in all dimensions and is strictly better in at least one dimension. 
The objective is to compute a Skyline set of paths from a start node $u$ to all other nodes in the graph. 

We construct the reduction from our problem to \mos. 
(1) We define $G_w$ as an edge weighted counterpart 
of a running graph $G_\T$. (a) Each vertex in $G_\T$ represents a unique state $s$ during the execution of \apxmodis, with each state corresponding to a specific dataset configuration in the data discovery process. The graph $G_\T$ contains $N$ vertices, corresponding to the $N$ states that \apxmodis has spawned and valuated. 
(b) Each edge $(s, s')$ in $G_\T$ represents a transition from state $s$ to state $s'$, resulting from applying an operation (e.g., reduction or augmentation) that modifies the dataset. The edge is weighted by the difference in performance measures in $\P$ between the two states: $e_w$ = $s.\P - s'.\P$. 
Here, $s.\P$ and $s'.\P$ are the performance vectors of the states $s$ and $s'$, respectively. The edge weight $e_w$ is a $d$-dimensional vector that quantifies how the performance metrics change as a result of the transition. 
A path $\rho \in G_\T$ corresponds to a sequence of transitions between states, starting from the initial state $s_U$. 
Similar to $\rho_w \in G_w$, the cumulative cost of this path $\rho.c$ is defined as the sum of the edge weights along the path, which represents the cumulative change in the performance measures $\P$ as the dataset evolves through different states. 

Given a solution $\Pi_w$ of an instance of \mos, which is an $\epsilon$-Skyline set of paths, we construct a solution for a corresponding 
instance of \modis. 
For each path $\rho_w \in \Pi_w$, we establish a corresponding path $\rho$ in $G_\T$ and identify the final state $s$ that the path reaches.
The final state $s$ corresponds to a specific dataset $D$, which is the result of applying the sequence of operations from $\rho$. 
We then include $D$ in the set $\D_F$. 
This forms a set of datasets as the solution to \modis. 

We next prove 
that %$\D_F$ is a $\epsilon$-Skyline set if and only if 
$\Pi_w$ is an $\epsilon$-Skyline set 
of paths $\Pi_w$ in $G_w$. 
if and only 
if $\D_F$ is an $\epsilon$-Skyline set 
of $\D_S$. 
%that contains the
%datasets from the set of $N$ valuated 
%states in $G_\T$. 

\etitle{If condition}. 
Let $\D_F$ be an $\epsilon$-Skyline set of $\D_S$.  
By the definition given in sec~\ref{sec-problem}, this means that for every dataset $D' \in (\D_S\setminus\D_F)$, there exists at least one dataest $D \in \D_F$ such that $D$ $\epsilon$-dominates $\D'$. 
Specifically, this means that $D$ has costs that are at most $(1+\epsilon)$ times the costs of $D'$ in all performance measures in $\P$, and $D$ has a strictly lower cost in at least one measure.
From the reduction, each path $\rho_w \in \Pi_w$ corresponds to a sequence of transitions in $G_\T$ leading to a final state $s$, which represents a dataset $D \in \D_F$. Similarly, $\rho'_w \notin \Pi_w$ corresponds to a dataset $D' \in (\D_S\setminus\D_F)$. 
Since $D$ $\epsilon$-dominates $D'$, the corresponding path 
$\rho_w$ $\epsilon$-dominates $\rho'_w$. 
This dominance is preserved because the performance measures $\P$, directly corresponding to the edge weights in $G_w$. 
Therefore, $\Pi_w$ is an $\epsilon$-Skyline set 
of paths $\Pi_w$ in $G_w$ if $\D_F$ is an $\epsilon$-Skyline set 
of $\D_S$.

\etitle{Only If condition}. Conversely, we assume the following:
(1) $\Pi_w$ is an $\epsilon$-Skyline set of paths $\Pi_w$ in $G_w$, but 
(2) the induced $\D_F$ is not an $\epsilon$-Skyline set of $\D_S$. 
Assumption (2) implies one of the following two cases:
\bi
\item \textbf{Case 1}:
There exists a dataset $D' \in (\D_S\setminus\D_F)$ that is not $\epsilon$-dominated by any dataset in $\D_F$. This means there is a corresponding path $\rho'_w$ in $G_w$ that is not $\epsilon$-dominated by any path in $\Pi_w$. This contradicts assumption (1)  because $\rho'_w$ should be $\epsilon$-dominated by at least one path in $\Pi_w$.
\item \textbf{Case 2}: There exists a dataset $D \in \D_F$ that is $\epsilon$-dominated by a dataset $D'' \in (\D_S\setminus\D_F)$. 
This means there exists a path $\rho''_w$ corresponding to $D''$ in $G_w$ that $\epsilon$-dominates the path $\rho_w$ corresponding to $D$ in $\Pi_w$. However, this would imply that $\Pi_w$ does not fully capture the $\epsilon$-Skyline set because $\rho_w$ should not be in $\Pi_w$ if it is $\epsilon$-dominated by $\rho''_w$. Thus, it contradicts assumption (1).
\ei
Both cases lead to a contradiction with the assumption (1) that $\Pi_w$ is an $\epsilon$-Skyline set of paths $\Pi_w$ in $G_w$. Therefore, the initial assumption (2) must be false, meaning $\Pi_w$ is an $\epsilon$-Skyline set 
of paths $\Pi_w$ in $G_w$, only 
if $\D_F$ is an $\epsilon$-Skyline set 
of $\D_S$. 

By proving both directions, we establish the equivalence that $\Pi_w$ is an $\epsilon$-Skyline set 
of paths $\Pi_w$ in $G_w$, if and only 
if $\D_F$ is an $\epsilon$-Skyline set 
of $\D_S$.

\eetitle{Correctness}. 
We then show that 
algorithm~\apxmodis 
is an optimized process of 
the algorithm in~\cite{tsaggouris2009multiobjective}, 
which correctly computes 
$\Pi_w$ for $G_w$.
Specifically, this means that in $G_w$, for any path $\rho'_w \notin \Pi_w$ with a corresponding state $s'$, there exists a path $\rho_w \in \Pi_w$ with a corresponding state $s$, such that for every performance measure $p_i$ (where $1\leq i \leq d$, and $d=|\P|$), the condition $s.\P(p_i) \leq (1+\epsilon)s'.\P(p_i)$ holds.

We prove the correctness of this result by induction.

\sstab
(1) \textbf{Base case}.
After the first iteration in the main procedure of \apxmodis, and due to the ``merge'' steps in the \upi procedure, the position $pos(s')$ in $\Pi^1_w$ will be occupied by a path $\rho_w$, for which: (i) $pos(s) = pos(s')$; and (ii) $s.\P(p_d) \leq s'.\P(p_d)$. From (i) and based on the Equation~(\ref{eq:pos}), for $1\leq i \leq d-1$, we have $\left\lfloor\log _{1+\epsilon} \frac{s.\P(p_i)}{p_{l_i}}\right\rfloor = \left\lfloor\log _{1+\epsilon} \frac{s'.\P(p_i)}{p_{l_i}}\right\rfloor$. This implies $\log _{1+\epsilon} \frac{s.\P(p_i)}{p_{l_i}} - 1 \leq \log _{1+\epsilon} \frac{s'.\P(p_i)}{p_{l_i}}$, so that $s.\P(p_i) \leq (1+\epsilon)s'.\P(p_i)$ for $1\leq i < d$. Combined with (ii), we conclude that $s.\P(p_i) \leq (1+\epsilon)s'.\P(p_i)$ holds for $1\leq i \leq d$.

\sstab
(2) \textbf{Induction}. 
Assume that after $i-1$ iterations, \apxmodis correctly computes the $\epsilon$-Skyline set $\Pi^{i-1}_w$ for all paths from the source node $s_U$ that contain up to $i-1$ edges. This means that for every path $\rho'_w \notin \Pi^{i-1}_w$ with at most $i-1$ edges, there exists a path $\rho_w \in \Pi^{i-1}_w$ such that the corresponding states $s$ and $s'$ satisfy: 
$$s.\P(p_i) \leq (1+\epsilon)s'.\P(p_i), \forall 1\leq i \leq d$$

%\sstab
%(3) \textbf{Induction Step}. 
We next prove that after $i$ iterations,  \apxmodis correctly computes the $\epsilon$-Skyline set $\Pi^i_w$ for all paths from the source node $s_U$ that contain up to $i$ edges.

By induction, every path in $\Pi^{i-1}_w$ $\epsilon$-dominates any other paths of up to $i-1$ edges not included in $\Pi^{i-1}_w$, so we only need to ensure the correctness of the $i$th iteration. 
In this iteration, paths are expanded to include $i$ edges. As seen in the base case, after the ``merge'' step in procedure \upi, \apxmodis ensures that for any state $s'$ corresponding to a path not included in $\Pi^i_w$, there exists at least one state $s$ with corresponding path in $\Pi^i_w$, such that:
$$s.\P(p_i) \leq (1+\epsilon)s'.\P(p_i), \forall 1\leq i \leq d$$
Thus, after $i$ iterations, $\Pi^i_w$ covers all paths with up to $i$ edges that should be included in the $\epsilon$-Skyline set.

Putting these together, we show that \apxmodis correctly computes the $\epsilon$-Skyline set $\Pi^i_w$ for all $i > 0$. This verifies the correctness of \apxmodis.
\end{proof}


\eat{
\stitle{Proof of Theorem~\ref{thm-fptas}} 
{\em Given  
datasets $\D$, 
 configuration $C$,
 and a number $N$, 
there exists an $(N,\epsilon)$-approximation 
for \modis in 
$O\left(\min(N_u^{|R_u|}, N)\cdot \left(\left(\frac{\log(p_m)}{\epsilon}\right)^{|\P|-1}+I\right)\right)$ time, 
where $|R_u|$ is the total number of 
attributes from $\D$, $N_u$ = $|R_u|+|\ad_m|$ 
with $\ad_m$ the largest active domain, $p_m$ = 
$\max\frac{p_u}{p_l}$ as 
$p$ ranges over $\P$; and $I$ the 
valuation cost per test. 
}

\begin{proof}
(1) To show there exists an FPTAS for the multi-objective data discovery problem (\modata), we first constructed an approximation preserving reduction from it to a multi-objective shortest path search problem (\mos). 

Given an instance $I$ of \modata, which comprises a data discovery system $\T$ and a configuration $C$=$(S_M, M, T)$. Here, $s_M$ is the initial state of a dataset $D$, and we have a transformation function $g$ that constructs a graph $G = (V, E)$ in {\em polynomial time} (PTIME). In $G$, each vertex represents a unique state $s$ of $D$, with $s_M$ being the source node, and each edge represents a transition $r = (s, op, s')$ between states $s$ and $s'$. For each edge $e=(s,s') \in E$, we generate two tests over $M$, labeled as $t$ = $(M, D_s)$ and $t'$ = $(M, D_{s'})$. The weight $\mathbf{c}(e)$ is calculated based on the variance in performance measures $\P$ over $M$ from $s$ to $s'$, we represent this as $\mathbf{w}(e) = f(t).\P - f(t').\P$. Such that, the objectives in \mos are the sum of weights for all edges in a given path $\rho$ in $G$, denoted as $\mathbf{c}(\rho) = \sum_{e \in \rho}\mathbf{w}(e)$. An oracle may be utilized to obtain P under the assumption of $100\%$ estimation accuracy within PTIME. Thus, $I$ is transformed to $g(I)$, which is an instance of \mos.

For any solution to \mos, which produces an $\epsilon$-Skyline set $\Pi$ of paths in $G$, we introduce a PTIME transformation function $h$ that maps the end vertex of the paths in $\Pi$ to corresponding dataset states, forming the $\epsilon$-Skyline set $\Pi'$ for \modata. In \mos, path $\rho$ dominates $\rho'$ if and only if all objectives of $\rho$ are superior to those of $\rho'$. After translating to \modata using $h$, it can be inferred that state $s$ resulting from $\rho$ is more desirable than state $s'$ resulting from $\rho'$ if all costs of $s$ are smaller. This consistency guarantees that the quality of both $\Pi$ and $\Pi'$ is preserved.

Therefore, there is an L-Reduction from \modata to \mos.

(2) Next, we prove Algorithm~\ref{alg:forward} is an FPTAS for \mos. \eat{\warn{May replace with the Bi-directional one later.}}

\stitle{Correctness}. 
We prove the correctness by induction. 
(i) In the first iteration($i=1$), $p \in \Pi^0_s$, if $\forall v \in V, \exists q \in \Pi_v^1$, such that $pos(q) = pos(p)$, and $c_d(q) \leq c_d(p)$. According to Equation~\ref{eq:pos}, we have $\lfloor\log _{r_k} \frac{c_k(q)}{c_k^{\min }}\rfloor=\lfloor\log _{r_k} \frac{c_k(p)}{c_k^{\min }}\rfloor, \forall k \in [1, d)$, therefore, $\log _{r_k} \frac{c_k(q)}{c_k^{\min }} - 1 \leq \log _{r_k} \frac{c_k(p)}{c_k^{\min }}$. Along with $r_d=1$, we have $c_k(q) \leq r_k c_k(p), \forall k \in [1, d]$. 
(ii) In the $i_{th}$ iteration, we consider a path $p=(e_1, e_2, \ldots, e_l=(u, v)) \in \Pi^i_v$, where $l \leq i$, and a subpath $p' = p/e_l$. Assuming that $\exists q' \in \Pi^{i-1}_u$, such that $c_k(q') \leq r^{i-1}_k c_k(p'), \forall k \in [1, d]$. Then for $q'' = q' + e_l$, we have $c_k(q'') \leq r^{i-1}_k c_k(p), \forall k \in [1, d]$. Moreover, after the $i_{th}$ iteration, $\exists q \in \Pi^i_v$, such that $pos(q)=pos(q'')$, and $c_d(q) \leq c_d(q'')$. Similar to (i), we have $c_k(q) \leq r_k c_k(q''), \forall k \in [1, d]$. Then, we can get $c_k(q) \leq r^i_k c_k(p), \forall k \in [1, d]$. Combine (i) and (ii), as $r_k=1+\epsilon_k$, in the special case, $\epsilon_k=\epsilon, \forall k \in [1, d)$, we ensure to get a $\epsilon$-Skyline set after the terminate iteration.

\stitle{Cost Analysis}. We next prove the cost is polynomial in terms of $n$, $m$, and $1/\epsilon$, \eat{but input $n$ is measured by an exponential function.} where $n$ is the maximum path length and $m$ is the number of edges in the given graph $G$. In Algorithm~\ref{alg:forward}, it takes up to $n$ iterations and checks up to $m$ edges per iteration for all possible filled positions in the last round's $\Pi$. Hence the total time cost is
$$O\left(n m \prod_{j=1}^{d_c + d_b-1}\left(\lfloor\log _{r_{j}}nC_{j}\rfloor+1\right)\right), C_{i}=\frac{c_{i}^{\max}}{c_{i}^{\min}} \vee C_{i}=\frac{b_{i-d_c}^{\min}}{b_{i-d_c}^{\max}}$$
Let's set $C=\max_{k\in [1,d)} C_j, d=d_c+d_b$ and $\epsilon_k=\epsilon, \forall k \in [1, d)$, then the time complexity is $O\left(n m\left(\frac{n \log nC}{\varepsilon}\right)^{d-1}\right)$.

Since we solved \mos by an FPTAS in total $O\left(n m\left(\frac{n \log nC}{\varepsilon}\right)^{d-1}\right)$ time and L-reduction ensures approximation quality of the solutions is preserved, we can get there exists an FPTAS for \modata, which takes in total $O\left(n m\left(\frac{n \log nC}{\varepsilon}\right)^{d-1}\right)$ time.

\end{proof}
}

\stitle{Proof of Lemma~\ref{cor-fptas}}. 
{\em Given $\T$ with configuration $C$, 
if $|\D_S|$ has a size in $O(f(|D_U|))$, 
where $f$ is a polynomial, 
then \apxmodis is an FPTAS for \modis. }

\begin{proof} We consider the reduction 
of an instance of \modis to its counterpart 
of \mos as detailed in the proof of Lemma~\ref{lm-approximability}.
\mos is known to be solvable by an FPTAS. That is, 
there is an algorithm that can compute an $\epsilon$-Skyline set in polynomial time relative to the size of the input graph 
%(\ie $|\D_S|$ in \modis) 
and $\frac{1}{\epsilon}$~\cite{tsaggouris2009multiobjective}. 
%The \modis problem is reduced to a multi-objective shortest path (\mos) problem, as detailed in the proof of Lemma~\ref{lm-approximability}. 

We configure \apxmodis to run in  
a $(|\D_S|, \epsilon)$-approximation, 
which is a simplified implementation of an  
FPTAS in~\cite{tsaggouris2009multiobjective} with multiple 
rounds of ``replacement'' 
strategy following path dominance. 
In the proof of Lemma~\ref{lm-approximability}, we have already shown that \apxmodis correctly computes the $\epsilon$-Skyline set for $G_w$,  which is equivalent to the $\epsilon$-Skyline set for $\D_S$ in \modis.
Meanwhile, as $|\D_S|$ is bounded by a polynomial of  
the input size $|D_U|$, 
the time complexity of \apxmodis is $O\left(f(|D_U|)\cdot \left(\left(\frac{\log(p_m)}{\epsilon}\right)^{|\P|-1}+I\right)\right)$, where 
$f$ is a polynomial. This ensures that \apxmodis approximates the Skyline set for all datasets within PTIME.
\end{proof}

\stitle{Space cost}. 
We also report the space cost. 
(1) It takes a vector of length in $O(|P|-1)$ to encode the 
position $pos(\rho)$. The replacement strategy 
in \apxmodis keeps one copy of position per path at runtime and 
``recycles'' the space once it is verified to be dominated. 
According to Equation~\ref{eq:pos}, there 
are at most $\prod_{i=1}^{|\P|-1}\left(\left\lfloor\log _{1+\epsilon}\frac{p^{max}_i}{p^{min}_i}\right\rfloor+1\right)$ paths to be remained 
in a $(|\P|-1)$-dimensional array 
at runtime, 
% \warn{add this, so the result is more straightforward},  
hence the total space cost is in 
$O\left(\prod_{i=1}^{|\P|-1}\left(\left\lfloor\log _{1+\epsilon}\frac{p^{max}_i}{p^{min}_i}\right\rfloor+1\right)\right)$. 


\eat{
For each valuedated path $\rho$, we use $(|\P|-1)$ measures in $|\P|$ (except the deterministic measure $p*$) to calculate its ``position'' $pos(\rho)$ in a ($|\P|-1$)-ary space, which we use as the Skyline set.
Here, the last measure in $\P$ is set as $p*$ by default.
When multiple paths are allocated to the same position in the Skyline set, only the one with the lowest $p^*$ is retained.
This ensures that each position in the Skyline set holds at most one path,
so the {\em Space Complexity} equals the size of the array for the Skyline set. Assuming all performance metrics are costs, according to Equation~\ref{eq:pos}, the Space Complexity is $O\left(\prod_{i=1}^{|\P|-1}\left(\left\lfloor\log _{1+\epsilon}\frac{p^{max}_i}{p^{min}_i}\right\rfloor+1\right)\right)$.
}

\subsection{\bimodis}

\stitle{Correlation based Pruning}. 
We present the details of Correlation-based Pruning. 
We first introduce a monotonicity property as the 
condition for the applicability of the pruning. 

\eetitle{Monotonicity property}. 
Given the {\em current} 
historical performances over valuated
states, we say a state $s$ (resp. $s'$) with  
a performance measure $p$ at a path $\rho$ 
has a {\em monotonicity property}, if for 
any state $s''$ reachable from $s$ (resp. can reach $s'$) via $\rho$, 
$s.\hat{p_u}\textless \frac{s''.\hat{p_l}}{1+\epsilon}$ (resp. $s'.\hat{p_u}\textless \frac{s''.\hat{p_l}}{1+\epsilon}$). 

\eat{
The pruning strategy is not applicable 
if the above condition no longer holds 
due to updates of new records. The overhead of 
checking whether the property holds for 
measurement pairs, upon the arrival 
of new tests are small. 
}

\eetitle{Pruning rule}. 
We next specify Correlation-based pruning with 
a {\em pruning rule} as follows. 
First, recall that \bimodis dynamically maintains, 
for each performance $p\in \P$ and each 
state $s$, an estimated range 
$[\hat{p_l}, \hat{p_u}]\subseteq [p_l, p_u]$. 
The bounds $\hat{p_l}$ (resp. $\hat{p_u}$ 
are updated with runtime performance estimation 
of $s$ upon the changes of correlated 
performance measures. 

Specifically, for any  
state $s'$ on a path $\rho$  
obtained by augmented features 
of its ``ancestor'' state $s$ on $\rho$, 
where $s$ has a performance $p$ ``learning cost'' 
$s.p$ with lower bound $s.\hat{p_l}$ = $0.4$, 
and an ``accuracy'' with estimated 
upperbound $s.\hat{p'_u}$ = $0.8$, 
then (1) $s'$ has an estimated running cost 
initialized as $s.\hat{p_l}$ = $0.4$, indicating 
a learning cost no smaller than the counterpart $s$ 
with smaller dataset; 
and (2) $s'$ has an estimated accuracy 
with an upperbound $s.\hat{p'_u}$ = $0.8$, 
as $p$ and $p'$ are statistically 
negatively correlated with a rule specified 
as: for every current valuated $s$,  
$p$ as ``learning cost'', and $p'$ as ``accuracy'', 
if $p$ is larger, then $p'$ is smaller. 
The algorithm \bimodis dynamically 
maintains a bounds list for 
all created states $s$ in the 
bidirectional search. 

Given two states $s$ and $s'$, 
%measures $p$ and $p'$ in $\P$, 
where $s' \succapprox_{\epsilon} s$, 
a state $s''$ on a path $\rho$ from $s$ or to $s'$ 
{\em can be pruned according to Correlation-Based Pruning} if 
%$s$, $s''$, and $s'$
% $s''$ can be reached by $s$ (can reach $s'$) via $\rho$, 
%and there exists a performance measure $p$ 
%with 
%estimated bound range $[s''.\hat{p_u}, s''.\hat[p_l]]$, 
%such that $s \succapprox_{\epsilon} s''$ or 
%$s'' \succapprox_{\epsilon} s$. 
for every $p\in \P$, $s''$ has $p$ 
at $\rho$ with a monotonicity property 
\wrt $s$ (resp. $s'$). 

Note that the above rule is checkable in PTIME 
in terms of input size $|\D_S|$. When $|\D_S|$ 
is large, one can generate a path with 
all states unevaluated, 
and check at runtime if the condition holds between 
two evaluated states and any unevaluated 
state in betwen in PTIME, to 
prune the unevaluated states. 

We are now ready to show Lemma~\ref{lm-prune}. 

\stitle{Proof of Lemma~\ref{lm-prune}}
{\em Let $s \in Q_f$ and $s' \in Q_b$. 
%or $s' \in Q_f$ and $s \in Q_b$. 
If $s' \succapprox_{\epsilon} s$, 
then any state node $s''$ 
on a path from $s$ or to $s'$, 
that can be pruned according to Correlation-Based Pruning, 
$D_{s''}$ is not in $\epsilon$-Skyline sets 
of the datasets from valuated states.}


\eat{
As the measures are normalized as ``costs'' (to be minimized), $\hat{p_l}$ (resp. $\hat{p_u}$) 
refer to an upper bound (resp. lower bound) estimation, 
both within the user specified range $[p_l, p_u]$. 
This property ensures that the estimated ranges for unvaluated measures accurately reflect each state's best and worst possible outcomes. 
}
%represents the best-case scenario, while $\hat{p_u}$ represents the worst-case scenario.

%We start by showing the following statement. 
%\stitle{Claim: $s'\succeq_\epsilon s$  $s' \succapprox_{\epsilon} s$, }

We next perform a case study of 
$s$ and $s'$ as follows, subject to 
the monotonicity property.  

\stitle{Case 1: Both $s'.\P(p)$ and $s.\P(p)$ are valuated.}
If $s' \succapprox_{\epsilon} s$, then by definition, $s'.\P(p)\leq (1+\epsilon) s.\P(p)$ for all $p \in \P$. This 
readily leads to $\epsilon$-dominance, \ie $s'\succeq_\epsilon s$. As $s''$ has every performance measures 
$p\in \P$ with a monotonicity property \wrt $s$, 
$s\succeq_\epsilon s''$. Hence $s''$ can be safely pruned 
without valuation. 

\sstab
\textbf{Case 2: Neither $s'.\P(p)$ nor $s.\P(p)$ is valuated.}
By definition, as $s' \succapprox_{\epsilon} s$, 
then for every $p\in \P$, $s'.\hat{p_u}\leq (1+\epsilon) s.\hat{p_l}$. 
Given that $s''$ has every performance measures 
$p\in \P$ with a monotonicity property \wrt $s$, 
then by definition, for each $p\in \P$, we have 
%one can infer that 
%$s.p\leq s''.p$. Hence 
$s'.p\leq s'.\hat{p_u}\leq (1+\epsilon) s.\hat{p_l}\leq (1+\epsilon)s.\hat{p_u}\textless (1+\epsilon)\frac{s''.\hat{p_l}}{1+\epsilon} \leq s''.p$, for 
every $p\in \P$. By definition of state dominance, $s' \succ s''$, for unevaluated $s''$.  
Following a similar proof, 
one can infer that $s \succ s''$ 
for a state $s$ in the forward front 
of \bimodis. 
Hence $s''$ can be safely pruned. 


\sstab
\textbf{Case 3: One of $s'.\P(p)$ or $s.\P(p)$ is valuated.}
Given that $s' \succapprox_{\epsilon} s$, we have  
\bi
\item (a) $s'.\P(p) \leq (1+\epsilon) s.\hat{p_l}$, if only $s'.\P(p)$ is valuated; or 
\item (b) $s'.\hat{p_u} \leq (1+\epsilon) s.\P(p)$, if only $s.\P(p)$ is valuated.
\ei
Consider case 3(a). 
As $s$ can reach $s''$ via a path $\rho$, and 
$s''$ satisfiies the pruning condition, 
we can infer that 
$s'.\P(p) \leq (1+\epsilon) s.\hat{p_l} \leq 
(1+\epsilon) s.\hat{p_u} \textless (1+\epsilon) 
\frac{s''.\hat{p_l}}{1+\epsilon}\leq s''.p$, 
hence $s'\succ s''$. 

Similarly for case 3(b), we can infer that   
$s'.\hat{p_u} \leq (1+\epsilon) s.p \leq 
(1+\epsilon) s.\hat{p_u} \textless (1+\epsilon) 
\frac{s''.\hat{p_l}}{1+\epsilon}\leq s''.p$. 
hence $s'\succ s''$.  
For both cases, $s''$ can be pruned 
without evaluation. 

Lemma~\ref{lm-prune} hence follows. 

\eat{
\begin{proof}
We break down the proof into two parts:
(1) 
%for two 
%states $s_1$ and $s_2$, 
%if $s_1 \succ_{\epsilon} s_2$, 
%then $s_1 \succapprox_{\epsilon} s_2$; 
the parameterized $\epsilon$-dominance  
includes cases as necessary or sufficient 
conditions for $\epsilon$-dominance. 
%then $s' \succ_{\epsilon} s$. 
This can be verified by contradiction, 
and the definition of 
parameterized dominance. 
(2) for any state $s''$ on a path from $s$ or to $s'$, if $s''$ is skipped according to Correlation-Based Pruning 
% at least a measure in  
% $P^k$ with monotonicity 
% property, 
, there exists another state $s_p \in G_\T$ that $\epsilon$-dominates 
$s''$. 

\eetitle{Parameterized $\epsilon$-dominance}
We prove that $s'\succeq_\epsilon s$ if and only if $s' \succapprox_{\epsilon} s$, considering the three cases outlined in Sec~\ref{sec-bimodis}.

Before that, we introduce the following condition: 
\begin{mdframed}
\textbf{Condition:} The monotonic relationships among performance measures derived from the correlation graph are considered ``valid''. 
These relationships are established based on recorded performances across various states and are continually refined as new records are added in.
\end{mdframed}
This condition ensures that the estimated ranges for unvaluated measures accurately reflect each state's best and worst possible outcomes. 
Since the measures are normalized as costs, where lower values are better, $\hat{p_l}$ represents the best-case scenario, while $\hat{p_u}$ represents the worst-case scenario.

\sstab
\textbf{Case 1: Both $s'.\P(p)$ and $s.\P(p)$ are valuated.}
\bi
\item Necessary Condition: if $s' \succapprox_{\epsilon} s$, then by definition, $s'.\P(p)\leq (1+\epsilon) s.\P(p)$ for all $p \in \P$. This condition directly aligns with the definition of $\epsilon$-dominance. Therefore, if $s' \succapprox_{\epsilon} s$ holds, then $s'\succeq_\epsilon s$ must hold.
\item Sufficient Condition: assume $s'\succeq_\epsilon s$ but $s' \not\succapprox_{\epsilon} s$. This implies that there exists at least one performance measure $p \in \P$ for which $s'.\P(p) > (1+\epsilon) s.\P(p)$. However, this violates the definition of $\epsilon$-dominance as $s$
would not dominate $s'$ in that measure. This contradiction proves that the assumption $s' \not\succapprox_{\epsilon} s$ is false, and therefore $s' \succapprox_{\epsilon} s$ must hold. Thus, the "Only If" direction is valid.
\ei

\sstab
\textbf{Case 2: Neither $s'.\P(p)$ nor $s.\P(p)$ is valuated.}

In this case, 
% neither $s'.\P(p)$ nor $s.\P(p)$ has been valuated, and instead, 
we have estimated ranges $[s.\hat{p_l}, s.\hat{p_u}]$ and $[s'.\hat{p_l}, s'.\hat{p_u}]$ for state $s$ and $s'$ based on monotonic relations derived from the correlation graph. 
Then the parameterized $\epsilon$-dominance condition will be
% $$s'.\hat{p_l} \leq (1+\epsilon) s.\hat{p_l} \text{ and } s'.\hat{p_u} \leq (1+\epsilon) s.\hat{p_u}$$
$s'.\hat{p_u} \leq (1+\epsilon) s.\hat{p_l}$.
\bi
\item Necessary Condition: If $s' \succapprox_{\epsilon} s$, then the estimated lower and upper bounds for $s$ and $s'$ must satisfy $s'.\hat{p_u} \leq (1+\epsilon) s.\hat{p_l}$. 
This ensures that even in the worst-case scenario, $s'$ remains $\epsilon$-dominant over the best-case scenario for $s$ on the performance measure $p$.
Therefore, if $s' \succapprox_{\epsilon} s$ holds, then $s'\succeq_\epsilon s$ must also hold.
\item Sufficient Condition: suppose $s'\succeq_\epsilon s$ but $s' \not\succapprox_{\epsilon} s$. This would imply that for at least one performance measure $p$, the estimated ranges do not satisfy $s'.\hat{p_u} \leq (1+\epsilon) s.\hat{p_l}$. 
This suggests that $s'$ could be $\epsilon$-dominanted by $s$ on $p$ in some scenarios, contradicting the assumption that $s'\succeq_\epsilon s$. Therefore, $s' \succapprox_{\epsilon} s$ must hold, making it sufficient for $\epsilon$-dominance. Thus, the "Only If" direction is valid.
\ei

\sstab
\textbf{Case 3: One of $s'.\P(p)$ or $s.\P(p)$ is valuated.}

In this scenario, one of the states (either $s'$ or $s$) has a valuated performance measure $p$ 
while the other state is assigned an estimated range based on the monotonic relationships. 
The parameterized $\epsilon$-dominance condition will differ depending on which state has the valuated measure:
\bi
\item $s'.\P(p) \leq (1+\epsilon) s.\hat{p_l}$ if only $s'.\P(p)$ is valuated.
\item $s'.\hat{p_u} \leq (1+\epsilon) s.\P(p)$ if only $s.\P(p)$ is valuated.
\ei
Given this parameterized $\epsilon$-dominance condition:
\bi
\item Necessary Condition: 
if $s' \succapprox_{\epsilon} s$, the above condition ensures that $s'$ $\epsilon$-dominates $s$ because either: (1) $s'$'s actual performance is better than or equal to $(1+\epsilon)$ times $s$'s best estimate, or 
(2) $s'$'s worst-case estimate is better than or equal to $(1+\epsilon)$ times $s$'s actual performance.
Therefore, if $s' \succapprox_{\epsilon} s$ holds, then $s'\succeq_\epsilon s$ must also hold.
\item Sufficient Condition: 
suppose $s'\succeq_\epsilon s$ but $s' \not\succapprox_{\epsilon} s$. 
This would mean one of the following two conditions is true:
\bi
\item $s'.\P(p) > (1+\epsilon) s.\hat{p_l}$ if only $s'.\P(p)$ is valuated.
\item $s'.\hat{p_u} > (1+\epsilon) s.\P(p)$ if only $s.\P(p)$ is valuated.
\ei
In either case, $s$ could $\epsilon$-dominant $s'$ under certain conditions, which contradicts the assumption that $s'\succeq_\epsilon s$. 
Therefore, $s' \not\succapprox_{\epsilon} s$ must hold to ensure that $s'\succeq_\epsilon s$, making the ``Only If'' direction valid.
\ei
Through this analysis, we have shown that parameterized $\epsilon$-dominance is both necessary and sufficient for standard $\epsilon$-dominance across all cases. 
The conditions ensure that $s'$ remains $\epsilon$-dominant $s$, even 
% in worst-case scenarios.
under rigorous conditions where $s'$ performs at its worst and 
$s$ performs at its best.

\eetitle{Existence of a State $s_p$}
In this part, we show that for any state $s''$ located between $s$ and $s'$ and can be pruned by correlation-based pruning, 
% which will be pruned during \bimodis,  
there must exists another state $s_p$ that can $\epsilon$-dominate $s''$.
This implies that $s''$ cannot be part of the $\epsilon$-Skyline set and can be safely pruned in \bimodis.

The correlation-based pruning strategy prunes $s''$ based on either the exact value $p$ or inferred bounds $[\hat{p_l}, \hat{p_u}]$ for each performance measure $p \in \P$.
The inference assumes that $s''$ lies within a bound in the bounds list. 
Let's define this bound as $[s_1.\P, s_2.\P]$, which means that the relationship $s_2 \succapprox_{\epsilon} s_1$ is validated, 
leading to the conclusion that $s_2 \succapprox_{\epsilon} s''$. 

From part(1), we can get $s_2\succeq_\epsilon s''$ from $s_2 \succapprox_{\epsilon} s''$, meaning that $s_2$ can serve as the state $s_p$. This implies that $s''$ cannot be part of the $\epsilon$-Skyline set and can be safely pruned. 

\eat{ 
From the previous analysis, we've established that $s' \succapprox_{\epsilon} s$ implies $s'\succeq_\epsilon s$. Therefore:
$$s'.\P(p) \leq (1+\epsilon) s.\P(p) \text{, for each } p \in \P$$
Since the measure $p^k \in \P^k$ is monotonically related to the number of ``1''s in labels and $s''$ is located between $s$ and $s'$, we consider the following cases:

\sstab
\textbf{Case 1: Positive Correlation}: If the correlation between $p^k$ and the number of ``1''s in labels is positive, then:
$$s.\P(p^k) \leq s''.\P(p^k) \leq s'.\P(p^k)$$ 
Since $s' \succapprox_{\epsilon} s$, , it follows that:
$$s'.\P(p^k) \leq (1+\epsilon) s.\P(p^k)$$
Given the above inequalities, we can deduce:
$$s'.\P(p^k) \leq (1+\epsilon) s.\P(p^k) \leq (1+\epsilon) s''.\P(p^k)$$
This shows that $s''$ cannot $\epsilon$-dominate $s$ on $p^k$, meaning that $s$ can serve as the state $s_p$. This implies that $s''$ cannot be part of the $\epsilon$-Skyline set and can be safely pruned.

\sstab
\textbf{Case 2: Negative Correlation}: If the correlation between $p^k$ and the number of ``1''s in labels is negative, then:
$$s.\P(p^k) \geq s''.\P(p^k) \geq s'.\P(p^k)$$

\eat{
Suppose, for the sake of contradiction, 
that there does not exist any state $s_p$ in $G_\T$ that $\epsilon$-dominant $s''$ for any $s''$ between $s$ and $s'$. 
This implies that $s''\succeq_\epsilon s_p$ holds on every performance measure $p \in \P$, including $p^k$, which means $s''.\P(p^k) \leq (1+\epsilon) s_p.\P(p^k)$. 
Assume that $p^k$ is monotonically related to another measure $p' \in \P$.
Next, we analyze both positive and negative monotonicity cases to establish whether $s''$ can truly dominate $s_p$:

\sstab
\textbf{Negative Monotonicity.} As $p^k$ increases, $p'$ decreases, and vice versa. Then we have:

\sstab
\textbf{Positive Monotonicity.} As $p^k$ increases, $p'$ also increases, and vice versa. Then we have:
}
}

Together, these findings 
establish Lemma~\ref{lm-prune}, 
validate the correlation-based pruning strategy employed by \bimodis, 
and confirm that it correctly identifies and prunes non-optimal states, 
thereby preserving the correctness and completeness of the $\epsilon$-Skyline set.
\end{proof}
}%%% EAT

%\stitle{Complete version}
We present the details of the 
algorithm \bimodis in Fig.~\ref{alg:com-bimodis}. 

\begin{figure}
\centering
\begin{algorithm}[H]
\caption{\bimodis
}
\label{alg:bimodis:complete}
\begin{algorithmic}[1]
\algtext*{EndFor}
\algtext*{EndIf}
\algtext*{EndWhile}
\algtext*{EndFunction}
\algtext*{EndProcedure}

\State \textbf{Input:} 
    Configuration $C$ = $(s_U, \O, M, T, \E)$, 
    Records $Rec$,
    a constant $\epsilon>0$;
\State \textbf{Output:} 
     $\epsilon$-Skyline set $\mathcal{D}_F$.
     \vspace{1ex}

% \For{\ad in $R_U.target$}
% \State $s_b.L = s_{empty}.L$ \textbf{with} $l$ \textbf{set} to 1
% \EndFor
\State \textbf{Set} $\D_F := \varnothing$, 
    \swb $:= \varnothing$,
    PrunS $:= \varnothing$;
    $s_b = \text{BackSt}(s_E, s_U)$; \label{a2:inis}
\State \textbf{queue} $Q_f := \{(s_U, 0)\}$,  
    \textbf{queue} $Q_b := \{(s_b, 0)\}$;
\State $\D_F^0[pos(s_U)] = \text{CorrFP}(s_U, Rec, \E)$;
\State $\D_F^0[pos(s_b)] = \text{CorrFP}(s_b, Rec, \E)$; \label{a2:inie}

\While{$Q_f \neq \varnothing$, $Q_b \neq \varnothing$ \textbf{and} $Q_f \cap Q_b = \varnothing$}
% Forward
\State $(s', d)= Q_f$.dequeue(), ${\D_F}^{d+1} = {\D_F}^d$; \Comment{Forward Serach} \label{a2:fss}
\For{all $s \in$ \opg($s'$, `F')}
    \State $\P_s = \text{CorrFP}(s, Rec, \E)$; \textbf{set} $\rho_s$  \textbf{with} $\P_s$; \label{a2:ss}
    \If{$pos(s) \in$ PrunS} \textbf{continue;} \EndIf \label{a2:pos_prune}
        \State pruned = False
        \For{bound \textbf{in} SandwBs}
        \State pruned = SandwPrun($\rho_s$, bound, SandwBs)
    \If{pruned} \textbf{break;} \EndIf
    \EndFor
    \State pruned = \upi($\D_F^{d+1}$, PrunS, pos(s), $\epsilon$) \label{a2:se}
    \If{not pruned} $Q_f$.enqueue((s, d+1)) \EndIf \label{a2:enqueue}
\EndFor \label{a2:fse}
% Backward
\State $(s', d)= Q_b$.dequeue(), ${\D_F}^{d+1} = {\D_F}^d$; \Comment{Backward Serach} \label{a2:bss}
\For{all $s \in$ \opg($s'$, `B')}
\State same with line~\ref{a2:ss} to \ref{a2:se} in Forward Search
\eat{
    \State $\P_s = \text{CorrFP}(s, Rec, \E)$; \textbf{set} $\rho_s$ \textbf{with} $\P_s$;
    \If{$\rho_s \in$ PrunS} \textbf{continue;} \EndIf
        \State pruned = False
        \For{bound \textbf{in} \swb}
        \State pruned = \swp($\rho_s$, bound, SandwBs)
    \EndFor
    \If{pruned} \textbf{break;} \EndIf
    \State pruned = UPareto($\D_F^{d+1}$, PrunS, pos[$\rho_s$], $\epsilon$)}
    \If{not pruned} $Q_b$.enqueue((s, d+1)) \EndIf
\EndFor \label{a2:bse}
\EndWhile

\vspace{1ex}
\Procedure{CorrFP}{$s$, $Rec$, $\E$} \label{a2:corrs}
\State Build $G_c$ for measures recorded in $Rec$;
\State StrongRs = GetSR($G_c$)
    \If{s \textbf{in} $Rec$.keys()} \Comment{Case 1: By $Rec$}
        \State $\P_s=Rec[s]$;
        \If{$|valid(\P_s)| \geq 0.8|\P_s|$} 
            \Return $\P_s$;
        \EndIf
    \EndIf

    \For{all missing $p_i^s$ in $\P_s$}  \Comment{Case 2: By $G_{c}$}
        \If{$(p_i, p_j) \in$ StrongRs \textbf{and} $p_j^s \in Rec[s]$} 
        \State \textbf{find} closed $p_j^l$ and $p_j^u$ with $p_j^s$ \textbf{in} $Rec$;
        \State $p_i^s = (p_i^l + p_i^u)/2$
        \EndIf
    \EndFor

    \If{$|valid(\P_s)| < 0.8|\P_s|$}  \Comment{Case 3: By $\E$}
    \State \textbf{fill} missing $p_s \in \P_s$ by invoking $\E$; \textbf{update} $Rec[s] = \P_s$
    \EndIf
\State \Return $\P_s$; 
\EndProcedure \label{a2:corre}
\end{algorithmic}
\end{algorithm}
\vspace{-3ex}
\caption{Complete Version of \bimodis}
\vspace{-3ex}
\label{alg:com-bimodis}
\end{figure}

\subsection{\divmodis}

\stitle{Proof of Lemma~\ref{lemma:div}}
{\em Given $N$ and $\epsilon$, 
\divmodis achieves a $\frac{1}{4}$ approximation for 
diversified \modis, \ie 
(1) it correctly computes a $\epsilon$-Skyline set $D^P_F$
over $N$ valuated datasets, and 
(2) $\kw{div}(D^P_F)\geq \frac{1}{4}\kw{\kw{div}(\D^*_F)}$.}


We here present a detailed analysis
for the lemma~\ref{lemma:div}.

\stitle{Monotone submodularity}.  
We first show that 
the diversification function $\kw{div}(\cdot)$
is a monotone submodular function.
Given a set of datasets $\D_F$, 
we show that for any 
set of datasets 
$Y \subseteq X \subseteq \D_F$,

\tbi
\item $\kw{div}(Y) \leq \kw{div}(X)$; and  
\item $\forall x \in \D_F \setminus X$,
$\kw{div}(X \cup \{x\})-\kw{div}(X) \leq \kw{div}(Y \cup \{x\})-\kw{div}(Y)$.
\ei

\sstab
(1) To see $\kw{div}(Y) \leq \kw{div}(X)$, we have 

\begin{small}
\begin{equation*}
\kw{div}(X) - \kw{div}(Y)
= \sum_{i=1}^{k-1}
\sum_{j=i+1}^{k} \kw{dis}(D_i^X, D_j^X) - \sum_{i=1}^{k-1} \sum_{j=i+1}^{k} \kw{dis}(D_i^Y, D_j^Y) 
\end{equation*}
\end{small}
Given $Y \subseteq X$, 
we have 
\\
\begin{small}
\begin{equation*}
\kw{div}(X) -\kw{div}(Y) = \sum_{D\in X\setminus Y, D\in Y}\kw{dis}(D,D')\geq 0;  
\end{equation*}
\end{small}
%where $D_l \in X \setminus Y$ and 
%\warn{$D_m \in  (X \cap Y) \setminus (X \setminus Y)$}. 
%\warn{if $Y\subseteq X$, would not $(X \cap Y) \setminus (X \setminus Y)$ simply be 
%$Y$? }

% \begin{small}
% \begin{equation*}
% \begin{aligned}
% &\kw{div}(X \cup x)-\kw{div}(X) = 
% \\
% &\sum_{i=1}^{k-2}
% \sum_{j=i+1}^{k-1} \kw{dis}(D_i^X, D_j^X) + 
% \sum_{i=1}^{k-1} \kw{dis}(D_i^X,x)- \sum_{i=1}^{k-1}\sum_{j=i+1}^{k} \kw{dis}(D_i^X, D_j^X)
% \end{aligned}
% \end{equation*}
% \end{small}
\eat{
Given that for any pair of datasets,
\[\kw{dis}(D_i, D_j) = 
\alpha\frac{1-\kw{cos}(s_i.L, s_j.L)}{2}  + 
(1-\alpha)\frac{\kw{euc}(t_i.\P, t_j.\P)}{\kw{euc_{m}}}\]
Since score function $\kw{dis(\cdot)}$ returns a
non-negative value, it ensures $\sum\kw{dis}(D_l,D_m) \geq 0$. In this case, the above analysis proves that $\kw{div}(\cdot)$ is monotone.
}
% \eat{
% \warn{as there is no dataset $x \in X \setminus Y$ which decreases $div(X)$ will be included to diversified $k$-subset $X$.} \warn{This is not English! And it's not clear why $\kw{div}(Y) - \kw{div}(X) \leq 0$ at all. Rewrite. Give exactly what are leftover/remaining terms. 
% Showing $\kw{div}(X)$ - $\kw{div}(Y)\geq 0$ seems easier. } Hence, $\kw{div}(Y) \leq \kw{div}(X)$.
% }
(2) We next show the submodularity of the function $\kw{div}(\cdot)$. 
To simplify the presentation, 
we introduce a notation {\em marginal gain}. 
For any $x \in \D_F \setminus X$ and $Y \subseteq X$, the marginal gain
of diversification score for  $X \cup \{x\}$ and  $Y \cup \{x\}$, denoted as $\margin(X,x)$ and $\margin(Y,x)$, %respectively. 
are defined as: 

% \begin{small}
% \begin{equation*}
% \begin{aligned}
% &\margin(Y,x) - \margin(X,x)  =  \\
% &\sum_{i=1}^{k-2}
% \sum_{j=i+1}^{k-1} \kw{dis}(D_i^Y, D_j^Y)
% - \sum_{i=1}^{k-2}
% \sum_{j=i+1}^{k-1} \kw{dis}(D_i^X, D_j^X)\\
% &+\\
% &\sum_{i=1}^{k-1} \kw{dis}(D_i^Y,x)- \sum_{i=1}^{k-1} \kw{dis}(D_i^X,x)\\
% &+\\
% &\sum_{i=1}^{k-1}\sum_{j=i+1}^{k} \kw{dis}(D_i^X, D_j^X) - \sum_{i=1}^{k-1}\sum_{j=i+1}^{k} \kw{dis}(D_i^Y, D_j^Y)
% \end{aligned}
% \end{equation*}
% \end{small}

\eat{
By definition, given any pair of datasets,
the diversification score
is computed by a linear combination 
of accumulated euclidian distance from the historical
performance and pairwise cosine similarities. 
Hence, the marginal gain can be calculated as
the gain of euclidian distance from the historical
performance (denoted as $\Delta\kw{relevance}$) plus the gain of the diversity (denoted as $\Delta\kw{diversity}$) of datasets,respectively.
In this case, we can have
}
% \begin{small}
% \begin{equation*}
% \begin{aligned}
% &\margin(X,x)  = 
% &\Delta \kw{diversity}(X,x) + \Delta \kw{relevance}(X,x)\\
% \end{aligned}
% \end{equation*}
% \end{small}
% \begin{small}
% \begin{equation*}
% \begin{aligned}
% &\margin(Y,x)  = 
% &\Delta \kw{diversity}(Y,x) + \Delta \kw{relevance}(Y,x)\\
% \end{aligned}
% \end{equation*}
% \end{small}
% where

\begin{small}
\begin{equation*}
\begin{aligned}
&\margin(X,x) = \\
&\sum_{i=1}^{k-1} \sum_{j=i+1}^{k} \alpha\frac{1-\kw{cos}(s_i.L, s_j.L)}{2} - \sum_{i=1}^{k-1} \sum_{j=i+1}^{k} \alpha\frac{1-\kw{cos}(s_l.L, s_m.L)}{2}\\
&+ \\
&\sum_{i=1}^{k-1} \sum_{j=i+1}^{k}(1-\alpha)\frac{\kw{euc}(t_i.\P, t_j.\P)}{\kw{euc_{m}}} - \sum_{i=1}^{k-1} \sum_{j=i+1}^{k}(1-\alpha)\frac{\kw{euc}(t_l.\P, t_m.\P)}{\kw{euc_{m}}}\\
\end{aligned}
\end{equation*}
\end{small}
where $D_i, D_j \in {X \cup \{x\}}$ and $D_l, D_m \in X$.


\begin{small}
\begin{equation*}
\begin{aligned}
&\margin(Y,x) = \\
&\sum_{i=1}^{k-1} \sum_{j=i+1}^{k} \alpha\frac{1-\kw{cos}(s_i.L, s_j.L)}{2} - \sum_{i=1}^{k-1} \sum_{j=i+1}^{k} \alpha\frac{1-\kw{cos}(s_l.L, s_m.L)}{2}\\
&+ \\
&\sum_{i=1}^{k-1} \sum_{j=i+1}^{k}(1-\alpha)\frac{\kw{euc}(t_i.\P, t_j.\P)}{\kw{euc_{m}}} - \sum_{i=1}^{k-1} \sum_{j=i+1}^{k}(1-\alpha)\frac{\kw{euc}(t_l.\P, t_m.\P)}{\kw{euc_{m}}}\\
\end{aligned}
\end{equation*}
\end{small}
where $D_i, D_j \in Y \cup x$ and $D_l, D_m \in Y$.

\eat{
\begin{mdframed}
\textbf{Condition:}
We assume that
the size of the $\epsilon$-Skyline sets are at least $k$, $|\kw{div}(D^P_F)| >= k$. 
For any pair of $\epsilon$-Skyline sets $X, Y$ such that $Y \subseteq X$ and $|Y| \geq k$, the  function $\kw{div}(\cdot)$
computes the diversification scores over
the same subset of datasets with size $k$.
\end{mdframed}
}

%We assume that 
%the size of the $\epsilon$-Skyline sets are at least $k$, 
In our problem,we only consider $\epsilon$-Skyline sets with size 
at most $k$. With this condition, we observe that
given the dataset $x$, $\margin(Y,x)$ and 
$\margin(X,x)$ measure the marginal gain of 
diversification scores by replacing
a dataset $x' \in Y$ with $x$ and
$x' \in X$ with $x$. $\margin(Y,x)$ and 
$\margin(X,x)$ measure the margin gain by replacing $x$ with a same dataset $x'$ 
over a same $\epsilon$-Skyline set with size $k$.
In this case, we can have
% \begin{small}
% \begin{equation*}
% \begin{aligned}
% &\Delta \kw{diversity}(X,x) = \Delta \kw{diversity}(Y,x)\\
% &\Delta \kw{relevance}(X,x) = \Delta \kw{relevance}(Y,x)\\
% \end{aligned}
% \end{equation*}
% \end{small}
% Hence, 
\begin{small}
\begin{equation*}
\begin{aligned}
\margin(X,x)  = \margin(Y,x)
\end{aligned}
\end{equation*}
\end{small}
due to \divmodis replace the
same dataset $x'$ that are in $X$ and $Y$. Thus, we can see that
the marginal gain of $X$ is no larger than marginal gain of $Y$
by including $x$.
This analysis completes the proof of diversification function $\kw{div}(\cdot)$ is a monotone submodular function.



% \begin{equation*}
% \begin{aligned}
% &\margin(X,x) =  \\
% &\Delta \kw{diversity}(X,x) + \Delta \kw{relevance}(X,x)=\\ 
% &\kw{diversityGain(X,x)}-\kw{divesityLoss(X,x)}\\
% &+ \\
% &\kw{relevanceGain(X,x)}-\kw{relevanceLoss(X,x)} 
% \end{aligned}
% \end{equation*}
 

\eat{
\warn{
To compute $\kw{\kw{div}(\D^*_F)}$,
$\divmodis$ iterativly adds the dataset
$d^*$ that maximizes the diversification score: 
WRONG PROOF. Show the function is submodular - 
it has nothing to do with how an algorithm optimize it!}
\begin{equation*}
d^* = \argmax_{d \in D \setminus D_F} \kw{div}(D_F \cup d^*) - \kw{div}(D_F)
\end{equation*} 
since $Y \subseteq X$, $Y$ selects 
the $d$ to the $D_F$ at the earlier iteration
than $X$. Therefore, for any $ x \in \D_F
\setminus X$,
$\kw{div}(X \cup x)-\kw{div}(X) \leq
\kw{div}(Y \cup x)-\kw{div}(Y)$.
}

\stitle{Approximability}. 
We next prove that \divmodis ensures a $\frac{1}{4}$-approximation of diversified size-$k$ Skyline set.
We verify this by proving an invariant that 
the approximation holds  
for any size-$k$ $\epsilon$-Skyline set $D^P_F$ 
generated at every level $i$.
By integrating a greedy selection and replacement policy, \divmodis keeps a 
-set with the most diverse and representative datasets to mitigate the biases in the Skyline set at each level.
Consider a set of datasets $D^{i-1}_F$
at level $i$, and a new batch of
datasets $D^i_F$ arrives. \divmodis
aims to maintain the set of datasets 
$D^i_F$ such that, at level $i$, $|D^i_F| \leq k$, and $\kw{div}(D^i_F)$ is maximized. At any level $i$,
\divmodis approximates the global 
optimal solution upon the newly generated datasets.
Consider the global optimal solution 
at level $i$, over $D_F^i$ as $D_F^{*P}$,
we can show that \divmodis maintains
$D^P_F$ at any level $i$ by solving 
a streaming submodular maximization
problem~\cite{chakrabarti2015submodular}.

\eetitle{Reduction}.
We show there exists an approximation at any level by a 
reduction from the diversification phase of \modis problem 
 to {\em stream submodular maximization problem}~\cite{badanidiyuru2014streaming,chakrabarti2015submodular}.
 Given a streaming of elements $E = \{e_0, \ldots e_m\}$, an integer $k$, and a submodular 
score function $f$, it
computes a set of elements $S$ with size $k$ 
with maximized $f(S)$.
Given the $\epsilon$-Skyline set $\D_F^P$, and integer $k$, the diversification of \modis problem
aims to compute an $\epsilon$-Skyline set ${\D_F}^{P}$ such that (1) $|{\D_F}^{P}| \leq k$
and $\kw{div}({\D_F}^{P})$ is maximized.
Given an instance of diversification of \modis
problem at any level $i$, we construct 
an instance of stream submodular maximization problem by setting (1) $f = \kw{div}$; 
(2) $E =  D_F^i$; (3) integer $k$ is equal
to the value of k in the instance of diversification of \modis.

\eetitle{Correctness}.\divmodis approximates
$D_F^{*P}$ with a ratio $\frac{1}{4}$ follows a greedy selection and replacement policy that integrates the "replace" strategy given the  ${\D_F}^{i}$ at level $i$.
\divmodis always terminates
when no datasets are generated at level $i$ by procedure \upi (See lines 1-3 in Fig.~\ref{alg:divmodis}).


\eetitle{Approximation}.
$\D^{P}_F$ approximates $\D^{*P}_F$ 
with a ratio $\frac{1}{4}$ 
when terminates at level $i$. 
\divmodis exploits the greedy selection 
as in~\cite{chakrabarti2015submodular} but specifies diversification function $\kw{div}(\D^{P}_F)$ to maintain the $\epsilon$-Skyline set of datasets and replaces newly arrived datasets whenever possible.  
It returns the $\epsilon$-Skyline set of datasets those corresponding 
elements in $E$ are selected by the instance stream submodular maximization problem. This ensures a $\frac{1}{4}$ 
approximability by this consistent construction from the solution 
for stream submodular maximization.

The above analysis completes the proof of Lemma~\ref{lemma:div}.

\begin{figure}[tb!]
\centerline{\includegraphics[width =0.5\textwidth]{fig/Exp2/effi_t53}}
\centering
\vspace{-1ex}
\caption{Efficiency Analysis on $T_5$ and $T_3$}
\label{fig:effict53}
\end{figure}

\begin{figure}[tb!]
\centerline{\includegraphics[width =0.5\textwidth]{fig/Exp2/scalability5}}
\centering
\caption{Scalability on $T_5$}
\label{fig:scala5}
\end{figure}

\begin{table*}[tb!]
\customsize
\centering
\renewcommand{\arraystretch}{1.1}
\begin{small}
\begin{tabular}{|c|c|c|c|c|c|c||c|c|c|c|}
\hline
$T_1$: Movie & Original & \metam & \metammo & \starmie & \sklearn & \ho & \apxmodis & \nomodis & \bimodis & \divmodis \\ \hline
$p_{Acc}$ & 0.8560 & 0.8743 & 0.8676 & 0.8606 & 0.8285 & 0.8545 & 0.9291 & \textbf{0.9874} & \ul{\textit{0.9755}} & 0.9427 \\ \hline
$p_{Train}$ & 1.4775 & 1.6276 & 1.1785 & 1.2643 & \textbf{0.6028} & 0.9692 & 0.9947 & 0.8766 & \ul{\textit{0.8027}} & 0.8803  \\ \hline
$p_{Fsc}$ & 0.0824 & 0.0497 & 0.0801 & 0.1286 & 0.7392 & 0.3110 & 0.6011 & \ul{\textit{0.7202}} & \textbf{0.9240} & 0.8010\\ \hline
$p_{MI}$ & 0.0538 & 0.0344 & 0.0522 & 0.1072 & 0.3921 & 0.1759  & \textbf{0.4178} & 0.3377 & 0.3839 & \ul{\textit{0.4165}}\\ \hline
% $p_{VIF}$ & 1.5831 & 1.9669 & 1.9782 & 1.2980 & 1.6742 & 1.5096 & \ul{\textbf{2.4092}} & \ul{\textit{2.1331}} & 1.7688 & 2.0188 \\ \hline
Output Size  & (3264, 10) & (3264, 11) & (3264, 11) & (3264, 23) & (3264, 3) & (3264, 8) & (2958, 9) & (1980, 12) & (1835, 11) & (2176, 10) \\ \hline

\hline
\hline

$T_3$: Avocado & Original & \metam & \metammo & \starmie & \sklearn & \ho & \apxmodis & \nomodis & \bimodis & \divmodis \\ \hline
MSE & 0.0428 & 0.0392 & 0.0312 & 0.036152 & 0.050903 & 0.0442 & 0.029769 & \textbf{0.022821} & \ul{0.027511} & \ul{0.027511} \\
\hline
MAE & 0.1561 & 0.1497 & 0.1452 & 0.145259 & 0.173676 & 0.1592 & 0.127916 & \textbf{0.115326} & \ul{0.123200} & \ul{0.123200} \\
\hline
Training Time & 0.0280 & 0.0178 & 0.0350 & 0.043600 & 0.008618 & 0.0156 & 0.006516 & \textbf{0.003293} & \ul{0.004366} & \ul{0.004366} \\
\hline
Output Size & (9999, 11) & (9999, 12) & (9999, 12) & (9999, 12) & (9999, 3) & (9999, 5) & (1589, 10) & (817, 5) & (1310, 9) & (1310, 9) \\
\hline

\end{tabular}
\end{small}
\caption{Comparison of Data Discovery Algorithms in Multi-Objective Setting ($T_1$, $T_3$)}
\label{tab:comparison2}
\end{table*}



\eat{
\stitle{Proof of Proposition~\ref{prop-simulate}} 
{\em A data discovery system 
$\T$ can be configured to express (1) data augmentation, 
and (2) feature selection. }
\begin{proof}
\end{proof}

\stitle{Proof of Lemma~\ref{prop-non-blocking}}
{\em The operators of type $\oplus$ and 
$\ominus$ are non-blocking. % in practice. }
}
\begin{proof}
\end{proof}

\stitle{Proof of Proposition~\ref{prop-equivalent}}
{\em Given a set of operators $\O$ with non-blocking 
operators $\oplus$ and $\ominus$,  
%and transitions $\delta$, 
Type 3, Type 4, and Type 5 systems 
have the same expressiveness. 
\ie $L(\T_3)$ = $L(\T_4)$ = 
$L(\T_5)$.} 
\begin{proof}
\end{proof}


\stitle{Proof of Proposition~\ref{thm-cr}}
{\em Given a configuration $C$ with non-blocking operators 
of types $\{\oplus, \ominus\}$, 
and a data discovery 
system $\T$, the running of 
$\T$ is terminating and confluent. }
\begin{proof}
\end{proof}
}

\section{Additional Experiments}
\label{sec:add:exp}

We have performed more complementary 
experimental studies.

\stitle{Efficiency}. 
\textbf{Fig.~\ref{fig:effict53}
(a, b)} evaluates the 
efficiency of \modis algorithms for task $T5$ on generating graph 
data for the link regression task. The observation is consistent with our findings for their counterparts over tabular data. In particular, \bimodis is quite feasible for generating graph data for GNN-based link regression task, with around $20$ seconds in all settings, and consistently 
outperforms other \modis algorithms.
\textbf{Fig.~\ref{fig:effict53}
(c, d)} presents the efficiency results for task $T_3$, which involves avocado price prediction. Similar to other tasks, \bimodis demonstrates superior efficiency, maintaining significantly lower search times compared to other methods. The results reinforce the scalability and practicality of \bimodis across diverse datasets and tasks, including both graph-based and tabular data.

\stitle{Scalability}.
\textbf{Fig.~\ref{fig:scala5}} presents the scalability test results for $T_5$. 
With a universal graph size of $(7925, 34)$, we performed $k$-means clustering on edges, setting $5$ as the minimum number of clusters, $30$ as the maximum, and identifying $13$ as the optimal number of clusters based on performance. For node features, we leveraged the graph's structure to reduce the input feature space from $34$ to $10$ by aggregating attributes from similar types of relations, such as combining multiple training records of an ML model, while preserving all augmented information.
Across all settings, methods applied bi-directional search (\bimodis, \nomodis, and \divmodis) consistently achieve superior efficiency, handling both increasing attributes and active domain sizes effectively. In contrast, \apxmodis exhibits slower performance as $\vert A \vert$ and
$\vert \ad \vert$ grows, highlighting the scalability of the bi-directional search strategy in managing large and complex graph datasets.

\stitle{Effectiveness}. The effectiveness results for $T_1$ and $T_3$ are reported in \textbf{Table~\ref{tab:comparison2}}, where we select the best results from the Skyline set based on the first metric for each task. 
These results align with those observed for other tasks, consistently showing that \modis methods outperform baseline approaches in most cases. Notably, \nomodis and \bimodis secure the first and second positions across the majority of metrics.

\stitle{Sensitivety analysis}.
\textbf{Fig.~\ref{fig:modsnet-sense}}
reports the impact of critical factors including the maximum 
length of paths, and $\epsilon$ (as in $\epsilon$-Skyline set) to 
the accuracy measures. The larger the ``Percentage Change'' is, the better the generated Skyline set can improve the prformance of the input model. We found that all the \modis algorithms benefit from larger maximum length and 
smaller $\epsilon$ in terms of percentage of accuracy improvement. This is consistent with our observation over the tests that report absolute 
accuracy measures. Moreover, \modis algorithms are relatively more sensitive to the maximum length, compared with the changes to $\epsilon$.

\begin{figure*}[h]
\centerline{\includegraphics[width =\textwidth]{fig/parameter}}
\centering
\caption{Sensitivity Analysis for Parameters on $T_5$}
\label{fig:modsnet-sense}
\end{figure*}
