\section{Introduction}
\label{sec:intro}

High-quality machine learning (ML) models have become criticale assets for various domain sciences research. 
A routine task in data-driven domain sciences is to prepare datasets that can be used to improve such data science models. 
%In other words, {\em how to create new data to improve the overall (expected) performance of a model?} 
%Crowdsourced platforms like HuggingFace~\cite{HuggingFaceAI} facilitate access to datasets and models, 
Data augmentation~\cite{roh2019survey} and feature selection~\cite{li2017feature} have been 
studied to suggest data for ML models~\cite{doan2012principles}. 
Nevertheless, they 
typically generate data by favoring a pre-defined,  
single performance goal, such as 
data completeness or feature importance. 
Such data may be biased and not very useful to 
actually improve the model performance, 
and moreover, fall short at 
addressing multiple user-defined ML performance  
measures (\eg expected accuracy, training cost). 
Such need 
is evident in multi-variable experiment optimization~\cite{konakovic2020diversity, low2023evolution, paleyes2022challenges}, 
%~\cite{galhotra2023metam}, 
feature selection~\cite{li2017feature}, and
AI benchmarking~\cite{donyavi2020diverse}, among others. 

Discovering datasets that can improve 
a model 
%how to construct (new) datasets 
%that can improve a model 
over {\em multiple} user-defined 
performance measures remains 
to be desirable yet less studied 
issue. Consider the following real-world example. 

%\vspace{.5ex}

%However, these methods are not fully optimized to improve data science models directly, especially in terms of {\em multiple} performance measures.

\eat{
Data-driven analytical pipelines 
with data science models are routinely processed in a wide range of applications. Such pipelines rely on high-quality data science (machine learning) models. 
Among the challenges is the 
effective selection and creation of datasets  
that lead to 
%effective training and validation of 
high-quality models. 
In other words, {\em how to create 
new data to improve 
the overall (expected) performance of a model?}
}

%Moreover, %in practice, 
%the model $M$ 
%is evaluated by {\em multiple} 
%(user-specified) performance measures 
%or constraints 
%such as training cost, processing 
%cost, accuracy, inference cost, memory consumption, etc. 
%It is desirable to 
%suggest a dataset, 
%over which the model has 
%{\em simultaneously} 
%desirable performances 
%in terms of {\em all} 
%the measures. 


\eat{
Crowdsourced data platforms such as HuggingFace~\cite{HuggingFaceAI} 
provide portals to make datasets and 
models available. Data augmentation~\cite{roh2019survey} and feature selection~\cite{li2017feature} 
have been also separately studied to 
improve machine learning, by 
carefully choosing useful data sources and feature space, 
respectively. Data integration has been adopted as an enabling technique to create new data for data 
augmentation~\cite{doan2012principles}. Nevertheless, these approaches are not optimized %to satisfy the practical need 
to improve data science models 
as first-class citizens, 
and moreover, in terms of 
 {\em multiple} performance measures. 
%for the practical needs to 
%optimize data analysis with 
%multiple performance evaluation metrics. 
\eat{ 
There is still a lack of effective solutions that 
(1) %interact data discovery and 
%integration that 
can suggest data that directly responds to 
a model as a ``query'' 
(\ie ``search data with a model''); 
and (2) in particular,  
 improve the expected performances 
 of the model in the presence of  %optimizing 
 {\em multiple} performance measures. 
 }
 }

\begin{example}
\label{exa-motivation}
To assess the impact and causes of harmful algal blooms (HABs) in a lake, a research team aims to forecast the chlorophyll-a index (CI-index), a key measure of algal blooms. 
The team has gathered over $50$ factors
% To understand the %short-term 
% impact of harmful algal bloom (HABs)  
% of a lake and its reason, a research team needs a model that can forecast the chlorophyll-a index (CI-index), a critical algal bloom quantification index, in the short term. 
% The team 
% has collected a set of tables that involve more than $50$ factors 
(\eg fertilizer, water quality, weather)
%and their historical readings 
of upstream rivers and watershed 
systems, % of the lake. 
%These values are distributed 
%in a dozen of data tables. 
and trained 
% The team has already initialized 
a random forest (RF) %Artificial Neural Networks (ANNs) 
with a small, regional dataset.
The team wishes to find new 
data with important spatiotemporal and chemical 
attributes, to generalize the RF model. In particular,  
the model is expected to perform well 
over such dataset  
in terms of three performance measures: 
%including two accuracy measures:  
root mean square error ($RMSE$),  $R^2$ test, for ``Level 2 bloom'' CI-index, 
%forecasting time, 
and training time cost. 
Desirably, 
the data generation process 
can inform {\em what} are crucial features to inspect,  
% (2) understand {\em what} 
% are  important features %and their values 
% to be considered to 
% achieve such improvement; 
track {\em where} 
the feature values are from, and {\em how}  
they are integrated from the data sources.

\vspace{.5ex}
%\label{exa-capability}
The research team may issue
a {\em skyline query}~\cite{chomicki2013skyline} that requests: 

\begin{figure}[tb!]
\vspace{-1ex}
\centerline{\includegraphics[width =0.45\textwidth]{fig/motivation}}
\centering
\vspace{-1ex}
\caption{
Data generation for CI index prediction addressing  
multiple user-defined ML performance criteria, in order to improve an input ML model.  %Our framework automatically suggests new datasets that 
%achieved the desirable metrics in a few minutes.
% Discover and create dataset to improve the performance of a 
% trained random forest (RF) for 
% predicting CI index in both accuracy and training cost. %Manual tuning with 
% %cross-validation and feature 
% %selection tools find 10 features 
% %from more than 50 after several weeks. 
% Our framework automatically  
% suggested a new dataset 
% with 8 features 
% with desirable model accuracy and training cost, in a few minutes.
}
\label{fig:motivation}
\vspace{-2ex}
\end{figure}


\vspace{-1ex}
\begin{center}
\fbox{
\begin{minipage}{.46\textwidth}
``{\em Generate a \underline{dataset} for which our \underline{random forest model} for predicting 
`Level 2 bloom' CI-index
is expected to have a 
\underline{RMSE} below $0.3$, \underline{$R^2$} score at least $0.7$, and incur a \underline{training cost} in 5 minutes?}'' 
\end{minipage}
}
\end{center}

Here, the thresholds  
``$0.3$'', ``$0.7$'', and ``$5$ minutes'' 
are set based on historical  
performance of the  
RF model over 
a data sample. 
%with random  
%model parameters. 

One may apply data integration, or 
perform feature engineering to 
refine existing datasets with important 
features. Nevertheless, these 
methods often fall short at consistently 
generate data towards optimizing 
user-defined ML performance, leaving 
alone the needs for addressing multiple measures 
\eg accuracy and 
training cost. 

Another approach is to 
introduce a utility function 
as a linear weighted sum 
of multiple measures. This 
turns the need into a single objective.  
However, achieving both high accuracy and 
low training cost %in CI index prediction 
can be ``conflicting''; moreover, a best dataset 
that optimizes such utility function may not 
necessarily satisfy the expected bounds as posed  
for each measure in the query. 

%%%%%%%%%%
Ideally, a data generation process should provide 
a dataset that 
ensures the model achieves best expected performance on at least one measure, with compromisingly good performance on the rest, and all satisfying the user-defined bounds if any. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\eat{
With an excessive 
number of trials that repeats  
a pipeline of (manual) hyper-parameter tuning,  
feature selection, data integration, 
cross-validation, and inference tests, 
%%%%%%%%%%%%%
% Moreover, In fact, 
the team has %manually  
eventually verified that only two datasets (a basin area and 
an upstream river) suffices to 
ensure the desired performance of the model, among 
which only ``\texttt{Nitrogen}'' and ``\texttt{Phosphorus}'' 
 jointly determines CI-index,  
and
not all records are needed.  
For short-term prediction in 2023, 
it suffices to use the seasonal ``\textit{Spring}'' and the years ``\textit{2013}'' and ``\textit{2015}'' due to similar high nutrition loads~\cite{ai2023short}. Nevertheless, 
it took a substantial time to construct such a dataset.
}
\end{example}

%%%%%%%%%%%%%%%%%
% reduced for simplicity
%%%%%%%%%%%%%%%%%
\eat{
A typical process involves an excessive 
number of trials that repeats  
a pipeline of (manual) hyper-parameter tuning,  
feature selection, data integration, 
cross-validation, and inference tests. It turns out that 
 only $8$ key features 
with certain ranges of values are 
needed to achieve a regression 
with desirable accuracy at $89.6\%$
(quantified by RMSE and $R^2$ test). 
This process took the team 8 weeks to get the desired model and the following 
observations. 

\sstab
(1) Not all the water data tables 
from every upstream river system are needed, but only 
two -  a basin area and 
an upstream river. 

\sstab
(2) A single physicochemical attribute 
``\texttt{Phosphorus}'' alone does not determine 
C1-index well for recent years, but ``\texttt{Nitrogen}'' and ``\texttt{Phosphorus}'' 
 jointly determines CI-index, as an 
emerging pattern. 

\sstab
(3) The original data contains, for both columns ``\texttt{Nitrogen}'' and ``\texttt{Phosphorus}'', 
the values over the past $11$ years. However, for an accurate short-term prediction in 2023, 
the fraction of the seasonal ``\textit{Spring}'' data and in particular years ``\textit{2013}'' and ``\textit{2015}'' 
are important, due to high nutrition loads.
}

The above example calls for data generation approaches that can respond to 
the question by providing ``skyline'' 
datasets that can address multiple ML performance measures. 
More formally, given a query that specifies an input data science model $M$, 
%, an initial 
%table %of data attributes (features) $D_o$, 
a set of source tables $\D$ = $\{D_1, \ldots D_n\}$, and a set of user-defined performance measures $\P$ (\eg accuracy, training time), our task is to generate a new table from $\D$, over which the expected performances of $M$ simultaneously reaches desirable goals for all measures in $\P$. 
As remarked earlier, traditional data 
integration and feature engineering 
with a pre-defined, single optimization 
objective falls short of generating data 
for such needs. 

%for data discovery that 
%simultaneously satisfy 
%multiple performance goals. 

\eat{
As remarked earlier, manipulating datasets from 
scratch is expensive. One may apply 
data integration to generate tables 
from ``joinable'' tables, and verify 
them by testing the performance 
of the model.  Nevertheless, this 
incurs excessive join operations, leaving alone the cost from model inferences, 
fine-tuning, and %~\cite{lu2018accelerating},  
additional in-lab validation 
(\eg experiments, simulations)~\cite{paleyes2022challenges}. 
Moreover, conventional data 
integration optimizes data completeness 
rather than model performance, 
leaving alone the need 
for data discovery that 
simultaneously satisfy 
multiple performance goals. 
}

Moreover, 
a desirable data generation process 
should 
(1) declaratively produce  
such a data by simple, primitive operators 
that are well supported by established  
query engines and 
data systems, 
%columns and rows from 
%raw table sources, and 
%prune irrelevant data; 
(2) perform data discovery 
without expensive 
model inference and validation; and 
(3) ensure quality guarantees on 
the resulting skyline dataset, 
for multiple 
performance measures. 
In addition, the generation should be 
efficient. This remains a challenging 
issue, considering 
large-scale data sources and 
the space of new datasets that 
can be generated from them. 

%This reduces overhead for downstream tasks. 
%Both are {\em new} challenges 
%that are not well-addressed by 
%existing data discovery methods for 
%improving data-driven pipelines. 
% Beyond data augmentation through column joining, we also aim to explore proper {\em selection conditions} and prune interfering rows, such as minimally contributing, causing imbalances, or containing outliers and inconsistencies.

%fine-tuning effort.

\eat{\warn{give an example data script that trains a model, an initial training/testing dataset, and a search request: what are additional features and values available for training so it performs better over 
the test dataset in terms of 
high accuracy *and* low learning  cost?  Here the input table may contain proper annotated data - in a supervised/semi-supervised case. Make it a simple 
case-like classification. 
Perhaps make a second search request 
on both accuracy and memory cost; highlight more examples of such trade-offs.}
}




\eat{This calls for an automated data discovery process that can:

\begin{itemize}
\item
suggests datasets from available ones 
to improve data science models 
for {\em multiple measures} at scale,
\item 
provides an integrative solution 
to suggest ``what'' datasets to choose from multiple resources and ``how'' to integrate the selected datasets, and
\item 
improves multiple model performances while avoiding actual, potentially expensive 
retraining and inference. 
\end{itemize}
}

%One may consider developing optimization 
%algorithms to compute solutions that are {\em Pareto optimal}, which aim to find a plan to integrate 
%datasets that conform to an optimal Skyline set. 

 
\eat{Goal-driven data integration 
provides guided design for 
this process, yet may only 
response to optimizing 
a single performance metric 
rather than multi-objective 
improvement for specific 
models. }

\eat{
\begin{example}
\label{exa-capability}
The research team may issue
a query that asks: 
``{\em What are proper \underline{datasets} for which our \underline{random forest model} 
can have an improved performance in forecasting 
`Level 2 bloom' CI-index, with 
expected accuracy in terms of 
\underline{RMSE} at most $0.3$, \underline{$R^2$} score 
at least $0.7$, and \underline{training cost} less than 5 minutes?}'' 

At first glance, this seems ``contradicting'' due to the trade-off between training cost and 
model accuracy for the measures on the CI index. 
Nevertheless, a data discovery process can still compromise to suggest one or more datasets, over which the model 
can ``showcase'' of desired accuracy for at least one metric (\eg a $\geq 86\%$ accuracy for forecasting Level 2 bloom) and demonstrate comparable performance to the rest 
two metrics. This encourages us to seek for 
data discovery that can (1) {\em combine} data integration 
and multi-objective model evaluation, 
(2) pursue {\em multi-objective} 
optimization. Both are {\em new} challenges 
that are not well-addressed by 
existing data discovery methods for 
improving data-driven pipelines. 

Moreover, the computation should not  
be limited to data augmentation by joining 
columns, but also to 
explore proper {\em selection conditions} and 
prune irrelevant rows. This reduces 
unnecessary overhead for downstream 
fine-tuning effort of the model. 
\end{example}
}

\eat{
methods require the 
computation of all possible 
choices of datasets, which is %time consuming and 
not scalable. Moreover, the %inference 
evaluation of $M$ over an 
integrated dataset to know its actual performance 
alone may already be expensive, 
especially when $M$  
has high model complexity or with expensive user-defined functions~\cite{lu2018accelerating}, 
or involves nontrivial in-lab experiments 
or simulations as seen in scientific 
workflows~\cite{paleyes2022challenges}. Hence  
it is also desirable to 
return {\em size-bounded} 
datasets for further validation. %Indeed, 
%the size of the dataset often 
%indicates the actual learning or inference 
%costs for downstream fine-tuning of %$M$. 
}

%\stitle{Contribution}. 
% This paper 
%We introduce \textbf{\modis}, a performance-aware, multi-objective data discovery 
%framework for data science models. 
%problem. process to 
%with feasible 
%algorithms that ensures provable 
%quality guarantees. 
%
%cost-effective approach,  
%to address the above two challenges. 
%Our main contributions are as follows. 
%\begin{itemize}

\vspace{.5ex}
\stitle{Contribution}. 
We introduce \modis, a 
multi-objective data discovery 
framework. \modis 
%fully automated data discovery process that 
interacts data integration 
and ML model performance estimation 
to pursue a {\em multi-objective} 
data discovery paradigm. We summarize 
our contributions as follows. 

\stab
(1) 
%In response to the need of 
%suggesting data with both columns and rows, 
We provide a formal computation model for 
the skyline data generation process in 
terms of a 
finite state transducer (FST). 
An FST extends finite automata by associating 
an output artifact that undergoes modifications 
via sequences of state transitions. 
%It is a state  that maps inputs to outputs through a sequence of states and transitions. It extends finite automata by associating outputs with transitions. 
The formal model is 
equipped with (1) simple and primitive 
operators, 
%reduction and augmentation operators, 
and (2) a model performance oracle %estimator 
(Section~\ref{sec-system}). 
We use FST as an abstract 
tool to describe 
data generation algorithms and 
perform formal analysis to 
verify costs and 
provable quality guarantees. 
%We justify the computation with a 
%confluence property, which  
%\mengying{[MY: we didn't talk about it in this version]}. 
%This  
%ensures the termination 
%of the process. %that 
%eventually terminates  
%at a single result. 
\eat{
We study the expressiveness  
and property of the 
system. 
We show that the 
system resembles 
of data integration 
and feature selection. 
Better still, its 
computation demonstrates 
confluence property and 
a Church Russel property. }

\sstab
(2) Based on the 
formal model, % of a data discovery system, 
%we 
%introduce quality measures for 
%multi-objective data discovery 
%(Section~\ref{sec-problem}) 
%in terms of Pareto optimality  
%determined by a class of 
%performance dominance relation. 
%formulate our data discovery as a 
we
introduce the skyline data generation 
problem, 
in terms of Pareto optimality 
%with the computation 
%of the data discovery process, 
%which performs sequential operators 
%to update an initial dataset 
(Section~\ref{sec-problem}) . 
The goal is to generate  
a skyline set of 
datasets, ensuring each has at least one 
performance measure where the model's expected performance is no worse than any other dataset.
%at least  
%one measure 
%that is 
%by any other 
%datasets. 
%We establish the 
%hardness 
%of the problem. 
While the problem is intractable, 
we present a fixed-parameter 
tractable result, for a polynomially bounded dataset exploration space from the running graph of an FST process, 
and a fixed measures set $\P$. 
%when the total number of 
%possible datasets is polynomially 
%bounded by the size of input. 

\vspace{.5ex}
Based on the above formulation, we provide 
three feasible algorithms to generate 
skyline datasets. 

\sstab 
(3) Our first algorithm provides an approximation 
on Pareto optimal datasets by 
exploring and verifying a bounded number of 
datasets that can be generated from 
data sources %for the problem 
(Section~\ref{sec-apxmodis}). 
%under a fixed parameter specification. 
The algorithm adopts a 
``reduce-from-universal'' strategy 
to dynamically drop  
values from a universal dataset towards 
a Pareto optimal set of tables. 
We show that this algorithm 
approximates Skyline set within 
a factor of $(1+\epsilon)$ 
for all performance metric, 
and ensures exact dominance 
for at least one measure.
In addition, we present a 
special case with a 
fully polynomial time 
approximation. 
%\mengying{Sould we change the Paretos here to Skyline?}
 %We also develop to reduce unnecessary 
%re-training and inference costs. 
%performance verification; 
%% will model freezing also a possible tech here? 
% we freeze the model parameters and incrementally do 
% the inference test 

\sstab 
(4) Our second 
algorithm further 
reduces unnecessary 
computation. It follows a bi-directional scheme to 
prune unpromising
data, and leverages   
a correlation analysis 
of the performance metrics 
to early terminate the 
search 
(Section~\ref{sec-bimodis}). 

\eat{
The first exploits 
known correlations of the 
performance metrics 
to perform early termination 
and pruning, via a bi-directional 
search scheme. }

\sstab 
(5)
% (Section~\ref{sec-bimodis}). 
Moreover, we  
introduce a diversification 
algorithm to mitigate the  
impact of data bias (Section~\ref{sec-divmodis}).  
% (Section~\ref{sec-divmodis}). 
We show that the algorithm achieves a  
$\frac{1}{4}$-approximation to an optimal 
diversified skyline dataset among 
all verified $(1+\epsilon)$ counterparts. 
%, with provable guarantees 
%%%%%%%%%%%
% which can be natually the lower and upperbound 
% estimation of the metrics; 
%%%%%%%%%%%
%of the output datasets. 
%Moreover, 
%we show that 
%the size of returned datasets 
%is bounded. 
%Our algorithm 
%uses a 
%%%%%%%%%%%%
% which should be a function of 
% k, \epsilon, and |D_m| -- the largest dataset. 
%%%%%%%%%%%%

\eat{
\sstab 
(4) We present  
an approximation algorithm that 
supports {\em configurable} trade-off between 
the utility of the dataset  
and their sizes. The algorithm 
fast computes a set of size-bounded
datasets that approximate a pareto optimal 
solution with a quality guarantee 
in terms of $\epsilon$-Pareto optimality. 
%%%%% more results could be added
We also introduce optimization 
techniques that utilize 
the correlation among 
utilities for early 
pruning and termination. 
}

\eat{
\sstab
(5) We finally introduce an 
efficient maintenance algorithm to 
maintain the datasets 
upon the updates of the 
underlying source dataset. 
}

%\end{itemize} 
\vspace{.5ex}
Using real benchmark datasets and tasks, we experimentally 
verify the effectiveness of our data discovery 
scheme. We found that \modis is practical 
in use. For example, our algorithms take 
$30$ seconds to generate new data, 
that can improve input 
% pre-trained regression or  
% classification 
models by 1.5-2 times in accuracy 
and simultaneously reduces their training cost by 1.7 times. 
It outperforms baseline approaches  
that separately performs data integration  
or feature selection; 
and remains feasible for larger 
datasets. Our case study 
also verified its practical application 
in domain science tasks. 


\stitle{Related works}. We categorize related works as follows. 
%We remark that these works involve a common goal of improving the performance of 
%data analysis models. 

\eetitle{Feature Selection.} Feature selection removes irrelevant and redundant attributes and identifies important ones for model training~\cite{li2017feature}. Filtering methods rank features in terms of correlation or mutual information~\cite{peng2005feature, nguyen2014effective} and choose the top ones. They typically assume  
linear correlation among features, omitting collective 
effects from feature sets and hence are often 
limited to support directly optimizing 
model performance. 
\eat{Wrappers~\cite{kohavi1997wrappers} choose features based on model performance estimated by a predictive model. The search cost is often extensive. Moreover, 
they risk overfitting due to optimizing a single  
criteria \eg model accuracy. Embedded methods directly learn feature selection models such as Lasso Regression, by penalizing (layer-wise) feature weights~\cite{zhang2019feature}. }
%While they 
%can be more efficient than wrapping, 
%While their performance 
%are more sensitive to %lacks interpretability and is selective with 
%model architectures and task types, 
%Hyper-parameter tuning and additional domain-specific knowledge from datasets and models.
% \vspace{2ex}
Our method differs from 
feature selection in the following. 
(1) It generates skyline dataset with primitive data augmentation and reduction operators, 
%which can mask data in the cell, 
beyond simply dropping the entire columns. 
(2) We generate data that improves the model over multiple ML performance measures, beyond retaining critical features;  
and (3) our method does not require internal knowledge of the models or incur learning overhead. 
%The process can be readily expressed by SPJ (select, project, join) queries.
% and benefit from established 
% query optimization techniques.  
%Our approach overcomes the above methods by considering multiple utilities from both dataset and model perspectives and constructing a Skyline set for optimal trade-off under the FPTAS Guarantee. We also consider active domains for each feature rather than stopping at the column level.

\eetitle{Data Augmentation.} Data augmentation aims to create data from multiple data sources 
towards a unified view~\cite{doan2012principles, ziegler2007data,roh2019survey,esmailoghli2023blend}. It is often specified 
to improve data completeness and richness~\cite{roh2019survey}
and may be sensitive to the quality of schema.  
%that in turn requires entity matching~\cite{li2017human} and schema matching~\cite{khatiwada2022integrating}. 
Our method aimes to generate data to improve the expected performance of  data-driven models. This is different from the conventional data integration which mostly focuses on improving the data completeness.
Generative data augmentation~\cite{desmet2024hydragan} synthesize new rows for multi-objective optimization with a predefined schema. In contrast, \modis generates data with both rows and columns manipulation.  %in tabular data. 
Also, HydraGAN requires a target column for each metric, while \modis supports user-defined metrics with configurable generation. 
%These  highlight the complementary yet fundamentally different paradigms of generative data augmentation and data discovery.



\eetitle{Data Discovery}. 
% \warn{``heterogeneous data lake''.} 
%interacting with specific models during evaluation is computationally expensive. Our utility-driven approach hits these pain points, focusing on identifying feature subsets and active domains that benefit the ML pipeline's performance in polynomial time.
\eat{
Closer to our work is goal-driven data discovery~\cite{galhotra2023metam}. 
The methods perform data augmentation  
(joins) to enrich input data to improve 
the performance of given tasks.}
%The process 
%discovers and augments datasets as path plans, 
%and achieves the desired performance quality 
%with a bounded number of queries. 
Data discovery 
aims to prepare datasets for ML models~\cite{laure2018machine,roh2019survey,esmailoghli2023blend,huang2023kitana,galhotra2023metam}. 
%They typically include three phases: retrieving joinable tables, integration of tables, and evaluating with a real or proxy model~\cite{cappuzzo2024retrieve}. In these approaches, the target model is separated from the data augmentation process. 
For example, Kitana~\cite{huang2023kitana} computes data profiles (\eg MinHash) and factorized sketches for each dataset to build a join plan, and then evaluates the plan using a proxy model. 
\metam~\cite{galhotra2023metam} involves the downstream task with a utility score for joinable tables.  
%but also focuses solely on augmentation and cannot seek trade-offs among multiple objectives.
Comparing with prior work,  we formalize data generation with cell-level operators, beyond joins. 
 %This allows 
%us to create data with more expressiveness, and to better mitigate over-fitting or under-fitting issues. 
We target multi-objective datasets and provide 
formalization in terms of Pareto optimality. 
We also provide algorithms with quality guarantees and 
optimization techniques. 


\eetitle{Model Estimation}. Model 
estimation aims to 
provide accurate estimation of 
a model's performance without 
incurring expensive re-training 
and inference cost. 
%Estimating model performance is particularly valuable when vast evaluation is needed. 
%In 
For example, AutoML~\cite{ hwang2018fast, nguyen2020avatar, yang2020automl} train %a simpler 
-surrogate models to estimate model performance~\cite{ hwang2018fast, nguyen2020avatar, yang2020automl}, or predict the model performance by learning from past attempts~\cite{feurer2018practical} or Reinforcement Learning~\cite{drori2019automatic}. 
%Transfer learning estimates 
%model performance by estimating ``transferability'' with pre-trained model features and target data~\cite{tran2019transferability,nguyen2020leep,you2021logme}. 
%, LEEP~\cite{nguyen2020leep}, and LogME~\cite{you2021logme}.
%Moreover, approaches like ModsNet~\cite{wang2023selecting}
Model selection~\cite{wang2023selecting} 
leverages metadata and historical observations to build graph neural network-based estimator for estimating model performance. 
Our work leverage Multi-output Gradient Boosting as the surrogate model for fast and 
reliable estimation, and benefits 
from established ML performance  
estimation approaches or other surrogate models. 

\eat{
\eetitle{Task-oriented Data Discovery.} In the era of data-centric AI, data discovery during preprocessing is crucial for data science tasks.
Similar to us, but :
1. they use single objective
2. we have a performance guarantee.
3. we involve models
}

\eat{\eetitle{Crowdsourced Data Services.} Several 
platforms are available to allow users to 
share and search datasets, such as Dataset search~\cite{brickley2019google},  Data.gov~\cite{DataGov}, Kaggle~\cite{KaggleYourHome},  Hugging Face~\cite{HuggingFaceAI}, and Zenodo~\cite{Zenodo}. These services exploit user-defined tags to retrieve 
relevant datasets, yet lack the necessary capability 
to provide datasets for improving the expected 
performance for specific models or tasks. Our approach is among the 
the first effort with the enhanced capability 
to directly create 
datasets that improve the expected performance 
of models as ``queries'', and in terms of 
multiple performance measures. 
}

\eat{
Efficient dataset search platforms are essential for researchers, data scientists, and professionals to find relevant datasets for machine learning (ML) pipelines and tasks. We categorized existing data search platforms as follows: (1) {\em Traditional data platforms.} Several web-based data platforms like Google Dataset Search~\cite{brickley2019google} and Data.gov~\cite{DataGov} gathered vast dataset resources while only supporting keyword search and isolated from ML models/pipelines. (2){\em Community-driven platforms.} Such as Kaggle~\cite{KaggleYourHome} and Hugging Face~\cite{HuggingFaceAI}, which build rich ecosystems by closely connecting datasets with crowdsourced models or scripts. And Zenodo~\cite{Zenodo}, a research-focused open-access repository linking datasets with publications. All of them rely on user-defined tags to retrieve datasets. (3){\em Enhanced data discovery platforms.} Several pioneer data discovery platforms have emerged, such as Aurum~\cite{fernandez2018aurum}, which captures relationships among datasets by building, maintaining, and querying an enterprise knowledge graph (EKG). And Auctus~\cite{castelo2021auctus} will profile and index the datasets to support table-wise data augmentation and various query types. One step forward, we introduce a utility-driven data discovery framework that juggles the data repository and ML models to optimize utilities of data science tasks.
}
 