\vspace{-1ex}
\subsection{Diversified Skyline Dataset Generation} 
\label{sec-divmodis}

%To cope with possible data biases in the initial recommendation and to achieve comprehensive exploration, which benefits \eg optimizing experimental design~\cite{konakovic2020diversity, low2023evolution},

A Skyline dataset may still contain data that largely overlap or are similar, hence leading to bias and reducing the generality of the model if adopted. 
This may occur due to skewed value distribution  %in the input, 
in the active domains,  
common attributes, over
specific performance metrics in the skyline 
data generation process. %tend to dominate the result set, 
%and because Skyline generation adheres only to the dominance relation, which can result in clustering in regions with good values for specific metrics.} 
It is often desirable to explore a diversified variant of skyline set generation to create varied datasets that mitigate such bias~\cite{konakovic2020diversity, low2023evolution}. 
% This motivates us to investigate 
% a diversified variant of skyline 
% set generation. %\modis. 

Given $\T$ and a configuration $C$, a set of datasets $\D$, constants $N$, $\epsilon$ and $k$,  
the {\em diversified skyline data generation} 
is to compute a set $\D^*_F$ 
of at most $k$ tables, such that 
(1) $\D^*_F$ is an $\epsilon$-Skyline set 
of $N$ valuated states by an $(N, \epsilon)$-approximation of \modis, 
and (2) among all $\epsilon$-Skyline sets 
over $N$ states valuated in (1), it 
maximizes a diversification score 
defined as: 
\begin{equation}
\kw{div}(\D_F) = \sum_{i=1}^{k-1} \sum_{j=i+1}^{k} \kw{dis}(D_i, D_j)
\label{eq:div}
\end{equation}
where a distance function $\kw{dis}$ quantifies   
the difference of datasets 
in terms of both value distributions and estimated performance, and is defined as:
\[\kw{dis}(D_i, D_j) = 
\alpha\frac{1-\kw{cos}(s_i.L, s_j.L)}{2}  + 
(1-\alpha)\frac{\kw{euc}(t_i.\P, t_j.\P)}{\kw{euc_{m}}}\]
We adopt Cosine similarity $\kw{cos}$ 
and Euclid Distance ($\kw{euc}$). 
The latter 
is normalized by the maximum Euclid Distance 
$euc_m$ among the historical performances in $T$. 
%Both \apxmodis and \bimodis can be readily extended to return a $k$-set of diversified $\epsilon$-Pareteo set. 
% maintaining $(N, \epsilon)$-approximations. 
% Here, we outline such an algorithm, denoted as \divmodis, as a variant of \bimodis.

\vspace{.5ex}
We next outline an algorithm, % diversification variant, 
denoted as \divmodis, that extends 
an $(N, \epsilon)$-approximation to 
computes an 
a diversified $\epsilon$-Skyline set $\D_F$  %\subset \D$
of at most $k$ datasets. 

\eat{
Given a configuration $C$, a set of datasets $\D$, an integer $k$, and constant $\epsilon$, 
the goal is to compute an $\epsilon$-Skyline set $\D_F \subset \D$
of at most $k$ datasets, maximizing a diversification measure as follows:
\begin{equation}
div(\D_F) = \sum_{i=1}^{k-1} \sum_{j=i+1}^{k} dis(D_i, D_j)
\label{eq:div}
\end{equation}
}


\eetitle{Algorithm}. 
\divmodis revises \modis by incrementally diversify an input $\epsilon$-Skyline set ${\D_F}^{i}$ at level $i$ (partially shown in Fig.\ref{alg:divmodis}). It derives 
$\D^P_F$ by a greedy selection and replace strategy 
as follows. 
(1) It initializes $\D^P_F$ as a random $k$-set from ${\D_F}^{i}$, and updates $\D^P_F$ by incrementally replacing tables with the highest marginal gain in diversification, hence an improved $div(\D^P_F)$. (2) $\D^P_F$ is passed to be processed at level $i+1$, upon the arrival of new states. \divmodis returns the diversified set $\mathcal{D}_F$, 
following the same termination condition 
as in \apxmodis. 
% \wrt $N$ valuated datasets.

\begin{figure}
\vspace{-2ex}
\centering
\begin{algorithm}[H]
\caption{:Diversification step at level $i$}
\begin{algorithmic}[1]
\algtext*{EndFor}
\algtext*{EndIf}
\algtext*{EndWhile}
\algtext*{EndFunction}
\algtext*{EndProcedure}

\State \textbf{Input:} 
    $\epsilon$-Skyline set ${\D_F}^{i}$ (from \upi),  integer $k$;
\State \textbf{Output:} 
     a diversified $k$-subset of ${\D_F}^{i}$ (to be passed to level $i+1$).
     \vspace{1ex}
    
\If{$|{\D_F}^{i}| \leq k$} \Return ${\D_F}^i$
\EndIf 
%\vspace{1ex}
\State initialize $\D_F^P$ with $k$ 
random dataset in ${\D_F}^i$; 
\State score := $div(\D_F^P)$; 
%\vspace{1ex}
\For{\textbf{all} $D \in \D_F^P$}
\For{\textbf{all} $D' \in {\D_F}^{i}$}
    \If{$D' \in \D_F^P$} Continue;
    \EndIf
    \State $\D_F^{P'} := (\D_F^P \setminus \{D\}) \cup \{D'\}$;
    \State score' := $div(\D_F^{P'})$
    \If{score' $>$ score} 
    \State $\D_F^P := \D_F^{P'}$, score $:=$ score';
    \EndIf
\EndFor
\EndFor
%\vspace{1ex}
\State \Return $\D_F^P$
\end{algorithmic}
\end{algorithm}
\vspace{-4ex}
\caption{Level-wise diversification of \divmodis}
\vspace{-3ex}
\label{alg:divmodis}
\end{figure}

We show that the diversified \modis 
can be approximated, 
%for the optimal $\epsilon$-Skyline set $\D^*_F$, 
for a submodular diversification function $\kw{div}$. 
Our result holds for the specification of 
$\kw{div}$ in Equation~\ref{eq:div}. 

\begin{lemma} 
\label{lemma:div}
Given $N$ and $\epsilon$, 
\divmodis achieves a $\frac{1}{4}$ approximation for 
diversified \modis, \ie 
(1) it correctly computes a $\epsilon$-Skyline set $D^P_F$
over $N$ valuated datasets, and 
(2) $\kw{div}(D^P_F)\geq \frac{1}{4}\kw{\kw{div}(\D^*_F)}$. 
\end{lemma}

\begin{proofS}
%Given a configuration $C$ = $(s_U, \O, M, T, \E)$, a number $N$, a constant $\epsilon>0$, and user-specified ranges $(p_l, p_u]$ for $p \in \P$, 
We show an induction on the levels. 
(1) We verify the guarantee at a single level, 
by constructing an approximation preserving 
reduction to the stream submodular maximization problem~\cite{chakrabarti2015submodular}. Given a  
 stream $E = \{e_0, \ldots e_m\}$, an integer $k$, and a submodular 
diversification function $f$, it
computes a $k$-set of elements $S$ 
that can maximize $f(S)$. 
Our reduction constructs a stream of 
datasets $\D_S$ following the level-wise 
generation. We show that 
the function $\kw{div}$ is 
a submodular function. 
(2) By integrating a 
greedy selection and replacement 
policy, 
\divmodis keeps a $k$-set with the most diverse and representative datasets to mitigate the biases in the Skyline set.
\divmodis achieves a $\frac{1}{4}$-approximation of an $\epsilon$-Skyline set with maximized diversity at each level $i$. 
%(3) With an induction on the level, 
%we verify the approximability. 
\eat{
To show the approximation 
ratio, we construct a
reduction from  \modis diversification variant
to {\em stream submodular maximization problem}~\cite{chakrabarti2015submodular}. Given a streaming of elements $E = \{e_0,e_1, ..., e_m\}$, an integer $k$, and a submodular function $f$, stream submodular maximization problem
maintains a set of elements $S$ with size $k$ at any time such that 
$f(S) \leq \frac{1}{4}f(S^*)$ where 
$S^*$ is the globe optimal solution.
}
Please see the detailed proof in~\cite{full}. 
\end{proofS}

\stitle{Analysis}.
\divmodis incurs an overhead to update the diversified $k$-set. 
As \modis valuates up to $\min(N_u^{|R_u|}, N)$ nodes (datasets), 
the total additional overhead is in $O(\min(N_u^{|R_u|}, N) \cdot k \cdot T_{\mathcal{S}})$, 
where $T_{\mathcal{S}}$ refers to the unit valuation cost for a single table, which is in PTIME. 
As both $k$ and $T_{\mathcal{S}}$ are relatively small, 
the practical overhead for \divmodis remains small (see Sec.~\ref{sec:exp}). 
%We present the details in~\cite{full}.

\eat{
\vspace{.5ex}
\stitle{Interpretability}. Our \modis algorithms 
can be readily extended to 
answer provenance questions such as 
``what'' data sources are used, 
``where'' the columns are 
from, and how a skyline set is 
generated. This can be done by 
retrieving, for each result 
dataset $D$ in the skyline set,
the paths and associated 
operators from the running graph. We provide more discussion in~\cite{full}. 
}

\stitle{Remarks}. 
Alternatives that solve multi-objective 
optimization may be applied, such as 
evolutionary algorithms such as NSGA-II~\cite{deb2002fast}, 
or reinforcement-learning based methods~\cite{liu2014multiobjective}. 
The former rely on costly stochastic processes (e.g., mutation and crossover) and may require extensive parameter tuning. 
The latter are effective for general state exploration but require 
high-quality training samples 
and may not converge over ``conflicting'' 
measures. In contrast, \modis is training 
and tuning free. Our experiments verified 
that \apxmodis provides early 
generation of high-quality datasets from a few large input datasets, 
due to ``reduce-from-Universal'' 
strategy, \bimodis enhances efficiency through bidirectional exploration and  pruning, hence 
benefits for larger number of small-scale datasets, and \divmodis 
%addresses biases in content and performance, 
%hence 
benefits most for datasets with skewed distribution.
%as validated by formal proof and experiments. }

\eat{\revise{
Compared to evolutionary algorithms like NSGA-II, which rely on costly stochastic processes (e.g., mutation and crossover), and require extensive parameter tuning, and RL-based methods, which are effective for general state-space exploration but face challenges like extensive training requirements, convergence issues with conflicting objectives, and the difficulty of obtaining sufficient training samples here, \modis avoids these limitations by leveraging the DAG structure from our FST formalization, which ensures a deterministic and efficient exploration process with provable guarantees.
\apxmodis starts from a richer feature set for early high-performing datasets, \bimodis enhances efficiency through bidirectional exploration and correlation-based pruning, benefits smaller promising datasets, and \divmodis addresses biases in content and performance, also validated by formal proof and experiments.
}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\eat{
\eat{
In real-world scenarios, it is essential to diversify the discovered datasets to ensure a robust Skyline set. This diversification 
enables rapid adjustments in response to biases or inadequacies in the initial selection.
Take medical diagnosis: if $D_1$ shows male bias and poor female diagnosis, an alternative dataset $D_2$ may offer a gender-balanced solution for accurate and inclusive outcomes.
Moreover, diverse results can provide a holistic view of the exploration, which aids in optimizing experimental design (OED)~\cite{konakovic2020diversity, low2023evolution}.
For instance, in developing eco-friendly cleaners, one dataset focuses on chemical formulations, while another one assesses effectiveness and user experience. Integrating them in development ensures the product is both environmentally friendly and consumer-effective.
From this, we introduce \divmodis, a variant of 
\modis that considers diversification: 
Given a configuration 
$C$, a set of datasets $\D$, an integer $k$, and a constant $\epsilon$, 
the task is to compute an $\epsilon$-Skyline set $\D_F \subset \D$
of at most $k$ tables, maximizing diversification as follows: 
$$\max_{D,D'\in\D_F} \sum_{i=1}^{k} \sum_{j=i+1}^{k} d(D_i, D_j)$$
}

\eat{
Here, we provide our second specification \divmodis, 
which is able to generate 
datasets that are diversified, 
yet still provide a sub-optimality 
guarantee in terms of Pareto optimality. 
Our goal is to 
maximize the pairwise distance, denoted as $Div(\cdot)$ of the generated datasets $\D_F$:
%~\cite{ma2022diversified}: 


$$\max_{D,D'\in\D_F} \sum_{i=1}^{k} \sum_{j=i+1}^{k} d(D_i, D_j)$$

$$d(u, v) = a \frac{euclid(u.pos, v.pos)}{euclid_{max}} + (1-a)\frac{1-cos(u.lable, v.lable)}{2}$$
$$euclid_{max} = euclid(pos(c_{min}, b_{max}), pos(c_{max}, b_{min}))$$
}


We show that both algorithms \apxmodis and 
\bimodis can be readily extended to return a 
$k$-set of diversified $\epsilon$-Pareteo set, which 
remains to be $(N, \epsilon)$-approximations. 
We outline such an algorithm, 
denoted as \divmodis is a variant 
of \bimodis. 

\stitle{Algorithm}. Algorithm~\divmodis addresses the max-sum diversification problem by enhancing \bimodis with on-the-fly diversification as it valuates tables. 
It maintains a diversified subset $\D^P_F\subseteq \D_F$ of size $k$,
using a greedy strategy to select tables from $\D_F \setminus \D_F^{P}$ for inclusion.
Starting with $\D^P_F$ as empty, \divmodis incrementally updates this subset with each execution of \opg($s'$), adding tables that maximize diversification by two cases:
% to gradually enlarge 
% and update $\mathcal{D}_F^{P}$ by deciding whether  
% a valuated table from $\mathcal{D}_F \setminus \mathcal{D}_F^{P}$ at a level should join the 
% diversified set or not. 
% Specifically, $\mathcal{D}_F^{P}$ 
% is initialized as $\emptyset$. Whenever $OpGen(s')$ \warn{(See line 8 in \ref{alg:bimodis})} is invoked, \divmodis updates $\mathcal{D}_F^{P}$ with each newly spawned table $D'$ in $\kw{OpGen}(s')$, following two cases: 

\sstab 
(1) Adding tables to $\D^P_F$ until it contains $k$ tables, choosing those with the greatest marginal gain.
% If $|\mathcal{D}_F^{P}| < k$, \divmodis adds $D' \in OpGen(s')$ that has the maximal marginal gain to
% $\mathcal{D}_F^{P}$; 

\sstab 
(2) Once $|\D_F^{P}| = k$, it employs a sub-modular maximization solver $\S_{\A}$~\cite{chakrabarti2015submodular} to potentially replace an existing table in $\D^P_F$ with a new table $D'$ that offers higher diversification.
% Otherwise, 
% it scans each table in $\kw{OpGen}(s')$ with a {\em sub-modular maximization solver}  $\S_{\mathcal{A}}$~\cite{chakrabarti2015submodular}, 
% which incrementally finds an old 
% table in $\mathcal{D}_F^{P}$ to be replaced by $D'$. 
% %updates current $\mathcal{D}_F^{P}$. 

It is known that
this greedy streaming selection strategy yields a $\frac{1}{4}$-approximation for stream-based Max-Sum Diversification~\cite{streamingdiv}. Differs from \bimodis, \divmodis updates $\D_F$ 
with the $k$-set $\D^P_F$ to ensure 
it contains at least $k$ diversified datasets, 
instead of feeding all spawned tables in $OpGen(s')$ to the next level of bi-directional spawning. 
It thus returns a diversified 
$\epsilon$-Skyline set $\mathcal{D}_F$ 
\wrt $N$ valuated. 

\eat{
The algorithm, denoted as \divmodis, 
is outlined below.
\divmodis solves a max-sum diversification problem. Given the $\epsilon$-Skyline set  $\mathcal{D}_F$, it computes a subset 
of $\mathcal{D}_F$, denoted as $\mathcal{D}_F^{P}$ with size $k$ and 
maximizes \warn{Div($\mathcal{D}_F^{P}$)}. \hanchao{[please confirm the notation for diversity]} 
\divmodis here is a variant of \bimodis that exploits a greedy strategy to iteratively enlarge $\mathcal{D}_F^{P}$ by selecting dataset $D'$ from $\mathcal{D}_F \setminus \mathcal{D}_F^{P}$ {\em ``on the fly"} such that, $\mathcal{D}_F^{P}$ maintains the diversified datasets at any time. To be more specific, $\mathcal{D}_F^{P}$ 
is initialized as $\emptyset$. Whenever $OpGen(s', `F')$ (See line 8 in \ref{alg:bimodis}) is invoked, \divmodis updates 
$\mathcal{D}_F^{P}$ with the newly spawned datasets in $OpGen(s', `F')$ following two cases: for each batch in the batch of spawned $OpGen(s', `F')$, (1) if $|\mathcal{D}_F^{P}| < k$, \divmodis iteratively adds $D' \in OpGen(s', `F')$ that has the maximal marginal gain to
$\mathcal{D}_F^{P}$ until $|\mathcal{D}_F^{P}| = k$; (2)  if $|\mathcal{D}_F^{P}| = k$,
\divmodis firstly iteratively process each datasets in $OpGen(s', `F')$ with a streaming submodular maximization slover, denoted as $\S_{\mathcal{A}}$~\cite{chakrabarti2015submodular}, then updates current $\mathcal{D}_F^{P}$. It is known that
the above greedy streaming selection strategy yields a $\frac{1}{4}$-approximation for stream-based Max-Sum Diversification~\cite{streamingdiv}. Varied from \bimodis,
\divmodis maintains the diversified subset of datasets  {\em ``on the fly"}  to form $\epsilon$-Skyline set instead of feeding all spawned datasets in $OpGen(s', `F')$ to next level of the searching process.
\divmodis thus generates
$\epsilon$-Skyline set $\mathcal{D}_F$ that includes  diversified datasets in terms
of pairwise distance. 
}

\stitle{Time cost}. 
% Let $|R_u|$ be the total number of 
% attributes in the universal schema 
% $R_u$ of $D_u$, and $|\ad_m|$ the 
% size of the largest active domain. 
\divmodis also performs 
$|R_u|$ levels of  
spawning, and at each node, 
spawns and selects at most $k$ 
children at each level. 
%Recall that,
%the time cost of \apxmodis is $O\left(\min(N_u^{|R_u|}, N)\cdot \left(\left(\frac{\log(p_m)}{\epsilon}\right)^{|\P|-1}+I\right)\right)$, 
\divmodis incurs additional overhead at each 
level to update the diversified $k$-set. 
The cost of updating $\mathcal{D}_F^{P}$ for a single level is in
$O(k \cdot (|R_u| + |\ad_m|) \cdot T_{\mathcal{S}})$ 
time, where $T_{\mathcal{S}}$
is a unit cost of invoking the solver $\mathcal{S}_{\mathcal{A}}$ to process a single 
table, which is in polynomial time. 
The total additional overhead for 
diversification of \divmodis 
is thus in $O(k(|R_u|^2 + |\ad_m||R_u|T_{\mathcal{S}}))$. 

As $k$, $R_u$, $\ad_m$ and $T_{\mathcal{S}}$ 
are all small costs, the overhead for diversification 
is in practice small. As verified by our tests, \divmodis has comparable time cost with 
\bimodis in almost all cases (see Section~\ref{sec:exp}). 
This verifies the feasibility of diversified 
data discovery. 


%$O\left(\min(N_u^{|R_u|}, N)\cdot \left(\left(\frac{\log(p_m)}{\epsilon}\right)^{|\P|-1}+I\right) + k \cdot (|R_u|^2 + |\ad_m||R_u| \cdot T_{\mathcal{S}})\right)$, 

\begin{lemma} 
\label{lemma:div}
\divmodis achieves a $\frac{1}{4}$-approximation for \modis diversification variant.
\end{lemma}
\eat{
\begin{proof}
\end{proof}
}
}


