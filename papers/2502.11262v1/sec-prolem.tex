\section{Skyline Dataset Generation: A Formalization}
\label{sec-system}

%\subsection{Data Discovery: A Formal Characterization} 
Given datasets $\D$, an input 
model $M$ and a set of measures $\P$, 
we formalize the generation process of a 
skyline dataset 
with a ``multi-goals'' {\em finite state transducer} (FST). An FST extends %finite state machine that maps inputs to outputs through a sequence of states and transitions. It 
extends finite automata by associating outputs with transitions. % enabling structured data transformations. 
We use FST to abstract and characterize the generation  
of Skyline datasets as a data transformation process. 
We introduce this formalization, with a counterpart 
for data integration~\cite{lenzerini2002data, doan2012principles}, 
to help us characterize the computation 
of skyline 
dataset generation.
%computation, and %clarify %optimization and 
%cost and quality analysis  
%in the following sections. } 
%for cost and termination analysis, as well as optimization through running graph representations. 
%This abstraction facilitates the design of 
%our algorithms and help us justify 
%quality guarantees.} 
% \eat{It has a former counterpart in %formalizing 
% formalizing 
% data integration~\cite{lenzerini2002data, doan2012principles} and shall be 
% used for our algorithm design, 
% as well as 
% %but also help explain 
% their provable 
% correctness and quality guarantees. 
% }

\stitle{Data Generator}. 
%A {\em data discovery system} 
A skyline dataset generator 
is a finite-state transducer, 
denoted as $\T$ = $(s_M, \S, \O, \S_F$, $\delta)$,  where (1) $\S$ is a set of states, 
(2) $s_M\in \S$ is a designated start state, 
(3) $\O$ is a set of operators of types $\{\oplus, \ominus\}$; 
%\item $M$ is a fixed, deterministic model; 
%\item $T$ is a set of observed tests of $M$, where each 
%test $t\in T$ has a computed performance vector $\F(t)$; 
(4) $\S_F$ is a set of output states; and 
(5) $\delta$ refers to a set of transitions. 
%\end{itemize}
We next specify its components. 

%%%%%%%%%%
% I introduced a class 
% of finer grained operators. 
% We can revisit to adjust 
% these. 
%%%%%%%%%%
\eetitle{States}. 
%We consider a 
%``universal 
%schema'' $\U$ defined on $\A$, 
%\ie 
%all the attributes seen 
%in the datasets 
%Given a set of datasets $\D$ = $\{%D_M, 
% D_1, \ldots D_n\}$, 
 %where 
%(a) $D_M$ a 
%{\em goal dataset} over which 
%a given, fixed pre-trained model $M$ 
%{\em should be} eventually generated, 
%and (b) $\{D_1, \ldots D_n\}$ refers to a set of source datasets\footnote{Here $D_M$ can be 
%either 
%initialized as a nonempty dataset 
%or $\emptyset$. Our analysis does not 
%assume the availability of 
%a non-empty $D_M$.}. 
A {\em state} $s$ %= $(D_s, R_s, \ad_s)$ 
specifies a table $D_s$ that conforms to 
schema $R_s$ and active domains $\ad_s$. 
For each attribute $A\in R_s$, 
$\ad_s(A) \subseteq \ad(A)$ 
refers to a fraction of values $A$ can take at state $s$. $\ad_s(A)$ 
can be set as empty set $\emptyset$, 
which indicates that 
the attribute $A$ 
is not involved for training or testing $M$; or a wildcard `\_' (`don't care'), 
which indicates that $A$ 
can take any value in $\ad(A)$. 

\eetitle{Operators}.
A skyline data generator adopts two %classes of 
%shorthanded, 
primitive 
polynomial-time computable operators, 
{\em Augment} and {\em Reduct}. 
%to construct datasets. 
These operators can be %efficiently 
expressed by SPJ (select, project,  
join) queries, or implemented as user-defined 
functions (UDFs). 
%, enhanced with  
%data augmentation or feature 
%selection semantics. 

\sstab
(1) {\em Augment}  
has a general form of $\oplus_c(D_M, D)$, %, R.A,\C$: 
which augments dataset $D_M$ with another  
$D\in \D$ subject to a literal $c$. Here $c$ is a literal in form of $A = a$
(an equality condition). An augmentation $\oplus_{c}(D_M, D)$ executes the following queries: 
(a) augment schema $R_M$ of $D_M$ 
with attribute $A$ from 
schema $R_D$ of $D$, {\em if $A\not\in R_M$};  
(b) augment $D_M$ with 
tuples from $D$ %with values of $R.A$ 
satisfying constraint $c$; and 
(c) fill the rest 
cells with ``null'' for unknown values.
%their values are 
%not known. 
% that specifies the valid domain values of $A$ in $D_s$ of state $s$.

\sstab
(2) {\em Reduct} $\ominus_c(D_M)$: 
this operator% executes the 
%following queries: 
(a) selects from $D_M$ the tuples 
that satisfy the selection condition posed by 
the literal $C$ posed on attribute $R_M.A$; and 
(b) %update the  
removes all such tuples from $D_M$. 
\eat{
mask cells of the tuples in $D_M$ that satisfy the literal $c$ on attribute $R_M.A$ with ``null".
} 
Here $c$ is a single literal defined on $R_M.A$ as in (1). 
%of the form $A = a$, where $a \in \ad(A)$.
%\revise{
%Intuitively, this operation removes the matching tuples from $D_M$. 
%For example, in Fig.~\ref{fig:transition}, a transition from $s_4$ to $s_5$ represents a reduction process. In this case, rows with a ``year'' smaller than $2003$ are removed, resulting in the exclusion of the 2nd row in $D_4$, where ``year'' = $2001$, after the reduction.
%}


%%%% Wu note: some APIs in (Auto)ML librareies also directly provide FS manipulation functions such as drop, etc.  https://scikit-learn.org/stable/modules/feature_selection.html
%% 
%%% how many distinct possible ops there 
% when D_M is given? one can infer
% there are bounded possibilities: 
% for augmentation, |R||d_m|; 
% for reduction, |R_M||d_m|; 
% where d_m is the largest domain size; 
% hence for a transition graph: 
% regardless from which state, 
% its size should be bounded 
% by a 2|R_m||d_m|-ary tree with the depth bounded by 2|R_m||D_m|, 
% where $|D_m|$ is the size of 
% the largest data set, and 
% R_m is the largest schema size. 

\eetitle{Transitions}. A {\em transition} $r$ = $(s,op, s')$ is a triple that 
specifies a state $s$ = $(D, R, \ad_s)$, an operator $\op\in \O$ over $s$, and a {\em result} state $s'= (D', R', \ad'_s)$, where 
$D'$ and $R'$ are obtained 
by applying \op over 
$D$ and $R$ conforming to 
domain constraint  $\ad'_s$, 
respectively. 
\eat{
Given a state $s$ %= $(D, R, \ad_s)$, 
and a set of operators $\O$ 
of the two classes ($\{\oplus, \ominus\}$),  applying an 
operator $\op\in \O$ over $s$ creates a new 
{\em result} state $s'= (D', R', \ad'_s)$, 
$D'$ and $R'$ are obtained 
by applying \op over 
$D$ and $R$ conforming to 
domain constraint  $\ad'_s$, % $\C$, 
respectively. 
We represent this as a {\em transition} $r$ = $(s,op, s')$, 
where $s'$ is the result of applying 
$op$ to $s$. 
%is a triple 
%that converts a state $s$ to another 
%$s'$ via an operator $op$. 
%Here $op$ is from a 
%set of operators 
%$\O$ of the two query classes $\{\oplus, \ominus\}$, such that 
}

In practice, the operators 
can be enriched by task-specific UDFs 
that perform additional data imputation, 
or pruning operations, to further improve 
the quality of datasets. 

\eat{
Given a set of datasets $\D$ = $\{D_M, D_1, \ldots, D_n\}$, a model $M$, and a set of observed tests $T$, 

where $s_M$ is an initialized 
start state $(D_M, R_M, \ad_M)$, 
$\O$ is a set of queries 
from query classes $\oplus$ 
or $\ominus$, $\D_F$ is a 
set of output states, and 
$\delta\subseteq \D \times\O \times \D$ 
is a set of transitions. 
}

\stitle{Running}. A {\em configuration} of $\T$, 
denoted as $C$ = $(s_M, \O, M, T, \E)$, initializes a start state $s_M$ with 
a dataset $D_M$, a finite set of operators $\O$,  
a fixed deterministic model $M$, an 
estimator $\E$, 
and a test set $T$, where each 
test $t\in T$ has a valuated performance vector $\F(t)$. 
Both $D_M$ and $T$ can be empty set $\emptyset$. 
A {\em running} 
of $\T$ \wrt a configuration $C$ = $(s_M, M, T, \E)$ follows a general, deterministic process below.

\sstab 
(1) Starting from $s_M$, and at each state $s$, $\T$ iteratively applies operators from $\O$ to update a table with new attributes and tuples or mask its tuple values. This spawns a set of child states. 
%, each specifying a new result.

\sstab 
(2) For each transition $r$ = $(s, \kw{op}, s')$ 
spawned from $s$ with a result $s'$ by applying  $\kw{op}$, $\T$ 
(a) initializes a test tuple $t(M, D_{s'}, \P)$ 
if $t\not\in \T$ and invokes 
estimator $\E$ {\em at runtime} to 
valuate the performance vector of
$t$; or (b) if $t$ is already in $T$, 
it directly loads $t.\P$. 
\eat{
otherwise, it invokes 
%utility functions to 
%compute (or estimate), 
estimator $\E$ {\em at runtime} to 
valuate  
the performance vector of
$t$, and adds $t$ to $T$. 
}

Consistently, we say a state node $s\in \V$ is {\em valuated}, 
if a corresponding test $t(M, D_s, \P)$ is valuated by 
$\E$. We denote  
its evaluated performance vector as $s.\P$.  

\vspace{.5ex}
The above process {\em terminates} at a set of output states $\S_F$,  
under 
% the validation of 
an external termination condition, 
or no transition can be spawned 
(no new datasets can be generated with $\O$).  
%\end{itemize}
%%% to be enriched by adding estimator's role. 


%\stitle{Termination}. To ensure 
%meaningful and practical data discovery 
%in response to a configuration 
%$C$ = $(s_M, M, T)$, a running of 
%$\T$ generates states 


%%%%%%%%%%%%%%%
% move to full version
%%%%%%%%%%%%%%%%
\eat{
\begin{figure}[tb!]
\centerline{\includegraphics[width =0.46\textwidth]{../fig/machine_types.eps}}
% \centerline{\includegraphics[width =0.43\textwidth]{./fig/machine_types_22}}
\centering
\caption{Data Discovery Systems: Specifications} 
 \vspace{-4ex}
\label{fig:dds}
\end{figure}
}



\begin{figure}[tb!]
\centerline{\includegraphics[width =0.45\textwidth]{fig/transition_graph}}
\centering
\vspace{-1ex}
\caption{A skyline data generation process, with a part of running graphs, and result datasets.}
\label{fig:transition}
\vspace{-2ex}
\end{figure}



The {\em result} of a running $\T$ refers to the set of corresponding datasets $\D_F$ 
induced from the output states $\S_F$. 
As each output state $s\in \S$ uniquely determines 
a corresponding output dataset $D_s$, 
for simplicity, 
we shall use a single general term 
``output'', denoted as $\D_F$, to 
refer to output states or datasets. 

%%%%%%%%%%%%%%%%
% move to full version. 
%%%%%%%%%%%%%%%%
\eat{
\begin{example}
\label{exa-op}
Fig.~\ref{fig:dds} illustrates several classes of data discovery systems, 
categorized by the type of 
computation they perform. 
%to create new datasets. 

\sstab
(1)\textbf{ Type 1} (resp. \textbf{Type 2}) %are data discovery 
system, denoted as 
$\T_1$ (resp. $\T_2$) starts from an initial dataset $D_M$ in 
its start state $s_M$, and 
only applies reduction (resp. augmentation 
operators) to update $D_M$. 

\sstab
(3) \textbf{Type 3} system, denoted as 
$\T_3$ is a general form 
(that generalizes Types 1-2 systems) 
and perform either augmentation 
or reduction in any step to 
create datasets. 

\sstab
(4) \textbf{Type 4} system is a variant 
of Type 1 under a configuration 
that starts with a designated universal state $s_U$, 
which is associated with a 
``universal'' dataset $D_U$. 
The dataset is obtained by 
concatenating all schema 
in $\D$ as a ``universal schema'' $R_U$, and by 
integrating (joining) all tuples from 
$\D$. It can be specified 
to use a 
{\em ``reduce-from-universal''}
strategy and only 
performs reduction operators to 
remove cell values or tuples from $D_U$. 
%\warn{add more on $T_5$ system as 
%``reduct-from-universal'' to 
%justify the completeness of \apxmodis. 
%$\T_3$ will justify \bimodis.}

\sstab
(2) \textbf{Type 5} system, denoted as $\T_5$, 
extends transitions $\delta$
with an $\epsilon$-transition (``do-nothing  
and jump''), so allows 
a {\em ``bi-directional''} 
computation. It can perform (a) a  
reduction to remove 
values from an initial dataset 
followed by an option to (b) jump 
and start to augmentation to another 
dataset with new attributes and their values. 
For example, it may start with a universal dataset 
$D_U$ as aforementioned 
and perform 
reduction; and choose to ``jump'' to 
augment a set of (small) datasets, or vice versa, until 
the computations converge to 
some same datasets. 
\end{example}
}


\eetitle{Running graph}. 
A running of $\T$ can be 
naturally represented 
as the dynamic generation of a {\em running graph} $G_\T$ = $(\V, \delta)$, which is a 
%with 
%output $\D_F$ is a 
directed acyclic graph 
(\dag) with a set of state nodes 
$\V$, and a set of transition edges 
$r$ = $(s, op, s')$. 
%two states $s$ and $s'$ 
%with a label $op$. 
%Specifically, it has a 
%single start state node 
%$s_M$ without any incoming edges, and a set of 
%output state nodes $\D_F$ without outgoing 
%edges. 
A {\em path} of length $k$ is a sequence of 
$k$ transitions $\rho$ = 
$\{r_1, \ldots r_k\}$ 
such that for 
any $r_i$ = $(s_i,op,s_{i+1})$, 
$r_{i+1}$ = $(s_{i+1},op,s_{i+2})$; \ie 
it depicts a sequence of transitions 
that converts an initial state 
$s_1$ with dataset $D_1$ to 
a result $s_k$ with $D_k$. 




\begin{example}
\label{exa-tg}
Following Example~\ref{exa-motivation}, 
%the observations specify a small fraction of data that is sufficient for CI-index forecasting 
%using a random forest (\kw{RF}) $M$ 
%at desired accuracy.  
Fig~\ref{fig:transition} shows a fraction of a running graph with input set $\D$=
%four datasets 
$\{D_w, D_b, D_N, D_P\}$ (water, basin, nitrogen, and phosphorus tables, respectively). 
The augmentation 
$\oplus$ uses
% is implemented 
% with a library of 
spatial joins~\cite{vsidlauskas2014spatial},
a common query 
that join tables with tuple-level spatial similarity. 
%%%%%%%%%%%%%%%%
With a configuration 
$C$ = $(s_M, \kw{RF}, \{RMSE, R^2, T_{train}\}, \E\})$ (where $\E$ is an MO-GBM estimator), a running starts by 
joining %two data tables 
$D_w$ and $D_b$
% , the basin 
% table and the upstream 
% river data 
to get $D_2$. 
$D_2$ is then augmented with the attribute 
``\texttt{Phosphorus}'' 
%and 
%``Nitrogen'', both under selection 
under 
%conditions ``\kw{season} = `Spring' 
a literal 
``\texttt{year} = {\em 2013}'', resulting in $D_3$ via a path $\{\oplus_1, \ldots, \oplus_3\}$. 
In each step, a test $t$ is initialized; 
and the estimator $\E$ 
is consulted to valuate the 
performance vector of $t$, and enrich $T$. 

Consider another path 
from $D_M$ that results a dataset $D_5$ in Fig.~\ref{exa-motivation}.  
It first
augmentes $D_M$ to $D_4$ 
with data in ``Spring''. 
A reduction with a condition 
``\texttt{year}<'2003'' selects 
and removes all the tuples 
in $D_4$ with historical data before $2003$, 
which leads to 
dataset $D_5$ that retains only 
the data since $2003$. 
%and a reduction 
%that retains only the 
%data from 2003.
\end{example}
\eat{
Fig~\ref{fig:transition} 
depicts a fraction of the 
transition graph of 
the above running, 
containing the two paths. 
}
\eat{
\comwu{refer to Fig 2 for 
Type 1 and Type 2 machine 
($\T_1$, $\T_2$); 
for type 3, a general case ($\T_3$); and 
for type 4 ($\T_4$), a case equivalent to 
type 3 machine.}
}



\eat{
\begin{example}
\label{exa-transition}

\end{example}
}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% verification moved to full anonymous version
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\eat{
\vspace{-2ex}
\subsection{Expressiveness and Properties}

We next discuss fundamental 
properties of a data discovery system. 
These properties help us 
justify the generality as well as practical  
implementations of data 
discovery algorithms (
see Section~\ref{sec-algorithm}). 

\stitle{Expressiveness}. 
We study the expressiveness power  
of the data discovery system 
in terms of all the paths with 
operators as ``labels'' it can 
produce in all its possible transition graphs. 
This gives us a set of strings, 
forming its language $L(\T)$. 
We show that 
a data discovery system and its runnings  
resembles, and can be specified in practice for data integration~\cite{lenzerini2002data}, 
%if one %emoves the model $M$ and 
%constrain $\O$ as data integration operators; 
%or (2) 
or feature selection~\cite{miao2016survey}, 
%in which $\O$ 
%contains %drop %operators 
%$\ominus$ with no 
%value constraints. 
We provide
the following result. 
%and by considering 
%cost such as feature importance 
%or information gain. 
 




\begin{proposition}
\label{prop-simulate}
A data discovery system 
$\T$ can be configured to express (1) data augmentation, 
and (2) feature selection. 
\end{proposition}

\begin{proofS}
We show the above cases by 
an analysis on the languages 
generated by the specifications of type 4 
system $\T_4$. To see this, 
it suffices to show that (1) $L(\T_1)\subseteq L(\T_4)$, and $\T_1$ can express 
feature selection; and (2) $L(\T_2)\subseteq L(\T_4)$, and $\T_2$ can simulate 
data augmentation process. 
%and (3) $L(\T_3)$ = $L(\T_4)$. 
One can readily infer 
(1) and (2) by treating 
$\T_1$ and $\T_2$ as special cases 
of $\T_4$, with proper configurations. 
For (1), one removes the model $M$ and 
constrain $\O$ as data reduction operators 
only without selection conditions. 
For (2), one constraint $\O$ as 
data augmentation (``join'') 
operators. 
\end{proofS}

\stitle{Non-blocking}. 
We also verify a desirable property 
of data discovery computation. 
A data operator (query) $\op$ is 
{\em non-blocking}~\cite{law2004query},   
if for any path $\rho$ with a transition 
that applies $\op$ and a result $D$ in the running of a transducer $\T$,  
% which contains 
%a transition that applies $\op$, 
%at step $j$, 
it generates the same result $D$
regardless of at which step 
it applies $\op$ in $\rho$ 
(whenever applicable). 
We have the following claim. 

\begin{lemma}
\label{prop-non-blocking}
%The processing of 
%any paths in a data discovery system 
%$\T$ is non-blocking. 
%\textbf{Claim}: {\em 
The operators of type $\oplus$ and 
$\ominus$ are non-blocking. % in practice. 
\end{lemma}

%\begin{proofS}
We observe 
that in most applications, 
(a) $\oplus$ is commutative 
and associative, and 
(b) any consecutive application of an $\oplus$ operator followed by an operator in  
$\ominus$ are commutative. 
This ensures that the 
operators $\oplus$ and $\ominus$ 
are non-blocking. Moreover, 
this suggests that 
any paths  
in $\T$ are order-independent. 
%non-blocking. 
%\end{proofS}

Following the above analysis, we 
have the result below. 

\begin{proposition}
\label{prop-equivalent}
Given a set of operators $\O$ with non-blocking 
operators $\oplus$ and $\ominus$,  
%and transitions $\delta$, 
Type 3, Type 4, and Type 5 systems 
have the same expressiveness. 
\ie $L(\T_3)$ = $L(\T_4)$ = 
$L(\T_5)$. 
\end{proposition}

In other words, any dataset 
that can be created by a 
running of a general 
form $\T_4$ can be simulated by 
a running of $T_3$ or $T_5$ 
under a proper   
configuration that leads to 
the same result. 
This allows us to 
choose a proper 
design of $\T$ from 
any of $\T_3$, $\T_4$ 
or $\T_5$ to perform the 
search with a completeness 
guarantee. 

The above properties further indicate effective asynchronous implementation 
of data discovery in distributed 
environment for non-blocking 
operators. We defer 
such discussion in future work. 

\stitle{Church-Russel Property}. 
With the above analysis, we verify that the 
computation of a data discovery system, 
from the perspective of a rewriting system, 
is both terminating and confluent, 
\ie demonstrate a ``Church-Russel'' property. 

\begin{proposition}
\label{thm-cr}
Given a configuration $C$ with non-blocking operators 
of types $\{\oplus, \ominus\}$, 
and a data discovery 
system $\T$, the running of 
$\T$ is terminating and confluent. 
\end{proposition}

We show this by verifying that: 
(1) any computation of $\T$ leads to a ``fixpoint'' 
dataset $D_f$ that has a universal schema $R_U$ with 
all the tuples having 'null' values only,  
%contains all the tuples from $\D$ with tuples 
given possible and applicable 
operators in $\oplus$ or $\ominus$. 
This is because augmentation does not 
repeatedly add new attributes from $R_U$, 
and each cell's value, once set to be `null' by 
a reduction, will not be changed again. 
(2) The sequences of operations are 
locally confluent (Lemma~\ref{prop-non-blocking}). 
%These results ensure that the algorithms that 
%implement data discovery systems 
%always terminate. 

Due to limited space, we provide 
all the detailed proofs in~\cite{full}. 

%%% will need to incorporate a KG component 
% thus a bi-component system. 
} 
%%%%%%%%%%%%%%%%%%%%%%
\vspace{-1ex}
\section{Skyline Data Generation Problem} 
\label{sec-problem}

%\subsection{Quality}
%%%%%%%%%%%%%%%%%%%%%%%%%
% the reduction may well cope with this; 
% if we have space, clarify in the 
% full version the performance 
% metric can include path quality 
% and ML performance models. 
% So it's sum aggregation of 
% edge cost. 
%%%%%%%%%%%%%%%%%%%%%%%%%%

%The running of a data discovery system $\T$
%= $(s_M, \S, \O, M, \D_F, \delta, T)$ eventually 
%converts an initial dataset $D_M$ %to a start state $s_M$ 
%to a set of output %as output states 
%$\D_F$. 
Given $\T$ and a configuration $C$, \modis  
aims find a running 
of $\T$ that ideally leads to a ``global'' optimal dataset, where $M$ is expected to deliver the highest performance over all metrics. Nevertheless, a single optimal solution may not always exist. 
First, %Even normalized to be minimized, 
two measures in $\P$ may 
in nature conflict 
due to trade-offs (\eg training cost versus accuracy, precision versus recall). 
Moreover, the ``no free lunch'' 
theorem~\cite{sterkenburg2021no} 
indicates that there may not exist a single test that demonstrate best performance 
over all measures. 
%One may convert the problem to single objective optimization. 
%For ML model performance optimization, 
%it is unlikely feasible  This 
%in turn justifies the need for computing a Pareto optimal set for our data discovery problem. 
We thus pursue {\em Pareto optimality} for $\D_F$. We start with
a dominance relation below. 

\stitle{Dominance}. Given a data discovery system $\T$ and performance measures $\P$, %we say 
a state $s$ 
= $(D_s, R_s, \ad_s)$ is {\em dominated} by 
$s'$ = $(D_{s'}, R_{s'}, \ad_{s'})$, 
%(resp. $s'$ dominates $s$), 
denoted as $s\prec s'$, %(resp. $s'\succ s$), 
if there are valuated tests $t$ = $(M, D_s)$ 
and $t'$ = $(M, D_{s'})$ in $T$, such that 
\bi
\item for each $p\in \P$,  %$(i\in[1,|\P|])$, 
$t'.p \leq t.p$; and 
\item there exists a measure $p^*\in \P$, such that 
$t'.p^*< t.p^*$.
\ei 
%We call $p^*$ a {\em decisive %deterministic 
%measure}. 
A dataset $D_s$ is dominated 
by $D_{s'}$, denoted as $D_s\prec D_{s'}$, if %for their states $s$ and $s'$, 
$s \prec s'$. 

\eetitle{Skyline set}. 
Given $\T$ %= $(s_M, \S, \O, M, \D_F$, $\delta)$, 
and a configuration $C$, let $\D_F$ be the set of all the possible output datasets from 
a running of $\T$,  a set of datasets $\D_F^*\subseteq\D_F$ is 
a {\em skyline set} \wrt $\T$ and $C$, if 
\begin{itemize}
\item for any dataset $D\in \D_F^*$, 
and any performance measure $p\in \P$, 
there exists a test $t\in T$, such that 
$t.p\in [p_l,p_u]$; 
\item  there is no pair %of datasets 
$\{D_1,D_2\} \subseteq \D_F^*$   
such that $D_1 \prec D_2$ or $D_2 \prec D_1$;  
and 
\item for any other %dataset 
$D\in \D_F\setminus\D_F^*$, 
and any $D'\in \D_F^*$, 
$D\prec D'$.
\end{itemize} 
We next formulate the skyline data generation problem. 

%%%%%%%%%%%
\eat{
\stitle{Encoding of the Search Space}

Configuration state graph.

Partial configuration/ Full configuration.

Augment edge/ Modify edge.

Start state/ Terminating state.
}

\eat{
\stitle{Path ($\rho$) Dominance}

$$(<P_1, P_2, ..., P_n>|<f_1, f_2, ..., f_n>)$$
$$\forall P_i \in \vec{P}, P_i = \text{{compute}} \vee P_i = \text{{estimate}}$$

\begin{equation*}
\begin{split}
\rho_1 \leq_{\epsilon} \rho_2 \iff
&
\left(\forall p^b \in P_{benifit}, \rho_1(p^b) \leq (1+\epsilon)\rho_2(p^b)\right) 
\land \\
&
\left(\forall p^c \in P_{cost}, (1+\epsilon)\rho_1(p^c) \geq \rho_2(p^c)\right)
\end{split}
\end{equation*}
}

\stitle{Skyline Data Generation}. 
Given 
%and a data 
%set of datasets $\D$, a fixed, deterministic model $M$, and a data 
%discovery system 
%$\T$ = $(s_M, \S, \O, \S_F, \delta)$, 
a skyline data generator $\T$ 
and its configuration $C$ = $(s_M, \O, M, T, \E)$, 
%that specifies %non-blocking 
%operators $\O$ and $\delta$, 
the {\em skyline data generation} problem 
%denoted as 
%\modis, 
is to 
%determine if 
%there exists a running of $\T$ that 
%outputs 
compute a skyline set $\D_F$ 
in terms of $\T$ and $C$. 
%and if so, compute the 
%set $\D_F$. 

\begin{example}
\label{exa-modis}
Revisiting prior example %~\ref{exa-tg} 
and consider 
the temporal results $\D_F$= 
$\{D_1, \ldots, D_5\}$ with the following performance 
vectors valuated by 
the estimator $\E$ so far: 
%for the random forest model $M$:
%\warn{give a small table}. 

\begin{center}
\begin{small}
    \begin{tabular}{|c|c|c|c|}  
        \hline
       T: (D, M, $\P$, $\E$)  & RMSE & $\hat{R^2}$ & $T_{train}$ \\  \hline
            $t_1:(D_1,\kw{RF},\P,  \kw{MO-GBM})$ & 0.48 & 0.33 & 0.37 \\
       \hline
            $t_2:(D_2,\kw{RF},\P, \kw{MO-GBM})$ & 0.41 & 0.24 & 0.37 \\
       \hline
            $t_3:(D_3,\kw{RF},\P, \kw{MO-GBM})$ & 0.26 & \underline{0.15} & 0.37 \\
       \hline
            $t_4:(D_4,\kw{RF},\P, \kw{MO-GBM})$ & 0.37 & 0.22 & 0.39\\
       \hline
            $t_5:(D_5,\kw{RF},\P, \kw{MO-GBM})$ & \underline{0.25} & 0.18 & \underline{0.35}\\
       \hline
    \end{tabular}
    \end{small}
\end{center}
Here $\hat{R^2}$ is inversed as 1-$R^2$:  
the smaller, the better. 
All the measures are 
normalized in $(0,1]$ \wrt 
user-specified upper and lower bounds, 
and the optimal values are underlined. 
One can verify the following dominance relation among the 
datasets: 
(1) $D_1 \prec D_2 \prec D_3$, 
and $D_4 \prec D_5$;
(2) $D_3 \not\prec D_5$ and vice versa.  
Hence a Skyline set 
$\D_F$ currently
contains $\{D_3, D_5\}$. 
% over the valuated datasets. 
\end{example}

We present the following hardness result.  
%to a complexity analysis below. 

\begin{theorem}
\label{them-np}
Skyline data generation is (1) \kw{NP}-hard; 
and (2) fixed-parameter tractable, if 
(a) $\P$ is fixed, and (b) $|\D_F|$ is polynomially bounded 
by the input size $|\D|$. 
%, and it is in 
%\kw{PTIME} to verify if a set of datasets 
%$\D_F$ is a skyline set. 
\end{theorem}

\begin{proofS}
The \kw{NP}-hardness can be verified by a reduction 
from the Multiobjective Shortest Path problem (\mos). 
Given an edge-weighted graph $G_w$, where each edge $e_w$ has a $d$-dimensional cost vector $e_w.c$, the cost of a path $\rho_w$ in $G_w$ is defined as $\rho_w.c$ = $\sum_{e_w\in\rho_w}$ $e_w.c$. 
The dominance relation between paths % $\rho_w$ and $\rho'_w$ 
is determined by comparing their costs.  
\eat{
Specifically, $\rho_w$ dominates $\rho'_w$ if $\rho_w$ has equal or lower costs than $\rho'_w$ in all dimensions and is strictly better in at least one dimension. }
\mos is to compute a Skyline set of paths from start node $s$ to target node $t$. 

%all other nodes in the graph. 
Given an instance of \mos, we construct an instance 
of our problem as follows. 
(1) We assign an arbitrally ordered index to the edges 
of $G_w$, say $e_1, \ldots e_n$.  
(2) We initialize a configuration $\T$ as follows. 
(a) $s_M$ has a single dataset $D_0$, 
where for each edge $e_i$ = $(v,v')$, 
there is a distinct tuple $t_i\in D_0$. 
(b) $\O$ contains a set of %$|E|$ 
reduction operators, where each operator $o_i$  
removes tuple $t_i$ from $D_0$, 
and incurs a pre-defined 
performance measure $e_i.c$. 
(c) $M$ %is a hard-coded bijection %function 
 maps each tuple $t_i$ in $D_0$ to a 
fixed embedding in $\mathbb{R}^d$. 
(d) The test set $T$ is $\emptyset$. 
We enforce the running graph of $\T$ to be 
 the input $G_w$, by setting 
the initial state as $s$ with associated 
dataset $D_0$,  a unique termination 
state as the node $t$, and the 
applicable transitions as the edges 
in $G_w$. 
One can verify that a solution of 
\mos is a Pareto set of paths 
from $s$ to $t$,  
each results in a dataset by 
sequentially applying 
the reduction operators 
following the edges of the path. 
This yields a set of datasets %which yields 
that constitutes 
a corresponding skyline set $\D_F$
as a solution for 
our problem. 
As \mos is shown to be 
NP-hard~\cite{hansen1980bicriterion, serafini1987some}, 
%, a known \kw{NP}-hard problem~\cite{hansen1980bicriterion, serafini1987some}. 
the hardness of skyline data generation follows. 
%\revise{The \kw{NP}-hardness can be established through a reduction to the Multiobjective Shortest Path problem (\mos), a well-known \kw{NP}-hard problem~\cite{serafini1987some}. 
%The reduction is detailed in the proof of Lemma~\ref{lm-approximability} in~\cite{full}.
%

% The \kw{NP}-hardness can be verified by 
% a reduction from 
% {\em Maximum Coverage}, 
% a known \kw{NP}-hard problem. 
To see the fixed-parameter tractability, 
we outline an exact algorithm. 
(1) The algorithm exhausts the runnings of a skyline 
generator $\T$, and invokes a PTIME inference 
process of the model $M$ and valuate at most 
$N\leq |\D_F|$ possible states (datasets). 
(2) It invokes a multi-objective 
optimizer such as Kung's algorithm~\cite{kung1975finding}. 
This incurs $O(N\log N)^{|\P|-2}$ 
valuations when $|\P|\geq 4$, 
or $O(N(\log N))$ if $|\P|\textless 4$. 
As $N\leq |\D_F|$, and $|\D_F|$ is in $O(|D|)$, 
and $P$ is a fixed constant, 
the overall cost is in PTIME (see~\cite{full} for details). 
\end{proofS}

While the above exact algorithm is able to 
compute a skyline dataset, it remains infeasible 
even when enlisting $N$ 
states as a ``once-for-all'' 
cost is affordable. Moreover, 
a solution may contain 
an excessive number of datasets to be 
inspected. 
%\vspace{.5ex}
%The  multi-objective optimization problem is in general intractable~\cite{glasser2010approximability}.  
We next present %investigate  
three feasible algorithms, 
%that implement \modis.  
that %They 
generate datasets that approximate skyline sets with  
bounded size and quality guarantees.

%We 
%also investigate how different 
%search strategies may impact 
%the efficiency. 
%framework 
%for our problem. We then analyze its performance 
%guarantees under several practical specifications. The 
%Our main results 
%are summarized in Table~\ref{tab:result}.

%%% We will need to discuss the 
% polynomial boundedness of the 
% result, in turns of *input* size!

\eat{
\comwu{We need to introduce fixed parameter results 
to clarify the polynomial boundedness of 
the size of the Skyline set. Then we need to 
clarify a ``FPTAS'' under this hypothesis.}
}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% If we need formal proofs here, use 
% the running of T as a finite state machine 
% or nondeterministic TM in natural. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%
% notes: https://drive.google.com/file/d/1iG4Q5BQbXw_R7YWnB1MXiKIQzM0_l552/view
%%%%%%%%%%%%%%%%%%%%%%%%