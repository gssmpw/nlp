\vspace{-1ex}
\section{Models and Performance Evaluation}
\label{sec:problem}

We start with several notations used in \modis framework. 

\stitle{Datasets}.
A dataset $D(A_1, \ldots A_m)$ is a structured 
table instance that conforms to a local 
 schema $R_D(A_1,\ldots A_m)$.  
Each tuple $t\in D$ is a m-ary vector, 
where $t.A_i$ = $a$ ($i\in [1,m]$) means 
the $i$th attribute $A_i$ of $t$ is assigned a value 
$a$. A dataset may have missing values 
at some attribute $A$ (\ie $t.A$ = $\emptyset$). 

Given a set of datasets $\D$ = $\{D_1, \ldots D_n\}$, each dataset $D_i$ confirms to a local schema $R_i$. 
The {\em universal schema} 
$R_U$ is the union of the local schemas of 
datasets in $\D$, \ie a set of
all the attributes involved in $\D$. 
\eat{
\revise{Here, $U$ refers to the unified set of attributes derived from $R_U$, encompassing all attributes available across $\D$. This forms the foundation for constructing the universal table $D_U$, which includes all joinable rows and attributes from $\D$ and $R_U$, providing a comprehensive dataset for specific tasks and serving as the start point of the discovery process.}
}
%Denote as $\A$ the set of 
%all the attributes from the datasets in $\D$. 
The {\em active domain} of an attribute $A$ 
from $D_U$, denoted as $\ad(A)$, 
refers to the finite 
set of its distinct values occurring in $\D$. 
The size of $\ad(A)$, 
denoted as $|\ad(A)|$, is  
the number of distinct values of 
$A$ in $\D$.  


\stitle{Models}. A data science model (or simply ``model'') is a function 
in the form of 
%$M(D(\bar{X}))$ = $D'(\bar{Y})$, 
$M: D \rightarrow \mathbb{R}^d$, which takes as input a 
dataset $D$,  and outputs  
a result embedding in $\mathbb{R}^d$ 
for some $d\in \mathbb{N}$. Here 
$\mathbb{R}$ and $\mathbb{N}$ 
are real and integer sets. 
%over attributes $\bar{Y}$. 
In practice, 
$M$ can be a pre-trained machine learning model, 
a statistical model, or a simulator.
The input $D$ may represent a feature matrix 
(a set of numerical feature vectors), or a tensor (from real-world physical systems), to be used 
for a data science model $M$ 
as training or testing data. 
The output embedding %$\E$ 
%``representation'' in 
%machine learning, 
can be conveniently 
converted to 
task-dependent output (\eg labels for classification, 
discrete cluster numbers for
clustering, or Boolean values
for outlier 
detection) with post-processing. 


\eat{
  For example, given an input 
image feature matrix $D$,  a classifier 
$M$ assigns labels converted from a label probability matrix $M(D)$. % = $\E$. 
}

\eetitle{Fixed Deterministic models}. 
We say a model $M$ is {\em fixed}, if 
its computation process does not change  
for fixed input. 
For example, a regression model 
$M$ is fixed if any factors 
that determines its inference   
(\eg  number of layers, learned model 
weights) remain fixed. 
The model $M$ is {\em deterministic} 
if it always outputs the same result 
for the same input. 
We consider fixed, deterministic models for the needs of 
consistent performance, which is a 
desired property in ML-driven data analytical tasks. 

\vspace{.5ex}
\stitle{Model Evaluation}. 
A {\em performance measure} $p$ (or simply ``measure'')
is a performance indicator of a model $M$, such as accuracy 
\eg precision, recall, F1 score (for classification); 
or mean average error (for regression analysis). 
It may also be a %``reverse indicator'', 
cost measure such as 
training time, 
inference time, or memory consumption. 

\eat{For example, a test $t$ = $(M,D, \{`F_1', `\text{inference cost}'\})$ 
of an image classifier $M$ specifies 
its performance over image 
dataset $D$ 
in terms of $F_1$ score 
and inference time cost. }

%\eetitle{Estimators}. 

\vspace{.5ex}
We use the following settings.

\sstab
(1) We unify $\P$ as a set of normalized measures to {\em minimize}, with a range $(0,1]$. 
% For a measure that are normalized in $(0, 1]$, 
% one can readily transform the 
% metric with inversed counterpart. 
Measures to be maximized (\eg accuracy)  
can be easily converted to an inversed counterpart (\eg relative error).

\sstab
(2) Each measure $p\in \P$ has 
an optional range $[p_l, p_u]\in (0, 1]$. 
%(by default, $(0,1]$). 
It specifies desired 
lower bound $p_l$ or an upper bound $p_u$ for model 
performance, such as 
maximum training or inference time, memory capacity, 
or error ranges. 

\stitle{Remarks}. 
As we unify $\P$ as a set of measures to be minimized, it is intuitive that an upper bound $p_u$ specifies a ``tollerence'' for the estimated performance. 
We necessarily introduce a ``lower bound'' $p_l > 0$ 
for the convinience of (1) ensuring well-defined theoretical 
quality guarantees (as will be discussed 
in Section~\ref{sec-apxmodis}), and (2) leaving the option 
open to users for the configuration needs of downstream tasks such as testing, comparison or benchmarking. 
%\warn{define $\boldsymbol{t}$ here. We used it in \apxmodis}. \mengying{changed it to $p_u$}


\begin{table}[tb!]
\begin{small}
\begin{tabular}{|c|c|}
\hline
\textbf{Symbol} & \textbf{Notation} \\ 
\hline
$\D$, $D$, $D_U$ & a set of datasets, a single dataset, universal table\\
\hline
$R_D$, $R_U$ & local schema of $D$, and universal schema\\
\hline
$\A$, $A$, $\ad(A)$ & attribute set, attribute, and active domain\\
\hline
$M$ & a data science model $D\rightarrow \mathbb{R}^d$\\
\hline
$\P$, $p$, $(p_l,p_u)$ & perform. measures, a measure, its range \\
\hline
$T$, $t$ = $(M,D,\P)$, $t.\P$ %$\F(t)$ 
& test set; single test, its performance vector \\
\hline
$\T$ = $(s_M, \S, \O, \S_F, \delta)$ & a data discovery system\\
\hline 
$\E$ & a performance estimation model \\
\hline 
$C$ = $(s_M, \O, M, T, \E)$ & a configuration of data discovery system \\
\hline
$G_\T$ = $(\V,\delta)$ & running graph \\
\hline
$s\prec s'$, $D\prec D'$ & state dominance, dataset dominance \\ \hline
\hline
%$s.L$ & bitmap encoding of attr. $A$ and $\ad(A)$ in $s$\\
%\hline
%$\D_F$ & $\epsilon$-Pareto set \\
%\hline
\end{tabular}
\caption{Table of notations}
\vspace{-8ex}
\label{tab:notation}
\end{small}
\end{table}

\eetitle{Estimators}. 
A performance measure $p\in\P$
%can be estimated by 
%an actual  
%application  %simulation process 
%of $M$ over $D$, yet 
%this can be expensive, 
%especially when it involves in-lab tests or human inspection.
%Model performance 
%estimation is, in particular 
%desirable when actual 
%inference is expensive 
%for large-scale tests and models~\cite{hernandez2022training, you2021logme}. 
can often be efficiently estimated by 
an estimation model $\E$ 
(or simply ``estimator''), in PTIME 
in terms of $|D|$ 
(the number of tuples in $D$). 
An estimator $\E$ makes use of a set of 
historically observed performance  
of $M$ (denoted as 
$T$) 
 to infer its performance over a new dataset. 
It can be a regression model that learns from historical 
tuning records $T$ to predict 
the performance of $M$ given a new 
dataset $D$. 

By default, we use 
a multi-output Gradient Boosting Model~\cite{scikit-learn} 
that allows us to obtain  
the performance vector 
by a single call with 
high accuracy (see Section~\ref{sec:exp}). 

\eetitle{Tests}. 
Given a model $M$ and a  
dataset $D$, a {\em test} $t$ 
is a triple $(M,D,\P)$, which  
specifies a test dataset $D$, 
an input model $M$, and 
% and a set of (user-defined) %{\em utilities} 
% {\em performance measures} 
% $\P$ = $\{p_1, \ldots p_l\}$. 
 a set of user-defined measures 
$\P$ = $\{p_1, \ldots p_l\}$. 
 A test tuple $t$ = $(M,D,\P)$ 
 is {\em valuated} by 
 an estimator $\E$ if 
 each of its measure $p\in \P$ is 
 assigned a (estimated) value by $\E$. 



%\stitle{Utility functions}.




\eat{
Below we provide a set of commonly 
seen utilities, all computable in 
low PTIME in terms of 
input size. 
\mengying{Add a table for optional measurements for each utility measure. }
}

\begin{example}
\label{exa-models}
%\warn{example of CI-index forecast}.
Consider
Example~\ref{exa-motivation}. A pre-trained 
random forest (\kw{RF}) model 
$M$ that predicts CI-index is evaluated by three measures 
$\P$ = $\{\kw{RMSE}$, $\kw{R^2}$, $\kw{T_{train}}\}$, which specifies 
the root mean square error, the $R^2$ score, 
and the training cost. A user specifies 
a desired normalized range of 
\kw{RMSE} to be within 
$(0,0.6]$, \kw{R^2} in $[0, 0.35]$ \wrt a ``inversed'' lower bound $1-0.65$,
and $\kw{T_{train}}$ 
in $(0,0.5]$  \wrt an upper bound of 
``3600 seconds'' (\ie 
no more than 1800 seconds).
% ; 
% and $T_{inf}$ in $(0,0.5]$ 
% with an upper bound of ``600 seconds'' 
% (\ie ``no more than 300 seconds''). 
\end{example}

We summarize the main notations 
in Table~\ref{tab:notation}. 


%%%%%%%%
\eat{
Feature-related objectives~\cite{li2017feature} will be computed exactly, and a surrogate model will estimate model-related objectives.

\eetitle{Benefits.}
1. Model performance

2. Feature separability (similarity-based): Fisher Score~\cite{fisher1936use}

3. Feature correlation (feature and target, information theoretical based): mutual information~\cite{lewis1992feature}

\eetitle{Costs.}

1. Feature redundancy: Variance Inflation Factor (VIF)

2. Model training time

3. Model complexity: Bayesian information criterion (BIC)~\cite{schwarz1978estimating}
}

