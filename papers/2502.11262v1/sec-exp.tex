%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%  https://docs.google.com/spreadsheets/d/1SpGE0Sf_sl_ypEfHr2NEvBJhZrCC_ci0SyLpdXFCqiE/edit#gid=1692132617
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Experiment Study}
\label{sec:exp}

We next experimentally verify 
the efficiency and effectiveness of 
our algorithms. We 
aim to answer three questions: 
\textbf{RQ1}: 
How well can our algorithms improve the performance of models in multiple measures? 
\textbf{RQ2}: 
What is the impact of generation settings, such as data size?
%quality requirement, 
%number of performance measures, 
%and optimization strategies? 
\textbf{RQ3}: 
How fast can they generate skyline sets, and how scalable are they? 
We also illustrate the applications of our approaches with case studies\footnote{
Our codes and datasets are available at 
github.com/wang-mengying/modis}.
%\footnote{Our code is made available at \url{xxx}}. 

\stitle{Datasets.}
We use three sets of tabular datasets: 
kaggle~\cite{KaggleYourHome}, \open~\cite{DataGov}, and \hf~\cite{HuggingFaceAI} (summarized in Table~\ref{tab-data}). 
% We set a base with a universal schema $s_U$ for each dataset by joining attributes from input tables. 

\eat{
\stitle{Datasets}. 
We use the following datasets summarized below:
%in Table~\ref{tab-data}. 
% \warn{No need to use all; refine to what we used.}

\sstab
(1) \kaggle~\cite{KaggleYourHome}, collected from 
a set of tables involving movie information;  %such as \tbf; 
%\mengying{$T_3$}

\sstab
(2) \open: a fraction of a large open public dataset~\cite{DataGov}. We sampled $2K$ tables, involing schools recording, school evaluations and school types, houses recording, housing price in New York and Chicago, geographical location, among others; 
%\mengying{$T_1$, $T_2$, extend from \metam}

\sstab
(3) \hf: a set of 
tables involving Avocado prices and relevant information,  %such as \tbf, 
sampled from Hugging Face~\cite{HuggingFaceAI}.  
%\mengying{$T_4$, find a regression model from HF}
}

% \begin{table}
%     \centering
%     \begin{tabular}{|c|c|c|c|c|}
%     \hline
%         Tasks \& Models  & Source & \# Columns & \# Rows \\ \hline
%        $T_1$: \gbm & \kaggle  & 12 & 3732 \\ \hline
%        $T_2$: \rfh & \open  & 27 & 1178 \\ \hline
%        $T_3$: LRavocado & \hf & 13  & 18249  \\ \hline
%     \end{tabular} \caption{Characteristics of Inputs}
%     \vspace{-4ex}
%    \label{tab-data}
% \end{table}


\begin{table}
    \centering
    \begin{small}
    \begin{tabular}{|c|c|c|c|}
    \hline
       Dataset Sets  & \# tables & \# Columns & \# Rows  \\ \hline
       \kaggle  & 1943 & 33573 & 7317K %& 704.6MB
       \\ 
       \hline
       \open  & 2457 & 71416 & 33296K
       %& 8.58GB
       \\ 
      % \hline
       % \tus  &  &  &  & \\ 
      % \hline
       % \santos  &  &  &  & \\ 
      % \hline
       % \uci  &  &  &  & \\ 
       \hline
       \hf & 255  & 1395 & 10207K 
       %& 3.5GB 
       \\ \hline
       
    \end{tabular}
    \end{small}
    \caption{Characteristics of Datasets}
    \vspace{-5ex}
   \label{tab-data}
      \vspace{-3ex}
\end{table}


\stitle{Tasks and Models.}
A set of tasks are assigned for evaluation. % to predict a target attribute. 
% {\em Trainer} 
We trained: (1) a Gradient Boosting model (\ul{\gbm}) to predict movie grosses using \kaggle for Task $T_1$; 
(2) a Random Forest model (\ul{\rfh}) to classify house prices using \open with the same settings in~\cite{galhotra2023metam} for Task $T_2$; and 
(3) a Logistic Regression model (\ul{LRavocado}) to predict Avocado prices using \hf for Task $T_3$. 
(4) a LightGBM model (\lgc)~\cite{ke2017lightgbm} to classify mental health status using \kaggle for Task $T_4$. 
%To showcase the generality of \modis, 
We also introduced 
task $T5$, a link regression task for
recommendation. This task takes as input a bipartite graph 
between users and products, and links indicate their interaction. 
A LightGCN~\cite{he2020lightgcn} (\lgr), 
a variant of graph neural networks (GNN) optimized for 
fast graph learning, 
is trained 
to predict top-$k$ missing edges in an input bipartite graph 
to suggest products to users. 
A set of $1873$ bipartite graphs is constructed from \kaggle for 
$T_5$.  %where each graph has on average \tbf nodes and \tbf edges. 
The ``augment'' (resp. ``reduct'') operators are  
defined as edge insertions (resp. edge deletions) 
to transform a bipartite graph to another.  
% Scripts are implemented with scikit-learn~\cite{scikit-learn}. 
%$5,660$ nodes and $16,800$ edges.
%\revise{and a set of knowledge graphs \kizoo that includes ML assets information from \kaggle, which consists of $5,660$ nodes and $16,800$ edges.}
%(5) a LightGCN model~\cite{he2020lightgcn} (\lgr) to recommend pre-trained models for a given test set, using \kizoo for Task $T_5$.}
% For a fair comparison, we use the exact same training script for all compared methods.

We use the same training scripts for each task and all methods %(needed by goal-oriented methods) 
%and evaluation 
for a fair comparison.
We assigned measures $\P_1$ through $\P_5$ for tasks $T_1$ to $T_5$, respectively
% For each task, we adopted a group of measures to 
% guide the data discovery
, as summarized in Table~\ref{tab-measures}.
% The first five ($p_{Acc}$, $p_{Tr}$, $p_{F1}$, $p_{MAE}$, $p_{MSE}$) directly quantify models' performance, while the latter two ($p_{Fsc}$, $p_{MI}$) for feature selection~\cite{li2017feature}. 
We also report the size of the data ($p_{DSize}$) in terms of (total $\#$ of rows total $\#$ of columns), excluding attributes with all cells masked.

\eat{
\stitle{Tasks and Models}. 
We have trained the following 
models: 
(1) a random forest models \rfh 
%and 
%\rfs, 
for classifying house price 
(Task $T_1$), 
%and 
%school performance 
%(Task $T_2$), respectively, 
using 
\open and the settings consistently in~\cite{galhotra2023metam, fan2022semantics};   
%(2) more models paired with datasets. 
(2) a Gradient Boosting Model 
(\gbm) for predicting the movies' worldwide gross sale, using \kaggle (Task $T_2$); and 
(3) a regression model 
for predicting Avocado price, 
using \hf (Task $T_3$).  

We trained all these models 
with scikit-learn~\cite{scikit-learn}.
%\warn{add more 
%training settings. }
For a fair 
comparison, we use the original 
training scripts provided by the baseline 
methods and validated that the 
reproduced models have 
consistent performance 
as reported. 
}

\eetitle{Estimator $\E$}. We 
%have evaluated representative estimation models for model estimation, such as Linear Regression, Random Forest, among others~\cite{he2021automl}, and 
adopt MO-GBM~\cite{scikit-learn}  
as a desired model performance estimator. It % can handle multiple outputs and 
outperforms other candidate models even with a simple training set %using the bitmap encoding $s.L$ as input to predict the performance measures $s.\P$ for state $s$, it achieves satisfactory results and 
%outperform other options. 
For example, for $T_1$, MO-GBM performs inference for all objectives on one state in at most $0.2$ seconds, with a small MSE of $0.0003$ when predicting ``Accuracy''. 
\eat{
We trained  
a multi-output Gradient Boosting Model (MO-GBM)~\cite{scikit-learn} 
as our estimator. 
}

\eat{
\mengying{To simplify estimation and improve accuracy, we used the bitmap encoding $s.L$ as input to predict the performance measures $\P_s$ for state $s$ over the given model $M$. 
}
}
% \warn{Give more details}. 
% \warn{Other options include 
% -- give references for high-quality 
% estimators, and perhaps cite Modsnet}.
\eat{which outputs predicted 
values for multiple variables, allowing us to valuate the performance vector for each test with one call.}

\stitle{Algorithms}. 
We implemented the following methods. 

\sstab
(1) \textbf{\underline{MODis}}: Our multi-objective data discovery algorithms, including \apxmodis, \bimodis, and \divmodis. We also implemented \nomodis, a counterpart of \bimodis without correlation-based pruning. 
%\divmodis is built upon \nomodis. 
(2) \underline{\metam}~\cite{galhotra2023metam}: 
a goal-oriented data discovery algorithm
that optimizes a single utility score with consecutive joins of tables. 
We also implemented an extension \metammo, 
by incorporating multiple measures into a single 
linear weighted utility function. 
%that queries a downstream task with a candidate dataset based on dataset utility. Although it attempts to find optimal augmentation, its performance in discovering more relevant data is limited and cannot satisfy users who want more data augmentation.
%https://github.com/TheDataStation/Metam
(3) \underline{\starmie}~\cite{fan2023semantics}: a data discovery method that focuses on table-union search and uses contrastive learning to identify joinable tables.
% A data discovery algorithm with 
% table union search as the main use case. Given a table, it discovers union-able 
% tables by learning feature semantic similarity with 
% contrastive learning. 
%However, it is limited by table-level search and long pre-training time. 
For \metam and \starmie, we used the code from 
original papers. 
(4) \underline{\sklearn}~\cite{scikit-learn}: 
% \kw{SelectFromModel} function in scikit-learn, which selects features automatically with a built-in 
% estimator.
An automated feature selection method 
in scikit-learn's \kw{SelectFromModel}, which recommends important features with a built-in 
estimator.
%\warn{add description.}
% doc-link: https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html#sklearn.feature_selection.SelectFromModel
(5) \underline{\ho}~\cite{h2o_platform}: an AutoML platform; we used its feature selection module, which fits features and predictors into a linear model.
% an AutoML platform used to automatically optimize machine learning pipelines. We utilized its feature selection module to fit features and predictors into a generalized linear model and then obtained a subset of selected features. 
% \warn{Add what ``Original'' is - we showed it in Fig 7.}

\stitle{Construction of 
$D_U$ and Operators}.
To prepare %universal tables 
universal datasets $D_U$ %as input 
for \modis, we %followed~\cite{galhotra2023metam} 
preprocess \kaggle, \open and \hf 
into joinable tables and construct 
$D_U$ with multi-way joins. 
This results in $D_U$ 
datasets with a size (in terms of 
\# of columns and \# of rows):
$(12, 3732)$, $(27, 1178)$, $(13, 18249)$ 
and $(20, 140700)$, for 
tasks $T_1$ to $T_4$, respectively. 
Specifically, we applied 
$k$-means clustering over 
the active domain of 
each %joinable (common) 
attribute (with a maximum $k$ 
set as $30$), and 
derived equality literals, 
one for each cluster. 
We then compressed the 
input tables by replacing 
rows into tuple clusters, reducing 
the number of rows. 
This pragmatically help us 
avoid starting from large 
$D_U$ by only 
retaining 
the values of interests,   
and still yield desired 
skyline datasets. 
For $T_5$, a 
large bipartite graph 
is constructed 
with a size of 
$(7925, 34)$ (\# of edges, \# of nodes' features). 
The generation of graphs consistently aligns with its table data counterpart, 
by conveniently replacing augment and reduction to their graph counterpart 
that performs link insertions and deletions. 

\eat{
\revise{
%To demonstrate the generalizability of \modis, 
We also include task $T_5$, where the base table is a graph. %The universal table 
The universal dataset $D_U$ for $T_5$ is constructed by augmenting the base table with information from \kizoo, resulting in a size of $(7925, 34)$ (\# of edges, \# of nodes' features). Other settings for $T_5$ follow the default configurations used in other tasks, showcasing \modis's applicability across diverse data modalities.}
\eat{
We identified related attributes based on a loose standard with attributes' names and overlaps to maximize the information included in $s_U$, with the size of $(12, 3732)$, $(27, 1178)$ and $(13, 18249)$.
}
}

\eat{
\warn{add description.}
Distributed and scalable machine learning and predictive analytics platform that allows the building of machine learning models on big data and provides easy productionalization of those models in an enterprise environment.
}


%$p_{Fsc}$,  $p_{MI}$ and $p_{VIF}$ 
%that quantifies 

%that highlight the need of 
%the ``benefit'' to be maximized 
%for data discovery. 
%\warn{more; give a table here}. 

% the baseline methods. 


\eat{
\item
\automl~\cite{}: \warn{add description.} 

\item 
\fselect~\cite{}: \warn{add description.}}

%\end{itemize} 


\begin{table}
    \centering
 \begin{center}
 \begin{small}
     \begin{tabular}{|c|c|c|} \hline
       Notation &  Measures    & Used In  \\ \hline
       $p_{Acc}$   & Model Accuracy &   $\P_1$, $\P_2$, $\P_4$ \\ \hline
       $p_{Tr}$   & Training Time Cost  &   $\P_1$-$\P_4$ \\ \hline
       $p_{F1}$   & $F_1$ score  &   $\P_2$, $\P_4$ \\ \hline
       $p_{AUC}$   & Area under the curve  &   $\P_4$ \\ \hline
       $p_{Nc(n)}$   & NDCG(@n)  &   $\P_5$ \\ \hline
%       $p_{MC}$   & Model Complexity &   $\P_1$ \\ \hline
       $p_{MAE}$, $p_{MSE}$   & Mean Absolute / Squared Error &   $\P_3$\\ \hline
       $p_{Pc(n)}$, $p_{Rc(n)}$   & Precision(@n), Recall(@n) &   $\P_5$\\ 
       \hline \hline
       $p_{Fsc}$   & Fisher Score~\cite{li2017feature} &   $\P_1$, $\P_2$ \\ \hline
       $p_{MI}$   & Mutual Information~\cite{li2017feature,galhotra2023metam} &   $\P_1$, $\P_2$ \\ \hline
       % $p_{VIF}$   & Variance Influence Factor~\cite{li2017feature} &  $\P_1$\\ \hline
 %      $p_{DSize}$   & Size of Created Dataset &  None \\ \hline
     \end{tabular}
     \end{small}
    \caption{Performance Measures}
     \label{tab-measures}
     \end{center}
\vspace{-5ex}
\end{table}






\eat{
We evaluate data discovery algorithms 
in terms of tasks, performance measures 
and test models, as summarized below. 

%We created different tasks, 
%summarized % in Table~\ref{tab-scene}. 
%below. 
\begin{small}
%\begin{table}
    \centering
    \begin{tabular}{|l|c|c|c|c|}  \hline
        \multicolumn{1}{|c|}{Tasks} & Type & Dataset & Model & Perf. \\  \hline
       $T_1$: Movie Gross (C) & Classification  & \kaggle & \eat{\modelthree}\gbm & $\P_1$ \\
%       \hline
%       $T_2:$ School Perform (C) & Classification  & \open & \eat{\modeltwo}\rfs  & $\P_2$ \\
       \hline
       $T_2:$ House Price (C) & Classification  & \open & \eat{\modelone}\rfh  & $\P_2$ \\
       \hline
       $T_3:$ Avocado Price (R) & Regression  & \hf & LRavocado & $\P_3$ \\
       \hline
    \end{tabular}
%    \caption{Task scenarios (Configurations)}
%    \vspace{-4ex}
%    \label{tab-task}
%\end{table}
\end{small}
    
%The baseline methods either does not 
%address performance measures or only 
%consider a single 
%measure. 
%, which are underlined in the table. 
}



\eat{
(1) The first four directly quantify 
a model's performance in terms of accuracy 
($p_{Acc}$ for classification and regression, and both $p_{MAE}$ 
and $p_{MSE}$ for regression) and 
training cost ($p_{Tr}$).  
(2) The latter three ($p_{Fsc}$,  $p_{MI}$ and $p_{VIF}$), generally used in feature 
selection~\cite{li2017feature} quantify the 
statistical relationship 
between a set of input variables 
(features) and a ``target'' 
feature (\eg `House Price' to be classified, or `Avocado Price' to be 
predicted); the larger, 
the better. Among these, 
$p_{MI}$ is also adopted by~\cite{galhotra2023metam} as 
an optimization goal for 
data discovery.  
(3) To evaluate the 
amount of result, we also report the size of the 
data ($p_{DSize}$), in terms of 
(total $\#$ of rows, total $\#$ of features). 
As all baselines only report a single table, 
and \modis report a set of tables, we 
report total size in favor of baselines. 
Here if a column has all cell masked, we 
consider the column reduced and remove it 
from the output table. 

For each tasks in $T_1$-$T_3$, we initialized 
our \modis methods consistently with a 
configuration that specifies 
an original dataset, the matching trained model, 
and the corresponding measures $\P_1$-$\P_3$.
}

\stitle{Evaluation metrics}. 
We adopt the following metrics 
to quantify the effectiveness of data discovery approaches. Denote $D_M$ as an initial dataset, and 
$\D_o$ a set of output datasets from 
a data discovery algorithm. 
%(1) Both \metam and \starmie 
%generate a single dataset $D_o$ 
%for a designated 
%task-oriented metric $p$. By default, 
%$p$ is \tbf for regression task, 
%and \tbf for classification. 
%(2) For \modis algorithms, 
%we set $\P$ to contain the measure $p$ accordingly as 
%one of \tbf or \tbf, with $5$ additional normalized 
%measures: \tbf, \tbf, \tbf, \tbf, and \tbf. 
%Given the output of \modis algorithms $\D_o$, 
%we choose $6$ datasets $D_{p_i}$ such that 
%each has the highest estimated 
%model performance for metric $p_i\in \P$ $(i\in[1,6])$. 
(1) We define the {\em relative improvement} 
$\relp(p)$ for a given 
measure $p$ achieved by a
method as $\frac{M(D_M).p}{M(D_o).p}$.
As all metrics are normalized to be minimized,
the larger $\relp(p)$ is,
the better $D_p$ is in improving $M$ \wrt $p$. 
Here $M(D_M).p$ and $M(D_p).p$ are obtained 
by actual model inference test. 
This allows us to fairly compare all 
methods in terms of the quality of data 
suggestion. 
For efficiency, we compare the time cost of data discovery upon 
receiving a given model or task 
as a ``query''. 

\eat{
\eetitle{Task scenarios}. We created the following 
task scenarios with specified task, model, and performance 
measures (Perf.), summarized in Table~\ref{tab-task}. \warn{Give the table}. Here $\P_1$ - $\P_3$ are 
defined as follows. \warn{No need to use the same 
set of performance metrics. Anything larger than one is good. 
Accordingly for radar graph -- you can have one with Axis of three, four, or five.}
}

\eat{
\eetitle{TUS~\cite{nargesian2018table}}} 
%https://github.com/megagonlabs/starmie

%\mengying{Add one for feature selection, one for AutoML}
%\mengying{\apxmodis: control in 5-40 mins}

\begin{table*}[tb!]
\customsize
\centering
\renewcommand{\arraystretch}{1.05}
\begin{small}
\begin{tabular}{|c|c|c|c|c|c|c||c|c|c|c|}
\hline
$T_2$: House & Original & \metam & \metammo & \starmie & \sklearn & \ho & \apxmodis & \nomodis & \bimodis & \divmodis \\ \hline
$p_{F1}$ & 0.8288 & 0.8510 & 0.8310 & 0.8351 & 0.7825 & 0.8333 & 0.9044 & \ul{\textbf{0.9125}} & \ul{\textbf{0.9125}} & 0.8732 \\ \hline
$p_{Acc}$ & 0.8305 & 0.8322 & 0.8333 & 0.8331 & 0.7826 & 0.8305 & 0.9050 & \ul{\textbf{0.9121}} & \ul{\textbf{0.9121}} & 0.8729 \\ \hline
$p_{Train}$ & 0.2000 & 0.21 & 0.19 & 0.2100 & 0.2000 & 0.2000 & 0.1533 & \ul{\textbf{0.1519}} & \ul{\textbf{0.1519}} & 0.2128 \\ \hline
$p_{F_{sc}}$ & 0.0928 & 0.0889 & 0.0894 & 0.0149 & 0.2472 & 0.0691 & 0.2268 & \ul{\textbf{0.2610}} & \ul{\textbf{0.2610}} & 0.2223 \\ \hline
$p_{MI}$ & 0.126 & 0.1109 & 0.1207 & 0.0243 & \ul{\textit{0.2970}} & 0.1054 & 0.2039 & 0.2018 & 0.2018 & \textbf{0.3164} \\ \hline
Output Size  & (1178, 27)  & (1178, 28)  & (1178, 28)  & (1178, 32)  & (1178, 4)  & (1178, 15)  & (835, 17)  & (797, 17)  & (797, 17)  & (1129, 5)\\ \hline

\hline
\hline

% $T_1$: \gbm & Original & \metam & \metammo & \starmie & \sklearn & \ho & \apxmodis & \nomodis & \bimodis & \divmodis \\ \hline
% $p_{Acc}$ & 0.8560 & 0.8743 & 0.8676 & 0.8606 & 0.8285 & 0.8545 & 0.9291 & \textbf{0.9874} & \ul{\textit{0.9755}} & 0.9427 \\ \hline
% $p_{Train}$ & 1.4775 & 1.6276 & 1.1785 & 1.2643 & \textbf{0.6028} & 0.9692 & 0.9947 & 0.8766 & \ul{\textit{0.8027}} & 0.8803  \\ \hline
% $p_{Fsc}$ & 0.0824 & 0.0497 & 0.0801 & 0.1286 & 0.7392 & 0.3110 & 0.6011 & \ul{\textit{0.7202}} & \textbf{0.9240} & 0.8010\\ \hline
% $p_{MI}$ & 0.0538 & 0.0344 & 0.0522 & 0.1072 & 0.3921 & 0.1759  & \textbf{0.4178} & 0.3377 & 0.3839 & \ul{\textit{0.4165}}\\ \hline
% % $p_{VIF}$ & 1.5831 & 1.9669 & 1.9782 & 1.2980 & 1.6742 & 1.5096 & \ul{\textbf{2.4092}} & \ul{\textit{2.1331}} & 1.7688 & 2.0188 \\ \hline
% Output Size  & (3264, 10) & (3264, 11) & (3264, 11) & (3264, 23) & (3264, 3) & (3264, 8) & (2958, 9) & (1980, 12) & (1835, 11) & (2176, 10) \\ \hline

% \hline
% \hline

$T_4$: Mental & Original & \metam & \metammo & \starmie & \sklearn & \ho & \apxmodis & \nomodis & \bimodis & \divmodis \\ \hline
$p_{Acc}$      & 0.9222 & 0.9468 & 0.9462  & 0.9505 & 0.8839 & 0.9236 & \textbf{0.9532} & 0.9471 & \ul{0.9525} & 0.9471 \\
\hline
$p_{Pc}$     & 0.7940 & 0.7991 & 0.8070 & 0.8106 & 0.6577 & 0.7892 & \textbf{0.8577} & 0.8454 & \ul{0.8549} & 0.8454 \\
\hline
$p_{Rc}$        & 0.7722 & 0.7846 & 0.7959 & 0.8030 & 0.7523 & 0.7879 & \textbf{0.8097} & \ul{0.8092} & 0.8075 & \ul{0.8092} \\
\hline
$p_{F1}$           & 0.7829 & 0.7918 & 0.8014 & 0.8068 & 0.7018 & 0.7885 & \textbf{0.8330} & 0.8269 & \ul{0.8305} & 0.8269 \\
\hline
$p_{AUC}$           & 0.9618 & 0.9757 & 0.9774 & 0.9784 & 0.9326 & 0.9615 & \textbf{0.9792} & 0.9755 & \ul{0.9789} & 0.9755 \\
\hline
$p_{Train}$ & 0.4098 & 0.3198 & 0.4027 & 0.3333 & \textbf{0.2359} & \ul{0.2530} & 0.3327 & 0.2818 & 0.3201 & 0.2818 \\
\hline
Output Size  & ($10^5$, 14) & ($10^5$, 15) & ($10^5$, 15) & ($10^5$, 16) & ($10^5$ 8) & ($10^5$, 8) & (128332, 16) & (116048, 16) & (128332, 17) & (116048, 16) \\
\hline

\end{tabular}
\end{small}
\caption{Comparison of Data Discovery Algorithms in Multi-Objective Setting ($T_2$, $T_4$)}
\label{tab:comparison}
\vspace{-5ex}
\end{table*}


\eat{
\begin{table*}[tb!]
\customsize
\centering
\begin{tabular}
{|c|c|c|c|c|c|c||c|c|c|c|}
\hline
$T_1$: Movie Gross (C) & Original & \metam & \metammo & \starmie & \sklearn & \ho & \apxmodis & \nomodis & \bimodis & \divmodis \\ \hline
%\midrule
%Accuracy
$p_{Acc}$ & 0.8560 & 0.8743 & 0.8676 & 0.8606 & 0.8285 & 0.8545 & 0.9291 & \ul{\textbf{0.9874}} & \ul{\textit{0.9755}} & 0.9427 \\ \hline
%Training Time
$p_{Train}$ & 1.4775 & 1.6276 & 1.1785 & \ul{\textbf{1.2643}} & 0.6028 & 0.9692 & 0.9947 & \ul{\textit{0.8766}} & 0.8027 &  \\ \hline
%$p_{MC}$ & 140.77 & 132.67 & 98.64 & \ul{\textbf{1442.00}} & \ul{\textit{623.71}} & 531.75 & 383.64 & 440.00 & 398.91 \\ \hline
$p_{Fsc}$ & 0.0824 & 0.0497 & 0.0801 & 0.1286 & 0.7392 & 0.3110 & 0.6011 & \ul{\textit{0.7202}} & \ul{\textbf{0.9240}} & 0.8010\\ \hline
$p_{MI}$ & 0.0538 & 0.0344 & 0.0522 & 0.1072 & 0.3921 & 0.1759  &\ul{\textbf{0.4178}} & \ul{\textit{0.3377}} & 0.3839 & 0.4165\\ \hline
$p_{VIF}$ & 1.5831 & 1.9669 & 1.9782 & 1.2980 & 1.6742 & 1.5096 & \ul{\textbf{2.4092}} & \ul{\textit{2.1331}} & 1.7688 & 2.0188 \\ \hline
Output Data Size  & (3264, 10) & (3264, 11) & (3264, 11) & (3264, 23) & (3264, 3) & (3264, 8) & (2958, 9) & (1980, 12) & (1835, 11) & (2176, 10) \\ \hline
%\bottomrule
\end{tabular}
\vspace{0.5ex}
\caption{Comparison of Data Discovery Algorithms in Multi-Objective Setting}
\label{tab:comparison}
\end{table*}
}

\begin{table}[tb!]
\customsize
\centering
\renewcommand{\arraystretch}{1.05}
\begin{small}
\begin{tabular}{|>{\centering\arraybackslash}p{1.32cm}|>{\centering\arraybackslash}p{0.93cm}|>{\centering\arraybackslash}p{1.16cm}|>{\centering\arraybackslash}p{1.05cm}|>{\centering\arraybackslash}p{1.05cm}|>{\centering\arraybackslash}p{1.05cm}|}
\hline
$T_5$: Model & Original 
% & Universal 
& ApxMODis & NOMODis & BiMODis & DivMODis \\
\hline
$p_{Pc_5}$    & 0.7200 
% & 0.7800 
& \textbf{0.8200} & 0.8000 & \textbf{0.8200} & 0.8000 \\
\hline
$p_{Pc_{10}}$  & 0.6600 
% & 0.7800 
& 0.8100 & 0.8000 & \textbf{0.8200} & 0.8000 \\
\hline
$p_{Rc_5}$       & 0.1863 
% & 0.1980 
& \textbf{0.2072} & 0.2022 & \textbf{0.2072} & 0.2022 \\
\hline
$p_{Rc_{10}}$      & 0.3217 
% & 0.3663 
& 0.3866 & 0.3816 & \textbf{0.3977} & 0.3816 \\
\hline
$p_{Nc_5}$        & 0.6923 
% & 0.7243 
& \textbf{0.7935} & 0.7875 & 0.7924 & 0.7875 \\
\hline
$p_{Nc_{10}}$        & 0.6646 
% & 0.7467 
& 0.7976 & 0.7891 & \textbf{0.8033} & 0.7891 \\
\hline
Output Size           & (7925, 0) 
% & (7925, 34) 
& (5826, 30) & (1966, 6) & (2869, 4) & (1966, 6) \\
\hline
\end{tabular}
\end{small}
\caption{Comparison of \modis Methods on $T_5$}
\label{tab:modsnet}
\vspace{-6ex}
\end{table}

\stitle{Exp-1: Effectiveness}. 
We first evaluate \modis methods 
over five tasks. 
Results for $T_1$ and $T_3$ are shown
% in radar graphs 
in Fig.~\ref{fig:mo-eff} (the outer, the better). 
% Results for $T_1$, which is used for movie grosses prediction,
% $T_2$, which use the same data repository and setting from \metam, 
% $T_2$, which is used for house price prediction,
% and $T_4$,  which is used for depression cases classification,
While results for $T_2$ and $T_4$
are presented in Table~\ref{tab:comparison}.
Results for $T_5$ are in Table~\ref{tab:modsnet}.
We also report the model performance over 
 the input tables as a ``yardstick'' 
 (``Original'') for all methods. 
As all baselines output a single table, to compare \modis algorithms, we select the table in the Skyline set with the best estimated $p_{Acc}$, $P_{F1}$, $P_{MSE}$, $p_{Acc}$ and $p_{Pc_5}$ for $T_1$ to $T_5$, respectively. 
As \metam optimizes a single utility score, we choose the same measure for each task as the utility. 
%We also apply it to \metammo by combining all measures into the utility; and 
We apply model inference to all the output tables to report actual performance values. 
We have the following observations. 

\begin{figure}[tb!]
\vspace{-2ex}
\centerline{\includegraphics[width =0.43\textwidth]{fig/radar}}
\centering
\vspace{-1ex}
\caption{Effectiveness: Multiple Measures}
\vspace{-3ex}
\label{fig:mo-eff}
\end{figure}

\sstab
(1) \modis algorithms outperform all the baselines in all tasks. 
As shown in Table~\ref{tab:comparison}, for example, for $T_4$, the datasets that bear best $p_{Acc}$ and the second best are returned by \apxmodis (0.9535) and \bimodis (0.9525), respectively, and all \modis methods generated datasets that achieve $0.87$ on $p_{F1}$ in $T_2$. 

\sstab
(2) Over the same dataset and for other measures, \modis algorithms outperform the baselines in most cases. 
% In particular, \bimodis, \divmodis, and \nomodis provide top results for multiple measures. 
For example, in $T_1$, the result datasets that most optimize $p_{Fsc}$ and $p_{MI}$ are obtained by \bimodis and \apxmodis, respectively; also in $T_2$  and $T_3$, \nomodis and \bimodis show absolute dominance in most measures. 
%\metam optimizes the chosen measure as utility score with one run. %Another goal-driven method, 
%\revise{While other baselines are limited to tabular data, \modis can effectively handle more data modalities, \eg graph data in $T_5$.}
Table~\ref{tab:modsnet} also verifies 
that \modis easily generalizes to suggest 
graph data for GNN-based analytics, beyond tabular data.  

\sstab
(3) Methods with data  
augmentation (\eg \metam and \starmie) enriches 
data to improve model accuracy, at a cost of training time, while feature selection methods (\eg \sklearn and \ho) reduce data at the cost of accuracy with improved training efficiency. \modis 
methods are able to balance these trade-offs better 
by {\em explicitly} performing multi-objective optimization. 
For example, $p_{Acc}$ and $p_{Train}$ in $T_4$, The best result for training cost (0.2359s) 
is contributed from \sklearn, yet at a cost of 
lowest model accuracy (0.8839). 

We also compared $p_{Acc}$ on $T_4$ with HydraGAN, a generative data augmentation method, which achieves $0.9355$ with $330$ rows but fell short of data discovery methods. Increasing the number of rows further reduced performance, reflecting the limitations of generative approaches in this context, which cannot utilize verified external data sources, and synthetic data often lacks inherent reliability and contextual relevance of discovered data.
%As \modis methods are able to 
%optimize various measures (among others), 
%by making flexible decisions to augment or 
%reduce data, 
%they are able to construct data that lead to improved 
%accuracy, and smaller training overhead,
%compared with baselines. 
% More results are shown in full version~\cite{full}.

\eat{
\stitle{Exp-1: Effectiveness with Single Measure}. 
Our first experiment evaluates all algorithms in evaluating how well the model's performance can be improved over the dataset(s) they created. As $p_{Acc}$ is the single measure considered by~\metam, and all baseline produce a single table, we (1) compare \modis 
algorithms by selecting the table in the 
Skyline set with best estimated $p_{Acc}$, 
and (2) apply model inference to 
all the datasets, to report the actual %$p_{Acc}$. 
measurement values. 
We show the results for $T_3$ in 
Table~\ref{tab:comparison} (``Original'' 
refers to the measures over the input dataset). 
We find the following. 

\sstab
(1) \modis algorithms outperform 
all the baselines in creating a 
dataset to improve the performance 
in terms of $p_{Acc}$. 
The one with best $p_{Acc}$ and second best 
is obtained by \bimodis and \nomodis, 
respectively, and all \modis methods 
finds data for which $p_{Acc}$ achieves 
$0.94$. 

\sstab
(2) Over the same dataset and for other 
measures, \modis algorithms still outperforms 
the baselines in most cases. 
For example, the result datasets that 
optimize $p_{Fsc}$, $p_{MI}$ and $p_{VIF}$ 
are obtained by \apxmodis, \nomodis and \divmodis, 
respectively; and \bimodis finds a dataset 
that achieves three second-best results 
in $p_{Train}$, $p_{Acc}$ and $p_{Fsc}$. 
This verifies their ability in 
optimize data discovery towards multiple measures 
simultaneously. 

\sstab
(3) All baseline methods perform data augmentation or 
feature selection that leads to a single 
table. The data augmentation 
methods (\metam, \starmie) mainly include more features 
to improve accuracy; and feature selection (\sklearn and \ho) reduce 
them at a cost of accuracy but improved training cost. \modis 
methods are able to balance these trade-offs better 
by {\em explicitly} performing multi-objective optimization. 
Consider $p_{Acc}$ and $p_{Train}$. 
The best result for training cost 
is contributed from \sklearn, yet at a cost of 
lowest model accuracy. As \modis methods are able to 
optimize both measures (among others), 
by making flexible decision to augment 
with new features or 
reduce cells and tuples to make the data smaller, 
they are able to find data with improved 
accuracy as well as smaller training cost, 
compared with baselines. 

\sstab
(4) Despite $p_{Acc}$ is a first-class citizen 
in this comparison, not all baselines improve 
it (given its value over ``Origin'') significantly, except \starmie.   
Yet \starmie improves accuracy at a cost of 
including the most number of 
features ($13$ new ones). Feature selection 
methods (\sklearn and \ho) achieved better 
result on accuracy with much less number of 
features, and consistently showing better 
results in feature correlation measures 
in terms of $p_{Fsc}$, $p_{MI}$ and $p_{VIF}$. 
On the other hand, \modis methods 
{\em explicitly} included these into optimization 
scope with a multi-objective estimator, 
and are able to improve accuracy without 
introducing many new attributes. 
%In fact, we found that \tbf finds 
%dataset at size of onl {\tbf,\tbf} that achieve an 
%accuracy at \tbf. 



%\eetitle{Effectiveness: single performance measure}. 
%\mengying{Add a table}
% We can also be comparable in single-objective

\stitle{Exp-2: Effectiveness with multiple measures}. 
We next evaluate \modis algorithms, \metam and \starmie, using multiple measures in $T_1$ and $T_3$. For each measure $p$ and an  
algorithm, we choose the dataset $D$
with the best estimated measure of $p$ it generates. 
We then retrain the model using $D$ 
to get the true measurement. 
We normalize all the values into 
a same range. 
%relative improvement \kw{rImp(p)}, 
%This gives us a radar graph, with 
% the larger, the better. 
The results are illustrated as radar graphs in Fig.~\ref{fig:mo-eff}. 
The lines ``Original'' 
mark the values of the 
measures in the original data. 

In general, \modis algorithms are able to create  
datasets that generally improve a model in a balanced performance. In particular, \bimodis, \divmodis and 
\nomodis provide top results for multiple measures. \metam 
is optimized to provide good results for a single 
measure, such as accuracy in $T_1$. \starmie 
is not specifically optimized for 
optimizing measures, and provides a balanced 
performance in $T_2$. \apxmodis provides 
in particular better results over $p_{VIF}$, 
a measure for feature correlation, with a possible 
reason that it performs more localized 
reduction only operations that is closer to 
feature selection process. 
}
\eat{
\begin{figure}[tb!]
\addtolength{\subfigcapskip}{-0.08in}
\begin{center}
\subfigure[Task 1 (Classification): $\P_1$]{\label{fig:T1}
{\includegraphics[scale=0.35]{./fig/movie_radar_chart.png}}
} 
%\quad
\subfigure[Task 3 (Regression): $\P_3$]{\label{fig:T3}
{\includegraphics[scale=0.35]{./fig/Avocado_radar_chart.png}}
}
\end{center}
\vspace{-2ex}
\caption{Effectiveness: Multi-Objective Optimization\label{fig:mo-eff}}
\vspace{-3ex}
\end{figure}
}

\begin{figure}[tb!]
\centerline{\includegraphics[width =0.5\textwidth]{fig/Exp1/effective}}
\centering
\vspace{-1ex}
\caption{Effectiveness: Impact of Factors}
 \vspace{-4ex}
\label{fig-effective}
\end{figure}


% \begin{figure}[tb!]
% \addtolength{\subfigcapskip}{-0.04in}
% %\vspace{-1.5ex}
% \begin{center}
% \subfigure[$T_1$: $P_{acc}$ vs. $\epsilon$]{\label{fig-eff-t1-acc-epsilon}
% {\includegraphics[width=3.9cm,height=3.2cm]{./fig/Exp1/effec1}}}
% \quad
% \subfigure[$T_1$: $P_{acc}$ vs. Path length]{\label{fig-eff-t1-acc-maxl}
% {\includegraphics[width=3.9cm,height=3.2cm]{./fig/Exp1/effec2}}}
% \quad
% \subfigure[$T_3$: $P_{MSE}$ vs. $\epsilon$]{\label{fig-eff-t4-mse-epsilon}
% {\includegraphics[width=3.9cm,height=3.2cm]{./fig/Exp1/effec3}}}
% \quad
% \subfigure[$T_3$: $P_{MSE}$ vs. Path length]{\label{fig-eff-t4-mse-maxl}
% {\includegraphics[width=3.9cm,height=3.2cm]{./fig/Exp1/effec4}}}
% \end{center}
% \vspace{-2ex}
% \caption{Effectiveness with Varying Factors \label{fig-eff-factor}}
% \vspace{-3ex}
% \end{figure}

\stitle{Exp-2: Impact factors}. 
We next investigate the \modis methods
under the impact of two factors: $\epsilon$ and the maximum path length (\maxl), as well as the impact of $\alpha$ on \divmodis. 
% and bound of 
% valuated states $N$. 

\eetitle{Varying $\epsilon$}.
Fixing $\maxl$ = 6, we varied $\epsilon$ from $0.5$ to $0.1$ for $T_1$. 
As shown in Fig.~\ref{fig-effective}(a), 
\modis algorithms are able to improve the 
model in $p_{acc}$ better with smaller $\epsilon$, 
as they all ensure to output a $\epsilon$-Skyline set that 
better approximate a Skyline set when $\epsilon$ is set to be 
smaller. In all cases, they achieve a relative improvement 
$\relp(p_{Acc})$ at least 1.07. 
\bimodis and \nomodis perform better 
in recognizing better solutions from both ends in 
reduction and augmentation as smaller $\epsilon$ is enforced. \apxmodis, 
with reduction only, is less sensitive to 
the change of $\epsilon$ due to that larger $\epsilon$ 
may ``trap'' it to  local 
optimal sets from one end. Adding diversification (\divmodis) is able to strike a balance between \apxmodis and \bimodis by enforcing to choose difference datasets out of local 
optimal sets, thus 
improving \apxmodis for smaller $\epsilon$. We choose a smaller range of $\epsilon$ for $T_2$ in Fig.~\ref{fig-effective}(c), as the variance of $p_{F1}$ is small.
As $\epsilon$ varies from $0.1$ to $0.02$, 
\nomodis 
% is able to generate datasets that 
improves F1 score from $0.84$ to $0.91$.

\eetitle{Varying $\maxl$}.  Fixing $\epsilon$ = 0.1, we varied $\maxl$ from $2$ to $6$. Fig.~\ref{fig-effective}(b, d) tells us 
that all \modis algorithms improve the 
task performance
% accuracy of 
% classification better 
for more rounds of processing.  Specifically, \bimodis and \nomodis benefit most 
as bi-directional search allows both to find 
better solution from wider search space as $\maxl$ becomes larger. \apxmodis is less sensitive, 
as the reduction strategy from dense tables 
incurs smaller loss in accuracy.
\divmodis finds datasets that ensure 
best model accuracy when $\maxl$ = 5, yet may 
``lose chance'' to maintain the accuracy, due to 
that the diversification step may update 
the Skyline set with 
less optimal but more different counterparts in 
future levels (\eg when $\maxl$ = $6$).  

% \eetitle{$T_2$: F1 vs. $\epsilon$ and $\maxl$}. Using \open, we report the impact of $\epsilon$ and $\maxl$ for $T_2$ in Fig.~\ref{fig-effective}(c) and (d). 
% The results are consistent with their counterparts 
% in Fig.~\ref{fig-effective}. %(a) and (b). 
% We choose a smaller range of $\epsilon$, as the variance of $p_{F1}$ is small.
% As $\epsilon$ varies from $0.1$ to $0.02$ (with $\maxl$ = $6$), 
% \nomodis 
% % is able to generate datasets that 
% improves F1 score from $0.84$ to $0.91$.
% by $1.5$ times. 
%With $\epsilon$ fixed as $0.1$ and by varying $\maxl$ to $6$, 
%\tbf improves $p_{mse}$ by $\tbf$ times. 

%\mengying{Impacts of $\epsilon$ and maximum length, v.s. accuracy for a classification task and RMSE for a regression task.}

\eetitle{Varying $\alpha$ in \divmodis}.
We demonstrate the effectiveness of \divmodis by adjusting $\alpha$.
A smaller $\alpha$ prioritizes performance, while a larger $\alpha$ emphasizes content diversity, measured by hamming distance.
Fig.~\ref{fig:divmodis}(a)  illustrates \textit{Performance Diversity}, where smaller 
$\alpha$ results in a wider accuracy range with a balanced and stable distribution. Both the mean and median remain centered. As $\alpha$ increases, the accuracy distribution narrows and shifts toward higher values, reflecting the dominance of high-accuracy datasets in the Skyline set.
Fig.~\ref{fig:divmodis}(b) verifies the impact of \textit{Content Diversity}, visualized as the percentage contribution of each \ad.  Larger 
$\alpha$ leads to more evenly distributed contributions. 
The standard deviation values above the heatmap quantify this trend, showing a consistent decrease as $\alpha$ increases, indicating improved balance.

% Fig.~\ref{fig:divmodis}(a) shows ...., at a smaller $\alpha$, it shows a larger range of Accuracy, and a more balanced spread, with both medium and mean are centered and stable, rather than being skewed towards higher values at a larger $\alpha$.
% Fig.~\ref{fig:divmodis}(b) shows ...,
% the distribution of .. is more, balance, as quantified by the Standerd divaition, which is continues lower with a larger $\aplpha$.

\begin{figure}[tb!]
\centerline{\includegraphics[width =0.5\textwidth]{fig/divmodis}}
\centering
\vspace{-1ex}
\caption{Impact of $\alpha$ for \divmodis}
\vspace{-2ex}
\label{fig:divmodis}
\end{figure}

\eat{
\begin{figure}[tb!]
\centerline{\includegraphics[width =0.3\textwidth]{./fig/blank.eps}}
\centering
\caption{Impact of factors}
 \vspace{-1ex}
\label{fig:factors}
\end{figure}
}

\eat{
\mengying{As a method for Table Union Search(TUS), Starmie is effective at finding related tables in a data lake, \tbf. However, its algorithm does not consider measures from a downstream data science task. So it is in expect that \tbf. This highlights the importance of goal-driven data discovery.}
}


\begin{figure}[tb!]
\centerline{\includegraphics[width =0.5\textwidth]{fig/Exp2/t1}}
\centering
\vspace{-1ex}
\caption{Efficiency and Scalabilitiy}
\vspace{-5ex}
\label{fig-efficiency}
\end{figure}

% \begin{figure}[tb!]
% \addtolength{\subfigcapskip}{-0.04in}
% %\vspace{-1.5ex}
% \begin{center}
% \subfigure[$T_1$: Varying $\epsilon$]{\label{fig-effe-t1-epsilon}
% {\includegraphics[width=3.9cm,height=3.2cm]{./fig/Exp2/effi1}}}
% \quad
% \subfigure[$T_1$: Varying $\maxl$]{\label{fig-effe-t1-maxl}
% {\includegraphics[width=3.9cm,height=3.2cm]{./fig/Exp2/effi2}}}
% \quad
% \subfigure[$T_3$:Varying $\epsilon$]{\label{fig-effe-t3-epsilon}
% {\includegraphics[width=3.9cm,height=3.2cm]{./fig/Exp2/effi3}}}
% \quad
% \subfigure[$T_3$: Varying $\maxl$]{\label{fig-effe-t3-maxl}
% {\includegraphics[width=3.9cm,height=3.2cm]{./fig/Exp2/effi4}}}
% \end{center}
% \vspace{-2ex}
% \caption{Efficiency with Varying Factors 
% \label{fig-eff-factor}}
% \vspace{-3ex}
% \end{figure}

\stitle{Exp-3: Efficiency and Scalibility}. 
% To answer RQ3, we next evaluate \modis' efficiency and stability for $T_1$ on \kaggle, 
% considering two major factors $\epsilon$ and $\maxl$.
We next report the 
the efficiency of \modis algorithms 
for task $T_1$ and $T_3$ over \kaggle and \hf, respectively, and 
the impact of factors 
% $\#$ of total attributes, 
% size of the largest active domain $|\ad_m|$, 
$\epsilon$ and $\maxl$. 
We also evaluate their scalability for $T_1$ and $T_5$ in terms of input size.
%\eetitle{Varying $\#$ of attributes}. 
%As shown in Fig.~\ref{fig-effe-t1-epsilon}, 
%\warn{more}.  

\eetitle{Efficiency: Varying $\epsilon$}. 
%Using the same setting as in its effectiveness 
%counterpart (Fig.~\ref{fig-eff-t1-acc-epsilon}), 
%we report the efficiency of \modis algorithms. 
Fixing $\maxl$ = $6$ and varying $\epsilon$ from $0.1$ to $0.5$, 
Fig.~\ref{fig-efficiency} (a)
%, c) 
verifies the following. 
(1) \bimodis, \nomodis and \divmodis take less time as $\epsilon$ increases, as a larger $\epsilon$ provides more chance to prune unnecessary valuations. 
\divmodis has a comparable performance 
with \nomodis, as it mainly benefits from 
the bi-directional strategy, which exploits early pruning and a stream-style placement strategy. 
%On average, \bimodis and \divmodis outperforms 
%\nomodis by \tbf times. 
(2) As shown in Fig.~\ref{fig-efficiency}(a), for  $T_1$, \bimodis, \nomodis, and \divmodis are 2.5, 2, and 2 times faster than \apxmodis on average, respectively. 
\apxmodis takes longer time to explore a larger  universal table with reduct operators. 
It is insensitive to $\epsilon$. We observe that its search from the ``data rich'' end   
may converge faster at high-quality 
$\epsilon$-Skyline sets. 


\eat{
This is because in both cases, 
(1) there are more states with non-$\epsilon$-dominance relation 
to existing solution to be resolved; 
and (2) there are more state nodes to be valuated. 
On the other hand, \apxmodis is most sensitive to $\maxl$ due to 
rapid growth of search space, and \bimodis is much less sensitive to $\maxl$ 
as it mitigates the impact better with bi-directional strategy. 
}
\eetitle{Efficiency: Varying $\maxl$}. 
Fixing $\epsilon$ = 0.2 for task $T_1$ and $\epsilon$ = 0.1 for task $T_3$, we varied $\maxl$ from $2$ to $6$, all \modis algorithms take longer as $\maxl$ increases, as shown in Fig.~\ref{fig-efficiency} (b). 
%d)}
Indeed, larger $\maxl$ results in more states to be valuated, and more non-$\epsilon$-dominance relation to be resolved. \apxmodis is sensitive to $\maxl$  due to the rapid growth of the search space. In contrast, \bimodis mitigates the impact with bi-directional strategy and effective pruning.

\eat{
\eetitle{$T_3$: Varying $\epsilon$ and $\maxl$}. 
We report the efficiency of our algorithms 
for regression task. Our observation is 
consistent with their counterparts for 
$T_1$. This verifies that the efficiency  
of our approach is not very sensitive to
the type of learning tasks or models.
}

\eat{
\begin{figure}[tb!]
\centerline{\includegraphics[width =0.5\textwidth]{../../fig/Exp2/scalability}}
\centering
\vspace{-1ex}
\caption{\revise{Scalability with Input Data Size}}
\vspace{-4ex}
\label{fig-scalability}
\end{figure}
}

\eetitle{Scalability}. We varied the number of total 
attributes $|A|$ and size of the largest active domain $|\ad|$. We perform $k$-means clustering over the tuples of the universal table with $k = |\ad|$, and extended operators with range queries to control $|\ad|$. %Fig.~\ref{fig-scalability} 
Fig.~\ref{fig-efficiency} (c) and (d) show that all \modis algorithms take more time for larger $|A|$ and $|\ad|$. \bimodis scales best due to 
bi-directional strategy. 
\divmodis remains more efficient 
than \apxmodis, indicating affordable 
overhead from diversification.

While our algorithms scale well with $\vert A \vert$ and $\vert \ad \vert$, high-dimensional datasets may present challenges due to the search space growth. Dimensionality reduction such as PCA or feature selection, or correlation-based pruning (to identify and eliminate highly correlated or redundant features), can be tailored to specific tasks to mitigate these challenges.
%For $T_5$, leveraging the graph's structure, we reduced the input feature space from 34 to 10 by aggregating attributes from similar types of relations, 
% in \kizoo,
%such as combining multiple training records of an ML model, while preserving full information. 
% Future work will explore integrating these methods to further improve efficiency in high-dimensional settings.

%This indicates \modis in maintaining scalability and managing larger datasets.
% Due to limited space, we 
% report the scalability results in~\cite{full}.


% Report our methods in figure, baselines' in text.



\eat{
\begin{figure}[tb!]
\centerline{\includegraphics[width =0.5\textwidth]{./fig/movie_radar_chart.png}}
\centering
\caption{Efficiency}
 \vspace{-1ex}
\label{fig:efficiency}
\end{figure}
}

\vspace{1ex}
\stitle{Exp-4: Case study}. We next report two real-world case studies to illustrate the application scenarios of \modis. 
% \footnote{Cases are made anonymous to preserve double-blind review.} 

\eetitle{(1) ``Find data with models''}. A material science team trained a random forest-based classifier to identify peaks in 2D X-ray diffraction data. They seek more datasets to improve the model's {\em accuracy, training cost, and F1 score} for downstream fine-tuning. Original X-ray datasets and models are uploaded to a crowd-sourced X-ray data platform we deployed~\cite{wang2022crux} with 
best performance of $\textless0.6435, 3.2, 0.77\textgreater$.
Within available X-ray datasets, \bimodis created three datasets $\{D_1, D_2, D_3\}$ and achieved the best  performance of 0.987, 2.88, and 0.91, respectively. We set \metam to optimize F1-score, and achieved a performance score of $\textless 0.972, 3.51, 0.89\textgreater$ over its output dataset.
Fig.~\ref{fig:cases} illustrates 
such a case that is manually validated with ground-truth from 
a third-party institution. 


\eetitle{(2) Generating test data for model evaluation}. 
We configure \modis algorithms to generate test datasets for model benchmarking, where specific performance criteria can be posed~\cite{ventura2021expand}. Utilizing a trained scientific image classifier from \kaggle, and a pool of image feature datasets $\D$ from \hf with $75$ tables, $768$ columns, and over $1000$ rows. 
We request \bimodis to generate  
datasets over which the classifier demonstrates: ``accuracy $>$ 0.85'' and ``training cost $<$ 30s.'' 
\bimodis successfully generated $3$ datasets to be chosen from within $15$ seconds, with performance $\textless0.95, 0.27\textgreater$, $\textless0.94, 0.26\textgreater$ and $\textless0.90, 0.25\textgreater$, as in Fig.~\ref{fig:cases}.


\begin{figure}[tb!]
\centerline{\includegraphics[width=\linewidth]{fig/case_study}}
\centering
\caption{Case 1 (left): Discover Datasets for Materials Peak Classification Analysis. Case 2 (right): Test Data Generation for Model Performance Benchmarking}
\vspace{-2ex}
\label{fig:cases}
\end{figure}

% Due to limited space, 
% We report the details of 
% more complementary tests and impact of factors to \modis in 
% full version~\cite{full}. 


% A material science team has a trained random forest-based classifier to recognize peaks in a 2-dimensional angle-intensive X-ray diffraction data. They hope to find good additional datasets for which the model has improved performance in terms of $F_1$ score, training cost, and accuracy for downstream fine-tuning. The team has uploaded their original X-ray datasets and models to a crowd-sourced X-ray data collection platform we have deployed, with a performance vector $\textless 0.6435, 3.2, 0.77 \textgreater$. 
% Over a set of shared 
% X-ray diffraction datasets from other facilities, \bimodis created three new datasets 
% $\{D_1, D_2, D_3\}$, which optimizes the model in each 
% measure to 0.975, 3.07, and 0.89, respectively. 
% A test 
% of the model over the datasets is illustrated in 
% Fig.~\ref{fig-case}, which is manually validated to 
% be accurate with ground-truth given by a database provided by 
% a third-party international institution. 


% In the second case study, we hows that our configurable \modis paradigm readily fits the need for generating data (from scratch) for model benchmarking with required performances~\cite{vicente2022benchmark}. 
% Given a trained scientific image classifier from \kaggle, we processed a pool of image features $\D$ from \hf with 75 tables and, in total, 768 columns and more than 1000 rows as data sources. We set the ranges of training time requirement to be ``accuracy > 0.85'', ``training cost <30s''.
% %``Fisher score''>0.45, ``MI''>0.38, 
% %and ``VIF<1.3''. 
% By setting these as {\em hard constraints} for \bimodis, 
% we found that it outputs a set of 5 datasets within $30$ seconds, over which the model's record on each measure is 0.95 in accuracy and 27s in training costs. This maps to 
% a set of test images that serve as better 
% training or testing data, with an example illustrated in Fig.~\ref{fig:cases}. 

\eat{
Given a trained regression model 
\kw{LRavocado}, we collect a pool of datasets $\D$ from \hf with \tbf 
tables and in total \tbf columns and \tbf rows as data sources. 
We set the ranges of training time requirement to be ``<1s'', ``accuracy'' >0.88, ``Fisher score''>0.45, ``MI''>0.38, 
and ``VIF<1.3''. By setting these as {\em hard constraints} for \bimodis, 
we found that it outputs a set of \tbf datasets within $30$ seconds, over which the model's record on each measure is \tbf, \tbf and \tbf. 
}
 
\eat{\mengying{Our algorithm generates a set of recommended datasets based on a model and user-defined metrics with expected ranges in just one round.}
}

