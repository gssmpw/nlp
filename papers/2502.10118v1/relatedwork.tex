\section{Related Works}
\subsection{Vision-Language Models (VLMs)}
A growth of interest in VLMs has continued due to the wide availability of multi-modal data on the web. Foundation VLMs can be applied to a range of tasks in a zero-shot manner. Notably, CLIP \cite{radford2021learning} jointly pre-trains an image encoder and a text encoder by maximizing and minimizing the cosine similarity of correct and incorrect image-text pair embeddings respectively using image-text contrastive (ITC) loss. In contrast, BLIP \cite{li2022blip} uses both ITC and image-text matching (ITM) loss for enhanced image-text data representation. Additionally, the BLIP \cite{li2022blip} \textit{captioner} uses language modeling (LM) loss for auto-regressive image caption generation along with a filter, \textit{capfilt} to improve the quality of image-text pairs for training.  

Flamingo \cite{alayrac2022flamingo} shows remarkable zero-shot ability in image captioning, visual question-answering (VQA), and image-text retrieval (ITR) tasks by leveraging the few-shot learning ability of pre-trained vision-only and language-only models. It simply interleaves input visual data with task-specific text examples, producing free-form texts for unseen visual data. Another general-purpose model, BEIT3 \cite{wang2022image} with Multiway Transformer structure, uses different types of modality experts to perform fusion and modality-specific training. A masked modeling objective on images only and image-text pairs is performed for computer vision tasks (\textit{e.g.}, image classification, semantic segmentation, object detection) and vision-language tasks (\textit{e.g.}, VQA), respectively. Whereas the VQA task uses a fused encoder for image-text pairs, the ITR task encodes images and texts independently with ITC loss. Lastly, sequence-to-sequence learning is applied to generate texts from images for the image captioning task. Inspired by these previous works, we propose a meta-VLM model that utilizes a pre-trained BLIP \cite{li2022blip} image captioning module to generate enhanced textual representations, which can later serve as useful data for various downstream tasks.

\subsection{Hierarchical Representation}\label{rw:hier}
Identifying and extracting regions of interest within images is crucial for a hierarchical representation. The most intuitive way to achieve this would typically 
involve the use of object detectors \cite{yao2019hierarchy,cornia2020meshed,zhang2021rstnet}. However, the heavy computational demands of object detectors inevitably leads to inefficiency during the inference stage \cite{yao2019hierarchy,cornia2020meshed,zhang2021rstnet}. In response, recent works sought to replace these cumbersome detectors by adopting visual concepts in the form of object tags \cite{fang2022injecting,shukor2022efficient} as an alternative. However, this detector-free approach is contingent upon the availability of object-specific data within the dataset. Employing pre-trained models is a more efficient way to identify areas of interest within images. GradCAM \cite{selvaraju2017grad} highlights essential regions that the pre-trained models used to predict any target concept using its gradients with respect to feature map activations of the final convolutional layer. DINOv2 \cite{oquab2023dinov2} capitalizes on existing self-supervised pre-trained models to generate robust, all-purpose visual features, supporting a wide array of tasks ranging from image-level classification to pixel-level segmentation. However, the image regions/features delineated by GradCAM/DINOv2 tend to show saliency for specific tasks and are unable to capture the full spectrum of visual representations. Conversely, SAM \cite{kirillov2023segany} intricately segments every semantically significant component of an image into high-quality segmentation masks generated by prompting with various inputs such as point, box, mask, or free-form text, unrestricted with types of tasks. In our framework, we integrate SAM \cite{kirillov2023segany} to create semantically meaningful segmentation masks for an entire image automatically.


Several prior studies have incorporated the principles of hierarchy or multi-scale representation into their model architectures, aiming to enhance the alignment between images and texts \cite{ji2021step,Shao2023ICCV,shukor2022efficient}. SHAN \cite{ji2021step} deconstructs the image-text matching process into two distinct facets: fragment-level and context-level alignments enabling matches across three different scopes: local-to-local, global-to-local, and global-to-global. HiVLP \cite{Shao2023ICCV} leverages both low- and high-dimensional features to represent coarse and fine details. ViCHA \cite{shukor2022efficient} aligns images and texts across various layers of neural network encoders with the underlying assumption that each layer reflects varying semantic levels. Unlike these approaches, we divide the segmentation masks hierarchically and use the embeddings of the extracted individual image patches for caption generation.

\subsection{Caption Evaluation}\label{rw:eval}
Common image captioning evaluation metrics, including BLEU \cite{bleu}, METEOR \cite{banerjee-lavie-2005-meteor}, ROUGE \cite{lin-2004-rouge}, and CIDEr \cite{cider} scores are primarily n-gram approaches that assess the quality of generated captions by considering their overlap with human-generated captions. Most SOTA VLMs frequently exhibit promising scores across these conventional evaluation metrics. However, these metrics are limited in their capabilities to measure the diversity of the generated captions. This limitation leads to a bias in these models towards generating an "average" and "safe" caption reflecting the most basic information in the image, rendering them less informative than human-generated captions. To address this gap, we incorporate several diversity metrics, including mBLEU-4, Div-2 \cite{aneja2019sequential}, and the proposed pairwise cosine distance (PCD), along with semantic integrity and relatedness scores to ensure that the captions generated by our framework are not only diverse but also meaningful and directly relevant to the given image.