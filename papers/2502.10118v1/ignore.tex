\begin{abstract}
The expressive power of Visual-Language Models (VLMs) has significantly improved through integration with pre-trained Large Language Models (LLMs). Effective utilization of LLMs in vision-language tasks requires comprehensive textual representations of the visual data. However, despite the recent advancements, we find that BLIP-based VLMs, though much more computationally efficient, typically suffer from the mode-collapse problem, focusing predominantly on a single aspect of an image. Consequently, the generated captions lack diversity and are less informative than the image descriptions given by humans or other VLMs (\textit{e.g.}, GPT, LLaVA). In this paper, we propose a simple training-free method, HBoP, to enhance captioning diversity while maintaining efficiency by automatically selecting several sub-sections of the input image embeddings for the captioning module. We evaluate the captioning ability of various VLMs and offer insights into mitigating mode collapse in pre-trained encoder-based VLMs, facilitating richer and more informative image captions.
\end{abstract}

\section{Introduction}
Vision-language models (VLMs) exhibit promising potential in understanding the intricate connections between images and text \cite{alayrac2022flamingo,li2022blip,Align_before_Fuse,radford2021learning,wang2022ofa,wang2022image}. However, their success is highly dependent on extensive pre-training and task-specific fine-tuning. Alternatively, employing large language models (LLMs) for vision-language tasks has gained traction in recent years \cite{tiong2022plug,guo2023images,tang2023context,liu2023hidden}. Yet, the effectiveness of this method is deeply rooted in the availability of comprehensive textual representations of the visual data \cite{xu2021towards}. Similar to other generative models, VLMs tend to suffer from the \textit{mode collapse} \cite{chen2022learning} problem during caption generation, often only focusing on one aspect of the image and leaving a wealth of fine-grained information unexplored \cite{aneja2019sequential,chen2022learning,bianco2023improving}. This phenomenon suggests that VLMs fail to accurately represent the richness and diversity of information present in the image \cite{yuksekgonul2022and} during caption generation. This necessitates the exploration of more efficient and effective methods for generating textual representations that can accurately reflect the entire scope of the image while ensuring the diversity and comprehensiveness of these representations.


The motivation behind our approach stems from the idea that image data encapsulates a wealth of information that can be interpreted at different levels of granularity to construct adequate textual representations, which can further be used for downstream vision-language tasks. Currently, SOTA VLMs generate conservative captions that merely reflect an abstract comprehension of the image content. For instance, as illustrated in Figure \ref{fig:BLIP-2vsHBoP}, the captions produced by BLIP-2 \cite{li2023blip} for two different images, both depicting scenes from a baseball match, are remarkably similar. This similarity highlights the challenges that VLMs face in fully representing the intricacies of visual content with text. In contrast, captions generated by our proposed framework, Hierarchical Bags of Phrases (HBoP), creates textual representations (\textit{i.e.}, HBoPhrases\footnote{The captions generated with the HBoP framework are referred to as HBoPhrases throughout this paper.}) that can clearly distinguish between the two images. PnP-VQA \cite{tiong2022plug} and Img2LLM \cite{guo2023images} adopt a methodology similar to ours, leveraging the capabilities of existing VLMs and LLMs to enhance the performance of visual question-answering tasks. While these works mitigate the fine-tuning requirements, they are limited in scope, focusing solely on a single vision-language task (Fig. \ref{fig:PNPvsHBoP}) and introducing additional computational overhead at inference time \cite{tiong2022plug}. This narrow focus and their inability to ensure the quality of the generated captions limit their applicability.  




Our framework integrates pre-trained foundation models with no additional training to enhance the diversity in caption generation. Inspired by works \cite{ji2021step,Shao2023ICCV,shukor2022efficient} that suggest a hierarchical approach shows improved image understanding in contrast to holistic image analysis, our framework is designed to dissect images on both a global scale and a more focused regional basis. Additionally, it incorporates a fine-grained layer that functions as image tags to supply supplementary information about the image. HBoP is structurally organized into three core components: 1. Image Segmentation Module (ISM), 2. Hierarchical Composition Module (HCM), and 3. Image Captioning Module (ICM). Initially, we segment an entire image into meaningful image patches using segmentation, which is then followed by clustering of the generated segmentation masks, that are parsed to create an explicit hierarchical comprehension of the image. Subsequently, the image captioning module generates relevant captions based on the hierarchically divided segmentation masks, effectively translating the image data into descriptive and diverse captions/textual representations.
 
We validate the efficacy of the embedding sampling method by evaluating the generated captions in terms of the conventional diversity metrics, such as mBLEU-4  and n-gram diversity \cite{aneja2019sequential} and the proposed pairwise cosine distance (PCD) . We demonstrate that HBoPhrases closely mirror human-generated captions in terms of these diversity metrics, and at the same time, we achieve the best diversity results compared to other models built specifically for diverse image captioning (\textit{e.g.}, Seq-CVAE \cite{aneja2019sequential} and ModeCap \cite{chen2022learning}). Additionally, we also demonstrate that HBoPhrases perform similarly to gold captions in terms of semantic integrity and relatedness to the image. By adopting a hierarchical framework, HBoP generates meaningful and diverse captions that enhance image representation in a textual format beyond simple high-level abstraction.


\section{Related Works}
\subsection{Vision-Language Models (VLMs)}
A growth of interest in VLMs has continued due to the wide availability of multi-modal data on the web. Foundation VLMs can be applied to a range of tasks in a zero-shot manner. Notably, CLIP \cite{radford2021learning} jointly pre-trains an image encoder and a text encoder by maximizing and minimizing the cosine similarity of correct and incorrect image-text pair embeddings respectively using image-text contrastive (ITC) loss. In contrast, BLIP \cite{li2022blip} uses both ITC and image-text matching (ITM) loss for enhanced image-text data representation. Additionally, the BLIP \cite{li2022blip} \textit{captioner} uses language modeling (LM) loss for auto-regressive image caption generation along with a filter, \textit{capfilt} to improve the quality of image-text pairs for training.  

Flamingo \cite{alayrac2022flamingo} shows remarkable zero-shot ability in image captioning, visual question-answering (VQA), and image-text retrieval (ITR) tasks by leveraging the few-shot learning ability of pre-trained vision-only and language-only models. It simply interleaves input visual data with task-specific text examples, producing free-form texts for unseen visual data. Another general-purpose model, BEIT3 \cite{wang2022image} with Multiway Transformer structure, uses different types of modality experts to perform fusion and modality-specific training. A masked modeling objective on images only and image-text pairs is performed for computer vision tasks (\textit{e.g.}, image classification, semantic segmentation, object detection) and vision-language tasks (\textit{e.g.}, VQA), respectively. Whereas the VQA task uses a fused encoder for image-text pairs, the ITR task encodes images and texts independently with ITC loss. Lastly, sequence-to-sequence learning is applied to generate texts from images for the image captioning task. Inspired by these previous works, we propose a meta-VLM model that utilizes a pre-trained BLIP \cite{li2022blip} image captioning module to generate enhanced textual representations, which can later serve as useful data for various downstream tasks.

\subsection{Hierarchical Representation}\label{rw:hier}
Identifying and extracting regions of interest within images is crucial for a hierarchical representation. The most intuitive way to achieve this would typically 
involve the use of object detectors \cite{yao2019hierarchy,cornia2020meshed,zhang2021rstnet}. However, the heavy computational demands of object detectors inevitably leads to inefficiency during the inference stage \cite{yao2019hierarchy,cornia2020meshed,zhang2021rstnet}. In response, recent works sought to replace these cumbersome detectors by adopting visual concepts in the form of object tags \cite{fang2022injecting,shukor2022efficient} as an alternative. However, this detector-free approach is contingent upon the availability of object-specific data within the dataset. Employing pre-trained models is a more efficient way to identify areas of interest within images. GradCAM \cite{selvaraju2017grad} highlights essential regions that the pre-trained models used to predict any target concept using its gradients with respect to feature map activations of the final convolutional layer. DINOv2 \cite{oquab2023dinov2} capitalizes on existing self-supervised pre-trained models to generate robust, all-purpose visual features, supporting a wide array of tasks ranging from image-level classification to pixel-level segmentation. However, the image regions/features delineated by GradCAM/DINOv2 tend to show saliency for specific tasks and are unable to capture the full spectrum of visual representations. Conversely, SAM \cite{kirillov2023segany} intricately segments every semantically significant component of an image into high-quality segmentation masks generated by prompting with various inputs such as point, box, mask, or free-form text, unrestricted with types of tasks. In our framework, we integrate SAM \cite{kirillov2023segany} to create semantically meaningful segmentation masks for an entire image automatically.


Several prior studies have incorporated the principles of hierarchy or multi-scale representation into their model architectures, aiming to enhance the alignment between images and texts \cite{ji2021step,Shao2023ICCV,shukor2022efficient}. SHAN \cite{ji2021step} deconstructs the image-text matching process into two distinct facets: fragment-level and context-level alignments enabling matches across three different scopes: local-to-local, global-to-local, and global-to-global. HiVLP \cite{Shao2023ICCV} leverages both low- and high-dimensional features to represent coarse and fine details. ViCHA \cite{shukor2022efficient} aligns images and texts across various layers of neural network encoders with the underlying assumption that each layer reflects varying semantic levels. Unlike these approaches, we divide the segmentation masks hierarchically and use the embeddings of the extracted individual image patches for caption generation.

\subsection{Caption Evaluation}\label{rw:eval}
Common image captioning evaluation metrics, including BLEU \cite{bleu}, METEOR \cite{banerjee-lavie-2005-meteor}, ROUGE \cite{lin-2004-rouge}, and CIDEr \cite{cider} scores are primarily n-gram approaches that assess the quality of generated captions by considering their overlap with human-generated captions. Most SOTA VLMs frequently exhibit promising scores across these conventional evaluation metrics. However, these metrics are limited in their capabilities to measure the diversity of the generated captions. This limitation leads to a bias in these models towards generating an "average" and "safe" caption reflecting the most basic information in the image, rendering them less informative than human-generated captions. To address this gap, we incorporate several diversity metrics, including mBLEU-4, Div-2 \cite{aneja2019sequential}, and the proposed pairwise cosine distance (PCD), along with semantic integrity and relatedness scores to ensure that the captions generated by our framework are not only diverse but also meaningful and directly relevant to the given image.


