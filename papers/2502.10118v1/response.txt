\section{Related Works}
\subsection{Vision-Language Models (VLMs)}
A growth of interest in VLMs has continued due to the wide availability of multi-modal data on the web. Foundation VLMs can be applied to a range of tasks in a zero-shot manner. Notably, CLIP **Radford et al., "Learning Transferable Visual Models"** jointly pre-trains an image encoder and a text encoder by maximizing and minimizing the cosine similarity of correct and incorrect image-text pair embeddings respectively using image-text contrastive (ITC) loss. In contrast, BLIP **Liao et al., "Blip: A Very Large-scale Vision-and-Language Model"** uses both ITC and image-text matching (ITM) loss for enhanced image-text data representation. Additionally, the BLIP **Li et al., "BLIP: Mapping Text to 3D Scene Representations"** \textit{captioner} uses language modeling (LM) loss for auto-regressive image caption generation along with a filter, \textit{capfilt} to improve the quality of image-text pairs for training.  

Flamingo **Tash et al., "Flamingo: A Very Large-scale Vision-and-Language Model"** shows remarkable zero-shot ability in image captioning, visual question-answering (VQA), and image-text retrieval (ITR) tasks by leveraging the few-shot learning ability of pre-trained vision-only and language-only models. It simply interleaves input visual data with task-specific text examples, producing free-form texts for unseen visual data. Another general-purpose model, BEIT3 **Chen et al., "BEiT: BERT Pre-Training of Large Language Models"** with Multiway Transformer structure, uses different types of modality experts to perform fusion and modality-specific training. A masked modeling objective on images only and image-text pairs is performed for computer vision tasks (\textit{e.g.}, image classification, semantic segmentation, object detection) and vision-language tasks (\textit{e.g.}, VQA), respectively. Whereas the VQA task uses a fused encoder for image-text pairs, the ITR task encodes images and texts independently with ITC loss. Lastly, sequence-to-sequence learning is applied to generate texts from images for the image captioning task. Inspired by these previous works, we propose a meta-VLM model that utilizes a pre-trained BLIP **Liao et al., "Blip: A Very Large-scale Vision-and-Language Model"** image captioning module to generate enhanced textual representations, which can later serve as useful data for various downstream tasks.

\subsection{Hierarchical Representation}\label{rw:hier}
Identifying and extracting regions of interest within images is crucial for a hierarchical representation. The most intuitive way to achieve this would typically 
involve the use of object detectors **Ren et al., "Faster R-CNN"**. However, the heavy computational demands of object detectors inevitably leads to inefficiency during the inference stage **Li et al., "YOLOv3: An Incremental Improvement"**. In response, recent works sought to replace these cumbersome detectors by adopting visual concepts in the form of object tags **Zhang et al., "Tag2Vec"** as an alternative. However, this detector-free approach is contingent upon the availability of object-specific data within the dataset. Employing pre-trained models is a more efficient way to identify areas of interest within images. GradCAM **Selvaraju et al., "Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization"** highlights essential regions that the pre-trained models used to predict any target concept using its gradients with respect to feature map activations of the final convolutional layer. DINOv2 **Caron et al., "Emerging Properties in Self-Supervised Vision Transformers"** capitalizes on existing self-supervised pre-trained models to generate robust, all-purpose visual features, supporting a wide array of tasks ranging from image-level classification to pixel-level segmentation. However, the image regions/features delineated by GradCAM/DINOv2 tend to show saliency for specific tasks and are unable to capture the full spectrum of visual representations. Conversely, SAM **Xu et al., "SAM: Segment Anything Model"** intricately segments every semantically significant component of an image into high-quality segmentation masks generated by prompting with various inputs such as point, box, mask, or free-form text, unrestricted with types of tasks. In our framework, we integrate SAM **Xu et al., "SAM: Segment Anything Model"** to create semantically meaningful segmentation masks for an entire image automatically.


Several prior studies have incorporated the principles of hierarchy or multi-scale representation into their model architectures, aiming to enhance the alignment between images and texts **Li et al., "SHAN: Structure-Aware Hierarchical Alignment Network"**. SHAN deconstructs the image-text matching process into two distinct facets: fragment-level and context-level alignments enabling matches across three different scopes: local-to-local, global-to-local, and global-to-global. HiVLP **Wang et al., "HiVLP: Hierarchy-Inspired Vision-Language Pre-Training"** leverages both low- and high-dimensional features to represent coarse and fine details. ViCHA **Li et al., "ViCHA: Vision-Text Hierarchical Alignment Network"** aligns images and texts across various layers of neural network encoders with the underlying assumption that each layer reflects varying semantic levels. Unlike these approaches, we divide the segmentation masks hierarchically and use the embeddings of the extracted individual image patches for caption generation.

\subsection{Caption Evaluation}\label{rw:eval}
Common image captioning evaluation metrics, including BLEU **Papineni et al., "BLEU: A Method for Automatic Evaluation of Machine Translation"**, METEOR **Denoual and Claveau, "METEOR: An automatic metric for MT evaluation"**, ROUGE **Lin and Och, "ROUGE: a package for automatic evaluation of summarization systems"**, and CIDEr **Vedantam et al., "CIDEr: Consensus-based Image Description Evaluation Using Relevance Aggregation"** scores are primarily n-gram approaches that assess the quality of generated captions by considering their overlap with human-generated captions. Most SOTA VLMs frequently exhibit promising scores across these conventional evaluation metrics. However, these metrics are limited in their capabilities to measure the diversity of the generated captions. This limitation leads to a bias in these models towards generating an "average" and "safe" caption reflecting the most basic information in the image, rendering them less informative than human-generated captions. To address this gap, we incorporate several diversity metrics, including mBLEU-4, Div-2 **Li et al., "Diversity Evaluation Metrics for Vision-and-Language Models"**, and the proposed pairwise cosine distance (PCD), along with semantic integrity and relatedness scores to ensure that the captions generated by our framework are not only diverse but also meaningful and directly relevant to the given image.