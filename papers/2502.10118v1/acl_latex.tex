% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
% \usepackage[review]{acl}
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{multirow} 
\usepackage{booktabs}
\usepackage{multicol}
\usepackage{colortbl}
\usepackage{tikz,pgfplots}
\usepackage{tcolorbox}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{hyperref}
\usepackage{silence}
\usepackage{fontawesome5}
\usepackage{MnSymbol}
\usepackage{amssymb}
\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%


\WarningFilter{latex}{`!h' float specifier changed to `!ht'}
\newcommand{\eunki}[1]{\textcolor{cyan}{#1}}
\pgfplotsset{compat=1.18}
% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Image Embedding Sampling Method for Diverse Captioning}


\author{Sania Waheed\thanks{\space{ } Equal contribution}~~ Na Min An\footnotemark[1]
\\
 University of Southhampton, KAIST
\\
 sw1m24@soton.ac.uk, naminan@kaist.ac.kr
}

\begin{document}
\maketitle



\begin{abstract}
Image Captioning for state-of-the-art VLMs has significantly improved over time; however, this comes at the cost of increased computational complexity, making them less accessible for resource-constrained applications such as mobile devices and assistive technologies. Alternatively, smaller VLMs prioritize high-level scene descriptions, overlooking finer details that contribute to a richer understanding of an image. In this paper, we introduce a training-free framework that enhances caption diversity and informativeness by explicitly attending to distinct image regions using a comparably small VLM, BLIP, as the backbone. Our approach leverages structured segmentation to produce hierarchical representations that capture both global and localized semantics. Without requiring additional model training, we demonstrate that our method allows smaller VLMs to achieve performance comparable to larger models in terms of image-caption alignment, semantic integrity, and diversity. We evaluate our framework on MSCOCO, Flickr30k, and Nocaps test datasets, achieving a Div-2 score of 0.735, 0.750, and 0.748 for each dataset respectively, while maintaining strong image-caption relevancy and semantic integrity with the human-annotated captions.
\end{abstract}



\section{Introduction}
\label{ch:intro}


\begin{figure*}[t!]
\centering
\includegraphics[width=\textwidth]{Figures/Fig1.pdf}
\caption{Comparison of captions generated by BLIP, HBoP, and human annotations. The images are overlaid with GradCAM heatmaps to highlight the regions focused on by the pretrained image-text matching model \cite{li2022blip}. HBoP captions exhibit greater diversity compared to BLIP captions and are closer to human-annotated gold captions.}
\label{fig:BLIP-2vsHBoP}
\end{figure*}



Visual-Language Models (VLMs) have seen rapid advancements in image captioning, benefiting from increasingly sophisticated architectures and larger training datasets \cite{alayrac2022flamingo, li2022blipbootstrappinglanguageimagepretraining, radford2021learningtransferablevisualmodels, wang2022imageforeignlanguagebeit}. State-of-the-art large-scale models generate highly detailed and diverse captions, yet their extensive computational requirements can be prohibitive in resource-constrained settings. Conversely, smaller VLMs, while more efficient, often prioritize dominant visual elements and overlook fine-grained details, resulting in captions that lack the depth and specificity seen in human-generated captions \cite{aneja2019sequentiallatentspacesmodeling, bianco2023improvingimagecaptioningdescriptiveness, chen2023learningdistinctrepresentativestyles, yuksekgonul2022and}.

Inspired by previous work \cite{ji2021step, Shao2023ICCV, shukor2022efficient} that demonstrates the advantages of hierarchical approaches in image understanding, our method leverages structured segmentation to capture both global and regional aspects of an image. We sample segmentation-driven embeddings to explicitly attend to distinct image regions while preserving contextual relationships, generating captions at multiple levels of granularity. This approach enables smaller VLMs to achieve performance comparable to larger models in terms of caption diversity and image-caption alignment.

We validate our approach, namely, \textbf{HBoP} - \textbf{H}ierarchical \textbf{B}ags \textbf{o}f \textbf{P}hrases, by evaluating generated captions for MSCOCO, Flickr30k, and Nocaps datasets on conventional diversity metrics such as mBLEU-4, n-gram diversity \cite{aneja2019sequential}, and newly presented pairwise cosine distance (PCD). Our findings show that structured caption generation effectively improves diversity while maintaining relevancy with images and human-generated captions (\textit{compare} BLIP \cite{li2022blip}, HBoP, and gold captions in Figure~\ref{fig:BLIP-2vsHBoP}). 

\section{Related Works}
\label{ch:relatedwork}
Vision-language models have demonstrated remarkable capabilities in various multi-modal tasks, with caption generation being a primary benchmark for evaluating their performance. Models such as CLIP \cite{radford2021learningtransferablevisualmodels}, Flamingo \cite{alayrac2022flamingo}, and BLIP-2 \cite{li2023blip} leverage contrastive learning and large-scale pre-training to improve vision-language alignment. Although these approaches improve caption fluency and coherence, they often generate high-level scene descriptions without capturing fine-grained details, limiting their utility for applications requiring detailed image understanding. Traditional captioning models treat images holistically, often overlooking fine-grained hierarchical details \cite{xu2021towards}, unless trained with an explicit training objective of improving diversity, such as ModeCap \cite{chen2022learning} which explores various modes in the training corpus and Seq-CVAE \cite{aneja2019sequential} that uses latent variables to generate every word within a sentence.

Inspired by hierarchical representation techniques \cite{ji2021step, Shao2023ICCV,shukor2022efficient}, our approach samples the latent image embeddings corresponding to structured segmentation to generate captions that reflect multiple levels of contextual details. PnP-VQA \cite{tiong2022plug} proposes a similar methodology but focuses primarily on visual question answering. While PnP-VQA leverages image segmentation to generate captions, its sampling approach results in less meaningful captions due to its reliance on high-activation regions extracted using Grad-CAMs \cite{selvaraju2017grad}. 



\begin{figure*}[t!]
\centering
\includegraphics[width=\textwidth]{Figures/Fig2.pdf}
\caption{The proposed HBoP framework consists of three components: (1) Image Segmentation Module (ISM), (2) Hierarchical Composition Module (HCM), and (3) Image Captioning Module (ICM). HBoP controls caption granularity by selecting meaningful patch embeddings of varying sizes from the segmentation model.}
\label{fig:HBoP}
\end{figure*}

\section{Methodology}
\label{ch:ourmethod}


In this section, we introduce our proposed framework, HBoP (depicted in Fig~\ref{fig:HBoP}), a modular architecture that uses pre-trained segmentation and captioning models. We show that HBoP ensures multiple levels of captions (\textit{i.e.}, global, regional, fine-grained) by inducing a hierarchical structure for image understanding.


\subsection{Image Segmentation Module (ISM)}\label{method:ims}
The first component of HBoP, ISM, selects patch embeddings ($E_{X}$) corresponding to image regions ($X=(X_1, X_2, ..., X_n)$) from the original image embeddings extracted using a Vision Transformer (ViT) \cite{dosovitskiy2020image} encoder. These regions are selected based on the segmentation masks produced by the state-of-the-art Segment Anything model (SAM) \cite{kirillov2023segany}. If we select a set of $p$ segmentation masks for the image, the resulting masks for the selected image regions would be: $M_{X} = \{\,M_{X_1}, M_{X_2}, ..., M_{X_p}\,\} = \text{SAM}(X), X \in \mathbb{R}^{H \times W \times C}$, where $H$, $W$, and $C$ represent the height, width, and channels of $X$.


\subsection{Hierarchical Composition Module (HCM)}\label{method:hcm}

The second component, HCM, is a key component that can control the level of captions. Specifically, we present three types of captions that can be derived using HCM. 

\paragraph{\textit{Global/Fine-grained level} captions} The global segmentation masks ($M_{G}$) are selected by choosing the top-$k$ (5 in our case) largest segmentation masks from $M_X$ after applying non-maximum suppression (NMS) \cite{hosang2017learning}:
\begin{align*}
    M_G &= \{M_{g_1}, M_{g_2}, ..., M_{n_g} \}, \\
    M_{g_i} &= \text{NMS}(\text{Top-}k (M_X)), \quad i=1, ..., n_g
\end{align*}
NMS removes multiple segmentation masks with overlapping, similar contexts using the Intersection over Union (IoU) and predicted confidence from SAM. The remaining masks, after applying NMS, can also be used to generate fine-grained captions (discussed in Appendix~\ref{app:fine}): 
\begin{align*}
    M_F &= \{M_{f_1}, M_{f_2}, ..., M_{n_f} \}, \\
    M_{f_i} &= \text{NMS}(M_X) \setminus M_G, \quad i=1, ..., n_f
\end{align*}

\paragraph{\textit{Regional level} captions} To create regional-level segmentation masks, $M_{R}$, we use $K$-means clustering to partition all the segmentation masks ($M_X$) and apply NMS to each cluster individually:
\begin{align*}
     M_R &= \{M_{r_1}, M_{r_2}, ..., M_{K} \}, \\
     M_{r_i} &= \text{NMS}(\text{K-means}(M_X)), i=1, ..., K
\end{align*}


The hierarchical segmentation masks ($M_{g}$, $M_{r}$ and $M_{f}$) are used to extract relevant patch embeddings, $E_{g}$, $E_{r}$ and $E_{f}$ using $E_{X}$ from the first stage. We extract ($\odot$) the corresponding embeddings by concatenating the extracted patch embeddings of different levels. Thus, the final selected image embeddings can be categorized as:

\begin{align*}
    E_{G} &= \{\,E_{g_1}, E_{g_2}, ..., E_{g_{n_g}}\,\}, E_{g_i} = E_X \odot M_{g_i}\\
    E_{R} &= \{\,E_{r_1}, E_{r_2}, ..., E_{K}\,\}, E_{r_i} = E_X \odot M_{r_i}\\
    E_{F} &= \{\,E_{f_1}, E_{f_2}, ..., E_{n_f}\,\}, E_{f_i} = E_X \odot M_{f_i}
\end{align*}


\subsection{Image Captioning Module (ICM)}\label{method:icm}
To generate captions for different levels of image embeddings, we use BLIP fine-tuned on image captioning \cite{li2022blip} with the stochastic sampling method, following the same procedure as \cite{tiong2022plug}. The caption generation process is repeated for $n_g$, $n_r$, and $n_f$ patch embeddings corresponding to the number of selected hierarchical masks. Since the patch embedding size may vary due to the different mask sizes, we use zero padding before using the captioning module. Our final HBoP captions would be:

\begin{align*}
    \text{HBoP}_{G} &= \{\,s_{g_1}, ..., s_{n_g}\,\}, s_{g_i} = \text{BLIP}(E_{g_i})\\
    \text{HBoP}_{R} &= \{\,s_{r_1}, ..., s_{K}\,\}, s_{r_i} = \text{BLIP}(E_{r_i})\\
    \text{HBoP}_{F} &= \{\,s_{f_1}, ..., s_{n_f}\,\}, s_{f_i} = \text{BLIP}(E_{f_i})
\end{align*}


\section{Results}
\noindent




\begin{table*}[t!]
\begin{center}
\resizebox{\textwidth}{!}{%
\begin{tabular}{l|c|c|cc|ccc|cc|ccc}
\toprule
\multicolumn{1}{l|}{} & & & \multicolumn{5}{c|}{MSCOCO (5k test set)}  & \multicolumn{5}{c}{Flickr30K (1k test set)}  \\
& LLM & \# of & \multicolumn{2}{c|}{Relevancy} & \multicolumn{3}{c|}{Diversity}  & \multicolumn{2}{c|}{Relevancy} & \multicolumn{3}{c}{Diversity}  \\

& Encoder & Param & SBERT $\uparrow$  & CLIP-S $\uparrow$ & PCD & mBLEU-4 $\downarrow$ & Div-2 $\uparrow$ & SBERT $\uparrow$ & CLIP-S $\uparrow$ & PCD & mBLEU-4 $\downarrow$ & Div-2 $\uparrow$ \\

\midrule
\textit{Random} & - & - & - &  17.77 &  0.963 & 0.001  & 0.868 & - &  17.54 & 0.962 &  0.003 &  0.860  \\

\midrule
BLIP ($-$NS) & \xmark & 446M & \cellcolor[HTML]{ADD8E6} 56.00 & \cellcolor[HTML]{FFCCCC} 29.98 & \cellcolor[HTML]{ADD8E6} 0.600 & \cellcolor[HTML]{FFCCCC} 1.000 & \cellcolor[HTML]{ADD8E6} 0.179 & \cellcolor[HTML]{FFCCCC} 55.78 & \cellcolor[HTML]{FFCCCC} 28.58 &  \cellcolor[HTML]{ADD8E6} 0.600 & \cellcolor[HTML]{FFCCCC} 1.000 & \cellcolor[HTML]{ADD8E6} 0.179  \\


BLIP ($+$NS) & \xmark &  446M  & \cellcolor[HTML]{FFCCCC} 57.23 & \cellcolor[HTML]{FFCCCC} 30.33 & \cellcolor[HTML]{ADD8E6}  0.668 & \cellcolor[HTML]{FFCCCC} 0.658 &  \cellcolor[HTML]{ADD8E6} 0.387 & \cellcolor[HTML]{ADD8E6} 46.99 & \cellcolor[HTML]{FFCCCC} 29.56 &  \cellcolor[HTML]{ADD8E6} 0.690 & \cellcolor[HTML]{FFCCCC} 0.664 & \cellcolor[HTML]{ADD8E6} 0.384 \\

Seq-CVAE & \xmark & - & - & - & - & \cellcolor[HTML]{FFCCCC} 0.640 & \cellcolor[HTML]{ADD8E6} 0.480 & - & - & - & - & -  \\

ModeCap & \xmark & - & - &\cellcolor[HTML]{FFCCCC} 29.35 & \cellcolor[HTML]{ADD8E6} 0.714 & \cellcolor[HTML]{FFCCCC} 0.281 & \cellcolor[HTML]{ADD8E6} 0.594 & - & - & - & - & -\\

\midrule

BLIP-2 & $\checkmark$ & 3.9B & 65.47 & 30.66 & 0.651  & 0.712  & 0.345 & 57.81 & 30.37 & 0.667 & 0.732  & 0.336   \\

Honeybee  & $\checkmark$ &  7B &  53.55 &  28.21 & 0.792 &  0.062  &  0.716 &  47.41 &  27.65 & 0.827 &   0.057 &  0.732  \\

Honeybee  & $\checkmark$ &  13B & 55.11 
 & 27.41 & - & 0.014  & 0.872 & 50.41 &  27.27 & - & 0.013 & 0.875  \\

LLaVA-1.5  & $\checkmark$ &  13B &  59.61 
 &  30.08 & - &  0.180  &  0.658 &  54.74 &  29.54 & - &  0.176 &  0.680  \\

LLaVA-1.6  & $\checkmark$ &  7B &  55.99 &  29.36 & - & 0.046  &  0.787 &  51.00  & 27.46 & - &  0.028 &  0.809  \\

\textit{Gold} & - & - & - &  30.33 & 0.753  &  0.043  &  0.748 & - &  30.87 & 0.776 &  0.049  &  0.760 \\

\midrule
HBoP (\textit{ours}) & \xmark & 1B & 56.30 & 29.12 & 0.772 & 0.049 & 0.735 & 54.00 & 28.46 & 0.815 & 0.042 & 0.750  \\

\multicolumn{3}{c|}{HBoP \textit{Ranking}} & 4/8 & 8/11 & 1/7 & 5/12 & 5/12 & 4/8 & 6/10 & 1/6 & 4/10 & 5/10  \\
\bottomrule
\end{tabular}
}
\caption{Relevancy and diversity scores across different model captions. HBoP achieves higher diversity scores with higher Div-2 and PCD values, and a lower mBLEU-4 score, on both the MSCOCO and Flickr30K datasets compared to smaller VLMs or models trained to enhance diversity. It also maintains comparable relevance scores (SBERT and CLIP-S). Simultaneously, HBoP scores competitive results compared to much larger VLMs with LLM encoders. The cells are colored based on comparison to HBoP, with blue indicating lower values and red indicating higher values.}
\label{table:Div}
\end{center}
\end{table*}




\begin{table}[t!]
\begin{center}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{l|cccc|c}
\toprule
 & PnP-VQA & BLIP & BLIP-2 & Gold & HBoP \\
 \midrule
LLama-2-13B & \begin{tabular}[c]{@{}c@{}}7.70\\($\pm$0.09)\end{tabular} & \begin{tabular}[c]{@{}c@{}}9.36\\ ($\pm$0.05)\end{tabular} & \begin{tabular}[c]{@{}c@{}}9.69\\ ($\pm$0.05)\end{tabular} & \begin{tabular}[c]{@{}c@{}}9.17\\ ($\pm$0.06)\end{tabular} & \begin{tabular}[c]{@{}c@{}}8.56\\ ($\pm$0.07)\end{tabular} \\

GPT-4 & \begin{tabular}[c]{@{}c@{}}2.18\\ ($\pm$0.84)\end{tabular} & \begin{tabular}[c]{@{}c@{}}2.97\\ ($\pm$0.10)\end{tabular} & \begin{tabular}[c]{@{}c@{}}2.96\\ ($\pm$0.19)\end{tabular} & \begin{tabular}[c]{@{}c@{}}2.94\\ ($\pm$0.49)\end{tabular} & \begin{tabular}[c]{@{}c@{}}2.48\\ ($\pm$0.73)\end{tabular}  \\
\bottomrule
\end{tabular}}
\caption{Semantic Integrity scores exhibit a similar trend across two LLM evaluations for the Flickr30K dataset (1k test set).}
\label{tab:2}
\end{center}
\end{table}


\noindent

\textbf{HBoP achieves the best diversity scores while maintaining relevance among smaller VLMs.}
We compare the diversity and relevance of captions generated by different models, as shown in Table~\ref{table:Div}. While larger VLMs with LLM-based text encoders generally achieve stronger overall performance, HBoP remains competitive, achieving the closest diversity scores to the gold captions among smaller VLMs in terms of PCD (see Appendix~\ref{app:exp} for details), mBLEU-4 and Div-2 \cite{aneja2019sequential}. 

Specifically, compared to captions generated by BLIP \cite{li2022blip} and BLIP-2 \cite{li2023blip}, HBoP achieves a PCD score closest to that of the gold captions, along with the lowest mBLEU-4 and highest Div-2 scores among BLIP (both with and without nucleus sampling (NS) \cite{li2022blip}), Seq-CVAE \cite{aneja2019sequential}, and ModeCAP\footnote{The dataset annotations and features necessary to train ModeCap are exclusively available for the MSCOCO dataset, making it difficult to replicate the experiments for fair comparison on the NoCaps and Flickr30k datasets.} \cite{chen2022learning}. Notably, in some cases, HBoP also surpasses larger VLMs with LLM-based encoders, such as BLIP-2 \cite{li2023blip}, Honeybee-7B \cite{cha2023honeybee}, and LLaVA-1.5 \cite{liu2023llava}, in diversity metrics, as indicated by lower mBLEU-4 and higher Div-2 scores (\textit{notice} red mBLEU-4 and blue Div-2, marking stronger diversity performance for HBoP).


At the same time, HBoP maintains strong similarity between generated captions and reference texts, as measured by SBERT \cite{reimers2019sentence}, and preserves high image-text alignment, as indicated by CLIP-Score \cite{hessel2021clipscore}. Notably, HBoP achieves SBERT and CLIP-Score values that are comparable to BLIP, BLIP-NS, and LLaVA, while outperforming HoneyBee in both metrics. Although BLIP-2 scores the highest, HBoP remains competitive, demonstrating a strong balance between relevance and diversity.

\noindent
\textbf{HBoP generates semantically meaningful captions.} We evaluate the semantic integrity of HBoP captions against those generated by other models, using LLama-2-13b \cite{touvron2023llama} and GPT-4 \cite{fu2023gptscore}. As shown in Table~\ref{tab:2}, HBoP achieves semantic integrity scores close to the gold captions. Notably, HBoP outperforms models like PnP-VQA in this metric. We attribute this surpass to our method, which samples more meaningful image embeddings using the proposed HCM component. 


\section{Conclusion}
\noindent
We propose HBoP, a hierarchical caption generation framework that leverages a modular architecture combining lightweight pre-trained VLMs and segmentation models to generate semantically meaningful yet diverse captions. Our experimental results demonstrate HBoPâ€™s ability to produce meaningful image embeddings for captioning, achieving performance comparable to larger VLMs and human-generated captions. HBoP sets a solid baseline for future work aiming to extract more relevant knowledge by controlling the intermediate image embeddings.



\section{Limitations}
\noindent
The HBoP architecture uses bounding box information to extract image embeddings, where the bounding boxes may contain additional information beyond the segmented object. The exploration of caption generation solely based on irregular-shaped segmentation masks is deferred as future work. 


\section{Ethical Statement}
Captions generated with HBoP might inadvertently contain harmful content. However, the final caption outputs mainly depend on the image content and pretrained image captioning model. Therefore, unless the images themselves are harmful or the pretrained model produces unsafe captions, HBoP captions are expected to pose minimal risk.  

\bibliography{custom}



\appendix
\onecolumn
\section{Appendix}

\section{Additional Related Works}

\subsection{Vision-Language Models (VLMs)}
A growth of interest in VLMs has continued due to the wide availability of multi-modal data on the web. Foundation VLMs can be applied to a range of tasks in a zero-shot manner. Notably, CLIP \cite{radford2021learning} jointly pre-trains an image encoder and a text encoder by maximizing and minimizing the cosine similarity of correct and incorrect image-text pair embeddings respectively with image-text contrastive (ITC) loss. In contrast, BLIP \cite{li2022blip} uses both ITC and image-text matching (ITM) loss for enhanced image-text data representation. Additionally, the BLIP \cite{li2022blip} \textit{captioner} uses language modeling (LM) loss for autoregressive image caption generation along with a filter, \textit{capfilt} to improve the quality of image-text pairs for training.  

Flamingo \cite{alayrac2022flamingo} shows remarkable zero-shot ability in image captioning, visual question-answering (VQA), and image-text retrieval (ITR) tasks by leveraging the few-shot learning ability of pre-trained vision-only and language-only models. It simply interleaves input visual data with task-specific text examples, producing free-form texts for unseen visual data. Another general-purpose model, BEIT3 \cite{wang2022image} with Multiway Transformer structure, uses different types of modality experts to perform fusion and modality-specific training. A masked modeling objective on images only and image-text pairs is performed for computer vision tasks (\textit{e.g.}, image classification, semantic segmentation, object detection) and vision-language tasks (\textit{e.g.}, VQA), respectively. Whereas the VQA task uses a fused encoder for image-text pairs, the ITR task encodes images and texts independently with ITC loss. Lastly, sequence-to-sequence learning is applied to generate texts from images for the image captioning task. Inspired by these previous works, we propose a meta-VLM model that utilizes a pre-trained BLIP \cite{li2022blip} image captioning module to generate enhanced textual representations, which can later serve as useful data for various downstream tasks.

\subsection{Hierarchical Representation}\label{rw:hier}
Identifying and extracting regions of interest within images is crucial for a hierarchical representation. The most intuitive way to achieve this would typically 
involve the use of object detectors \cite{yao2019hierarchy,cornia2020meshed,zhang2021rstnet}. However, the heavy computational demands of the object detectors inevitably lead to inefficiency during the inference stage \cite{yao2019hierarchy,cornia2020meshed,zhang2021rstnet}. In response, recent works sought to replace these cumbersome detectors by adopting visual concepts in the form of object tags \cite{fang2022injecting,shukor2022efficient} as an alternative. However, this detector-free approach is contingent upon the availability of object-specific data within the dataset. Employing pre-trained models is a more efficient way to identify areas of interest within images. GradCAM \cite{selvaraju2017grad} highlights essential regions that the pre-trained models used to predict any target concept using its gradients with respect to feature map activations of the final convolutional layer. DINOv2 \cite{oquab2023dinov2} capitalizes on existing self-supervised pre-trained models to generate robust, all-purpose visual features, supporting a wide array of tasks ranging from image-level classification to pixel-level segmentation. However, the image regions/features delineated by GradCAM/DINOv2 tend to show saliency for specific tasks and are unable to capture the full spectrum of visual representations. Conversely, SAM \cite{kirillov2023segany} intricately segments every semantically significant component of an image into high-quality segmentation masks generated by prompting with various inputs such as point, box, mask, or free-form text, unrestricted with types of tasks. In our framework, we integrate SAM \cite{kirillov2023segany} to create semantically meaningful segmentation masks for an entire image automatically.


Several prior studies have incorporated the principles of hierarchy or multi-scale representation into their model architectures, aiming to enhance the alignment between images and texts \cite{ji2021step,Shao2023ICCV,shukor2022efficient}. SHAN \cite{ji2021step} deconstructs the image-text matching process into two distinct facets: fragment-level and context-level alignments enabling matches across three different scopes: local-to-local, global-to-local, and global-to-global. HiVLP \cite{Shao2023ICCV} leverages both low- and high-dimensional features to represent coarse and fine details. ViCHA \cite{shukor2022efficient} aligns images and texts across various layers of neural network encoders with the underlying assumption that each layer reflects varying semantic levels. Unlike these approaches, we divide the segmentation masks hierarchically and use the embeddings of the extracted individual image patches for caption generation.

\subsection{Caption Evaluation}\label{rw:eval}
Common image captioning evaluation metrics, including BLEU \cite{bleu}, METEOR \cite{banerjee-lavie-2005-meteor}, ROUGE \cite{lin-2004-rouge}, and CIDEr \cite{cider} scores are primarily n-gram approaches that assess the quality of generated captions by considering their overlap with human-generated captions. Most SOTA VLMs frequently exhibit promising scores across these conventional evaluation metrics. However, these metrics are limited in their capabilities to measure the diversity of the generated captions. This limitation leads to a bias in these models towards generating an "average" and "safe" caption reflecting the most basic information in the image, rendering them less informative than human-generated captions. To address this gap, we incorporate several diversity metrics, including mBLEU-4, Div-2 \cite{aneja2019sequential}, and the proposed pairwise cosine distance (PCD), along with semantic integrity and relevance scores to ensure that the captions generated by our framework are not only diverse but also meaningful and directly relevant to the given image and human-annotated captions.

\section{Experiments}
\label{app:exp}
 
\subsection{Implementation Details}
 The ISM (Section~\ref{method:ims}) employs the fully automated SAM with no prompting \cite{kirillov2023segany}, along with the image encoder initialized from ViT (\verb|ViT-L/16|) pre-trained on ImageNet \cite{dosovitskiy2021image}, following the same settings as BLIP \cite{li2022blip}. Note that we use BLIP \cite{li2022blip} for captioning instead of BLIP-2 \cite{li2023blip} since BLIP-2 uses intermediate representations trained on pairs of entire images and texts for caption generation using an LLM, which is not directly applicable to HBoP that uses pairs of image patches and texts. The HCM (Section~\ref{method:hcm}) creates the global level by selecting the top ($k=$) 5 masks with the largest areas and designating the remaining masks as fine-grained. To create the regional level, K-means clustering, with ($K=$) 5 clusters per image, is applied to the bounding boxes of the segmentation masks. NMS with a threshold of 0.1 is applied at all three levels. Lastly, the ICM (Section~\ref{method:icm}) follows the methodology outlined in \citealp{tiong2022plug}. 
 
 Although HBoP presents a three-tier hierarchical structure, it is crucial to note that we adjust the different hierarchy levels depending on a given dataset. A dataset with information-rich complex images would require using all three hierarchy levels. However, a dataset with relatively simpler images, such as the MSCOCO dataset \cite{lin2014microsoft}, would benefit from a two-tier hierarchy with just the global and regional captions. We use the first two levels during evaluations unless specified otherwise.


 All the model captions in Tables~\ref{table:Div} and \ref{table:Div_nocaps} are regenerated, except for Seq-CVAE \cite{aneja2019sequential}, where the results are taken directly from the original paper. While HBoP benefits from bounding box information, it is important to note that other baseline methods (\textit{e.g.}, ModeCap) have the additional advantage of explicit learning objectives to improve diversity. The exact prompts we use for Honeybee \cite{cha2023honeybee} (top) and LLaVA-1.5/1.6 \cite{liu2023llava} are in Table~\ref{app:prompt_gen}.


\subsection{Evaluation}
We evaluate the model captions using three distinct metrics: 1) diversity across captions per image, 2) relevancy with images, and 3) semantic coherence and meaningfulness. The datasets we use for evaluation are: the Karpathy test split \cite{karpathy2015deep} of MSCOCO (5k images) \cite{lin2014microsoft}, Flickr30K zero-shot (1k test images) \cite{young2014image}, and NoCaps validation (4.5k images) \cite{agrawal2019nocaps}.



\subsubsection{Diversity}
 We measure the diversity in the generated captions using the cosine similarity between the sentence embeddings of all the corresponding captions per image. The comparison baselines are random captions, where each caption corresponds to different images, BLIP \cite{li2022blip} with and without nucleus sampling (NS\footnote{Unless otherwise specified, all the BLIP models in this paper refer to BLIP with NS.}) \cite{holtzman2019curious}, BLIP-2 \cite{li2023blip}, ModeCap \cite{chen2022learning}, Honeybee \cite{cha2023honeybee}, and gold captions\footnote{We exclude PnP-VQA since the captions are generated per question instead of per image, unlike other baselines.}. The diversity of the generated captions ($s_1, s_2, ... s_n$) per dataset instance\footnote{Note that $n = 5$ for all dataset instances, and we use one global caption and five regional captions for HBoP.} is measured using pairwise cosine distance (PCD):
 
\begin{equation}\label{eq:6}
    \text{PCD}(s_1, s_2, ... s_n) = \frac{1}{n}\sum_{i=1}^n \sum_{j=1}^{j<i} (1 - cos(M(s_i), M(s_j)))
\end{equation}

In the above equation, $cos$ represents the cosine similarity of the input embeddings. We use sentence embeddings from a pre-trained sentence transformer model (\verb|all-MiniLM-L6-v2|) \cite{reimers2019sentence}, denoted as $M$ in the Eq.~\ref{eq:6} that can capture the semantic relationships between captions. This measure evaluates the extent to which the generated captions differ from each other per image. We report the final diversity score for each dataset as the averaged PCD scores of all images in the dataset. Ideally, the PCD score should be lower than that of random captions that serve as the upper bound of the diversity score, but it should be higher than that for captions generated by existing baselines.

Additionally, we use mBLEU-4 and n-gram diversity (\textit{e.g.}, Div-1, Div-2) \cite{aneja2019sequential}, to compare with more challenging baseline models, such as ModeCap \cite{chen2022learning} and Seq-CVAE \cite{aneja2019sequential} that are built to achieve diversity within captions per image. For ModeCap \cite{chen2022learning}, we follow the default settings from the original paper to reproduce the results based on training the Transformer-DML model. We also prompt a recently introduced multimodal LLM called Honeybee \cite{cha2023honeybee} as follows: "Describe this image with 5 diverse captions, using less than 20 words for each caption."

\subsubsection{Relevancy} While confirming that each dataset contains captions with high semantic integrity is crucial, the captions must also be relevant to the corresponding images. We employ CLIP-Score \cite{hessel2021clipscore} that calculates the correlation between visual and textual CLIP embeddings \cite{radford2021learning} using pre-trained ViT (\verb|openai/| \verb|clip-vit-base-patch32|) without relying on human-generated references. Similar to the comparison baseline datasets for semantic integrity evaluation, we compare HBoP with PnP-VQA \cite{tiong2022plug}, BLIP \cite{li2022blip}, BLIP-2 \cite{li2023blip}, gold captions, along with random captions. We generate random captions by selecting five random captions for each image from a pool of HBoP captions corresponding to different images. In other words, although the random caption itself should make sense, they depict mismatched images. We randomly select one out of a total of five captions per image for each dataset and compute the correlation between CLIPScores of generated captions and gold captions.

Additionally, we measure the semantic similarity between ground-truth (or \textit{gold}) captions and captions generated with models using transformer-based SBERT \cite{reimers2019sentence}. Note that this metric is robust to synonyms or paraphrasing, unlike n-gram metrics \cite{papineni2002bleu, lin-2004-rouge}.


\subsubsection{Semantic Integrity}
We measure the semantic integrity of the generated captions (Table~\ref{tab:2}) to ensure meaningfulness and cohesiveness using LLM evaluation as it has empirically shown high correlation with human judgment \cite{chiang2023large,liu2023geval,fu2023gptscore}. We prompt Llama-2-13B (\verb|Llama-2-13b-chat| \verb|-hf|) \cite{touvron2023llama} to access the semantic integrity of HBoP captions along with gold and other baselines (PnP-VQA \cite{tiong2022plug}, BLIP \cite{li2022blip}, BLIP-2 \cite{li2023blip}) captions. Specifically, we randomly select two captions out of a total of five captions per image for each dataset and evaluate the semantic integrity by averaging the coherency and meaningfulness scores for each caption using the prompt shown in Table~\ref{app:prompt_eval}. We use the prompt "This is a picture of" to generate captions for all models in our experiments. This deliberate choice ensures a fair comparison of the general caption generation ability across models, as altering the prompt can yield significantly different results, making fair evaluation challenging.

Similarly, we use GPT-4 \cite{fu2023gptscore} for additional Semantic Integrity evaluation using only a single caption per image with the prompt shown in Table~\ref{app:prompt_gpteval}. Note that we sample the first 1k image instances in each dataset for this evaluation due to the cost limitations.


\section{Additional Results}

\subsection{Relevancy}
In Figure~\ref{fig:rel}, HBoP captions (y-axis values in the last column) show comparable relevance scores with gold captions (x-axis values in the last column) with the slope of a linear regression line\footnote{The p-values for all the regression lines are less than 0.001, except for the those of lines in the first columns, which are not statistically significant} being close to 0.5. Although the slopes of these regression lines (MSCOCO \cite{lin2014microsoft}: 0.42, Flickr30k \cite{young2014image}: 0.39, Nocaps \cite{agrawal2019nocaps}: 0.34) are less than those of BLIP \cite{li2022blip} (0.49, 0.44, and 0.45) and BLIP-2 \cite{li2023blip} (0.51, 0.45, 0.43), we observe a trend of having relevance scores in the range of 20 to 40 for both x and y axes values. On the other hand, relevance scores for random and PnP-VQA \cite{tiong2022plug} captions have a spurious and less-correlated relation with those of gold captions.


\subsection{GradCAM Results}
In addition to the evaluation results of the generated captions (samples in Figure~\ref{fig:motivation}), we illustrate how the generated captions correlate with specific image regions through GradCAMs \cite{selvaraju2017grad}. The visual representation identifies the image regions on which the generated captions are based. Specifically, we aggregate the gradients from all cross-attention layers of the pre-trained ITM model in PnP-VQA \cite{tiong2022plug}. Whereas PnP-VQA \cite{tiong2022plug} feeds the question for the textual input, we input BLIP \cite{li2022blip} and gold captions, along with HBoP captions. As shown in Figures \ref{fig:BLIP-2vsHBoP} and \ref{fig:Div}, the highlighted regions in the image for HBoP captions closely resemble the same pattern as those observed using human-generated captions. On the contrary, BLIP exhibits a more constrained range, predominantly concentrating on specific image regions. 


\subsection{Fine-grained Captions}\label{app:fine}
Although not evaluated in the perspectives of three main evaluation metrics, we can also create what we refer to as fine-grained captions that can serve as image tags using our proposed methodology. These serve as supplementary information, enhancing the depth of understanding of the image. They are more vital when dealing with complex images containing various small or intricate objects, which conventional caption generation processes may often overlook. By introducing the additional layer of granularity, our approach ensures a more detailed and inclusive interpretation of the image.



\begin{table}[t!]
\begin{center}
\resizebox{0.5\columnwidth}{!}{%
\begin{tabular}{l|c|ccc}
\toprule
 & \# Param & PCD & mBLEU-4 $\downarrow$ & Div-2 $\uparrow$ \\
\midrule
\textit{Random} & - & 0.962 (+0.223) & 0.001 & 0.867 \\
\midrule
BLIP ($-$NS) & 446M & 0.600 (-0.129) & 1.000 & 0.178  \\
BLIP-2 & 3.9B & 0.654 (-0.075) & 0.715 & 0.340  \\
BLIP ($+$NS)  & 446M &  0.679 (-0.050) & 0.629 & 0.400 \\
Honeybee  & 7B & 0.791 (+0.062) & 0.080 & 0.705 \\
\textit{Gold} & - & 0.729 & 0.078 & 0.666 \\
\midrule
HBoP (\textit{ours}) & 1B & 0.783 (+0.054) & 0.041 & 0.748  \\
\multicolumn{2}{c|}{HBoP \textit{Ranking}} & 2/6 & 2/7 & 2/7 \\
\bottomrule
\end{tabular}
}
\caption{Diversity scores for Nocaps test set. We observe a similar diversity trend across model captions as Table~\ref{table:Div}.}
\label{table:Div_nocaps}
\end{center}
\end{table}

\begin{table*}[t!]
\label{tab:prompt}
\begin{tcolorbox}[colback=white!5!white,colframe=black!75!white,arc=0pt,outer arc=0pt]
The following is a conversation between a curious human and AI assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.

Human: <image>

Human: Describe this image with 5 captions with numberings.

AI:

\end{tcolorbox}
\begin{tcolorbox}[colback=white!5!white,colframe=black!75!white,arc=0pt,outer arc=0pt]
A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.

Human: <im\_start><image><im\_end>

Human: Describe this image with 5 captions.\#\#\#Assistant:

\tcblower

[INST] <image>
What is shown in this image? Describe this image with 5 captions. [/INST]

\end{tcolorbox}
\caption{Image caption generation prompts for Honeybee (top) and LLaVA-1.5/1.6 (bottom).}
\label{app:prompt_gen}
\end{table*}



\begin{table*}[t!]
\begin{tcolorbox}[colback=white!5!white,colframe=black!75!white,arc=0pt,outer arc=0pt]
[INST] $<<$SYS$>>$ \\
You will be given a caption generated from an image. Given criteria and rating options, rate the response. Respond with a number only. \\
Evaluation Criteria: \textbf{[CRITERION]}: \textbf{[DEFINITION]} \\
Scale: from 1 to 10 \\
Answer:
$<<$/SYS$>>$ \\
INPUT [$/$INST] \\
\tcblower 
\textbf{[CRITERION]}: Coherence/Meaningfulness \\
\textbf{[DEFINITION]}: the logical and clear connection between ideas or elements within a context. It is characterized by the consistency, integrity, and clarity of information or arguments presented./the relevance and significance of the content in the caption. A meaningful caption goes beyond a literal description, providing insight, context, or emotion that enhances the viewer's understanding or appreciation of the image.
\end{tcolorbox}
\caption{The prompt for evaluating semantic integrity (coherence + meaningfulness) of generated model captions using Llama-2-13B.}
\label{app:prompt_eval}
\end{table*}


\begin{table*}[t!]
\begin{tcolorbox}[colback=white!5!white,colframe=black!75!white,arc=0pt,outer arc=0pt]
You will be given one caption written for describing an image. \\
\\
Your task is to rate the caption on one metric. \\
\\
Please make sure you read and understand these instructions carefully. Please keep this document open while reviewing, and refer to it as needed.\\
\\
Evaluation Criteria:
\\
Fluency (1-3): the quality of the caption in terms of grammar, spelling, punctuation, word choice, and sentence structure.
\\
- 1: Poor. The caption has many errors that make it hard to understand or sound unnatural. \\
- 2: Fair. The caption has some errors that affect the clarity or smoothness of the text, but the main points are still comprehensible. \\
- 3: Good. The caption has few or no errors and is easy to read and follow.
\\
\\
Example:
\\
Caption:
\\
{{Caption}}
\\
\\
Evaluation Form (scores ONLY):
\\
- Fluency (1-3):
\end{tcolorbox}
\caption{The prompt for evaluating semantic integrity (\textit{i.e.}, fluency) of generated model captions using GPT-4.}
\label{app:prompt_gpteval}
\end{table*}

\begin{figure*}[t!]
\centering
\includegraphics[width=\textwidth]{Figures/FigS2.pdf}
\caption{Correlation of relevance scores between gold captions and model captions. We observe higher correlations for HBoP, BLIP, and BLIP-2 captions as comapred to random and PnP-VQA captions.}
\label{fig:rel}
\end{figure*}

\begin{figure*}[t!]
\centering
\includegraphics[width=\textwidth]{Figures/Fig0.pdf}
\caption{Comparison between captions generated using BLIP-2 \cite{li2023blip} and HBoP. Our captions contain more diverse interpretations of the images while maintaining high relevancy.}
\label{fig:motivation}
\end{figure*}


\begin{figure*}[t!]
\centering
\includegraphics[width=\textwidth]{Figures/FigS1.pdf}
\includegraphics[width=\textwidth]{Figures/FigS1-2.pdf}
\caption{Additional visualizations of GradCAMs across different model captions.}
\label{fig:Div}
\end{figure*}



\end{document}