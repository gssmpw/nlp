\begin{figure*}[!t]
    \centering 
    \includegraphics[width=1\textwidth]{figure/pipeline.pdf}
    \caption{\textbf{ViDoRAG Framework.}}
    \label{fig:pipeline}
\end{figure*}

\section{Method}
In this section, drawing from insights and foundational ideas, we present a comprehensive description of our \textbf{ViDoRAG} framework, which integrates two modules: Multi-Modal Hybrid Retrieval (\S \ref{sec:ret}) and Multi-Scale View Generation (\S \ref{sec:gen}).

\subsection{Multi-Modal Hybrid Retrieval}
\label{sec:ret}
For each query, our approach involves retrieving information through both textual and visual pipelines, dynamically determining the optimal value of top-K using a Gaussian Mixture Model (GMM), and merging the retrieval results from both pipelines.
\paragraph{Adaptive Recall with Gaussian Mixture Model.}
Traditional methods rely on a static hyperparameter, $\mathcal{K}$, to retrieve the top-\textit{K} images or text chunks from a corpus. A smaller $\mathcal{K}$ might fail to capture sufficient references needed for accurate responses, as the most relevant nodes are not always ranked at the top. Conversely, a larger $\mathcal{K}$ can slow down inference and introduce inaccuracies due to noise. Additionally, manually tuning $\mathcal{K}$ for different scenarios is troublesome.

Our objective is to develop a straightforward yet effective method to automatically determine $\mathcal{K}$ for each modality, without the dependency on a fixed value. We utilize the similarity $\mathcal{S}$ of the embedding $E$ to quantify the relevance between the query and the document collection $\mathcal{C}$:
\begin{equation}
\mathcal{S}(q, \mathcal{C}) = \{s_i | cos(E_q,E_{p_i}) , p_i \in \mathcal{C}\}
\end{equation}
where $s_i$ represents the cosine similarity between the query $\mathcal{Q}$ and page $p_i$. In the visual pipeline, a page corresponds to an image, whereas in the textual pipeline, it corresponds to chunks of OCR text. We propose that the distribution of $\mathcal{S}$ follows a GMM and we consider they are sampled from a bimodal distribution $\mathcal{P}(s)$ shown in Fig.\ref{fig:pipeline}:
\begin{equation}
\small \mathcal{P}(s) = w_F \cdot \mathcal{N}(s \mid \mu_F, \sigma_F^2) + w_T \cdot \mathcal{N}(s \mid \mu_T, \sigma_T^2) \label{eq:gaussian}
\end{equation}
where $\mathcal{N}$ represents a Gaussian distribution, with $w,\mu,\sigma^2$ indicating the weight, mean, and variance, respectively. The subscripts $T$ and $F$ refer to the distributions of pages with high and low similarity. The distribution with higher similarity is deemed valuable for generation. The Expectation-Maximization (EM) algorithm is utilized to estimate the prior probability $\mathcal{P}(T|s, \mu_T, \sigma_T^2)$ for each modality. The dynamic value of $\mathcal{K}$ is defined as:
\begin{equation}
\mathcal{K} = | \{ p_i \in \mathcal{C} \mid p_i \sim \mathcal{N}(\mu_T, \sigma_T^2) \} |
\end{equation}

Considering that the similarity score distribution for different queries within a document collection may not strictly follow a standard distribution, we establish upper and lower bounds to manage outliers. The EM algorithm is employed sparingly, less than $\sim 1\%$ of the time. Dynamically adjusting $\mathcal{K}$ enhances generation efficiency compared to a static setting. Detailed analysis is available in \S \ref{sec:analysis:time}.

\paragraph{Textual and Visual Hybrid Retrieval.}
In the previous step, nodes were retrieved from both pipelines. In this phase, we integrate them:
\begin{equation}
\mathcal{R}_{hybrid} = Sort[\mathcal{F}(\mathcal{R}_{Text},\mathcal{R}_{Visual})]
\end{equation}
where $\mathcal{R}_{Text}$ and $\mathcal{R}_{Visual}$ denote the retrieval results from the textual and visual pipelines, respectively. The function $\mathcal{F}(\cdot)$ signifies a union operation, and $Sort(\cdot)$ arranges the nodes in their original sequence, as continuous pages often exhibit correlation \cite{yu2024defense}. 

The textual and visual retrieval pipelines demonstrate varying levels of performance for different features. Without adaptive recall, the combined retrieval $\mathcal{R}_{hybrid}$ can become excessive. Adaptive recall ensures that effective retrievals are concise, while traditional pipelines yield longer recall results. This strategy optimizes performance relative to context length, underscoring the value of adaptive recall in hybrid retrieval.

\subsection{Multi-Agent Generation with Iterative Reasoning}
\label{sec:gen}
During the generation, we introduce a multi-agent framework which consists of three types of agents: the Seeker Agent, the Inspector Agent, and the Answer Agent. As illustrated in Fig. \ref{fig:pipeline}, this framework extracts clues, reflects, and answers in a coarse-to-fine manner from a multi-scale perspective. More details are provided in Appendix \ref{appendix: gen}.



\paragraph{Seeker Agent: Hunting for relevant images.} 
The Seeker Agent is responsible for selecting from a coarse view and extracting global cues based on the query and reflection from the Inspector Agent. We have made some improvements to ReAct\cite{yao2022react} to facilitate better memory management. 
The action space is defined as the selection of the images. Initially, the agent will reason only based on the query $\mathcal{Q}$ and select the most relevant images $\mathbf{I}^{\text{s}}_0$ from the candidate images $\mathbf{I}^{\text{c}}_0$, while the initial memory $\mathcal{M}_0$ is empty.
In step $t$, the candidate images $\mathbf{I}^{\text{c}}_{t + 1}$ are the complement of previously selected images $\mathbf{I}^{\text{s}}_{t}$, defined as $\mathbf{I}^{\text{c}}_{t + 1}=\mathbf{I}^{\text{c}}_{t}\setminus\mathbf{I}^{\text{s}}_{t}$.
The seeker has received the reflection $\mathcal{F}_{t - 1}$ from the inspector, which includes an evaluation of the selected images and a more detailed description of the requirements for the images. The Seeker integrates feedback $\mathcal{F}_{t - 1}$ from the Inspector, which includes an evaluation of the selected images and a description of image requirements, to further refine the selection $\mathbf{I}^{s}_{t}$ and update the memory $\mathcal{M}_{t+1}$:
\begin{equation}
\mathbf{I}^{c}_{t+1},~\mathcal{M}_{t+1} = \Theta(\mathbf{I}^{c}_{t},\mathcal{Q}, \mathcal{M}_{t}, \mathcal{F}_{t-1})
\end{equation}
where $\mathcal{M}_{t+1}$ represents the model's thought content in step $t$ under the ReAct paradigm, maintaining a constant context length. The process continues until the Inspector determines that sufficient information is available to answer the query, or the Seeker concludes that no further relevant images exist among the candidates.

\paragraph{Inspector Agent: Review in detail and Reflect.}
In baseline scenarios, increasing the top-$K$ value improves recall@$K$, but accuracy initially rises and then falls. This is attributed to interference from irrelevant images, referred to as noise, affecting model generation. To address this, we use Inspector to perform a more fine-grained inspection of the images. In each interaction with the Seeker, the Inspector's action space includes providing feedback or drafting a preliminary answer. 
At step $t$, the inspector reviews images at high resolution, denoted as $\Theta(\mathbf{I}^c_t \cup \mathbf{I}^{r}_{t-1}, \mathcal{Q})$ where $\mathbf{I}^{r}_{t-1}$ are images retained from the previous step and $\mathbf{I}^{c}_{t}$ are from the Seeker.
If the current information is sufficient to answer the query, a draft answer $\hat{\mathcal{A}}$ is provided, alongside a reference to the relevant image:
\begin{equation}
\hat{\mathcal{A}},~\mathbf{I}^{ref} = \Theta(\mathbf{I}^c_t \cup \mathbf{I}^{r}_{t-1}, \mathcal{Q})
\end{equation}
Conversely, if more information is needed, the Inspector offers feedback $\mathcal{F}_{t}$ to guide the Seeker in better image selection and identifies images $\mathbf{I}^r_t$ to retain for further review in the next step $t+1$:
\begin{equation}
    \mathcal{F}_t,~\mathbf{I}^r_t = \Theta(\mathbf{I}^c_t \cup \mathbf{I}^{r}_{t-1}, \mathcal{Q})
\end{equation}
The number of images the Inspector reviews is typically fewer than the Seeker's, ensuring robustness in reasoning, particularly for Visual Language Models with moderate reasoning abilities.

% \input{table/overall_w_ocr}
\input{table/results_new}

\paragraph{Answer Agent: Synthesize the final answer.}~ In our framework, the Seeker and Inspector engage in a continuous interaction, and the answer agent provides the answer in the final step. To balance accuracy and efficiency, the Answer Agent verifies the consistency of the Inspector's draft answer $\hat{\mathcal{A}}$. If the reference image matches the Inspector's input, the draft answer is accepted as the final answer $\mathcal{A}=\hat{\mathcal{A}}$.
If the reference image is a subset of the input image, the answer agent should check for consistency between the draft answer $\hat{\mathcal{A}}$ and the reference image, then give the final answer $\mathcal{A}$: 
 If the reference image is a subset of Inspector's the input, the Answer Agent ensures consistency between the draft answer $\hat{\mathcal{A}}$ and the reference image before finalizing the answer $\mathcal{A}$:
\begin{equation}
\mathcal{A} = \Theta(\mathbf{I}_{ref}, \mathcal{Q}, \hat{\mathcal{A}})
\end{equation}
The Answer Agent utilizes the draft answer as prior knowledge to refine the response from coarse to fine. The consistency check between the Answer Agent and Inspector Agent enhances the depth and comprehensiveness of the final answer.