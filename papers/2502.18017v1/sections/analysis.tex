\input{table/ret_table}
\begin{figure}[!h]
    \centering 
    \includegraphics[width=0.95\linewidth]{figure/recall.pdf}
    \caption{Retrieval performance across different retrievers and hybrid retrieval, along with ablations on GMM. 
    }
    \label{fig:ret}
\end{figure}

\subsection{Retrieval Evaluation}
In Table \ref{tab:ret}, we report the detailed performance for various retrievers, including OCR-based and visual-based. 
Due to the uncertainty of dynamical retrieval across queries, we use the average length of results for analysis.
Our goal is to incorporate more relevant information within a shorter context while minimizing the impact of noise and reducing computational cost without losing valuable information. 
Dynamic retrieval can achieve better recall performance with a smaller context length, while hybrid retrieval combines the results of two pipelines achieving state-of-the-art performance.

\section{Analysis}
\subsection{Ablations}
Table \ref{tab:ablation} presents the impact of different retrievers and generation methods on performance. We have decomposed the dynamic retrieval into two components, Dynamic and Hybrid. Naive refers to the method of direct input, which is most commonly used as baselines. Dynamic indicates using GMM to fit the optimal recall distribution based solely on the visual pipeline. Hybrid refers to merging the visual and the textual retrieval results directly, which leads to suboptimal results due to long contexts. Experiments demonstrate that the effectiveness and scalability of our improvements on retrieval and generation modules, as well as their combination, can comprehensively enhance end-to-end performance from various perspectives.

\input{table/ablations}

\subsection{Time Efficiency}
\label{sec:analysis:time}
\paragraph{How does dynamic retrieval balance latency and accuracy?}
In traditional RAG systems, using a small top-K value may result in missing critical information, whereas employing a larger value can introduce noise and increase computational overhead. ViDoRAG dynamically determines the number of documents to retrieve based on the similarity distribution between the query and the corpus. This approach ensures that only the most relevant documents are retrieved, thereby reducing unnecessary computations from overly long contexts and accelerating the generation process. As shown in Table \ref{tab:time_recall}, we compare retrieval with and without GMM based on the Naive method. The experiments indicate that GMM may reduce recall due to distribution bias. However, because it significantly shortens the generation context, it effectively improves performance in end-to-end evaluations.
\input{table/time_recall}

\paragraph{Latency Analysis of the Multi-Agent Generation.}
There is an increase in delay due to the iterative nature of the multi-agent system, as shown in Fig. \ref{fig:gen_time}. Each agent performs specific tasks in a sequential manner, which adds a small overhead compared to traditional straightforward RAG. However, despite the increase in latency, the overall performance improves due to the higher quality of generated answers, making the trade-off between latency and accuracy highly beneficial for complex RAG tasks.

\begin{figure}[!h]
    \centering 
    \includegraphics[width=0.93\linewidth]{figure/time.pdf}
    \caption{Latency Analysis on Generation.}
    \label{fig:gen_time}
\end{figure}

\subsection{Modalities and Strategies of Generation}
As shown in Fig. \ref{fig:compare_radar}, the vision-based pipeline outperforms the text-based pipeline across all types, even for queries related to text content. Generally speaking, due to models' inherent characteristics, the reasoning ability of LLMs is stronger than that of VLMs. However, the lack of visual information makes it difficult for models to identify the intrinsic connections between pieces of information. This also poses a challenge for the generation of content based on visually rich documents. While obtaining visual information, VidoRAG further enhances the reasoning capabilities of VLMs, striking a balance between accuracy and computational load.

\begin{figure}[!h]
    \centering 
    \includegraphics[width=0.99\linewidth]{figure/compare_radar.pdf}
    \caption{Performance across different types of queries on our ViDoSeek and the refined SlideVQA datasets.}
    \label{fig:compare_radar}
\end{figure}

\begin{figure}[!h]
    \centering 
    \includegraphics[width=0.91\linewidth]{figure/scale.pdf}
    \caption{Scaling behavior with ViDoRAG.}
    \label{fig:scale}
\end{figure}

\subsection{Performance with Test-time Scaling}

Fig. \ref{fig:scale} illustrates the number of interaction rounds between the seeker and inspector within ViDoRAG based on different models. 
Due to the limited instruction capabilities of some models, we sampled 200 queries for the experiment.
Models with stronger performance require fewer reasoning iterations, while weaker models often need additional time to process and reach a conclusion.
Conditioning the model on a few demonstrations of the task at inference time has been proven to be a computationally efficient approach to enhance model performance\cite{brown2020language,min2021metaicl}. 
The results indicate that predefining tasks and breaking down complex tasks into simpler ones is an effective method for scaling inference.
