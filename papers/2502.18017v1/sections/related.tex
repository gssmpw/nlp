\section{Related Work}

\paragraph{Visual Document Q\&A Benchmarks.}
Visual Document Question Answering is focused on answering questions based on the visual content of documents\cite{antol2015vqa,ye2024mplug,wang2024leave}. While most existing research \cite{methani2020plotqa, masry2022chartqa, li2024multimodal, mathew2022infographicvqa} has primarily concentrated on question answering from single images, recent advancements have begun to explore multi-page document question answering, driven by the increasing context length of modern models \cite{mathew2021docvqa, ma2024mmlongbenchdocbenchmarkinglongcontextdocument, tanaka2023slidevqa}. However, prior datasets were not well-suited for RAG tasks involving large collections of documents. To fill this gap, we introduce ViDoSeek, the first large-scale document collection QA dataset, where each query corresponds to a unique answer across a collection of $\sim 6k$ images.

\begin{figure*}[t]
    \centering 
    \includegraphics[width=1.\textwidth]{figure/data_construction.pdf}
    \caption{\textbf{Data Construction pipeline.} (a) We sample and filter documents according to the requirements to obtain candidates. (b) Then experts construct the initial query from different contents. (c) After that, we prompt GPT-4 to directly determine whether the query is a general query. The remaining queries are carefully reviewed with top-\textit{K} recall images. (d) Finally, unqualified queries are refined paired with golden image by GPT-4o.}
    \label{fig:construction_pipeline}
\end{figure*}

\paragraph{Retrieval-augmented Generation.}
With the advancement of large models, RAG has enhanced the ability of models to incorporate external knowledge \cite{lewis2020retrieval,chen2024mindsearch,wu2025webwalker}. In prior research, retrieval often followed the process of extracting text via OCR technology \cite{chen2024bge,lee2024nv,robertson2009probabilistic}. Recently, the growing interest in multimodal embeddings has greatly improved image retrieval tasks \cite{faysse2024colpali,yu2024visrag}. 
Additionally, there are works that focus on In-Context Learning in RAG\cite{agarwal2025many,yue2024inference,team2024gemini,weijia2023replug}.
Our work builds upon these developments by combining multi-modal hybrid retrieval with a coarse-to-fine multi-agent generation framework, seamlessly integrating various embedding and generation models into a scalable framework.

