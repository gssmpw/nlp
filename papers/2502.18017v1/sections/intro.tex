\section{Introduction}

\begin{figure}[!t]
    \centering 
    \includegraphics[width=1.01\columnwidth]{figure/dataset.pdf}
    \caption{
    Comparison of our work with the existing datasets and methods.
    (a) In traditional datasets, each query must be paired with specific images or documents. In our ViDoSeek, each query can obtain a unique answer within the large corpus.
    (b) Our ViDoRAG is a multi-agent, coarse-to-fine framework specifically optimized for visually rich documents.
    }
    \label{fig:compare_data_rag}
\end{figure}


Retrieval-Augmented Generation (RAG) enhances Large Models (LMs) by enabling them to use external knowledge to solve problems. As the expression of information becomes increasingly diverse, we often work with visually rich documents that contain diagrams, charts, tables, etc. These visual elements make information easier to understand and are widely used in education, finance, law, and other fields. Therefore, researching RAG within visually rich documents is highly valuable.

In practical applications, RAG systems often need to retrieve information from a large collection consisting of hundreds of documents, amounting to thousands of pages. As shown in Fig. \ref{fig:compare_data_rag}, existing Visual Question Answering (VQA) benchmarks aren't designed for such large corpus. The queries in these benchmarks are typically paired with one single image\cite{methani2020plotqa,masry2022chartqa,li2024multimodal,mathew2022infographicvqa} or document\cite{ma2024mmlongbenchdocbenchmarkinglongcontextdocument}, which is used for evaluating Q\&A tasks but not suitable for evaluating RAG systems. The answers to queries in these datasets may not be unique within the whole corpus.

To address this gap, we introduce ViDoSeek, a novel dataset designed for visually rich document retrieval-reason-answer. In ViDoSeek, each query has a unique answer and specific reference pages. It covers the diverse content types and multi-hop reasoning that most VQA datasets include.  This specificity allows us to better evaluate retrieval and generation performance separately. 

Moreover, to enable models to effectively reason over a large corpus, we propose ViDoRAG, a multi-agent, coarse-to-fine retrieval-augmented generation framework tailored for visually rich documents. Our approach is based on two critical observations:
\textbf{(i) Inefficient and Variable Retrieval Performance.} 
Traditional OCR-based retrieval struggles to capture visual information. With the development of vision-based retrieval, it is easy to capture visual information\cite{faysse2024colpali,yu2024visrag,zhai2023sigmoid}. However, there lack of an effective method to integrate visual and textual features, resulting in poor retrieval of relevant content.
\textbf{(ii) Insufficient Activation of Reasoning Capabilities during Generation.} 
Previous studies on inference scaling for RAG focus on expanding the length of retrieved documents\cite{jiang2024longrag,shao2025scaling,xu2023retrieval}. However, due to the characteristics of VLMs, only emphasizing on the quantity of knowledge without providing further reasoning guidance presents certain limitations. There is a need for an effective inference scale-up method to efficiently utilize specific action spaces, such as resizing and filtering, to fully activate reasoning capabilities.

Building upon these insights, ViDoRAG introduces improvements in both retrieval and generation. We propose Multi-Modal Hybrid Retrieval, which combines both visual and textual features and dynamically adjusts results distribution based on Gaussian Mixture Models (GMM) prior. This approach achieves the optimal retrieval distribution for each query, enhancing generation efficiency by reducing unnecessary computations.
During generation, our framework comprises three agents: the seeker, inspector, and answer agents. The seeker rapidly scans thumbnails and selects relevant images with feedback from the inspector. The inspector reviews, then provides reflection and offers preliminary answers. The answer agent ensures consistency and gives the final answer. This framework reduces exposure to irrelevant information and ensures consistent answers across multiple scales.

Our major contributions are as follows:
\begin{itemize}
    \item We introduce ViDoSeek, a benchmark specifically designed for visually rich document retrieval-reason-answer, fully suited for evaluation of RAG within large document corpus.
    \item We propose ViDoRAG, a novel RAG framework that utilizes a multi-agent, actor-critic paradigm for iterative reasoning, enhancing the noise robustness of generation models. 
    \item We introduce a GMM-based multi-modal hybrid retrieval strategy to effectively integrate visual and textual pipelines.
    \item Extensive experiments demonstrate the effectiveness of our method. ViDoRAG significantly outperforms strong baselines, achieving over 10\% improvement, thus establishing a new state-of-the-art on ViDoSeek.
\end{itemize}