\section{Conclusion}
In this work, we introduced ViDoRAG, a novel multi-agent RAG framework tailored for visually rich documents. By proposing a coarse-to-fine reasoning process and a multi-modal retrieval strategy, ViDoRAG significantly outperforms existing methods, achieving new SOTA on the ViDoSeek benchmark. Future work will focus on further optimizing the framework's efficiency while maintaining high accuracy, and exploring its potential in diverse real-world applications, such as education and finance, where visually rich document RAG is crucial.

% \clearpage
\section*{Limitations}
In addition to the advanced improvements mentioned above, our work has several limitations:  
\textbf{(1) Potential Bias in Query Construction.} The queries in ViDoSeek were constructed by human experts, which may introduce bias in the types of questions and the way they are phrased. This could affect the model's ability to handle more diverse and natural language queries from real-world users.
\textbf{(2) Computational Overhead of ViDoRAG.} The multi-agent framework, while effective in enhancing reasoning capabilities, introduces additional computational overhead due to the iterative interactions between the seeker, inspector, and answer agents. This may limit the scalability of the framework in scenarios with strict latency requirements.
\textbf{(3) Model Hallucinations.} Despite the improvements in retrieval and reasoning, the models used in ViDoRAG can still generate hallucinated answers that are not grounded in the retrieved information. This issue can lead to incorrect or misleading responses, especially when the model is overconfident in its generated content.

In summary, while ViDoRAG demonstrates significant improvements in visually rich document retrieval and reasoning, there are still areas for further enhancement, particularly in terms of generalization to diverse document types, reducing potential biases in query construction, optimizing the computational efficiency of the multi-agent framework, and addressing the issue of model hallucinations. Future work will focus on addressing these limitations to further improve the robustness and applicability of the model.

\section*{Ethical Considerations}

Our data does not contain any private or sensitive information, and all content is derived from publicly available sources. Additionally, the construction and refinement of the dataset were conducted in a manner that respects copyright and intellectual property rights.

% \section*{Acknowledgments}
