\clearpage
\appendix

\definecolor{lightgray}{gray}{0.95}
\definecolor{deepblue}{RGB}{70,130,180}
\definecolor{deepgray}{RGB}{119,136,153}
\lstdefinestyle{prompt}{
    basicstyle=\ttfamily\fontsize{7pt}{8pt}\selectfont,
    frame=none,
    breaklines=true,
    backgroundcolor=\color{lightgray},
    breakatwhitespace=true,
    breakindent=0pt,
    escapeinside={(*@}{@*)},
    numbers=none,
    numbersep=5pt,
    xleftmargin=5pt,
    aboveskip=2pt,
    belowskip=2pt,
}
\tcbset{
  aibox/.style={
    top=10pt,
    colback=white,
    % colframe=black,
    % colbacktitle=black,
    enhanced,
    center,
    % attach boxed title to top left={yshift=-0.1in,xshift=0.15in},
    % boxed title style={boxrule=0pt,colframe=white,},
  }
}
\newtcolorbox{AIbox}[2][]{aibox, title=#2,#1}



\section{Additional Experiments Details}

\paragraph{Backbones.} To thoroughly validate the effectiveness of ViDoRAG, we conducted experiments on various models across various baselines, including both closed-source and open-source models: GPT-4o, Qwen2.5-7B, Llama3.2-3B, Qwen2.5-VL-7B\cite{yang2024qwen2}, Llama3.2-Vision-90B. For OCR-based pipelines, we use PPOCR\cite{ma2019paddlepaddle} to recognize text within documents. Optionally, VLMs can also be employed for text recognition, as their OCR capabilities are quite strong.

\paragraph{Experimental Environments.}
We conducted our experiments on a server equipped with 8 A100 GPUs and 96 CPU cores. Open-source models require substantial computational resources.

\paragraph{Retrieval Implementation Details.} Due to the context length limitations of the model, we use the Top-$2K$ pages to fit the GMM and we restrict the output chunks of the GMM algorithm to be between $K/2$ and $K$, we set $K=10$ in practice. 

\section{More Details on Datasets}
\label{appendix:dataset_composition}

\subsection{Annotation Case}
\input{case/ann_case}

\subsection{Details on ViDoSeek}
\paragraph{More Dataset Statistics.}
The statistical about ViDoSeek is presented in Table \ref{tab:data_statistic_slide}. We categorize queries from a logical reasoning perspective into single-hop and multi-hop. Text, Table, Chart and Layout represent different sources of reference.
\input{table/data_statistic}
\paragraph{Dataset Difficulty.}
ViDoSeek sets itself apart with its heightened difficulty level, attributed to the multi-document context and the intricate nature of its content types, particularly the Layout category. The dataset contains both single-hop and multi-hop queries, presenting a diverse set of challenges. Consequently, ViDoSeek serves as a more comprehensive and demanding benchmark for RAG systems compared to previous works.

\subsection{Details on SlideVQA-Refined}
\paragraph{Dataset Statistics.}
We supplemented our experiments with the SlideVQA dataset to demonstrate the scalability of our method. 
SlideVQA categorizes queries from a logical reasoning perspective into single-hop and multi-hop. 
Non-span, single-span, and multi-span respectively refer to answers derived from a single information-dense sentence, reference information that is sparse but located on the same page, and reference information distributed across different pages.
The statistical information about dataset is presented in Table \ref{tab:data_statistic_slide}.

\input{table/data_statistic_slide}
\paragraph{Dataset Difficulty.} The SlideVQA dataset focuses on evaluating the RAG system's ability to understand both visually sparse and visually dense information. When multi-hop questions involve reference information spread across different pages, it presents a significant challenge to the RAG system, further demonstrating the effectiveness of our approach.



\section{Data Construction Details}
\label{appendix:data_construction_pipeline}


To construct the ViDoSeek dataset, we developed a four-step pipeline to ensure that the queries meet our requirements. 
\paragraph{Step 1. Document Collecting.}
We collected English-language slides containing 25 to 50 pages, covering 12 domains such as economics, technology, literature, and geography, etc.

\paragraph{Step 2. Query Creation.}

To make the queries more suitable for RAG over a large-scale collection, our experts constructed queries based on the following requirements: (\romannumeral1) Each query must have a unique answer when paired with the document. (\romannumeral2) The query must include unique keywords that point to the specific document and pages. (\romannumeral3) The query should require external knowledge. Additionally, we encouraged constructing queries in various forms and with different sources and reasoning types to better reflect real-world scenarios. Our queries not only focus on types of references, including text, tables, charts, and layouts, but also provide a classification of reasoning types, including single-hop and multi-hop.

\paragraph{Step 3. Quality Review.}
To effectively evaluate the generation and retrieval quality of our RAG system, we require queries that yield unique answers, preferably located on a specific page or within a few pages. However, in large-scale retrieval and generation tasks, relying solely on manual annotation is challenging due to human cognitive limitations. To address this, we propose a review module that automatically identifies problematic queries. This module consists of two steps: (\romannumeral1) We prompt LLMs to filter out queries that may have multiple answers across the document collection; for example, the question \emph{What is the profit for this company in 2024?} might have a unique answer within a single document but could yield multiple answers in a multi-document setting. (\romannumeral2) For the remaining queries, we retrieve the top-\emph{k} slides for each query and use a VLM to determine whether each slide can answer the query. If only the golden page can answer the question, we consider it to meet the requirements. If pages other than the golden page can answer the query, we have experts manually evaluate and refine them.

\paragraph{Step 4. Multimodal Refine.}
% refine问题 同时保证问题中不含答案
In this final step, we refine the queries that did not meet our standards during the quality review. The goal is to adjust these queries so they satisfy the following requirements: (\romannumeral1) The refined query should point to specific pages within the large collection with minimal additional information; (\romannumeral2) The refined query must retain its original meaning. 
We use carefully designed VLM-based agents to assist us throughout the entire dataset construction pipeline. The prompt is presented in Fig. \ref{fig: reviewer} and Fig. \ref{fig: multi_reviewer}, respectively. We will first perform filtering based on semantics, and then conduct a fine-grained review using a multimodal reviewer.



\section{More Details about Multi-Agent Generation with Iterative Reasoning}
\label{appendix: gen}
We designed prompts to drive VLMs-based agents, and through our experiments, we found that some open-source models require the design of few-shot examples to learn specific thought patterns. See detailed prompts in Fig. \ref{fig: seeker}, Fig.\ref{fig: inspector} and Fig.\ref{fig: answer}.

\input{case/reviewer}
\input{case/rewrite}
\input{case/seeker}
\input{case/inspector}
\input{case/answer}
