\section{Problem Formulation}
\label{sec:problem_define}
Given a query as $q$, and we have a collection of documents $\mathcal{C} = \{ \mathcal{D}_1, \mathcal{D}_2, \ldots, \mathcal{D}_M \}$ which contains $M$ documents. Each document $\mathcal{D}_m$ consists of $N$ pages, each image representing an individual page, defined as $\mathcal{D}_m = \{ \mathbf{I}_1, \mathbf{I}_2, \ldots, \mathbf{I}_N \}$. The total number of images included in the collection is $\sum_{m=1}^{M} |\mathcal{D}_m|$. We aim to retrieve the most relevant information efficiently and accurately and generate the final answer $a$ to the query $q$.

\section{ViDoSeek Dataset}
\label{sec:dataset}

Existing VQA datasets typically consist of queries paired with a single image or a few images. However, in practical application scenarios, users often pose questions based on a large-scale corpus rather than targeting an individual document or image. To better evaluate RAG systems, we prefer questions that have unique answers when retrieving from a large corpus. 
To address this need, we introduce a novel \textbf{Vi}sually rich \textbf{Do}cument dataset specifically designed for RAG systems, called ViDoSeek. 
Below we provide the pipeline for constructing the dataset(\S \ref{sec:data_pipeline})  and a detailed analysis of the dataset(\S \ref{sec:data_analysis}).

\subsection{Dataset Construction.}
\label{sec:data_pipeline}
To construct the ViDoSeek dataset, we developed a four-step pipeline to ensure that the queries meet our stringent requirements. As illustrated in Figure \ref{fig:construction_pipeline}, our dataset comprises two parts: one annotated from scratch by our AI researchers, and the other derived from refining queries in the existing open-source dataset SlideVQA \cite{tanaka2023slidevqa}. 
For the open-source dataset, we initiate the query refinement starting from the third step of our pipeline. For the dataset we build from scratch, we follow the entire pipeline beginning with document collection. The following outlines our four-step pipeline:
\paragraph{Step 1. Document Collecting.}
As slides are a widely used medium for information transmission today, we selected them as our document source. We began by collecting English-language slides containing 25 to 50 pages, covering 12 domains such as economics, technology, literature, and geography. And we filtered out 300 slides that simultaneously include text, charts, tables, and two-dimensional layouts which refer to flowcharts, diagrams, or any visual elements composed of various components and are a distinctive feature of slides.
\input{table/data_compare}

\paragraph{Step 2. Query Creation.}

To make the queries more suitable for RAG over a large-scale collection, our experts were instructed to construct queries that are specific to the document.
Additionally, we encouraged constructing queries in various forms and with different sources and reasoning types to better reflect real-world scenarios. 

\paragraph{Step 3. Quality Review.}

In large-scale retrieval and generation tasks, relying solely on manual annotation is challenging due to human brain limitations. To address this, we propose a review module that automatically identifies problematic queries.

\paragraph{Step 4. Multimodal Refine.}
% refine问题 同时保证问题中不含答案
In this final step, we refine the queries that did not meet our standards during the quality review. We use carefully designed VLM-based agents to assist us throughout the entire dataset construction pipeline. 


\subsection{Dataset Analysis}
\label{sec:data_analysis}

\paragraph{Dataset Statistics.} ViDoSeek is the first dataset specifically designed for question-answering over large-scale document collections. It comprises approximately $\sim 1.2k$ questions across a wide array of domains, addressing four key content types: Text, Chart, Table, and Layout. Among these, the Layout type poses the greatest challenge and represents the largest portion of the dataset. Additionally, the queries are categorized into two reasoning types: single-hop and multi-hop. Further details of the dataset can be found in the Appendix \ref{appendix:dataset_composition} and \ref{appendix:data_construction_pipeline}.

\paragraph{Comparative Analysis.}
Table \ref{tab:data_compare} highlights the limitations of existing datasets, which are predominantly tailored for scenarios involving single images or documents, lacking the capacity to handle the intricacies of retrieving relevant information from large collections. ViDoSeek bridges this gap by offering a dataset that more accurately mirrors real-world scenarios. This facilitates a more robust and scalable evaluation of RAG systems.

