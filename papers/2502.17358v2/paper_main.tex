%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

\usepackage{soul} % for better UTF-8 support
% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{tabularx}
\usepackage{booktabs} % for professional tables
\usepackage{multirow}
\usepackage{lipsum}
\usepackage{comment}
\usepackage{threeparttable}
\usepackage{color,soul}
\usepackage{xspace}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{makecell}
\usepackage{float}

\usepackage[table]{xcolor}    % For colored cells
\usepackage{tcolorbox}        % For boxes with round corners
\usepackage{colortbl}
\tcbuselibrary{skins} 
\usepackage[utf8]{inputenc}


% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
\usepackage{inconsolata}
\usepackage[breaklinks=true]{hyperref}
\Urlmuskip=0mu plus 1mu\relax
\def\UrlBreaks{\do\/\do-}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
%\usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{icml2025}

\def\isaccepted{1}
\usepackage{icml2025}


% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{enumitem}



\definecolor{mybg}{RGB}{247,247,230} % Light yellow (same for heading & body)

\definecolor{top_frame}{RGB}{111,111,111}

% 1) Theorem-style box (like your original, no separate heading color)
\newenvironment{mytheorem}[1][]{%
  \begin{tcolorbox}[
    enhanced,
    colback=mybg,
    colframe=mybg,
    boxrule=0pt,
    arc=0pt,
    left=10pt,
    right=10pt,
    top=5pt,
    bottom=0pt,
    coltitle=black,
    fonttitle=\bfseries,
    % If you do not want any title, remove "title" or comment out the next line.
    title=#1 
  ]
}{
  \end{tcolorbox}
}



\newenvironment{mybox}[1][]{%
  \begin{tcolorbox}[
    enhanced,
    colback=mybg,
    colframe=top_frame,
    boxrule=0pt,
    arc=0pt,
    left=10pt,
    right=10pt,
    top=2pt,
    bottom=0pt,
    coltitle=black,
    fonttitle=\bfseries,
    % If you do not want any title, remove "title" or comment out the next line.
    title=#1
  ]
}{
  \end{tcolorbox}
}







% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\newcommand{\method}{DIS-CO\xspace}

\newcommand{\xuandong}[1]{{\color{red} [xuandong: #1]}}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}



% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{DIS-CO: Discovering Copyrighted Content in VLMs Training Data}


\begin{document}



\twocolumn[
\icmltitle{DIS-CO: Discovering Copyrighted Content in VLMs Training Data}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{André V. Duarte}{cmu,ist}
\icmlauthor{Xuandong Zhao}{berkeley}
\icmlauthor{Arlindo L. Oliveira}{ist}
\icmlauthor{Lei Li}{cmu}
\end{icmlauthorlist}

\icmlaffiliation{cmu}{Carnegie Mellon University}
\icmlaffiliation{ist}{INESC-ID / Instituto Superior Técnico, ULisboa}
\icmlaffiliation{berkeley}{UC Berkeley}

\icmlcorrespondingauthor{André V. Duarte}{andre.v.duarte@tecnico.ulisboa.pt}
\icmlcorrespondingauthor{Xuandong Zhao}{xuandongzhao@berkeley.edu}
\icmlcorrespondingauthor{Arlindo L. Oliveira}{arlindo.oliveira@tecnico.ulisboa.pt}
\icmlcorrespondingauthor{Lei Li}{leili@cs.cmu.edu}



% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Copyrighted Content Detection, Large Language Models, Natural Language Processing}


\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
\textit{How can we verify whether copyrighted content was used to train a large vision-language model~(VLM) without direct access to its training data?} Motivated by the hypothesis that a VLM is able to recognize images from its training corpus, we propose \method, a novel approach to infer the inclusion of copyrighted content during the model's development. By repeatedly querying a VLM with specific frames from targeted copyrighted material, \method extracts the content's identity through free-form text completions. To assess its effectiveness, we introduce MovieTection, a benchmark comprising 14,000 frames paired with detailed captions, drawn from films released both before and after a model’s training cutoff. Our results show that \method significantly improves detection performance, nearly doubling the average AUC of the best prior method on models with logits available. Our findings also highlight a broader concern: all tested models appear to have been exposed to some extent to copyrighted content. Our code and data are available at \url{https://github.com/avduarte333/DIS-CO}
\end{abstract}

\begin{figure}[ht]
\centering
  \includegraphics[width=0.48\textwidth]{Figures/crown_figure.pdf}
      \caption{Our \method reveals that VLMs successfully map frames of suspect movies to their titles, even when the frames are highly challenging. For example, the GPT-4o model accurately identifies the movie ``Frozen'' (blue circle), despite the varying complexity of its frames. In contrast, for newly released films, the models are unable to perform similar frame-to-title predictions.}
  \label{fig:crown_figure}
\end{figure}

\section{Introduction}
\label{Introduction}









The rapid evolution of large-scale models has driven a paradigm shift toward multimodality, with recent large vision-language models (VLMs) gaining prominence for their ability to process both visual and textual information \cite{flamingo, llava, GPT-4, qwen2}. While these models showcase remarkable performance across a variety of tasks, their reliance on vast, diverse datasets introduces challenges in ensuring compliance with ethical and legal standards. Without strict safeguards during the data collection step, proprietary content could be incorporated into the models' knowledge, opening the door to intellectual property infringements and potential legal conflicts \cite{CarliniShadow, ExtractRepeat,duan2024membership}. In fact, in the United States alone, more than 24 copyright lawsuits were filed against the AI industry since 2023 \cite{Wired_Copyright_Colection}, reflecting growing concerns about the use of protected material in training \cite{kadrey_v_meta, daily_news_v_microsoft}. %These legal and ethical disputes have, in turn, contributed to a growing wave of usage restrictions over popular high-quality data sources. For example, within the last year, over 28\% of the most actively maintained sources in the C4 dataset have become completely unusable, significantly reducing access to the diverse and representative data, much needed for AI training \cite{c4_evolution, c4_dataset}.
\par

Discovering training data is, therefore, essential for effectively addressing the ethical and legal challenges of model training. However, the lack of transparency in data collection (often justified by competitive concerns) makes it particularly difficult to determine whether specific materials have been used.

\par
%To tackle these challenges, Membership Inference Attacks~(MIAs) serve as a valuable tool to identify whether specific data samples were part of a model’s training set. While MIA techniques are well-studied for text-based models, their adaptation to multimodal settings, particularly VLMs, remains underexplored - a gap that our work aims to address.
To tackle these challenges, Membership Inference Attacks~(MIAs) serve as a tool to identify whether specific data samples were part of a model’s training set. While MIA techniques are well-studied for text-based models, their adaptation to multimodal settings, particularly VLMs, remains less explored - a gap that our work aims to address.
\par
Recent advancements have been made in this field, but certain challenges and limitations continue to exist. The $\mathrm{MaxR\acute{e}nyi\text{-}K\%}$ method \cite{renyi} is based on the intuition that data encountered during training leads to greater model confidence when generating outputs, reflected by a smaller Rényi entropy in the next-token probability distribution for image or text tokens. While this method proved effective in settings with unrestricted access to output logits, its applicability is limited in the context of black-box models like Gemini \cite{gemini_1.5}, which, at most, allow for inspection of the top-5 logits for each predicted token. The work of \citet{vqa_icml} addresses the challenge of discovering training data in a fully black-box fashion by introducing a method tailored to document-based VQA tasks. By removing key textual content from input images, the authors demonstrate the models' ability to recall memorized training data, including sensitive information. This approach is, nonetheless, limited to VQA datasets, which constitute only a small fraction of the diverse data types used to train VLMs.
\par

In this paper we propose \method, a novel method for detecting models' training data that overcomes the limitations of previous approaches while being applicable to both white- and black-box VLMs. The core idea of \method is to prompt models to map a set of images from a target media document to its corresponding identity (e.g., movie titles) in an unconstrained, free-form text-generation manner, enabling them to produce answers freely instead of selecting from predefined options. As a result, under the null hypothesis: \textit{the target content was not in the training dataset}, the model is much less likely to correctly identify the data, reducing false positives. Consequently, correct outputs become a more reliable indication that the target content was part of the training dataset \cite{null_hypothesis_mia}.

\par

This idea is well illustrated in Figure \ref{fig:crown_figure}, which demonstrates that models solve the task correctly far more often for content that was very likely included in their training data compared to content that was definitely excluded, such as movies released after the model’s cutoff knowledge date. For instance, \method maps frames from the movie \textit{Frozen} to the correct title in nearly 90\% of the test cases, while a movie like \textit{Aquaman and the Lost Kingdom} is correctly identified in less than 2\% of the time.
\par



We conduct experiments on two benchmarks, MovieTection (our newly introduced dataset) and VL-MIA/Flickr \citep{renyi}. MovieTection contains 14,000 diverse movie frames paired with descriptive captions, split chronologically based on films released before/after the models' training cutoff (October 2023). VL-MIA/Flickr, derived from COCO \citep{mscoco} (member data) and recent Flickr images (non-member data), serves as a proof-of-validity dataset for \method.

\par 
Our main contributions are as follows:
\begin{itemize}[leftmargin=*, itemsep=0pt, topsep=0pt]

\item We introduce \method, an innovative method applicable to both white-box and black-box VLMs, designed to detect whether copyrighted content was included in training. To the best of our knowledge, this is the first work to detect copyrighted movies in the context of VLMs.

\item We introduce MovieTection, a new benchmark designed for detecting training data of VLMs. The dataset includes content from 100 movie titles, unfolded into 14,000 frames, with a mix of easily identifiable and challenging examples to test model capabilities comprehensively.

\item Experiments show that \method effectively detects copyrighted movies across the six tested model families, with GPT-4o achieving an average accuracy of 34\% on the ``hard to guess'' frames of the suspect movies.

\item We show that fine-tuning a model to avoid disclosing memorized content for a particular movie is an effective defense strategy, capable of generalization to other titles.





\end{itemize}





\par


\section{Preliminary and Related Work}



Membership Inference Attacks (MIAs) are designed to determine whether a specific data instance was included in the training set of a machine learning model \cite{MIA_Original, mia_survey}. This area of research has seen growing interest with the increasing use of LLMs, which are known to memorize and occasionally reproduce training data \cite{ExtractRepeat, CarliniDiscoverableMemorization,goldfish}. 
\par
Classical MIAs are typically divided into two main approaches: reference-based and reference-free. Reference-based methods involve training a set of ``shadow models" to replicate the target model's behavior \cite{CarliniShadow,ShadowFineTuning1,ShadowFineTuning2,Shadow2}. In contrast, reference-free methods rely on calculating specific metrics, such as the perplexity of a sentence, to identify patterns indicative of training set membership~\cite{metric_based_paper1,metric_based_paper2,carlini2021extractingGPT2,metric_based_paper3}. Among these, the $\mathrm{Min\text{-}K}$\%-$\mathrm{Prob}$ method stands out as a more refined approach. It hypothesizes that the average log-likelihood of the top-k\% least probable tokens in an example is higher if the example was part of the training data compared to if it was not \cite{min-k-prob}. Building on this foundation, recent extensions such as $\mathrm{Min\text{-}K}$\%++ \cite{min_k_prob++} and DC-PDD~\cite{dcpdd_acl} have introduced further improvements. However, a key limitation of most reference-free methods is their dependence on access to token probability distributions, which restricts their interoperability with black-box models such as Gemini \cite{gemini_1.5}.
\par
With recent research shifting focus from text-only models to multi-modal architectures, the task of detecting training data and evaluating model memorization has begun to emerge in this domain as well \cite{deja_vu,deja_vu_2,vqa_icml}. Building on techniques originally developed for text-only models, \citet{renyi} propose a novel image-based MIA pipeline that adapts methods like $\mathrm{Min\text{-}K}$\%-$\mathrm{Prob}$ \cite{min-k-prob} to VLMs. The work presents the $\mathrm{MaxR\acute{e}nyi\text{-}K\%}$ metric, which enables image membership inference by analyzing the output logits corresponding to the model's image-specific slice.
\par

Detecting training data is especially significant when it involves copyrighted content, as the reproduction of such material by large models raises legal and ethical concerns~\cite{digger_copyright_paper, meeus_copyright_paper}. In the light of counterfactual memorization studies, the methods proposed by \citet{decop} and \citet{golchinQUIZ} perform membership inference through a multiple-choice question-answering (MCQA) setting. These approaches demonstrate solid results and have the advantage of being applicable to both white-box and black-box models, as they do not depend on access to token probabilities. However, it is known that multiple-choice scenarios may induce a selection bias~\cite{selectionBias} on the models, which introduces some uncertainty about whether chance played a role in the results. In contrast, \citet{CopyrightPromptingEMNLP2023} adopt a prompting approach with free-form text generation, aiming to elicit verbatim reproduction of copyrighted material. This provides stronger evidence of memorization because, in an unconstrained, free-form setting, the model is much less likely to produce correct outputs by chance. While this method may fail to detect cases where models are trained on copyrighted data without memorizing it \cite{igor_icml}, it also avoids the issues raised by \citet{blind_baselines} and \citet{did_you_train_on_my_dataset}, who warn that many membership inference methods risk overstating results by exploiting data distribution shifts, such as temporal patterns, rather than identifying genuine memorization. For these reasons, we also focus on free-form text generation in this work, as it provides a more robust and unbiased indication of whether the target content is part of the model's training data.




\begin{figure*}[t]
  \centering
  \includegraphics[width=0.95\textwidth]{Figures/pipeline.pdf}
  \caption{The pipeline begins with the construction of the MovieTection benchmark, where we categorize movie frames into main and neutral types to introduce varying levels of difficulty. Models are then queried with image frames and their corresponding captions, producing predictions for both modalities. Predictions from image frames that overlap with caption-based predictions are excluded, isolating cases where image-based memorization is inferred. Performance on the suspect movie is compared against the expected baseline performance, with discrepancies indicating potential training exposure.}
  \label{fig:method_full_pipeline}
\end{figure*}








\section{Benchmark: MovieTection}
\label{sec:MovieTection}

Our proposed benchmark, MovieTection, distinguishes member and non-member data based on a clearly defined temporal constraint. Movies released in 2024 or later are considered non-member data, as they fall outside the knowledge cutoff dates of all tested models. Movies from January to September 2023 are excluded due to uncertainty regarding models’ exposure to content from that period. For instance, Qwen2-VL~\cite{qwen2} reports a knowledge cutoff in June 2023. Movies released on or before 2022 are treated as potential member data, as they are more likely to have been included in the training datasets of such models.
\par
MovieTection currently comprises frames from 100 movies, with plans for future expansion. The selection of movies incorporated into the benchmark is guided by their status as box office hits, based on the assumption that highly popular movies, due to their widespread availability, are more likely to appear in training datasets. For the suspect data, we primarily select titles randomly from the Box Office Mojo's\footnote{\url{https://www.boxofficemojo.com/}} list of the all-time highest-grossing films, with some exceptions to accommodate specific experiments, such as analyzing the impact of IMDb\footnote{\url{https://www.imdb.com/}} ratings (Section \ref{sec:experiments}). For the clean data, we sample most titles from the Box Office Mojo's list of the highest-grossing films of 2024.
\par
For each movie, we extract frames categorized into two types: main frames and neutral frames. This categorization is designed to introduce varying levels of difficulty for assessing a model's knowledge about a movie. Main frames typically feature key characters to the movie’s plot. These frames are intended to be easily recognizable by viewers familiar with the movie. In contrast, neutral frames focus on ordinary visuals, such as landscapes, objects, or minor characters, that are not strongly tied to the movie's narrative. Neutral frames are designed to present a significantly higher challenge, as they rely on subtle contextual cues that are almost impossible to associate with the correct title without prior knowledge of the movie. Figure \ref{fig:method_full_pipeline} illustrates the two frame types.
\par
Each extracted frame is accompanied by a detailed caption, generated using the 7B version of the Qwen2-VL model. The prompt used for caption generation, along with an example, is provided in Appendix \ref{sec:captions_prompt_appendix}. In total, 140 frames are extracted per movie, comprising 100 main frames and 40 neutral ones.
\par
%The extraction process is performed manually to ensure that main frames are grouped by contextual similarity, which results on each set of five frames typically representing the same scene or sequence. This grouping choice serves to supports our specific experiment on whether longer context increases a model's ability to disclose memorized content (\textbf{REF TO SECTION}). Neutral frames, on the other hand, are harder to group with this logic in mind due to their scarcity within movies and are therefore selected individually without adhering to this pattern.



\section{DIS-CO}
\label{sec:Method}

Our proposed method, \method, determines whether examples are memorized by evaluating the model's performance on a question-answering task with free-form text responses. We argue that eliciting free-form completions is preferable to a multiple-choice question-answering (MCQA) format because it significantly reduces the influence of ``luck" associated with guessing. In multiple-choice settings, models can achieve high accuracy even when responding randomly, as the limited set of predefined options increases the likelihood of selecting the correct answer. In contrast, free-form responses require the model to generate answers without such constraints, making it far less likely for correct responses to arise by chance, even if this comes at the expense of a slightly lower accuracy.
\par
The task we propose involves models performing accurate identification of the content's identity which, in the case of copyrighted movie identification, corresponds to correctly identifying the movie title. We operate under the premise that models map a frame to the appropriate title far more reliably when that movie is included in their training data compared to when it is not. The specific prompts used for evaluating models on the MovieTection benchmark are provided in Appendix \ref{sec:eval_prompt_appendix}.
\par
The overall pipeline of \method is illustrated in Figure \ref{fig:method_full_pipeline}. After constructing the MovieTection dataset, we first query the models with clean data to establish a baseline for its expected performance on this set. While one might expect the models to fail completely on all these examples, given that these movies were unreleased at the time of the training cutoff, this is not always the case. Some of the movies in this set, though unreleased, were already announced and acknowledged by the models, leading to correct predictions for certain examples (See Table \ref{tab:main_results_accuracy_gpt4o} - Section \ref{sec:MainResults} and Appendix \ref{sec:clean_movies_ack_appendix}). Capturing this baseline performance is crucial to avoid incorrectly classifying a movie as part of the training data simply because some frames were accurately identified.
\par
Another important factor to consider is the time effect. In general terms, the older a movie is, the greater the likelihood that a model has residual knowledge about it. This knowledge can come from publicly available online content, such as movie posters, trailers, forum discussions, or datasets like OpenSubtitles \cite{opensubtitles_dataset}, which typically do not raise copyright infringement concerns. To estimate this baseline knowledge accumulated over time, we query the models using the detailed captions from the older movies, as making correct predictions based solely on textual descriptions is unlikely to be problematic.
\par
Finally, to determine whether a specific movie was likely included in the model’s training data, we query the model separately with image frames and then with their corresponding caption information from the suspect movie. After both queries are completed, we compare the predictions from these two inputs. If there is an overlap of correct predictions between the frame-based and caption-based queries, we disregard those results, as they suggest the model did not had to rely on the image content to make accurate predictions. By examining the remaining correct predictions, which rely solely on image content, we infer whether the model is utilizing memorized visual information. Ideally, after removing the intersection, the performance of the suspect movie should fall within this range defined by the baseline performance on the recent movies and the clean baseline performance on older movies with accumulated knowledge over time. However, if performance remains significantly higher than this range, even after removing the intersection, it suggests that the model relied on memorized visual information specific to the movie frames, indicating the movie was likely included in its training data. 




\subsection{Upper-Bound Estimation of Memorization}
While our proposed approach of removing the intersection between frame-based and caption-based correct predictions provides a more precise set of potentially memorized movies, we cannot rule out that those frames are not part of the training data. Consequently, we also consider an upper-bound estimation of memorization, where all correctly identified frames, regardless of their intersection with captions, contribute to the possible classification of the movie as part of the training data. For clarity, throughout the remainder of this paper, we use two notations: $\lfloor \text{\method} \rfloor$ represents the smaller set obtained after removing the intersection, while \method denotes the upper-bound estimation, including all correctly identified frames.


\subsection{Mitigating the Disclosure of Memorization}
\label{sec:Defense}
While training on copyrighted data may occasionally be unavoidable, the associated risks can be mitigated by ensuring that the model does not disclose memorization. For a movie that is identified as likely included in the training data, we propose fine-tuning the model on a subset of its frames while replacing the movie label with a neutral designation such as `Copyrighted Content'. More details on the fine-tuning in Appendix \ref{sec:fine_tuning_appendix}.







\section{Experiments}
\label{sec:experiments}

We assess the effectiveness of \method through a range of different experiments, which are guided by the following questions:


\begin{itemize}[label=•, leftmargin=*, itemsep=0pt, topsep=0pt]

\item \textbf{Does a longer context reveal more memorization?}
As LLMs often perform better with more context in their queries, we hypothesize that VLMs behave similarly. Using the MovieTection dataset, we examine the effect of varying the number of frames in the prompt ($N \in [1, 4]$).


\item \textbf{Are factors like movie popularity or quality good proxies for memorization?}
To test whether popularity (e.g., box-office revenue) or quality (e.g., IMDb ratings) are proxies for memorization, we collect movies where one factor varies while the other is controlled. For instance, in the box-office experiment, movies with similar IMDb ratings are chosen to isolate the impact of popularity.


\item \textbf{How susceptible are models to memorization when exposed to new data?}
We investigate the model's ability to memorize new content by fine-tuning it on a movie guaranteed to be outside the training data. 

\item \textbf{How to prevent a model to disclose memorization?} Similarly to the previous experiment, we fine-tune the model (this time on a suspect movie), with a modified labeling objective. This experiment investigates whether this defense mechanism can mitigate memorization disclosure for the suspect movie and whether its effects generalize to other movies.


\item \textbf{To what extent does generalization influence the model's performance?} Humans are capable of generalizing from partial information, often identifying movies they haven’t fully seen by relying on related content such as posters or trailers. To assess how closely models align with humans on this movie detection task, we compare the performance of the models with that of 10 human participants who were selected to identify 200 images from MovieTection.



\end{itemize}



\subsection{Experiment Setup}


To evaluate \method, we follow the procedure outlined as follows. Let the ``Suspect" group be represented as \( S = \{s_1, s_2, \ldots, s_{N_S}\} \) and the ``Clean" group as \( C = \{c_1, c_2, \ldots, c_{N_C}\} \), where \( N_S \) and \( N_C \) denote the number of movies in each group, respectively. For each movie, we calculate its accuracy: \( A(s_i) \) for \( s_i \in S \) and \( A(c_j) \) for \( c_j \in C \). The accuracy is calculated as the proportion of predictions aligning with the expected outcomes. By default, a weighted average is then applied to account for the unequal proportions of main and neutral frames and the total value is reported. Nonetheless, some results for main and neutral frames are reported individually to provide further insights on the performance across frame types.
\par
We then perform a random sampling process with replacement, repeated 10 times. In each iteration, \( M \) elements are sampled from each group, where \( M \) corresponds to \( N_S \) or \( N_C \), depending on the group being sampled. For each iteration, a threshold \( \theta \) is optimized to achieve maximum separation between the two groups, and the Area Under the Curve (AUC) is computed.  
\par  
To complete the analysis, we calculate the mean and standard deviation of the AUC or the average accuracy for the ``Suspect'' and ``Clean'' groups over these iterations. Detection is consistently conducted at the movie level, rather than on individual frames.




\subsubsection{Benchmarks and Baselines}
\label{sec:Benchmarks_and_Baselines}


We begin by evaluating \method using the proof-of-concept dataset VL-MIA/Flickr introduced by \citet{renyi}. This dataset consists of 600 images, evenly divided into training and non-training based on a temporal split. Member images are sourced from a subset of COCO \cite{mscoco}, while non-member images are obtained from Flickr, restricted to content published after January 1, 2024. This temporal separation aligns with the knowledge cutoff dates of the models used in our evaluation.
\par
For the fine-tuning experiments, we use two movies: \textit{IF} (2024) and \textit{Moana} (2016), which have nearly identical durations (1h48min and 1h47min, respectively), allowing us to sample frames at an equal rate, resulting in 6000 frames per movie. The remaining experiments utilize the MovieTection dataset, as detailed in Section \ref{sec:MovieTection}.
\par
We evaluate \method against three baselines: (i) Captions, (ii) MCQA, and (iii) Rényi ($\alpha = 0.5$). The Captions baseline involves prompting the models using only the textual information available in MovieTection. MCQA (Multiple-Choice Question Answering) presents the models with four possible answers per query, designed to be slightly challenging by including similar movies as distractors (e.g., animated movies are paired with other animated ones). The Rényi baseline applies the $\mathrm{Max\text{-}K}$\% method ($\alpha = 0.5$) proposed by \citet{renyi}. We report the results for the value of $\mathrm{K}$
 that achieves the best detection performance.













\section{Results}
\label{sec:Results}



\subsection{Proof-of-Concept}
\subsubsection{VL-MIA/Flickr}
\label{sec:poc_flickr}

As introduced in Section \ref{sec:Benchmarks_and_Baselines}, VL-MIA/Flickr is an MIA dataset where the `suspect' images are sourced from COCO \cite{mscoco}. This proof-of-concept is essential because, while the inclusion of movies in the training data of VLMs remains uncertain, the presence of COCO images is well-documented in the training data of various models \cite{clip, llava}, making it an ideal validation dataset for \method. Demonstrating \method's ability to detect COCO data supports its effectiveness and underscores its potential applicability to similar scenarios.



\begin{table}[ht]
\setlength\extrarowheight{1.4pt}
\centering
\caption{Accuracy of \method and MCQA on the suspect split of VL-MIA/Flickr.}
\resizebox{0.9\columnwidth}{!}{%
\begin{tabular}{lccc}
\toprule
  \textbf{Method} & \textbf{GPT-4o} & \textbf{Gemini-1.5 Pro} &\textbf{ Qwen2-VL 72B}\\ \midrule 
\method  & 0.413$_{0.01}$   & 0.243$_{0.00}$  &0.183$_{0.00}$   \\ MCQA  & 0.02$_{0.01}$   &  0.250$_{0.00}$  & 0.483$_{0.00}$ \\  \bottomrule
\end{tabular}%
}
\label{tab:poc_flickr}
\end{table}



As shown in Table \ref{tab:poc_flickr}, \method enables the models to achieve competitive accuracy, particularly with GPT-4o, which scores 0.413 despite the inherent difficulty of the task. This result underscores the models' capacity to identify their training data, aligning with the high probability that these images are part of the datasets used during pretraining.
\par
By contrast, GPT-4o faces difficulties when performing the task in a MCQA setting, achieving an accuracy of only 0.02. Further analysis reveals that this gap is due to selection bias, which, as illustrated in this example, can significantly affect a VLM’s performance (extra details in Appendix \ref{sec:selection_bias_appendix}).


\begin{table*}[t]
\centering
\caption{AUC Scores for detecting copyrighted movies present in models training data for MovieTection. The best score in each column is highlighted in bold.}
\label{tab:all_results}
\begin{tabular}{@{}clcccc@{}lc} % Change column alignment
\toprule
 &  & \textbf{GPT-4o} & \textbf{Gemini-1.5 Pro} & \textbf{LLaMA-3.2 90B} & \textbf{Qwen2-VL 72B}   && \textbf{Avg.}\\ \midrule
\multirow{4}{*}{Neutral Frames} 
 & Captions & 0.888$_{0.027}$& 0.908$_{0.031}$& 0.826$_{0.021}$& 0.811$_{0.027}$  &&0.858\\
 & MCQA & 0.758$_{0.048}$& 0.722$_{0.037}$& 0.737$_{0.052}$& \textbf{0.898}$_{0.015}$  &&0.778\\
 & Rényi ($\alpha$ = 0.5)& -& -& 0.363$_{0.052}$& 0.598$_{0.050}$  &&0.481\\
 & $\lfloor \text{\method} \rfloor$& 0.987$_{0.010}$& 0.936$_{0.024}$& 0.892$_{0.021}$& 0.897$_{0.023}$& &0.928\\
 & \method & \textbf{0.989}$_{0.010}$& \textbf{0.942}$_{0.025}$& \textbf{0.897}$_{0.020}$& 0.893$_{0.025}$  &&\textbf{0.930}\\ 
\cmidrule[0.5pt](l){1-8} % Subtle line for separation
\multirow{4}{*}{Main Frames} 
 & Captions & 1.000$_{0.000}$& 0.963$_{0.029}$& 0.912$_{0.028}$& 0.924$_{0.022}$  &&0.949\\
 & MCQA & 0.769$_{0.048}$& 0.704$_{0.040}$& 0.761$_{0.040}$& 0.899$_{0.014}$  &&0.783\\
 & Rényi ($\alpha$ = 0.5)& -& -& 0.514$_{0.050}$& 0.590$_{0.061}$  &&0.552\\
 & $\lfloor \text{\method} \rfloor$& 1.000$_{0.000}$& 0.978$_{0.024}$& 0.978$_{0.010}$& 0.979$_{0.014}$& &0.983\\
 & \method & \textbf{1.000}$_{0.000}$& \textbf{0.981}$_{0.022}$& \textbf{0.986}$_{0.006}$& \textbf{0.986}$_{0.016}$  &&\textbf{0.988}\\ \bottomrule
\end{tabular}
\end{table*}


\begin{table}[ht]
\setlength\extrarowheight{1.4pt}
\centering
\caption{Average accuracy scores in for GPT-4o on the MovieTection dataset. Scores are produced only based on the neutral frames.}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lcc}
\toprule
GPT-4o Accuracy& \textbf{Suspect Movies}& \textbf{Clean Movies}\\ \midrule 
\ \ Captions & 0.128$_{0.01}$   & 0.001$_{0.00}$     \\
\ \ MCQA & 0.721$_{0.02}$                 & 0.410$_{0.05}$                   \\
$\lfloor \text{\method} \rfloor$ & 0.226$_{0.02}$                 & 0.002$_{0.00}$                   \\
\ \ \method                & \textbf{0.338}$_{0.03}$   & \textbf{0.002}$_{0.00}$     \\ \bottomrule
\end{tabular}%
}
\label{tab:main_results_accuracy_gpt4o}
\end{table}



\subsubsection{Learning Clean Movie}
\label{sec:clean_movie_learn}


\begin{figure}[ht]

  \includegraphics[width=0.46\textwidth]{Figures/fine_tuning-dirty.pdf}
  \vspace{-0.35cm}
      \caption{Accuracy of Qwen2-VL 7B in identifying a clean movie as a function of the number of unique fine-tuning frames.}
  \label{fig:clean_tune}
\end{figure}

To pivot towards our primary goal of detecting copyrighted movies, this experiment investigates the memorization capabilities of a VLM by intentionally fine-tuning it on a movie it has definitively never encountered before.
\par
From Figure \ref{fig:clean_tune}, we draw three key observations. First, it is highly unlikely for the model to accurately predict a clean movie without prior exposure. Second, training on randomly ordered frames accelerates generalization compared to sequential ordering. Third, the model begins to accurately detect the movie after seeing as few as 1500 frames. 

% uncomment this for camera-ready.
%However, this apparent ``ease" of learning may be attributed to the fact that we are doing a targeted supervised fine-tuning rather than performing a full pretraining pipeline. In the latter scenario, memorization would likely diminish as training progresses \cite{LLM360}, ultimately converging to values more inline to what we report in \ref{sec:additional_main_results_appendix_small_open_source_accuracy}.
\par
These findings underscore the significant capacity of even relatively small models, like Qwen2-VL 7B, to memorize visual content with minimal exposure. If a model of this size can achieve such memorization under targeted fine-tuning, it is highly likely that larger, more expressive models, such as GPT-4o or LLaMA-3.2 90B, would demonstrate similar or greater tendencies, even with a different training strategy, like pretraining, as previous studies show that memorization scales with model size and capacity \cite{CarliniDiscoverableMemorization,decop}.






\subsection{Main Results}
\label{sec:MainResults}









Initially, we evaluate \method and $\lfloor \text{\method} \rfloor$ in comparison to baseline methods, focusing on their performance in distinguishing between training and non-training data, as shown in Table \ref{tab:all_results}. For instance, for neutral frames, \method achieves an average AUC of 0.930, with $\lfloor \text{\method} \rfloor$ closely following at 0.928, indicating that removing predictions overlapping with captions has minimal impact on detection performance. This finding underscores the robustness of both \method variants, with $\lfloor \text{\method} \rfloor$ offering an added advantage by reducing potential biases. Notably, both variants surpass other baselines across AUC metrics, with the Rényi method underperforming significantly, yielding an average AUC closer to 0.5. 
\par
Next, we assess the performance of \method and $\lfloor \text{\method} \rfloor$ in terms of accuracy. While captions achieve relatively strong AUC values (e.g., 0.858 for neutral frames), their overall accuracy on suspect movies is less compelling. As presented in Table \ref{tab:main_results_accuracy_gpt4o}, \method and $\lfloor \text{\method} \rfloor$ achieve consistently higher average accuracy scores for suspect movies, effectively identifying memorized content with greater reliability. Although MCQA achieves the highest accuracy for suspect movies, it also incorrectly classifies much of the clean data as suspect. This behavior inflates its accuracy which consequently results in a large number of false positives, ultimately lowering its AUC performance, as seen in Table \ref{tab:all_results}. By contrast, \method variants maintain a more balanced approach, avoiding such pitfalls and achieving superior performance across both suspect and clean datasets.







\subsection{Longer Context}
\label{sec:longer_context}


\begin{figure}[ht]
\centering
  \includegraphics[width=0.48\textwidth]{Figures/longer_context.pdf}
  \vspace{-0.35cm}
      \caption{DIS-CO’s accuracy on the MovieTection suspect split with varying numbers of frames in the prompt. Accuracy improves as the number of frames increases, suggesting that longer contexts enable models to perform better predictions. Scores are produced with the weighted combination of the main and neutral frames.}
  \label{fig:longer_context}
\end{figure}


\begin{figure*}
    \centering
    \begin{minipage}{.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Figures/box_office_total_closed_source.pdf}
        \captionsetup{justification=centering}
      \vspace{-0.8cm}
      \caption{Box-Office effect of suspect movies on \method's performance. Higher box-office revenue leads to improved performance across models. This suggests that popular movies are more likely to be memorized by models, likely due to their increased presence in training datasets. Scores are produced with the weighted combination of the main and neutral frames.}
        \label{fig:popularity}
    \end{minipage}\quad
    \begin{minipage}{.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Figures/imdb_rating_total_closed_source.pdf}
        \captionsetup{justification=centering}
        \vspace{-0.8cm}
        \caption{IMDb movie rating effect of suspect movies on \method's performance. Detection improves with higher IMDb ratings, with a notable trend across models starting from a rating of 6. Higher-quality movies might have a stronger presence in datasets and are therefore more likely to be memorized. Scores are produced with the weighted combination of main and neutral frames.}
        \label{fig:quality}
    \end{minipage}
\vspace{-0.3cm}
\end{figure*}

We evaluate the effect of increasing the number of frames in the prompt on \method's detection performance. As shown in Figure \ref{fig:longer_context}, there is a positive correlation between the number of frames and performance, with the trend closely approximating a linear pattern. Moreover, GPT-4o demonstrates a clear performance advantage, consistently outperforming Gemini and the two other white-box models. Further results and analysis can be found in Appendix \ref{sec:long_context_appendix}.








\subsection{Popularity and Quality}
\label{sec:popularity_quality}







We investigate the relationship between memorization and two key factors: movie popularity (box office revenue) and quality (IMDb ratings). As shown in Figures \ref{fig:popularity} and \ref{fig:quality}, both factors exhibit a positive correlation with detection performance, albeit with slightly different patterns. Higher box office revenue leads to a consistent improvement across models, with GPT-4o showing the strongest gains. For IMDb ratings, performance generally improves as ratings increase, with a minor U-shaped trend observed at the lower end for GPT-4o and Gemini-1.5 Pro. From a rating of 6 onward, the positive trend becomes more pronounced and consistent across models. These results suggest that both popularity and quality serve as useful proxies for memorization, with each exhibiting unique dynamics that may vary depending on the specific range of the factor being analyzed.














\subsection{Preventing Disclosure of Memorization}
\label{sec:preventing_memorization}



The results in Figure \ref{fig:fine_tuning} validate our premise that fine-tuning a model with an alternate target label can effectively prevent it from revealing its knowledge of a suspect movie.
\par
The results from this experiment align closely with those presented in Section \ref{sec:clean_movie_learn}.  The key insight, however, is that the model learns the task significantly faster, requiring only 500 frames compared to the 1500 frames needed for learning a new movie - a 3x reduction in the number of frames needed.


\par

To evaluate the generalization capabilities of our approach, we analyze the model's performance on a subset of the MovieTection subset, focusing on the neutral frames. As shown in Table \ref{tab:generalization_subset}, fine-tuning the model to label the \textit{Moana} movie as `Copyrighted Content' improved its ability to classify other animated movies (Lion King and Frozen) as copyrighted, with accuracies of 0.625 and 0.450, respectively. This suggests that the model successfully associates similar visual styles or content characteristics with the `Copyrighted Content' label. In contrast, non-animated movies (La La Land and Baywatch) exhibited much lower accuracies of 0.050 and 0.020, respectively. This highlights the model's capacity to generalize within a specific content domain while avoiding overgeneralization across dissimilar genres. Further results are presented in Appendix \ref{sec:fine_tuning_appendix}.


\begin{figure}[h]
\centering
  \includegraphics[width=0.46\textwidth]{Figures/fine_tuning-safe.pdf}
  \vspace{-0.35cm}
    \caption{Accuracy of Qwen2-VL 7B in preventing memorization disclosure of a previously learned movie, as a function of the number of unique fine-tuning frames.}
  \label{fig:fine_tuning}
  %\vspace{-0.3cm}
\end{figure}


\begin{table}[h]
\setlength\extrarowheight{1.4pt}
\centering
\caption{Accuracy for neutral frames of MovieTection subset before and after fine-tuning to prevent disclosing memorization.}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lcccc}
\toprule
\textbf{Fine-Tuning} & \textbf{Lion King}& \textbf{Frozen} & \textbf{La La Land} & \textbf{Baywatch}\\ \midrule 
\ \ Before & 0.000   & 0.000  &  0.000 & 0.000  \\
\ \ After  & 0.625& 0.450  &  0.050& 0.020 \\ \bottomrule
\end{tabular}%
}
\label{tab:generalization_subset}
\end{table}




\subsection{Human Experiment}

\begin{figure}[ht]
\centering
  \includegraphics[width=0.46\textwidth]{Figures/human_experiment_neutral.pdf}
      \caption{Human evaluators' performance on MovieTection subset. Scores are produced only based on the neutral frames.}
  \label{fig:human_experiment}
\end{figure}



In this final experiment, our goal is to assess whether some of the performance displayed by \method could be attributed to generalization capabilities rather than memorization. From Figure \ref{fig:human_experiment}, two key insights emerge. First, humans demonstrate the ability to recognize certain neutral frames when they have previously seen the movie, achieving an average detection accuracy of 0.190. This is closely aligned with the performance of $\lfloor \text{\method} \rfloor$ on suspect movies in Table \ref{tab:main_results_accuracy_gpt4o}, though slightly lower (0.190 vs. 0.226).
\par
The second insight concerns human accuracy when identifying movies they have not seen but may be aware of: a pure generalization result. In this case, the average accuracy drops significantly to 0.023, highlighting the difficulty of recognizing movies without prior exposure. Even if the scores of both \method variants in Table \ref{tab:main_results_accuracy_gpt4o} were adjusted to account for a similar generalization effect, their detection accuracy would still surpass text-only detection methods. This supports our hypothesis that the superior performance of \method is not merely a result of generalization or residual knowledge from publicly available content. Instead, it strongly suggests that the models were exposed to some copyrighted content from MovieTection during training.








\section{Conclusions}

In this study, we introduce \method to analyze the potential inclusion of copyrighted content in VLMs training data, by testing whether models can map movie frames to their titles using free-form text generation. The key intuition is that models trained on specific content are more likely to identify it, even when prompted with less distinctive frames.
\par
We validate \method on recognizing COCO images, a standard inclusion on VLM training, and then expand its use to detecting copyrighted movies. The results show that \method consistently outperforms existing approaches while being compatible with both white-box and black-box models.
\par
The limited ability of human evaluators to correctly identify movies they have not seen suggests that the models' accurate predictions are more likely a result of being trained on this content, rather than generalization or publicly available data.





% Acknowledgements should only appear in the accepted version.
%\section*{Acknowledgements}

%We acknowledge the financial support provided by the Recovery and Resilience Fund towards the Center for Responsible AI (Ref. C628696807-00454142), and the financing of the Foundation for Science and Technology (FCT) for INESC-ID (Ref. UIDB/50021/2020). Lei Li is partly supported by the CMU CyLab seed grant.


\section*{Impact Statement}


This research advances the field of Machine Learning by introducing a method for detecting data used in training vision-language models. Our work primarily serves as an academic reference, contributing to a broader understanding of the presence of copyrighted materials within training datasets, for which our findings may help inform discussions on compliance, attribution, and compensation for content owners. However, while our approach offers a new perspective, we emphasize that its real-world applications should be considered with caution, given the academic nature and limitations of our methodology.
\par
We also recognize that the release of the MovieTection dataset may raise ethical considerations related to copyright. However, we argue that the dataset falls within the scope of fair use due to the following reasons:
\par
First, we limit our dataset to 140 frames per title, a small fraction of any full-length film, ensuring minimal redistribution of copyrighted content.
\par
Second, the purpose and scope of MovieTection is strictly academic. The dataset is intended solely for research and serves no commercial purpose that could conflict with the interests of copyright holders.
\par
Finally, we believe that our dataset does not impact the market value of the original films. Since the dataset consists of a sparse collection of individual frames, it does not substitute for watching the films, nor does it reduce demand for legitimate viewings.


% Acknowledgements should only appear in the accepted version.
\section*{Acknowledgements}

We acknowledge the financial support provided by the Recovery and Resilience Fund towards the Center for Responsible AI (Ref. C628696807-00454142), and the financing of the Foundation for Science and Technology (FCT) for INESC-ID (Ref. UIDB/50021/2020). This work is also co-financed by FCT through the Carnegie Mellon Portugal Program under the fellowship PRT/BD/155049/2024. Lei Li is partly supported by the CMU CyLab seed grant.


\bibliography{paperref}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn


\newpage

\section{Neutral Frames - Examples of Model Predictions}
\label{sec:neutral_frames_examples}


\begin{figure}[H]
    \centering
    \begin{minipage}{.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Appendix_Figures/ex1.pdf}
        \captionsetup{justification=centering}
      \caption{Movie: \textit{Notting Hill}}
    \end{minipage}\quad
    \begin{minipage}{.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Appendix_Figures/ex2.pdf}
        \captionsetup{justification=centering}
        \caption{Movie: \textit{1917}}
    \end{minipage}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{minipage}{.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Appendix_Figures/ex3.pdf}
        \captionsetup{justification=centering}
      \caption{Movie: \textit{Baywatch}}
    \end{minipage}\quad
    \begin{minipage}{.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Appendix_Figures/ex4.pdf}
        \captionsetup{justification=centering}
        \caption{Movie: \textit{Jurassic Park}}
    \end{minipage}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{minipage}{.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Appendix_Figures/ex5.pdf}
        \captionsetup{justification=centering}
      \caption{Movie: \textit{La La Land}}
    \end{minipage}\quad
    \begin{minipage}{.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Appendix_Figures/ex6.pdf}
        \captionsetup{justification=centering}
        \caption{Movie: \textit{Life of Pi}}
    \end{minipage}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{minipage}{.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Appendix_Figures/ex7.pdf}
        \captionsetup{justification=centering}
      \caption{Movie: \textit{Moana}}
    \end{minipage}\quad
    \begin{minipage}{.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Appendix_Figures/ex8.pdf}
        \captionsetup{justification=centering}
        \caption{Movie: \textit{The Greatest Showman}}
    \end{minipage}
    \captionsetup{labelformat=empty}
\end{figure}
\begin{center}
Neutral frame examples from MovieTection. The tested models are GPT-4o, Qwen2-VL 72B, Gemini-1.5 Pro, and LLaMA-3.2 90B (from top to bottom). Green indicates correct predictions, while red denotes incorrect predictions.
\end{center}






\newpage



\section{Captions Generation Prompt - Example}
\label{sec:captions_prompt_appendix}





\begin{table}[H]
  \centering
    \caption{Qwen2-VL 7B Prompt (along with two examples), to generate a detailed image caption. We specifically instruct the model to not disclose specific knowledge of the image to avoid biasing the QA task.}
  \begin{tabularx}{0.95\textwidth}{X}
    \toprule[1.1pt] % Thick top rule
    \textbf{Prompt:} Your task is to generate a detailed caption of this image. You should not reveal if you know who are the characters / what movie this is from. You need to make an unbiased caption generation, writing at most a single paragraph. Start the answer with `The image depicts' ...
    

\\ 
\textbf{Example 1:}

    \begin{center}
\includegraphics[width=0.65\textwidth]{Appendix_Figures/background_Aladdin_scene_shot_14.png}
    \end{center}

    \textbf{Output:} The image depicts a grand, ancient tower with intricate architectural details, standing prominently in the foreground of a bustling, sprawling cityscape. The tower features multiple levels, each adorned with ornate balconies and arched windows, some of which are draped with red curtains. The city below is a vibrant mix of buildings, with a variety of architectural styles, including domes and spires, suggesting a rich cultural heritage. The scene is bathed in warm sunlight, casting soft shadows and highlighting the textures of the stone and brickwork.

\\ \\
\textbf{Example 2:}


    \begin{center}
\includegraphics[width=0.65\textwidth]{Appendix_Figures/Titanic_scene_16_shot_3.png}
    \end{center}

    \textbf{Output:} The image depicts a vintage car, likely from the early 20th century, parked indoors in a dimly lit room. The car has a classic design with a red body and gold accents, featuring large, spoked wheels and a prominent front grille. In the background, two individuals are standing near the car, one dressed in a white dress and the other in a dark coat, suggesting a scene from a historical or period drama. The setting appears to be a museum or a collection, given the presence of other vintage vehicles and maritime artifacts in the background.

    \\              % Add a line break
    \bottomrule[1.1pt] % Thick bottom rule
  \end{tabularx}
\end{table}



\newpage


\section{MovieTection Evaluation Prompts}
\label{sec:eval_prompt_appendix}


\subsection{\method: Image Input}


\begin{table}[ht]
  \centering
    \caption{Generic prompt example for a MovieTection movie. Image Input with Free-Form Text Generation.}
  \begin{tabularx}{0.95\textwidth}{X}
    \toprule[1.1pt] % Thick top rule
    \textbf{Prompt:} The following image is a frame from a certain scene from a certain movie.\\
    Can you name the movie?\\
    Here you should make your guess for the movie in the image. Your guess must be only the movie name.
    

    \\
    \begin{center}
    $<$\textbf{Image Here}$>$    
    \end{center}
    \\
    
    
    \\\textbf{Answer:}
    \\              % Add a line break
    \bottomrule[1.1pt] % Thick bottom rule
  \end{tabularx}
\end{table}



\subsection{MCQA Image Input}


\begin{table}[H]
  \centering
    \caption{Generic prompt example for a MovieTection movie. Image Input with MCQA.}
  \begin{tabularx}{0.95\textwidth}{X}
    \toprule[1.1pt] % Thick top rule

    \textbf{User Prompt:} Question: The following image is a frame from a certain scene from a certain movie.\\
    Which of the following 4 options is the movie of this frame?\\\\

    \textbf{Options:}\\
    \textbf{A.} Movie 1\\
    \textbf{B.} Movie 2 \\
    \textbf{C.} Movie 3\\
    \textbf{D.} Movie 4\\

    \\
    \begin{center}
    $<$\textbf{Image Here}$>$    
    \end{center}
    \\
    
    \\\textbf{Answer: }
    \\              % Add a line break
    \bottomrule[1.1pt] % Thick bottom rule
  \end{tabularx}
\end{table}


\subsection{Text Captions Only Input}


\begin{table}[ht]
  \centering
    \caption{Generic prompt example for a MovieTection movie. Caption Input with Free-Form Text Generation.}
  \begin{tabularx}{0.95\textwidth}{X}
    \toprule[1.1pt] % Thick top rule
    \textbf{Prompt:} The following caption describes a scene from a certain movie.\\
    Can you name the movie?\\
    Here you should make your guess for the movie in the image based on the caption. Your guess must be only the movie name.


    \\
    \begin{center}
    $<$\textbf{Caption Here}$>$    
    \end{center}
    \\
    
    
    \\\textbf{Answer:}
    \\              % Add a line break
    \bottomrule[1.1pt] % Thick bottom rule
  \end{tabularx}
\end{table}


\newpage

\section{Models Acknowledge Upcoming Movies}
\label{sec:clean_movies_ack_appendix}


\begin{table}[ht]
  \centering
    \caption{GPT-4o, without searching the web, correctly acknowledges the existence of an upcoming Aquaman movie.}
  \begin{tabularx}{0.95\textwidth}{X}
    \toprule[1.1pt] % Thick top rule
    \textbf{Prompt:} Without looking online, what can you tell me about the Aquaman 2 movie? \\ \ \\
    
    \textbf{Answer:} As of my latest knowledge update (October 2023), \textit{Aquaman and the Lost Kingdom} (commonly referred to as \textit{Aquaman 2}) is the sequel to the 2018 film \textit{Aquaman}. Directed by James Wan, it is part of the DC Extended Universe (DCEU) and stars Jason Momoa as Arthur Curry/Aquaman. The movie was expected to continue exploring Aquaman’s dual responsibilities as the King of Atlantis and a superhero defending the surface world.

\textbf{Key Details:} 
\begin{itemize}[label=•, leftmargin=*]
        \item \textbf{Plot:} The movie was rumored to delve into darker themes compared to its predecessor and would explore Arthur’s responsibilities as a leader of Atlantis. Black Manta (played by Yahya Abdul-Mateen II) was confirmed to return as a major antagonist, seeking revenge against Aquaman for their conflict in the first movie.
        \item \textbf{Returning Cast:} In addition to Jason Momoa, other returning cast members include Amber Heard as Mera, Patrick Wilson as Orm (Ocean Master), and Temuera Morrison as Thomas Curry (Aquaman’s father).
        \item \textbf{New Additions:} The movie was reported to feature new characters, with Pilou Asbæk (known from \textit{Game of Thrones}) joining the cast in an unspecified role.
        \item \textbf{Creative Team:} James Wan, who directed the first film, returned to helm the sequel. The script was written by David Leslie Johnson-McGoldrick, who co-wrote the original \textit{Aquaman}.
        \item \textbf{Visual Style:} Like the first film, \textit{Aquaman 2} was expected to have visually stunning underwater scenes and innovative effects to depict Atlantis and other aquatic realms.
    \end{itemize}
\\
    \bottomrule[1.1pt] % Thick bottom rule
  \end{tabularx}
\end{table}



\vspace{-0.5cm}

\begin{table}[H]
\centering
\caption{Complete List of Clean MovieTection Movies that GPT-4o acknowledges the existence of.}
\label{tab:upcoming_movies}
\begin{tabularx}{0.8\textwidth}{lcc}
\toprule
\textbf{Movie Title} & \textbf{Release Date (Month-Year)} & \textbf{Total Accuracy} \\
\midrule
A Quiet Place: Day One & June-2024 & 0.00 \\
Alien: Romulus & August-2024 & 0.00 \\
Aquaman and the Lost Kingdom & December-2023 & 0.02 \\
Argylle & February-2024 & 0.06 \\
Back to Black & April-2024 & 0.00 \\
Bad Boys: Ride or Die & June-2024 & 0.00 \\
Blink Twice & August-2024 & 0.00 \\
Bob Marley: One Love & February-2024 & 0.02 \\
Deadpool \& Wolverine & July-2024 & 0.00 \\
Despicable Me 4 & July-2024 & 0.00 \\
Exhuma & February-2024 & 0.00 \\
Furiosa: A Mad Max Saga & May-2024 & 0.00 \\
Ghostbusters: Frozen Empire & March-2024 & 0.00 \\
Godzilla Minus One & December-2023 & 0.06 \\
Godzilla x Kong: The New Empire & March-2024 & 0.00 \\
Inside Out 2 & June-2024 & 0.00 \\
Joker: Folie à Deux & October-2024 & 0.00 \\
Kingdom of the Planet of the Apes & May-2024 & 0.00 \\
Kung Fu Panda 4 & March-2024 & 0.00 \\
Trolls Band Together & November-2023 & 0.00 \\
\bottomrule
\end{tabularx}
\end{table}


From Table \ref{tab:upcoming_movies}, we observe that GPT-4o acknowledges 20 out of the 50 movies listed in MovieTection. Nonetheless, for the vast majority of these movies, this acknowledgment does not correspond to a positive accuracy.



\newpage


\section{Fine-Tuning Experiments}
\label{sec:fine_tuning_appendix}

To conduct the fine-tuning experiments, we select two distinct movies: \textit{IF} and \textit{Moana}, representing the clean and suspect titles, respectively. For each movie, we create a supervised fine-tuning (sft) dataset consisting of 6,000 frames. Each frame is paired with a task-specific prompt that instructs the model to identify the movie's title. To avoid overfitting, the prompts are randomly sampled from a pool of 30 paraphrased versions, generated using GPT-4o. Figure \ref{fig:sft_example_appendix} illustrates one example of the created sft data for the selected movies.
\par
We explore the trade-off between the quantity of training data and the model’s ability to memorize content by varying the proportion of frames used for fine-tuning. Specifically, we test seven configurations, using 1\%, 5\%, 10\%, 25\%, 50\%, 75\%, and 100\% of the movie’s frames. Additionally, we evaluate two strategies for sampling frames: randomly selecting frames from the entire movie or selecting frames sequentially in their original order


Fine-tuning is performed using the Qwen2-VL 7B model, leveraging Low-Rank Adaptation (LoRA) as implemented in the LlamaFactory framework \cite{llamafactory}. The number of training epochs is adjusted proportionally to the percentage of frames used, ensuring consistent exposure to the dataset. For instance, when training with the entire dataset (100\%), we perform one epoch, whereas using half the dataset (50\%) involves training for two epochs, effectively maintaining equivalent frame coverage across configurations.


\begin{figure}[ht]
\centering
  \includegraphics[width=\textwidth]{Appendix_Figures/fine_tune_data.pdf}
      \caption{Examples from the supervised fine-tuning datasets used in the experiments.}
  \label{fig:sft_example_appendix}
\end{figure}

\subsection{Preventing Disclosing Memorization - Additional Results}


\begin{figure}[ht]
\centering
  \includegraphics[width=0.8\textwidth]{Appendix_Figures/disclosing_memorization_evolution_appendix.pdf}
      \caption{Accuracy for MovieTection suspect split on preventing disclosing memorization, as a function of the number of fine-tuning frames. Randomly selected frames}
  \label{fig:disclosing_memorization_example_appendix}
\end{figure}



Figure \ref{fig:disclosing_memorization_example_appendix} expands upon the results presented in Table \ref{tab:generalization_subset}, summarizing the average accuracy for main and neutral frames within the suspect split of MovieTection. The key takeaways are that the model generalizes more effectively to neutral frames than to main ones and that longer fine-tuning on the new content leads to greater prevention of memorization disclosure.
\par
These observations align with expectations. Main frames typically contain highly distinctive visual elements, making movies easily recognizable. As a result, fine-tuning on a single movie (\textit{Moana}) may not be sufficient for the model to fully generalize the `Copyrighted Content' label to other titles. 
\par
In contrast, neutral frames are less distinctive. Since the model is already less confident in its responses on these frames, it becomes more susceptible to the influence of fine-tuning. Therefore, when encountering other animated movies, which share stylistic similarities with \textit{Moana}, the model is more likely to generalize the `Copyrighted Content' label, as we see in Table \ref{tab:generalization_subset}.




\newpage

\section{Implementation Details}
\label{sec:implementation_appendix}


We utilize a diverse set of models, including GPT-4o \cite{GPT-4o}, Gemini-1.5 Pro \cite{gemini_1.5}, LLaMA-3.2 \cite{llama_3.2}, Qwen2-VL \cite{qwen2}, LLaVA-v1.5 \cite{llava}, and Pixtral \cite{pixtral}.
\par
When generating detailed captions for the frames, our model requires a certain level of creativity while staying truthful to the image content, therefore, we set the \texttt{temperature=0.1} to achieve this. For evaluation, we aim for complete determinism, so the \texttt{temperature} parameter is fixed at 0.
\par
When performing inference with GPT-4o and Gemini, we leverage their API functionalities to output responses in JSON format, which ensures better adherence to the task instruction. However, some models, particularly LLaMA and Pixtral, tend to struggle with strictly outputting just the movie name, which complicates the automatic evaluation of the task. To mitigate this, whenever we observe such inconsistencies, we perform a second model iteration where we feed the outputs to GPT-4o Mini, specifically instructed to extract only the movie name.
\par
Most experiments with white-box models are conducted on a computing cluster equipped with four NVIDIA A100 80GB GPUs, allowing their efficient execution without requiring model quantization.
\par



\subsection{Time Analysis - \method and Baselines}
\label{sec:time_analysis}


\begin{comment}
{\begin{table}[h]
\centering
\caption{The time required to complete an evaluation on a random MovieTection Movie using Qwen2-VL 7B.}
\label{tab:decop-time1}
\begin{tabular}{@{}llc@{}}
\toprule
\textbf{} &  & \textbf{Seconds to Complete a Movie (Qwen2-VL 7B)} \\ \cmidrule(r){1-1} \cmidrule(l){3-3} 
Captions &  & 53 seconds \\
MCQA &  & 105 seconds \\
Rényi &  & 306 seconds \\
$\lfloor \text{\method} \rfloor$ &  &  94 seconds \\
\method &  &  41 seconds \\ \bottomrule
\end{tabular}
\end{table}}
\end{comment}



\begin{figure}[H]
\centering
\begin{minipage}[t]{0.54\textwidth}
    \setlength{\parindent}{0pt} % Set paragraph indentation
    \setlength{\parskip}{6pt} % Ensure spacing between paragraphs
    \vspace{0pt} % Ensures top alignment

We perform an analysis of the expected time that each method needs to evaluate a movie, which we present in Figure \ref{fig:disco-time1}.
\par
First, we observe that Rényi is the most time-consuming approach, requiring 306 seconds to complete. MCQA also has a relatively long completion time (105 seconds), which we attribute to the need for a second model iteration, in order to extract the correct label from the answer. While \method\ effectively leads the model produce the expected outputs, MCQA does not exhibit the same level of reliability. As a result, we must perform an additional step using GPT-4o Mini to extract the correct label, which explains why MCQA takes longer than \method. Notably, \method\ achieves the fastest completion time at just 41 seconds. Finally, $\lfloor \text{\method} \rfloor$ takes 95 seconds, which aligns with expectations, as it combines the steps of evaluating both on captions and images.
\end{minipage}%
\hfill
\begin{minipage}[t]{0.42\textwidth}
    \vspace{-8pt} % Ensures top alignment
    \centering
    \includegraphics[width=\textwidth]{Appendix_Figures/evaluation_time_comparison.pdf}
    \vspace{-0.98cm}
    \caption{The time required to complete an evaluation on a random MovieTection Movie using Qwen2-VL 7B.}
    \label{fig:disco-time1}
\end{minipage}
\end{figure}








\section{Selection Bias}
\label{sec:selection_bias_appendix}

\begin{figure}[H]
\centering
\begin{minipage}[t]{0.54\textwidth}
    \setlength{\parindent}{0pt} % Set paragraph indentation
    \setlength{\parskip}{6pt} % Ensure spacing between paragraphs
    \vspace{0pt} % Ensures top alignment
    On Figure \ref{fig:selection_bias_example}, we present the accuracies of GPT-4o on a MCQA setting for our proof-of-concept experiment of detecting images from the COCO dataset.
\par
    In the first experiment (left bar), the correct answer is randomly placed in a different position for each iteration. In the second experiment (right bar), the correct answer is always positioned at the same fixed location. Ideally, a model should be robust to variations in answer order, provided it has sufficient knowledge to answer the question accurately.
\par
    The \method results in Table \ref{tab:poc_flickr} suggest that the model possesses the knowledge to perform the task accurately. However, the MCQA results reveal that certain answer positions pose significant challenges for the model, hindering its ability to correctly select the appropriate option. This is therefore a consequence of selection bias. To emphasize this limitation, we report the accuracy from the second experiment (right bar) in Section \ref{sec:poc_flickr}.
\end{minipage}%
\hfill
\begin{minipage}[t]{0.42\textwidth}
    \vspace{-9pt} % Ensures top alignment
    \centering
    \includegraphics[width=\textwidth]{Appendix_Figures/selection_bias.pdf}
    \vspace{-1.75cm}
    \caption{Impact of option position on GPT-4o accuracy for VL-MIA/Flickr dataset proof-of-concept experiment.}
    \label{fig:selection_bias_example}
\end{minipage}
\end{figure}





\newpage





\section{Additional Main Results}

\subsection{MovieTection Accuracy on Clean and Suspect Movies - Large Models}
\label{sec:large_models_accuracy_table_appendix}

\begin{table}[H]
\centering
\caption{\textbf{Accuracy} scores for MovieTection movies included in the training data of VLMs - \textbf{Suspect}}
\label{tab:main_results_extra1}
\begin{tabular}{@{}clcccc@{}} % Change column alignment
\toprule
 &  & \textbf{GPT-4o} & \textbf{Gemini-1.5 Pro} & \textbf{LLaMA-3.2 90B} & \textbf{Qwen2-VL 72B} \\ \midrule
\multirow{4}{*}{Neutral Frames} 
 & Captions & 0.128$_{0.011}$& 0.079$_{0.012}$& 0.078$_{0.015}$& 0.075$_{0.010}$\\
 & MCQA & 0.721$_{0.024}$& 0.550$_{0.019}$& 0.540$_{0.028}$& 0.617$_{0.031}$\\
 & $\lfloor \text{\method} \rfloor$  & 0.226$_{0.021}$& 0.152$_{0.023}$& 0.134$_{0.017}$& 0.122$_{0.012}$\\
 & \method  & 0.338$_{0.030}$& 0.209$_{0.031}$& 0.176$_{0.023}$& 0.176$_{0.018}$\\  
\cmidrule[0.5pt](l){1-6} % Subtle line for separation
\multirow{4}{*}{Main Frames} 
 & Captions & 0.197$_{0.018}$& 0.126$_{0.020}$& 0.122$_{0.024}$& 0.121$_{0.020}$\\
 & MCQA & 0.770$_{0.025}$& 0.638$_{0.020}$& 0.651$_{0.036}$& 0.692$_{0.034}$\\
 & $\lfloor \text{\method} \rfloor$  & 0.512$_{0.017}$& 0.328$_{0.024}$& 0.300$_{0.032}$& 0.274$_{0.014}$\\
 & \method  & 0.704$_{0.023}$& 0.429$_{0.033}$& 0.404$_{0.039}$& 0.377$_{0.028}$\\  \bottomrule
\end{tabular}
\end{table}


\begin{table}[H]
\centering
\caption{\textbf{Accuracy} scores for MovieTection movies included in the training data of VLMs - \textbf{Clean}}
\label{tab:main_results_extra2}
\begin{tabular}{@{}clcccc@{}} % Change column alignment
\toprule
 &  & \textbf{GPT-4o} & \textbf{Gemini-1.5 Pro} & \textbf{LLaMA-3.2 90B} & \textbf{Qwen2-VL 72B} \\ \midrule
\multirow{4}{*}{Neutral Frames} 
 & Captions & 0.001$_{0.001}$& 0.000$_{0.000}$& 0.000$_{0.000}$& 0.001$_{0.001}$\\
 & MCQA & 0.410$_{0.057}$& 0.295$_{0.038}$& 0.295$_{0.052}$& 0.149$_{0.024}$\\
 & $\lfloor \text{\method} \rfloor$  & 0.002$_{0.001}$& 0.004$_{0.001}$& 0.005$_{0.002}$& 0.000$_{0.000}$\\
 & \method  & 0.002$_{0.001}$& 0.004$_{0.001}$& 0.005$_{0.002}$& 0.001$_{0.001}$\\ 
\cmidrule[0.5pt](l){1-6} % Subtle line for separation
\multirow{4}{*}{Main Frames} 
 & Captions & 0.000$_{0.00}$& 0.000$_{0.00}$& 0.000$_{0.00}$& 0.000$_{0.00}$\\
 & MCQA & 0.445$_{0.046}$& 0.380$_{0.039}$& 0.365$_{0.051}$& 0.188$_{0.025}$\\
 & $\lfloor \text{\method} \rfloor$  & 0.010$_{0.003}$& 0.010$_{0.005}$& 0.013$_{0.005}$& 0.000$_{0.000}$\\
 & \method  & 0.010$_{0.003}$& 0.010$_{0.005}$& 0.013$_{0.005}$& 0.000$_{0.000}$\\  \bottomrule
\end{tabular}
\end{table}


The additional accuracy results in Table \ref{tab:main_results_extra1} and Table \ref{tab:main_results_extra2} reinforce the trends observed in Tables \ref{tab:all_results} and \ref{tab:main_results_accuracy_gpt4o} from the main text. While GPT-4o consistently achieves the highest performance, the relative ranking of methods remains stable across all models.
\par 
(i) MCQA, once again, demonstrates relatively high accuracy for suspect movies across all models; however, this comes at the cost of a high false positive rate on clean movies. This tradeoff undermines its overall reliability, as it leads to misclassify non-memorized content as suspect.
\par
(ii) Captions, despite occasionally achieving moderate AUC scores (Table \ref{tab:all_results}), exhibit poor accuracy performance, even in detecting suspect movies. This limitation is most pronounced in models like Qwen2-VL 72B, where caption-based classification of neutral frames results in an accuracy below 10\%. Such results suggest that captions alone are insufficient indicators of memorization.
\par
By contrast, \method and $\lfloor \text{\method} \rfloor$ continue to outperform alternative baselines, demonstrating stronger detection capabilities for suspect movies while maintaining low false positive rates for clean movies. Their consistent superiority across models further underscores their robustness and reliability in identifying memorized content.




\newpage


\begin{comment}
    {\subsection{MovieTection AUC - Small Open Source Models}

\begin{table}[H]
\centering
\caption{\textbf{AUC} scores for MovieTection movies included in the training data of Smaller Open-Source VLMs}
\begin{tabular}{@{}clcccc@{}} % Change column alignment
\toprule
 &  & \textbf{Qwen2-VL 7B} & \textbf{Llava-v1.5 7B} & \textbf{LLaMA-3.2 11B} & \textbf{Pixtral-12B} \\ \midrule
\multirow{5}{*}{Neutral Frames} 
 & Captions & 0.706$_{0.021}$& 0.624$_{0.031}$& 0.745$_{0.024}$& 0.743$_{0.022}$\\
 & MCQA & 0.838$_{0.040}$& 0.736$_{0.050}$& 0.737$_{0.065}$& ------\\
 & Rényi ($\alpha$ = 0.5)& 0.436$_{0.048}$& 0.557$_{0.076}$& 0.632$_{0.048}$& ------\\
 & $\lfloor \text{\method} \rfloor$& 0.787$_{0.034}$& 0.621$_{0.039}$& 0.815$_{0.027}$& 0.755$_{0.033}$\\
 & \method& 0.787$_{0.034}$& 0.621$_{0.039}$& 0.815$_{0.027}$& 0.755$_{0.033}$\\
\cmidrule[0.5pt](l){1-6} % Subtle line for separation
\multirow{5}{*}{Main Frames} 
 & Captions & 0.797$_{0.038}$& 0.755$_{0.023}$& 0.854$_{0.022}$& 0.833$_{0.020}$\\
 & MCQA & 0.880$_{0.023}$& 0.736$_{0.046}$& 0.724$_{0.059}$& -----\\
 & Rényi ($\alpha$ = 0.5)& 0.582$_{0.052}$& 0.515$_{0.074}$& 0.677$_{0.078}$& -----\\
 & $\lfloor \text{\method} \rfloor$ & 0.939$_{0.022}$& 0.775$_{0.037}$& 0.945$_{0.029}$& 0.875$_{0.033}$\\
 & \method & 0.939$_{0.022}$& 0.775$_{0.037}$& 0.945$_{0.029}$& 0.875$_{0.033}$\\ \bottomrule
\end{tabular}
\end{table}}
\end{comment}



\subsection{MovieTection Accuracy on Clean and Suspect Movies - Small Open Source Models}
\label{sec:additional_main_results_appendix_small_open_source_accuracy}

\begin{table}[H]
\centering
\caption{\textbf{Accuracy} scores for MovieTection movies included in the training data of Smaller Open-Source VLMs - \textbf{Suspect}}
\label{tab:main_results_extra3}
\begin{tabular}{@{}clcccc@{}} % Change column alignment
\toprule
 &  & \textbf{Qwen2-VL 7B} & \textbf{LLaVA-v1.5 7B} & \textbf{LLaMA-3.2 11B} & \textbf{Pixtral-12B} \\ \midrule
\multirow{4}{*}{Neutral Frames} 
 & Captions & 0.035$_{0.010}$& 0.029$_{0.009}$& 0.047$_{0.008}$& 0.044$_{0.009}$\\
 & MCQA & 0.485$_{0.047}$& 0.397$_{0.069}$& 0.420$_{0.014}$& -\\
 & $\lfloor \text{\method} \rfloor$  & 0.075$_{0.015}$& 0.019$_{0.006}$& 0.089$_{0.016}$& 0.043$_{0.013}$\\
 & \method  & 0.099$_{0.023}$& 0.030$_{0.010}$& 0.110$_{0.020}$& 0.058$_{0.017}$\\  
\cmidrule[0.5pt](l){1-6} % Subtle line for separation
\multirow{4}{*}{Main Frames} 
 & Captions & 0.066$_{0.0016}$& 0.070$_{0.019}$& 0.087$_{0.017}$& 0.076$_{0.015}$\\
 & MCQA & 0.558$_{0.040}$& 0.425$_{0.074}$& 0.507$_{0.036}$& -\\
 & $\lfloor \text{\method} \rfloor$  & 0.201$_{0.023}$& 0.044$_{0.016}$& 0.215$_{0.028}$& 0.111$_{0.025}$\\
 & \method  & 0.260$_{0.0034}$& 0.072$_{0.027}$& 0.273$_{0.036}$& 0.160$_{0.0035}$\\  \bottomrule
\end{tabular}
\end{table}


\begin{table}[H]
\centering
\caption{\textbf{Accuracy} scores for MovieTection movies included in the training data of Smaller Open-Source VLMs - \textbf{Clean}}
\label{tab:main_results_extra4}
\begin{tabular}{@{}clcccc@{}} % Change column alignment
\toprule
 &  & \textbf{Qwen2-VL 7B} & \textbf{LLaVA-v1.5 7B} & \textbf{LLaMA-3.2 11B} & \textbf{Pixtral-12B} \\ \midrule
\multirow{4}{*}{Neutral Frames} 
 & Captions & 0.001$_{0.010}$& 0.000$_{0.000}$& 0.000$_{0.000}$& 0.001$_{0.001}$\\
 & MCQA & 0.115$_{0.023}$& 0.092$_{0.037}$& 0.277$_{0.065}$& -\\
 & $\lfloor \text{\method} \rfloor$  & 0.000$_{0.000}$& 0.000$_{0.000}$& 0.003$_{0.003}$& 0.000\\
 & \method  & 0.000$_{0.000}$& 0.000$_{0.000}$& 0.003$_{0.003}$& 0.000$_{0.000}$\\ 
\cmidrule[0.5pt](l){1-6} % Subtle line for separation
\multirow{4}{*}{Main Frames} 
 & Captions & 0.001$_{0.001}$& 0.000$_{0.000}$& 0.000$_{0.000}$& 0.001$_{0.001}$\\
 & MCQA & 0.116$_{0.020}$& 0.092$_{0.037}$& 0.277$_{0.065}$& -\\
 & $\lfloor \text{\method} \rfloor$  & 0.000$_{0.000}$& 0.000$_{0.000}$& 0.020$_{0.017}$& 0.000$_{0.000}$\\
 & \method  & 0.000$_{0.000}$& 0.000$_{0.000}$& 0.020$_{0.017}$& 0.000$_{0.000}$\\  \bottomrule
\end{tabular}
\end{table}



The accuracy results in Tables \ref{tab:main_results_extra3} and \ref{tab:main_results_extra4} extend our analysis to smaller open-source VLMs. \method and $\lfloor \text{\method} \rfloor$, while exhibiting reduced accuracy in absolute terms compared to the larger models (Tables \ref{tab:main_results_extra1} and \ref{tab:main_results_extra2}), maintain their advantage over the alternative baselines. These methods consistently demonstrate stronger detection capabilities for suspect movies while keeping false positives on clean movies to a minimum. LLaVA-v1.5 7B seems to be the only outlier in this trend, as both \method variants perform closer to captions rather than showing a clear advantage. 


\newpage



\section{Long Context - Additional Results}
\label{sec:long_context_appendix}


\begin{figure}[H]
    \centering
    \begin{minipage}{.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Appendix_Figures/longer_context_normal_closed_source.pdf}
        \captionsetup{justification=centering}
      \caption{DIS-CO’s accuracy on the MovieTection suspect split with varying numbers of frames in the prompt. Scores are produced with the \textbf{main frames} and using the \textbf{large models}.}
      \label{fig:long_context_appendix_1}
    \end{minipage}\quad
    \begin{minipage}{.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Appendix_Figures/longer_context_background_closed_source.pdf}
        \captionsetup{justification=centering}
        \caption{DIS-CO’s accuracy on the MovieTection suspect split with varying numbers of frames in the prompt. Scores are produced with the \textbf{neutral frames} and using the \textbf{large models}.}
        \label{fig:long_context_appendix_2}
    \end{minipage}
\end{figure}



\begin{figure}[H]
    \centering
    \begin{minipage}{.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Appendix_Figures/longer_context_normal_open_source.pdf}
        \captionsetup{justification=centering}
      \caption{DIS-CO’s accuracy on the MovieTection suspect split with varying numbers of frames in the prompt. Scores are produced with the \textbf{main frames} and using the \textbf{smaller models}.}
      \label{fig:long_context_appendix_3}
    \end{minipage}\quad
    \begin{minipage}{.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Appendix_Figures/longer_context_background_open_source.pdf}
        \captionsetup{justification=centering}
        \caption{DIS-CO’s accuracy on the MovieTection suspect split with varying numbers of frames in the prompt. Scores are produced with the \textbf{neutral frames} and using the \textbf{smaller models}.}
       \label{fig:long_context_appendix_4}
    \end{minipage}
\end{figure}




In the main text, we observed in Section \ref{sec:longer_context} a general trend where increasing the number of frames in the prompt led to improved detection performance. Here, we extend this analysis by separately evaluating the impact of the two frame types along the multiple models.
\par
\textbf{Large-Scale Models:} From Figure \ref{fig:long_context_appendix_1} and Figure \ref{fig:long_context_appendix_2} we observe that, regardless of the frame type, the trend remains: more frames in the prompt consistently lead to better performance. The only key distinction between the two types is that the neutral frames yield lower absolute accuracies. Nonetheless, this is expected given the increased difficulty of detection when using frames that are less informative.
\par
Interestingly, despite Meta's official recommendation that LLaMA performs best with a single image during inference\footnote{\url{https://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct/discussions/43\#66f98f742094ed9e5f5107d4}}, our results suggest that while the model may not have been explicitly optimized for multi-image inputs, it can still benefit from the extended context in this setting.



\textbf{Smaller-Scale Models:} These models follow the same pattern observed in Figures \ref{fig:long_context_appendix_1} and \ref{fig:long_context_appendix_2}. However, their overall accuracy remains lower, which is expected given their smaller size and capacity. Only LLaVA appears to be an exception, as it does not seem to effectively leverage multiple-image inputs, showing limited improvement compared to the other models.


%#### Summary of Findings  
%Across all model scales, increasing the number of frames in the prompt improves detection accuracy, confirming our earlier findings. However, the improvement is more pronounced when using **main frames**, which likely contain stronger contextual signals. The gap between large and small models is also evident, with larger-scale architectures leveraging extended contexts more effectively.  

%These results suggest that while prompting with more frames enhances performance, the type of frames included plays a crucial role. This distinction may be useful in optimizing prompts for different model architectures and computational constraints.  



\newpage


\section{Popularity - Additional Results}
\label{sec:box_office_appendix}


\begin{figure}[H]
    \centering
    \begin{minipage}{.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Appendix_Figures/box_office_normal_closed_source.pdf}
        \captionsetup{justification=centering}
      \caption{Box-Office effect of suspect movies on \method's performance. Scores are produced with the \textbf{main frames} and using the \textbf{large models}.}
      \label{fig:popularity_appendix_1}
    \end{minipage}\quad
    \begin{minipage}{.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Appendix_Figures/box_office_background_closed_source.pdf}
        \captionsetup{justification=centering}
      \caption{Box-Office effect of suspect movies on \method's performance. Scores are produced with the \textbf{neutral frames} and using the \textbf{large models}.}
      \label{fig:popularity_appendix_2}
    \end{minipage}
\end{figure}



\begin{figure}[H]
    \centering
    \begin{minipage}{.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Appendix_Figures/box_office_normal_open_source.pdf}
        \captionsetup{justification=centering}
      \caption{Box-Office effect of suspect movies on \method's performance. Scores are produced with the \textbf{main frames} and using the \textbf{smaller models}.}
      \label{fig:popularity_appendix_3}
    \end{minipage}\quad
    \begin{minipage}{.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Appendix_Figures/box_office_background_open_source.pdf}
        \captionsetup{justification=centering}
      \caption{Box-Office effect of suspect movies on \method's performance. Scores are produced with the \textbf{neutral frames} and using the \textbf{smaller models}.}
      \label{fig:popularity_appendix_4}
    \end{minipage}
\end{figure}


In the main text, we observed a general trend where higher box-office revenue correlates with improved detection performance across models (Figure \ref{fig:popularity}). Here, we extend this analysis by separately evaluating the impact of the two frame types along the multiple models.
\par
\textbf{Large-Scale Models:} Figures \ref{fig:popularity_appendix_1} and \ref{fig:popularity_appendix_2} show that higher box-office revenue consistently improves detection performance, remaining agnostic to the frame type used. Both main and neutral frames follow similar patterns, with the key distinction being that neutral frames yield slightly lower absolute accuracies due to their inherent difficulty. This consistency across frame types confirms that Figure \ref{fig:popularity} accurately captures the overall trend of the models, despite presenting results based on the grouping of both frame types.
\par
\textbf{Small-Scale Models:} Figures \ref{fig:popularity_appendix_3} and \ref{fig:popularity_appendix_4} show a much more inconsistent relationship between box-office revenue and detection accuracy compared to larger models. While LLaMA-3.2 11B, shows a noticeable improvement with higher-grossing films, other models, like LLaVA, display erratic fluctuations with less clear trends.





\newpage


\section{Quality - Additional Results}
\label{sec:rating_appendix}


\begin{figure}[H]
    \centering
    \begin{minipage}{.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Appendix_Figures/imdb_rating_normal_closed_source.pdf}
        \captionsetup{justification=centering}
      \caption{IMDb movie rating effect of suspect movies on \method's performance. Scores are produced with the \textbf{main frames} and using the \textbf{large models}.}
      \label{fig:quality_appendix_1}
    \end{minipage}\quad
    \begin{minipage}{.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Appendix_Figures/imdb_rating_background_closed_source.pdf}
        \captionsetup{justification=centering}
      \caption{IMDb movie rating effect of suspect movies on \method's performance. Scores are produced with the \textbf{neutral frames} and using the \textbf{large models}.}
      \label{fig:quality_appendix_2}
    \end{minipage}
\end{figure}


\begin{figure}[H]
    \centering
    \begin{minipage}{.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Appendix_Figures/imdb_rating_normal_open_source.pdf}
        \captionsetup{justification=centering}
      \caption{IMDb movie rating effect of suspect movies on \method's performance. Scores are produced with the \textbf{main frames} and using the \textbf{smaller models}.}
      \label{fig:quality_appendix_3}
    \end{minipage}\quad
    \begin{minipage}{.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Appendix_Figures/imdb_rating_background_open_source.pdf}
        \captionsetup{justification=centering}
      \caption{IMDb movie rating effect of suspect movies on \method's performance. Scores are produced with the \textbf{neutral frames} and using the \textbf{smaller models}.}
      \label{fig:quality_appendix_4}
    \end{minipage}
\end{figure}


In the main text, we observed that higher IMDb ratings generally led to improved detection performance across models (Figure \ref{fig:quality}). Here, we extend this analysis by separating the main and neutral frame types and evaluating performance across both large-scale and smaller models.
\par

\textbf{Large-Scale Models:} Figures \ref{fig:quality_appendix_1} and \ref{fig:quality_appendix_2} reveal an overall upward trend in detection performance as IMDb ratings increase. However, an interesting U-shaped pattern is noticeable, particularly in main frames, where detection accuracy initially drops for lower-rated movies (around Rating$\in$[4,5]) before rising sharply from Rating=6 onward. In contrast, neutral frames display a more gradual improvement without the same dip at low ratings. Only Gemini-1.5 Pro, unexpectedly, shows a sharp drop at Rating=9, deviating from the otherwise consistent trend.
\par
\textbf{Small-Scale Models:} Figures \ref{fig:quality_appendix_3} and \ref{fig:quality_appendix_4}, on the other hand, show that overall performance remains weak across most rating levels, with a notable exception in Rating=8, where most models exhibit a sudden increase in accuracy, though the reason for this improvement is unclear.



\newpage




\section{Time Effect on MovieTection}
\label{sec:time_effect}



\begin{figure}[H]
\centering
\begin{minipage}[t]{0.54\textwidth}
    \setlength{\parindent}{0pt} % Set paragraph indentation
    \setlength{\parskip}{6pt} % Ensure spacing between paragraphs
    \vspace{0pt} % Ensures top alignment

    The proposed temporal split of MovieTection was well suited for the tested models, but as new models emerge, the current suspect/clean split assumption may no longer hold. To explore this, we tested a newer model (Gemini-2.0 Flash) on the clean MovieTection data to assess whether it has started acquiring knowledge of these movies.
    \par
    From Figure \ref{fig:time_effect_appendix}, we see that while Gemini-1.5 Pro struggles with identifying clean movies, achieving an accuracy of only 0.01, Gemini-2.0 Flash shows a nearly 10× increase, reaching 0.078. Although these values remain low and do not suggest that most movies in the split were seen by the new model, individual inspection of the results indicates that some titles might raise suspicion. In fact, with Gemini-1.5 Pro, \textit{Bob Marley: One Love scores} 0.1, but with Gemini-2.0 Flash, the same movie reaches 0.69.

\end{minipage}%
\hfill
\begin{minipage}[t]{0.42\textwidth}
    \vspace{-15pt} % Ensures top alignment
    \centering
    \includegraphics[width=\textwidth]{Appendix_Figures/time_effect.pdf}
    \vspace{-0.98cm}
    \caption{Effect of knowledge cut-off date on MovieTection clean split performance across similar models.}
    \label{fig:time_effect_appendix}
\end{minipage}
\end{figure}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}
