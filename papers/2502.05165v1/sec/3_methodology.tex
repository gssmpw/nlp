\section{Methodology}
\label{sec:methodology}

We propose a diffusion-based model capable of simultaneously compositing multiple objects into a background in a controlled way, guided by both a layout and a text prompt. Training this model effectively requires extensive paired data, including: (i) ground truth images containing multiple objects, (ii) text descriptions with grounding information for these objects, (iii) segmented images of the objects, and (iv) their bounding boxes in the ground truth image. For simplicity, we focus on two objects at a time, which may interact through complex actions or positional relationships in the ground truth image. We present model architecture  in Section \ref{sec:model}, training and inference strategies in Sections \ref{sec:training} and \ref{sec:inference}, and data generation pipeline in Section \ref{sec:datageneration}.  %Effectively training such model requires a large amount of paired data including: (i) Ground truth image where multiple objects are in the scene, (ii) Text description with grounding information on those objects, (iii) Segmented images of such objects, and (iv) their corresponding bounding boxes in the ground truth image. For simplicity, we only consider two objects at a time, which can be interacting in the ground truth image through an intricate action or their positional relationship. 

\subsection{Model Architecture}
\label{sec:model}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/pipeline.jpg}
    \caption{Model Architecture. Our model consists of: (i) A Stable Diffusion backbone including a U-Net and an autoencoder ($\mathcal{G}$, $\mathcal{D}$); (ii) a text encoder $\mathcal{E}_{T}$; (iii) an image encoder $\mathcal{E}_{I}$; and (iv) an adaptor $\mathcal{A}$. Given a text prompt $\mathcal{C}$ and images of N objects $\mathcal{O}_{0\dots, N-1}$, the text embedding from (iii) is augmented by concatenating each image embedding after their corresponding text tokens. The resulting multimodal embedding $\mathcal{H}$ is fed to the U-Net via cross-attention. Masked background image $(1-\mathcal{M}_{G})*\mathcal{I}_{BG}$ and layout $\mathcal{I}_{L}$ with object-specific bboxes are concatenated to input $\mathcal{I}$. }
    \label{fig:pipeline}
    \vspace{-6mm}
\end{figure}

Our model takes as input a background image $\mathcal{I}_{BG}$, a layout $\mathcal{I}_{L}$, $N$ object images $\mathcal{O}_{i}$, with $i \in \{0 \dots, N-1\}$, and a descriptive caption $\mathcal{C}$. The layout includes a bounding box for each object ($\mathcal{M}_{0 \dots, N-1}$), along with a larger box $\mathcal{M}_{G}$ that encloses them all, defining  the region to be modified (inpainting region), allowing for additional room for object-object interactions. %the inpainting region. 
We encode all layout information on a single mask, where different values are assigned to the pixels belonging to each object's bounding box, their overlapping, the rest of the inpainting region and the background. As shown in Fig \ref{fig:pipeline}, $\mathcal{I}_{L}$ is concatenated to the 3-channel input noise $\mathcal{I}$ and $(1-\mathcal{M}_{G})*\mathcal{I}_{BG}$, a version of $\mathcal{I}_{BG}$, where the inpainting region is masked out. These concatenated images are fed into the model's backbone, Stable Diffusion 1.5 (SD) \cite{rombach2022ldm}, consisting of a variational autoencoder ($\mathcal{G}$, $\mathcal{D}$) and a U-Net. Each composited object $\mathcal{O}_{i}$ is processed by an image encoder $\mathcal{E}_{I}$ (DINO ViT-G/14 \cite{oquab2023dinov2}) and a content adaptor $\mathcal{A}$, which aligns the object embeddings with text embeddings as in \cite{song2022objectstitch}. The caption $\mathcal{C}$ is encoded using a text encoder $\mathcal{E}_{T}$ (CLIP ViT-L/14 \cite{radford2021clip}). These embeddings are then combined into a multimodal embedding and fed into the U-Net via cross-attention.

\noindent
\textbf{Multimodal Embeddings} Providing the model with text and image information in a balanced and interpretable way is one of the main challenges of our method. It must perform well when either input is missing, and neither component must dominate when both are present. The identity of each object $\mathcal{O}_{i}$ must be preserved in the output while matching the scene and interactions described by the caption $\mathcal{C}$. Grounding information specifies the subset of words $\mathcal{C}_{i}$ in the caption $\mathcal{C}$ that correspond to each object image $\mathcal{O}_{i}$. After encoding the text, each object embedding $\mathcal{A}(\mathcal{E}_{I}(\mathcal{O}_{i}))$ is concatenated after $\mathcal{E}_{T}(\mathcal{C}_{i})$, resulting in a multimodal embedding $\mathcal{H}$ that is then passed to the U-Net via cross-attention. We refer to the set of visual and textual information referring to $\mathcal{C}_i$ and $\mathcal{O}_i$ in the embedding $\mathcal{H}$ as $\mathcal{H}_i$.




%Grounding information is provided by adding a special token \textit{$<$image\{i\}$>$} after relevant words in $\mathcal{C}$, indicating which object $\mathcal{O}{i}$ corresponds to each set of words ($\mathcal{C}_{i}$). After encoding the text, each encoded \textit{$<$image\{i\}$>$} token is replaced by the corresponding object embedding $\mathcal{A}(\mathcal{E}_{I}(\mathcal{O}_{i}))$, forming the multi-modal embedding that is then passed to the U-Net via cross-attention. %Grounding information matching specific words in $c$ with each of the objects $\mathcal{O}_{i}$ is provided. A special token \textit{$<$image\{i\}$>$} is added after each of those words, specifying $\mathcal{O}_{i}$ contains the visual information for that particular word. After encoding the text with $\mathcal{E}_{T}$, each $\mathcal{E}_{T}(\textit{$<$image\{i\}$>$})$ is replaced by the corresponding object embedding $\mathcal{A}(\mathcal{E}_{I}(\mathcal{O}_{i}))$. The resulting multimodal embedding is consequently fed to the U-Net through cross-attention.

%\paragraph{Objects-Layout Alignment} 

\subsection{Training Strategy}
\label{sec:training}


Our training strategy is designed to balance textual and visual information, enabling object reposing and the generation of described backgrounds and new objects while preserving each object's identity and preventing identity mixing. To ensure the model is still able to perform with only text or image input, we randomly drop each modality with a 30\% probability during training. %For enabling the model to also perform with solely textual or visual information, we drop each individual modality with a 30\% probability during training.

\noindent
\textbf{Customization as an Auxiliary Task}  To enhance compositing performance and provide a more versatile model, we jointly train on multi-object compositing and multi-entity subject-guided generation. With 50\% probability, the inpainting region $\mathcal{M}_{G}$ is replaced by a mask covering the entire image, and $\mathcal{I}_{BG}$ is replaced with an empty image. In this customization setting, the model can focus on preserving object identity while reposing objects and generating scenes aligned with the text, without being burdened by compositing tasks like harmonization, relighting, or background inpainting. Introducing this joint training results in a better balance between textual and visual alignment. %For boosting compositing performance and providing a more versatile model, we simultaneously train the model for multi-object compositing and multi-subject guided generation, (\ie \textit{customization}). With $50 \%$ probability, the inpainting region delimited by $\mathcal{M}_{G}$ is replaced with a mask covering the entire image, and $\mathcal{I}_{BG}$ is replaced with an empty image. With the customization setting, the model can focus on identity preservation when reposing objects and generating scenes aligned with the text, without being tasked with certain sub-tasks inherent of compositing, such as color harmonization, re-lighting, or background inpainting. %In those occasions, the model is released from certain sub-tasks inherent of compositing, such as color harmonization, re-lighting and background inpainting. It can thus focus on achieving better identity preservation when reposing objects and generating a scene aligned with the text.

\noindent
\textbf{Object Identity Disentanglement} Diffusion models often struggle to accurately represent multiple subjects due to attention layers blending visual features and causing semantic leakage between them. Cross-attention maps indicate how text tokens influence latent pixels \cite{hertz2022prompt}. When tokens share similar semantics, a single pixel may respond to all, leading to identity blending. 
Additionally, self-attention features create dense correspondences within the same subject and across semantically similar ones \cite{dahary2024yourself,alaluf2024cross,cao2023masactrl}. While this behavior aids in generating coherent images with well-integrated subjects and backgrounds, it can also cause visual feature leakage between similar subjects \cite{dahary2024yourself}. To reduce this leakage and encourage identity separation, we introduce two additional losses ($\mathcal{L}_{c}$, $\mathcal{L}_{s}$) inspired by \cite{dahary2024yourself,xiao2024fastcomposer}:


\begin{equation}
% \scriptstyle
\label{eq:crossattn-loss}
\mathcal{L}_{c} =  \sum\limits_{i = 0}^{N-1}\frac{1}{N}
\bigg(1 - \frac{
\sum\limits_{\substack{\mathbf{x} \in \mathcal{S}_i,\\ \mathbf{h} \in \mathcal{H}_i}}
\hat{\mathbf{A}}\left[ \mathbf{x}, \mathbf{h} \right]
}{
\sum\limits_{\substack{\mathbf{x} \in \mathcal{S}_i, \\ \mathbf{h} \in \mathcal{H}_i}}
\hat{\mathbf{A}}\left[ \mathbf{x}, \mathbf{h} \right]
+
\sum\limits_{\substack{\mathbf{x} \notin \mathcal{S}_i,\\ \mathbf{h} \in \mathcal{H}_i}}
\hat{\mathbf{A}}\left[ \mathbf{x}, \mathbf{h} \right]}\bigg)
,
\end{equation}



% \begin{equation}
% \label{eq:selfattn-loss}
% \mathcal{L}_{s} =  \sum\limits_{i = 0}^{N-1}\frac{1}{N}
% \bigg(1 - \frac{
% 1}{1+\sum\limits_{\substack{\mathbf{x} \in \mathcal{S}_j, \\ \mathbf{h} \in \mathcal{H}_j, j \neq i}}
% \hat{\mathbf{A}}\left[ \mathbf{x}, \mathbf{h} \right]
% }\bigg)
% ,
% \end{equation}

\begin{equation}
\label{eq:selfattn-loss}
\mathcal{L}_{s} =  \sum\limits_{i=0}^{N-1}\frac{1}{N}
\bigg(1 - \frac{
1}{1+\sum\limits_{\substack{\mathbf{x} \in \mathcal{S}_i, \\ \mathbf{y} \in \mathcal{S}_j, j \neq i}}
\hat{\mathbf{A}}\left[ \mathbf{x}, \mathbf{y} \right]
}\bigg)
,
\end{equation}

where $\hat{\mathbf{A}}$ is the mean attention map, averaged across heads and layers, $\mathbf{S}_i$ is the segmentation map of $\mathcal{O}_{i}$ in the ground truth image, and $\mathbf{x}$, $\mathbf{y}$ correspond to pixel coordinates in $\mathcal{I}_{t}$ (the noisy version of $\mathcal{I}$ at timestep $t$). For each  $\mathbf{x}$, $\mathcal{L}_{c}$ encourages the cross-attention maps obtained from visual-language information from $\mathcal{C}_i$ and $\mathcal{O}_i$ ($\mathbf{h} \in \mathcal{H}_i$) to be close to their corresponding segmentation map $\mathbf{S}_i$. $\mathcal{L}_{s}$ discourages pixels $\mathbf{x} \in \mathcal{S}_{i}$ to attend to pixels  $\mathbf{y} \in \mathcal{S}_{j}$, $\forall j \neq i$.

%words with grounding information ($\mathbf{c} \in \mathcal{C}_i$) to be close to their corresponding segmentation map $\mathbf{S}_i$. $\mathcal{L}_{s}$ discourages pixels $\mathbf{x} \in \mathcal{S}_{i}$ to attend to pixels in  $\mathcal{S}_{j}$, $\forall j \neq i$.



% \paragraph{Object Identity Disentanglement} Diffusion models often struggle with representing multiple subjects accurately due to attention layers blending visual features, resulting in semantic leakage between subjects \cite{dahary2024yourself}. Cross-attention maps indicate how text tokens influence latent pixels; when tokens share similar semantics, a single pixel may respond to all, leading to identity blending. Additionally, self-attention features create dense correspondences within the same subject and across semantically similar ones \cite{dahary2024yourself,alaluf2024cross,cao2023masactrl}. While this behavior aids in generating coherent images with well-integrated subjects and backgrounds, it can also cause visual feature leakage between similar subjects.


% % \paragraph{Object Identities Disentanglement} Diffusion models often struggle to faithfully capture semantics of inputs containing multiple subjects. This is mostly due to attention layers in the diffusion model, which tend to blend their visual features, leading to semantic leakage between those subjects \cite{dahary2024yourself}.

% % The scores in cross-attention maps represent the influence of each text token to latent pixels. Therefore, if different text tokens have similar semantics, a single latent pixel may attend to all of them, leading to semantic leakage, or identity blending.


% % Semantic similarity between subjects lead to similar query values, and thus similar attention responses. This, in turn, implies that they will share semantic information from the token embeddings through the cross-attention layers or visual information via the self-attention layer.

% % Self-attention features exhibit dense correspondences within the same subject and across semantic similar ones \cite{dahary2024yourself,alaluf2024cross,cao2023masactrl}. This behaviour aids the model in generating coherent images with properly blended subjects and backgrounds, but may lead to leakage of visual features between similar subjects.

% % Therefore, to minimize both leakages and encourage identity disentanglement, we propose the addition of two losses, similar to \cite{dahary2024yourself,xiao2024fastcomposer}.



Based on the inpainting version of Stable Diffusion v1.5 \cite{rombach2022ldm}, our model is fine-tuned by optimizing a combination of three losses: $\mathcal{L} = \mathcal{L}_{d} + \alpha \mathcal{L}_{c} + \beta \mathcal{L}_{s}$. 

\begin{equation}
    \mathcal{L}_{d} = \mathbb{E}_{\mathcal{I}_{c}, \mathcal{H}, t,  \epsilon \sim \mathcal{N} (0,1)} \left[ \left\| \epsilon - \epsilon_\theta\left(\mathcal{I}_{c}, \mathcal{H}, t\right) \right\|_2^2 \right],
\label{eq:lossunet}
\end{equation}

where $\mathcal{I}_{c}=[\mathcal{I}_{t}, \mathcal{I}_{L}, (1-\mathcal{M}_{G})*\mathcal{I}_{BG}]$ ($\mathcal{I}_{t}$: noisy version of the input image $\mathcal{I}$ at timestep $t$, $\mathcal{I}_{L}$: layout, $(1-\mathcal{M}_{G})*\mathcal{I}_{BG}$: masked background image, $[\cdot]$: concatenation operation across the channel dimension), $\mathcal{H}$: multimodal embedding, $\epsilon_\theta$:denoising model being optimized. 


% where $a_o = \bigcup_{i}\mathcal{A}(\mathcal{E}_{I}(\mathcal{O}_{i}))$, $\mathcal{I}_{c}=[\mathcal{I}_{t}, \mathcal{I}_{L}, \mathcal{M}_{G}(\mathcal{I}_{BG})]$ ($\mathcal{I}_{t}$: noisy version of the input image $\mathcal{I}$ at timestep $t$, $\mathcal{I}_{L}$: layout, $\mathcal{M}_{G}(\mathcal{I}_{BG})$: masked background image, $[\cdot]$: concatenation operation across the channel dimension), $\mathcal{C}$: caption, $\epsilon_\theta$:denoising model being optimized. 

\subsection{Inference Strategy}
\label{sec:inference}

Although attention-based losses help disentangle visual and semantic features of different objects during training, some information leakage can still occur at inference. To address this, and to further encourage the desired layout $\mathcal{I}_L$, all cross-attention scores corresponding to $\mathbf{h} \in \mathcal{H}_i$  are masked with $\mathcal{M}_i$. Additionally, since $[EoT]$ tokens are known to contain information about foreground objects \cite{zhao2023loco}, we mask their corresponding cross-attention scores with the union of all object masks $\bigcup_{i=0\dots N-1} \mathcal{M}_i$. %TODO: need to revise equations, might delete these last ones


% Although attention-based losses help disentangle visual and semantic features of different objects during training, some information leakage can still occur at inference. To address this, and to further encourage the desired layout $\mathcal{I}_L$, all cross-attention scores corresponding to $\mathbf{c} \in \mathcal{C}_i$  are masked with $\mathcal{M}_i$ (\ie $\hat{\mathbf{A}}\left[ \mathbf{x}, \mathbf{c}\right]=0, \forall \mathbf{x} \in \mathcal{M}_i, \mathbf{c} \in \mathcal{C}i$). Additionally, since $[EoT]$ tokens are known to contain information about foreground objects \cite{zhao2023loco}, we mask their corresponding cross-attention scores with the union of all object masks ($\hat{\mathbf{A}}\left[ \mathbf{x}, \mathbf{c}\right]=0, \forall \mathbf{x} \in \cup_{i} \mathcal{M}_i, \mathbf{c}  = \left[EoT\right]$). %TODO: need to revise equations, might delete these last ones

\subsection{Training Data Generation}
\label{sec:datageneration}

%\andy{Do you want this section to appear as the first thing in the method} 
Balanced, curated data is essential for effective diffusion model training. For our model to learn correct object interaction, natural compositing, text-image alignment and identity preservation, paired training data must meet specific criteria: (i) diverse range of objects and relationships; (ii) varied text prompt styles; (iii) object images with different poses, views and lighting than the ground truth image; (iv) high-quality images; and (v) a large dataset size. Since no single source can provide all these qualities, we use three complementary data sources: Video Data, In-the-Wild Images, and Manually Collected Data. %(ii) diversity in text prompt styles; (iii) images of objects in different pose and illumination from ground truth image, (iv) high quality images, (v) a large amount of data. While obtaining all these characteristics from one source might not be possible, we consider three complementary data sources for fulfilling our requirements: In-the-Wild Images, Video Data, and Manually Collected Data.


%When working with diffusion models, and particularly when combining multiple modalities as input, balanced and curated training data is key for a successful training of the model. For this reason, we combine different data sources that provide complementary benefits: (i) Video Data, (ii) From the Wild Images, (iii) Manually collected Data. Generating paired data from in the wild images gives us access to a high quantity of data, including very high quality images. However, the two objects to composite will be extracted from the same image. If used on its own, it is very difficult for the model to learn to repose objects to align with the captions. Therefore, different data sources are required. Videos provide high quantity of data, where objects come from different sources (i.e. different frames), but the image quality might be blurry or lower quality than in the previous source. To bridge this gap, we bring a collected set of paired images to the mixture, in which each object comes from a different image and all images are of certain assured quality.

%We use three complementary data sources: (i) Video Data, (ii) In-the-Wild Images, and (iii) Manually Collected Data. Automating paired data collection from in-the-wild images provide a large volume of data with high-quality images, but the objects to be composited are often from the same image, making it hard for the model to learn object reposing based on captions. Video data, while abundant, typically involves lower-quality frames, though objects appear across different frames. To address these limitations, we include a curated set of paired images, ensuring each object originates from a different high-quality image. This combination optimally supports the model’s learning process.

%\paragraph{Video Data} We consider annotated data from different video object relation datasets. This data contains bboxes detecting each object in each frame, a word describing the action or positional relation between every pair of objects in the frame. Leveraging this information, we employ an entity segmentator \ref{entityseg TODO} for extracting the main segmented object in each annotated bbox. Then, for each object A - object B relation, we select a frame where that relation is depicted, a second frame containing object A and a third frame containing object B. This leads to two different views of each object which can be used for our model to learn how to repose each object to achieve the desired interaction. For ensuring those views are not too different, which could lead to identity preservation loss learnt by our model, we ensure the DINO scores \ref{TODO} between both views of the same object are similar enough (MSE >= 0.8). For obtaining the caption describing the frame containing objects A and B interaction, which will be used as our ground truth image, we provide LLaVA \ref{TODO} with our image and the annotated relation <object A> <relation> <object B>, and ask us to provide a grammatically correct caption describing such interaction in the given image.

\textbf{Video Data} We use annotated data from video object relation datasets \cite{shang2019annotating,shang2017video}, where each frame includes bounding boxes identifying objects and actions or positional relationships between object pairs in the format \textit{$<$object A$>$ $<$relation$>$ $<$object B$>$}. For each annotated relationship, we select one frame depicting it as the ground truth image, a second frame including \textit{object A} and a third with \textit{object B}. This provides alternative views for each object, which is essential for training the model to learn object reposing for intended interactions. To ensure identity consistency, we keep DINO score \cite{oquab2023dinov2} similarity for each object’s views above a threshold (MSE $\geq$ 0.8). We provide LLaVA \cite{liu2023improvedllava} with the ground truth image and the annotated relation and obtain a grammatically correct caption, used as aligned text prompt. Although this data source meets most of our training criteria, video data typically involves lower-quality frames.


\textbf{Image Data} An automated pipeline for paired data generation from in-the-wild images can provide unlimited data with high-quality images to enhance model training. We consider two approaches: top-down and bottom-up. \textbf{(i) Top-down approach:} Starting with a single image, we use a commercial subject selection tool to identify the main objects. A semantic segmentator \cite{qi2022entityseg} helps us select images where multiple key objects are present. From these, we randomly pick two entities to serve as composited objects. We outline each object in a different color on the original image and input it into ViP-LLaVA \cite{cai2024vip}, which identifies the selected entities and generates a caption describing their relationship. \textbf{(ii) Bottom-up approach:} To increase caption diversity, we include open-source images with ground-truth captions, such as those in OpenImages \cite{krylov2021open}. Given an image and its caption, GroundingDINO \cite{liu2023groundingdino} extracts grounding information that correlates the two. Using a semantic segmentator \cite{qi2022entityseg} and filtering for quality (e.g., removing duplicates, background entities, and objects that are too large ($>$75\%) or too small ($<$10\%)), we randomly select two grounded entities as composited objects. This second approach adds diversity to the image layouts and caption formats, which enhances model robustness through a mix of local and global descriptions. However, neither of the approaches provides different views of the objects, making them unsuitable as stand-alone sources for model training.


% \paragraph{Image Data} Developing an automated pipeline for obtaining all required paired data (gt image, object A, object B, bboxes, caption with grounding information) from any single image in the wild is a useful tool for obtaining unlimited amount of data and benefit our model training. With that end, we consider two separate pipelines: a bottom-up and a top-down approach. \textbf{(i) Bottom-up approach:} Starting from a single image, we run the SelectSubject tool \ref{TODO: is there a paper? should we cite photoshop? how to reference it?} for identifying the main objects in the scene. With a semantic segmentator \ref{TODO}, we identify those cases where multiple objects are in the center of attention. We discard the rest and randomly pick two of those entities for the selected images. Those two entities will be our composited objects. The contour of each of those objects is drawn on the original image in a different colour and provided to ViP-LLaVA \ref{} for (i) identifying each entity, and (ii) providing a caption describing their relationship in the original image. \textbf{(ii) Top-down approach:} While the previous approach offers a way to obtain paired data, with caption explicitly describing the desired interaction between composed objects, we need more diversity in caption format for our model to be more robust. With that end, we consider open-source images with available ground-truth captions, such as OpenImages \ref{}, and enrich our dataset with those groundtruth captions. In that case, given an image and its corresponding caption, we use GroundingDINO \ref{} for extracting grounding information correlating both sources. With the help of a semantic segmentator and after some filtering (i.e. duplicates, background entities, low confidence score or objects that are too big (>75\%) or too small(<10\%)) two of the grounding entities are randomly selected as the objects to composite. This step is important for adding diversity in training images layouts and caption formats, including both local and global descriptions, and therefore boosting the robustness of our model.

\textbf{Manually Collected Data} We manually compile a dataset containing 16,896 images of humans interacting with objects, 3,898 images of the same humans in neutral poses, and 1,250 images of those objects alone, captured from different angles and scenes. This data is segmented using \cite{qi2022entityseg} and their caption and grounding information is obtained using ViP-LLaVA \cite{cai2024vip}, following the same process described above. This collection provides a small curated set of high-quality images, including different views of each entity and covering human-object interactions, one of the most intricate relationships we aim to replicate.

% \paragraph{Manually Collected Data} We manually collect a set of images, containing 16,896 images of humans interacting with objects, 3,898 images of the same humans on varied neutral poses, and 1,250 images of the objects on their own, from different angles and in different scenes. This data is segmented with an off-the-shelf semantic segmentator \cite{qi2022entityseg} and their caption and grounding information is obtained using ViP-LLaVA \cite{cai2024vip}, following the same process described above.



%While the attention-based losses encourage the disentanglement of visual and semantic features of different objects in attention layers during training, some information leakage can still be produced at inference time. For avoiding this scenario, we further disentangle the different objects by masking all cross-attention layers corresponding to $\mathbf{c} \in \mathcal{C}_i$ with the mask $\mathcal{M}_i$ provided as input in $\mathcal{I}_{L}$ (\ie $\hat{\mathbf{A}}\left[ \mathbf{x}, \mathbf{c}\right]=0, \forall \mathbf{x} \in \mathcal{M}_i, \mathbf{c} \in \mathcal{C}_i$). Additionally, given $[EoT]$ tokens have been shown to contain information related to the foreground objects \cite{zhao2023loco}, We mask them using the union of all object masks ($\hat{\mathbf{A}}\left[ \mathbf{x}, \mathbf{c}\right]=0, \forall \mathbf{x} \in \cup_{i} \mathcal{M}_i, \mathbf{c} \in \mathcal{C}_{k \in \left[EoT\right]}$).
