\section{Experiments}
\label{sec:experiments}



\textbf{Evaluation Dataset} We evaluate image and text alignment in the customization task using the set of 300 paired two entities and text from \textit{MultiBench} \cite{li2024unimo}, a benchmark crafted for evaluating multi-entity subject-driven generation. We manually craft a bounding box for each category (\eg food, toy) to be used as input for our model and \cite{sun2024emugen}. For evaluating multi-object compositing, we create a test set of 118 paired data (background image, text prompt, two entities and their corresponding bounding boxes). We ensure different positional and action-based prompts are included, and entities include objects, animals and humans from \cite{pixabay,ruiz2023dreambooth,li2024unimo}. We also evaluate identity preservation on \textit{Dreambooth}, the single-object compositing set consisting of 113 background-object pairs used in \cite{tarres2024thinking,song2024imprint}.

\noindent
\textbf{Evaluation Metrics} We assess alignment to input text prompt and object images via CLIP-Score \cite{hessel2021clipscore} (CLIP-T, CLIP-I) and DINO-Score \cite{oquab2023dinov2} (DINO). When bounding boxes are specified for each object, we compare each input object image to a corresponding cropped region within the generated image, as in \cite{chen2023anydoor,yang2023paintbyexample,tarres2024thinking}. In the customization setting, where models may not specify object locations, each input object is instead compared to the entire generated image, following \cite{li2024unimo,pan2023kosmos}. We refer to this metric variation as CLIP-I\textit{gl}, DINO\textit{gl}. For scenes with multiple objects, we average DINO(\textit{gl}) and CLIP-I(\textit{gl}) scores across all objects.
For compositing multiple objects with single-object compositing models \cite{yang2023paintbyexample,zhang2023controlcom,chen2023anydoor,song2024imprint, tarres2024thinking}, we add objects sequentially. Thus, we compute performance for all possible object-order sequences and average the results. To assess alignment between text prompts and composite results, we calculate CLIP-T by comparing the entire image to the text prompt (CLIP-T\textit{gl}) and by comparing the cropped compositing area to the text prompt (CLIP-T\textit{loc}), evaluating local and global alignment. Additionally, we conduct user studies to evaluate alignment to input modalities, quality of composited images, and realism of generated interactions.

%When a bounding box is provided for each object as part of the input, each object image is compared to a cropped region of the generated image delimited by its corresponding bounding box, following \cite{chen2023anydoor,yang2023paintbyexample,tarres2024thinking}. In the customization setting, where not all compared models provide guidance for each object location, each input object image is compared to the entire generated image as in \cite{li2024unimo,pan2023kosmos}. When multiple objects are in the scene, DINO and CLIP-I scores are averaged across all objects. In the case of single-object compositing models, where multiple objects need to be added sequentially, we compute all possible object ordering options and average their performance. Additionally, when evaluating object compositing with aligned text and objects, we compute CLIP-T by (i) comparing the corresponding prompt to the entire image (CLIP-T\textit{gl}) and (ii) comparing the cropped compositing area of the generated image to the text prompt (CLIP-T\textit{loc}). Additionally, we perform user studies to evaluate alignment to each individual input modality, quality of our composite images and realism of generated interactions.

% \subsection{Metrics, Evaluation Data and Training Details}

\noindent
\textbf{Training Details} The U-Net and adaptor $\mathcal{A}$ are jointly trained on 8 A100, using Adam optimizer, learning rate of $4 \times 10^{-6}$ and effective batch size of 1024, via gradient accumulation. Losses are balanced using $\alpha = 10^{3}$, $\beta = 1$.


\subsection{Comparison to Existing Methods}

Our primary task is Object Compositing, but we also train for Subject-Driven Generation as an auxiliary task. Thus, we provide comprehensive evaluation across both tasks. %Our primary task is Object Compositing, however, as we also train our model for Subject-Driven Generation as an auxiliary task, we comprehensively evaluate our model on both tasks. %Our model is simultaneously trained on the tasks of (i) Object Compositing, and (ii) Subject-Driven Generation. Hence, we comprehensively evaluate our model for each task.


\noindent
\textbf{Generative Object Compositing}
For this task, we compare to recent generative object compositing models: Paint by Example (PbE) \cite{yang2023paintbyexample}, ControlCom \cite{zhang2023controlcom}, AnyDoor \cite{chen2023anydoor}, IMPRINT \cite{song2024imprint}, Thinking Outside the BBox (TTOB) \cite{tarres2024thinking}. These models support compositing from a single object, background, and bounding box, requiring sequential runs to add multiple objects individually. For comparison, we evaluate our model in three additional inference modes: (i) without text guidance, (ii) sequential single-object compositing, (iii) sequential compositing without text guidance. %These models enable compositing from a single object, background, and bounding box. When multiple objects are needed, we run the models sequentially to add each object individually. For comparison, we also evaluate our model in three settings: (i) without text control, (ii) sequentially compositing one object at a time, and (iii) sequentially without text control. %These models offer compositing from a single object, a background and a bounding box. Therefore, when required, we operate them sequentially for adding multiple objects. For comparison, we also evaluate our model (i) without text control, (ii) sequentially compositing one object at a time, (iii) in a sequential manner without text control.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/baselinescomp.jpg}
    \caption{Visual comparison to Generative Object Compositing models \cite{yang2023paintbyexample,zhang2023controlcom,chen2023anydoor,song2024imprint,tarres2024thinking}. Our model provides more realistic, harmonious and natural-looking interaction between composited objects via simultaneous multi-object compositing. See SuppMat for more examples.}

    \label{fig:baselinescomp}
    \vspace{-6mm}
\end{figure*}


\begin{table}[t!]
\centering
\begin{adjustbox}{width=\linewidth}
\begin{tabular}{lcccccc}
\toprule
%\multicolumn{1}{l}
\multirow{2}[3]{*}{\textbf{Method}} & \multicolumn{2}{c}{\textbf{DreamBooth}} & \multicolumn{2}{c}{\textbf{MultiComp \textit{-overlap}}} & \multicolumn{2}{c}{\textbf{MultiComp \textit{-nonoverlap}}} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}  & \textbf{CLIP-I$\uparrow$} & \textbf{DINO$\uparrow$} &  \textbf{CLIP-I$\uparrow$} & \textbf{DINO$\uparrow$} & \textbf{CLIP-I$\uparrow$} & \textbf{DINO$\uparrow$}\\ 
\cmidrule{1-7}
PbE \cite{yang2023paintbyexample}                                                                                                          &      0.778                   &  0.799             &        0.693             &   0.383  &     0.720 & 0.423                \\
\cmidrule{1-7}
ControlCom \cite{zhang2023controlcom}                                                                                          &      0.743          &  0.705 &          0.707                                                                 &                  0.478    &          0.740     &    0.543   \\ 
\cmidrule{1-7}
AnyDoor \cite{chen2023anydoor}                                                                                        &       0.806          & 0.836 &  0.727                                                               &       0.520          &   0.763    &               \textbf{0.593}         \\ 
\cmidrule{1-7}
IMPRINT \cite{song2024imprint}                                                                                                           &      \textbf{0.830}                   &  0.889   &           0.713                                                                                                         &        0.525             &  0.739   &      0.576                \\
\cmidrule{1-7}
TOTB \cite{tarres2024thinking}                                                                                            &    0.809      &  0.856   &      0.716    &                                                          0.485             &       0.740             &   0.531              \\ 
\cmidrule{1-7}
Ours                                                                                           &    0.803     &  0.892   &      \textbf{0.741}     &                                                              \textbf{0.532}             &   \textbf{0.768}            &   0.579              \\ 
\ \ w/o text                                                                                           &    0.816      &  \textbf{0.903}   &      0.729     &                                                          0.505             &       0.754             &    0.548             \\ 
\ \ sequential w/ text                                                                                          &    -      &  -   &      0.729     &   0.517                                                                  &       0.760             &    0.583              \\ 
\ \ sequential w/o text                                                                                           &    -      &  -   &      0.723     &    0.510                                                                &       0.756             &    0.578              \\ 




\bottomrule
\end{tabular}
\end{adjustbox}
% \vspace{-6mm}
\caption{Quantitative comparison of identity preservation against state-of-the-art generative object compositing methods. Single object compositing is evaluated on DreamBooth set. Two object compositing is on MultiComp set, comparing to both simultaneous and sequential uses of our model, by guiding inference with and without text. We distinguish between interacting (overlapping bboxes) and non-interacting cases for clarity. Details in SuppMat.} %Results on Multi-Comp set show average on both entities. All SoA models compose both entities sequentially in both orders and metrics are averaged. Deglossed results can be found in SuppMat.} %\sooye{\textbf{Soo Ye:} will it be too small if we make this a single column table?Changed! I think it's big enough}}
\label{tab:baselinescomp}
\vspace{-6mm}
\end{table}

As shown in Table \ref{tab:baselinescomp}, our model outperforms all compared models when composited objects interact, \ie when bounding boxes overlap. For non-overlapping or single-object compositions, our model maintains comparable identity and semantic preservation to state-of-the-art models. Furthermore, when objects overlap, compositing two objects simultaneously yields significantly better results than adding them sequentially, even with the same model, as in Fig \ref{fig:motivation}. Providing a text description of the interaction also boosts performance for scenes with multiple objects but is less critical when compositing a single object or non-interacting ones, where the expected composition is clearer.
%All SoA models can only composite one object at a time, therefore, we average the metrics after adding two sequential objects in the two possible ways. For comparison, we perform the same sequential action with our model as well as computing the composite image with two objects zero-shot. Table \ref{tab:baselinescomp} shows how our model is able to outperform all SoA sequential object compositing models in terms of identity preservation, in the case of compositing two objects that show some overlap. It also obtains comparable performance when the two objects are not overlapping or when only one object is composited. When the objects are overlapping, adding two objects in one shot shows some clear benefits over adding them sequentially, obtaining much better performance, even when employing the exact same model. It should also be noted that adding a text description describing the interaction shows to be beneficial when multiple objects are in the scene, but less necessary when compositing one single object.
Fig \ref{fig:baselinescomp} illustrates several advantages of compositing multiple objects simultaneously rather than sequentially: (i) it enables more cohesive harmonization and appearance consistency across objects and the scene (rows 1 - 5); (ii) it captures complex interactions involving object reposing with ease (rows 1, 6, 7); and (iii) with text guidance, the model can naturally complete scenes by adding any additional elements needed for realism (row 7).


%displays some benefits of compositing multiple objects simultaneously other than adding them sequentially. When simultaneously compositing multiple objects, (i) the model is able to achieve a more consistent harmonization and appearance between the objects and the scene (rows 1,2,3,5); (ii) complex interactions that require object reposing are easily achievable (rows 1,4,6,7); (iii) by guiding the generation through text describing the expected scene and training to perform accurate interaction, the model learns to complete the image with any additional necessary element, when convenient (row 7).

\noindent
\textbf{Multi-Entity Subject-Driven Generation} %Multimodal-to-Image Generation} 
We compare to customization methods accepting text and multiple object images as input (BLIP-Diffusion \cite{li2024blip}, KOSMOS-G \cite{pan2023kosmos}, UNIMO-G \cite{li2024unimo}). We also compare to Emu2Gen \cite{sun2024emugen}, additionally accepting layout guidance, therefore sharing input modalities with our model and allowing fair comparison.


\begin{table}[t!]

\centering
\begin{adjustbox}{width=\linewidth}

\begin{tabular}{lccccc}
\toprule
%\multicolumn{1}{l}
\textbf{Method}                            & \textbf{CLIP-I$\uparrow$} & \textbf{DINO$\uparrow$} & \textbf{CLIP-I\textit{gl}$\uparrow$} & \textbf{DINO\textit{gl}$\uparrow$} & \textbf{CLIP-T\textit{gl} $\uparrow$} \\ 
% \cmidrule{1-6}
\midrule \multicolumn{6}{c}{Input: \textit{Text, Object Images}}
\\ \midrule
BLIP-Diffusion \cite{li2024blip}       &  - & -          &  0.675                                                                                                  &  0.455    &    0.249                        \\
\cmidrule{1-6}
KOSMOS-G \cite{pan2023kosmos}       &    - & -         & \textbf{0.704}                                                                                                  &  0.465      &   0.279       \\
\cmidrule{1-6}
UNIMO-G \cite{li2024unimo}  & - & -     &  0.699  &    \textbf{0.485}                                                                                                              &   0.293                     \\
% \cmidrule{1-6}
\midrule \multicolumn{6}{c}{Input: \textit{Text, Object Images, Layout}} \\
\midrule
Emu2-Gen \cite{sun2024emugen}      &  0.595 & 0.414     &  0.616                                                                                                  &    0.434        &   0.287                                      \\
\cmidrule{1-6}
Ours    & \textbf{0.783} & \textbf{0.599}  &       0.688  &          0.454                                                                                                     &   \textbf{0.308}                             \\
% \ \ w/o bboxes        &     0.672  &       0.437                                                                                                       &    0.295  &        0.467                                \\



\bottomrule
\end{tabular}
\end{adjustbox}

\caption{Quantitative comparison of identity preservation and text fidelity against multi-entity subject-driven generation methods. We compare to state-of-the-art methods on two-entity subset of MultiBench. For models with layout guidance, additional identity preservation metrics (CLIP-I, DINO) are provided by considering cropped regions around each object in the generated images.}
\label{tab:baselines2objcustomiz}
\vspace{-4mm}
\end{table}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/emu2gen.jpg}
    \caption{Visual comparison of our multi-object compositing model to Emu2Gen \cite{sun2024emugen}. For fair comparison, exact same set of inputs (including background image) is provided to each model. }

    \label{fig:baselineEmu}
    \vspace{-6mm}
\end{figure}

\noindent
Although customization is introduced as a proxy task to improve multi-object compositing, our model achieves comparable performance to state-of-the-art customization models, as shown in Table \ref{tab:baselines2objcustomiz}. Notably, our model obtains improved text alignment, benefiting from training data balancing local and global caption descriptions, as well as joint training with the complex task of object compositing. Additionally, our model demonstrates improved performance in patch-based image metrics (CLIP-I, DINO), indicating high layout alignment and identity preservation. In contrast, Emu2Gen obtains lower patch-based than global metrics. Refer to Fig 1 (bottom) and SuppMat for visual examples.

%by cropping around each object based on the given layout, while Emu2-Gen shows decreased performance in comparison. This indicates better layout alignment of our model. %that our model achieves better alignment between the generated result and the input layout.
%Notably, our model achieves improved text alignment, benefiting from training data that balances local and global caption information, and training for the complicated task of object composited.


%Our model is optimized for multi-object compositing. However, as an essential part of our training, which provides better generation quality and much higher identity preservation, we alternate the training for object compositing and image customization. As a result, we obtain a model that outperforms other single-object compositing models, and that can also be used for image customization with comparable performance to SoA models as depicted in tables \ref{tab:baselines2objcustomiz} and \ref{tab:baselines1objcustomiz}. In particular, our model achieves an improved text alignment, due to our training data, containing some meticulously crafted balance between local and global captions.

\noindent
\textbf{User Studies} We complement the comparison to State-of-Art models by completing various user studies (Fig \ref{fig:userstudies}). Non-expert users are presented with side-by-side generations from our model and each of the object compositing baselines, and are asked to choose their preferred image based on `most realistic interaction' (in cases of mask overlapping) and `best image quality'. Due to our model's ability to composite multiple interacting or non-interacting objects in a natural way, users prefer our model with up to 66.7\% preference in terms of image quality and up to 97.1\% preference for realistic interactions, via majority consensus. We also assess alignment to each individual input modality by comparing our compositing model to Emu2Gen \cite{sun2024emugen}. Emu2Gen is provided with the same inputs (Fig \ref{fig:baselineEmu}), presented as a multimodal input interlacing text, object images, object-specific bounding boxes and background. Background image is provided by adding \textit{`in $<\mathcal{I}_{BG}>$'} to the multimodal input, as showcased in \cite{sun2024emugen}. In this comparison, users prefer our alignment across all modalities. See SuppMat for details.


%We also compare our model's alignment to that of the customization model Emu2Gen \cite{sun2024emugen}, by completing all captions provided to their model with \textit{"in $<\mathcal{I}_{BG}>$"}. This way, we provide the background information, feeding the model with the same input. In this comparison, users preferred our model's alignment across all modalities. Fig \ref{fig:baselineEmu} shows a visualization of the comparison.


%We provide users with side-by-side generations by our model and each of the object compositing baselines and ask for their preference in terms of 'most realistic interaction' (in case of mask overlapping) and image quality. Due to our model's ability to simultaneously composite multiple interacting or non-interacting objects in a natural way, users preferred our model with up to 66.7\% preference rate in terms of quality and up to 97.1\% in terms of realistic interactions. We additionally compare the alignment of different input modalities to the customization model Emu2Gen \cite{}, which accepts the same set of modalities. When generating images via Emu2Gen, we complete all captions with \textit{"in $\mathcal{I}_{BG}$"} as a way to provide the background. Our model's alignment in all modalities is preferred by the users.


\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/userstudies.jpg}
    \caption{User Studies. \textit{Top:} Percentage of users prefering our method or each baseline \cite{chen2023anydoor,zhang2023controlcom,song2024imprint,yang2023paintbyexample,tarres2024thinking} on `image quality' and `realistic interaction'. \textit{Bottom}: Percentage of users prefering our method or Emu2Gen \cite{sun2024emugen} in terms of text, layout, objects and background alignment. All results are via majority consensus.} %Although accepting the exact same input, in this comparison our method is performing object compositing and \cite{sun2024emugen} subject-driven generation.} % \sooye{\textbf{Soo Ye:} is it possible to make the bars in the lower results thinner similar to the upper results?Done!}}

    \label{fig:userstudies}
    \vspace{-4mm}
\end{figure}

\subsection{Ablation Study}


\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/ablationcustom.jpg}
    \caption{Visualization of the effect of Joint Training for Compositing and Customization tasks. When training solely on object compositing, balancing text and image alignment becomes significantly more complex. In this setup, the ablation model significantly drops identity preservation when inference is also guided with text. Our final model achieves better visual-language balance. }

    \label{fig:ablationcustom}
    \vspace{-6mm}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/ablationloss.jpg}
    \caption{Visualization of the effect of $\mathcal{L}_c$, $\mathcal{L}_s$. Without attention-based losses, our model merges objects with similar semantics or visual traits. Cross-attention loss $\mathcal{L}_c$ improves identity separation, but some leakage remains (\eg, cat ears on dog and teapot). Adding self-attention loss $\mathcal{L}_s$ further reduces this leakage.} %Without any attention-based loss, our model tends to merge entities with similar semantics or visual attributes. Adding cross-attention loss leads to more disentangled identities, while still observing some leakage (\eg cat ears on teapot (top), cat attributes on dog (bottom)). We minimize these leakages by adding a self-attention loss.}

    \label{fig:ablationloss}
    \vspace{-2mm}
\end{figure}




\begin{table}[t!]

\centering
\begin{adjustbox}{width=0.47\textwidth}

\begin{tabular}{lllll} %{lcccc}
\toprule
%\multicolumn{1}{l}
\textbf{Method}                            & \textbf{DINO$\uparrow$} & \textbf{CLIP-I$\uparrow$} & \textbf{CLIP-T\textit{loc}$\uparrow$} & \textbf{CLIP-T\textit{gl}$\uparrow$} \\ 
\midrule \multicolumn{5}{c}{MultiComp-\textit{action}}
\\ \midrule
% \multicolumn{5}{c}{MultiComp-\textit{action}}      
% \cmidrule{1-5}
Ours       &    \textbf{0.540}                                                                                                  &  0.745              &   0.286   &                    0.285                      \\
\ \ w/o self loss       &    0.538                                                                                                  &  0.744              &   0.283   &                    0.284                    \\
\ \ w/o attn loss       &    0.534                                                                                                  &  0.739             &   0.270   &                    0.274                     \\
\ \ w/o customiz.       &    0.449                                                                                                 &  0.705              &   \textbf{0.295}   &                    \textbf{0.298}                     \\
\ \ w/o multi-view       &    0.535                                                                                                  &  \textbf{0.751}              &   0.268   &                    0.273                     \\
\midrule \multicolumn{5}{c}{MultiComp-\textit{positional}}
\\ \midrule
% \cmidrule{1-5}
Ours       &    0.585                                                                                                  &  0.773              &   0.292   &                    0.294                      \\
\ \ w/o self loss       &    0.584                                                                                                  &  0.770              &   0.289   &                    0.291                     \\
\ \ w/o attn loss       &    0.581                                                                                                  &  0.770             &   0.281   &                    0.285                     \\
\ \ w/o customiz.       &    0.474                                                                                                 &  0.734              &   \textbf{0.298}   &                    \textbf{0.298}                     \\
\ \ w/o multi-view       &    \textbf{0.591}                                                                                                  &  \textbf{0.780}              &   0.280   &                    0.283                     \\




\bottomrule
\end{tabular}
\end{adjustbox}

\caption{Quantitative comparison of text and image alignment to different ablations of our compositing model. For better analysis we distinguish between \textit{action} and \textit{positional} subsets of MultiComp set, based on objects relationship in text description. We compare our full model to different versions obtained without (i) self-attention loss $\mathcal{L}_s$; (ii) self and cross-attention losses ($\mathcal{L}_c$,$\mathcal{L}_s$); (iii) joint training for compositing and customization; and (iv) multi-view data (\ie video, manually collected set).}
\label{tab:ablations}
\vspace{-6mm}
\end{table}

Table \ref{tab:ablations} displays identity and text fidelity metrics for various ablations of our model. (i) As seen in Fig \ref{fig:ablationloss}, adding attention-based losses helps prevent semantic and visual information leakage between objects, improving both identity preservation and text alignment, as well as the overall image quality. (ii) When training solely for object compositing, the model must learn to add new objects to a scene, repose them according to actions described in text input, complete the scene by adhering to the same text, and harmonize and relight the image to make everything look seamless while preserving object identity. By introducing customization as an auxiliary task and alternating between both tasks during training, the model can sometimes relieve the inpainting aspect of compositing, focusing only on balancing text alignment and identity preservation. As in Fig \ref{fig:ablationcustom}, in the absence of the customization task, the model struggles to balance text and image preservation, and the objects become irrecognizable. (iii) Without multi-view data, even if visual and geometrical transformations are applied to the object images, the model lacks the ability to repose them to match the actions described in the text. This results in a `copy-paste' behavior where objects are minimally transformed (high CLIP-I, DINO), preserving identity but leading to highly unnatural results and low text alignment (CLIP-T\textit{gl},\textit{loc}).


%the addition of a cross-attention and a self-attention loss help avoid leakage of semantics and visual information of one object into another, boosting both identity preservation and text alignment, as well as the overall image quality. When training the model solely for object compositing, it needs to simultaneously learn to add the new objects to a scene, repose them to align with the actions described in the text, complete the scene by following the same text and harmonize and relight everything to make the result look seamless without losing the identity of the objects. By introducing the task of customization as an additional task, and alternating between both tasks during training, the model can, on some steps, alleviate the inpainting side of the task, and focus on offering better identity preservation while reposing the objects and completing the background to align with the input text. When no reposing data is used, even if some visual and geometrical transformations are applied on the input objects as data augmentation, the model lacks the ability to repose the objects to match the actions described in the text description, leading to a 'copy-paste' behaviour in which the object is minimally transformed, leading to good identity perservation but highly unnatural results.



\vspace{-2mm}
\subsection{Applications}

Although not explicitly trained for them, our model exhibits some emerging capabilities. See SuppMat for more.
%Our model is trained for subject-driven generation and multi-object compositing, with two objects used simultaneously to drive the output. However, we observe this training enables additional capabilities to emerge. %Our model is jointly trained on subject-driven generation and multi-object compositing. In particular, only two objects are simultaneously used to drive our result. However, we observe additional capabilities emerge from such training. 

\textbf{Multi-Object Generation} As shown in Fig \ref{fig:applications} (top), our model is able to perform multi-object compositing with more than two objects. By learning one-to-one object interactions, it develops a strong prior that allows it to generalize to compositing multiple objects simultaneously. %By learning one-to-one object interactions as a foundational skill, the model develops a strong enough prior to generalize to simultaneous compositing of a higher number of objects. %The one-to-one object interactions learnt by our model are a strong enough prior for the model to generalize into simultaneos compositing of three or more objects.

\textbf{Subject-Driven Inpainting} Our joint training enables the model to learn key subtasks, such as background synthesis, blending, harmonization, and reposing. As shown in Fig \ref{fig:applications} (bottom), these skills can be applied to subject-driven inpainting. In this task, the model uses text and layout guidance to seamlessly complete a scene, generating and integrating additional objects while maintaining a natural and coherent composition with the given visuals. %Our joint training allows the model to learn all subtasks required for both tasks, such as background synthesis, blending, harmonization, reposing, etc. As shown in Fig \ref{fig:applications} (bottom), these skills can be simultaneously applied to the task of subject-driven inpainting. In this task, a background image is provided, and the model uses the text and layout guidance to complete the scene seamlessly, generating any additional object and integrating the given objects while maintaining a coherent and natural composition.

%\textbf{Subject-Driven Inpainting} By jointly training the model on the tasks of object compositing and generation, our model learns all subtasks required for both separate tasks, \ie background synthesis, blending, harmonization, reposing, etc. We show in Fig \ref{fig:applications} that all these skills can be simultaneously applied to the task of subject-driven inpaining. In such task, a background image is provided, and the model is able to follow the text and layout guidance for completing it in a seamless way, while integrating the given objects.


\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/applications.jpg}
    \caption{Applications of our model. \textit{Top:} Simultaneous Multi-Object Compositing of Three Objects. \textit{Bottom:} Subject-Driven Inpainting. Our model is able of simultaneously compositing objects while completing the scene with text and image alignment.} %Additionally to compositing different objects into the background, our model can complete the scene based on the textual description.}

    \label{fig:applications}
    \vspace{-6mm}
\end{figure}
\vspace{-2mm}
\subsection{Limitations}

Although our model can handle two or more objects, the pipeline is not designed for an unlimited number. As more objects are added, the multimodal embedding grows, which could lead to scalability issues. A potential future improvement could involve feeding each object embedding separately, impacting only corresponding attention maps. Additionally, we base our model on SD1.5 due to compute limitation. Overall harmonization and composition quality could be further improved with stronger diffusion base model such as SDXL \cite{podell2023sdxl} or SD3 \cite{esser2024scaling}.

%Although it is possible for our model to operate on two or more objects, the pipeline is not designed to accommodate an illimited amount of objects. The addition of input objects causes the multi-modal embedding to grow in size, potentially leading to scalability issues when the number of objects is too high. A potential solution to explore in future work would be to feed each object embedding parallelly in separate cross-attentions, affecting only the corresponding attention maps.

% - Scalability to multiple objects: multi-modal embedding grows with the number of objects. Instead, use object embeddings by directly altering corresponding words attention scores.
% - CLIP text embedding: nuances in long, detailed descriptions might be lost.
