\section{Introduction}
\label{sec:intro}

In recent years, advancements in image generation and editing have enabled the creation of realistic and complex visual scenes from various input modalities, such as text descriptions \cite{rombach2022ldm,podell2023sdxl,esser2024scaling}. These tools are becoming essential for enhancing creativity and streamlining workflows. A key component in this process is object compositing, enabling seamless integration of new objects in an existing scene. However, current compositing models \cite{song2024imprint,tarres2024thinking,yang2023paintbyexample,zhang2023controlcom,chen2023anydoor} are generally limited to handling a single object at a time, requiring sequential steps to composite multiple objects into a scene. This process is cumbersome and fails to capture the complex, real-world interactions between multiple objects, especially when simultaneous repositioning is necessary. For instance, sequential compositing struggles to accurately create interactions where objects need to interact very closely, such as two figures hugging, or someone playing an instrument (Fig \ref{fig:motivation}). %\sooye{\textbf{Soo Ye:} it would be good to have a small figure illustrating the limitations of sequential compositing on this page if there's time. Gemma: Good idea! I'll add one soon}

To address these limitations , we introduce a novel text-guided and layout-guided multi-object compositing model designed to handle and harmonize multiple objects simultaneously within a single composition. This model bridges the gap between single object insertion and realistic, context-aware scene construction via three main features: (i) it allows for simultaneous reposing and complex interaction between objects, seamlessly capturing intricate relationships; (ii) it ensures visual coherence between both the new objects and the scene, achieved through synchronized relighting and reharmonization;  (iii) it automatically generates additional elements essential to the action or interaction being depicted (\eg, a leash for a dog-walking scenario, or liquid pouring from a bottle to a glass), creating a more cohesive and natural scene.

Our model accepts multimodal input, including object images, text, object-specific bounding boxes, a background image and a mask delimiting the overall compositing region. To enable effective and balanced integration of these modalities, our model is trained on diverse, multimodal data that provides rich grounding for scene context and object relationships, allowing it to understand and implement nuanced multi-object compositions. Collecting data with all these aligned modalities is a complex task. However, advancements in Large Language Models (LLMs) \cite{liu2023improvedllava,driess2023palm,alayrac2022flamingo} and Visual Language Models (VLMs) \cite{liu2023groundingdino} offer a solution. We introduce a data generation pipeline that leverages these models to synthesize essential missing data, enabling the creation of a fully aligned, multimodal training set.
%\sooye{\textbf{Soo Ye:} How about adding something like: collecting data with all these inputs is hard -$>$ recent LLMs/VLMs have amazing performance -$>$ we present a data generation pipeline leveraging these. Since the data generation pipeline is a core contribution of this paper, we want to emphasize it in the intro. Good idea! Done!} 
By leveraging varied data sources during training and balancing real and synthetic data, our model learns to anticipate and position additional supporting objects, if needed, to ensure natural compositions. Furthermore, we jointly train the model on compositing and customization tasks to improve compositing performance and offer more versatility. This dual approach allows the model to separately focus on two key subtasks in multi-object compositing: (i) generating realistic interactions with strong text-image alignment, and (ii) improving inpainting, harmonization and relighting. %This dual training allows the model to separately focus on improving the two main subtasks in multi-object compositing: (i) generating a realistic interaction between both objects, with good text-image alignment, and (ii) improving the inpainting part of the compositing task, allowing for a better harmonization and relighting.

Our contributions are as follows:

\begin{itemize}
    \item We present the first generative model for simultaneous multi-object compositing, addressing the limitations of sequential object compositing.
    \item We propose a novel data generation pipeline that combines real labeled data, synthetic captions from vision-language models, and grounding methods to align global and local captions with images for improved training.
    \item We leverage customization as 
    % a complementary 
    an auxiliary
    task to improve text and image alignment in compositing, resulting in a model capable of performing both tasks with performance comparable to state-of-the-art customization and generative object compositing models.
\end{itemize}


\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/motivation.jpg}
    \caption{Comparison of simultaneous vs. sequential object compositing. Sequential addition prevents reposing of previously composited objects, resulting in limited, less cohesive compositions.} %We compare an image obtained compositing two objects in a single shot, to adding them sequentially in two different orders. Due to the inability to the model to repose already composited objects, the sequential approach fails to achieve a natural composition.}

    \label{fig:motivation}
    \vspace{-6mm}
\end{figure}


