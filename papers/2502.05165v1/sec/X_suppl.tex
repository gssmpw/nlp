\clearpage
\maketitlesupplementary
\setcounter{page}{1}
\setcounter{figure}{0}
\setcounter{section}{0}


\section{Training and Testing Data}

In this section, we provide additional information about the training data generation pipeline proposed in Main Paper Section \ref{sec:datageneration} and MultiComp, the multi-object compositing test set introduced in Main Paper Section \ref{sec:experiments}.

\subsection{Training Data}

As detailed in Main Paper Section \ref{sec:datageneration}, our paired training data --- comprising ground truth images with multiple objects, descriptive captions with grounding information, segmented images of two objects, and corresponding object-specific bounding boxes --- is collected from complementary sources: Video Data, In-the-Wild Images, and Manually Collected Data. Below, we expand on how paired data is extracted from each source.


\noindent\textbf{Video Data} Fig \ref{fig:suppvideo} illustrates the process for extracting paired training data from videos in \cite{shang2017video, shang2019annotating}. A ground-truth frame containing an annotated relationship between two objects is randomly selected. Two additional frames, each showing one of the interacting objects, are extracted from the same video, ensuring a similarity between the views of each object (DINO score MSE $\geq 0.8$) \cite{oquab2023dinov2}. A caption describing the relationship is automatically generated by feeding the ground-truth image and annotated relation into LLaVA v1.6 (34B) \cite{liu2023improvedllava} with the prompt:
\textit{``Can you provide a grammatically correct one-line caption for the relation $<$object A$>$ $<$relation$>$ $<$object B$>$ in the image?"}.
The segmented objects are then extracted using an off-the-shelf semantic segmentation model \cite{qi2022entityseg}.


%Figure \ref{fig:suppvideo} depicts the process followed for extracting paired training data from any video in \cite{shang2017video,shang2019annotating}. A ground-truth frame containing an annotated relation between two objects in the image is extracted. Two additional frames containing each of the interacting objects are extracted from the same video, ensuring certain similarity between the two views of each object (DINO score MSE $\geq 0.8$) \cite{oquab2023dinov2}. The corresponding caption is obtained by providing the ground truth image and the annotated relation to LLaVA v1.6 (34B) \cite{liu2023improvedllava} and asking for a grammatically correct caption via \textit{"can you give a grammatically correct descriptive one-line caption for the relation $<$object A$>$ $<$relation$>$ $<$object B$>$ in the image?"}. An off-the-shelf semantic segmentation model \cite{qi2022entityseg} is used for extracting a segmented image of each object.




\noindent\textbf{Image Data} We propose two automated approaches for obtaining paired data from in-the-wild images. 

\textbf{Top-down approach} As illustrated in Fig \ref{fig:suppimagevip}, this approach generates paired training data from a single image using a systematic process. First, a commercial subject selection tool identifies the main objects in the scene. A semantic segmentation model \cite{qi2022entityseg} then segments the selected region to determine the number of objects present. If multiple objects are detected, two are randomly selected as composited objects. Their outlines are highlighted on the image using distinct colors (e.g., orange and blue) and passed to ViP-LLaVA (13B) \cite{cai2024vip} for caption generation through two sequential prompts. In the first step, entities within each highlighted outline are identified with a question like: \textit{``Please follow the sentence pattern of the example to list the entities within each rectangle. Example: `orange: banana; blue: apple'"}. This produces responses such as \textit{``orange: teddy bear; blue: girl"}. Using these entity labels, the second step generates a descriptive caption with a prompt like: \textit{``Can you provide a one-line caption including the interaction between `teddy bear within the orange rectangle' and `girl within the blue rectangle' in the image by using these exact entity names?"}. The resulting caption, \eg, \textit{``A young girl within the blue rectangle is holding a large teddy bear within the orange rectangle"}, is refined by removing the grounding phrases \textit{``within the orange/blue rectangle"} after using them for correlating object images with text tokens. This method ensures accurate grounding information, even when both entities are labeled the same, resulting in the final caption: \textit{``A young girl is holding a large teddy bear"}.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/supp_video.jpg}
    \caption{Training Data Generation from Video Data. Paired training data is obtained from video object relation datasets \cite{shang2017video,shang2019annotating} by extracting three frames with corresponding annotations and leveraging Vision-Language Models \cite{liu2023improvedllava}.}
    \label{fig:suppvideo}
    % \vspace{-6mm}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/supp_imagevip.jpg}
    \caption{Training Data Generation from Image Data via Top-Down Approach. Paired training data is derived from in-the-wild images by leveraging a Vision-Language Model \cite{cai2024vip} and a Semantic Segmentator \cite{qi2022entityseg}.}
    \label{fig:suppimagevip}
    % \vspace{-6mm}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/supp_imagedino.jpg}
    \caption{Training Data Generation from Image Data via Bottom-Up Approach. Paired training data is extracted from in-the-wild images with a paired caption by leveraging a Grounding Model \cite{liu2023groundingdino} and a Semantic Segmentator \cite{qi2022entityseg}.}
    \label{fig:suppimagedino}
    % \vspace{-6mm}
\end{figure}


%As depicted in Fig \ref{fig:suppimagevip}, this aproach allows obtaining paired data from a single image. First, a commercial subject selection tool is used to identify the main objects in the scene. Then, we leverage a semantic segmentation model \cite{qi2022entityseg} to segment the selected region and identify the number of objects in it. If multiple objects are in the selected region, two of them are randomly picked as composited objects. Their outline is colored on top of the image using two different colors (\ie orange, blue) and provided to ViP-LLaVA (13B), asking two consecutive questions. First, each entity is named via the question \textit{"Please follow the sentence pattern of the example to list the entities within each rectangle. Example: 'orange: banana; blue: apple'."} The answer to this question (\eg "orange: teddy bear; blue: girl") is used to guide the second question \textit{"Can you provide a one-line caption including the interaction between "teddy bear within the orange rectangle" and "girl within the blue rectangle" in the image by using these exact entity names?"}, obtaining a caption such as \textit{"A young girl within the blue rectange is holding a large teddy bear within the orange rectange"}. Finally, "within the orange rectangle" and "within the blue rectangle" are used for correlating object images with text tokens, providing grounding information, and removed from the final caption (\textit{"A young girl is holding a large teddy bear."}).

\textbf{Bottom-up approach} This approach (Fig \ref{fig:suppimagedino}) leverages a grounding model like GroundingDINO \cite{liu2023groundingdino} to process paired ground truth images and captions. The model extracts bounding boxes that link specific words in the caption to objects in the image. Duplicates, background elements, and undesired objects (\eg, overly large or small objects, or those with low confidence scores) are removed, leaving a set of object candidates with corresponding grounding information. Two of those objects are then randomly selected, and an off-the-shelf semantic segmentation model \cite{qi2022entityseg} is used to extract them. This results in two segmented objects along with their associated grounding details from the original caption.


%By providing a paired ground truth image and caption to a grounding model such as GroundingDINO \cite{liu2023groundingdino}, several bounding boxes are obtained correlating specific words in the caption with objects in the image. By removing duplicates, background objects and unwanted objects (too big, too small, low confidence score), we obtain several object candidates with their grounding information. By randomly selecting two of those objects, and using an off-the-shelf semantic segmentator \cite{qi2022entityseg}, two segmented objects with their grounding information in the ground truth caption are obtained.

\noindent\textbf{Manually Collected Data} We manually collect and annotate images featuring objects, humans, and human-object interactions. For captioning with grounding information, we use ViP-LLaVA (13B) \cite{cai2024vip}, following the exact same procedure as the top-down approach described above. Additionally, a semantic segmentation model \cite{qi2022entityseg} is employed to segment entities in images containing either a single object or a human. Fig \ref{fig:suppcollected} illustrates this approach.

%We employ ViP-LLaVA (13B) \cite{cai2024vip} for obtaining a caption with grounding information, following the same procedure as in the top-down approach explained above. Additionally, we rely on a semantic segmentation model \cite{qi2022entityseg} for obtaining segmented entities from images containing a single object or human.





\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/supp_collected.jpg}
    \caption{Training Data Generation from Manually Collected Data. Paired training data is obtained from a collected dataset containing object images, human images, and images of humans interacting with objects. We leverage a Vision-Language Model \cite{cai2024vip} and a Semantic Segmentator \cite{qi2022entityseg} to extract segmented objects and corresponding caption with grounding information.}
    \label{fig:suppcollected}
    % \vspace{-6mm}
\end{figure}


\section{Inference Data}

%Explain how many images in each subset of MultiComp (overlap, action, positional...)

Our collected MultiComp set consists of 119 paired data entries, each containing: (i) a background image, (ii) two object images, (iii) object-specific bounding boxes, (iv) an inpainting bounding box encompassing the previous ones, and (v) a descriptive caption with grounding information. Background images are sourced from Pixabay \cite{pixabay}, while objects are from Pixabay \cite{pixabay}, MultiBench \cite{li2024unimo}, and DreamBooth \cite{ruiz2023dreambooth}. Bounding boxes and captions are manually crafted. For evaluation, we perform 5 iterations of each model on the entire set, resulting in 595 generated images.

Out of these, 395 images contain overlapping bounding boxes for the two objects (MultiComp-\textit{overlap}), while 200 show non-overlapping bounding boxes (MultiComp-\textit{nonoverlap}). We evaluate these subgroups separately due to their differing levels of difficulty. While simultaneous compositing offers benefits like cohesive harmonization in both cases, it is especially effective in the overlapping cases, where it allows for simultaneous object reposing and the generation of additional elements needed for the scene. %While simultaneous compositing of multiple objects offer some benefits in both overlapping and non-overlapping cases, such as cohesive harmonization, it's in the overlapping cases when it becomes particularly useful, due to its ability to simultaneously repose objects and extrapolate additional necessary elements in the scene.

%Out of these, 395 images contain overlapping bounding boxes for the two objects, grouped in MultiComp-\textit{overlap}, and 200 show non-overlapping bounding boxes (MultiComp-\textit{nonoverlap}). We consider the two separate subgroups separately for a fair evaluation due to different degrees of difficulty. Although there are still some benefits on performing simultaneous compositing of multiple objects (\eg cohesive harmonization) when their bounding boxes are not overlapping, the sequential composition of those objects is much more easy and effective than when in those cases there is some overlap.

When textual input is provided, we further categorize the set into two subgroups based on caption types: MultiComp-\textit{action} and MultiComp-\textit{positional}. The MultiComp-\textit{action} subset contains 375 images generated from action-based captions (\eg, \textit{running after, playing with, holding}), while the MultiComp-\textit{positional} subset includes 220 images generated from captions describing positional relations (\eg, \textit{next to, behind, in front}). This distinction allows us to separately evaluate cases where reposing objects is often necessary (MultiComp-\textit{action}) versus those primarily describing object layout (MultiComp-\textit{positional}).

%When textual input is provided, we instead consider two subgroups based on those captions : MultiComp-\textit{action}, MultiComp-\textit{positional}. MultiComp-\textit{action} contains 375 images generated using captions describing action relationships between objects (\eg \textit{running after, playing with, holding}), while MultiComp-\textit{positional} contains 220 images generated from prompts describing positional relations such as \textit{next to, behind, in front}. By making this distinction, we can evaluate separately cases in which reposing the object is often required (MultiComp-\textit{action}) and those whose caption mainly described the expected layout (MultiComp-\textit{positional})

\section{Comparison to Existing Methods}

We provide additional visualizations comparing our model to existing generative object compositing and multi-entity subject-driven generation models in Sections \ref{sec:supp_compbaselines} and \ref{sec:supp_custombaselines}. Further details on user studies can be found in Section \ref{sec:supp_userstudies}.

\subsection{Comparison to Generative Object Compositing Methods}
\label{sec:supp_compbaselines}

We visually compare our simultaneous multi-object compositing method to sequentially adding two objects using State-of-the-Art Generative Compositing Methods \cite{yang2023paintbyexample,zhang2023controlcom,chen2023anydoor,song2024imprint,tarres2024thinking} in Fig \ref{fig:supp_baselinescomp}.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.98\linewidth]{figs/supp_baselinescomp.jpg}
    \caption{Visual comparison of our Multi-Object Compositing Method and State-of-the-Art Generative Object Compositing Methods \cite{yang2023paintbyexample,zhang2023controlcom,chen2023anydoor,song2024imprint,tarres2024thinking}.}

    \label{fig:supp_baselinescomp}
    % \vspace{-6mm}
\end{figure*}

%One or two page of comparison, with two objects

\subsection{Comparison to Subject-Driven Generation Methods}
\label{sec:supp_custombaselines}

%One or two page of comparison, with two objects
We visually compare two-entity subject-driven generation using our method and existing methods with available code (BLIP-Diffusion \cite{li2024blip}, KOSMOS-G \cite{pan2023kosmos} and Emu2Gen \cite{sun2024emugen}) in Fig \ref{fig:supp_baselinescustom}.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/supp_baselinescustom.jpg}
    \caption{Visual comparison of our Customization Method and State-of-the-Art Subject-Driven Generation Methods \cite{li2024blip,pan2023kosmos,sun2024emugen}.}

    \label{fig:supp_baselinescustom}
    % \vspace{-6mm}
\end{figure*}

\subsection{User Studies}
\label{sec:supp_userstudies}

We conduct six user studies to evaluate our multi-object compositing model against other generative object compositing models \cite{yang2023paintbyexample,zhang2023controlcom,chen2023anydoor,song2024imprint,tarres2024thinking} and Emu2Gen \cite{sun2024emugen}. In each study, non-expert users are shown two side-by-side images, one generated by our model and the other by a baseline, presented in random order. Users are asked to choose the preferred image based on a specific criterion. The entire MultiComp set is used for each experiment, except for the `most realistic interaction' evaluation, where only images from MultiComp-\textit{overlap} are considered, as this subset best evaluates the task. At least five users rate each image pair, and the results are aggregated via majority consensus. Visual examples of each experiment and the specific questions posed to the users can be found in Figs \ref{fig:supp_userquality}, \ref{fig:supp_userinteraction}, \ref{fig:supp_userbg}, \ref{fig:supp_usercaption}, \ref{fig:supp_userobjs}, and \ref{fig:supp_userlayout}.

%In all cases, we provide non-expert users with two side-by-side images generated by our model and one of the baselines, in randomized order, and ask to pick the preferred one based on a specific criteria. We use the entire MultiComp set for each experiment, except when asking for 'most realistic interaction', in which case only images in MultiComp-\textit{overlap} are provided, since that is the specific subset we want to evaluate. We ask that at least 5 users rate each image pair and evaluate via majority consensus. A visual example of each experiment and the specific question asked to the users can be found in Figs \ref{fig:supp_userquality}, \ref{fig:supp_userinteraction}, \ref{fig:supp_userbg}, \ref{fig:supp_usercaption}, \ref{fig:supp_userobjs}, \ref{fig:supp_userlayout}.

%Details on user studies (num workers, exact questions, visualizations, num images)

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/supp_usercompquality.jpg}
    \caption{User Study on `Compositing Quality'. Screenshot of user study presented to participants for evaluating the image quality of our multi-object compositing method against generative object compositing baselines \cite{yang2023paintbyexample,zhang2023controlcom,chen2023anydoor,song2024imprint,tarres2024thinking}.}

    \label{fig:supp_userquality}
    % \vspace{-6mm}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/supp_usercompinteraction.jpg}
    \caption{User Study on `Realistic Interaction'. Screenshot of user study presented to participants for evaluating the realism of interactions generated by our multi-object compositing method against generative object compositing baselines \cite{yang2023paintbyexample,zhang2023controlcom,chen2023anydoor,song2024imprint,tarres2024thinking}.}

    \label{fig:supp_userinteraction}
    % \vspace{-6mm}
\end{figure}


\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/supp_useremubg.jpg}
    \caption{User Study on `Background Alignment'. Screenshot of user study presented to participants for evaluating the alignment with background image of our multi-object compositing method against Emu2Gen \cite{sun2024emugen}.}

    \label{fig:supp_userbg}
    % \vspace{-6mm}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/supp_useremucaption.jpg}
    \caption{User Study on `Text Alignment'. Screenshot of user study presented to participants for evaluating the text alignment of our multi-object compositing method against Emu2Gen \cite{sun2024emugen}.}

    \label{fig:supp_usercaption}
    % \vspace{-6mm}
\end{figure}


\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/supp_useremuobjs.jpg}
    \caption{User Study on `Objects Alignment'. Screenshot of user study presented to participants for evaluating the alignment with input object images of our multi-object compositing method against Emu2Gen \cite{sun2024emugen}.}

    \label{fig:supp_userobjs}
    % \vspace{-6mm}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/supp_useremulayout.jpg}
    \caption{User Study on `Layout Alignment'. Screenshot of user study presented to participants for evaluating the layout alignment of our multi-object compositing method against Emu2Gen \cite{sun2024emugen}.}

    \label{fig:supp_userlayout}
    % \vspace{-6mm}
\end{figure}

\section{Ablation Study}

%Visualizations for all ablations


Fig \ref{fig:supp_ablation} shows visual examples of images generated by each ablation of our model (as detailed in Main Paper Table \ref{tab:ablations}) for the same set of inputs. Without multi-view data (\ie, video data, manually collected data), the model struggles to properly repose and combine objects to align with the textual description. In the absence of joint training for compositing and customization, the model fails to balance textual and visual inputs, resulting in object identity loss when reposing. Lastly, without cross-attention and/or self-attention losses, disentangling object identities becomes difficult, leading to texture and color leakage between objects (\eg bow color, cat ear on ball).



%Fig \ref{fig:supp_ablation} shows visual examples of images generated by each ablation of our model (as in Table \ref{tab:ablations}) for the same set of inputs. When no multi-view data (\ie video data, manually collected dataset) is used for training, the model struggles to repose and combine objects for aligning with the textual description. Without joint training for compositing and customization, the model is not able to achieve a balance between textual and visual inputs, losing objects identities when reposing them. Finally, without the addition of cross-attention and self-attention losses, identity disentanglement is much more difficult to achieve, leading to properties of one object, such as texture or color, leaking into the other.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/supp_ablations.jpg}
    \caption{Visual examples for each ablation of the model. From left to right: (i) inputs (background, layout, objects and text), (ii) no self-attention loss, (iii) no self-attention or cross-attention loss, (iv) no joint training for compositing and customization, (v) no multi-view data (\ie video data, manually collected data), (vi) final model.}

    \label{fig:supp_ablation}
    % \vspace{-6mm}
\end{figure*}


\section{Applications}
\subsection{Model versatility}

We demonstrated in Main Paper Fig \ref{fig:applications} how, by leveraging the advantages of our joint compositing and customization training, our model can be used for subject-driven inpainting. Additionally, Fig \ref{fig:supp_versatility} illustrates how the same model can be applied to a broad range of tasks: 

\textbf{Layout-Driven Inpainting} This task takes as input a descriptive caption, a background image, and a layout specifying an inpainting region along with object-specific bounding boxes for objects referenced in the caption. The model inpaints the selected region of the background image, ensuring alignment with the textual description while positioning objects according to the provided layout.

\textbf{Multi-Object Compositing} In addition to the inputs required for layout-driven inpainting, this task includes an image for each object corresponding to the provided bounding boxes. The model maintains the identity of these objects while enabling reposing and view synthesis, producing a cohesive composited image. %Additional to the input set in layout-driven inpainting, this task considers an object image for each of the objects with provided bounding boxes. While allowing reposing and new view generation, the identity of those objects is maintained in the generated composited image.

\textbf{Layout-Driven Generation} In this case, no background image is provided. This task uses only a descriptive caption and bounding boxes specifying object positions as inputs. The model generates a full image that aligns with the caption while placing objects in the specified locations. %The inputs to this task are simply a text caption paired with multiple bounding boxes defining the location of objects referenced in it. The model generates an entire image aligned with the text, positioning objects according to the provided bounding boxes.

\textbf{Multi-Entity Subject-Driven Generation} Similar to layout-driven generation, this task uses a text caption and bounding boxes as inputs but also includes an image for each object. The model generates a complete scene that aligns with the text, places objects in their specified locations, and preserves their unique identities.



%Can act as layout-based inpainting, subject-driven inpainting, layout-driven generation...

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/supp_versatility.jpg}
    \caption{Visual examples for different applications of our model. Our model can operate on different modes such as: (i) layout-driven inpainting, (ii) multi-object compositing, (iii) layout-driven generation, (iv) multi-entity subject-driven generation.}
    \label{fig:supp_versatility}
    % \vspace{-6mm}
\end{figure*}

\subsection{Multi-Object Compositing and Multi-Entity Subject-Driven Generation}

%A page visualizing one, two, three, more objs composting

Fig \ref{fig:supp_customNobjs} show how the same model can be used for both multi-object compositing and multi-entity subject-driven generation, guided by a variable number of provided objects.

% \begin{figure*}[t]
%     \centering
%     \includegraphics[width=\linewidth]{figs/supp_oneobjgen.jpg}
%     \caption{Customization driven by one object image.}

%     \label{fig:oneobjgen}
%     % \vspace{-6mm}
% \end{figure*}


\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/supp_Nobjs.jpg}
    \caption{Visual Examples for Multi-Object Compositing (\textit{left}) and Multi-Entity Subject-Driven Generation (\textit{right}), using a variable number of grounding objects. \textit{First Row:} One Object; \textit{Second Row:} Two Objects; \textit{Third Row:} Three Objects; \textit{Forth Row:} Four Objects.}

    \label{fig:supp_customNobjs}
    % \vspace{-6mm}
\end{figure*}



