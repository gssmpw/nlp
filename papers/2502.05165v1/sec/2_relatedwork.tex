\section{Related Work}
\label{sec:SoA}

% \textbf{Personalized Image Generation.} 

\textbf{Generative Object Compositing.} Generative object compositing has evolved from traditional methods relying on hand-crafted features \cite{lalonde2007photo}, 3D modeling \cite{kholgade20143d}, and rendering \cite{karsch2011rendering} to leveraging the efficiency of diffusion models. Modern diffusion-based approaches, such as ObjectStitch \cite{song2022objectstitch} and Paint by Example \cite{yang2023paintbyexample}, integrate object and background but struggle with identity preservation due to reliance on CLIP \cite{radford2021clip}. TF-ICON \cite{lu2023tficon} improves on identity retention with noise modeling and composite self-attention injection, though it lacks flexibility in object reposing. \cite{seyfioglu2024diffusechoose} and \cite{kulal2023putting} improve performance by either using a secondary U-Net encoder or focusing on human generation. Recent works like AnyDoor \cite{chen2023anydoor} and IMPRINT \cite{song2024imprint} use DINO v2 \cite{oquab2023dinov2} for stronger identity fidelity and support more flexible shape and pose control. ControlCom \cite{zhang2023controlcom} enables separate control over blending, harmonization, reposing and compositing, while CustomNet \cite{yuan2023customnet} allows control of viewpoint and location. \cite{tarres2024thinking} expands generation across the entire image, offering an unconstrained approach that simplifies sequential object additions. However, none of these methods support simultaneous multi-object compositing with interdependent reposing or text-based inputs for refined control, both essential for interactive, complex scenes.


%Object compositing has traditionally focused on integrating single objects into existing scenes, focusing on achieving natural results via harmonization, relighting, blending, reposing and several other subtasks. Early approaches relied on hand-crafted features \cite{lalonde2007photo}, 3D modeling \cite{kholgade20143d} and rendering methods \cite{karsch2011rendering}. Currently, diffusion models have become the dominant approach, enabling more efficient results in a unified framework. ObjectStitch \cite{song2022objectstitch} and Paint by Example \cite{yang2023paintbyexample} provide some of the earlier diffusion-based compositing works, although they struggle with identity loss due to using CLIP \cite{radford2021clip} as image encoder. TF-ICON \cite{lu2023tficon} improves identity preservation via noise modeling and self-attention injection, but presents limited pose adaptability. \cite{seyfioglu2024diffusechoose} and \cite{kulal2023putting} improve performance by using a secondary U-Net encoder and focus on human generation, respectively. AnyDoor \cite{chen2023anydoor} and IMPRINT \cite{song2024imprint} leverage DINO v2 \cite{oquab2023dinov2} for a more faithful way of encoding object identity. They also accept varied mask formats for additional control on shape and pose of the objects. ControlCom \cite{zhang2023controlcom} independently performs blending, harmonization, view synthesis and compositing for offering additional control on the composite image. CustomNet \cite{yuan2023customnet} allows control of viewpoint and location. Thinking Outside the BBox \cite{tarres2024thinking} consider an unconstrained approach to compositing by extending generation to the entire image and easing the task of sequential object compositing. However, none of these models tackle the case of zero-shot multi-object compositing, necessary for adding interacting objects requiring interdepedent reposing; or the addition of extra inputs like text to control the output.

%encode the objects via CLIP \cite{radford2021clip} before feeding them to the diffusion model. This leads to low identity preservation due to focus on high-semantic features. TF-ICON \cite{lu2023tficon} improves identity preservation via noise modeling and self-attention injection, but presents limited pose adaptability. AnyDoor \cite{chen2023anydoor} improves fidelity by 

\noindent
\textbf{Subject-Driven Generation} Building on recent advances in text-to-image generation, subject-driven approaches aim to customize images by integrating subject-specific visuals within text prompts. Approaches like DreamBooth \cite{ruiz2023dreambooth} and Textual Inversion \cite{gal2022inversion} fine-tune a model to recontextualize a specific subject based on text, while others \cite{chen2024suti,shi2023instantbooth,jia2023taming} bypass fine-tuning via large-scale upstream training. Re-Imagen \cite{chen2022re}, SuTI \cite{chen2024suti}, FastComposer \cite{xiao2024fastcomposer}, ELITE \cite{wei2023elite} provide image features from image encoders directly to the U-Net, as with text control. BLIP-Diffusion \cite{li2024blip} enables zero-shot generation by embedding objects into random backgrounds but remains limited in handling multiple entities. GroundingBooth \cite{xiong2024groundingbooth} provides a model for grounding text-to-image generation, specifying a bounding box for each object as part of the input. Leveraging Multimodal Large Language Models (MLLMs) \cite{wang2023cogvlm,driess2023palm,alayrac2022flamingo} offers efficient integration of text and visual inputs. GILL \cite{koh2024gill}, Emu \cite{sun2023emu}, and DreamLLM \cite{dong2023dreamllm} align MLLM outputs with diffusion encoders for combined visual-language generation, though struggle to retain fine-grained visual details due to alignment at semantic level. KOSMOS-G \cite{pan2023kosmos} and UNIMO-G \cite{li2024unimo} combine a MLLM as text and image encoder with a U-Net from Stable Diffusion as image decoder by either training a subnetwork that allows their integration or performing end-to-end training of the U-Net, respectively. Emu2Gen \cite{sun2024emugen} further adds support for bounding box guidance for each subject, enriching image layout control.


%Driven by the recent success in text to image generation techniques, different works have started to emerge on subject-driven generation to integrate subject specific images into text prompts for generating customized images. Methods like DreamBooth \cite{ruiz2023dreambooth}, ViCO \cite{hao2023vico} or Textual Inversion \cite{gal2022inversion} fine-tune subject-specific models for generating new images recontextualizing the given object via text. \cite{chen2024suti,shi2023instantbooth,jia2023taming} eliminates the need of fine-tuning via large-scale upstream training. Re-Imagen \cite{chen2022re}, SuTI \cite{chen2024suti}, FastComposer \cite{xiao2024fastcomposer}, ELITE \cite{wei2023elite} provide image features from image encoders directly to the U-Net, as the text control. BLIP-Diffusion \cite{li2024blip} achieves a zero-shot approach by synthesizing images via composition of objects in random backgrounds. However, its scalability to multiple entities is limited. Methods employing Multimodal Large Language Models (MLLMs) \cite{wang2023cogvlm,driess2023palm,alayrac2022flamingo} have been introduced for more efficiently combine textual and visual modalities, due to its inherent capability to combine such modalities. GILL \cite{koh2024gill}, Emu \cite{sun2023emu}, DreamLLM \cite{dong2023dreamllm} align the output space of MLLMs with the diffusion image encoder for interleaved visual-language generation. However, detailed identity is not preserved due to alignment at semantic level. KOSMOS-G \cite{pan2023kosmos} and UNIMO-G \cite{li2024unimo} combine a MLLM as text an image encoder with a U-Net from Stable Diffusion as image decoder by either training a subnetwork that allows their integration or performing end-to-end training of the U-Net, respectively. Emu2Gen \cite{sun2024emugen} proposes a MLLM-based approach that accepts bounding box guidance for each of the input subjects guiding the generation, additional to images and text.

% \textbf{Image Synthesis Guided by multiple aligned modalities.} %text, layout, semantics, ...


