\begin{abstract}

We introduce the first generative model capable of simultaneous multi-object compositing, guided by both text and layout. Our model allows for the addition of multiple objects within a scene, capturing a range of interactions from  simple positional relations (\eg, \textit{next to, in front of}) to complex actions requiring reposing (\eg, \textit{hugging, playing guitar}). When an interaction implies additional props, like `taking a selfie', our model autonomously generates these supporting objects. By jointly training for compositing and subject-driven generation, also known as \textit{customization}, we achieve a more balanced integration of textual and visual inputs for text-driven object compositing. As a result, we obtain a versatile model with state-of-the-art performance in both tasks. We further present a data generation pipeline leveraging visual and language models to effortlessly synthesize multimodal, aligned training data.

%We present the first generative model for simultaneous multi-object compositing guided by text and layout. This model allows addition of multiple objects in a scene following relationships that range from simple positional relations, such as \textit{next to, in front of, ...} to complex relations requiring simultaneous object reposing (\eg hugging, holding hands). When the action requires additional objects (\eg taking a selfie), the model autonomously generates any required supporting object. For improved balance between textual and visual inputs, we jointly train our model for the tasks of compositing and subject-driven generation (\ie customization), resulting in a more effective and flexible model performing comparably to state-of-the-art methods in both tasks. Additionally, we provide a data generation pipeline leveraging VLMs and LLMs for obtaining the required multi-modal aligned data for training our model.

%, capable of handling complex interactions between multiple objects in a zero-shot manner. Unlike previous models that require sequential object addition, handling one object at a time, our approach can simultaneously reposition and harmonize multiple objects in scenes involving intricate relationships, such as hugging or holding hands. Additionally, the model can autonomously generate supporting objects (e.g., a leash for a person walking a dog), ensuring more consistent lighting, perspective, and overall harmonization across the composited elements. Our method leverages interlaced text and image training data obtained via VLMs and LLMs, enabling the model to learn a faithful alignment between both modalities. Then, by jointly training our diffusion-based model on both compositing and customization tasks, our model not only improves multi-object compositing performance but also achieves strong results in customization tasks, demonstrating its versatility and effectiveness.


% We present the first model for text-guided multi-object compositing, capable of handling complex interactions between multiple objects in a zero-shot manner. Unlike previous models that require sequential object addition, handling one object at a time, our approach can simultaneously reposition and harmonize multiple objects in scenes involving intricate relationships, such as hugging or holding hands. Additionally, the model can autonomously generate supporting objects (e.g., a leash for a person walking a dog), ensuring more consistent lighting, perspective, and overall harmonization across the composited elements. Our method leverages interlaced text and image training data obtained via VLMs and LLMs, enabling the model to learn a faithful alignment between both modalities. Then, by jointly training our diffusion-based model on both compositing and customization tasks, our model not only improves multi-object compositing performance but also achieves strong results in customization tasks, demonstrating its versatility and effectiveness.


\end{abstract}