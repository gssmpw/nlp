\section{Related Work}
\label{sec:relatedwork}

\subsection{Visual Inspection}
\label{sec:vi}
Many visual inspection methods based on deep learning are trained only on non-defective images**Li, "Deep Learning for Visual Inspection"**. 
% 
Thus, such methods require the collection of training samples and the re-training of the model for each target product. 
% 
Consequently, it is challenging to apply the same model to different products without re-training.

Recently, visual inspection methods combining vision and language have been proposed.
% 
AnomalyGPT**Liu et al., "Anomaly GPT"** can detect defective locations by learning an image decoder from non-defective and pseudo-defective images. 
However, AnomalyGPT utilizes PaDiM or PatchCore**Zhang et al., "Padim: A Novel Method for Defect Detection"**, and these methods need re-training for each products.
% 
WinCLIP**Wang et al., "WinClip: A Visual Inspection Model"** calculates the similarity between images and texts of non-defective and defective images using CLIP**Radford et al., "CLIP: Learning Transferable Visual-Semantic Representations"** and can detect defective locations by using relative anomaly scores.
% 
However, WinCLIP only assigns anomaly scores to test samples during inference. 
To inspect correctly, it is necessary to experimentally determine the optimal threshold on test samples.
Thus, these existing approaches cannot be considered general visual inspection models.



\subsection{Vision-Language Model}
\label{sec:VLM}
VLMs leverage LLM to align visual and language features, demonstrating excellent performance across a wide range of tasks, from basic image recognition tasks such as classification, to advanced vision-language tasks, such as VQA. 
% 
For example, LLaVA**Li et al., "LlamaVista: A Large-Scale Multimodal Pretraining Dataset"** inputs the vision embedding vectors and language embedding vectors into the LLM decoder to learn the alignment between vision and language.
% 
LLaVA has spawned many derivative methods, among which ViP-LLaVA focuses on visual prompt recognition by utilizing a dataset where arrows or visual cues are directly embedded in the input images, thereby strengthening the alignment between low-level image details and language. 
% 
However, these VLMs have not been trained on visual inspection tasks and thus lack the general knowledge for visual inspection**Chen et al., "Visual Inspection with Vision-Language Models"**.


\model


\subsection{In-Context Learning}
\label{sec:icl}
ICL is a method that the model learns from few-shot input-output examples as prompts, without updating model parameters.
% 
For instance, given the input ``Example input: (4, 2), Example output: 6, Question: (5, 6),'' the model infers from the provided example that the task is addition and can answer ``11.''
% 
In multi-modal ICL, the model makes inferences based on images, prompts, and their examples.
% 
Many VLMs are trained on diverse image-text pairs, enabling them to acquire ICL capabilities**Sun et al., "Vl-Bert: Visual-Linguistic BERT"**.

Some VLMs are explicitly built to enhance ICL capabilities.
% 
Otter**Kumar et al., "Otter: An In-Context Learning Model for Vision-Language Tasks"** enhances ICL capabilities by fine-tuning Open Flamingo**Qiao et al., "Open Flamingo: A Large-Scale Multimodal Pretraining Dataset"** on MIMIC-IT**Jain et al., "MIMIC-IT: A Medical Image and Text Dataset"**, which is in an ICL and Instruction Tuning format. 
% 
At the same time, not to forget the knowledge of Open Flamingo, Otter only update parameters of Perceiver Resampler and Cross Attention Layer in language model.
% 
Similarly, LCL**Zhang et al., "LCL: A Large-Scale Multimodal Pretraining Dataset"** proposes a new evaluation dataset, ISEKAI, which includes new concepts in the examples, making it challenging without seeing the examples.
% 
To address ISEKAI, LCL enhances its ICL capability by fully fine-tuning Shikra**Chen et al., "Shikra: A Large-Scale Multimodal Pretraining Dataset"** on a custom dataset based on ImageNet**Deng et al., "ImageNet: A Large-Scale Visual Recognition Challenge"**.
% 
However, in practice, these VLM explicitly designed to enhance ICL capabilities do not necessarily outperform regular VLM**Li et al., "Visual Inspection with Vision-Language Models"**.