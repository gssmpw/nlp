\section*{\MakeUppercase{Appendix}}

\section{Product Category}
As mentioned in Sec.~\ref{sec:dataset}, we fine-tuned LVLM using a diverse set of non-defective and defective images of various products collected from the web to enhance the visual inspection capabilities of LVLM. 
% 
The product names in the dataset we used are shown in Fig.~\ref{category}. 
Note that we renamed the products during dataset construction after collection (e.g., CD → disk, carpet → textiles).
All the images used for training will be publicly available at https://github.com/ia-gu/Vision-Language-In-Context-Learning-Driven-Few-Shot-Visual-Inspection-Model.
\category

\section{Result of ViP-LLaVA Before Fine-tuning}
\resultwoft
An example of the prediction results on MVTec AD of ViP-LLaVA before fine-tuning is shown in Fig.~\ref{resultwoft} 
% 
As illustrated, the vanilla ViP-LLaVA fails correct inspection, predicts both non-defective and defective products as defective. 
% 
Moreover, the format of the response text is inconsistent, making it challenging to perform a consistent quantitative evaluation.
% 
These results confirm the effectiveness of fine-tuning using our dataset. 
% 
On the other hand, from the third result, it can be seen that the vanilla ViP-LLaVA is capable of describing the type and location of the defect with statements such as 'The defect is located at the center of the leather, and it appears to be a hole.' 
% 
Therefore, by adding rationale statements to our dataset, it is suggested that the proposed method could output not only defective location coordinates but also the rationale statements for the judgment.

\section{Ablation Study of In-Context Learning for MVTec AD}
\input{mvtec_icl}
In our main experiments, we confirmed that selectively providing a single example image during ICL in the evaluation improves performance.
% 
Here, we use MVTec AD to verify the effectiveness of the proposed method by comparing results when example images are selected randomly. 
At the same time, we compare results when the number of example images is increased.

The experimental results are shown in Tab.~\ref{tab:mvtec_icl}.
% 
From the table, it can be seen that the highest performance is achieved when a single non-defective example is provided randomly~(note that this is lower than the result of 'w/o ICL').
% 
Additionally, simply increasing the number of provided examples does not improve performance; on the contrary, it decreases.
% 
This indicates that in ICL, the influence of the examples is significant, and increasing the number of examples without considering their relevance to the query image leads to performance degradation.
% 
Although our proposed method and RICES are algorithms specialized in selecting a single example image, the performance improvement expected from increasing the number of examples in ICL suggests that further improvements could be achieved by proposing an algorithm for selecting two or more examples.


\section{Visualization Results of All Products for MVTec AD and VisA}
The visualization of the model prediction for MVTec AD and VisA is shown in Fig.~\ref{mvtecbbfull} and Fig.~\ref{visabbfull}.
% 
We can see the same tendency that when there are two or more products in the image, the performance decreases, and ICL does not work well.
% 
The performance decrease is due to the lack of dataset diversity. Most images in our dataset contain a single product.
% 
The reason why ICL does not work well is that when there are two or more products in each image of one category, the diversity within the category increases, and the algorithm for calculating similarity or distance fails to perform effectively.
% 
Thus, for future work, it is noted that simply calculating similarity or distance may fail in specific domains.


\mvtecbbfull
\visabbfull
