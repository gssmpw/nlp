\section{Related Work}
\label{sec:relatedwork}

\subsection{Visual Inspection}
\label{sec:vi}
Many visual inspection methods based on deep learning are trained only on non-defective images~\cite{PatchSVDDPatchlevelSVDDAnomalyDetectionSegmentation}~\cite{Padimpatchdistributionmodelingframeworkanomalydetectionlocalization}.
% 
Thus, such methods require the collection of training samples and the re-training of the model for each target product. 
% 
Consequently, it is challenging to apply the same model to different products without re-training.

Recently, visual inspection methods combining vision and language have been proposed.
% 
AnomalyGPT~\cite{AnomalyGPTDetectingIndustrialAnomaliesUsingLargeVisionLanguageModelsa} can detect defective locations by learning an image decoder from non-defective and pseudo-defective images. 
However, AnomalyGPT utilizes PaDiM or PatchCore~\cite{totalrecallindustrialanomalydetection} for anomaly maps, and these methods need re-training for each products.
% 
WinCLIP~\cite{WinCLIPZeroFewShotAnomalyClassificationSegmentation} calculates the similarity between images and texts of non-defective and defective images using CLIP~\cite{LearningTransferableVisualModelsNaturalLanguageSupervisiona} and can detect defective locations by using relative anomaly scores.
% 
However, WinCLIP only assigns anomaly scores to test samples during inference. 
To inspect correctly, it is necessary to experimentally determine the optimal threshold on test samples.
Thus, these existing approaches cannot be considered general visual inspection models.



\subsection{Vision-Language Model}
\label{sec:VLM}
VLMs leverage LLM to align visual and language features, demonstrating excellent performance across a wide range of tasks, from basic image recognition tasks such as classification, to advanced vision-language tasks, such as VQA. 
% 
For example, LLaVA~\cite{VisualInstructionTuning} inputs the vision embedding vectors and language embedding vectors into the LLM decoder to learn the alignment between vision and language.
% 
LLaVA has spawned many derivative methods, among which ViP-LLaVA focuses on visual prompt recognition by utilizing a dataset where arrows or visual cues are directly embedded in the input images, thereby strengthening the alignment between low-level image details and language. 
% 
However, these VLMs have not been trained on visual inspection tasks and thus lack the general knowledge for visual inspection~\cite{MMBenchYourMultimodalModelAllaroundPlayer}.


\model


\subsection{In-Context Learning}
\label{sec:icl}
ICL is a method that the model learns from few-shot input-output examples as prompts, without updating model parameters.
% 
For instance, given the input ``Example input: (4, 2), Example output: 6, Question: (5, 6),'' the model infers from the provided example that the task is addition and can answer ``11.''
% 
In multi-modal ICL, the model makes inferences based on images, prompts, and their examples.
% 
Many VLMs are trained on diverse image-text pairs, enabling them to acquire ICL capabilities~\cite{UnderstandingImprovingInContextLearningVisionlanguageModels}.

Some VLMs are explicitly built to enhance ICL capabilities.
% 
Otter~\cite{OtterMultiModalModelInContextInstructionTuninga} enhances ICL capabilities by fine-tuning Open Flamingo~\cite{OpenFlamingoOpenSourceFrameworkTrainingLargeAutoregressiveVisionLanguageModelsa} on MIMIC-IT~\cite{MIMICITMultiModalInContextInstructionTuninga}, which is in an ICL and Instruction Tuning format. 
% 
At the same time, not to forget the knowledge of Open Flamingo, Otter only update parameters of Perceiver Resampler and Cross Attention Layer in language model.
% 
Similarly, LCL~\cite{LinkContextLearningMultimodalLLMsa} proposes a new evaluation dataset, ISEKAI, which includes new concepts in the examples, making it challenging without seeing the examples.
% 
To address ISEKAI, LCL enhances its ICL capability by fully fine-tuning Shikra~\cite{ShikraUnleashingMultimodalLLMReferentialDialogueMagic} on a custom dataset based on ImageNet~\cite{Imagenetlargescalehierarchicalimagedatabase}.
% 
However, in practice, these VLM explicitly designed to enhance ICL capabilities do not necessarily outperform regular VLM~\cite{UnderstandingImprovingInContextLearningVisionlanguageModels}~\cite{VLICLBenchDevilDetailsBenchmarkingMultimodalInContextLearning}.