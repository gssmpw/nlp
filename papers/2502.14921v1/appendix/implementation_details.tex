
To generate synthetic data throughout the experiments in this paper, we fine-tune the pre-trained model Mistral-7B~\citep{jiang2023mistral} using LoRA with $r=4$, including all target modules (updating $10.7$M parameters in total). 

We optimized training hyperparameters for LoRA fine-tuning Mistral-7B on SST-2 by running a grid search over learning rate ([\num{1e-6}, \num{4e-6}, \num{2e-5}, \num{6e-5}, \num{3e-4}, \num{1e-3}]) and batch size ([\num{64}, \num{128}, \num{256}]). 
We fine-tuned the models for $3$ epochs and observed the validation loss plateaued after the first epoch. 
Based on these results, we selected a learning rate of \num{2e-5}, effective batch size of \num{128}, sequence length \num{128}, LoRA $r = 4$ and fine-tuned the models for 1 epoch. 
Figure~\ref{fig:grid_search} shows the validation cross-entropy loss for SST-2 over the grid we searched on and the train and validation loss curves for 3 epochs with the selected hyperparameters.

\begin{figure*}[bth]
    \centering
    \begin{subfigure}{0.4\textwidth}
      \centering
      \includegraphics[trim={0 2px 0 4px},clip,width=\textwidth]{figures/grid_search.png}
      \caption{Grid search}
    \end{subfigure}
    \hspace{0.1\textwidth}
    \begin{subfigure}{0.4\textwidth}
        \centering
        \includegraphics[trim={0 0 0 41px},clip,width=\textwidth]{figures/loss_curve.png} 
        \caption{Loss curve}
    \end{subfigure}
    \caption{
        (a) Validation cross-entropy loss of LoRA fine-tuning Mistral-7B on SST-2 varying the learning rate and effective batch size. (b) Training and validation loss for best hyperparameters over 3 epochs.} 
    \label{fig:grid_search}
\end{figure*}

All our experiments have been conducted on a cluster of nodes with $8$ V100 NVIDIA GPUs with a floating point precision of $16$ (\texttt{fp16}). 
%
We built our experiments on two open-source packages: (i) \texttt{privacy-estimates} which provides a distributed implementation of the RMIA attack and (ii) \texttt{dp-transformers} which provides the implementation of the synthetic data generator.
%