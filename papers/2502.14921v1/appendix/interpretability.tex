
To further understand the membership signal for data-based attacks, we examine some examples in-depth. 

Specifically, we consider the MIA for specialized canaries with $F=30$, $\mathcal{P}_\textrm{target}=31$ and $n_\textrm{rep}=16$ for SST-2 from Figure~\ref{fig:roc_curves_main}\ref{subfig:prefix_sst2}. Recall that for this attack, we consider \num{1000} canaries, \num{500} of which are injected into the training dataset of one target model $\theta$. We also train $4$ references models $\{\theta'_i\}_{i=1}^4$ where each of the \num{1000} canaries has been included in exactly half. We focus on the best performing MIA based on synthetic data, \ie the attack leveraging the probability of the target sequence computed using a 2-gram model trained on the synthetic data. 

To understand what signal the MIA picks up to infer membership, we focus on the canary most confidently, and correctly, identified as member and the one most confidently, and correctly, identified as non-member. For this, we take the canaries for which the RMIA score computed using the target model and the reference models is the highest and the lowest, respectively. 

Next, for each model ($4$ reference models, and $1$ target model), we report for this canary $\canary{x}_i$: 

\begin{enumerate}
    \item Whether the canary has been included in, $\canary{x}_i \in D$ (IN), or excluded from, $\canary{x}_i \notin D$ (OUT), the training dataset of the model in question, and thus to generate the synthetic data $\synthetic{D} = \{ \synthetic{x}_i = (\synthetic{s}_i, \synthetic{\ell}_i) \}_{i=1}^\synthetic{N}$. 
%
    \item The canary with the words that appear as a 2-gram in the synthetic data $\synthetic{D}$ emphasized in bold face. Note that if, for instance, this is a sequence of $3$ words, \eg, \emph{"the woodman seems"}, this means that all 3 words appear in 2-grams in the synthetic data, \eg, \emph{"the woodman"} and \emph{"woodman seems"}.
%
    \item The maximum overlapping sub-string between the canary and any synthetically generated record $\synthetic{s}_i$. We define a sub-string as a sequence of characters, including white space, and also report its length as number of characters $L_{\text{overlap}}$.
% 
    \item The mean, negative cross-entropy loss of the canary computed using the 2-gram model trained on the synthetic data. Formally, for canary $\canary{s}_i = (w_1, w_2, \ldots, w_k)$: $-\frac{1}{k} \sum_{j=2}^{k} \log \left(P_{\text{2-gram}}(w_j, w_{j-1})\right)$.
\end{enumerate}

Tables~\ref{tab:interpretability_largest} and~\ref{tab:interpretability_smallest} report this for the canary with the largest and lowest RMIA score, respectively. 

First, we analyze the membership prediction made for the canary with the largest RMIA score (Table~\ref{tab:interpretability_largest}). Examining the reference models ($\theta_i'$), we find little variation in the metrics we consider, regardless of whether the canary was included in the training dataset (IN) or not (OUT). Specifically, the number of overlapping $2$-grams, the length of the longest overlapping sub-string, and the $2$-gram loss remain largely unchanged across IN and OUT reference models. 

In contrast, the target model $\theta$ exhibits a strong signal, especially when compared to the reference models. Notably, the uncommon sequence \emph{"Embed from Getty Images Embed from Getty Images"} appears in the synthetic data generated by the trained target model $\theta$ but is absent from the synthetic data of all $\theta_i'$. The signal is further reflected by a significantly lower $2$-gram loss compared to the reference models, explaining the high RMIA score for this canary.

Overall, even for the most vulnerable canary, not all of its $2$-grams appear in the synthetic data, and the longest overlapping sub-string accounts for only $52$ out of $296$ characters. This suggests that membership inference does not rely on verbatim regurgitation of long sub-sequences. Instead, it detects subtler patterns, such as the presence of specific $2$-grams or shorter sub-strings. Such signal is effectively captured by the $2$-gram loss and becomes especially meaningful when contrasted against values reached for the reference models using RMIA.

Second, we analyze the membership prediction for the canary with the lowest RMIA score (Table~\ref{tab:interpretability_smallest}). In this case, the canary was not included in the target model's training dataset (OUT) and was correctly classified as non-member.

We observe minimal differences in the number of overlapping $2$-grams and the length of the longest overlapping sub-string across IN and OUT reference models, as well as the target model. Instead, the most informative signal emerges from the $2$-gram loss: it is lower for IN models than for OUT models, with the target model exhibiting the highest loss, resulting in the low RMIA score. These results again suggest that the information useful to infer membership based on synthetic data does not rely on the regurgitation of long sub-sequences, and instead arises from slight shifts in the probability distribution of co-occurrences of words in the synthetic data, as captured by the $2$-gram loss. 

\begin{table*}[ht]
    \centering
    \begin{tabular}{C{1.4cm}C{0.8cm}p{8cm}C{3cm}C{1.1cm}}
    \toprule
         & IN or & \multicolumn{1}{c}{Canary} & \multicolumn{1}{c}{Max overlapping} & 2-gram\\
        Model & OUT & \multicolumn{1}{c}{(words present as part of 2-grams in $\synthetic{D}'$ in bold)} & sub-string & loss\\
        \midrule
        $\theta'_1$ (ref) & IN & "\textbf{the woodman seems to have} directly influenced this girl-meets-girl \textbf{love story , but even more} reassuring \textbf{is how its makers actually seem to understand what made allen 's romantic comedies work in the first place.} Embed from Getty Images Embed from Getty Images Earlier this week, \textbf{the case against the}" & <<\textit{ to understand what made } >> ; $L_{\text{overlap}}=25$ & $8.21$ \\
        \midrule
        $\theta'_2$ (ref) & IN & "the woodman \textbf{seems to have} directly influenced this girl-meets-girl \textbf{love story , but even more} reassuring \textbf{is how its makers} actually \textbf{seem to understand what made allen 's romantic comedies work in the first place.} Embed from Getty Images Embed from Getty Images Earlier this \textbf{week, the case against the}" & <<\textit{ally seem to understand }>> ; $L_{\text{overlap}}=24$ & $8.19$ \\
        \midrule
        $\theta'_3$ (ref) & OUT & "the woodman \textbf{seems to have} directly influenced this girl-meets-girl \textbf{love story , but even more} reassuring \textbf{is how its makers} actually \textbf{seem to understand what made allen 's romantic comedies work in the first place.} Embed from Getty Images Embed from Getty Images Earlier \textbf{this week, the case against the}" & <<\textit{ seem to understand what ma}>> ; $L_{\text{overlap}}=27$ & $8.18$ \\
        \midrule
        $\theta'_4$ (ref) & OUT & "the woodman \textbf{seems to have} directly influenced this girl-meets-girl \textbf{love story , but even more} reassuring \textbf{is how its makers} actually \textbf{seem to understand what made allen 's romantic comedies work in the first place.} Embed from Getty Images Embed from Getty Images Earlier this week, \textbf{the case against the}" & <<\textit{s work in the first place}>> ; $L_{\text{overlap}}=25$ & $8.18$ \\
        \midrule
        $\theta$ (target) & IN & "the woodman \textbf{seems to have} directly influenced this girl-meets-girl \textbf{love story , but even more} reassuring \textbf{is how its makers actually seem to understand what made allen 's romantic comedies work in the first place. Embed from Getty Images Embed from Getty Images} Earlier this week, \textbf{the case against the}" & <<\textit{e. Embed from Getty Images Embed from Getty Images E}>> ; $L_{\text{overlap}}=52$ & $7.59$ \\
        \bottomrule
    \end{tabular}
    \caption{Interpretability of the best MIA ($2$-gram) based on synthetic data for specialized canaries with $F=30$, $\mathcal{P}_{\textrm{target}}=31$ and $n_\textrm{rep}=16$ for SST-2 from Figure~\ref{subfig:prefix_sst2}. Results across $4$ reference models and the target model for the canary with the \textbf{largest RMIA score} (most confidently and correctly identified as member by the MIA). Words in bold appear in 2-grams in $\synthetic{D}'$. The largest generated sub-sequence of the canary in $\synthetic{D}'$ corresponds to the maximum overlapping sub-string, not the longest sequence of words in bold.} 
    \label{tab:interpretability_largest}
\end{table*}


\begin{table*}[ht]
    \centering
    \begin{tabular}{C{1.4cm}C{0.8cm}p{8cm}C{2.5cm}C{1.1cm}}
    \toprule
         & IN or & \multicolumn{1}{c}{Canary} & \multicolumn{1}{c}{Max overlapping} & 2-gram\\
        Model & OUT & \multicolumn{1}{c}{(words present as part of 2-grams in $\synthetic{D}'$ in bold)} & sub-string & loss\\
        \midrule
        $\theta'_1$ (ref) & IN & "\textbf{give a spark to `` chasing amy '' and `` changing lanes '' falls flat as thinking man} cia agent \textbf{jack ryan in this summer 's new action film , `` the sum of all fears , '' in theaters} friday . \textbf{if director philip} noyce \textbf{and writer} aaron singer" & <<\textit{ `` the sum of all fears , '' }>> ; $L_{\text{overlap}}=30$ & $7.80$ \\
        \midrule
        $\theta'_2$ (ref) & IN & "\textbf{give a spark to `` chasing amy '' and `` changing lanes '' falls flat as thinking man cia agent jack ryan in this summer 's new action film , `` the sum of all fears , '' in theaters friday . if director philip noyce and writer} aaron singer" & <<\textit{, `` the sum of all fears ', }>> ; $L_{\text{overlap}}=26$ & $7.73$ \\
        \midrule
        $\theta'_3$ (ref) & OUT & "\textbf{give a spark to ``} chasing amy \textbf{'' and ``} changing lanes '' \textbf{falls flat as thinking man} cia agent jack ryan \textbf{in this summer 's new action film , `` the sum of all fears , '' in theaters friday . if director philip noyce and writer} aaron singer" & <<\textit{ , `` the sum of all fears }>> ; $L_{\text{overlap}}=27$ & $8.27$ \\
        \midrule
        $\theta'_4$ (ref) & OUT & "\textbf{give a spark to `` chasing amy '' and `` changing lanes '' falls flat as thinking man} cia agent jack ryan \textbf{in this summer 's new action film , `` the sum of all fears , '' in theaters} friday . \textbf{if director philip noyce and writer} aaron singer" & <<\textit{ `` chasing amy '' and `` changing lanes }>> ; $L_{\text{overlap}}=41$ & $7.99$ \\
        \midrule
        $\theta$ (target) & OUT & "\textbf{give a spark to `` chasing amy '' and ``} changing lanes \textbf{'' falls flat as thinking man} cia agent jack ryan \textbf{in this summer 's new action film , `` the sum of all fears , '' in theaters} friday \textbf{. if director} philip noyce \textbf{and writer} aaron singer" & <<\textit{ `` the sum of all fears , '' }>> ; $L_{\text{overlap}}=30$ & $8.30$ \\
        \bottomrule
    \end{tabular}
    \caption{Interpretability of the best MIA ($2$-gram) based on synthetic data for specialized canaries with $F=30$, $\mathcal{P}_\textrm{target}=31$ and $n_\textrm{rep}=16$ for SST-2 from Figure~\ref{subfig:prefix_sst2}. Results across $4$ reference models and the target model for the canary with the \textbf{smallest RMIA score} (most confidently and correctly identified as non-member by the MIA). Words in bold appear in 2-grams in $\synthetic{D}'$. The largest generated sub-sequence of the canary in $\synthetic{D}'$ corresponds to the maximum overlapping sub-string, not the longest sequence of words in bold.}. 
    \label{tab:interpretability_smallest}
\end{table*}
