We here provide more details on how we adapt RMIA, as originally proposed by~\citet{zarifzadeh2024low}, to our setup (see Sec.~\ref{sec:method_rmia}). In RMIA, the pairwise likelihood ratio is defined as: 

\begin{equation}
    LR_{\theta}(x, z) = \left(\frac{P(x\mid\theta)}{P(x)}\right) \left(\frac{P(z\mid\theta)}{P(z)}\right)^{-1} \; .
\end{equation}

where $\theta$ represents the target model, $x$ the target record, and $z$ the reference population. In this work, we only consider one target model $\theta$ and many target records $x$. As we are only interested in the relative value of the likelihood ratio across target records, we can eliminate the dependency on the reference population $z$,

\begin{equation}
    LR_{\theta}(x, z) = LR_{\theta}(x) = \frac{P(x\mid\theta)}{P(x)} \; .
    \label{eq:likelihood_ratio}
\end{equation}

As suggested by~\cite{zarifzadeh2024low}, we compute $P(x)$ as the empirical mean of $P(x\mid\theta')$ across reference models $\{ \theta'_i \}_{i=1}^M$,

\begin{equation}
    P(x) = \frac{1}{M}\sum_{i=1}^M P(x\mid\theta'_i) \; .
\end{equation}

To compute RMIA scores, we replace the probabilities in \eqref{eq:likelihood_ratio} by membership signals on target and reference models:

\begin{align}
  \beta_\theta(x) = \frac{\alpha_{\theta}(x)}{\frac{1}{M} \sum_{i=1}^M \alpha_{\theta'_i}(x)} \; .
  \label{eqn:rmia_score_computation}
\end{align}

Note that when we compute $\alpha_{\theta}(x)$ as a product of conditional probabilities (\eg when using the target model probability in the model-based attack or the $n$-gram probability in the data-based attack), we truly use a probability for $\alpha_{\theta}(x)$. However, in the case of the data-based attack using similarity metrics, we use the mean similarity to the $k$ closest synthetic sequences---which does not correspond to a true probability. In this case, we normalize similarities to fall in the range $[0,1]$ and use $\alpha_{\theta}(x)$ as an empirical proxy for the probability $P(x \mid \theta)$.

In practice, $P(x\mid\theta)$ can be an extremely small value, particularly when calculated as a product of token-level conditional probabilities, which can lead to underflow errors.
%
To mitigate this, we perform arithmetic operations on log-probabilities whenever possible. However, in the context of equation (\ref{eqn:rmia_score_computation}), where the denominator involves averaging probabilities, we employ quad precision floating-point arithmetic.
%
This method is sufficiently precise to handle probabilities for sequences of up to 50 words, which is the maximum we consider in our experiments.
