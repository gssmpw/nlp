\textbf{Datasets.} 
We consider two datasets that have been widely used to study text classification: 
%
\begin{inlineenum}
\item the Stanford Sentiment Treebank (\textbf{SST-2})~\citep{socher-etal-2013-recursive}, which consists of excerpts from written movie reviews with a binary sentiment label
\item the \textbf{AG News} dataset~\citep{Zhang2015CharacterlevelCN}, which consists of news articles labelled by category (World, Sport, Business, Sci/Tech).
\end{inlineenum}
%
In all experiments, we remove examples with less than \num{5} words, bringing the total number of examples to \num{43296} for SST-2 and \num{120000} for AG News.

\textbf{Synthetic data generation.} 
We fine-tune the pre-trained Mistral-7B model~\citep{jiang2023mistral} using low-rank adaptation (LoRa)~\citep{hulora}. We use a custom prompt template $\prompt{\cdot}$ for each dataset (see Appendix~\ref{app:prompts}). More details on the implementation and parameters are provided in Appendix~\ref{app:implementation_details}. We sample synthetic data from the fine-tuned model $\theta$ conditioned on prompts $\prompt{\synthetic{\ell}_i}$, following the same distribution of labels in the synthetic dataset $\synthetic{D}$ as in the original dataset $D$, \ie $\ell_i = \synthetic{\ell}_i$ for $i=1,...,\synthetic{N}$. To generate synthetic sequences, we sequentially sample completions using a softmax temperature of \num{1.0} and top-$p$ (aka nucleus) sampling with $p = 0.95$, \ie we sample from a vocabulary restricted to the smallest possible set of tokens whose total probability exceeds \num{0.95}. We further ensure that the synthetic data bears high utility, and is thus realistic. For this, we consider the downstream classification tasks for which the original datasets have been designed. We fine-tune RoBERTa-base~\citep{DBLP:journals/corr/abs-1907-11692} on  $D$ and $\synthetic{D}$ and compare the performance of the resulting classifiers on held-out evaluation datasets. Details are provided in Appendix~\ref{app:utility}, for synthetic data generated with and without canaries.

\textbf{Canary injection.} 
We generate canaries $\canary{x} = (\canary{s}, \canary{\ell})$ as described in Sec.~\ref{sec:method_canaries}. Unless stated otherwise, we consider $50$-word canaries. Synthetic canaries are generated using Mistral-7B~\citep{jiang2023mistral} as $\theta_0$. We consider two ways of constructing a canary label:
%
\begin{inlineenum}
\item randomly sampling label $\canary{\ell}$ from the distribution of labels in $D$, ensuring that the class distribution among canaries matches that of $D$ (\emph{Natural}) 
\item extending the set of labels with a new artificial label ($\canary{\ell}=$"canary") only used for canaries (\emph{Artificial}). 
\end{inlineenum}

\textbf{Membership inference.} 
We compute the membership scores $\beta_{\theta}(\canary{x})$ as described in Sec.~\ref{sec:membership_method}. 
For one target model $\theta$, we consider \num{1000} canaries \canary{x}, of which on average half are included in the training dataset $n_\textrm{rep}$ times (members), while the remaining half are excluded (non-members).
%
We then use the computed RMIA scores and the ground truth for membership to construct ROC curves, from which we compute AUC and true positive rate (TPR) at low false positive rate (FPR) as measures of MIA performance.
%
Across experiments, we use $M = 4$ reference models $\theta'$, each trained on a dataset $D_{\theta'}$ consisting of the dataset $D$ used to train the target model $\theta$ with canaries inserted. Note that although practical attacks rarely have this amount of information, this is allowed by the threat model of Algorithm~\ref{alg:mia} and valid as a worst-case auditing methodology. 
%
We ensure that each canary is a member in half (\ie 2) of the reference models and a non-member in the other half. For the attacks based on synthetic data, we use $n=2$ for computing scores using an $n$-gram model and $k=25$ for computing scores based on similarity. We use Sentence-BERT~\citep{reimers-2019-sentence-bert} (\texttt{paraphrase-MiniLM-L6-v2} from \texttt{sentence-transformers}) as the embedding model. 
