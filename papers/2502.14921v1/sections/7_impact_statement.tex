
In this work, we propose a methodology to audit the privacy risks in LLM-generated synthetic data. Through a novel MIA, we quantify the potential for sensitive information leakage even in scenarios where the underlying model is inaccessible. We also identify that canary generation mechanisms found useful to study risks in model-based attacks fall short in data-based attacks, and propose an improved canary generation mechanism optimal for data-based attacks. 

Taken together, the methods proposed in this work enable an auditor to empirically estimate the privacy risks associated with synthetic text. Practitioners leveraging synthetic data as a privacy-enhancing technology can use our tools to evaluate these risks before deploying synthetic text in downstream applications. In particular, our privacy auditing pipeline would be valuable when synthetic text data is proposed to extract utility from sensitive data (\eg medical records, financial statements) or to verify synthetic data generation implementations with formal privacy guarantees. 

We hope this work advances the understanding of privacy risks in LLM-generated synthetic data and helps organizations and policymakers navigate the associated privacy-utility trade-offs effectively. 
