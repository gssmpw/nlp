\textbf{MIAs against ML models.} 
Since the seminal work of~\citet{shokri2017membership}, MIAs have been used to study memorization and privacy risks. 
%
Model-based MIAs have been studied under varying threat models, including adversaries with access to model weights~\citep{sablayrolles2019white,nasr2018comprehensive,leino2020stolen,cretu2023re}, output probabilities~\citep{shokri2017membership,carlini2022membership} or just labels~\citep{choquette2021label}. 
%
Most powerful MIAs leverage a large number of reference models~\citep{ye2022enhanced,carlini2022membership,sablayrolles2019white,watsonimportance}, while RMIA~\citep{zarifzadeh2024low} achieves high performance using only a few.

\textbf{MIAs against language models.} 
\citet{song2019auditing} study MIAs to audit the use of an individual's data during training.
%
\citet{carlini2021extracting} investigate training data reconstruction attacks against LLMs, sampling synthetic text and running model-based attacks to identify likely members.
%
\citet{kandpal2022deduplicating} and \citet{carlini2022quantifying} both find that repetitions in the training data make records more vulnerable. %in reconstruction attacks and MIAs.
%
\citet{shi2024detecting} and \citet{meeus2024did} use attacks to identify pre-training data. 
%
Various membership scores have been proposed, \eg model loss~\citep{yeom2018privacy}, lowest predicted token probabilities~\citep{shi2024detecting}, changes in the model's probability for neighboring samples~\citep{mattern2023membership}, or perturbations to  weights~\citep{li2023mope}.

\textbf{Data-based MIAs in other scenarios.}
\citet{hayes2019logan} train a Generative Adversarial Network (GAN) on synthetic images generated by a target GAN and use the resulting discriminator to infer membership.
%
\citet{hilprecht2019monte} explore MIAs using synthetic images closest to a target record.
%
\citet{chen2020gan} study attack calibration techniques against GANs for images and location data. 
%
Privacy risks of synthetic tabular data have been widely studied, using MIAs based on similarity metrics and shadow models~\citep{yale2019assessing,hyeong2022empirical,zhang2022membership}. 
%
\citet{stadler2022synthetic} compute high-level statistics, \citet{houssiau2022tapas} compute similarities between the target record and synthetic data, and~\citet{meeus2023achilles} propose a trainable feature extractor. 
%
Unlike these, we evaluate MIAs on text generated using fine-tuned LLMs.
This introduces unique challenges and opportunities, both in computing membership scores and identifying worst-case canaries, making our approach distinct from prior work. 

\textbf{Vulnerable records in MIAs.} 
Prior work found that some records (\emph{outliers}) have a disparate effect on a trained model~\citep{feldman2020neural}, making them more vulnerable to MIAs~\citep{carlini2022membership,carlini2022privacy}. Hence, specifically crafted canaries have been proposed to study memorization and for privacy auditing of language models, ranging from a sequence of random digits~\citep{carlini2019secret,stock2022defending} or tokens~\citep{wei2024proving} to synthetically generated sequences~\citep{meeuscopyright}. Also for synthetic tabular data, outliers have been found to have increased privacy leakage~\citep{stadler2022synthetic,meeus2023achilles}.

