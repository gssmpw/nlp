\subsection{Baseline evaluation with standard canaries}
\label{sec:baseline_results}

\begin{table*}[t!]
    \centering
    \footnotesize
    \input{tables/auc_datasets}
    \caption{ROC AUC across datasets, threat models (model-based $\mathcal{A}^\theta$ and data-based $\mathcal{A}^{\synthetic{D}}$) and MIA methodologies for standard, high perplexity canaries (target perplexity $\mathcal{P}_\textrm{target}=250$, no in-distribution prefix ($F=0$) and  $n_\textrm{rep}=12$).} 
    \label{tab:results_primary}
\end{table*}

We begin by assessing the vulnerability of synthetic text using standard canaries. Specifically, we utilize both in-distribution canaries and synthetically generated canaries with a target perplexity $\mathcal{P}_\textrm{target}=250$, no in-distribution prefix ($F=0$), $n_\textrm{rep}=12$ and \emph{natural} or \emph{artificial} labels, as described in Section~\ref{sec:exp_setup}.
Table~\ref{tab:results_primary} summarizes the ROC AUC for model- and data-based attacks.

First, we find that MIAs relying solely on the generated synthetic data achieve a AUC score significantly higher than a random guess (\ie $\text{AUC}=0.5$), reaching up to \num{0.71} for SST-2 and \num{0.66} for AG News. This shows that synthetic text can leak information about the real data used to generate it.

Next, we observe that the data-based attack using an $n$-gram model trained on synthetic data to compute membership scores outperforms the two attacks leveraging similarity metrics: Jaccard distance between a canary and synthetic strings ($\textsc{SIM}_\textrm{Jac}$) or cosine distance between their embeddings ($\textsc{SIM}_\textrm{emb}$).
%
This suggests that information critical to infer membership lies in subtle changes in the co-occurrence of $n$-grams in synthetic data rather than in the generation of many sequences with lexical or semantic similarity.

We also compare attack performance across different canary types under data-based attacks $\mathcal{A}^{\synthetic{D}}$.
The ROC AUC remains consistently higher than a random guess across all canaries.
For SST-2 and AG News, the highest AUC score of \num{0.74} and \num{0.68} is achieved when using in-distribution canaries.

As another baseline, we test RMIA on the target model trained on $D$, assuming the attacker has access to the model logits ($\mathcal{A}^\theta$).
This attack achieves near-perfect performance across all setups, highlighting an inherent gap between the performance of model- and data-based MIAs.
This suggests that, while a fine-tuned model memorizes standard canaries well, the information necessary to infer their membership is only partially transmitted to the synthetic text.

To investigate the gap between the two attacks in more detail, we vary the number of canary repetitions $n_\textrm{rep}$ to amplify the power of the data-based attack until its performance matches that of a model-based attack.
Fig.~\ref{subfig:repetitions_sst2} illustrates these results as a set of ROC curves.
%
We quantify this discrepancy by noting that the MIA performance for $\mathcal{A}^{\synthetic{D}}$ at $n_\textrm{rep}=16$ is comparable to $\mathcal{A}^\theta$ at $n_\textrm{rep}=2$ and for low FPR at $n_\textrm{rep}=1$.
We find similar results in Fig.~\ref{subfig:repetitions_agnews} for AG News.
The MIA performance for $\mathcal{A}^{\synthetic{D}}$ at $n_\textrm{rep}=16$ falls between the performance of $\mathcal{A}^{\theta}$ at $n_\textrm{rep}=1$ and $n_\textrm{rep}=2$. Under these experimental conditions, canaries would need to be repeated \numrange{8}{16}$\times$ to reach the same vulnerability in data-based attacks compared to model-based attacks.

We provide additional results for the standard canaries as appendices: TPR at low FPR scores in Appendix~\ref{app:add_mia_results}, ablations for data-based MIA hyperparameters in Appendix~\ref{app:ablation}, and a discussion on the disparate vulnerability of high perplexity canaries in model- and data-based attacks in Appendix~\ref{app:disparate_vulnerability}.

\begin{figure*}[ht]
  \centering
  \begin{subfigure}{0.32\textwidth}
    \centering
    \resizebox{\textwidth}{!}{\input{figures/n_rep_experiment_sst2_roc}}
    \caption{
        Canary repetitions $n_\textrm{rep}$. \\ $\mathcal{P}_\textrm{target} = 31$, $F=0$.
    }
    \label{subfig:repetitions_sst2}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \centering
    \resizebox{\textwidth}{!}{\input{figures/ppl_experiment_sst2_roc}}
    \caption{
        Canary perplexity $\mathcal{P}_\textrm{target}$. \\
        $n_\textrm{rep}^\theta=4$, $n_\textrm{rep}^{\synthetic{D}}=16$, $F=0$.
    }
    \label{subfig:perplexity_sst2}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \centering
    \resizebox{\textwidth}{!}{\input{figures/prefix_experiment_sst2_roc}}
    \caption{
        Canary in-distribution prefix $F$. \\
        $\mathcal{P}_\textrm{target}=31$, $n_\textrm{rep}^{\theta}=4$, $n_\textrm{rep}^{\synthetic{D}}=16$.
    }
    \label{subfig:prefix_sst2}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \centering
    \resizebox{\textwidth}{!}{\input{figures/n_rep_experiment_agnews_roc}}
    \caption{
        Canary repetitions $n_\textrm{rep}$.  \\ $\mathcal{P}_\textrm{target} = 31$, $F=0$.
    }
    \label{subfig:repetitions_agnews}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \centering
    \resizebox{\textwidth}{!}{\input{figures/ppl_experiment_agnews_roc}}
    \caption{
        Canary perplexity $\mathcal{P}_\textrm{target}$.\\
        $n_\textrm{rep}^\theta=4$, $n_\textrm{rep}^{\synthetic{D}}=16$, $F=0$.
    }
    \label{subfig:perplexity_agnews}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \centering
    \resizebox{\textwidth}{!}{\input{figures/prefix_experiment_agnews_roc}}
    \caption{
        Canary in-distribution prefix $F$. $\mathcal{P}_\textrm{target}=31$, $n_\textrm{rep}^\theta=4$, $n_\textrm{rep}^{\synthetic{D}}=16$.
    }
    \label{subfig:prefix_agnews}
  \end{subfigure}
  \caption{
    ROC curves of MIAs on synthetic data $\mathcal{A}^{\synthetic{D}}$ compared to model-based MIAs $\mathcal{A}^{\theta}$ on SST-2 (\ref{subfig:repetitions_sst2}--\ref{subfig:prefix_sst2}) and AG News (\ref{subfig:repetitions_agnews}--\ref{subfig:prefix_agnews}).
    We ablate over the number of canary insertions $n_\textrm{rep}$ in \ref{subfig:repetitions_sst2}, \ref{subfig:repetitions_agnews}, the target perplexity $\mathcal{P}_\textrm{target}$ of the inserted canaries in \ref{subfig:perplexity_sst2}, \ref{subfig:perplexity_agnews} and the length $F$ of the in-distribution prefix in the canary in \ref{subfig:prefix_sst2}, \ref{subfig:prefix_agnews}. Log-log plots in Appendix~\ref{app:loglogplots}.
  } 
  \label{fig:roc_curves_main}
\end{figure*}

\begin{figure}[h!]
  \centering
  \begin{subfigure}{0.45\textwidth}
    \centering
    \resizebox{\textwidth}{!}{\input{figures/ppl_experiment_sst2}}
    \caption{SST-2}
  \end{subfigure}
  \begin{subfigure}{0.45\textwidth}
    \centering
    \resizebox{\textwidth}{!}{\input{figures/ppl_experiment_agnews}}
    \caption{AG News}
  \end{subfigure}
  \caption{
    ROC AUC for synthetic canaries with varying perplexity (natural label). The model-based MIA $\mathcal{A}^\theta$ improves as canary perplexity increases, while the data-based MIA performance $\mathcal{A}^\synthetic{D}$ (2-gram) decreases. $n_\textrm{rep}^\theta=4$, $n_\textrm{rep}^{\synthetic{D}}=16$.
  } 
  \label{fig:ppl_exp}
\end{figure}

%\labelformat{subfigure}{(#1)}

\subsection{Specialized canaries for enhanced privacy auditing}
\label{sec:designing_canaries}

To effectively audit privacy risks in a worst-case scenario, we explore designing specialized canaries that are both memorized by the model and influential in the synthetic data.

First, we generate specialized canaries by controlling their target perplexity $\mathcal{P}_\textrm{target}$.
We evaluate MIAs for both threat models across a range of perplexities for canaries with natural labels, using $n_\textrm{rep}=4$ for the model-based MIA $\mathcal{A}^\theta$ and $n_\textrm{rep}=16$ for the data-based MIA $\mathcal{A}^\synthetic{D}$.
%
We explore a wide range of perplexities, finding \num{1e5} to align with random token sequences.
Figure~\ref{fig:ppl_exp} shows the ROC AUC score versus canary perplexity.
For the model-based attack $\mathcal{A}^\theta$, the AUC monotonically increases with canary perplexity, reaffirming that outlier records with higher perplexity are more vulnerable to MIAs~\citep{feldman2020neural,carlini2022membership,meeuscopyright}.
Conversely, for the data-based attack $\mathcal{A}^\synthetic{D}$, the AUC initially increases with perplexity but starts to decline beyond a certain threshold, eventually approaching a random guess (AUC of $0.5$). To further illustrate this, we present the complete ROC curve in Figures ~\ref{subfig:perplexity_sst2} and \ref{subfig:perplexity_agnews} for SST-2 and AG News, respectively.
We vary the canary perplexity $\mathcal{P}_\textrm{target}$ while keeping other parameters constant.
As $\mathcal{P}_\textrm{target}$ increases, the model-based attack improves across the entire FPR range, while the data-based attack weakens, approaching AUC of $0.5$ at high perplexities.
This suggests that identifying susceptible canaries is straightforward for model-based privacy audits, but assessing the privacy risk of synthetic data requires a careful balance between canary memorization and its influence on synthetic data.

\begin{table}[t!]
    \centering
    \footnotesize
    \input{tables/tpr_prefix}
    \caption{MIA performance (ROC AUC and TPR at low FPR) for data-based MIA $\mathcal{A}^\synthetic{D}$ ($2$-gram) for canaries with varying length of in-distribution prefix $F$ (results from Figs.~\ref{subfig:prefix_sst2},\ref{subfig:prefix_agnews}). $\mathcal{P}_\textrm{target}=31$ and $n_\textrm{rep}^{\synthetic{D}}=16$.}
    \label{tab:prefix_tpr}
\end{table}

We now examine whether canaries can be crafted to enhance both memorization and influence on the synthetic data, making them suitable to audit the privacy risks of releasing synthetic data.
In Sec.~\ref{sec:method_canaries}, we introduced a method that exploits the greedy nature of LLM decoding to design more vulnerable canaries.
We craft a canary with a low-perplexity, in-distribution prefix to optimize its impact on the synthetic dataset, followed by a high-perplexity suffix to enhance memorization. We generate this suffix sampling from the pre-trained LLM $\theta_0$ with high temperature.
%
Figures \ref{subfig:prefix_sst2} and \ref{subfig:prefix_agnews} illustrate the ROC curves for SST-2 and AG News, respectively, and Table~\ref{tab:prefix_tpr} summarizes the corresponding ROC AUC and TPR at low FPR.
We set the overall canary perplexity $\mathcal{P}_\textrm{target}=31$ and vary the prefix length $F$ from $F=0$ (fully synthetic canaries) to $F=\text{max}$ (in-distribution canaries). 
We observe that combining an in-distribution prefix ($F>0$) with a high-perplexity suffix ($F<\text{max}$) enhances attack effectiveness.
For both datasets, the optimal AUC, and often also the optimal TPR at low FPR, for the MIA is reached for a prefix length $0<F<\text{max}$ (see Table~\ref{tab:prefix_tpr}). 
This suggests that although the model's memorization of the canary stays consistent (as the overall perplexity remains unchanged), the canary's impact on the synthetic data becomes more prominent with longer in-distribution prefixes.
We hypothesize that familiar low-perplexity prefixes serve as starting points for text generation, enhancing the likelihood that traces of the canary appear in the synthetic data.

\subsection{Identifying the memorized sub-sequences}

We analyze what information from a canary leaks into the synthetic data that enables a data-based attack to infer its membership. For each canary $\canary{x} = (\canary{s}, \canary{\ell})$, we examine the synthetic data generated by a model trained on a dataset including (member) and excluding $\canary{x}$ (non-member). We leverage the $M=4$ reference models $\theta'$ used to develop the attack for \num{1000} specialized canaries from Fig.~\ref{subfig:prefix_sst2}.
%
For each model $\theta'$, we count the number of $n$-grams in \synthetic{s} that occur at least once in $\synthetic{D}'$ ($C_\textrm{unique}$). We also compute the median $C_\textrm{med}$ and average $C_\textrm{avg}$ counts of $n$-grams from \canary{s} in $\synthetic{D}'$. 
%
Table~\ref{tab:interpretability_stats} summarizes how these measures vary with $n$. As $n$ increases, the number of $n$-grams from the canary appearing in the synthetic data drops sharply, reaching $C_\textrm{med}=0$ for $n=4$ for models including and excluding a canary. This suggests that any verbatim reproduction of canary text in the generated synthetic data is of limited length. Further, we observe only slight differences in counts between members and non-members, indicating that the signal for inferring membership is likely in subtle shifts in the probability distribution of token co-occurrences within the synthetic data, as captured by the 2-gram model. We further analyze canaries with the highest and lowest RMIA scores in Appendix~\ref{app:Interpretability}. 

\begin{table*}[t!]
    \centering
    \footnotesize
    \input{tables/interpretability_stats}
    \caption{Count statistics of $n$-grams in a canary \canary{s} that also appear in the synthetic data $\synthetic{D}'$ generated using $4$ reference models including and excluding \canary{s}. Number of $n$-grams in \synthetic{s} that also appear in $\synthetic{D}'$ ($C_\textrm{unique}$), median ($C_\textrm{med}$) and average ($C_\textrm{avg}$) counts of $n$-grams from \canary{s} in $\synthetic{D}'$. We report mean and std. deviation of these measures over all canaries ($F=30$, $\mathcal{P}_\textrm{target}=31$, $n_\textrm{rep}=16$) for SST-2. Each canary \canary{s} contains exactly $50$ words and $\synthetic{D}'$ contains $685.1k\pm45.4k$ words.}
    \label{tab:interpretability_stats}
\end{table*}

