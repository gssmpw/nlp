
Large Language Models (LLMs) can generate synthetic data that mimics human-written content through domain-specific prompts. Besides their impressive fluency, LLMs are known to memorize parts of their training data~\citep{carlini2022quantifying} and can regurgitate exact phrases, sentences, or even longer passages when prompted adversarially~\citep{snapshotattack,carlini2021extracting,nasr2023scalable}. This raises serious privacy concerns about unintended information leakage through synthetically generated text. In this paper, we address the critical question: to what extent does synthetic text generated by LLMs leak information about the real data it is derived from?

Prior methods to audit privacy risks insert highly vulnerable, out-of-distribution examples, \textit{canaries}~\citep{carlini2019secret}, into the training data and test whether they can be identified using membership inference attacks (MIAs)~\citep{shokri2017membership}. Various MIAs have been proposed, typically assuming an attacker with access to the trained model or its output logits~\citep{carlini2019secret,shi2024detecting}. In the context of LLMs, MIAs often rely on analyzing the model's behavior when prompted with inputs related to the canaries~\citep{carlini2021extracting,chang2024context,shi2024detecting}. However, similar investigations are lacking in scenarios where LLMs are used to generate synthetic data and only this synthetic data is available to an attacker. 


\textbf{Contributions.} In this work, we study--for the first time--the factors that influence information leakage from a data corpus through synthetic data generated using LLMs.
 %
First, we introduce data-based attacks that only have access to synthetic data, not the model used to generate it, and therefore cannot probe it with adversarial prompts nor compute losses or other statistics used in model-based attacks~\citep{ye2022enhanced, carlini2022membership}. We propose approximating membership likelihood using either a model trained on the synthetic data or the target example similarity to its closest synthetic data examples. We design our attacks adapting pairwise likelihood ratio tests as in RMIA~\citep{zarifzadeh2024low} and evaluate our attacks on labeled datasets: SST-2~\citep{socher-etal-2013-recursive} and AG News~\citep{Zhang2015CharacterlevelCN}. Our results show that MIAs leveraging only synthetic data achieve AUC scores of $0.71$ for SST-2 and $0.66$ for AG News, largely outperforming a random guess baseline. This suggests that synthetic text can leak significant information about the real data used to generate it.

Second, we use the attacks we introduce to quantify the gap in performance between data- and model-based attacks. We do so in an auditing scenario, designing adversarial canaries and controlling leakage by varying the number of times a canary occurs in the training dataset. Experimentally, we find a sizable gap when comparing attacks adapted to the idiosyncrasies of each setting: a canary would need to occur $8\times$ more often to be as vulnerable against a data-based attack as it is against a model-based attack (see Figs.~\ref{subfig:repetitions_sst2} and \ref{subfig:repetitions_agnews}). 

Third, we discover that canaries designed for model-based attacks fall short when auditing privacy risks of synthetic text. Indeed, privacy auditing of LLMs through model-based MIAs relies on rare, out-of-distribution sequences of high perplexity~\citep{carlini2019secret,stock2022defending,wei2024proving,meeuscopyright}. We confirm that model-based MIAs improve as canary perplexity increases. In sharp contrast, we find that high perplexity sequences, although distinctly memorized by the target model, are less likely to be \emph{echoed} through synthetic data generated by the target model. Therefore, as a canary perplexity increases, the canary influence on synthetic data decreases, making its membership less detectable from synthetic data (see Figure~\ref{fig:ppl_exp}). We show that low-perplexity, and even in-distribution canaries, while suboptimal for model-based attacks, are more adequate canaries in data-based attacks.

Lastly, we propose an alternative canary design tailored for data-based attacks based on the following intuition: 
%
\begin{inlineenum}
\item in-distribution canaries aligned with the domain-specific prompt can influence the generated output 
\item memorization is more likely when canaries contain sub-sequences with high perplexity.
\end{inlineenum}
%
We construct canaries starting with an in-distribution prefix of length $F$, transitioning into an out-of-distribution suffix, increasing the likelihood that the model memorizes them and that they influence synthetic data. 
We show that, for fixed overall canary perplexity, the performance of attacks for canaries with in-distribution prefix and out-of-distribution suffix ($0<F<\text{max}$) improves upon both entirely in-distribution canaries ($F=\text{max}$) and out-of-distribution canaries ($F=0$), for both datasets (see Fig.~\ref{fig:roc_curves_main} and Table~\ref{tab:prefix_tpr}). 

Taken together, the proposed MIAs and canary design can be used to audit privacy risks of synthetic text. Auditing establishes a lower bound on the risk, useful to take informed decisions about releasing synthetic data in sensitive applications and also complements upper bounds on privacy risks from methods that synthesize text with provable guarantees.


