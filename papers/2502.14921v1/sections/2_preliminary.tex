\textbf{Synthetic text generation.} 
We consider a private dataset $D = \{x_i = (s_i, \ell_i)\}_{i=1}^N$ of labelled text records where $s_i$ represents a sequence of tokens (\eg a product review) and $\ell_i$ is a class label (\eg the review sentiment). 
%
A synthetic data generation mechanism is a probabilistic procedure mapping $D$ to a synthetic dataset $\synthetic{D} = \{ \synthetic{x}_i = (\synthetic{s}_i, \synthetic{\ell}_i) \}_{i=1}^\synthetic{N}$ with a desired label set $\{\ell_i\}_{i=1}^\synthetic{N}$. Unless stated otherwise, we consider $N = \synthetic{N}$. The synthetic dataset \synthetic{D} should preserve the \emph{utility} of the private dataset $D$, \ie, it should preserve as many statistics of $D$ that are useful for downstream analyses as possible. In addition, a synthetic data generation mechanism should preserve the \emph{privacy} of records in $D$, \ie it should not leak sensitive information from the private records into the synthetic records.
%
The utility of a synthetic dataset can be measured by the gap between the utility achieved by \synthetic{D} and $D$ in downstream applications. The fact that synthetic data is not \textit{directly} traceable to original data records does not mean that it is free from privacy risks. On the contrary, the design of a synthetic data generation mechanism determines how much information from $D$ leaks into $\synthetic{D}$ and should be carefully considered. Indeed, several approaches have been proposed to generate synthetic data with formal privacy guarantees~\citep{kim2021differentially,tangprivacy,wuprivacy,xiedifferentially}. 
%
We focus on privacy risks of text generated by a pre-trained LLM fine-tuned on a private dataset~$D$~\citep{yue2023synthetic,mattern2022differentially,kurakin2023harnessing}. 
%
Specifically, we fine-tune an LLM $\theta_0$ on records $(s_i, \ell_i) \in D$ to minimize the loss in completing $s_i$ conditioned on a prompt template \prompt{\ell_i}, obtaining $\theta$.
%
We then query $\theta$ using the same prompt template to build a synthetic dataset \synthetic{D} matching a given label distribution.


\textbf{Membership inference attacks.}
MIAs~\citep{shokri2017membership} provide a meaningful measure to quantify privacy risks of machine learning models, due to its simplicity but also due to the fact that protection against MIAs implies protection against more devastating attacks such as attribute inference and data reconstruction~\citep{sok-games:2023}. 
%
In a MIA on a target model $\theta$, an adversary aims to infer whether a target record is present in the training dataset of $\theta$. Different variants constrain the adversary's access to the model. 
%, ranging from full access to model parameters~\citep{nasr2018comprehensive} to query access~\citep{zarifzadeh2024low}.
%
In our setting, we consider model-based adversaries that observe the output logits on inputs of their choosing of a model $\theta$ fine-tuned on a private dataset $D$.
%
We naturally extend the concept of MIAs to synthetic data generation mechanisms by considering data-based adversaries that only observe a synthetic dataset $\synthetic{D}$ generated from $D$.


\textbf{Privacy auditing using canaries.} 
A common method used to audit the privacy risks of ML models is to evaluate the MIA vulnerability of canaries, \ie, artificial worst-case records inserted in otherwise natural datasets~\citep{carlini2019secret}. This method can also be employed to derive statistical lower bounds on the differential privacy (DP) guarantees of the training pipeline~\citep{jagielski2020auditing,bayesian-estimation:2023}. 
%
Records crafted to be out-of-distribution \wrt the underlying data distribution of $D$ give a good approximation to the worst-case~\citep{carlini2019secret,meeuscopyright}.
%
Canaries can take a range of forms, such as text containing sensitive information~\citep{carlini2019secret} and random~\citep{wei2024proving} or synthetically generated sequences~\citep{meeuscopyright}.
%
Prior work identified that longer sequences, repeated more often~\citep{carlini2022quantifying}, and with higher perplexity~\citep{meeuscopyright} are better memorized during training and hence are more vulnerable to model-based MIAs.
%
We study multiple types of canaries and compare their vulnerability against model- and synthetic data-based MIAs. 
%
We consider a set of canaries $\{\canary{x}_i = (\canary{s}_i, \canary{\ell}_i) \}_{i=1}^\canary{N}$, each crafted adversarially and inserted with probability \nicefrac{1}{2} into the private dataset $D$.
The resulting dataset is then fed to a synthetic data generation mechanism.
%
We finally consider each canary $\canary{x}_i$ as the target record of a MIA to estimate the privacy risk of the generation mechanism (or the underlying fine-tuned model).



\textbf{Threat model.} 
We consider an adversary $\mathcal{A}$ who aims to infer whether a canary \canary{x} was included in the private dataset $D$ used to synthesize a dataset $\synthetic{D}$. 
We distinguish between two threat models:
%
\begin{inlineenum}
\item an adversary $\mathcal{A}^\theta$ with query-access to output logits of a target model $\theta$ fine-tuned on $D$
%
\item an adversary $\mathcal{A}^{\synthetic{D}}$ with only access to the synthetic dataset $\synthetic{D}$.
\end{inlineenum}
%
To the best of our knowledge, for text data this latter threat model has not been studied extensively in the literature. 
In contrast, the privacy risks of releasing synthetic tabular data are much better understood~\citep{stadler2022synthetic,yale2019assessing,hyeong2022empirical,zhang2022membership}.
%
Algorithm~\ref{alg:mia} shows the generic membership inference experiment encompassing both model- and data-based attacks, selected by the \textsf{synthetic} flag. The adversary is represented by a stateful procedure $\mathcal{A}$, used to craft a canary and compute its membership score.
%
Compared to a standard membership experiment, we consider a fixed private dataset $D$ rather than sampling it, and let the adversary choose the target $\canary{x}$. This is close to the threat model of \emph{unbounded} DP, where the implicit adversary selects two datasets, one obtained from the other by adding one more record, except that in our case the adversary observes but cannot choose the records in $D$. The membership score $\beta$ returned by the adversary can be turned into a binary membership label by choosing an appropriate threshold. We further clarify assumptions made for the adversary in both threat models in Appendix~\ref{app:adversary_assumptions}.

\begin{algorithm*}[t!]
\caption{Membership inference against an LLM-based synthetic text generator}
\label{alg:mia}
\begin{algorithmic}[1]
\STATE \textbf{Input}: Fine-tuning algorithm $\mathcal{T}$, pre-trained model $\theta_0$, private dataset $D = \{ x_i = (s_i, \ell_i) \}_{i=1}^N$, labels $\{\synthetic{\ell}_i\}_{i=1}^{\synthetic{N}}$, prompt template \prompt{\cdot}, canary repetitions $n_\text{rep}$, sampling method $\textsf{sample}$, adversary $\mathcal{A}$
\STATE \textbf{Output}: Membership score $\beta$

\STATE $\canary{x} \gets \mathcal{A}(\mathcal{T}, \theta_0, D, \{\synthetic{\ell}_i\}_{i=1}^{\synthetic{N}}, \prompt{\cdot})$ 
\hfill \COMMENT{Adversarially craft a canary (see Sec.~\ref{sec:method_canaries})}

\STATE $b \sim \{0,1\}$ 
\hfill \COMMENT{Flip a fair coin}

\IF{b = 1}
    \STATE $\theta \gets \mathcal{T}(\theta_0, D \cup \{\canary{x}\}^{n_\textrm{rep}})$
    \hfill \COMMENT{Fine-tune $\theta_0$ with canary repeated $n_\textrm{rep}$ times}
\ELSE
    \STATE $\theta \gets \mathcal{T}(\theta_0, D)$
    \hfill \COMMENT{Fine-tune $\theta_0$ without canary}
\ENDIF

\FOR{$i = 1 \ldots \synthetic{N}$}
    \STATE $\synthetic{s}_i \sim \textsf{sample}(\theta(\prompt{\synthetic{\ell}_i}))$
    \hfill \COMMENT{Sample synthetic records using prompt template}
\ENDFOR

\STATE $\synthetic{D} \gets \left\{ (\synthetic{s}_i, \synthetic{\ell}_i) \right\}_{i=1}^\synthetic{N}$

%\hfill \COMMENT{}
\IF{\textsf{synthetic}} 
    \STATE $\beta \gets \mathcal{A}(\synthetic{D}, \canary{x})$
    \hfill \COMMENT{Compute membership score $\beta$ of $\canary{x}$, see Sec.~\ref{subsec:data_score} and algorithms in Appendix~\ref{app:pseudo_code}}
\ELSE
    \STATE $\beta \gets \mathcal{A}(\theta, \canary{x})$ 
    \hfill \COMMENT{Compute membership score $\beta$ of $\canary{x}$, see Sec.~\ref{subsec:model_score}}
\ENDIF

\STATE \textbf{return} $\beta$
\end{algorithmic}
\end{algorithm*}

\textbf{Problem statement.} 
We study methods to audit privacy risks associated with releasing synthetic text. Our main goal is to develop an effective data-based adversary $\mathcal{A}^{\synthetic{D}}$ in the threat model of Algorithm~\ref{alg:mia}. For this, we explore the design space of canaries to approximate the worst-case, and adapt state-of-the-art methods used to compute membership scores in model-based attacks to the data-based scenario.
