%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}


% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{xcolor}
\usepackage{multirow}
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}
\usepackage{tabularx}
\usepackage{wrapfig}
% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{bbm}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{subcaption} % 必须加载
\usepackage{makecell}
% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\newcommand{\ie}{\textit{i}.\textit{e}.}
\newcommand{\Eg}{\textit{e}.\textit{g}.}
\newcommand{\etc}{\textit{etc}.}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Mitigating Object Hallucinations in Large Vision-Language Models via Attention Calibration}

\begin{document}

\twocolumn[
\icmltitle{Mitigating Object Hallucinations in Large Vision-Language Models via \\
Attention Calibration}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Younan Zhu}{sch}
\icmlauthor{Linwei Tao}{sch}
\icmlauthor{Minjing Dong}{yyy}
\icmlauthor{Chang Xu}{sch}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{Department of Computer Science, City University of Hong Kong}
\icmlaffiliation{sch}{School of Computing Sciencee, University of Sydney}

\icmlcorrespondingauthor{Chang Xu}{c.xu@sydney.edu.au}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{} % otherwise use the standard text.

\begin{abstract}
% 1. background of hallucination
% 2. positional bias leads to the hallucination (from lens of attention)
% 3. different patterns of positional bias in different models -> we need to calibrate attention
% 4. introduce UAC and DAC (in general)
% 5. results
Large Vision-Language Models (LVLMs) exhibit impressive multimodal reasoning capabilities but remain highly susceptible to object hallucination, where models generate responses that are not factually aligned with the visual content.
Recent works attribute this issue to an inherent bias of LVLMs where vision token attention map has a fixed correlation with spatial position, and propose to mitigate this issue by reordering visual tokens.
However, we find that different LVLMs exhibit different correlations between attention and spatial position, which makes the existing solution difficult to generalize to other LVLMs.
To address this issue, we first introduce a training-free solution, Uniform Attention Calibration (UAC), that estimates the bias from single meaningless input image and applies a calibration matrix to rectify attention imbalances. To further alleviate the bias, we relax the assumption of single meaningless input in UAC and introduce a fine-tuning solution, Dynamic Attention Calibration (DAC), that enforces the consistent outputs wherever the object locates in the image via a plug-and-play module.
Comprehensive experiments across multiple benchmarks demonstrate that UAC and DAC significantly reduce object hallucination while improving general multimodal alignment.  Our methods achieve state-of-the-art performance across diverse LVLM architectures on various metrics.









\end{abstract}

\section{Introduction}

Large Vision-Language Models (LVLMs) \cite{liu2024visual,bai2023qwen,dai2024instructblip,zhu2023minigpt4,ye2024mplug} have garnered significant attention in the AI research community for their remarkable ability to comprehend the visual world and engage in conversational interactions with humans. 
% Their capacity to process multimodal sequences has enabled breakthroughs across a variety of vision and language tasks \cite{lai2023lisa,alayrac2022flamingo}, such as handling interleaved image-text inputs \cite{awadalla2023openflamingo,li2023otter} and responding to interactive user queries \cite{zhang2023prompt}. 
Despite these advances, LVLMs continue to face critical challenges, particularly in the form of object hallucination \cite{pope,anna2018chair,cui2023holistic}, a phenomenon where models generate responses that are not factually aligned with the visual content. This issue undermines the reliability of LVLMs, posing a significant barrier to their deployment in real-world applications.

A variety of approaches have been proposed to mitigate object hallucination in LVLMs. One common strategy involves post-hoc correction using revisor models~\cite{shukang2023woodpecker, zhou2024object,lee2023volcano}, which aim to reduce hallucinated responses by refining outputs. Another approach improves supervised fine-tuning through diversified instruction tuning data~\cite{liu2024mitigating,yu2024hallucidoctor} or aligns model responses with human preferences~\cite{sun2023aligning}. Recently, several studies have explored training-free methods for mitigating object hallucination by addressing issues in the autoregressive decoding process of LVLMs~\cite{sicong2023vcd, huo2024sid,qidong2023opera}. 









A recent study~\cite{xing2024cca} reveals that LVLMs' perception varies with object positions due to the inherent processing order in autoregressive models. As 2D vision tokens are concatenated with text tokens and flattened into a raster-scan sequence (top-to-bottom, left-to-right), the model develops a bias, prioritizing tokens in the bottom-right region closer to the instruction tokens (Figure~\ref{fig:attention_distribution}a), termed as Spatial Perception Bias (SPB). This spatial bias skews perception capabilities. To mitigate this, \citet{xing2024cca} propose a position alignment technique that reorders the perception sequence, reducing spatial bias.

However, this approach has two major limitations. First, the method is based on the assumption that the model assigns greater attention to tokens that are relatively nearby. As demonstrated in Figure~\ref{fig:attention_distribution}(a-c), our analysis reveals that the attention distributions of vision tokens vary significantly across different LVLM models and unexpectedly high attentions are assigned to arbitrary locations. This observation challenges the generalization of the heuristic reordering strategy proposed by~\citet{xing2024cca}, highlighting the need for a more dynamic and adaptable solution. Second, the proposed technique requires retraining the entire network, which is computationally expensive and often impractical for large-scale LVLMs, underscoring the necessity of developing a lightweight alternative.
\begin{figure*}[t!]
\begin{center}
% Placeholder rectangle
\includegraphics[width=2.0\columnwidth]{image/attention_heatmap_white_open_scaled.pdf}
\vskip -0.1in
\caption{Spatial Position Bias influences how LVLMs perceive objects based on their position within an image. The visualization above illustrates vision tokens attention weights during the decoding process for different models on a blank white image in response to the open-ended prompt: ``Please describe the image in detail." (a) shows LLaVA-1.5, which exhibits an increasing trend in attention distribution following a raster scan order, as identified by \cite{xing2024cca}. (b-c) represent other models, displaying arbitrary attention distributions. (d) depicts the calibrated vision tokens attention map of LLaVA-1.5 after Dynamic Attention Calibration.}
\label{fig:attention_distribution}
\end{center}
\end{figure*}

Building on this analysis, we aim to rectify the inherent SPB in vision token attention distributions. To achieve this, we introduce two attention calibration methods: Uniform Attention Calibration (UAC) and Dynamic Attention Calibration (DAC). UAC provides a simple training-free solution with competitive performance by calibrating biased attention through bias estimation on a meaningless input. 
Though effective and efficient, the performance of UAC could be limited due to the assumption of single meaningless input.
Thus, we further relax the assumption in UAC and introduce DAC to fine-tune LVLMs for better performance. Specifically, DAC consists of a learnable plug-and-play module integrated into the self-attention mechanism. With a simple yet effective data augmentation technique, the module is then fine-tuned via contrastive learning to encourage consistent outputs with different object positions in the image, which dynamically adjusts vision token attention map to tackle object hallucination.
% In addition, DAC is a learnable plug-and-play module integrated into the self-attention mechanism, which dynamically adjusts vision token attention map.

Comprehensive experiments confirm the effectiveness of UAC and DAC, revealing substantial improvements across multiple object hallucination benchmarks and a range of LVLMs, including LLaVA-1.5 \cite{liu2024visual, haotian2024llava}, mPLUG-Owl2 \cite{ye2024mplug}, and LLaVA-NeXT \cite{bo2024llavanext}. Additionally, our approach strengthens the overall perception capabilities of LVLMs, as demonstrated by its strong performance on MME \cite{chaoyou2023mme} and LLaVA-Bench \cite{liu2024visual}, emphasizing its utility beyond mitigating object hallucination. To summarize, our main contributions are as follows:

\begin{enumerate}
\item We systematically investigate Spatial Perception Bias(SPB) in vision token attention within LVLMs, revealing its strong correlation with object hallucination and its persistence across different models.
\item Based on these findings, we propose Uniform Attention Calibration (UAC), a simple yet effective training-free bias correction module, and Dynamic Attention Calibration (DAC), a learnable plug-and-play module that dynamically adjusts vision token attention.
\item Extensive experiments confirm that both UAC and DAC significantly reduce object hallucination and enhance overall perception. Our methods achieve notable improvements across various LVLMs.
\end{enumerate}
 

\section{Related Work}
\subsection{Visual-Language Models}
Large Vision-Language Models (LVLMs) have evolved from early BERT-based architectures \cite{devlin2018bert,lu2019vilbert,chen2019uniter} to models that integrate Large Language Models (LLMs) \cite{bai2023qwen,brown2020language,gilardi2023chatgpt,raffel2020exploring,taori2023stanford}. Early vision-language models, such as ViLBERT \cite{lu2019vilbert} and LXMERT \cite{tan2019lxmert}, fused visual and textual features through transformer-based architectures. The introduction of LLMs enabled contrastive learning approaches like CLIP \cite{radford2021clip} and ALIGN \cite{jia2021scaling}, improving multimodal adaptability. Recent LVLMs, such as LLaVA \cite{liu2024visual} and InstructBLIP \cite{dai2024instructblip}, leverage visual instruction tuning for improved context-aware generation. Advances have further enabled referential dialogues \cite{chen2023shikra,you2023ferret,zhang2023gpt4roi}, interleaved image-text processing \cite{alayrac2022flamingo,awadalla2023openflamingo}, and visual prompts \cite{peng2023kosmos,zhang2023prompt,chen2023llava}, broadening LVLM applications in interactive AI systems. These developments highlight a growing shift toward task-specific fine-tuning and multimodal interaction.

\subsection{Hallucination in VLMs}
Object hallucination arises when Large Vision-Language Models (LVLMs) generate textual descriptions containing objects or attributes not present in the accompanying image \cite{cui2023holistic,liu2024survey,guan2023hallusionbench,li2023evaluating,wang2024mementos,nie2024mmrel}. This phenomenon is frequently observed in tasks such as image captioning and visual question answering, where maintaining an accurate alignment between visual and textual content is critical. A range of methods has been proposed to address hallucination, from post-hoc correction using external or self-correcting models \cite{shukang2023woodpecker,zhou2024object,lee2023volcano} to enhanced instruction tuning that diversifies training data or aligns outputs with human feedback \cite{liu2024mitigating,yu2024hallucidoctor,sun2023aligning}. Recently, training-free approaches that rely on model-based distribution comparisons were proposed ~\cite{sicong2023vcd, huo2024sid,qidong2023opera}. As LVLMs grow more sophisticated and versatile, understanding and mitigating object hallucination remains a key focus in multimodal learning research. From a unique perspective, our design is rooted in the correlation between vision tokens attention and object hallucination.


\section{Preliminary}
In this section, we provide a brief overview of the widely adopted LVLMs architecture and explain how vision tokens are involved in the self attention module. Additionally, we review the how LVLMs exhibit spatial perception bias problem, highlighting systematic biases that affect LVLM hallucination.

\subsection{LVLMs: Generation and Attention Mechanism}
\paragraph{Vision and Language Inputs}
LVLMs process both image \( v \) and text \( t \) inputs. Raw images are divided into patches and encoded by a visual encoder, followed by a cross-modal projection module that maps visual features into the token space. This yields a sequence of vision tokens \( v = \{v_i \mid i = 1,2, \dots, n\} \), where \( n \) is the number of vision tokens. Text inputs are tokenized and embedded into text tokens \( t = \{t_j \mid j = 1,2, \dots, m\} \), where \( m \) is the number of text tokens. The vision and text tokens are then concatenated into a unified input sequence \( x = \{v, t\} \), ensuring a shared multimodal representation space, with \( v_i, t_j \in \mathbb{R}^d \), where \( d \) denotes the feature dimensionality.

\paragraph{Language Model Generation}
LVLMs are typically built on pre-trained LLMs such as Vicuna \cite{chiang2023vicuna} or LLaMA \cite{touvron2023llama}, parameterized by $\theta$. The model takes a input \(x\) and predicts the next token probability \(p(y_i)\) at time step \(i\) in an autoregressive manner:
\begin{equation}
p(y_i \mid x, y_{<i}) = \text{softmax}(\text{logit}_\theta(y_i \mid x, y_{<i}))
\end{equation}
\paragraph{Self-Attention Mechanism}
The self-attention mechanism computes token relevance by projecting the output of previous layer into query \( Q \), key \( K \), and value \( V \) with linear transformations $W_Q$, $W_K$, $W_V$. The self attention output is computed as
\begin{equation}
\text{SA}(Q, K, V) = \text{softmax}(\mathbf{A} + M) \cdot V, \; \mathbf{A} = \frac{Q \cdot K^T}{\sqrt{d_l}},
\label{eq:attention_weights}
\end{equation}
where \(\mathbf{A} \in \mathbb{R}^{B \times H \times (n+m) \times (n+m)}\)\footnote{We omit the system tokens for simplicity.} denotes attention weight matrix, \( B \) and \( H \) represent the batch size, and number of attention heads, respectively. $M$ denotes the causal mask, and $d_l$ is the dimensionality of $Q$, $K$, and $V$. We denote \(\mathbf{A^i}\) as the attention matrix after $i$-th layer of LVLM, In this paper, we denote vision tokens attention \( \mathbf{A}_{\text{img}} \in \mathbb{R}^{B \times H \times n} \) as the slice of the attention weights corresponding to the vision token inputs \( v \). 

\subsection{Spatial Perception Bias}
% \begin{figure}[ht]
% \vskip -0.1in
% \begin{center}
% % Placeholder rectangle with color
% \includegraphics[width=1\columnwidth]{image/toy2.pdf}
% \vskip -0.1in
% \caption{Model perception accuracy at different positions on a flattened 14×14 grid, highlighting spatial perception bias in LVLMs.}

% \label{toy2}
% \end{center}
% \vskip -0.1in
% \end{figure}
% Recent research on efficiency has underscored the redundancy and sparsity of vision tokens \cite{cao2023pumer,shang2024llava,chen2024image}
% . However, the relationship between such vision tokens attention patterns and object hallucination remains largely unexplored.
When given a blank white image and the open-ended prompt ``Please describe the image in detail," LVLMs are expected to distribute attention uniformly across the entire image. However, as shown in Figure~\ref{fig:attention_distribution}a, the self-attention module assigns varying levels of attention to different spatial regions. For instance, LLaVA-1.5 places greater attention on later visual tokens, particularly near the bottom-right. This systematic attention bias reflects position-dependent sensitivity to visual features. We define this phenomenon as Spatial Perception Bias(SPB)—a systematic error in the self-attention module that skews attention weights.  

\citet{xing2024cca} were the first to identify a similar issue, attributing it to the long-term decay effect of position encoding. Specifically, LVLMs tend to assign lower attention to top-left tokens compared to bottom-right tokens. To mitigate this, they proposed reordering the visual token sequence to achieve a more balanced attention distribution. However, when comparing Figure~\ref{fig:attention_distribution}(a–c), we find that SPB varies significantly across models and can result in unexpectedly high attention to arbitrary locations. Consequently, a predefined token reordering strategy \cite{xing2024cca} may not generalize well to LVLMs beyond LLaVA-1.5.

% LVLMs are sensitive to the spatial positioning of objects, leading to uneven attention allocation across spatial locations. Such sensitivity may overemphasize certain areas while neglecting others \cite{xing2024cca}, distorting the model's understanding of the scene and contributing to object hallucination, which we term \textit{Spatial Perception Bias}. 

%To better understand Spatial Perception Bias, we designed a controlled experiment to isolate the influence of background cues and interference from other objects, focusing exclusively on the object and its spatial position. Specifically, we randomly sampled 1,000 images from the MSCOCO dataset, systematically placing each cropped object across a 14x14 grid to create 196 variations where the object appeared at unique grid locations against a pure white background. This experimental setup reflects the patch-based encoder used in LVLMs like CLIP \cite{radford2021clip}, where images are divided into discrete patches before being processed by the model. Using a polling-based prompt, we elicited the model’s interpretation of each image. The results, shown in Figure~\ref{toy2}, reveal that LVLMs perceive objects disproportionately depending on their position, with perception accuracy differing by over threefold in extreme cases. These findings highlight the necessity for hallucination mitigation techniques to account for Spatial Perception Bias in LVLMs.



\section{Method}
% WWe observe that the attention distribution of vision tokens in LVLMs exhibits a pronounced imbalance, as shown in Figure~\ref{fig:attention_distribution}. This bias is distinct for each model and remains largely invariant to the specific input image or prompt, even when presented with a blank image. Additional experiments using different blank images, such as a pure black image or Gaussian noise, and various prompts reveal a similar trend, as detailed in the appendix.


% We argue that LVLMs should not exhibit vision token imbalance when processing meaningless images. Furthermore, we hypothesize that the presence of this imbalance is directly correlated with spatial perception bias, which can lead to object hallucination. By calibrating the attention distribution of vision tokens, the model can achieve better object perception.

A valid approach to mitigating bias is to calibrate the attention values. Since the attention map essentially forms a discrete probability distribution that sums to one, this problem closely resembles uncertainty calibration~\cite{guo2017calibration}. A common strategy in uncertainty calibration literature is to adjust the probability distribution by modifying the output logits.  Inspired by calibration literature, we propose two attention calibration techniques to meet different needs: \textbf{Uniform Attention Calibration (UAC)} and \textbf{Dynamic Attention Calibration (DAC)}. UAC is a training-free method that removes SPB estimated from a meaningless input, offering a simple yet effective solution with competitive performance. In addition, DAC is a learnable plug-and-play module within the self-attention mechanism, providing state-of-the-art performance with minimal computational overhead.

\subsection{\textbf{Uniform Attention Calibration}}\label{sec:training_free}
We hypothesize that LVLMs should assign uniform attention to meaningless images, such as blank white or random noise images. Therefore, we introduce UAC, which enforces uniform attention by first estimating SPB from a meaningless input (defaulting to a blank white image) and computing a calibration matrix \( W \) that adjusts vision tokens attention map to be uniform across all positions. This matrix is then applied as an affine transformation to vision tokens attention during inference.

Specifically, we first obtain the attention map \( \Tilde{\mathbf{A}}_{\text{img}} \) for a meaningless input and compute calibration matrix \( W \) as:
\begin{equation} \label{eq:bias_W}
    W = \frac{\text{avg}(\Tilde{\mathbf{A}}_{\text{img}})}{\Tilde{\mathbf{A}}_{\text{img}}}
\end{equation}
where \( \text{avg}(\cdot) \) denotes the average value over all elements. 

During inference, the calibrated attention matrix \( \mathbf{A}_{\text{img}}' \) for any input image is obtained as:
\begin{equation}
\mathbf{A}_{\text{img}}' = W \circ \mathbf{A}_{\text{img}}    
\end{equation}
where \( \mathbf{A}_{\text{img}} \) represents the original attention weights computed from the input image, \(\circ\) denotes the element-wise (Hadamard) product. Notably, \( W \) can be calcaulated and applied to any layer of the LVLM decoder, making it a flexible and model-agnostic calibration method.



% To validate our hypothesis, we testify a naive solution inspired by \cite{zhao2021calibrate}. 


% \begin{table}[ht]
% \centering
% \renewcommand{\arraystretch}{1}
% \setlength{\tabcolsep}{4pt}
% \begin{tabular}{l|ccccc}
% \hline
% {} & Baseline & 14 & SID & CCA \\
% \hline
% Accuracy & 78.92 & 82.67 & 80.25 & 83.97 \\
% F1       & 80.75 & 83.56 & 81.28 & 83.82 \\
% \hline
% \end{tabular}
% \caption{POPE COCO Adversarial on LLaVA-1.5}
% \label{tab:toy}
% \end{table}
% As shown in Table~\ref{tab:toy}, our naive solution demonstrates a positive improvement over the baseline when applied to the first layer. At the optimal layers, it surpasses the current training-free state-of-the-art method, Self-Introspective Decoding (SID), by a large margin, while remaining competitive with the spatial perception-focused Concentric Causal Attention (CCA). These results confirm that mitigating vision token imbalance can indeed reduce object hallucination.

% Nevertheless, the naive solution remains limited. Its simplistic design, which applies uniform reweighting, cannot adapt to varying contexts and ultimately sacrifices generation quality (see appendix). This limitation underscores the need for a more flexible, learning-based calibration approach—one that can dynamically adjust attention distributions without requiring extensive retraining or compromising the naturalness of generated text.




\subsection{Dynamic Attention Calibration} \label{sec:DAC}
% UAC 假定了某张白图是meaning图并基于此计算出了一个Spatial Perception Bias和calibration W，这间接去除了input image的Spatial Perception Bias，但并不能保证这是optimal的solution。因为，我们提出了一个可训练的mapping，force object在任何位置，输出都要consistent，（contrastive loss保证了这个目标）
% We introduce a training-free solution to tackle the spatial perception bias in Sec. \ref{sec:training_free}. Although it is efficient and effective in yes or no perception task, the open ended generalization ability of LVLMs could be hurt, as shown in XXX. 

% One possible explanation is that 
UAC removes SPB by assuming a single meaningless reference, but this one-size-fits-all approach does not guarantee optimal attention calibration across diverse inputs. To overcome this limitation, we seek an trainable solution that allows LVLMs to dynamically adjust their attention distributions while ensuring invariance to an object's spatial position within an image. To this end, we introduce Dynamic Attention Calibration (DAC), which involves plug-and-play learnable module to dynamically calibrate the SPB via contrastive learning \cite{wu2018unsupervised,chen2020simple}.


% 1. besides the method in sec 4.1, why we need to introduce another one. (discuss the motivation of DAC)
% 2. Introduce the method in details, designing of DAC. dynamically tackle various positional bias of different models. End-to-end solution.
% 3. contrastive learning


% Instead of estimating the model's inherent bias via blank white images in Sec. \ref{sec:training_free}, we propose to calibrate the vision token attention $\mathbf{A}_{\text{img}}$ directly.
\paragraph{DAC Design}~Motivated by the superior calibration performance of affine transformation in the field of uncertainty calibration \cite{platt1999probabilistic}, we introduce a lightweight trainable transformation $f$ to calibrate unreliable vision token attention weights before SoftMax function as
$\mathbf{A}_{\text{img}}' = f(\mathbf{A}_{\text{img}}),$
where $\mathbf{A}_{\text{img}}'$ denotes the calibrated vision token attention weights. Specifically, the transformation $f$ operates within the self-attention mechanism of the transformer decoder layers and consists of a small stack of linear transformations with ReLU activations. The details about building blocks can be found in the Appendix. The forward pass of DAC module can be defined as
\begin{equation}\label{eq:DAC}
\scalebox{0.95}{$ \displaystyle
\begin{aligned}
    & \mathbf{A}_{\text{img}}' = f(\mathbf{A}_{\text{img}}) = \mathbf{g}_{L-1} \mathbf{W}_L + \mathbf{b}_L,\\
    & \mathbf{g}_i = \text{ReLU}(\mathbf{g}_{i-1} \mathbf{W}_i + \mathbf{b}_i), \; \text{for } i \in \{1, \ldots, N-1\},\\
\end{aligned}
$}
\end{equation}
where $L$ denotes the layer number in DAC module, $\mathbf{W}_i \in \mathbb{R}^{D_{i} \times D_{i}}$ denotes the weight matrix of layer $i$, $\mathbf{b}_i \in \mathbb{R}^{D_{i}}$ denotes the bias vector, $\mathbf{g}_i$ represents the output of the $i$-th layer, and $\mathbf{g}_0 = \mathbf{A}_{\text{img}}$. The DAC module can be applied to any layer of the language model decoder, targeting the layers responsible for vision tokens processing.

\paragraph{DAC Optimization}~With the DAC module in Eq. \ref{eq:DAC}, a much stronger constraint can be imposed on vison token attention weights of LVLMs to alleviate the bias. Instead of the uniform constraint in UAC, we further propose to force the consistent outputs wherever the object locates in the image. The key idea is to ensure that the model maintains the same capability of identifying an object regardless of its position within the image. However, to impose such a constraint, it could be challenging to obtain sufficient training data variants with different object positions. Thus, we introduce a simple yet effective data augmentation technique inspired by the concept of instant discrimination \cite{wu2018unsupervised,chen2020simple}. 
% A common technique in uncertainty calibration tasks \cite{platt1999probabilistic, guo2017calibration} involves applying an affine transformation \( f \) to calibrate uncalibrated logits, ensuring a well-calibrated softmax probability distribution. The parameters of \( f \) are optimized using the negative log-likelihood (NLL) loss over the validation set. Inspired by this approach, we propose a similar method, learning a lightweight transformation \( f \) to calibrate unreliable vision token attention weights before applying softmax.
% To this end, we introduce the Attention Calibration (AC) framework, a learnable attention adjustment mapping that dynamically prioritizes tokens based on their importance within an image sequence.  This mapping operates within the transformer decoder layers and maintains the same input and output dimensions, i.e., \( \mathbb{R}^D \to \mathbb{R}^D \). Our proposed framework comprises the following two components:
% \begin{enumerate}
%     \item A lightweight plug-and-play learnable vision tokens attention mapping module that recalibrates attention weights to enhance multimodal alignment.
%     \item A contrastive learning strategy that aligns positive sample pairs while separating negative pairs in the embedding space, fostering robust token-level understanding.
% \end{enumerate}
% The DAC module \( f \) operates on the vision token attention weights \( \mathbf{A}_{\text{img}} \), and maps it to a calibrated attention sequence \( \mathbf{A}_{\text{img}}' \) of the same dimensions. Specifically, \( f \) is implemented as a small stack of linear transformations with ReLU activations.
% Each layer \( i \in \{1, 2, \ldots, N\} \) in the DAC module is parameterized by a weight matrix \( \mathbf{W}_i \in \mathbb{R}^{D_{\text{in},i} \times D_{\text{out},i}} \) and a bias vector \( \mathbf{b}_i \in \mathbb{R}^{D_{\text{out},i}} \). Unless otherwise stated, the dimensions of the hidden state are consistent and equal to \( L \) (i.e., \( D_{\text{in},i} = D_{\text{out},i} = L \)), with the depth of DAC set to \( N = 2 \).
% The forward pass of the DAC module is defined as:
% \begin{equation}
% \mathbf{g}_1 = \text{ReLU}(\mathbf{A}_{\text{img}} \mathbf{W}_1 + \mathbf{b}_1),
% \end{equation}
% \begin{equation}
% \mathbf{g}_i = \text{ReLU}(\mathbf{g}_{i-1} \mathbf{W}_i + \mathbf{b}_i), 
% \quad \text{for } i \in \{2, \ldots, N-1\}.
% \end{equation}
% The final output of the DAC module is:
% \begin{equation}
% \mathbf{A}_{\text{img}}' = f(\mathbf{A}_{\text{img}}) = \mathbf{g}_{L-1} \mathbf{W}_L + \mathbf{b}_L, \quad \mathbf{A}_{\text{img}}' \in \mathbb{R}^{B \times H \times L}.
% \end{equation}
% Here, \( \text{ReLU}(\cdot) \) is the rectified linear unit activation function, \( \mathbf{g}_i \) represents the output of the \( i \)-th layer.
% The DAC module can be applied to any layer of the language model decoder, targeting the layers responsible for vision tokens processing.
% \subsection{Implementation Detail}
% The main challenge in learning DAC lies in the lack of sufficient data or labels for effective training. To address this, we propose a novel augmentation and training procedure inspired by the concept of instant discrimination. The key idea is to ensure that the model maintains the same capability of identifying an object regardless of its position within the image.
% \paragraph{Calibration set}

Formally, 
% let \((T, V, Y)\) represent our datasets, where \(T\) is the text,  \(V\) is the image, and \(Y\) is the label.
we select a small portion of the validation set, \(20\%\) of the total validation data, as our calibration set, denoted as \(\mathcal{D}_{\text{cal}}\). Each image \(V \in \mathcal{D}_{\text{cal}}\) is paired with ground-truth annotations and their corresponding bounding boxes.
The calibration set \(\mathcal{D}_{\text{cal}}\) undergoes an augmentation process to produce the augmented dataset \(\mathcal{D}_{\text{aug}}\). Specifically, we crop the ground truth objects from the images using the annotations and bounding boxes provided, then apply random resizing and paste the cropped objects onto a pure white background as \(V_{\text{crop}}\). For each \(V_{\text{crop}}\), we generate balanced positive and negative query-label pairs, ensuring a well-balanced dataset.
Additionally, we include annotations for the cropped images \(V_{\text{crop}}\) to be utilized in instance discrimination tasks, as discussed later in the paper. 
The detailed augmentation process is summarized in the Appendix.

% The augmentation process ensures the dataset contains a diverse set of cropped objects placed at random positions while leaving the background meaningless. This forces the model to focus on the objects themselves rather than their absolute positions within the image or their co-occurrence with other objects. By reducing reliance on positional and contextual cues, the model learns to robustly identify individual objects regardless of spatial transformations or the presence of other elements.

% \paragraph{Training Process}
With sufficient augmented data from \(\mathcal{D}_{\text{aug}}\), we propose leveraging contrastive learning to encourage LVLMs to focus on objects themselves rather than their absolute positions in the image. This approach ensures consistent outputs regardless of object position. By reducing reliance on positional cues, the model learns to robustly identify objects despite spatial transformations. Specifically, contrastive learning is formulated to increase the similarity between embeddings of the same object at different spatial locations while pushing apart the embeddings of different objects. We begin with an \(\mathcal{D}_{\text{aug}}\) dataset and randomly sample a minibatch of \( B \) examples. Each example then undergoes an additional augmentation process, resulting in a total of \( 2B \) augmented data points. Following the approach of \cite{wu2018unsupervised}, for each positive pair, we consider the remaining \( 2(B - 1) \) augmented examples within the minibatch as negative examples.  
Given the embeddings \( z_i \) and \( z_j \) of the positive augmented pair \( \tilde{v}_i \) and \( \tilde{v}_j \), the contrastive loss can be expressed as:
\begin{equation}
\scalebox{0.95}{$ \displaystyle
\ell_{\text{CL}}(i, j) = -\log \frac{\exp\left(\text{sim}(\mathbf{z}_i, \mathbf{z}_j) / \tau \right)}
{\sum_{k=1}^{2B} \mathbbm{1}[k \neq i] \exp\left(\text{sim}(\mathbf{z}_i, \mathbf{z}_k) / \tau \right)},
$}
\end{equation}
where $B$ denotes the number of examples in a minibatch, \(\text{sim}(\cdot, \cdot)\) represents the cosine similarity, \(\mathbbm{1}[k \neq i]\) is an indicator function, and \(\tau\) is the temperature parameter. Combined with a cross-entropy (CE) loss, the final loss function is formulated as
\begin{equation} \label{eq:loss_function}
    \mathcal{L} = \mathcal{L}_{\text{CE}}(F(T_{\text{crop}}, V_{\text{crop}}), Y_{\text{crop}}) + \lambda \mathcal{L}_{\text{CL}},
\end{equation}
where \(F\) represents the model, \(T_{\text{crop}}\) and \(V_{\text{crop}}\) are the query and cropped image, \(Y_{\text{crop}}\) is the corresponding label, and \(\lambda\) is a hyperparameter balancing the two losses. We optimize our DAC using Eq. \ref{eq:loss_function} alongside instruction tuning, while keeping all other components frozen. The overall training process is summarized in Algorithm~\ref{alg:simclr}.

% We optimize our DAC using contrastive learning alongside instruction tuning, while keeping all other components frozen. Contrastive learning is employed to ensure that the embeddings of the same object at different spatial locations are brought closer together, thereby enhancing spatial consistency, while simultaneously pushing the embeddings of different objects further apart. The training process comprises two components: a cross-entropy (CE) loss and a contrastive learning (CL) loss, which are optimized jointly. We retain the CE loss to ensure alignment with instruction tuning. The overall training process is summarized in Algorithm~\ref{alg:simclr}.

% \paragraph{CE loss}
% The CE loss is defined as:
% \begin{equation}
% \mathcal{L}_{\text{CE}} = \text{CE}(F(T_{\text{crop}}, V_{\text{crop}}), Y_{\text{crop}}),
% \label{eq:ce_loss}
% \end{equation}
% where \(F\) represents the model, \(T_{\text{crop}}\) and \(V_{\text{crop}}\) are the query and cropped image, and \(Y_{\text{crop}}\) is the corresponding label. This component ensures the model learns to associate queries with the correct objects in the image.

% \paragraph{CL loss}
% To incorporate the principles of contrastive learning, we follow a traditional contrastive loss framework. For each example in \(\mathcal{D}_{\text{aug}}\), the original image from \(\mathcal{D}_{\text{cal}}\) undergoes the augmentation process again, as detailed in Algorithm~\ref{alg:augmentation}, utilizing the annotations included during augmentation. This results in a pair of augmented views, denoted as \(\tilde{v}_i\) and \(\tilde{v}_j\). A minibatch of \(N\) examples is randomly sampled from \(\mathcal{D}_{\text{aug}}\), producing \(2N\) augmented samples. 

% The contrastive loss for a positive pair \((i, j)\) is computed as:
% \begin{equation}
% \ell_{\text{CL}}(i, j) = -\log \frac{\exp\left(\text{sim}(\mathbf{z}_i, \mathbf{z}_j) / \tau \right)}
% {\sum_{k=1}^{2N} \mathbb{1}[k \neq i] \exp\left(\text{sim}(\mathbf{z}_i, \mathbf{z}_k) / \tau \right)},
% \end{equation}
% where \(\text{sim}(\cdot, \cdot)\) represents the cosine similarity, \(\mathbf{z}_i\) and \(\mathbf{z}_j\) are the embeddings of \(\tilde{v}_i\) and \(\tilde{v}_j\), \(\mathbb{1}[k \neq i]\) is an indicator function that equals 1 if \(k \neq i\), and \(\tau\) is the temperature parameter.

% The final contrastive loss, \(\mathcal{L}_{\text{CL}}\), is computed over all positive pairs \((i, j)\) and \((j, i)\) in the minibatch:
% \begin{equation}
%     \mathcal{L}_{\text{CL}} = \frac{1}{2B} \sum_{k=1}^B \left[ \ell_{\text{CL}}(2k-1, 2k) + \ell_{\text{CL}}(2k, 2k-1) \right].
%     \label{eq:cl_loss}
% \end{equation}

% During fine-tuning, the CE loss and CL loss are optimized jointly as:
% \begin{equation}
% \mathcal{L} = \mathcal{L}_{\text{CE}} + \lambda \mathcal{L}_{\text{CL}},
% \end{equation}
% where \(\lambda\) is a hyperparameter balancing the two losses. This joint optimization enables the model to learn both the alignment between queries and objects and robust feature representations through contrastive learning. 

\begin{algorithm}[tb]
   \caption{DAC’s Main Learning Algorithm}
   \label{alg:simclr}
\begin{algorithmic}
   \STATE {\bfseries Input:} Batch size \(B\), constant \(\tau\), frozen backbone networks \(f(\cdot)\) and projection head \(g(\cdot)\), augmentation distribution \(\mathcal{T}\), augmented set \(\mathcal{D}_{\text{aug}} = \{(T_{\text{aug}}, V_{\text{crop}}, Y_{\text{aug}})\}\)
   \FOR{sampled minibatch \(\{(t_k, v_k, y_k)\}_{k=1}^B \in \mathcal{D}_{\text{aug}}\)}

       \FOR{all \(k \in \{1, \dots, B\}\)}
            \STATE Draw one augmentation function \(t \sim \mathcal{T}\)
            \STATE \textcolor{gray}{\# Original augmentation}
            \STATE \(\tilde{v}_{2k-1} = v_k\)
            \STATE \(z_{2k-1} = f(\tilde{v}_{2k-1})\) \hfill \textcolor{gray}{\# Representation}
            \STATE \(\tilde{y}_{2k-1} = g(z_{2k-1})\) \hfill \textcolor{gray}{\# Prediction}
            \STATE \(y_{2k-1} = y_k\) \hfill \textcolor{gray}{\# Label}
            \STATE \textcolor{gray}{\# The second augmentation}
            \STATE \(\tilde{v}_{2k} = t(v_k)\)
            \STATE \(z_{2k} = f(\tilde{v}_{2k})\) \hfill \textcolor{gray}{\# Representation}
            \STATE \(\tilde{y}_{2k} = g(z_{2k})\) \hfill \textcolor{gray}{\# Prediction}
            \STATE \(y_{2k} = y_k\) \hfill \textcolor{gray}{\# Label}
        \ENDFOR
       \FOR{all \(i \in \{1, \dots, 2B\}\) and \(j \in \{1, \dots, 2B\}\)}
           \STATE \(s_{i,j} = z_i^\top z_j / (\|z_i\| \|z_j\|)\) \hfill \textcolor{gray}{\# Pairwise similarity}
       \ENDFOR    
        \STATE Compute the losses using:
        \[
        \mathcal{L} = \mathcal{L}_{\text{CE}} + \lambda \cdot \mathcal{L}_{\text{CL}}
        \]
       \STATE Update DAC parameters to minimize \(\mathcal{L}\)
   \ENDFOR
   \STATE {\bfseries Return:} Fine-tuned DAC
\end{algorithmic}
\end{algorithm}







\section{Experiment}
\subsection{Setup}
\paragraph{Models and Baselines}
We implement three representative LVLMs for evaluation: LLaVA-1.5 \cite{shang2024llava} and mPLUG-Owl2 \cite{ye2024mplug} at the 7B scale, and LLaVA-NeXT \cite{bo2024llavanext} at the 8B scale. Our method is compared against five methods. Baseline responses are generated using the original LVLMs, while other techniques such as Visual Contrastive Decoding (VCD) \cite{sicong2023vcd}, OPERA \cite{qidong2023opera}, Self-Introspective Decoding (SID) \cite{huo2024sid}, and Concentric Causal Attention (CCA) \cite{xing2024cca} are included for comparative analysis. Additional details on the compared methods are provided in the Appendix.


\paragraph{Experiment Settings}
Unless otherwise specified, we integrate the DAC module into two consecutive layers of the language model decoder. For the POPE COCO dataset, we select 20\% of the validation set, comprising 100 images, as our calibration set  $D_\text{cal}$ and report results on the remaining 80\% of the validation set. The optimal layers are determined based on performance on $D_\text{cal}$. For other datasets, we report results trained on the POPE COCO Random dataset. For each image, we select up to three ground truth objects; if an image contains fewer than three objects, all available objects are included. Using these ground truth objects, we generate 10 cropped images per object, resulting in a dataset of approximately 5.4K \((T, V, Y)\) pairs. By default, the contrastive loss strength \( \lambda \) is set to 0.01. 


We fine-tune our module on LLaVA-1.5 on the proposed \( D_\text{aug} \) dataset using a learning rate of $3 \times 10^{-6}$ and a batch size of 8, and gradient accumulate steps of 4. The training is conducted on two NVIDIA RTX 4090 GPUs with approximately 40 minutes. See Appendix for additional training details.

\begin{table*}[t!] % Use table* for spanning the entire page width
\centering % Center the table
\renewcommand{\arraystretch}{1.1} % Adjust row spacing for better readability
\begin{tabular}{p{1.4cm} p{1.9cm} p{1.2cm}|cc|cc|cc} % Define columns with desired layout
\hline
\multicolumn{3}{c|}{Setting} & \multicolumn{2}{c|}{Random} & \multicolumn{2}{c|}{Popular} & \multicolumn{2}{c}{Adversarial} \\
\hline
Dataset & Model & Method & Accuracy$\uparrow$ & F1 Score$\uparrow$ & Accuracy$\uparrow$ & F1 Score$\uparrow$ & Accuracy$\uparrow$ & F1 Score$\uparrow$ \\
\hline
\multirow{16}{*}{MSCOCO} 
 & \multirow{6}{*}{LLaVA1.5} 
 & Baseline & 89.41 & 89.32 & 85.33 & 85.77 & 78.92 & 80.75 \\
 &  & VCD & 87.53 & 87.81 & 84.43 & 85.20 & 78.13 & 80.38 \\
 &  & OPERA & 89.87 & 89.95 & 86.30 & 86.88 & 79.77 & 81.77 \\
 &  & SID & 89.38 & 89.00 & 85.45 & 85.01 & 80.25 & 81.28 \\
 &  & CCA & 89.77 & 89.05 & 86.45 & 86.02 & 83.97 & 83.82 \\
 &  & DAC & \textbf{90.83} & \textbf{90.60} & \textbf{89.50} & \textbf{89.10} & \textbf{84.12} & \textbf{84.42}\\
\cline{2-9}
 & \multirow{5}{*}{mPLUG-Owl2} 
 & Baseline & 86.27 & 86.88 & 80.73 & 82.52 & 76.17 & 77.69 \\
 &  & VCD & 84.40 & 84.79 & 81.00 & 81.12 & 77.10 & 77.00 \\
 &  & OPERA & 86.23 & 86.84 & 80.70 & 82.48 & 76.87 & 78.01 \\
 &  & SID & 86.13 & 86.69 & 81.20 & 82.77 & 77.25 & 79.84 \\
 &  & DAC & \textbf{87.71} & \textbf{87.57} & \textbf{84.96} & \textbf{84.46} & \textbf{82.58} & \textbf{82.32} \\
\cline{2-9}
 & \multirow{5}{*}{LLaVA-NeXT} 
 & Baseline & 89.37 & 88.82 & 83.68 & 84.62 & 80.08 & 80.74 \\
 &  & VCD & 87.83 & 87.09 & 82.68 & 83.55 & 79.61 &  81.20 \\
 &  & OPERA & 89.36 & 88.80 & 83.65 & 84.60 & 80.10 & 80.75 \\
 &  & SID & 90.05 & 89.97 & 86.13 &  85.69 & 84.06 & 82.95 \\
 &  & DAC & \textbf{90.33} & \textbf{90.71} & \textbf{88.25} & \textbf{87.69} & \textbf{84.38} & \textbf{84.38} \\
\hline
\multirow{16}{*}{A-OKVQA} 
 & \multirow{6}{*}{LLaVA1.5} 
 & Baseline & 87.30 & 88.49 & 80.30 & 83.21 &  69.33 & 76.10 \\
 &  & VCD & 85.00 & 86.49 & 77.50 & 81.07 & 67.90 & 75.01 \\
 &  & OPERA & 87.27 & 88.50 & 80.47 & 83.38 & 69.20 & 76.09 \\
 &  & SID & 86.92 & 87.64 & 81.63 & 83.46 & 72.24 & 70.08 \\
  &  & CCA & \textbf{90.00} & 90.11 & \textbf{85.45} & 85.01 & 74.77 & 78.32 \\
 &  & DAC & 89.70 & \textbf{90.33} & 83.96 & \textbf{85.52} & \textbf{75.42} & \textbf{79.21} \\
\cline{2-9}
 & \multirow{5}{*}{mPLUG-Owl2} 
 & Baseline & 81.57 & 83.89 & 75.97 & 79.98 & 67.37 & 74.63 \\
 &  & VCD & 82.53 & 84.16 & 75.70 & 79.21 & 68.80 & 74.85 \\
 &  & OPERA & 81.53 & 83.86 & 75.93 & 79.94 & 67.30 & 74.58 \\
 &  & SID & 86.13 & 84.98 & 77.21 & 80.69 & 68.51 & 75.12 \\
 &  & DAC & \textbf{86.56} & \textbf{87.24} & \textbf{82.83} & \textbf{83.47} & \textbf{75.88} & \textbf{77.78} \\
\cline{2-9}
 & \multirow{5}{*}{LLaVA-NeXT} 
 & Baseline & 86.54 & 85.70 & 84.75 & 84.10 & 75.50 & 76.70 \\
 &  & VCD & 86.21 & 85.41 & 84.29 & 83.71 & 75.58 & 76.86 \\
 &  & OPERA & 86.33 & 85.62 & 85.11 & 84.97 & 75.81 & 76.77 \\
 &  & SID & 86.41 & 85.43 & 84.83 & 84.01 & 76.08 & 76.91 \\
 &  & DAC & \textbf{90.67} & \textbf{90.71} & \textbf{86.21} & \textbf{86.84} & \textbf{77.04} & \textbf{79.31} \\
\hline
\end{tabular}
\caption{POPE results. Results are sourced from published papers or re-implemented using official code. The best performance within each setting is highlighted in \textbf{bold}.}
\label{tab:pope}
\end{table*}

\begin{table}[t]
\centering
\renewcommand{\arraystretch}{1.1} % Slightly reduce row spacing for compactness
\setlength{\tabcolsep}{4pt} % Reduce column spacing to make it narrower
\begin{tabularx}{0.8\columnwidth}{p{1.5cm}|XX|XX}
\hline
\multirow{2}{*}{Setting} & \multicolumn{2}{c|}{LLaVA1.5} & \multicolumn{2}{c}{LLaVA-NeXT} \\
 & $C_S$$\downarrow$  & $C_I$$\downarrow$  & $C_S$$\downarrow$  & $C_I$$\downarrow$  \\
\hline
Baseline     & 51.3 & 16.8 & 42.6 & 14.1 \\
VCD          & 48.0 & 14.3 & 41.3 & 12.9 \\
OPERA        & 45.2 & 12.7 & 39.4 & 11.8 \\
SID          & 45.0 & \textbf{11.7} & 38.4 & 11.4 \\
CCA          & 48.6 & 13.4 & - & - \\
DAC           & \textbf{30.8} & 12.7 & \textbf{21.4} & \textbf{10.2} \\
\hline
\end{tabularx}
\caption{CHAIR results on 500 randomly sampled MSCOCO images with a maximum sequence length of 512 tokens. The best performance within each setting is highlighted in \textbf{bold}.}
\label{tab:chair}
\end{table}

\begin{table}[t]
\centering
\renewcommand{\arraystretch}{1.1} % Slightly reduce row spacing for compactness
\setlength{\tabcolsep}{4pt} % Reduce column spacing to make it narrower
\begin{tabularx}{1\columnwidth}{p{1.1cm}|XX|XX|X}
\hline
\multirow{2}{*}{Setting} & \multicolumn{2}{c|}{Object-level} & \multicolumn{2}{c|}{Attribute-level} & \multirow{2}{*}{Total$\uparrow$} \\
 & \textit{existence}$\uparrow$ & \textit{count}$\uparrow$ & \textit{position}$\uparrow$ & \textit{color}$\uparrow$ & \\
\hline
Baseline     & 175.67 & 124.67 & 114.00 & 151.00 & 565.33 \\
VCD          & 184.66 & 138.33 & 128.67 & 153.00 & 604.66 \\
OPERA        & 180.67 & 133.33 & 123.33 & 155.00 & 592.33 \\
SID          & 190.00 & 148.33 & 128.33 & \textbf{175.00} & 641.66 \\
CCA          & 190.00 & 148.33 & 128.33 & \textbf{175.00} & 641.66 \\
DAC           & \textbf{195.00} & \textbf{158.33} & \textbf{133.33} & 170.00 & \textbf{656.67} \\
\hline
\end{tabularx}
\caption{MME hallucination subset results. The best performance within each setting is highlighted in \textbf{bold}.}
\label{tab:mme}

\end{table}




\subsection{Evaluation Results}
\paragraph{POPE} Polling-based Object Probing Evaluation (POPE) \cite{pope} is a method designed to assess object hallucination in Large Vision-Language Models (LVLMs). It evaluates model performance by querying the presence of specific objects in images using yes-or-no questions. POPE employs three strategies for sampling negative objects: Random, Popular, and Adversarial (refer to \cite{pope} for details). Our evaluation utilizes two datasets: COCO \cite{coco} and A-OKVQA \cite{aokvqa}. For each evaluation setup, every subset includes 3,000 questions across 500 images, resulting in a total of 18,000 yes-or-no questions. The evaluation pivots on two key metrics: Accuracy and the F1 score. Our method achieves the highest accuracy and F1 scores across most datasets and sampling setups, as shown in Table~\ref{tab:pope}. Specifically, DAC delivers an average improvement of 1.36\% in accuracy and 1.90\% in F1 score for Random sampling, 2.96\% in accuracy and 2.39\% in F1 score for Popular sampling, and 3.39\% in accuracy and 2.22\% in F1 score for Adversarial sampling, compared to the next best approach. Our method achieves more significant improvements in the more challenging Adversarial sampling setting because it effectively downplays visual cues unrelated to the object itself.





\paragraph{CHAIR} The Caption Hallucination Assessment with Image Relevance (CHAIR) metric \cite{anna2018chair} is specifically designed to assess object hallucinations in image captioning tasks. CHAIR quantifies the degree of hallucinations in a generated image caption by calculating the proportion of objects mentioned in the caption that are not present in the ground truth label pool. Two common variants of CHAIR are defined: CHAIR$_S$ ($C_S$) and CHAIRS$_I$ ($C_I$), which measure hallucination at the instance and sentence levels, respectively. These metrics are formulated as follows:
\[
\scalebox{0.99}{$
C_S = \frac{| \text{hallucinated objects} |}{| \text{all mentioned objects} |}, \quad 
C_I = \frac{| \text{captions with hallucinated objects} |}{| \text{all captions} |}
$}
\]
Lower values of \(C_S\) and \(C_I\) indicate better performances. Following \cite{qidong2023opera,huo2024sid}, we randomly select 500 images from the validation set of COCO 2014 and query various LVLMs using the prompt: \textit{``Please describe this image in detail.''} To ensure a fair evaluation, we limit the maximum number of new tokens to 512 when generating descriptions.
As shown in Table~\ref{tab:chair}, our method, DAC, consistently outperforms other methods in most cases. Notably, on CHAIR$_S$, DAC achieves a significant improvement, reducing the hallucination rate by an average of 37.07\% across models compared to the next best approach. The superior performance of DAC on CHAIR metrics showcases its capability to effectively mitigate hallucinations in open-ended generation settings.


\paragraph{MME} The MME benchmark \cite{chaoyou2023mme} provides a comprehensive framework for evaluating LVLMs across multiple dimensions. It includes ten perception-related subtasks and four cognition-focused tasks. Following \cite{sicong2023vcd,shukang2023woodpecker}, we evaluate four perception subtasks that assess object-level and attribute-level hallucinations, specifically measuring object existence, count, position, and color. Table~\ref{tab:mme} presents the performance of our method, DAC, on the MME hallucination subset using LLaVA-1.5. DAC achieves a notable improvement of 16.16\% over the baseline and 2.34\% over the current state-of-the-art hallucination mitigation approaches, demonstrating its effectiveness in enhancing the general perception capabilities of LVLMs. Detailed results for the full MME benchmark are provided in the Appendix.

\paragraph{GPT4V-Aided Evaluation}
We evaluate our approach on LLaVA-Bench~\cite{liu2024visual}, a benchmark comprising 30 images paired with a total of 90 questions. LLaVA-Bench is designed to assess the ability of models to generate coherent and contextually accurate responses for vision-language tasks. It categorizes questions into three types: conversation, detailed description, and complex reasoning. Following prior works~\cite{liu2024visual,qidong2023opera}, we prompt these models to generate responses and use the text-only GPT-4~\cite{achiam2023gpt4} as the judge to rate these responses. The results on LLaVA-1.5 are presented in Table~\ref{tab:llavabench}. Our method demonstrates strong performance across all question type. These results highlight the effectiveness of our approach at preserving language understanding and generation capabilities while significantly mitigating object hallucination. 

\paragraph{Uniform Attention Calibration results}
To address the SPB inherent in LVLMs, we propose a training-free method UAC in Sec. \ref{sec:training_free}. UAC recalibrates biased attention by estimating SPB from a meaningless input. We evaluate this method using LLaVA-1.5 on the POPE MSCOCO, CHAIR, and MME benchmarks, following the same experimental setup as in our other comparisons. As summarized in \ref{tab:uac}, UAC achieves the best overall performance on POPE MSCOCO compared to current state-of-the-art methods, surpassing other training-free approaches by a substantial margin. On the MME dataset, UAC attains competitive results. However, on the open-ended generation benchmark CHAIR, UAC falls short of the top performers. We attribute this to its reliance on a single meaningless image bias for calibration, which, while effective for structured tasks, may degrade generation quality in open-ended settings by limiting the model's ability to adapt to diverse contextual variations. Notably, UAC incurs no inference overhead since its parameters remain fixed across all tasks. In contrast, other training-free methods, including VCD, OPERA, and SID, impose significant inference costs—approximately $2\times$, $5\times$, and $1.7\times$, respectively \cite{huo2024sid}. These results highlight UAC as an effective and efficient solution for structured tasks, while DAC continues to provide state-of-the-art performance with minimal computational overhead.





\begin{table}[t]
\centering
\renewcommand{\arraystretch}{1.1} % Slightly reduce row spacing for compactness
\setlength{\tabcolsep}{4pt} % Reduce column spacing to make it narrower
\begin{tabular}{l|cccc}
\hline
{Method} & {Complex}$\uparrow$ & {Details}$\uparrow$ & {Conv}$\uparrow$ & {Average}$\uparrow$\\
\hline
Baseline     & 66.3 & 46.7 & 68.7  & 60.6\\
VCD          & 69.6 & 51.6 & 57.3  & 61.6\\
OPERA        & 66.4 & \textbf{56.9} & 44.0  & 61.3\\
SID          & 66.7 & 51.3 & 66.3  & 60.4\\
CCA          & 66.1 & 53.9 & 69.4  & \textbf{64.3}\\
DAC           & \textbf{70.3} & 50.0 & \textbf{72.7} & \textbf{64.3}\\
\hline
\end{tabular}
\caption{LLaVA-Bench results. The results are re-implemented using the official code and evaluated with the latest available text-only GPT-4 API. Scores are normalized by the total possible score. The best performances within each setting are highlighted in \textbf{bold}.}

\label{tab:llavabench}
\end{table}

\begin{table}[t]
\centering
\renewcommand{\arraystretch}{1.1} % Slightly reduce row spacing for compactness
\setlength{\tabcolsep}{4pt} % Reduce column spacing to make it narrower
\begin{tabular}{l|ccc|cc|c}
\hline
\multirow{2}{*}{Setting} & \multicolumn{3}{c|}{POPE MSCOCO} & \multicolumn{2}{c|}{CHAIR} & \multirow{2}{*}{MME$\uparrow$}  \\
 & \textit{Rnd}$\uparrow$ & \textit{Pop}$\uparrow$& \textit{Adv}$\uparrow$ & $C_S$$\downarrow$  & $C_i$$\downarrow$  & \\
\hline
Baseline     & 89.3  & 85.8 & 80.8 & 51.3 & 16.8 & 565.3 \\
VCD          & 87.8  & 85.2 & 80.4 & 48.0 & 14.3 & 604.7 \\
OPERA        & 90.0  & 86.9 & 81.8 & 45.2 & 12.7 & 592.3 \\
SID          & 89.0  & 85.0 & 81.3 & 45.0 & \textbf{11.7} & 641.7 \\
CCA          & 89.1  & 86.0 & 83.8 & 48.6 & 13.4 & 641.7   \\
UAC          & 90.2  & 87.6 & 83.7 & 49.0 & 14.9 & 638.3   \\
DAC          & \textbf{90.6}  & \textbf{89.1} & \textbf{84.4} & \textbf{30.8} & 12.7 & \textbf{656.7}   \\
\hline
\end{tabular}
\caption{Results on POPE MSCOCO, CHAIR, and MME hallucination subsets. ``Rnd" ``Pop" and ``Adv" represent the Random, Popular, and Adversarial settings, respectively. On POPE MSCOCO, results are reported as F1 scores. The best performances within each settings are highlighted in \textbf{bold}.}

\label{tab:uac}
\end{table}


\subsection{Ablation Study}

In this section, we present a detailed ablation study on key hyperparameters, focusing on two critical components: the contrastive loss strength \(\lambda\) and the decoder layers \(N_{DAC}\) to which DAC is applied. As shown in Figure~\ref{fig:abl}, DAC exhibits robustness across various settings and consistently outperforms the baseline.

To maintain effective language generation, we avoid setting \(\lambda\) too large, as excessively high values lead to degradation in the model’s generative capabilities. Conversely, when \(\lambda = 0\), meaning the model is fine-tuned solely on the CE loss, it yields the lowest performance among the tested settings. This result highlights the importance of the contrastive learning component. Our experiments indicate that \(\lambda = 0.01\) achieves the best performance, with further adjustments yielding negligible differences. For simplicity and consistency, we adopt \(\lambda = 0.01\) throughout this study.

DAC demonstrates significant improvements across various layer configurations. Following established calibration practices \cite{guo2017calibration,mukhoti2020focal}, we train DAC using the augmented dataset \(D_\text{aug}\) and utilize the calibration dataset \(D_\text{cal}\) as the validation set to determine the optimal \(N_{DAC}\), thereby preventing data leakage.


\begin{figure}[t]
\begin{center}
\includegraphics[width=0.9\columnwidth]{image/ablation.pdf}
\caption{DAC performance under different settings of \(\lambda\) and \(N_{DAC}\). Different lines represent various \(\lambda\) values, while the y-axis indicates \(N_{DAC}\). DAC is applied to 2 consecutive layers, and the results average sampling settings for POPE accuracy using LLaVA1.5-7B.}
\label{fig:abl}
\end{center}
\vspace{-1em}
\end{figure}

% \subsection{Computation Efficiency}  
% A major concern in hallucination mitigation methods is their computational efficiency compared to training-free, decoding-only approaches. AC requires two 4090 GPUs for training, with a training time of approximately 40 minutes and an inference time of 30 minutes. This translates to roughly \(1.3 + 0.5 = 1.8\) GPU hours in total. Despite the slight increase in computational cost, our method delivers substantial improvements in both accuracy and F1 score, making it a highly efficient solution for hallucination mitigation. We report the computational efficiency of different methods in GPU hours on 4090 GPU, as shown in Table~\ref{tab:comp}.

% \begin{table}[h]
% \centering
% \renewcommand{\arraystretch}{1.1} % Slightly reduce row spacing for compactness
% \setlength{\tabcolsep}{4pt} % Reduce column spacing to make it narrower
% \begin{tabular}{lccc}
% \hline
% {Method} & {GPU Hours} & {Accuracy} & {F1 Score} \\
% \hline
% Baseline     & 0.5 & 84.55 & 85.28  \\
% VCD          & 1.0 & 83.36 & 84.46  \\
% OPERA        & 2.5 & 85.31 & 86.20  \\
% SID          & 0.8 & 85.03 & 85.10  \\
% AC           & 1.8 & 89.68 & 88.40  \\
% \hline
% \end{tabular}
% \caption{Computation efficiency comparison of hallucination mitigation methods on 4090 GPU. Accuracy and F1 are average sampling settings for POPE COCO using LLaVA1.5-7B.}
% \label{tab:comp}
% \end{table}

\section{Conclusion and Limitation} 
In this paper, we investigated object hallucination in LVLMs and identified an inherent imbalance in vision token attention within these models, particularly in spatial positioning, as a key contributing factor. Our findings highlight that such imbalances amplify SPB, thereby increasing the likelihood of hallucinations. To address this challenge, we introduced Uniform Attention Calibration (UAC) and Dynamic Attention Calibration (DAC), two efficient techniques designed for vision token attention correction.
UAC offers a training-free solution that estimates SPB using a meaningless input and applies a calibration matrix to mitigate attention imbalances. DAC, on the other hand, is a learnable module that dynamically refines attention weights within the self-attention mechanism. Through extensive evaluations on multiple benchmarks and LVLM architectures, our methods consistently reduce hallucination while improving general perception. These results emphasize the significance of attention calibration in enhancing LVLM reliability and performance, providing a promising direction for future research in mitigating hallucination-related biases.
\paragraph{Limitation} Although this study employs a data-efficient approach to introducing SPB, its effectiveness may be limited when validation data is scarce. A promising direction for future work is the development of fine-grained, data-free calibration techniques and extend the idea of attention calibration to improve spatial perception in LVLMs.

% The text of the paper should be formatted in two columns, with an
% overall width of 6.75~inches, height of 9.0~inches, and 0.25~inches
% between the columns. The left margin should be 0.75~inches and the top
% margin 1.0~inch (2.54~cm). The right and bottom margins will depend on
% whether you print on US letter or A4 paper, but all final versions
% must be produced for US letter size.
% Do not write anything on the margins.

% The paper body should be set in 10~point type with a vertical spacing
% of 11~points. Please use Times typeface throughout the text.

% \subsection{Title}

% The paper title should be set in 14~point bold type and centered
% between two horizontal rules that are 1~point thick, with 1.0~inch
% between the top rule and the top edge of the page. Capitalize the
% first letter of content words and put the rest of the title in lower
% case.

% \subsection{Author Information for Submission}
% \label{author info}

% ICML uses double-blind review, so author information must not appear. If
% you are using \LaTeX\/ and the \texttt{icml2025.sty} file, use
% \verb+\icmlauthor{...}+ to specify authors and \verb+\icmlaffiliation{...}+ to specify affiliations. (Read the TeX code used to produce this document for an example usage.) The author information
% will not be printed unless \texttt{accepted} is passed as an argument to the
% style file.
% Submissions that include the author information will not
% be reviewed.

% \subsubsection{Self-Citations}

% If you are citing published papers for which you are an author, refer
% to yourself in the third person. In particular, do not use phrases
% that reveal your identity (e.g., ``in previous work \cite{langley00}, we
% have shown \ldots'').

% Do not anonymize citations in the reference section. The only exception are manuscripts that are
% not yet published (e.g., under submission). If you choose to refer to
% such unpublished manuscripts \cite{anonymous}, anonymized copies have
% to be submitted
% as Supplementary Material via OpenReview\@. However, keep in mind that an ICML
% paper should be self contained and should contain sufficient detail
% for the reviewers to evaluate the work. In particular, reviewers are
% not required to look at the Supplementary Material when writing their
% review (they are not required to look at more than the first $8$ pages of the submitted document).

% \subsubsection{Camera-Ready Author Information}
% \label{final author}

% If a paper is accepted, a final camera-ready copy must be prepared.
% %
% For camera-ready papers, author information should start 0.3~inches below the
% bottom rule surrounding the title. The authors' names should appear in 10~point
% bold type, in a row, separated by white space, and centered. Author names should
% not be broken across lines. Unbolded superscripted numbers, starting 1, should
% be used to refer to affiliations.

% Affiliations should be numbered in the order of appearance. A single footnote
% block of text should be used to list all the affiliations. (Academic
% affiliations should list Department, University, City, State/Region, Country.
% Similarly for industrial affiliations.)

% Each distinct affiliations should be listed once. If an author has multiple
% affiliations, multiple superscripts should be placed after the name, separated
% by thin spaces. If the authors would like to highlight equal contribution by
% multiple first authors, those authors should have an asterisk placed after their
% name in superscript, and the term ``\textsuperscript{*}Equal contribution"
% should be placed in the footnote block ahead of the list of affiliations. A
% list of corresponding authors and their emails (in the format Full Name
% \textless{}email@domain.com\textgreater{}) can follow the list of affiliations.
% Ideally only one or two names should be listed.

% A sample file with author names is included in the ICML2025 style file
% package. Turn on the \texttt{[accepted]} option to the stylefile to
% see the names rendered. All of the guidelines above are implemented
% by the \LaTeX\ style file.

% \subsection{Abstract}

% The paper abstract should begin in the left column, 0.4~inches below the final
% address. The heading `Abstract' should be centered, bold, and in 11~point type.
% The abstract body should use 10~point type, with a vertical spacing of
% 11~points, and should be indented 0.25~inches more than normal on left-hand and
% right-hand margins. Insert 0.4~inches of blank space after the body. Keep your
% abstract brief and self-contained, limiting it to one paragraph and roughly 4--6
% sentences. Gross violations will require correction at the camera-ready phase.

% \subsection{Partitioning the Text}

% You should organize your paper into sections and paragraphs to help
% readers place a structure on the material and understand its
% contributions.

% \subsubsection{Sections and Subsections}

% Section headings should be numbered, flush left, and set in 11~pt bold
% type with the content words capitalized. Leave 0.25~inches of space
% before the heading and 0.15~inches after the heading.

% Similarly, subsection headings should be numbered, flush left, and set
% in 10~pt bold type with the content words capitalized. Leave
% 0.2~inches of space before the heading and 0.13~inches afterward.

% Finally, subsubsection headings should be numbered, flush left, and
% set in 10~pt small caps with the content words capitalized. Leave
% 0.18~inches of space before the heading and 0.1~inches after the
% heading.

% Please use no more than three levels of headings.

% \subsubsection{Paragraphs and Footnotes}

% Within each section or subsection, you should further partition the
% paper into paragraphs. Do not indent the first line of a given
% paragraph, but insert a blank line between succeeding ones.

% You can use footnotes\footnote{Footnotes
% should be complete sentences.} to provide readers with additional
% information about a topic without interrupting the flow of the paper.
% Indicate footnotes with a number in the text where the point is most
% relevant. Place the footnote in 9~point type at the bottom of the
% column in which it appears. Precede the first footnote in a column
% with a horizontal rule of 0.8~inches.\footnote{Multiple footnotes can
% appear in each column, in the same order as they appear in the text,
% but spread them across columns and pages if possible.}

% \begin{figure}[ht]
% \vskip 0.2in
% \begin{center}
% \centerline{\includegraphics[width=\columnwidth]{icml_numpapers}}
% \caption{Historical locations and number of accepted papers for International
% Machine Learning Conferences (ICML 1993 -- ICML 2008) and International
% Workshops on Machine Learning (ML 1988 -- ML 1992). At the time this figure was
% produced, the number of accepted papers for ICML 2008 was unknown and instead
% estimated.}
% \label{icml-historical}
% \end{center}
% \vskip -0.2in
% \end{figure}

% \subsection{Figures}

% You may want to include figures in the paper to illustrate
% your approach and results. Such artwork should be centered,
% legible, and separated from the text. Lines should be darkAugmentation Process  and at
% least 0.5~points thick for purposes of reproduction, and text should
% not appear on a gray background.

% Label all distinct components of each figure. If the figure takes the
% form of a graph, then give a name for each axis and include a legend
% that briefly describes each curve. Do not include a title inside the
% figure; instead, the caption should serve this function.

% Number figures sequentially, placing the figure number and caption
% \emph{after} the graphics, with at least 0.1~inches of space before
% the caption and 0.1~inches after it, as in
% \cref{icml-historical}. The figure caption should be set in
% 9~point type and centered unless it runs two or more lines, in which
% case it should be flush left. You may float figures to the top or
% bottom of a column, and you may set wide figures across both columns
% (use the environment \texttt{figure*} in \LaTeX). Always place
% two-column figures at the top or bottom of the page.

% \subsection{Algorithms}

% If you are using \LaTeX, please use the ``algorithm'' and ``algorithmic''
% environments to format pseudocode. These require
% the corresponding stylefiles, algorithm.sty and
% algorithmic.sty, which are supplied with this package.
% \cref{alg:example} shows an example.

% \begin{algorithm}[tb]
%    \caption{Bubble Sort}
%    \label{alg:example}
% \begin{algorithmic}
%    \STATE {\bfseries Input:} data $x_i$, size $m$
%    \REPEAT
%    \STATE Initialize $noChange = true$.
%    \FOR{$i=1$ {\bfseries to} $m-1$}
%    \IF{$x_i > x_{i+1}$}
%    \STATE Swap $x_i$ and $x_{i+1}$
%    \STATE $noChange = false$
%    \ENDIF
%    \ENDFOR
%    \UNTIL{$noChange$ is $true$}
% \end{algorithmic}
% \end{algorithm}

% \subsection{Tables}

% You may also want to include tables that summarize material. Like
% figures, these should be centered, legible, and numbered consecutively.
% However, place the title \emph{above} the table with at least
% 0.1~inches of space before the title and the same after it, as in
% \cref{sample-table}. The table title should be set in 9~point
% type and centered unless it runs two or more lines, in which case it
% should be flush left.

% % Note use of \abovespace and \belowspace to get reasonable spacing
% % above and below tabular lines.


% Tables contain textual material, whereas figures contain graphical material.
% Specify the contents of each row and column in the table's topmost
% row. Again, you may float tables to a column's top or bottom, and set
% wide tables across both columns. Place two-column tables at the
% top or bottom of the page.

% \subsection{Theorems and such}
% The preferred way is to number definitions, propositions, lemmas, etc. consecutively, within sections, as shown below.
% \begin{definition}
% \label{def:inj}
% A function $f:X \to Y$ is injective if for any $x,y\in X$ different, $f(x)\ne f(y)$.
% \end{definition}
% Using \cref{def:inj} we immediate get the following result:
% \begin{proposition}
% If $f$ is injective mapping a set $X$ to another set $Y$, 
% the cardinality of $Y$ is at least as large as that of $X$
% \end{proposition}
% \begin{proof} 
% Left as an exercise to the reader. 
% \end{proof}
% \cref{lem:usefullemma} stated next will prove to be useful.
% \begin{lemma}
% \label{lem:usefullemma}
% For any $f:X \to Y$ and $g:Y\to Z$ injective functions, $f \circ g$ is injective.
% \end{lemma}
% \begin{theorem}
% \label{thm:bigtheorem}
% If $f:X\to Y$ is bijective, the cardinality of $X$ and $Y$ are the same.
% \end{theorem}
% An easy corollary of \cref{thm:bigtheorem} is the following:
% \begin{corollary}
% If $f:X\to Y$ is bijective, 
% the cardinality of $X$ is at least as large as that of $Y$.
% \end{corollary}
% \begin{assumption}
% The set $X$ is finite.
% \label{ass:xfinite}
% \end{assumption}
% \begin{remark}
% According to some, it is only the finite case (cf. \cref{ass:xfinite}) that is interesting.
% \end{remark}
% %restatable

% \subsection{Citations and References}

% Please use APA reference format regardless of your formatter
% or word processor. If you rely on the \LaTeX\/ bibliographic
% facility, use \texttt{natbib.sty} and \texttt{icml2025.bst}
% included in the style-file package to obtain this format.

% Citations within the text should include the authors' last names and
% year. If the authors' names are included in the sentence, place only
% the year in parentheses, for example when referencing Arthur Samuel's
% pioneering work \yrcite{Samuel59}. Otherwise place the entire
% reference in parentheses with the authors and year separated by a
% comma \cite{Samuel59}. List multiple references separated by
% semicolons \cite{kearns89,Samuel59,mitchell80}. Use the `et~al.'
% construct only for citations with three or more authors or after
% listing all authors to a publication in an earlier reference \cite{MachineLearningI}.

% Authors should cite their own work in the third person
% in the initial version of their paper submitted for blind review.
% Please refer to \cref{author info} for detailed instructions on how to
% cite your own papers.

% Use an unnumbered first-level section heading for the references, and use a
% hanging indent style, with the first line of the reference flush against the
% left margin and subsequent lines indented by 10 points. The references at the
% end of this document give examples for journal articles \cite{Samuel59},
% conference publications \cite{langley00}, book chapters \cite{Newell81}, books
% \cite{DudaHart2nd}, edited volumes \cite{MachineLearningI}, technical reports
% \cite{mitchell80}, and dissertations \cite{kearns89}.

% Alphabetize references by the surnames of the first authors, with
% single author entries preceding multiple author entries. Order
% references for the same authors by year of publication, with the
% earliest first. Make sure that each reference includes all relevant
% information (e.g., page numbers).

% Please put some effort into making references complete, presentable, and
% consistent, e.g. use the actual current name of authors.
% If using bibtex, please protect capital letters of names and
% abbreviations in titles, for example, use \{B\}ayesian or \{L\}ipschitz
% in your .bib file.

% \section*{Accessibility}
% Authors are kindly asked to make their submissions as accessible as possible for everyone including people with disabilities and sensory or neurological differences.
% Tips of how to achieve this and what to pay attention to will be provided on the conference website \url{http://icml.cc/}.

% \section*{Software and Data}

% If a paper is accepted, we strongly encourage the publication of software and data with the
% camera-ready version of the paper whenever appropriate. This can be
% done by including a URL in the camera-ready copy. However, do not}
% include URLs that reveal your institution or identity in your
% submission for review. Instead, provide an anonymous URL or upload
% the material as ``Supplementary Material'' into the OpenReview reviewing
% system. Note that reviewers are not required to look at this material
% when writing their review.

% % Acknowledgements should only appear in the accepted version.
% \section*{Acknowledgements}

% Do not} include acknowledgements in the initial version of
% the paper submitted for blind review.

% If a paper is accepted, the final camera-ready version can (and
% usually should) include acknowledgements.  Such acknowledgements
% should be placed at the end of the section, in an unnumbered section
% that does not count towards the paper page limit. Typically, this will 
% include thanks to reviewers who gave useful comments, to colleagues 
% who contributed to the ideas, and to funding agencies and corporate 
% sponsors that provided financial support.

% \section*{Impact Statement}

% Authors are required} to include a statement of the potential 
% broader impact of their work, including its ethical aspects and future 
% societal consequences. This statement should be in an unnumbered 
% section at the end of the paper (co-located with Acknowledgements -- 
% the two may appear in either order, but both must be before References), 
% and does not count toward the paper page limit. In many cases, where 
% the ethical impacts and expected societal implications are those that 
% are well established when advancing the field of Machine Learning, 
% substantial discussion is not required, and a simple statement such 
% as the following will suffice:

% ``This paper presents work whose goal is to advance the field of 
% Machine Learning. There are many potential societal consequences 
% of our work, none which we feel must be specifically highlighted here.''

% The above statement can be used verbatim in such cases, but we 
% encourage authors to think about whether there is content which does 
% warrant further discussion, as this statement will be apparent if the 
% paper is later flagged for ethics review.


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\clearpage
\nocite{langley00}

\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Additional Experimental Settings}
\paragraph{Evaluation Setup}
For comparison, we adopt the default settings for OPERA, VCD, and SID. For CCA, we directly use the provided weights.  
For polling-based tasks (POPE and MME), we employ greedy decoding, while for open-ended generation tasks (CHAIR and LLaVA-Bench), we utilize sampling with Top-$p$ = 1.
\paragraph{DAC Training}
For implementing DAC on LLaVA-1.5, we set the learning rate to $3 \times 10^{-6}$. For LLaVA-NeXT, the learning rate is set to $8 \times 10^{-7}$, while for mPLUG-Owl2, we use a learning rate of $3 \times 10^{-5}$. 

The application of DAC varies across models. For LLaVA-1.5 and LLaVA-NeXT, DAC is applied to the last token before prediction. In contrast, for mPLUG-Owl2, DAC is applied to all tokens after the image starting position. For LLaVA-1.5 and LLaVA-NeXT, DAC consists of two layers with a hidden dimension of 576, which matches both the input and output dimensions. For mPLUG-Owl2, DAC is set to three layers with a hidden dimension of 576 to maintain a similar capacity.


\section{SPB on other blank images}


\begin{figure}[H]
    \centering
    \begin{minipage}{0.25\textwidth}
        \centering
        \vspace{3mm}
        \fbox{\scalebox{0.89}{\includegraphics[width=\linewidth]{image/blank_image.pdf}}} % Scale down to 80%
    \end{minipage}%
    \begin{minipage}{0.75\textwidth}
        \centering
        \includegraphics[width=\linewidth]{image/attention_heatmap_white_poll_scaled.pdf}
    \end{minipage}
    \caption{Vision tokens attention weights during the decoding process for different models on a blank white image in response to the polling prompt: ``Is there a bear in the image?"  }
\end{figure}

\begin{figure}[H]
    \centering
    \begin{minipage}{0.25\textwidth}
        \centering
        \vspace{3mm}
        \fbox{\scalebox{0.89}{\includegraphics[width=\linewidth]{image/black_image.pdf}}} % Scale down to 80%
    \end{minipage}%
    \begin{minipage}{0.75\textwidth}
        \centering
        \includegraphics[width=\linewidth]{image/attention_heatmap_black_poll_scaled.pdf}
    \end{minipage}
    \caption{Vision tokens attention weights during the decoding process for different models on a blank black image in response to the polling prompt: ``Is there a bear in the image?"  }
\end{figure}

\begin{figure}[H]
    \centering
    \begin{minipage}{0.25\textwidth}
        \centering
        \vspace{3mm}
        \fbox{\scalebox{0.89}{\includegraphics[width=\linewidth]{image/noise_image.pdf}}} % Scale down to 80%
    \end{minipage}%
    \begin{minipage}{0.75\textwidth}
        \centering
        \includegraphics[width=\linewidth]{image/attention_heatmap_noise_poll_scaled.pdf}
    \end{minipage}
    \caption{Vision tokens attention weights during the decoding process for different models on a blank noise image in response to the polling prompt: ``Is there a bear in the image?"  }
\end{figure}

\begin{figure}[H]
    \centering
    \begin{minipage}{0.25\textwidth}
        \centering
        \vspace{3mm}
        \fbox{\scalebox{0.89}{\includegraphics[width=\linewidth]{image/actual_image.pdf}}} % Scale down to 80%
    \end{minipage}%
    \begin{minipage}{0.75\textwidth}
        \centering
        \includegraphics[width=\linewidth]{image/attention_heatmap_actual_poll_scaled.pdf}
    \end{minipage}
    \caption{Vision tokens attention weights during the decoding process for different models on an actual image in response to the polling prompt: ``Is there a bear in the image?"  }
\end{figure}

\begin{figure}[H]
    \centering
    \begin{minipage}{0.25\textwidth}
        \centering
        \vspace{3mm}
        \fbox{\scalebox{0.89}{\includegraphics[width=\linewidth]{image/black_image.pdf}}} % Scale down to 80%
    \end{minipage}%
    \begin{minipage}{0.75\textwidth}
        \centering
        \includegraphics[width=\linewidth]{image/attention_heatmap_black_open_scaled.pdf}
    \end{minipage}
    \caption{Vision tokens attention weights during the decoding process for different models on a blank black image in response to the open-ended prompt: ``Please describe the image in detail."  }
\end{figure}

\begin{figure}[H]
    \centering
    \begin{minipage}{0.25\textwidth}
        \centering
        \vspace{3mm}
        \fbox{\scalebox{0.89}{\includegraphics[width=\linewidth]{image/noise_image.pdf}}} % Scale down to 80%
    \end{minipage}%
    \begin{minipage}{0.75\textwidth}
        \centering
        \includegraphics[width=\linewidth]{image/attention_heatmap_noise_open_scaled.pdf}
    \end{minipage}
    \caption{Vision tokens attention weights during the decoding process for different models on a blank noise image in response to the open-ended prompt: ``Please describe the image in detail."  }
\end{figure}

\begin{figure}[H]
    \centering
    \begin{minipage}{0.25\textwidth}
        \centering
        \vspace{3mm}
        \fbox{\scalebox{0.89}{\includegraphics[width=\linewidth]{image/actual_image.pdf}}} % Scale down to 80%
    \end{minipage}%
    \begin{minipage}{0.75\textwidth}
        \centering
        \includegraphics[width=\linewidth]{image/attention_heatmap_actual_open_scaled.pdf}
    \end{minipage}
    \caption{Vision tokens attention weights during the decoding process for different models on an actual image in response to the open-ended prompt: ``Please describe the image in detail."  }
\end{figure}

\section{DAC architecture}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\linewidth]{image/acm.png} % Adjust width as needed
    \caption{The Dynamic Attention Calibration (DAC) architecture consists of a small stack of linear transformations with ReLU activation, operating within the self-attention mechanism of transformer decoder layers to calibrate vision tokens attention.}
    \label{fig:acm}
\end{figure}

\section{Data Augmentation Process}

The augmentation process consists of the following steps:

\begin{itemize}
    \item For each annotated object in \(V\):
    \begin{itemize}
        \item Crop the region defined by its bounding box.
        \item Randomly resize the cropped object to a minimum size of \((H/14) \times (W/14)\) pixels (the typical size of an image patch) and a maximum size of \((H/2) \times (W/2)\), where \(H\) and \(W\) are the height and width of the original image \(V\).
        \item Replace the background of the cropped object with pure white, resulting in \(V_{\text{crop}}\)
    \end{itemize}
    
    \item For each cropped object \(V_{\text{crop}}\):
    \begin{itemize}
        \item Generate a corresponding positive query \(T_{\text{pos}}\) that describes the cropped object and assign the label \(Y_{\text{pos}} = \text{yes}\). Obtaining positive query-label pair: \((T_{\text{pos}}, V_{\text{crop}}, Y_{\text{pos}})\)
        \item Generate a ground-truth negative query \(T_{\text{neg}}\), which refers to an object not present in the image, and assign the label \(Y_{\text{neg}} = \text{no}\). Obtaining negative query-label pair: \((T_{\text{neg}}, V_{\text{crop}}, Y_{\text{neg}})\)
        \item Each cropped image \(V_{\text{crop}}\) results in one positive query-label pair and one negative query-label pair, ensuring a balanced augmented set.
    \end{itemize}
\end{itemize}

Let \(I\) represent the number of original images in the calibration set \(\mathcal{D}_{\text{cal}}\), \(J\) represent the average number of annotated ground-truth objects per image \(V\), and \(K\) represent the number of crops generated per object. 
The total size of the augmented dataset is: \(\text{Total size of } \mathcal{D}_{\text{aug}} = I \cdot J \cdot K \cdot 2\)



\begin{algorithm}[h]
   \caption{Data Augmentation Algorithm}
   \label{alg:augmentation}
\begin{algorithmic}
   \STATE {\bfseries Input:} Calibration set \(\mathcal{D}_{\text{cal}} = \{(T_i, V_i, Y_i)\}_{i=1}^I \subset \mathcal{D}_{\text{val}}\)
   \STATE \(I\): Size of calibration set
   \STATE \(J\): Number of annotations per image
   \STATE \(K\): Number of crops per object
   \STATE Augmented set \(\mathcal{D}_{\text{aug}} = \{\}\)
   \FOR{\(i = 1\) to \(I\)}
       \FOR{\(j = 1\) to \(J\)}
            \FOR{\(k = 1\) to \(K\)}
                \STATE Get \(V_{\text{crop}}\) for object \(j\)
                \STATE Assign \((T_{\text{pos}}, V_{\text{crop}}, Y_{\text{pos}})\) 
                \STATE Assign \((T_{\text{neg}}, V_{\text{crop}}, Y_{\text{neg}})\) 
                \STATE Append both pairs to \(\mathcal{D}_{\text{aug}}\)
            \ENDFOR
        \ENDFOR
   \ENDFOR
   \STATE {\bfseries Return:} \(\mathcal{D}_{\text{aug}} = \{(T_{\text{aug}}, V_{\text{crop}}, Y_{\text{aug}})\}\) with size \(I \cdot J \cdot K \cdot 2\)
\end{algorithmic}
\end{algorithm}


\section{Additional Experimental Results}

\begin{table}[H]
\centering
\renewcommand{\arraystretch}{1.1} % Slightly reduce row spacing for compactness
\setlength{\tabcolsep}{4pt} % Reduce column spacing to make it narrower
\begin{tabular}{l|cccccccccc|c}
\hline
\multirow{2}{*}{Method} & \multicolumn{10}{c|}{Perception} & \multirow{2}{*}{Total}  \\
 & existence & count& position & color  & posters  & celebrity & scene& landmark & artwork &OCR & \\
\hline
UAC     & 190  & 155    & 128.3 & 165 & 145.6 & 136.5 & 158.8 & 163.8 & 124 & 142.5 &1509.4\\
DAC     & 195  & 158.3 & 133.3 & 170 & 140.5 & 131.2 & 161.5 & 165.5& 123.3 &137.5 &1516.0 \\
\hline
\end{tabular}
\caption{Results on full \textbf{MME Perception} subsets}

\label{tab:mme_perception}
\end{table}

\begin{table}[H]
\centering
\renewcommand{\arraystretch}{1.1} % Slightly reduce row spacing for compactness
\setlength{\tabcolsep}{6pt} % Adjust column spacing
\begin{tabular}{l|cccc|c}
\hline
\multirow{2}{*}{Method} & \makecell{Commonsense\\Reasoning} & \makecell{Numerical\\Calculation} & \makecell{Text\\Translation} & \makecell{Code\\Reasoning} & \multirow{2}{*}{Total}  \\
\hline
UAC     & 117.14  & 70 & 115 & 57.5 & 359.6 \\
DAC     & 121  & 57.5 & 85 & 80  & 343.9 \\
\hline
\end{tabular}
\caption{Results on full \textbf{MME Cognition} subsets}
\label{tab:mme_cognition}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
