\section{Related Work}
\subsection{Visual-Language Models}
Large Vision-Language Models (LVLMs) have evolved from early BERT-based architectures ____ to models that integrate Large Language Models (LLMs) ____. Early vision-language models, such as ViLBERT ____ and LXMERT ____, fused visual and textual features through transformer-based architectures. The introduction of LLMs enabled contrastive learning approaches like CLIP ____ and ALIGN ____, improving multimodal adaptability. Recent LVLMs, such as LLaVA ____ and InstructBLIP ____, leverage visual instruction tuning for improved context-aware generation. Advances have further enabled referential dialogues ____, interleaved image-text processing ____, and visual prompts ____, broadening LVLM applications in interactive AI systems. These developments highlight a growing shift toward task-specific fine-tuning and multimodal interaction.

\subsection{Hallucination in VLMs}
Object hallucination arises when Large Vision-Language Models (LVLMs) generate textual descriptions containing objects or attributes not present in the accompanying image ____. This phenomenon is frequently observed in tasks such as image captioning and visual question answering, where maintaining an accurate alignment between visual and textual content is critical. A range of methods has been proposed to address hallucination, from post-hoc correction using external or self-correcting models ____ to enhanced instruction tuning that diversifies training data or aligns outputs with human feedback ____. Recently, training-free approaches that rely on model-based distribution comparisons were proposed ____. As LVLMs grow more sophisticated and versatile, understanding and mitigating object hallucination remains a key focus in multimodal learning research. From a unique perspective, our design is rooted in the correlation between vision tokens attention and object hallucination.