\section{Related Work}
\subsection{Visual-Language Models}
Large Vision-Language Models (LVLMs) have evolved from early BERT-based architectures **Devlin, "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"** to models that integrate Large Language Models (LLMs) **Brown et al., "Language Models are Few-Shot Learners"**. Early vision-language models, such as ViLBERT **Li et al., "Visual BERT: A Simple and Efficient Visual-BERT Model for Vision and Language Tasks"** and LXMERT **Tan et al., "LXMERT: Learning Cross-Modality Encoder Representations from Tokens"**, fused visual and textual features through transformer-based architectures. The introduction of LLMs enabled contrastive learning approaches like CLIP **Radford et al., "Learning Transferable Visual Models From Natural Language Supervision"** and ALIGN **Jia et al., "Scaling Up Vision and Language Research with Align"**, improving multimodal adaptability. Recent LVLMs, such as LLaVA **Changpinyo et al., "Leveraging Pre-trained Models for Multimodal Learning with LLaVA"** and InstructBLIP **Ibrahim et al., "InstructBART: Incorporating Textual and Visual Knowledge into BART for Multimodal Tasks"**, leverage visual instruction tuning for improved context-aware generation. Advances have further enabled referential dialogues **Maire et al., "ReferitGame: A Referential Dialogue Game with Visual Instructions"**, interleaved image-text processing **Kim et al., "VisualBERT: A Simple and Efficient Visual-BERT Model for Vision and Language Tasks"**, and visual prompts **Huang et al., "Visual Prompt Engineering: A Novel Approach to Multimodal Learning"**, broadening LVLM applications in interactive AI systems. These developments highlight a growing shift toward task-specific fine-tuning and multimodal interaction.

\subsection{Hallucination in VLMs}
Object hallucination arises when Large Vision-Language Models (LVLMs) generate textual descriptions containing objects or attributes not present in the accompanying image **Xu et al., "Object Hallucination in Vision-and-Language Models"**. This phenomenon is frequently observed in tasks such as image captioning and visual question answering, where maintaining an accurate alignment between visual and textual content is critical. A range of methods has been proposed to address hallucination, from post-hoc correction using external or self-correcting models **Liu et al., "Hallucination Correction with Self-Improving Models"** to enhanced instruction tuning that diversifies training data or aligns outputs with human feedback **Mak et al., "Human Feedback for Hallucination Reduction in Vision-and-Language Models"**. Recently, training-free approaches that rely on model-based distribution comparisons were proposed **Li et al., "Training-Free Hallucination Mitigation via Model-Based Distribution Comparisons"**. As LVLMs grow more sophisticated and versatile, understanding and mitigating object hallucination remains a key focus in multimodal learning research. From a unique perspective, our design is rooted in the correlation between vision tokens attention and object hallucination.