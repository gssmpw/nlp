\section{Related Work}
\subsection{Visual-Language Models}
Large Vision-Language Models (LVLMs) have evolved from early BERT-based architectures \cite{devlin2018bert,lu2019vilbert,chen2019uniter} to models that integrate Large Language Models (LLMs) \cite{bai2023qwen,brown2020language,gilardi2023chatgpt,raffel2020exploring,taori2023stanford}. Early vision-language models, such as ViLBERT \cite{lu2019vilbert} and LXMERT \cite{tan2019lxmert}, fused visual and textual features through transformer-based architectures. The introduction of LLMs enabled contrastive learning approaches like CLIP \cite{radford2021clip} and ALIGN \cite{jia2021scaling}, improving multimodal adaptability. Recent LVLMs, such as LLaVA \cite{liu2024visual} and InstructBLIP \cite{dai2024instructblip}, leverage visual instruction tuning for improved context-aware generation. Advances have further enabled referential dialogues \cite{chen2023shikra,you2023ferret,zhang2023gpt4roi}, interleaved image-text processing \cite{alayrac2022flamingo,awadalla2023openflamingo}, and visual prompts \cite{peng2023kosmos,zhang2023prompt,chen2023llava}, broadening LVLM applications in interactive AI systems. These developments highlight a growing shift toward task-specific fine-tuning and multimodal interaction.

\subsection{Hallucination in VLMs}
Object hallucination arises when Large Vision-Language Models (LVLMs) generate textual descriptions containing objects or attributes not present in the accompanying image \cite{cui2023holistic,liu2024survey,guan2023hallusionbench,li2023evaluating,wang2024mementos,nie2024mmrel}. This phenomenon is frequently observed in tasks such as image captioning and visual question answering, where maintaining an accurate alignment between visual and textual content is critical. A range of methods has been proposed to address hallucination, from post-hoc correction using external or self-correcting models \cite{shukang2023woodpecker,zhou2024object,lee2023volcano} to enhanced instruction tuning that diversifies training data or aligns outputs with human feedback \cite{liu2024mitigating,yu2024hallucidoctor,sun2023aligning}. Recently, training-free approaches that rely on model-based distribution comparisons were proposed ~\cite{sicong2023vcd, huo2024sid,qidong2023opera}. As LVLMs grow more sophisticated and versatile, understanding and mitigating object hallucination remains a key focus in multimodal learning research. From a unique perspective, our design is rooted in the correlation between vision tokens attention and object hallucination.