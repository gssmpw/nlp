\section{Related Works}
Uncertainty quantification for traditional machine learning problems such as regression or classification has been well studied____. Most previous works on uncertainty quantification for nature language processing (NLP) consider text classifiers____ or text regressors____. To transfer NLP tasks into a classification task, previous work may consider using multi-choice question answering datasets or transferring questions into multi-choice form____. 

However, considering NLP tasks as simple classification tasks overlooks the generation nature of the LLMs____. To overcome this disadvantage, recent works focus on open-ended generation tasks. The first branch of research is inducing the LLMs to output their uncertainty along with the response to solve UQ for open-ended generation tasks____. ____ even fine-tuned LLMs so that LLMs can output better uncertainty. This is a straightforward solution. However, fine-tuning the LLMs to obtain a better uncertainty measure requires white-box access to the models and may cost computation resources. ____ first propose semantic entropy, which calculates entropy considering semantic information. However, such an approach still requires the token-related probability values as input.

To compute uncertainty for black-box MLLMs, previous works take a step further compared with semantic entropy and utilize the similarity and consistency between different generated answers from the same query to the LLMs. ____ uses NLI models to obtain the similarity. Then they treat the similarity matrix as from a weight connected graph and compute uncertainty using the structure of the graph such as using eigenvalues from the graph Laplacian. ____ identify unreliable or speculative answers by computing a confidence score. However, both works only consider semantic similarity, lacking an analysis of the deep meaning of the output. ____ contains a claim level response augmentation. However,  augmented responses share much common information with original responses, and such common information is not considered by ____, causing a potential performance downgrade.  Therefore, in our paper, we do not only generate implicit knowledge behind the original answers but also use tensor decomposition to fully utilize the additional information.  

\vspace{-3mm}