\section{Methodology}
In this section, we introduce our methods: Multi-Dimensional Uncertainty Quantification~(MD-UQ).

\begin{figure*}[h]
    \centering
    \includegraphics[trim=0 360 0 10, clip,width=\textwidth]{images/draw_comprasion.pdf}
    \caption{The overall pipeline of \ours. \ours utilizes tensor decomposition methods to ensure the information from both semantic and knowledge dimension responses could be fully utilized. In the example comparison of semantic dimension and knowledge dimension, we found that though knowledge dimension responses contain implicit knowledge and revise the unreasonable similarity, both dimension responses still share a lot of common information.}
    \vspace{-3mm}
    \label{fig:pipeline}
\end{figure*}

\subsection{Overview}


While our methodological framework generalizes to multiple response dimensions, this work focuses on two well-established and complementary dimensions: the \textit{semantic dimension} capturing surface-level linguistic patterns, and the \textit{knowledge dimension} encoding factual consistency \citep{da2024llm,choi2024fact,vashurin2024benchmarking}. 
The semantic similarity matrix $\mathbf{S}$ is computed using established natural language inference techniques \citep{he2021deberta}, while $\mathbf{S}_K$ leverages auxiliary LLM-generated claim comparisons as detailed in \cref{subsubsec:knowledge}. By jointly analyzing these dimensions through tensor operations, our framework addresses the limitations of single-axis approaches documented in \cref{sec:background}, particularly the inability to distinguish lexical variation from factual inconsistency.
As illustrated in \cref{fig:pipeline}, our approach integrates these dimensions through three key phases: 

\begin{enumerate}
\item \textbf{Tensor Representation} (\cref{subsec:tensor_rep}): Construct a multi-dimensional similarity tensor $\mathcal{S} \in \mathbb{R}^{n\times n\times 2}$ by concatenating semantic ($\mathbf{S}$) and knowledge ($\mathbf{S}K$) similarity matrices derived from $n$ responses
\item \textbf{Tensor Decomposition} (\cref{subsec:decomp}): Apply orthogonal tensor decomposition to isolate dimension-specific features and suppress redundant information
\item \textbf{Ensemble Scoring} (\cref{subsec:ensemble}): Combine decomposition residuals across dimensions to compute the final uncertainty measure $U_{\text{final}}$
\end{enumerate}




\subsection{Tensor Representation}
\label{subsec:tensor_rep}

After obtaining the similarity matrices $\mathbf{S}$ and $\mathbf{S}_K$, the next question will be how to fully utilize two similarity matrices to represent the information from multiple dimensions. There are many methods such as concatenating two similarity matrices within 2D dimensions:
\begin{equation}
    S_{\text{concat}} = \begin{bmatrix} \mathbf{S} \\ \mathbf{S}_K \end{bmatrix} \in \mathbb{R}^{n\times (n+n)},
\end{equation}
and then performing singular value decomposition. Therefore, we need to choose a method based on the feature of two similarity matrices. We find that though the knowledge dimension of responses could contain the implicit knowledge, $\mathbf{S}_K$ and $\mathbf{S}$ still share large repeated information. Since we actually care about the structure of both matrices, we focus on the spectral norm of $\mathbf{S}$ and $\mathbf{S}_K$ because the spectral norm captures the most dominant features of their structures. Therefore we use spectral norm ratio (SNR)~\cite{wang2020theoretical} to quantify the redundancy between the matrices:
\begin{equation}
   SNR = \frac{\|\Lambda_{\text{sem}}\|_2}{\|\Lambda_{\text{know}}\|_2},
\end{equation}

where $\Lambda_{\text{sem}}$ and $\Lambda_{\text{know}}$ denote the largest eigenvalue (i.e. the spectral norm) of $\mathbf{S}$ and $\mathbf{S_k}$, respectively. A SNR close to 1 indicates that $\mathbf{S}$ and $\mathbf{S_k}$
share a similar structure, implying a higher degree of repeated information. We calculate the mean SNR for every $\mathbf{S}$ and $\mathbf{S_k}$ in Coqa is 0.89, which demonstrates a high degree of repeated information between the two dimensions. 

For such large repeated information, if we consider using $S_{concat}$, the repeated information will dominate and thus lead to a sub-optimal result. To address the repeated information, we applied tensor decomposition, which can analyze the information from two matrices orthogonally. 
In detail, we construct a third-order tensor \( \mathcal{S} \) by stacking the semantic similarity matrix \( \mathbf{S} \) and the knowledge similarity matrix \( \mathbf{S}_K \) along a new dimension. We use $\mathcal{S} \in \mathbb{R}^{n\times n \times 2}$ to denote the concatencated tensor.



\subsection{Tensor Decomposition}
\label{subsec:decomp}
In our work, we utilize two prominent tensor decomposition methods, the Tucker decomposition and the Canonical Polyadic (CP) decomposition, to extract information contained in $\mathcal{S}$ and calculate the final uncertainty measures. CP decomposition represents the tensor as a sum of rank-one components, offering a more interpretable model. Its simplicity is advantageous in many scenarios, but it can be too rigid when the data exhibits rich interactions that a rank-one approximation fails to captureâ€”potentially resulting in suboptimal solutions.  On the other hand, although Tucker decomposition excels at capturing high-order interactions, Tucker decomposition may converge to a local optimum due to its sensitivity to initialization and cause an unstable result. Therefore, in our paper, we use two tensor decomposition methods. We briefly introduce them below.

\noindent\textbf{Tucker Decomposition}. The Tucker decomposition is a higher-order generalization of the singular value decomposition (SVD) and represents a tensor as a core tensor multiplied by factor matrices along each dimension~\cite{de2000multilinear}. Formally, for a Nth-order tensor $\mathcal{X} \in \mathbb{R}^{I_1\times I_2 \times\cdots \times I_n}$,  the Tucker decomposition is expressed as:
\begin{equation}
     \mathcal{X} \approx \mathcal{G} \times_1 U^{(1)} \times_2 U^{(2)} \cdots \times_N U^{(N)},
\end{equation}
where \(\mathcal{G} \in \mathbb{R}^{R_1 \times R_2 \times \cdots \times R_N}\) is the core tensor that captures the interactions among the dimensions and \(U^{(i)} \in \mathbb{R}^{I_i \times R_i}\) for \(i=1,\dots,N\) are the factor matrices associated with each dimension. 


Without loss of generality, we can assume that the Tucker factor matrices
$U^{(i)}$ are orthogonal and jointly form subspace bases of the different dimensions in the tensor. In that sense, Tucker decomposition jointly models the dimensions of the tensor while allowing the individual to model the subspaces of each dimension. Based on this feature, Tucker decomposition offers flexibility in choosing the ranks \((R_1, R_2, \dots, R_N)\) for different dimensions and each dimension \(n\) can have a different rank \(R_n\). 
A common way to compute the Tucker decomposition is via the Higher-Order Singular Value Decomposition (HOSVD)~\cite{de2000multilinear}.


\noindent\textbf{CP decomposition} Different from Tucker Decomposition, which might be sensitive to the noise in the tensor due to its high parameter complexity, CP decomposition offers a more robust alternative by representing the tensor as a sum of rank-one components. Specifically, for an \(N\)th-order tensor $\mathcal{X} $, the CP decomposition is expressed as:
\begin{equation}
    \mathcal{X} \approx \sum_{r=1}^{R} \lambda_r\, a^{(1)}_r \circ a^{(2)}_r \circ \cdots \circ a^{(N)}_r,
\end{equation}
where \(R\) is the CP rank, \(\lambda_r\) are scalar weights, \(a^{(n)}_r \in \mathbb{R}^{I_n}\) are the factor vectors associated with the \(n\)-th mode, and \(\circ\) denotes the outer product. CP decomposition can be computed using the Alternating Least Squares algorithm~\cite{kolda2009tensor}. Similar to the Tucker decomposition, the CP rank \(R\) can be chosen flexibly. 


\noindent\textbf{Uncertainty} After we apply different tensor decomposition to $\mathcal{S}$, we can obtain the reconstructed tensor with different rank. For example, if we apply CP decomposition to $\mathcal{S}$ with rank $R$, we have:

\begin{equation}
    \mathcal{S} \approx \hat{\mathcal{S}}^{cp}_R = \sum_{r=1}^{R} \lambda_r\, a^{(1)}_r \circ a^{(2)}_r \circ \cdots \circ a^{(N)}_r
    \label{eq:cp_decomposition}
\end{equation}

Then we could compute the reconstruction error, which measures how much information could be captured with rank $R$:

\begin{equation}
    \epsilon^{cp}_R = \frac{\|\mathcal{S} - \hat{\mathcal{S}^{cp}_R}\|_F}{\|\mathcal{S}\|_F},
    \label{eq:cp_error}
\end{equation}
where \(\|\cdot\|_F\) is the Frobenius norm. Similarly, we could have a reconstruction tensor and error for Tucker decomposition with rank $[R,R,2]$:
\begin{equation}
    \epsilon^{tucker}_R = \frac{\|\mathcal{S} - \hat{\mathcal{S}^{tucker}_R}\|_F}{\|\mathcal{S}\|_F},
    \label{eq:tucker_error}
\end{equation}



Here, we always use the last mode uncompressed because we hope our method can analyze the two dimensions of responses separately. We also show an experiment that uses tucker with the last rank 1 in \cref{sec:ablation}.


If the responses are more consistent, then $\mathcal{S}$ has an easier structure and thus it is easier to use a low-rank structure to capture the information. Therefore, the reconstruction error is expected to become lower when the responses are more consistent, i.e. the model is more confident. Therefore, we could directly use $\epsilon^{cp}_R$ or $\epsilon^{tucker}_R$ as the uncertainty measure.   To empirically prove this, we draw a figure to observe the relationship between different accuracy and reconstruction errors. We define the accuracy of a question to the mean accuracy of its $n$ responses. We present the results in \cref{fig:acc_vs_error}, which shows a higher accuracy sample indeed intends to have a lower reconstruction error.


\begin{figure}[ht]
    \centering
    \includegraphics[width=0.48\textwidth]{images/acc_rank.pdf}
    \vspace{-3mm}
    \caption{An plot of reconstruction errors v.s. rank for CP decomposition. The results show that a higher-accuracy sample tends to have a lower reconstruction error.}
    \label{fig:acc_vs_error}
\end{figure}


 
\subsection{Ensemble Uncertainty}
\label{subsec:ensemble}


Though it is possible for us to directly use $\epsilon^{cp}_R$ or $\epsilon^{tucker}_R$ as the uncertainty measure, we further enhance the uncertainty measures by ensemble methods. Firstly, the choice of rank $R$ might influence the quality of $\epsilon^{cp}_R$ or $\epsilon^{tucker}_R$. Therefore, our method ensemble all the $\epsilon_R$ from $R=1$ to $R=n$. In detail, we have an uncertainty ensemble from all rank $R$ for CP decomposition:
\begin{equation}
    U_{cp} = \sum_{r=1}^n \epsilon^{cp}_R.
\end{equation}
Similarly, we compute the uncertainty ensemble from all rank $R$ for Tucker decomposition:
\begin{equation}
    U_{tucker} = \sum_{r=1}^n \epsilon^{tucker}_R.
\end{equation}

Finally, to ensure robust and accurate uncertainty estimation, we ensemble the uncertainty from both decomposition methods to obtain the final uncertainty. With two uncertainties from two tensor decomposition methods, we could have multiple meaningful ways to ensemble the results in an unsupervised way. We introduce two different methods with different intuitions. 

\noindent\textbf{Sum} A simple yet effective ensemble is the summation. The summation means that we assign an equal weight to the uncertainty results obtained from each rank and from both decomposition methods. This approach is based on the intuition that each decomposition method captures the different aspects of information and by adding two uncertainties up, we ensure that every method's contribution is treated evenly and thus obtain a more comprehensive information in the final uncertainty. In detail, we have:
\begin{equation}
    U = U_{\text{Tucker}} + U_{\text{CP}}.
\end{equation}

As mentioned above, a lower $U$ indicates a higher consistency between responses and thus a lower uncertainty. 

\noindent\textbf{Min}
The second ensemble strategy is based on the minimum operator. The intuition behind this approach is that if one of the tensor decomposition methods produces a low uncertainty estimate for a given sample, then the overall uncertainty should also be considered low. In other words, we rely on the most confident (i.e., smallest uncertainty) prediction provided by either method. Thus, we define the final uncertainty as:
\begin{equation}
    U = \min\left( U_{\text{Tucker}}, U_{\text{CP}} \right).
\end{equation}
This \textbf{Min} strategy can be particularly beneficial when one of the decomposition methods is prone to overestimating uncertainty. By selecting the minimum, we effectively mitigate the impact of any overly pessimistic estimates, while still preserving the robust information provided by the other method.





\begin{table*}[h]
    \centering
    \begin{tabular}{lcccccccccc}
        \toprule
        \multirow{2}{*}{Methods} & \multicolumn{2}{c}{Llama2-13B} & \multicolumn{2}{c}{Llama2-7B} & \multicolumn{2}{c}{Llama3.1-8B} & \multicolumn{2}{c}{Phi4} & \multicolumn{2}{c}{Deepseek-R1-7B} \\
        \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9} \cmidrule(lr){10-11}
        & AUROC & AUARC & AUROC & AUARC & AUROC & AUARC & AUROC & AUARC & AUROC & AUARC \\
        \midrule
        \multicolumn{11}{c}{\textbf{Dataset: CoQA} [Easy]} \\
        \midrule
        Eigv(Dis) &0.7294 & 0.7775&0.5965 & 0.9485 & 0.5762 & 0.9071 &0.6656 &0.9120 & 0.7841&0.9175 \\
        Ecc(Dis) & 0.6984& 0.7553& 0.5762 & 0.9409 & 0.5802 & 0.9206 &0.6487 &0.9066 & 0.7756& 0.9157\\
        Degree(Dis) & 0.7369 &0.7815 & 0.5963 & 0.9473 & 0.5728& 0.9112 & \textbf{0.6677} & \textbf{0.9121} & 0.7885&0.9189 \\
        Eigv(Agre) &0.7541 & 0.7876& 0.5971 & \textbf{0.9507} & 0.5791 &0.9153 &0.6399 &0.9051 & 0.7969&0.9234 \\
        Ecc(Agre) &0.7593 &0.7840 & 0.5961 &0.9480 &0.5785 & 0.9144 & 0.6335& 0.9020 & 0.7937& 0.9224\\
        Degree(Agre) &0.7548 & 0.7877& 0.5908&0.9413 & 0.5755& 0.9097&0.6278 & 0.8996& 0.7930 & 0.9222\\ \hline
        D-UE & 0.7566& \textbf{0.7885}& 0.5954&0.9481 &0.5825 &0.9284 &0.6503 &0.9079 &0.7966 &0.9228 \\
        P(true) & 0.7102 & 0.7088 & 0.5404 &0.9348 &0.5816 & \textbf{0.9323}&0.6630 &0.9087 & 0.5389&0.8311 \\ \hline
        \ours-Sum & \textbf{0.7657} &0.7739 & 0.5994 & 0.9483 & 0.5972  & 0.9308&0.6653 & 0.9076 & 0.7981 & 0.9253 \\
        \ours-Min &0.7656 &0.7740 & \textbf{0.5999} &0.9483 & \textbf{0.5983}& 0.9309 & 0.6652 & 0.9089& \textbf{0.7987} & \textbf{0.9255}\\ \hline
        \midrule
        \multicolumn{11}{c}{\textbf{Dataset: HotpotQA} [Medium]} \\
        \midrule
        Eigv(Dis) & 0.6269 & 0.7770& 0.6111& 0.7715 & 0.6099&0.6874 &0.5534 &0.8614 &0.5969 & 0.5737\\
        Ecc(Dis) &0.6103 & 0.7774& 0.6085&0.7752 &0.6044 &0.6827 & \textbf{0.5675} & 0.8691 & 0.5602 & 0.5377\\
        Degree(Dis) & \textbf{0.6336}& \textbf{0.7790} & 0.6134 &0.7714 & 0.6202&0.7087 &0.5666 & 0.8590 & 0.5977&0.5756 \\
        Eigv(Agre) &0.6235 & 0.7638 &0.6035 &0.7648 &0.6176 &0.7035 & 0.5328& 0.8497& 0.6249 & 0.5878\\
        Ecc(Agre) & 0.6233&0.7670 & 0.6049&0.7666 & 0.6084&0.6991 &0.5469 &0.8594&0.6321 & 0.5907\\
        Degree(Agre) & 0.6217 & 0.7611& 0.5973 & 0.7600 & 0.6105&0.7016 & 0.5294& 0.8491&0.6278 & 0.5902\\ \hline
        D-UE & 0.6252&0.7659 & 0.6056&0.7669 & 0.6212&0.7083 &0.5335 &0.8511 & 0.6270 &0.5893 \\
        P(true) &0.6056 &0.7591 &0.5901 &0.7713 & 0.6362&0.7077 &0.5326 &0.8513 & 0.5081 & 0.4795\\ \hline
        \ours-Sum  &0.6292 & 0.7730 & \textbf{0.6163}& 0.7770& \textbf{0.6471}& \textbf{0.7235} & 0.5669 &0.8735 & \textbf{0.6331} & \textbf{0.5918} \\
        \ours-Min  & 0.6296 & 0.7735& 0.6155&\textbf{0.7771}& \textbf{0.6471}&0.7234 &0.5673 & \textbf{0.8736} & 0.6329 & 0.5912 \\
        \hline
        \midrule
        \multicolumn{11}{c}{\textbf{Dataset: NQ\_Open} [Hard]} \\
        \midrule
        Eigv(Dis) & 0.6162&0.7300 &0.7280 & 0.6367& 0.6742& 0.5343&0.7035 &0.6035 &0.6696 & 0.2451\\
        Ecc(Dis) &0.6210 &0.7330 &0.7167 &0.6172 & 0.6562& 0.5007&0.6898 &0.5828 &0.6607 &0.2237 \\
        Degree(Dis) &0.6130 &0.7168 &0.7273 & 0.6318&0.6865 &0.5430 & 0.7090&0.6094 &0.6675 & 0.2463\\
        Eigv(Agre) &0.6258 &0.7276 &0.7240 &0.6327 &0.7463 & 0.5801&0.7515 & 0.6351 & 0.7291&0.2758 \\
        Ecc(Agre) & 0.6273 &0.7311 &0.7307 & 0.6298&0.7612 &0.5875 & 0.7555& 0.6370 & 0.7437& 0.2836\\
        Degree(Agre) & 0.6286 &0.7355 & 0.7290& 0.6324& 0.7619 &0.5885 &0.7542 & 0.6341 &0.7353 & 0.2781\\ \hline
        D-UE &0.6281 &0.7320 & 0.7258&0.6342 &0.7551 &0.5833 & 0.7539 &0.6366 & 0.7333 & 0.2801 \\
        P(true) & 0.6197& 0.7289& 0.6532&0.5929 & 0.7061& 0.5522& 0.7096&0.6049 & 0.4865&0.1413 \\ \hline
        \ours-Sum & \textbf{0.6334} & \textbf{0.7410} & 0.7310& 0.6359 & \textbf{0.7642} & \textbf{0.5906} & 0.7562 & 0.6381 & \textbf{0.7560} & \textbf{0.2843} \\
        \ours-Min &0.6332 &0.7409 & \textbf{0.7313} & \textbf{0.6375} & 0.7641 & 0.5905& \textbf{0.7565} & \textbf{0.6386} & 0.7558 & 0.2842 \\ \hline
        \bottomrule
    \end{tabular}
    \caption{Comparison of our methods with different baselines on various datasets and large language models. The best result is shown in the \textbf{bold}. The results show that \ours performs better than baselines in general and \ours has a better advantage on more difficult datasets such as NQ\_Open.}
    \vspace{-5mm}
    \label{tab:main_results}
\end{table*}

