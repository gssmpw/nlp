\section{Related Works}
Uncertainty quantification for traditional machine learning problems such as regression or classification has been well studied~\cite{ye2024uncertainty,amini2020deep,sensoy2018evidential,ovadia2019can}. Most previous works on uncertainty quantification for nature language processing (NLP) consider text classifiers~\cite{jiang2021can,desai2020calibration,kamath2020selective} or text regressors~\cite{glushkova2021uncertainty,wang2022uncertainty}. To transfer NLP tasks into a classification task, previous work may consider using multi-choice question answering datasets or transferring questions into multi-choice form~\cite{kamath2020selective}. 

However, considering NLP tasks as simple classification tasks overlooks the generation nature of the LLMs~\cite{kuhn2023semantic}. To overcome this disadvantage, recent works focus on open-ended generation tasks. The first branch of research is inducing the LLMs to output their uncertainty along with the response to solve UQ for open-ended generation tasks~\cite{tian2023just,kadavath2022language,mielke2020linguistic}. \citet{lin2022teaching} even fine-tuned LLMs so that LLMs can output better uncertainty. This is a straightforward solution. However, fine-tuning the LLMs to obtain a better uncertainty measure requires white-box access to the models and may cost computation resources. \citet{kuhn2023semantic} first propose semantic entropy, which calculates entropy considering semantic information. However, such an approach still requires the token-related probability values as input.

To compute uncertainty for black-box MLLMs, previous works take a step further compared with semantic entropy and utilize the similarity and consistency between different generated answers from the same query to the LLMs. \citet{lin2023generating} uses NLI models to obtain the similarity. Then they treat the similarity matrix as from a weight connected graph and compute uncertainty using the structure of the graph such as using eigenvalues from the graph Laplacian. \citet{chen2024quantifying} identify unreliable or speculative answers by computing a confidence score. However, both works only consider semantic similarity, lacking an analysis of the deep meaning of the output. \citet{da2024llm} contains a claim level response augmentation. However,  augmented responses share much common information with original responses, and such common information is not considered by \citet{da2024llm}, causing a potential performance downgrade.  Therefore, in our paper, we do not only generate implicit knowledge behind the original answers but also use tensor decomposition to fully utilize the additional information.  

\vspace{-3mm}