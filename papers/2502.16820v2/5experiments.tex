\section{Experiments}

We conduct comprehensive experiments across multiple datasets and model architectures to validate our method's ability to decouple explanation robustness from classification robustness. Our evaluation addresses three key research questions:
\begin{itemize}
    \item \textbf{RQ1:} Does \ours have better quantify uncertainties?
    \item \textbf{RQ2:} How do different ensemble methods and information from both dimensions help?
    \item \textbf{RQ3:} Is\ours robust to different settings? 
\end{itemize}


\begin{table}[H]
\centering
\resizebox{!}{0.11\textwidth}{
\begin{tabular}{@{}lc@{}}
\toprule
\textbf{Measure} & \textbf{Details} \\ 
\midrule
$U_{\textit{Eigv}}(Dis)$ & \multicolumn{1}{c}{Spectral eigenvalue on the disagreement.} \\ 
$U_{\textit{Ecc}}(Dis)$ & \multicolumn{1}{c}{Average distance in responses' disagreement.} \\ 
$U_{\textit{Degree}}(Dis)$ & \multicolumn{1}{c}{Degree of disagreement similarity Matrix.} \\ 
$U_{\textit{Eigv}}(Agre)$ & \multicolumn{1}{c}{Spectral eigenvalue on the agreement.} \\ 
$U_{\textit{Ecc}}(Agre)$ & \multicolumn{1}{c}{Average distance in responses' agreement.} \\ 
$U_{\textit{Degree}}(Agre)$ & \multicolumn{1}{c}{Degree Matrix of agreement Matrix.} \\ 
$p(true)$ & \multicolumn{1}{c}{Entropy of knowledge dimension responses} \\ 
$D-UE$ & \multicolumn{1}{c}{eigenvalue from Laplacian of a directional graph} \\ 

\bottomrule
\end{tabular}}
\vspace{-1mm}
\caption{The baseline methods and explanations.}
\vspace{-5mm}
\label{tab:baslines}
\end{table}

\subsection{Experimental Setup}
\label{sec:setup}
\subsubsection{Datasets} As mentioned in \cref{sec:background}, following prior works~\cite{lin2022teaching}, we focus on open-form question-answering 
(QA) tasks in this paper. We adopt 4 different classic QA datasets. Coqa~\cite{reddy2019coqa} is a conversational question-answering dataset that contains dialogues with free-form answers grounded in diverse passages, which is the easiest dataset among all datasets. HotpotQA~\cite{yang2018hotpotqa} is a multi-hop QA dataset that demands reasoning over multiple Wikipedia paragraphs to derive correct answers. NQ-Open~\cite{kwiatkowski2019natural} consists of real-world queries from Google Search, requiring models to retrieve and answer questions without explicit context, which is the hardest dataset. 
\subsubsection{Models to Evaluate} We evaluate \ours on Llama family~\cite{touvron2023llama}, which is the one of the most popular LLMs. In detail, we use Llama-2-13b and Llama-2-7B to demonstrate the effectiveness of \ours with different model sizes and use Llama-3.1-8B~\cite{dubey2024llama} to that \ours could also work on the different version of Llama. To further demonstrate the generalization ability for other architectures,  we also use Phi4~\cite{abdin2024phi} and Deepseek-R1-distill-7B~\cite{guo2025deepseek} in our paper.


%\textcolor{red}{Not sure whether there is a section labeled as "sec eva metric" refered by Sec. }

\subsubsection{Evaluation Metrics} Effective uncertainty measures should accurately represent the reliability of LLM responses, with higher uncertainty more likely leading to incorrect generations and vice versa~\cite{lin2023generating,kuhn2023semantic}. Following prior works~\cite{lin2023generating,da2024llm}, we mainly use UQ values to predict whether an answer is correct or not. Following prior works~\cite{lin2023generating,da2024llm}, we will use Area Under Receiver Operating Characteristic (AUROC) and Area Under Accuracy Rejection Curve (AUARC) as evaluation metrics, where a higher AUROC or AUARC demonstrates better uncertainty measures. To compute AUROC and AUARC, the accuracy of each original response is required. Following previous works~\cite{da2024llm,lin2023generating}, we use another LLM to provide correctness from 0-100 to each response. If the correctness is greater than 70, we label the response as correct. In this paper, we use Qwen-34B~\cite{bai2023qwen} to evaluate the correctness.

\subsubsection{Knowledge Extracted Models} In this paper, we mainly use llama-2-13b~\cite{touvron2023llama} as the auxiliary models to extract the knowledge dimension of responses. To demonstrate the robustness of \ours with different knowledge-extracted models, we also contain the results for different LLMs as knowledge-extracted models.

\subsubsection{Baselines} In this paper, we compared \ours with baselines that use semantic dimension response and knowledge dimension response. For semantic dimension, we mainly compared with methods that come from \citet{lin2023generating}. In detail, we incorporate six distinct methods from \citet{lin2023generating}, which differ based on the operations applied after computing similarity and whether they utilize agreement (entailment) probabilities or disagreement (contradiction) logits to construct the similarity matrix. For knowledge dimension, we use D-UE~\cite{da2024llm} and $p(true)$~\cite{kadavath2022language} as the baselines. Note that we use $p(true)$ on the knowledge dimension of response. We show the detailed explanations of all baselines in \cref{tab:baslines}

\begin{figure*}[t]
\centering
\begin{minipage}[t]{0.32\linewidth}
  \centering
  \includegraphics[width=\linewidth,trim=0 0 0 1cm, clip]{images/ablation.pdf}
  \captionof{figure}{Ablation studies that show that \ours fully utilizes all the information from both dimensions.}
  \label{fig:ablation}
\end{minipage}\hfill
\begin{minipage}[t]{0.32\linewidth}
  \centering
  \includegraphics[width=\linewidth]{images/knowledge_extract.pdf}
  \captionof{figure}{Performance for different knowledge extract models on CoQA and NQ\_Open with llama3.1.}
  \label{fig:knowledge_extract}
\end{minipage}\hfill
\begin{minipage}[t]{0.32\linewidth}
  \centering
  \includegraphics[width=\linewidth]{images/Jacc.pdf}
  \captionof{figure}{Performance that uses Jaccard similarity on CoQA and NQ\_Open with llama3.1.}
  \label{fig:jacc}
\end{minipage}
\end{figure*}

\subsection{Does \ours have better quantify uncertainties? (RQ1)}
\label{sec:main_result}
In this section, we explore whether \ours has better uncertainties compared with state-of-the-art uncertainty quantification methods. In \cref{tab:main_results}, we compare \ours with 8 baselines across three different datasets and five different models as introduced in \cref{sec:setup} In detail, we have the following observations:

\noindent $\bullet$ Compared with all baseline methods, \ours achieves the best performance overall. Especially when we consider AUROC. For AUARC, \ours achieves the best performance for NQ\_Open while \ours also achieves the comparable performance for CoQA in most scenarios. These results demonstrate that \ours has better quantify uncertainties overall. \\
\noindent $\bullet$ Among all datasets, \ours achieves the highest performance improvement on NQ\_Open, which is the most difficult dataset among all datasets and may lose to baselines for an easier dataset like CoQA. This indicates \ours could perform even better when the task is harder, where uncertainty quantification is more important. \\
\noindent $\bullet$ Two different ensemble methods show very similar results. Min strategy performs better than the sum strategy under $61.51\%$ situations, indicating that difficult datasets might also have more complex structures that single one tensor decomposition might oversight some information while using min structure could reduce such oversight by considering the best cases. However, both ensemble methods show a better performance than all baselines, which proves the effectiveness of tensor decomposition. \\

From these results, we get a conclusion that overall, \ours have better uncertainties.


\subsection{How do different ensemble methods and information from both dimensions help? (RQ2)}
\label{sec:ablation}
In this section, we use more experiments to prove the necessity of using information from both semantic and knowledge dimensions as well as using tensor decomposition. In detail, we consider the following methods: 1) \ours with only semantic responses, 2) \ours with only knowledge responses and 3) Concatenating similarity matrices from semantic and knowledge dimensions into a 2D matrix and applying SVD, 4) only using one tensor decomposition. In \cref{fig:ablation}, we show the comparison between \ours and other methods.  The results show that \ours consistently outperforms its variants and SVD method that repeated information will dominate the features, showing the effectiveness of our framework.




\subsection{Is \ours robust to different settings? (RQ3)}
\subsubsection{Different Knowledge Extracted Models} Knowledge extracted models influence the claim extraction in \ours as stated in \cref{subsubsec:knowledge}. Therefore, in this section, We test the robustness of \ours on various knowledge extracted models. unlike using llama2-13b in \cref{sec:main_result} and \cref{sec:ablation}, we conduct experiments on CoQA and NQ\_open using llama2-7b and llama3.1 as the knowledge extracted models, We show the results in \cref{fig:knowledge_extract}. From the figure, we can see that using Phi4 could even achieve a better result, indicating \ours has more potential with the development of LLMs. 

\subsubsection{Different Accuracy Thresholds} Different accuracy thresholds lead to different accuracy and influence the evaluation of uncertainties. In the previous experiments, we all set the accuracy threshold to 70 as mentioned in \cref{sec:setup}.  To test the robustness of \ours under different accuracy thresholds, we choose an extra dataset TriviaQA~\cite{joshi2017triviaqa}, which is considered the easiest dataset, and NQ\_Open, which is the most challenging dataset in our paper to conduct experiments. We show the results with accuracy thresholds of 70 and 90 in \cref{tab:Accuracy_threshold}. From the results, we can see that increasing the accuracy threshold decreases the performance of all baselines while the performance of \ours could even increase for datasets with different difficulties, showing the robustness of \ours in different settings. 

\subsubsection{Different Similarity Metrics} Finally, different similarity metrics lead to different similarity matrices. Therefore, to test whether \ours also has a good performance for different similarities, we use Jaccard similarity instead of using an NLI model in this section and the results are presented in \cref{fig:jacc}. The results show that using Jaccard similarity will boost the performance for a simple dataset like CoQA but hurt the performance for a difficult dataset like NQ\_Open. This is because the answer to a simple question might not have a deeper semantic meaning that requires NLI models. However, \ours can still outperform baseline methods that also use Jaccard similarity, showing the robustness of \ours.





\begin{table}[h]
    \centering
    \resizebox{0.5\textwidth}{!}{
    \begin{tabular}{lcccc}
        \toprule
        \multirow{2}{*}{Methods} & \multicolumn{2}{c}{Accuracy Threshold: 0.7} & \multicolumn{2}{c}{Accuracy Threshold: 0.9} \\
        \cmidrule(lr){2-3} \cmidrule(lr){4-5}
        & AUROC & AUARC & AUROC & AUARC \\
        \midrule
        \multicolumn{5}{c}{\textbf{Dataset: TriviaQA} [Easy]} \\
        \midrule
        Eigv(Dis) & 0.8261 & 0.8094 & 0.8100& 0.7604\\
        Ecc(Dis) & 0.8063& 0.7940&0.7892 & 0.7415\\
        Degree(Dis) &0.8399 & 0.8163&0.8259 & 0.7694\\
        Eigv(Agre) &0.8436 &0.8116 &0.8351 & 0.7721 \\
        Ecc(Agre) & \textbf{0.8510}&0.8189 & 0.8374&0.7721 \\
        Degree(Agre) &0.8396 &\textbf{0.8397} &0.8384 & 0.7739\\
        \ours-Sum &0.8428 &0.8144 & 0.8438&0.7749 \\
        \ours-Min &0.8431 &0.8149 & \textbf{0.8440} & \textbf{0.7754}\\
        \midrule
        \multicolumn{5}{c}{\textbf{Dataset: NQ\_Open} [Hard]} \\
        \midrule
        Eigv(Dis) & 0.6162 & 0.7300 &0.5636 &0.6017 \\
        Ecc(Dis) & 0.6210& 0.7330& 0.5658&0.5941 \\
        Degree(Dis) &0.6130 & 0.7168&0.5662 &0.6033 \\
        Eigv(Agre) &0.6258 &0.7276 & 0.6146& 0.6290 \\
        Ecc(Agre) & 0.6273&0.7311 &0.6239 &0.6344\\
        Degree(Agre) &0.6286 &0.7355 & 0.6221&0.6299 \\
        \ours-Sum &\textbf{0.6334} &\textbf{0.7410} &\textbf{0.6351} &\textbf{0.6430} \\
        \ours-Min &0.6332 &0.7409 & 0.6350 &0.6429 \\
        \bottomrule
    \end{tabular}
    }
    \caption{Comparison of different methods across different accuracy thresholds on TrivialQA and NQ\_Open with llama2-13B. The results show that our methods outperform baselines after increasing the accuracy threshold, indicating that our methods have an advantage on more difficult datasets.}
    \vspace{-7mm}
    \label{tab:Accuracy_threshold}
\end{table}

