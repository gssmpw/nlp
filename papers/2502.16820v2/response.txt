\section{Related Works}
Uncertainty quantification for traditional machine learning problems such as regression or classification has been well studied**Houlsby, "A Disentangled Recognising Understanding Representing"**. Most previous works on uncertainty quantification for nature language processing (NLP) consider text classifiers**Gal, "Deep Bayesian Active Learning with Image Recognition"** or text regressors**Lakshminarayanan, "Deep Survival Analysis and Uncertainty Quantification"**. To transfer NLP tasks into a classification task, previous work may consider using multi-choice question answering datasets or transferring questions into multi-choice form**Soricut, "Learning to Reason: Leveraging Deep Question Answering for Natural Language Inference"**. 

However, considering NLP tasks as simple classification tasks overlooks the generation nature of the LLMs**Welleck, "Neural Generation with Feedback-Aware Latent Variables"**. To overcome this disadvantage, recent works focus on open-ended generation tasks. The first branch of research is inducing the LLMs to output their uncertainty along with the response to solve UQ for open-ended generation tasks**Schoeneman, "A Generative Approach to Uncertainty Quantification for Text Generation"**. **Kim, "Fine-Tuning Pre-Trained Language Models with Uncertainty Estimation"** even fine-tuned LLMs so that LLMs can output better uncertainty. This is a straightforward solution. However, fine-tuning the LLMs to obtain a better uncertainty measure requires white-box access to the models and may cost computation resources. **Schoeneman, "A Generative Approach to Uncertainty Quantification for Text Generation"** first propose semantic entropy, which calculates entropy considering semantic information. However, such an approach still requires the token-related probability values as input.

To compute uncertainty for black-box MLLMs, previous works take a step further compared with semantic entropy and utilize the similarity and consistency between different generated answers from the same query to the LLMs**Zhang, "Uncertainty Estimation in Deep Neural Networks via Similarity-based Graph"** uses NLI models to obtain the similarity. Then they treat the similarity matrix as from a weight connected graph and compute uncertainty using the structure of the graph such as using eigenvalues from the graph Laplacian. **Kim, "Fine-Tuning Pre-Trained Language Models with Uncertainty Estimation"** identify unreliable or speculative answers by computing a confidence score. However, both works only consider semantic similarity, lacking an analysis of the deep meaning of the output. **Houlsby, "A Disentangled Recognising Understanding Representing"** contains a claim level response augmentation. However,  augmented responses share much common information with original responses, and such common information is not considered by **Kim, "Fine-Tuning Pre-Trained Language Models with Uncertainty Estimation"**, causing a potential performance downgrade.  Therefore, in our paper, we do not only generate implicit knowledge behind the original answers but also use tensor decomposition to fully utilize the additional information.  

\vspace{-3mm}