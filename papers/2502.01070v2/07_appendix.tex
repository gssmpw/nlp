\newpage
\appendix
\onecolumn
\section{Appendix}\label{sec:appendix}

% Put ULP results here.

% We first investigate the effect of different FP8 configurations on GEMM operations for the normal distribution, which has few outliers, and the Cauchy distribution, which has a long tail of outlier values. In addition, we examine the effects of two features available in the Gaudi that are not available in NVIDIA GPUs, hardware acceleration for scaling factors that are powers of two and stochastic rounding. The outputs are compared by measuring the difference in ULP between the results computed in BF16 and those calculated in FP8. BF16 ULP values are used as both results are in BF16. 


\begin{table}[ht]
\centering
\caption{MMLU result for per-row, per-tensor options}
\label{tab:mmlu_per_row_tensor}
\begin{tabular}{c|cc|c}
\Xhline{1.2pt}  
\multirow{2}{*}{\textbf{model}}           & \multicolumn{2}{c|}{Rowwise Scaling}                   & \multirow{2}{*}{accuracy} \\
                                          \cline{2-3}

                                          & Activation       & Weight         &                               \\
\hline

\multirow{4}{*}{Llama-3.2-1B}  & \checkmark & \checkmark     & 35.12\%                       \\
                                          & \checkmark &                & 34.69\%                       \\
                                          &            & \checkmark     & 34.36\%                       \\
                                          &            &                & 34.65\%                       \\

\hline

\multirow{4}{*}{Llama-3.2-3B}  & \checkmark & \checkmark     & 53.90\%                       \\
                                          & \checkmark &                & 53.92\%                       \\
                                          &            & \checkmark     & 53.83\%                       \\
                                          &            &                & 53.75\%                       \\

\hline

\multirow{4}{*}{Llama-3.1-8B}  & \checkmark & \checkmark     & 62.95\%                       \\
                                          & \checkmark &                & 62.92\%                       \\
                                          &            & \checkmark     & 62.80\%                       \\
                                          &            &                & 62.90\%                       \\
\hline

\multirow{4}{*}{Llama-3.1-70B} & \checkmark & \checkmark     & 75.25\%                       \\
                                          & \checkmark &                & 75.23\%                       \\
                                          &            & \checkmark     & 75.32\%                       \\
                                          &            &                & 75.39\%                       \\
\Xhline{1.2pt}  

\end{tabular}

\end{table}





\begin{table}[ht]
\centering
\caption{MMLU 5-shot CoT for stochastic rounding using E4M3.}
\label{tab:sr_mmlu}
{\small
\begin{tabular}{c|cc|c}
\Xhline{1.2pt}  
\multirow{2}{*}{Model}                    & \multicolumn{2}{c|}{Stochastic Rounding} & \multirow{2}{*}{MMLU accuracy} \\
\cline{2-3}
                                          & Activation                       & Weight             &                                \\
                                          \hline
\multirow{4}{*}{Llama-3.2-1B}             & \checkmark                  & \checkmark         & 33.94\%                        \\
                                          & \checkmark                  &                    & 34.31\%                        \\
                                          &                             & \checkmark         & 35.20\%                        \\
                                          &                             &                    & 34.65\%                        \\
                                          \hline
\multirow{4}{*}{Llama-3.2-3B}             & \checkmark                  & \checkmark         & 54.09\%                        \\
                                          & \checkmark                  &                    & 53.95\%                        \\
                                          &                             & \checkmark         & 54.41\%                        \\
                                          &                             &                    & 53.75\%                        \\
                                          \hline
                                          
\multirow{4}{*}{Llama-3.1-8B}             & \checkmark                  & \checkmark         & 62.24\%                        \\
                                          & \checkmark                  &                    & 62.43\%                        \\
                                          &                             & \checkmark         & 62.55\%                        \\
                                          &                             &                    & 62.90\%                        \\
                                          \hline
                                          
\multirow{4}{*}{Llama-3.1-70B}            & \checkmark                  & \checkmark         & 75.05\%                        \\
                                          & \checkmark                  &                    & 75.18\%                        \\
                                          &                             & \checkmark         & 75.30\%                        \\
                                          &                             &                    & 75.39\%           
\\ \Xhline{1.2pt}  
                                          
\end{tabular}
}

\end{table}

\begin{table*}[]
\centering
\caption{Throughput and Power Measurement of Llama Instruct Models}
\begin{tabular}{c|c|c|c|cc}
\Xhline{1.2pt}  
\textbf{Model   Name} & \textbf{Mode} & \textbf{Batch Size} & \textbf{Seq Len} & \textbf{Power(W)} & \textbf{Prefill TFLOPS/HPU} \\
\cline{1-6}

Llama-3.2-1B-Instruct & FP8-Static    & 4                   & 512              & 509 	& 412                      \\
Llama-3.2-1B-Instruct & FP8-Static    & 4                   & 1024             & 509 	& 411                      \\ \cline{1-6}
Llama-3.2-3B-Instruct & FP8-Static    & 4                   & 512              & 498 	& 518                      \\
Llama-3.2-3B-Instruct & FP8-Static    & 4                   & 1024             & 511 	& 537                     \\ \cline{1-6}
Llama-3.1-8B-Instruct & FP8-Static    & 4                   & 512              & 471 	& 588                      \\
Llama-3.1-8B-Instruct & FP8-Static    & 4                   & 1024             & 436 	& 595                      \\ 
\Xhline{1.2pt}  

\end{tabular}
\label{tab:llama_prefill_throughput_power}
\end{table*}

% \begin{table*}[]
% \centering
% \caption{Throughput and Power Measurement of Matrix Multiplication}
% \label{tab:my-table}
% \begin{tabular}{ccccc}
% \Xhline{1.2pt}  
% 
% \textbf{Matrix Size} & \textbf{Power(W)} & \textbf{TFLOPS} & \textbf{Gaudi v2 MFU} & \textbf{Gaudi v3 MFU} \\
% \cline{1-5}
% 
% 1024 $\times$ 1024 $\times$ 1024         & 27.55             & 17.4                 & 2.00\%                & 0.90\%                \\
% 2048 $\times$ 2048 $\times$ 2048          & 150.00            & 139                  & 16.10\%               & 7.60\%                \\
% 4096 $\times$ 4096 $\times$ 4096         & 474.88            & 765.2                & 88.50\%               & 41.70\%               \\
% 8192 $\times$ 8192 $\times$ 8192         & 507.67            & 809.1                & 93.50\%               & 44.10\%              \\
% \Xhline{1.2pt}  
% 
% \end{tabular}
% 
% \end{table*}


% \begin{table*}[]
% \centering
% \caption{Throughput and Power Measurement of Matrix Multiplication}
% \label{tab:my-table}
% \begin{tabular}{cccc}
% \Xhline{1.2pt}  
% \textbf{Matrix Size} & \textbf{Power(W)} & \textbf{TFLOPS} & \textbf{Gaudi v2 MFU}  \\
% \cline{1-4}
% 1024 $\times$ 1024 $\times$ 1024         & 98                                    & 17                & 2.00\%                               \\
% 2048 $\times$ 2048 $\times$ 2048         & 220                                   & 139                & 16.10\%                              \\
% 4096 $\times$ 4096 $\times$ 4096         & 543                                   & 765               & 88.50\%                              \\
% 8192 $\times$ 8192 $\times$ 8192         & 576                                   & 809               & 93.50\%                             \\
% \Xhline{1.2pt}  
% \end{tabular}
% \end{table*}




\begin{table}[ht]
\caption{Gaudi 2 throughput in TFLOPS for scaled FP8 GEMM for square matrices of the given size, excluding quantization overhead. Measured throughput relative to the peak FP8 throughput (865 TFLOPS) is included in parentheses. Hardware acceleration is only available for per-tensor scaling.}
\centering
\begin{tabular}{@{}ccllllll@{}}
\toprule
Type &
Size &
\multicolumn{1}{c}{Per-Row} &
\multicolumn{1}{c}{Per-Tensor} &
\multicolumn{1}{c}{HW Accel.} \\ \midrule
\multirow{4}{*}{E4M3} & 1K & 494 (57.1\%) & 494 (57.1\%) & 494 (57.1\%) \\
                      & 2K & 506 (58.5\%) & 641 (74.1\%) & 641 (74.2\%) \\
                      & 4K & 735 (84.9\%) & 796 (92.1\%) & 801 (92.6\%) \\
                      & 8K & 742 (85.7\%) & 822 (95.0\%) & 852 (98.4\%) \\ \midrule
\multirow{4}{*}{E5M2} & 1K & 306 (35.4\%) & 494 (57.1\%) & 493 (57.0\%) \\
                      & 2K & 506 (58.5\%) & 642 (74.2\%) & 642 (74.2\%) \\
                      & 4K & 735 (84.9\%) & 802 (92.7\%) & 802 (92.7\%) \\
                      & 8K & 726 (83.9\%) & 825 (95.4\%) & 825 (95.4\%) \\ \bottomrule
\end{tabular}
\label{tab:gaudi2_fp8_tflops}
\end{table}

\begin{table}[ht]
\caption{H100 throughput in TFLOPS for scaled FP8 GEMM for square matrices of the given size, excluding quantization overhead. Measured throughput as a ratio of peak FP8 throughput (1989.9 TFLOPS) is included in parentheses. We only include results for E4M3 because we were unable to find libraries exposing low-level E5M2 GEMM APIs on NVIDIA GPUs.}
\centering
\begin{tabular}{@{}ccrrrr@{}}
\toprule
Accum. & Size & \multicolumn{1}{c}{Per-Row} & \multicolumn{1}{c}{Per-Tensor} \\ \midrule
\multirow{4}{*}{FP32} & 1K & 217  (11.0\%) & 186 \:  (9.4\%) \\
                      & 2K & 299  (15.1\%) & 840  (42.4\%) \\
                      & 4K & 362  (18.3\%) & 1099 (55.5\%) \\
                      & 8K & 396  (20.0\%) & 1300 (65.7\%) \\ \midrule
\multirow{4}{*}{Fast} & 1K & 237  (12.0\%) & 147 \: (7.4\%) \\
                      & 2K & 810  (40.9\%) & 896  (45.3\%) \\
                      & 4K & 1136 (57.4\%) & 1205 (60.9\%) \\
                      & 8K & 1123 (56.8\%) & 1388 (70.1\%) \\ \bottomrule
\end{tabular}
\label{tab:h100_fp8_tflops}
\end{table}





% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \begin{table}[]
% \centering
% \caption{Gaudi 2 GEMV}
% \label{tab:my-table}
% \begin{tabular}{cccccc}
% precision              & m  & k    & n    & TFLOPS & MFU     \\
% \multirow{12}{*}{FP8}  & 8  & 1024 & 1024 & 3.8    & 0.40\%  \\
%                        & 16 & 1024 & 1024 & 7.8    & 0.90\%  \\
%                        & 32 & 1024 & 1024 & 15.5   & 1.80\%  \\
%                        & 64 & 1024 & 1024 & 30.2   & 3.50\%  \\
%                        & 8  & 2048 & 2048 & 15.6   & 1.80\%  \\
%                        & 16 & 2048 & 2048 & 32.2   & 3.70\%  \\
%                        & 32 & 2048 & 2048 & 62.2   & 7.20\%  \\
%                        & 64 & 2048 & 2048 & 124.3  & 14.40\% \\
%                        & 8  & 4096 & 4096 & 36.3   & 4.20\%  \\
%                        & 16 & 4096 & 4096 & 71.9   & 8.30\%  \\
%                        & 32 & 4096 & 4096 & 140.7  & 16.30\% \\
%                        & 64 & 4096 & 4096 & 271.2  & 31.30\% \\
% \multirow{12}{*}{BF16} & 8  & 1024 & 1024 & 2.8    & 0.60\%  \\
%                        & 16 & 1024 & 1024 & 5.6    & 1.30\%  \\
%                        & 32 & 1024 & 1024 & 10.9   & 2.50\%  \\
%                        & 64 & 1024 & 1024 & 21.8   & 5.10\%  \\
%                        & 8  & 2048 & 2048 & 10.4   & 2.40\%  \\
%                        & 16 & 2048 & 2048 & 21.2   & 4.90\%  \\
%                        & 32 & 2048 & 2048 & 42.9   & 9.90\%  \\
%                        & 64 & 2048 & 2048 & 87.4   & 20.20\% \\
%                        & 8  & 4096 & 4096 & 18.7   & 4.30\%  \\
%                        & 16 & 4096 & 4096 & 37.3   & 8.60\%  \\
%                        & 32 & 4096 & 4096 & 73.8   & 17.10\% \\
%                        & 64 & 4096 & 4096 & 145.2  & 33.60\%
% \end{tabular}
% \end{table}


% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \begin{table}[]
% \centering
% \caption{H100 GEMV}
% \label{tab:my-table}
% \begin{tabular}{ccccll}
% precision              & m  & k    & n    & \multicolumn{1}{c}{TFLOPS} & \multicolumn{1}{c}{MFU} \\
% \multirow{12}{*}{FP8}  & 8  & 1024 & 1024 & 1.7                        & 0.10\%                  \\
%                        & 16 & 1024 & 1024 & 7.7                        & 0.40\%                  \\
%                        & 32 & 1024 & 1024 & 16.8                       & 0.90\%                  \\
%                        & 64 & 1024 & 1024 & 3.6                        & 0.20\%                  \\
%                        & 8  & 2048 & 2048 & 14.9                       & 0.80\%                  \\
%                        & 16 & 2048 & 2048 & 33.5                       & 1.70\%                  \\
%                        & 32 & 2048 & 2048 & 7.6                        & 0.40\%                  \\
%                        & 64 & 2048 & 2048 & 28.5                       & 1.40\%                  \\
%                        & 8  & 4096 & 4096 & 66.7                       & 3.40\%                  \\
%                        & 16 & 4096 & 4096 & 14.2                       & 0.70\%                  \\
%                        & 32 & 4096 & 4096 & 57.3                       & 2.90\%                  \\
%                        & 64 & 4096 & 4096 & 136.5                      & 6.90\%                  \\
% \multirow{12}{*}{BF16} & 8  & 1024 & 1024 & 1.7                        & 0.20\%                  \\
%                        & 16 & 1024 & 1024 & 6.5                        & 0.70\%                  \\
%                        & 32 & 1024 & 1024 & 14.5                       & 1.50\%                  \\
%                        & 64 & 1024 & 1024 & 3.5                        & 0.40\%                  \\
%                        & 8  & 2048 & 2048 & 12.8                       & 1.30\%                  \\
%                        & 16 & 2048 & 2048 & 29                         & 2.90\%                  \\
%                        & 32 & 2048 & 2048 & 6.3                        & 0.60\%                  \\
%                        & 64 & 2048 & 2048 & 25.5                       & 2.60\%                  \\
%                        & 8  & 4096 & 4096 & 69.1                       & 7.00\%                  \\
%                        & 16 & 4096 & 4096 & 13.8                       & 1.40\%                  \\
%                        & 32 & 4096 & 4096 & 52.3                       & 5.30\%                  \\
%                        & 64 & 4096 & 4096 & 134.6                      & 13.60\%                
% \end{tabular}
% \end{table}


