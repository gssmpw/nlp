\section{Matrix multiplication comparison}\label{sec:matmul}

\subsection{Differences in hardware capabilities}
% I need to cite the FP8 Intel white paper here.

We compare implementations of FP8 GEMM between NVIDIA GPUs and Intel Gaudi HPUs, identifying key differences despite both adhering to the FP8 specification \citep{micikevicius2022fp8formatsdeeplearning}.

\textbf{(Accumulation precision)} Hopper GPUs use a 14-bit accumulator for FP8 GEMMs \citep{deepseekai2024deepseekv3technicalreport}, requiring casting to CUDA cores for higher precision. Software optimizations, such as applying FP32 accumulation to only one in four WGMMA instructions, reduce error but increase kernel complexity and remain less precise than full FP32 accumulation. In contrast, Gaudi HPUs always accumulate in FP32, ensuring higher numerical precision.

\textbf{(E4M3 range)} The Gaudi 2 follows the IEEE specification for special values, unlike NVIDIA GPUs, which use a single special value representation \citep{noune20228bitnumericalformatsdeep}. This results in seven fewer magnitude representations and a maximum value of 240 for E4M3 in the Gaudi 2 compared to 448 on NVIDIA GPUs. This has been addressed in the Gaudi 3, but we were unable to test these in our experiments.

\textbf{(Power-of-2 scaling)} Gaudi HPUs allow modification of floating-point exponent biases to accelerate scaling factor application. The Gaudi 2 supports fixed hardware-accelerated scaling factors of  $2^{-8}, 2^{-4}, 2^{0}, 2^{4}$ for E4M3 while the Gaudi 3 extends this to arbitrary powers of 2 between $2^{-32}$ and $2^{31}$. However, this feature is limited to GEMMs with per-tensor scaling factors.

\textbf{(Stochastic rounding)} During FP8 quantization, stochastic rounding can be applied when converting 16/32-bit floating-point values to FP8, similar to the technique proposed for FP32-to-BF16 conversion in \citep{hlat}. This method is distinct from stochastic rounding applied at the inner-product \citep{doi:10.1137/22M1510819}.

\textbf{(Sparsity)} NVIDIA GPUs support semi-structured sparsity acceleration, potentially doubling tensor core peak throughput. However, despite attempts to leverage sparsity in LLMs \citep{sparsegpt, wanda_sun2024a}, dense GEMMs remain dominant due to accuracy loss and limited speedups. Gaudi HPUs do not support sparsity acceleration.

\subsection{GEMM throughput measurements}




For a GEMM between matrices of size $(M \times K) \times (K \times N)$, the total number of floating-point operations (FLOPs) performed is $2MKN$. This is derived from the $M \times N$ dot products of length $K$, where each element undergoes one multiplication and one addition.
Following the convention where FLOPS denotes FLOPs per second, we compute throughput in FLOPS using the theoretical FLOPs and the measured latency. We can thus calculate the MFU by dividing observed throughput by peak throughput.

As LLM computations are dominated by matrix multiplications, GEMM throughput serves as an upper bound for model end-to-end performance. While accelerator specifications list peak throughput values, actual throughput rarely reaches this limit, particularly for small matrices.

Table \ref{tab:gemm_tflops_power} presents GEMM throughput and power consumption measurements for row-wise scaled FP8 GEMM. We conducted measurements on NVIDIA H100 GPUs using the NGC PyTorch 24.12 image and on Intel Gaudi 2 HPUs using the Synapse AI v1.19.0 image. Power consumption was recorded via the "nvidia-smi" and "hl-smi" utilities, respectively. Input casting overheads were excluded from the measurements. Additional results for GEMM throughputs under different conditions are provided in Tables \ref{tab:gaudi2_fp8_tflops} and \ref{tab:h100_fp8_tflops}.
Our results show that the Gaudi 2 achieves higher utilization than the H100, particularly for small matrices. The performance drop for smaller matrices is steeper on the H100, with the Gaudi 2 providing a higher TFLOPS at matrix sizes of 1K. Also, the Gaudi 2 exhibits lower power consumption than its stated 600W TDP, whereas the H100 approaches its peak 700W TDP even at moderate utilizations.

Unlike NVIDIA GPUs, which incorporate multiple small matrix multiplication accelerators, Gaudi HPUs feature a few large matrix accelerators. As a result, Gaudi HPUs requires fewer input elements per cycle to fully utilize compute resources, thereby reducing first-level memory bandwidth requirements and improving efficiency \citep{gaudi2_whitepaper, gaudi3_whitepaper}. Additionally, Gaudi HPUs leverage a graph compiler to dynamically reconfigure the MME size based on the target GEMM dimensions, optimizing resource utilization.

This superior efficiency of the Gaudi 2 for small matrices is particularly relevant given that LLM hidden dimensions typically range between 1K (\textit{e.g.}, Llama 1B) and 8K (\textit{e.g.}, Llama 70B), with tensor parallelism further reducing matrix sizes. Moreover, the lower power draw of the Gaudi 2 relative to its TDP suggests that na√Øve TDP comparisons can be misleading, emphasizing the need for empirical evaluation.

\begin{table}
\small
\centering
\caption{Throughput and power measurements for square FP8 GEMMs with row-wise scaling. The ratio of measured TFLOPS and power draw relative to their peak values are included in parentheses. We include the TFLOPS/Watt ratio on the right. H100 has 1989.9 peak FP8 GEMM TFLOPS and 700W TDP. Gaudi 2 has 865 peak FP8 GEMM TFLOPS and 600W TDP.}
\vskip 0.15in
\begin{tabular}{@{}ccrcc@{}}
\toprule
Device & (M,K,N) & \multicolumn{1}{c}{TFLOPS} & \multicolumn{1}{r}{Power (W)} & TF/W \\ \midrule
\multirow{4}{*}{Gaudi 2} & 1K & 367.9  (42.5\%) & 375 (63\%) & 1.0 \\
                         & 2K & 586.2  (67.8\%) & 460 (77\%) & 1.3 \\
                         & 4K & 817.1  (94.5\%) & 460 (77\%) & 1.8 \\
                         & 8K & 741.8  (85.8\%) & 490 (82\%) & 1.5 \\ \midrule
\multirow{4}{*}{H100}    & 1K & 218.3  (11.0\%) & 350 (50\%) & 0.6 \\
                         & 2K & 879.7  (44.2\%) & 690 (99\%) & 1.3 \\
                         & 4K & 1167.6 (58.7\%) & 690 (99\%) & 1.7 \\
                         & 8K & 1084.7 (54.5\%) & 690 (99\%) & 1.6 \\ \bottomrule
\end{tabular}
\vskip -0.1in
\label{tab:gemm_tflops_power}
\end{table}
