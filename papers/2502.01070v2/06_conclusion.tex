\section{Conclusion}

In this study, we provide a comprehensive analysis of FP8, an emerging numerical format that remains underexplored. We examine various FP8 scaling factor configurations and analyze LLM inference workloads based on the constraints of real-world data center environments. We make a clear exposition of the key constraints in LLM inference, also providing a framework to make precise calculations for inference throughput.
Our analysis of the two phases of LLM inference highlights the strong dependence of LLM inference on hardware characteristics. 

However, beyond hardware development, software optimizations that fully leverage accelerator capabilities are equally essential to maximizing performance. 
One key limitation of the Gaudi is its limited software support and integration. For instance, many kernel fusion techniques for dynamic FP8 scaling are currently unavailable on the Gaudi. Additionally, the decode throughputs reported in our study relied on static memory allocation as the paged attention implementations remained uncompetitive despite recent optimizations \citep{lee2024debunkingcudamythgpubased}. Even for static memory allocation, the observed attention throughputs are lower than what would be expected from the hardware specifications. Furthermore, while not a primary focus of this work, the lack of KV cache quantization support present challenges for deployment.

Despite these concerns, the growing adoption of inference-time scaling via chain-of-thought (CoT) \citep{cot} prompting suggests that LLM workloads will increasingly be dominated by decoding. Emerging techniques that decouple the prefill and decode phases \citep{splitwise, distserve_ZhongLCHZL0024} enable hardware to be allocated to the tasks for which they are best suited. Given our findings that the Gaudi 2 demonstrates superior decoding efficiency for FP8, this has potential implications for cloud service providers seeking cost-effective inference solutions.
