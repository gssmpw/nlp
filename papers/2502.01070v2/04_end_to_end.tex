\section{Comparing end-to-end results}\label{sec:end2end}

\begin{table}
\centering
\caption{The ``Cited" columns include results from \citet{sqzb_fp8_blog} using static ``FP8 (S)" scaling from the Intel Neural Compressor (INC) library using the UltraChat 200K \citep{ultrachat_200k} dataset for calibration. The ``Measured" column includes results for dynamic ``FP8 (D)" scaling. Reference BF16 accuracies are included for both columns to control for evaluation condition differences.}
% The dynamic FP8 scaling experiments were conducted on an H100 due to HPUs not being supported for most tasks in LM Evaluation Harness v0.4.7.
\vskip 0.15in
\small
\begin{tabular}{@{}lcccc@{}}
\toprule
\multicolumn{1}{c}{\multirow{2}{*}{Llama v3.1 8B Inst.}} & \multicolumn{2}{c}{Cited} & \multicolumn{2}{c}{Measured} \\ \cmidrule(l){2-5} 
\multicolumn{1}{c}{}                      & BF16     & FP8 (S)     & BF16      & FP8 (D)      \\ \midrule
MMLU CoT 5-shot  & 68.4\% & 66.3\% & 68.8\% & 68.3\% \\
GSM8K CoT 5-shot & 75.7\% & 70.4\% & 83.1\% & 84.5\% \\
Winogrande       & 78.1\% & 77.6\% & 73.8\% & 73.8\% \\
TruthfulQA mc1   & 37.1\% & 34.9\% & 39.9\% & 39.4\% \\
TruthfulQA mc2   & 54.0\% & 52.2\% & 55.1\% & 54.3\% \\ \bottomrule
\end{tabular}
\vskip -0.1in
\label{tab:static_vs_dynamic}
\end{table}

We conduct evaluations on different FP8 features and come to the following conclusions. First, E4M3 consistently outperforms E5M2 in quantization accuracy across nearly all tested configurations.
Second, stochastic rounding during quantization has minimal impact on model accuracy and, in some cases, may even be detrimental.
Third, dynamic scaling achieves accuracy comparable to BF16 models while eliminating the need for a calibration set.

For evaluation, we use instruction-tuned versions of Llama v3.2 1B, v3.2 3B, v3.1 8B, and v3.3 70B. Unless stated otherwise, models employ dynamic row-wise scaling for all linear layers, while LM head remains in BF16.




\subsection{Dynamic vs static scaling}

Section \ref{sec:background} describes the trade-offs between static and dynamic scaling for activation quantization. Static scaling offers higher throughput for large matrices as scaling factors do not need to be computed for each input. It also enables per-tensor quantization, leading to improved GEMM utilization. In contrast, dynamic scaling enhances output quality by assigning separate scaling factors to each token. 


% We also need an analysis of why this degradation happens, most likely because the scaling factors are too small for certain inputs.
% However, this probably cannot be done before the revision period.

Table  \ref{tab:static_vs_dynamic} presents a comparative analysis of model output quality on static vs. dynamic FP8 scaling. 
The results indicate that dynamic quantization obtains results similar to the original BF16 models, whereas static quantization leads to noticeable accuracy degradation. While various techniques, such as those proposed by \citet{xiao2024smoothquantaccurateefficientposttraining, liu2024spinquant, ashkboos2024quarot}, can potentially mitigate this degradation, they still pose a risk as the scaling factors obtained during calibration may be suboptimal for unseen inputs. Therefore, dynamic quantization is the preferred approach, provided that the associated throughput reduction remains within acceptable limits.


\begin{table}
\centering
\caption{Comparison between different FP8 data types and rounding modes for MMLU CoT 5-shot performance on instruction-tuned Llama models. Stochastic rounding provides little or no benefit to accuracy while E5M2 is detrimental, especially for smaller models. Experiments were conducted on a Gaudi 2 HPU.}
\vskip 0.15in
\small
\begin{tabular}{@{}lccc@{}}
\toprule
\multicolumn{1}{l}{\textbf{Model}} & \textbf{Data Type} &  \textbf{Rounding} & \textbf{MMLU} \\ \midrule
\multirow{3}{*}{Llama v3.2}  & BF16 & -   & 46.3\% \\
\multirow{3}{*}{1B Instruct}              & E4M3 & SR  & 45.7\% \\
                                        & E4M3 & RTN & 45.5\% \\
                                        & E5M2 & RTN & 44.5\% \\ \midrule
\multirow{3}{*}{Llama v3.2}  & BF16 & -   & 61.8\% \\
\multirow{3}{*}{3B Instruct}          & E4M3 & SR  & 61.7\% \\
                                        & E4M3 & RTN & 61.6\% \\
                                        & E5M2 & RTN & 60.7\% \\ \midrule
\multirow{3}{*}{Llama v3.1}  & BF16 & -   & 68.8\% \\
\multirow{3}{*}{8B Instruct}           & E4M3 & SR  & 68.3\% \\
                                        & E4M3 & RTN & 68.3\% \\
                                        & E5M2 & RTN & 67.5\% \\ \midrule
\multirow{3}{*}{Llama v3.3} & BF16 & -   & 82.0\% \\
\multirow{3}{*}{70B Instruct}              & E4M3 & SR  & 82.0\% \\
                                        & E4M3 & RTN & 82.0\% \\
                                        & E5M2 & RTN & 82.2\% \\ \bottomrule
\end{tabular}
\vskip -0.1in
\label{tab:e5m2_sr}
\end{table}

\subsection{E4M3 vs. E5M2}




Following on the findings of \citet{MLSYS2024_dea9b4b6}, which demonstrated that the E4M3 format achieves superior accuracy on language tasks compared to E5M2, we extend this analysis to instruction-tuned models in Table \ref{tab:e5m2_sr}.
 
To determine the optimal format for inference, we conducted a comparative evaluation of E4M3 and E5M2, measuring MMLU accuracy using LM Evaluation Harness (v0.4.7, \citep{eval-harness}).
Our experimental results consistently indicate that E4M3 outperforms E5M2 across all evaluated scenarios.
Furthermore, as shown in Table \ref{tab:gaudi2_fp8_tflops}, the GEMM throughputs using E4M3 and E5M2 are comparable, making E4M3 the preferred choice for inference even considering the smaller representational capacity of E4M3 on Gaudi 2.




\subsection{Stochastic rounding}

Hardware-accelerated stochastic rounding during FP8 quantization is a distinctive feature of Gaudi HPUs, similar to the approach proposed by \citet{hlat}, and is unavailable in NVIDIA GPUs.
Equation  \ref{eq:stochastic_rounding} formalizes the concept, where a higher-precision value  $x$ is rounded up to $x_{up}$ or down to $x_{down}$ stochastically based on the distance from $x$. 

\begin{equation}
  x_{quantized} =
    \begin{cases}
      x_{up}   \quad (p=\frac{x-x_{down}}{x_{up}-x_{down}}) \\
      x_{down} \quad (p=\frac{x_{up}-x}{x_{up}-x_{down}})
    \end{cases}
\label{eq:stochastic_rounding}
\end{equation}

Stochastic rounding is expected to preserve more of the original numerical information post-quantization, potentially leading to improved model accuracy.
However, empirical results in Table \ref{tab:e5m2_sr} indicate that stochastic rounding during quantization does not significantly enhance model accuracy. Furthermore, as shown in Table \ref{tab:sr_mmlu}, it may even lead to accuracy degradation in certain cases. These findings suggest that while stochastic rounding during quantization theoretically retains more information, its practical benefits for FP8 inference in LLMs remains inconclusive.

\begin{figure*}[]
    \centering
    \includegraphics[width=\textwidth]{figures/LLM_phase_5.png}
    \vspace{-1em}
    \caption{Process and utilization characterization of the two phases of generative LLM inference.}
    \label{fig:decode_diagram}
    \vskip -0.1in 
\end{figure*}