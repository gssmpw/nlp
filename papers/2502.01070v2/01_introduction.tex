\section{Introduction}\label{sec:intro}

The FP8 format has emerged as a promising datatype due to its lower computational overhead, reduced power consumption, and improved memory efficiency compared to commonly used formats such as FP32 and BF16 \citep{kalamkar2019studybfloat16deeplearning}, particularly at large batch sizes. With the introduction of the Intel Gaudi HPU and NVIDIA Hopper GPUs, FP8 is gaining traction as a key datatype for next-generation AI workloads \citep{deepseekai2024deepseekv3technicalreport}.

While extreme low-bit quantization has been widely studied, its throughput remains inferior to conventional 16-bit floating-point formats on commercial hardware \citep{lin2024qserve}. In contrast, FP8 has rapidly become the preferred format due to its compatibility with existing accelerators while preserving model accuracy. This makes it particularly well-suited for datacenter-scale LLM inference, where both high throughput and minimal accuracy degradation are critical. In some accelerators, FP8 can be twice as fast as BF16. Given the immense computational costs of training and deploying large language models (LLMs), FP8 has garnered significant attention for its potential efficiency gains.

Despite its advantages, FP8 utilization is constrained by the widening gap between computing speed and memory bandwidth. A key challenge is that, while LLM sizes continue to scale, their hidden dimensions grow at a much slower rate (\textit{e.g.}, Llama 8B has a hidden size of 4096, but Llama v3.3 70B only increases hidden size to 8192). Tensor parallelism (TP) further partitions matrices into even smaller fragments, making small matrix efficiency a critical performance factor. 

Especially during the decode phase, Generalized Matrix-Vector (GEMV) and thin General Matrix-Matrix Multiply (GEMM) operations dominate, reducing Model FLOP Utilization (MFU) \citep{palm, megatron_lm}. With the growing popularity of ``reasoning" models \citep{deepseekai2025deepseekr1incentivizingreasoningcapability}, the proportion of decoding in LLM inference workloads can be expected to continue growing.

% To fully address the challenges of LLM inference, we set out to quantify the various constraints involved, including those that arise with the inclusion of FP8 quantization. 
To comprehensively address the challenges of LLM inference, we aim to quantify the various constraints that impact performance, including those introduced by FP8 quantization. 
To the best of our knowledge, we are the first to provide quantitative measurements associated with various FP8 quantization schemes. 

There is a notable lack of research providing comprehensive FP8 experimental results, particularly in studies analyzing hardware characteristics in LLM inference configurations. To address this gap, our work offers a detailed evaluation of FP8 performance across different hardware platforms, providing valuable insights for model developers on the trade-offs between model architecture and hardware design.
To summarize our contributions:

\begin{figure*}[ht]
    \centering
    \includegraphics[width=\textwidth]{figures/decode_4.png}
    \vspace{-1.6em}
    \caption{TFLOPS (top row) and TFLOPS/Watt (bottom row) comparison between the Gaudi 2 and H100 using FP8 in the decode phase with batch size set to 64. We measure inference throughput using Optimum Habana v1.15.0 for the Gaudi 2 and TensorRT-LLM v0.16.0 for the H100. For short sequences, the Gaudi 2 has an advantage both in absolute throughput and in TFLOPS/Watt. At longer sequences, the H100 achieves higher TFLOPS due to its superior memory bandwidth, but is less energy efficient due to higher power consumption.}
    \label{fig:decode_efficiency}
    \vskip -0.1in
\end{figure*}

\begin{enumerate}
    \itemsep0em
    \item 
    We analyze scaling factor configurations for FP8 formats and quantify key LLM inference constraints in datacenter environments, providing a unified framework to compare throughputs across different inference stages, model sizes, and sequence lengths.
    \item 
    We analyze the computational characteristics of LLM inference on the Gaudi 2 and H100 across both the prefill and decode phases. In the prefill phase, where compute performance is the key factor, H100's throughput is 50\% higher than Gaudi 2's. However, since the decode phase dominates overall inference time, accelerator utilization becomes more critical, and Gaudi 2 achieves 50\% higher throughput in this phase. This highlights the differing performance trade-offs between the two accelerators across LLM inference workloads.
\end{enumerate}
