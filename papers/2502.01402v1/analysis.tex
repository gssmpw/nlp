\section{Dataset Analysis}

In this section, we will analyze the  annotations we conducted on 7 podcast episodes. We plan larger-scale annotations in the future.

\begin{table}[t!!!]
\centering
\caption{Check-worthy and Not Check-worthy Counts by Topic}
\label{tab:Check-worthy_counts}
\begin{tabular}{lrr}
\hline
\textbf{Topic}             & \textbf{Check-worthy} & \textbf{Not Check-worthy} \\ \hline
News and Politics          & 223                  & 1191                     \\ 
Health and Wellness        & 77                   & 469                      \\ 
\midrule
\textbf{Total}             & \textbf{300}         & \textbf{1660}            \\ \hline
\end{tabular}
\end{table}

\subsection{Check-worthy Claim Distribution}

Table \ref{tab:Check-worthy_counts} presents the distribution of utterances identified as \textit{Check-worthy} and \textit{Not Check-worthy} across two main topics: \textit{News and Politics} and \textit{Health and Wellness}. Interestingly, the majority of utterances belong to the \textit{News and Politics} category, where only 223 out of 1414 utterances (approximately 16\%) were deemed Check-worthy.  On the other hand, in \textit{Health and Wellness}, while the total number of utterances is lower (546), 77 utterances (approximately 14\%) were labeled as Check-worthy. This indicates the Check-worthy claims are roughly same proportion across topics.


\begin{table*}[h!!!]
\centering
\caption{Per-Class and Weighted F1-Scores for Claim Detection and Stance Detection Tasks}
\label{tab:eval}
\begin{tabular}{lccc|ccc}
\toprule
\multirow{2}{*}{\textbf{Model}} & \multicolumn{3}{c|}{\textbf{Claim Detection (F1-Score)}} & \multicolumn{3}{c}{\textbf{Stance Detection (F1-Score)}} \\ 
\cline{2-7}
                                & \textbf{False} & \textbf{True} & \textbf{Weighted}         & \textbf{Refutes} & \textbf{Supports} & \textbf{Weighted} \\ 
\midrule
XLM-Roberta-Large (fine-tuned)  & 0.91           & 0.45          & 0.85                      & 0.44             & 0.79              & 0.67              \\ 
GPT-4                           & 0.91           & 0.57          & 0.86                      & 0.53             & 0.56              & 0.55              \\ 
\bottomrule
\end{tabular}
\end{table*}






\subsection{Claim Types}

Numerical Claims dominate with 188 (62.6\%)  of instances, highlighting a focus on statistics and quantitative data. 83 Factual Descriptions (27.6\%) involve verifiable statements about people, events, or objects. Cause and Effect claims account for 19 (6.3\%) are less frequent, and Quotations are rare, with only 10 occurrences, indicating infrequent attribution to specific individuals or entities.


The primary reason statements are not checkable is that the majority (326 utterances) are non-factual, including greetings, questions, or fillers. Emotions and Opinions rank second with 402 utterances, followed by Personal Experiences at 328 utterances. Broadcast Details, such as announcements or advertisements, come next with 306 utterances. Predictions, the smallest category with 60 utterances, involve future events that cannot be verified at present.


\subsection{Motivation to fact-check  claims}






The main motivations for fact-checking are a desire to learn more (127 instances) and public interest (77), reflecting curiosity and the societal importance of verifying claims. Concerns about potential harm (54) also drive fact-checking to prevent misinformation. Interestingly, fewer claims are fact-checked for being surprising facts (24) or said by prominent individuals (18), suggesting that practical and informational needs outweigh sensational or high-profile statements.

\section{Experimental Evaluation}

This section shows the utility of the annotated data for claim detection and stance detection (claim verification) tasks.


 
\begin{table}[t!!]
    \centering
    \caption{Dataset distribution.}
    \label{tab:dataset}
    \begin{tabular}{l|rrr|rr}
    \hline
      \textbf{Split}   & \textbf{Check-} & \textbf{Not Check-} & \textbf{Total} & \textbf{True} & \textbf{False}   \\ \hline
       Train           & 219                 & 1185         & 1,404   & 323           & 237                     \\
       Dev             & 57                  & 323            & 380   & 86            & 29                       \\
       Test            & 24                  & 152            & 176   & 18            & 32                       \\ \midrule
       \textbf{Total} & \textbf{300} & \textbf{1660}  & \textbf{1960}& \textbf{427} & \textbf{298}\\ \hline
    \end{tabular}
\end{table}

See Table~\ref{tab:dataset} for the dataset used for experiments. We compare the fine-tuned XLM-Roberta-Large and GPT-4 (version gpt-4-0613 deployed on Azure). For the prompt used for GPT-4 refer to \cite{Setty:SIGIR:2024a}.

\subsection{Fact-Checking Results}


From Table \ref{tab:eval}, XLM-Roberta-Large  and GPT-4 both perform well in claim detection, with GPT-4 better at identifying true claims (F1-score of 0.57 vs. 0.45). For stance detection, XLM-Roberta excels, especially in supporting claims (F1-score of 0.79 vs. GPT-4’s 0.56). Overall, GPT-4 is slightly stronger in claim detection, while XLM-Roberta is better in stance detection. This illustrates that smaller LMs are competitive for fact-checking tasks.



\subsection{Transcription Performance}



\begin{table}[h!!]
\centering
\caption{Whisper ASR Performance in terms of error rates (ER)}
\label{tab:asr}
\begin{tabular}{lcccc}
\toprule
       Model Size &  Word ER &  Match ER &  Character ER \\
\midrule
            Small &                   0.25 &                    0.22 &                        0.12  \\
           Medium &                   0.20 &                    0.17 &                        0.10  \\
Medium (Prompted) &                   0.18 &                    0.16 &                        0.09  \\
            Large &                   0.15 &                    0.13 &                        0.07 \\
\bottomrule
\end{tabular}
\end{table}


Table \ref{tab:asr} shows the performance of different sizes of the Whisper ASR model, measured by error rates in word, match, and character. Word Error Rate (WER)  measures the proportion of words incorrectly recognized. Lower WER indicates better performance. Whisper’s error rates improve with model size, with the Small model at 0.25, Medium at 0.20, Medium (Prompted) at 0.18, and Large at 0.15. Match Error Rate measures the order of words and phrases, and Character Error Rate measures the  proportion of individual character errors. Similar pattern is observed in MER (Match Error Rate) and CER (Character Error Rate).
