\section{Related Work}

The growing popularity of podcasts has driven research into automated transcription, claim detection, and fact-checking. Early contributions, such as the Spotify Podcast Dataset~\cite{clifton2020100}, provided valuable large-scale spoken document corpora for natural language processing (NLP). Similarly, datasets for podcast summarization, like those proposed by Manakul et al.~\cite{manakul2022podcast}, have advanced understanding in this domain. However, access to these datasets has become increasingly restricted, creating significant gaps in resources for developing robust podcast fact-checking tools.

While datasets for detecting check-worthy claims in political debates exist~\cite{ivanov2024detecting,hassan2017claimbuster}, they lack fine-grained annotations for claim types, motivations for fact-checking, and associated claim verification data, limiting their utility in fact-checking scenarios.

General purpose NLP annotation tools like Prodigy\footnote{\url{https://prodi.gy}} exist. There are also text editors for fact-checking~\cite{setty2024factcheck}, and systems for live fact-checking of audio streams~\cite{setty2024livefc} address various aspects of fact-checking. However, these tools are not designed for data annotation. Currently there are no tools specifically designed for podcast annotation that support simultaneous audio playback and transcription annotation. Additionally, there is a lack of fine-grained analysis of claim types and their motivations for fact-checking. Also, an end-to-end dataset for fact-checking podcasts is currently unavailable.
