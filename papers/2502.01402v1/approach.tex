


\section{Approach}

This section describes the web application for annotating and fact-checking podcasts and the data annotation process.

\subsection{Data Population}

We chose podcasts from two popular categories: \textit{News \& Politics} and \textit{Health \& Wellness}, based on their popularity and relevance. We downloaded podcasts and their metadata from RSS feeds of the original podcast creators. Then we transcribed the podcasts using OpenAIâ€™s Whisper ASR model (with open weights).\footnote{\url{https://github.com/openai/whisper/blob/main/model-card.md}} This model provided outputs that included word-level timestamps, confidence probabilities, and punctuation marks. The outputs were crucial for ensuring accurate alignment between the transcription and the original audio. We enhanced transcriptions by performing diarization using Pyannote~\cite{Plaquet23}, which assigned speaker labels to segments of the audio. These diarization outputs were stored as metadata linked to each transcription.

To facilitate downstream NLP tasks, the transcriptions were segmented into sentences using SpaCy, which split the transcribed text based on punctuation marks. Co-reference resolution was performed using the F-coref package~\cite{otmazgin2022f}. This step ensured that pronouns and proper nouns were linked to the correct entities, creating self-contained utterances that were ready for fact-checking. A sliding window approach was used to process batches of 50 utterances at a time, providing the necessary context for the co-reference resolution model.

\subsection{Crowdsourcing and Annotation}

The web application included an interface for browsing and listening to podcasts, along with a dedicated annotation interface for collecting human annotations on transcriptions (see Figure \ref{fig:claims}). This interface supported tasks such as classifying utterances for check-worthiness, identifying specific claim spans, and correcting transcription errors. Using the interface depicted in Figure \ref{fig:stance}, annotators performed fact-checking by generating search queries, evaluating retrieved documents, and assigning verdicts to claims. Fact-checking was carried out exclusively on claims identified as check-worthy.

We recruited crowdworkers through Prolific, a widely-used crowdsourcing platform, applying pre-screening filters to ensure participants met criteria like language proficiency and educational background. Tasks were broken down into manageable microtasks, typically taking less than 30 minutes to complete. A podcast browser interface provided real-time progress visualization and submission storage, allowing annotators to select podcasts of their choice, reducing annotation fatigue. In the future, we plan to explore more user-friendly annotation methods to further minimize user effort, such as leveraging head-nodding gestures with devices like Apple AirPods.

Each annotation task was assigned to three annotators, with unanimous labels retained to account for subjectivity. Annotations were stored hierarchically in a relational database, linking each annotation to its corresponding utterance and associating it with the relevant podcast episode. This structured storage facilitated flexible querying, enabling the creation of datasets tailored to various NLP applications.

The annotations followed options:

\subsection*{Checkable Claims}
\begin{enumerate}
    \item \textbf{Factual Descriptions:} Verifiable claims about people, places, events, or actions.  
    \textit{Example:} ``She won the London Marathon last year.''
    
    \item \textbf{Cause and Effect:} Claims linking one event to another.  
    \textit{Example:} ``Smoking causes cancer.''
    
    \item \textbf{Numerical Claims:} Claims involving statistics or numerical analysis.  
    \textit{Example:} ``80\% of people are unhappy with the government.''
    
    \item \textbf{Quotations:} Verifiable quotes from notable entities.  
    \textit{Example:} ``President Roosevelt said, `Ich bin ein Berliner.'''
\end{enumerate}

\subsection*{Not Checkable Claims}
\begin{enumerate}
    \item \textbf{Non-Factual Statements:} Phrases without factual assertions.  
    \textit{Example:} ``How are you?''
    
    \item \textbf{Broadcast Details:} Introductions or program descriptions.  
    \textit{Example:} ``Welcome to the show, I'm John Smith.''
    
    \item \textbf{Emotions and Opinions:} Non-verifiable feelings or opinions.  
    \textit{Example:} ``I love how the tulips look in spring.''
    
    \item \textbf{Personal Experience:} Statements about private experiences.  
    \textit{Example:} ``I passed four empty buses yesterday.''
    
    \item \textbf{Predictions:} Future events or plans that cannot be confirmed.  
    \textit{Example:} ``Elon Musk will visit Mars.''
\end{enumerate}

\section*{Motivations for Fact-Checking}
\begin{enumerate}
    \item \textbf{Potential to Cause Harm:} This statement could be harmful if it turns out to be false.
    \item \textbf{Said by a Prominent Person:} I want to verify whether this prominent person actually made this statement.
    \item \textbf{Public Interest:} Fact-checking this claim is important for the public's awareness and benefit.
    \item \textbf{Surprising:} This claim is unexpected, shocking, or difficult to believe without verification.
    \item \textbf{Learn More:} Fact-checking this statement would help me gain a deeper understanding of the topic.
\end{enumerate}

For more details on the annotation process see \cite{becker2023automated}.

\subsection{Application of Annotations}

We analyze the collected annotations and train models for claim detection and stance detection. Check-worthiness classifications and claim spans help identify high-priority claims, while search queries and document evaluations improve evidence retrieval for downstream fact-checking models.

We fine-tune the XLM-Roberta-Large\footnote{\url{https://huggingface.co/FacebookAI/xlm-roberta-large}} model for claim detection and stance detection tasks (NLI). Claim detection is framed as a binary classification task, distinguishing between check-worthy and not check-worthy claims. Similarly, stance detection is simplified to a binary classification task, categorizing claims as either Supports or Refutes. 

