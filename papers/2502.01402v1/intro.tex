\section{Introduction}


Podcasts have grown rapidly in popularity worldwide, offering creators significant freedom due to minimal regulation.\footnote{\url{https://web.archive.org/web/20250126182112/https://www.pewresearch.org/journalism/2023/04/18/podcasts-as-a-source-of-news-and-information/}} While this openness fosters creativity, it also increases the risk of misinformation, posing challenges for accurate transcription, contextual understanding, and multilingual support.\footnote{\url{https://web.archive.org/web/20250116085820/https://www.brookings.edu/articles/the-challenge-of-detecting-misinformation-in-podcasting/}} Existing fact-checking methods, which rely on claim-by-claim verification~\cite{thorne-etal-2018-fever,schlichtkrull2023averitec}, are further hindered by the lack of open podcast datasets. Previous datasets like the Spotify Podcast Dataset~\cite{clifton2020100} are no longer accessible, leaving a significant gap.

To address these challenges, we present an open-source tool for podcast transcription and annotation. The tool integrates audio playback with real-time annotation, enabling annotators to correct transcription errors, resolve ambiguities, and identify claims for fact-checking. This approach ensures efficiency while maintaining the integrity of spoken content. Large Language Models (LLMs) are limited in processing multi-hour transcripts and require significant computational resources, but our tool provides a lightweight, accessible alternative.

Built entirely with open-source components, including Whisper ASR~\cite{radford2023robust} for transcription and F-Coref~\cite{otmazgin2022f} for co-reference resolution, the pipeline supports over 90 languages (excluding co-reference resolution) and offers flexibility for annotators to work on multilingual podcasts. It also collects fine-grained annotations, such as claim types, reasons for fact-checking, and utterances unrelated to claims. Annotators can perform fact-checking within the same tool, streamlining the process. We release the source code of the annotation tool here: \url{https://github.com/factiverse/factcheck-podcasts}.


\begin{figure*}[h!]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/interface_example2.png}
    \caption{Podcast annotation interface for claim detection. Right hand size shows the options for fine-grained claim annotation.}
    \label{fig:claims}
\end{figure*}

\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{figures/factcheck.png}
    \caption{Fact-checking annotation interface.}
    \label{fig:stance}
\end{figure}

We release transcripts for 531 episodes from 38 podcasts in English, Norwegian, and German, alongside an annotated dataset specifically for end-to-end fact-checking. This includes annotations for claim detection, fine-grained claim categorization, and claim verification on selected episodes from 7 podcasts. Additionally, we demonstrate the utility of these annotations by fine-tuning transformer models such as XLM-RoBERTa for multilingual claim detection and stance classification, comparing their performance to LLMs like GPT-4 in few-shot scenarios.



% The annotated data is subsequently utilized to fine-tune transformer models, such as XLM-RoBERTa, facilitating effective multilingual claim detection and stance classification. To promote further research in podcast fact-checking and related fields, we have publicly released the annotated podcast transcripts, encompassing 38 podcasts with 531 episodes in English, Norwegian, and German. Our tool supports over 90 languages (excluding co-reference resolution), simplifying data annotation for a broader range of podcasts across various languages.