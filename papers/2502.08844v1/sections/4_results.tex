\section{Results}
\label{sec:results}

In this section, we report RL and sim-to-real results for environments in MuJoCo Playground. Sim-to-real experiments (see some examples in \Cref{fig:action_reel}) are performed for locomotion and manipulation environments from both proprioceptive state and from vision.
We briefly discuss RL training on different hardware devices and RL libraries.

\subsection{DM Control Suite}

We train state-based policies for all available tasks, with most environments training in under 10 minutes on a single GPU device. More details on the training process can be found in \Cref{sec:appendix_dm_control_curves}. All available environments in the MJX port of the DM Control Suite, including any modifications, are detailed in \Cref{sec:appendix_dm_control_envs}. 

Using the batch renderer, we also implement pixel-based observations for the CartpoleBalance environment. These observations are generated on the GPU, allowing us to keep physics, rendering, and training entirely on-device. Although other DM Control Suite environments can also be rendered with Madrona, we demonstrate end-to-end RL training on only one task, leaving a more comprehensive exploration for future work. \Cref{appendix:MadronaMJXBenchmark} provides more information on how CartpoleBalance was modified and trained for pixel observations.

\begin{figure}[h!]
    \centering
    \includegraphics[width=1\linewidth]{figures/dmc_grid.png}
    \caption{\small Several DM Control Suite environments.}
    \label{fig:dmc_grid}
\end{figure}

\subsection{Locomotion}
\label{sec:results_sim2real_locomotion}

We present sim-to-real locomotion results on both a quadruped (Unitree Go1) and three humanoid platforms (Berkeley Humanoid,Unitree G1, and Booster T1). Further details on the MDP formulation, including rewards, observation spaces, and action spaces, are provided in \Cref{sec:appendix_locomotion}.

\subsubsection{Quadruped Locomotion}

\begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{figures/madrona_renders.pdf}
    \caption{\small Sample renders from the Madrona batch renderer for the Panda and Aloha environments. Left-most images are the original environments. The remaining images highlight the the support for lighting, shadows, textures, and colors, including the ability to domain randomize these parameters during training.}
    \label{fig:madrona_highres}
\end{figure}

\paragraph{Task definition} We implement a joystick locomotion task as in \cite{rudin2022learning, Ji_2022}, where the command is specified by three values indicating the desired forward velocity, lateral velocity, and turning rate of the robot’s root body. Additionally, we design policies for handstand and footstand tasks, in which the robot balances on the front or hind legs, respectively, while minimizing actuator torque. For fall recovery, we follow \cite{lee2019robust, smith2022legged}, enabling the robot to return to a stable ``home'' posture from arbitrary fallen configurations.

\paragraph{Hardware} We deploy on the \textit{Unitree Go1}, which is a quadruped robot with four legs, each possessing three degrees of freedom. Trained policies run on real-world outdoor terrain (grass and concrete) and indoor surfaces with different friction properties.

\paragraph{Training} We domain randomize for sensor noise, dynamics properties and task uncertainties. We firstly train the policy in flat ground with restricted command ranges within 5 minutes (2x RTX 4090). and finetune it in rough terrain with wider ranges. See \Cref{sec:appendix_locomotion} for more detail.

\paragraph{Results} All four policies (joystick, handstand, footstand, and fall recovery) transfer robustly from simulation to reality, coping with uneven terrain and moderate external perturbations without additional fine-tuning. Videos of these deployments are provided on our project website.

\begin{figure*}[t]
    \centering
    \includegraphics[width=1\linewidth]{figures/action_reels_4.png}
    \caption{\small Footage from four of our deployed policies. a) Go1 joystick policy recovering from a kick while travelling at $\sim$ 2m/s, b) Berkeley humanoid joystick policy tracking an angular velocity command on a slippery surface. c) In-Hand Cube Reorientation transitioning between two target poses. d) Non-prehensile policy issuing torque commands to rotate a block by 180 degrees.}
    \label{fig:action_reel}
\end{figure*}

\subsubsection{Humanoid Locomotion}

\paragraph{Task definition} We implement the same joystick locomotion task as shown for the quadruped environment.

\paragraph{Hardware} We perform sim-to-real experiments on three different humanoid platforms: a) \textit{Berkeley Humanoid} \cite{liao2024berkeley}, a low-cost, lightweight bipedal robot with 6\,DoF per leg, b) \textit{Unitree G1}, a humanoid robot featuring 29\,DoF in total, and c) \textit{Booster T1}, a small-scale humanoid robot with 23\,Dof. All systems are evaluated in indoor environments, with slight variations in surface friction and ground compliance.

\paragraph{Training} We follow the domain randomization and finetuning strategies of the quadruped robot. Training on flat ground lasts under 15 minutes for the Berkeley Humanoid, and under 30 minutes for the Unitree G1 and the Booster T1 on two RTX 4090.

\paragraph{Results} We successfully deploy joystick-based locomotion on the Berkeley Humanoid, demonstrating robust tracking of velocity commands on surfaces ranging from rigid floors to soft and slippery terrains. On the Unitree G1 and Booster T1, our zero-shot policy similarly achieves stable walking and turning on standard indoor floors. Although minor tuning for each platform’s unique dynamics may further enhance performance, these results confirm that our approach generalizes across a range of legged robot morphologies.

\subsection{Manipulation}
\label{sec:results_sim2real_manipulation}

In this section, we present sim-to-real results for a broad range of manipulation tasks, including dexterous in-hand manipulation, non-prehensile manipulation, and vision-based grasping. These tasks illustrate Playground's ability to address a diverse segment of the manipulation spectrum and highlight its robust deployment in real-world settings.

\subsubsection{In-Hand Cube Reorientation}
\label{sec:in_hand_cube_reorientation}

\paragraph{Task definition} We implement an in-hand cube reorientation task using the low-cost, dexterous LEAP hand platform \cite{shaw2023leaphand}, closely following previous works on in-hand manipulation \cite{handa2023dextreme, andrychowicz2020learning}. The task involves reorienting a 7\,cm cube repeatedly from random initial poses to new target orientations in SE(3) without dropping it. Further task details are provided in \Cref{sec:appendix_leaphand_real}.

\paragraph{Hardware} We employ the same hardware configuration as in \cite{li2024_drop}, mounting the LEAP hand on an 80/20 frame with a 3D-printed bracket that tilts the palm downward by 20°. A single Intel RealSense D415 camera, positioned above the workspace, provides pose estimates of the cube via a pretrained detector \cite{handa2023dextreme}. Although occlusions can introduce observation noise, we leave multi-camera extensions to future work. The policy operates at 20\,Hz, which remains comfortably below the USB-Dynamixel control bandwidth.

\paragraph{Training} To promote sim-to-real transfer, we apply domain randomization on the robot parameters as well as cube mass and friction. We also include sensor noise, and we finetune with a progressive curriculum to increase both noisy pose estimates and action regularization. The policy trains within 30\,min on two RTX 4090 GPUs. Further training details are provided in \Cref{sec:appendix_leaphand_real}.

\paragraph{Results} As summarized in \Cref{tab:leaphandcubereorientationdetails}, our learned policy demonstrates early signs of robust in-hand reorientation with MuJoCo Playground. The most frequent failure occurs when the cube becomes wedged in the space present between the fingers and the palm of the LEAP hand, causing the policy to stall. Although less common, we also observe accidental interlocking of the index and thumb, attributed to physical flex in the low-cost hardware. Videos of real-world deployments can be found on our project page. We note that improved camera coverage and more accurate collision geometries could mitigate these edge-case failures, which we leave for future work.

\begin{table}[t]
  \centering
  \caption{\small{In-hand reorientation results on the LEAP hand over 10 trials, reporting the number of consecutive successful rotations before failure. The final two columns show the median and mean of the \#Rotations metric.}}
  \label{tab:leaphandcubereorientationdetails}
  \vspace{-5pt}
  \scriptsize
  \renewcommand{\arraystretch}{1.1}
  \setlength{\tabcolsep}{5pt}
  \resizebox{\columnwidth}{!}{%
    \begin{tabular}{l|cccccccccc|cc}
    \toprule
    & \multicolumn{10}{c|}{\textbf{Trial}} & \multicolumn{2}{c}{\textbf{Summary}} \\
    \cmidrule{2-13}
    & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & Median & Mean \\
    \midrule
    \# Rotations 
      & 3  & 27 & 8  & 2  & 15 & 3  & 4  & 1  & 3  & 5  
      & \textbf{3.5} 
      & \textbf{7.1} \\
    \bottomrule
    \end{tabular}
  }
  \vspace{-10pt}
\end{table}

\subsubsection{Non-Prehensile Block Reorientation}

\paragraph{Task definition}
We present a sim-to-real setup for non-prehensile reorientation of a yoga block on a commonly available Franka Emika Panda robot arm with a Robotiq gripper, achieving high zero-shot success. The task involves moving a yoga block from a random initial pose in the robot’s workspace to a fixed goal pose. A trial is deemed successful if the agent reorients the block within 3\,cm of the goal position and within 10° of the desired orientation.

\paragraph{Hardware}
The policy receives estimates of the block’s position and orientation from an open-source camera tracker \citep{artrackalvar}. We use direct high-frequency torque control at 200\,Hz, where the RL policy outputs motor torques for the arm’s seven joints (with the gripper closed). By learning to control torques rather than joint positions, the agent develops smooth, compliant behavior that transfers effectively to hardware, delivering superior performance even when direct torque control at high frequencies poses learning challenges~\citep{holt2024evolving}. This recipe, therefore, holds broad value for practitioners.

\paragraph{Training}
Robust zero-shot transfer is enabled by stochastic delays and progressive curriculum learning. Each training episode injects randomization into initial poses, joint positions, and velocities, while also imposing action and observation stochastic delays to mirror practical hardware latency. A simple curriculum gradually increases the block’s displacement and orientation range upon each success, preventing overfitting to easier conditions. Training takes 10 minutes on 16x A100 devices.

\paragraph{Results}
These techniques, combined with 200\,Hz direct torque control, produce a policy resilient to real-world perturbations. The agent reliably reorients the block on hardware with no additional fine-tuning as shown in \Cref{table:blockreorientfrankarobotiqrealresults}. Videos of real-world deployments are provided on our project website. Additional implementation details are given in \Cref{appendix:RealWorldNonprehensileReorientationPolicyPerformance}.

\begin{table}[!tb]
  \centering
  \caption{\small{Sim-to-real reorientation performance on the Franka Emika Panda robot, evaluated across 35 hardware trials. Each metric is reported as the median and mean (with a 95\% confidence interval). The success rate is bolded to highlight final task performance. The training was done on 16x A100 GPUs.}
  }
  \vspace{-10pt}
  \smallskip
  \resizebox{\columnwidth}{!}{
    \begin{tabular}{@{}l|c|c}
    \toprule
    Metric & Median & Mean ± 95\% Confidence Interval \\
    \midrule
    \textbf{Real Success (\%)} $\uparrow$ & \textbf{100} & \textbf{85.7 ± 12.2} \\
    Position Error (cm) $\downarrow$ & 1.95 & 5.28 ± 3.26 \\
    Rotation Error (°) $\downarrow$ & 1.72 & 3.32 ± 1.59 \\
    \bottomrule
    \end{tabular}
  }
  \vspace{-20pt}
  \label{table:blockreorientfrankarobotiqrealresults}
\end{table}


\subsubsection{Pick-Cube from Pixels}
\label{sec:results_pickcube_pixels}


\paragraph{Task definition}
We demonstrate sim-to-real transfer with pixel-based policies on a Franka Emika Panda robot. The robot must reliably grasp and lift a small 2\,\(\times\)\,2\,\(\times\)\,3\,cm block from a random location on the table and move it 10\,cm above the surface. The policy receives a \(64\times64\) RGB image as input and outputs a Cartesian command, which is processed by a closed-form inverse kinematics solution to yield joint commands. To simplify the task, we restrict the end-effector to a 2D Y-Z plane (while always pointing downward) and provide a binary jaw open/close action.

\paragraph{Hardware}
We use a Franka Emika Panda robot with a single Intel RealSense D435 camera mounted to capture top-down RGB images. The policy operates at 15\,Hz, and we run inference on an RTX 3090 GPU. Our setup ensures that the block starts within the field of view over a 20\,cm range along the y-axis.

\paragraph{Training}
To bridge the sim-to-real gap, we apply domain randomization across visual properties such as lighting, shadows, camera pose, and object colors. We also add random brightness post-processing, and introduce a stochastic gripping delay of up to 250\,ms.
We choose a reduced action dimension of three (Y-movement, Z-movement, and discrete jaw control) for training sample efficiency, but we have found that the task can also be solved in full Cartesian or joint space given additional camera perspectives and more training samples. Training in simulation takes ten minutes on a single RTX 4090.

\paragraph{Results}
Our policy achieves a 100\% success rate in 12 real-world trials, robustly grasping the block and lifting it clear of the table. It demonstrates resilience to moderate variations in lighting and minor camera shaking, as shown in the videos on our project website. These findings highlight MuJoCo Playground’s capacity for training pixel-based policies that transfer reliably to real hardware in a zero-shot manner. Additional implementation details are described in \Cref{appendix:MadronaMJXBenchmark}.

\subsection{Training Throughput}

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{figures/device_topo_leap_cube_reward.pdf}
    \caption{\small Training wallclock time for LeapCubeReorient on different GPU device topologies. 1x 4090 takes $\sim$ 2080 (s) to train and 8x H100 takes $\sim$ 670 (s) to train. All runs use the \emph{same hyperparams} (e.g. 8192 num envs); we leave tuning hyperparams per topology as a future exercise.}
    \label{fig:leap_cube_throughput}
\end{figure}

Across our sim-to-real studies, we used several GPU hardware setups and topologies, including NVIDIA RTX 4090, A100, and H100 GPUs. In \Cref{fig:leap_cube_throughput}, we break down the training performance of the LeapCubeReorient environment on different configurations for a fixed set of RL hyper-parameters, demonstrating that MJX is effective on both consumer-grade and datacenter graphics cards. We see that GPUs with higher theoretical performance and larger topologies can reduce training time by a factor of 3x on a contact-rich task like in-hand reorientation. We leave optimization of topology-specific hyper-parameters as future work (e.g. the number of environments should ideally increase for larger topologies to maximize throughput, as long as the RL algorithm can utilize the increase in data per epoch). In \Cref{tab:dm_control_env_training_throughput}, \Cref{tab:locomotion_training_throughput}, and \Cref{tab:manipulation_training_throughput} in the appendix, we report RL training throughput for all environments in MuJoCo Playground on a single~A100~GPU.

\subsubsection{Training Throughput with Batch Rendering}
\label{sec:madrona_bottlenecks_mini}

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{figures/madrona_mjx_fps_comparison.pdf}
    \caption{\small Environment steps per second on the single-camera CartpoleBalance and PandaPickCubeCartesian environments with pixel-based observations from our on-device renderer.}
    \label{fig:madrona_mjx_main}
\end{figure}

\Cref{fig:madrona_mjx_main} highlights the throughput of stepping two of our environments with pixel observations at different resolutions. By pairing MJX physics with Madrona batch rendering, our Cartpole and Franka environments unroll at roughly 403,000 and 37,000 steps per second respectively. Note that our Franka physics are over 20x more costly than Cartpole's, resulting in the lower sensitivity of FPS to image resolution.

Computationally, pixel-based policy training generally involves four main components: physics simulation, observation rendering, policy inference and policy updates. \Cref{fig:madrona_mjx_main} only encapsulates the former two and is not fully indicative of overall training throughput.

We find that in the context of a PPO training loop, physics, rendering, and inference together only comprise 9\% and 43\% of the Cartpole and Franka total training times, respectively, with most of the time spent updating the expensive CNN-based networks. Hence, compared to traditional on-policy training pipelines, we have shifted our bottleneck from collecting data to processing it. Training bottlenecks are further discussed in \Cref{sec:appendix_madrona_bottlenecks} under \Cref{tab:madrona_training_breakdown_measured} and \Cref{tab:madrona_training_breakdown_calc}. Further performance benchmarking and a rough comparison against prior simulators are in \Cref{sec:madrona_benchmarking}.

\subsubsection{RL Libraries}
\label{sec:results_different_rl_libs}

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{figures/torch_go1_comparison.pdf}
    \caption{\small Reward curves for PPO trained with RSL-RL and brax on an RTX-4090 GPU for 3 seeds each on the Unitree Go1.}
    \label{fig:torch_comparison_go1}
\end{figure}

While MuJoCo Playground primarily uses a JAX-based physics simulator, practitioners are able to use both JAX and torch-based RL libraries for training RL agents. In \Cref{fig:torch_comparison_go1}, we show reward curves for PPO agents trained using both Brax~\cite{freeman2021brax} and RSL-RL \cite{rsl_rl} implementations. Each corresponding RL library is trained with custom hyperparameters tailored to the corresponding PPO implementation. Both libraries are able to achieve successful rewards and gaits within similar wallclock times. All other results in this paper were obtained using the Brax PPO and SAC implementations.
