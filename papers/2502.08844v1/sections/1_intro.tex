\section{Introduction}

Reinforcement learning (RL)~\cite{kober2013reinforcement} with subsequent transfer to hardware (sim-to-real)~\cite{zhao2020sim}, is emerging as a leading paradigm in modern robotics~\cite{kaufmann2023champion, lee2019robust, miki2022learning}. The benefits of simulation are obvious -- safety and cheap data. The recipe involves four steps:
\begin{enumerate}[leftmargin=0.8cm, rightmargin=0.4cm, topsep=0em, itemsep=2pt] 
    \item Create a simulated environment that matches the~real~world.
    \item Encode desired robot behavior with a reward function.
    \item Train a policy in simulation.
    \item Deploy to the robot. 
\end{enumerate}

The key enabler of this approach is a simulator that is realistic, convenient, and fast. 

The realism requirement is self-evident, the ``digital twin'' of step 1 demands a minimal level of fidelity~\cite{zhao2020sim}. Convenience and usability are equally critical, streamlining the creation, modification, composition, and characterization (system identification) of simulated robots.

The importance of speed is less obvious -- why does it matter if training takes ten minutes or ten hours? The answer lies in reward design (step 2), which cannot be easily automated: what the robot \emph{ought} to do is an expression of human preference. Even if reward design is semi-automated~\cite{ma2023eureka}, the process remains iterative: RL excels at finding policies that obtain reward, but the resulting behavior is often irregular \href{https://www.youtube.com/watch?v=EI3gcbDUNiM&t=257s}{in unexpected ways}.
Since steps 2 and 3 (and occasionally step 4) must be repeated~\cite{chebotar2019closing}, \mbox{\emph{time-to-robot}} becomes critical: the time from when you ask the robot to do something until you see what it thinks you meant.

RL is computationally intensive, requiring an enormous number of agent-environment interactions to train effective policies~\cite{ibarz2021train}. GPU-based simulation can significantly accelerate this process for two key reasons. First, the median GPU is far more powerful than the median CPU~\cite{wang2020benchmarking}, and while high core-count CPUs exist, they are uncommon. Second, by keeping the entire agent-environment loop on device, we can harness the high-throughput, highly parallel architecture~\cite{makoviychuk2021isaac, freeman2021brax}. This is especially true for \emph{on-policy} RL~\cite{schulman2017proximal, andrychowicz2020matters}, which employs GPU-friendly, wide-batch operations. Locomotion and manipulation tasks which previously required days of training on multi-host setups~\cite{andrychowicz2020learning,tan2018sim}, can now be solved within minutes or hours on a single GPU~\cite{rudin2022learning, handa2023dextreme}.

With this work, we aim to further advance and make sim-to-real robot learning even more accessible. We introduce MuJoCo Playground, a fully open-source framework for robot learning designed for rapid iteration and deployment of sim-to-real reinforcement learning policies. We build upon MuJoCo XLA~\cite{mujocoxla} (MJX), a JAX-based branch of the MuJoCo physics engine that runs on GPU, enabling training directly on device. Besides physics and learning, we leverage the open-source nature of our ecosystem to incorporate on-device rendering through the Madrona batch renderer~\cite{shacklett2023extensible}, facilitating training of vision-based policies end-to-end, without teacher-student distillation~\cite{agarwal2023legged}. With a straightforward installation process (\texttt{pip install playground}) and cross-platform support, users can quickly train policies on a single GPU. The entire pipeline---from environment setup to policy optimization---can be executed in a single Colab notebook, with most tasks requiring only minutes of training~time.

MuJoCo Playgroundâ€™s lightweight implementation greatly simplifies sim-to-real deployment, transforming it into an interactive process where users can quickly tweak parameters to refine robot behavior. In our experiments, we deployed both state- and vision-based policies across six robotic platforms in less than eight weeks. We hope that MuJoCo Playground becomes a valuable resource for the robotics community and  expect it to continue building on MuJoCo's thriving open-source ecosystem.

\noindent Our work makes three main contributions:
\begin{enumerate}
    \item We develop a comprehensive suite of robotic environments using MJX \cite{mujocoxla}, demonstrating sim-to-real transfer across diverse platforms including quadrupeds, humanoids, dexterous hands, and robot arms.
    \item We integrate the open-source Madrona batch GPU renderer \cite{shacklett2023extensible} to enable end-to-end vision-based policy training on a single GPU device, achieving zero-shot transfer on manipulation tasks.
    \item We provide a complete, reproducible training pipeline with notebooks, hyperparameters, and training curves, enabling rapid iteration between simulation and real-world deployment.
\end{enumerate}

\begin{figure*}[t!]
    \centering
    \includegraphics[width=1\linewidth]{figures/env_grid.png}
    \caption{\small A preview of locomotion and manipulation environments available in MuJoCo Playground.}
    \label{fig:env_grid}
\end{figure*}
