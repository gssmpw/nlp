\section{Related Work}


\paragraph{Physics simulation on GPU} The PhysX GPU implementation \cite{liang2018gpu} has been heavily relied on for robotic sim-to-real workloads via IsaacGym \cite{makoviychuk2021isaac} and more recently Isaac Lab \cite{mittal2023orbit}. The PhysX GPU implementation, however, is closed-source~\cite{liang2018gpu} and researchers lack the ability to extend the simulator for their specific tasks or workloads. Several GPU-based physics engines are open-source, such as MJX \cite{todorov2012mujoco, mujocoxla}, Brax \cite{freeman2021brax}, Warp \cite{macklin2022warp}, and Taichi \cite{hu2019difftaichi}. Only a limited set of robot environments~\cite{sferrazza2024humanoidbench, xue2024full} leverage these open-source counterparts, in contrast to the wide range of robotic sim-to-real results that were achieved with IsaacGym and Isaac Lab. Most recently, Genesis \cite{Genesis} provides a rigid-body implementation similar to MJX implemented using Taichi, that allows for dynamic constraints/contacts. However, sim-to-real results are still limited to a few locomotion policies.

\paragraph{Sim-to-real RL} A variety of locomotion and manipulation policies have successfully been deployed in the real world zero-shot \cite{cheng2024extreme, zhuang2023robot, li2024reinforcement, long2024learning, radosavovic2024learning, singh2024dextrah, cho2024corn}. We complement these results by demonstrating zero-shot sim-to-real on the Leap Hand, Unitree Go1, Berkeley Humanoid, Unitree G1, Booster T1, and Franka arm using MuJoCo rather than closed-source simulators. Similar to \cite{mittal2023orbit, makoviychuk2021isaac}, we provide code for environments and training.

\paragraph{Vision-based RL} State-of-the-art algorithms such as DrQ \cite{yarats2021drqv2}, RL from Augmented Data (RAD) \cite{laskin_lee2020rad}, Dreamerv3 \cite{hafner2023dreamerv3}, TD-MPC2 \cite{hansen2024tdmpc2}, and EfficientZeroV2 \cite{wang2024efficientzero} have pushed pixel-based RL performance over the years. Transferring these advances to the real world is appealing, as visual control loops offer precise positioning and robust behaviour in uncontrolled in-the-wild scenarios \cite{haiderbhai2024cutrope}. The limitation of training directly from pixel data is the large visual sim-to-real gap between simulation and reality, which is often overcome using domain randomization \cite{domainrand2017}. However, such training methods require exponentially more training samples. As as result, policies are typically trained with proprioceptive observations in simulation and subsequently distilled into vision-based policies offline \cite{chen2022system, haarnoja2024learning, cheng2024extreme}, or trained with smaller exteroceptive observations \cite{miki2022learning, agarwal2023legged}. With Madrona, we are able to train vision-based policies directly in simulation without a distillation step using high-throughput batch rendering, similar to \cite{mittal2023orbit} and \cite{tao2024maniskill3}.
