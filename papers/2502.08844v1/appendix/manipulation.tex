\section{Manipulation}
\label{sec:appendix_manipulation}
\subsection{Environments}
\label{sec:appendix_manipulation_envs}

\begin{table*}[!ht]
\centering
\begin{tabular}{|l|p{10cm}|} %
\hline
\textbf{Robot}  & \textbf{Environment} \\ \hline
Aloha          & SinglePegInsertion \\ \hline
Franka Emika Panda &  PickCube, PickCubeOrientation, PickCubeCartesian, \mbox{OpenCabinet}  \\ \hline
Franka Emika Panda, Robotiq Gripper &  PushCube \\ \hline
Leap Hand & Reorient, RotateZAxis \\  \hline
\hline
\end{tabular}
\caption{Manipulation environments implemented in MuJoCo Playground by robot platform.}
\label{tab:manipulation_envs}
\end{table*}


\subsection{RL Training Results}
\label{sec:appendix_manipulation_curves}


For all manipulation environments implemented in MuJoCo Playground, we train with PPO using the RL implementation from \cite{freeman2021brax} and we report reward curves below. In \Cref{fig:manipulation_step_reward} we report environment steps versus reward and in \Cref{fig:manipulation_time_reward} we report wallclock time versus reward. All environments are run across 5 seeds on a single A100 GPU.

\begin{figure}[ht]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/manipulation_reward.pdf}
    \caption{\small Reward vs environment steps for brax PPO. All settings are run with 5 seeds on a single A100 GPU device.}
    \label{fig:manipulation_step_reward}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/manipulation_reward_wallclock.pdf}
    \caption{\small Reward vs wallclock time for brax PPO. All settings are run with 5 seeds on a single A100 GPU device. Notice that the initial flat region measures the compilation time for the training + environment code.}
    \label{fig:manipulation_time_reward}
\end{figure}

\subsection{RL Training Throughput}
\label{sec:appendix_manipulation_throughput}

We show RL training throughput for all manipulation environments below in \Cref{tab:manipulation_training_throughput}. In \Cref{fig:leaphand_device_topo} we show reward versus wallclock time on different GPU devices and topologies for the LeapCubeReorient environment.

\begin{table*}[ht]
\centering
\begin{tabular}{|l|c|c|}
\hline 
 Env                   & PPO Steps per Second     \\
\hline 
 AlohaSinglePegInsertion  & 121119 $\pm$ 2159 \\
 LeapCubeReorient         & 76354 $\pm$ 143   \\
 LeapCubeRotateZAxis      & 76602 $\pm$ 179   \\
 PandaOpenCabinet         & 136007 $\pm$ 1553 \\
 PandaPickCube            & 140386 $\pm$ 1707 \\
 PandaPickCubeCartesian   & 38015 $\pm$ 5302  \\
 PandaPickCubeOrientation & 140429 $\pm$ 1604 \\
 PandaRobotiqPushCube     & 487341 $\pm$ 4346 \\
\hline 
\end{tabular}
\caption{Training throughput is displayed for all the Manipulation environments on an A100 GPU device across 5 seeds using brax PPO and the RL hyperparameters in \Cref{sec:rl_hypers}. We report the 95th percentile confidence interval.}
\label{tab:manipulation_training_throughput}
\end{table*}


\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/device_topo_leap_cube_full_reward.pdf}
    \caption{\small Training wallclock time for LeapHandReorient on different GPU devices and topologies.}
    \label{fig:leaphand_device_topo}
\end{figure}

\clearpage



\subsection{Real-world Cube Reorientation with a Leap Hand}
\label{sec:appendix_leaphand_real}

In this section, we present the technical details of our real-world cube reorientation task using the LEAP Hand, covering the simulation environment, training process, hardware interface, and camera-based object pose estimation.

\subsubsection{Simulation Environment}

The in-hand reorientation environment is designed to sequentially re-orient a cube within the palm of a robotic hand, without dropping the cube. The cube is initialized randomly above the palm of the hand. The policy then receives the joint angle measurements, estimated cube pose, and previous action. Upon reaching a target orientation within a 0.4\,rad tolerance, a new orientation is sampled and the success counter is incremented. We continue re-sampling new target orientations until the cube is dropped or the hand becomes stuck for over 30\,s. To avoid trivial adjustments, new orientations are sampled at least 90° away from the previous goal (as in \cite{handa2023dextreme, li2024_drop}). The cube must reach a target orientation within 0.1\,rad (as opposed to 0.4\,rad in the real-world setup).

\paragraph{Policy Inputs and Actions}
As in locomotion environments, we use an asymmetric actor--critic setup, in which the policy network (actor) and the value network (critic) receive different observation inputs. The policy network is fed observations as outlined below, while the value network additionally receives uncorrupted robot pose, robot velocity, fingertip positions, cube pose, cube velocity, and perturbation forces.
\begin{itemize}[leftmargin=1em]
    \item \textbf{Observations} (a) noisy estimates of the hand joint positions and velocities, (b) joint position errors (commanded vs achieved), (c) noisy estimates of the cube pose (distance of the cube to the palm center and cube orientation error), and (d) the previous commanded joint positions.  
    \item \textbf{Actions} 16 relative joint positions.
\end{itemize}

\paragraph{Training Setup}  
To promote sim-to-real transfer, we apply domain randomization on friction, cube mass, joint offsets, motor friction, reflected inertia, and PD gains, as well as link masses and sensor noise. We also add 2\,cm positional and 0.1\,rad rotational noise to the cube pose. We conduct two main training phases. During the first 200\,M steps, we train without random pose injection and torque limits. We then perform a 100\,M-step fine-tuning stage in which we introduce random pose ``injections'' with a 0.1 probability to mimic ``freak-out'' moments in real pose estimation (e.g., due to occlusions) and impose torque limits to match the real hardware.

\subsubsection{System Identification and Domain Randomization}
The original simulation environment for sim-to-real transfer provided by the LEAP Hand \cite{shaw2023leaphand} does not include robust system identification and instead relies heavily on manual parameter tuning. To improve both the performance and transparency of the system, we performed system identification on the DYNAMIXEL servo actuator used in the hand.

The armature inertia (i.e., rotor inertia reflected through the gearbox) for each joint is:
$
I_\text{a} = k_\text{g}^2 I_\text{r},
$
where \( k_\text{g} = 288.35 \) is the gear ratio from the supplier's data sheet, and
$
I_\text{r} = \frac{1}{2} m_\text{r} r_\text{r}^2 = \SI{1.7e-8}{\kilogram\meter\squared}
$
is the rotor inertia. To obtain \(I_\text{r}\), we assumed a uniform mass distribution of the rotor, based on physical disassembly and measurements of the rotor mass (\(m_\text{r} = \SI{2.0e-3}{\kilogram}\)) and radius (\(r_\text{r} = \SI{4.12e-3}{\meter}\)).

Because accurately modeling and measuring friction losses is difficult, we set 10\% of the maximum torque as the nominal friction value, and employed heavy domain randomization to account for uncertainties. 

For training, the servo actuator was controlled using a PD mapping similar to the locomotion setup in~\eqref{eq:pd_mapping}. However, during real-world deployment, the control law running on the servo actuator is: $i = k_\text{p}^\text{m} \bigl(\theta_\text{des}^\text{m} - \theta^\text{m}\bigr)
\;-\;
k_\text{d}^\text{m} \dot{\theta}^\text{m}$,
where \(i\) is the motor current command, and \(k_\text{p}^\text{m}, k_\text{d}^\text{m}, \theta_\text{des}^\text{m}, \theta^\text{m}\) are expressed in units different from those used in training. To reconcile these discrepancies, we assume \(\tau = k_\text{t} i\) and carefully compute the mappings based on the motor specifications provided in the data sheet.

Unlike the locomotion setup, the DYNAMIXEL actuator does not perform true current control (i.e., no direct motor current feedback). As a result, the above PD controller may deviate from the ideal behavior. To mitigate this mismatch, we introduce randomization in \(k_\text{p}\) and \(k_\text{d}\) parameters during training.

\subsubsection{Real Robot Setup}
We deploy the learned policy on the hand using its open-source software, with the following modifications:
\begin{itemize}[leftmargin=1em]
    \item \textbf{Control Frequency.} We reduce the policy control frequency from 150\,Hz to 20\,Hz in both simulation and real-world deployment, due to jitter issues with the low-level USB driver at higher frequencies.
    \item \textbf{System Identification.} We use the same torque (current) limit, stiffness, and damping parameters in training, guided by the system identification results described above.
\end{itemize}

\subsubsection{Vision-based Pose Estimator}

We use the vision-based cube pose estimator from \citep{handa2023dextreme} in order to solve for the pose of the cube, although any equivalent method of obtaining the SE(3) camera-to-object transformation would work. Given the local-space 3D coordinates of the cube and the 2D keypoints from the pose estimator, we solve for the camera-to-world transformation. Using camera-to-hand intrinsics calibration, we can then find the hand-to-cube transformation which is used as input to the policy. We run the cube pose estimator at 15\,Hz. During manipulation, we observe some small jitters or missed detections, but generally it is stable. Despite having access to three cameras on the physical hardware setup (\cite{li2024_drop}, we elect to use only one for simplicity.

\clearpage


\subsection{Real-world Non-prehensile Block Reorientation with a Franka-Robotiq Arm}
\label{appendix:RealWorldNonprehensileReorientationPolicyPerformance}

In this section, we provide technical details for our block reorientation task on a real Franka Emika Panda robot with a Robotiq gripper, including the simulation environment, training process, robot hardware interface, and camera-based object pose estimation. Our approach enables reliably learning and deploying a policy for non-prehensile manipulation of a yoga block, requiring only a brief training time in simulation while allowing zero-shot transfer to the real robot.  

\subsubsection{Simulation Environment}
\label{appendix:RealWorldNonprehensileReorientationPolicyPerformance:sim-env}


The simulation environment (\Cref{fig:panda-robotiq-sim-env}) is designed to reorient a rectangular yoga block within a tabletop workspace region. The block is initialized at a random position and orientation subject to workspace bounds, and is then pushed, slid, or tapped to a desired goal pose at the center of the workspace. The policy uses 7D torque control signals for the robot arm and a fixed, closed Robotiq gripper. We include a simple termination condition when the block leaves the workspace or the end-effector violates safety constraints (e.g., collides with walls or floors).

\paragraph{Key Features}
\begin{itemize}[leftmargin=1em]
    \item \textbf{High-frequency torque control at 200\,Hz.} Each simulation step is advanced at a high frequency to match the targeted real-world controller rate.  
    \item \textbf{Curriculum learning.} We randomize initial joint positions, block poses, latencies in actions and observations, and other environment factors. A progressive curriculum increases the difficulty by gradually expanding the block's displacement and orientation range.  
    \item \textbf{Observation Delay.} Both actions and observations are delayed by random amounts at each episode step to approximate real hardware latencies.
    \item \textbf{Reward Shaping.} Shaped rewards encourage the robot to (i) stay near a nominal joint configuration, (ii) minimize velocities, (iii) keep the end-effector near the block, (iv) push the block toward the goal, and (v) orient the block to the desired angle.  
\end{itemize}

\paragraph{Policy Inputs and Actions} As shown in the environment code:
\begin{itemize}[leftmargin=1em]
    \item \textbf{Observations} (a) noisy estimates of the block pose, (b) current and recent robot joint positions and velocities, (c) the estimated end-effector pose, and (d) the target block pose.  
    \item \textbf{Actions} are 7D torque commands applied at the robot's joints. A constant action for the gripper (fingers closed) is appended for technical reasons in MuJoCo but remains fixed at a configured grasp.
\end{itemize}

\paragraph{Simulated Environment Details}  
\begin{itemize}[leftmargin=1em]
    \item \textbf{Gravity Compensation and Torque Bounds.} We configure the MuJoCo model to match the real robot's gravity compensation mode. Torque bounds are set to $8\,\mathrm{Nm}$ per joint in simulation, reflecting the approximate safe torque limit on real hardware.  
    \item \textbf{Collision Geometry.} The environment enforces collisions with floor, walls, and the block. The Robotiq gripper is held fixed but included for contact modeling.  
    \item \textbf{Delayed Observations and Actions.} We adopt random delays (between 1 and 3 steps for actions, and 6 to 12 steps for observations) to emulate real system communication latencies and sensor delay, following best practices in sim-to-real transfer.  
\end{itemize}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.4\linewidth]{figures/RealWorldNonprehensileReorientationPolicySim.png}
    \caption{\small Example MuJoCo scene of our block reorientation environment. The block is pushed toward the center.}
    \label{fig:panda-robotiq-sim-env}
\end{figure}

\paragraph{Training Setup}  
We train the policy on a 16x NVIDIA A100 GPU set-up for ten minutes of wall-clock time, with 3000 steps per episode (with action repeat set to 4, effectively running 750 policy decisions per episode). During training, the block's pose, robot states, and delays are heavily randomized. The final policy was selected from a checkpoint that achieved the highest success rate in the simulator.  

\subsubsection{Real Robot Setup}
\label{appendix:RealWorldNonprehensileReorientationPolicyPerformance:real-env}

We deployed the final trained policy on a Franka Emika Panda manipulator equipped with a Robotiq 2F-85 gripper and an integrated force torque sensor (Robotiq FT-300). Figure~\ref{fig:panda-real-setup} illustrates the hardware platform used for our experiments.

\paragraph{Direct Torque Control with Franka FCI}  
We interface with the robot via the Franka Control Interface (FCI) and send torque commands at 200\,Hz:
\begin{itemize}[leftmargin=1em]
    \item \textbf{Gravity Compensation.} The Panda is configured to compensate for the arm's own weight. The policy torques therefore focus on regulating the contact interactions with the block, making the system compliant.  
    \item \textbf{Bypassing Low-level PID Gains.} We avoid additional position or velocity tracking by sending raw joint torques. This significantly reduces the overhead of tuning any gain schedules and allows the learned policy to directly control contact forces.  
    \item \textbf{Safety Considerations.} We define software torque limits and monitor the robot's built-in safety stops and collision detection thresholds. In practice, the learned policy operates well within these limits to gently push the block.  
\end{itemize}

\paragraph{Control and Communication Pipeline}  
We use a lightweight C++/ROS node that relays torque commands to the Franka FCI at 200\,Hz:
\begin{itemize}[leftmargin=1em]
    \item \textbf{Policy Node in Python.} Our Python node loads the final trained policy (JIT-compiled for inference speed). At each 5\,ms tick, it receives the robot’s current joint positions, velocities, and the estimated block pose from ROS topics.  
    \item \textbf{Torque Message Publication.} The Python node computes a new 7D torque vector and publishes it as a ROS message to the C++ node. This node directly invokes the FCI’s real-time interface to set joint torques.  
    \item \textbf{Timing Synchronization.} We maintain a fixed 200\,Hz loop, matching the simulator’s update frequency. This avoids aliasing or missed steps and ensures that delays in the real system resemble the random delays already modeled in simulation.  
\end{itemize}

\subsubsection{Camera-based Block Pose Estimation}

The policy requires an estimate of the block’s 6D pose (position and orientation). We implement a multi-camera setup with four commodity RGB cameras:
\begin{itemize}[leftmargin=1em]
    \item \textbf{Intrinsics and Extrinsics.} Each camera is calibrated via OpenCV’s standard calibration procedure. We record images of a checkerboard pattern from various viewpoints to obtain precise intrinsic parameters (focal length, principal point) and extrinsic transformations.  
    \item \textbf{AR Tag Tracking.} We attach an Alvar~\citep{artrackalvar} fiducial marker to each face of the yoga block. Each camera runs the Alvar pose estimation pipeline. The final block pose is computed as the uniform average of valid detections.  
    \item \textbf{Placement Recommendations.} To improve coverage and reduce occlusions, we place two cameras at a lower height (approximately 40\,cm above the table) and two cameras overhead (around 80\,cm), all aimed toward the center of the workspace, inline with the base of the arm. This diversity of vantage points helps maintain robust tracking, even as the block is manipulated.  
    \item \textbf{ROS Integration.} Each camera node publishes pose estimates (with timestamps). A central ROS node fuses these estimates and broadcasts the block pose as a \texttt{geometry\_msgs/PoseStamped} message at about 30--60\,Hz.  
\end{itemize}

\paragraph{Summary} With this environment and training protocol, policies learned in simulation (under domain randomization and fast torque-control loops) exhibit a robust ability to transfer zero-shot to real hardware. Additionally, we encountered several limitations with the policy and workspace. For example, the policy sometimes pushed the block outside the robot’s workspace, making it impossible for the robot to reach it. We also observed that early versions of the policy moved the block too quickly, exceeding the robot’s force limit and causing it to pause. To address this, we introduced torque penalties, enabling the robot to maintain similar behavior while minimizing force. In summary we found that minimal engineering overhead was needed to align the MuJoCo-based environment with the real robot’s dynamic properties, underscoring the effectiveness of torque-based sim-to-real strategies with MuJoCo Playground.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/RealWorldNonprehensileReorientationPolicyReal.png}
    \caption{\small Real Franka Emika Panda robot with a Robotiq gripper, pushing the yoga block to the goal region.}
    \label{fig:panda-real-setup}
\end{figure}

\clearpage

\subsection{Real-world Franka PickCube from Pixels}
\label{appendix:RealWorldPixels}
To highlight the sim-to-real viability of our pixel-based environment, we highlight a robust real-world transfer using the Franka PickCube task.

\textbf{Task Description.} In our PickCube task, the goal of the robot is to move and grasp a 2x2x3 cm upright cube and return it to a fixed target position. To enable robust deployment with a single RGB camera, we limit both the object randomization and the robot's action space to a fixed Y-Z plane. We set the target in simulation to be (x, 0.0, 20.0), where x is set such that both the cube and the gripper initialize are in the same plane. Success is defined in simulation as lifting the object to a target height of 17 cm, and in real experiments as a stable grasp followed by lifting the object at least 10 cm above the table. The object's starting position is randomized along the Y-axis within a 20 cm range centered around 0. Because we train with randomized camera pose and a black background, we lay white tape over the range of possible cube starting positions to allow the memory-less policy to gauge its progress from the grasping site to the target height.

\textbf{Training.} We use a similar reward shaping scheme as \cite{petrenko2023dexPBT}, using sparse rewards to encourage lifting the cube and bringing it close to the target position and dense rewards to guide the policy search in between. To simplify reward tuning, the dense reward terms only consider progress: $r_t = \mbox{clip}\left(\sum_i r_{t,i} - \max(r_1, r_2, ..., r_{t-1}), 0\right)$. This helps to emphasize the sparse terms during training. To improve sample efficiency, we terminate the policy upon completion. We train with randomized lighting conditions, colors, brightness and camera pose for robust real-world transfer as shown in \Cref{fig:madrona-highres-domain-rando}. Similar to the non-prehensile task in Section \ref{appendix:RealWorldNonprehensileReorientationPolicyPerformance:sim-env}, we adopt a random delay of 0 to 5 steps for the gripper action, as the real system has a small delay before the grippers begin to close. With an environment step of 50 ms, this results in the agent learning to adapt to a action delay of up to 0.25 s. We find the resulting conservative grasp behaviour to be important for sim-to-real transfer. We disable all except the pair-wise collisions between the gripper fingers and cube to increase simulation throughput.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\linewidth]{figures/madrona_domain_rand_tiles.png}
    \caption{\small Policy inputs across domain randomized environments (64x64 pixels each) used while training the deployed PandaPickCubeCartesian agent. Lighting conditions, colors, brightness and camera pose are all randomized.}
    \label{fig:madrona-highres-domain-rando}
\end{figure}

\textbf{Agent.} Both the agent and critic networks comprise of a standard lightweight CNN architecture \cite{mnih2015human} followed by two hidden dense layers with size 256. Each channel of the input RGB image is individually normalised per sample by subtracting its mean and dividing it by its standard deviation. The policy network outputs a 3 value action from a single RGB camera looking down towards the gripper (Figure \ref{fig:panda-pixels-real-setup}). The first two actions are Cartesian increments in the Y and Z directions that is subsequently solved by an inverse kinematics controller \cite{he2021analytical}. X movement is ignored so that the gripper is restricted to the vertical plane of the block. We discretize the third action dimension to command a closed gripper when the policy value is below zero and an open position when greater than or equal to zero. All values are outputted in the range -1 to 1.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/RealWorldPixelsPickCube.png}
    \caption{\small \textbf{Left.} Franka Research robot with a Realsense camera capturing input images. \textbf{Right.} Policy inputs from one embodied rollout.}
    \label{fig:panda-pixels-real-setup}
\end{figure}

\textbf{Hardware.} We train our deployed policies within ten minutes on a single consumer-grade RTX 4090 GPU paired with a i9-14900KF processor. See \Cref{sec:madrona_training_curves} for training curves for the PandaPickCubeCartesian environment. We deploy on a Franka Research arm with an Intel D435 Realsense camera, using an RTX 3090 GPU for policy inference running at 15Hz.

\textbf{Control and Communication Pipeline.} We use a C++/ROS stack to execute our vision-based policies in real life. Camera images are square-cropped and down-sampled to 64x64 pixels before being passed to a lightweight C++ ONNX ROS Node for inference to produce a Cartesian increment and gripper command. This command is passed to a C++ ROS Node that computes joint commands using the same IK solution as used for training. These commands are output to a final ROS Node that wraps the Franka Control Interface (FCI) to control the robot joints. The control loop runs at 15 Hz, set by the incoming camera stream. We find that sim2real performance drastically improves from roughly calibrating the Cartesian increment scale in our physical setup to the one that the policy was trained on.

\clearpage
