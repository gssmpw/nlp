\section{Locomotion}
\label{sec:appendix_locomotion}

\subsection{Environment}
\label{sec:appendix_locomotion_env}

In \Cref{tab:locomotion_envs} we show all the locomotion environments available in MuJoCo Playground, broken down by robot platform and available controller.

\begin{table*}[!ht]
\centering
\begin{tabular}{|l|l|p{10cm}|} %
\hline
\textbf{Robot} & \textbf{Type} & \textbf{Environment} \\ \hline
Google Barkour     & Quadruped      & JoystickFlatTerrain, JoystickRoughTerrain \\ \hline
Berkeley Humanoid  & Biped         & Joystick \\ \hline
Unitree G1      & Biped     & Joystick \\ \hline
Booster T1      & Biped     & Joystick \\ \hline
Unitree Go1      & Quadruped     & JoystickFlatTerrain, JoystickRoughTerrain, Getup, \mbox{Handstand,} Footstand \\ \hline
Unitree H1 & Biped & InplaceGaitTracking, JoystickGaitTracking  \\ \hline
OP3 & Biped & Joystick \\ \hline
Boston Dynamics Spot & Quadruped & JoystickFlatTerrain, JoystickGaitTracking, Getup \\ \hline
\hline
\end{tabular}
\caption{Locomotion environments implemented in MuJoCo Playground by robot platform.}
\label{tab:locomotion_envs}
\end{table*}

\subsection{RL Training Details}

\subsubsection{Observation and Action}
We use a unified observation space across all locomotion environments:
\begin{enumerate}[label=(\alph*)]
    \item Gravity projected in the body frame,
    \item Base linear and angular velocity,
    \item Joint positions and velocities,
    \item Previous action,
    \item (Optional) User command for joystick-based tasks.
\end{enumerate}

For humanoid locomotion tasks, a phase variable~\cite{shao2021learning} is introduced to shape the gait. This phase variable cycles between $-\pi$ and $\pi$ for each foot, representing the gait phase. To capture this information effectively, the $\cos$ and $\sin$ of the phase variable for each foot are included in the observation space. This representation provides a continuous and smooth encoding of the phase, enabling the policy to synchronize its actions with the desired gait cycle.

The action space is defined differently depending on the task. 
For joystick tasks, we use an \emph{absolute} joint position with a default offset:
\[
    q_{\text{des}, t} = q_{\text{default}} + k_a \, a_t,
\]
where \(k_a\) is the action scale. 
For all other tasks, we use a \emph{relative} joint position:
\[
    q_{\text{des}, t} = q_{\text{des}, t-1} + k_a \, a_t.
\]
The desired joint position is mapped to torque via a PD controller:
\begin{equation}
    \tau = k_p \bigl(q_{\text{des}} - q\bigr) \;-\; k_d \,\dot{q},
    \label{eq:pd_mapping}
\end{equation}
where \(k_p\) and \(k_d\) are the proportional and derivative gains, respectively.

\subsubsection{Domain Randomization}
To reduce the sim-to-real gap, we randomize several parameters during training:
\begin{itemize}
    \item \textbf{Sensor noise:} All sensor readings are corrupted with noise.
    \item \textbf{Dynamic properties:} Physical parameters that are difficult to measure precisely 
    (e.g., link center-of-mass, reflected inertia, joint calibration offsets).
    \item \textbf{Task uncertainties:} Ground friction and payload mass.
\end{itemize}

\subsubsection{Reward and Termination}

\begin{table}[h!]
\centering
\caption{Reward Functions}
\label{table:reward-functions}
\begin{tabular}{|l|l|}
\hline
\textbf{Reward}                  & \textbf{Expression}                                                                                         \\ \hline
Linear Velocity Tracking         & $r_v = k_v \exp\left(-\|cmd_{v,xy} - v_{xy}\|^2 / \sigma_v\right)$                                          \\ \hline
Angular Velocity Tracking        & $r_\omega = k_\omega \exp\left(-\|cmd_{\omega,z} - \omega_z\|^2 / \sigma_\omega\right)$                     \\ \hline
Feet Airtime                     & $r_\text{air} = \text{clip}\left((T_\text{air} - T_\text{min}) \cdot C_\text{contact}, 0, T_\text{max} - T_\text{min}\right)$ \\ \hline
Feet Clearance                   & $r_\text{clear} = k_\text{clear} \cdot \|p_{f,z} - p^\text{des}_{f,z}\|^2 \cdot \|v_{f,xy}\|^{0.5}$          \\ \hline
Feet Phase                       & $r_\text{phase} = k_\text{phase} \cdot \exp\left(-\|p_{f,z} - r_z(\phi)\|^2 / \sigma_\text{phase}\right)$    \\ \hline
Feet Slip                        & $r_\text{slip} = k_\text{slip} \cdot \|C_{f,i} \cdot v_{f,xy}\|^2$                                          \\ \hline
Orientation                      & $r_\text{ori} = k_\text{ori} \cdot \| \phi_\text{body,xy}\|^2$                                              \\ \hline
Joint Torque                     & $r_\tau = k_\tau \cdot \|\tau\|^2$                                                                         \\ \hline
Joint Position                   & $r_q = k_q \cdot \|q - q_\text{nominal}\|^2$                                                               \\ \hline
Action Rate                      & $r_\text{rate} = k_\text{rate} \cdot \|a_t - a_{t-1}\|^2$                                                  \\ \hline
Energy Consumption               & $r_\text{energy} = k_\text{energy} \cdot \|\dot{q} \cdot \tau\|$                                           \\ \hline
Pose Deviation                   & $r_\text{pose} = k_\text{pose} \cdot \exp\left(-\|q - q_\text{default}\|^2\right)$                         \\ \hline
Termination (Penalty)            & $r_\text{termination} = k_\text{termination} \cdot \text{done}$                                            \\ \hline
Stand Still (Penalty)            & $r_\text{standstill} = k_\text{standstill} \cdot \|cmd_{v,xy}\|$                                           \\ \hline
Linear Velocity in Z (Penalty)   & $r_\text{lin\_z} = k_\text{lin\_z} \cdot \|v_{z}\|^2$                                                      \\ \hline
Angular Velocity in XY (Penalty) & $r_\text{ang\_xy} = k_\text{ang\_xy} \cdot \|\omega_{x,y}\|^2$                                             \\ \hline
\end{tabular}
\end{table}

In Table~\ref{table:reward-functions}, $cmd_{v,xy}$ and $cmd_{\omega,z}$ represent the commanded linear velocity in the $xy$-plane and angular velocity around the $z$-axis, respectively. $v_{xy}$ and $\omega_z$ are the actual linear and angular velocities. $T_s$ and $T_a$ represent the time of the last touchdown and takeoff of the feet. $p_{f,z}$ and $p^\text{des}_{f,z}$ denote the actual and desired foot heights, while $v_{f,xy}$ is the horizontal foot velocity. $\tau$ is the torque, $q$ is the joint position, and $\dot{q}$ is the joint velocity.

The total reward $r_\text{total}$ is calculated as the weighted sum of all the reward terms:
\[
r_\text{total} = \sum_i w_i r_i,
\]
Finally, the total reward is clipped to ensure it remains non-negative.

\textbf{Termination:} For joystick-controlled policies, we use a reduced collision model (only the feet) and terminate the episode if the robot inverts (e.g., ends up upside down). For other tasks, we employ the full collision model approximated using geometric primitives.

\subsubsection{Network Architecture}

We employ an asymmetric actor--critic~\cite{pinto2018asymmetric} setup, in which the policy network (actor) and the value network (critic) receive different observation inputs. The policy network is fed with the aforementioned observations, while the value network additionally receives uncorrupted versions of these signals and extra sensor readings such as contact forces, perturbation forces, and joint torques.

Both the policy and value networks use a three-layer multilayer perceptron (MLP) with hidden sizes of 512, 256, and 128. Each hidden layer uses the Swish \cite{ramachandran2017searchingactivationfunctions} activation function. A full set of hyper-parameters is available in \Cref{sec:rl_hypers}.

\subsubsection{Finetuning}
\paragraph{Joystick policy}
\begin{enumerate}
    \item Train for 100\,M timesteps with a command range of \(\{1.5, 0.8, 1.2\}\).
    \item Finetune for 50\,M timesteps with a command range of \(\{1.5, 0.8, 2\pi\}\).
    \item Finetune on rough terrain for 100\,M timesteps.
\end{enumerate}

\paragraph{Getup policy}
\begin{enumerate}
    \item Train with a power termination cutoff of 400\,W.
    \item Finetune with a joint velocity cost.
\end{enumerate}

\paragraph{Handstand and footstand policies}
\begin{enumerate}
    \item Finetune with a joint acceleration and energy cost.
    \item Progressively reduce the power termination budget from 400\,W to 200\,W.
\end{enumerate}

Finally, all policies are trained on flat terrain for 200\,M timesteps, then finetuned on rough terrain for 100\,M timesteps. The rough terrain is modeled as a heightfield generated from Perlin noise.


\subsection{RL Training Results}
\label{sec:appendix_locomotion_curves}

For all locomotion environments implemented in MuJoCo Playground, we train with PPO using the RL implementation from \cite{freeman2021brax} and we report reward curves below. In \Cref{fig:locomotion_step_reward} we report environment steps versus reward and in \Cref{fig:locomotion_time_reward} we report wallclock time versus reward. All environments are run across 5 seeds on a single A100 GPU.

\begin{figure}[ht]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/locomotion_reward.pdf}
    \caption{\small Reward vs environment steps for Brax PPO. All settings are run with 5 seeds on a single A100 GPU device.}
    \label{fig:locomotion_step_reward}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/locomotion_reward_wallclock.pdf}
    \caption{\small Reward vs wallclock time for Brax PPO. All settings are run with 5 seeds on a single A100 GPU device. Notice that the initial flat region measures the compilation time for the training + environment code.}
    \label{fig:locomotion_time_reward}
\end{figure}

\subsection{RL Training Throughput}
\label{sec:appendix_locomotion_throughput}

In \Cref{tab:locomotion_training_throughput} we show training throughput for all locomotion envs. In \Cref{fig:go1_device_topo} we show training throughput of the Go1JoystickFlatTerrain environment. Different devices and topologies do not make material difference in training wallclock time, since the environment is quite simple with limited contacts between the feet and the floor.

\begin{table*}[ht]
\centering
\begin{tabular}{|l|c|c|}
\hline 
 Env                   & PPO Steps per Second     \\
\hline 
 BarkourJoystick                      & 385920 $\pm$ 2162 \\
 BerkeleyHumanoidJoystickFlatTerrain  & 120145 $\pm$ 484  \\
 BerkeleyHumanoidJoystickRoughTerrain & 30393 $\pm$ 44    \\
 G1Joystick                           & 106093 $\pm$ 131  \\
 Go1Footstand                         & 204578 $\pm$ 906  \\
 Go1Getup                             & 96173 $\pm$ 230   \\
 Go1Handstand                         & 204416 $\pm$ 738  \\
 Go1JoystickFlatTerrain               & 417451 $\pm$ 2955 \\
 Go1JoystickRoughTerrain              & 291060 $\pm$ 727  \\
 H1InplaceGaitTracking                & 289372 $\pm$ 1498 \\
 H1JoystickGaitTracking               & 291018 $\pm$ 1111 \\
 Op3Joystick                          & 198910 $\pm$ 406  \\
 SpotFlatTerrainJoystick              & 404931 $\pm$ 2710 \\
 SpotGetup                            & 266792 $\pm$ 1038 \\
 SpotJoystickGaitTracking             & 407572 $\pm$ 4091 \\
\hline 
\end{tabular}
\caption{Training throughput is displayed for all the Locomotion environments on an A100 GPU device across 5 seeds using brax PPO and the RL hyperparameters in \Cref{sec:rl_hypers}. We report the 95th percentile confidence interval.}
\label{tab:locomotion_training_throughput}
\end{table*}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/device_topo_go1_full_reward.pdf}
    \caption{\small Training wallclock time for Go1JoystickFlatTerrain on different GPU devices and topologies.}
    \label{fig:go1_device_topo}
\end{figure}

\clearpage

\subsection{Real-world Setup}

All locomotion deployments are based on \texttt{ros2-control} and are written in \texttt{C++} with real-time guarantees. The Unitree SDK1, Unitree SDK2, and the Berkeley Humanoid EtherCAT master are each wrapped as abstract sensor and actuator hardware interfaces. These same interfaces are also used in Gazebo \cite{koenig2004design} to facilitate sim-to-sim verification.

Different RL policies can be loaded and executed within the same process—whether operating on physical hardware or in simulation—by receiving sensor readings and issuing control commands via the hardware interface. Each policy model is inferenced at 50 Hz using ONNX Runtime \cite{onnxruntime}, alongside a model-based estimator. In addition, a separate model-based estimator~\cite{Flayols_2017_estimator} runs at the hardware interface’s maximal communication frequency (500–2000 Hz), providing linear velocity observations and other diagnostic information.

\clearpage
