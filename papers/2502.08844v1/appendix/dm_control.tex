\section{DM Control Suite}
\label{sec:appendix_dm_control}

\subsection{Environments}
\label{sec:appendix_dm_control_envs}

In \Cref{tab:dm_control_envs}, we show the environments from DM Control Suite (\cite{tassa2018deepmind}) that were re-implemented in MuJoCo Playground. Certain XMLs were modified for performance and are shown in the table. 

\begin{table*}[!ht]
\centering
\begin{tabular}{|l|p{1.2cm}|p{10cm}|} %
\hline
\textbf{Env} & \textbf{MJX} & \textbf{XML Modifications} \\ \hline
acrobot-swingup           & $\checkmark$ & iterations=2, ls\_iterations=4 \\ \hline
acrobot-swingup\_sparse    & $\checkmark$ &  \\ \hline
ball\_in\_cup-catch         & $\checkmark$ & iterations=1, ls\_iterations=4 \\ \hline
cartpole-balance          & $\checkmark$ & iterations=1, ls\_iterations=4 \\ \hline
cartpole-balance\_sparse   & $\checkmark$ &  \\ \hline
cartpole-swingup          & $\checkmark$ &  \\ \hline
cartpole-swingup\_sparse   & $\checkmark$ &  \\ \hline
cheetah-run               & $\checkmark$ & iterations=4, ls\_iterations=8, max\_contact\_points=6, max\_geom\_pairs=4 \\ \hline
finger-spin               & $\checkmark$ & iterations=2, ls\_iterations=8, max\_contact\_points=4, max\_geom\_pairs=2, removed cylinder collision \\ \hline
finger\_turn\_easy          & $\checkmark$ &  \\ \hline
finger\_turn\_hard          & $\checkmark$ &  \\ \hline
fish-upright              & $\checkmark$ & iterations=2, ls\_iterations=6, disabled contacts \\ \hline
fish-swim                 & $\checkmark$ &  \\ \hline
hopper-stand              & $\checkmark$ & iterations=4, ls\_iterations=8, max\_contact\_points=6, max\_geom\_pairs=2 \\ \hline
hopper-hop                & $\checkmark$ &  \\ \hline
humanoid-stand            & $\checkmark$ & timestep=0.005, max\_contact\_points=8, max\_geom\_pairs=8 \\ \hline
humanoid-walk             & $\checkmark$ &  \\ \hline
humanoid-run              & $\checkmark$ &  \\ \hline
pendulum-swingup          & $\checkmark$ & timestep=0.01, iterations=4, ls\_iterations=8 \\ \hline
point\_mass-easy           & $\checkmark$ & iterations=1, ls\_iterations=4 \\ \hline
reacher-easy              & $\checkmark$ & timestep=0.005, iterations=1, ls\_iterations=6 \\ \hline
reacher-hard              & $\checkmark$ &  \\ \hline
swimmer-swimmer6          & $\checkmark$ & timestep=0.003, iterations=4, ls\_iterations=8, contype/conaffinity set to 0 \\ \hline
swimmer-swimmer15         & $\times$ &  \\ \hline
walker-stand              & $\checkmark$ & timestep=0.005, iterations=2, ls\_iterations=5, max\_contact\_points=4, max\_geom\_pairs=4 \\ \hline
walker-walk               & $\checkmark$ &  \\ \hline
walker-run                & $\checkmark$ &  \\ \hline
manipulator-bring\_ball    & $\times$ & \\ \hline
manipulator-bring\_peg     & $\times$ &  \\ \hline
manipulator-insert\_ball   & $\times$ &  \\ \hline
manipulator-insert\_peg    & $\times$ &  \\ \hline
dog-stand                 & $\times$ &  \\ \hline
dog-walk                  & $\times$ &  \\ \hline
dog-trot                  & $\times$ &  \\ \hline
dog-run                   & $\times$ &  \\ \hline
dog-fetch                 & $\times$ &  \\ \hline
\hline
\end{tabular}
\caption{DM Control Suite Environments ported to MJX. Where specified, XML modifications were made to the solver iterations, line search iterations, as well as contact custom parameters for MJX.}
\label{tab:dm_control_envs}
\end{table*}

\subsection{RL Training Results}
\label{sec:appendix_dm_control_curves}

For all DM Control Suite environments ported to MuJoCo Playground, we train both PPO~\cite{schulman2017proximal} and SAC~\cite{haarnoja2018soft} using the RL implementations in \cite{freeman2021brax} and we report reward curves below. In \Cref{fig:dm_control_step_reward} we report environment steps versus reward and in \Cref{fig:dm_control_time_reward} we report wallclock time versus reward. All environments are run across 5 seeds on a single A100 GPU.

\begin{figure}[ht]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/dm_control_reward.pdf}
    \caption{\small Reward vs environment steps for PPO and SAC on the full DM Control Suite environments in MuJoCo Playground. We run PPO for 60M steps, with a few selected environments running on 100M steps. SAC runs for 5M steps. All settings are run with 5 seeds on a single A100 GPU device.}
    \label{fig:dm_control_step_reward}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/dm_control_reward_wallclock.pdf}
    \caption{\small Reward vs wallclock time for PPO and SAC on the full DM Control Suite environments in MuJoCo Playground. All settings are run with 5 seeds on a single A100 GPU device.}
    \label{fig:dm_control_time_reward}
\end{figure}

\subsection{RL Training Throughput}
\label{sec:appendix_dm_control_training_throughput}

We report training throughput on all DM Control Suite environments in \Cref{tab:dm_control_env_training_throughput} by dividing the number of environment steps by wallclock time, as reported in \Cref{sec:appendix_dm_control_curves}, for each RL algorithm.

\begin{table*}[ht]
\centering
\begin{tabular}{|l|c|c|}
\hline 
 Env                   & PPO Steps per Second     & SAC Steps Per Second     \\
\hline 
 AcrobotSwingup        & 752092 $\pm$ 11562 & 30661 $\pm$ 244  \\
 AcrobotSwingupSparse  & 750597 $\pm$ 4640  & 30624 $\pm$ 210  \\
 BallInCup             & 235899 $\pm$ 565   & 15492 $\pm$ 283  \\
 CartpoleBalance       & 718626 $\pm$ 6894  & 30891 $\pm$ 168  \\
 CartpoleBalanceSparse & 721061 $\pm$ 14135 & 31031 $\pm$ 183  \\
 CartpoleSwingup       & 728088 $\pm$ 12503 & 30870 $\pm$ 207  \\
 CartpoleSwingupSparse & 718355 $\pm$ 10189 & 31061 $\pm$ 226  \\
 CheetahRun            & 435162 $\pm$ 12183 & 18819 $\pm$ 202  \\
 FingerSpin            & 246791 $\pm$ 1763  & 16475 $\pm$ 153  \\
 FingerTurnEasy        & 245255 $\pm$ 4561  & 16086 $\pm$ 112  \\
 FingerTurnHard        & 245421 $\pm$ 4278  & 16084 $\pm$ 69   \\
 FishSwim              & 183750 $\pm$ 1773  & 11591 $\pm$ 55   \\
 HopperHop             & 201313 $\pm$ 2833  & 12098 $\pm$ 166  \\
 HopperStand           & 201517 $\pm$ 3227  & 12008 $\pm$ 255  \\
 HumanoidRun           & 91617 $\pm$ 1019   & 5886 $\pm$ 62    \\
 HumanoidStand         & 91927 $\pm$ 1004   & 5893 $\pm$ 17    \\
 HumanoidWalk          & 91563 $\pm$ 1150   & 5842 $\pm$ 51    \\
 PendulumSwingup       & 724126 $\pm$ 21524 & 32836 $\pm$ 178  \\
 PointMass             & 730775 $\pm$ 3608  & 31710 $\pm$ 148  \\
 ReacherEasy           & 520021 $\pm$ 9637  & 24888 $\pm$ 149  \\
 ReacherHard           & 523441 $\pm$ 8012  & 24874 $\pm$ 156  \\
 SwimmerSwimmer6       & 167259 $\pm$ 2377  & 10012 $\pm$ 79   \\
 WalkerRun             & 141581 $\pm$ 831   & 6069 $\pm$ 48    \\
 WalkerStand           & 140360 $\pm$ 1762  & 6085 $\pm$ 29    \\
 WalkerWalk            & 139818 $\pm$ 1267  & 6098 $\pm$ 30    \\
\hline 
\end{tabular}
\caption{Training throughput is displayed for all the DM Control Suite environments on an A100 GPU device across 5 seeds using brax PPO and the RL hyperparameters in Appendix \Cref{sec:rl_hypers}. We report the 95th percentile confidence interval.}
\label{tab:dm_control_env_training_throughput}
\end{table*}

\clearpage
