\section{Madrona Rendering Environments}
\label{appendix:MadronaMJXBenchmark}

\subsection{RL Training Results}
\label{sec:madrona_training_curves}

MuJoCo Playground showcases two pixel observation environments using batch rendering; CartPoleBalance and PandaPickCubeCartesian. These two environments include complete training examples using brax PPO. We show the PPO training curves for both environments in Figures \ref{fig:madrona_step_reward} and \ref{fig:madrona_time_reward} across 5 seeds.

CartPoleBalance is adjusted for pixel-based observations by decreasing the control frequency such that more simulated experience can be factored into training with less policy updates, and by re-adjusting the RL training hyperparameters as necessary. The observations are of dimension 64x64x3 and consist of the current and previous two rendered observations, collapsed into grayscale then transposed for CNN inference. PandaPickCubeCartesian is derived from PandaPickCube. Our changes for faster and more stable pixel-based training are described in \Cref{appendix:RealWorldPixels}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/madrona_reward.pdf}
    \caption{\small Reward vs environment steps for brax PPO. All settings are run with 5 seeds on a single RTX 4090 GPU.}
    \label{fig:madrona_step_reward}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/madrona_reward_wallclock.pdf}
    \caption{\small Reward vs wallclock time for brax PPO. All settings are run with 5 seeds on a single RTX 4090 GPU.}
    \label{fig:madrona_time_reward}
\end{figure}

\subsection{Performance Benchmarking}
\label{sec:madrona_benchmarking}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.85\linewidth]{figures/cartpole_benchmark_combined.png}
    \caption{\small Comparison of raw environment-stepping throughput with prior simulators for CartpoleBalance with state-based and pixel observations of varying sizes.}
    \label{fig:cartpole_benchmark_combined}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.85\linewidth]{figures/cartpole_benchmark_stacked_full.png}
    \caption{\small Time-cost breakdown of unrolling physics simulation and rendering for CartpoleBalance with pixel observations. \emph{Lower is better.} Per-step  rendering time is stacked without overlap over physics simulation time.}
    \label{fig:cartpole_benchmark_stacked}
\end{figure}

In this section we benchmark the througput of Madrona MJX GPU batch rendering. For reference, we plot our results alongside those from IsaacLab \cite{mittal2023orbit} and Maniskill3 \cite{tao2024maniskill3}; data for IsaacLab and Maniskill3 is obtained from \cite{tao2024maniskill3}. This is only a rough comparison as we only take steps to ensure similar hardware and timestep size, as a fully controlled performance benchmark is difficult due to the inherent differences between simulators. Our goal in these comparisons is to only highlight that our batch rendering is competitive with other state-of-the-art simulators that also include high-throughput rendering. %

The y-axis of Figure \ref{fig:cartpole_benchmark_combined} measures the rate of generating environment transitions $(s_t, o_t, r_t, s_{t+1})$ with random actions, comprising the basic data unit of most on and off-policy training algorithms. The first subplot measures Cartpole simulation with computationally trivial state-based observations. The next three plots show the cost of generating transitions where $o_t$ involves rendering with increasing resolution.

Figure \ref{fig:cartpole_benchmark_stacked} evaluates how much of our throughput increase is due to MJX's faster physics step. For each bar, the dark area shows the cost of the physics step and the non-overlapping light area shows the cost of generating the pixel observation. Note that lower values are better, as we display the inverse of frequency. While MJX's faster physics simulation indeed benefits throughput at lower image resolutions, Madrona's rendering speed improvements appear to be the primary driver of the measured speed-ups.

\clearpage

\subsection{Bottlenecks in Pixels-based Training}
\label{sec:appendix_madrona_bottlenecks}

\begin{table}[htbp]
    \centering
    \begin{tabular}{l c c c c}
    \toprule
             & \textbf{Env Step} & \textbf{with Pixels} & \textbf{and Inference} & \textbf{and Training} \\
    \midrule
    \multicolumn{5}{l}{\textbf{CartpoleBalance}} \\ 
    FPS         & \(1.37 \times 10^6\) & \(4.03 \times 10^5\) & \(3.41 \times 10^5\) & \(3.13 \times 10^4\) \\
    Time/Env Step (s) & \(7.30 \times 10^{-7}\) & \(2.48 \times 10^{-6}\) & \(2.93 \times 10^{-6}\) & \(3.20 \times 10^{-5}\) \\
    \midrule
    \multicolumn{5}{l}{\textbf{PandaPickCubeCartesian}} \\ 
    FPS         & \(6.40 \times 10^4\) & \(3.69 \times 10^4\) & \(3.60 \times 10^4\) & \(1.56 \times 10^4\) \\
    Time/Env Step (s) & \(1.56 \times 10^{-5}\) & \(2.71 \times 10^{-5}\) & \(2.78 \times 10^{-5}\) & \(6.39 \times 10^{-5}\) \\
    \bottomrule
    \end{tabular}
    \caption{Raw throughput of our two pixel-based environments in various settings. \emph{Env step}: stepping the physics with random actions. \emph{with Pixels}: Same, with the overhead of rendering pixel-based observations. \emph{with Inference}: random actions are replaced with policy inference. \emph{and Training}: the speed of PPO training. Results averaged over 5 runs on an RTX4090.}
    \label{tab:madrona_training_breakdown_measured}
\end{table}

\begin{table}[htbp]
    \centering
    \begin{tabular}{l c c c c}
    \toprule
             & \textbf{Physics} & \textbf{Rendering} & \textbf{Inference} & \textbf{Policy Update} \\
    \midrule
    \multicolumn{5}{l}{\textbf{CartpoleBalance}} \\ 
    Time/Env Step  (s)   & \(7.30 \times 10^{-7}\) & \(1.75 \times 10^{-6}\) & \(4.49 \times 10^{-7}\) & \(2.91 \times 10^{-5}\) \\
    Fraction & 0.02 & 0.06 & 0.01 & 0.91 \\
    \midrule
    \multicolumn{5}{l}{\textbf{PandaPickCubeCartesian}} \\ 
    Time/Env Step (s)        & \(1.56 \times 10^{-5}\) & \(1.15 \times 10^{-5}\) & \(6.45 \times 10^{-7}\) & \(3.62 \times 10^{-5}\) \\
    Fraction & 0.24 & 0.18 & 0.01 & 0.57 \\
    \bottomrule
    \end{tabular}
    \caption{Breakdown of total training time by component for CartpoleBalance and PandaPickCubeCartesian tasks, derived from Table \ref{tab:madrona_training_breakdown_measured}. Results averaged over 5 runs on an RTX4090.}
    \label{tab:madrona_training_breakdown_calc}
\end{table}




\Cref{tab:madrona_training_breakdown_calc} isolates the contributions of policy rollout (physics simulation, rendering, inference) and policy update to the overall cost per step in a training loop. We amortize the cost of policy update per policy rollout step, setting $t_4 = t_{training} + t_{inference} + t_{rendering} + t_{env step}$, where $t_4$ corresponds to \emph{Time/Env Step} in the last column of Table \ref{tab:madrona_training_breakdown_measured}. Working in reverse order through the table, we isolate each component. For example, $t_{training} = t_4 - t_3$ corresponds to the Policy Update \emph{Time/Env Step}.

We see that in both of our provided pixel-based environments, the training speed bottleneck is shifted from rendering to policy updates. This is especially true for the Cartpole, as the policy and value architectures includes convolutions determined by the size of the input image regardless of robot and task complexity. The expensive architecture coupled with the trivial embodiment, shift over 90\% of the training burden to network updates. For the Franka Panda environment, we buffer more of the computation into the physics by training with a lower control frequency. At 20 Hz control with a 5ms physics timestep, the policy makes only one decision per 10 simulator sub-steps. Similar to Cartpole, rendering is less of a bottleneck than the cost of processing the resultant images via convolutional-based network architectures.

\clearpage
