\section{The \ourMethod problem}\label{sec:methodology}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/main_fig.pdf}
    \caption{
    Overview of \ourMethod framework. \colorbox{sbBlue025}{Blue} shows our environment wrapper with two policies, novice and expert, embedded inside as acting agents. 
    \colorbox{sbOrange025}{Orange} encapsulates the logic for coordination policy.
    The wrapped environment returns the cost as well as the reward to the coordination agent. $\phi(\pi_n, s_t)$ returns the internal representation of the novice policy $\pi_n$ given the state $s_t$.
    }
    \label{fig:wrapper_demo}
\end{figure}

The \ourMethod problem concerns a novice agent and an expert agent who take turns controlling the body of the novice.\footnote{The body can be a virtual body (e.g., a video-game character) or a physical body (e.g., a robot).} 
% At any time, only one agent can control the body.
The two agents each implement a policy for controlling the body, which, at each time step, recommends an action for the body to take. 
The goal of the problem is to learn a coordination policy to decide whose action recommendation will actually be executed by the body in each time step.
The quality of the policy is measured by a reward function that takes into account the environment reward and the cost of expert assistance. 
We illustrate the key concepts of \ourMethod in \autoref{fig:wrapper_demo}.

% In this section, we formalize the problem of learning to \ourMethod and present our proposed approach. The goal is to develop a coordination policy that enables a novice policy to decide, at each time step, whether to act autonomously (retaining control) or yield control to a black-box expert. Crucially, the coordination policy is implemented by the novice: it leverages the novice's internal decision-making representations but interacts with the expert solely through its final actions, without access to the expert's internal reasoning. This setup reflects practical constraints where the novice must adapt to distribution shifts at test time, including environmental changes and the novel presence of the expert, despite never interacting with the expert during training. 
 


\subsection{Problem Formulation}


We formalize the problem of performing a task in a given environment as a Markov Decision Process (MDP) with state space $\mathcal{S}$, action space $\mathcal{A}$, and reward function $R: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$. The environment dynamics are specified by an initial state distribution $P_0$ and a transition function $P: \mathcal{S} \times \mathcal{A} \rightarrow \Delta(\mathcal{S})$, where $\Delta(\mathcal{S})$ denotes the probability simplex over  $\mathcal{S}$ \citep{sutton2018reinforcement}. 
With $\mathcal{S}$, $\mathcal{A}$, and $R$ fixed, a task distribution $\mathcal{E}$ is a distribution over MDPs with varying $P_0$ and $P$ \citep{hallak2015contextual, langford2017contextual}. 
Training occurs under environment task $\mathcal{E}_{\text{train}}$, while testing under distribution $\mathcal{E}_{\text{test}} \neq \mathcal{E}_{\text{train}}$.


Let $\pi_n: \mathcal{S} \rightarrow \Delta(\mathcal{A})$ denote a \textit{novice policy}, trained to perform well on tasks sampled from $\mathcal{E}_{\text{train}}$, and $\pi_e: \mathcal{S} \rightarrow \Delta(\mathcal{A})$ denote the \textit{expert policy}, trained to perform well on tasks sampled from $\mathcal{E}_{\text{test}}$. The novice's goal is to learn a \textit{coordination policy} $\mu: \mathcal{S} \times \Phi_n \rightarrow \Delta(\{n, e\})$, where $\Phi_n$ represents the space over internal representations extracted from $\pi_n$ during its decision-making process. Specifically, in state $s_t$ encountered at time step $t$, the novice computes $\pi_n(s_t)$.
During this process, an internal representation $\phi(\pi_n, s_t) \in \Phi_n$ is extracted.
% $\pi_n$ samples its proposed action $a^n_t \sim \pi_n(s_t)$ and computes its  $\phi(\pi_n, s_t)$. 
The coordination policy $\mu$ then makes a binary decision $x_t \in \{ n, e \}$ based on $s_t$ and $\phi(\pi_n, s_t)$:
\begin{align}
    x_t \sim \mu(s_t, \phi(\pi_n, s_t))
\end{align}
The action that gets executed in the environment is:
\begin{align}
    a_t = a^{x_t}_t \sim \pi_{x_t}(s_t)
\end{align} Specifically, if $x_t = n$, the action is sampled from the novice's policy $a_t = a^n_t \sim \pi_n(s_t)$. Otherwise, it is sampled from the expert's policy.
% \begin{align}
%     a_t = \begin{cases} 
%         a^{\texttt n}_t \sim \pi_{\texttt n}(s_t) & \text{if } x_t = \texttt{n} \\
%         a^{\texttt e}_t \sim \pi_{\texttt e} (s_t) & \text{otherwise}
%     \end{cases}
% \end{align}
% Here, $a^{x_t}_t$ denotes an action sampled from policy $\pi_{x_t}$.
Crucially, $\mu$ observes $\pi_n$'s internal representations but does not receive any information from $\pi_e$ (e.g., its parameters, gradients, internal states, etc.)

At test time, $\mu$ is evaluated with tasks sampled from $\mathcal{E}_{\text{test}}$ and $\pi_e$ is present to assist $\pi_n$ in those tasks. 
For the learning of $\mu$, however, $\pi_e$ is unavailable.
This mimics scenarios where querying $\pi_e$ is extremely costly or simply unnecessary (as the novice has mastered the training tasks).
The challenge of \ourMethod is to construct a \textit{learning method} $\mathcal{T}$ that can find an ``effective'' coordination policy using access to only $\pi_n$ and $\mathcal{E}_{\textrm{train}}$:
% that operates solely on $\pi_n$ and $\mathcal{E}_{\text{train}}$:
\begin{align}
    \mu = \mathcal{T}(\pi_n, \mathcal{E}_{\text{train}}; \mathcal{P}, \mathcal{V})
\end{align}

In this work, we consider a general class of methods that implements two components: a policy proposer $\mathcal{P}$ and a policy validator $\mathcal{V}$.
During training, $\mathcal{P}$ considers a policy class and generates a finite set of candidate policies $\textsc{Cand} = \{ \mu_1, \mu_2, \ldots \}$ from this class. 
These policies are evaluated by $\mathcal{V}$, which predicts the test performance of a policy.
The best policy $\argmax_{\mu \in \textsc{Cand}} \mathcal{V}(\mu)$ according to $\mathcal{V}$ is chosen for testing. 
For example, a deep RL method considers policies parameterized by neural networks. 
It employs a gradient-based optimizer as the policy proposer, which continuously updates the current set of parameters to generate candidates for validation. Moreover, OOD detection methods can leverage novelty detection to determine when to query the expert.
Another example is a simple approach that queries the expert with probability $p$ at each time step.
Its policy proposer conducts a grid search through possible values of $p \in [0, 1]$.
The specific training strategies, their advantages, and implementation details are discussed in \aref{app:coord_policy}.


% Here, $\mathcal{V}$ is a \textit{validator} that any algorithm for \ourMethod must specify for policy selection.
% This function takes as input a candidate policy and predicts its test performance. 
% For example, during standard neural-network training, multiple sets of model parameters are considered but only the one with the highest validation score is selected for testing. 
In this work, solving a \ourMethod problem means specifying a policy proposing approach \textit{and} a policy validation approach.
Since \ourMethod is an OOD generalization problem, devising a reliable validation approach is non-trivial.
Such an approach must be able to accurately predict the test performance of a coordination policy \textit{without} access to the novice policy $\pi_e$ and the test task distribution $\mathcal{E}_{\textrm{test}}$.
%\looseness=-1



\subsection{Performance Metric}

The effectiveness of a coordination policy is measured by a reward function that substracts the cost of querying the expert from the environment reward 
\begin{align}
    r_t(\alpha) = R(s_t, a_t) - \alpha \cdot \bar R  \ 
\end{align} where $ R(s_t, a_t)$ is the environment reward obtained for the taken action $a_t$, $\alpha \in [0, 1]$ is a user-specified hyperparameter, and $\bar R$ is the approximate average reward per action.
To compute $\bar R$, we run $\pi_e$ on $\mathcal{E}_{\text{test}}$ for $N$ episodes, calculate the mean episode return $\bar{G}$ and mean episode length $\bar{L}$, and divide the former by the latter: $\bar R = \bar G / \bar L$.
% \begin{align}
    % \bar R = \frac{\bar G}{\bar L}
% \end{align}

When $\alpha = 1$ and the expert is queried in all time steps, the expected return $\mathbb{E}[\sum r_t]$ will be approximately $0$. 
In other words, when $\alpha = 1$, if the novice always delegates the entire task to the expert, then on average  it receives approximately zero reward.

In practice, users may specify a wide range of values of $\alpha$.
Hence, it is crucial to evaluate a policy with multiple values of $\alpha$, simulating diverse scenarios.
To summarize performances with multiple values of $\alpha$ with a single number, we propose an area-under-the-curve (AUC) metric. 
As the name suggests, this metric estimates the area under the curve formed by the points $\{(\alpha_i, \bar G(\alpha_i))\}_{i = 1}^K$ where $\bar G(\alpha_i)$ denotes the mean return of the evaluated policy for a given $\alpha_i$.
We approximate the metric and provide error bars using a bootstrap procedure, described in \autoref{alg:metric}. 
% \khanh{TODO: will have to move this to appendix} \mohamad{the subsection of Performance Metric?}

\begin{algorithm}
\caption{Bootstrap procedure to compute AUC metric. \texttt{AreaUnderCurve} computes the area under the curve formed by the input points.}
\small
\label{alg:metric}
\begin{algorithmic}[1]  % [1] for line numbers
    \State \textbf{Input:} Data points $\{ (\alpha_i, \{G_{i,j}\}_{j = 1}^M) \}_{i = 1}^K$ where $\alpha_i = \frac{i}{K} $ and $G_{i, j}$ is the return of the evaluated policy in the $j$-th episode, during which $\alpha$ is set to $\alpha_i$. $m < M$ is number of samples used to compute the mean episode returns in each simulation. We use $N = 1000, K = 6, M = 1600, m = 256$ in our experiments.
    \State \textbf{Output:} Mean estimation and its standard deviation 
    \State Initialize $E = \emptyset$
    \For{$N$ simulations}
    \State Initialize $D = \emptyset$
    \For{$i = 1 \ldots K$}
    \State Draw an $m$-element sample $S_i$ from $\{ G_{i, j} \}_{j = 1}^M$
    \State Compute $\bar G_{i} = \texttt{mean}(S_i)$
    \State $D \leftarrow D \cup \{(\alpha_i, \bar G_i)\}$
    \EndFor
    \State $E \leftarrow E \cup \{ \texttt{AreaUnderCurve}(D) \}$
    \EndFor
    \Return $\texttt{numpy.mean}(E)$, $\texttt{numpy.std}(E)$
\end{algorithmic}
\end{algorithm}

% \begin{align}
% \bar{R} = \frac{1}{N} \sum_{i=1}^N R_i \quad \bar{L} = \frac{1}{N} \sum_{i=1}^N L_i
% \end{align}
% where $R_i$ is the reward and $L_i$ the length of the $i$-th episode. The reward per action is then calculated by dividing $\bar{R}$ by $\bar{L}$.
% Finally, the query cost per action is determined by multiplying the reward per action by a given cost factor $C \in [0, 1]$:
% \begin{align}
% \text{Query cost per action} = \left( \frac{\bar{R}}{\bar{L}} \right) \times C.
% \end{align}
% Whenever the coordination policy $\mu$ requests assistance from $\pi_e$, it receives the environment reward, adjusted by the query cost per action.
% This provides a measure of the efficiency of expert interventions, balancing task success with the associated cost of querying the expert policy.

\subsection{Oracle Performance}
\label{sec:rl-oracle}

To track progress toward solving a \ourMethod problem, it is essential to derive an oracle coordination policy.
While many machine learning benchmarks employ human decision-makers as oracles, this approach would likely yield pessimistic performance estimations in \ourMethod problems because, due to mismatched mental representations, it is difficult for a human to determine exactly when an AI agent needs or does not need help.
Our solution is to run an RL algorithm (PPO \citep{schulman2017proximal}) to find a near-optimal coordination policy, directly optimizing for test performance.
This is not a valid solution to the problem, as it has access to the expert $\pi_e$ and test environment $\mathcal{E}_{\text{test}}$.
The approach is cheap to run and universally applicable to any environment. 
We refer to this approach as \textsc{RLOracle}.

% \begin{align}
%     \mu_{\text{true}} = \mathcal{T}(\pi_n, \pi_e, \mathcal{E}_{\text{test}}).
% \end{align}
% Later, it is evaluated by: $p_{\text{true}} = \textsc{Eval}(\mu_{\text{true}}, \pi_n, \pi_e, \mathcal{E}_{\text{test}})$. While impractical in real-world settings, this oracle provides a reference for evaluating learned policies.

\section{Policy Validation by Simulating Test Conditions} \label{sec:sim-val}

As mentioned, a major challenge in solving \ourMethod is policy validation, i.e., predict the test performance of policies proposed by the learning method, in order to select a final policy for testing. 
In this section, we propose a simple yet effective solution to this problem. 


We first define an oracle validator, which evaluates a policy exactly under the test conditions:
\begin{align}
    \mathcal{V}^{\star}(\mu) = \textsc{Eval}(\mu, \pi_{n}, \pi_{e}, \mathcal{E}_{\textrm{test}})
\end{align} where $\textsc{Eval}$ rolls out $\mu$ to coordinate $\pi_n$ and $\pi_e$ to perform tasks sampled from $\mathcal{E}_{\textrm{test}}$, and returns the AUC metric capturing the quality of $\mu$.
Our solution constructs a \textit{simulated validator} that evaluates a policy under conditions imitating the test conditions:
\begin{align}
    \mathcal{\tilde V}(\mu) = \textsc{Eval}(\mu, \tilde \pi_{n}, \tilde \pi_{e},\mathcal{ \tilde E}_{\textrm{test}})
\end{align} where we refer to $\tilde \pi_n$, $\tilde \pi_e$, and $\mathcal{\tilde E}_{\textrm{test}}$ as the simulated novice, expert, and test distribution, respectively.
The question is: how to choose these components to mimic closely the test conditions?

First of all, we set $\mathcal{ \tilde E}_{\textrm{test}} = \mathcal{E}_{\textrm{train}}$, as we only have access to $\mathcal{E}_{\textrm{train}}$ during training.
Given this choice, since $\pi_e$ performs well on $\mathcal{E}_{\textrm{test}}$, we want its imitation $\tilde \pi_e$ to perform well on $\mathcal{E}_{\textrm{train}}$ ($=\mathcal{ \tilde E}_{\textrm{test}}$).
A natural choice is to set $\tilde \pi_e = \pi_n$, as we assume the novice has mastered the training tasks.
Finally, for $\tilde \pi_n$, we want to construct a policy that performs poorly on $\mathcal{E}_{\textrm{train}}$ ($=\mathcal{ \tilde E}_{\textrm{test}}$), ideally at the same level as when $\pi_n$ performs tasks drawn from $\mathcal{E}_{\textrm{test}}$. 
Our approach is to learn a \textit{weakened novice} $\pi_n^-$ by running the same algorithm that was used to train $\pi_n$ on a \textit{limited} number of training tasks.  
This creates a policy whose performance regresses significantly when evaluated under the full training distribution.

Put all together, our simulated validator takes the following form
\begin{align}
    \mathcal{\tilde V}(\mu) = \textsc{Eval}(\mu, \tilde \pi_{n} = \pi_n^-, \tilde \pi_{e} = \pi_n,\mathcal{ \tilde E}_{\textrm{test}} = \mathcal{E}_{\textrm{train}})
\end{align} 

Let $\bar G(\pi, \mathcal{E})$ be the mean episode return of a policy $\pi$ on tasks sampled from a distribution $\mathcal{E}$.
To achieve a faithful simulation of the test conditions, we wanted to adjust the amount of tasks used to train $\pi^-_n$ such that $\bar G(\pi_n^-, \mathcal{E}_{\textrm{train}})/\bar G(\pi_n, \mathcal{E}_{\textrm{test}}) = 1$.
However, due to computational constraints and a large number of environments to evaluate, we choose the amount of training tasks to satisfy the following constraints 
\begin{align}
\frac{\bar G(\pi_n^-, \mathcal{E}_{\textrm{train}})}{\bar G(\pi_n, \mathcal{E}_{\textrm{test}})} \leq 5 
\ \ \ \  \frac{\bar G(\pi_n^-, \mathcal{E}_{\textrm{train}})}{\bar G(\pi_n, \mathcal{E}_{\textrm{train}})} \leq \frac{1}{2}
\end{align} and exclude several environments where these constraints are not satisfied.\footnote{We empirically observed that our validation approach performs poorly in many of those environments, in which the simulated test 
conditions diverge significantly from the true ones.} 


We note that our validation approach requires knowledge of $\bar G(\pi_n, \mathcal{E}_{\textrm{test}}$), the performance of the weak policy on test tasks.
This is a minimal and reasonable assumption, as without any knowledge of the discrepancy between training and test conditions, predicting the test performance of a policy would be impossible.

% The key challenge in learning the coordination policy $\mu$ arises from the \textit{training-test asymmetry}: during training, we only have access to the novice policy $\pi_n$  and the training environment distribution $\mathcal{E}_{\text{train}}$, with \textbf{no access} to either the expert policy $\pi_e$ or the test distribution $\mathcal{E}_{\text{test}}$. To address this challenge while maintaining the black-box constraint on $\pi_e$, we develop a \textit{simulated training framework} that preserves the essential coordination dynamics.

% Our solution centers on creating two synthetic components: (1) a \textit{simulated novice} $\tilde{\pi}_n$ that underperforms relative to $\pi_n$ over $\mathcal{E}_{\text{train}}$, and (2) a \textit{proxy expert} implemented by the original novice $\pi_n$. The simulated novice $\tilde{\pi}_n$ is constructed by deliberately limiting $\pi_n$'s capabilities (e.g., through partial training or early stopping) to create a performance gap analogous to the ($\pi_n, \pi_e$) relationship expected at test time. The coordination policy under the simulated training framework $\mu_{\text{sim}}$ is then trained to mediate between $\tilde{\pi}_n$ and $\pi_n$ using algorithm $\mathcal{T}$: \ben{This to reads to me like this is our primary proposed solution. Is that intentional? Or are we still just proposing this as one of many possible solutions.} \ben{oh wait, is the idea that this is the meta procedure we use for model selection for all of the approaches we consider?}
% \begin{align}
%     \mu_{\text{sim}} = \mathcal{T}\big(\underbrace{\tilde{\pi}_n}_{\text{simulated novice}}, \underbrace{\pi_n}_{\text{proxy expert}}, \mathcal{E}_{\text{train}}; \mathcal{V}\big)
% \end{align}
% This framework preserves three crucial properties from our problem formulation. First, the coordination policy only observes internal representations from the acting novice ($\tilde{\pi}_n$ in training, $\pi_n$ at test time). Second, the ``expert'' ($\pi_n$ during training) remains a black-box action provider. And third, the training environment  $\mathcal{E}_{\text{train}}$ differs from the test environment $\mathcal{E}_{\text{test}}$. At test time, we evaluate $\mu_{\text{sim}}$ under  $\mathcal{E}_{\text{test}}$ with the true expert $\pi_e$, measuring both task performance and query efficiency:
% \begin{align}
% p_{\text{sim}} = \textsc{Eval}\big(\mu_{\text{sim}}, \pi_n, \pi_e, \mathcal{E}_{\text{test}}\big)
% \end{align}

% To establish an upper bound on achievable performance, we define an \textit{oracle} coordination policy $\mu_{\text{true}}$ trained with full access to both $\pi_e$ and  $\mathcal{E}_{\text{test}}$: $\mu_{\text{true}} = \mathcal{T}(\pi_n, \pi_e, \mathcal{E}_{\text{test}})$. Then, the evaluations of $\mu_{\text{true}}$ is done by:
% \begin{align}
%     p_{\text{true}} = \textsc{Eval}\big(\mu_{\text{true}}, \pi_n, \pi_e, \mathcal{E}_{\text{test}}\big)
% \end{align}
% The performance gap between $p_{\text{true}}$ and $p_{\text{sim}}$ quantifies how effectively our simulated training framework preserves the coordination dynamics needed for test-time deployment. Crucially, this evaluation measures both the policy's ability to generalize to novel environment dynamics in $\mathcal{E}_{\text{test}}$, adapt to a previously unseen expert policy $\pi_e$, and maintain efficient coordination despite training-test asymmetry. This approach explicitly tests our core hypothesis: that coordination policies trained through our simulated framework can effectively generalize to novel experts and environment shifts, despite having no direct exposure during training.

% \subsection{Oracle Coordination Policy}

% To establish an upper bound on coordination performance, we define an \textbf{Oracle} coordination policy trained with privileged access to the expert $\pi_e$ and test environment $\mathcal{E}_{\text{test}}$:
% \begin{align}
%     \mu_{\text{true}} = \mathcal{T}(\pi_n, \pi_e, \mathcal{E}_{\text{test}}).
% \end{align}
% Later, it is evaluated by: $p_{\text{true}} = \textsc{Eval}(\mu_{\text{true}}, \pi_n, \pi_e, \mathcal{E}_{\text{test}})$. While impractical in real-world settings, this oracle provides a reference for evaluating learned policies.

% Since $\pi_e$ and $\mathcal{E}_{\text{test}}$ are unavailable during training, we propose an RL-based method to learn $\mu$ using a simulated novice $\tilde{\pi}_n$ and proxy expert $\pi_n$ (see \autoref{sec:training_eval}):
% \begin{align}
%     \mu_{\text{sim}} = \mathcal{T}_{\text{RL}}(\tilde{\pi}_n, \pi_n, \mathcal{E}_{\text{train}}).
% \end{align}
% This approach outperforms standard baselines:
% \begin{align}
%     \resizebox{0.85\columnwidth}{!}{$
%     p_{\text{sim}}(\text{RL}) > p_{\text{sim}}(A) \quad \text{for} \quad A \in \{\text{Random}, \text{Threshold}, \text{OOD}\}.
%     $}
% \end{align}
% A key challenge is mitigating distribution shifts between training ($\tilde{\pi}_n$, $\mathcal{E}_{\text{train}}$) and testing ($\pi_n$, $\mathcal{E}_{\text{test}}$). To improve generalization, we evaluate different feature spaces for $\mu$, focusing on two main properties. First, \textit{novice-awareness}, which captures $\pi_n$’s internal decision-making process through hidden states or action logits. Second, \textit{environment-robustness}, which relies on raw observations or task-specific embeddings that remain stable across environment shifts. We empirically identify feature combinations that minimize the performance gap between $\mu_{\text{sim}}$ and $\mu_{\text{true}}$, enabling robust real-world coordination.  




