\section{Experiments}\label{sec:experiments}

% \khanh{Let's be consistent in terminology. LEARNING METHOD (not ALGORITHM) = policy proposing + validation METHOD/APPROACH}

In this section, we leverage \ourMethod-Bench to compare numerous learning methods in a wide range of environments and gain insights into their strengths and weaknesses. 
We will also showcase the effectiveness of the validation approach proposed in \autoref{sec:sim-val}.

\begin{figure}[t]
\centering
    \includegraphics[width=0.9\linewidth]{figures/wins.pdf}
    \caption{Number of environments in which a learning method achieves the highest mean AUC. Solid bars indicate methods that use our proposed validation method.}
    \label{fig:method-comparison}
\end{figure}




\begin{figure*}[t]
\centering
    \includegraphics[width=\textwidth]{figures/ratio_to_rl.pdf}
    \caption{Test performance of learning methods across environments, normalized by the performance of the best \textsc{RLOracle} method. For each environment, we show three variants: the best performing method with simulated validation, the same method with oracle validation (\texttt{+oracle validation}), and the best RL method with simulated validation (\texttt{oracle policy proposer}). The gaps between the latter two variants and the original indicate room for improvement in the replaced components of the learning method. Error bars represent $2\times$ standard deviation.}
    \label{fig:test-performance}
\end{figure*}


\subsection{Methods}\label{sec:exp_alg}

We consider rule-based methods, which include: \textsc{AlwaysExpert}, which always yields control to the expert, and the \textsc{AlwaysNovice}, which always requests control, and \textsc{AlwaysRandom0.5}, which at every step tosses a fair coin to decide whether to yield control. 
These approaches implement a trivial policy proposer, which proposes a single candidate, and therefore does not require a validator.

We also evaluate more sophisticated policy-proposing approaches that require non-trivial validation approaches. 
We specifically combine them with the simulated validator described in \autoref{sec:sim-val}. 

\textsc{Random} queries the expert with a probability of $p \in [0,1]$, which is selected to maximize validation performance.
% It searches for the probability threshold $p \in $ to switch between novice and expert policies maximizing validation performance on $\mathcal{E}_{\text{train}}$. And second, a fixed threshold policy with $p=0.5$ where control decisions are made randomly with equal probability.

\textsc{Logit-based} approaches compute a confidence score based on the logit output of the novice. If the score falls below a pre-selected threshold, the agent yields control to the expert. The threshold is selected to maximize validation performance; otherwise, it requests control. To determine the optimal threshold, we roll out $\tilde \pi_{n}$ under the training environment for $64$ rollouts to generate a distribution of confidence scores. We then sweep through percentiles of this distribution, from the $\nth{0}$ to the \nth{100} percentile in steps of $10$) to identify candidate thresholds. These candidate thresholds are evaluated on simulated and true validation settings, and the threshold that maximizes the reward mean is selected as the final threshold for each setting. We explore several choices for the confidence score. \textsc{MaxLogit} uses the highest logit value. \textsc{MaxProb} computes the highest probability derived by applying the softmax function to the logits. \textsc{Margin} takes the difference between the top two softmax probabilities. \textsc{NegEntropy} calculates the negative entropy of the softmax distribution. Finally, \textsc{NegEnergy} uses the negative \texttt{logsumexp} of the logits \citep{liu2020energy}. This systematic approach ensures that the chosen threshold is robust and tailored to the specific confidence score metric being used.

\textsc{OOD} detection determines whether the input data is in-distribution or OOD. 
If the input is classified as OOD, the novice yields control. 
We specifically implement Deep SVDD \citep{pmlr-v80-ruff18a}, which identifies deviations from the training distribution by learning a neural network that maps input states to a minimal hypersphere in latent space. States outside this hypersphere (characterized by larger distances to the sphere center) are flagged as OOD. 
To determine the optimal threshold for classifying states as OOD, we follow a process similar to \textsc{Logit-based} approaches.

% OOD detection-based methods are grounded in the intuition that OOD states represent novel situations where the novice's training experience is inadequate, making expert intervention critical for task success \citep{amodei2016concrete}. 

Lastly, we include the \textsc{RLOracle} approach (\autoref{sec:rl-oracle}) to provide a feasible upper bound of the performance. 

% Lastly, we include \textbf{RL-based policies}, which are learned using RL algorithms to optimize the coordination between novice and expert policies dynamically. For this purpose, we utilize PPO algorithm, which is known for its robustness and efficiency in learning complex policies.

For both OOD detection-based and RL-based policies, we attempt various types of input $\phi$ to the coordination policy. 
We try every possible (non-empty) combination of the raw environment 
 observation (\texttt{obs}), the hidden features computed by the novice policy (to account for the novice's uncertainty) (\texttt{hidden}), and the probabilities of the novice's action distribution (\texttt{dist}), computed by applying the softmax function to the logits.
 % Other configurations include the hidden features of the novice policy and its logits (\texttt{hidden\_dist}), the raw observation combined with both the hidden features and logits of the novice policy (\texttt{obs\_hidden\_dist}), only the logits of the novice policy (\texttt{dist}), or only the hidden features of the novice policy (\texttt{hidden}). These variations allow the policies to leverage different levels of information and abstraction for optimal coordination.

We evaluate all methods on \ourMethod-Bench environments. Due to time and computational constraints, we exclude environments where we could not implement the simulated validator successfully (i.e. we could not construct a simulated weak agent whose performance on training environments closely matches that of the novice on test environments). 
In the end, we report results on $19$ environments. 

% \subsection{Environments}

% We evaluate the coordination policies across a diverse set of environments, including $3$ from the \textit{Minigrid} domain \cite{MinigridMiniworld23}, $11$ from the \textit{Procgen} suite \cite{pmlr-v119-cobbe20a}, and $5$ from the \textit{Cliport} robotic manipulation domain \cite{shridhar2021cliport}. These environments present a range of challenges, such as high-dimensional visual inputs, abstract low-dimensional states, and tasks requiring precise physical interaction. \autoref{fig:envs} provides examples of raw observations from each environment category.

% The \textbf{Minigrid environments} are grid-based domains designed to evaluate decision-making in navigation and exploration tasks. These environments vary in grid size and complexity, with simpler setups used during training and simulated evaluation, while larger, more complex configurations are reserved for true evaluation and testing. This progression challenges the agent's ability to scale its decision-making capabilities to more demanding scenarios.

% The \textbf{Procgen environments} are procedurally generated video games that introduce stochastic dynamics and complex tasks, testing the agent's adaptability to unseen scenarios. These environments feature two distributions: \texttt{easy}, used for training and simulated evaluation, and \texttt{hard}, used for true evaluation and testing. This setup ensures a meaningful distribution shift between training and testing conditions.

% The \textbf{Cliport environments} focus on robotic manipulation tasks that require precise spatial reasoning and interaction with objects. The domain includes objects or colors split into two sets: \texttt{seen} and \texttt{unseen}, corresponding to training and simulated evaluation, and true evaluation and testing, respectively. This distinction enforces a distribution shift to test the robustness of coordination policies.

% Across all environments, the combination of training and testing setups is intentionally designed to induce distribution shifts, challenging the coordination policies to generalize effectively to new and varied conditions. More details about the environments are provided in \aref{app:envs}.



% \subsection{Experimental Setup}

% In all experiments, the coordination policies are trained in simulated settings and evaluated in test settings to assess their generalization capabilities across different domains and environments. The performance of each coordination policy is compared using three key metrics.

% The first metric, \textbf{Task Completion Rate}, measures the proportion of tasks successfully completed by the coordination policy, serving as an indicator of its overall effectiveness. The second metric, \textbf{Expert Intervention Frequency}, tracks how often the novice policy yields control to the expert, providing insights into the agentâ€™s reliance on expert assistance. Finally, \textbf{Generalization Performance} evaluates how well each policy transfers its learned behavior from simulated training environments to true test environments, highlighting its ability to adapt to new and unseen scenarios. \ben{It's still not clear to me how these are defined, except for expert intervention frequency which seems intuitive. What does task completion rate mean for environments with intermediate rewards? and how is generalization performance different from that given that performance is always evaluated in the test environment, i.e., under distribution shift? I think we should provide specific mathematical definitions of these metrics.}

% Additionally, we evaluate the policies under different query cost factors $\alpha \in \{0.0,0.2,0.4,0.6,0.8,1.0\}$, which represent the cost of seeking expert assistance. This allows us to analyze how the coordination policies adapt their behavior to varying levels of cost associated with expert intervention.




\subsection{Results}

\textbf{Overview}.
% Our evaluation reveals several surprising findings about the effectiveness of different coordination strategies across environments. 
\autoref{fig:method-comparison} presents a comparison of methods based on the number of environments in which each achieves the highest mean AUC.

These results first of all show the effectiveness of our simulated validation approach. 
Methods leveraging this approach collectively outperform their counterparts in $14$ out of $19$ environments. 
% Among these, the Random policy with simulated validation emerges as the most successful approach, achieving the highest mean AUC in three environments.
Furthermore, three out of the four most successful methods employ the simulated validator. 

% \ben{How are the methods that don't use simulated validation doing model selection/choosing a threshold?} \mohamad{All the methods we study do simulated validation, except for the RL one which is used as the Oracle.} \ben{Wait so then what does it mean that 75\% of successful methods use simulated validation? It sounds like 100\% of all methods use simulated validation?}





A surprising finding is the strong performance of \textsc{Random}. Despite its simplicity, this approach outperforms more sophisticated methods in multiple environments. This result challenges the intuition that complex coordination strategies are superior for effective expert-novice collaboration.

Our analysis reveals a lack of consistency across methods. No method dominates: even the most successful ones achieve top performance in only $3$ out of $19$ environments. 
This result underscores the importance of a thorough empirical evaluation when selecting a solution approach for a specific \ourMethod problem.
It also suggests the necessity of having a comprehensive benchmark like \ourMethod-Bench, which supports quick evaluation of diverse methods by providing a unified interface, standardized evaluation pipeline, and off-the-shelf baseline implementations. 
% The variable performance of different methods across environments suggests that practitioners need to experiment with multiple approaches to find the best solution for their specific use case. Our codebase simplifies this process by providing standardized implementations and evaluation protocols for all tested methods.



\textbf{Diagnosing Weaknesses of Current Approaches}.
Our analysis reveals significant room for improvement, particularly in the more challenging environments (Procgen and CLIPort). As shown in \autoref{fig:test-performance}, the performance of current methods often falls significantly short of the theoretical maximum (normalized score of $1.0$).

To offer more specific guidance for future development, we introduce a systematic diagnostic method based on the proposer-validator decomposition of each algorithm. 
As a reminder, the policy proposer generates candidate coordination policies, while the validator evaluates these candidates to select the best one. 
% For instance, the Random method considers candidate policies that yield control with probability $p = 0, 0.2, ..., 1.0$, and selects the one that maximizes validation score for testing.
Ideally, we want a policy proposer that identifies the optimal policy as a candidate, and a validator that ranks it above all other policies.
When an approach falls short, either the policy proposer, or the validator, or both are deficient.

The proposer-validator decomposition enables us to identify which component limits performance of an algorithm by replacing each with an \textit{oracle counterpart} and measuring the resulting improvement. A dramatic performance boost after replacement indicates that the replaced component is severely deficient and requires enhancement.

We first examine the validator component by replacing the simulated validator with an oracle validator that accurately estimates the test performance. 
As shown in \autoref{fig:test-performance}, this replacement yields minimal improvement across most environments, with \texttt{bossfight} and \texttt{coinrun} being notable exceptions. This suggests that our simulated validation approach generally offers reliable policy evaluation.

More revealing is the replacement of the policy proposer  (\texttt{+oracle policy proposer}). We use \textsc{RLOracle}'s proposer, which generates candidate neural-network-based policies through PPO training on test environments. This replacement produces substantial performance improvements in $10$ out of $19$ environments.
This indicates that current methods are primarily limited by their policy proposers rather than their validators.

Taken together, our results reveal a fundamental limitation of current approaches: their search is constrained to an overly restricted policy space. While logit-based and OOD detection methods are conceptually appealing, their underperformance stems from their inability to consider sufficiently complex coordination strategies. 
Our finding suggests that future research should focus on methods capable of exploring richer policy spaces while maintaining computational efficiency. 
% \ben{I wonder if we should emphasize this more as a contribution, because it's both a clever technical idea and a valuable conceptual insight} \mohamad{Agree. If Khanh's also ok with it, we can expand more on this.}
% \khanh{sure}

\textbf{Best features for \textsc{RLOracle}}.
While being an oracle in our setting, \textsc{RLOracle} is a viable approach in a life-long learning setting, where the novice continuously adapts to test conditions. 
We investigate the best recipe for this approach to provide useful recommendations for researchers who want to tackle this setting.

Our experiments reveal that including raw environment observations as input to the coordination policy consistently improves performance compared to using only its hidden representations or its logit outputs.
This trend presents in $15$ out of $19$ environments (\autoref{fig:rl_summary}), suggesting that the novice does not acquire helpful, easily extractable uncertainty information if trained only to perform tasks autonomously.
% We observe that RLOracle policies conditioned on raw observations (which are \texttt{obs}, \texttt{hidden obs}, \texttt{obs dist}, \texttt{obs hidden dist}) show performance improvements compared to those relying solely on the novice's internal representations. 
% This suggests that providing RLOracle policies with direct access to environmental observations enables them to learn more effective coordination strategies, rather than constraining them to work only with pre-extracted features from the novice policy. The ability to process raw observations allows these policies to extract and leverage relevant information that might not be fully captured in the noviceâ€™s internal representations alone. 
% We present more analyses in \aref{app:exp_rloracle}.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/rl_summary_across_envs.pdf}
    \caption{Aggregate performance comparison of \textsc{RLOracle} methods across all environments. Observation-conditioned methods outperform those using only novice policy's internal representations. See \autoref{sec:exp_alg} for each input feature explanation.}
    \label{fig:rl_summary}
\end{figure}

Our results also highlight the critical relationship between environment complexity and observation-space utility. While raw observations generally provide richer learning signals, their value diminishes in structured environments with comprehensive feature representations (\aref{app:exp_rloracle}). 
We thus suggest practitioners to prefer observation-conditioned coordination policies unless observations are complex to model and hidden representations are sufficiently rich.


\textbf{Comparison of Logit-based and OOD detection-based Methods}.
Our experiments reveal a fundamental advantage of logit-based methods over the Deep SVDD OOD detection approach, as quantified in \autoref{fig:logit_ood_summary}. Overall, in $10$ out of $19$ evaluated environments, logit-based methods  significantly outperform deep learning OOD detection-based techniques. This performance gap emerges most strongly in Procgen and CLIPort suites.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/logit_ood_summary.pdf}
    \caption{Comparison of \colorbox{boxred}{logit-based } and \colorbox{boxpurple}{OOD detection} methods. Error bars show the standard deviation across each method's variants.}
    \label{fig:logit_ood_summary}
\end{figure}


This suggests that practitioners may prefer computationally lightweight logit-based coordination unless operating in domains with known visual-semantic mismatch between observation space and task requirements.
Based on our results, we suggest practitioners reconsider the prevailing assumption that complex OOD detection is universally preferable for safety-critical coordination \citep{yang2024generalized}. We demonstrate that simpler approaches often suffice when distribution shifts primarily affect agent behavior rather than environmental appearance.


% \textbf{Near-Optimal Coordination Achievements}.
% Another insight extracted from environment-specific analyses is that select coordination strategies approach oracle-level performance as shown in \autoref{fig:env_perf_selected}. In some contexts and environments, such as \texttt{DoorKey}, \texttt{LavaGap}, and \texttt{Climber}, logit-based or OOD detection-based methods demonstrate near-perfect coordination without test-time distribution knowledge. More detailed results are provided in \aref{app:exp_optimal}.


% \begin{figure*}[t]
% \centering
% \includegraphics[width=0.8\textwidth]{figures/env_performance_selected.pdf}
% \caption{Comprehensive performance comparison across \texttt{DoorKey}, \texttt{LavaGap}, \texttt{climber} and methods. \colorbox{boxbrown}{Skyline} performance represents the oracle upper bound, with \colorbox{boxred}{logit} and \colorbox{boxpurple}{OOD} methods approaching this limit in structured environments. \ben{Wait I'm confused... The brown ones are the skyline? Then where are the RL-based non-skyline methods?} \ben{Also, is there a key for the x axis in the appendix or something? Because some of these terms have not been defined}} \mohamad{Due to time constraints we couldn't run the RL-based non-skyline methods. About the x-axis keys, they're defined in the last paragraph of Sec 5.1. Should we elaborate on those? \ben{Oh gotcha. And we removed the description of the RL-based non-skylines earlier in the paper, right? For the terms, can we just add a pointer to section 5.1?}}
% \label{fig:env_perf_selected}
% \end{figure*}

% Our findings challenge the assumption that test-time distribution knowledge is essential for robust coordination. The proximity to skyline performance across different environments suggests that carefully designed simulation-based training can produce coordination policies robust to real-world distribution shifts. This opens new possibilities for deploying adaptive human-AI coordination systems in safety-critical domains without requiring exhaustive test environment sampling.

