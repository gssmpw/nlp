\section{Conclusion \& Limitations}

In this work, we formalize the Yield and Request Control (\ourMethod) problem, a critical challenge for AI agents operating in dynamic, safety-critical environments. Our contributions include: (1) a rigorous formulation of \ourMethod under practical constraints, emphasizing train-test distribution shifts and black-box expert interactions; (2) \ourMethod-Bench, a modular benchmark for evaluating coordination strategies across diverse domains; and (3) empirical insights revealing surprising limitations of existing methods. Key findings demonstrate that simple strategies like randomized interventions often match or surpass complex approaches, while RL-based policies leveraging raw environmental observations outperform those relying solely on novice internal representations. Our analysis further identifies policy proposer limitations as a primary bottleneck, underscoring the need for richer policy spaces in future work. These results challenge assumptions about the necessity of intricate coordination mechanisms and provide actionable guidance for practitioners deploying human-AI collaborative systems.

%\khanh{don't repeat what is in introduction here. we want to call for some actions from the community. solving \ourMethod is an important first step for tackling more complex human-AI collaboration problems. we show there is still lot of room for improvement in this problem, necessating the development of new methods.}
Solving the \ourMethod problem is an important first step toward tackling more complex human-AI collaboration challenges. Our findings highlight significant room for improvement, necessitating the development of new methods and encouraging the community to advance research in this critical area. By addressing these gaps, we can pave the way for more robust and effective human-AI collaborative systems in the future.

While our work advances the understanding of expert-novice coordination, several limitations warrant consideration. First, simulated experts in \ourMethod-Bench, may not fully capture the variability and cognitive biases of human operators. Second, while our benchmark incorporates distribution shifts across environments, real-world shifts may involve more complex, multimodal dynamics not yet modeled. Third, the cost model assumes fixed query costs, whereas practical deployments often face context-dependent or time-varying costs. Finally, our evaluation focuses on episodic tasks, leaving open questions about lifelong coordination in non-stationary settings. %Finally, the computational demands of RL-based methods may limit scalability in resource-constrained scenarios.
Addressing these limitations through more advanced models of human cognition, dynamic cost modeling, and more effective, computationally efficient learning methods presents promising directions for future research.

