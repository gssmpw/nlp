When deployed in dynamic environments, AI agents will inevitably encounter challenges that exceed their individual capabilities. Leveraging assistance from expert agents—whether human or AI—can significantly enhance safety and performance in such situations. However, querying experts is often costly, necessitating the development of agents that can efficiently request and utilize expert guidance.
In this paper, we introduce a fundamental coordination problem called Learning to Yield and Request Control (\ourMethod), where the objective is to learn a strategy that determines when to act autonomously and when to seek expert assistance. We consider a challenging practical setting in which an agent does not interact with experts during training but must adapt to novel environmental changes and expert interventions at test time.
To facilitate empirical research, we introduce \ourMethod-Bench, an open-source benchmark featuring diverse domains. \ourMethod-Bench provides a standardized Gym-like API, simulated experts, evaluation pipeline, and implementation of competitive baselines. 
Towards tackling the \ourMethod problem, we propose a novel validation approach and investigate the performance of various learning methods across diverse environments, yielding insights that can guide future research.
