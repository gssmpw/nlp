\onecolumn
\doparttoc %
\faketableofcontents %

\part{Appendix}\label{appendix} %
\parttoc %


\clearpage

\section{Details about \ourMethod-Bench}\label{app:imp_details}

\subsection{Coordination Environment Wrapper}


\input{figures/env_wrapper}

\input{figures/algorithm}


To standardize coordination policy training and evaluation, we introduce the \texttt{HelpEnvironment} wrapper. This tool converts any Gym-compatible environment \citep{1606.01540, towers2024gymnasium} into an \textit{MDP for the coordination policy} that preserves the original state space $\mathcal{S}$ but replaces the action space with a binary choice $\{\texttt{n}, \texttt{e}\}$, representing the coordination policy's decision to request control (novice acts) or yield control (expert acts). At each timestep, the wrapper resolves the coordination policy $\mu$'s decision into a concrete environment action: if $\mu(s_t) = \texttt{n}$, then $x_t = a^n_t \sim \pi_n(s_t)$ which is the novice's proposed action; if $\mu(s_t) = \texttt{e}$, then $x_t = a^e_t \sim \pi_e(s_t)$ which is the expert's action. Consequently, the next state $s_{t+1}$ is generated by the base environment's transition dynamics $P(s_{t+1} | s_t, a_t)$.

\autoref{fig:wrapper_demo}'s \colorbox{sbBlue025}{blue region} provides a high-level demonstration of coordination environment. In addition, \autoref{fig:env_wrapper} provides its pseudocode. The \texttt{make()} utility function initializes four coordination environments: a training environment featuring the simulated novice $\tilde{\pi}_n$ and simulated expert $\pi_n$ coordinating on tasks sampled from $\mathcal{E}_{\text{train}}$, a simulated validation environment which is similar to the training environment but includes held-out tasks sampled from $\mathcal{E}_{\text{train}}$, 
a validation environment featuring $\pi_n$ and $\pi_e$ coordinating on tasks sampled from $\mathcal{E}_{\text{test}}$, 
and a testing environment which is similar to the validation environment but includes held-out tasks sampled from $\mathcal{E}_{\text{test}}$. 
By modularizing the coordination logic into a reusable environment wrapper, we support systematic evaluation of policies across diverse domains (e.g., grid navigation, procedural generation, robotic manipulation) and enforce a standardized interface for control delegation between novice and expert policies.

The \texttt{HelpEnvironment} wrapper performs three essential tasks: \begin{itemize}
    \item \textbf{Policy Integration:} As shown in \autoref{fig:wrapper_demo} and \autoref{fig:env_wrapper}, the wrapper accepts both novice and expert policies, managing the handover of control between them dynamically. This process is central to evaluating and optimizing coordination strategies.
    \item \textbf{Cost Computation:} The wrapper incorporates cost functions that consider the environment's reward, switching costs, and expert labor costs. These cost components are crucial for realistic assessments of coordination trade-offs.
    \item \textbf{Performance Tracking:} To facilitate robust research, the wrapper includes standardized metrics for evaluating coordination performance. These metrics include cumulative cost, task completion rates, and the frequency of expert interventions, ensuring comprehensive assessments.
\end{itemize}

\autoref{fig:env_wrapper} highlights the modularity of this design, which enables researchers to seamlessly adapt the wrapper to new environments or agents. By leveraging this implementation, users can efficiently conduct experiments on coordination policy without significant modifications to existing environments.



\subsection{Training Framework}

\input{figures/train}

The \texttt{train.py} script, with pseudocode in \autoref{fig:train}, orchestrates the entire training process for the \ourMethod problem.  It begins by loading configuration parameters from a specified file. Using these configurations, it instantiates the necessary components: training, validation, and testing environments; a coordination policy; and an evaluator.  If the configuration specifies a simple, non-trainable algorithm (e.g., \textsc{AlwaysExpert}), the script directly evaluates the pre-defined policy. Otherwise, it instantiates a training algorithm based on \texttt{Algorithm} class, then calls the \texttt{train()} method. This main training loop takes the policy, environments, and evaluator as input, iteratively improving the coordination policy. The evaluator periodically assesses the policy's performance on validation splits to track progress.





The \ourMethod benchmark supports the training of coordination policies using the \texttt{Algorithm} class, a modular and extensible framework for implementing training routines. The \texttt{Algorithm} class encapsulates the core training logic, providing methods for initializing training parameters and managing the iterative process of policy improvement. An overview of the class implementation is shown in \autoref{fig:wrapper_demo}'s \colorbox{sbOrange025}{orange region} with its pseudocode available at \autoref{fig:algorithm}. This class organizes the training flow, allowing researchers to define how policies should be updated based on interactions with the environment or datasets.

The \texttt{train()} method iterates through training cycles, calling the \texttt{train\_one\_iteration} function in each loop to refine the policy. At specified intervals, the method invokes the evaluator to assess the policy’s performance on validation and test environments, providing important feedback and facilitating the saving of the best model. This iterative process, demonstrated in \autoref{fig:algorithm}, is central to the policy optimization.

By following the structure outlined in \autoref{fig:algorithm}, researchers can easily integrate new algorithms into the benchmark. The modular design ensures that the training and evaluation pipeline is easily adaptable, promoting reproducibility and flexibility for different types of coordination policy experiments.

\subsection{Evaluation Framework}

\input{figures/eval}

To assess the performance of the coordination policy, the benchmark includes a comprehensive evaluation framework. The framework provides standardized tools for comparing algorithms across different domains and scenarios. Evaluation metrics include cumulative cost, task success rate, expert intervention frequency, and other domain-specific measures. These metrics are essential for understanding the trade-offs between autonomy and expert reliance. The evaluation framework ensures that comparisons between methods are fair, standardized, and meaningful. Central to this framework is the \texttt{Evaluator} class, which handles the execution of policies and the collection of metrics. The \texttt{eval.py} script leverages this class to perform standardized evaluation runs, as shown in \autoref{fig:eval}. It first loads the same configuration used for training, then instantiates the environment and the policy to be evaluated. If the evaluated algorithm requires a trained model, it is loaded from a specified checkpoint. The \texttt{eval.py} script then calls the \texttt{evaluator.eval()} method to evaluate the policy on the designated test split. The Evaluator class uses the evaluation metrics, providing a comprehensive assessment of the policy's performance and adhering to standardized metrics.



\subsection{Benchmark Dependencies}
Our benchmark implementation leverages several open-source repositories for environment implementations and algorithm baselines:

\begin{itemize}
\item \textbf{MiniGrid Environments}: We utilize the Farama Foundation's MiniGrid implementation \citep{MinigridMiniworld23} for grid-based navigation tasks. The environment wrapper and agent policies interface with the Gymnasium API provided by this repository. Codebase: \url{https://github.com/Farama-Foundation/Minigrid}
\item \textbf{Cliport Environments}: Robotic manipulation tasks are implemented using the CLIPort repository \citep{shridhar2021cliport}, which provides RGB-D observation spaces and physics-based manipulation challenges. Codebase: \url{https://github.com/cliport/cliport}
\item \textbf{Procgen Environments}: Procedurally generated environments are adapted from the ProcgenAISC fork, which maintains compatibility with asynchronous actor-critic algorithms. We use commit \texttt{7821f2c} for experiment reproducibility. Codebase: \url{https://github.com/JacobPfau/procgenAISC/tree/7821f2c00be9a4ff753c6d54b20aed26028ca812}
\item \textbf{OOD Detection}: The PyOD library \citep{zhao2019pyod} provides implementations of various outlier detection algorithms, including the Deep SVDD method used in our OOD-based policies. Codebase: \url{https://github.com/yzhao062/pyod}
\end{itemize}

All environments are wrapped using our custom \texttt{HelpEnvironment} class (described in \autoref{app:imp_details}) to enable standardized coordination policy evaluation. The PyOD implementations were particularly valuable for implementing the OOD detection-based policies. We modified the original repositories only to the extent required for policy coordination mechanics, preserving their core environment dynamics and observation spaces.



\subsection{Training Novice and Expert Policies}
The \ourMethod framework requires three acting policies for coordination policy training:
\begin{itemize}
    \item \textbf{Expert} ($\pi_e$): High-performing policy for test-time assistance
    \item \textbf{Novice} ($\pi_n$): Policy trained on $\mathcal{E}_{\text{train}}$
    \item \textbf{Weakened Novice} ($\pi_n^-$): Suboptimal novice policy
\end{itemize}

\textbf{Expert Policy Training}.
For MiniGrid and Procgen environments, we train $\pi_e$ using PPO on $\mathcal{E}_{\text{test}}$ until convergence \citep{huang2024open}. For CLIPort's robotic manipulation tasks, we use predefined rule-based oracles as experts, leveraging their guaranteed success rates through handcrafted logic.

\textbf{Novice Policy Training}. 
The novice $\pi_n$ is trained exclusively on $\mathcal{E}_{\text{train}}$. MiniGrid and Procgen novices are trained using PPO on $\mathcal{E}_{\text{train}}$ until convergence. CLIPort novices are taken from provided checkpoints trained on $100$ demonstrations, establishing baseline task proficiency.

\textbf{Weakened Novice Policy Training}.
We create $\pi_n^-$ by deliberately limiting training exposure. For MiniGrid and Procgen, we halve PPO training epochs while maintaining $\mathcal{E}_{\text{train}}$ exposure. CLIPort's $\pi_n^-$ uses checkpoints trained on only $10$ demonstrations, reflecting partial task mastery. This mimics test-time performance degradation while preserving training distribution familiarity.



\subsection{Training Coordination Policy}\label{app:coord_policy}

\subsubsection{\textsc{Logit-Based} Methods}
A widely used family of coordination policies in our benchmark is based on thresholding techniques applied to confidence scores computed from the novice's logits. The fundamental principle behind these methods is to quantify the model’s certainty using a specific metric and then compare this score against a threshold. If the score falls below the threshold, the policy delegates control to the expert; otherwise, it acts autonomously.

\textbf{Confidence Metrics}.  
We consider five different metrics for computing the confidence score of the novice:
\begin{itemize}
    \item \textsc{MaxLogit}: The maximum logit value is used directly as a confidence measure.
    \item \textsc{MaxProb}: The softmax function is applied to the logits, and the highest probability is selected.
    \item \textsc{Margin}: The difference between the top two probabilities in the softmax distribution is computed, where larger margins indicate higher confidence.
    \item \textsc{NegEntropy}: The negative entropy of the softmax probability distribution is used, with lower entropy (higher negative entropy) corresponding to more certainty.
    \item \textsc{NegEnergy}: The log-sum-exponential (logsumexp) of the logits is computed, offering an energy-based measure of certainty.
\end{itemize}

\textbf{Threshold Selection via Rollouts}.
To determine an optimal threshold, we conduct a grid search over a range of candidate threshold values. Specifically, we perform $64$ rollouts in the training environment, where the environment is set up to run $64$ parallel instances. From these rollouts, we generate a distribution of confidence scores given the environment's raw observations, from which candidate thresholds are computed as percentiles. The search space consists of percentiles ranging from $0$ to $100$, incremented in steps of $10$. 

This process is implemented in the \texttt{ThresholdAlgorithm} class, inherited from the \texttt{Algorithm} class. For each candidate threshold, the policy is evaluated on simulated and true validation splits, and records the candidate yielding the highest mean reward. During inference, the policy processes an input observation as follows: 
\begin{itemize}
    \item Computes the confidence score using the configured metric.
    \item Compares the score to the current threshold. Since a higher score corresponds to greater confidence, a score below the threshold triggers delegation, yielding control to the expert.
\end{itemize}

This threshold-based method enables a systematic, data-driven approach to determining delegation decisions. By evaluating different confidence metrics, it provides flexibility in choosing the most effective measure of certainty for a given environment. 




\subsubsection{\textsc{OOD-Detection} Methods}
The OOD detection methods in \ourMethod-Bench are built upon the Deep SVDD method \citep{pmlr-v80-ruff18a}. These methods aim to identify when the novice's observations fall outside the training distribution, thereby signaling that control should be delegated to the expert. In our implementation, the OOD detector is initialized by gathering rollouts from the training environment; specifically, we perform $64$ rollouts with $64$ parallel environment instances. The collected observations serve a dual purpose: they are used both to train the Deep SVDD model and to determine a suitable threshold for delegation via a grid search, similar to the \textsc{Logit-Based} methods.

The Deep SVDD algorithm minimizes the distance between feature representations and a pre-defined center. After training, the detector computes decision scores for a separate set of rollout observations. Candidate thresholds are then determined by linearly spacing values between the minimum and maximum decision scores, following the same procedure as the \textsc{Logit-Based} methods. Our implementation leverages the PyOD library, which provides a suite of OOD detection algorithms, including Deep SVDD \citep{zhao2019pyod}. All hyperparameters for Deep SVDD are set to their default values in PyOD, without additional tuning. Furthermore, with minimal modifications, other PyOD-based OOD detection methods can be seamlessly integrated to evaluate their effectiveness in the \ourMethod problem.

A key feature of our OOD-detection approach is its flexibility in the input feature space. The observation space may comprise raw observations, hidden features from the novice policy, or combinations thereof (e.g., \texttt{obs}, \texttt{hidden}, \texttt{hidden\_obs}, \texttt{dist}, \texttt{hidden\_dist}, \texttt{obs\_dist}, \texttt{obs\_hidden\_dist}). This design enables the OOD detector to leverage a richer set of features, potentially enhancing its ability to distinguish in-distribution inputs from OOD ones.

During inference, the OOD policy computes an anomaly score using the detector's decision function. A delegation decision is then made by comparing the anomaly score to the learned threshold: if the score is below the threshold, the policy yields control to the expert; otherwise, it retains control. 




\subsubsection{\textsc{RL-Based} Methods}
The RL-based coordination policy in our benchmark is trained using Proximal Policy Optimization (PPO), an on-policy actor–critic method that balances efficient policy updates with sample efficiency \citep{schulman2017proximal}. In our implementation, the coordination policy is parameterized via an actor–critic architecture, where the actor produces a probability distribution over actions and the critic estimates the corresponding state values.

During training, multiple parallel environments (e.g., $64$ instances) are run simultaneously to collect a batch of trajectories over a fixed number of steps. The observation space for the RL methods is flexible and can be configured to include raw observations, hidden features extracted by the novice agent, or combinations thereof (such as raw observations concatenated with hidden features or with action logits), similar to the \textsc{OOD-detection} methods. This flexibility allows the policy to leverage richer contextual information when making delegation decisions.

After collecting trajectories, advantage estimates are computed using Generalized Advantage Estimation \citep{Schulmanetal_ICLR2016}, with the critic bootstrapping the final state value to compute temporal-difference errors. These advantage estimates are typically normalized prior to being used in the policy update. The PPO update itself minimizes a surrogate objective that includes three key components: a clipped policy loss to restrict large updates, a value loss to improve the accuracy of the critic, and an entropy bonus to encourage exploration.

The underlying network architecture is based on an Impala model that extracts features from the input observations \citep{espeholt2018impala}. Depending on the chosen configuration, these features may be combined with latent representations from the novice or with softmax-transformed logits. A fully connected layer then projects the aggregated features to produce policy logits over the available actions.

Additional training techniques such as dynamic learning rate annealing and gradient clipping are employed to ensure stable convergence. Overall, the PPO-based method iteratively collects data, computes gradients on mini-batches, and updates the policy and value networks until the coordination policy converges.

This method can operate in two distinct modes: as a \textit{skyline approach} that utilizes access to the expert policy $\pi_e$ and test environment $\mathcal{E}_{\text{test}}$ during training to derive near-optimal policies, and as a \textit{baseline method} where such access is intentionally restricted during training. The latter configuration enables fair comparison with alternative coordination strategies by matching their practical constraints.


\clearpage

\section{Environment Details}\label{app:envs}

We evaluate coordination policies across three distinct domains, each containing multiple environments with carefully designed train-test splits to test policy generalization under distribution shifts. Below we describe the specific environments and their configurations.

\subsection{MiniGrid Environments}
The grid-based navigation domain contains three environment families with progressive complexity:

\begin{itemize}
\item \texttt{DistShift}: Training uses \textit{1-v0} (small grid), while testing uses \textit{2-v0} (expanded grid with longer trajectories)
\item \texttt{DoorKey}: Training on \textit{-5x5-v0} ($5\times5$ grid), testing on \textit{-8x8-v0} ($8\times8$ grid with more complex door-key relationships)
\item \texttt{LavaGap}: Training on \textit{S5-v0} (5-tile lava gap), testing on \textit{S7-v0} ($7$-tile gap requiring longer jumps)
\end{itemize}

All MiniGrid environments use partially observable grids with discrete actions. The test versions feature larger state spaces and more complex spatial relationships than their training counterparts.

\subsection{Procgen Environments}
The procedural generation suite includes $11$ distinct platformer games, each with two difficulty levels:

\begin{itemize}
\item \texttt{bossfight}: Combat-focused game with escalating enemies
\item \texttt{caveflyer}: Navigation through procedural caverns
\item \texttt{chaser}: Avoidance of pursuing enemies
\item \texttt{climber}: Vertical ascension challenge
\item \texttt{coinrun}: Collection-based platformer
\item \texttt{dodgebal}: Projectile avoidance game
\item \texttt{heist}: Stealth-based item retrieval
\item \texttt{jumper}: Precision jumping challenges
\item \texttt{maze}: Complex spatial navigation
\item \texttt{ninja}: Timing-based obstacle course
\item \texttt{plunder}: Resource gathering under threat
\end{itemize}

The \textit{easy} distribution (training/simulated evaluation) uses simplified dynamics and predictable patterns, while the \textit{hard} distribution (true evaluation/testing) introduces stochastic elements, and more complex terrain generation.

\subsection{Cliport Environments}
The robotic manipulation domain contains five tasks with object configuration splits:

\begin{itemize}
\item \texttt{Assembling-Kits-Seq}: Sequential object placement in kits
\item \texttt{Packing-Boxes-Pairs}: Object pairing and containerization
\item \texttt{Put-Block-in-Bowl}: Precise object-in-container placement
\item \texttt{Stack-Block-Pyramid-Seq}: Vertical structure assembly
\item \texttt{Separating-Piles}: Object sorting and segregation
\end{itemize}

The \textit{seen} split (training/simulated evaluation) uses a fixed set of object shapes and color configurations, while the \textit{unseen} split (testing) introduces novel object geometries and color combinations not encountered during training. All tasks require $6$-DOF control and pixel-level spatial reasoning.

The combination of these environments provides comprehensive coverage of key challenge domains: discrete vs continuous control, 2D vs 3D spatial reasoning, and symbolic vs pixel-based observations. \autoref{fig:envs} in the main text illustrates representative observations from each domain.


\clearpage

\section{Detailed Results}

\subsection{Performance of \textsc{RLOracle} Methods} \label{app:exp_rloracle}

We further analyze the performance of individual \textsc{RLOracle} algorithms across different environments, as shown in \autoref{fig:rl_performance}. This detailed breakdown reveals that the advantage of raw observation-based policies is more pronounced in Procgen and CLIPort environments, whereas it is less salient in the MiniGrid suite. This discrepancy can be attributed to the nature of the environments: Procgen and CLIPort feature visually rich, high-dimensional observation spaces where direct access to raw observations provides a clear advantage in learning nuanced coordination behaviors. In contrast, MiniGrid consists of low-dimensional, symbolic representations where the distinction between raw observations and the novice’s internal features is less significant. In such structured environments, the novice policy’s internal representations already capture most of the relevant task information, reducing the advantage of using raw observations.

% \khanh{can you explain why cliport seems to benefit less from observations than procgen?} \mohamad{my guess is that in cliport, the hidden representations coming from the novice are rich enough to holp the coord policy make an informed decision without the raw obs. while in procgen, the novice's hidden representations are not informative enough so giving the raw obs to the coord policy makes a bigger difference. after all this is just based on my intuition and bias. it should be investigated to verify whether the novice’s hidden representations are informative enough by analyzing feature distributions and running an ablation study.}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.92\linewidth]{figures/rl_performance.pdf}
    \caption{Per-environment performance of \textsc{RLOracle} variants. \colorbox{boxsteelblue}{Observation-conditioned methods} show strongest advantages in high-dimensional environments, while MiniGrid environments show smaller differences due to their low-dimensional state representations.}
    \label{fig:rl_performance}
\end{figure}

\subsection{Near-Optimal Coordination Achievements} \label{app:exp_optimal}

\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]{figures/env_performance.pdf}
\caption{Comprehensive performance comparison across all and methods. \colorbox{boxbrown}{Skyline} performance represents the oracle upper bound, with \colorbox{boxred}{logit} and \colorbox{boxpurple}{OOD} methods approaching this limit in structured environments. Gray backgrounds denotes such environments.}
\label{fig:env_perf}
\end{figure}

\autoref{fig:env_perf} illustrates the overall performance of each algorithm and input feature type across all environments studied in this paper. It reveals an interesting pattern: logit-based and OOD detection-based coordination policies achieve near-skyline performance in $3$ environments. We analyze these representative success cases:

\textbf{DoorKey (MiniGrid):} The $8\times8$ grid environment exhibits deterministic dynamics but requires precise multi-step sequencing (find key, then unlock door, then navigate to goal). The \textsc{MaxLogit} policy matches skyline performance matches skyline performance by interfering the novice's potentially flawed decision-making, preventing costly mistakes and ensuring efficient completion of the task.


\textbf{LavaGap (MiniGrid):} This environment's lethal consequences (falling into lava) create clean separation between high-confidence navigation actions and uncertainty ``cliff edges.'' The OOD-based method with \texttt{hidden-dist} features and Margin logit policy are the closest to skyline performance.

\textbf{Climber (Procgen):} Despite procedural generation, the logit-based methods are statistically the same as skyline methods. The policy successfully distinguishes between challenging-but-seen obstacles (handled by novice) and truly novel gap configurations (referred to expert), despite being trained solely on the \texttt{easy} distribution.
% \khanh{again, feel like a figure illustrating these scenarios would help}

Our analysis reveals significant performance gaps between \textsc{RLOracle} and other methods in certain scenarios. Notably, across all CLIPort manipulation tasks, no coordination policy approaches ``worst'' \textsc{RLOracle}'s performance. The most striking example occurs in the \texttt{packing-boxes-pairs} task: while the lowest-performing \textsc{RLOracle} variant (using only the novice's action probability distribution as input) achieves a performance of $0.83$, the best non-\textsc{RLOracle} methods (logit-based approaches) reach only $0.73$ - a $13.7\%$ relative performance gap. Other CLIPort tasks exhibit even wider disparities, with \textsc{RLOracle} outperforming alternatives by at least $30.7\%$ across \texttt{assembling-kits-seq}, $35.1\%$ across \texttt{put-block-in-bowl}, $40.3\%$ accross \texttt{stack-block-pyramid-seq}, and $20.9\%$ across \texttt{separating-piles} environments. These substantial gaps highlight fundamental limitations in current coordination strategies for high-dimensional manipulation tasks, suggesting urgent needs for improved policy architectures that better leverage both environmental observations and novice uncertainty signals. 
Moreover, in other environments, as shown in \autoref{fig:env_perf}, the \textsc{RLOracle} methods significantly outperform the other methods, showing the gap between the oracle method and baselines.