\section{Introduction}\label{sec:introduction}

The deployment of AI agents in real-world environments presents a significant challenge: they must operate successfully in dynamic, unpredictable settings where their individual capabilities may often be insufficient for success \citep{amodei2016concrete, leike2017ai,zhou2024hazard}. 
A promising solution is to teach these agents to seek assistance from more capable (human or AI) agents when necessary. 
This approach has improved safety and performance in various domains  \citep{sadigh2016planning,reddy2018shared, nguyen2021learning}. 
Nevertheless, providing expert assistance is often resource-intensive, necessitating the development of AI agents that not only effectively utilize expert assistance but also minimize associated costs. 

We address this challenge by formulating a fundamental coordination problem called \textit{Learning to \textbf{Y}ield and \textbf{R}equest \textbf{C}ontrol} (\ourMethod).
In this problem, an AI agent, called a novice, must learn a policy to decide at each time step whether to act independently or cede control of its body to an expert. Our work generalizes prior work that focuses solely on requesting expert assistance \citep{nguyen-daume-iii-2019-help,Nguyen_2019_CVPR,shi2022learning,singh2022ask4help,liu2022asking,knowno2023} by introducing the additional challenge of determining when to \textit{terminate} expert intervention.
This allows AI agents to leverage their computational power to generate more optimized coordination plans.


% In this paper, we address this challenge through the lens of a fundamental coordination problem that we term \textit{learning to \textbf{Y}ield and \textbf{R}equest \textbf{C}ontrol} (\ourMethod). 
% This problem requires a novice policy to decide at each time step whether to \textit{request} control of its (virtual or physical) body, or \textit{yield} that control to an expert. 
% The \ourMethod problem generalizes prior work on asking for help  by introducing the option to proactively terminate expert assistance (request control). 
% This option potentially further minimizes expert time and resources. 
% \citep{hadfield2016cooperative}.


% Our problem represents a further step in automating human-AI coordination. 
% \ben{Maybe ``furthering human-AI collaboration''? Automating feels like the wrong word}
% \khanh{how about this?} \ben{I worry that the phrasing ``reducing effort'' makes it sound like less of a big deal, how about ``ensuring that the human's valuable time is spent as efficiently as possible in human-AI collaboration''? Can also omit the ``in human-AI collaboration'' at the end} \mohamad{Updated.}

% In this work, requesting control refers to the novice retaining authority to execute its own actions, while yielding control transfers this authority to the expert, enabling it to directly operate the body. 
% The core challenge lies in learning when to act autonomously versus defer to the expert under dynamic environmental conditions.

We present a problem setting that introduces two critical challenges inspired by real-world scenarios. First, we model the expert as a black box: its internal decision-making process is unobservable to the novice, reflecting practical constraints where experts may be either humans with opaque cognition or proprietary AI systems accessible only through limited APIs.
Second, the novice faces a significant train-test distribution shift due to environmental changes and novel interactions with the expert. 
Specifically, the novice masters the training tasks and never needs to interact with an expert while performing those tasks. However, at test time, it encounters unfamiliar tasks and must effectively collaborate with an expert despite having no prior experience in doing so.
Overall, our setting presents a novel problem that combines the difficulties of out-of-distribution (OOD) generalization, cognitive modeling, and sequential decision-making.

% Together, these challenges demand policies that dynamically coordinate with opaque experts under non-stationary dynamics, without prior exposure to expert interactions during training.



To advance research on \ourMethod, we introduce \textit{\ourMethod-Bench}, a comprehensive benchmark with four appealing features: (1) diverse environments with a unified interface tailored for multi-agent coordination (MiniGrid \citep{MinigridMiniworld23}, Procgen \citep{pmlr-v119-cobbe20a}, and CLIPort \citep{shridhar2021cliport}), (2) simulated experts with configurable competence levels, (3) standardized evaluation pipeline with well-defined performance metric, (4) clean, extendible implementations of popular baselines. 
The benchmark provides ready-to-use tools for tackling \ourMethod problems and enables the development of robust methods that generalize across diverse environments. It also supports comprehensive comparisons and analyses of each method's strengths and weaknesses. 
Moreover, its extensive collection of environments lays the foundation for future research on large-scale, multi-environment learning approaches.
% we implement $23$ baseline algorithms spanning logit-based heuristics, OOD detection, and learned policies through a unified Gym interface. 
% The framework supports controlled comparisons via shared task splits and pretrained novice/expert policies\footnote{Anonymized source code: \href{https://anonymous.4open.science/r/yield_request_control-E42F/README.md}{yield request control}}.

Utilizing \ourMethod-Bench, we develop solutions to the \ourMethod problem. 
A solution to this problem comprises a policy-proposing method, which generates candidate policies, and a policy validation method, which predicts the test performance of each candidate to select the best one for testing.
We introduce a novel policy validation method and conduct a large-scale empirical study to gain insights into the performance of various policy-proposing methods.
In total, we learn and evaluate more than $2600$ policies, comparing $23$ policy-proposing methods across $19$ environments.
Our results demonstrate the effectiveness of our validation approach and shed light on the behaviors of different policy-proposing methods.
Specifically, we find that: (1) no single method consistently outperforms others, (2) a substantial gap remains between the policies found by these methods and the best possible policies, and (3) the performance of these methods is not limited by our validation approach but by their reliance on a simple policy class.
We translate these insights into practical recommendations for future research\footnote{Benchmark is available at: \url{https://github.com/modanesh/YRC-Bench}.}.


% While prior work has explored mechanisms for requesting human assistance \citep{nguyen-daume-iii-2019-help, xie2022ask} and querying external knowledge \citep{liu2022asking, knowno2023}, our setting introduces unique challenges due to the unobservability of expert representations and the need for agents to adapt to both environmental and expert-induced distribution shifts.  By focusing on the fundamental problem of when to yield control, our framework provides a principled approach to optimizing performance while minimizing the cognitive burden on experts.

% Furthermore, our experiments reveal three critical findings. First, current coordination strategies exhibit significant room for improvement--simpler approaches (e.g., random intervention) match or outperform more complex ones such as OOD detection-based and logit-based in several scenarios, despite performance gap versus our reinforcement learning (RL) oracle which is trained with access to the expert policy and test environments. This suggests existing methods explore insufficiently rich policy spaces. 
% Second, we demonstrate that policies leveraging raw environmental observations during training achieve superior generalization compared to those relying solely on the novice's internal representations. 
% Third, our simulated validation framework (\autoref{sec:sim-val}), which trains coordination policies using proxy experts and limited novices, reduces the expert query cost compared to naive baselines while maintaining task success rates.




% Furthermore, our experiments reveal these critical findings: (1) Methods leveraging simulated validation collectively outperform their counterparts in the majority of test environments, demonstrating the effectiveness of our validation approach. (2) Simple coordination strategies, particularly random expert querying with optimized probability, can match or exceed more sophisticated approaches in multiple scenarios, challenging common assumptions about the necessity of complex coordination mechanisms. (3) Current approaches are primarily limited by their restricted policy space rather than validation accuracy, suggesting future research should focus on methods that can explore richer coordination strategies while maintaining computational efficiency. (4) When building coordination policies, raw observations consistently provide better learning signals than internal policy representations across most environments. (5) Traditional logit-based methods significantly outperform deep learning OOD detection approaches in over half of the evaluated environments, particularly in more complex domains like Procgen and CLIPort, indicating that simpler approaches often suffice for effective coordination.


% \khanh{i see that you have been trying to promote RL as a method. in that case, you must make it clear what setting you are talking about. if you are talking about the setting in the third paragraph (distribution shift setting), RL is an oracle because it has access to expert and test environments, which no method should have. if you want it to be a method, the setting must be life-long learning (i.e. constantly adapt in test environments).}

% \khanh{however, i think we should not emphasize RL as an effective method in the life-long learning setting here, because we lack baselines in that setting. let's just focus on the distribution-shift setting. we can run more experiments to study the life-long setting after the submission. i am not saying we remove the analysis on the RL features, just saying we don't necessarily need to mention it here to introduce new complications.}

% \khanh{so in summary, for findings i would merge first and third points into one, with first point being the main finding and third point being analysis providing deeper insights.}

% \khanh{also please add contributions of the simulated validation method. it is quite effective as the experiments show.}

In summary, our contributions are:
\begin{itemize}
\item We formalize the \ourMethod problem and introduce a challenging practical setting that captures key aspects of real-world scenarios; 
\item We provide the fundamental experimental methodology and infrastructure for developing and evaluating robust solutions to \ourMethod;
\item We propose a simple yet effective validation approach using simulated agents and demonstrate its efficacy across multiple environments;
\item We experimentally evaluate a wide range of policy proposal methods, uncovering novel insights that inform future research.
\end{itemize}

% Our contributions are threefold. First, we formalize the \ourMethod problem and propose a challenging practical setting that captures key aspects of real-world deployment, such as unobservable expert representations and substantial distribution shifts. Second, we develop a comprehensive experimental framework to evaluate existing methods and highlight their limitations. Our experiments reveal that current approaches achieve only a moderate fraction of the potential oracle performance.
% %, with methods using simulated validation generally outperforming alternatives in most tested environments. 
% Third, we propose a novel reinforcement learning (RL)-based approach for training the coordination strategy, which we show surpasses traditional methods by better generalizing from simulated to real-world settings.


