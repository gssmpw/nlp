\section{\ourMethod-Bench}


\begin{figure}[tbp]
\centering
    \begin{subfigure}{}
        \centering
        \includegraphics[width=0.3\linewidth]{figures/minigrid.png}
    \end{subfigure}
    \begin{subfigure}{}
        \centering
        \includegraphics[width=0.3\linewidth]{figures/coinrun.png}
    \end{subfigure}
    \begin{subfigure}{}
        \centering
        \includegraphics[width=0.3\linewidth]{figures/cliport.png}
    \end{subfigure}
    % \vspace{em}
    \begin{subfigure}{}
        \centering
        \includegraphics[width=0.3\linewidth]{figures/minigrid_hard.png}
    \end{subfigure}
    \begin{subfigure}{}
        \centering
        \includegraphics[width=0.3\linewidth]{figures/coinrun_hard.png}
    \end{subfigure}
    \begin{subfigure}{}
        \centering
        \includegraphics[width=0.3\linewidth]{figures/cliport_hard.png}
    \end{subfigure}
    \caption{Sample tasks integrated into \ourMethod, with training tasks ($\mathcal{E}_{\text{train}}$) on the top and test tasks ($\mathcal{E}_{\text{test}}$) on the bottom row. From left to right: DoorKey with different maze sizes from Minigrid, CoinRun with varying difficulty levels from Procgen, and stack-block-pyramid with diverse block colors from CLIPort.}
    \label{fig:envs}
\end{figure}


To advance research in learning \ourMethod, we introduce a comprehensive benchmark that provides the necessary infrastructure for evaluating coordination policies in a wide range of environments. 
% This benchmark is built around four core principles: simulation of experts, diverse environments, standardized baseline implementations, and extensibility, all while being easy to work with. 
The benchmark enables cost-effective, reproducible, and generalizable solution development, which is a serious concern in the current state of machine learning research \citep{kapoor2022leakage}. 

% \subsection{Core Principles}

\textbf{Diverse Environments.}  
Our benchmark spans multiple domains, allowing the evaluation of coordination policies across a broad spectrum of task complexity and diversity. 
It comprises MiniGrid, Procgen, and CLIPort, each offering unique coordination challenges. MiniGrid is a suite of grid-based navigation tasks ranging from simple key-door puzzles to dynamic obstacle courses, testing fundamental coordination in abstract state spaces where agents must balance autonomous navigation with expert interventions under partial observability \citep{MinigridMiniworld23}. Procgen is a procedurally generated video game suite featuring multiple task variations with stochastic dynamics, where pixel-based observations and unpredictable gameplay shifts stress-test adaptation to novel visual and mechanical challenges \citep{pmlr-v119-cobbe20a}. CLIPort is a language-guided robotic manipulation domain requiring spatial reasoning with RGB-D observations, demanding precise, sustained coordination simulating real-world robotic assistance scenarios \citep{shridhar2021cliport}. Collectively, these environments span: low-dimensional states (MiniGrid), high-dimensional pixels (Procgen), and multi-modal RGB-D with language inputs (CLIPort); and discrete navigation to continuous control tasks. 
In total, we study $19$ environments, $3$ from MiniGrid, $11$ from Procgen, and $5$ from CLIPort.

\textbf{Simulation of Experts.}  
The \ourMethod-Bench includes high-quality expert agents emulating real-world experts. For environments from MiniGrid and Procgen, we obtain these experts by training PPO policies on $\mathcal{E}_{\text{test}}$ until convergence, ensuring they represent competent (but non-human) policies. 
In the case of CLIPort, we use the already available task-specific rule-based oracle as the expert agent. Simulated experts enable researchers to perform evaluations at scale without incurring the costs, risks and complexities associated with deploying actual human operators or resource-intensive AI systems.




\textbf{Standardized Baseline Implementations.}  
Our benchmark provides implementations of competitive approaches, allowing users to use them to immediately tackle their \ourMethod problems or compare with their novel approaches. These baselines fall into three main families. First, \textit{logit-based methods} that use measures such as entropy, margin (difference between the highest and second-highest probabilities), or energy to decide whether to yield control to the expert. Second, \textit{OOD detection-based} approaches such as Deep SVDD \citep{pmlr-v80-ruff18a}, which detect anomalies in the input distribution to trigger expert intervention. And finally, \textit{RL-based} policies where coordination strategies are learned through RL \citep{sutton2018reinforcement, schulman2017proximal}. While our current implementation uses RL with full environment and expert access to establish oracle performance (\autoref{sec:rl-oracle}), the benchmark architecture supports training RL policies without such privileged access---a promising direction for future work.
% \khanh{did we define "RL" anywhere before this?} \mohamad{we have definition of the MDP but not RL as method to solve it.}



% \input{figures/env_wrapper}

% \input{figures/algorithm}


\textbf{Extensibility.}  
% Our benchmark is designed with extensibility in mind. Since the \texttt{HelpEnvironment} wrapper inherits the OpenAI Gym environment, adding new environments to study the \ourMethod problem requires minimal code changes. Researchers can easily integrate new environments to test the proposed methods in different settings. Additionally, the framework allows for the inclusion of new coordination policies. As long as these algorithms inherit the \texttt{Algorithm} class, they can be seamlessly incorporated into the benchmark by modifying the class to suit their specific requirements. These provide a robust foundation for assessing new methods, with clear definitions and reproducible implementations.
Our benchmark is designed for extensibility. The environment wrapper is built on the \texttt{gym3} interface\footnote{\url{https://github.com/openai/gym3}}, a high-performance API for RL environments that supports vectorized environments and efficient data handling. Unlike \texttt{gym}, which requires additional wrappers for vectorization, \texttt{gym3} natively supports vectorized environments, simplifying the implementation and improving performance. This design allows new environments to be seamlessly integrated into the benchmark, enabling researchers to study the \ourMethod problem with minimal code changes. By leveraging \texttt{gym3}, we ensure compatibility with a wide range of environments while maintaining high performance and scalability. Researchers can easily integrate new environments to test the proposed methods in different settings, and test existing methods on them with minimal code changes. Additionally, our modular code structure makes it easy to add new methods, especially those belonging to existing families of methods. The flexibility of \texttt{gym3} in handling custom methods, rendering, and environment management further enhances its utility for diverse research needs.
% They can be seamlessly incorporated into the benchmark to suit their specific requirements. 
% These provide a robust foundation for assessing new methods, with clear definitions and reproducible implementations.

Further details about the \ourMethod-Bench is available at \aref{app:imp_details}.

% \subsection{Coordination Environment Wrapper}

% To standardize coordination policy training and evaluation, we introduce the \texttt{HelpEnvironment} wrapper. This tool converts any Gym-compatible environment \citep{1606.01540, towers2024gymnasium} into a \textit{coordination MDP} that preserves the original state space $\mathcal{S}$ but replaces the action space with a binary choice $\{\texttt{n}, \texttt{e}\}$, representing the coordination policy's decision to request control (novice acts) or yield control (expert acts). At each timestep, the wrapper resolves the coordination policy $\mu$'s decision into a concrete environment action: if $\mu(s_t) = \texttt{n}$, the novice's proposed action $a^n_t \sim \pi_n(s_t)$ is executed; if $\mu(s_t) = \texttt{e}$, the expert's action $a^e_t \sim \pi_e(s_t)$ is used instead. The next state $s_{t+1}$ is generated by the base environment's transition dynamics $P(s_{t+1} | s_t, a_t)$, where $a_t \in \{a^n_t, a^e_t\}$.

% \autoref{fig:env_wrapper} illustrates this architecture. The \texttt{make\_help\_envs} utility initializes three environment variants: a training environment using the simulated novice $\tilde{\pi}_n$ and proxy expert $\pi_n$ under $\mathcal{E}_{\text{train}}$, a validation environment testing generalization on held-out tasks from $\mathcal{E}_{\text{train}}$, and a testing environment evaluating final performance with the true novice $\pi_n$, expert $\pi_e$, and $\mathcal{E}_{\text{test}}$. By abstracting coordination mechanics into a reusable wrapper, we enable cross-environment benchmarking while ensuring consistent policy interactions.

% The \texttt{HelpEnvironment} wrapper performs three essential tasks: \begin{itemize} 
%     \item \textbf{Policy Integration:} As shown in \autoref{fig:wrapper_demo} and \autoref{fig:env_wrapper}, the wrapper accepts both novice and expert policies, managing the handover of control between them dynamically. This process is central to evaluating and optimizing coordination strategies. \ben{Similar to my comments on the figures, this feels a bit odd. It's kind of like saying ``we promise we wrote good code''. Like if I were reviewing this paper, it would be my expectation that the code was implemented this way and disappointed otherwise, so it's not necessarily a ``positive'' contribution beyond what we've already said.}
%     \item \textbf{Cost Computation:} The wrapper incorporates cost functions that consider the environment's reward, switching costs, and expert labor costs. These cost components are crucial for realistic assessments of coordination trade-offs. 
%     \item \textbf{Evaluation Tools:} To facilitate robust research, the wrapper includes standardized metrics for evaluating coordination performance. These metrics include cumulative cost, task completion rates, and the frequency of expert interventions, ensuring comprehensive assessments. 
% \end{itemize}

% \autoref{fig:env_wrapper} highlights the modularity of this design, which enables researchers to seamlessly adapt the wrapper to new environments or agents. By leveraging this implementation, users can efficiently conduct experiments on coordination policy without significant modifications to existing environments.







% \subsection{Training Coordination Policy}

% The \ourMethod benchmark supports the training of coordination policies using the \texttt{Algorithm} class, a modular and extensible framework for implementing training routines. The \texttt{Algorithm} class encapsulates the core training logic, providing methods for initializing training parameters and managing the iterative process of policy improvement. An overview of the class implementation is shown in Figure \ref{fig:algorithm}. This class organizes the training flow, allowing researchers to define how policies should be updated based on interactions with the environment or datasets.

% The \texttt{train} method iterates through training cycles, calling the \texttt{train\_one\_iteration} function in each loop to refine the policy. At specified intervals, the method invokes the evaluator to assess the policyâ€™s performance on validation and test environments, providing important feedback and facilitating the saving of the best model. This iterative process, demonstrated in Figure \ref{fig:algorithm}, is central to the policy optimization.

% By following the structure outlined in Figure \ref{fig:algorithm}, researchers can easily integrate new algorithms into the benchmark. The modular design ensures that the training and evaluation pipeline is easily adaptable, promoting reproducibility and flexibility for different types of coordination policy experiments.

% \subsection{Evaluation Framework}

% To assess the performance of the coordination policy, the benchmark includes a comprehensive evaluation framework. The framework provides standardized tools for comparing algorithms across different domains and scenarios. Evaluation metrics include cumulative cost, task success rate, expert intervention frequency, and other domain-specific measures. These metrics are essential for understanding the trade-offs between autonomy and expert reliance. The evaluation framework ensures that comparisons between methods are fair, standardized, and meaningful. By leveraging the \texttt{Evaluator} class, researchers can focus on designing better algorithms without worrying about the complexities of evaluation logistics.






