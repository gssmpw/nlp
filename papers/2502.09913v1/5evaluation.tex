\section{Experiments}\label{case}

In this section, we introduce experimental setup, baseline algorithms, and evaluation metrics. The project code can be found here\footnote[1]{https://gitee.com/parallelsimlab/autos2earch}.

\subsection{Experimental Setup}
The source search activities are performed by a virtual robot within a simulated 2D environment measuring $20m\times20m$. The search area is divided into a $20\times20$ grid of cells. Each cell has a probability $P_o$ of containing an obstacle, with $P_o$ set to 0.75 to introduce a relatively high difficulty (more obstacles). This higher complexity is chosen because simpler environments (with fewer obstacles) do not require external assistance. In this study, we did not consider the specific types or shapes of obstacles. If a cell contains an obstacle, it is considered completely obstructed, meaning the robot cannot enter or traverse it.

\subsection{Baseline Algorithms}
As detailed in the published work~\cite{zhao2023leveraging} on human-collaborative source search, the baselines adopted in this study naturally follow from that setup. Baseline 1 employs the Infotaxis algorithm directly, while Baseline 2 incorporates our proposed automatic problem detection method, navigating the robot to a random location to escape problematic scenarios. For consistency, we adopt an aided control interaction model of human-AI collaboration in this comparative analysis.
Furthermore, we introduce Baseline 3, where the robot navigates to a randomly chosen direction from four possible options (mentioned in Section 4.2) upon detecting a problem. It is worth noting that both Baseline 2 and Baseline 3 represent state-of-the-art improvements over traditional source search algorithms.

\subsection{Evaluation Metrics}
In this study, we evaluate the effectiveness and efficiency of the source search process and its outcomes. Effectiveness is measured by the success rate, defined as the robot successfully locating the source within 400 steps (where a step represents one iteration of updating search states). If the robot fails to find the source within 400 steps, regardless of whether large models are involved, the task is considered unsuccessful. Efficiency is assessed by the number of steps the robot takes to find the source, with failed attempts excluded from the calculation. Additionally, we measure the execution time of large models per task to see whether they hold an advantage over human workers in time-sensitive tasks.


% \subsection{Procedures}


\section{Results}
In this section, we present the results of (1) an illustrative run, (2) the comparison study, and (3) the ablation study. 

\begin{figure}[htbp]
    \centering
    \includegraphics[width=.7\linewidth]{fig4.jpg}
    \caption{An illustrative run of the proposed framework at different time steps. (a) step=0; (b) step=63; (c) step=151; (d) step=203}
    \label{fig:Illus}
\end{figure}

\subsection{Illustrative Run} % (\textit{RQ1})} 

We conducted an experiment using one scenario from a set of 20 benchmark scenarios to illustrate a successful search process. The illustrative run of AutoS$^2$earch is shown in Fig.~\ref{fig:Illus}. The process includes the initiation of the search, the progression of the algorithm-driven search, the involvement of large models when a problem is detected, and ultimately resolving the issue to successfully locate the source. As we can see in Fig.~\ref{fig:Illus}(c), large models (both GPT-4o\footnote[2]{https://github.com/topics/gpt-4o}) are activated at search step=151. Based on the current visual inputs and the provided prompt for MLLMs, the language description of this scene is presented as follows. 

% Output of MLLM
\begin{center}
\begin{minipage}{\linewidth}
\begin{shaded}
% \centering
\textit{\textbf{Output of the MLLM}}

Region A:
Distance to dense area of small green dots: close;
Density of surrounding black exploration area: medium

Region B:
Distance to dense area of small green dots: medium;
Density of surrounding black exploration area: medium

Region C:
Distance to dense area of small green dots: medium;
Density of surrounding black exploration area: medium

Region D:
Distance to dense area of small green dots: far;
Density of surrounding black exploration area: low

\end{shaded}
\end{minipage}
\end{center}

Using the language descriptions of the four directional choices, the LLM initiates CoT reasoning, guided by the prompt for LLMs. The LLM's final choice is \textbf{A}, and its explanation for this selection is also provided. The decision-making process closely mirrors that of humans. 

To explore this further, we recruited three human raters to independently evaluate the LLM's reasoning process and compare the rationale behind the optimal choices made by the LLM and human intelligence. Our findings reveal that the LLM generally adheres strictly to a predefined priority order, whereas humans display an ability to flexibly adjust priorities based on the relative magnitudes of differences between the choices. For instance, when two regions are similarly distant from a dense cluster of green dots (indicating minimal differences in second-level priority), but the surrounding black area densities differ significantly (indicating greater differences in third-level priority), humans tend to prioritize the third-level criteria over the second. In contrast, the LLM struggle with such adaptive decision-making, even when additional rules are introduced.
% The total time taken by large models for the entire search was approximately 22 seconds, which is faster than the average execution time of the human-aided mode (over 30 seconds). 
% \sq{needs some analysis on AI explanations. If we want to make this part good, it is suggested to have ~3 human raters independently evaluate all the AI's reasoning processes. This depends on how much time we have before the deadline, otherwise we can choose some typical examples and analyze their qualities.}

% Explanation of LLM
\begin{center}
\begin{minipage}{\linewidth}
\begin{shaded}
% \centering
\textit{\textbf{Reasoning Process of the LLM}}

Based on the description and priority rules, the analysis is conducted to identify the highest-priority region that meets the criteria:

1.First, check if any regions are missing:
Regions A, B, C, and D all exist.

2.Next, consider the closest distance to the dense area of green dots:
Region A is "close", regions B and C are "medium", and region D is "far"; thus, region A has the highest priority.

3.Then, consider the density of unexplored black areas surrounding each region:
Regions A, B, and C have "medium" density, Region D has "low" density.

\textbf{According to the priority rules, the region with the highest priority is A. Therefore, my selection is: A.}
\end{shaded}
\end{minipage}
\end{center}

\subsection{Comparative Study} % (\textit{RQ2})}

In this subsection, we evaluated the effectiveness of large models-assisted source search over 20  benchmark scenarios (each scenario was run ten times) by measuring the effectiveness (success rate), the efficiency (the number of steps taken to find the source), the execution time of large models. The experimental results, averaged over Monte Carlo simulations across 20 scenarios, are presented in Table ~\ref{tab:results-1}. Clearly, large models-assisted source search has proved to be effective, achieving a success rate of 97\% in most cases. This represents an improvement of approximately 18.5\% over Baseline 1, 9\% over Baseline 2, and 7\% over Baseline 3, while being only 3\% lower than the success rate of human-AI collaborative search. Note that Baseline 2 and 3 are improvements based on the original algorithm (Baseline 1) since automatic problem detection and rule-based problem-solving strategies are used. Furthermore, we observe that the efficiency of AutoS$^2$earch (in terms of steps taken) is comparable to that of human-AI collaborative search, while the average execution time of large models is even shorter. For details on how human workers complete the crowdsourcing task, interested readers can refer to the previous work~\cite{zhao2022crowd}.

\begin{table}[!ht]
    \centering
    \caption{Results of the comparisons over various baselines.}
    \label{tab:results-1}
    \resizebox{0.9\textwidth}{!}{
    \begin{tabular}{llccc}
    \toprule
         \textbf{\emph{Methods}} & \textbf{Expertise} & \makecell[c]{\textbf{Effectiveness}\\(\% success rate)} & \makecell[c]{\textbf{Efficiency}\\(\# steps per task)} & \makecell[c]{\textbf{Human/MLLM+LLM execution time}\\(seconds per task)} \\
    \midrule
         \multirow{2}{*}{\emph{Human Aided}}
         & Expert   & 100  & 175.10 $\pm$ 67.67  & 33.58 $\pm$ 27.87\\
         & Non-expert  & 100  & 165.67 $\pm$ 80.60  & 29.01 $\pm$ 29.51\\
         \midrule
         \emph{Baseline 1} & -  & 78.5 & 154.04 $\pm$ 91.32  & - \\
         \midrule
         \emph{Baseline 2} & -  & 88  & 179.64 $\pm$ 96.45  & - \\
         \midrule
         \emph{Baseline 3} & -  & 90  & 179.76 $\pm$ 97.40  & - \\
         \midrule
         \emph{Ours} & -  & 97  & 170.97 $\pm$ 89.57  & 25.95 $\pm$ 38.20 \\
    \bottomrule
    \end{tabular}
    }
\end{table}

To further explore whether the impressive performance is solely due to GPT-4o's strong capabilities, we evaluated various combinations of MLLMs and LLMs from different companies. The results, presented in Table~\ref{tab:results-2}, reveal that our proposed framework is highly robust, consistently achieving success rates above 95\%. Notably, the Qwen model\footnote[3]{https://github.com/JMaiGC/ComfyUI-Qwen-VL-API} from the Chinese company Alibaba achieves the highest success rate at 98\%.

\vspace{-5mm}

\begin{table}[!ht]
    \centering
    \caption{Results of the comparisons over various large models.}
    \label{tab:results-2}
    \resizebox{0.9\textwidth}{!}{
    \begin{tabular}{llccc}
    \toprule
         \textbf{\emph{LLMs}} & \makecell[c]{\textbf{Effectiveness}\\(\% success rate)} & \makecell[c]{\textbf{Efficiency}\\(\# steps per task)} & \makecell[c]{\textbf{MLLM+LLM execution time}\\(seconds per task)} \\
    \midrule
         \emph{GLM-4v-plus + GLM-4-plus}  & 95 & 171.13 $\pm$ 92.64  & 26.39 $\pm$ 32.75 \\
         \midrule
         \emph{Qwen-VL-plus + Qwen-max}  & 98  & 172.85 $\pm$ 91.08  & 26.74 $\pm$ 36.39 \\
         \midrule
         \emph{GPT-4o + GPT-4o}  & 97  & 170.97 $\pm$ 89.57  & 25.95 $\pm$ 38.20 \\
    \bottomrule
    \end{tabular}
    }
\end{table}

\vspace{-5mm}

\subsection{Ablation Study}

We further ablation studies to validate the importance of main elements designed in our framework: the Chain-of-Thought prompt for the LLM and the size of directional choices A, B, C, and D (which determine the number of candidate cells for each option). We designated the model without CoT reasoning as Our-A and the model with reduced block sizes as Our-B. The average results across 20 scenarios are presented in Table \ref{tab:results-3}. As we can see, both the removal of CoT reasoning and the reduction in block sizes significantly decrease the success rate by approximately 6\% and 7\%, respectively. Notably, while removing CoT reasoning compromises the effectiveness performance, it does lead to improved efficiency and shorter execution time due to fewer reasoning steps.

\begin{table}[!ht]
    \centering
    \caption{Results of the ablation study.}
    \label{tab:results-3}
    \resizebox{0.8\textwidth}{!}{
    \begin{tabular}{llccc}
    \toprule
         \textbf{\emph{Methods}} & \makecell[c]{\textbf{Effectiveness}\\(\% success rate)} & \makecell[c]{\textbf{Efficiency}\\(\# steps per task)} & \makecell[c]{\textbf{MLLM+LLM execution time}\\(seconds per task)} \\
    \midrule
         \emph{Ours-A}  & 91 & 157.74 $\pm$ 85.33  & 23.82 $\pm$ 36.45 \\
         \midrule
         \emph{Ours-B}  & 90  & 170.28 $\pm$ 93.35  & 24.49 $\pm$ 34.19 \\
         \midrule
         \emph{Ours}  & 97  & 170.97 $\pm$ 89.57  & 25.95 $\pm$ 38.20 \\
    \bottomrule
    \end{tabular}
    }
\end{table}











