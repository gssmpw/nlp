\section{Discussions}

The source search results convey three main messages: (1) By incorporating carefully designed prompts that enable large language models with scene comprehension and multi-step reasoning capabilities, autonomous source search capabilities can be integrated into web-based systems to support decision-making in time-sensitive scenarios. (2) The large models-assisted method is effective and efficient for improving source search, approaching the performance of human-AI collaborative approaches while reducing execution time by approximately 25\%. (3) Whether in scene element presentation, problem detection mechanisms, or CoT prompt design, each component reflects human intelligence, highlighting that complex task solving fundamentally relies on human-AI hybrid intelligence.

\noindent\textbf{\textit{Drawbacks.}} Despite the strengths, this work has several limitations. (1) \textit{Environmental Complexity Gap:} The simplified $20 \times 20$ grid with static obstacles fail to capture real-world dynamics (e.g., moving obstructions, multi-source scenarios). The visual environment used here is insufficient to test whether large models truly possess robust scene understanding and multi-step reasoning capabilities in complex settings. (2) \textit{Limited Task Understanding:} While simple scene elements were designed to help the large model understand tasks, the lack of domain-specific knowledge makes it difficult for the model to balance exploration and exploitation during the search, sometimes leading to hallucinations by selecting irrelevant areas. (3) \textit{Underutilization of MLLM Potential}: In this work, MLLMs were mainly used to convert visual observations into textual descriptions, with large language models handling subsequent reasoning. This separation of visual understanding and language reasoning may limit the integrated capabilities MLLMs are designed to offer.

\noindent\textbf{\textit{Potential Avenues.}} To address these limitations, we propose to explore: (1) \textit{Dynamic Environment Adaptation:} Design LLM-empowered search agent and develop online prompt tuning mechanisms where LLMs could adjust decision rules according to the environment variations. (2) \textit{Visual Thinking Augmentation:} Integrate graph-based scene representations and reflection mechanisms to help MLLMs directly reason on the visual inputs without hallucinations. (3) \textit{Human-AI Value Alignment:} Implement human-in-the-loop feedback mechanisms in complex and high-risk scenarios and ensure alignment of decision objectives between humans and AI.

\noindent\textbf{\textit{Implications.}} The implications of AutoS$^2$earch extend far beyond the technical achievements in web-based autonomous systems. Its design reflects a broader trend in human-AI collaborative systems, where the goal is to harness the cognitive strengths of both entities in tandem.  Moreover, it may redefine the role of humans in web crowdsourcing systemsâ€”from task executors to validators of AI rationality in the future. 


