\section{Conclusions}

In this work, we present AutoS$^2$earch to address the issue of human dependency in web-based crowdsourcing systems for source search tasks. Through AutoS$^2$earch, we demonstrate that large models can effectively improve the performance of human-designed search algorithms in complex environments through visual-language translation and CoT reasoning. Our experimental validation shows AutoS$^2$earch achieves 95-98\% of human-AI collaborative source search algorithm effectiveness while eliminating labor costs and response time. \textit{This implies that modern large models can sufficiently replicate human scene reasoning for critical tasks like source search in complex environments.} As global industries increasingly lean on such systems for effective management, our work establishes a solid foundation for web engineering in other industrial applications.

% Our primary contributions include: 1) The first systematic integration of MLLMs with web technology for autonomous source search, eliminating human intervention requirements; 2) A novel chain-of-thought prompting architecture that enables zero-shot scene reasoning through linguistic state representation; 3) Comprehensive empirical validation showing 95-98\% success rate approaching the performance of human-AI hybrid systems across 20 benchmark scenarios. 


% Technical Contributions and Performance Insights
% Our experiments reveal that MLLMs can achieve 97–98% success rates in resolving search deadlocks, closely matching human-AI collaborative approaches (100%) while reducing execution time by ~25%. This validates three key innovations:
% Hierarchical Scene Encoding: By projecting critical visual elements (e.g., posterior probability distributions, passable areas) into a simplified web interface, we enable MLLMs to bypass the "sensory overload" typical of raw environmental data.
% Chain-of-Thought (CoT) Anchoring: The structured prompts for MLLMs and LLMs mimic expert human reasoning—prioritizing proximity to high-probability regions (Rule 2) and exploration potential (Rule 3). This reduces hallucination risks compared to open-ended reasoning.
% Real-Time Hybrid Workflow: Migrating the system to a web platform with Flask and WebSocket ensures sub-second latency between problem detection and LLM-guided resolution, critical for time-sensitive scenarios like gas leak mitigation.

