\section{System Design}
%说明web-based系统开发设计
%\sq{To answer \textbf{RQ1}, we designed the AutoS$^2$earch framework based on web platforms.} To support the AutoS$^2$earch framework, we developed a prototype system utilizing advanced web technologies. 
To answer \textbf{\textit{RQ1}}, we designed the AutoS$^2$earch framework based on web platforms. This involved migrating our previously developed crowd-powered source search prototype system~\cite{zhao2022crowd} from a desktop application to a web platform. The primary goals of this implementation were to achieve cross-platform accessibility, real-time interaction, and dynamic visualization.

(1) \textbf{Back-End Implementation:} we selected the lightweight and scalable Flask framework, and initialized the application using Flask and Socket.IO. The functions are handled by defining routes and Socket.IO events.

% % design 1
% \begin{center}
% \begin{minipage}{0.92\linewidth}
% \begin{shaded}
% % \centering
% \textit{Python:}
% app = Flask(name),
% socketio = SocketIO(app)
% \end{shaded}
% \end{minipage}
% \end{center}

(2) \textbf{Real-Time Communication:} Socket.IO was used to support WebSocket and polling to ensure low-latency communication. Data is sent to the front-end using `socketio.emit', and on the front-end, events sent by the back-end are received using `socket.on'.

(3) \textbf{Map Drawing and Updating:} we first determined the map drawing logic (initializing the map and updating it based on changes), and then converted the map data into a format recognizable by the front-end.

(4) \textbf{Front-End Rendering:} we utilized HTML5's Canvas -- an ideal choice for dynamic map displays—to achieve efficient graphic rendering. We defined the Canvas element and implemented the drawing logic using JavaScript. To facilitate user interaction, we added control buttons on the interface and bound click events to them. Additionally, a click event is bound to the Canvas to send the user's click coordinates to the back-end.

% % design 2
% \begin{center}
% \begin{minipage}{0.92\linewidth}
% \begin{shaded}
% % \centering
% \textit{HTML:}
% <canvas id="maze-canvas"></canvas>
% \end{shaded}
% \end{minipage}
% \end{center}

The user interface of this system, shown in Fig.~\ref{fig:system}, uses graphical elements to illustrate the source search task and the problem. It displays the robotic searcher, search environment, current state, posterior probability distribution of the source location, and four directional choices. When a problem is detected, the system automatically generates a task for large models, and then large models analyze the scene and reason step by step to plan a path for the robot. Once a deadlock is resolved, the system resumes automatic search, continuing until the source is found. This user interface allows decision-makers to observe the source search process in real-time. It provides the capability to: 1) enable decision-makers to interrupt the search process as needed, 2) facilitate crowdsourcing during the search, and 3) integrate large models for handling the detected problems.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{fig1.jpg}
    \caption{A screenshot of the web-based source search prototype system.}
    \label{fig:system}
\end{figure}




