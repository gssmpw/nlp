\section{Related Work}
\subsection{Biomedical Instruction-Tuning Datasets}
\label{dataset}
Many Med-VQA datasets such as VQA-RAD~\cite{lau2018dataset} and SLAKE~\cite{liu2021slake} contain only medical images and simple QA pairs. Specifically, these QA pairs do not have the fine-grained descriptions of pathological features. This limitation constrains the ability of MLLMs to perform accurate spatial localization. To boost and evaluate MLLMs' capabilities in the Med-VQA domain, many biomedical instruction-tuning datasets are designed.

For example, the LLaVA-Med-qa0.2k~\cite{zhang2025biomedclipmultimodalbiomedicalfoundation} contains 50 medical images sampled from several modalities such as CT, Angiography and PET and 193 questions according to these medical images. The medical QA pairs are generated by GPT-4 based on the metadata. This dataset is designed to evaluate the capabilities of describing medical images, specifically the biomedical features.

Unlike the LLaVA-Med-qa0.2k~\cite{zhang2025biomedclipmultimodalbiomedicalfoundation}, the Med-GRIT-270k~\cite{huang2024BiRD} includes multi-task fine-grained medical QA pairs especially for grounding spatial locations. It is designed to enhance the model's capabilities in referring and grounding. This multi-task dataset comprises four distinct tasks: Visual Grounding (VG), Referring Object Classification (ROC), Referring Captioning (RC), and Medical Image Analysis (MIA). The VG task evaluates the model's capability of accurately matching text descriptions to corresponding image regions. The RC task assesses the model's capability to recognize specific image areas and generate descriptive captions for them. The ROC task examines the model's understanding of textual information related to particular image regions and their associated visual details. The MIA task evaluates the model's comprehension of medical images and their multi-modal context. All data is sourced from eight modalities: CT, MR, X-ray, PET, Endoscopy, Dermoscopy, Fundus, and Ultrasound. Utilizing data from these eight modalities allows for a more comprehensive evaluation of the model's capabilities.
\subsection{Multimodal Large Language Models (MLLMs) for Biomedicine}
With the development of MLLMs, an increasing number of approaches have been applied to the medical field, such as BioMedGPT~\cite{Zhang_2024}, LLaVA-Med~\cite{li2024llava}, and BiRD~\cite{huang2024BiRD}. These methods drive the development of MLLMs in the biomedical domain from different perspectives. For instance, LLaVA-Med~\cite{li2024llava} applied instruction fine-tuning to the medical imaging field and built a medical question-answering system based on LLaVA~\cite{llava}, which provided reasonable responses by processing user-submitted images and instructions. However, LLaVA-Med did not support fine-grained interactions with medical images, meaning that the system cannot precisely locate pathological regions within the images. To address the lack of referring and grounding capabilities, the BiRD~\cite{huang2024BiRD}, which is based on Qwen-VL~\cite{bai2023qwenvlversatilevisionlanguagemodel}, leveraged a finer-grained medical dataset. However, the ViT of BiRD was not trained by sufficient medical data before being frozen, resulting in a gap in medical knowledge domain. Compared with their work~\cite{huang2024BiRD, li2024llava}, our method modifies the position embedding method and introduces an effective pseudo-labels medical knowledge distillation framework.
% \begin{figure*}[htbp!]
% \centering
% \includegraphics[width=\textwidth]{ICCV2025-Author-Kit-Feb/figure/framework.pdf} % Reduce the figure size so that it is slightly narrower than the column.
% \caption{\textbf{Overview of our ClinKD's framwork}}
% \label{fig:framwork}
% \end{figure*}
\subsection{Rotary Position Embedding (RoPE)}
The introduced RoPE~\cite{su2023roformerenhancedtransformerrotary} makes the relative position features of the context in the text more easily captured by LLMs. Further, the RoPE-Mixed~\cite{heo2024rotarypositionembeddingvision} introduced mixed learnable frequencies to RoPE, enabling it to handle diagonal directions and making RoPE itself learnable. It showed more efficiency when dealing with diagonal features of the image. The VideoRoPE~\cite{wei2025videoropemakesgoodvideo} extended the diagonal layout and variable frequency to the video domain, demonstrating its advantages in video analysis tasks. Unlike existing methods, our approach focuses on 2D multi-task medical images where capturing semantic information is challenging. We utilize cross-modal intervals to enable the model to better distinguish modal information, thereby improving image-text alignment.

