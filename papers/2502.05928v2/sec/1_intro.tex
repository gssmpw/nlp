\section{Introduction}
% \begin{figure*}[h!]
% \centering
% \includegraphics[width=\textwidth]{ICCV2025-Author-Kit-Feb/figure/framework.pdf} % Reduce the figure size so that it is slightly narrower than the column.
% \caption{Overview of our ClinKD's framework: The framework begins with multi-task medical images being segmented into patches. The med-extractor module with Med-CLIP Guided Rotary Position Embedding extracts dimensional features from the patched image embeddings. The ClinKD system undergoes distillation before supervised fine-tuning (SFT). During the SFT, answers with low semantic similarity or incorrect groundings are prioritized for retraining. When responding to queries, the framework selects the answer with the highest score, ensuring accurate medical image analysis and interpretation.}
% \label{fig:framework}
% \end{figure*}
Medical Visual Question Answering (Med-VQA)~\cite{Lin_2023,Abacha2019VQAMedOO, Hasan2018OverviewOI} is a critical domain for applying MLLMs~\cite{yang2024modelmergingllmsmllms, liang2024comprehensivesurveyguidemultimodal, caffagni-etal-2024-revolution} to medical image analysis~\cite{li2024towards}. Recent advancements in MLLMs have enabled them to achieve preliminary capabilities in image analysis, annotation, and user instruction compliance~\cite{chen2025rllavaimprovingmedvqaunderstanding}. Previous studies have made multiple attempts~\cite{yang2025llmmedqaenhancingmedicalquestion,  kumar2024medvisionllamaleveragingpretrainedlarge, he2024parameterefficientfinetuningmedicalmultimodal,liu2024hcllmhistoricalconstrainedlargelanguage,li2024llava, huang2024BiRD} to enhance the performance of general-purpose MLLMs in Med-VQA. 

For instance, LLaVA-Med~\cite{li2024llava} extends instruction-tuning techniques for MLLMs to the medical domain. This work applied instruction tuning and curated image-text datasets to align visual-textual features in LLaVA~\cite{llava}, enabling LLaVA-Med to recognize and analyze medical images. However, this system lacked fine-grained image understanding, causing erroneous spatial localization when questioned about specific pathological locations~\cite{gai2024enhancing, huang2024BiRD}. 

Subsequently, the BiRD~\cite{huang2024BiRD}, a model that incorporated fine-grained image recognition by developing a biomedical refer-and-ground instruction-tuning dataset, addresses the absence of grounding capabilities~\cite{you2023ferretrefergroundgranularity, zhang2024ferretv2improvedbaselinereferring} in LLaVA-Med's work~\cite{li2024llava}. Nevertheless, as shown in Figure~\ref{fig:compare}, due to its reliance on traditional supervised fine-tuning (instruction-tuning)~\cite{sft1, sft2, sft3} and conventional 2D Rotary Position Embedding (RoPE)~\cite{su2023roformerenhancedtransformerrotary}, BiRD tends to generate erroneous spatial localization and misinterpretation of medical images because of incomplete image-text alignment~\cite{alignment1, alignment2}. Moreover, the visual encoder of BiRD is constrained by the lack of medical knowledge, resulting in difficulties in medical knowledge adaptation~\cite{adaptation, lin2025healthgptmedicallargevisionlanguage}.

% Although many proposed methods~\cite{yang2025llmmedqaenhancingmedicalquestion, yan-etal-2024-multi, liu2024medcotmedicalchainthought,kumar2024medvisionllamaleveragingpretrainedlarge, he2024parameterefficientfinetuningmedicalmultimodal} have achieved great improvements in general Med-VQA, most of their studies still lack multi-task fine-grained Med-VQA data. Compared with the general Med-VQA tasks, the multi-task fine-grained Med-VQA not only requires the model to accurately recognize and describe medical images, but also demands that the model can correctly classify and grounding the pathological locations. In contrast to natural images, medical images inherently exhibit limited fine-grained semantic information, which constrains the ability of MLLMs to adapt to medical knowledge. Meanwhile, the image-text alignment also affects the model's ability to learn and adapt to cross-modal medical data. If the model does not achieve complete image-text alignment, it will generate erroneous spatial localization and misinterpretation of medical images.

To overcome these limitations, we propose a medical distillation framework named ClinKD to boost capabilities in the Med-VQA domain. Specifically, our method mainly focuses on two issues: incomplete image-text alignment and the professional medical knowledge gap between the general-purpose MLLMs and medical domain that requires finer-grained medical knowledge~\cite{medical_fine-grained, zeng2024visualorientedfinegrainedknowledgeediting}.

For the first issue, we propose Med-CLIP Guided Rotary Position Embedding (MCG-RoPE), a novel position embedding method that focuses on the inter-modal and intra-modal intervals during joint image-text training, while ensuring the equivalence of context intervals immediately adjacent to images. Compared to traditional RoPE~\cite{su2023roformerenhancedtransformerrotary}, the MCG-RoPE uses the distinct index intervals to better capture cross-modal information, thereby achieving more effective image-text alignment.

For the second issue, we propose \textbf{Clin}ical \textbf{K}nowledge \textbf{D}istiller (\textbf{ClinKD}) which consists of Pseudo-Labels Medical Knowledge Distillation and Reflective Correction Training. Pseudo-Labels Medical Knowledge Distillation provides prior medical knowledge by using pseudo-labels~\cite{pseudolabels, chen2023mixedpseudolabelssemisupervised}, filling the gap of prior medical knowledge so that the model can better adapt to medical knowledge during the supervised fine-tuning. Reflective Correction Training allows samples with low semantic similarity to be sent for training again after being enhanced by MLLMs.

Our contributions are summarized as follows:  
\begin{itemize}
    \item We introduce the ClinKD, an innovative distiller designed to establish more effective medical knowledge adaptation mechanisms.
    \item For solving incomplete
image-text alignment, we propose the Med-CLIP Guided Rotary Position Embedding (MCG-RoPE). The MCG-RoPE utilizes the distinct inter-modal and intra-modal features to improve image-text alignment.
    \item For bridging medical knowledge gap
between general-purpose MLLMs and specialized medical
applications, we propose the Pseudo-Labels Medical Knowledge Distillation, filling the medical knowledge gap before supervised fine-tuning. 
    \item Extensive experimental results show that the proposed ClinKD achieves the best performance on Med-GRIT-270k~\cite{huang2024BiRD}.
\end{itemize}

% \begin{figure*}[h]
% \centering
% \includegraphics[width=0.9\textwidth]{ICCV2025-Author-Kit-Feb/figure/MRoPE.pdf} % Reduce the figure size so that it is slightly narrower than the column.

% \caption{The visualized example of Med-CLIP Guided Rotary Position Embedding. The original vector will be rotated by different angles according to the transformed position indexes. The inter-modal and intra-modal intervals are different so that the Med-CLIP Guided Rotary Position Embedding can better align image feature and text feature.}
% \label{fig:3}
% \end{figure*}
\begin{figure*}[h!]
\centering
\includegraphics[width=\textwidth]{figure/framework.pdf} % Reduce the figure size so that it is slightly narrower than the column.
\caption{Overview of our ClinKD's framework: The framework begins with multi-task medical images being segmented into patches. The med-extractor module with Med-CLIP Guided Rotary Position Embedding extracts dimensional features from the patched image embeddings. The ClinKD system undergoes distillation before supervised fine-tuning (SFT). During the SFT, answers with low semantic similarity or incorrect groundings are prioritized for retraining. When responding to queries, the framework selects the answer with the highest score, ensuring accurate medical image analysis and interpretation.}
\label{fig:framework}
\end{figure*}
