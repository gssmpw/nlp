\section{Conclusion}
% \vspace{-1pt}
In this work, we propose ClinKD, a novel medical distiller centered on Pseudo-Labels Medical Knowledge Distillation and Reflective Correction Training. In addition, we propose the Med-CLIP Guided RoPE to improve image-text alignment. They are designed to enhance the performance on fine-grained multi-task datasets in Med-VQA domain. Extensive experiments demonstrate that our Med-CLIP Guided RoPE achieves superior image-text alignment, while the Pseudo-Labels Medical Knowledge Distillation effectively bridges prior medical knowledge gaps. These mechanisms synergistically enhance the model's medical knowledge adaptation capabilities. 

In future work, further refinements to the distillation process, along with the incorporation of more comprehensive medical knowledge and higher-quality datasets, could significantly improve model's robustness and reduce the model's reliance on manually labeled data.