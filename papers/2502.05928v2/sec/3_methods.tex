\section{Methodology}
\begin{figure*}[h]
\centering
\includegraphics[width=0.9\textwidth]{figure/MRoPE.pdf} % Reduce the figure size so that it is slightly narrower than the column.

\caption{The visualized example of Med-CLIP Guided Rotary Position Embedding. The original vector will be rotated by different angles according to the transformed position indexes. The inter-modal and intra-modal intervals are different so that the Med-CLIP Guided Rotary Position Embedding can better align image feature and text feature.}
\label{fig:3}
\end{figure*}
As shown in Figure~\ref{fig:framework}, we aim at enhancing the ClinKD's performance by mainly leveraging Med-CLIP Guided Rotary Position Embedding, Pseudo-Labels Medical Knowledge Distillation, Reflective Correction Training and Semantic-Aware Selective Generation. The proposed Med-CLIP Guided Rotary Position uses the distinct inter-modal and intra-modal space to improve image-text alignment. The distiller provides prior medical knowledge to enhance the model's capability of medical knowledge adaptation. In the inference procedure, the Semantic-Aware Selective Generation will allow the model to choose the answer with the best score.
\subsection{Med-CLIP Guided Rotary Position Embedding}
% \begin{figure*}[h]
% \centering
% \includegraphics[width=0.9\textwidth]{ICCV2025-Author-Kit-Feb/figure/MRoPE.pdf} % Reduce the figure size so that it is slightly narrower than the column.

% \caption{The visualized example of Med-CLIP Guided Rotary Position Embedding. The original vector will be rotated by different angles according to the transformed position indexes. The inter-modal and intra-modal intervals are different so that the Med-CLIP Guided Rotary Position Embedding can better align image feature and text feature.}
% \label{fig:3}
% \end{figure*}

The 2D Rotary Position Embedding (RoPE)~\cite{su2023roformerenhancedtransformerrotary} tends to lack attention on cross-modal features, so we propose the Med-CLIP Guided Rotary Position Embedding (MCG-RoPE) to modify rotation angles that are crucial for image-text alignment.
Figure~\ref{fig:3} shows the example of our method.\\
\textbf{Distinct Inter-Modal Intervals.}
Suppose we have a sentence-image-sentence position index sequence:
\begin{equation}
\begin{aligned}
S_{idx} = (&T_1, T_2\cdots, T_\ell, V_{\ell+1}, V_{\ell+2}, \cdots, \\ 
&V_{\ell+wh}, T_{\ell+wh+1},T_{\ell + wh +2}, \cdots, T_{\ell+wh+k})
\end{aligned}
\end{equation}
where $T_i = (t_i, t_i)$ denotes the 2D position index of text tokens, $V_i = (v^{(1)}_{i}, v^{(2)}_{i})$ denotes the 2D image index.


To distinguish between intra-modal features and inter-modal features, the indexes of image tokens need to be transformed.
\begin{equation}
\left( v_i^{(1)}, v_i^{(2)} \right) \xrightarrow{\mathcal F} \left( \alpha_1 v_i^{(1)} + \lambda_1, \alpha_2 v_i^{(2)} + \lambda_2 \right) = \mathcal V_i
\end{equation}
where $\mathcal{V}_{i}$ is the position after being scaled and shifted. The $\lambda_1,\lambda_2, \alpha_1, \alpha_2$ are parameters. The $\mathcal{F}$ denotes the operator of shifting and scaling.

The inter-modal intervals should be consistent, and thus the following equation needs to be satisfied:
\begin{equation}
\begin{cases}
    \alpha_{1} + \lambda_{1}-\ell=\ell + wh+1 - (\alpha_{1}h + \lambda_{1}) \\
    \alpha_{2} + \lambda_{2}-\ell=\ell + wh+1 - (\alpha_{2}w + \lambda_{2})
\end{cases}
\end{equation}

We need to fix $\alpha_1,\alpha_2$ first to set the intervals between each image's position index. So the general solution is 
\begin{equation}
    \begin{cases}
\lambda_{1}=\ell + \frac{[wh + 1-\alpha_{1}(h+1)]}{2} \\
\lambda_{2}=\ell + \frac{[wh + 1-\alpha_{2}(w+1)]}{2}
\end{cases}
\end{equation}

After setting intervals, the new sequence can be expressed as:
\begin{equation}
    \begin{aligned}
S_{h-idx} = (&t_1, \cdots, t_\ell, \alpha_1v^{(1)}_{\ell+1}+\lambda_1,  \cdots, \\ 
&\alpha_1v^{(1)}_{\ell+wh}+\lambda_1, , t_{\ell+wh+1}, \cdots, t_{\ell+wh+k})
\end{aligned}
\end{equation}
where $S_{h-idx}$ is the position index on height dimension,
\begin{equation}
% \setlength{\abovedisplayskip}{2pt}
% \setlength{\belowdisplayskip}{2pt}
 \begin{aligned}
S_{w-idx} = (&t_1, \cdots, t_\ell, \alpha_2v^{(2)}_{\ell+1}+\lambda_2,  \cdots, \\ 
&\alpha_2v^{(2)}_{\ell+wh}+\lambda_2, , t_{\ell+wh+1}, \cdots, t_{\ell+wh+k})
\end{aligned}
\end{equation}
where $S_{w-idx}$ is the position index on width dimension.\\
\textbf{Distinct Interval-Based Rotation.}
Suppose we have a query/key vector: $q \in \mathbb{R}^{d_h+d_w}$ where $\mathbb{R}^{d_h+d_w}$ denotes the $d_h+d_w$ dimension Euclidean space. The $d_h$, $d_w$ denotes the embedding dimension on height dimension and width dimension respectively.
Then $q$ will be split into $q^{(h)} \in \mathbb{R}^{d_h}$ and $q^{(w)} \in \mathbb{R}^{d_w}$

The corresponding frequency on the two dimensions is set as: 
\vspace{-2pt}
\begin{equation}
% \setlength{\abovedisplayskip}{3pt}
% \setlength{\belowdisplayskip}{2pt}
   \begin{aligned}
    \omega_{i}^{(h)}=10000^{-\frac{2i}{d_h}}\\
    \omega_{i}^{(w)}=10000^{-\frac{2i}{d_w}}
\end{aligned} 
\end{equation}

Then the angles of rotation are expressed as:
\begin{equation}
\setlength{\abovedisplayskip}{1pt}
\setlength{\belowdisplayskip}{1pt}
\begin{aligned}
\theta_{x_h,i}^{(h)}=x_h\cdot\omega_i^{(h)}\\
\theta_{x_w,i}^{(w)}=x_w\cdot\omega_i^{(w)}
\end{aligned}
\end{equation}
where $x_h$, $x_w$ denote the transformed position index on height and width dimension respectively.
This means the angle of rotation will be determined by inter-modal and intra-modal intervals. A large rotation angle brings information of modal transformation while a smaller angle contains fine-grained features of intra-modal.
The rotation matrix can be defined as:
\begin{equation}
\begin{aligned}
    \boldsymbol{R}_{\boldsymbol{\Theta},x_h}^{d_h}=\begin{pmatrix}R_{\theta_{x_h,1}^{(h)}} &  &   \\  & \ddots &   \\    &  & R_{\theta_{x_h,d_h/2}^{(h)}} \end{pmatrix}\\
     \boldsymbol{R}_{\boldsymbol{\Theta},x_w}^{d_w}=\begin{pmatrix}R_{\theta_{x_w,1}^{(w)}} &  &   \\  & \ddots &   \\    &  & R_{\theta_{x_w,d_w/2}^{(w)}} \end{pmatrix}
\end{aligned}
\end{equation}
where $R_{\theta_{x_h,i}^{(h)}}$ and $R_{\theta_{x_w,i}^{(w)}}$ are defined as:
\begin{equation}
\begin{aligned}
R_{\theta_{x_w,i}^{(w)}}=\begin{pmatrix}
{\operatorname{cos}\theta_{x_w,i}^{(w)}} & {-\operatorname{sin}\theta_{x_w,i}^{(w)}} \\
{\operatorname{sin}\theta_{x_w,i}^{(w)}} & {\operatorname{cos}\theta_{x_w,i}^{(w)}} \\
\end{pmatrix}
\\
R_{\theta_{x_h,i}^{(h)}}=\begin{pmatrix}
{\operatorname{cos}\theta_{x_h,i}^{(h)}} & {-\operatorname{sin}\theta_{x_h,i}^{(h)}} \\
{\operatorname{sin}\theta_{x_h,i}^{(h)}} & {\operatorname{cos}\theta_{x_h,i}^{(h)}} \\
\end{pmatrix}
\end{aligned}
\end{equation}

% \begin{equation}
% \begin{aligned}
% \boldsymbol{R}_{\boldsymbol{\Theta},h}^{d_h} \boldsymbol{q^{(h)}} &= 
% \begin{pmatrix}
% q_1 \\ q_2 \\ q_3 \\ q_4 \\ \vdots \\ q_{d_h-1} \\ q_{d_h}
% \end{pmatrix}
% \otimes
% \begin{pmatrix}
% \cos \theta^{(h)}_1 \\ \cos \theta^{(h)}_1 \\ \cos \theta^{(h)}_2  \\cos \theta^{(h)}_2 \\ \vdots  \\cos \theta^{(h)}_{d_h/2} \\ \cos \theta^{(h)}_{d_h/2}
% \end{pmatrix}\\
% &+
% \begin{pmatrix}
% -{q}_2 \\ {q}_1 \\ -{q}_4 \\ {q}_3 \\ \vdots \\ -{q}_{d_h-1} \\ {q}_{d_h}
% \end{pmatrix}
% \otimes
% \begin{pmatrix}
% \sin \theta^{(h)}_1 \\ \sin \theta^{(h)}_1 \\ \sin \theta^{(h)}_2 \\ \sin \theta^{(h)}_2 \\ \vdots \\ \sin \theta^{(h)}_{d_h/2} \\ \sin \theta^{(h)}_{d_h/2}
% \end{pmatrix}
% \end{aligned}
% \end{equation}
% where $\boldsymbol{R}_{\boldsymbol{\Theta},h}^{d_h}$ is the rotation matrix.
% Similarly, Med-CLIP Guided RoPE on the width dimension is given by:
% \begin{equation}
% \begin{aligned}
% \boldsymbol{R}_{\boldsymbol{\Theta},h}^{d_w} \boldsymbol{q^{(w)}} &= 
% \begin{pmatrix}
% q_1 \\ q_2 \\ q_3 \\ q_4 \\ \vdots \\ q_{d_h-1} \\ q_{d_h}
% \end{pmatrix}
% \otimes
% \begin{pmatrix}
% \cos \theta^{(w)}_1 \\ \cos \theta^{(w)}_1 \\ \cos \theta^{(w)}_2  \\cos \theta^{(w)}_2 \\ \vdots  \\cos \theta^{(w)}_{d_w/2} \\ \cos \theta^{(w)}_{d_w/2}
% \end{pmatrix}\\
% &+
% \begin{pmatrix}
% -{q}_2 \\ {q}_1 \\ -{q}_4 \\ {q}_3 \\ \vdots \\ -{q}_{d_w-1} \\ {q}_{d_w}
% \end{pmatrix}
% \otimes
% \begin{pmatrix}
% \sin \theta^{(w)}_1 \\ \sin \theta^{(w)}_1 \\ \sin \theta^{(w)}_2 \\ \sin \theta^{(w)}_2 \\ \vdots \\ \sin \theta^{(w)}_{d_w/2} \\ \sin \theta^{(w)}_{d_w/2}
% \end{pmatrix}
% \end{aligned}
% \end{equation}

% \begin{equation}
%     \begin{aligned}
%         \boldsymbol{R}_{\boldsymbol{\Theta},x_h}^{(d_h)}q^{(h)} = \begin{pmatrix}R_{\theta_{x_h,1}^{(h)}} &  &   \\  & \ddots &   \\    &  & R_{\theta_{x_h,d_h/2}^{(h)}} \end{pmatrix}
%         \begin{pmatrix}
% q^{(h)}_1 \\ q^{(h)}_2 \\ q^{(h)}_3 \\ q^{(h)}_4 \\ \vdots \\ q^{(h)}_{d_h-1} \\ q^{(h)}_{d_h}
% \end{pmatrix}\\
% \boldsymbol{R}_{\boldsymbol{\Theta},x_w}^{(d_w)}q^{(w)} = \begin{pmatrix}R_{\theta_{x_w,1}^{(w)}} &  &   \\  & \ddots &   \\    &  & R_{\theta_{x_w,d_w/2}^{(w)}} \end{pmatrix}
%         \begin{pmatrix}
% q^{(w)}_1 \\ q^{(w)}_2 \\ q^{(w)}_3 \\ q^{(w)}_4 \\ \vdots \\ q^{(w)}_{d_w-1} \\ q^{(w)}_{d_w}
% \end{pmatrix}\\
%     \end{aligned}
% \end{equation}

The rotated vector is shown in Eq.~(\ref{rope}) :
\begin{equation}
\label{rope}
    \widetilde{q} =\left[ \boldsymbol{R}_{\boldsymbol{\Theta},x_h}^{d_h}  {q^{(h)}};\boldsymbol{R}_{\boldsymbol{\Theta},x_h}^{d_h}{q^{(w)}}\right]
\end{equation}
% \begin{figure*}[t]
% \centering
% \includegraphics[width=0.9\textwidth]{ICCV2025-Author-Kit-Feb/figure/MRoPE.pdf} % Reduce the figure size so that it is slightly narrower than the column.
% \caption{The figure shows the main idea of Med-CLIP Guided Rotary Position Embedding. Texts and images will all be tokenized after being pocessed by tokenizer. The queries or keys will have the same embedding dimension which is noted as $q \in \mathbb{R}^{d_h+d_w}$. Then we will seperate the into 2 parts: $q^{(h)} \in \mathbb{R}^{d_h}$ and $q^{(w)} \in \mathbb{R}^{d_w}$ to make attentions easier to obtain height and width features from them respectively. The original position index of vision modal will be scaled and shifted to ensure the distinct gap between different modal and the parameters: $\lambda_1, \lambda_2, \alpha_1, \alpha_2$ will be chosen specifically to keep the same gap when text modal transforms to vision modal and vision modal transforms back to text modal. }
% \label{fig:3}
% \end{figure*}
\subsection{Pseudo-Labels Medical Knowledge Distillation (Pseudo-KD)}
% \begin{figure}[t]
%     \centering
%     % \hspace{-12pt}
%     \includegraphics[width=3.2in]{ICCV2025-Author-Kit-Feb/figure/comp.pdf}
%     % \vspace{0.5cm}
%     \caption{The left part shows that the BiRD~\cite{huang2024BiRD} just utilizes supervised fine-tuning to obtain the capabilities of understanding and grounding medical images. The right part is our method which uses the teacher model (trained by a small part of the entire dataset) to generate pseudo-labels for distillation.}
%     \label{fig:compare}
%     \vspace{-2em}
% \end{figure}
% As the Qwen-VL~\cite{bai2023qwenvlversatilevisionlanguagemodel} was not exposed to medical images during the training of its vision encoder, it lacks the requisite prior knowledge for medical image analysis. Retraining the vision encoder from scratch would be both computationally expensive and logistically complex. To address this limitation, we employ the Pseudo-Augmented Medical Distillation approach, which imparts medical domain knowledge to the model prior to instruction fine-tuning.

% Given that the original dataset is formatted in ChatML, which is incompatible with mainstream frameworks, we first reformat it into the ShareGPT format to facilitate processing. In the initial phase of the distillation process, we instantiate two models and fine-tune one of them using 11.1\% of the dataset, designating it as the teacher model. Subsequently, leveraging the entire dataset, we perform knowledge distillation~\cite{hinton2015distillingknowledgeneuralnetwork} into the second model. 
Our clinical knowledge distiller enables the student model to learn the output distribution of the teacher model. Therefore, we provide prior knowledge of medical images to the student model through Pseudo-Labels Medical Knowledge Distillation.

The distillation loss function is defined by a linear combination of cross-entropy loss and the Kullback-Leibler Divergence (KL Divergence). 

The cross-entropy loss is defined as:
\begin{equation}
    \mathcal{L}_{CE} = -\sum_{i=1}^Ny_i\log p_s(z_{s_i})
\end{equation}
where $\mathcal{L}_{CE}$ denotes the cross-entropy loss, $p_s(z_i)$ is the logits generated by the student ClinKD model and $y_i$ is the hard label of ground truth.

The KL Divergence mentioned before is defined as:
\begin{equation}
    \mathcal{L}_{KD} = T^2\mathbb{D}^{\boldsymbol{\epsilon_{t}}, \boldsymbol{\epsilon_{s}}}_{KL}(p_{t} \|p_{s})
\end{equation}
where $\mathcal{L}_{KD}$ denotes the loss function constructed by KL Divergence, $\mathbb{D}^{\boldsymbol{\epsilon_{t}}, \boldsymbol{\epsilon_{s}}}_{KL}(p_{t} \|p_{s})$ is the KL Divergence, $T$ is called temperature which can smooth the distribution and the $\epsilon_{s_j}$, $\epsilon_{t_j}$ are noise values. 

Furthermore, the $\mathbb{D}^{\boldsymbol{\epsilon_{t}}, \boldsymbol{\epsilon_{s}}}_{KL}(p_{t} \|p_{s})$ is:
\begin{equation}
    \mathbb{D}^{\boldsymbol{\epsilon_{t}}, \boldsymbol{\epsilon_{s}}}_{KL}(p_{t} \|p_{s}) = \sum_{i=1}^np_t(z_{t_i}+\epsilon_{t_j})\log\frac{p_t(z_{t_i}+\epsilon_{t_j})}{p_s(z_{s_i}+\epsilon_{s_j})}
\end{equation}
where $p_t(z_i)$ is the probability distribution generated by the teacher ClinKD model, $p_s(z_i)$ denotes the probability distribution generated by the student ClinKD model. 

For details, the distributions are smoothed by temperature $T$:
\begin{equation}
    \begin{aligned}
        p_s(z_{s_j}+\epsilon_{s_j}) = \dfrac{exp((z_{s_j}+\epsilon_{s_j})/T)}{\displaystyle\sum_i^nexp((z_{s_i}+\epsilon_{s_i})/T)}\\
        p_t(z_{t_j}+\epsilon_{t_j}) = \dfrac{exp((z_{t_j}+\epsilon_{t_j})/T)}{\displaystyle\sum_i^nexp((z_{t_i}+\epsilon_{t_i})/T)}  
    \end{aligned}
\end{equation}
% $$
% \mathcal{L}_{Distill} = \dfrac{(\mathcal{L}_{CE} + \alpha L_{KD})}{1+\alpha}=\dfrac{-\sum_{i=1}^Ny_i\log p_s(x_i)+\alpha T^2\sum_{i=1}^Np_t(x_i)\log\frac{p_t(x_i)}{p_s(x_i)}}{1+\alpha}
% $$
% \begin{equation}
%     \begin{aligned}
%     \mathcal{L}_{Distill} &= \dfrac{(\mathcal{L}_{CE} + \alpha L_{KD})}{1+\alpha}\\
%     &=\dfrac{-\displaystyle\sum_{i=1}^Ny_i\log p_s(x_i)+\alpha T^2\sum_{i=1}^Np_t(x_i)\log\frac{p_t(x_i)}{p_s(x_i)}}{1+\alpha}
% \end{aligned}
% \end{equation}
Finally the whole distillation loss is expressed as:
\begin{equation}
    \mathcal{L}_{Distill} = \dfrac{\mathcal{L}_{CE} + \alpha \mathcal{L}_{KD}}{1+\alpha}
    \label{distillation}
\end{equation}
After obtaining teacher's prior knowledge about medical images, the student ClinKD model will be trained by supervised fine-tuning (SFT). 

\subsection{Reflective Correction Training}
After the model distillation, we restart two rounds of supervised fine-tuning using the whole dataset. During the supervised fine-tuning process, the model's outputs are recorded and compared with the ground truth. If the semantic similarity between the output and the ground truth falls below 80\%, GPT-4o will correct the output based on the ground truth (retaining only the original meaning and anchor box coordinates) to enrich the diversity of the training set labels. The corrected data is then fed back into the training process for further learning.

\subsection{Semantic-Aware Selective Generation}
The responses generated by large language models exhibit randomness, and relying on a single inference may result in missing the optimal answer.
Semantic-Aware Selective Generation means that instead of directly using the model's single-round generation for evaluation, we allow the model to generate multiple samples. We then utilize the CLIP models to score the alignment between the generated text and image. Finally, we reorder the outputs based on these scores and select the one with the highest score as the final result. The pseudo code is shown in Algorithm~\ref{algorithm:1}.
\begin{algorithm}[h!]
\caption{Semantic-Aware Selective Generation}
\begin{algorithmic}[1]
\STATE \textbf{Input:} Image $I$, Model $M$
\STATE \textbf{Output:} Optimal generated text $T_{final}$
\STATE  $T_1, T_2, \dots, T_n \gets \text{GenerateSamples}(M, I)$
\FOR{$i = 1$ \textbf{to} $n$}
    \STATE $S_i \gets \text{CLIPScore}(T_i, I)$
\ENDFOR
\STATE $T_{ordered} \gets \text{Sort}(T_1, T_2, \dots, T_n)$
\STATE $T_{final} \gets T_{ordered}[1]$
\RETURN $T_{final}$
\end{algorithmic}
\label{algorithm:1}
\end{algorithm}


