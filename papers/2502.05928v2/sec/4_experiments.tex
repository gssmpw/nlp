
\section{Experiments}
\begin{table*}[t]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{c|c|cccc|c}
\toprule
\textbf{Model} & \textbf{Test dataset} & \textbf{VG (Recall@0.5)} & \textbf{ROC (Recall)} & \textbf{RC (SPICE)} & \textbf{MIA (mBMR)} & \textbf{Average} \\ \midrule
LLaVA-Med~\cite{li2024llava}  & Med-GRIT-Test30k~\cite{ye2023samed2d20m, cheng2023sammed2d} & 0 & 2.75 & 8.18 & 11.20 & 5.53 \\ 

BiRD~\cite{huang2024BiRD} & Med-GRIT-Test30k~\cite{ye2023samed2d20m, cheng2023sammed2d} & 53.92 & 65.33 & 55.23 & 52.17 & 56.66 \\ 
\rowcolor[RGB]{255,245,235}
\textbf{ClinKD (Ours)} & Med-GRIT-Test30k~\cite{ye2023samed2d20m, cheng2023sammed2d} & \textbf{67.51} & \textbf{82.35} & \textbf{70.56} & \textbf{65.69} & \textbf{71.53} \\
\midrule
LLaVA-Med~\cite{li2024llava}  & LLaVA-Med-qa0.2k~\cite{zhang2025biomedclipmultimodalbiomedicalfoundation} & - & - & - & \textbf{20.04} & - \\
BiRD~\cite{huang2024BiRD} & LLaVA-Med-qa0.2k~\cite{zhang2025biomedclipmultimodalbiomedicalfoundation} & - & - & - & 10.55 & - \\ 

ClinKD (Ours)  & LLaVA-Med-qa0.2k~\cite{zhang2025biomedclipmultimodalbiomedicalfoundation} & - & - & - & 13.24 & - \\
\bottomrule
\end{tabular}%
}
\caption{Comparison with LLaVA-Med~\cite{li2024llava} and BiRD (previous SOTA)~\cite{huang2024BiRD}. The best scores are shown in bold.}
\label{tab:new_sota_data}
\end{table*}

\begin{table*}[t]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{c|ccc|cccc|c}
\toprule
\multirow{2}{*}{\textbf{Model}} & \multicolumn{3}{c|}{\textbf{Proposed Methods}}  & \multirow{2}{*}{\textbf{VG (Recall@0.5)}} & \multirow{2}{*}{\textbf{ROC (Recall)}} & \multirow{2}{*}{\textbf{RC (SPICE)}} & \multirow{2}{*}{\textbf{MIA (mBMR)}} & \multirow{2}{*}{\textbf{Average}} \\ 
\cline{2-4}
&MCG-RoPE & Pseudo-KD &SASG & & & & & \\ 
% & \multicolumn{1}{c}{MCG-RoPE} & \multicolumn{1}{c}{Pseudo-KD} & \multicolumn{1}{c|}{SASG} & & & & & \\
\midrule
Qwen2-VL~\cite{wang2024qwen2vlenhancingvisionlanguagemodels}  &\ding{55}&\ding{55}&\ding{55} & 55.43 \textcolor{blue}{(+1.51)} & 68.33 \textcolor{blue}{(+3.00)} & 57.39 \textcolor{blue}{(+2.16)} & 62.23 \textcolor{blue}{(+10.06)} & 61.37 \textcolor{blue}{(+4.71)} \\
\midrule
\multirow{4}{*}{BiRD~\cite{huang2024BiRD}}  &\ding{55} &\ding{55}&\ding{55} & 53.92 & 65.33 & 55.23 & 52.17 & 56.66 \\ 
~  &\ding{51}&\ding{55}&\ding{55}& 64.03 \textcolor{blue}{(+10.11)} & 72.74 \textcolor{blue}{(+7.41)} & 61.32 \textcolor{blue}{(+6.09)} & 59.63 \textcolor{blue}{(+7.46)} & 64.44 \textcolor{blue}{(+7.78)} \\
~ &\ding{55}&\ding{51}&\ding{55}& 66.33 \textcolor{blue}{(+12.41)} & 79.52  \textcolor{blue}{(+14.19)} & 67.21 \textcolor{blue}{(+11.98)} & 64.32 \textcolor{blue}{(+12.15)} & 69.35 \textcolor{blue}{(+12.69)} \\
~ &\ding{55}&\ding{55}&\ding{51}& 55.89 \textcolor{blue}{(+1.97)} & 67.84 \textcolor{blue}{(+2.51)} & 54.32 \textcolor{blue}{(-0.91)} & 51.27 \textcolor{blue}{(-0.90)} & 57.33 \textcolor{blue}{(+0.67)} \\
\midrule
\rowcolor[RGB]{255,245,235}
\textbf{ClinKD (Ours)} &\ding{51}&\ding{51}&\ding{51} & \textbf{67.51} \textcolor{blue}{(+13.59)} & \textbf{82.35} \textcolor{blue}{(+17.02)} & \textbf{70.56} \textcolor{blue}{(+15.33)} & \textbf{65.69} \textcolor{blue}{(+13.52)} & \textbf{71.53} \textcolor{blue}{(+14.87)} \\
\bottomrule
\end{tabular}%
}
\caption{Ablation study for different models and proposed methods. The value which is blue shows how much the metric scores improved. The best scores are shown in bold. The MCG-RoPE, Pseudo-KD and SASG stand for Med-CLIP Guided RoPE, Pseudo-Labels Medical Knowledge Distillation and Semantic-Aware Selective Generation respectively.}
\label{tab:ablation_data}
\end{table*}


\begin{table*}[t]
\centering

\resizebox{\textwidth}{!}{%
\begin{tabular}{c|c|c|cccccccc|c}
\toprule
\textbf{Task} & \textbf{Metric} &\textbf{Method} & \textbf{CT} & \textbf{MR} & \textbf{X-ray} & \textbf{PET} & \textbf{Endoscopy} & \textbf{Dermoscopy} & \textbf{Fundus} & \textbf{Ultrasound} & \textbf{Average} \\ \midrule

~   & ~ & BiRD~\cite{huang2024BiRD} & 44.47\textcolor{red}{$\pm$0.14} & 29.26\textcolor{red}{$\pm$0.11} & 41.73\textcolor{red}{$\pm$0.16} & 56.46\textcolor{red}{$\pm$0.15} & 53.60\textcolor{red}{$\pm$0.12} & 75.63\textcolor{red}{$\pm$0.17} & 84.15\textcolor{red}{$\pm$0.13} & 46.04\textcolor{red}{$\pm$0.18} & 53.92\textcolor{red}{$\pm$0.14} \\
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
~   & ~ & BiRD~\cite{huang2024BiRD} + RoPE-Mixed~\cite{heo2024rotarypositionembeddingvision} & 46.90\textcolor{red}{$\pm$0.16} & 53.11\textcolor{red}{$\pm$0.13} & 42.03\textcolor{red}{$\pm$0.17} & 60.84\textcolor{red}{$\pm$0.14} & 53.49\textcolor{red}{$\pm$0.15} & 78.25\textcolor{red}{$\pm$0.11} & 85.16\textcolor{red}{$\pm$0.12} & 52.83\textcolor{red}{$\pm$0.13} & 59.08\textcolor{red}{$\pm$0.19}   \\
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\rowcolor[RGB]{237,238,254}
\cellcolor{white}~   & \cellcolor{white}~ & BiRD~\cite{huang2024BiRD} + MCG-RoPE (Ours) & 51.22\textcolor{red}{$\pm$0.14} & 49.40\textcolor{red}{$\pm$0.18} & 45.92\textcolor{red}{$\pm$0.17} & 68.60\textcolor{red}{$\pm$0.13} & 60.37\textcolor{red}{$\pm$0.19} & 88.28\textcolor{red}{$\pm$0.12} & 86.59\textcolor{red}{$\pm$0.15} & 61.88\textcolor{red}{$\pm$0.16} & 64.03\textcolor{red}{$\pm$0.15}   \\
~   & ~ & BiRD~\cite{huang2024BiRD} + $V_{k}D$ ~\cite{Miles_2024_CVPR}  & 58.77\textcolor{red}{$\pm$0.12} & 50.95\textcolor{red}{$\pm$0.16} & 45.69\textcolor{red}{$\pm$0.11} & 65.89\textcolor{red}{$\pm$0.18} & 57.63\textcolor{red}{$\pm$0.14} & 93.32\textcolor{red}{$\pm$0.15} & 79.27\textcolor{red}{$\pm$0.12} & 59.90\textcolor{red}{$\pm$0.15} & 63.93\textcolor{red}{$\pm$0.19}   \\
\rowcolor[RGB]{237,238,254}
\cellcolor{white}~   &\cellcolor{white} ~ & BiRD~\cite{huang2024BiRD} + Pseudo-KD (Ours) & 62.59\textcolor{red}{$\pm$0.15} & 52.86\textcolor{red}{$\pm$0.17} & 47.19\textcolor{red}{$\pm$0.13} & 66.28\textcolor{red}{$\pm$0.16} & 59.84\textcolor{red}{$\pm$0.11} & 93.68\textcolor{red}{$\pm$0.18} & 85.37\textcolor{red}{$\pm$0.14} & 62.87\textcolor{red}{$\pm$0.19} & 66.33\textcolor{red}{$\pm$0.13}   \\
\rowcolor[RGB]{255,245,235}
\cellcolor{white}\multirow{-6}*{\textbf{VG}}   &\cellcolor{white} \multirow{-6}*{Recall@0.5} & ClinKD (Ours) & \textbf{65.52}\textcolor{red}{$\pm$0.13} & \textbf{52.23}\textcolor{red}{$\pm$0.30} & \textbf{48.56}\textcolor{red}{$\pm$0.19} & \textbf{69.25}\textcolor{red}{$\pm$0.12} & \textbf{60.37}\textcolor{red}{$\pm$0.16} & \textbf{94.22}\textcolor{red}{$\pm$0.10} & \textbf{86.59}\textcolor{red}{$\pm$0.14} & \textbf{64.47}\textcolor{red}{$\pm$0.15} & \textbf{67.51}\textcolor{red}{$\pm$0.18} \\ \midrule

~   & ~ &BiRD~\cite{huang2024BiRD}& 34.76\textcolor{red}{$\pm$0.11} & 61.79\textcolor{red}{$\pm$0.15} & 53.74\textcolor{red}{$\pm$0.14} &  -     & 60.40\textcolor{red}{$\pm$0.17} & 96.61\textcolor{red}{$\pm$0.18}     & -     & 84.65\textcolor{red}{$\pm$0.12}     &  65.33\textcolor{red}{$\pm$0.14}  \\
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
~   & ~ &BiRD~\cite{huang2024BiRD} + RoPE-Mixed~\cite{heo2024rotarypositionembeddingvision} & 39.78\textcolor{red}{$\pm$0.13} & 62.13\textcolor{red}{$\pm$0.19} & 74.27\textcolor{red}{$\pm$0.11} &  -     & 61.33\textcolor{red}{$\pm$0.13} & 96.61\textcolor{red}{$\pm$0.17}     & -     & 84.65\textcolor{red}{$\pm$0.15}     &  69.80\textcolor{red}{$\pm$0.12}  \\
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\rowcolor[RGB]{237,238,254}
\cellcolor{white}~   &\cellcolor{white} ~ &BiRD~\cite{huang2024BiRD} + MCG-RoPE (Ours)& 44.47\textcolor{red}{$\pm$0.15} & 64.20\textcolor{red}{$\pm$0.18} & 83.75\textcolor{red}{$\pm$0.11} &  -     & 62.19\textcolor{red}{$\pm$0.14} & 96.61\textcolor{red}{$\pm$0.19}     & -     & 85.22\textcolor{red}{$\pm$0.16}     &  72.74\textcolor{red}{$\pm$0.13}  \\
~   & ~ &BiRD~\cite{huang2024BiRD} + $V_{k}D$~\cite{Miles_2024_CVPR}& 53.19\textcolor{red}{$\pm$0.17} & 69.10\textcolor{red}{$\pm$0.14} & 90.84\textcolor{red}{$\pm$0.19} &  -     & 61.71\textcolor{red}{$\pm$0.10} & \textbf{100}\textcolor{red}{$\pm$0.00}     & -     & 81.15\textcolor{red}{$\pm$0.12}     &  76.00\textcolor{red}{$\pm$0.15}  \\
\rowcolor[RGB]{237,238,254}
\cellcolor{white}~   & \cellcolor{white}~ &BiRD~\cite{huang2024BiRD} + Pseudo-KD (Ours) & 58.47\textcolor{red}{$\pm$0.16} & 69.92\textcolor{red}{$\pm$0.12} & 95.83\textcolor{red}{$\pm$0.18} &  -     & 66.32\textcolor{red}{$\pm$0.11} & \textbf{100}\textcolor{red}{$\pm$0.00}     & -     & 86.58\textcolor{red}{$\pm$0.13}     &  79.52\textcolor{red}{$\pm$0.14}  \\
\rowcolor[RGB]{255,245,235}
\cellcolor{white}\multirow{-6}{*}{\textbf{ROC}}   & \cellcolor{white}\multirow{-6}{*}{Recall} &ClinKD (Ours)& \textbf{61.42}\textcolor{red}{$\pm$0.17} & \textbf{71.30}\textcolor{red}{$\pm$0.19} & \textbf{97.00}\textcolor{red}{$\pm$0.18} & - & \textbf{68.84}\textcolor{red}{$\pm$0.12} &  \textbf{100}\textcolor{red}{$\pm$0.00}   & - & \textbf{95.54}\textcolor{red}{$\pm$0.11} & \textbf{82.35}\textcolor{red}{$\pm$0.15}  \\ \midrule
~   & ~ &BiRD~\cite{huang2024BiRD}& 41.88\textcolor{red}{$\pm$0.12} & 51.69\textcolor{red}{$\pm$0.14} & 37.39\textcolor{red}{$\pm$0.18} & 47.95\textcolor{red}{$\pm$0.15} & 54.07\textcolor{red}{$\pm$0.10} & 77.44\textcolor{red}{$\pm$0.16} & 48.73\textcolor{red}{$\pm$0.13} & 82.65\textcolor{red}{$\pm$0.19} & 55.23\textcolor{red}{$\pm$0.14}    \\
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
~   & ~ &BiRD~\cite{huang2024BiRD} + RoPE-Mixed~\cite{heo2024rotarypositionembeddingvision}& 43.51\textcolor{red}{$\pm$0.15} & 51.88\textcolor{red}{$\pm$0.18} & 54.61\textcolor{red}{$\pm$0.13} & 52.83\textcolor{red}{$\pm$0.19} & 54.07\textcolor{red}{$\pm$0.11} & 68.15\textcolor{red}{$\pm$0.17} & 53.99\textcolor{red}{$\pm$0.12} & 65.28\textcolor{red}{$\pm$0.14} & 55.54\textcolor{red}{$\pm$0.13}    \\
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\rowcolor[RGB]{237,238,254}
\cellcolor{white}~   & \cellcolor{white}~ &BiRD~\cite{huang2024BiRD} + MCG-RoPE (Ours)& 50.73\textcolor{red}{$\pm$0.14} & 53.11\textcolor{red}{$\pm$0.16} & 60.33\textcolor{red}{$\pm$0.12} & 70.16\textcolor{red}{$\pm$0.18} & 54.07\textcolor{red}{$\pm$0.11} & 70.18\textcolor{red}{$\pm$0.19} & 61.52\textcolor{red}{$\pm$0.15} & 70.46\textcolor{red}{$\pm$0.17} & 61.32\textcolor{red}{$\pm$0.13}    \\
~   & ~ &BiRD~\cite{huang2024BiRD} + $V_{k}D$ ~\cite{Miles_2024_CVPR} & 51.33\textcolor{red}{$\pm$0.13} & 54.39\textcolor{red}{$\pm$0.17} & 62.83\textcolor{red}{$\pm$0.19} & 74.31\textcolor{red}{$\pm$0.14} & 54.07\textcolor{red}{$\pm$0.15} & 76.23\textcolor{red}{$\pm$0.12} & 67.89\textcolor{red}{$\pm$0.18} & 72.15\textcolor{red}{$\pm$0.11} & 64.15\textcolor{red}{$\pm$0.16}    \\
\rowcolor[RGB]{237,238,254}
\cellcolor{white}~   &\cellcolor{white} ~ &BiRD~\cite{huang2024BiRD} + Pseudo-KD (Ours)& 52.78\textcolor{red}{$\pm$0.12} & 58.41\textcolor{red}{$\pm$0.15} & 70.39\textcolor{red}{$\pm$0.18} & 75.85\textcolor{red}{$\pm$0.13} & \textbf{55.22}\textcolor{red}{$\pm$0.16} & 81.34\textcolor{red}{$\pm$0.11} & 71.54\textcolor{red}{$\pm$0.17} & 72.15\textcolor{red}{$\pm$0.14} & 67.21\textcolor{red}{$\pm$0.19}    \\
\rowcolor[RGB]{255,245,235}
\cellcolor{white}\multirow{-6}{*}{\textbf{RC}}   &\cellcolor{white} \multirow{-6}{*}{SPICE~\cite{anderson2016spicesemanticpropositionalimage}} &ClinKD (Ours)& \textbf{53.94}\textcolor{red}{$\pm$0.14} & \textbf{58.41}\textcolor{red}{$\pm$0.12} & \textbf{71.47}\textcolor{red}{$\pm$0.18} & \textbf{79.91}\textcolor{red}{$\pm$0.15}& 52.10\textcolor{red}{$\pm$0.11} & \textbf{83.92}\textcolor{red}{$\pm$0.19} & \textbf{84.51}\textcolor{red}{$\pm$0.13} & \textbf{80.19}\textcolor{red}{$\pm$0.16} & \textbf{70.56}\textcolor{red}{$\pm$0.17} \\ \midrule

~  & ~ &BiRD~\cite{huang2024BiRD}& 47.01\textcolor{red}{$\pm$0.14} & 49.35\textcolor{red}{$\pm$0.18} & 37.17\textcolor{red}{$\pm$0.15} & 57.15\textcolor{red}{$\pm$0.12} & 39.91\textcolor{red}{$\pm$0.17} & 72.13\textcolor{red}{$\pm$0.13} & 48.87\textcolor{red}{$\pm$0.16} & 65.78\textcolor{red}{$\pm$0.19} & 52.17\textcolor{red}{$\pm$0.11} \\ 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
~  & ~ &BiRD~\cite{huang2024BiRD} + RoPE-Mixed~\cite{heo2024rotarypositionembeddingvision} & 57.10\textcolor{red}{$\pm$0.17} & 50.89\textcolor{red}{$\pm$0.13} & 42.55\textcolor{red}{$\pm$0.16} & 69.28\textcolor{red}{$\pm$0.11} & 41.63\textcolor{red}{$\pm$0.18} & 73.32\textcolor{red}{$\pm$0.14} & 59.63\textcolor{red}{$\pm$0.15} & 61.80\textcolor{red}{$\pm$0.10} & 57.03\textcolor{red}{$\pm$0.19} \\ 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\rowcolor[RGB]{237,238,254}
\cellcolor{white}~  &\cellcolor{white} ~ &BiRD~\cite{huang2024BiRD} + MCG-RoPE (Ours)& 60.53\textcolor{red}{$\pm$0.16} & 53.77\textcolor{red}{$\pm$0.11} & 54.10\textcolor{red}{$\pm$0.18} & 62.01\textcolor{red}{$\pm$0.12} & 43.88\textcolor{red}{$\pm$0.15} & 75.33\textcolor{red}{$\pm$0.14} & 62.89\textcolor{red}{$\pm$0.19} & 64.53\textcolor{red}{$\pm$0.13} & 59.63\textcolor{red}{$\pm$0.17} \\ 
~  & ~ &BiRD~\cite{huang2024BiRD} + $V_{k}D$ ~\cite{Miles_2024_CVPR} & 62.33\textcolor{red}{$\pm$0.10} & 53.77\textcolor{red}{$\pm$0.15} & 52.27\textcolor{red}{$\pm$0.19} & 62.01\textcolor{red}{$\pm$0.16} & 47.29\textcolor{red}{$\pm$0.13} & 75.33\textcolor{red}{$\pm$0.17} & 68.04\textcolor{red}{$\pm$0.12} & 70.31\textcolor{red}{$\pm$0.18} & 61.42\textcolor{red}{$\pm$0.14} \\ 
\rowcolor[RGB]{237,238,254}
\cellcolor{white}~  & \cellcolor{white}~ &BiRD~\cite{huang2024BiRD} + Pseudo-KD (Ours)& 66.85\textcolor{red}{$\pm$0.11} & 55.73\textcolor{red}{$\pm$0.14} & 52.27\textcolor{red}{$\pm$0.17} & 64.74\textcolor{red}{$\pm$0.15} & \textbf{50.58}\textcolor{red}{$\pm$0.12} & 78.21\textcolor{red}{$\pm$0.19} & 70.02\textcolor{red}{$\pm$0.13} & \textbf{76.16}\textcolor{red}{$\pm$0.18} & 64.32\textcolor{red}{$\pm$0.16} \\ 
\rowcolor[RGB]{255,245,235}
\cellcolor{white}\multirow{-6}{*}{\textbf{MIA}}  &\cellcolor{white} \multirow{-6}{*}{mBMR~\cite{huang2024BiRD}} &ClinKD (Ours)& \textbf{66.85}\textcolor{red}{$\pm$0.13} & \textbf{56.82}\textcolor{red}{$\pm$0.16} & \textbf{54.01}\textcolor{red}{$\pm$0.18} & \textbf{67.08}\textcolor{red}{$\pm$0.14} & \textbf{50.58}\textcolor{red}{$\pm$0.11} & \textbf{81.00}\textcolor{red}{$\pm$0.15} & \textbf{70.02} \textcolor{red}{$\pm$0.17} & 76.05\textcolor{red}{$\pm$0.19} & \textbf{65.69}\textcolor{red}{$\pm$0.12} \\ \midrule

~ & ~ &BiRD~\cite{huang2024BiRD}& 43.03\textcolor{red}{$\pm$0.12} & 48.02\textcolor{red}{$\pm$0.11} & 42.51\textcolor{red}{$\pm$0.17} & 53.85\textcolor{red}{$\pm$0.19} & 51.99\textcolor{red}{$\pm$0.14} & 80.45\textcolor{red}{$\pm$0.16} & 60.58\textcolor{red}{$\pm$0.13} & 69.78\textcolor{red}{$\pm$0.15} &  -     \\ 
~ & ~ &BiRD~\cite{huang2024BiRD} + RoPE-Mixed~\cite{heo2024rotarypositionembeddingvision}& 46.82\textcolor{red}{$\pm$0.17} & 54.50\textcolor{red}{$\pm$0.12} & 53.37\textcolor{red}{$\pm$0.15} & 60.98\textcolor{red}{$\pm$0.13} & 52.63\textcolor{red}{$\pm$0.18} & 79.08\textcolor{red}{$\pm$0.10} & 66.26\textcolor{red}{$\pm$0.19} & 66.14\textcolor{red}{$\pm$0.11} &  -     \\ 
\rowcolor[RGB]{237,238,254}
\cellcolor{white}~ & \cellcolor{white}~ &BiRD~\cite{huang2024BiRD} + MCG-RoPE (Ours)& 51.74\textcolor{red}{$\pm$0.13} & 55.12\textcolor{red}{$\pm$0.19} & 61.03\textcolor{red}{$\pm$0.12} & 66.92\textcolor{red}{$\pm$0.17} & 55.13\textcolor{red}{$\pm$0.14} & 82.60\textcolor{red}{$\pm$0.15} & 70.33\textcolor{red}{$\pm$0.11} & 70.52\textcolor{red}{$\pm$0.18} &  -     \\ 
~ & ~ &BiRD~\cite{huang2024BiRD} + $V_{k}D$~\cite{Miles_2024_CVPR} & 56.41\textcolor{red}{$\pm$0.16} & 57.05\textcolor{red}{$\pm$0.11} & 62.91\textcolor{red}{$\pm$0.19} & 67.40\textcolor{red}{$\pm$0.14} & 55.18\textcolor{red}{$\pm$0.18} & 86.22\textcolor{red}{$\pm$0.10} & 71.73\textcolor{red}{$\pm$0.15} & 70.88\textcolor{red}{$\pm$0.13} &  -     \\ 
\rowcolor[RGB]{237,238,254}
\cellcolor{white}~ &\cellcolor{white} ~ &BiRD~\cite{huang2024BiRD} + Pseudo-KD (Ours)& 60.17\textcolor{red}{$\pm$0.14} & 59.23\textcolor{red}{$\pm$0.16} & 66.42\textcolor{red}{$\pm$0.12} & 68.96\textcolor{red}{$\pm$0.19} & \textbf{57.99}\textcolor{red}{$\pm$0.13} & 88.31\textcolor{red}{$\pm$0.15} & 75.64\textcolor{red}{$\pm$0.17} & 74.44\textcolor{red}{$\pm$0.10} &  -     \\ 
\rowcolor[RGB]{255,245,235}
\cellcolor{white}\multirow{-6}{*}{\textbf{Average}} & \cellcolor{white}\multirow{-6}{*}{-} &ClinKD (Ours)& \textbf{61.93}\textcolor{red}{$\pm$0.15} & \textbf{59.69}\textcolor{red}{$\pm$0.18} & \textbf{67.76}\textcolor{red}{$\pm$0.19} & \textbf{72.08}\textcolor{red}{$\pm$0.13} & 57.97\textcolor{red}{$\pm$0.16}  & \textbf{89.79}\textcolor{red}{$\pm$0.11} & \textbf{80.37}\textcolor{red}{$\pm$0.14} & \textbf{79.06}\textcolor{red}{$\pm$0.12} & -  \\ \bottomrule
\end{tabular}%
}
\caption{Comparison with other SOTA methods on Med-GRIT-Test30k~\cite{ye2023samed2d20m, cheng2023sammed2d}. We evaluate the performance on eight modalities.}
\label{tab:detail_res}
\vspace{-1em}
\end{table*}

\subsection{Datasets}
\textbf{Med-GRIT-270k~\cite{huang2024BiRD}.} The Med-GRIT-270k dataset comprises 270k samples, with 240k allocated for training and the remaining 30k for multi-task evaluation. These samples are collected from eight medical modalities: CT, MR, X-ray, PET, Endoscopy, Dermoscopy, Fundus and Ultrasound. Additionally, we randomly select 30K samples from the entire dataset for pseudo-label generation, which is subsequently used by the Pseudo-Labels Medical Knowledge Distillation. This fine-grained multi-task dataset not only demands that the model accurately understands medical images, but also requires it to precisely classify and ground pathological regions and physiological structures. This poses a significant challenge for MLLMs. \\
% \begin{figure}[t]
%     \centering
%     \includegraphics[width=3.2in]{ICCV2025-Author-Kit-Feb/figure/comp_with_labels.pdf}
%     \vspace{-1em}
%     \caption{Average scores
%     of 4 tasks arrange different types of data}
%     \label{fig:4}
%     % \vspace{-1em}
% \end{figure}
\textbf{LLaVA-Med-qa0.2k~\cite{zhang2025biomedclipmultimodalbiomedicalfoundation}.} The LLaVA-Med-qa0.2k~\cite{zhang2025biomedclipmultimodalbiomedicalfoundation} contains 0.2k QA pairs of MIA task which are used for evaluating the models' ability to analyze medical images. It consists of 50 images with caption pairs and two types of questions: conversation and detailed description. \\ The images are not only from common modalities such as CT and MR, but also from some modalities that are not available in the Med-GRIT-270k~\cite{huang2024BiRD} dataset, such as 3D reconstruction. The wide range of data sources evaluates the capabilities of MLLMs in general medical scenarios.

\subsection{Evaluation Metrics}
We use Recall@0.5 for Visual Grounding (VG), Recall for Referring Object Classification (ROC), SPICE~\cite{anderson2016spicesemanticpropositionalimage} for Referring Caption (RC), and mBMR~\cite{huang2024BiRD} for Medical Image Analysis (MIA). The final overall result is computed as the mean of the evaluation scores across these four tasks.

For the evaluation on LLaVA-Med-qa0.2k~\cite{li2024llava} which has 0.2k QA pairs of MIA task, we use mBMR as the metric.

\subsection{Implementation Details}
We use Qwen2-VL~\cite{wang2024qwen2vlenhancingvisionlanguagemodels} as the base model for ClinKD. For the Med-CLIP Guided RoPE component of ClinKD, in order to ensure that the position index is an integer, we resample the image resolution to \( w = h \). During the distillation process, we set \( \alpha = 0.5 \), $T=0.5$. The number of distillation epochs is 2, with an additional 2 epochs for subsequent training. The entire training process is conducted using 6 NVIDIA A100 40G GPUs. The learning rate is set to \( 2 \times 10^{-5} \), and the AdamW optimizer is employed. The learning scheduler follows a cosine schedule. For model generation, we use non-sampling beam search with a beam size of 3.

\subsection{Analysis}
\subsubsection{Comparison with the State-of-the-Art Methods}

\textbf{Evaluation on the Med-GRIT-Test30k~\cite{ye2023samed2d20m, cheng2023sammed2d}.} As shown in Table~\ref{tab:new_sota_data}, we compare ClinKD with BiRD~\cite{huang2024BiRD} and LLaVA-Med~\cite{li2024llava}. Results show that ClinKD achieves scores of 67.51\%, 82.35\%, 70.56\%, and 65.69\% in the VG, ROC, RC, and MIA tasks respectively, outperforming other comparisons by a clear margin in all tasks. On average, ClinKD outperforms BiRD~\cite{huang2024BiRD} and LLaVA-Med~\cite{li2024llava} by 14.87\% and 66.00\% respectively. ClinKD's advantages lie in its more adequate image-text alignment and the prior medical knowledge provided by Pseudo-Labels Medical Knowledge Distillation.\\
% These scores demonstrate that ClinKD significantly improves image-text alignment through the use of MCG-RoPE. Additionally, the proposed pseudo-label medical knowledge distillation addresses the medical knowledge gap in Qwen2-VL, facilitating better adaptation to medical knowledge.
\textbf{Evaluation on the LLaVA-Med-qa0.2k~\cite{zhang2025biomedclipmultimodalbiomedicalfoundation}.} Table~\ref{tab:new_sota_data} represents the models' performance on LLaVA-Med-qa0.2k~\cite{zhang2025biomedclipmultimodalbiomedicalfoundation}. LLaVA-Med~\cite{li2024llava} still achieves the best performance. Although ClinKD has not yet surpassed LLaVA-Med~\cite{li2024llava} on this dataset, it still achieved a 2.69\% improvement over BiRD~\cite{huang2024BiRD} in the MIA task. The limitation may be caused by the different sources of medical data so that the ClinKD may fail to leverage medical knowledge to analyze medical images.
\begin{figure}[t]
    % \centering
    \hspace{-0.8cm}
    \includegraphics[width=3.8in]{figure/alpha_with_curve_and_labels.pdf}
    \vspace{-0.8cm}
    \caption{Effect of value of $\alpha$ in Pseudo-Labels Medical Knowledge Distillation.}
    \vspace{-0.5cm}
    \label{fig:alpha}
    \vspace{-1em}
\end{figure}

% \begin{figure}[t]
%     \centering
%     \begin{center}
%     \includegraphics[width=3.2in]{ICCV2025-Author-Kit-Feb/figure/RAD.pdf}
%     \end{center}
%     \vspace{-1em}
%     \caption{An example from VQA-RAD~\cite{lau2018dataset} and the performance of different RoPE.}
%     \label{fig:rad}
%     \vspace{-1em}
% \end{figure}

\subsubsection{Ablation study}

\textbf{Effect of Qwen2-VL~\cite{wang2024qwen2vlenhancingvisionlanguagemodels}.} Table \ref{tab:ablation_data} presents the ablation study of  proposed methods. To investigate the impact of model upgrade on the experiment, we applied the same training strategy to Qwen2-VL as we did to BiRD~\cite{huang2024BiRD}. The results show that the model upgrade only has a significant effect on the MIA task, which shows Qwen2-VL~\cite{wang2024qwen2vlenhancingvisionlanguagemodels} outperforms BiRD by 10.06\%.\\
% For the three main components of our method, we conducted comparisons respectively. Results show that our method can enhance the model's performance when used individually. According to the results, we can see that pseudo-labels medical knowledge distillation has a significant improvement on the results. It enables the model to achieve an increase of more than 10.0 in all four tasks. For SASG, all its improvements were on the tasks of VG and ROC, which test the model's ability to recognize regions.
\textbf{Effect of Med-CLIP Guided Rotary Position Embedding.} As shown in Table~\ref{tab:ablation_data}, with adding MCG-RoPE to BiRD, all metrics exhibit significant enhancements, with the average rising from 56.66\% to 64.44\%. This indicates the importance of adequate image-text alignment.\\
\textbf{Effect of Pseudo-Labels Medical Knowledge Distillation.} Table~\ref{tab:ablation_data} represents improvements achieved by using Pseudo-Labels Medical Knowledge Distillation that leads to enhancements by 12.41\%, 14.19\%, 11.98\% and 12.15\% in the VG, ROC, RC and MIA tasks, respectively. The results illustrate that providing prior medical knowledge by using Pseudo-Labels Medical Knowledge Distillation can fill MLLMs' gap of medical knowledge, boosting capabilities of adapting to new medical knowledge.

Figure~\ref{fig:alpha} shows the effect of different values of $\alpha$ in Eq.~(\ref{distillation}). In this experiment, the average performance achieves the best when we set $\alpha=0.5$. Both excessively high and low alpha values can lead to performance degradation. This might be because a low $\alpha$ value prevents ClinKD from learning sufficient prior medical knowledge, while an overly high alpha value tends to make ClinKD generate irrational pseudo-labels that contradict common medical knowledge.\\
\textbf{Effect of Semantic-Aware Selective Generation.} From the results in Table~\ref{tab:ablation_data}, we can see a light enhancement on VG and ROC task. However, there is a slight decrease in the metric scores on the RC and MIA tasks. These results may be caused by the semantic similarity from CLIP models that are affected by complex sentences.

\subsubsection{Comparison with Other Methods}
\textbf{Comparison with BiRD + RoPE-Mixed~\cite{heo2024rotarypositionembeddingvision}.}
The RoPE-Mixed uses frequencies for both axes as learnable network parameters, effectively handling image features in diagonal direction. It achieves the best performance in different visual scenarios~\cite{ade20k, zhou2018semanticunderstandingscenesade20k, coco}. Hence we compare our MCG-RoPE with RoPE-Mixed. As shown in Table~\ref{tab:detail_res}, by adding RoPE-Mixed to BiRD~\cite{huang2024BiRD}, great achievements take place on all modalities (CT, MR, X-ray, etc.). Compared with RoPE-Mixed~\cite{heo2024rotarypositionembeddingvision}, our MCG-RoPE outperforms it by 4.95\%, 2.94\%, 5.78\% and 2.60\% on VG, ROC, RC and MIA tasks, respectively. For more detailed performance on the 8 modalities, MCG-RoPE still achieves the best improvements most of the time.\\
\textbf{Comparison with BiRD + $V_{k}D$~\cite{Miles_2024_CVPR}.}
$V_{k}D$ is a novel method that can improve knowledge distillation. This work utilizes a projection layer for maximizing the knowledge transfer to the student backbone. Many experiments~\cite{DLGAN_CVPR, spkd, pkd, label-kd} have been made to prove the best performance of this method. Compared with BiRD with $V_kD$ in the Table~\ref{tab:detail_res}, Pseudo-KD shows its advantages on multi-task fine-grained Med-VQA scenarios, outperforming $V_kD$ almost on all modalities. 
\begin{figure}[t]
    \centering
    \begin{center}
    \includegraphics[width=3.2in]{figure/Visualization.pdf}
    \end{center}
    \vspace{-1em}
    \caption{Visualization of attentions on GC and VG tasks. The ClinKD generates better answers compared with BiRD~\cite{huang2024BiRD}.}
    \label{fig:visual1}
    \vspace{-1em}
\end{figure}
\subsubsection{Case Study}
% \begin{figure}[t]
%     \centering
%     \begin{center}
%     \includegraphics[width=3.2in]{ICCV2025-Author-Kit-Feb/figure/RAD.pdf}
%     \end{center}
%     \vspace{-2em}
%     \caption{An example from VQA-RAD~\cite{lau2018dataset} and the performance of RoPE variants.}
%     \label{fig:rad}
%     \vspace{-1em}
% \end{figure}
\textbf{Case study on Med-GRIT-Test30~\cite{ye2023samed2d20m, cheng2023sammed2d}.}
Figure~\ref{fig:visual1} illustrates the visualization of the attention map and a comparison among BiRD~\cite{huang2024BiRD}, ClinKD (ours), and the ground truth. The grounding of ClinKD is more accurate because MCG-RoPE can improve image-text alignment, and at the same time, Pseudo-KD enables ClinKD to better adapt to medical knowledge.\\
\textbf{Case study on Additional VQA-RAD~\cite{lau2018dataset}.}
As shown in Figure~\ref{fig:rad}, we conducted an experiment on VQA-RAD~\cite{lau2018dataset}. Despite the relatively simple annotations, ClinKD provides fine-grained answers with three accurate groundings, demonstrating its precision in response.

To assess the models' attention to pathological features, we conducted a comparative analysis of three RoPE variants. The results indicate that 2D-RoPE~\cite{su2023roformerenhancedtransformerrotary} tends to show low attention on pathological regions. In contrast, RoPE-Mixed~\cite{heo2024rotarypositionembeddingvision} demonstrates a greater emphasis on diagonal features. Notably, our Med-CLIP Guided RoPE specifically targets and enhances the model's attention to pathological regions, thereby facilitating more accurate alignment between text and images, especially in the identification of relevant medical features.
\begin{figure}[t]
    \centering
    \begin{center}
    \includegraphics[width=3.2in]{figure/RAD.pdf}
    \end{center}
    \vspace{-2em}
    \caption{An example from VQA-RAD~\cite{lau2018dataset} and the performance of RoPE variants.}
    \label{fig:rad}
    \vspace{-1em}
\end{figure}
\subsubsection{Limitation}
Although our method alleviates, to some extent, the issue of insufficient prior knowledge and achieves impressive state-of-the-art performance on the Med-GRIT-270k~\cite{ye2023samed2d20m, cheng2023sammed2d} dataset, the model still exhibits hallucinations. Furthermore, through comparative evaluations on the LLaVA-Med-qa0.2k~\cite{zhang2025biomedclipmultimodalbiomedicalfoundation} dataset, our approach remains insufficient to enable the model to emerge with strong generalization capabilities in Med-VQA domain. Creating high-quality Med-VQA datasets and training medical prior knowledge before freezing the visual encoder could significantly enhance the model's generalization ability on medical data.
% \vspace{cm}