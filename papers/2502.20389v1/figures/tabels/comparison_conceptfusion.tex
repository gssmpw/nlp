\begin{table}[h]
    \centering
        \caption{\textbf{Comparison to 3D pseudolabels.} A mask decoder trained on top of frozen LIFT-GS features matches and even outperforms a decoder trained on top of lifted 3D pseudolabels (voxel-pooled ConceptFusion~\cite{conceptfusion}). LIFT-GS learns to pool features in 3D in order to optimally reproduce the pseudolabels after rendering, which outperforms using a hand-crafted aggregation.
        Note: in this experiment we used a more expressive mask decoder in this experiment with a larger MLP ratio, which improves the results for all methods, including LIFT-GS.}
\resizebox{.49\textwidth}{!}{
        \begin{tabular}{lcc}
            \toprule
            Features & Acc$@$0.25 & Acc$@$0.5\\
            \midrule
            Scratch (RGB) & 44.1 & 30.6  \\
            3D pseudolabels & 50.1 & 34.7 \\
            2D pseudolabels (LIFT-GS features) & 51.8 & 38.3 \\ 
            LIFT-GS (finetuned) & 54.7 & 40.5 \\ 
            \bottomrule
        \end{tabular}
        }
    \label{tab:comparison_pseudolabels_3d}
\end{table}
