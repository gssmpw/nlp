\section{Experiments}
\input{figures/tabels/ins_seg.tex}
In this section, we first provide training details and demonstrate how pretraining significantly enhances downstream task performance through a set of carefully designed ablations. Additionally, we uncover several intriguing findings by scaling pretraining and fine-tuning data, as well as exploring the impact of different 2D foundation models.


\subsection{Training Details}
We provide details below with more details in Appendix. 

\noindent\textbf{Training Datasets}
We use ScanNet~\cite{dai2017scannet} as the primary dataset for downstream task fine-tuning and evaluation, as its annotations form the basis for established benchmarks.
Our method's flexibility enables training on diverse 3D datasets without requiring language annotations or instance masks. For pretraining, we primarily use ScanNet~\cite{dai2017scannet} and ScanNet++\cite{Yeshwanth2023ScanNetAH} and also explore Taskonomy\cite{Zamir2018TaskonomyDT} and Aria Synthetic~\cite{Somasundaram2023ProjectAA}. 


\noindent\textbf{Architecture}
Our pipeline imposes no constraints on the 3D architecture or inductive biases, allowing flexibility in model selection. For experiments, we use SparseConv UNet as the encoder backbone, a widely adopted architecture for point cloud processing. The Mask Decoder is an 8-layer Transformer decoder with a hidden state size of 512.

\noindent\textbf{Training Details}
We train the \emph{Encoder} and \emph{Decoder} jointly using a batch size of 32 for 76k steps on 32 A100 GPUs.
We use AdamW~\cite{Loshchilov2017DecoupledWD} optimizer with a learning rate of 1e-4 and weight decay of 1e-4. 
 
\subsection{Evaluation on 3D Vision-Language Grounding}
We fine-tune and evaluate our pretrained model on two representative 3D VLG tasks: 3D open-vocabulary instance segmentation and 3D referential grounding. Our results, shown in Tables~\ref{table:3dins} and~\ref{table:grounding}, demonstrate significant improvements over models trained from scratch and achieve state-of-the-art performance with pertaining.




\subsubsection{Grounding Simple Nouns in 3D}
We first evaluate simple grounding for simple noun-phrases, using object categories without spatial relationships. Following the protocol in~\cite{Zhu2024Unifying3V}, we convert the standard 3D instance segmentation benchmark on ScanNet into an open-vocabulary 3D instance segmentation task. The categories of objects are used as language queries, which are input to the model to predict the corresponding 3D masks.

\noindent\textbf{Evaluation setting:} We evaluate using the standard metric mAP, a measure of mask overlap averaged across categories. We fine-tune \model{} for 500 epochs.

\noindent\textbf{Results:} Compared against the state-of-the-art baselines PQ3D~\cite{Zhu2024Unifying3V} and OpenMask3D~\cite{takmaz2023openmask3d}, our pretrained model (\model) achieves substantial performance gains (mAP 25.7\% vs 20.2\%), as shown in Table~\ref{table:3dins}. It significantly outperforms its counterpart trained from scratch (\model-Scratch mAP +3.2\%).





\input{figures/tabels/grounding_ref_exp.tex}
\input{figures/tabels/pretraining_compare}
\subsubsection{Grounding Complex Phrases in 3D}
Next, we examine grounding multiple objects using more complex phrases that contain spatial references, referred to as \emph{3D Referential Grounding}~(3D RG). 





\textbf{Evaluation Setting.}
We evaluate \model\ on the most common \emph{3D Referential Grounding} benchmarks: ScanRefer~\cite{Chen2019ScanRefer3O}, SR3D, and NR3D~\cite{Achlioptas2020ReferIt3DNL, Abdelreheem2022ScanEnts3DEP}.
We use standard top-1 accuracy as the evaluation metric, considering a predicted bounding box correct if its IoU with the ground truth exceeds 0.25 or 0.5. Since \model\ outputs masks instead of axis-aligned bounding boxes, we derive bounding boxes by extracting the extreme corner points from the point cloud within the predicted masks.

\model{} is designed to be practical. To ensure a more realistic evaluation for embodied applications, we introduce two modifications: (1) we predict 3D masks without assuming the availability of ground-truth 3D bounding boxes, and (2) we utilize sensor point clouds (\emph{Sensor PC}) from RGB-D scans instead of mesh-derived point clouds (\emph{Mesh PC}).

Existing methods are typically trained on mesh-derived point clouds (\emph{Mesh PC}), requiring creating scene meshes from RGB-D scans, processing, and sampling point clouds. They  generally rely on \emph{segments} derived from clustering centers generated during human annotation. Since such information is unavailable in real-world applications, we instead use raw sensor point clouds (\emph{Sensor PC}) from RGB-D scans.
This setting is inherently more challenging, as reflected in the significant performance drop of BUTD-DETR~\cite{Jain2021BottomUT} when transitioning from \emph{Mesh PC} to \emph{Sensor PC} (Table~\ref{table:grounding}), consistent with findings in~\cite{Jain2024ODINAS}.

\textbf{Baselines}
We compare \model\ against the state-of-the-art two-stage methods, 3D-VisTA~\cite{Zhu20233DVisTAPT} and PQ3D~\cite{Zhu2024Unifying3V}, as well as the SOTA single-stage method, BUTD-DETR~\cite{Jain2021BottomUT}. All two-stage baselines assume access to ground-truth 3D masks or boxes during inference, so we re-evaluate them using predicted boxes from the SOTA object detector Mask3D~\cite{Schult2022Mask3DMT}. For fairness, we re-train 3D-VisTA and BUTD-DETR on sensor point clouds. Because PQ3D uses multiple backbones and a multi-stage training pipeline, we were not able to reproduce PQ3D on the sensor point cloud setting.

\textbf{Results}
Our model without pretraining (\model-Scratch) achieves slightly better performance than the state-of-the-art single-stage method BUTD-DETR~\cite{Jain2021BottomUT}, likely due to architectural similarities with extra modifications.

With pretraining, \model\ achieves significant improvements across all three datasets, with relative gains of $10\%-30\%$, demonstrating the effectiveness of our pretraining approach. Notably, \model\ outperforms 3D-VisTA in Acc$@25$, despite 3D-VisTA being a two-stage method with bounding box proposals from Mask3D using \emph{Mesh PC}.



\subsection{Ablation and Analyses}
We conduct an in-depth analysis of the proposed method through a series of ablation and scaling experiments. For these evaluations, we use a model pretrained only on ScanNet as the baseline.
To simplify the presentation for the ablations, we report results on the combined evaluation set of ScanRefer, SR3D, and NR3D. Additionally, we report the higher accuracy threshold Acc$@0.75$.


\noindent\textbf{Compare to SOTA pretraining methods}
We compare to Ponder-v2~\cite{zhu2023ponderv2}, a state-of-the-art method for point cloud pretraining. Ponder-v2 takes point clouds as input and predicts voxel grids, which are then used to render 2D images via NeRF (specifically NeuS, which additionally models surfaces~\cite{wang2021neus}). Pretraining is supervised by 2D photometric loss and CLIP features, where features are computed based on per-pixel text labels for the mask (GT category labels and masks for each instance).

We found that the official Ponder-v2 checkpoint fails to improve performance on 3D referential grounding, likely due to the limited size and diversity of its per-pixel text labels.
Therefore, we retrain the Ponder-v2 while using the SAM-CLIP pseudolabels described in Section~\ref{sec:pseudolabels}. Training on the pseudolabels (indicated by Ponder-v2$\dagger$ in Table~\ref{tab:pretraining_com}) significantly improves Acc@$0.25$ +4.5\% vs. the official Ponder-v2, and +3.2\% vs. training from scratch. Since both approaches use render-supervision, this experiment underscores the advantages of pseudolabel distillation in data-scarce regimes.
Moreover our method does not rely on human-annotated per-pixel text labels, making it highly scalable to other datasets, as shown in Table~\ref{tab:pretrain_data_scale}.
\input{figures/tabels/pretrain_loss_compare}






\noindent \textbf{Loss Ablation}
Existing pretraining pipelines primarily focus on the encoder~\cite{zhu2023ponderv2, ElBanani2021UnsupervisedRRUP}, whereas the render-supervised formulation can pretrain the entire architecture in a unified manner. To assess the impact of the grounding loss, which applies only to the decoder, we compare models pretrained with different losses in Table~\ref{tab:pretraining_loss}.

A model trained with $\mathcal{L}_{\text{ground}}$ alone (row 2) still substantially improves downstream task performance, performing only slightly worse than the model trained with the full loss (row 5), demonstrating the effectiveness of the grounding loss.
Furthermore, comparing models with and without $\mathcal{L}{\text{ground}}$ (row 5 vs. row 4) clearly shows that $\mathcal{L}_{\text{ground}}$ significantly enhances downstream performance, particularly in more challenging scenarios (IoU thresholds of 0.5 and 0.75). 




\input{figures/tabels/tab_finetune_data_scaling}
\subsection{Data Scaling}
\label{sec:data_scaling}
\noindent\textbf{Finetuning Data Scaling}
We limit the fine-tuning data size to $0.1$, $0.2$, and $0.5$ of the full dataset for 3D referential grounding to analyze performance variation with different fine-tuning ratios. The results are presented in Figure~\ref{fig:ref-exp-data} and Table~\ref{tab:ref_data_scaling}, with additional findings provided in Appendix~\ref{sec:supp_data_scaling}.


Intriguingly, we find that \model{} ``pretraining effectively multiplies the fine-tuning dataset''~\cite{hernandez2021scaling} by a constant factor (roughly 2x), across varying amounts of fine-tuning data.
For instance, the pretrained model fine-tuned on $20\%$ of the data achieves performance comparable to training from scratch with $50\%$ of the data. The benefits of pretraining are even more pronounced at higher IoU thresholds. Overall, a pretrained model can achieve the same performance as a model trained from scratch on all the data using only $30{-}40\%$ of the fine-tuning data.

This observation matches empirical data scaling laws for pretraining in other modalities~\cite{hernandez2021scaling}. And interestingly, the fact that the coefficient does not show diminishing returns, even when we use all available fine-tuning data, which underscores that 3D VLG models are currently operating in the data scarce regime.



\input{figures/tabels/fig_finetune_data_scaling}

\textbf{Pretraining Data Scaling}
A key advantage of our framework is its ability to pretrain on large-scale RGB-D scans without requiring 3D or language annotations, ensuring scalability. We scale the pretraining dataset and analyze its impact on fine-tuning performance, as shown in Table~\ref{tab:pretrain_data_scale}.

Scaling the pretraining dataset generally improves performance. Adding ScanNet++\cite{Yeshwanth2023ScanNetAH} yields a notable 0.8$\%$ improvement, despite its limited data size. Incorporating Taskonomy\cite{Zamir2018TaskonomyDT} and Aerial Synthetic~\cite{Somasundaram2023ProjectAA} also enhances performance, though the gains are less pronounced due to distributional differences from ScanNet, likely due to the mesh reconstruction quality in Taskonomy as well.


Since our method doesn't need 3D masks or language annotations, expanding the pretraining dataset with real-world scans is fairly easy. We anticipate continuous improvements as larger-scale data collections become available.

\input{figures/tabels/data_scalling}

\subsection{2D Foundation Models Scaling and Exploration}
Our pipeline leverages powerful 2D foundation models to generate pseudo-labels. Here, we investigate their impact by analyzing performance variations with different 2D foundation models, with results presented in Table~\ref{tab:2dmodel}.

\noindent\textbf{Weaker CLIP and SAM}
The main experiments use SAM-H and CLIP-L for pseudo-labeling. Replacing them with smaller models, MobileSAM~\cite{mobile_sam}(ViT-tiny)~ and CLIP-B, leads to a noticeable performance drop, especially at higher accuracy thresholds. This suggests that our model benefits from advancements in 2D foundation models, highlighting the importance of stronger 2D models.
\input{figures/tabels/foundation_model}


\noindent\textbf{Captions from VLMs}
We explore an alternative pseudo-labeling strategy by combining SAM with the LLAMA-V model (details in Appendix). After segmenting objects in 2D images using SAM, we prompt LLAMA-V to describe the segmented regions and encode these descriptions into CLIP embeddings.

Pretraining with this pseudo-labeling method achieves performance comparable to our original pipeline with SAM-H and CLIP-L, highlighting the flexibility and extensibility of our framework. We believe text-based captions hold significant potential for future research, and our pipeline is well-positioned to benefit from advancements in this area.












