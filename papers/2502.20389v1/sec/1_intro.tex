\section{Introduction}



When a user mentions \emph{the keys by the door} or \emph{the blue mug on the table}, they use language to indicate a specific set of objects and 3D locations in space. 
Such \emph{3D language grounding} provides a particularly natural interface for people to communicate about their surroundings.

\input{figures/tabels/fig_teaser}

For AI systems operating in physical spaces, identifying the set of 3D masks or bounding boxes indexed by a language query represents a core functionality; with applications across autonomous navigation, robotic manipulation, and AR/VR. Yet, despite its importance and recent advances in both 2D vision-language grounding and 3D reconstruction, the gap between human and machine performance in \emph{3D Vision-Language Grounding} remains one of the most significant challenges in embodied AI.

3D vision-language grounding (\textbf{3D VLG}) is severely constrained by data scarcity. Generative multimodal models are routinely trained on billions to trillions of tokens from language~\cite{Achiam2023GPT4TR, Touvron2023LLaMAOA}, images~\cite{Radford2021LearningTV, flux2023},
videos~\cite{Polyak2024MovieGA,videoworldsimulators2024}. Nearer to the task of 3D VLG, models trained for promptable image and video mask segmentation use millions to billions of labeled masks~\cite{Kirillov_2023_ICCV_SAM, ravi2024sam2}. 
In contrast, existing 3D VLG models are limited to training on only tens of thousands of labeled 3D masks and scenes, restricting their ability.
















In this paper, we propose a 
scalable pretraining approach for 3D VLG that enables grounding language into a set of target 3D masks; without requiring 3D mask labels. \emph{Language-Indexed Field Transfer with Gaussian Splatting} (\textbf{LIFT-GS}), is supervised on 2D frames directly.  It uses differentiable rendering to render the predicted 3D features and masks into 2D frames (e.g. via Gaussian Splatting~\cite{Kerbl20233DGS}).
\model{} is then distilled directly from powerful frame-based foundation models, using pseudolabels from SAM~\cite{Kirillov_2023_ICCV_SAM}, CLIP~\cite{Radford2021LearningTV}, and Llama 3.2~\cite{meta2024llama3}.







\model{} conducts both \emph{Promptable 3D Segmentation} and \emph{Reconstruction}. The render-supervised formulation is highly flexible; as it imposes essentially no constraints on the model input or network architecture. Therefore, in order to demonstrate the effectiveness of differentiable rendering for structured prediction tasks, we choose model inputs and meta-architectures that are scalable and offer advantages for 3D vision-language grounding.


As visual input, LIFT-GS exclusively utilizes point clouds (positions and colors) to align with the preferences for many modern embodied AI applications that operate in real time.  Point clouds from SLAM, for example, serve as widely adopted input in robotics and AR/VR. By relying on raw point clouds rather than 2D feature-enhanced point clouds like ConceptFusion~\cite{Jatavallabhula2023ConceptFusionOM}, \model{} eliminates the need for preprocessing and feature fusion during inference, reducing inference time from approximately 60 seconds per frame to just 1 second for the forward pass. 

\model{} follows the common MaskFormer~\cite{cheng2021perpixelclassificationneedsemantic} meta-architecture. However, the output tokens from the \emph{Encoder} and a \emph{Mask Decoder} are instead interpreted as 3D Gaussians rather than image patches. The entire pipeline is then trained end-to-end by optimizing the 2D loss between the rendered 2D masks and the pseudo-grounding masks. 



We validate the effectiveness of LIFT-GS on two popular 3D vision-language grounding (3D VLG) downstream tasks: open-vocabulary 3d instance segmentation and 3D referential grounding. The results show that distillation provides significant improvements over models trained from scratch, and achieves state-of-the-art performance on both tasks. More importantly, the approach consistently improves from both more data and better 2D foundation models, which indicates the potential for further improvement.

Intriguingly, we find that \model{} ``pretraining effectively multiplies the fine-tuning dataset''~\cite{hernandez2021scaling} by a constant factor (roughly 2x), across varying amounts of fine-tuning data. This somewhat counterintuitive observation indeed matches empirical data scaling laws for pretraining in other modalities~\cite{hernandez2021scaling}, where pretraining shifts the power-law scaling in log-dataset size to the left by that constant factor. The fact that the coefficient does not show diminishing returns, even when we use all available fine-tuning data, underscores that 3D VLG models are currently operating in the data scarce regime.



The seeming universality of scaling laws across modalities, and the effectiveness of \model{} pretraining on in 3D VLG settings, suggests that the underlying tools may also be useful at least in other 3D understanding settings. Specifically, render supervised framework can be used with essentially \emph{any} 3D/4D task or model, provided the results are renderable. This can be combined with knowledge distillation from frame-based models in order to overcome data scarcity in 3D, and \model{} specifically addresses a scarcity of 3D grounded masks.
For summary, our contributions are:




\begin{itemize}
\setlength{\itemsep}{1pt}       %
\setlength{\parskip}{1pt}       %
\setlength{\topsep}{1pt}        %

    \item \textbf{Differentiable rendering as a tool for training large-scale 3D promptable segmentation models.} Instead of proposing a new architecture, the contribution is to train the model using differentiable rendering with a structured 2D grounding loss.
    \item \textbf{A pseudo-labeling strategy for distilling a 2D vision-language grounding pipeline into 3D versions.} The pretrained 2D models are needed only for pseudo-labeling and they are not needed during inference.
    \item \textbf{State-of-the-art performance and realistic evaluations.} We demonstrate the effectiveness of the approach using sensor pointclouds, common in embodied settings. Rigorous experiments show state-of-the-art performance and reveal scaling properties. 
\end{itemize}
