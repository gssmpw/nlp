\input{figures/tabels/fig_ref_exp_ilu}

\section{Related Work}
\textbf{3D Vision-Language Grounding}
3D Vision-Language Grounding (3D VLG) refers to the task of mapping language descriptions of objects in a scene, to a set of corresponding 3D masks or bounding boxes that indicate the location of the objects in the observed scene. Despite its importance, there is very little annotated data for 3D VLG available | mainly because there are very little 3D data with annotated masks or bounding boxes period, as well as language annotation for each instance. This provides a major challenge for essentiall all existing models, since they require access to use ground-truth 3D masks or bounding boxes for trainings~\cite{Yuan2021InstanceReferCH, Roh2021LanguageReferSM, Yang2021SAT2S, Zhu20233DVisTAPT, Zhu2024Unifying3V}, and many require them during inference in the form of proposal masks or bounding boxes~\cite{Fang2024Transcrib3D3R, Zhang2023Multi3DReferGT}. This lack of annotated data is a major challenge for all existing models, as the number of 3D scenes with annotated masks or 3D bounding boxes is in the thousands~\cite{dai2017scannet, Yeshwanth2023ScanNetAH, Somasundaram2023ProjectAA}, compared to the millions of images and videos used for image and video segmentation~\cite{Kirillov_2023_ICCV_SAM}.

Unlike prior approaches, LIFT-GS introduces a render-supervised approach for 3D VLG. It uses differentiable rendering to train directly on image masks, without requiring 3D bounding boxes or masks during training or inference. This formulation is architecture agnostic and allows the model to be trained using only image and language losses, without the need for 3D masks or bounding boxes. LIFT-GS demonstrates this by training an end-to-end 3D feedforward model. LIFT-GS takes as input a RGB pointcloud and a language utterance as input, and outputs 3D masks.



\textbf{3D Instance Segmentation}
3D instance segmentation refers to the task of predicting masks for a set of instance in a 3D scene, not necessarily given a language prompt. For example, using a predefined set of category labels~\cite{Qian2021RecognizingSF, Schult2022Mask3DMT, Hou20183DSIS3S} or interactive segmentation using input ``prompts''~\cite{sal2024eccv, Zhou2024PointSAMP3, Ma2024FindAP, Chen2024PartGenP3}.

Because of the lack of 3D data with masks with which to train a 3D segmentation model, a class of recent work has emerged that trains no predictive 3D segmentation model at all, but instead directly lifts powerful pretrained image and video models to 3D using \emph{per-scene optimization and multiview geometry}. For example, by using depth-unprojection with heuristic view-merging strategies such as voxel-voting~\cite{conceptfusion, Zhou2024PointSAMP3, Xu2023SAMPro3DLS}. Another more recent approach to lift 2D segmentation to 3D is by using differentiable rendering to optimize the 3D representations to minimize the 2D loss between rendered outputs and ground-truth RGB values, 2D features~\cite{lerf2023, garfield2024, Dou2024TactileAugmentedRF, Chen2024RealAF, Gu2024EgoLifter}, or masks~\cite{Cen2023SegmentAI, Xu2023NeRFDetLG}. These approaches leverage the large-scale data available for image and video segmentation, but use a fixed per-scene lifting process that does not improve with more training data and takes a long time to run (e.g. minutes per scene on a H100).

These lifted 3D pseudolabels can be used to partially address the lack of 3D data with ground-truth masks, by pre-training a 3D instance segmentation model on the 3D pseudolabels~\cite{Genova20213DVLearning3Dwith2D, Peng2023OpenScene}.
However, this fixed lifting pipeline introduces accumulated errors from inaccuracies in 3D reconstruction and label-merging strategies, creating a bottleneck for overall performance.


LIFT-GS instead trains a promptable 3D segmentation model using only 2D losses, eliminating the need for a fixed lifting process and label-merging strategies. Not relying on heuristic lifting methods or specialized losses (e.g., cross-mask contrastive loss), LIFT-GS is trained on a large-scale dataset using the same mask losses (\emph{Dice}, \emph{Cross-Entropy Focal}) as the original frame-level segmentation models. As shown in Appendix Section~\ref{sec:comparison_3d_pseudolabels}, LIFT-GS features trained directly on 2D pseudolabels outperform these lifted 3D pseudolabels. And we show effective data scaling in Section~\ref{sec:data_scaling}.


\textbf{Render-Supervised 3D}
While photometric losses using differentiable rendering~\cite{mildenhall2020nerf, Kerbl20233DGS} were originally used for per-scene optimization,  \emph{render-supervision} is emerging as a powerful and generic technique to train predictive 3D models using only 2D losses. For 3D reconstruction, 2D photometric losses can be used to train strong feedforward models~\cite{hong2024lrm, Tang2024LGMLM}. PonderV2~\cite{Zhu2023PonderV2PT} instead uses render-supervision for representation learning, by adding additional constrastive losses on the rendered pixels, using ground-truth category labels to pretrain an \emph{Encoder}. 

LIFT-GS introduces render-supervision for 3D VLG, using mask- and grounding-losses typically used for 2D VLG. LIFT-GS jointly trains an \emph{Encoder} and \emph{Mask Decoder}, and for the first time demonstrates using differentiable rendering to train a 3D mask decoder. 














