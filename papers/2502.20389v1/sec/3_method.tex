\section{Method}

\input{figures/tabels/fig_psedu_engine}

\input{figures/tabels/fig_method}

\model{} is a (pre)training pipeline for 3D vision-language grounding models that uses only 2D supervision. It accomplishes this using differentiable rendering and structured 2D losses, and uses pseudo-labeling for frame annotations.






\subsection{Task Formulation}

The formulation of 3D Vision-Language Grounding used in \model{} is shown in Figure~\ref{fig:ref-exp-illu}, and based on MDETR~\cite{Kamath2021MDETRM}. The model takes as input a language query, such as \emph{``the black \textcolor{red}{chair} close to the \textcolor{blue}{table} near the \textcolor{green}{wall}''} and a pointcloud input. The objective is to predict 3D gaussian masks, for all mentioned noun phrases (\textcolor{red}{chair}, \textcolor{blue}{table}, \textcolor{green}{wall}); ensuring that the predicted masks adhere to the semantic relationships described in the language query, via a mask-to-query correspondence matrix.

As shown in Figure~\ref{fig:method-arch}, \model{} takes as input a point cloud, $\mathbf{P}$, and language query embeddings, $\mathbf{Q}$. It outputs 3D feature Gaussians $\mathbf{G}$ with a correspondence matrix $\mathbf{C}$.
\vspace{-1mm}
\begin{equation}
    \text{LIFT-GS}{:}~(\mathbf{P}, \mathbf{Q}) \mapsto (\mathbf{G}, \mathbf{C}),
\end{equation}
\vspace{-5mm}




The pointcloud $\mathbf{P} \in \mathbb{R}^{|P|{\times}6}$ contains positions and RGB colors. \model{} is trained with pointclouds from unprojected posed multi-view RGBD, but any pointcloud reconstruction will work; including pointmap-based methods that do not require depth or camera pose~\cite{Wang2023DUSt3RG3, Yang2025Fast3RT3, Wang2025Continuous3P}.
This makes the method highly flexible, applicable across diverse scenarios, from sparse-view settings to extremely long video sequences.

The Language queries are encoded into embeddings $\mathbf{Q} \in \mathbb{R}^{|Q|{\times}F_{Q}}$, i.e. via CLIP~\cite{Radford2021LearningTV} in our design.
Each embedding can be either from a single word or a full sentence, depending on the desired ganulariry of grounding. 

The model predicts 3D Gaussians  $\mathbf{G} \in \mathbb{R}^{|G|{\times}|F_{G}|}$ with auxilliary features, enabling the differentiable rendering of 3D information into 2D. In \model{}, the first part of channels of $F_{G}$ are standard Gaussian splatting parameters (i.e. positions $\mu$, covariances $\Sigma$, 3rd-degree SH coefficients $\psi$). The remaining channels are renderable \emph{auxiliary 3D attributes}: predicted 3D masks $\mathbf{M} \in \mathbb{R}^{|G|{\times}m}$, and renderable \emph{ auxiliary features}: predicted features $\mathbf{F} \in \mathbb{R}^{|G|{\times}F}$.



Following the trend in mask grounding~\cite{liu2023grounding}, the predicted 3D masks $\mathbf{M} \in \mathbb{R}^{|G| \times m}$ contains $m$ predicted masks for each Gaussian, where $m$ is the mask proposal number. Consequently, the rows in the correspondence matrix $\mathbf{C} \in \mathbb{R}^{m \times |Q|}$ represent the log-probability that the $i$-th predicted mask corresponds to each embedding of $\mathbf{Q}$.












\subsection{Training 3D with 2D Supervision}
Predicting a 3D Gaussian Splatting with desired 3D attributes enables training a 3D model using only 2D supervision, eliminating the need for large-scale 3D annotations. In this section, we first discuss the 2D losses used for training and then describe the process of generating pseudo-labels when perfect labels are unavailable. We show ablations for both losses and pseudo-labels in the experiments.

\model{} utilizes three 2D losses to train the model: $\mathcal{L}_{\text{RGB}}$, $\mathcal{L}_{\text{feat}}$ and $\mathcal{L}_{\text{ground}}$. The rendered image, feature map, and 2D mask are denoted in the equations below as $\tilde{I} \in \mathbb{R}^{H \times W \times 3}$, $\tilde{\mathbf{F}}_{\text{2D}} \in \mathbb{R}^{H \times W \times F}$, and $\tilde{\mathbf{M}}_{\text{2D}} \in \mathbb{R}^{H \times W \times m}$ , respectively. Their corresponding ground-truth counterparts are $I$, $\mathbf{F}_{\text{2D}}$, and $\mathbf{M}_{\text{2D}}$, where $\mathbf{M}_{\text{2D}} \in \mathbb{R}^{H \times W \times K}$.





\textbf{Reconstruction losses:}
$\mathcal{L}_{\text{RGB}}$ is the 2D photometric loss,  formulated as a combination of $\mathcal{L}_1$ loss and SSIM loss~\cite{Wang2004ImageQA}:

\begin{equation}
    \mathcal{L}_{\text{RGB}} = \lambda_1 \mathcal{L}_1(I, \tilde{I}) + \lambda_2 \mathcal{L}_{\textbf{SSIM}}(I, \tilde{I})
\end{equation}



\textbf{Grounding losses:}
The grounding losses, $\mathcal{L}{\text{mask}}$, measure the prediction quality between the rendered 2D masks, $\tilde{\mathbf{M}}_{\text{2D}}$, and the ground-truth masks, $\mathbf{M}_{\text{2D}}$. 
Since the predicted and ground-truth masks differ in number, we employ Hungarian matching to compute the set-to-set loss. $\sigma(i)$ is the index of predicted masks matching to $i$-th ground truth mask according to the matching distance $\mathbf{d}_{\text{match}}$.

\begin{equation}
    \sigma(i) = \arg \min_{j}\mathbf{d}_{\text{match}}(\tilde{\mathbf{M}},\mathbf{M}_i, \mathbf{C})
\end{equation}



The mask shape is supervised with $\mathcal{L}_{\text{mask}}$, the combination of \emph{Focal}~\cite{Lin2017FocalLF} and \emph{Dice}~\cite{Sudre2017GeneralisedDO} loss used in SAM~\cite{Kirillov_2023_ICCV_SAM}:

\begin{minipage}{\linewidth}
\small

\begin{equation}
    \mathcal{L}_{\text{ground}}\! = \!
    \frac{1}{K}\sum_{i}^{K} \lambda_{\text{3}}\mathcal{L}_{\text{mask}}(\tilde{\mathbf{M}}_{\text{2D}}^{\sigma(i)}, \mathbf{M}_{\text{2D}}^{i})\! + \!\lambda_{\text{4}}\mathcal{L}_{\text{CE}}(\mathbf{C}_{\sigma(i)}, i)
\end{equation}


\end{minipage}


$\mathcal{L}_{\text{CE}}$ is the correspondence loss, encouraging correct correspondences between predicted masks and language tokens. Specifically,
the ground-truth mask set $\mathbf{M}_{\text{2D}}$ consists of $K$ binary masks, each corresponding to a span $\mathbf{Q}_k \subseteq \mathbf{Q} $ of language query embeddings $\mathbf{Q}$. $\mathbf{C}_{\sigma(i)}$ represents the correspondence probability of the $\sigma(i)$-th mask proposal across spans of the $|Q|$ language embeddings. We optimize this using a cross-entropy loss.

\vspace{-3mm}
\begin{equation}
    \mathcal{L}_{\text{CE}}(\mathcal{C}_{\sigma(i)}, i) = - \log \frac{\exp(\mathbf{C}_{\sigma(i),i})}{\sum_{j}^{K}\exp(\mathbf{C}_{\sigma(i),j})}
\end{equation}
\vspace{-3mm}



\textbf{Feature loss:}
$\mathcal{L}_{\text{feat}}$ is the feature rendering loss which measures the difference between rendered feature map $\tilde{\mathbf{F}}_\text{2D}$ and ground-truth feature map $\mathbf{F}_\text{2D}$. As in~\cite{zhu2023ponderv2}, \model{} uses a contrastive loss.
\vspace{-3mm}

\begin{equation}
    \mathcal{L}_{\text{feat}} =\frac{1}{H \times W}\sum_{u,v}^{H,W} - \log \frac{\exp(\tilde{\mathbf{f}}_{(u,v)} \cdot\mathbf{f}_k) }{\sum_{j} \exp(\tilde{\mathbf{f}}_{(u,v)} \cdot \mathbf{f}_j)}
\end{equation}
\vspace{-3mm}

$\tilde{\mathbf{f}}{(u,v)}$ is the feature vector of $\tilde{\mathbf{F}}_\text{2D}$ at location $(u, v)$. ${\mathbf{f}}_{j}$ denotes a batch of unique feature vectors from $\mathbf{F}$, and $\mathbf{f}k$ is the ground-truth feature vector corresponding to $\tilde{\mathbf{f}}{(u,v)}$. 


\subsection{SAM-CLIP 2D Pseudo-Label}
\label{sec:pseudolabels}
Although \model\ doesn't need 3D annotations, getting high-quality 2D supervision remains challenging. We show how leveraging 2D foundation models for pseudo-label generation enables reasonable zero-shot performance and significantly enhances downstream tasks after fine-tuning. 

As shown in Figure~\ref{fig:pseudo-label-gen}, we leverage 2D foundation models to generate pseudo-labels, including \emph{pseudo language queries} and corresponding 2D masks. For each image, we apply SAM~\cite{Kirillov_2023_ICCV_SAM} to obtain segmentation masks. For each segmented region, we extract a CLIP~\cite{Radford2021LearningTV} image embedding as the \emph{pseudo language query embedding}. Since CLIPâ€™s text and image embeddings share the same feature space, \model\ can take text embeddings as input during inference. During pretraining, we concatenate these CLIP embeddings to form $\mathbf{Q}$ and construct $\mathbf{C}$, using the corresponding 2D masks as ground truth.


\input{figures/tabels/fig_zero_shot}
Trained with 2D pseudo-labels, \model\ can perform zero-shot 3D grounding using real text queries without fine-tuning, as shown in Figure~\ref{fig:zero-shot}. However, the zero-shot model suffers from low accuracy and struggles with complex expressions, a common limitation of CLIP-based methods that function as bag-of-words models~\cite{yuksekgonul2023when}. Future improvements in pseudo-labeling, such as captioning~\cite{meta2024llama3} and 2D language grounding models~\cite{liu2023grounding}, could alleviate the need for fine-tuning altogether.
In this work, we focus on fine-tuning. 

Overall, \model\ shows that even simple pseudo-labeling strategies can be effectively distilled into 3D models. Our experiments show that pretraining with 2D pseudo-labels substantially boost performance on downstream tasks.






\subsection{Architecture}
\model\; is network-agnostic, imposing few architectural constraints and being readily adapted to other architectures. 
We describe the network used in experiments, as in Figure~\ref{fig:method-arch}.

\noindent\textbf{\emph{Encoder}: Predicting 3DGS from Point Cloud} 
\label{sec:method:3dgs}

\emph{Encoder} takes point clouds as input and predicts  \emph{3D Gaussian splatting} (without masks) and \emph{per-point features} as latent variables. 
Generally, the number of predicted Gaussians $|G|$ can differ from the number of input points $|P|$. 

However, our focus is on demonstrating the effectiveness of differentiable rendering for structured prediction tasks like \emph{Promptable 3D segmentation}. So in our experiments, we set $|G| = |P|$ with a bijective mapping to ensure consistency with point cloud-based evaluation tasks.
For comparison to prior work~\cite{zhu2023ponderv2} , \model{} uses a SparseConv UNet~\cite{spconv2022} as the \emph{Encoder Backbone} to get \emph{per-point features}, and uses an MLP as \emph{Gaussian Head} to regress Gaussian parameters of $\mathbf{G}$ with features $\mathbf{F}$.














\noindent\textbf{\emph{Decoder}: Mask Decoder for 3D VLG}
\label{sec:method:language-grounding}

The \emph{Decoder} takes \emph{per-point features} from the \emph{Encoder} and language embeddings $\mathbf{Q}$ as input to predict the 3D mask $\mathbf{M} \in \mathbb{R}^{N \times m}$ and the correspondence matrix $\mathbf{C} \in \mathbb{R}^{m \times |Q|}$. The predicted $\mathbf{M}$ serves as a renderable feature of the 3DGS $\mathbf{G}$ and is subsequently used to generate 2D masks.
While this work focuses on 3D vision-language grounding (3D VLG), the proposed framework is a general pipeline that can be extended to other 3D tasks with renderable outputs.

We adopt a 3D variant of the Transformer-based mask decoder in MaskFormer~\cite{Cheng2021PerPixelCI, jain2025rodin}, inspired by the success of MDETR~\cite{Kamath2021MDETRM} and BUTD~\cite{Jain2021BottomUT}. As illustrated in Figure~\ref{fig:method-arch}, the 3D Mask Decoder Transformer takes three inputs: \emph{visual tokens} from \emph{per-point features}, \emph{language tokens} from $\mathbf{Q}$, and $m$ learnable \emph{mask proposal tokens}. It outputs $m$ predicted \emph{mask embeddings} and a correspondence matrix $\mathbf{C}$.
The 3D masks $\mathbf{M}$ are computed as the dot product between \emph{visual tokens} and output \emph{mask embeddings}. These similarity scores are then differentiably rendered into mask images $\tilde{\mathbf{M}}_{\text{2D}}$.



