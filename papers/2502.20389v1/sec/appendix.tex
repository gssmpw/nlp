\section{More Details}

\subsection{Training Details}
\model takes point clouds and posed RGB images for training.
For efficiency, we preprocess point clouds and posed RGB images, caching the processed features.

Point clouds originate from multi-frame RGB-D scans. We unproject them using depth information and fuse the unprojections into the final point clouds. Each dataset sample is preprocessed into 5cm-resolution point cloud chunks with corresponding posed RGB images.

For 2D pseudo-labels, we precompute SAM-CLIP features and cache them. Given the large size of the feature map, we decompose it into two components: \emph{Semantics} and \emph{Index2Semantics}.

\emph{Semantics}: A tensor of shape $H \times W$, where each pixel stores the index of the segment it belongs to.
\emph{Index2Semantics}: A tensor of shape $N \times F$, where $N$ is the number of unique segments, and $F$ is the CLIP feature dimension.
This decomposition significantly reduces storage costs. When computing the feature rendering loss $\mathcal{L}_{\text{feat}}$, we directly use features from \emph{Index2Semantics} for contrastive loss.

Each training sample consists of a sparse point cloud and a posed image with corresponding SAM-CLIP features. We randomly sample up to 8 unique instances, using their CLIP features as \emph{pseudo language queries} and their masks as target 2D masks. To ensure mask quality, we filter out masks smaller than 1024 pixels.

Randomly sampling instances is important for training, especially for zero-shot segmentation, as it prevents the model to reconstruct the whole images given all the input embeddings. 

For grounding loss, we assign weights of 15.0, 2.0, and 6.0 to the mask cross-entropy loss, soft token loss, and Dice loss, respectively. We also use a photometric loss (L1 and SSIM) with a weight of 1.0 and a feature loss with a weight of 0.1.

UNet Encoder: 8 layers, maximum channel dimension of 256, output feature dimension of 96.
MaskDecoder: 8-layer Transformer decoder with a hidden state size of 512. It uses 256 learnable mask proposal tokens, generating 256 masks. Each Transformer block has 8 attention heads, a feedforward MLP of dimension 2048, and a dropout ratio of 0.15.
Language Encoder: We use \texttt{clip-vit-large-patch14}, with a feature dimension of 768.


\subsection{Comparison to 3D pseudolabels}
\label{sec:comparison_3d_pseudolabels}
\input{figures/tabels/comparison_conceptfusion}

\subsection{Data Scaling Results}
\label{sec:supp_data_scaling}
A similar trend is observed for 3D open-vocabulary instance segmentation, though the benefits of pretraining are slightly less pronounced due to the task's lower complexity. This aligns with our findings that pretraining is more beneficial for challenging tasks, such as those with higher IoU thresholds or greater complexity.


\subsection{VLM Captions}

\input{figures/tabels/fig_3dins_finetune_data_scaling}
We explore using vision-language models (VLMs) to generate captions for each SAM-segmented object and encode these captions into CLIP embeddings as \emph{pseudo language queries}.

Specifically, given a SAM-segmented region, we draw a red bounding box on the 2D image and highlight the masked region using alpha blending, as shown in Figure~\ref{fig:apx_caption}. We then prompt a VLM, such as LLama-3.2v, with the following instruction:

You are a helpful assistant for image captioning. You are given an image with a red bounding box specifying the object of interest. Caption that object in a few words, keeping it precise and concise. The object is also slightly highlighted. Examples output: "a red traffic light," "the box near the wall." Just output the caption; no other text is needed.

This approach leverages VLM-generated textual descriptions to improve pseudo-language queries for training.

\section{Discussion and Limitations}
\begin{figure*}
    \centering
    \includegraphics[width=0.8\textwidth]{figures/material/vlm_caption.pdf}
    \caption{\textbf{Input Image to VLM for Captions.} Given the segments from SAM, we draw red bounding box around the segments and ask VLM models to describe the segments inside the red bounding boxes.}
    \label{fig:apx_caption}
\end{figure*}
The core contribution of \model\ is training a 3D model without 3D supervision by leveraging differentiable rendering and distilling knowledge from 2D foundation models. This approach is novel and motivated by the fact that 2D foundation models, trained on vast amounts of 2D data, currently outperform any existing 3D model. Distilling knowledge from these powerful 2D models presents a promising and scalable direction for 3D learning.

Our proposed pipeline is general and unified. Beyond 3D masks, any renderable 3D attributes can, in principle, be trained using 2D supervision. This idea could extend to dynamic scenes and other properties, opening new opportunities for 3D model training.

However, \model\ is inherently constrained by how well we leverage 2D foundation models for pseudo-labeling. Currently, we use CLIP image embeddings as text queries, but CLIPâ€™s claim of a shared embedding space for images and text is imperfect. In practice, these embeddings can differ significantly, leading to challenges in zero-shot 3D segmentation.

Our CLIP-SAM features may not be optimal pseudo-labels for pretraining, and we anticipate that improved pseudo-labeling strategies will lead to better scaling properties, stronger performance, and even robust zero-shot 3D segmentation without fine-tuning. Addressing our current limitations presents a key opportunity for future work.

Although \model\ significantly improves performance and surpasses the single-stage SOTA method BUTD-DETR~\cite{Jain2021BottomUT}, it still falls short of two-stage SOTA methods like 3D-VisTA~\cite{Zhu2024Unifying3V} on 3D referential grounding at an IoU threshold of 0.5. A robust single-stage 3D VLG model would have a major impact across various applications. We hope that our architecture-agnostic pretraining pipeline can further enhance future models.
