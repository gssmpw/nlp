%\subsection{Data}
%Some prior work also used Booking.com data~\cite{ALDERIGHI2022769}

\vspace{-.5pc}
\paragraph{Data.}
We constructed a dataset of hotel reviews sourced from \texttt{Booking.com},\footnote{Data processing code: \href{https://github.com/Crowd-AI-Lab/Contextually-Aligned-Online-Reviews}{https://github.com/Crowd-AI-Lab/Contextually-Aligned-Online-Reviews}} which has been used in prior research studies~\cite{ALDERIGHI2022769,barnes-etal-2018-multibooked}.
%This dataset included 4,447,853 reviews in both \twChinese and \cnChinese, written by users from Taiwan and Mainland China. 
%This dataset consists of 4,447,853 reviews labeled as written in Chinese by the platform, written by users who self-identified as being from Taiwan and Mainland China. 
This dataset consists of 4,447,853 reviews labeled by the platform as written in Chinese.
%, authored by users who self-identified as being from Taiwan and Mainland China.
%The reviews encompass 149,879 hotels across Japan, Mainland China, South Korea, Taiwan, Thailand, and Vietnam, and were gathered between August 2021 and August 2024.
The reviews cover 149,879 hotels located in Japan, Mainland China, South Korea, Taiwan, Thailand, and Vietnam, and were collected from August 2021 to August 2024. 
These locations were selected to ensure a substantial volume of data, as they are popular destinations for Mandarin-speaking travelers.
%\kenneth{TODO Zixin: Update the locations and time span} 
%The dataset comprises both positive and negative feedback, ratings, room types, and traveler labels. 
%We constructed a dataset that consists of the hotel reviews from \texttt{booking.com}, which is a data source for a series of prior studies~\cite{ALDERIGHI2022769,barnes-etal-2018-multibooked}. \kenneth{TODO Sam: Find one or two more papers from NLP conference that also used Booking.com data.}
%We deliberately selected locations outside Taiwan and Mainland China to avoid potential biases associated with users posting reviews from a particular language variety's region.
Each review comprises three main components: the review title, positive feedback, and negative feedback.
Additionally, it includes review ratings (ranging from 1 to 10 stars) and metadata such as hotel ID, posting time, and more (see \Cref{app:booking-data-sample} for an actual sample). 
Booking.com claims to invest significant effort in ensuring that reviews are posted by real users and in maintaining review quality. 
%Based on their assertion, we chose not to conduct extensive data cleaning. 
We included only non-empty reviews, meaning reviewers provided input in at least one of the following: 
the review title, positive feedback, or negative feedback. 
In total, we collected 1,513,056 reviews written in Chinese.
%\kenneth{TODO Zixin: Update the numbers}


%\kenneth{TODO GG and HH: Please describe how you scraped Booking.com. What region (and why)? What time span (and why)? How many raw reviews were collected? How did you select hotels (and why?}

%\paragraph{Data Content and Pre-Processing.}

% \kenneth{TODO Zixin and CY: Describe how you cleaned and pre-processed the data. Mostly (1) We believe Booking.com already made sure all the comments are real. (2) Each review has what information: title, pos, neg, and all the meta data. (3) we had three settings (and why): (i) original, (ii) plain, and (iii) shuffled. Say we use original to calculate the length.}

%\begin{itemize}
    %\item \textbf{Language Identification:}\kenneth{TODO: CY will figure out}
    %\item \textbf{Short Comments:}\kenneth{What is the criteria? (Chinese word seg?)---CY will figure out, maybe use LLMs to plot accuracy drop (will it drop?)--- Pay attention to bias}
    %\item \textbf{Bot-like/Dup Comments:}\kenneth{Zixin will look into is to figure out the model}
    %\item \textbf{User Accounts Acting Like Bots:} (1) Always post the same content\kenneth{Zixin--Find papers used Booking.com data and see how they cleaned it}; 
    %(2) Unreasonable stay schedule\kenneth{Can random people leave reviews? Or do you need to stay in the hotel? YES??????}
    %\item \textbf{Lack of user id.....}
%\end{itemize}
%\paragraph{Aligning Reviews Using Contexts.}

%\kenneth{TODO Zixin and CY: Describe how you paired the reviews.}

%+Zixin should just use nationality
%+Chieh-Yang will run lang detection tool

\vspace{-.6pc}
\subsection{Contextually Aligning Reviews}
%We only included reviews with textual input, where reviewers have input in either the review title, the positive feedback, or the negative feedback. 
%The users' self-specified ``nationality/region'' labels, which are required by the Booking.com platform, defined the users' language varieties in this study. 
%We then aligned and formed review pairs following the rules below:
We used users' self-specified ``nationality/region'' labels from Booking.com to determine the reviews' language varieties. %in this study. 
In total, we collected 1,403,669 reviews written in \twChinese and \cnChinese, where 95.591\% of them come from \twChinese users.
To ensure a balanced representation between \textbf{\twChinese (TW)} and \textbf{\cnChinese (CN)} reviews, we paired them based on the following criteria:
%we employed a sampling method that pairs reviews in two varieties using the following criteria:
%We then paired \twChinese reviews with \cnChinese reviews according to the following criteria:


\vspace{-.8pc}

\begin{itemize}[leftmargin=*]
%\item \textbf{Both reviews contribute to the same hotel:} Both reviews in the same pair are from the same hotel. Such a pairing rule ensures that reviewers comment on highly similar scenarios or objects, which is the target hotel in our study.
\item
\textbf{Same hotel for both reviews:} 
Both reviews in each pair are from the same hotel, ensuring that the reviewers are commenting on similar scenarios or objects---the hotel itself.



\vspace{-.5pc}

    
%\item \textbf{Both reviews share similar ratings:} To maximize the size of paired reviews while maintaining similar sentiment components in each pair of reviews, we selected pairing candidates with a 3-class label scheme ( 1-3 as negative reviews, 4-7 as neutral reviews, and 8-10 as positive reviews). This process helps align reviews with similar ratings to form comparable pairs.
\item
\textbf{Similar ratings for both reviews:} 
To form comparable pairs with similar sentiments, we used a 3-class rating scheme (1-3 as negative, 4-7 as neutral, and 8-10 as positive) and paired reviews based on this classification. 
This approach maximizes the number of review pairs while maintaining comparable sentiment.



\vspace{-.5pc}

%\item \textbf{Both reviews have a similar amount of text input:} To minimize the effect of input size, we classified all reviews into 10-token wide bins before the pairing process. This ensures the paired reviews will share a similar size of text input. We remove reviews longer than 500 tokens to enhance the overall quality of the data set (see \cref{app:length-exp} for more information regarding potential impacts of text length).\zixin{we need a further explanation on this.}
\item
\textbf{Similar text length for both reviews:} 
%To minimize the effect of input text length, 
To ensure paired reviews have similar text lengths, we grouped reviews into 10-token bins before pairing and required both reviews in each pair to fall within the same length bin.
%This ensures paired reviews have similar text lengths.
% 22 r
Reviews longer than 500 tokens were excluded (see \Cref{app:length-exp}.)
%\kenneth{TODO Zixin: Update numbers}


\end{itemize}



\vspace{-.5pc}


The final dataset contained 22,918 review pairs, each with one TW and one CN user review.








%Following the criteria, the final paired dataset contained 22,918 pairs of reviews, with each pair containing one review from a \twChinese user and one from a \cnChinese user.
%from \twChinese users and \cnChinese users. 
%Each pair 
%making 45,836 data entries in total.

\subsection{Data Quality Validation\label{sec:data-quality-validation}}
%To validate the data quality, we recruited 10 participants, evenly divided between native speakers of \twChinese (TW) and \cnChinese (CN), to rate 200 randomly selected reviews of each language variety.
%To validate the data quality, 
%We recruited five native speakers of \twChinese (TW) to review 200 randomly selected reviews written in \twChinese. 
%We followed the same process for reviews in \cnChinese (CN).
Five native speakers of \twChinese reviewed 200 random \twChinese reviews; the same process applied to \cnChinese.
The focus was on two key aspects: {\em (i)} \textbf{writing quality} and {\em (ii)} \textbf{content-rating agreement}, evaluated on a 5-point Likert scale (see Appendix~\ref{app:human-validation}.) 
%Evaluations were conducted using a 5-point Likert scale, with 1 indicating strong disagreement and 5 indicating strong agreement.
%Appendix~\ref{app:human-validation} shows the material used. 
Each participant was paid \$10.
%\kenneth{TODO Sam: Describe human validation process and results}
%To validate the data quality,
%To construct a contextually-aligned review dataset for language varieties,
%we recruited 10 participants, evenly divided between native speakers of Taiwan Mandarin (TW) and Mainland Mandarin (CN), to rate 200 hotel reviews. The focus was on two key aspects: writing quality and content-rating agreement. Evaluations were conducted using a 5-point Likert scale, with 1 indicating strong disagreement and 5 indicating strong agreement. 
%The 200 comments from Taiwan and China were paired based on the same hotel, similar length, and category ratings. \sam{seems to be mentioned in the previous paragraph}
%\kenneth{TODO Sam: Report the mean and SD for each group here.}
%\sam{done}
As a result, for the writing quality ratings, the TW group had a mean of 4.18 (SD=0.44), and the CN group had a mean of 3.94 (SD=0.49). 
Regarding the rating-content agreement, the TW group had a mean of 4.00 (SD=0.46), and the CN group had a mean of 3.56 (SD=0.55).
%\kenneth{Displaying two decimal (instead of 3) places is sufficient}
%The average rating correlation between participants for writing quality was 0.22 for the TW group and 0.14 for the CN group.
%For content-rating agreement, the average rating correlation between participants was 0.21 for the TW group and 0.17 for the CN group.
%\kenneth{TODO Sam: (1) Displaying two decimal places is sufficient; our sample size isn't large enough to justify showing four decimal places. (2) Calculate (a) the avg correlation between two TW raters, and (b) the avg correlation between two CN raters. This is to show the inner-annotator agreement (IAA) in each group.}
%To validate the dataset's quality, 
%The correlation between the CN and TW groups was 0.258 for writing quality and 0.194 for content-rating agreement. These low positive correlations suggest limited agreement between the groups, indicating potential differences in evaluation criteria or perspectives.


%We calculated the Mean Squared Error (MSE) between the evaluations of TW and CN groups for both  writing quality and content-rating agreement, resulting in values of 0.3766 and 0.5738, respectively.
%\kenneth{I'd report avg corelation between two participants in each group. MSE is not very straighforward.} \sam{done!}

% These MSE values indicate a moderate level of agreement between the two groups. The relatively low MSE for writing quality suggests consistent evaluations across locales, enhancing the dataset's reliability for further analysis. However, significant differences were found in the Two Paired Samples T-Test between the CN and TW groups for both writing quality and content-rating agreement.\sam{singificant found which means CN vs TW groups are not aligned, probably find some interesting examples to address --> may delete if not enough space}
% Regarding this, we tried to study the example with the highest MSE within top 10\% of the example. In content-rating agreement, it is interesting that CN groups are consistently rating lower compared to TW group. And nearly all the comments are from the "positive" category (only 1 from neutral, and no from negative).
% In writing quality, it is also the similar case that most of all comments deviated are from positive comment (only 2 from neutral, and also no from negative)

%\kenneth{I don't feel we have space for expert vs. non-expert...}
%Considering the evaluation of an expert with a Master's degree in Hospitality Management from the CN group, we conducted a Wilcoxon Signed-Rank Test for paired samples between expert and non-expert evaluations. Significant differences were observed in both writing quality $(p\text{-value} = 7.08 \times 10^{-10})$ and content-rating agreement $(p\text{-value} = 7.76 \times 10^{-21})$. Additionally, the expert's ratings showed higher variability (SD = 0.91, 0.86) compared to non-experts (SD = 0.47, 0.57), suggesting that the expert's background may lead to more nuanced evaluations, reflecting deeper insights or different criteria compared to non-experts. 
%\sam{BUT if I do SD for individual participants one by one for each aspect, P1 (expert) rating is not that greatly deviated compared}
%We acknowledge that the sample size for the expert is small, indicating that further evaluation from more experts could be valuable in future studies.

%\begin{itemize}
%   \item The writing quality is reasonable.
%  \item The rating reflects the content of the review.
%\end{itemize}

%\subsection{Data Quality Validation}
