
%This paper presents a novel and cost-effective method for benchmarking model performance across language varieties. 
%We propose using international online review platforms as data sources to capture comments in different language varieties from similar real-world contexts.
This paper introduces a cost-effective method for benchmarking model performance across language varieties using international online reviews from similar contexts.
To validate this, we built a contextually aligned dataset of \twChinese and \cnChinese reviews and tested six LLMs on sentiment analysis, finding that LLMs consistently underperform in \twChinese. 
%Our approach demonstrates the potential for evaluating performance gaps across language varieties, with future work aiming to extend this method to more varieties for reliable benchmarking and addressing performance disparities. The ultimate goal is to develop models that perform equally well across diverse language varieties and tasks.
%\kenneth{TODO: A few sentences here to conclude the work.}
We aim to extend this approach to more language varieties, with the ultimate goal of creating LLMs that perform equally well across them.







%In the future, we aim to extend the proposed approach to more language varieties.
%, enabling reliable benchmarking and helping address performance disparities. 
%Our ultimate goal is to create LLMs that perform equally well across diverse language varieties.