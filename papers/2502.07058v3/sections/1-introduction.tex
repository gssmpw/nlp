

A language can have different varieties. 
Of the world's 7,000 languages, sixty (60) million people speak British English, 23 million speak \twChinese, and 10 million speak European Portuguese, compared to 330 million, 900 million, and 200 million who speak American English, \cnChinese, and Brazilian Portuguese, respectively. 
%Although often mutually intelligible, 
These varieties differ enough in accent, vocabulary, or syntax for native speakers to distinguish them. 
NLP technologies, including LLMs, are known to perform better in English varieties that are more widely represented in the internet data they are trained on, particularly Mainstream American English (MAE), compared to less represented varieties like African American English (AAE)~\cite{valuebench,multivaluebench}.
Specifically, LLMs more accurately predict sentiment scores in MAE~\cite{valuebench}, 
generate higher-quality texts in MAE~\cite{valuebench}, and 
hold better conversations in MAE~\cite{multivaluebench}.
These comparisons were made possible by intensive, targeted efforts specific to each language variety, such as 
%developing a language variety classifier to partition test sets by variety, or applying linguistic rules to 
``translating'' data instances from a standard variety (\eg, MAE) to less widely represented varieties (\eg, AAE), followed by validation from native speakers~\cite{valuebench,multivaluebench}.
%of the less-represented varieties (\ie, AAE in the case of MAE and AAE)
What is not known is whether these performance gaps and biases extend to a broader range of languages and their numerous varieties, 
such as \cnChinese versus \twChinese.
%American English versus British English, %Brazilian Portuguese versus European Portuguese, 
%or European Spanish versus Central/South American Spanish varieties. 
Building effective benchmarking datasets for evaluating model performance across language varieties is expensive---creating ``fair'' comparisons between varieties often needs native speakers and language experts.
%to pair data accurately. 
%There is a pressing need for a reliable, generalizable, and affordable method to measure the performance gaps of LLMs across different varieties of any given language. 
%Failing to meet this need risks relying on biased information extracted, derived, or predicted by LLMs---a common practice in modern AI-powered systems---from underrepresented language varieties spoken by millions, leading to discrimination or even harmful decision-making against certain populations.

\begin{figure}[t]
    \centering
    \includegraphics[width=.9\columnwidth]{figure/contextually-paired.png}
    \vspace{-.5pc}
    \caption{Online review platforms can be data sources to build datasets that capture comments in different language varieties from similar real-world scenarios. These \textit{contextually aligned} datasets can then be used to benchmark LLMs' performance across language varieties.}
    \vspace{-1.5pc}
    %Everything, including the new artrile, in \dataset is CC-licensed and releasable.
    \label{fig:overview}
\end{figure}


%This paper aims to
%The long-term goal of this proposal is to 
%enable the development of language models that perform equally well across different language varieties, such as Taiwanese Mandarin and Mainland Mandarin, or Mainstream American English (MAE) and African American English (AAE), across a wide range of language tasks. 
%As a much-needed step toward this goal, the overall objective of our proposal is to, 
%In this paper, 
Using Mandarin Chinese as an example, we propose an approach that uses large-scale user-generated reviews to construct benchmarking datasets across varieties of a given language. 
%The central hypothesis is that 
We argue that the international online review platforms with millions of users, like Booking.com, when properly curated, can serve as effective data sources for constructing datasets that capture \textbf{comments in different language varieties from similar real-world scenarios}, like comments for the same hotel with the same rating using the same language (\eg, Mandarin Chinese) but different language varieties (\eg, \twChinese, \cnChinese). 
%, Singaporean Mandarin
These datasets, being \textbf{contextually aligned}, can then be used to benchmark LLMs' performance across language varieties for tasks like sentiment analysis and text generation (Figure~\ref{fig:overview}). 
%The rationale underlying the proposed research is that, 
Once a low-cost and generalizable approach becomes available, researchers can then compare model performance across a wide range of language varieties, enabling reliable benchmarking of progress in addressing performance gaps and moving toward an LLM that performs equally well across all language varieties. 

%\kenneth{TODO: Maybe add a few words in the Introduction to mention the key results.}

%------------------- old draft ----------------

%Also maybe cite this: https://arxiv.org/abs/2404.18286

%We pick this variety pair because they are actually very similar (cite the MT paper);
%the rationle is that if our method works for \twChinese and \cnChinese, it could likely work for other variety pairs that differ more from each other.


% Maybe this: https://eric.ed.gov/?id=EJ1341954