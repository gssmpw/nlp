\section{Booking.com Data\label{app:booking-data-sample}}
\Cref{tab:sample data} shows a sample of the collected Booking.com review.

% Define a new column type for wrapping
\newcolumntype{Y}{>{\raggedright\arraybackslash}X}

\begin{table*}[htbp]
    \centering \small

    \begin{tabularx}{\textwidth}{@{}lY@{}}
        \toprule
        \textbf{Field} & \textbf{Value} \\
        \midrule
        \texttt{hotel\_\_booking\_id} & \texttt{311092} \\
        \texttt{hotel\_\_ufi} & \texttt{-240213} \\
        \texttt{user} & --------- (\textit{Removed the user identity}) \\
        \texttt{user\_nationality} & \texttt{tw} \\
        \texttt{room\_type} & \begin{CJK*}{UTF8}{bsmi}雙床房－附加床－禁煙\end{CJK*} \\
        & \textit{(English Translation: Twin Room - Extra Bed - Non-Smoking)} \\
        \texttt{checking\_date} & \texttt{2023-04-23} \\
        \texttt{checkout\_date} & \texttt{2023-04-26} \\
        \texttt{length\_of\_stay} & \texttt{3} \\
        \texttt{guest\_type} & \texttt{null} \\
        \texttt{score} & \texttt{10.0} \\
        \texttt{review\_title} & \texttt{null} \\
        \texttt{positive\_review} & \begin{CJK*}{UTF8}{bsmi}櫃檯很友善，有事情都很熱心協助，環境乾淨整潔，住的很舒適，還貼心附上各種充電頭，超級滿意！\end{CJK*} \\
        & \textit{(English Translation: The front desk is very friendly and helpful. The environment is clean and tidy. The stay was comfortable. They thoughtfully provided various charging heads. Super satisfied!)} \\
        \texttt{negative\_review} & \texttt{null} \\
        \texttt{hotel\_response} & \texttt{null} \\
        \texttt{review\_time} & \texttt{2023-05-15 10:55:59+00:00} \\
        \texttt{created} & \texttt{2024-08-18 07:11:29.971276+00:00} \\
        \bottomrule
        \multicolumn{2}{l}{{\footnotesize\textit{*Note: English translations in italics are provided for readability and are not part of the actual data.}}}
    \end{tabularx}
    \caption{Sample data entry from the collected Booking.com. There are three review components: review\_title, positive\_review, and negative\_review.}
    \label{tab:sample data}
\end{table*}



\section{Human Validation}%\zixin{may need to update this with additional human validation questions. Plus, shall we move detailed analysis here to shorten page?}
\subsection{Questions for data quality validation}\label{app:human-validation}
We used the following two questions in the human evaluation to assess data quality. 
For each part of the study, participants were shown both the English text and its translation, either into \twChinese or \cnChinese, depending on the context.

\begin{enumerate}
    \item The review (including the title, positive, and negative sections) is easy to read, and the writing quality is comparable to online reviews written by native speakers, based on my experience.

    \begin{itemize}[leftmargin=*]
        \item \begin{CJK*}{UTF8}{bsmi} \twChinese:
        根據我的經驗，這篇評論（包括標題、優點和缺點部分）很容易閱讀，且寫作品質與母語使用者撰寫的網路評論相當。\end{CJK*}
    
        \item \begin{CJK*}{UTF8}{gbsn} \cnChinese:
        根据我的经验，这篇评论（包括标题、优点和缺点部分）很容易阅读，而且写作质量与母语者撰写的网络评论相当。\end{CJK*}
    
    \end{itemize}
    
    \item The score (1-10, 1 is the worst, 10 is the best) assigned to this review accurately reflects the content of the review.

    \begin{itemize}[leftmargin=*]
        \item \begin{CJK*}{UTF8}{bsmi} \twChinese:
        這篇評論的分數（1-10，1是最差，10是最好）準確反映了評論的內容。
        \end{CJK*}
        
        \item \begin{CJK*}{UTF8}{gbsn} \cnChinese:
        这篇评论的评分（1-10，1是最差，10是最好）准确反映了评论的内容。
        \end{CJK*}
    \end{itemize}
    
\end{enumerate}
\subsection{Score prediction}\label{app:human-prediction}\zixin{new subsection.}

We used the following questions to further investigate potential content differences in review pairs, which can further lead to gaps in LLMs' performance differences. In this study, participants were asked to rate 1) the readability of the review, 2) the overall nativeness of the review, and 3) the score of the review. For the convenience of reading, all reviews were converted into either traditional or simplified Chinese characters so that all participants could process them in the writing style of their native language variety. Both English and its translation, in either \cnChinese or \twChinese based on the participants' language background, were provided in the instruction.
\begin{enumerate}
    \item Readability (1-5), where: 1 = The writing doesn't contain any literal information; 3 = The writing requires additional effort to process/comprehend; 5 = The writing is fluent and clear in terms of content delivery
    \begin{itemize}[leftmargin=*]
        \item \begin{CJK*}{UTF8}{bsmi}\twChinese: 評論可讀性(1-5分)，其中：1分表示評論不具備可讀性，或其語句無任何實際意義；3分表示評論存在語句不通的情況，且該情況會導致歧義或理解困難；5分表示評論語句通順，表達連貫，語義明確且清晰。
        \end{CJK*}
        \item \begin{CJK*}{UTF8}{gbsn}\cnChinese: 评论可读性(1-5分)，其中：1分表示评论不具备可读性，或其语句无任何实质意义；3分表示评论存在语句不通的情况或语病，且该情况会影响阅读或理解；5分表示评论语句通顺，表达连贯，语义明确且清晰。
        \end{CJK*}
    \end{itemize}
    
    \item Nativeness - the review is generated by: 1. a less proficient non-native Chinese speaker; 2. a highly proficient non-native Chinese speaker or a native Chinese speaker; 3. machine translation from another language; or 4. not sure/inconclusive
    \begin{itemize}[leftmargin=*]
        \item \begin{CJK*}{UTF8}{bsmi} \twChinese: 你覺得該評論可能出自：1. 低水平的中文非母語者；2. 高水平的中文非母語者或中文母語者；3. 來自其他語言的機器翻譯；4. 不確定/無法判斷。
        \end{CJK*}
        \item \begin{CJK*}{UTF8}{gbsn} \cnChinese: 你觉得该评论可能出自：1. 低水平中文非母语者；2. 高水平中文非母语者或中文母语者；3. 来自其他语言的机器翻译；4. 不确定/无法判断。
        \end{CJK*}
    \end{itemize}

    \item Score Rating (1-10, 1 is the lowest, 10 is the highest)
    \begin{itemize}[leftmargin=*]
        \item \begin{CJK*}{UTF8}{bsmi} \twChinese: 旅館評分 (1-10，1為最差，10為最好)
        \end{CJK*}
        \item \begin{CJK*}{UTF8}{gbsn} \cnChinese: 酒店评分 (1-10，1为最差，10为最好)
        \end{CJK*}
    \end{itemize}
\end{enumerate}

We further excluded two participants' responses due to the lack of score agreement against other participants and their significantly lower performance in prediction accuracy. Among the other 8 participants, there are no significant differences in score predictions among the data pairs, indicating raters have no biases in reading and understanding reviews from either group of speakers/writers. However, results showed statistical significance in both Accuracy (37.00\% vs. 28.75\%, \textit{p}=.016) and MSE (2.795 vs. 3.510, \textit{p}=.036), showing that native speakers might have more difficulties in correctly guessing the review scores for reviews in \twChinese. 

\section{Prompts\label{app:prompts}}

%\kenneth{TODO: Add all the prompts here.}
The following prompt is used for the structured condition.
\begin{quote}
    \small
    \textbf{System} \\
    \texttt{You are a grading assistant for hotel reviews}

    \textbf{User} \\
    \texttt{
        The following is a hotel review from a user. Based on the title, positive feedback, and negative feedback provided below, give an overall score from 1 to 10, where 1 is the worst and 10 is the best. DO NOT include any words in your output, just provide the number. \\ \\
        Title: [title] \\
        Positive Feedback: [positive\_review] \\
        Negative Feedback: [negative\_review] \\
        Overall Score (1-10):
    }
\end{quote}

The following prompt is used for both the plain and shuffled conditions.
\begin{quote}
    \small
    \textbf{System} \\
    \texttt{You are a grading assistant for hotel reviews}

    \textbf{User} \\
    \texttt{
        The following is a hotel review from a user. Based on the input review below, give an overall score from 1 to 10, where 1 is the worst and 10 is the best. DO NOT include any words in your output, just provide the number. \\ \\
        input: [text] \\
        Overall Score (1-10):
    }
\end{quote}

For LLMs that don't have a system role setting (\eg Gemma2), the system instruction is removed from the prompts.


\input{table/valid-num-samples}
%\section{Experiment Input Size}

\section{Distribution of Valid and Invalid Predictions\label{appendix:valid-sample-count}}
\Cref{tab:valid-num-samples} and \Cref{tab:missing-num-samples} present the numbers of valid and invalid predictions obtained from our experimental procedures.
Invalid predictions encompass instances where models deviated from the task requirements,
such as providing explanations instead of numerical outputs,
generating values outside the specified range of 1-10,
or failing to engage with the task altogether.
%Our analysis reveals that LLaMA-3.1 8B exhibits a notably higher proportion of invalid predictions compared to its counterparts.
We only included pairs with completely valid data entries for the prediction analysis (\Cref{tab:new-acc-results} and \Cref{tab:new-mse-results}), referring to the smallest number of each model in \Cref{tab:valid-num-samples}.




\begin{figure*}
    \centering
    \includegraphics[width=1.0\linewidth]{figure/accuracy_score.png}
    \includegraphics[width=1.0\linewidth]{figure/mse_score.png}
    \caption{Impact of text length on sentiment classification performance. The top graph shows accuracy, and the bottom graph shows MSE for negative, neutral, positive, and overall sentiments across different text lengths (0-500 characters). While overall performance remains relatively stable, individual sentiment categories show varying levels of accuracy and error, particularly for shorter texts.}
    \label{fig:length-exp}
\end{figure*}



\section{Pilot Study on Impact of Text Length\label{app:length-exp}}
During our data exploration phase, we investigated whether short texts should be removed due to potentially insufficient information for accurate sentiment classification.
To address this, we conducted a pilot experiment to analyze the relationship between text length and model performance.

\paragraph{Data}
We used the initial Booking.com dataset, assigning sentiment labels based on review scores:
positive (8-10), neutral (4-7), and negative (1-3).
The input text was created by concatenating three review components:
\begin{quote}
    \small
    \texttt{[review-title]}\\
    \texttt{[positive-review]}\\
    \texttt{[negative-review]}
\end{quote}
We categorized the texts into 50 bins of 10 characters each, up to 500 characters in length.
For each bin, we selected a balanced set of 600 samples (200 per sentiment label) where possible.
It's worth noting that for texts longer than 290 characters, maintaining this balance became challenging due to insufficient samples.

\paragraph{Predictions}
We employed GPT-4o (\texttt{gpt-4o-2024-08-06}) to classify each sample into one of the three sentiment categories using the following prompt (without a system prompt):
\begin{quote}
    \small
    \textbf{User} \\
    \texttt{
        Predict the sentiment of the following text. Please answer one of the following label: (positive, negative, neutral). Do not reply anything like `The sentiment is...'. Do not replay with any explanation. Directly output the answer. \\ \\
        Text: [text]
    }
\end{quote}
Predictions outside the specified labels were excluded from the analysis (only one sample was removed in this experiment).

\paragraph{Results}
\Cref{fig:length-exp} illustrates the accuracy and MSE for each sentiment label and the overall performance across different text lengths.
While the overall performance remains relatively stable across text lengths, we observed variations in performance for individual sentiment labels.
This effect is particularly noticeable for negative sentiments in shorter texts.
Our findings indicate that text length does influence model performance, though not to the extent of completely compromising the model's ability to classify sentiments.
Based on these results, we decided against filtering samples based on text length.
Instead, we report scores for different text length groups (short: 1-49 and long: 50+) to provide a comprehensive view of the model's performance across text lengths.


\begin{figure*}
    \centering
    \includegraphics[width=0.48\linewidth]{figure/accuracy-short-camera-ready.png}
    \hfill
    \includegraphics[width=0.48\linewidth]{figure/accuracy-long-camera-ready.png}
    \caption{Comparison of accuracy between \cnChinese and \twChinese for short (left) and long (right) texts. Each point represents a [model, setting]'s performance. The diagonal line ($x=y$) indicates equal performance. Points above the line suggest better performance in \twChinese, while points below suggest better performance in \cnChinese. We do not see a big difference between the short and long texts.}
    \label{fig:main-lengh-analysis-accuracy}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=0.48\linewidth]{figure/mse-short-camera-ready.png}
    \hfill
    \includegraphics[width=0.48\linewidth]{figure/mse-long-camera-ready.png}
    \caption{Comparison of MSE between \cnChinese and \twChinese for short (left) and long (right) texts. Each point represents a model's performance. The diagonal line ($x=y$) indicates equal performance. Points below the line suggest better performance in \twChinese, while points above suggest better performance in \cnChinese. Note the larger performance gap for short texts compared to long texts.}
    \label{fig:main-length-analysis-mse}
\end{figure*}

\section{Impact of Length on Model Performance\label{app:length-analysis-in-main-result}}
To further analyze the effect of text length on our main study results presented in \Cref{sec:experiment},
we plotted the performance on scatter plots.
The x-axis represents the performance for \cnChinese, while the y-axis represents the performance for \twChinese.
The results are displayed in \Cref{fig:main-lengh-analysis-accuracy} and \Cref{fig:main-length-analysis-mse}.

In these plots, the diagonal line ($x=y$) represents equal performance between the two language variations.
The distance of each point from this line indicates the performance gap.
For the accuracy plot (\Cref{fig:main-lengh-analysis-accuracy}), points closer to the bottom-right indicate better performance in \cnChinese, while points closer to the top-left indicate better performance in \twChinese.
Conversely, in the MSE plot (\Cref{fig:main-length-analysis-mse}), points closer to the top-left indicate better performance in \cnChinese.

Our analysis of \Cref{fig:main-lengh-analysis-accuracy} does not reveal a significant difference between the short and long text groups in terms of accuracy.
However, \Cref{fig:main-length-analysis-mse} shows a larger gap for the short text group compared to the long text group in terms of MSE. 
Based on these observations, we hypothesize that shorter reviews may introduce more bias. This could be due to insufficient contextual information in shorter texts, where models have to judge based on its prior knowledge.


\begin{table}[t]
  \includegraphics[width=\linewidth]{table/character-example.pdf}
  \caption{Example of special characters found in our dataset.}
  \label{tab:special-characters}
\end{table}

\input{table/language-distribution}
\section{Language Detection Analysis\label{appendix:language-analysis}}
To have a better understanding of Chinese and non-Chinese script elements in reviews, we conducted a detailed character-level analysis across our dataset. Using predefined vocabulary sets from zhon~\cite{githubzhon}, the Unicode Character Database~\cite{unicode}, and emoji~\cite{githubemoji}, we categorized characters into the following groups: traditional Chinese characters, simplified Chinese characters, English letters, emojis, bopomofo, Japanese characters, Korean characters, mathematical symbols, punctuation, and numbers. The table below presents the distribution of these elements across CN and TW users' reviews.

Our analysis revealed that CN and TW users exhibit similar patterns when incorporating non-Chinese elements into their primary writing system (Simplified Chinese with other elements for CN users, Traditional Chinese with other elements for TW users). The key difference lies in cross-script usage: CN users demonstrate a higher frequency of Traditional character usage compared to TW users' usage of Simplified characters.

Beyond the identified script elements, we found 103 characters in an ``Unknown'' category, appearing across 388 samples. Further investigation revealed these primarily consist of (1) rare Chinese characters not included in the zhon~\cite{githubzhon} vocabulary list (\ref{tab:special-characters} (A)), (2) fullwidth Latin letters (\ref{tab:special-characters} (B)), and (3) characters from other languages, with the latter mainly used in emoticons (\ref{tab:special-characters} (C)). As our current analysis is conducted at the character level, we cannot identify complete pinyin words or emoticon compositions. We will acknowledge this limitation and encourage future research to explore these aspects more comprehensively.


\paragraph{How Non-Chinese Elements Affect LLM Performance?}
To investigate how non-Chinese elements affect LLM performance,
we analyzed GPT-4o's performance on review pairs under different language constraints.
We define ``Chinese'' as the primary writing system for each user group
(Traditional for \twChinese users, Simplified for \cnChinese users).
We included only pairs where both reviews strictly adhered to these constraints.
For instance, \cnChinese reviews must contain only Simplified Chinese characters,
while \twChinese reviews must contain only Traditional Chinese characters.
``Chinese+English'' refers to reviews containing only the primary Chinese writing system plus English letters.

The results are presented in \ref{tab:language-subset-exp}.
When restricting the analysis to primary Chinese characters only (the Chinese row),
the performance gap between \twChinese and \cnChinese widened (see [plain, $\Delta$MSE] and [shuffled, $\Delta$MSE]),
indicating a potential bias in processing Traditional versus Simplified Chinese characters.
In the code-switching scenario with English letters, both groups showed relatively closer performance, with a smaller gap between them. This suggests that English elements may help normalize the performance across both language groups.

% When restricting the analysis to primary Chinese characters only, the performance gap between TW and CN either remained constant or widened ([plain, MSE-Diff] and [shuffled, MSE-Diff]), indicating the LLM bias in processing Traditional versus Simplified Chinese characters. In the code-switching scenario with English letters, both groups showed improved performance scores, with a reduced performance gap between them. This suggests that English elements provide strong signals for LLMs (which generally perform better with English) and affect both language groups in a similar manner.

\input{table/language-subset-study}

\begin{comment}
To investigate the relation between self-reported nationality and language usage in reviews,
we conducted an analysis using Hanzidentifier to identify Chinese language variations.
We concatenated the review title, positive review, and negative review for each entry, separating them with newline characters, and then processed this combined text using Hanzidentifier.
\Cref{tab:language-distribution} presents the distribution of identified languages.
In this context, \cnChinese is referred to as Simplified Chinese, and \twChinese as Traditional Chinese.
The ``Both'' category indicates the presence of both Simplified and Traditional Chinese within a single text,
while ``Others'' means texts containing other languages or those where the language could not be determined (possibly due to insufficient text length).
Our findings reveal that the CN group exhibits a higher ratio (15.72\%) to use languages other than Simplified Chinese.
In contrast, the TW group shows a much lower rate (3.86\%) of using languages other than Traditional Chinese.
\end{comment}