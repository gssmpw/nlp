As the study that is among the first to benchmark LLMs' performance across language varieties using contextually aligned data, this study and the data pairing method we introduced have several limitations. 

\begin{itemize}
    \item 

The first limitation is that, despite the contextual alignment, unknown confounding factors might contribute to performance gaps. 
This is an inherent challenge when using user-generated data in the wild for apple-to-apple comparisons, as controlling all variables is almost impossible.
Relaxing strict semantic alignment between paired text items inevitably introduces confounding variables.
We believe that this trade-off is worth exploring because it enables researchers to compare model behaviors across language varieties in new ways. 

\item 
Another limitation relates to the input prompts, which are code-mixed. Previous studies found that LLMs might still have deficits in dealing with cultural context and code-mixing input~\cite{ochieng2024beyond}. 
We used English for instruction to exclude potential biases introduced if it is prompted in Chinese, regardless of its variety. 
However, such a setup may introduce additional confusion for LLMs to process, leading to lower performance results. The usage of English prompts regarding non-English tasks, or code-switching prompts, requires thorough studies to better investigate LLMs' capability of multilingualism and awareness of language and cultural diversity.

\item 
A third limitation concerns our machine translation-based analysis.
We recognize that the observed performance differences when translating between \twChinese and \cnChinese
may arise from a combination of morphosyntactic variations, script differences,
and normalization of non-Chinese script elements.
More importantly, while MT-based approaches are technically feasible,
they can introduce additional biases, as MT systems themselves exhibit performance disparities across language varieties.
Further analyses are required to better isolate and address these compounding factors.

\end{itemize}
