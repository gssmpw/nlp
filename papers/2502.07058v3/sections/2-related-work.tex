% Google Sheet is here: https://docs.google.com/spreadsheets/d/1kLe5e_I4hCsyASmnrVgtOnTQlCswiDwc-JOLCiXNoWY/edit?usp=sharing

% Should also cite these: https://x.com/zraytam/status/1842514965821087853
Beyond machine translation~\cite{kantharuban2023quantifying},
researchers tried to benchmark NLP models across language varieties~\cite{Zampieri_Nakov_Scherrer_2020,joshi2024natural,blodgett2020biasreview,hovy-johannsen-2016-exploring,vardial2019report}, but the focus on identifying gaps between these varieties varies widely.
Some prior work focused solely on a single less-representative variety, such as \twChinese~\cite{tamtmmlu+,chen2024measuring}, without measuring performance gaps across multiple varieties.
Other studies that measured these gaps employed different levels of granularity.
The most common approach, \textbf{task-level comparison}, benchmarks the same NLP task across language varieties~\cite{dialectbench}, such as sentiment analysis, but datasets often differ in source or genre across varieties, making the reported performance numbers not directly comparable. 
For instance, sentiment analysis datasets for \cnChinese and \twChinese often used different sources~\cite{seki2007overview}. %leading to incomparable results despite using the same label scheme.
A more refined approach, \textbf{scenario-level comparison}, evaluates performance within the same dataset or scenario, 
such as essay grading~\cite{liang2023gptesl} or speech rating~\cite{kwako2023bertbias},
across data partitions of different language varieties~\cite{lwowski2022measuring,AAE2017racial}. 
While this method eliminates biases caused by differing data sources, it cannot fully address biases introduced during dataset construction.
%, such as cultural or population differences among language varieties. 
The most rigorous method, \textbf{instance-level comparison}, involves constructing parallel datasets with an item-by-item alignment between varieties~\cite{valuebench,multivaluebench,groenwold2020AAENLP,macucocorpus}, where each instance is converted between language varieties. 
However, creating such comparisons is very costly, requiring native speakers and language experts to ensure accuracy. 
Our approach achieves instance-level comparability with lower costs.






%\cite{kantharuban2023quantifying}: This study investigates the performance difference regarding multiple dialectal language inputs on two sets of tasks: machine translation and automatic speech recognition. In the MT task, most LLMs experienced a performance drop when dealing with dialectal input compared to the "standard" variation of language input, indicating that the dialect gap exists in the current LLM/NLP mechanism. This paper further found that training data's impact is significant but inconsistent across models and dialect variations, indicating that the solution to mitigating such a gap is not universal.
%\textbf{Methods}: parallel corpus of each language for MT tasks. The performance gap is measured using BLEU score on the generation product.


%\cite{joshi2024natural}

%\subsection{Measuring NLP models' performance gaps across Language Variations}
%\input{sections/2-1-eval-peroformance-gap}

%\subsection{Chinese Sentiment Analysis Datasets}
%\input{sections/2-2-Chinese-NLP}



