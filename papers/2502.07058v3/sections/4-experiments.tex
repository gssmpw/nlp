



%\paragraph{Data Setups.}
%To study the potential biases introduced by the input review structure, we experimented with three different settings: 
%{\em (i)} In the \textbf{structured review} setting, the content retained its original structure, consisting of the review title, positive feedback, and negative feedback. 
%{\em (ii)} In the \textbf{plain review} setting, the content was concatenated into a single paragraph, starting with the review title followed by the positive and negative feedback. 
%{\em (iii)} In the \textbf{shuffled review} setting, all three input elements were included but randomly shuffled before being concatenated. 

To examine biases from review structure, we tested three settings:
{\em (i)} \textbf{Structured review} retains the original format with title, positive, and negative feedback.
{\em (ii)} \textbf{Plain review} concatenates all elements into a single paragraph.
{\em (iii)} \textbf{Shuffled review} includes all elements but in random order.
%We excluded pairs that did not have complete predictions in all three settings. 
%or predictions beyond the given scale in the prompt for data analysis. 
For the analysis, we excluded pairs that lacked complete predictions or received predictions that did not follow the specified format (see \Cref{appendix:valid-sample-count}).
%\zixin{should I include the actual data size for each model as appendix?}\cy{Yes, please!}
%\zixin{report model size \& training budget?}\kenneth{Maybe not; or in Appendix only}\zixin{got it.}
%\paragraph{Experiment Setups.}
Once the contextually aligned dataset was constructed and available, we tested it using six LLMs: GPT-4o, Llama3 (8b, 70b, 405b), and Gemma2 (9b, 27b). 
The task involved predicting a rating score (from 1 to 10, where 1 is the worst and 10 is the best) based on the review content.
%The prompts used for each model are provided in Appendix~\ref{app:prompts}. 
% For models support system role setting (\eg GPT-4o and Llama3), a brief system description is included as the character definition of the model regarding the upcoming tasks. 
The prompt (\Cref{app:prompts}) includes the task description, the review content, and the prediction scale (1-10). 
%See \Cref{app:prompts} for the prompt details.
% Further details and sample prompts can be found in \Cref{app:prompts}.
\Cref{tab:new-acc-results} and \Cref{tab:new-mse-results} show the prediction accuracy (Acc) and mean squared error (MSE) across models and settings (see \Cref{appendix:valid-sample-count} for valid prediction counts.)
%\footnote{We are aware that Gemma2 27b underperforms significantly. Further research is needed to identify the root cause.}
%, yielding results similar to random guesses and more invalid predictions
%In general, all models except Gemma2 27b perform better than random guesses (ACC = 0.1).




\paragraph{LLMs performed significantly worse in \twChinese compared to \cnChinese.}
%when predicting sentiment scores 
Among all 54 experiments with different models and prompt settings, 38 of them had significant group differences in accuracy (Table~\ref{tab:new-acc-results}), and 47 had significant group differences in MSE (Table~\ref{tab:new-mse-results}).
%\kenneth{TODO Zixin: Update the numbers here.}
Among all significant accuracy differences, LLMs made less accurate sentiment predictions toward \twChinese users (36 out of 38 in Acc, and 45 out of 47 in MSE).
%This effect is stronger and more consistent with shuffled input.
%\kenneth{TODO Zixin: Do not say ``typically'', say how many or the percentage.}
%Such an effect is robust and more consistent when the structure of the input is removed (\ie, shuffled text).
%suggesting the differences in pure textual input between different user groups are associated with the performance gaps across LLMs.


%\kenneth{TODO Zixin: Describe the results and their stats significance.}

\paragraph{When the reviews' structures are disrupted, the performance gap increases.}
\Cref{tab:new-acc-results} and \Cref{tab:new-mse-results} show that % a clear trend: 
structured input reduces performance gaps and generally improves model performance.
%models provided with structured information exhibit a smaller performance gap between user groups
%and generally outperform those in plain and shuffled conditions.
%suggesting the knowledge that a sentence belongs to either a ``positive-review'' or ``negative-review'' category can enhance model performance.
Without knowing the structure inside reviews (\ie, plain or shuffled cases), bias toward \twChinese and \cnChinese increases.
%This observation suggests that when models must rely solely on the text for prediction (\ie, in the plain and shuffled cases),
%bias towards \twChinese and \cnChinese becomes more pronounced.


\paragraph{Shorter reviews tend to produce larger MSE gaps.}
%In our pilot exploration, we considered whether shorter texts should be excluded due to potentially insufficient information (see \Cref{app:length-exp}) and found that text length does often influence model performance and behavior. 
Our pilot study (\Cref{app:length-exp}) found that shorter texts may lack information and often affect model performance and behavior.
We thus categorized our dataset into two groups based on review's text length: 
short (1-49 Chinese characters) and long (50+ Chinese characters). 
\Cref{tab:new-mse-results} shows that the MSE gap between \twChinese and \cnChinese widens in the short text group (also see \Cref{fig:length-exp} in \Cref{app:length-exp}), while this trend is less clear for Acc (\Cref{tab:new-acc-results}).
%\kenneth{TODO: Add the analysis figure}
%Further analysis (see \Cref{}) reveals that shorter reviews consistently produced larger MSE gaps between the two varieties. 
%One possible explanation is that shorter reviews offer less information for LLMs to base predictions on, which makes language variety biases more pronounced. 
%Further research is needed to deepen our understanding.




%\paragraph{Longer reviews reveal the performance gaps more reliably.}
%\kenneth{TODO CY: Explain why we break things down by length (maybe add the classifier results in Appendix.)}
%In our pilot exploration, we investigated whether shorter texts should be excluded due to potentially insufficient information (see \Cref{app:length-exp}).
%We observed that text length does impact model performance and behavior.
%49 characters) and long (50+ characters).
%\Cref{tab:results-by-length} demonstrates that the performance gap between \twChinese and \cnChinese widens in the long text group.
%Specifically, models show substantially better performance on \cnChinese compared to \twChinese when processing longer texts.
%\cy{Is there a way to compute a metric or draw a figure to clearly show this?}\kenneth{Maybe use different shades of red to represent the size of the gap, with darker shades indicating a larger gap? I remember some papers did this but couldn't find it.}

\input{table/translation-table}

\subsection{Can We Just Use Machine Translation?}
%\kenneth{TODO CY: Add results here.}
A natural question is whether we could use machine translation to convert \twChinese to \cnChinese, and vice versa, to create a paired dataset for benchmarking. 
% \kenneth{TODO CY: I am confused. Did we compare (a) (TW review MT to CN, CN review in the same pair), or (b) (TW review MT to CN, original TW review)?}
%\kenneth{This might need some more thinking....hmm}
%A natural question is whether using off-the-shelf machine translation tools, such as Google Translate, to convert \twChinese to \cnChinese and vice versa would eliminate the observed performance gap.
To explore this, %we conducted a baseline study using Google Translate.
we translated all texts to their opposite version (\twChinese to \cnChinese, or vice versa) using the Google Translate API.
% For the shuffled condition, we maintained the order of these components in the translation.
We then conducted sentiment analysis experiments using GPT-4o, comparing each original sample with its translated version (\eg, [a review in TW, its translation into CN].)
%(\ie, [\twChinese, \twChinese translation], and [\cnChinese translation, \cnChinese]).
% \kenneth{TODO CY: I'm confused about the following. Table 3 shows that translating \twChinese data to \cnChinese gives WORSE MSE, rather than BETTER? Translate CN to TW is the only setting that gives us green results?}
The results (\Cref{tab:translation-result}) show an \textbf{asymmetry between the two translation directions}.
Translating \twChinese data to \cnChinese increased accuracy and decreased MSE (\Cref{tab:translation-result}'s 1st, 3rd, and 5th rows).
% \kenneth{TODO CY: What exactly did we mean by IMPROVE? IMPROVE what performance?}
However, translating \cnChinese to \twChinese produced mixed results: it decreased accuracy but improved MSE.
These results suggest that while using machine translation to create review pairs between language varieties is technically feasible, it can introduce an additional layer of bias, as machine translation itself is a language technology that is not immune from biases across language varieties.
In our case, machine translation might be better at \twChinese to \cnChinese than the other way around \cite{kantharuban2023quantifying}. %\kenneth{TODO Zixin: Please add citation here}
%~\cite{costa2022no,arivazhagan2019massively}
%may have been more effective at translating \twChinese into \cnChinese than the other way around~\cite{costa2022no,arivazhagan2019massively}.
% \kenneth{TODO CY and Zixin: add a few citations here}, leading to the observed asymmetry in outcomes.
Furthermore, mature machine translation systems for specific language varieties are not always readily available~\cite{multivaluebench,kumar-etal-2021-machine}.
%\kenneth{TODO CY and Zixin: add a few citations here---maybe VALUE?}







%----------- dead kitten -------------
\begin{comment}







that is not immune from biases across language varieties. 

---though many language varieties lack mature machine translation systems---it can introduce an additional layer of bias into the data, as machine translation itself is a language technology influenced by biases across language varieties. 









These results suggest that while machine translation can technically be used to create review pairs between language varieties, many varieties lack mature translation systems. Moreover, machine translation itself introduces biases, as it is a language technology influenced by linguistic variations. In our case, translation from \twChinese to \cnChinese appeared more effective than the reverse, contributing to the observed asymmetry in outcomes.










These results indicate that performance gaps persist even when pairs are constructed using a review and its machine-translated version, but the gaps' inconsistencies become less predictable.
Since machine translation itself is a language technology, it can introduce an additional layer of bias into the data.
In our case, machine translation may have been more effective at translating \twChinese into \cnChinese than the other way around, leading to the observed asymmetry in outcomes.
Moreover, mature machine translation systems for specific language varieties are not always available. 
%, further limiting the reliability of this approach.










%, making it an unsuitable method for evaluating NLP model performance across language varieties. 
In our case, machine translation may have been more effective at translating \twChinese into \cnChinese than the other away around,\kenneth{TODO Zixin and CY: ADD citations to this?} leading to the observed asymmetry in outcomes.










\kenneth{TODO: I guess the takeaway here really is--- using MT to form pairs,  (1) it does NOT eliminate the gap, (2) the gap is less consistent, one possible reason is we might want too consider MT technology itself has some biasss across varieties, as CN to TW might perform worse. (3) Many varieties do not have mature MT tools.}

% While these experiments clearly demonstrate a model bias between \twChinese and \cnChinese, further research is needed to fully understand the effects of translation on model performance.
While these experiments demonstrate a model bias between \twChinese and \cnChinese,
it is important to note that the purpose of this machine translation study was primarily
to investigate whether off-the-shelf NLP tools could help align language varieties.
Our findings suggest that even established tools like Google Translate API
may not significantly mitigate the performance gap between language varieties.
This underscores the significance of our main findings and points to the need for more specialized approaches.
\cy{I added a few words to make it clearer and included some texts from our rebuttal.}


%This is our rebuttal:
\kenneth{The reviewer pointed out that machine translation (MT) did not yield satisfactory consistency across language varieties, limiting our findings' generalizability. We want to clarify that the purpose of the MT study is to show that handy, off-the-shelf NLP tools (Google Translate API in this case)---that are seemingly useful to align language varieties---may not help to mitigate the performance gap too much, highlighting the meaningfulness of our main finding. Our current MT results also indicate more studies should be conducted besides generic MT. We will leave those additional explorations as future studies, including potential techniques for representation alignment across language varieties, translation and/or rephrasing for accents/dialects within a target language, and machine translation for closely related languages such as Mandarin-Cantonese, and so on. We will also revise the future study section to mention these potential working directions.}

To exclude potential influences introduced by the input review structure, we designed three types of input:
\begin{itemize}
    \item \textbf{Structured review:} The content is on its original structures, which consists of the review title, the positive feedback, and the negative feedback. At most two of these three columns can be empty.
    \item \textbf{Plain review:} The content is concatenated and combined as a single paragraph. The paragraph starts with the review title content, followed by the positive and negative feedback input.
    \item \textbf{Shuffled review:} The content has all three original input elements, but is randomly shuffled before concatenated into a single paragraph. This input provides additional insights regarding the robustness and consistency of the model generation result.
\end{itemize}
    
\end{comment}





