\section{Related work}
Initial work applying transformers to tabular data considered fitting a single transformer per dataset, with attention applied to every column of a single datapoint \cite{gorishniy2023}. \citet{hollmanntabpfn} proposed instead to apply attention to the entire dataset at once (e.g., applying attention over the rows of the dataset) and demonstrated how such a model could be trained on synthetic data. This approach has the advantage of requiring only a single forward pass through a transformer to perform both training and predictions. Several works have leveraged this architecture for applications such as Bayesian Optimization \cite{muller23a}, forecasting \cite{dooley2024}, learning curve extrapolation \cite{adriaensen23}, and fairness analysis \cite{robertson2024}. \todo{David: find another application; should be easy.}

To improve the original architecture, follow-up work proposed new approaches, such as learning the (fast) weights of an MLP capable of performing well on the test portion of a particular dataset \cite{muller2023mothernet} while achieving much lower latency. Other research focused on modifying the original architecture to align with the natural symmetries of tabular data. For instance, \citet{hollmann2025nature,mueller2024gamformer}
applied a bi-attention mechanism \cite{kossen2022}, enabling predictions invariant to the order of features. In particular, the recent work TabPFNv2 \cite{hollmann2025nature} introduced several improvements to the original TabPFN architecture and prior. In addition to being concurrent work, the focus is orthogonal to ours, as we specifically analyze and enhance the output representation of TabPFN.



To the best of our knowledge, no previous approach has proposed a \emph{target-equivariant} architecture for foundational tabular models, though several works have proposed modifications to the output architecture of TabPFN. \citet{mueller2024gamformer} introduced a modification of the output, replacing the linear projection from token embeddings to the target with Generalized Additive Models. This approach improves the interpretability of the model by constructing shape functions for each feature, which can be analyzed independently. \citet{margeloiu2024tabmda} explored training classifiers on features generated by TabPFN. While their method includes non-parametric classifiers such as KNN, the proposed model is not target-equivariant. It also requires training a model at inference time unlike our approach that learns this adaptation during the pre-training.

We are not aware of many theoretical studies analyzing TabPFN, with the exception of \cite{nagler2023statistical}\todo{cite other works too}. Our work demonstrates the existence of an incompressible term in the pre-training loss, which quantifies the lack of equivariance. Additionally, our architecture enables tabular foundational models to be viewed as (deep) kernel machines, as the output is non-parametric. We believe this perspective opens exciting research directions, given that kernel approaches naturally lend themselves to theoretical analysis \cite{jacot2018neural}.

\begin{figure*}[t]
\center
\includegraphics[width=0.8\textwidth]{figures/boundary-grid.pdf}
\caption{
Prediction for both TabPFN and our model when presenting the same datasets but with different class ordering. We pass 9 training points (indicated with dark crosses) each having a distinct class and show the prediction of both models when using 3 different class numbering on a dense grid. The predictions of our model are identical under different class numbering as opposed to TabPFN as the model is not equivariant.
 %
\label{fig:equi-illustration}
}
\end{figure*}

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%