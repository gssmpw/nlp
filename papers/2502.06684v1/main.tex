%

\documentclass{article}

%
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} %
\usepackage{stfloats}
%
%
%
%
\usepackage{hyperref}


%
\newcommand{\theHalgorithm}{\arabic{algorithm}}

%
%

%
\usepackage[accepted]{icml2025}

\usepackage{hyperref}       %
\usepackage{url}            %
\usepackage{booktabs}       %
\usepackage{amsfonts}       %
\usepackage{nicefrac}       %
\usepackage{microtype}      %
\usepackage{amssymb}

\usepackage{amsfonts}       %
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{dsfont}
\usepackage{nicefrac}       %
\usepackage{microtype}      %
\usepackage{xspace}
\usepackage{bm}
%
\newcommand{\ma}[1]{\textcolor{red}{[MA: #1]}}
\newcommand{\ds}[1]{\todo[inline]{DS: #1}}
\usepackage{color}
%



\usepackage{enumitem}
\usepackage[noabbrev,capitalize]{cleveref} 
\newlist{assumplist}{enumerate}{1}
\setlist[assumplist]{label=(\textbf{\Alph*})}
\Crefname{assumplisti}{Assumption}{Assumptions}
\newlist{assumplist2}{enumerate}{2}
\setlist[assumplist2]{label=(\textbf{\alph*})}
\Crefname{assumplist2i}{Assumption}{Assumptions}

\newlist{assumplistobs}{enumerate}{3}
\setlist[assumplistobs]{label=(\textbf{$\mathcal{F}$-\alph*})} 
\Crefname{assumplistobs}{Assumption}{Assumptions}
\Crefname{assump}{Assumption}{Assumptions}

%
%
%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\input{notation}



%
%
%
\usepackage[disable,textsize=tiny]{todonotes}

\newcommand{\dst}[1]{{\color{RubineRed}{#1}}}
\newcommand{\mat}[1]{{\color{CornflowerBlue}{#1}}}

%
%
\icmltitlerunning{KernelPFN}

\begin{document}

\twocolumn[
\icmltitle{\methodname{}: A Target-Permutation Equivariant Prior Fitted Networks}

%
%
%
%

%
%
%
%

%
%
%
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Michael Arbel}{equal,yyy}
\icmlauthor{David Salinas}{equal,comp}
\icmlauthor{Frank Hutter}{comp,sch}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{Univ. Grenoble Alpes, Inria, CNRS, Grenoble INP, LJK, 38000 Grenoble, France}
\icmlaffiliation{comp}{University of Freiburg, Freiburg, Germany}
\icmlaffiliation{sch}{ELLIS Institute T\"{u}bingen, T\"{u}bingen, Germany}

\icmlcorrespondingauthor{Michael Arbel}{michael.n.arbel@gmail.com}
\icmlcorrespondingauthor{David Salinas}{david.salinas.pro@gmail.com}

%
%
%
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

%

%
%
%
%
%

%
\printAffiliationsAndNotice{\icmlEqualContribution} %

\begin{abstract}
	Recent foundational models for tabular data, such as TabPFN, have demonstrated remarkable effectiveness in adapting to new tasks through in-context learning. 
	However, these models overlook a crucial equivariance property: the arbitrary ordering of target dimensions should not influence model predictions.
In this study, we identify this oversight as a source of incompressible error, termed the equivariance gap, which introduces instability in predictions. To mitigate these issues, we propose a novel model designed to preserve equivariance across output dimensions. Our experimental results indicate that our proposed model not only addresses these pitfalls effectively but also achieves competitive benchmark performance.
\end{abstract}



\section{Introduction}

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

Tabular data, a prevalent format in many real-world applications, has historically presented unique challenges to foundational models due to its lack of inherent structure compared to image or text data. 
%
Foundation models like TabPFN  \cite{hollmanntabpfn} have recently emerged, leveraging transformers to address classification problems in tabular settings. These models stand out by eliminating the need for hyperparameter tuning by performing both training and prediction in a single forward pass through a transformer.

The core of those foundational tabular models is a pre-training procedure that trains a transformer to predict on randomly sampled test data given the corresponding (random) training data. Such approaches leverage transformers to perform attention over rows of data, considering all training points simultaneously. Applying attention over rows is critical as it allows us to respect one symmetry of tabular data, e.g. that the order of rows does not matter, which is the case when data is drawn from an i.i.d. distribution (a frequent assumption in supervised learning).

However, row-order symmetry is not the only symmetry relevant to tabular data. Another key symmetry pertains to feature order, where the arrangement of columns should not influence model predictions. 
Recent work \cite{mueller2024gamformer,hollmann2025nature} has addressed this challenge by employing bi-attention mechanisms similar to those studied in earlier work \cite{kossen2022}. This approach alternates attention over rows and columns, making the models equivariant to feature permutations and better suited for handling another inherent symmetry of tabular data.

A third important symmetry is the \emph{target equivariance} of the model: the order in which we encode the target component should not affect the predictions. To the best of our knowledge, target equivariance has not been considered in the context of foundational tabular models. In this paper, we study this aspect and make the following contributions:

\begin{itemize}
    \item We demonstrate the critical role of target equivariance in tabular foundational models.
    \item We provide a theoretical analysis, showing that the error of TabPFN models can be decomposed into two components: one that measures the modelâ€™s equivariance gap and another that evaluates its fit to the data.
    \item We propose a novel model that incorporates target equivariance, ensuring more robust and consistent predictions.
    \item We show how the architecture we propose improves current foundational tabular architecture in both artificial and real-world datasets.
\end{itemize}


%
%
%
%
%
%
%
%

\section{Related work}

Initial work applying transformers to tabular data considered fitting a single transformer per dataset, with attention applied to every column of a single datapoint \cite{gorishniy2023}. \citet{hollmanntabpfn} proposed instead to apply attention to the entire dataset at once (e.g., applying attention over the rows of the dataset) and demonstrated how such a model could be trained on synthetic data. This approach has the advantage of requiring only a single forward pass through a transformer to perform both training and predictions. Several works have leveraged this architecture for applications such as Bayesian Optimization \cite{muller23a}, forecasting \cite{dooley2024}, learning curve extrapolation \cite{adriaensen23}, and fairness analysis \cite{robertson2024}. \todo{David: find another application; should be easy.}

To improve the original architecture, follow-up work proposed new approaches, such as learning the (fast) weights of an MLP capable of performing well on the test portion of a particular dataset \cite{muller2023mothernet} while achieving much lower latency. Other research focused on modifying the original architecture to align with the natural symmetries of tabular data. For instance, \citet{hollmann2025nature,mueller2024gamformer}
applied a bi-attention mechanism \cite{kossen2022}, enabling predictions invariant to the order of features. In particular, the recent work TabPFNv2 \cite{hollmann2025nature} introduced several improvements to the original TabPFN architecture and prior. In addition to being concurrent work, the focus is orthogonal to ours, as we specifically analyze and enhance the output representation of TabPFN.



To the best of our knowledge, no previous approach has proposed a \emph{target-equivariant} architecture for foundational tabular models, though several works have proposed modifications to the output architecture of TabPFN. \citet{mueller2024gamformer} introduced a modification of the output, replacing the linear projection from token embeddings to the target with Generalized Additive Models. This approach improves the interpretability of the model by constructing shape functions for each feature, which can be analyzed independently. \citet{margeloiu2024tabmda} explored training classifiers on features generated by TabPFN. While their method includes non-parametric classifiers such as KNN, the proposed model is not target-equivariant. It also requires training a model at inference time unlike our approach that learns this adaptation during the pre-training.

We are not aware of many theoretical studies analyzing TabPFN, with the exception of \cite{nagler2023statistical}\todo{cite other works too}. Our work demonstrates the existence of an incompressible term in the pre-training loss, which quantifies the lack of equivariance. Additionally, our architecture enables tabular foundational models to be viewed as (deep) kernel machines, as the output is non-parametric. We believe this perspective opens exciting research directions, given that kernel approaches naturally lend themselves to theoretical analysis \cite{jacot2018neural}.

\begin{figure*}[t]
\center
\includegraphics[width=0.8\textwidth]{figures/boundary-grid.pdf}
\caption{
Prediction for both TabPFN and our model when presenting the same datasets but with different class ordering. We pass 9 training points (indicated with dark crosses) each having a distinct class and show the prediction of both models when using 3 different class numbering on a dense grid. The predictions of our model are identical under different class numbering as opposed to TabPFN as the model is not equivariant.
 %
\label{fig:equi-illustration}
}
\end{figure*}

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
\section{Background on Prior-Fitted Networks }\label{sec:background}
%
%
%
%

\citet{hollmanntabpfn} introduced a pre-trained model, TabPFN, that leverages the  transformer architecture \citep{Vaswani:2017} to perform \emph{in-context learning} on unseen tabular datasets without the need for any further training. Below, we briefly review the three main ingredients for pre-training of this model as it will help to describe the architectural changes we propose to make the model equivariant to target permutations.

\subsection{Dataset prior}
A key ingredient to the pre-training procedure is the use of artificial datasets that are sampled using a sophisticated generative model meant to capture the real-world distribution of datasets. 
More precisely, any dataset of multiple covariate/target pairs $(x,y)$ is generated by first sampling a \emph{latent} parameter $\psi$ characterizing the dataset according to a pre-defined prior $p(\psi)$, then sampling each covariate/target pair $(x,y)$ in the dataset according to a conditional distribution $p(x,y|\psi)$ characterized by $\psi$. 
The pre-training procedure requires repeatedly generating two collections of training and test samples $(x_n,y_n)_{n=1}^{N}$ and $(x_m^{\star}, y_m^{\star})_{m=1}^M$ all of which are sampled from the conditional distribution $p(x,y|\psi)$ using a shared %
value of $\psi$ sampled from the prior $p(\psi)$. 
Here, we will use the same generative mechanism as \citet{hollmanntabpfn} for pre-training the proposed equivariant model. 

\todo{David: I wonder if we should describe in 1-2 sentence how the prior is implemented in practice (SCM, MLPs, etc).}

\subsection{Pre-training procedure}
TabPFN is trained by predicting test target values $(y_m^{\star})_{m=1}^M$ given corresponding test covariates $(x_m^{\star})_{m=1}^M$ as well as training covariates/target pairs $(x_n,y_n)_{n=1}^{N}$. Denoting for simplicity $(X,Y)$ and $(X^\star, Y^\star)$, the collection of training and test samples, TabPFN's  pre-training objective is given by: 
%
%
%
%
%
%
\begin{align}\label{eq:pre-training_loss}
    \mathcal{L}(f) := \mathbb{E}\brackets{ \ell\parens{f_{X,Y}\parens{X^{\star}},Y^{\star}} }, 
\end{align}
where $(y,y')\mapsto\ell(y,y')\in \R$ is a point-wise loss, while $f_{X,Y}\parens{X^{\star}}$ is the output of the network when provided with the training collection $(X,Y)$ and test queries $X^{\star}$. Here, the expectation is over the collections  $(X,Y)$ and $(X^{\star},Y^{\star})$ sampled according to the dataset prior. The loss $\ell$ is, typically, chosen to be the mean-squared-error loss in the case of regression or the cross-entropy loss in the case of classification. Note that each test query $x_m^{\star}$ is processed independently by the network $f$ so that $f_{X,Y}\parens{X^{\star}} = (f_{X,Y}\parens{x_m^{\star}})_{m=1}^M$. The above procedure trains the  model to directly infer new covariate/target dependences from the provided training pairs and will be used in our setting as well.

\subsection{TabPFN architecture}


The architecture used in TabPFN can be decomposed as into three modules, an encoder, a backbone and a decoder. 

{\bf Linear Encoder. }  The encoder module constructs training and test tokens $(e_n)_{n=1}^N$ and $(e^{\star}_m)_{m=1}^M$ that are provided to the transformer backbone assuming the inputs $x$ and $y$ are vectors of fixed dimensions $\numcovariates$ and $q_{max}$. 
Each training token $e_n$ is obtained by  linearly embedding both covariate $x_n$ and target $y_n$ into a feature space of fixed dimension $d$ and then summing both embeddings, i.e.  $e_n = U x_n +Vy_n$, where $U$ and $V$ are trainable matrices of sizes $d\times \numcovariates$ and $d\times \numlabels$. On the other hand, the test token consists only in embedding the test covariate $x^{\star}_m$, i.e. $e^{\star}_m = U x_m^{\star}$ since the test target $y^{\star}_m$ is not provided to the network. While targets with smaller dimensions can be handled by a simple zero-padding procedure (see \citet{hollmanntabpfn}), the encoder cannot easily accommodate target data with dimensions greater than  $\numlabels$.

{\bf Transformer backbone.} The backbone consists of a succession of residual multi-head self-attention layers between all tokens followed by a residual feed-forward network applied to each token. 
In order to avoid information leakage from test to train data, an attention mask ensures that all tokens can only attend to the training tokens. This also ensures that test tokens are processed independently from each other.  
The residual connections in the backbone preserve the initial feature dimension $d$, so that each token is still associated to a particular sample while gradually incorporating information from all training tokens.      
%

{\bf MLP decoder.} The decoder consists of a one-hidden layer MLP that takes each test output token $e^{\star}_{m}$ produced by the transformer backbone and produces a prediction vector $\hat{y}_m$ of fixed dimension $\numlabels$.  
%
This choice of decoder requires a pre-defined target dimension $\numlabels$. Thus, it cannot be used post-hoc on new data with higher target dimensions. 
%

\todo[inline]{We probably want to introduce the proj matrix here to be consistent with the input description.}

A notable property of the TabPFN architecture is its invariance of the test predictions to the order by which training and test points are provided\todo{Also equiv to test order, should we include this?}. This property is desirable since the order of training points is arbitrary and should not influence predictions on test samples. 
However, the network lacks \emph{equivariance} w.r.t. the targets' dimensions, meaning that predictions are highly dependent on the order by which the training target dimensions are provided, an undesirable property as we show next. 


\section{Target permutation equivariance in PFN}
\subsection{Target permutation equivariance}
When presented with a new unseen dataset of covariate/target pairs $(x_n,y_n)_{n=1}^N$, the order of the component's target is arbitrary. In other words, given target vectors of the form $y_n = \parens{(y_n)_1,\dots, (y_n)_{\numlabels}}$, these could as well be presented in a different order by applying a permutation $\sigma$ to the components of $y_n$ to obtain a permuted target $\sigma(y_n) = \parens{(y_n)_{\sigma(1)},\dots, (y_n)_{\sigma(\numlabels)}}$.
%
The transformed dataset $(x_n,\sigma(y_n))_{n=1}^N$ is still essentially the same as the original dataset up to the permutation as we only changed the order of the target components. For instance, when the target represents a one hot encoding vector of $2$ classes: "red" or "blue", it should not matter whether we encode "red" as the first or the second class in the one-hot vector. 
Consequently, a pre-trained model should be able to provide consistent predictions regardless of the component's order. 
More formally, the model should satisfy the following equivariance property:
\begin{definition}[Target permutation  equivariance]\label{def:equi}
A function $f$ is permutation \emph{equivariant} in the targets' components iff for any training data $(X,Y)$ and test covariates $X^{\star}$:  
\begin{align}\label{eq:equivariance}
\forall \sigma \in \mathfrak{S}_{\numlabels}, \quad \sigma^{-1}\parens{f_{X, \sigma(Y)}\parens{X^*}} = f_{X, Y}\parens{X^*},
\end{align}
where $\mathfrak{S}_{\numlabels}$ denotes the set of all possible permutations of the components of a vector of $\numlabels$ elements. 
\end{definition}
%

\subsection{Non-equivariance of TabPFN model}

As introduced in \citet{hollmanntabpfn}, TabPFN is not permutation equivariant in the target's components. Consequently, it is not guaranteed to provide consistent predictions when the target components are permuted, thus affecting its robustness. 
To illustrate the implications of non-equivariance on the robustness of TabPFN, we consider a toy classification problem in $2$ dimensions, where $9$ training covariates are positioned on a $2$-dimensional regular grid, each corresponding to a different class inspired by \citet{mccarter2024}. The pre-trained TabPFN model is then used to predict the classes on a regular grid of 900 points. 
\cref{fig:equi-illustration} (top) shows the classification boundaries when using the same data but with $3$ different orderings for the classes. It is clear from the figure that the ordering heavily affects the prediction even in this simple example, which strongly impacts robustness of models like TabPFN. While \citet{hollmanntabpfn} propose a mitigation strategy by averaging several predictions using random permutations of the target dimensions, that approach is costly and can be impractical when the output dimension $\numlabels$ is large since $\mathcal{O}(\numlabels!)$ forward predictions are required to make the function equivariant as discussed in the next section.
 
The non-equivariance of TabPFN is mainly due to the choice of both encoders and decoders that force an arbitrary order on the dimensions of the targets. 
In \cref{sec:equitabfn}, we overcome this limitation by introducing a new architecture that is target permutation \emph{equivariant}. As illustrated in \cref{fig:equi-illustration} (bottom), the predictions made by the proposed network remain the same regardless of the class ordering. 
Next, we provide a theoretical justification for enforcing equivariance in the model by analyzing the optimal solution to the pre-training objective considered in \cref{eq:pre-training_loss}.   

%

%
%
%
%
%
%
%



\subsection{Bounds on non-equivariant models}



%
%
%
%
%
%
%
%

From a theoretical standpoint, permutation equivariance of a pre-trained model is also a natural and important property to satisfy. 
To illustrate this fact, we start with a decomposition of the objective into an \emph{equivariance gap} and an optimality error and then establish that the optimal solution of the pre-training problem in \cref{eq:pre-training_loss} must be permutation equivariant in the target's components, as soon as the model is expressive enough. 

{\bf Equivariance gap.} We define the equivariance gap $\equigap{f}$ of a function $f$ relative to $\mathcal{L}$ as the difference between the objective value at $f$ and at its \emph{symmetrized} version $\equiproj{f}$: 
\begin{align}\label{eq:equivariance_gap}
	\equigap{f} := \mathcal{L}(f) - \mathcal{L}(\equiproj{f}).
\end{align}
Here, the symmetrized function $\equiproj{f}$ is an equivariant function obtained from $f$ by the  averaging operation:
\begin{align}
    \equiproj{f}_{X,Y}\parens{X^{\star}} = 
    \mathbb{E}_{\sigma}\brackets{\sigma^{-1}\parens{f_{X,\sigma(Y)}\parens{X^{\star}}}}, 
    \label{eq:equivariant-sum}
\end{align}
where $\mathbb{E}_{\sigma}$ denotes the expectation w.r.t. to the uniform distribution over all permutations $\sigma$. 
By construction, $\equiproj{f}$ is permutation equivariant w.r.t. the target components, meaning that it satisfies \cref{eq:equivariance} in \cref{def:equi}. Moreover, a function $f$ is itself equivariant iff $\equiproj{f}=f$. Hence, the gap $\equigap{f}$ vanishes as soon as $f$ is itself equivariant. 

\ds{I wonder if we should reintroduce the proof for \cref{eq:equivariant-sum} as it is pedagogic and I suspect some readers will find it useful/struggle to derive it themselves.}

In general, the equivariance gap can take negative values. 
However, we establish later that this equivariance gap must be non-negative under the following assumptions on the pointwise loss $\ell$ and the marginal distribution $p$ of the data:
\begin{assumplist}
\item \label{assump:convex_loss} {\bf Invariance and convexity of the pointwise loss.} The pointwise loss $\ell$ is strictly convex in its first argument and is invariant to permutations of the components of its arguments, i.e. for any permutation $\sigma$, it holds that: 
\begin{align*}
	\ell(\sigma(y),\sigma(y')) = \ell(y,y').
\end{align*} 
\item\label{assump:invariant_distribution} {\bf Invariance of the data distribution.} The marginal distribution of the data is invariant to permutations applied to $Y$ and $Y^{\star}$, i.e. 
\begin{align*}
	p(X,\sigma(Y),X^{\star},\sigma(Y^{\star})) = p(X,Y,X^{\star},Y^{\star}).
\end{align*}
\end{assumplist}
\cref{assump:convex_loss} is satisfied for most commonly used losses such as the cross-entropy loss or the quadratic loss.  \cref{assump:invariant_distribution} holds as soon as data can be presented without a preferred ordering, as most i.i.d. tabular datasets.  The next proposition, proved in \cref{sec:proofs}, decomposes the objective into a non-negative equivariance gap and an optimality error.
\begin{proposition}\label{prop:error_decomposition}
    Under \cref{assump:convex_loss,assump:invariant_distribution}, the objective $\mathcal{L}(f)$ in  \cref{eq:pre-training_loss} admits the decomposition:
    \begin{align}
        \mathcal{L}(f) = \mathcal{L}(\equiproj{f})
        + \equigap{f},
    \end{align}
where the equivariance gap $\equigap{f}$ is always non-negative and only equal to $0$ when $f$ is equivariant to permutations. Furthermore, when $\ell$ is the squared loss (i.e. $\ell(y,y') = \Verts{y-y'}^2$), then 
$$ 
\equigap{f} = \mathbb{E}_{p}\brackets{\Verts{f_{X,Y}\parens{X^{\star}}- \equiproj{f}_{X,Y}\parens{X^{\star}}}^2}.
$$
\end{proposition}
\begin{proof}[Proof sketch.] The key step is to express the objective as an expectation over permutations using the  invariance assumptions \cref{assump:convex_loss,assump:invariant_distribution} on the data and loss:
$$
        \mathcal{L}(f) = \mathbb{E}_p\mathbb{E}_{\sigma} \brackets{\ell\parens{\sigma^{-1}f_{X,\sigma(Y)}\parens{X^{\star}},Y^{\star}}}.
 $$
The non-negativity of the equivariance gap is then established using Jensen's inequality by convexity of the loss $\ell$ in its first argument (\cref{assump:convex_loss}). 
Finally, the expression of the gap for the quadratic objective follows by explicit calculations. 
\end{proof}
\cref{prop:error_decomposition} shows that minimizing the objective $\mathcal{L}$ requires reducing both the equivariance gap and the optimality error of symmetrized functions. Hence, the model must employ some of its expressive power solely for being equivariant, which can be wasteful. This fact is further emphasized by the following proposition which shows that if $\mathcal{F}$ is rich enough, any minimizer of $\mathcal{L}$ over $\mathcal{F}$ must be equivariant.
\begin{proposition}\label{prop:equvariant_solution}
    Assume that $\mathcal{F}$ is rich enough so that for any $f\in \mathcal{F}$, then $\equiproj{f}$ also belongs to $\mathcal{F}$. Then,  under \cref{assump:convex_loss,assump:invariant_distribution}, any minimizer $f^{\star}$ of $\mathcal{L}$ over $\mathcal{F}$ must be equivariant.
\end{proposition}
\begin{proof}
    Let $f^{\star}$ be a minimizer of $\mathcal{L}$ over $\mathcal{F}$. Assume by contradiction that $f^{\star}$ is not equivariant. Hence, by \cref{prop:error_decomposition}, it follows that $\equigap{f^{\star}}>0$. Moreover, the error decomposition $\mathcal{L}(f^{\star})= \mathcal{L}(\tilde{f^{\star}}) + \equigap{f^{\star}}$ from  \cref{prop:error_decomposition} ensures that $\mathcal{L}(f^{\star})>  \mathcal{L}(\equiproj{f^{\star}})$. However, by assumption on $\mathcal{F}$, the function $\equiproj{f^{\star}}$ belongs to $\mathcal{F}$, which contradicts the optimality of $f^{\star}$. By contradiction, we conclude that $f^{\star}$ must be equivariant. 
\end{proof}
A consequence of \cref{prop:equvariant_solution} is that a non-equivariant model must learn to be equivariant in addition to reducing the prediction error, as we further verify empirically in \cref{sec:experiments}. Next, we introduce an equivariant architecture that bypasses these limitations while resulting in competitive performance.


%
%
%
%



\section{Target Equivariant Prior Fitted Network}\label{sec:equitabfn}

\begin{figure*}[t]
\center
\includegraphics[width=0.99\textwidth]{figures/architecture.pdf}
\caption{Overview of \methodname{}'s architecture. 
%
%
%
%
Data is tokenized via an encoder, processed using self-attention, and decoded to obtain predictions. The encoder maps covariates to a single token and embeds target components into tokens via a $1\times 1$ convolution. Missing test tokens are replaced by prediction tokens. Self-attention alternates between two blocks: (1) Feature-wise attention, where target tokens attend only to covariate tokens (gray arrows), and covariate tokens attend to all tokens (blue arrows); (2) Data-wise attention, where test tokens attend only to training tokens (blue arrows), and training tokens attend to themselves (gray arrows). The decoder applies cross-attention between test and training tokens (queries $Q$, keys $K$), using original target vectors as values $V$, followed by a residual connection to preserve equivariance. The final output $f_{X,Y}(X^{\star})$ is compared to the test target $Y^{\star}$ 
in the training objective.
  }
\label{fig:architecture}
\end{figure*}

\todo[inline]{Caption for \cref{fig:architecture}}

%
%
%
%
%
We introduce \methodname{}, a new model architecture for in-context learning on tabular data, that is permutation equivariant w.r.t. the target's components. 
Our architecture integrates self-attention mechanisms across  data points and data components to leverage relationships between datapoints while preserving equivariance by processing individual attributes (such as the targets components). 
%
Following the notation in \cref{sec:background}, \methodname{} takes training covariate/target pairs $(X,Y)$ and test covariate $X^{\star}$ as inputs and produces a prediction $\hat{Y} = f_{X,Y}\parens{X^{\star}}$ of the test target $Y^{\star}$. 
%
Unlike TabPFN which requires fixed target dimensions for all datasets, \methodname{} allows the dimensions of the target to change depending on the dataset as a consequence of its equivariant architecture. 
\methodname{} consists of four major modules: a target equivariant encoder, an attention module both across data components and across data points, and a non-parametric equivariant decoder, each designed to facilitate the end-to-end learning of components interactions and datapoint relationships, see \cref{fig:architecture}. 
Below, we elaborate on each module and their interactions.

\subsection{Target equivariant encoder}
The encoder constructs training and test tokens by applying a linear projection to both covariates and targets so that target equivariance is preserved. 
Specifically, following \cite{hollmanntabpfn}, each training and test covariate vector $x_{n}$ and $x^{\star}_m$ is encoded into a single token of dimension $d$ by applying a linear projection matrix $U$ of size $d\times p$.  
However, instead of adding a linear projection of the training targets to each corresponding training token, as done in the case of TabPFN (see \cref{sec:background}), we compute a token for each component $(y_n)_j$ of a training target by multiplying them with an embedding vector $V$ of dimension $d$ for all $1\leq j\leq q$. This operation amounts to applying a $1\times 1$ convolution along the components of each target which preserves target equivariance. 
%
Since, the validation target $Y^{\star}$ is not provided to the model as input, it is replaced by a trainable \emph{prediction token} $W_{pred}$ of dimension $d$ that is repeated $M\times q$ times to form an initial guess $\tilde{Y}^0$ of the target. 
All these embeddings along with prediction tokens are collected to form a single tensor $E$ of shape $(N+M, q+1, d)$, where the blocks $E_{:N,1,:}$,  $E_{N:M,1,:}$ correspond to embeddings of $X$ and $X^{\star}$, $E_{:N,1:q,:}$ represents the embedding of $Y$ while $E_{N:M,1:q,:}$ denotes the initial guess $\tilde{Y}^0$ obtained using the prediction token. This tensor is then processed by the attention modules as described next. 
\ds{Write matrix dimensions}
\subsection{Self-Attention Mechanisms}
The core of the architecture involves two alternating self-attention modules:  self-attention across components {\bf $\text{SelfAtt}_c$} and self-attention across datapoints {\bf $\text{SelfAtt}_b$} used for transforming the tokens. These alternating self-attention layers allow the model to learn both intra-samples components interactions and inter-samples relationships. Following standard design choices for  transformers \citep{Vaswani:2017}, we apply residual connections and layer normalization to ensure stability and robust gradient flow, i.e.: 
\begin{align*}
	E&\leftarrow \text{LN}\parens{E + \textbf{SelfAtt}_{c/b}(E)},\\
	E&\leftarrow \text{LN}\parens{E+ \text{MLP}(E)},
\end{align*}
where {\bf LN} denotes the layer normalization layer \citep{Ba:2016}, {\bf $\text{SelfAtt}_{c/b}$} denotes one of the considered self-attention mechanisms and {\bf MLP} is a one hidden-layer network acting on each embedding independently. Below, we describe both self-attention mechanisms in more detail.   

\paragraph{Self-attention across target components} allows interactions among components within each datapoint. 
It is applied independently per samples to preserve equivariance w.r.t. to the samples. 
We further employ a masking strategy that we found useful empirically: forcing target tokens to attend only to the covariate token, while allowing the covariate token to attend to all tokens. 
%

\paragraph{Self-Attention across datapoints} captures relationships between datapoint embeddings, allowing the model to aggregate information globally. 
It is applied between samples and independently per each input dimensions $p$ and $q$ to preserve equivariance. Similarly to \cite{hollmanntabpfn}, training and validation tokens only attend to training tokens. 
This is ensured in practice applying a suitable mask when computing attention. %

\subsection{Non-parametric equivariant decoder}
The decoder aggregates the processed embeddings to produce prediction $\hat{Y}$. This is achieved in two steps: an attention module first computes an intermediate prediction $\tilde{Y} = (\tilde{y}_m)_{m=1}^M$ in the form of a weighted average of training targets $Y$, then a residual correction is added to produce the final prediction. 
More precisely, the attention module uses the embeddings of the training and validation samples as keys and queries,  while the attention values are simply the training targets $Y$:
\begin{align}
	\tilde{y}_m = \sum_{n=1}^N y_{n} \text{SoftMax}\parens{\frac{\sum_{i,u}E_{n,i,u}E_{m,i,u}}{\sqrt{(1+q)d}}}.
\end{align} 
The residual correction, in the form of a point-wise MLP, operates independently on each dimension $j$ of the attention output $\tilde{y}_m$ so that equivariance is preserved while enabling nonlinear interactions between training values. 

Without the residual correction and removing the dependence of the keys and queries embeddings on the training targets $Y$ (for instance by setting the weights of the target encoder and pointwise MLP to $0$), the decoder becomes a  \textit{linear non-parametric regression estimator} \citep[Definition 1.7]{Tsybakov:2009}, which is a generalization of Nadaraya-Watson's estimator \footnote{Linearity, here, is w.r.t. the targets $Y$.} \citep{nadaraya1964estimating,watson1964smooth}. 
However, linear estimators are known to be suboptimal compared to non-linear ones \citep{donoho1998minimax}. This motivates introducing a nonlinear dependence of the estimator to $Y$, in our setting, to increase the expressiveness of the decoder allowing it to adapt to the prediction task at hand. Experimentally, we also found a clear improvement when adding such a residual correction and making the embeddings dependent on the targets.

%
%
%

\section{Experiments}\label{sec:experiments}
\begin{figure*}[t]
\center
\includegraphics[width=1.9\columnwidth]{figures/equivariance-error.pdf}
\caption{Equivariance error for TabPFN observed while training (left) and after training when considering different number of classes and ensembles (right). 
Training reduces slowly the equivariance error but even at the end of the pre-training the model is not close to being equivariant, even when considering ensembles, in particular when considering more classes.\label{fig:equi-gap-ensemble}}
\end{figure*}
We perform numerical experiments to evaluate our proposed model and compare it with two state-of-the-art pre-trained models for tabular dataset (TabPFN \citep{hollmanntabpfn} and MotherNet \citep{muller2023mothernet}), as well as classical machine learning methods. 

\subsection{Experimental setup}
{\bf Training procedure.} 
We use a similar training protocol as in \citet{hollmanntabpfn}, in which the model is trained on classification datasets generated according to their proposed artificial dataset prior. In this protocol, each dataset has a fixed size of $1024$ and is split into training and test uniformly at random. The maximum number of classes is fixed to $10$, while the maximum dimension of the covariate vector is fixed to $100$. Following \citet{muller2023mothernet}, we represent the target $y$ as a one-hot encoding vector whose dimension is the number of classes in the dataset. Moreover, we employ the  exact same strategy for handling missing values in the covariates. 
Training is performed using 153600 batches of 72 synthetically generated datasets each, which means the model was exposed to $\sim$11M artificial datasets during pre-training. 
This is a similar order of magnitude of datasets used for pre-training TabPFN by \citet{hollmanntabpfn}.  
We used the Adam optimizer \citep{kingmaadam} with initial learning rate of $0.0001$ and linear-warmup scheduler for the first $10$ epochs followed by cosine annealing~\citep{loshchilov-iclr17a} as in \citet{hollmanntabpfn}. The total training time of the network lasts  approximately 4 days on a single A100 GPU with 80GB of GPU memory. The resulting network is then used for all our evaluations without altering its parameters. 





{\bf Architecture details.} 
We use an \methodname{} network with $12$ self-attention layers alternating between both type of attention introduced in \cref{sec:equitabfn}: 6 blocks {\bf $\text{SelfAtt}_c$} and 6 blocks {\bf $\text{SelfAtt}_b$}. 
Each self-attention layer consists of a multi-head attention blocks with $4$ heads, embeddings of dimension $512$, and hidden layers of dimension $1024$. 
This choice ensures a fair comparison with the models used in \citet{hollmanntabpfn,muller2023mothernet}, since the number of parameters ($25.17M$) are of the same order when counting them as proposed in \citet{kaplan2020scaling}. 
%




\subsection{Evaluation of the equivariance gap}

%
%
%
%
%
%
%

%
%
%
%
%

We re-trained TabPFN using the same training protocol as \citet{hollmanntabpfn} to then evaluate its equivariance gap at intermediate training steps. \cref{fig:equi-gap-ensemble} (left) shows the equivariance error of TabPFN in terms of percentage of violation of \cref{eq:equivariance}, e.g. how frequently the predicted classes $f_{X,\sigma(Y)}(X^*)$ and $\sigma\parens{f_{X,Y}(X^*)}$ differ. We use over 512 random datasets drawn from the prior and report standard deviation. We do not make those plots for \methodname{} as the equivariance error is exactly 0 by construction.

%

In \cref{fig:equi-gap-ensemble} (left), the  equivariance error is  clear and slowly decreases  during training. This non-equivariance 1) induces additional error for the model as demonstrated in  \cref{prop:error_decomposition} and 2) causes the model to provide surprising predictions given that permuting the output order can change the results as seen in \cref{fig:equi-illustration}.  

To avoid this issue, \citet{hollmanntabpfn} proposed to run multiple predictions while randomly permuting the output. If done with all possible permutations, this gives an equivariant function as discussed in \cref{eq:equivariant-sum} however, this requires making $\mathcal{O}(\numlabels!)$ forward passes where $\numlabels$ is the number of features. This becomes quickly prohibitive, even for $\numlabels=10$ as considered in the original study.  We illustrate how fast the model becomes equivariant when using ensembles in \cref{fig:equi-gap-ensemble} (right). This clearly shows that while ensembling more permutation helps to make the model more equivariant, many ensembles are required, in particular when considering more classes as one needs to approximate the expectation of Eq. \ref{eq:equivariant-sum} which has $\mathcal{O}(\numlabels!)$ terms.

\subsection{Evaluation on real-world datasets}

Following \citet{hollmanntabpfn}, we evaluate \methodname{} on 30 real-world classification datasets from the OpenML-CC18 benchmark suite \citep{bischl2021openml} that contain less than $2000$ samples, $100$ covariates and $10$ classes (see \cref{table:datasets} of the appendix for a list of the 30 datasets). \cref{tab:accuracy} reports the average AUC on these datasets. 
%
%

We compare \methodname{} with two types of models: recent SOTA pre-trained models and more traditional machine learning methods. 
For pre-trained models, we use the ones provided by  TabPFN \citep{hollmanntabpfn} and MotherNet \citep{muller2023mothernet} as they achieved state-of-the-art results. We refer to these works for an in-depth comparison with other recent AutoML methods. 
For the traditional baselines, we consider similar baselines as \citet{muller2023mothernet}: Histogram Gradient Boosting (XGBoost) \citep{chen2016xgboost}, Logistic Regression, k-Nearest Neighbors, Random Forests and a vanilla MLP. We refer to that work for the details of the hyper-parameters for these baselines.

The method we propose outperforms TabPFN as well as other baselines which illustrates the practical benefit of having a target-equivariant model. We also report the performance of TabPFN and \methodname{} when using ensembling (method + Ens). Ensembling is done by averaging the predictions over several subsamples of the dataset and permutations of the target with the same procedure proposed by \citet{hollmanntabpfn}. Our method also benefits from ensembling but less so than TabPFN since the equivariance of the target is built in the inductive bias of our architecture.

In \cref{fig:critdiag}, we show the average rank for each method together with  horizontal bars to denote methods that are tied statistically \cite{demvsar2006}. 
While our method outperforms TabPFN in average rank and AUC, it is statistically tied with TabPFN but is the only baseline that is not tied statistically with XGBoost.


\begin{table}
\center
\begin{tabular}{lr}
\toprule
 Model & Average AUC \\
\midrule
\methodname{} $+$ Ens &        {\bf 0.894851}\\
TabPFN $+$ Ens &                       0.893162\\
Mothernet $+$ Ens &                     0.886897\\
\midrule
\methodname{} & {\bf 0.893040} \\
TabPFN & 0.890675 \\
XGBoost & 0.885853 \\
Logistic regression & 0.883984 \\
Mothernet & 0.879750 \\
Random Forest & 0.876427 \\
K-nearest neighbors & 0.848294 \\
MLP & 0.788029 \\
\bottomrule
\end{tabular}
\caption{Average AUC over 30 real-world datasets from OpenML-CC18 benchmark suite with less than $10$ classes.}
\label{tab:accuracy}
\end{table}


\begin{figure}
\center
\includegraphics[width=0.5\textwidth]{figures/critical-diagram.pdf}
\caption{Critical diagram on the 30 real-world datasets from OpenML-CC18 benchmark. \label{fig:critdiag}}
\end{figure}

\subsection{Evaluation on datasets with unseen class counts}
\begin{table}
	\center
\begin{tabular}{lr}
\toprule
 Model & Averaged AUC \\
\midrule
EquiTabPFN & {\bf 0.950626} \\
Random Forest & 0.942480 \\
XGBoost & 0.938711 \\
Logistic regression & 0.921125 \\
K-nearest neighbors & 0.872002 \\
\bottomrule
\end{tabular}
\caption{Average AUC over 9 real-world datasets from OpenML-CC18 benchmark suite with more than $10$ classes.}
\label{tab:accuracy_ood}
\end{table}




\cref{tab:accuracy_ood} presents the average AUC for $9$ datasets with more than $10$ classes from the OpenML-CC18 benchmark suite \citep{bischl2021openml} (see \cref{table:ood_datasets} of the appendix for a list of the $9$ datasets).  This represents an out-of-distribution scenario, as the pretraining phase only included datasets with up to $10$ classes. However, since our architecture is equivariant, it can handle any number of classes during inference, within memory constraints. Notably, \methodname{} outperforms the baselines, even with the number of classes exceeding those encountered during training.

It is worth emphasizing that performing inference on datasets with more classes than those seen during pre-training would be challenging
%
for TabPFN and MotherNet. This limitation arises because the method would need to project vectors with dimensions exceeding those encountered during training.

\subsection{Additional experiment details}
The results of \cref{tab:accuracy,tab:accuracy_ood} are obtained using the same evaluation protocol as in \citet{hollmanntabpfn} with code for training the baseline methods and selecting their hyper-parameters provided by the authors of \citep{muller2023mothernet}\footnote{https://github.com/microsoft/ticl}.
Specifically, each dataset is split randomly into training and test subsets of the same size ($N=M=1000$). Predictions for each method are averaged over $5$ random splits of each dataset. 
The three pre-trained models we consider (\methodname{}, TabPFN, MotherNet) do not require any dataset specific tuning, while all other baselines are trained by  allocating 60 minutes budget for hyper-parameter selection for each dataset split as done in \citet{hollmanntabpfn}.

\todo[inline]{@Michael: can you list the tids? And the number of classes considered? Are we using ensemble? No ensembles} 



\section{Conclusion}

In this paper, we discussed the property of target equivariance and its importance for foundational tabular models. We proved that having a non-equivariant model provably worsens the pre-training objective with an incompressible term and proposed an architecture that is equivariant to target permutation. We showed that our new architecture performs better than TabPFN, Mothernet and traditional tabular baselines on artificial and real-world datasets. We hope this work can be useful to interpret tabular foundational models as deep kernel machines and offer new insights to understand the performance of tabular foundational models.

We will release our code for training and evaluating our model, enabling reproducibility of our results.

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

\newpage

\section*{Impact Statement}

This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.



\bibliography{biblio}
\bibliographystyle{icml2025}


%
%
%
%
%
\newpage
\appendix
\onecolumn

\section{Proofs}\label{sec:proofs}
\begin{proof}[Proof of \cref{prop:error_decomposition}]
    By \cref{assump:invariant_distribution} we can express $\mathcal{L}$ as an expectation of the form:
    \begin{align}
        \mathcal{L}(f) &= \mathbb{E}_p\mathbb{E}_{\sigma} \brackets{\ell\parens{f_{X,\sigma(Y)}\parens{X^{\star}},\sigma(Y^{\star})}}.
    \end{align}

By \cref{assump:convex_loss}, $\ell$ is invariant to permutations, which allows to further write:
    \begin{align}
        \mathcal{L}(f) &= \mathbb{E}_p\mathbb{E}_{\sigma} \brackets{\ell\parens{\sigma^{-1}\parens{f_{X,\sigma(Y)}\parens{X^{\star}}},Y^{\star}}}.
    \end{align}

We will show that $\equigap{f}=\mathcal{L}(f)-\mathcal{L}(\equiproj{f})$ is non-negative and vanishes only when $f$ is equivariant. This is a direct consequence of Jensen's inequality applied to the strictly convex function $y\mapsto \ell(y,y^{\star})$ (\cref{assump:convex_loss}). Indeed, for any samples $(X,Y,X^{\star},Y^{\star})$, the following holds:
\begin{align*}
    \ell\parens{\equiproj{f}_{X,Y}\parens{X^{\star}},Y^{\star}} :&= \ell\parens{\mathbb{E}_{\sigma}\brackets{\sigma^{-1}\parens{f_{X,\sigma(Y)}\parens{X^{\star}}}},Y^{\star}}\\
    &\leq \mathbb{E}_{\sigma}\ell\parens{\sigma^{-1}\parens{f_{X,\sigma(Y)}\parens{X^{\star}}},Y^{\star}},
\end{align*} 
where the first line follows by definition of $\equiproj{f}$ while the second line uses Jensen's inequality. Further taking the expectation w.r.t. $p$ shows that $\equigap{f}\geq 0$. If $\equigap{f}=0$, then by the above inequality it holds that $\ell\parens{\equiproj{f}_{X,Y}\parens{X^{\star}},Y^{\star}} = \ell\parens{f_{X,Y}\parens{X^{\star}},Y^{\star}}$ almost surely. However, since $\ell$ is strictly convex in its first argument (\cref{assump:convex_loss}), the previous equality is only possible when $\equiproj{f}=f$ almost surely, meaning that $f$ is equivariant. 

Finally, to show the final result, we note that:
\begin{align*}
    \equigap{f} =& \mathcal{L}(f)- \mathcal{L}(\equiproj{f})\\
    =& \mathbb{E}_{p}\mathbb{E}_{\sigma}\brackets{\Verts{\sigma^{-1}\parens{f_{X,\sigma(Y)}\parens{X^{\star} }} - \equiproj{f}_{X,Y}\parens{X^{\star}} 
    + \equiproj{f}_{X,Y}\parens{X^{\star}}
    - Y^{\star}}^2} - \mathcal{L}(\equiproj{f})\\
    =& \mathbb{E}_{p}\mathbb{E}_{\sigma}\brackets{\Verts{\sigma^{-1}\parens{f_{X,\sigma(Y)}\parens{X^{\star} }} - \equiproj{f}_{X,Y}\parens{X^{\star}}}^2}\\
    &+2 \mathbb{E}_{p}\mathbb{E}_{\sigma}\brackets{\parens{\sigma^{-1}\parens{f_{X,\sigma(Y)}\parens{X^{\star} }} - \equiproj{f}_{X,Y}\parens{X^{\star}}}^{\top}\parens{\equiproj{f}_{X,Y}\parens{X^{\star}}-Y^{\star}}}\\
    =&\mathbb{E}_{p}\mathbb{E}_{\sigma}\brackets{\Verts{\sigma^{-1}\parens{f_{X,\sigma(Y)}\parens{X^{\star} }} - \equiproj{f}_{X,Y}\parens{X^{\star}}}^2}\\ 
    &+2 \underbrace{\mathbb{E}_{p}\brackets{\parens{\mathbb{E}_{\sigma}\brackets{\sigma^{-1}\parens{f_{X,\sigma(Y)}\parens{X^{\star} }}} - \equiproj{f}_{X,Y}\parens{X^{\star}}}^{\top}\parens{\equiproj{f}_{X,Y}\parens{X^{\star}}-Y^{\star}}}}_{=0}.
\end{align*}
Here, the cross-product term equals $0$ since $\mathbb{E}_{\sigma}\brackets{\sigma^{-1}\parens{f_{X,\sigma(Y)}\parens{X^{\star} }}} = \equiproj{f}_{X,Y}\parens{X^{\star}}$ by definition of $\equiproj{f}$. Hence, we have shown that:
\begin{align*}
    \equigap{f} = \mathbb{E}_{p}\mathbb{E}_{\sigma}\brackets{\Verts{\sigma^{-1}\parens{f_{X,\sigma(Y)}\parens{X^{\star} }} - \equiproj{f}_{X,Y}\parens{X^{\star}}}^2}
\end{align*}
Finally, we use the invariance of the squared error to permutations, the equivariance of $\equiproj{f}$ to permutations, and the invariance of $p$ to permutations to get:
\begin{align*}
    \equigap{f} &= \mathbb{E}_{p}\mathbb{E}_{\sigma}\brackets{\Verts{f_{X,\sigma(Y)}\parens{X^{\star} } - \sigma\parens{\equiproj{f}_{X,Y}\parens{X^{\star}}}}^2}\\
    &= \mathbb{E}_{p}\mathbb{E}_{\sigma}\brackets{\Verts{f_{X,\sigma(Y)}\parens{X^{\star}} -  \equiproj{f}_{X,\sigma(Y)}\parens{X^{\star}}}^2}\\
    &= \mathbb{E}_{p}\brackets{\Verts{f_{X,Y}\parens{X^{\star} } - \equiproj{f}_{X,Y}\parens{X^{\star}}}^2}.
\end{align*}
\end{proof}


\section{Binary classification decision boundary}

In \cref{fig:binary_boundary}, we show the decision boundary on 3 binary classification datasets for multiple baselines. To illustrate the stability of the method, we do not do ensembling for TabPFN and \methodname{}.

\begin{figure}
\center
\includegraphics[width=0.98\textwidth]{figures/classification_boundary_comparison.pdf}
\caption{Binary classification decision boundary for 7 methods on 3 datasets.
Even without ensembling, the boundary of \methodname{} is stable and smooth as opposed to TabPFN \label{fig:binary_boundary}.}
\end{figure}


\section{Additional experiment results}
\begin{figure}
\center
\includegraphics[width=0.75\textwidth]{figures/box_plot.pdf}
\caption{Boxplot of normalized AUC for all methods on the 30 classification datasets of \citet{hollmanntabpfn}.\label{fig:boxplot}
}
\end{figure}



In \cref{fig:boxplot}, we show the boxplot of normalized AUC in all datasets. The AUC is min-max normalized with the minimum and maximum scores observed on each dataset.

When computing critical diagram in \cref{fig:critdiag}, we use Autorank implementation \cite{Herbold2020} with default hyperparameters which corresponds to a significance level of $0.05$.
\section{Experimental details}

\begin{table*}
\center
\input{datasets_multiclass.tex}
	\caption{Test dataset names and properties, taken from \citet{hollmanntabpfn}. Here \emph{did} is the OpenML Dataset ID, \emph{p} the number of covariates, \emph{n} the number of samples, and \emph{q} the number of classes in each dataset.}
\label{table:datasets}
\end{table*}


\begin{table*}
\center
	\input{datasets_multiclass_ood.tex}
\caption{Names and properties of the  datasets  used for evaluating \methodname{} on unseen number of classes. The datasets are extracted from the OpenML-CC18 benchmark suite \citep{bischl2021openml} and have a number of classes greater than $10$ (ranging from $11$ to $30$).  Here \emph{did} is the OpenML Dataset ID, \emph{p} the number of covariates, \emph{n} the number of samples, and \emph{q} the number of classes in each dataset.  }
\label{table:ood_datasets}
\end{table*}






\end{document}
