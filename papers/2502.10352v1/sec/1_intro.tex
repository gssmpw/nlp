\section{Introduction}
\label{sec:intro}

\begin{figure*}
\centering
\includegraphics[width=.95\linewidth]{fig/overview.png}
\caption{
Comparison of (a) DtV (Diversify-then-Verify) and (b) \ours (\ourslong, ours).
}
\label{fig:overview}
\end{figure*}


Retrieval-Augmented Generation (RAG; \citealp{lewis-etal-2020-retrieval-neurips}) is expected to complement large language models (LLMs), trained on static data, when they struggle to provide reliable responses without external retrieval.
This challenge is particularly evident in enterprise settings, where queries are asked on a domain-specific and potentially evolving corpora, inaccessible during LLM training.
In such cases, RAG must (1) \textbf{disambiguate} short, vague user queries and (2) \textbf{retrieve} supporting evidence, %
to ensure the verifiability of the generated response.

A line of work on handling ambiguous question answering~\citep{min-etal-2020-ambigqa,cole-etal-2023-selectively,in-etal-2024-diversify-arxiv} adopted a Diversify-then-Verify (DtV) pipeline, which first generates diverse interpretations of the query using an LLM and then retrieves supporting documents using interpretations as search queries.
However, this process often leads to ungrounded or noisy interpretations, particularly when LLMs struggle with domain-specific or evolving meanings. For example, as shown in Figure~\ref{fig:overview}(a), the query ``What is HP'' may refer to Hewlett-Packard, horsepower, or Harry Potter.
LLMs trained on general knowledge would suggest all three, while an enterprise corpus may contain no references to Harry Potter.
In other words, this interpretation is \textbf{ungrounded}.
However, DtV still retrieves documents for all three interpretations, introducing noise from unnecessary retrieval.
Figure~\ref{fig:overview}(a) illustrates DtV, where the verification step comes after retrieval to prune out noisy passages, though 
``pseudo-interpretations'' have already  negatively influenced the process.

To address these limitations, we propose \ours (\ourslong), a unified  framework that integrates diversification and verification:
\ours is \textbf{agentic}, in the sense that we ask feedback from retriever and generator to ensure all  disambiguations are grounded upfront, as in Figure~\ref{fig:overview}(b).
This joint process, by avoiding 
noisy and unnecessary retrieval
of ungrounded interpretation,
mitigates cascading errors while improving efficiency. 
Specifically, agentic feedback from two sources play a critical role in the process:


\begin{itemize}
\item Relevance feedback from the retriever: We use a retriever to obtain a set of passages 
$U_q$, that are relevant to $q$ 
and encompass all interpretations found in the corpus. This retrieval-augmented diversification, as also employed in RAC~\citep{kim-etal-2023-tree}, naturally filters out unsupported interpretations, such as ``Harry Potter,''  as it is irrelevant, or may become obsolete as corpus evolves.


\item Execution feedback from the generator: %
Figure~\ref{fig:overview}(b) illustrates why relevance alone does not guarantee that a question can be answered, necessitating \emph{execution feedback}.
Passage $p_2$, though relevant to Hewlett-Packard as it describes its products, cannot provide an answer to the given question.
This motivates us to leverage the generator feedback to decide whether the question can be answered from the passage, based on which we prune out $p_2$.

\end{itemize}


Lastly, since feedback signals themselves can also be noisy, we introduce a consolidation phase to leverage consistency.
By aggregating  multiple feedback signals through clustering, \ours further mitigates potential noises.
This phase does not incur an additional feedback cost while enabling a more robust decision-making.

As a bonus, this inherently grounded design of \ours,
by tightly connecting each interpretation with its supporting passage, can be evaluated for
verifiability metrics popular in RAG~\citep{li-etal-2023-survey-arxiv,liu-etal-2023-evaluating}, such as citation quality.
We use both existing and grounded metrics, to show ours disambiguation is grounded and accurately answered. 




Our key contributions are as follows:
\begin{itemize}
\item We introduce \ours, a novel framework that unifies diversification and verification, enabled by an
agentic approach of leveraging feedback from the retriever and the generator.
\item We demonstrate that \ours improves efficiency by reducing unnecessary retrieval and verification steps, and also effectiveness, by mitigating cascading errors common in DtV.
\item We empirically validate \ours on widely adopted ambiguous QA benchmark, ASQA~\citep{stelmakh-etal-2022-asqa}, with average gain of 23\% in $\textrm{F}_1$ score across different backbone LLMs.\footnote{The gain is computed against the best baseline for each backbone LLM.}
We also release our code and verifiability evaluation framework to facilitate future research.
\end{itemize}
