\begin{figure*}[tbp]
\centering
\includegraphics[width=\linewidth]{fig/ours.png}
\caption{
Illustration of the full pipeline of \ours: Verified diversification (Section~\ref{subsec:extraction}) followed by consolidation phase (Section~\ref{subsec:clustering}).
On the right, yellow and blue dots represent embeddings of generated interpretations and their answers, embedded together after concatenation, while yellow color denotes medoids, or representatives chosen from each cluster.
}
\label{fig:ours}
\end{figure*}

\section{Method}
\label{sec:method}

We illustrate \ours with two distinctions:
Verified Diversification (Section~\ref{subsec:extraction})
and 
Consolidating Feedback (Section~\ref{subsec:clustering}).
In Section~\ref{subsec:eval}, we provide grounded evaluation protocols.




\subsection{Verified Diversification}
\label{subsec:extraction}

Unlike DtV,
we ensure that $\hat{\mathcal{Q}}$ includes only grounded interpretations by leveraging agentic feedback from both the retriever and the generator.
Unlike Eq.~\ref{eq:diva_q_extract}, where pseudo-interpretations are conditioned solely on the ambiguous query $q$, our approach conditions on both the query $q$ and the corpus $C$.
This ensures that each interpretation $\hat{q} \in \hat{\mathcal{Q}}$ is explicitly grounded to at least one passage from the corpus, ensuring 
both \textbf{relevance} to $q$, using the relevance feedback of retrieving $U_q$, and \textbf{answerability} of the interpretation.

\begin{equation}
\hat{\mathcal{Q}},
\hat{\mathcal{P}},
\hat{\mathcal{Y}} \leftarrow 
\textrm{VD}(q,U_q).
\label{eq:ours_q_extract}
\end{equation}




Note that Eq.~\ref{eq:ours_q_extract}
jointly infers
whether the retriever can retrieve a relevant passage (relevance) and an answer to $q$ can be generated from the retrieved passage.
As a result,
interpretations $\hat{\mathcal{Q}}$ and answers $\hat{\mathcal{Y}}$ are obtained simultaneously, along with the list of passages $\hat{\mathcal{P}}$ that support each identified question-answer pair.


\paragraph{Relevance Feedback.}


``Relevance feedback from the retriever'' in Figure~\ref{fig:ours} shows how
we first retrieve
$U_q$ with a single round of retrieval with the original query $q$ before generating interpretations. To ensure high recall within the universe, we use $q'$ relaxed as below:

\begin{equation}
q^\prime \leftarrow \texttt{LLM}(q; I_{\textrm{R}}),  
\end{equation}
\begin{equation}
U_q \leftarrow \argtopk_{p} \mathrm{sim} \left( q^\prime, p \right).
\end{equation}
With relaxed $q^\prime$ and
choice of a larger \(k\) for retrieval,\footnote{$k$ was empirically tuned to 20, which nearly matched G-$\textrm{F}_1$ (described in \S~\ref{subsec:eval}) from top-100 with GPT-4o backbone.}
we further increase the diversity and coverage of the interpretations while keeping the process computationally efficient, marked as ``relevance feedback from the retriever'' in Figure~\ref{fig:ours}.



\paragraph{Execution Feedback.} 
Execution feedback from the LLM generator in Figure~\ref{fig:ours}
prunes interpretatons that cannot be answered.
Conditioned on our high-coverage universe $U_q$, the LLM generator is prompted to identify interpretations $\hat{\mathcal{Q}}$ and corresponding answers $\hat{\mathcal{Y}}$ from \emph{each} passage.

This ensures the answerability of each disambiguated query $\hat{q}_i$, given the passage $\hat{p}_i$ from which it was derived:
For example, in Figure~\ref{fig:ours},
passages like $p_2$, which mentions HP products but cannot answer the query asking what HP is--are effectively pruned through failed execution of generating question-answer pair.
In contrast, with passages like $p_1$, a valid question-answer pair is generated.
Such execution feedback is formalized as follows,
\begin{equation}
(\hat{q}_i, \hat{y}_i) \leftarrow \texttt{LLM}(q, p_i; I_{\textrm{E}}),
\label{eq:ours_q_extract_single}
\end{equation}
of which the LLM prompt $I_{\textrm{E}}$ can be found in Figure~\ref{fig:prompt_q_extract}.

From a practical standpoint, Eq.~\ref{eq:ours_q_extract_single} can be executed in parallel, with each LLM call processing a single passage at a time.
This minimizes the input sequence length for each call, optimizing both latency and computational overhead while reducing hallucination in less capable models.







\input{sec/alg}
\input{tab/tab_asymptotic_cost}

Table~\ref{tab:comp} provides comparison of asymptotic cost of retrieval and LLM calls in \ours and DtV.
First, while \ours
makes a single call to the retriever as both builds on $U_q$, DtV issues several search queries, proportional to the number of pseudo-interpretations $|\hat{\mathcal{Q}}|$.
Next, the number of LLM calls and input size of each call, measured in terms of the number of passages, also shows the efficiency of \ours.
DtV provides several passages as a list input, increasing the context size and the chance of introducing noise while processing the elongated input.
In contrast, 
\ours requests for answerability feedback per  each passage in $U_q$, 
using only a single passage as a  context.

















\subsection{Consolidated Feedbacks}
\label{subsec:clustering}


While retriever and generator feedback are essential for verification, both are prone to noise. %
For robust verification,
we propose to consolidate retriever and generator feedback into a unified, robust signal, without additional retrieval or inference calls. %






Specifically, question-answer pair from generator  are projected into a latent space $\mathbb{R}^d$ using an encoder $f$ from the retriever (Figure~\ref{fig:ours}).
We then cluster similar pairs,
to choose those consistently supported by relevant passages, while filtering out outliers from noisy passages.
In this process,
multiple generator and retriever feedback signals are consolidated, which aligns with seeking the most consistent outputs~\citep{chen-etal-2023-codet-iclr}, improving robustness by inference scaling of aggregating multiple LLM outputs.
Additionally, selecting a medoid question per cluster deduplicates redundant interpretations, whereas DtV defers deduplication to the answer generation stage in Eq.~\ref{eq:diva_answer_gen}.

While DtV uses similarity for pruning $U$, 
we use both retriever and generator feedback, for another purpose of consolidation, to leverage richer signals including interaction between the two.






\subsection{Grounding in Evaluation}
\label{subsec:eval}

This section discusses existing  evaluation metrics, contrasted with
 grounded evaluation, often required  in enterprise RAG scenarios.
 

\paragraph{Existing: Ungrounded Metrics.}
Current benchmarks~\citep{min-etal-2020-ambigqa, stelmakh-etal-2022-asqa} measure whether diverse human-annotated interpretations ``match''  model-generated ones, typically using recall as the primary metric. Lexical similarity (e.g., BLEU) determines the match between generated ($\hat{q}$) and reference ($\tilde{q}$) interpretations:

\begin{equation}
V(\hat{q},\tilde{q}) \in \{0,1\}.
\label{eq:simple_match}
\end{equation}

We classify this as \textbf{ungrounded evaluation}, as it does not assess whether retrieved passage supports the generated interpretation.

\paragraph{Enterprise: Grounded Precision and Recall.}

In enterprise RAG, answer verifiability—measured through whether answer can be supported by the correct citation of a supporting passage in the corpus~\citep{li-etal-2023-survey-arxiv, liu-etal-2023-evaluating}—has been a  evaluation criterion. This concept naturally extends to verify disambiguation, where an interpretation is validated based on whether its answer can be supported by a passage in the corpus, as a binary evaluation function below:

\begin{equation}
V(\hat{q},\hat{p}) \in \{0,1\},
\end{equation}
where  $\hat{p}$
is predicted as the supporting passage associated with $\hat{q}$ by the model.










With this extended $V$,
grounded precision is defined as
the ratio of correctly grounded
among model-generated interpretations:
\begin{align}
\mathrm{G\text{-}Precision} = \frac{1}{|\hat{\mathcal{Q}}|} \sum_{\hat{q}\in \hat{\mathcal{Q}}} V(\hat{q}, \hat{p}).
\label{eq:def_precision}
\end{align}


While optimizing for existing recall may encourage ungrounded diversity, pursuing G-precision counterbalances this by assessing whether the interpretation is accurately grounded. 

Similarly, we extend  to define a grounded recall metric that evaluates model-generated interpretations,  only against the \textbf{grounded gold} set, $\bar{\mathcal{Q}}$. This ensures that generating ungrounded interpretations, such as “Harry Potter,” is penalized in G-Recall. As a result, models that prioritize ungrounded diversity will experience a significant drop when transitioning from ungrounded recall to G-Recall.






\begin{equation}
\mathrm{G\text{-}Recall} = \frac{1}{|\bar{\mathcal{Q}}|} \sum_{\bar{q} \in \bar{\mathcal{Q}}} V(\bar{q}, \hat{p}),
\label{eq:def_recall}
\end{equation}
We provide further details on how we obtain $\bar{\mathcal{Q}}$ in Appendix~\ref{app:precision_recall}.











