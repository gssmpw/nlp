\begin{figure*}[!ht]
\centering
\includegraphics[width=.85\linewidth]{fig/production_example.png}
\caption{Query clarification in Snowflake’s Cortex Agents API setup with tool access to a series of synthetically generated insurance documents retrieved via Cortex Search services.}
\label{fig:arctic}
\end{figure*}


\section{Application in Production }

Figure~\ref{fig:arctic} illustrates a real-world deployment of our method within Snowflake’s Cortex Agents API, highlighting its practical impact. In this production setting, our approach is applied to a synthetic corpus for an insurance company analyzing repair invoices related to auto insurance claims.

The figure demonstrates how a vague inquiry, such as “rental cars,” is disambiguated into grounded clarifications. Each clarified question is grounded to the  cited passages
that can answer the question, obtained from the verification process (Eq. 5). This approach significantly enhances retrieval accuracy and improves the overall user experience.




\section{Conclusion}

We question the common workflow of DtV for grounding ambiguous questions in RAG.
We proposed \ours,
which more efficiently and effectively
grounds diversification to the retrieval corpus, ensuring verifiability of the generated interpretations and answers.
Both ungrounded and grounded evaluation empirically validated our method significantly improves groundedness, while balancing with human-level diversity.






\section*{Limitations}

Though we propose consolidation to handle noisy feedback, we have not considered extreme cases  where the retriever or generator performs unreasonably poor or adversarially.

This would impact the initial retrieval step, where an adaptive scheme can mitigate by better balancing the retrieval breadth (with additional calls) and efficiency.
Developing an approach that allows the agent to dynamically adapt, based on feedback quality, remains an open challenge for future work.
