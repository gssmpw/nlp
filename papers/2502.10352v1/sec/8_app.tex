



\section{Grounding Evaluation}
\label{app:precision_recall}

Here, we explain in more detail how our evaluation protocol identified matching pairs for computing precision and recall.
We begin by reviewing the setting of previous works and how original precision/recall has been computed in ambiguous question answering evaluation.

\paragraph{Ungrounded Precision/Recall} To obtain precision, the ratio of correct interpretations among generated, correctness labels have been determined by regarding human interpretations $\tilde{\mathcal{Q}}$ as ground-truth.
If the generated interpretation matches one of the human interpretations, it is considered correct; otherwise, it is considered incorrect.
This can be formulated as
\begin{equation}
\mathrm{Precision} = \frac{1}{|\hat{\mathcal{Q}}|} \sum_{\hat{q}\in \hat{\mathcal{Q}}} \texttt{Match} \left(\hat{q},\tilde{\mathcal{Q}}\right), 
\label{eq:def_precision_old}
\end{equation}
where $\texttt{Match}(\cdot, \cdot)$ denotes whether two questions are matched, or, the extended match
\begin{equation}
\texttt{Match}(\hat{q}, \tilde{\mathcal{Q}}) = \mathds{1} \left( \exists_{\tilde{q}\in\tilde{\mathcal{Q}}} \ \texttt{Match}\left(\hat{q},\tilde{q}\right) \right),
\end{equation}
compared against the set of interpretations $\tilde{\mathcal{Q}}$. %
Following the notation from the main text, from this point we denote such matching function as $V$, for the sake of notational simplicity.

Before LLM-as-a-judge was widely adopted, 
$V$ was often instantiated with measures such as lexical-overlap based scores, such as BLEU, exceeding some threshold $\tau$ or not,
\begin{equation}
V \left(\hat{q},\tilde{q}\right) = \mathds{1} \left( \texttt{BLEU}\left(\hat{q},\tilde{q}\right) > \tau \right).
\end{equation}
While such pairwise match can be easily replaced with querying an LLM judge, it would incur $|\hat{\mathcal{Q}}|\times|\tilde{\mathcal{Q}}|$ LLM calls;
for the sake of efficiency, we let the LLM judge to directly determine the match against the set of human interpretations as follows:
\begin{equation}
V\left(\hat{q},\tilde{\mathcal{Q}}\right) = \texttt{LLM}\left(\hat{q},\tilde{\mathcal{Q}}; I_{\textrm{M}}\right).
\label{eq:list_match_llm}
\end{equation}

Similarly, recall, the proportion of ground-truth interpretations successfully generated, is defined as
\begin{equation}
\mathrm{Recall} = \frac{1}{|\tilde{\mathcal{Q}}|} \sum_{\tilde{q}\in \tilde{\mathcal{Q}}}
V \left(\tilde{q},\hat{\mathcal{Q}}\right), 
\label{eq:def_recall_old}
\end{equation}
where whether each $\tilde{q}$ has been covered or not is determined with an LLM in the same way as Eq.~\ref{eq:list_match_llm}.

\paragraph{Grounded Precision/Recall} Our grounded evaluation essentially replaces 
$V(\cdot, \cdot)$ with new matching mechanism that also counts grounding.
For grounding the precision metric, we directly verify if the supporting passage $\hat{p}$ provided along with $\hat{q}$ can answer $\hat{q}$. 
Thus, Eq.~\ref{eq:def_precision_old} is rewritten to consider the `match' between $\hat{q}$ and $\hat{p}$ as
\begin{equation}
\mathrm{Grounded\ Precision} = \frac{1}{|\hat{\mathcal{Q}}|} \sum_{\hat{q}\in \hat{\mathcal{Q}}}
V \left(\hat{q},\hat{p}\right),
\label{eq:def_precision_transition}
\end{equation}
where we replace $\hat{p}$ with
retrieved passages from $U_q$,
proxy to the whole passages $C$,
if such supporting passage $\hat{p}$ is not available, for example for the case of pseudo-interpretations which are obtained independently of retrieval.
Implementation-wise, $V$ is realized with an LLM judge as
\begin{equation}
V(\hat{q}, \hat{p}) = \texttt{LLM}\left(\hat{q},\hat{p};I_{\textrm{V}}\right).
\end{equation}




With the same rationale, we redefine recall as a grounded metric
\begin{multline}
\mathrm{Grounded\ Recall} =
\frac{1}{|\bar{\mathcal{Q}}|} \sum_{\bar{q}\in \bar{\mathcal{Q}}}
V \left( \bar{q}, \hat{p} \right),
\label{eq:def_recall_transition}
\end{multline}
where we also accommodate models that do not provide $\hat{p}$ for each $\hat{q}$ with $\tilde{\mathcal{P}}$, as for precision.

In Eq.~\ref{eq:def_recall_old}, the ground-truth interpretation has been approximated by human interpretations $\tilde{\mathcal{Q}}$.
To increase the recall of this proxy,
we add verified model prediction $V(\hat{q},\hat{p})=1$.
\begin{equation}
\bar{\mathcal{Q}} =  \left\{ \hat{q}\in (\hat{\mathcal{Q}} \cup \tilde{\mathcal{Q}}) \,\middle|\, 
V \left(\hat{q},\hat{p}\right)\right\}.
\end{equation}
Eq.~\ref{eq:def_recall_transition} shows how
$\bar{\mathcal{Q}}$ complements the human interpretations $\tilde{\mathcal{Q}}$.


The prompts used to instruct the judge LLM, $I_\textrm{M}$ for finding a match in a list of questions and $I_\textrm{V}$ for verifying a passage can be found in Appendix~\ref{app:prompts}.




 


\begin{figure*}[tbp]
\centering
\includegraphics[width=.85\linewidth]{fig/comparison.png}
\caption{
Comparison of end-to-end workflow of \ours and DtV (DIVA; \citealp{in-etal-2024-diversify-arxiv}) for handling ambiguous question answering task. Vertical arrangement denotes sequential dependency, while calls that can run in parallel are placed at the same horizontal level.
}
\label{fig:e2e_compare}
\end{figure*}

\section{E2E Pipeline of \ours and DtV}
\label{app:detailed_workflow}
Figure~\ref{fig:e2e_compare} describes how an ambiguous question is answered, with our \ours and DIVA~\citep{in-etal-2024-diversify-arxiv}, a representative of Diversify-then-Verify (DtV) workflow.
DtV involves more sequential steps, verifying the relevance of passages post hoc while in \ours verification is integrated into diversification, as shown on the left.


\section{LLM Prompts}
\label{app:prompts}
In this section, we provide the instruction prompts provided to LLMs.
For reproducing baselines, we reused their prompts, $I_{\textrm{P}}$ for generating pseudo-interpretations~\citep{in-etal-2024-diversify-arxiv} or $I_{\textrm{G}}$ for generating a list of interpretations and answers at once~\citep{kim-etal-2023-tree}, which can be found in their respective papers.

Figure~\ref{fig:prompt_q_extract} shows the prompt used to generate interpretation and answer in \ours, as described in Eq.~\ref{eq:ours_q_extract_single}.
Figure~\ref{fig:prompt_q_Q_match} and \ref{fig:prompt_q_p_veirfy} present prompts used for evaluation, as discussed in detail in Appendix~\ref{app:precision_recall}.

\begin{figure*}[!htbp]
\centering
\begin{tcolorbox}[boxrule=0pt, title=Prompt for Verified Diversification]
Given an ambiguous query and one of the passages from retrieval results, provide a disambiguated query which can be answered by the passage.
Try to infer the user's intent with the ambiguous query and think of possible concrete, non-ambiguous rewritten questions.
If you cannot find any of them, which can be answered by the provided document, simply abstain by replying with `null'.
You should provide at most one subquestion, the most relevant one you can think of.\\

Here are the rules to follow when generating the question and answer:\\
1. The generated question must be a disambiguation of the original ambiguous query.\\
2. The question should be fully answerable from information present in given passage. Even if the passage is relevant to the original ambiguous query, if it is not self-contained, abstain by responding with `null'.\\
3. Make sure the question is clear and unambiguous, while clarifying the intent of the original ambiguous question.\\
4. Phrases like `based on the provided context', `according to the passage', etc., are not allowed to appear in the question. Similarly, questions such as ``What is not mentioned about something in the passage?'' are not acceptable.\\
5. When addressing questions tied to a specific moment, provide the clearest possible time reference. Avoid ambiguous questions such as ``Which country has won the most recent World Cup?'' since the answer varies depending on when the question is asked.\\
6. The answer must be specifically based on the information provided in the passage. Your prior knowledge should not intervene in answering the identified clarification question.\\

Input fields are:\\
\textbf{Question}: \{\texttt{ambiguous question} ($q$)\}\\
\textbf{Passage}: \{\texttt{passage} ($p$)\}\\

Output fields are:\\
\textbf{Interpretation}:  \{\texttt{generated interpretation} ($\hat{q}$)\} \\
\textbf{Answer}:  \{\texttt{generated answer} ($\hat{y}$)\}
\end{tcolorbox}
\caption{Prompt $I_{\textrm{E}}$ for obtaining interpretation $\hat{q}$ and answer $\hat{y}$ with execution feedback from the LLM.}
\label{fig:prompt_q_extract}
\end{figure*}



\begin{figure*}[!htbp]
\centering
\begin{tcolorbox}[boxrule=0pt, title=Evaluation Prompt for Ungrounded Precision/Recall]
Given a list of generated disambiguated subquestions that clarify the intent of an ambiguous question, compare them with the list of predefined subquestions and determine how many have been successfully identified. You should return a binary label, Yes or No, for each subquestion indicating whether it was covered or not.\\

Input fields are:\\
\textbf{Question}: \{\texttt{ambiguous question} ($q$)\}\\
\textbf{Generated Disambiguations}: \{\texttt{generated interpretations} ($\hat{\mathcal{Q}}$)\}\\
\textbf{Ground-truth Disambiguations}: \{\texttt{human-annotated interpretations} ($\tilde{\mathcal{Q}}$)\}\\

Output fields are:\\
\textbf{Decisions}:  \{\texttt{match} ($V(\hat{q},\tilde{q})$'s)\} \\
\end{tcolorbox}
\caption{Prompt $I_{\textrm{M}}$ for determining matches between $\hat{q}$'s and $\tilde{q}$'s.}
\label{fig:prompt_q_Q_match}
\end{figure*}




\begin{figure*}[!htbp]
\centering
\begin{tcolorbox}[boxrule=0pt, title=Evaluation Prompt for Verification]
Given a question, an answer and an associated passage, decide if the passage can support the answer, providing enough evidence to reach the answer given the question.
Your answer should be either Yes or No.\\

Input fields are:\\
\textbf{Question}: \{\texttt{interpretation} ($\hat{q}$)\}\\
\textbf{Passage}: \{\texttt{passage} ($\hat{p}$)\}\\

Output fields are:\\
\textbf{Decision}:  \{\texttt{match} ($V(\hat{q},\hat{p})$)\} \\
\end{tcolorbox}
\caption{Prompt $I_{\textrm{V}}$ for determining a match between $\hat{q}$ and $\hat{p}$.}
\label{fig:prompt_q_p_veirfy}
\end{figure*}


\begin{figure*}[!htbp]
\centering
\begin{tcolorbox}[boxrule=0pt, title=Evaluation Prompt for Deduplication]
Given a list of subquestions, which are derived disambiguations of an ambiguous query,
remove nearly identical duplicates and leave only distinct ones.
You should provide a list of the remaining subquestions, one at a line.\\

Input fields are:\\
\textbf{Ambiguous Question}: \{\texttt{ambiguous question} ($q$)\}\\
\textbf{List of Disambiguated Subquestions}: \{\texttt{interpretations} ($\hat{\mathcal{Q}}$ or $\tilde{\mathcal{Q}}$)\}\\

Output fields are:\\
\textbf{List of Unique Subquestions}:  \{\texttt{deduplicated interpretations} \} \\
\end{tcolorbox}
\caption{Prompt $I_{\textrm{D}}$ for removing (near-)duplicates in a list of interpretations for an ambiguous question.}
\label{fig:prompt_dedup}
\end{figure*}


\section{More on Implementation Details}
\label{app:impl_details}

Here, we provide more details regarding the experimental settings.
For main experiments, \ours (ours) and RAC have used top-20 passages, retrieved with relaxed question $q'$ as the search query.
For DtV, we have used the same encoder, Qwen, for pruning the passages and then fixed verifier LLM as GPT-4o, as mentioned previously in Section~\ref{subsubsec:implt_details}.
Following the original paper's setting, we retrieved top-5 passages for each pseudo-interpretation, then finally selected top-5 among them, after pruning.
