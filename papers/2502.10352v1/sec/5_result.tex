

\section{Result}

We empirically validate \ours ensures human-level diversity, while balancing with grounding accuracy.
We ablate different components to 
show their contributions and also analyze errors.
\input{tab/tab_main}

\subsection{Experimental Settings}

\subsubsection{Evaluation Metrics}\label{sec:met}


To evaluate diversity,
we report the average number of generated interpretations per query $|\hat{\mathcal{Q}}|$,
the proportion of queries with 
sufficient diversity, denoted as Sufficient\%\footnote{We deduplicate the generated interpretations using the prompt $I_{\textrm{D}}$ in Figure~\ref{fig:prompt_dedup} and count how many unique interpretations are found.}
and (ungrounded) recall, all computed against human interpretations without grounding.

For grounded metrics, we use
our agentic evaluation protocol described in Section~\ref{subsec:eval} to compute G-precision, G-recall, and G-$\textrm{F}_1$ score defined as the harmonic mean of the precision and recall, balancing the objective of two.



\subsubsection{Evaluation Datasets}
We consider the ASQA benchmark~\citep{stelmakh-etal-2022-asqa} as our main target dataset for evaluation.
It is built upon the ambiguous question $q$ from the AmbigNQ benchmark~\citep{min-etal-2020-ambigqa}, adding: (1) 2-6 human-annotated interpretations $\tilde{\mathcal{Q}}$, 
along with corresponding answers $\tilde{\mathcal{Y}}$,
 (2)  optionally attached with a supporting passage $\tilde{p}_i$ per each interpretation $\tilde{q}_i$ from the corpus $C$, Wikipedia.
We utilize its validation split, which consists of 948 ambiguous questions with list of human-annotated interpretations attached to each question, list of answers to those questions, and also long-form answers composed by aggregating those answers.


\subsubsection{Implementation Details}
\label{subsubsec:implt_details}

The retrieval system comprises \texttt{arctic-embed} \citep{merrick-etal-2024-arctic-arxiv}\footnote{\texttt{Snowflake/snowflake-arctic-embed-m-v2.0}} based first-phase retrieval and second-phase passage reranking using \texttt{gte-Qwen2}~\citep{li-etal-2023-towards-arxiv}.\footnote{\texttt{Alibaba-NLP/gte-Qwen2-7B-instruct}}

For the clustering algorithm for consolidation phase described in Section~\ref{subsec:clustering}, we used HDBSCAN~\citep{campello-etal-2013-density-pakdd}, a hierarchical density-based clustering algorithm.
As the encoder $f$ for obtaining embeddings, we reused the same \texttt{gte-Qwen2} embedding model used in retrieval.
For decoding, greedy decoding was used to obtain deterministic responses from LLMs.

For verification in DtV, we fixed the verifier as the most expensive model, GPT-4o, as less capable 8B and 70B models struggled to provide reliable results for long-context inputs, damaging its performance severely.
More details can be found in Appendix~\ref{app:impl_details}.

\subsubsection{Baselines}
\label{subsubsec:baselines}

\input{tab/tab_comparison}

While we mainly compare \ours against DtV and its ablated version as baseline,
we also consider retrieval-augmented clarification~(RAC; \citealp{kim-etal-2023-tree})
as contrasted in Table~\ref{tab:comp_strategies}
which leverages relevance feedback
in diversification similar to \ours:
\begin{equation}
\texttt{LLM}(q, U_q; I_{\textrm{G}}).
\label{eq:rac}
\end{equation}

Our distinction is
first extracting
$(\hat{q},\hat{y})$
then jointly verifying relevance and answerability,
while RAC evaluates the entire $U_q$ with a single long-context LLM inference---negative impact of such decision is empirically discussed in Section~\ref{sec:results}.


\subsection{Results}\label{sec:results}

In this section, we validate the effectiveness and efficiency of \ours in disambiguating ambiguous questions, answering the following research questions:
\begin{itemize}
\item RQ1: Is \ours diverse?
\item RQ2: Does diversity balance with grounding?

\item RQ3: What is the effect of consolidation phase, and how is it tunable?
\end{itemize}




\paragraph{\ours ensures human-level diversity.}
Table~\ref{tab:main_orig_query} reports
scores of \ours, human-annotation, RAC~\citep{kim-etal-2023-tree}, and DtV~\citep{in-etal-2024-diversify-arxiv},
in terms of diversity metrics
`$|\hat{\mathcal{Q}}|$',
`Sufficient\%',
and `Recall' (Section~\ref{sec:met}).

Note, in all metrics,
\ours's output is comparable to human interpretation, and remains consistent across diverse models sizes.
In contrast, diversity of interpretations from DtV and RAC tend to be highly affected by model choice/size.
For example, with smaller model, 8B, the output becomes noticeably less diverse, e.g., Sufficient\% is dropped to <1\%.
This validates \ours generalizes better for smaller models and performs consistently over model sizes, by
decomposing context size to a single passage and verifying each $\hat{y}$ at a time, and allowing separate reasoning on each passage.
 

While ungrounded recall scores are generally higher for DtV, 
we show the negative consequences of  
incentivizing ungrounded diversity in the next section.
 

\paragraph{\ours ensures verifiable diversity.}



Table~\ref{tab:main_orig_query} shows ungrounded metrics fail to ground to corpus:
35\% of human-annoated 
fall into such category, 
which will incentivize recall when matches are found from generated interpretations.



It also contrasts
ungrounded metrics, with verifiable metrics G-precision, G-recall and G-$\textrm{F}_1$, to show distinctions.
\ours achieves the highest G-precision, of 93\% with GPT-4o and 81\% with LLaMA 70B as the backbone LLM.

These results reinforce our earlier discussion in Section 4.3:  DtV without verification, despite achieving higher ungrounded recall, suffers from the lowest G-precision across all backbone LLMs, highlighting its tendency to generate unverified interpretations. In contrast, both RAC and \ours produce more accurately grounded interpretations, with \ours further benefiting from both retriever and generator feedback. The G-recall results confirm this advantage, showing that interpretations $\hat{q}_i$ generated by \ours are well-supported by their corresponding passages $\hat{p}_i$, ultimately boosting grounded recall to approximately 57\%.


\input{tab/tab_cluster}

Finally, to balance the conflicting optimization objective of precision and recall, G-$\textrm{F}_1$ is reported.
G-$\textrm{F}_1$ scores show clear gap in performance between \ours and baselines for the goal of pursuing both accurately grounded and diverse interpretations.
For instance, while G-precision of RAC is higher with the 8B backbone, it fails to ensure enough diversity, where the average number of interpretations obtained per question is less than 1.
This leads to significantly lower G-recall and thus G-$\textrm{F}_1$ score, reflecting its failure to provide meaningful disambiguations of the original ambiguous question.














\paragraph{Consolidation with Clustering.}
To continue with our RQ3, Table~\ref{tab:cluster_strats} (top half) shows the consolidation step in \ours can be parameterized
to control the number of resulting interpretations.
Specifically, it can be adjusted to %
favor less clusters, denoted as `conservative.'
DtV approaches do not provide such knobs to control,
as deciding the number of resulting interpretations is up to the LLM.

Table~\ref{tab:cluster_strats} (bottom half) also shows \ours can 
control how to embed
a clustering space.
By default, \ours concatenates interpretation $\hat{q}$ and answer $\hat{y}$, then encodes it with an encoder $f$ to obtain $f(\hat{q};\hat{y})$, utilizing match between answers or the accordance of question-answer.
It is shown that embedding questions only also achieves comparable performance,
suggesting interpretations from \ours are already reliable,
as \ours already pruned much of the noisy questions and passages during diversification.


\begin{figure}[tbp]
\centering
\includegraphics[width=\linewidth]{fig/errors.png}
\caption{
Analysis on accuracy of generated $\hat{q}$'s and $\hat{y}$'s from \ours.
(\romannumeral 1, answer correctness) Models easily derive correct $\hat{q}, \hat{y}$ given an answerable passage. (\romannumeral 2, interpretation error rate) Impact of model scale is more critical in discerning unanswerable passages.
}
\label{fig:error_types}
\end{figure}

\paragraph{Error Analysis.}
We conclude our discussion with error analysis:
Figure~\ref{fig:error_types} shows categorization $(\hat{q}, \hat{y})$ pairs generated from \ours.
Specifically, an LLM judge decided whether $\hat{q}$ is relevant to the original question $q$, and whether $\hat{q}$ can be answered with $\hat{p}$.

First, the top-left corner of each plot in Figure~\ref{fig:error_types} illustrates cases where the retrieved passage is both relevant and answerable. The 8B model correctly answers 92\% of these instances, while stronger models achieve even higher accuracy, reaching 98\% and 99\%, respectively.

Second, the remaining L-shaped sections depict scenarios where the passage $\hat{p}$ is either irrelevant or unanswerable.
These numbers indicate the proportion of incorrect answers due to ungrounded interpretations, showing smaller models struggling more.
However, these errors are well mitigated with scaling model size,
where the 70B model cut errors by more than half and GPT-4o reduces them to less than one-sixth, compared to 8B.


