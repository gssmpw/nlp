\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

% \usepackage{lineno}
% \linenumbers
\usepackage{listings}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{amssymb}
\usepackage{float}
\usepackage{graphicx}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% FOR MATPLOTLIB TO PGF PLOTS %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage[utf8]{inputenc}
\usepackage{adjustbox}
\DeclareUnicodeCharacter{2212}{−}
\usepgfplotslibrary{groupplots,dateplot}
\usetikzlibrary{patterns,shapes.arrows}
\pgfplotsset{compat=newest}
\pgfplotsset{compat=1.18}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% END MATPLOTLIB TO PGF PLOTS %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{xintexpr}
% \usepackage{minted}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage[ruled, vlined, linesnumbered]{algorithm2e}
\usepackage{subcaption}
\usepackage{makecell}

% shortcuts for vectors and matrices
\newcommand{\va}{{\mathbf{a}}}
\newcommand{\vb}{{\mathbf{b}}}
\newcommand{\vc}{{\mathbf{c}}}
\newcommand{\vd}{{\mathbf{d}}}
\newcommand{\ve}{{\mathbf{e}}}
\newcommand{\vf}{{\mathbf{f}}}
\newcommand{\vg}{{\mathbf{g}}}
\newcommand{\vh}{{\mathbf{h}}}
\newcommand{\vi}{{\mathbf{i}}}
\newcommand{\vj}{{\mathbf{j}}}
\newcommand{\vk}{{\mathbf{k}}}
\newcommand{\vl}{{\mathbf{l}}}
\newcommand{\vm}{{\mathbf{m}}}
\newcommand{\vn}{{\mathbf{n}}}
\newcommand{\vo}{{\mathbf{o}}}
\newcommand{\vp}{{\mathbf{p}}}
\newcommand{\vq}{{\mathbf{q}}}
\newcommand{\vr}{{\mathbf{r}}}
\newcommand{\vs}{{\mathbf{s}}}
\newcommand{\vt}{{\mathbf{t}}}
\newcommand{\vu}{{\mathbf{u}}}
\newcommand{\vv}{{\mathbf{v}}}
\newcommand{\vw}{{\mathbf{w}}}
\newcommand{\vx}{{\mathbf{x}}}
\newcommand{\vy}{{\mathbf{y}}}
\newcommand{\vz}{{\mathbf{z}}}
\newcommand{\valpha}{{\boldsymbol{\alpha}}}
\newcommand{\vlambda}{{\boldsymbol{\lambda}}}
\newcommand{\mA}{{\mathbf{A}}}
\newcommand{\mB}{{\mathbf{B}}}
\newcommand{\mC}{{\mathbf{C}}}
\newcommand{\mD}{{\mathbf{D}}}
\newcommand{\mE}{{\mathbf{E}}}
\newcommand{\mF}{{\mathbf{F}}}
\newcommand{\mG}{{\mathbf{G}}}
\newcommand{\mH}{{\mathbf{H}}}
\newcommand{\mI}{{\mathbf{I}}}
\newcommand{\mJ}{{\mathbf{J}}}
\newcommand{\mK}{{\mathbf{K}}}
\newcommand{\mL}{{\mathbf{L}}}
\newcommand{\mM}{{\mathbf{M}}}
\newcommand{\mN}{{\mathbf{N}}}
\newcommand{\mO}{{\mathbf{O}}}
\newcommand{\mP}{{\mathbf{P}}}
\newcommand{\mQ}{{\mathbf{Q}}}
\newcommand{\mR}{{\mathbf{R}}}
\newcommand{\mS}{{\mathbf{S}}}
\newcommand{\mT}{{\mathbf{T}}}
\newcommand{\mU}{{\mathbf{U}}}
\newcommand{\mV}{{\mathbf{V}}}
\newcommand{\mW}{{\mathbf{W}}}
\newcommand{\mX}{{\mathbf{X}}}
\newcommand{\mY}{{\mathbf{Y}}}
\newcommand{\mZ}{{\mathbf{Z}}}
\newcommand{\cA}{{\mathcal{A}}}
\newcommand{\cB}{{\mathcal{B}}}
\newcommand{\cC}{{\mathcal{C}}}
\newcommand{\cD}{{\mathcal{D}}}
\newcommand{\cE}{{\mathcal{E}}}
\newcommand{\cF}{{\mathcal{F}}}
\newcommand{\cG}{{\mathcal{G}}}
\newcommand{\cH}{{\mathcal{H}}}
\newcommand{\cI}{{\mathcal{I}}}
\newcommand{\cJ}{{\mathcal{J}}}
\newcommand{\cK}{{\mathcal{K}}}
\newcommand{\cL}{{\mathcal{L}}}
\newcommand{\cM}{{\mathcal{M}}}
\newcommand{\cN}{{\mathcal{N}}}
\newcommand{\cO}{{\mathcal{O}}}
\newcommand{\cP}{{\mathcal{P}}}
\newcommand{\cQ}{{\mathcal{Q}}}
\newcommand{\cR}{{\mathcal{R}}}
\newcommand{\cS}{{\mathcal{S}}}
\newcommand{\cT}{{\mathcal{T}}}
\newcommand{\cU}{{\mathcal{U}}}
\newcommand{\cV}{{\mathcal{V}}}
\newcommand{\cW}{{\mathcal{W}}}
\newcommand{\cX}{{\mathcal{X}}}
\newcommand{\cY}{{\mathcal{Y}}}
\newcommand{\cZ}{{\mathcal{Z}}}
\DeclareMathOperator{\Imageof}{Im}
\DeclareMathOperator{\Kernelof}{Ker}
    
\begin{document}

\title{Assembly of FETI dual operator using CUDA
}

\author{\IEEEauthorblockN{Jakub Homola, Radim Vav\v{r}\'{i}k, Ond\v{r}ej Meca, Tom\'{a}\v{s} Brzobohat\'{y}, and Lubom\'{i}r \v{R}\'{i}ha}
\IEEEauthorblockA{\textit{IT4Innovations}, 
\textit{VSB – Technical University of Ostrava}
Ostrava-Poruba, Czech Republic \\
jakub.homola@vsb.cz, radim.vavrik@vsb.cz, ondrej.meca@vsb.cz, tomas.brzobohaty@vsb.cz, and lubomir.riha@vsb.cz}
}

% % puvodni:
% \author{\IEEEauthorblockN{Jakub Homola}
% \IEEEauthorblockA{\textit{IT4Innovations} \\
% \textit{VSB – Technical University of Ostrava}\\
% Ostrava-Poruba, Czech Republic \\
% jakub.homola@vsb.cz}
% \and
% \IEEEauthorblockN{Radim Vav\v{r}\'{i}k}
% \IEEEauthorblockA{\textit{IT4Innovations} \\
% \textit{VSB – Technical University of Ostrava}\\
% Ostrava-Poruba, Czech Republic \\
% radim.vavrik@vsb.cz}
% \and
% \IEEEauthorblockN{Ond\v{r}ej Meca}
% \IEEEauthorblockA{\textit{IT4Innovations} \\
% \textit{VSB – Technical University of Ostrava}\\
% Ostrava-Poruba, Czech Republic \\
% ondrej.meca@vsb.cz}
% \and
% \IEEEauthorblockN{Lubom\'{i}r \v{R}\'{i}ha}
% \IEEEauthorblockA{\textit{IT4Innovations} \\
% \textit{VSB – Technical University of Ostrava}\\
% Ostrava-Poruba, Czech Republic \\
% lubomir.riha@vsb.cz}
% \and
% \IEEEauthorblockN{Tom\'{a}\v{s} Brzobohat\'{y}}
% \IEEEauthorblockA{\textit{IT4Innovations} \\
% \textit{VSB – Technical University of Ostrava}\\
% Ostrava-Poruba, Czech Republic \\
% tomas.brzobohaty@vsb.cz}
% }

\maketitle
\thispagestyle{plain}
\pagestyle{plain}


\begin{abstract}

%%%% background - feti, dual operator
FETI is a numerical method used to solve engineering problems.
It builds on the ideas of domain decomposition, which makes it highly scalable and capable of efficiently utilizing whole supercomputers.
One of the most time-consuming parts of the FETI solver is the application of the dual operator $\mF$ in every iteration of the solver.

%%%% state of the art, gap in research - implicit, current explit needs many iterations
It is traditionally performed on the CPU using an implicit approach of applying the individual sparse matrices that form $\mF$ right-to-left.
Another approach is to apply the dual operator explicitly, which primarily involves a simple dense matrix-vector multiplication and can be efficiently performed on the GPU.
However, this requires additional preprocessing on the CPU where the dense matrix is assembled, which makes the explicit approach beneficial only after hundreds of iterations are performed.

%%%% aim, methods - use gpu also for assembly, need less iterations
In this paper, we use the GPU to accelerate the assembly process as well. This significantly shortens the preprocessing time, thus decreasing the number of solver iterations needed to make the explicit approach beneficial.

%%%% results, conclusion - amortization point decreased, speedup
With a proper configuration, we only need a few tens of iterations to achieve speedup relative to the implicit CPU approach.
Compared to the CPU-only explicit approach, we achieved up to 10$\times$ speedup for the preprocessing and 25$\times$ for the application.

\end{abstract}

\begin{IEEEkeywords}
FETI, domain decomposition, acceleration, GPU, CUDA
\end{IEEEkeywords}

\section{Introduction}

FETI methods (finite element tearing and interconnecting) are highly scalable numerical methods used to solve large-scale engineering problems. It builds on the principles of the finite element method (FEM) and domain decomposition. The spatial domain is decomposed into several smaller subdomains, which are glued together using Lagrange multipliers to enforce equality on the interfaces between them. The resulting system is usually solved using iterative methods, where in each iteration, subdomains can be solved concurrently. The convergence properties are, in a sense, optimal~\cite{FETI_OPT,FETI_CONVERGENCE}.

With the increasing power of modern high-performance clusters, many systems are based on GPUs since they offer much higher FLOP/s performance, memory bandwidth, and energy efficiency compared to CPUs.
Hence, it is tempting to utilize GPUs for FETI algorithms as well.

It is natural to try to accelerate the most time-consuming part of an algorithm. In the FETI solver, a significant portion of the runtime is taken up by the application of the dual operator $\mF = \mB \mK^{+} \mB^\top$ in every iteration of the solver.

The application of $\mF$ is traditionally performed on the CPU using the implicit approach of applying the individual sparse matrices right-to-left.
The other option is to use the explicit approach. There, the dense matrices that form $\mF$ need to be assembled first. The application is then just a simple dense matrix-vector multiplication. This approach significantly speeds up iterations of the FETI solver at the cost of expensive preprocessing, where the assembly takes place.

Because the dense matrix-vector multiplication kernel is very well suited for GPUs, previous attempts at acceleration used the explicit approach. They assembled the matrices that form $\mF$ on the CPU and copied them to the accelerator only for the application~\cite{BDDS_ACC,FETI_PHI,ESPRESO-SC}.

Some of them used a more clever algorithm for assembly -- an augmented incomplete factorization method from the PARDISO library~\cite{pardiso}. It takes advantage of the high sparsity of $\mB$ to speed up the assembly.
However, this approach is still beneficial only for problems requiring thousands of iterations of the FETI solver (e.g., ill-conditioned problems).

In this work, we also use the explicit approach. However, contrary to all of the mentioned acceleration attempts, we use the GPU to assemble the local dual operator as well. As shown, this approach is faster than the augmented incomplete factorization on the CPU for most non-trivial 3D problems.

There are multiple ways in which the explicit local dual operator can be assembled. We can choose between cuBLAS and cuSPARSE kernels; we can call each of them with different parameters (e.g., use the transpose flag or transpose the matrix manually) or even use a slightly different algorithm. The best parameter setting depends on the type of solved problem (2D or 3D), the size of the subdomain, and, surprisingly, the version of CUDA libraries. In this paper, we provide a summary of the optimal parameter settings, which is based on an exhaustive search of the parameter space.

With the appropriate settings, we achieved up to $10\times$ speedup of the explicit assembly
and $25\times$ speedup of the application,
both relative to the Intel MKL PARDISO CPU-only explicit approach (Karolina GPU node -- 8x Nvidia A100 GPUs, 2x 64-core AMD EPYC 7763 CPUs).
With this, we reduce the number of iterations required for the GPU approach to become beneficial (the amortization point) from hundreds to tens.
For problems requiring hundreds of iterations, we observed a speed-up of around 10 (relative to the traditional CPU implicit approach) in the dual operator-related parts of the FETI solver.

The paper is structured as follows.
Section~\ref{sec: theory} presents the basic theory of FETI algorithms focusing on the FETI dual operator.
Section~\ref{sec:implementation} describes key aspects of the implementation and efficient parallelization of FETI methods needed to fully understand the acceleration.
Section~\ref{sec:acceleration} describes the acceleration of explicit evaluation of $\mF$.
Section~\ref{sec:performance} compares different approaches and summarizes achieved results. 






































\section{Finite Element Tearing and Interconnecting}
\label{sec: theory}

% scalar variables:
%   N = number of subdomains

Let us first quickly introduce the FETI method~\cite{FETI}. It is a modification of the finite element method (FEM)~\cite{fem}, which is a numerical method for solving differential equations. FETI uses domain decomposition techniques to allow solving of FEM on large-scale parallel systems, which is its main advantage over FEM. 
% We assume readers are already familiar with FETI; for completeness, we include a brief overview.
For readers unfamiliar with FETI, we provide a short description.

We start with the FEM system
\begin{equation}
    \label{eq:femsystem}
    \tilde{\mK} \tilde{\vu} = \tilde{\vf},
\end{equation}
where $\tilde{\mK}$ is the stiffness matrix, $\tilde{\vf}$ is the load vector, and $\tilde{\vu}$ is the solution.

By rearranging the order of DOFs and adding several carefully crafted equations and variables, we are able to transform the FEM system into an equivalent system with a block structure,
\begin{equation}
    \label{eq:fetisystemblocked}
    \begin{bmatrix}
        \mK_1 & & & & \mB_1^\top \\
        & \mK_2 & & & \mB_2^\top \\
        & & \ddots & & \vdots \\
        & & & \mK_N & \mB_N^\top \\
        \mB_1 & \mB_2 & \dots & \mB_N & \mO
    \end{bmatrix}
    \begin{bmatrix}
        \vu_1 \\
        \vu_2 \\
        \vdots \\
        \vu_N \\
        \vlambda
    \end{bmatrix}
    =
    \begin{bmatrix}
        \vf_1 \\
        \vf_2 \\
        \vdots \\
        \vf_N \\
        \vc
    \end{bmatrix}
\end{equation}
or, shortly,
\begin{equation}
    \label{eq:fetisystem}
    \begin{bmatrix}
        \mK & \mB^\top \\
        \mB & \mO
    \end{bmatrix}
    \begin{bmatrix}
        \vu \\
        \vlambda
    \end{bmatrix}
    =
    \begin{bmatrix}
        \vf \\
        \vc
    \end{bmatrix}
\end{equation}
where each block represents one of the $N$ subdomains into which we decomposed the spatial domain.
$\mK$ is the block diagonal stiffness matrix comprising of the subdomain stiffness matrices $\mK_i$, $\vlambda$ is the Lagrange multiplier vector (we call all vectors with size equal to $\vlambda$ \textit{dual vectors}), and the matrix $\mB$ represents the subdomain gluing.
We use the Total FETI variant~\cite{totalfeti}, where we isolate the Dirichlet boundary conditions and append them to the gluing matrix $\mB$ and the right-hand side $\vc$. This makes all the subdomain stiffness matrices singular.
Such a rearrangement and expansion of the system makes the matrix larger, but it makes the parallelization of the solution more straightforward.

We denote $\mR_i$ to be the matrix containing the basis vectors of $\Kernelof \mK_i$ in its columns, and we create $\mR$ by placing $\mR_i$ on the main block diagonal.
From the solvability of the first equation in \eqref{eq:fetisystem}, we eventually get
\begin{equation}
    \label{eq:solvability}
    - \mR^\top \mB^\top \vlambda = - \mR^\top \vf.
\end{equation}
Given the Lagrange multipliers $\vlambda$, we can express $\vu$ using
\begin{equation}
    \label{eq:solutionUeval}
    \vu = \mK^+ (\vf - \mB^\top \vlambda) + \mR \valpha,
\end{equation}
where $\mK^+$ is a generalized inverse of $\mK$ satisfying $\mK \mK^+ \mK = \mK$.
Using \eqref{eq:solutionUeval} in the second equation in \eqref{eq:fetisystem}, we eventually get
\begin{equation}
    \label{eq:alphaconstraint}
    \mB \mK^+ \mB^\top \vlambda - \mB \mR \valpha = \mB \mK^+ \vf - \vc.
\end{equation}
We define $\mF = \mB \mK^+ \mB^\top$ (the \textit{dual operator}), $\mG = \mB \mR$, $\vd = \mB \mK^+ \vf - \vc$, $\ve = \mR^\top \vf$ and combine \eqref{eq:solvability} with \eqref{eq:alphaconstraint}, and get
\begin{equation}
    \label{eq:tfetidualproblem}
    \begin{bmatrix}
        \mF & -\mG \\
        -\mG^\top & \mO
    \end{bmatrix}
    \begin{bmatrix}
        \vlambda \\
        \valpha
    \end{bmatrix}
    =
    \begin{bmatrix}
        \vd \\
        -\ve
    \end{bmatrix}.
\end{equation}

The system \eqref{eq:tfetidualproblem} is solved using the preconditioned conjugate projected gradient method (PCPG), using the projector
\begin{equation}
    \mP = \mI - \mG (\mG^\top \mG)^{-1} \mG^\top.
\end{equation}
and a preconditioner $\mM$. We use Algorithm~\ref{alg:fetipcpg} to find $\vlambda$, $\valpha$ can be then computed using
\begin{equation}
\label{eq:alphacompute}
    \valpha = - (\mG^\top \mG)^{-1} \mG^\top (\vd - \mF \vlambda).
\end{equation}

\begin{algorithm}
\caption{The preconditioned conjugate projected gradient method}
\label{alg:fetipcpg}
\DontPrintSemicolon
$\vlambda_0 \gets \vlambda_I$ \;
$\vr_0 \gets \vd - \mF \vlambda_I$ \;
$\vw_0 \gets \mP \vr_0$ \;
$\vy_0 \gets \mP \mM \vw_0$ \;
$\vp_0 \gets \vy_0$ \;
\For{$k = 0, 1, 2, \dots$ until convergence}{
    $\vq_k \gets \mF \vp_k$ \;
    $\delta_k \gets (\vw_k^\top \vy_k) / (\vp_k^\top \vq_k)$ \;
    $\vlambda_{k+1} \gets \vlambda_k + \delta_k \vp_k$ \;
    $\vr_{k+1} \gets \vr_k - \delta_k \vq_k$ \;
    $\vw_{k+1} \gets \mP \vr_{k+1}$ \;
    $\vy_{k+1} \gets \mP \mM \vw_{k+1}$ \;
    $\beta_k \gets (\vw_{k+1}^\top \vy_{k+1}) / (\vw_k^\top \vy_k)$ \;
    $\vp_{k+1} \gets \vy_{k+1} + \beta_k \vp_k$ \;
}
\end{algorithm}



\subsection{Dual operator $\mF$}
\label{sec:dualop}

The matrix $\mF$, defined previously as
\begin{equation}
    \mF = \mB \mK^+ \mB^\top,
\end{equation}
is sometimes called the dual operator, dual Schur complement matrix, or the FETI operator. 
It is square, and each row and column corresponds to a single Lagrange multiplier (gluing connection between subdomains).
Due to the block structure of the matrices, we are able to define a \textit{local dual operator} for each subdomain as
\begin{equation}
    \label{eq:localdualoperator}
    \tilde{\mF}_i = \tilde{\mB}_i \mK_i^+ \tilde{\mB}_i^\top.
\end{equation}
It is a (possibly non-contiguous) submatrix of $\mF$ defined by only the Lagrange multipliers connected to the $i$-th subdomain. The individual $\tilde{\mF}_i$ can be combined additively to form $\mF$.

The application of the dual operator $\mF$, therefore, primarily consists of the (possibly concurrent) application of the local dual operator $\tilde{\mF}_i$ for each subdomain.































\section{Implementation of FETI solver}
\label{sec:implementation}

This section describes the fundamental aspects of the FETI solver w.r.t. the dual operator and its main constraints for porting it to the GPU accelerators. 
The fundamental principle of FETI algorithms is to divide the workload and distribute it among available hardware resources.
With current architectures, a good distribution usually takes into account both shared and distributed memory.

\begin{figure}
 \centering
 \includegraphics[width=\columnwidth]{img/domains.png}
 \caption{Example of domain decomposition and clusterization of subdomains to clusters. Clusterization allows a hybrid parallelization for both shared and distributed memory systems.}
 \label{fig:domains}
\end{figure}

One of the possible decompositions of a domain into subdomains is shown in Fig.~\ref{fig:domains}.
The figure shows several subdomains and their gluings to neighboring subdomains (the Lagrange multipliers), which are drawn with dashed lines. 
Several subdomains are grouped into a \textit{cluster} of subdomains (in the figure, subdomains D2 and D3 form the cluster C1, as highlighted by the dotted line).
Each process (distributed memory) then handles a single cluster.
In each cluster, subdomains are handled by threads (sharing the memory).
Typically, the number of subdomains per cluster is an integer multiple of the number of threads in order to utilize the available cores equally and achieve optimal performance of the FETI solver~\cite{espreso-pasc}.
Traditionally, each subdomain stores only the Lagrange multipliers that are connected to it.
With the introduction of clusters, Lagrange multipliers are stored per cluster.

The generalized inverse $\mK_i^+$ in \eqref{eq:localdualoperator} can be expressed as $\mK_i^+ = \mK_{i,reg}^{-1} = (\mL_i \mU_i)^{-1} = \mU_i^{-1} \mL_i^{-1}$, where $\mK_{i,reg}$ is the regularized subdomain stiffness matrix that can be obtained, e.g., through analytic regularization~\cite{brzyngeninverse}.
The local dual operator can, therefore, be written as
\begin{equation}
    \label{eq:localdualoperatorwithU}
    \tilde{\mF}_i = \tilde{\mB}_i \mU_i^{-1} \mL_i^{-1} \tilde{\mB}_i^\top.
\end{equation}

It can be applied implicitly, where individual matrices are applied from right to left,
\begin{equation}
\label{eq:dualop_apply_impl}
    \tilde{\vq}_i
    =
    \tilde{\mF}_i \tilde{\vp}_i
    =
    \tilde{\mB}_i (\mU_i^{-1} (\mL_i^{-1} (\tilde{\mB}_i^\top \tilde{\vp}_i))),
\end{equation}
or explicitly, where all matrices are multiplied at first and then applied at once,
\begin{equation}
\label{eq:dualop_apply_expl}
    \tilde{\vq}_i
    =
    \tilde{\mF}_i \tilde{\vp}_i
    =
    (\tilde{\mB}_i \mU_i^{-1} \mL_i^{-1} \tilde{\mB}_i^\top) \tilde{\vp}_i.
\end{equation}

The implicit application consists of performing a sparse matrix-vector multiplication (SPMV), 
followed by two triangular solves (TRSV) and another sparse matrix-vector multiplication.
The explicit application is a fast dense matrix-vector multiplication (GEMV), but the dense matrix $\tilde{\mF}_i$ needs to be assembled first, which can be done, e.g., with two triangular solves with a right-hand-side matrix (TRSM) and a single sparse-dense matrix multiplication (SPMM).
Comparing both approaches, \textbf{applying $\tilde{\mF}_i$ explicitly is typically faster, but the explicit assembly is a highly expensive operation, which makes the explicit approach beneficial only after a sufficient number of iterations is performed}. The amortization point (the number of iterations after which the explicit approach is faster) depends on the time spent in the explicit assembly and the time saved by the faster application.

The explicit assembly can also be done using a more sophisticated algorithm. The matrix $\tilde{\mF}_i$ can be expressed as the negative of a Schur complement of the matrix
\begin{equation}
\label{eq:matrix_for_sc}
    \begin{bmatrix}
        \mK_{i,reg} & \tilde{\mB}_i^\top \\
        \tilde{\mB}_i & \mO
    \end{bmatrix}.
\end{equation}
With this, we can use an augmented incomplete factorization designed and optimized for evaluation of the Schur complement of sparse matrices~\cite{pardiso}.

In both implicit and explicit approaches, a factorization $\mK_{i,reg} = \mL_i \mU_i$ must be performed. There are many libraries available for this task~\cite{ssolvers}.
To handle a sparsity pattern efficiently, they typically divide the factorization of the matrix into two stages~\cite{Csparse} -- symbolic and numerical. During the symbolic factorization, they search for a permutation of the matrix such that the resulting factors $\mL_i$ and $\mU_i$ have the number of non-zeros as low as possible (minimizing fill-in), and they also create the non-zero pattern of the factor. In the numerical factorization, the factor is computed and filled with values.

\begin{algorithm}
\caption{Skeleton of a multi-step simulation w.r.t. dual operators $\tilde{\mF}_i$.}
\label{alg:solverdualopview}
\DontPrintSemicolon
preparation (symbolic fact., memory buffers,...) \;
\For{every step}{
    FETI preprocessing (num. fact., assembling $\tilde{\mF}_i)$ \;
    \For{each PCPG iteration} {
        scatter Lagrange multipliers \;
        Apply $\tilde{\vq}_i = \tilde{\mF}_i \tilde{\vp}_i$ for each subdomain \;
        gather Lagrange multipliers \;
    }
}
\end{algorithm}

Algorithm~\ref{alg:solverdualopview} sketches a multi-step simulation from the dual operator point of view. It shows when the above-described routines are called.

In our use case, the structure of the finite element mesh does not change between time steps, so the non-zero pattern of the matrices $\mK_i$ also stays constant. It is, therefore, enough to call symbolic factorization only once at the beginning, in the preparation phase.

However, the non-zero values of $\mK_i$ do change between time steps, the FETI preprocessing must, therefore, be repeated. In the case of the explicit approach, this also includes reassembling the local dual operator matrices $\tilde{\mF}_i$.

After FETI preprocessing, the dual operator is ready for application in the PCPG iterations. There, the cluster-wide dual vector first needs to be \textit{scattered} into the individual subdomain-wide dual vectors $\tilde{\vp}_i$. After that, the local dual operators $\tilde{\mF}_i$ are applied, and the subdomain results $\tilde{\vq}_i$ are \textit{gathered} back into the cluster-wide dual vector. Finally, the processes synchronize their dual vectors to incorporate the changes from the neighboring clusters.



































\section{The local dual operator on GPU}
\label{sec:acceleration}

This section describes the explicit assembly of the local dual operator using GPU accelerators.
We restrict ourselves to an approach based on routines provided by accelerated versions of standard high-performance mathematical libraries, such as cuBLAS or cuSPARSE~\cite{cudalibraries}, and avoid any higher-level GPU libraries.
Hence, our approach can be easily ported to a different architecture.

The local dual operator $\tilde{\mF}_i$ is assembled from the matrix $\tilde{\mB}_{i}$ and factors of $\mK_i$ using triangular solve and matrix multiplication routines.
Despite the apparent simplicity, an inappropriate algorithm can significantly degrade computational time or have unnecessarily high memory demands. The next sections describe both aspects in detail.

We use a one-to-one mapping between GPUs and clusters. This simplifies the implementation, as each process controls only a single GPU that is not shared with other processes. We will now focus on a single process, all other processes behave equally.

\subsection{Management of GPU memory}

In general, the maximum size of a problem solved by FETI on a single CPU (or GPU) is limited by the available memory.
To solve as large problems as possible, one must approach memory management wisely, especially in the case of a GPU, which typically has lower memory capacity than a CPU.
GPU memory allocations should be avoided in the hot loop because they typically cause synchronization delay and add memory management overhead.

We mentally split the GPU memory into two parts -- persistent and temporary. All persistent memory is allocated in the preparation phase and deallocated at the end of the program.
It holds mainly the structures that are not too large to fill up the memory and their size or content does not change between steps.
This includes the factors
$\mL_i$ and $\mU_i$, matrices $\tilde{\mB}_i$ and $\tilde{\mF}_i$ and the dual vectors. Some GPU libraries also require workspace buffers that need to be allocated as long as they are used, i.e., for the lifetime of the solver instance.

The rest of the memory is allocated for use in our allocator, which manages the temporary memory. In our usage, the phrase \textit{temporary memory} refers to memory buffers that are needed only for the duration of a specific kernel. These buffers are typically large, and if they were allocated persistently, the GPU memory capacity would not suffice. The temporary memory allocator is able to reuse memory without a need to call the GPU library's memory allocation routines. If there is enough remaining memory in the allocator's memory pool, memory is assigned and returned right away. Otherwise, the allocating thread is blocked until enough memory becomes available -- until other threads deallocate a sufficient amount of memory.

\subsection{Execution of the explicit dual operator stages}

As we showed in Algorithm~\ref{alg:solverdualopview}, there are three points in the program where the dual operator structures are used -- preparation, preprocessing, and application. We already briefly described them. Now, we will go into more detail.

\paragraph{Preparation} The main part of preparation is a parallel loop iterating over all subdomains in the cluster. For each subdomain, we perform the symbolic factorization, allocate the persistent structures in GPU memory, copy constant data (e.g. $\tilde{\mB}_i$ and the non-zero pattern of the factors), and call the analysis phase of cuSPARSE kernels. After the loop, we allocate the remaining memory for the temporary memory allocator.

\paragraph{Preprocessing} The FETI preprocessing also contains a parallel loop, where for each subdomain, we first perform the numerical factorization and copy the factors' non-zero values to the GPU. Then, we allocate the workspace buffers and all temporary matrices in the temporary memory allocator. Next, we convert the gluing matrices $\tilde{\mB}_i$ to dense on the GPU and submit the TRSM and matrix multiplication kernels that perform the actual assembly of the local dual operator. Finally, the temporarily allocated matrices are deallocated, which concludes the parallel loop. Because all the operations submitted to the GPU are asynchronous, we need to perform a synchronization to wait for all the GPU work to finish.

The asynchronicity of the GPU operations is important. While the GPU is executing kernels corresponding to one of the previous subdomains, we can perform numerical factorization and submit kernels corresponding to another subdomain in the following iteration of the subdomain loop. After the first iteration, we, therefore, achieve CPU-GPU computation overlap. Furthermore, since we use multiple CUDA streams, kernels and memory transfers coming from different subdomains can run concurrently, thus increasing GPU occupancy and allowing copy-compute overlap.

\paragraph{Application} When we need to apply the dual operator, we first scatter the cluster-wide dual vector to the subdomains. Then, we submit all the GEMV or SYMV\footnote{If $\tilde{\mF}_i$ is symmetric, we only store its upper or lower triangle. We share memory such that two opposite triangles are stored in a single allocation.} kernels to perform the matrix-vector multiplications $\tilde{\vq}_i = \tilde{\mF}_i \tilde{\vp}_i$ for each subdomain in the cluster. After this, we gather the results back into the cluster-wide dual vector. Because all the operations are again asynchronous, the function ends with a synchronization that waits for all the GPU operations to finish.

The scatter and gather can be performed on the CPU or on the GPU, the required dual vectors are copied to the GPU at the appropriate time. The behavior is controlled by a parameter, see Section~\ref{sec:settings_description} for more details.

\subsection{Parameters of the explicit assembly process}
\label{sec:settings_description}

The most time-consuming kernel during the explicit assembly of $\tilde{\mF}_i$ is TRSM.
This kernel is usually provided by both sparse and dense BLAS libraries, i.e., one can call it with factors in sparse or dense format.
Both versions also allow us to provide the matrices (factor and right-hand side) in row-major or column-major order (indirectly through other parameters).
These options have an impact on the performance and memory demands of the TRSM kernel and the whole assembly process.
Moreover, in the general form, TRSM is called twice (forward and backward solve). For SPD systems with $\mL \mL^\top$ or $\mU^\top \mU$ factorization, it is possible to avoid the second TRSM using the SYRK function (as described below). 

\begin{table}
    \centering
    \caption{Overview of parameters for the explicit assembly of $\tilde{\mF}_i$ using GPUs.}
    \label{tab:settings_overview}
    \begin{tabular}{ll}
        \toprule
        Setting & Options \\
        \midrule
        Path & TRSM, SYRK \\
        Forward solve factor storage & sparse, dense \\
        Backward solve factor storage & sparse, dense \\
        \midrule
        Forward solve factor order & row-major, column-major \\
        Backward solve factor order & row-major, column-major \\
        RHS memory order & row-major, column-major \\
        \midrule
        Scatter and gather & CPU, GPU \\
        \bottomrule
    \end{tabular}
\end{table}

All the parameters are summarized in Table~\ref{tab:settings_overview}. We follow with a more detailed explanation of their meaning.

\textbf{Path} determines the matrix operations that are performed to assemble $\tilde{\mF}_i$ in case of SPD systems. There are two possibilities named after the second invoked kernel:
\begin{itemize}
\item TRSM: $\tilde{\mF}_i = \tilde{\mB}_i (\mU_i^{-1} (\mU_i^{-\top} \tilde{\mB}_i^\top))$.
\item SYRK: $\tilde{\mF}_i = (\mU_i^{-\top} \tilde{\mB}_i^\top)^\top (\mU_i^{-\top} \tilde{\mB}_i^\top)$.
\end{itemize}
For the TRSM version, two triangular solves are followed by a sparse-dense matrix-matrix multiplication. In SYRK, only the first triangular solve is performed, after which we do a dense matrix-matrix multiplication -- SYRK.
Note that the performance difference is practically only in replacing the TRSM kernel with SYRK, avoiding the SPMM has a negligible effect.
In the case of dense factor storage, SYRK is theoretically faster -- both kernels are level 3 BLAS, but we call SYRK with smaller matrices. With sparse factors, the theoretically better option depends on the sparsity of the factors, and we leave the decision up to experiments.

\textbf{Factor storage} denotes if we use the sparse TRSM from cuSPARSE or the dense TRSM from cuBLAS.
Due to the higher density of the factors caused by fill-in, using the dense TRSM might be faster, even though it cannot utilize the sparsity.
The sparse-to-dense conversion is performed on the GPU to minimize the amount of data transferred.

\textbf{Factor order} denotes the memory order of the factor passed to the TRSM kernel. For sparse TRSM, this parameter controls whether CSR or CSC format is used for the factor. The GPU library functions typically assume column-major order of dense matrices or the CSR format for sparse, but we can tweak other parameters to indirectly enable this option. If necessary, we convert the matrix order (which is equivalent to transposing the matrix) on the CPU.

\textbf{RHS memory order} specifies the memory order of dense RHS (right-hand side) and solution matrices passed to/from the TRSM routine.

\textbf{Scatter and gather} can be performed either on the CPU or on the GPU. If the CPU is used, then the subdomain-wide dual vectors are individually copied to and from the GPU right before and after the GEMV or SYMV kernel is submitted. If we use the GPU, we copy the whole cluster-wide dual vector to the GPU, submit a single kernel that performs its scattering, and submit all the GEMV/SYMV kernels. Finally, we perform the gather and copy the output dual vector back to the CPU. The difference between the options is that the CPU version submits more GPU operations, which causes more overheads, but allows for more concurrency and copy-compute overlap.









































\section{Results}
\label{sec:performance}

In this section, the performance of the accelerated FETI solver is presented.
The comparison is divided into three sections: A) choosing the optimal parameters for the explicit assembly of $\tilde{\mF}_i$ and its application, B) comparison of different versions of the dual operator, and C) the overall performance of the FETI solver.

The measurements were performed for problems of heat transfer and linear elasticity, both in 2D and 3D, and with varying sizes of subdomains (denoted as degrees of freedom, DOFs, in the graphs). With decreasing the subdomain size, we increase their count (up to 2000) to have the total number of DOFs approximately constant, mainly to keep the runtime at a reasonable scale\footnote{Note that all GPU operations bring some overhead, and for small subdomains, where the operations themselves are quick, the overheads can dominate the runtime.}. That was around 8.4M DOFs for 2D and 1.1M DOFs for 3D problems.
We used a square or cube domain discretized into a mesh composed of triangles or tetrahedral elements.

The performance was tested on the GPU partition of the Karolina cluster at IT4Innovations~\cite{karolinadocs}.
The partition is equipped with two 64-core AMD EPYC 7763 processors with 1 TB of DDR4 memory and has 8 NVIDIA A100 GPUs with 40 GB of HBM2 memory.
For most of the experiments, we use only a single GPU and a proportional fraction of CPU cores and memory, that is, 16 cores and 128 GB of memory (which corresponds to a single NUMA domain).
This configuration reflects production runs with fully utilized nodes since using a single MPI process per NUMA domain with a single GPU usually delivers optimal performance.
GPU kernels were submitted using 16 CUDA streams, i.e., one stream per OpenMP thread.
The source code and experiment scripts are a part of the ESPRESO library and can be found in~\cite{espresogithub}. All the measured data are available at~\cite{experimentdatasetfigshare}.

We tested two versions of CUDA libraries, \textit{legacy} and \textit{modern}. The legacy stands for CUDA toolkit 11.7, where the legacy cuSPARSE API was used. The modern stands for CUDA toolkit 12.4, where we use the generic cuSPARSE API functions.
On the CPU, we used two sparse linear solver libraries -- CHOLMOD~\cite{cholmod} from SuiteSparse 7.6.0 and PARDISO from Intel MKL 2024.2.0~\cite{intelmkl}. Both libraries used Metis to reduce fill-in.
MKL PARDISO provides the augmented incomplete factorization algorithm for the assembly of explicit $\tilde{\mF}_i$ via Schur complement on the CPU. It does not allow the extraction of the factors $\mL_i$ and $\mU_i$, so it cannot be used for GPU acceleration of the explicit assembly.
CHOLMOD provides functions for the extraction of factors from the solver.

\subsection{Optimal parameters of the assembly}

As we discussed in Section~\ref{sec:settings_description}, the explicit assembly function has several parameters that influence both runtime and memory requirements. Now, we show the optimal values of those parameters according to our experiments.
To account for potential dependencies, we tested all possible combinations of the parameters.
Listing and reasoning through all of the results would not fit in this paper; therefore, in this text, we provide only a representative example for each parameter.

\begin{table}
    \centering
    \caption{Optimal parameters for the explicit assembly of the local dual operator.}
    \label{tab:optimal_settings}
    \begin{tabular}{lll}
    \toprule
    CUDA library                          & legacy (v11.7)                 & modern (v12.4)               \\
    \midrule
    path                                  & SYRK                           & SYRK                         \\
    \midrule
    \multirow{3}{2.5cm}{factor storage}   & 2D: sparse                     & \multirow{3}{2cm}{dense}     \\
                                          & 3D: $<$ 12k DOFs: dense        &                              \\
                                          & 3D: $>$ 12k DOFs: sparse       &                              \\
    \midrule
    \multirow{2}{2.5cm}{factor order}     & sparse: row-major              & \multirow{2}{2cm}{col-major} \\
                                          & dense: col-major               &                              \\
    \midrule
    \multirow{2}{2.5cm}{RHS memory order} & \multirow{2}{2.5cm}{row-major} & 2D: col-major                \\
                                          &                                & 3D: row-major                \\
    \bottomrule
    \end{tabular}
\end{table}

An overview of the optimal parameter settings can be found in Table~\ref{tab:optimal_settings}.
The configuration for modern CUDA is straightforward; for legacy CUDA, the settings are more problem-dependent, but the explicit assembly process is faster there. In our implementation, we have an option to auto-configure these parameters based on the problem that is being solved. In the following paragraphs, we will describe the reasoning behind the optimal parameter settings in more detail.

\begin{figure}
 \centering
 \input{tikz/settings/path_2.tex}
 % Karolina cuda modern i legacy. U trsm jsem nechal bud obe sparse nebo obe dense. Pro trsm je tam U i UHH. Jenom triangle3 a tetra4.
 \caption{Speedup of SYRK path compared to TRSM in the explicit assembly of $\tilde{\mF}_i$.}
 \label{fig:settings_path}
\end{figure}

\paragraph{Path} For the path parameter, it was faster to use SYRK for the majority of the problems.
Fig.~\ref{fig:settings_path} shows the FETI preprocessing speedup of the SYRK path compared to TRSM for all tested configurations using both CUDA versions.
The average speedup was 1.58. TRSM was better only in 16 cases for very small subdomains.
In some cases, TRSM failed because of excessive memory consumption or because it exceeded our time limit.
This experiment confirms the better performance of SYRK for dense factors.
In the case of sparse factors, the SYRK is still better due to the relatively high fill-in of the factors.

\begin{figure}
 \centering
 \input{tikz/settings/factor_storage.tex}
 % karolina cuda legacy heat transfer 3D tetra10 parallel herk colmajor LHH
 \caption{Comparison of factor storage in explicit assembly of $\tilde{\mF}_i$.
 Settings: heat transfer 3D, quadratic tetrahedra, SYRK path.}
 \label{fig:settings_factor_storage}
\end{figure}

\paragraph{Factor storage} Fig.~\ref{fig:settings_factor_storage} shows the comparison of factor storage for both versions of CUDA for a 3D heat transfer problem.
The optimal setting is highly dependent on the problem dimensionality, subdomain size, and even the used version of the cuSPARSE API; however, the behavior between forward and backward TRSM is almost identical.

The modern CUDA with generic cuSPARSE API has a very underperforming sparse TRSM kernel, as one can observe in the graph. The kernel also requires very large persistently allocated memory buffers, which very significantly limits the maximum problem size we are able to handle. It is, therefore, always better to use dense TRSM across all other parameters, problem types, and subdomain sizes with modern CUDA. 

The TRSM from legacy cuSPARSE performs much better, mainly due to the used block algorithm.
For 2D meshes, the factors are very sparsely populated, and it is better to use them as sparse.
As can be observed in the graph, for 3D meshes where the factors are denser, it is unclear which is the better option.
For large subdomains that contain more than 12000 DOFs, the sparsity is still high enough to make sparse storage better.
However, for subdomains smaller than that, the factors are denser, and the choice is challenging.
We decided to use dense storage there because it was the better option for 51/64 measurements of medium-sized subdomains (1000-12000 DOFs).
However, for a production run, we recommend performing a few benchmarks to select the right factor storage for the specific problem that is being solved.

\paragraph{Factor order} This parameter influences the runtime of the assembly process only very little (hence, we do not provide any graph). The main difference is in the size of the workspace buffers in the sparse TRSM from legacy CUDA. If a column-major (CSC) factor is used, the function requires additional memory (both persistent and temporary) with the size around the size of the factor, and it is, therefore, better to use row-major order there. In modern CUDA, the workspace buffer size is not affected by any parameters. For dense matrices, we use column-major order.

\paragraph{RHS memory order} For dense factor storage and 2D subdomains, where the number of columns in the RHS is proportionally small, column-major order was slightly faster. For dense storage and 3D subdomains, where the right-hand side is proportionally wider, row-major order was better.

For sparse factor storage, workspace buffer sizes again play a major role. With column-major right-hand side, the sparse TRSM kernel in legacy CUDA required an additional temporary workspace memory equal to the size of the right-hand side matrix, most probably for its transpose (row-major copy). Therefore, with sparse factors, we use row-major right-hand side and solution.

\begin{figure}
 \centering
 \input{tikz/settings/scatter_gather.tex}
 % karolina cuda legacy heat transfer 3d tetra10 parallel
 \caption{Comparison of performing scatter and gather on CPU or GPU. Settings: heat transfer 3D, quadratic tetrahedra.}
 \label{fig:settings_scatter_gather}
\end{figure}

\paragraph{Scatter and gather} Fig.~\ref{fig:settings_scatter_gather} shows the per-subdomain application time when the scatter and gather operations are performed on the CPU and on the GPU.
For small subdomains, using the CPU is slower because of the overhead of submitting more GPU operations.
With increasing the subdomain size, the overheads become less dominant, and for very large subdomains, using the CPU is slightly faster because it allows for more concurrency. However, we will use the GPU because it is better for a wider range of subdomain sizes, and the difference for the large subdomains was only 3 \% on average.
% ten max rozdil kde cpu je lepsi byl 43 %, ale to byla chyba mereni
% pak to je 10 %, 9 %, 8 %, 6 %, pak 6 pripadu mezi 5-2 %, a pak 19 pripadu pod 2 %







\subsection{Comparison of preprocessing and application performance}

This section compares all available approaches for assembling and applying the dual operator.
All approaches are summarized in Table~\ref{tab:dual_approaches}.
Apart from the explicit GPU approach, we also implemented and tested the implicit GPU approach, explicit and implicit CPU approaches, and a hybrid approach equivalent to the original acceleration attempts, as seen in~\cite{BDDS_ACC,ESPRESO-SC}.
For the implicit approaches, only the factorization of matrices $\mK_i$ was performed during the FETI preprocessing. In the implicit GPU approach, the factors were also copied to GPU.
In the case of explicit approaches, after factorization of matrices $\mK_i$, $\tilde{\mF}_i$ were explicitly assembled on an architecture defined in the description.

\begin{table}
    \centering
    \caption{A description of all tested approaches for the dual operator.}
    \label{tab:dual_approaches}
    \begin{tabular}{ll}
    \toprule
    approach      & description           \\
    \midrule
    impl\_mkl     & the MKL PARDISO solver on CPU \\
    impl\_cholmod & the CHOLMOD solver on CPU\\
    impl\_legacy  & CUDA legacy with factors from CHOLMOD \\
    impl\_modern  & CUDA modern with factors from CHOLMOD \\
    \midrule
    expl\_mkl     & aug. incomplete fact. from MKL PARDISO on CPU \\
    expl\_cholmod & TRSM with the CHOLMOD solver on CPU \\
    expl\_legacy  & CUDA legacy with factors from CHOLMOD \\
    expl\_modern  & CUDA modern with factors from CHOLMOD \\
    \midrule
    expl\_hybrid  & assembly expl\_mkl, application CUDA modern \\
    \bottomrule
    \end{tabular}
\end{table}

\begin{figure*}
    \begin{subfigure}[b]{0.47\textwidth}
        \centering
        \input{tikz/time_karolina/heat_transfer_2d-TRIANGLE3-update.tex}
        \caption{Heat transfer 2D, preprocessing}
        \label{fig:time_karolina_2d_update}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.47\textwidth}
        \centering
        \input{tikz/time_karolina/heat_transfer_2d-TRIANGLE3-apply.tex}
        \caption{Heat transfer 2D, application}
        \label{fig:time_karolina_2d_apply}
    \end{subfigure}

    \vspace{0.5cm}
    
    \begin{subfigure}[b]{0.47\textwidth}
        \centering
        \input{tikz/time_karolina/heat_transfer_3d-TETRA4-update.tex}
        \caption{Heat transfer 3D, preprocessing}
        \label{fig:time_karolina_3d_update}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.47\textwidth}
        \centering
        \input{tikz/time_karolina/heat_transfer_3d-TETRA4-apply.tex}
        \caption{Heat transfer 3D, application}
        \label{fig:time_karolina_3d_apply}
    \end{subfigure}
    \caption{Execution time of preprocessing and application}
    \label{fig:time_karolina}
\end{figure*}

The comparison for heat transfer is shown in Fig.~\ref{fig:time_karolina}; for linear elasticity problems, the results are similar, thus we do not present them here.
In the top two graphs, 2D problems are measured, while in the bottom two, there are 3D problems. On the left we plotted the time of preprocessing, on the right you can see the time of application.
The explicit assembly parameters were set according to the recommendation in Table~\ref{tab:optimal_settings}.

% fastest preprocessing in general
In general, the fastest preprocessing of the dual operator provides PARDISO from the Intel MKL library using the implicit approach. Comparing it with CHOLMOD from the SuiteSparse library, MKL PARDISO is able to factorize subdomains from 2D and small 3D meshes about 2 times faster. For large 3D subdomains, differences are only marginal.

% comparing implicit cpu and gpu
The preprocessing in the implicit GPU approach is only slightly slower than in implicit CHOLMOD.
The difference is caused only by extracting and copying the factors.
If MKL PARDISO allowed the extraction of factors, the implicit GPU preprocessing would copy the trend of the implicit MKL PARDISO preprocessing.

% explicit preprocessing slower than implicit, except for 2d mkl pardiso
As the explicit preprocessing assembles the matrices $\tilde{\mF}_i$, it is naturally slower than implicit preprocessing.
A surprising exception is the explicit MKL PARDISO approach for 2D subdomains, which is faster than the implicit CHOLMOD approach.
This is caused by the very fast factorization for the 2D problems by MKL PARDISO and the efficient utilization of a sparsity pattern by the augmented incomplete factorization.

% explicit preprocessing GPU
The assembly of the explicit $\tilde{\mF}_i$ by CUDA is very fast for moderately-sized 2D subdomains, where it takes only slightly longer than CHOLMOD preprocessing. In 3D, the time increases at most 4 times for up to 5000 DOFs.
In the case of small subdomains, the overhead of CUDA kernels is high.
For large subdomains, the slower assembly is caused by the increasing number of Lagrange multipliers in $\tilde{\mB}_i$.
The difference between legacy and modern CUDA is caused by the low performance of the sparse TRSM in modern CUDA.

% explicit 3D
The situation is different for subdomains from the 3D mesh. Factors are much denser, and the difference between factorization with MKL PARDISO and CHOLMOD is small.
Explicit assembly using CUDA is faster than the explicit approach with MKL PARDISO. Due to the higher density of factors and the use of dense TRSM, there is minimal difference between CUDA modern and CUDA legacy for subdomains below $12000 \approx 2^{13.5}$ DOFs.
With the increasing subdomain sizes (and increasing sparsity), the advantage of CUDA over the incomplete augmented factorization from MKL PARDISO is getting smaller.
Naturally, the ratio between these two approaches is highly dependent on the ratio between CPU and GPU performance and memory bandwidth.
The explicit approach with CHOLMOD is generally the slowest, since it does not utilize the sparsity pattern of $\tilde{\mB}_i$ and uses the CPU.

% application - gpu explicit vs cpu implcit
Moving on to the application, using the explicitly assembled $\tilde{\mF}_i$ on the GPU is generally the fastest (except for small subdomains where overheads dominate).
The improvement is caused by the dense matrix representation of $\mF$ and by the higher GPU memory bandwidth compared to the CPU.

% na 2d je speedup vetsi
The higher speed-up of explicit approaches for subdomains from the 2D mesh compared to speed-up for 3D mesh is caused by a better ratio between the number of DOFs and the number of Lagrange multipliers, i.e., $\tilde{\mF}_i$ is proportionally smaller compared to $\mK_i$ for the 2D mesh.

% GEMV/SYMV performance better with newere cuda
Contrary to the assembly, the application of $\tilde{\mF}_i$ is slightly faster with CUDA modern than with CUDA legacy. This is probably caused by better optimizations in the newer version.

% cpu apply
There is no difference in the application between the two explicit CPU approaches. This is because they use the same matrices, and the library that assembles $\tilde{\mF}_i$ does not have an influence here.
Implicit application on CPU is again faster in MKL PARDISO than CHOLMOD for subdomains from the 2D mesh and small subdomains from the 3D mesh.

% hybrid
The hybrid approach (original GPU implementation) simply copies the trend of the MKL PARDISO assembly on the CPU and the explicit application on the GPU.







\subsection{Choosing the best dual operator approach}

\begin{figure*}
    \begin{subfigure}[b]{0.47\textwidth}
        \centering
        \input{tikz/bestdualop/karolina-heat_transfer_2d-TRIANGLE3.tex}
        \caption{Heat transfer 2D}
        \label{fig:best_dualop_2d}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.47\textwidth}
        \centering
        \input{tikz/bestdualop/karolina-heat_transfer_3d-TETRA4.tex}
        \caption{Heat transfer 3D}
        \label{fig:best_dualop_3d}
    \end{subfigure}
    \caption{Time-step time of the best dual operator}
    \label{fig:best_dualop}
\end{figure*}

In Fig.~\ref{fig:best_dualop}, we plot the overall time spent in the dual operator -- it shows the time of preprocessing and iterations summed together. For each subdomain size, the line denotes the time of only the best dual operator approach for the number of iterations given by the X-axis. The color and style of the line denote the approach used.

As can be seen, if the solver does only several iterations, the implicit approach with MKL PARDISO is the best option for both 2D and 3D subdomains.
As the number of iterations increases, the explicit approaches start to improve.
The amortization point (the number of iterations, where the explicit approach starts to be better than implicit) depends on problem dimensionality and subdomain size.

For 2D meshes, MKL PARDISO dominates.
For small subdomains, the CPU-only explicit approach is the best because of the high latency of CUDA.
For larger subdomains, the hybrid approach is optimal.
The MKL PARDISO domination is caused by the slow factorization in the CHOLMOD library, as discussed in the previous subsection.
If we had a library that provides factors with the same speed as MKL PARDISO, our accelerated approach for medium-size subdomains would be better according to the graphs in Fig.~\ref{fig:time_karolina_2d_update} (difference between the \textit{impl\_cholmod} and \textit{expl\_legacy} is smaller than the difference between \textit{impl\_mkl} and \textit{expl\_mkl}).

For 3D meshes, accelerated explicit approaches are better (CUDA legacy or CUDA modern).
About ten iterations are enough to make the acceleration beneficial for medium-sized domains (about 500 - 2000 DOFs).
With increasing the subdomain size, the number of iterations until amortization increases due to the longer preprocessing time.

The graphs in Fig.~\ref{fig:best_dualop} can be used to decide whether acceleration is beneficial and which dual operator approach should be used for best timing results.

\begin{figure*}
    \begin{subfigure}[b]{0.47\textwidth}
        \centering
        \input{tikz/speedup/karolina-heat_transfer_2d-TRIANGLE3.tex}
        \caption{Heat transfer 2D}
        \label{fig:speedup_2d}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.47\textwidth}
        \centering
        \input{tikz/speedup/karolina-heat_transfer_3d-TETRA4.tex}
        \caption{Heat transfer 3D}
        \label{fig:speedup_3d}
    \end{subfigure}
    \caption{Speedup of the best dual operator relative to the implicit CPU approach}
    \label{fig:speedup}
\end{figure*}

Fig.~\ref{fig:speedup} shows the maximal speedup for a given number of iterations and subdomain sizes compared to the implicit CPU approach with Intel MKL.
The beginning of each line denotes the number of iterations in which another approach begins to be beneficial. The lines (speedup) go up with the increasing number of iterations until they reach the limit determined by the speedup of a single application shown in Fig.~\ref{fig:time_karolina_2d_apply} and~\ref{fig:time_karolina_3d_apply}.
The visible changes in the trends are caused by a change in the optimal dual operator approach (e.g., from implicit to explicit).

Fig.~\ref{fig:speedup} also shows that using the correct approach can have an enormous impact on the computation time.
For 2D meshes with subdomains with more than 10,000 DOFs and 100 PCPG iterations, the FETI solver can be more than 10 times faster if the explicit approach is used.
For ill-conditioned problems with hundreds of iterations, the explicit approach reaches an overall speed-up of about 100.
In general, the best speed-up was observed for subdomains with 100,000--300,000 DOFs.
For 3D meshes, the speed-up is substantially smaller due to the mentioned ratio between the number of Lagrange multipliers and the size of subdomains, which is better for 2D meshes.
A speed-up of 4 was achieved for subdomains with around 2,000 DOFs and 100 PCPG iterations.
For ill-conditioned problems with hundreds of iterations, a speed-up of about 10 was observed for subdomains with 4,000--20,000 DOFs.

The graphs in Fig.~\ref{fig:speedup} can be used to estimate the expected speedup of the FETI solver for given subdomain sizes and the number of PCPG iterations.






























\section{Conclusion}

The paper presents an approach for the acceleration of explicit assembly of the FETI dual operator $\mF = \mB \mK^+ \mB^\top$ using Nvidia A100.
Despite the operator being composed of sparse matrices, we have shown that modern Nvidia accelerators can be successfully used.

Large differences were observed between major versions of CUDA libraries and between various parameters of the assembly process.
Based on exhaustive measurements of all possible configurations on matrices generated by heat transfer and linear elasticity physics, the paper suggests optimal settings.
With the recommended settings, one can expect the best time-to-solution among all the available options.

Using Karolina GPU node with two 64-core AMD Zen3 CPUs and 8 Nvidia A100 GPUs, for problems that need hundreds of iterations of the FETI solver, we reach a speedup of up to 100 for 2D meshes with the hybrid explicit approach, and for 3D meshes the speedup is up to 10 with the explicit GPU approach.
The amortization point was between 5 and 10 iterations for 2D meshes and between 10 and 100 iterations for 3D meshes.

In future work, our aim is to test our approach on AMD and Intel GPU accelerators, and also on the newest Nvidia Grace-Hopper.
Additionally, we would like to develop an algorithm that is able to better utilize the sparsity pattern of matrices during the explicit assembly process on GPUs.













\section*{Acknowledgement}
This work was supported by the EUPEX project. This project has received funding from the European High-Performance Computing Joint Undertaking (JU) under grant agreement No 101033975. The JU receives support from the European Union’s Horizon 2020 research and innovation programme and France, Germany, Italy, Greece, United Kingdom, Czech Republic, Croatia. And by the Ministry of Education, Youth and Sports of the Czech Republic through the e-INFRA CZ (ID:90254).

\bibliographystyle{IEEEtran}
\bibliography{bib}

\end{document}
