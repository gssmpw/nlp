\subsection{Convergence}\label{sec:convergence}
Next, we prove that an Erlang mixture kernel of infinite order converges to any regular kernel as the rate parameter, $a$, goes to infinity, for a specific choice of the coefficients $\{c_m\}_{m=0}^\infty$, which depends on the kernel that is being approximated. First, we define the Erlang mixture delta family, $\delta_a$, which is shown in Figure~\ref{fig:erlang:mixture:delta:family} for different values of $t$ and the rate parameter, $a$. Next, we prove a number of lemmas, and we will draw on the analogy of $\delta_a$ to the probability density function of a non-negative random variable, e.g., by determining its mean and variance. Finally, we present a general theorem about the approximation of regular kernels and use it to prove the above statement about the Erlang mixture kernel.
%
\begin{definition}\label{def:erlang:mixture:delta:family}
	An Erlang mixture delta family, $\delta_a: \Rnn \times \Rnn \rightarrow \Rnn$, with rate parameter $a \in \Rp$ is the piecewise constant function
	%
	\begin{align}\label{eq:erlang:mixture:delta:familiy}
		\delta_a(t, s) &= \ell_m(t), & s &\in[s_m, s_{m+1}), &
		s_m &= m \Delta s, &
		\Delta s &= 1/a,
	\end{align}
	%
	where $\ell_m$ is an Erlang kernel of $m$'th order with rate parameter $a$. The size of the intervals over which $\delta_a$ is piecewise constant is $\Delta s = s_{m+1} - s_m \in \Rp$ for all $m \in \Nnn$, which depends on $a$, and in each interval, $\delta_a$ is equal to an Erlang kernel of different order.
\end{definition}
%
\begin{figure}[t]
	\centering
	\subfloat{\includegraphics[width=\textwidth]{./fig/ErlangMixtureDeltaFamily-eps-converted-to}}
	\caption{The Erlang mixture delta family for a fixed rate parameter, $a$, and different values of $t$ (left) and for fixed $t$ and different values of $a$ (right). For $t = 0$ in the left figure, the value of $\delta_a$ is 2 for $s \in [0, 0.5)$ (see also Lemma~\ref{lem:erlang:mixture:delta:t:equal:zero}).}
	\label{fig:erlang:mixture:delta:family}
\end{figure}
%
\begin{definition}
	The mean, $\mu: \Rnn \rightarrow \Rnn$, associated with a delta family, $\delta_a: \Rnn \times \Rnn \rightarrow \Rnn$, is given by the integral
	%
	\begin{align}\label{eq:definition:mean}
		\mu(t)
		&= \int_0^\infty \delta_a(t, s) s \incr s.
	\end{align}
\end{definition}
%
\begin{definition}
	The variance, $\sigma^2: \Rnn \rightarrow \Rnn$, associated with a delta family, $\delta_a: \Rnn \times \Rnn \rightarrow \Rnn$, is given by the integral
	%
	\begin{align}
		\sigma^2(t) &= \int_0^\infty \delta_a(t, s) (s - \mu(t))^2 \incr s,
	\end{align}
	%
	or, equivalently, by
	%
	\begin{align}\label{eq:erlang:mixture:delta:family:variance}
		\sigma^2(t) &= \mu_2(t) - \mu^2(t), &
		\mu_2(t) &= \int_0^\infty \delta_a(t, s) s^2 \incr s,
	\end{align}
	%
	where $\mu: \Rnn \rightarrow \Rnn$ is the mean given by~\eqref{eq:definition:mean}.
\end{definition}
%
\begin{lemma}\label{lem:erlang:mixture:delta:t:equal:zero}
	For $t = 0$, the Erlang mixture delta family is given by
	%
	\begin{align}
		\delta_a(0, s) &=
		\begin{cases}
			a, & s \in [0, 1/a), \\
			0, & \mathrm{otherwise}.
		\end{cases}
	\end{align}
\end{lemma}
%
\begin{proof}
	By direct substitution, $\ell_0(0) = a$ and $\ell_m(0) = 0$ for $m > 0$. Furthermore, by definition, $[s_0, s_1) = [0, 1/a)$.
\end{proof}
%
\begin{lemma}\label{lem:erlang:mixture:delta:family:integral}
	The integral of an Erlang mixture delta family, $\delta_a: \Rnn \times \Rnn \rightarrow \Rnn$, over all values of the second argument is one for all values of $a \in \Rp$ and $t \in \Rnn$:
	%
	\begin{align}
		\int_0^\infty \delta_a(t, s) \incr s &= 1.
	\end{align}
\end{lemma}
%
\begin{proof}
	We substitute the expression for $\delta_a$ in~\eqref{eq:erlang:mixture:delta:familiy} and simplify:
	%
	\begin{align}
		\int_0^\infty \delta_a(t, s) \incr s
		&= \sum_{m=0}^\infty \ell_m(t) \int_{s_m}^{s_{m+1}} 1 \incr s = \sum_{m=0}^\infty \ell_m(t) \Delta s = \sum_{m=0}^\infty \frac{(at)^m}{m!} a e^{-at} \frac{1}{a} \nonumber \\
		&= \sum_{m=0}^\infty \frac{(at)^m}{m!} e^{-at} = e^{-at} e^{at} = 1.
	\end{align}
	%
	Here, we have used~\eqref{eq:exponential}.
\end{proof}
%
\begin{lemma}\label{lem:erlang:mixture:delta:family:convergence:to:zero}
	Let $\delta_a: \Rnn \times \Rnn \rightarrow \Rnn$ be an Erlang mixture delta family. Then, for all $t, s \in \Rnn$, $\delta_a(t, s) \rightarrow 0$ as $a \rightarrow \infty$ if $s \neq t$.
\end{lemma}
%
\begin{proof}
	In the proof, we will use Stirling's approximation~\cite{Romik:2000}: $\ln n! \approx n \ln n - n + \frac{1}{2} \ln 2 \pi n$.
	%
	The error of the approximation decreases with $1/n$. We will show that for given $\epsilon \in \Rp$ and for fixed $s$ and $t$, it is possible to choose $a$ sufficiently large that the inequality
	%
	\begin{align}
		\delta_a(t, s) &= \ell_m(t) < \epsilon
	\end{align}
	%
	is satisfied if $s \neq t$. For simplicity, we assume that $a$ is chosen such that $m = as$ is integer. First, we assume that $t > 0$ and substitute $\ell_m$ and the value of $m$:
	%
	\begin{align}
		\frac{(at)^{as}}{(as)!} a e^{-at} &< \epsilon.
	\end{align}
	%
	Next, we take the logarithm on both sides and substitute Stirling's approximation:
	%
	\begin{subequations}
		\begin{align}
			as \ln at - \ln (as)! + \ln a - at &< \ln \epsilon, \\
			as \ln at - \left(as \ln as - as + \frac{1}{2} \ln 2 \pi as\right) + \ln a - at &< \ln \epsilon.
		\end{align}
	\end{subequations}
	%
	Finally, we rearrange terms:
	%
	\begin{align}
%		as \left(\ln at - \ln as\right) + a (s - t) + \frac{1}{2} \ln a - \frac{1}{2} \ln 2 \pi s &< \ln \epsilon, \\
		a \left(s \left(\ln \frac{t}{s} + 1\right) - t\right) + \frac{1}{2} \ln a &< \ln \epsilon + \frac{1}{2} \ln 2 \pi s.
	\end{align}
	%
	For $s \neq t$, the factor of $a$ in the first term is always negative, and it is always possible to satisfy the bound for sufficiently large $a$. This follows from the inequality $\ln x \leq x - 1$ for $x > 0$. In contrast, if $s = t$, the factor of $a$ is zero, and the inequality is only satisfied if
	%
	\begin{align}
		\frac{1}{2} \ln a &< \ln \epsilon + \frac{1}{2} \ln 2 \pi s & \mathrm{or} &&
		a < 2 \pi s \epsilon^2,
	\end{align}
	%
	i.e., if $a$ is bounded from above. For $t = 0$, $\delta_a(t, s)$ is only nonzero for $s < 1/a$ (see Lemma~\ref{lem:erlang:mixture:delta:t:equal:zero}). Consequently, for a fixed $s \neq t$, it is always possible to choose $a$ large enough that $\delta_a(t, s) = 0$.
\end{proof}

\begin{lemma}\label{lem:erlang:mixture:delta:family:monotonicity}
	An Erlang mixture delta family, $\delta_a: \Rnn \times \Rnn \rightarrow \Rnn$, is non-decreasing in the second argument, $s$, when it is below the first argument, $t$, and non-increasing when it is above. Specifically, since $\delta_a$ is piecewise constant over intervals of size $\Delta s$, $\delta_a(t, s) \leq \delta_a(t, s + \Delta s)$ for $s < t$, and $\delta_a(t, s) \geq \delta_a(t, s + \Delta s)$ for $s \geq t$.
\end{lemma}
%
\begin{proof}
	Let $s \in [s_{m-1}, s_m)$. Then, $s + \Delta s \in [s_m, s_{m+1})$, and we derive a condition on $m$ for $\delta_a$ to be non-decreasing:
	%
	\begin{align}
		\delta_a(t, s) &\leq \delta_a(t, s + \Delta s).
	\end{align}
	%
	We substitute the expressions for $\ell_{m-1}(t)$ and $\ell_m(t)$ on the left- and right-hand side, respectively,
	%
	\begin{align}
		\frac{(at)^{m-1}}{(m-1)!} a e^{-at} &\leq \frac{(at)^m}{m!} a e^{-at},
	\end{align}
	%
	and obtain the condition
	%
	\begin{align}
		m &\leq at.
	\end{align}
	%
	This corresponds to $s_m = m \Delta s = m/a \leq t$, and $s < s_m$ by assumption. Consequently, $\delta_a$ is non-decreasing in $s$ for $s < t$, and the proof that $\delta_a$ is non-increasing in $s$ for $s \geq t$ is analogous.
\end{proof}
%
\begin{lemma}
	The mean, $\mu: \Rnn \rightarrow \Rnn$, associated with an Erlang mixture delta family, $\delta_a: \Rnn \times \Rnn \rightarrow \Rnn$, with rate parameter $a$ is given by
	%
	\begin{align}\label{eq:erlang:mixture:delta:family:mean}
		\mu(t) &= t + \frac{1}{2a},
	\end{align}
	%
	and for finite $a \in \Rp$, $\mu(t) > 0$ for all $t \in \Rnn$.
\end{lemma}
%
\begin{proof}
	We substitute the definition of $\delta_a$ in~\eqref{eq:erlang:mixture:delta:familiy}:
	%
	\begin{align}\label{eq:erlang:mixture:delta:family:mu:proof}
		\mu(t)
		&= \int_0^\infty \delta_a(t, s) s \incr s = \sum_{m=0}^\infty \ell_m(t) \int_{s_m}^{s_{m+1}} s \incr s.
	\end{align}
	%
	Next, we write out the expression for the integral,
	%
	\begin{align}
		\int_{s_m}^{s_{m+1}} s \incr s
		&= \frac{1}{2} (s_{m+1}^2 - s_m^2) = \frac{1}{2} (s_{m+1} - s_m)(s_{m+1} + s_m) = \frac{1}{2} \Delta s ((m+1) + m) \Delta s \\
		&= \frac{2m + 1}{2} \Delta s^2, \nonumber
	\end{align}
	%
	and substitute into~\eqref{eq:erlang:mixture:delta:family:mu:proof}:
	%
	\begin{align}
		\mu(t)
		&= \sum_{m=0}^\infty \frac{(at)^m}{m!} a e^{-at} \frac{2m+1}{2} \Delta s^2 = t \sum_{m=1}^\infty \frac{(at)^{m-1}}{(m-1)!} e^{-at} + \frac{1}{2a} \sum_{m=0}^\infty \frac{(at)^m}{m!} e^{-at} \\
		&= t + \frac{1}{2a}. \nonumber
	\end{align}
	%
	Here, we have used the identity~\eqref{eq:exponential} and that $\Delta s = 1/a$.
\end{proof}
%
\begin{lemma}
	The variance, $\sigma^2: \Rnn \rightarrow \Rnn$, associated with an Erlang mixture delta family, $\delta_a: \Rnn \times \Rnn \rightarrow \Rnn$, with rate parameter $a$ is given by
	%
	\begin{align}
		\sigma^2(t) &= \frac{1}{a} \left(t + \frac{1}{12 a}\right).
	\end{align}
\end{lemma}

\begin{proof}
	First, we write out the integral in the definition of $\mu_2$ in~\eqref{eq:erlang:mixture:delta:family:variance}:
	%
	\begin{align}\label{eq:erlang:mixture:delta:family:variance:proof}
		\mu_2(t) &= \int_0^\infty \delta_a(t, s) s^2 \incr s = \sum_{m=0}^\infty \ell_m(t) \int_{s_m}^{s_{m+1}} s^2 \incr s.
	\end{align}
	%
	Next, we write out the integral of $s^2$,
	%
	\begin{align}
		\int_{s_m}^{s_{m+1}} s^2 \incr s
		&= \frac{1}{3} (s_{m+1}^3 - s_m^3) = \frac{1}{3} (s_{m+1} - s_m) (s_{m+1}^2 + s_{m+1} s_m + s_m^2) \nonumber \\
		&= \frac{1}{3} \Delta s ((m+1)^2 + (m+1) m + m^2) \Delta s^2 \nonumber \\
		&= \left(m (m-1) + 2m + \frac{1}{3}\right) \Delta s^3,
	\end{align}
	%
	and substitute it into the right-hand side of~\eqref{eq:erlang:mixture:delta:family:variance:proof}:
	%
	\begin{align}
		\mu_2(t) &= \sum_{m=0}^\infty \frac{(at)^m}{m!} a e^{-at} \left(m (m-1) + 2m + \frac{1}{3}\right) \Delta s^3.
	\end{align}
	%
	We simplify each of the three sums separately:
	%
	\begin{subequations}\label{eq:erlang:mixture:delta:family:variance:terms}
		\begin{align}
			\sum_{m=0}^\infty \frac{(at)^m}{m!} a e^{-at} m (m-1) \Delta s^3
			= t^2 \sum_{m=2}^\infty \frac{(at)^{m-2}}{(m-2)!} e^{-at} &= t^2, \\
			%
			\sum_{m=0}^\infty \frac{(at)^m}{m!} a e^{-at} 2m \Delta s^3
			= 2 \frac{t}{a} \sum_{m=1}^\infty \frac{(at)^{m-1}}{(m-1)!} e^{-at} &= 2 \frac{t}{a}, \\
			\sum_{m=0}^\infty \frac{(at)^m}{m!} a e^{-at} \frac{1}{3} \Delta s^3
			= \frac{1}{3 a^2} \sum_{m=0}^\infty \frac{(at)^m}{m!} e^{-at} &= \frac{1}{3 a^2}.
		\end{align}
	\end{subequations}
	%
	In all three cases, we have used the identity~\eqref{eq:exponential} and that $\Delta s = 1/a$.
	%
	Finally, we substitute the resulting expression for $\mu_2$ and the expression for the mean~\eqref{eq:erlang:mixture:delta:family:mean} into the definition of $\sigma^2$ in~\eqref{eq:erlang:mixture:delta:family:variance} and simplify:
	%
	\begin{align}
		\sigma^2(t)
		&= \mu_2(t) - \mu^2(t) = t^2 + 2 \frac{t}{a} + \frac{1}{3 a^2} - \left(t + \frac{1}{2 a}\right)^2 = t^2 + 2 \frac{t}{a} + \frac{1}{3 a^2} - t^2 - \frac{1}{4 a^2} - \frac{t}{a} \\
		&= \frac{t}{a} + \frac{1}{12 a^2}. \nonumber
	\end{align}
\end{proof}

\begin{lemma}\label{lem:theorem:kallenberg}
	Let $\delta: \Rnn \times \Rnn \rightarrow \Rnn$ be an Erlang mixture delta family with mean $\mu: \Rnn \rightarrow \Rnn$ and rate parameter $a \in \Rp$. Then, the inequality
	%
	\begin{align}
		1 - \int_{\mu(t) - \gamma}^{\mu(t) + \gamma} \delta_a(t, s) \incr s \leq \frac{\sigma^2(t)}{\gamma^2}
	\end{align}
	%
	holds for all $\gamma \in \Rp$ that satisfy $\gamma \leq \mu(t)$.
\end{lemma}
%
\begin{proof}
	This follows from Chebyshev's (or Markov's) well-known inequality in probability theory and the fact that $\mu(t) > 0$ for all $t \in \Rnn$ and finite rate parameters, $a \in \Rp$. See, e.g., Lemma~4.1 in the book by Kallenberg~\cite{Kallenberg:2002}.
\end{proof}
%
\begin{lemma}\label{lem:unit:integral}
	Let $\delta: \Rnn \times \Rnn \rightarrow \Rnn$ be an Erlang mixture delta family with rate parameter $a \in \Rp$. Then, for all $\gamma \in \Rp$ and $t \in \Rnn$,
	%
	\begin{align}\label{eq:erlang:mixture:delta:family:probability:concentration}
		\int_{[t - \gamma]^+}^{t + \gamma} \delta_a(t, s) \incr s \rightarrow 1
	\end{align}
	%
	as $a \rightarrow \infty$.
\end{lemma}
%
\begin{proof}
	First, we assume that $t > 0$ and bound the integral from above and below using Lemma~\ref{lem:erlang:mixture:delta:family:integral} and~\ref{lem:theorem:kallenberg}:
	%
	\begin{align}
		1 - \frac{\sigma^2(t)}{\bar \gamma^2} \leq \int_{\mu(t) - \bar \gamma}^{\mu(t) + \bar \gamma} \delta_a(t, s) \incr s \leq \int_{[\mu(t) - \gamma]^+}^{\mu(t) + \gamma} \delta_a(t, s) \incr s \leq \int_0^\infty \delta_a(t, s) \incr s = 1.
	\end{align}
	%
	Here, $\bar \gamma = \min\{t, \gamma\}$ such that the limits in the leftmost integral are non-negative. Consequently, since $\mu(t) \rightarrow t$ and $\sigma^2(t) \rightarrow 0$ as $a \rightarrow \infty$, the integral in~\eqref{eq:erlang:mixture:delta:family:probability:concentration} approaches one. Next, if $t = 0$, the integral in~\eqref{eq:erlang:mixture:delta:family:probability:concentration} is equal to one for all $a \geq 1/\gamma$.
\end{proof}
%
Next, we prove the main results of this section. Theorem~\ref{thm:convergence:delta:family} is an adaptation of the theorem presented in Section~9.3 of the book by Korevaar~\cite{Korevaar:1968} regarding the convergence of approximations based on delta families.
%
\begin{theorem}\label{thm:convergence:delta:family}
	Let $\delta_a: \Rnn \times \Rnn \rightarrow \Rnn$ be a delta family parametrized by $a \in \Rp$ which satisfies the following conditions.
	%
	\begin{enumerate}
		\item For all $\gamma \in \Rp$ and $t \in \Rnn$,
		%
		\begin{align}
			\int_{[t - \gamma]^+}^{t + \gamma} \delta_a(t, s) \incr s \rightarrow 1
		\end{align}
		%
		as $a \rightarrow \infty$.
		%
		\item For all $\gamma \in \Rp$ and for all $s \in \Rnn$ satisfying $\gamma \leq |t-s| < \infty$, $\delta_a(t, s) \rightarrow 0$ uniformly as $a \rightarrow \infty$ for all $t \in \Rnn$.
		%
		\item $\delta_a(t, s) \geq 0$ for all $t, s \in \Rnn$ and $a \in \Rp$.
	\end{enumerate}
	%
	Furthermore, let $\alpha: \Rnn \rightarrow \Rnn$ be a regular kernel.
	%
	Then, the approximation $\hat \alpha: \Rnn \rightarrow \Rnn$ given by
	%
	\begin{align}\label{eq:theorem:approximation}
		\hat \alpha(t) &= \int_0^\infty \delta_a(t, s) \alpha(s) \incr s
	\end{align}
	%
	converges (pointwise) to $\alpha(t)$ for all $t \in \Rnn$ as $a \rightarrow \infty$.
\end{theorem}
%
\begin{proof}
	First, we split the integral into two terms:
	%
	\begin{align}
		\hat \alpha(t)
		&= \int_0^\infty \delta_a(t, s) \alpha(s) \incr s \\
		&= \underbrace{\int_{[t - \gamma]^+}^{t + \gamma} \delta_a(t, s) \alpha(s) \incr s}_{\mathcal I_1(t)} + \underbrace{\int_0^{[t - \gamma]^+} \delta_a(t, s) \alpha(s) \incr s + \int_{t + \gamma}^\infty \delta_a(t, s) \alpha(s) \incr s}_{\mathcal I_2(t)}\hspace{-2pt}.
	\end{align}
	%
	Next, we add zero to the first term and split the integral:
	%
	\begin{align}
		\mathcal I_1(t) &= \underbrace{\alpha(t) \int_{[t - \gamma]^+}^{t + \gamma} \delta_a(t, s) \incr s}_{\mathcal I_3(t)} + \underbrace{\int_{[t - \gamma]^+}^{t + \gamma} \delta_a(t, s) (\alpha(s) - \alpha(t)) \incr s}_{\mathcal I_4(t)}\hspace{-2pt}.
	\end{align}
	%
	The second term,  $\mathcal I_4(t)$, can be bounded through the continuity of $\alpha$. Specifically, choose $\gamma$ such that $|\alpha(s) - \alpha(t)| < \epsilon$ for $|s - t| < \gamma$. Then,
	%
	\begin{align}
		\mathcal I_4(t) < \epsilon \int_{[t - \gamma]^+}^{t + \gamma} \delta_a(t, s) \incr s < \epsilon.
	\end{align}
	%
	Next, we prove that the term $\mathcal I_2(t) \rightarrow 0$ as $a \rightarrow \infty$. We bound $\mathcal I_2$ from above by substituting the supremum of $\delta_a$, bounding the integrals by 1, and utilizing the non-negativity of $\alpha$ and $\delta_a$:
	%
	\begin{align}
		\mathcal I_2(t) &= \int_0^{[t - \gamma]^+} \delta_a(t, s) \alpha(s) \incr s + \int_{t + \gamma}^\infty \delta_a(t, s) \alpha(s) \incr s \leq \sup_{\substack{s \in \Rnn \\ \gamma \leq |t - s| < \infty}} \delta_a(t, s).
	\end{align}
	%
	Furthermore, we note that by assumption, the supremum of $\delta_a$ over values of $s$ outside of an interval around $t$ goes to zero as $a \rightarrow \infty$.
	%
	We choose $a$ large enough that $\mathcal I_2(t) < \epsilon$. Additionally, we choose $a$ large enough that
	%
	\begin{align}
		1 - \int_{[t - \gamma]^+}^{t + \gamma} \delta_a(t, s) \incr s < \epsilon.
	\end{align}
	%
	Then,
	%
	\begin{align}
		|\mathcal I_3(t) - \alpha(t)| &= \alpha(t) \left(1 - \int_{[t - \gamma]^+}^{t + \gamma} \delta_a(t, s) \incr s\right) < \alpha(t) \epsilon,
	\end{align}
	%
	and by utilizing the above bounds, we can choose $a$ sufficiently large that
	%
	\begin{align}
		|\hat \alpha(t) - \alpha(t)|
		&< |\mathcal I_1(t) - \alpha(t)| + \mathcal I_2(t) \nonumber \\
		&< |\mathcal I_3(t) - \alpha(t)| + \mathcal I_4(t) + \mathcal I_2(t) \nonumber \\
		&< (\alpha(t) + 2) \epsilon,
	\end{align}
	%
	for any $\epsilon \in \Rp$. We have made repeated use of triangle equalities and the fact that the integrals $\mathcal I_i(t)$ for $i = 1, \ldots, 4$ and $t \in \Rnn$ are non-negative. To summarize, for any $\bar \epsilon \in \Rp$, we can choose $\bar a \in \Rp$ such that $|\hat \alpha(t) - \alpha(t)| < \bar \epsilon$ for all $a \geq \bar a$.
\end{proof}
%
\begin{theorem}\label{thm:erlang:mixture:approximation}
	Let $\hat \alpha: \Rnn \rightarrow \Rnn$ be an Erlang mixture kernel of infinite order with rate parameter $a \in \Rp$,
	%
	\begin{align}
		\hat \alpha(t) &= \sum_{m=0}^\infty c_m \ell_m(t),
	\end{align}
	%
	where the coefficients are given by
	%
	\begin{align}\label{eq:erlang:mixture:approximation:coefficients}
		c_m &= \int_{s_m}^{s_{m+1}} \alpha(s) \incr s, & m &= 0, 1, \ldots,
	\end{align}
	%
	for $s_m = m \Delta s \in \Rnn$ and $\Delta s = 1/a \in \Rp$. Furthermore, let $\alpha: \Rnn \rightarrow \Rnn$ be a regular kernel. Then, $\hat \alpha$ converges pointwise to $\alpha$, i.e.,
	%
	\begin{align}
		\hat \alpha(t) \rightarrow \alpha(t)
	\end{align}
	%
	as $a \rightarrow \infty$ for all $t \in \Rnn$.
\end{theorem}
%
\begin{proof}
	First, we prove that the Erlang mixture delta family $\delta_a$ satisfies the three properties of Theorem~\ref{thm:convergence:delta:family}. The first property follows directly from Lemma~\ref{lem:unit:integral}. The second property follows from Lemma~\ref{lem:erlang:mixture:delta:family:convergence:to:zero} and the monotonicity properties in Lemma~\ref{lem:erlang:mixture:delta:family:monotonicity}, i.e., the convergence is uniform outside of an interval around $t$ because $\delta_a$ is monotonically non-decreasing and non-increasing for values of $s$ lower and higher than $t$, respectively. Finally, it is straightforward to verify the third property from Definition~\ref{def:erlang:kernel} and~\ref{def:erlang:mixture:delta:family} because $\ell_m$ is the product of a monomial and an exponential function with non-negative arguments. Next, it follows from the definition of the Erlang mixture delta family, $\delta_a$, that the approximation in~\eqref{eq:theorem:approximation} is an Erlang mixture kernel of infinite order,
	%
	\begin{align}
		\hat \alpha(t)
		&= \int_0^\infty \delta_a(t, s) \alpha(s) \incr s
		 = \sum_{m=0}^\infty \ell_m(t) \int_{s_m}^{s_{m+1}} \alpha(s) \incr s,
	\end{align}
	%
	and that the coefficients are given by~\eqref{eq:erlang:mixture:approximation:coefficients}.
\end{proof}
%
\begin{corollary}
	The coefficients defined by~\eqref{eq:erlang:mixture:approximation:coefficients} sum to one, i.e.,
	%
	\begin{align}
		\sum_{m=0}^\infty c_m &= 1.
	\end{align}
\end{corollary}
%
\begin{proof}
	This is a straightforward consequence of the fact that, by assumption, the integral of $\alpha$ is one. We write out the sum of the coefficients:
	%
	\begin{align}
		\sum_{m=0}^\infty c_m &= \sum_{m=0}^\infty \int_{s_m}^{s_{m+1}} \alpha(s) \incr s = \int_0^\infty \alpha(s) \incr s = 1.
	\end{align}
\end{proof}
%
\begin{remark}
	To the best of the author's knowledge, the result in Theorem~\ref{thm:erlang:mixture:approximation} is novel, i.e., it has not been proven that the probability density function of an Erlang mixture distribution of infinite order converges pointwise to that of any non-negative random variable with a density function that satisfies the regularity conditions in Definition~\ref{def:regular:kernel}. It has been proven~\cite{Cossette:etal:2016, Tijms:1995} that the cumulative distribution function of such an Erlang mixture distribution converges to that of any non-negative random variable (i.e., convergence in distribution has been proven).
	%
	Furthermore, it is well-known that, according to Scheff{\'{e}}'s lemma~\cite{Scheffe:1947}, the convergence of probability density functions implies convergence of cumulative distribution functions. However, the converse is only true if the probability density function is bounded and equicontinuous~\cite{Boos:1985, Sweeting:1986}. Alternatively, the converse is also true if the probability density function converges uniformly~\cite{Rudin:1976}. However, it is outside the scope of the present work to further investigate these alternative proofs.
\end{remark}
