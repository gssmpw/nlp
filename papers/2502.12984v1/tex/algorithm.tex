\section{Algorithm}\label{sec:algo}
In this section, we describe an algorithm for determining the coefficients and the rate parameter in an $M$'th order Erlang mixture approximation of a regular kernel, $\alpha: \Rnn \rightarrow \Rnn$, for given $M \in \Nnn$. For simplicity, we only consider scalar kernels in this section. The algorithm consists of two steps: First, we identify a domain in which we approximate the kernel and then, we use a least-squares approach to determine the coefficients and the rate parameter. Furthermore, we describe a reference approach based on the theoretical expressions~\eqref{eq:erlang:mixture:approximation:coefficients} for the coefficients. Finally, we describe the implementation of the algorithm and the simulation of the original DDEs~\eqref{eq:system:x}--\eqref{eq:system:r} and the approximate system of ODEs~\eqref{eq:lct:approximate:system:ODE}.

\subsection{Domain}\label{sec:algo:domain}
In order to determine the approximation domain, we introduce $\beta: \Rnn \rightarrow [0, 1]$ given by
%
\begin{align}\label{eq:cumulative:distribution:function}
	\beta(t) &= \int_0^t \alpha(s) \incr s,
\end{align}
%
which is analogous to the cumulative distribution function of a random variable with probability density function $\alpha$. As $\alpha$ is non-negative, $\beta$ is non-decreasing, and for a given threshold, $\epsilon \in \Rp$, we choose $t_h \in \Rnn$ as the solution to the equation $1 - \beta(t) = \epsilon$ and approximate $\alpha$ in the interval $[0, t_h]$.
%
We use bisection to determine $t_h$ approximately.
%
We assume that an initial interval, $[t_0^\ell, t_0^u]$, is available and that it contains the root, i.e., that $1 - \beta(t_0^\ell) > \epsilon$ and $1 - \beta(t_0^u) < \epsilon$. If that is not the case, the interval can be shifted or scaled such that it does. Next, the $k$'th iteration has converged and an approximate solution has been found if $|1 - \beta(t_k^m) - \epsilon|$ is below a specified threshold, where $t_k^m = \frac{1}{2}(t_k^\ell + t_k^u)$ is the midpoint. Otherwise, the next iteration considers the lower half of the interval, $[t_{k+1}^\ell, t_{k+1}^u] = [t_k^\ell, t_k^m]$, if $1 - \beta(t_k^m) < \epsilon$. Similarly, if $1 - \beta(t_k^m) > \epsilon$, the next iteration considers the upper half of the interval, $[t_{k+1}^\ell, t_{k+1}^u] = [t_k^m, t_k^u]$.
%
\begin{remark}
	If the function $\beta$ given by~\eqref{eq:cumulative:distribution:function} cannot be derived analytically for a given kernel, $\alpha$, it can be approximated using numerical quadrature (e.g., with \matlab{}'s function \integral{}).
\end{remark}
%
\begin{remark}
	The equation $1 - \beta(t) = \epsilon$ can also be solved using a gradient-based approach, e.g., Newton's method. However, $1 - \beta(t)$ rounds to zero in finite-precision arithmetic when $t$ is too large, e.g., in a computer implementation. Consequently, Newton iterations may stall if an iterate happens to become too large.
\end{remark}

\subsection{Identify kernel parameters}\label{sec:algo:least:squares}
For a given order, $M \in \Nnn$, and $t_h \in \Rnn$, the objective is to determine the values of the coefficients, $c_m \in [0, 1]$ for $m = 0, \ldots, M$, and the rate parameter, $a \in \Rp$, that minimize the integral of the squared error,
%
\begin{align}
	\int_0^{t_h} (\alpha(t) - \hat \alpha(t))^2 \incr t,
\end{align}
%
while satisfying the constraint that the coefficients must sum to one. We approximate this integral by a left rectangle rule (such that the objective function includes $\alpha(0)$) with $N$ rectangles, and we determine the coefficients and rate parameters by solving the inequality-constrained nonlinear program (NLP)
%
\begin{subequations}\label{eq:algo:nlp}
	\begin{align}
		\label{eq:algo:nlp:obj}
		\min_{\{c_{m}\}_{m=0}^{M}, a} \quad &\phi = \frac{1}{2} \sum_{k=0}^{N-1} (\alpha(t_k) - \hat \alpha(t_k))^2 \Delta t, \\
		%
		\label{eq:algo:nlp:sum}
		\text{subject to} \quad &
		\sum_{m=0}^{M} c_{m} = 1, \\
		%
		\label{eq:algo:nlp:c}
		&0 \leq c_{m} \leq 1, \quad m = 0, \ldots, M, \\
		%
		\label{eq:algo:nlp:a}
		&a_{\min} \leq a,
	\end{align}
\end{subequations}
%
where the quadrature points are $t_k = k \Delta t$ for $k = 0, \ldots, N-1$ and $a_{\min} \in \Rp$ is an arbitrary lower bound, which ensures that only positive values of $a$ are feasible. Furthermore, the objective function involves the regular kernel, $\alpha: \Rnn \rightarrow \Rnn$, and the $M$'th order Erlang mixture kernel, $\hat \alpha: \Rnn \rightarrow \Rnn$, with coefficients $\{c_m\}_{m=0}^M$ and rate parameter $a$, evaluated at the quadrature points.

\subsubsection{Derivatives}
Many algorithms for solving NLPs in the form~\eqref{eq:algo:nlp} can benefit from analytical expressions for the first- and second-order derivative of the objective function. The derivatives of the objective function are
%
\begin{subequations}
	\begin{align}
		\pdiff{\phi}{v} &= -\sum_{k=0}^{N-1} (\alpha(t_k) - \hat \alpha(t_k)) \pdiff{\hat \alpha}{v}(t_k) \Delta t, \\
		\mdiff{\phi}{v}{w} &= -\sum_{k=0}^{N-1} \left((\alpha(t_k) - \hat \alpha(t_k)) \mdiff{\hat \alpha}{v}{w}(t_k) - \pdiff{\hat \alpha}{w}(t_k) \pdiff{\hat \alpha}{v}(t_k)\right) \Delta t,
	\end{align}
\end{subequations}
%
where $v$ and $w$ represent either $a$ or $c_m$. Furthermore, the first-order derivatives of the Erlang mixture kernel are
%
\begin{subequations}
	\begin{align}
		\pdiff{\hat \alpha}{a}(t)     	&= \sum_{m=0}^{M} c_m \pdiff{\ell_m}{a}(t), \\
		\pdiff{\hat \alpha}{c_k}(t) 	&= \ell_k(t), & k &= 0, \ldots, M,
	\end{align}
\end{subequations}
%
and the second-order derivatives are
%
\begin{subequations}
	\begin{align}
		\pdiff[2]{\hat \alpha}{a}(t)        &= \sum_{m=0}^{M} c_m \pdiff[2]{\ell_m}{a}(t), \\
		\mdiff{\hat \alpha}{a}{c_k}(t) 		&= \pdiff{\ell_k}{a}(t), & k &= 0, \ldots, M, \\
		\mdiff{\hat \alpha}{c_k}{c_n}(t) 	&= 0, & k &= 0, \ldots, M, & n &= 0, \ldots, M.
	\end{align}
\end{subequations}
%
Finally, the first- and second-order derivatives of the Erlang kernel with respect to the rate parameter are
%
\begin{subequations}
	\begin{align}
		\pdiff{\ell_m}{a}(t) 		&= \left(\frac{m+1}{a} - t\right) \ell_m(t), & m &= 0, \ldots, M, \\
		\pdiff[2]{\ell_m}{a}(t) 	&= \left(\frac{m+1}{a} - t\right) \pdiff{\ell_m}{a}(t) - \frac{m+1}{a^2} \ell_m(t), & m &= 0, \ldots, M.
	\end{align}
\end{subequations}

\subsection{Theoretical Erlang mixture approximation}\label{sec:algo:theoretical:approximation}
We will compare the accuracy obtained with the least-squares approach described in Section~\ref{sec:algo:least:squares} to that of a reference approach based on the theoretical values of the coefficients from Theorem~\ref{thm:erlang:mixture:approximation}. First, we determine the size of the approximation interval, $t_h$, such that $1 - \beta(t_h) \approx \epsilon$, e.g., using bisection as described in Section~\ref{sec:algo:domain}. Next, for a given order, $M$, we determine the rate parameter by $a = t_h/(M+1)$, and we use the theoretical expression~\eqref{eq:erlang:mixture:approximation:coefficients} to compute the coefficients, $c_m$, for $m = 0, \ldots, M$. In general, $\epsilon$ should be chosen to be small. Consequently, the coefficients will almost sum to one, and it is not necessary to scale them.

\subsection{Implementation}\label{sec:algo:implementation}
We implement the evaluation of the objective function in the NLP~\eqref{eq:algo:nlp} and its first- and second-order derivatives in C, and we use \matlab{}'s \texttt{MEX} interface to call the C routines from \matlab{}. Furthermore, we use \matlab{}'s \fmincon{} to approximate the solution to the NLP, and we provide analytical first- and second-order derivatives. We exploit that $\alpha_m(t) = (at/m) \alpha_{m-1}(t)$ for $m = 1, \ldots, M$ where $\alpha_0(t) = a e^{-at}$ is less prone to rounding errors than a straightforward evaluation of the expression~\eqref{eq:erlang:pdf} in the definition, and we use either double or long double precision when evaluating the Erlang kernels, the Erlang mixture kernels, and their derivatives.
%
We simulate the approximate system of ODEs~\eqref{eq:lct:approximate:system:ODE} using either \matlab{}'s \odeff{} or \odeofs, and depending on the order of the Erlang mixture kernel, we implement the matrices $A$, $B$, and $C$ as either dense or sparse. Furthermore, we provide the analytical Jacobian of the right-hand side function of the approximate system of ODEs~\eqref{eq:lct:approximate:system:ODE} to the simulator.
%
We implement the numerical method for non-stiff DDEs described in Appendix~\ref{sec:numerical:simulation:non:stiff} in both \matlab{} and C (with a \texttt{MEX} interface). The numerical method from Appendix~\ref{sec:numerical:simulation:stiff} is implemented in \matlab{}, we use \fsolve{} to solve the residual equations, and we provide the analytical Jacobian.  In all three implementations, the right-hand side function, $f$, the delay function, $h$, and the kernel, $\alpha$, are evaluated in \matlab{}. Finally, in Section~\ref{sec:ex}, we carry out parallel simulations and kernel approximations using a high-performance computing cluster~\cite{DTU:DCC:Resource}.
