\input{model_comparison_table}

\section{Avoiding catastrophe in \citet{plaut_avoiding_2024} and in MDPs}\label{sec:ac-overview}

Since our main result is a reduction to \citet{plaut_avoiding_2024}, we first describe their work. This section also serves to establish the novelty of our contribution in relation to \citet{plaut_avoiding_2024}. Some technical details are deferred to Appendix~\ref{sec:ac-full}.

Our model and that of \citet{plaut_avoiding_2024} are quite similar, so their model is perhaps best understood through comparison with our model. In place of $r$, they consider a sequence of reward functions $\bfmu = (\mu_1,\dots,\mu_T)$, where $\mu_t: \s\times\A\to[0,1]$ represents the probability of avoiding catastrophe at time $t$ (conditioned on no prior catastrophe). They aim to maximize the product of rewards from $\bfmu$, which is equal to the overall chance of avoiding catastrophe by the chain rule of probability. They also make a local generalization assumption: letting $\mu_t^m(s) = \mu_t(s, \pi^m(s))$ for brevity, we say that $\bfmu$ satisfies local generalization if $|\mu_t^m(s) - \mu_t(s, \pi^m(s'))| \le L\norm{s - s'}$ for all $s, s'\in \s$ and $t \in [T]$.


% \emph{Regret definition.} We evaluate the agent and mentor on their respective state sequences (as determined by their respective actions and the transition kernel). Instead, \citet{plaut_avoiding_2024} assume that a single state sequence is provided by an adversary\footnote{Their adversary can be adaptive: the choice of $s_t$ and $\mu_t$ can depend on the events of prior time steps.} and evaluate both the agent and the mentor on that same sequence. 

% \emph{Desired regret.} Instead of sublinear regret, \citet{plaut_avoiding_2024} demand \emph{subconstant} regret. That is, the total (not average) regret should go to 0 as $T \to\infty$. However, we will see that subconstant regret in their setting is of the same ``difficulty'' as sublinear regret in our setting (\Cref{sec:ac-mdp}).



\Cref{tab:model-comparison} summarizes the differences between our models. Formally, we arrive at \Cref{def:ac}:

\begin{restatable}{definition}{defAC}
\label{def:ac}
Let $\rac = \sup_{\M, \pi^m,\bfmu} \E[\sum_{t=1}^T \mu_t^m(s_t) - \sum_{t=1}^T \mu_t(s_t, a_t)]$, where the supremum ranges over all $\bfmu$ satisfying local generalization. Then an algorithm \emph{avoids catastrophe} if $Q_T \in o(T)$ and $\lim_{T\to\infty} \rac = 0$.\looseness=-1
\end{restatable}

\Cref{def:ac} was intended by \citet{plaut_avoiding_2024} to represent avoiding catastrophe. This interpretation can be debated, but ultimately we primarily use \Cref{def:ac} as a mathematical tool for our main goal: a no-regret algorithm for general MDPs. As such, what really matters is that \citet{plaut_avoiding_2024} give an algorithm satisfying \Cref{def:ac}.

% First, instead of evaluating the mentor on $\sm$ and the agent on $\smols$, \Cref{def:ac} evaluates both on $\smols$: this makes the problem easier. On the flip side, \Cref{def:ac} demands subconstant regret instead of sublinear regret, which makes the problem harder.  problem is not obviously easier or harder than our problem

% There are two subtleties in \Cref{def:ac}. 

Before stating the relevant lemma, we discuss two subtleties of \Cref{def:ac}. First, \citet{plaut_avoiding_2024} assume that the agent never observes $\mu_t(s_t,a_t)$ and only learns from mentor queries. Thus for fixed $\M$ and $\pi^m$, the choice of $\bfmu$ does not affect the behavior of the algorithm. As a result, \Cref{def:ac} in effect requires that for any $\M$ and $\pi^m$, the algorithm satisfies $\E[\sum_{t=1}^T \mu_t^m(s_t) - \sum_{t=1}^T \mu_t(s_t, a_t)] \le \rac$ \emph{simultaneously} for every $\bfmu$ satisfying local generalization. This enables us to use multiple choices of $\bfmu$ in our reduction.

Second, the reader may notice that \Cref{def:ac} involves the sum of rewards, while we stated previously that \citet{plaut_avoiding_2024} study the products of rewards. However, in this particular context, the sum and product formulations are roughly equivalent (\Cref{prop:sum-prod}).\footnote{This is mostly due to the approximation $1+x \approx e^x$ for $x \approx 0$.} Indeed, \citet{plaut_avoiding_2024} also include additive versions of all of their regret bounds. We use their additive bounds since those are more convenient for us. Specifically, they prove the following:\looseness=-1

% \footnote{Although \Cref{lem:ac-regret} assumes binary actions, \citet{plaut_avoiding_2024} also provide a version for any finite number of actions (Algorithm 3 and Theorem C.1 in their paper). Our result applies equally to that version -- indeed, our result is a general reduction -- but the many-action version requires more terminology to state, so we have chosen to only include the binary-action version here.}
\begin{lemma}[Theorems 5.2 and 5.3 in \citet{plaut_avoiding_2024}]
\label{lem:ac-regret}
    Let $\A = \{0,1\}$. Assume $\pi^m \in \Pi$ and either (1) $\Pi$ has finite VC dimension $d$ and $P$ is $\sigma$-smooth or (2) $\Pi$ has finite Littlestone dimension $d$.\footnote{The second case does not assume $\sigma$-smoothness, so the regret and query bounds in that case do not actually depend on $\sigma$. However, for brevity, we just write a single set of bounds.} Then Algorithm 1 in \citet{plaut_avoiding_2024} avoids catastrophe (\Cref{def:ac}) with
    % $Q_T \in  O(T^\frac{4n+1}{4n+2}(\frac{d}{\sigma} \log T + \diam(\smols)^n))$ and $\rac \in O(\frac{dL}{\sigma}T^\frac{-1}{2n+1} \log T)$.
\begin{align*}
Q_T \in&\  O\left(T^\frac{4n+1}{4n+2}\Big(\frac{d}{\sigma} \log T + \diam(\smols)^n\Big)\right)\\
\rac \in&\ O\left(\frac{dL}{\sigma}T^\frac{-1}{2n+1} \log T\right)
\end{align*}
\end{lemma}

Their algorithm has several beneficial properties beyond the regret and query bounds: it can handle an unbounded state space (the number of queries scales with the diameter of observed states), it does not need to know $L$, and it does not need to know how states are encoded in $\bbr^n$. See Appendix~\ref{sec:ac-full} for pseudocode for their algorithm.

Although \Cref{lem:ac-regret} requires $\A = \{0,1\}$, \citet{plaut_avoiding_2024} also provide a version for any finite number of actions (Algorithm 3 and Theorem C.1 in their paper). Our reduction applies equally to that version, but the many-action version requires more terminology to state, so we have chosen to only include the binary-action version here.


Lastly, although this does not directly bear on our results, it is worth mentioning that \citet{plaut_avoiding_2024} also prove a negative result: without the assumptions of \Cref{lem:ac-regret}, no algorithm satisfies both $Q_T \in o(T)$ and $\rac \in o(1)$, even for a simple environment: $\s = [0,1]$, i.i.d. states, and $\bfmu$ does not vary between time steps. We can expect this negative result to apply to our setting too, since obtaining high reward requires avoiding catastrophe. But what does ``catastrophe'' mean in an MDP?\looseness=-1

\textbf{Catastrophe in MDPs.} In contrast to the model of \citet{plaut_avoiding_2024}, MDPs do not directly model catastrophe in an objective function. Instead, notions of catastrophe arise naturally through the environment dynamics: for example, the agent might get stuck in an inescapable zero-reward state.\looseness=-1

\input{mdp_trap_fig}

To be concrete, consider the MDP in \Cref{fig:trap}. An agent obtains sublinear regret in this MDP iff it obtains subconstant regret with respect to $\bfmu$: in particular, both occur iff the agent reaches the success state. This means that on a technical level, avoiding catastrophe in an MDP and avoiding catastrophe in the sense of \citet{plaut_avoiding_2024} are the same ``difficulty''. However, on top of avoiding catastrophe, we expect the agent to obtain high reward. Framed differently,  \citet{plaut_avoiding_2024} only consider the simple class of MDPs in \Cref{fig:trap}, while we prove that their algorithm performs well across all MDPs.
