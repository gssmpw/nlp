\section{Conclusion}

In this paper, we showed that any algorithm that avoids catastrophe in the sense of \citet{plaut_avoiding_2024} is guaranteed to be a no-regret algorithm in general MDPs. Applying our reduction to the algorithm from \citet{plaut_avoiding_2024} produces the first no-regret guarantee for general MDPs. Conceptually, we prove that this algorithm obtains high reward while becoming self-sufficient, even when some errors may be catastrophic.

Although we think these insights are broadly applicable, this particular algorithm has some practical limitations. For one, the algorithm relies on perfectly computing distances between states in order to exploit local generalization. This is analogous to requiring a perfect out-of-distribution (OOD) detector, which is a very unsolved problem \citep{yang2024generalized}. Future work might consider a more realistic model of OOD detection and/or a less strict version of the local generalization assumption. Our work also makes the standard yet limiting assumptions of total observability and knowledge of the policy class, which could be relaxed in future work.\looseness=-1

% We are also interested in other models of learning about catastrophe. One way humans avoid catastrophe in the real world is by learning from bad-but-not-catastrophic experiences: for example, someone who gets food poisoning may be more cautious with food safety in the future. This phenomenon is not captured in our current model.

We are also interested in cautious learning without a mentor. We argued for the employment of mentors in high-stakes AI applications, but realistically, a mentor may not always be available (or may not always respond promptly). Future work could explore safe learning when asking for help is not possible, but the agent still has some way to make inferences about novel states. One possibility is by learning from bad-but-not-catastrophic experiences: for example, someone who gets serious (but not fatal) food poisoning may be more cautious with food safety in the future. This phenomenon is not captured in our current model.

% \todo{Discuss bad to very bad generalization}

More broadly, we think that the principle of acting cautiously when uncertain may be more powerful than hitherto suspected. We are hopeful that this idea can help make AI systems safer and more beneficial for all of society.

