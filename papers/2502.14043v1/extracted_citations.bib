@book{altman2021constrained,
  title={Constrained Markov decision processes},
  author={Altman, Eitan},
  year={2021},
  publisher={Routledge}
}

@inproceedings{azar_minimax_2017,
	title = {Minimax Regret Bounds for Reinforcement Learning},
	url = {https://proceedings.mlr.press/v70/azar17a.html},
	language = {en},
	urldate = {2024-01-13},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Azar, Mohammad Gheshlaghi and Osband, Ian and Munos, Rémi},
	month = jul,
	year = {2017},
	pages = {263--272},
}

@inproceedings{cohen_pessimism_2020,
	title = {Pessimism {About} {Unknown} {Unknowns} {Inspires} {Conservatism}},
	url = {https://proceedings.mlr.press/v125/cohen20a.html},
	abstract = {If we could define the set of all bad outcomes, we could hard-code an agent which avoids them; however, in sufficiently complex environments, this is infeasible. We do not know of any general-purpose approaches in the literature to avoiding novel failure modes. Motivated by this, we define an idealized Bayesian reinforcement learner which follows a policy that maximizes the worst-case expected reward over a set of world-models. We call this agent pessimistic, since it optimizes assuming the worst case. A scalar parameter tunes the agent’s pessimism by changing the size of the set of world-models taken into account. Our first main contribution is: given an assumption about the agent’s model class, a sufficiently pessimistic agent does not cause “unprecedented events” with probability \$1-{\textbackslash}delta\$, whether or not designers know how to precisely specify those precedents they are concerned with. Since pessimism discourages exploration, at each timestep, the agent may defer to a mentor, who may be a human or some known-safe policy we would like to improve. Our other main contribution is that the agent’s policy’s value approaches at least that of the mentor, while the probability of deferring to the mentor goes to 0. In high-stakes environments, we might like advanced artificial agents to pursue goals cautiously, which is a non-trivial problem even if the agent were allowed arbitrary computing power; we present a formal solution.},
	language = {en},
	urldate = {2024-01-13},
	booktitle = {Proceedings of {Thirty} {Third} {Conference} on {Learning} {Theory}},
	publisher = {PMLR},
	author = {Cohen, Michael K. and Hutter, Marcus},
	month = jul,
	year = {2020},
	pages = {1344--1373},
}

@article{he2021nearly,
  title={Nearly minimax optimal reinforcement learning for discounted {MDPs}},
  author={He, Jiafan and Zhou, Dongruo and Gu, Quanquan},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={22288--22300},
  year={2021}
}

@article{jaksch_near-optimal_2010,
	title = {Near-optimal {Regret} {Bounds} for {Reinforcement} {Learning}},
	volume = {11},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v11/jaksch10a.html},
	number = {51},
	urldate = {2024-01-13},
	journal = {Journal of Machine Learning Research},
	author = {Jaksch, Thomas and Ortner, Ronald and Auer, Peter},
	year = {2010},
	pages = {1563--1600},
}

@inproceedings{kosoy_delegative_2019,
	title = {Delegative {Reinforcement} {Learning}: learning to avoid traps with a little help},
	shorttitle = {Delegative {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1907.08461},
	doi = {10.48550/arXiv.1907.08461},
	abstract = {Most known regret bounds for reinforcement learning are either episodic or assume an environment without traps. We derive a regret bound without making either assumption, by allowing the algorithm to occasionally delegate an action to an external advisor. We thus arrive at a setting of active one-shot model-based reinforcement learning that we call DRL (delegative reinforcement learning.) The algorithm we construct in order to demonstrate the regret bound is a variant of Posterior Sampling Reinforcement Learning supplemented by a subroutine that decides which actions should be delegated. The algorithm is not anytime, since the parameters must be adjusted according to the target time discount. Currently, our analysis is limited to Markov decision processes with finite numbers of hypotheses, states and actions.},
	urldate = {2024-01-13},
	publisher = {arXiv},
	author = {Kosoy, Vanessa},
	month = jul,
	year = {2019},
	note = {arXiv:1907.08461 [cs, stat]},
	keywords = {68Q32, Computer Science - Machine Learning, I.2.6, Statistics - Machine Learning},
}

@inproceedings{lattimore_asymptotically_2011,
	address = {Berlin, Heidelberg},
	series = {{ALT}'11},
	title = {Asymptotically optimal agents},
	isbn = {978-3-642-24411-7},
	abstract = {Artificial general intelligence aims to create agents capable of learning to solve arbitrary interesting problems. We define two versions of asymptotic optimality and prove that no agent can satisfy the strong version while in some cases, depending on discounting, there does exist a non-computable weak asymptotically optimal agent.},
	urldate = {2024-11-19},
	booktitle = {Proceedings of the 22nd international conference on {Algorithmic} learning theory},
	publisher = {Springer-Verlag},
	author = {Lattimore, Tor and Hutter, Marcus},
	month = oct,
	year = {2011},
	pages = {368--382},
}

@article{liu2021learning,
  title={Learning policies with zero or bounded constraint violation for constrained {MDPs}},
  author={Liu, Tao and Zhou, Ruida and Kalathil, Dileep and Kumar, Panganamala and Tian, Chao},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={17183--17193},
  year={2021}
}

@misc{liu_regret_2021,
	title = {Regret {Bounds} for {Discounted} {MDPs}},
	url = {http://arxiv.org/abs/2002.05138},
	urldate = {2024-11-19},
	publisher = {arXiv},
	author = {Liu, Shuang and Su, Hao},
	month = may,
	year = {2021},
	note = {arXiv:2002.05138},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:/Users/benjamin/Zotero/storage/QVW4QERH/Liu and Su - 2021 - Regret Bounds for Discounted MDPs.pdf:application/pdf;Snapshot:/Users/benjamin/Zotero/storage/QXCVI5VX/2002.html:text/html},
}

@unpublished{maillard_active_2019,
	title = {Active {Roll}-outs in {MDP} with {Irreversible} {Dynamics}},
	url = {https://hal.science/hal-02177808},
	abstract = {In Reinforcement Learning (RL), regret guarantees scaling with the square root of the time horizon have been shown to hold only for communicating Markov decision processes (MDPs) where any two states are connected. This essentially means that an algorithm can eventually recover from any mistake. However, real-world tasks usually include situations where taking a single "bad" action can permanently trap a learner in a suboptimal region of the state-space. Since it is provably impossible to achieve sub-linear regret in general multi-chain MDPs, we assume a weak mechanism that allows the learner to request additional information. Our main contribution is to address: (i) how much external information is needed, (ii) how and when to use it, and (iii) how much regret is incurred. We design an algorithm that minimizes requests for external information in the form of rollouts of a policy specified by the learner by actively requesting it only when needed. The algorithm provably achieves O(√ T) active regret after T steps in a large class of multi-chain MDPs, by only requesting O(log(T)) rollout transitions. The superiority of our algorithm to standard algorithms such as R-Max and UCRL is demonstrated in experiments on some illustrative grid-world examples. (a) (b) (c) Figure 1: Example of (a) a communicating MDP, (b) a unichain MDP with a single recurrent class, and (c) a multi-chain MDP with two recurrent classes. The circles represent states while the labeled edges represent transitions due to executing actions \{a, b, c\}.},
	urldate = {2024-01-15},
	author = {Maillard, Odalric-Ambrym and Mann, Timothy and Ortner, Ronald and Mannor, Shie},
	month = jul,
	year = {2019},
	keywords = {MDP, Multi-chain, Recoverability c xxxx Odalric-Ambrym, Regret analysis, Reinforcement learning},
}

@InProceedings{model_zhao_22a,
  title = 	 {Model-free Safe Control for Zero-Violation Reinforcement Learning},
  author =       {Zhao, Weiye and He, Tairan and Liu, Changliu},
  booktitle = 	 {Proceedings of the 5th Conference on Robot Learning},
  pages = 	 {784--793},
  year = 	 {2022},
  editor = 	 {Faust, Aleksandra and Hsu, David and Neumann, Gerhard},
  volume = 	 {164},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {08--11 Nov},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v164/zhao22a/zhao22a.pdf},
  url = 	 {https://proceedings.mlr.press/v164/zhao22a.html},
  abstract = 	 {While deep reinforcement learning (DRL) has impressive performance in a variety of continuous control tasks, one critical hurdle that limits the application of DRL to physical world is the lack of safety guarantees. It is challenging for DRL agents to persistently satisfy a hard state constraint (known as the safety specification) during training. On the other hand, safe control methods with safety guarantees have been extensively studied. However, to synthesize safe control, these methods require explicit analytical models of the dynamic system; but these models are usually not available in DRL. This paper presents a model-free safe control strategy to synthesize safeguards for DRL agents, which will ensure zero safety violation during training. In particular, we present an implicit safe set algorithm, which synthesizes the safety index (also called the barrier certificate) and the subsequent safe control law only by querying a black-box dynamic function (e.g., a digital twin simulator). The theoretical results indicate the implicit safe set algorithm guarantees forward invariance and finite-time convergence to the safe set. We validate the proposed method on the state-of-the-art safety benchmark Safety Gym. Results show that the proposed method achieves zero safety violation and gains $ 95% \pm 9%$ cumulative reward compared to state-of-the-art safe DRL methods. Moreover, it can easily scale to high-dimensional systems.}
}

@misc{plaut_avoiding_2024,
	title = {Avoiding {Catastrophe} in {Online} {Learning} by {Asking} for {Help}},
	url = {http://arxiv.org/abs/2402.08062},
	doi = {10.48550/arXiv.2402.08062},
	abstract = {Most learning algorithms with formal regret guarantees assume that no mistake is irreparable and essentially rely on trying all possible behaviors. This approach is problematic when some mistakes are {\textbackslash}emph\{catastrophic\}, i.e., irreparable. We propose an online learning problem where the goal is to minimize the chance of catastrophe. Specifically, we assume that the payoff in each round represents the chance of avoiding catastrophe that round and aim to maximize the product of payoffs (the overall chance of avoiding catastrophe) while allowing a limited number of queries to a mentor. We first show that in general, any algorithm either constantly queries the mentor or is nearly guaranteed to cause catastrophe. However, in settings where the mentor policy class is learnable in the standard online learning model, we provide an algorithm whose regret and rate of querying the mentor both approach 0 as the time horizon grows. Conceptually, if a policy class is learnable in the absence of catastrophic risk, it is learnable in the presence of catastrophic risk if the agent can ask for help.},
	urldate = {2024-10-22},
	publisher = {arXiv},
	author = {Plaut, Benjamin and Zhu, Hanlin and Russell, Stuart},
	month = oct,
	year = {2024},
	note = {arXiv:2402.08062},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/benjamin/Zotero/storage/KCG36R7M/Plaut et al. - 2024 - Avoiding Catastrophe in Online Learning by Asking .pdf:application/pdf;Snapshot:/Users/benjamin/Zotero/storage/ZRTW44GF/2402.html:text/html},
}

@article{stradi2024learning,
  title={Learning Adversarial {MDPs} with Stochastic Hard Constraints},
  author={Stradi, Francesco Emanuele and Castiglioni, Matteo and Marchesi, Alberto and Gatti, Nicola},
  journal={arXiv preprint arXiv:2403.03672},
  year={2024}
}

@inproceedings{zhao_state-wise_2023,
	title = {State-wise {Safe} {Reinforcement} {Learning}: {A} {Survey}},
	volume = {6},
	shorttitle = {State-wise {Safe} {Reinforcement} {Learning}},
	url = {https://www.ijcai.org/proceedings/2023/763},
	doi = {10.24963/ijcai.2023/763},
	abstract = {Electronic proceedings of IJCAI 2023},
	language = {en},
	urldate = {2024-05-11},
	author = {Zhao, Weiye and He, Tairan and Chen, Rui and Wei, Tianhao and Liu, Changliu},
	month = aug,
	year = {2023},
	note = {ISSN: 1045-0823},
	pages = {6814--6822},
}

