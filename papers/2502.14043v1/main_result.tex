
\section{Our results}\label{sec:main}

We are now ready to state our main result:

\begin{restatable}{theorem}{thmMain}
\label{thm:main}
Any algorithm which avoids catastrophe (\Cref{def:ac}) satisfies $R_T \le (T+1)\rac\in o(T)$ and $Q_T \in o(T)$.
\end{restatable}

Combining \Cref{thm:main} with the work of \citet{plaut_avoiding_2024} (specifically, \Cref{lem:ac-regret}) produces the first no-regret guarantee for general MDPs:

\begin{theorem}
\label{thm:combined}
Under the conditions of \Cref{lem:ac-regret}, Algorithm 1 in \citet{plaut_avoiding_2024} makes $o(T)$ mentor queries and is a no-regret algorithm. Specifically,
\begin{align*}
Q_T \in&\ O\left(T^\frac{4n+1}{4n+2}\Big(\frac{d}{\sigma} \log T + \diam(\smols)^n\Big)\right)\\
R_T  \in&\ O\left(\frac{dL}{\sigma}T^\frac{2n}{2n+1} \log T\right)
\end{align*}
\end{theorem}

\Cref{def:ac} states that $Q_T \in o(T)$ and $\lim_{T\to\infty} \rac = 0$, the latter of which implies that $(T+1)\rac \in o(T)$. Thus our main task is to prove that $R_T \le (T+1)\rac$.

\subsection{Regret decomposition}

Our analysis revolves around a decomposition of regret into \emph{state-based regret} and \emph{action-based regret}. \Cref{fig:proof} defines each of these components and illustrates the proof structure. To the best of our knowledge, this decomposition is novel and could be of independent interest.

\emph{State-based regret} measures how bad the agent's states $\smols$ are compared to the mentor's states $\sm$. Specifically, ``bad'' is evaluated with respect to the actions the mentor would take in each state. To bound the state-based regret, we will use the local generalization of $P$ to bound the deviation between the distributions of $\smols$ and $\sm$ as measured by $\Delta_t = \sup_{X\subseteq \s}(p_t^m(X) - p_t(X))$. Most of the proof is focused on showing that the state-based regret is at most $T \rac$.

\emph{Action-based regret} measures how bad the agent's actions are compared to the mentor's, evaluated on the agent's states $\smols$. Local generalization of $r$ will imply that the action-based regret is at most $\rac$.\footnote{Appendix~\ref{sec:alt} provides an alternative proof that bounds the action-based regret using a different method which not require local generalization for $r$ (although it is still required for $P$). However, this proof requires the algorithm to satisfy an additional property, which does hold for Algorithm 1 in \citet{plaut_avoiding_2024}, but may not hold for all algorithms which avoid catastrophe.} Combining this with our bound on state-based regret yields  $R_T \le (T+1)\rac$, as desired.

Superficially, our regret decomposition might resemble existing techniques like the reference-advantage decomposition \citep{zhang2020almost}. The key difference is that each term in our decomposition includes only short-term rewards, and long-term effects remain implicit. In contrast, prior decompositions (including reference-advantage) typically revolve around $Q$-functions and/or value functions which explicitly capture the long-term value of policies/states/actions. Neither approach is necessarily ``better'', but they may be suitable for different contexts.

