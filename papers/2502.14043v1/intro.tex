\section{Introduction}\label{sec:intro}

Recent advances in AI have amplified concerns about a variety of risks \citep{critch2023tasra, hendrycks2023overviewcatastrophicairisks, national_institute_of_standards_and_technology_us_artificial_2024}. Such risks include autonomous weapon accidents \citep{Abaimov2020}, autonomous vehicle crashes \citep{kohli2020enabling}, bioterrorism \citep{mouton2024operational}, medical errors \citep{rajpurkar2022ai}, and discriminatory sentencing \citep{villasenor2020artificial}.\footnote{This breakdown of risks is borrowed from \citet{plaut_avoiding_2024}.} These errors are particularly concerning due to their irreparable nature. For example, a fatal error made by an AI surgeon can never be rectified.\looseness=-1

% fatalities from a robotic vehicle crash can never be rectified.

Despite the severity of these risks, they remain largely unaddressed by reinforcement learning (RL) theory. Specifically, nearly all of RL theory assumes that any error can be recovered from. This assumption enables algorithms to cheerfully try all possible behaviors, since no matter how badly the algorithm performs in the short term, it can always eventually recover. Although such approaches have produced positive results, they forfeit any possibility of safety guarantees for the high-stakes AI applications mentioned above. Algorithms that try all possible behaviors are clearly not appropriate for such applications; indeed, they may primarily be suitable for simulation-based learning, where the rewards are not real at all. We contend that a fundamentally different algorithmic paradigm is needed.

% \todo{phrasing}

One alternative is to devise agents that recognize when they are uncertain and then ask for help. When help is requested, a supervisor (here called a \emph{mentor} to reflect their collaborative role) guides the agent to avoid problematic actions without the agent having to try those actions firsthand. This approach is particularly suitable for high-stakes applications where AI systems are already configured to work alongside humans. For example, imagine a human doctor who oversees AI surgeons and is prepared to take over in tricky situations, or a backup human driver in an autonomous vehicle. In these scenarios, occasional requests for human guidance are both practical and valuable for ensuring safety.

Recent work \citep{plaut_avoiding_2024} asked whether a limited number of queries to a mentor is sufficient for an agent to avoid catastrophe (i.e., irreparable errors) in an online learning model where the agent can transfer knowledge between similar states. They proved that avoiding catastrophe in this model is quite tractable: specifically, it is no harder than standard online learning. 

Their work shows that access to a mentor can yield strong safety guarantees for AI agents. Safety guarantees are necessary, but not sufficient: agents must also be able to complete useful tasks. For example, an autonomous vehicle that ensures safety by never moving is hardly a vehicle at all.

In this paper, our goal is not only to avoid catastrophe but also to maximize reward. Perhaps surprisingly, we prove that \emph{any} algorithm that is guaranteed to avoid catastrophe in the sense of \citet{plaut_avoiding_2024} is also guaranteed to obtain high reward in Markov Decision Processes (MDPs). In other words, asking for help enables safety guarantees without sacrificing effectiveness.

\subsection{Our model}\label{sec:model}

We study online RL, where the agent must perform well while learning: finding a good policy is futile if catastrophe is caused along the way. We allow MDP to be \emph{non-communicating}\footnote{Detailed definitions of terms like ``communicating'' can be found in \citet{puterman2014markov} as part of a thorough treatment of MDPs.} (some states might not be reachable from other states), and we do not require the agent to be reset. We will refer to this class of MDPs as ``general MDPs''.

On each time step, the agent has the option to query a mentor. When queried, the mentor illustrates the action they would take in the current state. Our goal is to minimize \emph{regret}, defined as the difference between the expected sum of rewards obtained by the agent and the mentor when each is given the same starting state. (Regret typically refers to the performance gap between the agent and an optimal policy, but we allow the mentor to be either optimal or suboptimal, so our definition is strictly more general.)

An algorithm for this problem should satisfy two criteria:
\begin{enumerate}
    \item The agent's performance should approach that of the mentor. Formally, the average regret per time step should go to 0 as the time horizon $T$ goes to infinity, or equivalently, the cumulative regret should be sublinear in $T$ (denoted $o(T)$). An algorithm that guarantees $o(T)$ regret is called a \emph{no-regret} algorithm.  % guarantees in all MDPs?
    \item The agent should eventually become self-sufficient. Formally, the rate of querying the mentor should go to 0 as $T \to \infty$, or equivalently, the cumulative number of queries should be $o(T)$.
\end{enumerate}
In summary, we study the central open question posed by \citet{plaut_avoiding_2024}: is there a no-regret algorithm for (undiscounted) general MDPs that makes $o(T)$ mentor queries?

\subsection{Our contribution}\label{sec:contribution}
We show that the answer to that question is yes whenever avoiding catastrophe (per \Cref{def:ac}) is possible:

\begin{theoremNumbered}[\ref{thm:main} \textnormal{(Informal)}] 
Any algorithm that is guaranteed to avoid catastrophe with $o(T)$ mentor queries is a no-regret algorithm for MDPs (still with $o(T)$ mentor queries).
\end{theoremNumbered}

\Cref{thm:main} does not say that avoiding catastrophe itself is sufficient for high reward: that claim is false whenever there exist multiple safe policies with different amounts of expected reward. Rather, \Cref{thm:main} says that if an algorithm can avoid catastrophe in any situation, it must have certain beneficial properties that also lead to high reward. 

% Conceptually, we show that high reward in MDPs corresponds to avoiding a variety of ``mini-catastrophes'' in addition to major catastrophes such as entering an inescapable low-reward state.\looseness=-1

Our proof techniques diverge substantially from the standard repertoire of RL theory: we do not use $Q$-functions, upper confidence bounds, policy gradients, etc. Instead, our proof revolves around a decomposition of regret into ``state-based regret'' (whether the agent ends up in bad states) and ``action-based regret'' (whether the agent takes bad actions in a given state). To our knowledge, this decomposition is novel and could be of independent interest.

However, \Cref{thm:main} itself would be of limited value without an algorithm that satisfies \Cref{def:ac}. Crucially, such an algorithm was given by \citet{plaut_avoiding_2024} under certain assumptions. First, they assume \emph{local generalization}, meaning that the agent can transfer knowledge between similar states (formalized in \Cref{sec:ac-overview}). Second, they assume that either (1) the mentor policy class has finite VC dimension and the state sequence is $\sigma$-smooth\footnote{In MDPs, $\sigma$-smoothness means that the transition kernel can never concentrate at a single point: its concentration must be upper-bounded by $1/\sigma$. The idea is to prevent rare pathological inputs from dominating the analysis. This concept is not crucial to our work, but we define it for completeness. See \Cref{sec:model} for details.\looseness=-1} or (2) the mentor policy class has finite Littlestone dimension. These are also the two main settings where standard online learning is tractable. See \Cref{sec:ac-overview} for details.\looseness=-1

% \todo{Maybe state the final theorem too, e.g. that Alg 1 in \cite{plaut_avoiding_2024} is such an algorithm}

Utilizing the aforementioned algorithm, our work produces (to our knowledge) the first no-regret guarantee for general MDPs:

% \todo{Maybe we should just say ``general MDPs''?}

\begin{theoremNumbered}[\ref{thm:combined} (Informal)] 
Algorithm 1 in \citet{plaut_avoiding_2024} is a no-regret algorithm for MDPs and makes $o(T)$ mentor queries.
\end{theoremNumbered}


\textbf{In other words, we prove that it is possible for an agent to obtain high reward while becoming self-sufficient in an unknown and unbounded environment without causing catastrophe or requiring resets.} We are not aware of any prior work that can be said to imply this conclusion.\looseness=-1