
@article{krasowski_provably_2023,
	title = {Provably Safe Reinforcement Learning: Conceptual Analysis, Survey, and Benchmarking},
	issn = {2835-8856},
	url = {https://openreview.net/forum?id=mcN0ezbnzO},
	shorttitle = {Provably Safe Reinforcement Learning},
	journaltitle = {Transactions on Machine Learning Research},
	author = {Krasowski, Hanna and Thumm, Jakob and Müller, Marlon and Schäfer, Lukas and Wang, Xiao and Althoff, Matthias},
	urldate = {2025-01-24},
	date = {2023-07-14},
	langid = {english},
        year = {2023},
}


@book{kallenberg1997foundations,
  title={Foundations of modern probability},
  author={Kallenberg, Olav},
  volume={2},
  year={1997},
  publisher={Springer}
}

@book{bertsekas1996stochastic,
  title={Stochastic optimal control: the discrete-time case},
  author={Bertsekas, Dimitri and Shreve, Steven E},
  volume={5},
  year={1996},
  publisher={Athena Scientific}
}

@article{zhang2020almost,
  title={Almost optimal model-free reinforcement learningvia reference-advantage decomposition},
  author={Zhang, Zihan and Zhou, Yuan and Ji, Xiangyang},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={15198--15207},
  year={2020}
}

@InProceedings{model_zhao_22a,
  title = 	 {Model-free Safe Control for Zero-Violation Reinforcement Learning},
  author =       {Zhao, Weiye and He, Tairan and Liu, Changliu},
  booktitle = 	 {Proceedings of the 5th Conference on Robot Learning},
  pages = 	 {784--793},
  year = 	 {2022},
  editor = 	 {Faust, Aleksandra and Hsu, David and Neumann, Gerhard},
  volume = 	 {164},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {08--11 Nov},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v164/zhao22a/zhao22a.pdf},
  url = 	 {https://proceedings.mlr.press/v164/zhao22a.html},
  abstract = 	 {While deep reinforcement learning (DRL) has impressive performance in a variety of continuous control tasks, one critical hurdle that limits the application of DRL to physical world is the lack of safety guarantees. It is challenging for DRL agents to persistently satisfy a hard state constraint (known as the safety specification) during training. On the other hand, safe control methods with safety guarantees have been extensively studied. However, to synthesize safe control, these methods require explicit analytical models of the dynamic system; but these models are usually not available in DRL. This paper presents a model-free safe control strategy to synthesize safeguards for DRL agents, which will ensure zero safety violation during training. In particular, we present an implicit safe set algorithm, which synthesizes the safety index (also called the barrier certificate) and the subsequent safe control law only by querying a black-box dynamic function (e.g., a digital twin simulator). The theoretical results indicate the implicit safe set algorithm guarantees forward invariance and finite-time convergence to the safe set. We validate the proposed method on the state-of-the-art safety benchmark Safety Gym. Results show that the proposed method achieves zero safety violation and gains $ 95% \pm 9%$ cumulative reward compared to state-of-the-art safe DRL methods. Moreover, it can easily scale to high-dimensional systems.}
}


@inproceedings{wachi_survey_2024,
	title = {A {Survey} of {Constraint} {Formulations} in {Safe} {Reinforcement} {Learning}},
	volume = {9},
	url = {https://www.ijcai.org/proceedings/2024/913},
	doi = {10.24963/ijcai.2024/913},
	abstract = {Electronic proceedings of IJCAI 2024},
	language = {en},
	urldate = {2024-12-04},
	author = {Wachi, Akifumi and Shen, Xun and Sui, Yanan},
	month = aug,
	year = {2024},
	note = {ISSN: 1045-0823},
	pages = {8262--8271},
	file = {Full Text PDF:/Users/benjamin/Zotero/storage/WB2I7FIS/Wachi et al. - 2024 - A Survey of Constraint Formulations in Safe Reinfo.pdf:application/pdf},
}

@article{rajpurkar2022ai,
  title={{AI} in health and medicine},
  author={Rajpurkar, Pranav and Chen, Emma and Banerjee, Oishi and Topol, Eric J},
  journal={Nature Medicine},
  volume={28},
  number={1},
  pages={31--38},
  year={2022},
  publisher={Nature Publishing Group US New York}
}

@inproceedings{cortes2018online,
  title={Online learning with abstention},
  author={Cortes, Corinna and DeSalvo, Giulia and Gentile, Claudio and Mohri, Mehryar and Yang, Scott},
  booktitle={international conference on machine learning},
  pages={1059--1067},
  year={2018},
  organization={PMLR}
}

@inproceedings{li2008knows,
  title={Knows what it knows: a framework for self-aware learning},
  author={Li, Lihong and Littman, Michael L and Walsh, Thomas J},
  booktitle={Proceedings of the 25th international conference on Machine learning},
  pages={568--575},
  year={2008}
}

@techreport{national_institute_of_standards_and_technology_us_artificial_2024,
	address = {Gaithersburg, MD},
	title = {Artificial intelligence risk management framework : generative artificial intelligence profile},
	shorttitle = {Artificial intelligence risk management framework},
	url = {https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.600-1.pdf},
	abstract = {This document is a cross-sectoral profile of and companion resource for the AI Risk Management Framework (AI RMF 1.0) for Generative AI, 1 pursuant to President Biden’s Executive Order (EO) 14110 on Safe, Secure, and Trustworthy Artificial Intelligence.2 The AI RMF was released in January 2023, and is intended for voluntary use and to improve the ability of organizations to incorporate trustworthiness considerations into the design, development, use, and evaluation of AI products, services, and systems.},
	language = {en},
	urldate = {2024-12-09},
	institution = {National Institute of Standards and Technology (U.S.)},
	author = {{National Institute of Standards and Technology (US)}},
	month = jul,
	year = {2024},
	doi = {10.6028/NIST.AI.600-1},
}


@book{boyd2004convex,
  title={Convex optimization},
  author={Boyd, Stephen and Vandenberghe, Lieven},
  year={2004},
  publisher={Cambridge university press}
}

@book{altman2021constrained,
  title={Constrained Markov decision processes},
  author={Altman, Eitan},
  year={2021},
  publisher={Routledge}
}

@article{esser_actioneffect_2023,
	title = {Action–effect knowledge transfers to similar effect stimuli},
	volume = {87},
	issn = {1430-2772},
	url = {https://doi.org/10.1007/s00426-023-01800-4},
	doi = {10.1007/s00426-023-01800-4},
	language = {en},
	number = {7},
	urldate = {2024-11-17},
	journal = {Psychological Research},
	author = {Esser, Sarah and Haider, Hilde and Lustig, Clarissa and Tanaka, Takumi and Tanaka, Kanji},
	month = oct,
	year = {2023},
	pages = {2249--2258},
	file = {Full Text PDF:/Users/benjamin/Zotero/storage/6IX8VLMH/Esser et al. - 2023 - Action–effect knowledge transfers to similar effec.pdf:application/pdf},
}

@article{hajian_transfer_2019,
	title = {Transfer of {Learning} and {Teaching}: {A} {Review} of {Transfer} {Theories} and {Effective} {Instructional} {Practices}},
	volume = {7},
	shorttitle = {Transfer of {Learning} and {Teaching}},
	url = {https://eric.ed.gov/?id=EJ1217940},
	language = {en},
	number = {1},
	urldate = {2024-11-17},
	journal = {IAFOR Journal of Education},
	author = {Hajian, Shiva},
	year = {2019},
	note = {Publisher: International Academic Forum
ERIC Number: EJ1217940},
	keywords = {Communities of Practice, Comparative Analysis, Computer Games, Educational Games, Learning Theories, Problem Based Learning, Simulation, Situated Learning, Teaching Methods, Transfer of Training},
	pages = {93--111},
	file = {Full Text PDF:/Users/benjamin/Zotero/storage/7X24J4II/Hajian - 2019 - Transfer of Learning and Teaching A Review of Tra.pdf:application/pdf},
}


@book{osa_algorithmic_2018,
	series = {Foundations and trends in robotics},
	title = {An {Algorithmic} {Perspective} on {Imitation} {Learning}},
	isbn = {978-1-68083-410-9},
	url = {https://books.google.com/books?id=6p6EtQEACAAJ},
	publisher = {Now Publishers},
	author = {Osa, T. and Pajarinen, J. and Neumann, G. and Bagnell, J.A. and Abbeel, P. and Peters, J.},
	year = {2018},
}

@book{puterman2014markov,
  title={Markov decision processes: discrete stochastic dynamic programming},
  author={Puterman, Martin L},
  year={2014},
  publisher={John Wiley \& Sons}
}

@article{liu2021learning,
  title={Learning policies with zero or bounded constraint violation for constrained {MDPs}},
  author={Liu, Tao and Zhou, Ruida and Kalathil, Dileep and Kumar, Panganamala and Tian, Chao},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={17183--17193},
  year={2021}
}

@article{stradi2024learning,
  title={Learning Adversarial {MDPs} with Stochastic Hard Constraints},
  author={Stradi, Francesco Emanuele and Castiglioni, Matteo and Marchesi, Alberto and Gatti, Nicola},
  journal={arXiv preprint arXiv:2403.03672},
  year={2024}
}

@inproceedings{ji_regret-optimal_2023,
	address = {Red Hook, NY, USA},
	series = {{NIPS} '23},
	urldate = {2024-11-20},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Ji, Xiang and Li, Gen},
	year = {2023},
	pages = {80674--80689},
}

@inproceedings{kash_slowly_2024,
	title = {Slowly {Changing} {Adversarial} {Bandit} {Algorithms} are {Efficient} for {Discounted} {MDPs}},
	url = {https://proceedings.mlr.press/v237/kash24a.html},
	language = {en},
	urldate = {2024-11-20},
	booktitle = {Proceedings of {The} 35th {International} {Conference} on {Algorithmic} {Learning} {Theory}},
	publisher = {PMLR},
	author = {Kash, Ian A. and Reyzin, Lev and Yu, Zishun},
	month = mar,
	year = {2024},
	note = {ISSN: 2640-3498},
	pages = {683--718},
	file = {Full Text PDF:/Users/benjamin/Zotero/storage/DMWVHKVR/Kash et al. - 2024 - Slowly Changing Adversarial Bandit Algorithms are .pdf:application/pdf},
}


@inproceedings{lattimore_asymptotically_2011,
	address = {Berlin, Heidelberg},
	series = {{ALT}'11},
	title = {Asymptotically optimal agents},
	isbn = {978-3-642-24411-7},
	abstract = {Artificial general intelligence aims to create agents capable of learning to solve arbitrary interesting problems. We define two versions of asymptotic optimality and prove that no agent can satisfy the strong version while in some cases, depending on discounting, there does exist a non-computable weak asymptotically optimal agent.},
	urldate = {2024-11-19},
	booktitle = {Proceedings of the 22nd international conference on {Algorithmic} learning theory},
	publisher = {Springer-Verlag},
	author = {Lattimore, Tor and Hutter, Marcus},
	month = oct,
	year = {2011},
	pages = {368--382},
}


@article{he2021nearly,
  title={Nearly minimax optimal reinforcement learning for discounted {MDPs}},
  author={He, Jiafan and Zhou, Dongruo and Gu, Quanquan},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={22288--22300},
  year={2021}
}

@misc{liu_regret_2021,
	title = {Regret {Bounds} for {Discounted} {MDPs}},
	url = {http://arxiv.org/abs/2002.05138},
	urldate = {2024-11-19},
	publisher = {arXiv},
	author = {Liu, Shuang and Su, Hao},
	month = may,
	year = {2021},
	note = {arXiv:2002.05138},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:/Users/benjamin/Zotero/storage/QVW4QERH/Liu and Su - 2021 - Regret Bounds for Discounted MDPs.pdf:application/pdf;Snapshot:/Users/benjamin/Zotero/storage/QXCVI5VX/2002.html:text/html},
}


@inproceedings{block2022smoothed,
  title={Smoothed online learning is as easy as statistical learning},
  author={Block, Adam and Dagan, Yuval and Golowich, Noah and Rakhlin, Alexander},
  booktitle={Conference on Learning Theory},
  pages={1716--1786},
  year={2022},
  organization={PMLR}
}

@article{hanneke2014active,
  title={Theory of disagreement-based active learning},
  author={Hanneke, Steve and others},
  journal={Foundations and Trends{\textregistered} in Machine Learning},
  volume={7},
  number={2-3},
  pages={131--309},
  year={2014},
  publisher={Now Publishers, Inc.}
}

@article{hendrycks2023overviewcatastrophicairisks,
  title={An overview of catastrophic {AI} risks},
  author={Hendrycks, Dan and Mazeika, Mantas and Woodside, Thomas},
  journal={arXiv preprint arXiv:2306.12001},
  year={2023}
}

@article{critch2023tasra,
  title={{TASRA}: a taxonomy and analysis of societal-scale risks from {AI}},
  author={Critch, Andrew and Russell, Stuart},
  journal={arXiv preprint arXiv:2306.06924},
  year={2023}
}

@article{di2019should,
  title={Should we be afraid of medical {AI}?},
  author={Di Nucci, Ezio},
  journal={Journal of Medical Ethics},
  volume={45},
  number={8},
  pages={556--558},
  year={2019},
  publisher={Institute of Medical Ethics}
}

@article{freund1997decision,
  title={A decision-theoretic generalization of on-line learning and an application to boosting},
  author={Freund, Yoav and Schapire, Robert E},
  journal={Journal of computer and system sciences},
  volume={55},
  number={1},
  pages={119--139},
  year={1997},
  publisher={Elsevier}
}

@inproceedings{kohli2020enabling,
  title={Enabling pedestrian safety using computer vision techniques: A case study of the 2018 uber inc. self-driving car crash},
  author={Kohli, Puneet and Chadha, Anjali},
  booktitle={Advances in Information and Communication: Proceedings of the 2019 Future of Information and Communication Conference (FICC), Volume 1},
  pages={261--279},
  year={2020},
  organization={Springer}
}

@article{villasenor2020artificial,
  title={Artificial Intelligence, Due Process and Criminal Sentencing},
  author={Villasenor, John and Foggo, Virginia},
  journal={Mich. St. L. Rev.},
  pages={295},
  year={2020},
  publisher={HeinOnline}
}

@article{guembe_emerging_2022,
	title = {The {Emerging} {Threat} of {Ai}-driven {Cyber} {Attacks}: {A} {Review}},
	volume = {36},
	issn = {0883-9514, 1087-6545},
	shorttitle = {The {Emerging} {Threat} of {Ai}-driven {Cyber} {Attacks}},
	url = {https://www.tandfonline.com/doi/full/10.1080/08839514.2022.2037254},
	doi = {10.1080/08839514.2022.2037254},
	language = {en},
	number = {1},
	urldate = {2024-09-04},
	journal = {Applied Artificial Intelligence},
	author = {Guembe, Blessing and Azeta, Ambrose and Misra, Sanjay and Osamor, Victor Chukwudi and Fernandez-Sanz, Luis and Pospelova, Vera},
	month = dec,
	year = {2022},
}

@article{slivkins2019introduction,
  title={Introduction to multi-armed bandits},
  author={Slivkins, Aleksandrs and others},
  journal={Foundations and Trends{\textregistered} in Machine Learning},
  volume={12},
  number={1-2},
  pages={1--286},
  year={2019},
  publisher={Now Publishers, Inc.}
}

@article{littlestone1988learning,
  title={Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm},
  author={Littlestone, Nick},
  journal={Machine learning},
  volume={2},
  pages={285--318},
  year={1988},
  publisher={Springer}
}

@article{de2024biosecurity,
  title={Biosecurity Risk Assessment for the Use of Artificial Intelligence in Synthetic Biology},
  author={De Haro, Leyma P},
  journal={Applied Biosafety},
  year={2024},
  publisher={Mary Ann Liebert, Inc., publishers 140 Huguenot Street, 3rd Floor New~…}
}

@techreport{mouton2024operational,
  title={The operational risks of AI in large-scale biological attacks},
  author={Mouton, C and Lucas, Caleb and Guest, ELLA},
  year={2024},
  institution={RAND Corporation, Santa Monica}
}

@Inbook{Abaimov2020,
author="Abaimov, Stanislav
and Martellini, Maurizio",
editor="Martellini, Maurizio
and Trapp, Ralf",
title="Artificial Intelligence in Autonomous Weapon Systems",
bookTitle="21st Century Prometheus: Managing CBRN Safety and Security Affected by Cutting-Edge Technologies",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="141--177",
isbn="978-3-030-28285-1",
doi="10.1007/978-3-030-28285-1_8",
url="https://doi.org/10.1007/978-3-030-28285-1_8"
}

@article{oxford2024california,
  title={California’s {AI} bill promises more ethical oversight},
  author={Oxford Analytica},
  journal={Emerald Expert Briefings},
  number={oxan-es},
  year={2024},
  publisher={Oxford Analytica}
}

@book{candela2009dataset,
author = {Quionero-Candela, Joaquin and Sugiyama, Masashi and Schwaighofer, Anton and Lawrence, Neil D.},
title = {Dataset Shift in Machine Learning},
year = {2009},
isbn = {0262170051},
publisher = {The MIT Press},
}

@article{shah2022goal,
  title={Goal misgeneralization: Why correct specifications aren't enough for correct goals},
  author={Shah, Rohin and Varma, Vikrant and Kumar, Ramana and Phuong, Mary and Krakovna, Victoria and Uesato, Jonathan and Kenton, Zac},
  journal={arXiv preprint arXiv:2210.01790},
  year={2022}
}

@inproceedings{langosco2022goal,
  title={Goal misgeneralization in deep reinforcement learning},
  author={Lauro Langosco and Koch, Jack and Sharkey, Lee D and Pfau, Jacob and Krueger, David},
  booktitle={International Conference on Machine Learning},
  pages={12004--12019},
  year={2022},
  organization={PMLR}
}

@article{russo2024online,
  title={Online Learning with Sublinear Best-Action Queries},
  author={Russo, Matteo and Celli, Andrea and Baldeschi, Riccardo Colini and Fusco, Federico and Haimovich, Daniel and Karamshuk, Dima and Leonardi, Stefano and Tax, Niek},
  journal={arXiv preprint arXiv:2407.16355},
  year={2024}
}

@article{yang2024generalized,
  title={Generalized out-of-distribution detection: A survey},
  author={Yang, Jingkang and Zhou, Kaiyang and Li, Yixuan and Liu, Ziwei},
  journal={International Journal of Computer Vision},
  pages={1--28},
  year={2024},
  publisher={Springer}
}

@book{cesa2006prediction,
  title={Prediction, learning, and games},
  author={Cesa-Bianchi, Nicolo and Lugosi, G{\'a}bor},
  year={2006},
  publisher={Cambridge university press}
}

@book{boucheron2013concentration,
    author = {Boucheron, Stéphane and Lugosi, Gábor and Massart, Pascal},
    title = "{Concentration Inequalities: A Nonasymptotic Theory of Independence}",
    publisher = {Oxford University Press},
    year = {2013},
    month = {02},
    isbn = {9780199535255},
    doi = {10.1093/acprof:oso/9780199535255.001.0001},
    url = {https://doi.org/10.1093/acprof:oso/9780199535255.001.0001},
}




@article{haussler1995sphere,
  title={Sphere packing numbers for subsets of the Boolean n-cube with bounded Vapnik-Chervonenkis dimension},
  author={Haussler, David},
  journal={Journal of Combinatorial Theory, Series A},
  volume={69},
  number={2},
  pages={217--232},
  year={1995},
  publisher={Elsevier}
}

@article{spielman2004smoothed,
  title={Smoothed analysis of algorithms: Why the simplex algorithm usually takes polynomial time},
  author={Spielman, Daniel A and Teng, Shang-Hua},
  journal={Journal of the ACM (JACM)},
  volume={51},
  number={3},
  pages={385--463},
  year={2004},
  publisher={ACM New York, NY, USA}
}

@phdthesis{haghtalab2018foundation,
  title={Foundation of Machine Learning, by the People, for the People},
  author={Haghtalab, Nika},
  year={2018},
  school={Microsoft Research}
}

@article{haghtalab2024smoothed,
  title={Smoothed analysis with adaptive adversaries},
  author={Haghtalab, Nika and Roughgarden, Tim and Shetty, Abhishek},
  journal={Journal of the ACM},
  volume={71},
  number={3},
  pages={1--34},
  year={2024},
  publisher={ACM New York, NY}
}

@article{Jung1901,
author = {Jung, Heinrich},
journal = {Journal für die reine und angewandte Mathematik},
pages = {241-257},
title = {Ueber die kleinste Kugel, die eine räumliche Figur einschliesst.},
url = {http://eudml.org/doc/149122},
volume = {123},
year = {1901},
}

@article{gonzalez2012successive,
  title={Successive radii and Minkowski addition},
  author={Gonz{\'a}lez, Bernardo and Hern{\'a}ndez Cifre, Mar{\'\i}a A},
  journal={Monatshefte f{\"u}r Mathematik},
  volume={166},
  number={3},
  pages={395--409},
  year={2012},
  publisher={Springer}
}

@book{schneider2013convex,
  title={Convex bodies: the Brunn--Minkowski theory},
  author={Schneider, Rolf},
  volume={151},
  year={2013},
  publisher={Cambridge university press}
}

@article{weilcourse,
  title={A Course on Convex Geometry},
  author={Weil, Wolfgang}
}


@article{gu_review_2024,
	title = {A Review of Safe Reinforcement Learning: Methods, Theories, and Applications},
	volume = {46},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2024.3457538},
	shorttitle = {A Review of Safe Reinforcement Learning},
	pages = {11216--11235},
	number = {12},
	journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	author = {Gu, Shangding and Yang, Long and Du, Yali and Chen, Guang and Walter, Florian and Wang, Jun and Knoll, Alois},
	urldate = {2025-01-24},
	date = {2024-12},
        year = [2024],
	note = {Conference Name: {IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Benchmark testing, Complexity theory, constrained Markov decision processes, Costs, Optimization, Reviews, Robots, Safe reinforcement learning ({RL}), Safety, safety optimisation, safety problems},
}


@inproceedings{zhao_state-wise_2023,
	title = {State-wise {Safe} {Reinforcement} {Learning}: {A} {Survey}},
	volume = {6},
	shorttitle = {State-wise {Safe} {Reinforcement} {Learning}},
	url = {https://www.ijcai.org/proceedings/2023/763},
	doi = {10.24963/ijcai.2023/763},
	abstract = {Electronic proceedings of IJCAI 2023},
	language = {en},
	urldate = {2024-05-11},
	author = {Zhao, Weiye and He, Tairan and Chen, Rui and Wei, Tianhao and Liu, Changliu},
	month = aug,
	year = {2023},
	note = {ISSN: 1045-0823},
	pages = {6814--6822},
}

@inproceedings{ding_natural_2020,
	title = {Natural {Policy} {Gradient} {Primal}-{Dual} {Method} for {Constrained} {Markov} {Decision} {Processes}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/5f7695debd8cde8db5abcb9f161b49ea-Abstract.html},
	urldate = {2024-05-11},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Ding, Dongsheng and Zhang, Kaiqing and Basar, Tamer and Jovanovic, Mihailo},
	year = {2020},
	pages = {8378--8390},
}

@inproceedings{chen_learning_2022,
	title = {Learning {Infinite}-horizon {Average}-reward {Markov} {Decision} {Process} with {Constraints}},
	url = {https://proceedings.mlr.press/v162/chen22i.html},
	language = {en},
	urldate = {2024-05-11},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Chen, Liyu and Jain, Rahul and Luo, Haipeng},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {3246--3270},
}

@misc{efroni_exploration-exploitation_2020,
	title = {Exploration-{Exploitation} in {Constrained} {MDPs}},
	url = {http://arxiv.org/abs/2003.02189},
	language = {en},
	urldate = {2024-05-11},
	publisher = {arXiv},
	author = {Efroni, Yonathan and Mannor, Shie and Pirotta, Matteo},
	month = mar,
	year = {2020},
	note = {arXiv:2003.02189 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}


@inproceedings{barman_fairness_2023,
	series = {{AAAI}'23/{IAAI}'23/{EAAI}'23},
	title = {Fairness and welfare quantification for regret in multi-armed bandits},
	volume = {37},
	isbn = {978-1-57735-880-0},
	url = {https://doi.org/10.1609/aaai.v37i6.25829},
	doi = {10.1609/aaai.v37i6.25829},
	urldate = {2024-05-10},
	booktitle = {Proceedings of the {Thirty}-{Seventh} {AAAI} {Conference} on {Artificial} {Intelligence} and {Thirty}-{Fifth} {Conference} on {Innovative} {Applications} of {Artificial} {Intelligence} and {Thirteenth} {Symposium} on {Educational} {Advances} in {Artificial} {Intelligence}},
	publisher = {AAAI Press},
	author = {Barman, Siddharth and Khan, Arindam and Maiti, Arnab and Sawarni, Ayush},
	month = feb,
	year = {2023},
	pages = {6762--6769},
}

@article{natarajan_learning_1989,
	title = {On learning sets and functions},
	volume = {4},
	issn = {1573-0565},
	url = {https://doi.org/10.1007/BF00114804},
	doi = {10.1007/BF00114804},
	language = {en},
	number = {1},
	urldate = {2024-05-06},
	journal = {Machine Learning},
	author = {Natarajan, B. K.},
	month = oct,
	year = {1989},
	keywords = {connectionist networks, learning functions, Learning sets, probabilistic analysis},
	pages = {67--97},
}

@article{haussler_generalization_1995,
	title = {A generalization of {Sauer}'s lemma},
	volume = {71},
	issn = {0097-3165},
	url = {https://www.sciencedirect.com/science/article/pii/0097316595900012},
	doi = {10.1016/0097-3165(95)90001-2},
	number = {2},
	urldate = {2024-05-06},
	journal = {Journal of Combinatorial Theory, Series A},
	author = {Haussler, David and Long, Philip M},
	month = aug,
	year = {1995},
	pages = {219--240},
}

@book{shalev-shwartz_understanding_2014,
	edition = {1},
	title = {Understanding {Machine} {Learning}: {From} {Theory} to {Algorithms}},
	copyright = {https://www.cambridge.org/core/terms},
	isbn = {978-1-107-05713-5 978-1-107-29801-9},
	shorttitle = {Understanding {Machine} {Learning}},
	url = {https://www.cambridge.org/core/product/identifier/9781107298019/type/book},
	language = {en},
	urldate = {2024-05-06},
	publisher = {Cambridge University Press},
	author = {Shalev-Shwartz, Shai and Ben-David, Shai},
	month = may,
	year = {2014},
	doi = {10.1017/CBO9781107298019},
}

@article{fan_hoeffdings_2012,
	title = {Hoeffding’s inequality for supermartingales},
	volume = {122},
	issn = {0304-4149},
	url = {https://www.sciencedirect.com/science/article/pii/S0304414912001378},
	doi = {10.1016/j.spa.2012.06.009},
	number = {10},
	urldate = {2024-05-06},
	journal = {Stochastic Processes and their Applications},
	author = {Fan, Xiequan and Grama, Ion and Liu, Quansheng},
	month = oct,
	year = {2012},
	keywords = {Bennett’s inequality, Concentration inequalities, Freedman’s inequality, Hoeffding’s inequality, Martingales, Supermartingales},
	pages = {3545--3559},
}

@book{wu_lecture_2020,
	title = {Lecture notes on: {Information}-theoretic methods for high-dimensional statistics},
	language = {en},
	author = {Wu, Yihong},
        year = {2020},
}

@inproceedings{zhu_provably_2023,
	title = {Provably {Efficient} {Reinforcement} {Learning} via {Surprise} {Bound}},
	booktitle = {International {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Zhu, Hanlin and Wang, Ruosong and Lee, Jason},
	year = {2023},
	pages = {4006--4032},
}

@misc{wang_reinforcement_2020,
	title = {Reinforcement {Learning} with {General} {Value} {Function} {Approximation}: {Provably} {Efficient} {Approach} via {Bounded} {Eluder} {Dimension}},
	author = {Wang, Ruosong and Salakhutdinov, Ruslan and Yang, Lin F.},
	year = {2020},
	note = {\_eprint: 2005.10804},
}

@article{foster_statistical_2021,
	title = {The statistical complexity of interactive decision making},
	journal = {arXiv preprint arXiv:2112.13487},
	author = {Foster, Dylan J and Kakade, Sham M and Qian, Jian and Rakhlin, Alexander},
	year = {2021},
}

@article{feng_rethinking_2023,
	title = {Rethinking {Model}-based, {Policy}-based, and {Value}-based {Reinforcement} {Learning} via the {Lens} of {Representation} {Complexity}},
	journal = {arXiv preprint arXiv:2312.17248},
	author = {Feng, Guhao and Zhong, Han},
	year = {2023},
}

@inproceedings{dong_expressivity_2020,
	title = {On the expressivity of neural networks for deep reinforcement learning},
	booktitle = {International conference on machine learning},
	publisher = {PMLR},
	author = {Dong, Kefan and Luo, Yuping and Yu, Tianhe and Finn, Chelsea and Ma, Tengyu},
	year = {2020},
	pages = {2627--2637},
}

@article{zhu_representation_2023,
	title = {On {Representation} {Complexity} of {Model}-based and {Model}-free {Reinforcement} {Learning}},
	journal = {arXiv preprint arXiv:2310.01706},
	author = {Zhu, Hanlin and Huang, Baihe and Russell, Stuart},
	year = {2023},
}

@article{russo_eluder_2013,
	title = {Eluder dimension and the sample complexity of optimistic exploration},
	volume = {26},
	journal = {Advances in Neural Information Processing Systems},
	author = {Russo, Daniel and Van Roy, Benjamin},
	year = {2013},
}

@inproceedings{foster_practical_2018,
	title = {Practical contextual bandits with regression oracles},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Foster, Dylan and Agarwal, Alekh and Dudik, Miroslav and Luo, Haipeng and Schapire, Robert},
	year = {2018},
	pages = {1539--1548},
}

@inproceedings{moldovan_safe_2012,
	address = {Madison, WI, USA},
	series = {{ICML}'12},
	title = {Safe exploration in {Markov} decision processes},
	isbn = {978-1-4503-1285-1},
	abstract = {In environments with uncertain dynamics exploration is necessary to learn how to perform well. Existing reinforcement learning algorithms provide strong exploration guarantees, but they tend to rely on an ergodicity assumption. The essence of ergodicity is that any state is eventually reachable from any other state by following a suitable policy. This assumption allows for exploration algorithms that operate by simply favoring states that have rarely been visited before. For most physical systems this assumption is impractical as the systems would break before any reasonable exploration has taken place, i.e., most physical systems don't satisfy the ergodicity assumption. In this paper we address the need for safe exploration methods in Markov decision processes. We first propose a general formulation of safety through ergodicity. We show that imposing safety by restricting attention to the resulting set of guaranteed safe policies is NP-hard. We then present an efficient algorithm for guaranteed safe, but potentially suboptimal, exploration. At the core is an optimization formulation in which the constraints restrict attention to a subset of the guaranteed safe policies and the objective favors exploration policies. Our framework is compatible with the majority of previously proposed exploration methods, which rely on an exploration bonus. Our experiments, which include a Martian terrain exploration problem, show that our method is able to explore better than classical exploration methods.},
	urldate = {2024-01-23},
	booktitle = {Proceedings of the 29th {International} {Coference} on {International} {Conference} on {Machine} {Learning}},
	publisher = {Omnipress},
	author = {Moldovan, Teodor Mihai and Abbeel, Pieter},
	month = jun,
	year = {2012},
	pages = {1451--1458},
}

@article{vapnik_uniform_1971,
	title = {On the {Uniform} {Convergence} of {Relative} {Frequencies} of {Events} to {Their} {Probabilities}},
	volume = {16},
	issn = {0040-585X},
	url = {https://epubs.siam.org/doi/10.1137/1116025},
	doi = {10.1137/1116025},
	number = {2},
	urldate = {2024-01-14},
	journal = {Theory of Probability \& Its Applications},
	author = {Vapnik, V. N. and Chervonenkis, A. Ya.},
	month = jan,
	year = {1971},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	pages = {264--280},
}

@inproceedings{turchetta_safe_2016,
	title = {Safe {Exploration} in {Finite} {Markov} {Decision} {Processes} with {Gaussian} {Processes}},
	volume = {29},
	url = {https://proceedings.neurips.cc/paper/2016/hash/9a49a25d845a483fae4be7e341368e36-Abstract.html},
	abstract = {In classical reinforcement learning agents accept arbitrary short term loss for long term gain when exploring their environment. This is infeasible for safety critical applications such as robotics, where even a single unsafe action may cause system failure or harm the environment. In this paper, we address the problem of safely exploring finite Markov decision processes (MDP). We define safety in terms of an a priori unknown safety constraint that depends on states and actions and satisfies certain regularity conditions expressed via a Gaussian process prior. We develop a novel algorithm, SAFEMDP, for this task and prove that it completely explores the safely reachable part of the MDP without violating the safety constraint. To achieve this, it cautiously explores safe states and actions in order to gain statistical confidence about the safety of unvisited state-action pairs from noisy observations collected while navigating the environment. Moreover, the algorithm explicitly considers reachability when exploring the MDP, ensuring that it does not get stuck in any state with no safe way out. We demonstrate our method on digital terrain models for the task of exploring an unknown map with a rover.},
	urldate = {2024-01-13},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Turchetta, Matteo and Berkenkamp, Felix and Krause, Andreas},
	year = {2016},
}

@inproceedings{slivkins_contextual_2011,
	title = {Contextual {Bandits} with {Similarity} {Information}},
	url = {https://proceedings.mlr.press/v19/slivkins11a.html},
	abstract = {In a multi-armed bandit (MAB) problem, an online algorithm makes a sequence of choices. In each round it chooses from a time-invariant set of alternatives and receives the payoff associated with this alternative. While the case of small strategy sets is by now well-understood, a lot of recent work has focused on MAB problems with exponentially or infinitely large  strategy sets, where one needs to assume extra structure in order to make the problem tractable. In particular, recent literature considered information on similarity between arms. We consider similarity information in the setting of contextual bandits, a natural extension of the basic MAB problem where before each round an algorithm is given the context – a hint about the payoffs in this round. Contextual bandits are directly motivated by placing advertisements on webpages, one of the crucial problems in sponsored search. A particularly simple way to represent similarity information in the contextual bandit setting is via a similarity distance between the context-arm pairs which bounds from above the difference between the respective expected payoffs. Prior work on contextual bandits with similarity uses “uniform” partitions of the similarity space, so that each context-arm pair is approximated by the closest pair in the partition. Algorithms based on “uniform” partitions disregard the structure of the payoffs and the context arrivals, which is potentially wasteful. We present algorithms that are based on adaptive partitions, and take advantage of “benign” payoffs and context arrivals without sacrificing the worst-case performance. The central idea is to maintain a finer partition in high-payoff regions of the similarity space and in popular regions of the context space. Our results apply to several other settings, e.g. MAB with constrained temporal change (Slivkins and Upfal, 2008) and sleeping bandits (Kleinberg et al., 2008a).},
	language = {en},
	urldate = {2024-01-13},
	booktitle = {Proceedings of the 24th {Annual} {Conference} on {Learning} {Theory} ({COLT})},
	author = {Slivkins, Aleksandrs},
	month = dec,
	year = {2011},
	note = {ISSN: 1938-7228},
	pages = {679--702},
}

@article{rakhlin_online_2015,
	title = {Online learning via sequential complexities},
	volume = {16},
	issn = {1532-4435},
	abstract = {We consider the problem of sequential prediction and provide tools to study the minimax value of the associated game. Classical statistical learning theory provides several useful complexity measures to study learning with i.i.d. data. Our proposed sequential complexities can be seen as extensions of these measures to the sequential setting. The developed theory is shown to yield precise learning guarantees for the problem of sequential prediction. In particular, we show necessary and sufficient conditions for online learnability in the setting of supervised learning. Several examples show the utility of our framework: we can establish learnability without having to exhibit an explicit online learning algorithm.},
	number = {1},
	journal = {The Journal of Machine Learning Research},
	author = {Rakhlin, Alexander and Sridharan, Karthik and Tewari, Ambuj},
	month = jan,
	year = {2015},
	keywords = {online learning, regret minimization, sequential complexities},
	pages = {155--186},
}

@article{peters_evaluating_2016,
	title = {Evaluating gambles using dynamics},
	volume = {26},
	issn = {1054-1500},
	url = {https://doi.org/10.1063/1.4940236},
	doi = {10.1063/1.4940236},
	abstract = {Gambles are random variables that model possible changes in wealth. Classic decision theory transforms money into utility through a utility function and defines the value of a gamble as the expectation value of utility changes. Utility functions aim to capture individual psychological characteristics, but their generality limits predictive power. Expectation value maximizers are defined as rational in economics, but expectation values are only meaningful in the presence of ensembles or in systems with ergodic properties, whereas decision-makers have no access to ensembles, and the variables representing wealth in the usual growth models do not have the relevant ergodic properties. Simultaneously addressing the shortcomings of utility and those of expectations, we propose to evaluate gambles by averaging wealth growth over time. No utility function is needed, but a dynamic must be specified to compute time averages. Linear and logarithmic “utility functions” appear as transformations that generate ergodic observables for purely additive and purely multiplicative dynamics, respectively. We highlight inconsistencies throughout the development of decision theory, whose correction clarifies that our perspective is legitimate. These invalidate a commonly cited argument for bounded utility functions.},
	number = {2},
	urldate = {2024-01-16},
	journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
	author = {Peters, O. and Gell-Mann, M.},
	month = feb,
	year = {2016},
	pages = {023103},
}

@article{nash_bargaining_1950,
	title = {The {Bargaining} {Problem}},
	volume = {18},
	issn = {0012-9682},
	url = {https://www.jstor.org/stable/1907266},
	doi = {10.2307/1907266},
	abstract = {A new treatment is presented of a classical economic problem, one which occurs in many forms, as bargaining, bilateral monopoly, etc. It may also be regarded as a nonzero-sum two-person game. In this treatment a few general assumptions are made concerning the behavior of a single individual and of a group of two individuals in certain economic environments. From these, the solution (in the sense of this paper) of the classical problem may be obtained. In the terms of game theory, values are found for the game.},
	number = {2},
	urldate = {2024-01-16},
	journal = {Econometrica},
	author = {Nash, John F.},
	year = {1950},
	note = {Publisher: [Wiley, Econometric Society]},
	pages = {155--162},
}

@inproceedings{mindermann_active_2018,
	title = {Active {Inverse} {Reward} {Design}},
	url = {http://arxiv.org/abs/1809.03060},
	abstract = {Designers of {AI} agents often iterate on the reward function in a trial-and-error process until they get the desired behavior, but this only guarantees good behavior in the training environment. We propose structuring this process as a series of queries asking the user to compare between different reward functions. Thus we can actively select queries for maximum informativeness about the true reward. In contrast to approaches asking the designer for optimal behavior, this allows us to gather additional information by eliciting preferences between suboptimal behaviors. After each query, we need to update the posterior over the true reward function from observing the proxy reward function chosen by the designer. The recently proposed Inverse Reward Design (IRD) enables this. Our approach substantially outperforms IRD in test environments. In particular, it can query the designer about interpretable, linear reward functions and still infer non-linear ones.},
	urldate = {2024-01-15},
	booktitle = {Proceedings of the 1st {Workshop} on {Goal} {Specifications} for {Reinforcement} {Learning}},
	author = {Mindermann, Sören and Shah, Rohin and Gleave, Adam and Hadfield-Menell, Dylan},
	year = {2018},
	note = {arXiv:1809.03060 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@unpublished{maillard_active_2019,
	title = {Active {Roll}-outs in {MDP} with {Irreversible} {Dynamics}},
	url = {https://hal.science/hal-02177808},
	abstract = {In Reinforcement Learning (RL), regret guarantees scaling with the square root of the time horizon have been shown to hold only for communicating Markov decision processes (MDPs) where any two states are connected. This essentially means that an algorithm can eventually recover from any mistake. However, real-world tasks usually include situations where taking a single "bad" action can permanently trap a learner in a suboptimal region of the state-space. Since it is provably impossible to achieve sub-linear regret in general multi-chain MDPs, we assume a weak mechanism that allows the learner to request additional information. Our main contribution is to address: (i) how much external information is needed, (ii) how and when to use it, and (iii) how much regret is incurred. We design an algorithm that minimizes requests for external information in the form of rollouts of a policy specified by the learner by actively requesting it only when needed. The algorithm provably achieves O(√ T) active regret after T steps in a large class of multi-chain MDPs, by only requesting O(log(T)) rollout transitions. The superiority of our algorithm to standard algorithms such as R-Max and UCRL is demonstrated in experiments on some illustrative grid-world examples. (a) (b) (c) Figure 1: Example of (a) a communicating MDP, (b) a unichain MDP with a single recurrent class, and (c) a multi-chain MDP with two recurrent classes. The circles represent states while the labeled edges represent transitions due to executing actions \{a, b, c\}.},
	urldate = {2024-01-15},
	author = {Maillard, Odalric-Ambrym and Mann, Timothy and Ortner, Ronald and Mannor, Shie},
	month = jul,
	year = {2019},
	keywords = {MDP, Multi-chain, Recoverability c xxxx Odalric-Ambrym, Regret analysis, Reinforcement learning},
}

@inproceedings{lu_showing_2009,
	title = {Showing relevant ads via context multi-armed bandits},
	url = {http://david.palenica.com/papers/clicks/lipschitz-clicks.pdf},
	urldate = {2024-01-15},
	booktitle = {Proceedings of {AISTATS}},
	author = {Lu, Tyler and Pál, Dávid and Pál, Martin},
	year = {2009},
}

@inproceedings{leike_bad_2015,
	title = {Bad {Universal} {Priors} and {Notions} of {Optimality}},
	url = {https://proceedings.mlr.press/v40/Leike15.html},
	language = {en},
	urldate = {2024-01-15},
	booktitle = {Proceedings of {The} 28th {Conference} on {Learning} {Theory}},
	publisher = {PMLR},
	author = {Leike, Jan and Hutter, Marcus},
	month = jun,
	year = {2015},
	note = {ISSN: 1938-7228},
	pages = {1244--1259},
}

@inproceedings{krishnamurthy_contextual_2019,
	title = {Contextual bandits with continuous actions: {Smoothing}, zooming, and adapting},
	shorttitle = {Contextual bandits with continuous actions},
	url = {https://proceedings.mlr.press/v99/krishnamurthy19a.html},
	abstract = {We study contextual bandit learning for any competitor policy class and continuous action space.  We obtain two qualitatively different regret bounds: one competes with a smoothed version of the policy class under no continuity assumptions, while the other requires standard Lipschitz assumptions. Both bounds exhibit data-dependent “zooming" behavior and, with no tuning, yield improved guarantees for benign problems. We also study adapting to unknown smoothness parameters, establishing a price-of-adaptivity and deriving optimal adaptive algorithms that require no additional information.},
	language = {en},
	urldate = {2024-01-16},
	booktitle = {Proceedings of the {Thirty}-{Second} {Conference} on {Learning} {Theory}},
	publisher = {PMLR},
	author = {Krishnamurthy, Akshay and Langford, John and Slivkins, Aleksandrs and Zhang, Chicheng},
	month = jun,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {2025--2027},
}

@inproceedings{kosoy_delegative_2019,
	title = {Delegative {Reinforcement} {Learning}: learning to avoid traps with a little help},
	shorttitle = {Delegative {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1907.08461},
	doi = {10.48550/arXiv.1907.08461},
	abstract = {Most known regret bounds for reinforcement learning are either episodic or assume an environment without traps. We derive a regret bound without making either assumption, by allowing the algorithm to occasionally delegate an action to an external advisor. We thus arrive at a setting of active one-shot model-based reinforcement learning that we call DRL (delegative reinforcement learning.) The algorithm we construct in order to demonstrate the regret bound is a variant of Posterior Sampling Reinforcement Learning supplemented by a subroutine that decides which actions should be delegated. The algorithm is not anytime, since the parameters must be adjusted according to the target time discount. Currently, our analysis is limited to Markov decision processes with finite numbers of hypotheses, states and actions.},
	urldate = {2024-01-13},
	publisher = {arXiv},
	author = {Kosoy, Vanessa},
	month = jul,
	year = {2019},
	note = {arXiv:1907.08461 [cs, stat]},
	keywords = {68Q32, Computer Science - Machine Learning, I.2.6, Statistics - Machine Learning},
}

@inproceedings{kleinberg_multi-armed_2008,
	address = {Victoria British Columbia Canada},
	title = {Multi-armed bandits in metric spaces},
	isbn = {978-1-60558-047-0},
	url = {https://dl.acm.org/doi/10.1145/1374376.1374475},
	doi = {10.1145/1374376.1374475},
	abstract = {In a multi-armed bandit problem, an online algorithm chooses from a set of strategies in a sequence of n trials so as to maximize the total payoff of the chosen strategies. While the performance of bandit algorithms with a small ﬁnite strategy set is quite well understood, bandit problems with large strategy sets are still a topic of very active investigation, motivated by practical applications such as online auctions and web advertisement. The goal of such research is to identify broad and natural classes of strategy sets and payoff functions which enable the design of efﬁcient solutions.},
	language = {en},
	urldate = {2024-01-13},
	booktitle = {Proceedings of the fortieth annual {ACM} symposium on {Theory} of computing},
	publisher = {ACM},
	author = {Kleinberg, Robert and Slivkins, Aleksandrs and Upfal, Eli},
	month = may,
	year = {2008},
	pages = {681--690},
}

@article{kearns_efficient_1994,
	title = {Efficient distribution-free learning of probabilistic concepts},
	volume = {48},
	issn = {0022-0000},
	url = {https://www.sciencedirect.com/science/article/pii/S0022000005800625},
	doi = {10.1016/S0022-0000(05)80062-5},
	abstract = {In this paper we investigate a new formal model of machine learning in which the concept (Boolean function) to be learned may exhibit uncertain or probabilistic behavior—thus, the same input may sometimes be classified as a positive example and sometimes as a negative example. Such probabilistic concepts (or p-concepts) may arise in situations such as weather prediction, where the measured variables and their accuracy are insufficient to determine the outcome with certainty. We adopt from the Valiant model of learining [28] the demands that learning algorithms be efficient and general in the sense that they perform well for a wide class of p-concepts and for any distribution over the domain. In addition to giving many efficient algorithms for learning natural classes of p-concepts, we study and develop in detail an underlying theory of learning p-concepts.},
	number = {3},
	urldate = {2024-01-14},
	journal = {Journal of Computer and System Sciences},
	author = {Kearns, Michael J. and Schapire, Robert E.},
	month = jun,
	year = {1994},
	pages = {464--497},
}

@article{kaneko_nash_1979,
	title = {The {Nash} {Social} {Welfare} {Function}},
	volume = {47},
	issn = {0012-9682},
	url = {https://www.jstor.org/stable/1914191},
	doi = {10.2307/1914191},
	abstract = {We investigate the notion of the Nash social welfare function and make a fundamental assumption that there exists a distinguished alternative called an origin, which represents one of the worst states for all individuals in the society. Under this assumption, in Sections 1 and 2, we formulate several rationality criteria that a reasonable social welfare function should satisfy. Then we introduce the Nash social welfare function and the Nash social welfare indices which are the images of the welfare function. The function is proved to satisfy the criteria. In Section 3 it is shown that the Nash social welfare function is the unique social welfare function that satisfies the criteria. Then, in Section 4, we examine two examples which display plausibility of the welfare function.},
	number = {2},
	urldate = {2024-01-16},
	journal = {Econometrica},
	author = {Kaneko, Mamoru and Nakamura, Kenjiro},
	year = {1979},
	note = {Publisher: [Wiley, Econometric Society]},
	pages = {423--435},
}

@article{jaksch_near-optimal_2010,
	title = {Near-optimal {Regret} {Bounds} for {Reinforcement} {Learning}},
	volume = {11},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v11/jaksch10a.html},
	number = {51},
	urldate = {2024-01-13},
	journal = {Journal of Machine Learning Research},
	author = {Jaksch, Thomas and Ortner, Ronald and Auer, Peter},
	year = {2010},
	pages = {1563--1600},
}

@book{hutter_universal_2004,
	title = {Universal {Artificial} {Intelligence}: {Sequential} {Decisions} {Based} on {Algorithmic} {Probability}},
	isbn = {978-3-540-22139-5},
	shorttitle = {Universal {Artificial} {Intelligence}},
	language = {en},
	publisher = {Springer Science \& Business Media},
	author = {Hutter, Marcus},
	month = oct,
	year = {2004},
	note = {Google-Books-ID: NP53iZGt4KUC},
	keywords = {Computers / Computer Science, Computers / Information Theory, Computers / Intelligence (AI) \& Semantics, Computers / Mathematical \& Statistical Software, Computers / Software Development \& Engineering / General, Language Arts \& Disciplines / Library \& Information Science / General, Mathematics / Discrete Mathematics, Mathematics / Logic},
}

@inproceedings{hadfield-menell_inverse_2017,
	address = {Red Hook, NY, USA},
	series = {{NIPS}'17},
	title = {Inverse reward design},
	isbn = {978-1-5108-6096-4},
	urldate = {2024-01-18},
	booktitle = {Proceedings of the 31st {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Hadfield-Menell, Dylan and Milli, Smitha and Abbeel, Pieter and Russell, Stuart and Dragan, Anca D.},
	month = dec,
	year = {2017},
	pages = {6768--6777},
}

@inproceedings{hazan_online_2007,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Online {Learning} with {Prior} {Knowledge}},
	isbn = {978-3-540-72927-3},
	doi = {10.1007/978-3-540-72927-3_36},
	abstract = {The standard so-called experts algorithms are methods for utilizing a given set of “experts” to make good choices in a sequential decision-making problem. In the standard setting of experts algorithms, the decision maker chooses repeatedly in the same “state” based on information about how the different experts would have performed if chosen to be followed. In this paper we seek to extend this framework by introducing state information. More precisely, we extend the framework by allowing an experts algorithm to rely on state information, namely, partial information about the cost function, which is revealed to the decision maker before the latter chooses an action. This extension is very natural in prediction problems. For illustration, an experts algorithm, which is supposed to predict whether the next day will be rainy, can be extended to predicting the same given the current temperature.},
	language = {en},
	booktitle = {Learning {Theory}},
	publisher = {Springer},
	author = {Hazan, Elad and Megiddo, Nimrod},
	editor = {Bshouty, Nader H. and Gentile, Claudio},
	year = {2007},
	keywords = {Decision Maker, Online Algorithm, Online Learn, Side Information, State Information},
	pages = {499--513},
}

@inproceedings{grinsztajn_there_2021,
	title = {There {Is} {No} {Turning} {Back}: {A} {Self}-{Supervised} {Approach} for {Reversibility}-{Aware} {Reinforcement} {Learning}},
	volume = {34},
	shorttitle = {There {Is} {No} {Turning} {Back}},
	url = {https://proceedings.neurips.cc/paper/2021/hash/0e98aeeb54acf612b9eb4e48a269814c-Abstract.html},
	abstract = {We propose to learn to distinguish reversible from irreversible actions for better informed decision-making in Reinforcement Learning (RL). From theoretical considerations, we show that approximate reversibility can be learned through a simple surrogate task: ranking randomly sampled trajectory events in chronological order. Intuitively, pairs of events that are always observed in the same order are likely to be separated by an irreversible sequence of actions. Conveniently, learning the temporal order of events can be done in a fully self-supervised way, which we use to estimate the reversibility of actions from experience, without any priors.We propose two different strategies that incorporate reversibility in RL agents, one strategy for exploration (RAE) and one strategy for control (RAC). We demonstrate the potential of reversibility-aware agents in several environments, including the challenging Sokoban game. In synthetic tasks, we show that we can learn control policies that never fail and reduce to zero the side-effects of interactions, even without access to the reward function.},
	urldate = {2024-01-18},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Grinsztajn, Nathan and Ferret, Johan and Pietquin, Olivier and preux, philippe and Geist, Matthieu},
	year = {2021},
	pages = {1898--1911},
}

@article{garcia_comprehensive_2015,
	title = {A {Comprehensive} {Survey} on {Safe} {Reinforcement} {Learning}},
	volume = {16},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v16/garcia15a.html},
	abstract = {Safe Reinforcement Learning can be defined as the process of learning policies that maximize the expectation of the return in problems in which it is important to ensure reasonable system performance and/or respect safety constraints during the learning and/or deployment processes. We categorize and analyze two approaches of Safe Reinforcement Learning. The first is based on the modification of the optimality criterion, the classic discounted finite/infinite horizon, with a safety factor. The second is based on the modification of the exploration process through the incorporation of external knowledge or the guidance of a risk metric. We use the proposed classification to survey the existing literature, as well as suggesting future directions for Safe Reinforcement Learning.},
	number = {42},
	urldate = {2024-01-13},
	journal = {Journal of Machine Learning Research},
	author = {García, Javier and Fern and Fernández, O},
	year = {2015},
	pages = {1437--1480},
}

@inproceedings{cohen_pessimism_2020,
	title = {Pessimism {About} {Unknown} {Unknowns} {Inspires} {Conservatism}},
	url = {https://proceedings.mlr.press/v125/cohen20a.html},
	abstract = {If we could define the set of all bad outcomes, we could hard-code an agent which avoids them; however, in sufficiently complex environments, this is infeasible. We do not know of any general-purpose approaches in the literature to avoiding novel failure modes. Motivated by this, we define an idealized Bayesian reinforcement learner which follows a policy that maximizes the worst-case expected reward over a set of world-models. We call this agent pessimistic, since it optimizes assuming the worst case. A scalar parameter tunes the agent’s pessimism by changing the size of the set of world-models taken into account. Our first main contribution is: given an assumption about the agent’s model class, a sufficiently pessimistic agent does not cause “unprecedented events” with probability \$1-{\textbackslash}delta\$, whether or not designers know how to precisely specify those precedents they are concerned with. Since pessimism discourages exploration, at each timestep, the agent may defer to a mentor, who may be a human or some known-safe policy we would like to improve. Our other main contribution is that the agent’s policy’s value approaches at least that of the mentor, while the probability of deferring to the mentor goes to 0. In high-stakes environments, we might like advanced artificial agents to pursue goals cautiously, which is a non-trivial problem even if the agent were allowed arbitrary computing power; we present a formal solution.},
	language = {en},
	urldate = {2024-01-13},
	booktitle = {Proceedings of {Thirty} {Third} {Conference} on {Learning} {Theory}},
	publisher = {PMLR},
	author = {Cohen, Michael K. and Hutter, Marcus},
	month = jul,
	year = {2020},
	pages = {1344--1373},
}

@article{cohen_curiosity_2021,
	title = {Curiosity {Killed} or {Incapacitated} the {Cat} and the {Asymptotically} {Optimal} {Agent}},
	volume = {2},
	issn = {2641-8770},
	url = {https://ieeexplore.ieee.org/document/9431093},
	doi = {10.1109/JSAIT.2021.3079722},
	abstract = {Reinforcement learners are agents that learn to pick actions that lead to high reward. Ideally, the value of a reinforcement learner's policy approaches optimality-where the optimal informed policy is the one which maximizes reward. Unfortunately, we show that if an agent is guaranteed to be “asymptotically optimal” in any (stochastically computable) environment, then subject to an assumption about the true environment, this agent will be either “destroyed” or “incapacitated” with probability 1. Much work in reinforcement learning uses an ergodicity assumption to avoid this problem. Often, doing theoretical research under simplifying assumptions prepares us to provide practical solutions even in the absence of those assumptions, but the ergodicity assumption in reinforcement learning may have led us entirely astray in preparing safe and effective exploration strategies for agents in dangerous environments. Rather than assuming away the problem, we present an agent, Mentee, with the modest guarantee of approaching the performance of a mentor, doing safe exploration instead of reckless exploration. Critically, Mentee's exploration probability depends on the expected information gain from exploring. In a simple non-ergodic environment with a weak mentor, we find Mentee outperforms existing asymptotically optimal agents and its mentor.},
	number = {2},
	urldate = {2024-01-13},
	journal = {IEEE Journal on Selected Areas in Information Theory},
	author = {Cohen, Michael K. and Catt, Elliot and Hutter, Marcus},
	month = jun,
	year = {2021},
	note = {Conference Name: IEEE Journal on Selected Areas in Information Theory},
	pages = {665--677},
}

@inproceedings{cesa-bianchi_algorithmic_2017,
	title = {Algorithmic {Chaining} and the {Role} of {Partial} {Feedback} in {Online} {Nonparametric} {Learning}},
	url = {https://proceedings.mlr.press/v65/cesa-bianchi17a.html},
	abstract = {We investigate contextual online learning with nonparametric (Lipschitz) comparison classes under different assumptions on losses and feedback information. For full information feedback and Lipschitz losses, we design the first explicit algorithm achieving the minimax regret rate (up to log factors). In a partial feedback model motivated by second-price auctions, we obtain algorithms for Lipschitz and semi-Lipschitz losses with regret bounds improving on the known bounds for standard bandit feedback. Our analysis combines novel results for contextual second-price auctions with a novel algorithmic approach based on chaining. When the context space is Euclidean, our chaining approach is efficient and delivers an even better regret bound.},
	language = {en},
	urldate = {2024-01-16},
	booktitle = {Proceedings of the 2017 {Conference} on {Learning} {Theory}},
	publisher = {PMLR},
	author = {Cesa-Bianchi, Nicolò and Gaillard, Pierre and Gentile, Claudio and Gerchinovitz, Sébastien},
	month = jun,
	year = {2017},
	note = {ISSN: 2640-3498},
	pages = {465--481},
}

@inproceedings{caragiannis_unreasonable_2016,
	address = {New York, NY, USA},
	series = {{EC} '16},
	title = {The {Unreasonable} {Fairness} of {Maximum} {Nash} {Welfare}},
	isbn = {978-1-4503-3936-0},
	url = {https://doi.org/10.1145/2940716.2940726},
	doi = {10.1145/2940716.2940726},
	abstract = {The maximum Nash welfare (MNW) solution --- which selects an allocation that maximizes the product of utilities --- is known to provide outstanding fairness guarantees when allocating divisible goods. And while it seems to lose its luster when applied to indivisible goods, we show that, in fact, the MNW solution is unexpectedly, strikingly fair even in that setting. In particular, we prove that it selects allocations that are envy free up to one good --- a compelling notion that is quite elusive when coupled with economic efficiency. We also establish that the MNW solution provides a good approximation to another popular (yet possibly infeasible) fairness property, the maximin share guarantee, in theory and --- even more so --- in practice. While finding the MNW solution is computationally hard, we develop a nontrivial implementation, and demonstrate that it scales well on real data. These results lead us to believe that MNW is the ultimate solution for allocating indivisible goods, and underlie its deployment on a popular fair division website.},
	urldate = {2024-01-15},
	booktitle = {Proceedings of the 2016 {ACM} {Conference} on {Economics} and {Computation}},
	publisher = {Association for Computing Machinery},
	author = {Caragiannis, Ioannis and Kurokawa, David and Moulin, Hervé and Procaccia, Ariel D. and Shah, Nisarg and Wang, Junxing},
	month = jul,
	year = {2016},
	keywords = {fair division, nash welfare, resource allocation},
	pages = {305--322},
}

@inproceedings{azar_minimax_2017,
	title = {Minimax Regret Bounds for Reinforcement Learning},
	url = {https://proceedings.mlr.press/v70/azar17a.html},
	language = {en},
	urldate = {2024-01-13},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Azar, Mohammad Gheshlaghi and Osband, Ian and Munos, Rémi},
	month = jul,
	year = {2017},
	pages = {263--272},
}

@article{agarwal_reinforcement_nodate,
	title = {Reinforcement {Learning}: {Theory} and {Algorithms}},
	language = {en},
	author = {Agarwal, Alekh and Jiang, Nan and Kakade, Sham M and Sun, Wen},
}

@misc{betancourt_conceptual_2018,
	title = {A {Conceptual} {Introduction} to {Hamiltonian} {Monte} {Carlo}},
	url = {http://arxiv.org/abs/1701.02434},
	doi = {10.48550/arXiv.1701.02434},
	abstract = {Hamiltonian Monte Carlo has proven a remarkable empirical success, but only recently have we begun to develop a rigorous understanding of why it performs so well on difficult problems and how it is best applied in practice. Unfortunately, that understanding is confined within the mathematics of differential geometry which has limited its dissemination, especially to the applied communities for which it is particularly important. In this review I provide a comprehensive conceptual account of these theoretical foundations, focusing on developing a principled intuition behind the method and its optimal implementations rather of any exhaustive rigor. Whether a practitioner or a statistician, the dedicated reader will acquire a solid grasp of how Hamiltonian Monte Carlo works, when it succeeds, and, perhaps most importantly, when it fails.},
	urldate = {2024-01-15},
	publisher = {arXiv},
	author = {Betancourt, Michael},
	month = jul,
	year = {2018},
	note = {arXiv:1701.02434 [stat]},
	keywords = {Statistics - Methodology},
}

@misc{plaut_avoiding_2024,
	title = {Avoiding {Catastrophe} in {Online} {Learning} by {Asking} for {Help}},
	url = {http://arxiv.org/abs/2402.08062},
	doi = {10.48550/arXiv.2402.08062},
	abstract = {Most learning algorithms with formal regret guarantees assume that no mistake is irreparable and essentially rely on trying all possible behaviors. This approach is problematic when some mistakes are {\textbackslash}emph\{catastrophic\}, i.e., irreparable. We propose an online learning problem where the goal is to minimize the chance of catastrophe. Specifically, we assume that the payoff in each round represents the chance of avoiding catastrophe that round and aim to maximize the product of payoffs (the overall chance of avoiding catastrophe) while allowing a limited number of queries to a mentor. We first show that in general, any algorithm either constantly queries the mentor or is nearly guaranteed to cause catastrophe. However, in settings where the mentor policy class is learnable in the standard online learning model, we provide an algorithm whose regret and rate of querying the mentor both approach 0 as the time horizon grows. Conceptually, if a policy class is learnable in the absence of catastrophic risk, it is learnable in the presence of catastrophic risk if the agent can ask for help.},
	urldate = {2024-10-22},
	publisher = {arXiv},
	author = {Plaut, Benjamin and Zhu, Hanlin and Russell, Stuart},
	month = oct,
	year = {2024},
	note = {arXiv:2402.08062},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/benjamin/Zotero/storage/KCG36R7M/Plaut et al. - 2024 - Avoiding Catastrophe in Online Learning by Asking .pdf:application/pdf;Snapshot:/Users/benjamin/Zotero/storage/ZRTW44GF/2402.html:text/html},
}
