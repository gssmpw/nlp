\section{Proof of \Cref{thm:main}}\label{sec:main-proof}

In \Cref{sec:main}, we stated our main result and provided a proof sketch. Here we provide the formal proof.

Recall the following definitions:
\begin{itemize}
    \item $N_t(s, X) = \Pr[s_{t+1} \in X \mid s_t = s]$
    \item $N_t^m(s, X) = \Pr[s_{t+1}^m \in X \mid s_t^m = s]$
    \item $\alpha_t(X) = \E[N_t^m(s_t, X) - N_t(s_t, X)]$ 
    \item $\Delta_t = \sup_{X\subseteq \s} (p_t^m(X) - p_t(X))$
\end{itemize}

We refer the reader to Chapter 8 of \citet{bertsekas1996stochastic} for a thorough measure-theoretic treatment of MDP theory. In particular, we will not rehash why the assumptions in \Cref{sec:model} are sufficient to ensure that all random variables and expectations are well-defined. However, it is worth proving that $N_t$ and $N_t^m$ are measurable (and thus that $\alpha_t$ is well-defined), since these are not standard functions. Readers unconcerned with measure theory can safely skip \Cref{prop:measure}.

% Before beginning the main proof, we take a moment to ensure that all of the probabilities and expectations we analyze throughout the proof are well-defined. We have tried to strike a balance of covering the measure-theoretic essentials without reinventing the entirety of measure-theoretic MDP theory. Readers unconcerned with measure theory can safely skip \Cref{prop:measure}, while readers who want more details should refer to Chapter 8 of \citet{bertsekas1996stochastic}.

\begin{proposition}
\label{prop:measure}
% For each $t \in [T]$, the random variables $s_t,a_t,q_t$, and $s_t^m$ are well-defined. 
For any fixed $X\subseteq\s$ and any $t \in [T]$, the functions $N_t(\cdot,X) \to [0,1]$ and $N_t^m(\cdot, X) \to [0,1]$ are measurable on $\s$.
\end{proposition}

\begin{proof}
% We show inductively that $s_t,a_t,q_t$, and $s_t^m$ are well-defined. The initial state $s_1 = s_1^m$ is fixed. Suppose that the agent's history $h_t = s_1,a_1,q_1,\dots,s_t$ and the mentor's history $h_t^m = s_1^m,\pi^m(s_1^m),\dots,s_t^m$ are well-defined random variables. By assumption, the agent's choices of $a_t,q_t$ and the mentor's action policy $\pi^m$ are measurable functions of the history,\footnote{In the mentor's case, $\pi^m$ only depends on $s_t^m$, but the statement still holds.} so $a_t,q_t$ and $\pi^m(s_t)$ are well-defined. Since $s_t, a_t, s_t^m,$ and $\pi^m(s_t)$ are well-defined and $P$ is a transition kernel, $s_{t+1}$ and $s_{t+1}^m$ are also well-defined. Thus $h_{t+1} = h_t, a_t,q_t,s_{t+1}$ and $h_{t+1}^m = h_t^m, \pi^m(s_t^m), s_{t+1}^m$ are well-defined, which completes the induction. We conclude that $s_t,a_t,q_t$, and $s_t^m$ are well-defined random variables.

Let $\D(a_t)$ denote the distribution of the random variable $a_t$. By the law of total expectation,
\begin{align*}
N_t(s,X) =&\ \Pr[s_{t+1} \in X \mid s_t = s]\\
=&\ \E_{a \sim \D(a_t)}\big[\E[\bfone(s_{t+1} \in X) \mid s_t = s, a_t=a]\big]\\
=&\ \E_{a \sim \D(a_t)} \big[\E[P(s,a,X)]\big]\\
=&\ \E[P(s,a_t,X)]
\end{align*}
Since $P(\cdot,\cdot,X)$ is measurable, the expectation $\E[P(s,a_t,X)]$ is well-defined and is itself a measurable function on $\s$. Therefore $N_t(\cdot,X) = \E[P(\cdot,a_t,X)]$ is measurable on $\s$.

Since the mentor's action in state $s$ is deterministically $\pi^m(s)$, we have $N_t^m(s, X)= P(s,\pi^m(s),X)$. Thus $N_t^m(\cdot,X)$ is the composition of measurable functions $P(\cdot,\cdot,X)$ and $\pi^m$, so $N_t^m(\cdot,X)$ is also measurable.
\end{proof}

We now proceed to the main proof.

\lemSplit*

\begin{proof}
By the tail sum formula for expected value (see, e.g., Lemma 4.4 in \citet{kallenberg1997foundations}), we have
\begin{align*}
\E[f(s_t^m)] =&\ \int_{h=0}^\infty \Pr[f(s_t^m) > h] \d h\\
\E[f(s_t)] =&\ \int_{h=0}^\infty \Pr[f(s_t) > h] \d h
\end{align*}
Since $f(s) \in [0,1]$ for all $s \in \s$, we can restrict the integrals to $[0,1]$. Next, for each $h \in \bbrpos$, define $X_h = \{s \in \s: f(s) > h\}$. Each $X_h$ is measurable since it is the preimage of a measurable function $f$ on the open set $(h, \infty)$. Therefore
\begin{align*}
\Pr[f(s_t^m) > h] =&\ \Pr[s_t^m \in X_h] = p_t^m(X_h)\\
\Pr[f(s_t) > h] =&\ \Pr[s_t \in X_h] = p_t(X_h)
\end{align*}
and so
\begin{align*}
\E[f(s_t^m) - f(s_t)] =&\ \int_{h=0}^1 \big(\Pr[f(s_t^m) > h] - \Pr[f(s_t) > h]\big) \d h\\
=&\ \int_{h=0}^1 \big(p_t^m(X_h) - p_t(X_h)\big) \d h\\
\le&\ \int_{h=0}^1 \Delta_t \d h\\
=&\ \Delta_t
\end{align*}
\end{proof}


\lemTrajInd*

\begin{proof}
We proceed by induction. Since the agent and mentor have the same initial state, we have $p_1(X) = p_1^m(X)$ for all $X \subseteq \s$, so $\Delta_1 = 0 = \sum_{i=1}^0 \sup_{X\subseteq\s} \alpha_i(X)$. Thus the lemma holds for $t=1$, so assume the lemma holds for some $t \in [T]$.

Fix any $X \subseteq \s$. The next step is to analyze $\E[N_t(s_t, X)]$, which requires a bit of care since $\E[N_t(s_t, X)]$ is an expectation over $s_t$ but $s_t$ also appears in the definition of $N_t(s,X)$. To prevent confusion, we can rewrite $\E[N_t(s_t, X)]$ as $\E_{s\sim p_t} [N_t(s, X)]$, where $s \sim p_t$ indicates that $s$ is sampled from the same distribution as $s_t$. Then
\begin{align*}
\E[N_t(s_t, X)] =&\ \E_{s \sim p_t}[N_t(s, X)]\\
=&\ \E_{s \sim p_t}[\Pr[s_{t+1} \in X \mid s_t = s]] \\
=&\ \E_{s \sim p_t} \left[\E[\bfone(s_{t+1} \in X) \mid s_t = s]\right] 
\end{align*}
By the law of total expectation,
\[
\E_{s \sim p_t} \E[\bfone(s_{t+1} \in X) \mid s_t = s]] = \E[\bfone(s_{t+1} \in X)] = \Pr[s_{t+1} \in X]
\]
Therefore $\E[N_t(s_t, X)] = p_{t+1}(X)$. The same argument can be used to show that $\E[N_t^m(s_t^m, X)] = p_{t+1}^m(X)$ by simply replacing $s_t$ and $p_t$ with $s_t^m$ and $p_t^m$ respectively. Thus
\begin{align*}
p_{t+1}^m(X) - p_{t+1}(X) =&\ \E[N_t^m(s_t^m, X)] - \E[N_t(s_t, X)]\\
=&\ \E[N_t^m(s_t^m, X) - N_t^m(s_t, X)] + \E[N_t^m(s_t, X) - N_t(s_t, X)]\\
=&\ \E[N_t^m(s_t^m, X) - N_t^m(s_t, X)] + \alpha_t(X)
\end{align*}
Next, define $f:\s \to[0,1]$ by $f(s) = N_t^m(s,X)$. \Cref{prop:measure} implies that $f$ is measurable, so \Cref{lem:split} implies that $\E[N_t^m(s_t^m,X) - N_t^m(s_t, X)] \le \Delta_t$, which gives us $p_{t+1}^m(X) - p_{t+1}(X) \le \Delta_t + \alpha_t(X) \le \Delta_t + \sup_{Y\subseteq \s} \alpha_t(Y)$. Since this holds for all $X \subseteq \s$, we have 
\[
\Delta_{t+1} = \sup_{X\subseteq \s} (p_{t+1}^m(X) - p_{t+1}(X)) \le \Delta_t + \sup_{X\subseteq \s} \alpha_t(X)
\]
Combining this with the inductive hypothesis of $\Delta_t \le \sum_{i=1}^{t-1} \sup_{X \subseteq \s} \alpha_i(X)$ gives us
\[
\Delta_{t+1} \le \Delta_t + \sup_{X\subseteq \s} \alpha_t(X) \le \sum_{i=1}^t \sup_{X\subseteq \s} \alpha_t(X)
\]
which completes the induction.
\end{proof}

\lemTraj*

\begin{proof}
Fix an arbitrary sequence of subsets $X_1,\dots,X_{t-1} \subseteq \s$ and define $\bfmu$ as follows:
\[
\mu_i(s,a) =
\begin{cases}
P(s,a,X_i) & \text{ if } i \in [t-1]\\
1 & \text{ otherwise}
\end{cases}
\]
For $i > t-1$ we have $|\mu_i(s, \pi^m(s)) - \mu_i(s, \pi^m(s'))| = 0 \le L\norm{s - s'}$. For for $i \in [t-1]$, local generalization of $P$ implies that
\begin{align*}
|\mu_i(s, \pi^m(s)) - \mu_i(s, \pi^m(s'))| =&\ \big|P\big(s,\pi^m(s), X_i)\big) - P(s,\pi^m(s'),X_i)\big)\big|\\
\le&\ \sup_{V \subseteq \s}\big|P\big(s,\pi^m(s), V\big) - P(s,\pi^m(s'),V)\big)\big|\\
=&\ \tv{P(s, \pi^m(s)) - P(s, \pi^m(s'))}\\
\le&\ L \norm{s-s'}
\end{align*}
Therefore $\bfmu$ satisfies local generalization. Next, we analyze $\mu(s_i, a_i)$. As in the proof of \Cref{lem:trajectories-induction}, let $s \sim p_i$ denote sampling $s$ from the same distribution as $s_i$. Also let $\D(a_i)$ denote the distribution of $a_i$. Then for each $i \in [t-1]$,
\begin{align*}
\E[\mu_i(s_i, a_i)] =&\ \E_{s\sim p_i, a\sim \D(a_i)}[\mu_i(s, a)]\\
=&\ \E_{s\sim p_i, a\sim \D(a_i)}[P(s,a,X_i)] && (\text{Definition of $\mu_i$ for $i \in [t-1]$})\\
=&\ \E_{s\sim p_i, a\sim \D(a_i)}[\Pr[s_{i+1} \in X_i \mid s_i = s, a_i = a]] && (\text{Definition of $P(s,a,X_i)$})\\
=&\ \E_{s\sim p_i, a\sim \D(a_i)}[\E[\bfone(s_{i+1} \in X_i) \mid s_i = s, a_i = a]] && (\text{Expectation of indicator variable})\\
=&\ \E[\bfone(s_{i+1} \in X_i)] && (\text{Law of total expectation})\\
=&\ \E[N_i(s_i, X_i)] && (\text{Proof of \Cref{lem:trajectories-induction}})
\end{align*}
Similarly but more simply,
\begin{align*}
N_i^m(s_i,X_i)=&\ \Pr[s_{i+1}^m \in X_i \mid s_i^m = s_i]\\
=&\ P(s_i, \pi^m(s_i), X_i)]\\
=&\ \mu_i(s_i, \pi^m(s_i))
\end{align*}
Therefore
\begin{align*}
\sum_{i=1}^{t-1} \alpha_i(X_i) =&\ \sum_{i=1}^{t-1} \E\left[N_i^m(s_i, X_i) - N_i(s_i, X_i)\right]\\
=&\ \sum_{i=1}^{t-1} \E[\mu_i(s_i, \pi^m(s_i)) - \mu_i(s_i, a_i)]\\
% =&\ \sum_{i=1}^{t-1} \E[\mu_i(s_i, \pi^m(s_i)) - \mu_i(s_i, a_i)] + \sum_{i=t}^T \E[\mu_i(s_i, \pi^m(s_i)) - \mu_i(s_i, a_i)]\\
=&\ \sum_{i=1}^T \E[\mu_i(s_i, \pi^m(s_i)) - \mu_i(s_i, a_i)]\\
\le&\ \sup_{\M, \pi^m, \bfmu}\ \sum_{i=1}^T \E[\mu_i(s_i, \pi^m(s_i)) - \mu_i(s_i, a_i)]\\
=&\ \rac
\end{align*}
Since this holds for any sequence of subsets $X_1,\dots,X_{t-1} \subseteq \s$, we have
\begin{align*}
\sum_{i=1}^{t-1} \sup_{X\subseteq\s} \alpha_i(X)= \sup_{X_1,\dots,X_{t-1} \subseteq \s} \sum_{i=1}^{t-1} \alpha_i(X_i) \le \rac
\end{align*}
Applying \Cref{lem:trajectories-induction} completes the proof.
\end{proof}

\thmMain*

\begin{proof}
The query bound follows trivially from \Cref{def:ac}, so we proceed to the regret bound. Let $r^m(s) = r(s, \pi^m(s))$ for brevity. Then $r^m: \s \to [0,1]$ is the composition of measurable functions $r$ and $\pi^m$, so $r^m$ is also measurable. Hence \Cref{lem:split} implies that $\E[r^m(s_t^m) - r^m(s_t)] \le \Delta_t$ for all $t \in [T]$. Therefore
\begin{align*}
\E\left[r^m(s_t^m) - r(s_t, a_t)\right] =&\ \E[r^m(s_t^m) - r^m(s_t)] + \E[r^m(s_t) - r(s_t, a_t)]\\
\le&\ \Delta_t +\E[r^m(s_t) - r(s_t, a_t)]
\end{align*}
Lemmas~\ref{lem:action-regret} and \ref{lem:trajectories} respectively imply that $\E \big[\sum_{t=1}^T r^m(s_t) - r(s_t, a_t)\big] \le  \rac$ and $\Delta_t \le \rac$, so\looseness=-1
\begin{align*}
R_T =&\ \E\left[\sum_{t=1}^T r^m(s_t^m) - \sum_{t=1}^T r(s_t, a_t)\right]\\
\le&\ \sum_{t=1}^T \Delta_t +\E \left[\sum_{t=1}^T r^m(s_t) - r(s_t, a_t)\right]\\
\le&\ T \rac +\rac\\
\le&\ (T+1) \rac
\end{align*}
as required.
\end{proof}

\subsection{Less general proof without assuming local generalization for $r$}\label{sec:alt}

In case the of Algorithm 1, we can avoid assuming local generalization for $r$ by using the following result from \citet{plaut_avoiding_2024}, where $M_T = \{t \in [T]: a_t \ne \pi^m(s_t)\}$:

\begin{lemma}[Lemma B.1 in \citet{plaut_avoiding_2024}]
Under the conditions of \Cref{lem:ac-regret}, $\E[|M_T|] \in O(\frac{d}{\sigma} T^\frac{1}{2n+1} \log T)$.
\end{lemma}

Two minor differences exist between the version of Lemma B.1 in \citet{plaut_avoiding_2024} and what we have stated above:
\begin{enumerate}
    \item Lemma B.1 in \citet{plaut_avoiding_2024} is in terms of $\ep$; we have again plugged in $\ep = T^\frac{-2n}{2n+1}$, as discussed above. 
    \item In \citet{plaut_avoiding_2024}, $M_T$ is actually defined as $\{t \in [T]: \pi_t(s_t) \ne \pi^m(s_t)\}$, where $\pi_t(s_t)$ is an action proposed by the algorithm, and the algorithm ends up either taking action $\pi_t(s_t)$ or querying. Clearly if the agent queries then $a_t = \pi^m(s_t)$, so $\{t \in [T]: a_t \ne \pi^m(s_t)\} \subseteq \{t \in [T]: \pi_t(s_t) \ne \pi^m(s_t)\}$. Thus their bound from Lemma B.1 also applies to our slightly more restrictive definition of $M_T$.
\end{enumerate}

\begin{theorem}
\label{thm:no-lg}
Assume $P$ satisfies local generalization, but $r$ does not. Under the conditions of \Cref{lem:ac-regret}, Algorithm 1 from \citet{plaut_avoiding_2024} satisfies $Q_T \in o(T)$ and $R_T \in o(T)$.
\end{theorem}

\begin{proof}
We perform the same analysis as in the original proof to reach $\E[\sum_{t=1}^Tr(s_t^m, \pi^m(s_t^m)) - \sum_{t=1}^T r(s_t, a_t)] \le \sum_{t=1}^T \Delta_t+\E [\sum_{t=1}^T r(s_t, \pi^m(s_t)) - r(s_t, a_t)]$. As before, we use \Cref{lem:trajectories} to show that $\sum_{t=1}^T \Delta_t \le T\rac$, which requires $P$ to satisfy local generalization. Previously, we bounded $\E [\sum_{t=1}^T r(s_t, \pi^m(s_t)) - r(s_t, a_t)]$ by applying \Cref{def:ac} to $\mu_t(s,a) = r(s,a)$, which requires $r$ to satisfy local generalization. Here, we instead note that $r(s_t, \pi^m(s_t)) > r(s_t, a_t)$ can only occur when $a_t \ne \pi^m(s_t)$. Therefore
\begin{align*}
\E \left[\sum_{t=1}^T r(s_t, \pi^m(s_t)) -\sum_{t=1}^T r(s_t, a_t)\right] \le&\ \E \left[\sum_{t \in M_T} (r(s_t, \pi^m(s_t)) - r(s_t, a_t))\right]\\
\le&\ \E \left[\sum_{t \in M_T} 1\right]\\
=&\ \E \left[|M_T|\right]\\
\in&\ O\left(\frac{d}{\sigma} T^\frac{1}{2n+1} \log T\right)
\end{align*}
\Cref{lem:ac-regret} implies that $\rac \in O(\frac{dL}{\sigma} T^\frac{1}{2n+1} \log T)$, so we conclude that
\begin{align*}
R_T \le&\ T \cdot O\left(\frac{dL}{\sigma} T^\frac{1}{2n+1} \log T\right) + O\left(\frac{d}{\sigma} T^\frac{1}{2n+1} \log T\right)\\
\in&\ O\left(\frac{dL}{\sigma} T^\frac{2n}{2n+1} \log T\right)
\end{align*}
We conclude that $R_T \in o(T)$.
\end{proof}