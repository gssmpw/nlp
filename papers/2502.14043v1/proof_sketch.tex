\subsection{Proof sketch}

\input{proof_fig}

The full proof is deferred to Appendix~\ref{sec:main-proof}, but we provide a visualization (\Cref{fig:proof}) and proof sketch here. 

First, we bound the action-based regret by a direct application of \Cref{def:ac}.\footnote{The fact that bounding the action-based regret is so simple is more a property of \Cref{def:ac} than of the decomposition.}

\begin{restatable}{lemma}{lemActionRegret}
\label{lem:action-regret}
If an algorithm satisfies \Cref{def:ac}, then $\E \big[\sum_{t=1}^T r(s_t, \pi^m(s_t)) - \sum_{t=1}^Tr(s_t, a_t)\big] \le  \rac$.
\end{restatable}

\begin{proof}
Let $\mu_t(s,a) := r(s,a)$ for all $t \in [T]$. Since $r$ satisfies local generalization, so does $\bfmu$. Hence by  \Cref{def:ac}, $\E \big[\sum_{t=1}^T r(s_t, \pi^m(s_t)) - \sum_{t=1}^T r(s_t, a_t)\big] \le  \rac$.
\end{proof}

It remains to bound the state-based regret. Rather than analyzing when $\smols$ is better or worse than $\sm$, we simply bound how much their distributions differ at all. Specifically, we will show that $\sup_{X\subseteq \s}(p_t^m(X) - p_t(X)) \le \rac$ for all $t \in [T]$. Let $\Delta_t = \sup_{X\subseteq \s}(p_t^m(X) - p_t(X))$; we will refer to this quantity frequently. 
First, we show that an entire class of expected values can be bounded by $\Delta_t$.


\begin{restatable}{lemma}{lemSplit}
\label{lem:split}
For any $t \in [T]$ and any measurable function $f: \s \to [0,1]$,  we have $\E[f(s_t^m) - f(s_t)] \le \Delta_t$.
\end{restatable}

The idea behind \Cref{lem:split} is that $\E[f(s_t^m) - f(s_t)]$ can only be large if $s_t^m$ is more concentrated than $s_t$ in states where $f$ is large. We use $\Delta_t$ to measure how large that difference in concentration can be. Also, since $f(s) \in [0,1]$, $f$ cannot be too much larger in these states where $s_t^m$ is more concentrated than $s_t$. Although there are some technical details, conceptually the proof amounts to:
\begin{align*}
\E[f(s_t^m) - f(s_t)] \le \sup_{s,s' \in \s} |f(s) - f(s')|\cdot  \Delta_t
\le  \Delta_t
\end{align*}
The most obvious application of \Cref{lem:split} is with $f(s) = r(s, \pi^m(s))$, and indeed, that will be one usage. However, we will also use this lemma to analyze the divergence between $\smols$ and $\sm$. Specifically, we will write $p_{t+1}^m(X) - p_{t+1}(X) = \E[f(s_t^m) - f(s_t)] + \alpha_t(X)$ for some functions $f$ and $\alpha_t$. Then \Cref{lem:split} will imply that
\begin{align*}
p_{t+1}^m(X) - p_{t+1}(X) \le&\ \Delta_t + \alpha_t(X) \quad \text{$\forall X \subseteq \s$}\\
\sup_{X\subseteq \s} (p_{t+1}^m(X) - p_{t+1}(X)) \le&\ \Delta_t + \sup_{X\subseteq \s} \alpha_t(X)\\
\Delta_{t+1} \le&\ \Delta_t + \sup_{X\subseteq \s} \alpha_t(X)
\end{align*}
The agent and mentor have the same initial state, so $\Delta_1 = 0$ and thus $\Delta_t \le \sum_{i=1}^{t-1} \sup_{X\subseteq \s} \alpha_t(X)$ by induction.

To enact this plan, we choose $f(s) = N_t^m(s,X)$ and $\alpha_t(X) = \E[N_t^m(s_t, X) - N_t(s_t, X)]$. Recall from \Cref{sec:model} that $N_t^m(s, X) = \Pr[s_{t+1}^m \in X \mid s_t^m = s]$ and $N_t(s,X) = \Pr[s_{t+1} \in X \mid s_t = s]$. The law of total expectation implies that $p_{t+1}^m(X) = \E[N_t^m(s_t^m, X)]$ and $p_{t+1}(X) = \E[N_t(s_t, X)]$, so
\begin{align*}
& \ \ p_{t+1}^m(X) - p_{t+1}(X)\\
=&\ \E[N_t^m(s_t^m, X)] - \E[N_t(s_t, X)]\\
=&\ \resizebox{\linewidth}{!}{%
\(\E[N_t^m(s_t^m, X) - N_t^m(s_t, X)] + \E[N_t^m(s_t, X) - N_t(s_t, X)]\)
}
\\
=&\ \E[N_t^m(s_t^m, X) - N_t^m(s_t, X)] + \alpha_t(X)
\end{align*}
(Interestingly, this decomposition is similar in structure to the state- vs action-based regret decomposition, although the terms here do not depend on $r$ at all.) 

This decomposition allows us to apply the aforementioned inductive strategy to obtain the following lemma:

\begin{restatable}{lemma}{lemTrajInd}\label{lem:trajectories-induction}
For any $t \in [T]$, $\Delta_t \le \sum_{i=1}^{t-1}\sup_{X\subseteq \s}\alpha_i(X)$.
\end{restatable}

This brings us to the trickiest part of the proof: bounding $\sum_{i=1}^{t-1}\sup_{X\subseteq \s}\alpha_i(X)$. The main idea is to invoke \Cref{def:ac} with $\mu_i(s, a) = P(s,a,X_i)$ for each $i \in [t-1]$ for every possible choice of $X_1,\dots,X_{t-1} \subseteq \s$. (It is also helpful to define $\mu_i(s,a)$ to be constant for $i > t$.) 

One can use the definition of total variation distance to show that if $P$ satisfies local generalization, so does this definition of $\bfmu$. Next, we can manipulate conditional expectations to show that $\E[N_i(s_i,X_i)] = \E[P(s_i,a_i,X_i)] = \E[\mu_i(s_i,a_i)]$ and $\E[N_i^m(s_i, X_i)] = \E[P(s_i, \pi^m(s_i), X_i)] = \E[\mu_i^m(s_i)]$. Thus applying \Cref{def:ac} to \Cref{lem:trajectories-induction} gives us
\begin{align*}
\Delta_t \le \sum_{i=1}^{t-1}\alpha_i(X_i) 
= \sum_{i=1}^T \E[\mu_i^m(s_i) - \mu_i(s_i, a_i)]
\le R_T^{AC}
\end{align*}
The result is \Cref{lem:trajectories}:

\begin{restatable}{lemma}{lemTraj}
\label{lem:trajectories}
If an algorithm satisfies \Cref{def:ac}, then $\Delta_t \le \rac$ for all $t \in [T]$.
\end{restatable}

Now we can bound the state-based regret by applying \Cref{lem:split} with $f(s) = r(s, \pi^m(s))$ followed by \Cref{lem:trajectories}. Specifically, we obtain
\[
\E\big[r(s_t^m, \pi^m(s_t^m)) - r(s_t, \pi^m(s_t))\big] \le \Delta_t \le \rac
\]
Summing this over $t$ produces \Cref{lem:state-regret}:

\begin{restatable}{lemma}{lemStateRegret}
\label{lem:state-regret}
If an algorithm satisfies \Cref{def:ac}, then \resizebox{1.02\linewidth}{!}{%
$\E\big[\sum_{t=1}^T r(s_t^m, \pi^m(s_t^m)) - \sum_{t=1}^T r(s_t, \pi^m(s_t))\big] \le T \rac$
.}
\end{restatable}

Since the state-based regret is at most $T \rac$ (\Cref{lem:state-regret}) and the action-based regret is at most $\rac$ (\Cref{lem:action-regret}), we conclude that $R_T \le (T+1)\rac$.