\section{Prior work}
\label{sec:related}

% Our work builds heavily on ____ and we discuss their work in detail in \Cref{sec:ac-overview}. However, they only study avoidance of catastrophe (i.e., irreparable errors) and do not consider reward maximization. In contrast, our goal of sublinear regret in general MDPs requires both avoiding catastrophe and obtaining high reward.

There is a vast body of work studying regret in MDPs, but all prior no-regret guarantees rely on restrictive assumptions. For context, the agent has no hope of success without further assumptions: any unexplored action could lead to paradise (so the agent cannot neglect it) but the action could also lead to disaster (so the agent cannot risk it). This intuition is formalized by the ``Heaven or Hell'' problem in \Cref{fig:heaven_hell_problem}. Our work is not immune to this problem: rather, we argue that for high-stakes AI applications, access to a mentor is more realistic than prior assumptions.

\Cref{fig:prior_work} subdivides the literature based on which assumption is used to circumvent this problem. Appendix~\ref{sec:related-full} discusses each node in depth (and covers some related work outside of this taxonomy), but we provide an overview here. Unless otherwise specified, citations are representative, not exhaustive.




% There is a vast body of work studying regret in MDPs, but all prior no-regret guarantees rely on restrictive assumptions. For context, the agent has no hope of success without further assumptions: any unexplored action could lead to paradise (so the agent cannot neglect it) but the action could also lead to disaster (so the agent cannot risk it). This intuition is formalized by the ``Heaven or Hell'' problem in \Cref{fig:heaven_hell_problem}, which succinctly shows that some sort of additional assumption is necessary to achieve meaningful regret guarantees.


% \Cref{fig:prior_work} subdivides the literature based on which assumption is used for this purpose. Appendix~\ref{sec:related-full} discusses each node in depth (and covers some related work outside of this taxonomy), but we provide an overview here. Unless otherwise specified, citations are representative, not exhaustive.


\input{heaven_or_hell}

\input{related_work_fig}

\begin{figure*}[b]
\usebox{\taxonomy}
\caption{A taxonomy of assumptions which enable meaningful theoretical results in RL. The Heaven or Hell problem (\Cref{fig:heaven_hell_problem}) shows that meaningful guarantees are impossible without some such assumption. \Cref{sec:related} provides a high-level overview of each approach; see Appendix~\ref{sec:related-full} for a comprehensive discussion.}
\label{fig:prior_work}
\end{figure*}


\textbf{Assuming that all errors are recoverable.} The majority of prior work sidesteps the Heaven or Hell problem by assuming that any error can be offset by performing well enough in the future. This assumption can take various forms, including periodically resetting the agent ____ or assuming a communicating MDP ____. A less obvious form is defining regret relative to prior errors, i.e., allowing catastrophic errors as long as the agent does as well as possible going forward  ____. Defining regret relative to prior errors is discussed in depth in Appendix~\ref{sec:related-full}. 

Regardless of the specific form of the assumption, this approach faces difficulty in high-stakes domains where some errors cannot be recovered from. For example, a robotic vehicle cannot make up for a crash by driving superbly afterward. Thus we argue that this approach is ill-suited for the types of AI risks we are concerned with.

 % ____. 


\textbf{Requiring prior knowledge.} A smaller body of work allows for the possibility of catastrophe but provides the agent sufficient prior knowledge to avoid it, primarily within the formalism of Constrained MDPs (CMDPs) ____. Some work assumes that the exact safety constraint is known, i.e., the agent knows upfront exactly which actions cause catastrophe ____. Other work assumes the agent knows a fully safe policy and the agent is periodically reset ____. In contrast, we assume no prior knowledge of the environment.\looseness=-1

\textbf{Allowing external help.} An even less common approach is to allow the agent to ask for help from a mentor. Crucially, this approach had not previously produced a no-regret guarantee. Most prior work in this category focuses on safety and does not provide regret bounds on reward\footnote{We emphasize ``on reward'' because ____ provides a regret guarantee on a safety objective.} ____. 

We are aware of just two papers that utilize external help to obtain regret guarantees on reward: ____ and ____. The former paper ____ does provide a regret bound for general MDPs, but their regret bound is linear in the worst case due to its dependence on the $\gamma^*$ parameter which roughly measures how costly an action can be. Our regret bound is sublinear in $T$ for all MDPs, and we allow actions to be arbitrarily costly. The latter paper ____ studies an infinite-horizon setting and analyzes the dependence of the regret on the discount factor. Their result is not a no-regret guarantee since it does not say anything about what happens to the regret as time passes: in particular, it does not imply that the average regret per time step goes to 0. Furthermore, their method relies on computing a Bayesian posterior, which is intractable without strong assumptions. In their case, they assume a finite number of states, actions, and possible hypotheses. In contrast, we only assume a finite number of actions.

Since our result involves a reduction to the work of ____, we discuss their work in depth in \Cref{sec:ac-overview}. \looseness=-1