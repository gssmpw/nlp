\section{Model}\label{sec:model}

\textbf{MDPs.} Let $\Delta(X)$ denote the set of probability measures with sample space $X$, let $\bbn$ denote the strictly positive integers, and let $[k] = \{1,\dots,k\}$ for $k \in \bbn$. A (finite-horizon undiscounted) MDP is a tuple $\M = (\s, \A, P, r, s_1, T)$ where $\s$ is the state space, $\A$ is the (finite) action space, $P: \s \times \A \to \Delta(\s)$ is the transition kernel, $r: \s \times \A \to [0,1]$ is the reward function, $s_1 \in \s$ is the initial state, and $T \in \bbn$ is the time horizon. 

On each time step $t \in [T]$, the agent observes a state $s_t \in \s$ and selects an action $a_t \in \A$. The agent then obtains a reward $r(s_t,a_t)$ and the next state $s_{t+1}$ is sampled from the distribution $P(s_t,a_t)$.\footnote{Typically the agent would still observe the reward, but \Cref{thm:main} actually does not require the agent to observe rewards: the mentor queries alone are sufficient.} For each $X \subseteq \s$,  let $P(s,a,X)$ denote the probability which $P(s,a)$ assigns to $X$, i.e., $P(s,a,X) =\Pr[s_{t+1} \in X \mid s_t = s, a_t = a]$ for any $t \in [T]$. Also let $\smols = (s_1,\dots,s_T)$ and $\smola = (a_1,\dots,a_T)$. 

% Let $P(s,a,  X)$ be the probability that the distribution $P(s,a)$ assigns to $X \subseteq \s$. 

We allow the agent's choice of action to be randomized, and all expectations are over the randomness in $P$ and any randomness in the agent's choice of action.

\textbf{Asking for help.} We assume there exists a mentor endowed with a deterministic policy $\pi^m: \s \to \A $. For each $t \in [T]$, let $q_t=1$ be the indicator variable of whether the agent queries the mentor at time $t$. If $q_t = 1$, then the agent observes $\pi^m(s_t)$ and takes action $a_t = \pi^m(s_t)$. If $q_t = 0$, the agent does not observe $\pi^m(s_t)$ and selects an action $a_t$ of its choice. Let $s_t^m$ be the mentor's state at time $t$, where $s_1^m = s_1$ and $s_{t+1}^m \sim P(s_t^m, \pi^m(s_t^m))$. Let $\sm = (s_1^m,\dots,s_T^m)$.

\textbf{Measure-theoretic considerations.} Since we do not assume a discrete state space, it behooves us to briefly discuss the underlying measure spaces in order to ensure full mathematical rigor. Readers less interested in measure theory can safely skip this discussion. We assume that $\s$ is equipped with the usual Borel $\sigma$-algebra $\B(\s)$. All discussion of measurability is with respect to $(\s, \B(\s))$. Then for any $s \in \s$ and $a \in \A$, $P(s,a)$ is a probability measure on $(\s,\B(\s))$. Any reference to subsets of $\s$ is assumed to be restricted to $\B(\s)$: in other words, we treat $X \subseteq \s$ as shorthand for $X \in \B(\s)$. We assume that $r$ is measurable on $\s \times \A$, $\pi^m$ is measurable on $\s$, and the agent's choices of $q_t$ and $a_t$ are measurable function of the agent's observation history $s_1,a_1,q_1,\dots,s_t$. Lastly, we assume that for any fixed $X\subseteq \s$, $P(\cdot,\cdot,X)$ is measurable on $\s\times\A$. These assumptions ensure that all relevant expectations and probabilities are well-defined. See Chapter 8 \cite{bertsekas1996stochastic} for a thorough treatment of measure-theoretic considerations in MDPs.

\textbf{Distributions of interest.} Let $p_t$ and $p_t^m$ denote the distributions of $s_t$ and $s_t^m$ respectively. That is, for any $X \subseteq \s$, $p_t(X) = \Pr[s_t\in X]$ and $p_t^m(X)= \Pr[s_t^m \in X]$. With ``$N$'' standing for ``next'', let $N_t(s,X) = \Pr[s_{t+1} \in X \mid s_t = s]$ and $N_t^m(s,X) = \Pr[s_{t+1}^m \in X \mid s_t^m = s]$. We say that $P$ is \emph{$\sigma$-smooth} \cite{spielman2004smoothed, haghtalab2024smoothed} if for all $s \in \s, a \in \A$, and $X \subseteq \s$, $P(s,a,X) \le \frac{1}{\sigma}U(X)$, where $U$ is the uniform distribution over $\s$. 

\textbf{Local generalization.} \citet{plaut_avoiding_2024} introduced local generalization to capture the intuition that knowledge can be transferred between similar states. This requires a notion of state similarity, so we assume $\s \subseteq \bbr^n$. (One could also consider a general metric space, but we focus on $\bbr^n$ for convenience.) Let $\norm{\cdot}$ denote Euclidean distance. The diameter of a set $X \subseteq \s$ is defined as $\diam(X) = \max_{s, s' \in \s} \norm{s-s'}$.

% an ability which is fundamental to learning in humans \cite{esser_actioneffect_2023,hajian_transfer_2019}.

Following \citet{plaut_avoiding_2024}, local generalization of $P$ states that for any $s,s' \in \s$, we have
\[
||\!\!\!
  \underbrace{P(s,\pi^m(s))}_{\text{Taking the right action}}\!
  -\!
  \underbrace{P(s,\pi^m(s'))}_{\text{Using what you learned in $s'$}} \!\!\!\!\!\!\!\!||_{TV}
\le\;
\underbrace{L \,\|s - s'\|}_{\text{State similarity}}
\]
We assume local generalization of $P$ throughout. The most general version of our result will also require local generalization for $r$:  $|r(s,\pi^m(s)) - r(s,\pi^m(s'))| \le L\norm{s-s'}$ for any $s,s' \in \s$. We assume that $L$ is the same for both $P$ and $r$ (if not, simply use the maximum).

Although local generalization may not be satisfied when states simply represent physical orientation, we argue that for a sufficiently rich state representation, it should be the case that the agent can transfer knowledge between ``similar'' states. Importantly, we make no assumptions on how states are represented in $\bbr^n$.

\textbf{Objectives.} Our first objective is for the agent to become self-sufficient over time. For an MDP $\M$ and mentor policy $\pi^m$, let $Q_T(\M, \pi^m) = \E[\sum_{t=1}^T q_t]$ be the expected number of queries to the mentor. Our second objective is for the agent to perform nearly as well as the mentor. Given $\M$ and $\pi^m$, the agent's expected regret is\looseness=-1
\[
R_T(\M, \pi^m) = \E\left[\sum_{t=1}^T r(s_t^m, \pi^m(s_t^m) )- \sum_{t=1}^T r(s_t, a_t)\right]
\]
This corresponds to independently running the mentor and the agent each for $T$ steps from the same initial state and comparing their expected total reward. (One typically compares the agent to the optimal policy, but our mentor can be optimal or suboptimal, so our definition is more general.)

We want both the number of queries $Q_T(\M,\pi^m)$ and the regret $R_T(\M, \pi^m)$ to be sublinear in $T$. Formalizing this goal requires a bit of care, since $Q_T$ and $R_T$ depend on $\M$ and $\pi^m$ in addition to $T$. Formally, we desire
\[
\lim_{T\to\infty} \sup_{\M, \pi^m} \frac{Q_T(\M, \pi^m)}{T} = \lim_{T\to\infty} \sup_{\M, \pi^m} \frac{R_T(\M, \pi^m)}{T} = 0
\]
% \begin{align*}
% \lim_{T\to\infty} \sup_{\M, \pi^m} \frac{Q_T(\M, \pi^m)}{T} =&\ 0\\
% \lim_{T\to\infty} \sup_{\M, \pi^m} \frac{R_T(\M, \pi^m)}{T} =&\ 0
% \end{align*}
The order of the limit and the supremum means that the worst-case instance can depend on $T$, which is standard for finite-horizon RL and also aligns with \citet{plaut_avoiding_2024}. With slight abuse of notation, let $Q_T = \sup_{\M, \pi^m} Q_T(\M, \pi^m)$ and $R_T = \sup_{\M, \pi^m} 
 R_T(\M, \pi^m)$. Then we can equivalently and more concisely write our goal as $Q_T \in o(T)$ and $R_T \in o(T)$.

 \textbf{VC and Littlestone dimensions.} VC dimension \citep{vapnik_uniform_1971} and Littlestone dimension \citep{littlestone1988learning} are standard metrics of learning difficulty relating to the complexity of a hypothesis class (in our case, the mentor policy class). Since we do not work with these concepts directly, we omit their definitions and refer interested readers to \citet{shalev-shwartz_understanding_2014}.

  % which measure the extent to which a hypothesis class can produce arbitrary labelings of data points. In our case, this means the extent to which the mentor policy class can assign arbitrary mentor actions to different states. 

 % difficulty of learning a given hypothesis class. More specifically, these dimensions measure the 