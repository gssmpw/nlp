
\section{Comprehensive discussion of prior work}\label{sec:related-full}

Here we provide a more complete discussion of prior work, structured according to \Cref{fig:prior_work}. As mentioned in \Cref{sec:related}, citations are representative, not exhaustive.
\begin{figure}[h]
\usebox{\taxonomy}
\caption*{\Cref{fig:prior_work}. A taxonomy of assumptions which enable meaningful theoretical results in RL. The Heaven or Hell problem (\Cref{fig:heaven_hell_problem}) shows that meaningful guarantees are impossible without some such assumption. \Cref{sec:related} provides a high-level overview of each approach; see Appendix~\ref{sec:related-full} for a comprehensive discussion. The citations in the "Safety and reward" subtree are (to our knowledge) exhaustive, but elsewhere we have simply chosen representative citations.}
\end{figure}

\subsection{Assuming that any error can be recovered from}\label{sec:recoverable}

The defining feature of this category is the assumption that any error can be offset by sufficiently good performance in the future.

\paragraph{Assuming all actions are reversible.} This reversibility assumption appears in two main forms. (1) Assume that any state is reachable from any other, i.e., the MDP is communicating \citep{jaksch_near-optimal_2010}. Note that the Heaven or Hell MDP in \Cref{fig:heaven_hell_problem} is non-communicating. Under this assumption, the agent itself has the power to reverse its prior actions. (2) Assume that the agent is reset after each ``episode'', in which case all actions are automatically reversed periodically \citep{azar_minimax_2017}.

Unfortunately, irreversibility abounds in the real world: indeed, it is a key component of the AI risks described in \Cref{sec:intro} (e.g., robotic vehicles cannot be uncrashed). Furthermore, resetting the agent is futile if irreparable harm has already occurred.

\paragraph{Defining regret relative to prior errors.} Informally, catastrophic errors are permitted as long as the agent performs as well as possible from that point on. Specifically, this approach compares the performance of the agent and the reference policy \emph{on the agent's sequence of states} and does not consider whether the reference policy would have led to much better states. For example, an agent which immediately destroys itself\footnote{One can think of this as entering Hell in \Cref{fig:heaven_hell_problem}.} is considered successful, since all actions are equivalent once it is destroyed and thus it obtains ``optimal'' reward on all future time steps. This comparison can be formulated as a type of regret \citet{he2021nearly, liu_regret_2021} or via the related notion of ``asymptotic optimality'' introduced by \citet{lattimore_asymptotically_2011}. 

In the context of a mentor policy $\pi^m$, one could write
\[
R_T= \E\left[\sum_{t=1}^T r(s_t, \pi^m(s_t) )- \sum_{t=1}^T r(s_t, a_t)\right]
\]
Each time step typically contributes at most 1 to the total regret, so even a catastrophic error adds at most 1 to the regret, since that error will be taken as ``given'' on all future time steps. (Typically instead of $r(s_t, \pi^m(s_t))$, one would use some sort of long-term value of $\pi^m$ starting from $s_t$. But this term will still be normalized to at most 1 and is still using the agent's state $s_t$ as its starting state, so the intuition above is unaffected.) \looseness=-1

In practice, one typically does care if the agent could have ended up in better states, especially if some states are catastrophic. Consequently, we evaluate the mentor and the agent on their respective sequences of states:\looseness=-1
\[
R_T = \E\left[\sum_{t=1}^T r(s_t^m, \pi^m(s_t^m) )- \sum_{t=1}^T r(s_t, a_t)\right]
\]
In other words, we simply run the agent and the mentor independently each for $T$ time steps, both starting from $s_1$. We argue that our definition better captures the type of performance guarantee that is relevant in practice. For example, our definition heavily penalizes the agent for making catastrophic errors (e.g., being destroyed) which the mentor avoids.



\subsection{Requiring significant prior knowledge}

Constrained MDPs (CMDPS) \citep{altman2021constrained} include both a reward objective and a safety constraint, where zero constraint violation is analogous to avoiding catastrophe. Some work in CMDPs allows significant (e.g., sublinear) constraint violation. This in effect assumes that all errors are recoverable, so we do not discuss that work here. See \citet{wachi_survey_2024} for a breakdown of the various constraint formulations in the literature.\looseness=-1

CMDPs also must contend with the Heaven or Hell problem (\Cref{fig:heaven_hell_problem}): for example, suppose the safety constraint is simply to avoid Hell. The solution favored by the CMDP literature is to endow the agent with enough prior knowledge to avoid catastrophe. This prior knowledge takes two main forms:

\paragraph{The safety constraint is known.} In this case, clearly the agent has sufficient information to ensure zero constraint violation. However, full knowledge of the safety constraint upfront is a very strong assumption that is unlikely to be satisfied in the real world. See \citet{zhao_state-wise_2023} for a survey of CMDP papers which assume this type of knowledge of the environment; one representative paper is \citet{model_zhao_22a}.


\input{purgatory}


\paragraph{A strictly safe policy is known.} Another option is to assume that a safe policy $\pi_0$ is known upfront \citep{liu2021learning,stradi2024learning}. Not only is this a strong assumption, it is also insufficient: there exist simple instances where $\pi_0$ produces low reward, but the agent has no safe way to explore beyond $\pi_0$ (\Cref{fig:purgatory}). In order to allow the agent to safely explore beyond $\pi_0$, the two references above both also assume a multi-episode setting and assume that $\pi_0$ is ``strictly safe'', i.e., it satisfies the safety constraints with slack.


\subsection{Allowing external help.}

We avoid repeating information from \Cref{sec:related}, so we have just two points we wish to add here. First, we mentioned that \citet{kosoy_delegative_2019} relies on Bayesian inference, but it is worth noting that the majority of papers in this category rely on Bayesian inference \citep{cohen_pessimism_2020, cohen_curiosity_2021, mindermann_active_2018}. These papers also face Bayesian inference's tractability challenges, as discussed in \Cref{sec:related}. To our knowledge, the only prior papers which use external help to obtain safety guarantees without Bayesian inference are \citep{maillard_active_2019,plaut_avoiding_2024}.

Second, several papers which we list under ``focus is safety only'' do include guarantees relating to the agent's reward \citep{cohen_pessimism_2020, cohen_curiosity_2021}. However, those guarantees are of the form ``define regret relative to prior errors'' and thus are subject to the weaknesses discussed previously.

\subsection{Other related work} 

The idea of designing agents which act cautiously when uncertain is not novel. Indeed, this idea is so fundamental that the prior work on this topic far outstrips what we can cover here. Representative theoretical papers include \citet{cortes2018online, hadfield-menell_inverse_2017, li2008knows}. Our model is also reminiscent of active learning \citep{hanneke2014active} due to how the agent proactively requests information and of imitation learning \citep{osa_algorithmic_2018} due to how the agent attempts to imitate the mentor. However, we are not aware of any relevant technical implications. Finally, see \citet{garcia_comprehensive_2015,gu_review_2024, krasowski_provably_2023} for surveys of the safe RL literature.

