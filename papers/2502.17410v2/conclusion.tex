\section{Conclusion}
We develop a hybrid adaptive optimizer, COSMOS, which leverages the varying importance of eigensubspaces in the gradient matrix to achieve token efficiency, memory efficiency, and high computation throughput simultaneously. 
By decomposing the gradient matrix into leading and remaining eigensubspaces and applying SOAP-like and MUON-like updates to them correspondingly, COSMOS uses significantly less memory than SOAP while achieving equal or better optimization performance. 
Comprehensive experiments show that COSMOS performs consistently well across different settings. 
