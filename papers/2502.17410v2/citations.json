[
  {
    "index": 0,
    "papers": [
      {
        "key": "kingma2014adam",
        "author": "Kingma, Diederik P",
        "title": "Adam: A method for stochastic optimization"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "loshchilov2017decoupled",
        "author": "Loshchilov, I",
        "title": "Decoupled weight decay regularization"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "chen2023symbolic",
        "author": "Chen, Xiangning and Liang, Chen and Huang, Da and Real, Esteban and Wang, Kaiyuan and Liu, Yao and Pham, Hieu and Dong, Xuanyi and Luong, Thang and Hsieh, Cho-Jui and others",
        "title": "Symbolic Discovery of Optimization Algorithms"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "liu2023sophia",
        "author": "Liu, Hong and Li, Zhiyuan and Hall, David and Liang, Percy and Ma, Tengyu",
        "title": "Sophia: A scalable stochastic second-order optimizer for language model pre-training"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "shazeer2018adafactor",
        "author": "Shazeer, Noam and Stern, Mitchell",
        "title": "Adafactor: Adaptive Learning Rates with Sublinear Memory Cost"
      },
      {
        "key": "zhai2022scaling",
        "author": "Zhai, Xiaohua and Kolesnikov, Alexander and Houlsby, Neil and Beyer, Lucas",
        "title": "Scaling vision transformers"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "martens2010deep",
        "author": "Martens, James",
        "title": "Deep learning via Hessian-free optimization"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "martens2015optimizing",
        "author": "Martens, James and Grosse, Roger",
        "title": "Optimizing Neural Networks with Kronecker-factored Approximate Curvature"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "martens2015optimizing",
        "author": "Martens, James and Grosse, Roger",
        "title": "Optimizing Neural Networks with Kronecker-factored Approximate Curvature"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "gupta2018shampoo",
        "author": "Gupta, Vineet and Koren, Tomer and Singer, Yoram",
        "title": "Shampoo: Preconditioned stochastic tensor optimization"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "martens2015optimizing",
        "author": "Martens, James and Grosse, Roger",
        "title": "Optimizing Neural Networks with Kronecker-factored Approximate Curvature"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "martens2018kronecker",
        "author": "Martens, James and Ba, Jimmy and Johnson, Matt",
        "title": "Kronecker-factored Curvature Approximations for Recurrent Neural Networks"
      },
      {
        "key": "osawa2018large",
        "author": "Osawa, Kazuki and Tsuji, Yohei and Ueno, Yuichiro and Naruse, Akira and Yokota, Rio and Matsuoka, Satoshi",
        "title": "Large-Scale Distributed Second-Order Optimization Using Kronecker-Factored Approximate Curvature for Deep Convolutional Neural Networks"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "george2018fast",
        "author": "George, Thomas and Laurent, C{\\'e}sar and Bouthillier, Xavier and Ballas, Nicolas and Vincent, Pascal",
        "title": "Fast Approximate Natural Gradient Descent in a Kronecker-factored Eigenbasis"
      },
      {
        "key": "gao2021trace",
        "author": "Gao, Kaixin and Liu, Xiaolei and Huang, Zhenghai and Wang, Min and Wang, Zidong and Xu, Dachuan and Yu, Fan",
        "title": "A trace-restricted kronecker-factored approximation to natural gradient"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "ba2017distributed",
        "author": "Ba, Jimmy and Grosse, Roger and Martens, James",
        "title": "Distributed Second-Order Optimization using Kronecker-Factored Approximations"
      },
      {
        "key": "puiu2022randomized",
        "author": "Puiu, Constantin Octavian",
        "title": "Randomized K-FACs: Speeding Up K-FAC with Randomized Numerical Linear Algebra"
      },
      {
        "key": "puiu2022brand",
        "author": "Puiu, Constantin Octavian",
        "title": "Brand New K-FACs: Speeding up K-FAC with Online Decomposition Updates"
      },
      {
        "key": "eschenhagen2023kronecker",
        "author": "Eschenhagen, Runa and Immer, Alexander and Turner, Richard E and Schneider, Frank and Hennig, Philipp",
        "title": "Kronecker-Factored Approximate Curvature for Modern Neural Network Architectures"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "gupta2018shampoo",
        "author": "Gupta, Vineet and Koren, Tomer and Singer, Yoram",
        "title": "Shampoo: Preconditioned stochastic tensor optimization"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "duchi2011adaptive",
        "author": "Duchi, John and Hazan, Elad and Singer, Yoram",
        "title": "Adaptive subgradient methods for online learning and stochastic optimization."
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "shi2023distributed",
        "author": "Shi, Hao-Jun Michael and Lee, Tsung-Hsien and Iwasaki, Shintaro and Gallego-Posada, Jose and Li, Zhijing and Rangadurai, Kaushik and Mudigere, Dheevatsa and Rabbat, Michael",
        "title": "A distributed data-parallel pytorch implementation of the distributed shampoo optimizer for training neural networks at-scale"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "dahl2023benchmarking",
        "author": "Dahl, George E and Schneider, Frank and Nado, Zachary and Agarwal, Naman and Sastry, Chandramouli Shama and Hennig, Philipp and Medapati, Sourabh and Eschenhagen, Runa and Kasimbeg, Priya and Suo, Daniel and others",
        "title": "Benchmarking neural network training algorithms"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "anil2020scalable",
        "author": "Anil, Rohan and Gupta, Vineet and Koren, Tomer and Regan, Kevin and Singer, Yoram",
        "title": "Scalable second order optimization for deep learning"
      },
      {
        "key": "peirson2022fishy",
        "author": "Peirson, Abel and Amid, Ehsan and Chen, Yatong and Feinberg, Vladimir and Warmuth, Manfred K and Anil, Rohan",
        "title": "Fishy: Layerwise Fisher Approximation for Higher-order Neural Network Optimization"
      },
      {
        "key": "lin2024can",
        "author": "Lin, Wu and Dangel, Felix and Eschenhagen, Runa and Bae, Juhan and Turner, Richard E and Makhzani, Alireza",
        "title": "Can we remove the square-root in adaptive gradient methods? a second-order perspective"
      },
      {
        "key": "wang20244",
        "author": "Wang, Sike and Zhou, Pan and Li, Jia and Huang, Hua",
        "title": "4-bit Shampoo for Memory-Efficient Network Training"
      },
      {
        "key": "zhao2024deconstructing",
        "author": "Zhao, Rosie and Morwani, Depen and Brandfonbrener, David and Vyas, Nikhil and Kakade, Sham",
        "title": "Deconstructing what makes a good optimizer for language models"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "vyas2024soap",
        "author": "Vyas, Nikhil and Morwani, Depen and Zhao, Rosie and Shapira, Itai and Brandfonbrener, David and Janson, Lucas and Kakade, Sham",
        "title": "Soap: Improving and stabilizing shampoo using adam"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "zhang2024adam",
        "author": "Zhang, Yushun and Chen, Congliang and Li, Ziniu and Ding, Tian and Wu, Chenwei and Ye, Yinyu and Luo, Zhi-Quan and Sun, Ruoyu",
        "title": "Adam-mini: Use fewer learning rates to gain more"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "shazeer2018adafactor",
        "author": "Shazeer, Noam and Stern, Mitchell",
        "title": "Adafactor: Adaptive Learning Rates with Sublinear Memory Cost"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "zhao2024galore",
        "author": "Zhao, Jiawei and Zhang, Zhenyu and Chen, Beidi and Wang, Zhangyang and Anandkumar, Anima and Tian, Yuandong",
        "title": "GaLore: Memory-efficient llm training by gradient low-rank projection"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "liang2024memory",
        "author": "Liang, Kaizhao and Liu, Bo and Chen, Lizhang and Liu, Qiang",
        "title": "Memory-efficient llm training with online subspace descent"
      }
    ]
  },
  {
    "index": 23,
    "papers": [
      {
        "key": "jordan2024MUON",
        "author": "Unknown",
        "title": "Unknown"
      }
    ]
  }
]