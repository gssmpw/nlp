\newcommand{\RR}{\mathbb{R}}
\section{COSMOS: A Hybrid Adaptive Optimizer}

We present a novel hybrid optimizer -- COSMOS, which can achieve memory efficiency without compromising optimization performance for training LLMs. Without loss of generality, we use $m$ and $n$ to denote the numbers of rows and columns in a $m$ by $n$ matrix, and we assume $m>n$. For simplicity, we use the following notations.

\vskip2pt
\noindent $\bullet$ Matrix Sign Operator \citep{roberts1980linear}: Given a matrix $X\in\RR^{m \times n}$, we consider its SVD decomposition $X=UDV^\top$, where $D\in\RR^{n\times n}$ is a diagonal matrix containing all singular values of $X$, and $U\in\RR^{m\times m}$ and $V\in\RR^{n \times n}$ are left and right singular vector matrices, respectively. We define
\begin{align*}
\texttt{MatSgn}(X) = UV^\top. 
\end{align*}
The Shampoo algorithm uses the matrix sign operator to normalize the first moment of the stochastic gradient.

\vskip2pt
\noindent $\bullet$ Newton Schulz (NS) transformation: Given a matrix $X_0\in\RR^{m \times n}$, where $\norm{X_0}_{\rm F}\leq 1$, we define
\begin{align*}
\texttt{NS5}(X_0)=X_5.
\end{align*}
where $X_5$ is obtained by
\begin{align*}
X_{k+1}=aX_{k}+bX_{k}X_{k}^\top X_{k}+cX_{k}X_{k}^\top X_{k}X_{k}^\top X_{k}
\end{align*}
for $k=0,1,...,4$ with $a=3.4445$, $b=-4.7750$ and $c=2.0315$. The NS transformation was mentioned in \citet{bernstein2024modular} to approximate the matrix sign operator without specifying the coefficient. \citet{jordan2024MUON} later used an ad-hoc gradient based approach to find such a set of coefficients.

\vskip2pt
\noindent $\bullet$ Normalization operator:
\begin{align}\label{new-norm}
\texttt{NORM}(X) = \sqrt{n}X/\norm{X}_{\rm F},
\end{align}
where $\norm{\cdot}_{\rm F}$ denotes the Frobenius norm. The normalization operator is used to normalize the output of the NS transformation.

\vskip2pt
\noindent $\bullet$ Gramâ€“Schmidt procedure: $\texttt{QR}(X)$.

\vskip2pt
\noindent $\bullet$ Compute the top-$r$ eigenvectors: $\texttt{SVD}(X,r)$.

We summarize COSMOS in Algorithm \ref{alg:COSMOS}.

\begin{algorithm}[htb!]
	\begin{algorithmic}[1]
        \INPUT{Learning rate $\eta$, combination weight $\gamma$, projection rank $r\ll n$, momentum parameters $(\beta_1,\beta_2)$, perturbation parameter $\epsilon$. For simplicity, we omit the initialization.}
        \FOR{$t=0,...$}
		\STATE Sample batch $\cM_t$
		\STATE $G_t \gets \nabla_W \phi_{\cM_t}(W_t)$
        \STATE $M_t \gets \beta_1M_{t-1}+(1-\beta_1)G_t$
        \STATE $U_t \gets \texttt{QR}(\beta_2U_{t-1}S_{t-1}+(1-\beta_2)G_t^\top G_tU_{t-1})$
        \STATE $S_t \gets U_t^\top(\beta_2 U_{t-1}S_{t-1}U_{t-1}^\top + (1-\beta_2)G_t^\top G_t)U_t$
		\STATE $V_t \gets \beta_2 V_{t-1} + (1-\beta_2) (G_tU_t) \odot (G_tU_t)$
		\STATE $\displaystyle A_t = \left(\frac{M_tU_t/(1-\beta_1^t)}{\sqrt{(V_t+\epsilon)/(1-\beta_2^t)}}\right)U_t^\top$
        \STATE $\displaystyle B_t \gets \texttt{NORM} \left(\texttt{NS5}\left(\frac{M_t-M_tU_tU_t^\top}{\norm{M_t-M_tU_tU_t^\top}_{\rm F}}\right)\right)$
        \STATE $\displaystyle \tilde{G}_t \gets A_t + \gamma \cdot B_t\cdot\sqrt{m}$ 
		\STATE $\displaystyle W_{t+1} \gets W_{t} -\eta \cdot\texttt{NORM}(\tilde{G}_t)$
        \ENDFOR
	\end{algorithmic}
	\caption{COSMOS for an $m \times n$ layer $W$. Per layer, we maintain four matrices: $U \in \RR^{n \times r}, S \in \mathbb{R}^{r \times r}$, $V\in\RR^{m\times r}$ and $M \in \mathbb{R}^{m \times n}$.}
	\label{alg:COSMOS}
\end{algorithm}

We have two remarks regarding the differences between COSMOS and SOAP (Algorithm \ref{alg:SOAP}\footnote{To fairly compare with COSMOS, we only consider the one-sided SOAP, which is discussed in Section 7.1 of \citet{vyas2024soap}.}):

\begin{remark}
Lines 4--8 in Algorithm \ref{alg:COSMOS} can be viewed as a low-rank variant of SOAP with significantly lower memory usage. Specifically, Line 4 obtains the Exponential Moving Average (EMA) of $G_t$ -- denoted by $M_t$. Line 5 approximates the leading eigenbases of $M_t$ -- denoted by $U_t$ -- by running a simple power iteration on the approximate EMA of $G_t^\top G_t$. Note that we maintain the low dimensional projection of the EMA of $G_t^\top G_t$ -- denoted by $S_t$ as in Line 6. When computing the approximate EMA of $G_t^\top G_t$, we can project $S_t$ back to the original space. Line 7 then obtains the EMA of the second moment of the stochastic gradient projected onto the low dimensional eigensubspace of $M_t$ -- denoted by $V_t$. Line 8 first applies an Adam-like adaptive update in the low dimensional eigensubspace of $M_t$, and then projects the update back to the original space -- denoted by $A_t$. 

As can be seen, COSMOS only needs to maintain four matrices in the memory: $M_t\in\RR^{m\times n}$, $U_t\in\RR^{n\times r}$, $S_t\in\RR^{r\times r}$ and $V_t\in\RR^{m\times r}$. In sharp contrast, SOAP needs to maintain $M_t\in\RR^{m\times n}$, $L_t \in\RR^{n\times n}$, $U_t\in\RR^{n\times n}$ and $V_t\in\RR^{m\times n}$. The resulting memory overhead is which is significantly larger than that of COSMOS.
\end{remark}

\begin{remark} Line 9 in Algorithm \ref{alg:COSMOS} is a MUON-like update -- denoted by $B_t$, but only applied to the complementary subspace of $M_t$ -- denoted by $M_t(I-U_tU_t^\top)$. Line 10 obtains a weighted combination of $B_t$ and the SOAP-like update $A_t$. Line 11 then applies such a combination to update the weight.

The NS transformation does not need to maintain any additional matrix in the memory, and therefore incurs no additional memory overhead.
\end{remark}

\begin{algorithm}[htb!]
	\begin{algorithmic}[1]
        \INPUT{Learning rate $\eta$, momentum parameters $(\beta_1,\beta_2)$, perturbation parameter $\epsilon$. For simplicity, we omit the initialization.}
        \FOR{$t=0,...$}
		\STATE Sample batch $\cM_t$
		\STATE $G_t \gets \nabla_W \phi_{\cM_t}(W_t)$
        \STATE $M_t \gets \beta_1M_{t-1}+(1-\beta_1)G_t$
        \STATE $L_t \gets \beta_2G_t^\top G_tU_{t-1}+(1-\beta_2)G_t^\top G_tU_{t-1}$
        \STATE $U_t \gets \texttt{QR}(L_tU_{t-1})$
        \STATE $G_t' \gets M_tU_t$
		\STATE $V_t \gets \beta_2 V_{t-1} + (1-\beta_2) (G_t' \odot G_t')$
		\STATE $\displaystyle A_t = \left(\frac{G_t'/(1-\beta_1^t)}{\sqrt{(V_t+\epsilon)/(1-\beta_2^t)}}\right)U_t^\top$
        \STATE $\displaystyle W_{t+1} \gets W_{t} -\eta A_t$
        \ENDFOR
	\end{algorithmic}
	\caption{(One-side) SOAP for an $m \times n$ layer $W$. Per layer, we maintain four matrices: $U_t\in\RR^{n\times n}$, $L \in \mathbb{R}^{n \times n}$, $V\in\RR^{m\times n}$ and $M \in \mathbb{R}^{m \times n}$.}
	\label{alg:SOAP}
\end{algorithm}

We next compare COSMOS with MUON (Algorithm~\ref{alg:MUON}) and have the following two remarks:

\begin{remark} The NS transformation in MUON behaves like a fixed point iteration without changing the eigensubspace of $M_t$. Therefore, the resulting preconditioning is dedicated to the eigenbases of $M_t$. In contrast, COSMOS adopts a power iteration to the approximate EMA of $G_t^\top G_t$ for incrementally updating the projection matrix $U_t$ from $U_{t-1}$. Therefore, we can separately control the training dynamics of $U_t$, yielding a smoother transition of eigenbases for preconditioning, which potentially leads to better optimization performance.
\end{remark}

\begin{remark}  We propose a new normalization procedure \texttt{NORM} in \eqref{new-norm}. It is motivated by the observation that \texttt{NS5} is only an approximation of the standard Newton Schulz iteration, and the singular values of its output are not exactly $1$'s, especially for small singular values. Therefore, the Frobenius norm of the output may substantially deviate from the desirable $\sqrt{n}$ (corresponding to the case where all singular values equal to $1$). In contrast, our normalization procedure guarantees the Frobenius norm of the output to be exactly $\sqrt{n}$, which improves training stability.
\end{remark}

\begin{algorithm}[htb!]
	\begin{algorithmic}[1]
        \INPUT{Learning rate $\eta$, momentum parameters $\beta_1$. For simplicity, we omit the initialization.}
        \FOR{$t=0,...$}
		\STATE Sample batch $\cM_t$
		\STATE $G_t \gets \nabla_W \phi_{\cM_t}(W_t)$
        \STATE $M_t \gets \beta_1M_{t-1}+(1-\beta_1)G_t$
        \STATE $B_t \gets \texttt{NS5}(M_t/\norm{M_t}_{\rm F})$
        \STATE $\displaystyle W_{t+1} \gets W_{t} -\eta B_t\cdot \sqrt{m}$
        \ENDFOR
	\end{algorithmic}
	\caption{MUON for an $m \times n$ layer $W$. Per layer, we maintain one matrix: $M \in \mathbb{R}^{m \times n}$.}
	\label{alg:MUON}
\end{algorithm}

\vskip2pt
\noindent \textbf{Memory Usage Comparison:} For comparison, we list the memory usage of the optimization states in Adam, Adam-mini, SOAP, MUON and COSMOS for training transformer models in Table \ref{tab:pred-memory}. For simplicity, we assume that the attention weight matrices $W_Q,W_K,W_V,W_O\in \RR^{d\times d}$ and the MLP weight matrices $W_1\in \RR^{d\times 4d}$ and $W_2\in \RR^{4d\times d}$. Note that in practical LLMs, the dimensionalities of $W_1$ and $W_2$ might slightly vary. Moreover, we assume that the rank of the projection is $r=0.05d$ for COSMOS.

We remark that Table \ref{tab:pred-memory} only compares the optimization states. In practice, however, besides the optimization states, the overall memory usage also includes the model weights and intermediate variables used in the forward and backward passes as well as additional memory overhead of I/O and computation. Therefore, we present a more detailed and practical memory profiling for training LLaMA-1B model in Section \ref{experiments}.

\begin{table}[htb!]
    \caption{Memory usage of the optimization states in different algorithms for training transformer models.}
    \label{tab:pred-memory}
    \centering
    \begin{tabular}{ccccc}
    \toprule
    Adam &Adam-mini &SOAP &MUON &COSMOS\\
    \hline
    24$d^2$& 12$d^2$ &$66d^2$ &12$d^2$ &13.5$d^2$\\
    \bottomrule
    \end{tabular}
\end{table}

\vskip2pt
\noindent\textbf{Two-sided COSMOS.} For simplicity, we only consider the one-side version of COSMOS in this paper. Like SOAP, COSMOS can be further generalized to a two-sided version. See more details in Appendix \ref{sec:COSMOS-two-side}.