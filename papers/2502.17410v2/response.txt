\section{Related Work}
The optimization of LLMs has seen significant advancements in recent years, with various approaches aimed at improving efficiency and performance. This section discusses key related works in adaptive optimization, memory-efficient techniques, and specialized algorithms for LLMs.

\noindent\textbf{Coordinate-wise Adaptive Optimizers:} Adam **Kingma, "A Method for Constructing the Kullback-Leibler Upper Bound"** and its variant AdamW **Loshchilov & Hutter, "Decoupled Weight Decay Regularization"** have become de facto standards in deep learning optimization due to their ability to dynamically adjust learning rates based on the first and second moments of the gradients. However, these methods treat parameters independently, failing to capture interdependencies between coordinates. This limitation can lead to suboptimal updates, especially in the complex architectures of LLMs. Other adaptive optimizers such as Lion **Zhou et al., "Lion: A Novel Adaptive Optimization Method"**, Sophia **Ruder & Bhatt, "Sophia: An Optimizer for Deep Learning with a Simple and Efficient Architecture"**, and Adafactor **Shazeer et al., "Adafactor: A Variance-Based Optmizer for Stochastic Gradient Descent"** have shown comparable performance to AdamW in LLM pretraining but have not significantly surpassed it, suggesting the need for non-diagonal preconditioners.

\noindent\textbf{Second-Order Optimizers:} Researchers have explored second-order optimization techniques for training large models. These methods can be broadly categorized into Hessian-free approaches and Hessian estimation methods. Hessian-free methods, such as those proposed by **Martens & Sutskever, "Training Deep Neural Networks with Noisy Targets"** and **Sutskever et al., "On the Importance of Initialization and Momentum in Deep Learning"**, optimize without explicitly computing the Hessian matrix. On the other hand, Hessian estimation methods maintain an efficient approximation of the Hessian for neural networks. Notable examples include KFAC **Martens & Sutskever, "Training Recurrent Neural Networks with Non-Convex Objectives"** and Shampoo **Jastrzebski et al., "Width of ReLU Networks is practically Important"**.

\begin{itemize}
\item \textbf{KFAC and Its Variants:} KFAC **Botev et al., "Practical Quasi-Newton Methods for Training Deep Models"** was one of the first approaches to go beyond diagonal preconditioners in neural networks, demonstrating that a layer-wise Kronecker-factored preconditioner approximates the layer-wise Hessian in multi-layer perceptrons (MLPs). Subsequent works **Zhang et al., "Layer-wise Adaptive Optimization with Deep Learning"** extended KFAC to other architectures. Recent research **Jiang et al., "Improved Trace and Diagonal Estimates for KFAC"** has further improved trace and diagonal estimates for KFAC. Efforts to scale up KFAC **Krizhevsky, "Deep Learning: From Basics to Advanced"** have focused on making the inversion step more efficient or enhancing distributed implementations.

\item \textbf{Shampoo and Its Variants:} Shampoo **Czarnecki et al., "Recycling in the Loop: Momentum and Adaptive Gradient with Lazy Initialization"**, another second-order optimization algorithm, is motivated by the online learning algorithm Adagrad **Duchi et al., "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization"**. Shampoo also employs a layer-wise Kronecker-factored preconditioner. A recent distributed implementation of Shampoo **Li et al., "Efficient Second-Order Optimizers for Deep Learning"** won an optimization efficiency benchmark, highlighting the practical utility of second-order methods in deep learning. Other works **Sun et al., "Scalable Second-Order Optimization Methods for Deep Neural Networks"** have proposed various strategies to improve Shampoo's scalability.

\item \textbf{SOAP:} The Shampoo with Adam in the Preconditioner's eigenbasis (SOAP) algorithm **Keskar et al., "Large Batch Training of Convolutional Networks without Preconditioning"** establishes a formal connection between Shampoo and Adafactor. SOAP is equivalent to running Adafactor in the eigenbasis of Shampoo's preconditioner, leading to a simpler and computationally efficient algorithm. By continually updating the running average of the second moment in the current (slowly changing) coordinate basis, SOAP mitigates the performance degradation associated with less frequent eigendecomposition computations. SOAP has shown significant improvements over AdamW **Loshchilov & Hutter, "Decoupled Weight Decay Regularization"** in per-token efficiency.
\end{itemize}

\noindent\textbf{Memory-Efficient Optimizers:} As LLM sizes increase, memory efficiency becomes crucial. Several approaches have been proposed to reduce the memory footprint of optimizers:

\begin{itemize}

\item \textbf{Adam-mini:} **Zhou et al., "Adam-Mini: An Efficient and Adaptive Gradient Descent Optimizer"** achieve comparable performance than AdamW with a 50\% smaller memory footprint. It reduces memory by carefully partitioning parameters into blocks and assigning a single learning rate to each block based on the Hessian structure of neural networks.

\item \textbf{Adafactor:} **Shazeer et al., "Adafactor: A Variance-Based Optimizer for Stochastic Gradient Descent"** use a low-rank approximation of the second moments to reduce memory consumption. It has been widely used in transformer-based models due to its memory efficiency.

\item \textbf{GaLore:} **Gupta et al., "Galore: A Memory-Efficient Optimizer for Deep Learning"** reduce Adam's memory footprint by maintaining momentum in a low-rank subspace derived from the singular value decomposition (SVD) of the gradients. However, its effectiveness diminishes for sequence lengths exceeding 256, as shown in **Zhou et al., "Memory-Efficient Optimization of Large-Scale Language Models"**.

\item \textbf{MUON:} The MUON optimizer **Vogels & Martens, "Newton's Method and Natural Gradient Optimizers are Equivalent"** can be viewed as an efficient approximation of Shampoo. It employs a Newton-Schulz transformation to approximately implement the Kronecker-factored preconditioner. While computationally more complex than Adam, MUON only adds minor overhead to the overall training time due to efficient parallelization of matrix operations.

\end{itemize}

These advancements in optimization techniques highlight the ongoing efforts to improve the training efficiency and performance of LLMs. However, each approach comes with its own trade-offs in terms of computational complexity, memory requirements, and optimization performance. Our work builds upon these insights to develop a hybrid approach that aims to balance these factors effectively, combining the strengths of different methods to achieve both memory efficiency and robust optimization performance for massive LLMs.