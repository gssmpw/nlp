\section{Introduction}

The optimization of Large Language Models (LLMs) is fundamental to their success, enabling these models to achieve state-of-the-art performance across diverse tasks. However, the high-dimensional and non-convex loss landscapes inherent to LLMs, which can contain hundreds of billions or even trillions of parameters \citep{brown2020language,achiam2023gpt}, present significant optimization challenges. Adaptive optimizers, such as Adam \citep{kingma2014adam} and its variants AdamW \citep{loshchilov2017decoupled}, have emerged as de facto standards due to their ability to dynamically adjust learning rates based on the second moment of the gradient matrix. Despite their widespread adoption, these methods suffer from two critical limitations that impede their effectiveness and scalability in the context of increasingly large and complex LLMs:

\begin{itemize}
\item One limitation of Adam and its variants lies in its coordinate-wie use of adaptive learning rates. While this approach dynamically adjusts learning rates for individual parameters based on their gradient history, it inherently fails to capture interdependencies between coordinates. By treating each parameter independently, this method essentially implements an efficient diagonal approximation of the preconditioning matrix. Although this reduces computational complexity, it inadequately represents the intricate curvature information of the loss landscape, particularly in the high-dimensional spaces characteristic of LLMs. Consequently, this approach often leads to suboptimal parameter updates, especially in scenarios where parameter interactions are significant, such as in the complex architectures of large language models \citep{zhang2024transformers}.

\item Another limitation of Adam and its variants lies in the substantial memory requirement for storing per-parameter adaptive learning rates and gradient statistics. As LLM sizes increase, it becomes prohibitively large, impeding scalability. 
\end{itemize}

To address the limitations of Adam and its variants, researchers have pursued two main approaches. The first approach, exemplified by algorithms such as Shampoo \citep{gupta2018shampoo} and the more recent SOAP \citep{vyas2024soap}, employs sophisticated techniques to capture curvature information and parameter interdependencies. These methods utilize rotational matrices, derived through (approximate) singular value decomposition (SVD) of the gradient matrix, to provide a more comprehensive representation of the loss landscape's geometry. This approach allows for a better approximation of the full preconditioning matrix, enabling the capture of inter-coordinate dependencies. However, the improved capability of representing parameter interactions comes at the cost of substantial computational and memory overhead (approximately twice of memory usage of Adam for transformers), rendering these algorithms challenging to implement for large-scale LLMs, where memory efficiency is crucial.

The second approach focuses on reducing memory consumption through various approximation techniques. Algorithms such as AdaFactor \citep{shazeer2018adafactor} and Adam-mini \citep{zhang2024adam} aim to decrease memory usage by approximating the second moment of the gradient matrix. Adam-mini employs a component-specific approach, averaging second moments neuron-wise for certain layers (e.g., output embeddings, value projections, and MLP layers) and head-wise for others (e.g., query and key projections). Meanwhile, AdaFactor utilizes a rank-1 approximation of the second moments. While these methods effectively reduce memory consumption, their approximations often oversimplify the complex structure of the gradient matrix's moments, leading to a significant loss of critical curvature information and potentially compromising optimization performance. The trade-off between memory efficiency and the preservation of essential gradient statistics remains a crucial challenge in the development of scalable optimizers for LLMs.

More recent approaches, such as GaLore \citep{zhao2024galore} and MUON \citep{jordan2024MUON}, have attempted to strike a balance between computational complexity, memory consumption, and optimization performance in LLM training. GaLore, which can be viewed as a memory-efficient variant of SOAP, approximates the first and second moments of the gradient matrix in the leading eigensubspace. While this approach effectively reduces memory consumption, \citet{liang2024memory} find that its effectiveness diminishes for sequence lengths exceeding 256. MUON, essentially an approximation of Shampoo based on some Newton-Schulz transformation proposed in \citet{bernstein2024modular}, aims to decrease computational complexity. However, this algorithm tends to overfit to the eigensubspaces of the gradient matrix at the current iteration, failing to account for their dynamic nature throughout the optimization process. These methods, while innovative, highlight the ongoing challenge of developing optimizers that are both scalable and effective for LLMs. 

In this paper, we propose COSMOS, a novel hybrid optimizer that addresses the limitations of existing methods by exploiting the varying importance of eigensubspaces in the gradient matrix. Our approach decomposes the gradient into two parts: a projection onto the leading eigensubspace and a projection onto the remaining eigensubspace. The leading eigensubspace captures the most significant directions of change in the gradient, typically corresponding to the most important optimization dynamics. For this part, we apply a SOAP-like optimization strategy, tailored specifically to this reduced-dimensional space. The remaining eigensubspace, while less critical, still significantly influences optimization performance.

To address this, we employ MUON as a more efficient alternative to SOAP for this high-dimensional space. Such a hybrid approach allows COSMOS to maintain optimization effectiveness while significantly reducing memory requirements, potentially enabling the training of even larger LLMs or the use of increased batch sizes. Central to our method are (1) the exponential moving averages (EMAs) of the gradient matrix's first and second moments, which provide smoothed estimates of recent gradient statistics, enhancing the stability of parameter updates and (2) an efficient one-step power update to identify the top $k$ eigenvectors (where $k$ is usually small, only $5\%$--$10\%$ of the original dimension) that capture these crucial gradient directions, designating them as the leading eigensubspace. 
The design of COSMOS is motivated by empirical insights and practical considerations gleaned from extensive experimentation with large language models, offering a promising solution for scaling up LLM training.

We highlight the key contributions of this paper as follows:

\begin{itemize}
\item We propose a novel hybrid optimization strategy. This leads us to develop the COSMOS algorithm, which synergizes the strengths of SOAP and MUON by decomposing the gradient matrix into eigensubspaces of varying importance.

\item COSMOS achieves significant memory consumption reduction compared to the SOAP algorithm, while achieving equally or better optimization performance.

\item Unlike GaLore, COSMOS's performance does not degrade when the sequence length exceeds 256.
\end{itemize}

The rest of the paper is organized as follows: Section 2 reviews the related work; Section 3 presents the COMOS algorithm; Section 4 presents our extensive numerical experiments; Section 5 draws a brief conclusion.
