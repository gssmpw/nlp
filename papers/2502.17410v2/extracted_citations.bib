@article{anil2020scalable,
  title={Scalable second order optimization for deep learning},
  author={Anil, Rohan and Gupta, Vineet and Koren, Tomer and Regan, Kevin and Singer, Yoram},
  journal={arXiv preprint arXiv:2002.09018},
  year={2020}
}

@inproceedings{ba2017distributed,
  title={Distributed Second-Order Optimization using Kronecker-Factored Approximations},
  author={Ba, Jimmy and Grosse, Roger and Martens, James},
  booktitle={International Conference on Learning Representations},
  year={2017}
}

@article{chen2023symbolic,
  title={Symbolic Discovery of Optimization Algorithms},
  author={Chen, Xiangning and Liang, Chen and Huang, Da and Real, Esteban and Wang, Kaiyuan and Liu, Yao and Pham, Hieu and Dong, Xuanyi and Luong, Thang and Hsieh, Cho-Jui and others},
  journal={arXiv e-prints},
  pages={arXiv--2302},
  year={2023}
}

@article{dahl2023benchmarking,
  title={Benchmarking neural network training algorithms},
  author={Dahl, George E and Schneider, Frank and Nado, Zachary and Agarwal, Naman and Sastry, Chandramouli Shama and Hennig, Philipp and Medapati, Sourabh and Eschenhagen, Runa and Kasimbeg, Priya and Suo, Daniel and others},
  journal={arXiv preprint arXiv:2306.07179},
  year={2023}
}

@article{duchi2011adaptive,
  title={Adaptive subgradient methods for online learning and stochastic optimization.},
  author={Duchi, John and Hazan, Elad and Singer, Yoram},
  journal={Journal of machine learning research},
  volume={12},
  number={7},
  year={2011}
}

@article{eschenhagen2023kronecker,
  title={Kronecker-Factored Approximate Curvature for Modern Neural Network Architectures},
  author={Eschenhagen, Runa and Immer, Alexander and Turner, Richard E and Schneider, Frank and Hennig, Philipp},
  journal={arXiv preprint arXiv:2311.00636},
  year={2023}
}

@inproceedings{gao2021trace,
  title={A trace-restricted kronecker-factored approximation to natural gradient},
  author={Gao, Kaixin and Liu, Xiaolei and Huang, Zhenghai and Wang, Min and Wang, Zidong and Xu, Dachuan and Yu, Fan},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={9},
  pages={7519--7527},
  year={2021}
}

@article{george2018fast,
  title={Fast Approximate Natural Gradient Descent in a Kronecker-factored Eigenbasis},
  author={George, Thomas and Laurent, C{\'e}sar and Bouthillier, Xavier and Ballas, Nicolas and Vincent, Pascal},
  journal={arXiv preprint arXiv:1806.03884},
  year={2018}
}

@inproceedings{gupta2018shampoo,
  title={Shampoo: Preconditioned stochastic tensor optimization},
  author={Gupta, Vineet and Koren, Tomer and Singer, Yoram},
  booktitle={International Conference on Machine Learning},
  pages={1842--1850},
  year={2018},
  organization={PMLR}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@article{liang2024memory,
  title={Memory-efficient llm training with online subspace descent},
  author={Liang, Kaizhao and Liu, Bo and Chen, Lizhang and Liu, Qiang},
  journal={arXiv preprint arXiv:2408.12857},
  year={2024}
}

@article{lin2024can,
  title={Can we remove the square-root in adaptive gradient methods? a second-order perspective},
  author={Lin, Wu and Dangel, Felix and Eschenhagen, Runa and Bae, Juhan and Turner, Richard E and Makhzani, Alireza},
  journal={arXiv preprint arXiv:2402.03496},
  year={2024}
}

@article{liu2023sophia,
  title={Sophia: A scalable stochastic second-order optimizer for language model pre-training},
  author={Liu, Hong and Li, Zhiyuan and Hall, David and Liang, Percy and Ma, Tengyu},
  journal={arXiv preprint arXiv:2305.14342},
  year={2023}
}

@article{loshchilov2017decoupled,
  title={Decoupled weight decay regularization},
  author={Loshchilov, I},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}

@inproceedings{martens2010deep,
  title={Deep learning via Hessian-free optimization},
  author={Martens, James},
  booktitle={Proceedings of the 27th International Conference on International Conference on Machine Learning},
  pages={735--742},
  year={2010}
}

@article{martens2015optimizing,
  title={Optimizing Neural Networks with Kronecker-factored Approximate Curvature},
  author={Martens, James and Grosse, Roger},
  journal={arXiv preprint arXiv:1503.05671},
  year={2015}
}

@inproceedings{martens2018kronecker,
  title={Kronecker-factored Curvature Approximations for Recurrent Neural Networks},
  author={Martens, James and Ba, Jimmy and Johnson, Matt},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@article{osawa2018large,
  title={Large-Scale Distributed Second-Order Optimization Using Kronecker-Factored Approximate Curvature for Deep Convolutional Neural Networks},
  author={Osawa, Kazuki and Tsuji, Yohei and Ueno, Yuichiro and Naruse, Akira and Yokota, Rio and Matsuoka, Satoshi},
  journal={arXiv preprint arXiv:1811.12019},
  year={2018}
}

@inproceedings{peirson2022fishy,
  title={Fishy: Layerwise Fisher Approximation for Higher-order Neural Network Optimization},
  author={Peirson, Abel and Amid, Ehsan and Chen, Yatong and Feinberg, Vladimir and Warmuth, Manfred K and Anil, Rohan},
  booktitle={Has it Trained Yet? NeurIPS 2022 Workshop},
  year={2022}
}

@article{puiu2022brand,
  title={Brand New K-FACs: Speeding up K-FAC with Online Decomposition Updates},
  author={Puiu, Constantin Octavian},
  journal={arXiv preprint arXiv:2210.08494},
  year={2022}
}

@inproceedings{puiu2022randomized,
  title={Randomized K-FACs: Speeding Up K-FAC with Randomized Numerical Linear Algebra},
  author={Puiu, Constantin Octavian},
  booktitle={International Conference on Intelligent Data Engineering and Automated Learning},
  pages={411--422},
  year={2022}
}

@article{shazeer2018adafactor,
  title={Adafactor: Adaptive Learning Rates with Sublinear Memory Cost},
  author={Shazeer, Noam and Stern, Mitchell},
  journal={arXiv preprint arXiv:1804.04235},
  year={2018}
}

@article{shi2023distributed,
  title={A distributed data-parallel pytorch implementation of the distributed shampoo optimizer for training neural networks at-scale},
  author={Shi, Hao-Jun Michael and Lee, Tsung-Hsien and Iwasaki, Shintaro and Gallego-Posada, Jose and Li, Zhijing and Rangadurai, Kaushik and Mudigere, Dheevatsa and Rabbat, Michael},
  journal={arXiv preprint arXiv:2309.06497},
  year={2023}
}

@article{vyas2024soap,
  title={Soap: Improving and stabilizing shampoo using adam},
  author={Vyas, Nikhil and Morwani, Depen and Zhao, Rosie and Shapira, Itai and Brandfonbrener, David and Janson, Lucas and Kakade, Sham},
  journal={arXiv preprint arXiv:2409.11321},
  year={2024}
}

@article{wang20244,
  title={4-bit Shampoo for Memory-Efficient Network Training},
  author={Wang, Sike and Zhou, Pan and Li, Jia and Huang, Hua},
  journal={arXiv preprint arXiv:2405.18144},
  year={2024}
}

@inproceedings{zhai2022scaling,
  title={Scaling vision transformers},
  author={Zhai, Xiaohua and Kolesnikov, Alexander and Houlsby, Neil and Beyer, Lucas},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={12104--12113},
  year={2022}
}

@article{zhang2024adam,
  title={Adam-mini: Use fewer learning rates to gain more},
  author={Zhang, Yushun and Chen, Congliang and Li, Ziniu and Ding, Tian and Wu, Chenwei and Ye, Yinyu and Luo, Zhi-Quan and Sun, Ruoyu},
  journal={arXiv preprint arXiv:2406.16793},
  year={2024}
}

@article{zhao2024deconstructing,
  title={Deconstructing what makes a good optimizer for language models},
  author={Zhao, Rosie and Morwani, Depen and Brandfonbrener, David and Vyas, Nikhil and Kakade, Sham},
  journal={arXiv preprint arXiv:2407.07972},
  year={2024}
}

@article{zhao2024galore,
  title={GaLore: Memory-efficient llm training by gradient low-rank projection},
  author={Zhao, Jiawei and Zhang, Zhenyu and Chen, Beidi and Wang, Zhangyang and Anandkumar, Anima and Tian, Yuandong},
  journal={arXiv preprint arXiv:2403.03507},
  year={2024}
}

