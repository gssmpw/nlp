\section{Related Work}
The optimization of LLMs has seen significant advancements in recent years, with various approaches aimed at improving efficiency and performance. This section discusses key related works in adaptive optimization, memory-efficient techniques, and specialized algorithms for LLMs.

\noindent\textbf{Coordinate-wise Adaptive Optimizers:} Adam ____ and its variant AdamW ____ have become de facto standards in deep learning optimization due to their ability to dynamically adjust learning rates based on the first and second moments of the gradients. However, these methods treat parameters independently, failing to capture interdependencies between coordinates. This limitation can lead to suboptimal updates, especially in the complex architectures of LLMs. Other adaptive optimizers such as Lion ____, Sophia ____, and Adafactor ____ have shown comparable performance to AdamW in LLM pretraining but have not significantly surpassed it, suggesting the need for non-diagonal preconditioners.

\noindent\textbf{Second-Order Optimizers:} Researchers have explored second-order optimization techniques for training large models. These methods can be broadly categorized into Hessian-free approaches and Hessian estimation methods. Hessian-free methods, such as those proposed by ____ and ____, optimize without explicitly computing the Hessian matrix. On the other hand, Hessian estimation methods maintain an efficient approximation of the Hessian for neural networks. Notable examples include KFAC ____ and Shampoo ____.

\begin{itemize}
\item \textbf{KFAC and Its Variants:} KFAC ____ was one of the first approaches to go beyond diagonal preconditioners in neural networks, demonstrating that a layer-wise Kronecker-factored preconditioner approximates the layer-wise Hessian in multi-layer perceptrons (MLPs). Subsequent works ____ extended KFAC to other architectures. Recent research ____ has further improved trace and diagonal estimates for KFAC. Efforts to scale up KFAC ____ have focused on making the inversion step more efficient or enhancing distributed implementations.

\item \textbf{Shampoo and Its Variants:} Shampoo ____, another second-order optimization algorithm, is motivated by the online learning algorithm Adagrad ____. Shampoo also employs a layer-wise Kronecker-factored preconditioner. A recent distributed implementation of Shampoo ____ won an optimization efficiency benchmark ____, highlighting the practical utility of second-order methods in deep learning. Other works ____ have proposed various strategies to improve Shampoo's scalability.

\item \textbf{SOAP:} The Shampoo with Adam in the Preconditioner's eigenbasis (SOAP) algorithm ____ establishes a formal connection between Shampoo and Adafactor. SOAP is equivalent to running Adafactor in the eigenbasis of Shampoo's preconditioner, leading to a simpler and computationally efficient algorithm. By continually updating the running average of the second moment in the current (slowly changing) coordinate basis, SOAP mitigates the performance degradation associated with less frequent eigendecomposition computations. SOAP has shown significant improvements over AdamW in per-token efficiency.
\end{itemize}

\noindent\textbf{Memory-Efficient Optimizers:} As LLM sizes increase, memory efficiency becomes crucial. Several approaches have been proposed to reduce the memory footprint of optimizers:

\begin{itemize}

\item \textbf{Adam-mini:} ____ achieve comparable performance than AdamW with a 50\% smaller memory footprint. It reduces memory by carefully partitioning parameters into blocks and assigning a single learning rate to each block based on the Hessian structure of neural networks. 

\item \textbf{Adafactor:} ____ use a low-rank approximation of the second moments to reduce memory consumption. It has been widely used in transformer-based models due to its memory efficiency.

\item \textbf{GaLore:} ____ reduce Adam's memory footprint by maintaining momentum in a low-rank subspace derived from the singular value decomposition (SVD) of the gradients. However, its effectiveness diminishes for sequence lengths exceeding 256, as shown in ____.

\item \textbf{MUON:} The MUON optimizer ____ can be viewed as an efficient approximation of Shampoo. It employs a Newton-Schulz transformation to approximately implement the Kronecker-factored preconditioner. While computationally more complex than Adam, MUON only adds minor overhead to the overall training time due to efficient parallelization of matrix operations.

\end{itemize}

These advancements in optimization techniques highlight the ongoing efforts to improve the training efficiency and performance of LLMs. However, each approach comes with its own trade-offs in terms of computational complexity, memory requirements, and optimization performance. Our work builds upon these insights to develop a hybrid approach that aims to balance these factors effectively, combining the strengths of different methods to achieve both memory efficiency and robust optimization performance for massive LLMs.