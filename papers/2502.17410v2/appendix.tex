\section{Experiment Details}\label{sec:details}
Many aspects of our setup such as models are the same as in \citet{zhao2024galore}. We train language models on C4 tokenized with the T5 tokenizer \citep{raffel2020exploring} and report results in terms of validation loss.

{\bf Models}. We start from the GaLore Codebase \citep{zhao2024galore} and train LLaMA models of
three sizes: 130M, 350M, and 1B. 
The models have widths of 768, 1024, and 2048 and depths of 12, 16, and 24. 
We use the 130M model to explore various ablations as shown in \Cref{sec:exp-130m}. 
The MLP hidden dimension of the 130M model is $4$ times the width and the hidden dimension of the 350M and 1B model is $\frac{8}{3}$ times the width. 
The activation function is SiLU \citep{elfwing2018sigmoid}. 
The architecture uses RoPE positional encodings \citep{su2024roformer}. 
Attention heads are always dimension 64. 
For more architecture details please refer to \citet{zhao2024galore}. 
We train in mixed precision with FP32.

{\bf Algorithms}. We use the standard Pytorch implementation of Adam, and the official GaLore implementation provided by \citet{zhao2024galore}. Since Two-Sided SOAP is too memory consuming and is not within our comparison scope, we modify the code provided by \citet{vyas2024soap} to apply One-Sided SOAP discussed in \citet{vyas2024soap}. We use the official Adam-mini implementation provided by \citet{zhang2024adam}. For MUON and \texttt{NS5}, we use their implementation provided by \citet{jordan2024MUON} in their Github records. We implement our COSMOS starting from an older version of Pytorch implementation of AdamW.

{\bf Default hyperparameters.} In all algorithms, we choose momentum $\beta_1 = 0.9$, $\beta_2 = 0.98$ and smoothing term $\epsilon = \text{1e-8}$ to align with the standard hyperparameter choice of Adam. We use the linear learning rate schedule to decay the learning rate to 0. To align with \citet{zhao2024galore}, we set the warmup ratio to be 10\% and weight decay to be 0.

{\bf Token counts.} For all of our runs we use a sequence length of 1024. For the 130M model, we set the batch size to be 960, and for the 350M and 1B models, we set the batch size to be 2000. We train the 130M and 350M models for 5k steps and train the 1B model for 13k steps. Thus the number of training tokens for the  130M mode $\approx$ 5B, which is beyond the “chinchilla optimal” number of tokens. The numbers of training tokens for the 350M model and 1B model are 10B and 26B respectively, which follow the chinchilla optimal” number of tokens.

\subsection{Learning rate tuning}

To avoid unfair comparisons caused by excessive hyperparameter tuning, for all algorithms we set the learning rate as the only tunable hyperparameter in all the main results in \Cref{experiments}. The rank $r$ for COSMOS for all main results is fixed at 64.

\subsubsection{Tuning on 130M model}
\label{130M_tuning}

For Adam, we tune the learning rate on $\{$5e-4, 1e-3, 2e-3, 4e-3, 8e-3$\}$. In our experiments, 2e-3 is the optimal learning rate and 8e-3 diverges. Then for SOAP, we also tune the learning rate on $\{$5e-4, 1e-3, 2e-3, 4e-3$\}$. For Adam-mini, we just use the optimal learning rate of Adam, which is 2e-3.

For GaLore, We follow the setting in \citet{zhao2024galore}, set rank=256, and scale factor $\alpha = 0.25$. The projection update frequency is 200 for 20k training steps, thus we decrease it to 50 for our 5k training steps. According to \citet{zhao2024galore}, the learning rate should be larger than Adam's. 

For the implementation of MUON and COSMOS, the embedding and output layer will use Adam while other parts will use MUON/COSMOS algorithm. To avoid multiple hyperparameter tuning, we fix the learning rate for embedding and output layer to 2e-3 and only tune the learning rate of hidden layers, whose optimizer is MUON/COSMOS. To be more specific, we tune the learning rate of hidden layers on $\{$2e-4, 5e-4, 1e-3, 2e-3$\}$.

For COSMOS, as we mentioned before, to avoid tuning $\gamma$, we simply set $\gamma$ to be the ratio of the learning rate of hidden layers to the learning rate of the embedding layer (which is fixed at 2e-3). We find that this trick can provide a satisfactory result without extra tuning on $\gamma$. Please note that we find in many extra experiments that this trick isn't the optimal choice for $\gamma$. Tuning $\gamma$ may output a better result.

\subsubsection{Tuning on 350M model}\label{sec:config-350m}
For Adam and SOAP, we tune the learning rate on $\{$5e-4, 1e-3, 2e-3, 4e-3$\}$. For GaLore, we set the rank to be 384, projection update frequency to be 50, and scale factor $\alpha = 0.25$. Then we tune the learning rate of GaLore on $\{$5e-3, 1e-2, 2e-2, 4e-2$\}$. 

For the implementation of MUON and COSMOS, we still fix the learning rate for embedding and output layer to be 2e-3 and only tune the learning rate of MUON/COSMOS for hidden layers. For MUON and COSMOS, we tune the learning rate on $\{$2e-4, 5e-4, 1e-3$\}$. Also for COSMOS, we still set $\gamma$ to be the ratio of the learning rate of hidden layers to the learning rate of the embedding layer (which is fixed at 2e-3).

\subsubsection{Tuning on 1B Model}\label{sec:config-1b}
We do not have enough resources to tune hyperparameters carefully on the 1B model. For Adam, we first try learning rate $\eta=\text{2e-3}$, but an extremely large loss spike occurred in the early stage. Then we decrease $\eta$ to 1e-3 and get the result. For MUON and COSMOS, we still fix the learning rate for embedding and output layer to be 2e-3 and tune their learning rate on $\{$2e-4, 5e-4$\}$. For COSMOS, we still set $\gamma$ to be the ratio of the learning rate of hidden layers to the learning rate of the embedding layer.

\subsection{Ablation of $r$ and $\gamma$ on 130M model}
For the ablation experiments on COSMOS in \Cref{sec:exp-130m}, we tune discount factor $\gamma$ and rank $r$ together to show COSMOS isn't very sensitive to hyperparameters. We fix the learning rate for embedding and output layers to be $2e-3$, fix the learning rate for COSMOS to be 5e-4, and sweep over the cross product of $r \in \{32, 64, 128\}$ and $\gamma \in \{0.1, 0.25, 0.5, 1\}$. With all these hyperparameters COSMOS outputs comparable results to MUON.

\subsection{Ablation of normalization}
As mentioned in the normalization paragraph in \Cref{sec:exp-130m}, to exclude the possibility that the better performance of COSMOS than MUON is simply because the normalization function \texttt{NORM}, we modify the normalization method of MUON to be \texttt{NORM} and rerun the experiments for 130M and 350M models. We still tune the learning rate of MUON $+$ \texttt{NORM} on $\{$5e-3, 1e-2, 2e-2, 4e-2$\}$, and present their best performance.

\subsection{Profiling experiments}
We do the profiling experiments on the 1B model. We set the sequence length to 1024, which aligns with our previous settings. We set batch size 10 and accumulation steps 25. Then we record the maximum GPU memory usage and time usage on this setting by using Pytorch API during the entire forward-backward and optimizer update process.



\section{Two-sided COSMOS}\label{sec:COSMOS-two-side}
Similar to two-sided SOAP in \citet{vyas2024soap}, we provide a two-sided variant of COSMOS in \Cref{alg:COSMOS-two-side}. 

\begin{algorithm}[htb!]
	\begin{algorithmic}[1]
        \INPUT{Learning rate $\eta$, combination weight $\gamma$, projection rank $r\ll n$, momentum parameters $(\beta_1,\beta_2)$, perturbation parameter $\epsilon$. For simplicity, we omit the initialization.}
        \FOR{$t=0,...$}
		\STATE Sample batch $\cM_t$
		\STATE $G_t \gets \nabla_W \phi_{\cM_t}(W_t)$
        \STATE $M_t \gets \beta_1M_{t-1}+(1-\beta_1)G_t$
        \STATE $U_t \gets \texttt{QR}(\beta_2U_{t-1}S_{t-1}+(1-\beta_2)G_t^\top G_tU_{t-1})$
        \STATE $O_t \gets \texttt{QR}(\beta_2R_{t-1}O_{t-1}+(1-\beta_2)G_t G_t^\top O_{t-1})$
        \STATE $S_t \gets U_t^\top(\beta_2 U_{t-1}S_{t-1}U_{t-1}^\top + (1-\beta_2)G_t^\top G_t)U_t$
        \STATE $R_t \gets O_t^\top(\beta_2 O_{t-1}R_{t-1}O_{t-1}^\top + (1-\beta_2)G_t G_t^\top)O_t$
		\STATE $V_t \gets \beta_2 V_{t-1} + (1-\beta_2) (O_t^\top G_tU_t) \odot (O_t^\top G_tU_t)$
        %\ENDIF
		\STATE $\displaystyle A_t = O_t\left(\frac{O_t^\top M_tU_t/(1-\beta_1^t)}{\sqrt{(V_t+\epsilon)/(1-\beta_2^t)}}\right)U_t^\top$
        \STATE $\displaystyle B_t \gets \texttt{NORM} \left(\texttt{NS5}\left(\frac{M_t-O_t^\top O_t M_tU_tU_t^\top}{\norm{M_t-O_t^\top O_t M_tU_tU_t^\top}_{\rm F}}\right)\right)$
        \STATE $\displaystyle \tilde{G}_t \gets A_t + \gamma \cdot B_t\cdot\sqrt{m}$ 
		\STATE $\displaystyle W_{t+1} \gets W_{t} -\eta \cdot\texttt{NORM}(\tilde{G}_t)$
        \ENDFOR
	\end{algorithmic}
	\caption{Two-sided version of COSMOS for a $m \times n$ layer. Per layer, we maintain six matrices: $U \in \mathbb{R}^{n \times r}$, $O\in\mathbb{R}^{m \times r}$, $S,R\in \mathbb{R}^{r \times r}$, $V\in\RR^{m\times r}$ and $M \in \mathbb{R}^{m \times n}$.}
	\label{alg:COSMOS-two-side}
\end{algorithm}

\section{Additional Experiments}
This section provides supplementary experiments not presented in the main text. 
\subsection{Additional Experiments for Normalization Ablation}
\label{sec:exp-350m-norm}
We conduct additional experiments on LLaMA-350M to further validate that normalization is not the main reason for COSMOS outperforming MUON.
As shown in \Cref{fig:exp-350m-norm}, normalization does not make much difference to MUON, while COSMOS consistently outperforms both of them. 

\begin{figure}[htb!]
    \centering
    \includegraphics[width=0.55\linewidth]{imgs/350M_norm_ablation.pdf}
    \caption{Comparison of COSMOS, MUON and MUON with normalization for LLaMA-350M on C4. MUON's performance does not differ by much with normalization, implying that normalization is not the dominant reason why COSMOS outperforms MUON.}
    \label{fig:exp-350m-norm}
\end{figure}

\subsection{Additional Experiments on WikiText}\label{sec:wiki}
This section provides additional experiments on WikiText \citep{merity2016pointer} and GPT-2 \citep{radford2019language}. To be more specific, we train GPT2-small(125M) and GPT2-medium (355M) on the Wikitext-103 dataset. We discard learnable position embeddings and use RoPE \citep{su2024roformer} as a replacement. For GPT2-small, we use learning rate 4e-3 for the embedding layer and 4e-4 for MUON/COSMOS. 
For GPT2-medium, we use learning rate 2e-3 for the embedding layer and 5e-4 for MUON/COSMOS. 
$\gamma$ is still set to be the ratio of the hidden layer learning rate to the embedding layer learning rate. 

We set the sequence length to be 1024, and the batch size is also 1024. 
We train both models for 5k steps, which means the models are trained on 5B tokens. For such many training tokens on Wikitext-103, overfitting will occur and validation loss will start to increase after training for a certain number of steps. Therefore, we use the training loss as the metric for comparison.

The results for GPT2-small and GPT2-medium are provided in \Cref{fig:exp-gpt2s,fig:exp-gpt2m}, respectively. 
We observe that COSMOS consistently outperforms MUON, showing that it does not overfit any particular model or dataset. 

\begin{figure}[htb!]
    \centering
    \includegraphics[width=0.55\linewidth]{imgs/wiki_loss_gpt2s.pdf}
    \caption{GPT2-small trained on WikiText-103 dataset. COSMOS consistently outperforms MUON. }
    \label{fig:exp-gpt2s}
\end{figure}
\begin{figure}[htb!]
    \centering
    \includegraphics[width=0.55\linewidth]{imgs/wiki_loss_gpt2m.pdf}
    \caption{GPT2-medium trained on WikiText-103 dataset. COSMOS consistently outperforms MUON. }
    \label{fig:exp-gpt2m}
\end{figure}

\subsection{Smaller learning rate for MUON/COSMOS on LLaMA-1B}
As discussed in section \ref{sec:config-1b}, we tune the learning rate for MUON/COSMOS on $\{$2e-4, 5e-4$\}$. We find 5e-4 is better and present its corresponding results in the main text. Here we present the result for 2e-4 in Figure \ref{fig:exp-1b-small}, where COSMOS still outperforms MUON.

\begin{figure}[htb!]
    \centering
    \includegraphics[width=0.55\linewidth]{imgs/1B_small_lr.pdf}
    \caption{LLaMA-1B trained on C4 dataset with learning rate 2e-4 for MUON/COSMOS. COSMOS still outperforms MUON.}
    \label{fig:exp-1b-small}
\end{figure}