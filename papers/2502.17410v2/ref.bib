@misc{jordan2024muon,
  author       = {Keller Jordan and Yuchen Jin and Vlado Boza and You Jiacheng and
                  Franz Cecista and Laker Newhouse and Jeremy Bernstein},
  title        = {Muon: An optimizer for hidden layers in neural networks},
  year         = {2024},
  url          = {https://kellerjordan.github.io/posts/muon/}
}

@article{bernstein2024modular,
  title={Modular Duality in Deep Learning},
  author={Bernstein, Jeremy and Newhouse, Laker},
  journal={arXiv preprint arXiv:2410.21265},
  year={2024}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@article{loshchilov2017decoupled,
  title={Decoupled weight decay regularization},
  author={Loshchilov, I},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}

@inproceedings{gupta2018shampoo,
  title={Shampoo: Preconditioned stochastic tensor optimization},
  author={Gupta, Vineet and Koren, Tomer and Singer, Yoram},
  booktitle={International Conference on Machine Learning},
  pages={1842--1850},
  year={2018},
  organization={PMLR}
}

@article{vyas2024soap,
  title={Soap: Improving and stabilizing shampoo using adam},
  author={Vyas, Nikhil and Morwani, Depen and Zhao, Rosie and Shapira, Itai and Brandfonbrener, David and Janson, Lucas and Kakade, Sham},
  journal={arXiv preprint arXiv:2409.11321},
  year={2024}
}

@article{shazeer2018adafactor,
  title={Adafactor: Adaptive Learning Rates with Sublinear Memory Cost},
  author={Shazeer, Noam and Stern, Mitchell},
  journal={arXiv preprint arXiv:1804.04235},
  year={2018}
}

@article{zhang2024adam,
  title={Adam-mini: Use fewer learning rates to gain more},
  author={Zhang, Yushun and Chen, Congliang and Li, Ziniu and Ding, Tian and Wu, Chenwei and Ye, Yinyu and Luo, Zhi-Quan and Sun, Ruoyu},
  journal={arXiv preprint arXiv:2406.16793},
  year={2024}
}

@article{zhao2024galore,
  title={GaLore: Memory-efficient llm training by gradient low-rank projection},
  author={Zhao, Jiawei and Zhang, Zhenyu and Chen, Beidi and Wang, Zhangyang and Anandkumar, Anima and Tian, Yuandong},
  journal={arXiv preprint arXiv:2403.03507},
  year={2024}
}


@article{liang2024memory,
  title={Memory-efficient llm training with online subspace descent},
  author={Liang, Kaizhao and Liu, Bo and Chen, Lizhang and Liu, Qiang},
  journal={arXiv preprint arXiv:2408.12857},
  year={2024}
}

@article{chen2023symbolic,
  title={Symbolic Discovery of Optimization Algorithms},
  author={Chen, Xiangning and Liang, Chen and Huang, Da and Real, Esteban and Wang, Kaiyuan and Liu, Yao and Pham, Hieu and Dong, Xuanyi and Luong, Thang and Hsieh, Cho-Jui and others},
  journal={arXiv e-prints},
  pages={arXiv--2302},
  year={2023}
}

@article{liu2023sophia,
  title={Sophia: A scalable stochastic second-order optimizer for language model pre-training},
  author={Liu, Hong and Li, Zhiyuan and Hall, David and Liang, Percy and Ma, Tengyu},
  journal={arXiv preprint arXiv:2305.14342},
  year={2023}
}

@inproceedings{zhai2022scaling,
  title={Scaling vision transformers},
  author={Zhai, Xiaohua and Kolesnikov, Alexander and Houlsby, Neil and Beyer, Lucas},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={12104--12113},
  year={2022}
}

@article{shi2023distributed,
  title={A distributed data-parallel pytorch implementation of the distributed shampoo optimizer for training neural networks at-scale},
  author={Shi, Hao-Jun Michael and Lee, Tsung-Hsien and Iwasaki, Shintaro and Gallego-Posada, Jose and Li, Zhijing and Rangadurai, Kaushik and Mudigere, Dheevatsa and Rabbat, Michael},
  journal={arXiv preprint arXiv:2309.06497},
  year={2023}
}

@article{anil2020scalable,
  title={Scalable second order optimization for deep learning},
  author={Anil, Rohan and Gupta, Vineet and Koren, Tomer and Regan, Kevin and Singer, Yoram},
  journal={arXiv preprint arXiv:2002.09018},
  year={2020}
}

@article{zhao2024deconstructing,
  title={Deconstructing what makes a good optimizer for language models},
  author={Zhao, Rosie and Morwani, Depen and Brandfonbrener, David and Vyas, Nikhil and Kakade, Sham},
  journal={arXiv preprint arXiv:2407.07972},
  year={2024}
}

@inproceedings{martens2010deep,
  title={Deep learning via Hessian-free optimization},
  author={Martens, James},
  booktitle={Proceedings of the 27th International Conference on International Conference on Machine Learning},
  pages={735--742},
  year={2010}
}

@article{martens2015optimizing,
  title={Optimizing Neural Networks with Kronecker-factored Approximate Curvature},
  author={Martens, James and Grosse, Roger},
  journal={arXiv preprint arXiv:1503.05671},
  year={2015}
}

@inproceedings{martens2018kronecker,
  title={Kronecker-factored Curvature Approximations for Recurrent Neural Networks},
  author={Martens, James and Ba, Jimmy and Johnson, Matt},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@article{osawa2018large,
  title={Large-Scale Distributed Second-Order Optimization Using Kronecker-Factored Approximate Curvature for Deep Convolutional Neural Networks},
  author={Osawa, Kazuki and Tsuji, Yohei and Ueno, Yuichiro and Naruse, Akira and Yokota, Rio and Matsuoka, Satoshi},
  journal={arXiv preprint arXiv:1811.12019},
  year={2018}
}

@article{george2018fast,
  title={Fast Approximate Natural Gradient Descent in a Kronecker-factored Eigenbasis},
  author={George, Thomas and Laurent, C{\'e}sar and Bouthillier, Xavier and Ballas, Nicolas and Vincent, Pascal},
  journal={arXiv preprint arXiv:1806.03884},
  year={2018}
}

@inproceedings{gao2021trace,
  title={A trace-restricted kronecker-factored approximation to natural gradient},
  author={Gao, Kaixin and Liu, Xiaolei and Huang, Zhenghai and Wang, Min and Wang, Zidong and Xu, Dachuan and Yu, Fan},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={9},
  pages={7519--7527},
  year={2021}
}

@inproceedings{ba2017distributed,
  title={Distributed Second-Order Optimization using Kronecker-Factored Approximations},
  author={Ba, Jimmy and Grosse, Roger and Martens, James},
  booktitle={International Conference on Learning Representations},
  year={2017}
}

@inproceedings{puiu2022randomized,
  title={Randomized K-FACs: Speeding Up K-FAC with Randomized Numerical Linear Algebra},
  author={Puiu, Constantin Octavian},
  booktitle={International Conference on Intelligent Data Engineering and Automated Learning},
  pages={411--422},
  year={2022}
}

@article{puiu2022brand,
  title={Brand New K-FACs: Speeding up K-FAC with Online Decomposition Updates},
  author={Puiu, Constantin Octavian},
  journal={arXiv preprint arXiv:2210.08494},
  year={2022}
}

@article{eschenhagen2023kronecker,
  title={Kronecker-Factored Approximate Curvature for Modern Neural Network Architectures},
  author={Eschenhagen, Runa and Immer, Alexander and Turner, Richard E and Schneider, Frank and Hennig, Philipp},
  journal={arXiv preprint arXiv:2311.00636},
  year={2023}
}

@article{dahl2023benchmarking,
  title={Benchmarking neural network training algorithms},
  author={Dahl, George E and Schneider, Frank and Nado, Zachary and Agarwal, Naman and Sastry, Chandramouli Shama and Hennig, Philipp and Medapati, Sourabh and Eschenhagen, Runa and Kasimbeg, Priya and Suo, Daniel and others},
  journal={arXiv preprint arXiv:2306.07179},
  year={2023}
}

@inproceedings{peirson2022fishy,
  title={Fishy: Layerwise Fisher Approximation for Higher-order Neural Network Optimization},
  author={Peirson, Abel and Amid, Ehsan and Chen, Yatong and Feinberg, Vladimir and Warmuth, Manfred K and Anil, Rohan},
  booktitle={Has it Trained Yet? NeurIPS 2022 Workshop},
  year={2022}
}

@article{lin2024can,
  title={Can we remove the square-root in adaptive gradient methods? a second-order perspective},
  author={Lin, Wu and Dangel, Felix and Eschenhagen, Runa and Bae, Juhan and Turner, Richard E and Makhzani, Alireza},
  journal={arXiv preprint arXiv:2402.03496},
  year={2024}
}

@article{wang20244,
  title={4-bit Shampoo for Memory-Efficient Network Training},
  author={Wang, Sike and Zhou, Pan and Li, Jia and Huang, Hua},
  journal={arXiv preprint arXiv:2405.18144},
  year={2024}
}

@article{roberts1980linear,
  title={Linear model reduction and solution of the algebraic Riccati equation by use of the sign function},
  author={Roberts, John Douglas},
  journal={International Journal of Control},
  volume={32},
  number={4},
  pages={677--687},
  year={1980},
  publisher={Taylor \& Francis}
}


@article{duchi2011adaptive,
  title={Adaptive subgradient methods for online learning and stochastic optimization.},
  author={Duchi, John and Hazan, Elad and Singer, Yoram},
  journal={Journal of machine learning research},
  volume={12},
  number={7},
  year={2011}
}


@article{zhang2024transformers,
  title={Why transformers need adam: A hessian perspective},
  author={Zhang, Yushun and Chen, Congliang and Ding, Tian and Li, Ziniu and Sun, Ruoyu and Luo, Zhi-Quan},
  journal={arXiv preprint arXiv:2402.16788},
  year={2024}
}


@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of machine learning research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}

@article{elfwing2018sigmoid,
  title={Sigmoid-weighted linear units for neural network function approximation in reinforcement learning},
  author={Elfwing, Stefan and Uchibe, Eiji and Doya, Kenji},
  journal={Neural networks},
  volume={107},
  pages={3--11},
  year={2018},
  publisher={Elsevier}
}

@article{su2024roformer,
  title={Roformer: Enhanced transformer with rotary position embedding},
  author={Su, Jianlin and Ahmed, Murtadha and Lu, Yu and Pan, Shengfeng and Bo, Wen and Liu, Yunfeng},
  journal={Neurocomputing},
  volume={568},
  pages={127063},
  year={2024},
  publisher={Elsevier}
}

@article{merity2016pointer,
  title={Pointer sentinel mixture models},
  author={Merity, Stephen and Xiong, Caiming and Bradbury, James and Socher, Richard},
  journal={arXiv preprint arXiv:1609.07843},
  year={2016}
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{pan2024lisa,
  title={LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning},
  author={Pan, Rui and Liu, Xiang and Diao, Shizhe and Pi, Renjie and Zhang, Jipeng and Han, Chi and Zhang, Tong},
  journal={arXiv preprint arXiv:2403.17919},
  year={2024}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}