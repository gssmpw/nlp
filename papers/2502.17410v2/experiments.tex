\section{Experiments}\label{experiments}
We evaluate the performance of COSMOS on pre-training various sizes of LLMs, in comparison with baseline algorithms including Adam \citep{kingma2014adam}, Adam-mini \citep{zhang2024adam}, GaLore \citep{zhao2024galore}, SOAP \citep{vyas2024soap} and MUON \citep{jordan2024MUON}. Note that for SOAP, MUON and COSMOS, the embedding and output weights are trained by Adam.


\noindent \textbf{\bf Models and Datasets.} We train LLaMA-type models \citep{touvron2023llama} on the C4 dataset \citep{raffel2020exploring}, which is a colossal, cleaned version of Common Crawl's web crawl corpus for pre-taining. 
We conduct comprehensive experiments and ablation studies on 130M models and demonstrate the token efficiency of COSMOS. 
We then scale up to 350M and 1B models to showcase the memory efficiency and small computational overhead of COSMOS. 
Due to limited computational resources, experiments on these larger models are less comprehensive, while still capable of illustrating the efficacy of our method. 
We train for one epoch on a portion of the C4 dataset, ranging from 5B to 26B tokens, and scaling with the model size according to the scaling law \citep{kaplan2020scaling}. 
We set the maximum sequence length as 1024 by default. 
Besides LLaMA models and C4 dataset, we also experiment with GPT-2 \citep{radford2019language} on WikiText-103 dataset \citep{merity2016pointer}, presented in \Cref{sec:wiki}. 

\begin{figure*}[htb!]
    \centering
    \includegraphics[width=0.45\linewidth]{imgs/130M_main_loss.pdf}
    \includegraphics[width=0.45\linewidth]{imgs/130M_main_loss_patch.pdf}
    \caption{Performance on LLaMA-130M. COSMOS consistently outperforms baseline methods. In the right plot, we hide GaLore to better compare the performance of COSMOS with SOAP and MUON, as the curves are close in the left plot. }
    \label{fig:exp-130m-main}
\end{figure*}

\subsection{Comparison on LLaMA-130M}\label{sec:exp-130m}
For LLaMA-130M, we train on a 5B subset of the C4 dataset. 
We compare COSMOS's validation loss with that of Adam, Adam-mini, GaLore, MUON, and SOAP. 
\vskip2pt
\noindent {\bf Hyperparameters. }
For each method, we tune the corresponding learning rate to obtain optimal performance. 
We select the rank $r=64$ for COSMOS and $r=256$ for GaLore. 
To avoid multiple hyperparameter tuning, we set the discount factor in COSMOS as $\gamma=\eta/\eta_{0}$, where $\eta_0$ is the learning rate of Adam for training the embedding and output weights in the implementation of COSMOS. 
Other hyperparameter choices follow \citet{zhao2024galore} and are provided in detail in \Cref{130M_tuning}. 

\vskip2pt
\noindent {\bf Main results.}
We plot the validation loss curves in \Cref{fig:exp-130m-main}. 
As illustrated, COSMOS consistently outperforms MUON with better stability and is comparable to SOAP. 
This showcases that our hybrid approach leveraging the leading eigensubspace captures the most important information for efficient update, allowing COSMOS to achieve similar per-token efficiency as SOAP. 
In addition, all three methods outperform the vanilla Adam and are much better than Adam-mini and GaLore, validating that inter-coordinate dependence is crucial for efficient optimization. 
We report the final validation perplexity in \Cref{tab:exp-token}. 

\begin{table}[htb!]
    \caption{Validation perplexity after training on C4 dataset. We train for 5000 steps on 130M and 350M models and 13000 steps on 1B model. COSMOS achieves the best validation perplexity. }
    \label{tab:exp-token}
    \centering
    \begin{tabular}{c|ccc}
    \toprule
    Size(Tokens) & 130M(5B) & 350M(10B) & 1B(26B) \\
    \hline
    Adam & 21.28&17.28 &12.97  \\
    Adam-mini &21.78 & - & - \\
    GaLore &24.07 &19.03 & - \\
    SOAP &20.59 &16.32 & - \\
    MUON &20.69 &16.49 &12.57 \\
    \hline
    COSMOS &{\bf 20.54} &{\bf 16.21} &{\bf 12.46}  \\
    \bottomrule
    \end{tabular}
\end{table}

\vskip2pt
\noindent {\bf Ablation on learning rates.} 
We experiment with different learning rates while keeping the rank $r=64$ and discount factor $\gamma=\eta/\eta_0$ for COSMOS. 
As shown in the \Cref{tab:exp-130m-lr}, COSMOS is not very sensitive to the learning rate, and it achieves the best performance at 5e-4. 
As a comparison, MUON is more sensitive to the learning rate, and it underperforms COSMOS across all learning rates. 

\begin{table}[htb!]
    \caption{Validation perplexity under different learning rates. We set $\gamma=\eta/\eta_0$ and $r=64$ for COSMOS. Our method outperforms MUON across all learning rates. }
    \label{tab:exp-130m-lr}
    \centering
    \begin{tabular}{c|ccccc}
    \toprule
    lr &2e-4 & 5e-4 & 1e-3 & 2e-3 & \\
    \hline
    MUON &21.72 & 20.75 & 20.69 & 26.74 \\
    COSMOS &{\bf 21.17} & {\bf 20.54} & {\bf 20.62} & {\bf 21.00}\\
    \bottomrule
    \end{tabular}
\end{table}

\vskip2pt
\noindent {\bf Ablation on rank and discount factor.} 
We also experiment with different ranks $r$ and discount factors $\gamma$ while keeping the learning rate as 5e-4 for COSMOS, and the results are summarized in \Cref{tab:exp-130m-rank-discount}. 
As illustrated, COSMOS is not very sensitive to $r$ and $\gamma$, and the best discount factor is around $0.25$ to $0.5$ across different ranks. 
In practice, our choice $\gamma=\eta/\eta_0$ falls in this range, so it serves as a valid heuristic that prevents extra tuning of $\gamma$.
We provide details in \Cref{sec:details}.
Moreover, we observe that as rank increases, COSMOS performs slightly worse and is more sensitive to the choice of $\gamma$. 
One possible explanation is that when the rank is large, the top-$r$ eigenvalues of $M_t$ contain some smaller values that are close to the remaining eigenvalues. 
For these eigenvalues, the one-step power iteration (Line 5 in \Cref{alg:COSMOS}) cannot accurately approximate their corresponding eigensubspaces, leading to larger approximation errors and worse performance. 


\begin{table}[htb!]
    \caption{Validation perplexity of COSMOS under different ranks $r$ and discount factors $\gamma$. Our method is not very sensitive to $r$ and $\gamma$, and consistently outperforms MUON (20.69) except for only one configuration ($r=128$, $\gamma=1$). }
    \label{tab:exp-130m-rank-discount}
    \centering
    \begin{tabular}{c|ccccc}
    \toprule
    $r$\textbackslash{$\gamma$} &0.1 & 0.25&0.5 & 1 \\
    \hline
     32& 20.58 & 20.55 & {\bf 20.54} & 20.54 \\
     64& 20.62 & {\bf 20.54} &20.57 &20.61 \\
     128& 20.65 & 20.58&20.63 &20.72 \\
    \bottomrule
    \end{tabular}
\end{table}


\begin{figure}[htb!]
    \centering
    \includegraphics[width=0.45\linewidth]{imgs/256_plot.pdf}
    \caption{Comparison of COSMOS, Adam, and GaLore on 256 sequence length. The performance of GaLore on shorter sequences does not deteriorate as for long sequences, validating the correctness of our implementation. }
    \label{fig:exp-130m-galore}
\end{figure}

\vskip2pt
\noindent {\bf Effect of normalization.}
COSMOS applies a normalization step (Line 9 in \Cref{alg:COSMOS}) after the NS transformation. 
To illustrate that normalization alone cannot fully explain the COSMOS's efficiency, we apply the same normalization to MUON and compare their performances. 
As shown in \Cref{fig:exp-130m-norm}, MUON with normalization performs similarly to the version without normalization, and both are worse than COSMOS. 
This implies that normalization does not contribute as much as other components of COSMOS, including the SOAP-like update in the leading eigensubspace. 
We present the same experiment with LLaMA-350M in \Cref{sec:exp-350m-norm}.

\begin{figure}[htb!]
    \centering
    \includegraphics[width=0.45\linewidth]{imgs/130M_norm_ablation_v2.pdf}
    \caption{Comparison of COSMOS, MUON and MUON with normalization for LLaMA-130M on C4. MUON's performance does not differ by much with normalization, implying that normalization is not the dominant reason why COSMOS outperforms MUON.}
    \label{fig:exp-130m-norm}
\end{figure}

\vskip2pt
\noindent {\bf GaLore degradation on long sequences.} In our experiment, we observe that GaLore performs much worse than COSMOS and other baselines including Adam. 
To validate the correctness of our implementation, we compare GaLore with COSMOS and Adam on 256 sequence length as adopted in \citet{zhao2024galore}. 
As shown in \Cref{fig:exp-130m-galore}, GaLore and Adam are comparable in this shorter sequence setting, suggesting that GaLore degenerates on long sequences. 
This observation aligns with \citet{liang2024memory}, while a similar phenomenon appearing in fine-tuning is reported by \citet{pan2024lisa}.
In contrast, COSMOS consistently outperforms Adam from short to long sequences without suffering from degradation. 

\subsection{Scaling up to LLaMA-350M and LLaMA-1B}
To further illustrate the efficacy of COSMOS, we scale up to larger models and more tokens. 
For LLaMA-350M, we train on a 10B subset of the C4 dataset for 5000 steps. 
We compare COSMOS with Adam, GaLore, MUON, and SOAP. 
For LLaMA-1B, we train on a 26B subset for 13000 steps. 
Given limited computation resources, we compare COSMOS with Adam and MUON. Adam serves as the standard baseline, while MUON achieves better performance than Adam while requiring less memory and little computational overhead. 
Although SOAP demonstrated superior performance among baselines, we exclude it from our comparison as its complete training for the 1B model exceeds our available resources.
More experiment details are provided in \Cref{sec:config-350m,sec:config-1b}. 


\vskip2pt
\noindent {\bf Main results of token efficiency.}
\Cref{fig:exp-350m-main,fig:exp-1b-main} display the validation loss curves for the 350M and 1B models, and \Cref{tab:exp-token} presents their final validation perplexities. 
COSMOS demonstrates superior performance compared to all baselines across both model sizes, matching the results on the 130M model and showcasing consistent token efficiency across different model sizes. 

\begin{figure}[htb!]
    \centering
    \includegraphics[width=0.45\linewidth]{imgs/350M_main_loss_v4.pdf}
    \caption{Performance of LLaMA-350M trained on C4 dataset. COSMOS consistently outperforms baseline methods. }
    \label{fig:exp-350m-main}
\end{figure}

\begin{figure}[htb!]
    \centering
    \includegraphics[width=0.45\linewidth]{imgs/1B_main_loss_v4.pdf}
    \caption{Performance of LLaMA-1B trained on C4 dataset. COSMOS consistently outperforms Adam and MUON.}
    \label{fig:exp-1b-main}
\end{figure}



\vskip2pt
\noindent {\bf Memory and computation time profiling. } 
To illustrate the memory efficiency and small computation overhead of COSMOS, we conduct a profiling experiment on LLaMA-1B to track memory usage and wall-clock time. 
We fix the batch size 10 and gradient accumulation step 25 for all methods for a fair comparison. 
We record the maximum GPU memory usage and time spent during the entire forward-backward propagation and optimizer update process for one iteration. 
As shown in \cref{tab:exp-memory}, COSMOS achieves much lower maximum GPU memory usage than Adam (\textbf{6.8\%}) and SOAP (\textbf{19.4\%}), with slightly more overhead compared to MUON. 
In terms of wall-clock time per iteration, COSMOS is comparable to MUON and is much better than SOAP. 
The fastest Adam method cannot achieve the same level of token efficiency as COSMOS. 
Therefore, COSMOS strikes a good balance between token and memory/computation overheads, achieving the best final perplexity at a much lower cost. 

\begin{table}[htb!]
    \caption{GPU memory usage and wall-clock time per iteration on 1B model. We fix the batch size to be $10$ for all methods. COSMOS has significantly less memory usage than SOAP and is comparable to memory-efficient methods like Adam-mini and MUON, without introducing much computation overhead.}
    \label{tab:exp-memory}
    \centering
    \begin{tabular}{c|ccc}
    \toprule
    Method & Memory & Wall-clock time \\
    \hline
    Adam & 62.75 G &\textbf{34.73} s \\
    SOAP & 72.58 G &39.51 s \\
    MUON & \textbf{58.25} G &\textbf{35.56} s \\
    \hline
    COSMOS &\textbf{58.47} G &\textbf{35.75} s \\
    \bottomrule
    \end{tabular}
\end{table}



\begin{table}[htb!]
    \caption{System performance on single NVIDIA A100-80G GPU and corresponding throughput (number of samples processed per second on C4 dataset) of 1B model. Max batch size is defined as the maximum number of samples that fit within the GPU’s memory capacity. Throughput is reported as the number of samples the GPU processes per second (samples/s)}
    \label{tab:exp-throughput}
    \centering
    \begin{tabular}{c|cc}
    \toprule
    Method & Max batch size & Throughput(sample/s)  \\
    \hline                   
    Adam & 13   &\textbf{7.24} \\
    SOAP & 10  &6.33   \\
    MUON &\textbf{14} &\textbf{7.23}\\
    \hline
    COSMOS &\textbf{14} & \textbf{7.07}\\
    \bottomrule
    \end{tabular}
\end{table}

\vskip2pt
\noindent {\bf Throughput. }
To better compare the methods in practical setting, we evaluate the maximal batch size and throughput of COSMOS and baselines. 
We experiment on a single NVIDIA A100 GPU with 80GB memory. The input sequence length is set to 1024. 
As shown in \Cref{tab:exp-throughput}, COSMOS is \textbf{10.8\% }faster than SOAP and comparable to MUON.

\subsection{Summary of Numerical Experiments}
We summarize our main experiment results. 
(1) COSMOS achieves the best token efficiency comparable to SOAP. 
(2) Our memory usage and throughput are significantly better than SOAP. 
(3) Our method performs consistently across different model sizes, different sequence lengths, and different hyperparameter configurations. 