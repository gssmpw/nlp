@misc{bommasani2022opportunitiesrisksfoundationmodels,
      title={On the Opportunities and Risks of Foundation Models}, 
      author={Rishi Bommasani and Drew A. Hudson and Ehsan Adeli and Russ Altman and Simran Arora and Sydney von Arx and Michael S. Bernstein and Jeannette Bohg and Antoine Bosselut and Emma Brunskill and Erik Brynjolfsson and Shyamal Buch and Dallas Card and Rodrigo Castellon and Niladri Chatterji and Annie Chen and Kathleen Creel and Jared Quincy Davis and Dora Demszky and Chris Donahue and Moussa Doumbouya and Esin Durmus and Stefano Ermon and John Etchemendy and Kawin Ethayarajh and Li Fei-Fei and Chelsea Finn and Trevor Gale and Lauren Gillespie and Karan Goel and Noah Goodman and Shelby Grossman and Neel Guha and Tatsunori Hashimoto and Peter Henderson and John Hewitt and Daniel E. Ho and Jenny Hong and Kyle Hsu and Jing Huang and Thomas Icard and Saahil Jain and Dan Jurafsky and Pratyusha Kalluri and Siddharth Karamcheti and Geoff Keeling and Fereshte Khani and Omar Khattab and Pang Wei Koh and Mark Krass and Ranjay Krishna and Rohith Kuditipudi and Ananya Kumar and Faisal Ladhak and Mina Lee and Tony Lee and Jure Leskovec and Isabelle Levent and Xiang Lisa Li and Xuechen Li and Tengyu Ma and Ali Malik and Christopher D. Manning and Suvir Mirchandani and Eric Mitchell and Zanele Munyikwa and Suraj Nair and Avanika Narayan and Deepak Narayanan and Ben Newman and Allen Nie and Juan Carlos Niebles and Hamed Nilforoshan and Julian Nyarko and Giray Ogut and Laurel Orr and Isabel Papadimitriou and Joon Sung Park and Chris Piech and Eva Portelance and Christopher Potts and Aditi Raghunathan and Rob Reich and Hongyu Ren and Frieda Rong and Yusuf Roohani and Camilo Ruiz and Jack Ryan and Christopher Ré and Dorsa Sadigh and Shiori Sagawa and Keshav Santhanam and Andy Shih and Krishnan Srinivasan and Alex Tamkin and Rohan Taori and Armin W. Thomas and Florian Tramèr and Rose E. Wang and William Wang and Bohan Wu and Jiajun Wu and Yuhuai Wu and Sang Michael Xie and Michihiro Yasunaga and Jiaxuan You and Matei Zaharia and Michael Zhang and Tianyi Zhang and Xikun Zhang and Yuhui Zhang and Lucia Zheng and Kaitlyn Zhou and Percy Liang},
      year={2022},
      eprint={2108.07258},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2108.07258}, 
}

@misc{dong2024promptpromptedadaptivestructuredpruning,
      title={Prompt-prompted Adaptive Structured Pruning for Efficient LLM Generation}, 
      author={Harry Dong and Beidi Chen and Yuejie Chi},
      year={2024},
      eprint={2404.01365},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2404.01365}, 
}

@misc{frankle2019lotterytickethypothesisfinding,
      title={The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks}, 
      author={Jonathan Frankle and Michael Carbin},
      year={2019},
      eprint={1803.03635},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1803.03635}, 
}

@misc{frantar2023sparsegptmassivelanguagemodels,
      title={SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot}, 
      author={Elias Frantar and Dan Alistarh},
      year={2023},
      eprint={2301.00774},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2301.00774}, 
}

@inproceedings{hale-2001-probabilistic,
    title = "A Probabilistic {E}arley Parser as a Psycholinguistic Model",
    author = "Hale, John",
    booktitle = "Second Meeting of the North {A}merican Chapter of the Association for Computational Linguistics",
    year = "2001",
    url = "https://aclanthology.org/N01-1021/"
}

@misc{liu2023dejavucontextualsparsity,
      title={Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time}, 
      author={Zichang Liu and Jue Wang and Tri Dao and Tianyi Zhou and Binhang Yuan and Zhao Song and Anshumali Shrivastava and Ce Zhang and Yuandong Tian and Christopher Re and Beidi Chen},
      year={2023},
      eprint={2310.17157},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2310.17157}, 
}

@misc{liu2023modelbasedcontrolsparseneural,
      title={Model-Based Control with Sparse Neural Dynamics}, 
      author={Ziang Liu and Genggeng Zhou and Jeff He and Tobia Marcucci and Li Fei-Fei and Jiajun Wu and Yunzhu Li},
      year={2023},
      eprint={2312.12791},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2312.12791}, 
}

@misc{malach2020provinglotterytickethypothesis,
      title={Proving the Lottery Ticket Hypothesis: Pruning is All You Need}, 
      author={Eran Malach and Gilad Yehudai and Shai Shalev-Shwartz and Ohad Shamir},
      year={2020},
      eprint={2002.00585},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2002.00585}, 
}

@misc{oh2024frequencyexplainsinversecorrelation,
      title={Frequency Explains the Inverse Correlation of Large Language Models' Size, Training Data Amount, and Surprisal's Fit to Reading Times}, 
      author={Byung-Doh Oh and Shisen Yue and William Schuler},
      year={2024},
      eprint={2402.02255},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.02255}, 
}

@misc{pan2024densetrainingsparseinference,
      title={Dense Training, Sparse Inference: Rethinking Training of Mixture-of-Experts Language Models}, 
      author={Bowen Pan and Yikang Shen and Haokun Liu and Mayank Mishra and Gaoyuan Zhang and Aude Oliva and Colin Raffel and Rameswar Panda},
      year={2024},
      eprint={2404.05567},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2404.05567}, 
}

@inproceedings{salicchi-hsu-2025-every,
    title = "Not Every Metric is Equal: Cognitive Models for Predicting N400 and P600 Components During Reading Comprehension",
    author = "Salicchi, Lavinia  and
      Hsu, Yu-Yin",
    editor = "Rambow, Owen  and
      Wanner, Leo  and
      Apidianaki, Marianna  and
      Al-Khalifa, Hend  and
      Eugenio, Barbara Di  and
      Schockaert, Steven",
    booktitle = "Proceedings of the 31st International Conference on Computational Linguistics",
    month = jan,
    year = "2025",
    address = "Abu Dhabi, UAE",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.coling-main.246/",
    pages = "3648--3654",
    abstract = "In recent years, numerous studies have sought to understand the cognitive dynamics underlying language processing by modeling reading times and ERP amplitudes using computational metrics like surprisal. In the present paper, we examine the predictive power of surprisal, entropy, and a novel metric based on semantic similarity for N400 and P600. Our experiments, conducted with Mandarin Chinese materials, revealed three key findings: 1) expectancy plays a primary role for N400; 2) P600 also reflects the cognitive effort required to evaluate linguistic input semantically; and 3) during the time window of interest, information uncertainty influences the language processing the most. Our findings show how computational metrics that capture distinct cognitive dimensions can effectively address psycholinguistic questions."
}

@misc{szatkowski2024exploitingactivationsparsitydense,
      title={Exploiting Activation Sparsity with Dense to Dynamic-k Mixture-of-Experts Conversion}, 
      author={Filip Szatkowski and Bartosz Wójcik and Mikołaj Piórczyński and Simone Scardapane},
      year={2024},
      eprint={2310.04361},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2310.04361}, 
}

@misc{yuan2024llminferenceunveiledsurvey,
      title={LLM Inference Unveiled: Survey and Roofline Model Insights}, 
      author={Zhihang Yuan and Yuzhang Shang and Yang Zhou and Zhen Dong and Zhe Zhou and Chenhao Xue and Bingzhe Wu and Zhikai Li and Qingyi Gu and Yong Jae Lee and Yan Yan and Beidi Chen and Guangyu Sun and Kurt Keutzer},
      year={2024},
      eprint={2402.16363},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.16363}, 
}

@misc{zhang2022moeficationtransformerfeedforwardlayers,
      title={MoEfication: Transformer Feed-forward Layers are Mixtures of Experts}, 
      author={Zhengyan Zhang and Yankai Lin and Zhiyuan Liu and Peng Li and Maosong Sun and Jie Zhou},
      year={2022},
      eprint={2110.01786},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2110.01786}, 
}

@misc{zheng2024learnefficientbuildstructured,
      title={Learn To be Efficient: Build Structured Sparsity in Large Language Models}, 
      author={Haizhong Zheng and Xiaoyan Bai and Xueshen Liu and Z. Morley Mao and Beidi Chen and Fan Lai and Atul Prakash},
      year={2024},
      eprint={2402.06126},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.06126}, 
}

@misc{zhong2024loryfullydifferentiablemixtureofexperts,
      title={Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training}, 
      author={Zexuan Zhong and Mengzhou Xia and Danqi Chen and Mike Lewis},
      year={2024},
      eprint={2405.03133},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.03133}, 
}

@misc{zhu2024llamamoebuildingmixtureofexpertsllama,
      title={LLaMA-MoE: Building Mixture-of-Experts from LLaMA with Continual Pre-training}, 
      author={Tong Zhu and Xiaoye Qu and Daize Dong and Jiacheng Ruan and Jingqi Tong and Conghui He and Yu Cheng},
      year={2024},
      eprint={2406.16554},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.16554}, 
}

