[
  {
    "index": 0,
    "papers": [
      {
        "key": "bommasani2022opportunitiesrisksfoundationmodels",
        "author": "Rishi Bommasani and Drew A. Hudson and Ehsan Adeli and Russ Altman and Simran Arora and Sydney von Arx and Michael S. Bernstein and Jeannette Bohg and Antoine Bosselut and Emma Brunskill and Erik Brynjolfsson and Shyamal Buch and Dallas Card and Rodrigo Castellon and Niladri Chatterji and Annie Chen and Kathleen Creel and Jared Quincy Davis and Dora Demszky and Chris Donahue and Moussa Doumbouya and Esin Durmus and Stefano Ermon and John Etchemendy and Kawin Ethayarajh and Li Fei-Fei and Chelsea Finn and Trevor Gale and Lauren Gillespie and Karan Goel and Noah Goodman and Shelby Grossman and Neel Guha and Tatsunori Hashimoto and Peter Henderson and John Hewitt and Daniel E. Ho and Jenny Hong and Kyle Hsu and Jing Huang and Thomas Icard and Saahil Jain and Dan Jurafsky and Pratyusha Kalluri and Siddharth Karamcheti and Geoff Keeling and Fereshte Khani and Omar Khattab and Pang Wei Koh and Mark Krass and Ranjay Krishna and Rohith Kuditipudi and Ananya Kumar and Faisal Ladhak and Mina Lee and Tony Lee and Jure Leskovec and Isabelle Levent and Xiang Lisa Li and Xuechen Li and Tengyu Ma and Ali Malik and Christopher D. Manning and Suvir Mirchandani and Eric Mitchell and Zanele Munyikwa and Suraj Nair and Avanika Narayan and Deepak Narayanan and Ben Newman and Allen Nie and Juan Carlos Niebles and Hamed Nilforoshan and Julian Nyarko and Giray Ogut and Laurel Orr and Isabel Papadimitriou and Joon Sung Park and Chris Piech and Eva Portelance and Christopher Potts and Aditi Raghunathan and Rob Reich and Hongyu Ren and Frieda Rong and Yusuf Roohani and Camilo Ruiz and Jack Ryan and Christopher R\u00e9 and Dorsa Sadigh and Shiori Sagawa and Keshav Santhanam and Andy Shih and Krishnan Srinivasan and Alex Tamkin and Rohan Taori and Armin W. Thomas and Florian Tram\u00e8r and Rose E. Wang and William Wang and Bohan Wu and Jiajun Wu and Yuhuai Wu and Sang Michael Xie and Michihiro Yasunaga and Jiaxuan You and Matei Zaharia and Michael Zhang and Tianyi Zhang and Xikun Zhang and Yuhui Zhang and Lucia Zheng and Kaitlyn Zhou and Percy Liang",
        "title": "On the Opportunities and Risks of Foundation Models"
      },
      {
        "key": "yuan2024llminferenceunveiledsurvey",
        "author": "Zhihang Yuan and Yuzhang Shang and Yang Zhou and Zhen Dong and Zhe Zhou and Chenhao Xue and Bingzhe Wu and Zhikai Li and Qingyi Gu and Yong Jae Lee and Yan Yan and Beidi Chen and Guangyu Sun and Kurt Keutzer",
        "title": "LLM Inference Unveiled: Survey and Roofline Model Insights"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "liu2023modelbasedcontrolsparseneural",
        "author": "Ziang Liu and Genggeng Zhou and Jeff He and Tobia Marcucci and Li Fei-Fei and Jiajun Wu and Yunzhu Li",
        "title": "Model-Based Control with Sparse Neural Dynamics"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "frankle2019lotterytickethypothesisfinding",
        "author": "Jonathan Frankle and Michael Carbin",
        "title": "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks"
      },
      {
        "key": "malach2020provinglotterytickethypothesis",
        "author": "Eran Malach and Gilad Yehudai and Shai Shalev-Shwartz and Ohad Shamir",
        "title": "Proving the Lottery Ticket Hypothesis: Pruning is All You Need"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "frantar2023sparsegptmassivelanguagemodels",
        "author": "Elias Frantar and Dan Alistarh",
        "title": "SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "liu2023dejavucontextualsparsity",
        "author": "Zichang Liu and Jue Wang and Tri Dao and Tianyi Zhou and Binhang Yuan and Zhao Song and Anshumali Shrivastava and Ce Zhang and Yuandong Tian and Christopher Re and Beidi Chen",
        "title": "Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "zhang2022moeficationtransformerfeedforwardlayers",
        "author": "Zhengyan Zhang and Yankai Lin and Zhiyuan Liu and Peng Li and Maosong Sun and Jie Zhou",
        "title": "MoEfication: Transformer Feed-forward Layers are Mixtures of Experts"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "pan2024densetrainingsparseinference",
        "author": "Bowen Pan and Yikang Shen and Haokun Liu and Mayank Mishra and Gaoyuan Zhang and Aude Oliva and Colin Raffel and Rameswar Panda",
        "title": "Dense Training, Sparse Inference: Rethinking Training of Mixture-of-Experts Language Models"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "zhu2024llamamoebuildingmixtureofexpertsllama",
        "author": "Tong Zhu and Xiaoye Qu and Daize Dong and Jiacheng Ruan and Jingqi Tong and Conghui He and Yu Cheng",
        "title": "LLaMA-MoE: Building Mixture-of-Experts from LLaMA with Continual Pre-training"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "zheng2024learnefficientbuildstructured",
        "author": "Haizhong Zheng and Xiaoyan Bai and Xueshen Liu and Z. Morley Mao and Beidi Chen and Fan Lai and Atul Prakash",
        "title": "Learn To be Efficient: Build Structured Sparsity in Large Language Models"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "zhong2024loryfullydifferentiablemixtureofexperts",
        "author": "Zexuan Zhong and Mengzhou Xia and Danqi Chen and Mike Lewis",
        "title": "Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "liu2023dejavucontextualsparsity",
        "author": "Zichang Liu and Jue Wang and Tri Dao and Tianyi Zhou and Binhang Yuan and Zhao Song and Anshumali Shrivastava and Ce Zhang and Yuandong Tian and Christopher Re and Beidi Chen",
        "title": "Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "zhang2022moeficationtransformerfeedforwardlayers",
        "author": "Zhengyan Zhang and Yankai Lin and Zhiyuan Liu and Peng Li and Maosong Sun and Jie Zhou",
        "title": "MoEfication: Transformer Feed-forward Layers are Mixtures of Experts"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "zheng2024learnefficientbuildstructured",
        "author": "Haizhong Zheng and Xiaoyan Bai and Xueshen Liu and Z. Morley Mao and Beidi Chen and Fan Lai and Atul Prakash",
        "title": "Learn To be Efficient: Build Structured Sparsity in Large Language Models"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "szatkowski2024exploitingactivationsparsitydense",
        "author": "Filip Szatkowski and Bartosz W\u00f3jcik and Miko\u0142aj Pi\u00f3rczy\u0144ski and Simone Scardapane",
        "title": "Exploiting Activation Sparsity with Dense to Dynamic-k Mixture-of-Experts Conversion"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "dong2024promptpromptedadaptivestructuredpruning",
        "author": "Harry Dong and Beidi Chen and Yuejie Chi",
        "title": "Prompt-prompted Adaptive Structured Pruning for Efficient LLM Generation"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "dong2024promptpromptedadaptivestructuredpruning",
        "author": "Harry Dong and Beidi Chen and Yuejie Chi",
        "title": "Prompt-prompted Adaptive Structured Pruning for Efficient LLM Generation"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "hale-2001-probabilistic",
        "author": "Hale, John",
        "title": "A Probabilistic {E}arley Parser as a Psycholinguistic Model"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "salicchi-hsu-2025-every",
        "author": "Salicchi, Lavinia  and\nHsu, Yu-Yin",
        "title": "Not Every Metric is Equal: Cognitive Models for Predicting N400 and P600 Components During Reading Comprehension"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "oh2024frequencyexplainsinversecorrelation",
        "author": "Byung-Doh Oh and Shisen Yue and William Schuler",
        "title": "Frequency Explains the Inverse Correlation of Large Language Models' Size, Training Data Amount, and Surprisal's Fit to Reading Times"
      }
    ]
  }
]