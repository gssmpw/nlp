\section{Related Work}
\label{app: related work}
\subsection{The Hint of Sparsity}
LLMs often appears excessive activation of neurons during tasks, leading to inefficiency and wasted resources____. Studies____ show that dense neural networks often display surplus activation. Treating sparsity as a continuous process can optimize model architecture holistically. The Lottery Hypothesis____ highlights pruning techniques to remove unnecessary connections and leverage inherent sparsity.

\subsection{Static Sparsity}
Early efforts to improve LLM efficiency focused on \textbf{static sparsity}, where redundant parameters are permanently removed or compressed.

\paragraph{Weight Pruning} Methods like SparseGPT____ prune weights based on magnitude criteria, achieving up to 50\% sparsity without retraining. However, such approaches ignore input-specific sparsity patterns, leading to suboptimal performance on dynamic tasks like dialogue generation. Static methods lack adaptability to varying input contexts and often require retraining to recover performance.  

\subsection{Dynamic Activation}
Dynamic activation(DA) methods selectively activate subsets of neurons during inference, offering a balance between efficiency and flexibility. Existing researches on dynamic activation methods can be categorized in Table \ref{table: two types of da}.

\paragraph{Training-Dependent Methods}
DejaVu____predicts activation sparsity during training by leveraging ReLU’s inherent sparsity. While achieving 20\% speedup on OPT-style models, it fails on non-ReLU architectures like LLaMA’s SwiGLU.

MoEfication____ dynamically routes inputs to expert subnets, but requires costly expert training and introduces routing overhead. DS-MoE____ introduces a framework that employs dense computation during training and switches to sparse computation during inference. LLaMA-MoE____ offers a new lightweight method to transform FFNs into MoEs. LTE____ achieves a superior balance between sparsity and performance by activating fewer neurons and is applicable to models with both ReLU and non-ReLU activation functions.
Lory____ retains the autoregressive properties of language models by adopting a causally segmented routing strategy and a similarity-based data batching method. This enables efficient expert merging operations and promotes specialization among experts in processing similar documents during training sessions.

\begin{table*}[h]
\begin{threeparttable}
    \centering
    \scalebox{0.7}{
    \begin{tabular}{c|p{7cm}p{3cm}p{2cm}p{4cm}}
    \toprule
    \textbf{DA Types} & \textbf{Definetions} & \textbf{Examples} & \textbf{Advantages} & \textbf{Current Limitations} \\
    \midrule
    \multirow{2}{*}{Training-Dependent DA} & Some leverage a \textit{predictor}, which is pre-trained using the model's training data, to dynamically identify essential activation neurons or experts during the model's forward. (Figure \ref{figure:Training-Dependent DA}) & DejaVu ____, MoEfication____ & High Sparsity & Tend to underperform on models with non-ReLU activations(See Table~\ref{table: dejavu}) \\
    \cmidrule(lr){2-5}
    ~ & Others aim to reduce computational costs by employing multi-stage MoE-style training and introducing efficiency and separability loss penalties. & LTE ____ and D2DMoE ____ & High performance & Extra training required \\
    \midrule
    Training-Free DA & Employs pre-searched or pre-defined thresholds or sparsity levels to decide which neurons to retain or discard. Neurons with activation values falling below this bar are eliminated during current forward, thereby reducing computational overhead and latency.(Figure \ref{figure:Training-Free TDA}) & Griffin____, CLADA(Ours) & Training-free for all model archs & Lower performance \\
    \bottomrule
    \end{tabular}
    }
    \caption{Two types of DA methods}
    \label{table: two types of da}
\end{threeparttable}
\end{table*}

\begin{figure*}[htbp]
    \begin{minipage}[t]{0.5\linewidth}
        \centering
        \includegraphics[height=2.7in]{images/dejavu.png}
        \caption{Training-Dependent DA}
        \label{figure:Training-Dependent DA}
    \end{minipage}
    \begin{minipage}[t]{0.5\linewidth}
        \centering
        \includegraphics[height=2.7in]{images/TDA.png}
        \caption{Training-Free DA}
        \label{figure:Training-Free TDA}
    \end{minipage}%
\end{figure*}
     
\paragraph{Training-Free Methods}
Griffin____ uses sequence-level activation clustering (flocking) to skip redundant computations only in generation phase. Despite its simplicity, Griffin suffers from significant performance drops (>3\% on QA tasks) due to heuristic threshold selection.

\subsection{Cognitive Load and Language Modeling}
Cognitive studies on human language processing provide inspiration for efficient computational models. Understanding how humans process language can lead to the development of more efficient and accurate language models. In this section, we explore the relationship between cognitive load, as measured by ERP components such as N400 and P600, and language modeling metrics like surprisal and entropy.

\paragraph{Surprisal and Entropy}
Surprisal, proposed by ____, is defined as the negative log probability of a word given its context. Surprisal captures the idea that the cognitive effort required to process a word is proportional to its unpredictability in a given context. This theory has been widely used to predict human reading times and has been shown to correlate with N400 amplitudes____.

Entropy____, on the other hand, measures the level of uncertainty about the upcoming linguistic input at a given point in a sentence. It is calculated based on the probability distribution of possible words in a context and has been linked to the complexity of language processing.