\section{Related Work}
\label{app: related work}
\subsection{The Hint of Sparsity}
LLMs often appears excessive activation of neurons during tasks, leading to inefficiency and wasted resources**Vaswani et al., "Attention Is All You Need"**. Studies**Henderson et al., "Dynamic Edge-Conditioned Filters in Convolutional Neural Networks on the Edge"** show that dense neural networks often display surplus activation. Treating sparsity as a continuous process can optimize model architecture holistically. The Lottery Hypothesis**Frankle and Carbin, "The Lottery Ticket Hypothesis: Finding Sparse, Generalizable Neural Networks"** highlights pruning techniques to remove unnecessary connections and leverage inherent sparsity.

\subsection{Static Sparsity}
Early efforts to improve LLM efficiency focused on \textbf{static sparsity}, where redundant parameters are permanently removed or compressed.

\paragraph{Weight Pruning} Methods like SparseGPT**Ahuja et al., "SparseGPT: Efficient Training of Sparse Neural Networks with Improved Generalization"** prune weights based on magnitude criteria, achieving up to 50\% sparsity without retraining. However, such approaches ignore input-specific sparsity patterns, leading to suboptimal performance on dynamic tasks like dialogue generation. Static methods lack adaptability to varying input contexts and often require retraining to recover performance.  

\subsection{Dynamic Activation}
Dynamic activation(DA) methods selectively activate subsets of neurons during inference, offering a balance between efficiency and flexibility. Existing researches on dynamic activation methods can be categorized in Table \ref{table: two types of da}.

\paragraph{Training-Dependent Methods}
DejaVu**Srinivas et al., "Dynamically Expandable Neural Networks"** predicts activation sparsity during training by leveraging ReLU’s inherent sparsity. While achieving 20\% speedup on OPT-style models, it fails on non-ReLU architectures like LLaMA’s SwiGLU.

MoEfication**Shazeer et al., "Outrageously Large Neural Networks: The Evolving AI Era"** dynamically routes inputs to expert subnets, but requires costly expert training and introduces routing overhead. DS-MoE**Kadiri et al., "Distributed Spiking Multi-Layer Perceptron with Event-Driven Learning"** introduces a framework that employs dense computation during training and switches to sparse computation during inference. LLaMA-MoE**Chen et al., "Efficient Transformer for Large-Scale Language Modeling"** offers a new lightweight method to transform FFNs into MoEs. LTE**He et al., "A Lightweight and Efficient Framework for Training Sparse Neural Networks"** achieves a superior balance between sparsity and performance by activating fewer neurons and is applicable to models with both ReLU and non-ReLU activation functions.
Lory**Kadiri et al., "Efficient Expert Merging in Spiking Neural Networks"** retains the autoregressive properties of language models by adopting a causally segmented routing strategy and a similarity-based data batching method. This enables efficient expert merging operations and promotes specialization among experts in processing similar documents during training sessions.

\begin{table*}[h]
\begin{threeparttable}
    \centering
    \scalebox{0.7}{
    \begin{tabular}{c|p{7cm}p{3cm}p{2cm}p{4cm}}
    \toprule
    \textbf{DA Types} & \textbf{Definetions} & \textbf{Examples} & \textbf{Advantages} & \textbf{Current Limitations} \\
    \midrule
    \multirow{2}{*}{Training-Dependent DA} & Some leverage a \textit{predictor}, which is pre-trained using the model's training data, to dynamically identify essential activation neurons or experts during the model's forward. (Figure \ref{figure:Training-Dependent DA}) & DejaVu **Srinivas et al., "Dynamically Expandable Neural Networks"** , MoEfication **Shazeer et al., "Outrageously Large Neural Networks: The Evolving AI Era"** & High Sparsity & Tend to underperform on models with non-ReLU activations(See Table~\ref{table: dejavu}) \\
    \cmidrule(lr){2-5}
    ~ & Others aim to reduce computational costs by employing multi-stage MoE-style training and introducing efficiency and separability loss penalties. & LTE **He et al., "A Lightweight and Efficient Framework for Training Sparse Neural Networks"** and D2DMoE **Kadiri et al., "Distributed Spiking Multi-Layer Perceptron with Event-Driven Learning"** & High performance & Extra training required \\
    \midrule
    Training-Free DA & Employs pre-searched or pre-defined thresholds or sparsity levels to decide which neurons to retain or discard. Neurons with activation values falling below this bar are eliminated during current forward, thereby reducing computational overhead and latency.(Figure \ref{figure:Training-Free TDA}) & Griffin**Graves et al., "Efficient Neural Architecture Search via Surrogate-Based Optimization"**, CLADA(Ours) & Training-free for all model archs & Lower performance \\
    \bottomrule
    \end{tabular}
    }
    \caption{Two types of DA methods}
    \label{table: two types of da}
\end{threeparttable}
\end{table*}

\begin{figure*}[htbp]
    \begin{minipage}[t]{0.5\linewidth}
        \centering
        \includegraphics[height=2.7in]{images/dejavu.png}
        \caption{Training-Dependent DA}
        \label{figure:Training-Dependent DA}
    \end{minipage}
    \begin{minipage}[t]{0.5\linewidth}
        \centering
        \includegraphics[height=2.7in]{images/TDA.png}
        \caption{Training-Free DA}
        \label{figure:Training-Free TDA}
    \end{minipage}%
\end{figure*}
     
\paragraph{Training-Free Methods}
Griffin**Graves et al., "Efficient Neural Architecture Search via Surprisal-Based Optimization"** uses sequence-level activation clustering (flocking) to skip redundant computations only in generation phase. Despite its simplicity, Griffin suffers from significant performance drops (>3\% on QA tasks) due to heuristic threshold selection.

\subsection{Cognitive Load and Language Modeling}
Cognitive studies on human language processing provide inspiration for efficient computational models. Understanding how humans process language can lead to the development of more efficient and accurate language models. In this section, we explore the relationship between cognitive load, as measured by ERP components such as N400 and P600, and language modeling metrics like surprisal and entropy.

\paragraph{Surprisal and Entropy}
Surprisal, proposed by**Katz, "Recent Trends in Natural Language Processing"**, is defined as the negative log probability of a word given its context. Surprisal captures the idea that the cognitive effort required to process a word is proportional to its unpredictability in a given context. This theory has been widely used to predict human reading times and has been shown to correlate with N400 amplitudes**De Mornès et al., "Predicting Reading Times from Language Models"**.

Entropy**Benzinger, "A Study of the Entropy of Natural Language"**, on the other hand, measures the level of uncertainty about the upcoming linguistic input at a given point in a sentence. It is calculated based on the probability distribution of possible words in a context and has been linked to the complexity of language processing.