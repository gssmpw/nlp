%%%%%%%% ICML 2024 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage[accepted]{arxiv}

% If accepted, instead use the following line for the camera-ready submission:

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{enumitem}
% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Preprint. Under Review.}

\begin{document}

\twocolumn[
\icmltitle{CAT: Contrastive Adversarial Training for Evaluating the Robustness of Protective Perturbations in Latent Diffusion Models}



% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Sen Peng}{yyy}
\icmlauthor{Mingyue Wang}{yyy}
\icmlauthor{Jianfei He}{yyy}
\icmlauthor{Jijia Yang}{yyy}
\icmlauthor{Xiaohua Jia}{yyy}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{Department of Computer Science, City University of Hong Kong, Kowloon, Hong Kong SAR}
\icmlcorrespondingauthor{Sen Peng}{first1.last1@xxx.edu}
% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Diffusion Model, Protective Perturbation}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Latent diffusion models have recently demonstrated superior capabilities in many downstream image synthesis tasks. 
However, customization of latent diffusion models using unauthorized data can severely compromise the privacy and intellectual property rights of data owners.
Adversarial examples as protective perturbations have been developed to defend against unauthorized data usage by introducing imperceptible noise to customization samples, preventing diffusion models from effectively learning them.
In this paper, we first reveal that the primary reason adversarial examples are effective as protective perturbations in latent diffusion models is the distortion of their latent representations, as demonstrated through qualitative and quantitative experiments.
We then propose the Contrastive Adversarial Training (CAT) utilizing adapters as an adaptive attack against these protection methods, highlighting their lack of robustness. 
Extensive experiments demonstrate that our CAT method significantly reduces the effectiveness of protective perturbations in customization configurations, urging the community to reconsider and enhance the robustness of existing protective perturbation methods. Code is available at \hyperlink{here}{https://github.com/senp98/CAT}.
\end{abstract}



%================Introduction================
\section{Introduction}
%============================================
Diffusion-based image generation models, especially the Latent Diffusion Models (LDMs)~\cite{rombach2022high}, have exhibited strong capabilities across various downstream image generation tasks, including text-to-image synthesis~\cite{dhariwal2021diffusion,rombach2022high}, image-to-image translation~\cite{saharia2022palette,zhang2023inversion}, image editing~\cite{kawar2023imagic}, and image inpainting~\cite{lugmayr2022repaint}.
By utilizing the open-source LDM such as Stable Diffusion~\cite{stablediffusion} as the pre-trained model, users can customize image generation models on their datasets at a significantly lower cost than training from scratch.
Despite their prominence and popularity, the customization of LDMs also raises significant concerns regarding privacy and Intellectual Property Rights (IPRs)~\cite{Dixit2023, Joseph2023}. 
It can be misused to generate fake human face images of a specific identity~\cite{li2022seeing} or 
mimic the style of a specific artist by utilizing unauthorized data.
Undoubtedly, such malicious customization severely compromises the rights of data owners and raises ethical concerns across the community.

Several protective perturbation methods~\cite{salman2023raising, liang2023adversarial, shan2023glaze, van2023anti, liang2023mist, xue2023toward, liu2024metacloak} have been proposed to address these concerns in diffusion-based image generation. 
These methods introduce imperceptible perturbations to image samples in the customization dataset, creating adversarial examples that effectively degrade the performance of downstream task customization.
Although significant progress has been made in developing perturbation methods, the robustness of these protection techniques has received limited attention. 
Recent studies~\cite{zhao2024can, cao2024impress, honig2024adversarial} have investigated the resilience of protective perturbations against purification-based adaptive attacks.
By purifying the protected images before customization, these methods effectively compromise the robustness of existing protective perturbation.
However, existing purification-based adaptive attacks depend on optimization, resulting in high computational costs. 
Without an efficient mechanism to detect the protected perturbed images, purification must be applied to every sample in the customization dataset, increasing resource demand.
Furthermore, purification without prior knowledge of the original data distribution introduces uncertainty, potentially altering the learned distribution. 
In extreme cases, optimization-based purification may entirely replace the original image content if the substituted one achieves a lower loss for the optimization objective.

In this paper, we introduce Contrastive Adversarial Training (CAT) from a novel perspective, focusing on model adaptation rather than purification as a solution approach.
We first reveal that the primary reason adversarial examples are effective as protective perturbations in LDM customization is that their latent representations are distorted.
Building on this insight, we develop the CAT method targeting the latent autoencoder of the LDM, utilizing the contrastive adversarial loss to minimize the reconstruction loss of protected image samples.  
During adversarial training, we introduce LoRA adapters~\cite{hu2022lora} to specific layers of the latent autoencoder, ensuring that only the adapter weights are modified. 
This approach expands the parameter space to mitigate the effectiveness of adversarial examples and reduces computational costs, enhancing its adaptability across different scenarios.
Extensive experiments demonstrate that applying CAT decreases the effectiveness of protective perturbations, urging the community to reconsider their robustness.
In summary, our main contributions are as follows:  
\begin{enumerate}[left=0pt, labelindent=0pt, align=left, itemsep=0pt, topsep=0pt]
\item We identify that the primary reason adversarial examples effectively serve as protective perturbations in LDM customization is the distortion of their latent representations.  
\item We propose contrastive adversarial training with adapters as an adaptive attack to evaluate the robustness of existing protective perturbation methods from a novel perspective of model adaptation rather than purification.  
\item We conduct extensive experiments demonstrating that applying CAT can significantly reduce the effectiveness of existing perturbation methods, urging the community to reconsider their robustness.  
\end{enumerate}
\begin{figure}[ht]
    \vskip 0.2in
    \begin{center}
        \centerline{\includegraphics[width=\columnwidth, trim=1.5cm 0cm 1.8cm 0cm, clip]{figures/cat_design.pdf}}
        \caption{
            Overview of the proposed CAT method. 
            The CAT adapters ($\Delta \phi$ and $\Delta \omega$) are integrated into the latent encoder and decoder to realign the latent representations of protected samples. 
            Instead of directly using $z_a$ for the denoising module in the diffusion process, the protected image $x_a$ is first encoded into the realigned latent representation $z^{(\text{cat})}_a$ through the adapted encoder, then decoded into $\hat{x}_a^{(\text{cat})}$ using the adapted decoder. 
            This process ensures that the generated image $\tilde{x}_a^{(\text{cat})}$ is correctly decoded into the pixel space, effectively neutralizing the protective perturbations.
        }
        \label{figure:CAT_design}
    \end{center}
    \vskip -0.2in
\end{figure}




%================Related Works================
\section{Related Works}
%=============================================
\subsection{Diffusion-based Text-to-Image Models}
Diffusion-based text-to-image models have demonstrated remarkable capabilities across various downstream tasks. 
Images are learned through the forward diffusion process, while generation involves denoising in the reverse diffusion process, typically using a U-Net~\cite{ronneberger2015u} as the backbone model~\cite{ho2020denoising}.
Latent diffusion models~\cite{rombach2022high} improve training and inference efficiency while maintaining generation quality by encoding samples into their latent space representations through a latent autoencoder before the diffusion process.
Stable Diffusion~\cite{stablediffusion} is later released as an open-source LDM, significantly boosting the development of various downstream applications in diffusion-based text-to-image generation.


\subsection{Customization of LDMs}
Customization of LDMs can be applied to various downstream tasks, which are classified into two categories: text-driven image synthesis and text-driven image editing.  
Text-driven image synthesis can be divided into (1) object-driven image synthesis and (2) style mimicry. 
The former generates images of a specific object based on the given text prompt, while the latter generates images that adopt a specific style defined by the text prompt.
DreamBooth~\cite{ruiz2023dreambooth} customizes text-to-image models by fine-tuning the U-Net and text encoder with a few images and employing a class-specific prior preservation loss, enabling the image generation of the target subject while preserving its key features.
LoRA~\cite{hu2022lora} was initially proposed to reduce the cost of fine-tuning large language models and later applied to diffusion model customization. 
It assumes that the model's weight updates form a low-rank matrix, which can be represented through its rank decomposition. 
By introducing lightweight adaptation layers to a frozen pre-trained model and updating only these layers during customization, LoRA significantly lowers the computational cost during customization.


\subsection{Protective Perturbations}
The idea of protective perturbations originates from adversarial attacks~\cite{goodfellow2014explaining} in classification models, where small perturbations mislead the model into making incorrect predictions. 
Recent work~\cite{salman2023raising,liang2023adversarial,shan2023glaze,van2023anti,liang2023mist,xue2023toward} shows that diffusion-based generative models are also vulnerable to such attacks, with adversarial examples serving as protective perturbations that significantly degrade the performance of downstream tasks, preventing unauthorized customization.
Photoguard~\cite{salman2023raising} first introduced protective perturbations to increase the cost of diffusion-based image editing such as SDEdit~\cite{meng2021sdedit}.
The perturbation is created by maximizing the latent and diffusion denoising loss over the protected images.
Both AdvDM~\cite{liang2023adversarial} and SDS~\cite{xue2023toward} generate perturbations targeting the entire LDM through end-to-end optimization to prevent learning, while Glaze~\cite{shan2023glaze} focuses on perturbations against the latent encoder of the LDM. Mist~\cite{liang2023mist} combines latent and denoising losses in perturbation generation, while Anti-DreamBooth~\cite{van2023anti} creates protective perturbations against the DreamBooth~\cite{ruiz2023dreambooth} customization approach.
MetaCloak~\cite{liu2024metacloak} leverages a meta-learning framework with transformation sampling to generate transferable and robust perturbations.

Purification-based adaptive attacks have recently been proposed against existing protection methods. 
IMPRESS~\cite{cao2024impress} introduces a purification technique that minimizes the difference between the protected image and its reconstruction from the latent autoencoder of the LDM. 
GrIDPure~\cite{zhao2024can} builds upon DiffPure~\cite{nie2022diffusion} to pre-process protected samples. 
Additionally, the authors in~\cite{honig2024adversarial} propose using Noisy-Upscaling to purify protected image samples before customization.
However, all these methods decrease the effectiveness of protective perturbations by purifying the protected images. 
To the best of our knowledge, we are the first to address the robustness of protection perturbations in the customization of LDMs from the novel perspective of model adaptation.

\section{Understanding Protective Perturbations}
\subsection{Distortion in Latent Representations}
\begin{figure}[ht]
    \vskip 0.2in
    \begin{center}
        \centerline{\includegraphics[width=\columnwidth, trim=0.5cm 0cm 0.5cm 0cm, clip]{figures/latent_demo.pdf}}
        \caption{
           Distortion of latent representations for protected image samples in LDMs.
           Given a clean image $x_c$, it is encoded into a latent representation $z_c$ and decoded back to pixel space as $\tilde{x}_c$. 
           For the protected image $x_a$, the latent representation $\hat{z}_a$ is distorted, leading to a reconstructed image $\tilde{x}_a$ that deviates significantly from $x_a$, resulting in a larger reconstruction error.
        }
        \label{figure:latent_demo}
    \end{center}
\vskip -0.2in
\end{figure}
We first identify a key reason why adversarial examples serve as effective protective perturbations in LDMs: their representations are distorted in the latent space.
As illustrated in~\cref{figure:latent_demo}, the latent autoencoder of a LDM first encodes a given clean image $x_c$ from pixel space into a latent representation $z_c$. 
Subsequently, the denoising module takes the latent representation as input because it has significantly lower dimensionality while still preserving the semantic features.
Additionally, it requires that the autoencoder can correctly decode the latent representation $z_c$ back to pixel space, ensuring that the reconstructed image $\tilde{x}_c$ matches the original image $x_c$ in the ideal case.
However, the mappings between pixel and latent space are distorted for adversarial examples. 
For an adversarial example $x_a$ within a perturbation budget $\delta$, the latent representation $\hat{z}_a$ should remain close to $z_c$ within a distance $\delta_z$ and correctly decode back to $x_a$ in pixel space in the ideal case. 
However, $x_a$ is instead mapped to a distorted latent representation $z_a$.
This distorted representation is decoded to $\Tilde{x}_a$, which exhibits a significantly larger reconstruction error.

Similar observations about the latent representation are also reported in previous works~\cite{xue2023toward,cao2024impress}. 
In contrast, our study conducts more extensive qualitative and quantitative experiments to thoroughly validate the distortion in latent representations of protected samples. 
To illustrate this, we use t-SNE to visualize the latent representations of various protected samples, as shown in~\cref{fig:visualization_latent9}.
\begin{figure}[ht]
    \vskip 0.2in
    \begin{center}
        \centerline{\includegraphics[width=\columnwidth]{figures/visualization_latent9.pdf}}
        \caption{
            Visualization of latent representations of clean sample $z_c$, noisy sample $z_r$, protected sample $z_a$, and the latent representation of protected sample $z_a^{(\text{cat})}$ encoded by the latent encoder with CAT adapter. 
            Protected samples are generated using nine protective perturbation methods, with four clean images from the VGGFace2 dataset as the base. 
            The perturbation budget for protected and noisy samples is fixed at $\delta=16/255$.
        }
        \label{fig:visualization_latent9}
    \end{center}
    \vskip -0.2in
\end{figure}
We use four clean images of an identity from the VGGFace2~\cite{cao2018vggface2} dataset as an illustration. 
For each clean sample $x_c$, we generate its adversarial example $x_a$ as the protected image using nine different protective perturbation methods, within the perturbation budget fixed at $\delta=16/255$. 
The protected sample is then encoded into the latent space through the latent encoder as $z_a=\mathcal{E}(x_a)$, while the clean sample is encoded as $z_c=\mathcal{E}(x_c)$.
Additionally, we construct a noisy sample $x_r=x_c+clip(r, \delta)$ by adding Gaussian noise $r\sim N(0,\delta^2)$, clipped using the same perturbation budget $\delta$. 
This noisy sample is encoded as $z_r=\mathcal{E}(x_r)$. 
Finally, the adversarial sample $x_a$ is also encoded using the latent encoder with the CAT adapter (in the CAT-both setting), resulting in $z_a^{(\text{cat})}=\mathcal{E}^{(\text{cat})}(x_a)$.
For each protection method shown in the subplots, $z_a$, $z_r$ and $z_a^{(\text{cat})}$ are clustered with their corresponding clean sample $z_c$. 
Since four images are evaluated for each protection method, their latent representations naturally form four distinct groups. 
Within each group, it can be observed that the latent representation of the protected sample $z_a$ exhibits a larger distance from the clean sample's latent representation $z_c$ compared to the noisy sample $z_r$ under the same perturbation budget.
This pattern holds across all four image samples for every protective perturbation method, indicating that the latent representations of adversarial examples as protected samples are distorted across all evaluated protection methods.
\begin{figure}[ht]
    \vskip 0.2in
    \begin{center}
        \centerline{\includegraphics[width=\columnwidth]{figures/latent_distance.pdf}}
        \caption{
            Comparison of latent representation distances $\|d_a-d_r\|$ and $\|d_a^{(\text{cat})}-d_r\|$ for various protective perturbation methods across (a) CelebA-HQ and (b) VGGFace2 datasets. 
            The mean distance values over four images for each method are presented, highlighting the CAT adapter's effectiveness in reducing latent representation distortion.
        }
        \label{fig:latent_distance}
    \end{center}
\vskip -0.2in
\end{figure}
In addition to the qualitative experiments, we provide a quantitative analysis of the distances of latent representations. 
We define the MAE distance between the latent representation of a sample and its corresponding clean sample as $d$. 
Specifically, the distance between $z_a$ and its corresponding $z_c$ is denoted as $d_a=\|z_a-z_c\|_1$, while the distance between $z_a^{(\text{cat})}$ and its corresponding $z_c$ is represented as $d_a^{(\text{cat})} = \|z_a^{(\text{cat})} - z_c\|_1$.
The distance between a noisy sample $z_r$ and its corresponding $z_c$ is denoted as $d_a=\|z_a-z_c\|_1$. 
For comparison, we present the differences between $d_a$ and $d_a^{(\text{cat})}$ over $d_r$, as shown in~\cref{fig:latent_distance}.
As we can observe, $d_a$ is larger than $d_r$ across all the evaluated protection methods, indicating that adversarial examples significantly distort their latent representations compared to noisy samples with the same perturbation budget. 
Furthermore, our proposed CAT adapter method can effectively reduce this distortion, realigning the latent representations of protected examples closer to their correct positions.


\subsection{Learnability of Distorted Latent Representations}
We have identified that a key reason for the effectiveness of adversarial examples as protective perturbations is the distortion of their latent representations.
Building on this finding, we seek to explore a further question: Can the latent representations of adversarial examples be effectively learned by the diffusion process?
\begin{figure}[ht]
    \vskip 0.2in
    \begin{center}
        \centerline{\includegraphics[width=\columnwidth]{figures/difference_ratio.pdf}}
        \caption{
            Difference ratios for nine protection methods, evaluated using an image from the VGGFace2 dataset. 
            Results are shown for two fine-tuning approaches: (a) by DreamBooth and (b) by LoRA.
            The $s_r$ range $[s_c - \|s_c - s_r\|, s_c + \|s_c - s_r\|]$ is shaded, demonstrating that $s_a$ consistently falls within or near this range across all methods, indicating that both clean samples and adversarial examples can be effectively learned by the diffusion process.
        }
        \label{fig:difference_ratio}
    \end{center}
\vskip -0.2in
\end{figure}

To explore this, we design the following experiment: diffusion models are known to overfit when trained or fine-tuned on a very small number of image samples. 
In such cases, the overfitted model exhibits memorization, generating samples nearly identical to the training images when given the same text prompts used during fine-tuning.
We first fine-tune the pre-trained LDM on a single image obtained from one identity in the dataset. 
Once the model overfits, we generate an image from the fine-tuned model using the exact same text prompt. 
For the latent representation $z_c$ of a clean image, we expect the generated $\tilde{z}_c$ to be close to $z_c$ in the latent space. 
The difference ratio for this clean image is defined as $s_c=\|z_c-\tilde{z}_c\|/z_b$, where $z_b=\max(z_c)-\min(z_c)$ serves as a normalization term.
Subsequently, the difference ratios for $s_r$, $s_a$ and $s_a^{(\text{cat})}$ are defined as $\|z_r-\tilde{z}_c\|/z_b$, $\|z_a-\tilde{z}_c\|/z_b$ and $\|z_a^{\text{cat}}-\tilde{z}_c\|/z_b$, respectively.
For simplicity, we also define the $s_r$ range as $[s_c - \|s_c - s_r\|, s_c + \|s_c - s_r\|]$.
We present the evaluation results in~\cref{fig:difference_ratio}. 
The difference ratios are evaluated for nine protection methods based on the sample clean image from an identity in the VGGFace2 dataset and two fine-tuning approaches, DreamBooth and LoRA.
As shown in~\cref{fig:difference_ratio}, $s_a$ consistently falls within or near the range of $s_c \pm s_r$ across all protection methods and both fine-tuning approaches, which suggests that the latent representations of both clean samples and adversarial examples can be effectively learned by the diffusion process. 
This indicates that the primary reason for the effectiveness of adversarial examples as protective perturbation methods lies in the distortion of their latent representations, rather than the limitation of the diffusion process in learning these distorted representations.

In summary, this section provides a deeper investigation into why adversarial examples of LDMs are effective as protective perturbations against unauthorized data usage. 
We identify that the primary reason lies in the distortion of latent representations for these adversarial examples. 
Furthermore, we verify that these distorted latent representations can still be effectively learned by the diffusion process, confirming that the distortion itself is the key factor behind their effectiveness.

\section{CAT: Contrastive Adversarial Training}
\subsection{Contrastive Adversarial Loss}
We have observed that adversarial examples are effective as protective perturbations because their latent representations are distorted. 
This indicates that the latent autoencoder $\{\mathcal{E}_{\phi},\mathcal{D}_{\omega}\}$ of the LDM is attacked. 
To mitigate this, we apply contrastive adversarial training to the latent autoencoder to realign the latent representations of adversarial examples.
We first introduce the contrastive adversarial loss used for contrastive adversarial training as follows.  
Given the protected image $x_a$ and the latent autoencoder $\{\mathcal{E}_{\phi},\mathcal{D}_{\omega}\}$, we define the loss function as  
\begin{equation}
    \mathcal{L}_{\text{cat}}(\mathcal{E}_{\phi},\mathcal{D}_{\omega}, x_a)=\mathbb{E}_{x_a}\|\mathcal{D}_{\omega}(\mathcal{E}_{\phi}(x_a))-x_a\|^2_2.
\end{equation}  
The protected image is first encoded into the latent space by the encoder $\mathcal{E}_{\phi}$ and then decoded back into the pixel space by $\mathcal{D}_{\omega}$.  
Since the decoded image should ideally match the original protected image, we define the contrastive loss as the MSE distance between them over all given $x_a$.  
This loss encourages the autoencoder to correctly reconstruct $x_a$, restoring its latent representation.

\begin{figure*}[ht]
    \vskip 0.2in
    \begin{center}
        \centerline{\includegraphics[width=\textwidth, trim=2cm 0cm 0cm 0cm, clip]{figures/db_vggface2.pdf}}
        \caption{
            Qualitative results for object-driven image synthesis customization in an identity from the VGGFace2 dataset using DreamBooth and CAT settings with the text prompt "a dslr portrait of \textit{sks} person".
            Each row represents a different setting: Baseline, CAT-both, CAT-en, and CAT-de, while each column corresponds to a different protective perturbation setting. 
            The results illustrate the effectiveness of different CAT settings on generating human faces from protected images.
        }
        \label{figure:object_driven_synthesis_iid_db}
    \end{center}
\vskip -0.2in
\end{figure*}


\subsection{CAT Adapters}
Fine-tuning all layers of the latent autoencoder may affect the model's performance in encoding and decoding unprotected image samples.
Moreover, we aim to introduce additional parameters to expand the parameter space, improving the model's resilience against adversarial examples after the contrastive adversarial training.
Therefore, we incorporate adapters in our proposed contrastive adversarial training.  
Specifically, we apply the LoRA~\cite{hu2022lora} adapter to all convolutional and attention layers within the latent autoencoder.
For the model weights $\phi_0 \in \mathbb{R}^{d\times k}$ of the latent encoder, the latent representation $z^{(\text{cat})}_a$ of a protected sample $x_a$ is given by  
\begin{equation}
    z^{(\text{cat})}_a=\phi_0x_a + \Delta \phi x_a = \phi_0x_a + B^{\phi}A^{\phi}x_a,
\end{equation}
where $B^{\phi}\in\mathbb{R}^{d\times r}$ and $A^{\phi}\in\mathbb{R}^{r\times k}$ are the weights of the CAT adapter attached to the latent encoder.

Similarly, for the model weights $\omega_0 \in \mathbb{R}^{k\times d}$ of the latent decoder, the reconstruction $\hat{x}^{(\text{cat})}_a$ in pixel space of the latent representation $z^{(\text{cat})}_a$ is given by  
\begin{equation}
\begin{split}
    \hat{x}^{(\text{cat})}_a 
    &= \omega_0 \, z^{(\text{cat})}_a 
       + \Delta \omega \, z^{(\text{cat})}_a \\
    &= \omega_0 \, z^{(\text{cat})}_a 
       + B^{\omega} \, A^{\omega} \, z^{(\text{cat})}_a,
\end{split}
\end{equation}
where $B^{\omega}\in\mathbb{R}^{k\times r}$ and $A^{\omega}\in\mathbb{R}^{r\times d}$ are the weights of the CAT adapter attached to the latent decoder.
The overall design of our proposed CAT method is illustrated in~\cref{figure:CAT_design}.  
For a protected image sample $x_a$, we do not directly use its encoded latent representation $z_a$ for the denoising module to learn in the diffusion process. 
Instead, we first apply CAT by encoding $x_a$ using the latent encoder attached with the CAT adapter $\Delta \phi$, obtaining the realigned latent representation $z^{(\text{cat})}_a$.  
This realigned representation is then decoded back using the latent decoder, which is also attached with the CAT adapter $\Delta \omega$, resulting in the reconstructed image $\hat{x}_a^{(\text{cat})}$ in the pixel space.
Our proposed CAT method is optimized by solving the following objective:
\begin{equation}
    \Delta \phi,\Delta \omega=\arg_{\Delta \phi,\Delta \omega}\min \mathcal{L}_{\text{cat}}(\mathcal{E}^{\text{cat}}_{\phi},\mathcal{D}^{\text{cat}}_{\omega}, x_a),
\end{equation}
where the weights for model $\mathcal{E}^{\text{cat}}_{\phi}$ are $\phi_0+\Delta \phi$ and the weights for model $\mathcal{D}^{\text{cat}}_{\omega}$ are $\omega_0+\Delta \omega$.

Once CAT is applied and the CAT adapters are obtained, customization on protected image samples is performed using the latent autoencoder augmented with the CAT adapters.  
For image generation, since the denoising module is fine-tuned on the realigned latent representation $z^{(\text{cat})}_a$, the generated output in the latent space for a given prompt is also realigned. 
Consequently, it is decoded into the image $\tilde{x}_a^{(\text{cat})}$ in the pixel space instead of $\tilde{x}_a$, effectively neutralizing the protective perturbations.

\begin{table*}[ht!]
\caption{
    Quantitative results for object-driven image synthesis using CAT methods customized in DreamBooth.  
    The table presents performance metrics FSS and FQS for the CelebA-HQ and VGGFace2 datasets across different protective perturbation settings. 
    The optimal values among different CAT settings for each metric are \textbf{bolded}, where higher FSS indicates improved human face similarity, and higher FQS reflects better face image quality.
}
\label{table:object_driven_synthesis_iid_db}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\setlength{\tabcolsep}{1pt} 
\resizebox{\textwidth}{!}{
\begin{tabular}{l|cccc|cccc|cccc|cccc}
\toprule
Dataset & \multicolumn{8}{c|}{CelebA-HQ} & \multicolumn{8}{c}{VGGFace2} \\\midrule
Metric & \multicolumn{4}{c|}{FSS $\uparrow$} & \multicolumn{4}{c|}{FQS $\uparrow$} & \multicolumn{4}{c|}{FSS $\uparrow$} & \multicolumn{4}{c}{FQS $\uparrow$} \\\midrule
Method & Baseline & CAT-both & CAT-en & CAT-de & Baseline & CAT-both & CAT-en & CAT-de & Baseline & CAT-both & CAT-en & CAT-de & Baseline & CAT-both & CAT-en & CAT-de \\\midrule
AdvDM(+) & 0.340 & \textbf{0.643} & 0.529 & 0.344 & 0.244 & 0.431 & \textbf{0.448} & 0.340 & 0.390 & 0.534 & \textbf{0.560} & 0.416 & 0.257 & 0.460 & \textbf{0.600} & 0.331 \\
AdvDM(-) & 0.354 & \textbf{0.623} & 0.571 & 0.401 & 0.275 & 0.549 & \textbf{0.611} & 0.401 & 0.436 & \textbf{0.564} & 0.547 & 0.454 & 0.281 & 0.617 & \textbf{0.662} & 0.492 \\
Mist     & 0.160 & \textbf{0.572} & 0.501 & 0.152 & 0.286 & \textbf{0.597} & 0.580 & 0.375 & 0.035 & \textbf{0.557} & 0.521 & 0.101 & 0.384 & 0.656 & \textbf{0.712} & 0.279 \\
SDS(+)   & 0.352 & \textbf{0.602} & 0.499 & 0.342 & 0.282 & 0.413 & \textbf{0.423} & 0.399 & 0.397 & 0.486 & \textbf{0.508} & 0.305 & 0.406 & 0.502 & \textbf{0.532} & 0.410 \\
SDS(-)   & 0.411 & \textbf{0.678} & 0.599 & 0.405 & 0.352 & \textbf{0.597} & 0.587 & 0.449 & 0.439 & \textbf{0.570} & 0.569 & 0.435 & 0.368 & \textbf{0.730} & 0.689 & 0.456 \\
SDST     & 0.175 & \textbf{0.594} & 0.485 & 0.157 & 0.208 & 0.587 & \textbf{0.588} & 0.313 & 0.146 & \textbf{0.559} & 0.546 & 0.196 & 0.248 & 0.663 & \textbf{0.694} & 0.346 \\
Glaze    & 0.363 & \textbf{0.610} & 0.577 & 0.398 & 0.491 & 0.618 & \textbf{0.676} & 0.587 & 0.387 & \textbf{0.607} & 0.576 & 0.472 & 0.576 & 0.724 & \textbf{0.740} & 0.595 \\
Anti-DB  & 0.471 & \textbf{0.662} & 0.597 & 0.489 & 0.434 & 0.608 & \textbf{0.664} & 0.546 & 0.456 & \textbf{0.584} & 0.546 & 0.440 & 0.416 & 0.673 & \textbf{0.704} & 0.401 \\
MetaCloak & 0.497 & \textbf{0.642} & 0.578 & 0.507 & 0.372 & 0.460 & \textbf{0.475} & 0.403 & 0.421 & 0.560 & \textbf{0.631} & 0.487 & 0.355 & 0.508 & \textbf{0.625} & 0.412 \\
\bottomrule
\end{tabular}}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}


\section{Experiments}
\subsection{Experimental Setup}
\textbf{Evaluated Protective Perturbation Methods}
To evaluate the effectiveness of our proposed CAT method in compromising the robustness of protective perturbations, we test CAT across nine protective perturbation settings: 
(1) AdvDM(+)~\cite{liang2023adversarial}, 
(2) AdvDM(-)~\cite{xue2023toward}, 
(3) Mist~\cite{liang2023mist}, 
(4) SDS(+)~\cite{xue2023toward}, 
(5) SDS(-)~\cite{xue2023toward}, 
(6) SDST~\cite{xue2023toward}, 
(7) Glaze~\cite{shan2023glaze}, 
(8) Anti-DreamBooth~\cite{van2023anti}, 
and (9) Metacloak~\cite{liu2024metacloak}.  
The (+) sign denotes the use of gradient ascent to construct the perturbation, whereas the (-) sign denotes the use of gradient descent, following the setting in~\cite{xue2023toward}.
We set the perturbation budget for all evaluated protection methods to $\delta=16/255$. 
Since Glaze does not release its codebase and its application does not allow adjustment of the specific perturbation budget, we use its maximum perturbation strength.


\textbf{Datasets}
We evaluate the effectiveness of CAT in two main diffusion-based image synthesis applications: (1) object-driven image synthesis and (2) style mimicry.  
Object-driven image synthesis focuses on generating images of a specific object based on a given text prompt, such as the face of a particular person or a photo of a specific dog.  
Style mimicry, conversely, aims to generate images that mimic the style of the provided customization images.

For object-driven image synthesis, we focus on human face generation using the CelebA-HQ~\cite{karras2018progressive} and VGGFace2~\cite{cao2018vggface2} datasets.  
For each dataset, we select five identities, with 12 images per identity, equally split into three subsets: a clean reference subset, a target protection subset, and an additional clean reference subset.  
For style mimicry, due to page constraints, we present only qualitative results to demonstrate the effectiveness of CAT.  
We use a subset of \textit{Claude Monet}'s artworks from the WikiArt dataset~\cite{mancini2018adding}, consisting of 12 images, divided into three subsets following the same configuration.  
All images are center-cropped and resized to $512 \times 512$.


\textbf{CAT Settings}
For object-driven image synthesis, we evaluate three CAT settings: (1) CAT-both, (2) CAT-en, and (3) CAT-de. 
In CAT-both, CAT adapters are added to the target layers in both the encoder and decoder of the latent autoencoder. 
CAT-en applies CAT adapters only to the encoder, while CAT-de applies them solely to the decoder. 
Since we have identified that the primary reason adversarial examples are effective as protective perturbations is their latent representation distortion, the CAT-de method should have no impact on compromising the protection. 
We conduct this evaluation to verify our hypothesis.
For style mimicry, we only evaluate (1) CAT-both, and (2) CAT-en settings.

We select all convolutional and attention layers within the module as the target layers. 
The adapter rank is set to $r = 128$ for the CAT-both setting and $r = 256 $ for the CAT-en and CAT-de settings to maintain the same number of parameters.  
During CAT, only the weights of the attached adapters are updated, while all other weights remain frozen. 
Training is performed with a batch size of 4 and a learning rate of 1 $\times 10^{-4}$ for 1000 steps. 


\textbf{Customization Settings}
For the customization settings, we adopt the widely used DreamBooth~\cite{ruiz2023dreambooth} and LoRA~\cite{hu2022lora} methods, using the latest Stable Diffusion v2.1 as the pre-trained LDM.
Details of the customization settings are provided in~\cref{app:customization_settings}.


\textbf{Evaluation Metrics For Qualitative Experiments}
The object-driven image synthesis task aims to generate human faces that preserve the identity features from the given protected images.  
Therefore, for qualitative experiments, we use Face Detection Rate (Retina-FDR), measured with the RetinaFace detector~\cite{deng2020retinaface}, to evaluate whether a face is detected in the generated images.  
If a face is detected, we extract its embedding using the ArcFace recognizer~\cite{deng2019arcface} and compute the cosine distance between the extracted embedding and those from the clean dataset, denoted as Identity Score Matching (ISM).  
To quantify the expected identity similarity in the generated image set, we define Face Similarity Score (FSS) as the product of Retina-FDR and ISM.

Moreover, we employ the TOPIQ method~\cite{chen2024topiq}, trained on the face IQA dataset and implemented in~\cite{pyiqa}, to measure Face Detection Rate (TOPIQ-FDR) and Face Image Quality (FIQ).  
To quantify the expected quality of the generated face images, we define their product as the Face Quality Score (FQS).  
We use two text prompts for sampling, "a photo of \textit{sks} person" and "a dslr portrait of \textit{sks} person".
For each test prompt, we generate 30 images and compute the average metric scores across all tested identities to obtain the final results.


\begin{figure*}[ht!]
    \vskip 0.2in
    \begin{center}
        \centerline{\includegraphics[width=\textwidth, trim=2cm 0cm 0cm 0cm, clip]{figures/db_wikiart.pdf}}
        \caption{
            Qualitative results for style mimicry customization in DreamBooth using CAT settings with the text prompt "a painting of shoe with a plant growing inside by \textit{claude monet} artist".
            Each row represents a different setting: Baseline, CAT-both and CAT-en, while each column corresponds to a different protective perturbation setting. 
            The results illustrate the effectiveness of different CAT settings on generating artworks of the same style from protected images.
        }
        \label{figure:style_mimicy_iid_db}
    \end{center}
\vskip -0.2in
\end{figure*}


\subsection{Object-driven Synthesis Results}
The evaluation results for applying CAT methods in the object-driven image synthesis task are presented in~\cref{table:object_driven_synthesis_iid_db}.  
We observe that both the CAT-both and CAT-en settings significantly improve task performance compared to the baseline, which directly customizes using the protected image samples across all evaluated protections. 
This demonstrates the effectiveness of these two settings in compromising the protective perturbations.  
Furthermore, the CAT-both setting achieves a higher FSS score than CAT-en, while CAT-en obtains a better FQS score overall.  
Additionally, the CAT-de setting has little to no effect on improving task performance, further verifying that the primary reason adversarial examples function effectively as protective perturbations is their latent representation distortion.
Qualitative results for the identity \textit{n000050} from the VGGFace2 dataset, as shown in~\cref{figure:object_driven_synthesis_iid_db} (for the identity \textit{5} from the CelebA-HQ dataset in~\cref{app:object_driven_db_celebahq}), further confirm the observations from the quantitative results.
Moreover, we also experimented with object-driven image synthesis customization using LoRA, as shown in~\cref{app:object_driven_lora}.  
The results verify the effectiveness of CAT in the LoRA customization setting across all nine evaluated protective perturbations and two datasets.


\subsection{Style Mimicry Results}  
We evaluate only the CAT-both and CAT-en settings for style mimicry customization using DreamBooth to verify their effectiveness in compromising the protective perturbations.  
Qualitative results are presented in~\cref{figure:style_mimicy_iid_db}, demonstrating that applying CAT leads to a significant improvement in the task performance of style mimicry customization.  
Across all evaluated protective perturbations, applying CAT preserves the specific artist's style and improves the fidelity of generated images while reducing the distortions introduced by the protections.  
These results demonstrate that CAT weakens the robustness of existing protective perturbations, enabling more precise style mimicry and highlighting the need for more effective defense mechanisms.


\subsection{Ablation Study: Impact of CAT Adapter Rank}
To investigate the impact of the CAT adapter rank on the effectiveness, we conduct experiments using different rank settings on the VGGFace2 dataset under the AdvDM(+) protective perturbation and CAT-both setting.  
The results shown in~\cref{table:ablation} indicate that increasing the rank generally improves performance, with $r=128$ achieving the highest FSS (0.544) and FQS (0.460).
This demonstrates that higher-rank adapters enhance robustness against protective perturbations.

However, higher-rank CAT adapters result in a larger model size.  
For instance, the CAT-both setting with $r=128$ produces a 104 MB adapter model, and this size continues to grow with increasing rank.  
This trade-off suggests that while higher-rank adapters enhance robustness against protective perturbations, practical deployment requires balancing performance gains and model efficiency.

\begin{table}[ht!]
\setlength{\tabcolsep}{1pt} 
    \caption{Effectiveness of the CAT-both method against AdvDM(+) protective perturbation on the VGGFace2 dataset at various CAT adapter rank settings for human face generation using DreamBooth. The optimal values for each metric are highlighted in \textbf{bold}.}
    \label{table:ablation}
    \vskip 0.15in
    \begin{center}
    \begin{small}
        \begin{sc}
        \setlength{\tabcolsep}{4pt} % Adjust column spacing
        \resizebox{\columnwidth}{!}{ % Ensure the table fits within a single column
            \begin{tabular}{l|ccccccc}
                \toprule
                Rank & Baseline & $r=4$ & $r=8$ & $r=16$ & $r=32$ & $r=64$ & $r=128$ \\
                \midrule
                FSS $\uparrow$ & 0.350 & 0.525 & 0.500 & 0.513 & 0.489 & 0.540 & \textbf{0.544} \\
                FQS $\uparrow$ & 0.260 & 0.372 & 0.390 & 0.458 & 0.414 & 0.381 & \textbf{0.460} \\
                \bottomrule
            \end{tabular}
        }
        \end{sc}
    \end{small}
    \end{center}
\vskip -0.1in
\end{table}
\section{Conclusions} 
In conclusion, we introduce contrastive adversarial training as a novel approach to evaluating the robustness of LDM customization against protective perturbations.
We first reveal that the primary reason adversarial examples serve as effective protective perturbations in LDM customization is the distortion of their latent representation. 
We then propose CAT, which employs adapters in adversarial training by leveraging contrastive adversarial loss.
Extensive experiments demonstrate that CAT significantly decreases the effectiveness of existing protective perturbations. 
We believe this work can remind the community to reconsider the robustness of current protective perturbation methods.
% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}
\clearpage

\bibliography{example_paper}
\bibliographystyle{arxiv}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Additional Experimental Results}
\subsection{Qualitative Results for Object-Driven Image Synthesis in DreamBooth (CelebA-HQ)}
\label{app:object_driven_db_celebahq}
\begin{figure*}[ht]
    \vskip 0.2in
    \begin{center}
        \centerline{\includegraphics[width=\textwidth, trim=2cm 0cm 0cm 0cm, clip]{figures/db_celebahq2.pdf}}
        \caption{
            Qualitative results for object-driven image synthesis customization in an identity from the CelebA-HQ dataset using DreamBooth and CAT settings with the text prompt "a dslr portrait of \textit{sks} person".
            Each row represents a different setting: Baseline, CAT-both, CAT-en, and CAT-de, while each column corresponds to a different protective perturbation setting. 
            The results illustrate the effectiveness of different CAT settings on generating human faces from protected images.
        }
        \label{figure:object_driven_synthesis_iid_db_celebahq}
    \end{center}
\vskip -0.2in
\end{figure*}
\subsection{Customization for Object-driven Image Synthesis in LoRA}
\label{app:object_driven_lora}

\begin{table*}[ht!]
\caption{
    Quantitative results for object-driven image synthesis using CAT methods customized in DreamBooth.  
    The table presents performance metrics FSS and FQS for the CelebA-HQ and VGGFace2 datasets across different protective perturbation settings. 
    The optimal values among different CAT settings for each metric are \textbf{bolded}.
}
\label{table:object_driven_synthesis_iid_lora}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\setlength{\tabcolsep}{1pt} 
\resizebox{\textwidth}{!}{
\begin{tabular}{lcccc|cccc|cccc|cccc}
\toprule
Dataset & \multicolumn{8}{c|}{CelebA-HQ} & \multicolumn{8}{c}{VGGFace2} \\\midrule
Metric & \multicolumn{4}{c|}{FSS $\uparrow$} & \multicolumn{4}{c|}{FQS $\uparrow$} & \multicolumn{4}{c|}{FSS $\uparrow$} & \multicolumn{4}{c}{FQS $\uparrow$} \\\midrule
Method & Baseline & CAT-both & CAT-en & CAT-de & Baseline & CAT-both & CAT-en & CAT-de & Baseline & CAT-both & CAT-en & CAT-de & Baseline & CAT-both & CAT-en & CAT-de \\\midrule
advdm+ & 0.351 & \textbf{0.574} & 0.455 & 0.402 & 0.260 & 0.369 & \textbf{0.585} & 0.377 & 0.301 & \textbf{0.490} & 0.417 & 0.377 & 0.257 & 0.400 & \textbf{0.499} & 0.404 \\
advdm- & 0.424 & \textbf{0.653} & 0.608 & 0.505 & 0.235 & 0.568 & \textbf{0.686} & 0.480 & 0.367 & \textbf{0.616} & 0.598 & 0.482 & 0.281 & 0.593 & \textbf{0.729} & 0.409 \\
mist     & 0.135 & \textbf{0.565} & 0.516 & 0.005 & 0.363 & 0.469 & \textbf{0.532} & 0.022 & 0.069 & \textbf{0.596} & 0.476 & 0.001 & 0.384 & 0.565 & \textbf{0.706} & 0.000 \\
sds+     & 0.464 & 0.497 & \textbf{0.559} & 0.285 & 0.381 & 0.328 & \textbf{0.525} & 0.389 & 0.393 & \textbf{0.541} & 0.443 & 0.413 & 0.406 & 0.399 & \textbf{0.505} & 0.392 \\
sds-     & 0.476 & \textbf{0.625} & 0.586 & 0.535 & 0.297 & 0.583 & \textbf{0.665} & 0.490 & 0.353 & \textbf{0.620} & 0.591 & 0.480 & 0.368 & 0.633 & \textbf{0.695} & 0.416 \\
sdsT5    & 0.203 & \textbf{0.572} & 0.486 & 0.032 & 0.217 & 0.488 & \textbf{0.574} & 0.137 & 0.112 & \textbf{0.549} & 0.544 & 0.052 & 0.248 & 0.578 & \textbf{0.716} & 0.217 \\
glaze2   & 0.355 & \textbf{0.669} & 0.575 & 0.479 & 0.474 & 0.714 & \textbf{0.752} & 0.617 & 0.330 & 0.602 & \textbf{0.611} & 0.472 & 0.576 & 0.673 & \textbf{0.755} & 0.586 \\
antidb   & 0.539 & 0.596 & \textbf{0.616} & 0.465 & 0.497 & 0.458 & \textbf{0.541} & 0.411 & 0.460 & 0.583 & \textbf{0.586} & 0.439 & 0.416 & 0.569 & \textbf{0.725} & 0.494 \\
metacloak & 0.528 & \textbf{0.597} & 0.543 & 0.549 & 0.376 & 0.442 & \textbf{0.458} & 0.444 & 0.416 & \textbf{0.520} & 0.513 & 0.431 & 0.355 & 0.531 & \textbf{0.551} & 0.496 \\
\bottomrule
\end{tabular}}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}



\section{Details of Customization Settings}
\label{app:customization_settings}
In the DreamBooth customization setting, both the U-Net and text encoder are fine-tuned with a batch size of 2 and a learning rate of $5 \times 10^{-7}$ for 2000 training steps.
The training instance and prior class prompt for object-driven image synthesis are "a photo of \textit{sks} person" and "a photo of person", respectively.  
For style mimicry, the training instance and prior class prompt are "a painting by \textit{[ARTIST]} artist" and "a painting by artist", where \textit{[ARTIST]} is replaced with the target artist's name.
In the LoRA customization setting, LoRA adapters are added only to the U-Net with a rank of 4 and fine-tuned using a batch size of 2 and a learning rate of $5 \times 10^{-5}$ for 2000 training steps.





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.

