\section{Related Works}
%=============================================
\subsection{Diffusion-based Text-to-Image Models}
Diffusion-based text-to-image models have demonstrated remarkable capabilities across various downstream tasks. 
Images are learned through the forward diffusion process, while generation involves denoising in the reverse diffusion process, typically using a U-Net~\cite{ronneberger2015u} as the backbone model~\cite{ho2020denoising}.
Latent diffusion models~\cite{rombach2022high} improve training and inference efficiency while maintaining generation quality by encoding samples into their latent space representations through a latent autoencoder before the diffusion process.
Stable Diffusion~\cite{stablediffusion} is later released as an open-source LDM, significantly boosting the development of various downstream applications in diffusion-based text-to-image generation.


\subsection{Customization of LDMs}
Customization of LDMs can be applied to various downstream tasks, which are classified into two categories: text-driven image synthesis and text-driven image editing.  
Text-driven image synthesis can be divided into (1) object-driven image synthesis and (2) style mimicry. 
The former generates images of a specific object based on the given text prompt, while the latter generates images that adopt a specific style defined by the text prompt.
DreamBooth~\cite{ruiz2023dreambooth} customizes text-to-image models by fine-tuning the U-Net and text encoder with a few images and employing a class-specific prior preservation loss, enabling the image generation of the target subject while preserving its key features.
LoRA~\cite{hu2022lora} was initially proposed to reduce the cost of fine-tuning large language models and later applied to diffusion model customization. 
It assumes that the model's weight updates form a low-rank matrix, which can be represented through its rank decomposition. 
By introducing lightweight adaptation layers to a frozen pre-trained model and updating only these layers during customization, LoRA significantly lowers the computational cost during customization.


\subsection{Protective Perturbations}
The idea of protective perturbations originates from adversarial attacks~\cite{goodfellow2014explaining} in classification models, where small perturbations mislead the model into making incorrect predictions. 
Recent work~\cite{salman2023raising,liang2023adversarial,shan2023glaze,van2023anti,liang2023mist,xue2023toward} shows that diffusion-based generative models are also vulnerable to such attacks, with adversarial examples serving as protective perturbations that significantly degrade the performance of downstream tasks, preventing unauthorized customization.
Photoguard~\cite{salman2023raising} first introduced protective perturbations to increase the cost of diffusion-based image editing such as SDEdit~\cite{meng2021sdedit}.
The perturbation is created by maximizing the latent and diffusion denoising loss over the protected images.
Both AdvDM~\cite{liang2023adversarial} and SDS~\cite{xue2023toward} generate perturbations targeting the entire LDM through end-to-end optimization to prevent learning, while Glaze~\cite{shan2023glaze} focuses on perturbations against the latent encoder of the LDM. Mist~\cite{liang2023mist} combines latent and denoising losses in perturbation generation, while Anti-DreamBooth~\cite{van2023anti} creates protective perturbations against the DreamBooth~\cite{ruiz2023dreambooth} customization approach.
MetaCloak~\cite{liu2024metacloak} leverages a meta-learning framework with transformation sampling to generate transferable and robust perturbations.

Purification-based adaptive attacks have recently been proposed against existing protection methods. 
IMPRESS~\cite{cao2024impress} introduces a purification technique that minimizes the difference between the protected image and its reconstruction from the latent autoencoder of the LDM. 
GrIDPure~\cite{zhao2024can} builds upon DiffPure~\cite{nie2022diffusion} to pre-process protected samples. 
Additionally, the authors in~\cite{honig2024adversarial} propose using Noisy-Upscaling to purify protected image samples before customization.
However, all these methods decrease the effectiveness of protective perturbations by purifying the protected images. 
To the best of our knowledge, we are the first to address the robustness of protection perturbations in the customization of LDMs from the novel perspective of model adaptation.