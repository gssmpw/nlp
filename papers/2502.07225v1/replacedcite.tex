\section{Related Works}
%=============================================
\subsection{Diffusion-based Text-to-Image Models}
Diffusion-based text-to-image models have demonstrated remarkable capabilities across various downstream tasks. 
Images are learned through the forward diffusion process, while generation involves denoising in the reverse diffusion process, typically using a U-Net____ as the backbone model____.
Latent diffusion models____ improve training and inference efficiency while maintaining generation quality by encoding samples into their latent space representations through a latent autoencoder before the diffusion process.
Stable Diffusion____ is later released as an open-source LDM, significantly boosting the development of various downstream applications in diffusion-based text-to-image generation.


\subsection{Customization of LDMs}
Customization of LDMs can be applied to various downstream tasks, which are classified into two categories: text-driven image synthesis and text-driven image editing.  
Text-driven image synthesis can be divided into (1) object-driven image synthesis and (2) style mimicry. 
The former generates images of a specific object based on the given text prompt, while the latter generates images that adopt a specific style defined by the text prompt.
DreamBooth____ customizes text-to-image models by fine-tuning the U-Net and text encoder with a few images and employing a class-specific prior preservation loss, enabling the image generation of the target subject while preserving its key features.
LoRA____ was initially proposed to reduce the cost of fine-tuning large language models and later applied to diffusion model customization. 
It assumes that the model's weight updates form a low-rank matrix, which can be represented through its rank decomposition. 
By introducing lightweight adaptation layers to a frozen pre-trained model and updating only these layers during customization, LoRA significantly lowers the computational cost during customization.


\subsection{Protective Perturbations}
The idea of protective perturbations originates from adversarial attacks____ in classification models, where small perturbations mislead the model into making incorrect predictions. 
Recent work____ shows that diffusion-based generative models are also vulnerable to such attacks, with adversarial examples serving as protective perturbations that significantly degrade the performance of downstream tasks, preventing unauthorized customization.
Photoguard____ first introduced protective perturbations to increase the cost of diffusion-based image editing such as SDEdit____.
The perturbation is created by maximizing the latent and diffusion denoising loss over the protected images.
Both AdvDM____ and SDS____ generate perturbations targeting the entire LDM through end-to-end optimization to prevent learning, while Glaze____ focuses on perturbations against the latent encoder of the LDM. Mist____ combines latent and denoising losses in perturbation generation, while Anti-DreamBooth____ creates protective perturbations against the DreamBooth____ customization approach.
MetaCloak____ leverages a meta-learning framework with transformation sampling to generate transferable and robust perturbations.

Purification-based adaptive attacks have recently been proposed against existing protection methods. 
IMPRESS____ introduces a purification technique that minimizes the difference between the protected image and its reconstruction from the latent autoencoder of the LDM. 
GrIDPure____ builds upon DiffPure____ to pre-process protected samples. 
Additionally, the authors in____ propose using Noisy-Upscaling to purify protected image samples before customization.
However, all these methods decrease the effectiveness of protective perturbations by purifying the protected images. 
To the best of our knowledge, we are the first to address the robustness of protection perturbations in the customization of LDMs from the novel perspective of model adaptation.