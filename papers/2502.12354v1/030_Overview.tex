\section{Study Purpose and Overview}
%Our study aims to examine two facets of purposes as follows: 1) how contrastive and selective explanations -- as widely accepted properties of good explanations -- also hold preferable when presented to lay people in AI-assisted decision-making contexts, and 2) what factors come into play in evaluating such explanation strategies on AI-assisted decisions. In our study, we consider a variety AI-assisted decision-making scenarios where data subjects are given a decision related to their individual circumstances in diverse sociotechnical contexts, followed by a textual explanation presented to them as the rationale on why they were given the decision. To illustrate them with concrete examples, we began by developing a handful of written scenarios spanning multiple decision-making contexts, such as a medical diagnosis scenario as illustrated below:
Our study aims to examine two key aspects of explanation strategies in AI-assisted decision-making: (1) how contrastive and selective explanations—commonly accepted as properties of good explanations—are perceived by laypeople, and (2) which factors influence the evaluation of these explanation strategies in AI-assisted decisions. We explore various AI-assisted decision-making scenarios, where human subjects receive decisions related to their personal circumstances in different sociotechnical contexts. These decisions are accompanied by textual explanations that provide the rationale behind the outcomes.

To ground our study in concrete examples, we developed a range of written scenarios across various decision-making contexts, such as medical diagnoses and loan approvals. These scenarios demonstrate how explanations are provided and how participants might evaluate them in different sociotechnical contexts. Below, we present one of the six selected scenarios.

% \textbf{Motivating scenario.} On Sunday morning, Jane, in her age of sixty-seven, was preparing to go for a walk as usual. When headed outside home, however, she suddenly felt a bit dizzy and had some chest pain. She wanted to check the health status with a diagnosis app tracking the sensor data and medical records. She entered the symptom and checked if this is related to some disease. Then the AI-based diagnosis told her that “you are at a risk for Acute Coronary Syndrome and are recommended to visit your primary care physician.” It was really an unexpected result for her because when looking back at her past experiences, her dizziness used to be gone in a minute, and she assumed it to be anemia.

% Jane found it crucial to her long-term health to know but there seemed to be many underlying factors that are hard to understand without medical knowledge. She was also unsure what medical or personal factors led to such a diagnosis, and also wanted to know what she can do to get better. She found that, in the application, there was an AI-based chatbot available for seeking explanations about why she got a diagnosis from the app, so she asked for it. In response, the chat service responded back with an explanation.

\textbf{Loan approval case as a motivating scenario:} On a Tuesday morning, Jane, a sixty-seven-year-old retiree, applied for a loan to renovate her home. A few hours later, she received a notification from the bank stating that her loan request had been denied. The AI-based system explained the decision was based on her credit score and financial history. This result surprised Jane, as she had always been responsible with her finances. She wanted to understand why the loan was denied---whether it was her age, retirement status, or something else in her profile.

\input{tables/tab_exp_strategies}

Jane found an AI-based chatbot on the bank's website offering explanations for loan decisions. She asked the chatbot why her loan was denied, and it provided a detailed response outlining the factors behind the decision.

\subsection{Evaluation of Contrastive and Selective Explanations}
% In such scenarios, AI systems may concern how to not only provide explanations to satisfy the explainability but also make it better and tailored to fit into users' different characteristics, level of understanding and expectations in a given AI-assisted decision-making contexts.

% Our choice of explanations based on literature review allows us a thorough examination of potential variants of contrastive and selective explanations. As summarized in Table \ref{tab:strategies}, these explanations can be also viewed in terms of two explanatory strategies, contrastive explanations and information selectivity. First, an explanation depends on contrastive strategies (i.e., \textit{whether} or \textit{how} to compare), such as contrastive (to compare with the opposite case), analogous (to compare with a similar case), or no strategy. Within each contrastive strategy, we include variants of explanations discussed in existing literature depending on \textit{what/who} to compare, such as making contrast against others (\cto), their own status in the past (\ctt), or hypothetical status of myself (\cf) with the opposite outcome. 

% Also, the variants of explanations in our study are also differentiated based on information selectivity, with the matter of information complexity and alignment as its sub-components. terms of information complexity, contrastive and counterfactual explanations (\cto, \ctt, and \cf) as well as \cbhe provide simple explanations with only minimal set of features that differentiate cases described in the texts, while \comp and \cbho describe all the features in a thorough manner. This variation may further influence information selectivity, i.e., how information presented in the explanations, depending on whether it is partial vs. all the causes, are aligned with people's prior beliefs. To provide concrete examples of explanation, Fig. \ref{fig:exp-strategies} illustrates six explanations with their definitions in a scenario of loan approval scenario where a data subject (denoted as `Me') was given a decision of loan denial. As shown in the examples, explanations with simple information include partial set of features as the minimal causes, as compared with thorough explanations with all the features. We hypothesize that the value of explanations are likely affected by how these features are aligned with data subjects' prior beliefs. From our perspective, this can be better understood through an analogy to a trade-off between bias and variance, a well-known concept in the context of machine learning. When a partial set of information is presented (i.e., providing only minimal causes), it leads to high bias (less likely to fit users' prior beliefs), but once it fits in, its low variance will lead to a greater precision, leading to more satisfaction, while presenting a broader scope of information (i.e., providing all the causes) is the opposite case. 

% Overall, our study is designed to take a deeper dive into variants of such explanations, on whether, what, and how to compare (contrastive strategies) and what information and how much to explain (information selectivity). 

In such scenarios, AI systems must not only provide explanations to meet explainability requirements but also tailor them to users' varying characteristics, levels of understanding, and expectations within AI-assisted decision-making contexts.

Our selection of explanation types, informed by a thorough literature review, enables us to examine different forms of contrastive and selective explanations. As summarized in Table \ref{tab:strategies}, these explanations fall into two main categories: contrastive strategies and information selectivity.

{\bf Contrastive strategies.} Contrastive explanations compare a given decision to alternative outcomes using different strategies ({\it how} to compare). These strategies include comparing to an opposite outcome (contrastive), comparing to a similar case (analogous), or providing no comparison at all. The focus of the comparison ({\it what/who} to compare) can also vary, such as comparing the user's outcome to someone else's (\cto) or comparing their current status to their past (\ctt) \cite{miller2019explanation, RemoteCausesBadExplanations}, or considering a hypothetical alternative outcome they could face (\cf). Each of these variants is rooted in the existing literature (see detailed review in Section \ref{sec:related-work-exp}) and represents a specific approach to helping users understand the AI's decision-making process.

{\bf Information selectivity.} The explanations in our study are also differentiated by information selectivity, which involves two sub-components: information complexity and alignment with users' prior beliefs. 

In terms of information complexity, some explanations, such as contrastive and counterfactual explanations (\cto, \ctt, and \cf) and \cbhe \cite{ EvaluationUsefulnessCaseBasedExplanationa, stolpmann1999optimierung}, provide simple explanations that highlight only the minimal features differentiating the cases. In contrast, other explanations, like \comp and \cbho \cite{el2015case, ExplanationsCaseBasedReasoningFoundational}, offer a comprehensive overview, describing all relevant features in detail. This variation further influences how much and what information is going to be described in each explanation. Figure \ref{fig:exp-strategies} illustrates six different explanation strategies in the context of a loan denial decision. As shown, explanations with minimal information focus on a few key features, while thorough explanations include all relevant factors.

We hypothesize that the perceived value of these explanations depends on how well the selected features align with users' prior beliefs. This relationship can be compared to the trade-off between bias and variance in prediction. Presenting minimal information may result in higher bias (less alignment with users' expectations), but when the explanation aligns with their expectations, it can lead to greater satisfaction due to its greater precision. In contrast, presenting more comprehensive explanations tends to reduce bias but may introduce greater variance, potentially overwhelming users with information.


\subsection{Understanding of Human Valuation Process of Explanations} 
Given an array of contrastive and selective explanations, which explanation strategies will people find preferable and intuitive? While theoretical frameworks advocate for these types of explanations as generally intuitive and human-friendly, we posit that their value is highly contextual — depending on factors such as \textit{when}, \textit{how}, and \textit{for whom} the explanations are provided.

\begin{figure*}[h]%
\centering
\includegraphics[width=\textwidth]{figures/exp-strategies}
\vspace{-2em}
\caption{The detailed definitions and examples of six variants of explanations.}\label{fig:exp-strategies}
% \vspace{-0.75em}
\end{figure*}

Prior research in cognitive and social science has presented numerous findings on how individuals engage with explanatory processes or decision-making contexts differently based on their personal traits and situational contexts. Drawing from the literature review (see details in Section \ref{sec:variable-selection}), we present the framework for conceptualizing human valuation of explanation. As presented in Fig. \ref{fig:study-design}A, we synthesize them as an interplay of various factors, which can fall into one of the following dimensions: 1) \darkgray{individual-dependent} factors: Individuals are known to have inherent \darkgray{demographic traits} and \darkgray{decision-making style} (Fig. \ref{fig:study-design}A-a) that shape their preferences and ability based on their inherent traits such as demographics and decision-making style. 2) \orange{context-dependent} factors: When given a decision in a \orange{sociotechnical context} (e.g., denied a loan) (Fig. \ref{fig:study-design}A-b), individuals may perceive the context (e.g., does the decision have a significant consequences?), in turn exhibiting a different level of \darkorange{cognitive engagement} (Fig. \ref{fig:study-design}A-c), depending on whether they are motivated, perceive a certain level of opportunities or ability to seek explanations. When presented with an \green{explanation} from an AI system (Fig. \ref{fig:study-design}A-d), individuals may experience different levels of \darkgreen{cognitive load} (Fig. \ref{fig:study-design}A-e), impacting their interpretation of the provided explanation. Depending on these factors, users will evaluate explanations regarding different \darkpurple{explanatory values} (Fig. \ref{fig:study-design}A-f). In our study, we scrutinize this framework by investigating RQ3 and RQ4 as outlined in Section \ref{sec:rqs}, aiming to identify key aspects of the human valuation process of explanations.


\subsection{Research Questions}\label{sec:rqs}
Based on two facets of our study purposes as described above, we distill them into four key research questions listed below: \\

% \textbf{Evaluating lay people's preferences over contrastive and selective explanations.} The first part of our study is dedicated to evaluate contrastive and selective explanations in different AI-assisted decision scenarios. We take both quantitative and qualitative approach to validate the relative preferences of different types of explanations via a survey experiment and further scrutinize the rationales behind their preferences through an analysis of responses on open-ended questions to see why they preferred certain explanations (i.e., what properties of explanations led to their preferences) (see detailed methods on Section \ref{sec:study-design} and \ref{sec:method-open}). With this purpose, we challenge the widely accepted assumption of the value of contrastive and selective explanations.
{\bf Evaluating laypeople's preferences for contrastive and selective explanations:} The first part of our study focuses on evaluating preferences for contrastive and selective explanations across various AI-assisted decision-making scenarios. We use both quantitative and qualitative approaches: a survey experiment to measure preferences and an analysis of responses to open-ended questions to explore the reasons behind these preferences (i.e., what features of the explanations influenced their choices) (see detailed methods in Sections \ref{sec:study-design} and \ref{sec:method-open})). Through this, we aim to critically examine the commonly held assumption that contrastive and selective explanations are inherently valuable in all contexts.
\begin{itemize}
    \item \textbf{RQ1.} How are contrastive and selective explanations preferred in general and within specific sociotechnical contexts?
    \item \textbf{RQ2.} Which aspects and properties of explanations are linked to the distinct valuation of those explanation strategies?
\end{itemize}

% \textbf{Understanding factors influencing the preference of contrastive and selective explanations.} In addition to evaluating the preferences, we further explore the study framework to contribute to a nuanced understanding of human valuation of explanations. In our experiment, we translate the factors included in the framework into the survey items and conduct the analysis for the between-variable and collective relationship among all factors.
{\bf Exploring factors influencing preferences for contrastive and selective explanations:} In addition to evaluating preferences, we aim to gain deeper insights into the factors that shape how people value different explanations. In our experiment, we translate these factors into specific survey items and analyze their interactions, examining both individual variables and their combined effects. This design seeks to explain the factors influencing preferences for contrastive and selective explanations.

\begin{itemize}
    \item \textbf{RQ3.} How do individual, contextual, and cognitive factors interact in the valuation process of explanations?
    \item \textbf{RQ4.} How do these factors collectively influence the preference on valuation of contrastive and selective explanations?
\end{itemize}

\begin{figure*}[h]%
\centering
\includegraphics[width=\textwidth]{figures/study-design.pdf}
\caption{A) \textbf{Study framework}: Our study aims to systematically explore the inner workings of how individuals attribute AI-generated decisions when confronted with AI-driven decisions (e.g., why was I denied a loan?) and evaluate explanations presented by AI systems. The study framework for human valuation on explanations presents a cognitive journey (in the order from a to f) explainees may go through while processing explanations about the decision. B) \textbf{Survey design}: To examine the valuation process, we distilled the framework into a survey for scenario-based experiment. C) \textbf{AI-assisted decision scenarios}: These decision scenarios are carefully selected to be contrasted in three aspects: high-stakes, professional, and timely.}
\label{fig:study-design}
\end{figure*}

\subsection{Theoretical Foundation and Rationale behind Variable Selection} \label{sec:variable-selection}
To design the framework, we have reviewed literature throughout a wide range of disciplines. Studies in social and cognitive science have been to examine theories and findings about how people attribute decisions and process information based on a given context, leading to shaping individual, cognitive, and contextual factors. An extensive review from and outside of XAI literature have led us to examine explanatory values (i.e., criteria for evaluating explanations in AI contexts).

\textbf{From various disciplines outside of XAI literature.} Literature from social and cognitive science have established theories and findings on individuals' cognitive capability and status in the decision-making and explanatory process. Our objective is to integrate these insights into AI decision contexts, to not only theoretically support our framework but also make connections between human and AI studies, from human-human and human-AI communication.

\begin{itemize}
    \item \textbf{\darkgray{Decision-making styles}} (Fig \ref{fig:study-design}A-a): Research in Psychology and Business has studied how individuals have different levels of cognitive tendency based on inner traits such as gender, age, or education. Especially when processing decisions and relevant information, individuals tend to exhibit different decision-making styles (See detailed literature review in Section \ref{sec:related-work-cognition}). We introduce five different styles in a scale called General Decision Making Style (GDMS) \cite{examinationgeneraldecisionmakingstyle}, including rational, avoidant, intuitive, dependent, and swift (details in Table \ref{tab:cog_vars}).
    \item \textbf{\darkorange{Cognitive engagement}} (Fig \ref{fig:study-design}A-c): In the literature from social science such as Education and Consumer Behavior, scholars have attempted to establish theories, often referred to as social attribution, theorizing individuals' internal cognitive states that drive behavioral changes (See detailed literature review in Section \ref{sec:related-work-cognition}) triggered by events such as a failure or success of their career or educational performances. In this study, we employ the MOA (Motivation, Opportunity, Ability) model \cite{EnhancingMeasuringConsumersMotivationOpportunity, jepson2018applying, fazio2014attitude} integrating three internal states, indicating whether they are more willing to act or change when they feel more motivated, perceive more opportunities or ability depending on surrounding environments or situations (details in Table \ref{tab:cog_vars}).
    \item \textbf{\darkgreen{Cognitive load}} (Fig \ref{fig:study-design}A-e): Cognitive load theory is a widely used construct to measure cognitive burden in humans’ information processing  (See detailed literature review in Section \ref{sec:related-work-cognition}). We examine three facets of cognitive load, including intrinsic, extraneous, and germane load. Each facet of cognitive load serves a distinct purpose: Intrinsic load is contingent on the difficulty of information covered in explanations, heavily influenced by specific contexts. On the other hand, extraneous and germane load pertain more to the presentation of information, and this variability is tied to the explanation strategies (details in Table \ref{tab:cog_vars}).
\end{itemize}

\textbf{From XAI literature.} Recent XAI studies have examined AI explainability specific to a context or multiple contexts to evaluate its effect in enhancing various explanatory values.

\begin{itemize}
    \item \textbf{\darkpurple{Explanatory values}} (Fig \ref{fig:study-design}A-f): We examined XAI surveys \cite{MetricsExplainableAIChallenges, TaxonomyHumanSubjectEvaluation, ScienceHumanAIDecisionMakingSurvey} investigating measures, criteria, and taxonomy for evaluating explainable AI. In our study, we employ multiple values for users to evaluate the effect of explanations, whether explanations have sufficiency (detailed and complete), understandability (helping understand why the decision was made), usefulness (facilitating comprehension of actions to take), trust (increasing the willingness to act on the basis of, the recommendations, actions, and decisions of an artificially intelligent decision aid)\footnote{A recent study \cite{HowEvaluateTrustAIAssistedDecision} provides the definition of trust in evaluating XAI systems and how it should be integrated in the experimental protocols. We note that our experimental setting and measure satisfy the three elements of trust discussed in the paper, including attitude (i.e., measures in the experiment capture how participants perceive the AI system), vulnerability (i.e., the given scenario involves uncertainty of the outcomes of a decision), positive expectation (i.e., the instruction ensures participants that the AI system is reliable and accurate).}, and overall preference.
    \item \textbf{\orange{Sociotechnical contexts}} (Fig \ref{fig:study-design}A-b): The six decision scenarios (Fig. \ref{fig:study-design}C) were selected to examine the impact of sociotechnical contexts. These decision contexts were carefully selected to differentiate between them in three contextual aspects: (a) {\it high-stakes}: a decision in the scenario involves significant consequences, (b) {\it professional}: a decision in the scenario requires one to have the professional knowledge to reason the given information, and (c) {\it timely} (or {\it time-critical}): a decision in the scenario must be made promptly.

    \indent We selected four application contexts widely studied in AI explanability studies including loan approval, medical diagnosis, driving app, and movie recommendation. We further differentiate them with the outcome of a decision -- whether the decision is favorable (positive) or unfavorable (negative) -- may alter people's expectations regarding the explanations. As illustrated in Fig.~\ref{fig:study-design}C, we create six scenarios featuring the various aspects: loan decision (\loanN) and driving app (\drivN) with undesirable decision, medical diagnosis with desirable (\mediP) and undesirable decision (\mediN), desirable (\recomP) and undesirable (\recomN) movie recommendation. The positive and negative signs indicate whether the provided information is generally desirable or undesirable.
\end{itemize}