\section{Discussion}\label{sec:discussion}
% Recent research in explainable AI (XAI) has sought to theorize about the characteristics of user-friendly explanations and cognitive frameworks. In the following sections, we discuss how our findings both support and challenge these existing frameworks, and offer  implications for the design of XAI interfaces (e.g., AI chatbots), with an emphasis on promoting interactive and personalized explanation processes.
Recent research in explainable AI (XAI) has focused on identifying the key features of user-friendly explanations and exploring cognitive frameworks that enhance users' understanding of AI-assisted decisions. In this section, we discuss how our findings both support and challenge existing theories of explanation design. We also provide practical recommendations for designing XAI interfaces, such as AI chatbots, highlighting the need to prioritize interactive and personalized explanation processes tailored to diverse user profiles and decision-making contexts.

\subsection{Effectiveness of Contrastive and Selective explanations}
% Alignment between analysis results and the properties of human-friendly explanations
% \label{sec:discussion-properties} 
% As discussed in Section \ref{sec:intro}, Miller \cite{miller2019explanation}, the properties of human-friendly explanation, particularly contrastive and selective explanations, have gained attention in a number of recent XAI literature and been used as a theoretical backbone in some studies \cite{SelectiveMutableDialogicXAIReview, SelectiveExplanationsLeveragingHumanInput}. Overall, our study finds empirical evidence that challenges two properties in AI-assisted decision contexts -- (1) \textbf{Contrastive}: In our study, contrastive explanation was not perceived as the most preferred or understandable in some contexts. Contrary to findings advocating for its intuitive and human-friendly nature, their values were highly dependent to given contexts (e.g., \ctt in time-sensitive and \cto in a win or loss situation). Participants' preferences also differentiated based on contrastive strategies (i.e., how and who/what to compare), for example, \cto or \ctt with contrastive received as actionable and relevant respectively depending on whether to contrast with other with the opposite outcome or previous state of myself. As described in Section \ref{sec:contrastive}, some participants perceived \cto explanation as less relevant than \ctt or \cf as it was not solely pertaining to themselves; (2) \textbf{Selective}: We found evidence from the analysis of open-ended responses that this property does not hold in general but should be understood in a nuanced manner. As summarized in Section \ref{sec:selective}, some participants found certain explanations preferable when they mention particular types of features (e.g., annual income, browsing history) that conform to prior beliefs. However, this type of rationale on their preferences were particularly salient in the loan approval or movie recommendations which require less professional knowledge compared with medical contexts with only a few of responses regarding information selectivity.

% Overall, our study challenges the assumption that selective and contrastive explanations are not always intuitive or advantageous. Instead, these strategies should be carefully introduced into AI systems with an understanding of individual and contextual differences.

As discussed in Section \ref{sec:intro}, the properties of human-friendly explanations---especially contrastive and selective explanations---have garnered increasing attention in recent XAI research and are often used as theoretical foundations \cite{SelectiveMutableDialogicXAIReview, SelectiveExplanationsLeveragingHumanInput}. However, our study presents empirical evidence that challenges two key properties in AI-assisted decision-making contexts:

\begin{enumerate}
\item {\bf Contrastive} explanations: In some contexts, contrastive explanations were not perceived as the most understandable or preferred. While earlier studies highlight their intuitive and human-friendly nature, our findings suggest their effectiveness is highly context-dependent (e.g., contrastive explanations were less effective in time-sensitive situations or when comparing outcomes like winning vs. losing). Participant preferences also varied based on the type of contrast (e.g., comparing with others' outcomes or with their own previous state). For example, when contrasting with another individual's outcome (\cto), participants found explanations actionable, whereas comparing their current state to a previous one (\ctt) was perceived as more relevant to personal decision-making. However, as discussed in Section \ref{sec:contrastive}, some participants found \cto explanations less relevant, as they often focused on their own circumstances.
\item {\bf Selective} explanations: Our analysis of open-ended responses revealed that selective explanations should not be assumed universally effective. Participants preferred explanations that highlighted specific features (e.g., annual income, browsing history), especially when those features aligned with their expectations. However, this preference was more pronounced in simpler decision contexts, such as loan approvals or movie recommendations, where professional knowledge is not required. In more complex contexts, like medical decision-making, participants were less likely to express preferences related to selective information, as noted in Section \ref{sec:selective}.
\end{enumerate}

Overall, our findings challenge the assumption that contrastive and selective explanations are universally intuitive or advantageous. These strategies should be carefully tailored to individual needs and contextual factors when incorporated into AI systems.


\subsection{Need to Balance Individual Engagement and Cognitive Load}
%Need of personalized cognitive device or framework.

% Recent XAI literature has introduced cognitive devices or frameworks such as cognitive forcing \cite{TrustThinkCognitiveForcing} or evaluative AI \cite{miller2023explainable} to enhance user engagement and mitigate the risk of over-reliance problems. These studies propose cognitive interfaces to let them engage in slow and deliberate thinking mode \cite{kahneman2011thinking} or a framework to let users contrast between multiple explanation options with different outcomes and generate hypotheses rather than recommendation-driven explanations. While these ideas offer ways to enhance user engagement, such mechanisms can impose a higher cognitive load especially for individuals predisposed to intuitive thinking. It can be hardly expected that participants who do not attend to explanations presented by systems at hand will actively engage in more complex explainability tasks. Our findings suggest that such explainability mechanisms need to be tailored to individual level of engagement rather than adopted as one-size-fits-all solutions. 

Recent XAI literature has introduced cognitive frameworks, such as cognitive forcing \cite{TrustThinkCognitiveForcing} and evaluative AI \cite{miller2023explainable}, aimed at enhancing user engagement and mitigating the risk of over-reliance. These studies suggest using cognitive interfaces that encourage slow, deliberate thinking \cite{kahneman2011thinking} or offer multiple explanations with contrasting outcomes, allowing users to generate their own hypotheses rather than relying solely on system-driven recommendations. While these approaches can improve engagement, they also risk increasing cognitive load, particularly for users inclined toward intuitive thinking. It is unrealistic to expect participants who typically overlook system-generated explanations to actively engage with more complex explainability tasks. Our findings suggest that these approaches should be tailored to individual levels of engagement rather than implemented as one-size-fits-all solutions.

% \textbf{Design Implications for AI Chatbot Interfaces to Facilitate Interactive and Personalized Explanations} Given the recent widespread use of chat-based AI services or large language models, more interactive forms of explanatory processes may take place in chatbot interfaces to provide rationales for AI-based decisions. Our study suggests several design implications for such interactive systems: 

% \begin{itemize}
%     \item \textbf{Personalized explanations}: Given various factors within individuals' cognitive traits and sociotechnical contexts, it is recommended to tailor explanations based on the cognitive states of users. This can be achieved by prompting questions to gather information about the user's level of engagement, knowledge, and contextual properties, and then tailoring the explanations to address the unique needs of each user. With sufficient user data available, a data-driven approach can be employed to accurately infer a user's cognitive states and other crucial attributes such as their demographic traits and decision-making styles. To alleviate potential errors in prediction, it is recommended to offer clear explanations about the methods used to make the prediction and also provide the users with the opportunity to rectify any incorrect predictions.
%     \item \textbf{Impact of prior knowledge}: To prevent the risk of biased preferences towards explanations that conform to incorrect prior knowledge (Section \ref{sec:selective}), the system can support participants to select a part of texts on explanations or press interfaces such as a question mark or ask icon to know more about features in the explanation (e.g., what does the feature indicate? How is it attributed as important?)
%     \item \textbf{Interactive generation of explanation strategies}: The systems can offer an interface with explicit options to let participants explore and choose various explanation strategies or modify explanations towards desirable explanatory properties such as simple, detailed, easy, or professional (Section \ref{sec:exp-properties}) or explanatory values such as understandable, trustworthy, learnable, actionable, selective, caring, or privacy-preserving.
% \end{itemize}

\subsection{The Role of Content and Tone in Shaping User Perceptions of AI Explanations}
%Negative Perception on Explanations Influenced by Content and Tone

% Our research discovered that explanation design has the potential to influence participants' negative perceptions. While most explanations were well received, 15 participants expressed dissatisfaction, particularly with the use of demographic information.
% Five people in the study reported feeling uneasy; one of them said, ``{\it [Anything] involving demographics or comparisons raises questions of profiling.}'' ``{\it [The] data they collect on me makes me distrust the system},'' said another participant. These responses suggest that the use of personal and comparative data can make some people feel uneasy or even distrustful of AI systems. 
% Moreover, our study revealed that the emotional tone of explanations is crucial. Six participants emphasized the importance of the explanation's tone, with one participant suggesting that "{\it explanations shouldn't sound cold or sarcastic.}" They favored AI systems that mimic human-like interactions by communicating politely and conversationally. This implies that creating a good explanation requires not only clarity and utility, but also emotional satisfaction. Although the emotional aspect of explanations was outside the scope of our study, it has been identified as a crucial factor in explanation design, especially in chat support scenarios, as noted in previous research \cite{UnderstandingBenefitsChallengesDeployingConversationala}.

Our research revealed that the design of explanations significantly affects participants' perceptions. While most explanations were well received, 15 participants expressed dissatisfaction, particularly with the use of demographic data. One participant noted, ``{\it [Anything] involving demographics or comparisons raises questions of profiling,}'' while another expressed distrust in the system, stating, ``{\it [The] data they collect on me makes me distrust the system.}'' These responses indicate that incorporating personal or comparative data can cause discomfort and raise concerns about privacy and fairness in AI systems.

Additionally, the tone of explanations emerged as an important factor in shaping user perceptions. Six participants stressed that the explanations should not sound ``{\it cold or sarcastic.}'' They preferred AI systems that communicated in a polite, conversational manner, suggesting that the emotional tone is as crucial as the content itself. While our study primarily focused on clarity and utility, these findings highlight the importance of incorporating human-like warmth into AI explanations, particularly in customer support scenarios, as noted in prior research \cite{UnderstandingBenefitsChallengesDeployingConversationala}.


\subsection{Design implications for AI chatbot interfaces}

With the growing adoption of chat-based AI services and large language models, chatbot interfaces have the potential to provide more interactive explanations for AI-driven decisions. Our study highlights several design recommendations for these systems:

\begin{itemize}
    \item {\bf Personalized explanations:} Explanations should be tailored to individual users based on their cognitive traits and contextual factors. This can be achieved by gathering information about the user's engagement, knowledge, and circumstances through interactive prompts. Where sufficient data is available, a data-driven approach can infer user preferences and cognitive states, though it is crucial to explain how these predictions are made and allow users to correct any inaccuracies.
\item {\bf Managing prior knowledge:} To prevent users from favoring explanations that reinforce incorrect prior knowledge (Section \ref{sec:selective}), the system should enable users to explore specific parts of the explanation. Features like question icons or clickable sections can provide further clarification (e.g., ``What does this feature mean?'' or ``Why is this feature important?'').
\item {\bf Interactive explanation customization:} Systems should offer users options to customize the style of explanations they receive. Users could select between simple or detailed explanations, or those that emphasize values such as trustworthiness, clarity, or privacy (Section \ref{sec:exp-properties}). This interactivity allows users to better control their experience and improve the relevance of the information provided.
\end{itemize}




% \subsubsection{Negative Perception on Explanations Influenced by Content and Tone.} 
% We discovered that explanation design has the potential to influence participants' negative perceptions. For example, 15 participants expressed their uneasy or distrustful feeling with the use of demographic information, for example, ``{\it [The] data they collect on me makes me distrust the system},''. Six participants emphasized the importance of the explanation's tone, such as "{\it explanations shouldn't sound cold or sarcastic.}" They favored AI systems that mimic human-like interactions by communicating politely and conversationally. This implies that creating a good explanation requires not only clarity and utility, but also emotional satisfaction.

% \subsection{Limitations}
% In this section, we summarize the limitation and the possible future work in three aspects: study scope, explanation design, and survey design.

% \paragraph{Application domains} The goal of our research was to simulate AI-assisted decision-making for data subjects, with explanations serving as an algorithmic rationale for AI's actions. However, the field of AI-assisted decision-making extends beyond this, involving a broad spectrum of users, tasks, and various aspects of human-AI interaction. For instance, previous research has considered problem-solving tasks, such as sentiment or content classification \cite{TrustThinkCognitiveForcing,HumanAIInteractionHealthcareThree}, or proxy tasks, such as maze-solving \cite{ExplanationsCanReduceOverrelianceAI}. These scenarios typically involve laypeople who, despite a lack of specialist knowledge, are required to make final decisions with the assistance of AI. Other studies have looked at professional contexts such as medical diagnosis \cite{DesigningAITrustCollaborationTimeConstrained, ExplainableAIDeadLongLive}, where clinicians use AI to help them diagnose diseases. Although our findings, such as individual differences in how explanations are valued based on cognitive characteristics and contexts, may largely apply to these tasks, other tasks may involve different human roles or AI-human collaborations. This could alter the value placed on different explanation strategies. Nonetheless, the nuances of each task, as well as its unique human-AI dynamics, should guide the explanation strategy approach.

% \paragraph{Algorithm or explanation aversion} Our survey was not designed to investigate the phenomenon known as explanation or algorithm aversion \cite{logg:2019algorithm, dietvorst:2015algorithm}, which refers to a dislike or distrust of AI explanations or systems. The reason for this is that our survey was not meant to compare the preference for explanations to the absence of explanations, but rather to evaluate preferences for various explanation strategies. For example, out of all our participants, only three expressed distrust with comments like, "{\it I don’t trust or respect its explanations}" and "{\it AI should not be diagnosing.}" Unfortunately, the design of our study did not permit us to delve further into these negative sentiments expressing algorithm aversion (or, conversely, algorithm appreciation). Understanding the nuance of these attitudes would benefit greatly from a more in-depth study. By contrasting a preference for explanations versus no explanation, we could gain a deeper understanding of the cognitive and contextual factors that may contribute to explanation or algorithm aversion.

% \paragraph{Measuring the perceived understandability.} Our study was intended to evaluate the perceived impact of explanation strategies. Given the subjective nature of our survey, the results may not provide an objective indication of participants' enhanced understanding of decisions regarding some factors such as understandability or germane load. In light of this limitation, future research could aim to evaluate how participants accurately make sense of decisions with the help of given explanations, under tasks designed to measure accuracy, task completion, or response time.

% \paragraph{Explanation quality.} Our study assumes that the explanations used in our experiment are accurate and faithful to the prediction. However, there is a potential for these explanations to be designed inaccurately or with malicious intent to manipulate users into accepting a decision. To cope with this issue, XAI systems may necessitate enhanced transparency, such as providing information on uncertainty or the input used to generate explanations. Exploring user vulnerability to ill-designed explanations could be a potential avenue for future research, involving comparative analysis and an investigation into user behavior and perception.


\subsection{Limitations and Future Directions}

This section outlines the study's limitations and potential future research across four key areas: application domains, explanation aversion, understandability measures, and explanation quality.

\paragraph{Application domains}
Our research focused on simulating AI-assisted decision-making for human data subjects, where explanations provide a rationale for the AI's actions. However, the broader field of AI-assisted decision-making encompasses a wide variety of tasks, users, and interactions. Previous studies have examined tasks such as sentiment analysis, content classification \cite{TrustThinkCognitiveForcing,HumanAIInteractionHealthcareThree}, and maze-solving \cite{ExplanationsCanReduceOverrelianceAI}, typically involving laypeople making decisions with AI assistance. In contrast, other studies have explored professional settings like medical diagnosis \cite{DesigningAITrustCollaborationTimeConstrained, ExplainableAIDeadLongLive}, where AI supports clinicians in diagnosing diseases. While our findings on individual differences in valuing explanations may apply to these diverse tasks, other domains may involve different human roles or AI-human collaboration dynamics, potentially altering the effectiveness of various explanation strategies. Future research should consider the unique requirements and challenges of specific application domains to refine explanation strategies accordingly.

\paragraph{Algorithm and explanation aversion}
Our survey did not specifically examine the phenomenon of algorithm or explanation aversion \cite{logg:2019algorithm, dietvorst:2015algorithm}, where users exhibit a general distrust of AI systems or their explanations. Instead, we focused on assessing preferences for different explanation strategies. Only three participants expressed explicit distrust, with comments such as, "{\it I don't trust or respect its explanations}" and "{\it AI should not be diagnosing.}" However, due to the design of our study, we were unable to explore these negative perceptions in detail. Future research should delve deeper into these attitudes, possibly by comparing user preferences for receiving explanations versus none. Gaining a clearer understanding of the cognitive and contextual factors that contribute to explanation or algorithm aversion could help in designing more effective and trustworthy AI systems.

\paragraph{Measuring perceived and actual understandability}
While our study focused on evaluating the perceived effectiveness of various explanation strategies, subjective perceptions alone may not fully capture the depth of participants' understanding. Perceived understandability is important, as it influences user satisfaction and trust, but it doesn't necessarily indicate how well users comprehend AI-driven decisions. Future research may incorporate objective metrics---such as task accuracy, decision-making speed, and users' ability to make informed decisions based on explanations---to better gauge actual understanding. By combining subjective and objective measures, researchers can gain a holistic view of how explanation strategies impact both user confidence and cognitive comprehension. This would provide more robust insights into the design of AI systems that not only explain decisions clearly but also empower users to act on them effectively. Improving both perceived and actual understanding will be crucial in building trustworthy, reliable AI systems that meet user needs.

\paragraph{Explanation quality}
In our study, we assumed that the explanations provided were accurate and trustworthy. However, there is always a risk that explanations could be designed to mislead or manipulate users. To address this concern, future XAI systems should prioritize transparency, offering users information about uncertainties or the data used to generate the explanations. Investigating user vulnerability to misleading explanations could be an important area of future research. This would involve exploring how users react to inaccurate or biased explanations and developing safeguards to protect against manipulation.
