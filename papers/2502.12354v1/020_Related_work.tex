\section{Related Work}
\label{sec:related-work}

\subsection{Valuation of Explanation Strategies in AI-assisted Decision-making}
\label{sec:related-work-exp}

Evaluating post-hoc explanations in the form of text has gained much attention as it provides rationales on AI-based decisions and impacts users' perceived trust and understandability  \cite{vilone2020explainable, lipton2018mythos, ExplainableSoftwareAnalytics}. Such explanations incorporating a variety of strategies and logics vary by techniques \cite{AreExplanationsHelpfulComparative, adhikari2019leafage} including feature importance or nearest neighbors or logics/strategies such as contrastive \cite{miller2019explanation, ConversationalProcessesCausalExplanation, DeductiveApproachCausalInference}, counterfactual \cite{Contrastscounterfactualscauses, CounterfactualStoryReasoningGeneration, CounterfactualExplanationsOpeningBlack, PsychologicalStudiesCausalCounterfactualReasoning} explanations in social and cognitive science, and case-based explanations from expert system studies \cite{Similaritymeasuresattributeselectioncasebased, GainingInsightCasebasedExplanation, EvaluationUsefulnessCaseBasedExplanation, SurveyCBRApplicationAreas}. 

However, recent XAI research has produced contradictory findings regarding the effect of explanation strategies. For example, providing such explanations in AI-assisted decision were found to help justify the decision and increase users' trust when compared with no explanation provided in some contexts such as medical chatbot \cite{ExploringPromotingDiagnosticTransparency} or self-driving context \cite{TrustingXAIEffectsdifferenttypes}. On the other hand, they tend to be distracting or lead to either overreliance \cite{ExplanationsCanReduceOverrelianceAI, PartneringPeopleDeepLearning} or cognitive overload \cite{WaitWhyAssessingBehaviorExplanation, EvolutionCognitiveLoadTheoryMeasurement} for lay users without a careful design of how explanations are presented \cite{CheXplainEnablingPhysiciansExploreUnderstand}. 
The impact of different explanation strategies in AI-assisted decision-making is also diverging. For example, complete explanations with a greater information complexity, for example, were proved to gain trust better in medical diagnosis \cite{EffectExplanationStylesUser, TooMuchTooLittlea, HowMuchInformationEffects} while they led to over-reliance on AI in medical diagnosis in \cite{RoleExplanationsTrustReliance}. Contrastive explanations known as intuitive and human-friendly were helpful in improving decision quality with semantic evidence in emotion recognition task \cite{RelatableExplainableAIPerceptualProcess}, while it was found to fail to calibrate the trust of AI in human perception in \cite{AreExplanationsHelpfulComparative}.

A recent XAI study \cite{SelectiveExplanationsLeveragingHumanInput} has advanced the discussion on making explanations more selective. The proposed framework draws on literature exploring how humans produce explanations selectively, focusing on aspects such as abnormality \cite{hilton1986knowledge}, relevance \cite{woodward2006sensitive}, and changeability \cite{hilton2007course}. Empirical results from this study suggest that allowing users to contribute to the generation of explanations reduces over-reliance on automated systems.

In this study, we pursue a deeper and comprehensive understanding of users' valuation on explanations strategies. By conducting a survey-based experiment, we find that the valuation process is a complex interplay of individual and contextual factors, highlighting the importance of personalizing the degree of explainability carefully based on individual traits and cognitive abilities.


\subsection{Individual and Cognitive Dimension of Explanatory Process}
\label{sec:related-work-cognition}

When making sense of a social situation such as interpersonal communication or decision-making processes, individuals go through certain cognitive processes to analyze, interpret, and remember information \cite{SocialCognition}. Previous studies found that these process are influenced by individualsâ€™ own cognitive tendency or given contexts. For example, people tend to have their own decision-making style \cite{DecisionmakingstylesreallifedecisionChoosing,Individualdifferencesadultdecisionmakingcompetence, DecisionMakingStyleDevelopmentAssessmentNew} -- whether they rely on hunches or a thorough search for information. According to the Motivation-Opportunity-Ability (MOA) model \cite{EnhancingMeasuringConsumersMotivationOpportunity}, individuals are either empowered or hindered in exhibiting behavioral changes or attributing decisions when making sense of their success and failure in their careers or education \cite{weiner1972attribution}. These cognitive processes can further influence the degree to which individuals seek more information to reason about situations, manifest in either spontaneous or deliberate modes within the dual processing theory \cite{SocialCognition, CogitoergoquidEffectCognitive, InfluenceCognitiveStylesUsersUnderstandinga}.

A number of studies \cite{PartneringPeopleDeepLearning, Doeshighereducationhonecognitive, Domainspecificpreferencesintuitiondeliberationdecision} have found that all these cognitive traits are highly dependent on their demographics. For example, older adults rely more on emotions and experience rather than being rational due to aging cognitive ability \cite{lockenhoff2018aging}. The level of education also influences the cognitive load, intelligence, thinking, and working memory \cite{Doeshighereducationhonecognitive}.  Rational thinkers tend to actively seek explanations when finding the best recommendations than intuitive thinkers. 

Despite these findings, there is limited understanding of the complex cognitive dimensions that affect individuals' willingness to seek explanations and their valuation of specific explanation strategies in AI-assisted decision-making. Our study explores these connections, highlighting the need for careful designed explanations that consider these cognitive processes. 


\subsection{Evaluating Explanations in Various AI-assisted Decision Contexts}
\label{sec:related-work-context}

The effectiveness of various explanation types in enhancing trust and understanding has been studied across diverse AI-assisted decision contexts, including human-robot interaction \cite{ExplainableAgentsRobotsResults, GuidelinesDevelopingExplainableCognitive, SelfExplainingSocialRobotsVerbal, TheoryExplanationsHumanRobotCollaboration, DifferentXAIDifferentHRI}, self-driving technologies \cite{ExplainableAutonomyStudyExplanation, TextualExplanationsSelfDrivingVehicles, TrustingXAIEffectsdifferenttypes}, and medical diagnosis \cite{humanbodyblackboxsupporting, EffectExplanationStylesUser, jimenez2020drug}.

For instance, in medical context, providing explanations helped improve health awareness, facilitate learning, and aid decision-making by offering patients new information about their symptoms \cite{ExploringPromotingDiagnosticTransparency} or help laypeople understand complex medical concepts during cancer diagnosis \cite{EffectExplanationStylesUser}. In self-driving contexts, explanations provided in a timely manner during sequential driving scenes have been found to improve understanding \cite{TrustingXAIEffectsdifferenttypes, ExplainableAutonomyStudyExplanation, TextualExplanationsSelfDrivingVehicles}.

Despite all these studies highlighting the impact of various explanation styles within a certain context, it remains uncertain how different explanation types affect the levels of trust, understandability, and other aspects of user perceived values across multiple contexts. While previous studies \cite{CapturingTrendsApplicationsIssues, ScienceHumanAIDecisionMakingSurvey, ehsan2023charting} have conceptually examined that AI transparency is entangled with sociotechnical contexts, our research empirically demonstrates that the effectiveness of explanations varies based on the application context, highlighting the need for context-aware design in explainable AI systems.
