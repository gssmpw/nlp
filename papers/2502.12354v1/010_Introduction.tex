\section{Introduction}
\label{sec:intro}

% Artificial intelligence (AI) has made a significant impact on our daily lives, from personalized purchase discounts to media recommendations. As there is a growing concern that AI systems may generate unfair or harmful decisions, it has become increasingly imperative to ensure transparency and explainability in the decision-making process \cite{lipton2018mythos}. While the field of eXplainable AI (XAI) has introduced various explanation strategies to enhance transparency, recent studies have shifted the focus towards more user-centered explainability and raised a crucial question: what makes an explanation \textit{good} in AI-driven decision-making?

% Introduced by \cite{miller2019explanation}, the properties of human-friendly explanations -- selective, contrastive, and conversational -- brought from findings in social and cognitive science have gained much attention. Among them, contrastive and selective explanations, which provide a minimal set of causes against the opposite case or focus on relevant information, have been widely accepted to a number of studies ranging from algorithmic methods to empirical studies.

% \begin{figure}[t]%
% \centering
% \includegraphics[width=0.7\columnwidth]{figures/teaser.pdf}
% \vspace{-1em}
% \caption{The process of humans' valuation of explanations. This study explores the interplay of \darkgray{individual-}, \orange{context-}, and \darkgreen{explanation-dependent} factors influencing the \darkpurple{valuation} of explanation strategies.}
% \label{fig:teaser}
% \end{figure}


% Despite the theoretical foundations that advocate for intuitive properties of such explanations, recent studies have yielded inconsistent findings. For instance, contrastive explanation was helpful in improving decision quality in emotion recognition task \cite{RelatableExplainableAIPerceptualProcess}, while they failed to calibrate the trust of AI in human perception in \cite{AreExplanationsHelpfulComparative}. Similarly, while comprehensive and detailed explanations have been found to facilitate an understanding of AI processes \cite{TooMuchTooLittlea, ExplainableAutonomyStudyExplanation}, they can also lead to over-reliance on AI-generated decisions \cite{RoleExplanationsTrustReliance} and pose a threat to user trust \cite{HowMuchInformationEffects}. These findings raise questions about whether the effect of contrastive and selective explanations, widely accepted as intuitive and human-friendly, really hold true or in what contexts they may be more beneficial than others.

% We argue that this misalignment can be attributed to a lack of comprehensive scrutiny of the underlying factors that shape laypeople’s valuation process over explanations. Studies in social and cognitive science have found that cognitive and individual differences, such as cognitive load and thinking styles, are recognized as significant determinants of the efficacy of explanations \cite{PartneringPeopleDeepLearning, SpanishvalidationGeneralDecisionMakingStyle, Doeshighereducationhonecognitive, Domainspecificpreferencesintuitiondeliberationdecision}. While recent XAI literature \cite{CapturingTrendsApplicationsIssues, ScienceHumanAIDecisionMakingSurvey, ehsan2023charting} have conceptually examined that AI transparency is entangled with sociotechnical contexts or individual differences, existing studies remain constrained by evaluating explanations within a single context and limited range of study subjects, failing to demonstrate how various factors can influence the impact of explanations.
  
% In response to these research gaps, our study shifts focus to the cognitive aspects of explanation evaluation in two aspects: 1) We evaluate explanation strategies that are widely accepted as intuitive -- contrastive and selective explanations, by conducting an experiment with laypeople to assess their preferences. Specifically, we explore six explanation styles from existing literature, differentiated by their use of contrastive strategies and information selectivity; 2) We further scrutinize how various factors -- individual, contextual, and cognitive factors, as identified through a review of cognitive and social science literature -- shape the valuation process. These factors are synthesized into a framework to explain when, why, and for whom specific explanation strategies are found to be preferable.

% In light of our analysis results, we challenge the assumption that selective and contrastive explanations are always intuitive or advantageous. Instead, we demonstrate that their effectiveness is highly context-dependent and varying depending on the context and the specific comparison or information being explained. Overall, our study provides evidence for a more context-sensitive and cognitively grounded approach to designing AI systems, highlighting the importance of considering contextual and cognitive factors in evaluating explanations. The contributions of this work are as follows:

% \begin{itemize}
%     \item The findings from our experiment contribute to a nuanced understanding of the efficacy of contrastive and selective explanations. We reveal that the perceived value of explanations is highly context-specific and varies among individuals, challenging the theory-driven properties of human-intuitive explanations.
%     \item We introduce a novel framework (Fig. \ref{fig:teaser}) designed to probe the process of human valuation of explanations. This framework enables us to scrutinize this intricate dynamics as a social and explanatory process synthesizing various factors—\darkgray{individual differences}, \orange{sociotechnical contexts}, and \darkgreen{cognitive aspects}— into the framework. While our study aimed to examine specific types of explanations, the framework is generally applicable to any other explanations in AI-assisted decision contexts.
%     \item Expanding upon our analysis, we provide pragmatic design implications for XAI interfaces to better accommodate individual and contextual differences. We suggest mechanisms to guide the generative process for explanations towards specific explanatory strategies or values, enhancing the usability and effectiveness of XAI systems.
% \end{itemize}
% \vspace{-0.5em}

Artificial intelligence (AI) has increasingly become integrated into everyday life, influencing decisions from personalized purchase recommendations to media content suggestions. However, concerns about AI's potential to produce unfair or harmful outcomes have highlighted the need for greater transparency and explainability in AI-driven decision-making \cite{lipton2018mythos}. In response, the field of eXplainable AI (XAI) has developed various explanation strategies aimed at making AI systems more transparent; However, a critical question still remains: what constitutes a {\it good} explanation in the context of AI decision-making?

Drawing from cognitive and social sciences, Miller \cite{miller2019explanation} introduced human-centered explanation properties---such as selective, contrastive, and conversational explanations---that have since gained considerable attention. Of particular interest are {\it contrastive} explanations, which highlight the differences between a decision and its alternatives, and {\it selective explanations}, which focus on the most relevant information. These strategies have been widely studied, from algorithmic methods to user-facing empirical studies.

\begin{figure}[t]%
\centering
\includegraphics[width=\columnwidth]{figures/teaser.pdf}
\caption{The process of humans' valuation of explanations. This study explores the interplay of \darkgray{individual-}, \orange{context-}, and \darkgreen{explanation-dependent} factors influencing the \darkpurple{valuation} of explanation strategies.}
\label{fig:teaser}
\end{figure}

However, despite their theoretical appeal, findings on these explanation types have been inconsistent. For instance, while contrastive explanations have been shown to improve decision quality in certain tasks like emotion recognition \cite{RelatableExplainableAIPerceptualProcess}, they have also failed to align AI trust with human perception in other contexts \cite{AreExplanationsHelpfulComparative}. Similarly, while detailed explanations can enhance users' understanding of AI processes \cite{TooMuchTooLittlea, ExplainableAutonomyStudyExplanation}, they also risk fostering over-reliance on AI decisions \cite{RoleExplanationsTrustReliance} and reducing trust in AI systems \cite{HowMuchInformationEffects}. These mixed findings raise the question: are contrastive and selective explanations universally beneficial, or do their advantages depend on specific contexts?

We argue that the inconsistencies in previous studies stem from insufficient attention to the cognitive and contextual factors that influence how users evaluate explanations. Research in cognitive science suggests that individual traits such as cognitive load and decision-making style play a critical role in how explanations are perceived and valued \cite{PartneringPeopleDeepLearning, SpanishvalidationGeneralDecisionMakingStyle}. Recent XAI literature has touched on the influence of sociotechnical contexts and user diversity on AI transparency \cite{CapturingTrendsApplicationsIssues, ScienceHumanAIDecisionMakingSurvey, ehsan2023charting}, but most studies are limited to single contexts or small sample sizes, preventing a comprehensive understanding of how these factors shape the impact of explanations.

To address these gaps, our study focuses on the cognitive dimensions of explanation evaluation. Specifically, we examine the following:
(1) We evaluate the effectiveness of widely accepted explanation strategies---contrastive and selective---through an experiment involving laypeople. By comparing six explanation styles drawn from existing literature, we assess how different approaches to contrast and selectivity influence user preferences.
(2) We explore how individual, contextual, and cognitive factors shape the evaluation process. Drawing on insights from cognitive and social science, we synthesize these factors into a comprehensive framework to explain when, why, and for whom specific explanation strategies are most effective.

\textbf{Study contexts.} While a variety of contexts in AI-assisted decision-making has been studied in existing literature, we particularly focus on the context of algorithmic decision-making that determines various aspects of individuals’ everyday lives, such as loan approval, medical diagnosis, autonomous driving, and movie recommendations. These contexts typically entail a passive nature of decisions made by algorithms embedded within AI systems or infrastructures in various socio-technical contexts, where decisions are notified or presented to data subjects, triggering their cognitive responses or emotions on the given decisions. This contrasts with the general or more objective nature of AI-assisted decision-making contexts, such as image/sentiment classification, where explanations can assist with better understanding of feature attribution. Due to this nature, the design of our study, which best approximates the context with written scenarios then asks subjective ratings of users, centers on exploring their perceived and cognitive responses, along with the various factors that shape them, particularly when explanations describe them, contain personal information, and contrast with others’.

Overall, our findings challenge the assumption that selective and contrastive explanations are universally intuitive or advantageous. Instead, we demonstrate that their effectiveness is highly dependent on context and user characteristics. This evidence supports the need for a more context-sensitive, user-centered approach to designing XAI systems. The key contributions of our study are as follows:
\begin{itemize}
\item {\bf Context-specific values of explanations:} Our study reveals that the perceived value of contrastive and selective explanations varies significantly by context and individual traits, challenging the assumption that these strategies are inherently human-friendly.
\item {\bf Novel framework for explanation evaluation:} We introduce a comprehensive framework (Fig. \ref{fig:teaser}) that captures the complex interplay of \darkgray{individual differences}, \orange{sociotechnical contexts}, and \darkgreen{cognitive aspects} in explanation valuation. While our study focuses on specific types of explanations, this framework can be applied broadly across various AI decision-making contexts.
\item {\bf Design implications for XAI:} We offer actionable insights for designing more interactive and personalized XAI systems. Our recommendations emphasize the need to adapt explanation strategies based on user characteristics and contextual factors, improving both the usability and trustworthiness of AI systems.
\end{itemize}
