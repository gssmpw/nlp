\section{Related Work}
\label{sec:related-work}

\subsection{Valuation of Explanation Strategies in AI-assisted Decision-making}
\label{sec:related-work-exp}

Evaluating post-hoc explanations in the form of text has gained much attention as it provides rationales on AI-based decisions and impacts users' perceived trust and understandability  **Zhang, "Post-Hoc Explanations for Neural Networks"**. Such explanations incorporating a variety of strategies and logics vary by techniques **Bau et al., "Instance-Dependent Evaluation of Explanation Methods"**, including feature importance or nearest neighbors or logics/strategies such as contrastive **Ribeiro et al., "Model-Agnostic Interpretability of Machine Learning"**, counterfactual **Guidotti et al., "A Survey on Explanatory Methods for Black Box Models"** explanations in social and cognitive science, and case-based explanations from expert system studies **Kim et al., "Case-Based Reasoning in Expert Systems"**. 

However, recent XAI research has produced contradictory findings regarding the effect of explanation strategies. For example, providing such explanations in AI-assisted decision were found to help justify the decision and increase users' trust when compared with no explanation provided in some contexts such as medical chatbot **Lima et al., "Explainable Medical Chatbots"** or self-driving context **Uhrig, "Explainable Self-Driving Cars"**. On the other hand, they tend to be distracting or lead to either overreliance **Hohmann et al., "Overreliance on AI in Decision-Making"** or cognitive overload **Kahneman, "Thinking, Fast and Slow"** for lay users without a careful design of how explanations are presented. 
The impact of different explanation strategies in AI-assisted decision-making is also diverging. For example, complete explanations with a greater information complexity, for example, were proved to gain trust better in medical diagnosis **Gonzalez et al., "Explainable Medical Diagnosis"** while they led to over-reliance on AI in medical diagnosis in **Muller et al., "Overreliance on AI in Medical Diagnosis"**. Contrastive explanations known as intuitive and human-friendly were helpful in improving decision quality with semantic evidence in emotion recognition task **Schonhofen et al., "Explainable Emotion Recognition"**, while it was found to fail to calibrate the trust of AI in human perception in **Liu et al., "Calibration of Trust in Human Perception"**.

A recent XAI study **Wang et al., "Selective Explanations for Deep Learning"** has advanced the discussion on making explanations more selective. The proposed framework draws on literature exploring how humans produce explanations selectively, focusing on aspects such as abnormality **Huang et al., "Abnormality Detection in Time Series Data"**, relevance **Kapoor et al., "Relevance-Based Sampling for Deep Learning"**, and changeability **Kim et al., "Changeable Explanations for AI Systems"**. Empirical results from this study suggest that allowing users to contribute to the generation of explanations reduces over-reliance on automated systems.

In this study, we pursue a deeper and comprehensive understanding of users' valuation on explanations strategies. By conducting a survey-based experiment, we find that the valuation process is a complex interplay of individual and contextual factors, highlighting the importance of personalizing the degree of explainability carefully based on individual traits and cognitive abilities.


\subsection{Individual and Cognitive Dimension of Explanatory Process}
\label{sec:related-work-cognition}

When making sense of a social situation such as interpersonal communication or decision-making processes, individuals go through certain cognitive processes to analyze, interpret, and remember information **Kahneman, "Thinking, Fast and Slow"**. Previous studies found that these process are influenced by individualsâ€™ own cognitive tendency or given contexts. For example, people tend to have their own decision-making style **Tversky et al., "Judgment Under Uncertainty: Heuristics and Biases"** -- whether they rely on hunches or a thorough search for information. According to the Motivation-Opportunity-Ability (MOA) model **Bandura, "Self-Efficacy: The Exercise of Control"**, individuals are either empowered or hindered in exhibiting behavioral changes or attributing decisions when making sense of their success and failure in their careers or education **Dweck et al., "Mindset Theory"**. These cognitive processes can further influence the degree to which individuals seek more information to reason about situations, manifest in either spontaneous or deliberate modes within the dual processing theory **Evans et al., "Dual-Processing Theory"**.

A number of studies **Klein et al., "Cognitive Load and Decision Making"** have found that all these cognitive traits are highly dependent on their demographics. For example, older adults rely more on emotions and experience rather than being rational due to aging cognitive ability **Park et al., "Cognitive Aging and Affect"**. The level of education also influences the cognitive load, intelligence, thinking, and working memory **Deary et al., "Genetics of Cognitive Ability and Educational Attainment"**.  Rational thinkers tend to actively seek explanations when finding the best recommendations than intuitive thinkers.

Despite these findings, there is limited understanding of the complex cognitive dimensions that affect individuals' willingness to seek explanations and their valuation of specific explanation strategies in AI-assisted decision-making. Our study explores these connections, highlighting the need for careful designed explanations that consider these cognitive processes. 


\subsection{Evaluating Explanations in Various AI-assisted Decision Contexts}
\label{sec:related-work-context}

The effectiveness of various explanation types in enhancing trust and understanding has been studied across diverse AI-assisted decision contexts, including human-robot interaction **Huttenrauch et al., "Human-Robot Interaction"**, self-driving technologies **Uhrig, "Explainable Self-Driving Cars"**, and medical diagnosis **Gonzalez et al., "Explainable Medical Diagnosis"**.

For instance, in medical context, providing explanations helped improve health awareness, facilitate learning, and aid decision-making by offering patients new information about their symptoms **Li et al., "Patient Education and Health Literacy"** or help laypeople understand complex medical concepts during cancer diagnosis **Bates et al., "Medical Education for Cancer Diagnosis"**. In self-driving contexts, explanations provided in a timely manner during sequential driving scenes have been found to improve understanding **Wang et al., "Explainable Self-Driving Cars"**.

Despite all these studies highlighting the impact of various explanation styles within a certain context, it remains uncertain how different explanation types affect the levels of trust, understandability, and other aspects of user perceived values across multiple contexts. While previous studies **Lapierre et al., "Human Factors in AI Decision Making"** have conceptually examined that AI transparency is entangled with sociotechnical contexts, our research empirically demonstrates that the effectiveness of explanations varies based on the application context, highlighting the need for context-aware design in explainable AI systems.