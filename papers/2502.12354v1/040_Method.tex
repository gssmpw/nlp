\section{Method}\label{sec:method}

\subsection{Study Design}\label{sec:study-design}

We designed a survey experiment to investigate the process of evaluating explanation and answer the research questions. The survey was designed as a between-subject experiment, where participants were randomly assigned to one of the six decision scenarios.

Overall, the survey was designed to emulate the process of humans' valuation of explanation strategies (Fig. \ref{fig:study-design}A), consisting of multiple stages (a-f) as illustrated in Fig. \ref{fig:study-design}B (see the details for the survey material in the \appsec{sec:survey}): a) Participants were asked questions about their individual characteristics including demographics and decision-making style; b) A randomly assigned scenario was presented to engage participants in the AI-assisted decision-making context. To devise each scenario, we consulted with domain experts to determine the details of the outcomes and features, and various user cases that serve as counterparts of the focal data subject regarding contrastive and analogous comparisons; c) Questions regarding perceived contextual properties (i.e., high-stakes, professional, timely) and cognitive engagement (i.e., motivation, opportunity, ability) were prompted to examine context-dependent factors; d) Six explanation strategies were presented; e,f) users were asked to rank explanations to rate their cognitive load as well as explanatory values. Finally, we collected participants' opinions and reasoning for the explanation variants in free-form text responses.

% \begin{figure}[t]%
% \centering
% \includegraphics[width=\columnwidth]{figs/contexts.pdf}
% \caption{The six scenarios of AI-assisted decision-making in our study. These decision scenarios are carefully selected to be contrasted in three aspects: high-stakes, professional, and timely.}
% \label{fig:contexts}
% \end{figure}

\subsection{Study Implementation}
\label{sec:study-procedure}

\paragraph{Participants}\label{sec:participants}
We recruited participants from Prolific crowdsourcing platform. Workers living in the US (age 18+) are fluent in English were eligible to participate in this study to make sure their ability to reason about the given scenario in English. One of the three rounds aimed to recruit senior people (age 55 and older), as they are typically underrepresented on the crowdsourcing platform. A total of 839 participants took part in the study. We excluded respondents who left their responses as default or who did not respond to all survey questions. This yielded a final sample of 698 participants for our data analysis (390 females, age: 222 participants were between the ages of 18 and 24 and 62 were 65 or older; 1 participant preferred not to state age, and 3 participants preferred not to say their gender). Fig. \ref{fig:demographics} in the \appsec{sec:demo-breaks} provides the demographic breakdown of the participants. The experiment was approved by the Institutional Review Board of the University\footnote{The university name was omitted for blind review.}. We pre-registered the experiment on the Open Science Framework (OSF)\footnote{\url{https://osf.io/hp86t/?view_only=f49fb230b8e8478288d4844869a88863}}. 

\paragraph{Sample size determination}
We used the Mann-Whitney test to determine the sample size. Based on the power analysis with a significance level of 0.05 and a medium effect size, we determined that at least 106 samples per group were needed, resulting in a suggested minimum sample size of 636 for six survey variants, in order to ensure that the effect of the explanations could be tested.

\paragraph{Procedure}
The survey experiment was conducted on Prolific for three rounds in 2022, on January 29, March 29, and April 25. Participants receive a small amount of compensation with the base rate of \$7.74 per hour in exchange for their effort. The survey consisted of 19 questions, seven of which asked for demographic and survey identification information. The median time for a respondent to complete the survey was 11.6 minutes. 

In the context of AI-based decision-making, participants were tasked with evaluating various types of explanations as decision-making rationales. The survey began with questions regarding demographics. Participants were then asked to read a paragraph describing one of six AI-assisted decision-making scenarios provided by the researchers. Given the context, participants first rated their perceptions of contextual properties and cognitive engagement when presented with the scenario. These questions use a five-point  Likert scale with values ranging from 1 to 5. Participants were then instructed to read each explanation style presented in random order and rank them according to their preferences. At the end of the study, participants provided written comments on the rationale behind their choice of the preferred explanations and thoughts about the AI system.

\subsection{Statistical Analyses}
\label{sec:stat}

We conducted multiple types of statistical analyses to examine the research questions listed in Section \ref{sec:rqs}. As a pre-processing step, we converted all ranking responses to a {\it relative rating} variable, such that higher scores indicate positive cognitive loads and explanatory values. 

For RQ3, we employed the Mann-Whitney U test to determine if significant differences existed between two distinct sets of scenarios (e.g., high-stakes vs. low-stakes contexts). The Wilcoxon signed-rank test was additionally utilized for identifying significant differences between pairs of styles, acknowledging that the relative ratings across these styles are interdependent. In our pairwise testing, we applied the Bonferroni correction to adjust for multiple comparisons. Given that our analysis involved both 6 scenarios and 6 styles, the significance level was accordingly adjusted to 0.05/6, which is approximately 0.0083.

To examine RQ4 in scrutinizing the interplay of individual-, cognitive-, and explanation-dependent factors, we used Structural Equation Modeling (SEM) to take into account all direct and indirect relationships between variables included in our study framework (Fig. \ref{fig:study-design}A). For these models per explanation type, we converted participants' ratings of explanation styles into a binary variable indicating whether a particular style was absolutely favorable (i.e., ranked as top or second one) or not.


\subsection{Analysis for Open-ended Questions}
\label{sec:method-open}

In the survey, we further collected open-ended responses from participants to ask for their detailed rationales for their overall preferences with two questions: (1) Detailed rationales on their valuation process of ranking the explanations with the question, “please briefly describe the rationale on ranking the explanations,” and (2) general perceptions on values of explanations and AI systems, “what is the most important aspect of explanations for you? Do you have any additional comments about the AI system?”

\begin{figure}[h]%
\centering
\includegraphics[width=\columnwidth]{figures/overallPref.pdf}
\caption{The relative ratings (along the $x$-axis) for each of the six explanation variants for the {\it extraneous} and {\it germane} cognitive capacities influenced by how information is presented, as well as the preference rankings in five distinct value dimensions. To facilitate the summary, participants' {\it overall} preference was highlighted in gray.}\label{fig:overallPref}
\end{figure}

To extract and summarize the characteristics of their evaluation process from texts, we took a two-step approach. First, we used qualitative coding to identify recurring themes in participants' valuations. Two coders read a random sample of 30\% of written responses and engaged in multiple rounds of discussion to group them into high-level categories, resulting in four distinct types of valuations: explanatory properties, features, comparison, and XAI system. With these categories, we iterated over all written responses to quantitatively and qualitatively identify whether each written response contains any of four valuation categories (e.g., explanatory values) and what specific values (e.g., simple, easy) are encoded. The definitions and methods of analysis for four categories are summarized below:

\begin{itemize}
    \item \textbf{Property-oriented valuation}: 524 responses mentioned the desired properties of explanations regarding information quantity, logic, or value (e.g., brief, simple, informative). We extracted adjectives that express positive explanatory values by conducting the part-of-text analysis and filtering out negative and irrelevant words.
    
    \item \textbf{Feature-oriented valuation}: 135 responses referred to certain feature(s) given in the decision-making scenario (e.g., credit score, BMI, weather, genre, or age) as a rationale of why they chose certain explanations as the least/most preferred. From these responses, we extract some findings on what aspects of information selectivity mainly influence the valuation of explanations.
    
    \item \textbf{Comparison-related valuation}: 77 responses preferred certain type(s) of contrastive strategies. As this type of feedback was expressed in an unstructured manner, we manually coded the types of comparisons mentioned in their responses.
    \item \textbf{XAI-system-related valuation}
    25 responses expressed their expectations on how explanations in AI systems were received or what they are supposed to do. Similarly, we manually coded whether each response contained system-related valuation.
\end{itemize}



