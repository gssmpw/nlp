\section{Conclusion}\label{sec:conclusion}

% Prior studies have shown inconsistent findings on the effect of explanation strategies. Based on a review of literature in cognitive science, our study seeks to investigate a deeper understanding of what factors shape individuals’ valuation of different explanation strategies. We designed a scenario-based survey and conducted between-subject experiment that distilled the complex relationship among human valuations, explanations, and various contextual properties, built on social and cognitive science literature. Our findings showed that the properties of human-friendly explanations -- selective and contrastive -- are not necessarily advantageous: complete explanations are the most preferable for most values; selective explanations are preferable much more in less professional context; and participants prefer different comparative strategies for different reasons rather than contrastive explanations. Throughout the analysis, we found that various factors ranging from demographic traits, cognitive load to contexts impact individual differences in favoring different types of explanation and comparative strategies. We discussed that these findings imply that cognitive devices or framework that promote deliberate thinking and tasks with a one-size-fits-all  approach may fail to successfully engage in individuals with different cognitive abilities.

% In the upcoming era of AI systems affecting nearly everyone's lives, it is urgently needed to tailor the explainability to individuals’ contexts. In this context, this study provides a nuanced view of explanation strategies, and conveys implications for designing more interactive and personalized AI services to accommodate individual and context differences. We believe this will make ways to improve user interaction and trust, and make the technology more accessible. 

Previous research on explanation strategies has yielded mixed results, leaving gaps in understanding how individuals value different types of explanations. Drawing from cognitive and social science literature, our study aimed to explore the specific factors influencing user preferences for various explanation strategies. Using a scenario-based survey and a between-subject experiment, we investigated the relationships between explanation types, human valuations, and contextual factors.

Our findings challenge the assumption that human-friendly explanations, such as selective and contrastive strategies, are always beneficial. We showed that complete explanations were generally preferred across most values, while selective explanations were favored primarily in less professional contexts. Additionally, participants showed diverse preferences for comparative strategies, instead of the contrastive approach typically emphasized in prior studies.

We identified several key factors---demographics, cognitive load, and contextual conditions---that significantly shape individual preferences for explanation and comparative strategies. These results suggest that one-size-fits-all cognitive frameworks, including those promoting deliberate thinking, may fail to effectively engage users with varying cognitive abilities and needs.

As AI systems increasingly influence daily decision-making, there is an urgent need to personalize explainability according to users' individual contexts and cognitive traits. Our study offers actionable insights for designing AI systems that deliver more interactive and tailored explanations. By accommodating user diversity, these systems can enhance user engagement, foster trust, and make AI technologies more accessible to a broader audience.
