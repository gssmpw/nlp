\section{Continuous Credibility Building}
\label{sec:ongoing}

The three model development components depicted in the top of Figure~\ref{fig:model-development} appear sequential. 
However, practical SciML development is iterative, often requiring steps to be repeated.
Poor validation metrics may necessitate model updates or additional data collection. 
Sometimes, simplifying objectives or the QoI scale may be warranted to achieve more accurate results for a simpler but still valuable problem. 

The iterative nature of modeling requires ongoing documentation of the critical aspects that affect V\&V including data sources and processing, loss functions and optimizers and model tuning. Additionally, it is important to perform software quality assurance (SQA), model comparisons, and explain and interpret predictions and model behaviors. Enacting these steps is referred to as continuous credibility building in Figure \ref{fig:model-development}.

\subsection{Data Sources}
\label{sec:data-sources}

\begin{essrec}[Document data characteristics and impact]
Document the properties of all data sources used, including their fidelity level, inherent biases, noise characteristics, and quantity limitations. Explain how these data characteristics affect model performance and V\&V processes. Determine data requirements based on specific accuracy needs for the intended application rather than general guidelines. When using simplified or lower-fidelity data, quantify its impact on model predictions and consider multi-fidelity approaches to mitigate potential biases.
Following this recommendation enables reproducibility, helps identify potential sources of model error, and ensures transparency about the model's limitations due to its training data.
\end{essrec}


In addition to listing prior knowledge that can be used to build an ML model, the types of data used to train a model affects model V\&V. The data sources, which will be used to train the SciML model, must be carefully chosen and the reasons for those choices must be documented. Important data properties to consider are data fidelity and bias, data noise and the cost of collecting training data, which limits the amount that can be collected. Some studies make recommendations on the amount of data that should be used to train an ML model based on the number of model input features~\cite{Zhu_YR_EST_2023}. However, we believe the amount of data should be defined by the accuracy requirements of the intended use of the SciML model. Future work on generalization accuracy and statistical learning theory for SciML models will help better elucidate these mathematical trade-offs.

When training a SciML model, the fidelity of data impacts the performance of a model. For example, when training using simulation data, the numerical discretization of the model generating the data is important. Ideally, training data should be generated using the most accurate (highest-fidelity) discretization. However, often a cheaper but less accurate discretization (lower-fidelity) is used because it allows for more training data to be collected. While more low-fidelity training data allows a better approximation of the low-fidelity simulation model, it introduces a bias in the SciML model relative to the high-fidelity model. The impact of this error on quantities predicted by a model, for instance the optimal objective of a design problem, must be quantified. Alternatively, multi-fidelity SciML models, which combine limited high-fidelity data with larger amounts of low-fidelity data~\cite{Gorodetsky_JG_CM_2021, Meng_K_JCP_2020, Cutajar_PDLG_Arxiv_2019}, can sometimes be used to mitigate this issue.


The creation of informative benchmarks can substantially improve the quality of V\&V analyses and enable easier comparison of methods and SciML structures. However, unlike traditional ML, SciML, and \CSE, lack a set of commonly-used and agreed-upon benchmark problems, for both solution verification and validation. 
Consequently, further work is needed to complement early efforts to create SciML benchmarks~\cite{Takamoto_PLMAPN_NIPS_2022}. Motivated by the work of Oberkampf~\cite{Oberkampf_TH_2004}, we believe the purpose and details of the benchmark should be precisely stated, including how comparisons with the benchmark must be made and the the exact verification or validation acceptance criteria. 
For example, a natural acceptance criteria for a SciML surrogate used for UQ is that the surrogate can accurately compute the mean and variance of the true model it replaces to a specified level of accuracy. In contrast, a reasonable acceptance criterion of a SciML model used for optimization would be based on the comparison of the optimal solution obtained by the SciML model with known or trusted reference optima. Lastly, benchmarks must be made widely accessible, though this can be difficult when some benchmarks are associated with terabytes of data.
Thus, until community benchmarks and mechanisms for their dissemination are developed, it is crucial that researchers thoroughly document these choices and explain the significance of the problems they create. 

\subsection{Data processing}

\begin{essrec}[Document data processing procedures]
Document all procedures used to handle missing or abnormal data and any transformations applied to the data. Explain the rationale behind each processing choice and its potential impact on model performance. This documentation enables reproducibility and helps identify potential sources of bias introduced during preprocessing.
\end{essrec}

Before an ML model can be constructed, the data that will be used to train the ML model must be processed for missing and abnormal data.\footnote{It goes without saying that unprocessed raw data should be preserved and properly archived before any transformations are applied.} 
When building ML models from simulation data, missing data can occur when one or more simulations fail to complete, caused for instance by the model becoming unstable for a certain set of inputs, or the computer system crashing. Similarly, abnormal data can be generated when the instability of the model does not cause the simulation to fail but leads to errors, or when a soft fault occurs on a high-performance computing system.
The methods employed for dealing with abnormal data---such as removal, interpolation or imputation---must be defined as they can substantially impact the accuracy and credibility of a SciML model. 

Variable transformations are also often used to process data when training a SciML model. For instance, it is common to normalize the data to have zero mean and unit standard deviation, or to take the logarithm of the data with the goal of improving the predictive performance of a SciML model.
Modelers should report the use of any transformations because the use of different transformations can lead to substantial changes to the predictive performance of the ML model. 
Moreover, data processing scripts should be preserved in a trusted repository and published alongside other study artifacts.



\subsection{Loss functions and optimizers}\label{sec:loss-and-opt}

\begin{essrec}[Quantify SciML model sensitivity]
Test and document the model's sensitivity to optimization parameters including random initialization, stochastic processes, and regularization choices. Report all optimization configurations and hyperparameters, and quantify how variations in these choices affect model performance. This systematic assessment helps identify potential instabilities in the training process and ensures others can reproduce results, while providing confidence bounds on the model's reliability.
\end{essrec}

Training an ML model requires minimizing a loss function that quantifies how well the model approximates the training data. This loss function is often the mean squared error or mean absolute error. Nevertheless, a plethora of other loss functions can be used. Indeed, the loss function should be tailored to the intended use of the ML model. For example, the mean squared error captures the average size of the squared residuals between the ML model and the data, but treats positive and negative residuals equally. Thus, if learning a function intended to capture large events it may be more appropriate to train the SciML model using asymmetric loss functions such as the quantile loss function~\cite{Jakeman_KH_RESS_2021}.

Additionally, the optimization method used to minimize the loss function can also substantially impact model trustworthiness. For example, it is common to use stochastic optimization methods such as Adam~\cite{Kingma_B_arxiv_2017} to train a SciML model. Thus, the optimal solution depends on the random seed, the initial condition and the computing platform used to perform the optimization. Consequently, the impact of this randomness must be quantified, for example, by reporting the variability in the error of the SciML model learned using different perturbations of these factors.

\subsection{Model tuning}

\begin{essrec}[Document the hyperparameter selection process]
Document all aspects of hyperparameter selection, including the optimization method used, the ranges of values tested, and the criteria for final selection. Explain how model complexity was balanced against performance, and report any cross-validation procedures used to prevent overfitting. Such documentation enables reproducibility, supports model interpretability, and allows other researchers to understand the trade-offs made in model development while ensuring the selected configuration is appropriate for the intended application.
\end{essrec}

Training a SciML model is typically an iterative process, which involves optimizing the parameters of a model, e.g., weights and biases of a neural network, for different values of the model hyper-parameters, e.g., the depth and width of a neural network, also called hyperparameter tuning~\cite{Feurer_H_AMLMSC_2019}. 
Model structures with large numbers of parameters (common in SciML) can lead to overfitting, where the model performs well on the training data but poorly on new, unseen data. Additionally, an overly complex model can become less interpretable, making it harder for researchers to understand and explain the model's predictions. Furthermore, it typically becomes harder to identify the impact of each parameter, a problem known as reduced parameter identifiability~\cite{Guillaume_Jetal_EMS_2019}, as the number of model parameters increases. 
The final choice of hyperparameters can significantly impact a model's predictive power, interpretability, and the uncertainty of its predictions. Yet, despite the importance of hyperparameter tuning, a review of over 140 highly cited papers using ML for environmental research~\cite{Zhu_YR_EST_2023} found that only 19.6\% of those papers conducted formal hyperparameter optimization methods (such as grid search) and another 25.7\% used trial and error. To ensure that the scientific community can fully evaluate and understand published works in SciML, detailed documentation of these processes is essential. 

\subsection{Software quality assurance and reproducibility}

\begin{essrec}[Use software testing and ensure reproducibility]
Implement comprehensive testing across the entire model development workflow using unit, system, integration, acceptance, and regression tests, while accounting for ML-specific challenges like stochastic behavior and emergent properties. Release complete code repositories containing source code, trained models, data, reproduction scripts, and detailed environment documentation including software versions and computing requirements. This testing and documentation enables independent verification of results, supports long-term maintenance, and builds trust in the model's implementation.
\end{essrec}

Software quality assurance (SQA) is required by both \CSE{} and SciML models to minimize the impact of programming errors in model predictions, facilitate collaboration, and build credibility into the modeling activity. Some \CSE{} guidelines call for SQA as part of code verification~\cite{Oberkampf_T_PAS_2002}, given its focus on correctness of the software by finding and fixing programming errors or `bugs' that are prevalent in all software~\cite{Hatton_book_1997, Zhang_IEEECSS_2009}. 
While SQA is an essential component of code verification, we discuss it separately here to emphasize its importance to the entirety of \CSE{} and SciML workflows. 

SQA requires both static analysis~\cite{Louridas_IEEES_2006,Fatima_BH_book_2018} and dynamic testing~\cite{Jamil_ANA_ICT4M_2016}.  
Static analysis checks for errors without executing code and can be applied to an entire code base before dynamic tests are executed.
This is useful because runtime tools, such as Valgrind~\cite{Valgrind_2007}, only test the code that is executed and thus can miss corner cases that are not tested~\cite{Gulabovska_P_IEEE_2019}.
Dynamic testing comprises unit tests, integration tests and system tests.
All of these software engineering practices have low adoption in ML software~\cite{Seban_BHV_ACM_2020}.
Software engineering is particularly challenging in SciML models because it has ``\ldots all of the problems of non-ML software systems plus an additional set of ML-specific issues''~\cite{Wan_IEEE_XLM_2021}.
While deterministic CSE can often use theoretically derived test ``oracles'' that capture the expected behavior of a model~\cite{Johanson_H_CSE_2018}, ML's stochastic nature requires aggregating multiple model runs to assess variability.
System-wide testing is often needed to understand emergent properties, making unit testing less useful.
Additionally, code coverage metrics are less meaningful for SciML since decision logic is relearned with each training~\cite{Zhang_HML_IEEE_TSE_2022}.
Despite the importance of SQA, it is often neglected in both CSE and SciML. An analysis of 62 scientific software studies found that most used only one type of testing, and none used all types (unit, system, integration, acceptance, and regression testing)~\cite{Kanewala_B_IST_2014}. 
Improving SQA adoption is thus crucial for building trustworthy SciML models.
SciML studies should make every effort to establish confidence in the software implementation, via modular tests and benchmarks, and adoption of software engineering best practices. 

Reproducibility is essential for credibility building of computational models~\cite{nasem_2019}, and can be understood as another form of quality assurance. 
For reproducibility, authors should provide comprehensive documentation, including data specifications, computing requirements, software versions, and public code repositories. The optimized parameter and hyper-parameters of SciML models, e.g., the width and depth of a neural network and its weights and biases, should also be released. Repositories should be well-organized with tutorials and reproduction scripts~\cite{Kapoor_et_al_SA_2024}, and standard public licenses should be adopted that stipulate reuse rights of the artifacts. 

Warnings about lack of reproducibility in ML research have been raised before~\cite{hutson2018reproducibility,haibe2020transparency}.
A common failing, with an obvious solution, is not sharing the source code associated with the published research. 
Yet simply depositing the software in a public repository is far from sufficient to ensure reproducibility in ML research. 
Rigorous documentation of the training pipeline is necessary, including details of the optimization and regularization applied, activation functions, hyperparameters, learning-rate schedules, and more.
Data provenance, as discussed earlier, should be managed strictly and documented; ideally the full data sets should be deposited in trusted repositories that provide a persistent identifier. 
None of these efforts is minor, and sometimes they may seem unrewarding, yet they are necessary, and increasingly paid attention to by funders, journals, and conferences.
Higher reproducibility standards will advance SciML's trustworthiness, and benefit authors by reduced barriers to the adoption of their proposed methods. 




\subsection{Cost-accuracy comparison with alternatives}

\begin{essrec}[Compare developed SciML model against alternatives]
Compare the SciML model against both alternative SciML approaches and traditional CSE methods, using validation metrics specific to the model's purpose. Document all computational costs including data generation, parameter optimization, training, and inference times. Clearly report model limitations and negative results alongside successes, with quantitative evidence for all comparisons. This enables informed decisions about model selection while avoiding reporting bias and establishing trust in the results through balanced presentation of both strengths and weaknesses.
\end{essrec}

SciML models must be compared with state-of-the-art alternatives throughout model development, including verification, validation, and prediction stages. However, a review of environmental SciML studies found that 37.8\% made no ML comparisons, 44.6\% compared with one SciML method, and only 14.9\% compared with multiple methods~\cite{Zhu_YR_EST_2023}.

Model comparison credibility depends on the types of SciML models compared. When a parsimonious model outperforms more complex ones~\cite{Tiantian_etal_WR_2017}, comparing it only within its model family is insufficient. Comparing a neural network with Gaussian processes and linear regression better contextualizes performance than comparing multiple neural networks. While comparing multiple ML methods is valuable, when CSE alternatives exist, SciML methods must also be evaluated against state-of-the-art CSE approaches using purpose-specific metrics, particularly those used in validation.

Although accuracy is crucial in method comparison, computational costs must also be documented, yet total modeling costs are often underreported. Researchers should detail all costs, including data generation (e.g., running a \CSE{} model), parameter optimization, and prediction.
They should also compare the total of these costs against state-of-the-art alternatives. For instance, in PDE-constrained optimization, direct PDE optimization costs should be compared to combined surrogate training and optimization costs. 

Dismissing training costs as ``offline" is misleading, however, these costs can be amortized when SciML models are used across multiple studies, as in learning to optimize~\cite{chen2022learning}. Nevertheless, such amortization claims need qualification since SciML models' cost-effectiveness---the trade-off between accuracy, development time, and cost---is harder to predict than for \CSE{} models~\cite{Sculley_etal_Neurips_2015}.


Finally, every effort must be made to avoid ``the misuse of language, intentionally or unintentionally, as this affects the interpretation of study findings and has been described as ``spin''~\cite{Constanza_et_al_JCE_2023} or reporting bias. While this may appear obvious, a recent review of 152 studies of clinical prediction using ML models found that 95\% of abstracts that recommended the use of an ML in daily practice lacked any external validation of the developed models. Moreover, only 55.6\% of studies recommended the clinical use of an ML model in their main text without any external validation. McGreivy and Hakim~\cite{Mcgreivy_H_arxiv_2024} found that 79\% of 76 ML papers solving fluid PDEs used weak baselines when claiming to outperform standard methods. The same study also found that outcome reporting bias and publication bias are widespread. 
Of 232 abstracts surveyed, 94.8\% reported only positive results, 5.2\% reported mixed results, and none reported only negative results, suggesting reporting bias.


\subsection{Explain and Interpret}

\begin{essrec}[Explain the SciML prediction mechanism]
Document how the model arrives at its predictions by explaining both its structural features and decision-making processes. When claiming improved interpretability, specify the aspects that are enhanced (such as parsimony, robustness, or alignment with domain knowledge) and provide quantitative evidence supporting these claims. Explaining a model's reasoning process helps build trust in its predictions and enables domain experts to validate its behavior against scientific understanding.
\end{essrec}

SciML and \CSE{} researchers should explicitly identify how their methods advance specific scientific objectives. Sufficient information should be provided ``to infer the purpose of the technology and set their expectations sensibly based on such understanding''~\cite{Toreini_ACEZM_ACM_2020}.
Model trustworthiness depends both on the process and its results.
Consequently, researchers should explain how they came to their conclusions.
According to Doshi-Velez and Kim~\cite{Doshi_K_arxiv_2017}, this can involve experts interpreting highly intelligible models, where interpretability is built into their model structure. 
For example, a physicist might understand the reasons for the bifurcation of a dynamical system or use a second model to understand the learned model's behavior. 
Doshi-Velez and Kim~\cite{Doshi_K_arxiv_2017} also advocate for evaluating algorithm robustness to parameter and input variations and for validating that predicted changes in model outputs correspond to real system behavior.

The limited interpretability and transparency of ML models are significant challenges, especially in critical scientific applications such as medicine and aerospace engineering.
The inner workings of SciML models are hard for humans to understand, possibly deterring us from trusting their predictions. This is especially problematic in settings where model-assisted decisions can significantly impact people's lives.
To address these problems, researchers must continue to develop new techniques for making SciML models more interpretable and transparent.

While many papers compare the interpretability of different methods, no universal definition of interpretability exists because interpretability is subjective and varies across scientific disciplines~\cite{Lipton_arxiv_2017}. 
Linear models are often considered interpretable because coefficients show each feature's importance in predictions. 
However, this interpretability breaks down with thousands of parameters~\cite{Molnar_CB_chapter_2020}.
Claims of superior interpretability must be thoroughly justified. This creates an opportunity for research communities to define domain-specific properties that make models interpretable, complemented by explainability methods that reveal how models make their predictions.


