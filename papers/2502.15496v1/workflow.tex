\section{Problem definition}
\label{sec:problem-def}

The first step in building and using a \CSE{} or SciML model is defining the problem scope: the model's intended purpose, application domain and operating environment, required quantities of interest (QoI) and their scales, and how prior knowledge informs model conceptualization.

\subsection{Model purpose}

\begin{essrec}[Specify prior knowledge and model purpose]
Define the model's intended use and document the essential model properties that must be satisfied. Document any known limitations and constraints of the chosen approach. This ensures appropriate data selection and physics-informed objectives while preventing model misuse outside its intended scope.
\end{essrec}

A SciML model's purpose, as discussed in Section~\ref{sec:scope}, dictates all subsequent modeling choices.
This purpose determines required outer-loop processes and essential properties.

To highlight the importance of specifying the target outer-loop process, consider a model used for explanatory modeling. An explanatory model must simulate all system processes, like ice-sheet thickness and velocity evolution. In contrast, a risk assessment model needs only decision-relevant quantities, like ice-sheet mass loss under varying emissions scenarios. Design and control models, meanwhile, have different requirements than those for risk assessment.
The model purpose dictates the data types and formulations needed to train a SciML model. The impact of this purpose on data requirements and physics-informed objectives varies by application domain. Thus, the exact model formulation should be chosen in light of these problem-specific considerations.


\subsection{Verification, calibration, validation and application domain}

\begin{essrec}[Specify verification, calibration, validation, and application domains]
Define the specific conditions under which the model will operate across the verification, calibration, validation, and prediction phases. These domains are specified by relevant boundary conditions, forcing functions, geometry, and timescales. Account for potential differences between these domains and address any data distribution shifts that could affect model performance. This ensures the selected model architecture and training data align with the intended use while preventing unreliable predictions when operating outside validated conditions.
\end{essrec}

The trustworthy development and deployment of a model (see Figure~\ref{fig:model-development}) requires using the model in regards to verification, calibration, validation, and application domains. These domains are defined by the conditions under which the model operates during these respective phases and must be defined before model construction because they determine viable model classes. Key features include boundary conditions, forcing functions, geometry, and timescales. For ice sheets, examples include surface mass balance, land mass topography, and ocean temperatures.

Each domain will often require the prediction of different quantities of interest under different conditions. Moreover the complexity of the processes being modeled typically increase when transitioning from verification to calibration, to validation to prediction. Additionally the amount of data to complement or inform the model decreases as we move through these domains. For example, the verification domain for our ice-sheet examplar predicts the entire state of the ice-sheet for simple manufactured or analytical solutions. The calibration domain predicts Humboldt Glacier surface velocity under steady-state preindustrial conditions. The validation domain predicts grounding-line change rates from the first decade of this century. The application domain predicts glacier mass change in 2100. Figure~\ref{fig:computational-domains} illustrates these distinct domains. When transitioning between domains, data shifts across domains must be considered. For example, a model trained only on calibration data from recent ice-sheet forcings may fail to predict ice-sheet properties under different future conditions.

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.65\linewidth]{application-domain.pdf}
    \caption{Verification, validation, calibration and application domains.}
    \label{fig:computational-domains}
\end{figure}


\subsection{Quantities of interest}

\begin{essrec}[Carefully select and specify the quantities of interest]
Select and specify the model outputs (quantities of interest, QoI) required for the intended use, considering their form and scale. For risk assessment and design applications, identify the minimal set of QoIs needed for decision-making or optimization. For explanatory modeling, specify the broader range of QoIs needed to capture system behavior. This choice fundamentally determines the required model complexity, training data requirements, and computational approach needed to achieve reliable predictions.
\end{essrec}

Quantities of interest (QoI) are the model outputs required by users. Their form and scale depend on modeling purpose and application domain. We now discuss key considerations for QoI selection.

Risk assessment requires reproducing only decision-critical QoI. For ice-sheets, these include sea-level rise from mass loss and infrastructure damage costs. Design applications similarly need few QoI to evaluate objectives and constraints, like thermal and structural stresses in aerospace vehicles. Design models need accurate QoI predictions only along optimizer trajectories\footnote{For each design iteration the model may still need to be accurate across all uncertain model inputs}, while risk assessment models must predict across many conditions. Explanatory modeling demands more extensive QoI sets, such as complete ice-sheet depth and velocity fields for studying calving. Therefore, simple surrogates often suffice for risk assessment and design, but explanatory modeling may require operators or reduced order models.


\subsection{Model conceptualization}


\begin{essrec}[Select and document model structure]
Select a model structure that fits the model's purpose, domain, and quantities of interest based on relevant prior knowledge such as conservation laws or system properties. Document the alternative model structures considered and the reasoning behind the final selection, including how available resources and computational constraints influenced the choice. This systematic approach ensures the model balances usability, reliability, and feasibility while maintaining transparency about structural assumptions and limitations.
\end{essrec}

Model conceptualization, which follows problem definition, involves selecting model structure based on prior knowledge. While essential to \CSE{} model development~\cite{Jakeman_LN_EMS_2006}, this step requires clear identification of the application domain and relevant QoI.

Model structure selection draws on key prior knowledge: conservation laws, system invariances like rotational and translational symmetries. These guide method selection---for example, symplectic time integrators~\cite{ruth1983canonical} preserve system dynamics properties. Moreover, this knowledge informs and justifies the selection of candidate model structures.
A \CSE{} modeler chooses between model types like lumped versus distributed PDE models, and linear versus nonlinear PDEs. The optimal choice depends on application domain, QoI, and available resources. For example, linear PDEs may introduce more error but their lower computational cost enables better error and uncertainty characterization for tasks like optimal design.
Similar considerations guide SciML model selection. For example, Gaussian processes excel at predicting scalar QoI with few inputs and limited data, but become intractable for larger datasets without variational inference approximations~\cite{Liu_CO_KBS_2018}. In contrast, deep neural networks handle high-dimensional data but require large datasets. The intended use also shapes model structure and training, e.g., optimization applications require controlling derivative errors~\cite{bouhlel2020scalable,o2024derivative} to ensure convergence~\cite{cao2024lazy,luo2023efficient}. These approximation errors must be understood and quantified where possible.

\CSE{} has a strong history of using prior knowledge to formulate governing equations for complex phenomena and deriving numerical methods that respect important physical properties. However, all models are approximate and the best model must balance usability, reliability, and feasibility~\cite{Hamilton_PSFJEMS_2022}. While SciML methods can be usable and feasible, more attention is needed to establish their trustworthiness. In the following two sections, we discuss how \CSE{} V\&V can improve the trustworthiness of SciML research.

\section{Verification}
\label{sec:verification}

Verification increases the trustworthiness of numerical models by demonstrating that the numerical method can adequately solve the equations of the desired mathematical model and the code correctly implements the algorithm. Verification consists of code verification and solution verification, which enhance credibility and trust in the model's predictions. Code and solution verification are well-established in \CSE{} to reduce algorithmic errors. However, verification for SciML models has received less attention due to the field's young age and unique challenges. Moreoever, because SciML models heavily rely on data, unlike \CSE{} models, existing \CSE{} verification notions need to be adapted for SciML.

\subsection{Code verification}
\label{sec:code-verification}

\begin{essrec}[Verify code implementation with test problems]
Evaluate the SciML model's accuracy on simple manufactured test problems using verification data that is independent from training data. Assess how the model error responds to variations in training data samples and optimization parameters while increasing both model complexity and training data size. This systematic testing approach reveals implementation issues, quantifies the impact of sampling and optimization choices, and establishes confidence in the model's numerical implementation.
\end{essrec}

Code verification ensures that a computer code correctly implements the intended mathematical model. For \CSE{} models, this involves confirming that numerical methods and algorithms are free from programming errors (``bugs"). PDE-based \CSE{} models commonly use the method of manufactured solutions (MMS) to verify code on arbitrarily complex solutions. MMS substitutes a user-provided solution into the governing equations, then differentiates it to obtain the exact forcing function and boundary conditions. These solutions check if the code produces the known theoretical convergence rate as the numerical discretization is refined. If the observed order of convergence is less than theoretical, causes such as software bugs, insufficient mesh refinement, or singularities and discontinuities affecting convergence must be identified.

Code verification for SciML models is important but challenging due to the large role of data and nonconvex numerical optimization. Three main challenges limit code verification for many SciML models.
First, while theoretical analysis of SciML models is increasing~\cite{schwab2019deep,schwab2023deep,opschoor2022exponential,leshno1993multilayer,lanthaler2023curse,kovachki2021universal,kovachki2023neural}, many models like neural networks do not generally admit known convergence rates outside specific map classes~\cite{schwab2019deep,schwab2023deep,opschoor2022exponential,herrmann2024neural}, despite their universal approximation properties~\cite{hornik1989multilayer,cybenko1989approximation,leshno1993multilayer}.
Second, generalizable procedures to refine models, such as consistently refining neural-network width and depth as data increases, do not exist.
Finally, regardless of data amount and model unknowns, modeling error often plateaus at a much higher level than machine precision due to nonconvex optimization issues like local minima and saddle points~\cite{Dauphin_PGCGB_NIPS_2014,Bottouleon_CN_SIAMR_2018}.

Developing theoretical and algorithmic advances to address the three main challenges limiting code verification can substantially improve the trustworthiness of SciML models. Convergence-based code verification is currently possible only for certain SciML models with theory that bounds approximation errors in terms of model complexity and training data amount, such as operator methods~\cite{Turnage_et_al_arxiv_2024}, polynomial chaos expansions~\cite{Cohen_M_SMAIJCM_2017,xiu2002wiener}, and Gaussian processes~\cite{Burt_RV_PMLR_2019}.

For SciML models without supporting theory, convergence tests should still be conducted and reported. Studies providing evidence of model convergence engender greater trustworthiness than those that do not, even when the empirically estimated convergence rate cannot be compared to theoretical rates. For example, observing Monte Carlo-type sampling rates in a regime of interest for a fixed overparametrized model can provide intuition into whether the model should be enhanced.

To account for the heavy reliance of SciML models on training data optimization, code verification should be adapted in two ways.
First, report errors in the ML model for a given complexity and data amount for different realizations of the training data to quantify the impact of sampling error, which is not present in \CSE{} models.
Second, because most SciML algorithms introduce optimization error, conduct verification studies that artificially generate data from a random realization of an ML model, then compare the recovered parameter values with the true parameter values or compare the predictions of the learned and true approximations, or at the very least compare the predictions of the two models. Additionally, quantify the sensitivity of the SciML model error to randomness in the optimization by varying the random seed and initial guess passed to the optimizer (see Section~\ref{sec:loss-and-opt}).
All verification tests must employ test data or \emph{verification data}, independent of the training data, to measure the accuracy of the SciML model.


\subsection{Solution verification}

\begin{essrec}[Verify solution accuracy with realistic benchmarks]
Test the model's performance on well-designed, realistic benchmark problems that reflect the intended application domain. Quantify how the model error varies with different training data samples and optimization parameters. When feasible, examine error patterns across different model complexities and data amounts; otherwise, focus on verifying the specific configuration intended for deployment. This ensures the model meets accuracy requirements under realistic conditions while accounting for uncertainties in training and optimization.
\end{essrec}

Code verification establishes a code's ability to reproduce known idealized solutions, while solution verification, performed after code verification, assesses the code's accuracy on more complex yet tractable problems defined by more realistic boundary conditions, forcing, and data. For example, code verification of ice sheets may use manufactured solutions, whereas solution verification may use more realistic MISMIP benchmarks~\cite{Cornford_et_al_TC_2020}. In solution verification, the numerical solution cannot be compared to a known exact solution, and the convergence rate to a known solution cannot be established. Instead, solution verification must use other procedures to estimate the error introduced by the numerical discretization.

Solution verification establishes whether the exact conditions of a model result in the expected theoretical convergence rate or if unexpected features like shocks or singularities prevent it. The most common approach for \CSE{} models compares the difference between consecutive solutions as the numerical discretization is refined and uses Richardson extrapolation to estimate errors. A posteriori error estimation techniques that require solving an adjoint equation can also be used.

While thorough solution verification of CSE models is challenging, these difficulties are further amplified for SciML models. Currently, solution verification of SciML models simply consists of evaluating a trained model's performance using test data separate from the training data. However, this is insufficient as solution verification requires quantifying the impact of increasing data and model complexity on model error. Yet, unfortunately, performing a posteriori error estimation for many SciML models using techniques like Richardson extrapolation is difficult due to the confounding of model expressivity, statistical sampling errors, and variability introduced by converging to local solutions or saddle points of nonconvex optimization, making it challenging to monotonically decrease the error of SciML models such as neural networks. 

Until convergence theory for SciML models improves and automated procedures are developed to change SciML model hyperparameters as data increases, solution verification of SciML models should repeat the sensitivity tests proposed for code verification (Section~\ref{sec:code-verification}) with two key differences:
First, verification experiments used to generate verification data must be specifically designed for solution verification, as not all verification data equally informs solution verification efforts, similar to observations made when creating validation datasets for \CSE{} models~\cite{Oberkampf_T_PAS_2002}. See Section~\ref{sec:data-sources} for more information on important properties of verification benchmarks.
Second, while ideally the convergence of SciML errors on realistic benchmarks should be investigated, it may be computationally impractical. Thus, solution verification should prioritize quantifying errors using the model complexity and data amount that will be used when deploying the SciML model to its application domain.

\section{Validation}
\label{sec:validation}

Verification establishes if a model can accurately produce the behavior of a system described by governing equations. In contrast, validation assesses whether a \CSE{} model's governing equations---or data for SciML models---and the model's implementation can reproduce the physical system's important properties, as determined by the model's purpose.

Validation requires three main steps: (1) solve an inverse problem to calibrate the model to observational data; (2) compare the model's output with observational data collected explicitly for validation; and (3) quantify the uncertainty in model predictions when interpolating or extrapolating from the validation domain to the application domain. We will expand on these steps below.
But first note that the issues affecting the verification of SciML models also affect calibration and validation. Consequently, we will not revisit them here but rather will highlight the unique challenges in validating SciML models.

\subsection{Calibration}

\begin{essrec}[Perform probabilistic calibration]
Calibrate the trained SciML model using observational data to optimize its predictive accuracy for the application domain. Implement Bayesian inference when possible to generate probabilistic parameter estimates and quantify model uncertainty. Choose calibration metrics that account for both model and experimental uncertainties, and select calibration data strategically to maximize information content within experimental constraints. This approach enables reliable uncertainty estimation and optimal use of available observational data.
\end{essrec}

Once a \CSE{} model has been verified, it must be calibrated to match experimental data that contains observational noise. This calibration requires solving an inverse problem~\cite{Stuart_AN_2010}, which can be either deterministic or statistical (e.g., Bayesian). The deterministic approach formulates the inverse problem as a (nonlinear) optimization problem that minimizes the mismatch between model and experimental data. This formulation requires regularization to ensure well-posedness, typically chosen using the L-curve~\cite{hansen1999curve} or the Morozov discrepancy principle~\cite{anzengruber2009morozov}. The Bayesian approach replaces the misfit with a likelihood function based on the noise model, while using a prior distribution for regularization. This prior distribution ensures well-posedness while encoding typical parameter ranges and correlation lengths. We recommend Bayesian methods for calibration because they provide insight into the uncertainty of the reconstructed model parameters. 

The calibration of SciML operator, reduced-order, and hybrid CSE-SciML models is distinct from SciML training and follows similar principles to \CSE{} model calibration. These models are first trained using simulation data for solution verification. Next, observational data (called \emph{calibration data}) determines the optimal model input values that match experimental outputs. For instance, calibrating a SciML ice-sheet model such as that of Ref.~\cite{He_PKS_JCP_2023} requires finding optimal friction field parameters of a trained SciML model, which best predict observed glacier surface velocities, given the noise in the observational data.

Calibration typically improves a model's predictive accuracy on its application domain, but the informative value of calibration data varies significantly. Therefore, researchers should select calibration data strategically to maximize information content within their experimental budget. See Section~\ref{sec:data-sources} for further discussion on collecting informative data.

\subsection{Model validation}

\begin{essrec}[Validate model against purpose-specific requirements]
Define validation metrics that align with the model's intended purpose. Then validate the model using independent data that was not used for training or calibration, ensuring it captures essential physics and boundary conditions of interest. If validation reveals inadequate performance, iterate by collecting additional training data, refining the model structure, or gathering more calibration data until the model achieves satisfactory accuracy for its intended application. This systematic approach will help ensure the model meets stakeholder requirements while maintaining scientific rigor.
\end{essrec}

Model validation is the ``substantiation that a model within its domain of applicability possesses a satisfactory range of accuracy consistent with the intended application of the model''~\cite{Refsgaard_H_AWR_2004}. Validation involves comparing computational results with observational data, then determining if the agreement meets the model's intended purpose~\cite{Lee_et_all_AIAA_2016}. For \CSE{} models with unacceptable validation agreement, modelers must either collect additional calibration data or refine the model structure until reaching acceptable accuracy. SciML models follow a similar iterative process but offer an additional option: to collect more training data.

Model validation must occur after calibration and requires independent data not used for calibration or training. For our conceptual ice-sheet model, calibration matches surface velocities assumed to represent pre-industrial conditions, while validation assesses the calibrated model's ability to predict grounding line change rates at the start of this decade. Performance metrics must target the specific modeling purpose. For optimization tasks, metrics should measure the distance from true optima obtained via the SciML model or bound the associated error~\cite{cao2024lazy}. For uncertainty estimation, metrics should quantify errors in uncertainty statistics through moment discrepancies or density-based measures like (shifted) reverse and forward Kullback--Leibler divergences.
For explanatory SciML modeling, validation metrics must also assess physical fidelity: adherence to physical laws, conservation properties (such as mass and energy), and other constraints. As with verification, the validation concept should encompass \emph{data validation}, particularly whether training data adequately represents the application space.

Validation determines whether a model is acceptable for its specific purpose rather than universally correct. The definition of acceptable is subjective, depending on validation metrics and accuracy requirements established by model stakeholders in alignment with the problem definition and model purpose (see Section~\ref{sec:problem-def}). Moreoever, validation itself does not constitute final model acceptance, which must be based on model accuracy in the application domain, as discussed in Section~\ref{sec:prediction}.

Two additional considerations complete our discussion of model validation. First, this validation differs from the concept of \emph{cross validation}, which estimates ML model accuracy on data representative of the training domain during development. The validation described here assesses accuracy in a separate validation domain. Second, validation data varies in informative value. Validation experiments should ``capture the essential physics of interest, including all relevant physical modeling data and initial and boundary conditions required by the code''~\cite{Oberkampf_T_NED_2008}. Most critically, validation data must remain independent from training and calibration data. 

\subsection{Prediction}
\label{sec:prediction}

\begin{essrec}[Quantify prediction uncertainties]
Assess and quantify all sources of uncertainty affecting model predictions in the application domain, including numerical approximation errors, input and parameter uncertainties, sampling errors from finite training data, and optimization errors. Propagate these uncertainties through the model using appropriate techniques to estimate relevant statistics that match validation criteria. Define acceptance thresholds for prediction uncertainty to ensure the model's reliability for its intended use while acknowledging inherent limitations in uncertainty quantification.
\end{essrec}

Although extensive data may be available for model calibration, validation data is typically scarcer and may not represent the model's intended application domain. According to Schwer~\cite{Schwer_EWC_2007}, ``The original reason for developing a model was to make predictions for applications of the model where no experimental data could, or would, be obtained.'' Therefore, minimizing validation metrics at nominal conditions cannot sufficiently validate a model. Modelers must also quantify accuracy and uncertainty when predictions are extrapolated to the application domain.

SciML models, like \CSE{} models, are subject to numerous sources of uncertainty. The impact of these uncertainties on model predictions must be quantified. Several sources of uncertainty affect \CSE{} models. These include: numerical errors, from approximating the solution to governing equations; input uncertainty arises, which is caused by inexact knowledge of model inputs; parameter uncertainty, which stems from inexact knowledge of model coefficients; and model structure error representing the difference between the model and reality. SciML models contain all these uncertainties. They also incorporate additional uncertainties from sampling and optimization errors, as discussed previously.

Sampling error arises from training a model with a finite amount of possibly noisy data. For a fixed ML model structure and zero optimization error, this error decreases as the amount of data increases. Optimization error represents the difference between the optimized solution, which is often a local optimum, and the global solution for fixed training data. Optimization error can enter \CSE{} models during calibration. Optimization error affects SciML models more significantly because it occurs both during calibration and training. Linear approximations, for example, based on polynomials, achieve zero optimization error during training to machine precision. However, nonlinear approximations such as neural networks often produce non-trivial optimization errors. Stochastic gradient descent demonstrates this by producing different parameter estimates due to stochastic optimization randomness and initial guesses.

The identified sources of modeling uncertainty require parameterization for sampling. Expert knowledge typically guides the construction of prior distributions that represent parametric uncertainty. This parameterization should occur during problem definition. Bayesian calibration updates these priors into posterior distributions using calibration data. The model must then propagate all uncertainties onto predictions in the application domain. Monte Carlo quadrature accomplishes this propagation by drawing random samples from the uncertainty distributions. The method collects model predictions at these samples and computes empirical estimates of important statistics defined by validation criteria, such as mean and variance.

We emphasize the impact of all sources of error and uncertainty must be quantified. Simply estimating the impact of error caused by using finite sample sets, for example estimated by generative models such as variational autoencoders of Gaussian processes is insufficient. Moreover, complete elimination of uncertainty is impossible. Consequently, model acceptance, like validation, must rely on subjective accuracy criteria established through stakeholder communication. For instance, acceptance criteria for predicted sea-level change from melting ice-sheets at year 2100 may specify that prediction precision reaches 1\% of the mean value. Yet, engineering applications, such as those focused on aerospace design, may have much higher accuracy requirements.

The aforementioned Monte-Carlo based UQ procedure effectively quantifies the impact of parameterized uncertainties on model predictions. However, model structure error remains difficult to parameterize in both SciML and \CSE{} modeling. Validation can partially assess model structure error. However, experiments rarely cover all conditions of use. Specifically, validation tests only the model's interpolation ability within the convex hull of available data and assumptions. This limitation creates challenges when applying the model outside its validation domain. Some progress exists in quantifying extrapolation error for ``models based upon highly-reliable theory that is augmented with less-reliable embedded models''~\cite{Oliver_TSM_CMAME_2015}. However, such hybrid CSE-SciML models rely on well-established physics-based governing equations to support extrapolation confidence. Pure SciML models still require substantial research to develop reliable methods for estimating model structure uncertainty.

