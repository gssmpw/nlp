\section{Related Work}
\subsection{Multi-speaker Lip-to-Speech Synthesis}
Compared to the prior datasets focusing on constrained experimental settings, lip-to-speech research based on multi-speaker datasets has shown enhanced scalability and practicality for real-world applications. Scalable video-to-speech synthesis (SVTS) \cite{schoburgcarrillodemira2022} is the first attempt to produce intelligible speech from silent videos on an unconstrained multi-speaker dataset. This method incorporates a scalable mel-spectrogram generator and a pre-trained neural vocoder. The generator includes a 3D Convolution and ResNet-18\cite{he2016deep} encoder followed by a conformer\cite{gulati2020conformer} decoder. The width and depth of the Conformer blocks are adjusted based on the scale of datasets in the experiments. At the inference phase, SVTS exploits the pre-trained Parallel WaveGAN\cite{yamamoto2020parallel} as the vocoder to transform the generated mel-spectrogram into the waveform. Lip-to-speech Multi-Task\cite{kim2023-liptospeech} further improves this framework by introducing the multi-task learning strategy to simultaneously supervise the text and audio modality. Specifically, unlike previous works, which solely use the reconstruction loss to supervise acoustic features, both the feature level and output-level content supervision are also added to guide the model toward generating more intelligible speech. However, it employs the Griffin-Lim algorithm as the vocoder, so the quality of synthesized speech was limited. Hedge et al. \cite{hegde2022} leveraged the probabilistic nature of a variational autoencoder (VAE) to mitigate the one-to-many mapping challenge inherent in lip-to-speech synthesis. By explicitly modeling speech content uncertainty through latent space variational inference, their VAE-GAN framework captures the inherent variability between lip movements and corresponding speech, avoiding deterministic overfitting in one-to-many mapping problems.

Several works that adopt pre-trained models to obtain text from silent videos have shown considerable performance superiority. Hedge et al. \cite{hegde2023} employed a pre-trained lip-reading model presented in \cite{prajwal2022} to acquire text predictions and visual representations of each video frame. Additionally, the scaled dot-product attention module\cite{vaswani2017attention} is exploited to learn the correspondence between the text phoneme features and visual representations, yielding the content representations. All representations are finally passed through a cascade pipeline consisting of a spectrogram decoder and a pre-trained BigVGAN\cite{lee2023} to generate the final speech waveforms. LipVoicer\cite{yemini2023lipvoicer} also utilizes a pre-trained lip-reading model\cite{ma2023-autoavsr} to infer the text from the silent videos, which helps distinguish the ambiguous syllables visually. The predicted text is used to provide classifier guidance\cite{ho2022classifier} to improve the performance of a diffusion-based mel-spectrogram generator\cite{kong2020diffwave}. As a consequence of the utilization of a powerful lip-reading model, LipVoicer exhibits exceptionally high intelligibility in generated speech samples. However, the performance of these approaches heavily relies on the efficacy of the pre-trained lip-reading model, and methods without text modality dependence are more practical in real-world environments, in which not all videos have corresponding text transcripts. 

The emergence of large-scale audio-visual self-supervised models such as AV-HuBERT \cite{shi2022learning} provides a text-free paradigm for lip-to-speech synthesis through cross-modal representation learning. Pre-trained on massive audio-visual corpora (LRS3 and VoxCeleb2 \cite{chung2018voxceleb2}), AV-HuBERT employs a masked prediction objective where the model reconstructs discrete cluster labels for masked audio or visual segments. An iterative offline k-means process refines these cluster assignments during training. This multi-modal pretraining strategy enables AV-HuBERT to learn transferable representations, which generalize effectively across diverse downstream tasks (e.g., lip-reading, lip-to-speech synthesis). ReVISE \cite{hsu2023} demonstrates this capability by employing HuBERT \cite{hsu2021hubert} to generate speech units that connect two subsystems: 1) a pseudo audio-visual speech recognizer (P-AVSR) initialized with AV-HuBERT, and 2) a unit-driven HiFi-GAN synthesizer (P-TTS). The framework trains both components using targets from a fine-tuned HuBERT-BASE, achieving superior intelligibility over non-AV-HuBERT baselines. In \cite{choi2023-intelligible}, researchers utilized the pre-trained AV-HuBERT to generate both mel-spectrogram and speech units as the input of HiFi-GAN. However, they observed that the intermediate mel-spectrograms suffered from blurriness and noise artifacts compared to the ground truth. This can be attributed to the inherent limitations of input information from silent video. To mitigate the mismatch between the generated mel-spectrograms and real mel-spectrograms, the authors proposed an augmentation technique to simulate the blurry and noisy generated mel-spectrogram during multi-input vocoder training.

Focusing on the difficulty of getting speaker timbre, most of the above-mentioned methods provide a prior speaker embedding to assist the model during both training and inference stages since timbre information is difficult to obtain from videos accurately. However, relying on speaker embedding limits the application scope for real-world scenarios. To solve this problem, DiffV2S\cite{choi2023} addresses this by introducing a vision-guided speaker embedding extractor to predict speaker characteristics. The speaker characteristics are regarded as the conditions of the diffusion model\cite{sohl2015deep} to generate a mel-spectrogram, which is then converted to a waveform using HiFi-GAN. 

\subsection{Differentiable Digital Signal Processing}
Differentiable digital signal processing (DDSP) \cite{engel2019} combines traditional digital signal processing with deep learning to employ the inherent acoustic knowledge in classic signal processing components like filters or oscillators. This approach enables interpretable high-fidelity audio generation. Originally developed for interpretable high-fidelity audio generation, the DDSP model comprises an encoder that maps the input log mel-spectrogram to a latent representation and a decoder that predicts parameters for additive and filtered noise synthesizers.

The modularity and interpretability of DDSP make it highly effective for various audio synthesis tasks, including singing voice synthesis \cite{zhang2022}, artificial reverberation\cite{lee2022differentiable}, and vocoders\cite{li2023}. Our work is inspired by \cite{zhang2022}, in which researchers designed a conditional HiFi-GAN framework to enhance an end-to-end variational autoencoder singing voice synthesis system. This framework integrates a DDSP synthesizer to produce periodic and aperiodic signals from the latent representation, which are then used as an auxiliary input for the multi-receptive field fusion (MRF) module in HiFi-GAN.

By incorporating a DDSP synthesizer, NaturalL2S leverages these advantages to not only achieve high-quality speech but also surpass many state-of-the-art lip-to-speech models in both fidelity and naturalness.