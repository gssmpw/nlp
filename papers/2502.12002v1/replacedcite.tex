\section{Related Work}
\subsection{Multi-speaker Lip-to-Speech Synthesis}
Compared to the prior datasets focusing on constrained experimental settings, lip-to-speech research based on multi-speaker datasets has shown enhanced scalability and practicality for real-world applications. Scalable video-to-speech synthesis (SVTS) ____ is the first attempt to produce intelligible speech from silent videos on an unconstrained multi-speaker dataset. This method incorporates a scalable mel-spectrogram generator and a pre-trained neural vocoder. The generator includes a 3D Convolution and ResNet-18____ encoder followed by a conformer____ decoder. The width and depth of the Conformer blocks are adjusted based on the scale of datasets in the experiments. At the inference phase, SVTS exploits the pre-trained Parallel WaveGAN____ as the vocoder to transform the generated mel-spectrogram into the waveform. Lip-to-speech Multi-Task____ further improves this framework by introducing the multi-task learning strategy to simultaneously supervise the text and audio modality. Specifically, unlike previous works, which solely use the reconstruction loss to supervise acoustic features, both the feature level and output-level content supervision are also added to guide the model toward generating more intelligible speech. However, it employs the Griffin-Lim algorithm as the vocoder, so the quality of synthesized speech was limited. Hedge et al. ____ leveraged the probabilistic nature of a variational autoencoder (VAE) to mitigate the one-to-many mapping challenge inherent in lip-to-speech synthesis. By explicitly modeling speech content uncertainty through latent space variational inference, their VAE-GAN framework captures the inherent variability between lip movements and corresponding speech, avoiding deterministic overfitting in one-to-many mapping problems.

Several works that adopt pre-trained models to obtain text from silent videos have shown considerable performance superiority. Hedge et al. ____ employed a pre-trained lip-reading model presented in ____ to acquire text predictions and visual representations of each video frame. Additionally, the scaled dot-product attention module____ is exploited to learn the correspondence between the text phoneme features and visual representations, yielding the content representations. All representations are finally passed through a cascade pipeline consisting of a spectrogram decoder and a pre-trained BigVGAN____ to generate the final speech waveforms. LipVoicer____ also utilizes a pre-trained lip-reading model____ to infer the text from the silent videos, which helps distinguish the ambiguous syllables visually. The predicted text is used to provide classifier guidance____ to improve the performance of a diffusion-based mel-spectrogram generator____. As a consequence of the utilization of a powerful lip-reading model, LipVoicer exhibits exceptionally high intelligibility in generated speech samples. However, the performance of these approaches heavily relies on the efficacy of the pre-trained lip-reading model, and methods without text modality dependence are more practical in real-world environments, in which not all videos have corresponding text transcripts. 

The emergence of large-scale audio-visual self-supervised models such as AV-HuBERT ____ provides a text-free paradigm for lip-to-speech synthesis through cross-modal representation learning. Pre-trained on massive audio-visual corpora (LRS3 and VoxCeleb2 ____), AV-HuBERT employs a masked prediction objective where the model reconstructs discrete cluster labels for masked audio or visual segments. An iterative offline k-means process refines these cluster assignments during training. This multi-modal pretraining strategy enables AV-HuBERT to learn transferable representations, which generalize effectively across diverse downstream tasks (e.g., lip-reading, lip-to-speech synthesis). ReVISE ____ demonstrates this capability by employing HuBERT ____ to generate speech units that connect two subsystems: 1) a pseudo audio-visual speech recognizer (P-AVSR) initialized with AV-HuBERT, and 2) a unit-driven HiFi-GAN synthesizer (P-TTS). The framework trains both components using targets from a fine-tuned HuBERT-BASE, achieving superior intelligibility over non-AV-HuBERT baselines. In ____, researchers utilized the pre-trained AV-HuBERT to generate both mel-spectrogram and speech units as the input of HiFi-GAN. However, they observed that the intermediate mel-spectrograms suffered from blurriness and noise artifacts compared to the ground truth. This can be attributed to the inherent limitations of input information from silent video. To mitigate the mismatch between the generated mel-spectrograms and real mel-spectrograms, the authors proposed an augmentation technique to simulate the blurry and noisy generated mel-spectrogram during multi-input vocoder training.

Focusing on the difficulty of getting speaker timbre, most of the above-mentioned methods provide a prior speaker embedding to assist the model during both training and inference stages since timbre information is difficult to obtain from videos accurately. However, relying on speaker embedding limits the application scope for real-world scenarios. To solve this problem, DiffV2S____ addresses this by introducing a vision-guided speaker embedding extractor to predict speaker characteristics. The speaker characteristics are regarded as the conditions of the diffusion model____ to generate a mel-spectrogram, which is then converted to a waveform using HiFi-GAN. 

\subsection{Differentiable Digital Signal Processing}
Differentiable digital signal processing (DDSP) ____ combines traditional digital signal processing with deep learning to employ the inherent acoustic knowledge in classic signal processing components like filters or oscillators. This approach enables interpretable high-fidelity audio generation. Originally developed for interpretable high-fidelity audio generation, the DDSP model comprises an encoder that maps the input log mel-spectrogram to a latent representation and a decoder that predicts parameters for additive and filtered noise synthesizers.

The modularity and interpretability of DDSP make it highly effective for various audio synthesis tasks, including singing voice synthesis ____, artificial reverberation____, and vocoders____. Our work is inspired by ____, in which researchers designed a conditional HiFi-GAN framework to enhance an end-to-end variational autoencoder singing voice synthesis system. This framework integrates a DDSP synthesizer to produce periodic and aperiodic signals from the latent representation, which are then used as an auxiliary input for the multi-receptive field fusion (MRF) module in HiFi-GAN.

By incorporating a DDSP synthesizer, NaturalL2S leverages these advantages to not only achieve high-quality speech but also surpass many state-of-the-art lip-to-speech models in both fidelity and naturalness.