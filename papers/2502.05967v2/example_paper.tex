\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
%\usepackage{subfigure}
\usepackage{url}
\usepackage{subcaption}
\usepackage{afterpage}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage[information]{paperinfo}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{enumitem}

% Saaketh: Added these packages and components
\usepackage{multirow}
\usepackage{makecell}
\usepackage{soul}
\def\addcomment #1{\textcolor{blue}{Comment: #1}}
\def\needcite #1{\textcolor{orange}{CITE: #1}}
\def\needfig #1{\textcolor{purple}{fig ref: #1}}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\newcommand{\me}{\mathrm{e}}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% A short form for the running title is supplied here:
\paperinfotitlerunning{µnit Scaling: Simple and Scalable FP8 LLM Training}

\begin{document}

\twocolumn[
\paperinfotitle{µnit Scaling: Simple and Scalable FP8 LLM Training}

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\paperinfosetsymbol{equal}{*}

\begin{paperinfoauthorlist}
\paperinfoauthor{Saaketh Narayan}{databricks}
\paperinfoauthor{Abhay Gupta}{databricks}
\paperinfoauthor{Mansheej Paul}{databricks}
\paperinfoauthor{Davis Blalock}{databricks}
\end{paperinfoauthorlist}

\paperinfoaffiliation{databricks}{Databricks Mosaic Research, San Francisco, CA}

\paperinfocorrespondingauthor{Saaketh Narayan}{saaketh.narayan@databricks.com}
\paperinfocorrespondingauthor{Davis Blalock}{davis.blalock@databricks.com}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\paperinfokeywords{LLM, FP8, Transformer, Model Training, Attention}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution

\begin{abstract}
Large Language Model training with 8-bit floating point (FP8) formats promises significant efficiency improvements, but reduced numerical precision makes training challenging. It is currently possible to train in FP8 only if one is willing to tune various hyperparameters, reduce model scale, or accept the overhead of computing dynamic scale factors.

We demonstrate simple, scalable FP8 training that requires no dynamic scaling factors or special hyperparameters, even at large model sizes. Our method, \textit{µnit Scaling (µS)}, also enables simple hyperparameter transfer across model widths, matched numerics across training and inference, and other desirable properties. µnit Scaling is straightforward to implement, consisting of a set of minimal interventions based on a first-principles analysis of common transformer operations.

We validate our method by training models from 1B to 13B parameters, performing all hidden linear layer computations in FP8. We achieve quality equal to higher precision baselines while also training up to 33\% faster.
\end{abstract}

\section{Introduction}

Because LLM training is computationally expensive, low-precision training provides large compute savings. Modern LLMs are typically trained in mixed-precision bfloat16 (BF16), where most computation occurs in BF16, but some components requiring higher precision (such as accumulators and master weights) use FP32 \cite{mixedPrecision}. Thanks to increased hardware support for FP8 formats, mixed precision training using FP8 computation promises even greater training efficiency \cite{fp8Formats}. However, the reduced range and resolution of FP8 make LLM training challenging. In this work, we demonstrate a simple, scalable FP8 training method with straightforward hyperparameter transfer on large LLMs, called ``µnit Scaling" (µS).

Our µnit Scaling method builds on Unit Scaling \cite{unitScaling}, which aims to maintain unit variance in weights, activations, and gradients. To ensure this, it scales neural network operations with static constants and initializes network parameters to have unit variance. If all tensors used in training can maintain unit variance, they are representable with sufficient range and resolution by low-precision formats like FP16 and FP8. However, preserving high-quality tensor representations in low-precision formats is challenging for large models.

Besides faster training, several other properties are desirable in a low-precision training scheme. Examples include minimizing extra hyperparameters, avoiding dynamic scale factor overhead, and allowing optimal hyperparameters from small models to transfer to large models. As summarized in Fig.~\ref{methods-comparison-table}, µS is the only method that provides these benefits. We elaborate on each of these properties below.

\begin{figure*}[h]
\centering
\includegraphics[width=1.0\textwidth]{figures/LP_methods_table.png}
\caption{\textbf{Comparison of low-precision training methods.} Our proposed method, µnit Scaling (µS, bottom row), enables FP8 training and hyperparameter transfer at scale. Unlike existing methods, it does not use dynamic scaling, requires only a small set of hyperparameters, permits FP8 computation for all hidden layers, and makes the model more easily quantizable for inference.}
\label{methods-comparison-table}
\end{figure*}

\textbf{Straightforward hyperparameter transfer}\quad Tuning hyperparameters for large LLMs is expensive. A promising way to reduce this cost is to tune the hyperparameters for smaller LLMs and ``transfer'' them to large ones, either by using them directly or by applying a model-size-based formula as explored in µ-Parametrization (µP)\cite{tensorProgramsV, spectralConditionFeatureLearning, tensorProgramsVI}. However, applying hyperparameter transfer techniques in practice to low-precision training can be challenging; frequent divergences due to numerical issues may require training in higher precisions like FP32 \cite{tensorProgramsV}. To address this, \citet{uMuP} introduced u-µP, which combines Unit Scaling \cite{unitScaling} and µP to enable hyperparameter transfer in low precision. Unfortunately, compared to conventional BF16 mixed precision training (henceforth termed ``standard parametrized'' (SP) models), both µP and u-µP have many more hyperparameters to sweep over as shown in Table~\ref{hparams_table}, diminishing realized compute savings and increasing complexity. Specific implementation intricacies, such as zero-initialized queries in µP or LR scaling for embeddings by fan-out in u-µP, make these schemes harder to use in practice than SP. In contrast, our µnit Scaling (µS) scheme combines µP and Unit Scaling in a greatly simplified way, making it easier to use and more cost-effective. We demonstrate hyperparameter transfer of learning rate ($\eta$) and weight decay ($\lambda$) to models of up to 20x larger widths.

\textbf{No Dynamic Scaling}\quad With dynamic scaling, one calculates per-tensor scaling factors for each weight, activation, and gradient tensor in training. These scales shift BF16 tensors into the representable ranges of FP8 formats in each forward and backward pass. Typically, one also decouples the forward and backward formats, using e4m3 for weights and activations and e5m2 for gradients \cite{hybridFp8}. NVIDIA's TransformerEngine is a notable example of an FP8 training library that uses dynamic scaling \cite{transformerEngine}. Calculating scaling factors dynamically adds training and inference overhead and complicates large-scale distributed training and checkpointing.

\textbf{Apply to All Linear Layers}\quad Existing work on applying Unit Scaling at larger scales requires certain ``critical matmuls'' (attention out projection, FFN down projection) to stay in BF16 \cite{uMuP}. Assuming a transformer model with conventional multiheaded attention and an MLP with an expansion ratio of 4, this means 41.7\% of all hidden linear layer FLOPs are not in FP8. In contrast, µS ensures that, regardless of scale, \textit{all} hidden layers use FP8.

\textbf{Match Inference-Time Quantization} For efficient inference, LLMs are often quantized to FP8 or INT8 for faster computation and reduced memory footprints \cite{fbgemm, llmInt8}. Since training typically occurs in higher bitwidths (e.g., BF16), a mismatch in precisions at training time and inference time means that some level of quantization error is unavoidable, degrading model quality. Training with µS avoids this mismatch---since the LLM has already been trained in FP8, it is immediately ready for inference in FP8 for both weights and activations (W8A8).

\subsection{Contributions}

Our work makes the following contributions:
\begin{itemize}[noitemsep,nolistsep,topsep=0pt,partopsep=0pt]
    \item Identifying root causes for poor numerics in conventional transformer blocks---for example, explaining diminishing variance in self-attention outputs with increasing sequence position.
    \item Introducing a simple method for fixing these issues that enables FP8 training in all hidden linear layers and with less overhead than existing methods. It also achieves desirable properties such as improved training efficiency and matched numerics at training and inference time.
\end{itemize}

\section{Methods}

In this section, we detail the components of our proposed method, µnit Scaling (µS). The modifications to the standard transformer training scheme that µS requires are summarized in Table~\ref{modifications_table}. We elaborate on novel components such as our handling of self-attention numerics, residual modifications, and hyperparameter transfer below.

\begin{table*}[t]
\caption{\textbf{Components of the µS training scheme.} µS makes the following modifications to standard decoder-only transformer practices.}
\label{modifications_table}
\begin{center}
\begin{tabular}{l|c}
\hline
\textbf{Modification} & \textbf{Description} \\ \hline
Linear layer scaling factors & \makecell{$\frac{1}{\sqrt{\text{fan\_in}}}$ static scaling factor applied in \textit{both} forward and backward pass. The final \\ LM head uses a multiplier of $\frac{1}{\text{fan\_in}}$ instead, in line with µP. \vspace{0.5mm}} \\ \hline
Res-Post-LayerNorm & \makecell{LayerNorm is the last operation in each residual branch instead of the first. \vspace{0.5mm}} \\ \hline
``Fixed'' residual modification & \makecell{Use a fixed constant $\tau$ to make residuals variance-preserving, according to Eq.~\ref{eq:res-mod-fixed}. \vspace{0.5mm}} \\ \hline
Unit variance initialization & \makecell{All linear layer weights initialized with variance 1. \vspace{0.5mm}} \\ \hline
FP8 hidden layers & \makecell{Use FP8E4M3 for weights and activations, FP8E5M2 for gradients. Before casting, \\ clip BF16 values to FP8 dtype max. Keep embedding table and LM head in BF16.  \vspace{0.5mm}} \\ \hline
Learning rate ($\eta$) scaling & \makecell{Optimal $\eta$ stays constant for input and output layers, but is scaled by $\frac{\sqrt{d_\text{base}}}{\sqrt{d_\text{model}}}$ for all \\ hidden layers, when transferring from a base model with width $d_\text{base}$ \vspace{0.5mm}} \\ \hline
Weight decay ($\lambda$) scaling & \makecell{With fully decoupled weight decay, optimal $\lambda$ stays constant for all layers with \\ increasing width. \vspace{0.5mm}} \\ \hline
\end{tabular}
\end{center}
\end{table*}

\subsection{Self-attention Numerics}\label{subsec:self-attn-numerics}

The causal self-attention mechanism at the core of decoder layers in LLMs is not variance-preserving, making low-precision training challenging.

Recall that standard self-attention is defined as:
\begin{equation}
    \text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}\left(\frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{d}}\right)\mathbf{V} \label{eq:standard_attn}
\end{equation}
\begin{proposition}\label{proof:attn-output-variance}
Suppose we have $\mathbf{x} \in \mathbb{R}^{k}$ and $\mathbf{V} \in \mathbb{R}^{k \times m}$. Define $\mathbf{s} \triangleq 
\mathrm{softmax}(\mathbf{x})$, $\mathbf{a} \triangleq \mathbf{s}^T\mathbf{V}$, and $\sigma^2_{\mathbf{a}} \triangleq Var[\mathbf{a}]$. Assume that each element $x_i \overset{\text{iid}}{\sim} \mathcal{N}(0, 1)$, and that entries $V_{ij}$ are independent and distributed with $\mu_{\mathbf{V}} \triangleq E[{\mathbf{V}}] = 0, \sigma^2_{\mathbf{V}} \triangleq \mathrm{Var}[{\mathbf{V}}] = 1$. Then, up to a first-order Taylor approximation, $\sigma^2_{\mathbf{a}} \propto \frac{1}{k}$ for $k \gg 1$.
\end{proposition}
\begin{proof} 
Recall that by the definition of the softmax function, $s_i = \text{softmax}(\mathbf{x})_i = \frac{\me^{x_i}}{\sum_{j=1}^{k} \me^{x_j}}$. Denote the vector of elements' numerators $\me^{x_i}$ as $\mathbf{n}$ and the vector of denominators $\sum_{j=1}^{k} \me^{x_j}$ as $\mathbf{d}$, such that $\mathbf{s} = \frac{\mathbf{n}}{\mathbf{d}}$. Since $x_i \overset{\text{iid}}{\sim} \mathcal{N}(0,1)$, $\mathbf{n}$ is log-normally distributed and $\mathbf{d}$ is a sum of log-normals. This implies that\footnote{See Appendix ~\ref{subsec:softmax-covariance} for the derivation of $\mathrm{Cov}[\mathbf{n}, \mathbf{d}]$}:
\begin{equation}
    \begin{split}
        \mu_\mathbf{n} = \me^{1/2}, \quad \sigma^2_\mathbf{n} = \me(\me-1) \\
        \mu_\mathbf{d} = k\me^{1/2}, \quad \sigma^2_\mathbf{d} = k\me(\me-1) \\
        \mathrm{Cov}[\mathbf{n}, \mathbf{d}] = \sigma^2_\mathbf{n} = \me(\me-1)
    \end{split}
    \label{eq:softmax-dist-properties}
\end{equation}
We can then use first-order Taylor approximations to estimate the moments of $\mathbf{s}$ as the ratio $\frac{\mathbf{n}}{\mathbf{d}}$, as shown in \citet{statisticalInference}, to obtain:
\begin{equation}
\mu_\mathbf{s} = \mathrm{E}\left[ \frac{\mathbf{n}}{\mathbf{d}} \right] = \frac{\mu_\mathbf{n}}{\mu_\mathbf{d}} = \frac{1}{k} 
\label{eq:softmax-mean}
\end{equation}
\begin{equation}
    \begin{split}
        \sigma^2_\mathbf{s} = \mathrm{Var}\left[ \frac{\mathbf{n}}{\mathbf{d}} \right] &\approx \frac{\sigma^2_\mathbf{n}}{\mu^2_\mathbf{d}} + \frac{\mu^2_\mathbf{n} \sigma^2_\mathbf{d}}{\mu^4_\mathbf{d}} - 2\frac{\mu_\mathbf{n} \mathrm{Cov}[\mathbf{n}, \mathbf{d}]}{\mu^3_\mathbf{d}} \\
        &= \frac{\me-1}{k^2} - \frac{\me-1}{k^3}
    \end{split}
\end{equation}
Note that Eq.~\ref{eq:softmax-mean} holds exactly from the fact that all $k$ entries in $\mathbf{s}$ are positive and must sum to 1.
Now, because each element $a_j = \sum_{i=1}^{k} s_iV_{ij}$, with independent entries $V_{ij}$, and with the fact that $\mu_{\mathbf{V}} = 0$ and $\sigma^2_{\mathbf{V}} = 1$, the mean and variance of $\mathbf{a}$ can be determined as:
\begin{equation}
\mu_\mathbf{a} = \sum_{i=1}^{k} \mu_\mathbf{s} \mu_\mathbf{V} = 0
\end{equation}
\begin{equation}
\sigma^2_\mathbf{a} = \sum_{i=1}^{k} \sigma^2_\mathbf{s} \sigma^2_\mathbf{V} + \sigma^2_\mathbf{s} \mu^2_\mathbf{V} + \sigma^2_\mathbf{V} \mu^2_\mathbf{s} = \frac{\me}{k} - \frac{\me-1}{k^2}
\end{equation}
The first term dominates for large $k$ and so $\sigma^2_\mathbf{a} \sim \frac{1}{k}$.
\end{proof}
In the causal self-attention operation shown in Eq.~\ref{eq:standard_attn}, the attention logits matrix $\frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{d}}$ is causally masked such that the row of logits for a token at sequence position $k$ has length $k$. For a given token, by Prop.~\ref{proof:attn-output-variance}, the output of the self-attention operation will therefore have variance inversely related to that token's sequence position $k$. This causes tokens that appear later in the sequence to have much smaller variance than those that appear earlier, as shown in Fig.~\ref{attn-output-variance}.

To address this issue, we make use of a basic property of the variance of linear combinations of independent random variables. With $\mathbf{a}(k)$ denoting the outputs of self-attention applied over a sequence of length $k$, the variance of $\mathbf{a}(k)$ (denoted $\sigma^2_{\mathbf{a}(k)}$) is the variance of a sum of $k$ random variables $\{X_i,\ldots,X_k\}$ with coefficients $\mathbf{c} \in \mathbb{R}^k$:
\begin{equation}
    \begin{split}
    &\mathrm{Var}\left[ \sum_{i=1}^{k} c_i X_i \right] = \sum_i c_i^2 \mathrm{Var}[X_i] = \mathbf{c}^T \mathbf{v},
    \end{split}
\end{equation}
where $v_i \triangleq \mathrm{Var}[X_i]$, and the equality holds if all $X_i$ are independent. If $\forall i\colon  v_i = 1$, we further have $\sigma^2_{\mathbf{a}(k)} = \|\mathbf{c}\|_2$.

Now recall that the softmax operation outputs positive coefficients $\mathbf{s}$ that sum to 1. This means that if we simply set coefficients $c_i = \sqrt{s_i}$, we obtain:
\begin{equation}
\sigma^2_{\mathbf{a}(k)} = \|\mathbf{c}\|_2 = \sqrt{\sum_i c_i^2} = \sqrt{\sum_i s_i} = 1.
\end{equation}
That is, by taking the square root of attention scores, attention can be made variance-preserving for independent value tokens. This modification, which we term ``Square-Root Softmax attention'', is shown in Eq.~\ref{eq:sqrt_softmax_attn}. Square-Root Softmax attention is also easily implemented via modern attention kernels like Flex-Attention \cite{flexAttention}.
\begin{equation}
    \text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \sqrt{\text{softmax}\left(\frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{d_k}}\right)}\mathbf{V}
    \label{eq:sqrt_softmax_attn}
\end{equation}
In practice, standard self-attention does have diminishing $\sigma$ as sequence position increases; however, the observed variance is consistently higher than predicted by the above analysis of independent elements. This same effect is observed even when using Square-Root Softmax attention, causing observed $\sigma$ to increase over sequence position instead (Fig.~\ref{attn-output-variance}).

\begin{figure}[h]
\centering
\includegraphics[width=1.0\columnwidth]{figures/attention_variance.png}
\caption{\textbf{Attention output variance changes over sequence length.} For standard attention, $\sigma$ decreases over sequence position both when simulated with iid value tokens (light red) and when observed in training (red). Taking the square root of attention scores keeps $\sigma$ constant when simulated with iid value tokens (light blue), but during training (blue), causes $\sigma$ to increase with sequence position. In practice, neither attention variant provides a consistent scale across outputs.}
\label{attn-output-variance}
\end{figure}

We provide a mechanistic explanation for this phenomenon: this increase in attention variance is an unavoidable consequence of the statistics of natural data. If all value tokens are truly independent, then Square-Root Softmax attention keeps $\sigma_\mathbf{a}$ constant. However, due to a high number of repeated tokens in real text data, value tokens are often highly correlated (Fig.~\ref{values-cossim}). Due to this correlation, $\sigma_\mathbf{a}$ will be higher than predicted, and in the case of standard self-attention, diminish more slowly with respect to the token position.

\begin{figure}[h]
\centering
\includegraphics[width=1.0\columnwidth]{figures/values_cossim.png}
\caption{\textbf{Value tokens in text are highly correlated.} Comparison of cosine similarity between observed value tokens in a text data distribution versus value tokens $\stackrel{iid}{\sim} \mathcal{N}(0,1)$. Repeated tokens in the value matrix, an unavoidable result of token frequency in real text data, lead to higher-than-random $\sigma$ as sequence position increases (c.f. Fig.~\ref{attn-output-variance}).
}
\label{values-cossim}
\end{figure}

\begin{figure}[htb]
\begin{subfigure}[h]{\columnwidth}
\centering
\caption{\quad}
\includegraphics[width=0.8\columnwidth]{figures/ln_placement.png}
\label{fig:ln-placement}
\end{subfigure}

\begin{subfigure}[h]{\columnwidth}
\centering
\caption{\quad}
\includegraphics[width=0.8\columnwidth]{figures/depth_bois_loss_curves.png}
\label{fig:deep-ln-placement-convergence}
\end{subfigure}
\caption{\textbf{Res-Post-LayerNorm.} \textbf{(a)} Pre-LayerNorm transformer architecture versus Res-Post-LayerNorm architecture. Res-Post-LayerNorm moves the LayerNorm operation from the start of each residual branch to the end \cite{swinV2}. This ensures consistent variance across tokens when added to the residual stream. In contrast, Pre-LayerNorm networks permit unnormalized representations with inconsistent variance to be added to the residual stream, as shown with self-attention outputs in Fig.~\ref{attn-output-variance}. \textbf{(b)} Convergence test loss curves with 100-layer models show that µS with Res-Post-LayerNorm achieves nearly identical convergence versus SP with Pre-LayerNorm.
}
\label{combined_respostln}
\end{figure}

To address this inconsistency in attention output variance, we use Res-Post-LayerNorm placement, as shown in Fig.~\ref{combined_respostln}(a).
This architecture change consists of moving the normalization operation from the start of each residual branch to the end, and was first proposed in \citet{swinV2} for training stability. Res-Post-LayerNorm ensures consistent $\sigma$ for all tokens in the residual stream, regardless of sequence position, correlation with other tokens, or the distribution of attention scores. A convergence test on 100-layer models validating the Res-Post-LayerNorm transformer against the standard Pre-LayerNorm transformer is shown in Fig.~\ref{combined_respostln}(b). All µS models we train use Res-Post-LayerNorm.

\subsection{Residual Modification Schemes}

Every skip connection in a neural network adds another tensor into the residual stream. Summing all these tensors tends to increase the variance of the residual stream deeper in the network. To instead make residual connections variance-preserving, \citet{unitScaling} proposed to replace simple summation with weighted summation, where the weights $a$ and $b$ of the skip connection and residual branch satisfy $a^2 + b^2 = 1$. They proposed two methods for setting these coefficients: \textit{fixed} and \textit{running-mean}, which are shown in Eq.~\ref{eq:res-mod-fixed} and Eq.~\ref{eq:res-mod-running-mean} respectively. The former uses a constant coefficient $\tau$ while the latter uses coefficients which are a function of the layer index $l$.
\begin{equation}
    \text{fixed}(\tau): x_{l+1} = \sqrt{1-\tau} \cdot x_l + \sqrt{\tau} \cdot f(x_l)
    \label{eq:res-mod-fixed}
\end{equation}
\begin{equation}
    \text{running-mean}: x_{l+1} = \sqrt{\frac{l}{l+1}} \cdot x_l + \sqrt{\frac{1}{l+1}} \cdot f(x_l)
    \label{eq:res-mod-running-mean}
\end{equation}
As shown in Fig.~\ref{res-mod-runs}, we found that the \textit{fixed} scheme provides better convergence than the \textit{running-mean} scheme. All µS models we train therefore use the \textit{fixed} scheme. We set the coefficient $\tau$ based on the depth using the results in Appendix~\ref{subsec:residual-modification}.

\begin{figure}[h]
\begin{center}
\includegraphics[width=1.0\columnwidth]{figures/depth_bois_res_mod_loss_curves.png}
\end{center}
\caption{\textbf{Residual Modification schemes affect µnit Scaled model convergence.} The \textit{fixed} residual modification (green, Eq.~\ref{eq:res-mod-fixed}) achieves better training convergence for deep transformers than the \textit{running-mean} residual modification (blue, Eq.~\ref{eq:res-mod-running-mean}). The \textit{fixed} residual coefficient for this model is $\tau = 0.1$.}
\label{res-mod-runs}
\end{figure}

\subsection{Hyperparameter Transfer with µnit Scaling}\label{sec:hparam-transfer}

Zero-shot hyperparameter transfer allows hyperparameters to be tuned on a small proxy network, then directly used on much larger networks without any further tuning \cite{tensorProgramsV}. The width of the small proxy network is typically referred to as the ``base width'', or $d_\text{base}$. Because it eliminates the need to sweep hyperparameters at a large scale, such hyperparameter transfer yields massive compute savings.

Hyperparameter transfer with µnit Scaling follows from neural network equivalencies set forth in \citet[Appendix J.2.1]{tensorProgramsV}, reproduced below for convenience. As detailed in \citet{uMuP}, Equations~\ref{eq:weight_init},~\ref{eq:hidden_layer}, and~\ref{eq:weight_update} define the hidden layer in a model undergoing training. All hidden layers are initialized with weights $\mathbf{W}_0$ drawn from a normal distribution with variance $b^2$, use a learning rate of $c$, and have an output multiplier $a$. $\mathbf{X}$ and $\mathbf{Y}$ denote input and output activation matrices respectively; $t$ is the training time step; and $\mathbf{\Phi}_t(\nabla\mathcal{L}_0, \ldots, \nabla\mathcal{L}_t)$ denotes the weight update for time step $t$ using prior loss gradients.
\begin{equation}
\begin{split}
    \mathbf{W}_0 \sim \mathcal{N}(0, b^2)
\end{split}
\label{eq:weight_init}
\end{equation}
\begin{equation}
\begin{split}
    \mathbf{Y} = a \cdot \mathbf{X}\mathbf{W}_t
\end{split}
\label{eq:hidden_layer}
\end{equation}
\begin{equation}
\mathbf{W}_{t+1} = \mathbf{W}_t + c \cdot 
    \mathbf{\Phi}_t(\nabla\mathcal{L}_0, \ldots, \nabla\mathcal{L}_t)
\label{eq:weight_update}
\end{equation}
Under Adam-like optimizers, the output of this hidden layer is invariant to any scale factor $\theta > 0$ that changes $a, b, c$ as:
\begin{equation}
    a \leftarrow a\theta, \quad b \leftarrow b/\theta, \quad c \leftarrow c/\theta
    \label{eq:ABC_equivalence}
\end{equation}
Under µP, $a = 1$, $b = \frac{1}{\sqrt{\text{fan\_in}}}$, and $c = \frac{1}{\text{fan\_in}}$. If we instead set $\theta = \frac{1}{\sqrt{\text{fan\_in}}}$, we obtain:
\begin{equation}
    a = \frac{1}{\sqrt{\text{fan\_in}}}, \quad b = 1, \quad c = \frac{1}{\sqrt{\text{fan\_in}}}
    \label{eq:µS_equivalence}
\end{equation}
Notice that $a=\frac{1}{\sqrt{\text{fan\_in}}}$ and $b=1$ are exactly the output multiplier and unit initialization that Unit Scaling requires. Therefore, the learning rate for hidden layers should scale as $\frac{1}{\sqrt{\text{fan\_in}}}$ for Unit Scaled models. This leads to the µS hyperparameter transfer scheme in Table~\ref{µS_table}. 

In practice, given a base model with a width $d_\text{base}$, a new model with a width $d_\text{new}$, and optimal base model learning rate $\eta^*_\text{base}$, µS keeps $\eta^*_\text{new}$ constant for the embedding table, all LayerNorm parameters, and the LM head. The learning rate only changes for hidden layers, with $\eta^*_\text{new} = \eta^*_\text{base} \frac{\sqrt{d_\text{base}}}{\sqrt{d_\text{new}}}$. 

\begin{table}[h]
\setlength{\tabcolsep}{3.5pt}
\caption{\textbf{µS scaling rules.} To transfer hyperparameters across model widths with µS, initialize layers, scale their outputs, and modify their learning rates as shown here.}
\label{µS_table}
\begin{center}
\begin{tabular}{lccc}
& \multicolumn{3}{c}{Weight Type} \\ 
\cline{2-4}
& Input Layer & Final Layer & Hidden Layers \\ \hline
Init. Var. & 1 & 1 & 1 \\
Output Mult. & 1 & $1/\text{fan\_in}$ & $1/\sqrt{\text{fan\_in}}$ \\
Adam-like $\lambda$ & 1 & 1 & $1/\sqrt{\text{fan\_in}}$ \\ \hline
\end{tabular}
\end{center}
\end{table}

In addition to enabling hyperparameter transfer, µS also requires sweeping over a much smaller set of hyperparameters than existing schemes (Table~\ref{hparams_table}).

\begin{table}[h]
\caption{\textbf{Required hyperparameters in transfer schemes.} Hyperparameters used in practice to train transformer models under various schemes. While µP and related schemes provide better hyperparameter transfer than SP, they require sweeping over more hyperparameters to get reasonable model quality. In contrast, µS provides hyperparameter transfer and model quality with a much smaller set of hyperparameters. This makes the implementation simple and makes hyperparameter sweeps less expensive.}
\label{hparams_table}
\begin{center}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{ccc}
\hline
\renewcommand{\arraystretch}{1.2}
Scheme & \# Hparams & Hparams \\ \hline
\textbf{µS (ours)} & 3 & \makecell{$\eta, \lambda, \tau$} \\ \hline\hline
\textbf{SP} & 3 & \makecell{$\eta, \lambda, \sigma_{\text{init}}$} \\ \hline
\textbf{µP} & 6 & \makecell{$\eta, \lambda, \sigma_{\text{init}},$ \\ $\alpha_{\text{res}}, \alpha_{\text{attn}}, \alpha_{\text{out}}$} \\ \hline
\textbf{u-µP} & 7 & \makecell{$\eta, \lambda, \alpha_{\text{ffn-act}}, \alpha_{\text{attn-softmax}},$ \\ $\alpha_{\text{res}}, \alpha_{\text{res-attn-ratio}}, \alpha_{\text{loss-softmax}}$} \\ \hline
\end{tabular}
\end{center}
\end{table}

\section{Results}

\subsection{Successful Hyperparameter Transfer}

\textbf{Setup:}\quad To evaluate hyperparameter transfer, we first train four-layer decoder-only LLMs with widths of 256 through 8192 using Standard Parametrization (SP) and µnit Scaling (µS). We begin with these these small models since doing so allows us to collect ground truth optimal hyperparameters. All models use multi-headed attention \cite{attentionIsAllYouNeed} and were trained for 10,000 training steps with a global batch size of 64 and sequence length of 1024 (i.e., 655M total tokens). SP models use Pre-LayerNorm placement and are trained in both BF16 and FP8 (using TransformerEngine). µS models were trained in both BF16 and FP8 and use Res-Post-LayerNorm placement (Fig.~\ref{combined_respostln}). µS used base models of width 256. For all models described in this and subsequent sections, we used the Lion optimizer \cite{lion} with fully decoupled weight decay and a cosine learning rate schedule decaying to 10\% of the maximum learning rate. For details on why Lion is an Adam-like optimizer for hyperparameter transfer, please refer to Appendix~\ref{subsec:lion}. All models were trained on Nvidia H100 GPUs using the Databricks MosaicML LLMFoundry \cite{foundry}, Composer \cite{composer}, and Streaming \cite{streaming} libraries.

\textbf{Hyperparameters:}\quad We evaluate hyperparameter transfer over learning rate ($\eta$) and weight decay ($\lambda$). While µP \citet{tensorProgramsV} does not give a theoretical basis for $\lambda$ transfer over width, we evaluate its transfer empirically because of its practical importance. We use fully decoupled weight decay, motivated by findings
from \citet{smallScaleTransformerInstabilities}, \citet{largeScaleExplorationMuTransfer}, and \citet{adamWWeightDecay}. To elucidate how $\lambda$ scales with model width, we jointly sweep over both $\eta$ and $\lambda$ with powers of 2. Based on the relationship between the residual coefficient $\tau$ and depth in Appendix~\ref{subsec:residual-modification}, the residual coefficient $\tau$ is 0.4 for these four layer models.

As shown in Fig.~\ref{hparam_transfer}, µS models have stable optimal learning rate ($\eta^*$) and weight decay ($\lambda^*$) from width 256 up to width 8192. Mirroring previous findings, $\eta^*$ for SP models decreases as the inverse of the width. $\lambda^*$ transfer across widths is relatively stable for both model types, with µS showing the most consistency.

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.85\columnwidth]{figures/combined_hparam_transfer.png}
\end{center}
\caption{\textbf{With µS, optimal learning rate ($\eta^*$) and weight decay ($\lambda^*$) are stable across widths.} Optimal $\eta$ (left column) and $\lambda$ (right column) are shown across a range of model widths for models trained with SP (top row) and µS (bottom row). For each curve, the other hyperparameter is fixed at its optimal value. The base model width is 256. µS models have stable optimal $\eta$ and $\lambda$, even when width increases 32x to 8192. As expected, $\eta^*$ for SP models decreases with width. $\lambda^*$ is relatively stable as the width increases across both model types.}
\label{hparam_transfer}
\end{figure}

\subsection{FP8 Training at Scale}

The previous section demonstrated hyperparameter transfer for small, shallow models. However, the real test of utility is scaling up to multi-billion-parameter models. This section demonstrates that µS allows us to train in FP8 while transferring hyperparameters for realistic model sizes. We also validate that our method is compatible with efficient distributed training.

\begin{table*}[t]
\caption{\textbf{Large model training configurations.} Model training configurations for 1B, 3B, 7B, and 13B models. Only µS models use the residual coefficient $\tau$, which is dictated by model depth using results in Appendix~\ref{subsec:residual-modification}.}
\label{model-config-table}
\begin{center}
{
\begin{tabular}{l|c|c|c|c|c|c|c|c|c|c}
\hline
Model & Param. & Tokens & TPR & Steps & Batch Sz. & Seq. Len. & Width & Depth & \# Heads & $\tau$ \\
\hline
\textbf{1B} & 1.6B & 31.5B & 19.4 & 7.5k & 1024 & 4096 & 2048 & 24 & 16 & 0.3 \\
\textbf{3B} & 3.0B & 62.9B & 20.8 & 15k & 1024 & 4096 & 2560 & 32 & 20 & 0.3 \\
\textbf{7B} & 7.3B & 140.0B & 19.3 & 16.7k & 2048 & 4096 & 4096 & 32 & 32 & 0.3 \\
\textbf{13B} & 13.6B & 260.1B & 19.1 & 31k & 2048 & 4096 & 5120 & 40 & 40 & 0.2 \\
\hline
\end{tabular}
}
\end{center}
\end{table*}

\begin{figure*}[htb]
\begin{center}
\includegraphics[width=0.96\textwidth]{figures/big_runs_loss_curves.png}
\end{center}
\caption{\textbf{µS models successfully train in FP8 at scale.} Comparison of training loss curves for standard parametrized (SP) and µnit scaled (µS) models in both FP8 and BF16, across 1B, 3B, 7B, and 13B parameter models. µS models successfully train in FP8 and converge to similar train loss values as their BF16 and SP counterparts. SP FP8 models are trained with TransformerEngine (TE). In our experiments at the 13B scale, SP models trained in FP8 with TE experienced frequent loss spikes and did not properly converge. We achieve state-of-the-art FP8 training efficiency via µS, with further details in Appendix~\ref{sec:training-efficiency}.}
\label{big_runs_loss_curves}
\end{figure*}

\textbf{Setup:}\quad We train 1B, 3B, 7B, and 13B parameter LLMs on approximately compute-optimal token budgets ($\sim$20x token to parameter ratio) using SP and µS, and in both BF16 and FP8, resulting in 4 individual models for each model size. The training configurations are detailed in Table~\ref{model-config-table}. 
% Hyperparameters $\eta^*_{\text{base}}$ and $\lambda^*_{\text{base}}$ for all models were found using a base model of width 256.
%Then, $\eta^*$ and $\lambda^*$ for large models was determined for each model type in accordance with the hyperparameter transfer rules detailed in the previous section.
Based on the previous sections' hyperparameter transfer results (Fig.~\ref{hparam_transfer}), we sweep $\eta$ and $\lambda$ on small models with a base width of $d_\text{base} = 256$, then transfer optimal hyperparameters to large models with width $d_\text{new}$, as shown below.

\begin{itemize}
    \item \textbf{SP:} $\; \text{all layers:}\ \eta^*_{\text{new}} = \eta^*_{\text{base}}\frac{d_\text{base}}{d_\text{new}},\ \lambda^*_{\text{new}} = 0.5\lambda^*_\text{base}$
    \item \textbf{µS:} $\; \text{hidden layers:}\ \eta^*_{\text{new}} = \eta^*_{\text{base}}\frac{\sqrt{d_\text{base}}}{\sqrt{d_\text{new}}}, \ \lambda^*_{\text{new}} = \lambda^*_\text{base} \\ \phantom{.......} \text{other layers:}\  \eta^*_{\text{new}} = \eta^*_{\text{base}}, \ \lambda^*_{\text{new}} = \lambda^*_\text{base}$
\end{itemize}

\textbf{Evaluation:}\quad We use the Databricks Model Gauntlet to evaluate the quality of all models on specific tasks \cite{gauntletEval, gauntletCalibration}. These results are shown in Table~\ref{model-evals-table}.

We also compare model convergence via the final training cross-entropy loss averaged over the last 41.9M tokens (corresponding to 10 steps for 1b and 3b models and 5 steps for 7b and 13b models). Training loss curves are shown in Fig.~\ref{big_runs_loss_curves}.

\begin{table*}[htb]
\caption{\textbf{Large model evaluation results.} We evaluate SP and µS models in FP8 and BF16 on a variety of tasks, with best results per eval and model size in bold. Final train loss (avg. over last $\sim$40M tokens) is also shown. µS models have equal or better quality than SP models, and maintain this quality even when training in FP8 as model size increases. Note that 13B SP FP8 models failed to properly converge, denoted by an asterisk.}

\label{model-evals-table}
\begin{center}
\renewcommand{\arraystretch}{1.2}

{\tiny
\setlength{\tabcolsep}{3pt}
\begin{tabular}{|l|cc|cc|cc|cc|cc|cc|cc|cc|}
\hline
 & \multicolumn{4}{c|}{\textbf{1b}} & \multicolumn{4}{c|}{\textbf{3b}} & \multicolumn{4}{c|}{\textbf{7b}} & \multicolumn{4}{c|}{\textbf{13b}} \\
 & \multicolumn{2}{c|}{SP} & \multicolumn{2}{c|}{µS} & \multicolumn{2}{c|}{SP} & \multicolumn{2}{c|}{µS} & \multicolumn{2}{c|}{SP} & \multicolumn{2}{c|}{µS} & \multicolumn{2}{c|}{SP} & \multicolumn{2}{c|}{µS} \\
 & BF16 & FP8 & BF16 & FP8 & BF16 & FP8 & BF16 & FP8 & BF16 & FP8 & BF16 & FP8 & BF16 & FP8* & BF16 & FP8 \\
\hline\hline
Final Train Loss & 2.590 & 2.588 & \textbf{2.580} & 2.590 & 2.399 & 2.400 & \textbf{2.381} & 2.390 & 2.228 & 2.231 & \textbf{2.216} & 2.226 & 2.112 & 2.211 & \textbf{2.108} & 2.119 \\
\hline
ARC Easy (3-shot) & 52.1\% & 52.4\% & \textbf{53.4\%} & 53.3\% & 60.7\% & 60.8\% & \textbf{61.9\%} & 60.8\% & 67.2\% & 65.6\% & 67.1\% & \textbf{68.0\%} & \textbf{72.3\%} & 35.7\% & 71.8\% & 69.7\% \\
Jeopardy (3-shot) & 4.1\% & 4.3\% & \textbf{4.5\%} & 3.5\% & 13.4\% & 11.3\% & \textbf{16.8\%} & 16.6\% & 27.3\% & 27.4\% & \textbf{32.7\%} & 30.6\% & 40.2\% & 0.2\% & \textbf{43.1\%} & 41.7\% \\
SQuAD (3-shot) & 32.6\% & \textbf{33.2\%} & 30.9\% & 31.3\% & 42.3\% & 45.3\% & \textbf{47.9\%} & 47.8\% & 53.9\% & 50.0\% & \textbf{57.1\%} & 55.1\% & 52.9\% & 1.5\% & \textbf{62.8\%} & 61.6\% \\
HellaSwag (0-shot) & 47.2\% & 47.5\% & \textbf{48.3\%} & 47.4\% & 57.1\% & 57.7\% & \textbf{59.6\%} & 59.5\% & 66.8\% & 66.5\% & \textbf{69.2\%} & 68.2\% & 73.9\% & 29.7\% & \textbf{74.6\%} & 74.3\% \\
BIG-bench Wikidata QA (3-shot) & 47.3\% & 48.6\% & 49.3\% & \textbf{50.2\%} & 53.0\% & 55.0\% & 56.2\% & \textbf{57.5\%} & \textbf{60.4\%} & 60.0\% & 60.0\% & 59.9\% & \textbf{66.9\%} & 4.0\% & 66.1\% & 62.9\% \\
WinoGrande (5-shot) & \textbf{55.0\%} & 52.6\% & 51.1\% & 52.0\% & 58.8\% & 54.9\% & \textbf{59.5\%} & 58.6\% & 62.8\% & 64.1\% & \textbf{65.7\%} & 65.3\% & 70.3\% & 57.8\% & \textbf{71.1\%} & 70.5\% \\
OpenBookQA (10-shot) & \textbf{32.8\%} & 32.4\% & 32.0\% & 32.4\% & 37.8\% & 38.2\% & \textbf{38.8\%} & 36.2\% & 42.4\% & 42.0\% & \textbf{44.0\%} & 41.8\% & 45.2\% & 26.6\% & 45.8\% & \textbf{46.6\%} \\
PIQA (0-shot) & 70.7\% & 71.1\% & \textbf{71.5\%} & 71.2\% & 74.5\% & \textbf{75.2\%} & 74.3\% & 74.3\% & \textbf{77.2\%} & 77.0\% & 76.7\% & 76.5\% & 78.7\% & 54.5\% & \textbf{80.1\%} & 79.4\% \\
TriviaQA (3-shot) & 9.7\% & 10.5\% & \textbf{10.8\%} & 9.7\% & 17.8\% & 17.7\% & \textbf{20.4\%} & 18.7\% & 30.2\% & 29.1\% & 32.5\% & \textbf{33.8\%} & 42.4\% & 0.5\% & 44.3\% & \textbf{44.8\%} \\
Winograd (3-shot) & 64.5\% & \textbf{69.6\%} & 67.0\% & 68.9\% & 73.3\% & 74.0\% & 75.8\% & \textbf{76.6\%} & 78.8\% & \textbf{80.6\%} & \textbf{80.6\%} & \textbf{80.6\%} & 83.9\% & 62.6\% & \textbf{86.1\%} & 82.8\% \\
LAMBADA (0-shot) & \textbf{44.8\%} & 44.5\% & 43.6\% & 41.3\% & 52.8\% & 54.2\% & 55.9\% & \textbf{57.4\%} & 60.3\% & 60.7\% & 63.0\% & \textbf{64.6\%} & \textbf{65.7\%} & 34.8\% & 61.6\% & 64.3\% \\
CoQA (0-shot) & 19.3\% & \textbf{21.3\%} & 20.8\% & 20.0\% & 26.2\% & 25.4\% & 27.9\% & \textbf{28.6\%} & 28.2\% & 32.0\% & 33.3\% & \textbf{35.0\%} & 39.8\% & 13.2\% & 44.4\% & \textbf{44.6\%} \\
ARC Challenge (3-shot) & 25.4\% & 26.0\% & \textbf{27.8\%} & 25.0\% & 30.3\% & 30.1\% & \textbf{31.8\%} & 30.9\% & 36.1\% & 35.7\% & 38.3\% & \textbf{39.0\%} & 42.0\% & 27.6\% & \textbf{42.2\%} & 41.5\% \\
COPA (0-shot) & 65.0\% & 68.0\% & 64.0\% & \textbf{70.0\%} & 69.0\% & 68.0\% & 68.0\% & \textbf{71.0\%} & 76.0\% & 76.0\% & 78.0\% & \textbf{80.0\%} & 83.0\% & 62.0\% & \textbf{84.0\%} & 78.0\% \\
BIG-bench Operators (3-shot) & 12.4\% & 12.9\% & 13.8\% & \textbf{14.3\%} & \textbf{19.5\%} & 17.1\% & 17.1\% & 18.6\% & 21.4\% & 20.0\% & 20.0\% & \textbf{23.3\%} & 31.4\% & 24.3\% & \textbf{37.6\%} & 37.1\% \\
GSM8K (0-shot) & 2.4\% & \textbf{2.6\%} & 2.4\% & 2.4\% & \textbf{3.7\%} & 1.7\% & 2.3\% & 2.0\% & 3.9\% & \textbf{5.0\%} & 4.0\% & 3.9\% & 8.7\% & 0.0\% & 9.3\% & \textbf{10.9\%} \\
\hline
\end{tabular}
}

\end{center}
\end{table*}

As shown in Fig.~\ref{big_runs_loss_curves}, µS models train stably with FP8 even as the model size increases. We successfully transfer hyperparameters from a narrow base model with a width of 256 to models with widths up to 5120, demonstrating 20x width transfer ($\sim$400x fewer FLOPs per run) in realistic, practical LLM training scenarios. This validates zero-shot hyperparameter transfer using µS. Evaluation results in Table~\ref{model-evals-table} show that µS models achieve equal or better quality than SP models. These models demonstrate that µS successfully combines FP8 training with zero-shot hyperparameter transfer. To emphasize, all hidden layers use FP8 computation, and there are no dynamic scaling factors.

We also note that at the 13B scale, we attempted to remedy the divergence of the SP FP8 model by using multiple different values of $\lambda$, but this did not mitigate the frequent loss spikes and eventual divergence. µS models, by contrast, train stably.

\subsection{FP8 Training Efficiency}\label{sec:training-efficiency}

To achieve state-of-the-art FP8 distributed training efficiency with µnit Scaling, we make use of operator fusion and static scaling. As shown in Fig.~\ref{mus-fp8-training-speedup}, FP8 training with µS is 25-33\% faster than in BF16, and 1-6\% faster than FP8 training with TransformerEngine (TE) \cite{transformerEngine}. All models were benchmarked on 64 NVIDIA H100 GPUs, and characteristics such as batch size and distributed training configuration were held constant. While TransformerEngine has fused modules such as LayerNorm-Linear or LayerNorm-MLP, we did not use those modules in order to make an equal comparison between µS and TE.

\begin{figure}[h]
\centering
\includegraphics[width=0.9\columnwidth]{figures/fp8_training_speedup.png}
\caption{\textbf{Training in FP8 with µS achieves state-of-the-art efficiency.} FP8 training with µnit Scaling provides 25-33\% higher throughput than BF16 training and 1-6\% higher throughput than FP8 training with TransformerEngine (TE), over 1B, 3B, 7B, and 13B model sizes. Models are configured as specified in Table~\ref{model-config-table} and benchmarked on 64 NVIDIA H100 GPUs. Static scaling, operator fusion, and simplifications to Unit Scaling make this efficiency possible.}
\label{mus-fp8-training-speedup}
\end{figure}

By relying on dynamic scaling, FP8 training with libraries like TE imposes addtional overhead that is eliminated in µS. Calculating the absolute max of both the weight and activation tensors (or storing and reading past absolute max values in a delayed scaling approach) are operations that can be completely discarded in µS. Weights, activations, and gradients can be directly cast to FP8 formats, with a constant $\alpha = \frac{1}{\sqrt{fan\_in}}$ scaling factor used in the hidden linear layers' GEMM calls, where a GEMM is defined as:
\begin{equation}
    \mathbf{C} \leftarrow \alpha\mathbf{A}\mathbf{B} + \beta\mathbf{C}
    \label{eq:gemm-eq}
\end{equation}
NVIDIA's H100 GPUs support FP8 GEMMs through the $cublasLtMatmul()$ operation \cite{cuBLAS}.

To maximize training speed and mirror TransformerEngine \citet{transformerEngine}, we fuse clipping to the FP8 range, casting to FP8, and transposing into a single Triton \citet{triton} kernel. A transpose is necessary because H100s only support one layout (``TN'') with FP8, but the forward and backward passes use different layouts (thanks to using $\mathbf{W}$ vs $\mathbf{W}^T$).  

\section{Conclusion}

This work presents µnit Scaling (µS), an LLM training method enabling both statically-scaled FP8 computation and zero-shot hyperparameter transfer at scale. µnit Scaling consists of a set of principled model and optimization modifications, including Res-Post-LayerNorm, variance-preserving skip connections, unit-variance initialization, and straightforward scaling of optimization hyperparameters with model width.
Compared to alternatives, µnit Scaling is simpler, faster, more stable across model scales, and has fewer hyperparameters. We demonstrate successful FP8 training with hyperparameter transfer at scale with high-quality µnit Scaled LLMs at 1B, 3B, 7B, and 13B sizes.

% % Acknowledgements should only appear in the accepted version.
% \section*{Acknowledgements}

% \textbf{Do not} include acknowledgements in the initial version of
% the paper submitted for blind review.

% If a paper is accepted, the final camera-ready version can (and
% usually should) include acknowledgements.  Such acknowledgements
% should be placed at the end of the section, in an unnumbered section
% that does not count towards the paper page limit. Typically, this will 
% include thanks to reviewers who gave useful comments, to colleagues 
% who contributed to the ideas, and to funding agencies and corporate 
% sponsors that provided financial support.

\section*{Impact Statement}

% This paper presents work whose goal is to advance the field of 
% Machine Learning, specifically through enabling more efficient model training. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.

This paper introduces µnit Scaling (µS), a method designed to enhance the efficiency of Large Language Model (LLM) training through scalable FP8 computation and straightforward hyperparameter transfer. The advancements provided by µS could reduce both the computational and environmental costs associated with training large-scale models, potentially democratizing access to high-performance machine learning by lowering resource requirements. While this work's primary goal is advancing training efficiency, we acknowledge that, as with all machine learning technologies, continued attention to ethical considerations and societal implications remains important.

\bibliography{example_paper}
\bibliographystyle{paperinfo}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\appendix
\section{Appendix}

\subsection{Covariance of softmax numerator and denominator}\label{subsec:softmax-covariance}

In the proof for Prop.~\ref{proof:attn-output-variance}, we state that $\mathrm{Cov}[\mathbf{n}, \mathbf{d}] = \sigma^2_\mathbf{n}$. Here we derive this result. Just as in Sec.~\ref{subsec:self-attn-numerics}, define $\mathbf{s}$ as the output of the softmax function applied to a vector of $k$ independent elements $\mathbf{x}$. The softmax function is defined as $s_i = \text{softmax}(\mathbf{x})_i = \frac{\me^{x_i}}{\sum_{j=1}^{k} \me^{x_j}}$. As shown previously, we denote the vector of elements containing numerators of elements of $\mathbf{s}$ as $\mathbf{n}$ and denominators of elements of $\mathbf{s}$ as $\mathbf{d}$, such that $\mathbf{s} = \frac{\mathbf{n}}{\mathbf{d}}$. By the definition of covariance:
\begin{equation}
\mathrm{Cov}[\mathbf{n}, \mathbf{d}] = \mathrm{E}[(n_i - \mu_\mathbf{n})(d_i - \mu_\mathbf{d})]
\end{equation}
By the definition of softmax, $d_i = \sum_{i=1}^{k} n_i$, and by linearity of expectation, $\mu_\mathbf{d} = k\mu_\mathbf{n}$. Using this, we obtain:
\begin{equation}
    \mathrm{Cov}[\mathbf{n}, \mathbf{d}] = \mathrm{E}[(n_i - \mu_\mathbf{n})(n_1 + n_2 + \ldots + n_i + \ldots + n_k - k\mu_\mathbf{n})]
\end{equation}
Expanding this expression:
\begin{equation}
    \mathrm{Cov}[\mathbf{n}, \mathbf{d}] = \mathrm{E}[(n_i - \mu_\mathbf{n})((n_1 -\mu_\mathbf{n}) + (n_2 - \mu_\mathbf{n}) + \ldots + (n_i - \mu_\mathbf{n}) + \ldots + (n_k - \mu_\mathbf{n}))]
\label{eq:intermediate-result-softmax-cov}
\end{equation}
By linearity of expectation:
\begin{equation}
\mathrm{Cov}[\mathbf{n}, \mathbf{d}] = \mathrm{E}[(n_i - \mu_\mathbf{n})^2] 
+ \sum_{j \neq i} \mathrm{E}[(n_i - \mu_\mathbf{n})(n_j -\mu_\mathbf{n})]
\end{equation}
Because elements of the softmax input $\mathbf{x}$ are independent, and $n_i = \me^{x_i}$, elements of $\mathbf{n}$ are also independent. Therefore $\mathrm{E}[(n_i - \mu_\mathbf{n})(n_j -\mu_\mathbf{n})] = 0$ for $j \neq i$. Then by the definition of variance as $\mathrm{Var}[\mathbf{n}] = \mathrm{E}[(n_i - \mu_\mathbf{n})^2]$, we obtain:
\begin{equation}
\mathrm{Cov}[\mathbf{n}, \mathbf{d}] = \mathrm{Var}[\mathbf{n}]
\end{equation}



\subsection{Modifying Residual Connections with $\tau$}\label{subsec:residual-modification}

To make skip connections variance-preserving, we use the \textit{fixed} residual modification scheme, as shown in Eq.~\ref{eq:res-mod-fixed}, with coefficients based on the hyperparameter $\tau$ \cite{unitScaling}. To understand the relationship of the optimal residual coefficient $\tau^*$ with network depth, we swept over various values of $\tau$ for models of different widths (256, 512, 1024, 2048) and depths (20, 40, 60, 80, 100). In order to assess potential confounding effects between $\tau^*$ and $\eta^*$ and $\lambda^*$, we tuned those two hyperparameters as well. We trained each model for 10.5B tokens with a global batch size of 256 and sequence length of 4096. We define the optimal subset of models as those which had a final cross-entropy loss within 0.25\% of the optimum (with loss averaged over the last 10 steps, i.e. 10.5M tokens). As shown in Fig.~\ref{optimal-res-coeff-depth}, $\tau^*$ (for the optimal subset of models) decreases as network depth increases. Since the contribution of each residual branch exponentially decays with depth, a lower $\tau$ ensures a lower rate of decay, likely useful as networks get deeper. This relationship between $\tau^*$ and depth is consistent even as model width increases. In our experiments, $\tau$ can be coarsely swept. We use the results shown in Fig.~\ref{optimal-res-coeff-depth}, to directly choose $\tau^*$ for all µS model training.

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.5\textwidth]{figures/optimal_residual_coefficients.png}
\end{center}
\caption{\textbf{Optimal residual coefficient $\tau^*$ decreases with depth.} The 3 hyperparameters of $\tau$, $\eta$, and $\lambda$ are swept for models of varying widths (256, 512, 1024, 2048) and depths (20, 40, 60, 80, 100). The mean and standard error of $\tau$ is shown for the optimal subset of models from each hyperparameter sweep, where a model is included in the optimal subset if it had final cross-entropy loss within 0.25\% of the sweep optimum. $\tau^*$, which controls the decay rate of residual branch contributions in the residual stream, decreases as network depth increases.}
\label{optimal-res-coeff-depth}
\end{figure}


\subsection{Lion Optimizer and Hyperparameter Transfer}\label{subsec:lion}

Here, we show why Lion \citet{lion} is an "Adam-like" optimizer, so the µP rules for hyperparameter transfer with Adam \cite{adam} are applicable to Lion as well. Because Adam and Lion are both adaptive optimizers that normalize gradients coordinatewise before updating parameters, the nonlinear tensor product matrix results obtained in \citet[Appendix J.1.3]{tensorProgramsV} apply to both optimizers. One can see that Lion differs from Adam only in that it has a different second moment estimate. Under both optimizers, with gradient $g_t$, a parameter $\theta$ is updated as:

\begin{equation}
    \theta_{t+1} = \theta_t - \eta \frac{\beta_1 m_t + (1-\beta_1)g_t}{\sqrt{s_t}}
    \label{eq:adam-like-update}
\end{equation}

For Lion, this follows by expressing $\text{sign}(c_t)$ as $c_t / c_t^2$. Then, the second moment estimate $s_t$ for Adam (Eq.~\ref{eq:adam-second-moment}) and Lion (Eq.~\ref{eq:lion-second-moment}) are below.

\begin{equation}
    s_t^{\text{Adam}} = \beta_2 v_t + (1-\beta_2)g_t^2 + \epsilon
    \label{eq:adam-second-moment}
\end{equation}

\begin{equation}
    s_t^{\text{Lion}} = c_t^2 = \beta_1^2 m_t^2 + 2\beta_1(1-\beta_1)m_tg_t + (1-\beta_1)^2g_t^2
    \label{eq:lion-second-moment}
\end{equation}

This justifies why Lion is an Adam-like optimizer for the purposes of hyperparameter transfer. We use Lion for its reduced memory footprint in all our experiments.

\subsection{Activation Outliers}\label{subsec:activation-outliers}

We analyze activation distributions taken over 32,768 tokens at every 10 layers for all FP8 models trained according to Table~\ref{model-config-table}, with results shown in Fig.~\ref{activation-distributions}. These figures show the distribution of activation values for attention and FFN block inputs and outputs in the final 1B, 3B, 7B, and 13B FP8 models. While SP models consistently have outliers in the attention block and FFN block inputs at all model scales, µS models do not have these outliers in block inputs. This may make µS models more easily quantizable. It is important to note, however, that in SP models, the Pre-LayerNorm placement means that activations from the residual stream are first normalized before subsequent operations.

While we do not identify the exact mechanism by which these outliers arise in the residual stream in SP models, we show their absence in µS models here, with activation distributions that may be more conducive to quantization. An activation distribution with fewer outliers requires fewer bits to represent it.

\subsection{Activation Function Choice}\label{subsec:activation-functions}

The choice of activation function can have a signficant impact on activation underflow when training in FP8. For example, recent work by \cite{fp8TrainingTrillionTokenLLMs} identifies outlier amplification from SwiGLU as a challenge for FP8 LLM training. Nearly all state-of-the-art LLMs today use either SiLU or GELU as their activation function, but when training in FP8, this may lead to underflow in activations during training. This is because these functions asymptotically approach zero as inputs $x \rightarrow -\infty$. We define the FP8 underflow fraction, or the fraction of elements flushed to 0 from a BF16 to FP8 cast, as a metric to evaluate various activation functions.
As shown in Fig.~\ref{act-fn-underflow-dist}, this can cause many activations to underflow.

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.75\textwidth]{figures/act_fn_underflow_distributions.png}
\end{center}
\caption{\textbf{Different activation functions cause different amounts of FP8 underflow.} When casting $\mathcal{N}(0, 1)$ or $\text{Unif}(-128, 128)$ values from BF16 to FP8 (e4m3), GELU, SiLU, and ReLU (green) erroneously round to zero (underflow) with different probabilties. GELU and SiLU experience significant FP8 underflow because they slowly approach 0 for increasingly negative inputs. SiLU approaches 0 more slowly than GELU and so underflows for a wider range of inputs. ReLU simply maps all negative values to 0, regardless of the numerical format.}
\label{act-fn-underflow-dist}
\end{figure}

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.7\textwidth]{figures/ffn_underflow_comparison.png}
\end{center}
\caption{\textbf{Activation function choice impacts FP8 underflow and low-precision convergence error.} FP8 underflow of activation function outputs for each block in a 4 layer transformer model during training is shown for GELU, SiLU, and ReLU. Low precision convergence error, defined as the percent difference in final cross entropy loss between an FP8 model and its BF16 counterpart, is shown in the rightmost chart. GELU and SiLU cause significant underflow over the course of training, and models trained with these activation functions have twice as much low precision convergence error as with ReLU. ReLU greatly reduces this FP8 underflow by multiple orders of magnitude.}
\label{training-act-fn-underflow}
\end{figure}

To better understand how activation function choice influences FP8 underflow when training with µnit scaling, we train small 4 layer models with GELU, SiLU, and ReLU. Our findings, detailed in Fig.~\ref{training-act-fn-underflow} that during unit scaled model training, the choice of activation function drastically impacts the FP8 underflow rate for activation outputs. GELU greatly degrades the representation of FFN down projection inputs, reaching up to 30\% underflow during training. SiLU causes similar degradation, but at a lower rate, reaching up to 7\% during training. In contrast, ReLU does not suffer from this problem, with a maximum of 0.04\% FP8 underflow during training. As a result, FP8 unit scaled models trained with ReLU have smaller low-precision convergence error (defined as the percent difference between the final cross entropy loss an FP8 model and its BF16 counterpart). Based on these observations and results, ReLU minimizes FP8 underflow and low-precision convergence error. ReLU also has the added benefit of sparsifying activations, a property which enables significant inference-time optimizations \cite{reluStrikesBack}. However, using GELU results in models with lower final training loss. For this reason, we use GELU when training all µS models. Additional investigations into activation functions more suitable for FP8 training can help mitigate underflow while also providing improved convergence.


\afterpage{
\begin{figure*}[p]
    \centering

    %%%%%%%%%%%%%%%
    \begin{subfigure}[t]{\textwidth}
    \centering
    \caption{1B SP FP8 model activation distributions.}
    \includegraphics[width=0.9\textwidth]{figures/1b_basefp8.png}
    
    \label{fig:1b-basefp8-dist}
    \end{subfigure}
    %%%%%%%%%%%%%%%

    %%%%%%%%%%%%%%%
    \begin{subfigure}[t]{\textwidth}
    \centering
    \caption{1B µS FP8 model activation distributions.}
    \includegraphics[width=0.9\textwidth]{figures/1b_musfp8.png}
    \label{fig:1b-musfp8-dist}
    \end{subfigure}
    %%%%%%%%%%%%%%%

    %%%%%%%%%%%%%%%
    \begin{subfigure}[t]{\textwidth}
    \centering
    \caption{3B SP FP8 model activation distributions.}
    \includegraphics[width=0.9\textwidth]{figures/3b_basefp8.png}
    \label{fig:3b-basefp8-dist}
    \end{subfigure}
    %%%%%%%%%%%%%%%

    %%%%%%%%%%%%%%%
    \begin{subfigure}[t]{\textwidth}
    \centering
    \caption{3B µS FP8 model activation distributions.}
    \includegraphics[width=0.9\textwidth]{figures/3b_musfp8.png}
    \label{fig:3b-musfp8-dist}
    \end{subfigure}
    %%%%%%%%%%%%%%%

    \end{figure*}
    \clearpage
}
    
\afterpage{
    \begin{figure*}[p]
    \ContinuedFloat
    \centering
    %%%%%%%%%%%%%%%
    \begin{subfigure}[t]{\textwidth}
    \centering
    \caption{7B SP FP8 model activation distributions.}
    \includegraphics[width=0.9\textwidth]{figures/7b_basefp8.png}
    \label{fig:7b-basefp8-dist}
    \end{subfigure}
    %%%%%%%%%%%%%%%

    %%%%%%%%%%%%%%%
    \begin{subfigure}[t]{\textwidth}
    \centering
    \caption{7B µS FP8 model activation distributions.}
    \includegraphics[width=0.9\textwidth]{figures/7b_musfp8.png}
    \label{fig:7b-musfp8-dist}
    \end{subfigure}
    %%%%%%%%%%%%%%%

    %%%%%%%%%%%%%%%
    \begin{subfigure}[t]{\textwidth}
    \centering
    \caption{13B SP FP8 model activation distributions.}
    \includegraphics[width=0.9\textwidth]{figures/13b_basefp8.png}
    \label{fig:13b-basefp8-dist}
    \end{subfigure}
    %%%%%%%%%%%%%%%

    %%%%%%%%%%%%%%%
    \begin{subfigure}[t]{\textwidth}
    \centering
    \caption{13B µS FP8 model activation distributions.}
    \includegraphics[width=0.9\textwidth]{figures/13b_musfp8.png}
    \label{fig:13b-musfp8-dist}
    \end{subfigure}
    %%%%%%%%%%%%%%%
    
    \caption{\textbf{Activation distributions of µS and SP models.} Activation distributions for attention and FFN block inputs and outputs are shown for 1B, 3B, 7B, and 13B FP8 models at every 10th layer. µS models lack the notable right tail of activation outliers in block inputs that SP models suffer from. This may make them easier to quantize.}
    \label{activation-distributions}

    \end{figure*}
    \clearpage
}

\end{document}
