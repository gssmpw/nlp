% \section{Proposed framework} \label{sec:framework}
\section{Three Orders of Welfare-Maximizing ML} \label{sec:orders}

%========================================================================
\begin{figure*}[t!]
\centering
\includegraphics[width=0.97\textwidth]{illust.v5-crop.pdf}
% \vspace{-2mm}
\caption{
\textbf{Proposed framework:} The three orders of welfare-maximizing machine learning.
}
\label{fig:illust}
\end{figure*}
%========================================================================


% We are now ready to present our proposed framework of welfare-maximizing machine learning and its three orders.

% \paragraph{From accuracy to welfare.}
% \paragraph{From maximizing accuracy to maximizing welfare.}

Our framework presents a hierarchy of problem formulations for welfare-maximizing learning,
organized into three `orders' of gradually increasing complexity.
The orders are built in a bottom-up fashion:
The lowest order (Order 0)
coincides with the standard objective of maximizing predictive accuracy,
but provides a novel welfare perspective suited for social tasks.
Each higher order then builds on and generalizes the one below it,
this by adding to the objective another layer of economic complexity:
first focusing on system decisions (Order 1),
and then on user choices (Order 2).
As such, our framework is based on accuracy maximization at its core,
but enables---and in fact requires---to explicitly model resources, allocations, and agency.
It also requires to specify the role predictions play in shaping social outcomes.
An illustration of the framework and its orders is given in Fig.~\ref{fig:illust}.
\squeeze

We begin with a welfare interpretation of supervised learning,
and then proceed to discuss the three framework orders.
Further prospects and challenges are discussed in Appx.~\ref{appx:beyond}.
\squeeze

\todo{agency: orders 0+1 - info/report, order 2 - actions}

% \squeeze
% \begin{itemize}[leftmargin=1em,topsep=0em,itemsep=0.3em]
% \item
% \textbf{\underline{Order 0}: Accuracy as a resource.}
% When prediction is provided as a service \emph{for humans},
% and when those humans benefit from accurate predictions,
% accuracy itself becomes a scarce resource
% that requires careful allocation.
% Examples include
% medical diagnosis, financial risk prediction, career advice,
% and personalized recommendations.

% \item
% \textbf{\underline{Order 1}: Predictions as decision aids}.
% Predictions can help inform decisions made \emph{about us}.
% But when decisions must be made collectively about a group of people, 
% this often entails constraints on individual decisions.
% Examples include resume screening, school admissions,
% loans approval, insurance claims, and medical program eligibility.
% \squeeze

% \item
% \textbf{\underline{Order 2}: Predictions that empower choices.}
% A primary use of predictions in social settings is to facilitate and improve the choices made \emph{by us}.
% This makes us, in some sense, a finite resource over which platforms,
% service providers, sellers, and content creators contend.
% Examples include recommendation systems,
% online market platforms,
% and job applications and hiring.

% \end{itemize}


\paragraph{Supervised learning from a welfare perspective.}
Given labeled training data of pairs $(x,y)$ sampled iid from an unknown distribution $\dist$,
the goal in supervised learning is to find a function $h$ from a chosen class $H$ whose predictions $\hat{y}=h(x)$ are accurate
on future examples in expectation:
\begin{equation}
\label{eq:accuracy_objective}
\argmax\nolimits_{h \in H} \expect{\dist}{\one{y = h(x)}}
\end{equation}
Since in our setting examples $(x,y)$ are associated with individuals
% \hf{May need to think a bit about terminology, shall we call user or agent?}
in the population $\dist$,
then from a welfare perspective,
we can think of Eq.~\eqref{eq:accuracy_objective}
as prescribing a particular social welfare function---%
a special case of Eq.~\eqref{eq:welfare}---%
% then from the welfare perspective of Eq.~\eqref{eq:welfare}, 
% what Eq.~\eqref{eq:accuracy_objective} is advancing is a welfare function
in which:
% \niradd{
(i) utility to users derives from accurate predictions,
(ii) all users share the same utility function,
$u(x,y;\policy_h) = \one{y = h(x)}$,
(iii) possible outcomes include correct and incorrect predictions,
% and so depend directly on the classifier, ,
hence (iv) the policy is degenerate, $u(x,y;\policy_h)=u(x,y;h)$),
and finally (v) weights are uniform, $w(x) \equiv 1$.
% }
% (i) all individuals are assumed to share the same utility $u(x,y,\policy) = \one{y = h(x)}$; (ii) each individual's utility is independent of policy $\pi$ or outcomes,
% % $u(x)=\one{y = h(x)}$, 
% and (iii) weights $w(x) \equiv 1$ are uniform. 
\squeeze


\paragraph{From accuracy to welfare maximization.}
Despite an apparent `neutrality', 
the above reveals that
accuracy maximization in fact makes an (implicit)
statement about social preferences.
This entails a particular notion of equity---%
one that derives operationally from the predictive task at hand,
rather than from a planned or designated social goal.
But learning is of course not bound to maximizing objectives only of the form of Eq.~\eqref{eq:accuracy_objective}.
If our true goal is to maximize welfare, then we should modify the objective to support this goal.
For example, if users differ in how much they benefit from accurate predictions,
then we can plug in an appropriate utility function $u$;
or if there are exogenous constraints on predictions,
then we can express these via the policy $\policy_h$.
Such steps are certainly possible, but introduce challenges beyond those
found in standard machine learning tasks.
% The three orders of our framework, presented next,
Towards this, we pave a path that
gradually transforms Eq.~\eqref{eq:accuracy_objective} to support increasingly complex economic considerations.
% \squeeze


% This reveals two limitations of applying conventional learning tools in societal settings.
% First, despite an apparent `neutrality', 
% accuracy maximization in fact makes an (implicit)
% statement about social preferences.
% This entails a particular notion of equity---%
% one that derives operationally from the predictive task at hand,
% rather than from a planned or designated social goal.
% % easier to pred are up-weighted
% Second, while useful for common prediction tasks over `static' inputs 
% (e.g., object recognition in images),
% accuracy alone
% % \hfr{%
% % While this classic formulation is natural for ML tasks such as object recognitions, it 
% fails to capture the `active' nature of users:
% their autonomy, preferences, and interactions.
% This means that even if maximizing the objective in
% Eq.~\eqref{eq:accuracy_objective}
% is what we want,
% it will not necessarily be what we will get.
% % This renders it inadequate for guaranteeing promoting welfare.
% % welfare-maximizing machine learning. 
% % }

% \paragraph{From accuracy to welfare maximization.}
% Learning is of course not bound to maximizing objectives only of the form of Eq.~\eqref{eq:accuracy_objective}.
% If our true goal is to maximize welfare, then we should modify the objective to support this goal.
% For example, if users differ in how much they benefit from accurate predictions,
% then we can plug in an appropriate utility function $u$;
% or if there are exogenous constraints on predictions,
% then we can express these via the policy $\policy_h$.
% Such steps are certainly possible, but introduce challenges beyond those
% found in standard machine learning tasks.
% The three orders of our framework, presented next,
% gradually transform Eq.~\eqref{eq:accuracy_objective} to support increasingly complex economic settings.
% \squeeze

% \hfr{In this section, we describe a general   framework with three orders of increasing richness that significantly generalizes the above classic framework in order to cast various learning tasks in societal and economic setups. Most learning tasks in these setups exhibit challenges of these different orders. For concreteness, we illustrate these orders using an example of making purchase recommendations on digital markets such as Amazon, Wayfair, etc.; more application examples can be found in Section \ref{sec:agenda} when we elaborate on research tasks.   }


% \hf{I ended up using "making purchase recommendations on digital markets" as an illustrative domain, since I think job hiring via ML is a less mature application at this moment (most hires are arguably still through human decisions). }

\priority{%
\todo{consider adding table of orders with resources, agency (want, know, do), decisions, policy, outcomes, solution concepts (stack, stack-nash, nash-stack), etc.}
}

% \todo{add notation - user value/utility $v_h(x)$ ($=v(x;\yhat)$?)}  


\subsection{Order 0: Allocating Accuracy} \label{sec:order0}
% Accuracy is beneficial to users in any setting where predictions are given to them as a service.
% \hfr{
Accuracy will be beneficial to users in any a platform or service
% that are driven by predictions; 
that relies on predictions;
consider recommendation systems, online market platforms,
financial aid tools, or diagnistic health services.
% examples include various prediction-driven recommendations for products, contents, financial service, etc.
% } 
% ; consider \todo{examples}.
We argue that in such cases,
and as long as
learning is prone to some level of error
% perfect accuracy is unattainable
(which is plausible), then
\emph{accuracy itself is a limited resource}---%
simply because not all users can obtain
the same level of accuracy.%
\extended{But since there can be many models with similarly high overall accuracy but that differ in who they are accurate on,
these create points along the \emph{Pareto frontier}---since by definition, 
any deviation that increases accuracy for some users will necessarily come at the cost of reduced accuracy for others.}
% \hfr{the same level of  prediction accuracies  --- on the Pareto frontier of the (imperfect) prediction accuracy for different users, the accuracy increase for one group of users   comes at the cost of decreasing the accuracy   of another group.} 
% accurate predictions.
% In this sense,
Thus, the choice of classifier becomes, in effect, a decision on which users will be `allocated' more accurate predictions, and which less. %will not.
This then begs the question:
who \emph{should} be allocated an accurate prediction?
% \hf{an accurate, or a more accurate? I guess no one will be perfectly accurate?} 
% \nir{i think this is a question of classification vs regression. the loss we considered so far is the 0-1 loss, so the discussion is geared towards classification. but i guess the point holds also more generally for regression.}
One answer is users who most need it, or who benefit mostly.
In the reasonable case where there is natural variation in individual utilities,
we can to optimize a utility-weighted accuracy objective:
% \hf{a potential issue is that this cannot capture $v$'s dependence on $y$...}
% \squeeze
\begin{equation}
\label{eq:order0_objective}
\argmax\nolimits_{h \in H} \expect{\dist}{u(x,y; \hat{y}) \cdotp \one{y = h(x)}}
\end{equation}
% \nir{should $u$ depend also on $y$? otherwise it can't be a function of the prediction being correct}
where $u(x,y; \hat{y})$ is the utility for user $x$ given prediction $\hat{y}$.%
\priority{
\footnote{\hfr{This modeling assumes that $x, \hat{y}$ fully captures the individual's preference. More generally, if there are factors that cannot be captured by these two, a more general model of individual's utility function  $u(x; \hat{y}) + \epsilon(x)$ for some noise term $\epsilon$  (possibly depending on $x$) that captures hidden factors' effects.} }
}
%\hf{plan is to change $v(x)$ to $u(x; \hat{y})$. More generally, we can model $u$ as $u(x; \hat{y}) + \epsilon(x)$ to capture un-modeled components. }
This transforms Eq.~\eqref{eq:accuracy_objective} into an explicit utilitarian welfare objective in which individualized utilities derive from prediction correctness:
$u(x;\yhat)$ if $\yhat=y$, and zero if not.
Conditioning $u$ also on predictions $\yhat$ enables to express utilities that depend on predictions themselves (beyond their correctness);
for example, if users benefit more from positive predictions,
as in loans, hiring, admissions, etc.
% and takes one step towards Eq.~\eqref{eq:welfare} by incorporating individual importance weights.

\paragraph{Agency.}
While apparently simple, optimizing Eq.~\eqref{eq:order0_objective} poses significant challenges
\emph{once we accept that users have agency}.
If users seek to promote their own interests, and have control of information that is important to the system, then their choices regarding how and what to report can significantly affect learning outcomes.
For example, consider that if we simply ask people to report their utilities,
then each individual will be incentivized to report the highest possible value,
making all reports completely uninformative.
Another example is that users may be prone to manipulate their features if this helps them secure favorable predictions,
as is the case of strategic classification \citep{hardt2016strategic}.
% Another example is that, even if we have $v$,
% users can still misreport or manipulate their $x$ to influence predictions in their favor \tocitec{sc?}.
% \blue{We discuss some ideas for how such problems can be reconciled 
% in Sec.~\toref.}

\paragraph{Challenge: Optimizing welfare objectives.}
Maximizing welfare through learning presents new algorithmic challenges.
One task is to identify learners that optimize a given social welfare function---%
intrinsically, a question of identifying a desirable point on the Pareto front.
Another task is to generate \emph{all} points on the frontier \citep[see, e.g.,][]{navon2021learning}.
One possible approach is to cast
Eq.~\eqref{eq:order0_objective} as a problem of distribution shift.
Here shift can stem from several factors, including
the welfare function weights,
user utilities, and user inputs.
But agency means that utilities can be misreported and features manipulated;
as a result, the way in which the distribution shifts becomes
\emph{dependent on the choice of classifier},
indirectly through how users respond.
This makes Eq.~\eqref{eq:order0_objective} a challenging instance of
\emph{model-dependent distribution shift} \citep{drusvyatskiy2023stochastic}.
Here the interaction between the system and its users 
can be modeled as a Stackelberg game \citep{chen2020learning}.
% Some advances have been made on this front,
This perspective has been useful for questions on learnability,
e.g. via generalization bounds
that rely on \emph{strategic VC} analysis
\citep{zhang2021incentive,sundaram2023pac}
or that accommodate feature-dependent utilities \citep{pardeshi2024learning}.
Nonetheless, many important open questions remain.
\squeeze

% Even if we have faithful access to user utilities,
% the algorithmic question of how to optimize welfare in practice remains.




% \paragraph{Challenge \#2: Learnability and generalization.}


% \paragraph{Challenge \#1: the optimization problem -- which learning algorithm optimizes a given welfare objective?} It is widely observed that there often exist multiple predictors that all achieve optimal or close-to-optimal accuracy. Indeed, this phenomenon has been widely known as \emph{model multiplicity} \cite{black2022model,d2022underspecification} or ``Rashomon effect'' \cite{semenova2022existence,rudin2024amazing},  coined by Leo Breiman. \cite{d2022underspecification} specifically attributes this phenomenon to underspecification of the learning pipeline.
% \nir{i added model multiplicity/underspecification to the end of sec 2, so probably safe to reference these ideas more loosely here}
% A natural question thus is, among the predictors that have high accuracies, which one would maximize a given welfare function? This new challenge  is intrinsically about identifying algorithms that  achieve  the Pareto frontier   concerning various parties' utilities, since the optimal solution under any welfare function must be at the Pareto frontier and conversely, any point at the frontier   must correspond to the optimal solution of some welfare functions.   %  by allowing varied  utility functions, the performance distinction will almost necessarily show up. This hence raises the important machine learning question  of studying which predictor is  best-suited for which welfare objective  and, sometimes, the need of designing new predictors that are optimal under new objectives.  

% \paragraph{Challenge 2: the statistical question -- how does agency change learnability?} The second fundamental question is how classic learnability theory established based on accuracy maximization (or risk minimization) will change under the new welfare objective. Two key factors will affect the development of a new theory of learnability for welfare-maximizing ML: (1) users' agency behaviors such as strategically disguising or altering their private data from the learner; (2) the shift from equal-weights of each data point  to varied and possibly feature-dependent utilities in the learning objective. Recent study of \cite{sundaram2023pac} developed a generalized theory for Probabilistic Approximately Correct (PAC) learning  to address factor (1) assuming known user utility function, whereas \cite{pardeshi2024learning} addresses factor (2) for a particular class of welfare functions called weighted power mean functions. More general welfare functions as well as the combined situation with presence of both factor (1) and (2) have not been examined thus far. Among others, one factor that  will affect learning is the variance among different users' utility values. \hf{add some citations here about similar result on effective sample size that depends on the variance of propensity weights.  }  This also raises an interesting societal question of how inequality could affect welfare-maximizing learning. 



% \paragraph{Challenge \#1: the optimization problem -- which learning algorithm optimizes a given welfare objective?} It is widely observed that there often exist multiple predictors that all achieve optimal or close-to-optimal accuracy. Indeed, this phenomenon has been widely known as \emph{model multiplicity} \cite{black2022model,d2022underspecification} or ``Rashomon effect'' \cite{semenova2022existence,rudin2024amazing},  coined by Leo Breiman. \cite{d2022underspecification} specifically attributes this phenomenon to underspecification of the learning pipeline.
% \nir{i added model multiplicity/underspecification to the end of sec 2, so probably safe to reference these ideas more loosely here}
% A natural question thus is, among the predictors that have high accuracies, which one would maximize a given welfare function? This new challenge  is intrinsically about identifying algorithms that  achieve  the Pareto frontier   concerning various parties' utilities, since the optimal solution under any welfare function must be at the Pareto frontier and conversely, any point at the frontier   must correspond to the optimal solution of some welfare functions.   %  by allowing varied  utility functions, the performance distinction will almost necessarily show up. This hence raises the important machine learning question  of studying which predictor is  best-suited for which welfare objective  and, sometimes, the need of designing new predictors that are optimal under new objectives.  

% \paragraph{Challenge 2: the statistical question -- how does agency change learnability?} The second fundamental question is how classic learnability theory established based on accuracy maximization (or risk minimization) will change under the new welfare objective. Two key factors will affect the development of a new theory of learnability for welfare-maximizing ML: (1) users' agency behaviors such as strategically disguising or altering their private data from the learner; (2) the shift from equal-weights of each data point  to varied and possibly feature-dependent utilities in the learning objective. Recent study of \cite{sundaram2023pac} developed a generalized theory for Probabilistic Approximately Correct (PAC) learning  to address factor (1) assuming known user utility function, whereas \cite{pardeshi2024learning} addresses factor (2) for a particular class of welfare functions called weighted power mean functions. More general welfare functions as well as the combined situation with presence of both factor (1) and (2) have not been examined thus far. Among others, one factor that  will affect learning is the variance among different users' utility values. \hf{add some citations here about similar result on effective sample size that depends on the variance of propensity weights.  }  This also raises an interesting societal question of how inequality could affect welfare-maximizing learning. 



% \hf{Does agency only mean response and report? Agency inlcudes   (want, know, do):  information witheheld, action taken, welfare/utility (including potential  externality to capture fairness, envy)   }
 
% This is a direct result of the fact that \emph{users have agency}---a notion which is typically conveniently abstracted away.
% One possible solution is to augment the learning objective with an appropriate mechanism for eliciting true valuations.
% We present this idea and other alternatives in Sec.~\ref{sec:order1}.
% % \squeeze
 
\subsection{Order 1: Incorporating System Decisions} \label{sec:order1}
Generally, predictions are useful for the system if they aid in making better decisions about uncertain outcomes. 
Since decisions in social settings often directly prescribe how to allocate limited resources to users,
a useful next step is to encode them explicitly into the objective.
We formalize this idea by modeling a system that makes decisions about users
(e.g., who to hire)
through predictions (e.g., resume screening).
Consider a user $x$ with label $y$, and denote by $a$ the action the system takes (e.g., hire or not).
Let $\reward(a,y)$ be the reward for the system on this user given action $a$
% \hf{maybe unify the reference to "decisions". }
(e.g., the quality of the candidate, if hired).
The direct dependence of $\reward$ on $y$ means that if we know $y$, then we can write the optimal action as $a^*=\policy(y)$ for some policy $\policy$.
Since $y$ is generally unknown, the common approach is to replace $y$ in $\policy$ with a prediction $\yhat = h(x)$,
denoted $a = \policy(h(x)) = \policy_h(x)$,
in hopes that better predictions translate to better decisions. %
\priority{\footnote{An alternative approach is to directly learn the mapping from $x$ to $a$, which also known as decision-focused learning \citep{wilder2019melding,kotary2021end,dontidc3}. However, this approach has thus far been mostly studying learning optimization decisions without any consideration of agency and welfare effects.}}
Such policies,
referred to as \emph{prediction policies} \citep{kleinberg2015prediction},
are appropriate when the uncertainty in $y$ is a stronger factor for outcomes than potential causal effects (we discuss this distinction further in Appendix~\ref{appx:causal}).
In this setting, 
choosing to work with prediction policies
gives reason for the system to optimize $h$ for accuracy (Eq.~\eqref{eq:accuracy_objective}).
\squeeze
% Such policies are referred to as a .
% \hf{emphasize this more, as this is subtle yet importantly different from our method. } 

% even though deeper thinking reveals that this may not be the right thing to proceed.    

% \hf{need to explain why we separate $h, \pi$ though they could have been comibned. Causal effect (pi may affect $y$), and practically done like this }

\paragraph{From predictions to actions.}
We consider cases where there is a global restriction on the set of all actions $a$. These can express, for example,
a limited number of available jobs (via cardinality constraints),
% ($\sum_i a_i \le k, \, a_i \in \{0,1\}$),
a total sum of funds that a bank can lend (knapsack constraints),
% ($\sum_i a_i \le K, \, a_i \in \R_+$),
regulation on the amount of financial risk an insurer can take (bounded expected risk),
or a limit on the number of posts a social platform can block to still enable free speech (lower-bounded rates).
% or a lower bound on the number of posts a social platform must allow to enable free speech ($\sum_i a_i \ge k$).
Given a set $A$ of feasible actions,
the objective can be written as:
\begin{equation}
\label{eq:order1_objective}
\argmax\nolimits_{h \in H} \expect{\dist}{\reward(\policy_h(x),y)}
\quad \text{s.t.} \quad
\policy_h(\dist) \in A
\end{equation}
where $\policy_h(\dist)$ is the set of actions over the entire population.  



\paragraph{Agency.}
Machine learning has many tools for coping with constraints.
For example, if there is a quota on the number of available interview slots,
then top-$k$ classification \citep[e.g.][]{lapin2015top,petersen2022differentiable} or ranking \citep[e.g.][]{cao2007learning}
% or learning-to-rank algorithms 
seem like adequate solutions.
The crux, however, is that such methods do not account for agency:
since users seek to be `allocated' (e.g., get the interviews),
they will likely try to present themselves as relevant
(e.g. by tweaking their resume).
% widely recognized and studied in economics---but not much in machine learning.
% But has not been carefully investigated in learning algorithm design.  
% In effect, user agency combined with limited resources creates competition between users---which learning must account for in order to achieve its goals.
% In Sec.~\toref\ we discuss how such challenges manifest in the learning objective, and what can be done about it.
A key point is that since users contend over resources,
this introduces dependencies into their actions and subsequent outcomes.
Such behaviors are ubiquitous in economics
(e.g., costly signaling, adverse selection),
but have been underexplored in machine learning.
% Such 
% research competition
% behavior, known as ``adverse selection'' \cite{cohen2010testing,greenwald2018adverse}, 
% in machine learning, it has yet to receive the appropriate attention it requires.
% To achieve its goals, learning must account for these.
\squeeze

% \nir{revise:}
% A key point is that in either case,
% users will have incentive to steer outcomes in their favor,
% which they can achieve for example by manipulating their features $x$ to influence predictions \tocitec{sc?}.
% \blue{In Sec.~\ref{sec:order1} we discuss the challenges that arise when
% such behavior interacts with the limitation on resources,
% and how to approach this.} 

% - top-k loss and why it doesn't suffice (but still helps and should be built on)

\paragraph{Social welfare.}
In terms of welfare outcomes,
we make a distinction between two settings of interest:
\begin{itemize}[leftmargin=1em,topsep=0em,itemsep=0.2em]
\item
An \emph{aligned} setting in which $\reward=\welf$,
and so the interests of the system align with societal preferences,
e.g., as for government and nonprofit organizations. % organizations, and international agencies).
Note Eq.~\eqref{eq:order0_objective} becomes a special case of
Eq.~\eqref{eq:order1_objective} when utility derives from accuracy
and $A$ does not impose restrictions. 

\item
A \emph{misaligned} setting in which $\reward$ can be at odds with welfare,
for example if it concerns revenue or user engagement,
as is more common in commercial settings.
Here a social planner is needed to incentivize the learner to account for welfare outcomes, as we detail in Sec.~\ref{sec:regulation}.
\end{itemize}


% - accuracy -- implicit? as constraint? weighted in objective? \\
% - users can change x \\
% - welfare, valuations


% \subsection{Challenges at Order 1 } 






% Under Order 1, the learner looks to optimize   decisions, often subject to resource constraints. Learning such constrained decisions   differs from   directly learning the welfare function itself as at Order 0.  While each individual have similar importance in the welfare function, the sensitivity of  the welfare-maximizing   decision boundary to each individual can differ dramatically. Consider the , the real problem here is actually not just an accuracy maximization problem as --- instead, it is actually a problem at Order 1.\footnote{In fact, it could even be placed to Order 2 since students in turn have their own choices after getting offers. However, such choices have been less considered during university admissions due to relatively stable acceptance rate.}  This is because each university has its own admission limit each year, hence their true task is not to assign an uncorrelated binary label to acceptance/reject to every applicant as modeled by strategic classification, but rather to pick the best possible $k$ candidates for some admission quota $k$.  Hence as widely observed in practice, most of the effort spent on  deciding admissions are often those boundary applicants, to whom the optimal decisions are significantly more sensitive compared applicants who are at very top or bottom. Such  unequal influence of individuals to the optimal decision also show up in other applications such as approving loan applicants where the decisions are even more complex since it is not only about approving a loan or not but rather about how much loan amount (possibly $0$, meaning declining) to approve for each individual, typically  subject to total  available loan amount. Very little previous studies have gone beyond  classification or regression to study such Order-1   questions of optimally learning the welfare-maximizing decisions.   



% \paragraph{Challenge \#1: Decisions under limited resources correlate user behavior.}  
\paragraph{Challenge \#1: Decisions and externalities.}  
At Order 0, user responses are generally made independently,
and so the learning objective decomposes over examples.%
\footnote{Note this is a key assumption in standard strategic classification.}
But once the system makes decisions under constrained resources,
user behavior can become intricately dependent.
Consider for example admissions under a limited quota:
here applicants cannot simply pass a fixed acceptance threshold,
but instead, must surpass other applicants---i.e., the bar adjusts according to competition.
The tension for both decision-makers and applicants tends
to focus on `marginal' students;
complex gaming behavior among such candidates is well-documented in 
the education literature \citep{bound2009playing}.
In economics, inter-user dependencies are known as \emph{externalities}.
One way to model how learning creates externalities and coordinates user behavior
is by casting the problem as a Stackelbeg-Nash game \citep{liu1998stackelberg},
where the system plays first, and users respond collectively by playing an induced simultaneous game (with externalities).
Not much work has studied
learning in this challenging setting.
% \squeeze
\priority{scext, scgnn; recdiv?}

% \paragraph{Challenge \#1: Complex inter-dependent agency behaviors under resource-constrained decisions.}  
% To see how learning differs at Order 0 and Order 1, let us illustrate with the problem of learning university
% admission decisions, which is the introductory example that
% populates the study of strategic classification \cite{hardt2016strategic}. Previous works treat it as an Order-0 problem, under which there is no admission limit hence each individual responds independently to a university's admission rules. However, this problem is in fact at  Order 1 since admissions in practice are constrained decisions with  a limited quota; consequently, a student's strategic behavior is   not  simply to cross the classification boundary as modeled  in previous works, but rather to surpass other similar peers in order to get admitted, introducing \emph{externalities} among students, particularly those at admission thresholds known as ``marginal students'' in the literature (complicate gaming among marginal students are well-documented in education research literature  \cite{bound2009playing}). Such inter-dependent agent actions lead to a  \emph{Stackelberg-Nash} style game.  Similar issues happen in loan applications where limited loan opportunities also introduce externality among applicants who   compete for limited loan resources while not merely for passing the bar. Little previous studies have considered  such Order-1   questions of  learning the welfare-maximizing decisions.   

% bound2009playing


\paragraph{Challenge \#2:  Misaligned system and user objectives.}  
As we note, generally we cannot expect the reward for the system to align with user interests.
If no external forces intervene, 
% then the system faces a
% Stackelberg-Nash game with a single leader and many followers.
then the challenge is to 
% use samples of past system-user interactions to
learn a decision policy that is effective at the Stackelberg-Nash equilibrium.
This necessitates learning objectives that can express and anticipate
how users will collectively respond to a deployed model.
Contrarily, if there is regulation (e.g., on welfare or risk),
then the system's decision space becomes much more limited.
An interesting question then is \emph{how} to align incentives through regulation.
For example, would requiring the system to optimize a weighted sum of accuracy and welfare help to promote equity? And if so, which weights, and by what means?
\squeeze

% \paragraph{Challenge \#2:  Misaligned system and user objectives.}  
% %The third fundamental challenge that exhibits under Order 1 is the mis-aligned objectives between learner and users. While in some situations, agents and the system have aligned objectives (e.g., high education potential as in school admission), in others their objectives can be different and sometimes conflicting. For instance, during loan application, the system   tends to maximize profit subject to loan availability, whereas applicants maximizes the amount of approved loan. 
% There are two situations concerning misaligned objectives.
% First, if no external force intervenes in the system, the system then faces a Stackelberg-Nash game with a single leader and many followers. The challenge is to effectively  learn the optimal system equilibrium strategy from sampled system-user interactions. Second, if there are regulations that impose risk   and welfare controls, this often helps to mitigate the conflicting system-individual interest. This   challenges   the social planner to learn  effective regulation policies.  For example, would enforcing the system to optimize a weighted sum of revenue and welfare help to achieve better equity, and if so which weighing scheme is the best?  

 

 
\subsection{Order 2: Enabling User Choices} \label{sec:order2}
Even if a system has the capacity to make decisions about users,
% outcomes are typically also the product of user choices.
often those very users will also have some say regarding final outcomes.
% In fact, 
% Even in straightforward 
For example, even in hiring and admissions,
candidates much first \emph{choose} to apply, and if selected, \emph{choose} to accept.
Thus, both reward for the system and utility to users depend on outcomes that result from the interplay of system decisions and user choices.
Similarly to system decisions, we will model user choices as also depending on predictions, or more generally on the learned model $h$.
Let $\choice_{\policy_h}(x)$ be the choice of user $x$ in response to $h$.
Incorporating user choices into the system's objective gives:
\squeeze
% \begin{align}
% \label{eq:order2_objective}
% \argmax_{h \in H} & \expect{\dist}{\reward(\policy_h(x),\choice_h(x), y)} \\
% \text{s.t.} 
% & \quad \choice_h(x) = \red{\argmax v?} \,\,\, \forall x \nonumber \\
% & \quad \policy_h(\dist) \in A \nonumber
% \end{align}
\begin{equation}
\label{eq:order2_objective}
\argmax_{h \in H} \expect{\dist}{\reward(\policy_h(x),\choice_{\policy_h}(x),y)}
\quad \text{s.t.} \quad
\policy_h(\dist) \in A
\end{equation}
where $\reward$ now depends jointly on system decisions $\policy_h$
and user choices $\choice_{\policy_h}$ through the learned predictor $h$.

\paragraph{Agency and resources.}
When users make choices, 
this makes them a part of the allocation process---but also the reason for scarcity.
Here we outline three scenarios of interest:
\squeeze
\squeeze
\begin{itemize}[leftmargin=1em,topsep=0em,itemsep=0.2em]
\item 
\textbf{Self-selection:}
For most systems that makes decisions about users,
those users must first choose to join the system;
consider hiring, admissions, medical programs, educational aid,
and welfare benefits.
In econometrics, the choice of users to participate (or not)
based on the utility they expect to obtain
is known as \emph{self-selection} \citep{roy1951some}.
Since self-selection in aggregate determines the user population,
once user decisions depend on the learned model,
learning obtains the power to shape the population's composition---%
a power which should not be taken lightly.%
\priority{\tocitec{evoML}}
Although self-selection has a long history in economics,
it is only now beginning to draw attention in machine learning research \citep{zhang2021classification,cherapanamjeri2023makes,horowitz2024classification}.

% In some tasks where the system selects users,
% those users must also choose the system.
% Examples include
% hiring, admissions, medical programs, educational aid,
% and welfare benefits,
% in which users must agree to participate, or even apply in the first place.
% Such behavior, known in econometrics as \emph{self-selection} \tocite,
% has so far received little attention in machine learning \tocite.



\item
\textbf{Matching:}
Prime examples in which user choices drive economic outcomes are recommendation systems and online market platforms.
These typically include a single learner whose role is
to match users with relevant items \citep{jagadeesan2023learning};
once matched, it is the choices of users that ultimately determine allocations.
Since matchings are often based on predictions,
their quality determines outcomes for users, suppliers, and the system. % \tocitec{ML}.
This makes them unique in relation to conventional \emph{two-sided markets}  \citep{rochet2003platform}.
In modern human-facing systems, key resources primarily include the attention, time, and budgets of users.
This calls for careful and responsible modeling of user choice behavior
\citep{kleinberg2024challenge}.%
\priority{\citep{saig2023learning}}

%\vspace{-2mm}

\item
\textbf{Competition:}
For many tasks and services of interest, users can now choose between multiple platforms or providers.
This creates competition:
once providers aim to maximize their market share,
users themselves become the scarce resource,
over which providers compete \citep{ben2019regression,jagadeesan2023competition,yao2023bad,yao2024user}.
% A natural extension of self-selection is to allow users to choose among multiple  alternatives, such as between competing platforms or service providers \tocite.
When the benefit of users depends on the quality of predictions,
competition will revolve around which provider learns more accurate models---and for which users.
This creates a novel `market for accuracy',
in which learning can have much impact on efficiency and equity.%
\priority{\tocitec{cmpt}}
% require deliberation.
% \squeeze

% \hf{This case might have multiple providers as well; this will lead to a competitive game. Shall we consider this? Maybe leave it to the last as this is the most complicate. }

% how users make such choices.

% Since the quality of matchings will determine 
% outcomes for users, suppliers, and the system,
% Hence, this requires careful modeling of user behavior.

% The quality of matchings, as well as their diversity,
% are likely to determine outcomes for users, suppliers, and the system.
\squeeze

\end{itemize}

 
% The above suggest that Eq.~\eqref{eq:order2_objective} can benefit from encoding structure in the form of a bi-partite graph between users and service providers (for competition) or producers (for matching). We elaborate on this further in Sec.~\toref.
% This draws a strong connection to the economic literature on \emph{matching markets} \tocite.
 

% - bipartite graphs

\paragraph{Social welfare.}
A canonical property of classic markets is that they coordinate the behavior of many self-interested agents in a way which can naturaly lead to welfare maximization \citep[e.g.,][]{arrow1954existence,shapley1971assignment}.
A key question in our context is whether this emerges also in 
markets where coordination is mediated by predictions
\citep{nahum2024decongestion}.
Given the growing concerns regarding how recommendation systems 
may drive polarization, echo chambers, informational barriers, economic inequity, and unhealthy usage patterns---the answer is likely negative.
Welfare economics may be helpful in posing the question of \emph{why} 
this happens as one of \emph{market failure}.
This provides tools for uncovering the mechanisms underlying failure,
such as negative externalities,
public goods,
information asymmetry,%
\priority{(e.g., moral hazard),}
market power and control (e.g., by monopolies),
or collusion,
as they manifest through learning.%
\priority{This raises interesting questions regarding how these manifest in learning settings \citep{hardt2022performative},
and what can be done about them---e.g., via regulation by a social planner
(see Sec.~\ref{sec:regulation}).}
\squeeze 


% \subsection{Challenges at Order 2 } 
\paragraph{Challenge \#1: Escaping echo chambers.}
Since its early days, % of recommendation systems,
the task of recommending content has been treated as a pure prediction problem.
But accumulating evidence of its likely ill effects
% as to the possible ill affects of this approach
has motivated a search for more viable alternatives.
A major issue is the reinforcing nature of accuracy-driven recommendation:
users that are recommended certain items become `locked in' on similar content
through a positive feedback loop of indefinite
model retraining, choice behavior, and recommendation.
One aspect of this loop that is often overlooked, and relevant to our context,
is that recommendations also incentivize the \emph{creation} of new content
by exposure-seeking creators.
Recommendation is essentially a problem of matching
demand (of users) and supply (of new content) over a bi-partite user-content graph;
thus, if we wish to prevent the formation of echo chambers and filter bubbles,
we should find ways to exploit this structure to promote socially-beneficial outcomes.
Some examples include incentivizing exploration \citep{mansour2020bayesian}
or the creation of diverse content \citep{eilat2023performative,yao2024user},
but many other paths are possible.
\squeeze


% - reinforce behavior: observed data, recommended content, incentivizing creators
% - limited resources
% - bip graph

% - accuracy-first/centric
% - incentivize diversity, exploration



% \paragraph{Challenge 1: escaping from the echo chamber in recommendation learning.}
% A prominent problem at  Order 2 is the  \emph{recommendation system}  (RS).  An RS's  can be viewed as  a \emph{bipartite graph} where one side is the contents and the other side is users (i.e., content consumers).  The system learns the relevance score for each (content, user) pair, based on which the most relevant few contents are  recommended to each user. At a first glance, this may appear  a standard learning problem,  but  the fact that users make selections from limited choices, either due to limited user attention or limited system supplies, causes the issue of echo chamber -- that is, the system learns users' preferences over a limited choices, then repeatedly reinforces this learned preference by repeating similar recommendations, and ultimately stuck within an ``information bubble'' always with similar contents. What's worse, this problematic content consumption behavior is further reinforced by increasing supply of the similar contents, driven by content creators' traffic-seeking behaviors. 
% %give rise to the following important new challenges:  (1) reinforcing users' preferences ; (2) strategic reactions from content creators.  For (1), the system   uses a user's past behavior trajectory to learn new recommendations, which mostly are similar contents, which then reinforces similar user behavior. For (2), traffic-seeking content creators further intensify this issue by constantly creating   contents that were popular in the past. These two factors together builds ``content bubbles'' for users who continuously see very similar contents, leading to the a famousely known echo chamber issue.  This not only leads to unhealth user content consumption behavior but also drive the entire content ecosystem towards high-concentrated content themes with little diversity. 
% Addressing this pressing challenge requires us to go beyond traditional accuracy-first thinking, and design more explorative learning algorithms to account for users' choices and promote content diversity \cite{mansour2020bayesian,eilat2023performative,yao2024user}.  


 
% WE PROBABLY DO NOT HAVE ROOM FOR FOLLOWING POINTS  ANY MORE, BUT IT IS ALSO IMPORTANT. Strategic behavior and stable marketing, this is when one sided supply is limited. so not true for recommendation, but true for job positions like Linkedin (online labor market, upwork)

\paragraph{Challenge \#2: Learning to compete.} 
Once predictions are given as a service,
it is only  natural that multiple providers will compete over who
can give users better predictions.
One example are housing market platforms such as Zillow and Redfin which compete over who
provides better pricing recommendations. % (e.g., Zillow, Redfin).
To some extent, another example are LLMs who compete over better text generation.
Competition is driven by the fact that users are free to choose a provider,
but that choices are costly (e.g., time, premium features, opportunity costs).
Thus, although users gain from accuracy,
optimizing expected accuracy is likely \emph{not} a good strategy for providers.
As a result, welfare could suffer.
% even if overall accuracy is high, 
Welfare in such markets will be high if the market is efficient.
This requires a planner that can steer competing 
learners towards a favorable \emph{Nash-Stackelberg} equilibrium,
where users follow the simultaneous moves of multilpe systems. 



% \paragraph{Challenge 2: learner competitions.}  Our discussions thus far have focused on  a single learner/system. To account for user choices, we need to zoom  out and consider situations with multiple systems; a notable   example is the house hunting applications with competing systems \emph{Zillow} and \emph{Redfine}. Users can choose from these   systems, and users' opportunity costs such as time and payment for premium features induce systems' competition. Users  always prefer higher accuracy, but no single  system  can  achieve the highest accuracy across the entire user population. Hence traditional learning approach that simply maximizes accuracy can fail miserably ---   it may produce a learning algorithm that has \emph{reasonably high accuracy for every user but not the best for any user}. Welfare-maximizing machine learning in such competitive situations   features complex tradeoff between what  model performances are obtainable and what user population on the market needs better accuracy. Zooming out further, from a social planner's perspective, it is valuable to understand what market equilibrium outcome can arrive and  whether the equilibrium will be efficient. Such situation with users responding to multiple  competing systems features a novel \emph{Nash-Stackelberg} style of interactions, featured with  learning algorithms as system strategies. 


% \todo{give examples of matching markets and how and why they need help/prices don't suffice?}

% - we know it doesn't in recsys \\
% - as market failure \\
% - monopolies and market power \\
% - matching markets - known to be `different' \\
% - things to do about it

\extended{%
\nir{most applications can fit all three orders. [think of example: only predictions, +decisions, +choices; maybe hiring].
it is therefore a question of how and to what resolution we want to model the setting, and what aspects are most significant.
actually, a problem with the field of recsys is that they've been thinking about it only as order 0 (or less), but we need to think about it as order 2 for things to work well in terms of welfare.}

% If users gain from accuracy, and if they \emph{can exercise their agency},
% then each user will likely to act to secure accurate predictions for herself.

% - learning affects behavior affects learning outcomes
% - outcomes for users become dependent through model
% - replace uniform $u$ with true valuations $v$

\blue{%
\underline{checklist}: \\
- resources \\
- policy \\
- choices \\
- agency, behavior \\
- utility
}
}

% \blue{%
% \paragraph{Hierarchy.}
% Eq.~\eqref{eq:order2_objective} reduces to Eq.~\eqref{eq:order1_objective}
% when $r=\welf$ (utilitarian),
% utility derives from accuracy,
% and $A$ does not impose restrictions. 
% }

