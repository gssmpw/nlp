
% %========================================================================
% \section{Related fields}
% - strategic classification \\
% - performative prediction \\
% - algorithms with predictions \\
% - decision-focused learning \\
% - collaborative learning (eg., Unravelling in Collaborative Learning)

% % \section{Relation to fairness}
% % \todo{...}

% % -- Fair Classification and Social Welfare / hu, chen [FAT 2020]
% % -- Fairness behind a veil of ignorance: A welfare analysis for automated decision making / heidari, ferrari, gummadi, kraus [NeirIPS 2018]

% % - replace `just cuz' reasons for social good (eg fairness) with more grounded (econ) reasons/forces



%========================================================================
\extended{
\section{Social welfare functions}
% - types \\
% - examples of use cases

everal Social-Welfare functions (SWf) can be considered depending on the system's preferred values. One function in the paper captures a utilitarian approach focusing on the community's overall welfare with an equal weight assigned to all individuals. 

\[SW = \sum_{i \in [n]} \frac{1}{n} u_i\]

An egalitarian approach would focus on equity rather than only on the overall welfare. In this case, we'll use the geometric mean of utilities to calculate the SW. This prevents cases where some individuals might have a significantly higher welfare than others. The geometric mean forces the different values to have a smaller dispersion than the utilitarian case.

\[SW = \left( \Pi_{i \in [n]}  u_i \right)^\frac{1}{n}\]
 

An additional interesting case follows from the Rawlsian theory of justice, where the system's main concern is maximizing the utility of the worst-off individual. However, note that this approach does not oppose improving welfare across all ranks of society as long as we focus on the less advantaged. This introduces a delicate trade-off between improving the overall welfare and the welfare of the least advantaged allowing some flexibility in the choice of the SWf. We'll give two examples of suitable SWf:

The first uses the weighted SWf function depicted in the paper and gives lower weight to those better-off. If we take some $\alpha \in (0,1)$ then we can formulate the function as follows:

\[SW = \sum_{i \in [n]} \alpha^{i} \frac{1}{n} u_{(i)}\]

where $u_{(i)}$ is the $i_{th}$ lowest utility. Here, the utility of individuals is diminished as their rank improves. 

A different function might consider the utilitarian function with an addition of a penalty calculated according to some inequality measure, such as the range between the highest and lower-ranking utilities. 

 \( SW_{rawls} = \sum_{i \in [n]} \frac{1}{n} u_i - \text{inequality\_measure}(u_1, \cdots, u_n) \) 

A slightly different approach would discard improving the utility of the most advantaged and focus only on maximizing the low-ranked utilities. In this case 

 \( SW = \min\{u_1, u_2, \ldots, u_n\} \) 
}

%========================================================================

\iffalse 
{%
\section{Limitations of current machine learning practice}

We begin by identifying four key reasons that underlie why current machine learning practice is not yet well-suited for the task of learning in social contexts.
These will help establish our case for why welfare can make for an effective solution.


\paragraph{1) Predictions vs. beneficial outcomes.}
Predictions can be useful for informing decisions,
whose outcomes then determine benefits for individuals.
But machine learning trains models to maximize predictive accuracy---%
not to directly promote better 
social outcomes.
% decision-making.
% which lacks to account for their mediating role between inputs and social outcomes.
% ---not the well-being that results from decision outcomes.
It is therefore a-priori unclear why a system with better predictions should necessarily
improve 
% social outcomes and 
the well-being of users.
In economics, the well-being of a population is called \emph{welfare},
and is typically defined as the overall utility that users gain from the allocation of resources or goods.
Welfare is therefore defined directly over \emph{outcomes}, which makes it useful for informing policy.
This suggests that welfare considerations can also help guide learning when it used to inform decision-making.

\todo{illustrate how good predictions can lead to bad outcomes}

% Machine learning is predominantly about maximizing predictive accuracy.
% As aids for decision-making, we can hope that better predictions
% lead to better outcomes for individuals.
% But despite this mediating role between inputs and outcomes,


% Predictions can be useful for informing decisions,
% whose outcomes determine benefits for individuals.
% Predictions therefore mediate between inputs and outcomes---%
% but are not optimized under this broader perspective.
% It is therefore a-priori unclear why better predictions should 
% % improve well-being.
% lead to better social outcomes.
% In economics, the well-being of a population is called \emph{welfare},
% and is typically defined as the overall utility that users gain from the allocation of resources or goods.
% Welfare is defined directly over \emph{outcomes}, and is therefore useful for guiding policy.
% This suggests that welfare considerations can also help guide learning


% Machine learning is predominantly about maximizing predictive accuracy,
% not well-being.
% In economics, the well-being of a population is called \emph{welfare},
% and is typically defined as the overall utility that users gain from the allocation of resources or goods. Welfare is therefore about \emph{outcomes}, and explicitly considers how these translate to benefit for users.
% In contrast, predictions are optimized for accuracy,
% % is only a means to support decision-making,
% and only mediate between inputs and decisions.
% This does not directly account for decision outcomes nor their social implications.

\paragraph{2) Limited resources.}
When inputs represent users,
predictions can be helpful in reducing uncertainty about likely outcomes
for individuals. 
% tell us what is likely to happen 
This can be useful, but has two shortcomings.
First,
predictions apply independently per user;
%  for each individual in the population,
but % it neglects to account for the fact that 
once resources are limited,
outcomes across users necessarily becomes dependent.
For example, two users can be predicted to like, and therefore be recommended, the same vacation home,
but at any given time only one of them can enjoy it.
Thus, under scarcity, outcomes for one user have implications on others.
% ---even if they are predicted to equally enjoy it. 
Second, accurate predictions tell us what \emph{will} happen,
but cannot express what \emph{should} happen.
In contrast, 
welfare specifies well-being as a function of how resources are allocated.
When predictions affect allocations, whether directly or indirectly,
welfare as an objective can help steer learning towards desired social outcomes.
\squeeze
% it not only accounts for scarcity by definition,
% but also tells us what \emph{should} happen.
% if predictions are to be used, then the learning pipeline must be made to...

\todo{be more concrete about what can change: between equally accurate models, some outcomes can be bad -- how? (!)}

\paragraph{3) Human agency.}
Even if we constrain predictions to comply with resource limitations,
this still lacks to consider the fact that
humans will also act to ensure outcomes end up being in their favor.
% how humans contend with scarcity.
For example, consider a firm that is hiring, but has a limited number of interview slots.
We can train a predictive model on past data, and restrict it to output at most $k$ positive predictions.
But once the model is deployed, applicants can turn down offers, modify their applications,
or even decide not to apply in the first place---%
all of which have implications for the firm, as well as for other users.
The key point here is that humans have \emph{agency}:
they act, using their knowledge and beliefs, to promote their own self-interests.
To cope with human agency, we must equip learning with ability to anticipate such behavior.

\paragraph{4) Equity and the distribution of value.}
Learning objectives typically optimize for average outcomes.
But when data points represent people,
this makes a statement about their relative importance.
An unweighted average means all points are given equal weights:
this seems natural, perhaps even `neutral',
% this creates an appearance of neutrality
and can be justified as promoting a certain notion of fairness.
But in effect, this means that outcomes are driven by which points are easier to predict---%
rather than by how  predictions benefit different users.
If we wish to promote favorable outcomes, then this necessarily requires making judgement about which outcomes are more desirable to society.
Welfare economics makes use of \emph{social welfare functions}
which assigns importance weights as a function of individualized outcomes, as
% in which per-example weights are 
determined by a \emph{social planner} who represents societal interests.
This idea can, and should, be easily adapted to learning objectives.
% \looseness=-1

% 1. ml is about predicion, not welfare. considers predictions - not outcomes.
% 2. welfare is about limited resources, requires allocation
% 3. allocation is not just about constraints, needs to account for agency. humans as inputs.
% 4. ml optimizes average (implicitly states that weights should be uniform), should be weighted. need social planner, requires judgements (about outcomes - only then this is meaningful)
}
\fi


%========================================================================

\extended{%
\section{Desiderata} \label{appx:desiderata}
% \subsection{Connections: En route to welfare machine learning}
We believe that machine learning has much to gain from adopting a welfare perspective.
% it is possible to form a tight connection between welfare economics and machine learning.
% Whereas microeconomics typically concerns aggregate outcomes and in expectation,
% welfare economics emphasizes the importance of considering the distribution of outcomes.
% As we know, 
Consider how conventional machine learning is predominantly concerned with maximizing expected accuracy;
from a welfare point of view, this means that all efforts are geared towards efficiency.
% where utility takes the form of per-user accuracy.
But when inputs represent humans, it becomes imperative to also consider how learning outcomes vary across the population---%
% ---a task for which adpting a welfare perspective can be useful.
which is precisely what equity captures.
% adopting a welfare perspective on equity can prove useful.
\nir{cite Chen \& Hooker '22?}
We next specify the steps that are needed for integrating welfare into learning.

\paragraph{Specifying the un(der)specified.}
Optimizing for expected accuracy 
is a notoriously underspecified objective:
for most learning tasks and model classes,
there is typically a large set of (approximately) optimal models
\citep{breiman2001statistical,marx2020predictive,hsu2022rashomon}.
These all attain similarly high accuracy---%
and so are equivalent in the eyes of the objective---%
but can nonetheless vary significantly in other important aspects, 
% including general goals 
such as 
robustness \citep{d2022underspecification}, % and
explainability \citep[see][]{rudin2019stop},
% as well as social criteria such as 
and
fairness \citep[e.g.,][]{rodolfa2020case,coston2021characterizing,black2022model}.
% or demographic disparities \citep{rodolfa2020case}.
We conjecture that this holds also and more broadly in most social learning tasks.
Given this, we propose that one way to guide learning towards more socially preferable models
is by augmenting the learning objective with a well-defined social welfare function,
as specified by a social planner.
\squeeze
\bbox
\textbf{Desideratum:}
Accuracy as an objective is inherently underspecified;
to promote welfare, 
we should provide means for a social planner to encode preferences regarding outcomes, e.g. by a social welfare function.
% \squeeze
\ebox


% either as constraints, regularization, or by other means (see Secs.~\toref).
% This allows a social planner to make a formally express an
% equity statement by
% encoding societal preferences into the learning objective.

\paragraph{Avoiding the arbitrary.}
If we do not explicitly define our preferences over social outcomes,
then underspecification implies that such outcomes can in effect be arbitrary,
as we are leaving them up to either chance,
(possibly intentional) bias, or (unintentional) irrelevant factors,
% for example, outcomes can differ significantly due to 
such as 
different model initializations \cite{d2022underspecification}
or choices regarding feature representation \citep{hron2023modeling}.
% of regularization 
% or sample size .
% \hf{is this a bit overlapping with "specifying the underspecified"? Maybe try to merge the two?}
\bbox
\textbf{Desideratum:}
The choice of welfare function should be made explicit;
since any learned model implicitly %adheres to
endorses \emph{some} preferences,
refraining to do so means leaving social specification to chance or bias. 
\ebox

\todo{add footnote about SIFs by Thaler}

Working under a welfare-aware learning framework \emph{requires}
the learner to make precise his or her position on equity regarding social outcomes.
It also enables a social planner to inspect, monitor, and scrutinize
% audit, regulate, and subsidize
learning approaches that do not make such definitive statements.
We discuss this and other roles that a social planner can play in Sec.~\ref{sec:beyond}.
\bbox
\textbf{Desideratum:}
Learning frameworks should be transparent about the social preferences they prescribe. %foster
\ebox
% This is where adopting a welfare perspective can prove useful:
% If we augment the learning objective with a social welfare function,
% then this serves as a statement.

% - efficiency vs equity in ML


\paragraph{Embedding agency.}
Working under constraints on predictions is not new to machine learning.
This makes it tempting to believe that, for example,
constraining a classifier for resume screening to have at most $k$ positive predictions can account for the limited number of available interview slots as a scarce resource.
But this fails to consider that welfare is defined over \emph{outcomes}, not predictions.
The key difference is that while predictions are fully controlled by the learner, outcomes are not.
In economic settings, the main difficulty in controlling outcomes is precisely that users have interest in them,
and will act steer outcomes to secure those interests.
Thus, top-$k$ prediction that does not account for agency
may be accurate and constrained---%
until users have their say.
% but not in the right way.
\squeeze

\bbox
\textbf{Desideratum:}
Properly accounting for resource limitations requires
accounting for what humans want, know, and do.
Human agency should therefore be modeled directly into the objective.
\ebox

\todo{think if we should say something about fairness here, in terms of constraints on predictions vs outcomes and/or agency. there is some work on this.}

% Two key aspects of social welfare functions is that they are defined over outcomes---not predictions, and measure utility to users---not the learning algorithm.
% For example, 
% This makes them distinct from other types of constraints over predictions,
% such as those targeting
% social bias (e.g., fairness) or
% rankings (e.g., cardinality constraints).


 
% \paragraph{Accounting for policies' causal effects. } 
% \hfr{An important consequence of human agency is the interactions between users and  prediction model. That is, models do not only elicit data from users but also will lead to policies that alter user behaviors. Classic machine learning techniques such as empirical risk minimazation are designed for fixed environments hence cannot account for such complex interactions arising from agents' autonomy. 
% \bbox
% \textbf{Desideratum:}
% Predictions in economic and societal contexts should account for its downstream causal effects to the subjects to whom the predictions apply to (which   often are also the data sources for the prediction task). 
% \ebox
% } 

% \blue{%
% - prediction policies - what people do in practice, even if wrong
% - usually has causal aspects
% - even if y is only source of uncertainty:
%  -- errors in pred do not translate to errors in decisions (consider real y and binary pi - can have same mse for very different error distributions)
%  -- constraints on pi not the same as constraining yhat? (k-hot?)
% } 


\paragraph{Balancing objectives.}
Welfare considers outcomes to \emph{all} agents in an economic system.
For example, and perhaps surprisingly,
welfare in markets sums the utilities of both buyers \emph{and} sellers
(and so is agnostic to money transfers).
In some learning tasks the learner itself is an important entity whose utility we may wish to consider.
A simple first step can therefore be to define a social welfare function that balances between utility to the learner, namely accuracy,
and utility to all users jointly, e.g. via utilitarian welfare:
\begin{equation}
\label{eq:acc_vs_welfare}
\alpha \cdot \acc(h) + (1-\alpha) \cdot \welf(h)
\end{equation}
where $\alpha \in [0,1]$ determines weights.
% \todo{connect this to welfare eq and weights}
We see two 
% use cases for this idea:
regimes where this can be useful.
First,
if accuracy is imperative,
then Eq.~\eqref{eq:acc_vs_welfare} dictates how ties between equally accurate models
should be broken---%
in line with societal preferences.
% to align with 
Second,
if welfare is a primary consideration,
then tuning $\alpha$ allows a social planner to determine the desired operating point, in terms of equity, along the Pareto front.
\blue{This idea is illustrated in Fig.~\toref.}
% \squeeze

\todo{add figure with pareto curve for acc vs welfare}


\paragraph{Tools of the trade.}
Once our objective accounts for users exercising their agency,
reasoning about welfare will require us to model and anticipate their individual and collective behavior.
Luckily,
% economics offers many useful tools for this,
many useful tools for this can be adopted from game theory, mechanism design, information design, and behavioral economics,
as demonstrated by a growing literature on the intersection of machine learning and economics \tocite.
% may require notions beyond those in the standard machine learning toolkit.
But not all problems require new solutions.
Notice that one way to interpret 
Eq.~\eqref{eq:acc_vs_welfare}
is as a regularized accuracy objective in which
\emph{welfare serves as regularization}.
We believe that maximizing welfare can often be achieved 
using methods from the standard machine learning toolkit.
% \hf{to be discussed}

\bbox
\textbf{Desideratum:} 
When possible, welfare should be promoted 
% Welfare should first be pursued
using conventional ML tools:
regularization,
example reweighing,
architecture design, 
loss specification, etc.
In cases where these do not suffice,
economics offers many tools that can be borrowed.
\ebox

This, however, should not be taken to imply that \emph{implementing} the correct solution is simple or straightforward.
For example, although Eq.~\eqref{eq:acc_vs_welfare} is `just' a weighted objective,
working with welfare introduces unique challenges,
due to limited resources and user agency,
that simply do not exist for accuracy.
% working with a welfare term introduces unique challenges that result from the interaction of limited resources and user agency.
Our next section makes these challenges precise.
% highlights some of these challenges,
% suggests how they arise, and proposes ways to treat them.
\squeeze

\todo{evaluation:}


\bbox
\textbf{Desideratum:} Machine learning is renowned for its emphasis on evaluation; this should only be accentuated
when learning has concrete social implications.
\ebox



\todo{causal:}

\bbox
\textbf{Desideratum:}
Predictions in economic and societal contexts should account for its downstream causal effects to the subjects to whom the predictions apply to (which   often are also the data sources for the prediction task). 
\ebox
}



%======================================================================
\section{Further Prospects and Challenges} \label{appx:beyond}
Sec.~\ref{sec:orders} lays out the main goals and principles of 
our framework,
% welfare-maximizing machine learning, 
along with possible avenues to pursue within each of its three orders.
But there are further opportunities, as well as challenges,
that lie beyond the framework's core and
are of both interest and significance.
In this section we highlight some of these directions.
% \squeeze

\subsection{Regulation: Social planner, revisited} \label{sec:regulation}
Our perspective of a social planner so far has been that
of a useful construct for defining the social preferences that should guide learning.
As such, we have mostly discussed the question of how to learn in a way that promotes or aligns with \emph{given} social preferences.
In a sense, this depicts the social planner as acting \emph{before} learning.
But there are additional roles a social planner can play during or after learning, actively and through various mechanisms. For example:
% under various modes of interaction with the learning process.
\squeeze
\begin{itemize}[leftmargin=1em,topsep=0em,itemsep=0.3em]
% \item \textbf{Measuring.}
% - measure/estimate welfare - nontrivial

\item \textbf{Monitoring.}
One defining characteristic of data-driven systems
is that they hold a distinct informational advantage over their users.
In economics, information asymmetry is a well-known source of hazard for markets, with two prominent issues known as \emph{moral hazard} \citep{holmstrom1979moral} (arising from asymmetric information on decisions) and \emph{adverse selection} \citep{akerlof1978market} (arising from asymmetric information on system states). 
Monitoring can help to mitigate this by identifying what information gives the system an unfair advantage, and then channeling this information to users.
This provides an operational motivation for transparency,
with the definitive purpose of improving welfare.
% and can also serve to inform legislation (e.g., consumer protection laws).
% - supervision
% - transparency
% - adverse selection
% - consumer protection laws (eg lemon law), monitoring bodies
% - monitor banks' activities, risk management practices, and compliance with regulations.

\todo{consumer protection laws?}

% \item \textbf{Regulating.}
% objective, constraints (hard, soft)

\priority{%
\item \textbf{Auditing.}
- random cost
- risk aversion of users (vs system that plays for expectation)
}

\item \textbf{Subsidizing.}
When resources are limited, their allocation often depends on the wealth or assets of individuals.
This presents an opportunity to intervene by injecting funds into the system in a way that balances outcomes, e.g., via targeted subsidies.
Subsidies can either be given in response to a learned classifier,
or determined by policy prior to learning.
Since outcomes in our case depend on predictions,
this presents a unique opportunity for a social planner to intervene by directly providing users with useful `features',
thus bypassing the need for a monetary mechanism.
% In either case, 
An interesting question is then how to design learning objectives that operate under such subsidies.
\extended{\tocitec{scgk}}
% Another question is determining when it suffices to 
% - (after model deployment)
% - give money to users
% - give features to users
% - give/inject money to system to allocate - when do interest align?

\priority{%
\item \textbf{Stress-testing.}
- counterfactual (ref stress-test paper)
}

\end{itemize}

Other roles include:
setting standards, licensing, auditing,
mandating disclosure and transparency, and stress-testing.
% \squeeze

\subsection{Inverse tasks: policy preference elicitation}
A key point we have made throughout is that 
any prediction policy advances \emph{some} social preferences.
Until now, our focus was on learning to maximize welfare under a specified social welfare function.
But we can also ask the inverse question:
given a policy---what social welfare function is it optimizing?
This can be useful for revealing the preferences of general,
unspecified policies,
and for evaluating whether a policy that has been specified truly achieves its goals.
One way to approach this would be to use tools from econometrics,
e.g. as in \citep{bjorkegren2022machinelearningpoliciesvalue,pardeshi2024learning}.
But is also interesting to ask whether ideas from inverse problems in machine learning can be adopted.

% \todo{check out: Learning Social Welfare Functions / procaccia [NeurIPS 2024]}

% reverse problem, from policy to infer welfare function. see: (Machine) Learning What Policies Value / bjorkegren, hardt, knight  [2022] 



\subsection{Evaluation}
A main driving force of modern machine learning is the ability to evaluate the performance of different methods effectively and consistently under a wide range of settings and conditions.
Unfortunately, such privileges are not possible when 
learning in social settings.
The challenge of evaluation is shared by all fields at the intersection of learning, economics, and human behavior,
but welfare-maximizing machine learning presents its own unique difficulties.
For example, even the basic task of measuring or estimating social welfare is far from trivial. % \tocite.
One immediate challenges is the difficulty (and restrictions) of obtaining and working with representative and informative user data.
Another is that, as we have stressed, welfare depends on outcomes, which are the product of decisions due to policy,
and so evaluation requires either control of the policy or a way to estimate counterfactuals.
Other than a few noteworthy exceptions \citep[e.g.,][]{bjorkegren2020manipulation,haupt2023recommending,mendler-dunner2024an},
most current works settle for `semi-synthetic' evaluation that uses real data and simulated user behavior.
Simulation is often useful as an intermediary step to drive the field forward (e.g., as in reinforcement learning),
but the ultimate goal should be to enable seamless, realistic, in-the-wild evaluation.



% many examples from both machine learning and econometrics
% suggest that 


% - semi-synth, in the field/wild
%   - measure/estimate social good - not trivial!
%   - alignment
%   - wrt outcome, not predictions

\todo{accuracy-welfare gaps (what did i mean?!)}




\subsection{Accounting for causal effects.}  \label{appx:causal}

As we state in Sec.~\ref{sec:setting},
our focus has been on prediction policies \citep{kleinberg2015prediction} that specify actions
as $a = \policy_h(x) = \policy(h(x))$.
The key assumption of prediction policies is that knowing the true $y$ suffices for making an optimal decision; hence the motivation for replacing $y$ with the prediction $\yhat=h(x)$.
But in realistic settings, $y$ is rarely the only source of uncertainty.
Most decision settings have a casual aspect,
meaning that actions $a$ can \emph{affect} outcomes $y$.
Hence, there is no `true' $y$ on which we can draw,
and the optimal action $a$ (or `intervention')
must include an understanding of its causal effects on $y$.
% \squeeze

When learning in social settings, the issue of causality is further complicated 
by the fact that users have agency, and therefore act.
Because learning determines policy, and policies shape users' choice behavior,
learning comes to have a second-order causal effect on $y$.
The task of causal inference therefore attains two goals:
understand the impact of decisions on outcomes directly,
and indirectly through user choices.
% \squeeze

Econometrics offers many tools for contending with questions of 
causal estimation.
The main challenge is to make inference of causal effects on the basis of observed data---which is the common setting considered in learning.
In cases where we can experiment, randomized control trials (or A/B tests)
offer an alternative.
Some of these ideas have been used in the literature on strategic learning
\citep{miller2020strategic,shavit2020causal,horowitz2023causal},
mostly in relation to the interaction between the system and its users.
A prominent point that concerns welfare is that a causal understanding can be used
to \emph{improve} outcomes---if incentives are set correctly
\citep{kleinberg2020classifiers}.
Despite the importance of causal effects,
we believe there is merit in first exploring question of welfare
under policy predictions, as means to set the stage for more advanced questions concerning causality.




% \blue{%
% - prediction policies - what people do in practice, even if wrong
% - usually has causal aspects
% - even if y is only source of uncertainty:
%  -- errors in pred do not translate to errors in decisions (consider real y and binary pi - can have same mse for very different error distributions)
%  -- constraints on pi not the same as constraining yhat? (k-hot?)
% } 


