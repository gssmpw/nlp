\pdfoutput=1
\documentclass{article}
\usepackage{ifthen}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs}
\usepackage{hyperref}

\newboolean{isInternal}
\setboolean{isInternal}{false}

\newboolean{isOnBorisLaptop}
\IfFileExists{~/boris_machine_marker}{
  \setboolean{isOnBorisLaptop}{true}
}{%
  \setboolean{isOnBorisLaptop}{false}
}

\ifthenelse{\boolean{isInternal}}{
  \usepackage[inline]{showlabels}

  \providecommand{\showlabelfont}{}
  \renewcommand{\showlabelfont}{\ttfamily\fontsize{3.75}{6}\selectfont}

  \newcommand{\internalComment}[1]{\textbf{\color{red}[}{\footnotesize#1}\textbf{\color{red}]}}
}{
  \newcommand{\internalComment}[1]{}
}

\usepackage{mathstyle}

\ExplSyntaxOn
\DeclareExpandableDocumentCommand{\IfNotEmptyT}{mm}%
{%
  \tl_if_empty:nTF{#1}{}{#2}%
}
\ExplSyntaxOff

\usepackage[
backend=biber,
natbib=true,
hyperref,
backref,
style=authoryear,
doi=false,
url=false,
eprint=false,
]{biblatex}

\renewbibmacro{in:}{}

% Links in the bibliography; adapted from https://arxiv.org/pdf/2303.12875
\newbibmacro{string+doiurlisbn}[1]{%
  \iffieldundef{doi}{%
      \iffieldundef{url}{%
          \iffieldundef{isbn}{%
              \iffieldundef{issn}{%
                  #1%
                }{%
                  \href{http://books.google.com/books?vid=ISSN\thefield{issn}}{#1}%
                }%
              }{%
                \href{http://books.google.com/books?vid=ISBN\thefield{isbn}}{#1}%
              }%
            }{%
              \href{\thefield{url}}{#1}%
            }%
          }{%
            \href{https://doi.org/\thefield{doi}}{#1}%
          }%
        }%

        \DeclareFieldFormat{title}{\usebibmacro{string+doiurlisbn}{\mkbibemph{#1}}}
        \DeclareFieldFormat[article,incollection,inproceedings]{title}%
        {\usebibmacro{string+doiurlisbn}{\mkbibquote{#1}}}

\addbibresource{momentum.bib}

% bold symbols
\newcommand{\ba}{\boldsymbol{a}}
\newcommand{\bb}{\boldsymbol{b}}
\newcommand{\bc}{\boldsymbol{c}}
\newcommand{\bd}{\boldsymbol{d}}
\newcommand{\be}{\boldsymbol{e}}
\newcommand{\bof}{\boldsymbol{f}}
\newcommand{\bg}{\boldsymbol{g}}
\newcommand{\bh}{\boldsymbol{h}}
\newcommand{\bi}{\boldsymbol{i}}
\newcommand{\bj}{\boldsymbol{j}}
\newcommand{\bk}{\boldsymbol{k}}
\newcommand{\bl}{\boldsymbol{l}}
\newcommand{\bm}{\boldsymbol{m}}
\newcommand{\bn}{\boldsymbol{n}}
\newcommand{\bo}{\boldsymbol{o}}
\newcommand{\bp}{\boldsymbol{p}}
\newcommand{\bq}{\boldsymbol{q}}
\newcommand{\br}{\boldsymbol{r}}
\newcommand{\bs}{\boldsymbol{s}}
\newcommand{\bt}{\boldsymbol{t}}
\newcommand{\bu}{\boldsymbol{u}}
\newcommand{\bv}{\boldsymbol{v}}
\newcommand{\bw}{\boldsymbol{w}}
\newcommand{\bx}{\boldsymbol{x}}
\newcommand{\by}{\boldsymbol{y}}
\newcommand{\bz}{\boldsymbol{z}}

\newcommand{\balpha}{\boldsymbol{\alpha}}
\newcommand{\bbeta}{\boldsymbol{\beta}}
\newcommand{\bgamma}{\boldsymbol{\gamma}}
\newcommand{\bdelta}{\boldsymbol{\delta}}
\newcommand{\bepsilon}{\boldsymbol{\epsilon}}
\newcommand{\bzeta}{\boldsymbol{\zeta}}
\newcommand{\bfeta}{\boldsymbol{\eta}}
\newcommand{\btheta}{\boldsymbol{\theta}}
\newcommand{\biota}{\boldsymbol{\iota}}
\newcommand{\bkappa}{\boldsymbol{\kappa}}
\newcommand{\blambda}{\boldsymbol{\lambda}}
\newcommand{\bmu}{\boldsymbol{\mu}}
\newcommand{\bnu}{\boldsymbol{\nu}}
\newcommand{\bxi}{\boldsymbol{\xi}}
\newcommand{\bomicron}{\boldsymbol{\omicron}}
\newcommand{\bpi}{\boldsymbol{\pi}}
\newcommand{\brho}{\boldsymbol{\rho}}
\newcommand{\bsigma}{\boldsymbol{\sigma}}
\newcommand{\btau}{\boldsymbol{\tau}}
\newcommand{\bupsilon}{\boldsymbol{\upsilon}}
\newcommand{\bphi}{\boldsymbol{\phi}}
\newcommand{\bchi}{\boldsymbol{\chi}}
\newcommand{\bpsi}{\boldsymbol{\psi}}
\newcommand{\bomega}{\boldsymbol{\omega}}

\newcommand{\bSigma}{\boldsymbol{\Sigma}}
\newcommand{\bTheta}{\boldsymbol{\Theta}}
\newcommand{\bGamma}{\boldsymbol{\Gamma}}

\newcommand{\bA}{\boldsymbol{A}}
\newcommand{\bB}{\boldsymbol{B}}
\newcommand{\bC}{\boldsymbol{C}}
\newcommand{\bD}{\boldsymbol{D}}
\newcommand{\bE}{\boldsymbol{E}}
\newcommand{\bF}{\boldsymbol{F}}
\newcommand{\bG}{\boldsymbol{G}}
\newcommand{\bH}{\boldsymbol{H}}
\newcommand{\bI}{\boldsymbol{I}}
\newcommand{\bJ}{\boldsymbol{J}}
\newcommand{\bK}{\boldsymbol{K}}
\newcommand{\bL}{\boldsymbol{L}}
\newcommand{\bM}{\boldsymbol{M}}
\newcommand{\bN}{\boldsymbol{N}}
\newcommand{\bO}{\boldsymbol{O}}
\newcommand{\bP}{\boldsymbol{P}}
\newcommand{\bQ}{\boldsymbol{Q}}
\newcommand{\bR}{\boldsymbol{R}}
\newcommand{\bS}{\boldsymbol{S}}
\newcommand{\bT}{\boldsymbol{T}}
\newcommand{\bU}{\boldsymbol{U}}
\newcommand{\bV}{\boldsymbol{V}}
\newcommand{\bW}{\boldsymbol{W}}
\newcommand{\bX}{\boldsymbol{X}}
\newcommand{\bY}{\boldsymbol{Y}}
\newcommand{\bZ}{\boldsymbol{Z}}

% === delimiters ===
% see https://tex.stackexchange.com/questions/238643/parenthesis-size-in-a-multiline-equation
\newcommand\MTkillspecial[1]{% helper macro
  \bgroup
  \catcode`\&=9
  \let\\\relax%
  \scantokens{#1}%
  \egroup
}
\newcommand{\DeclareCustomDelim}[3]{
  \DeclarePairedDelimiter{#1}{#2}{#3}
  \reDeclarePairedDelimiterInnerWrapper{#1}{star}{
    \mathopen{##1\vphantom{\MTkillspecial{##2}}\kern-\nulldelimiterspace\right.}
  ##2
  \mathclose{\left.\kern-\nulldelimiterspace\vphantom{\MTkillspecial{##2}}##3}
  }
}

\DeclareCustomDelim{\prn}{\lparen}{\rparen}
\DeclareCustomDelim{\crl}{\{}{\}}
\DeclareCustomDelim{\brk}{[}{]}
\DeclareCustomDelim{\norm}{\|}{\|}
\DeclareCustomDelim{\abs}{|}{|}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}

\providecommand\given{}
\DeclarePairedDelimiterXPP\Prob[1]{\Problet}\{\}{}{
  \renewcommand\given{\nonscript\:\delimsize\vert\nonscript\:\mathopen{}}
  #1}
\DeclarePairedDelimiterXPP\Expect[1]{\Expectlet}[]{}{
  \renewcommand\given{\nonscript\:\delimsize\vert\nonscript\:\mathopen{}}
  #1}

% === Jumping to definitions ===
\newcounter{notationcnt}

\makeatletter
\newcommand\newtarget[1]{\refstepcounter{notationcnt}\label{#1}}
\makeatother
\newcommand{\protectedLink}[2]{
  {
    \hypersetup{hidelinks}
    \protect\hyperref[#1]{#2}
  }
}

% === Labeling relation signs ===
\newcommand*\makeAlph[1]{\symbol{\numexpr96+#1}}
\NewDocumentCommand{\labrel}{m o }{%
  \IfNoValueTF{#2}{(\makeAlph{#1})}{\stackrel{\mathrm{(\makeAlph{#1})}}{#2}}%
}

% === notations ===
\renewcommand{\geq}{\geqslant}
\renewcommand{\leq}{\leqslant}

\newcommand{\subarrsymbol}{:}
\newcommand{\subarr}{\nolinebreak\mathinner{\subarrsymbol}\nolinebreak}
\NewDocumentCommand{\range}{omm}{\brk[#1]{#2 \subarr #3}}

\newcommand{\Int}{\int\limits}
\newcommand{\Intr}{\int\limits_0^{+\infty}}
\newcommand{\Problet}{\ensuremath\mathbb{P}}
\newcommand{\Expectlet}{\ensuremath\mathbb{E}}
\newcommand{\EmpExpect}[1]{\ensuremath\Expectlet_n\left[#1\right]}
\newcommand{\Varlet}{\mathbb{V}}
\newcommand{\Indlet}{\ensuremath\mathbbm{1}}
\newcommand{\Var}[1]{\Varlet\left[#1\right]}
\newcommand{\ind}[1]{\mathbf{1}\left\{#1\right\}}
\newcommand{\inde}[1]{\mathbf{1}_{#1}}
\newcommand{\iid}{i.\,i.\,d. }
\DeclareMathOperator*{\supp}{supp}
\DeclareMathOperator*{\cov}{cov}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\diag}{diag}
\DeclareMathOperator{\tay}{Tay}
\DeclareMathOperator{\sign}{sign}

\newcommand{\trans}{^{\mkern-1.5mu\mathsf{T}}}  % from Kevin Murphy

\NewDocumentCommand{\extmomfuncNoLink}{}{\boldsymbol{\Phi}}
\NewDocumentCommand{\extmomfunc}{}{\protectedLink{def:extmomfunc}{\extmomfuncNoLink}}
\NewDocumentCommand{\extmomfuncsc}{}{\protectedLink{def:extmomfunc}{\Phi}}

\NewDocumentCommand{\bcorNoLink}{m m}{b_{#1}^{(#2 + 1)}}
\NewDocumentCommand{\bcor}{m m}{\protectedLink{def:bcor}{\bcorNoLink{#1}{#2}}}

\NewDocumentCommand{\mcorscNoLink}{m m m}{m_{#1; #2}^{(#3)}}
\NewDocumentCommand{\mcorNoLink}{m m}{\bm_{#1}^{(#2)}}
\NewDocumentCommand{\mcor}{m m}{\protectedLink{eq:cor-general-momentum-methods-defs}{\mcorNoLink{#1}{#2}}}
\NewDocumentCommand{\mcorsc}{m m m}{\protectedLink{eq:cor-general-momentum-methods-defs}{\mcorscNoLink{#1}{#2}{#3}}}

\NewDocumentCommand{\bcorc}{m}{\protectedLink{def:bcor}{b_{#1}}}

\NewDocumentCommand{\momfuncNoLink}{}{\boldsymbol{g}}
\NewDocumentCommand{\momfunc}{}{\protectedLink{def:momfunc}{\momfuncNoLink}}
\NewDocumentCommand{\momfuncscNoLink}{}{g}
\NewDocumentCommand{\momfuncsc}{}{\protectedLink{def:momfunc}{\momfuncscNoLink}}

\NewDocumentCommand{\gbar}{m m}{\bar{\boldsymbol{g}}_{#1}\IfNotEmptyT{#2}{^{(#2)}}}%

\NewDocumentCommand{\numofmomsNoLink}{}{L}
\NewDocumentCommand{\numofmomsDef}{}{\newtarget{def:numofmoms}{\numofmomsNoLink}}
\NewDocumentCommand{\numofmoms}{}{\protectedLink{def:numofmoms}{\numofmomsNoLink}}
\NewDocumentCommand{\momsumNoLink}{m m}{\bar{\bm}_{#1}^{(#2)}}
\NewDocumentCommand{\momsum}{m m}{\protectedLink{eq:cor-general-momentum-methods-defs}{\momsumNoLink{#1}{#2}}}
\NewDocumentCommand{\momsumc}{m}{\protectedLink{eq:cor-general-momentum-methods-defs}{\bar{\bm}_{#1}}}

\NewDocumentCommand{\lossNoLink}{}{\mathcal{L}}
\NewDocumentCommand{\lossDef}{}{\newtarget{def:loss}{\lossNoLink}}
\NewDocumentCommand{\loss}{}{\protectedLink{def:loss}{\lossNoLink}}

\NewDocumentCommand{\lrNoLink}{}{h}
\NewDocumentCommand{\lrDef}{}{\newtarget{def:lr}{\lrNoLink}}
\NewDocumentCommand{\lr}{}{\protectedLink{def:lr}{\lrNoLink}}

\NewDocumentCommand{\corr}{m}{\boldsymbol{M}^{(#1)}}
\NewDocumentCommand{\corrsc}{m m}{M_{#1}^{(#2)}}

% number a specific equation in align*
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}

% === constants numeration
\newcounter{ccnt}
\NewDocumentCommand{\nc}{m}{
  \refstepcounter{ccnt}\ensuremath{c_{\theccnt}}\IfValueT{#1}{\label{#1}}%
}
\NewDocumentCommand{\oc}{m}{
  \ensuremath{c_{\ref{#1}}}
}
\newcounter{bigccnt}
\NewDocumentCommand{\nC}{m}{%
  \refstepcounter{bigccnt}\ensuremath{C_{\thebigccnt}}\IfValueT{#1}{\label{#1}}%
}
\NewDocumentCommand{\oC}{m}{
  \ensuremath{C_{\ref{#1}}}
}

\graphicspath{{pics/}}

\newcommand{\papertitle}{
  How Memory in Optimization Algorithms\\
  Implicitly Modifies the Loss} %,\\ Sometimes Hurting Generalization
\newcommand{\runningpapertitle}{How Memory in Optimization Algorithms Implicitly Modifies the Loss}

\title{\papertitle}

\author{
  Matias D. Cattaneo\thanks{Equal contribution} \\
  Princeton University\\
  \texttt{cattaneo@princeton.edu} \\
  \and
  Boris Shigida\footnotemark[1] \\
  Princeton University\\
  \texttt{bs1624@princeton.edu} \\
}


\allowdisplaybreaks[4]

\begin{document}

\maketitle

\begin{abstract}
    In modern optimization methods used in deep learning, each update depends on the history of previous iterations, often referred to as \textit{memory}, and this dependence decays fast as the iterates go further into the past. For example, gradient descent with momentum has exponentially decaying memory through exponentially averaged past gradients. We introduce a general technique for identifying a memoryless algorithm that approximates an optimization algorithm with memory. It is obtained by replacing all past iterates in the update by the current one, and then adding a correction term arising from memory (also a function of the current iterate). This correction term can be interpreted as a perturbation of the loss, and the nature of this perturbation can inform how memory implicitly (anti-)regularizes the optimization dynamics. As an application of our theory, we find that Lion does not have the kind of implicit anti-regularization induced by memory that AdamW does, providing a theory-based explanation for Lion's better generalization performance recently documented \citep{chen2023symbolic}.
\end{abstract}

\section{Introduction}

Many optimization methods used in deep learning are first-order methods with exponentially decaying memory. For example, adding ``momentum'' to gradient descent (GD) is a well-established practice to make training smoother and convergence faster (e.\,g. \citet{krizhevsky2012imagenet}). Adaptive methods such as Adam \citep{kingma2017adammethodstochasticoptimization}, RMSProp \citep{tieleman2012lecture}, AdamW \citep{loshchilov2019decoupled}, and AdaFactor \citep{shazeer2018adafactor}, which are commonly used to train large language models \citep{dubey2024llama,liu2024deepseek,chowdhery2023palm}, all have exponentially decaying memory. Despite the popularity of such optimization methods, there is little theoretical knowledge about the implicit regularization memory introduces to them (potentially informing what regions of the loss space the method takes the iterates to, what minima they converge to, how such minima influence the generalization of the trained model, and so on). In this article, we introduce a general framework for identifying such regularization.

We study a general class of optimization algorithms described by the following iteration
\begin{equation}\label{eq:general-iteration}
\btheta^{(n + 1)} = \btheta^{(n)} - \lr \boldsymbol{F}^{(n)}(\btheta^{(n)}, \ldots, \btheta^{(0)}),
\end{equation}
where $\btheta^{(n)} \in \mathbb{R}^d$ are the (evolving) parameters of the machine learning model, $\btheta^{(0)}$ is some initial condition, $\lr$ is the step size or learning rate, and the functions $\boldsymbol{F}^{(n)}$ map from (some subset of) $(\mathbb{R}^d)^{n + 1}$ to $\mathbb{R}^d$ and are allowed to be different at each iteration. The right-hand side in \cref{eq:general-iteration} depends on the whole history of previous iterates, which means the algorithm has memory.

For many algorithms used in practice, dependence on the history comes in one specific form: by using what we call ``\textit{momentum variables}'', that is, exponential averages of some functions of the iterate $\btheta^{(n)}$ (usually, more specifically, functions of the loss gradient). We present five\internalComment{!} leading examples to illustrate this point.

\begin{example}[Heavy-ball momentum gradient descent; \citet{polyak1964some}]\label{ex:heavy-ball}
    This optimizer can be written in the form~\eqref{eq:general-iteration} with
    \begin{align*}
        \boldsymbol{F}^{(n)}(\btheta^{(n)}, \ldots, \btheta^{(0)}) &= \mcor{1}{n + 1},\\
        \text{where}\quad \mcor{1}{n + 1} &= \sum_{k = 0}^n \beta^{n - k} \nabla \loss(\btheta^{(k)})\numberthis\label{eq:sgd-momentum},
    \end{align*}
    for some initial condition $\btheta^{(0)}$, and where $\beta \in [0, 1)$ is the momentum parameter, $\loss$ is the loss function to be optimized, and $\nabla \loss$ is its gradient.
    \hfill$\blacksquare$
\end{example}

This optimizer in \cref{ex:heavy-ball} is often just referred to as GD with momentum, where the exponential sum $\mcor{1}{n + 1}$ in \cref{eq:sgd-momentum} is the \textit{momentum variable}: it exponentially averages past gradients.  The aforementioned optimizer is well-known and often used for training recurrent neural networks and convolutional neural networks, but it underperforms adaptive optimizers when training other architectures such as transformers \citep{zhang2020adaptive,liu2020understanding,anil2019memory,kunstner2023noise}. The following modification is also commonly used (this formulation is taken from \citet{choi2020empiricalcomparisonsoptimizersdeep} and matches the standard PyTorch implementation). \internalComment{\href{https://towardsdatascience.com/is-pytorchs-nesterov-momentum-implementation-wrong-5dc5f5032008}{post} about why this is equivalent to classical Nesterov}

\begin{example}[Nesterov's accelerated gradient descent; \citet{nesterov1983method}]\label{ex:nesterov}
    This optimizer can be written in the form~\eqref{eq:general-iteration} with
    \begin{align*}
        \boldsymbol{F}^{(n)}(\btheta^{(n)}, \ldots, \btheta^{(0)}) &= \mcor{1}{n + 1} + \mcor{2}{n + 1},\\
        \text{where}\quad \mcor{1}{n + 1} &= \beta \sum_{k = 0}^n \beta^{n - k} \nabla \loss(\btheta^{(k)}),\\
        \mcor{2}{n + 1} &= \nabla \loss(\btheta^{(n)}),
    \end{align*}
    for some initial condition $\btheta^{(0)}$, and where $\beta \in [0, 1)$ is the momentum parameter, $\loss$ is the loss function to be optimized, and $\nabla \loss$ is its gradient.
    \hfill$\blacksquare$
\end{example}

The next example presents a leading adaptive optimizer nowadays commonly used for training large language models \citep{dubey2024llama,liu2024deepseek}.

\begin{example}[AdamW; \citet{loshchilov2019decoupled}]\label{ex:adamw}
    The optimizer can be written in the form~\eqref{eq:general-iteration} with
    \begin{align*}
      \boldsymbol{F}^{(n)}(\btheta^{(n)}, \ldots, \btheta^{(0)}) &= \frac{\mcor{1}{n + 1}}{\sqrt{\mcor{2}{n + 1} + \varepsilon}} + \mcor{3}{n + 1},\\
      \text{where}\quad \mcor{1}{n + 1} &= \frac{1 - \beta_1}{1 - \beta_1^{n + 1}} \sum_{k = 0}^n \beta_1^{n - k} \nabla \loss(\btheta^{(k)}),\\
      \mcor{2}{n + 1} &= \frac{1 - \beta_2}{1 - \beta_2^{n + 1}} \sum_{k = 0}^n \beta_2^{n - k} \prn[\big]{{\nabla \loss(\btheta^{(k)})}}^2,\\
      \mcor{3}{n + 1} &= \lambda \btheta^{(n)},
    \end{align*}
    for some initial condition $\btheta^{(0)}$, and where $0 \leq \beta_1, \beta_2 < 1$ are momentum parameters, $\varepsilon > 0$ is a numerical stability parameter, $0 < \lambda < 1$ is a weight decay parameter, and the squares and square roots are taken component-wise.
    \hfill$\blacksquare$
\end{example}

In \cref{ex:adamw}, $\mcor{1}{n + 1}$ and $\mcor{2}{n + 1}$ are also \textit{momentum variables}: exponentially averaged gradients and exponentially averaged squared gradient components respectively, with coefficients in front of the sum, such as $(1 - \beta_1) (1 - \beta_1^{n + 1})^{-1}$, providing ``bias correction'' \citep{kingma2017adammethodstochasticoptimization}. The variable $\mcor{3}{n + 1}$ here is a degenerate momentum variable, with memory decaying infinitely fast.

The following modification incorporates Nesterov's momentum into AdamW. This formulation is taken from \citet{choi2020empiricalcomparisonsoptimizersdeep} (except here $\varepsilon$ is inside the square root in the denominator).
\begin{example}[NAdam with decoupled weight decay; \citet{dozat2016incorporating}]\label{ex:nadamw}
    The optimizer can be written in the form~\eqref{eq:general-iteration} with
    \begin{align*}
      \boldsymbol{F}^{(n)}(\btheta^{(n)}, \ldots, \btheta^{(0)}) &= \frac{\beta_1 \mcor{1}{n + 1} + (1 - \beta_1) \mcor{4}{n + 1}}{\sqrt{\mcor{2}{n + 1} + \varepsilon}} + \mcor{3}{n + 1},\\
      \text{where}\quad \mcor{1}{n + 1} &= \frac{1 - \beta_1}{1 - \beta_1^{n + 1}} \sum_{k = 0}^n \beta_1^{n - k} \nabla \loss(\btheta^{(k)}),\\
      \mcor{2}{n + 1} &= \frac{1 - \beta_2}{1 - \beta_2^{n + 1}} \sum_{k = 0}^n \beta_2^{n - k} \prn[\big]{{\nabla \loss(\btheta^{(k)})}}^2,\\
      \mcor{3}{n + 1} &= \lambda \btheta^{(n)},\\
      \mcor{4}{n + 1} &= \nabla \loss(\btheta^{(n)}),
    \end{align*}
    for some initial condition $\btheta^{(0)}$, and where $0 \leq \beta_1, \beta_2 < 1$ are momentum parameters, $\varepsilon > 0$ is a numerical stability parameter, $0 < \lambda < 1$ is a weight decay parameter, and the squares and square roots are taken component-wise.
    \hfill$\blacksquare$
\end{example}

As a final example, consider a new optimizer called Lion (Evo\textbf{L}ved S\textbf{i}gn M\textbf{o}me\textbf{n}tum), which was recently discovered by an evolutionary search, and then verified to generalize better than AdamW on a variety of tasks \citep{chen2023symbolic}. We consider a generalized version of the Lion algorithm.

\begin{example}[Lion-$\mathcal{K}$; \citet{chen2024lion}]\label{ex:lion-K}
    The optimizer can be written in the form of~\eqref{eq:general-iteration} with
    \begin{align*}
      \boldsymbol{F}^{(n)}(\btheta^{(n)}, \ldots, \btheta^{(0)}) &= - \nabla \mathcal{K}(\mcor{1}{n + 1} + \mcor{2}{n + 1}) + \mcor{3}{n + 1},\\
      \text{where}\quad \mcor{1}{n + 1} &= - (1 - \rho_2) \frac{\rho_1}{\rho_2} \sum_{k = 0}^n \rho_2^{n - k} \nabla \loss(\btheta^{(k)}),\\
      \mcor{2}{n + 1} &= - \prn[\bigg]{1 - \frac{\rho_1}{\rho_2}} \nabla \loss(\btheta^{(n)}),\\
      \mcor{3}{n + 1} &= \lambda \btheta^{(n)},\numberthis\label{eq:lion-k}
    \end{align*}
    for some initial condition $\btheta^{(0)}$, and where $0 \leq \rho_1, \rho_2 < 1$ are Lion's momentum parameters (\cref{sec:identifying-the-effect-of-memory} makes clear why we choose the letter $\rho$ rather than $\beta$ for them), $\lambda > 0$ is a weight decay parameter, $\mathcal{K}\colon \mathbb{R}^d \to \mathbb{R}$ is some convex function, and $\nabla \mathcal{K}$ is its subgradient.
    \internalComment{To get their equation (24), multiply $\mcor{1}{n + 1}$ and $\mcor{2}{n + 1}$ by $\lr \alpha / (1 - \rho_2)$.}
    \hfill$\blacksquare$
\end{example}

Ordinary Lion corresponds to $\mathcal{K}(\bx) = \norm{\bx}_1$ and $\nabla \mathcal{K}(\bx) = \sign(\bx)$ in \cref{ex:lion-K}, where the $\sign$ function is understood component-wise. We consider the generalized Lion-$\mathcal{K}$ algorithm because it covers a few known algorithms as special cases: see Table~1 and Section~3.1 in \citet{chen2024lion}. In fact, it also includes \cref{ex:heavy-ball} as a special case by taking $\mathcal{K}(\bx) = \norm{\bx}^2 / 2$, $\rho_1 = \rho_2$, and $\lambda = 0$, but we will deal with that important specific example separately for clarity.

It is reasonable to expect that adding exponentially decaying memory to an algorithm in such a way as described above (for example, replacing the gradient with exponentially averaged past gradients) changes the optimization dynamics, thereby affecting the performance of the trained model. The technique we introduce identifies \textit{how} the iteration evolution changes when memory is added. This technique starts with an iteration having memory, and replaces it by a memoryless iteration that approximates the original one, provided a correction term is added. Specifically, we start with algorithm~\eqref{eq:general-iteration}, and then construct a corresponding new memoryless iteration of the form
\begin{equation}\label{eq:memoryless-iteration}
  \tilde{\btheta}^{(n + 1)} = \tilde{\btheta}^{(n)} - \lr \brk[\big]{\boldsymbol{F}^{(n)}(\tilde{\btheta}^{(n)}) + \corr{n}(\tilde{\btheta}^{(n)})},
\end{equation}
where we slightly abuse notation and put
\begin{equation*}
  \boldsymbol{F}^{(n)}(\tilde{\btheta}^{(n)}) \equiv \boldsymbol{F}^{(n)}(\underbrace{\tilde{\btheta}^{(n)}, \ldots, \tilde{\btheta}^{(n)}}_{\text{$n + 1$ times}}),
\end{equation*}
and where the function $\corr{n}(\btheta)$ captures a correction due to the presence of memory.
We then prove an explicit bound on the approximation error $\norm{\btheta^{(n)} - \tilde{\btheta}^{(n)}}$, as a function of the learning rate $\lr$. Interpreting the correction term can sometimes generate predictions on whether memory helps or hurts generalization of first-order methods with momentum.

Our general theory only relies on memory decaying sufficiently fast, not necessarily in the form of momentum variables, and thus covers all the examples listed above and many others, while also allowing for both full-batch and mini-batch training. \cref{sec:developing-intuition} first presents a heuristic discussion of our proposed technique focusing on the simplest possible case for clarity: GD with momentum (\cref{ex:heavy-ball}). Then, \cref{sec:identifying-the-effect-of-memory} presents our main theoretical contribution, which is more cumbersome in terms of notation and technicalities due to its generality.

Depending on specific optimization algorithm considered, our general result can lead to different theoretical and practical conclusions. As a substantive application, \cref{sec:adamw-anti-reg-but-lion-not} studies AdamW (\cref{ex:adamw}) and Lion-$\mathcal{K}$ (\cref{ex:lion-K}), and demonstrates that \textit{Lion does not suffer from the anti-regularization effect that AdamW's memory has}, which may be a partial explanation for Lion's better generalization on many tasks. Section \ref{sec:further-implications} discusses further implications of our main theoretical result: modified equations, and mini-batch training.

\subsection{Related Literature}

Approximating a memoryful iteration with a memoryless one is closely connected with the method of modified equations (sometimes called \textit{backward error analysis}), where a discrete algorithm like \eqref{eq:general-iteration} is approximated by a continuous solution of an ordinary differential equation or stochastic differential equation. Typically, this method can only be applied to an algorithm with no memory, in a possibly enlarged phase space as opposed to $\mathbb{R}^d$; for example, heavy-ball momentum GD has no memory when viewed as a discrete iteration $(\btheta^{(n)}, \bm^{(n)})$ in $\mathbb{R}^{2 d}$, where $\bm^{(n)}$ is the momentum variable. The general technique introduced in this paper can be used to derive a memoryless discrete iteration which can then be approximated by a continuous trajectory. Background on the method of modified equations can be found in \citet{hairer2006,pmlr-v70-li17f}.

Works deriving modified equations for (S)GD with or without momentum include \citet{barrett2021implicit,smith2021on,ghosh2023implicit,farazmand2020multiscale,kovachki2021continuous,miyagawa2022toward,rosca2023on,pmlr-v70-li17f}. In particular, \citet{ghosh2023implicit} identified that momentum strongly regularizes the loss function in the case of GD, though their error bounds both have a different focus (continuous approximation rather than discrete one), and follow a different approach which appears hard to generalize to other algorithms.
Our approach works for a wide class of algorithms, and we recover their main results in \cref{sec:further-implications}.
Works approximating adaptive methods with continuous trajectories include \citet{ma2022qualitative,malladi2022sdes,barakat2021convergence}. More recently, \citet{pmlr-v235-cattaneo24a} studied the special case of Adam / RMSProp.
Their focus is not on memory but on continuous approximations; in particular, they do not have approximation bounds between two discrete iterations like we do. In addition, their arguments are highly specialized to Adam, and they do not incorporate weight decay. Although we also discuss Adam (with weight decay) extensively, it is only because of its importance in practice, and our results cover a much broader class of optimizers.

This paper is also connected to the strand of the literature studying the implicit bias of optimization algorithms. For example, \citet{xie2024implicit} and \citet{chen2024lion} prove that weight decay causes AdamW and Lion to solve an $\ell_{\infty}$-norm constrained optimization problem. In addition, a large body of work is devoted to the bias of optimization algorithms towards the direction of the max-margin vector
\citep{soudry2018implicit,nacson2019convergence,nacson2019stochastic,qian2019implicit,wang2022does,gunasekar2018characterizing,ji2018risk,ji2019implicit}. Similarly, \citet{damian2021label,li2022what,arora2022understanding,wen2023how,damian2023selfstabilization} explore the sharpness of regions SGD converges to. \citet{gunasekar2017implicit,arora2019implicit} study implicit regularization in matrix factorization. \citet{li2019towards} prove in a certain setting that a larger learning rate leads to better generalization.

\subsection{Notation}

We use standard notations for the $\ell_p$ norm of a vector $\norm{\bv}_p = \prn[\big]{\sum_i \abs{v_i}^p}^{1 / p}$; the infinity-norm is defined as $\norm{\bv}_\infty = \max_i \abs{v_i}$; finally, the norm without indices is by default Euclidean: $\norm{\bv} \equiv \norm{\bv}_2$. When we write $\bu_{n, \lr} = O(g(\lr))$, where $g(\lr)$ is some fixed function of $\lr$ and $\bu_{n, \lr}$ is some sequence of vectors possibly depending on $\lr$, we mean that there is a constant $C$ not depending on $\lr$ or $n$ such that $\norm{\bu_{n, \lr}} \leq C g(\lr)$. For a vector-valued function $\boldsymbol{f}(\bu_1, \ldots, \bu_k) \in \mathbb{R}^d$, the notation $\frac{\partial \boldsymbol{f}}{\partial \bu_{\ell}}$, where $\bu_{\ell} \in \mathbb{R}^m$, means a $d \times m$ matrix whose $(i, j)$-th element is $\frac{\partial f_i}{\partial u_{\ell; j}}$. Similarly, $\frac{\partial^2 \boldsymbol{f}}{\partial \bu_{\ell} \partial \bu_p}$ is a tensor of order 3 whose $(i, j, k)$-th coordinate is $\frac{\partial^2 f_i}{\partial u_{\ell; j} \partial u_{p; k}}$. (S)GD is an abbreviation for (stochastic) gradient descent, O(S)DE for ordinary (stochastic) differential equation. We will contract repeating arguments when convenient, e.\,g. instead of $\boldsymbol{F}^{(n)}(\btheta, \ldots, \btheta)$ we will write just $\boldsymbol{F}^{(n)}(\btheta)$. We will use notation $\lossDef(\cdot)$ for the loss and $\nabla \loss(\cdot)$ for its gradient, $\lrDef$ for the learning rate.

\section{Developing Intuition: Memory Regularizes GD with Momentum}\label{sec:developing-intuition}

We provide a heuristic explanation of our technique, considering the simplest algorithm with exponentially decaying memory: heavy-ball momentum GD (\cref{ex:heavy-ball}). As explained above, we would like to remove the dependence of the right-hand side in
\begin{equation}\label{eq:heavy-ball-in-one-eq}
\btheta^{(n + 1)} = \btheta^{(n)} - \lr \sum_{k = 0}^n \beta^{n - k} \nabla \loss(\btheta^{(k)})
\end{equation}
on the ``past'' iterates $\btheta^{(n - 1)}, \ldots, \btheta^{(0)}$, leaving only the dependence on the ``current'' iterate $\btheta^{(n)}$. Let us represent ``past'' iterates through the ``current'' one. First, write
\begin{align*}
  \btheta^{(n - 1)}
  &= \btheta^{(n)} + \lr \sum_{b = 0}^{n - 1} \beta^{b} \nabla \loss(\btheta^{(n - 1 - b)})\\
  &= \btheta^{(n)} + \lr \sum_{b = 0}^{n - 1} \beta^{b} \nabla \loss(\btheta^{(n)}) + O(\lr^2),
\end{align*}
where the second equality relies on exponential averaging to replace historical iterates with $\btheta^{(n)}$, influencing only higher-order terms. Similarly,
\begin{align*}
  \btheta^{(n - 2)} &= \btheta^{(n - 1)} + \lr \sum_{b = 0}^{n - 2} \beta^{b} \nabla \loss(\btheta^{(n - 2 - b)})\\
                    &= \btheta^{(n - 1)} + \lr \sum_{b = 0}^{n - 2} \beta^{b} \nabla \loss(\btheta^{(n)}) + O(\lr^2),\\
                    &= \btheta^{(n)} + \lr \crl[\bigg]{\sum_{b = 0}^{n - 1} \beta^{b} + \sum_{b = 0}^{n - 2} \beta^{b}} \nabla \loss(\btheta^{(n)}) + O(\lr^2),
\end{align*}
where the last equality follows by inserting the expression for $\btheta^{(n - 1)}$. Continue like this up to
\begin{align*}
\btheta^{(n - k)} = \btheta^{(n)} + \lr \sum_{l = 1}^k \sum_{b = 0}^{n - l} \beta^{b} \nabla \loss(\btheta^{(n)}) + O(k^2 \lr^2),
\end{align*}
where the $k^2$ provides an estimate on the accumulation of error terms of order $\lr^2$.

We have now represented all the historical iterates through the current one. Combining it with Taylor expansion around $\btheta^{(n)}$ in \cref{eq:heavy-ball-in-one-eq}, we obtain
\begin{align*}
  \btheta^{(n + 1)}
  &= \btheta^{(n)} - \lr \sum_{k = 0}^n \beta^k \crl[\bigg]{\nabla \loss(\btheta^{(n)}) + \lr \nabla^2 \loss(\btheta^{(n)}) \sum_{l = 1}^k \sum_{b = 0}^{n - l} \beta^{b} \nabla \loss(\btheta^{(n)})  + O(k^2 \lr^2)}\\
  &= \btheta^{(n)} - \lr \frac{1 - \beta^{n + 1}}{1 - \beta} \nabla \loss(\btheta^{(n)})\\
  &\quad- \lr^2 \frac{\beta \brk{1 - (2 n + 1) \beta^n (1 - \beta) - \beta^{2 n + 1}}}{(1 - \beta)^3} \nabla^2 \loss(\btheta^{(n)}) \nabla \loss(\btheta^{(n)}) + O(\lr^3).
\end{align*}
Now using $\nabla^2 \loss(\btheta) \nabla \loss(\btheta) = (1 / 2) \nabla \norm{\nabla \loss(\btheta)}^2$ and neglecting coefficients that go exponentially fast to zero as $n \to \infty$, we obtain that heavy-ball momentum GD is close to ordinary GD (no momentum) with a different step size and different loss, given by
\begin{equation}\label{eq:hb-mod-loss}
\begin{aligned}
  \btheta^{(n + 1)} &= \btheta^{(n)} - \frac{\lr}{1 - \beta} \nabla \tilde{\loss}(\btheta),\quad\text{where}\\
  \tilde{\loss}(\btheta) &= \loss(\btheta) + \frac{\lr \beta}{2 (1 - \beta)^2} \norm{\nabla \loss(\btheta)}^2.
\end{aligned}
\end{equation}
The term $\frac{\lr \beta}{2 (1 - \beta)^2} \norm{\nabla \loss(\btheta)}^2$ that is added implicitly to the loss by the momentum can be interpreted as implicit regularization. Since $\beta$ is usually taken to be close to one, the term strongly penalizes the squared norm of the gradient. There is empirical evidence that such penalization improves generalization \citep{barrett2021implicit,smith2021on,ghosh2023implicit}.
In fact, this term (up to coefficients) can be interpreted as a first-order approximation of $\ell_2$ sharpness \citep{ghosh2023implicit}, which suggests that it moves the trajectory towards ``flatter'' minima; this is often thought to improve generalization \citep{foret2021sharpnessaware}, although in general the evidence on the connection between sharpness of minima and generalization is mixed \citep{pmlr-v202-andriushchenko23a}.

\section{General Theory: The Effect of Memory}\label{sec:identifying-the-effect-of-memory}

%\subsection{Assumption and Key Observation}\label{sec:assumption-and-key-observation}

The general form of an optimization algorithm with memory is given by \cref{eq:general-iteration}. The only property of memory we use is that it (uniformly in $n$) decays exponentially fast, as made precise by \cref{ass:form-of-fn} below. Openness and convexity of the domain of optimization $\mathcal{D}$, that is, where all $\crl{\btheta^{(n)}}$ will be assumed to lie, are innocuous assumptions (e.g., $\mathbb{R}^d$ is open and convex); we impose them to avoid technicalities with differentiation and Taylor expansion.

\begin{assumption}[Memory Decay]\label{ass:form-of-fn}
    Let $\mathcal{D}$ be an open convex domain in $\mathbb{R}^d$. Let $\crl{\boldsymbol{F}^{(n)}(\btheta^{(n)}, \ldots, \btheta^{(0)})}_{n = 0}^{\infty}$ be a family of functions $\mathcal{D}^{n + 1} \to \mathbb{R}^d$, two times continuously differentiable on their respective domains, such that for any $n \in \mathbb{Z}_{\geq 0}$, $k_1, k_2 \in \crl{0, \ldots, n}$, $r, i, j \in \crl{1, \ldots, d}$,
    \begin{gather}
    \abs{F^{(n)}_r} \leq \gamma_{-1},\\
    \abs[\bigg]{\frac{\partial F_r^{(n)}}{\partial \theta_i^{(n - k_1)}}} \leq \gamma_{k_1},\label{eq:first-der-bound}\\
    \abs[\bigg]{\frac{\partial^2 F_r^{(n)}}{\partial \theta_i^{(n - k_1)} \partial \theta_j^{(n - k_2)}}} \leq \gamma_{k_1, k_2},\label{eq:second-der-bound}
    \end{gather}
    where $\boldsymbol{F}^{(n)}=(F_1^{(n)}, \ldots,F_d^{(n)})\trans$, and $\gamma_{-1}$, $\gamma_{k_1}$ and $\gamma_{k_1, k_2}$ are a families of positive reals (not depending on $n$) satisfying $\sum_{k_1 = 1}^{\infty} \gamma_{k_1} k_1^2 + \sum_{k_1, k_2 = 1}^{\infty} \gamma_{k_1, k_2} k_1 k_2 < \infty$.
    \end{assumption}

    In the examples listed in the introduction, $\boldsymbol{F}^{(n)}$ satisfies a more specific form that can be used to give more primitive conditions for \cref{ass:form-of-fn}. The following remark discusses this case.

    \begin{remark}[$\boldsymbol{F}^{(n)}$ as a function of momentum variables]\label{rem:fn-as-a-func-of-mom-vars}
    Let $\newtarget{def:momfunc}{\crl{\momfuncNoLink_{\ell}^{(n)}}_{\ell = 1}^{\numofmoms}}$ be $\numofmomsDef$ two times continuously differentiable functions $\mathcal{D} \to \mathbb{R}^d$, uniformly bounded along with two derivatives, and let $\newtarget{def:extmomfunc}\extmomfuncNoLink(\bm_1, \ldots, \bm_\numofmoms)\colon (\mathbb{R}^d)^\numofmoms \to \mathbb{R}^d$ be a fixed two times continuously differentiable function, uniformly bounded along with two derivatives. Let $\crl{\beta_{\ell}}_{\ell = 1}^{\numofmoms}$ be fixed reals in $[0, 1)$, and $\crl{\newtarget{def:bcor}\bcorNoLink{\ell}{n}}_{\ell = 1}^{\numofmoms}$ be $\numofmoms$ bounded nonnegative sequences of reals (for $n \in \mathbb{Z}_{\geq 0}$). If the function $\boldsymbol{F}^{(n)}$ has the specific form
    \begin{align*}
        \boldsymbol{F}^{(n)}(\btheta^{(n)}, \ldots, \btheta^{(0)}) &:= \extmomfunc(\mcor{1}{n + 1}, \ldots, \mcor{\numofmoms}{n + 1}),\\
        \text{where }\mcor{\ell}{n + 1} &:= \bcor{\ell}{n} \sum_{k = 0}^n \beta_\ell^k \momfunc_{\ell}^{(n - k)}(\btheta^{(n - k)}),\numberthis\label{eq:cor-general-momentum-methods-defs}
    \end{align*}
    then it satisfies \cref{ass:form-of-fn} (\cref{lem:memory-dec}). In the full-batch case, the same holds true except $\momfunc_{\ell}^{(n)} \equiv \momfunc_{\ell}$ are not allowed to depend on $n$.\hfill$\lrcorner$
\end{remark}

\cref{rem:fn-as-a-func-of-mom-vars} considers the special case of optimization methods based on momentum variables. For instance, in the case of AdamW (\cref{ex:adamw}), \cref{rem:fn-as-a-func-of-mom-vars} applies with $\numofmoms = 3$,
\begin{gather*}
  \momfunc_{1}(\btheta) = \nabla \loss(\btheta),\quad \momfunc_{2}(\btheta) = \prn[\big]{{\nabla \loss(\btheta)}}^2,\quad \momfunc_{3}(\btheta) = \btheta,\\
  \bcor{1}{n} = \frac{1 - \beta_1}{1 - \beta_1^{n + 1}} \to \bcorc{1} = 1 - \beta_1,\\
  \bcor{2}{n} = \frac{1 - \beta_2}{1 - \beta_2^{n + 1}} \to \bcorc{2} = 1 - \beta_2,\\
  \bcor{3}{n} \equiv \bcorc{3} = \lambda,\quad \beta_3 = 0.
\end{gather*}
In the case of Lion-$\mathcal{K}$ (\cref{ex:lion-K}), the remark applies with $\numofmoms = 3$,
\begin{gather*}
  \momfunc_{1}(\btheta) = - \nabla \loss(\btheta),\quad \momfunc_{2}(\btheta) = - \nabla \loss(\btheta),\quad \momfunc_{3}(\btheta) = \btheta,\\
  \beta_1 = \rho_2,\quad \beta_2 = 0,\quad \beta_3 = 0,\\
  \bcor{1}{n} \equiv \bcorc{1} = (1 - \rho_2) \frac{\rho_1}{\rho_2},\\
  \bcor{2}{n} \equiv \bcorc{2} = 1 - \frac{\rho_1}{\rho_2},\\
  \bcor{3}{n} \equiv \bcorc{3} = \lambda.
\end{gather*}
We used the letter $\rho$ when defining the Lion iteration to avoid confusion with the $\beta$ in the definition of momentum variables.

\subsection{Deriving the Memoryless Approximation}\label{sec:deriving-memoryless-approximation}

By Taylor expansion with the Lagrange remainder,
\begin{align*}
  &F_r^{(n)}(\btheta^{(n)}, \ldots, \btheta^{(0)}) - F_r^{(n)}(\btheta^{(n)}, \ldots, \btheta^{(n)})\\
  &\quad = \sum_{k = 1}^n \prn[\big]{\btheta^{(n - k)} - \btheta^{(n)}}\trans \frac{\partial F_r^{(n)}}{\partial \btheta^{(n - k)}}(\btheta^{(n)}, \ldots, \btheta^{(n)})\\
  &\qquad + \frac{1}{2} \sum_{k_1, k_2 = 1}^n \prn[\big]{\btheta^{(n - k_1)} - \btheta^{(n)}}\trans \frac{\partial^2 F_r^{(n)}}{\partial \btheta^{(n - k_1)} \partial \btheta^{(n - k_2)}}(\bzeta) \prn[\big]{\btheta^{(n - k_2)} - \btheta^{(n)}}\\
  &\quad = \sum_{k = 1}^n \prn[\big]{\btheta^{(n - k)} - \btheta^{(n)}}\trans \frac{\partial F_r^{(n)}}{\partial \btheta^{(n - k)}}(\btheta^{(n)}, \ldots, \btheta^{(n)}) + O(\lr^2),\numberthis\label{eq:llfjh}
\end{align*}
where $\bzeta$ is some point on the segment between $\prn[\big]{\btheta^{(n)}, \ldots, \btheta^{(0)}}$ and $\prn[\big]{\btheta^{(n)}, \ldots, \btheta^{(n)}}$; in the last step we used \cref{eq:second-der-bound}, $\btheta^{(n - k_1)} - \btheta^{(n)} = O(k_1 \lr)$, and $\btheta^{(n - k_2)} - \btheta^{(n)} = O(k_2 \lr)$.

Next, write
\begin{align*}
  &\btheta^{(n - k)} - \btheta^{(n)}\\
  &\quad = \sum_{s = n - k}^{n - 1} \prn[\big]{\btheta^{(s)} - \btheta^{(s + 1)}}\\
  &\quad = \lr \sum_{s = n - k}^{n - 1} \boldsymbol{F}^{(s)}(\btheta^{(s)}, \ldots, \btheta^{(0)})\\
  &\quad = \lr \sum_{s = n - k}^{n - 1} \boldsymbol{F}^{(s)}(\btheta^{(n)}, \ldots, \btheta^{(n)}) + O(k^2 \lr^2),
\end{align*}
where in the last step we used $\boldsymbol{F}^{(s)}(\btheta^{(s)}, \ldots, \btheta^{(0)}) - \boldsymbol{F}^{(s)}(\btheta^{(n)}, \ldots, \btheta^{(n)}) = O((n - s) \lr)$, which follows from Taylor expansion and \cref{eq:first-der-bound}. Insert this into \cref{eq:llfjh} and use \cref{eq:first-der-bound} again to continue:
\begin{align*}
  &F_r^{(n)}(\btheta^{(n)}, \ldots, \btheta^{(0)}) - F_r^{(n)}(\btheta^{(n)}, \ldots, \btheta^{(n)})\\
  &\quad = \lr \sum_{k = 1}^n \frac{\partial F_r^{(n)}}{\partial \btheta^{(n - k)}}(\btheta^{(n)}, \ldots, \btheta^{(n)})\trans \sum_{s = n - k}^{n - 1} \boldsymbol{F}^{(s)}(\btheta^{(n)}, \ldots, \btheta^{(n)}) + O(\lr^2).
\end{align*}

\textit{We conclude that the original numerical iteration can be rewritten as}
\begin{equation*}
\begin{multlined}
  \btheta^{(n + 1)} = \btheta^{(n)} - \lr \brk[\big]{\boldsymbol{F}^{(n)}(\btheta^{(n)}) + \corr{n}(\btheta^{(n)})} + O(\lr^3),
\end{multlined}
\end{equation*}
where the linear in $\lr$ correction function is defined as $\corr{n}=(\corrsc{1}{n}, \ldots, \corrsc{d}{n})\trans$ with
\begin{equation}\label{eq:correction-function-general-def}
  \corrsc{r}{n}(\btheta)\numberthis := \lr \sum_{k = 1}^n \frac{\partial F_r^{(n)}}{\partial \btheta^{(n - k)}}(\btheta)\trans \sum_{s = n - k}^{n - 1} \boldsymbol{F}^{(s)}(\btheta).
\end{equation}

The derivation of the memoryless iteration is now complete. To make it easier to apply it to our examples, we can write it more specifically as the following section shows.

\subsection{\texorpdfstring{$\boldsymbol{F}^{(n)}$}{Fn} as a Function of Momentum Variables}

Specializing to the setup of \cref{rem:fn-as-a-func-of-mom-vars}, and for any $s, n \in \mathbb{Z}_{\geq 0}$, $\boldsymbol{F}^{(s)}(\btheta) = \extmomfunc\prn[\big]{\gbar{1}{s + 1}(\btheta), \ldots, \gbar{\numofmoms}{s + 1}(\btheta)}$, where $\gbar{\ell}{s + 1}(\btheta) := \bcor{\ell}{n} \sum_{k = 0}^s \beta_{\ell}^k \momfunc_{\ell}^{(n - k)}(\btheta)$, and
\begin{align*}
  &\frac{\partial F_r^{(n)}}{\partial \btheta^{(n - k)}}(\btheta)\\
  &\quad = \sum_{\ell = 1}^{\numofmoms} \sum_i \bcor{\ell}{n} \beta_\ell^k \frac{\partial \extmomfuncsc_r}{\partial m_{\ell; i}}\prn[\big]{\gbar{1}{n + 1}(\btheta), \ldots, \gbar{\numofmoms}{n + 1}(\btheta)}\trans \nabla \momfuncsc^{(n - k)}_{\ell; i}(\btheta).
\end{align*}
Therefore, in this special case, the correction term in the memoryless iteration \eqref{eq:memoryless-iteration} is given by, for $r=1,\ldots,d$,
\begin{align*}
  &\corrsc{r}{n}(\btheta)\\
  &\quad = \lr \sum_{\ell = 1}^{\numofmoms} \sum_i \bcor{\ell}{n} \frac{\partial \extmomfuncsc_r}{\partial m_{\ell; i}}\prn[\big]{\gbar{1}{n + 1}(\btheta), \ldots, \gbar{\numofmoms}{n + 1}(\btheta)}\\
  &\qquad \times \sum_{k = 1}^n \beta_\ell^k \nabla \momfuncsc_{\ell; i}^{(n - k)}(\btheta)\trans \sum_{s = n - k}^{n - 1} \extmomfunc\prn[\big]{\gbar{1}{s + 1}(\btheta), \ldots, \gbar{\numofmoms}{s + 1}(\btheta)}.
\end{align*}

In the full-batch case $\momfunc_{\ell}^{(n)}(\btheta) \equiv \momfunc_{\ell}(\btheta)$, this can be simplified further.
Let us assume $\bcor{\ell}{n} \underset{n \to \infty}{\longrightarrow} \bcorc{\ell}$, where $\bcorc{\ell}$ is constant in $n$. Then $\gbar{\ell}{n + 1}(\btheta)$ also become constant in $n$: specifically, they settle to $\gbar{\ell}{}(\btheta) := \bcorc{\ell} (1 - \beta_{\ell})^{-1} \momfunc_{\ell}(\btheta)$. \cref{lem:decaying-sums} \internalComment{mb details} then implies that the iteration becomes close to
\begin{equation*}
\btheta^{(n + 1)} = \btheta^{(n)} - \lr \brk[\big]{\extmomfunc(\gbar{1}{}(\btheta^{(n)}), \ldots, \gbar{\numofmoms}{}(\btheta^{(n)})) + \corr{n}(\btheta)}
\end{equation*}
with
\begin{align*}
  &\corrsc{r}{n}(\btheta)\\
  &\quad = \lr \sum_{\ell = 1}^{\numofmoms} \sum_i \frac{\bcorc{\ell} \beta_{\ell}}{(1 - \beta_{\ell})^2} \frac{\partial \extmomfuncsc_r}{\partial m_{\ell; i}}\prn[\big]{\gbar{1}{}(\btheta^{(n)}), \ldots, \gbar{\numofmoms}{}(\btheta^{(n)})}\\
  &\qquad \times \nabla \momfuncsc_{\ell; i}(\btheta^{(n)})\trans \extmomfunc\prn[\big]{\gbar{1}{}(\btheta^{(n)}), \ldots, \gbar{\numofmoms}{}(\btheta^{(n)})}.
\end{align*}
\internalComment{where we used that $\sum_{k = 1}^n k \beta_{\ell}^k = (1 - \beta_{\ell})^{-2} \beta_{\ell} - (1 - \beta_{\ell})^{-2} \beta_{\ell}^{n + 1}(n + 1 - n \beta_{\ell}) \to (1 - \beta_{\ell})^{-2} \beta_{\ell}$.}

Correction terms for all examples are provided in \cref{sec:correction-terms-for-all-examples}.

\subsection{Approximation Bound}

An argument similar to the derivation in \cref{sec:deriving-memoryless-approximation} can be made to obtain the following result.

\begin{theorem}[Memoryless approximation: 1-step error bound]\label{th:general-momentum-methods}
Under \cref{ass:form-of-fn}, there exists a discrete memoryless iteration $\crl[\big]{\tilde{\btheta}^{(n)}}_{n = 0}^{\infty}$ satisfying \eqref{eq:memoryless-iteration} with initial condition $\tilde{\btheta}^{(0)} = \btheta^{(0)}$, correction function defined in \cref{eq:correction-function-general-def}, and a constant $\nC{localerrorbound}$ not depending on $\lr$, such that
\begin{align*}
    \sup_{n \in \mathbb{Z}_{\geq0}} \big\|\tilde{\btheta}^{(n + 1)} - \tilde{\btheta}^{(n)}
    + \lr \boldsymbol{F}^{(n)}(\tilde{\btheta}^{(n)}, \ldots, \tilde{\btheta}^{(0)}) \big\|_\infty \leq \oC{localerrorbound} \lr^3.
\end{align*}

\end{theorem}

The proof is available in \cref{sec:proof-of-local}.

The importance of this one-step approximation result is that it allows to bound the global error between the memoryfull iteration $\btheta^{(n)}$ and memoryless iteration $\tilde{\btheta}^{(n)}$ on a finite time horizon.

\begin{corollary}[General momentum methods: global error bound on a finite time horizon]\label{cor:global-error}
In the setting of \cref{th:general-momentum-methods}, let $\crl{\btheta^{(n)}}_{n \in \mathbb{Z}_{\geq 0}}$ be the sequence of vectors generated by the iteration in \cref{eq:general-iteration} with initial condition $\btheta^{(0)}$. Let $T \geq 0$ be a fixed time horizon. Then there exists a constant $\nC{globalerrorbound}$, depending on $T$ but independent of $\lr$, such that $\max_{n \in \range{0}{\lfloor T / \lr \rfloor}} \norm[\big]{\btheta^{(n)} - \tilde{\btheta}^{(n)}}_\infty \leq \oC{globalerrorbound} \lr^2$.
\end{corollary}

The proof is in \cref{sec:proof-of-global}.

\section{Application: Memory Anti-Regularizes AdamW but not Lion}\label{sec:adamw-anti-reg-but-lion-not}

We first study AdamW with memory by an application of \cref{th:general-momentum-methods,cor:global-error}.
Neglecting coefficients decaying to zero exponentially fast, we have
\begin{equation*}
  \btheta^{(n + 1)} = (1 - \lr \lambda) \btheta^{(n)} - \lr \prn[\bigg]{\underbrace{\frac{\nabla \loss(\btheta^{(n)})}{\sqrt{\prn[\big]{\nabla \loss(\btheta^{(n)})}^2 + \varepsilon}}}_{\approx \; \sign(\nabla \loss(\btheta^{(n)}))} + \corr{n}(\btheta^{(n)})},
\end{equation*}
where $\corr{n}(\btheta)$ is given by
\begin{equation*}
  \lr \prn[\bigg]{\frac{\beta_1 (1 - \beta_1)^{-1} - \beta_2 (1 - \beta_2)^{-1}}{\prn[\big]{\abs[\big]{\nabla \loss(\btheta)}^2 + \varepsilon}^{1 / 2}} + \frac{{\color{blue} \varepsilon} \beta_2 (1 - \beta_2)^{-1}}{\prn[\big]{\abs[\big]{\nabla \loss(\btheta)}^2 + \varepsilon}^{3 / 2}}} \prn[\big]{\nabla \norm{\nabla \loss(\btheta)}_{1, \varepsilon} + \lambda \nabla^2 \loss(\btheta) \btheta}.
\end{equation*}
Here $\norm{\cdot}_{1, \varepsilon}$ is the perturbed one-norm defined as $\norm{\bv}_{1, \varepsilon} := \sum_{i = 1}^d \sqrt{v_i^2 + \varepsilon}$. Taking $\varepsilon$ to zero, we can write this in the form of preconditioned gradient descent (with decoupled weight decay):
\begin{equation*}
\btheta^{(n + 1)} = (1 - \lambda \lr) \btheta^{(n)} - \lr \frac{\nabla \tilde{\loss}(\btheta^{(n)})}{\abs[\big]{\prn[\big]{\nabla \loss(\btheta^{(n)})}}},
\end{equation*}
where
\begin{equation*}
  \tilde{\loss}(\btheta) = \loss(\btheta) + \lr \prn[\bigg]{\frac{\beta_1}{1 - \beta_1} - \frac{\beta_2}{1 - \beta_2}} \norm{\nabla \loss(\btheta)}_1 + \lr \lambda \prn[\bigg]{\frac{\beta_1}{1 - \beta_1} - \frac{\beta_2}{1 - \beta_2}} \prn{\nabla \loss(\btheta)\trans \btheta - \loss(\btheta)}
\end{equation*}
is the modified loss. We see that the correction term $\beta_1 / (1 - \beta_1) \norm{\nabla \loss(\btheta)}_1$, coming from exponentially averaging the gradients, penalizes the one-norm of the $\nabla \loss$, acting as a regularizer, but the correction term $-\beta_2 / (1 - \beta_2) \norm{\nabla \loss(\btheta)}_1$, coming from exponentially averaging the squared gradient components, anti-penalizes this one-norm and acts as an anti-regularizer. Moreover, if $\beta_1 < \beta_2$ (essentially always in practice), the anti-regularization is stronger than the regularization. Thus, \textit{memory anti-regularizes AdamW}.

Consider now Lion-$\mathcal{K}$ (\cref{ex:lion-K}). Neglecting terms going to zero exponentially fast as $n \to \infty$, the memoryless iteration is
\begin{equation*}
  \btheta^{(n + 1)} = (1 - \lr \lambda) \btheta^{(n)} - \lr \brk[\big]{- \nabla \mathcal{K}\prn[\big]{- \nabla \loss(\btheta^{(n)})} + \corr{n}(\btheta^{(n)})},
\end{equation*}
where
\begin{equation*}
  \corr{n}(\btheta) = - \lr \frac{\rho_1}{1 - \rho_2} \nabla^2 \mathcal{K}(- \nabla \loss(\btheta))  \nabla^2 \loss(\btheta) \brk[\big]{\nabla \mathcal{K}(- \nabla \loss(\btheta)) - \lambda \btheta}.
\end{equation*}

As mentioned above, ordinary Lion is recovered by setting $\mathcal{K}(\bx) = \norm{\bx}_1$. This function is not differentiable, so let us replace it with the smooth convex approximation $\norm{\bx}_{1, \varepsilon}$, where $\varepsilon$ is a small positive constant. The results of \cref{sec:identifying-the-effect-of-memory} can be applied, and the memoryless iteration is
\begin{equation*}
  \btheta^{(n + 1)} = (1 - \lambda \lr) \btheta^{(n)} - \lr \brk[\bigg]{\frac{\nabla \loss(\btheta^{(n)})}{\prn[\big]{\abs{\nabla \loss(\btheta^{(n)})}^2 + \varepsilon}^{1 / 2}}
    + \corr{n}(\btheta^{(n)})},
\end{equation*}
where
\begin{equation*}
  \corrsc{r}{n}(\btheta) = \lr \frac{\rho_1}{1 - \rho_2} \frac{\color{blue} \varepsilon}{\prn[\big]{\abs{\nabla_r \loss(\btheta)}^2 + \varepsilon}^{3 / 2}}  \nabla_r \brk[\big]{\norm{\nabla \loss(\btheta)}_{1, \varepsilon} + \lambda (\nabla \loss(\btheta)\trans \btheta - \loss(\btheta))}.
\end{equation*}
This term is small as long as $\varepsilon$ is small. Therefore, better generalization of Lion on a number of tasks \citep{chen2023symbolic} may be attributed to the fact that memory does \textit{not} anti-regularize Lion (while it does anti-regularize AdamW).

\section{Further Implications}\label{sec:further-implications}

\subsection{Modified Equations}\label{sec:modified-equations}

We have taken a very general algorithm \eqref{eq:general-iteration} and, assuming smoothness and that memory decays sufficiently fast, converted it to a memoryless iteration $\btheta^{(n + 1)} = \btheta^{(n)} - \lr \brk[\big]{\boldsymbol{F}^{(n)}(\btheta^{(n)}) + \corr{n}(\btheta^{(n)})} + O(\lr^3)$. In this section we find an ODE in the form $\dot{\btheta} = \boldsymbol{G}_\lr(\btheta)$ whose continuous solution, with initial condition $\btheta^{(0)}$, will approximate the memoryless iteration established in \cref{sec:identifying-the-effect-of-memory}. Let us derive $\boldsymbol{G}_\lr(\btheta)$ in the form of a power series $\boldsymbol{G}_1(\btheta) + \lr \boldsymbol{G}_2(\lr) + O(\lr^2)$, where $O(\lr^2)$ means ``terms of order at least two in $\lr$''. Relating the iteration number $n$ of a discrete iteration and the time point $t = n \lr$ on a continuous trajectory, we would like the continuous trajectory to satisfy the same one-step relation as the discrete iteration, up to $O(\lr^3)$:
\begin{equation*}
  \btheta((n + 1) \lr) = \btheta(n \lr) - \lr \brk[\big]{\boldsymbol{F}^{(n)}(\btheta(n \lr), \ldots, \btheta(n \lr)) + \corr{n}(\btheta(n \lr))} + O(\lr^3).
\end{equation*}
In fact, we will ensure it is true for $n \lr$ replaced by any $t$:
\begin{equation}\label{eq:continuous-iteration-to-match}
  \btheta(t + \lr) = \btheta(t) - \lr \brk[\big]{\boldsymbol{F}^{(n)}(\btheta(t), \ldots, \btheta(t)) + \corr{n}(\btheta(t))} + O(\lr^3).
\end{equation}
But, using a Taylor expansion, and recalling that we are finding the trajectory satisfying $\dot{\btheta}(t) = \boldsymbol{G}_\lr(\btheta(t))$, hence $\ddot{\btheta}(t) = \nabla \boldsymbol{G}_\lr(\btheta(t)) \dot{\btheta}(t)$, we have
\begin{align*}
  \btheta(t + \lr) &= \btheta(t) + \lr \dot{\btheta}(t) + \frac{\lr^2}{2} \ddot{\btheta}(t) + O(\lr^3)\\
                 &= \btheta(t) + \lr \crl[\big]{\boldsymbol{G}_1(\btheta(t)) + \lr \boldsymbol{G}_2(\btheta(t)) + O(\lr^2)}\\
                 &\quad + \frac{\lr^2}{2} \crl[\big]{\nabla \boldsymbol{G}_1(\btheta(t)) \boldsymbol{G}_1(\btheta(t)) + O(\lr)} + O(\lr^3)\\
                 &= \btheta(t) + \lr \boldsymbol{G}_1(\btheta(t))\\
                 &\quad + \lr^2 \crl[\bigg]{\boldsymbol{G}_2(\btheta(t)) + \frac{\nabla \boldsymbol{G}_1(\btheta(t)) \boldsymbol{G}_1(\btheta(t))}{2}} + O(\lr^3).
\end{align*}

In order to match \eqref{eq:continuous-iteration-to-match}, we need to have
\begin{align*}
  &\boldsymbol{G}_1(\btheta) = - \boldsymbol{F}^{(n)}(\btheta, \ldots, \btheta),\\
  &\boldsymbol{G}_2(\btheta) = - \prn[\bigg]{\corr{n}(\btheta) / \lr + \frac{\nabla \boldsymbol{G}_1(\btheta) \boldsymbol{G}_1(\btheta)}{2}}.
\end{align*}
So, apart from the correction term coming from memory, the ODE $\dot{\btheta} = \boldsymbol{G}_1(\btheta) + \lr \boldsymbol{G}_2(\lr)$ derived has another term
\begin{equation*}
\lr^2 \frac{\nabla \boldsymbol{G}_1(\btheta) \boldsymbol{G}_1(\btheta)}{2}
\end{equation*}
arising from the fact that the algorithm is discrete.

For the example of full-batch heavy-ball momentum GD as in \cref{sec:developing-intuition}, where $\boldsymbol{G}_1(\btheta) = - (1 - \beta)^{-1} \nabla \loss(\btheta)$ (ignoring coefficients going to zero exponentially fast in $n$), this additional term is equal to $\lr^2 (1 - \beta)^{-2} \nabla \norm{\nabla \loss(\btheta)}^2 / 4$, providing additional implicit regularization. We recover the ODE derived by \citet{kovachki2021continuous,ghosh2023implicit}:
\begin{equation*}
\dot{\btheta} = - \frac{\nabla \loss(\btheta)}{1 - \beta} - \lr \frac{1 + \beta}{(1 - \beta)^3} \frac{\nabla \norm{\nabla \loss(\btheta)}^2}{4}.
\end{equation*}

\subsection{Mini-Batch Training}

In specific cases, it is possible to identify the additional implicit regularization that is introduced to the algorithm by noise, if mini-batch training is used as opposed to full-batch. Assume that the form of $\boldsymbol{F}^{(n)}(\btheta^{(n)}, \ldots, \btheta^{(0)})$ is given by
\begin{equation*}
\boldsymbol{F}^{(n)}(\btheta^{(n)}, \ldots, \btheta^{(0)}) = \sum_{k = 0}^n \beta^k \boldsymbol{g}^{(n - k)}(\btheta^{(n - k)}),
\end{equation*}
where the $\boldsymbol{g}^{(n - k)}(\cdot)$ functions are uniformly bounded along with their derivatives.

The correction term introduced by memory \eqref{eq:correction-function-general-def} is
\begin{align*}
  \corrsc{r}{n}(\btheta) &:= \lr \sum_{k = 1}^n \frac{\partial F_r^{(n)}}{\partial \btheta^{(n - k)}}(\btheta, \ldots, \btheta)\trans \sum_{s = n - k}^{n - 1} \boldsymbol{F}^{(s)}(\btheta, \ldots, \btheta)\\
  &= \lr \beta \sum_{k = 0}^{n - 1} \beta^k \nabla g_r^{(n - 1 - k)}(\btheta)\trans \sum_{l = 1}^{k + 1} \sum_{b = 0}^{n - l} \beta^{b} \boldsymbol{g}^{(n - l - b)}(\btheta).
\end{align*}

Assume that $n$ is a (large) number of mini-batches in one epoch, and mini-batches are sampled randomly without replacement, with each permutation equally likely. Let us take the expectation of the correction term with respect to this randomness, that is, take the average over all re-orderings $\pi$ of $(\boldsymbol{g}^{(0)}, \ldots, \boldsymbol{g}^{(n)})$:
\begin{align*}
  &\Expectlet \sum_{k = 0}^{n - 1} \beta^k \nabla g_r^{(n - 1 - k)}(\btheta)\trans \sum_{l = 1}^{k + 1} \sum_{b = 0}^{n - l} \beta^{b} \boldsymbol{g}^{(n - l - b)}(\btheta)\\
  &\quad := \frac{1}{(n + 1)!} \sum_{\pi} \sum_{k = 0}^{n - 1} \beta^k \nabla g_r^{(\pi(n - 1 - k))}(\btheta)\trans\\
  &\qquad \times \sum_{l = 1}^{k + 1} \sum_{b = 0}^{n - l} \beta^{b} \boldsymbol{g}^{(\pi(n - l - b))}(\btheta).
\end{align*}
Note that $\Expectlet \nabla g_r^{(i)}(\btheta)\trans \boldsymbol{g}^{(j)}(\btheta)$ depends only on whether $i = j$ or $i \neq j$. Therefore,
\begin{align*}
  \Expect{\corrsc{r}{n}(\btheta)} / \lr &= \oC{twoeq}(\beta) \Expect{\nabla g_r^{(1)}(\btheta)\trans \boldsymbol{g}^{(1)}(\btheta)}\\
  &+ \oC{twoneq}(\beta) \Expect{\nabla g_r^{(1)}(\btheta)\trans \boldsymbol{g}^{(2)}(\btheta)},
\end{align*}
where $\nC{twoeq}(\beta)$ and $\nC{twoneq}(\beta)$ can be calculated as
\begin{align*}
  &\oC{twoeq}(\beta) := \beta \sum_{b = 0}^{n - 1} \beta^b \sum_{l = 1}^{b + 1} \beta^{b + 1 - l} \underset{n \to \infty}{\longrightarrow} \frac{\beta}{(1 - \beta)^2 (1 + \beta)},\\
  &\oC{twoneq}(\beta) := \beta \sum_{k = 0}^{n - 1} \beta^k \sum_{l = 1}^{k + 1} \sum_{b = 0}^{n - l} \beta^{b} - \oC{twoeq}(\beta) \underset{n \to \infty}{\longrightarrow} \frac{2 \beta^2}{(1 - \beta)^3 (1 + \beta)}.
  %&\qquad \underset{n \to \infty}{\longrightarrow} \frac{\beta}{(1 - \beta)^3} - \frac{\beta}{(1 - \beta)^2 (1 + \beta)}\\
\end{align*}

We can simplify
\begin{align*}
  &\Expect{\nabla g_r^{(1)}(\btheta)\trans \boldsymbol{g}^{(2)}(\btheta)} = \frac{1}{(n + 1) n} \sum_{i \neq j} \nabla g_r^{(i)}(\btheta)\trans \boldsymbol{g}^{(j)}(\btheta)\\
  &\quad = \nabla g_r(\btheta)\trans \boldsymbol{g}(\btheta) + o_n(1),
\end{align*}
where $\boldsymbol{g}(\btheta) = \Expectlet \boldsymbol{g}^{(1)}(\btheta) = (n + 1)^{-1} \sum_{k = 0}^{n + 1} \boldsymbol{g}^{(k)}(\btheta)$ is the average of $\crl{\boldsymbol{g}^{(k)}(\btheta)}$, $o_n(1)$ tends to zero as $n \to \infty$.

So, for large $n$ we can write
\begin{align*}
  &\Expect{\corrsc{r}{n}(\btheta)} / \lr\\
  &\quad \approx \oC{twoeq}(\beta) \Expect{\nabla g_r^{(1)}(\btheta)\trans \boldsymbol{g}^{(1)}(\btheta)} + \oC{twoneq}(\beta) \nabla g_r(\btheta)\trans \boldsymbol{g}(\btheta)\\
  &\quad = \prn[\big]{\oC{twoeq}(\beta) + \oC{twoneq}(\beta)} \nabla g_r(\btheta)\trans \boldsymbol{g}(\btheta)\\
  &\qquad + \oC{twoeq}(\beta) \Expect{\prn{\nabla g_r^{(1)}(\btheta) - \nabla g_r(\btheta)}\trans \prn{\boldsymbol{g}^{(1)}(\btheta) - \boldsymbol{g}(\btheta)}}\\
  &\quad \approx \frac{\beta}{(1 - \beta)^3} \nabla g_r(\btheta)\trans \boldsymbol{g}(\btheta)\\
  &\qquad + \oC{twoeq}(\beta) \Expect{\prn{\nabla g_r^{(1)}(\btheta) - \nabla g_r(\btheta)}\trans \prn{\boldsymbol{g}^{(1)}(\btheta) - \boldsymbol{g}(\btheta)}}.
\end{align*}
The second term can be interpreted as additional implicit regularization by noise.

For example, take $\boldsymbol{g}^{(k)}(\btheta) = \nabla \loss^{(k)}(\btheta)$ the $k$th minibatch loss. Then we obtained that ``on average'' mini-batch GD with momentum is given by the iteration like \eqref{eq:hb-mod-loss}, except the modified loss has an additional term:
\begin{equation*}
  \tilde{\loss}(\btheta) = \loss(\btheta) + \frac{\lr \beta}{2 (1 - \beta)^2} \norm{\nabla \loss(\btheta)}^2 + \frac{\lr \beta}{2 (1 - \beta) (1 + \beta)} \Expectlet \norm{\nabla \loss^{(1)}(\btheta) - \nabla \loss(\btheta)}^2.
\end{equation*}

\section*{Acknowledgments}

We thank Boris Hanin for his comments. Cattaneo gratefully acknowledges financial support from the National Science Foundation through DMS-2210561 and SES-2241575. We acknowledge the Princeton Research Computing resources, coordinated by the Princeton Institute for Computational Science and Engineering (PICSciE) and the Office of Information Technology's Research Computing.

\printbibliography

\newpage
\appendix
\onecolumn

\section{Proof of \cref{th:general-momentum-methods}}\label{sec:proof-of-local}

Since, by the assumptions of the theorem,
\begin{align*}
  &\tilde{\theta}_r^{(n + 1)} - \tilde{\theta}_r^{(n)} = - \lr F_r^{(n)}(\tilde{\btheta}^{(n)}, \ldots, \tilde{\btheta}^{(n)})\\
  &\quad - \lr^2 \underline{\sum_{k = 1}^n \frac{\partial F_r^{(n)}}{\partial \btheta^{(n - k)}}(\tilde{\btheta}^{(n)}, \ldots, \tilde{\btheta}^{(n)})\trans \sum_{s = n - k}^{n - 1} \boldsymbol{F}^{(s)}(\tilde{\btheta}^{(n)}, \ldots, \tilde{\btheta}^{(n)})},
\numberthis\label{eq:jjfhhhtb}
\end{align*}
we need to show that
\begin{align*}
  &F_r^{(n)}(\tilde{\btheta}^{(n)}, \ldots, \tilde{\btheta}^{(0)}) -
    F_r^{(n)}(\tilde{\btheta}^{(n)}, \ldots, \tilde{\btheta}^{(n)})\\
  &\quad = \lr \sum_{k = 1}^n \frac{\partial F_r^{(n)}}{\partial \btheta^{(n - k)}}(\tilde{\btheta}^{(n)}, \ldots, \tilde{\btheta}^{(n)})\trans \sum_{s = n - k}^{n - 1} \boldsymbol{F}^{(s)}(\tilde{\btheta}^{(n)}, \ldots, \tilde{\btheta}^{(n)})
    + O(\lr^2).\numberthis\label{eq:general-momentum-methods-nts}
\end{align*}

By Taylor expansion with the Lagrange remainder,
\begin{align*}
  &F_r^{(n)}(\tilde{\btheta}^{(n)}, \ldots, \tilde{\btheta}^{(0)}) - F_r^{(n)}(\tilde{\btheta}^{(n)}, \ldots, \tilde{\btheta}^{(n)})\\
  &\quad = \sum_{k = 1}^n \frac{\partial F_r^{(n)}}{\partial \btheta^{(n - k)}}(\tilde{\btheta}^{(n)}, \ldots, \tilde{\btheta}^{(n)})\trans \prn[\big]{\tilde{\btheta}^{(n - k)} - \tilde{\btheta}^{(n)}}\\
  &\qquad + \frac{1}{2} \sum_{k_1, k_2 = 1}^n \prn[\big]{\tilde{\btheta}^{(n - k_1)} - \tilde{\btheta}^{(n)}}\trans \frac{\partial^2 F_r^{(n)}}{\partial \btheta^{(n - k_1)} \partial \btheta^{(n - k_2)}}(\bzeta) \prn[\big]{\tilde{\btheta}^{(n - k_2)} - \tilde{\btheta}^{(n)}}\\
  &\quad \labrel{1}[=] \sum_{k = 1}^n \frac{\partial F_r^{(n)}}{\partial \btheta^{(n - k)}}(\tilde{\btheta}^{(n)}, \ldots, \tilde{\btheta}^{(n)})\trans \prn[\big]{\tilde{\btheta}^{(n - k)} - \tilde{\btheta}^{(n)}} + O(\lr^2),\numberthis\label{eq:ttfhw}
\end{align*}
where $\bzeta$ is some point on the segment between $\prn[\big]{\tilde{\btheta}^{(n)}, \ldots, \tilde{\btheta}^{(0)}}$ and $\prn[\big]{\tilde{\btheta}^{(n)}, \ldots, \tilde{\btheta}^{(n)}}$; in \labrel{1} we used \cref{eq:second-der-bound} and $\tilde{\btheta}^{(n - k_1)} - \tilde{\btheta}^{(n)} = O(k_1 \lr)$, $\tilde{\btheta}^{(n - k_2)} - \tilde{\btheta}^{(n)} = O(k_2 \lr)$.
\internalComment{
\begin{equation}\label{eq:xfnc}
  \frac{\partial F_r^{(n)}}{\partial \btheta^{(n - k)}}(\tilde{\btheta}^{(n)}, \ldots, \tilde{\btheta}^{(n)}) = \sum_{\ell = 1}^{\numofmoms} \bcor{\ell}{n} \beta_\ell^k \sum_i \frac{\partial \extmomfuncsc_r}{\partial m_{\ell; i}}\prn[\big]{\gbar{1}{n + 1}(\tilde{\btheta}^{(n)}), \ldots, \gbar{\numofmoms}{n + 1}(\tilde{\btheta}^{(n)})} \nabla \momfuncsc_{\ell; i}(\tilde{\btheta}^{(n)}).
\end{equation}
}

Since the underlined term in \cref{eq:jjfhhhtb} is $O(1)$, we have
\begin{align*}
  &\tilde{\btheta}^{(n - k)} - \tilde{\btheta}^{(n)} = \sum_{s = n - k}^{n - 1} \prn[\big]{\tilde{\btheta}^{(s)} - \tilde{\btheta}^{(s + 1)}}\\
  &\quad = \lr \sum_{s = n - k}^{n - 1} \crl[\big]{\boldsymbol{F}^{(s)}(\tilde{\btheta}^{(s)}, \ldots, \tilde{\btheta}^{(s)}) + O(\lr)}\\
  &\quad = \lr \sum_{s = n - k}^{n - 1} \boldsymbol{F}^{(s)}(\tilde{\btheta}^{(n)}, \ldots, \tilde{\btheta}^{(n)}) + O(k^2 \lr^2),
\end{align*}
where in the last step we used $\boldsymbol{F}^{(s)}(\tilde{\btheta}^{(s)}, \ldots, \tilde{\btheta}^{(s)}) - \boldsymbol{F}^{(s)}(\tilde{\btheta}^{(n)}, \ldots, \tilde{\btheta}^{(n)}) = O((n - s) \lr)$, which follows from Taylor expansion, \cref{eq:first-der-bound} and $\tilde{\btheta}^{(n + 1)} - \tilde{\btheta}^{(n)} = O(\lr)$. Combine this with \cref{eq:ttfhw} to get
\begin{align*}
  &F_r^{(n)}(\tilde{\btheta}^{(n)}, \ldots, \tilde{\btheta}^{(0)}) - F_r^{(n)}(\tilde{\btheta}^{(n)}, \ldots, \tilde{\btheta}^{(n)})\\
  &\quad = \sum_{k = 1}^n \frac{\partial F_r^{(n)}}{\partial \btheta^{(n - k)}}(\tilde{\btheta}^{(n)}, \ldots, \tilde{\btheta}^{(n)})\trans \crl[\bigg]{\lr \sum_{s = n - k}^{n - 1} \boldsymbol{F}^{(s)}(\tilde{\btheta}^{(n)}, \ldots, \tilde{\btheta}^{(n)}) + O(k^2 \lr^2)} + O(\lr^2)\\
  &\quad = \lr \sum_{k = 1}^n \frac{\partial F_r^{(n)}}{\partial \btheta^{(n - k)}}(\tilde{\btheta}^{(n)}, \ldots, \tilde{\btheta}^{(n)})\trans \sum_{s = n - k}^{n - 1} \boldsymbol{F}^{(s)}(\tilde{\btheta}^{(n)}, \ldots, \tilde{\btheta}^{(n)}) + O(\lr^2),
\end{align*}
which is \eqref{eq:general-momentum-methods-nts}, and the proof is complete.

\section{Proof of \cref{cor:global-error}}\label{sec:proof-of-global}

We follow a standard argument, e.\,g. \citet{ghosh2023implicit,pmlr-v235-cattaneo24a}. We prove the following claim by induction over $n \in \mathbb{Z}_{\geq 0}$:
\begin{equation*}
\norm{\btheta^{(n)} - \tilde{\btheta}^{(n)}}_{\infty} \leq d_1 e^{d_2 n \lr} \lr^2,\quad\norm{\btheta^{(n + 1)} - \tilde{\btheta}^{(n + 1)} - \btheta^{(n)} + \tilde{\btheta}^{(n)}}_{\infty} \leq d_3 e^{d_2 n \lr} \lr^3,
\end{equation*}
where
\begin{equation*}
d_1 = \oC{localerrorbound},\quad d_2 = 1 + d \sum_{k = 0}^\infty \gamma_k,\quad d_3 = \oC{localerrorbound} d_2.
\end{equation*}

Because $n \lr \leq T$, \cref{cor:global-error} will follow.

Base: $n = 0$. It is indeed true that $\norm{\btheta^{(0)} - \tilde{\btheta}^{(0)}}_{\infty} \leq d_1 \lr^2$ because the left-hand side is zero. It is indeed true that $\norm{\btheta^{(1)} - \tilde{\btheta}^{(1)} - \btheta^{(0)} + \tilde{\btheta}^{(0)}}_{\infty} \leq d_3 \lr^3$ for the same reason.

Assume $n \in \mathbb{Z}_{\geq 1}$ and it is true that
\begin{equation*}
\norm{\btheta^{(n')} - \tilde{\btheta}^{(n')}}_{\infty} \leq d_1 e^{d_2 n' \lr} \lr^2,\quad\norm{\btheta^{(n' + 1)} - \tilde{\btheta}^{(n' + 1)} - \btheta^{(n')} + \tilde{\btheta}^{(n')}}_{\infty} \leq d_3 e^{d_2 n' \lr} \lr^3.
\end{equation*}
for all $0 \leq n' \leq n - 1$. Then
\begin{align*}
  \norm{\btheta^{(n)} - \tilde{\btheta}^{(n)}}_{\infty} &\leq
  \norm{\btheta^{(n - 1)} - \tilde{\btheta}^{(n - 1)}}_{\infty}
  + \norm{\btheta^{(n)} - \tilde{\btheta}^{(n)} - \btheta^{(n - 1)} + \tilde{\btheta}^{(n - 1)}}_{\infty}
                                                 \shortintertext{by the triangle inequality,}\\
  &\leq d_1 e^{d_2 (n - 1) \lr} \lr^2 + d_3 e^{d_2 (n - 1) \lr} \lr^3
    \shortintertext{by the induction hypothesis,}\\
                                               &= d_1 \prn[\bigg]{1 + \frac{d_3}{d_1} \lr} e^{d_2 (n - 1) \lr} \lr^2 \leq d_1 \prn{1 + d_2 \lr} e^{d_2 (n - 1) \lr} \lr^2
                                                 \shortintertext{by $d_3 \leq d_1 d_2$\internalComment{!},}\\
                                               &\leq d_1 e^{d_2 n \lr} \lr^2
\shortintertext{by the inequality $1 + x \leq e^x$ for all $x \geq 0$.}
\end{align*}

Next, write
\begin{align*}
  &\btheta^{(n + 1)} - \btheta^{(n)} - \tilde{\btheta}^{(n + 1)} + \tilde{\btheta}^{(n)}\\
  &\quad = - \lr \boldsymbol{F}^{(n)}(\btheta^{(n)}, \ldots, \btheta^{(0)}) - \crl[\big]{\tilde{\btheta}^{(n + 1)} - \tilde{\btheta}^{(n)}}\\
  &\quad = \lr \brk[\big]{\boldsymbol{F}^{(n)}(\tilde{\btheta}^{(n)}, \ldots, \tilde{\btheta}^{(0)}) - \boldsymbol{F}^{(n)}(\btheta^{(n)}, \ldots, \btheta^{(0)})} - \crl[\big]{\tilde{\btheta}^{(n + 1)} - \tilde{\btheta}^{(n)} + \lr \boldsymbol{F}^{(n)}(\tilde{\btheta}^{(n)}, \ldots, \tilde{\btheta}^{(0)})}\\
\end{align*}

Then
\begin{align*}
  &\abs{\theta_r^{(n + 1)} - \theta_r^{(n)} - \tilde{\theta}_r^{(n + 1)} + \tilde{\theta}_r^{(n)}}\\
  &\quad \leq \lr \abs[\big]{F_r^{(n)}(\tilde{\btheta}^{(n)}, \ldots, \tilde{\btheta}^{(0)}) - F_r^{(n)}(\btheta^{(n)}, \ldots, \btheta^{(0)})} + \abs[\big]{\tilde{\theta}_r^{(n + 1)} - \tilde{\theta}_r^{(n)} + \lr F^{(n)}_r(\tilde{\btheta}^{(n)}, \ldots, \tilde{\btheta}^{(0)})}\\
  &\quad \leq \lr \abs[\big]{F_r^{(n)}(\tilde{\btheta}^{(n)}, \ldots, \tilde{\btheta}^{(0)}) - F_r^{(n)}(\btheta^{(n)}, \ldots, \btheta^{(0)})} + \oC{localerrorbound} \lr^3
\shortintertext{by \cref{th:general-momentum-methods},}
  &\quad = \lr \abs[\bigg]{\sum_{k = 0}^n \frac{\partial F_r^{(n)}}{\partial \btheta^{(n - k)}}(\bzeta)\trans \prn[\big]{\tilde{\btheta}^{(n - k)} - \btheta^{(n - k)}}} + \oC{localerrorbound} \lr^3,
\shortintertext{where $\bzeta$ is a point on the segment between $\prn[\big]{\tilde{\btheta}^{(n)}, \ldots, \tilde{\btheta}^{(0)}}$ and $\prn[\big]{\btheta^{(n)}, \ldots, \btheta^{(0)}}$,}
  &\quad \leq \lr d \sum_{k = 0}^{n} \gamma_k
    \norm{\tilde{\btheta}^{(n - k)} - \btheta^{(n - k)}}_{\infty} + \oC{localerrorbound} \lr^3
  \shortintertext{by \eqref{eq:first-der-bound} (recall that $d$ is the dimension of $\btheta$),}\\
  &\quad \leq d_1 \lr^3 d \sum_{k = 0}^\infty \gamma_k e^{d_2 (n - k) \lr} + \oC{localerrorbound} \lr^3
    \shortintertext{by the induction hypothesis and the bound on $\norm{\tilde{\btheta}^{(n)} - \btheta^{(n)}}_{\infty}$ already proven}\\
  &\quad \leq \underbrace{\prn[\bigg]{d_1 d \sum_{k = 0}^\infty \gamma_k + \oC{localerrorbound}}}_{\leq d_3} e^{d_2 n \lr} \lr^3\\
  &\quad \leq d_3 e^{d_2 n \lr} \lr^3.
\end{align*}

\section{Correction Terms for All Examples}\label{sec:correction-terms-for-all-examples}

For GD with momentum (\cref{ex:heavy-ball}):
\begin{equation*}
\corr{n}(\btheta) = \frac{\lr \beta}{2 (1 - \beta)^3} \nabla \norm{\nabla \loss(\btheta)}^2.
\end{equation*}

For Nesterov's accelerated GD (\cref{ex:nesterov}):
\begin{equation*}
\corr{n}(\btheta) = \frac{\lr \beta^2}{2 (1 - \beta)^3} \nabla \norm{\nabla \loss(\btheta)}^2.
\end{equation*}

For AdamW (\cref{ex:adamw}, also discussed in \cref{sec:adamw-anti-reg-but-lion-not}):
\begin{equation*}
\corr{n}(\btheta) = \lr \prn[\bigg]{\frac{\beta_1 (1 - \beta_1)^{-1} - \beta_2 (1 - \beta_2)^{-1}}{\prn[\big]{\abs[\big]{\nabla \loss(\btheta)}^2 + \varepsilon}^{1 / 2}}\\
+ \varepsilon \frac{\beta_2 (1 - \beta_2)^{-1}}{\prn[\big]{\abs[\big]{\nabla \loss(\btheta)}^2 + \varepsilon}^{3 / 2}}} \prn[\big]{\nabla \norm{\nabla \loss(\btheta)}_{1, \varepsilon} + \lambda \nabla^2 \loss(\btheta) \btheta}.
\end{equation*}

For Nadam (\cref{ex:nadamw}):
\begin{equation*}
  \corr{n}(\btheta) = \lr \prn[\bigg]{\frac{\beta_1^2 (1 - \beta_1)^{-1} - \beta_2 (1 - \beta_2)^{-1}}{\prn[\big]{\abs[\big]{\nabla \loss(\btheta)}^2 + \varepsilon}^{1 / 2}} + \varepsilon \frac{\beta_2 (1 - \beta_2)^{-1}}{\prn[\big]{\abs[\big]{\nabla \loss(\btheta)}^2 + \varepsilon}^{3 / 2}}} \prn[\big]{\nabla \norm{\nabla \loss(\btheta)}_{1, \varepsilon} + \lambda \nabla^2 \loss(\btheta) \btheta}.
\end{equation*}

For Lion-$\mathcal{K}$ (\cref{ex:lion-K}, also discussed in \cref{sec:adamw-anti-reg-but-lion-not}):
\begin{equation*}
\corr{n}(\btheta) = - \lr \frac{\rho_1}{1 - \rho_2} \nabla^2 \mathcal{K}(- \nabla \loss(\btheta)) \nabla^2 \loss(\btheta) \brk[\big]{\nabla \mathcal{K}(- \nabla \loss(\btheta)) - \lambda \btheta}.
\end{equation*}

\section{Auxiliary Results}

\begin{lemma}[Memory decays exponentially fast]\label{lem:memory-dec}
If $\boldsymbol{F}^{(n)}$ is a function of momentum variables as described in \cref{rem:fn-as-a-func-of-mom-vars}, then for any $n$ and $k \leq n$
\begin{equation}
\max_{r, i} \abs[\bigg]{\frac{\partial F_r^{(n)}}{\partial \theta_i^{(n - k)}}} \leq \gamma_k,\label{eq:lkjfjhs}
\end{equation}
and similarly for any $n$ and $k_1, k_2 \leq n$
\begin{equation}\label{eq:kljlja}
\max_{r, i, j} \abs[\bigg]{\frac{\partial^2 F_r^{(n)}}{\partial \theta_i^{(n - k_1)} \partial \theta_j^{(n - k_2)}}} \leq \gamma_{k_1, k_2},
\end{equation}
where $\crl{\gamma_k}$ and $\crl{\gamma_{k_1, k_2}}$ are sequences decaying exponentially fast: specifically,
\begin{equation*}
\gamma_k := C_{\gamma} \crl{\max_{\ell} \beta_{\ell}}^k,\quad \gamma_{k_1, k_2} := C_{\gamma} \crl{\max_{\ell} \beta_{\ell}}^{k_1 + k_2}
\end{equation*}
for some constant $C_{\gamma} > 0$.
\end{lemma}

\begin{proof}
It is easy to see~\eqref{eq:lkjfjhs} by taking the derivative:
\begin{align*}
  &\frac{\partial}{\partial \btheta^{(n - k)}} F_r^{(n)}(\btheta^{(n)}, \ldots, \btheta^{(0)})\\
  &\quad = \sum_{\ell = 1}^{\numofmoms} \sum_i \frac{\partial \extmomfuncsc_r}{\partial m_{\ell; i}}(\mcor{1}{n + 1}, \ldots, \mcor{\numofmoms}{n + 1}) \frac{\partial \mcorsc{\ell}{i}{n + 1}}{\btheta^{(n - k)}}\\
  &\quad = \sum_{\ell = 1}^{\numofmoms} \sum_i \bcor{\ell}{n} \beta_\ell^k \frac{\partial \extmomfuncsc_r}{\partial m_{\ell; i}}(\mcor{1}{n + 1}, \ldots, \mcor{\numofmoms}{n + 1})\\
  &\qquad \times \nabla \momfuncsc^{(n - k)}_{\ell; i}(\btheta^{(n - k)}),
\end{align*}
and using the uniform boundedness of derivatives of $\momfuncsc^{(n - k)}_{\ell; i}$ and $\extmomfuncsc_r$. \cref{eq:kljlja} is proven similarly.
\end{proof}

\begin{lemma}\label{lem:decaying-sums}
  Let $\crl{a_k}_{k = 1}^{\infty}$ and $\crl{b_k}_{k = 1}^{\infty}$ be sequences of reals such that $\sum_{k = 1}^{\infty} \prn{\abs{a_k} + \abs{b_k}} < \infty$. Then
\begin{equation*}
\sum_{k = 1}^n a_k \sum_{s = n - k}^{n - 1} b_s \underset{n \to \infty}{\longrightarrow} 0.
\end{equation*}
\end{lemma}

\begin{proof}
Fix $\varepsilon > 0$. Take such positive integer $n_0$ that for any $n_0 \leq n_1 \leq n_2$ we have $\sum_{s = n_1}^{n_2} \prn{\abs{a_k} + \abs{b_k}} < \varepsilon$. Then for any $n \geq 2 n_0 - 1$ the following holds:
\begin{equation*}
\sum_{k = 1}^n \abs{a_k} \sum_{s = n - k}^{n - 1} \abs{b_s} = \sum_{k = 1}^{n - n_0} \abs{a_k} \underbrace{\sum_{s = n - k}^{n - 1} \abs{b_s}}_{< \varepsilon} + \underbrace{\sum_{k = n - n_0 + 1}^n \abs{a_k}}_{< \varepsilon} \sum_{s = n - k}^{n - 1} \abs{b_s} < \varepsilon \sum_{k = 1}^{\infty} \prn{\abs{a_k} + \abs{b_k}}.
\end{equation*}
Since $\varepsilon$ is arbitrary and $\sum_{k = 1}^{\infty} \prn{\abs{a_k} + \abs{b_k}}$ is a finite constant, the statement follows.
\end{proof}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% copilot-mode: t
%%% TeX-master: t
%%% eval: (setq cdlatex-command-alist (append cdlatex-command-alist
%%%                         '(("L" "Insert loss" "\\loss" cdlatex-position-cursor nil nil t))))
%%% End:
