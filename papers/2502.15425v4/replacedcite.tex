\section{Related Works}
\vspace{0.1in}
\subsection{Multi-Agent Reinforcement Learning} 
\vspace{0.1in}
\label{sec:marl}

Research in multi-agent systems has gained significant attention in recent years ____. ____ proposed that innovation in intelligent systems emerges through social interactions via \emph{autocurricula}—naturally occurring sequences of challenges resulting from competition and cooperation between adaptive units, which drive continuous innovation and learning. The authors argue that advancing intelligent systems requires a strong focus on multi-agent research.

To support this growing field, several benchmarks have emerged ____. ____ introduced PettingZoo, which provides a standardized OpenAI Gym-like ____ interface for multi-agent environments, while ____ introduced BenchMARL, which addresses fragmentation and reproducibility challenges by offering comprehensive benchmarking tools and standardized baselines.

MARL approaches can be broadly categorized into three main groups based on their coordination strategy:

\begin{enumerate}
\item \emph{Independent learners} operate without inter-agent communication, with each agent maintaining its own learning algorithm and treating other agents as part of the environment. Common examples include IPPO ____, IQL ____, and ISAC ____, which are independent adaptations of their single-agent counterparts: PPO ____, Q-Learning ____, and SAC ____ respectively;
\item \emph{Parameter sharing} approaches have agents share components like critics or value functions, as in MAPPO ____, MASAC ____, and MADDPG ____;
\item \emph{Communicating agents} actively exchange information, either through consensus-based approaches ____ where agents must reach agreement over a communication network, or through learned communication protocols ____.
\end{enumerate}
For a comprehensive taxonomy and review, we refer readers to ____.

% For a more fine-grained division of the agent taxonomy and a more in-depth review of the field, we direct the reader to ____.

A significant challenge in MARL is the non-stationarity of the environment from each agent's perspective. As other agents learn and change their behaviors, the state transition dynamics also change. This impacts experience replay mechanisms, as stored experiences quickly become obsolete ____. The dominant paradigm of \emph{centralized learning} with \emph{decentralized execution} ____ attempts to address these challenges through shared learning components. However, this approach constrains the architecture during training and limits applicability to lifelong learning scenarios.

% This affects experience replay mechanisms, with ____ noting that replay buffer methods like DQN require modifications for multi-agent settings since stored experiences quickly become obsolete.
% In contrast, our hierarchical, fully decentralized approach enables continuous learning while maintaining agent independence.
% \giuseppe{Not sure about leaving this part in here, as we don't really tackle this.}

\subsection{Hierarchical Reinforcement Learning}
\vspace{0.1in}
\label{sec:hrl}

Hierarchical organization is fundamental to intelligent behavior in nature. Human infants naturally decompose complex tasks into hierarchical goal structures ____, enabling both temporal and behavioral abstractions. This hierarchical approach offers two key advantages: it improves credit assignment through abstraction-based value propagation and enables more semantically meaningful exploration through temporal and state abstraction ____. ____ demonstrates that this enhanced exploration capability is one of the major benefits of hierarchical RL over flat RL approaches.

The foundational approaches to HRL focus on two-level architectures. The Options framework formalizes temporal abstraction through Semi-Markov Decision Processes (SMDPs), where temporally-extended actions ("options") consist of a policy, termination condition, and initiation set ____. The framework supports concurrent option execution and allows for option interruption, providing flexibility beyond simple hierarchical structures. While options were initially predefined ____, later work enabled learning them with fixed high-level policies ____ or through end-to-end training, as in Option-Critic ____.

An alternative approach, Feudal RL ____, implements a manager-worker architecture where managers provide intrinsic goals to lower-level workers. This creates bidirectional information hiding—managers need not represent low-level details, while workers focus solely on their immediate intrinsic rewards without requiring access to high-level goals. These approaches face a common challenge: the non-stationarity of the lower level during learning complicates value estimation for the higher level.

Model-based approaches attempt to address this—____ learn symbolic models for high-level planning, while ____ build on MAXQ’s value function decomposition by breaking down the global MDP into task-specific local MDPs. However, these typically require hand-specified state abstractions or task decompositions. Recent work focuses on learning stability, with ____ introducing attention-based reward shaping to guide exploration, and ____ developing uncertainty-aware techniques to handle distribution shifts between levels.

The multi-agent setting introduces additional complexity, as hierarchical coordination must now handle both temporal and agent-to-agent dependencies. ____ addresses this through temporal abstraction with specialized replay buffers to handle the resulting non-stationarity. Meanwhile, ____ introduces hierarchical reward machines but require significant domain knowledge. The scarcity of work combining HRL and MARL highlights the challenges of stable learning with multiple sources of non-stationarity.

Our approach, \gls{name}, departs from traditional hierarchical frameworks by directly learning to shape lower-level observation spaces, rather than explicitly assigning goals like Feudal RL. This is directly inspired by the work of ____, which proposes that in biological systems, local environmental changes drive coordinated responses without central control. The closest approach to our work is FMH ____, but in this work, the agent is limited to shallow two-depth hierarchies and has only top-bottom information flow in the form of goals. In contrast, \gls{name} supports arbitrary-depth hierarchies without requiring explicit task specifications, and the communication across levels relies on bottom-up messages and top-down actions modifying the observations of the agents, rather than providing them goals. In this way, \gls{name} offers a flexible solution for multi-agent coordination.