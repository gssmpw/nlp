\section{Related Works}
\vspace{0.1in}
\subsection{Multi-Agent Reinforcement Learning} 
\vspace{0.1in}
\label{sec:marl}

Research in multi-agent systems has gained significant attention in recent years \citep{Nguyen2020, Oroojlooy2023}. \citet{Leibo2019} proposed that innovation in intelligent systems emerges through social interactions via \emph{autocurricula}—naturally occurring sequences of challenges resulting from competition and cooperation between adaptive units, which drive continuous innovation and learning. The authors argue that advancing intelligent systems requires a strong focus on multi-agent research.

To support this growing field, several benchmarks have emerged \citep{Samvelyan2019, Hu2021, Bettini2024, Terry2021}. \citet{Terry2021} introduced PettingZoo, which provides a standardized OpenAI Gym-like \citep{Brockman2016} interface for multi-agent environments, while \citet{Bettini2024} introduced BenchMARL, which addresses fragmentation and reproducibility challenges by offering comprehensive benchmarking tools and standardized baselines.

MARL approaches can be broadly categorized into three main groups based on their coordination strategy:

\begin{enumerate}
\item \emph{Independent learners} operate without inter-agent communication, with each agent maintaining its own learning algorithm and treating other agents as part of the environment. Common examples include IPPO \citep{De2020}, IQL \citep{Thorpe1997}, and ISAC \citep{Bettini2024}, which are independent adaptations of their single-agent counterparts: PPO \citep{Schulman2017}, Q-Learning \citep{Watkins1992}, and SAC \citep{Haarnoja2018} respectively;
\item \emph{Parameter sharing} approaches have agents share components like critics or value functions, as in MAPPO \citep{Yu2022}, MASAC \citep{Bettini2024}, and MADDPG \citep{Lowe2017};
\item \emph{Communicating agents} actively exchange information, either through consensus-based approaches \citep{Cassano2020, Zhang2018} where agents must reach agreement over a communication network, or through learned communication protocols \citep{Foerster2016, Jorge2016}.
\end{enumerate}
For a comprehensive taxonomy and review, we refer readers to \citet{Oroojlooy2023}.

% For a more fine-grained division of the agent taxonomy and a more in-depth review of the field, we direct the reader to \cite{Oroojlooy2023}.

A significant challenge in MARL is the non-stationarity of the environment from each agent's perspective. As other agents learn and change their behaviors, the state transition dynamics also change. This impacts experience replay mechanisms, as stored experiences quickly become obsolete \citep{Foerster2016}. The dominant paradigm of \emph{centralized learning} with \emph{decentralized execution} \citep{Oroojlooy2023} attempts to address these challenges through shared learning components. However, this approach constrains the architecture during training and limits applicability to lifelong learning scenarios.

% This affects experience replay mechanisms, with \cite{Foerster2016} noting that replay buffer methods like DQN require modifications for multi-agent settings since stored experiences quickly become obsolete.
% In contrast, our hierarchical, fully decentralized approach enables continuous learning while maintaining agent independence.
% \giuseppe{Not sure about leaving this part in here, as we don't really tackle this.}

\subsection{Hierarchical Reinforcement Learning}
\vspace{0.1in}
\label{sec:hrl}

Hierarchical organization is fundamental to intelligent behavior in nature. Human infants naturally decompose complex tasks into hierarchical goal structures \citep{Spelke2007}, enabling both temporal and behavioral abstractions. This hierarchical approach offers two key advantages: it improves credit assignment through abstraction-based value propagation and enables more semantically meaningful exploration through temporal and state abstraction \citep{Hutsebaut2022}. \citet{Nachum2019} demonstrates that this enhanced exploration capability is one of the major benefits of hierarchical RL over flat RL approaches.

The foundational approaches to HRL focus on two-level architectures. The Options framework formalizes temporal abstraction through Semi-Markov Decision Processes (SMDPs), where temporally-extended actions ("options") consist of a policy, termination condition, and initiation set \citep{Sutton1999}. The framework supports concurrent option execution and allows for option interruption, providing flexibility beyond simple hierarchical structures. While options were initially predefined \citep{Sutton1999}, later work enabled learning them with fixed high-level policies \citep{Silver2012, Mann2014} or through end-to-end training, as in Option-Critic \citep{Bacon2017}.

An alternative approach, Feudal RL \citep{Dayan1992, Kumar2017, Vezhnevets2017}, implements a manager-worker architecture where managers provide intrinsic goals to lower-level workers. This creates bidirectional information hiding—managers need not represent low-level details, while workers focus solely on their immediate intrinsic rewards without requiring access to high-level goals. These approaches face a common challenge: the non-stationarity of the lower level during learning complicates value estimation for the higher level.

Model-based approaches attempt to address this—\citet{Xu2021} learn symbolic models for high-level planning, while \citet{Li2017} build on MAXQ’s value function decomposition by breaking down the global MDP into task-specific local MDPs. However, these typically require hand-specified state abstractions or task decompositions. Recent work focuses on learning stability, with \citet{Luo2023} introducing attention-based reward shaping to guide exploration, and \citet{Hu2023} developing uncertainty-aware techniques to handle distribution shifts between levels.

The multi-agent setting introduces additional complexity, as hierarchical coordination must now handle both temporal and agent-to-agent dependencies. \citet{Tang2018} addresses this through temporal abstraction with specialized replay buffers to handle the resulting non-stationarity. Meanwhile, \citet{Zheng2024} introduces hierarchical reward machines but require significant domain knowledge. The scarcity of work combining HRL and MARL highlights the challenges of stable learning with multiple sources of non-stationarity.

Our approach, \gls{name}, departs from traditional hierarchical frameworks by directly learning to shape lower-level observation spaces, rather than explicitly assigning goals like Feudal RL. This is directly inspired by the work of \citet{Levin22}, which proposes that in biological systems, local environmental changes drive coordinated responses without central control. The closest approach to our work is FMH \citep{Ahilan2019}, but in this work, the agent is limited to shallow two-depth hierarchies and has only top-bottom information flow in the form of goals. In contrast, \gls{name} supports arbitrary-depth hierarchies without requiring explicit task specifications, and the communication across levels relies on bottom-up messages and top-down actions modifying the observations of the agents, rather than providing them goals. In this way, \gls{name} offers a flexible solution for multi-agent coordination.