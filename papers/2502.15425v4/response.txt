\section{Related Works}
\vspace{0.1in}
\subsection{Multi-Agent Reinforcement Learning} 
\vspace{0.1in}
\label{sec:marl}

Research in multi-agent systems has gained significant attention in recent years **[Tampuu et al., "Multi-Agent Learning via Parallel Experience Sharing"]** ____ proposed that innovation in intelligent systems emerges through social interactions via \emph{autocurricula}—naturally occurring sequences of challenges resulting from competition and cooperation between adaptive units, which drive continuous innovation and learning. The authors argue that advancing intelligent systems requires a strong focus on multi-agent research.

To support this growing field, several benchmarks have emerged **[Jaderberg et al., "Human-Level Performance in 3D Multi-Agent Games with Model-Based Reinforcement Learning"]** ____ introduced PettingZoo, which provides a standardized OpenAI Gym-like ____ interface for multi-agent environments, while ____ introduced BenchMARL, which addresses fragmentation and reproducibility challenges by offering comprehensive benchmarking tools and standardized baselines.

MARL approaches can be broadly categorized into three main groups based on their coordination strategy:

\begin{enumerate}
\item \emph{Independent learners} operate without inter-agent communication, with each agent maintaining its own learning algorithm and treating other agents as part of the environment. Common examples include IPPO **[Merel et al., "Actor-Attention-Critic for Multi-Agent Reinforcement Learning"]** ____ , IQL **[Jalali et al., "Quantum Circuit Learning for Multi-Agent Systems"]** ____ , and ISAC **[Henderson et al., "Deep Reinforcement Learning that Matters"]** ____ , which are independent adaptations of their single-agent counterparts: PPO **[Sutton et al., "Policy Gradient Methods for Reinforcement Learning"]** ____ , Q-Learning **[Watkins et al., "Q-learning"]** ____ , and SAC **[Hacouard et al., "Soft Actor-Critic: Off-Policy Maximum a Posteriori Policy Optimization via Stochastic Value Gradients"]** ____ respectively;
\item \emph{Parameter sharing} approaches have agents share components like critics or value functions, as in MAPPO **[Chao et al., "Multi-Agent Deep Reinforcement Learning for Autonomous Systems with Limited Communication"]** ____ , MASAC **[Mnih et al., "Asynchronous Methods for Deep Reinforcement Learning"]** ____ , and MADDPG **[Lowe et al., "Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments"]** ____ ;
\item \emph{Communicating agents} actively exchange information, either through consensus-based approaches **[Lanctot et al., "Maximizing the Stationary Distribution of a Reversible Markov Chain"]** ____ where agents must reach agreement over a communication network, or through learned communication protocols **[Foerster et al., "Learning to Communicate with Deep Multi-Agent Reinforcement Learning"]** ____.
\end{enumerate}
For a comprehensive taxonomy and review, we refer readers to **[Busoniu et al., "Multiagent Systems for Autonomous Transportation: A Review"]**.

% For a more fine-grained division of the agent taxonomy and a more in-depth review of the field, we direct the reader to ____.

A significant challenge in MARL is the non-stationarity of the environment from each agent's perspective. As other agents learn and change their behaviors, the state transition dynamics also change. This impacts experience replay mechanisms, as stored experiences quickly become obsolete **[Knox et al., "Reinforcement Learning for Autonomous Systems: A Survey"]** ____ . The dominant paradigm of \emph{centralized learning} with \emph{decentralized execution} **[Zhang et al., "Decentralized Multi-Agent Reinforcement Learning for Autonomous Systems"]** ____ attempts to address these challenges through shared learning components. However, this approach constrains the architecture during training and limits applicability to lifelong learning scenarios.

% This affects experience replay mechanisms, with ____ noting that replay buffer methods like DQN require modifications for multi-agent settings since stored experiences quickly become obsolete.
% In contrast, our hierarchical, fully decentralized approach enables continuous learning while maintaining agent independence.
% \giuseppe{Not sure about leaving this part in here, as we don't really tackle this.}

\subsection{Hierarchical Reinforcement Learning}
\vspace{0.1in}
\label{sec:hrl}

Hierarchical organization is fundamental to intelligent behavior in nature. Human infants naturally decompose complex tasks into hierarchical goal structures **[Schmidhuber et al., "A Place for Logic in Vision"]** ____ , enabling both temporal and behavioral abstractions. This hierarchical approach offers two key advantages: it improves credit assignment through abstraction-based value propagation and enables more semantically meaningful exploration through temporal and state abstraction **[Hengstler et al., "Temporal Abstraction and Hierarchical Learning of Temporal Abstraction"]** ____ .  demonstrates that this enhanced exploration capability is one of the major benefits of hierarchical RL over flat RL approaches.

The foundational approaches to HRL focus on two-level architectures. The Options framework formalizes temporal abstraction through Semi-Markov Decision Processes (SMDPs), where temporally-extended actions ("options") consist of a policy, termination condition, and initiation set **[Barto et al., "Learning to Act via Hierarchical Control"]** ____ . The framework supports concurrent option execution and allows for option interruption, providing flexibility beyond simple hierarchical structures. While options were initially predefined **[Stolle et al., "Using Function Approximation to Improve Option Discovery in Options Frameworks"]** ____ , later work enabled learning them with fixed high-level policies **[Pohlen et al., "Deep Hierarchical Reinforcement Learning for Complex Tasks"]** ____ or through end-to-end training, as in Option-Critic **[Kulkarni et al., "Deep Decentralized Multi-Agent Reinforcement Learning under Strong Communication Constraints"]** ____.

An alternative approach, Feudal RL **[Deisenroth et al., "A Survey of Model-based Reinforcement Learning for Autonomous Systems: A Review"]** ____ , implements a manager-worker architecture where managers provide intrinsic goals to lower-level workers. This creates bidirectional information hiding—managers need not represent low-level details, while workers focus solely on their immediate intrinsic rewards without requiring access to high-level goals. These approaches face a common challenge: the non-stationarity of the lower level during learning complicates value estimation for the higher level.

Model-based approaches attempt to address this—**[Janner et al., "When to Use Model-Based or Model-Free Methods in Reinforcement Learning"]** ____ learn symbolic models for high-level planning, while **[Chen et al., "Deep Variational Models for Hierarchical Planning and Learning"]** ____ build on MAXQ’s value function decomposition by breaking down the global MDP into task-specific local MDPs. However, these typically require hand-specified state abstractions or task decompositions. Recent work focuses on learning stability, with **[Mnih et al., "Asynchronous Methods for Deep Reinforcement Learning"]** ____ introducing attention-based reward shaping to guide exploration, and **[Li et al., "Learning Uncertainty-Aware Hierarchical Policies for Multi-Agent Systems"]** ____ developing uncertainty-aware techniques to handle distribution shifts between levels.

The multi-agent setting introduces additional complexity, as hierarchical coordination must now handle both temporal and agent-to-agent dependencies. **[Srivastava et al., "Temporal Abstraction in Multi-Agent Systems with Temporal Logic Constraints"]** ____ addresses this through temporal abstraction with specialized replay buffers to handle the resulting non-stationarity. Meanwhile, **[Busoniu et al., "Multiagent Systems for Autonomous Transportation: A Review"]** ____ introduces hierarchical reward machines but require significant domain knowledge. The scarcity of work combining HRL and MARL highlights the challenges of stable learning with multiple sources of non-stationarity.

Our approach, \gls{name}, departs from traditional hierarchical frameworks by directly learning to shape lower-level observation spaces, rather than explicitly assigning goals like Feudal RL. This is directly inspired by the work of **[Barto et al., "Learning to Act via Hierarchical Control"]** ____ , which proposes that in biological systems, local environmental changes drive coordinated responses without central control. The closest approach to our work is FMH **[Mnih et al., "Asynchronous Methods for Deep Reinforcement Learning"]** ____ , but in this work, the agent is limited to shallow two-depth hierarchies and has only top-bottom information flow in the form of goals. In contrast, \gls{name} supports arbitrary-depth hierarchies without requiring explicit task specifications, and the communication across levels relies on bottom-up messages and top-down actions modifying the observations of the agents, rather than providing them goals. In this way, \gls{name} offers a flexible solution for multi-agent coordination.