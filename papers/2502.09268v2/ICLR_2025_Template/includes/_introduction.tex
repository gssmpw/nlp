\section{Introduction}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% dpx: # dpx: v3

% With the recent surge in robotics and embodied AI, significant progress has been made in general visuomotor control. 
% However, most existing control policies are trained in idealized environments, without accounting for the inevitable external disturbances encountered during deployment. 
% Consequently, these policies struggle to accurately and reliably estimate the robot’s state, hindering the generation of robust and stable actions, and resulting in a significant decline in performance.
% Inspired by the principles of classical internal model control, we propose the Robot Intrinsic Model visuomotor Control (RoboIMC) method. 
% It integrates internal models to achieve precise estimation of the robot’s state, thereby enhancing the robustness of its actions. 
% RoboIMC comprises a text-conditioned video diffusion model for generating accurate visual plan states as reference inputs, two visual internal encoders for simulating responses, and a feedback-driven controller for action generation.
% Our method achieves state-of-the-art results on both standard and environmentally disturbed CALVIN benchmarks, and also shows significant improvements in realistic robotics tasks.

% 1: 引入general visuomotor control，列举一些例子
Robotics and embodied generalists have gained remarkable progress in recent years, with the successful advancement of large-scale data collection~\citep{vuong2023open}, universal state representation learning~\citep{li2023vision,du2024learning} and expressive policy learning~\citep{brohan2022rt,chi2023diffusion}, \textit{etc}. 
%
The above policies have proven effective in estimating the robot’s state and generating robust action in diverse environments, from physical simulators~\citep{mu2021maniskill,ding2023quar} to real-world settings meticulously designed.
% 
However, these meticulously designed environments don't account for the inevitable external disturbances encountered during deployment, e.g., fluctuated lighting conditions or noise video streams due to signal transmission issues.
%
When deployed in these non-ideal environments, the above visuomotor policy may struggle to accurately and reliably estimate the robot’s state, hindering the generation of robust and stable actions and resulting in a significant decline in performance.
% 
Consequently, deploying robots in scenarios with inevitable external disturbances remains a persistent challenge~\citep{}.


%
Some literature
have attempted to tackle the problem with large language models, splitting a prolonged job into detailed instructions for each minor movement~\citep{}, or sub-goals.
%
Though effective in high-level descriptions, texts could still be inadequate for detailed descriptions of the environment and robot state, leading to considerable issues under cross-morphology or multi-environments~\citep{du2024learning}.
Therefore, 
recent efforts have started embracing vision as a universal medium
to develop an embodied agent capable of planning diverse tasks through imagination and execution~\citep{black2023zero, du2024learning, Yang2023LearningIR}. These approaches involve a generative model for predicting future videos or goal images, followed by a goal-conditioned policy for translating the visual plan into actual actions. 
%
% Despite their successes, these methods often rely on idealized evaluation paradigms, where robot actions are executed in controlled environments without disturbances.
% %
% In real-world scenarios, however, lighting conditions may fluctuate, and video streams may suffer from noise due to signal transmission issues. 
% %
% Such subtle variations in non-ideal environments can lead to incorrect subgoal planning by the visual planner and imprecise execution by the goal-conditioned policy. Consequently, these challenges impose higher demands on the generalization capabilities of both the visual planner and the goal-conditioned policy.





% However, despite the promising results demonstrated by these approaches in controlled evaluation benchmarks, we have identified a critical limitation: these evaluations often assume an idealized perception environment that remains static throughout the testing process. Unfortunately, this assumption does not hold in real-world scenarios, where conditions such as dynamic lighting changes or subtle variations in camera positioning can cause substantial performance degradation.




% 2: 讲一些现有的局限性
% However, most existing control policies are trained in idealized environments, without accounting for the inevitable external disturbances encountered during deployment. 

% 3: 问题是struggle to accurately and reliably estimate the robot’s state，hindering the generation of robust and stable actions，resulting in a significant decline in performance

% 3: 讲解classical internal model control

% 4: 提出我们的方案


% 











%
% Some literature
% have attempted to tackle the problem with large language models, splitting a prolonged job into detailed instructions for each minor movement~\citep{}, or sub-goals.
% %
% Though effective in high-level descriptions, texts could still be inadequate for detailed descriptions of the environment and robot state, leading to considerable issues under cross-morphology or multi-environments~\citep{du2024learning}.
% Therefore, 
% recent efforts have started embracing vision as a universal medium
% to develop an embodied agent capable of planning diverse tasks through imagination and execution~\citep{black2023zero, du2024learning, Yang2023LearningIR}. These approaches involve a generative model for predicting future videos or goal images, followed by a goal-conditioned policy for translating the visual plan into actual actions. 
% %
% Despite their successes, these methods often rely on idealized evaluation paradigms, where robot actions are executed in controlled environments without disturbances.
% %
% In real-world scenarios, however, lighting conditions may fluctuate, and video streams may suffer from noise due to signal transmission issues. 
% %
% Such subtle variations in non-ideal environments can lead to incorrect subgoal planning by the visual planner and imprecise execution by the goal-conditioned policy. Consequently, these challenges impose higher demands on the generalization capabilities of both the visual planner and the goal-conditioned policy.

%








%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% dpx: v1--素材

% general robotic simulator
% RoboSora: a learned simulator for general purpose robotics
% a robotics world model that understands contact physics and generates high fidelity video- towards a robot that plans, evaluated, and simulates in its own neural space.

% first proof of robot data significantly advancing scaling laws.

% 这个世界模型还能预测非平凡物体（如刚体）的相互作用、掉落物体的影响、部分可观测性、可变形物体（窗帘、衣物）和铰接物体（门、抽屉、窗帘、椅子）。

% 泛化性的能力是重要的
% 现有的方案，主要分为两套方案：显式和隐式的
% 隐式的方法受限于机器人的数据集的问题（高质量数据对缺失），显式的方法虽然解决了这类的问题，但是还有两个问题：
% 感知模型的通用性很成问题

% 下层模型的逆运动学的动作映射也很成问题

% 

% 扩充成一段：
% 世界模型解决了构建通用机器人时一个非常实际、但经常被忽视的挑战，即评估。如果你训练的机器人可以执行 1000 项任务，与先前模型相比，我们也很难确定新模型用在机器人上，在这 1000 项任务上表现优越。由于周围环境如光照细微的变化，即使模型权重相同，机器人也可能在几天内经历性能的快速下降。
% 如果环境随着时间的推移不断变化，那么此前在该环境中达到的实验性能很难复现，因为旧环境不存在了。如果你在不断变化的环境（如家庭或办公室）中评估多任务系统，那么这个问题将会变得更加糟糕。这一状况使得在现实世界中进行机器人科学研究变得异常困难。

% 借鉴一下1X的写法，现在的模型虽然可以用，但是在
% 实际应用过程中，由于场景的泛化性问题啊，性能往往会有很大的drop



% 机器人对于

% 机器人对广泛任务的通用性很关注
% For robots to be practical and useful, they must be general
% to the wide variety of conditions they will encounter in the
% world. 
% 因此激烈了机器人社区对于泛化性的追求
% It therefore has been of great interest in the robotics community to apply various recipes, such as larger datasets, more general model architecture, better explainable， to achieve broad generalization.
% 但是，尽管这些工作在评估的指标上很promising, 但是我们发现一个很重要的问题：这些评估过程往往假设是理想的感知环境，这种环境在各种评估过程中往往保持完全不变。不幸的是，这个假设在实际应用过程中往往是不成立的。比如说，开放环境下的光照的实时变化，相机位置设定的细微角度变化，都会给最终的性能带来巨大的drop.
%
% 如图表XXX所示，我们测试了现有一些SOTA方法对于这些变化的性能drop情况。我们可以发现，大部分的implict的工作，由于图像信息被压缩，天然损失了对于图像更好的理解能力，即使有工作提出希望借由预训练的video模型提升模型的性能, 但是我们发现这类工作因为训练的数据集有限，没有internet级别的数据集的训练，模型无法处理这些物理规律的变化。究其原因，这类模型缺少了对于感知世界的通用理解能力，这进一步限制了模型面向更通用的应用场景下。


% 为此，我们在这篇工作，想要探索 a general robotics world model that understands contact physics and generates high fidelity video- towards a robot that plans, evaluated, and simulates in its own neural space.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% dpx: v2



% For robots to be practical and useful, they must be general to the wide variety of conditions they will encounter in the world. 

% There has been significant interest in the robotics community to improve generalization by employing various strategies, such as leveraging larger models~\citep{alayrac2022flamingo}.

% % 得益于数据集的captures much of the complexity and variation in the domain that models will need to reason about

% % 但是这些工作都需要非常大量的pair的数据去训练，后面大家开始转向通用的world model 学习，因为这种方式可以跨越不同的数据集，不同的action space, 因而可以很好地泛化。

% % 其中，最经典的范式就是通过语言条件和当前的image去生成未来的一连串视频。

% % 这类工作在一些简单的arrangement上表现得很好，但是在涉及到长时序的任务上时候表现得一般

% % 究其原因是其时序视频的生成质量太差，导致整体性能变化得很大

% % 后面有哥们通过了只预测未来的帧来解决这个问题，在长时序的任务上表现得很好

% % 但是这种范式会导致很严重的泛化问题。

% % 在真实的环境受到干扰的情况下，SuSIE的鲁棒性会差很多

% % 由于缺乏了这种渐进式的推理过程，执行的鲁棒性下降了很多

% % 因此，本文想要探索一个更好地生成视频，且执行鲁棒性更强的世界模型理解器。


% % 

% % 在Sora和LLM都产生了大量的工作赋能了泛化性的提升

% % 尽管有大量的基于LLM的模型出现了，也提高了模型的整体感知能力，但是这类工作仍然受限于一个问题啊，就是说模型是从语言端解决问题的，需要大量的robot data去训练

% % 需要将感知空间对齐到机器人的action 空间

% % 这是需要大量的数据pair的

% % 由于不同的环境的action space什么的都不一样，因此

% % 但是这种方式

% % 先前的工作


% Consequently, there has been significant interest in the robotics community to improve generalization by employing various strategies, such as scaling up datasets~\citep{brohan2022rt}, designing more general model architectures~\citep{alayrac2022flamingo}, and enhancing explainability~\citep{du2024learning}.

% However, despite the promising results demonstrated by these approaches in controlled evaluation benchmarks, we have identified a critical limitation: these evaluations often assume an idealized perception environment that remains static throughout the testing process. Unfortunately, this assumption does not hold in real-world scenarios, where conditions such as dynamic lighting changes or subtle variations in camera positioning can cause substantial performance degradation.
% %
% As shown in \textcolor{red}{Figure 1}, we evaluated the performance drop of SOTA methods under these varying conditions. Our findings reveal that many \textcolor{red}{implicit models}, which rely on compressed image information, inherently lose the ability to fully understand the visual context. Although some approaches attempt to improve performance by using pre-trained video models, these models are constrained by the limited size of their training datasets, which are not on the scale of internet-level data. As a result, they struggle to handle changes governed by fundamental physical laws. {The core issue is that these models lack a general understanding of the perceptual world, which restricts their applicability to more generalized real-world scenarios.}

% To address these limitations, in this work, we propose \textcolor{red}{a robotics world model} that understands contact physics and generates high fidelity video towards a robot that plans, evaluated, and simulates in its own neural space.
% Specifically, 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% zhy: v1
% In the fields of computer vision and natural language processing (NLP), the current dominant paradigm is to build general models that can solve a wide variety of tasks. 
% Excellent zero-shot learning capabilities have emerged in some large pre-trained models \citep{chowdhery2023palm,hoffmann2022training,radford2021learning,alayrac2022flamingo}. 
% Inspired by the success of these studies, the field of robot learning is also building a large number of datasets of various decision-making tasks in different environments and striving to build general robot agents.

% Building a general model that can solve a variety of robot tasks is an important research issue in the field of robot learning. 
% The primary challenge is that the collection of paired vision, language, and robot expert actions is time-consuming and laborious. 
% Although there are some public robot decision-making task datasets \citep{mees2022calvin,walke2023bridgedata,vuong2023open,khazatsky2024droid}, the amount of data is still far from the massive text data on the Internet used in the field of NLP. 
% Some previous studies \citep{brohan2022rt,reed2022generalist} collected paired vision, language, and action data and trained an agent for solving long-term tasks. 
% Some other studies \citep{li2022pre,driess2023palm,kim2024openvla}  utilized the specific robot demonstrations to fine-tune large language models (LLMs) for visual and language inputs.
% However, due to the limited data size, these agents can only generalize within a limited data distribution and are difficult to expand to new environments and tasks that have never been seen before.

% Moreover, the diversity of environments and tasks in robot decision-making tasks also poses a severe challenge to training general robot agents. 
% Different decision-making tasks have different state-action spaces and environmental dynamic models. 
% Perceptual representation, knowledge sharing, and skill generalization across tasks and environments are greatly hindered. 
% % To alleviate this problem, the work \citep{reed2022generalist} argued that different environments and tasks can be encoded by utilizing universal tokens in a sequence modeling framework. 
% % However, it is unclear whether this approach can retain the rich semantic knowledge in the pre-trained model and efficiently transfer this knowledge to downstream robot tasks.
% To alleviate this problem, some works \citep{ajay2024compositional,du2024learning,black2023zero} fine-tune the language-conditioned video (or image) prediction model on robot data to obtain a robot behavior planner, and then utilize the goal-conditioned policy (or inverse dynamics model) to predict the robot's action. 
% However, the current model still has room for improvement in robot trajectory videos (and images). 
% These methods have difficulty simulating the physical characteristics of complex scenes in long-term tasks, and may even produce hallucinations of objects or actions that are unreal or do not exist in the physical world. 
% Therefore, inspired by the recent success of advanced text-conditioned video models \citep{videoworldsimulators2024,esser2024scaling} in greatly improving the quality of image and video generation, we leverage the diffusion transformer structure with more powerful expression capabilities and the efficient generation method rectified flow \citep{liu2022flow} to model and train the robot behavior planner.

% On the other hand, previous methods have the problem of inconsistency between behavior video (or image) inference and action prediction during the test phase. 
% This is because the  goal-conditioned policy utilizes future images in real data as goals during training, while only future images inferred by the behavior planner can be obtained during testing. 
% The difference between real images and generated images will lead to inaccurate action prediction. 
% To alleviate this discrepancy, we utilize contrastive learning to enhance the robustness of the image representation of the goal-conditioned policy so that it generalizes better to new environments. 
% Specifically, if the observation image and the goal image in a batch come from the same trajectory, they are positive samples, otherwise they are negative samples. 
% These pairs are optimized by swapping assignment tasks similar to \citep{caron2020unsupervised}.

% % In this work, we propose a new robot behavior planning model conditioned on abstract language text instructions and historical video trajectories. 
% % It utilizes text instructions as a universal interface for expressing task descriptions and utilizes the robot's sequential decision videos as a universal interface for conveying observed behaviors and actions in different environments and tasks. 
% % It generates the robot's future trajectory in the form of an image sequence conditioned on the robot's historical trajectory video and text instructions describing the current goal. 
% % Meanwhile, we also construct an inverse dynamics model as the goal-conditioned policy to predict actions. 
% % It utilizes the robot's historical decision videos and future images generated by the behavior planning model as a universal interface for conveying observed behaviors and goals in different environments and tasks.

% % Therefore, the robot behavior planning model and inverse dynamics model we constructed together form a hierarchical policy framework, which is characterized by the decoupling of environmental perception and decision-making actions under long-term decision-making tasks of robots. 
% % This decoupling helps to enhance the robot's general and robust decision-making control capabilities. 
% % The reason is that the upper-level behavior planning model can retain the massive knowledge of the video generation model pre-trained on a large-scale text video dataset, which can deepen the robot's perception of environmental objects and its understanding of the physical laws of the world. 
% % The model not only understands the specific content required by the abstract text instructions, but also understands the existence and operation of these things in the physical world. 
% % Moreover, the rich combinatorial nature of language instructions can also be utilized to synthesize new combined behaviors in many different environments and tasks. 
% % For specific downstream robot decision-making tasks, the training of the lower-level inverse dynamics model only requires a small number of robot action trajectories without text annotations.

% The main contribution of our work is to establish a new robot behavior planning model, which realizes accurate inference and reasonable planning of robot behavior for long-horizon tasks, and enhances the robot's deep perception and understanding of the underlying operation mode of the physical world.
% Moreover, the proposed representation enhancement method enables the goal-reaching policy to predict more accurate actions and achieve general and robust robot decision-making.
% Extensive experiments show that our method outperforms baseline methods in terms of robot behavior trajectory video generation quality and decision task success rate.
% Therefore, further extending our robot behavior planning model is a promising way to build a general robot agent in the physical world.
