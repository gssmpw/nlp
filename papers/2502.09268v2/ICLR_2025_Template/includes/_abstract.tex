\begin{abstract}

% Robotic universal visuomotor control based on world models has achieved significant advancements. However, the generalization capability of such methods remains insufficient. Most of these works are trained in ideal environments without considering the inevitable disturbances that occur during the testing phase.
% World models trained in disturbance-free conditions often experience representation collapse when faced with disturbances in the testing environment. As a result, these models fail to generate precise actions, leading to a significant drop in performance.
% Inspired by the classic Internal Model Control (IMC) systems, we propose a closed-loop visuomotor control framework called RoboIMC, which integrates internal model feedback mechanisms to improve adaptive robotic control.
% RoboIMC specifically employs a robust Opensora as a visuomotor internal model designed to generate future expectations in the face of disturbances. By integrating a feedback-driven policy, RoboIMC can dynamically adjust its actions in real-time, effectively alleviating the impact of environmental disturbances. This approach not only enhances the system's adaptability but also significantly improves its reliability in practical applications, ensuring stable performance even in complex and ever-changing conditions.
% Compared to existing methods, our framework demonstrates significant performance improvements across various robotic tasks.

% zhy_v1:
% With the recent surge in robotics and embodied AI, significant progress has been made in general visuomotor control. 
% However, most existing control policies are trained in idealized environments, without accounting for the inevitable external disturbances encountered during deployment. 
% Consequently, these policies struggle to accurately and reliably estimate the robot’s state, hindering the generation of robust and stable actions, and resulting in a significant decline in performance.
% Inspired by the principles of classical internal model control, we propose the Robot Intrinsic Model visuomotor Control (RoboIMC) method. 
% It integrates internal models to achieve precise estimation of the robot’s state, thereby enhancing the robustness of its actions. 
% RoboIMC comprises a text-conditioned video diffusion model for generating accurate visual plan states as reference inputs, two visual internal encoders for simulating responses, and a feedback-driven controller for action generation.
% Our method achieves state-of-the-art results on both standard and environmentally disturbed CALVIN benchmarks, and also shows significant improvements in realistic robotics tasks.

With the rapid development of embodied artificial intelligence, significant progress has been made in vision-language-action (VLA) models for general robot decision-making. 
However, the majority of existing VLAs fail to account for the inevitable external perturbations encountered during deployment. 
These perturbations introduce unforeseen state information to the VLA, resulting in inaccurate actions and consequently, a significant decline in generalization performance. 
The classic internal model control (IMC) principle demonstrates that a closed-loop system with an internal model that includes external input signals can accurately track the reference input and effectively offset the disturbance. 
We propose a novel closed-loop VLA method GEVRM that integrates the IMC principle to enhance the robustness of robot visual manipulation. 
The text-guided video generation model in GEVRM can generate highly expressive future visual planning goals.
Simultaneously, we evaluate perturbations by simulating responses, which are called internal embeddings and optimized through prototype contrastive learning. 
This allows the model to implicitly infer and distinguish perturbations from the external environment.
The proposed GEVRM achieves state-of-the-art performance on both standard and perturbed CALVIN benchmarks and shows significant improvements in realistic robot tasks.
\end{abstract}