\vspace{-2em}
\section{experimental evaluation}
 \vspace{-0.5em}
% In this section, we evaluate the GEVRM in terms of its ability to enable robust visual manipulation.  
In this section, we evaluate the state generation and visual manipulation capabilities of GEVRM.
%
To this end, our experiments aim to investigate the following questions:
%
\textbf{1)} Can GEVRM have strong generalization ability to generate expressive goal in various environments?
\textbf{2)} Does GEVRM exhibit a higher success rate in executing robot tasks compared to the baseline in various environments?
\textbf{3)} How important are the core components of the GEVRM for achieving robust decision action?

 \vspace{-0.5em}
\subsection{Evaluation on Goal Generation}
\label{sec: exp_video_generation}

% In this section, we conduct a thorough assessment of GEVRM's robustness generalization capabilities across two dimensions: previously unseen environments and perturbed environments.



\textbf{Setup.} 
We utilized two types of datasets (realistic Bridge~\citep{walke2023bridgedata} and simulated CALVIN~\citep{mees2022calvin}) to evaluate the generalization of goal generation. 
% Both datasets consist of various robotic manipulation tasks, such as placing a potato on a plate. 
% We train the model on pre-defined training sets and tested on unseen test sets to assess the generation performance.
% We also evaluate its robustness in goal generation by introducing external perturbations. 
% We train the model on a predefined training set and test it on the test set with or without external interference to evaluate the robot gaol generation performance.
We train the model on a predefined training set and evaluate the robot goal generation performance on a test set with and without external perturbations.
The hyperparameters are shown in Appendix Tab.~\ref{Appd:Behavior planner training optimizer hyperparameters} and Tab.~\ref{Appd:Behavior planner test hyperparameters}.

\textbf{Baselines.} 
To make a fair comparison, we have chosen open-source video generative models: 
1) AVDC~\citep{ko2023learning}, a typical diffusion-style generation model for robotics.
2) GR-1~\citep{Wu2023UnleashingLV}, which is an autoregressive-style generation model that takes language instructions, and state sequences as inputs, and predicts robot actions and future images in an end-to-end manner. 
3) SuSIE~\citep{black2023zero}, uses the image editing diffusion model as a high-level planner and proposes intermediate sub-goals that can be achieved by the low-level controller.
% Given that more methods are not open-source or belong to the category of image-level generative paradigm, they are not included in this comparison. 


\textbf{Metrics.} 
The evaluation metrics employed are the Frechet Inception Distance (FID) \citep{Seitzer2020FID} and the Frechet Video Distance (FVD) \citep{stylegan_v,digan}, both widely recognized in the domains of image and video generation.
% We also test other standard reconstruction metrics: Structural Similarity Index (SSIM), Peak Signal-to-Noise Ratio (PSNR), Learned Perceptual Image Patch Similarity (LPIPS) following~\citep{bu2024closed}.
We also evaluate the quality of videos generated by different models on other standard metrics~\citep{bu2024closed}: Structural Similarity Index (SSIM), Peak Signal-to-Noise Ratio (PSNR), Learned Perceptual Image Patch Similarity (LPIPS).
% These metrics are utilized to assess the quality of the videos anticipated to be generated by the different video generation models. 
% The lower the values of these two metrics, the higher the quality of the video produced by the model, indicating a closer resemblance to real video footage.




% \begin{wraptable}{r}{0.4\textwidth}\small
%     \centering
%     \vspace{-0.5em}
%     \caption{{Goal generation quality comparison}.
%         Here, ``B'' and ``C'' mean Bridge and CALVIN datasets. 
%         % Our method greatly surpasses the baseline across all metrics. 
%     % The best results for each task are bolded. 
%     }
%     \begin{tabular}{ccc}
%      \toprule
%         Algorithms & \textbf{FID} ($\downarrow$) & \textbf{FVD} ($\downarrow$)  \\ 
%          \midrule
%          AVDC (B) & 246.45$\pm${\scriptsize 39.08} & 22.89$\pm${\scriptsize 4.99}  \\ 
%         Ours (B) & \textbf{35.70}$\pm${\scriptsize 10.77} & \textbf{4.16}$\pm${\scriptsize 1.35}  \\ 
%         % Variation Rate (B) & 60.1\% & 70.4\%  \\
%         \midrule
%         GR-1 (C) & 236.75$\pm${\scriptsize 38.87} & 12.83$\pm${\scriptsize 2.6} \\ 
%         Ours (C) & \textbf{94.47}$\pm${\scriptsize 22.54} & \textbf{3.8}$\pm${\scriptsize 1.2} \\ 
%         % Variation Rate (C) & \% & \%  \\
%          \bottomrule
%     \end{tabular}
%        \label{tab:calvin_FID_FVD}
% \end{wraptable}
\begin{table}\small
    \centering
    \caption{Goal generation quality comparison.
        Our method greatly surpasses the baseline across all metrics. 
    The best results for each task are bolded. 
    }
    \vspace{1em}
    \begin{tabular}{ccccccc}
     \toprule
        \textbf{Benchmark} & \textbf{Algorithms} & \textbf{FID} ($\downarrow$) & \textbf{FVD} ($\downarrow$) & \textbf{LPIPS} ($\downarrow$)& \textbf{SSIM} ($\uparrow$)& \textbf{PSNR} ($\uparrow$) \\ 
         \midrule
         BridgeData& AVDC  & 246.45$\pm${\scriptsize 39.08} & 22.89$\pm${\scriptsize 4.99} &0.23$\pm${\scriptsize 0.03}&  0.73$\pm${\scriptsize 0.05}&18.22$\pm${\scriptsize 2.53} \\ 
        BridgeData & SuSIE & 114.79$\pm${\scriptsize 21.38} & --               & 0.22 $\pm${\scriptsize 0.08} & 0.71$\pm${\scriptsize 0.07} & 16.39$\pm${\scriptsize 2.90} \\
        BridgeData&GEVRM (Ours) & \textbf{35.70}$\pm${\scriptsize 10.77}   & \textbf{4.16}$\pm${\scriptsize 1.35}&\textbf{0.06}$\pm${\scriptsize 0.03}&\textbf{0.89}$\pm${\scriptsize 0.04}& \textbf{22.36}$\pm${\scriptsize 2.75}\\ 
        % Variation Rate (B) & 60.1\% & 70.4\%  \\
        \midrule
        CALVIN&GR-1  & 236.75$\pm${\scriptsize 38.87} &12.83$\pm${\scriptsize 2.60} &0.20$\pm${\scriptsize 0.02}&0.65$\pm${\scriptsize 0.03}&18.59$\pm${\scriptsize 0.95}\\ 
        CALVIN& SuSIE & 214.14$\pm${\scriptsize 45.45} & --               & 0.15$\pm${\scriptsize 0.04} & 0.75$\pm${\scriptsize 0.05} & 18.12$\pm${\scriptsize 2.29} \\
        CALVIN&GEVRM (Ours) & \textbf{94.47}$\pm${\scriptsize 22.54} & \textbf{3.80}$\pm${\scriptsize 1.2}&\textbf{0.09}$\pm${\scriptsize 0.04}&\textbf{0.80}$\pm${\scriptsize 0.05}&\textbf{21.10}$\pm${\scriptsize 3.29} \\ 
        % Variation Rate (C) & \% & \%  \\
         \bottomrule
    \end{tabular}
       \label{tab:calvin_FID_FVD}
\end{table}
\begin{figure}
\centering
% \includegraphics[width=0.5\textwidth,height=0.28\textwidth]{LaTeX/pictures/data_dis_visua.png}
\includegraphics[width=0.9\textwidth]{ICLR_2025_Template/figures/Visulization.pdf}
\caption{
Comparison of goal generation on task “\textit{put blueberry in pot or pan on stove}”.
}
  \label{fig:Visualization of Bridge Data}
\end{figure}

\textbf{Goal generation comparison.}
We evaluate the generalization of the goal generation in the unseen environment (Tab.~\ref{tab:calvin_FID_FVD}).
% The results in Tab.~\ref{tab:calvin_FID_FVD} reveal a significant enhancement in the model's performance.
The results show that the performance of our GEVRM model is significantly improved compared to the baseline.
% In terms of FID and FVD metrics, compared with the baseline GR-1 scores of $236.75$ and $12.83$, our proposed method is significantly lower than theirs, with only $94.47$ and $3.8$.  
The results indicate that GEVRM exhibits enhanced expressive capabilities, effectively modeling the intricate textures and temporal coherence of robotic image sequences.
Then, we compare the robustness of goal generation in the perturbed environment (Fig.~\ref{fig:Visualization of Bridge Data}).
The baselines struggle with environmental variations, generating severe hallucinations that distort objects and may even completely ruin the scene.
In contrast, our method produces fewer hallucinations and can generate expressive goal states following language instructions.
This confirms that GEVRM is indeed better able to understand the laws of the physical world and maintain the 3D consistency of objects.
% For more goal generation results, see Appendix Fig.~\ref{APP:fig:calvin_video_1},~\ref{APP:fig:calvin_video_2},~\ref{APP:fig:calvin_video_3} and ~\ref{APP:fig:calvin_video_4}.
More goal generation results are in Appendix Fig.~\ref{APP:fig:calvin_video_1}$\sim$\ref{APP:fig:real_video_3}.



\subsection{Evaluation on Action Execution}

% In this section, we delve into the practical viability of GEVRM for robotic manipulation tasks.

\textbf{Setup.} We conduct experiments on CALVIN, a benchmark for language-conditioned manipulation to evaluate the GEVRM's capabilities in closed-loop action execution.
CALVIN consists of four simulated environments (A, B, C, and D), each with a dataset of human-collected play trajectories. 
We study zero-shot multi-environment training on A, B, and C, and testing on D, varying in table texture, furniture positioning, and color patches. We also test GEVRM's robustness to perturbations (Fig.~\ref{fig:calvinTask}).
Details of environmental disturbances are given in Appendix Section~\ref{Appe:Environment perturbations.}.
The policy training hyperparameters are shown in Appendix Tab.~\ref{Appd:Goal-guided policy optimizer Hyperparameters}.


\textbf{Baselines.} We select the representative baselines to verify the generalization performance on standard unseen environments: 
1) UniPi~\citep{du2024learning}: Recasts decision-making into text-conditioned video generation firstly, enabling the production of predictive video sequences and subsequent extraction of control actions.
2) HiP~\citep{ajay2024compositional}: 
% Advances upon UniPi by incorporating hierarchical inference for extended long-term planning capabilities.
This model improves upon UniPi by incorporating hierarchical inference to extend long-term planning capabilities.
3) GR-1~\citep{Wu2023UnleashingLV}:
This model leverages a pre-trained video model to enhance autoregressive action generation.
4) RoboFlamingo~\citep{li2023vision}, uses a pre-trained VLM for single-step visual language understanding and models sequential history information with an explicit policy head.
In addition, in the test environment with external perturbations, we choose the representative baseline SuSIE~\citep{black2023zero}, because it adopts common data augmentation strategies to cope with perturbations and achieves state-of-the-art results in previous works.
We consider third-view RGB images from static cameras as observations, which makes the robot execution more challenging.
More comparison with language-conditioned methods are in Appendix Sec.~\ref{Appe:Baseline method introductions.} and Tab.~\ref{APP:tab:calvin_performance}.
 
\begin{wraptable}{r}{0.6\textwidth}\small
    \centering
    \vspace{-2.em}
    \caption{Generalization on unseen environments in CALVIN (train A, B, C → test D). *: reproduced version training on third-view images.
    }
    \vspace{0.5em}
    \begin{tabular}{lccccc}
        \toprule
        \multirow{2}{*}{\textbf{Algorithms}} & \multicolumn{5}{c}{\textbf{No. of Instructions Chained}} \\
        \cmidrule(lr){2-6}
        & 1 & 2 & 3 & 4 & 5 \\
        \midrule
        HiP & 0.08 & 0.04 & 0.00 & 0.00 & 0.00 \\
        UniPi & 0.56 & 0.16 & 0.08 & 0.08 & 0.04 \\
        GR-1* & 0.75 & 0.45 & 0.2 & 0.15 & 0.10 \\
        SuSIE & 0.87 & 0.69 & 0.49 & 0.38 & \textbf{0.26} \\
        \midrule
        GEVRM (Ours) & \textbf{0.92} & \textbf{0.70} & \textbf{0.54} & \textbf{0.41} & \textbf{0.26} \\
        \bottomrule
    \end{tabular}
    \label{tab:calvin_performance}
\end{wraptable}

\begin{figure}[tbp]
\vspace{-2em}
\centering
\includegraphics[width=0.75\textwidth]{ICLR_2025_Template/figures/calvin_environment.pdf}
% \includegraphics[width=0.5\textwidth,height=0.28\textwidth]{LaTeX/pictures/data_dis_visua.png}
\caption{
% We compare the proposed method with baselines in the zero-shot setting of the CALVIN benchmark. 
The model is trained only on data collected in environments A, B, and C (\textbf{a}), and tested on environment D (\textbf{b}). 
Besides, we apply five perturbations to the image observations of environment D to further test the generalization of the model in more challenging scenarios (\textbf{c}).
} 
  \label{fig:calvinTask}
  \vspace{-1em}
\end{figure}

\textbf{Action execution comparison.}
We show the success rate of completing each language instruction in the chain in Tab.~\ref{tab:calvin_performance}. 
The model is trained on environments A, B, and C (Fig. \ref{fig:calvinTask} (\textbf{a})), and test in D (Fig. \ref{fig:calvinTask} (\textbf{b})).
Compared with the baseline, the GEVRM has a significant performance improvement. This shows that our method based on the IMC principle has better goal generation ability when facing new environments and induces the robot to predict more general decision actions.
% Compared with baseline methods, our proposed GEVRM achieves the best performance. 
% Compared with baselines of the video diffusion generation category, our method GEVRM has a significant performance improvement.
% Therefore, the proposed method, based on the IMC principle, has better robot goal generation ability when facing new environments and induces the robot to predict more general decision actions.
% This shows that our method has better robot behavior skill inference ability and environmental perception understanding ability, as well as more robust low-level actions. 
% Moreover, the previous state-of-the-art baseline method SuSIE only generates future image sub-goals, rather than future video trajectories. 
% This will cause the model to lack the understanding of the temporal consistency of the robot's behavior trajectory, which affects the robot's action execution accuracy.



\begin{table}[ht]\small
    \centering
    \vspace{-1.5em}
    \caption{Generalization on perturbed environments in CALVIN (train A, B, C → perturbed test D).
    % Our proposed method surpasses the previous state-of-the-art method SuSIE in terms of task success rate and task completion length on average.
     % The best results for each task are bolded.
     % The 'AATL' is short for 'Average Achieved Task Length'.
    }
    \vspace{0.5em}
    \renewcommand{\arraystretch}{0.8}
    \begin{tabular}{l c ccccc c}
        \toprule
        \multirow{2}{*}{\textbf{Five Perturbed Tasks}} & \multirow{2}{*}{\textbf{Algorithms}} & \multicolumn{5}{c}{\textbf{No. of Instructions Chained}} & \multirow{2}{*}{\textbf{Avg. Length ($\uparrow$)}} \\
        \cmidrule(lr){3-7}
        & & 1 & 2 & 3 & 4 & 5 & \\
        \midrule
        % \multirow{2}{*}{Image Shift} & SuSIE & \textbf{0.56} & 0.28 & 0.08 & \textbf{0.04} & 0.00 & 0.96 \\
        % & GEVRM (Ours)  & 0.52 & \textbf{0.40} & 0.08 & 0.00 & 0.00 & \textbf{1.00} \\
        % \midrule
        % \multirow{2}{*}{Image Rotation} & SuSIE & 0.48 & 0.16 & 0.08 & 0.00 & 0.00 & 0.72 \\
        % & GEVRM (Ours)   & \textbf{0.60} & \textbf{0.32} & \textbf{0.12} & \textbf{0.08} & \textbf{0.04} & \textbf{1.16} \\
        % \midrule
        % \multirow{2}{*}{Color Jitter} & SuSIE& \textbf{0.72} & 0.36 & 0.16 & 0.12 & 0.08 & 1.44 \\
        % & GEVRM (Ours)   & 0.64 & \textbf{0.48} & \textbf{0.32} & 0.12 & 0.08 & \textbf{1.64} \\
        % \midrule
        % \multirow{2}{*}{Image Occlusions} & SuSIE & 0.72 & 0.48 & 0.32 & \textbf{0.32} & \textbf{0.24} & 2.08 \\
        % & GEVRM (Ours)   & \textbf{0.92} & \textbf{0.68} & \textbf{0.48} & 0.24 & 0.20 & \textbf{2.52} \\
        % \midrule
        % \multirow{2}{*}{Noise Interference} & SuSIE& 0.32 & 0.04 & 0.00 & 0.00 & 0.00 & 0.36 \\
        % & GEVRM (Ours)   & \textbf{0.80} & \textbf{0.48} & \textbf{0.32} & \textbf{0.12} & \textbf{0.04} & \textbf{1.76} \\
        % \midrule
        % \multirow{2}{*}{Average} & SuSIE & 0.56 & 0.26 & 0.13 & 0.10 & 0.06 & 1.11 \\
        % & GEVRM (Ours)  & \textbf{0.70} & \textbf{0.47} & \textbf{0.26} & \textbf{0.11} & \textbf{0.07} & \textbf{1.62} \\

       \multirow{4}{*}{Average} & SuSIE & 0.56 & 0.26 & 0.13 & 0.10 & 0.06 & 1.11 \\
         & RoboFlamingo & 0.63&0.35&0.18&0.09&0.05&1.31  \\
         & GR-1 & 0.67&0.38&0.22& \textbf{0.11} &0.06&1.44  \\
        & GEVRM (Ours)  & \textbf{0.70} & \textbf{0.47} & \textbf{0.26} & \textbf{0.11} & \textbf{0.07} & \textbf{1.62} \\
        
        \bottomrule
    \end{tabular}
    \label{tab:calvin_harder_tasks}
    \vspace{-1em}
\end{table}

\textbf{Action execution comparison under external perturbations.}
To thoroughly evaluate the performance of our proposed GEVRM against the baseline SuSIE, we tested both models across five more challenging scenarios (Fig. \ref{fig:calvinTask} (\textbf{c})). 
% The results are summarized in Tab.~\ref{tab:calvin_harder_tasks}.
The average performance on five perturbed tasks is in Tab.~\ref{tab:calvin_harder_tasks}, and the specific results are in Appendix Tab.~\ref{tab:calvin_harder_tasks_all}.
These scenarios were crafted to challenge the models' perception of environmental stimuli and comprehension of physical laws. 
% GEVRM surpassed SuSIE in these tests, with an average success rate increase across all tasks. The second task exhibited the most significant enhancement, with a success rate rise from $0.26$ to $0.47$, surpassing the gains in the first and third tasks.
% Moreover, GEVRM also improved the average task completion length by $45.9\%$, elevating it from $1.11$ to $1.62$.
The results show that GEVRM can well simulate robot response and guide the policy to generate robust decision actions to resist external perturbations.
More action execution comparison results are in Appendix Tab.~\ref{APP:tab:calvin_performance}.
Real-world deployment of GEVRM is in Appendix Sec.~\ref{App:Real-World Tasks.}.

% \textcolor{red}{The SuSIE simply predicts future images at a certain time step as a sub-goal of the low-level policy. 
% This makes it difficult for the model to understand the temporal dependencies of the robot's behavior trajectory, resulting in a sharp drop in the generalization performance of the model when there is a large disturbance in the external perception. 
% In contrast, our proposed model has better spatio-temporal consistency, long-range coherence, and object permanence through random mask training and SAIT visual representation enhancement. 
% This gives the model stronger visual perception and more robust action execution, allowing it to better cope with external perceptual perturbations and generalize to new, unseen environments and tasks.}


\vspace{-0.5em}
\subsection{Ablation study}
\vspace{-0.5em}
% In this section, we conduct an ablation study on the proposed method to verify the effectiveness of the components VAE and SAIT. 
% Specifically, on CALVIN Env. A, B, and C data, we first compare the impact of fine-tuning or not fine-tuning VAE on model performance when training the robot behavior planner. 
% Then compare the impact of using or not using SAIT on model performance when training the robot's goal-guided policy.

In this section, we perform an ablation study to assess the contributions of VAE and state alignment to our method. 
We evaluate the effects of VAE fine-tuning and state alignment application on model performance across CALVIN environments A, B, and C, focusing on robot behavior planning and goal-guided policy training.
%
Results in Fig. \ref{fig:ablationStudy} (a) indicate that omitting VAE fine-tuning or state alignment integration significantly degrades model performance on CALVIN Env. D, due to the VAE's pre-training on diverse video data enhancing spatio-temporal consistency and subsequent fine-tuning on robot data aiding in decision-making generalization. 
State alignment bolsters the policy's visual state representation for better task generalization.
%
Moreover, the hyperparameter $\lambda$, crucial for balancing expert imitation and state alignment in policy training, was tested across five values (Fig. \ref{fig:ablationStudy} (b)). 
Performance metrics varied minimally, showing robustness to $\lambda$ adjustments, with $\lambda=1$ optimal for our method.
%
To illustrate the impact of state alignment on goal-guided representation, we conducted a visual comparison experiment.
We utilize \textit{T-SNE}~\citep{van2008visualizing} to analyze the latent space representations of current and future image states with and without state alignment in the CALVIN ABC → D “Noise Interference” task, and the results are shown in Fig.~\ref{fig:embedding_tsne} and Appendix Fig.~\ref{Appe:embedding_tsne_goal}.
Results indicated that state alignment improves clustering and classification by enhancing intra-category cohesion and inter-category separation. 
Additionally, state alignment ensures temporal consistency in image state sequences, thereby bolstering the policy's environmental and task recognition, and facilitating generalization to novel scenarios.
The ablation experiments on the execution efficiency of the goal generation and the goal-guided diffusion policy are in Appendix Tab.~\ref{tab:goal_efficiency} and ~\ref{tab:policy_efficiency}, respectively.




\begin{figure}[tbp]
\centering
\includegraphics[width=0.85\textwidth]{ICLR_2025_Template/figures/ablation_study.pdf}
\vspace{-1em}
\caption{
Ablation study on the CALVIN ABC → D. (a) We compare different training paradigms.
(b) We examine the impact of different values of the state alignment (SA) hyperparameter $\lambda$.
} 
  \label{fig:ablationStudy}
\end{figure}

\begin{figure}[tbp]
\centering
\includegraphics[width=0.8\textwidth]{ICLR_2025_Template/figures/embedding_tsne.pdf}
% \includegraphics[width=0.5\textwidth,height=0.28\textwidth]{LaTeX/pictures/data_dis_visua.png}
\vspace{-1em}
\caption{
% Comparing latent space representations without and with state alignment (SA).
% The current image state with state alignment exhibits enhanced cluster centers, category boundaries, and temporal consistency.
Comparison of state representations. 
The representations with state alignment (SA) show enhanced cluster centers, class boundaries, and temporal consistency.
} 
 \vspace{-1.5em}
  \label{fig:embedding_tsne}
\end{figure}

% \begin{figure}[tbp]
% \centering
% \includegraphics[width=0.95\textwidth]{ICLR_2025_Template/figures/embedding_tsne.png}
% % \includegraphics[width=0.5\textwidth,height=0.28\textwidth]{LaTeX/pictures/data_dis_visua.png}
% \caption{
% Visual comparison of the latent space representation of the future target image state and the current image state with and without SATI. The state representation sequence with SATI has better cluster centers, category boundaries, and temporal consistency.
% } 
%   \label{fig:embedding_tsne}
% \end{figure}

% To further explore the impact of SAIT on the visual representation of the goal-reaching strategy, a visual comparison experiment was introduced.
% We use the T-SNE algorithm \citep{van2008visualizing} to compare the latent space representation of the future target image state and the current image state with and without SATI in several subtasks of the challenging ABC→D Test 2 "Noise Interference" task, as shown in Fig. \ref{fig:embedding_tsne}.
% The visual comparison results of the target image states show that the use of SAIT helps the visual representation form better cluster centers and classification boundaries. 
% In other words, the image trajectory representations of the same category are more aggregated, while those of different categories are more dispersed. 
% The visual comparison of image states confirms that the image trajectory sequence based on SAIT has better temporal consistency. 
% Therefore, SAIT enhances the recognition ability of the goal-reaching strategy for different environments and tasks, and can better generalize to new scenarios.

