\section{Introduction}
The pursuit of robust and adaptable robotic systems is the cornerstone of embodied general intelligence.
Recently, with the successful advancement of large-scale robot data collection~\citep{vuong2023open}, universal state representation learning~\citep{li2023vision,du2024learning}, and expressive policy learning~\citep{brohan2022rt,chi2023diffusion}, the research on vision-language-action (VLA) models for robots has made significant progress. 
The above strategies have been shown to be effective in estimating the robot's state and generating robust actions in a variety of environments, from physical simulators~\citep{mu2021maniskill,ding2023quar} to carefully designed real-world environments. 
However, these carefully designed environments do not take into account the inevitable external perturbations during deployment, such as fluctuating lighting conditions or video stream noise due to signal transmission problems. 
When VLA models are deployed in these non-ideal environments, external perturbations will bring unpredictable state information to the robot. 
This makes VLA produce fragile and unstable actions in inaccurate environmental states, resulting in a significant decrease in its generalization performance.
Therefore, enhancing the robustness of VLA models to cope with the inevitable external perturbations when deployed is an ongoing challenge. 

In the fields of computer vision~\citep{simard2003best,cirecsan2011high,ciregan2012multi,chen2020simple} and reinforcement learning~\citep{laskin2020reinforcement,hansen2021stabilizing,zheng2023stabilizing}, image augmentation is a common technique to alleviate the problem of model over-fitting, resist input image perturbations, and enhance model robustness. 
The idea is to apply task label-invariant transformations to the model's input images. 
For example, for object recognition tasks, image flipping and rotation do not change the semantic labels. 
Therefore, this technique has also been applied to robot visual language manipulation tasks. 
Some previous work has utilized vision as a general medium to develop specific agents that can plan various tasks through imagination and execution ~\citep{black2023zero,Yang2023LearningIR,du2024learning}. 
These methods involve generative models for predicting future videos or target images, followed by goal-conditioned policies that transform visual plans into actual actions. 
Image augmentation technology is utilized when training goal-conditioned policies, which to some extent alleviates the policy's over-fitting of specific tasks. 
% However, the expressive power of the future goal image (or video) state generated by these models is limited, and image augmentation can only generalize the model within a narrow task distribution.
However, these models are limited by their generative capabilities, the future goal image (or video) states they generate are not expressive enough, and image augmentation only allows the model to generalize within a narrow task distribution.
% The model as a whole lacks strong adaptability to environmental perturbations, and it struggles to generate robust and stable actions on a wide task distribution.
It lacks strong resilience to environmental perturbations and struggles to produce actions that are consistently effective across diverse task scenarios.

\begin{figure}[t]
\vspace{-2em}
\centering
\includegraphics[width=0.9\textwidth]{ICLR_2025_Template/figures/motivation.pdf}
% \includegraphics[width=0.5\textwidth,height=0.28\textwidth]{LaTeX/pictures/data_dis_visua.png}
\caption{
We are inspired by the classical internal model control (\textbf{a}) in automation systems. 
The principle illustrates that a closed-loop system equipped with an internal model that accounts for external input signals can precisely follow the reference input and effectively neutralize the perturbations.
In this work, an internal model visuomotor control framework (\textbf{b}) is motivated and designed.
We leverages a text-guided video model for generating highly expressive visual goal states as reference input, goal-state and current-state internal encoders for modeling responses, and a goal-guided policy for robust action generation.
} 
\vspace{-2em}
  \label{fig:IMC_motivation}
\end{figure}
We are inspired by the principle of classical internal model control (IMC) shown in Fig.\ref{fig:IMC_motivation} (\textbf{a}). 
The core idea of this principle~\citep{rivera1986internal} is that in a closed-loop control system, by building a model inside the controller that can simulate external perturbations and reference inputs, the desired output can be accurately tracked and the perturbations can be effectively offset. 
That is, it leverages an internal model to replicate the system's behavior and subsequently assess the system's perturbations, thereby augmenting the closed-loop stability.  It is widely believed that intelligent mammals also rely on internal models to generate their actions ~\citep{nguyen2011model} and such mechanism is also revealed and supported by behavioral, neurophysiological, and imaging data~\citep{kawato1999internal}. More importantly, the integration of the internal model into the robot control system ~\citep{emken2005robot} has been verified to enhance the robustness of the robot motion control. 
However, the results are limited to specific scenarios and hard to extend to more complex and general tasks, such as visual-language manipulation. 
How to instantiate the internal model in the VLA framework to improve the robustness of decision actions has not been explored.


To this end, we propose \textbf{GEVRM}, a \textbf{G}oal-\textbf{E}xpressive \textbf{V}ideo Generation Model for \textbf{R}obust Visual \textbf{M}anipulation.
As shown in Fig.\ref{fig:IMC_motivation} (\textbf{b}), to effectively implement the classic IMC principle in the VLA model, some components of our method are adjusted accordingly. 
\textbf{1) Goal generation.} Taking video frames as a universal interface to describe the robot state, we introduce an advanced text-guided video diffusion generation model as a robot behavior planner to generate future goal frames as reference input. 
To improve the expressiveness of future goal states, we train the visual planner through efficient video spatiotemporal compression and random mask strategies to prioritize the understanding of physical world laws~\citep{phyworld}.
\textbf{2) State alignment.} We estimate system perturbations by leveraging the simulated responses of the robot. 
These responses are called internal embeddings and are extracted from the robot state. 
Since the responses are inherently embedded in the robot's historical observations, the internal embeddings can be optimized through prototypical contrastive learning~\citep{caron2020unsupervised,yarats2021reinforcement,deng2022dreamerpro} to align the robot's future expressive goal states with its current state. 
This enables the model to implicitly infer and distinguish perturbations from the external environment.
\textbf{3) Goal-guided policy.} We propose a diffusion policy conditioned on the generated highly expressive goals to better model the multi-modal task distribution of robot manipulation~\citep{chi2023diffusion}. 
This policy and the aforementioned internal embedding are jointly optimized through inverse dynamics and contrastive learning objectives to track highly expressive goals well even in the presence of perturbations. 
In summary, our contributions are threefold:
\begin{itemize}
\item We introduce GEVRM, a novel robust VLA model that incorporates the IMC principle to enhance robot visual manipulation.
\item We study how to obtain highly expressive goals with a text-guided video generation model and align state representations through prototypical contrastive learning to resist external perturbations at deployment.
\item Extensive experiments verify the effectiveness and advancement of the proposed GEVRM. 
It significantly outperforms the previous state-of-the-art on the CALVIN benchmark with standard and external perturbations. 
The expressiveness of the goal states generated in real visual manipulation is significantly improved compared to previous baseline methods.
\end{itemize}

