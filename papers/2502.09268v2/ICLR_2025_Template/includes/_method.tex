\section{Problem Formulation}
In this work, we investigate how to generate highly expressive goal states and induce robust actions to be resilient to external disturbances. 
Formally, we study robot trajectory and action generation in a non-Markov decision process framework, which is specified by the following tuple: ${M} := (\mathcal{X}, \mathcal{A}, \mathcal{G}, \mathcal{T}, \rho_0)$, where $\mathcal{X}$ and $\mathcal{A}$ denote the image state and action spaces, $\mathcal{G}$ represents the language text goal space, $\mathcal{T}(\bx_{t+1}|\bx_{1:t},\ba_t,\bg)$ is the transition dynamics, and $\rho_0(\bx)$ is the initial image state distribution.
We aim to generate expressive future image goal states and current actions to be performed given abstract language instructions $g$ and historical image sequence states (i.e., videos) $\tau_{0:t}$ in visual manipulation tasks: $p(a_t,\tau_{t:T}|g,\tau_{0:t})$.
The problem is decomposed into two hierarchical levels: 1) Robot behavior planning $--$ given language instructions $g$ and historical video states $x_{0:t}$, infer image goal states $x_{t:T}$; 2) Robot action prediction $--$ given historical and inferred expressive future image goal states $\tau_{0:T}$, predict the current action $a_t$ to be performed. 
The decoupled process can be expressed as:
\begin{equation}
p_{\Theta}(a_t,\tau_{t:T}|g,\tau_{0:t})=p_{\phi}(\tau_{t:T}|g, \tau_{0:t})p_{\varphi}(a_t|\tau_{0:T}).
\end{equation}
This decoupling process greatly reduces the model training's dependence on language, image sequence, and robot action pairs. 
Specifically, the training of the behavior trajectory planning model $p_{\phi}$ only requires text-video pairs $\mathcal{D}_{\tau,g}=\{(\tau^i,g^i)\}_{i=0}^{I}$ without robot action labels, which can be derived from large-scale video clips with language labels and robot sequence decision data with text annotations on the Internet. 
The training of $p_{\varphi}$ only requires a small amount of play data $\mathcal{D}_{\tau,a}=\{(\tau^j,a^j)\}_{j=0}^{J}$ without language labels for specific downstream tasks.
In the test phase, given a natural language description $g_{test}$ and an initial image state $x_{0,test}$ of a new task, We need to evaluate not only the expressiveness of the future goal states inferred by the model but also the success rate of completing the task under external perturbations.

\section{Methodology}
Our goal is to build a robust VLA model that incorporates IMC concepts into robotic visuomotor control, as shown in Fig.~\ref{fig:method_overview}. 
To set highly expressive goals before execution, we introduce a powerful video generation model as a visual planner (Section \ref{subsection:Robot Behavior Planner}). 
In Section \ref{subsection:Goal-guided action predictor}, we detail how to align the goal state to evaluate perturbations and show how to induce the generation of robust decision actions. 
Finally, in Section \ref{subsection:Test-time execution pipeline}, we implement the overall test-time execution pipeline of GEVRM.
\begin{figure}[tbp]
\centering
\includegraphics[width=0.95\textwidth]{ICLR_2025_Template/figures/method_overview.pdf}
% \includegraphics[width=0.5\textwidth,height=0.28\textwidth]{LaTeX/pictures/data_dis_visua.png}
\caption{
% The proposed GEVRM model architecture. 
% Given the language instruction and robot's historical observation video, the robot behavior planner infers the future image goal.
% The goal state and current state are encoded via contrastive learning to simulate responses and evaluate perturbations.
% The goal-guided diffusion policy predicts the 7-dimensional robot control action. 
% The robot behavior planner consists of three key components: (1) a text encoder \textit{T5}~\citep{2020t5}  for language commands, (2) a 2D VAE~\citep{rombach2022high2dvae} and a causal 3D VAE~\citep{yu2023language3dvae} for video spatio-temporal compression, and (3) a diffusion transformer \textit{Open-Sora 1.2}~\citep{opensora} for video generation.
The proposed GEVRM model.
First, the \textit{T5}~\citep{2020t5} model is utilized to encode language instructions, and 2D and 3D VAE are utilized to compress and restore the original pixel space of the robot image state sequence, followed by the DiT module and random mask mechanism to generate the goal image state.
Then, through prototypical contrast learning, the current and goal states are aligned to simulate responses and evaluate perturbations.
Finally, the goal-guided policy predicts the 7-dimensional robot decision action.
} 
  \label{fig:method_overview}
\end{figure}

\subsection{Robot Behavior Planner} \label{subsection:Robot Behavior Planner}
Inspired by the recent success of video generation models~\citep{videoworldsimulators2024,esser2024scaling}, we seek to build a text-guided video diffusion transformer model as a robot behavior planner $P_{\phi}(\tau_{t:T}|g, \tau_{0:t})$ for robotic goal states generation. 
The planner can faithfully synthesize future goal image frames based on historical video observations and abstract textual task descriptions. 
Planning via video generation requires a model that can both generate constrained videos starting from the given video and complete the downstream task.
Specifically, to obtain highly expressive future goal states, three core aspects need to be considered when designing a robot behavior planner: \textbf{1)} video spatio-temporal compression to reduce computing resources, \textbf{2)} a random mask mechanism that highlights the understanding of physical laws and object consistency, and \textbf{3)} a strong model backbone and an efficient training paradigm.

\textbf{Video spatio-temporal compression.} 
% If the diffusion transformer model performs expensive function evaluation in the original robot video pixel space, it will lead to huge demands for computational time and energy resources. 
% Therefore, it is necessary to design a variational autoencoder (VAE) model that allows the diffusion transformer model to learn in a low-dimensional dense space, thereby significantly reducing the computational complexity. 
% When designing the auto-encoding model, considering that the computational resources for training 3D VAE are also expensive, we hope to reuse the knowledge learned in 2D VAE. 
% In fact, after compression by 2D VAE, adjacent features in the temporal dimension are still highly correlated. 
% Therefore, in the video encoding stage, we first compress the video by $8x8$ in the spatial dimension using 2D VAE \citep{rombach2022high2dvae}, and then compress it by $4x$ in the temporal dimension using 3D VAE \citep{yu2023language3dvae}. 
% In the video decoding stage, the video is first restored in the temporal dimension and then in the spatial dimension. 
% 3D VAE uses Causal 3D convolution instead of ordinary 3D CNN, so that the output of each frame depends only on the previous frames \citep{yu2023language3dvae}. 
% If the diffusion transformer model carries out complex functional evaluations within the original pixel space of robot video data, it results in substantial computational time and energy expenditure. 
% Consequently, there is a requirement to engineer a variational autoencoder (VAE) framework that enables the diffusion transformer to operate within a reduced-dimensional, information-rich space, consequently achieving a marked decrease in computational overhead.
% In the process of architecting the auto-encoding framework, acknowledging the high computational costs associated with training a 3D VAE, we aim to leverage the insights garnered from 2D VAE.
% In practice, following spatial compression by 2D VAE, there remains a significant temporal correlation between adjacent features. 
% The diffusion transformer's complex computaion of robot video data in its native pixel space demands considerable computation and power. 
Diffusion transformers (DiT) require massive computational resources to perform complex operations on robot image state sequence data in native pixel space.
To alleviate this problem, we first compress the original pixel space with 2D VAE, and then further compress it with 3D VAE to obtain an information-rich low-dimensional dense space.
The advantage is that the high computational cost of 3D VAE in the original pixel space is avoided. 
In fact, after the spatial compression of 2D VAE, there is still considerable temporal correlation between adjacent features.
Specifically, during the image state sequences encoding phase, we initially affect spatial dimension reduction by a factor of $8\times8$ through the application of 2D VAE~\citep{rombach2022high2dvae}, and subsequently condense the temporal dimension by a factor of $4\times$ via 3D VAE~\citep{yu2023language3dvae}.
In the image state sequences decoding phase, the temporal dimension is restored prior to the spatial dimension. 
The 3D VAE incorporates Causal 3D convolutional layers in place of 3D CNNs, ensuring that each frame's output is contingent solely on its antecedent frames~\citep{yu2023language3dvae}.

\textbf{Random mask mechanism.} 
% To effectively realize the conditional generation of video (or image), we adopt a random mask strategy \citep{tay2022ul2}.
% Specifically, we randomly unmask the frames during training, including unmask the first frame, the first $k$ frames, the last frame, the last $k$ frames, the first frame and the last $k$ frames, random frames, etc. 
% In the test phase of the robot sequence decision task, we can only obtain historical image observations, but not future image observations. 
% Therefore, in the training phase of the model, the weight of unmask the first $k$ frames accounts for the highest proportion, which is $75\%$. 
% The others can be regarded as auxiliary tasks, accounting for $25\%$ together. 
% Although the idea of the mask strategy is simple and direct, it realizes the robot behavior planner to predict other image frames based on image frames at various time steps, which greatly deepens the model's understanding and cognition of object motion and temporal logical relationships.
To achieve efficient goal image synthesis, we implement a random mask mechanism~\citep{tay2022ul2}. 
Precisely, the training process involves the random unmasking of frames, encompassing scenarios such as revealing the initial frame, the first $h$ frames, the final frame, the last $h$ frames, a combination of the initial and last $h$ frames, and arbitrary frames.
During the testing phase, we have access to the historical image state but are devoid of future image state. 
Consequently, in the model's training regimen, the unmasking of the first $h$ frames is assigned the greatest weight, specifically $75\%$. The remaining unmasking strategies are categorized as supplementary objectives, collectively constituting the remaining $25\%$.
Though the masking mechanism is conceptually straightforward, it enables the robot behavior planner to anticipate subsequent frames based on various temporal snapshots, significantly enhancing the model's comprehension and perception of object dynamics and temporal sequential correlations.
For the specific parameter configuration of the random mask, please refer to Appendix Tab.~\ref{Appd:Behavior planner training optimizer hyperparameters}.

\textbf{Model backbone and training.}
% We leverage pre-trained Open-Sora 1.2~\citep{opensora} as the backbone of the robot behavior planning model and utilize a frozen T5 encoder~\citep{2020t5} to encode the language instructions. 
% Open-Sora 1.2 is a 1.1B text-conditioned video diffusion transformer model trained on $>30M$ text video data (about 80k hours), supporting video generation of different time periods, different resolutions, and various aspect ratios. 
% To obtain higher quality robot trajectory generation videos, inspired by the recent advanced diffusion model Stable Diffusion 3 \citep{esser2024scaling}, we fine-tune the Open-Sora 1.2 model with rectified flow \citep{liu2022flow} instead of DDPM \citep{ddpm} on the robot decision sequence data $D_{x,g}$. 
% Rectified flow can be utilized to learn a transfer mapping between the noise distribution and the real image distribution.
% It does this by connecting the straight line paths between samples and learning an ordinary differential equation model. 
% By utilizing rectified flow, we can reduce the number of video sampling steps from $100$ to $30$, thereby greatly accelerating training and reducing inference time.
% We employ the pre-trained \textit{Open-Sora 1.2}~\citep{opensora} as the backbone of our robot behavior planner, integrating a frozen \textit{T5} encoder~\citep{2020t5} to process language instructions~\citep{2020t5}. 
Our DiT module is derived from a pre-trained text-guided video generation model~\citep{opensora} and integrates a frozen \textit{T5} encoder~\citep{2020t5} to process language instructions~\citep{2020t5}.
% The \textit{Open-Sora 1.2}, is a $1.1B$ parameter text-guided video diffusion transformer, trained on over $30$ million text-video pairs, offering the capability to produce videos of varying durations, resolutions, and aspect ratios~\citep{opensora}. 
Drawing inspiration from the recent advancements in \textit{Stable Diffusion 3}~\citep{esser2024scaling}, we fine-tune robot behavior planner with \textit{Rectified Flow}~\citep{liu2022flow}, transcending the conventional \textit{DDPM}~\citep{ddpm}.
The \textit{Rectified Flow} facilitates the learning of a mapping from noise to real image distribution by solving an ordinary differential equation along straight paths between samples~\citep{liu2022flow}. 
% The rectified flow learns the mapping from noise to true image distribution by solving ordinary differential equations along straight-line paths between samples, and has been shown to be a more effective training paradigm~\citep{liu2022flow}.
% This approach allows us to condense the video sampling steps from $100$ to $30$, significantly enhancing training speed and reducing inference time.
% This approach has been shown to be a more efficient training paradigm and enables us to compress the video sampling steps from $100$ to $30$, significantly improving training speed and reducing inference time.
This approach has been shown to be a more efficient training paradigm, resulting in a significant reduction in video sampling steps, which in turn significantly increases the model training speed and reduces its inference time.

\subsection{Robot action prediction} \label{subsection:Goal-guided action predictor}
% To control robot decisions using a fine-tuned behavior planner, we also need to train a low-level controller to select appropriate robot actions. 
% The upper-level behavior planner generates future image sequences conditioned on natural language task descriptions and historical video observations, which serve as sub-goals for the low-level controller. 
% Therefore, our low-level controller can simply be a language-independent goal-reaching policy. 
% Furthermore, we design a clustering-based self-supervision method to enhance the robustness of the policy’s visual representation, thereby alleviating the visual inconsistency problem during policy training and testing.
The expressive goal state generated by the robot behavior planner is utilized to guide the prediction of decision actions.
From the visual goal state $x_{goal}$ and the current visual state $x$ to the final action $a$ output, our goal-guided $\pi_{\varphi}(a | x_{goal},x)$ can be divided into the following two parts:
\textbf{1) State alignment to simulate responses.} Extract informative features from the visual goal state and the current visual state and utilize prototypical contrast learning to align state representations, simulate robot responses, and evaluate disturbances. 
\textbf{2) Goal-guided action prediction.} Decode the goal and current internal compact encoded signals into actions that the robot can robustly perform.

% \textbf{Modeling and training goal-reaching policies.}
% Our goal-reaching policy is parameterized as $\pi_{\varphi}(a | x,x_{goal})$, where $x_{goal}$ is the future image goal state that the policy intends to achieve by taking action $a$ in the current image observation $x$.
\textbf{State alignment to simulate responses.}
% In traditional control systems, the IMC principle requires that an internal model of the system be included in the controller. 
% This internal model can predict and compensate for external disturbances and reference inputs to achieve accurate system performance and stability.
% To implement this principle in a learning-based framework, we first utilize a residual network \textit{ResNet 34}~\citep{he2016deep} as visual encoders for the goal state and the current state to convert the raw pixel input into a rich visual representation $f_{\psi}(x), f_{\psi'}(x_{goal})$. 
% For the current visual representation $f_{\psi}(x)$, the key is how to optimize it to simulate the robot's response, which is naturally embedded in the robot's goal visual state to evaluate external interference. 
% Based on the principle of IMC, we propose to model it in the latent space $z$ and optimize it through contrastive learning to align it with the goal visual state. 
% It is worth noting that learning representations to distinguish different instruction and visual representation is a long-standing scientific issue~\citep{pathak2017curiosity}, while few studies have explored their ability to simulate robot responses. 
% Internal embeddings with this ability are not available in pre-trained visual encoders or policy models learned only based on current observations (i.e., behavior cloning).
% Further visual embedding analysis is shown in the ablation experiment section.
Within the domain of classical control systems, the IMC framework necessitates the integration of a system's internal model within its controller. 
This internal model is capable of offsetting external disturbances and reference inputs, thereby ensuring precise system behavior and reliability.
To implement the IMC principle in a learning-based framework, we initially deploy residual networks \textit{ResNet 34}~\citep{he2016deep} to serve as visual encoders for both the goal and current states. 
This conversion transforms raw pixel data into an enriched visual representation $f_{\psi}(x)$, and $f_{\psi'}(x_{goal})$.
% Regarding the visual representation $f_{\psi}(x)$, the crux lies in its optimization to simulate the robot's response, inherently intertwined with the robot's intended visual state for assessing external interference.
% For the current visual representation $f_{\psi}(x)$, the key is how to optimize it to simulate the robot's response to evaluate external interference, and this response is naturally embedded in the robot's target visual state.
For the current visual state representation $f_{\psi}(x)$, the key is how to optimize it to simulate the robotic response for assessing external perturbations, with this response being inherently encoded within the visual goal state.
Adhering to the IMC principle, we advocate for the modeling of this process within the latent space $z$ and optimize it through contrastive learning to achieve alignment with the visual goal state.
% Following the IMC principle, we advocate modeling this process in the latent space $z$ with the goal of refining it via prototype contrastive learning to align with the target visual state.
% Based on the principle of IMC, we propose to model it in the latent space $z$ and optimize it through contrastive learning to achieve alignment with the target visual state.
% It is important to highlight that the development of representations adept at discerning diverse tasks and visual states represents a perennial challenge in scientific research~\citep{pathak2017curiosity}. Studies probing their capacity to mimic robotic reactions are relatively scarce. Pre-trained visual encoders or policy models that are solely trained on current observations, commonly known as behavior cloning, do not possess these internal embeddings.
% A more detailed examination of visual embeddings is presented in the subsequent ablation study segment.

In the play data $\mathcal{D}_{\tau,a}$, if a pair of $x_{goal}$ and $x$ originates from the same trajectory, they are a positive pair, otherwise they are a negative pair. 
These pairs are optimized by swapping the assigned tasks~\citep{caron2020unsupervised}.
Specifically, given a sequence of image observations sampled from the play data $\mathcal{D}_{\tau,a}$, we can derive the future goal image $x_{goal}$ from the transition as the target vector, and the current image observation $x$ as the source vector. 
The source and target vectors are fed to the source and target encoders, respectively, to obtain latent features, which are mapped onto the unit sphere in a high-dimensional space and $\mathcal{L}_2$-normalized: 
\begin{equation}
z=\frac{f_{\psi}(x)}{\Vert f_{\psi}(x) \Vert_2} \text{ , and } \widehat{z}=\frac{f_{\psi'}(x_{goal})}{\Vert f_{\psi'}(x_{goal}) \Vert_2}. 
\end{equation}
To predict cluster assignment probabilities $p^{source}$ and $p^{target}$ from latent features, we first apply $\mathcal{L}_2$ normalization to the prototypes to obtain a trainable normalized matrix $\mathbf{E}=\{e_n\}_{n=1}^N$, and then take the soft maximum of the dot product of the source or target vectors of all prototypes: 
\begin{equation}
p^{source}=\frac{e^{\frac{1}{\delta}ze_n}}{\sum_{n'}e^{\frac{1}{\delta}ze_{n'}}}
\text{, and }
p^{target}=\frac{e^{\frac{1}{\delta}\widehat{z}e_n}}{\sum_{n'}e^{\frac{1}{\delta}\widehat{z}e_{n'}}}.
\end{equation}
Here $\delta$ is the temperature parameter. 
$p^{source}$ and $p^{target}$ are the predicted probability that the current and goal image observations $x$ and $x_{goal}$ map to individual cluster with index $n$.
To obtain the predicted probabilities $\{q_{n}^{source}\}_{n=1}^{N}$ and $\{q_{n}^{target}\}_{n=1}^{N}$ targets while avoiding trivial solutions, the \textit{Sinkhorn-Knoppal} algorithm \citep{cuturi2013sinkhorn} is applied. 
Now that we have cluster assignment predictions and targets, the state alignment objective is to maximize the prediction accuracy:
\begin{equation}
\mathcal{J}_{\psi}=-\mathbb{ E}_{x,x_{goal}\sim\mathcal{D}_{a,x}}(q^{source}\ln p^{target}+q^{target}\ln p^{source}).
\end{equation}

It is worth noting that learning representations to distinguish different instruction and visual representation is a long-standing scientific issue~\citep{pathak2017curiosity}, while few studies have explored their ability to simulate robot responses. 
This capability is not directly accessible in pre-trained visual encoders or policy models learned based only on the current observation (i.e., behavior cloning).

\textbf{Goal-guided action prediction.} 
To keep the model concise, general, and generalizable, we leverage a goal-guided diffusion policy to decode the action output from the state encoding of the simulated response. 
We only utilize the third-view RGB images from the static camera as input and the action labels as training labels. 
The robot proprioceptive observation and gripper view images are not applied.
The action space of a 7-DoF robot is considered, consisting of the position of the end-effector $a_{EE}\in\mathbb{R}^6$ and the gripper state $a_{gripper}\in\{-1,1\}$. 
% Our policy is learned via an inverse dynamics objective πϕ(a0|O0, Ok) , where it infers the action a0 given the current observation O0 and the specified subgoal Ok . To improve the transferability of the framework, we only utilize the third-view RGB images as input and the action labels as training targets. State information such as proprioceptive signals or gripper view images are not used to facilitate manipulation. See Appendix B for more training details.
% Based on the robust visual representation, we further leverage a diffusion model to implement the goal-reaching policy, because diffusion-based policies can better capture the multi-modality in robotics data \citep{chi2023diffusion}, thereby improving the performance of various robotics tasks.
The goal-guided diffusion policy is a latent variable model using Markov noise and de-noising process, which can be utilized to model the parameterized behavior distribution $\pi_{\varphi}(a|z,\widehat{z}) = \int\pi_{\varphi}(a_{0:K}|z,\widehat{z})da_{1:K}$ for the latent variable $\{a_k\}_{k=1}^K$. 
The forward noise process follows a fixed variance schedule $\{\beta_k\}_{k=1}^K$, which follows the distribution
$q(a_k|a_{k-1})=\mathcal{N}(\sqrt{1-\beta_t}a_{t-1},\beta_{t}\mathcal{I})$.
Following \textit{DDPM}~\citep{ddpm}, our practical implementation involves directly parameterizing the score network $\pi_{\varphi}(a_{k-1}|a_k,z,\widehat{z},k)$ to recover the behavior cloning objective:
\begin{equation}
\mathcal{J}_{\varphi}=\mathbb{E}_{k\sim\mathbb{U}(1,K),\epsilon\sim\mathbb{N}(0,\mathcal{I}),x,x_{goal},a\sim\mathbb{D}_{x,a}}[\Vert \epsilon-\pi_{\varphi}(\sqrt{\widehat{\alpha}_k}a+\sqrt{1-\widehat{\alpha}_k}\epsilon),z,\widehat{z},k \Vert_2].
\end{equation}
% Therefore, we can obtain the optimization target of the final visual representation enhanced goal-reaching diffusion policy:
% Therefore, we can obtain the optimization objectives of the state encoding and goal-guided diffusion policy:
We utilize this objective to train a goal-guided policy and provide it with internal embeddings of the goal and current state. 
In each policy training iteration, the state encoding is optimized by the state alignment objective, which enables the policy to implicitly infer and distinguish perturbations from the external environment. 
Therefore, the final optimization objective of the state encoding and goal-guided diffusion policy is:
\begin{equation}
\mathcal{J}=\mathcal{J}_{\varphi}+\lambda\mathcal{J}_{\psi},   
\end{equation}
where $\lambda$ is a temperature parameter.
To sample from $\pi_{\varphi}(a_0|z,\widehat{z})$, a reverse diffusion process where $a_K \sim \mathbb{N}(0,\mathcal{I}) $ and $\epsilon\sim\mathbb{N}(0,\mathcal{I})$ is utilized, resampling at each step:
\begin{equation}
a_{k-1}=\frac{1}{\sqrt{\alpha_k}}(a_k-\frac{\beta_t}{\sqrt{1-\widehat{\alpha}_k}}\pi_{\varphi}(a_k|z,\widehat{z},k))+\sqrt{\beta_t}\epsilon \text{, for } k= \{K,\cdot\cdot\cdot,1\}.
\end{equation}

\subsection{Test-time execution pipeline of GEVRM}
\label{subsection:Test-time execution pipeline}
Once both the robot behavior planner $P_{\phi}$ and the goal-guided policy $\pi_{\varphi}$ are trained, they can be utilized to solve new manipulation tasks.
% based on user-specified natural language commands. 
Given a new scenario $ x_{0,test}$ and a new language command $ g_{test}$, GEVRM attempts to solve the task by iteratively generating highly expressive goal states and inducing goal-guided policies to achieve these sub-goals. 
% Initially, we sample the first goal state $x_{0,goal}\in Pθ_{\phi}(\cdot| x_{0,test}, g_{test})$. 
Initially, we sample a set of goals $\{x_{m,goal}\}_{m=0}^{M}\sim P_{\phi}(\cdot| x_{0,test}, g_{test})$, where $M$ represents the number of goal state generation.
We pass goal state $x_{t,goal}$ and current state $x_{t}$ through the state encoders over $L_{test}$ time steps to obtain the internal embedding and derive the goal-guided policy $\pi_{\varphi}$, where $L_{test}$ is the fixed interval number. 
After $L_{test}$ time steps, we refresh the goal states by sampling from the behavior planner again and repeat the process. 
The test execution of the algorithm is shown in Algo.~\ref{algo:test-time deployment}.
\begin{algorithm}
\caption{GEVRM: Test-time Execution}
\begin{algorithmic}[1]
\State Robot behavior planner $P_{\phi}$, state encoder $f_{\psi}$, goal state encoder $f_{\psi'}$, goal-guided policy $\pi_{\varphi}$, time limit $T$, goal sampling interval $L_{test}$, goal generation number $M$, initial state $x_{0,test}$, language instruction $g_{test}$.
\State  $t \leftarrow 0$
\While {$t \leq T$}
    % \State \textcolor{gray}{\# Behavior planner generates goals.}
    \State Sample goals $\{x_{m,goal}\}_{m=t}^{t+M}\sim P_{\phi}(\cdot| x_{t,test}, g_{test})$ \textcolor{gray}{\Comment{Behavior planner generates goals.}}
    % Sample $\widehat{\mathbf{x}}_{+} \sim p_\theta\left(\mathbf{s}_{+} \mid \mathbf{s}_t^{\text {test }}, l^{\text {test }}\right) \quad \triangleright$ Generate a new subgoal every $k_{\text {test }}$ steps
    \For{$l=1$ to $L_{test}$}
        % \State \textcolor{gray}{\# State encoding.}
        % \State $z_l=f_{\psi}(x_{l,test})$
        % \State $z_{l,goal}=f_{\psi'}(x_{l,goal})$
        \State $z_t \leftarrow \frac{f_{\psi}(x_{t,test})}{\Vert f_{\psi}(x_{t,test}) \Vert_2}$ \textcolor{gray}{\Comment{State encoding and $\mathcal{L}_2$ normalization.}}
        % \State \textcolor{gray}{\# Goal state encoding.}
        \State $\widehat{z}_l \leftarrow \frac{f_{\psi'}(x_{l,goal})}{\Vert f_{\psi'}(x_{l,goal}) \Vert_2}$ \textcolor{gray}{\Comment{Goal state encoding and $\mathcal{L}_2$ normalization.}}
        % \State \textcolor{gray}{\# Goal-guided action prediction.}
        \State Sample action $a_t \sim \pi_{\varphi}(\cdot|z_t,\widehat{z}_l)$ \textcolor{gray}{\Comment{Goal-guided action prediction.}}
        % \State Execute $a_t$ in the environment, return $x_{t+1,test}$
         \State$x_{t+1,test}  \leftarrow  \text{Env.Step}(a_t) $
        % \State $x_{t,test} \leftarrow x_{t+1,test}$
        \State $t \leftarrow t+1$
        
    \EndFor
\EndWhile
\end{algorithmic}
\label{algo:test-time deployment}
\end{algorithm}


% Algorithm 1 SuSIE: Zero-Shot, Test-Time Execution
% Require: Subgoal model $p_\theta\left(\mathbf{s}_{+} \mid \mathbf{s}_t, l\right)$, policy $\pi_\phi\left(\mathbf{a} \mid \mathbf{s}_t, \mathbf{s}_{+}\right)$, time limit $T$, subgoal sampling interval $k_{\text {test }}$, initial state $\mathbf{s}_0^{\text {test }}$, language command $l^{\text {test }}$.
%     $t \leftarrow 0$
%     while $t \leq T$ do
%         Sample $\widehat{\mathbf{s}}_{+} \sim p_\theta\left(\mathbf{s}_{+} \mid \mathbf{s}_t^{\text {test }}, l^{\text {test }}\right) \quad \triangleright$ Generate a new subgoal every $k_{\text {test }}$ steps
%         for $j=1$ to $k_{\text {test }}$ do
%             Sample $\mathbf{a}_t \sim \pi_\phi\left(\mathbf{a} \mid \mathbf{s}_t^{\text {test }}, \widehat{\mathbf{s}}_{+}\right) \quad \triangleright$ Predict the action from current state and subgoal
%             Execute $\mathbf{a}_t$
%             $\mathrm{s}_{t+1}^{\text {test }} \leftarrow$ robot observation
%             $t \leftarrow t+1$
%         end for
%     end while
