\section{RELATED WORK}


\noindent\textbf{Vision-Language-Action models.} 
With the rise of extensive multi-task robotic datasets~\citep{vuong2023open}, the robotics community is increasingly focusing on multi-task execution capabilities. The Vision-Language-Action models~\citep{brohan2022rt,yue2024deer} have gained traction for their ability to use language for goal commands, enabling robots to make informed decisions based on visual perceptions. Early studies~\citep{brohan2022rt, Wu2023UnleashingLV} utilized cross-modal attention between language and vision, but limited model performance hindered effectiveness. Recently, attention has shifted to large foundational models~\citep{alayrac2022flamingo,li2023vision,kim2024openvla}, for improved versatility. However, text descriptions often lack detail about environmental states, complicating cross-morphology tasks. As a result, some researches~\citep{du2024learning,ko2023learning,zhou2024robodreamer,black2023zero,ajay2024compositional,yang2023learning} now leverage vision as a universal medium, employing generative models to forecast future actions, followed by goal-conditioned policies for execution.
UniPi~\citep{du2024learning} was one of the first to leverage internet-scale data to train a text-conditioned video generator, using an inverse dynamics model to estimate actions. Similarly,  SuSIE~\citep{black2023zero} uses an image-editing model to plan high-level sub-goals for low-level controllers, while ADVC~\citep{ko2023learning} infers actions from predicted video content with dense correspondences.
These efforts aim for a universal state representation but fall short for two reasons. First, existing visual plans experience temporal and spatial inconsistencies due to poor dynamics modeling. We propose a robust video generation model that addresses this issue and enhances action execution. Second, prior work focuses on controlled environments, overlooking the robot's responses to external interference. Our GEVRM method employs contrastive learning for state alignment, effectively simulating responses and resisting disturbances. Together, these elements define our expressive goal representation.


\noindent\textbf{Internal Model Control framework.} 
The IMC framework is a widely recognized control strategy that leverages an internal model of the system to predict future behavior and adjust control actions accordingly, making it highly robust against disturbances and model inaccuracies. First introduced by Garcia and Morari (1982), IMC has been applied extensively in both linear and nonlinear process control, offering significant benefits in terms of stability and adaptability~\citep{garcia1982internal,rivera1986internal,morari1989robust}. Its feedback mechanism allows for real-time adjustments, particularly valuable in dynamic environments such as robotics, where precision is critical. IMCâ€™s design has been further explored and refined for multivariable and complex systems, proving its versatility and robustness in various control applications~\citep{skogestad2005multivariable}.
However, most previous research works are limited to specific control scenarios and are difficult to extend to general visual language manipulation tasks.
More recently, inspired by classical closed-loop control systems, a closed-loop visuomotor control framework~\citep{bu2024closed} has been proposed that incorporates feedback mechanisms to improve adaptive robot control.
Different from these works, we study how to effectively instantiate internal models in the VLA framework to improve the robustness of decision actions.

