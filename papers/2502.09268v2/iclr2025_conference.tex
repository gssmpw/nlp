
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}


% add by myself
\usepackage{booktabs} % Add this line in your preamble if it's not already included
\usepackage{multirow}


% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx} 
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{wrapfig}
\usepackage{subfigure}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{booktabs}
\usepackage{colortbl} % 导入 colortbl 宏包
\usepackage{xcolor} 
\usepackage{authblk}
\definecolor{deepyellow}{RGB}{0,0,128}

\usepackage{multirow}

\newcommand{\lbr}{\left [}
\newcommand{\rbr}{\right ]}
\newcommand{\lpa}{\left (}
\newcommand{\rpa}{\right )}

\newcommand{\bx}{\mathbf{x}}
\newcommand{\ba}{\mathbf{a}}
\newcommand{\bg}{\mathbf{g}}
\newcommand{\B}{\mathcal{B}}
\newcommand{\G}{\mathcal{G}}
\newcommand{\D}{\mathcal{D}}

% \title{Learning Robust Visuomotor manipulation with Internal Model}
% \title{Robust Video-Generated Internal Model for Visual Robot Manipulation}
\title{GEVRM: Goal-Expressive Video Generation Model For Robust Visual Manipulation}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.
\author{
  Hongyin Zhang$^{1,2}$ \quad
  Pengxiang Ding$^{1,2}$ \quad
  Shangke Lyu$^{2}$  \quad
  Ying Peng$^{2}$  \quad
  Donglin Wang$^{2}$\thanks{Corresponding author.} \\
  $^{1}$ Zhejiang University. $^{2}$ Westlake University. \\
  \texttt{wangdonglin@westlake.edu.cn}
}
% \author[1]{}
% \author[2]{}
% \author[2]{}
% \author[2]{}
% \author[23]{}
% \affil[1]{}
% \affil[2]{}
% \affil[3]{\footnote{Corresponding Author.} wangdonglin@westlake.edu.cn}
% \author{Hongyin Zhang  \\
% % \thanks{ Use footnote for providing further information
% % about author (webpage, alternative address)---\emph{not} for acknowledging
% % funding agencies.  Funding acknowledgements go at the end of the paper.} \\
% % School of Engineering\\
% Zhejiang University\\
% Hangzhou, Zhejiang 310000, China \\
% % \texttt{\{wangdonglin\}@westlake.edu.cn} \\
% \And
% Pengxiang Ding, Shangke Lyu, Ying Peng, Donglin Wang \\
% % School of Engineering\\
% Westlake University\\
% Hangzhou, Zhejiang 310000, China \\
% \texttt{\{wangdonglin\}@westlake.edu.cn} \\
% \AND
% Coauthor \\
% Affiliation \\
% Address \\
% \texttt{email}
% }

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\input{ICLR_2025_Template/includes/_abstract}
\input{ICLR_2025_Template/includes/_introduction_928}
\input{ICLR_2025_Template/includes/_relatedwork}
\input{ICLR_2025_Template/includes/_method}
\input{ICLR_2025_Template/includes/_experiment}
\input{ICLR_2025_Template/includes/_cons}

% \subsubsection*{Author Contributions}
% If you'd like to, you may include  a section for author contributions as is done
% in many journals. This is optional and at the discretion of the authors.

\subsubsection*{Acknowledgments}
This work was supported by the National Science and Technology Innovation 2030 - Major Project (Grant No. 2022ZD0208800), and NSFC General Program (Grant No. 62176215).

\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\appendix
\clearpage
\section{Appendix}
% \rowcolor{gray!10} % 设置背景颜色为浅灰色
% \multicolumn{2}{c}{\textbf{HIM hypers}} \\ % 合并单元格并添加标题
% \rowcolor{white} % 恢复白色背景


\subsection{Environment perturbations.} \label{Appe:Environment perturbations.}

\textbf{CALVIN Datasets.} We conduct experiments on CALVIN \citep{mees2022calvin}, a benchmark for long-horizon, language-conditioned manipulation to evaluate the GEVRM's capabilities in closed-loop action execution.
CALVIN consists of four simulated environments (A, B, C, and D), each with a dataset of human-collected play trajectories. 
Each environment consists of a \textit{Franka Emika Panda} robot arm positioned next to a table with various manipulable objects, including drawers, sliding cabinets, light switches, and colored blocks. 
Environments are distinguished by their tabletop texture, the positions of furniture objects, and the configuration of colored blocks.
We study zero-shot multi-environment training on A, B, and C, and testing on D, varying in table texture, furniture positioning, and color patches.

\textbf{Environment perturbations Details.} 
We list the details of perturbations in Fig~\ref{fig:calvinTask}:
\textbf{1)} The image state is randomly translated to the upper left, with a maximum translation ratio of 0.1 relative to the image size.
\textbf{2)} The image state is randomly rotated counterclockwise, with a maximum rotation angle of 30 degrees.
\textbf{3)} The image state saturation, brightness, contrast, and sharpness are randomly jittered with a maximum random factor of 3.
\textbf{4)} The image state is randomly occluded with a random number of occlusion blocks ranging from 1 to 3 and a maximum length of 60. 
\textbf{5)} The image state is perturbed with random noise blocks.
We follow the evaluation protocol of Mees et al. \citep{mees2022calvin}. 
During the evaluation, the policy is required to complete five chains of language instructions for $360$ time steps. 
Notably, we only consider RGB images from the static camera as observations, which makes CALVIN much more challenging.


\subsection{Baseline method introductions.} \label{Appe:Baseline method introductions.}
For a fair comparison, we here select the video generation-based baselines to verify the zero-shot generalization performance on standard unseen environments: 
These baseline methods include language-conditioned policies that leverage pre-trained visual-language models in various ways: \textbf{1) HULC} \citep{mees2022matters}, a model that employs a multi-modal transformer encoder for language-conditioned robotic manipulation, combines self-supervised contrastive learning to align video and language representations and uses hierarchical robotic control learning to tackle complex tasks. 
\textbf{2) MCIL} \citep{lynch2020language}, a multi-context imitation learning framework, is capable of handling large-scale unlabelled robot demonstration data. 
MCIL trains a single goal-conditioned policy by mapping various contexts, such as target images, task IDs, and natural language, into a shared latent goal space.
\textbf{3) MdetrLC} \citep{kamath2021mdetr}, which integrates visual and textual information to perform object detection and multi-modal understanding. 
MdetrLC uses text query modulation to detect objects within images and demonstrates strong performance in tasks like visual question answering and phrase localization.
\textbf{4) AugLC} \citep{pashevich2019learning}, which optimizes image augmentation strategies to enable Sim2Real policy transfer from simulated environments to real-world scenarios, applies random transformation sequences to enhance synthetic depth images and uses auxiliary task learning to reduce the domain gap between synthetic and real images.
\textbf{5) LCBC} \citep{walke2023bridgedata}, which employs ResNet-34 as the image encoder combined with MUSE language embeddings for robotic decision-making, uses FiLM conditioning to embed language information into the visual encoding, which in turn generates robot actions.
\textbf{6) UniPi} \citep{du2024learning}, which transforms decision-making problems into text-conditioned video generation tasks, produces future video sequences of the target task and extracts control actions from the generated videos.
\textbf{7) HiP} \citep{ajay2024compositional}, an extension of the UniPi method, enhances the model's ability to handle long-horizon tasks by introducing hierarchical inference and planning. 
This approach decomposes tasks into high-level planning and low-level action generation, improving task execution in complex scenarios.
\textbf{8) SuSIE} \citep{black2023zero}, which leverages a pretrained image-editing diffusion model to generate sub-goal images, guides the robot through complex manipulation tasks via language instructions. 
SuSIE integrates a large-scale internet visual corpus during sub-goal generation and achieves these generated sub-goals through a low-level goal-oriented policy.

\subsection{Real-World Tasks.} \label{App:Real-World Tasks.}
\textbf{Protocol.} To examine the effectiveness of the proposed GEVRM on real-world robotic manipulation tasks, we propose a real-machine deployment protocol. We evaluate GEVRM on a robotic arm UR5 for the pick-and-place tasks of a cup, a bowl, and a tiger plush toy. Specifically, we use a camera to capture third-person images as the observation space (image width 640, height 480), and relative poses and binarized gripper states as the action space (7 dimensions). The total number of collected real-world teleoperation expert trajectories is over 400, with trajectory lengths ranging from 20 to 120 steps and a control frequency of 5Hz.

\textbf{Experiments.} We train and evaluate GEVRM under real-world protocols. The VAE and DiT in the behavior planner are trained for 30,000 and 12,000 iterations, respectively, while the goal-guided policy is trained for 100,000 iterations. Other hyperparameters remain the same as in the experiments in CALVIN (and Bridge). Fig.~\ref{APP:fig:ur5} shows the policy execution process of our proposed GEVRM on three types of real-world tasks, indicating that our method can be effectively deployed on real machines. In terms of task success rate (SR), we evaluated each type of task 10 times. The experimental results show that compared with the grasping and placing of cups (or bowls) with regular shapes (success rate of about 0.8), the grasping of tiger plush toys with soft materials and irregular shapes is more challenging (success rate of about 0.6). Further improving GEVRM's perception of real-world scenes and task execution accuracy is an important future work.
\begin{figure}[tbp]
\centering
\includegraphics[width=0.95\textwidth]{ICLR_2025_Template/figures/ur5_realTask.png}
\caption{
Real-world task results. Our method GEVRM can be effectively deployed in real-world scenarios, such as the picking and placing tasks of cups, bowls and tiger plush toys.
} 
  \label{APP:fig:ur5}
\end{figure}


\begin{table}[ht]\small
    \centering
    \vspace{-1.5em}
    \caption{Generalization on perturbed environments in CALVIN (train A, B, C → perturbed test D).
    % Our proposed method surpasses the previous state-of-the-art method SuSIE in terms of task success rate and task completion length on average.
     % The best results for each task are bolded.
     % The 'AATL' is short for 'Average Achieved Task Length'.
    }
    \vspace{0.5em}
    \renewcommand{\arraystretch}{0.8}
    \begin{tabular}{l c ccccc c}
        \toprule
        \multirow{2}{*}{\textbf{Perturbed Tasks}} & \multirow{2}{*}{\textbf{Algorithms}} & \multicolumn{5}{c}{\textbf{No. of Instructions Chained}} & \multirow{2}{*}{\textbf{Avg. Length ($\uparrow$)}} \\
        \cmidrule(lr){3-7}
        & & 1 & 2 & 3 & 4 & 5 & \\
        \midrule
        \multirow{4}{*}{Image Shift} & SuSIE & 0.56 & 0.28 & 0.08 & 0.04 & 0.00 & 0.96 \\
         & RoboFlamingo & 0.48 & 0.32 & 0.12 & 0.00 & 0.00 & 0.92  \\
         & GR-1 & 0.43 & 0.33 & 0.20 & 0.10 & 0.00 & \textbf{1.00}  \\
        & GEVRM (Ours)  & 0.52 & 0.40 & 0.08 & 0.00 & 0.00 & \textbf{1.00} \\
        \midrule
        \multirow{4}{*}{Image Rotation} & SuSIE & 0.48 & 0.16 & 0.08 & 0.00 & 0.00 & 0.72 \\
         & RoboFlamingo & 0.42 & 0.24 & 0.11 & 0.02 & 0.02 & 0.82  \\
         & GR-1 & 0.46 & 0.32 & 0.14 & 0.10 & 0.03 & 1.07  \\
        & GEVRM (Ours)   & 0.60 & 0.32 & 0.12 & 0.08 & 0.04 & \textbf{1.16} \\
        \midrule
        \multirow{4}{*}{Color Jitter} & SuSIE& 0.72 & 0.36 & 0.16 & 0.12 & 0.08 & 1.44 \\
         & RoboFlamingo & 0.52&0.22&0.08&0.08&0.04&0.94  \\
         & GR-1 & 0.6&0.35&0.21&0.12&0.07&1.35  \\
        & GEVRM (Ours)   & 0.64 & 0.48 & 0.32 & 0.12 & 0.08 & \textbf{1.64} \\
        \midrule
        \multirow{4}{*}{Image Occlusions} & SuSIE & 0.72 & 0.48 & 0.32 & 0.32 & 0.24 & 2.08 \\
         & RoboFlamingo & 0.43 & 0.30 & 0.13 & 0.06 & 0.03 & 0.96  \\
         & GR-1 & 0.78 & 0.60 & 0.46 & 0.32 & 0.23 & 2.39  \\
        & GEVRM (Ours)   & 0.92 & 0.68 & 0.48 & 0.24 & 0.20 & \textbf{2.52} \\
        \midrule
        \multirow{4}{*}{Noise Interference} & SuSIE& 0.32 & 0.04 & 0.00 & 0.00 & 0.00 & 0.36 \\
         & RoboFlamingo & 0.49 & 0.23 & 0.03 & 0.01 & 0.01 & 0.80  \\
         & GR-1 & 0.67&0.42&0.26&0.14&0.08&1.57  \\
        & GEVRM (Ours)   & 0.80 & 0.48 & 0.32 & 0.12 & 0.04 & \textbf{1.76} \\
        \midrule
        \multirow{4}{*}{Average} & SuSIE & 0.56 & 0.26 & 0.13 & 0.10 & 0.06 & 1.11 \\
         & RoboFlamingo & 0.63&0.35&0.18&0.09&0.05&1.31  \\
         & GR-1 & 0.67&0.38&0.22&0.11&0.06&1.44  \\
        & GEVRM (Ours)  & 0.70 & 0.47 & 0.26 & 0.11 & 0.07 & \textbf{1.62} \\
        \bottomrule
    \end{tabular}
    \label{tab:calvin_harder_tasks_all}
    \vspace{-1em}
\end{table}


\begin{table}
    \centering
    \caption{Zero-shot Generalization.
    The experiment is set up to train on data from environments A, B, and C (Fig.~\ref{fig:calvinTask} (\textbf{a})), and test in D (Fig.~\ref{fig:calvinTask} (\textbf{b})). *: reproduced version on static camera. Static camera: camera of fixed third-person view.
    Our proposed method can chain more instructions together with a higher success rate than all previous baseline methods.
    Baseline results are from previous work \citep{black2023zero}. 
    The best results for each task are bolded.
    }
    \begin{tabular}{llccccc}
        \toprule
        \multirow{2}{*}{\textbf{Algorithms}} &\multirow{2}{*}{\textbf{Source}} & \multicolumn{5}{c}{\textbf{No. of Instructions Chained}} \\
        \cmidrule(lr){3-6}
       & & 1 & 2 & 3 & 4 & 5 \\
        \midrule
        HULC~\citep{mees2022matters}& static camera & 0.43 & 0.14 & 0.04 & 0.01 & 0.00 \\
        MCIL~\citep{lynch2020language} &static camera& 0.20 & 0.00 & 0.00 & 0.00 & 0.00 \\
        MdetrLC~\citep{kamath2021mdetr}& static camera& 0.69 & 0.38 & 0.20 & 0.07 & 0.04 \\
        AugLC~\citep{pashevich2019learning} &static camera& 0.69 & 0.43 & 0.22 & 0.09 & 0.05 \\
        LCBC~\citep{walke2023bridgedata} &static camera& 0.67 & 0.31 & 0.17 & 0.10 & 0.06 \\
        HiP~\citep{ajay2024compositional}& static camera& 0.08 & 0.04 & 0.00 & 0.00 & 0.00 \\
        UniPi~\citep{du2024learning}&static camera & 0.56 & 0.16 & 0.08 & 0.08 & 0.04 \\
        SuSIE~\citep{black2023zero}&static camera & 0.87 & 0.69 & 0.49 & 0.38 & {0.26} \\
        GR-1*~\citep{black2023zero}& static camera & 0.75 & 0.45 & 0.2 & 0.15 & {0.1} \\
        \midrule
        GEVRM (Ours)& static camera & \textbf{0.92} & \textbf{0.70} & \textbf{0.54} & \textbf{0.41} & \textbf{0.26} \\
        \bottomrule
    \end{tabular}
    \label{APP:tab:calvin_performance}
\end{table}

\begin{figure}
\centering
\includegraphics[width=0.95\textwidth]{ICLR_2025_Template/figures/embedding_tsne_goal.pdf}
% \includegraphics[width=0.5\textwidth,height=0.28\textwidth]{LaTeX/pictures/data_dis_visua.png}
\caption{
Visual comparison of the latent space representation of the goal image state with and without state alignment (SA). 
The gaol state representation sequence with SA also has better cluster centers, category boundaries, and temporal consistency.
} 
  \label{Appe:embedding_tsne_goal}
\end{figure}

% \subsection{More goal generation results.}  \label{Appe:More goal generation results.}
% Here, we list more goal-generation comparisons on simulation environments.


\begin{figure}[tbp]
\centering
\includegraphics[width=0.95\textwidth]{ICLR_2025_Template/figures/calvin_video_1_com.png}
\caption{
Comparison of goal generation. We visually compare Oracle, GR-1, and our proposed algorithm on the "\textit{push the switch upwards}" task in CALVIN Env. D. 
Compared with the baseline GR-1, the goal video we generate is more realistic and better restores details.
} 
  \label{APP:fig:calvin_video_1}
\end{figure}

\begin{figure}[tbp]
\centering
\includegraphics[width=0.95\textwidth]{ICLR_2025_Template/figures/calvin_video_2_com.png}
\caption{
Comparison of goal generation. We visually compare Oracle, GR-1, and our proposed algorithm on the "\textit{go towards the blue block in the drawer and lift it}" task in CALVIN Env. D. 
Compared with the baseline GR-1, the goal video we generate is more realistic and better restores details.
} 
  \label{APP:fig:calvin_video_2}
\end{figure}

\begin{figure}[tbp]
\centering
\includegraphics[width=0.95\textwidth]{ICLR_2025_Template/figures/calvin_video_3_com.png}
\caption{
Comparison of goal generation. We visually compare Oracle, GR-1, and our proposed algorithm on the "\textit{grasp the red block lying in the slider}" task in CALVIN Env. D. 
Compared with the baseline GR-1, the goal video we generate is more realistic and better restores details.
} 
  \label{APP:fig:calvin_video_3}
\end{figure}
\begin{figure}[tbp]
\centering
\includegraphics[width=0.95\textwidth]{ICLR_2025_Template/figures/calvin_video_4_com.png}
\caption{
Comparison of goal generation. We visually compare Oracle, GR-1, and our proposed algorithm on the "\textit{go push the red block to the right}" task in CALVIN Env. D. 
Compared with the baseline GR-1, the goal video we generate is more realistic and better restores details.
} 
  \label{APP:fig:calvin_video_4}
\end{figure}


\begin{figure}[tbp]
\centering
\includegraphics[width=0.95\textwidth]{ICLR_2025_Template/figures/APP_real_1.pdf}
\caption{
Comparison of goal generation. We visually compare Oracle, AVDC, and our proposed algorithm on the "\textit{put pan on stove and put stuf edduck in pan}" task in Bridge Data. 
Compared with the baseline AVDC, the goal video we generate is more realistic and better restores details.
} 
  \label{APP:fig:real_video_1}
\end{figure}

\begin{figure}[tbp]
\centering
\includegraphics[width=0.95\textwidth]{ICLR_2025_Template/figures/APP_real_2.pdf}
\caption{
Comparison of goal generation. We visually compare Oracle, AVDC, and our proposed algorithm on the "\textit{put yellowpepper on plate and cookiebox in pot or pan on stove}" task in Bridge Data. 
Compared with the baseline AVDC, the goal video we generate is more realistic and better restores details.
} 
  \label{APP:fig:real_video_2}
\end{figure}

\begin{figure}[tbp]
\centering
\includegraphics[width=0.95\textwidth]{ICLR_2025_Template/figures/APP_real_3.pdf}
\caption{
Comparison of goal generation. We visually compare Oracle, AVDC, and our proposed algorithm on the "\textit{put pot or pan on stove and put egg in pot or pan}" task in Bridge Data. 
Compared with the baseline AVDC, the goal video we generate is more realistic and better restores details.
} 
  \label{APP:fig:real_video_3}
\end{figure}

\begin{table}[h!]
\centering
\begin{tabular}{cccccccc}
\toprule
\textbf{Sampling steps} & \textbf{Infer. time [s]} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} & \textbf{Avg. Length} \\ 
\midrule
50 & 0.598 & 0.80 & 0.48 & 0.32 & 0.12 & 0.04 & 1.76 \\ 
40 & 0.501 & 0.73 & 0.53 & 0.20 & 0.13 & 0.06 & 1.67 \\
30 & 0.379 & 0.73 & 0.40 & 0.23 & 0.20 & 0.06 & 1.63 \\ 
20 & 0.260 & 0.71 & 0.46 & 0.22 & 0.11 & 0.08 & 1.60 \\ 
10 & 0.135 & 0.77 & 0.47 & 0.17 & 0.15 & 0.10 & 1.67 \\ 
\bottomrule
\end{tabular}
\caption{
The comparative analysis of the computational efficiency and task success rate of the behavior planner (Noise Interference task). 
Due to the good properties of the adopted Rectified Flow, when the video sampling steps are reduced, the model inference time is greatly reduced, while the success rate is not significantly reduced.
}
\label{tab:goal_efficiency}
\end{table}



\begin{table}[h!]
\centering
\begin{tabular}{lcccccccc}
\toprule
 \textbf{Algo.} & \textbf{Control steps} & \textbf{Infer. time [s]} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} & \textbf{Avg. Length} \\
\midrule
\multirow{4}{*}{\textbf{DP}} & 1 & 0.077 & 0.80 & 0.48 & 0.32 & 0.12 & 0.04 & 1.76 \\
 & 2 & 0.044 & 0.85 & 0.50 & 0.20 & 0.15 & 0.05 & 1.75 \\
 & 3 & 0.027 & 0.82 & 0.50 & 0.22 & 0.10 & 0.07 & 1.72 \\
 & 4 & 0.020 & 0.68 & 0.48 & 0.24 & 0.16 & 0.08 & 1.64 \\
\midrule
\textbf{MLP} & - & 0.019 & 0.73 & 0.40 & 0.13 & 0.06 & 0.06 & 1.40 \\
\bottomrule
\end{tabular}
\caption{
Comparison of goal-guided diffusion policies (DP) with different open-loop control steps (Noise Interference task).
% We also conducted experimental comparisons on goal-guided diffusion policies with different open-loop control steps, as shown in the table below (Noise Interference task). 
The results show that the state-aligned policy has better action robustness, and increasing the number of open-loop control steps can significantly reduce the inference time while having little effect on the task success rate. 
Therefore, the control frequency of our goal-guided diffusion policy can be maintained at the order of tens of Hz, which is sufficient for most robot manipulation tasks in reality. 
Moreover, when the number of open-loop control steps is 4, the diffusion policy has higher performance, while the inference speed is very close to that of MLP.}
\label{tab:policy_efficiency}
\end{table}

% \subsection{Model hyperparameters details.}  \label{Appe:Model hyperparameters details.}
% In this section, we list more details of model hyperparameters.
\begin{table}[h]
\centering
\caption{Behavior planner training hyperparameters.}
\begin{tabular}{l l l} % Two columns, left-aligned
\toprule
\textbf{Component}   & \textbf{Parameter}     & \textbf{Value} \\
\midrule
% \multirow{4}{*}{Dataset settings} & batch\_size  &  6  \\
% & num\_frame\_total  &  51 \\
% & micro\_frame\_size  &  17 \\
% & grad\_checkpoint  &  True  \\
% \midrule % 添加水平线以分隔
\multirow{3}{*}{Dataset settings} 
& num\_frame\_total  &  51 \\
& transform\_name  &  resize\_crop     \\
& image\_size  & (256, 256)    \\ 
% & num\_frames  & 64    \\

\midrule % 添加水平线以分隔
\multirow{4}{*}{Acceleration settings}  &  num\_workers  &  8   \\
& num\_bucket\_build\_workers  &  16  \\
&  dtype  &  bf16  \\
&  plugin  &  zero2  \\

\midrule % 添加水平线以分隔
\multirow{5}{*}{DIT Model settings} & type & STDiT3-XL/2   \\
% &  from\_pretrained & opensora\_from\_pretrained  \\
&  qk\_norm & True  \\
&  enable\_flash\_attn & True  \\
&  enable\_layernorm\_kernel & True  \\
&  freeze\_y\_embedder & True  \\

\midrule % 添加水平线以分隔
\multirow{2}{*}{VAE settings} 
% & type & OpenSoraVAE\_V1\_2   \\
% &from\_pretrained & vae\_from\_pretrained   \\
&micro\_frame\_size & 17   \\
&micro\_batch\_size & 4   \\

\midrule % 添加水平线以分隔
\multirow{3}{*}{Text encoder settings} & type & T5   \\
% &from\_pretrained & text\_from\_pretrained   \\
&model\_max\_length & 300   \\
&shardformer & True   \\

\midrule % 添加水平线以分隔
\multirow{3}{*}{Scheduler settings} & type & rflow   \\
  &  use\_timestep\_transform & True   \\
  &  sample\_method & logit-normal   \\

\midrule % 添加水平线以分隔
\multirow{10}{*}{Random Mask settings} &  random  &  0.025   \\
 &  intepolate  &  0.025   \\
 &    quarter\_random  &  0.025   \\
 &    quarter\_head  &  0.75   \\
 &   quarter\_tail  &  0.025   \\
 &   quarter\_head\_tail  &  0.05   \\
 &   image\_random  &  0.0   \\
 &   image\_head  &  0.025   \\
 &    image\_tail  &  0.025   \\
 &    image\_head\_tail  &  0.05   \\


\midrule % 添加水平线以分隔
\multirow{6}{*}{Optimization settings} 
& batch\_size & 6  \\
 & grad\_clip  & 1.0  \\
 & learning\_rate  & 1e-4  \\
 & ema\_decay  & 0.99  \\
 & adam\_eps  & 1e-15  \\
 & warmup\_steps  & 1000  \\
\bottomrule
\end{tabular}
\label{Appd:Behavior planner training optimizer hyperparameters}
\end{table}


\begin{table}[h]
\centering
\caption{GEVRM test hyperparameters.}
\begin{tabular}{l l l} % Two columns, left-aligned
\toprule
\textbf{Component}    & \textbf{Parameter}    & \textbf{Value} \\

\midrule

 \multirow{11}{*}{General settings} & fixed\_interval\_number  & 20  \\

  & condition\_frame\_length  & 5   \\
  &  goal\_generation\_number  & 51  \\
  &   micro\_frame\_size  & 17   \\
  % & history\_len  & 51   \\

  & num\_sampling\_steps  & 50  \\

  % & align  & None        \\

  & resolution  &  256  \\
  &  aspect\_ratio  &  1:1  \\
  & image\_size  & (256, 256)   \\
  & fps  & 30 for CALVIN; 5 for Bridge    \\
  & frame\_interval  & 1    \\
  % & save\_fps  & 30   \\

  % & seed  & 42    \\
  % & batch\_size  & 1  \\
  % & multi\_resolution  & STDiT2  \\
  & dtype  & bf16   \\

  % & aes  & None   \\
  % & flow  & None   \\

\bottomrule
\end{tabular}
\label{Appd:Behavior planner test hyperparameters}
\end{table}

\begin{table}[h]
\centering
\caption{Goal-guided policy training hyperparameters.}
\begin{tabular}{l l l}
\toprule
\textbf{Component}   & \textbf{Parameter}     & \textbf{Value} \\  
\midrule
\multirow{7}{*}{General settings} 
% & early\_goal\_concat & False \\ 
 % & shared\_goal\_encoder       & False \\
 % & use\_proprio               & False \\
 & beta\_schedule             & Cosine \\
 & diffusion\_steps           & 20 \\
 & action\_samples            & 1 \\
 & repeat\_last\_step         & 0 \\
 & learning\_rate             & 3e-4 \\
 & warmup\_steps              & 2000 \\
 & actor\_decay\_steps        & 2e6 \\

\midrule
\multirow{5}{*}{Score network settings} & time\_dim                  & 32 \\
 & num\_blocks                & 3 \\
 & dropout\_rate              & 0.1 \\
 & hidden\_dim                & 256 \\
 & use\_layer\_norm           & True \\

\midrule
 \multirow{3}{*}{SA Encoder settings} & hidden\_dim & 512  \\
 & num\_prototype & 3000 \\
 & temperature & 0.1 \\
 % & him\_alpha & 1. \\  % FLags.him_alpha


\bottomrule
\end{tabular}
\label{Appd:Goal-guided policy optimizer Hyperparameters}
\end{table}
% You may include other additional sections here.

% \begin{figure}[htbp]
% 	\centering
% 	\begin{subfigure}
% 		\centering
% 		\includegraphics[width=0.9\linewidth]{ICLR_2025_Template/figures/calvin_video_2.png}
% 		% \caption{chutian3}
% 		% \label{chutian3}%文中引用该图片代号
% 	\end{subfigure}
% 	\centering
% 	\begin{subfigure}
% 		\centering
% 		\includegraphics[width=0.9\linewidth]{ICLR_2025_Template/figures/calvin_video_3.png}
% 		% \caption{chutian3}
% 		% \label{chutian3}%文中引用该图片代号
% 	\end{subfigure}
% 	\centering
% 	\begin{subfigure}
% 		\centering
% 		\includegraphics[width=0.9\linewidth]{ICLR_2025_Template/figures/calvin_video_4.png}
% 		% \caption{chutian3}
% 		% \label{chutian3}%文中引用该图片代号
% 	\end{subfigure}
% 	\caption{More comparisons of robot behavior planning.}
% 	\label{App:calvin_video}
% \end{figure}



\end{document}
