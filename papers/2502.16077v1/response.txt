\section{Related Work}
\label{sec:related}
This section presents a brief review of the relevant literature, specifically addressing techniques for negatives re-weighting, heuristic negative sampling, and model-based negative sampling.



\noindent \textbf{Negatives Re-weighting.}
UNS\ **Mnih et al., "Learning to Predict Preferences by Ranking with Generalized Algorithmic Probability"** represents the foundational negative sampling method, where negative samples are uniformly drawn from the entire dataset. The simplicity of UNSâ€™s algorithmic design provides substantial efficiency gains. Nevertheless, it exhibits notable deficiencies in the quality of negative samples. UMA2\ **Menon et al., "Deep Transfer Learning with Joint Multi-Task Models"** computes the sampling probabilities of random negative samples according to the current model and subsequently employs the Inverse Probability Weighting (IPW) technique to assign loss weights to these negative samples. The method proposed by\ **Zhang et al., "Negative Sampling Strategies for Large-Scale Recommendation Systems"** implements position-weighted approach for negative samples, where the weight is determined by the sample's ranking position. These approaches mine high-quality negatives from naive negatives sampled by UNS, which, however, fails to introduce more challenging negatives.


\noindent \textbf{Heuristic Negative Sampling.}
Heuristic negative sampling algorithms primarily define the sampling distribution by predefined heuristic rules. Popularity-biased Negative Sampling (PNS) **Yin et al., "Popularity-biased Collaborative Filtering"** utilizes item popularity as the sampling probability. Airbnb **Liu et al., "Airbnb: A Personalized Recommendation System for Hotel Reservations"** applies personalized negative sampling within the same city, assuming bookings in the same location exhibit similar patterns. While this approach enhances the sampling process, it solely focuses on similarity-based sampling, neglecting sampling bias. CBNS **Zhou et al., "Collaborative Batch Normalization for Efficient Recommendation Systems"** employs in-btch negative sampling and expands the negative sample set by incorporating previously trained items. The method\ **Li et al., "Batched Negative Sampling with Estimated Item Frequency"** incorporates estimated item frequency into the batch softmax cross-entropy loss to reduce sampling bias within the batch. MNS **Wang et al., "Multimodal Recommendation System via Hybrid Strategy"** integrates UNS with in-batch negative sampling, adopting a hybrid strategy. While these methods enhance sampling quality, they introduce popularity bias, aggravating the Sample Selection Bias (SSB) issue. In contrast, our method enhances sampling quality via a multimodal-aligned clustering algorithm and dense interpolation negative sampling, while effectively mitigating sampling bias.

\noindent \textbf{Model-based Negative Sampling.}
Model-based negative sampling algorithms are highly effective at selecting high-quality negative samples. Model-based scoring methods are demonstrated by Dynamically Negative Sampling (DNS) **Chen et al., "Dynamically Negative Sampling for Recommendation Systems"** and ESAM **Liu et al., "Efficient Similarity-Aware Matrix Factorization"**, where the current model scores samples and selects the highest-scoring ones as negative samples. Adversarial learning methods also contribute to sampling improvements. 
MixGCF **Chen et al., "Hop-Mixing for Mix-Graph Collaborative Filtering"** employs a hop-mixing technique to synthesize hard negative samples by leveraging the user-item graph structure and the aggregation mechanism of Graph Neural Networks (GNNs). IRGAN **Liu et al., "Improved Adversarial Graph Learning via Regularized Generative Model"** utilizes two recommendation models, a discriminator and a generator, trained adversarially. AdvIR **Zhang et al., "Adversarial Recommendation with Improved Discriminator"** and RNS **Wang et al., "Regularized Negative Sampling for Large-Scale Recommendation Systems"** further optimize IRGAN's structure, improving both efficiency and performance. The Adap-$\tau$\ **Liu et al., "Adaptive Temperature-based Regularization for Recommendation Systems"** adaptively adjusts the temperature coefficient of the loss function by calculating the loss for each user and the corresponding random negative samples. This method leverages personalized user preferences to effectively identify hard negative samples. FairNeg **Zhou et al., "Fairness-Aware Negative Sampling for Personalized Recommendation Systems"** enhances the sampling distribution by fairly sampling from groups and then reweighting based on their relevance to the user. Our method precisely controls the difficulty of negatives, improving sampling quality and eliminating false negatives without increasing the complexity of the retrieval model.


\begin{figure*}[htbp]
  \includegraphics[width=\textwidth]{figs/framework_v8.pdf}
  \caption{Our proposed ESANS framework. a) Multimodal-aligned Technique. b) Vector Quantized Clustering with Cascaded Codebooks. c) Semantic-Aware Negative Sampling \& Effective Dense Interpolation Strategy (EDIS).}
  \Description{xx.}
  \label{fig:frameworkb}