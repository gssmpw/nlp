\section{Related Work}
\label{sec:related}
This section presents a brief review of the relevant literature, specifically addressing techniques for negatives re-weighting, heuristic negative sampling, and model-based negative sampling.



\noindent \textbf{Negatives Re-weighting.}
UNS\ ____ represents the foundational negative sampling method, where negative samples are uniformly drawn from the entire dataset. The simplicity of UNSâ€™s algorithmic design provides substantial efficiency gains. Nevertheless, it exhibits notable deficiencies in the quality of negative samples. UMA2\ ____ computes the sampling probabilities of random negative samples according to the current model and subsequently employs the Inverse Probability Weighting (IPW) technique to assign loss weights to these negative samples. The method proposed by\ ____ implements position-weighted approach for negative samples, where the weight is determined by the sample's ranking position. These approaches mine high-quality negatives from naive negatives sampled by UNS, which, however, fails to introduce more challenging negatives.


\noindent \textbf{Heuristic Negative Sampling.}
Heuristic negative sampling algorithms primarily define the sampling distribution by predefined heuristic rules. Popularity-biased Negative Sampling (PNS) ____ utilizes item popularity as the sampling probability. Airbnb ____ applies personalized negative sampling within the same city, assuming bookings in the same location exhibit similar patterns. While this approach enhances the sampling process, it solely focuses on similarity-based sampling, neglecting sampling bias. CBNS ____ employs in-btch negative sampling and expands the negative sample set by incorporating previously trained items. The method\ ____ incorporates estimated item frequency into the batch softmax cross-entropy loss to reduce sampling bias within the batch. MNS ____ integrates UNS with in-batch negative sampling, adopting a hybrid strategy. While these methods enhance sampling quality, they introduce popularity bias, aggravating the Sample Selection Bias (SSB) issue. In contrast, our method enhances sampling quality via a multimodal-aligned clustering algorithm and dense interpolation negative sampling, while effectively mitigating sampling bias.

\noindent \textbf{Model-based Negative Sampling.}
Model-based negative sampling algorithms are highly effective at selecting high-quality negative samples. Model-based scoring methods are demonstrated by Dynamically Negative Sampling (DNS) ____ and ESAM ____, where the current model scores samples and selects the highest-scoring ones as negative samples. Adversarial learning methods also contribute to sampling improvements. 
MixGCF ____ employs a hop-mixing technique to synthesize hard negative samples by leveraging the user-item graph structure and the aggregation mechanism of Graph Neural Networks (GNNs). IRGAN ____ utilizes two recommendation models, a discriminator and a generator, trained adversarially. AdvIR ____ and RNS ____ further optimize IRGAN's structure, improving both efficiency and performance. The Adap-$\tau$\ ____ adaptively adjusts the temperature coefficient of the loss function by calculating the loss for each user and the corresponding random negative samples. This method leverages personalized user preferences to effectively identify hard negative samples. FairNeg ____ enhances the sampling distribution by fairly sampling from groups and then reweighting based on their relevance to the user. Our method precisely controls the difficulty of negatives, improving sampling quality and eliminating false negatives without increasing the complexity of the retrieval model.


\begin{figure*}[htbp]
  \includegraphics[width=\textwidth]{figs/framework_v8.pdf}
  \caption{Our proposed ESANS framework. a) Multimodal-aligned Technique. b) Vector Quantized Clustering with Cascaded Codebooks. c) Semantic-Aware Negative Sampling \& Effective Dense Interpolation Strategy (EDIS).}
  \Description{xx.}
  \label{fig:frameworkb}
\end{figure*}