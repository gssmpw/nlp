\documentclass{article}

\usepackage{PRIMEarxiv}
\usepackage{natbib}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs} % for professional tables

\usepackage{hyperref}
\usepackage{algorithm}
\usepackage[noend]{algorithmic}


% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage[capitalize]{cleveref}



% CUSTOM COMMANDS
\usepackage{dsfont}
\usepackage{comment}
\usepackage{bm}
\usepackage{subcaption}
\usepackage{adjustbox}
\usepackage{pgfmath}
\usepackage{thm-restate, thmtools}
\usepackage{mathtools}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\newcommand\pttocm[1]{%
  \pgfmathsetmacro\len{#1pt/1cm}\len{}\,cm.
}

\renewcommand{\algorithmicensure}{\textbf{return}}
\renewcommand{\algorithmiccomment}[1]{\hfill $\triangleright$ #1}

% Add algorithm line number referencing.
\newcommand{\alglinelabel}{%
  \addtocounter{ALC@line}{-1}% Reduce line counter by 1
  \refstepcounter{ALC@line}% Increment line counter with reference capability
  \label% Regular \label
}
\crefname{ALC@line}{line}{lines}

%%%%%%%%%%%%%%%%%%%%
% Math operators
%%%%%%%%%%%%%%%%%%%%
\DeclareMathOperator*{\argmax}{arg\,max} % thin space, limits underneath in displays

\declaretheorem[numberwithin=section]{theorem}
\declaretheorem[sibling=theorem]{definition,corollary,proposition,lemma,conjecture,assumption,remark}

\crefname{assumption}{Assumption}{Assumptions}

% Latin letters: A, B, C, D, E, F, G, H, I, J, K, L, M N O P Q R S T U V W
% A
% B
% C 
\newcommand{\C}{{\mathcal{C}}}
% D
% E, 
\newcommand{\E}{{\mathbb{E}}}
% F,
% G
\newcommand{\GP}{{\mathcal{GP}}}
% H, 
%I,
\newcommand{\I}{{\mathbf{I}}}
%J, 
%K, 
\renewcommand{\k}{{\mathbf{k}}}
\newcommand{\K}{{\mathbf{K}}}
% L, 
% M 
% N
\newcommand{\N}{{\mathcal{N}}}
% O
\renewcommand{\O}{{\mathcal{O}}}
% P
\newcommand{\Pb}{{\mathbb{P}}}
\renewcommand{\P}{{\mathcal{P}}}
% Q 
% R 
\newcommand{\R}{{\mathbb{R}}}
% S 
% T 
% U 
\newcommand{\U}{{\mathbf{U}}}
\renewcommand{\u}{{\mathbf{u}}}
% V 
\newcommand{\V}{{\mathcal{V}}}
% W
\newcommand{\w}{{\bm{w}}}
\newcommand{\W}{{\mathbf{W}}}
\newcommand{\bcW}{{\bm{\mathcal{W}}}}
% X
\newcommand{\x}{{\mathbf{x}}}
\newcommand{\X}{{\mathbf{X}}}
\newcommand{\Xc}{{\mathcal{X}}}
% Y
\newcommand{\y}{{\mathbf{y}}}

\newcommand{\bmu}{{\bm{\mu}}}

%%%%%%%%%%%%%%%%%%%%%%%%
% Other commands
%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\BR}{{\text{BR}}}
\newcommand{\brt}{{\text{BR}_t}}
\newcommand{\sumt}{{\sum_{t \in [T]}}}

\let\emptyset\varnothing
\newcommand{\eqd}{\overset{d}{=}}


\title{Efficient Prior Selection in Gaussian Process Bandits with Thompson Sampling}


\author{
  Jack Sandberg \\
  Chalmers University of Technology and\\ University of Gothenburg\\
  Gothenburg, Sweden\\
  \texttt{jack.sandberg@chalmers.se} \\
   \And
  Morteza Haghir Chehreghani \\
  Chalmers University of Technology and\\University of Gothenburg\\
  Gothenburg, Sweden\\
  \texttt{morteza.chehreghani@chalmers.se} \\
}

%Header
\pagestyle{fancy}
\thispagestyle{empty}
\rhead{ \textit{ }} 

\begin{document}
\twocolumn[
\maketitle
]



\begin{abstract}
    Gaussian process (GP) bandits provide a powerful framework for solving blackbox optimization of unknown functions.
    The characteristics of the unknown function depends heavily on the assumed GP prior. 
    Most work in the literature assume that this prior is known but in practice this seldom holds. 
    Instead, practitioners often rely on maximum likelihood estimation to select the hyperparameters of the prior - which lacks theoretical guarantees. 
    In this work, we propose two algorithms for joint prior selection and regret minimization in GP bandits based on GP Thompson sampling (GP-TS): Prior-Elimination GP-TS (PE-GP-TS) and HyperPrior GP-TS (HP-GP-TS). 
    We theoretically analyze the algorithms and establish upper bounds for their respective regret. 
    In addition, we demonstrate the effectiveness of our algorithms compared to the alternatives through experiments with synthetic and real-world data.
\end{abstract}

\section{Introduction}
In Gaussian process bandits, we consider a variant of the multi-armed bandit problem where the arms are correlated and their expected reward is sampled from a Gaussian process (GP). The flexibility of GPs have made GP bandits applicable in a wide range of areas that need to optimize blackbox functions with noisy estimates, including machine learning hyperparameter tuning \cite{turnerBayesian2021a}, drug discovery \cite{hernandez-lobatoParallel2017,pyzer-knappBayesian2018}, chemical design \cite{griffithsConstrained2020}, battery charging protocols \cite{jiangFast2022}, online advertising \cite{nuaraCombinatorialBanditAlgorithmOnline2018}, and portfolio optimization \cite{gonzalvezFinancial2019}. However, most of the theoretical results in the literature assume that the GP prior is known but this is seldom the case in practical applications. Even with expert domain knowledge, selecting the exact prior to use can be a difficult task. Most practitioners tend to utilize maximum likelihood estimation (MLE) to identify suitable prior parameters. However, in a sequential decision making problem MLE is not guaranteed to recover the correct parameters.

In the literature, \citet{wangTheoretical2014,berkenkampNoRegret2019,ziomekBayesian2024} provided algorithms with theoretical guarantees when the kernel lengthscale is unknown. More recently, \citet{ziomekTimeVarying2024} introduced an elimination-based algorithm with theoretical guarantees for an arbitrary set of discrete priors. Their algorithm, Prior-Elimination GP-UCB (PE-GP-UCB), selects the arm and prior which provide the most optimistic upper confidence bound (UCB). If a prior generates too many incorrect predictions, then it may be eliminated. The previous work has focused on optimistic UCB methods which are known to over-explore.   

In this work, we investigate the use of Thompson sampling for solving GP-bandit problems with unknown priors and we propose two algorithms. The first algorithm, Prior-Elimination GP-TS (PE-GP-TS), is an extension of PE-GP-UCB that replaces the doubly optimistic selection rule with posterior sampling and one less layer of optimism. We obtain a regret bound for PE-GP-TS of order $\O(\sqrt{T \beta_T \hat{\gamma}_T})$ where $T$ is the horizon and $\hat{\gamma}_T$ is the worst-case maximum information gain, which matches that of PE-GP-UCB. The second algorithm, HyperPrior GP-TS (HP-GP-TS), uses bi-level posterior sampling to efficiently explore the priors and arms. We derive a regret bound of order $\O(\sqrt{T \beta_T \bar{\gamma}_T})$ where $\bar{\gamma}_T$ is the maximum information gain averaged across a hyperprior. The obtained bound matches that of standard GP-TS up to a factor $\O(\sqrt{\log T})$ whenever the hyperprior is deterministic. Finally, we evaluate our methods on three sets of synthetic experiments and two experiments with real-world data. Across the experiments, our Thompson sampling based methods outperform PE-GP-UCB. Additionally, we analyze the priors selected by the algorithms and observe that HP-GP-TS selects the correct prior more often than the other algorithms.  

The contributions of this work can be summarized as:
\begin{itemize}
    \item We propose two novel algorithms for GP-bandits with unknown prior: PE- and HP-GP-TS.
    \item We provide theoretical regret bounds of order $\O(\sqrt{T \beta_T \hat{\gamma_T}})$ and  $\O(\sqrt{T \beta_T \bar{\gamma_T}})$ for PE- and HP-GP-TS, respectively.
    \item We experimentally evaluate our algorithms on both synthetic and real-world data, demonstrating that they achieve superior performance by avoiding the optimistic exploration of PE-GP-UCB.
\end{itemize}


\section{Background and problem statement}
\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{priorselection.pdf}
    \caption{Agent deciding between priors based on observed data. The unknown function $f$ is sampled from a periodic prior and the observed data is in the top left. The agent discards the priors on the right since they do not match the periodic observations.}
    \label{fig:priorselection}
\end{figure}
\paragraph{Problem statement}
We consider a sequential decision making problem where an agent repeatedly selects among a set of arms and receives a random reward whose mean depends on the selected arm and is unknown to the agent. The goal of the agent is to maximize the cumulative sum of rewards over a finite time horizon. We assume that the distribution of the means, the {\it prior}, is sampled from a set of priors, the {\it hyperprior}. An effective agent must distinguish which prior the means are sampled from to ensure it explores efficiently. We illustrate this in \cref{fig:priorselection}.

Now, let us formally state the problem. Let $\Xc \subseteq [0,r]^d \subset \R^d$ denote the finite set of arms and $P$ a finite set of priors with associated prior mean and kernel functions $\mu_{1, p}: \Xc \mapsto \R$ and $k_{1, p}: \Xc \times \Xc \mapsto \R $, $\forall p \in P$. Let $p^* \in P$ denote the true prior and assume the expected reward function $f: \Xc \mapsto \R \sim \GP(\mu_{1,p^*}, k_{1,p^*})$ is a sample from a Gaussian process with prior $p^*$. Both the function $f$ and the true prior $p^*$ are considered unknown. We will consider two settings: In the frequentist selection setting, the prior $p^* \in P$ is picked arbitrarily. In the Bayesian selection setting, the prior is sampled from a hyperprior $p^* \sim \mathcal{P}(P)$. To simplify notation, let $P_0$ denote the hyperprior.

Let $T$ denote the horizon. For time step $t = 1, 2, \ldots, T$, the agent selects an arm $x_t \in \Xc$ and observes the reward $r_t = f(x_t) + \epsilon_t$ where $\{\epsilon_t\}_{t=1}^T$ are i.i.d. zero-mean Gaussian noise with variance $\sigma^2$. The goal of the agent is to select a sequence of arms $\{ x_t \}_{t=1}^T$ that minimizes the regret $R(T) = \sum_{t \in [T]} f(x^*) - f(x_t)$ where $[T] = \{1,\ldots, T\}$ and $x^* = \argmax_{x \in \Xc} f(x)$. In the Bayesian selection setting, we evaluate the agent based on the Bayesian regret $\BR(T) = \E \left[ R(T) \right]$ where the expectation is taken over the prior $p^*$, the expected reward function $f$, the noise $\{\epsilon_t\}_{t=1}^T$ and the potentially stochastic selection of arms.

\paragraph{Gaussian processes} A Gaussian process $f(x) \sim \GP(\mu, k)$ is a collection of random variables such that for any subset $\{ x_1, \ldots, x_n\} \subset \Xc$, the vector $[f(x_1), \ldots f(x_n)] \in \R^n$ has a multivariate Gaussian distribution. The probabilistic nature of GPs makes them very useful for defining and solving bandit problems where the arms are correlated. Given $t$ observations $\{(x_i, y_i)\}_{i=1}^t$, the posterior mean and kernel functions of a Gaussian process $\GP(\mu, k)$ are given by 
\begin{align}
    \mu_t(x) &= \mu(x) + \k^\top \left( \K + \sigma^2 I \right)^{-1} (\y - \bmu), \\
    k_t(x, \tilde{x}) &= k(x, \tilde x) - \k^\top \left( \K + \sigma^2 I \right)^{-1} \tilde{\k}.
\end{align}
Above, $\k, \tilde \k \in \R^t$ are vectors such that $(\k)_i = k(x_i, x)$ and $(\tilde{\k})_i = k(x_i, \tilde x)$. Additionally, $\y, \bmu \in \R^t$ are also vectors such that $(\y)_i = y_i$ and $(\bmu)_i = \mu(x_i)$. The gram matrix is denoted by $\K \in \R^{t \times t}$ where $(\K)_{i,j} = k(x_i, x_j)$. Let $\mu_{t, p}$ and $k_{t, p}$ denote the posterior mean and kernel for a Gaussian process with prior $p \in P$ at time $t$ and let $\sigma^2_{t,p}(x) = k_{t, p}(x, x)$ denote the posterior variance at time $t$.

The kernel $k$ determines important characteristics of the functions $f$ and in the following we provide some examples. The RBF kernel, $k(x, \tilde{x}) = \exp(-|| x - \tilde x ||^2 / \ell^2)$ guarantees that $f$ is smooth. The lengthscale parameter $\ell > 0$ determines how quickly $f$ changes, smaller values lead to more fluctuations. The rational quadratic (RQ) kernel $k(x, \tilde x) = \left( 1 + \frac{|| x - \tilde x ||^2}{2\alpha \ell^2} \right)^{-\alpha}$ where $\alpha > 0$ is a mixture of RBF kernels with varying lengthscales.
The Matérn kernel \cite{maternSpatial1986} $k(x, \tilde x) = \frac{2^{1-\nu}}{\Gamma(\nu)} \left( \frac{\sqrt{2\nu}|| x - \tilde x ||}{\ell}\right)^{\nu} K_{\nu} \left( \frac{\sqrt{2\nu} ||x - \tilde x||}{\ell}\right)$ where $\nu > 0$ is the smoothness parameter that imposes that $f$ is k-times differentiable if $\nu > k$ for integer $k$. The functions $\Gamma(\nu)$ and $K_v$ correspond to the gamma function and a modified Bessel function \cite{williamsGaussianProcessesMachine2006}.
The periodic kernel $k(x, \tilde{x}) = \exp \left(-\frac{1}{2} \sum_{i=1}^d \sin^2 (\frac{\pi}{\rho}(x - \tilde x)) / \ell \right)$ generates smooth and periodic functions with period $\rho > 0$ \cite{mackayIntroductionGaussianProcesses1998}.
The linear kernel $k(x, \tilde{x}) = v x^\top \tilde x / \nu$ generates linear functions where $v$ is the variance parameter.

\paragraph{Information gain} The maximal information gain (MIG) is a measure of reduction in uncertainty of $f$ after observing the most informative data points up to a specified size. The MIG commonly occurs in regret bounds for GP bandit algorithms \cite{srinivasInformationTheoretic2012,vakiliInformationGainRegret2021} and its growth rate is strongly determined by the prior kernel of the GP. Hence, we will define the MIG for any fixed GP prior $p \in P$. Let $\y_A$ denote noisy observations of $f$ at the locations $A \subset \Xc$. Then, the MIG given prior $p \in P$, $\gamma_{T,p}$, is defined as 
\begin{equation}
    \gamma_{T, p} := \sup_{A \subset \Xc, |A| \leq T} I_p(\y_A; f),
\end{equation}
where $I_p(\y_A; f) = H(\y_A | p) - H(\y_A | f, p)$ is the mutual information between $\y_A$ and $f$ given $p$, and $H(\cdot)$ denotes the entropy. To aid our analysis later, we also define the worst-case MIG as $\hat{\gamma}_T := \max_{p \in P} \gamma_{T,p}$ and the average MIG as $\bar{\gamma}_T := \E_{p \sim P_0} [\gamma_{T, p}]$. For the RBF and Matérn kernels, $\gamma_{T,p} = \O ( \log^{d+1}(T) )$ and $\gamma_{T,p} = \O ( T^{\frac{d}{2\nu + d}} \log^{\frac{2\nu}{2\nu + d}}(T) )$ \cite{srinivasInformationTheoretic2012,vakiliInformationGainRegret2021}.


\paragraph{Previous work} \citet{wangTheoretical2014} was the first work that presented regret bounds for Gaussian process bandits with unknown lengthscale. \citeauthor{wangTheoretical2014} studied the popular Expected Improvement algorithm \cite{mockusBayesian1975}. But their algorithm requires a lower bound on the lengthscale and their regret bound depends on the worst-case MIG. Later work by \citet{berkenkampNoRegret2019} introduced Adaptive GP-UCB (A-GP-UCB) that continually lowers lengthscale parameter. Once the lengthscale parameter is sufficently small, then the function $f$ will lie within the reproducing kernel Hilbert space (RKHS) and then the regular GP-UCB theory can be applied. However, A-GP-UCB lacks a stopping mechanism and will overexplore as the lengthscale continues to shrink. Recent work by \citet{ziomekTimeVarying2024} introduced Prior-Elimination GP-UCB (PE-GP-UCB) for time-varying GP-bandits with unknown prior. Unlike the work before, the regret bound of PE-GP-UCB holds for arbitrary types of hyperparametersin the GP prior. PE-GP-UCB is doubly optimistic and selects the prior {\it and} arm with the highest upper confidence bound. PE-GP-UCB tracks the cumulative prediction error made by the selected priors and eliminates priors that exceed a threshold level.

Other works have introduced regret balancing algorithms that maintain a set of base learning algorithms and balance their selection frequency to achieve close to optimal regret \cite{abbasi-yadkoriRegret2020, pacchianoRegret2020}. \citet{ziomekBayesian2024} built on this idea and introduced length-scale balancing GP-UCB which can adaptively explore smaller lengthscales but can return to longer ones, unlike A-GP-UCB.

\citet{wangRegret2018} provided regret bounds for probability of improvement \cite{kushnerNew1964} and GP-UCB when the GP prior is learnt from offline data, which they called {\it Meta BO}. The aforementioned works are based on UCB, EI, PI or regret balancing. However, another line of work has studied Thompson sampling in standard and linear bandits with unknown prior distribution \cite{kvetonMetaThompson2021, basuNo2021, hongHierarchical2022, liModified2024}. In their setting (meta or hierarchical bandits), the agent plays multiple bandit instances, either simultaneously or sequentially. The unknown means are sampled from the same (unknown) prior and by gathering knowledge across instances, the agent can solve later instances more efficiently once it has identified the prior.
We emphasize that these methods have been studied only for standard stochastic and linear bandits, not for GP bandits. 



\section{Algorithms}
As discussed by \citet{russoLearningOptimizePosterior2014}, TS can offer advantages over UCB algorithms for problems where constructing tight confidence bounds is difficult. In addition, Thompson sampling is often observed to perform better than UCB in practice \cite{chapelleEmpiricalEvaluationThompson2011,wenEfficientLearningLargeScale2015,kandasamyParallelisedBayesianOptimisation2018,akerblomOnlineLearningNetwork2023,akerblomOnlineLearningEnergy2023}. Motivated by this, we present two algorithms for efficient prior selection based on TS.

\subsection{Prior-Elimination with Thompson sampling}
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{priorelim.pdf}
    \caption{Elimination procedure of PE-GP-TS. The solid lines correspond to posterior means and the shaded regions are confidence intervals. The dashed lines are samples from the posteriors. 
    The figure has been adapted from \citet{ziomekTimeVarying2024} to illustrate the elimination step of PE-GP-TS.}
    \label{fig:priorelimts}
\end{figure}

\begin{algorithm}[htb]
    \caption{Prior Elimination GP-TS (PE-GP-TS)} \label{alg:PEGPTS}
    \begin{algorithmic}[1]
        \INPUT Horizon $T$, prior functions $\{\mu_{1, p}, k_{1, p}\}_{p \in P}$, confidence parameters $\{\beta_t\}_{t=1}^T$ and $\{\xi_t\}_{t=1}^T$.
        \STATE $P_0 = P$, $S_{0, p} = \emptyset$ $\forall p \in P$
            \FOR{$t = 1, 2\ldots, T$}
                \STATE Sample $\tilde{f}_{t, p} \sim \GP(\mu_{t,p}, k_{t,p})$ $\forall p \in P_{t-1}$
                \STATE Set $x_t, p_t = \argmax_{x, p \in \Xc \times P_{t-1}} \tilde{f}_{t, p}(x)$
                \STATE $S_{t, p_t} = S_{t-1,p_t} \cup \{ t \}$ and $S_{t, p} = S_{t-1, p}$ for $p \in P \setminus \{p_t\}$
                \STATE Observe $y_t = f(x_t) + \epsilon_t$
                \STATE Set $\eta_t = y_t - \mu_{t, p_t}(x_t)$ \alglinelabel{line:predictiveerror}
                \STATE Set $V_t = \sqrt{\xi_t | S_{t, p_t} |} + \sum_{i \in S_{t, p_t}} \sqrt{\beta_i} \sigma_{i, p_t}(x_i)$
                \IF{$ \left| \sum_{i \in S_{t,p_t}} \eta_i \right| > V_t$} \alglinelabel{line:elimincriteria}
                    \STATE $P_t = P_{t-1} \setminus \{p_t\}$
                \ELSE
                    \STATE $P_t = P_{t-1}$
                \ENDIF
            \ENDFOR
    \end{algorithmic}
\end{algorithm}
Our first algorithm is an extension of the PE-GP-UCB algorithm \cite{ziomekTimeVarying2024} to be employed with Thompson sampling - instead of UCB. The key difference is that instead of maximizing the upper confidence bound $U_{t}(x, p) = \mu_{t,p}(x) + \sqrt{\beta_t} \sigma_{t,p}(x)$ over $\Xc \times P$, we instead sample $\tilde{f}_{t, p}$ from the posterior $\GP(\mu_{t,p}, k_{t,p})$ for all priors $p \in P_{t-1}$ where $P_{t-1}$ is the set of active priors. Then, we select the arm and prior $x_t, p_t$ such that $x_t, p_t = \argmax_{x, p \in \Xc \times P_{t-1}} \tilde{f}_{t, p}(x)$. Whilst PE-GP-UCB has two layers of optimism, the upper confidence bound and joint maximization of $x$ and $p$. PE-GP-TS has only a single layer of optimism - which should alleviate potential overexploration issues.

The elimination procedure of PE-GP-TS is illustrated in \cref{fig:priorelimts}. Samples $\tilde{f}_{t, p}$ are drawn from the active prior $p \in P_{t-1}$. Then, the unknown function $f$ is queried at the selected arm $x_t$. If the observed value differs too much from the prediction made by the selected prior, then the selected prior is eliminated. Otherwise, it remains active.

The PE-GP-TS algorithm is presented in \cref{alg:PEGPTS}. Similar to PE-GP-UCB, the set $S_{t, p}$ is used to store the time steps where prior $p$ was selected up to and including time $t$. When prior $p_t$ is selected, the prediction error $\eta_t = y_t - \mu_{t, p_t}(x_t)$ between the observed and predicted value made by the prior $p_t$ is computed. If the sum of prediction errors made by the prior $p_t$ exceeds the threshold value $V_t$, then $p_t$ is eliminated from the active priors $P_t$, see \cref{line:elimincriteria}. Note that at time step $t$, only the selected prior $p_t$ can be eliminated. As such, if a prior is very pessimistic it may never be selected and therefore will never be eliminated. Thus, the final set of active priors $P_T$ should be viewed as non-eliminated priors rather than necessarily being reasonable priors.



\subsection{HyperPrior Thompson sampling}
\begin{algorithm}[H]
    \caption{HyperPrior GP-TS (HP-GP-TS)} \label{alg:HPGPTS}
    \begin{algorithmic}[1]
        \INPUT Horizon $T$, prior functions $\{\mu_{1, p}, k_{1, p}\}_{p \in P}$, hyperprior $P_0$.
            \FOR{$t = 1, 2\ldots, T$}
                \STATE Sample $p_t \sim P_{t-1}$
                \STATE Sample $\tilde{f}_{t} \sim \GP(\mu_{t, p_t}, k_{t, p_t})$
                \STATE Set $x_t = \argmax_{x} \tilde{f}_t$
                \STATE Observe $y_t = f(x_t) + \epsilon_t$
                \STATE Set ${P}_t(p) \propto \Pb (y_t | x_t, \{ x_i, y_i \}_{i = 1}^{t-1}, p) P_{t-1}(p)$ 
                \\ \COMMENT{Update hyperposterior}
            \ENDFOR
    \end{algorithmic}
\end{algorithm}
In our first algorithm, we removed one layer of optimism. In our second algorithm, we adopt a fully Bayesian algorithm by using a hyperposterior sampling scheme where both the prior and the mean function are sampled from their respective posteriors. By shedding the optimism over the selected prior $p_t$, HP-GP-TS should be able avoid costly exploration by selecting likely priors instead of optimistic ones.

The algorithm is presented in \cref{alg:HPGPTS}. In the first step, the current prior $p_t$ is sampled from the hyperposterior $P_{t-1}$. Then, a single sample $\tilde f_t$ is taken from the selected posterior $\GP(\mu_{t, p_t}, k_{t, p_t})$ and is used to select the current arm: $x_t = \argmax_{x \in \Xc} \tilde{f}_t(x)$. After observing $y_t$, the hyperposterior is updated by computing the likelihood of $y_t$ under the different priors. Note that since the set of priors $P$ is finite, computing the posterior is tractable albeit computationally costly with a complexity of $\O(t^3|P|)$. The likelihood $\Pb (y_t | x_t, \{ x_i, y_i \}_{i = 1}^{t-1}, p) = \N(y_t; \mu_{t, p}(x_t), \sigma_{t,p}^2(x_t) + \sigma^2)$ is simply the Gaussian likelihood of the posterior at $x_t$ plus Gaussian noise with variance $\sigma^2$.


\section{Regret bounds}
In this section, we present regret bounds for the proposed algorithms. Recall from the problem statement that we consider two slightly different settings for the two algorithms. Specifically, for PE-GP-TS we assume the unknown prior $p^*$ is sampled arbitrarily from $P$ whilst for HP-GP-TS we assume that the unknown prior $p^*$ is selected from a known hyperprior distribution $P_0$.

\citet{ziomekTimeVarying2024} structured the proof of the regret bound of PE-GP-UCB into 4 larger steps; First, showing that $p^*$ is never eliminated with high probability. Second, establishing a bound on the simple regret. Third, bounding the cumulative regret. Finally, the cumulative bound is re-expressed in terms of the worst-case MIG. For PE-GP-TS, we note that it is sufficient to establish a new bound on the simple regret and then adapt the steps of \citeauthor{ziomekTimeVarying2024} to accommodate the new simple regret bound.

To bound the simple regret, we require two concentration inequalities to hold for both the posteriors and the posterior samples which we present in the following lemma.

\begin{restatable}{lemma}{lemConcentrationIntervals} \label{lem:concentrationintervals}
    For any $p^* \in P$, if $f(x) \sim \GP(\mu_{1, p^*}, k_{1, p^*})$ and $\beta_t = 2 \log \left ( \frac{|\Xc| |P| \pi^2 t^2}{3 \delta}\right)$. Then, with probability at least $1 - \delta$, the following holds for all $t, x \in [T] \times \Xc$:
    \begin{equation}
        | f(x) - \mu_{t, p^*}(x) | \leq \sqrt{\beta_t} \sigma_{t, p^*}(x), \label{eq:postconfinv}
    \end{equation}
    and for all $t, x, p \in [T] \times \Xc \times P:$
    \begin{equation}
        | \tilde{f}_{t, p} (x) - \mu_{t, p} (x) | \leq \sqrt{\beta_t} \sigma_{t, p}(x). \label{eq:sampleconfinv}
    \end{equation}
\end{restatable}
All proofs can be found in \cref{sec:allproofs}.
\cref{lem:concentrationintervals} is based on Lemma 5.1 of \citet{srinivasInformationTheoretic2012} but adapted to TS by specifying that it holds for any sequence of $x_1, \ldots, x_T$, as discussed by \cite{russoLearningOptimizePosterior2014}. Additionally, we add \cref{eq:sampleconfinv} which can be shown through the same steps and an additional union bound over $P$. Next, we state our bound for the simple regret of PE-GP-TS.

\begin{restatable}{lemma}{lemSimpleRegret}
If the event of \cref{lem:concentrationintervals} holds, then the following holds for the simple regret of PE-GP-TS for all $t \in [T]$:
\begin{equation}
    f(x^*) - f(x_t) \leq 2 \sqrt{\beta_t} \sigma_{t, p^*}(x^*) + \sqrt{\beta_t} \sigma_{t, p_t}(x_t) - \eta_t + \epsilon_t. \label{eq:simpleregret}
\end{equation}
\end{restatable}

Compared to the simple regret bound for PE-GP-UCB, we obtain the additional term $2 \sqrt{\beta_t} \sigma_{t, p^*}(x^*)$. Since $p^*$ is fixed, the sum over $t$ of the new term is $\O(\sqrt{T\beta_T\gamma_{T,p^*}})$ and we obtain the following regret bound:


\begin{restatable}{theorem}{thmPEGPTSregretbound}
    If $p^* \in P$ and $f \sim \GP(\mu_{1,p^*}, k_{1,p^*})$, then PE-GP-TS with confidence parameters $\beta_t = 2 \log (2 |\Xc| |P| \pi^2 t^2 / 3 \delta)$ and $\xi_t = 2 \sigma^2 \log (|P| \pi^2 t^2 / 3\delta)$, satisfies the following regret bound with probability at least $1 - \delta$:
    \begin{equation}
    \begin{aligned}
        R(T) \leq &2 |P| B_{p^*} + 2 \sqrt{\xi_T |P| T} \\
        &+ 2 \sqrt{C T \beta_T \gamma_{T,p^*}} +2 \sqrt{C T \beta_T \hat{\gamma}_{T} |P|}
    \end{aligned}
    \end{equation}
    where $B_{p^*} = \beta_1 + \sup_{x \in \Xc} |\mu_{1, p^*}(x)|$ and $C = 2 /\log(1 + \sigma^{-2})$.
\end{restatable}

Since $\gamma_{T,p^*} \leq \hat{\gamma}_T$, the bound is of order $\O( \sqrt{T \beta_T \hat{\gamma}_T})$ w.r.t. $T$ which matches that of PE-GP-UCB. To our knowledge, the best lower bound for standard GP bandits in the Bayesian setting, where $f$ is sampled from a GP, is $\Omega(\sqrt{T})$ for $d = 1$ \cite{scarlettTight2018}. This would suggest that our bound is tight up to a factor $\O(\sqrt{\beta_T \hat{\gamma}_T})$. 

A critical property of standard GP-TS is that $x_t | H_t \eqd x^* | H_t$ where $\eqd$ denotes equal in distribution. For HP-GP-TS, this property holds and we have the additional property $p_t | H_t \overset{d}{=} p^* | H_t$ since $p_t$ is sampled from the posterior distribution of $p^*$. To establish the regret bound on HP-GP-TS, we adopt the regret decomposition of \citet{russoLearningOptimizePosterior2014} whilst utilizing the properties of HP-GP-TS. To bound the expected sum of posterior variances w.r.t. the true prior $p^*$ ($\E[\sumt \sigma^{2}_{t,p^*}(x_t)]$), we require the following lemma and corollary.


\begin{restatable}{lemma}{lemPriorInfoGain}\label{lem:priorinfogain}
    Let $C = 2/ \log (1 + \sigma^{-2})$ and assume $\sigma_{t,p}(x) \leq 1$, then for any sequence of arms $x_1, \ldots, x_T \in \Xc$ and any $p \in P$
    \begin{equation}
        I_p(\y_T; f) = \frac{1}{2} \sum_{t \in [T]} \log \left(1 +  \sigma^{-2} \sigma_{t, p}^2 (x_t) \right), \label{eq:infgainposterior}
    \end{equation}
    \begin{equation}
        \sumt \sigma^2_{t,p}(x_t) \leq C I_p(\y_t; f). \label{eq:sumvariance}
    \end{equation}
\end{restatable}

\begin{restatable}{corollary}{lemAvgInfoGain}\label{cor:avginfogain}
    For any stochastic or deterministic sequence of arms $x_1, \ldots, x_T \in \Xc$ and $p^* \sim P_0$, 
    \begin{equation}
        \E\Big[ \sumt \sigma_{t,p^*}^2(x_t) \Big]  \leq C \bar{\gamma_{T}}. \label{eq:avginfogain}
    \end{equation}
\end{restatable}

The proof of \cref{lem:priorinfogain} is based on Lemma 5.3 and 5.4 in \cite{srinivasInformationTheoretic2012} but is adapted to randomized selection by specifying that it holds for any sequence of arms. \cref{cor:avginfogain} follows by \cref{lem:priorinfogain} and the definition of maximum information gain. Next, we state our regret bound for HP-GP-TS.

\begin{restatable}{theorem}{thmFBGPTS}
If $p^* \sim P_0$ and $f \sim \GP(\mu_{1,p^*}, k_{1, p^*})$, then the Bayesian regret of HP-GP-TS is bounded by
\begin{equation}
    \BR(T) \leq \frac{\pi^2}{6} + \sqrt{C_1 T \beta_T \bar{\gamma}_T}
\end{equation}
where $C_1 = 2 \log (1 + \sigma^{-2})$ and $\beta_t = 2 \log(|\Xc| t^2 / \sqrt{2\pi)}$.
\end{restatable}
Unlike PE-GP-TS and PE-GP-UCB, the regret bound for FB-GP-TS depends on the average $\bar{\gamma}_T$ rather than the worst case $\hat{\gamma}_T$ which can impact the theoretical regret significantly if the complexity of learning the priors differ. Although, this is reasonable since the elimination methods assume arbitrary selection of $p^*$ as opposed to sampling from a hyperprior. If the hyperprior is deterministic then the regret bound for FB-GP-TS matches that of GP-TS up to a factor $\O(\sqrt{\log T})$ \cite{takenoPosterior2024} and the average $\bar{\gamma}_T$ would be equal to the worst case $\hat{\gamma}_T$. Again, using the lower bound of \citet{scarlettTight2018}, our upper bound would tight up to a factor of $\O(\sqrt{\beta_T \bar{\gamma}_T})$.


\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{pwlinear.pdf}
    \caption{Prior mean functions for the hill experiment. Mean function 0 consists of 10 smaller hills whereas mean functions 1 to 4 have 9 smaller hills and 1 taller hills.}
    \label{fig:hills}
\end{figure}
\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{syntheticregret.pdf}
    \caption{Cumulative regret for synthetic experiments with varying kernel (left), lengthscale (center) and mean function (right). The average final regret for PE-GP-UCB is 116.5 in the lengthscale experiment. Errorbars correspond to $\pm1$ standard error.}
    \label{fig:regretsynthetic}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{elimination.pdf}
    \caption{Average amount of priors remaining in $P_t$ over time for PE-GP-UCB and -TS.}
    \label{fig:elimination}
\end{figure*}


\section{Experiments}
In this section, we describe our experiments based on synthetic and real-world data.

\paragraph{Synthetic experiments}



We consider three synthetic setups with different choices of priors in $P$. For the first setup, the priors have varying kernels selected according to one of the following kernels: i) squared-exponential kernel, Matérn kernel with $\nu=5/2$, ii) Matérn kernel with $\nu=3/2$, iii) periodic kernel with period $\rho = 5$, iv) linear kernel with $v = 0.05^2$, and, v) the rational quadratic kernel with $\alpha = 0.5$. All kernels use a lengthscale of $1.0$ and are scaled s.t. $k(x, \tilde{x}) \leq 1$. In addition, the mean function for all priors are zero everywhere. For the second setup, the priors use the squared-exponential kernel with lengthscales $4$, $2$, $1$ or $1/2$. The third setup is based on the toy problem in an earlier version of \cite{ziomekTimeVarying2024} where the mean functions vary. The first mean function has 10 smaller `hills' across the first half of the interval. The four other mean functions are the same except the height of one of the hills is doubled in height, see \cref{fig:hills}. All priors in the third setup use a squared-exponential kernel with lengthscale 1. For all of the setups, the true prior $p^*$ is sampled uniformly from $P$, 500 arms are equidistantly spaced in $[0, 20]$, the noise variance $\sigma^2 = 0.25^2$, and the horizon $T = 500$. The prior elimination methods use $\delta = 0.05$. All models are evaluated on 500 seeds on each setup. As baselines, we use PE-GP-UCB and Maximum A Posteriori (MAP) GP-TS where MAP GP-TS is identical to HP-GP-TS except for greedily selecting $p_t$ from the posterior: $p_t = \argmax_p P_{t-1}(p)$. Regardless of the selected prior, HP-GP-TS observes an observation $y_t$ to update the hyperposterior $P_t$. Hence, greedily selecting the prior could reduce unnecessary exploration. In addition, we investigate the oracle variants of PE-GP-TS and PE-GP-UCB with $\delta = 0.05$ that are only given the true prior: $P_0 = \{ p^* \}$.


\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{entropy.pdf}
    \caption{Average entropy in the hyperposterior $P_t$ over time for HP- and MAP GP-TS. The dashed reference values correspond to entropies of discrete distributions with prob. $q$ on one choice and prob. $\frac{1-q}{|P|-1}$ on the other $|P|-1$ choices.}
    \label{fig:entropy}
\end{figure*}

The cumulative regret for the three synthetic experiments is shown in \cref{fig:regretsynthetic}. Across all three experiments, we observe that HP-GP-TS has lower regret than the other methods and performs close to the oracle GP-TS. For the kernel and hill experiments, PE-GP-TS has lower regret than the oracle GP-UCB. Hence, even if PE-GP-UCB was optimized to perform as well as the oracle, it would still not achieve the regret of our proposed methods. MAP GP-TS has slightly higher regret than HP-GP-TS for the lengthscale and hill experiments but has significantly higher regret for the kernel experiment. The greedy selection causes under-exploration for MAP GP-TS in certain instances. 

In the three left-most columns of \cref{fig:elimination}, the average amount of priors remaining is shown over time for the synthetic experiments (for the PE-methods methods). For the hill experiment, no priors are eliminated for either PE-GP-TS nor PE-GP-UCB. For the kernel and lengthscale experiment, PE-GP-TS eliminates more priors than its UCB-counterpart. However, note that, on average, less than 0.15 priors are eliminated in the lengthscale experiment and slightly more than 1.2 priors in the kernel experiment. 

Similarly, in \cref{fig:entropy}, the average entropy in the hyperposterior $P_t$ is shown over time. Across the three experiments, the hyperposterior of HP-GP-TS concentrates more than that of MAP GP-TS - which could be due to the greedy prior selection. As a comparison, HP-GP-TS places more than 95\% of the probability mass on one prior (based on the reference values) in the lengthscale experiment where as the PE-methods barely removed priors. 
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{confmat_kernels.pdf}
    \caption{Confusion matrices for the true prior $p^*$ and the selected priors $p_t$ across all time steps of the kernel experiment.}
    \label{fig:kernelconfusion}
\end{figure}

In \cref{fig:kernelconfusion}, we visualize how often the methods select the true prior $p^*$ (or kernel) in the synthetic experiments as confusion matrices. PE-GP-UCB almost always selects the prior using the Matérn-3/2 kernel. This prior induces a distribution over functions that are less smooth compared to the other kernels. Hence, the Matérn-3/2 kernel produces much higher confidence intervals outside the observed data and is thus more optimistic, leading to excessive exploration. PE-GP-TS also shows a bias towards the Matérn-3/2 kernel but does not select it as frequently as PE-GP-UCB - demonstrating that one layer of optimism has been removed. The overall ``accuracy" of the selected priors, i.e. $\sumt \mathds{1}\{ p_t = p^* \} / T$ where $\mathds{1}$ is the indicator function, of the elimination-based methods is around $17\%$ in the kernel experiment compared to $62.5\%$ and $63.2\%$ for MAP and HP-GP-TS respectively. For HP-GP-TS, we observe that it can easily identify the periodic and linear kernels. However, the RBF, Matérn and RQ kernels are often confused with each other. These kernels do not have as easily distinguishable characteristics and are likely to produce similar posteriors given enough data.  



\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{intelpemsregret.pdf}
    \caption{Cumulative regret on Intel temperature data (top) and PeMS data (bottom). Errorbars correspond to $\pm1$ standard error.} 
    \label{fig:realworldregret}
\end{figure}


\paragraph{Real-world data}
We consider two setups with real-world data. Using the Intel Berkeley dataset \cite{madden2004intel}, the first setup considers 46 temperature sensors with measurements over 19 days. The goal is to select the sensor with the highest temperature. The data is split into 12 training days and 7 test days. For each day in the training set, the empirical mean and covariance is computed and set as a prior. For each run, the unknown function $f$ is selected as a set of measurements from the test data and the noise variance is set to $\sigma^2 = 0.7^2$. 

The second setup uses sensor data from 211 sensors along the I-880 highway in March 2023 from the California Performance Measurement System (PEMS) \cite{chenFreeway2001, pems}. The goal is to select the sensor with the lowest speed in order to identify congestions. The sensor data prior to March 21st is used as training data and the rest is used as test data. Only sensor readings between 6AM and 13AM are used. For each hour in the training set (across all training days), the empirical mean and covariance is again used to construct a prior - yielding 7 priors. Again, the test data is used to sample the unknown function $f$ and the noise variance is set to $\sigma^2 = 2.25^2$. PE-GP-UCB and MAP-GP-TS are used as baselines to compare against PE-GP-TS and HP-GP-TS. The elimination methods use $\delta = 0.05$, the horizon is set to $T = 500$ and all methods are evaluated on $500$ seeds\footnote{One outlier was removed from the Intel experiment, see \cref{app:expdetails} for more details.}. 


The cumulative regret for the real-world data experiments is presented in \cref{fig:realworldregret}. For the Intel temperature data, HP-GP-TS and MAP-GP-TS obtain the lowest and second lowest cumulative regret respectively. For the PeMS data, PE-GP-TS obtains the lowest cumulative regret of all methods. HP-GP-TS and MAP-GP-TS have lower regret than PE-GP-UCB but their slopes are almost linear for the final time steps. To understand this better, we visualize quantiles of the total regret in \cref{fig:regretboxplot} and the median cumulative regret in \cref{fig:realworldmedianregret} in \cref{app:expdetails}. The figures show that MAP- and HP-GP-TS have the lowest median regret for both experiments and hence perform best in a majority of instances. However, the 90th and 95th quantiles are considerably larger for the PeMS data which impacts the average regret significantly. Hence, for the PeMS data, the prior elimination methods seem to yield more stable results.

The two right-most columns of \cref{fig:elimination,fig:entropy} shows the average amount of active priors and the average entropy over time in the Intel and PeMS experiment. For the PeMS data, barely any priors are eliminated whilst for the Intel data around 0.75 to 0.80 priors are eliminated on average for the PE-methods. For the Intel data, the HP- and MAP GP-TS place around than 80\% to 90\% of the posterior probability mass in one prior based on the reference values. On the PeMS data, the posteriors concentrate more heavily - which could be premature given the average regret we observed.

\begin{figure}
    \centering
    \begin{subfigure}[c]{0.5\linewidth}
        \includegraphics[width=\linewidth]{boxplot-intel.pdf}
        \caption{Intel}
    \end{subfigure}%
    \begin{subfigure}[c]{0.5\linewidth}
        \includegraphics[width=\linewidth]{boxplot-pems.pdf}
        \caption{PeMS}
    \end{subfigure}
    \caption{Quantiles of the final cumulative regret on the real-world data experiments. The median is highlighted in green. The whiskers correspond to the 5th and 95th percentile and the lower and upper edge of the box show the first and third quantile.}
    \label{fig:regretboxplot}
\end{figure}



\section{Discussion}
\paragraph{Limitations}
The main limitation of all the methods we have considered is that they require the set of priors $P$ to be discrete. In addition, their computational cost scales linearly with the number of priors considered. In theory, the elimination-based methods could have lower computational cost compared to HP-GP-TS. However, in practice, priors are rarely eliminated as shown by \cref{fig:elimination}. At best, one sixth of the priors is eliminated in the kernel experiment for PE-GP-TS. Since only priors that are selected can be eliminated and the confidence parameters $\beta_t$, $\xi_t$ increasing over time, including more priors could lead to less priors being eliminated overall. 


\paragraph{Future work}
To lower the computational complexity of the elimination methods, one would either need tighter confidence bounds for the elimination criteria or need to allow more than the selected priors to be eliminated. The bi-level sampling approach of HP-GP-TS provides efficient exploration but suffers from higher computational cost since it cannot discard priors. An interesting line of future work could study how the probabilities in the hyperposterior could be used to discard priors that are unlikely to be selected in the remaining rounds. It would also be interesting to study selection criteria for the prior $p_t$ that maximize the disagreement among the priors such that evidence for the true prior is generated quicker. 

A key difference between PE-GP-TS and the meta bandits methods of \citet{kvetonMetaThompson2021,basuNo2021,hongHierarchical2022} is that they consider multiple bandit instances sampled from the true prior $p^*$. Specifically, \citet{hongHierarchical2022} study a setting with arbitrary interleaving of the instances. Generalizing our results for HP-GP-TS to a similar setting would provide an interesting complement to these methods beyond the standard and linear setting. 



\section{Conclusion}
In this paper, we have proposed two algorithms for joint prior selection and regret minimization in GP bandits based on GP-TS. We have analyzed the algorithms theoretically and provided regret bounds whose order closely match previous work. We have experimentally evaluated both algorithms on both synthetic and real-world data. We find that they both select the true prior more often and obtain lower regret than previous work due to lowering the amount of optimistic exploration. 


\section*{Acknowledgements}
The work of Jack Sandberg and Morteza Haghir Chehreghani was partially supported by the Wallenberg AI, Autonomous Systems and Software Program (WASP) funded by the Knut and Alice Wallenberg Foundation. The computations were enabled by resources provided by the National Academic Infrastructure for Supercomputing in Sweden (NAISS), partially funded by the Swedish Research Council through grant agreement no. 2022-06725.


\bibliography{references}
\bibliographystyle{referenceformat}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Proofs} \label{sec:allproofs}
In the following section, we state and prove the results shown in the main text.

\subsection{PE-GP-TS} \label{sec:proofsPEGPTS}
First, we state and prove concentration inequalities for $f(x)$ and $\tilde{f}_{t,p}(x)$.

\lemConcentrationIntervals*
\begin{proof}
    Follows by the same steps as Lemma 5.1 of Srinivas except we condition on the complete history $H_t$ instead of only $\y_{1:t-1}$. Additionally, for \cref{eq:sampleconfinv} we must take an additional union bound over $p \in P$.

    Fix $t, x, p \in [T] \times \Xc \times P$. Given the history $H_t$, $\tilde{f}_{t, p}(x) \sim \N(\mu_{t,p}(x), \sigma^2_{t,p}(x))$. 
    Using that $\Pb( Z > c ) \leq 1/2 e^{-c^2/2}$ for $Z \sim \N(0,1)$, we get that 
    \begin{align}
        \Pb\left( \left| \frac{\tilde{f}_{t, p}(x) - \mu_{t, p^*}(x) }{\sigma_{t, p^*} (x)} \right|  > \sqrt{\beta_t} \right) &\leq \exp( - \beta_t / 2 ) \\
        &= \frac{3 \delta }{ |\Xc| |P| \pi^2 t^2}
    \end{align}
    Note that $\sum_{t\geq1} \frac{1}{t^2} = \frac{\pi^2}{6}$By taking the union bound over $\Xc$, $P$ and $t \geq 1$, \cref{eq:sampleconfinv} holds w.p. at least $1 - \delta/2$. By the same reasoning and skipping the union bound over $P$, \cref{eq:postconfinv} holds w.p. at least $1-\delta/2$. Thus, both events hold w.p. at least $1 - \delta$.
\end{proof}

Next, we state three lemmas from \citet{ziomekTimeVarying2024} that are used in the proof of our regret bound.

\begin{lemma}{(Lemma 5.1 of \citet{ziomekTimeVarying2024})} \label{lem:noiseconc}
    If $\xi_t = 2 \sigma^2 \log  \left( \frac{|P| \pi^2 t^2}{6 \delta} \right)$, then the following holds with probability at least $1 - \delta$:
    \begin{equation}
        \left| \sum_{i \in S_{t, p}} \epsilon_i \right| \leq \sqrt{\xi_t |S_{t, p}|} \quad \forall t, p \in [T] \times P.
    \end{equation}
\end{lemma}
\begin{lemma}{(Lemma 5.2 of \citet{ziomekTimeVarying2024})} \label{lem:fbound}
    Let $B_{p^*} = \beta_1 + \sup_{x \in \Xc} |\mu_{1, p^*}(x)|$, then if $\mu_{1,p^*}$ and $k_{1,p^*}$ satisfy $|\mu_{1,p^*} (\cdot)| < \infty$ and $k_{1,p^*}(\cdot, \cdot) \leq 1$ and \cref{lem:concentrationintervals} holds, then 
    \begin{equation}
        \sup_{x \in \Xc} |f(x)| \leq B_{p^*}.
    \end{equation}
\end{lemma}
\begin{lemma}{(Lemma 5.3 of \citet{ziomekTimeVarying2024})} \label{lem:infgainbound}
    For $C = 2 / \log(1 + \sigma^{-2})$, $\sum_{t \notin \C} \sqrt{\beta_t} \sigma_{t, p_t} (x_t) \leq \sqrt{C T \beta_T \hat{\gamma}_{T} |P|}$ where $\beta_T = \max_{p \in P} \beta_T$ and $\hat{\gamma}_T = \max_{p \in P} {\gamma}_{T,p}$.
\end{lemma}

Then, we state and prove the new simple regret bound for PE-GP-TS. 

\lemSimpleRegret*
\begin{proof}
    First, we upper bound $f(x^*)$ as follows
    \begin{align}
        f(x^*) &\leq \mu_{t, p^*}(x^*) + \sqrt{\beta_t} \sigma_{t, p^*} (x^*)
        &(\text{\cref{eq:postconfinv}}) \\
        &\leq \tilde{f}_{t, p^*}(x^*) + 2 \sqrt{\beta_t} \sigma_{t, p^*}(x^*) 
        &(\text{\cref{eq:sampleconfinv}}) \\
        &\leq \tilde{f}_{t, p_t}(x_t) + 2 \sqrt{\beta_t} \sigma_{t, p^*}(x^*).
        &(\text{TS selection rule}) \label{eq:simpleupper}
    \end{align}
    Then, we lower bound $f(x_t)$
    \begin{align}
        f(x_t) &= \mu_{t, p_t}(x_t) + \eta_t - \epsilon_t 
        &(\text{Def. of } \eta_t) \\
        &\geq \tilde{f}_{t, p_t} (x_t) - \sqrt{\beta_t} \sigma_{t, p_t} (x_t) + \eta_t - \epsilon_t.
        &(\text{\cref{eq:sampleconfinv}}) \label{eq:simplelower}
    \end{align}
    Combining, \cref{eq:simpleupper,eq:simplelower} we obtain 
    \begin{align}
        f(x^*) - f(x_t) \leq 2 \sqrt{\beta_t} \sigma_{t, p^*} (x^*) + \sqrt{\beta_t} \sigma_{t, p_t}(x_t) - \eta_t + \epsilon_t. \label{eq:simpleregretbound}
    \end{align}
\end{proof}

Finally, we state and prove the cumulative regret bound for PE-GP-TS.

\thmPEGPTSregretbound*
\begin{proof}    
    First, we show that the true prior $p^*$ is never rejected if \cref{lem:concentrationintervals,lem:noiseconc} hold.
    \begin{align}
        \left| \sum_{i \in S_{t, p^*}} \eta_i \right| &= \left| \sum_{i \in S_{t, p^*}} (y_i - f(x_i) + f(x_i) - \mu_{i, p^*}(x_i) \right| \\
        &\leq \left| \sum_{i \in S_{t, p^*}} \epsilon_i \right| + \sum_{i \in S_{t, p^*}} \left| f(x_i) - \mu_{i, p^*}(x_i) \right| 
        & (\text{Triangle ineq.)}\\
        &\leq \sqrt{\xi_t |S_{t, p^*}| } + \sum_{i \in S_{t, p^*}} \sqrt{\beta_i} \sigma_{i, p^*}(x_i).
        &(\text{\cref{lem:noiseconc,lem:concentrationintervals})}
    \end{align}

    Next, we bound the cumulative regret. To establish a bound on the cumulative regret, we must separate out the rounds where priors are eliminated. Hence, define the set of critical iterations as
    \begin{equation}
        \C = \left\{ t \in [T] : \left| \sum_{i \in S_{t, p_t}} \eta_i \right| > \sqrt{\xi_t S_{t, p_t}} + \sum_{i \in S_{t, p_t}} \sqrt{\beta_i} \sigma_{i, p_t}(x_i) \right\}.
    \end{equation}
    
    Using \cref{lem:fbound,eq:simpleregretbound}, we can bound the cumulative regret as follows:
    \begin{align}
        \BR(T) &= \sum_{t \in \C} \BR_t + \sum_{t \notin \C} \BR_t \\
        &\leq 2 |P| B_{p^*}  + \sum_{t \notin \C} 2 \sqrt{\beta_t} \sigma_{t, p^*}(x^*) 
        + \sum_{t \notin \C} \sqrt{\beta_t} \sigma_{t, p_t} (x_t) + \sum_{p \in P} \sum_{t \in S_{T, p} \setminus \C} (\epsilon_t - \eta_t).
    \end{align}
    where $B_{p^*} := \beta_1 + \sup_{x \in \Xc} | \mu_{1, p^*} (x)|$.
    If $t \notin \C$, \cref{line:elimincriteria} in \cref{alg:PEGPTS} evaluates to {\tt false} and hence
    \begin{equation}
        \sum_{p \in P} \sum_{t \in S_{T, p} \setminus \C} -\eta_t \leq \sum_{p \in P} \sqrt{\xi_T |S_{T, p}|} + \sum_{p \in P} \sum_{t \in S_{T, p} \setminus \C} \sqrt{\beta_t} \sigma_{t, p}(x_t). 
    \end{equation}
    Additionally, using \cref{lem:noiseconc}, we can bound the Gaussian noise:
    \begin{align}
        \sum_{p \in P} \sum_{t \in S_{T, p} \setminus \C} \epsilon_t 
        &\leq \sum_{p \in P} \left| \sum_{t \in S_{T, p} \setminus \C} \epsilon_t \right| \leq \sum_{p \in P} \left| \sum_{t \in S_{T, p}} \epsilon_t \right| \\
        &\leq \sum_{p \in P} \sqrt{\xi_T |S_{T, p}|} 
        &(\text{\cref{lem:noiseconc}})\\
        &\leq \sqrt{\xi_T |P| T} 
        & (\text{Cauchy-Schwarz})
    \end{align}
    Combining the above, the cumulative regret is bounded by
    \begin{equation}
        \BR(T) \leq 2 |P| B_{p^*} + 2 \sqrt{\xi_T |P| T} 
        + 2 \sum_{t \notin \C} \sqrt{\beta_t} \sigma_{t, p^*}(x^*)
        + 2 \sum_{t \notin \C} \sqrt{\beta_t} \sigma_{t, p_t}(x_t).
    \end{equation}
    Finally, applying \cref{lem:infgainbound,cor:avginfogain}, we obtain the result
    \begin{equation}
        \BR(T) \leq 2 |P| B_{p^*} + 2 \sqrt{\xi_T |P| T} + 2 \sqrt{C T \beta_T \gamma_{T,p^*}} +2 \sqrt{C T \beta_T \gamma_{T} |P|}.
    \end{equation}
\end{proof}


\subsection{HP-GP-TS} \label{sec:proofsHPGPTS}
First, we state and prove \cref{lem:priorinfogain}.
\lemPriorInfoGain*
\begin{proof}
    First, note that $I_p(\y_T; f) = H(\y_T | p) - H(\y_T | f, p)$. As in \citet{srinivasInformationTheoretic2012}, $H(\y_T | f, p)$ is simply the entropy of $T$ zero-mean Gaussians with variance $\sigma^2$: $\frac{T}{2} \log (2\pi e \sigma^2 )$. Using the chain rule of entropy, $H(\y_T | p) = H(\y_{T-1} | p) + H(y_T | \y_{T-1}, p) = H(\y_{T-1} | p) + \frac{1}{2}\log(2\pi e (\sigma^2 + \sigma^2_{t}(x_T))$ where the second equality is due to $y_T | \y_{T-1}, p \sim \N( \mu_{T,p}(x_T), \sigma^2_{T, p} (x_T) + \sigma^2)$. The rest follows by induction.

    The second part follows from the proof of Lemma 5.4 in \cite{srinivasInformationTheoretic2012}: 
    \begin{align}
        \sigma_{t, p}^2(x_t) &= \sigma^2 (\sigma^{-2} \sigma^2_{t,p}(x_t)) \\
        &\leq \sigma^2 \frac{\sigma^{-2}}{\log (1 + \sigma^{-2})} \log (1 + \sigma^{-2} \sigma^2_{t, p}(x_t)) \\
        &\leq \frac{1}{\log(1 + \sigma^{-2})} \log(1 + \sigma^{-2}\sigma^2_{t, p}(x_t)).
    \end{align}
    Using \cref{eq:infgainposterior}, we get the desired result.
\end{proof}

Based on \cref{lem:priorinfogain}, \cref{cor:avginfogain} follows almost immediately.

\lemAvgInfoGain*
\begin{proof}
    From \cref{lem:priorinfogain}, we have for any fixed sequence of arms $x_1, \ldots, x_T \in \Xc$ and prior $p^* \in P$, that $\sumt \sigma^2_{t,p^*} \leq C I_{p^*}(\y_T; f)$. The mutual information $I_{p^*}(\y_T; f)$ given the prior $p^*$ is in turn bounded by the maximal information gain $\gamma_{T, p^*}$ for the same prior.
    Hence, 
    \begin{align}
        \E \left[ \sumt \sigma^2_{t,p^*}(x_t) \right] \leq \E \left[ C \gamma_{T,p^*} \right] = \E_{p^* \sim P_0} \left[ C \gamma_{T,p^*} \right].
    \end{align}
\end{proof}

Then, we are ready to state and prove our regret bound for HP-GP-TS.

\thmFBGPTS*
\begin{proof}
    To begin, we note that $x_t | H_t \eqd x^* | H_t$ and $p_t | H_t \eqd p^* | H_t$ since both $x_t$ and $p_t$ are sampled from their respective posteriors. Let $U_{t, p}(x) := \mu_{t, p}(x) + \sqrt{\beta_t} \sigma_{t, p}(x)$. Then, we start decomposing the instant regret into two terms:
    \begin{align}
        \BR(T) &= \sumt \E\left[ f(x^*) - f(x_t) \right] \\
        &= \sumt \E\left[ f(x^*) - U_{t, p^*}(x^*) + U_{t, p_t}(x_t) - f(x_t) \right]
        &(x_t,p_t | H_t \eqd x^*,p^* | H_t) \\
        &= \underbrace{ \sumt \E\left[ f(x^*) - U_{t, p^*}(x^*)\right]}_{(1)} 
        + \underbrace{ \sumt \E\left[ U_{t, p_t}(x_t) - f(x_t) \right]}_{(2)}.
    \end{align}
    We begin by bounding term $(1)$,
    \begin{align}
        (1) &= \sumt \E \left[ f(x^*) - \mu_{t, p^*}(x^*) - \sqrt{\beta_t} \sigma_{t, p^*}(x^*) \right] \\
        &\leq \sumt \E \left[ \left[ f(x^*) - \mu_{t, p^*}(x^*) - \sqrt{\beta_t} \sigma_{t, p^*}(x^*) \right]_{+} \right]
        &\left( [\cdot]_{+} := \max(\cdot, 0 ) \right) \\
        &\leq \sumt \sum_{x \in \Xc} \E \left[ \left[ f(x) - \mu_{t, p^*}(x) - \sqrt{\beta_t} \sigma_{t, p^*}(x) \right]_{+} \right]
        &(x^* \in \Xc, [\cdot]_+ \geq 0) \\
        &\leq \sumt \sum_{x \in \Xc} \E_{p^*, H_t} \left[ \E_{t} \left[ \left[ f(x^*) - \mu_{t, p^*}(x^*) - \sqrt{\beta_t} \sigma_{t, p^*}(x^*) \right]_+ \big|\, p^*, H_t \right] \right] 
        &(\text{Tower rule})
    \end{align}
    Recall that for $Z \sim \N(\mu, \sigma)$ with $\mu \leq 0$, $\E[ [Z]_+ ] = \frac{\sigma}{\sqrt{2\pi}} \exp\left( \frac{-\mu^2}{2 \sigma^2} \right)$. 
    In our case, note that $f | p^*, H_t \sim \N(\mu_{t, p^*}, k_{t, p^*})$ and $\mu_{t,p^*}(x) - \sqrt{\beta_t} \sigma_{t, p^*}(x)$ is deterministic given $p^*, H_t$. Hence,
    \begin{align}
        (1) &\leq \sumt \sum_{x \in \Xc} \E_{p^*, H_t} \left[ \frac{\sigma_{t, p^*(x)}}{\sqrt{2\pi}} \exp\left( \frac{- \beta_t}{2} \right) \right] \\
        &\leq \sumt \sum_{x \in \Xc} \E_{p^*, H_t} \left[ \frac{1}{\sqrt{2\pi}} \exp \left( - \frac{\beta_t}{2} \right) \right] 
        &(\sigma_{t, p^*}(x) \leq \sigma_{0, p^*}(x) \leq 1) \\
        &= \sumt \sum_{x \in \Xc} \frac{1}{\sqrt{2\pi}} \exp( -\beta_t / 2) \\
        &\leq \sumt \frac{1}{t^2} \leq \frac{\pi^2}{6}.  
        &( \beta_t = 2 \log(|\Xc| t^2 / \sqrt{2\pi}) )
    \end{align}
    
    Next, we bound $(2)$ as follows:
    \begin{align}
        (2) &= \sumt \E \left[ U_{t, p_t} (x_t) - f(x_t) \right] \\
        &= \sumt \E_{H_t} \left[ \E_t \left[ U_{t, p_t} (x_t) - f(x_t) | H_t \right] \right] 
        &(\text{Tower rule})\\
        &= \sumt \E_{H_t} \left[ \E_t \left[ U_{t, p_t} (x_t) - \mu_{t, p_t}(x_t) | H_t \right] \right] 
        &( \E[f(x_t) | H_t] = \E[ \mu_{t, p_t}(x_t) | H_t ]) \\
        &= \sumt \E_{H_t} [ \E_t [ \sqrt{\beta_t} \sigma_{t, p_t} (x_t) | H_t ]] 
        &( U_{t, p_t}(\cdot) = \mu_{t, p_t}(\cdot) + \sqrt{\beta_t}\sigma_{t,p_t}(\cdot) )\\
        &= \sumt \E_{H_t} [ \E_t [ \sqrt{\beta_t} \sigma_{t, p^*} (x_t) | H_t ]]
        &( p_t | H_t \eqd p^* | H_t ) \\
        &= \sumt \E [ \sqrt{\beta_t} \sigma_{t, p^*} (x_t) ] 
    \end{align}
    Continuing, 
    \begin{align}
        (2) &= \E \left[ \sumt \sqrt{\beta_t} \sigma_{t, p^*} (x_t) \right] \\
        &\leq \E \left[ \sqrt{\sumt \beta_t \sumt \sigma^2_{t, p^*} (x_t) }\right]  
        &(\text{Cauchy-Schwarz}) \\
        &\leq \sqrt{\sumt \beta_t} \E \left[ \sqrt{\sumt \sigma^2_{t, p^*} (x_t) }  \right] 
        &(\beta_t \text{ deterministic}) \\
        &\leq \sqrt{\sumt \beta_t} \sqrt{\E \left[ \sumt \sigma^2_{t, p^*} (x_t)  \right]} 
        &( \text{Jensen's inequality} ) \\
        &\leq \sqrt{\beta_T T} \sqrt{ C \E_{p^*}[\gamma_{T, p^*}] } 
        &( \beta_t \text{ increasing and \cref{cor:avginfogain}} )
    \end{align}
    Combining the bounds for (1) and (2), we obtain 
    \begin{equation}
        \BR(T) \leq \frac{\pi^2}{6} + \sqrt{C T \beta_t \E_{p^*}[\gamma_{T, p^*}]}.
    \end{equation}
    
\end{proof}


\section{Additional experimental details} \label{app:expdetails}
In this section, we provide some additional experimental details and results. First, we include the full set of confusion matrices for the synthetic lengthscale and hill experiments in \cref{fig:confusionmatrices}. In the lengthscale experiments, we observe that PE-GP-UCB and -TS oversample the shortest lengthscale. This is similar to the kernel experiment where the Matérn 3/2 kernel was also oversampled. However, we see that HP-GP-TS and MAP GP-TS do not suffer from this optimistic bias. In the hill experiment, we see that PE-GP-UCB almost never selects Hill 0, i.e. the mean function with only smaller hills. PE-GP-TS selects the prior almost uniformly whilst HP-GP-TS and MAP-GP-TS select the correct mean function in a majority of the time steps.

In \cref{fig:realworldmedianregret}, we visualize the median cumulative regret on the real-world data experiments. 

In the Intel experiment, we removed one outlier seed. All methods had a final cumulative regret around 6000°C, note that the average for the worst performing model across the other seeds was $\approx 150$°C. The outlier is shown \cref{fig:outlier}. We can see that one of the sensors display very high temperatures compared to all other sensors, which is why all methods performed poorly on this seed. It should be noted that many of the sensors in the Intel data logged degrees above 100°C after a certain time - likely due to sensor failure rather boiling temperatures in an office environment. Also note that these days were excluded from both our training and test data. The outlier could be indication that this particular sensor was starting to fail earlier than others.

\begin{figure}
    \centering
    \begin{subfigure}[c]{0.5\linewidth}
    \includegraphics[width=\linewidth]{confmat_lengthscales.pdf}
    \caption{Lengthscale experiment}
    \end{subfigure}%
    \begin{subfigure}[c]{0.5\linewidth}
    \includegraphics[width=\linewidth]{confmat_pwlinear.pdf}
    \caption{Hill experiment}
    \end{subfigure}
    \caption{Confusion matrices for the true prior $p^*$ and $p_t$ across all time steps of the synthetic lengthscale and hill experiments.}
    \label{fig:confusionmatrices}
\end{figure}


\begin{figure}[htb]
    \centering
    \includegraphics[]{intelpemsmedianregret.pdf}
    \caption{Median cumulative regret on Intel temperature data (top) and PeMS data (bottom). Errorbars correspond to first and last decile.}
    \label{fig:realworldmedianregret}
\end{figure}

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.5\linewidth]{fbgpts-74.log.pdf}
    \caption{Removed sample from the test data in the Intel experiment. One of the sensors display very high temperatures but }
    \label{fig:outlier}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}

