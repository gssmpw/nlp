\section{Related Work}
\label{sec:appendix_related_work}


Knowledge editing techniques for large language models (LLMs) primarily fall into two paradigms: non-parametric approaches that preserve original parameters and parametric methods that directly modify model weights. Parametric approaches, while effective for targeted updates, often introduce uncontrolled parameter perturbations that adversely affect unrelated knowledge --- a challenge addressed through various constraint mechanisms.
The parametric category features two dominant subclasses: One is ``Meta-Learning Based Methods'', such as MEND \cite{DBLP:conf/iclr/MitchellLBFM22} and MALMEN \cite{DBLP:conf/iclr/TanZF24} which train meta-networks using carefully designed datasets containing both unrelated knowledge samples and paraphrased sentences, aiming to enhance generalization while minimizing collateral damage. Another is Locate-and-Edit Methods, which includes techniques such as Knowledge Neuron (KN) \cite{DBLP:conf/acl/DaiDHSCW22}, identify critical knowledge storage locations before executing precise edits. ROME \cite{DBLP:conf/nips/MengBAB22} extends this by incorporating knowledge preservation terms in its optimization objective to maintain model integrity.

Our work builds upon MEMIT, a state-of-the-art locate-and-edit approach that enables batch knowledge editing through MLP layer modifications. Building on MEMIT, many recent methods have made modifications to parameter update methods during editing or to the architecture and location of the edits. PMET \cite{DBLP:conf/aaai/Li0SYMY24} incorporates the output of the attention layer in the calculation of parameter updates. AlphaEdit \cite{fang_alphaedit_2024} improves upon MEMIT's parameter matrix update method by projecting the update matrix into the null space of the original knowledge to mitigate interference with unrelated knowledge. UNKE \cite{deng_unke_2024} extends structured knowledge editing to unstructured editing.