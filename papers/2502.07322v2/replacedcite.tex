\section{Related Work}
\label{sec:appendix_related_work}


Knowledge editing techniques for large language models (LLMs) primarily fall into two paradigms: non-parametric approaches that preserve original parameters and parametric methods that directly modify model weights. Parametric approaches, while effective for targeted updates, often introduce uncontrolled parameter perturbations that adversely affect unrelated knowledge --- a challenge addressed through various constraint mechanisms.
The parametric category features two dominant subclasses: One is ``Meta-Learning Based Methods'', such as MEND ____ and MALMEN ____ which train meta-networks using carefully designed datasets containing both unrelated knowledge samples and paraphrased sentences, aiming to enhance generalization while minimizing collateral damage. Another is Locate-and-Edit Methods, which includes techniques such as Knowledge Neuron (KN) ____, identify critical knowledge storage locations before executing precise edits. ROME ____ extends this by incorporating knowledge preservation terms in its optimization objective to maintain model integrity.

Our work builds upon MEMIT, a state-of-the-art locate-and-edit approach that enables batch knowledge editing through MLP layer modifications. Building on MEMIT, many recent methods have made modifications to parameter update methods during editing or to the architecture and location of the edits. PMET ____ incorporates the output of the attention layer in the calculation of parameter updates. AlphaEdit ____ improves upon MEMIT's parameter matrix update method by projecting the update matrix into the null space of the original knowledge to mitigate interference with unrelated knowledge. UNKE ____ extends structured knowledge editing to unstructured editing.