% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
% \usepackage[review]{acl}
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

%%%%%%%%%% My packages adding
% \usepackage{booktabs} 
\usepackage{xspace}
\usepackage{amsmath}
\usepackage{adjustbox,booktabs,multirow}
% \usepackage{lmodern, babel}

\newcommand{\methodname}{MEMIT-Merge\xspace}
\newcommand{\eg}{\emph{e.g}\xspace}
\newcommand{\ie}{\emph{i.e}\xspace}
\newcommand{\vs}{\emph{v.s.}\xspace}
\newcommand{\etc}{\emph{etc}\xspace}

\title{MEMIT-Merge: Addressing MEMIT's Key-Value Conflicts in Same-Subject Batch Editing for LLMs}


\author{Zilu Dong \and  Xiangqing Shen \and Rui Xia\\
        School of Computer Science and Engineering  \\ 
        Nanjing University of Science and Technology, China \\
        \{zldong, xiangqing.shen, rxia\}@njust.edu.cn}

\begin{document}
\maketitle
\begin{abstract}
    As large language models continue to scale up, knowledge editing techniques that modify models' internal knowledge without full retraining have gained significant attention. 
    MEMIT, a prominent batch editing algorithm, stands out for its capability to perform mass knowledge modifications.
    However, we uncover a critical limitation that MEMIT's editing efficacy significantly deteriorates when processing batches containing multiple edits sharing the same subject.
    Our analysis reveals that the root cause lies in
MEMIT's key value modeling framework: When multiple facts with the same subject in a batch are modeled through MEMIT's key value mechanism, identical keys (derived from the shared subject) are forced to represent different values (corresponding to different knowledge), resulting in updates conflicts during editing.
    Addressing this issue, we propose MEMIT-Merge, an enhanced approach that merges value computation processes for facts sharing the same subject, effectively resolving the performance degradation in same-subject batch editing scenarios. 
    Experimental results demonstrate that when MEMIT's edit success rate drops to around 50\% at larger batch sizes, \methodname maintains a success rate exceeding 90\%, showcasing remarkable robustness to subject entity collisions.

\end{abstract}
\section{Introduction}
\label{sec:introduction}

Knowledge editing techniques aim to update models' internal knowledge without retraining.
As large language models (LLMs) continue to scale up, the prohibitive cost of full model retraining has made knowledge editing increasingly crucial in this domain. Among prevalent editing algorithms, a class of algorithms, termed ``Locate and Edit'' methods by \citet{zhang_comprehensive_2024}, operates on the fundamental assumption that specific knowledge representations are localized within particular regions of the model's parameter space, enabling targeted modifications through precise manipulation of these identified regions.
MEMIT \cite{DBLP:conf/iclr/MengSABB23}, one of the most prominent algorithms in this class, has gained significant attention \cite{DBLP:conf/aaai/Li0SYMY24, fang_alphaedit_2024, DBLP:conf/emnlp/GuptaSA24}.
MEMIT inherits the core architectural feature of ROME \cite{DBLP:conf/nips/MengBAB22}, which localizes knowledge to specific layers and modifies the output linear layer of MLP modules to update knowledge. 
The distinctive advancement of MEMIT lies in its capability to perform batch-wise mass knowledge editing, enabling simultaneous modification of multiple knowledge instances within a single batch.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{imgs/same_diff_editsuccess.pdf}
                   \caption{The edit success rate of the MEMIT method on same-subject and distinct-subject datasets, showing the changes with varying batch sizes. A significant decline is observed when the subjects are the same. }
    \label{fig:same_diff_editsuccess}
\end{figure}

However, our investigation reveals a critical limitation in MEMIT: When handling batches with multiple edits that share the same subject (such as ``John Smith now plays basketball.'' and ``John Smith comes from England.'' where both statements have the subject ``John Smith''), the method will exhibit significant performance degradation. In contrast, edits with different subjects (such as ``Jack Johnson now plays basketball'' and ``Paul Morand comes from China'' with subjects `Jack Johnson'' and ``Paul Morand'', respectively) maintain stable efficacy.
To systematically demonstrate this performance degradation, we constructed two contrastive datasets comprising batches with identical subjects versus fully unique subjects, named distinct-subject and same-subject, respectively. The experimental results are in Fig.~\ref{fig:same_diff_editsuccess}, where the vertical axis represents efficacy (which means the editing success rate) and the horizontal axis indicates the batch size per edit. The results reveal that MEMIT maintains a high success rate as batch size increases when editing distinct-subject cases, but exhibits significant performance degradation for same-subject cases. More detailed experimental settings can be found in Sec.~\ref{subsec:same_diff_edit}.
Batch editing of same-subject knowledge is essential in practical applications. For instance, updating multiple attributes about a specific individual (such as occupation, nationality, and affiliations) inherently requires concurrent editing of same-subject facts.


Our analysis reveals that the root cause lies in MEMIT's key-value modeling framework: when handling edits involving the same subject, identical keys (derived from shared subject representations) are forced to map to divergent values (corresponding to distinct knowledge updates). 
This results in inconsistencies during batch editing.
Specifically, MEMIT treats each knowledge instance as a key-value pair, where the input vector to the MLP's output linear layer serves as the key, and the desired output becomes the value.
\footnote{Note that the key-value here refers to the hidden state and output within the MLP module as described by \citet{DBLP:conf/nips/MengBAB22}, rather than the query, key and value in the attention module.} 
The algorithm modifies the MLP layer by computing target values based on new knowledge objects and updating weights to map existing keys to these new values.
However, MEMIT's per-fact value computation creates inherent conflicts: within a single batch, multiple entries sharing identical keys (from same-subject cases) require different value mappings. 
As MLP output linear layers cannot produce multiple outputs for identical inputs, these conflicting updates lead to the performance degradation. Furthermore, we observe that the closer the keys are to each other, the more significant the performance degradation.

To resolve this fundamental conflict, we propose \methodname, an enhanced variant of MEMIT. The core insight stems from addressing MEMIT's critical flaw: independent value computation for same-key entries. Our solution enforces value consistency by merging multiple knowledge entries sharing identical keys, \ie, the same-subject cases into a unified value representation through constrained optimization.
To validate the effectiveness of our approach, we conducted experiments using the aforementioned same-subject and distinct-subject datasets. The results demonstrate that on same-subject data, \methodname consistently outperforms the standard MEMIT as the batch size increases. 
Notably, when MEMIT's success rate drops to around 50\% at larger batch sizes, \methodname maintains a success rate exceeding 90\%. While on distinct-subject data, both methods exhibit comparable performance, with no statistically significant differences observed. 

\section{Related Work}
\label{sec:appendix_related_work}


Knowledge editing techniques for large language models (LLMs) primarily fall into two paradigms: non-parametric approaches that preserve original parameters and parametric methods that directly modify model weights. Parametric approaches, while effective for targeted updates, often introduce uncontrolled parameter perturbations that adversely affect unrelated knowledge --- a challenge addressed through various constraint mechanisms.
The parametric category features two dominant subclasses: One is ``Meta-Learning Based Methods'', such as MEND \cite{DBLP:conf/iclr/MitchellLBFM22} and MALMEN \cite{DBLP:conf/iclr/TanZF24} which train meta-networks using carefully designed datasets containing both unrelated knowledge samples and paraphrased sentences, aiming to enhance generalization while minimizing collateral damage. Another is Locate-and-Edit Methods, which includes techniques such as Knowledge Neuron (KN) \cite{DBLP:conf/acl/DaiDHSCW22}, identify critical knowledge storage locations before executing precise edits. ROME \cite{DBLP:conf/nips/MengBAB22} extends this by incorporating knowledge preservation terms in its optimization objective to maintain model integrity.

Our work builds upon MEMIT, a state-of-the-art locate-and-edit approach that enables batch knowledge editing through MLP layer modifications. Building on MEMIT, many recent methods have made modifications to parameter update methods during editing or to the architecture and location of the edits. PMET \cite{DBLP:conf/aaai/Li0SYMY24} incorporates the output of the attention layer in the calculation of parameter updates. AlphaEdit \cite{fang_alphaedit_2024} improves upon MEMIT's parameter matrix update method by projecting the update matrix into the null space of the original knowledge to mitigate interference with unrelated knowledge. UNKE \cite{deng_unke_2024} extends structured knowledge editing to unstructured editing.


\section{Problem}

\subsection{Preliminaries}
\label{preliminaries}

The MEMIT framework posits that factual knowledge is stored within the MLP layer parameters in transformer-based models. Each MLP layer in the standard architecture constitutes a two-layer neural network comprising input and output linear layers, whose parameter matrices are denoted as $W_{in}$ and $W_{out}$, respectively.
MEMIT refers to the hidden state in the middle of this two-layer neural network as the key, and the final output of the MLP as the value. The output linear layer can thus be regarded as a key-value mapping, which is the optimization target of the MEMIT method, corresponding to the parameter matrix $W_{out}$. Typically, during MEMIT editing, all knowledge to be edited can be placed in a single batch, allowing for the simultaneous editing of a large amount of knowledge.

MEMIT employs a triple $(s, r, o)$ to represent the subject, relation, and object of a piece of knowledge to be edited, and constructs a complete sentence based on these elements for the editing process. During the editing process of MEMIT, the key is determined by the subject of the sentence and the part preceding the subject. The value, on the other hand, needs to be a value that enables the model to output the object. Therefore, it is optimized backward from the object of the sentence:
%The calculation is shown in the following formula, 
\begin{equation}
\label{eq:MEMIT_v_update}
v=\arg\min_v (-\log P_v [o|(s, r)]).
\end{equation} 
%\footnote{For ease of understanding, the random prefix used for generalization in the original paper is omitted here}.
This process generates a $(k, v)$ pair representing the knowledge triplet $(s,r,o)$. The model subsequently employs the following formula to update $W_{out}$ to enable the key to map to the corresponding value:
\begin{equation}
W_{out} = W_0 +(V-W_0K)K^T(C+KK^T)^{-1}
\end{equation}
where $K$ and $V$ represent the sentences composed of the k and v values in a batch, respectively. $W_0$ denotes the original $W_out$ weights before editing, and $C$ is a constant representing the existing knowledge. In this way, the model can output the edited $o$ with $(s,r)$.

\subsection{Same subject issue in MEMIT}
\label{sec:issues in MEMIT edits}

\begin{figure*}[htbp]
    \includegraphics[width=\linewidth]{imgs/samesub.pdf}
    \caption{The architecture of MEMIT processing two same subject sentences. The left and right sides of the figure depict the processing flow of the two sentences respectively. Below, we expand the details of the MLP module to be modified, which consists of two linear layers. In MEMIT, the key is determined by the subject, resulting in identical keys on both sides. The value is optimized from the relation and object, leading to different values on each side. Consequently, the optimization target for the editable $W_{out}$ requires producing different values for the same input key.}
    \label{fig:general_process}
\end{figure*}


Normally, MEMIT is capable of maintaining its efficacy without a pronounced decline in performance when the edit batch size approaches 1,000. Moreover, the edit success rate remains relatively stable even when editing up to 10,000 knowledge triples. However, we have identified a notable issue: when the edit batch encompasses knowledge triples sharing the same subject, the editing capacity of MEMIT experiences a substantial degradation.


To verify this phenomenon, we constructed two counterfactual editing datasets. In the first dataset, the subjects of the knowledge triples are all distinct. In the second dataset, the subjects of the knowledge triples are replaced by a single, fixed subject, while all other parts of these two datasets remain identical. The details of the datasets construction are provided in App.~\ref{sec:appendix_dataset_construct}.


As illustrated in Fig.~\ref{fig:same_diff_editsuccess}, when the subjects are identical, the performance of the MEMIT method drops sharply with a batch size of only 2, and the edit success rate falls below 50\% when the batch size reaches 10. In contrast, when subjects are distinct, increasing the batch size has virtually no impact on edit success.

\section{Approach}
\label{sec:methods}


\subsection{Cause Analysis}
\label{sec:reason_analysis}

In our analysis, the degradation of editing capability caused by identical subjects is closely related to the key-value modeling of knowledge inherent in locate-and-edit class editing methods.


In the standard MEMIT, a piece of knowledge to be edited can be represented by a knowledge triplet (subject, relation, object), and a complete sentence is constructed based on this triplet for the editing process. In this paper, we use the format ``{subject}'s {relation} is {object}'' to construct the sentence. For example, the knowledge triple (John,father,Bob) is formulated into the sentence ``John's father is Bob.''.


As described in Sec.~\ref{preliminaries}, during MEMIT editing, the key is derived from the subject, while the value is determined by the object.
However, when editing multiple pieces of knowledge with the same subject but different objects in one batch, this mechanism forces the MLP to map the same key to two distinct values. As illustrated in Fig.~\ref{fig:general_process}, a given key can only produce a single fixed value through deterministic $W_{out}$. This creates a conflict when optimizing the parameter matrix, making it extremely challenging. We refer to this issue as the key collision problem. 
Consequently, when a batch contains multiple edits with the same subject, as demonstrated in Fig.~\ref{fig:same_diff_editsuccess}, the editing capability of MEMIT is significantly degraded.


To further investigate the relationship between the decline in editing capability and the distance between keys, we propose an evaluation metric: the Average Keys Distance Inside Batch (AKD). This metric is defined as the average Euclidean distance between the key values of all pairs of knowledge within a batch. It reflects the average distance between keys in the batch and is represented as

$$
AKD^{(l)}=\frac{1}{\binom{|B|}{2}}\sum_{\substack{e_1\in B\\ e_2\in B}} ||k^{(l)}_{e_1}-k^{(l)}_{e_2}||_2
$$
 
where $l$ represents the $l$-th layer,  $B$ denotes the batch of knowledge to be edited, $k^{(l)}_{e_1}$ represents the key value computed by the MLP module in the $l$-th layer for the input knowledge $e_1$. 

We compute $AKD$ for all layers of the model at the subject's last token position.
As the degree of subject variation increases across sentences, the $AKD$ value proportionally rises. Conversely, when all sentences share identical subjects, the $AKD$ value remains constant at 0.
 

We construct sentence batches using predefined templates, where batches sharing the same template exhibited similar $AKD$ values, while distinct templates yielded significantly different $AKD$ measurements. The specific templates and the corresponding values of $AKD$ are detailed in App.~\ref{sec:appendix_akd_dataset}.
For experimental validation, we select three $AKD$ groups (0, 10, 25) and conduct editing tests using Qwen2.5-1.5B-Instruct. As shown in Fig.~\ref{fig:akd}, where $AKD$ values are computed using keys from MEMIT's final editing layer, the results demonstrate an inverse relationship: lower $AKD$ values correspond to reduced editing success rates. This pattern remains consistent across other $AKD$ values, establishing a statistically significant negative correlation between $AKD$ and editing efficacy.

\begin{figure}[th]
    \centering
    \includegraphics[width=\linewidth]{imgs/akd.pdf}
    \caption{Datasets with different $AKD$ values and the results of edit efficacy. The lower the $AKD$ value, the more severe the decline in edit capability.}
    \label{fig:akd}

\end{figure}

\subsection{The MEMIT-Merge Approach}


To address the issue of a single key corresponding to multiple distinct values, we organize all the knowledge triples to be edited within a batch into multiple ``same-subject groups'' based on whether their subjects are identical. Within each group, the subjects of the knowledge triple to be edited are the same.


Compared with the optimization objective of standard MEMIT in Eq.~\ref{eq:MEMIT_v_update}, our improved version aims to optimize the knowledge triples in the same-subject group to a single value
$$v=\arg\min_v \sum_{(s, r_j, o_j)\in S} -\log P_v[o_j|(s, r_j)]$$
where $S$ represents the set of knowledge triples with the subject $s$,  $v$ is the value to be optimized in a backward manner, and $P_v$ denotes the model when the value is equal to $v$.

This approach ensures that a same-subject group gets the same value while sharing the same key, thereby significantly alleviating the decline in edit efficacy observed in standard MEMIT.


\section{Experiments}
\subsection{Dataset}
\label{subsec:dataset}


As mentioned in the Introduction section, we constructed two counterfactual knowledge editing datasets based on Wikidata, one with the same subject and the other with distinct subjects. The dataset named ``same-subject'' dataset, contains 100 editing knowledge triples, all with the subject John Smith. The other dataset, named ``distinct-subject'', has knowledge triples with subjects unique to each other, while the relations and objects remain consistent with those in the ``same-subject'' dataset. For specific details on the construction of the datasets, refer to App.~\ref{sec:appendix_dataset_construct}.

In terms of evaluation metrics, we refer to the metrics used by \citet{DBLP:conf/iclr/MengSABB23}, namely Efficacy, Paraphrase, and Specificity. Efficacy is the edit success rate, which measures the probability that the edited model produces the correct answer to the sentences used for editing. Paraphrase is measured in the same way as Efficacy, but it uses paraphrased versions of the sentences used for editing. Specificity measures the probability that facts unrelated to the edit remain consistent before and after the edit.

Although we constructed our own datasets due to the lack of specialized data on the same subject in the past, addressing same-subject issue has a practical necessity. Editing multiple attributes of an entity (\eg, updating a person’s profile) is a highly realistic demand. Furthermore, changing certain information about an entity sometimes requires a chain of changes to other information about that entity. All of these make the same subject scenarios essential for practical applications.

\subsection{Experimental Setup}
\label{subsec:setup}

We conducted experiments on three models with different architectures: Qwen2.5-1.5B-Instruct \cite{qwen_qwen25_2025}, GPT-J-6B \cite{gpt-j}, and Llama-3-8B-Instruct \cite{llama3modelcard}.

For baselines, we selected MEMIT and an improved version of MEMIT, PMET \cite{DBLP:conf/aaai/Li0SYMY24}, as our baselines. The layers to be edited for these methods were determined based on the parameter settings in the Easyedit framework \cite{wang_easyedit_2024}. Specifically, we edited layers 4 to 8 for the Qwen model and layers 3 to 8 for the GPT-J model. The token position for editing was consistent with the original papers, which is selecting the subject last token.
In addition to the MEMIT-based methods, we also included FT-L \cite{zhu_modifying_2020}, which was used for comparison in the ROME paper, as another baseline to verify that the same-subject issue exists only in methods with the MEMIT-based architecture. For FT-L, we edited layer 21 in the GPT-J and Llama model, and layer 15 in the Qwen model.

\subsection{Results when Batch Size is 100 }
\label{subsec:same_diff_edit}


\begin{table*}[th]
\centering
\small
\begin{tabular}{@{}cccccc@{}}
\toprule
Model                                  & Dataset                       & Method & Efficiency & Parapharse & Specificity \\ \midrule
\multirow{8}{*}{Qwen2.5-1.5B-Instruct} & \multirow{4}{*}{same-subject} & FT          & 0.26       & 0.23       & 0.95        \\
 &                                   & MEMIT  & 0.30 & 0.24 & 1.00 \\
 &                                   & PMET   & 0.23 & 0.17 & 0.99 \\
 &                                   & MEMIT-Merge & 0.55 & 0.36 & 0.99 \\ \cmidrule(l){2-6} 
 & \multirow{4}{*}{distinct-subject} & FT     & 0.23 & 0.21 & 0.99 \\
 &                                   & MEMIT  & 1.00 & 0.77 & 0.90 \\
 &                                   & PMET   & 0.51 & 0.40 & 0.85 \\
 &                                   & MEMIT-Merge & 1.00 & 0.77 & 0.90 \\ \midrule
\multirow{8}{*}{GPT-J-6B}               & \multirow{4}{*}{same-subject} & FT          & 0.52       & 0.19       & 0.23        \\
 &                                   & MEMIT  & 0.27 & 0.21 & 1.00 \\
 &                                   & PMET   & 0.26 & 0.21 & 0.98 \\
 &                                   & MEMIT-Merge & 0.51 & 0.32 & 1.00 \\ \cmidrule(l){2-6} 
 & \multirow{4}{*}{distinct-subject} & FT     & 0.47 & 0.28 & 0.22 \\
 &                                   & MEMIT  & 1.00 & 0.77 & 0.93 \\
 &                                   & PMET   & 0.25 & 0.25 & 0.99 \\
 &                                   & MEMIT-Merge & 1.00 & 0.77 & 0.93 \\ \midrule
\multirow{8}{*}{Llama-3-8B-Instruct}  & \multirow{4}{*}{same-subject} & FT          & 0.67       & 0.47       & 0.27        \\
 &                                   & MEMIT  & 0.38 & 0.29 & 0.98 \\
 &                                   & PMET   & 0.23 & 0.21 & 0.98 \\
 &                                   & MEMIT-Merge & 0.71 & 0.44 & 0.98 \\ \cmidrule(l){2-6} 
 & \multirow{4}{*}{distinct-subject} & FT     & 0.73 & 0.58 & 0.24 \\
 &                                   & MEMIT  & 0.99 & 0.91 & 0.82 \\
 &                                   & PMET   & 0.46 & 0.46 & 0.92 \\
 &                                   & MEMIT-Merge & 1.00 & 0.91 & 0.81 \\ \midrule
\multirow{8}{*}{Qwen2.5-7B-Instruct}   & \multirow{4}{*}{same-subject} & FT          & 0.28       & 0.23       & 0.99        \\
 &                                   & MEMIT  & 0.31 & 0.25 & 1.00 \\
 &                                   & PMET   & 0.23 & 0.18 & 0.99 \\
 &                                   & MEMIT-Merge & 0.67 & 0.43 & 0.99 \\ \cmidrule(l){2-6} 
 & \multirow{4}{*}{distinct-subject} & FT     & 0.23 & 0.22 & 0.98 \\
 &                                   & MEMIT  & 0.99 & 0.84 & 0.91 \\
 &                                   & PMET   & 0.52 & 0.47 & 0.84 \\
 &                                   & MEMIT-Merge & 1.00 & 0.86 & 0.90 \\ \bottomrule
\end{tabular}
\caption{The complete results of the four editing methods—MEMIT, \methodname, PMET, and FT-L—on the same-subject and distinct-subject datasets at a batch size of 100. All experimental results were obtained by re-running each editing method on our dataset.}
\label{table:batch100_result}
\end{table*}


We first compared the edit success rates of standard MEMIT, PMET, \methodname, and FT-L on the two datasets across several models. \footnote{The results for all baselines were obtained by running the code from the Easyedit framework on our datasets.}

Tab.~\ref{table:batch100_result} presents the complete results of several editing methods on the same subject and distinct-subject datasets at a batch size of 100. Compared with the standard MEMTI, it can be observed that our method achieves an improvement in the paraphrase metric. This improvement is likely attributable to the originally low edit success rate, and by enhancing the edit success rate, we naturally improved the paraphrase performance.
Regarding specificity, it is noticeable that standard MEMIT exhibits an anomalously high specificity when the subjects are the same. However, comparing this with the results from the distinct-subject dataset reveals that specificity is actually low under normal editing conditions. Therefore, we infer that the abnormally high specificity is due to the low edit success rate, which indicates a minimal impact on the original model. In contrast, our \methodname has a higher edit success rate, resulting in a specificity that is comparable to the distinct subject scenario and the FT method. Thus, \methodname does not negatively affect specificity.

Compared to FT-L, it can be observed that the FT-L results do not decrease when subjects are the same. This situation is consistent with our analysis in Sec.~\ref{sec:reason_analysis}, which indicates that the decline in editing performance under the same subject is a unique issue of the MEMIT editing method. Similarly, the results of PMET also show a decline in efficacy only when the subjects are the same. The ability of \methodname to successfully mitigate the decline in editing performance by merging multiple edits when computing the value for the same key further supports the conclusion that key collision is the cause of this problem.

\begin{figure}[th]
    \centering
    \includegraphics[width=\linewidth]{imgs/MEMIT-Merge_samediff.pdf}
    \caption{The results of \methodname and MEMIT methods on same-subject and distinct-subject datasets using the GPT-J-6B model, showing the changes with varying batch sizes. \methodname is capable of significantly alleviating the decline in editing performance under the same-subject condition.}
    \label{fig:kmemit_samediff}
\end{figure}

\subsection{Results with Varying Batch Sizes}

As can be seen in Fig.~\ref{fig:kmemit_samediff}, when the subjects of the editing knowledge in the edit batch are the same, the standard edit success rate plummets at a batch size of 2, whereas \methodname is able to maintain a much higher success rate, with a significantly smaller decline compared to MEMIT. This also confirms the effectiveness of our method.

\begin{figure}[th]
    \centering
    \includegraphics[width=\linewidth]{imgs/qwen_llama_samediff.pdf}
    \caption{The results of \methodname and MEMIT methods on same-subject and distinct-subject datasets using the Qwen2.5-7B-Instruct and Llama-3-8B-Instruct.}
    \label{fig:qwen_llama_samediff}
\end{figure}

In the case of distinct subjects, the editing capability of both MEMIT and \methodname does not exhibit a significant decline even at a batch size of 100, which is consistent with our previous analysis.
The results of other editing methods are given in App.~\ref{sec:appendix_varyingbatchsize}.



Additionally, the experimental results for Qwen2.5-1.5B-Instruct and Llama-3-8B-Instruct, two models with different architectures, as shown in Fig.~\ref{fig:qwen_llama_samediff}, demonstrate that the same phenomenon observed in the GPT-J model also exists in these models. Moreover, \methodname is equally capable of significantly mitigating the performance degradation of standard MEMIT under the same-subject condition. Therefore, it can be concluded that this phenomenon is universally present across different model architectures, and our method is applicable to various model structures.


\section{Conclusion}


This paper identifies the issue of significant performance degradation in MEMIT when a batch contains knowledge sharing the same subject during batch editing. We analyze the reason based on the MEMIT framework, and develop the metric $AKD$ to further analyze the cause of this phenomenon. This study identifies a critical flaw in MEMIT's batch editing: parameter update conflicts arising from identical keys requiring divergent values in same-subject scenarios. Our proposed MEMIT-Merge resolves this through key-wise value merging, significantly improving same-subject editing success while maintaining original performance on distinct-subject cases. These findings advance mass-editing techniques for evolving LLM knowledge bases.


% Entries for the entire Anthology,  followed by custom entries
\bibliography{references,  anthology, custom}


\newpage

\appendix



\section{Details of Constructing Same-Subject and Distinct-Subject Data}
\label{sec:appendix_dataset_construct}

Our dataset construction is based on Wikidata. First, we retrieve all relations and properties associated with human subject entities from Wikidata. Then, we manually filter the relations, removing those that are less commonly used, such as ID and Wikidata categories. Finally, we obtain 100 relations.

Subsequently, we select a number of individuals from Wikidata and query their corresponding objects for the knowledge triples composed of these relations. Finally, we retain only one knowledge triple for each relation, thereby obtaining 100 knowledge triples, formatted as (subject, relation, object).

We then select another 100 distinct names from Wikidata and replace the subject entities in the previously obtained 100 knowledge triples with these new names, thereby creating the distinct-subject dataset. Conversely, we replace the subject entities in the 100 knowledge triples with a single, identical name to create the same-subject dataset.

Using the template ``{subject}'s {relation} is {object},'' we construct natural language sentences from these knowledge triples, which form the edit sentences in the dataset. For example, a knowledge triple in the same-subject dataset is (John Smith, doctoral advisor, Dennis W. Sciama), which is formulated into the natural sentence John Smith's doctoral advisor is Dennis W. Sciama. In the distinct-subject dataset, the corresponding knowledge triple with the same relation and object is (Paul Morand, doctoral advisor, Dennis W. Sciama), which is formulated into the natural sentence Paul Morand's doctoral advisor is Dennis W. Sciama.

Subsequently, following the dataset metrics in \citet{DBLP:conf/nips/MengBAB22}, we add two types of questions: specificity and paraphrase. For paraphrase questions, we use the same knowledge triples as the edit sentences, but with a different template format: ``The name of the {relation} of {subject} is {object}.''. For specificity, there are two types of questions. One is completely unrelated knowledge, for which we use the prompt ``The capital city of America is''. The other type has the same relation as the edited knowledge but a different subject. For example, if the edited knowledge is (John, father, Bob), a specificity question could be (Paul, father, Eugène).

\section{Diverse $AKD$ Dataset}
\label{sec:appendix_akd_dataset}

\begin{table}[th]
\adjustbox{max width=\linewidth}{%
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{dataset} & \textbf{formatting template} & \textbf{$AKD$} \\ \midrule
same-subject & \{subject\}'s \{relation\} is \{object\}                  & 0.0  \\
distinct-subject & \{subject\}'s \{relation\} is \{object\}                  & 25.8 \\
same-subject & The name of the \{relation\} of \{subject\} is \{object\} & 10.5 \\
distinct-subject & The name of the \{relation\} of \{subject\} is \{object\} & 26.2 \\ \bottomrule
\end{tabular}}
\caption{The average $AKD$ values obtained using different data and templates.}
\label{table:data_format_akd}
\end{table}

The construction of datasets with three distinct $AKD$ values, where the keys within each dataset have a relatively consistent distance between each other.



We utilize the knowledge triples from the same-subject and distinct-subject datasets collected in Sec.~\ref{sec:appendix_dataset_construct} to construct data using different natural language sentence templates. The two templates we employ are ``{subject}'s {relation} is {object}'' and ``The name of the {relation} of {subject} is {object}''.

Tab.~\ref{table:data_format_akd} presents the average $AKD$ values obtained using different data and templates with the Qwen2.5-1.5B-Instruct model. We selected several datasets with distinct $AKD$ values. Since these datasets have consistent internal templates, the keys of the multiple knowledge triples within them are relatively uniform and close in distance. Therefore, when performing batch editing on these datasets, they can be used to study the correlation between efficacy and $AKD$.

\begin{figure}[th]
    \centering
    \includegraphics[width=\linewidth]{imgs/qwen_same_more.pdf}
    \caption{Editing same-subject dataset using Qwen2.5-1.5B-Instruct with four editing methods.}
    \label{fig:qwen_same_more}
\end{figure}

\begin{figure}[th]
    \centering
    \includegraphics[width=\linewidth]{imgs/llama_same_more.pdf}
    \caption{Editing same-subject dataset using Llama-3-8B-Instruct with four editing methods.}
    \label{fig:llama_same_more}
\end{figure}


\section{More Results with Varying Batch Sizes}
\label{sec:appendix_varyingbatchsize}


Here in Fig.~\ref{fig:qwen_same_more} and Fig.~\ref{fig:llama_same_more} we demonstrate some more results about editing same subject batch with varying batch sizes.

It shows clearly that MEMIT-based methods suffers from same subject issue, while methods like FT do not.




\end{document}
