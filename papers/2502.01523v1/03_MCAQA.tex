% 把数据集和评估指标描述清楚,数据集部分要求把SOP描述清楚,并且衔接过渡部分要讲清楚

% 讲的时候把 limitation去掉,这部分不讲,这些应该放到最后面conclusion讲解
% 先讲condition 定义 ,再讲数据集的组成,记得把condition重复的去掉,然后校对表格的Feature


In this section, we present our dataset and its construction process. We first define the concept of ``condition'' and then provide a comprehensive overview of the dataset. To position our work in the broader context, we compare our dataset with existing ones in the field. We discuss both the strengths and limitations of our dataset to provide a balanced assessment of its utility.

\subsection{Define ``Condition''}
In RAG systems, a \textbf{condition} refers to \textit{a specific context or circumstance that determines the validity or applicability of an answer}. We formally define a condition as \textbf{a set of contextual constraints that must be satisfied for an answer to be considered correct within its particular scope}. The need for conditions arises when users pose questions that yield multiple valid answers \cite{qian2024tell}. Without handling these multiple answers, the system may present conflicting or incomplete information, leading to user confusion. Simply choosing one interpretation arbitrarily would risk providing misleading or contextually inappropriate responses. Consider the question ``Who was the king of England in 1688?'' This query yields multiple valid answers due to the political transition during that year. James II was king until December 1688, under the condition of the period before the Glorious Revolution, while William III became king in December 1688, under the condition of the period after the Glorious Revolution and his acceptance of the Declaration of Rights. By explicitly identifying and presenting these conditions, the system enables users to understand why multiple answers exist, select the most appropriate answer based on their specific context, and refine their queries with additional constraints if needed.

\subsection{Dataset composition and structure}
%In this section, we introduce Conditional Ambiguous Question Answering (CondAmbigQA), a retrieval-based dataset designed to address real-world query ambiguity. 
We identify conditions from retrieved documents, which are obtained by using standard retrieval procedures, i.e., chunking, embedding, and retrieval based on Wikipedia dataset \cite{douze2024faiss}. This step simulates a realistic retrieval-and-generation scenario of RAG. The retrieval results provide a scope of condition exploration and identification and direct evidence to support answer generation. We include the retrieval results in the dataset to assure the trustworthiness and reproducibility of annotation as well as the evaluation results. The dataset comprises 200 annotated instances, each structured to capture how different contexts lead to diverse valid answers.
Each instance in CondAmbigQA contains three essential components: a user query, retrieved document fragments, and a list of condition-answer-citation entries. Formally, we represent each instance as:
\begin{equation}
\small
\begin{split}
        \texttt{Query} \vert \{\texttt{RetrievalDocs}\}: \{ &(\texttt{Condition}_1,\texttt{Answer}_1, \{\texttt{Citation}_1\}),\\&(\texttt{Condition}_2,\texttt{Answer}_2, \{\texttt{Citation}_2\}),...\}.
\end{split}
\end{equation}
This structure represents a significant advancement over traditional datasets that typically contain only simple query-answer pairs or single intermediate attributes \cite{lin-etal-2022-truthfulqa}. By incorporating retrieved documents and explicit conditions, our dataset enables systems to not only provide answers but also explain the contexts that make each answer valid. We provide a detailed example in Appendix~\ref{appendix_label} .

%为什么不用人工标注？人工标注的缺点是什么？
\subsection{Annotation process}

Our annotation process aims to create a high-quality dataset that captures ambiguous questions with their corresponding conditions, answers, and citations. Traditional manual annotation presents several challenges in achieving this goal: the process of interpreting ambiguous queries and identifying disambiguating conditions is particularly time-consuming, as it requires annotators to thoroughly analyze context and potential meanings. Additionally, inconsistencies in annotation quality often stem from the lack of universally standardized methodologies, making it difficult to ensure uniformity across large datasets. Human annotators often introduce variability in interpretations and may unintentionally inject default knowledge biases \cite{geva2019we}. On the other hand, synthetic datasets composed using LLMs, while fast and cost-effective, face fundamental limitations in the generation process itself. LLMs often struggle with accurately capturing nuanced contexts and handling complex logical reasoning tasks, leading to errors that compromise data quality. Additionally, these datasets inherit inherent biases from the models, which may amplify pre-existing flaws in the training data. %Although post-generation quality control processes can mitigate some of these issues, they remain labor-intensive and are prone to overlooking subtle yet critical errors, similar to the challenges faced in manual annotation.

Our annotation process begins by filtering ambiguous questions from the ALCE-ASQA dataset, following templates provided in Appendix~\ref{appendix_labelb}. In the first screening round, ALCE questions and their long-form answers were input into GPT-4o \cite{achiam2023gpt,openai2024gpt4} to assess genuine ambiguity. This filtering addressed issues in prior studies where many ambiguous questions lacked meaningful differences in answers. After screening, 200 questions were retained, and the Faiss library retrieved the top 20 relevant segments for each question from Wikipedia. We use the embedding model BAAI/bge-base-en-v1.5 \cite{bge_embedding} to encode the queries and the Wikipedia dataset sourced from the Hugging Face repository \texttt{WhereIsAI/bge\_wikipedia-data-en}.

Annotation involves iterative collaboration between LLMs and human. LLMs generated standardized annotations using predefined prompts, while human reviewers evaluate and refine outputs. This process minimized LLM biases and human subjectivity, producing consistent and accurate annotations. By grounding answers in evidence and standardizing outputs, this pipeline ensures high-quality data to support further research while demonstrating the potential of human-LLM collaboration in ambiguity analysis.



\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{SOP2.pdf}
\caption{The flowchart of CondAmbigQA dataset construction process.}
\label{alg:annotation_process}
\end{figure}

Following the LLM generation phase, we implemented a collaborative calibration process where domain experts (calibrators) reviewed the same dataset to ensure logical consistency, factual accuracy, and completeness of condition-answer pairs. Each calibrator independently assessed the alignment between conditions and answers, verified the reliability of cited evidence, and ensured the completeness of the conditions. Any disagreements were resolved through discussions among the calibrators, with state-of-the-art LLMs such as GPT-4o providing additional support to refine the results. Multiple rounds of calibration and alignment are conducted to ensure consistency and reliability throughout the process, as detailed in Figure~\ref{alg:annotation_process}.
To maintain reproducibility, we have included all base prompts used for both LLM generation and human annotation guidelines in the appendix. These prompts specifically guide annotators to verify that: (1) each condition is supported by the retrieved fragments, (2) answers accurately reflect the information in the cited sources, and (3) the set of conditions covers the main interpretations present in the retrieved documents.


\textbf{Data Availability}  CondAmbigQA dataset has been made publicly available on the Hugging Face platform\footnote{\url{https://huggingface.co/datasets/Apocalypse-AGI-DAO/CondAmbigQA}}.

\subsection{Unique features and advantages}

The retrieval-based design and structured format of CondAmbigQA naturally leads to several key advantages over existing datasets. As shown in Table \ref{tab:mcaqa-comparison}, our dataset excels in four crucial aspects that address the challenges in handling ambiguous queries.

1. \textbf{Retrieval included}. By incorporating retrieval documents as a component, CondAmbigQA enhances the retrieval process beyond simple document matching. The retrieved fragments not only provide evidence for answers but also serve as sources for extracting conditions, enabling systems to better understand why certain documents are relevant to different interpretations of a query.

2. \textbf{Improved answer quality}. The condition-answer-citation structure improves answer quality. Unlike datasets that force a single answer or list multiple answers without context, our format guides systems to generate answers that are explicitly grounded in specific conditions. For instance, in the previous example about England's king in 1688, the system can generate distinct answers for different time periods while maintaining clarity about when each answer is valid.

3. \textbf{Advanced reasoning}. The formal relationship between conditions and answers creates a logical framework for reasoning. When a system encounters an ambiguous query, it can follow a clear process: (1) retrieve relevant documents, (2) identify different conditions from these documents, and (3) generate appropriate answers based on these conditions. This structured approach makes the reasoning process more transparent and verifiable.

4. \textbf{Ambiguity resolution}. Our dataset excels at ambiguity resolution by design. Rather than treating ambiguity as a problem to be eliminated, CondAmbigQA provides a systematic way to handle it through explicit condition-answer mappings. This approach allows systems to maintain multiple valid interpretations while providing users with clear criteria for choosing between them.


\begin{table}
\centering
\caption{Comparison of CondAmbigQA against other datasets. CondAmbigQA excels in enhanced retrieval, improved generation, advanced reasoning, and ambiguity resolution.}
\label{tab:mcaqa-comparison}
\begin{tabular}{l|c|c|c|c}
\toprule
Dataset & \begin{tabular}[c]{@{}c@{}}Retrieval\\ Included\end{tabular} & \begin{tabular}[c]{@{}c@{}}Improved\\ Answer \\Quality\end{tabular} 
 & \begin{tabular}[c]{@{}c@{}}Advanced\\ Reasoning\end{tabular} & \begin{tabular}[c]{@{}c@{}}Ambiguity\\ Resolution\end{tabular} \\
\midrule
CondAmbigQA & \cmark & \cmark & \cmark & \cmark \\
\midrule
ASQA \cite{stelmakh-etal-2022-asqa} & \xmark & \cmark & \cmark & \cmark \\
AmbigNQ \cite{min-etal-2020-ambigqa} & \xmark & \xmark & \xmark & \cmark \\
ALCE \cite{gao-etal-2023-enabling} & \cmark & \cmark & \xmark & \xmark  \\
Multihop-RAG \cite{tang2024multihoprag} & \cmark & \xmark & \cmark & \xmark \\
NaturalQuestions \cite{kwiatkowski-etal-2019-natural} & \cmark & \xmark & \xmark & \xmark \\
TriviaQA \cite{joshi-etal-2017-triviaqa}& \xmark & \xmark & \xmark & \xmark  \\
ELI5 \cite{fan-etal-2019-eli5} & \cmark & \cmark & \cmark & \xmark  \\
TruthfulQA \cite{lin-etal-2022-truthfulqa} & \xmark & \cmark & \cmark & \xmark  \\
\bottomrule
\end{tabular}
\end{table}




