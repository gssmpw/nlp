
%介绍清楚我们发现了什么问题,然后提出了什么方法,定义了什么样子的任务,记住重点是condition,从来不是检索推理,逻辑不通
%消岐研究的重要性,他们做了什么详细讲解他们做了啥,GAP,任务方面有问题,聚焦条件的定义,并且设计一套标注策略,标注的前任的问题.延伸,标注的心路历程,简单讲,result,实验,得处那些结论,结论,证明出现,我们发现了什么.clawing law,在大模型 所以这个任务是合理的.



Large language models (LLMs) have made remarkable progress in NLP tasks, particularly in QA systems. However, these advanced models remain prone to generating unreliable responses, particularly in ambiguous contexts and with hallucinations being a primary concern \cite{ji-etal-2023-towards}. This issue stems from a fundamental misalignment between user expectations and model capabilities. %Although LLMs demonstrate remarkable proficiency in generating high-quality responses, their inability to grasp human-like contextual understanding and adapt to varying forms of common sense reasoning often results in misinterpretations and inaccurate responses \cite{banerjee2024llms}.
Despite their proficiency in generating high-quality responses, LLMs often misinterpret questions due to their limited ability to infer human-like context and common-sense reasoning \cite{banerjee2024llms}.

The problem of question ambiguity in QA tasks is particularly critical as it represents the intersection where this capability gap becomes most apparent. This challenge is multifaceted, arising from how human communication inherently relies on shared cognitive frameworks and background knowledge, often omitting mutually understood context. However, such contexts may not be globally or widely recognized beyond certain environments. Additionally, language itself is inherently ambiguous, and people tend to favor concise over exhaustive expressions \cite{wasow2005puzzle}. %Users typically approach QA systems with implicit assumptions, unconsciously supplementing questions with default conditions based on their common sense and context, often overlooking the fact that these assumptions are not provided to the model. 
Users often approach QA systems with implicit assumptions, unconsciously supplementing questions with default conditions derived from their context and common sense. However, these assumptions are rarely communicated explicitly to the model. However, LLMs, trained primarily on vast corpora of human-generated text, may not always align with these unstated assumptions. %This misalignment can lead to responses that, while logically consistent with the literal question, appear as hallucinations from the user's perspective. 
This misalignment produces responses that are logically consistent with the literal question but diverge from user intent, resulting in hallucinations from the user's perspective.
%消岐研究的重要性,他们做了什么详细讲解他们做了啥
Identifying and addressing unstated assumptions is critical for effective disambiguation, without which, it is difficult to ensure that the generated response is accurate or fully aligned with user expectations. %By prioritizing the completeness of the underlying context and clarifying implicit assumptions, disambiguation bridges the gap between the model’s interpretation and user intent, significantly improving response reliability. 
Current research directions focus on enhancing model logical reasoning, expanding context length, and improving the retrieval and utilization of relevant information \cite{ding2024longrope,sun2024determlr,petroni2024ir}, addressing these challenges from two primary perspectives. The first focuses on enhancing model capabilities through methods like GPT-o1, Chain-of-Thought (CoT) prompting, and Reinforcement Learning (RL) \cite{wei2022chain,ahmadian2024back}. Additionally, efforts like BeaverTails \cite{ji2024beavertails} introduce extensive QA datasets that leverage human preference annotations to enhance model alignment and safety by providing a rich resource for training and evaluating LLMs against criteria of helpfulness and harmlessness. While these approaches can improve reasoning and alignment, they are not specifically tailored to address ambiguity within QA tasks. The second approach directly addresses ambiguity in QA contexts, though each method has limitations. AmbigQA \cite{min-etal-2020-ambigqa} rewrites ambiguous questions to capture possible answers; However, its reliance on human annotators introduces bias and fails to codify the implicit conditions driving different interpretations. %manual rewrites often reflect annotator biases, which can inadvertently shape the dataset in ways that misrepresent real-world usage. Additionally, these rewrites may introduce unnecessary context, leading to redundancies where the model produces identical answers, often because the variations fail to meaningfully alter the underlying query structure or intent. 
ASQA \cite{stelmakh-etal-2022-asqa} extends this by generating long-form answers to cover multiple interpretations, but the annotation process can lead to logical inconsistencies, especially when linking different answer components. ALCE \cite{gao-etal-2023-enabling} adds citations to enhance credibility but fails to address or totally neglect implicit ambiguity within queries. APA \cite{kim2024aligning} further detects ambiguity by using an agent to prompt users for clarification, but this method depends heavily on the model’s internal biases, which may inadvertently lead users toward biased or unintended choices.

% 延伸 宏观,标注流程,所以内容,每个都需要一段话,简单讲那部分实验,得处了那些重要结论,有condition, scaling Law 

We review the prior works and reveal the persistent challenges in resolving ambiguity within a QA system. The fundamental limitation of existing approaches lies in their treatment of ambiguity: question rewriting (like AmbigQA) and long-form answers (like ASQA) attempt to cover all possible interpretations simultaneously, while clarification prompts (like APA) rely heavily on model judgment. These methods fail to address a key issue where ambiguity in questions often arises from unstated conditions that, once made explicit, naturally resolve into distinct interpretation paths \cite{wasow2005puzzle}. To address these challenges, we first introduce the concept of {``conditions''}, the contextual prerequisites that determine how a question can be interpreted and answered. Instead of attempting to enumerate all possible interpretations, an inherently infeasible task, our approach prioritizes identifying and explicitly representing key underlying conditions that are most relevant to the query at hand. This theoretical foundation is rooted in linguistic pragmatics, which emphasizes that meaning emerges from shared context and assumptions \cite{grice1975logic,levinson1983pragmatics}.
By uncovering these implicit conditions, we can systematically address ambiguity by clarifying the context and narrowing down the range of plausible interpretations, ensuring that the model’s responses align more closely with user expectations.

Building on this key concept, we introduce \textbf{Cond}itional \textbf{Ambig}uous \textbf{Q}uestion \textbf{A}nswering (\textbf{CondAmbigQA}), a novel framework that structures QA responses according to explicit \textbf{conditions}. A \textbf{condition} in our framework represents a specific contextual constraint or assumption that, when made explicit, disambiguates the meaning of a question. Unlike previous approaches \cite{stelmakh-etal-2022-asqa} that mix multiple interpretations or rely on model capacity, our method provides a principled way to separate and organize answers based on their underlying conditions. %This structure eliminates redundancy by leveraging a retrieval-based approach to focus on the most relevant conditions. 
The dataset contains 200 ambiguous queries. We employ standard retrieval model to retrieve 20 Wikipedia fragments for each query according to their relevancy. Annotators are instructed to analyze the retrieval results and identify plausible but distinct answers together with the conditions that pinpoints the answers. By grounding responses in retrieved evidence, it narrows the scope of ambiguity and provides a practical framework for systematically evaluating the alignment between conditions and answers. 
We notice that it is extremely tedious for human annotators to identify conditions from the retrieval results and conclude the key contextual factors in a consistent manner. In contrast, LLMs can better comprehend all contexts and summarize the conditions, given their superior long text comprehension capacity. Therefore, in building CondAmbigQA, we adopt an interactive annotation process to discover and refine the condition-answer pairs. GPT-4o assists annotators to review all retrieved fragments and give initial draft of the condition-answer pairs. Annotators scrutinize LLM's responses and give further instructions to correct if there are any errors. In this process, annotators only need to monitor the responses from LLM, which reduces their cognitive load and avoids introducing their own subjectivity in the annotation. In addition to the improved data reliability and consistency, such a strategy significantly improves the annotation efficiency. Human-only annotation takes up to 15–20 minutes to manually annotate and validate a single query with complex ambiguity, while our interactive process reduced this time to 5–7 minutes on average.

%minimize model biases and ensure alignment with predefined conditions, as well as improve annotation efficiency, which specify the contextual factors and assumptions necessary for resolving ambiguity in QA tasks. By focusing on filtering out assumptions and minimizing human subjectivity, we aimed to improve data reliability and consistency. This process involved iterative refinements and quality checks to align with predefined standards. This human-LLM interactive approach significantly reduced annotation costs by leveraging the model’s ability to generate initial drafts, which human annotators then refined. 
%For instance, while traditional  This efficiency, combined with reduced cognitive load on annotators, minimized alignment bias and produced a high-standard dataset that supports disambiguation more effectively. 

Experiments demonstrate that condition-based generation significantly improves answer quality compared to standard RAG approaches \cite{lewis2020retrieval}. In our evaluation framework, models were tasked with first identifying disambiguating conditions from ambiguous queries and retrieved passages and then generating condition-specific answers. By introducing explicit conditions into the answer generation process, the condition-guided setting enables the model to better align responses with the intended query context, as reflected in higher scores across metrics such as factual accuracy, response clarity, and citation reliability. In addition, our experiments show that LLMs’ capacity for ambiguity resolution follows scaling laws, where larger models, such as GPT-4o and GLM4-Plus, outperform smaller models like \texttt{LLaMA3.1} 8B. The larger models demonstrated superior condition adherence and answer quality, while smaller models often struggled to produce coherent and accurate responses. Moreover, our integration of citation generation further enhances answer reliability, with larger models demonstrating a notable advantage in generating precise citations. In summary, our conditional framework and high-quality data offer a scalable, unbiased solution for managing QA ambiguity, setting a new standard for clarity and logical precision in complex disambiguation tasks.


Our key contributions advance both the theoretical understanding and practical handling of ambiguity in QA systems:

\begin{itemize}
\item  We are the first to identify the root cause of QA ambiguity as implicit conditions in user queries, providing a framework for systematic disambiguation through explicit condition representation.

\item Recognizing the critical role of implicit conditions in resolving ambiguity, we propose CondAmbigQA, a novel condition-based framework that explicitly organizes responses around identified conditions. This approach ensures clarity and relevance in context-specific answers, while providing a structured methodology to handle query ambiguity and maintain logical consistency.

\item To support CondAmbigQA, we design a human-LLM interactive annotation process, leveraging GPT-4o to iteratively refine prompts. This method significantly reduces annotation costs by streamlining manual efforts, while maintaining high data quality through systematic bias minimization.

\item Through comprehensive experiments, our study highlights the importance of condition-based generation in ensuring high-quality answers, demonstrating that adherence to conditions plays a critical role in generating accurate and reliable responses. Larger models exhibit better performance in both maintaining condition adherence and delivering precise answers, further emphasizing the value of leveraging model scale. Additionally, integrating citation generation enhances the reliability of responses, particularly when employed with larger models, showcasing the potential for combining these approaches to improve task outcomes.
\end{itemize}

Rest of this paper is organized as follows: Section 2 reviews related work in QA disambiguation and discusses their limitations. Section 3 introduces our CondAmbigQA dataset and details the condition-based annotation methodology. Section 4 presents our experimental setup and implementation details. Section 5 provides a comprehensive analysis of our results, demonstrating the effectiveness of our approach. Finally, Section 6 discusses the implications of our findings and concludes with future directions.