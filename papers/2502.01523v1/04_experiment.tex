%QA 任务

In this section, we present a comprehensive evaluation framework for the CondAmbigQA benchmark, which introduces a novel task of resolving ambiguous questions through explicit condition identification. Unlike traditional question answering tasks that directly generate answers, we propose that ambiguous questions should first be disambiguated by identifying explicit conditions that affect the answer, then generating appropriate responses for different condition combinations. This decomposition of the ambiguous QA process into condition identification and conditional answer generation represents a more structured approach to handling query ambiguity. Through carefully designed metrics and experimental protocols, our benchmark evaluates both the model's ability to identify and articulate these disambiguating conditions, and its capacity to generate condition-specific answers.

\subsection{Evaluation framework}
The CondAmbigQA benchmark adopts a multi-metric evaluation approach to comprehensively assess model performance. 
Let $M$ denote the model output, $G$ denote the ground truth, and $\textit{G-Eval}(x,y)$ represent the G-Eval function that evaluates the quality of output $x$ against reference $y$ based on pre-defined criteria~\cite{yao2024clave,liu2023g}. The four evaluation metrics are defined as follows:

\textit{Condition Score} measures the quality of condition identification:
\begin{equation}
\textit{Condition Score}(M,G) = \textit{G-Eval}(M.conditions, G.conditions),
\end{equation}
where the G-Eval function assesses both completeness and clarity of identified conditions.

\textit{Answer Score} evaluates the quality of generated answers:
\begin{equation}
\textit{Answer Score}(M,G) = \textit{G-Eval}(M.answers, G.answers),
\end{equation}
focusing on factual accuracy and condition-specific response quality.

\textit{Citation Score} quantifies source attribution accuracy:
\begin{equation}
\textit{Citation Score}(M,G) = \frac{|{c \in M.citations} \cap {c \in G.citations}|}{|{c \in G.citations}|},
\end{equation}
where citations are normalized and compared as sets to produce a score in [0,1].

\textit{Answer Count} measures response completeness:
\begin{equation}
\textit{Answer Count}(M,G) = |M.answer\ count - G.answer\ count|,
\end{equation}
reflecting the model's understanding of required answer granularity.

\subsection{Experimental protocol}

To evaluate the effectiveness of condition guidance in ambiguous question answering, we conduct two sets of experiments. 

In the main experiment, we assess models' native ability in condition identification and answer generation. Given a query $Q$ and retrieved passages $P$ (whole passages fragments) as input, models are required to first identify disambiguation conditions and then generate appropriate answers based on these \textbf{identified conditions}. Specifically, this protocol evaluates models' end-to-end capability in understanding and resolving query ambiguity through:

\begin{itemize}
   \item Condition identification: extracting key conditions that resolve ambiguity;
   \item Answer generation: providing appropriate answers based on identified conditions;
   \item Citation: supporting answers with relevant passages.
\end{itemize}

In the comparative experiment, we design two controlled settings to quantify the impact of condition guidance:

\begin{itemize}
  \item \textbf{Standard RAG}: Models directly generate answers from $Q$ and $P$ without explicit condition information;
   \item \textbf{Condition-guided}: Models receive additional ground-truth conditions alongside $Q$ and $P$.
 
\end{itemize}

This controlled comparison helps isolate the effect of condition guidance on answer quality and citation accuracy. By comparing model performance between these two settings, we can quantitatively assess how explicit condition information influences the quality of generated answers.

\subsection{Baseline models}
We evaluate our benchmark using five representative open-source language models: \texttt{LLaMA3.1} (8B) \cite{dubey2024llama}, trained on 1.2T tokens with optimized attention mechanism, \texttt{Mistral} (7B) \cite{jiang2023mistral}, known for its efficient architecture; \texttt{Gemma} (9B) \cite{team2024gemma}, trained on high-quality curated dataset, \texttt{GLM4} (9B) \cite{glm2024chatglm}, featuring enhanced cross-lingual abilities; and \texttt{Qwen2.5} (7B) \cite{yang2024qwen2}, optimized for comprehensive language understanding. These models, with parameters ranging from 7B to 9B, provide a diverse yet comparable foundation for baseline performance assessment.
To ensure reproducibility, all models are deployed through the \texttt{Ollama} framework, using default sampling parameters and 8K context window size. Model outputs are evaluated using G-Eval implemented via the \texttt{DeepEval} package, with \texttt{GPT4-mini} serving as the evaluation model through OpenAI's API.

\subsection{Scaling analysis}
To understand how model scale influences performance on our benchmark, we conduct additional experiments with two larger-scale models. This analysis aims to investigate whether performance on conditional ambiguous question answering follows established scaling laws~\cite{kaplan2020scaling}, providing insights into the relationship between model capacity and task performance. Through this evaluation framework, our benchmark provides a standardized way to assess and compare model performance in handling conditional ambiguous questions. The multi-metric approach and diverse experimental protocols enable detailed analysis of model capabilities. In particular, the scaling experiments validate the applicability of scaling laws to ambiguity resolution tasks, demonstrating that larger models consistently outperform smaller ones in condition adherence and answer quality. These findings offer valuable insights into the relationship between model size and performance, guiding future model development and optimization.