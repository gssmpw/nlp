\label{sec:related_work}

\subsection{QA datasets with ambiguity}
The issue of ambiguity in QA tasks has gained significant attention in recent years, as exemplified by the AmbigQA dataset \cite{min-etal-2020-ambigqa}. Unlike traditional word or sentence disambiguation, ambiguity in QA tasks more closely mirrors real-life scenarios. AmbigQA aims to identify all possible answers by rewriting queries more specifically, with each rewrite corresponding to a single answer. Building on this, ASQA \cite{stelmakh-etal-2022-asqa} recognized the limitations of single answers and proposed complex, long-form responses to explain multiple possible answers. However, this approach often results in overly complex and redundant explanations, with evaluation methods that inadequately reflect answer reliability. ALCE \cite{gao-etal-2023-enabling} introduced citations to enable answer verification through source Wikipedia documents, enhancing credibility. Other works, such as BeaverTails \cite{ji2024beavertails}, attempt to align LLMs using human preference datasets, but this approach may inadvertently exacerbate biases in answer distributions for ambiguous questions. It is crucial to note that these datasets primarily rely on crowdsourcing or small-scale trained annotators, potentially compromising quality. Furthermore, the lack of published Wikipedia versions used for annotation renders many answers obsolete as Wikipedia content evolves, highlighting the need for more robust and transparent annotation processes in ambiguous QA datasets.


\subsection{LLM benchmark and evaluation metrics }
The rapid advancement of Large Language Models (LLMs) has exposed limitations in traditional evaluation metrics, prompting a shift towards more sophisticated benchmarking approaches \cite{bavaresco2024llms}. While some researchers advocate using LLMs as judges \cite{zheng2023judging,yu2024evaluation}, this method risks introducing hallucinations and biases \cite{liu2023g}. Conventional metrics like ROUGE or BLEU, designed for shorter texts, struggle to capture the complexity of modern LLM outputs \cite{lin2004rouge,papineni2002bleu}. In response, researchers have developed enhanced evaluation frameworks such as G-Eval, which incorporates chain-of-thought reasoning \cite{wei2022chain}, and self-evolving benchmarks \cite{wang2024benchmark} to improve robustness. LiveBench addresses dataset contamination concerns by frequently updating its challenges \cite{white2024livebench}, while MixEval combines various benchmarks for a more comprehensive assessment \cite{ni2024mixeval}. The field of retrieval-augmented generation has further complicated evaluation needs \cite{lewis2020retrieval}. Despite these advancements, significant challenges remain in designing unbiased, comprehensive evaluation systems for LLMs, particularly for domain-specific tasks \cite{gehrmann2021gem,magesh2024hallucination}. As LLMs continue to evolve, the development of adaptable and meaningful evaluation metrics remains crucial for guiding future advancements in natural language processing.

\subsection{Current RAG development}
Recent advancements in RAG have primarily focused on enhancing the retrieval process, with particular emphasis on improving precision, optimizing retrieval order, refining the structure of content provided to LLMs, and reformulating queries \cite{lewis2020retrieval}. The majority of RAG systems are non-end-to-end, allowing for separate optimization of various components including retrieval, the interface between retrieval and generation, the generation process itself, query optimization (pre-retrieval), and post-generation processing \cite{gao2023retrieval}. Existing RAG frameworks employ iterative approaches to enhance logical reasoning and disambiguate information. For instance, the Self-RAG methodology enables the model to identify necessary retrieval points and utilizes a smaller model in conjunction with a self-reflection mechanism to ensure response accuracy \cite{asai2023self}, while CRAG implements a trained retrieval evaluator to assess the quality of each retrieved document \cite{yan2024corrective}. Some studies have found that introducing contrastive noise can facilitate more accurate answer generation, although this is also influenced by the order in which information is presented to the LLM \cite{cuconasu2024power}. Certain approaches even aim to ``reinforce correct cognition'' by removing noise during the model training phase, further enhancing model robustness \cite{zhang2024raft}.
With the rising popularity of CoT prompting, an increasing number of methods, such as Retrieval-Augmented Thought (RAT), are being employed to train models for improved long-term reasoning capabilities \cite{wang2024rat}. However, these approaches often operate under the assumption that each step in their reasoning process is inherently correct. While they may engage in multi-step thinking and verification processes similar to GPT-o1, if the fundamental premises are flawed or misaligned with user intentions, these methods may simply increase the computational time without necessarily improving reasoning accuracy \cite{es2023ragas}. This underscores the need for future research to not only focus on enhancing the retrieval and generation processes but also to develop mechanisms for validating the underlying assumptions and aligning the model's reasoning with user intent, thereby ensuring that increased complexity translates to genuine improvements in output quality and relevance.

\subsection{LLM or Agents for QA task}
Recent trends in the field have largely focused on leveraging CoT prompting, process supervision, and reinforcement learning to further align models that have already undergone dataset fine-tuning \cite{NEURIPS2020_1457c0d6,bai2022constitutional}. However, these alignment efforts are inherently biased towards reward preferences set by humans, often prioritizing content that we expect or desire to see more of, rather than allowing for a broader range of probabilistic outcomes \cite{hewitt2024instruction}. This approach can be particularly problematic when models encounter new information, as they may heavily skew towards strategies rewarded during their initial training. Such methodologies require frequent updates and can be unstable over time \cite{zelikman2022star}. Beyond fine-tuning large language models, there is a growing trend towards utilizing agent-based systems to enhance logical reasoning through collective intelligence \cite{zhu2022solving}. However, most of these approaches merely incorporate supervision, communication, and reinforcement learning at a macro level, or rely solely on prompt-based communication and feedback mechanisms \cite{li2023camel,kojima2022large}.
While these methods have shown promise, they often lack specific optimizations for addressing ambiguities or issues that require comparative search to resolve. The current landscape of research, while innovative, may not fully address the nuanced challenges of logical reasoning, particularly in scenarios requiring disambiguation or extensive comparative analysis \cite{park2023generative}. This gap suggests a need for more targeted approaches that can handle complex reasoning tasks while maintaining flexibility and adaptability to new information. Future research directions may benefit from exploring hybrid systems that combine the strengths of large language models with more specialized reasoning modules or developing novel architectures that can more effectively balance between learned biases and open-ended exploration of logical possibilities \cite{guo2024large}.