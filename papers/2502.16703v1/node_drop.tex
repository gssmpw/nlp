\section{Node Subsampling}\label{sec:node_drop}

In this section, we present our approach for subsampling nodes of a graph dataset so that the performance of a downstream GNN trained on the subsample is preserved. Proofs for results in this section are in Appendix~\ref{sec:apx-node}.

Suppose we have a graph dataset $X= \{G_1, ..., G_n\}$ where $G_i = (V_i, E_i)$, and a node budget $k$. For a subset $S_i \subseteq V_i$ of $k$ nodes, let $G_i[S_i]$ denote the subgraph of $G_i$ induced by $S_i.$ Our goal is to {select these subsets so} that a GNN produces similar readouts on the original dataset $X$ and on the induced subgraphs $X' = \{G_1[S_1], ..., G_n[S_n]\}$. The following corollary shows that if the GNN layers have small Lipschitz constants and the distances $\cTMD(G_i[S_i], G_i)$ are small, then training over $X'$ yields a nearly optimal predictor over $X$. 

\begin{restatable}{corollary}{corrermfirst}\label{corr:erm-first} Let $\cH$ be a set of $(L-1)$-layer GNNs $h: \cG \to \R^d$, where the $\ell$-th layer has Lipschitz constant at most $\Phi_\ell$. Let $\cL: \R^d \to \R$ be an $M$-Lipschitz loss function. 

Suppose that $M \cdot \prod_{\ell \in [L-1]} \Phi_\ell \leq c$ and $\cTMD_w^L(G_i, G_i[S_i]) \leq \epsilon_i$ for all $i \in [n]$. Finally, let $\hat{h} \in \cH$ be a GNN with minimum loss over the subsampled training set with respect to the training labels $y_1, \dots, y_n$: $\hat{h} = \argmin\nolimits_{h \in \cH}\sum\nolimits_{i \in [n]} \cL(h(G_i[S_i]); y_i).$
Then $\hat{h}$ has near-optimal loss over the original training set:
\begin{align*}
    \frac{1}{n}\sum\nolimits_{i \in [n]} \cL\Big({\hat{h}(G_i); y_i}\Big) \leq  \min_{h \in \cH} \frac{1}{n}\sum\nolimits_{i \in [n]} \cL(h(G_i); y_i) + \frac{2c}{n}\sum\nolimits_{i \in [n]} \epsilon_i.
\end{align*}
\end{restatable}

Thus, the additional {loss incurred by} training on $X'$ rather than $X$ is proportional to the average of {$\cTMD_{w}^L(G_i, G_i[S_i])$}, so for each $G \in X$, we would ideally solve:
\begin{equation}
    \min_{S \subset V: \abs{S} \leq k} \cTMD_w^L(G, G[S]).\label{eq:node-subsampling}
\end{equation}

We face {two} key challenges in doing so. First, the number of candidate subsets $\{S \subset V: \abs{S} \leq {k}\}$ grows exponentially.  Indeed, solving \eqref{eq:node-subsampling} is NP-hard (see Appendix~\ref{sec:hardness}).
 We therefore 
 restrict
 our search to an appropriately chosen feasible set $\cS$.
In our experiments, we combine well-motivated, fast heuristics for selecting small candidate sets $\cS$ based on prior analyses \citep{salha2022degeneracy, razin2023ability, alimohammadi2023local}. (Details are in Appendix~\ref{sec:heuristic}.)

The next challenge is that computing $\cTMD_w^L(G, G[S])$ for each $S \in \cS$ can be expensive: the algorithm by \citet{Chuang22:Tree} takes $\cO(L|V|^4)$ time. To address this, we prove that, surprisingly, computing $\cTMD_w^L(G, G[S])$ is equivalent to a much simpler optimization problem. 

\begin{restatable}{theorem}{nodesamplingrelaxed}\label{thm:node-subsampling-relaxed} Let $G = (V, E, f)$ be a graph and $\cS$ be a set of candidate node subsets. Then \begin{align}\label{eq:final-opt-problem}
    \argmin_{S \in \cS} \cTMD_w^L(G, G[S]) = \argmax_{S \in \cS} \cTreeNorm{G[S]}.
\end{align}
\end{restatable}

We prove Theorem~\ref{thm:node-subsampling-relaxed} in Section~\ref{sec:proof} and in Section~\ref{sec:alg}, we provide a linear-time algorithm for computing tree norms and, thereby, the solution to Equation~\eqref{eq:final-opt-problem}.

\begin{restatable}{theorem}{algorithm}\label{thm:node-subsampling-relaxed-algorithm} Given a graph $G = (V, E, f)$, Algorithm~\ref{alg:1} computes $\cTreeNorm{G}$ in $\bigO(\abs{E}L)$ time.
\end{restatable}

\subsection{TMD Between Graphs and Subgraphs}\label{sec:proof}

In this section, we sketch our proof of Theorem~\ref{thm:node-subsampling-relaxed}. In doing so, we prove several properties of the TMD which may be of independent interest given the broad applications of TMD to out-of-distribution generalization. Our analysis crucially relies on the following lemma, which shows that an appealing simple \emph{identity} transport plan can be used to compute $\cTMD$ between a graph and its induced subgraphs.

\begin{restatable}{lemma}{generaldecomposition}\label{lemma:general-decomposition} Let {$w: \N \to \R$ be a weight function and} $G = (V, E, f)$ be a graph.
For $S \subseteq V$, define the identity transportation plan $\bm{I}$ that maps $T_v(G)$ to $T_v(G[S])$ if $v \in S$ and to $\emptyset$ otherwise. Then $\bm{I}$ is an optimal transport plan for
\begin{align}\label{eq:ot-problem}
    \cTMD_w^L(G, G[S]) := \cOTbar\paren{\cT_G^L, \cT_{G[S]}^L}.
\end{align}
Consequently, we can decompose $\cTMD_w^L(G, G[S])$ as: 
\begin{align}
    \cTMD_w^L(G, G[S]) \label{eq:decomposition} &= \underbrace{\sum_{v \notin S} \|f^v\|}_{\textnormal{Deleted nodes' features}} +\underbrace{w(L-1)\sum_{v \notin S} \cOTbar
    \left(\cT_v\left(T_v^L(G)\right),\,\emptyset\right)}_{\textnormal{Cost of removing deleted nodes' trees}}\\
    &+\;
    \underbrace{w(L-1)\sum_{v \in S} \cOTbar
    \left(\cT_v\left(T_v^L(G)\right),\, \cT_v\left(T_v^L(G[S])\right)\right)}_{\textnormal{Cost of matching retained nodes' trees}}.\nonumber
\end{align}
\end{restatable}
\begin{hproof} We prove the statement by induction on $L$. In the base case $(L= 1)$, by definition, $\cOT(\cT_G^1, \cT_{G[S]}^1)$ 
equals the sum of features of the nodes that are in $G$ but not in $G[S]$, i.e., $\cOT(\cT_G^1, \cT_{G[S]}^1) = \sum_{v \notin S} \Vert f^v \Vert = \langle C, \bm{I} \rangle$, so the base case holds. Inductively, we use the TD's recursive formulation to reduce OT problems of depth $L$ to those of depth $L-1$.
\end{hproof}

Next, we use Lemma~\ref{lemma:general-decomposition} to characterize how TMD accumulates as we remove a sequence of nodes. Lemma~\ref{lemma:chain-simple} shows that this accumulation is \emph{additive} under certain conditions, which is unexpected given the TMD's combinatorial nature.


\begin{restatable}{lemma}{finegraineddecomp}\label{lemma:chain-simple} 
For $T \subset S \subset V$, $\cTMD_w^L(G, G[S \setminus T ]) = \cTMD_w^L(G, G[S]) + \cTMD_w^L(G[S], G[S\setminus T])$.  
\end{restatable}

 This equality is noteworthy because the triangle inequality only guarantees $\cTMD_w^L(G, G[S \setminus T ]) \leq \cTMD_w^L(G, G[S]) + \cTMD_w^L(G[S], G[S\setminus T])$. However, we show that the triangle inequality is \emph{always} tight when $T \subset S \subset V$.

\begin{hproof}[Proof sketch of Lemma~\ref{lemma:chain-simple}] We apply Lemma~\ref{lemma:general-decomposition} {inductively over $L$}. When $L=1$, the second and third summands of \eqref{eq:decomposition} equal 0: $\cT_G^1$ and $\cT_{G[S]}^1$ are multisets of depth 1, so for any $v \in V$, $T_v^1(G) = \{v\}$ and thus $\cT_v(T_v^1(G)) = \emptyset$. Likewise, for any $v \in S$, $\cT_v(T_v^1(G[S])) = \emptyset$. Therefore, 
\begin{align*}
    \cTMD_w^1(G, G[S \setminus T]) = \sum\nolimits_{v \notin (S \setminus T) } \norm{f^v} = \sum\nolimits_{v\notin S} \norm{f^v} + \sum\nolimits_{v \in T} \norm{f^v}, 
\end{align*} 
because for $T \subset S$, $\{v \in V : v \notin (S \setminus T) \} = \{v \in V : v \notin S\} \sqcup T$, where $\sqcup$ denotes the \emph{disjoint union}.
Similarly, 
\begin{align*}
    \cTMD_w^1(G, G[S]) = \sum\nolimits_{v \notin S} \norm{f^v} \text{\,\,\,\,and\,\,\,\,} \cTMD_w^1(G[S], G[S \setminus T]) = \sum\nolimits_{v \in T} \norm{f^v}, 
\end{align*} 
thus verifying the base case. The intuition is similar for $L>1$ but requires
care to handle the recursive OT terms.
\end{hproof}

Finally, we can prove Theorem~\ref{thm:node-subsampling-relaxed}.

\begin{proof}[Proof of Theorem~\ref{thm:node-subsampling-relaxed}] Let $S \in \cS$. By Lemma~\ref{lemma:chain-simple} with $T = S$, $\cTreeNorm{G} =  \cTMD_w^L(G, \emptyset) = \cTMD_w^L(G, G[S]) + \cTMD_w^L(G[S], \emptyset) = \cTMD_w^L(G, G[S]) + \cTreeNorm{G[S]}.$  Thus, 
\begin{align*}
    \max_{\substack{S \in \cS \\ \abs{S} = k}}
 \cTreeNorm{G[S]} \equiv \max_{\substack{S \in \cS \\ \abs{S} = k}} \cTreeNorm{G} - \cTMD_w^L(G, G[S]) \equiv \min_{\substack{S \in \cS \\ \abs{S} = k}} \cTMD_w^L(G, G[S]). \tag*{\qedhere} 
\end{align*}
\end{proof}





\subsection{Faster Algorithm for Tree Norms}\label{sec:alg}
We next leverage Lemma~\ref{lemma:general-decomposition} to obtain  Algorithm~\ref{alg:1}, a faster algorithm for computing $\cTreeNorm{G}$ for $G = (V, E, f).$ The algorithm by \citet{Chuang22:Tree} requires $\bigO(L \abs{V}^4)$ time, whereas ours {has runtime} $\bigO(L |E|)$. Algorithm~\ref{alg:1} is based on the fact that by Lemma~\ref{lemma:general-decomposition}, $\cTreeNorm{G}$ is essentially a weighted sum of the number of vertices in all of its depth-$L$ computation trees, which Algorithm~\ref{alg:1} {computes}. Its runtime is dominated by the cost of $L$ matrix-vector {multiplies} with the adjacency matrix, which takes $\bigO(|E|)$-time. Proofs of correctness and runtime are in Appendix~\ref{sec:apx-node}. \\

\RestyleAlgo{ruled}
\SetKwComment{Comment}{/* }{ */}
\begin{algorithm2e}[H]
\caption{\text{TreeNorm}($G, L, w$)}\label{alg:1}
\KwInput{Graph $G = (V, E, f)$ {with adjacency matrix $A$}, weights $w: \{1, ..., L-1\} \to \R_+, L\geq 1$}
Define $x \in \R^{|V|}$ such that $x_v = \norm{f^v}$ \label{line:x}\; 
Initialize $z^{(0)} = x$\; 
\For{$\ell \in [{L-1}]$}{
    $z^{(\ell)} \gets A z^{(\ell-1)}$\label{line:z}\;
}
$b \gets z^{(0)} + \sum_{\ell = 1}^{L-1} \paren{\prod_{t=1}^\ell w(L-t)} \cdot z^{(\ell)}$\;  
\Return{$\normInline{b}_1$}
\end{algorithm2e}

