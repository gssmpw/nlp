\newcommand{\xmark}{\ensuremath{\boldsymbol{\times}}}

\section{Graph Subsampling}\label{sec:graph_drop}

Here we present our approach for subsampling graphs in a dataset while preserving the performance of a downstream GNN.  Our method relies on the following Lipschitz bound that relates a GNN's stability to TMD\footnote{While Theorem 8 in \citet{Chuang22:Tree} is stated for GINs, they extend it to other GNN architectures.}.

\begin{theorem}[Informal, Theorem 8 by \citet{Chuang22:Tree}, restated]\label{thm:stable} {There exists a weight function $w$ such that} for any $(L-1)$-layer message-passing GNN with readout $h: \cG \mapsto \R^d$ and layer-wise Lipschitz constants $\phi_\ell$, the following holds for any two graphs $G_a, G_b$: \[\norm{h(G_a) - h(G_b)}\leq \cTMD_{w}^{L} (G_a, G_b) \cdot \prod_{\ell=1}^{L-1} \phi_\ell.\] 
\end{theorem}

Let $X= \{G_1, ..., G_n\}$ be our graph dataset. Given a budget $k$, we aim to choose a subset $\cI \subset [n]$ of size $k$ such that a GNN trained on the subsampled set {$\{G_i\}_{i \in \cI}$} obtains similar readouts and loss as if it were trained on the entire set $X$.
To ensure $\cI$ is representative of $X$, we select $\cI$ by optimizing a medoids objective, which quantifies how well each graph $G_i \in X$ is represented by at least one graph in $\cI$. 

\begin{definition}[Medoids objective] Let $X= \{G_1, ..., G_n\}$ be a graph dataset and $\cI \subset [n]$ with $\absInline{\cI} \leq k$. The medoids objective value of $\cI$ with respect to distance $D : X \times X \to \R_{\geq 0}$ is defined by 
\begin{align*}
    f_D(\cI; X) = \frac{1}{\abs{X}}\sum\nolimits_{i \in [n]} \min_{j \in\cI} D(G_i, G_j).
\end{align*}
For $j \in \cI$, let $\tau_j$ be the number of graphs closest to $G_j$:
\[\tau_j \defeq \abs{\{i \in [n]: D(G_i, G_j) < D(G_i, G_k),~\forall k \neq j\}},\]
breaking ties arbitrarily. We call $\tau_j$ the size of cluster $j$. 
\end{definition}

The provide a lemma that bounds the difference in a GNN's loss on the (weighted) subsampled dataset $\cI \subset X$ and the full dataset $X$ in terms of the medoids objective with respect to TMD. Proofs of results in this section are in Appendix~\ref{sec:apx_graph_drop}.

\begin{restatable}{lemma}{tmdgen}\label{lemma:tmd-gen} Let $\cH$ be a hypothesis class of $(L-1)$-layer GNNs $h: \cG \to \R^d$, where the $\ell$-th layer has Lipschitz constant at most $\Phi_\ell$. Let $\cL: \R^d \to \R$ be an $M$-Lipschitz loss function, let {$y = (y_1, ..., y_n)$ be labels for $X$}, and $\cI$ be a subset of $[n]$. For any GNN $h \in \cH$ and graph $G \in \cG$,  
\begin{align}
    \Big\lvert{\frac{1}{n} \sum\nolimits_{i \in \cI} \tau_{i} \cL(h(G_i); y_i) - \frac{1}{n}\sum\nolimits_{i \in [n]}\cL(h(G_i); y_i)}\Big\rvert \nonumber \leq  M \cdot f_{\cTMD_w^L}(\cI; X) \cdot {\prod\nolimits_{\ell \in [L-1]} \Phi_\ell} . \label{eq:mediods_bound}
\end{align}
\end{restatable}
\begin{hproof} We use the Lipschitz constant $M$ of $\cL$ and Theorem~\ref{thm:stable} to bound the average deviation in the loss on $X$ from the loss on $\cI$ (reweighted according to the $\tau_i$'s). 
\end{hproof}

If $f_{\cTMD_w^L}(\cI; X)$ is small, then each $G_i \in X$ is close to some subsampled graph, which keeps the overall loss on the subsampled set similar to the loss on $X$. If we minimize loss over $\cI$, the next corollary shows that the resulting hypothesis will incur only a small increase in loss on $X$.
\begin{restatable}{corollary}{erm}\label{cor:ERM}
    Suppose $M \cdot \prod_{\ell \in [L-1]} \Phi_\ell \leq c$, $f_{\cTMD_w^L}(\cI; X) \leq \epsilon$, and $\hat{h} \in \cH$ minimizes the weighted loss $\sum_{i \in \cI} \tau_{i} \cL(h(G_i); y_i).$ Then $
    \frac{1}{n}\sum\nolimits_{i \in [n]}\cL\big(\hat{h}(G_i); y_i\big) \leq \min_{h \in \cH}\frac{1}{n}\sum\nolimits_{i \in [n]}\cL(h(G_i); y_i) + 2c\epsilon.$
\end{restatable}
% \paragraph{Graph subsampling.}
% \begin{figure}[t]
% \centering
%     \includegraphics[width=\textwidth]{figures/GraphSubsampling.png}
% \caption{Test accuracy versus fraction of graphs in the training set, subsampled with our approach and existing model-agnostic methods.}
% \label{fig:graph_subsampling}
% \end{figure}
%Remark~\ref{remark:tmd-gen-remark}
Hence, the additional training loss incurred by training on $\cI$ instead of $X$ is bounded by an error proportional to $f_{\cTMD_w^L}(\cI; X).$ This bound could be combined with prior work on the VC dimension of GNNs~\citep[e.g.,][]{scarselli2018vapnik} to obtain bounds on the \emph{test loss} as well.

Corollary~\ref{cor:ERM} motivates selecting $\cI$ to minimize $f_{\cTMD_w^L}(\cI; X).$
% \begin{definition}[Graph subsampling problem]\label{def:graph-subsampling} Let $X= \{G_1, ..., G_n\}$ be a set of graphs and $k$ be a budget. The \emph{graph subsampling problem} is to select a subset $\cI \subset [n]$ of $k$ graphs that minimizes the objective $f_{\cTMD_w^L}(\cI; X).$
% \end{definition}
Importantly, this optimization problem is independent of graph labels. Training a GNN on the subsampled graphs in only requires knowing the labels of those graphs. Additionally, our results make mild assumptions on the GIN architecture---we need only know its depth $L$. Though the $k$-medoids problem---and thus optimizing $f_{\cTMD_w^L}(\cI; X)$---is NP-hard~\citep{kazakovtsev2020application}, {there are} efficient approximation algorithms, for example Python's $k$-medoids algorithm from the sklearn package \citep{sklearn_api}, which we use in our experiments.

\paragraph{Other pseudo-metrics.} Given these results, a natural question is whether the TMD pseudo-metric is essential for the stability result in Theorem~\ref{thm:stable}. One might hope that other graph pseudo-metrics could also be used to bound GNN stability, thereby yielding analogs of Corollary~\ref{cor:ERM}.  We express this intuition as the following conjecture.

\begin{conjecture}\label{conj:TMD-not-needed} Let $D$ denote a graph pseudo-metric. There exists a function $C : \{\phi_\ell\}_{\ell=1}^{L-1} \to \R_{> 0}$ such that for any $(L-1)$-layer message-passing GNN with readout function $h: \cG \to \R^d$ and layer-wise Lipschitz constants $\phi_\ell$, the following holds for any two graphs $G_a, G_b$: $\norm{h(G_a) - h(G_b)} \leq C\paren{\phi_1, ..., \phi_{L-1}} \cdot D(G_a, G_b)$. 
\end{conjecture}

Surprisingly, we show that this conjecture is \emph{provably} false for four of the most widely used graph pseudo-metrics: Weisfeilerâ€“Lehman (WL) \citep{shervashidze2011weisfeiler}, WL-Optimal Transport (WL-OA) \citep{kriege2016valid}, shortest-paths (SP) \citep{borgwardt2005shortest}, and graphlet sampling (GS) \citep{shervashidze2009efficient}. 

\begin{restatable}{theorem}{tmdneeded}\label{thm:TMD-is-needed}
    Let $D^L$ denote any of the following pseudo-metrics: GS, $L$-layer WL, $L$-layer WL-OA, or the $L$-layer SP. Then Conjecture~\ref{conj:TMD-not-needed} is false for $D = D^L$.
\end{restatable}

Consequently, our theoretical findings and the main result in Corollary~\ref{cor:ERM} rely \emph{crucially} on TMD as the underlying pseudo-metric. See Appendix~\ref{sec:thm-tmd-is-needed} for details.