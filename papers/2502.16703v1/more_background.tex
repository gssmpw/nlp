\section{Additional background}\label{sec:additional-background}


In this Appendix, we discuss additional helpful background for the discussions in the main body. 

\subsection{Additional details about the TMD}\label{app:TMD}

Here we describe the padding function $\rho$ using in $\cOTbar$.

We use $\blankTree^n$ to denote $n$ disjoint copies of a blank tree $\blankTree.$ Given two tree multisets $\cT_u(T)$, $\cT_v(T')$, we define $\rho$ to be the following augmentation function, which returns two multi-sets of the same size: 
\begin{align}\label{eq:rho}
    \rho: (\cT_v(T'), \cT_u(T)) \mapsto \paren{\cT_v(T') \cup {\blankTree}^{\max(|\cT_u(T)| - |\cT_v(T')|, 0)}, \cT_u(T) \cup {\blankTree}^{\max(|\cT_v(T)| - |\cT_u(T')|, 0)} }.
\end{align}

Equipped with this definition, we define $\cOTbar$ for a given weight function $w: \N \to \R_{> 0}$ as follows: 
\begin{align}\label{eq:OT-bar}
    \cOTbar(\cdot, \cdot) = \cOT_{\cTD_w}(\rho(\cdot, \cdot)). 
\end{align}

\subsection{Visual notation aids}\label{sec:notation}

\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{figures/ComputationTrees.png}
    \caption{Computation trees up to depth $L=3$ for an example 4-node graph (Definition~\ref{def:tree}).}
    \label{fig:ctrees}
\end{figure}

We have included some visualizations to aid in understanding the notations introduced in Section~\ref{sec:prelim}. In the following figures, we use black outlines to denote the roots of rooted trees.  Figure~\ref{fig:ctrees} gives a visualization of computation trees for a simple four-node graph, as per Definition~\ref{def:tree}. Figure~\ref{fig:multiset} gives a visualization of a tree multiset as per Definition~\ref{def:multiset}. Figure~\ref{fig:td} gives a visualization of the recursive definition of the tree distance (TD) Definition~\ref{def:TD}. Figure~\ref{fig:tmd} shows a visualization of computing TMD as an OT problem between multiset of computation trees of two graphs. We hope that these figures help the reader to develop a more intuitive understanding of the TMD.

\begin{figure}
    \centering
    \includegraphics[width=0.4\linewidth]{figures/TreeMultiset.png}
    \caption{The tree multiset associated with an example depth 3 rooted tree (Definition~\ref{def:multiset}).}
    \label{fig:multiset}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=1\linewidth]{figures/TD.png}
    \caption{The tree distance between example graphs (Definition~\ref{def:TD}).}
    \label{fig:td}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=.25\linewidth]{figures/TMD.png}
    \caption{TMD computation between example graphs (Definition~\ref{def:TMD}). We first construct the computation trees of the original graphs, followed by computing the OT cost with respect to the tree distance. Since the graphs have unequal number of nodes, we pad the computation trees of $G_2$ with a \emph{blank} tree (see also \eqref{eq:rho}).}
    \label{fig:tmd}
\end{figure}

\subsection{Heuristic for selecting $\cS$ in the relaxed node subsampling problem}\label{sec:heuristic}

Here we present
our heuristic for constructing the set of candidate subsets $\cS$ in our experiments. Most real-world networks are scale-free and small-world, and hence can be well-modeled by random graphs, such as preferential attachment, configuration models, and {inhomogeneous} random graphs \citep{bollobas2004coupling,newman1999scaling}. {These} 
graphs
converge \textit{locally} to {graph limits} \citep[Vol.~2, Ch.~2]{van2024random}. By a ``transitive'' argument, {we can} view real-world networks as parts of sequences converging to graph limits in the same sense. 
\citet{alimohammadi2023local} showed that for 
such graphs, sampling {nodes'} {local} breadth-first search (BFS) trees asymptotically preserves critical motifs of the {graph limit} (see Appendix~\ref{sec:random-graph-limits} for an overview). Thus, in our experiments, to construct our candidate subset $\cS$ for the \emph{relaxed node subsampling problem}, we include the BFS search trees rooted at the graph's nodes up to a certain node budget.  

\begin{definition}[$k$-BFS subset] {Given $G = (V, E, f)$ and $v \in V$}, let $\ell_k$ be the deepest level such that the $v$-rooted BFS tree of depth $\ell_k$ has at most $k$ nodes (breaking ties in a fixed but arbitrary way). {The $k$-BFS subset of $v$, denoted $S_{\mathrm{BFS}(v;k)}$, is} the set of nodes at distance $\leq \ell_{k}$ from $v$ in $G$.
\end{definition}

In addition, we augment the candidate subsets with additional candidate subgraphs proposed in prior random-walk-inspired heuristics based on random walks (RW) \citep{razin2023ability} and graph cores (k-cores) \citep{salha2022degeneracy}. Concretely, given a node budget $k$, \citet{razin2023ability} provide an algorithm to construct a subset $S_{\mathrm{RW}}$ of size at most $k$; and \citet{salha2022degeneracy} provide an algorithm to construct a subset $S_{\mathrm{k-core}}$ of size at most $k$. 

Combining these three heuristics, we use: 
\begin{align*}
    \cS = \cup_{v \in V} S_{\mathrm{BFS}(v;k)} \cup \{S_{\mathrm{RW}}, S_{\mathrm{k-core}}\} 
\end{align*}
as our candidate subsets for the \emph{relaxed node subsampling problem} in our node subsampling experiments. We then apply Theorem~\ref{thm:node-subsampling-relaxed} to select among these candidate node subsets in $\cS$ using the TMD. Note that $\cS$ contains a total of $|V| + 2 = O(|V|)$ graphs, each consisting of $k$ nodes. Thus, using Theorem~\ref{thm:node-subsampling-relaxed} and Theorem~\ref{thm:node-subsampling-relaxed-algorithm}, we obtain an overall runtime of $O(L |V| |E|)$ for our node subsampling procedure.

\subsection{Random graph limits}\label{sec:random-graph-limits}

As we discussed in Section~\ref{sec:heuristic}, most real networks are scale-free and small-world, and hence well-modeled by random graph models, such as preferential attachment, configuration models and inhomogenous random graphs. As shown by \citet{van2024random}, such random graph models produce graphs that can be shown to converge \textit{locally} to a limit $(G,o)$, where $G$ is a graph to which we assign a root node $o$. By a ``transitive'' argument, it makes sense to see real networks as parts of sequences converging to graph limits in a similar sense. 

To be precise, let $\mathcal{G}_*$ be the set of all possible rooted graphs. A limit graph is defined as a measure over the space $\mathcal{G}_*$ with respect to the local metric
\begin{align*}
    d_{loc}((G_1,o_1),(G_2,o_2)) = \frac{1}{1+\inf_k\{k:B_k(G_1,o_1)\not\simeq B_k(G_2,o_2)\}}
\end{align*}
 where $B_k(G,v)$ is the $k$-hop neighborhood of node $v$, and $\simeq$ is the graph isomorphism. A sequence of graphs converging to this limit is defined as follows.

\begin{definition}[Local convergence \citep{alimohammadi2023local}] Let $G_n = (V_n, E_n)$ denote a finite connected graph. Let $(G_n, o_n)$ be the rooted graph obtained by letting $o_n \in V_n$ be chosen uniformly at random. We say that $(G_n, o_n)$ converges locally to the connected rooted graph $(G, o)$, which is a (possibly random) element of $\mathcal{G}_*$ having law $\mu$, when, for every bounded and continuous function $h: \mathcal{G}_* \to \mathbb{R}$,
$$
\mathbb{E}[h(G_n,o_n)] \to \mathbb{E}_\mu(G,o)
$$
where the expectation on the right-hand-side is with respect to $(G, o)$ having law $\mu$, while the expectation on the left-hand-side is with respect to the random vertex $o_n$.
\end{definition}

\begin{table}[ht]
\centering
\caption{Overview of Graph Datasets}
\vskip 0.1in
\begin{tabular}{lccccc}
\toprule
\textbf{Dataset} & \textbf{\#Graphs} & \textbf{Avg. Nodes} & \textbf{Avg. Edges} & \textbf{\#Classes} & \textbf{Domain}\\
\midrule
MUTAG & 188 & 17.93 & 19.79 & 2 & Molecules\\
PROTEINS & 1,113 & 39.06 & 72.82 & 2 & Molecules \\
COX2 & 467 & 43.45 & 43.45 & 2 & Molecules \\
NCI1 & 4,110 & 29.87 & 32.30 & 2 & Molecules \\
DD & 1,178 & 284.32 & 715.66 & 2 & Proteins \\
OGBG-MOLBACE & 1,513 & 34.1 & 36.9 & 2 & Molecules \\
OGBG-MOLBBBP & 2,039 & 24.1 & 26.0 & 2 & Molecules \\
\bottomrule
\end{tabular}
\label{table:graph_datasets_summary}
\end{table}

\section{Additional experiments and experimental details}\label{sec:additional-background-experiments}

In this Appendix, we cover additional experimental details and experimental results to complement our discussion in the main body. 

\subsection{Details about datasets considered in empirical evaluation}\label{sec:dataset_details}

In Table~\ref{table:graph_datasets_summary}, we provide statistics pertaining to the datasets we consider in our empirical study. 

\subsection{Additional details regarding experimental setup for Table~\ref{tab:all-results-graph-subsampling1} and \ref{tab:all-results-graph-subsampling2}}\label{sec:additional-experiment-details}

All experiments were run on an NIVIDIA A6000 GPU with 1TB of RAM. The GNNs were implemented using Pytorch Geometric \citep{he2024pytorch}.  The TMD weight function was set according to Pascal's triangle rule~\citep[][Theorem 8]{Chuang22:Tree} and our implementation of TMD was based on \citet{Chuang22:Tree}. We used the Graph Kernel Library \citep{siglidis2020grakel} for the kernel distances in our experiments.  We will make the code for all of our experiments publicly available if the paper is accepted. For now, we have included an anonymous repository link in the main body of the paper. 

For the experiments in Table~\ref{tab:all-results-graph-subsampling1} using TUDatasets \citep{morris2020tudataset} (MUTAG, PROTEINS, COX2, NCI1), we trained a graph isomorphism network (GIN) with three layers. The model was optimized using the Adam optimizer with a learning rate of 0.01 and a binary cross-entropy (BCE) loss with logits. The batch size was set to 16 for TUD datasets and 64 for OGBG datasets. Each layer contained 128 hidden channels for TUD datasets and 256 for OGBG datasets. We used a global add pooling function for aggregation, with no weight regularization or dropout applied. Additionally, batch normalization was not used in the model.








For the TUDatasets, this is the same architecture used in the original paper by \citet{Chuang22:Tree} for evaluating the TMD as a measure of GNN robustness, since all of their experiments were also on TUDatasets. For the OGBG datasets, (OGBG-MOLBACE, OGBG-MOLBBBP) because they are significantly larger in terms of the \emph{number} of graphs, we use the same architecture and hyperparameters with the exception that we set the batch size to 64 to accommodate the large number of graphs and increase training efficiency, and we used a larger number of hidden channels (2x) to handle the larger, more complex distribution over graphs. We refrained from using batch normalization in both models to best align with our theoretical analysis. Models were implemented using standard GIN implementations in PytorchGeometric \citep{fey2019fast}. We focus on the GIN architecture because it is known to come with strong theoretical expressiveness guarantees and is a common choice for achieving state-of-the-art message-passing GNN performance \citep{xu19gin}. However, as the results of \citet{Chuang22:Tree} can be extended to other architectures, such as Graph Convolutional Networks \citep{kipf2016semi}, our results can easily be extended to other message-passing architectures. 

\subsection{Definition of feature-medoids distance metric}\label{sec:additional-background-experiments-feature}

For the ``Feature'' rows of our empirical results we use the following feature-distance function for graphs $G_1 = (V_1, E_1, f_1), G_2 = (V_2, E_2, f_2)$
\begin{align*}
    D_{\mathrm{feature}}(G_1, G_2) \defeq \norm{\frac{1}{\abs{V_1}} \sum_{v \in V_1} f_1 - \frac{1}{\abs{V_2}}\sum_{v \in V_2} f_2}_2. 
\end{align*}
To interpret this formula, note that it is essentially the Euclidean distance between the \emph{average} feature value in $G_1$ and the \emph{average} feature value in $G_2$ where the average is taken across all nodes in the graph. We then run $k$-medoids clustering in the distance metric $D_\mathrm{feature}$ to construct our subsampled graph datasets. 






\subsection{Effects of validation and label usage in KiDD and DosCond}\label{sec:additional-background-experiments-vallabel}

The original KiDD and DosCond methods rely on access to graph labels for both the training dataset and a held-out validation dataset. In contrast, our approach is fully label-agnostic, requiring neither a labeled validation dataset nor labeled training data. To ensure a fair comparison, we adapt KiDD and DosCond by removing their dependence on labeled validation and training datasets. Specifically, we eliminate the validation set by selecting the GNN at the final epoch, after the default number of epochs used for the original model, rather than relying on validation-based model selection. We also remove the need for training labels by modifying the loss function to exclude label-dependent terms and initializing the sampled graphs randomly instead of using class-dependent initialization.

These adapted versions of KiDD and DosCond, which operate without labeled validation and training data, serve as the most direct comparison to our label-agnostic approach. As shown in Tables~\ref{tab:kidd} and~\ref{tab:doscond}, the removal of validation and label information in these methods each results in a measurable decline in performance. The simultaneous removal of both further exacerbates this decline, highlighting the challenges of achieving strong performance in fully label-agnostic settings. These results underscore the performance of our approach, which achieves competitive outcomes under label constraints.


\begin{table}[ht]
\centering
\caption{Performance comparison of KiDD across multiple datasets and percentages of graphs sampled. Performance is reported as mean ± standard deviation of accuracy (ACC) and area under the receiver operating curve (ROC-AUC), for configurations of KiDD with and without labels (lab.) and validation (val.).}
\vskip .1in
\label{tab:kidd}
\resizebox{1.0\textwidth}{!}{%
\begin{tabular}{cc cccccccccc}
\toprule
\multicolumn{2}{c}{} & \multicolumn{10}{c}{\textbf{\% of graphs sampled}} \\
\cmidrule(lr){3-12}
\textbf{lab.} & \textbf{val.} 
& \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} 
& \textbf{6} & \textbf{7} & \textbf{8} & \textbf{9} & \textbf{10} \\
\midrule
\multicolumn{12}{c}{\textbf{MUTAG} (ACC)} \\
\midrule

\xmark & \xmark
& 0.684±0.000 & 0.684±0.000 & 0.684±0.000 & 0.439±0.174 & 0.684±0.000
& 0.684±0.000 & 0.316±0.000 & 0.702±0.025 & 0.316±0.000 & 0.316±0.000 \\

\checkmark & \xmark
& 0.684±0.000 & 0.316±0.000 & 0.684±0.020 & 0.684±0.000 & 0.684±0.000
& 0.684±0.000 & 0.316±0.000 & 0.700±0.030 & 0.316±0.000 & 0.684±0.010 \\

\xmark & \checkmark
& 0.684±0.000 & 0.684±0.000 & 0.684±0.000 & 0.684±0.000 & 0.684±0.000
& 0.684±0.000 & 0.684±0.000 & 0.684±0.000 & 0.684±0.000 & 0.684±0.000 \\

\checkmark & \checkmark
& 0.684±0.000 & 0.684±0.000 & 0.684±0.000 & 0.684±0.000 & 0.684±0.000
& 0.754±0.099 & 0.684±0.000 & 0.684±0.000 & 0.684±0.000 & 0.737±0.043 \\
\midrule


\multicolumn{12}{c}{\textbf{COX2} (ACC)} \\
\midrule

\xmark & \xmark
& 0.170±0.000 & 0.170±0.000 & 0.390±0.311 & 0.830±0.000 & 0.830±0.000
& 0.830±0.000 & 0.830±0.000 & 0.830±0.000 & 0.830±0.000 & 0.390±0.311 \\

\checkmark & \xmark
& 0.610±0.311 & 0.390±0.311 & 0.500±0.200 & 0.610±0.311 & 0.830±0.000
& 0.830±0.000 & 0.610±0.311 & 0.390±0.311 & 0.390±0.311 & 0.610±0.311 \\

\xmark & \checkmark
& 0.390±0.311 & 0.170±0.170 & 0.170±0.000 & 0.170±0.000 & 0.830±0.000
& 0.170±0.000 & 0.170±0.000 & 0.830±0.000 & 0.170±0.000 & 0.830±0.000 \\

\checkmark & \checkmark
& 0.170±0.000 & 0.830±0.000 & 0.170±0.000 & 0.610±0.311 & 0.830±0.000
& 0.830±0.000 & 0.170±0.000 & 0.170±0.000 & 0.170±0.000 & 0.610±0.311 \\
\midrule

\multicolumn{12}{c}{\textbf{NCI1} (ACC)} \\
\midrule

\xmark & \xmark
& 0.550±0.030 & 0.555±0.028 & 0.560±0.026 
& 0.540±0.050  & 0.565±0.024 & 0.568±0.022 & 0.570±0.023 & 0.574±0.022 & 0.578±0.020 & 0.580±0.021 \\

\checkmark & \xmark
& 0.572±0.020 & 0.578±0.022 & 0.582±0.023 & 0.585±0.020 & 0.588±0.021
& 0.590±0.019 & 0.591±0.020 & 0.593±0.022 & 0.595±0.018 & 0.598±0.021 \\

\xmark & \checkmark
& 0.585±0.020 
& 0.552±0.044 
& 0.590±0.018 & 0.592±0.013 & 0.593±0.017
& 0.595±0.015 & 0.596±0.012 & 0.598±0.016 & 0.599±0.014 & 0.600±0.013 \\

\checkmark & \checkmark
& 0.602±0.013 & 0.605±0.010 & 0.608±0.015 & 0.612±0.012 & 0.613±0.016
& 0.615±0.015 & 0.615±0.013 & 0.617±0.010 & 0.619±0.011 & 0.621±0.012 \\
\midrule


\multicolumn{12}{c}{\textbf{PROTEINS} (ACC)} \\
\midrule

\xmark & \xmark
& 0.600±0.023 & 0.580±0.018 & 0.550±0.013 & 0.560±0.018 & 0.600±0.020
& 0.610±0.028 & 0.600±0.023 & 0.620±0.023 & 0.580±0.013 & 0.560±0.020 \\

\checkmark & \xmark
& 0.560±0.020 & 0.610±0.030 & 0.590±0.030 & 0.590±0.025 & 0.600±0.025
& 0.620±0.030 & 0.620±0.025 & 0.600±0.025 & 0.610±0.030 & 0.610±0.025 \\

\xmark & \checkmark
& 0.640±0.020 & 0.660±0.030 & 0.660±0.025 & 0.650±0.020 & 0.640±0.030
& 0.620±0.025 & 0.630±0.030 & 0.660±0.020 & 0.680±0.030 & 0.670±0.025 \\

\checkmark & \checkmark
& 0.660±0.030 & 0.680±0.025 & 0.710±0.030 & 0.690±0.030 & 0.700±0.030
& 0.680±0.025 & 0.660±0.020 & 0.640±0.020 & 0.690±0.030 & 0.680±0.025 \\
\midrule


\multicolumn{12}{c}{\textbf{OGBG-molbace} (ROC-AUC)} \\
\midrule

\xmark & \xmark
& 0.53±0.03 & 0.51±0.03 & 0.46±0.02 & 0.48±0.02 & 0.56±0.03
& 0.54±0.03 & 0.58±0.03 & 0.55±0.02 & 0.50±0.03 & 0.52±0.03 \\

\checkmark & \xmark
& 0.58±0.03 & 0.56±0.03 & 0.61±0.02 & 0.62±0.03 & 0.60±0.03
& 0.59±0.03 & 0.61±0.02 & 0.62±0.03 & 0.58±0.03 & 0.56±0.03 \\

\xmark & \checkmark
& 0.60±0.03 & 0.59±0.03 & 0.56±0.03 & 0.58±0.03 & 0.61±0.03
& 0.62±0.03 & 0.63±0.03 & 0.62±0.03 & 0.60±0.03 & 0.61±0.03 \\

\checkmark & \checkmark
& 0.64±0.03 & 0.66±0.03 & 0.65±0.03 & 0.64±0.03 & 0.61±0.03
& 0.63±0.03 & 0.66±0.03 & 0.67±0.03 & 0.69±0.03 & 0.68±0.03 \\
\midrule

\multicolumn{12}{c}{\textbf{OGBG-molbbp} (ROC-AUC)} \\
\midrule

\xmark & \xmark
& 0.57±0.04 & 0.57±0.05 & 0.58±0.04 & 0.58±0.05 & 0.58±0.06
& 0.58±0.05 & 0.58±0.06 & 0.59±0.07 & 0.59±0.07 & 0.59±0.08 \\

\checkmark & \xmark
& 0.59±0.04 & 0.59±0.04 & 0.60±0.05 & 0.60±0.05 & 0.60±0.06
& 0.60±0.06 
& 0.57±0.10
& 0.60±0.08 & 0.60±0.08 & 0.60±0.09 \\

\xmark & \checkmark
& 0.61±0.03 & 0.61±0.03 & 0.61±0.04 & 0.61±0.05 & 0.61±0.06
& 0.61±0.07 & 0.61±0.07 & 0.61±0.08 & 0.61±0.09 & 0.61±0.09 \\

\checkmark & \checkmark
& 0.62±0.02 & 0.62±0.04 & 0.62±0.05 & 0.62±0.06 & 0.62±0.07
& 0.58±0.15
& 0.62±0.08 & 0.63±0.10 & 0.63±0.11 & 0.63±0.12 \\
\bottomrule
\end{tabular}
}
\end{table}


\begin{table}[h]
\vskip .1in
\centering
\caption{Performance comparison of DosCond across multiple datasets and percentages of graphs sampled. Performance is reported as mean ± standard deviation of accuracy (ACC) and area under the receiver operating curve (ROC-AUC), for configurations of DosCond with and without labels (lab.) and validation (val.).}
\vspace{.1in}
\resizebox{1.0\textwidth}{!}{%
\begin{tabular}{cc cccccccccc}
\toprule
\multicolumn{2}{c}{} & \multicolumn{10}{c}{\textbf{\% of graphs sampled}} \\
\cmidrule(lr){3-12}
\textbf{lab.} & \textbf{val.} 
& \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} 
& \textbf{6} & \textbf{7} & \textbf{8} & \textbf{9} & \textbf{10} \\
\midrule
\multicolumn{12}{c}{\textbf{MUTAG} (ACC)} \\
\midrule

\xmark & \xmark
& 0.671±0.003 & 0.736±0.027 & 0.731±0.008 & 0.727±0.000 & 0.709±0.008
& 0.702±0.008 & 0.696±0.021 & 0.680±0.009 & 0.716±0.006 & 0.704±0.011 \\

\checkmark & \xmark
& 0.682±0.015 & 0.730±0.020 & 0.726±0.018 & 0.719±0.016 & 0.698±0.018
& 0.691±0.021 & 0.684±0.020 & 0.679±0.015 & 0.712±0.015 & 0.705±0.020 \\

\xmark & \checkmark
& 0.656±0.035 & 0.687±0.009 & 0.687±0.024 & 0.698±0.018 & 0.667±0.022
& 0.698±0.016 & 0.700±0.020 & 0.707±0.019 & 0.704±0.017 & 0.684±0.023 \\

\checkmark & \checkmark
& 0.707±0.020 & 0.693±0.005 & 0.687±0.025 & 0.704±0.027 & 0.667±0.020
& 0.691±0.028 & 0.700±0.016 & 0.704±0.017 & 0.702±0.019 & 0.693±0.036 \\
\midrule

\multicolumn{12}{c}{\textbf{COX2} (ACC)} \\
\midrule

\xmark & \xmark
& 0.439±0.142 & 0.217±0.004 & 0.248±0.031 & 0.254±0.025 & 0.772±0.017
& 0.394±0.077 & 0.650±0.090 & 0.219±0.003 & 0.216±0.003 & 0.215±0.000 \\

\checkmark & \xmark
& 0.600±0.060 & 0.456±0.033 & 0.500±0.050 & 0.600±0.044 & 0.700±0.033
& 0.650±0.022 & 0.720±0.040 & 0.500±0.056 & 0.550±0.030 & 0.750±0.022 \\

\xmark & \checkmark
& 0.754±0.021 & 0.760±0.009 & 0.678±0.080 & 0.678±0.083 & 0.755±0.034
& 0.776±0.014 & 0.786±0.000 & 0.763±0.028 & 0.765±0.022 & 0.783±0.002 \\

\checkmark & \checkmark
& 0.744±0.051 & 0.769±0.024 & 0.783±0.004 & 0.787±0.005 & 0.780±0.006
& 0.784±0.003 & 0.780±0.016 & 0.759±0.034 & 0.784±0.005 & 0.784±0.007 \\
\midrule

\multicolumn{12}{c}{\textbf{NCI1} (ACC)} \\
\midrule
%---- (no val, no labels)
\xmark & \xmark
& 0.503±0.022 & 0.510±0.025 & 0.490±0.030
& 0.520±0.028 & 0.521±0.021
& 0.535±0.019 & 0.537±0.025 & 0.540±0.022 
& 0.512±0.040 
& 0.545±0.026 \\
%---- (no val, labels)
\checkmark & \xmark
& 0.521±0.031 & 0.530±0.028 & 0.570±0.022 & 0.512±0.039
& 0.555±0.021
& 0.560±0.033 & 0.526±0.040
& 0.575±0.020 & 0.576±0.019 & 0.580±0.022 \\

\xmark & \checkmark
& 0.540±0.020 & 0.550±0.025 & 0.555±0.019 & 0.557±0.021 
& 0.518±0.035
& 0.560±0.018 & 0.563±0.024 & 0.580±0.020 & 0.578±0.026 & 0.590±0.022 \\

\checkmark & \checkmark
& 0.562±0.012 & 0.560±0.015 
& 0.570±0.018 & 0.575±0.021 & 0.583±0.020
& 0.555±0.030
& 0.590±0.024 & 0.605±0.019 & 0.608±0.022 & 0.612±0.018 \\
\midrule


\multicolumn{12}{c}{\textbf{PROTEINS} (ACC)} \\
\midrule

\xmark & \xmark
& 0.481±0.033 & 0.670±0.009 & 0.651±0.010 & 0.663±0.004 & 0.551±0.016
& 0.634±0.003 & 0.610±0.013 & 0.645±0.006 & 0.610±0.006 & 0.628±0.009 \\

\checkmark & \xmark
& 0.620±0.010 & 0.630±0.020 & 0.645±0.025 & 0.660±0.018 & 0.583±0.012
& 0.633±0.017 & 0.650±0.020 & 0.655±0.012 & 0.650±0.023 & 0.645±0.015 \\

\xmark & \checkmark
& 0.647±0.010 & 0.653±0.009 & 0.647±0.017 & 0.668±0.003 & 0.658±0.020
& 0.641±0.002 & 0.669±0.005 & 0.647±0.016 & 0.642±0.023 & 0.663±0.007 \\

\checkmark & \checkmark
& 0.661±0.011 & 0.643±0.033 & 0.633±0.003 & 0.681±0.027 & 0.655±0.004
& 0.637±0.011 & 0.653±0.003 & 0.658±0.006 & 0.653±0.003 & 0.664±0.002 \\
\midrule

\multicolumn{12}{c}{\textbf{OGBG-MOLBACE} (ROC-AUC)} \\
\midrule

\xmark & \xmark
& 0.53±0.02 & 0.45±0.04 & 0.64±0.03 & 0.60±0.02 & 0.37±0.01
& 0.56±0.06 & 0.62±0.02 & 0.54±0.01 & 0.60±0.03 & 0.62±0.03 \\

\checkmark & \xmark
& 0.62±0.02 & 0.61±0.02 & 0.65±0.01 & 0.65±0.03 & 0.60±0.03
& 0.64±0.02 & 0.63±0.02 & 0.65±0.03 & 0.64±0.02 & 0.66±0.02 \\
\xmark & \checkmark
& 0.68±0.03 & 0.66±0.02 & 0.64±0.01 & 0.64±0.03 & 0.62±0.02
& 0.67±0.01 & 0.65±0.02 & 0.65±0.03 & 0.66±0.01 & 0.65±0.04 \\

\checkmark & \checkmark
& 0.67±0.01 & 0.67±0.03 & 0.64±0.01 & 0.67±0.01 & 0.65±0.03
& 0.68±0.02 & 0.66±0.02 & 0.66±0.01 & 0.67±0.04 & 0.66±0.02 \\
\midrule

\multicolumn{12}{c}{\textbf{OGBG-MOLBBBP} (ROC-AUC)} \\
\midrule

\xmark & \xmark
& 0.43±0.02 & 0.44±0.02 & 0.30±0.02
& 0.45±0.02 & 0.46±0.03
& 0.46±0.03 & 0.32±0.01 
& 0.47±0.03 & 0.47±0.04 & 0.48±0.03 \\

\checkmark & \xmark
& 0.45±0.03 & 0.46±0.03 & 0.46±0.03 & 0.40±0.02 
& 0.46±0.03
& 0.47±0.03 & 0.48±0.03 & 0.44±0.03 
& 0.48±0.03 & 0.49±0.03 \\

\xmark & \checkmark
& 0.48±0.02 & 0.48±0.02 & 0.49±0.02 & 0.49±0.02
& 0.50±0.03 & 0.47±0.03 
& 0.50±0.02 & 0.50±0.02 & 0.51±0.02 & 0.51±0.02 \\

\checkmark & \checkmark
& 0.51±0.01 & 0.51±0.02
& 0.52±0.02 & 0.52±0.02 
& 0.40±0.02 
& 0.54±0.02 & 0.55±0.02 & 0.56±0.02 & 0.58±0.02 & 0.62±0.02 \\
\bottomrule
\end{tabular}%
}
\end{table}


\subsection{Performance of MIRAGE}\label{sec:additional-background-experiments-mirage}

Table~\ref{tab:mirage} reports the performance of Mirage \citep{mirage} on NCI1 and OGBG-MOLBACE for certain subsample percentages, which were the only cases where we were able to execute the publicly available referenced in the original paper. Unresolved errors were encountered when running Mirage on MUTAG, PROTEINS, NCI1, DD, OGBG-MOLBBBP, and the omitted percentages in Table~\ref{tab:mirage}, due to the MP Tree search either returning an empty selection set or generating trees in a format incompatible with GNN training.

Unlike our method, MIRAGE is not label-agnostic, as it explicitly relies on a labeled validation set with an 80\%-train / 10\%-validation / 10\%-test split. Despite this supervision, MIRAGE underperforms compared to TMD and other methods, typically achieving accuracy below 0.5 for NCI1 and ROC-AUC below 0.5 for OGBG-MOLBBBP.


\begin{table}[ht]
\centering

\caption{Performance of Mirage on the NCI1 and molbbp datasets across percentages of graphs sampled, using the original implementation with labels and validation. Performance is reported as mean ± standard deviation of accuracy (ACC) and area under the receiver operating curve (ROC-AUC). Results for '--', other datasets, and configurations without labels or without validation could not be generated due to unresolved errors in execution.}
\vskip .1in
\label{tab:doscond}
\resizebox{1.0\textwidth}{!}{%
\begin{tabular}{cc cccccccccc}
\toprule
\multicolumn{1}{c}{} & \multicolumn{10}{c}{\textbf{\% of graphs sampled}} \\
\cmidrule(lr){2-11}

\textbf{Dataset} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} 
& \textbf{6} & \textbf{7} & \textbf{8} & \textbf{9} & \textbf{10} \\
\midrule

\makecell{\textbf{NCI1} (ACC)} & 0.509±0.021 & 0.513±0.010 & -- & 0.492±0.015 & 0.531±0.022
& 0.488±0.030 & -- & -- & 0.488±0.022 & 0.501±0.018 \\
\midrule

\makecell{\textbf{OGBG-MOLBBBP} (ROC-AUC)} & 0.51±0.0 & 0.42±0.1 & 0.49±0.0 & 0.41±0.0 & 0.42±0.1 
& 0.58±0.0 & 0.43±0.1 & 0.21±0.0 & 0.41±0.0 & 0.37±0.2 \\
\bottomrule
\end{tabular}%
}
\label{tab:mirage}
\end{table}

%\FloatBarrier

\subsection{Demonstration of generalization to alternative GNN architecture}\label{def:generalize-architecture}

To demonstrate the effect of modifying the model architecture, we also consider the effect of increasing the size of the neural network--from 128 neurons to 256 for TUDatasets (MUTAG, PROTEINS, COX2, NCI1), and from 256 to 512 for OGBG--and modify the aggregation function to global mean pool, which is another popular choice of pooling function in the applied GNN literature. 

\paragraph{Graph subsampling} Because the graph condensation methods are significantly more time-intensive and require re-running the entire distillation process for a new model architecture, we did not run KiDD and DOSCOND on this alternative architecture. 

Our results for all of the medoids-methods as well as Random are shown in Table~\ref{tab:performance_comparison_large}. We use the same experimental setup as for Table~\ref{tab:all-results-graph-subsampling1} and \ref{tab:all-results-graph-subsampling2}, except for modifying the pooling layer and number of neurons per layer, as described above. Each entry reports the mean performance (measured in test accuracy for the TUDatasets and and test AUC-ROC for the OGBG datasets) and 95\% confidence bars across 20 trials, randomized over train/test splits as well as neural network initialization. 

Combined with Table~\ref{tab:all-results-graph-subsampling1} and \ref{tab:all-results-graph-subsampling2} in the main body, our results indicate that our method can perform well on different-sized neural networks with different aggregation functions, despite not factoring this information into the choice of the graph subsamples. 

\paragraph{Node subsampling} We use the same experimental setup as for Table~\ref{tab:combined-results-nodes}, except for modifying the pooling layer and number of neurons per layer, as previously described. 

Results are shown in Table~\ref{tab:combined-results-nodes-2}. Each entry reports the mean and 95\% confidence bars across 20 trials, randomized over train/test splits as well as neural network initialization. Combined with Table~\ref{tab:combined-results-nodes} in the main body, our results indicate that our method can perform well on different-sized neural networks with different aggregation functions, despite not factoring this information into the choice of the node subsamples.

\begin{table}[ht]
\caption{Performance comparison for different methods across the percentage of subsampled graphs and various datasets on alternative GIN archiecture. Performance for MUTAG, PROTEINS, COX2, NCI1 is reported in test-accuracy. Performance for OGBG-MOLBACE and OGBG-MOLBBP is reported in test-AUC-ROC. TMD almost always performs best or second-best among the compared methods.  Dark and light green highlight best and second best performance in each row, respectively. All performances are reported as average $\pm$ 95\% confidence bars.}
\vspace{.1in}
\centering
\resizebox{1.0\textwidth}{!}{
\scriptsize
\begin{tabular}{lccccccccccc}
\toprule
Method & 1\% & 2\% & 3\% & 4\% & 5\% & 6\% & 7\% & 8\% & 9\% & 10\% \\
\midrule
\multicolumn{11}{c}{\textbf{MUTAG}} \\
\midrule
TMD & 0.58±0.07 & \cellcolor{green!80}{0.71±0.03} & \cellcolor{green!80}{0.71±0.02} & \cellcolor{green!25}{0.71±0.04} & \cellcolor{green!80}{0.76±0.04} & \cellcolor{green!80}{0.77±0.03} & \cellcolor{green!80}{0.77±0.03} & \cellcolor{green!25}{0.74±0.06} & \cellcolor{green!80}{0.75±0.04} & \cellcolor{green!80}{0.76±0.02} \\
WL & \cellcolor{green!80}{0.67±0.05} & \cellcolor{green!80}{0.71±0.04} & \cellcolor{green!25}{0.70±0.02} & \cellcolor{green!80}{0.72±0.03} & 0.72±0.03 & \cellcolor{green!25}{0.69±0.03} & 0.69±0.04 & 0.69±0.04 & \cellcolor{green!25}{0.73±0.03} & 0.72±0.04 \\
Random & \cellcolor{green!25}{0.65±0.05} & \cellcolor{green!25}{0.65±0.06} & 0.66±0.07 & 0.67±0.03 & 0.72±0.04 & \cellcolor{green!25}{0.69±0.04} & 0.72±0.04 & 0.73±0.03 & \cellcolor{green!80}{0.75±0.04} & 0.72±0.03 \\
Feature & 0.55±0.07 & 0.62±0.08 & \cellcolor{green!80}{0.71±0.03} & 0.69±0.04 & \cellcolor{green!25}{0.74±0.04} & \cellcolor{green!80}{0.77±0.03} & \cellcolor{green!25}{0.76±0.02} & \cellcolor{green!80}{0.75±0.02} & \cellcolor{green!25}{0.73±0.03} & \cellcolor{green!25}{0.74±0.03} \\
\midrule
\multicolumn{11}{c}{\textbf{PROTEINS}} \\
\midrule
TMD & \cellcolor{green!80}{0.63±0.03} & \cellcolor{green!25}{0.64±0.04} & \cellcolor{green!80}{0.69±0.02} & \cellcolor{green!80}{0.70±0.02} & \cellcolor{green!25}{0.67±0.03} & \cellcolor{green!80}{0.69±0.01} & \cellcolor{green!25}{0.69±0.02} & \cellcolor{green!80}{0.69±0.02} & \cellcolor{green!25}{0.70±0.02} & \cellcolor{green!25}{0.70±0.02} \\
WL & \cellcolor{green!25}{0.62±0.02} & \cellcolor{green!80}{0.65±0.03} & \cellcolor{green!25}{0.68±0.02} & \cellcolor{green!25}{0.68±0.02} & \cellcolor{green!80}{0.68±0.01} & \cellcolor{green!25}{0.67±0.02} & 0.68±0.02 & \cellcolor{green!80}{0.69±0.02} & 0.66±0.04 & 0.67±0.02 \\
Random & 0.61±0.02 & \cellcolor{green!25}{0.64±0.02} & 0.64±0.02 & 0.64±0.04 & \cellcolor{green!25}{0.67±0.02} & 0.65±0.02 & \cellcolor{green!25}{0.69±0.02} & \cellcolor{green!25}{0.67±0.02} & 0.68±0.02 & 0.66±0.03 \\
Feature & \cellcolor{green!25}{0.62±0.03} & 0.60±0.04 & 0.64±0.03 & 0.65±0.04 & \cellcolor{green!80}{0.68±0.03} & \cellcolor{green!25}{0.67±0.03} & \cellcolor{green!80}{0.70±0.02} & 0.66±0.04 & \cellcolor{green!80}{0.71±0.02} & \cellcolor{green!80}{0.72±0.01} \\
\midrule
\multicolumn{11}{c}{\textbf{COX2}} \\
\midrule
TMD & \cellcolor{green!80}{0.70±0.04} & \cellcolor{green!80}{0.76±0.02} & \cellcolor{green!80}{0.75±0.03} & \cellcolor{green!80}{0.78±0.02} & \cellcolor{green!80}{0.78±0.02} & \cellcolor{green!80}{0.77±0.01} & \cellcolor{green!80}{0.77±0.01} & \cellcolor{green!80}{0.78±0.02} & \cellcolor{green!25}{0.77±0.02} & \cellcolor{green!80}{0.78±0.02} \\
WL & 0.57±0.06 & 0.70±0.04 & \cellcolor{green!80}{0.75±0.02} & \cellcolor{green!25}{0.76±0.02} & \cellcolor{green!25}{0.77±0.02} & \cellcolor{green!80}{0.77±0.02} & \cellcolor{green!80}{0.77±0.02} & \cellcolor{green!25}{0.77±0.02} & \cellcolor{green!80}{0.78±0.02} & \cellcolor{green!80}{0.78±0.01} \\
Random & \cellcolor{green!25}{0.65±0.07} & 0.70±0.04 & 0.69±0.05 & 0.68±0.06 & 0.76±0.04 & 0.74±0.04 & \cellcolor{green!25}{0.74±0.03} & 0.74±0.04 & 0.75±0.06 & \cellcolor{green!25}{0.77±0.03} \\
Feature & \cellcolor{green!25}{0.65±0.06} & \cellcolor{green!25}{0.72±0.05} & \cellcolor{green!25}{0.71±0.04} & 0.72±0.02 & \cellcolor{green!80}{0.78±0.01} & \cellcolor{green!25}{0.75±0.03} & \cellcolor{green!25}{0.74±0.04} & 0.73±0.03 & \cellcolor{green!80}{0.78±0.02} & 0.76±0.02 \\
\midrule
\multicolumn{11}{c}{\textbf{NCI1}} \\
\midrule
TMD & 0.52 ± 0.01 & \cellcolor{green!25}{0.54 ± 0.02} & 0.53 ± 0.02 & \cellcolor{green!25}{0.51 ± 0.01} & \cellcolor{green!25}{0.52 ± 0.01} & 0.54 ± 0.02 & 0.55 ± 0.02 & \cellcolor{green!25}{0.52 ± 0.01} & 0.54 ± 0.02 & 0.53 ± 0.02 \\
WL & 0.50 ± 0.01 & 0.51 ± 0.00 & 0.52 ± 0.01 & \cellcolor{green!25}{0.51 ± 0.01} & 0.51 ± 0.01 & 0.50 ± 0.00 & 0.50 ± 0.01 & 0.50 ± 0.01 & 0.52 ± 0.01 & 0.51 ± 0.01 \\
Random & \cellcolor{green!80}{0.56 ± 0.02} & \cellcolor{green!80}{0.60 ± 0.02} & \cellcolor{green!25}{0.60 ± 0.02} & \cellcolor{green!80}{0.61 ± 0.00} & \cellcolor{green!80}{0.63 ± 0.01} & \cellcolor{green!80}{0.62 ± 0.01} & \cellcolor{green!80}{0.63 ± 0.01} & \cellcolor{green!80}{0.62 ± 0.01} & \cellcolor{green!25}{0.62 ± 0.02} & \cellcolor{green!80}{0.64 ± 0.01} \\
Feature & \cellcolor{green!25}{0.53 ± 0.01} & \cellcolor{green!80}{0.60 ± 0.02} & \cellcolor{green!80}{0.61 ± 0.01} & \cellcolor{green!80}{0.61 ± 0.02} & \cellcolor{green!80}{0.63 ± 0.01} & \cellcolor{green!25}{0.60 ± 0.02} & \cellcolor{green!25}{0.62 ± 0.02} & \cellcolor{green!80}{0.62 ± 0.01} & \cellcolor{green!80}{0.63 ± 0.00} & \cellcolor{green!25}{0.62 ± 0.01} \\
\midrule
\multicolumn{11}{c}{\textbf{OGBG-MOLBACE}} \\
\midrule
TMD & 0.45±0.01 & \cellcolor{green!80}{0.52±0.03} & \cellcolor{green!80}{0.55±0.04} & 0.49±0.02 & \cellcolor{green!25}{0.54±0.03} & \cellcolor{green!80}{0.54±0.03} & \cellcolor{green!80}{0.58±0.03} & \cellcolor{green!80}{0.60±0.03} & \cellcolor{green!80}{0.57±0.02} & \cellcolor{green!80}{0.58±0.02} \\
WL & 0.48±0.01 & \cellcolor{green!25}{0.49±0.02} & 0.48±0.02 & \cellcolor{green!25}{0.51±0.03} & 0.48±0.02 & 0.50±0.02 & 0.53±0.03 & 0.50±0.03 & 0.55±0.03 & 0.53±0.03 \\
Random & \cellcolor{green!25}{0.49±0.02} & \cellcolor{green!25}{0.49±0.03} & 0.51±0.03 & 0.49±0.02 & 0.48±0.03 & 0.50±0.04 & 0.48±0.02 & 0.48±0.02 & 0.49±0.03 & 0.51±0.03 \\
Feature & \cellcolor{green!80}{0.50±0.03} & \cellcolor{green!80}{0.52±0.03} & \cellcolor{green!25}{0.53±0.02} & \cellcolor{green!80}{0.55±0.02} & \cellcolor{green!80}{0.55±0.02} & \cellcolor{green!25}{0.51±0.02} & \cellcolor{green!25}{0.55±0.02} & \cellcolor{green!25}{0.56±0.03} & \cellcolor{green!25}{0.56±0.02} & \cellcolor{green!25}{0.56±0.02} \\
\midrule
\multicolumn{11}{c}{\textbf{OGBG-MOLBBBP}} \\
\midrule
TMD & 0.69 ± 0.07 & 0.67 ± 0.07 & \cellcolor{green!25}{0.75 ± 0.02} & \cellcolor{green!80}{0.76 ± 0.00} & \cellcolor{green!80}{0.76 ± 0.01} & \cellcolor{green!80}{0.77 ± 0.02} & \cellcolor{green!25}{0.77 ± 0.01} & \cellcolor{green!25}{0.76 ± 0.01} & \cellcolor{green!25}{0.76 ± 0.01} & 0.76 ± 0.01 \\
WL & 0.44 ± 0.10 & \cellcolor{green!25}{0.74 ± 0.04} & 0.73 ± 0.05 & \cellcolor{green!25}{0.75 ± 0.03} & \cellcolor{green!80}{0.76 ± 0.01} & \cellcolor{green!25}{0.76 ± 0.01} & \cellcolor{green!80}{0.78 ± 0.01} & \cellcolor{green!25}{0.76 ± 0.01} & \cellcolor{green!80}{0.78 ± 0.01} & \cellcolor{green!80}{0.78 ± 0.00} \\
Random & \cellcolor{green!25}{0.74 ± 0.02} & 0.65 ± 0.08 & 0.68 ± 0.07 & \cellcolor{green!80}{0.76 ± 0.01} & \cellcolor{green!80}{0.76 ± 0.01} & 0.72 ± 0.07 & 0.67 ± 0.09 & 0.73 ± 0.04 & \cellcolor{green!25}{0.76 ± 0.00} & \cellcolor{green!25}{0.77 ± 0.01} \\
Feature & \cellcolor{green!80}{0.78 ± 0.01} & \cellcolor{green!80}{0.77 ± 0.01} & \cellcolor{green!80}{0.77 ± 0.01} & 0.71 ± 0.07 & \cellcolor{green!80}{0.76 ± 0.02} & 0.72 ± 0.07 & 0.76 ± 0.01 & \cellcolor{green!80}{0.77 ± 0.01} & 0.72 ± 0.07 & \cellcolor{green!80}{0.78 ± 0.00} \\
\bottomrule
\end{tabular}
\label{tab:performance_comparison_large}
}
\end{table}




\begin{table*}[t]
\scriptsize
\caption{ Performance comparison for different methods across the percentage of subsampled graphs and various datasets on alternative GIN archiecture. All performances are reported in test accuracy. TMD tends to perform better than other methods, but is comparable with RW on COX-2 and PROTEINS on this alternative architecture.  Dark and light green highlight best and second best performance in each row, respectively. All results display average $\pm$ 95\% confidence bars. }
\vspace{0.1in}
\centering

\begin{subtable}{\textwidth}
\centering
\resizebox{1.0\textwidth}{!}{%
\begin{tabular}{l|ccccccccc|c}
\hline
\textbf{Method} & \textbf{1\%} & \textbf{2\%} & \textbf{3\%} & \textbf{4\%} & \textbf{5\%} & \textbf{6\%} & \textbf{7\%} & \textbf{8\%} & \textbf{9\%} & \textbf{Wins} \\
\hline
\textbf{RW}
  & \cellcolor{green!25}{0.66±0.04} 
  & \cellcolor{green!80}{0.67±0.03} 
  & 0.66±0.03 
  & 0.66±0.04
  & 0.61±0.06
  & 0.64±0.07
  & \cellcolor{green!25}{0.72±0.03}
  & \cellcolor{green!25}{0.75±0.04}
  & 0.79±0.04
  & 1 \\
\textbf{k-cores}
  & 0.59±0.07 
  & \cellcolor{green!25}{0.63±0.04} 
  & 0.45±0.07 
  & \cellcolor{green!25}{0.69±0.03}
  & \cellcolor{green!25}{0.63±0.06}
  & 0.62±0.08
  & \cellcolor{green!25}{0.72±0.03}
  & 0.74±0.04
  & 0.74±0.05
  & 0 \\
\textbf{Random}
  & \cellcolor{green!80}{0.70±0.03}
  & \cellcolor{green!80}{0.67±0.03} 
  & \cellcolor{green!80}{0.70±0.03} 
  & \cellcolor{green!80}{0.70±0.03}
  & \cellcolor{green!80}{0.68±0.04}
  & \cellcolor{green!25}{0.67±0.04}
  & 0.71±0.03
  & \cellcolor{green!25}{0.75±0.03}
  & \cellcolor{green!25}{0.81±0.03}
  & 5 \\
\textbf{TMD}
  & 0.42±0.07 
  & \cellcolor{green!80}{0.67±0.03} 
  & \cellcolor{green!25}{0.68±0.04} 
  & 0.68±0.02
  & \cellcolor{green!80}{0.68±0.04}
  & \cellcolor{green!80}{0.74±0.03}
  & \cellcolor{green!80}{0.75±0.04}
  & \cellcolor{green!80}{0.80±0.02}
  & \cellcolor{green!80}{0.83±0.03}
  & 6 \\
\hline
\end{tabular}
}
\caption{MUTAG}
\label{tab:mutag-node-2-pivot}
\end{subtable}

\vspace{1em}


\begin{subtable}{\textwidth}
\centering
\resizebox{1.0\textwidth}{!}{%
\begin{tabular}{l|ccccccccc|c}
\hline
\textbf{Method} & \textbf{1\%} & \textbf{2\%} & \textbf{3\%} & \textbf{4\%} & \textbf{5\%} & \textbf{6\%} & \textbf{7\%} & \textbf{8\%} & \textbf{9\%} & \textbf{Wins} \\
\hline
\textbf{RW}
  & \cellcolor{green!80}{0.61±0.01}
  & \cellcolor{green!80}{0.60±0.01}
  & \cellcolor{green!80}{0.63±0.01}
  & 0.63±0.01
  & \cellcolor{green!25}{0.67±0.02}
  & \cellcolor{green!25}{0.69±0.01}
  & \cellcolor{green!25}{0.68±0.02}
  & \cellcolor{green!80}{0.72±0.01}
  & \cellcolor{green!80}{0.70±0.02}
  & 5 \\
\textbf{k-cores}
  & 0.59±0.01
  & \cellcolor{green!80}{0.60±0.01}
  & 0.60±0.01
  & \cellcolor{green!25}{0.64±0.02}
  & 0.65±0.01
  & 0.68±0.02
  & 0.68±0.02
  & \cellcolor{green!25}{0.69±0.02}
  & \cellcolor{green!25}{0.69±0.02}
  & 1 \\
\textbf{Random}
  & \cellcolor{green!25}{0.60±0.01}
  & \cellcolor{green!80}{0.60±0.01}
  & \cellcolor{green!25}{0.62±0.02}
  & \cellcolor{green!80}{0.65±0.02}
  & \cellcolor{green!25}{0.67±0.02}
  & \cellcolor{green!25}{0.69±0.02}
  & \cellcolor{green!80}{0.69±0.02}
  & 0.68±0.02
  & \cellcolor{green!80}{0.70±0.02}
  & 4 \\
\textbf{TMD}
  & \cellcolor{green!25}{0.60±0.01}
  & \cellcolor{green!80}{0.60±0.01}
  & 0.61±0.01
  & \cellcolor{green!25}{0.64±0.01}
  & \cellcolor{green!80}{0.67±0.01}
  & \cellcolor{green!80}{0.70±0.02}
  & \cellcolor{green!80}{0.70±0.02}
  & \cellcolor{green!25}{0.69±0.02}
  & \cellcolor{green!80}{0.70±0.03}
  & 5 \\
\hline
\end{tabular}
}
\caption{PROTEINS}
\label{tab:proteins-node-2-pivot}
\end{subtable}

\vspace{1em}


\begin{subtable}{\textwidth}
\centering
\resizebox{1.0\textwidth}{!}{%
\begin{tabular}{l|ccccccccc|c}
\hline
\textbf{Method} & \textbf{1\%} & \textbf{2\%} & \textbf{3\%} & \textbf{4\%} & \textbf{5\%} & \textbf{6\%} & \textbf{7\%} & \textbf{8\%} & \textbf{9\%} & \textbf{Wins} \\
\hline
\textbf{RW}
  & \cellcolor{green!25}{0.58±0.01}
  & \cellcolor{green!80}{0.59±0.01}
  & \cellcolor{green!80}{0.61±0.01}
  & \cellcolor{green!25}{0.64±0.02}
  & \cellcolor{green!80}{0.67±0.02}
  & 0.65±0.02
  & \cellcolor{green!25}{0.72±0.02}
  & \cellcolor{green!25}{0.73±0.02}
  & 0.73±0.03
  & 3 \\
\textbf{k-cores}
  & \cellcolor{green!80}{0.59±0.01}
  & \cellcolor{green!80}{0.59±0.01}
  & 0.59±0.02
  & 0.59±0.01
  & 0.60±0.01
  & 0.59±0.01
  & 0.59±0.01
  & 0.60±0.01
  & 0.59±0.01
  & 2 \\
\textbf{Random}
  & \cellcolor{green!80}{0.59±0.01}
  & \cellcolor{green!25}{0.58±0.01}
  & \cellcolor{green!25}{0.60±0.01}
  & \cellcolor{green!25}{0.64±0.02}
  & \cellcolor{green!25}{0.66±0.02}
  & \cellcolor{green!80}{0.69±0.02}
  & 0.71±0.02
  & \cellcolor{green!80}{0.74±0.01}
  & \cellcolor{green!25}{0.74±0.01}
  & 3 \\
\textbf{TMD}
  & \cellcolor{green!25}{0.58±0.01}
  & \cellcolor{green!80}{0.59±0.01}
  & \cellcolor{green!25}{0.60±0.01}
  & \cellcolor{green!80}{0.65±0.01}
  & 0.65±0.01
  & \cellcolor{green!25}{0.67±0.03}
  & \cellcolor{green!80}{0.73±0.02}
  & 0.71±0.03
  & \cellcolor{green!80}{0.75±0.02}
  & 4 \\
\hline
\end{tabular}
}
\caption{DD}
\label{tab:dd-node-2-pivot}
\end{subtable}

\vspace{1em}


\begin{subtable}{\textwidth}
\centering
\resizebox{1.0\textwidth}{!}{%
\begin{tabular}{l|ccccccccc|c}
\hline
\textbf{Method} & \textbf{1\%} & \textbf{2\%} & \textbf{3\%} & \textbf{4\%} & \textbf{5\%} & \textbf{6\%} & \textbf{7\%} & \textbf{8\%} & \textbf{9\%} & \textbf{Wins} \\
\hline
\textbf{RW}
  & \cellcolor{green!25}{0.78±0.01}
  & 0.77±0.01
  & \cellcolor{green!80}{0.79±0.01}
  & \cellcolor{green!80}{0.80±0.02}
  & \cellcolor{green!80}{0.79±0.02}
  & \cellcolor{green!80}{0.78±0.02}
  & \cellcolor{green!25}{0.79±0.02}
  & \cellcolor{green!80}{0.79±0.02}
  & 0.77±0.02
  & 5 \\
\textbf{k-cores}
  & \cellcolor{green!25}{0.78±0.01}
  & \cellcolor{green!25}{0.78±0.02}
  & 0.77±0.01
  & \cellcolor{green!25}{0.79±0.02}
  & 0.74±0.05
  & \cellcolor{green!80}{0.78±0.01}
  & 0.77±0.02
  & \cellcolor{green!25}{0.78±0.02}
  & \cellcolor{green!80}{0.79±0.02}
  & 2 \\
\textbf{Random}
  & \cellcolor{green!80}{0.79±0.02}
  & \cellcolor{green!25}{0.78±0.02}
  & \cellcolor{green!25}{0.78±0.01}
  & \cellcolor{green!25}{0.79±0.02}
  & \cellcolor{green!80}{0.79±0.01}
  & \cellcolor{green!80}{0.78±0.02}
  & 0.77±0.02
  & \cellcolor{green!25}{0.78±0.01}
  & \cellcolor{green!25}{0.78±0.01}
  & 3 \\
\textbf{TMD}
  & \cellcolor{green!80}{0.79±0.02}
  & \cellcolor{green!80}{0.79±0.02}
  & \cellcolor{green!25}{0.78±0.01}
  & 0.77±0.02
  & \cellcolor{green!25}{0.78±0.01}
  & \cellcolor{green!80}{0.78±0.02}
  & \cellcolor{green!80}{0.80±0.02}
  & \cellcolor{green!25}{0.78±0.03}
  & \cellcolor{green!80}{0.79±0.02}
  & 5 \\
\hline
\end{tabular}
}
\caption{COX2}
\label{tab:cox2-node-2-pivot}
\end{subtable}
\label{tab:combined-results-nodes-2}
\end{table*}


