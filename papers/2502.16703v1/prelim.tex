\section{Notation and Background}\label{sec:prelim}

\paragraph{Notation.} An undirected graph $G = (V, E, f)$ consists of a node set $V$, an edge set $E$, and node features $f \in \mathbb{R}^{p \times |V|}$. Each node $v \in V$ has an associated feature vector $f^v \in \mathbb{R}^p$. The neighborhood of a node $v \in V$ is denoted by $N_G(v)$. We use $\mathcal{G}$ to represent a set of graphs. An $r$-rooted tree is denoted as $T = (V, E, f, r)$. The all-ones vector, all-zeros vector, and identity matrix are represented by $\bm{1}$, $\bm{0}$, and $\bm{I}$, respectively. The notation $\normInline{\cdot}$ is used to denote an arbitrary norm. Visualizations of definitions are provided in Appendix~\ref{sec:notation}.


\paragraph{Message-Passing Graph Neural Networks (GNNs).} We describe the \emph{Graph Isomorphism Network (GIN)} \citep{xu19gin} as a representative example of a GNN architecture, though the results presented extend to other architectures. A GIN is parameterized by $L$ functions $\phi^{(\ell)}: \mathbb{R}^d \to \mathbb{R}^d$, typically implemented as shallow feed-forward neural networks, and a fixed weight $\eta > 0$. Each node is initialized with an embedding $z_v^{(0)} = f^v$ and updates its embedding as follows:
\begin{align*}
    &\text{\textbf{Message passing:} for } \ell \in [L-1], \quad
    z_v^{(\ell)} = \phi^{(\ell)} \Big(z_v^{(\ell-1)} + \eta \sum\nolimits_{u \in N(v)} z_u^{(\ell-1)}\Big), \\
    &\text{\textbf{Graph readout:} } \quad
    h(G) = \phi^{(L)} \Big(\sum\nolimits_{v \in V} z_v^{(L-1)}\Big).
\end{align*}
Other GNN architectures may generalize this framework by replacing the sum operator with an alternative aggregation function, among other variations.

\paragraph{Tree Mover's Distance (TMD).}  
TMD \citep{Chuang22:Tree} quantifies graph similarity by representing each graph as a multiset of \emph{computation trees}. A node's computation tree is constructed by first including its immediate neighbors at the first level, followed by their neighbors at the next level, continuing recursively up to a predefined depth. TMD then measures the similarity between graphs by comparing their multisets of computation trees using optimal transport (OT).


\begin{figure}[t]
  \centering
  \includegraphics[width=.4\textwidth]{figures/computation_tree.png}
  \caption{A graph $G$ and the depth-2 computation tree $T = T^2_r(G)$ of node $r$ (Definition~\ref{def:tree}). The set $\cT_r(T)$ (Definition~\ref{def:multiset}) consists of the two boxed subtrees.}
  \label{fig:tree}
\end{figure}

\begin{definition}[Computation tree]\label{def:tree} 
Given $G = (V, E, f)$ and a node $v \in V$, let $T_v^1(G) = (v, \emptyset, \{f^v\}, v)$ be a singleton tree rooted at $v$, containing only the node $v$ (with node feature vector $f^v$) and
no edges. Inductively, the depth-$L$ computation tree $T_v^L(G)$ is formed by attaching the neighbors of each leaf node in $T_v^{L-1}(G)$. Concretely, for each leaf $\ell$ of $T_v^{L-1}(G)$ and each neighbor $u \in N_G(\ell)$, we add a new node $\ell_u$ and connect $\ell$ to $\ell_u.$ The multiset of depth-$L$ computation trees defined by {$G$} is $\cT_G^L \defeq \{T_v^{L}(G)\}_{v \in V}$. An example is depicted in Figure~\ref{fig:tree}. 
\end{definition}

\begin{definition}[Tree multisets]\label{def:multiset} Let $T = (V, E, f, r)$ be an $r$-rooted tree of depth $\ell$. $\cT_r(T)$ is the multiset of depth $(\ell-1)$ computation trees rooted at each neighbor {$u \in N_T(r)$}. That is, each $u \in N_T(r)$ has subtree $T_u$ of depth $\ell-1$, and $\cT_r(T) \defeq \cup_{u \in N_T(r)} {T^{(\ell-1)}_u}({T_u})$. (Here $\cup$ is multiset union).
\end{definition}

\begin{definition}[Optimal transport]\label{def:OT} Let $X = \{x_{i}\}_{i=1}^q$ and $Y = \{y_{i}\}_{i=1}^q$ be two multisets. Given a metric $d: X \times Y \to \R_{+}$, let $C\in \R^{q\times q}$ be a matrix where $C_{i,j} = d(x_i, y_j)$ is the \emph{transportation cost} between $x_i$ and $y_j$. A \emph{transportation plan} $\gamma$ is a $q \times q$ nonnegative doubly-stochastic matrix, i.e., whose rows and columns sum to 1. The cost of a plan $\gamma$ is $\langle \gamma, C \rangle \defeq \sum_{i,j} \gamma_{i,j} C_{i,j}.$ The (unnormalized) Wasserstein distance $\cOT_d$ is defined as ${\cOT_d (X, Y) \defeq q \min_{\gamma \in \Gamma(X, Y)} \langle C, \gamma \rangle}$, where ${\Gamma(X, Y) \defeq \{\gamma \in \R^{q \times q}_+ \mid \gamma \bm{1} = \gamma^\top \bm{1} = \bm{1} \}}$ is the feasible set of transportation plans. A plan $\gamma^*$ achieving the minimum is called an \emph{optimal transport plan.}
\end{definition} 

To compare tree multisets with OT requires a metric between trees. To achieve this, TMD employs the \emph{tree distance}, which measures similarity by comparing the features of the root nodes and \emph{recursively} comparing their subtrees. Specifically, for each rooted tree, the collection of subtrees rooted at each neighbor of the root is constructed, as illustrated in Figure~\ref{fig:tree}. Since OT operates on multisets of equal cardinality, comparability is ensured by padding the smaller set with ``blank trees'' (isolated nodes with a feature vector of $\vec{0}$) using a padding function $\rho(\cdot, \cdot)$.   

\begin{definition}[Tree distance]\label{def:TD} Let $T_a = (V_a, E_a, f_a, r_a)$ and $T_b = (V_b, E_b, f_b, r_b)$ be two rooted trees with maximum depth $\ell$  and let $w: \N \to \R_{+}$ assign a weight to each depth level.
   The \emph{tree distance} is defined recursively as:
\begin{align*}
    \cTD_w(T_a, T_b) \defeq \norm{f_a^{r_a} - f_b^{r_b}} + \begin{cases}
        w(\ell) \cdot \cOT_{\cTD_w}(\rho(\cT_{r_a}(T_a), \cT_{r_b}(T_b)))  &\text{if }\ell > 1 \\
        0 &\text{otherwise.} 
    \end{cases}\end{align*}
\end{definition}

The tree distance $\cTD_w$ provides a way to compare individual computation trees by aligning their root features and recursively matching their subtrees using optimal transport. Extending this to entire graphs, TMD represents each graph as a multiset of computation trees and defines the distance between two graphs as the optimal transport cost between these multisets, using the tree distance between the computation trees in the multisets.  For brevity, we use the notation $\cOTbar(\cdot, \cdot) = \cOT_{\cTD_w}(\rho(\cdot, \cdot))$ to denote the optimal transport cost with tree distance and padding.

\begin{definition}[Tree mover's distance]\label{def:TMD} Given graphs $G$ and $G'$, weight function $w: \N \to \R$, and depth parameter $L > 0$, we define 
$\cTMD_w^L(G, G') \defeq \cOTbar(\cT_G^L, \cT_{G'}^L).$
We also define the \emph{tree norm} of a graph $G$ as $\cTreeNorm{G} \defeq \cTMD_w^L(G, \emptyset)$, where $\emptyset$ represents an empty graph.
\end{definition}
By decomposing graphs into local computation trees and comparing them globally, TMD captures both structural and feature-based similarities at multiple scales. As a pseudometric, it satisfies non-negativity and symmetry.
