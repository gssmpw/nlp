\section{Discussion}

We proposed the first theoretically grounded approaches for subsampling graph datasets that preserve the performance of a downstream GNN. Our methods apply to both graph and node subsampling. By leveraging the TMD, our methods ensure that the subsampled data maintain structural similarities with the original dataset. Thus, we can bound the loss incurred by subsampling. Our experiments demonstrate our method outperforms other methods for these problems. 

Future work could combine node and graph subsampling to optimize tradeoffs between storage, computational efficiency, and accuracy. While our analysis is tailored to graph-level tasks, extending these techniques to node-level tasks, where graphs are often much larger, is a promising direction. Additionally, exploring TMD's implications for edge-level tasks, such as link prediction, could further enhance its theoretical and practical utility.