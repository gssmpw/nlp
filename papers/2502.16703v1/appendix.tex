\FloatBarrier
\newpage
\section{Omitted Proofs}\label{sec:apx}


In this Appendix, we include full proofs of the theoretical results, which formally expand the proof sketches in the main body. Throughout this section, we use $\text{deg}_G(v)$ to denote the degree of $v \in V$. 

\subsection{Omitted proofs from Section~\ref{sec:graph_drop}}\label{sec:apx_graph_drop}

\tmdgen*
\begin{proof} Let $\kappa: [n] \to \cI$ map $i \in [n]$ to the index $j \in \cI$ such that $G_j$ is the closest graph in $\{G_i\}_{i \in \cI}$ to $G_i.$ That is, 
\begin{align*}
    \kappa(i) = \argmin_{j \in \cI} \cTMD_w^L(G_i, G_j), 
\end{align*}
with ties broken arbitrarily (but consistently with the mapping $\tau$.) First, we can rearrange the sums as follows
\begin{align*}
    \abs{\frac{1}{n} \sum_{i \in \cI} \tau_i \cL(h(G_i); y_i) - \frac{1}{n}\sum_{i \in [n]}\cL(h(G_i); y_i)} &= \abs{ \frac{1}{n}\sum_{i \in [n]} \cL(h(G_{\kappa(i)}); y_{\kappa(i)})- \cL(h(G_i); y_i)} \\
    &\leq  \frac{1}{n}\sum_{i \in [n]} \abs{\cL(h(G_{\kappa(i)}; y_{\kappa(i)}))- \cL(h(G_i); y_i)} \\
    &\leq  \frac{M}{n}\sum_{i \in [n]} \norm{\cL(h(G_{\kappa(i)}); y_{\kappa(i)})- \cL(h(G_i); y_i)},
\end{align*}
where in the last inequality, we used that $\cL$ is $M$-lipschitz. Now, by Theorem~\ref{thm:stable}, we have 
\begin{align*}
    \frac{M}{n}\sum_{i \in [n]} \norm{\cL(h(G_{\kappa(i)}); y_{\kappa(i)})- h(G_i; y_i)} &\leq \frac{M}{n} \paren{\prod_{\ell \in [L-1]} \Phi_\ell} \sum_{i \in [n]} \cTMD_w^L(G_{\kappa(i)}, G_i) \\
    &= M \paren{\prod_{\ell \in [L-1]} \Phi_\ell} f_{\cTMD_w^L}(S; X).
\end{align*}
\end{proof}

\erm*
\begin{proof}
From Lemma~\ref{lemma:tmd-gen}, we know that \[\frac{1}{n}\sum_{i \in [n]}\cL\paren{\hat{h}(G_i); y_i} \leq \frac{1}{n} \sum_{i \in S} \tau_{i} \cL(\hat{h}(G_i); y_i) + c\epsilon.\]
    Let $h^*$ be the hypothesis that minimizes average loss over the original dataset $X$: \[h^* = \argmin_{h \in \cH}\frac{1}{n}\sum_{i \in [n]}\cL(h(G_i); y_i).\] By definition of $\hat{h}$, this means that \[\frac{1}{n}\sum_{i \in [n]}\cL\paren{\hat{h}(G_i); y_i} \leq \frac{1}{n} \sum_{i \in S} \tau_{i} \cL(h^*(G_i); y_i) + c\epsilon.\] Applying Lemma~\ref{lemma:tmd-gen} again, we have that  \[\frac{1}{n}\sum_{i \in [n]}\cL\paren{\hat{h}(G_i); y_i} \leq \frac{1}{n}\sum_{i \in [n]}\cL\paren{h^*(G_i); y_i} + 2c\epsilon.\]
\end{proof}

\subsubsection{Proof of Theorem~\ref{thm:TMD-is-needed}}\label{sec:thm-tmd-is-needed}

In this section, we prove Theorem~\ref{thm:TMD-is-needed}. The key insight driving our analysis is that in order to disprove Conjecture~\ref{conj:TMD-not-needed} for a specific graph pseudo-metric $D$, it suffices to produce a pair of graphs $G, G'$ and an $L$-layer GNN $h$ such that $D(G, G') = 0$ while $h(G) \neq h(G')$. In words, this means that if we have a GNN that outputs different values on two graphs $G, G'$ (i..e, the GNN can \emph{distinguish} them) but $D(G, G') = 0$, then we \emph{cannot hope} to show that the difference in GNN outputs is bounded proportional to $D(G, G')$.
The following Lemma formalizes this intuition.

\begin{lemma}\label{lemma:sufficient-condition} Let $D$ denote a graph pseudo-metric. Suppose there exists a pair of graphs $G, G'$ and a GNN  $h: \cG \to \R^d$ with $\ell$-th layer Lipschitz constant $\Phi_\ell$ such that $D(G, G') = 0$ and $h(G) \neq h(G')$. Then, Conjecture~\ref{conj:TMD-not-needed} is not true for the kernel $D$. 
\end{lemma}
\begin{proof} Suppose for the sake of contradiction that Conjecture~\ref{conj:TMD-not-needed} is true for the kernel $D$. Then, there is a constant $C(\{\phi_\ell\}_{\ell=1}^{L}) > 0$ (depending on the Lipschitz-constants $\{\phi_\ell\}_{\ell=1}^{L}$) such that $\norm{h(G) - h(G')} \leq C(\{\phi_\ell\}_{\ell=1}^{L}) \cdot D(G, G') = 0$. Since norms are positive definite, this contradicts that $h(G) \neq h(G')$. 
\end{proof}

In the following lemmas, we show that the assumption of Lemma~\ref{lemma:sufficient-condition} holds for four of the most commonly used graph pseudo-metrics in the graph learning literature: WL (Weisfeller-Lehman), WL-OA (Weisfeller-Lehman Optimal Transport), SP (Shortest Paths), and GS (Graphlet Sampling.) Indeed, the fact that the assumption of Lemma~\ref{lemma:sufficient-condition} holds for Shortest Paths and Graphlet Sampling was previously shown by \citet{kriege2020survey} (Figure 7 of \citet{kriege2020survey}), in which the authors demonstrated examples of graphs where the Shortest Paths and Graphlet Sampling pseudo-metrics would equal 0 \emph{even} when a GNN can distinguish between them. 

\begin{lemma}[Result of \citet{kriege2020survey}]\label{lemma:partial} The assumption of Lemma~\ref{lemma:sufficient-condition} holds for $D = D_{GS}$ and $D= D_{SP}$. 
\end{lemma}

However, such a result (to our knowledge) was not previously known for the original WL (Weisfeller-Lehman) pseudo-metric or its variant WL-OA (Weisfeller-Lehman Optimal Transport) 
 We refer to these metrics as WL-based metrics. To prove that the assumption of Lemma~\ref{lemma:sufficient-condition} holds for these kernels, we first give an overview of how the WL-based metrics are constructed. First, we define WL iterations as follows \citep{zhang2017weisfeiler, shervashidze2011weisfeiler}. These form the basis of the definition of the various WL-based pseudo-metrics. 

\begin{definition}[WL Iterations \citep{zhang2017weisfeiler}]\label{def:WL-iterations}
  Let \(G = (V, E)\) be a labeled graph with initial labels \(\ell_0(v)\) for each \(v \in V\).
  Let \(\Gamma(v)\) denote the set of neighbors of \(v\).
  For a fixed number of iterations \(L \ge 0\), the \emph{Weisfeiler--Lehman (WL) labeling} proceeds as follows:

  \begin{itemize}
    \item For each iteration \(h = 1, 2, \dots, L\):
    \begin{enumerate}
      \item \textbf{Multiset labeling:} 
        \[
          m_h(v) \;=\; \{\ell_{h-1}(u) : u \in \Gamma(v)\}.
        \]
      \item \textbf{Label compression:} 
        \[
          \ell_h^G(v) \;=\; \mathrm{Hash}\!\bigl(\ell_{h-1}(v),\, m_h(v)\bigr),
        \]
        where \(\mathrm{Hash}\) is an injective function (often a hash or string-encoding).
      \item \textbf{Feature extraction:} For each iteration \(h\),
        define a feature map \(\phi^{(h)}(G)\in\mathbb{R}^{|\mathcal{L}_h|}\), 
        counting the occurrences of each \emph{compressed} label in \(G\) at iteration \(h\).
        Here, \(\mathcal{L}_h\) is the set of all possible labels at iteration \(h\).
    \end{enumerate}
  \end{itemize}

We call \(\{\ell_h^G\}_{h=0}^L\) the WL-refined labels of \(G\), and
  \(\{\phi^{(h)}(G)\}_{h=0}^L\) the corresponding feature vectors (for each iteration $\ell \in [L])$.
\end{definition}

Equipped with this definition, we now define the WL kernel and pseudo-metric. 

\begin{definition}[WL \citep{shervashidze2011weisfeiler}]\label{def:WL-kernel}
  Given two graphs \(G\) and \(G'\) and an integer $L > 0$, let \(\{\phi^{(h)}(G)\}_{h=0}^L\) and \(\{\phi^{(h)}(G')\}_{h=0}^L\) be their WL feature vectors from Definition~\ref{def:WL-iterations}.
  The \emph{Weisfeiler--Lehman (WL) kernel} is defined by
  \[
    k_{\mathrm{WL}}(G, G') 
    \;=\; 
    \sum_{h=0}^L
      \bigl\langle \phi^{(h)}(G),\, \phi^{(h)}(G') \bigr\rangle,
  \]
  where \(\langle \cdot,\cdot\rangle\) is the standard dot product in \(\mathbb{R}^{|\mathcal{L}_h|}\). From the WL kernel \(k^L_{\mathrm{WL}}\), one can define a \emph{pseudo-metric} 
  \(D_{\mathrm{WL}}(G, G')\) by
  \[
    D^L_{\mathrm{WL}}(G, G')
    \;=\;
    \sqrt{
      k_{\mathrm{WL}}(G, G)
      \;+\;
      k_{\mathrm{WL}}(G', G')
      \;-\;
      2\,k_{\mathrm{WL}}(G, G')
    }.
  \]
\end{definition}

The WL-OA kernel is defined similarly; however, we perform an optimal assignment between the WL- labels. 

\begin{definition}[WL-OA Kernel]\label{def:WL-OA-kernel}
  After performing \(L\) WL iterations on a graph \(G\), each node \(v \in V(G)\) 
  has a final label (or embedding) \(\ell_L(v)\).  
  Let \(\kappa\bigl(\ell_L(v), \ell_L(w)\bigr) = \begin{cases}
      1, & \ell_L(v) = \ell_L(w) \\
      0, & \text{otherwise}.
  \end{cases}\) An \emph{optimal assignment} \(\alpha : V(G) \to V(G')\) between two graphs \(G\) 
  and \(G'\) maximizes the total node-level similarity:
  \[
    \max_{\alpha : V(G)\!\to\!V(G')}
    \sum_{v \in V(G)}
      \kappa\!\bigl(\ell_L(v),\, \ell_L(\alpha(v))\bigr).
  \] The WL-OA kernel is then defined as follows:
  \[
    k_{\mathrm{WL\text{-}OA}}(G, G')
    \;=\;
    \sum_{h=0}^L\;
      \max_{\alpha_h : V(G)\!\to\!V(G')} 
      \sum_{v \in V(G)} 
        \kappa\bigl(\ell_h(v), \ell_h(\alpha_h(v))\bigr).
  \]
  From the WL-OA kernel \(k^L_{\mathrm{WL-OA}}\), one can define a \emph{pseudo-metric} 
  \(D_{\mathrm{WL-OA}}(G, G')\) by
  \[
    D^L_{\mathrm{WL-OA}}(G, G')
    \;=\;
    \sqrt{
      k_{\mathrm{WL-OA}}(G, G)
      \;+\;
      k_{\mathrm{WL-OA}}(G', G')
      \;-\;
      2\,k_{\mathrm{WL-OA}}(G, G')
    }.
  \]
\end{definition}

Next, we construct a pair of graphs which cannot be distinguished by the WL or WL-OA kernels. 
\begin{lemma}\label{lemma:prev} Let $G = (V, E, f)$ and $G' = (V, E, f')$ where $V = [n]$, $f_i = i$ for all $i \in [n]$, and $f'_i = 10\cdot i$ for all $i \in [n]$. Then, $D^L_{\mathrm{WL-OA}}(G, G') = D^L_{\mathrm{WL}}(G, G') = 0$ for every integer $L > 0$. 
\end{lemma}
\begin{proof} Note that $G, G'$ have the exact same graph structure (i.e., edges) and differ only in the feature attributes $f, f'$ respectively. Consequently, by Definition~\ref{def:WL-iterations}, we see that $\{\ell_h^G(i)\}_{h=0}^L = \{\ell_h^{G'(i)}\}_{h=0}^L$ for all $i \in [n]$. Since the distance pseudo-metrics $D^L_{\mathrm{WL}}$ and $D^L_{\mathrm{WL-OA}}$ depend on $G, G'$ only through these label functions, the lemma follows.
\end{proof}

Since a GNN can trivially distinguish between these two graphs proposed in Lemma~\ref{lemma:prev}, we can prove that the assumption of Lemma~\ref{lemma:sufficient-condition} holds for the WL and WL-OA pseudo-metrics. 

\begin{corollary}\label{corr:WL}The assumption of Lemma~\ref{lemma:sufficient-condition} holds for $D = D^L_{WL}$ and $D = D^L_{WL-OA}$. 
\end{corollary}
\begin{proof} A single-layer graph neural network can distinguish between the candidates $G, G'$ guaranteed by Lemma~\ref{lemma:prev}, by simply setting all aggregation functions and nonlinearities to identity mappings. Thus, the assumption of Lemma~\ref{lemma:sufficient-condition} holds for $D = D^L_{WL}$ and $D = D^L_{WL-OA}$. 
\end{proof}

\tmdneeded*
\begin{proof} Combining Lemma~\ref{lemma:sufficient-condition} with Lemma~\ref{lemma:partial} proves the result for $D \in \{D_{SP}, D_{GS}\}$.  Combining Lemma~\ref{lemma:sufficient-condition} with Corollary~\ref{corr:WL} proves the result for $D \in \{D_{WL}, D_{WL-OA}\}$. 
\end{proof}

\subsection{Omitted proofs from Section~\ref{sec:node_drop}}\label{sec:apx-node}

\corrermfirst*
\begin{proof} Consider any $h \in \cH$. By Theorem~\ref{thm:stable}, we have that for each $i \in [n]$, 
\begin{align}\label{eq:stability-eq}
    \norm{{h}(G_i; y_i) - {h}(G_i[S_i]; y_i)} \leq \paren{\prod_{\ell \in [L-1]} \Phi_\ell } \cTMD_w^L(G_i, G_i[S_i]). 
\end{align}
Consequently, using the fact that $\cL$ is $M$-Lipschitz and \eqref{eq:stability-eq}, 
\begin{align*}
    \abs{\cL\paren{{h}(G_i); y_i} - \cL\paren{{h}(G_i[S_i])}} &\leq M \norm{{h}(G_i; y_i) - {h}(G_i[S_i]; y_i)} \\
    &\leq M \paren{\prod_{\ell \in [L-1]} \Phi_\ell } \cTMD_w^L(G_i, G_i[S_i]) \\
    &= c \cTMD_w^L(G_i, G_i[S_i]) \leq c\epsilon_i. 
\end{align*}
Consequently, by the triangle inequality, 
\begin{align*}
    \abs{ \frac{1}{n}\sum_{i \in [n]} \cL\paren{{h}(G_i); y_i} - \frac{1}{n}\sum_{i \in [n]} \cL\paren{{h}(G_i[S_i])}} &\leq \frac{1}{n}\sum_{i \in [n]} \abs{\cL\paren{{h}(G_i); y_i} - \cL\paren{{h}(G_i[S_i]; y_i)}} \\
    &\leq \frac{c}{n}\sum_{i \in [n]} \epsilon_i. 
\end{align*}
So, we know that for any $h \in \cH$
\begin{align}\label{eq:gen}
    \frac{1}{n} \sum_{i \in [n]} \cL({h}(G_i; y_i) \leq \frac{1}{n} \sum_{i \in [n]} \cL(h({G_i[S_i]}; y_i) + \frac{c}{n} \sum_{i\in[n]} \epsilon_i.
\end{align}
Let $h^\star$ be the hypothesis that minimizes average loss across the original dataset $X$: 
\begin{align*}
    h^\star = \argmin_{h \in \cH} \frac{1}{n} \sum_{i \in [n]} \cL(h(G_i); y_i). 
\end{align*}
By definition of $\hat{h}$, this means that
\begin{align*}
    \frac{1}{n} \sum_{i \in [n]} \cL(\hat{h}(G_i[S_i]; y_i) \leq \frac{1}{n} \sum_{i \in [n]} \cL(h^\star(G_i[S_i]); y_i) + \frac{c}{n} \sum_{i \in [n]} \epsilon_i. 
\end{align*}
Applying \eqref{eq:gen} again to $h^\star$, we have that 
\begin{align*}
    \frac{1}{n} \sum_{i \in [n]} \cL(\hat{h}(G_i[S_i]; y_i) \leq  \frac{1}{n} \sum_{i \in [n]} \cL(h^\star(G_i); y_i) + \frac{c}{n} \sum_{i \in [n]} \epsilon_i. 
\end{align*}
\end{proof}

\generaldecomposition*
\begin{proof} Let $\bar{G}[S]$ be the graph obtained by augmenting $G[S]$ with node features $\bm{0}$ for each $v \in V \setminus S.$ That is, $\bar{G}[S] = (V, E[S], {f'})$ where $f'^v = f^v$ if $v \in S$ and $\bm{0}$ otherwise By the definition of the augmentation function $\rho$ and $\cOTbar$ (recall \eqref{eq:rho}, \eqref{eq:OT-bar}), we can instead consider the following OT problem, which is equivalent to \eqref{eq:ot-problem}:
\begin{align*}
    \cOT_{\cTD_w}\paren{\cT_G^L, \cT_{\bar{G}[S]}^L}. 
\end{align*}

In the base case where $L=1$, we see that \emph{any} transportation plan between the nodes of $\bar{G}[S]$ and $G$ must transport the excess node feature mass $\sum_{v \notin S} \norm{f^v}$ in $G$ to some nodes in $G[S].$ The transportation plan $\bm{I}$ has cost exactly equal to $\sum_{v \in V} \norm{f^v - f'^v} = \sum_{v \notin S} \norm{f^v}$, so $\bm{I}$ must be an optimal transportation plan. 

Now, if $L > 1$, consider the OT problem~\ref{eq:ot-problem}, and let $C$ and $\Gamma$ be the distance matrix and feasible transportation plans for this OT problem (recall Definition~\ref{def:OT}). Let $\gamma \in \Gamma$ be any feasible transportation plan.

By the definition of $\cTD$, for any $i, j \in V$, transporting $\gamma_{i,j}$ mass from $i$ to $j$ incurs two costs:
\begin{enumerate}[label={(\arabic*)}]
    \item $\gamma_{i,j}$ incurs the cost of transporting $f^i$ to $f'^j$.
    \item $\gamma_{i,j}$ \emph{also} incurs the cost of taking all of the depth $(L-1)$ computation trees of all neighbors of $i$ in $T_i^L(G)$, and transporting them to the depth $(L-1)$ computation trees of all neighbors of $j$ in $T_j^L(G[S])$ (after appropriately augmenting with isolated nodes of feature $\bm{0}$ for each $v \notin S$).
\end{enumerate}
To quantify these two costs, let us define $\bar{\cT}_j(T_j^L(G[S]))$ to be the multiset of computation trees obtained by augmenting $\cT_j(T_j^L(G[S]))$ with isolated nodes of feature vectors $\bm{0}$ for each $j \notin S$. That is, let $\bar{\cT}_j(T_j^L(G[S]))$ be defined as the second output of $\rho$ (recall \eqref{eq:rho}) when applied to $\paren{\cT_j(T_j^L(G)), \cT_j(T_j^L(G[S]))}:$
\begin{align*}
\paren{\cT_j(T_j^L(G)), \bar{\cT}_j(T_j^L(G[S]))} \defeq \rho\paren{\cT_j(T_j^L(G)), \cT_j(T_j^L(G[S]))}.
\end{align*}
We can now quantify the cost of transportation plan $\gamma$ in \eqref{eq:ot-problem} as follows. In the following equation, the first term corresponds to item (1) above and the second term corresponds to item (2) discussed above. 
\begin{align*}
    \sum_{i,j \in V} \gamma_{i,j} C_{i,j} &= \sum_{i,j \in V} \gamma_{i,j} \norm{f^i - f'^j} + w(L-1) \sum_{i,j \in V} \gamma_{i,j} \cOT_{\cTD_{w}}{(\cT_i(T_i^L(G)), \bar{\cT}_j(T_j^L({G}[S])))}. 
\end{align*}
Now, we can immediately lower bound the cost $\sum_{i,j} \gamma_{i,j} C_{i,j}$ of $\gamma$ as follows. Using the fact that the minimum of the sum of two functions is at least as large as the sum of their minimums, we have
\begin{align*}
    \sum_{i,j \in V} \gamma_{i,j} C_{i,j} &\geq \min_{\nu \in \Gamma}\sum_{i,j\in V} \nu_{i,j} \norm{f^i - f'^j} \\
    &+ w(L-1) \min_{\tau \in \Gamma}\sum_{i,j\in V} \tau_{i,j} \cOT_{\cTD_{w}}{(\cT_i(T_i^L(G)), \bar{\cT}_j(T_j^L(G[S])))}.
\end{align*}

For the first term, it is easy to see that $\nu = \bm{I}$ is an optimal solution, by the same argument as in the base case: any transportation plan $\nu$ must incur the cost of transporting the excess mass $\{\norm{f^v}\}_{v \notin S}$ in the node features of $G$ which is not present in the node features of $S.$

For the second term, we can notice that $\cOT_{\cTD_{w}}{(\cT_i(T_i^L(G)), \bar{\cT}_j(T_j^L(G[S])))}$ is also an TMD between graphs which contain the corresponding multisets of depth $L-1$ trees! So, by the inductive hypothesis, $\tau = \bm{I}$ is an optimal solution for this OT problem as well. 

Consequently, substituting $\tau = \nu = \bI$, we can further simplify the lower bound to 
\begin{align*}
    \sum_{i,j \in V} \gamma_{i,j} C_{i,j} &\geq \sum_{i,j\in V} \bm{I}_{i,j} \norm{f^i - f'^j} + w(L-1) \sum_{i,j\in V} \bm{I}_{i,j} \cOT_{\cTD_{w}}{(\cT_i(T_i^L(G)), \bar{\cT}_j(T_j^L({G}[S])))} \\
    &= \sum_{i \in V}  \norm{f^i - f'^i} + w(L-1) \sum_{i \in V} \cOT_{\cTD_{w}}{\paren{\cT_i(T_i^L(G), \bar{\cT}_i(T_i^L(G[S]))}} \\
    &= \sum_{v \notin S}  \norm{f^v} + w(L-1) \sum_{v \notin S} \cOT_{\cTD_{w}}\paren{\rho{\paren{\cT_v(T_v^L(G)), \emptyset}}} \\
    &+ w(L-1) \sum_{v \in S} \cOT_{\cTD_{w}}{\paren{\cT_v(T_v^L(G)), \bar{\cT}_v(T_v^L(G[S]))}} \\
    &= \sum_{v \notin S}  \norm{f^v} + w(L-1) \sum_{v \notin S} \cOT_{\cTD_{w}}\paren{\rho\paren{\cT_v(T_v^L(G)), \emptyset}} \\
    &+ w(L-1) \sum_{v \in S} \cOT_{\cTD_{w}}\paren{\rho\paren{\cT_v(T_v^L(G)), \cT_v(T_v^L(G[S]))}}. 
\end{align*}
Finally, the inequality is tight, because $\gamma = \bI \in \Gamma$ is clearly a feasible transportation plan. So, by induction, the lemma holds. 
\end{proof}

\begin{remark}\label{remark:tmd-ot} The terms $\cOT_{\cTD_{w}}\paren{\rho\paren{\cT_v(T_v^L(G)), \cT_v(T_v^L(G[S]))}}$ in the expression of Lemma~\ref{lemma:general-decomposition} can themselves be viewed as the TMD between two graphs $\bar{G}$ and $\bar{G}'$, where $\bar{G}$ is the graph consisting of all elements in $\cT_v(T_v^L(G))$ as disjoint trees, and  $\bar{G}'$ is the graph consisting of all elements in $\cT_v(T_v^L(G[S]))$ as disjoint trees. In this sense, Lemma~\ref{lemma:general-decomposition} precisely characterizes the recursive structure of TMD between a graph and its subgraph. 
\end{remark}

\finegraineddecomp*
\begin{proof} Let $Z = \{z \in V: z \notin S\}.$ Since $T \subset S$, $T$ and $Z$ are disjoint. Moreover, $\{v \in V : v \notin S\setminus{T}\} = Z \cup T$. Thus, $Z \sqcup T = \{v \in V : v \notin S \setminus{T}\}$, where we use $\sqcup$ to denote disjoint union. So, Lemma~\ref{lemma:general-decomposition} and using the definition of $\cOTbar$ \eqref{eq:OT-bar}) shows that 
\begin{align*}
    \cTMD_{w}^L(G, G[S \setminus T]) &= \sum_{z \in Z}\norm{f^z} + w(L-1) \sum_{z \in Z} \cOT_{\cTD_{w}}\paren{\rho\paren{\cT_z(T_z^L(G)), \emptyset}} \\ 
    &+ \sum_{t \in T} \norm{f^t} + w(L-1) \sum_{t \in T} \cOT_{\cTD_{w}}\paren{\rho\paren{\cT_t(T_t^L(G)), \emptyset}}\\
    &+ w(L-1) \sum_{u \in S \setminus{T}} \cOT_{\cTD_{w}}\paren{\rho\paren{\cT_u(T_u^L(G)), \cT_u(T_u^L(G[S \setminus T]))}}.
\end{align*}
Now, each term in $\cOT_{\cTD_{w}}\paren{\rho\paren{\cT_u(T_u^L(G)), \cT_u(T_u^L(G[S \setminus T]))}}$ inside the summation is, in fact a depth $L-1$ TMD between two graphs corresponding to the two disjoint forests of computation trees (as we discussed in Remark~\ref{remark:tmd-ot}). Thus, by the inductive hypothesis, we can expand the last term as follows:
\begin{align*}
    \sum_{u \in S \setminus{T}} \cOT_{\cTD_{w}}\paren{\rho\paren{\cT_u(T_u^L(G)), \cT_u(T_u^L(G[S \setminus T]))}} &= \sum_{u \in S \setminus{T}} \cOT_{\cTD_{w}}\paren{\rho\paren{\cT_u(T_u^L(G)), \cT_u(T_u^L(G[S]))}} \\
    &+\cOT_{\cTD_{w}}\paren{\rho\paren{\cT_u(T_u^L(G[S])), \cT_u(T_u^L(G[S \setminus T]))}}.
\end{align*}
Consequently, we obtain: 
\begin{align*}
    \cTMD_{w}^L(G, G[S \setminus T]) &= \sum_{z \in Z}\norm{f^z} + w(L-1) \sum_{z \in Z} \cOT_{\cTD_{w}}\paren{\rho\paren{\cT_z(T_z^L(G)), \emptyset}} \\ 
    &+ \sum_{t \in T} \norm{f^t} + w(L-1) \sum_{t \in T} \cOT_{\cTD_{w}}\paren{\rho\paren{\cT_t(T_t^L(G)), \emptyset}}\\
    &+ w(L-1) \sum_{u \in S \setminus{T}} \cOT_{\cTD_{w}}\paren{\rho\paren{\cT_u(T_u^L(G)), \cT_u(T_u^L(G[S]))}} \\
    &+ w(L-1) \sum_{u \in S \setminus{T}}  \cOT_{\cTD_{w}}\paren{\rho\paren{\cT_u(T_u^L(G[S])), \cT_u(T_u^L(G[S \setminus T]))}}.
\end{align*}
Next, we can decompose the third summand above as follows. 
\begin{align}\label{eq:new_eq}
    \cTMD_{w}^L(G, G[S \setminus T]) &= \sum_{z \in Z}\norm{f^z} + w(L-1) \sum_{z \in Z} \cOT_{\cTD_{w}}\paren{\rho\paren{\cT_z(T_z^L(G)), \emptyset}} \\ 
    &+ \sum_{t \in T} \norm{f^t} + w(L-1) \sum_{t \in T} \cOT_{\cTD_{w}}\paren{\rho\paren{\cT_t(T_t^L(G)), \emptyset}} \notag\\
    &+ w(L-1) \sum_{u \in S} \cOT_{\cTD_{w}}\paren{\rho\paren{\cT_u(T_u^L(G)), \cT_u(T_u^L(G[S]))}} \notag \\
    &- w(L-1) \sum_{u \in T} \cOT_{\cTD_{w}}\paren{\rho\paren{\cT_u(T_u^L(G)), \cT_u(T_u^L(G[S]))}} \notag \\
    &+ w(L-1) \sum_{u \in S \setminus{T}}  \cOT_{\cTD_{w}}\paren{\rho\paren{\cT_u(T_u^L(G[S])), \cT_u(T_u^L(G[S \setminus T]))}}.\notag
\end{align}
Now, $\cOT_{\cTD_{w}}\paren{\rho\paren{\cT_u(T_u^L(G)), \cT_u(T_u^L(G[V \setminus S]))}}$ is also a depth $L-1$ TMD between two graphs corresponding to two disjoint forests of computation trees (again, see Remark~\ref{remark:tmd-ot}). So, we can apply the inductive hypothesis to see that for each $t \in T$,
\begin{align*}
    \cOT_{\cTD_{w}}\paren{\rho\paren{\cT_t(T_t^L(G)), \emptyset}} &= \cOT_{\cTD_{w}}\paren{\rho\paren{\cT_t(T_t^L(G)), \cT_t(T_t^L(G[S]))}} \\
    &+ \cOT_{\cTD_{w}}\paren{\rho\paren{\cT_t(T_t^L(G[S])), \emptyset}}. 
\end{align*}
Thus, summing over $t \in T$, we have
\begin{align*}
        \sum_{t \in T} \cOT_{\cTD_{w}}\paren{\rho\paren{\cT_t(T_t^L(G[S])), \emptyset}}   = \sum_{t \in T} \cOT_{\cTD_{w}}\paren{\rho\paren{\cT_t(T_t^L(G)), \emptyset}} - \sum_{t \in T} \cOT_{\cTD_{w}}\paren{\rho\paren{\cT_t(T_t^L(G)), \cT_t(T_t^L(G[S]))}}. 
\end{align*}
That is, by a simple rearrangement,
\begin{align*}
        \sum_{t \in T} \cOT_{\cTD_{w}}\paren{\rho\paren{\cT_t(T_t^L(G[S])), \emptyset}}   + \sum_{t \in T} \cOT_{\cTD_{w}}\paren{\rho\paren{\cT_t(T_t^L(G)), \cT_t(T_t^L(G[S]))}} = \sum_{t \in T} \cOT_{\cTD_{w}}\paren{\rho\paren{\cT_t(T_t^L(G)), \emptyset}}.
\end{align*}

By substituting this into the equation \eqref{eq:new_eq} from above, we can cancel terms to obtain 
\begin{align*}
    \cTMD_{w}^L(G, G[S \setminus T]) &= \sum_{z \in Z}\norm{f^z} + w(L-1) \sum_{z \in Z} \cOT_{\cTD_{w}}\paren{\rho\paren{\cT_z(T_z^L(G)), \emptyset}} \\ 
    &+ \sum_{t \in T} \norm{f^t} + w(L-1) \sum_{t \in T} \cOT_{\cTD_{w}}\paren{\rho\paren{\cT_t(T_t^L(G[S])), \emptyset}}\\
    &+ w(L-1) \sum_{u \in S} \cOT_{\cTD_{w}}\paren{\rho\paren{\cT_u(T_u^L(G)), \cT_u(T_u^L(G[S]))}} \\
    &+ w(L-1) \sum_{u \in S \setminus{T}}  \cOT_{\cTD_{w}}\paren{\rho\paren{\cT_u(T_u^L(G[S])), \cT_u(T_u^L(G[S \setminus T]))}}.
\end{align*}
Rearranging, we obtain 
\begin{align*}
    &\cTMD_{w}^L(G, G[S \setminus T]) = \\
    &\sum_{u \notin S}\norm{f^u} + w(L-1)\Brac{ \sum_{u \notin S} \cOT_{\cTD_{w}}\paren{\rho\paren{\cT_u(T_u^L(G)), \emptyset}} + \sum_{u \in S} \cOT_{\cTD_{w}}\paren{\rho\paren{\cT_u(T_u^L(G)), \cT_u(T_u^L(G[S]))}}} \\
    &+ \sum_{t \in T} \norm{f^t} + w(L-1) \Brac{\sum_{t \in T} \cOT_{\cTD_{w}}\paren{\rho\paren{\cT_t(T_t^L(G[S])), \emptyset}} + \sum_{t \in S \setminus{T}} \cOT_{\cTD_{w}}\paren{\rho\paren{\cT_t(T_t^L(G[S])), \cT_t(T_t^L(G[S \setminus T]))}}} \\
    &= \cTMD_{w}^L(G, G[S]) + \cTMD_{w}^L(G[S], G[S \setminus T]), 
\end{align*}
where the last equality is due to Lemma~\ref{lemma:general-decomposition} and the definition of $\cOTbar$ \eqref{eq:OT-bar}. 
\end{proof}

\algorithm*
\begin{proof} Consider Algorithm~\ref{alg:1}, TreeNorm. The runtime is immediate from the algorithm pseudocode, as matrix-vector products can be computed in $O(|E|)$. In this proof, let $A_G$ denote the adjacency matrix of a graph $G$ and let $x_G$ be the vector in $\R^{V}$ such that $x_G(v) = \norm{f^v}.$

To verify the correctness, we proceed by induction. We just need to show that for all $L$, $\cTreeNorm{G} = \norm{x_G}_1 + \norm{\sum_{\ell=1}^{L-1} \paren{\prod_{t=1}^{\ell} w(L-t)} A_G^{\ell} {x}_{G}}_1, $ as this is the exact computation computed by the algorithm pseudocode. Note that an equivalent expression is
$\cTreeNorm{G} = \norm{\sum_{\ell=0}^{L-1} \paren{\prod_{t=1}^{\ell} w(L-t)} A_G^{\ell} {x}_{G}}_1$, where the product over the emptyset is defined as 1. 


If $L = 1$, then $\cTreeNorm{G}$ is simply the sum of its feature values, so this is trivially true. 

Consider the case of a depth-$L$ tree-norm computation. Throughout this proof, all variables are positive, and consequently we can move the $\ell_1$ norm in and out of sums freely (i.e., we have a triangle \emph{equality}.)

By taking $S = \emptyset$ in Lemma~\ref{lemma:general-decomposition}, we see that $\cTreeNorm{G}$ is equal to the norm of its nodes plus $w(L-1)\sum_{v \in V}\norm{{\cT_v(T_v^L(G))}}_{\text{TN}_{w}^{L-1}}.$ That is, 
\begin{align*}
    \cTreeNorm{G} = \norm{x_G}_1 + w(L-1) \sum_{v \in V} \norm{{\cT_v(T_v^L(G))}}_{\text{TN}_{w}^{L-1}}. 
\end{align*}
To interpret this expression, let $G'_v$ be the graph which is a forest of all the trees in ${\cT_v(T_v^L(G))}$-- for each $u \in N_G(v)$, we $G'_v$ will contain a depth-$(L-1)$ tree $G'_{v, u}$. Then, by inductive hypothesis,
\begin{align*}
    \norm{{\cT_v(T_v^L(G))}}_{\text{TN}_{w}^{L-1}} &= \sum_{u \in N_G(v)} \norm{{(T_v^L(G))}}_{\text{TN}_{w}^{L-1}} = \norm{G'_v}_{\text{TN}_w^{L-1}} = \sum_{u \in N_G(v)} \norm{G'_{u,v}}_{\text{TN}_w^{L-1}} \\
    &= \sum_{u \in N_G(v)} \norm{\sum_{\ell=0}^{L-2} \paren{\prod_{t=1}^\ell w(L-t-1)} A_{G_{u,v}}^{\ell} x_{G_{u,v}}}_1 \\
    &=  \sum_{u \in N_G(v)} \sum_{\ell=0}^{L-2} \paren{\prod_{t=1}^\ell w(L-t-1)} \norm{A_{G_{u,v}}^{\ell} x_{G_{u,v}}}_1,  
\end{align*}
Letting $A_G(u, v)$ be the $(u, v)$-th entry of $A_G$, we can turn the sum over $N_G$ into a sum over $V$ by taking an additional product with the adjacency matrix $A_G$: 
\begin{align*}
    \norm{{\cT_v(T_v^L(G))}}_{\text{TN}_{w}^{L-1}} &= \sum_{u \in N_G(v)} \sum_{\ell=0}^{L-2} \paren{\prod_{t=1}^\ell w(L-t-1)} \norm{A_{G_{u,v}}^{\ell} x_{G_{u,v}}}_1 \\
    &= \norm{ \sum_{u \in V} \sum_{\ell=0}^{L-2} \paren{\prod_{t=1}^\ell w(L-t-1)} A_G(u, v) A_{G_{u,v}}^{\ell} x_{G_{u,v}}}_1. 
\end{align*}
Now, by the way we constructed the trees ${\cT_v(T_v^L(G))}$, observe that $u$ has the exact same $(L-1)$-hop neighborhood in $G$ as it does in $G_{u,v}$. Consequently,
\begin{align*}
    \cTreeNorm{G} &= \norm{x_G}_1 + w(L-1) \sum_{v \in V} \norm{{\cT_v(T_v^L(G))}}_{\text{TN}_{w}^{L-1}} \\
    &= \norm{x_G}_1 + w(L-1) \norm{  \sum_{\ell=0}^{L-2} \paren{\prod_{t=1}^\ell w(L-t-1)} \sum_{v \in V} \sum_{u \in V} A_G(u, v) A_{G_{u,v}}^{\ell} x_{G_{u,v}}}_1 \\
    &= \norm{x_G}_1 + \norm{\sum_{\ell=1}^{L-2}  \paren{\prod_{t=1}^\ell w(L-t-1)} \sum_{v \in V}A_G^{\ell+1} {x}_{G}}_1 \\
    &= \norm{x_G}_1 + \norm{\sum_{\ell=1}^{L-1} \paren{\prod_{t=1}^{\ell} w(L-t)} A_G^{\ell} {x}_{G}}_1, 
\end{align*}
completing the induction and the proof of correctness. 
\end{proof}


\subsection{Proof of NP-completeness of node subsampling problem \eqref{eq:node-subsampling}} \label{sec:hardness}

Finally, we prove that optimizing \eqref{eq:node-subsampling} is NP-complete. Concretely, by Theorem~\ref{thm:node-subsampling-relaxed}, we have \eqref{eq:node-subsampling} is equivalent to \eqref{eq:final-opt-problem} for $\cS = 2^V$. Consequently, it suffices to show that \eqref{eq:final-opt-problem} is NP-complete for $\cS = 2^V$.

As is standard in NP-completeness proofs, we consider the following decision version of the problem, for any \emph{arbitrary} positive weights $w$. 

\begin{definition}[Tree norm decision problem]\label{def:decision-problem} In the tree-norm decision problem, we are given $(G, L, k, \tau)$ where $G = (V, E, f)$ is a node-weighted graph, $k, L \geq 1$ are integers, and $\tau > 0$ is a threshold. We must output YES if $\max_{S \in 2^V; \abs{S} = k} \cTreeNorm{G[S]} \geq \tau$, and output NO otherwise. 
\end{definition}

The proof will proceed by reduction to $k$-clique, which is a well-established NP-hard problem. 
\begin{definition}[$k$-clique problem]\label{def:k-clique} In the $k$-clique problem, we are given an \emph{unweighted} graph $G = (V, E)$ and integer $k \geq 1$. We must output YES if $G$ contains a $k$-clique and NO otherwise. 
\end{definition}

\begin{theorem}\label{thm:np-hard} Solving the tree norm decision problem (Problem~\ref{def:decision-problem}) is NP-complete. 
\end{theorem} 

\begin{proof} To show that the problem is in NP, note that given a candidate subset $S \subset V$ as a witness, we can compute the tree norm $\cTreeNorm{G[S]}$ in polynomial time (e.g., using Algorithm~\ref{alg:1}), and verify whether or not $\cTreeNorm{G[S]} \geq \tau$ in additional constant time. Thus, the problem is polynomial-time verifiable, and thus in NP. 

Next, we will prove that the problem is NP hard, by reduction to $k$-clique. Suppose, there exists a polynomial time algorithm for solving the tree norm decision problem. Then, will show that there exists a polynomial time algorithm for the $k$-clique problem. 

So, suppose we are given an unweighted graph $G = (V, E)$ and $k$ as input. Let $\bar{G} = (V, E, g)$ where $g^v = 1 \in \R^d$ for all $v \in V$. Let $K_k = (V_k, E_k)$ denote a $k$-clique graph, and let $\bar{K}_K \defeq (V_k, E_k, f)$ where $f^v = 1 \in \R$ for all $v \in V_k$. That is, $\bar{K}_K$ is just a $k$-clique in which every node is assigned a 1-dimensional feature value of 1, and $\bar{G}$ is just the original input graph $G$ in which every node is assigned a 1-dimensional feature value of 1.

Consider the algorithm which does the following: 
\begin{enumerate}
    \item Compute $\tau_{k\mathrm{-clique}} \defeq \norm{\bar{K}_k}_{\mathrm{TN}_w^k}$. This can be computed in polynomial time (e.g., using Algorithm~\ref{alg:1}.)
    \item Solve and output the solution to the tree norm decision for $(\bar{G}, k, k, \tau_{k\mathrm{-clique}})$. 
\end{enumerate}

We will show that this procedure solves the $k$-clique problem. Suppose that the algorithm outputs NO. Then, clearly, $G$ does not contain a $k$-clique, or else, $\bar{G}$ would contain $\bar{K}_k$ as a subgraph and thus, 
\begin{align*}
    \max_{S \in 2^V; \abs{S} = k} \norm{G[S]}_{\mathrm{TN}_w^k} \geq \norm{\bar{K}_k}_{\mathrm{TN}_w^k} = \tau_{k\mathrm{-clique}}, 
\end{align*}
which is a contradiction. 

Now, suppose the algorithm outputs YES. Then, $\bar{G}$ contains a $k$-subgraph with tree norm greater than or equal to $\tau_{k\mathrm{-clique}}$. However, from the definition of the tree norm, it is easy to see that on a graph where all nodes have feature value 1 and the weight function $w > 0$ is positive, the tree-norm $\norm{G[S]}_{\mathrm{TN}_w^k}$ is \emph{monotonic} as a function of the number of edges in the multiset of depth-$k$ computation trees defined by $G[S]$ (see Definition~\ref{def:tree}.) Thus, 
\begin{align*}
    \max_{S \in 2^V; \abs{S} = k} \norm{G[S]}_{\mathrm{TN}_w^k} \leq \bar{K}_k, 
\end{align*}
since $\bar{K}_k$ is the \emph{unique} subgraph on $k$-nodes which contains \emph{every possible} edge in its depth-$k$ computation trees. And consequently, the algorithm outputs YES if and only if $\bar{G}$ contains $\bar{K}_k$ as a $k$-clique.  
\end{proof}