\section{Introduction}

Graph neural networks (GNNs) have demonstrated remarkable success in graph classification tasks across diverse domains, including drug discovery \citep{gaudelet2021utilizing}, quantum chemistry \citep{stuyver2022quantum}, social networks \citep{awasthi2023gnn}, and recommendation systems \citep{sharma2024survey}. The effectiveness of GNNs critically depends on the availability of large, labeled graph datasets. However, acquiring and managing these datasets presents several key challenges:

\begin{description}[leftmargin=1.5em, labelindent=1.5em]
\item[Cost of Graph Labeling.]  Across diverse domains, labeling large graph datasets can be extremely costly. In pharmaceutical development, labeling drug molecules requires synthesizing the molecules and experimentation to characterize their properties \citep{gaudelet2021utilizing}. Meanwhile, in diagnostic settings, labeling requires expert clinical annotation \citep{li_graph_2022}. Additionally, GNNs have shown promise for combinatorial optimization, but labeling training instances requires solving large optimization problems---a computationally expensive process \citep{Cappart23:Combinatorial}.

\item[Data Storage and Streaming.] Large datasets can require substantial storage, posing challenges for data sharing, streaming, and distributed training on limited-memory devices, especially as graph datasets continue to grow \citep{zeng_gnn_2023, liu_federated_2024}. Further, many datasets contain sensitive information, such as patient records or proprietary molecular structures, making storage a potential security risk \citep{gaudelet2021utilizing, li_graph_2022}. 
    
\item[Cost of GNN Training.] Training GNNs on large datasets is computationally intensive, necessitating repeated training, architecture searches, and hyperparameter tuning. This requires significant computational resources, energy consumption, financial costs, and time-to-deployment \citep{khemani_review_2024}.  These challenges are further exacerbated when datasets exceed memory limits, requiring distributed training and additional processing overhead \citep{strubell2020energy, zeng_gnn_2023, tripathy_reducing_2020, liu_federated_2024}.

\end{description}

A natural approach to address these challenges is to subsample the graph dataset and use the subsample for GNN development. This provides a unified solution, as smaller datasets are cheaper to label, require less storage, and streamline training.  While subsampling adds a preprocessing step, it is a one-time cost amortized over multiple rounds of training and hyperparameter tuning, ultimately reducing overall computational burden. This motivates a fundamental theoretical question: \emph{how can we select a smaller set of graphs and/or nodes while preserving GNN performance?} 

This problem can be formulated as \emph{coreset construction}. A coreset is a (weighted) subset of the data that ensures a downstream task remains \emph{provably competitive} when performed on the subset instead of the full dataset \citep{bachem_practical_2017}. We aim to construct a coreset from a graph dataset such that training a GNN on the coreset achieves a loss competitive with training on the full dataset. Coreset construction is challenging as it depends both on the data and the downstream task. This is especially difficult in our case since GNNs are complex functions operating on graphs with diverse structures and feature distributions. As a result, it is difficult to formally analyze how subset selection influences model performance. Furthermore, a poorly chosen subset can severely compromise model performance. For instance, if it has graphs from only one class, the resulting GNN will fail to generalize. While it is possible to empirically assess multiple candidate subsets for quality, doing so requires labeling, storing, and training models on each subset, undermining the benefits of subsampling. This motivates the need for principled methods for coreset construction with strong guarantees on model performance.

An ideal approach should be both \emph{label-agnostic} and \emph{model-agnostic}. A coreset is \emph{label-agnostic} if its construction does not require access to graph labels and \emph{model-agnostic} if it remains representative across different GNN parameters and architectures rather than being suited for a specific model. These properties enable subsampling at the earliest stages of model development—before data annotation, model selection, and hyperparameter tuning—minimizing costs and resources needed for storage, labeling, and training.




\subsection{Our Contributions}

We present a coreset selection approach for graph classification with GNNs that, to our knowledge, is the first to come with provable guarantees on model performance. Our method is \emph{label-agnostic}, in that it does not require access to the labels of the original training set in order to construct this coreset. Our method is also \emph{model-agnostic}, in that it requires minimal assumptions on the downstream GNN architecture hyperparameters (such as layer width, type of pooling layers, and activation functions) as these details may be unknown at the time of subsampling. In contrast, prior approaches satisfy \emph{at most} one of these criteria. We make the following contributions:

\begin{description}[leftmargin=1.5em, labelindent=1.5em]
\item[Graph subsampling.] Given a training set of graphs, our method returns a subset of $k$ graphs which form a graph coreset. We provide a formal guarantee that bounds the increase in loss incurred when training a GNN on the subsampled graphs compared to the full dataset. 

\item[Node subsampling.] We extend our approach to node subsampling. We present a method that, given a graph, returns a $k$-node subgraph. Our approach again comes with strong coreset guarantees which bound the increase in loss incurred when training a GNN on these $k$-node subgraphs as opposed to the original graphs.

\item[Technical approach.] Our results are driven by new theoretical insights into the \emph{tree mover's distance (TMD)}~\citep{Chuang22:Tree}, which may be of independent interest. In particular, we introduce a linear-time algorithm to compute the TMD between a graph and any of its subgraphs, significantly improving on prior work's super-cubic runtime \citep{Chuang22:Tree}.

\item[Experiments.] We support our theoretical results with experiments comparing our method to existing subsampling approaches on datasets from OGBG and TUDatasets \citep{morris2020tudataset}. For graph subsampling, our method outperforms \citet{jin2022condensing, mirage}, and \citet{kidd}, while for node subsampling, it outperforms \citet{razin2023ability} and \citet{salha2022degeneracy}. 
\end{description}


\subsection{Related Work}\label{sec:related-work}

\paragraph{Coreset construction.}
Coresets have been extensively studied in classical ML for Euclidean data, with applications in mean approximation, regression, clustering, and convex optimization \citep{bachem_practical_2017, mirzasoleiman2020coresets, woodruff_coresets_2024, cohen-addad_improved_2022, tukan_coresets_2020}. These methods construct small, weighted subsets of datapoints that provably approximate solutions obtained on the full dataset. More recently, coresets have been explored for improving the efficiency of training deep learning models \citep{yang_towards_2023, ding_spectral_2024, mirzasoleiman2020coresets}.  

Despite this progress, coreset selection for structured data, particularly graphs, remains largely unexplored. \citet{ding_spectral_2024} propose a subset selection approach for node classification, but it lacks the provable guarantees typical of coreset methods and differs fundamentally from our work, which focuses on graph classification. To our knowledge, ours is the first work to introduce coreset selection for graph classification and to provide formal guarantees on GNN performance when trained on the coreset.


\paragraph{Graph dataset compression.}{ A related but distinct line of research focuses on graph condensation, which reduces the size of a GNN training dataset by building a smaller, \emph{synthetic} dataset that resembles the original. Most works in this area target node-level task, making them unsuitable for comparison with our graph-level approach. For graph-level tasks, DosCond \citep{jin2022condensing} and KiDD \citep{kidd} use \emph{gradient matching}, which requires access to the full GNN hyperparameter specifications and the training trajectories of a GNN trained on the original dataset. These methods build the synthetic graphs to replicate the training trajectories on the full dataset. Thus, they are neither label- nor model-agnostic and lack theoretical guarantees. Moreover, gradient matching requires training a model on the full dataset to construct the subsample, which undermines key practical motivations of subsampling.
}

\paragraph{Model-agnostic graph dataset compression.} \citet{mirage} propose a model-agnostic graph dataset compression procedure, MIRAGE, that uses the training graphs' computation trees to compress the training set. Unlike gradient-matching approaches, MIRAGE is model-agnostic: it does not require knowledge of downstream model parameters to build the subsample. However, it is \emph{not} label-agnostic, as its sampling procedure relies on labels from the full dataset.



\paragraph{Other applications of TMD.} \citet{georgiev23} uses a normalized variant of TMD to select training sets for neural algorithmic reasoning tasks. However, their work is not directly applicable to graph or node subsampling.