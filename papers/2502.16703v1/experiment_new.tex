
\section{Experiments}\label{sec:experiments}

\paragraph{Graph subsampling.} We compare our approach with several graph subsampling and condensation methods. KiDD \citep{kidd} and DosCond \citep{jin2022condensing} condense datasets into small synthetic graphs but are not label-agnostic. For a fair comparison, we modify them to operate in a label-agnostic manner by removing their label-aware components in the ``without labels'' section of Tables~\ref{tab:all-results-graph-subsampling1} and \ref{tab:all-results-graph-subsampling2}, as detailed in Appendix~\ref{sec:additional-background-experiments-vallabel}. Since these methods do not support randomized train/test splits and rely on fixed initializations, we do not randomize over splits but do randomize over GNN parameter initializations. MIRAGE \citep{mirage} is another condensation method that subsamples computation trees, but persistent runtime errors in its implementation prevented thorough benchmarking, an issue also reported by \citet{sun2024gc}. We successfully ran MIRAGE on two datasets for certain sampling percentages, as detailed in Appendix~\ref{sec:additional-background-experiments-mirage}.  In addition to these methods, we construct three additional baselines. \emph{WL} applies $k$-medoids clustering using the widely-used Weisfeiler-Lehman (WL) kernel \citep{shervashidze2011weisfeiler}. \emph{Random} performs uniform graph subsampling. \emph{Feature} applies $k$-medoids clustering based only on node features, ignoring graph structure (see Appendix~\ref{sec:additional-background-experiments-feature}). This baseline is particularly useful for evaluating the impact of structure-aware methods such as TMD.  We use an 80/20 train/test split and select between 1\% and 10\% of the training graphs. A GNN is trained on the selected graphs, and we report the average test performance with 95\% confidence intervals over 20 trials with random train/test splits and network initializations.  

Our findings are summarized in Tables~\ref{tab:all-results-graph-subsampling1} and \ref{tab:all-results-graph-subsampling2}. Following \citet{jin2022condensing}, we measure test AUC-ROC on OGBG datasets and classification test accuracy on the remaining datasets. ``Wins'' count how often a method outperforms others, while ``Fails'' count how often a method falls below 50\% accuracy, as all tasks involve binary classification.  TMD consistently ranks first or second across nearly all datasets and subsampling percentages, except on NCI1, where no method outperforms Random. While Random achieves strong performance on NCI1, it performs poorly on most other datasets, a trend also observed with DosCond, which achieves strong results on OGBG-MOLBACE but struggles without labels. Overall, TMD attains the highest total wins across datasets, as shown in Table~\ref{tab:wins-fails-graph}. Additionally, while DosCond and KiDD exhibit high variance, particularly on PROTEINS, TMD maintains stable performance across different subsampling percentages. This consistency is reflected in its low fail count in Table~\ref{tab:wins-fails-graph}, underscoring the robustness of our approach.  

\begin{table}[t]
\caption{Summary of graph and node subsampling performance. The ``Wins" column counts the number of times each method achieves the best test performance across datasets and sampling percentages. The ``Fails" column counts instances where test performance falls below 50\% (worse than random chance). No fails were observed for node subsampling. TMD achieves the highest number of wins across both tasks, while the strong performance of Random is primarily observed on the NCI1 dataset. Full results are presented in Tables~\ref{tab:all-results-graph-subsampling1}, \ref{tab:all-results-graph-subsampling2}, and \ref{tab:combined-results-nodes}.}
\centering
\label{tab:summary}
\begin{tabular}{cc} 
\begin{subtable}{0.48\textwidth}
    \centering
    \caption{Graph subsampling}
    \label{tab:wins-fails-graph}
    \begin{tabular}{l | c | c}
    \hline
    \textbf{Method} & \textbf{Wins} $\uparrow$ & \textbf{Fails} $\downarrow$ \\
    \hline
    DosCond & 11 & 20 \\
    KiDD & 7  & 7 \\
    Feature & 13  & 3 \\
    WL & 8 & 4 \\
    Random & 18 & 7 \\
    \textbf{TMD} & \textbf{23} & \textbf{2} \\
    \hline
    \end{tabular}
\end{subtable}
\begin{subtable}{0.48\textwidth}
    \centering
    \caption{Node subsampling}
    \label{tab:wins-fails-node}
    \begin{tabular}{l | c }
    \hline
    \textbf{Method} & \textbf{Wins} $\uparrow$  \\
    \hline
    RW & 15 \\
    k-cores & 5  \\
    Random & 16 \\
    \textbf{TMD} & \textbf{25} \\
    \hline
    \end{tabular}
\end{subtable}
\end{tabular}
\end{table}


\paragraph{Node subsampling.} To our knowledge, no existing methods focus on node subsampling for graph classification, so we adapt two related approaches as benchmarks. \emph{K-cores}, proposed by \citet{razin2023ability}, is a node selection heuristic based on $k$-core decomposition, which identifies structurally important nodes by iteratively pruning low-degree nodes. \emph{RW}, introduced by \citet{salha2022degeneracy}, is a random-walk-based heuristic originally designed for subgraph sampling in large graph autoencoders. Additionally, we compare against \emph{Random}, which performs uniform node subsampling.  For our proposed node subsampling method (Theorems~\ref{thm:node-subsampling-relaxed} and~\ref{thm:node-subsampling-relaxed-algorithm}), we construct the candidate set $\mathcal{S}$ using breadth-first search (BFS) trees, leveraging the fact that BFS preserves critical motifs in real-world networks~\citep{alimohammadi2023local}. We further augment $\mathcal{S}$ with subsets generated by the RW and k-cores heuristics. The final subset is then selected using TMD. Additional details are provided in Appendix~\ref{sec:heuristic}.  Datasets with relatively larger graphs, such as COX2, PROTEINS, and DD, are well-suited for node subsampling, whereas MUTAG, NCI1, OGBG-MOLBACE, and OGBG-MOLBBBP contain fewer than 35 nodes per graph. For completeness, we include results on MUTAG to demonstrate that our method remains competitive even on smaller graphs.  

Table~\ref{tab:combined-results-nodes} compares test accuracy across these methods for sampling fractions ranging from 10\% to 90\%. We report averages with 95\% confidence intervals over 20 trials with random neural network initializations and train/test splits. The ``W'' row counts the number of times a method outperforms others. TMD consistently ranks among the top two methods and achieves the best overall performance (Table~\ref{tab:wins-fails-node}).  \\

%\newpage
\input{ellen-experiments}
\clearpage
\newpage

\input{ellen-experiments2}
\input{node_subsampling_tables}


%\FloatBarrier

