\documentclass[11pt]{article}

\input{macros.tex}



\title{Subsampling Graphs with GNN Performance Guarantees}

\author{
	Mika Sarkin Jain \\ Stanford University \\ \texttt{mjain4@stanford.edu} \and 
	Stefanie Jegelka \\ TUM and MIT \\ \texttt{stefje@csail.mit.edu}
    \and
	Ishani Karmarkar \\ Stanford University \\ \texttt{ishanik@stanford.edu }
  \and
  	Luana Ruiz \\ Johns Hopkins University \\ \texttt{lrubini1@jhu.edu}
    \and
	Ellen Vitercik \\ Stanford University \\ \texttt{vitercik@stanford.edu}}

\begin{document}

\maketitle

\begin{abstract}
How can we subsample graph data so that a graph neural network (GNN) trained on the subsample achieves performance comparable to training on the full dataset? This question is of fundamental interest, as smaller datasets reduce labeling costs, storage requirements, and computational resources needed for training. Selecting an effective subset is challenging: a poorly chosen subsample can severely degrade model performance, and empirically testing multiple subsets for quality obviates the benefits of subsampling. Therefore, it is critical that subsampling comes with guarantees on model performance. In this work, we introduce new subsampling methods for graph datasets that leverage the \emph{Tree Mover’s Distance} to reduce both the number of graphs and the size of individual graphs. To our knowledge, our approach is the first that is supported by rigorous theoretical guarantees: we prove that training a GNN on the subsampled data results in a bounded increase in loss compared to training on the full dataset. Unlike existing methods, our approach is both \emph{model-agnostic}, requiring minimal assumptions about the GNN architecture, and \emph{label-agnostic}, eliminating the need to label the full training set. This enables subsampling early in the model development pipeline—before data annotation, model selection, and hyperparameter tuning—reducing costs and resources needed for storage, labeling, and training. We validate our theoretical results with experiments showing that our approach outperforms existing subsampling methods across multiple datasets.
 \end{abstract}


\input{introduction}
\input{prelim}
\input{graph_drop}
\input{node_drop}
\input{experiment_new}
\input{conclusion}

\section*{Acknowledgements}
The authors thank Ching-Yao Chuang and Joshua Robinson for valuable discussions.  This work was supported in part by NSF grant CCF-2338226, the NSF AI Institute TILOS and an Alexander von Humboldt fellowship.

%\FloatBarrier

\bibliography{ref}
\bibliographystyle{plainnat}



\newpage
\appendix

\input{more_background}
\input{appendix}

\end{document}

