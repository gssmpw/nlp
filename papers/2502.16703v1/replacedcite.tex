\section{Related Work}
\label{sec:related-work}

\paragraph{Coreset construction.}
Coresets have been extensively studied in classical ML for Euclidean data, with applications in mean approximation, regression, clustering, and convex optimization ____. These methods construct small, weighted subsets of datapoints that provably approximate solutions obtained on the full dataset. More recently, coresets have been explored for improving the efficiency of training deep learning models ____.  

Despite this progress, coreset selection for structured data, particularly graphs, remains largely unexplored. ____ propose a subset selection approach for node classification, but it lacks the provable guarantees typical of coreset methods and differs fundamentally from our work, which focuses on graph classification. To our knowledge, ours is the first work to introduce coreset selection for graph classification and to provide formal guarantees on GNN performance when trained on the coreset.


\paragraph{Graph dataset compression.}{ A related but distinct line of research focuses on graph condensation, which reduces the size of a GNN training dataset by building a smaller, \emph{synthetic} dataset that resembles the original. Most works in this area target node-level task, making them unsuitable for comparison with our graph-level approach. For graph-level tasks, DosCond ____ and KiDD ____ use \emph{gradient matching}, which requires access to the full GNN hyperparameter specifications and the training trajectories of a GNN trained on the original dataset. These methods build the synthetic graphs to replicate the training trajectories on the full dataset. Thus, they are neither label- nor model-agnostic and lack theoretical guarantees. Moreover, gradient matching requires training a model on the full dataset to construct the subsample, which undermines key practical motivations of subsampling.
}

\paragraph{Model-agnostic graph dataset compression.} ____ propose a model-agnostic graph dataset compression procedure, MIRAGE, that uses the training graphs' computation trees to compress the training set. Unlike gradient-matching approaches, MIRAGE is model-agnostic: it does not require knowledge of downstream model parameters to build the subsample. However, it is \emph{not} label-agnostic, as its sampling procedure relies on labels from the full dataset.



\paragraph{Other applications of TMD.} ____ uses a normalized variant of TMD to select training sets for neural algorithmic reasoning tasks. However, their work is not directly applicable to graph or node subsampling.