\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs}

\usepackage{hyperref}
\usepackage{bbm}

\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage{svg}
\usepackage{multirow}
\usepackage{multicol}
\usepackage[accepted]{icml2025}

\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{xcolor}

\newcommand{\ds}[1]{\textcolor{red}{#1}}

\usepackage[capitalize,noabbrev]{cleveref}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\usepackage[textsize=tiny]{todonotes}

\icmltitlerunning{Analyzing Similarity Metrics for Data Selection for Language Model Pretraining}

\begin{document}

\twocolumn[
\icmltitle{Analyzing Similarity Metrics for Data\\Selection for Language Model Pretraining}

\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Dylan Sam}{yyy,equal}
\icmlauthor{Ayan Chakrabarti}{comp}
\icmlauthor{Afshin Rostamizadeh}{comp}
\icmlauthor{Srikumar Ramalingam}{comp}\\
\icmlauthor{Gui Citovsky}{comp}
\icmlauthor{Sanjiv Kumar}{comp}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{Machine Learning Department, Carnegie Mellon University}
\icmlaffiliation{comp}{Google Research}

\icmlcorrespondingauthor{Dylan Sam}{dylansam@andrew.cmu.edu}

\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

\printAffiliationsAndNotice{} 

\begin{abstract}

Similarity between training examples is used to curate pretraining datasets for language models by many methods---for diversification and to select examples similar to high-quality data. However, similarity is typically measured with off-the-shelf embedding models that are generic or trained for tasks such as retrieval. This paper introduces a framework to analyze the suitability of embedding models specifically for data curation in the language model pretraining setting. We quantify the correlation between similarity in the embedding space to similarity in pretraining loss between different training examples, and how diversifying in the embedding space affects pretraining quality. We analyze a variety of embedding models in our framework, with experiments using the Pile dataset for pretraining a 1.7B parameter decoder-only language model. We find that the embedding models we consider are all useful for pretraining data curation. Moreover, a simple approach of averaging per-token embeddings proves to be surprisingly competitive with more sophisticated embedding models---likely because the latter are not designed specifically for pretraining data curation. Indeed, we believe our analysis and evaluation framework can serve as a foundation for the design of embedding models that specifically reason about similarity in pretraining datasets.

\end{abstract}

\section{Introduction}

The recent success of general purpose language models~\citep{brown2020language, chowdhery2023palm} is in no small part due to pretraining on large and diverse text corpora scraped from a variety of sources~\citep{raffel2020exploring, gao2020pile, penedo2024fineweb}. Such pretraining allows the model to quickly adapt to various downstream tasks with just few-shot prompting or minimal finetuning. And so, researchers have explored a variety of approaches to assemble effective pretraining sets, typically by selecting a high-quality and diverse subset from larger corpora of examples scraped from various sources.

These data curation approaches have shown promising results by improving example quality and by reducing redundancy in pretraining sets. Many of these methods use notions of \emph{similarity} between examples. 
Similarity of an example to text from known high-quality sources (such as Wikipedia) has been used as a proxy for the quality of that example~\citep{gunasekar2023textbooks, penedo2024fineweb}.  Meanwhile, methods focused on diversification~\citep{abbas2023semdedup, tirumala2023d4} make direct use of similarity metrics to identify and remove redundant examples. 

In this context, similarity between examples has been measured in terms of distances in an embedding space, and these methods have typically used generic embeddings for this purpose---e.g., intermediate representations learned by language models, or those trained for semantic retrieval. 
But whether these embeddings are optimal for reasoning about similarity between pretraining examples---and whether there is any benefit to using with more sophisticated and potentially expensive models---remains an open question. 

\begin{figure*}[!t]
    \centering
    \includegraphics[width=\textwidth]{figs/cluster_fig_v2.pdf}
    \caption{Correlation between pretraining loss and embedding distance. Each row shows a pair of examples close in the embedding space (in particular, from the same cluster when using K-Means clustering in this space) with examples in different rows being relatively far from each other (from different clusters). We find that the close pairs of examples in each row tend to have similar pretraining losses, while there is a greater variation in losses across rows. Note that the close example pairs are "thematically" similar, but have very different content. These results are from using the average embeddings from the final layer of a small decoder-only language model.}
    \label{fig:similarity_examples}
\end{figure*}

In this paper, we introduce a framework to analyze and quantify the suitability of a given embedding model for the task of data curation for pretraining language models. 
We consider the question of what a similarity metric should ideally capture for reasoning about pretraining examples. 
In this context, we characterize how well similarity in the embedding space correlates with similar pretraining behavior---in particular, similar pretraining losses with respect to the same model state (see Fig.~\ref{fig:similarity_examples}). 
We also quantify whether the embedding space is able to separate examples from different data sources from among a mixture of these sources, with the assumption that these sources were curated by human expertise and combined since they were found to be complementary. 
And last but not least, we use the embeddings in a simple diversification recipe in curating a pretraining set and measure the gain in quality of the resulting pretrained model over a large number of downstream tasks.

We conduct these experiments using the Pile~\citep{gao2020pile} as our data corpus, and in the context of pretraining a 1.7B decoder-only language model with a UL2 objective~\citep{tay2022ul2}. 
We evaluate a number of different embeddings models---from representations of generic language models (e.g., trained with mask-based reconstruction or mixtures of pretraining objectives) to embeddings specifically trained for retrieval or semantic equivalence. 

Our results find that most embeddings bring some utility to the pretraining data curation setting---in particular, diversity-based pretraining data curation with all of the embeddings that we consider outperforms baseline training. 
But perhaps surprisingly, a very simple embedding approach---derived from averaging over the independent per-token embeddings from the input layer of a small language model---performs fairly competitively to more sophisticated and computationally expensive approaches.

These results also strongly suggest that embedding models for reasoning about pretraining examples should be specialized, and that the attributes that make an embedding suitable for semantic matching and retrieval may not translate to benefits for the pretraining setting. 
More broadly, our work establishes a framework to evaluate current embeddings, and facilitate the design of new ones, for the important task of data curation for language model pretraining.

\section{Related Work}

\paragraph{Data Curation.} Many works have studied the problem of selecting high-quality and informative subsets from a larger corpus for efficient and effective language pretraining~\citep{Albalak2024ASO}. Indeed, works have shown that data curation for pretraining (both for language and vision-language models) improves neural scaling laws \citep{sorscher2022beyond}, and that curation techniques should be aware of the particular compute budget \citep{goyal2024scaling}.

One family of works approaches data curation with the goal of retaining only high quality examples. The simplest approaches in this family look at heuristics that can filter out noisy or extremely low-quality instances such as documents containing less than 5 sentences or those with extremely short lines of text \citep{raffel2020exploring, xue2020mt5}. Beyond these heuristic approaches, a line of work focus on extracting high-quality examples based on similarity to known high-quality sources, such as Wikipedia or textbooks~\citep{gunasekar2023textbooks, penedo2024fineweb}. 
Other works look at creating targeted sets---defining quality in terms of their relevance for particular downstream tasks \citep{xie2023data}, with some creating these by adaptively updating weights over domain mixtures \citep{xie2024doremi, jiang2024adaptive}. 

The other family of data curation approaches focus on pruning datasets to ensure a notion of \textit{diversity} \citep{abbas2023semdedup, abbas2024effective, tirumala2023d4}. While some of these works~\citep{abbas2024effective, tirumala2023d4} compare different embedding models as part of brief ablations, these comparisons are in the narrow context of their specific divsersification algorithms, and do not provide intuition as to why one embedding may perform better than another. 

Finally, it is worth mentioning that some works in data selection use the model being trained as part of the selection process, often through the use of influence functions or gradient-based methods \citep{garima2020estimating, xia2024less, engstrom2024dsdm}. 
However, these are typically used during for data-curation for multi-epoch training on small datasets or only during the finetuning stage, and would be prohibitively expensive to use for pretraining.


\paragraph{Text Embedding Models.} Another related line of work is the task of learning embedding models for text. 
Many various approaches to learn text embedding models, with objectives including mask-based reconstruction \citep{devlin2018bert, liu2019roberta}, a combination of multiple different tasks \citep{cer2018universal}, and contrastive learning-based approaches \citep{gao2021simcse, neelakantan2022text, izacard2022unsupervised, lee2024gecko}.
More recent work has demonstrated extracting embeddings via prompting, specifically designed for predicting model performance \citep{sam2025predicting}.

These embedding models have largely been studied in the context for classification or similarity measures \citep{gomaa2013survey, agirre2013sem, agirre2016semeval}, and recent focus has been improving performance on aggregate on large-scale benchmarks \citep{muennighoff2022mteb} that are comprised of multiple tasks (e.g., retrieval, clustering, classification, ranking). 
Many models that achieve strong performance on these large-scale benchmarks have benefitted from scaling \citep{jiang2023scaling, chowdhery2023palm}, with the help of distilled information from autoregressive models \citep{lee2024gecko}. 
However, this has conflicting incentives with their utility in the pretraining setting, as large text embedding models are impractical and too computationally expensive to run inference over the full pretraining corpus. 
To the best of our knowledge, our work provides the first study of various text embedding models for pretraining data curation. 

\section{Evaluating Embeddings for Pretraining Data Curation}

We now describe our approach to analyze and evaluate a given embedding model in terms of its suitability for reasoning about similarity among pretraining examples. As mentioned above, similarity is used to (a) find examples that are similar to known ``desirable'' examples (e.g., examples of known high-quality or those representing downstream tasks of interest) and to (b) discover and remove redundancies in the pretraining corpus. Accordingly, we design experiments that measure an text embedding model's performance towards these criteria.

\subsection{Evaluating Correlation with Pretraining Loss} \label{var_reduction}

We begin by evaluating whether low distances in a model's embedding space correlates with similar values of difficulty, which we measure by intermediate losses during language model pretraining. 
To do so, we first use a balanced K-Means clustering algorithm in the given embedding space to cluster all examples in the pretraining set for a target cluster size. Then, we look at the variance of loss values within each cluster. 
We note that we use balanced clustering (allowing only some variation in the sizes of different clusters) to ensure that average within-cluster variance is comparable across clusterings in different embedding spaces. We repeat this process for a multiple target cluster sizes for all embeddings.

When reporting our results, we measure \textbf{variance reduction}, or the ratio of the overall variance across all examples in the pretraining dataset to the variance in loss computed within clusters in the given embedding space. 
Specifically, we define variance reduction as
\begin{align*}
    V(C) & = \frac{E_{x \sim D}[(\ell(x) - E[\ell(x)])^2]}{E_{C_i \sim \mathbbm{C}} \left[E_{x \sim C_i}[(\ell(x) - E_{x \sim C_i}[\ell(X)])^2 ] \right] },
\end{align*}
where $C = \{C_1, ..., C_m\}$ is a partition of all datapoints in the dataset $D$, and $\mathbbm{C}$ is a uniform distribution over each disjoint set in the partition $C$. 
This metric can be interpreted as a measure of predicting pretraining generalization, as nearby points have similar loss values. 
We remark that random clusters achieve a variance reduction of 1, and larger values of variance reduction imply that points with more similar pretraining loss are clustered together.

A high value of variance reduction implies that similarity in the embedding space correlates well with pretraining loss. While an isolated pair of examples having similar pretraining loss may be entirely unrelated in quality and content, the fact that an embedding space that \emph{consistently} brings together examples of similar loss values strongly suggests that these similar examples will behave similarly in terms of contributing to the quality of the language model.

Beyond finding examples of similar quality and utility, a high variance reduction score implies that the clustering above can serve as a particularly useful proxy for more dynamic and online data sampling strategies \citep{xie2024doremi}, reducing the number of required forward passes in strategies where datapoints are selected based on their current pretraining loss. 
This also has implications towards approaches that look to self-improve models through propagating labeled information to nearby examples \citep{wei2021theoretical, cai2021theory, pukdee2023label}, such as weak-to-strong generalization \citep{burns2023weak}. 

\subsection{Measuring Cluster Purity of Data Sources} \label{cluster_purity}

Next, we study the purity of the respective (K-Means) clusters generated with each considered embedding model with respect to the underlying data source or mixture.
Large pretraining datasets such as the Pile \citep{gao2020pile} are comprised of various distinct yet complementary hand-curated data sources (e.g., high quality sources such as Wikipedia, code-based data, data from medical domains).
While there may be some similarities between datapoints from different domains, we believe that clusters should generally group points from the same domain together and separate isntances from different domains.
In fact, we remark that since embedding models are generally not trained with explicit knowledge of source labels or any domain metadata, and the ability to group together data from the same source and distinguish those from different sources shows an embedding model's alignment with human judgement (or at least that of the dataset curators).

Letting $s \in \{0, \cdots, S \}$ represent the index of the source in some finite number of sources $S$, we compute the \textbf{cluster purity} of a set of clusters $C$ as
\begin{align*}
    P(C) & = \mathbbm{E}_{C_i \sim \mathbbm{C}}\left[ \frac{\max_s | C_i \cap D_s |}{|C_i|} \right],
\end{align*}
where $D_s$ represents the subset of datapoints that are from source $s$. Intuitively, this represents the proportion of the maximum frequency source to the total cluster size. A cluster purity of 1 represents a clustering that completely separates examples from different domains.

\subsection{Diversification-based Pretraining Data Curation} \label{sec:diverse_selection}

Prior work has demonstrated that diversification, based on similarity in embedding spaces, can yield more effective pretraining datasets that in-turn lead to improved models~\citep{tirumala2023d4}. While the diversification techniques themselves tend to be sophisticated, our goal here is to evaluate the utility of a given embedding for diversification. Therefore, we use a simplified version of these diversification of approaches to (1) select a subset from a larger pretraining corpus, (2) pretrain a model on this curated subset, and then (3) report performance on a large set of downstream tasks.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.495\textwidth]{figs/vary_ckpt_v3.pdf}
    \includegraphics[width=0.495\textwidth]{figs/var_reduction_v3.pdf}
    \caption{Reduction in variance in loss among clusters (e.g., overall variance / clustering variance) as increase the number of gradient steps in pretraining (left). Reduction in variance as we vary the approximate cluster size used in K-means for clustering examples with various embedding models (right). We compute our results on the checkpoint after 26k gradient steps. In both figures, we compute our results over 50 million sampled clusters from the Pile, and we remark that larger values are better. The overall pretraining loss variances for checkpoints of 26k, 50k, and 76k training steps are 0.521, 0.508, and 0.489 respectively. We find that embeddings extracted from a small version of the downstream language model being trained best cluster points with similar pretraining losses, across different intermediate training checkpoints and cluster sizes.}
    \vspace{-2mm}
    \label{fig:loss_clusters}
\end{figure*}


We begin by clustering the larger pretraining corpus in the given embedding space; in contrast to \Cref{var_reduction} and \Cref{cluster_purity}, we do not use balanced clustering which can lead to clusters with a wide variation in ``diameters'' (distances among points in the same cluster). 
Instead, since the goal here is diversification, we find clusters such that distances between all pairs of points in the same cluster are within a specified threshold $\epsilon$.

Given clusters of pretraining data, prior work often uses complex pruning or sampling schemes from large clusters. For simplicity, we simply take the point in each cluster that is closest to the cluster centroid (i.e., the average embedding of all points in the cluster), which likely represents the most representative example for that cluster. 
We note that prior work \citep{tirumala2023d4} emphasizes the importance of a deduplication step in language model pretraining, and our curation strategy implicitly performs this process when selecting only a single point from each cluster.

Given that we select a single point from each cluster, this implies that we need the number of clusters to be equal or greater to the number of desired pretraining examples. In practice, we sweep a large grid of $\epsilon$ values and select the largest threshold that still produces a sufficient number of clusters. 
Another important point of note is that producing such a large number of clusters is computationally expensive. 
To be able to scale to such a large number of clusters, we use the reciprocal agglomerative clustering (RAC) \citep{sumengen2021scaling} algorithm. 
This has the advantage of building a clustering from progressively larger values of $\epsilon$, which works well with our need of a grid search over values of $\epsilon$.

Finally, we pretrain a language model on the selected subset of examples, and measure few-shot performance of the pretrained model on a diverse variety of downstream tasks.


\section{Experiments}

We analyze various different approaches to extract text embeddings across multiple different pretraining settings. 

\subsection{Experiment Details}

\paragraph{Text Embedding Models} For text embedding models, we compare Universal Sentence Encoders (\textbf{USE}) \citep{cer2018universal}, which are general purpose text encoders, and \textbf{Gecko} \citep{lee2024gecko}, a retrieval-focused text embedding model trained via synthetic data distilled from LLMs, and \textbf{BERT} \citep{devlin2018bert} models. 
We also compare against various techniques to extract embeddings from a small version of the downstream language model we are training (e.g., a ~200M parameter language model). 
First, we extract an embedding by simply averaging the token embeddings matrix over all tokens in the input sequence, which we refer to as \textbf{LM Token Embeds}. We note that this is equivalent to a learned unigram model (i.e., ignoring all positional information). 
We also extract an embeddings from a forward pass of the language model by averaging the activations over all tokens in the input sequence at the final hidden layer, which we refer to as \textbf{LM Output Embeds}. 
In our diversity-based pretraining data curation experiments, we also add a comparison to a naive, random subset selection (\textbf{Baseline}) to evaluate how much benefit is added from our standardized curation strategy with these various embedding models.

To make clustering with a large number of output clusters feasible and efficient at pretraining data scales, we perform dimensionality reduction using PCA on a large subset of the datapoints ($\sim$500,000 examples) and extract the top 64 dimensions upon which to project. The projected embeddings are then normalized to have a unit L2 norm.
We also explore using random projections \citep{bingham2001random} as a different dimensionality reduction strategy, finding that random projections do not perform as well as PCA (see \Cref{sec:ablations} for ablation studies). 

\paragraph{Dataset Details}
For all of our experiments (e.g., pretraining data curation and predicting loss generalization), we use the Pile \citep{gao2020pile}. 
We pack together documents into a sequence of length 1280, with ``[eod]'' as delimeters between documents.

\paragraph{Pretraining Details}
For our pretraining experiments, we train a 1.7B parameter decoder-only language model with a UL2 objective \citep{tay2022ul2}, on the curated subset of the Pile data through the process outlined in \Cref{sec:diverse_selection}. We use a selection budget of 200B tokens, which is approximately 20\% of sequences from the Pile. This corresponds to roughly 170 million clusters for each embedding model, with an average cluster size of 5 examples. 

We pretrain with a learning rate of 0.001 with a linear decay and a batch size of 1024. 
For our tokenizer, we use sentencepiece with a vocabulary size of 256k tokens. 
For our pretraining evaluation, we consider the same downstream evaluation datasets from the work of \citet{brown2020language}. 
This invovles a wide variety of 1-shot scoring tasks, as well as open-ended text generation tasks. The full details about our evaluation sets are deferred to \Cref{appx:eval_details}. 
For the embeddings produced via LM Token Embeds and LM Output Embeds, we train a 200M parameter language model in the same fashion as above, where we train on a total of 200B randomly selected tokens.

\subsection{Pretraining Loss Correlation Results}

To measure the ability of embedding models to reflect pretraining loss generalization and to cluster together datapoints with similar difficulty, we report their variance reduction as we vary (1) the number of gradient updates in language model pretraining and (2) as we vary the average cluster size in our K-Means clustering. 

We observe that embeddings produced via a small UL2 model (i.e., LM Token Embeds and LM Output Embeds) trained on the same data achieve a much larger reduction in loss variance when compared to off-the-shelf models; these embeddings produce better clusters with datapoints with similar intermediate pretraining losses (Figure \ref{fig:loss_clusters}). 
These trends hold across multiple different intermediate training checkpoints and for all different cluster sizes. 
This suggests that training models specifically on the data to perform data curation is preferable to using off-the-shelf embedding models for predicting pretraining loss generalization.
Furthermore, we also generally observe that newer embedding models (e.g., Gecko) do not improve upon older alternatives (e.g., BERT and USE), supporting that embedding model improvements in other settings such as retrieval \textit{do not translate} to this application in the pretraining setting.

\paragraph{Visualizing Within-Cluster Examples} 

To better interpret what is computed by these similarity metrics, we also visualize pairs of datapoints from the various clusters in \Cref{fig:similarity_examples} to understand what types of examples are placed closely together and which have similar pretraining losses. 
We randomly sample 3 clusters produced by K-Means clusters with the smallest average cluster size with the LM Output Embeds method to extract embeddings. 
We randomly sample 2 points from these 3 clusters and observe that samples from the same cluster are thematically similar, although they contains very different information (e.g., the last example referring to two different people in history with some governmental relations, although from different times and different countries). 
We observe that pretraining losses are similar between within-cluster examples, and lower distances correspond to lower differences in pretraining loss. 

\subsection{Cluster Purity Results}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.99\columnwidth]{figs/purity_v3.pdf}
    \caption{Comparison of the purity with respect to data source of K-Means clustering produced by various embedding models on the Pile. Cluster purities are averaged over 50 million clusters.}
    \label{fig:purity}
    \vspace{-2mm}
\end{figure}

To better understand and interpret the similarity metrics defined by these embedding models, we measure the purity of clusters with respect to the underlying data source. 
We again remark that none of these embedding models have been trained with knowledge of the data source. 
Overall, we observe that most embedding models produce fairly pure clusters, where a majority of points come from the same underlying data source (\Cref{fig:purity}). 
This supports that these embeddings models are generally aligned with human judgement about differences between types of data. 

We observe that the embeddings extracted from the small version of the trained downstream language model achieve the highest cluster purity. 
We also note that Gecko embeddings achieve the third-highest cluster purity, whereas it performs poorly on the task of predicting loss generalization through variance reduction. 
This suggests that an improved ability to predict pretraining loss generalization cannot simply be explained via producing clusters that are more pure with respect to underlying data sources.
Future work could explore incorporating domain information or metadata into the embedding model's learning objective, as a form of weak supervision \citep{sam2023losses}.

\begin{table*}[t]
    \centering
    \begin{tabular}{l|c | ccc | cc} \toprule
    \multirow{2}{*}{\textbf{Task}}  & \multirow{2}{*}{\textbf{Baseline}}  & \multirow{2}{*}{\textbf{USE}}  & \multirow{2}{*}{\textbf{Gecko}} & \multirow{2}{*}{\textbf{BERT}} & \multirow{2}{*}{\shortstack{\textbf{LM Token}\\\textbf{Embeds}}} & \multirow{2}{*}{\shortstack{\textbf{LM Output}\\\textbf{Embeds}}} \\ 
    & & & & & & \\ \midrule
    ARC - Challenge    & 32.44            & 32.99         & \textbf{33.73}   & 32.71          & 32.54            & \underline{33.53}   \\
    ARC - Easy         & 63.78            & \underline{65.12}         & 64.24          & 65.08          & 64.91            & \textbf{65.46}      \\
    BoolQ              & 56.47            & 59.18         & 58.31          & 60.08          & \textbf{62.86}  & \underline{61.93}   \\
    SuperGLUE - CB     & 42.41            & 43.45         & \underline{48.21} & 41.67          & 42.86            & \textbf{48.81}      \\
    SuperGLUE - Copa   & 75.25            & 76.33         & \underline{77.33} & 74.00          & 74.67            & \textbf{78.00}      \\
    HellaSwag          & 54.98            & 56.71         & \underline{57.11} & 56.54          & 56.76            & \textbf{57.53}      \\
    Multi RC           & \textbf{57.71}   & 55.69         & 55.91          & \underline{56.27} & 55.95            & 55.19              \\
    OpenBook QA        & 46.30            & 46.20         & 46.27          & \underline{46.60} & 45.93            & \textbf{46.67}      \\
    PiQA               & 72.27            & 72.98         & 72.74          & \underline{73.58} & 72.76            & \textbf{73.63}      \\
    Race H             & 38.02            & 37.94         & 38.47          & \underline{38.75} & \textbf{38.84}   & 38.70              \\
    Race M             & 51.71            & 52.00         & \textbf{53.32}   & \underline{52.88} & 52.09            & 52.60              \\
    ReCoRD             & \textbf{84.95}   & 84.35         & 84.64          & 84.71          & 84.51            & \underline{84.91}   \\
    RTE                & \underline{54.51}& \textbf{54.87}& 51.50          & \textbf{54.87} & 51.62            & 52.83              \\
    Story Cloze        & 73.93            & 74.10         & \underline{74.26} & 73.95          & 73.26            & \textbf{74.49}      \\
    WiC                & \textbf{48.20}   & 47.86         & 47.49          & 47.54          & \underline{48.17} & 47.28              \\
    Winograd           & 73.99            & 75.95         & \underline{77.05} & \textbf{77.29} & \underline{77.05} & 76.56              \\
    WinoGrande         & 59.23            & 58.91         & 59.01          & \underline{59.46} & \textbf{59.96}   & 59.25              \\
    WSC                & 74.12            & 73.92         & 73.68          & \underline{74.74} & 73.22            & \textbf{74.97}      \\
    \midrule
    Lambada            & 21.37            & 29.86         & \textbf{40.12}   & \underline{34.69} & 31.43            & 33.82              \\
    Natural Questions  & 10.01            & 10.76         & 10.13          & 10.54          & \underline{11.13} & \textbf{11.42}      \\
    SQuAD v2           & 51.05            & 54.26         & 51.70          & 51.79          & \textbf{58.56}   & \underline{55.37}   \\
    TriviaQA Wiki      & 34.39            & \underline{35.01} & 32.99          & 34.95          & 34.29            & \textbf{36.08}      \\
    Web Questions      & 17.05            & 17.04         & 17.87          & 17.98          & \underline{18.73} & \textbf{19.13}      \\
    \midrule
    \textbf{Average}   & 51.92            & 52.85         & \underline{53.30}& 53.07          & 53.13            & \textbf{53.83}      \\
    \bottomrule
    \end{tabular}
    \caption{Performance of using various embedding models in the diversity-based selection scheme to select a subset of 200B tokens from the Pile to pretrain a 1.7B parameter language model with a UL2 \citep{tay2022ul2} objective. Bolding and underlining denote the best and second-best performing methods on each task, respectively. Results are averaged over 3 pretraining runs. We find that embeddings extracted from small version of the downstream language model and Gecko perform the best for pretraining data curation.}
    \vspace{-2mm}
    \label{tab:pretraining_results}
\end{table*}



\subsection{Diversification-based Pretraining Data Curation Results}

We report the average downstream task performance of language models trained on pretraining data subsets produced by using various embedding models in the simple diversity-based data curation strategy in \Cref{tab:pretraining_results}. 
We first observe that the standardized diversity-based data curation technique applied to all embedding models (both off-the-shelf and those based on small downstream trained language models) outperform the naive random subset selection baseline. 
Secondly, we observe the LM Output Embeds and Gecko embeddings achieve the best and second-best performance when averaged across all tasks, respectively. 

A notable result is that LM Token Embeds performs comparably to Gecko, while outperforming all the other off-the-shelf embedding models. 
This is of practical interest due to the simplicity of producing these embeddings; they are simply constructed via averaging over the learned token embeddings and, consequently, are extremely quick to compute in comparison to all other embedding methods. Thus, LM Token Embeds serves as a powerful alternative in settings that are compute-limited.

These pretraining results also suggest that even simple notions of similarity, even those that do not account for positional information, are sufficient for many pretraining data curation applications.
Finally, they support that the training of models specifically for this task (e.g., on the same pretraining dataset) is preferable to the use of general purpose off-the-shelf embedding models.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.49\textwidth]{figs/gecko_pca.pdf}
    \includegraphics[width=0.49\textwidth]{figs/lm_output_pca.pdf}
    \vspace{-3mm}
    \caption{Ablation on the number of components in PCA for Gecko and LM Output Embeds. Results are averaged over 50 million sampled clusters from the Pile. Using more components in PCA better cluster points with similar pretraining loss.}
    \label{fig:clustering_pca}
\end{figure*}


\begin{figure*}[t]
    \centering
    \includegraphics[width=0.49\textwidth]{figs/gecko_rp.pdf}
    \includegraphics[width=0.49\textwidth]{figs/lm_output_rp.pdf}
    \vspace{-3mm}
    \caption{Ablation comparing the use of PCA or Random projections for dimensionality reduction in Gecko and LM Output Embeds. Results are averaged over 50 million sampled clusters from the Pile. Dimensionality reduction via PCA performs better for variance reduction when compared to Random Projections.}
    \label{fig:random_projections}
\end{figure*}

\subsection{Ablations} \label{sec:ablations}

We present ablations on the various dimensionality reduction strategies (i.e., technique and resulting size) that we use for the embedding models before running clustering. We present the ablations on Gecko and LM Output Embeds here and defer results on others embeddings to Appendix \ref{appx:extra_results}.

\paragraph{Impact of Dimensionality on Variance Reduction} The dimensionality of the embeddings used in clustering often must be low for pretraining scales. Here, we run an ablation studying the role of dimensionality (i.e., the number of components in PCA onto which the embeddings are projected) in the variance reduction through clustering (\Cref{fig:clustering_pca}). 
We remark that scaling clustering to accommodate smaller average cluster sizes (i.e., more cluster centers) is intractable for embeddings that have high dimensionality. Thus, we can only report results on various embeddings with a large number (i.e., 256) of components in PCA with larger average cluster sizes.  
We observe the trend across all embeddings that a higher dimension and larger number of PCA components improves the embedding models' ability to cluster points by pretraining loss (see other embedding model results in Appendix \ref{appx:pca}).

\paragraph{Dimensionality Reduction Techniques}
Another common technique to perform dimensionality reduction (especially with respect to maintaining pairwise distances and similarities) is to use random projections. 
We experiment with using random projections in our loss clustering experiments and observe that it is outperformed by PCA in terms of variance reduction across all cluster sizes (\Cref{fig:random_projections}). 

\section{Discussion}

We analyze the fundamental role of computing similarity in pretraining data curation, finding that coarse-grained notions of similarity are particularly useful and that specific models (even simple ones based on averaging learned token embeddings) can outperform other off-the-shelf embedding models. 
We believe that both our results and the new evaluation framework for similarity metrics provide new insights into the role of similarity in pretraining data curation. 
For instance, our variance reduction results shows that embedding models tightly cluster points with similar values of pretraining losses, which has larger implications for dynamic data selection.
Future work could study that, during training, we can compute the loss on the centroid in each cluster, and use this to select clusters on which to train, that contain points with high values of these intermediate loss.
Our method has other, broad potential applications in the pretraining data setting, such as selecting task-specific data for finetuning \citep{xia2024less} or even in evaluating the generation of synthetic data for \citep{meng2022generating, sam2024finetuning} by looking at the distance between these potential data to be selected or generated and real data.
Finally, our results highlight the benefits for designing models specifically for language model pretraining settings, and our framework provides an evaluation to exactly capture their utility.  

\bibliography{icml2025_main}
\bibliographystyle{icml2025}

\newpage
\appendix
\onecolumn

\include{appendix}

\end{document}