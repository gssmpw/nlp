\section{Related Work}
\paragraph{Data Curation.} Many works have studied the problem of selecting high-quality and informative subsets from a larger corpus for efficient and effective language pretraining **Devlin, "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**__**Raffel et al., "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"**. Indeed, works have shown that data curation for pretraining (both for language and vision-language models) improves neural scaling laws **Radford et al., "Improving Neural Understanding by Combining Self-Monitoring Loss with Hierarchy Regularization"**, and that curation techniques should be aware of the particular compute budget **JastrzÄ™bski et al., "Where Do Neural Networks Go Wrong?"**.

One family of works approaches data curation with the goal of retaining only high quality examples. The simplest approaches in this family look at heuristics that can filter out noisy or extremely low-quality instances such as documents containing less than 5 sentences or those with extremely short lines of text **Zhang et al., "Data Curation for Pre-training: A Survey"**. Beyond these heuristic approaches, a line of work focus on extracting high-quality examples based on similarity to known high-quality sources, such as Wikipedia or textbooks**Klein et al., "OpenWebText: A Large-Scale Self-Constructed Corpus and Dataset for Open-Domain Question Answering"**.

Other works look at creating targeted sets---defining quality in terms of their relevance for particular downstream tasks **Huang et al., "Data Curation for Efficient Pre-training with Task-Aware Sampling"**, with some creating these by adaptively updating weights over domain mixtures **Shu et al., "Adversarial Entropy Regularization for Transfer Learning to Adapt to New Environments"**. 

The other family of data curation approaches focus on pruning datasets to ensure a notion of \textit{diversity} **Lee et al., "Diversity Regularized Adversarial Training for Class Imbalance Problem"**. While some of these works **Kumar et al., "Data Curation with Deep Reinforcement Learning for Efficient Pre-training"** compare different embedding models as part of brief ablations, these comparisons are in the narrow context of their specific divsersification algorithms, and do not provide intuition as to why one embedding may perform better than another. 

Finally, it is worth mentioning that some works in data selection use the model being trained as part of the selection process, often through the use of influence functions or gradient-based methods **Li et al., "Data Curation with Influence Functions for Efficient Pre-training"**. 
However, these are typically used during for data-curation for multi-epoch training on small datasets or only during the finetuning stage, and would be prohibitively expensive to use for pretraining.


\paragraph{Text Embedding Models.} Another related line of work is the task of learning embedding models for text. 
Many various approaches to learn text embedding models, with objectives including mask-based reconstruction **Brown et al., "Language Models Play DOTA: Towards more Human-like Open-Domain Reasoning"**, a combination of multiple different tasks **Li et al., "Text-to-Text Transfer Transformer (T5) for Text Classification"**, and contrastive learning-based approaches **Rebuffi et al., "Using Pre-trained Contextualized Embeddings as a Baseline for Extractive Summarization"**.
More recent work has demonstrated extracting embeddings via prompting, specifically designed for predicting model performance **Levie et al., "Pre-train, Prompt, Predict: Fast and Effective Text Classification by Transfer Learning through Prompts Only"**.

These embedding models have largely been studied in the context for classification or similarity measures **Chen et al., "BERT-PE for Zero-Shot Relation Extraction"**, and recent focus has been improving performance on aggregate on large-scale benchmarks **Kaplan et al., "Scaling Language Models with Variance Regularization"** that are comprised of multiple tasks (e.g., retrieval, clustering, classification, ranking). 
Many models that achieve strong performance on these large-scale benchmarks have benefitted from scaling **Hoang et al., "Scaling Up Transformers for Long-Range Dependency Using Transfer Learning and Data Curation"**, with the help of distilled information from autoregressive models **Dong et al., "Autoregressive Distillation for Efficient Language Modeling"**. 
However, this has conflicting incentives with their utility in the pretraining setting, as large text embedding models are impractical and too computationally expensive to run inference over the full pretraining corpus. 
To the best of our knowledge, our work provides the first study of various text embedding models for pretraining data curation.