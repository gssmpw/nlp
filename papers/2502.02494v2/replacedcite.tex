\section{Related Work}
\paragraph{Data Curation.} Many works have studied the problem of selecting high-quality and informative subsets from a larger corpus for efficient and effective language pretraining____. Indeed, works have shown that data curation for pretraining (both for language and vision-language models) improves neural scaling laws ____, and that curation techniques should be aware of the particular compute budget ____.

One family of works approaches data curation with the goal of retaining only high quality examples. The simplest approaches in this family look at heuristics that can filter out noisy or extremely low-quality instances such as documents containing less than 5 sentences or those with extremely short lines of text ____. Beyond these heuristic approaches, a line of work focus on extracting high-quality examples based on similarity to known high-quality sources, such as Wikipedia or textbooks____. 
Other works look at creating targeted sets---defining quality in terms of their relevance for particular downstream tasks ____, with some creating these by adaptively updating weights over domain mixtures ____. 

The other family of data curation approaches focus on pruning datasets to ensure a notion of \textit{diversity} ____. While some of these works____ compare different embedding models as part of brief ablations, these comparisons are in the narrow context of their specific divsersification algorithms, and do not provide intuition as to why one embedding may perform better than another. 

Finally, it is worth mentioning that some works in data selection use the model being trained as part of the selection process, often through the use of influence functions or gradient-based methods ____. 
However, these are typically used during for data-curation for multi-epoch training on small datasets or only during the finetuning stage, and would be prohibitively expensive to use for pretraining.


\paragraph{Text Embedding Models.} Another related line of work is the task of learning embedding models for text. 
Many various approaches to learn text embedding models, with objectives including mask-based reconstruction ____, a combination of multiple different tasks ____, and contrastive learning-based approaches ____.
More recent work has demonstrated extracting embeddings via prompting, specifically designed for predicting model performance ____.

These embedding models have largely been studied in the context for classification or similarity measures ____, and recent focus has been improving performance on aggregate on large-scale benchmarks ____ that are comprised of multiple tasks (e.g., retrieval, clustering, classification, ranking). 
Many models that achieve strong performance on these large-scale benchmarks have benefitted from scaling ____, with the help of distilled information from autoregressive models ____. 
However, this has conflicting incentives with their utility in the pretraining setting, as large text embedding models are impractical and too computationally expensive to run inference over the full pretraining corpus. 
To the best of our knowledge, our work provides the first study of various text embedding models for pretraining data curation.