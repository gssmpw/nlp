\section{Related Work}
\paragraph{Data Curation.} Many works have studied the problem of selecting high-quality and informative subsets from a larger corpus for efficient and effective language pretraining~\citep{Albalak2024ASO}. Indeed, works have shown that data curation for pretraining (both for language and vision-language models) improves neural scaling laws \citep{sorscher2022beyond}, and that curation techniques should be aware of the particular compute budget \citep{goyal2024scaling}.

One family of works approaches data curation with the goal of retaining only high quality examples. The simplest approaches in this family look at heuristics that can filter out noisy or extremely low-quality instances such as documents containing less than 5 sentences or those with extremely short lines of text \citep{raffel2020exploring, xue2020mt5}. Beyond these heuristic approaches, a line of work focus on extracting high-quality examples based on similarity to known high-quality sources, such as Wikipedia or textbooks~\citep{gunasekar2023textbooks, penedo2024fineweb}. 
Other works look at creating targeted sets---defining quality in terms of their relevance for particular downstream tasks \citep{xie2023data}, with some creating these by adaptively updating weights over domain mixtures \citep{xie2024doremi, jiang2024adaptive}. 

The other family of data curation approaches focus on pruning datasets to ensure a notion of \textit{diversity} \citep{abbas2023semdedup, abbas2024effective, tirumala2023d4}. While some of these works~\citep{abbas2024effective, tirumala2023d4} compare different embedding models as part of brief ablations, these comparisons are in the narrow context of their specific divsersification algorithms, and do not provide intuition as to why one embedding may perform better than another. 

Finally, it is worth mentioning that some works in data selection use the model being trained as part of the selection process, often through the use of influence functions or gradient-based methods \citep{garima2020estimating, xia2024less, engstrom2024dsdm}. 
However, these are typically used during for data-curation for multi-epoch training on small datasets or only during the finetuning stage, and would be prohibitively expensive to use for pretraining.


\paragraph{Text Embedding Models.} Another related line of work is the task of learning embedding models for text. 
Many various approaches to learn text embedding models, with objectives including mask-based reconstruction \citep{devlin2018bert, liu2019roberta}, a combination of multiple different tasks \citep{cer2018universal}, and contrastive learning-based approaches \citep{gao2021simcse, neelakantan2022text, izacard2022unsupervised, lee2024gecko}.
More recent work has demonstrated extracting embeddings via prompting, specifically designed for predicting model performance \citep{sam2025predicting}.

These embedding models have largely been studied in the context for classification or similarity measures \citep{gomaa2013survey, agirre2013sem, agirre2016semeval}, and recent focus has been improving performance on aggregate on large-scale benchmarks \citep{muennighoff2022mteb} that are comprised of multiple tasks (e.g., retrieval, clustering, classification, ranking). 
Many models that achieve strong performance on these large-scale benchmarks have benefitted from scaling \citep{jiang2023scaling, chowdhery2023palm}, with the help of distilled information from autoregressive models \citep{lee2024gecko}. 
However, this has conflicting incentives with their utility in the pretraining setting, as large text embedding models are impractical and too computationally expensive to run inference over the full pretraining corpus. 
To the best of our knowledge, our work provides the first study of various text embedding models for pretraining data curation.