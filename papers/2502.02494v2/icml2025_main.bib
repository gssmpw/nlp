@article{tay2022ul2,
  title={Ul2: Unifying language learning paradigms},
  author={Tay, Yi and Dehghani, Mostafa and Tran, Vinh Q and Garcia, Xavier and Wei, Jason and Wang, Xuezhi and Chung, Hyung Won and Shakeri, Siamak and Bahri, Dara and Schuster, Tal and others},
  journal={arXiv preprint arXiv:2205.05131},
  year={2022}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{Albalak2024ASO,
  title={A Survey on Data Selection for Language Models},
  author={Alon Albalak and Yanai Elazar and Sang Michael Xie and Shayne Longpre and Nathan Lambert and Xinyi Wang and Niklas Muennighoff and Bairu Hou and Liangming Pan and Haewon Jeong and Colin Raffel and Shiyu Chang and Tatsunori Hashimoto and William Yang Wang},
  journal={ArXiv},
  year={2024},
  volume={abs/2402.16827},
}

@article{tirumala2023d4,
  title={D4: Improving llm pretraining via document de-duplication and diversification},
  author={Tirumala, Kushal and Simig, Daniel and Aghajanyan, Armen and Morcos, Ari},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={53983--53995},
  year={2023}
}

@article{abbas2023semdedup,
  title={Semdedup: Data-efficient learning at web-scale through semantic deduplication},
  author={Abbas, Amro and Tirumala, Kushal and Simig, D{\'a}niel and Ganguli, Surya and Morcos, Ari S},
  journal={arXiv preprint arXiv:2303.09540},
  year={2023}
}

@article{gunasekar2023textbooks,
  title={Textbooks are all you need},
  author={Gunasekar, Suriya and Zhang, Yi and Aneja, Jyoti and Mendes, Caio C{\'e}sar Teodoro and Del Giorno, Allie and Gopi, Sivakanth and Javaheripi, Mojan and Kauffmann, Piero and de Rosa, Gustavo and Saarikivi, Olli and others},
  journal={arXiv preprint arXiv:2306.11644},
  year={2023}
}

@article{penedo2024fineweb,
  title={The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale},
  author={Penedo, Guilherme and Kydl{\'\i}{\v{c}}ek, Hynek and Lozhkov, Anton and Mitchell, Margaret and Raffel, Colin and Von Werra, Leandro and Wolf, Thomas and others},
  journal={arXiv preprint arXiv:2406.17557},
  year={2024}
}

@article{xie2023data,
  title={Data selection for language models via importance resampling},
  author={Xie, Sang Michael and Santurkar, Shibani and Ma, Tengyu and Liang, Percy S},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={34201--34227},
  year={2023}
}

@inproceedings{
xia2024less,
title={{LESS}: Selecting Influential Data for Targeted Instruction Tuning},
author={Mengzhou Xia and Sadhika Malladi and Suchin Gururangan and Sanjeev Arora and Danqi Chen},
booktitle={Forty-first International Conference on Machine Learning},
year={2024},
url={https://openreview.net/forum?id=PG5fV50maR}
}


@inproceedings{garima2020estimating,
  title={Estimating training data influence by tracing gradient descent},
  author={Garima and Liu, Frederick and Kale, Satyen and Sundararajan, Mukund},
  booktitle={Proceedings of the 34th International Conference on Neural Information Processing Systems},
  pages={19920--19930},
  year={2020}
}

@article{sachdeva2024train,
  title={How to Train Data-Efficient LLMs},
  author={Sachdeva, Noveen and Coleman, Benjamin and Kang, Wang-Cheng and Ni, Jianmo and Hong, Lichan and Chi, Ed H and Caverlee, James and McAuley, Julian and Cheng, Derek Zhiyuan},
  journal={arXiv preprint arXiv:2402.09668},
  year={2024}
}

@inproceedings{
engstrom2024dsdm,
title={DsDm: Model-Aware Dataset Selection with Datamodels},
author={Logan Engstrom and Axel Feldmann and Aleksander Madry},
booktitle={Forty-first International Conference on Machine Learning},
year={2024},
url={https://openreview.net/forum?id=GC8HkKeH8s}
}

@article{gomaa2013survey,
  title={A survey of text similarity approaches},
  author={Gomaa, Wael H and Fahmy, Aly A and others},
  journal={international journal of Computer Applications},
  volume={68},
  number={13},
  pages={13--18},
  year={2013},
  publisher={Citeseer}
}

@article{jiang2023scaling,
  title={Scaling sentence embeddings with large language models},
  author={Jiang, Ting and Huang, Shaohan and Luan, Zhongzhi and Wang, Deqing and Zhuang, Fuzhen},
  journal={arXiv preprint arXiv:2307.16645},
  year={2023}
}

@inproceedings{
abbas2024effective,
title={Effective pruning of web-scale datasets based on complexity of concept clusters},
author={Amro Kamal Mohamed Abbas and Evgenia Rusak and Kushal Tirumala and Wieland Brendel and Kamalika Chaudhuri and Ari S. Morcos},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=CtOA9aN8fr}
}


@inproceedings{goyal2024scaling,
  title={Scaling Laws for Data Filtering--Data Curation cannot be Compute Agnostic},
  author={Goyal, Sachin and Maini, Pratyush and Lipton, Zachary C and Raghunathan, Aditi and Kolter, J Zico},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={22702--22711},
  year={2024}
}

@article{sorscher2022beyond,
  title={Beyond neural scaling laws: beating power law scaling via data pruning},
  author={Sorscher, Ben and Geirhos, Robert and Shekhar, Shashank and Ganguli, Surya and Morcos, Ari},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={19523--19536},
  year={2022}
}

% old LMs that do heuristic filtering
@article{xue2020mt5,
  title={mt5: A massively multilingual pre-trained text-to-text transformer},
  author={Xue, L},
  journal={arXiv preprint arXiv:2010.11934},
  year={2020}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of machine learning research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}


% text embedding models

@article{lee2024gecko,
  title={Gecko: Versatile text embeddings distilled from large language models},
  author={Lee, Jinhyuk and Dai, Zhuyun and Ren, Xiaoqi and Chen, Blair and Cer, Daniel and Cole, Jeremy R and Hui, Kai and Boratko, Michael and Kapadia, Rajvi and Ding, Wen and others},
  journal={arXiv preprint arXiv:2403.20327},
  year={2024}
}

@article{cer2018universal,
  title={Universal sentence encoder},
  author={Cer, D},
  journal={arXiv preprint arXiv:1803.11175},
  year={2018}
}

@article{neelakantan2022text,
  title={Text and code embeddings by contrastive pre-training},
  author={Neelakantan, Arvind and Xu, Tao and Puri, Raul and Radford, Alec and Han, Jesse Michael and Tworek, Jerry and Yuan, Qiming and Tezak, Nikolas and Kim, Jong Wook and Hallacy, Chris and others},
  journal={arXiv preprint arXiv:2201.10005},
  year={2022}
}


@article{muennighoff2022mteb,
  title={MTEB: Massive text embedding benchmark},
  author={Muennighoff, Niklas and Tazi, Nouamane and Magne, Lo{\"\i}c and Reimers, Nils},
  journal={arXiv preprint arXiv:2210.07316},
  year={2022}
}

% contrastive learning
@inproceedings{chen2020simple,
  title={A simple framework for contrastive learning of visual representations},
  author={Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  booktitle={International conference on machine learning},
  pages={1597--1607},
  year={2020},
  organization={PMLR}
}



@inproceedings{gao2021simcse,
  title={SimCSE: Simple Contrastive Learning of Sentence Embeddings},
  author={Gao, T and Yao, X and Chen, Danqi},
  booktitle={EMNLP 2021-2021 Conference on Empirical Methods in Natural Language Processing, Proceedings},
  year={2021}
}


% datasets
@article{gao2020pile,
  title={The pile: An 800gb dataset of diverse text for language modeling},
  author={Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and others},
  journal={arXiv preprint arXiv:2101.00027},
  year={2020}
}

% OTHER
@article{sumengen2021scaling,
  title={Scaling hierarchical agglomerative clustering to billion-sized datasets},
  author={Sumengen, Baris and Rajagopalan, Anand and Citovsky, Gui and Simcha, David and Bachem, Olivier and Mitra, Pradipta and Blasiak, Sam and Liang, Mason and Kumar, Sanjiv},
  journal={arXiv preprint arXiv:2105.11653},
  year={2021}
}

@inproceedings{sam2023losses,
  title={Losses over labels: Weakly supervised learning via direct loss construction},
  author={Sam, Dylan and Kolter, J Zico},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={37},
  pages={9695--9703},
  year={2023}
}

@inproceedings{bingham2001random,
  title={Random projection in dimensionality reduction: applications to image and text data},
  author={Bingham, Ella and Mannila, Heikki},
  booktitle={Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining},
  pages={245--250},
  year={2001}
}

@inproceedings{guo2020accelerating,
  title={Accelerating large-scale inference with anisotropic vector quantization},
  author={Guo, Ruiqi and Sun, Philip and Lindgren, Erik and Geng, Quan and Simcha, David and Chern, Felix and Kumar, Sanjiv},
  booktitle={International Conference on Machine Learning},
  pages={3887--3896},
  year={2020},
  organization={PMLR}
}

@article{meng2022generating,
  title={Generating training data with language models: Towards zero-shot language understanding},
  author={Meng, Yu and Huang, Jiaxin and Zhang, Yu and Han, Jiawei},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={462--477},
  year={2022}
}

@article{sam2024finetuning,
  title={Finetuning CLIP to Reason about Pairwise Differences},
  author={Sam, Dylan and Willmott, Devin and Semedo, Joao D and Kolter, J Zico},
  journal={arXiv preprint arXiv:2409.09721},
  year={2024}
}

@inproceedings{he2022masked,
  title={Masked autoencoders are scalable vision learners},
  author={He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Doll{\'a}r, Piotr and Girshick, Ross},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={16000--16009},
  year={2022}
}

% evals
@article{allenai:arc,
      author    = {Peter Clark  and Isaac Cowhey and Oren Etzioni and Tushar Khot and
                    Ashish Sabharwal and Carissa Schoenick and Oyvind Tafjord},
      title     = {Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge},
      journal   = {arXiv:1803.05457v1},
      year      = {2018},
}

@inproceedings{clark2019boolq,
  author =    {Clark, Christopher and Lee, Kenton and Chang, Ming-Wei and Kwiatkowski, Tom and Collins, Michael and Toutanova, Kristina},
  title =     {BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions},
  booktitle = {NAACL},
  year =      {2019},
}

@article{wang2019superglue,
  title={Superglue: A stickier benchmark for general-purpose language understanding systems},
  author={Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{zellers2019hellaswag,
  title={Hellaswag: Can a machine really finish your sentence?},
  author={Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  journal={arXiv preprint arXiv:1905.07830},
  year={2019}
}

@inproceedings{nie-etal-2020-adversarial,
    title = "Adversarial {NLI}: A New Benchmark for Natural Language Understanding",
    author = "Nie, Yixin  and
      Williams, Adina  and
      Dinan, Emily  and
      Bansal, Mohit  and
      Weston, Jason  and
      Kiela, Douwe",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    year = "2020",
    publisher = "Association for Computational Linguistics",
}

@inproceedings{MultiRC2018,
    author = {Daniel Khashabi and Snigdha Chaturvedi and Michael Roth and Shyam Upadhyay and Dan Roth},
    title = {Looking Beyond the Surface:A Challenge Set for Reading Comprehension over Multiple Sentences},
    booktitle = {NAACL},
    year = {2018}
}

@inproceedings{OpenBookQA2018,
 title={Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering},
 author={Todor Mihaylov and Peter Clark and Tushar Khot and Ashish Sabharwal},
 booktitle={EMNLP},
 year={2018}
}


@inproceedings{bisk2020piqa,
  title={Piqa: Reasoning about physical commonsense in natural language},
  author={Bisk, Yonatan and Zellers, Rowan and Gao, Jianfeng and Choi, Yejin and others},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  year={2020}
}

@inproceedings{lai2017race,
  title={RACE: Large-scale ReAding Comprehension Dataset From Examinations},
  author={Lai, Guokun and Xie, Qizhe and Liu, Hanxiao and Yang, Yiming and Hovy, Eduard},
  booktitle={Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
  pages={785--794},
  year={2017}
}

@article{zhang2018record,
  title={Record: Bridging the gap between human and machine commonsense reading comprehension},
  author={Zhang, Sheng and Liu, Xiaodong and Liu, Jingjing and Gao, Jianfeng and Duh, Kevin and Van Durme, Benjamin},
  journal={arXiv preprint arXiv:1810.12885},
  year={2018}
}

@article{dagan2010recognizing,
  title={Recognizing textual entailment: Rational, evaluation and approaches--erratum},
  author={Dagan, Ido and Dolan, Bill and Magnini, Bernardo and Roth, Dan},
  journal={Natural Language Engineering},
  volume={16},
  number={1},
  pages={105--105},
  year={2010},
  publisher={Cambridge University Press}
}

@article{mostafazadeh2016corpus,
  title={A corpus and evaluation framework for deeper understanding of commonsense stories},
  author={Mostafazadeh, Nasrin and Chambers, Nathanael and He, Xiaodong and Parikh, Devi and Batra, Dhruv and Vanderwende, Lucy and Kohli, Pushmeet and Allen, James},
  journal={arXiv preprint arXiv:1604.01696},
  year={2016}
}

@article{pilehvar2018wic,
  title={WiC: the word-in-context dataset for evaluating context-sensitive meaning representations},
  author={Pilehvar, Mohammad Taher and Camacho-Collados, Jose},
  journal={arXiv preprint arXiv:1808.09121},
  year={2018}
}

@inproceedings{levesque2012winograd,
  title={The winograd schema challenge},
  author={Levesque, Hector and Davis, Ernest and Morgenstern, Leora},
  booktitle={Thirteenth international conference on the principles of knowledge representation and reasoning},
  year={2012}
}

@article{sakaguchi2021winogrande,
  title={Winogrande: An adversarial winograd schema challenge at scale},
  author={Sakaguchi, Keisuke and Bras, Ronan Le and Bhagavatula, Chandra and Choi, Yejin},
  journal={Communications of the ACM},
  volume={64},
  number={9},
  pages={99--106},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@inproceedings{paperno2016lambada,
  title={The LAMBADA dataset: Word prediction requiring a broad discourse context},
  author={Paperno, Denis and Kruszewski, Germ{\'a}n and Lazaridou, Angeliki and Pham, Ngoc-Quan and Bernardi, Raffaella and Pezzelle, Sandro and Baroni, Marco and Boleda, Gemma and Fern{\'a}ndez, Raquel},
  booktitle={Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={1525--1534},
  year={2016}
}

@article{kwiatkowski2019natural,
  title={Natural questions: a benchmark for question answering research},
  author={Kwiatkowski, Tom and Palomaki, Jennimaria and Redfield, Olivia and Collins, Michael and Parikh, Ankur and Alberti, Chris and Epstein, Danielle and Polosukhin, Illia and Devlin, Jacob and Lee, Kenton and others},
  journal={Transactions of the Association for Computational Linguistics},
  volume={7},
  pages={453--466},
  year={2019},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@inproceedings{rajpurkar2018know,
  title={Know What You Don’t Know: Unanswerable Questions for SQuAD},
  author={Rajpurkar, Pranav and Jia, Robin and Liang, Percy},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  pages={784--789},
  year={2018}
}

@inproceedings{joshi2017triviaqa,
  title={TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension},
  author={Joshi, Mandar and Choi, Eunsol and Weld, Daniel S and Zettlemoyer, Luke},
  booktitle={Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={1601--1611},
  year={2017}
}

@inproceedings{berant2013semantic,
  title={Semantic parsing on freebase from question-answer pairs},
  author={Berant, Jonathan and Chou, Andrew and Frostig, Roy and Liang, Percy},
  booktitle={Proceedings of the 2013 conference on empirical methods in natural language processing},
  pages={1533--1544},
  year={2013}
}

@article{magueresse2020low,
  title={Low-resource languages: A review of past work and future challenges},
  author={Magueresse, Alexandre and Carles, Vincent and Heetderks, Evan},
  journal={arXiv preprint arXiv:2006.07264},
  year={2020}
}

@article{sam2025predicting,
  title={Predicting the Performance of Black-box LLMs through Self-Queries},
  author={Sam, Dylan and Finzi, Marc and Kolter, J Zico},
  journal={arXiv preprint arXiv:2501.01558},
  year={2025}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}
@article{burns2023weak,
  title={Weak-to-strong generalization: Eliciting strong capabilities with weak supervision},
  author={Burns, Collin and Izmailov, Pavel and Kirchner, Jan Hendrik and Baker, Bowen and Gao, Leo and Aschenbrenner, Leopold and Chen, Yining and Ecoffet, Adrien and Joglekar, Manas and Leike, Jan and others},
  journal={arXiv preprint arXiv:2312.09390},
  year={2023}
}

@inproceedings{
pukdee2023label,
title={Label Propagation with Weak Supervision},
author={Rattana Pukdee and Dylan Sam and Pradeep Kumar Ravikumar and Nina Balcan},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=aCuFa-RRqtI}
}
@inproceedings{
wei2021theoretical,
title={Theoretical Analysis of Self-Training with Deep Networks on Unlabeled Data},
author={Colin Wei and Kendrick Shen and Yining Chen and Tengyu Ma},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=rC8sJ4i6kaH}
}

@article{xie2024doremi,
  title={Doremi: Optimizing data mixtures speeds up language model pretraining},
  author={Xie, Sang Michael and Pham, Hieu and Dong, Xuanyi and Du, Nan and Liu, Hanxiao and Lu, Yifeng and Liang, Percy S and Le, Quoc V and Ma, Tengyu and Yu, Adams Wei},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{jiang2024adaptive,
  title={Adaptive data optimization: Dynamic sample selection with scaling laws},
  author={Jiang, Yiding and Zhou, Allan and Feng, Zhili and Malladi, Sadhika and Kolter, J Zico},
  journal={arXiv preprint arXiv:2410.11820},
  year={2024}
}

@article{chowdhery2023palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={Journal of Machine Learning Research},
  volume={24},
  number={240},
  pages={1--113},
  year={2023}
}

@article{
izacard2022unsupervised,
title={Unsupervised Dense Information Retrieval with Contrastive Learning},
author={Gautier Izacard and Mathilde Caron and Lucas Hosseini and Sebastian Riedel and Piotr Bojanowski and Armand Joulin and Edouard Grave},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2022},
url={https://openreview.net/forum?id=jKN1pXi7b0},
note={}
}

@article{liu2019roberta,
  title={RoBERTa: A Robustly Optimized BERT Pretraining Approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv e-prints},
  pages={arXiv--1907},
  year={2019}
}

@inproceedings{cai2021theory,
  title={A theory of label propagation for subpopulation shift},
  author={Cai, Tianle and Gao, Ruiqi and Lee, Jason and Lei, Qi},
  booktitle={International Conference on Machine Learning},
  pages={1170--1182},
  year={2021},
  organization={PMLR}
}

@inproceedings{agirre2016semeval,
  title={Semeval-2016 task 1: Semantic textual similarity, monolingual and cross-lingual evaluation},
  author={Agirre, Eneko and Banea, Carmen and Cer, Daniel and Diab, Mona and Gonzalez Agirre, Aitor and Mihalcea, Rada and Rigau Claramunt, German and Wiebe, Janyce},
  booktitle={SemEval-2016. 10th International Workshop on Semantic Evaluation; 2016 Jun 16-17; San Diego, CA. Stroudsburg (PA): ACL; 2016. p. 497-511.},
  year={2016},
  organization={ACL (Association for Computational Linguistics)}
}

@inproceedings{agirre2013sem,
  title={* SEM 2013 shared task: Semantic textual similarity},
  author={Agirre, Eneko and Cer, Daniel and Diab, Mona and Gonzalez-Agirre, Aitor and Guo, Weiwei},
  booktitle={Second joint conference on lexical and computational semantics (* SEM), volume 1: proceedings of the Main conference and the shared task: semantic textual similarity},
  pages={32--43},
  year={2013}
}