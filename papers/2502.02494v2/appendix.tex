\section{Additional Experimental Results} \label{appx:extra_results}

\subsection{Impact of Dimensionality Reduction on Variance Reduction} \label{appx:pca}

We now present the remaining results for other embedding models, when we ablate the number of components used in PCA for K-Means clustering, specifically when looking at the reduction in variance of pretraining loss of points within the same cluster. We observe that more components in PCA indeed help achieve higher variance reduction across all embedding models.

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.46\textwidth]{figs/bert_pca.pdf}
    \includegraphics[width=0.46\textwidth]{figs/use_pca.pdf}
    \includegraphics[width=0.46\textwidth]{figs/lm_token_pca.pdf}
    \caption{Ablation on the number of components in PCA for embeddings from BERT, USE, and LM Token Embeds. Results are averaged over 50 million sampled clusters from the Pile.}
    \label{fig:clustering_pca_2}
\end{figure*}


\subsection{Comparison of Random Projections to PCA for Dimensionality Reduction} \label{appx:random_proj}

We now present the remaining results for other embedding models, when we use Random Projections for dimensionality reduction instead of using PCA. We consistently observe that embeddings paired with PCA outperform those using Random Projections.

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.46\textwidth]{figs/bert_rp.pdf}
    \includegraphics[width=0.46\textwidth]{figs/use_rp.pdf}
    \includegraphics[width=0.46\textwidth]{figs/lm_token_rp.pdf}
    \caption{Ablation where we compare using Random Projections for dimensionality reduction with embeddings from BERT, USE, and LM Token Embeds. Results are averaged over 50 million sampled clusters from the Pile.}
    \label{fig:random_proj_2}
\end{figure*}


\section{Additional Experimental Details}

\subsection{Additional Evaluation Details}\label{appx:eval_details}

We report results averaged over a large numer of downstream tasks. These datasets largely follow two categories: scoring and decoding tasks. Scoring tasks primarly look at the output distribution of the model, while decoding tasks look at text generations from the language model. Scoring tasks are performed as 1-shot (i.e., giving one demonstration of format and answer), while decoding is performed zeroshot. For scoring tasks, we look at the standard top-1 accuracy. The list of scoring tasks is as follows:

\begin{itemize}
    \item ARC Challenge: Easy and Challenge \citep{allenai:arc} 
    \item BoolQ \citep{clark2019boolq}
    \item SuperGLUE - CB and Copa \citep{wang2019superglue}
    \item HellaSwag \citep{zellers2019hellaswag}
    \item MultiRC \citep{MultiRC2018}
    \item OpenBookQA \citep{OpenBookQA2018}
    \item PIQA \citep{bisk2020piqa}
    \item RACE-H and RACE-M \citep{lai2017race}
    \item ReCoRD \citep{zhang2018record}
    \item RTE \citep{dagan2010recognizing}
    \item Story Cloze \citep{mostafazadeh2016corpus}
    \item WIC \citep{pilehvar2018wic}
    \item Winograd \citep{levesque2012winograd}
    \item WinoGrande \citep{sakaguchi2021winogrande}
    \item WSC \citep{levesque2012winograd}
\end{itemize}

For decoding or text generation tasks, we evaluate the language model outputs with its F1 score. Decoder tasks are also evaluated as 1-shot. The list of decoding tasks is as follows:
\begin{itemize}
    \item Lambada \citep{paperno2016lambada}
    \item Natural Questions \citep{kwiatkowski2019natural}
    \item Squad v2 \citep{rajpurkar2018know}
    \item Trivia QA Wiki \citep{joshi2017triviaqa}
    \item Web Questions \citep{berant2013semantic}
\end{itemize}

\subsection{Additional Embedding Model Details}

The embedding models of BERT, Gecko and USE are trained with sequence lengths of 512, which we apply on the first 512 tokens of data from the Pile. 
For Gecko, we use the 110 million parameter model version, while for USE we use the 109 million parameter version. 
For BERT, we also use a 109 million parameter model. 
For the small language model that we train with the UL2 objective, we use one with approximately 200 million parameters. 

Both the off-the-shelf BERT and USE embedding models have a dimensionality of 512. The Gecko embedding model has a dimensionality of 768.
The small language model has a token embedding dimension of 512 and an hidden activation dimension of 512, which means that both LM Token Embeds and LM Output Embeds have 512 dimensions.

\subsection{Additional Hyperparameter Details}\label{appx:hyperparam_details}

\paragraph{Clustering}

For performing RAC clustering for our pretraining experiments, we use a value of $\epsilon$ as the particular diameter of clusters:
\begin{itemize}
    \item USE: $\epsilon = 0.2$, which defines roughly 225 million clusters
    \item Gecko: $\epsilon = 0.2$, which defines roughly 220 million clusters
    \item BERT: $\epsilon = 0.001$, which defines roughly 175 million clusters
    \item LM Token Embeds: $\epsilon = 0.001$, which defines roughly 170 million clusters
    \item LM Output Embeds: $\epsilon = 0.03$, which defines roughly 180 million clusters
\end{itemize}

For our K-Means clustering, we perform clustering at 4 different levels of granularity in our variance reduction and cluster purity results. We create four sets of clusterings with an average cluster size of 25, 50, 100, 150, with a minimum cluster size of $\frac{1}{5}$ times the average cluster size and a maximum cluster size of $5$ times the average cluster size. 
For both RAC and K-Means, we use the squared L2 (Euclidean) distance.

\subsection{Dimensionality Reduction Details}

For running our dimensionality reduction via PCA, we compute the means and components on which to project on a subset of the data ($\sim$500,000 points). 
We first standardize the embeddings to have a mean of 0 and variance of 1 before running PCA. As previously mentioned, after projecting onto the desired number of principal components, we perform L2 normalization.

For our random projections, we use a sparse random projections onto values of $-\frac{\sqrt{n}}{\sqrt{64}}, 0, \frac{\sqrt{n}}{\sqrt{64}}$ with probabilities $\frac{1}{4}, \frac{1}{2}, \frac{1}{4}$ respectively (i.e., the default parameters via scikit-learn). We also L2 normalize the result of random projections.

\subsection{Compute Details}

Pretraining experiments for our 1.7B parameter language models are run on 512 v5 TPUs, where each pretraining run takes approximately 3 days. 

