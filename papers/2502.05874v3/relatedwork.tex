\section{Related Work}
\subsubsection{Scene Graph.}
Scene graph provides a structured and hierarchical representation of complex scenes by using nodes (objects) and edges (relationships) \cite{zhou2019scenegraphnet}. 
Following their introduction, subsequent works have refined hierarchical scene graph \cite{rosinol20203d} and focused on predicting local inter-object relationships \cite{koch2024lang3dsg,liao2024gentkg}.
Such advancements have driven the widespread application of scene graphs across both 2D and 3D domains, enabling sophisticated tasks such as image synthesis \cite{johnson2018image, wu2023scene} and caption generation \cite{basioti2024cic} in 2D, as well as video synthesis \cite{cong2023ssgvs}, 3D scene understanding \cite{wald2020learning} and scene synthesis \cite{para2021generative} in 3D. 
However, in the current 3D indoor scene synthesis tasks, scene graphs are predominantly derived from text input by users \cite{strader2024indoor}. We propose the MMG, which more effectively accommodates flexible user inputs.

\subsubsection{3D Scene Generation.}
3D scene generation is an area of ongoing research that focuses on developing plausible layouts \cite{engelmann2021points} and generating accurate object shapes \cite{xu2023discoscene}.
A substantial body of contemporary research synthesizes scenes from text \cite{fang2023ctrl}, panorama \cite{wang2023perf, hara2024magritte}, or spatial layout \cite{yan2024frankenstein, jyothi2019layoutvae} using autoregressive paradigms \cite{wang2021sceneformer}, object decoupling techniques \cite{zhang2024style, epstein2024disentangled}, or prior learning \cite{hollein2023text2room}.
In particular, CommonScenes \cite{zhai2024commonscenes} utilizes scene graphs as conditions and adopts an end-to-end framework to generate both object shapes and scene layouts simultaneously. 
EchoScene \cite{zhai2024echoscene} advances the CommonScenes by incorporating an information echo scheme.
Nevertheless, these approaches are inadequate for effectively controlling object geometry. To overcome this limitation, we introduce MMGDreamer, which fully leverages the MMG to achieve precise control over object geometry.

% \subsubsection{Diffusion Models.}
% A diffusion model \citet{sohl2015deep} comprises a forward process that progressively adds noise and a reverse process that gradually denoises a normally distributed variable. 
% In the realms of text-to-image \cite{ rombach2022high} and text-to-video \cite{blattmann2023align}, diffusion-based methods have achieved remarkable generation results and have become a leading research direction.
% Following the success of diffusion models in the image and video domains, research in 3D scene generation \cite{tang2023diffuscene} has increasingly drawn inspiration from text-to-image models.
% GraphDreamer \cite{gao2024graphdreamer} leverages score distillation sampling \cite{poole2022dreamfusion} to extract prior knowledge from a pre-trained text-to-image diffusion model, generating compositional 3D scenes.
% Frankenstein \cite{yan2024frankenstein} utilizes a diffusion-based approach capable of producing semantic-compositional 3D scenes in one tri-plane. 
% MMGDreamer integrates a dual-branch diffusion model for indoor scene generation.