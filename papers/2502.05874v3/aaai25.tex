%File: formatting-instructions-latex-2025.tex
%release 2025.0
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage{aaai25}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{verbatim}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{makecell}

\usepackage{tcolorbox}

\DeclareUnicodeCharacter{0301}{\'{e}}

\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in}  % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in}  % DO NOT CHANGE THIS

\usepackage{colortbl}
\definecolor{yan}{rgb}{0.5,0.3,0.5}
\newcommand{\yan}[1]{\textcolor{yan}{\emph{Yan Di:~{#1}}}}
\newcommand{\evin}[1]{\textcolor{red}{\emph{Evin:~{#1}}}}
\newcommand{\dave}[1]{\textcolor{blue}{\emph{Dave:~{#1}}}}
\definecolor{ben}{rgb}{0.9,0.,0.5}
\newcommand{\ben}[1]{\textcolor{ben}{\emph{Ben:~{#1}}}}

\definecolor{navyblue}{RGB}{191, 209, 229} 
\definecolor{light_yellow}{RGB}{255,243,194}
\definecolor{orange}{RGB}{255,200,100}
\definecolor{red}{RGB}{255, 0, 0}
\definecolor{green}{RGB}{0, 176, 80}
\newcommand{\best}{\cellcolor{orange}}
\newcommand{\second}{\cellcolor{light_yellow}}

%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
\usepackage{algorithm}
\usepackage{algorithmic}

%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.
\pdfinfo{
/TemplateVersion (2025.1)
}

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai25.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Title

% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
\title{MMGDreamer: Mixed-Modality Graph for Geometry-Controllable \\ 3D Indoor Scene Generation}
\author{
    %Authors
    % All authors must be in the same font size and format.
    Zhifei Yang\textsuperscript{\rm 1}\thanks{Work done at Beijing Digital Native Digital City Research Center.}, 
    Keyang Lu\textsuperscript{\rm 2},
    Chao Zhang\textsuperscript{\rm 3}\footnote{Corresponding author.},
    Jiaxing Qi\textsuperscript{\rm 2},
    Hanqi Jiang\textsuperscript{\rm 3},
    Ruifei Ma\textsuperscript{\rm 3},
    Shenglin Yin\textsuperscript{\rm 1},
    Yifan Xu\textsuperscript{\rm 2},
    Mingzhe Xing\textsuperscript{\rm 1}, 
    Zhen Xiao\textsuperscript{\rm 1}\footnotemark[2],
    Jieyi Long\textsuperscript{\rm 4},
    Guangyao Zhai\textsuperscript{\rm 5}
}
\affiliations{
    %Afiliations
    \textsuperscript{\rm 1} School of Computer Science, Peking University \\
    \textsuperscript{\rm 2} Beihang University\\
    \textsuperscript{\rm 3} Beijing Digital Native Digital City Research Center \\
    \textsuperscript{\rm 4} Theta Labs, Inc. \\
    \textsuperscript{\rm 5} Technical University of Munich\\[0.5em]
    % \{yangzhifei, yinsl, mzxing\}@stu.pku.edu.cn, \{lukeyang, jiaxingqi, yifan\_xu\}@buaa.edu.cn, \{ariczhang2009, jhqyyds, liuxiangde\}@gmail.com, m202220831@xs.ustb.edu.cn, xiaozhen@pku.edu.cn, jieyi@thetalabs.org, guangyao.zhai@tum.de
    yangzhifei@stu.pku.edu.cn, lukeyang@buaa.edu.cn, ariczhang2009@gmail.com, \\ 
    xiaozhen@pku.edu.cn, jieyi@thetalabs.org, guangyao.zhai@tum.de
    % If you have multiple authors and multiple affiliations
    % use superscripts in text and roman font to identify them.
    % For example,

    % Sunil Issar\textsuperscript{\rm 2}, 
    % J. Scott Penberthy\textsuperscript{\rm 3}, 
    % George Ferguson\textsuperscript{\rm 4},
    % Hans Guesgen\textsuperscript{\rm 5}
    % Note that the comma should be placed after the superscript

%
% See more examples next
}

%Example, Single Author, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\iffalse
\title{My Publication Title --- Single Author}
\author {
    Author Name
}
\affiliations{
    Affiliation\\
    Affiliation Line 2\\
    name@example.com
}
\fi

\iffalse
%Example, Multiple Authors, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\title{My Publication Title --- Multiple Authors}
\author {
    % Authors
    First Author Name\textsuperscript{\rm 1,\rm 2},
    Second Author Name\textsuperscript{\rm 2},
    Third Author Name\textsuperscript{\rm 1}
}
\affiliations {
    % Affiliations
    \textsuperscript{\rm 1}Affiliation 1\\
    \textsuperscript{\rm 2}Affiliation 2\\
    firstAuthor@affiliation1.com, secondAuthor@affilation2.com, thirdAuthor@affiliation1.com
}
\fi


% REMOVE THIS: bibentry
% This is only needed to show inline citations in the guidelines document. You should not need it and can safely delete it.
\usepackage{bibentry}
% END REMOVE bibentry

\begin{document}

\maketitle

\begin{abstract}
Controllable 3D scene generation has extensive applications in virtual reality and interior design, where the generated scenes should exhibit high levels of realism and controllability in terms of geometry. Scene graphs provide a suitable data representation that facilitates these applications.
However, current graph-based methods for scene generation are constrained to text-based inputs and exhibit insufficient adaptability to flexible user inputs, hindering the ability to precisely control object geometry.
To address this issue, we propose \textbf{MMGDreamer}, a dual-branch diffusion model for scene generation that incorporates a novel \textbf{Mixed-Modality Graph}, visual enhancement module, and relation predictor. 
The mixed-modality graph allows object nodes to integrate textual and visual modalities, with optional relationships between nodes. It enhances adaptability to flexible user inputs and enables meticulous control over the geometry of objects in the generated scenes.
The visual enhancement module enriches the visual fidelity of text-only nodes by constructing visual representations using text embeddings.
Furthermore, our relation predictor leverages node representations to infer absent relationships between nodes, resulting in more coherent scene layouts.
Extensive experimental results demonstrate that MMGDreamer exhibits superior control of object geometry, achieving state-of-the-art scene generation performance.
% Project page: https://yangzhifeio.github.io/project/MMGDreamer.

\end{abstract}

\begin{links}
    \link{Project page}{https://yangzhifeio.github.io/project/MMGDreamer}
\end{links}
% Uncomment the following to link to your code, datasets, an extended version or similar.
%
% \begin{links}
%     \link{Code}{https://aaai.org/example/code}
%     \link{Datasets}{https://aaai.org/example/datasets}
%     \link{Extended version}{https://aaai.org/example/extended-version}
% \end{links}

\begin{figure*}[t]
\centering
\includegraphics[width=1\textwidth]{Figure/introduction_MMG.pdf} % Reduce the figure size so that it is slightly narrower than the column.
\caption{\textbf{MMGDreamer} processes a Mixed-Modality Graph to generate a 3D indoor scene, where object geometry can be precisely controlled. Starting from the fifth type of input (Mixed-Modality) shown in module A as an example, the framework utilizes a vision-language model (B) to produce a Mixed-Modality Graph (C). This graph is further refined by the Generation Module (D) to create a coherent and precise 3D scene (E).}
\label{introduction}
\end{figure*}

\section{Introduction}
Deep generative models have initiated a new era of artificial intelligence-generated content, driving developments in natural language generation \cite{zheng2024judging}, video synthesis \cite{liao2024videoinsta}, and 3D generation \cite{poole2022dreamfusion}. 
Controllable Scene Generation refers to generating realistic 3D scenes based on input prompts, allowing for precise control and adjustment of specific objects within those scenes. 
It is widely applied in Virtual Reality \cite{bautista2022gaudi}, Interior Design \cite{ ccelen2024design}, and Embodied Intelligence\cite{yang2024physcene, zhai2024sg}, providing immersive experiences and enhancing decision-making processes.
Within these applications, scene graphs serve as a powerful tool by succinctly abstracting the scene context and interrelations between objects, enabling intuitive scene manipulation and generation \cite{dhamo2021graph}.

Despite the significant progress made by retrieval-based \cite{lin2024instructscene}, semi-generative \cite{ren2024xcube}, and fully-generative \cite{zhai2024commonscenes} methods in graph-based controllable scene generation, these approaches predominantly rely on textual descriptions to construct input scene graphs. While text serves as a high-level representation encapsulating rich semantic information, it falls short in accurately describing the geometry of objects in the generated scenes, resulting in inadequate geometric control over the generated objects \cite{rombach2022high}. 
Moreover, each node in the scene graph contains only textual information about object category, which limits its adaptability to flexible user input.
To address these limitations, we introduce MMGDreamer, a dual-branch diffusion model designed for processing multimodal information, incorporating a novel Mixed-Modality Graph (MMG) as a key component. 
As depicted in Fig.~\ref{introduction}, the node of MMG can be represented in three ways: text, image, or a combination of both. 
Additionally, edges between nodes can be selectively provided or omitted based on user input.
This flexible graph structure supports five types of user input, as illustrated in Fig.~\ref{introduction}.A, significantly enhancing adaptability to diverse user demands and enabling precise control over object geometry in generated scenes.

To fully leverage the capabilities of MMG, MMGDreamer features two pivotal modules: the visual enhancement module and the relation predictor. 
When nodes of the input scene graph contain solely textual information, the visual enhancement module employs text embeddings to construct visual representations of these nodes. 
By incorporating visual priors associated with the text, this approach enriches the visual content of nodes, enhancing geometric control over the generated objects. 
The relation predictor, a relationship classifier based on the GCN, leverages prior knowledge and node representations within the scene to infer relationships between nodes in the absence of explicit relational information. By capturing global and local scene-object relationships, this module ensures the generation of more coherent and contextually appropriate scene layouts. 
% Despite the fact that MMG is generally easier to obtain than 3D spatial layouts \cite{schult2024controlroom3d}, we have also devised a text prompt to query the vision language model such as GPT-4V \cite{achiam2023gpt}, enabling the parsing of MMG from unstructured text and image inputs, as illustrated in Fig.~\ref{introduction}. 
% We provide the prompt in the supplementary material.
 We briefly summarize our primary contributions as follows:
\begin{itemize}
    \item We introduce a novel \textbf{Mixed-Modality Graph}, where nodes can selectively incorporate textual and visual modalities, allowing for precise control over the object geometry of the generated scenes and more effectively accommodating flexible user inputs. 
    \item We present \textbf{MMGDreamer}, a dual-branch diffusion model for scene generation based on Mixed-Modality Graph, which incorporates two key modules: a visual enhancement module and a relation predictor, dedicated to construct node visual features and predict relations between nodes, respectively.
    \item Extensive experiments on the SG-FRONT dataset demonstrate that MMGDreamer attains higher fidelity and geometric controllability, and achieves state-of-the-art performance in scene synthesis, outperforming existing methods by a large margin.
\end{itemize}


\section{Related Work}
\subsubsection{Scene Graph.}
Scene graph provides a structured and hierarchical representation of complex scenes by using nodes (objects) and edges (relationships) \cite{zhou2019scenegraphnet}. 
Following their introduction, subsequent works have refined hierarchical scene graph \cite{rosinol20203d} and focused on predicting local inter-object relationships \cite{koch2024lang3dsg,liao2024gentkg}.
Such advancements have driven the widespread application of scene graphs across both 2D and 3D domains, enabling sophisticated tasks such as image synthesis \cite{johnson2018image, wu2023scene} and caption generation \cite{basioti2024cic} in 2D, as well as video synthesis \cite{cong2023ssgvs}, 3D scene understanding \cite{wald2020learning} and scene synthesis \cite{para2021generative} in 3D. 
However, in the current 3D indoor scene synthesis tasks, scene graphs are predominantly derived from text input by users \cite{strader2024indoor}. We propose the MMG, which more effectively accommodates flexible user inputs.

\subsubsection{3D Scene Generation.}
3D scene generation is an area of ongoing research that focuses on developing plausible layouts \cite{engelmann2021points} and generating accurate object shapes \cite{xu2023discoscene}.
A substantial body of contemporary research synthesizes scenes from text \cite{fang2023ctrl}, panorama \cite{wang2023perf, hara2024magritte}, or spatial layout \cite{yan2024frankenstein, jyothi2019layoutvae} using autoregressive paradigms \cite{wang2021sceneformer}, object decoupling techniques \cite{zhang2024style, epstein2024disentangled}, or prior learning \cite{hollein2023text2room}.
In particular, CommonScenes \cite{zhai2024commonscenes} utilizes scene graphs as conditions and adopts an end-to-end framework to generate both object shapes and scene layouts simultaneously. 
EchoScene \cite{zhai2024echoscene} advances the CommonScenes by incorporating an information echo scheme.
Nevertheless, these approaches are inadequate for effectively controlling object geometry. To overcome this limitation, we introduce MMGDreamer, which fully leverages the MMG to achieve precise control over object geometry.

% \subsubsection{Diffusion Models.}
% A diffusion model \citet{sohl2015deep} comprises a forward process that progressively adds noise and a reverse process that gradually denoises a normally distributed variable. 
% In the realms of text-to-image \cite{ rombach2022high} and text-to-video \cite{blattmann2023align}, diffusion-based methods have achieved remarkable generation results and have become a leading research direction.
% Following the success of diffusion models in the image and video domains, research in 3D scene generation \cite{tang2023diffuscene} has increasingly drawn inspiration from text-to-image models.
% GraphDreamer \cite{gao2024graphdreamer} leverages score distillation sampling \cite{poole2022dreamfusion} to extract prior knowledge from a pre-trained text-to-image diffusion model, generating compositional 3D scenes.
% Frankenstein \cite{yan2024frankenstein} utilizes a diffusion-based approach capable of producing semantic-compositional 3D scenes in one tri-plane. 
% MMGDreamer integrates a dual-branch diffusion model for indoor scene generation.


\section{Preliminary}
 
\subsection{Scene Graph Representation}
Scene graph can be formally defined as a directed graph $ \mathcal{G} = \{\mathcal{V}, \mathcal{R}\} $, where $ \mathcal{V} = \{v_i \ | \  i \in \{1, \ldots, N \} \} $ represents the set of nodes (objects in the scene) and $ \mathcal{R} = \{r_{i \to j} \ | \ i, j \in \{1, \ldots, N\}, i \neq j\} $ represents the set of edges (relationships between objects).
The node $v_i$ represents an object in the scene and contains information about the object's category. The edges $ r_{i \to j} $ define the connections between objects, which can include spatial relationships (e.g., front/behind) or attribute relationships (e.g., same style).

\subsection{Scene Graph Encoder}
Graph Convolutional Network (GCN) \cite{johnson2018image} facilitates the processing of graph-structured data by learning node representations through the layer-by-layer aggregation of features from neighboring nodes.
In our work, we utilize the Triplet Graph Convolutional Network (Triplet-GCN) as scene graph encoder to process the scene graph, assuming that the initial node and edge attribute features are given by $(\delta^{(0)}_{v_i}, \delta^{(0)}_{r_{i \to j}}, \delta^{(0)}_{v_j})$.
Specifically, each layer of GCN updates the node and edge representations according to the following formula:
\begin{equation}
(\gamma^{l}_{v_i}, \delta^{l+1}_{r_{i \to j}}, \gamma^{l}_{v_j}) = \text{MLP}_1(\delta^{l}_{v_i}, \delta^{l}_{r_{i \to j}}, \delta^{l}_{v_j}), 
\end{equation}
\begin{equation}
\delta^{l+1}_{v_i} = \gamma^{l}_{v_i} + \text{MLP}_2\left(\text{Avg}(\gamma^{l}_{v_j} \mid v_j \in \mathcal{N}_G(v_i))\right),
\end{equation}
where $l \in \{1, \ldots, L-1\}$ denotes an independent layer within the Triplet-GCN, $\mathcal{N}_G(v_i)$ represents the set of all neighboring nodes of $v_i$, Avg indicates the use of average pooling operation, and $\text{MLP}_1$ and $\text{MLP}_2$ refer to the Multi-Layer Perceptron (MLP) layers. 
\begin{figure*}[t]
\centering
\includegraphics[width=1\textwidth]{Figure/pipeline.pdf} 
\caption{\textbf{Overview of MMGDreamer.} Our pipeline consists of the Latent Mixed-Modality Graph, the Graph Enhancement Module, and the Dual-Branch Diffusion Model. 
During inference, MMGDreamer initiates with the Latent Mixed-Modality Graph, which undergoes enhancement via the Visual Enhancement Module and the Relation Predictor, resulting in the formation of a Visual-Enhanced Graph and a Mixed-Enhanced Graph. 
The Mixed-Enhanced Graph is then input into the Graph Encoder $E_g$ within the Dual-Branch Diffusion Model for relationship modeling, using a triplet-GCN structured module integrated with an echo mechanism.
Subsequently, the Layout Branch (C.2) and the Shape Branch (C.3) use denoisers conditioned on the nodes' latent representations to generate layouts and shapes, respectively.
The final output is a synthesized 3D indoor scene where the generated shapes are seamlessly integrated into the generated layouts.}
\label{pipeline}
\end{figure*}

\subsection{Latent Diffusion Model}
Latent Diffusion Model (LDM) \cite{rombach2022high} generally involves two Markov processes: a forward process that incrementally corrupts the data and a reverse process that progressively denoises it. 
Given a sample $x_0$, the LDM first employs a pre-trained VQ-VAE \cite{van2017neural} to encode $ x_0 $ into a reduced-dimensional latent representation $ z_0$.
The forward process is defined by:
\begin{equation}
q(z_t \mid z_{t-1}) = \mathcal{N}(z_t; \sqrt{1 - \beta_t} z_{t-1}, \beta_t \mathbf{I}),
\end{equation}
where $ t $ ranges from $ 1 $ to $ T $, with $ T $ denoting the total number of timesteps. The parameter $ \beta_t $ controls the noise level introduced at each timestep $ t $. 
% 反向过程
In the reverse process, $ z_t $ is processed through a denoiser $ \epsilon_{\theta} $, such as UNet \cite{ronneberger2015u}, to estimate the noise, enabling a progressive denoising process to recover a clean latent representation. The objective function can be written as:
\begin{equation}
    \mathcal{L}_{LDM} = \mathbb{E}_{z_t, t, \epsilon \sim \mathcal{N}(0, 1)} \left[ \| \epsilon - \epsilon_{\theta}(z_t, t, c) \|^2_2 \right],
\end{equation}
where $c$ is a condition to guide the reverse process.


\section{Method}
We propose MMGDreamer, a framework adept at handling MMG as input for indoor scene synthesis tasks, as illustrated in Fig.~\ref{pipeline}. 
The MMG is a novel graph structure where nodes can optionally carry textual or visual information, thereby more effectively accommodating flexible user inputs.
MMGDreamer first utilizes CLIP and an embedding layer to encode the MMG, producing the Latent Mixed-Modality Graph (LMMG). We then apply the visual enhancement module to construct visual information in the nodes of the LMMG, yielding a Visual-Enhanced Graph. Subsequently, a Relation Predictor is utilized to predict the missing edges between nodes, forming the Mix-Enhanced Graph. Finally, we model the relations within the scene using the Graph Encoder and employ a dual-branch diffusion model to generate the corresponding layout and shape, synthesizing the 3D indoor scene.

\subsection{Mixed-Modality Graph}
% 生成fine-grained的场景只使用文本信息是不足够的，它不能精确的控制生成物体的外形。同时，用户灵活的输入应该是多模态的，应该可以根据需求选择性的输入文本和图像，如Fig. \ref{}.A所示。但是现有方法均不支持这种输入形式，然而graph作为一种紧凑的结构表示，其中的node能很好的支持多种attribute。此外，用户的语言描述通常不具备所有物体之间的关系，而echoscene和commoscenes虽然使用graph去生成场景，但是需要很强的constraints，并不user friendly，一种模仿语言的图结构应该具有稀疏边relation。为此，我们设计了Mixed-Modality Graph，一种节点能包含文本和图像模态，边可选择的全新图。
Generating fine-grained scenes using only text information is insufficient, as it cannot precisely control the geometry of generated objects. At the same time, users' flexible input should be multimodal, allowing for the selective input of text or images based on specific needs, as shown in Fig.~\ref{introduction}.A. However, existing methods \cite{ hu2024scenecraft} do not support this input format. 
Graphs, as a compact and flexible structural representation, enable the effective encoding of diverse attributes within nodes, facilitating the seamless integration of multimodal information. Furthermore, users' textual descriptions often lack information about the relationships between all objects. While methods such as EchoScene\cite{zhai2024echoscene} and CommoScenes \cite{zhai2024commonscenes} utilize graphs to generate scenes, they impose strict relation constraints, making them less user-friendly. A graph structure that mimics natural language should feature sparse edge relations. 
To address these issues, we propose the Mixed-Modality Graph, a novel graph where nodes can contain both textual and visual modalities, and edges are selectable.
% In order to better accommodate users' flexible input, we designed the Mixed-Modality Graph, a novel graph where nodes can contain both textual and visual modalities, and edges are selectable.
% Generating fine-grained scenes using only text information is insufficient, as it cannot precisely control the shape of generated objects. At the same time, users' flexible input should be multimodal, incorporating both text and images, as shown in Fig. \ref{introduction}.A. However, existing methods do not support such input formats. Graphs, as a compact structural representation, allow nodes to effectively support various attributes. Furthermore, users' language descriptions often lack information about the relationships between all objects. While methods like EchoScene and CommoScenes use graphs to generate scenes, they require strong constraints, which are not very user-friendly. A graph structure that mimics language should have sparse edge relations. To address this, we designed the Mixed-Modality Graph, a novel graph where nodes can contain both text and image modalities, and edges are selectable.

A Mixed-Modality Graph $\mathcal{G}^m = \{\mathcal{V}^m, \mathcal{R}^m\}$ contains nodes and their relations:
\begin{equation}
\mathcal{V}^m = \{v_i^m \ | \ i \in {1, \ldots, N}\},
\end{equation}
\begin{equation}
\mathcal{R}^m = \{r_{i \to j}^m \ | \ i, j \in \{1, \ldots, N\}, i \neq j\}.
\end{equation}
Each node $v_i^m = \{[o_i^m, i_i^m] \ | \ [o_i^m] \ | \ [i_i^m]\}$ represents an object with text category information $[o_i^m]$, image information $[i_i^m]$ or both text category and image information $[o_i^m, i_i^m]$, as shown in Fig.~\ref{introduction}.C.
Despite the fact that MMG is generally easier to obtain than 3D spatial layouts \cite{schult2024controlroom3d}, we have also devised a text prompt to query the vision language model such as GPT-4V \cite{achiam2023gpt}, enabling the parsing of MMG from unstructured text and image inputs, as illustrated in Fig.~\ref{introduction}.B. 

Assuming $v_i^m = [o_i^m, i_i^m] $, we utilize embedding layers to encode the category information $o_i^m$ and the relational information of edge $r_{i \to j}^m$, transforming them into $c_i^m$ and $e_{i \to j}^m$, respectively.
To enrich high-level semantic features while simultaneously encoding image information, we leverage the pre-trained and frozen visual language model CLIP \cite{radford2021learning}, using its text encoder to transform $o_i^m$ into $t_i^m$ and its image encoder to convert $i_i^m$ into $u_i^m$.
To ensure consistent processing within MMGDreamer, we apply zero-padding at the feature level for missing node modality information or edge relationships between two nodes.
As illustrated in Fig.~\ref{pipeline}.A, a LMMG $\mathcal{G}_l^m$ can be uniformly represented as:
\begin{equation}
\mathcal{F}_{\mathcal{V}^m} = \left\{ f_i^{v} = [c_i^m, t_i^m, u_i^m] \mid i \in \{1, \ldots, N \} \right\},
\end{equation}
\begin{equation}
\mathcal{F}_{\mathcal{R}^m} = \left\{ f_{i \to j}^{e} = [e_{i \to j}^m] \mid i, j \in \{1, \ldots, N \} , i \neq j\ \right\},
\end{equation}
where $\mathcal{F}_{\mathcal{V}^m}$ represents the set of node features, and $\mathcal{F}_{\mathcal{R}^m}$ represents the set of edge features.

\subsection{Visual Enhancement Module}
Incorporating visual features within graph nodes enhances the generation of object geometry. However, in the LMMG, some nodes only contain textual information. We introduce a visual enhancement module to bolster the ability to generate object shapes. 
This module employs an architecture similar to VQ-VAE, comprising an encoder $E$, a decoder $D$, and a codebook $\mathcal{C}$, to effectively construct visual features from the textual features of nodes within the LMMG. 
The encoder $ E $ processes the textual features $ t_i^m $ into latent vectors $h_i^m=E(t_i^m) $. 
These latent vectors are then quantized using the codebook $\mathcal{C}$, which contains a set of embedding vectors $\{e_k\}_{k=1}^{K}$. The quantization process selects the $n$ nearest embedding vectors from the codebook:
\begin{equation}
\hat{h}_i^m = \{ e_{k_j} \mid k_j \in \arg\min_{e_{k_l} \in \mathcal{C}} \sum_{l=1}^n \| h_i^m - e_{k_l} \|^2 \}.
\end{equation}
The quantized latent vectors $\hat{h}_i^m$ are subsequently processed by the decoder $D$ to generate visual features $\hat{u}_i^m = D(\hat{h}_i^m)$. 
The training objective for the visual enhancement module is to maximize the evidence lower bound (ELBO) for the likelihood of the data:
\begin{equation}
\mathcal{L}_r = \mathbb{E}_{h \sim q_E(h|t)} \left[ \log p_D(u | h) - \beta D_{\text{KL}}(q_E(h|t) \| p(h)) \right],
\end{equation}
where $q_E(h|t)$ denotes the latent vector distribution given the textual features, $p_D(u|h)$ is the likelihood of the visual features given the latent vectors, and $D_{\text{KL}}$ denotes the Kullback-Leibler divergence. 
The prior \( p(h) \) is typically a Gaussian distribution, and \( \beta \) is a weighting factor.
To address the non-differentiable nature of the quantization process, the Gumbel-Softmax relaxation \cite{jang2016categorical} technique is applied to optimize the ELBO. 
Utilizing this VQ-VAE-based framework, the visual enhancement module produces a Visual-Enhanced Graph $\mathcal{G}_I^m$, enhancing the capability of the LMMG to generate accurate and detailed object geometry for scene generation tasks.

\subsection{Relation Predictor}
Relations are crucial in indoor scene generation, as they impact layout configuration. To address the challenge of missing relationships among nodes in the LMMG, we develop a Relation Predictor that infers these connections, enabling the generation of more reasonable layouts.
The Relation Predictor takes triples of latent representations \((f_i^v, f_{i \to j}^e, f_j^v)\) as input.
In cases where relationships are missing, \(f_{i \to j}^e\) is filled with zeros to ensure consistency in the feature space.
The module comprises a GCN layer followed by a series of MLP layers. The GCN layer processes the input triples to capture the relational context between nodes, while the MLP layers further refine the edge predictions.
The Relation Predictor is trained using a cross-entropy loss, defined as: 
\begin{equation}
    \mathcal{L}_e = -\frac{1}{N}\sum_{i=1}^{N}\sum_{c=1}^{C} y_{ic}\log(\hat{y}_{ic})
\end{equation}
where \(N\) is the number of node pairs, \(C\) is the number of edge classes, \(y_{ic}\) is the one-hot encoded true label, and \(\hat{y}_{ic}\) is the predicted probability.
The Relation Predictor refines the graph $\mathcal{G}_I^m$ into a Mixed-Enhanced Graph $\mathcal{G}_E^m$, by predicting and integrating missing node relationships to improve overall layout coherence.

\begin{table*}[t!]
% \textbf   
\centering
\setlength{\tabcolsep}{1mm}
    \begin{tabular}{l  c  c c c | c c c | c c c }
     \toprule 
        \multirow{2}{*}{Method} & Shape & \multicolumn{3}{c}{Bedroom} & \multicolumn{3}{c}{Living room} & \multicolumn{3}{c}{Dining room}
        \\ & Representation & FID & $\text{FID}_{\text{CLIP}}$ & KID & FID & $\text{FID}_{\text{CLIP}}$ & KID & FID & $\text{FID}_{\text{CLIP}}$ & KID \\
    \midrule 
    CommonScenes~\cite{zhai2024commonscenes} & rel2shape & 57.68 & \phantom{0}4.86 & \phantom{0}6.59 & 80.99 & \phantom{0}7.05 & \phantom{0}6.39 & 65.71 & \phantom{0}7.04 & \phantom{0}5.47\\
    EchoScene \cite{zhai2024echoscene} & echo2shape &  {48.85} & \phantom{0}{4.26} & \phantom{0}{1.77} &  {75.95} & \phantom{0}{6.73} & \phantom{0}{0.60} & {62.85} & \phantom{0}{6.28} & \phantom{0}{1.72}\\
    \midrule 
    \textbf{MMGDreamer (MM+R)} & echo2shape &  \textbf{45.75}  & \phantom{0}\textbf{3.84} & \phantom{0}\textbf{1.72} &  \textbf{68.94}  & \phantom{0}\textbf{6.19} & \phantom{0}\textbf{0.40} &  \textbf{55.17}  & \phantom{0}\textbf{5.86} & \phantom{0}\textbf{0.05}\\
    \bottomrule
    \end{tabular}
\caption{\textbf{Scene generation realism} is quantified by comparing generated top-down renderings with real scene renderings at a resolution of $256^{2}$ pixels, using FID, $\text{FID}_{\text{CLIP}}$, and KID $(\times 0.001)$, following the methodology in~\cite{zhai2024commonscenes} (lower is better). 
\textbf{MM} denotes nodes using mixed-modality representations. \textbf{R} represents the relationships of nodes. 
% The best results are highlighted in \textbf{bold}. 
}
\label{tab:fidkid}
% EchoLayout and CommonLayout both refer to the single layout branch.
\end{table*}

\begin{table*}[t]
    \centering
    \setlength{\tabcolsep}{1mm}
    \begin{tabular}{l c|cccccccccc}
    \toprule
    Method & Metric & Bed & N.stand & Ward. & Chair & Table & Cabinet & Lamp & Shelf & Sofa & TV stand\\
    \midrule 
        Graph-to-3D~\cite{dhamo2021graph} & \multirow{4}{*}{MMD} & 1.56 & 3.91 & 1.66 & 2.68 & 5.77 & 3.67 & 6.53 & 6.66 & 1.30 & 1.08 \\
        CommonScenes~\cite{zhai2024commonscenes}& & {0.49} & {0.92} & {0.54} & 0.99 & {1.91} & {0.96} & {1.50} & {2.73} & {0.57} & {0.29} \\
        EchoScene~\cite{zhai2024echoscene} & & 0.37 & 0.75  & 0.39 & 0.62 & 1.47 & 0.83 & 0.66 & 2.52 & 0.48 & 0.35 \\
        \textbf{MMGDreamer (I+R)} & & \textbf{0.22} & \textbf{0.41} & \textbf{0.24}  & \textbf{0.35} & \textbf{0.55} & \textbf{0.71} & \textbf{0.34} & \textbf{1.58} & \textbf{0.43} & \textbf{0.24}  \\
    \midrule 
        Graph-to-3D~\cite{dhamo2021graph} & \multirow{4}{*}{COV} & 4.32 & 1.42 & 5.04 & 6.90 & 6.03 & 3.45 & 2.59 & 13.33 & 0.86 & 1.86 \\
        CommonScenes~\cite{zhai2024commonscenes}& & {24.07} & {24.17} & {26.62} & {26.72} & {40.52} & {28.45} & {36.21} & {40.00} & {28.45} & {33.62} \\
        EchoScene~\cite{zhai2024echoscene} & & 39.51 & 25.59 & 37.07 & 17.25 & 35.05 & 43.21 & 33.33 & 50.00 & 41.94 & 40.70\\
        \textbf{MMGDreamer (I+R)} & & \textbf{42.59} & \textbf{30.81}  & \textbf{44.44} & \textbf{19.95} & \textbf{44.12} & \textbf{49.38} & \textbf{40.56} & \textbf{70.00} & \textbf{47.31} & \textbf{45.35} \\
    \midrule 
        Graph-to-3D~\cite{dhamo2021graph} & \multirow{4}{*}{1-NNA} & 98.15 & 99.76 & 98.20 & 97.84 & 98.28 & 98.71 & 99.14 & 93.33 & 99.14 & 99.57 \\
        CommonScenes~\cite{zhai2024commonscenes}& & {85.49} & {95.26} & {88.13} & {86.21} & {75.00} & {80.17} & {71.55} & {66.67} & {85.34} & {78.88} \\
        EchoScene~\cite{zhai2024echoscene} & & 72.84 & 91.00 & 81.90 & 92.67 & 75.74 & {69.14} & 78.90 & \textbf{35.00} & 69.35 & 78.49\\
        \textbf{MMGDreamer (I+R)} & & \textbf{69.44} &  \textbf{90.52} & \textbf{74.81} & \textbf{89.56} & \textbf{68.85} & \textbf{68.35} & \textbf{72.38} & {30.00} & \textbf{62.37} & \textbf{73.26} \\
    \bottomrule
    \end{tabular}
    \caption{\textbf{Object-level generation performance.} We present MMD ($\times0.01$, $\downarrow$), COV($\%, \uparrow$), and 1-NNA($\%, \downarrow$) metrics to assess the quality and diversity of the generated shapes. \textbf{I} denotes nodes using image representations. \textbf{R} represents node relationships.}
    \label{tab:objfd}
\end{table*}

\begin{figure*}[t]
\centering
\includegraphics[width=1\textwidth]{Figure/demo_graph_h_mmg.pdf} % Reduce the figure size so that it is slightly narrower than the column.
\caption{\textbf{Qualitative comparison} with other methods. The first column shows the input mixed-modality graph, which visualizes only the most critical edges in the scene. Red rectangles denote areas of inconsistency in the generated scenes, while green rectangles signify regions of consistent generation.}
\label{demo}
\end{figure*}

\subsection{Shape and Layout Branch}
We employ a dual-branch diffusion model to generate object shapes and scene layouts. 
To facilitate effective information exchange and relationship modeling between nodes during each denoising process, as depicted in Fig.~\ref{pipeline}.C.1, we employ a triplet-GCN structured module that integrates the echo mechanism \cite{zhai2024echoscene} as a Graph Encoder $E_g$.

\subsubsection{Shape Branch.}
For the shape branch, as shown in Fig.~\ref{pipeline}.C.2, we use Truncated Signed Distance Field \cite{curless1996volumetric} as shape representations and employ a pretrained and frozen VQ-VAE to encode them into latent representations $z_i^0$ and decode them back.
% At each denoising step \( t \in \{1, 2, \dots, T\} \), $E_g$ is applied to process the latent codes \( z_i^t \) and the latent graph \( \mathcal{G}_t^l \) (which originates from either \( \mathcal{G}_l^m \) or \( \mathcal{G}_E^m \)), yielding updated representations \( \tilde{z}_i^t \) and \( \tilde{\mathcal{G}_t^l} \). 
At each denoising step \( t \in \{1, 2, \dots, T\} \), $E_g$ is applied to process the latent codes \( z_i^t \) and the latent graph \( \mathcal{G}_t^l \) (which originates from \( \mathcal{G}_E^m \)), yielding updated representations \( \tilde{z}_i^t \) and \( \tilde{\mathcal{G}_t^l} \).
The updated nodes of  \( \tilde{\mathcal{G}_t^l} \), denoted as \( \tilde{\mathcal{V}_t} = \{ \tilde{v}_i^t \} \), are used as conditions for denoiser \( \epsilon_{\theta} \) (3D-UNet). 
The training objective is to minimize the deviation between the true noise \( \epsilon \) and the predicted noise \( \epsilon_{\theta} (\tilde{z}_i^t, t, \tilde{v}_i^t) \). The loss function is defined as:
\begin{equation}
\mathcal{L}_s = \mathbb{E}_{\tilde{z}_i^t, \epsilon \sim \mathcal{N}(0,1), t} \left\| \epsilon - \epsilon_{\theta} (\tilde{z}_i^t, t, \tilde{v}_i^t) \right\|^2.
\end{equation}

\subsubsection{Layout Branch.}
We utilize object bounding boxes to represent the layout of the scene.
Each bounding box \( l_i^0 \) is characterized by its location \( \mathbf{t}_i^0 \in \mathbb{R}^3 \), size \( \mathbf{s}_i^0 \in \mathbb{R}^3 \), and rotation angle \( \varphi_i^0 \). Specifically, the rotation angle \( \varphi_i^0 \) is parameterized by \( [\cos( \varphi_i^0), \sin( \varphi_i^0)]^\top \). 
To ensure proper scale and numerical stability during training, \( \mathbf{t}_i^0 \) and \( \mathbf{s}_i^0 \) are normalized.
As illustrated in Fig.~\ref{pipeline}.C, the layout branch utilizes $E_g$ for relationship modeling. This results in updated latent layout representations \( \tilde{l}_i^t \) and refined graph node embeddings  \( \tilde{\mathcal{N}}_t = \{ \tilde{n}_i^t \} \).
Conditioned on the updated node embeddings, a 1D-UNet is utilized as the denoiser $\psi_{\theta}$ for the denoising process.
The corresponding loss function is formulated as:
\begin{equation}
\mathcal{L}_l = \mathbb{E}_{\tilde{l}_i^t, \psi \sim \mathcal{N}(0,1), t} \left\| \psi - \psi_{\theta} (\tilde{l}_i^t, t, \tilde{n}_i^t) \right\|^2.
\end{equation}
The overall training objective for the layout and shape branches is expressed as:
\begin{equation}
    \mathcal{L}_o = \alpha_1 \mathcal{L}_{l} + \alpha_2 \mathcal{L}_{s},
\end{equation}
where $\alpha_1$ and $\alpha_2$ are weighting factors.
\subsection{Training and Inference Strategy}
The training process is divided into two stages. In the first stage, the visual enhancement module is trained with the loss function $\mathcal{L}_r$, which utilizes textual information from nodes to construct the corresponding visual features. The Relation Predictor is trained with $\mathcal{L}_e$ using triplet representations of the graph. 
In the second stage, the LMMG serves as the input, and the loss function $\mathcal{L}_o$ is employed to jointly optimize the graph encoder with the layout and shape branches, as depicted in Fig.~\ref{pipeline}.A and C.
During inference, as shown in Fig.~\ref{pipeline}, the LMMG is processed through modules B and C to generate the indoor scene. Please see the Supplementary Material for further details.

\section{Experiments}

\subsection{Experimental Settings}

\subsubsection{Evaluation Dataset.}
We validate our approach using the SG-FRONT dataset \cite{zhai2024commonscenes}, which provides comprehensive scene-graph annotations for indoor scenes. 
This dataset includes 45K object instances and 15 types of relationships within bedrooms, dining rooms, and living rooms.
Nodes in the scene graphs represent object categories, while edges indicate relationships between the nodes.
In our experiments, we extracted corresponding images from the 3D-FUTURE dataset \cite{fu20213d_future} based on node IDs to construct a Full-Modality Graph (node contains text and image). We then applied a random mask to mask the text, images, and relationships between nodes in the Full-Modality Graph, producing the Mixed-Modality Graph.
\subsubsection{Evaluation Metrics.}
We evaluate the scene-level and object-level fidelity of the synthesized 3D scenes. 
Scene-level fidelity is quantified using Fŕchet Inception Distance (FID) \cite{heusel2017gans} and Kernel Inception Distance (KID) \cite{binkowski2018demystifying}, which measure the similarity between generated top-down renderings and real scenes renderings. 
For object-level fidelity, we assess the quality of generated object geometry using Minimum Matching Distance (MMD), Coverage (COV), and 1-Nearest Neighbor Accuracy (1-NNA) \cite{yang2019pointflow}, all derived from Chamfer Distance (CD) \cite{fan2017point}.

\subsubsection{Baselines.}
We compare our approach with three state-of-the-art scene synthesis methods: 
1) \textbf{Graph-to-3D} \cite{dhamo2021graph}, which generates 3D scenes directly from scene graphs using a GCN-based VAE; 
2) \textbf{CommonScenes} \cite{zhai2024commonscenes}, which converts scene graphs into controllable 3D scenes through a dual-branch framework with a VAE and LDM; 
3) \textbf{EchoScene} \cite{zhai2024echoscene}, which employs a dual-branch diffusion model with an information echo mechanism for generating globally coherent 3D scenes from scene graphs.
% Additionally, EchoLayout/CommonLayout+SDFusion, which we develop by sequentially combining the layout branch with the text-to-shape generation model SDFusion \cite{cheng2023sdfusion}.

\subsubsection{Implementation Details.}
All experiments are performed on a single NVIDIA A100 GPU with 80 GB memory.
We train our models using the AdamW optimizer, initializing the learning rate at $1 \times 10^{-4}$ and utilizing a batch size of 128. The weighting factors for our loss components, $\alpha_1$ and $\alpha_2$, are consistently set to 1.0.
% For further details, please refer to the Supplementary Material.

\subsection{Scene Generation}

\begin{table}[t!]
\centering
    \begin{tabular}{llccc}
    \toprule
         VEM & RP & FID & KID & mSG \\
     \midrule
           & & 45.10& 3.74 & 0.63 \\
          \phantom{0}\checkmark & & 43.50& 3.27 & 0.63\\
           & \checkmark & 44.07 & 3.43& 0.83 \\
    \midrule 
         \phantom{0}\checkmark & \checkmark&\textbf{ 41.84}& \textbf{2.55}& \textbf{0.86}\\
     \bottomrule
    \end{tabular}
\caption{\textbf{Ablation studies.} Visual Enhancement Module and Relation Predictor are abbreviated as VEM and RP, respectively. The best results are highlighted in \textbf{bold}.}
\label{tab:ablation}

\end{table}

\subsubsection{Quantitative Comparison.}
We evaluate the realism of generated scenes using FID, FID$_{\text{CLIP}}$, and KID scores, as detailed in Tab. \ref{tab:fidkid}. 
MMGDreamer consistently outperforms the previous state-of-the-art method Echoscene, across all metrics when scene graph nodes are represented with mixed-modality. 
Specifically, in living room generation, MMGDreamer (MM+R) achieves a remarkable improvement, reducing FID by 9\%, FID$_{\text{CLIP}}$ by 8\%, and KID by 33\%, highlighting its superior ability to control object geometry while enhancing overall scene realism.

\subsubsection{Qualitative Comparison.}
We present the generated results for different methods across various room types in Fig.~\ref{demo}. In comparison across different room types, our method MMGDreamer consistently demonstrates superior geometry control and visual fidelity in every scenario. 
For instance, in the bedroom, MMGDreamer accurately generates the bed and nightstands with higher geometric consistency, while other methods like Graph-to-3D and EchoScene display noticeable distortions and inconsistencies.
In the dining room, both Graph-to-3D and EchoScene display significant deficiencies, particularly with chair backrests and the sideboard. In contrast, our method, MMGDreamer, not only preserves the correct geometry of these elements but also successfully generates the intricate details of objects placed on the sideboard.
For the complex living room scene, MMGDreamer accurately generates the sofa, coffee table, and lamp, maintaining a coherent spatial layout and ensuring a high degree of consistency between the generated objects and the input images. By contrast, other methods exhibit geometry errors in several pieces of furniture, such as the lamp and chair. 
Notably, the sofa generated by EchoScene contains numerous visible holes, significantly deviating from the actual object geometry.
% We provide additional results in the supplementary material.

\subsection{Object Generation}
We extend our analysis to the object level fidelity, following PointFlow \cite{yang2019pointflow}, by reporting the MMD (×0.01), COV (\%), and 1-nearest neighbor accuracy (1-NNA, \%) metrics to assess per-object generation.
As presented in Tab. \ref{tab:objfd}, our method consistently surpasses the previous state-of-the-art across all object categories in both MMD and COV metrics.
This result highlights MMGDreamer’s geometric control capabilities, ensuring the precise generation of object geometry across various categories.
The 1-NNA measures the distributional similarity between the generated objects and the ground truth, with values near 50\% indicating better capturing of the shape distribution.
Across most object categories, our method consistently outperforms EchoScene in terms of distributional similarity. 
Overall, MMGDreamer demonstrates superior geometric control, resulting in more consistent object-level generation compared to previous approaches.

\subsection{Ablation Study}
We utilize scene-level fidelity (FID and KID) and mean scene graph consistency (mSG) to quantitatively evaluate the effectiveness of different modules within MMGDreamer, as presented in Tab. \ref{tab:ablation}. 
We observe that the configuration with VEM (second row) shows a significant decrease in FID and KID compared to the baseline (first row), indicating that VEM enhances the fidelity of scene generation. Additionally, when the RP module is introduced (third row), there is a notable improvement in mSG, demonstrating that RP effectively predicts relationships between objects, resulting in more coherent scene layouts. It is evident that including both VEM and RP achieves the best performance across all metrics, highlighting the complementary benefits of these modules in producing high-quality scene generation.

\section{Conclusion}
We present MMGDreamer, a dual-branch diffusion model for geometry-controllable 3D indoor scene generation, leveraging a novel Mixed-Modality Graph that integrates both textual and visual modalities. Our approach, enhanced by a Visual Enhancement Module and a Relation Predictor, provides precise control over object geometry and ensures coherent scene layouts. 
% Extensive experiments on the SG-FRONT dataset demonstrate that MMGDreamer significantly outperforms existing methods, achieving state-of-the-art results in scene fidelity and object geometry controllability.
Extensive experiments demonstrate that MMGDreamer significantly outperforms existing methods, achieving state-of-the-art results in scene fidelity and object geometry controllability.

\section{Acknowledgments}
The authors would like to thank the anonymous reviewers for their comments. This work was supported by the National Key R\&D Program of China under Grant 2023YFB2703800 and the Beijing Natural Science Foundation under Funding No. IS23055. The contact author is Zhen Xiao and Chao Zhang.

\bibliography{aaai25}
\input{supplementary_material}

\end{document}
