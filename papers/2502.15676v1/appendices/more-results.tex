\section{More Results}
\label{sec:more_results}

\subsection{Qualitative Results}
% an example of one iteration that improves the model utility

% BigToM example 
% AutoToM - bbfb question 78: needs sobag and initial proposal is sob --> sobag 
% Question 78': "Initial model propose: ['State', 'Observation', 'Belief']\nAssigned models: {0: ['State', 'Observation', 'Belief', 'Action', 'Goal']}"
Among general methods, AutoToM achieves state-of-the-art results across all benchmarks. We provide two qualitative examples to illustrate the effect of variable adjustment (example 1) and timestep adjustment (example 2). These examples also demonstrate the interpretability of \ours, as the constructed model offers us insights into how the method is modeling the agent behavior for the inference. 

\textbf{Example 1: BigToM (Backward Belief Inference)}

\begin{tcolorbox}[
    breakable,
    colframe=gray!40!black,
    colback=gray!5,
    coltitle=white,
    fonttitle=\bfseries,
    colbacktitle=gray!40!black
]
\textbf{Story:} Kavya is a florist in a vibrant Indian market. Kavya wants to create a beautiful bouquet of fresh roses for a customer's anniversary celebration. Kavya sees a batch of roses in her shop that appear to be fresh and vibrant. Unbeknownst to her a mischievous monkey sneaks into the shop and nibbles on the rose petals leaving them damaged and unsuitable for the bouquet. Kavya starts arranging the bouquet using the roses she initially saw. \newline

\textbf{Question: Does Kavya believe the roses are fresh and perfect for the bouquet or damaged by the monkey?}

(a) Kavya believes the roses are fresh and perfect for the bouquet. \textbf{\textcolor[RGB]{110, 170, 110}{(Correct Answer)}}

(b) Kavya believes the roses are damaged by the monkey.
\end{tcolorbox}



\textbf{Variables in the Initial Model Proposal: } State, Observation, Belief

Since the scenario involves only one timestep, a single model suffices. In the initial model, the state of the world indicates that the flowers are damaged after the monkey nibbles on them. However, it remains unclear whether Kavya observes the true condition of the flowers. The model lacks crucial information about Kavya's actions, which are observable and influenced by her beliefs about the flowers' state. These actions can help infer her true belief. Initially, the probability that Kavya believes the flowers are fresh is moderate, $P(\text{Kavya believes the roses are fresh and perfect}$ $\text{for the bouquet} | X^1) = 0.50$. Without variable adjustment, the model cannot answer the question.

\textbf{Variables in the Adjusted Model: } State, Observation, Belief, Action, Goal 

For the initial model, the reward is $R(M,q)=-H(P(q | X^{t_s:t}))=-0.693$ and the model cost is $C(M)=\alpha|M|=0.04$, resulting in a utility $U(M, q)=-0.733$, which does not exceed the utility threshold $U_\text{min}=-0.693$. To address the insufficiency of the initial model's utility relative to our termination threshold, we propose an enhanced model incorporating state, observation, belief, action, and goal. In this revised model, Kavya’s actions—specifically arranging the bouquet using the roses—align with her goal of creating a beautiful bouquet. These observations allow us to infer with high probability that Kavya believes the roses are fresh and suitable for the bouquet, increasing the belief probability to $P(\text{Kavya believes the roses are fresh and perfect}$ $ \text{for the bouquet}| X^1) = 0.97$. With this revised model, the reward is $R(M,q)=-H(P(q | X^{t_s:t}))=-0.135$ and the model cost is $C(M)=\alpha|M|=0.06$, resulting in a utility $U(M, q)=-0.195$, which exceeds our utility threshold $U_{\text{min}}=-0.693$. Based on the adjusted model, \ours can confidently determine the correct answer: (a) Kavya believes the roses are fresh and perfect for the bouquet.\newline


\textbf{Example 2: MMToM-QA (Belief Inference)}

%\textbf{Text input}
\begin{tcolorbox}[
    breakable,
    colframe=gray!40!black,
    colback=gray!5,
    coltitle=white,
    fonttitle=\bfseries,
    colbacktitle=gray!40!black
]

\textbf{Video input:}

\includegraphics[width=\linewidth]{figures/MMToM-frame.png}
\vspace{-3pt}
  
\textbf{What's inside the apartment:} The apartment consists of a bedroom, kitchen, living room, and bathroom. 
In the bedroom, there is a coffee table and a desk. 
The kitchen is equipped with four cabinets, a fridge, a kitchen table, a microwave, and a stove. The 3rd kitchen cabinet from the left houses a water glass and a dish bowl. Inside the fridge, there are two apples, a salmon, a plate, and a dish bowl. The 2nd kitchen cabinet from the left contains a water glass, a chips, a condiment bottle, and a dish bowl. The 1st kitchen cabinet from the left holds a wineglass, a wine, and a condiment bottle. The microwave contains a salmon, and there is a cupcake in the stove. The 4th kitchen cabinet from the left has a plate. 
The living room features a cabinet, a sofa, a coffee table, and a desk. Inside the cabinet, there are two apples and four books. A plate and a remote control are placed on the coffee table. 
The bathroom is furnished with a bathroom cabinet, which is currently empty. \newline

\textbf{Actions taken by Mark:} Mark is situated in the bathroom. He proceeds towards the kitchen, making his way to the stove. He opens and then closes the stove. Subsequently, he strides towards the 4th kitchen cabinet, opens it, and then shuts it. He then moves to the 2nd kitchen cabinet, opens and closes it, before doing the same with the 3rd kitchen cabinet. Finally, he heads towards the 1st kitchen cabinet, opens and closes it, and is about to open the microwave. \newline

\textbf{Question: If Mark has been trying to get a salmon, which one of the following statements is more likely to be true?}

(a) Mark thinks that the salmon is not inside the microwave.

(b) Mark thinks that the salmon is inside the microwave. \textbf{\textcolor[RGB]{110, 170, 110}{(Correct Answer)}}
\end{tcolorbox}




In this problem, we first fuse the information from text and video following \citet{jin2024mmtom}. The fused information is structured into 23 timesteps, each corresponding to an action of Mark at the time. We then propose the initial model: State, Observation, Belief, Action, Goal.

\textbf{Without timestep adjustment.} Bayesian inference must be performed sequentially from the first timestep, even though most actions do not contribute to answering the final question. The model will compute across all timesteps, while the most informative action is actually the last one: if Mark wants to get a salmon but does not believe there is one inside the microwave, he will not open it.

\textbf{With timestep adjustment.} We begin inference from the last timestep, 
where the action likelihood $P(a|b, g)$ is low when $b=$ \textit{Mark thinks that the salmon is not inside the microwave}, and high when $b=$ \textit{Mark thinks that the salmon is inside the microwave}. After performing inference at the last timestep, the belief probabilities corresponding to the choices are $0.998$ and $0.002$. The reward is given by $R(M,q)=-H(P(q | X^{t_s:t}))=-0.014$, while the model cost is $C(M)=\alpha|M|=0.06$. This results in a utility of $U(M, q)=-0.074$, which exceeds the threshold $U_{\text{min}}=-0.693$, allowing our model to determine the final answer without considering earlier timesteps.

\begin{figure}[t!]
  \centering
  \includegraphics[width=1.0\linewidth]{figures/higher_order.pdf}
    % \vspace{-15pt}
  \caption{Comparison of accuracy between \ours and GPT-4o on the HiToM dataset across different reasoning orders. Order 0 refers to questions about an object's actual location; order 1 questions are about an agent's belief about an object's location; order 2 involves questions about an agent's belief regarding another agent's belief, and so forth.}
  \label{fig:higher_order}
  % \vspace{-10pt}
\end{figure}

\subsection{Results for Higher Order Inference}
% accuracy table for orders in hi-tom

Higher-order Theory of Mind (ToM) involves recursive reasoning about others' mental states across multiple levels. The Hi-ToM benchmark \cite{he2023hi} includes questions ranging from Order 0, which involves no agents and asks about the actual location of objects, up to Order 4, which requires recursive reasoning among four agents. Figure \ref{fig:higher_order} compares the performance of GPT-4o and \ours across these different question orders. While GPT-4o experiences a significant decline in accuracy as the ToM order increases, \ours maintains a smaller performance drop and achieves substantially higher accuracy on higher-order questions. This demonstrates that our model-based approach is more robust and scalable, effectively handling complex scenarios involving multiple agents and various levels of recursive reasoning.



\subsection{Full Results of the Ablation Study}\label{sec:more_results_ablation}

Table~\ref{tab:results_ablation_accuracy} shows the performance of ablated methods compared to the full \ours method on all benchmarks.

\begin{table*}[t!]
\centering
% \vspace{5pt}
\begin{small}
\begin{tabular}{c|c|c|c|c|c|c}
\toprule
\textbf{Method} & \textbf{ToMi} & \textbf{BigToM} & \textbf{MMToM-QA} & \textbf{MuMA-ToM} & \textbf{Hi-ToM} &\textbf{All} \\
\midrule
w/o hypo. reduction & 87.60 & 86.17 & 75.83 & 81.67 & 69.50 & 80.15 \\
w/ POMDP & 76.00 & 86.50 & 79.83 & 50.78 & 67.00 & 72.02 \\ 
%\ours w/o model discovery \\
w/o variable adj. & 85.80 & 78.25 & 76.17 & 77.89 & 66.50 & 76.92 \\
% \ours w/o timestep adjustment \\
w/ last timestep & 68.40 & 77.83 & 74.33 & 78.33 & 44.50 & 68.68 \\
w/ all timesteps & 86.00 & 79.09 & 76.50 & 79.33 & 69.00 & 77.98 \\
\midrule
%\ours w/ Model Input & 86.72 & 72.73 & 80.17 & 84.00 & & \textbf{80.91} \\
\ours & 88.30 & 86.92 & 75.50 & 81.44 & 72.50 & 80.93 \\
\bottomrule
\end{tabular}
\end{small}
\caption{Results of ablated methods compared to the full \ours method.}
\label{tab:results_ablation_accuracy}
\end{table*}


In Table \ref{tab:results_ablation_tokens} and \ref{tab:results_ablation_api}, we compare the ablated methods and the full model on the averaged number of tokens per question (in thousands) and the averaged number of API calls at inference per question.



\begin{table*}[t!]
\centering
% \vspace{5pt}
\begin{small}
\begin{tabular}{c|c|c|c|c|c|c}
\toprule
\textbf{Method} & \textbf{ToMi} & \textbf{BigToM} & \textbf{MMToM-QA} & \textbf{MuMA-ToM} & \textbf{Hi-ToM} &\textbf{All} \\
\midrule
w/o hypo. reduction & 15.8 & 6.8 & 19.2 & 24.4 & 20.4 & 17.3 \\
 w/ POMDP & 14.9 & 5.5 & 15.6 & 20.0 & 18.8 & 15.0 \\ 
%\ours w/o model discovery \\
w/o variable adj. & 8.5 & 6.1 & 16.4 & 14.0 & 10.0 & 11.0 \\
% \ours w/o timestep adjustment \\
w/ last timestep & 7.8 & 6.1 & 6.4 & 11.6 & 4.0 & 7.2 \\
w/ all timesteps & 14.2 & 7.7 & 57.2 & 16.4 & 12.4 & 21.6 \\ 
\midrule
%\ours w/ Model Input & 86.72 & 72.73 & 80.17 & 84.00 & & \textbf{80.91} \\
\ours & 9.8 & 6.5 & 14.4 & 13.6 & 12.0 & 11.3 \\
\bottomrule
\end{tabular}
\end{small}
\caption{Comparison of ablated models and the full model on the averaged number of tokens per question (in thousands). Lower is better.}
\label{tab:results_ablation_tokens}
\end{table*}

\begin{table*}[t!]
\centering
% \vspace{5pt}
\begin{small}
\begin{tabular}{c|c|c|c|c|c|c}
\toprule
\textbf{Method} & \textbf{ToMi} & \textbf{BigToM} & \textbf{MMToM-QA} & \textbf{MuMA-ToM} & \textbf{Hi-ToM} &\textbf{All} \\
\midrule
w/o hypo. reduction & 38.91 & 13.99 & 45.97 & 70.73 & 72.58 & 48.44 \\
 w/ POMDP & 36.25 & 8.32 & 41.18 & 42.10 & 51.73 & 35.92 \\ 
%\ours w/o model discovery \\
w/o variable adj. & 22.91 & 12.99 & 35.46 & 35.76 & 29.81 & 27.39 \\
% \ours w/o timestep adjustment \\
w/ last timestep &  21.60 & 12.76 & 12.75 & 28.39 & 9.39 & 16.98 \\
w/ all timesteps & 39.83 & 15.95 & 116.81 & 43.25 & 36.27 & 50.42 \\ 
\midrule
%\ours w/ Model Input & 86.72 & 72.73 & 80.17 & 84.00 & & \textbf{80.91} \\
\ours & 32.23 & 13.81 & 31.36 & 35.08 & 36.45 & 29.79 \\
\bottomrule
\end{tabular}
\end{small}
\caption{Comparison of ablated models and the full model on the averaged number of API calls at inference per question. Lower is better.}
\label{tab:results_ablation_api}
\end{table*}

\subsection{Per-type Accuracy on All Benchmarks}

In Tables~\ref{tab:per_type_acc_tomi} - \ref{tab:per_type_acc_hitom}, we present the results of \ours and baselines on each question type of all benchmarks. Here we compare general methods that can be applied to all benchmarks.


% tomi 
\begin{table*}[t!]
\centering
% \vspace{5pt}
\begin{small}
\begin{tabular}{c|c|c|c|c|c}
\toprule
\textbf{Question Type} & \textbf{First order} & \textbf{Second order} & \textbf{Reality} & \textbf{Memory} & \textbf{All} \\
\midrule
Llama 3.1 70B & 73.75 & 56.25 & 100.00 & 100.00 & 72.00 \\
% Gemini 1.5 Flash & 53.50 & 58.75 & 100.00 & 100.00 & 64.90 \\
Gemini 2.0 Flash & 58.50 & 58.25 & 100.00 & 100.00 & 66.70 \\
% Gemini 2.0 Flash Thinking & 83.25 & 61.75 & 100.00 & 100.00 & 78.00 \\
Gemini 2.0 Pro & 75.00 & 54.75 & 100.00 & 100.00 & 71.90 \\ 
GPT-4o & 80.25 & 62.25 & 100.00 & 100.00 & 77.00 \\ 
% GPT-o3-mini & 79.50 & 53.25 & 100.00 & 100.00 & 73.10\\ 
SimToM & 84.75 & 65.00 & 100.00 & 100.00 & 79.90 \\ 
\ours & {95.00} & {77.50} & 93.00 & 100.00 & 88.30 \\
\bottomrule
\end{tabular}
\end{small}
\caption{Detailed accuracy for ToMi.}
\label{tab:per_type_acc_tomi}
\end{table*}


% bigtom 
\begin{table*}[t!]
\centering
% \vspace{5pt}
\begin{small}
\begin{tabular}{c|c|c|c|c|c}
\toprule
\textbf{Question Type} & \textbf{Forward TB} & \textbf{Forward FB}  & \textbf{Backward TB} & \textbf{Backward FB} & \textbf{All}\\
\midrule
Llama 3.1 70B & 93.75 & 81.00 & 57.00 & 60.50 & 77.83\\
% Gemini 1.5 Flash & {96.50} & 61.50 & {79.00} & 57.50 & 75.42\\
Gemini 2.0 Flash & 94.25 & 87.50 & 77.50 & 51.00 & 82.00\\
% Gemini 2.0 Flash Thinking & 94.75 & 91.50 & 77.50 & 47.00 & 82.83 \\
Gemini 2.0 Pro & 96.00 & 93.75 & 70.00 & 68.50 & 86.33\\ 
GPT-4o & 96.00 & 88.50 & 63.50 & 62.00 & 82.42\\
% GPT-o3-mini & 93.25 & 90.75 & 78.50 & 75.00 & 86.92\\ 
SimToM & 92.50  & 90.00 & 25.00 & 75.00 & 77.50 \\
% TimeToM & 95.00 & {99.00} & - & - & -\\
\ours & 91.25 & {93.75} & 73.00 & {78.50} & {86.92} \\
\bottomrule
\end{tabular}
\end{small}
\caption{Detailed accuracy for BigToM.}
\label{tab:per_type_acc_bigtom}
\end{table*}

% mmtom
\begin{table*}[t!]
\centering
% \vspace{5pt}
\begin{small}
\begin{tabular}{c|c|c|c}
\toprule
\textbf{Question Type} & \textbf{Belief} & \textbf{Goal} &\textbf{All} \\
\midrule
Llama 3.1 70B & 51.33 & 36.33 & 43.83\\
% Gemini 1.5 Flash & 39.00 & 28.00 & 33.50 \\
Gemini 2.0 Flash & 62.67 & 33.33 & 48.00\\
% Gemini 2.0 Flash Thinking & 73.33 & 34.67 & 54.00 \\
Gemini 2.0 Pro & 57.00 & 44.67 & 50.84\\ 
GPT-4o & 55.67 & 32.33 & 44.00 \\
% GPT-o3-mini & 88.67 & 40.67 & 64.67\\ 
SimToM & 75.67 & 26.33 & 51.00 \\
% SimToM & 64.30 & 40.70 & 52.50 \\
\ours & {88.67} & {62.33} & {75.50} \\
\bottomrule
\end{tabular}
\end{small}
\caption{Detailed accuracy for MMToM-QA.}
\label{tab:per_type_acc_mmtom}
\end{table*}


% mumatom
\begin{table*}[t!]
\centering
% \vspace{5pt}
\begin{small}
\begin{tabular}{c|c|c|c|c}
\toprule
\textbf{Question Type} & \textbf{Belief} & \textbf{Goal} & \textbf{Belief of Goal} & \textbf{All} \\
\midrule
Llama 3.1 70B & 68.67 & 51.33 & 47.33 &  55.78 \\
% Gemini 1.5 Flash & 70.67 & 39.34 & 47.67 & 52.56\\
Gemini 2.0 Flash & 68.33 & 50.67 & 47.00 & 55.33\\
% Gemini 2.0 Flash Thinking & 95.33 & 79.00 & 73.33 & 82.56\\
Gemini 2.0 Pro & 63.00 & 66.67 & 57.00 & 62.22 \\ 
GPT-4o & 85.33 & 57.00 & 48.33 & 63.55 \\
% GPT-o3-mini & 74.00 & 67.67 & 68.33 & 70.00 \\ 
SimToM & 54.60 & 43.50 & 44.80 & 47.63 \\
\ours & 88.33 & 77.00 & 79.00 & 81.44 \\
\bottomrule
\end{tabular}
\end{small}
\caption{Detailed accuracy for MuMA-ToM.}
\label{tab:per_type_acc_mumatom}
\end{table*}




% hitom
\begin{table*}[t!]
\centering
% \vspace{5pt}
\begin{small}
\begin{tabular}{c|c|c|c|c|c|c}
\toprule
\textbf{Question Type} & \textbf{Order 0} & \textbf{Order 1} & \textbf{Order 2} & \textbf{Order 3} & \textbf{Order 4} & \textbf{All} \\
\midrule
Llama 3.1 70B & 65.00 & 47.50 & 22.50 & 20.00 & 20.00 & 35.00 \\  
% Gemini 1.5 Flash & 87.50 & 65.00 & 37.50 & 25.00 & 20.00 & 47.00\\
Gemini 2.0 Flash & 95.00 & 70.00 & 50.00 & 27.50 & 20.00 & 52.50\\
% Gemini 2.0 Flash Thinking & 100.00 & 85.00 & 72.50 & 50.00 & 60.00 & 65.50\\ 
Gemini 2.0 Pro & 100.00 & 62.50 & 50.00 & 37.50 & 37.50 & 57.50 \\ 
GPT-4o & 92.50 & 65.00 & 40.00 & 27.50 & 25.00 & 50.00 \\
% GPT-o3-mini & 100.00 & 72.50 & 65.00 & 60.00 & 77.50 & 75.00 \\
% Llama 3.1 70B & 95.00 & 60.00 & 30.00 & 30.00 & 35.00 & 50.00 \\
% Gemini 1.5 Flash & 95.00 & 80.00 & 45.00 & 25.00 & 15.00 & 52.00\\
% GPT-4o & {97.50} & 72.50 & 45.00 & 32.50 & 32.50 & 56.00 \\
SimToM & 100 & 77.50 & 60.00 &  60.00 & 57.50 & 71.00 \\
\ours & 95.00 & {75.00} & 70.00 & 67.50 & {55.00} & {72.50} \\
\bottomrule
\end{tabular}
\end{small}
\caption{Detailed accuracy for HiToM.}
\label{tab:per_type_acc_hitom}
\end{table*}