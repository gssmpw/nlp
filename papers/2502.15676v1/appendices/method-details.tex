\section{\ours Implementation Details}
\label{sec:method_details}

\subsection{Variable Adjustments}\label{sec:app_var_adj}

Table~\ref{tab:model_adjustment} summarizes possible variable adjustments at each timestep.

\begin{table}[t!]
\centering
% \vspace{5pt}
\begin{small}
\begin{tabular}{p{1.2cm}cc}
\toprule
New Var. & Before & After \\
\midrule
\multirow{4}{*}{Goal}
& $P(a^t \mid s^t)$ & $P(a^t \mid s^t, g) P(g)$ \\
& $P(a^t \mid b^t)$ & $P(a^t \mid b^t, g) P(g)$ \\
& $P(a^t)$ & $P(a^t \mid s^t, g) P(g)$ \\
& $P(a^t)$ & $P(a^t \mid b^t, g) P(g)$ \\
\midrule
\multirow{3}{*}{Belief}
& $P(a^t \mid s^t)$ & $P(a^t \mid b^t) P(b^t \mid s^t, b^{t-1})$ \\
& $P(a^t \mid s^t, g)$ & $P(a^t \mid b^t, g) P(b^t \mid s^t, b^{t-1})$ \\
% & None & $P(b^t \mid s^t, b^{t-1})$ \\
\midrule
\multirow{1}{*}{Observ.}
& $P(b^t \mid s^t, b^{t-1})$ & $P(b^t \mid o^t, b^{t-1}) P(o^t \mid s^t)$ \\
\midrule
%\multirow{2}{*}{Action}
%\midrule
\multirow{1}{*}{Int. State}
%& Belief & Belief of other agents \\
& $b(s^t)$ & $b(is^t)$ \\
\bottomrule
\end{tabular}
\end{small}

\caption{Potential variable adjustments, including introducing goal, belief, observation, and interactive state (for high-order ToM). We show the corresponding local conditionals before and after introducing the new variables.}
\label{tab:model_adjustment}

\end{table}


\subsection{Automated Bayesian Inverse Planning}

\textbf{Hypothesis Sampling.} At each timestep, hypotheses for the latent variables are generated using a Large Language Model (LLM) as the backend, guided by the observed variables. Specifically, when the state is not explicitly provided, the LLM acts as a world model, tracking state changes in the story based on the previous state and current actions. For an agent's observation, the LLM is prompted to adopt the perspective of a character, simulating what that character might see, know, or hear in the given environment (e.g., inside a closed room). If no new observation is available at a specific timestep, we neither generate new observations nor update the belief. Additionally, the LLM proposes plausible hypotheses for the agent's belief and goal based on the available information.

\textbf{Hypothesis reduction.} We examine all local conditional probabilities involving a single uncertain variable with multiple hypotheses and eliminate those hypotheses that result in significantly low likelihood values. For example, in $P(o^t \mid s^t)$, where $s^t$ represents a determined state, any observation hypothesis that yields a low likelihood for this term is discarded. This approach reduces the computational cost of estimating $P(b^t \mid o^t, b^{t-1})$. Similarly, the same principle is applied to $P(a^t \mid b^t, g^t)$ and $P(u^t \mid b^t, g^t)$, where unlikely belief hypotheses are removed to further reduce computational complexity.


\subsection{Automated Model Discovery}
% \subsection{Hyperparameters}
When exploring different models during the model discovery, \ours can reuse the hypothesis proposals of variables and local conditionals from previously computed models to avoid repeated computation.

We configure the hyperparameters in Automated Model Discovery as follows: $\alpha = 0.02$, $U_\text{min} = -0.693$.



\subsection{Recursive Reasoning}

Interactive Partially Observable Markov Decision Process (I-POMDP) extends POMDP to multi-agent settings by introducing the concept of interactive states, which include agent models into the state space to capture the recursive reasoning process \citep{gmytrasiewicz2005framework}.
We denote $is_{i, l}$ as the interactive state of agent $i$ at level $l$. For two agents $i$ and $j$, where agent $i$ is interacting with agent $j$, the interactive states at each level are defined as:
\begin{itemize}
\setlength\itemsep{0pt}
    \item \textbf{Level 0:} $is_{i,1}=s$
    \item \textbf{Level 1:} $is_{i,1}=(s,b_{j,0},g_j)$ where $b_{j,0}$ is a distribution over $j$'s interactive state at level 0, $is_{j,0}$
    \item ...
\end{itemize}
%\end{addmargin}

The framework provides a generative model for agents: given agent $i$'s belief of interactive state $b(is_{i,l})$, its action policy will be $\pi (a_i|is_{i,l},g_i)$, and its utterance policy will be $\pi (u_i|is_{i,l},g_i)$. 

In our implementation, we sample one possible state based on $b(s)$ at level $l$ to approximate the state at level $l-1$ as imagined by the agent at level $l$. We can recursively apply this process until reaching level $0$. Based on the state sampled for level $0$, we can then conduct the typical automated BIP based on the model structure at that level. This approach can be conveniently applied to arbitrary levels of recursive reasoning, allowing us to answer higher-order Theory of Mind questions using the same method.


\subsection{BToM Model Space}
\label{sec:model_space}

To apply Bayesian Inverse Planning (BIP) across various scenarios, we define the mental variables and their causal relationships with agent behavior using a family of Bayesian Theory of Mind (BToM) models. These models accommodate different levels of complexity in how agents behave and reason about their environment. 

At each timestep $t$, the observable variables are represented by:
$$X^t = \{x_i^t\}_{i \in N_X} \text{, where } N_X = \{s^t, a^t, u^t\}$$
Here, the state $s^t$ always appear in $X^t$, while either $a^t$ (action) or $u^t$ (utterance) is included at timestep $t$, depending on whether physical motion or verbal communication is presented. In some cases, $a^t$ is only used to update the state and does not affect the inference of beliefs or goals, while in other scenarios it can be crucial for inferring hidden mental states (e.g., an agent’s belief or goal).

The latent variables are denoted by
$$V^t = \{v_i^t\}_{i \in N_V} \text{, where } N_V = \{o^t, b^t, g^t\}$$

Here, the observation $o^t$ is only included when the agent’s belief $b^t$ is part of the model, as it updates $b^t$. The goal $g^t$ is included only if it influences action and is relevant to inference. In cases of higher-order recursive reasoning among multiple agents, the belief over the state $b^t(s^t)$ extends to belief over an interactive state $b^t(is^t)$.

Combining these choices at each timestep yields a model space with 30 possible configurations:
%\begin{addmargin}[-10pt]{0pt}
\begin{itemize}
\setlength\itemsep{0pt}
    \item Action/Utterance: which one is included (2 options).
    \item Belief/Observation: no belief, belief of state, belief of interactive state, belief of state, or belief of interactive state + observation (5 options).
    \item Action(Utterance)/Goal: no goal (action(utterance) irrelevant), action(utterance) only, or action(utterance) + goal (3 options).
\end{itemize}
%\end{addmargin}
Over a time interval from $t_s$ to $t$, this scales to $30^{t-t_s+1}$ possible models.



\textbf{Examples.} In addition to the Markov Decision Process (MDP), Partially Observable Markov Decision Process (POMDP), and Interactive POMDP (I-POMDP) models introduced in Section~\ref{sec:preliminaries}, we present additional examples of models from the BToM model space:
%\begin{addmargin}[-10pt]{0pt}
\begin{itemize}
\setlength\itemsep{0pt}
    \item Observation Update Model: Used in the ToMi benchmark (see Figure \ref{fig:benchmarks_and_models}), this model focuses on how observations update beliefs. Actions are present but only serve to update states and are irrelevant to the inference questions. This model is well-suited for passive scenarios where the focus is on understanding how hidden states produce observable evidence and how the agent updates its beliefs about the world.
    %\item Simple Markov Model: The agent's environment is described as a sequence of states that generate observations. The agent's belief at any given time step is determined solely by the current and past observations. This model assumes no explicit actions, making it well-suited for passive scenarios where the focus is on understanding how hidden states produce observable evidence and how the agent updates its beliefs about the world.
    \item POMDP Variant without Goal: A partially observable scenario in which goals are trivial or irrelevant. This variant emphasizes how partial observability affects belief formation and action selection, without explicit goal-driven behavior.
\end{itemize}
%\end{addmargin}