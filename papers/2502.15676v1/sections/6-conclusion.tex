\section{Conclusion}

We have proposed \ours, a novel framework for open-ended Theory of Mind. Given any ToM inference problem, \ours can automatically construct a suitable BToM model and conduct automated Bayesian inverse planning with an LLM backend. Our experimental results demonstrated that \ours can answer different Theory of Mind questions in diverse scenarios, significantly outperforming baselines. \ours suggests a promising direction toward cognitively grounded Theory of Mind modeling that is scalable, robust, and open-ended. In the future, we intend to further improve the robustness of \ours while reducing its inference cost by exploring the possibility of implicit model proposal and Bayesian inference.

\section*{Limitations}

\ours still makes mistakes in several aspects of the inference and model discovery. First, it sometimes proposes hypotheses unrelated to the ToM inference problem, particularly in questions where the definitions of certain mental variables are more ambiguous. Second, the LLM backend may also produce inaccurate likelihood estimation when there are multiple similar hypotheses for a latent variable. Last, model adjustment may fail to recognize the relevance of certain mental variables, resulting in an insufficient model. In addition, while \ours can balance accuracy and cost to a certain degree, it still requires multiple API calls. For applications with a strict computational budget, there is a need for further reducing the cost.

\section*{Ethics Statement}

Engineering machine Theory of Mind is an important step toward building socially intelligent AI systems that can safely and productively interact with humans in the real world. Our work provides a novel framework for achieving open-ended and reliable machine Theory of Mind, which may serve as a component of any AI systems designed to interact with humans. The explicit BToM model discovered by \ours offers an interpretable explanation of the model results, enabling human users to examine and diagnose the model inference. While we do not foresee any negative impact or risk of our work, we acknowledge the importance of robust and trustworthy machine Theory of Mind. Interpretable and cognitively grounded machine Theory of Mind methods such as \ours may help mitigate the negative effects of LLMs, including hallucinations and biases. Additionally, current Theory of Mind benchmarks are typically constructed using procedurally generated stories and questions. There is a need to carefully examine the potential biases in these benchmarks, to ensure that the models evaluated on these benchmarks are fair and unbiased.