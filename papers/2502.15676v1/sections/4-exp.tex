\begin{table*}[t!]
\centering
% \vspace{5pt}
\begin{small}
\begin{tabular}{l|c|c|c|c|c|c|c}
\toprule
\textbf{Method} & \textbf{Type} & \textbf{ToMi} & \textbf{BigToM} & \textbf{MMToM-QA} & \textbf{MuMA-ToM} & \textbf{Hi-ToM} & \textbf{All} \\
\midrule
SymbolicToM & Specific & \textbf{98.60} & - &  - & - & - & - \\
TimeToM & Specific & 87.80 & - &   - & - & - & - \\
% \textbf{96.00$^*$}
PercepToM & Specific & 82.90 & - & - & - & - & - \\
BIP-ALM & Specific & - & - & 76.70 & 33.90 & - & - \\
LIMP & Specific & - & - & - & 76.60 & - & - \\
\ours w/ Model Spec. & Specific & 88.80 & \textbf{86.75} & \textbf{79.83} & \textbf{84.00} & \textbf{74.00} & \textbf{82.68} \\
\midrule
Llama 3.1 70B & General & 72.00 & 77.83 & 43.83 & 55.78 & 35.00 & 47.41 \\
Gemini 2.0 Flash & General & 66.70 & 82.00 & 48.00 & 55.33 & 52.50 & 60.91\\
Gemini 2.0 Pro & General & 71.90 & 86.33 & 50.84 &  62.22 & 57.50 & 65.76 \\ 
GPT-4o & General & 77.00 & 82.42 & 44.00 & 63.55 & 50.00 & 63.39 \\
SimToM & General & 79.90 & 77.50 & 51.00 & 47.63 & 71.00 & 65.41\\ 
\ours & General & \textbf{88.30} & \textbf{86.92} & \textbf{75.50} & \textbf{81.44} & \textbf{72.50} & \textbf{80.93} \\
\bottomrule
\end{tabular}
\end{small}
\caption{Results of \ours and baselines on all benchmarks. There are two groups of methods: methods that require domain-specific knowledge (e.g., AutoToM w/ Model Spec.) or implementations (e.g., SymbolicToM) and methods that can be generally applied to any domain. ``-'' indicates that the domain-specific method is not applicable to the benchmark. The best results for each method type are highlighted in bold.}
\label{tab:results}
\vspace{-10pt}
\end{table*}



\section{Experiments}
\subsection{Experimental Settings}



We evaluated our method on multiple Theory of Mind benchmarks, including ToMi \citep{le2019revisiting}, BigToM \citep{gandhi2024understanding}, MMToM-QA \cite{jin2024mmtom}, MuMA-ToM \citep{shi2024muma}, and Hi-ToM \cite{he2023hi}. The diversity and complexity of these benchmarks pose significant reasoning challenges. For instance, MMToM-QA and MuMA-ToM incorporate both visual and textual input, while MuMA-ToM and Hi-ToM require higher-order inference. Additionally, MMToM-QA features exceptionally long contexts, and BigToM presents open-ended scenarios.



Besides the full \ours method, we additionally evaluated \ours given manually specified models (AutoToM w/ Model Spec.). 

We compared \ours against state-of-the-art baselines:
    \textbf{LLMs:} Llama 3.1 70B \citep{dubey2024llama}, Gemini 2.0 Flash, Gemini 2.0 Pro \cite{team2023gemini} and GPT-4o \cite{achiam2023gpt};
    
     \textbf{ToM prompting for LLMs:} SymbolicToM \cite{sclar2023minding}, SimToM \cite{wilf2023think}, TimeToM \cite{hou2024timetom}, and PercepToM \citep{jung2024perceptions};
 
  \textbf{Model-based inference:} BIP-ALM \cite{jin2024mmtom} and LIMP \cite{shi2024muma}.


For multimodal benchmarks, MMToM-QA and MuMA-ToM, we adopt the information fusion methods proposed by \citet{jin2024mmtom} and \citet{shi2024muma} to fuse information from visual and text inputs respectively. The fused information is in text form. We ensure that all methods use the same fused information as their input.


We use GPT-4o as the LLM backend for \ours and all ToM prompting and model-based inference baselines to ensure a fair comparisonâ€”except for TimeToM, which relies on GPT-4 and is not open-sourced.


\subsection{Results}
The main results are summarized in Table~\ref{tab:results}. Unlike \ours, many recent ToM baselines can only be applied to specific benchmarks. Among general methods, \ours achieves state-of-the-art results across all benchmarks. In particular, it outperforms its LLM backend, GPT-4o, by a large margin. This is because Bayesian inverse planning is more robust for inferring mental states given long contexts with complex environments and agent behavior. It is also more adept at recursive reasoning which is key to higher-order inference. Notably, \ours performs comparably to manually specified models, showing that automatic model discovery without domain knowledge is as effective as human-provided models. We provide additional results and qualitative examples in Appendix~\ref{sec:more_results}.


\subsection{Ablated Study}



\begin{figure}[t!]
  \centering
  \includegraphics[width=0.8\linewidth]{figures/comparison.pdf}
    \vspace{-10pt}
  \caption{Averaged performance and compute of the full \ours method (star) and the ablated methods (circles) on all benchmarks.}
  \label{fig:ablation}
  \vspace{-10pt}
\end{figure}


We evaluated the following variants of \ours for an ablation study: no hypothesis reduction (\textbf{w/o hypo. reduction}); always using POMDP (\textbf{w/ POMDP}); always using the initial model proposal without variable adjustment (\textbf{w/o variable adj.}); only considering the last timestep (\textbf{w/ last timestep}); and considering all timesteps without timestep adjustment (\textbf{w/ all timesteps}).

The results in Figure~\ref{fig:ablation} show that the full \ours method constructs a suitable BToM model, enabling rich ToM inferences while reducing compute. We analyze key model components below:

\textbf{Hypothesis reduction.}
Compared to the full method, \ours w/o hypo. reduction has a similar accuracy but consumes 53\% more tokens on average, demonstrating that hypothesis reduction optimizes efficiency without sacrificing performance.

\textbf{Variable adjustment.}
\ours dynamically identifies relevant variables for ToM inference, generalizing domain-specific BIP approaches to open-ended scenarios. Compared to its variant without variable adjustment, \ours improves performance with minimal additional compute. The variant that always uses POMDP performs well in scenarios aligned with the POMDP assumption (e.g., MMToM-QA) but generalizes poorly elsewhere and incurs much higher computational costs. %, leading to an 8.5% performance deficit.

\textbf{Timestep adjustment.}
By selecting relevant steps for inference, timestep adjustment enhances performance by focusing on essential information. In contrast, the variant using only the last timestep misses crucial details, significantly lowering performance. The variant incorporating all timesteps suffers from higher computational costs and reduced accuracy due to conditioning on unnecessary, potentially distracting information.



Full ablation results are provided in Appendix~\ref{sec:more_results_ablation}.
