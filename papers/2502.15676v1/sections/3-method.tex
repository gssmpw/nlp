\section{AutoToM}
\label{method}

\subsection{Preliminaries}
\label{sec:preliminaries}

Bayesian Inverse Planning (BIP) is a computational framework that models how observers infer unobservable mental states—such as beliefs and goals—from an agent's behavior \citep{baker2009action}. It assumes that the agent acts rationally according to a generative model, a Bayesian Theory of Mind (BToM) model \cite{baker2017rational}, which specifies how internal variables lead to observable actions in a Bayesian network (e.g., the example models on the bottom panels in Figure~\ref{fig:benchmarks_and_models}). Using inverse inference, BIP inverts this generative process to assess what latent mental variables can lead to observed agent behavior. This probabilistic inference reasons about how agents make decisions, serving as a robust solution to ToM challenges.

There have been different instantiations of BIP in prior works \citep[e.g.,][]{baker2009action, 
ullman2009help,ong2019computational,jha2024neural}. Here we formally define BIP in a unified way. We denote the observable variables at time $t$ describing the environment and an agent's behaviors as $X^t = \{x_i^t\}_{i \in N_X}$, where $N_X$ is the set of observable variables and $x^t_i$ is a particular variable (state, action, or utterance) at $t$. We can extract the values of these observable variables from the context provided in a ToM problem. We denote an agent's latent mental variables at time $t$ as $V^t = \{v_i^t\}_{i \in N_V}$, where $N_V$ is the set of mental variables and $v^t_i$ is a particular mental variable (e.g., goal, desire, belief) at $t$. BIP formulates a BToM model as a Bayesian network that defines $P(V^t, X^t)$, which indicates how the mental variables drive an agent's behavior. Given this model, BIP infers the latent mental variables for the current step $t$:
\begin{equation}
P(V^{t} | X^{t}) =  \frac{P(V^{t}, X^{t})}{\sum_V P(V, X^{t})} \propto P(V^{t}, X^{t}).
\end{equation}
In many real-world scenarios, past observations (such as actions taken at the previous steps) are often valuable for inferring the mental variables at the current step. Suppose the context from step $t_s$ to step $t$ is relevant for the current mental variable inference, then the inference becomes:
\begin{equation}
P(V^{t_s:t} | X^{t_s:t}) \propto P(V^{t_s:t}, X^{t_s:t}).
\end{equation}

In a ToM problem, there is a query concerning a specific target variable $q$ to be inferred. We can answer the query via $P(q | X^{t_s:t})$. Typically, the query asks about a latent mental variable $q = v_i^t \in V^t$, the posterior probability is obtained by marginalizing over other latent variables $V_{-i}^{t_s:t}$ which is the subset of $V^{t_s:t}$ excluding $v_i^t$:
\begin{equation}
% P(v_i| X) \propto \sum_{V_{-i}} P(v_i, V_{-i}, X)
 P(v_i^t| X^{t_{s}:t}) \propto \sum_{V_{-i}^{t_s:t}} P(v_i^t, V_{-i}^{t_{s}:t}, X^{t_{s}:t})
\label{eq:latent_posterior}.
\end{equation}
This can also be extended to predicting a future observable variable $q = x_i^{t+1}$ given observations from $t_{s}$ to $t$:
\begin{equation}
% P(x_i^{t+1}| X^{1:t}) \propto \sum_{V^{1:t}} P(x_i^{t+1}, V^{1:t}, X^{1:t})
 P(x_i^{t+1}| X^{t_{s}:t}) \propto \sum_{V^{t_{s}:t}} P(V^{t_{s}:t}, x_i^{t+1}, X^{t_{s}:t}).
\label{eq:prediction}
\end{equation}

To conduct BIP in different scenarios, we must formulate the mental variables and their causal relationships with agent behavior using suitable BToM models. Each model $M$ is uniquely defined by the observable variables and the latent mental variables, i.e., $M = (V^{t_s:t}, X^{t_s:t})$. Let $s^t \in S$ be the state at time $t$, and $a^t \in A$ be the action taken by the agent at time $t$. The current state and action determines the next state $s^{t+1}$. When the agent has an explicit goal $g \in G$, this setup constitutes a Markov Decision Process (MDP). If the agent only has a partial observation of the state, the model becomes a Partially Observable Markov Decision Process (POMDP) \cite{kaelbling1998planning}. In POMDP, the agent receives a partial observation $o^t$ of the true state $s^t$, maintains a belief $b^t$ over the possible states, and selects its action $a^t$ based on this belief and goal. When there is high-order recursive reasoning between two agents ($i$ and $j$), we can adopt an Interactive POMDP (I-POMDP) \cite{gmytrasiewicz2005framework}, where the belief of state at level $l > 0$ for agent $i$ will become the belief of interactive state $is^t = (s, b_{j,l-1}, g_{j})$, where $b_{j,l-1}$ is the belief of agent $j$ at the lower level $l-1$ and $g_j$ is agent $j$'s goal.

For instance, given a POMDP model, we can conduct the following Bayesian inference to infer the agent's belief $b^t$ at time $t$ from the observed state $s^t$ and $a^t$:
\begin{equation}
\begin{aligned}
P(b^t \mid s^{t}, a^{t})
 \propto & \sum_{b^{t-1}} \sum_{o^t} \sum_g P(a^t \mid b^t, g) \\ 
& \cdot P(b^t \mid b^{t-1}, o^t) P(o^t \mid s^t) \\
& \cdot P(b^{t-1}) P(g).
\end{aligned}
\end{equation}
% \begin{equation}
% \begin{aligned}
% P(b^t \mid s^{t}, a^{t})
% \propto & \sum_{b^{t-1}} \sum_{o^t} \sum_g P(b^{t-1}, s^t, o^t, b^t, a^t, g) \\
% = & \sum_{b^{t-1}} \sum_{o^t} \sum_g P(a^t \mid b^t, g)P(b^t \mid b^{t-1}, o^t) \\
% & \cdot P(o^t \mid s^t)P(b^{t-1})P(g).
% \end{aligned}
% \end{equation}

%The joint probability $P(b^{t-1}, s^t, o^t, b^t, a^t, g)$ is expanded to reflect the dependencies within the model: $o^t$ depends on $s^t$, $b^t$ depends on $b^{t-1}$ and $o^t$, and $a^t$ depends on $b^t$ and $g$. By summing over the latent variables $b^{t-1}$, $o^t$, and $g$, we marginalize out these unobserved factors, yielding the posterior distribution of $b^t$ given $s^t$.

\subsection{Overview of \ours}

As shown in Figure~\ref{fig:overview}, \ours aim to construct a suitable BToM model for Bayesian inverse planning to confidently infer any target variable. There are several key challenges in achieving this: First, different ToM inference problems require different BToM models (as illustrated in Figure~\ref{fig:benchmarks_and_models}); our model does not know which is most suitable a priori. Second, in a given context, our method must determine which time steps are relevant. Third, there is no predefined hypothesis space for each mental variable, and each space could be infinite. Last, to infer mental variables in any context, our method must flexibly represent them without assuming specific types of representations.


\ours addresses these challenges in the two key components: (1) automated Bayesian inverse planning which conducted Bayesian inverse planning given a specified BToM model and (2) automated model discovery which proposes and adjusts the BToM model based on the question and the inference results. These two components form a self-improvement loop to iteratively update the BToM model and corresponding inference result as summarized in Algorithm~\ref{alg:trm}. We discuss these two components in Section~\ref{sec:inverse_planning} and Section~\ref{sec:model_discovery} respectively. More details are provided in Appendix~\ref{sec:method_details}.


\begin{algorithm}[t!]
\caption{\ours}
\begin{algorithmic}[1]
\small
\Require 
    Question $Q$, terminate threshold $U_\text{min}$
\LineComment{Automated Bayesian inverse planning}
\Function{BIP}{$M=(V^{t_s:t}, X^{t_s:t}), q$}  
    \State \textbf{Sample} hypotheses for latent variables $V^{t_s:t}$
    \State \textbf{Conduct} Bayesian inference via LLMs to compute $P(q \mid ^{t_s:t})$ \Comment{Based on Eqn.~(\ref{eq:latent_posterior}) or Eqn.~(\ref{eq:prediction})}
    \State \Return $P(q \mid X^{t_s:t})$
\EndFunction
\LineComment{Automated Model Discovery}
\State \textbf{Extract} query $q$ from $Q$  %$q \gets E_q(Q)$
\State \textbf{Extract} observable variables $X^{1:t}$ from $Q$% \gets E_o(Q)$
\State $t_s \gets t$
\While{$t_s \geq 1$}
\State \textbf{Propose} initial $V^{t_s}$
\State $M \leftarrow (V^{t_s:t}, X^{t_s:t})$
\State $P(q \mid X^{t_s:t}) \gets \textproc{BIP} (M, q)$
\State \textbf{Compute} the model utility $U(M, q)$
\While{$V^{t_s}$ does not contain all mental variables}
    \State $v^{t_s}_\text{new} = \argmax_{v\notin V^{t_s}}U(M + v, q)$ \Comment{Based on results from $\textproc{BIP}(M + v, q)$}
    \If{$U(M + v^{t_s}_\text{new}, q) > U(M, q)$} % cost is considered
        \State $M \gets M + v^{t_s}_\text{new}$
        \State $P(q \mid  X^{t_s:t}) \gets \textproc{BIP} (M, q)$
    \Else
        \State \textbf{Exit} loop
    \EndIf
\EndWhile
\If{$U(M, q) \geq U_\text{min}$} % cost isn't considered
    \State \textbf{Exit} loop
\Else
    \State $t_s \gets t_s - 1$
\EndIf
\EndWhile
\State \textbf{Return} the answer $A \gets \argmax_{q} P(q \mid  X^{t_s:t})$

\end{algorithmic}
\label{alg:trm}
\end{algorithm}




\subsection{Automated Bayesian Inverse Planning}
\label{sec:inverse_planning}

Given a BToM model, $M$, including the necessary latent mental variables $V^{t_s:t}$ and the observable variables $X^{t_s:t}$, we integrate LLMs as the computational backend to implement every aspect of the Bayesian inverse planning (Line 2-6 in Algorithm~\ref{alg:trm}). In particular, the hypothesis sampling module suggests a small set of possible values of latent variables. The Bayesian inference module then computes the posterior distribution of the target variable in the query based on Eqn.~(\ref{eq:latent_posterior}) or Eqn.(~\ref{eq:prediction}).

\textbf{Hypothesis Sampling.} Conventional BIP assumes a manually defined hypothesis space and hypothesis representation for each latent mental variable. Our hypothesis sampling module instead leverages an LLM to propose only a small set of quality hypotheses for each latent variable in $V^{t_s:t}$. This is similar to amortized inference \cite{ritchie2016deep,jha2024neural} but does not require learning a data-driven proposal distribution. To ensure that the sampled hypotheses are relevant to the ToM inference problem, we guide the sampling process with both the question and the observable variables $X^{t_s:t}$. To remove spurious hypotheses generated by the LLM, we further apply \textit{hypothesis reduction} to eliminate unlikely hypotheses and reduce the hypothesis space. Unlikely hypotheses are identified by evaluating the local conditionals. For instance, we discard observation hypotheses with low likelihood conditioned on the state as shown in Figure~\ref{fig:inverse_planning}.

\begin{figure}[t!]
  \centering
  \includegraphics[width=0.95\linewidth]{figures/inverse_planning.pdf}
    \vspace{-10pt}
  \caption{Illustration of automated Bayesian inverse planning given a BToM model. We sample hypotheses for each latent variable ($o^t$ and $b^t$ in this example), remove spurious hypotheses, and finally conduct Bayesian inference based on estimated local conditionals.}
  \label{fig:inverse_planning}
  \vspace{-10pt}
\end{figure}



\textbf{Bayesian Inference.} As shown in Figure~\ref{fig:inverse_planning}, we estimate each local conditional in $P(V^{t_s:t}, X^{t_s:t})$ using an LLM. After marginalizing the joint distribution over non-target latent variables, we then produce the posterior probabilities of the target variable, i.e., Eqn.~(\ref{eq:latent_posterior}). This also applies to predicting a future observable variable, i.e., Eqn.~(\ref{eq:prediction}). 

Our automated Bayesian inverse planning greatly generalizes prior methods that combine BIP and LLMs, such as BIP-ALM \cite{jin2024mmtom} and LIMP \cite{shi2024muma}. Specifically, prior methods assume a fixed model structure for a few specific ToM inference problems. They also cannot propose hypotheses for non-target latent variables. In contrast, \ours can conduct any ToM inference based on any BToM model structure and consider multiple non-target latent variables simultaneously. Additionally, unlike prior methods, our Bayesian inference can work with arbitrary levels of recursive for high-order ToM inference.


\subsection{Automated Model Discovery}
\label{sec:model_discovery}


Prior works on Bayesian inverse planning rely on manually designed BToM models, which limits their applicability to domain-specific scenarios. In contrast, the Automated Model Discovery component automatically proposes a model and dynamically adjusts it to ensure both the \textit{effectiveness} of the model—confidently inferring agents' mental states—and the \textit{efficiency} of the inference by minimizing model complexity. To achieve this, we formulate the utility of a model $M = (V^{t_s:t}, X^{t_s:t})$ used for answering a given query $q$ as 
\begin{equation}
    U(M, q) = R(M, q) - C(M),
\end{equation}
where $R(M, q)$ assesses the model's confidence in answering the query, and $C(M)$ is its computational cost. In this work, the reward is defined as $R(M, q) = -H(P(q | X^{t_s:t}))$, where $P(q | X^{t_s:t})$ is the probability distribution of the target variable based on Eqn.~(\ref{eq:latent_posterior}) or Eqn.~(\ref{eq:prediction}), and $H(\cdot)$ is its entropy. This is designed to decrease the uncertainty in the inference. To minimize the compute needed for the inference, we define the cost of the model as  $C(M) = \alpha |M|$, where $|M|$ denotes the model's complexity, measured by the number of latent mental variables, and $\alpha > 0$ is a weighting factor. The cost increases with complexity, encouraging parsimonious models with lower compute.

There are three modules for Automated Model Discovery:


\textbf{Information Extraction.} The information extraction module (Line 9 in Algorithm~\ref{alg:trm}) processes the context to identify the values of observable variables $X^{1:t}$, including states ($s^t$), actions ($a^t$), and utterances ($u^t$), organized along a timeline (the number of timesteps is determined by the number of actions and utterances). When there are multiple agents, we identify whose mental state the question is asking about (i.e., the target agent), and then construct the timesteps based on the target agent's actions and/or utterances. The extraction is performed once using an LLM and used for model proposal and Bayesian inverse planning.  


\textbf{Initial Model Proposal.} We employ an LLM to propose an initial BToM model based on $X^{1:t}$ and the query (Line 12-15 in Algorithm~\ref{alg:trm}). This initial model represents a minimal model, containing only the essential mental variables needed to answer the question. This initial proposal also includes assessing the level of recursive reasoning necessary for higher-order ToM inference. Note that we always begin with only considering the last timestep in context, i.e., $t_s=t$. Following this model, we conduct automated Bayesian inverse planning, as described in Section \ref{sec:inverse_planning}. If the model utility exceeds a threshold $U_\text{min}$, we accept the inference result as the final answer. Otherwise, we use the model utility to guide model adjustments.

\begin{figure}[t!]
  \centering
  \includegraphics[width=0.98\linewidth]{figures/model_adjustment.pdf}
    \vspace{-10pt}
  \caption{Given any ToM inference problem, we automatically refine the BToM model by alternating between (\textbf{A}) variable adjustment (introducing belief in this example) and  (\textbf{B}) timestep adjustment.}
  \label{fig:model_adjustment}
  \vspace{-10pt}
\end{figure}


\textbf{Model Adjustment.} We iteratively adjust the proposed model to maximize the utility (Line 11-30 in Algorithm~\ref{alg:trm}) by considering two types of model adjustments: variable adjustment (Figure~\ref{fig:model_adjustment}\textbf{A}) and timestep adjustment (Figure~\ref{fig:model_adjustment}\textbf{B}):



\textit{Variable Adjustment.} We refine the model structure at a specific timestep by iteratively introducing new, relevant latent variables into the model to address uncertainty in the inference. These variables include goal, belief, observation, and interactive state as summarized in Table~\ref{tab:model_adjustment} in Appendix~\ref{sec:method_details}. This follows the typical causal structures introduced in prior decision-making models \citep[e.g.,][]{kaelbling1998planning,baker2017rational,ullman2009help,gmytrasiewicz2005framework}. Such restricted variable adjustment helps reduce the model space and ensures the proposed models can explain human behavior. For each adjustment, we compute the updated model utility and accept the modification that offers the biggest increase in utility. This iterative process continues until no further significant improvements are possible. Note that our method can still propose diverse models beyond standard MDP, POMDP, and I-POMDP even with this restricted model adjustment. Appendix~\ref{sec:model_space} provides more details on the model space. 



\textit{Timestep Adjustment.} If model utility remains low and no significant improvement can be achieved via variable adjustment within the current timesteps $t_s:t$, we incorporate an additional step, $t_s-1$, to enhance context for inference. Upon adding a timestep, we first apply the initial model structure and then adjust variables accordingly.

We iterate the variable and timestep adjustments, as outlined in Algorithm~\ref{alg:trm}, until either the model utility exceeds the desired threshold or no further meaningful improvement is possible.