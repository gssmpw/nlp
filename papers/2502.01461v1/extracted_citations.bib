@article{ChithranandaChemBERTa,
  title={ChemBERTa: Large-scale self-supervised pretraining for molecular property prediction},
  author={Chithrananda, Seyone and Grand, Gabriel and Ramsundar, Bharath},
  journal={arXiv preprint arXiv:2010.09885},
  year={2020}
}

@article{DiffDock2023,
  title={DiffDock: Diffusion Steps, Twists, and Turns for Molecular Docking},
  author={Corso, Gabriele and Stärk, Hannes and Jing, Bowen and Barzilay, Regina and Jaakkola, Tommi},
  journal={International Conference on Learning Representations},
  year={2023}
}

@article{McNutt2021,
  title={A deep learning approach to scoring protein-ligand poses using molecular dynamics-based metrics},
  author={McNutt, Ariel T and Francoeur, Paul and Aggarwal, Rachit and Masuda, Tomohide and Meli, Rocco and Ragoza, Matthew and Sunseri, Jocelyn and Koes, David Ryan},
  journal={Journal of Chemical Information and Modeling},
  volume={61},
  number={8},
  pages={3710--3724},
  year={2021}
}

@article{Probst2022,
  title={Biocatalysed synthesis planning using data-driven learning},
  author={Probst, Daniel and Manica, Matteo and Teukam, Yves Gaetan Nana and Castrogiovanni, Alessandro and Paratore, Federico and Laino, Teodoro},
  journal={Nature Communications},
  volume={13},
  number={964},
  year={2022}
}

@article{Stark2022,
  title={EquiBind: Geometric Deep Learning for Drug Binding Structure Prediction},
  author={Stärk, Hannes and Ganea, Octavian-Eugen and Pattanaik, Lagnajit and Barzilay, Regina and Jaakkola, Tommi},
  journal={Proceedings of the 39th International Conference on Machine Learning},
  year={2022}
}

@article{Vaswani2017,
    title={Attention is All you Need},
    author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
    journal={Advances in Neural Information Processing Systems},
    volume={30},
    year={2017}
}

@article{brandes2021proteinbert,
  title={ProteinBERT: A universal deep-learning model of protein sequence and function},
  author={Brandes, Nadav and Ofer, Dan and Peleg, Yam and Rappoport, Nadav and Linial, Michal},
  journal={Bioinformatics},
  volume={38},
  pages={2102--2110},
  year={2021}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{elnaggar2021prottrans,
  title={ProtTrans: Towards cracking the language of life's code through self-supervised deep learning and high performance computing},
  author={Elnaggar, Ahmed and Heinzinger, Michael and Dallago, Christian and Rehawi, Ghalia and others},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2021},
  pages={1--1}
}

@article{fan2023continuous,
  title={Continuous-Discrete Convolution for Geometry-Sequence Modeling in Proteins},
  author={Fan, Hehe and Wang, Zhangyang and Yang, Yi and Kankanhalli, Mohan},
  journal={ICLR},
  year={2023}
}

@article{hayes2024simulating,
  title={Simulating 500 million years of evolution with a language model},
  author={Hayes, Tomas and Rao, Roshan and Akin, Halil and Sofroniew, Nicholas J and Oktay, Deniz and Lin, Zeming and Verkuil, Robert and Tran, Vincent Q and Deaton, Jonathan and Wiggert, Marius and others},
  journal={bioRxiv},
  pages={2024--07},
  year={2024},
  publisher={Cold Spring Harbor Laboratory}
}

@article{hermosilla2020intrinsic,
  title={Intrinsic-extrinsic convolution and pooling for learning on 3d protein structures},
  author={Hermosilla, Pedro and Sch{\"a}fer, Marco and Lang, Mat{\v{e}}j and Fackelmann, Gloria and V{\'a}zquez, Pere Pau and Kozl{\'\i}kov{\'a}, Barbora and Krone, Michael and Ritschel, Tobias and Ropinski, Timo},
  journal={arXiv preprint arXiv:2007.06252},
  year={2020}
}

@article{jing2020learning,
  title={Learning from protein structure with geometric vector perceptrons},
  author={Jing, Bowen and Eismann, Stephan and Suriana, Patricia and Townshend, Raphael JL and Dror, Ron},
  journal={arXiv preprint arXiv:2009.01411},
  year={2020}
}

@article{jumper2021highly,
  title={Highly accurate protein structure prediction with AlphaFold},
  author={Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and {\v{Z}}{\'\i}dek, Augustin and Potapenko, Anna and others},
  journal={nature},
  volume={596},
  number={7873},
  pages={583--589},
  year={2021},
  publisher={Nature Publishing Group}
}

@article{kreutter2021predicting,
  title={Predicting enzymatic reactions with a molecular transformer},
  author={Kreutter, David and Schwaller, Philippe and Reymond, Jean-Louis},
  journal={Chemical science},
  volume={12},
  number={25},
  pages={8648--8659},
  year={2021},
  publisher={Royal Society of Chemistry}
}

@article{lin2023evolutionary,
  title={Evolutionary-scale prediction of atomic-level protein structure with a language model},
  author={Lin, Zeming and Akin, Halil and Rao, Roshan and Hie, Brian and Zhu, Zhongkai and Lu, Wenting and Smetanin, Nikita and Verkuil, Robert and Kabeli, Ori and Shmueli, Yaniv and others},
  journal={Science},
  volume={379},
  number={6637},
  pages={1123--1130},
  year={2023},
  publisher={American Association for the Advancement of Science}
}

@article{lu2022tankbind,
  title={Tankbind: Trigonometry-aware neural networks for drug-protein binding structure prediction},
  author={Lu, Wei and Wu, Qifeng and Zhang, Jixian and Rao, Jiahua and Li, Chengtao and Zheng, Shuangjia},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={7236--7249},
  year={2022}
}

@article{rives2021biological,
  title={Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences},
  author={Rives, Alexander and Meier, Joshua and Sercu, Tom and Goyal, Siddharth and Lin, Zeming and Liu, Jason and Guo, Demi and Ott, Myle and Zitnick, C Lawrence and Ma, Jerry and others},
  journal={Proceedings of the National Academy of Sciences},
  volume={118},
  number={15},
  pages={e2016239118},
  year={2021},
  publisher={National Acad Sciences}
}

@article{ross2022large,
    title={Large-scale chemical language representations capture molecular structure and properties},
    author={Ross, Jerret and Belgodere, Brian and Chenthamarakshan, Vijil and Padhi, Inkit and Mroueh, Youssef and Das, Payel},
    journal={Nature Machine Intelligence},
    volume={4},
    number={12},
    pages={1256--1264},
    year={2022},
    publisher={Nature Publishing Group UK London}
}

@article{schwaller2019molecular,
  title={Molecular transformer: a model for uncertainty-calibrated chemical reaction prediction},
  author={Schwaller, Philippe and Laino, Teodoro and Gaudin, Th{\'e}ophile and Bolgar, Peter and Hunter, Christopher A and Bekas, Costas and Lee, Alpha A},
  journal={ACS central science},
  volume={5},
  number={9},
  pages={1572--1583},
  year={2019},
  publisher={ACS Publications}
}

@inproceedings{stark2022equibind,
  title={Equibind: Geometric deep learning for drug binding structure prediction},
  author={St{\"a}rk, Hannes and Ganea, Octavian and Pattanaik, Lagnajit and Barzilay, Regina and Jaakkola, Tommi},
  booktitle={International conference on machine learning},
  pages={20503--20521},
  year={2022},
  organization={PMLR}
}

@article{zhang2023protein,
  title={Protein Representation Learning by Geometric Structure Pretraining},
  author={Zhang, Zuobai and Xu, Minghao and Jamasb, Arian Rokkum and others},
  journal={ICLR},
  year={2023}
}

