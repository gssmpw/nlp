\section{Related Works}
\subsection{Biocatalysis Prediction}
The evolution of biocatalysis prediction has been significantly shaped by advances in chemical reaction prediction, particularly in sequence-to-sequence models. Neural sequence-to-sequence approaches \cite{schwaller2019molecular} pioneered the treatment of reactions as translation tasks, with transformer architectures like ChemBERTa \cite{ChithranandaChemBERTa} and MolFormer \cite{ross2022large} further improving the capture of molecular dependencies.

The adaptation of these models to biocatalysis presented unique challenges in representing enzyme-substrate interactions. Kreutter et al. \cite{kreutter2021predicting} first demonstrated the viability of transformer models for biocatalysis by leveraging upsampled enzymatic reaction data. ECREACT \cite{Probst2022} advanced this approach by representing enzymes through EC numbers as special tokens in the sequence-to-sequence framework. However, this discrete representation limited the model's ability to capture nuanced enzyme-substrate interactions.

Our work addresses these limitations through the DAA mechanism, which creates dynamic, substrate-specific enzyme representations within the sequence-to-sequence framework, enabling more precise modeling of enzyme-substrate interactions.
% \subsection{Biocatalysis Prediction}
% The evolution of biocatalysis prediction has been significantly shaped by advances in chemical reaction prediction, particularly in the domain of sequence-to-sequence models. Understanding this progression requires examining how these approaches were adapted from chemical to enzymatic reactions.

% \subsubsection{Chemical Reaction Prediction Foundations}
% Chemical reaction prediction saw significant advancement through sequence-to-sequence architectures. Neural sequence-to-sequence models \cite{schwaller2019molecular} pioneered the treatment of reactions as translation tasks, converting reactant SMILES strings into product SMILES strings. This approach was further enhanced by transformer architectures in models like ChemBERTa \cite{ChithranandaChemBERTa} and MolFormer \cite{ross2022large}, which improved the capture of long-range molecular dependencies and reaction mechanisms.

% \subsubsection{Extension to Enzymatic Reactions}
% The adaptation of these sequence-to-sequence models to biocatalysis presented unique challenges due to the need to incorporate enzyme information. Kreutter et al. \cite{kreutter2021predicting} made the first breakthrough by demonstrating that transformer models trained on chemical reaction data could be effectively adapted to biocatalysis through strategic upsampling of enzymatic reactions, even without explicit enzyme representations. This work established that chemical reaction prediction principles could be directly extended to enzymatic systems.

% Building on this foundation, ECREACT \cite{Probst2022} introduced a more sophisticated approach by representing enzymes through their EC numbers as special tokens in the sequence-to-sequence framework. While this improved the model's ability to distinguish between different enzyme classes, the discrete EC number representation limited its capacity to capture nuanced enzyme-substrate interactions.

% Our work addresses these limitations by introducing the DAA mechanism, which creates dynamic, substrate-specific enzyme representations within the sequence-to-sequence framework. This approach enables more precise modeling of enzyme-substrate interactions while maintaining the benefits of transformer-based prediction.

\subsection{Representation Learning in Proteins}
Protein representation learning has developed along sequence-based and structure-based approaches.

\subsubsection{Sequence-based Protein Language Models}
Significant advances in protein language models have been seen in recent years, with transformer-based architectures\cite{Vaswani2017} leading the way. Notable developments include ProteinBERT \cite{brandes2021proteinbert} and ProtTrans\cite{elnaggar2021prottrans}, which adapted BERT\cite{devlin2018bert} for protein sequences, and the ESM model family \cite{hayes2024simulating, lin2023evolutionary, rives2021biological}, which demonstrated the benefits of scale in protein modeling. These models have shown remarkable success in capturing local and long-range protein interactions, establishing new benchmarks in protein property prediction tasks.

\subsubsection{Structure-based Representation Learning}
Following AlphaFold's \cite{jumper2021highly} breakthrough in protein structure prediction, structure-based representation learning has gained prominence. Models like GearNet \cite{zhang2023protein} have introduced novel architectures for capturing protein structure through graph-based approaches. Various methods have emerged to better model protein geometry and structural relationships \cite{fan2023continuous, hermosilla2020intrinsic, jing2020learning}, highlighting the importance of three-dimensional information in protein understanding.

Despite these advances, both sequence-based and structure-based approaches generate static protein representations that remain fixed regardless of molecular context, limiting their ability to capture the dynamic nature of protein-molecule interactions.

\subsection{Molecular Docking Approaches}
The evolution of molecular docking methods provides crucial context for our work. Traditional approaches relied on physics-based scoring functions and search algorithms \cite{McNutt2021,Stark2022}, facing computational challenges particularly in blind docking scenarios. Recent learning-based methods have made significant strides in addressing these limitations. EquiBind \cite{stark2022equibind} introduced keypoint-based methods for pocket-ligand alignment, while TANKBind \cite{lu2022tankbind} enabled independent predictions for multiple binding sites. DiffDock \cite{DiffDock2023} represented a paradigm shift by reformulating docking as a generative modeling problem. While these approaches have improved docking accuracy and efficiency, they primarily focus on predicting binding poses rather than using docking information to enhance protein representations. Our DAA method uniquely leverages docking predictions to create dynamic protein embeddings that adapt to different molecular interaction contexts.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%        ALGO  %%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{letex_figures_tables/algo}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%  ALGO  %%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%        Main Figure  %%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{letex_figures_tables/main_fig}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%  END Main Figure  %%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%