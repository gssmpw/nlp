\subsection{Stochastic Reaction Networks}
\label{subsec:SRNs}
Consider a chemical system with $d$ interacting species $S_1, \dots, S_d$ and $J$ reactions given by 
\begin{equation}
    \sum_{i=1}^{d} \nu_{ij}^{-} S_i \longrightarrow \sum_{i=1}^{d} \nu_{ij}^{+} S_i, \quad \text{for } j = 1, \dots, J, 
\end{equation}
where $\nu_{ij}^{-}$ is the number of molecules of $S_i$ consumed by reaction $j$, and $\nu_{ij}^{+}$ is the number of molecules of $S_i$ produced by the reaction $j$. Let $\boldsymbol{Z}(t) = \left( Z_1(t), \dots, Z_d(t) \right)^\top \in \mathsf{Z} \subseteq \mathbb{Z}_{\geq 0}^d$ be the copy numbers (amount of molecules) of each species at time $t$.

In a low copy number regime, stochastic effects dominate, and the system dynamics can be modeled as a continuous-time Markov chain \cite{Gillespie1976, Gillespie1992DerivationCME} with transition probabilities between times $t$ and $t+h \, (h>0)$, given by
\begin{equation}
    \Probcond{ \boldsymbol{Z}(t+h) = \boldsymbol{z} + \boldsymbol{\nu}_j }{ \boldsymbol{Z}(t) = \boldsymbol{z} } = a_j (\boldsymbol{z})h + o(h), 
\end{equation}
where $\boldsymbol{\nu}_j := \left( \nu_{1j}^{+} - \nu_{1j}^{-}, \dots,  \nu_{dj}^{+} - \nu_{dj}^{-}  \right)^\top$ is a stoichiometric vector, and $a_j: \mathsf{Z} \to \mathbb{R}_{\geq 0}$ is the \textit{propensity function} of reaction $j$. For chemical reactions, the propensities are typically given by the \textit{mass action kinetics}: 
\begin{equation}
    \label{eq:mass_action}
    a_j (\boldsymbol{z}) = 
    \begin{dcases}
        \theta_j \prod_{i=1}^{d} \frac{z_i !}{(z_i - \nu_{ij}^{-})!} & \text{if } \forall i \; z_i \geq \nu_{ij}^{-} \\
        0 & \text{otherwise,} \\
    \end{dcases} 
\end{equation}
where the constant $\theta_j$ is the \textit{reaction rate} of reaction $j$. This is not the only possible form of propensity functions, and the results in this work can be directly extended to other types of propensity functions (e.g., Hill-type propensities \cite{murray2002mathematical}).

Using the random time change representation \cite{Ethier1986MarkovProcess}, one can express the current state of the process $\boldsymbol{Z}$ via mutually independent unit-rate Poisson processes $R_1, \dots, R_J$:
\begin{equation}
\label{eq:RTC_representation}
    \boldsymbol{Z}(t) = \boldsymbol{Z}(0) + \sum_{j=1}^{J} R_j \left( \int_0^t  a_j \left( \boldsymbol{Z}(s) \right) \diff s \right) \cdot \boldsymbol{\nu}_j ,
\end{equation}
where $\boldsymbol{Z}(0)$ is a random vector (independent of $R_1, \dots, R_J$) characterized by the initial distribution $\mu$. 

A fundamental problem associated with the \ac{SRN} is to determine $p (\boldsymbol{z}, t) := \Probmu{\boldsymbol{Z}(t) = \boldsymbol{z}}$ for a given initial distribution $\mu$ and a set of reactions $\{ a_j, \boldsymbol{\nu}_j \}, \, j = 1,\dots,J$. This forward problem can be addressed using \ac{MC} methods, for example, a \acf{SSA} \cite{Gillespie1977SSA}. This approach involves sampling exponential waiting times between reactions and recomputing the propensities after each. Another approach is to approximate the solution of the \acf{CME} \cite{Gillespie1992DerivationCME}, which describes the time evolution of the \ac{PMF} $p(\boldsymbol{z}, t)$.

This work focuses on the filtering problem, which is structurally similar to the forward problem. Therefore, the numerical methods for its solution are closely related to the corresponding methods for the forward problem.










\subsection{Filtering Problem}
\label{subsec:filtering_problem}
Consider a noise-free filtering problem with exact observations, where the current state $\boldsymbol{Z}(t)$ at time $t \in [0, T]$ can be split as follows:
$$
    \boldsymbol{Z}(t) = \begin{bmatrix} \boldsymbol{X}(t) \\ \boldsymbol{Y}(t) \end{bmatrix},
$$
where $\boldsymbol{X}(t) \in \mathsf{X}$ is the hidden process corresponding to the unobserved species, and $\boldsymbol{Y}(t) \in \mathsf{Y}$ is the observed process corresponding to the species that can be tracked. For continuous and noise-free observations, the filtering problem is to estimate the conditional distribution of the unobserved process:
\begin{equation}
\label{eq:pi_def}    
    \pi_{\boldsymbol{y}} (\boldsymbol{x}, t) := \Probcondmu{\boldsymbol{X}(t) = \boldsymbol{x} }{ \boldsymbol{Y}(s) = \boldsymbol{y}(s), s \leq t }
\end{equation}
for a given trajectory $\{ \boldsymbol{y}(s), s \leq t \}$. 

Following \cite{DAmbrosio2022FFSP}, we introduce the following non-explosivity condition to ensure the uniqueness of $\pi_{\boldsymbol{y}}$:
\begin{equation}
\label{eq:non_expl_cond}
\tag{A1}
    \sup_{s \in [0, T]} \sum_{j=1}^{J} \Emu{a_j^2 (\boldsymbol{Z}(s))} < \infty.
\end{equation}
This condition means the system state does not increase to infinity (almost surely) in a finite time interval $[0, T]$. This assumption holds for most of the \acp{SRN} considered in the literature.




We split the stoichiometric vectors $\boldsymbol{\nu}_j$ into parts $\boldsymbol{\nu}_{\boldsymbol{x}, j}$ and $\boldsymbol{\nu}_{\boldsymbol{y}, j}$, corresponding to $\boldsymbol{X}$ and $\boldsymbol{Y}$, respectively. Let us denote by $\mathcal{O} := \{ j : \boldsymbol{\nu}_{\boldsymbol{y}, j} \neq \boldsymbol{0} \}$ the set of observable reactions that can alter $\boldsymbol{Y}$ and denote by $\mathcal{U} := \{ j : \boldsymbol{\nu}_{\boldsymbol{y}, j} = \boldsymbol{0} \}$ the set of unobservable reactions that cannot alter $\boldsymbol{Y}$. For a given trajectory $\{ \boldsymbol{y}(s), s \leq T \}$, the corresponding jump times are denoted by $t_1, \dots, t_n$.


Under the non-explosivity assumption \eqref{eq:non_expl_cond}, the conditional distribution $\pi_{\boldsymbol{y}}$ at each of the intervals $(t_k, t_{k+1})$ evolves according to the following \ac{ODE} \cite{DAmbrosio2022FFSP, Rathinam2021PFwithExactState}:
\begin{equation}
    \label{eq:filtering_equation_pi}
    \begin{aligned}
        \frac{\mathrm{d}}{\mathrm{d} t} \pi_{\boldsymbol{y}} (\boldsymbol{x}, t) = &\sum_{j \in \mathcal{U}} \pi_{\boldsymbol{y}} (\boldsymbol{x} - \boldsymbol{\nu}_{\boldsymbol{x}, j}, t) a_j \left( \boldsymbol{x} - \boldsymbol{\nu}_{\boldsymbol{x}, j}, \boldsymbol{y}(t_k) \right) - \sum_{j \in \mathcal{U}} \pi_{\boldsymbol{y}}(\boldsymbol{x}, t) a_j (\boldsymbol{x}, \boldsymbol{y}(t_k)) \\
        &- \pi_{\boldsymbol{y}} (\boldsymbol{x}, t) \cdot \sum_{j \in \mathcal{O}} \left( a_j (\boldsymbol{x}, \boldsymbol{y}(t_k)) -  \sum_{\hat{\boldsymbol{x}} \in \mathsf{X}} a_j (\hat{\boldsymbol{x}}, \boldsymbol{y}(t_k)) \pi_{\boldsymbol{y}} (\hat{\boldsymbol{x}}, t)\right).
    \end{aligned}
\end{equation}

At each jump point $t_k$, we know exactly how the state vector changed and therefore can identify the set of reactions that might have caused the jump: $\mathcal{O}_k := \{ j : \boldsymbol{\nu}_{\boldsymbol{y}, j} = \boldsymbol{y}(t_k) - \boldsymbol{y}(t_k^{-}) \}$. At jump times $t_k$, the conditional probability satisfies the following:
\begin{equation}
    \label{eq:filtering_equation_pi_jump}
    \pi_{\boldsymbol{y}} (\boldsymbol{x}, t_k) = \frac{ 
    \sum\limits_{j \in \mathcal{O}_k} a_j (\boldsymbol{x} - \boldsymbol{\nu}_{\boldsymbol{x}, j}, \boldsymbol{y}(t_{k-1})) \pi_{\boldsymbol{y}} (\boldsymbol{x} - \boldsymbol{\nu}_{\boldsymbol{x}, j}, t_{k}^{-})
    }{ 
    \sum\limits_{\hat{\boldsymbol{x}} \in \mathsf{X}} \sum\limits_{j \in \mathcal{O}_k} a_j (\hat{\boldsymbol{x}}, \boldsymbol{y}(t_{k-1})) \pi_{\boldsymbol{y}} (\hat{\boldsymbol{x}}, t_k^{-})
    }.
\end{equation}

The pair of equations \eqref{eq:filtering_equation_pi} and \eqref{eq:filtering_equation_pi_jump}, called the \textit{filtering equations}, characterize the complete dynamics of $\pi_{\boldsymbol{y}}$. The initial condition 
\begin{equation}
\label{eq:pi0_def}
    \pi_{\boldsymbol{y}} (\boldsymbol{x}, 0) = \Probcond{\boldsymbol{X}(0) = \boldsymbol{x}}{\boldsymbol{Y}(0) = \boldsymbol{y}(0)} 
\end{equation} 
can be derived from the initial distribution $\mu$, which is assumed to be given.

Due to the non-linearity of \eqref{eq:filtering_equation_pi} and \eqref{eq:filtering_equation_pi_jump} it is more convenient to introduce an unnormalized \ac{PMF} $\rho_{\boldsymbol{y}} \propto \pi_{\boldsymbol{y}}$ that obeys the following equations:
\begin{equation}
\label{eq:filtering_equation_rho}
\begin{aligned}
    \frac{\mathrm{d}}{\mathrm{d} t} \rho_{\boldsymbol{y}} (\boldsymbol{x}, t) = &\sum_{j \in \mathcal{U}} \rho_{\boldsymbol{y}} (\boldsymbol{x} - \boldsymbol{\nu}_{\boldsymbol{x}, j}, t) a_j (\boldsymbol{x}-\boldsymbol{\nu}_{\boldsymbol{x}, j}, \boldsymbol{y}(t_k))
    - \sum_{j=1}^{J} \rho_{\boldsymbol{y}} (\boldsymbol{x}, t) a_j (\boldsymbol{x},\boldsymbol{y}(t_k)),  \quad t \in (t_k, t_{k+1}) 
\end{aligned}
\end{equation}

\begin{equation}
\begin{aligned}
\label{eq:filtering_equation_rho_jump}
    \rho_{\boldsymbol{y}} (\boldsymbol{x}, t_k) =  \frac{1}{\abs{\mathcal{O}_k}} \sum\limits_{j \in \mathcal{O}_k} a_j (\boldsymbol{x} - \boldsymbol{\nu}_{\boldsymbol{x}, j}, \boldsymbol{y}(t_{k-1})) \rho_{\boldsymbol{y}} (\boldsymbol{x} - \boldsymbol{\nu}_{\boldsymbol{x}, j}, t_{k}^{-}),
\end{aligned}
\end{equation}
with the initial condition $\rho_{\boldsymbol{y}}(\boldsymbol{x}, 0) = \pi_{\boldsymbol{y}}(\boldsymbol{x}, 0)$.  
After reaching a solution to \eqref{eq:filtering_equation_rho} and \eqref{eq:filtering_equation_rho_jump},  $\pi_{\boldsymbol{y}}$ can be obtained with normalization:
\begin{equation}
\label{eq:rho_normalization_full}
    \pi_{\boldsymbol{y}}(\boldsymbol{x}, t) = \frac{\rho_{\boldsymbol{y}} (\boldsymbol{x}, t)}{\sum\limits_{\hat{\boldsymbol{x}} \in \mathsf{X}} \rho_{\boldsymbol{y}} (\hat{\boldsymbol{x}}, t)} .
\end{equation}

The filtering equation \eqref{eq:filtering_equation_pi} should be solved simultaneously for all possible states $\boldsymbol{x} \in \mathsf{X}$, leading to a coupled system of possibly an infinite number of equations. Thus, it requires numerical approximation. The same applies to the unnormalized version \eqref{eq:filtering_equation_rho}.

The following section briefly summarizes the two methods for numerically approximating \eqref{eq:pi_def}.



\subsubsection{Filtered Finite State Projection}
The \acf{FFSP} method \cite{DAmbrosio2022FFSP} is based on truncating the state space, similar to the idea introduced in \cite{Munsky2006FSP} for solving the \acf{CME}. Let us consider a finite subset of states $\mathsf{X}_N \subset \mathsf{X}$ with $ \abs{\mathsf{X}_N} = N < \infty$ and assume that the probability of visiting the remaining states at the time interval $[0,T]$ is negligible. 

By setting $\rho_{\boldsymbol{y}}(\hat{\boldsymbol{x}}, \cdot) = 0$ for all $\hat{\boldsymbol{x}} \in \mathsf{X}_N \setminus \mathsf{X}$, Equation \eqref{eq:filtering_equation_rho} becomes a system of $N$ linear \acp{ODE}, which can be solved analytically or numerically. 

Following \cite{DAmbrosio2022FFSP}, we assume that the observed propensities are bounded for each $\boldsymbol{y}$. That is, there exists a function $C_1 : \mathsf{Y} \to \mathbb{R}_{\geq 0}$ such that
\begin{equation}
    \label{eq:assumption_bound_propens} \tag{A2}
    \sup_{\boldsymbol{x} \in \mathsf{X}} \sum_{j \in \mathcal{O}} a_j \left( \boldsymbol{x}, \boldsymbol{y} \right) \leq C_1(\boldsymbol{y}) .
\end{equation}

The error of the \ac{FFSP} method can be controlled by selecting a truncated space $\mathsf{X}_N$ according to \cite[Theorems~2 and 4]{DAmbrosio2022FFSP}. More precisely, the error can be reduced by adaptively including more states in $\mathsf{X}_N$ and including more equations in the \ac{FFSP} system (for more detailed numerical algorithms and the error analysis of this method, refer to \cite{DAmbrosio2022FFSP}).

The main issue of the \ac{FFSP} method is that once the number of states $N = \abs{\mathsf{X}_N}$ is too large, solving the system \eqref{eq:filtering_equation_rho} becomes computationally expensive. In particular, the computational complexity increases exponentially with respect to the dimensionality of the hidden state space.





\subsubsection{Particle Filter}
\label{subsubsec:PF}
Another method for the filtering problem, called the \ac{PF}, was introduced in \cite{Rathinam2021PFwithExactState}. The idea is to sample a pair of processes $\boldsymbol{V}$ and $w$ such that 
\begin{equation}
    \label{eq:PF_expectation_rho}
    \rho_{\boldsymbol{y}} (\boldsymbol{x}, t) = \Emu{ 1_{\{ \boldsymbol{V}(t) = \boldsymbol{x} \}} w(t) },
\end{equation}
where $1_{\{\cdot\}}$ denotes the indicator function defined as follows:
$$
    1_{A} (\omega) = 
    \begin{cases}
        1 & \text{if } \omega \in A \\
        0 & \text{if } \omega \not \in A \\
    \end{cases} .
$$
The process $\boldsymbol{V}$ is associated with another \ac{SRN} with reactions from $\mathcal{U}$; therefore, it can be sampled with \ac{SSA}. The process $w(t)$ represents a weight of the trajectory of $\boldsymbol{V}$ given observations $\{ \boldsymbol{y}(s), s \leq t \}$. For more details on constructing the processes $\boldsymbol{V}$ and $w$, refer to \cite{Rathinam2021PFwithExactState}.

Let $(\boldsymbol{V}_{i}, w_{i}), \, i = 1, \dots, M$ be independent realizations (particles) of $(\boldsymbol{V}, w)$, then 
\begin{equation}
    \label{eq:PF_weighed_avg}
    \rho_{\boldsymbol{y}} (\boldsymbol{x}, t) \approx \frac{1}{M} \sum_{i=1}^{M} 1_{\left\{ \boldsymbol{V}_{i}(t) = \boldsymbol{x} \right\}} w_{i}(t).
\end{equation}

In practice, only a few particles significantly contribute to the weighted average \eqref{eq:PF_weighed_avg} even using many samples, (i.e., the effective sample size can be critically small). This problem is known as sample collapse and occurs because, for most trajectories, the weight process $w$ rapidly decreases to zero. To avoid this problem, one can use the \textit{resampling} procedure, discarding particles with small weights and multiplying the number of particles with large weights (e.g., using a bootstrap algorithm \cite{Gordon1993PF}). However, the resampling step breaks particle independence and introduces bias in the resulting estimator. Therefore, a trade-off exists between the error introduced by resampling and the error from weight degeneracy. This problem becomes significant for high-dimensional processes \cite{Snyder2008HighDimPF, Djuric2013HighDimPF}. Refer to \cite{Bain2009Fundamentals} for a more detailed discussion on this problem and other resampling algorithms. 

In addition, a common problem of \ac{MC}-based methods is related to high variance. The indicator function in \eqref{eq:PF_weighed_avg} is prone to this problem if $\boldsymbol{x}$ is a low-probability state (e.g., if $\boldsymbol{x}$ is in the tail of the distribution). In contrast, if we are not interested in the distribution itself but in the conditional expectation of $f(\boldsymbol{X}(t))$ with a given function $f$, then the \ac{PF} typically outperform other methods.

Although, to the best of our knowledge, no results exist on the convergence of this specific version of \ac{PF}, for the sake of error analysis of our approach (Section~\ref{subsec:FMP_error_analysis}), we assume that the error of the \ac{PF} decreases as $O(M^{-1/2})$ with respect to the number of particles $M$. This assumption is motivated by the results of the general \ac{PF} in \cite{Chopin2004CLTforSeqMC, crisan2002survey} and can be formulated as follows, let $Q_{\boldsymbol{y}} := \Econdmu{f(\boldsymbol{X}(t))}{\boldsymbol{Y}(s) = \boldsymbol{y}(s), s \leq t}$ be a \ac{QOI} with a measurable function $f$ s.t. $\Emu{(f(\boldsymbol{X}(t)))^2} < \infty$, and $Q^{M}_{\boldsymbol{y}} := \frac{ \sum_{i=1}^{M} f(\boldsymbol{V}_i (t)) w_i (t) }{ \sum_{i=1}^{M} w_i (t) }$ be the corresponding \ac{PF} estimator based on $M$ particles, then we assume 
\begin{equation}
\label{eq:PF_error_rate} \tag{A3}
    \Emu { \abs{ Q_{\boldsymbol{y}} - Q_{\boldsymbol{y}}^M } } = O(M^{-1/2}).
\end{equation}







\subsection{Marginal Filtering Problem}
\label{sec:margin_filtering_problem}

In this paper, we focus on situations in which only the distribution of a small subset of the hidden species should be estimated. The hidden state vector is split as follows: $ \boldsymbol{X}(t) = \begin{bmatrix} \boldsymbol{X}'(t) \\ \boldsymbol{X}''(t) \end{bmatrix} $, where $\boldsymbol{X}'$ corresponds to the species of interest, and the process $\boldsymbol{X}''$ corresponds to the remaining ones. The goal of the marginal filtering problem is to estimate the following:
\begin{equation}
\label{eq:filtering_problem_margin}
    \pi'_{\boldsymbol{y}} (\boldsymbol{x}', t) = \Probcondmu{\boldsymbol{X}'(t) = \boldsymbol{x}' }{ \boldsymbol{Y}(s) = \boldsymbol{y}(s), s \leq t },
\end{equation}
for a given trajectory $\{ \boldsymbol{y}(s), s \leq t \}$. Clearly,  $\pi'_{\boldsymbol{y}}$ can be derived from $\pi_{\boldsymbol{y}}$ via marginalization:
\begin{equation}
\nonumber    
    \pi'_{\boldsymbol{y}} ({\boldsymbol{x}'}, t) = \sum_{{\boldsymbol{x}''}} \pi_{\boldsymbol{y}} \left( \begin{bmatrix} \boldsymbol{x}' \\ \boldsymbol{x}'' \end{bmatrix}, t \right).
\end{equation}
However, estimating $\pi_{\boldsymbol{y}}$ entails the curse of dimensionality and significant computational cost for solving the high-dimensional systems \eqref{eq:filtering_equation_pi} and \eqref{eq:filtering_equation_pi_jump}. Therefore, we aim to exclude species from $\boldsymbol{X}''$ and solve the filtering problem only for the $d'$-dimensional process
$$
    \boldsymbol{Z}'(t) := 
    \begin{bmatrix}
        \boldsymbol{X}'(t) \\
        \boldsymbol{Y}(t)
    \end{bmatrix} .
$$
This process is coupled with the $d''$-dimensional process $\boldsymbol{Z}''(t) := \boldsymbol{X}''(t)$, complicating the analysis of $\boldsymbol{Z}'$ as a separate process. The overall splitting of the process $\boldsymbol{Z}$ is summarized in the diagram:

\begin{center}
\begin{tikzpicture}[node distance=3cm, auto]
    \node (Z) [align=left]{$\boldsymbol{Z} =
        \begin{bmatrix}  
            \boldsymbol{X}' \\ 
            \boldsymbol{X}'' \\ 
            \boldsymbol{Y} \\ 
        \end{bmatrix}$};
    \node (Z') [align=left, right of=Z] {$\begin{bmatrix}
            \boldsymbol{X}' \\
            \boldsymbol{Y}
        \end{bmatrix}
        = \boldsymbol{Z}'$};
    \node (Z'') [align=left, below of=Z', node distance=1.5cm] {$\phantom{'}\begin{bmatrix} \boldsymbol{X}'' \end{bmatrix} = \boldsymbol{Z}''$};
    
    \draw[->, thick] (Z.east) -- (Z'.west);
    \draw[->, thick] (Z.east) -- (Z''.west);

\end{tikzpicture}
\end{center}

To clarify the difficulty of treating $\boldsymbol{Z}'$ as a distinct process separate from $\boldsymbol{Z}''$, we consider the random time change representation \eqref{eq:RTC_representation} for it:
\begin{equation}
\label{eq:RTC_splited}
\begin{aligned}
    \boldsymbol{Z}'(t)  &= \boldsymbol{Z}'(0) + \sum_{j=1}^{J} R_j \left( \int_0^t  a_j \left( \begin{bmatrix} \boldsymbol{Z}'(t) \\ \boldsymbol{Z}''(t)\end{bmatrix} \right) \diff s \right) \cdot \boldsymbol{\nu}_j' , \\ 
    \boldsymbol{Z}''(t)  &= \boldsymbol{Z}''(0) + \sum_{j=1}^{J} R_j \left( \int_0^t  a_j \left( \begin{bmatrix} \boldsymbol{Z}'(t) \\ \boldsymbol{Z}''(t)\end{bmatrix} \right) \diff s \right) \cdot \boldsymbol{\nu}_j'' , 
\end{aligned}
\end{equation}
where $\boldsymbol{\nu}_j'$ and $\boldsymbol{\nu}_j''$ are the corresponding parts of the stoichiometric vector $\boldsymbol{\nu}_j$. The process $\boldsymbol{Z}'$ can be considered as an \ac{SRN} with fewer species, but its propensities are random (due to $\boldsymbol{Z}''(t)$). Moreover, $\boldsymbol{Z}'$ is not a Markov process because $\boldsymbol{Z}''(t)$ depends on the past states of $\boldsymbol{Z}'$, which prevents applying classical filtering approaches. 

The following two sections discuss constructing a $d'$-dimensional Markov process that mimics $\boldsymbol{Z}'$, enabling solving the marginal filtering problem more efficiently.


