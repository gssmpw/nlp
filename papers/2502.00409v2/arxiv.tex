\documentclass[12pt,a4paper]{article}

\usepackage[british]{babel}

\usepackage[a4paper,top=2cm,bottom=2cm,left=2.5cm,right=2.5cm,marginparwidth=1.75cm]{geometry}
\usepackage[style=apa, backend=biber]{biblatex} % APA 7th edition style citations using biblatex
\addbibresource{arxiv.bib} % Your .bib file

\DeclareLanguageMapping{british}{british-apa} % Set language mapping
\DeclareFieldFormat[article]{volume}{\apanum{#1}} % Format volume number

\renewcommand*{\finalnamedelim}{%
 \ifnumgreater{\value{liststop}}{2}{\finalandcomma}{}%
 \addspace\&\space}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{hyperref}
\usepackage[title]{appendix}
\usepackage{mathrsfs}
\usepackage{amsfonts}
\usepackage{booktabs} 
\usepackage{caption} 
\usepackage{threeparttable}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{listings}
\usepackage{pdflscape}
\usepackage{enumitem}
\usepackage{chngcntr}
\usepackage{booktabs}
\usepackage{subcaption}
\usepackage{authblk}
\usepackage[T1]{fontenc} 
\usepackage{csquotes} 
\usepackage{diagbox}
\usepackage{varwidth}
\usepackage{csquotes}
\usepackage{tabularray}
\usepackage{hyperref}
\usepackage{courier}

\let\oldFootnote\footnote
\newcommand\nextToken\relax

\renewcommand\footnote[1]{%
  \oldFootnote{#1}\futurelet\nextToken\isFootnote}

\newcommand\isFootnote{%
  \ifx\footnote\nextToken\textsuperscript{,}\fi}

\usepackage{setspace}
\onehalfspacing

\usepackage{titlesec}
\titleformat{\section} 
 {\normalfont\Large\bfseries}{\thesection.}{1em}{}

\graphicspath{ {} }

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand{\subsubsubsection}[1]{%
 \vspace{\baselineskip}% Add some space
 \noindent\textbf{#1\\}\quad% Adjust formatting as needed
}

\usepackage{float}  % for customizing caption position
\usepackage{caption} % for customizing caption format
\captionsetup[table]{position=top} % caption position for tables

\makeatletter
\newenvironment{unlist}{%
 \begin{list}{}{%
  \setlength{\labelwidth}{0pt}%
  \setlength{\labelsep}{0pt}%
  \setlength{\leftmargin}{2em}%
  \setlength{\itemindent}{-2em}%
  \setlength{\topsep}{\medskipamount}%
  \setlength{\itemsep}{3pt}%
 }%
}{%
 \end{list}%
}
\makeatother

\pdfsuppresswarningpagegroup=1

\newcolumntype{M}{>{\begin{varwidth}{4cm}}l<{\end{varwidth}}} %M is for Maximal column



\lstdefinestyle{mystyle}{
  backgroundcolor=\color{backcolour},  
  commentstyle=\color{codegreen},
  keywordstyle=\color{magenta},
  numberstyle=\tiny\color{codegray},
  stringstyle=\color{codepurple},
  basicstyle=\ttfamily\footnotesize,
  breakatwhitespace=false,     
  breaklines=true,         
  captionpos=b,          
  keepspaces=true,         
  numbers=left,          
  numbersep=5pt,         
  showspaces=false,        
  showstringspaces=false,
  showtabs=false,         
  tabsize=2
}

\lstset{style=mystyle}


%-------------------------------------------
% Paper Head
%-------------------------------------------
\title{\textbf{Doing More with Less -- Implementing Routing Strategies in Large Language Model-Based Systems: An Extended Survey}}

\author[1, 2, *]{Clovis Varangot-Reille}
\author[1]{Christophe Bouvard}
\author[2]{Antoine Gourru}
\author[1]{Mathieu Ciancone}
\author[1]{Marion Schaeffer}
\author[2]{FranÃ§ois Jacquenet}
\affil[1]{\small Wikit, Lyon, France}
\affil[2]{\small Laboratoire Hubert Curien, UMR CNRS 5516, Saint-Etienne, France}
\affil[*]{\small \textit{Corresponding author: Clovis Varangot-Reille,}  {\fontfamily{pcr}\selectfont\texttt{clovis.varangot\{wikit.ai\}}}}

\begin{document}
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\date{}
\maketitle

\begin{abstract}

Large Language Models (LLM)-based systems, i.e. interconnected elements that include an LLM as a central component (e.g., conversational agents), are typically monolithic static architectures that rely on a single LLM for all user queries. However, they often require different preprocessing strategies, levels of reasoning, or knowledge. Generalist LLMs (e.g. GPT-4) trained on very large multi-topic corpora can perform well in a variety of tasks. They require significant financial, energy, and hardware resources that may not be justified for basic tasks. This implies potentially investing in unnecessary costs for a given query. To overcome this problem, a routing mechanism routes user queries to the most suitable components, such as smaller LLMs or experts in specific topics. This approach may improve response quality while minimising costs. Routing can be expanded to other components of the conversational agent architecture, such as the selection of optimal embedding strategies. This paper explores key considerations for integrating routing into LLM-based systems, focusing on resource management, cost definition, and strategy selection. Our main contributions include a formalisation of the problem, a novel taxonomy of existing approaches emphasising relevance and resource efficiency, and a comparative analysis of these strategies in relation to industry practices. Finally, we identify critical challenges and directions for future research.

\end{abstract}

\textbf{Keywords}: \textit{Routing, Large Language Model, Optimisation, Cost, Survey}

%-------------------------------------------
% Paper Body
%-------------------------------------------

\section{Background}\label{background}
In the domain of computer network, a \textbf{router} can be defined as ``\textit{a device that sends data to the appropriate parts of a computer network}''. In LLM-based systems, a router is a system component that routes an item (i.e. user query) to the most appropriate element from a pool of elements candidates to carry out a task. \\ This paper focuses on one of the most common applications of LLM-based systems: \emph{ conversational agents} \parencite{dam2024}. Conversational agents respond to user queries by simulating human conversation \parencite{caldarini2022, lin2023}. They process the user's query and provide an answer that is relevant, given some metrics, to it \parencite{caldarini2022}. The Retrieval-Augmented Generation (RAG) architecture further improves the relevance of the response by adding an information retrieval step to the process \parencite{Lewis2020rag, gao2023}. This step involves retrieving the information most relevant to the user query from a knowledge database and including it into the prompt alongside the user query (see Figure \ref{fig:fig1} for a RAG architecture schema). We can adapt all RAG steps to the user's query by routing it to the most appropriate element at each step (Figure \ref{fig:fig1}). A router allows the whole system to dynamically adapt to different input types \parencite{wang2024}. \\

Given a set of $n$ models $\mathcal{M} = \{M_1,...,M_n\}$, for a given query $q$, the router function $\mathcal{R}$ aims to maximise the scoring function $s$ (e.g. \emph{accuracy}) while adhering to a budget constraint $B$: 
 \begin{equation}\label{eq1}
 \begin{aligned}
    \mathcal{R}_{\mathcal{M}}(q) =&\argmax_{M \in \mathcal{M}} s(q,M)\\
   &\text{s.t. } C_{M}(q) \le B
 \end{aligned}
 \end{equation}

where $C_{M}$ is the cost (i.e, $\$/token$) to call the model $M$ for a query $q$ and $B$ is the user's budget. The budget could be the amount of resources available.\\

\begin{figure*}[!t]
  \centering
  \includegraphics[width=\textwidth]{figure1v2.PNG}
  \caption{\textbf{DynamicRAG: the RAG architecture as an user query-dependent dynamic system} - We propose an architecture that views each step of the RAG framework as a routing opportunity. This approach redefines the architecture of the RAG framework and transforms it into a user-centric system.}
  \label{fig:fig1}
\end{figure*}

A straightforward optimisation of conversational agent architectures is the selection of the pre-trained LLM which has the ability to answer the user's query. Various LLMs exist that vary greatly in terms of number of parameters, such as \textit{Llama-3.2-3B} and \textit{Llama-3.1-405B}, as well as models specialised on distinct domains, like \textit{mathstral-7B-v0.1}\footnote{\href{https://mistral.ai/fr/news/mathstral/}{MistralAI's Mathstral}} for mathematical tasks or \textit{Med-Palm}\footnote{\href{https://sites.research.google/med-palm/}{Google's Med-Palm}} for medical-related tasks. However, most LLM-based systems rely on a single generalist LLM to respond to all user queries. By routing each query to the smallest possible LLM that has the ability to answer correctly to a user query, we can expect to improve the quality of the response by leveraging specialised expertise and optimise costs, as not all tasks require a large LLM. The router allows maximising performance while avoiding the cost of using models that are oversized, have insufficient reasoning capacity, or lack the knowledge needed to provide desired output. \\

This survey evaluates routing strategies that select the most appropriate element to solve a subtask within an LLM-based system, focusing in particular on the generation step by selecting the appropriate LLM for generation. We exclude approaches that identify the answer closest to the truth among all generated candidate responses \parencite{guha2024, si2023} as well as ensemble methods that combine multiple answers from various candidates to create a meta-answer \parencite{jiang2023, wang2024fusing, hu2024_der}. Although these approaches demonstrate effectiveness, they focus solely on maximising performance without addressing cost constraints. Consequently, they do not align with our definition of routing.\\

This survey is structured into several sections that focus on the critical aspects of routing. We begin by describing the essential elements of routing. Next, we examine the pipeline stage at which routing is implemented in the literature. Finally, we detail and classify routing strategies according to the frameworks used and the resources required. We discuss these strategies considering industrial practices and highlight the key challenges that the field must address.

\section{Which elements should be optimised for routing?}\label{which-section}

The primary objectives of routing are to minimise unnecessary resource consumption while maximising performance by using a model or element appropriate to the task. In other words, it seeks to optimise the performance - cost trade-off.


\subsection{A cost to minimise}\label{cost-section}
Most existing LLM-based systems depend on API calls to closed models, such as those provided by OpenAI. The primary cost to minimise for a router is the price per token. To calculate the total cost of running the pipeline, it is essential to consider all invoked pre-trained models, including LLMs and embedding models in RAG settings.\\

Other production-related costs, such as average latency and computational costs, can also be considered \cite{irugalbandara2024}. Latency is measured with respect to the delay between the user's request and the pipeline response. Effective routing could reduce the average latency by routing simple user queries, such as greetings, to LLMs that require fewer computing resources.\\
As LLM increases in use and size, the power and computing requirements increase significantly \cite{Luccioni2024}, increasing the ecological impacts associated with LLM-based systems. This impact is often measured considering the energy consumption (kWh) or the global warming potential (kgCO2eq) of the application. Various tools\footnote{\href{https://ecologits.ai/latest/}{Ecologits}, \href{https://github.com/mlco2/codecarbon}{CodeCarbon}, \href{https://mlco2.github.io/impact/}{MLCO2}, \href{https://github.com/Boavizta/boaviztapi}{Boavizta}} have been proposed to estimate this environmental footprint.

\subsection{A performance metric to maximise}\label{performance-section}

The router function is also to maximise a scoring function that evaluates the model ability to produce accurate answers, as explained in equation \ref{eq1}.\\

There are several scoring function possibilities to maximise. In a traditional supervised learning framework, the evaluation process involves comparing these generated answers with the ground truth. In cases where the data lack ground truth or annotation, including a human evaluator in the evaluation loop allows us to assess whether the response is factually correct, in the expected format and consistent with the expected ground truth \cite{chang2024acm}. However, evaluating thousands of queries can require significant time and intense human involvement, affecting scalability. In addition, subjective bias might appear while evaluating, such as lack of expertise or preferences. More straightforward strategies include exact matching, partial matching, ROUGE score \cite{lin-2004-rouge}, or even semantic similarity to the ground truth \cite{chang2024acm, zhang2024}. These metrics may not adequately assess the factual accuracy of the generated content. As a result, previous studies have investigated automatic evaluation methods involving an LLM, where instructions and rating criteria are provided within the prompt along with the generated response \cite{chiang2023}. Their alignment with human evaluation remains uncertain, and factors such as instructions and sentence structure play an important role in the generated rating \cite{wei2024}. Many frameworks have been proposed, such as Retrieval Augmented Generation Assessment, also known as RAGAS \cite{es2023}. \\
Preference learning, based on the preferences of human reviewers, can be used to assess the quality of responses in addition to traditional methods \cite{jiang2024pref}. Data related to preferences are typically represented as \( \lambda_i \prec_x \lambda_j \), indicating that for a given query \( x \), the user prefers the generated answer \( \lambda_i \) over the alternative \( \lambda_j \) \cite{FÃ¼rnkranz2016}.


\section{When should routing take place?}\label{when-section}
We propose two stages in the pipeline for the routing process: \emph{pre-generation routing} (see Figure \ref{fig:fig2}) and \emph{post-generation routing} (also known as \emph{cascade routing}) (see Figure \ref{fig:fig3}). Pre-generation routing takes place before generating a response to the user query, while post-generation routing takes place after generating the response.

\subsection{Routing as a Pre-generation Step}\label{Routing as a Pre-generation Step}

\begin{figure}[h]
  \centering
  \includegraphics[width=1.1\textwidth]{figure2v3.png}
  \caption{\textbf{Routing as pre-generation step} -- Before generating an answer, each LLM ability to provide an appropriate answer is assessed based on the complexity and/or topic of the user's query. \textit{Dotted arrows represent non-selected LLM candidates.}}
  \label{fig:fig2}
\end{figure}

To implement \textbf{pre-generation routing}, we need to infer the ability of the LLM to answer a query \emph{a priori} (see Figure \ref{fig:fig2}). This method minimises latency by not waiting for the LLM response. There are two main approaches to achieve this: 1) infer the domain of knowledge of the query and route it to the associated LLMs trained as domain experts; and 2) assess the LLM candidate ability to answer a query of a given complexity and then route the query to the LLM with sufficient reasoning ability.

In this survey, we define the complexity of a query as the predicted performance score of the LLM for that specific query. The lower the score, the more complex the query. Within the context of RAG-based conversational agents, complexity can be categorised as follows: (a) \textit{low complexity user query} may consist of a simple greeting that does not require retrieval; (b) \textit{intermediate complexity user query} might involve extracting explicit information from a single document; (c) \textit{high complexity user query} may necessitate the extraction of implicit information from multiple documents through reasoning.

\subsection{Cascading: Routing as a Post-Generation Step}\label{Cascading: Routing as a Post-Generation Step}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.7\textwidth]{figure3v2.png}
  \caption{\textbf{Routing as post-generation step (or cascade routing)} -- The relevance of the use of a larger LLM is determined by the evaluation of the answers generated by the current LLM. Each candidate response is evaluated sequentially. If an answer is deemed inadequate or untrustworthy, the user query is routed to a larger LLM. Typically, the cascade sequence is static.}
  \label{fig:fig3}
\end{figure}

One might conceptualise routing as a \textbf{post-generation step}, where each model response is assessed iteratively (or in a cascade manner) by selecting progressively more advanced models until the response is considered pertinent (see Figure \ref{fig:fig3}). In other words, the challenge is no longer to infer the ability of a potential LLM to meet a demand, but to assess the quality of the generated current response. By definition, this approach is less optimal than generating a single response with the pre-generation approach because, in some cases, several answers will be generated for the same query. This entails financial, computational, and latency costs.\\

The process can be enhanced by hybridising it with the pre-generation step \parencite{dekoninck2024}. We consider it as a \textbf{multi-choice cascading} method within the post-generation category.The evaluation of the response determines whether it should be routed to another LLM, similar to a post-generation approach. However, instead of routing the query to the next LLM in the predefined cascade sequence (as shown in Figure \ref{fig:fig3}), the query can be routed to any available model within the set of LLMs at any stage of the cascade. This hybrid process outperforms simple sequential cascading or supervised pre-generation approaches \parencite{dekoninck2024}, but still requires multiple generations per query.

\section{How routing should be implemented?}\label{how-section}

Figure \ref{fig:fig4} represents the different techniques discussed in this survey. The strategies are divided into high- and low-resource strategies. We define a \textbf{high-resource strategy} as one that (1) \emph{possibly generates multiple full-sequence responses to the same user query} and/or (2) \emph{uses an LLM with an arbitrary threshold value chosen as a 1B parameter only for the routing process}. 

\begin{figure}[h]
  \noindent
  \hspace*{-2cm}
  \includegraphics[width=1.1\textwidth]{figure4v7.PNG}
  \caption{\textbf{Overview of routing strategies} -- Routing strategies are classified according to the resources they require and whether routing occurs before or after the generation step.\textit{ RL: Reinforcement Learning.}}
  \label{fig:fig4}
\end{figure}

\subsection{High-Resource Strategies}\label{high-resource}

\subsubsection{Supervised Routing}\label{high-supervised}
\subsubsubsection{Answer confidence inference}
In a cascading approach, at each iteration, a model, such as a fine-tuned \textit{DistilBERT} \parencite{sanh2020distilbert}, classifies whether the generated answer aligns with the reference answer \parencite{chen2023}. \textcite{chen2023} proposed FrugalGPT\footnote{\href{https://github.com/stanford-futuredata/Frugalgpt}{stanford-futuredata/Frugalgpt}}.  This method infers the probability that the generated answer aligns with the reference answer using a \textit{DistilBERT} regression model \parencite{sanh2020distilbert},  and compares this probability to an optimised threshold. The process of learning the threshold, denoted as $\tau_{i}$ for each model, is framed as a constrained optimisation problem. If the score exceeds the threshold, the response generated by $M_{i}$ is retained; otherwise, a larger model,$M_{i+1}$, is called.  They used 12 LLM APIs from 5 providers (OpenAI, AI21, Cohere, ForeFrontAI, Textsynth) with 10M input tokens cost ranging from \$0.2 (\textit{Textsynth's GPT-J}) to \$30 (\textit{GPT-4}). The authors reported that their framework could save between 59\% and 98\% of costs while maintaining similar accuracy to larger models, such as \textit{GPT-4}. This strategy has often been used for comparisons, but it often underperforms against alternatives, such as LLM-based repeated calls routing \parencite{aggarwal2024} and graph-based supervised routing \parencite{feng2024}. Moreover, even lower-resource LLM-based strategies, such as the token probability method \parencite{ramirez2024}, match its efficacy.

\subsubsubsection{Decoder-only encoding}
\textcite{mohammadshahi2024} proposed the \textit{Routoo Orchestrator}\footnote{\href{https://github.com/Leeroo-AI/leeroo_orchestrator}{Leeroo-AI/leeroo\_orchestrator}}, formerly known as the \textit{Leero Orchestrator}. Their strategy differs from the \textit{\textbf{query complexity inference}} approach in that they use a decoder-only LLM to encode the query for supervised training instead of relying on bidirectional language models (i.e., BERT models \parencite{devlin2019}). They extract the representation of the predefined last token, typically '<\textbackslash s>' or '<EOS>', as these models are autoregressive and process sentences from left to right. This approach has demonstrated promising performance, with top-ranked models on the MTEB leaderboard being decoder-only embedding models such as Nvidia's \textit{NV-Embed-v2} \parencite{lee2024nvembed} and BAAI's \textit{bge-en-icl }\parencite{li2024makingtextembeddersfewshot}. However, due to their larger size, these models require significant computing resources. Decoder-only models are typically fine-tuned for dense retrieval \parencite{ma2024, wang2024improving-text, li2024llama2vec, li2024makingtextembeddersfewshot, lee2024nvembed, muennighoff2024}. It remains unclear whether the \textit{Routoo Orchestrator} employs a non-fine-tuned \textit{Mistral-7B} model \parencite{jiang2023mistral7b} or has fine-tuned it using the LoRA  method \parencite{hu2022lora}. Once the input sequence is embedded by the decoder-only model, it is processed through a linear layer alongside an embedding representation of the LLM. The objective is to determine the expected quality score for an LLM \( m \) when generating an answer to a specific query \( q \). The performance inference model is trained by minimising the cross-entropy loss. During inference, we select the model that maximises the inferred evaluation score, while ensuring that the inference cost \( c_m \) remains within a predefined budget.  The researchers optimise the selection of models by identifying the subset of LLMs that achieve the highest score for a given set of queries, in order to ensure complementary LLMs for routing. In initial experiments using only open-source models, 56 LLM with parameters of 7 billion, 13 billion, and 34 billion, demonstrated superior accuracy on the MMLU dataset compared to a larger stand-alone LLMs such as \textit{Llama2-70B} \parencite{touvron2023} (75.9\% vs 69.9\%) and \textit{Mixtral-8x7B} \parencite{jiang2024mixtralexperts}(75.9\% vs 70.6\%). Notably, these models achieved this accuracy at similar or lower costs: \textit{Routoo} costs \$0.6 per million tokens, while \textit{Llama2-70B} costs \$0.9 per million tokens, and \textit{Mixtral-8x7B} is priced at \$0.6 per million tokens. When \textit{GPT-4-turbo} was included as a routing option, its performance was closely matched (84.9\% vs 86.4\%) at half the cost (\$10.2 vs \$20 per million tokens), as it was used for approximately 50\% of the queries.

\subsubsection{Generative-based routing}\label{high-llm}
This class of strategy uses a generative approach specifically for routing. Generative-based routing can potentially take advantage of LLM emergent capability for unsupervised multitasking to generalise to new contexts \parencite{radford2019language, wei2022emergent}.

\subsubsubsection{Prompt-based routing}
It can be used as either a pre-generation approach or as a post-generation approach. 

In a pre-generation approach, the simplest method involves using a pre-trained LLM with prompt-based routing, also known as "function calling".  This process entails passing descriptions of the routing options, with or without examples, in the prompt along with the user query.  We route to the option returned by the LLM \parencite{ning2024, shen2023}. Although it leverages an LLM, it is more energy-efficient and requires fewer resources than fine-tuning an LLM.\textcite{ning2024}\footnote{\href{https://github.com/imagination-research/sot}{imagination-research/sot}} prompt \textit{ GPT-4} to determine whether a query possesses the necessary characteristics to be addressed by their prompt technique (SoT). Despite using a large LLM, the absence of examples affects inference. Furthermore, this strategy did not surpass a small language model, \textit{RoBERTa} \parencite{zhuang2021}, specifically trained for task classification. Similarly, \textcite{shen2023} proposed \textit{HuggingGPT}\footnote{\href{https://github.com/microsoft/JARVIS}{microsoft/JARVIS}}, a framework for task planning and execution \parencite{shen2023}. An LLM is prompted to select between different models based on their descriptions.\\ This approach, despite its limitations, offers the advantage of performing inference with minimal examples in the prompt, known as ``few-shot inference'', or without any examples at all, known as ``zero-shot inference''. This is particularly useful when limited resources are available for annotation.

In a post-generation approach, the LLM can be prompted to express its uncertainty when responding to user queries \parencite{li2024}. This is known as ``verbalised confidence'' \parencite{xiong2024}. \textcite{li2024} proposed \textit{Self-Route} to decide whether retrieving traditional chunks using a RAG strategy is sufficient. Otherwise, they suggest using the entire document from which the chunks were extracted. This is an example of how routing can improve another step of a LLM-based conversational agent. The model was instructed to indicate whether the query is answerable based on the retrieved chunks. They used the prompt: "\textit{Write 'unanswerable'}" if the query cannot be answered based on the text. They ran a new generation with a larger context if the question was considered unanswerable.  Their findings reveal that implementing this strategy outperformed a naive RAG system in terms of accuracy for the most commonly used LLMs (i.e. \textit{GPT-4o} and \textit{GPT-3.5-turbo}). For \textit{GPT-3.5}, which has a smaller context window (16k), it also outperformed sending large contexts. In contrast, performance between \textit{Self-Route} and sending a large context proved comparable for\textit{GPT-4o} which has a larger context window (128k) while using an average of 61\% fewer tokens. Finally, for \textit{gemini-1.5-pro}, which features a context window of 1M, sending only large contexts appears to perform better than \textit{Self-Route}, although  the latter's performance remains reasonably close. The results for \textit{gemini-1.5-pro} stem from its large context window, which allows the transmission of extensive contexts without truncation. Given these results, it may be tempting to send only large contexts. The authors show how it is possible to save costs while achieving comparable performance with fewer tokens.\\ However, it has been reported that LLMs are often too confident in expressing their certainty\parencite{xiong2024}, suggesting caution about the reported effectiveness of this method. Furthermore, verbal confidence has been shown to be, in the best case, comparable to random routing to a larger LLM and could even perform worse \parencite{chuang2024}.

\subsubsubsection{Sequence probability}
\textcite{lee2024} proposed using the normalised sequence-level probability of a smaller LLM when determining whether to route a query to a larger LLM. The routing is based on the sequence uncertainty. However, this approach tends to rely too heavily on the larger LLM, resulting in an efficacy comparable to that of the larger LLM alone.  Consequently, this approach is less effective in reducing call frequency to the larger LLM than the supervised methods discussed earlier.

\subsubsubsection{LLM fine-tuning}
If numerous resources are available, an LLM can be fine-tuned for routing tasks like classification or regression task \parencite{liu2024meswitch, ong2024}, or for code generation to use providers API \parencite{patil2023}. Routing can be achieved by fine-tuning an LLM to add new routing-related tokens to its vocabulary: uncertainty-related tokens \parencite{chuang2024} or token identifiers for domain experts \parencite{chai2024}. The LLM can also be fine-tuned for domain classification \parencite{liu2024meswitch}, but adding new domains will require a retraining of the model.\\
\textcite{liu2024meswitch} suggested using \textit{Qwen1.5-1.8B-Chat} \parencite{bai2023qwentechnicalreport}, which they fine-tuned for a domain-classification task\footnote{\href{https://github.com/godcherry/ExpertTokenRouting}{godcherry/ExpertTokenRouting}}. This meta-model categorises the prompt, and the corresponding pre-trained expert associated with that category then generates a response.  Similarly, \textcite{ong2024} fine-tuned a \textit{Llama-3-8B model} \parencite{grattafiori2024llama3herdmodels} on a scoring task designed to evaluate both the complexity of a query and the model's ability to answer it through LLM evaluation  \parencite{ong2024}. This score is subsequently converted into the probability that the user has a preference for the larger model answer. While \textcite{ong2024} found that this strategy is not significantly more efficient regarding cost or quality compared to other non-LLM techniques, such as matrix factorisation, \textcite{liu2024meswitch} demonstrated a marked improvement in performance across all datasets through supervised fine-tuning. The accuracy increased from 15\% to nearly 100\% for the MMLU dataset \parencite{liu2024meswitch}. The authors' differing levels of optimism regarding the fine-tuning of an LLM for classification or regression tasks may stem from \textcite{ong2024} comparing this technique with other optimised approaches, while \textcite{liu2024meswitch} evaluated it against the same standalone LLM without fine-tuning.\\
\textcite{patil2023} approached the problem as a code generation task.\footnote{\href{https://github.com/ShishirPatil/gorilla}{ShishirPatil/gorilla}} They fine-tuned a \textit{Llama-7B model} to direct queries to the appropriate model via API calls. To achieve this, they undertook the following steps: first, they created a dataset of API calls to model libraries, including Torch Hub, TensorFlow Hub v2, and HuggingFace. Second, they generated instructions to call a specific API based on in-context examples and the API documentation, utilising \textit{GPT-4} in accordance with the Self-Instruct paradigm \parencite{wang2023}. To enhance the LLM's ability to utilise the retrieved information in a RAG context, they incorporated the API documentation for a specific model into the training instructions. Finally, they trained a \textit{Llama-7B-based} model to generate the API call code.  The model can perform zero-shot inference without documentation retrieval or RAG-based inference with the top-1 documentation retrieved. In a zero-shot context, without fine-tuning, \textit{Llama-7B} fails to accomplish the task, scoring O\%. It outperforms larger models after fine-tuning, achieving an average improvement of 35 points over \textit{GPT-3.5-turbo-0301} and 46 points over \textit{GPT-4-0314}.

\subsubsubsection{Repeated calls}
In a post-generation approach, we could route to the next model whenever the uncertainty of the LLM for its answer is high. Smaller LLMs tend to give consistent answers to simple questions but show inconsistencies when confronted with more complex questions \parencite{yue2024}. Thus, the uncertainty of the model can be assessed by repeatedly querying the LLM with the same prompt, using temperatures ranging from 0.4 \parencite{yue2024} to 1 \parencite{aggarwal2024}. By analysing the consistency of the responses, researchers can assess the level of uncertainty \parencite{aggarwal2024, yue2024}.  \textcite{aggarwal2024} introduced a three-step approach called \textit{Automix}\footnote{\href{https://github.com/automix-llm/automix}{automix-llm/automix}} \parencite{aggarwal2024}. The models used by the authors were \textit{GPT-3.5}, \textit{Llama-2-13B} \parencite{touvron2023}, and \textit{Mistral-7B-Instruct-v0.2} \parencite{jiang2023mistral7b}, as the smaller language models, and \textit{GPT-4} as the larger language model. Given a set of $N$ unique models with increasing size $\mathcal{M} = \{M_{1},...,M_{N}\}$: (1) using the smaller model $M_{j}$ (j=1) to generate an answer $A_{i,j}$ to a query $q_i$ based on a related context $C_{i}$; (2) initiating a self-verification process with the same model $M_{i}$ with a few-shot meta-prompt (\textit{verification prompt}) to determine whether $A_{i,j}$ aligns with the provided context $C_{i}$. To estimate a confidence score of $M_{j}$ aligning with $C_{i}$, they produced $A_{i,j}$ $k$ times ($A^k_{i,j}$, k > 1) at a high temperature and calculated the proportion of $A^k_{i,j}$ aligning with $C_{i}$; (3) based on this confidence score, the router either retained the current answer $A_{i}$ or call a larger model $M_{j}$ (j>i). This procedure repeats until the confidence score reaches a satisfactory level or the entire set of LLM $\mathcal{M}$ series has been tested. The authors employed two routing strategies: a more complex one based on a Partially Observable Markov Decision Process (POMDP) and a simpler one grounded in a confidence cost/quality trade-off threshold.\\
On the other hand, \textcite{yue2024} did not assess the correctness of the answers finding this concept too challenging to evaluate. Instead, they scored the responses based on the ratio of identical answers among $k$ samples for the same query, which reflects answer consistency\footnote{\href{https://github.com/MurongYue/LLM_MoT_cascade}{MurongYue/LLM\_MoT\_cascade}}. They assessed response consistency across different representations of thought by comparing responses generated by Chain-of-Thought (CoT) and Programme-of-Thought (PoT) prompts, with further details provided in Appendix \ref{annex1}.   The \textit{Mixture-of-Thoughts} (MoT) representation employs both prompting strategies by mixing samples from CoT and PoT. For each query, they generated $k$ samples for each prompting approach (PoT, CoT, or MoT). The algorithm then calculates a consistency score, which is defined as the proportion of identical answers among $k$ samples).\\

The authors reported that \textit{Automix} \parencite{aggarwal2024} achieves a higher F1 score on the QASPER and COQA datasets and superior accuracy across a range of costs compared to \textit{HybridLLM} \parencite{ding2024}, \textit{FrugalGPT}, \parencite{chen2023} or stand-alone models such as \textit{GPT-4} and \textit{Llama-2-13B} \parencite{touvron2023}, particularly the routing based on the POMDP approach. Even in low-resource scenarios with a small training dataset size, their method significantly outperforms both \textit{HybridLLM} \parencite{ding2024} and \textit{FrugalGPT} \parencite{chen2023}. These findings are supported by \textcite{yue2024}, who demonstrated that including various thought-based reasoning (MoT) when deciding whether to call a larger LLM resulted in similar accuracy at lower cost compared to assessing consistency between CoT or PoT samples alone \parencite{yue2024}. This underlines the need for diversity and complementarity between routing options. In both studies, users have to call an LLM multiple times for each query, and despite their efficiency, these approaches prove to be resource-intensive.

\subsubsubsection{Code execution}
This strategy applies exclusively to code generation tasks. It directly evaluates the confidence in the answer by executing the generated code. \textit{EcoAssistant}\footnote{\href{https://github.com/JieyuZ2/EcoAssistant}{JieyuZ2/EcoAssistant}}, an iterative multi-agent code generator designed to query external knowledge for question and answering, has been proposed by \textcite{zhang2023}. In this generator, an LLM interacts with a code executor to generate and execute code. If the code generated by the smaller LLMs does not succeed, the request will be forwarded to a larger LLM. Here, success, rather than uncertainty, serves to guide routing.  To determine the correctness of the generated code for a given query, the authors implemented an evaluation using \textit{GPT-4} alongside the success of code execution. They used two architectures: \textit{GPT-3.5-turbo} + \textit{GPT-4} and \textit{Llama-2-13B}-chat \parencite{touvron2023} + \textit{GPT-3.5-turbo} + \textit{GPT-4}. The authors reported that their framework generated more successful code snippets at a lower cost than using \textit{GPT-4}. Although more expensive than using \textit{GPT-3.5-turbo}, \textit{EcoAssistant} significantly exceeded the percentage of successful code generation..\\

In all the high-resources routing approaches discussed, the savings from the routing process may be outweighed by the implementation requirements of high-resource routing strategies.

\subsection{Low-Resource Strategies}\label{low-resource}

The drawbacks of the routing strategies presented in the previous section can be addressed using low-resource strategies. They attempt to avoid such resource requirements while maintaining routing performance. Most of them are pre-generation, except the last one.

\subsubsection{Similarity-based Routing}\label{low-similarity}
\subsubsubsection{Query similarity}
The primary concept behind these approaches is to route a user's query to the LLM that has the best performance on similar queries answered in previous interactions (e.g. cosine similarity).\\
In its simplest form, the approach routes the user's query to the elements that have successfully responded to the $n$ most similar previous queries, with the assumption that similar queries require similar processing. \textcite{stripelis2024} propose a 1NN router that routes the user query to the LLM identified as producing the most appropriate response for the most similar query in the training data, based on cosine similarity. However, this strategy fails to capture complex relationships between user queries and expert answers, and performs worse than randomly selecting from available models.
\textcite{manias2024} suggests \textit{Semantic-Router}, a framework developed by Aurelio AI\footnote{\href{https://github.com/aurelio-labs/semantic-router}{aurelio-labs/semantic-router}}, which uses $n$ examples. \textit{Semantic-Router} detects intentions by routing the user's requests to an intention that is associated with $n$ similar previous requests. The authors demonstrated that \textit{Semantic-Router} performed similarly to prompt-based intent detection, achieving around 90\% accuracy without requiring LLM inference.\\ Similarly, \textcite{jang2023} proposes a method that routes to different expert adapters based on the training tasks that are most similar to the user query. Each training task is embedded and associated with one or more expert IDs. During the inference process, they identify the most similar training tasks and the most frequently associated expert IDs from the training tasks most similar to the user's query. They showed improved performance on non-generative tasks by using a smaller language model (t5-3B \parencite{raffel2020}) compared to a fine-tuned multi-task t0-3B model \parencite{sanh2022multitask}. However, their framework showed inferior performance on generative tasks.\\\textcite{malekpour2024} applied this approach within a text-to-SQL framework by examining previous successes among similar SQL queries. They proposed selecting the cheapest LLM that yields a higher proportion of SQL generation matching the ground truth among the first $n$ most similar SQL queries. To ensure minimal performance, the authors implemented a threshold that represents the minimum proportion of queries a LLM must success to be considered. They utilised three LLMs as routing options: \textit{gpt-4o}, \textit{gpt-4o-mini}, and a quantised version of \textit{Llama3.1-8B-instruct} \parencite{grattafiori2024llama3herdmodels}.The authors demonstrated that they could achieve performance close to that of the best stand-alone model for this task (\textit{gpt-4o})â60.1\% compared to 61.0\%âat a lower cost (1.1 times cheaper). However, it is important to emphasise that most of the time, the strategy routed queries to the best LLM (\textit{gpt-4o}) -81\%. A routing strategy must not only minimise costs while maximising quality, but also avoid achieving higher quality by exclusively routing to the larger LLM.  In addition, the choice of LLM did not include a specialist code generation LLM, which could potentially have led to a cheaper option with similar performance. Despite these limitations, employing similarity retrieval based on previous successes or utterances effectively reduces costs while maintaining performance. These approaches require minimal resources.\\ 

The query representation can be improved by optimising the encoding process using contrastive learning. The optimisation ensures that similar samples are close together in the multidimensional semantic space, while dissimilar samples are far apart \parencite{LeKhac2020}. \\ 
\textcite{lee2024} applied it to Dialogue State Tracking (DST) by fine-tuning a \textit{SenBERT-based} bi-encoder \parencite{reimers2019} within a framework called \textit{OrchestraLLM}. DST is a task whose objective is to extract the user's intention and dialogue-related information in a structured representation (see Annex \ref{annex1} for an example). Some dialogues may be too complex to be handled with a smaller LLM, and the conversation processing may therefore require a larger LLM. The authors assigned a series of utterances to either the smallest or largest LLM, depending on which model could accurately generate the correct dialogue state representation. In each iteration, they selected the appropriate LLM  using a \textit{K-NN} algorithm applied to the embedding of the new instance. The majority vote from the k closest neighbours determined which LLM had been most successful in similar past examples. \textit{OrchestraLLM} shows a slight improvement in accuracy compared to a direct call to the larger LLM, while also reducing the number of calls to this model by 50\% to 80\%. Optimising the representation strategy with the contrastive loss improves the assignment ratio to a smaller model by 8 points. Alternatively, \textcite{chen2024routerdc} proposes a framework called \textit{RouterDC}, which improves user query representation through multi-level contrastive learning. They map user queries into a shared space, learning embeddings of both the LLM representation and the query. This multi-level loss consists of two parts. The first is the contrastive loss between the query and the LLM to evaluate positive/negative LLM sets based on their performance compared to the query. The second is the contrastive loss between similar queries to ensure they are located closer together in this space. Subsequently, they generate a selection probability distribution over the set of LLMs by applying a softmax function to the similarity between the user query and the learnable LLM representation embeddings. During inference, it selects the LLM that maximises this similarity probability. The options include a mix of generalist and expert LLMs: \textit{Mistral-7B} \parencite{jiang2023mistral7b}, \textit{MetaMath-Mistral-7B} \parencite{yu2024metamath}, \textit{zephyr-7b-beta} \parencite{tunstall2024zephyr},\textit{ Chinese-Mistral-7B, dolphin-2.6-mistral-7b}, \textit{Llama-3-8B} \parencite{grattafiori2024llama3herdmodels}, and \textit{dolphin-2.9-llama3-8b}.  Additionally, \textit{RouterDC} is compared to \textit{Zooter}, a strategy described in section \ref{low-reinforcement} \parencite{lu2024}, which employs a \textit{\textbf{Reward-based inference}} approach. It is also evaluated against a majority voting method, a multiclass classification model, and a clustering technique for embeddings.  Except for the MMLU dataset, \textit{RouterDC} outperforms the best stand-alone LLMs and performs as well as or better than other routing methods on different datasets. It achieves the highest average performance across out-of-distribution datasets by effectively approximating the results of the best performing models on each dataset.

\subsubsubsection{Clustering previous interactions}
Incorporating unsupervised learning can enhance similarity-based routing by identifying the relevant cluster associated with the user query. Once the closest cluster is determined, the query is directed to the LLM that has exhibited the best performance in previous interactions within that cluster. The two studies that proposed cluster-based routing use a \textit{ k-means} algorithm. \textcite{pichlmeier2024} proposed the \textit{Expert Router} framework, which employs TF-IDF encoding reduced to 100 dimensions through singular value decomposition. Using k-means is more energy efficient than using an LLM, but the researchers didn't train it from scratch. Instead, they used a pre-trained \textit{k-means} model trained on the C4 dataset \parencite{raffel2020} - a 300GB cleaned version of the Common Crawl web crawling corpus \parencite{gururangan2023}. While \textcite{pichlmeier2024} do not describe how they assign an LLM to each cluster, \textcite{srivatsa2024} evaluate which LLM has the most frequently generated answers that match the ground truth for each cluster derived from the training data set. The authors evaluated the framework's feasibility rather than its performance by testing latency, response time, and session throughput.  They demonstrated that the infrastructure remains robust under high-load scenarios. Their work underscores the potential necessity of training on large datasets to enable smaller algorithms to effectively capture the information contained in queries.\\
When \textcite{srivatsa2024} trained the K-means algorithm from scratch for the task, the clusters did not generalise effectively from the training dataset to the test dataset. The size and diversity of the training data set appear to be key factors when training clustering methods for routing. The authors did not observe any impact on the results from altering the encoding strategy, whether using a dense representation strategy with \textit{RoBERTa} \parencite{zhuang2021} or a sparse strategy with \textit{TF-IDF}.
Another significant point to highlight in this latest study is the selection of LLM for routing: the researchers did not compare a smaller LLM with a substantially larger one, using only smaller models such as \textit{gemma-7b and  gemma-7b-it} \parencite{gemmateam2024}, \textit{metamath-7b }\parencite{yu2024metamath}, \textit{mistral-7b-it} \parencite{jiang2023mistral7b}, \textit{llama-2-13B-chat and llama-2-7b} \parencite{touvron2023}. The study did not involve topic experts, such as those in mathematical tasks, and relied on non-specialist models. Consequently, the clusters formed lacked sufficient distinctiveness to differentiate between the LLMs; often, the LLM assigned to a cluster was also the best-performing model across the dataset. Therefore, selecting complementary models based on topic or parameter values is essential for effective routing in the context of LLMs.

\subsubsubsection{Preference similarity}
User preferences can improve similarity retrieval by incorporating additional information. By analysing which model was favoured for similar queries, one can infer the likelihood of a larger model being preferred. Both strategies used preference data to develop a ranking-based algorithm.  \textcite{ong2024} used preference data from the Chatbot Arena Leaderboard \footnote{\href{https://chat.lmsys.org/}{Chatbot Arena Leaderboard}}, a preference-based LLM ranking interface, to propose \textit{RouteLLM}:  a range of routing strategies based on user preferences\footnote{\href{https://github.com/lm-sys/RouteLLM}{lm-sys/RouteLLM}} \footnote{\href{https://huggingface.co/routellm}{https://huggingface.co/routellm}}. They propose to reformulate the routing problem between a smaller,\textit{Mixtral-8x7B}, versus a larger LLM,\textit{GPT-4-1106-preview}, as a binary classification task. This task involves predicting the probability of the larger LLM being preferred for a specific query. To determine this probability, the researchers employ a Bradley-Terry (BT) algorithm \parencite{bradleyterry1952} for similarity-weighted ranking. They weight the queries from the training dataset according to their similarity to the user's query and subsequently use these weighted queries to learn the BT coefficients.  These coefficients enable the estimation of the probability of preferring a larger LLM for a specific query. The model parameters are learned through maximum likelihood estimation based on the preference data. Once this probability is inferred, the appropriate route is selected according to a cost threshold ($\alpha \in [0,1]$), optimised to balance cost and quality.

In parallel, \textcite{zhao2024} introduced an ELO-based algorithm called \textit{Eagle} \parencite{eloranking}, which aims to rank LLMs through a dual-component structure that includes both a global ELO ranking and a local ELO ranking. First, they computed a global ELO rating for each LLM by leveraging all available pairwise comparison information to establish an overall ranking. Then, they derived a local ELO ranking using pairwise information from the $N$ nearest neighbour queries, which were identified through cosine similarity. The algorithm selects the optimal LLM for a specific query by calculating a weighted sum of its global and local ELO rankings, while ensuring the selected model's cost remains within the user's budget. \\

\textit{Eagle} demonstrated notable superiority over various supervised models, including linear SVM, MLP, and KNN, showcasing a marginal performance improvement of 5.14\% over MLP and 4.73\% over KNN across multiple benchmarks (i.e., ARC-Challenge, WinoGrande, HellaSWAG, etc.) \parencite{zhao2024}. Similarly, \textcite{ong2024} demonstrated that their proposed strategies not only matched but, in many instances, surpassed the performance of BERT-based classifiers and fine-tuned LLMs. These results highlight the potential of using information about user preferences to train routing algorithms.

\subsubsection{Supervised Routing}\label{low-supervised}
Similarity-based routing can struggle in complex tasks due to its unsupervised nature, especially when dealing with similar domains of knowledge or when there is significant noise. Thus, once the ability of an LLM to successfully respond to user queries or a specific domain of knowledge has been evaluated, pre-generation routing can be viewed from a supervised paradigm.

\subsubsubsection{Recommendation System}
 The matrix factorization approach from \textcite{ong2024} extrapolate users' preferences from $\mathcal{M}$, a set of various LLM, and $\mathcal{Q}$, a set of queries. This method attempts to deduce a hidden scoring function $s : \mathcal{M} \times \mathcal{Q} \xrightarrow{}\mathbb{R}$ where the score $s(LLM_n,q_i)$ for a $LLM_{n}$ and a query $q_{i}$ reflects user preference. For instance, if $LLM_{n}$ is better than $LLM_{a}$, $s(LLM_{n},q_{i}) > s(LLM_{a},q_{i})$. Matrix factorisation proves to be more efficient than a classification-based or LLM-based approach for routing between a smaller and a larger model, particularly in terms of call savings and cost-quality trade-offs. Specifically, while using \textit{GPT-4-1106-preview} over \textit{Mixtral-8x7B} \parencite{jiang2024mixtralexperts}, it was possible to achieve 80\% quality gain on an open-ended question dataset called MT-Bench by relying on only 30\% of calls to \textit{GPT-4-1106-preview}. In contrast, the random assignment required 78\% of calls to this model. The results are similar when comparing \textit{Claude-3-Opus} to \textit{Llama-3-8B} \parencite{grattafiori2024llama3herdmodels} (42\% required) on this dataset.
 
\subsubsubsection{Domain Classification}
A straightforward approach to routing is domain-based routing, which directs queries to specific expert models based on their domain, such as, for example, health-related or financial queries. This strategy focuses on the level of granularity associated with the domain of knowledge of the query, rather than its specific details.\\

Two studies implemented a BERT-based classifier model \parencite{simonds2024, wang2024CoE}. Notably, \textcite{simonds2024} proposed an implementation that involves fine-tuning a DeBERTA-v3-large model \parencite{he2023debertav3i} for domain classification, called \textit{MoDEM}. This implementation encompasses domains including Math, Health, Science, Coding, and \textit{Other}.  Based on the identified domain, queries are routed to the appropriate expert LLM  for Health with \textit{Palmyra-Med-70B}, for Math with \textit{Qwen2.5-72B-Math-Instruct} \parencite{yang2024qwen25math}, for Science with \textit{Qwen2.5-72B-Instruct} \parencite{yang2024qwen2}, for Coding with \textit{Qwen2.5-72B-Instruct}, and for Other with \textit{Meta-Llama-3.1-70B-Instruct}.  Similarly, \textcite{wang2024CoE} found that integrating domain-based classification routing by fine-tuning a BERT-based model effectively matched the performance of the best LLM from their set of small specialised LLM options (\textit{Llama-3-Smaug-8B, Mathstral-7B-v0.1, Qwen2-7B-Instruct} \parencite{yang2024qwen2}, and \textit{Gemma-2-9b-it} \parencite{gemmateam2024gemma2}). The router achieved 52.2\%, compared to 52.0\% for \textit{Gemma-2-9b-it} \parencite{wang2024CoE}. It was less effective than a much larger LLM (\textit{Llama-3-70B} \parencite{grattafiori2024llama3herdmodels}), however, perform similarly to moderately larger models such as \textit{Mixtral-8x7B-Instruct-v0.1} \parencite{jiang2024mixtralexperts} and \textit{Yi-1.5-34B-Chat}. It seems that there is a marge of improvement as \textcite{simonds2024}'s approach achieved an MMLU accuracy of 87.7\%, which is closer to that of Llama-3.1-405B (88.6\%) than Qwen 2.5-72B (86.1\%), while using a pool of expert 70B models at a cost per token four times lower than that of the 405B model \parencite{simonds2024}. 

BERT-based models do not appear to be necessary for effective domain-based routing. \textcite{jain2024} use a domain classification task, termed \textit{Composition-of-Experts}, which they enhance with a two-step routing approach. In the first step, a \textit{k-NN} model maps queries to knowledge domains such as finance, coding, medicine or Russian language.  If classifier uncertainty âmeasured by entropyâ exceeds a predefined threshold, the model assigns the user query to a 'general' category. In the second step, a mixed integer linear programme allocates categories to experts, aiming to minimise costs while adhering to budget constraints related to the available billions of parameters. For certain datasets, such as Arena-Hard or MT-Bench, this framework achieved scores comparable to larger language models, like \textit{Llama-3-70B-Instruct} \parencite{grattafiori2024llama3herdmodels} and \textit{Qwen2-72B-Instruct} \parencite{yang2024qwen2}, using fewer average active parameters (approximately 30-50 compared to 70).

This underscores the necessity of defining well-specified domains when implementing domain-based classification routing instead of relying on a default method \parencite{simonds2024}. This strategy demonstrates high accuracy when routing across various domains on the MMLU Multi-Domain benchmark, achieving scores between 77\% and 97\% in Health, Math, Science, and Coding. However, performance significantly declines in the 'Other' domain, where it drops to 53\%, indicating that reliance on a default approach may not be effective. Employing a higher level of granularity and focusing specifically on the knowledge domain improves the routerâs generalisation ability on out-of-distribution datasets \parencite{wang2024CoE}. This approach allows less restrictive routing rules by linking a routing candidate to more general concepts (i.e., the knowledge domain), resulting in a 3.6\% increase in accuracy compared to direct mapping of queries to experts.

\subsubsubsection{Query complexity inference}
Focusing on user queries can uncover more intricate relationships than those established through domain knowledge. By classifying user queries according to their complexity and assessing whether a candidate LLM can handle each category, the routing process becomes a classification task \parencite{wang2024, ding2024, malekpour2024, stripelis2024, srivatsa2024}.\\

\textcite{wang2024} demonstrate that employing the granularity level of the query captures more specific information. Their query-level routing significantly outperforms domain-based routing on datasets related to the training data, achieving  64.3\% compared to 52.2\%.  Remarkably, despite using a set of small LLMs (under 9B parameters), it surpasses the performance of a much larger LLM, Llama-3-70b \parencite{grattafiori2024llama3herdmodels}. However, it is less efficient than the domain-based router on out-of-distribution datasets, scoring 67.1\% compared to 69.9\%. This illustrates the bias-variance trade-off, where high performance on related data comes at the cost of generalisation ability, and vice versa.

\textcite{ding2024}  suggested determining whether a difference in answer quality exists between smaller and larger models for a given query by computing the BART score \parencite{yuan2021}. This score calculates the probability of generating a sequence of $m$ tokens $\textbf{y}=\{y_{1},...,y_{m}\}$ based on a reference sequence of $n$ tokens $\textbf{x}=\{x_{1},...,x_{n}\}$ with a seq2seq model, called \textit{BART} \parencite{lewis2020}. To train their classification model, the authors implemented a series of progressively refined strategies. They began with a \textbf{deterministic strategy} using binary cross-entropy loss with hard labels, which indicate whether the BART score of the smaller LLM is equal to or exceeds that of the larger LLM. Building on this, they introduced a \textbf{probabilistic strategy}, replacing hard labels with soft labels derived from the average BART scores of ten responses per model, accommodating the non-deterministic nature of LLMs. To further enhance this approach, they incorporated a \textbf{transformation step} to address label imbalance, as the smaller LLM typically scores lower. By introducing a relaxation constant, \( t \), they reduced the influence of the larger LLM, ensuring a more balanced label set. This constant was optimised to maximise the pairwise score difference between the two models.  The study demonstrated that when there is a significant difference in parameter sizes between two large language models (LLMs), such as FLAN-T5 (800m) \parencite{chungflant5} compared to LLaMA-2-13B \parencite{touvron2023}, 40\% of queries were directed to the smaller model, resulting in only a 10\% decrease in quality. In cases where the parameter size differences were more subtle, such as between LLaMA-2-13B and GPT-3.5-turbo, 20\% of queries were assigned to the smaller model, with a reduction in quality of less than 1\%. The use of probabilistic strategies seems to outperform deterministic strategies. The method used to label the training data is crucial. Its deterministic strategy has been compared with LLM-based repeated calls routing \parencite{aggarwal2024} and token probability-based routing \parencite{ramirez2024}. The findings suggest that \textit{HybridLLM} is less effective than these alternatives.

One could classify based on previous successes rather than relying on sequence probability \parencite{malekpour2024, stripelis2024}. \textcite{malekpour2024} demonstrated that training a \textit{DistilBERT} \parencite{sanh2020distilbert} to classify the cheapest LLM capable of handling a query could reduce costs by 1.4 times, albeit with a loss of nearly six performance points compared to \textit{GPT-4o-mini} (55.2\% vs 61.0\%). They showed that this strategy resulted in lower successful SQL generation compared to the similarity retrieval method discussed in \ref{low-similarity}. However, it is important to note that this classification strategy relied more on smaller LLMs, which performed poorly in generating SQL queries. A better selection of LLM might yield different results (i.e., SQL Expert LLM). \textcite{stripelis2024} trained a BERT-based model \parencite{devlin2019} and discovered that this strategy reduces costs by 30\% and latency by 40\% compared to stand-alone generalist (\textit{Fox-1.6B} \parencite{hu2024fox1}) or experts LLM across various domains, including biomedical (e.g. \textit{BioLlama-7B}), coding (e.g. \textit{CodeLlama-7B} \parencite{roziÃ¨re2024codellama}), and mathematics (e.g. \textit{MathDeepSeek-7B}). Their integration of a 2-layer MLP, although suboptimal, highlighted the need for more complex architectures for query-based classification (i.e. BERT architectures).\\

For \textcite{srivatsa2024}, LLM answer ability to a specific query can be scored by evaluating the robustness of generated responses \footnote{\href{https://github.com/kvadityasrivatsa/llm-routing}{kvadityasrivatsa/llm-routing}}. This assessment involves generating ten responses and comparing them to the reference answer \parencite{srivatsa2024}. An LLM $M_{N}$ from a set $\mathcal{M} = \{M_{1},...,M_{N}\}$ is considered adequate for a query if the majority of its generated answers are consistent with the ground truth. During inference, the task can be approached as a multi-label classification task, where multiple outputs correspond to all LLMs deemed reliable for $q_i$, or as separate binary classifications, where one classifier is assigned to each LLM. Once this was accomplished, the authors implemented various strategies to identify the most suitable LLM for routing: selecting the model with the highest confidence score, randomly choosing from those models that surpassed a predetermined arbitrary confidence threshold, and employing a Random Forest algorithm that used the confidence scores of all modelsâalongside the confidence score of the initial gold reference modelâas input.  Various LLMs were included: \textit{gemma-7b and  gemma-7b-it} \parencite{gemmateam2024}, \textit{metamath-7b }\parencite{yu2024metamath}, \textit{mistral-7b-it} \parencite{jiang2023mistral7b}, \textit{Llama-2-13B-chat and llama-2-7b}\parencite{touvron2023}. Although the router exhibited reduced latency, it did not achieve the same accuracy as the best-performing LLM, \textit{gemma-7b}. \\

Alternatively, one can directly infer the performance score of the LLM for a given user query, making it a regression task. \textcite{hu2024} derive a performance score $Performance_{i,j}$ for an LLM $M_j$ on a query $q_i$ while considering its budget \parencite{hu2024}. This is defined as $Performance_{i,j} = \lambda \cdot P_{i,j} - cost_{j}$, where $ P_{i,j}$ represents the predicted ability of $M_j$ to answer to $q_i$, $\lambda$ represents the user's willingness to pay and $cost_{j}$ denotes the cost of calling $M_j$.\footnote{\href{https://github.com/withmartian/routerbench}{withmartian/routerbench}} The authors evaluated the value of $P_{i,j}$ in the training set using \textit{GPT-4} evaluation or exact matching and used it to train the regression model, but did not provide details. It was demonstrated that using a regression model as a routing mechanism resulted in performance scores that were comparable to those of larger language models, such as GPT-4 or Claude V2, on standard datasets (i.e., MMLU, MBPP, and GSM8K) and RAG-related datasets, but at less cost. Conversely, \textcite{sakota_2024} proposed strategies for selecting the most suitable LLM based on performance criteria. These strategies include selecting the model with the highest performance score, irrespective of cost; opting for a model that exceeds a specified performance threshold while being the less expensive; or choosing the model that achieves the highest score within the user's budget by solving an integer linear programming problem. The different models used are OpenAI's: \textit{text-ada-001, text-babbage-001, text-curie-001 and text-davinci-002}. Maximising the performance score without considering cost achieves an accuracy equivalent to the best-performing model, \textit{text-davinci-2}. However, this approach incurs a cost comparable to that of routing to the highest-performing model, resulting in an 11\% cost reduction. By implementing either the threshold approach or the cost-sensitive method, they achieved comparable accuracy at a significantly reduced cost (approximately a 62\% decrease). \\
\textcite{shnitzer2023} proposed an approach to identify which LLM might perform best on a new task that was not seen during training.  They implemented a collection of binary classifiers to determine whether a model $M_i$ within a set of $n$ LLMs $\mathcal{M} = \{M_1,...,M_n\}$, could provide an answer that matches the reference answer for a given query. However, the methodology for labelling the training set is not explicitly described. The routing approach infer which LLM will have the highest proportion of expected correct answers for a new task $d$. To generalise on data outside the distribution, the researchers developed an out-of-distribution confidence model. This model estimates the probability that a binary classifier's prediction is correct for a specific data point within a given task, thereby reflecting the model's uncertainty. It employs a regression approach to predict this probability by analysing the distance between the new task and previously encountered tasks. They demonstrated that these strategies outperformed the best model on average, \textit{Llama-2-70B}  \parencite{touvron2023}, by enabling the selection of smaller models that could provide adequate answers.\\

The routing process extends beyond simply directing to various pre-trained LLM; it also encompasses routing to retrieval strategies, prompting techniques, and even systems. Actually, \textcite{jeong2024} demonstrate similar results to those commented earlier with their \textit{Adaptive-RAG} framework\footnote{\href{https://github.com/starsuzi/Adaptive-RAG}{starsuzi/Adaptive-RAG}}. They route to alternative RAG systems, including no-retrieval, single-step naive RAG, and multi-step RAG methods. Complexity is assessed by generating answers for a set of queries: if the no-retrieval architecture returns the correct answer, the query is labelled as low complexity; if no-retrieval fails while the single and multi-step architectures succeed, the complexity is rated as moderate; and if the single-step fails, it is labelled as high complexity.  Subsequently, a \textit{T5-Large} model \parencite{raffel2020} is trained to classify user queries into one of three complexity categories. Although this strategy tends to favour more complex architectures over a naive RAG approach, it is comparable in matching the reference response within the same latency.  The performance remains consistent when using either \textit{Flan-T5-XXL} \parencite{chungflant5} or \textit{GPT-3.5-turbo} for generation. Similarly, in \textcite{ning2024}'s study, the model determines whether a query can be answered using their \textit{Squeletton-of-Thought} (SoT) prompting approach, treating it as a binary classification task\footnote{\href{https://github.com/imagination-research/sot}{imagination-research/sot}}. The authors of the study found that incorporating a router between prompts is more effective than applying \textit{SoT} to all queries. However, no comparison was made between the efficacy of their approach and that of a basic prompt as a baseline. It is therefore difficult to ascertain the relative efficacy of this approach.  \\

When explicitly detailed, most studies use a model from the \textit{BERT} family as a backbone, adapting it to supervised training: \textit{BERT-based} \parencite{devlin2019, ong2024, stripelis2024}, \textit{RoBERTa} \parencite{zhuang2021, srivatsa2024, ning2024}, \textit{DistilBERT} \parencite{sanh2020distilbert, srivatsa2024, sakota_2024, malekpour2024}. The choice of backbone model must align with the difficulty of the task of interest. \textcite{feng2024} demonstrated that replacing the \textit{DeBERTa} model \parencite{he2023} with \textit{RoBERTa} \parencite{zhuang2021} enhanced the performance of the \textit{HybridLLM} framework. Other methods have implemented \textit{T5} \parencite{raffel2020,srivatsa2024, jeong2024}, multi-layers perceptron \parencite{hu2024, stripelis2024}, \textit{Random Forest} \parencite{srivatsa2024}, \textit{linear optimisation programmes} \parencite{dekoninck2024}, or \textit{K-nearest neighbour} \parencite{shnitzer2023,hu2024}.\\

Supervised pre-generation routing proves to be both rapid and effective \parencite{ding2024, srivatsa2024}.  Furthermore, studies highlighting the ineffectiveness of classification-based routing in improving the cost / quality trade-off emphasise the importance of complementary capabilities among the available LLMs \cite{srivatsa2024}. This limitation is discussed in the section \ref{Using complementary routing options}. \textcite{yue2024} found that a \textit{RoBERTa} model \parencite{zhuang2021}, fine-tuned to route between the smaller \textit{GPT-3.5} and the larger \textit{GPT-4}, was less effective than a specific prompt engineering case discussed previously. This finding indicates that the effectiveness of a routing framework may be influenced by the available routing candidates. The use of strategies based on query information captures more specific information and significantly outperforms strategies based on domain knowledge. However, it shows reduced efficiency on out-of-distribution datasets, highlighting the bias-variance trade-off for performance \cite{wang2024CoE}.
 
 \subsubsubsection{Knowledge Graph}
Most methods discussed in this survey have one major drawback: they cannot generalise to new routing options (e.g. models, tools). As noted by \textcite{feng2024}, these methods rely on a transductive learning framework, which involves learning specific rules from a corpus and applying them to particular cases, similar to those encountered during training. However, in a rapidly evolving environment where new tools frequently emerge, maintaining this type of architecture becomes costly, necessitating frequent retraining whenever new tools are introduced. To address this issue, \textcite{feng2024} proposes an \textit{inductive graph framework}, \textit{GraphRouter}. This framework focuses on learning contextual information regarding the interactions between tasks and tools, ultimately enhancing generalisation.
The graph consists of three types of nodes: task nodes, query nodes, and LLM nodes. To initialise task and LLM nodes, we first generate their descriptions and include additional calling cost information for the LLM nodes. We then encode these descriptions using a BERT-like model \parencite{devlin2019}. Query nodes are also encoded using the same model. The relationships among these nodes are represented by edge features. The task-query edge indicates whether a query is related to a specific task, such as a math-related query in the context of GSM8K. Additionally, the LLM-query edge provides a score that reflects the performance of an LLM while considering a desired cost by concatenating cost and performance.
They employ a two-layer graph attention network with a 32-dimensional hidden layer to update the representations of nodes using information from their local neighbourhoods. Subsequently, the task of selecting an LLM for a query $i$ can be reframed as an edge prediction between the query $i$ and LLM nodes.\\ The routing options included a set of open-source LLMs, featuring various Llama-based models such as \textit{Llama-3-7b and Llama-3-70b} \parencite{grattafiori2024llama3herdmodels}, \textit{Llama-2-7b} \parencite{touvron2023}, NousResearch's 34b LLM, \textit{Mistral-7b} \parencite{jiang2023mistral7b}, and \textit{Qwen-1.5-72b }\parencite{bai2023qwentechnicalreport}. They demonstrated a performance surpassing that of the largest LLM on a multi-task dataset while requiring lower costs. Their results also exceeded those of prompt-based routing, \textit{HybridLLM} \parencite{ding2024}, \textit{FrugalGPT} \parencite{chen2023}, and a bandit-based model. Notably, with new LLM options, they achieved a performance improvement of 4 points over \textit{FrugalGPT} and 21 points over \textit{HybridLLM}.

\subsubsection{Reinforcement Learning (RL)-based Routing}\label{low-reinforcement}
Supevised routing, similarity-based routing and generative-based routing strategies may not be able to adapt to new routing options or a new context, such as changes in the way users express themselves. This limitation arises because the routing process needs to learn from interactions \cite{Sutton2014}. In RL, the router acts as an agent with a set of actions. In the context of optimising the generation step in a conversational agent, an action involves selecting a specific model. Based on the current state or context (user query), the model must identify the appropriate action to maximise a reward function while adhering to constraint functions, such as the cost.

\subsubsubsection{Stateless algorithms}
Stateless algorithms provide a low-resource alternative by mapping rewards directly to actions. One such approach is \emph{Stochastic Learning Automaton} \parencite{sikeridis2024} from the \textit{PickLLM} framework. It considers a finite set of actions, specifically the LLM candidates. Each candidate has a probability associated with it that indicates its likelihood of being selected, determined by the reward returned. After each interaction between the user and the system, each probability is updated using a very basic linear reward-inaction scheme \parencite{narendra1974}: increasing the probability of selecting the LLM with the highest reward, while decreasing the probability of other candidates.  A learning rate controls this process. They used a set of open source models, including \textit{Mistral-7B} \parencite{jiang2023mistral7b}, \textit{WizardLM-70B, Llama-2-13B}  and \textit{Llama-2-70B}\parencite{touvron2023}. They used the LLM-as-a-Judge (i.e., \textit{GPT-3.5-turbo}) framework alongside a reward model to evaluate accuracy. This strategy significantly reduces both cost and latency compared to using the most expensive option or randomly selecting a model. Specifically, the SLA option reduces latency more consistently than the Q-learning option.In terms of accuracy, the model performed slightly better than Llama-2-70B and Mixtral-8x7B \parencite{jiang2024mixtralexperts} on a medical subset and a computer science subset. On other datasets, such as open-ended question-answering and financial datasets, it matched their performance. However, especially when compared to Mixtral-8x7B, it underperformed on a subset of Reddit. This discrepancy may be due to the absence of Mixtral-8x7B in the set of options provided.

Instead of relying on action probabilities, one could optimise selection using expected rewards given the selection of an LLM candidate, referred to as \emph{Q-values} \parencite{kaelbling1996}. In Q-learning, an agent iteratively updates its \emph{Q-values} based on the rewards it receives from the environment after performing actions \parencite{kaelbling1996}. The agent selects the LLM candidate with the highest expected reward. However, focusing solely on highest rewards lead the algorithm to favour previously successful actions. To address this issue, \textcite{sikeridis2024} introduced an epsilon-greedy approach, which balances exploration and exploitation. This technique selects a random action with a probability of $\epsilon$ and chooses the action with the highest expected reward, as indicated by \emph{Q-values}, with a probability of $(1 - \epsilon)$ \parencite{sikeridis2024}. This strategy effectively reduces both cost and latency in comparison to purely random selection or routing only to the most expensive LLM. Performance remains comparable to the \textbf{\textit{Stochastic Learning Automaton}} approach in terms of accuracy.

Stateless strategies effectively converge on the LLM candidate that best fits the data and resource requirements \parencite{sikeridis2024}. It simplifies the identification of the most appropriate LLM in a given static context. However, it comes at the expense of the ability to select actions based on the current query. 

\subsubsubsection{State-based algorithms}
State-based algorithms differ from stateless algorithms in that they consider contextual information when selecting actions. Unlike stateless algorithms, which identify a single action that maximises expected reward, state-based algorithms consider multiple actions that may be optimal depending on the context.\\ \textcite{nguyen2024} proposed the \textit{Meta-LLM} framework which incorporates a contextual multiarmed bandit (MAB) model with a discrete set of actions, or arms, that can be selected at each step. An arm is an LLM option to which the router can route. Each arm has an unknown reward distribution. At each iteration, the model chooses an arm and receives a reward, allowing it to update the Q-values. The objective is to select the optimal arm while maximising the reward. \textcite{nguyen2024} implemented a framework to address classification tasks by considering both accuracy and cost within their reward function.  Initially, they used OpenAI LLMs as options: \textit{ text-ada-001, text-babbage-001, text-curie-001 and text-davinci-002}.  MAB model achieved comparable accuracy to the most expensive and high-performing LLM, \textit{text-davinci-002} (91.0 versus 90.9), while incurring significantly lower costs (0.3 versus 4.8 \$ per 10,000 queries). The authors explain this by saying that some queries were correctly classified by small LLMs but not by large ones. They conducted alternative experiments using a more heterogeneous set of providers, including \textit{Amazon's Titan Lite, Cohere Command-Light, Llama-2-7B \parencite{touvron2023}}, and \textit{Claude Instant}. In these trials, the MAB matched the performance of the most performant LLM, \textit{Llama-2-7B} (91.6 versus 91.5), and even outperformed the most expensive model, \textit{Claude Instant} (91.6 versus 87.6), while requiring drastically lower costs: \textit{MetaLLM} at 0.5 \$ per 10,000 queries, \textit{Claude Instant} at 2.5 \$ per 10,000 queries, and \textit{Llama-2-7B} at 2.4 \$ per 10,000 queries. This strategy effectively reduced costs while matching or exceeding the best available LLMs performance.  It is important to investigate how the inclusion of state-based algorithms improves the router ability to adapt to its environment.

\subsubsubsection{Reward-based inference}
\textcite{hari2023} noted that it would be impractical to create a Q-table to map the actual reward $r_{M_i}$ for selecting an LLM $M_i$ over all queries. An alternative approach is to estimate future $r_{M_i}$  using a training set. In their \textit{Tryage} framework, \textcite{hari2023}  infer expert model losses (rewards) by training a \textit{BERT-small} model \parencite{turc2019, bhargava-etal-2021-generalization} on a dataset containing queries and corresponding expert model losses. The list of expert models includes  \textit{ClinicalBERT} \parencite{alsentzer-etal-2019-publicly}, \textit{SECBERT, FinancialBERT, PatentBERT, CodeBERT } \parencite{feng-etal-2020-codebert} and \textit{RoBERTa} \parencite{zhuang2021}, among others.\\
This strategy was taken a step further by \textcite{lu2024} with \textit{Zooter}, a regression model trained from \textit{mdberta-v3-base} \parencite{he2023} through knowledge distillation from a reward model, \textit{QwenRM} \parencite{bai2023qwentechnicalreport}. The router encompasses a series of open-source models: 
\textit{WizardMath} \parencite{luo2025wizardmath}, \textit{WizardCoder} \parencite{luo2024wizardcoder}, \textit{WizardLM} \parencite{xu2024wizardlm}, \textit{Llama-2-Chat} \parencite{touvron2023}, \textit{OpenChat} \parencite{wang2024openchat}, and \textit{Vicuna}, all of which have 13 billion parameters. During the training process, the answer for a set of queries is generated using the LLMs, and a corresponding reward is attributed to each using a reward model. The distribution of rewards is normalised by a softmax function and used to train a student model (\textit{mdberta-v3-base}), where the Kullback-Leibler divergence is used as the distillation loss.

Using a reward model proves to be an effective approach to the routing problem. \textcite{hari2023} discovered that their framework achieved greater routing effectiveness to the "ideal" expert model (50.8\% accuracy) compared to a stand-alone \textit{GPT-3.5} (23.6\%) or the fine-tuned LLM \textit{Gorilla} \parencite{patil2023} (10.8\%). There are no explicit details on the selection criteria for the ideal expert model. In addition to demonstrating the effectiveness of reward models, \textcite{lu2024} also confirms two hypotheses from previous sections \parencite{lu2024}.

\begin{itemize}
  \item  The first hypothesis highlights the importance of integrating complementary LLMs within the routing process. The scores of the independent models vary significantly depending on the dataset, with some models excelling on specific datasets; for instance, \textit{WizardLM} performs well on the Flask Dataset, while \textit{Llama-2-Chat} \parencite{touvron2023} performs on AlpacaEval. Nevertheless, \textit{Zooter} achieves an average performance comparable to the best model across each dataset. 
  \item The second hypothesis emphasises the need for variability in LLM parameter sizes. In a benchmark comprising MMLU, GSM8K, and HumanEval, all open-source models exhibited poor performance, which constrained Zooter's overall effectiveness. However, Zooter's performance closely aligns with that of \textit{GPT-4}, apart from the last benchmark, where \textit{GPT-4} significantly outperforms it (32.3 for \textit{Zooter} versus 88.3 for \textit{GPT-4}). Changing the reward model ranking did not improve \textit{Zooter}'s performance on this particular benchmark. This difference may be related to the complexity of the tasks assigned, which may be too challenging for 13B parameter models, given that \textit{GPT-4} is much larger than the LLMs integrated into the router.
\end{itemize}

Reward-based supervised training enhances the representation of independent LLM abilities in the embedding latent space \parencite{lu2024}.

\subsubsection{Generative-based routing}\label{low-llm}
A common post-generation routing approach is to generate a response, assess confidence and determine whether further generation by an LLM is required. An optimal method would simplify this process by assessing confidence during the generation phase itself.
 
\subsubsubsection{Token probability}
Actually, \textcite{ramirez2024} achieves this in a low-resource manner by measuring \textit{output uncertainty} or \textit{margin}. Using the list of possible tokens returned by a LLM for the token in the first position of the generation, researchers calculate the margin between the probabilities of the first and second most likely tokens. This method has the advantage of assessing confidence without generating the entire output, making it the most resource-efficient option within the post-generation/cascade category. This approach is akin to the BART score discussed in section \ref{low-supervised}; however, the BART score requires the complete final sequence of tokens from the generated answer and does not consider the probabilities of less likely tokens at the same position. A larger language model is called if the margin exceeds a threshold established by a budget-based criterion. They used different pairs of small-large models: \textit{Mistral-7B-Instruct-v0.2} \parencite{jiang2023mistral7b} \& \textit{Mixtral-8x7B-Instruct-v0.1} \parencite{jiang2024mixtralexperts}, \textit{Llama-2-13B}-hf \& \textit{Llama-2-70b-hf}\parencite{touvron2023},  and finally \textit{davinci-002 (GPT-3)} and \textit{GPT-4}. This approach has proven to be effective in maximising performance while minimising costs across all pairs of language models compared to other methods, including regression models, \textit{HybridLLM} \parencite{ding2024} and \textit{FrugalGPT} \parencite{chen2023}. The differences are most significant when using the small-large LLM pair \textit{GPT-3} and \textit{GPT-4}. \textit{Frugal-GPT} performs better on several datasets of classification task (e.g. ISEAR, RT-Pol) with smaller LLM pairs (\textit{Mistral-7B-Instruct-v0.2 \& Mixtral-8x7B-Instruct-v0.1}, \textit{Llama-2-13B}-hf \& \textit{Llama-2-70b-hf}), but not on Q/A and reasoning tasks.

\section{Discussion}\label{Discussion}
A wide range of routing strategies exists, from similarity learning to LLM fine-tuning. Most of the strategies presented in this paper are based on the pre-generation approach for routing, employing various lightweight strategies (i.e., similarity learning, supervised learning, RL).  On the other hand, post-generation strategies tend to be resource-intensive, as they generally require the generation of several responses.
 
This survey highlights that the routing problem can be effectively addressed through low-resource solutions, which do not require extensive costs, even for strategies that can generalise to new routing options \parencite{feng2024}.

\subsection{Industrial Consideration}\label{Industrial Consideration}
In addition to understanding the scientific state of the art, it is essential to consider the industrial landscape. In other words, \emph{what current strategies are companies implementing and disseminating in light of the findings from this review?} 
Most of the modules used by companies are based on a pre-generation routing approach, primarily aiming to direct the user's query to specific tools, prompts, or scripts. These modules are generally based on a rather simplistic methodology, including conditional approach\footnote{\href{https://docs.haystack.deepset.ai/docs/conditionalrouter}{Haystack's ConditionalRouter},  \href{https://docs.haystack.deepset.ai/docs/filetyperouter}{Haystack's FileTypeRouter}}, similarity routing\footnote{\href{https://github.com/aurelio-labs/semantic-router}{aurelio-labs/semantic-router}}, or simply a prompt-based classification system\footnote{\href{https://python.langchain.com/v0.1/docs/use_cases/query_analysis/techniques/routing/}{LangChain's router}, \href{https://docs.llamaindex.ai/en/stable/module_guides/querying/router/}{Llamaindex's router}}
Some companies leverage more sophisticated architecture by employing an LLM to generate synthetic data used to implement a classification-based routing with a small classifier\footnote{\href{https://github.com/lamini-ai/llm-routing-agent}{lamini-ai/llm-routing-agent}}. Many of the mainstream LLM providers propose prompt-based routing, such as AWS\footnote{\href{https://github.com/awslabs/multi-agent-orchestrator}{awslabs/multi-agent-orchestrator}} or OpenAI\footnote{\href{ https://github.com/openai/swarm}{openai/swarm}}. With this survey, our goal is to improve the transfer of various lightweight routing strategies identified from research to industry.



 \subsection{Key challenges}\label{Key challenges}

\subsubsection{Going beyond financial costs}\label{Going beyond financial costs}
Most studies focus on optimising the trade-off between financial costs and answer quality \cite{chen2023}, while overlooking other significant expenses, such as computational and ecological costs.  Computational requirements and environmental impacts are intricately linked. Models with a larger number of parameters generally require more floating-point operations, resulting in increased energy consumption \cite{Kaack2022, Desislavov2023}. It is essential to minimise not only the financial costs but also the computational requirements given the urgent need to mitigate the impact of LLMs on climate change \cite{Kaack2022, Luccioni2024}. This can be accomplished by employing routing modules that activate more resource-intensive components only when necessary. Future work should consider incorporating the computational and environmental costs discussed in section \ref{cost-section} into the cost function $C_{M}$ from the  equation \ref{eq1}. 

\subsubsection{Standardisation of routing strategy experiments}\label{Standardisation of routing strategy experiments}
There is a clear lack of standardisation in terms of the datasets, metrics and evaluation methodology used. This makes identifying the most appropriate routing methods for their application difficult for readers. Furthermore, few studies have compared their routing approaches with those of other works. The authors often compare these strategies with non-routing approaches or other methods they have developed themselves. There is urgent need of a comprehensive benchmark that would allow to compare routing approaches in the same context. Future research could greatly benefit from established routing evaluation frameworks \footnote{\href{https://github.com/lm-sys/RouteLLM?tab=readme-ov-file\#evaluation}{lm-sys/RouteLLM}} \parencite{hu2024}.

In future studies, researchers should always include comparison of their proposed stategy with the following baselines: \textit{1) random routing, 2) the best-performing LLM for each query (the gold routing), 3) the stand-alone overall best performing LLM,  and 4) alternative routing strategies discussed in this survey.} These comparisons will help us understand whether routing success comes from available routing options or routing architecture. The difference between the overall best-performing LLM and the gold routing gives us the possible margin of improvement. Also, researchers must look at the number of calls made to each set of options: a routing strategy may perform well while also costing less, simply because it \emph{almost always} routes to the most expensive LLM. Including stand-alone LLMs from the available options as comparison with the custom routing approach will show the capabilities of the models on the different datasets.

\subsubsection{Using complementary routing options}\label{Using complementary routing options}
Routing to candidates with complementary rather than redundant skills is essential to optimise routing performance. For example, multiple models of the same size (e.g., 7B parameters) trained on a generalist corpus may exhibit some complementarity due to differences in their training datasets. However, we cannot expect significant performance enhancements for queries on specific or complex topics requiring advanced reasoning. The primary objective of routing is to maximise quality. Incorporating models with varying parameter sizes or expert models allows the routing strategy to adapt effectively to various contexts. In addition, when evaluating different routing approaches, it is critical to consider the complementarity between routing options as a confounding factor. 

\subsubsection{Consider all steps in the LLM-based system as routing possibilities}\label{Consider all steps in the LLM-based system as routing possibilities}
Almost all of the studies focused on the generation step by selecting the most appropriate LLM to answer the user's query.  However, current LLM-based systems typically include several additional steps. The routing approach can be effectively applied during the embedding step in the RAG architecture (Figure \ref{fig:fig1}). This includes routing between different embedding strategies, such as dense or sparse vectors, or the selection of fine-tuned embedding models \parencite{gao2023}. It also allows routing to databases tailored to specific topics, selection of appropriate similarity functions such as cosine similarity or BM25, and selection of the most appropriate prompting approaches \parencite{gao2023}. This framework can also be extended to facilitate routing between static knowledge sources, such as databases, and dynamic knowledge sources, such as website searches. Some authors have studied the use of routing for additional steps, such as context size selection \parencite{li2024}, prompt strategies \parencite{ning2024}, and even alternative pipeline designs \parencite{jeong2024}. Viewing these systems as dynamic systems rather than traditional static models facilitates optimisation at each stage and enhances modularity \parencite{gao2024}. This transforms query answering into a singular dynamic process that depends on the specific query being addressed.

\subsubsection{Towards autonomous adaptive routing strategies}\label{Towards autonomous adaptive routing strategies}
A significant drawback of the routing processes investigated in this survey is the need to re-train or adapt the entire routing system when a new routing option is introduced, such as an additional LLM, a novel prompting strategy, or a new pipeline option.  Consequently, routing becomes a non-adaptive process reliant on its initial configuration. 
Future research should assess the feasibility of autonomous processes that decide when to exploit established actions and when to explore new options. This addresses the \emph{exploration-exploitation dilemma}, which involves balancing the use of current knowledge with the exploration of new routing options to gain additional information \cite{Berger-Tal2014}. The research community should consider the routing process as an autonomous adaptive agent capable of adjusting to available resources and, more broadly, to its environment.

\section{Conclusion}\label{Conclusion}
Routing in an LLM-based system can be defined as a process that aims at selecting components of the system which maximise performance while minimising cost. The definition of the cost to be minimised and the score function to be maximised is essential to the construction of a routing algorithm.  We classified routing strategies as pre-generation or post-generation. We highlighted that implementing a routing strategy to maintain performance while minimising cost can be efficient and does not necessarily require high resources. We highlighted the need to develop products based on the lightweight strategies discussed in this survey.  We emphasised that there is a need to work on well-designed benchmarks across routing strategies to assess which approach offers the best potential by proposing key baseline comparisons. We also discussed the importance of considering computational and environmental costs in addition to financial costs.  Finally, we explore future perspectives for routing improvement through the complementarity of routing options and its potential confounding effect, we consider LLM-based systems as dynamic systems, and highlight the need for the router to be able to autonomously generalise to new routing options.
\break
\printbibliography

 
 \break
\appendix

\section{Appendix A - More information about different concepts addressed by the different studies}\label{annex1}

\subsection{Example of Dialogue State Tracking}

\begin{itemize}
  \item \textbf{Iteration 1:}
  \begin{itemize}
    \item \textbf{User:} What are the hours for the Natural History Museum?
    \item \textbf{Assistant:} The Natural History Museum is open from 10 AM to 5 PM daily.
    \item \textbf{Dialogue State $S_1$:} \{ "topic": "museum-hours", "entity": "Natural History Museum" \}
  \end{itemize}
  
  \item \textbf{Iteration 2:}
  \begin{itemize}
    \item \textbf{User:} How about the Science Museum?
    \item \textbf{Assistant:} The Science Museum is open from 9 AM to 6 PM every day.
    \item \textbf{Dialogue State $S_2$ $(+S_1)$:} \{ "topic": "museum-hours", "entity": "Science Museum" \}
  \end{itemize}
  
  \item \textbf{Iteration 3:}
  \begin{itemize}
    \item \textbf{User:} Are there any special exhibits at the Natural History Museum this weekend?
    \item \textbf{Assistant:} Yes, there is a special exhibit on ancient civilizations at the Natural History Museum this weekend.
    \item \textbf{Dialogue State $S_3$ $(+S_1+S_2)$:} \{ "topic": "special-exhibits", "entity": "Natural History Museum", "time-frame": "this weekend" \}
  \end{itemize}
\end{itemize}
\subsection{Prompting approaches used by \textcite{yue2024} \parencite{yue2024}}
\subsubsection{Chain-of-Thoughts (CoT)}

This approach was proposed by \textcite{wei2022} where intermediate reasoning is generated in natural language to solve a problem \parencite{wei2022}. Here is an example from the work of \textcite{yue2024}:

\begin{lstlisting}[language=tcl]
Question: Kobe and Pau went to a restaurant...
Answer: Pau ordered 5 x 2 = 10 fried chickens in total. Therefore, Pau ate 10 x 2 = 20 pieces of fried chicken. Ans = 20

Question: Joelle has 5 orchids and 4 African daisies on her balcony...How many petals do the daisies have compared to the orchids?
Answer:
\end{lstlisting}

\subsubsection{Program-of-Thoughts (PoT)}

This approach has been proposed by \textcite{gao2023}, where the intermediate reasoning steps are translated into Python code \parencite{gao2023}. Here is an example from the work of \textcite{yue2024}:
\begin{lstlisting}[language=python]
#Question: Kobe and Pau went to a restaurant...
#Python code, return ans
kobe_order=5
pau_order=kobe_order*2
pau_eaten =2*pau_order

#Question: Joelle has 5 orchids and 4 African daisies on her balcony...How many petals do the daisies have compared to the orchids?
#Python code, return ans
\end{lstlisting}

This example differs slightly from the original version of \textcite{gao2023} where the intermediate steps are in the form of natural language \textit{and} Python code. For example:
\begin{lstlisting}[language=python]
# Kobe ordered 5 pairs of fried chicken
kobe_order=5
\end{lstlisting}

 \break

\begin{landscape}% Landscape page
\section{Appendix B - Descriptive table of the routing approaches}\label{annex2}
\begin{longtblr}[
caption = {A description of the various works included in the survey},
 entry = name,
 label = {tab:tab_annex},
        ]{
  hlines,
  colspec = {X[3.5cm, l]X[4cm, l]X[4.5cm, l]X[5cm, l]X[2cm, l]X[1.5cm, l]X[4cm, l]},
  rowhead = 1}
\textbf{Study} & \textbf{Task} & \textbf{Benchmarks} & \textbf{Compared Strategies} & \textbf{Resources needed}& \textbf{Routing Step}& \textbf{Routing approach} \\
 \hline
\textcite{aggarwal2024} & Short and Multi-Choice Q/A, Text understanding, Reasoning & QASPER,QUALITY, COQA, MUTUAL, DIPLOMAT & Stand-alone models (\textit{GPT-4, Llama-2-13B}), FrugalGPT (\parencite{chen2023}), HybridLLM (\parencite{ding2024}) & High & Post & Repeated calls \\ 
\parencite{chai2024} & Multi-domain Q/A & MMLU Expert & Prompting methods, gold routing, LLM-Blender & High & Pre & LLM fine-tuning\\ 
\textcite{chen2023} & Short Q/A, Text classification & HEADLINES, OVERRULING, COQA & Stand-alone models (e.g., \textit{GPT-4, J1-Large, Xlarge, FAIRSEQ}), best LLM per query & High & Post & Answer confidence inference\\
\textcite{chen2024routerdc} & Multi-domain and Multi-Choice Q/A, Code generation &  MMLU, GSM8K, CMMLU, ARC-Challenge, HumanEval, PreAlgebra, MBPP, C-EVAL & stand-alone models (e.g., \textit{MetaMath-Mistral-7B, Chinese-Mistral-7B}), majority voting, ZOOTER \parencite{lu2024}, multi-class classification and clustering   & Low & Pre & Query similarity\\
\textcite{chuang2024} & Multi-domain and Multi-Choice Q/A, Code generation &  MMLU, OpenbookQA, GSM8K, MedQA & Verbalised confidence, logits-based uncertainty, random routing & High & Post & LLM fine-tuning \\
\textcite{dekoninck2024} & Multi-domain and Multi-Choice Q/A  &  ARC-Challenge, MMLU-Pro, MixEval, GSM8k   & Linear interpolation, linear optimisation programs (query-based classification, cascading) & High & Post & Answer confidence inference \\
\textcite{dekoninck2024} & Multi-domain and Multi-Choice Q/A  &  ARC-Challenge, MMLU-Pro, MixEval, GSM8k   & Linear interpolation, linear optimisation programs (query-based classification, cascading) & Low & Pre & Query complexity inference \\
\textcite{ding2024} & Instructions & MixInstruct & Random routing, smallest LLM, largest LLM & Low & Pre & Query complexity inference \\
\textcite{feng2024} & Multi-domain Q/A, Text understanding, Summarization & Alpaca, GSM8K, SQUAD, Multi-News & FrugalGPT \parencite{chen2023}, HybridLLM \textcite{ding2024}, prompt routing, smallest LLM, largest LLM, gold routing, bandit-based model & Low & Pre & Knowledge Graph \\
\textcite{hari2023} & Multi-Domain Text Corpus & Expert corpus (e.g., Pile-CC, Pubmed Central, ArXiv ) & Stand-alone model (\textit{GPT-3.5-turbo}), Gorilla (\parencite{patil2023}) & Low & Pre & Reward-based inference \\
\textcite{hu2024} & Multi-domain and Multi-Choice Q/A, Instructions, Reasoning & MMLU, MT-Bench, MBPP, HellaSwag, Winogrande, GSM8K, Arc-Challenge & Stand-alone models (e.g., \textit{WizardLM-13B, Claude-V2, Llama-70B}), K-NN, MLP, linear interpolation, gold routing & Low & Pre & Query complexity inference \\
\textcite{jain2024} & Multi-domain Q/A &  MMLU Pro, GSM8K & Stand-alone models (e.g., \textit{Qwen2-7B-Instruct, Gemma-2-9B-it}) & Low & Pre & Domain classification\\
\textcite{jang2023} & Q/A, Reasoning, Text understanding, Text classification, Instructions &  RTE, CB, ANLI, COPA, HellaSWAG, Storycloze, WinoGrande, WSC, WiC, Big-Bench, wiki-auto, HGen, COVID-QA, ELI5 & Stand-alone models (e.g., T0-11B, GPT-3, T0-3B)  & Low & Pre & Query similarity\\
\textcite{jeong2024} & Q/A, Text understanding & Single-step Q/A (e.G., SQuAD-v1.1, TriviaQA), Multi-step Q/A (e.g., MuSiQue, HotpotQA)& Adaptive
Retrieval, Self-RAG, gold routing & Low & Pre & Query complexity inference \\
\textcite{lee2024} (1) & DST & MultiWOZ, SGD & Prompt-DST, IC-DST and DS2-T5 & Low & Pre & Query similarity\\ 
\textcite{lee2024} (2) & DST & MultiWOZ, SGD & Prompt-DST, IC-DST and DS2-T5 & High & Post & Sequence probability \\ 
\textcite{li2024} & Q/A, Fact extraction, Text understanding & NarrativeQA, QASPER, MultiFieldQA, HotpotQA,etc. & NaÃ¯ve RAG, long context & High & Post & Prompt-based routing\\ 
\textcite{liu2024meswitch} & Multi-domain and Multi-choice Q/A, Code generation & MMLU, GSM8K, MATH, HumanEval, MBPP, C-Eval and C-MMLU & Stand-alone models (e.g., \textit{Dolphin-2.2.1-Mistral-7B, Llama-2-13B-Chat, MetaMath-13B}) & High & Pre & LLM fine-tuning \\ 
\textcite{lu2024} & Q/A, Instructions,  Multi-domain Q/A, Code generation & AlpacaEval, FLASK, MT-Bench, MMLU, GSM8K, HumanEval & Stand-alone models (e.g., \textit{WizardCoder}, \textit{Vicuna}, \textit{ GPT-4}), overall best LLM, gold routing & Low & Pre & Reward-based inference \\
\textcite{malekpour2024} & Text-to-SQL & BIRD & stand-alone models (\textit{GPT-4o}, \textit{GPT-4o-mini} and \textit{Llama-3.1-8b}) & Low & Pre & Query similarity, Query complexty inference \\ 
\textcite{manias2024} & Intent detection & Custom Dataset & / & Low & Pre & Query similarity \\ 
\textcite{mohammadshahi2024} & Multi-Domain Q/A & MMLU & Stand-alone models (e.g., \textit{Llama-2-70B, Mistral-7B, Mixtral-8x7B, GPT-3.5}) & High & Pre & Decoder-based encoding \\ 
\textcite{nguyen2024} & Text classification & IMDB, SST-2 & Stand-alone models (e.g., \textit{text-ada-001, text-babbage-001, Claude Instant, Cohere Command-Light}) & Low & Pre & State-based algorithms\\ 
\textcite{ning2024} & Q/A & FastChat, LLMZoo & No routing & Low & Pre & Query complexity inference\\ 
\textcite{ning2024} & Q/A & FastChat, LLMZoo & No routing & High & Pre & Prompt-based routing\\ 
\textcite{ong2024} & Multi-Domain Q/A, Instructions & MT Bench, MMLU, GSM8K & Random routing & Low & Pre & Query complexity inference, preference similarity, recommendation system \\ 
\textcite{ong2024} & Multi-Domain Q/A, Instructions & MT Bench, MMLU, GSM8K & Random routing & High & Pre & LLM fine-tuning \\ 
\textcite{patil2023} & Code generation & Custom API calling dataset & Stand-alone models (\textit{Llama-7B, pt-3.5, GPT-4}) & High & Pre & LLM fine-tuning \\ 
\textcite{pichlmeier2024} & Feasibility & / & / & Low & Pre & Clustering previous interactions\\
\textcite{ramirez2024} & Text classification, Multi-Domain and Multi-Choice Q/A, Fact Extraction & Wikifact, Openbook, ISEAR, FEVER, bAbI, Natural Questions, SST-2, CR, RT-Polarity & FrugalGPT (\parencite{chen2023}), HybridLLM (\parencite{ding2024}), query-based classification & Low & Post & Token probability\\
\textcite{sakota_2024} & Text classification, Multi-Domain and Multi-Choice Q/A & MMLU, GSM8K, WikiFact, RAFT, LegalSupport, etc. & Gold routing, stand-alone models (e.g., \textit{text-ada-001, text-babbage-001, text-
curie-001})  & Low & Pre & Query complexity inference \\ 
\textcite{shen2023} & Task Planning & Custom Dataset & / & High & Pre & Prompt-based routing \\
\textcite{shnitzer2023} & Multi-Domain Q/A, Instructions, Text classification & HELM, MixInstruct & Stand-alone models (\textit{Open-Assistant,Vicuna}),\textit{ MLM-Scoring, SimCLS, SummaReranker, PairRanker}, overall best LLM, gold routing, & Low & Pre & Query complexity inference\\
\textcite{sikeridis2024} & Q/A & HC3 & Random routing, stand-alone models (\textit{Mixtral-8x7B, Llama2-70B}) & Low & Pre & Stateless algorithms \\
\textcite{simonds2024} & Multi-Domain and Multi-Choice Q/A, Code generation & MMLU, MMLU Pro, GPQA, HumanEval, College Math, MATH, GSM8k, Olympiad Bench & Stand-alone models (e.g.,\textit{ Llama 3.1 405B, Qwen 2.5-72B, etc.}) & Low & Pre & Domain Classification\\
\textcite{srivatsa2024} & Multi-domain Q/A & MMLU, GSM8K & Stand-alone models (e.g., \textit{gemma-7b, mistral-7b}), random routing, gold routing  & Low & Pre & Clustering previous interactions, query complexity inference\\
\textcite{stripelis2024} & Multi-domain Q/A, Code generation & Ai2-ARC, GSM8K, MBPP, PubMedQA & Random routing, stand-alone models (e.g., \textit{BioLlama-8B, Fox-1.6B, MathDeepSeek-7B}), gold routing &  Low & Pre & Query similarity, Query complexity inference\\
\textcite{wang2024CoE} & Multi-domain and Multi-Choice Q/A / Code generation / Reasoning & MMLU Pro, GSM8K, Winogrande, Big Bench Hard, MMMU, MMStar & Stand-alone models (\textit{Gemma-2-9b-it, Llama-3-Smaug-8B, Mathstral-7B-v0.1, Qwen2-7B-Instruct}) & Low & Pre & Domain classification, Query complexity inference\\
\textcite{yue2024} & Multi-Domain Q/A, Text classification, Reasoning, Text understanding & GSM8K, ASDIV, TabMWP, Big-Bench Hard, CREPE & Stand-alone models wit CoT (\textit{GPT-3.5, GPT-4}) & Low & Pre & Query complexity inference \\
\textcite{yue2024} & Multi-Domain Q/A, Text classification, Reasoning, Text understanding & GSM8K, ASDIV, TabMWP, Big-Bench Hard, CREPE & Stand-alone models wit CoT (\textit{GPT-3.5, GPT-4}) & High & Post & Repeated calls \\
\textcite{zhang2023} & Instructions & ToolBench & stand-alone models (GPT-3.5, GPT-4) & High & Post & Code execution\\ 
\textcite{zhao2024} & Multi-Domain and Multi-Choice Q/A, Code generation , Instructions & MMLU, Hellaswag, GSM8K, ARC-Challenge, Winogrande, MBPP, MT-Bench  & Linear SVM, KNN and MLP & Low & Pre & Preference similarity\\ [1ex] 
\end{longtblr}


% Change equation numbering format to be sequential within sections in the appendix
\renewcommand\theequation{\Alph{section}\arabic{equation}} % Redefine equation numbering format
\counterwithin*{equation}{section} % Number equations within sections
\renewcommand\thefigure{\Alph{section}\arabic{figure}} % Redefine equation numbering format
\counterwithin*{figure}{section} % Number equations within sections
\renewcommand\thetable{\Alph{section}\arabic{table}} % Redefine equation numbering format
\counterwithin*{table}{section} % Number equations within sections
\end{landscape}


\end{document}