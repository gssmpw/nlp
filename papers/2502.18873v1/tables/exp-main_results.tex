
\begin{table*}[t]
% \captionsetup{justification=centering, width=\textwidth} % Automatically adjust width
\caption{
Main results.
Those rows marked by $\ddagger$ were reported by the rStar paper~\cite{qi2024mutual} using Llama-3-8B-Instruct. All other results are reported by our experiments.
\textsc{Multi} refers to multi-LLM while \textsc{STG} represents StrategyQA.
\textbf{The highest number} on each dataset is marked in bold while \underline{the secondary high} is underlined.
}
\label{tab:main_results}
\vskip 0.1in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{l|cc|cccc|cr}
\toprule
Method & Multi? & Search? & GSM8K & SVAMP & MATH & STG & Avg. \\
\midrule
Few-shot CoT:                                       &   &   &   &  &  &  & \\
\hspace{10pt} $\circ$ Llama-3.1-8B-Instruct & $\times$ & $\times$ & 84.00 & 86.80 & 41.60 & 67.39 & 69.95 \\
\hspace{10pt} $\circ$ Ministral-8B-Instruct-2410 & $\times$ & $\times$ & 82.41 & 89.20 & 40.00 & 70.60 & 70.55 \\
\hspace{10pt} $\circ$ Qwen-2-7B-Instruct & $\times$ & $\times$ & 84.00 & 88.60 & 24.20 & 66.67 & 65.87 \\
\hspace{10pt} $\circ$ GLM-4-9B-Chat & $\times$ & $\times$ & 83.85 & 89.70 & 40.00 & 71.32 & 71.22 \\
% \cdashline{1-5}[1.2pt/2pt]
\hline
\emph{w/} Llama-3.1-8B-Instruct:                    &   &   &   &  &  &  &   \\
\hspace{10pt} $\circ$ Self-Consistency@4 & $\times$ & $\times$ & 88.02 & 89.70 & 43.80 & 69.43 & 72.74 \\
\hspace{10pt} $\circ$ Self-Consistency@32 & $\times$ & $\times$ & 90.37 & 92.40 & 44.80 & 70.89 & 74.62 \\
\hspace{10pt} $\circ$ Self-Consistency@128 & $\times$ & $\times$ & 90.98 & 93.30 & 52.20 & 71.32 & 76.95 \\
\hspace{10pt} $\circ$ Self-Consistency@256 & $\times$ & $\times$ & 90.90 & 92.90 & 53.20 & 71.03 & 77.01 \\
% \cdashline{1-5}[1.2pt/2pt]
\hline
\emph{w/} All Four LLMs:                            &   &   &   &  &  &  &   \\
\hspace{10pt} $\circ$ Self-Consistency@4 & $\surd$ & $\times$ & 90.45 & 92.20 & 46.20 & 70.45 & 74.82 \\
\hspace{10pt} $\circ$ Self-Consistency@32 & $\surd$ & $\times$ & 90.75 & 93.20 & 52.60 & 71.76 & 77.08 \\
\hspace{10pt} $\circ$ Self-Consistency@128 & $\surd$ & $\times$ & \underline{91.21} & 93.70 & 53.80 & 72.78 & 77.87 \\
\hspace{10pt} $\circ$ Self-Consistency@256 & $\surd$ & $\times$ & 90.98 & 93.50 & 54.20 & 71.47 & 77.54 \\
% \cdashline{1-5}[1.2pt/2pt]
\hline
RAP $\ddagger$ & $\times$ & $\surd$ & 80.59 & 85.70 & 18.80 & 68.71 & 63.45 \\
% rStar (majarity-voting)$^\ddagger$                   &  88.70 &  91.89 &  38.30 &  71.47  \\
% rStar (discriminator)$^\ddagger$                     &  91.13 &  94.29 &  42.94 &  71.57  \\
RAP & $\times$ & $\surd$ & 90.52 & 91.60 & 53.00 & 75.40 & 77.63 \\
\hspace{10pt} + Single-LLM as Aggregator & $\times$ & $\surd$ & 90.05 & 92.50 & \underline{54.80} & \underline{75.69} & 78.26 \\
\hspace{10pt} + \textsc{MoSA} as Proposers & $\surd$ & $\surd$ & 91.13 & \underline{94.50} & 54.60 & \underline{75.69} & \underline{78.98} \\
% \cdashline{1-5}[1.2pt/2pt]
\hspace{10pt} + \mosa{} as Proposers \& Aggregators & $\surd$ & $\surd$ & \textbf{91.96} & \textbf{94.90} & \textbf{56.60} & \textbf{76.42} & \textbf{79.97} \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}

% \begin{table*}[t]
% % \captionsetup{justification=centering, width=\textwidth} % Automatically adjust width
% \caption{Main results. Those results marked by $\ddagger$ were reported by the rStar paper~\cite{qi2024mutual} using Llama-3-8B-Instruct. All the other results are reported by our experiments.}
% \label{tab:main_results}
% % \vskip 0.15in
% \begin{center}
% % \begin{small}
% % \begin{sc}
% \begin{tabular}{lcccc}
% \toprule
% Method & GSM8K & SVAMP & MATH & StrategyQA & GAIA \\
% \midrule
% Few-shot CoT                                        &   &   &   &   &   \\
% \hspace{10pt} $\circ$ Llama-3.1-8B-Instruct         &  84.00 &  86.80 &  41.60 &  67.39 &  3.87 \\
% \hspace{10pt} $\circ$ Ministral-8B-Instruct-2410    &  82.41 &  89.20 &  40.00 &  70.60 &  6.45 \\
% \hspace{10pt} $\circ$ Qwen-2-7B-Instruct            &  84.00 &  88.60 &  24.20 &  66.67 &  2.58 \\
% \hspace{10pt} $\circ$ GLM-4-9B-Chat                 &  83.85 &  89.70 &  40.00 &  71.32 &  1.94 \\
% Self-Consistency@4                                  &  88.02 &  91.10 &  43.80 &  73.80 &  6.45 \\
% Self-Consistency@32                                 &  90.37 &  92.60 &  44.80 &  73.36 &  3.87 \\
% Self-Consistency@128                                &  90.98 &  92.80 &  52.20 &  73.07 &  5.84 \\
% Self-Consistency@256                                &  90.90 &  92.50 &  53.20 &  73.22 &  6.45 \\
% \cdashline{1-6}[1.2pt/2pt]
% RAP$^\ddagger$                                       &  80.59 &  85.70 &  18.80 &  68.71 &  -- \\
% % rStar (majarity-voting)$^\ddagger$                   &  88.70 &  91.89 &  38.30 &  71.47 &  -- \\
% % rStar (discriminator)$^\ddagger$                     &  91.13 &  94.29 &  42.94 &  71.57 &  -- \\
% RAP                                                 &  90.52 &  91.60 &  53.00 &  75.40 &  7.10 \\
% % rStar (majarity-voting)                             &   &   &   &  74.96 &   \\
% % rStar (discriminator)                               &   &   &   &   &   \\
% % \cdashline{1-6}[1.2pt/2pt]
% MoSA                                                &  \textbf{91.96} &  \textbf{94.90} &  \textbf{56.60} &  \textbf{77.73} &  \textbf{8.39} \\
% \bottomrule
% \end{tabular}
% % \end{sc}
% % \end{small}
% \end{center}
% \vskip -0.1in
% \end{table*}