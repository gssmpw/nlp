
\section{Introduction}
\label{sec:intro}


%
%Large language models (LLMs) face challenges with complex reasoning, even augmented with linearized reasoning chains (e.g., CoT), due to the vast reasoning space created by the complexity and ambiguity of natural languages.
%A promising solution is step-wise search-based reasoning, where the reasoning problem is decomposed into traversing over a directed graph whose nodes and edges are individual reasoning sub-steps distributed in the vast reasoning space.
%Related methods have applied various search algorithms to LLMs, such as breadth-first search (BFS), depth-first search (DFS)~\cite{yao2024tree,besta2024graph}, and Monte Carlo tree search (MCTS)~\cite{hao-etal-2023-reasoning,zhang2024accessinggpt4levelmathematical,qi2024mutual}).
%
%
%
%As real-life reasoning problems are often multi-disciplinary, it is crucial to search for the next sub-steps diversely, avoiding local optima in which an LLM might get trapped if it exhibits a preference for certain knowledge snippets or reasoning paths due to inherent constraints in its training data and architectural design.
%A direct solution is to scale up the LLM to expand its capabilities and the scope of its knowledge, but it comes with excessively high cost.
%
%
%To mitigate this limitation, an alternative solution is to aggregate the specialties of multiple \emph{smaller} LLMs, i.e., ensemble.
%Recent work~\cite{moa} demonstrated that multiple LLMs could collaboratively enhance their instruction-following capabilities by post-editing each other's responses to the same instruction.
%Motivated by their progress, we consider how to leverage the collective expertise of multiple LLMs for complex reasoning. 
%In this work, we propose a step-wise search-based reasoning framework that aggregates the complementary strengths of multiple smaller LLMs, leveraging both independent and collaborative contributions to explore the reasoning space more effectively. 
%At each reasoning step, multiple LLMs suggest diverse potential search directions, either independently or by iteratively refining one another's outputs. 
%This hybrid approach ensures that the reasoning process is not constrained by the limitations or biases of any single model. 
%For instance, one model may excel at identifying a promising initial direction, while another might build on it to refine or extend the reasoning path. 
%By balancing independence and collaboration, this framework avoids local optima and enhances the diversity, efficiency, and robustness of the search process without incurring the prohibitive costs of scaling up a single model.

% ### Rationale for Edits:

% 1. **Grammar and Coherence**:
%    - "even augmented with linearized reasoning chains (e.g., CoT)" -> "even when augmented with linearized reasoning chains (e.g., CoT)": Added "when" for clarity.
%    - "the vast reasoning space created by the complexity and ambiguity of natural languages" -> "the vast reasoning space inherent in the complexity and ambiguity of natural languages": Improved phrasing.
%    - "decomposed into traversing over a directed graph" -> "decomposed into a traversal over a directed graph": Improved readability.
%    - "avoiding local optima in which an LLM might get trapped" -> "avoiding local optima where an LLM might get trapped": Simplified for readability.
%    - "a preference for certain knowledge snippets or reasoning paths due to inherent constraints in its training data and architectural design" -> "a preference for certain knowledge snippets or reasoning paths due to constraints inherent in its training data and architectural design": Improved phrasing.

% 2. **Logical Consistency**:
%    - "A direct solution is to scale up the LLM to expand its capabilities and the scope of its knowledge, but it comes with excessively high cost" -> "A direct solution is to scale up the LLM to expand its capabilities and scope of knowledge, but this approach comes with excessively high costs": Ensured logical consistency by specifying "this approach" and correcting "cost" to "costs".

% 3. **Sub-optimal Expressions**:
%    - "Recent work~\cite{moa} demonstrated that multiple LLMs could collaboratively enhance their instruction-following capabilities by post-editing each other's responses to the same instruction" -> "Recent work~\cite{moa} has demonstrated that multiple LLMs can collaboratively enhance their instruction-following capabilities by post-editing each other's responses to the same instruction": Improved verb tense consistency.
%    - "Motivated by their progress, we consider how to leverage the collective expertise of multiple LLMs for complex reasoning" -> "Motivated by this progress, we explore how to leverage the collective expertise of multiple LLMs for complex reasoning": Improved phrasing for clarity.

% 4. **Factual Errors**:
%    - No factual errors were identified.

% ### Edited Version:


% llm search
Large language models (LLMs) face challenges with complex reasoning, even when augmented with linearized reasoning chains (e.g., Chain-of-Thought), due to the vast reasoning space inherent in the complexity and ambiguity of natural languages.
A promising approach is step-wise search-based reasoning, which decomposes the reasoning problem into a traversal over a directed graph, where nodes and edges represent individual reasoning sub-steps distributed across the expansive reasoning space.
Related methods have applied various search algorithms to LLMs, such as breadth-first search (BFS), depth-first search (DFS)~\cite{yao2024tree,besta2024graph}, and best-first search~\cite{hao-etal-2023-reasoning,zhang2024accessinggpt4levelmathematical,qi2024mutual}.

% As real-life reasoning problems are often multi-disciplinary, it is crucial to search for the next sub-steps diversely.
% A direct solution is to scale up the LLM to expand its capabilities and scope of knowledge, but this approach comes with excessively high costs.

A successful search trial is featured with diverse yet effective explorations~\cite{hao-etal-2023-reasoning,yao2024tree}.
% diversity matters but hard to increase diversity while maintaining quality, figure 1
A straightforward method to enhance diversity involves increasing the temperature, thereby making the probability distribution more uniform.
This is typically combined with top-$k$ and top-$p$ sampling to balance diversity and quality.
% Despite these sampling techniques, a single LLM might still get trapped in local optima if it exhibits a preference for certain knowledge snippets or reasoning paths due to constraints inherent in its training data and architectural design. 
However, as shown in Figure~\ref{fig:intro}, despite these sampling techniques, achieving a balance between diversity and quality remains challenging and necessitates careful tuning.
Besides, even with near-optimal sampling parameters, a single LLM might still get trapped in local optima due to constraints inherent in its training data and architectural design. 
% Scaling up the training cost (e.g., model parameters and corpora size) could expand the LLM's capabilities and scope of knowledge but comes with excessively high costs~\cite{chen2024understandingtreethoughtssucceeds}.

\begin{figure}
    \centering
    \includegraphics[width=0.76\linewidth]{figures/intro2.0_comic.pdf}
    \caption{
    Reasoning performance on MATH-500 against search trajectory diversity. 
    While the diversity of single-LLM search varies with different sampling temperatures, the multi-LLM search consistently achieves superior performance. 
    More details are provided in $\S$~\ref{sec:analysis:diversity}.
    }
    \label{fig:intro}
    \vskip -0.2in
\end{figure}


% the concept of multi-agent -> multi-agent for search, figure 1
To mitigate this limitation, an alternative solution is to aggregate the specialized strengths of multiple LLMs.
Recent work~\cite{moa} has demonstrated that multiple LLMs can collaboratively enhance their instruction-following capabilities by post-editing each other's responses to the same instruction. 
% However, as far as we know, such collaborative features were never tested on search-based reasoning.
Motivated by this progress, we explore leveraging the collective expertise of multiple LLMs for search-based reasoning, which, to the best of our knowledge, has not been previously tested.
Figure~\ref{fig:intro} illustrates the reasoning accuracy on the MATH-500 dataset as a function of search diversity.
The performance of search using a single LLM initially improves with increased temperature but subsequently degrades, remaining consistently lower than that of multiple-LLM search.

In this work, we propose Mixture-of-Search-Agents (\mosa{}), an advanced paradigm for step-wise search-based reasoning that aggregates the complementary strengths of multiple LLMs, leveraging both independent and collaborative contributions to search for reasoning sub-steps more effectively. 
As illustrated in Figure~\ref{fig:ours}, multiple LLMs propose diverse potential search directions at each reasoning step, either independently or through iterative refinement of each other's outputs.
This hybrid approach ensures that the reasoning process is not constrained by the limitations or biases of any single model. 
For instance, one model may excel at identifying a promising initial direction, while another might build on it to refine or extend the reasoning path. 
By combining independence and collaboration, the framework avoids local optima while enhancing reasoning accuracy in the search process.


We performed a comprehensive evaluation of \ourmethod{} across four reasoning benchmarks.
The findings indicate that \ourmethod{} consistently outperforms its single-LLM counterpart in reasoning accuracy with an average improvement of 1.71\%.
% On the challenging MATH-500 dataset, \ourmethod{} yielded an improvement of 1.80\%.
Additionally, our results indicate a synergistic interaction between multi-agent collaboration and search-based reasoning.
Further analysis and ablation studies reveal a key challenge for single-agent search-based reasoning: balancing diversity and quality varies across different benchmarks.
We also confirm a positive correlation between reasoning performance and the number of distinct search agents, validating the efficacy of multi-agent search.
Finally, experiments with an extended action set demonstrate the robustness of \ourmethod{} across diverse types of search actions.
