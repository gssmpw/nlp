




\section{Experiments}
\label{sec:exp}

% We perform evaluations over four reasoning benchmarks in this section.

\subsection{Baselines}
\label{sec:exp:baselines}

% We mainly consider three types of baselines:
% \begin{itemize}
%     \item Few-shot Chain-of-Thought (CoT) prompting~\cite{}: taking one forward call;
%     \item Few-shot CoT with self-consistency@$n$ prompting~\cite{}: taking $n$ forward calls;
%     \item Reasoning-via-Planning (RAP, \citet{hao-etal-2023-reasoning}): the baseline MCTS method, taking the same order of magnitude of forward calls as our method does.
% \end{itemize} 
\textbf{Few-shot Chain-of-Thought (CoT)}~\cite{wei2023chainofthoughtpromptingelicitsreasoning} feeds the LLM with a few demonstrations followed by the input question.
Since we are using instruction-tuned LLMs, we format the demonstrations as multi-turn dialogues.
In each turn, the human asks a question and then the assistant answers it.

\textbf{Self-Consistency@$\mathbf{\textit{n}}$}~\cite{wang2023selfconsistency} also adopts the few-shot CoT prompting scheme, but it samples $n$ independent answers per instance.
The final answer is then given by majority voting over the $n$ candidate answers.
Except for the conventional single-LLM self-consistency experiments, we also evaluate self-consistency with multiple different LLMs.
Such a multi-LLM self-consistency setting can be regarded as a simplified version of ~\citet{moa}, which collects direct answers from various agents and aggregates them with majority voting.

\textbf{Reasoning-via-Planning (RAP)}~\cite{hao-etal-2023-reasoning} is a representative LLM-based reasoning method using MCTS.
We use it as the foundation to apply \mosa.
In each search step, RAP generates one or more sub-questions along with their sub-answers.
The original RAP paper adopted different reward functions for different types of tasks.
In this work, we use the simple self-consistency score as the reward value, which has been shown to be competitive with those manually designed ones in Appendix A.1 of \citet{qi2024mutual}.
Note that we ensure the total number of LLM forward calls of a single-LLM method are approximately the same as its multi-LLM counterpart, e.g., RAP \emph{versus} RAP + \mosa{} as Proposers in Table~\ref{tab:main_results}.
% We use RAP as the main baseline in main experiments (Table~\ref{tab:main_results}) and ablation studies regarding proposers and aggregators (Table~\ref{tab:ablation}).

\textbf{rStar}~\cite{qi2024mutual} is one of the recent SoTA MCTS-based LLM reasoning methods.
% It integrated various types of actions into the action set, like \textit{to propose an one-step thought} and \textit{to rephrase the question}.
% After the terminal nodes are obtained, it adopts an LLM (usually different from the search agent) as a discriminator to post-verify the correctness of the reasoning trajectories, which they named as \textit{mutual reasoning consistency}.
The authors proposed a comprehensive set of search actions, which we have introduced in $\S$~\ref{sec:method:background}.
We adopt their innovative set of actions to evaluate the effects brought by the scope of action set on \mosa{} in $\S$~\ref{sec:analysis:action_set}.




\input{tables/exp-main_results}






% \input{tables/exp-num_of_agents}


\subsection{Experimental Settings}
\label{sec:exp:exp_settings}

\textbf{Benchmarks}
\hspace{5pt}
% MoSA is generally applicable to various reasoning tasks.
We perform evaluation on four reasoning benchmarks covering different scopes, including three mathematical reasoning datasets (GSM8K~\cite{gsm8k}, SVAMP~\cite{svamp}, \mbox{MATH-500}~\cite{MATH,lightman2023lets}) and one commonsense reasoning dataset (StrategyQA~\cite{strategyqa}).


\textbf{Models}
\hspace{5pt}
We adopt four open-sourced instruction-following LLMs to formulate the LLM pool of \mosa{}: Llama-3.1-8B-Instruct~\cite{grattafiori2024llama3herdmodels}, Qwen-2-7B-Instruct~\cite{yang2024qwen2technicalreport}, Ministral-8B-Instruct-2410~\cite{ministral}, and GLM-4-9B-Chat~\cite{glm2024chatglm}.
The number of LLMs could also be made larger or smaller, depending on customized choices.
Our later experiments will show that benchmark performances are positively correlated with the number of distinct LLMs.




\textbf{Implementation Details}
\hspace{5pt}
For few-shot CoT baselines, we report the results of all four LLMs.
For other single-LLM baselines, like Self-Consistency@$n$ and RAP, we adopt \textbf{Llama-3.1-8B-Instruct} due to its competitiveness and robustness across various benchmarks.
For all experiments regarding sampling from multiple LLMs, we try to maintain a pseudo uniform distribution for the \texttt{SelectLLM} function in Algorithm~\ref{alg:mosa-A_1}.
That is, if 7 completions need to be sampled and there are 4 distinct LLMs, we manually assign each LLM to sample one completion and then uniformly sample 3 LLMs out of 4 without replacement to finish the remaining 3 completions.
%
Hyper-parameter settings are listed in Appendix~\ref{sec:appendix:exp_setting}.
% For few-shot CoT baselines, we report the best number out of the four numbers yielded by the four LLMs.
% For all other single-LLM baselines, we adopt the best LLM with few-shot CoT on the corresponding dataset.
% For example, on StrategyQA, XX is the best using few-shot CoT, so we adopt XX for all other single-LLM baselines for StrategyQA then.












\subsection{Main Results}
\label{sec:exp:main_results}
We report the main results on the four benchmarks in Table~\ref{tab:main_results}. Below we highlight our key findings.

% \textbf{While multi-LLM collaboration and search-based reasoning perform competitively when used separately, their integration results in the best overall performance.}
% The best of multi-LLM collaboration (i.e., self-consistency@256) and the best of RAP perform on par with each other,

\textbf{\mosa{} Leads in Reasoning Tasks}\hspace{5pt}
% For example, the best of \mosa{} achieves an accuracy number of 56.60\% on MATH-500, significantly higher than that of multi-LLM self-consistency@256 (54.20\%) and that of RAP (54.80\%).
% \revise{complete method compared with RAP}
RAP + \mosa{} as Proposers \& Aggregators consistently yields superior performances across all datasets (GSM8K, SVAMP, \mbox{MATH-500}, StrategyQA), reaching an average performance (Avg.) of 79.97\%.
Specifically, it obtains exceptional improvements (+1.8\%) over the best baseline on the challenging MATH-500 benchmark, suggesting it is effective at handling complex reasoning problems.


% % \textbf{Multi-LLM collaboration is effective on its own; however, its effectiveness becomes significantly more pronounced when combined with search.}
% \paragraph{Synergistic Effect between Multi-Agent Collaboration and Search-based Reasoning}
% \mosa{} is concerned with two lines of work: multi-agent collaboration and search-based reasoning.
% When independently applied, the two show little to moderate improvements; however, the improvements become significantly more pronounced when the two get combined.
% % Multi-agent collaboration is effective on its own; however, its effectiveness becomes significantly more pronounced when combined with search.
% (\textbf{1}) Ablate over multi-agent collaboration: 
% Across all four benchmarks, self-consistency@256 obtains an average absolute improvement of +0.53\% when it changes from a single LLM to multiple LLMs.
% In comparison, the average absolute improvement from single-agent search (i.e., RAP) to multi-agent search (i.e., \mosa{} as Proposers) is notably larger, at +1.35\%.
% When further augmented with aggregators, the improvement from single-agent search (i.e., RAP + Single-LLM as Aggregator) to multi-agent search (i.e., \mosa{} as Proposers \& Aggregators) increases to +1.71\%.
% (\textbf{2}) Ablate over search-based reasoning:
% With a single LLM, the gap between non-search (i.e., Self-consistency@256) and search (i.e., RAP) is +0.62\%; while the gap increases to +1.44\% when it turns to multiple LLMs.
% These results highlight that the synergy between multi-agent collaboration and search, especially with aggregators, achieves substantially greater performance gains than multi-agent collaboration alone.

\textbf{Synergistic Effect between Multi-Agent Collaboration and Search-based Reasoning}\hspace{5pt}
\mosa{} integrates two research paradigms: multi-agent collaboration and search-based reasoning. 
When applied independently, each achieves moderate improvements, but their combination yields significantly enhanced results due to synergy effects.
(\textbf{1}) Transitioning from single-agent to multi-agent:
Across all four benchmarks, transitioning from a single LLM to multiple LLMs with the best non-search baseline (Self-consistency) results in an average absolute improvement of +0.53\%. 
By contrast, transitioning from single-agent search (RAP) to multi-agent search (\mosa{} as Proposers) yields a larger average absolute improvement of +1.35\%. Augmenting with aggregators further increases the improvement from single-agent search (RAP + Single-LLM as Aggregator) to multi-agent search (\mosa{} as Proposers and Aggregators), achieving +1.71\%.
(\textbf{2}) Transitioning from non-search to search-based reasoning:
Using a single LLM, the performance gap between non-search (Self-consistency@256) and search (RAP) is +0.62\%. 
This gap widens to +1.44\% when employing multiple LLMs, showcasing the synergy between multi-agent collaboration and search. 
% Notably, with non-search, the improvement from single-LLM to multi-LLM is +0.53\%; while with search, the improvement from single-LLM to multi-LLM is +1.35\% (or even +1.71\% when aggregators are employed).
These results highlight that combining multi-agent collaboration with search-based reasoning yields significantly greater performance gains than applying either approach in isolation.

% \ourmethod{} consistently outperforms the RAP baseline while both methods require similar numbers of LLM forward calls.
% Specifically, the improvements are obtained both with and without aggregators, which suggests that both \mosa{} as Proposers and \mosa{} as Aggregators can effectively boost reasoning performances.




\textbf{Boosting Search-based Reasoning with \mosa{} as Aggregators}
\hspace{5pt}
% \revise{mosa as aggregators}
While vanilla RAP performs well, the inclusion of aggregators, particularly with \mosa{} as Aggregators, significantly enhances performance. 
For instance, augmenting RAP with a single-LLM aggregator yields an average improvement of +0.63\%. 
This improvement increases to +0.99\% when \mosa{} as Proposers is further enhanced with \mosa{} as Aggregators.


% \paragraph{Self-Consistency@$n$ As a Strong Baseline}
% \revise{merge with the next para: Search-based Methods v.s. Self-consistency}
% Under the single-LLM setting, self-consistency@$n$ performs almost on par with vanilla search-based reasoning (i.e., RAP), attaining slightly higher accuracy numbers on GSM8K and SVAMP and lower ones on MATH-500 and StrategyQA.

\textbf{Search-Based Methods Excel in Complex Reasoning Tasks}
\hspace{5pt}
% Under the single-LLM setting, self-consistency@$n$ performs almost on par with vanilla search-based reasoning (i.e., RAP), attaining slightly higher accuracy numbers on GSM8K and SVAMP and lower ones on MATH-500 and StrategyQA.
The best accuracy numbers on GSM8K and SVAMP, both exceeding 90\%, suggest these datasets are relatively easier. 
In contrast, MATH-500 and StrategyQA, with best scores around 55\% and 80\%, respectively, are more challenging. 
Notably, search-based methods demonstrate a clear advantage on these more complex datasets, underscoring their effectiveness in tackling intricate reasoning tasks.
Take StrategyQA as an example, the best accuracy number with non-search methods (Self-consistency) is 72.78\%, which is significantly lower than the best search counterpart (RAP) accuracy (75.69\%).



