



\section{Method}
\label{sec:method}

% We are interested in problems that require multiple steps of reasoning.
Search-based methods have been extensively used to tackle complex reasoning tasks, such as coding and mathematics, by breaking these problems into multiple search steps~\cite{zhou2023leasttomost,yao2024tree,hao-etal-2023-reasoning}.
% where the solving process is broken down into multiple search steps within the reasoning space~\cite{zhou2023leasttomost,yao2024tree,hao-etal-2023-reasoning}.
Our proposed paradigm is readily applicable to various search algorithms, with the Monte Carlo Tree Search (MCTS) algorithm~\cite{mcts-1,mcts-2} adopted as the search backbone in this work.
This section first introduces the baseline MCTS-based reasoning method with a single search agent~\cite{hao-etal-2023-reasoning,qi2024mutual} in $\S$~\ref{sec:method:background}, followed by our method, which leverages the expertise of multiple LLMs as search agents in $\S$~\ref{sec:method:mosa}.



\subsection{Baseline Framework}
\label{sec:method:background}

% \revise{hard to see the necessity of discussing RAP here}
% We adopt Reasoning-via-Planning (RAP, \cite{hao-etal-2023-reasoning}) as the baseline framework due to its simplicity, which minimizes interference with the evaluation of further improvements, and its widespread adoption and validation in previous studies~\cite{NEURIPS2023_65a39213,yao2024tree,koh2024treesearchlanguagemodel,zhang2024restmctsllmselftrainingprocess}.
% The background section will introduce RAP in more details.

\textbf{Overview}
\hspace{5pt}
Given a problem $x$ and a generator $\pi^*$, MCTS involves iteratively building a search tree starting from the root node $x$.
% Each node in the search tree represents a state $s_i$, and each directed edge from a parent node to a child node represents an action $a_i$. 
% During this process, \textit{Expansion} is defined as the step in which one or more child nodes are added to expand the tree according to the available actions~\cite{mcts_survey}. 
% Typically, each MCTS iteration involves four steps: \textit{Selection}, \textit{Expansion}, \textit{Simulation}, and \textit{Back-propagation}.
% This work focuses on \textit{Expansion}, as we target at how to expand the effective search space while maintaining validity.
We first define the state space $\mathcal{S}$ and the action space $\mathcal{A}$. 
In our case, each state $s_j\in \mathcal{S}$ captures the actions (i.e., reasoning steps) generated so far alongside a specific trajectory in the search tree, while each action $a_j \in \mathcal{A}$ represents the next reasoning step based on the current state and the type of action chosen. 
% As shown in Algorithm~\ref{alg:expand} and 
As shown in the upper part of Figure~\ref{fig:baseline}, given the selected node $s_i$ (i.e., the reasoning steps generated so far), a step of \textit{Expansion} essentially creates a set of child nodes.
A child node is created by concatenating $s_i$ with the new action, with that action being the next reasoning step generated by a search agent (e.g., an LLM) given $s_i$.



% \input{algorithms/general_mcts}






\begin{figure}[t]
% \vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.7\columnwidth]{figures/baseline_v2.pdf}}
\caption{\emph{Top}: An overview of the root node $s_0$ and its expanded child nodes. \emph{Bottom}: The detailed framework for generating new actions (i.e., sampling sub-questions and sub-answers).}
\label{fig:baseline}
\end{center}
\vskip -0.2in
\end{figure}

\input{algorithms/mosa}








% \revise{where is the discussion of action=concat(q,a).}
\paragraph{Action Space}
% Existing work normally considers two types of actions: (1) to propose a one-step thought, like ``\textit{Step-3. Check if the sum of the two digits equals 8.}''~\cite{yao2024tree}; (2) to propose a sub-question along with its answer, like ``\textit{Q3: Does the sum of the two digits equal 8? A3: The two digits are 3 and 5. We have $3 + 5 = 8$, so the answer is yes.}''~\cite{hao-etal-2023-reasoning}
We follow rStar~\cite{qi2024mutual} to define a comprehensive set of actions into MCTS-based LLM reasoning. 
The set of actions, $A = \{A1, A2, A3, A4, A5\}$, includes:
% $A1$: Propose a one-step thought; $A2$: Propose the remaining thought steps; $A3$: Propose the next sub-question along with its answer; $A4$: Answer the sub-question again; $A5$: Rephrase the question.
\vspace{-10pt}
\begin{itemize}
\setlength\itemsep{0.0em}
    \item $A1$: Propose a one-step thought;
    \item $A2$: Propose the remaining thought steps;
    \item $A3$: Propose the next sub-question along with its answer;
    \item $A4$: Answer the sub-question again;
    \item $A5$: Rephrase the question.
\end{itemize}
\vspace{-10pt}
% Among all those actions, we adopt $A3$ as the primary action, which consists of a \emph{sub-question} and a respective \emph{sub-answer}, i.e., $\text{action}_i \equiv \text{concat}(\text{sub\_question}_i,\text{sub\_answer}_i)$. 
Among these actions, we designate $A3$ as the primary action, comprising a \emph{sub-question} and its corresponding \emph{sub-answer}, i.e., $\text{action}_i \equiv \text{concat}(\text{sub\_question}_i,\text{sub\_answer}_i)$.
For instance, an action can be ``\textit{\#\#\# Sub-question 3: Does the sum of the previous two digits equal 8? \#\#\# Sub-answer 3: The two digits are 3 and 5. We have $3 + 5 = 8$, so the answer is yes.}''. 
We consider the other actions along with their effects in an ablation analysis ($\S$~\ref{sec:analysis:action_set}).
% Besides, we also include $A2$ into the action space, but since it is simply direct CoT and has little to do with step-wise search, our focus is still on $A3$.
We present a detailed illustration of generating new actions, i.e., combinations of sub-question \& sub-answer in Algorithm~\ref{alg:mosa-A_1}.
For a given state $s_i$,  the algorithm traverses all possible actions, where the final sub-answer for each sub-question is determined by a heuristic function, e.g., majority voting. 
% Starting from the selected node $s_i$, $n_q$ LLMs from the pool of LLMs are selected with replacement to generate $n_q$ sub-questions independently.
% For each sub-question, $n_a$ LLMs are similarly selected with replacement to generate $n_a$ sub-answer candidates independently.
% The $n_a$ candidates are then aggregated to reach a finalized sub-answer.
% The baseline method adopts majority voting as the aggregating method, i.e., self-consistency~\cite{wang2023selfconsistency}.
% Each pair of sub-question \& sub-answer forms a new action for $s_i$




\begin{figure*}[t]
% \vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.88\textwidth]{figures/ours_v4.pdf}}
\caption{
Generate three new actions using \mosa. \emph{Left}: Use \mosa{} to propose sub-questions and sub-answers. \emph{Right}: Use \mosa{} to aggregate candidate sub-answers.
}
\label{fig:ours}
\end{center}
\vskip -0.2in
\end{figure*}





\textbf{Reward Function}
\hspace{5pt}
Following \citet{hao-etal-2023-reasoning,qi2024mutual}, we consider a simple yet effective reward function: actions that frequently lead to correct final answers are assigned higher rewards. 
Specifically, $Q(s,a)$, the reward value for node $s$ created by action $a$, receives a positive reward if a trajectory containing node $s$ reaches a correct final answer, and no reward otherwise. 
Since the gold answer is not available during testing, the confidence given by \emph{majority voting} is regarded as an approximation of the reward value.


\textbf{MCTS Iterations}
\hspace{5pt}
% To obtain multiple candidate solutions, we perform multiple MCTS iterations per problem.
Typically, each MCTS iteration involves four steps: \textit{Selection}, \textit{Expansion}, \textit{Simulation}, and \textit{Back-propagation}.
% This work focuses on \textit{Expansion}, as we target at how to expand the effective search space while maintaining validity.
To balance exploration and exploitation, we adopt the widely-used \textit{Upper Confidence Bounds for Trees} (UCT) algorithm~\cite{mcts-1} for \textit{Selection}.
Formally, a node $s$ is selected to maximize:
\begin{small}
\begin{equation}
    \mathrm{UCT}(s,a) = \frac{Q(s,a)}{N(s,a)} + c \sqrt{\frac{\ln{N_{\mathrm{parent}} (s)}}{N(s,a)}}
\end{equation}
\end{small}
where $N_{\mathrm{parent}} (s)$ is the number of times the parent node of $s$ has been visited, $N(s,a)$ is the number of times node $s$ has been visited, and $c$ is a constant.
Once the node $s$ is selected, an \textit{Expansion} step is performed to add child nodes to $s$.
After that, starting from a random child node, a \textit{Simulation} is performed using the default rollout policy until a terminal node is obtained or a predefined maximum depth is reached.
The outcome of the simulation determines the reward, which is then propagated back up the tree during the \textit{Back-propagation} step. 
% When a predefined computational budget is reached or a certain number of valid leaf nodes are reached, the search is halted.
% Each leaf node is considered as a solution then.
Upon multiple iterations, we consider each leaf node as a solution. 
In this work, we focus on \textit{Expansion}, which aims to effectively expand the search space.
% Since this work focuses on how to effectively expand the search space, our main focus is on \textit{Expansion}.

% \input{algorithms/generate_action_vanilla}


% \subsection{Baseline}
% \label{sec:method:vanilla_mcts}
% By ``vanilla", we refer to the single-LLM scheme that has been widely adopted by existing work~\cite{hao-etal-2023-reasoning,qi2024mutual,zhang2024accessinggpt4levelmathematical}.
% We consider the following setups for 

\textbf{Sampling Diversity}
\hspace{5pt}
Applying stochastic sampling techniques in LLM generation is essential for introducing diversity to MCTS.
As presented in the lower part of Figure~\ref{fig:baseline}, given the selected state $s_0$, the sub-questions and the sub-answer candidates are all stochastically sampled using temperature scaling, top-$k$ sampling and nucleus sampling~\cite{Holtzman2020The}.
In $\S$~\ref{sec:analysis:diversity}, we empirically alter search diversity by manipulating generation temperature for single-LLM search.
% After that, for each sub-question, a set of sub-answers is sampled from the LLM and then gets aggregated into a finalized sub-answer.
% The baseline method adopts majority voting as the aggregating method, i.e., self-consistency~\cite{wang2023selfconsistency}.
% The sub-questions and their sub-answers are concatenated together to form the set of new actions whose diversity results from stochastic sampling.


% \subsection{Mixture-of-Search-Agents}
% \label{sec:method:mosa}




\subsection{Mixture-of-Search-Agents}
\label{sec:method:mosa}

% This work aims to broaden the reasoning space explored during step-wise, search-based reasoning.
% Traditional search-based methods often rely on a single search agent to enhance diversity by prompting an LLM with various stochastic sampling techniques.
% This strategy requires careful tuning of sampling parameters to balance the trade-off between diversity and performance.

Conventional Monte Carlo Tree Search (MCTS) methods utilizing a single model face two significant limitations:
(1) Encouraging search diversity while maintaining generation quality is challenging~\cite{tradeoff}, necessitating meticulous tuning of sampling parameters to balance the trade-off between these aspects;
(2) using heuristic metrics like majority voting to determine the final sub-answer can be less accurate when the model favors incorrect search directions.
To this end, we explore a simple yet effective alternative, Mixture-of-Search-Agents (\mosa), which employs multiple agents to perform search algorithms like MCTS and utilizes a neural function to refine the candidate step-wise outputs.
Firstly, leveraging the distinct distributions from different models intrinsically yields better generation diversity, alleviating the necessity for sampling parameters optimization.
Additionally, incorporating a neural function enhances the robustness of answer aggregation.

Figure~\ref{fig:ours} illustrates how our method generates three new actions starting from the current node $s_i$. 
Unlike vanilla single-model search, \mosa{} employs multiple agents (denoted by distinct colors) to explore diverse actions, such as sub-questions and sub-answers.
In the remainder of this section, we illustrate two roles performed by \textsc{MoSA} when generating new actions in MCTS.
Specifically, we will start with the straightforward improvement, \emph{\textsc{MoSA} as Proposers}, where multiple agents are involved for sampling actions;
then we will introduce the more intricate \emph{\textsc{MoSA} as Aggregators}, which extends the heuristic majority voting method to an aggregating phase where multiple LLMs read and refine the answers given by all.


% We hypothesize that the performances of search-based reasoning will increase if the search directions are guided by multiple different search agents, resulting in a more varied and comprehensive exploration of reasoning paths. 
% This diversity arises from the distinct preferences and strengths of various LLMs, which originate from their different pre-training and alignment tuning data and strategies.
% \emph{\textsc{\textit{MoSA}} is simple}: Instead of searching homogeneously with a single agent, \textsc{MoSA} employs multiple distinct LLMs as search agents. 
% \emph{\textsc{\textit{MoSA}} is effective}: The search directions are guided by diverse LLMs, resulting in a more varied and comprehensive exploration of reasoning paths. 
% This diversity arises from the distinct preferences and strengths of various LLMs, which originate from their different pre-training and alignment tuning data and strategies.
% Its effectiveness will be empirically verified in $\S$~\ref{sec:exp}.

% Figure~\ref{fig:ours} illustrates how our method generates three new actions starting from the current node $s_i$.
% It is worth clarifying that, by \textsc{MoSA}, we are not referring to the entire workflow shown in Figure~\ref{fig:ours}. 
% Instead, we consider \textsc{MoSA} as a way to adopt multiple LLMs as search agents in search-based reasoning, which takes the place of conventional approach that adopts a single LLM as the search agent.
% % \textsc{MoSA} can be applied to various action types in search, such as proposing a one-step thought, generating sub-questions, or reevaluating previous steps.
% % In $\S$~\ref{sec:exp:analysis}, we will show that \textsc{MoSA} yields large improvements upon being combined with a state-of-the-art (SoTA) MCTS method, rStar~\cite{qi2024mutual}, which employs a rich set of actions.
% In the remaining of this section, we will illustrate the various roles performed by \textsc{MoSA} when generating new actions in MCTS.
% Specifically, we will start with the straightforward improvement, \emph{\textsc{MoSA} as Proposers}; then we will introduce the more intricate \emph{\textsc{MoSA} as Aggregators}, which extends the heuristic majority voting method to an aggregating phase where multiple LLMs read and refine the answers given by all.


%\paragraph{\textsc{MoSA} as Proposers}
%% \label{sec:method:mosa:proposer}
%
%The left side of Figure~\ref{fig:ours} shows \emph{\textsc{MoSA} as Proposers}, where the the single search agent adopted by the baseline MCTS method (as in Figure~\ref{fig:baseline}) is replaced by \textsc{MoSA}.
%In more detail, we may consider two roles for \textsc{MoSA}: to propose sub-questions and to propose sub-answers.
%
%% \paragraph{Question Proposers}
%Generating a new search action begins with sampling a sub-question continuing from the current state $s_i$. 
%Intuitively, the sub-question proposing phase essentially controls the directions of the search because whatever follows within this search step is bounded by the scope of the current sub-question.
%Based on this intuition, we consider maintaining the independency among sub-questions, ensuring that the initial search direction pointed by each sub-question is not influenced by others.
%As shown in the upper-left part of Figure~\ref{fig:ours}, this effectively diversifies the sampled sub-questions as the same $s_i$ gets colored with distinct flavors after going through different agents. 
% ### Rationale for Edits

% 1. **Grammar and Coherence**: The phrase "the the single search agent" contains a repetition error. Also, "we may consider" is less assertive; "we consider" is more direct and appropriate for academic writing. The phrase "continuing from the current state $s_i$" is slightly awkward; "from the current state $s_i$" is clearer. The phrase "bounded by the scope of the current sub-question" is somewhat unclear; "constrained by the scope of the current sub-question" is more precise. The phrase "maintaining the independency" should be "maintaining the independence" for grammatical correctness. The phrase "the same $s_i$ gets colored with distinct flavors" is metaphorical and might be less formal; "the same $s_i$ is associated with distinct characteristics" is clearer.

% 2. **Logical Consistency**: The explanation about the roles of \textsc{MoSA} and how it diversifies the search process should flow logically. The sentence structure should ensure clarity in presenting the idea that \textsc{MoSA} proposes sub-questions independently, leading to diverse search directions.

% 3. **Sub-optimal Expressions**: The phrase "Intuitively" can be omitted for a more formal tone. Also, "ensuring that the initial search direction pointed by each sub-question is not influenced by others" can be rephrased to "ensuring that the initial search direction indicated by each sub-question is independent of others" for clarity and precision.

% ### Edited Version

\textbf{\textsc{MoSA} as Proposers to Diversify Actions}
\hspace{5pt}
% \label{sec:method:mosa:proposer}
The left side of Figure~\ref{fig:ours} shows \emph{\textsc{MoSA} as Proposers}, where the single search agent adopted by the baseline MCTS method (as in Figure~\ref{fig:baseline}) is replaced by \textsc{MoSA}. 
\textsc{MoSA} leverages multiple LLMs to enhance action diversity by fulfilling two sub-roles: multi-agent proposers that generate \emph{sub-questions} and \emph{sub-answers}.
% In more detail, we consider two sub-roles for \textsc{MoSA}:
% \paragraph{Question Proposers}

Generating a new search action begins with sampling a sub-question from the current state $s_i$. 
The sub-question proposing phase essentially controls the directions of the current search step because whatever follows within this step is constrained by the scope of that sub-question. 
Because of this, we consider maintaining the independence among sub-questions, ensuring that the initial search direction indicated by each sub-question is independent of others. 
As shown in the upper-left part of Figure~\ref{fig:ours}, this effectively diversifies the sampled sub-questions as the same $s_i$ is colored with distinct characteristics after going through different LLMs.

After the initial search directions are created, the target is to comprehensively explore each search direction. 
To achieve this, each sub-question is answered by various LLMs, generating a diverse set of candidate sub-answers. 
These candidates are then aggregated to reach a finalized sub-answer.
A simple yet effective aggregating method is majority voting, leveraging the principle of self-consistency~\cite{wang2023selfconsistency}.
% Notably, each sub-question is independently generated by an agent, ensuring that the 
% Additionally, the sub-question proposing phase essentially controls the directions of the search because whatever follows within this search step is bounded by the scope of the current sub-question. 
% For example, if the sampled sub-question asks about the largest prime number smaller than 100, it is unlikely to sample a float or a number larger than 100 as the sub-answer from a well-performing LLM. Due to their control over search directions, an educated guess would be that question samplers play a critical role in increasing search diversity, which we will empirically verify in $\S$~\ref{}.



% Each sub-question is then answered by all LLMs, producing a diverse set of candidate sub-answers. 
% Specifically, each search direction initiated by a sub-question is extensively explored by different search agents. 
% To aggregate this diverse set of candidate sub-answers, a simple yet highly effective solution is majority voting, that is, self-consistency.




% \paragraph{Answer Proposers}





\textbf{\textsc{MoSA} as Aggregators for Collaborative Refinement}
\hspace{5pt}
% \revise{to be revised}
% \label{sec:method:mosa:aggregator}
We introduce a neural function, termed ``aggregator'', to mitigate the limitation of majority voting for selecting the final answer. 
An aggregator leverages the innate capability of the LLM to critique, compare and aggregate multiple answers into a final answer. 
Specifically, we prompt each LLM to consolidate all responses into an aggregated answer (see Appendix~\ref{sec:appendix:prompt} for detailed prompts), resulting in a new set of aggregated answers as illustrated in the right section of Figure~\ref{fig:ours}.
The underlying intuition is that this aggregation process enhances the likelihood of producing correct answers by facilitating comparisons among different responses, thereby increasing the overall success rates for correct answers under majority voting.
We present an example below to illustrate this intuition.

In the previous section, we consider majority-voting after obtaining candidate sub-answers from diverse proposers.
% While simple, this heuristic strategy can be enhanced by employing \textsc{MoSA} as aggregators to improve sub-answer aggregation.
Let us consider a sub-question that requires 3 sub-answers to be generated and an \textsc{MoSA} component consisting of 3 distinct LLMs.
We simply assume that each LLM proposes one sub-answer.
If there are $k$ LLMs that are proficient at this sub-question and the other $3-k$ are not, then it is likely that we would have $k$ \emph{good} sub-answer candidates and $3-k$ \emph{bad} candidates\footnote{For clarity in illustrating our motivation, we simplify the correctness of candidate sub-answers into two groups: \emph{good} and \emph{bad}. This abstraction helps explain the role of aggregators in improving answer correctness, though actual correctness exists on a spectrum depending on task complexity and evaluation criteria.}.
With majority voting, a \emph{bad} finalized answer is likely if $k\geq2$.

Now we turn to use \textsc{MoSA} to aggregate the candidate sub-answers and then include the aggregated sub-answers into majority-voting.
The inputs for all three aggregator LLMs are the same, which concatenates the sub-question and all the three candidate sub-answers.
We hypothesize that a \emph{bad} aggregator that receives at least a \emph{good} sub-answer could yield a sub-answer that is at least better than its original \emph{bad} sub-answer.
Such a hypothesis has been empirically verified in the case of instruction following by \citet{moa}, who showed that many LLMs can generate higher-quality responses by building upon outputs from other LLMs.
Thus, if the two \emph{bad} aggregator LLMs can learn from the \emph{good} sub-answer and generate \emph{good} aggregated sub-answers, then we will have $4$ \emph{good} sub-answers and $2$ \emph{bad} ones, which lead to a \emph{good} finalized sub-answer.