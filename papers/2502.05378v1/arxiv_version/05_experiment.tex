
\section{Experiments}

\subsection{Experimental Setup}

\noindent \textbf{Dataset and simulation setup.} 
%
We evaluate our model on the Matterport3D~(MP3D) dataset~\citep{Matterport3D} and our own AiMDoom dataset.

For MP3D, we use the same setting as prior work~\citep{yan2023active} for fair comparison. The input posed depth images have a resolution of $256 \times 256$ with a horizontal field of view~(hFOV) of $90^\circ$. 
The mobile agent starts in the traversable space at a height of $1.25m$ and chooses its next camera pose by moving forward by $6.5cm$ or turning left/right by $10^\circ$. 
Depending on the size of each scene, the agent can take a maximum of 1000 or 2000 steps. 
We focus only on single-floor scenes following \cite{yan2023active}  with 10 and 5 scenes in training and evaluation respectively.

For AiMDoom, we utilize a 70/30 train/test split for scenes in each difficulty level. 
The input RGB-D images are rendered at the resolution of $456 \times 256$ with hFOV of $90^\circ$.
The agent navigates in a traversable space of height $1.65m$.
The moving step includes 4 position movements (move forward, backward, left, or right by $1.5m$) and 8 rotation movements (turn left or right by increments of $45^\circ$, covering the full $360^\circ$).
For dense reconstruction, all methods capture three additional images between adjacent poses using linear interpolation.
The maximum steps for Simple, Normal, Hard, and Insane levels are set to 100, 200, 400, and 500 respectively, to adapt to their different complexity. 


\input{arxiv_version/tables/results_doom}
    
\noindent \textbf{Evaluation metrics.}
%
We follow prior works~\citep{chen2024gennbv,guedon2023macarons} and adopt two key metrics to evaluate the performance of active 3D reconstruction:
(1) \textbf{\textit{Final Coverage}} measures the scene coverage at the end of the trajectory,
and (2) \textbf{\textit{AUCs}} evaluates the efficiency of the reconstruction process by calculating the area under the curve of coverage over time.
The surface coverage is computed using ground truth meshes, consistent with prior work~\citep{guedon2023macarons}. %\vincentrmk{ussing the code from macarons?} \shiyaormk{No, but very similar}
We evaluate five trajectories per scene using identical random initial camera poses for different methods. 
We report the mean and standard deviation for each metric across all testing trajectories.

For a fair comparison with prior work in MP3D, we employ another set of metrics to evaluate coverage: (1) \textbf{\textit{Comp. (\%)}}, the proportion of ground truth vertices within $5cm$ of any observation, and (2) \textbf{\textit{Comp. ($cm$)}}, the average minimum distance between ground truth vertices and observations. 

% The goal of active 3D reconstruction is to gain the most comprehensive understanding and coverage of the current environment with limited movement. To this end, we compare: (1) \textbf{\textit{Final Coverage (\%)}}—the extent of coverage obtained after a limited camera trajectory length, as utilized in previous studies \cite{chen2024gennbv, yan2023active, rlnbv}; (2) \textbf{\textit{AUCs (\%)}}—a measure of the efficiency of the reconstruction process in terms of coverage ratio, as prior work \cite{guedon2023macarons, guedon2022scone, zeng2020pc} did. Following \cite{guedon2023macarons}, we use ground truth meshes for computing the surface coverage.


\noindent \textbf{Implementation details.}
% Our complete model (the encoder and the two decoders) has 49.96M parameters. 
Our model takes a stack of $K=4$ projected 2D images and one previous trajectory projected image as inputs, each with a resolution of $256 \times 256$ covering a $40m \times 40m$ exploration area centred on the camera’s current position.
The extracted feature $e_{c_t}$ from the encoder is of size $16 \times 16 \times 1024$. The output value map $M_{c_t}$ is of size $64 \times 64 \times 8$ and an obstacle map of $256 \times 256 \times 1$, both representing the same $40m \times 40m$ area.
The model is trained for at most $N=15$ iterations, with the first $N_e=1$ iterations using easier samples and $S_n=2$ trajectories per scene.
For subsequent iterations, we use all samples and reduce the trajectory count to $S_n=1$ per scene. Each trajectory has a length of 100 steps and starts at a random location.
During the first data collection iteration, we randomly sample 1,000 validation examples from memory and exclude them from training.  
Gradient accumulation is used in training which results in an effective batch size of 448. The learning rate is set to 0.001 and is decayed by a factor of 0.1 if the validation loss plateaus. We apply early stopping to terminate training when validation loss no longer decreases. 
The training is performed on a single NVIDIA RTX A6000 GPU, with an average completion time of 25 hours.


% We conduct the training of our model using a single NVIDIA RTX A6000 GPU, while implementations of other methods utilize four NVIDIA Tesla V100 GPUs. As described in Sec.~\ref{learning-stragety}, to train our model, we use a domain-shift fashion. The iterative process of our method is structured into two distinct phases: data collection and training. In each data collection phase, we sampled a trajectory of length 100 steps from each training scene at a random location, and each trajectory contained several sub-trajectories from Sec.~\ref{sub: decision}. 

% We implemented our method with PyTorch \cite{paszke2019pytorch}, PyTorch3D \cite{pytorch3d} and Trimesh \cite{trimesh}. 


\subsection{Comparison with State of the Art Methods}

\noindent \textbf{MP3D.}
%
We compare our method with five baselines on the MP3D dataset, including: 
1) \emph{Random}, which randomly selects a camera pose among all candidates for the next step;
2) \emph{Frontier-based Exploration~(FBE)~\citep{fbe}}, which heuristically moves the agent to the nearest frontier; 
3) \emph{OccAnt~\citep{occant}}, which predicts the occupancy status of unexplored areas and rewards the agent for accurate predictions; 
4) \emph{UPEN~\citep{upen}}, which utilizes an ensemble of occupancy prediction models to guide the agent towards paths with the highest uncertainty; 
5) \emph{ANM~\citep{yan2023active}}, which guides exploration through a continually-learned neural scene representation. 
%
Results in Table~\ref{tab:comparison3} show our NBP performs best, with a 6.23 absolute gain for the completion ratio compared to the state-of-the-art ANM~\citep{yan2023active} model.
% We provide more visualizations on MP3D in the supplementary material.

% \input{tables/results_doom}

\input{tables/results_mp3d}
\noindent \textbf{AiMDoom.}
%
The proposed AiMDoom dataset is more challenging than MP3D dataset for active 3D mapping.
We benchmark our approach against state-of-the-art Next-Best-View (NBV) approaches, including: 
\textit{{SCONE \citep{guedon2022scone}}} which employs volumetric integration to sum the potential visibility points for each candidate camera pose in the subsequent step and is trained using supervised learning; and
\textit{MACARONS~\citep{guedon2023macarons}} which quantifies the coverage gains of potential next camera poses to select the best one and utilizes a self-supervised online learning paradigm.
Both approaches select the next camera pose in a greedy manner.
Unfortunately, we were unable to include UPEN~\citep{upen} and ANM~\citep{yan2023active} in our comparison.
These methods rely on the navigation policy DD-PPO~\citep{ddppo} trained on their environments~\citep{habitat19iccv}, which requires extensive GPU hours and thus is infeasible to retrain it on our dataset.
However, we implemented FBE~\citep{fbe} on our dataset, a recognized strong baseline in reconstruction and exploration tasks. 


% The methods we compared against include: 1) \textit{Random:} We random sample one camera pose among 32 candidates as the next step. 2) \textit{Frontier-based Exploration (FBE) \cite{fbe}:} To ensure a fair comparison, we adapt the FBE method to align with our approach. At each decision time $t$, we select a target camera pose at the frontier between known and unknown space, within the same radius $r$ as our observation window from the current camera position. We employ Dijkstra's algorithm on the current reconstructed point cloud to generate a trajectory to this target. For each node along the path, we orient the camera to face the surrounding boundaries. 3) \textit{{SCONE \cite{guedon2022scone}:}} SCONE employs volumetric integration to sum the potential visibility points for each candidate camera pose in the subsequent step, which is trained using supervised learning. 4) \textit{MACARONS \cite{guedon2023macarons}:} MACARONS focuses on quantifying the coverage gains for potential next camera poses and selecting the best one which utilizes a self-supervised online learning paradigm.

% \shizhermk{Add some analysis, e.g., 1) insane < hard ... < simple as expected; 2) why SCONE MACARONS performs much worse, even compared to FBE; 3) link to the qualitative results and maybe show the failures cases}
\input{arxiv_version/figures/viz_results}

As shown in Table~\ref{tab:doom_main_experiments}, our method significantly outperforms the baselines across all metrics on four levels of AiMDoom. 
While NBV approaches such as SCONE and MACARONS excel in outdoor or single-object scenarios, their performance deteriorates in complex indoor environments. 
As illustrated in Figure~\ref{fig:combined_results}, MACARONS struggles to escape local areas due to its short-term focus. It only selects the next best pose in nearby regions, and once these areas - such as the interior of a single room - are fully reconstructed, it has difficulty moving out of the room to explore under-explored, distant regions.
In contrast, our approach overcomes this limitation by incorporating long-term goal guidance to determine the next-best path.
%
In addition, our method surpasses the strong baseline FBE. Although FBE enables better exploration compared to state-of-the-art NBV methods on our dataset, its simple heuristic of moving to the nearest frontier leads to sub-optimal scene reconstruction as it lacks strategic planning for efficient coverage. 


Despite the superior performance of our model, the results in hard and insane environments are still unsatisfactory, highlighting the significant challenges posed in our dataset.


% For instance, navigating a long corridor requires not only exploration but also an exit strategy, a complexity that NBV methods, with their focus on immediate next steps, struggle to address effectively. 
% This short-term focus introduces an element of randomness in transitioning between areas, particularly when optimal next views happen to be outside the immediate indoor space, such as through windows or doors. 
% In contrast, our approach evaluates the value of surrounding areas, thus incorporating both short-term gains and long-term benefits. This strategy significantly enhances our spatial exploration capabilities as illustrated in Figure~\ref{}.

% Moreover, our method surpasses FBE in two critical aspects: we achieve more precise valuation of long-term goals, and we utilize predicted obstacle maps for path planning. These advancements result in substantially more efficient mapping processes.

% This comprehensive approach, combining precise long-term goal evaluation with obstacle-aware path planning, positions our method as a more robust and efficient solution for complex indoor exploration tasks, addressing the limitations of both traditional NBV and area-based methods.

% While SCONE and MACARONS, the current state-of-the-art NBV approach, excel in outdoor or single-object scenarios due to their relative simplicity and lack of environmental constraints, they face significant challenges in indoor settings. Indoor environments, unlike their outdoor counterparts, present a multifaceted challenge: they are characterized by omnidirectional object presence and demand more extensive planning horizons. For instance, navigating a long corridor requires not only exploration but also an exit strategy, a complexity that NBV methods, with their focus on immediate next steps, struggle to address effectively. This short-term focus introduces a element of randomness in transitioning between areas, particularly when optimal next views happen to be outside the immediate indoor space, such as through windows or doors. Our approach and FBE evalutate the value of surrounding areas, thus incorporating both short-term gains and long-term benefits. This strategy significantly enhances our spatial exploration capabilities. Moreover, our method surpasses FBE in two critical aspects: we achieve more precise valuation of long-term goals, and we utilize predicted obstacle maps for path planning. These advancements result in substantially more efficient mapping processes.
% This comprehensive approach, combining precise long-term goal evaluation with obstacle-aware path planning, positions our method as a more robust and efficient solution for complex indoor exploration tasks, addressing the limitations of both traditional NBV and area-based methods.
% \shizhermk{Add some analysis, e.g., 1) insane < hard ... < simple as expected; 2) why SCONE MACARONS performs much worse, even compared to FBE; 3) link to the qualitative results and maybe show the failures cases}




\input{figures/ablation_range}
\subsection{Ablation Study}

In this section, we perform ablation experiments to demonstrate the effectiveness of different components in our model. All the experiments below are conducted on the Normal level of AiMDoom. 

\noindent \textbf{Spatial range of long-term goal.} 
We compare the impact of different spatial ranges for the prediction of the value map $M_{c_t}$ and obstacle map $O_{c_t}$, which in turn determines the maximum distance of the long-term goals $c_g$. 
Specifically, we experiment with map sizes of $20m \times 20m$ to $50m \times 50m$.
The results are presented in Figure~\ref{fig:ablation_range}.
When the value map covers a smaller area, the goal $c_g$ is close to the agent's current position, leading to behaviour similar to existing NBV methods that struggle with exploration. 
On the other hand, if the map size is too large, predicting $M_{c_t}$ and $O_{c_t}$ becomes much more challenging. 
Our findings demonstrate that selecting an appropriate spatial range for the value map is crucial for balancing exploration efficiency and prediction accuracy.



% Our first experiment examined the effect of varying the camera score map's prediction range while keeping the input size constant. Results show optimal performance when the value map's coverage area matches the encoded area.
\input{tables/ablation_gt_partial}

\noindent \textbf{Oracle obstacle map.}
In Table~\ref{tab:ablation_gt}, we replace the predicted obstacle map with the ground truth obstacle map for path planning during inference, while maintaining to use the predicted value map for long-term goals.
Using the oracle obstacle map improves the performance by 0.074 on final coverage and 0.054 on AUCs, but is far from perfect.
This suggests that the major bottleneck is the value map prediction.

\input{tables/ablation_multi_partial}


\noindent \textbf{Multi-task training.} 
We also explore the influence of multi-task learning in predicting the value map $M_{c_t}$ and the obstacle map $O_{c_t}$. For comparison, we train two separate models that use the same input to predict $M_{c_t}$ and $O_{c_t}$ respectively. 
The results show that multi-task learning improved the precision of obstacle prediction to 0.805, exceeding the 0.754 achieved by single-task learning. Table~\ref{tab:ablation_multi_unseen} further demonstrates that multi-task learning achieves better performance, indicating that the two tasks complement each other to enhance learning.
% As shown in Table~\ref{tab:ablation_multi_unseen}, multi-task learning yields better performance compared to training independently, indicating that the two tasks are complementary with each other to enhance learning.






