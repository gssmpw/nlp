\section{Details of mapping process encoder}

We provide more details of the Mapping Process Encoder of our proposed approach in this section.

The mapping encoding is predicted from both the current reconstruction progress and historical trajectory data. At each time step $t \geq 0$, we construct and refine a surface point cloud $\mathcal{P}_t$ by integrating information from newly captured depth map $D_t: \Omega \to \mathbb{R}^+$ and merging it with our existing reconstructed point cloud. For each camera pose $c_t = (c^\text{pos}_t, c^\text{rot}_t)$, we transform the corresponding depth map $D_t$ into a set of 3D points. This transformation makes use of the camera's intrinsic matrix $K \in \mathbb{R}^{3\times3}$ and the pose matrix $T_t \in SE(3)$, derived from the 6D pose $c_t$:
%
\begin{equation}
\mathbf{p}_{surface}(u, v) = T_t \cdot \left( D_t(u, v) \cdot K^{-1} \cdot \begin{bmatrix} u \ v \ 1 \end{bmatrix}^\top \right), \quad (u, v) \in \Omega \> ,
\end{equation}
%
where $\Omega \subset \mathbb{R}^2$ represents the domain of the depth map. We accumulate points over time:
%
\begin{equation}
\mathcal{P}_t = \mathcal{P}_{t-1} \cup \{\mathbf{p}_{surface}(u, v) \mid (u, v) \in \Omega, D_t(u, v) > 0\} \> .
\end{equation}

To enhance scalability and generalization, we introduce a slice mapping approach that transforms the point cloud into a set of $K$ images. We begin by filtering the point cloud based on the camera's position:
%
\begin{equation}
\mathcal{P}_{c_t}^f = \{\mathbf{p} = (p_x, p_y, p_z) \in \mathcal{P}_t \mid |p_x - x_{c_t}| \leq r \text{ and } |p_z - z_{c_t}| \leq r\} \> ,
\end{equation}
%
where $r$ is the radius of our observation window and $(x_{c_t}, y_{c_t}, z_{c_t})$ is the current camera position. We then divide $\mathcal{P}_{c_t}^f$ into $n$ equal vertical slices along the Y-axis, $y_{min}$ and $y_{mmax}$ come from a defined exploration bounding box, as \cite{guedon2023macarons} did:
%
\begin{equation}
\mathcal{S}_{{c_t},j} = \{\mathbf{p} = (p_x, p_y, p_z) \in \mathcal{P}_{c_t}^f \mid y_{min} + (j-1)h_\text{slice} \leq p_y < y_\text{min} + jh_\text{slice}\} \> ,
\end{equation}
%
where $h_\text{slice} = (y_\text{max} - y_\text{min}) / n$ and $j \in {1, \ldots, n}$. Each slice $\mathcal{S}_{{c_t},j}$ is mapped to an image $I_{{c_t},j}$ of size $H \times W$ using a projection function $\phi: \mathbb{R}^3 \to \mathbb{R}^2$:
%
\begin{equation}
\phi(\mathbf{p}) = \left(\left\lfloor\frac{(p_x - x_{c_t} + r) \cdot W}{2r}\right\rfloor, \left\lfloor\frac{(p_z - z_{c_t} + r) \cdot H}{2r}\right\rfloor\right) \> .
\label{projection}
\end{equation}

This projection centres the camera in the middle of the image. Finally, we calculate the point density for each pixel $(u, v)$ in the image $I_{{c_t},j}$:
%
\begin{equation}
I_{{c_t},j}(u, v) = \int_{\mathcal{S}_{{c_t},j}} \delta(\phi(\mathbf{p}) - (u, v)) d\mathbf{p} \> .
\end{equation}
%
Here, $\delta(\cdot)$ is the Dirac delta function and $\mathbf{p}$ represents points from the slice $\mathcal{S}{{c_t},j}$. This process yields $n$ density images ${I_{{c_t},1}, \ldots, I_{{c_t},n}}$ for each time step $t$, effectively transforming 3D point cloud data into 2D representations.

In addition, we apply a similar approach to project the camera's historical trajectory, resulting in a single 2D image. We filter the camera's historical positions based on their proximity to the current camera position in the XZ-plane, using the same threshold $\tau_{xz}$:
%
\begin{equation}
\mathcal{C}_t^f = \{c^{pos}_k = (x_k, y_k, z_k) \mid k < t, |x_k - x_t| \leq \tau_{xz} \text{ and } |z_k - z_t| \leq \tau_{xz}\} \> .
\end{equation}

We then map these filtered positions onto a single image $H_{c_t}$ of the same size $H \times W$:
%
\begin{equation}
H_{c_t}(u, v) = \sum_{c^{pos}_k \in \mathcal{C}_t^f} \delta(\phi(c^{pos}_k) - (u, v))
\end{equation}
%
This results in a single-density image $H_{c_t}$ representing the camera's historical trajectory near its current position. To synthesize the information obtained, we define a set $\mathcal{E}_{c_t}$ encapsulating the entirety of the current exploration embedding: $\mathcal{E}_{c_t} = \{I_{{c_t},1}, \ldots, I_{{c_t},n}, H_{c_t}\}$.

