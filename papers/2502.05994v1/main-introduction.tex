% Score-based diffusion models provide a powerful mechanism to generate new samples from complex distributions through a two-step process~\citep{song2021scorebased}. First, they learn the score of the data distribution, defined as the gradient of the log-probability density function, at every time step $t$ of the noise process. Second, using this score, they iteratively refine noisy inputs to generate new samples from the data distribution.


\begin{figure*}[ht!]
\centering
\includegraphics[width=\textwidth]{plots/figure_diffusion_30_Jan.pdf}
\caption{\textbf{Illustration of the approach using Diffusion Models for Inverse Problems in the Exponential Family.} By leveraging the posterior score $\nabla_{\mathbf{x}_t} p_{\mathbf{x}_t|\mathbf{y}}(\mathbf{x}_t|\mathbf{y})$, a reverse stochastic differential equation (SDE) can be solved to generate posterior samples of the latent variable $\mathbf{x}_0$ from noise. Posterior samples of the parameter $\boldsymbol{\theta}$ are obtained by applying a deterministic inverse link function. The prior score function, $\nabla_{\mathbf{x}_t} p_{\mathbf{x}_t}(\mathbf{x}_t)$, is estimated using a neural network, following established approaches. A novel method is introduced to estimate the likelihood score function, $\nabla_{\mathbf{x}_t} p_{\mathbf{y}|\mathbf{x}_t}(\mathbf{y}|\mathbf{x}_t)$, leveraging the \textit{evidence trick} in combination with amortized variational inference.  The Figure illustrates the inference of a spatially inhomogeneous Poisson process where the intensity is as intricate as an ImageNet image.}
\label{fig-intro-summary-figure}
\end{figure*}


Score-based diffusion models offer a powerful framework for generating new samples from complex data distributions through a two-step process~\citep{song2021scorebased}. First, the score of the data distribution is estimated by learning to denoise corrupted samples. Second, leveraging this learned score, the noisy inputs are iteratively refined to produce new samples that align with the data distribution.


This capability is particularly advantageous in solving inverse problems. Given access to noisy observations $\mathbf{y} \in \mathbb{R}^{d_y}$, we are interested in inferring a latent signal $\mathbf{x}_0 \in \mathbb{R}^{d_x}$ that generated the observations by sampling from the posterior distribution $p_{\mathbf{x}_0|\mathbf{y}}(\mathbf{x}_0|\mathbf{y})$.
Diffusion models do not require the prior distribution $p_{\mathbf{x}_0}(\mathbf{x}_0)$ to be analytically specified or explicitly parameterized, they rely only on the ability to sample from it.
Therefore, they can be trained to learn the score of highly complex prior distributions, for instance where $\mathbf{x}_0$ are images from ImageNet. 
This distinguishes them from traditional methods like Markov Chain Monte Carlo (MCMC), which necessitates evaluating the prior density.


Despite these advantages, current diffusion-based methodologies are predominantly confined to Gaussian likelihoods (e.g.,~\citet{kadkhodaie2021, kawar2021, kawar2022, chung2023, song2023pseudoinverseguided, boys2024, rozet2024}), limiting their applicability to scenarios such as image deblurring or denoising. 
However, many scientific applications involve likelihoods that deviate significantly from Gaussian distributions. For instance, event data (e.g., COVID-19 case counts) is naturally modeled using a Poisson distribution, while proportion data (e.g., prevalence rates) aligns with a Binomial distribution. 
% These deviations present significant challenges, as existing diffusion-based methods are ill-equipped to handle such complexities.

One major obstacle to extending diffusion models to non-Gaussian likelihoods lies in the intractability of the posterior distribution score at diffusion time $t$, $ p_{\mathbf{x}_t | \mathbf{y}}(\mathbf{x}_t | \mathbf{y})$. Specifically, the posterior distribution score incorporates both the the prior score at time $t$, $ p_{\mathbf{x}_t}(\mathbf{x}_t)$, which can be trained, and the likelihood score at time $t$, $p_{\mathbf{y} | \mathbf{x}_t }(\mathbf{y}| \mathbf{x}_t )$. 
The latter is generally intractable because it depends on an integral over the reverse diffusion process density, which is itself difficult to compute. 
A common approach, when the observations follow a Gaussian distribution, is to approximate the reverse diffusion process and derive a closed-form expression for the likelihood score.
This approach is used by methods like Diffusion Posterior Sampling (DPS)~\citep{chung2023}, which proposes a delta function approximation centered at the Tweedie's posterior first moment, and Tweedie Moment Projected Diffusions~\citep{boys2024}, which employed a Multivariate anisotropic Gaussian distribution approximation. 


These methods have significant limitations. They often struggle to accurately quantify uncertainty in the reverse diffusion process and they rely on Tweedie’s formula~\citep{Efron2011}, which exhibits high variance at high noise levels (see Section 1.2 of~\citet{target_score_matching}). 
Additionally, they cannot accommodate non-Gaussian observations. While~\citet{chung2023} proposed using Gaussian approximations for non-Gaussian distributions --- such as approximating a Poisson distribution with a Gaussian --- this approach is highly unstable for small values and entirely inapplicable to certain distributions.
These limitations underscore the need for robust methodologies that extend the applicability of diffusion models to non-Gaussian settings, ensuring both stability and accurate representation of diverse likelihoods.

To address these limitations, we introduce an approach that we term, the \textit{evidence trick}. By leveraging the properties of the exponential family and employing an amortized variational approach, we extend the applicability of diffusion models for inverse problems to any likelihood distribution within the one-parameter exponential family, which includes the Poisson and Binomial distributions. 
To approximate the likelihood score, $p_{\mathbf{y}|\mathbf{x}_t}(\mathbf{y}|\mathbf{x}_t)$, we use the conjugate prior distribution as a variational approximation for the reverse diffusion process. This reformulation makes the integral over the reverse process tractable and accounts for parameter uncertainty. In contrast to previous approaches, we derive an objective to optimize the variational distribution that is independent of the reverse process expectation, bypassing the need for Tweedie's formula. Figure~\ref{fig-intro-summary-figure} graphically summarises our methodology.


We leverage our methodology to introduce a \textit{``Score-Based Cox process''}, a discrete Cox process where the intensity is modeled using a score-based diffusion process. We demonstrate that our model can effectively capture rough and intricate intensity patterns, including those as complex as image samples from the ImageNet database. Furthermore, we show that our methodology can address real-world scientific challenges by performing competitively with the current state-of-the-art in predicting malaria prevalence estimates in Sub-Saharan Africa.

% In summary, this paper makes the following contributions:
% \begin{itemize}
%     \item We extend score-based diffusion models for inverse problem methodologies to distributions within the one-parameter exponential family (e.g., Poisson, Binomial), enabling their application to a wider range of scientific problems.
%     \item We demonstrate that there exists a variational distribution that can optimally approximate the reverse process through an objective function that is independent of expectations over the reverse process, thereby avoiding reliance on Tweedie’s formula.
% \end{itemize}



% To the best of our knowledge, no prior work has systematically addressed inverse problems with non-Gaussian likelihoods using diffusion models. The rest of this paper is organized as follows: [insert structure overview].



% diffusion model using score based, show equaion o the score
% diffusin model for inverse bayesian problem with equation of the score. the score of the prior and the score of the likelihood. 
% while it seems trighforwar dte score of the likelihood is not tractable because it depends on t. so you need to approximate it. 
% We can obtain sample from the posterior. And the keu difference with MCMC is that the priro should not have to be evaluated but only sampled from. With their flexibility, diffusion models replace handcrafted priors on the latent signal with pretrained and
% strong empirical priors. For example, given a latent signal x0 (say a face image), we can train a diffusion model to sample from the prior p(x0). . In other words, we could sample prior from images, which have an underlying complex probability distribution, but it does not have to be tractable to do inference with it. 
% current methodologies have heavly been focused on the gaussian likelihood case, ss it was applied to images debluring xxx , xx. However this methodlogies could be extended to more scienific problem msot of which do not have a gaussian lileihood. Thi could include event data (cases of COVID-19 in epidemiology) or proportion data (prevalence of COVID-19 in epiedemiogloy). These data are often mdoelling with a latent representation, either a gaussian process or splines. But these methods are prohbitive computational cost or flexibility. Additionaly they require to have a clost form prior. This methodology could allow to use represnetation of latent function where the prior is specified by images or other very complex distribution one can sample from, and could model the latent funciton of other type of data. In this paper we show that the methodology can be extended to any data for which the distibution belongs to the one parameter exponential family, this includes the poisson, binomial as well as the gaussian witha fixed variance. Note that in the preivous application, the gaussian had always a fixed variance as the latent function is always placed on the mean. In our best knowledge,  no other methodologies have been apploeid to a broader class of distirbution except if the distribution was approximated with a gaussian, for example Chung proposed an approach to use Poisson distirbuted data, however this considers the data as a gaussian approxiamation, which is incorrect for small poisson values. 


% This paper is devoted to developing novel methods to solving inverse problems, given a latent (target) signal x0 ∈ R
% dx , noisy observed data y ∈ R
% dy , a known linear observation map H, and a pretrained diffusion prior.

% the likelihodo dependent of the process can be shown to be an integral of the likelihood and the reverse sde process, which is untractbale. all methods proposed have proposed to employ a gaussian approximation on the reverse sde, which will be more detailed in the background. this gaussian approximation is then parametrized to be as close as possible to the true distirbution  relying on tweedie formula which can be highly unstabel because it is divided by alpha which is close to 0 near t. Instead we propose the evidence trick, which place the conjugate distirbution of the likelihood as a variation distribution and allow the integral to be available in close form. Then to match the best distribution with amortized variational inference by showing that the objective functin can be reduced to not be dependent on the untractable reverse process.



% Things we need to explain in the intro
% \begin{itemize}
% \item Why is it important to extend the current methodologies beyond the normal normal case?
% \item Why is it important to have parametric models (e.g. intensity based models) instead of fully non-parametric models (e.g. intensity free models)? 
% \end{itemize}
% We notice that Chung proposed an approach to use Poisson distirbuted data, however this considers the data as a gaussian approxiamation, which is incorrect for small poisson values.

