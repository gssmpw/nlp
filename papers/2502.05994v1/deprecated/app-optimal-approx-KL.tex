Our next result establishes that the natural parameters of the optimal mean-field variational approximation to  $p_{\mathbf{x}_0\vert \mathbf{x}_t}(\mathbf{x}_{0}\vert \mathbf{x}_{t})$, i.e., the minimizers of the KL divergence, can be determined by solving $d$-independent systems of algebraic equations. 
% These equations depend on the sufficient statistics of  $q_{\theta_i\vert \mathbf{x}_t}(\theta_{i}\vert \mathbf{x}_{t})$, the link function $g$, and the moments of the posterior distributions $p_{\mathbf{x}_0\vert\mathbf{x}_ t}(\mathbf{x}_{0}\vert\mathbf{x}_{t})$.
\begin{theorem}[Optimal approximation to $p_{\mathbf{x}_0\vert \mathbf{x}_t} (\mathbf{x}_{0}\vert \mathbf{x}_{t})$]
\label{thm-minimizer-KL-divergence}  
The optimal set of natural parameters $(\hat{\boldsymbol{\eta}}_{i})_{i=1}^{d}$ which minimizes the KL divergence of $p_{\mathbf{x}_0\vert \mathbf{x}_t}$ from $q_{\mathbf{x}_0\vert \mathbf{x}_t}$ in~\eqref{eq-kl-divergence-exp} corresponds to the solution of the following $d$-independent systems of algebraic equations
\begin{equation}
\label{eq-optimal-approx-system}
     \mathbb{E}_{q_{\boldsymbol{\theta}\vert \mathbf{x}_t}}[\mathbf{T}(\boldsymbol{\theta})] = \mathbb{E}_{p_{\mathbf{x}_{0}\vert \mathbf{x}_t}}[ \mathbf{T}(g^{-1}(\mathbf{x}_{0}))].
\end{equation}
\end{theorem}
The proof of Theorem \ref{thm-minimizer-KL-divergence} is postponed to Appendix \ref{sec:proof_thm-minimizer-KL-divergence}. 
The expectations on the LHS of \eqref{eq-optimal-approx-system} is the Maximum Likelihood solution of the distribution $q_{\theta_i\vert \mathbf{x}_t}$ which are expressed purely in terms of the parameters $\boldsymbol{\zeta}_i(\mathbf{x}_{t})$.
The computation of the RHS of~\eqref{eq-optimal-approx-system} is performed on a case-by-case basis, depending on the choice of the link function and the likelihood distribution, which influences the corresponding conjugate prior distribution and its sufficient statistics function. For several commonly used distributions, we demonstrate how this computation can be effectively carried out by leveraging a closed-form Tweedie-type moments of $p_{{x}_{0,i} \vert \mathbf{x}_t}$. Details are provided in Appendix~\ref{app-table_distributions} and Appendix~\ref{app-moments-computation-Tweedie}.
To conclude this section, we provide an example in which we explicitly compute the optimal approximation equations of \eqref{eq-optimal-approx-system} for a Poisson-Gamma model.

\begin{example}[Poisson-Gamma Example (continued)]
The sufficient statistics of the Gamma distribution are given by
\begin{equation}\label{eq-T-gamma}
\mathbf{T}(\theta) =  (\log(\theta), \theta)  .
\end{equation}
The expectation of the sufficient statistics is given by
\begin{equation}\label{eq-expectation-T-gamma}
     \mathbb{E}_{q_{\theta_i\vert \mathbf{x}_t}}[\mathbf{T}(\theta_i)] = \left(\psi(\alpha_i(\mathbf{x}_t)) - \log(\beta_i(\mathbf{x}_t)),  \frac{\alpha_i(\mathbf{x}_t)}{\beta_i(\mathbf{x}_t)}\right).
\end{equation}
where $\psi(\cdot)$ is the digamma function. 
It follows from~\eqref{eq-T-gamma}, ~\eqref{eq-expectation-T-gamma} and Theorem \ref{thm-minimizer-KL-divergence} that
\begin{equation}
\label{eq-system-gamma}
\begin{cases}
     \psi(\alpha_i(\mathbf{x}_t)) - \log(\beta_i(\mathbf{x}_t))  &= \mathbb{E}_{p_{{x}_{0,i}\vert \mathbf{x}_t}}[x_{0,i}] \\
    \alpha_i(\mathbf{x}_t)/\beta_i(\mathbf{x}_t) &=  \mathbb{E}_{p_{{x}_{0,i}\vert \mathbf{x}_t}}[e^{x_{0,i}}]
\end{cases}
\end{equation}
for $i=1,\ldots,d$.
\end{example}
\begin{remark}
    We remark that we recover the moment matching result of~\cite{boys2024} when $q_{\theta_i\vert \mathbf{x}_t}(\theta_i\vert \boldsymbol{\zeta}(\mathbf{x}_{t}))$ is a normal distribution with know variance $\sigma^{2}$. In this case, $g(x)=x$ and $T(x)= x/\sigma$. \todo{This remark is slightly inaccurate because in our case we assume that the covariance matrix is diagonal.}
\end{remark}

\begin{remark}[MLE estimation] \label{remark-MLE_estimation}
    \todo{review after first results}
    From the definition of KL divergence, it follows that minimizing the KL divergence is equivalent to maximizing the log-likelihood. Thus, this approach aligns with estimating the parameters $\boldsymbol{\eta}_{i}$ using maximum likelihood estimation (MLE).
    Alternative methods to MLE exist. For instance, the natural parameters $\boldsymbol{\eta}_{i}$ can also be estimated using the method of moments. This method may offer a more tractable means of parameter estimation in certain cases; however, it does not guarantee the optimal approximation (see Appendix \ref{app-mom-estimation}). 
\end{remark}

\todo{
Question: How is our method related to "\textit{natural} gradient descent"? }