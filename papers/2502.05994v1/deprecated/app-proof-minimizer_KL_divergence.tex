\subsection{Proof of Theorem \ref{thm-minimizer-KL-divergence}} \label{sec:proof_thm-minimizer-KL-divergence}

\begin{proof}[Proof of Theorem \ref{thm-minimizer-KL-divergence}]
The KL divergence in \eqref{eq-kl-divergence-exp} is composed of a sum of independent terms. This means that minimizing the total KL divergence $D_{\text{KL}}(p_{\mathbf{x}_{0}\vert \mathbf{x}_{t}} \vert\vert q_{\mathbf{x}_{0}\vert \mathbf{x}_{t}})$ can be achieved by independently minimizing each term within the parentheses. To determine the set of parameters $\boldsymbol{\eta}$ that minimize each term, we calculate the gradient of the term with respect to 
$\boldsymbol{\eta}$	and set it equal to zero. By doing so we obtain
\begin{equation}
    -\nabla_{\boldsymbol{\eta}} A(\boldsymbol{\eta}) = \mathbb{E}_{p_{\mathbf{x}_{0}\vert \mathbf{x}_{t}}}[ \mathbf{T}(g^{-1}(\mathbf{x}_{0}))].
\end{equation}
As shown in Bishop (Section 2.4.1)\todo{add proper citation}, the following equality holds for the expectation of the sufficient statistics under the exponential family distribution
\begin{equation}
-\nabla_{\boldsymbol{\eta}} A(\boldsymbol{\eta})  = \mathbb{E}_{q_{\boldsymbol{\phi}\vert \mathbf{x}_t}}[\mathbf{T}(\boldsymbol{\phi})].
\end{equation}
By equating the two expressions for $-\nabla_{\boldsymbol{\eta}} A(\boldsymbol{\eta})$, we arrive at the relation
\begin{equation}
\mathbb{E}_{q_{\boldsymbol{\phi}\vert \mathbf{x}_t}}[\mathbf{T}(\boldsymbol{\phi})] = \mathbb{E}_{p_{\mathbf{x}_{0}\vert \mathbf{x}_{t}}}[ \mathbf{T}(g^{-1}(\mathbf{x}_{0}))]
\end{equation}
which shows the desired result. 
\end{proof}
