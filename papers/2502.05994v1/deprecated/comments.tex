
% we can reframe the posterior distribution of $\boldsymbol{\phi}$ in terms of the transformed variables $\mathbf{x}_{0}$. 
% It is a standard result (see [...]) that the posterior $p(\boldsymbol{\phi}\vert \mathbf{y})$ is related to $p(\mathbf{x}_{0}\vert \mathbf{y})$ through the change of variable formula
% \begin{equation}
% \label{eq-cov-formula}
%     p(\boldsymbol{\theta}\vert \mathbf{y})  = \prod_{i=1}^{d}\left\vert g'(\theta_{i}) \right\vert p(\mathbf{x}_{0}\vert \mathbf{y}).
% \end{equation}
% Based on these considerations, we intend to use the score-based diffusion posterior sampling approach of Section~\ref{sec-diffusion-posterior-sampling} to generate samples from $p(\mathbf{x}_{0}\vert \mathbf{y})$.
% We consider a partial measurement $\mathbf{y} \in \mathbb{R}^{d}$ derived from $\mathbf{x}_0 \in \mathbb{R}^{d}$, where $\mathbf{y}=(y_1, y_2, \dots, y_d)$ and $\mathbf{x}_0=(x_{0,1}, x_{0,2}, \dots, x_{0,d})$.
% Additionally, we work under the assumption that the data $\mathbf{y}$ are conditionally independent on the parameters $\mathbf{x}_{0}$. Furthermore, ${y}_i | {x_{0,i}}$ is assumed to be conditionally independent of $\{{x}_{0,j}\}$ for all $j \neq i$, such that 
% \begin{equation}
% p_{\mathbf{y}|0}(\mathbf{y}|\mathbf{x}_{0}) = \prod_i p_{\mathbf{y}|0}({y}_i|{x}_{0,i}).
% \end{equation}
% Note that this assumption excludes the use of covariates, such as a matrix $\mathbf{H}$ that scales $\mathbf{x}_{0}$ in the expectation of $\mathbf{y}$, as in the Gaussian case of \eqref{eq:gaussian_gaussian_case}. \textcolor{red}{We are currently considering ways of extending our analysis to include the matrix $\mathbf{H}$.} talk about GLM here (natural parameter vs parameter).
% For the discussion that follows, it is convenient to focus on distributions from the exponential family which can be expressed as, 

% \begin{equation}
% \label{eq-exponential-family}
%     p({y}|{\theta}) = h({y})  \exp\left({\eta}(\theta){T}({y}) - A({\eta(\theta)})\right)
% \end{equation}
% where $\eta(\cdot)$ is the natural parameter, ${T}(\cdot)$ is the sufficient statistics and $A({\cdot})$ is the log-partition function. 
% We aim to extend the tractability of the score of the likelihood, $\nabla_{\mathbf{x}_t} \log p_{\mathbf{y}|t}(\mathbf{y}|\mathbf{x}_t)$, as expressed in~\eqref{eq:score_posterior}, for specific likelihoods $p_{\mathbf{y}|0}(\mathbf{y}_t|\mathbf{x}_0)$ that belong to the one-parameter exponential family. In particular, we focus on those likelihoods for which the conjugate prior is either a Gamma or a Beta distribution. This restriction allows us to handle likelihoods such as the Poisson, Gamma,  Exponential, Bernoulli, Binomial, and Negative Binomial distributions.
% For the discussion that follows, it is convenient to focus on distributions from the exponential family which can be expressed as, 
% \begin{equation}
% \label{eq-exponential-family}
%     p({y}|{\theta}) = h({y})  \exp\left({\eta}(\theta){T}({y}) - A({\eta(\theta)})\right)
% \end{equation}
% where $\eta(\cdot)$ is the natural parameter, ${T}(\cdot)$ is the sufficient statistics and $A({\cdot})$ is the log-partition function. 
% It is well-known that for any likelihood in the exponential family, there exists a corresponding conjugate prior, which we denote by $p_{\theta}(\theta \mid \psi)$, where $\psi$ represents the parameters of the prior distribution. Here we could state the general form of the prior, so later we can present the general form of the evidence. We can add in the appendix specific cases for the distributions of interest.


% This choice makes the integral in equation~\eqref{eq-likelihood-y-xt} tractable.

% We begin by expressing the likelihood of the data $ p_{\mathbf{y}\vert 0}(y_{i}\vert x_{0,i})$ in terms of the parameter $\theta_{i}\in\Theta\subseteq \mathbb{R}$ and a deterministic \textit{link function} $g(\cdot)$ satisfying the following assumptions.
% % \begin{assumption}
% % \label{ass-link-function}
% %     We assume that the link function is $g:\mathbb{R}\to \Theta$ has the following conditions:
% %     \begin{enumerate}
% %         \item Monotonicity: $g(\cdot)$ must be monotonic (increasing or decreasing).
% %         \item Differentiability: $g(\cdot)$ must be differentiable.
% %         \item Nonzero Derivative: $\frac{\mathrm{d}g(\theta)}{\mathrm{d}\theta}\neq 0$.
% %         \item Support Compatibility: The range of $g(\cdot)$ must match the support of $\theta$.
% % \end{enumerate}
% % \end{assumption}
% \begin{assumption}
%     We assume that $g:\Theta\to \mathbb{R}$ is a differentiable, monotonic function.
% \end{assumption}
% The likelihood data can be written as:


% For Gaussian distributions, the mean can be expressed as $\phi_i = x_{0, i}$ as presented in~\eqref{eq:gaussian_gaussian_case}.
% However, many other distributions in the exponential family involve parameters that are constrained to specific ranges. For instance, in the case of the Poisson distribution, the rate parameter $\phi$ is restricted to $\phi \in \mathbb{R}_{+}$.
% To address these constraints, we introduce a transformation on the parameter $\phi_i = g(x_{0,i})$, where $g(\cdot)$ acts as a link function. This transformation ensures that the parameter is appropriately mapped to the domain required by the specific distribution. For example, in the case of the Poisson distribution, we can define $\phi_i = \exp(x_{0, i})$.

%  We define the density $q_{0|t,i}(x_{0,i} \mid \mathbf{x}_t)$ as the transformed density of the conjugate prior, expressed as:
% \begin{equation} \label{eq:transformed_conjugate_prior}
%     q_{0|t,i}(x_{0,i} \mid \mathbf{x}_t) = p_{\phi_i}(g(x_{0,i}) \mid \psi(\mathbf{x}_t)) \left| \frac{d}{dx_{0,i}} g(x_{0,i}) \right|.
% \end{equation}
% where $p_{\phi_i}$ is the conjugate prior of $p(y_i \mid \phi_i)$.
% This transformed conjugate prior takes on specific forms for certain common distributions and link functions. For example:
% \begin{itemize}
%     \item If the conjugate prior is a Gamma distribution and the link function $g(\cdot)$ is exponential, the resulting transformed prior is a Log-Gamma distribution.
%     \item If the conjugate prior is a Beta distribution and the link function $g(\cdot)$ is sigmoid, the resulting transformed prior is a Sigmoid-Beta distribution.
% \end{itemize}
% The explicit forms of the densities for the Log-Gamma and Sigmoid-Beta distributions are provided in Appendix XX.
% Our approach interprets the term $ p_{\mathbf{y}|t}(\mathbf{y}|\mathbf{x}_{t}) $ as the evidence, also referred to as the marginal likelihood, in traditional Bayesian statistics. 
% The following integral is tractable
% \begin{align}
%     p_{\mathbf{y}|t}(\mathbf{y}|\mathbf{x}_{t}) 
%     &= \int_{-\infty}^{\infty} p_{\mathbf{y}|\mathbf{x}_0}(\mathbf{y}|\mathbf{x}_0) q_{0|t}(\mathbf{x}_0|\mathbf{x}_t) \mathrm{d}\mathbf{x}_0 \\
%     &= \prod_i \int_{-\infty}^{\infty} p_{\mathbf{y}|0}({y}_i|{x}_{0, i}) q_{0|t,i}(x_{0, i}|\mathbf{x}_t) \mathrm{d}{x}_{0, i}\\
%     &= \prod_i \int_{\phi} p_{\mathbf{y}|0}({y}_i|\phi) q_{0|t,i}(g^{-1}(\phi)|\mathbf{x}_t) \left|\frac{d}{d\phi}g^{-1}(\phi) \right| d\phi \\
%     &= \prod_i \int_{\phi} p_{\mathbf{y}|0}({y}_i|\phi) p_{\phi_i}(\phi|\psi(\mathbf{x}_t)) d\phi \\
%     &= \prod_i p_{\mathbf{y}|t}({y}_{i}|\mathbf{x}_{t}) 
% \end{align}

% \begin{proposition}
% As shown in XX, a conjugate prior $p_{\phi}(\phi)$ exists for every distribution $p(y|\phi)$ in the exponential family. 
% It is a standard result (see \cite{}) that under Assumption~\ref{ass-link-function} the prior distribution $p(\psi) = g^{-1}(\phi)$ is well-defined and is given by 
% \begin{equation} \label{eq:transformed_prior}
%     p(\psi) = p_{\phi}(g(\psi)) \left\vert\frac{d\phi}{d\psi} \right\vert 
% \end{equation}
% It follows immediately from the fact that $p_{\phi}(\phi)$ is the conjugate prior to $p(y|\phi)$ that the following evidence is tractable:
% \begin{equation}
% \label{eq-evidence-psi}
%     p(y) = \int p(y|\psi) p (\psi) \mathrm{d}\psi
% \end{equation}
% \end{proposition}

%  (\textcolor{red}{fishy,needs review})First, given the link function $g(\cdot)$, notice that the likelihood $p_{\mathbf{y}\vert 0}(y_{i}\vert x_{0,i})$ can be expressed as 
% \begin{equation}
% \label{eq-data-likelihood-cov}
%     p_{\mathbf{y}\vert 0}(y_{i}\vert x_{0,i}) := \int p_{\mathbf{y}\vert 0}(y_{i}\vert \phi_{i})\delta(x_{0,i} - g(\phi_{i})) \mathrm{d}\phi_{i}, \quad i=1,\ldots,d,
% \end{equation}
% where $\delta(x)$ is the Dirac delta function. 


% A standard result for Poisson-distributed random variables is that their likelihood is conjugate to a Gamma prior. This result is not directly applicable to our case since we are placing a prior on $\mathbf{x}(0)$ which are defined on the reals, not on the positive reals. Nevertheless, we show that if we use the exponential link 
% \begin{equation}
%     p(\mathbf{y}|\mathbf{x}(0)) = \text{Poisson}(\mathbf{\lambda} )
% \end{equation}
% where $\mathbf{\lambda}=\exp (\mathbf{x}(0))$ and that we use a \textit{Log-Gamma} distribution for $q(\mathbf{x}(0)|\mathbf{x}(t))$, then the evidence is tractable. The density of the Log-Gamma distribution is 
% \begin{lemma}{Density of the Log-Gamma distribution}
% \label{lemma-density-log-gamma}
% Let $X$ be a Gamma distribution with parameters $\alpha$ and $\beta$. Then $Y = \log{X}$ follows a Log-Gamma distribution with parameters $\alpha$ and $\beta$. 
% \end{lemma}
% The proof of Lemma \ref{lemma-density-log-gamma} is postponed to Appendix \ref{app-proofs}.



% \begin{proposition}[Evidence is Well define]
% \label{prop-existence-prior-transformed}
% Let the link function $g: \mathbb{R}^d \to \mathbb{R}^d$ satisfy the following regularity assumptions:
% \begin{enumerate}
%     \item $g(\cdot)$ is a smooth (continuously differentiable) and bijective mapping, and its inverse $g^{-1}(\cdot)$ is also continuously differentiable.
%     \item The Jacobian determinant of the transformation, $\left|\frac{d\boldsymbol{\eta}}{d\boldsymbol{\psi}}\right| = \left|\det \left( \frac{\partial \boldsymbol{\eta}}{\partial \boldsymbol{\psi}} \right)\right|$, is well-defined and non-zero for all $\boldsymbol{\psi}$ in the domain of interest.
% \end{enumerate}

% Under these assumptions, the prior $p(\boldsymbol{\psi})$ on the transformed parameter $\boldsymbol{\psi}$ is given by:
% \begin{align}
%     p(\boldsymbol{\psi}) &= p_{\eta}(g^{-1}(\boldsymbol{\psi})) \left|\frac{d\boldsymbol{\eta}}{d\boldsymbol{\psi}}\right|,
% \end{align}
% where $p_{\eta}(\boldsymbol{\eta})$ is the prior on the original parameter $\boldsymbol{\eta}$, and $\frac{d\boldsymbol{\eta}}{d\boldsymbol{\psi}}$ denotes the Jacobian of the inverse transformation.
% \end{proposition}
% \textbf{For the link function applied to the density, let's try to borrow the notation of the pushforward of measure, e.g. } $(g_{*}p) (y)$, $(p\circ g^{-1}) (y)$
% The natural exponential family of distributions in canonical form:
% \begin{equation}
% \label{eq-exponential-family}
%     p_{0\vert t}(\mathbf{x}_{0}|\mathbf{x}_{t}) = q(\mathbf{x}_{0})  e^{\mathbf{x}_{t}^{\top}\cdot\mathbf{T}(\mathbf{x}_{0}) - A(\mathbf{x}_{t})} 
% \end{equation}

% We can always find a distribution $q(x0|xt)$ that makes the 




% \begin{proof}[Proof of Lemma \ref{lemma-density-log-gamma}.]

% By the definition of the Gamma distribution, 
% \begin{align}
%     p_X(x) = \int_0^{\infty} \frac{\beta^{\alpha}}{\Gamma(\alpha)} x^{\alpha-1} e^{-\beta x}dx. 
% \end{align}
% We use a change of variable $Y = \log X$. By the Transformation Theorem, the density of $Y$ is given by
% \begin{align}
%     p_Y(y) &= \int_{-\infty}^{\infty}  p_X(x(y)) |\frac{dx}{dy}| dy\\
%     &= \frac{\beta^{\alpha}}{\Gamma(\alpha)} e^{y\alpha} e^{-\beta e^y}.
% \end{align}
% \end{proof}

%%%%% OLD

% \begin{proof}[Proof of Lemma \ref{lemma-KL-divergence}.]
% By applying the factorization in \eqref{eq-mean-field-variational-approx}, the KL divergence between $p_{0\vert t}$ and $ q_{0\vert t} $ can be expressed as:
% \begin{equation}
% \begin{aligned}
%    D_{\text{KL}}(p_{0\vert t} \vert\vert q_{0\vert t}) &= - H(p_{0\vert t}) - \int p_{0\vert t}(\mathbf{x}_{0}\vert \mathbf{x}_{t}) \log q_{0\vert t}(\mathbf{x}_{0}\vert \mathbf{x}_{t}) \, \mathrm{d}\mathbf{x}_{0}  \\
%    &= - H(p_{0\vert t}) - \sum_{i=1}^{d} \int p_{0\vert t}(\mathbf{x}_{0}\vert \mathbf{x}_{t}) \log q_{0\vert t,i}(x_{0,i}\vert \mathbf{x}_{t}) \, \mathrm{d}\mathbf{x}_{0},
% \end{aligned}
% \end{equation}
% where the entropy $ H(p_{0\vert t}) $ is given by:
% \begin{equation}
% H(p_{0\vert t}) = - \int p_{0\vert t}(\mathbf{x}_{0}\vert \mathbf{x}_{t}) \log p_{0\vert t}(\mathbf{x}_{0}\vert \mathbf{x}_{t}) \, \mathrm{d}\mathbf{x}_{0}.
% \end{equation}
% Recall the definition of $q_{0\vert t,i}(\phi_{i}\vert \mathbf{x}_{t})$ given in \eqref{eq-prior-q-theta}.
% From the change of variable formula it follows that
% \begin{equation}
% \begin{aligned}
%     q_{0\vert t,i}(x_{0,i}\vert \mathbf{x}_{t})  &= q_{0\vert t,i}(g^{-1}(x_{0,i})\vert \mathbf{x}_{t}) \left\vert (g^{-1})'(x_{0,i}) \right\vert  \\
%      p_{0\vert t}(\mathbf{x}_{0}\vert \mathbf{x}_{t})  &=\prod_{i=1}^{d} \left\vert (g^{-1})'(x_{0,i})\right\vert p_{0\vert t}(g^{-1}(\mathbf{x}_{0})\vert \mathbf{x}_{t}).
% \end{aligned}
% \end{equation}
% We can plug these expressions in the KL divergence to obtain
% \begin{equation}
% \begin{aligned}
%    D_{\text{KL}}(p_{0\vert t} \vert\vert q_{0\vert t}) &= - H(p_{0\vert t} ) \\&\quad- \sum_{i=1}^{d} \int p_{0|t}(g^{-1}(\mathbf{x}_{0})\vert \mathbf{x}_{t}) \left(\prod_{i=1}^{d} \left\vert (g^{-1})'(x_{0,i})\right\vert\right)  \log \left({q}_{0\vert t,i}(g^{-1}(x_{0,i}))\left\vert (g^{-1})'(x_{0,1}) \right\vert \right) \mathrm{d}\mathbf{x}_{0} \\  
%     & =- H(p_{0\vert t} ) - \sum_{i=1}^{d} \int p_{0|t}(g^{-1}(\mathbf{x}_{0})\vert \mathbf{x}_{t})\left( \prod_{i=1}^{d} \left\vert (g^{-1})'(x_{0,i})\right\vert\right) \log \left(\left\vert (g^{-1})'(x_{0,i}) \right\vert\right) \mathrm{d}\mathbf{x}_{0}  \\ &\quad- \sum_{i=1}^{d} \int p_{0\vert t}(\boldsymbol{\theta}\vert \mathbf{x}_{t}) \left( \prod_{i=1}^{d} \left\vert (g^{-1})'(x_{0,i})\right\vert\right)\log q_{0\vert t,i}({\theta}_{i}\vert \mathbf{x}_{t})\mathrm{d}\boldsymbol{\theta}.
% \end{aligned}
% \end{equation}
% The distribution $q_{0\vert t,i}({\theta}_{i}\vert \mathbf{x}_{t})$ is in the exponential family, therefore it can be expressed as in \eqref{eq-exponential-family} for a natural parameter $\boldsymbol{\eta}$, a log-partition function $A(\boldsymbol{\eta})$ and a sufficient statistics $\mathbf{T}(\theta_{i})$.
% We can plug this representation in the KL divergence and by using 
% the fact that
% \begin{equation}
%     \int p_{0\vert t}(\boldsymbol{\theta}\vert \mathbf{x}_{t}) \left( \prod_{i=1}^{d} \left\vert (g^{-1})'(x_{0,i})\right\vert\right)\mathrm{d}\boldsymbol{\theta} =     \int p_{0\vert t}(\mathbf{x}_{0}\vert \mathbf{x}_{t}) \mathrm{d}\mathbf{x}_{0}= 1
% \end{equation}
% we obtain the following expression
% \begin{equation}
% \begin{aligned}
%    D_{\text{KL}}(p_{0\vert t} \vert\vert q_{0\vert t}) &=H(p_{0\vert t} )- A(\boldsymbol{\eta})\left( \int p_{0\vert t}(\boldsymbol{\theta}\vert \mathbf{x}_{t})\left( \prod_{i=1}^{d} \left\vert (g^{-1})'(x_{0,i})\right\vert\right) \mathrm{d}\boldsymbol{\theta}\right) - \boldsymbol{\eta} \cdot \left(\int p_{0\vert t}(\boldsymbol{\theta}\vert \mathbf{x}_{t}) \mathbf{T}(\theta_{i}) \mathrm{d}\boldsymbol{\theta}  \right)  \\
%     &= C- A(\boldsymbol{\eta}) - \boldsymbol{\eta}\cdot \mathbb{E}_{p_{0\vert t}(\boldsymbol{\theta}|\mathbf{x}_{t})}[ \mathbf{T}(\theta_{i})]
% \end{aligned}
% \end{equation}
% where $C$ is the following constant 
% \begin{equation}
% C = - H(p_{0\vert t} ) - \sum_{i} \int p_{0|t,i}(g^{-1}(\mathbf{x}_{0})\vert \mathbf{x}_{t}) \left\vert \frac{d}{dx}g^{-1}(x) \right\vert  \log \left(\Big\vert\frac{dx}{dy} \Big\vert\right) \mathrm{d}\mathbf{x}_{0}.
% \end{equation}
% which does not depend on $\boldsymbol{\eta}$.
% \end{proof}

%%% OLD

% \begin{proposition}[Recursive Formula for Diagonal Moments]
% \label{prop-recursive-moment}
%  Let $\mathbf{t}_{i}$ be a vector whose entries are given by $(\mathbf{t}_{i})_{j} = t_{i}\delta_{ij}$ where $\delta_{ij}$ is the Kronecker delta and $t_{i}\geq 0$. Notice that the only non-zero entry in $\mathbf{t}_{i}$ is the $i^{\text{th}}$ entry. Evaluating the MFG at $\mathbf{t}_{i}$  returns
% \begin{align}
% \label{eq-one-d-mgf}
%    M_{{x}_{0}\vert\mathbf{x}_{t}}(\mathbf{t}_{i}) 
%    &= \mathbb{E}_{{\mathbf{x}_0\vert \mathbf{x}_t}}[e^{\mathbf{x}_{0}^{\top} \mathbf{t}_{i}}] = \mathbb{E}_{{{x}_0\vert \mathbf{x}_t}}[e^{t_{i}x_{0,i}}] \\
%    &= \exp\left(t^{2}_{i}\frac{v_{t}}{2\alpha_{t}} + \frac{1}{\sqrt{\alpha_{t}}}t_{i} x_{t,i} + \log\left(\frac{p_{\mathbf{x}_t}\left(\mathbf{x}_{t}+\frac{v_{t}}{\sqrt{\alpha}}\mathbf{t}_{i}\right)}{p_{\mathbf{x}_t}\left(\mathbf{x}_{t}\right)}\right)\right)
% \end{align}
% Then the $n$th derivative of $M_{{x}_{0}\vert\mathbf{x}_{t}}()$


% Let $\mu_{\mathbf{x}_t}(i)$ and $\sigma^{2}_{\mathbf{x}_t}(i)$ be defined as in~\eqref{eq-mu-sigma-mgf}.
% Then, given the marginal density $p_{\mathbf{x}_t}(\mathbf{x}_{t})$, the $j^{\text{th}}$ moment of $x_{0,i}\vert \mathbf{x}_{t}$ is given by 
% \begin{equation}
% \label{eq-diagonal-moment}
% \begin{aligned}
%     m_{x_0|\mathbf{x}_t}(k,i) =\ell_{x_{0}\vert\mathbf{x}_{t}}(k,i)+ b_{{x}_{0}\vert\mathbf{x}_{t}} (k,i)
% \end{aligned}
% \end{equation}
% where $\ell_{x_{0}\vert\mathbf{x}_{t}}(k,i) $ satisfies the following recursive relationship
% \begin{equation}
% \begin{aligned}
% \ell_{x_{0}\vert\mathbf{x}_{t}}(k,i)  = \mu_{\mathbf{x}_t}(i) \ell_{{x}_{0}\vert\mathbf{x}_{t}}(k-1,i) + (k-1)\sigma^{2}_{\mathbf{x}_t}(i)  \ell_{{x}_{0}\vert\mathbf{x}_{t}}(k-2,i), \quad \ell_{{x}\vert\mathbf{x}_{t}}(0, i) = 0,\quad \ell_{{x}_{0}\vert\mathbf{x}_{t}}(1, i) = \mu_{\mathbf{x}_t}(i)
% \end{aligned}
% \end{equation}
% and $b_{{x}_{0}\vert\mathbf{x}_{t}}(k,i) $ satisfies the following relationship
% \begin{equation}
% \begin{aligned}
% b_{{x}_{0}\vert\mathbf{x}_{t}}(k,i)  = \sum_{j=2}^{k}(k-2)m_{{x}_{0}\vert\mathbf{x}_{t}}(j,i)\partial_{i}^{k-j-1}\nabla_{\mathbf{z}_{t}}\log\left(p_{\mathbf{x}_t}\left(\mathbf{z}_{t}\right)\right)\vert_{\mathbf{z}_{t}=\mathbf{x}_{t}}, \quad b_{{x}_{0}\vert\mathbf{x}_{t}}(0,i)= b_{{x}_{0}\vert\mathbf{x}_{t}}(1,i)=b_{{x}_{0}\vert\mathbf{x}_{t}}(2,i) = 0
% \end{aligned}
% \end{equation}
% for any $k\in\mathbb{N}$.
% \end{proposition}



% \begin{remark}
%      The Moment Generating Function (MGF) in \eqref{eq-moment-generating-function-tweedie} provides heuristic insights into the quality of the normal approximation to the conditional density $ p_{\mathbf{x}_{0} \vert \mathbf{x}_{t}} $. Specifically, by expanding the MGF as a Taylor series, we have: 
% \begin{equation}
% \begin{aligned}
%     M_{\mathbf{x}_{0} \vert \mathbf{x}_t}(\mathbf{t}) &= \exp\left(||\mathbf{t}||^2\frac{v_{t}}{2\alpha_{t}} + \frac{1}{\sqrt{\alpha_{t}}}\mathbf{t}^{\top}\mathbf{x}_{t} + \log\left(\frac{p_{\mathbf{x}_{t}}\left(\mathbf{x}_{t}+\frac{v_{t}}{\sqrt{\alpha_t}}\mathbf{t}\right)}{p_{\mathbf{x}_{t}}\left(\mathbf{x}_{t}\right)}\right)\right)  \\
%     &= \exp\left(\frac{\mathbf{t}^{\top}\boldsymbol{\sigma}^{2}_{\mathbf{x}_{t}}(\mathbf{0})\mathbf{t}}{2} + \mathbf{t}^{\top}\boldsymbol{\mu}_{\mathbf{x}_{t}}( \mathbf{0})\right) \left[1 + \mathcal{O}(||\mathbf{t}||^{3})\right].
% \end{aligned}
% \end{equation}
% This expression indicates that, when terms of order $\mathcal{O}(||\mathbf{t}||^{3})$ are neglected, the conditional distribution $ p_{\mathbf{x}_{0} \vert \mathbf{x}_{t}} $ can be well approximated by a multivariate Gaussian distribution with mean $\boldsymbol{\mu}_{\mathbf{x}_{t}}$ and covariance matrix $\boldsymbol{\sigma}^{2}_{\mathbf{x}_{t}}$.
% \end{remark}



% We can now taylor expand the moment generating function to an arbitrary order
% \begin{equation}
%     M_{\mathbf{x}_{0}\vert\mathbf{x}_{t}}(\hat{\mathbf{k}}_{i}) = 1 + \nabla_{\mathbf{k}}M_{\mathbf{x}_{0}\vert\mathbf{x}_{t}}(\mathbf{k}) \vert_{\mathbf{k}=\mathbf{0}}\cdot \hat{\mathbf{k}}_{i} + \frac{1}{2}\hat{\mathbf{k}}_{i}^{\top}\cdot \nabla_{\mathbf{k}}^{2}M_{\mathbf{x}_{0}\vert\mathbf{x}_{t}}(\mathbf{k}) \vert_{\mathbf{k}=\mathbf{0}}\cdot \hat{\mathbf{k}}_{i} + ...
% \end{equation}
% where the dots indicate higher order terms.
% \begin{proposition}[First Moment]
% \label{prop-first-moment-mgf}
% Let $M_{\mathbf{x}_{0}\vert\mathbf{x}_{t}}(\mathbf{k})$ be the moment generating function of \eqref{eq-moment-generating-function-tweedie}. Then, 
% \begin{equation}
% \nabla_{\mathbf{k}}M_{\mathbf{x}_{0}\vert\mathbf{x}_{t}}(\mathbf{k}) \vert_{\mathbf{k}=\mathbf{0}} 
% =\left( \frac{\mathbf{x}_{t}}{\sqrt{\alpha_{t}}} +  \frac{v_{t}}{\sqrt{\alpha}}\nabla_{\mathbf{z}_{t}}\log\left(p_{t}\left(\mathbf{z}_{t}\right)\right)\vert_{\mathbf{z}_{t}=\mathbf{x}_{t}}\right).
% \end{equation}
% \end{proposition}
% \begin{proof}
% We compute the first derivative of the expression in \eqref{eq-moment-generating-function-tweedie} to obtain
%     \begin{equation}
% \begin{aligned}
% \nabla_{\mathbf{k}}M_{\mathbf{x}_{0}\vert\mathbf{x}_{t}}(\mathbf{k}) \vert_{\mathbf{k}=\mathbf{0}} &= M_{\mathbf{x}_{0}\vert\mathbf{x}_{t}}(\mathbf{0})\nabla_{\mathbf{k}}\left(\mathbf{k}\cdot\mathbf{k}\frac{v_{t}}{2\alpha_{t}} + \frac{1}{\sqrt{\alpha_{t}}}\mathbf{k}\cdot\mathbf{x}_{t} + \log\left(\frac{p_{t}\left(\mathbf{x}_{t}+\frac{v_{t}}{\sqrt{\alpha}}\mathbf{k}\right)}{p_{t}\left(\mathbf{x}_{t}\right)}\right)\right)\vert_{\mathbf{k}=\mathbf{0}} \\
% &=\left( \frac{\mathbf{x}_{t}}{\sqrt{\alpha_{t}}} + \nabla_{\mathbf{k}}\log\left(p_{t}\left(\mathbf{x}_{t}+\frac{v_{t}}{\sqrt{\alpha}}\mathbf{k}\right)\right)\vert_{\mathbf{k}=\mathbf{0}}\right).
% \end{aligned}
% \end{equation}
% Using $\mathbf{z}_{t} = \mathbf{x}_{t} +\frac{v_{t}}{\sqrt{\alpha}}\mathbf{k} $ we compute
% \begin{equation}
%     \nabla_{\mathbf{k}}\log\left(p_{t}\left(\mathbf{x}_{t}+\frac{v_{t}}{\sqrt{\alpha}}\mathbf{k}\right)\right)\vert_{\mathbf{k}=\mathbf{0}} = \nabla_{\mathbf{k}}\mathbf{z}_{t} \nabla_{\mathbf{z}_{t}}\log\left(p_{t}\left(\mathbf{z}_{t}\right)\right)\vert_{\mathbf{z}_{t}=\mathbf{x}_{t}} = \frac{v_{t}}{\sqrt{\alpha}}\nabla_{\mathbf{z}_{t}}\log\left(p_{t}\left(\mathbf{z}_{t}\right)\right)\vert_{\mathbf{z}_{t}=\mathbf{x}_{t}}
% \end{equation}
% from which we obtain the desired result.
% \end{proof}
% Notice that Proposition~\ref{prop-first-moment-mgf} recovers the usual Tweedie's formula for the mean. We can perform similar computations to obtain the second moment.
% \begin{proposition}[Second Moment]
% \label{prop-first-moment-mgf}
% Let $M_{\mathbf{x}_{0}\vert\mathbf{x}_{t}}(\mathbf{k})$ be the moment generating function of \eqref{eq-moment-generating-function-tweedie}. Then, 
% \begin{equation}
% \nabla_{\mathbf{k}}^{2}M_{\mathbf{x}_{0}\vert\mathbf{x}_{t}}(\mathbf{k}) \vert_{\mathbf{k}=\mathbf{0}} 
% =\left( \frac{\mathbf{x}_{t}}{\sqrt{\alpha_{t}}} +  \frac{v_{t}}{\sqrt{\alpha}}\nabla_{\mathbf{z}_{t}}\log\left(p_{t}\left(\mathbf{z}_{t}\right)\right)\vert_{\mathbf{z}_{t}=\mathbf{x}_{t}}\right).
% \end{equation}
% \end{proposition}
% \begin{proof}
% \begin{equation}
% \nabla_{\mathbf{k}}M_{\mathbf{x}_{0}\vert\mathbf{x}_{t}}(\mathbf{k}) = M_{\mathbf{x}_{0}\vert\mathbf{x}_{t}}(\mathbf{k})\nabla_{\mathbf{k}}\left(\mathbf{k}\cdot\mathbf{k}\frac{v_{t}}{2\alpha_{t}} + \frac{1}{\sqrt{\alpha_{t}}}\mathbf{k}\cdot\mathbf{x}_{t} + \log\left(\frac{p_{t}\left(\mathbf{x}_{t}+\frac{v_{t}}{\sqrt{\alpha}}\mathbf{k}\right)}{p_{t}\left(\mathbf{x}_{t}\right)}\right)\right)
% \end{equation}
% Taking another derivative leads to 
% \begin{equation}
% \begin{aligned}
% \nabla_{\mathbf{k}}^{2}M_{\mathbf{x}_{0}\vert\mathbf{x}_{t}}(\mathbf{k}) = \nabla_{\mathbf{k}}M_{\mathbf{x}_{0}\vert\mathbf{x}_{t}}(\mathbf{k})\nabla_{\mathbf{k}}\left(\mathbf{k}\cdot\mathbf{k}\frac{v_{t}}{2\alpha_{t}} + \frac{1}{\sqrt{\alpha_{t}}}\mathbf{k}\cdot\mathbf{x}_{t} + \log\left(\frac{p_{t}\left(\mathbf{x}_{t}+\frac{v_{t}}{\sqrt{\alpha}}\mathbf{k}\right)}{p_{t}\left(\mathbf{x}_{t}\right)}\right)\right)\\
%  +M_{\mathbf{x}_{0}\vert\mathbf{x}_{t}} (\mathbf{k})\nabla_{\mathbf{k}}^{2}\left(\mathbf{k}\cdot\mathbf{k}\frac{v_{t}}{2\alpha_{t}} + \frac{1}{\sqrt{\alpha_{t}}}\mathbf{k}\cdot\mathbf{x}_{t} + \log\left(\frac{p_{t}\left(\mathbf{x}_{t}+\frac{v_{t}}{\sqrt{\alpha}}\mathbf{k}\right)}{p_{t}\left(\mathbf{x}_{t}\right)}\right)\right)
% \end{aligned}
% \end{equation}
% Hence, carrying out the computations gives
% \begin{equation}
% \begin{aligned}
% \nabla_{\mathbf{k}}^{2}M_{\mathbf{x}_{0}\vert\mathbf{x}_{t}}(\mathbf{k}) = (\nabla_{\mathbf{k}}M_{\mathbf{x}_{0}\vert\mathbf{x}_{t}}(\mathbf{k})\vert_{\mathbf{k}=\mathbf{0}})\cdot (\nabla_{\mathbf{k}}M_{\mathbf{x}_{0}\vert\mathbf{x}_{t}}(\mathbf{k})\vert_{\mathbf{k}=\mathbf{0}})\\
%  +\left(\mathbf{1}\frac{v_{t}}{2\alpha_{t}} + \nabla_{\mathbf{z}_{t}}^{2}\log\left(p_{t}\left(\mathbf{z}_{t}\right)\right)\vert_{\mathbf{z}_{t}=\mathbf{x}_{t}}\right)
% \end{aligned}
% \end{equation}

% \end{proof}



% Notice that the gradients of the loss actually simplify if we leverage the fact that 
% \begin{equation}
% \begin{aligned}
% \nabla_{\rho}\mathcal{L}(\rho)&=  - \mathbb{E}_{t\sim U(\epsilon, 1), \mathbf{x}_t \sim p_{\mathbf{x}_t|\mathbf{x}_0}, \mathbf{x}_0\sim  p_{\mathbf{x}_0}}\left[ \mathbf{J}_{\rho}^{\top} \nabla_{\boldsymbol{\eta}_{\rho}}\left(A(\boldsymbol{\eta}_{\rho}(\mathbf{x}_t,t)) + \boldsymbol{\eta}_{\rho}(\mathbf{x}_t,t)^{\top}\mathbb{E}_{p_{\mathbf{x}_{0}\vert\mathbf{x}_t}}[\mathbf{T}(g^{-1}(\mathbf{x}_{0}))]\right)\right] \\
% &=  - \mathbb{E}_{t\sim U(\epsilon, 1), \mathbf{x}_t \sim p_{\mathbf{x}_t|\mathbf{x}_0}, \mathbf{x}_0\sim  p_{\mathbf{x}_0}}\left[ \mathbf{J}_{\rho}^{\top} \left( - \mathbb{E}_{q_{\boldsymbol{\theta}\vert \mathbf{x}_t}}[\mathbf{T}(\boldsymbol{\theta})] + \mathbb{E}_{p_{\mathbf{x}_{0}\vert\mathbf{x}_t}}[\mathbf{T}(g^{-1}(\mathbf{x}_{0}))]\right)\right]
% \end{aligned}
% \end{equation}
% for the Jacobian 
% \begin{equation}
% \mathbf{J}_{\rho}(\mathbf{x}_t,t) = \nabla_{\rho}\boldsymbol{\eta}_{\rho}(\mathbf{x}_t,t)
% \end{equation}
% and where $\mathbb{E}_{q_{\boldsymbol{\theta}\vert \mathbf{x}_t}}[\mathbf{T}(\boldsymbol{\theta})] $ depends on $\boldsymbol{\eta}_{\rho}$. Notice that when $\nabla_{\rho}\mathcal{L}(\rho,\mathbf{x}_t,t)=0$, then $\boldsymbol{\eta}_{\rho}$ solves~\eqref{eq-optimal-approx-system}. 
% The parameters $\rho$ can be updated with the following gradient descent algorithm
% \begin{equation}
% \rho_{t+1} = \rho_{t} - \alpha\ \mathbb{E}_{t\sim U(\epsilon, 1), \mathbf{x}_t \sim p_{\mathbf{x}_t|\mathbf{x}_0}, \mathbf{x}_0\sim  p_{\mathbf{x}_0}}\left[\mathbf{J}_{\rho}(\mathbf{x}_t,t)^{\top} \left( - \mathbb{E}_{q_{\boldsymbol{\theta}\vert \mathbf{x}_t}}[\mathbf{T}(\boldsymbol{\theta})] +\mathbb{E}_{p_{\mathbf{x}_{0}\vert\mathbf{x}_t}}[\mathbf{T}(g^{-1}(\mathbf{x}_{0}))]\right)\right]
% \end{equation}
% where $\alpha$ is some learning rate. Here, the expectation can be approximated via the usual Monte Carlo estimator. When the set of parameters of the distribution is "information orthogonal", the corresponding fisher information matrix is diagonal. Therefore, we could extend in principle to \textit{natural} gradient descent by using the Fisher information matrix at almost no cost (since matrix inversion of a diagonal matrix is trivial). If only there was a way to write,
% \begin{equation}
% \mathbb{E}_{t\sim U(\epsilon, 1), \mathbf{x}_t \sim p_{\mathbf{x}_t|\mathbf{x}_0}, \mathbf{x}_0\sim  p_{\mathbf{x}_0}}\left[ \mathbf{J}_{\rho}(\mathbf{x}_t,t)^{\top}\mathbb{E}_{p_{\mathbf{x}_{0}\vert\mathbf{x}_t}}[\mathbf{T}(g^{-1}(\mathbf{x}_{0}))]\right]
% \end{equation}
% only in terms of the forward process $\mathbf{x}_{t}\vert\mathbf{x}_0$, this would basically imply that you do not need to know the score network and maybe avoid any potential accumulation of error. Also would mean that you can effectively train your models "in parallel/independently". My guess/hope is that

% \begin{equation}
% \mathbb{E}_{t\sim U(\epsilon, 1), \mathbf{x}_t \sim p_{\mathbf{x}_t|\mathbf{x}_0}, \mathbf{x}_0\sim  p_{\mathbf{x}_0}}\left[\mathbb{E}_{p_{\mathbf{x}_{0}\vert\mathbf{x}_t}}[\mathbf{T}(g^{-1}(\mathbf{x}_{0}))]\right]= \mathbb{E}_{t\sim U(\epsilon, 1), \mathbf{x}_t \sim p_{\mathbf{x}_t|\mathbf{x}_0}, \mathbf{x}_0\sim  p_{\mathbf{x}_0}}\left[\mathbf{T}(g^{-1}(\mathbf{x}_{0}))\cdot  f(\mathbf{x}_{t},t)\right]
% \end{equation} 
%  for some function $f$ to figure out and which then would take the score network out of the picture. Below we show  that actually this is possible! This is still missing the jacobian but we are certaintly squeezing in! The good news is that, even with the jacobian, the score network is completely out of the picture. This approach therefore will not suffer from the accumulation of error seen for example in the framework of simulation based inference. 
% \begin{proposition}[\textit{If only there was a way...}]
% \begin{multline}
% \mathbb{E}_{t\sim U(\epsilon, 1), \mathbf{x}_t \sim p_{\mathbf{x}_t|\mathbf{x}_0}, \mathbf{x}_0\sim  p_{\mathbf{x}_0}}\left[\mathbf{J}_{\rho}(\mathbf{x}_t,t)^{\top}\mathbb{E}_{p_{\mathbf{x}_{0}\vert\mathbf{x}_t}}[\mathbf{T}(g^{-1}(\mathbf{x}_{0}))]\right] = \\\mathbb{E}_{t\sim U(\epsilon, 1), \mathbf{x}_0\sim  p_{\mathbf{x}_0}}\left[\mathbb{E}_{\mathcal{N}(?,?)}\left[\mathbf{J}_{\rho}(\mathbf{x}_t,t)^{\top}\right]\mathbf{T}(g^{-1}(\mathbf{x}_{0}))\right]
% \end{multline}
% for some constant $c_{t}$ that I was too lazy to figure out.
% \end{proposition}

% leveraging the chain rule.
% The functions $\boldsymbol{\zeta}_{i}(\mathbf{x}_t)$ are obtained by numerically solving the system of equations presented in~\eqref{eq-optimal-approx-system}. To compute the required gradients, $\boldsymbol{\zeta}_{i}(\mathbf{x}_t)$ are treated as implicit layers, allowing their derivatives to be extracted effectively. \todo{Provide additional details on the numerical differentiation process, e.g., techniques or assumptions used.}
% To conclude this section, we provide an illustrative example that explicitly computes $\nabla_{\mathbf{x}_t}\log p_{\mathbf{y}|\mathbf{x}_t}(\mathbf{y}|\mathbf{x}t)$ in terms of the gradients of $\boldsymbol{\zeta}_{i}(\mathbf{x}_t)$ for the case of Poisson-distributed data.
% \begin{example}[Poisson-Gamma Example (continued)]
% Let $p_{\mathbf{y}|\mathbf{x}_t}(\mathbf{y}|\mathbf{x}_{t})$ be defined as in~\eqref{eq-poisson-gamma-evidence}. Then,
% \begin{equation*}
% \begin{aligned}
% \nabla_{\mathbf{x}_{t}}\log p_{\mathbf{y}|\mathbf{x}_t}(\mathbf{y}|\mathbf{x}_{t}) 
% &\approx \mathlarger{\sum}_{i=1}^{d} \Bigg[ \nabla_{\mathbf{x}_t}\alpha_i(\mathbf{x}_t) \left( \log (\beta_i(\mathbf{x}_t)) + \left( \sum_{k=0}^{y_i-1} \frac{1}{\alpha_i(\mathbf{x}_t) + k} \right) - \log (\beta_i(\mathbf{x}_t) + 1) \right) \\
% &\quad\quad\quad+ \nabla_{\mathbf{x}_t}\beta_i(\mathbf{x}_t) \left( \frac{\alpha_i(\mathbf{x}_t)}{\beta_i(\mathbf{x}_t)} - \frac{y_i + \alpha_i(\mathbf{x}_t)}{\beta_i(\mathbf{x}_t) + 1} \right) \Bigg]
% \end{aligned}
% \end{equation*}
% where have used the identity
% \begin{equation*}
% \frac{\Gamma(\alpha_{i}(\mathbf{x}_t) + y_{i}) }{\Gamma(\alpha_{i}(\mathbf{x}_t))} = \prod_{k=0}^{y_{i}-1} (\alpha_{i}(\mathbf{x}_t) + k).
% \end{equation*}
% \end{example}