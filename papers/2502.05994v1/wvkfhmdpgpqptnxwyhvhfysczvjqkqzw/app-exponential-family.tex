In this appendix, we introduce the notation for exponential families and provide a summary of key results, drawing inspiration from~\citet[Appendix B]{Johnson2016}. Throughout this manuscript we take all densities to be absolutely continuous with respect to the appropriate Lebesgue measure (when the underlying set $\mathcal{Y}$ is Euclidean space) or counting measure (when $\mathcal{Y}$ is discrete), and denote the Borel $\sigma$-algebra of a set $\mathcal{Y}$ as $\mathcal{B}(\mathcal{Y})$ (generated by Euclidean and discrete topologies, respectively). We assume measurability of all functions as necessary.

Given a statistic function $\mathbf{T}_{\mathbf{y}} : \mathcal{Y} \to \mathbb{R}^k$ and a base measure $h_{\mathbf{y}}(\mathbf{y})$, we can define an exponential family of probability densities on $\mathcal{Y}$ and indexed by natural parameter $\boldsymbol{\eta} \in \mathbb{R}^k$ by
\begin{equation*}
p_{\mathbf{y}\vert \boldsymbol{\eta}}(\mathbf{y} \vert \boldsymbol{\eta}) \propto  h_{\mathbf{y}}(\mathbf{y}) \exp\left( \boldsymbol{\eta}^{\top} \mathbf{T}_{\mathbf{y}}(\mathbf{y})  \right), \quad \forall \boldsymbol{\eta} \in \mathbb{R}^k.
\end{equation*}
 We define the partition function  as
\begin{equation*}
Z_{\mathbf{y}}(\boldsymbol{\eta}) := \int h_{\mathbf{y}}(\mathbf{y}) \exp\left( \boldsymbol{\eta}^{\top} \mathbf{T}_{\mathbf{y}}(\mathbf{y})  \right) \mathrm{d}\mathbf{y}
\end{equation*}
and the log-partition function as
\begin{equation*}
A_{\mathbf{y}}(\boldsymbol{\eta}) := \log Z_{\mathbf{y}}(\boldsymbol{\eta}).
\end{equation*}
Lastly, we define $\mathcal{H} \subseteq \mathbb{R}^k$ to be the set of all normalizable natural parameters,
\begin{equation*}
\mathcal{H} := \left\{ \boldsymbol{\eta} \in \mathbb{R}^k : A_{\mathbf{y}}(\boldsymbol{\eta}) < \infty \right\}.
\end{equation*}
We can write the normalized probability density as
\begin{equation*}
p_{\mathbf{y}\vert \boldsymbol{\eta}}(\mathbf{y} \vert \boldsymbol{\eta}) =h_{\mathbf{y}}(\mathbf{y}) \exp \left(\boldsymbol{\eta}^{\top}\mathbf{T}_{\mathbf{y}}(\mathbf{y}) -A_{\mathbf{y}}(\boldsymbol{\eta}) \right).
\end{equation*}
We say that an exponential family is \emph{regular} if $\mathcal{H}$ is open, and \emph{minimal} if there is no $\boldsymbol{\eta} \in \mathbb{R}^k \setminus \{ 0 \}$ such that $\boldsymbol{\eta}^{\top} \mathbf{T}_{\mathbf{y}}(\mathbf{y}) = 0$. We assume all families are regular and minimal.

We parameterize the family by the parameters $\boldsymbol{\theta}$ instead of the natural parameters. We write the natural parameter as a continuous function of the parameters, $\boldsymbol{\eta}(\boldsymbol{\theta})$ and take $\Theta = \boldsymbol{\eta}^{-1}(\mathcal{H})$ to be the open set of parameters that correspond to normalizable densities. We summarize this notation in the following definition.

\begin{definition}[Exponential family of densities]
\label{def-exponential-family}
Given a measure space $(\mathcal{Y}, \mathcal{B}(\mathcal{Y}))$, a statistic function $\mathbf{T}_{\mathbf{y}} : \mathcal{Y} \to \mathbb{R}^k$, and a natural parameter function $\boldsymbol{\eta} : \boldsymbol{\theta} \to \mathbb{R}^k$, the corresponding exponential family of densities is
\begin{equation*}
p_{\mathbf{y}\vert\boldsymbol{\theta}}(\mathbf{y} \vert \boldsymbol{\theta}) = h_{\mathbf{y}}(\mathbf{y})   \exp \left( \boldsymbol{\eta}(\boldsymbol{\theta})^{\top} \mathbf{T}_{\mathbf{y}}(\mathbf{y}) - A_{\mathbf{y}}(\boldsymbol{\eta}(\boldsymbol{\theta})) \right).
\end{equation*}
\end{definition}
When we write exponential families of densities for different random variables, we change the subscripts on the statistic function, natural parameter function, and log partition function to correspond to the symbol used for the random variable. 

The next proposition shows that the log partition function of an exponential family generates cumulants of the statistic.

\begin{proposition}[Gradients of log-partition function and expected sufficient statistics]\label{prop:gradient_A_suff_stats}
The gradient of the log partition function $A_{\mathbf{y}}$ of an exponential family distributed random variable $\mathbf{y}$ equals its expected sufficient statistic:
\begin{equation*}
\nabla_{\boldsymbol{\eta}} A_{\mathbf{y}}(\boldsymbol{\eta}) = \mathbb{E}_{p_{\mathbf{y} \vert \boldsymbol{\eta}}} \left[ \mathbf{T}_{\mathbf{y}}(\mathbf{y}) \right],
\end{equation*}
where the expectation is taken with respect to the density $p_{\mathbf{y} \vert \boldsymbol{\eta}}(\mathbf{y} \vert \boldsymbol{\eta})$. 
\end{proposition}
It is convenient to introduce the following notation for the Jacobian matrix $\mathbf{J}_{\mathbf{f}}(\mathbf{x}) \in \mathbb{R}^{n \times m}$ of a vector-value function $\mathbf{f}:\mathbb{R}^{m}\to\mathbb{R}^{n}$ with $\mathbf{f}(\mathbf{x}) = (f_{1}(\mathbf{x}),\ldots,f_{n}(\mathbf{x}))$ and where
\begin{equation*}
[\mathbf{J}_{\mathbf{f}}(\mathbf{x})]_{ij} = \frac{\partial f_{i}}{\partial x_{i}}\Big\vert_{\mathbf{x}}
\end{equation*}
\begin{corollary} \label{corollary:derivative_log_partition_function}
From Proposition~\ref{prop:gradient_A_suff_stats}, applying the chain rule yields
\begin{equation*}
 \mathbf{J}_{\boldsymbol{\theta}}(\boldsymbol{\eta})\Big|_{\boldsymbol{\eta} = \boldsymbol{\eta}(\theta)} \nabla_{\boldsymbol{\theta}} A_{\mathbf{y}}(\boldsymbol{\theta}) =  \mathbb{E}_{p_{\mathbf{y} \vert \boldsymbol{\theta}}} \left[ \mathbf{T}_{\mathbf{y}}(\mathbf{y}) \right],
\end{equation*}
where the expectation is over the random variable $\mathbf{y}$ with density $p_{\mathbf{y} \vert \boldsymbol{\theta}}(\mathbf{y} \vert \boldsymbol{\theta})$. 
\end{corollary}

% More generally, the moment generating function of $\mathbf{T}_{\mathbf{y}}(\mathbf{y})$ can be written
% \begin{equation}
% M_{\mathbf{T}_{\mathbf{y}}(\mathbf{y})}(\mathbf{s}) := \mathbb{E}_{p_{\mathbf{y} \vert \boldsymbol{\eta}}} \left[ e^{ \mathbf{s}^{\top} \mathbf{T}_{\mathbf{y}}(\mathbf{y}) } \right] = e^{A_{\mathbf{y}}(\boldsymbol{\eta} + \mathbf{s}) - A_{\mathbf{y}}(\boldsymbol{\eta})}
% \end{equation}
% and so derivatives of $A_{\mathbf{y}}$ give cumulants of $\mathbf{T}_{\mathbf{y}}(\mathbf{y})$, where the first cumulant is the mean and the second and third cumulants are the second and third central moments, respectively.

Given an exponential family of densities on $\mathcal{Y}$ as in Definition~\ref{def-exponential-family}, we can define a related exponential family of densities on $\boldsymbol{\theta}$ in terms of the functions $\boldsymbol{\eta}(\boldsymbol{\theta})$ and $A_{\mathbf{y}}(\boldsymbol{\eta}(\boldsymbol{\theta}))$ and by defining hyperparameters $\boldsymbol{\zeta} = (\boldsymbol{\nu}, \tau)$ and a base function $h_{\boldsymbol{\theta}}(\boldsymbol{\nu}, \tau)$.

\begin{definition}[Natural exponential family conjugate prior]
\label{def-exponential-family-conjugate}
Given the exponential family $p_{\mathbf{y}\vert\boldsymbol{\theta}}(\mathbf{y} \vert \boldsymbol{\theta})$ of Definition~\ref{def-exponential-family}, 
the natural exponential family conjugate prior to the density $p_{\mathbf{y}\vert\boldsymbol{\theta}}(\mathbf{y} \vert \boldsymbol{\theta})$ is
\begin{equation*}
\label{eq:exponential-family-conjugate}
p_{\boldsymbol{\theta}|\boldsymbol{\zeta}}(\boldsymbol{\theta}|\boldsymbol{\zeta}) = h_{\boldsymbol{\theta}}(\boldsymbol{\theta}) \exp \left(\boldsymbol{\zeta}^\top  \mathbf{T}_{\boldsymbol{\theta}}(\boldsymbol{\theta}) -  A_{\boldsymbol{\theta}}(\boldsymbol{\nu}, \tau) \right),
\end{equation*}
where $\boldsymbol{\zeta} = (\boldsymbol{\nu}, \tau)$, $\boldsymbol{\nu} \in \mathbb{R}^{k}$ and  $\tau \in \mathbb{R}$ are hyperparameters, $ \mathbf{T}_{\boldsymbol{\theta}}(\boldsymbol{\theta}) = (\boldsymbol{\eta}(\boldsymbol{\theta}), -A_{\mathbf{y}}(\boldsymbol{\eta}(\boldsymbol{\theta})))$,  and the density is taken on $(\boldsymbol{\theta}, \mathcal{B}(\boldsymbol{\theta}))$.
\end{definition}
The next proposition demonstrates that the joint density of independent variables, each following the same univariate exponential family distribution but with distinct parameters, also belongs to the exponential family. 
\begin{proposition}[Exponential Family Form of Independent Univariate Variables]
\label{prop:expfam_form_independent_variables}
    Let $y_j \in \mathcal{Y} \subseteq \mathbb{R}$ be part of the one-parameter univariate exponential family with parameter $\theta_j \in \Theta \subseteq \mathbb{R}$, natural parameter $\eta(\theta_j)$, base measure $h_{y}(y_j)$, sufficient statistics $T_{y}(y_j)$ and log-partition function $A_y(\eta(\theta_j))$ for $j = 1, \ldots, d$. Further, let $y_j|\theta_j$ be independent of $y_k|\theta_k$ for all $k \neq j$. Then, the joint density of $\boldsymbol{y} = (y_1, \ldots, y_d)$ conditional on $\boldsymbol{\theta} = (\theta_1, \ldots, \theta_d)$, $p_{\boldsymbol{y}\vert\boldsymbol{\theta}}(\boldsymbol{y} \vert \boldsymbol{\theta})$, is given by 
    \begin{equation*}
        \begin{aligned}
    %\label{eq:exponential_likelihood_independent_variables}
    &p_{\boldsymbol{y}\vert\boldsymbol{\theta}}(\boldsymbol{y} \vert \boldsymbol{\theta}) = h_{\boldsymbol{y}}(\boldsymbol{y})   \exp \left( \boldsymbol{\eta}(\boldsymbol{\theta})^{\top} \mathbf{T}_{\boldsymbol{y}}(\boldsymbol{y}) - \mathbf{1}_d^\top \mathbf{A}_{\boldsymbol{y}}(\boldsymbol{\eta}(\boldsymbol{\theta})) \right)\\
        &h_{\boldsymbol{y}}(\boldsymbol{y}) = \prod_{j = 1}^d h_{y}(y_j),\quad
        \boldsymbol{\eta}(\boldsymbol{\theta}) = (\eta(\theta_1), \dots, \eta(\theta_d)), \quad
        \mathbf{T}_{\boldsymbol{y}}(\boldsymbol{y}) = (T_y(y_1), \ldots, T_y(y_d)) \\
        &\mathbf{A}_{\boldsymbol{y}}(\boldsymbol{\eta}(\boldsymbol{\theta}))= (A_y(\eta(\theta_1)), \ldots, A_y(\eta(\theta_d)))
            \end{aligned}
    \end{equation*}
    where $\mathbf{1}_d$ is a vector of ones of dimension $d$.
\end{proposition}
The next proposition demonstrates that the joint density of $N$ multivariate independent variables following the same exponential family distribution also belongs to the exponential family. 
\begin{proposition}[Exponential Family Form of Independent Multivariate Variables]
\label{prop:expfam_form_independent_multivariate_variables}
    Let $\boldsymbol{y}_i \in \mathcal{Y}^d \subseteq \mathbb{R}^d$ be part of the multivariate exponential family with parameter $\boldsymbol{\theta} \in \Theta^d \subseteq \mathbb{R}^d$, natural parameter $\boldsymbol{\eta}(\boldsymbol{\theta})$, base measure $h_{\boldsymbol{y}}(\boldsymbol{y}_i)$, sufficient statistics $\mathbf{T}_{\boldsymbol{y}}(\boldsymbol{y}_i)$ and log-partition function $\mathbf{1}_d^\top \mathbf{A}_{\boldsymbol{y}}(\boldsymbol{\eta}(\boldsymbol{\theta}))$ for $i, \dots, N$. Further, let $\boldsymbol{y}_i|\boldsymbol{\theta}$ be independent of $\boldsymbol{y}_l|\boldsymbol{\theta}$ for all $i \neq l$. Then, the joint density of $\mathbf{y} = (\boldsymbol{y}_1, \ldots, \boldsymbol{y}_N)$ conditional on $\boldsymbol{\theta}$, $p_{\mathbf{y}\vert\boldsymbol{\theta}}(\mathbf{y} \vert \boldsymbol{\theta})$, is given by 
    \begin{equation*}
        \begin{aligned}
    %\label{eq:exponential_likelihood_independent_multivariate_variables}
    &p_{\mathbf{y}\vert\boldsymbol{\theta}}(\mathbf{y} \vert \boldsymbol{\theta}) = h_{\mathbf{y}}(\mathbf{y})   \exp \left( \boldsymbol{\eta}(\boldsymbol{\theta})^{\top} \mathbf{T}_{\mathbf{y}}(\mathbf{y}) - N \mathbf{1}_d^\top \mathbf{A}_{\boldsymbol{y}}(\boldsymbol{\eta}(\boldsymbol{\theta})) \right)\\
        &h_{\mathbf{y}}(\mathbf{y}) = \prod_{i = 1}^N h_{\boldsymbol{y}}(\boldsymbol{y}_i),\quad
        \boldsymbol{\eta}(\boldsymbol{\theta}) = (\eta(\theta_1), \dots, \eta(\theta_d)), \quad
        \mathbf{T}_{\mathbf{y}}(\mathbf{y}) = \sum_{i=1}^N \mathbf{T}_{\boldsymbol{y}}(\boldsymbol{y}_i) 
            \end{aligned}
    \end{equation*}
    where $\mathbf{1}_d$ is a vector of ones of dimension $d$.
\end{proposition}
The next proposition shows that the conjugate prior for the parameters of independent variables, each following the same univariate exponential family distribution but with distinct parameters, is of the natural exponential family form.
\begin{proposition}[Natural Exponential Family Conjugate Prior for Independent Univariate Parameters]
\label{prop:expfam_form_independent_parameters}
    Let $y_j \in \mathcal{Y} \subseteq \mathbb{R}$ be part of the one-parameter univariate exponential family with parameter $\theta_j \in \Theta \subseteq \mathbb{R}$, natural parameter $\eta(\theta_j)$, base measure $h_{y}(y_j)$, sufficient statistics $T_{y}(y_j)$ and log-partition function $A_y(\eta(\theta_j))$ for $j = 1, \ldots, d$. 
    Let the natural exponential family conjugate prior of $\theta_j$ with hyperparameters $\boldsymbol{\zeta}_j = (\nu_j, \tau_j), \nu_j, \tau_j \in \mathbb{R}$ be given by, 
    \begin{align*} 
        p_{{\theta}|\boldsymbol{\zeta}}({\theta}_j|\boldsymbol{\zeta}_j) = h_{{\theta}}(\theta_j) \exp \left(\boldsymbol{\zeta}_j^\top \mathbf{T}_{\theta}(\theta_j) - A_{{\theta}}({\nu}_j,  {\tau}_j)) \right).
    \end{align*}
    where $ \mathbf{T}_{\theta}(\theta_j)= ({\eta}({\theta}_j), -{A}_{{y}}({\eta}({\theta}_j))$.
    Further, let $\theta_j|\zeta_j$ be independent of $\theta_k|\zeta_k$ for all $k \neq j$. Then the joint natural exponential family conjugate prior of $\boldsymbol{\theta}= (\theta_1, \ldots, \theta_d)$ conditional on $\boldsymbol{\zeta} = (\boldsymbol{\nu}, \boldsymbol{\tau})$, where $\boldsymbol{\nu} = (\nu_1, \ldots,\nu_d)$ and $\boldsymbol{\tau} = (\tau_1, \ldots,\tau_d)$, is given by, 
    \begin{equation*}
        \begin{aligned}
        %\label{eq:natural_exponential_prior_independent_parameters}
        &p_{\boldsymbol{\theta}|\boldsymbol{\zeta}}(\boldsymbol{\theta}|\boldsymbol{\zeta}) = h_{\boldsymbol{\theta}}(\boldsymbol{\theta}) \exp \left(
        \boldsymbol{\zeta}^T \mathbf{T}_{\boldsymbol{\theta}}(\boldsymbol{\theta}) - A_{\boldsymbol{\theta}}(\boldsymbol{\nu},  \boldsymbol{\tau})  \right) \\
        &h_{\boldsymbol{\theta}}(\boldsymbol{\theta}) = \prod_{j = 1}^d h_{{\theta}}({\theta}_j), \quad
        \mathbf{T}_{\boldsymbol{\theta}}(\boldsymbol{\theta}) = (\boldsymbol{\eta}(\boldsymbol{\theta}), -\mathbf{A}_{\boldsymbol{y}}(\boldsymbol{\eta}(\boldsymbol{\theta}))), \\
        &\boldsymbol{\eta}(\boldsymbol{\theta}) = (\eta(\theta_1), \dots, \eta(\theta_d)), \quad \mathbf{A}_{\boldsymbol{y}}(\boldsymbol{\eta}(\boldsymbol{\theta}))= (A_y(\eta(\theta_1)), \ldots, A_y(\eta(\theta_d))), \\
        &A_{\boldsymbol{\theta}}(\boldsymbol{\nu},  \boldsymbol{\tau}) = \sum_{j = 1}^d A_{{\theta}}({\nu}_j,  {\tau}_j),
            \end{aligned}
    \end{equation*} 
\end{proposition}
When the exponential family $p_{\mathbf{y}\vert\boldsymbol{\theta}}(\mathbf{y} \vert \boldsymbol{\theta})$ is a likelihood function and the family $p_{\boldsymbol{\theta}|\boldsymbol{\zeta}}(\boldsymbol{\theta}|\boldsymbol{\zeta})$ is used as a prior, the pair exhibits a convenient conjugacy property, formalized in the following proposition.
\begin{proposition}[Conjugacy]  \label{prop:conjugacy}
Let the densities $p_{\mathbf{y}\vert\boldsymbol{\theta}}(\mathbf{y} \vert \boldsymbol{\theta})$ and $p_{\boldsymbol{\theta}|\boldsymbol{\zeta}}(\boldsymbol{\theta}|\boldsymbol{\zeta})$ be defined as in Proposition~\ref{prop:expfam_form_independent_multivariate_variables} and~\ref{prop:expfam_form_independent_parameters}, respectively. We have the relations
\begin{equation*}
\begin{aligned}
p_{\boldsymbol{\theta}, \mathbf{y}\vert\boldsymbol{\zeta}}(\boldsymbol{\theta}, \mathbf{y}\vert\boldsymbol{\zeta}) &= h_{\mathbf{y}}(\mathbf{y})h_{\boldsymbol{\theta}}(\boldsymbol{\theta})  \exp\left(- A_{\boldsymbol{\theta}}(\boldsymbol{\nu}, \boldsymbol{\tau}) \right) \exp \left( \boldsymbol{\eta}(\boldsymbol{\theta})^{\top} \left(\boldsymbol{\nu}+\mathbf{T}_{\mathbf{y}}(\mathbf{y}) \right) - \left(\boldsymbol{\tau} 
 + N \mathbf{1}_d\right)^\top A_{\boldsymbol{y}}(\boldsymbol{\eta}(\boldsymbol{\theta})) \right) \\
p_{\boldsymbol{\theta} \vert \mathbf{y}, \boldsymbol{\zeta}}(\boldsymbol{\theta} \vert \mathbf{y}, \boldsymbol{\zeta}) &= 
h_{\boldsymbol{\theta}}(\boldsymbol{\theta})  \exp\left(-A_{\boldsymbol{\theta}}(\boldsymbol{\nu}+\mathbf{T}_{\mathbf{y}}(\mathbf{y}), \boldsymbol{\tau} + N \mathbf{1}_d) \right) \exp \left( \boldsymbol{\eta}(\boldsymbol{\theta})^{\top} \left(\boldsymbol{\nu}+\mathbf{T}_{\mathbf{y}}(\mathbf{y})\right) - \left(\boldsymbol{\tau} 
 + N\mathbf{1}_d\right)^\top A_{\boldsymbol{y}}(\boldsymbol{\eta}(\boldsymbol{\theta})) \right) \\
p_{\mathbf{y}\vert\boldsymbol{\zeta}}(\mathbf{y}\vert\boldsymbol{\zeta}) &= h_{\mathbf{y}}(\mathbf{y}) \frac{\exp(-A_{\boldsymbol{\theta}}(\boldsymbol{\nu}, \boldsymbol{\tau}))}{\exp(-A_{\boldsymbol{\theta}}(\boldsymbol{\nu}+\mathbf{T}_{\mathbf{y}}(\mathbf{y}) , \boldsymbol{\tau} + N \mathbf{1}_d))}
\end{aligned}
\end{equation*}
and hence in particular the posterior $p_{\boldsymbol{\theta} \vert \mathbf{y}}(\boldsymbol{\theta} \vert \mathbf{y})$ is in the same exponential family as $p_{\boldsymbol{\theta}|\boldsymbol{\zeta}}(\boldsymbol{\theta}|\boldsymbol{\zeta})$ with the parameters $\boldsymbol{\nu}+\mathbf{T}_{\mathbf{y}}(\mathbf{y}) , \boldsymbol{\tau} + N \mathbf{1}_d$.
\end{proposition}








% \begin{equation}
% \begin{aligned}
% \label{eq:likelihood_t}
%      p_{\mathbf{y}|\mathbf{y}_t}(\mathbf{y}|\mathbf{y}_{t})
%      &=  \int p_{\mathbf{y} \vert \boldsymbol{\phi} }(\mathbf{y}\vert \boldsymbol{\phi}) p_{\boldsymbol{\phi}\vert \mathbf{y}_{t}} (\boldsymbol{\phi}\vert \mathbf{y}_{t}) \mathrm{d}\boldsymbol{\phi}  \\ 
%      &= h_{\mathbf{y} \vert \boldsymbol{\phi}}(\mathbf{y}) d(\boldsymbol{\xi}(\mathbf{y}_t) ,\lambda(\mathbf{y}_t)) 
%       \int \exp \left(\boldsymbol{\phi}^{\top}\left(\mathbf{T}_{\mathbf{y} \vert \boldsymbol{\phi}}(\mathbf{y}) + \boldsymbol{\xi}(\mathbf{y}_t)\right) - \left(N + \lambda(\mathbf{y}_t)\right) A_{\mathbf{y} \vert \boldsymbol{\phi}}(\boldsymbol{\phi}) \right)  \mathrm{d}\boldsymbol{\phi} \\
%       &= h_{\mathbf{y} \vert \boldsymbol{\phi}}(\mathbf{y}) \frac{d(\boldsymbol{\xi}(\mathbf{y}_t) ,\lambda(\mathbf{y}_t))}{d(\mathbf{T}_{\mathbf{y} \vert \boldsymbol{\phi}}(\mathbf{y}) + \boldsymbol{\xi}(\mathbf{y}_t) ,N + \lambda(\mathbf{y}_t))}.
% \end{aligned}
% \end{equation}

% Finally, we give a few more exponential family properties that are useful for gradient-based optimization algorithms and variational inference. In particular, we note that the Fisher information matrix of an exponential family can be computed as the Hessian matrix of its log partition function, and that the KL divergence between two members of the same exponential family has a simple expression.