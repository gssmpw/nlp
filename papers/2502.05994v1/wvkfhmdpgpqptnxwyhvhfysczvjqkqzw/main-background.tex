Throughout this work, vectors and matrices will be represented using boldface notation, denoted as $\mathbf{x}$, while scalars will be expressed in standard font as $x$. Furthermore, we will use $\norm{\mathbf{x}}$ to denote the $\ell^{2}$-norm of a vector $\mathbf{x}$.
\subsection{Score-Based Diffusion Models} 
\label{sec-score-based-diffusions}
Score-based diffusion models aim to generate samples from a target distribution $ p_{\mathbf{x}_0}(\mathbf{x}_0) $ by progressively perturbing data with increasing noise levels and then learning to reverse this perturbation. This reversal defines a generative model capable of approximating the original data distribution. In this work, we adopt the framework introduced by \citet{song2021scorebased}, who define the forward noising process via an Itô stochastic differential equation (SDE). Specifically, we focus on the Variance-Preserving (VP) formulation of the SDE presented by~\citet{song2021scorebased}, which corresponds to the Denoising Diffusion Probabilistic Models (DDPM) introduced by~\citet{ho_denoising}.

We construct a forward diffusion process $ (\mathbf{x}_t)_{t \in [0,T]} $, with $ \mathbf{x}_t \in \mathbb{R}^{d_x} $, governed by the following equation:
\begin{equation*}
\mathrm{d}\mathbf{x}_t = - \frac{1}{2} \beta(t) \mathbf{x}_t \, \mathrm{d}t + \sqrt{\beta(t)} \, \mathrm{d}\mathbf{w}_t, \quad \mathbf{x}_0 \sim p_{\mathbf{x}_0},
\end{equation*}
where $ \mathbf{w}_t $ denotes a standard Wiener process, and $ \beta(t) : \mathbb{R} \to \mathbb{R}_+$ is a noise schedule. A commonly chosen parametrization for the noise schedule is the linear schedule $ \beta(t) = \beta_0 + t \, (\beta_1 - \beta_0) $, as discussed in \citet[Appendix C]{song2021scorebased}. The forward process is associated with the following transition kernel:
\begin{equation}
\label{eq-forward-transition-kernel}
p_{\mathbf{x}_t | \mathbf{x}_0}(\mathbf{x}_t | \mathbf{x}_0) = \mathcal{N}(\mathbf{x}_t; \sqrt{\alpha_t} \mathbf{x}_0, v_t \mathbf{I}_{d_x}),
\end{equation}
where $ \alpha_t := \exp\left( - \int_0^t \beta(s) \, \mathrm{d}s \right) $ and $ v_t := 1 - \alpha_t $. 

To recover the data-generating distribution, we reverse the noising process by solving the reverse SDE, derived from the forward process \cite{anderson_1982}:
\begin{multline*}
\mathrm{d}\mathbf{x}_t = - \beta(t) \left( \frac{1}{2} \mathbf{x}_t + \nabla_{\mathbf{x}_t} \log p_{\mathbf{x}_{t}}(\mathbf{x}_t) \right) \, \mathrm{d}t   \\ + \sqrt{\beta(t)} \, \mathrm{d}\bar{\mathbf{w}}_t, \quad \mathbf{x}_T \sim p_{\mathbf{x}_T},
\end{multline*}
where $\mathrm{d}t$ corresponds to time running backward, $\mathrm{d}\bar{\mathbf{w}}_t$ to the standard Wiener process running backward. Importantly, the term $ \nabla_{\mathbf{x}_t} \log p_{\mathbf{x}_{t}}(\mathbf{x}_t) $ is the score function, which guides the reverse process and it is typically approximated using a neural network $ \mathbf{s}_{\boldsymbol{\phi}}$, with learnable parameters $\boldsymbol{\phi}$, trained via Denoising Score Matching (DSM)~\cite{vincent2011} using the objective
\begin{multline}\label{eq:phi_star}
    \mathcal{J}_{\text{DSM}}(\boldsymbol{\phi}) =\\  \mathbb{E}_{t\sim U(\epsilon, 1)}\Big[ \lambda(t)  \mathbb{E}_{ \mathbf{x}_0\sim  p_{\mathbf{x}_0}, \mathbf{x}_t \sim p_{\mathbf{x}_t\vert\mathbf{x}_0}}\Big[ \mathcal{L}_{\text{DSM}}(\boldsymbol{\phi},  \mathbf{x}_0, \mathbf{x}_t, t)\Big]\Big], 
\end{multline}
with 
\begin{multline*}
\mathcal{L}_{\text{DSM}}(\boldsymbol{\phi}, \mathbf{x}_0, \mathbf{x}_t, t) = \\  \norm{\mathbf{s}_{\boldsymbol{\phi}}(\mathbf{x}_t, t) 
    -  \nabla_{\mathbf{x}_t} \log p_{\mathbf{x}_t\vert\mathbf{x}_0}(\mathbf{x}_t\vert\mathbf{x}_0)}^2,
\end{multline*}
and where $\epsilon \approx 0$ is a small positive constant, $\lambda(t) :[0,T]\to\mathbb{R}_+$ is a positive weighting function typically set to $\lambda(t) = 1/ \mathbb{E}\left[\left\vert\left\vert\nabla_{\mathbf{x}_t} \log p_{\mathbf{x}_t\vert\mathbf{x}_0}(\mathbf{x}_t|\mathbf{x}_0) \right\vert\right\vert_2^2\right]$ (see~\citet[Section 3.3]{song2021scorebased}). Once $\boldsymbol{\phi}^{*}$ is acquired by minimizing~\eqref{eq:phi_star}, one can use the approximation $\nabla_{\mathbf{x}_t} \log p_{\mathbf{x}_t}(\mathbf{x}_t) \simeq \mathbf{s}_{\boldsymbol{\phi}^*}(\mathbf{x}_t, t)$. 

\subsection{Inverse Problems with Diffusion Models}
\label{sec-diffusion-posterior-sampling}

Inverse problems across various scientific domains share a unified mathematical framework. The objective in these problems is to infer unknown parameters $\mathbf{x}_{0}$ given a set of measurements 
$\mathbf{y} \in \mathbb{R}^{d_{y}}$.
To solve such problems in a Bayesian framework, one adopts a prior distribution $p_{\mathbf{x}_0}(\mathbf{x}_0)$ and a likelihood distribution  $p_{\mathbf{y}|\mathbf{x}_0}(\mathbf{y}|\mathbf{x}_0)$, and seeks to sample from the posterior distribution $p_{\mathbf{x}_0|\mathbf{y}}(\mathbf{x}_0|\mathbf{y})$. 
Using Bayes’ rule, the posterior is given by:
\begin{equation*}
p_{\mathbf{x}_0|\mathbf{y}}(\mathbf{x}_0|\mathbf{y}) = \frac{ p_{\mathbf{y}|\mathbf{x}_0}(\mathbf{y}|\mathbf{x}_0)p_{\mathbf{x}_0}(\mathbf{x}_0)}{p_{\mathbf{y}}(\mathbf{y})},
\end{equation*}
where the \textit{evidence} is $p_{\mathbf{y}}(\mathbf{y}) = \int p_{\mathbf{y}|\mathbf{x}_0}(\mathbf{y}|\mathbf{x}_0)p_{\mathbf{x}_0}(\mathbf{x}_0) d\mathbf{x}_0$. The diffusion-based approaches of Section~\ref{sec-score-based-diffusions} can be adapted to sample from the posterior by adopting the following reverse process
\begin{multline}
\label{eq:reverse_SDE_posterior}
\mathrm{d}\mathbf{x}_t = - \beta(t) \left(\frac{1}{2}\mathbf{x}_t+ \nabla_{\mathbf{x}_t}\log p_{\mathbf{x}_{t}\vert \mathbf{y}}(\mathbf{x}_t|\mathbf{y})  
\right)  \mathrm{d}t \\ + \sqrt{\beta(t)} \mathrm{d}\bar{\mathbf{w}}_t, \quad \mathbf{x}_T \sim p_{\mathbf{x}_T|\mathbf{y}}. 
\end{multline}
It follows from Bayes' rule that the score of the posterior is 
\begin{multline} 
\label{eq:score_posterior}
    \nabla_{\mathbf{x}_t }\log p_{\mathbf{x}_t | \mathbf{y}}(\mathbf{x}_t | \mathbf{y}) = \\ \nabla_{\mathbf{x}_t } \log p_{\mathbf{x}_t}(\mathbf{x}_t) + \nabla_{\mathbf{x}_t } \log p_{\mathbf{y}|\mathbf{x}_t}(\mathbf{y}|\mathbf{x}_t).
\end{multline}
Hence, computing the score of the posterior distribution can be reduced to evaluating two terms: the prior score function, $\nabla_{\mathbf{x}_t }\log p_{\mathbf{x}_t}(\mathbf{x}_t)$, and the likelihood score function,$\nabla_{\mathbf{x}_t} \log p_{\mathbf{y}|\mathbf{x}_t}(\mathbf{y}|\mathbf{x}_t)$. The former can be directly obtained using the trained prior score function $\mathbf{s}_{\phi^*}(\mathbf{x}_t, t)$. However, computing the latter is challenging in closed form due to its dependence on time $t$, as there is only an explicit dependence between $\mathbf{y}$ and $\mathbf{x}_0$. To address this, \citet{chung2023} propose to factorize
$p_{\mathbf{y}|\mathbf{x}_t}(\mathbf{y}|\mathbf{x}_t )$ as:
\begin{equation}
\label{eq-likelihood-y-xt}
    p_{\mathbf{y}|\mathbf{x}_t}(\mathbf{y}|\mathbf{x}_t ) 
    = \int p_{\mathbf{y}|\mathbf{x}_0}(\mathbf{y}|\mathbf{x}_0) p_{\mathbf{x}_0|\mathbf{x}_t}(\mathbf{x}_0|\mathbf{x}_t) \mathrm{d}\mathbf{x}_0,
\end{equation} 
which follows from the fact that $\mathbf{y}$ and $\mathbf{x}_{t}$ are conditionally independent given $\mathbf{x}_{0}$. The density $p_{\mathbf{x}_0|\mathbf{x}_t}(\mathbf{x}_0|\mathbf{x}_t)$ is generally intractable, making the approximation of the integral in~\eqref{eq-likelihood-y-xt} a challenging task.


\subsection{Sampling for Linear Inverse Problems}
\label{sec-sampling-linear-inverse-problem}

The existing literature has predominantly focused on applications where observations follow a Gaussian likelihood:
\begin{equation*} 
%\label{eq:gaussian_gaussian_case}
    \mathbf{y} = \mathcal{H}( \mathbf{x}_0) + \mathbf{u}, \quad \text{where} \quad \mathbf{u} \sim \mathcal{N}(0, \sigma_y^2 \mathbf{I}_{d_y}),
\end{equation*}
where $\mathcal{H}:\mathbb{R}^{d_{x}}\to\mathbb{R}^{d_{y}}$ is the forward measurement operator and $\mathbf{u}$ is the measurement noise.
Practical applications relevant to this work often involve a potentially non-invertible linear setting, where $ \mathcal{H}(\mathbf{x}_0) = \mathbf{H}\mathbf{x}_0 $ for an $ d_{y} \times d_{x} $ real matrix $ \mathbf{H} $ with $ d_{x} \leq d_{y} $. In this context, existing studies~\cite{chung2023,song2023pseudoinverseguided, boys2024,rozet2024} approximate the integral in~\eqref{eq-likelihood-y-xt} by employing a Gaussian approximation, $ q_{\mathbf{x}_0|\mathbf{x}_t}(\mathbf{x}_0|\mathbf{x}_t) $, for the true posterior distribution $ p_{\mathbf{x}_0|\mathbf{x}_t}(\mathbf{x}_0|\mathbf{x}_t) $. This Gaussian approximation is defined as:  
\begin{equation*} %\label{eq:gaussian_variational_distribution}  
q_{\mathbf{x}_0|\mathbf{x}_t}(\mathbf{x}_0|\mathbf{x}_t) = \mathcal{N}_{d_x}(\mathbf{x}_0; \mathbf{m}_0(\mathbf{x}_t), C_0(\mathbf{x}_t)),  
\end{equation*}  
where $ \mathbf{m}_0(\mathbf{x}_t) $ and $ C_0(\mathbf{x}_t) $ are the mean and covariance of the approximation, respectively. This approach enables the computation of closed-form expressions for $ p_{\mathbf{y}|\mathbf{x}_t}(\mathbf{y}|\mathbf{x}_t) $, as the integral in Equation~\eqref{eq-likelihood-y-xt} becomes analytically tractable\footnote{We informally interpret the delta function approximation in \citet{chung2023} as a degenerate Gaussian distribution where the variance approaches zero.}. 

Relevant to our work is the approach adopted by \citet{boys2024} who proposed approximating $ p_{\mathbf{x}_0|\mathbf{x}_t}(\mathbf{x}_0|\mathbf{x}_t) $ by projecting it onto the closest Gaussian distribution $ q_{\mathbf{x}_0|\mathbf{x}_t}(\mathbf{x}_0|\mathbf{x}_t) $ in terms of the Kullback-Leibler (KL) divergence. The closest Gaussian in this sense is the one that matches the first two moments, $\mathbb{E}_{p_{\mathbf{x}_0|\mathbf{x}_t}}[\mathbf{x}_0]$ and $\mathbb{E}_{p_{\mathbf{x}_0|\mathbf{x}_t}}[\mathbf{x}_0 \mathbf{x}_0^\top]$,  of the true posterior distribution. They estimated these moments using Tweedie's formula~\cite{Efron2011}.  