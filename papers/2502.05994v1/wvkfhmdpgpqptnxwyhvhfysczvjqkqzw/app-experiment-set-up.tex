\subsection{One-dimensional Benchmark Analysis}

\paragraph{Likelihood.} We considered multiple likelihood distributions to evaluate our approach in comparison to MCMC and DPS. Specifically, we included the Gaussian and Poisson likelihoods, defined as follows:
\begin{align*} 
&p_{\mathbf{y}^{\text{normal}} \vert \boldsymbol{\theta}}(\mathbf{y}^{\text{normal}}|\boldsymbol{\theta}) = \prod_{i=1}^{N}\prod_{j=1}^{d} \mathcal{N}\left(y_{i,j};\theta_{j}, 1\right),\\ &p_{\mathbf{y}^{\text{poisson}} \vert \boldsymbol{\theta}}(\mathbf{y}^{\text{poisson}}|\boldsymbol{\theta}) = \prod_{i=1}^{N}\prod_{j=1}^{d} \text{Poisson}\left(y_{i,j};\theta_{j}\right), \end{align*}
where we set $N = 1$ and $d = 30$.
Furthermore, to compare our approach to MCMC for additional likelihood distributions, we considered two additional distributions: the Exponential and Log-Normal likelihoods. These are given by
\begin{align*} 
&p_{\mathbf{y}^{\text{log-normal}} \vert \boldsymbol{\theta}}(\mathbf{y}^{\text{log-normal}}|\boldsymbol{\theta}) = \prod_{i=1}^{N}\prod_{j=1}^{d} \text{Log-Normal}\left(y_{i,j};\theta_{j}, 1\right),\\
&p_{\mathbf{y}^{\text{exponential}} \vert \boldsymbol{\theta}}(\mathbf{y}^{\text{exponential}}|\boldsymbol{\theta}) = \prod_{i=1}^{N}\prod_{j=1}^{d} \text{Exponential}\left(y_{i,j};\theta_{j}\right), 
\end{align*}
again with $N = 1$ and $d = 30$.


\paragraph{Prior.} The prior was a zero-mean Gaussian Process:
\begin{equation*}
    \mathbf{x}_0 \sim \mathcal{GP}(\boldsymbol{0}, \mathbf{K})
\end{equation*}
where $\mathbf{K}$ is a covariance matrix defined by an RBF kernel with variance 1 and length-scale $0.1$. 

\paragraph{Inverse link function.} 
The inverse link functions for the Gaussian, Log-Normal, and Exponential likelihood were specified as follows:
\begin{equation*}
\begin{aligned}
     \boldsymbol{\theta} = g^{-1}(\mathbf{x}_0) &= \mathbf{x}_0, &&  (\text{Normal likelihood})\\
    \boldsymbol{\theta} = g^{-1}(\mathbf{x}_0) &= \mathbf{x}_0, &&  (\text{Log-Normal likelihood})\\
    \boldsymbol{\theta} = g^{-1}(\mathbf{x}_0) &= 1 / \exp(\mathbf{x}_0)  && (\text{Exponential likelihood})
\end{aligned}
\end{equation*}
For the Poisson likelihood, we used two different link functions to explore both high-rate and low-rate regimes,
\begin{equation*}
\begin{aligned}
    \boldsymbol{\theta} = g^{-1}(\mathbf{x}_0) &= \exp(\mathbf{x}_0)  && (\text{Poisson likelihood with low rate})\\
    \boldsymbol{\theta} = g^{-1}(\mathbf{x}_0) &= \exp(5 +\mathbf{x}_0) && (\text{Poisson likelihood with high rate})
\end{aligned}
\end{equation*}

\paragraph{True Intensity.} The true intensity, denoted as $\boldsymbol{\theta}^{\text{true}}$, was generated by sampling from the prior distribution.

\paragraph{Observations.} For each likelihood (Normal, Poisson, Log-Normal, Exponential), we generated a single sample ($N = 1$) with dimension $d = 30$, using the corresponding true intensity. All the observations were allocated to the training set.

\subsection{Score-Based Cox Process.}
\label{app-score-based-cox-process-experiment}
\paragraph{Cox Process definition.}
We consider the inhomogeneous Poisson process on a domain $\mathcal{S}\subset\mathbb{R}^{D}$. This process is characterized by a stochastic intensity function $\lambda(\boldsymbol{s}):\mathcal{S}\to\mathbb{R}_{+}$, which specifies the rate at which events occur at each location $\boldsymbol{s}\in\mathcal{S}$. For any subregion $\mathcal{T}\subset\mathcal{S}$, the number of events $N(\mathcal{T})$ within $\mathcal{T}$ is Poisson-distributed with parameter $\lambda_{\mathcal{T}} = \int_{\mathcal{T}} \lambda(\boldsymbol{s})d\boldsymbol{s}$. Additionally, for any disjoint subsets $\mathcal{S}_{i}$ of $\mathcal{S}$, the event counts $N(\mathcal{S}_{i})$ are independent. This process is frequently referred to as a Cox process, as it was introduced by \citet{cox_process}.

Let $\mathbf{s} =(\boldsymbol{s}_{1},\ldots,\boldsymbol{s}_{M})$ with $\boldsymbol{s}_{m}\in \mathcal{S}$ for $m=1,\ldots,M$ be  a set of $M$ observed points. The probability density of $\mathbf{s}$ conditioned on the intensity function $\lambda$ is 
\begin{equation*}
    p_{\mathbf{s}\vert \lambda}(\mathbf{s}| \lambda) = \exp\left(-\int_{\mathcal{S}} \lambda(\boldsymbol{s})\mathrm{d}\boldsymbol{s}\right)\prod_{m=1}^{M}\lambda(\boldsymbol{s}_{m})
\end{equation*}
To make inference on $\lambda$, a prior distribution over the intensity function, $p_{\lambda}(\lambda)$, is introduced. Applying Bayesâ€™ rule, the posterior distribution of $\lambda$ conditioned on the observations is
\begin{equation*}
    p_{\lambda\vert\mathbf{s}}(\lambda | \mathbf{s}) = \frac{p_{\lambda}(\lambda)p_{\mathbf{s}\vert \lambda}(\mathbf{s}| \lambda)}{\int p_{\lambda}(\lambda)p_{\mathbf{s}\vert \lambda}(\mathbf{s}| \lambda) d\lambda}
\end{equation*}
This posterior is \textit{doubly intractable} due to the two challenging integrals: one over $\mathcal{S}$ in the numerator and another over $\lambda$ in the denominator.
% To ensure $\theta(s)\geq 0$ for all $s\in\mathcal{S}$ while allowing flexibility in modeling, we construct $\theta(s)$ from a base function $\theta(s):\mathcal{S}\to\mathbb{R}$. Specifically, we set
% \begin{equation}
%     \theta(s) = \exp(\theta(s))
% \end{equation}
% This approach contrasts with existing method, which either apply a sigmoid transformation to $\theta(s)$ and scale it by a maximum intensity, or alternatively, square $\theta(s)$, as proposed in \cite{Adams2009} and \cite{variational_gp}, respectively.

\paragraph{Likelihood.}
For computational simplicity, we assume that the observations $\mathbf{s}$ can be grouped into $d$ distinct sets, represented by a finite partition $\mathcal{P} = \{\mathcal{S}_{j}\}_{j=1}^{d}$ of $\mathcal{S}$. The count of observations over the partition is denoted by $\boldsymbol{y} = (y_{1},\ldots, y_{d})$, where $y_{j}$ is the number of observations in the set $\mathcal{S}_{j}$ and it is defined as $y_{j} = \sum_{m=1}^{M} \mathbbm{1}_{\{\boldsymbol{s}_{m}\in\mathcal{S}_{j}\}}$. Under this partition, we approximate the function $\lambda$ with a finite dimensional approximation $\boldsymbol{\theta}= (\theta_{1},\ldots,\theta_{d})\in\mathbb{R}^{d}$, where each $\theta_{j}$ approximates the aggregated function $\lambda$ over the subset $\mathcal{S}_{j}$. The discrete Cox process has the finite-dimensional distribution 
\begin{equation}
\label{eq-discrete-nhpp}
   p_{\boldsymbol{y}\vert \boldsymbol{\theta}}(\boldsymbol{y}|\boldsymbol{\theta})=\prod_{j=1}^{d} \text{Poisson}\left(y_{j};\theta_{j}|\mathcal{S}_{j}|\right)
\end{equation}
where $|\mathcal{S}_{j}|>0$ denotes the measure of $\mathcal{S}_{j}$. 
Henceforth, we assume that the sets $\{\mathcal{S}_j\}_{j=1}^d$ are of equal measure, which without loss of generality we set to $|\mathcal{S}_1|$. Furthermore, we assume that we can observe multiple samples, denoted as $\mathbf{y} = (\boldsymbol{y}_1, \ldots, \boldsymbol{y}_N)$, where each 
$\boldsymbol{y}_i$ is independently and identically distributed (i.i.d.) according to the distribution in~\eqref{eq-discrete-nhpp}. The likelihood of the discrete Cox process becomes, 
\begin{equation*}
   p_{\mathbf{y}\vert \boldsymbol{\theta}}(\mathbf{y}|\boldsymbol{\theta})=\prod_{i=1}^{N}\prod_{j=1}^{d} \text{Poisson}\left(y_{i,j};\theta_{j}|\mathcal{S}_{1}|\right).
\end{equation*}
% This discrete approximation enables a simplified sampling scheme: we can approximate samples from the process by drawing counts $N$
% independently from a Poisson distribution with rate parameter $\theta_{\mathcal{B}_{i}}|\mathcal{B}_{i}|$ for each $\mathcal{B}_{i}$. 
% The posterior $p_{\boldsymbol{\theta} \vert \mathbf{y} }(\boldsymbol{\theta} \vert \mathbf{y} )$ under this setting remains intractable due to the integral over $\boldsymbol{\theta}$ in the denominator of the posterior. 
% The task of sampling from the posterior distribution $p_{\boldsymbol{\theta} \vert \mathbf{y} }(\boldsymbol{\theta} \vert \mathbf{y} ) $ can be addressed using the methodology described in Section~\ref{sec:method}. Specifically, we first generate samples from $p_{\mathbf{x}_0 \vert \mathbf{y}}(\mathbf{x}_0 \vert \mathbf{y})$ and then obtain approximate samples from $p_{\boldsymbol{\theta} \vert \mathbf{y} }(\boldsymbol{\theta} \vert \mathbf{y} )$ by applying the inverse link function 

\paragraph{Evidence.}
As shown in~\eqref{eq-discrete-nhpp}, the intensity of the Poisson depends on the measure $|\mathcal{S}_1|$. Using the evidence trick, the log-density $\log p_{\mathbf{y}|\mathbf{x}_t}(\mathbf{y}|\mathbf{x}_t)$ is still tractable. A straightforward computation shows that $\log p_{\mathbf{y}|\mathbf{x}_t}(\mathbf{y}|\mathbf{x}_t)$ can be approximated as  
\begin{multline*}
    \log p_{\mathbf{y}|\mathbf{x}_t}(\mathbf{y}|\mathbf{x}_{t})
     \approx \log h_{\mathbf{y}}(\mathbf{y}) + \vert \mathcal{S}_1\vert\, \mathbf{T}_{\mathbf{y}}(\mathbf{y})  - A_{\boldsymbol{\theta}}\left(\boldsymbol{\nu}(\mathbf{x}_t), \boldsymbol{\tau}(\mathbf{x}_t)\right) + A_{\boldsymbol{\theta}}\left(\mathbf{T}_{\mathbf{y}}(\mathbf{y}) + \boldsymbol{\nu}(\mathbf{x}_t), \boldsymbol{\tau}(\mathbf{x}_t) + (N  \vert \mathcal{S}_1\vert) \mathbf{1}_d\right) .
\end{multline*}
where the likelihood's base measure $h_{\mathbf{y}}$ and sufficient statistics $\mathbf{T}_{\mathbf{y}}$, as well as the natural conjugate prior's log-partition function $A_{\boldsymbol{\theta}}$ are the same as in standard Poisson Likelihood case, presented in Appendix~\ref{app-table_distributions}.

\paragraph{Prior.}
The prior samples of $\mathbf{x}_0$ are images from the ImageNet train dataset, which are first converted to grayscale, then resized such that their smaller
edge measured 256 pixels, center-cropped to the size $256\times 256$ and finally scaled to the range $[-1, 1]$ using the transformation
\begin{align*}
\mathbf{x}_0 = 2 \frac{\mathbf{x}_0^{\text{orginal}} - \min(\mathbf{x}_0^{\text{orginal}})} { \max(\mathbf{x}_0^{\text{orginal}}) -  \min(\mathbf{x}_0^{\text{orginal}})} - 1
\end{align*}
Note that we converted the images to grayscale, reducing them to a single channel, as this aligns with the latent parameter space typically encountered in real-world problems.

\paragraph{Inverse link function.}
We used the following inverse link function:
\begin{equation*}
\boldsymbol{\theta} = g^{-1}(\mathbf{x}_0) = \exp(\mathbf{x}_0).
\end{equation*}

\paragraph{True intensity.}
The true intensity $\boldsymbol{\theta}^{\text{true}}$ is obtained by applying the inverse link function on the true latent variable  $\mathbf{x}_0^{\text{true}}$. We selected the following true latent variables:
\begin{itemize}
    \item Two images from the ImageNet validation set: A picture of a corn and a house were used in the experiments. The images underwent the same preprocessing steps as the prior samples.
    \item Building Height in London: We used the average building height in 2023 (variable \texttt{MEAN\_OBJECT\_HEIGHT\_M}) in Greater London sourced from~\citet{LBSM}. The map was cropped to a region defined by latitudes between $51.3274$ and $51.6874$ and longitudes between $-0.4178$ and $0.1622$. The data were then rescaled to a $256 \times 256$ grid, where each grid cell contained the average building height within its boundaries. Finally, the building height values were normalized to the range $[-1, 1]$ using the following transformations:  
\begin{align*}
\mathbf{x}_0^{\text{true}} = 2  \frac{\mathbf{x}_0^{\text{true, norm}} - \min(\mathbf{x}_0^{\text{true, norm}})}{\max(\mathbf{x}_0^{\text{true, norm}}) - \min(\mathbf{x}_0^{\text{true, norm}})} - 1, \quad \mathbf{x}_0^{\text{true, norm}} = \log(\mathbf{x}_0^{\text{true, original}} + 1).
\end{align*}
\item Satellite image: The satellite image was obtained using Google Earth Engine with Sentinel-2 Image Collection. The data was filtered to cover the region of New York City Manhattan area defined by latitudes between $-74.02$ and $-73.97$ and longitudes between $40.7$ and $40.75$, at a resolution of $10$ meters, and in the time range from January 1, 2021, to December 31, 2021. The RGB image was converted to grayscale.
A $256\times256$ region was cropped starting at coordinates 
$(100,200)$. The least cloudy image was selected based on the \texttt{CLOUDY\_PIXEL\_PERCENTAGE} property.  Finally, entries were normalized following transformations:  
\begin{align*}
\mathbf{x}_0^{\text{true}} = -\left(2  \frac{\mathbf{x}_0^{\text{true, norm}} - \min(\mathbf{x}_0^{\text{true, norm}})}{\max(\mathbf{x}_0^{\text{true, norm}}) - \min(\mathbf{x}_0^{\text{true, norm}})}\right)^4 + 1, \quad \mathbf{x}_0^{\text{true, norm}} = \log(\mathbf{x}_0^{\text{true, original}} + 0.01).
\end{align*}
The purpose of this transformation was to assign high values to the streets and low values to the buildings.
\end{itemize}

\paragraph{Observations.}
We generated $N$ i.i.d samples of the Cox process, $\mathbf{y} = \{\boldsymbol{y}_i\}_{i=1, \ldots, N}$, where the set of variables for one sample is $\boldsymbol{y}_i = \{y_{i,j}\}_{j=1, \ldots, d}$. Given the true intensity $\boldsymbol{\theta}^{\text{true}}$, we generated the variables $y_{i,j}$ with
\begin{equation*}
    y_{i,j} \sim \text{Poisson}(\theta_j^{\text{true}} |\mathcal{S}_1|),
\end{equation*}
where $|\mathcal{S}_1| = 1$ and for $j = 1, \ldots, d$ and $i = 1, \ldots, N$. This resulted in $N \times 256 \times 256 = N \times 65,536$ observations. We randomly allocated 80\% of the observations ($N \times 52,428$) to the training set and 20\% to the test set ($N \times 13,108$).




\subsection{Prevalence of Malaria in Sub-Saharan Africa.}

\paragraph{Likelihood.} 
In each location $j = 1, \ldots, d$, the observations consist of the number of positive cases, denoted as $y_j$, out of the total number of individuals examined, $n_j$. 
Accordingly, the number of positive cases follows a Binomial distribution
\begin{align*}
p_{\mathbf{y}|\boldsymbol{\theta}}(\mathbf{y}|\boldsymbol{\theta}) = \prod_{j = 1}^d \text{Binomial}(y_j; n_j, \theta_j)
\end{align*}
We note that in this experiment $N =1$, so we omit the indexing on $i$.

\paragraph{Prior.} The same prior as in the Score-Based Cox Process experiment was used.

\paragraph{Inverse link function.}
We used the following inverse link function:
\begin{equation*}
\boldsymbol{\theta} = g^{-1}(\mathbf{x}_0) = \text{sigmoid}(s\, \mathbf{x}_0),
\end{equation*}
with $s=5$. Such that the range covered by the prior on parameters $\boldsymbol{\theta}$ is $\text{sigmoid}(-5) \approx 0.001$ and $\text{sigmoid}(5) \approx 0.99$. 


\paragraph{Observations.}
We used a real-world dataset on PfPR in Sub-Saharan Africa from the Malaria Atlas Project~\citep{Bhatt2015-uk,Pfeffer2018-cm,Weiss2019-au}. 
We ignore temporal aspects, and only aim to interpolate spatial data across all of Sub-Saharan Africa. We use a grid resolution of $256 \times 256$, equivalent to a $\sim 111 \text{ km}^2$ resolution, and aggregate the count of the number of positive cases and the number of examined to this resolution. Out of the grid, $7,048$ ($10.75$\%) entries had non-missing observations. We randomly allocated 80\% of the observations ($5,621$) to the train set and 20\% to the test set ($1,427$). 





\subsection{Benchmark Methods}
\label{app-benchmark}

\subsubsection{Diffusion Posterior Sampling}
\label{app-dps}
Recall from~\eqref{eq-likelihood-y-xt} the following factorization of the conditional density $p_{\mathbf{y}|\mathbf{x}_t}(\mathbf{y}|\mathbf{x}_t)$ that we repeat here for convenience:
\begin{equation*}
    p_{\mathbf{y}|\mathbf{x}_t}(\mathbf{y}|\mathbf{x}_t) 
    = \int p_{\mathbf{y}|\mathbf{x}_0}(\mathbf{y}|\mathbf{x}_0) \, p_{\mathbf{x}_0|\mathbf{x}_t}(\mathbf{x}_0|\mathbf{x}_t) \, \mathrm{d}\mathbf{x}_0.
\end{equation*}
for all $t\in[\epsilon,1]$.
When $p_{\mathbf{y}|\mathbf{x}_0}(\mathbf{y}|\mathbf{x}_0)$ is a Gaussian distribution with mean $\mathcal{H}(\mathbf{x}_0)$ and known covariance matrix $\boldsymbol{\Sigma}_{d} = \sigma^2 \mathbf{I}_{d}$, \citet{chung2023} proposed approximating $p_{\mathbf{y}|\mathbf{x}_t}(\mathbf{y}|\mathbf{x}_t)$ by  $p_{\mathbf{y}|\mathbf{x}_0}(\mathbf{y}|\hat{\mathbf{x}}_0)$ where $\hat{\mathbf{x}}_0$ denotes the mean of $p_{\mathbf{x}_0|\mathbf{x}_t}(\mathbf{x}_0|\mathbf{x}_t)$:
\begin{equation*}
\hat{\mathbf{x}}_0= \mathbb{E}_{\mathbf{x}_0 \sim p_{\mathbf{x}_0|\mathbf{x}_t}}[\mathbf{x}_0]. 
\end{equation*}
This approach approximates $p_{\mathbf{x}_0|\mathbf{x}_t}(\mathbf{x}_0|\mathbf{x}_t)$ with a Dirac delta function centered at $\hat{\mathbf{x}}_0$. Consequently, the integral in~\eqref{eq-likelihood-y-xt} simplifies to the following closed-form expression:
\begin{equation*}
p_{\mathbf{y}|\mathbf{x}_t}(\mathbf{y}|\mathbf{x}_t) \approx \frac{1}{\sqrt{(2\pi)^{n}\sigma^{2n}}}\exp\left[-\frac{\norm{\mathbf{y}-\mathcal{H}(\hat{\mathbf{x}}_{0} (\mathbf{x}_t) )}^{2}}{2\sigma^{2}}\right]
\end{equation*}
where the dependency of $\hat{\mathbf{x}}_{0}$ on $\mathbf{x}_t$ has been made explicit by the notation $\hat{\mathbf{x}}_{0} (\mathbf{x}_t)$.Here, $n$ represents the total number of observations, defined in our notation as $n=N \times d$. It is worth noting that~\citet{chung2023}, in their experiments, focused on the case where $N=1$, making $n=d$. Nevertheless, their methodologies naturally extend to cases where $N>1$. Moreover, note that under our notation, $\mathcal{H}(\cdot) = g^{-1}(\cdot)$.

In the context of VP-SDEs, $\hat{\mathbf{x}}_0$ has an analytically tractable form:
\begin{equation}
\label{eq:tweedieformula}
    \hat{\mathbf{x}}_0 = \frac{1}{\sqrt{\alpha_t} }\left(\mathbf{x}_t + (1-\alpha_t) \nabla_{\mathbf{x}_t} \log p_{\mathbf{x}_t}(\mathbf{x}_t)\right),
\end{equation}
where $\alpha_t = \exp\left(-\left(t \beta_0 + \left(\beta_1 - \beta_0\right) \frac{t^2}{2}\right)\right)$. The expression in~\eqref{eq:tweedieformula}
is often referred to as \textit{Tweedie's formula}. 
Here, the prior score function $\nabla_{\mathbf{x}_t} \log p_{\mathbf{x}_t}(\mathbf{x}_t)$ appearing in~\eqref{eq:tweedieformula} can be approximated using the score network $\mathbf{s}_{\boldsymbol{\phi}^{\star}}(\mathbf{x}_t, t)$. 

\textbf{Observations following a Normal distribution.} For inverse problems corrupted by Gaussian noise with unknown mean and known variance $\sigma^2$, \citet{chung2023} proposed to approximate the posterior score function with:
\begin{equation*}
\nabla_{\mathbf{x}_t}\log p_{\mathbf{x}_t \vert  \mathbf{y}}(\mathbf{x}_t \vert  \mathbf{y}) \simeq \mathbf{s}_{\boldsymbol{\phi}^{\star}}(\mathbf{x}_t, t) - \rho \sum_{i = 1}^N  \nabla_{\mathbf{x}_t} \norm{ \boldsymbol{y}_i - \mathcal{H}(\hat{\mathbf{x}}_{0}) }^2,
\end{equation*}
where $\rho = 1/ \sigma^2$.

\textbf{Observations following a Poisson distribution.} For inverse problems corrupted by Poisson noise, \citet{chung2023} proposed two methodologies. The first, termed \textit{Poisson-LS}, applies the same least squares (LS) method used for Gaussian noise. This approach is justified by the fact that Poisson noise approximates Gaussian noise in high signal-to-noise regimes. The posterior score function associated with this approximation is expressed as:
\begin{equation*}
\nabla_{\mathbf{x}_t}\log p_{\mathbf{x}_t \vert  \mathbf{y}}(\mathbf{x}_t \vert  \mathbf{y}) \simeq \mathbf{s}_{\boldsymbol{\phi}^{\star}}(\mathbf{x}_t, t) - \rho_t \sum_{i = 1}^N  \nabla_{\mathbf{x}_t} \norm{ \boldsymbol{y}_i - \mathcal{H}(\hat{\mathbf{x}}_{0}) }^2.
\end{equation*}
where $\rho_t$ is a weight that depends on diffusion timestep $t$.
The second approach, which we call \textit{Poisson-DPS}, adopts a shot noise approximation. The posterior score function associated with this approximation is expressed as:
\begin{equation} \label{eq:poisson_dps}
\nabla_{\mathbf{x}_t}\log p_{\mathbf{x}_t \vert  \mathbf{y}}(\mathbf{x}_t \vert  \mathbf{y}) \simeq \mathbf{s}_{\boldsymbol{\phi}^{\star}}(\mathbf{x}_t, t) - \rho_t \sum_{i = 1}^N  \nabla_{\mathbf{x}_t} 
\left\lVert \boldsymbol{y}_i - \mathcal{H}(\hat{\mathbf{x}}_{0}) \right\rVert_{\boldsymbol{\Lambda}_i}^2.
\end{equation}
where $\left\lVert \mathbf{a} \right\rVert_{\boldsymbol{\Lambda}}^2 = \mathbf{a}^\top \boldsymbol{\Lambda} \mathbf{a}$ and $\boldsymbol{\Lambda}_i$ is a diagonal matrix with entries $[\boldsymbol{\Lambda}_i]_{jj} = 1/(2 y_{i,j})$.
The step size $\rho_t$ in each method is set to 
\begin{equation*}
    \rho_t = \frac{\rho'}{\sqrt{\sum_{i = 1}^N \norm{\boldsymbol{y}_i - \mathcal{H}(\hat{\mathbf{x}}_{0}(\mathbf{x}_t))}^2}}
\end{equation*}
where $\rho'$ is a hyperparameter which is manually chosen and where the dependency on $\mathbf{x}_{t}$ has been made explicit. The recommended value for Poisson noise is $\rho' = 0.3$. 

The experiments presented in Section~\ref{sec-1d-synthetic-data} have results provided in Appendix~\ref{app-results-synthetic-1d}. For the Gaussian likelihood (Figure~\ref{fig:synthetic-data-experiment-gaussian}), we observed that DPS provided point estimates within the correct range. However, the uncertainty captured by DPS was underestimated. We hypothesize that this discrepancy arises from the uncertainty of the conditional distribution $p_{ \mathbf{y} \vert  \mathbf{x}_t}(\mathbf{y} \vert  \mathbf{x}_t)$ not being fully captured. In contrast, our method produced results that were closer to the ground truth MCMC posterior for both the point estimate and the uncertainty.
For the Poisson likelihood, we initially tested a low-rate regime (Figure~\ref{fig:synthetic-data-experiment-poisson-low-rate}), but both Poisson-LS and Poisson-DPS performed poorly. We suspected that the norm used in Equation~\eqref{eq:poisson_dps} introduces numerical instabilities because of the division by Poisson observations, which cannot naturally handle zero-count events. To mitigate this, we added a small noise term ($0.01$) to the zero observations, but it did not improve the results. We then tried a high-rate regime (Figure~\ref{fig:synthetic-data-experiment-poisson-high-rate}) but the performance remained unsatisfactory. 
We experimented with different values of $\rho'$ but found that these variations had no noticeable impact on performance. The results presented are with the recommended value $\rho' = 0.3$.
  
\subsubsection{Gaussian Process}
To obtain the ground-truth MCMC results of the experiment presented in Section~\ref{sec-1d-synthetic-data}, we used \texttt{PyStan} version 3.10.0~\citep{pystan}. Four Hamiltonian Monte Carlo (HMC) chains were run in parallel for 600 iterations, with the first 100 iterations considered as warm-up. 






\subsubsection{Gaussian Markov Random Field}

In the experiment described in Section~\ref{sec-experiment-malaria}, we compared our results with the case where the prior $p_{\mathbf{x}_{0}}(\mathbf{x}_{0})$ was defined by a GMRF. 
We considered a discrete set of $d$ points $\mathcal{S}=\{\mathbf{s}_{1},\ldots,\mathbf{s}_{d}\}$, where $\mathbf{s}_{j}\in\mathbb{R}^{2}$ for $j=1,\ldots,d$.
%Let $W$ and $H$ denote the number of points along the $x$-axis (width) and $y$-axis (height), respectively. The total number of points on the grid is $d = W \times H$. The set of grid points is denoted as $\boldsymbol{G} = (\mathbf{g}_1, \ldots, \mathbf{g}_d)$, where each $\mathbf{g}_j \in \mathbb{R}^2$ for $j = 1, \ldots, d$.
%Given observations $\{(\mathbf{g}_j, \mathbf{x}_0(\mathbf{g}_j))\}_{j=1}^d$, the function $\mathbf{x}_0$ is modeled as a realization of a zero-mean two-dimensional Gaussian Markov Random Field:
% \paragraph{Score-Based Cox Process.}
% In the experiment described in Section~\ref{sec-experiment-cox-process}, we use the following model:
% \begin{align*}
%     &y_{i,j} \sim \text{Poisson}(\theta_j), \\
%     &\theta_j = \exp(\beta_0 + x_{0,j})\\ 
%     &p(\beta_0) \propto 1, \\
%     &\mathbf{x}_{0} \sim \mathcal{N}(\boldsymbol{0}, \mathbf{K}),
% \end{align*}
% for $i = 1, \ldots N$, $j = 1, \ldots, d$ and where $\mathbf{K}$ is a covariance matrix. 
% \paragraph{Malaria Prevalence in Sub-Saharan Africa.}
We use the following model:
\begin{align*}
    &y_{j} \sim \text{Binomial}(n_j, \theta_j), \\
    &\theta_j = \text{Sigmoid}(\beta_0 + x_{0,j})\\ 
    &p(\beta_0) \propto 1, \\
    &\mathbf{x}_{0} \sim \mathcal{N}(\boldsymbol{0}, \mathbf{K}),
\end{align*}
for  $j = 1, \ldots, d$ and where $\mathbf{K}$ is a covariance matrix. Note that there are only one sample of the observations so we omit the indexing on $i$.

The entries of $\mathbf{K}$ are given by
% \begin{equation}
%     \mathbf{K}_{\mathbf{s}, \mathbf{s}'} = \text{Cov}(\mathbf{x}_0(\mathbf{g}), \mathbf{x}_0(\mathbf{g}')) = k(\mathbf{s}, \mathbf{s}') \quad \text{for } \mathbf{s}, \mathbf{s}' \in \mathcal{S},
% \end{equation}
\begin{equation*}
    \mathbf{K}_{i, j}  = k(\mathbf{s}_{i}, \mathbf{s}_{j}) \quad \text{for } \mathbf{s}_{i}, \mathbf{s}_{j} \in \mathcal{S},
\end{equation*}
where $k(\cdot, \cdot)$ is the kernel function. 
\citet{Lindgren2011-fv} introduced an explicit link between a certain stochastic partial differential equation and GMRFs. \citet{Lindgren2011-fv} considers linear stochastic partial differential equations of the form $\mathcal{L} u(\cdot) = \mathcal{W}(\cdot)$ to define random fields $u(\cdot)$ with differential operator $\mathcal{L}$ and $\mathcal{W}$ is a Gaussian white noise process on a general domain. Choosing $\mathcal{L}=\tau(\kappa^2-\triangle)^{\frac{\alpha}{2}}$, the resulting stochastic partial differential equation
\begin{equation*}
    \tau(\kappa^2-\triangle)^{\frac{\alpha}{2}} u = \mathcal{W}
\end{equation*}
have stationary solutions with a MatÃ©rn kernel function of the form 
\begin{equation*}
    k(\mathbf{s}, \mathbf{s}') = \frac{\sigma^2}{\Gamma(\nu)2^{\nu-1}}(k||\mathbf{s}-\mathbf{s}'||)^\nu K_\nu (k||\mathbf{s}-\mathbf{s}'||).
\end{equation*}
Where $\nu=\alpha = \frac{d}{2}$ and $\sigma^2 = \Gamma(\nu)\{\Gamma(\alpha)(4\pi)^{d/2}\kappa^{2\nu}\tau^2\}^{-1}$ and $K_\nu$ is the modified Bessel function of the second kind. $\nu>0$ is called the smoothness index (generally fixed to $\frac{1}{2},\frac{3}{2},\frac{5}{2}$), $\kappa>0$ is the spatial range and $\sigma^2$ is the marginal variance. The stochastic partial differential equation can be solved using the finite element method resulting in a GMRF. This approach therefore allows the construction of sparse precision matrices for GMRFs that are invariant to geometry of the spatial neighborhood and allow for extremely accurate low rank approximations with computational tractability. Inference using these precision matricies is performed using approximate Bayesian inference via the integrated nested Laplace approximation~\citep{Rue2009-ty} and constitutes the state-of-the-art in spatial statistics for approximate methods~\citep{Heaton2017-vl}. 

We used \texttt{INLA} version 24.12.11 to fit the GMRF. 
In the experiment, the MatÃ©rn stochastic partial differential equation with the parameter $\alpha = 1$ yielded the best results.
