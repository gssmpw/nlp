In this appendix, we discuss our proposed approach to select the link function $g(\cdot)$. It is important to note that this is a suggested method and that any link function satisfying Assumption~\ref{ass-link-function} can be utilized. The proposed link functions, categorized by likelihood distribution, are summarized in Table~\ref{tab:data-distributions-details}.

Let us recall the problem formulation outline in Section~\ref{sec:method}. For notational convenience, we omit the subscripts on $y$ and $\theta$. We assume partial measurements $y\in \mathcal{Y} \subseteq \mathbb{R}$ dependent on a parameter $\theta \in \Theta \subseteq \mathbb{R}$ following a one-parameter univariate exponential family distribution $p(y|\theta)$ of the form
\begin{equation*}
p(y \vert {\theta}) = h_y(y) \exp\left( {\eta}({\theta}) {T}_y(y) - A_y(\eta({\theta})) \right),
\end{equation*}
where ${\eta}({\theta})$ are the natural parameters, $A_y(y)$ is the log-partition function, ${T}_y(y)$ are the sufficient statistics and $h_y(y)$ is the base measure. 
Further, the parameter $\theta$ is related to a latent variable $x_0$ through the relationship: 
\begin{align*}
    \theta = g^{-1}(x_0).
\end{align*}

\subsection{Direct Derivation of the Link Function}

For a parameter that represents a probability, it is common to directly model the relationship between the latent variable and the parameter using a sigmoid function. 
Similarly, when the parameter represents a variance, it is typical to model the relationship between the latent variable and the parameter with an exponential function.

\paragraph{Link function for the Binomial, Negative Binomial, and Geometric distributions.} We define the link function as $g^{-1}(\mathbf{x}_0) = \text{sigmoid}(\mathbf{x}_0)$.

\paragraph{Link function for the Normal and Log-Normal distributions, with fixed mean.} We define the link function as $g^{-1}(\mathbf{x}_0) = \exp(\mathbf{x}_0)$.

\subsection{Derivation of the Link Function via the Expectation of the Sufficient Statistic}

For the discussion that follows, we define the function $g_2 : \Theta \to \mathcal{M}$ as:
\begin{equation*}
    g_2(\theta) := \frac{\mathrm{d}\theta}{\mathrm{d}\eta}\Big|_{\eta = \eta(\theta)} \frac{\mathrm{d}}{\mathrm{d}\theta} A(\eta(\theta)),
\end{equation*}
where $\mathcal{M} = \left\{g_2(\theta): \theta \in \Theta \right\}$.
From Corollary~\ref{corollary:derivative_log_partition_function}, it follows that the expectation of the sufficient statistic $T_y(y)$ is given by:
\begin{align*}
    \mathbb{E}_{y|\theta}[T_y(y)] = g_2(\theta).
\end{align*}

In the context of our hierarchical probabilistic model (Figure~\ref{fig-graphical-model}), when the parameter of interest $\theta$ is a shape, a rate or a scale, it is usually more intuitive to model the relationship between the latent variable $x_0$ and the expectation of the sufficient statistics $E_{y|\theta}[T_y(y)]$, rather than directly modeling the relationship between a latent variable $x_0$ and the parameter $\theta$ itself.
For example, in modeling with the exponential distribution, it is common to set the parameter as the inverse of a function of the latent variable. This arises because the expectation of a random variable following the exponential distribution with rate $\theta$ is the inverse of its parameter, $1/\theta$.
Therefore, we propose that the link function $g(\cdot)$ should be constructed as a composition of two distinct link functions. The first link function relates the latent variable to the expectation of the sufficient statistics, while the second maps the expectation of the sufficient statistics to the parameter. Specifically, the inverse link function can be expressed as:
\begin{align*}
    &g^{-1}(\cdot) = g_2^{-1}(g_1^{-1}(\cdot)),  \\
    &g_1^{-1}(\cdot) : \mathbb{R} \to \mathcal{M}, \quad g_2^{-1}(\cdot) : \mathcal{M} \to \Theta, 
\end{align*}
such that
\begin{align*}
    \mathbb{E}_{y|\theta}[T_y(y)] &= g_1^{-1}(x_0)\\
    \theta &= g_2^{-1}\left(\mathbb{E}_{y|\theta}[T_y(y)]\right).
\end{align*}
The following assumption is imposed on the link function $g_1$
\begin{assumption}
\label{ass-link-function-appendix}
    We assume that $g_1: \mathcal{M} \to \mathbb{R}$ is a continuously differentiable, one-to-one function and with $\frac{\mathrm{d}g}{\mathrm{d}x}\neq~0$ for all $x\in\mathcal{M}$.
\end{assumption}
Assumption~\ref{ass-link-function-appendix} is standard and is consistent with those typically used in the context of the change-of-variable technique in probability and statistics (see Theorem 17.2 in \cite{billingsley_prob}).
% \begin{example}
%     Consider the case of the exponential distribution. The corresponding components of the exponential family are given by:
%     \begin{align}
%         \eta(\theta) &= -\theta, \quad A_y(\theta) = -\log(\theta), \quad T_y(y) = y.
%     \end{align}
%     From this, it follows that $ g_2(\theta) = 1/{\theta}$.
%     We specify that the expectation of $T_y(y)$ under $p_{y|\theta}(y|\theta)$ is related to the latent variable $x_0$ through:
%     \begin{equation}
%         \mathbb{E}_{y|\theta}[T_y(y)] = \mathbb{E}_{y|\theta}[y] = g_1^{-1}(x_0) = \exp(x_0).
%     \end{equation}
%     Substituting this relationship, we obtain the relationship between the parameter and the latent variable:
%     \begin{equation}
%         \theta = g^{-1}(x_0) = 1 / \exp(x_0).
%     \end{equation}
% \end{example}
Note that this approach of deriving the link function is related to that of a Generalized Linear Model (GLM), which considers distributions from the exponential family for which $T_y(y) = y$. In GLMs, the linear predictor $x_0 :=\mathbf{X} \boldsymbol{\beta} \in \Lambda$ is associated with the first moment, denoted by $\mu := \mathbb{E}_{y|\theta}[y] \in  \mathcal{M} $, through a link function $\tilde{g}(\cdot)$. This relationship is expressed as $\mu = \tilde{g}^{-1}(x_0)$. 
GLMs assume that the linear predictor belongs to the same space as the natural parameters $\eta \in \Lambda$, which is often not the case in practice.
A commonly used link function in GLMs, known as the canonical link function, assumes that $\tilde{g}^{-1}(\cdot): \Lambda \to \mathcal{M} =  \frac{\mathrm{d}}{\mathrm{d}\eta }A(\eta)$.

\paragraph{Link function for the Normal distribution with variance $\sigma^2$.} We find that $g_2(\theta) = \theta$, further we decide to use $g^{-1}_1(\mathbf{x}_0) = \mathbf{x}_0$. Therefore, $g^{-1}(\mathbf{x}_0) = \mathbf{x}_0$.

\paragraph{Link function for the Log-Normal distribution with variance $\sigma^2$.} We find that $g_2(\theta) = \theta$, further we decide to use $g^{-1}_1(\mathbf{x}_0) = \mathbf{x}_0$. Therefore, $g^{-1}(\mathbf{x}_0) = \mathbf{x}_0$.

\paragraph{Link function for the Poisson distribution.} We find that $g_2(\theta) = \theta$, further we decide to use $g^{-1}_1(\mathbf{x}_0) = \exp(\mathbf{x}_0)$. Therefore, $g^{-1}(\mathbf{x}_0) = \exp(\mathbf{x}_0)$.

\paragraph{Link function for the Exponential distribution.} We find that $g_2(\theta) = 1/\theta$, further we decide to use $g^{-1}_1(\mathbf{x}_0) = \exp(\mathbf{x}_0)$. Therefore, $g^{-1}(\mathbf{x}_0) = 1/\exp(\mathbf{x}_0)$.

\paragraph{Link function for the Gamma distribution with fixed shape $a$.} We find that $g_2(\theta) = a/\theta$, further we decide to use $g^{-1}_1(\mathbf{x}_0) = \exp(\mathbf{x}_0)$. Therefore, $g^{-1}(\mathbf{x}_0) = a/\exp(\mathbf{x}_0)$.

\paragraph{Link function for the Pareto distribution with fixed scale $x_m$.} We find that $g_2(\theta) = 1/\theta + \log(x_m)$, further we decide to use $g^{-1}_1(\mathbf{x}_0) = \exp(\mathbf{x}_0)$. Therefore, $g^{-1}(\mathbf{x}_0) = 1/(\exp(\mathbf{x}_0) - \log(x_m))$.

\paragraph{Link function for the Weibull distribution with fixed shape $k$.} We find that $g_2(\theta) = \theta$, further we decide to use $g^{-1}_1(\mathbf{x}_0) = \exp(\mathbf{x}_0)$. Therefore, $g^{-1}(\mathbf{x}_0) = \exp(\mathbf{x}_0)$.

\subsection{Ensuring Adequate Prior Coverage}
To ensure the stability of the neural network, it is often desirable for $\mathbf{x}_0$ --- the latent variable upon which $\mathbf{x}_t$ depends for $t <1$ --- to lie within a scalable range, such as $[0, 1]$ or $[-1, 1]$. In this context, it is crucial to consider whether the range of the prior distribution can encompass the desired values of the parameter $\boldsymbol{\theta}$. 
For instance, if $\mathbf{x}_0$ is constrained to the range $[-1, 1]$ and the data are binomial, the value of the parameter $\boldsymbol{\theta}$ that the prior covers are limited to the range $\text{sigmoid}(-1) \approx 0.27$ and $\text{sigmoid}(1) \approx 0.73$. 
A practical solution is to introduce a scaling factor $s$ to the link function. For binomial data, this adjustment modifies the link function to become $g^{-1}(\mathbf{x}_0) = \text{sigmoid}(s\,\mathbf{x}_0)$. In the example above, if $s=5$, the range of the parameters covered become $\text{sigmoid}(-5) \approx 0.001$ and $\text{sigmoid}(5) \approx 0.99$. 