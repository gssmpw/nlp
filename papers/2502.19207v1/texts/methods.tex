Unlearning methods should erase only the knowledge associated with the target knowledge while preserving all other knowledge.
In this section, we describe the method, \ourmodel, that identifies neurons contextually related to the target knowledge and updates only them during the unlearning process.



\subsection{Quantifying Knowledge Relevance}



\subsubsection{Knowledge Quantification} We utilize an attribution method \citep{shrikumar2016not} to extract the importance of neurons for specific world knowledge from language models.
It is usually used to derive the importance of the input features \textit{(i.e., pixel, token)} for performing a specific task, but \citet{yang2023task} expands the attribution formula to the importance of intermediate neurons in language models.
Formally, suppose we have $P_{\theta}(y|x) = \prod_{t=1}^{T} P_{\theta}(y_{t}|x,y_{1},...,y_{t-1})$ that represents a language model.
The contribution of an $i$-th neuron to the representation $h$ in a particular layer, in predicting an answer $a$ given a question $q$ using $P_{\theta}$, is defined as follows:



\vspace{-0.2cm}
\begin{equation}
\begin{aligned}
    \text{\normalsize $A^{(q,a)}_{i}(h^{l})=h^{l}_{i}\times \frac{\partial P_{\theta}(a|q)}{\partial h^{l}_{i}},$}
     \\
    \text{\normalsize $A^{(q,a)}_{i}(h) = \max_{l} A^{(q,a)}_{i}(h^{l}),$}\\
\end{aligned}
\label{eq:attr_lm}
\end{equation}
\vspace{-0.2cm}

\noindent where $h^{l}$ means $l$-th token representation of $h$, and $\partial P_{\theta}(a|q)/\partial h^{l}_{i}$ is the gradient of $P_{\theta}(a|q)$ with respect to $h^{l}_{i}$.
In this study, we use transformer variants for experiments; thus, activation scores and gradients of a specific layer are computed for each input token representation.
Therefore, if an input text includes $L$ tokens, we have $L$ attribution scores for each neuron; thus, we aggregate attributions of tokens by using \textit{max aggregation} to acquire a single neuron knowledge attribution $A^{(q,a)}_{i}(h)$.


\subsubsection{Superficial Knowledge Regularization} Equation~\ref{eq:attr_lm} computes the knowledge relevance of each neuron for a specific $(q,a)$ pair.
However, this equation may include undesirable information that only serves to increase the likelihood of the answer $a$ regardless of the given context.
To eliminate undesirable information from the computed attribution, we construct synthetic mismatched QA pairs $(q',a) \in \mathcal{C}'$, where the answers remain the same as the target answer $a$, while the questions are randomly sampled independently of the answer.
Then, we compute the attribution score for each mismatched pair and average them.
Since a question and an answer included in mismatched pairs are contextually irrelevant, the computed attribution corresponds to the degree that unconditionally increases the likelihood of the answer regardless of the context (superficial knowledge).
Therefore, we can compute the final knowledge attribution, $\mathcal{I}$, containing only contextual knowledge by excluding the information of the mismatched attribution from the basic knowledge attribution as follows:

% \vspace{-0.2cm}
% \begin{equation}
% \begin{aligned}
%     \text{\small $\mathcal{I}^{(q,a)}_{i}(h) = A^{(q,a)}_{i}(h) - \alpha \times \frac{1}{N} \times \hspace{-0.3cm} \sum_{(q',a) \in \mathcal{C}^{'}}{\hspace{-0.25cm}\Tilde{A}^{(q',a)}_{i}(h)},$} \\
% \end{aligned}
% \label{eq:attr_regular}
% \end{equation}
% \vspace{-0.2cm}


\vspace{-0.2cm}
\begin{equation}
\begin{aligned}
    \text{\small $S^{(q,a)}_{i}(h) = \hspace{-0.2cm} \sum_{(q',a) \in \mathcal{C}^{'}}{\hspace{-0.25cm}\Tilde{A}^{(q',a)}_{i}(h)}$,}\:\:\:\:\:\:\:\:\:\:\:\\
    \text{\small $\mathcal{I}^{(q,a)}_{i}(h) = A^{(q,a)}_{i}(h) - \alpha \times \frac{1}{N} \times S^{(q,a)}_{i}(h)$,} \\
\end{aligned}
\label{eq:attr_regular}
\end{equation}
\vspace{-0.2cm}


\noindent where $\mathcal{C}^{'}$ is a set including mismatched question and answer pairs. $N$ is the number of mismatched samples, and $\alpha$ is a hyper-parameter to determine the magnitude of knowledge exclusion. $\Tilde{A}$ means a negative value of $A$ is converted to the zero value.
Since the negative values of the attribution score are negative contributions to a specific knowledge, it is reasonable to eliminate that unnecessary information.
We use $\mathcal{C}^{f}$ and $\mathcal{C}^{r}$ as a pool to sample mismatched questions.
Notice that this regularization enhances the quantification of contextual knowledge; thus, it can improve multi-hop reasoning and mitigate shortcut unlearning.


% \subsection{Knowledge-localized Unlearning} % data selection과 parameter unlearning 설명
% This section describes the process of knowledge-localized unlearning.
% We first select only unforgotten samples from the forget set $\mathcal{C}_{f}$ and compute loss for the selected samples to mitigate superficial unlearning.
% Then, \ourmodel~determines knowledge neurons to unlearn and finally updates only the gradients of the selected knowledge neurons to achieve faithful unlearning.


% \noindent\textbf{Unforgotten Samples Selection.} 
\subsection{Unforgotten Sample-localized Unlearning}
If we repeatedly unlearn samples that have already been sufficiently unlearned, it leads to overfitting in language models.
Therefore, we select only the samples that are not completely forgotten in the unlearning process to preserve the generalization performance.
Specifically, in each epoch’s unlearning process, we select and unlearn only questions that satisfy the knowledge memorization criteria (Described in Section~\ref{problem_def}).




% \noindent\textbf{Knowledge Neuron Localization.}
\subsection{Knowledge Neuron-localized Unlearning}
After selecting unforgotten samples, we localize and update only the knowledge neurons corresponding to those selected samples in the language model.
Specifically, we first compute gradients of parameters for the selected unforgotten samples.
Then, we quantify the knowledge relevance of each neuron by using the equations \ref{eq:attr_lm} and \ref{eq:attr_regular}, and sort neurons of the whole target layers by the knowledge relevance scores; then, we select the top-$n$ knowledge neurons.
We finally mask gradients of the parameters for knowledge-irrelevant neurons to exclude them from the unlearning process.
Suppose that a weight matrix $\mathbf{W} \in \mathbb{R}^{d \times k}$ is a linear matrix multiplication parameter of a language model, and the gradient computed for the parameter is $\nabla_{\mathbf{W}} \mathcal{L} = \partial \mathcal{L} / \partial \mathbf{W}$.
Then, the gradient of $i$-th neuron (i.e., column) of the weight matrix after masking is denoted as $\nabla_{\mathbf{W}_{:,i}} \tilde{\mathcal{L}}=$ $\gamma$ $\odot$ $\nabla_{\mathbf{W}_{:,i}} \mathcal{L}$, where $\gamma \in \{\mathbf{0}_{d}, \mathbf{1}_{d}\}$ and $\odot$ means the Hadamard product.
We also can mask bias terms similar to the weight matrix.
Notice that this method is model-agnostic since all neural networks consist of linear transformation layers.



