% \subsection{Machine Unlearning for Language Models}
% patil2023can, huang2024transferable

Machine unlearning has been used as a solution to address privacy and copyright issues in the text generation process of language models.
% Notable examples include the gradient ascent \citep{jang2022knowledge, yao2023large, barbulescu2024each}, preference optimization approaches \citep{rafailov2024direct, zhang2024negative, jin2024rwku}, and representation learning \citep{li2024wmdp, yao2024machine} to unlearn the target sample.
Notable examples include gradient ascent-based methods \citep{jang2022knowledge, yao2023large, barbulescu2024each}, preference optimization approaches \citep{rafailov2024direct, zhang2024negative, jin2024rwku}, and representation learning techniques \citep{li2024wmdp, yao2024machine}.

However, the effectiveness of these methods has not been clearly demonstrated, prompting prior studies to introduce benchmarks in the field of unlearning to assess them.
\citet{eldan2023s, shi2024muse, tian2024forget} have aimed to unlearn the knowledge of copyrighted texts (e.g., BBC News and Harry Potter book) in a language model.
\citet{li2024wmdp} have introduced a benchmark dealing with hazardous knowledge in various professional domains (e.g., biosecurity and cybersecurity).
\citet{maini2024tofu, jin2024rwku} have proposed benchmarks for unlearning various entities. Specifically, \citet{maini2024tofu} have created synthetic entity profiles and removed their knowledge from a language model.
\citet{jin2024rwku} have tried to unlearn the knowledge about real-world entities and evaluated the knowledge memorization in various forms of assessment (e.g., cloze test and question answering).
However, existing studies remain limited as they have only examined independent knowledge and overlooked the intricate nature of world knowledge.
World knowledge is highly complex and interconnected, which means that unlearning the target knowledge requires examining related knowledge carefully.
Our research focuses on this aspect, examining and facilitating faithful unlearning.





