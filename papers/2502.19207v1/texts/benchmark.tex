\subsection{Problem Definition}
\label{problem_def}
The \ourdata~task evaluates unlearning algorithms under real-world knowledge QA settings.
Formally, given a language model $P_{\theta}(y|x) = \prod_{t=1}^{T} P_{\theta}(y_{t}|x,y_{1},...,y_{t-1})$ with parameters $\theta$, an unlearning algorithm $f$ updates $\theta$ to $\theta'$, erasing the target knowledge from $P_{\theta}$.
\ourdata~includes various question-answer pairs $(q, a) \in \mathcal{C}$, where $\mathcal{C}$ is a question-answer pair set.
Our task provides forget set $\mathcal{C}_{f}$, which contains target question-answer pairs to be forgotten, where $\mathcal{C}_{f} \subset \mathcal{C}$.
\ourdata~also provides retain set $\mathcal{C}_{r}\subset \mathcal{C} \backslash \mathcal{C}_{f}$ and test set $\mathcal{C}_{t}\subset \mathcal{C} \backslash (\mathcal{C}_{f} \cup \mathcal{C}_{r})$.
$\mathcal{C}_{r}$ is used in the unlearning process as training samples to maintain the original knowledge of $P_{\theta}$, and $\mathcal{C}_{t}$ is used as unseen data to evaluate an unlearned model $P_{\theta'}$ to reveal whether the unlearned model maintains the original knowledge.
Furthermore, \ourdata~provides other new types of test sets (i.e., paraphrased, multi-hop, and same-answer sets) to assess the faithfulness of unlearning methods. Before introducing the other datasets, we first define key aspects of our benchmark.



\input{Fig_texts/table_dataset_comparison}



\paragraph{World Knowledge Graph.}
A world knowledge graph $\mathcal{K}$ is a directed multi-graph where nodes are entities and edges are labeled with relations, i.e., elements of two sets $\mathcal{E}$ and $\mathcal{R}$, respectively.
We define $\mathcal{K}$ as a collection of triples $(s,r,o) \subseteq \mathcal{E} \times \mathcal{R} \times \mathcal{E}$, where $s$, $r$, $o$ denote the subject, relation, and object, respectively \citep{ruffinelli2020you, loconte2024turn}.
We assume that a world knowledge question is mapped to triples of $\mathcal{K}$; thus, we also define a \textbf{\textit{knowledge mapping}} function, $\tau: \mathcal{Q} \rightarrow \mathcal{P}(\mathcal{K})$, where $\mathcal{Q}$ is a set of questions and $\mathcal{P}(\mathcal{K})$ represents the power set of $\mathcal{K}$.
For example, the knowledge of a multi-hop question, $q_{i}$ = "Which continent is Tom Cruise’s country in?", can be denoted as a set of triples like $\kappa_{i} = \tau(q_{i})$ = \{("Tom Cruise", "country", "U.S.A"), ("U.S.A", "continent", "North America")\}.




To quantify memorization after unlearning, we define knowledge memorization of a language model following the general QA task, as follows:

\paragraph{Knowledge Memorization.}
\label{def:knoweldge_mem}
Let $P_{\theta}$ be a language model, and let $a$ be the correct answer to the question $q$. Then, knowledge memorization $\mathcal{M}_{\theta}: \mathcal{Q} \times \mathcal{A} \rightarrow \{0, 1\}$ is defined as
% and $a' \in {\mathcal{A}'}$ is a false answer, where $\mathcal{A}'$ is the false answer set

% \begin{equation}
% \begin{aligned}
%     \mathcal{M}(q,a) = \begin{cases}\:\ 1 & \text{if} \:\:\ P_{\theta}(a|\iota, q) > P_{\theta}(a'|\iota, q) \:\:\:\:\ \textbf{for} \:\:\ \forall a' \in \mathcal{A}'\\
%     \:\ 0 & \text{otherwise}
%     \end{cases}
% \end{aligned}
% \label{def:memory}
% \end{equation}


\begin{equation}
\begin{aligned}
    \text{\small $\mathcal{M}_{\theta}(q,a) =$} \begin{cases}\:\ \text{\small $1$} & \text{if} \:\ \text{\small $\argmax_{a'\in \mathcal{A}} P_{\theta}(a'|\iota, q) = a$}\:\:\:\:\ \\
    \:\ \text{\small $0$} & \text{\small otherwise}
    \end{cases}
\end{aligned}
\label{def:memory}
\end{equation}


\noindent where $\iota$ is an input prompt template for the language model $P_{\theta}$, and $\mathcal{Q}$ and $\mathcal{A}$ are question and answer sets, respectively.
From the definition, $\mathcal{M}_{\theta}(q,a)=1$ indicates that the language model retains the knowledge of $(q,a)$, while $\mathcal{M}_{\theta}(q,a)=0$ signifies that it does not.

Furthermore, we define \textit{Superficial Unlearning} using \textit{Knowledge Memorization} as follows:





\paragraph{Superficial Unlearning.}
\label{def:sup_unlearn}
Let $g: \Theta \rightarrow \Theta$ be an unlearning algorithm, and $\tau$ represent the \textit{knowledge mapping}. Assume there is a forget set $\mathcal{C}_{f}$, where $\mathcal{M}_{\theta}(q,a) = 1$ holds for all $(q,a) \in \mathcal{C}_{f}$, and that $(q_{j}, a_{j}) \notin \mathcal{C}_{f}$ with $\mathcal{M}_{\theta}(q_{j},a_{j}) = 1$.
Furthermore, suppose we unlearn the knowledge of $\mathcal{C}_{f}$ using $g$ from a language model $P_{\theta}$, and finally get an unlearned model $P_{\theta'}$.
Then, $g$ is called a superficial unlearning algorithm for $\mathcal{C}_{f}$ if 


% \[ \scalebox{2}{$\displaystyle 2 + 2 = 4$} \]
% \text{\normalsize $
\vspace{-0.15cm}
\begin{equation}
\begin{aligned}
    ((\kappa_{f} \cap \kappa_{j} \neq \text{\scriptsize Ø}) \:\wedge\: \mathcal{M}_{\theta'}(q_{j},a_{j}) = 1)\:\:\:\:\:\:\:\:\:\:\:\:\\\:\:\:\:\:\:\:\:\ \vee ((\kappa_{f} \cap \kappa_{j} = \text{\scriptsize Ø}) \wedge \mathcal{M}_{\theta'}(q_{j},a_{j}) = 0),
\end{aligned}
\label{def:sup_unlearn}
\end{equation}

\noindent where $\kappa_{f} = \bigcup_{(q,a) \in \mathcal{C}_{f}}{\hspace{-0.05cm}\tau(q)}$ and $\kappa_{j} = \tau(q_{j})$.



% \yny{Suppose that $\mathcal{K}_{f} = \bigcup_{q \in \mathcal{C}_{f}}{\hspace{-0.05cm}\tau(q)}$ and $\mathcal{K}_{j} = \tau(q_{j})$, and $\mathcal{K}_{f} \cap \mathcal{K}_{j} \neq \o$, then $g$ is called a \textbf{superficial unlearning} algorithm for $q_{j}$ if $\mathcal{M}_{\theta'}(q_{j},a_{j}) = 0$.
% In addition, suppose that $\mathcal{K}_{k} = \tau(q_{k})$ and $\mathcal{K}_{f} \cap \mathcal{K}_{k} = \o$, then $g$ is also called a superficial unlearning algorithm for $q_{k}$ if $\mathcal{M}_{\theta'}(q_{k},a_{k}) = 0$.}


% \begin{equation}
% \begin{aligned}
%     \mathcal{M}_{\theta'}(q_{i},a_{i}) = 0 \implies \mathcal{M}_{\theta'}(q_{k},a_{k}) = 0.
%     %\:\:\:\ \text{\bf where} \:\ \mathcal{M}_{\theta}(q_{i},a_{i}) = 1 \:\ \text{and} \:\ \mathcal{M}_{\theta}(q_{k},a_{k}) = 1 \\
% \end{aligned}
% \label{def:sup_unlearn2}
% \end{equation}


% If $\kappa_{j}$ does not include any knowledge of $\kappa_{f}$, an unlearning algorithm should not unlearn $q_{j}$ although any $q \in \mathcal{C}_{f}$ and $q_{k}$ have the same answer and $q_{i}$ is successfully unlearned.
% For example, suppose an unlearning algorithm $f$ unlearns the question $q_{i}$ = "What is the country of citizenship of Tom Cruise?", but as a result, also unlearns $q_{k}$ = "What country is Andy Warhol a citizen of?", which has a different context but may seem superficially interconnected since they share the answer.
% Then, the knowledge of two questions can be denoted as a set of knowledge triples like $\kappa_{i}$ = \{("Tom Cruise", "country of citizenship", "United States of America")\} and $\kappa_{k}$ = \{("Andy Warhol", "country of citizenship", "United States of America")\}. In this case, $g$ is called a superficial unlearning algorithm since $\kappa_{i} \cap \kappa_{k} = \o$ is true and the equation~\ref{def:sup_unlearn} is satisfied.
% In this paper, we focus on mitigating the phenomenon of superficial unlearning to achieve faithful unlearning.


For example, suppose that an unlearning algorithm $g$ unlearns the knowledge of the question $q_{i}$ = "Which country is Tom Cruise from?", but it does not unlearn the multi-hop question $q_{j}$ = "Which continent is Tom Cruise’s country in?".
Then, the knowledge of two questions can be denoted as a set of knowledge triples like $\kappa_{i} = \tau(q_{i}) = $ \{("Tom Cruise", "country", "U.S.A")\} and $\kappa_{j} = \tau(q_{j}) = $ \{("Tom Cruise", "country", "U.S.A"), ("U.S.A", "continent", "North America")\}. In this case, $g$ is called a superficial unlearning algorithm since $\kappa_{i} \cap \kappa_{j} \neq \text{\scriptsize Ø}$ and $ \mathcal{M}_{\theta'}(q_{j},a_{j}) = 1$ is true; thus, the equation~\ref{def:sup_unlearn} is satisfied.




\paragraph{Faithful Unlearning Benchmark.}
Based on the definition of \textit{superficial unlearning}, we construct three new types of datasets: paraphrased, multi-hop, and same-answer sets to investigate the phenomenon of superficial unlearning.
% Based on the definition of \textit{superficial unlearning}, we examine three types of unlearning challenges: generalization, shortcut unlearning, and multi-hop unlearning to investigate the phenomenon of superficial unlearning.
% Generalization \citep{anil2022exploring, yang2024unveiling, albalak2024improving}, shortcut learning \citep{du2023shortcut, tang2023large, zhou2023explore}, and the understanding of multi-hop knowledge \citep{zhong2023mquake, li2024making, yang2024large} are crucial challenges to consider in machine learning research.
% Since an unlearning process commonly uses fewer data instances than a general training process, these problems can be even more challenging.
% Therefore, we construct three new dataset types—paraphrased, multi-hop, and same-answer datasets—to address generalization, multi-hop knowledge unlearning, and shortcut unlearning, respectively, and to investigate superficial unlearning.
The paraphrased set $\mathcal{C}_{p}^{i}$, multi-hop set $\mathcal{C}_{m}^{i}$, and same-answer set $\mathcal{C}_{s}^{i}$ is matched with each question-answer pair $(q_{i}, a_{i}) \in \mathcal{C}$.
The paraphrased set includes the same context questions with varying textual forms to the matched target question; thus, we should unlearn $\mathcal{C}_{p}^{i}$ if a matched question-answer pair $(q_{i}, a_{i})$ is included in the forget set $\mathcal{C}_{f}$.
The multi-hop set includes multi-hop question-answer pairs interconnected with the target question. Therefore, we should also unlearn $\mathcal{C}_{m}^{i}$ if a mapped pair $(q_{i}, a_{i})$ is included in $\mathcal{C}_{f}$.
The same-answer set includes question-answer pairs where the questions are from different contexts but share the same answer as $a_{i}$; thus, we should maintain the knowledge of the same-answer set, although a matched pair $(q_{i}, a_{i})$ is included in $\mathcal{C}_{f}$.






\subsection{Data Collection and Construction}
% We construct the dataset, \ourdata, which includes various question-answer pairs $(q_{i}, a_{i}) \in \mathcal{C}$ and mapped other question-answer pairs for the $(q_{i}, a_{i})$ to investigate superficial unlearning.
% Our benchmark contains four types of datasets: (1) Base QA, (2) Paraphrased QA, (3) Multi-hop QA, and (4) Same-answer QA.
% The Base QA includes QA pairs to construct the forget set $\mathcal{C}_{f}$, retain set $\mathcal{C}_{r}$, test set $\mathcal{C}_{t}$.
% The other QA datasets are used to investigate superficial unlearning; thus, those datasets are matched with the Base QA dataset to evaluate whether the interconnected knowledge is well unlearned and other irrelevant knowledge is maintained after unlearning the QA pairs of $\mathcal{C}_{f}$.
% Our dataset construction process follows \citep{zhong2023mquake}, which generates questions using retrieved knowledge triples.


\noindent\textbf{Data Source.} We construct \ourdata~using Wikidata \citep{vrandevcic2014wikidata}, a knowledge base including knowledge triples $(s,r,o)$ matched with millions of entities.
We first select 200 of the most famous people as the entity set $\mathcal{E}$ from \textit{The Most Famous People Rank} \footnote{\url{https://today.yougov.com}}, and manually select 19 common relations as the relation set $\mathcal{R}$. The selected relations are shown in Appendix~\ref{apx:relations}. 



\noindent\textbf{The Base QA dataset.}
We retrieve all the triples $(s,r,o)$ from Wikidata, where $s \in \mathcal{E}$ and $r \in \mathcal{R}$.
Based on these triples, we use GPT-4o mini\footnote{\url{https://openai.com/}} to generate natural language form questions using a prompt template shown in Figure~\ref{qgen_templates}.
We use an object (i.e., $o$) of each triple as the answer for each generated question.
The constructed Base QA dataset $\mathcal{C}$ is split into three types of datasets: forget set $\mathcal{C}_{f}$, retain set $\mathcal{C}_{r}$, and test set $\mathcal{C}_{t}$.

\noindent\textbf{Evaluation of Unlearning Generalization.}
We also generate the Paraphrased QA dataset $\mathcal{C}_{p}$ to evaluate the generalization of an unlearning method.
Each question-answer pair $(q,a) \in \mathcal{C}$ is matched with three paraphrased questions.
The Paraphrased QA dataset is generated during the Base QA dataset construction process by making GPT-4o mini generate four different questions for each triple.
We use the first question as a sample of the Base QA dataset and the others for the Paraphrased QA dataset.
We have strictly checked whether there are the same texts in the generated four texts by examining the lexical overlap between texts.


\noindent\textbf{Evaluation of Multi-hop Knowledge Unlearning.}
We construct the Multi-hop QA dataset $\mathcal{C}_{m}$ to investigate superficial unlearning.
Each question-answer pair $(q,a) \in \mathcal{C}$ is matched with multi-hop questions.
After constructing the triples of the Base QA dataset, we additionally retrieve a set of chain-of-triples $((s_{1}, r_{1}, o_{1}), (s_{2}, r_{2}, o_{2}))$ from Wikidata, where $s_{1} \in \mathcal{E}$ and $r_{1}, r_{2} \in \mathcal{R}$ and $o_{1} = s_{2}$.
For each chain-of-triples, we also generate natural language questions using GPT-4o mini with the prompt template shown in Figure~\ref{qgen_templates2}.
We strictly validate that $o_{1}$ and $o_{2}$ are not included in the questions using instructions.


\noindent\textbf{Evaluation of Shortcut Unlearning.}
We further build the Same-answer QA dataset $\mathcal{C}_{s}$.
Each question-answer pair $(q,a) \in \mathcal{C}$ is also matched with the same-answer but different-context questions.
After constructing the triples of the Base QA dataset, we also retrieve other triples $(s',r',o)$ that share the same object (i.e., $o$) with each triple from the Base QA dataset, where $s' \notin \mathcal{E}$.
We also generate natural language form questions using GPT-4o mini with the same prompt template used in constructing the Base QA dataset.







\subsection{Dataset Summary}
\noindent\textbf{Dataset Format.}
% \input{Fig_texts/table_example}
Each instance of the dataset is denoted as a tuple: $d = \langle \mathcal{C}^{i}, \mathcal{C}_{p}^{i}, \mathcal{C}_{m}^{i}, \mathcal{C}_{s}^{i} \rangle$.
The \ourdata~dataset starts from a core factual triple \( (s, r, o) \), which forms the knowledge of the Base QA dataset \( \mathcal{C}^{i} \). There are also the Paraphrased QA dataset \( \mathcal{C}_{p}^{i} \), based on the same triple, the Multi-hop QA dataset \( \mathcal{C}_{m}^{i} \), which extends from the original triple \( (s, r, o) \), and the Same-answer QA dataset \( \mathcal{C}_{s}^{i} \), which shares the same answers as the Base QA dataset's questions but has different contexts. Each of these datasets ($\mathcal{C}^{i}$, $\mathcal{C}_{p}^{i}$, $\mathcal{C}_{m}^{i}$, and $\mathcal{C}_{s}^{i}$) is composed of question-answer pairs \( (q, a) \), and they also include false answer options to enable evaluation through Multiple-choice QA (MCQA). The details for the MCQA setting are described in Section \ref{bench:eval_framework}.
We also describe detailed examples in Table~\ref{tab:more_examples}.
In addition, we summarize the differences our benchmark addresses compared to existing benchmarks \citep{shi2024muse, tian2024forget, li2024wmdp, maini2024tofu, jin2024rwku} in Table~\ref{tab:dataset_comparison}.
% An example of an instance is shown in Table~\ref{tab:intance_example}, and more detailed examples are described in Table~\ref{tab:more_examples}.



\noindent\textbf{Dataset Statistics.}
After collecting triples of the Base QA dataset, we filter only triples including matched Multi-hop QA or Same-answer QA samples.
Therefore, each QA instance in the Base QA dataset serves as a cluster for evaluating the faithfulness of unlearning methods.
Consequently, we collect 664 QA pairs for the Base QA dataset.
Each Base QA instance includes three paraphrased questions, for a total of 1,992 paraphrased QA instances in our dataset.
\ourdata~also include 1,714 instances for multi-hop QA datasets. %  2,068 instances for 3-hop QA
Furthermore, our dataset includes 4,671 instances for the Same-answer QA dataset.
The statistics of the constructed \ourdata~datasets are shown in Table~\ref{tab:stats}.





\noindent\textbf{Dataset Quality.}
We adopt a ChatGPT variant to generate natural language questions, a commonly used and powerful approach, following existing studies \citep{shi2024muse, jin2024rwku, maini2024tofu}.
However, to further investigate the quality of the dataset, we conducted a human evaluation for the generated questions.
Specifically, we recruited crowd workers fluent in English through the university’s online community and had them evaluate 800 generated natural language questions. The results revealed an error rate of 0\%, confirming the reliability of our benchmark.






\subsection{Evaluation Framework}
\label{bench:eval_framework}
To evaluate the faithfulness of unlearning methods, we first split the forget set $\mathcal{C}_{f}$, the retaining set $\mathcal{C}_{r}$, and the test set $\mathcal{C}_{t}$ from the entire Base QA dataset $\mathcal{C}$.
Then, we train a language model to unlearn the forget set while maintaining knowledge of the retaining set.
We further evaluate the unlearned model to the test set to assess knowledge retention for unseen data.
In addition, we evaluate the unlearned model with the other constructed datasets (i.e., $\mathcal{C}_{p}$, $\mathcal{C}_{m}$, and $\mathcal{C}_{s}$) mapped to the forget and test sets to analyze the aspect of superficial unlearning.


Our unlearning framework consists of two types of input formats: (1) general QA format, and (2) multiple-choice QA (MCQA) format. We use the general QA format for unlearning and the MCQA format for evaluation.
The general QA format inputs a question without an additional template, while the MCQA format uses a template that includes instructions and answer options.
Suppose we aim to unlearn the knowledge of the question \textit{"Who is the mother of Barack Obama?"}, then we train a language model not to output the correct answer (i.e., \textit{"Stanley Ann Dunham"}) using only the question as an input.
However, many users use a language model with various instruction templates, and an unlearned model should be evaluated in a stricter environment considering generalization.
Furthermore, evaluating all possible answers to a question is one of the most challenging aspects of QA evaluation.
Therefore, we utilize the MCQA form to evaluate an unlearned model.
This makes it easier for LLMs to derive knowledge since they are given answer options; thus, it makes unlearning algorithms harder to apply.
For this reason, we use the MCQA setting to evaluate unlearned models in more challenging and practical settings.




\subsection{Evaluation Metrics}
\label{bench:eval_metrics}
We propose various metrics to evaluate the basic unlearning performance and the superficial unlearning performance. We use \textit{exact match} to calculate the score of all metrics.
\textbf{(1) Unlearning Accuracy (UA):} We compute accuracy for the forget set $\mathcal{C}_{f}$ to evaluate the basic unlearning performance.
\textbf{(2) Extended Unlearning Accuracy (UA$^{\ddag}$):} We compute accuracy for the Paraphrased QA set $\mathcal{C}_{p}$ to evaluate the generalized unlearning performance.
\textbf{(3) Test Accuracy (TA):} We compute accuracy for the test set $\mathcal{C}_{t}$ to evaluate whether knowledge of unseen instances is maintained after the unlearning process.
\textbf{(4) Same-answer Test Accuracy (SA):} We compute accuracy for the Same-answer QA set $\mathcal{C}_{s}$ to analyze shortcut unlearning. An unlearning algorithm may only superficially degrade the probability of the answer regardless of context.
\textbf{(5) Multi-hop Test Accuracy (MA):} We compute accuracy for $\mathcal{C}_{m}$ matched with each instance of $\mathcal{C}_{f}$ and $\mathcal{C}_{t}$ to evaluate whether the interconnected knowledge of instances is effectively unlearned.
To derive the aggregated MA score, we first compute the individual accuracies, MA$_{f}$ for all $(q,a) \in \mathcal{C}_{m}$ mapped to $\mathcal{C}_{f}$ and MA$_{t}$ for all $(q,a) \in \mathcal{C}_{m}$ mapped to $\mathcal{C}_{t}$; then, we compute the aggregated score, MA, by averaging the scores, ($100-$MA$_{f}$) and MA$_{t}$.
Although the number of samples in $\mathcal{C}_{t}$ is generally higher than in $\mathcal{C}_{f}$, we average the scores with equal weight, as we assume that unlearning samples in $\mathcal{C}_{f}$ is important due to significant privacy concerns.
\textbf{(6) Total Score (Score):} We aggregate all the evaluation scores by averaging ($100-$UA$^{\ddag}$), TA, SA, and MA, to present the overall performance.



% \textbf{(1) Unlearning Accuracy (UA):} We compute accuracy for the forget set $\mathcal{C}_{f}$ to evaluate the basic unlearning performance.
% \textbf{(2) Retaining Accuracy (RA):} We compute accuracy for the retaining set $\mathcal{C}_{r}$ to evaluate the knowledge retaining performance.
% \textbf{(3) Test Accuracy (TA):} We compute accuracy for the test set $\mathcal{C}_{t}$ to evaluate whether unseen instances in the unlearning process are well maintained.
% \textbf{(4) Unlearning Test Accuracy (UA$^{\ddag}$):} We compute accuracy for the Paraphrased QA set $\mathcal{C}_{p}$ to evaluate the generalization performance.
% \textbf{(5) Multi-hop Unlearn Accuracy (MA$_{unlearn}$):} We compute accuracy for the Multi-hop QA set $\mathcal{C}_{m}$ matched with each instance of the $\mathcal{C}_{f}$ to evaluate the implicit and interconnected knowledge of instances in $\mathcal{C}_{m}$ is well unlearned.
% \textbf{(6) Multi-hop Test Accuracy (MA$_{test}$):} We compute accuracy for $\mathcal{C}_{m}$ matched with each instance of the $\mathcal{C}_{t}$ to evaluate the implicit and interconnected knowledge of instances in $\mathcal{C}_{m}$ is well maintained.
% \textbf{(7) Same-answer Test Accuracy (SA):} We compute Accuracy for the Same-answer QA set $\mathcal{C}_{s}$ to evaluate the preservation of irrelevant knowledge. An unlearning algorithm may only superficially degrade the probability of the answer regardless of context.

