Large language models (LLMs) are trained on a vast corpus of text, enabling them to achieve outstanding performance across various tasks \citep{radford2019language, chowdhery2023palm, team2024gemma}.
However, LLMs may present privacy risks, as sensitive or private information could unintentionally be included in the large text corpus used for training.
Therefore, prior studies have investigated unlearning undesirable knowledge in language models.
To assess unlearning, most studies have examined whether a model successfully forgets the targeted knowledge while retaining irrelevant knowledge \citep{shi2024muse, li2024wmdp, maini2024tofu, jin2024rwku}. 


\input{Fig_texts/fig_main}






However, they are limited since they have overlooked the complex and interconnected nature of knowledge, which requires careful investigation of related knowledge.
Specifically, these studies have examined only the independent knowledge and failed to evaluate whether an unlearning method effectively erases interconnected knowledge that should be removed, while retaining knowledge that appears relevant but exists in a completely different context.
Figure~\ref{fig:main} presents an example of faithful unlearning in the real-world knowledge setting.
Unlearning methods should also remove paraphrased and multi-hop questions, as they involve knowledge interconnected with the target question being unlearned.
Conversely, unlearning methods should retain knowledge of other questions with the same answer as the target, if they actually contain different knowledge despite appearing relevant.



% In this case, an unlearning method aims to unlearn the knowledge related to the target question, \textit{``What is the country of citizenship of Tom Cruise?"} from a language model. To ensure successful unlearning, the language model should forget the knowledge for answering the paraphrased question, \textit{``Which country is Tom Cruise a citizen of?"}, and the multi-hop question, \textit{``What is the continent of the country where Tom Cruise holds citizenship?"} since they share interconnected knowledge with the target question.
% However, another question, \textit{``What country is Andy Warhol a citizen of?"} should still be answered correctly after the unlearning process, despite sharing its answer with the target question and seemingly involving interconnected knowledge.







To address this gap, we first define \textbf{\textit{superficial unlearning}}, which refers to the phenomenon where an unlearning method either fails to erase the interconnected knowledge it should remove or unintentionally erases irrelevant knowledge.
Based on the definition, we introduce \textbf{\ourdata}~(A \textbf{Faith}ful \textbf{Un}learning Evaluation Benchmark for Real-world Knowledge Question Answering), a new benchmark to examine three types of unlearning challenges: generalization, multi-hop unlearning, and shortcut unlearning to investigate superficial unlearning.
Generalization \citep{anil2022exploring, yang2024unveiling, albalak2024improving}, the multi-hop reasoning \citep{zhong2023mquake, li2024making, yang2024large}, and shortcut learning \citep{du2023shortcut, tang2023large, zhou2023explore} are crucial challenges in machine learning research.
Since the unlearning process typically relies on fewer data instances than general training, these challenges can be further amplified.
Therefore, we construct three types of new datasets—paraphrased, multi-hop, and same-answer datasets—to examine superficial unlearning.
These datasets address generalization, multi-hop knowledge unlearning, and shortcut unlearning, respectively.
We demonstrate that existing unlearning methods do not ensure faithful unlearning, which raises new research questions for knowledge unlearning.


% Based on the definition, we introduce \textbf{\ourdata}~(\textbf{Faith}ful \textbf{Un}learning Evaluation Benchmark for Real-world Knowledge Question Answering), a new benchmark to evaluate the faithfulness of existing unlearning methods.
% \ourdata~consists of three types of datasets for evaluating faithful unlearning: Paraphrased QA, Multi-hop QA, and Same-answer QA datasets.
% Three datasets are used to evaluate whether unlearning methods faithfully unlearn the interconnected knowledge while retaining knowledge that appears superficially relevant but exists in a different context.
% We reveal that existing unlearning methods do not ensure faithful unlearning, raising new research questions in the field of knowledge unlearning.



Furthermore, we propose a novel method, \textbf{\ourmodel}, which stands for \textbf{K}nowledge-\textbf{L}ocalized \textbf{U}nl\textbf{E}arning, to achieve faithful unlearning by precisely identifying and updating neurons related to the target knowledge.
Specifically, we use attribution \citep{yang2023task}, an explainability method, to determine which neurons should be updated by quantifying how much information each neuron contributes to predicting the answer to a given question.
However, the quantified score may include superficial knowledge that simply affects the target output's probability without considering contextual meaning.
Therefore, we propose a robust knowledge regularization method that accurately quantifies each neuron's knowledge score, mitigating the superficial contribution of neurons.
After identifying knowledge neurons, our method selectively unlearns the target knowledge while preserving other knowledge by updating only knowledge-related neurons with selected unforgotten samples.
In our experiments, our method significantly outperforms the baselines in the \ourdata~setting, demonstrating that knowledge-localized unlearning effectively achieves faithful unlearning.

% In summary, this work makes the following contributions:
% \vspace{-0.2cm}
% \begin{itemize}[leftmargin=0.8cm]
% \item We first define superficial unlearning and construct a new benchmark, \ourdata, to evaluate various aspects of it in real-world knowledge QA settings.
% \vspace{-0.05cm}
% \item We reveal that existing unlearning methods do not ensure faithful unlearning, raising new research questions in the field of knowledge unlearning.
% \vspace{-0.05cm}
% \item To achieve faithful unlearning, we propose a novel unlearning method, \ourmodel, which accurately identifies and updates only knowledge-related neurons. We demonstrate that \ourmodel~significantly outperforms the widely-used baselines in the \ourdata~setting.
% \end{itemize}
% \vspace{-0.3cm}


