\section{\ourdata~Details}
\label{apx:sup_detail}



\subsection{Detailed Dataset Comparison}
\label{apx:dataset_comparison}
In this section, we propose detailed comparisons with existing datasets to show the novelty of our benchmark clearly. 
Our benchmark aims to unlearn the knowledge of famous real-world entities, which can be prevalent in various language models, to consider the most practical situation of knowledge unlearning. Furthermore, our benchmark deals with the complex and interconnected nature of world knowledge; thus, we introduce three types of unlearning evaluation aspects (Generalization, Multi-hop knowledge unlearning, and Shortcut unlearning) for more deep analysis of real-world knowledge unlearning. 

In summary, MUSE, KnowUnDo, and TOFU require fine-tuning to inject knowledge before unlearning, which may reduce their practicality.
Additionally, only RWKU and our benchmark address real-world entities as targets for unlearning. Furthermore, most existing benchmarks, except for RWKU and our benchmark, have not considered related knowledge.
However, RWKU has not explored the interconnections between knowledge and shortcut unlearning problems, which have become increasingly significant in unlearning due to its reliance on a limited number of training instances.
For example, RWKU includes a target text for unlearning: 'Please forget Stephen King, who is an American author, renowned as the "King of Horror"'. It also contains a related knowledge question: "'Who plays the character Jack Torrance in the film The Shining?'". While the two questions are somewhat related, they represent independent pieces of knowledge, as they are not interconnected like multi-hop questions.
In conclusion, the main contribution of our benchmark lies in evaluating whether unlearning methods perform faithful unlearning while considering knowledge interconnectedness within the real-world entity unlearning setting.





\subsection{Dataset Format}
Our \ourdata~benchmark includes four types of datasets: the Base QA dataset ($\mathcal{C}$), the Paraphrased QA dataset ($\mathcal{C}_{p}$), the Multi-hop QA dataset ($\mathcal{C}_{m}$), and the Same-answer QA dataset ($\mathcal{C}_{s}$).
Each instance in the Base QA dataset is matched with instances in other datasets (i.e., Paraphrased QA, Multi-hop QA, and Same-answer QA) to examine the impact of unlearning on these datasets.
Dataset statistics for the \ourdata~benchmark are shown in Table~\ref{tab:stats}.
Examples in the \ourdata~benchmark are shown in Table~\ref{tab:more_examples}.

\input{Fig_texts/table_stat}





\subsection{Details in Dataset Construction}



\subsubsection{Selected Entities and Relations.}
\label{apx:relations}
We select 200 famous human entities and 19 relations appropriate for constructing knowledge triples from Wikidata.
Specifically, we manually select \textit{mother}, \textit{country}, \textit{religion}, \textit{founded by}, \textit{highest point}, \textit{country of citizenship}, \textit{place of birth}, \textit{position played on team / speciality}, \textit{headquarters location}, \textit{country of origin}, \textit{native language}, \textit{field of work}, \textit{father}, \textit{occupation}, \textit{sport}, \textit{capital}, \textit{currency}, \textit{location}, \textit{continent} as relations, which are widely-used relations to describe knowledge of human entities or other entities related to human (e.g., United States of America).


\subsubsection{Dataset Analysis.}

\paragraph{The Number of Data Instances for Each Entity.}
We investigate the number of data instances (cluster) for each entity, as shown in Figure~\ref{num_entity_cluster}.
The X-axis of the figure corresponds to the entity index, which is sorted in descending order of popularity.
From this figure, we can confirm that our dataset maintains a balanced distribution of entities, regardless of popularity.
The average number of data instances of each entity is 3.32, and the standard deviation is 1.25.

\input{Fig_texts/fig_dataset_num_entity_cluster}


\paragraph{The Frequency of Each Relation.}
we plot the number of each relation on the Base QA, the Multi-hop QA, and the Same-answer QA datasets, as shown in Figure~\ref{num_relation}.
The Multi-hop QA dataset contains diverse relations, allowing for a broader evaluation of superficial unlearning.
In contrast, the Same-answer QA dataset has a distribution of relation similar to the Base QA dataset, making unlearning more challenging.
When evaluating shortcut unlearning on datasets with standardized relations, we can more effectively identify issues that lower the likelihood of predicting the given answer, regardless of context.


\input{Fig_texts/fig_dataset_num_relation}





\subsubsection{Question Generation Prompt Templates}
\label{apx:qgen_prompt_template}
We utilize GPT-4o mini to generate questions from constructed Wikidata triples, similar to \citep{zhong2023mquake, mallen2022not}.
An example of generating single-hop questions (the base QA, paraphrased QA, and same-answer QA datasets) is shown in Figure~\ref{qgen_templates}.
Multi-hop questions are generated similarly to single-hop questions, shown in Figure~\ref{qgen_templates2}.








\input{Fig_texts/question_generation_template}

\input{Fig_texts/question_generation_template2}








\section{Experimental Setup}
\label{apx:exp_setup}


\subsection{MCQA Prompt Templates}
\label{apx:mcqa_prompt_template}
The \ourdata~framework evaluates unlearned models by using an MCQA format.
The MCQA format consists of three parts: an instruction,  a question, and options. After sampling false options for each question, we randomly shuffle the options to mitigate position bias \citep{pezeshkpour2023large, zheng2023large}, consistently maintaining the determined order during all the experiments for fair experiments.
The utilized MCQA template is shown in Figure~\ref{mcqa_templates}.

\input{Fig_texts/mcqa_template}


\subsection{MCQA False Options Selection}
\label{apx:mcqa_false_opt}
To prevent the situation that the false options include a possible correct answer, we use GPT-4o \footnote{\url{https://openai.com/index/hello-gpt-4o/}} to cluster the entire answer options of each relation and we manually double-check the answer clusters are well constructed.
After constructing answer clusters, we sample two incorrect options from the answer set, excluding those in the same cluster as the correct answer.





\subsection{More Details for the Experiments}
\label{apx:exp_setups}
\paragraph{Training Setups.}
We train and evaluate \ourmodel~and other baselines on NVIDIA A100 GPU. 
For a fair comparison, we early stop the training procedure when UA$\leq 0.33$ is satisfied (random sampling from three answer options) to select the optimal model.
Since a language model forgets all the knowledge when a learning rate is set too high, we have searched for the lowest learning rates, which can reach UA$\leq 0.33$ within the range $\lambda \in$ [1e-07, 3e-03].
We adopt batch size $\beta = 4$ for all unlearning methods.
We compute the final loss by weighted-summing the loss of forget samples and retaining samples.
Specifically, we use $0.7$ and $1.0$ for the loss of forget samples and the retaining samples, respectively.
We select $e = 150$ as the maximum number of epochs in the training process.


\paragraph{Baselines.}
\textbf{(1) Gradient Ascent (GA):} Unlike the gradient descent used during the pre-training phase, GA \citep{jang2022knowledge, yao2023large} maximize the negative log-likelihood loss on the forget set. This method helps shift the model away from its original predictions, aiding in the unlearning process.
\textbf{(2) Gradient Ascent with a Retaining Loss (GA$_{ret}$):} GA tends to unlearn other unrelated knowledge since it just maximizes the negative log-likelihood loss on the forget set. Therefore, we add an auxiliary retention loss to maximize the log-likelihood of the retaining set, securing the retention of other irrelevant knowledge.
\textbf{(3) Direct Preference Optimization (DPO):} We adopt preference optimization to unlearn a language model to generate another answer. DPO \citep{rafailov2024direct, jin2024rwku} utilizes positive and negative instances to train the model.
Therefore, we select the correct answer as the negative instance and also define two types of DPO methods to determine positive ones: (1) DPO$_{mis}$ (DPO using a mismatched answer) and (2) DPO$_{neg}$ (DPO using a rejection answer).
DPO$_{mis}$ utilizes a randomly sampled answer as the positive instance.
On the other hand, DPO$_{rej}$ utilizes a rejection text \textit{``I can't answer the question."} as the positive instance.
Two DPO methods both aim to increase the probability of the positive instance compared to the negative one for the forget set, and they switch the positive and negative instances for training the retaining set.
We search for $\beta_{DPO} \in [0.1, 0.5]$ to optimize models.
\textbf{(4) NPO:} NPO is a modified version of DPO that exclusively retains negative examples without positive ones. NPO can also be explained as a straightforward modification of the GA loss. We implement NPO \citep{zhang2024negative} for extended experiments. We search for $\beta_{NPO} \in [0.1, 0.5]$ to optimize models.
\textbf{(5) RMU:} We implement RMU \citep{li2024wmdp}, the representation learning-based unlearning model.
For RMU experiments, we search for $\alpha_{RMU} \in \{20, 50, 100, 150, 200, 300\}$ and use hyper-parameters $c=20$ and $l=7$, following the implementation details on the original GitHub Page\footnote{\url{https://github.com/centerforaisafety/wmdp}}.
\textbf{(6) Knowledge-Localized Unlearning (\ourmodel)}:
We select only 5\% of neurons from Feed-forward networks for the knowledge neuron localization, and update them using general gradient ascent with retention loss.
We also use $\alpha = 10$ and $N = 5$ for the Superficial Knowledge Regularization term.
The experiments analyzing varying hyper-parameters are shown in Section~\ref{exp:ratio_neuron}, Appendix~\ref{apx:alpha}, and Appendix~\ref{apx:neuron_ratio}.






\subsection{\ourmodel~successfully Mitigates Superficial Unlearning on Various Language Models}
\label{apx:extend_exp}
We conduct experiments on Llama-3.2 (3B) and Gemma-2 (9B) to reveal that our method is model-agnostic and generalizable.
Table~\ref{table_extended} shows the experimental results for Llama-3.2 (3B) and Gemma-2 (9B), respectively.
In the Llama-3.2 (3B) results, all baselines are significantly exposed to superficial unlearning, while our method successfully outperforms other baselines in most metrics.
Likewise, in the Gemma-2 (9B) results, the enhancement of mitigating superficial unlearning is dramatic after applying knowledge localization.
These results demonstrate that our method can be applied to various language models and successfully mitigates superficial unlearning.



\input{Fig_texts/table_extended_experiments}



\subsection{\ourmodel~is Robust to Various Forget Sample Sizes} % unlearn set, retain set data num graph
\label{apx:data_size}
We conduct experiments on Gemma-2 (2B) for the varying sizes (i.e., 1\%, 5\%, and 10\%) of the forget set to analyze the effect of unlearning samples.
The experimental results are shown in Table~\ref{table_gemma} (5\%) and Table~\ref{exp:num_unlearn_table} (1\% and 10\%).
Our experiments reveal that existing methods undergo more problems in unlearning when the number of forget samples increases.
Increasing the number of samples to be forgotten is more challenging since it requires modifying a greater amount of knowledge.
However, our proposed method consistently outperforms other baselines; thus, the performance gap between our method and the baselines widens as the number of forget samples increases.

\input{Fig_texts/table_num_unlearn}



\subsection{Hyper-parameter Experiments}
\label{apx:hyperparams}
\subsubsection{Sequential vs. Batch Unlearning}
\label{apx:batch}
We conduct experiments on Gemma-2 (2B) to show the performance variation for varying numbers of samples unlearned in each batch.
We select 5\% of neurons to unlearn.
We adopt various batch size $\beta \in$ \{1, 4, 8, 16, 32\} for the experiments, shown in Figure~\ref{hyper_batch}.
The experimental results reveal that \ourmodel~is effective when using $\beta \in [4, 16]$.
Sequential unlearning restricts unlearning to specific knowledge for only a single data sample, which impacts overfitting in the unlearning process, resulting in good performance only on UA$^{\ddag}$.
In contrast, a large batch size makes it hard for a language model to unlearn the knowledge since it can not identify appropriate knowledge neurons from the attribution computed by large samples.


\input{Fig_texts/fig_hyper_batch}


\subsubsection{Hyper-parameter ($\alpha$) Experiments}
\label{apx:alpha}
We conduct hyper-parameter experiments on Gemma-2 (2B) for $\alpha \in$ \{0.5, 1.0, 10.0, 20.0\}, which is used to determine the magnitude of the superficial knowledge regularization, shown in Figure~\ref{hyper_alpha}.
The experimental results show that low values of $\alpha$ damage the retention of the original knowledge (TA, SA), although they show better performance for unlearning interconnected knowledge of the forget set (UA$^{\ddag}$).
On the other hand, higher values of $\alpha$ contribute to preserving the retention of the original knowledge.


\input{Fig_texts/fig_hyper_alpha}


\subsubsection{Neuron Ratio ($p$) Experiments}
\label{apx:neuron_ratio}
We conduct experiments on various neuron ratios to investigate the \ourmodel~method further for Gemma-2 (2B), as shown in Table~\ref{tab:larger}.
We reveal that even the larger ratios show comparable results, however, simply increasing the neuron ratio does not enhance the performance.
The results also demonstrate that it is more important to exclude irrelevant neurons than to include relevant neurons during training to mitigate superficial unlearning.

\input{Fig_texts/table_larger_neuron_ratio}




\subsubsection{The Various Prompt Templates Experiments}
\label{apx:prompt_templates}
We conduct experiments on various prompt templates to investigate the unlearning abilities of the \ourmodel~method further for Gemma-2 (2B), as shown in Table~\ref{tab:larger}.
Specifically, we newly select five templates: (1) \textit{"Pick the appropriate option for the question from the provided options. You should answer without further explanation."}, (2) \textit{"Select the correct answer for the given question from the options. Write only the word without explanation."}, (3) \textit{"Answer the given question by choosing the appropriate answer from the given options. Do not include any explanations."}, (4) \textit{"Select the correct answer to the following question among the options. Only the exact word should be written, with no explanation."}, and (5) \textit{"Select the proper answer to the question from among the given options. Write only the exact word without any additional explanation."}.
From the experiments, we reveal that the newly adopted prompts perform similarly to the original prompt. Their performance on the UA score is slightly higher than the original one since we early stopped the unlearning process based on the UA score evaluation for the original prompt.

\input{Fig_texts/table_different_prompt_templates}



\subsection{Ablation Studies}
\label{apx:ablation}
We perform ablation experiments on each \ourmodel~method using Gemma-2 2B to better understand their relative importance, as shown in Table~\ref{tab:ablation}.
\textit{Regularization} means the strategy of using the auxiliary regularization term for quantifying the knowledge relevance of each neuron, mitigating superficial unlearning. 
\textit{Localization} corresponds to the entire knowledge neuron localization strategy.
\textit{Sample Selection} is the strategy that selects unforgotten samples by evaluating the memorization of each sample.
For the ablation study, we remove each of them and measure the accuracy. 
As a result, we reveal that three methods significantly affect the faithfulness of unlearning.
\textit{Regularization} and \textit{Localization} are useful to enhance SA and MA, mitigating superficial unlearning related to interconnected knowledge.
These results demonstrate that selecting proper knowledge neurons to be updated is useful for handling interconnected knowledge.
In addition, we illuminate that \textit{Sample Selection} significantly increases TA and SA, mitigating overfitting and shortcut unlearning issues.

\input{Fig_texts/table_ablation}




\input{Fig_texts/table_more_examples}