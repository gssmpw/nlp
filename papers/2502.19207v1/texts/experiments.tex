\subsection{\ourdata~Setups}
\noindent\textbf{Models.} We adopt the instruction-tuned Gemma-2 \citep{team2024gemma} models (2B \& 9B) and the Llama-3.2 \citep{dubey2024llama} model (3B) to evaluate unlearning methods since they are among the latest open-source language models showing excellent performance.




\noindent\textbf{Data.} We sample 5\% as the forget set and 10\% as the retaining set from the Base QA dataset $\mathcal{C}$ since there are generally fewer samples to unlearn than to retain in real-world scenarios.
More experiments on varying numbers of samples for the forget set are shown in Appendix~\ref{apx:data_size}.
We select 70\% of $\mathcal{C}$ as the test set, guaranteeing it is completely separate from $\mathcal{C}_{f}$ and $\mathcal{C}_{r}$.
For the MCQA evaluation (Section \ref{bench:eval_framework}), we manually select the instruction and randomly sample two false answer options from possible answers for each relation $r$.
The details of an example of the MCQA format and selecting false answer options are shown in Appendix~\ref{apx:mcqa_prompt_template} and~\ref{apx:mcqa_false_opt}, respectively.
We also conduct experiments on various prompt templates, and the results are described in Appendix~\ref{apx:prompt_templates}.

\noindent\textbf{Training Setups.} When unlearning is applied to a language model, there is often a trade-off between unlearning knowledge (i.e., UA, UA$^{\ddag}$, and MA$_{f}$) and retaining the model's overall knowledge (i.e., TA, SA, and MA$_{t}$).
Therefore, choosing the optimal model in the unlearning process is challenging since unlearning and retention are both important.
For a fair comparison, we early stop the training procedure when UA$\leq 0.33$ is satisfied (random sampling from three options) to select the optimal model.
More detailed experimental settings can be found in Appendix~\ref{apx:exp_setups}.










\subsection{Baselines}
We adopt widely-used unlearning methods to evaluate the superficial unlearning: Gradient Ascent (GA), Gradient Ascent with a Retaining Loss (GA$_{ret}$), two Direct Preference Optimization variants (DPO$_{mis}$ and DPO$_{rej}$), NPO \citep{zhang2024negative}, and RMU \citep{li2024wmdp}.
More details for the baselines are described in Appendix~\ref{apx:exp_setups}.
For \ourmodel, we select only 5\% of neurons from Feed-forward networks for the knowledge neuron localization, and update them using general gradient ascent with retention loss.
We also use $\alpha = 10$ and $N = 5$ for the Superficial Knowledge Regularization term.
The experiments analyzing various hyper-parameters are shown in Section~\ref{exp:ratio_neuron} and Appendix~\ref{apx:hyperparams}.




\input{Fig_texts/table_main_experiments}

\input{Fig_texts/table_qualitative_analysis}




\subsection{\ourmodel~Mitigates Superficial Unlearning}
We investigate superficial unlearning on all baselines with Gemma-2 (2B) in the \ourdata~setting, as shown in Table~\ref{table_gemma}.
% Table~\ref{table_gemma} shows the accuracy of various methods on the evaluation metrics.
First, the default Gemma-2 model can correctly answer most questions, validating that \ourdata~is well constructed.
After the unlearning process, all baselines reach UA$\leq 0.33$, which validates that all methods can unlearn target knowledge.
However, they fail to reliably remove implicit and interconnected knowledge, suggesting that their unlearning process is superficial.
However, our method mitigates superficial unlearning and achieves faithful unlearning compared to other baselines, without significantly damaging the other knowledge to maintain (i.e., TA, SA, and MA).
These results demonstrate that our method accurately identifies neurons relevant to contextual knowledge and successfully erases this knowledge.
In addition, experiments on Gemma-2 (9B) and Llama-3.2 (3B) reveal that our method outperforms baselines, with results presented in Appendix~\ref{apx:extend_exp}.
We also conduct ablation studies for \ourmodel~and demonstrate the validity of our proposed methods, as detailed in Appendix~\ref{apx:ablation}.





\subsection{\ourmodel~is Robust to Unlearning Trade-off.}
We demonstrate how the unlearning process affects other knowledge by plotting all scores from the Gemma-2 (2B) unlearning process against UA.
As the UA score represents the progress of unlearning target knowledge (decreasing with unlearning), we can observe each method's impact on other knowledge in Figure~\ref{fig:tradeoff}.
All methods' impact on the paraphrased questions (UA$^\ddag$) shows a strong correlation with the UA score, suggesting that all methods pose robustness in dealing with different lexical forms (but hold the same meaning) of the questions. 
However, the baselines struggle to maintain other knowledge (TA and SA) and to forget interconnected knowledge (MA).
In contrast, \ourmodel~demonstrates robust unlearning performance by effectively forgetting interconnected knowledge and preserving other knowledge.


\input{Fig_texts/fig_tradeoff}








% \subsection{Batch vs. Sequential Unlearning}
% \yny{From the experiments, we reveal that existing methods undergo superficial unlearning more when the forget samples are expanded.
% Furthermore, our proposed method consistently outperforms other baselines, mitigating superficial unlearning phenomena. From the experiments, we reveal that existing methods undergo superficial unlearning more when the forget samples are expanded. Furthermore, our proposed method consistently outperforms other baselines, mitigating superficial unlearning phenomena.}



\subsection{The Impact of Neuron Localization}
\label{exp:ratio_neuron}
We adopt varying ratios of neuron selection $p \in \{0.01, 0.05, 0.1\}$ to investigate the effect of the knowledge neuron on Gemma-2 (2B).
Also, we conduct experiments for the random neuron selection (i.e., $p \in \{0.01, 0.05\}$).
As a result, we reveal that a neuron ratio of 0.05 or 0.1 contributes to achieving faithful unlearning, showing that random neuron selection more significantly triggers superficial unlearning.


\input{Fig_texts/fig_hyper_neuron}





\subsection{Qualitative Analysis}
\label{exp:case_study}
We conduct a qualitative analysis for \ourmodel~and GA$_{ret}$ on Gemma-2 (2B).
Both \ourmodel~and GA$_{ret}$ successfully unlearn the paraphrased question ($\mathcal{C}_{p}$), degrading label logits to 0.33.
However, GA$_{ret}$ has difficulty in unlearning the multi-hop question ($\mathcal{C}_{m}$), while mistakenly unlearns the same-answer questions ($\mathcal{C}_{s}$).
On the other hand, \ourmodel~faithfully unlearns them, mitigating superficial unlearning.








