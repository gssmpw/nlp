\section{Evaluation}
Throughout the development of \textsc{Finch}, we continuously tested the tool with various visualization and machine learning experts, leading to significant improvements. To further validate the final version, we conducted a user study with machine learning experts of varying expertise in xAI methods.
We specifically focused on the goals of understandability, usability and helpfulness.

\subsection{Study design}
Participants were recruited by contacting machine learning experts.
The five participants (2F, 3M) had an average age of 35, ranging from 32 to 45.
They work as professors, researchers, graduate and postgraduate students in the field or had significant experience from a previous job.
One participant also participated in the requirement analysis study. None are authors of this paper.
The study was conducted either in person or via online video calls. We began by collecting demographic information and learning about the participant's previous experiences. 
Next, we introduced our bike rental example dataset to create a simple tutorial scenario, providing a brief description of the prediction task followed by a SHAP graph to overview the current instance and its prediction. We chose SHAP as a widely known explanation method~\cite{holzinger2022explainable}. \textsc{Finch} complements SHAP by offering additional functionality for exploring feature relationships. We guided participants through the tool's features using a set of tasks, explaining how it works along the way.

Participants then worked independently with a dataset on diabetes risk factors derived from the BRFSS telephone study~\cite{BRFSS2015}. They were tasked with answering questions to assess their ability to use the tool independently.

Throughout the process, participants were encouraged to verbalize their thoughts using the think-aloud method. At the end of the session, they rated their experience through a series of questionnaires.
We used the widely recognized System Usability Scale (SUS)~\cite{Brooke1996SUS} to evaluate the tool's usability. To assess the provided explanations regarding their helpfulness, we employed the explanation satisfaction scale (ESC) by Hoffman et al.~\cite{hoffman2018metrics}. We also included specific questions to measure how well our tool met requirements and participants' satisfaction with the visualizations (Fig. \ref{fig:Custom1} and \ref{fig:Custom2}). 

\subsection{Study Results}
All participants reported high experience in machine learning. Their familiarity with explainable AI varied from moderate to very experienced. All were well-acquainted with SHAP and PDP, except for one participant being unfamiliar with PDP.

We split up our findings into the three design goals of understandability, usability and helpfulness.

\subsubsection{Understandability}

During the study, all participants quickly grasped the concept of subsets for one feature. One participant described it as \say{nice and intuitive}. Some participants took longer to understand how additional features were visualized when added to the subset, but with some help all understood it and were able to use and interpret it (meet R1). 

While all participants understood the uncertainty visualization, there was initial confusion due to the varying meanings of "uncertainty"; for instance, one participant thought it referred to the model's prediction uncertainty.
The most challenging aspect for participants was the interaction effect visualization. All struggled with it, though three eventually felt they understood it.

The results from our custom questions were largely positive (see Figure~\ref{fig:Custom1}). Participants agreed that the tool emphasizes unique data aspects (meet R3), distinguishes positive and negative contributions of features (meet R2), and provides an easily interpretable summary of features.
The only negative feedback concerned whether the tool helps validate displayed results. Discussions revealed varied interpretations of this question; while we aimed to assess trust calibration features, some participants interpreted it as evaluating model validation, explaining the mixed responses. 

\begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{Custom1.jpg}
    \caption{Answers to our custom questions about the tools functionality. For each question, colored bars display the number of people per answer, centered around the neutral answer with disagreeing answers to the left and agreeing answers to the right.}
    \label{fig:Custom1}
\end{figure}

\subsubsection{Usability}
When using the tool on the diabetes dataset, all participants frequently adjusted feature values to see how predictions changed. Three participants used the ground truth and uncertainty features to validate their results.

One participant suggested increasing contrast for clarity and using textures for colorblind users.
One recommended including a logical description of the current subset for better clarity and a more precise legend for easier recall.
Participants expressed a desire for units to be displayed alongside the numbers. One requested additional information, such as a global explanations, while another wished to move or delete individual features. One participant also wanted to read specific values directly from the chart.
One person raised concerns about scalability, as the tool occasionally ran slowly,
Additionally, one participant found the feature overview overwhelming.
The additional visualizations displaying data distribution, trust, and uncertainty were well received. One participant suggested adding a color legend for the data distribution heatmap. The ground truth visualization was considered very important by one participant, while another expressed interest in using the tool directly on the ground truth for insights before training a model.

Our tool received a score of 82 on the SUS, indicating excellent usability.
Overall, participants found the charts easy to interpret, visually pleasing, and helpful for identifying and interpreting feature interactions (Fig. ~\ref{fig:Custom2}).
Three experts provided freeform feedback. Two suggested improvements, including, in addition to what was discussed already, an info button to explain the visible interactions and functionality enabling users to \say{play} with the tool by clicking on data points to update values. Two freeform comments also included praise of the tool.

\begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{Custom2.jpg}
    \caption{Answers to our custom questions about the visual chart design. For each question, colored bars display the number of people per answer, centered around the neutral answer with disagreeing answers to the left and agreeing answers to the right.}
    \label{fig:Custom2}
\end{figure}

\subsubsection{Helpfulness}
The ESC gives an overview of how helpful our tool was seen (see Fig. ~\ref{fig:ESG}). Experts agreed that our explanation helps them understand the model, is satisfying, provides sufficient detail, and is useful for their goals. However, they were neutral regarding the completeness of the explanation and its guidance on using the model. Opinions varied on whether the explanation conveyed the model's accuracy, reliability, or trustworthiness.

One expert noted they had used a similar approach in their work, manually creating visualizations of different subsets of the data. 
Participants noted that the tool is limited to tabular data, restricting its applicability. Another pointed out that our fuzzy definition of neighborhoods might be problematic for users seeking specific values. 

\begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{ESG.jpg}
    \caption{Answers to the explainability satisfaction scale. For each question, colored bars display the number of people per answer, centered around the neutral answer with disagreeing answers to the left and agreeing answers to the right.}
    \label{fig:ESG}
\end{figure}