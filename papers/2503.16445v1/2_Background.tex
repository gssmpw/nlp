\section{Background}
xAI methods are classically divided into \textit{local} and \textit{global} explanation methods~\cite{danilevsky2020survey}. Global methods aim to explain the entire model at once by describing the overall relevance of input features for predictions. Feature interactions are visualized to show how one or two features interact with the prediction. In contrast, local methods focus on explaining the model's behavior for a specific instance, highlighting the relevant features for that instance's prediction and visualizing their influence as negative or positive. This section reviews two of the most frequently used methods.

\subsection{PDPs - Partial Dependence Plots}
\label{sec:pdp}
Partial Dependence Plots (PDPs)~\cite{friedman2001greedy} are a global explanation method that visualizes the influence of one or two features on a prediction, typically using a line chart or heatmap. The x-axis represents the values of a feature, while the y-axis shows the mean prediction. PDPs work well with smaller datasets because they use a mutation-based approach to generate sufficient data points. To calculate the average prediction for a feature value, all available data instances are set to this value, while keeping their other features at their original value.  

A visualization similar to PDPs is the ICE plot. While calculated similarly, each instance is given its own curve instead of averaging all instances to produce a single curve. This results in a more detailed but also more cluttered visualization.

\subsection{SHAP plots}
SHAP plots~\cite{lundberg2018consistent} are a local explanation method based on game theory that visualizes the positive and negative influences of features on the current instance. Each feature is represented by a bar, where the length depicts the strength of its influence, and the direction and color indicate whether the influence is positive or negative. This visualization shows the influence of each feature individually without displaying their interactions, which can be confusing when variables are highly correlated. In such cases, their combined influence might be assigned to one feature or split between them, leading to potentially misleading variations in the final SHAP plot.

Visualizations similar to SHAP, but with different calculation methods, include LIME plots, which are created by learning a model around the local instance~\cite{ribeiro2016should}.