As AI grows due to the increasing availability of data, high-dimensional datasets with numerous instances are becoming the norm. Explainable AI (xAI) is evolving alongside this trend; however, traditional xAI methods are reaching their limits as modern machine learning models increasingly rely on interactions among countless features. Explaining features independently or focusing solely on one or two-feature interactions is insufficient. New methods are needed to make higher-order feature interactions —complex interactions among multiple features— understandable for humans, as these often contain the true predictive power of AI models.

Our focus is on local explanations that clarify the prediction for individual instances. For example, in a decision support system, which feature interactions drive the current recommendation? This need becomes evident when considering the following example by Tsang et al.~\cite{tsang2021interpretable}, who describes the prediction function $f(x)=x+y+xy$. If relevance is assigned only to individual features, how do we attribute the interaction between $x$ and $y$? Similar issues arise in practice; Wang et al.~\cite{Wang2019DesigningTU} found that domain experts prefer to see feature interactions, such as the relationship between systolic and diastolic blood pressure. Moreover, from a security perspective, Xin et al.~\cite{xin2024you} show that partial dependence plots focused on a single feature can be manipulated to conceal discriminatory behavior. These examples underscore the growing need for more effective methods to characterize feature interactions.


Our visual analytics tool \textsc{FINCH} was designed to tackle this challenge for high-dimensional, tabular data. Its intuitive user interface allows machine learning experts to easily trace how multiple features interact and see how each additional feature influences the outcome. This is achieved through a subset-based approach using visualizations similar to the well-known partial dependence plots~\cite{friedman2001greedy}. Instead of visualizing all possible interactions in a feature space, our approach focuses on the local interactions relevant to the current data instance. In summary, we make the following contributions: 

\begin{itemize}
    \item We introduce a novel approach for visualizing higher-order feature interactions at the instance level.
    \item We developed an interactive online tool tailored for machine learning experts. 
    \item We conduct a comprehensive evaluation of our tool through both case studies and a user study.
\end{itemize}

