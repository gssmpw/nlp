\section{Conclusion}
Current xAI approaches often focus on attributing relevance to individual features, overlooking their influential interactions. Our approach advances explainable AI by enabling the visualization of local feature interactions up to high orders. Our tool, \textsc{Finch}, employs a subset-based method to create intuitive visualizations of feature interactions relevant to a specific data instance, while also offering functionalities to validate displayed curves and help users calibrate their trust in the results. Our approach can be adapted to any machine learning model.
Our case studies demonstrated our tools' generalizability, and a user study with five machine learning experts confirmed \textsc{Finch}'s high usability. Using \textsc{Finch}, machine learning experts can examine how high-order feature interactions influence any data instance.

Future research includes generating more reliable results when data instances are sparse at the edges of distributions. Further work is also needed to automatically detect local feature interactions. Given the frequent use of other data types, such as images, future efforts could adapt this approach to those formats.
We also want to provide more support for analyzing feature interactions directly on the ground truth, instead of using the model.
Lastly, we aim to simplify our approach to make it more intuitively understandable for a broader audience, including domain experts and the general public.