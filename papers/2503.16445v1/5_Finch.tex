\section{Presenting \textsc{FINCH}}
Based on our subset-based algorithm, we designed the visual analytics tool \textsc{Finch} that enables machine lerning experts to incrementally explore feature interactions.
In this section, we outline the initial requirements for \textsc{Finch}, explain its core concept, discuss how the tool facilitates trust calibration, and describe its implementation.

\subsection{Requirements}
The requirements were derived from literature review, as well as discussions with three machine learning experts of varying expertise (ranging from grad student to professor). The discussions were guided by using a rain prediction example, from which they described or sketched the information and visualizations needed to understand feature interactions. They reflected on their past experiences with xAI tools, sharing what they appreciated and what they found lacking. In combination with the literature review, we formulated the following requirements.

\begin{itemize}
    \item (R1) Understandable feature interactions:
    Fundamental to our tool was making feature interactions understandable to experts. Using explanations without completely understanding them often leads to false assumptions~\cite{kaur2020interpreting}.
    \item (R2) Differentiate between positive and negative contributions: 
    Machine learning experts prefer to see how each feature or feature interaction contributes to the prediction, whether positive or negative. This is one of the basic requirements for feature attribution methods outlined by Lundberg et al.~\cite{lundberg2018consistent}.
    \item (R3) Highlight subgroups, outliers, and special characteristics: 
    The experts wanted to know what was unusual about the current instance, which is also seen as the most important information in an explanation in the social sciences~\cite{miller2019explanation}.
\end{itemize}

\subsection{Visual Design of the Dependence Plots}
Our plots are designed to be interpretable in a smiliar manner to PDPs, making it easy for experts to transition between them. The line plots display the relationship between features (x-axis) and predictions (y-axis) through a clear curve. To enhance usability, we incorporate several visualization techniques (Fig. \ref{fig:dependency_plot}).

\begin{itemize}
    \item \textbf{centered around mean}: To enhance differentiation between positive and negative contributions (R2), a horizontal line marks the mean prediction, making it easy to see when predictions are above or below it.
    \item \textbf{colored background}: To further emphasize this, the background is colored in blue below the line, and red above it.
    \item \textbf{+/- symbols}: The two symbols are used to indicate that values the areas indicate predictions above, or below the line.
    \item \textbf{two axes}: Accordingly, the y-axis is also centered around the mean prediction. To allow reading the absolute values, a second y-axis is positioned on the right side of the plot. 
    \item \textbf{highlight whats important}:  We use highlighting through saturated red and blue tones that stand out from the background, depending on the specific visualization.
    \item \textbf{mark current instance}: The x-axis feature value of the current instance is marked by a green vertical line.
\end{itemize}
 
\subsection{Visualizing Interactions Incrementally}

\begin{figure}[h!]
    \centering
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{dependency_plot.png}
        \caption{bike rentals per hour}
        \label{fig:dependency_plot}
    \end{subfigure}
    \hspace{0.001\textwidth}
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{weekend.png}
        \caption{bike rentals per hour on weekends}
        \label{fig:weekend}
    \end{subfigure}
    \hspace{0.001\textwidth}
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{winter_weekend.png}
        \caption{bike rentals per hour on weekends in winter}
        \label{fig:winter_weekend}
    \end{subfigure}
    \caption{The incremental visualization of the interaction of hour, weekends, and winter in the bike rental data set. Colored areas visualize the change in each step.}
\end{figure}

The crucial point of our tool was combining the individual curves for each incremental subset into an intuitive explanation.
For each additional feature added to the current interaction, a new curve is generated.
Simply showing all curves at once quickly leads to cluttered visualizations
We therefore carefully selected which curves to include at each step of the process.
The base curve, calculated based on all instances, is included in all visualizations. This ensures a consistent base for comparison. It is displayed in an unobstrusive grey.
The current subset is displayed as a purple curve, being the most prominent curve of the plot.
The previous subset is visible as a desaturated purple curve to enable comparison.
All older curves are hidden to reduce visual clutter.

We employ highlighting techniques to lead the experts attention to the change (Fig. \ref{fig:dependency_plot} - \ref{fig:winter_weekend}).
\begin{itemize}
    \item \textbf{one feature}: Only the base prediction is visible, showing the base prediction across the values of the first feature, that is depicted on the x-axis. We highlight the difference of the base prediction to the mean (Fig. \ref{fig:dependency_plot}).
    \item \textbf{two features}: A purple line is added for the first subset generated from the new feature. We highlight the difference between the base prediction and the subset (Fig. \ref{fig:weekend}).
    \item \textbf{three or more features}: The previous subset is shown as a desaturated purple line, and the new subset displayed in purple. We highlight the difference between the last subset and the current subset (Fig. \ref{fig:winter_weekend}). Alternatively, the expert can choose to highlight the difference of the current subset to the base prediction instead.
\end{itemize}

\subsubsection{Separating Main and Interaction Effects}
\label{sec:interaction}
One of the optional visualizations that \textsc{Finch} provides is the separation of main and interaction effects.

When adding a new feature to the subset, we highlight the difference between the previous prediction without, and the current prediction including it. This difference can be divided into two components: the main effect of the newly added feature and its interaction effect with previously included features.

In the bike rental scenario, in winter there are overall fewer bike rentals due to its main effect. Additionally, winter interacts with hour, resulting in even fewer bike rentals in the morning but slightly more in the evening on winter days.


Mathematically, we can summarize the previously predicted curve as $c + f_X(y)$ where  $c$ is the mean prediction (as our visualizations center around the mean) and $f_X(y)$ the influence of the previous subset $X$ on the variable on the x-axis, $y$. When we add a new feature $Z$, the updated function becomes $f_{X,Z}(y) = c + f_X(y) + a_Z + g^X_Z(y)$, where $a_Z$ represents the main effect of $Z$, and $g^X_Z(y)$ captures the interaction effect of $Z$ with all previous features. 
We can calculate $a_Z$ independently of all other features as the average of all instances that are in the subset defined by only $Z$.   

To help experts distinguish between these effects, we provide an option to display a new line in our plot showing only the main effect of the newly added feature added to the previous prediction. Since the main effect is independent of all other features, including the one on the x-axis, it shifts the previous curve up or down by a constant. We then highlight the difference between this new line and the actual prediction, revealing the interaction effect between the new feature and previously added features (Fig. \ref{fig:interaction_effect}).

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{interaction_effect.png}
    \caption{Interaction effect visualization. This visualization separates the main and interaction effects of a newly added feature by showing the main effect through a blue dotted line and the interaction effect through its difference from the actual purple line.}
    \label{fig:interaction_effect}
\end{figure}

\subsubsection{Choosing the Next Feature}
We use small multiples to help experts choose the next feature that should be added to the currently visualized interaction.

For each potential feature to be added to the subset, we display its dependence plot, ranking features by the strength of their interaction effect at the instance's x-axis value. 
The user can choose to either view the standard dependence plots, or those displaying the interaction effects ass described in Section \ref{sec:interaction}.

\subsection{Calibrating Trust}
xAI is subject to various cognitive biases~\cite{bertrand2022cognitive}. For instance, merely having an explanation, even if not meaningful, can increase trust in the explained prediction~\cite{eiband2019impact}. Our explanations are approximations of the model's behavior based on the provided datasets, and the model may not always accurately reflect the data. Thus, it is crucial to provide machine learning experts with a variety of tools to calibrate their trust in both the explanations and the model to combat cognitive biases.

Displaying additional information directly on the existing plots was deemed disadvantageous, as it made our plots too cluttered. We therefore decided to display the information in separate plots, or provide different views of the plots that hide aspects of it to make space for the new information. We aimed to make each plot version look slightly different, to help experts in keeping track of which view they are currently seeing, whilst still keeping visual consistency to enable easy interpretation (R1).

\subsubsection{Subset Characterization}
First, we provide a detailed characterization of the subset used for our calculations, including the number of instances and their distribution across selected features compared to the general distribution. The distributions of features used for neighborhood calculations are shown on the right, while the distribution for the x-axis feature is displayed directly below the dependence plot for quick reference.

Both distributions are visualized using heatmaps (Fig. \ref{fig:density}). Their brightness reflects the relative number of instances per distribution rather than the absolute count. This ensures that the distribution of the subset is visible, even when it is much smaller than the data set. The current instance is indicated by a green line, as in the dependence plot.

\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{density_small.png}
    \caption{Distribution heatmap. Directly below the dependence plot are the density distributions for the first selected feature, which is visualized on the x-axis.}
    \label{fig:density}
\end{figure}

\subsubsection{Ground Truth}
\textsc{Finch} uses only real instances, without permuting or generating new ones. By working with actual data, we allow users to provide the ground truth for these instances, enabling direct comparison between the model’s predictions and true values. When the expert selects the ground truth view, it is displayed as a dotted line, with differences between the model's prediction and the ground truth highlighted in blue or red areas. For example, in Figure~\ref{fig:truth}, the model's predictions generally align with the ground truth but exceed it beyond a certain threshold.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{truth.png}
    \caption{Truth visualization. The deviation between the model prediction and ground truth is highlighted, and the ground truth is displayed through a dotted line.}
    \label{fig:truth}
\end{figure}

\subsubsection{Uncertainty}
The uncertainty induced by mean approximation, as well as the uncertainty innate to the data are addressed through an uncertainty visualization.
This visualization uses a simple area plot to display the standard deviation around the mean for each feature value along the x-axis.

As shown in Figure~\ref{fig:uncertainty}, the predicted curve shows greater deviation at the peaks and less deviation at lower feature values. It is important to note that this visualization represents the uncertainty of the mean predictions for each feature value, not the model's uncertainty when predicting an individual instance.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{uncertainty.png}
    \caption{Uncertainty visualization. The standard deviation at each feature value is shown as an area around the curve.}
    \label{fig:uncertainty}
\end{figure}



\subsection{Implementation}
\textsc{Finch} is implemented using the Python library Panel\footnote{https://panel.holoviz.org/index.html} for the user interface and Bokeh\footnote{https://docs.bokeh.org/en/latest/docs/gallery.html} for visualizations. The UI of \textsc{Finch} is shown in Figure~\ref{fig:UI}. A test version is hosted on Hugging Face Spaces. Currently, it supports loading all sklearn models saved with python version 3.11 and sklearn version 1.5.1, and datasets saved as CSV files. The code is open source and available on GitHub\footnote{https://github.com/akleinau/Finch}.

The user initially loads a data set (here bike-rentals) and gets an overview of the dependence plots for all features, centered on the current instance's values. Selecting a feature takes the user to the main \textsc{Finch} view, where additional features can be added to refine the subset.


Alternatively, users can load a different data set and model, with the option to provide the ground truth. If there are multiple outcome variables, the desired one can be selected and renamed if needed. Users can choose from the instances of the data set or create a custom one, and any missing values are automatically imputed with the mean.

\textsc{Finch} is tested for both regression and classification models. 
Supported features include continuous and categorical/ordinal types, with any feature having fewer than 24 unique values classified as categorical/ordinal.
Curves of features with more than 24 different values are smoothed using an exponentially weighted average for a smooth appearance that reduces the noise of random statistical fluctuations. The degree of smoothing increases when less data is available or when subsets contain fewer instances. Smoothing can be optionally deactivated.
Categorical feature curves are not smoothed to keep predictions for each class accurate, with the threshold of 24 chosen to preserve the ordinal nature of hourly data.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{interface.png}
    \caption{The user interface of \textsc{Finch}. On the left side, the dataset, target variable, and instance can be selected. The main view shows the dependence plot and the distribution heatmaps. On top are options to select features, and on the bottom are different views of the plot.}
    \label{fig:UI}
\end{figure}