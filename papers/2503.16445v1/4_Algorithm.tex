\section{A Subset-Based Algorithm for Local Higher-Order Feature Interactions}
This section describes the general idea that \textsc{Finch} is based on, and how it allows scaling up to higher-order feature interactions.

We will use the publicly available bike-sharing dataset~\cite{fanaee2014event} from the UCI machine learning library~\footnote{https://archive.ics.uci.edu/dataset/275/bike+sharing+dataset} as a running example. This dataset contains hourly bike rental data from 2011 and 2012, with input features including hour, weekday, working day, month, and season.


\subsection{A Preservative Approach to Dependence Plots}
\label{methods_pdps}

The typical way to visualize feature interactions is using PDPs, which permutate the instances of an available data set to calculate predictions for each possible value of a feature, as described in Section \ref{sec:pdp}. 

The primary issue is that the permutation process does not consider the conditional distributions of features, ignoring their dependencies. This makes them less reliable for highly correlated features, as altering one feature without adjusting others can create unrealistic instances~\cite{apley2020visualizing}. For example, changing the month in our bike rental dataset without adjusting the season can lead to instances that cannot exist (e.g. July in winter). Addressing this issue is crucial, as such permutation-based methods can be vulnerable to adversarial attacks that conceal discriminatory behavior~\cite{xin2024you}. 

One proposed solution to this problem is the use of the conditional probability distributions. For example,
Apley et al.\cite{apley2020visualizing} proposed ALE plots to account for the conditional probability of other features.
While their method still introduces slight perturbations, these are less pronounced than before. Each original data point generates two new points with slightly higher or lower values.


As dataset sizes have grown significantly over recent decades, we question whether perturbations are still needed to generate additional data points in explainable AI. We propose using only the original data points, avoiding even slight perturbations. This approach preserves all feature distributions and interactions in the dataset that might otherwise be distorted by artificially generated points.  

This method reduces computational costs since it eliminates the need to generate and predict new data points for each interaction being investigated. Instead, we only need to generate predictions once for the original data points.

This approach offers another advantage: even before model training, we can calculate feature interactions directly on the original dataset by simply using the ground truth values instead of the model predictions. This also enables direct comparisons between model predictions and actual values after model training.

In cases of actual data scarcity, modern techniques for generating new points could still provide a viable solution\cite{figueira2022survey}.

The calculated dependency can be visualized similar to PDPs using a line curve with the feature on the x-axis and the outcome (probability) on the y-axis. 


\subsection{Using Subsets for Higher-Order Interactions}

To scale our approach up to higher-order interactions, we focus on only those interactions relevant to a specific instance rather than attempting to show all possible interactions.


We want to illustrate our approach on our example of predicting bike rentals (see Fig. \ref{fig:alg_bikes}). When considering only the feature \say{hour}, the predicted dependency curve typically shows peaks around the rush hours.
We now want to consider second-order interactions, using the \say{weekday} as our second feature. As our current instance was recorded on a weekend, we create a subset of instances also recorded on weekends and calculate the new curve based on them. This second curve will lack these peaks due to fewer commuters, showing an afternoon peak instead. 
We can refine this further by considering additional features, such as showing bike rentals by the hour on weekends during winter. By applying such filters to the dataset, we can seamlessly scale up to any number of features, providing a more detailed characterization of how the current instance behaves.


Our algorithm works as follows:
The original line curve is calculated using all instances in the dataset.
For each additional feature, we consider how it interacts with the previous ones when fixed to the current instance's value.
We calculate a second line based solely on instances from the dataset where the second feature matches this value, ensuring we include only realistic, pre-existing data points.
By incrementally calculating a new curve each time a new feature is added, the user can observe how each new feature influences and interacts with previous ones (Fig. \ref{fig:algorithm}).

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{algorithm_bikes.png}
    \caption{Bike rentals based on different subsets of the data set}
    \label{fig:alg_bikes}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{algorithm.png}
    \caption{Our algorithm incrementally adding features, by calculating and visualizing subsets for each added feature.}
    \label{fig:algorithm}
\end{figure}


\subsection{Calculating Subsets}
Ideally, our subsets would contain only instances that have identical feature values as our target instance for the selected features (except for the first one shown on the x-axis). However, since our dataset is limited in size and continuous features rarely have exact matches, we use heuristics to select instances for each subset. These heuristics balance two goals: maintaining high similarity within the subset while ensuring enough data points for reliable calculations.

To achieve this, we employ a set of approximations chosen experimentally:
\begin{itemize}
    \item \textbf{include the 5\,\% most similar instances}
    \item \textbf{include at least the 50 most similar instances.} This makes sure our subset will not be too small and statistically insignificant.
    \item \textbf{include all instances that are almost identical.} this is particularly important for categorical features, where many instances share the same feature value.
\end{itemize}


Similarity is calculated using Euclidean distance between the selected features. Each feature is normalized beforehand to ensure equal weighting.
This approach works for categorical features too, since they must be numerically encoded for use in \textsc{finch}.
By calculating similarity only on selected features, we have access to many more instances compared to requiring similarity across all features.

Instances are treated as almost identical to the current instance if their distance is smaller or equal to the number of columns times 0.1: $d <= len(columns) * 0.1$. 
This value was chosen experimentally.


As our procedure ensures at least 50 instances to be selected for each subset, when an instance does not have highly similar instances, the subset may contain instances that are less similar. Thats why in \textsc{finch}, we caracterize the subset using distribution plots to let the user know how similar the instances are, allowing them to gauge the credibility of the curve.


\subsubsection{Categorical Features}
Our current algorithm adapts to a categorical target by letting the user select one of the classes and visualizing the probability of belonging to that class on the y-axis. 
Further categorical features can be entered into the tool in two ways: through numerical encoding (which creates an implicit ordering) or through one-hot encoding (where each possible feature value becomes its own feature).
When using categorical features, experts should note that numerical encoding creates an implicit ordering that affects similarity calculations between instances. With one-hot encoding, \textsc{finch}'s ability to handle higher-order interactions enables it to consider relationships across all encoded feature values.