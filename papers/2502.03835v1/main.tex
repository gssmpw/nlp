% CVPR 2025 Paper Template; see https://github.com/cvpr-org/author-kit

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage{cvpr}              % To produce the CAMERA-READY version
% \usepackage[review]{cvpr}      % To produce the REVIEW version
\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version
\usepackage{multirow} % 包含 multirow 宏包
% Import additional packages in the preamble file, before hyperref
\input{preamble}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, 
% e.g. with the file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete *.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you should be clear).
\definecolor{cvprblue}{rgb}{0.21,0.49,0.74}
\usepackage[pagebackref,breaklinks,colorlinks,allcolors=cvprblue]{hyperref}

%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\paperID{9969} % *** Enter the Paper ID here
\def\confName{CVPR}
\def\confYear{2025}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Single-Domain Generalized Object Detection by Balancing Domain Diversity and Invariance}

%%%%%%%%% AUTHORS - PLEASE UPDATE
\author{Zhenwei He, Hongsu Ni\\
Chongqing University of Technology\\
{\tt\small hzw@cqut.edu.cn, nhs2@stu.cqut.edu.cn }}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
% \and
% Second Author\\
% Institution2\\
% First line of institution2 address\\
% {\tt\small secondauthor@i2.org}
% }

\begin{document}
\maketitle
% \input{sec/0_abstract}    
% \input{sec/1_intro}
% \input{sec/2_formatting}
% \input{sec/3_finalcopy}

\begin{abstract}
Single-domain generalization for object detection (S-DGOD) aims to transfer knowledge from a single source domain to unseen target domains. In recent years, many models have focused primarily on achieving feature invariance to enhance robustness. However, due to the inherent diversity across domains, an excessive emphasis on invariance can cause the model to overlook the actual differences between images. This overemphasis may complicate the training process and lead to a loss of valuable information. To address this issue, we propose the Diversity Invariance Detection Model (DIDM), which focuses on the balance between the diversity of domain-specific and invariance cross domains. Recognizing that domain diversity introduces variations in domain-specific features, we introduce a Diversity Learning Module (DLM). The DLM is designed to preserve the diversity of domain-specific information with proposed feature diversity loss while limiting the category semantics in the features. In addition, to maintain domain invariance, we incorporate a Weighted Aligning Module (WAM), which aligns features without compromising feature diversity. We conducted our model on five distinct datasets, which have illustrated the superior performance and effectiveness of the proposed model.
\end{abstract}

\section{Introduction}
\label{sec:intro}

In recent years, deep learning has led to significant improvements in the performance of detection models. However, traditional detectors often rely on the assumption of Independent and Identically Distributed (i.i.d.) data. This assumption becomes a limitation when the model encounters a domain shift, leading to a significant drop in performance and restricting the applicability of these models in real-world scenarios.
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{72.png}
    \caption{A brief overview of DIDM is presented. The DLM is designed for the domain-specific feature to preserve their diversity and suppress the semantic information. The WAM is employed to align features without compromising the feature diversity with loss weight.}
    \label{fig:enter-label1}
\end{figure}

To address the domain shift problem, domain generalization (DG) has been developed to transfer knowledge from the source domains to the unseen target domain \cite{dou2019domain, liu2024unbiased, zhou2022domain,bi2024learning}. These methods enhance model robustness, making it more robust to more application scenarios. Recently, single-domain generalization \cite{liu2020towards,wu2022single,wu2024g,lee2024object,vidit2023clip} has gained significant attention as it reduces data requirements by training the model on a single domain, which is a much more challenging task compared to the traditional DG.

Most existing single-domain generalization models aim to reduce domain bias by learning domain-invariant features, which often restrict the model and prevent the capture of domain-specific information. This approach requires the model to extract similar, or even identical, features from images across different domains. Considering significant domain differences, such a constraint is counterintuitive and can limit effective feature extraction, potentially complicating the training process. To address this, we proposed that a domain generalization (DG) model should not only consider the learning of domain invariance but also the diversity of the domain-specific information. The learning of domain invariance ensures cross-domain robustness, while the diversity of domain-specific features enhances feature learning and model training.

Based on the above discussion, specifically, in this paper, we introduce the Diversity Invariance Domain Generalization Detection Model (DIDM), designed to not only learn the domain invariance but also keep the diversity domain-specific. To manage these two feature types, we incorporate the Diversity Learning Module (DLM) to capture domain-specific features and the Weighted Aligning Module (WAM) to focus on domain-invariant features, as illustrated in \cref{fig:enter-label1}. 

The Diversity Learning Module (DLM) is designed to preserve the domain diversity and limit the semantic limitation. Firstly, to eliminate the semantic information among the domain-specific features, we apply an Entropy Maximization loss to reduce category-level semantics. However, focusing solely on constraining domain-specific information may limit the model's ability to learn diverse features. To address this, we introduce a feature diversity loss (FD) to reinforce the diversity of domain-specific information. Finally, the DLM offers three key advantages: first, by reducing category semantics in domain-specific features, it supports domain-invariant training; second, it preserves feature diversity across domains, enhancing the training phase of the model. Additionally, by encouraging feature diversity with FD loss, the module compels the model to capture richer semantic information from images, resulting in more semantically nuanced features.

In addition to domain-specific feature learning with DLM, domain invariance learning may also influence the feature diversity in the Domain Generalization (DG) model. Thus, we introduce the Weighted Aligning Module (WAM) to alleviate diversity loss with feature alignment. The proposed WAM is designed with two main objectives: First, by feature alignment across domains, it enables the model to learn domain-invariant features for the detection task; Second, by incorporating loss weights, the model prevents the overemphasizes the feature alignment. When the feature differences do not influence the detection, the model down-weight the alignment to preserve the domain diversity. By combining both the DLM and WAM, our model achieves effective domain generalization on the detection task. In conclusion, the contribution of the proposed DIDM is presented as follows:

\begin{itemize}
\item We introduce the Diversity Learning Module (DLM) to preserve the feature diversity while limiting the semantic information within domain-specific features. The feature diversity loss is proposed to enhance the feature diversity.

\item The Weighted Aligning Module (WAM) is designed to align features without influencing the feature diversity. The loss weight is proposed to prevent an overemphasis on domain-specific features.

\item Experiments were conducted under five distinct weather conditions, effectively showcasing the robustness of our methodology.
\end{itemize}

%-------------------------------------------------------------------------
%------------------------------------------------------------------------
\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{67.png}
    \caption{The general architecture of DIDM. The input image is processed through the backbone to extract both original and enhanced features, as well as to obtain the objective domain-specific feature. Subsequently, this feature is processed through the Diversity Learning Module (DLM) and the Weighted Aligning Module (WAM). DLM is implemented for domain-specific features with maximum entropy loss and feature diversity loss, where the diversity is enhanced and semantic information is suppressed. Weighted Aligning Module (WAM) imposes a constraint on feature alignment, which prevents the model's overemphasis on the alignment and compromises diversity learning. Together, the DLM and WAM work synergistically to ensure that the model focuses on robust features.}
    \label{fig:enter-label2}
\end{figure*}


\section{Related Work}
\label{sec:Work}

\textbf{Domain Generalization (DG).} Researchers have been working extensively in single domain generalization and have developed numerous methods to enhance it \cite{dou2019domain, liu2024unbiased, li2024prompt, wu2024g, zhou2022domain}. These methods can generally be grouped into three main categories: First, the data argumentation approach \cite{kang2022style,lee2022wildnet,li2023deep,somavarapu2020frustratingly} increases the diversity of source domain data by applying various data manipulation techniques. For instance, Somavarapu et al. \cite{somavarapu2020frustratingly} introduce a straightforward image stylization transform to generate diverse samples, exploring variability across sources. Similarly, Wang et al. \cite{wang2021learning} employ adversarial training to create varied input images, further enhancing generalization. Second, some kinds of DG methods focus on the presentation learning of domain invariance. Shao et al. \cite{shao2019multi} use multi-adversarial discriminative training to extract both shared and distinctive feature representations across multiple source domains. Last, Various learning strategies \cite{chen2023meta,du2020learning,peng2022semantic,seo2020learning,wang2023improving,zhao2021learning} are employed to improve model generalization. For example, Zhao et al. \cite{zhao2021learning} apply a meta-learning approach that simulates the model’s adaptation to new, unseen domains during training, thereby boosting its adaptability in unfamiliar environments. These approaches illustrate significant advancements in domain generalization, highlighting different techniques designed to improve models' performance on previously unseen domains.

\textbf{Object Detection.} 
To mitigate the impact of domain bias on model performance, researchers have conducted numerous experiments and proposed various methods in object detection. One prominent approach is Domain Adaptive Object Detection (DAOD) \cite{chen2021i3net,deng2021unbiased,krishna2023mila,li2022cross,oza2023unsupervised,zhao2020review}, which enhances the model's robustness in new domains by simultaneously training on both source and target domains. For instance, a common strategy involves minimizing the distance between global and local features, as outlined in \cite{cao2023contrastive,chen2020harmonizing,deng2021unbiased,saito2019strong}. However, these methods require access to the target domain during model training, which imposes limitations on practical applications. Therefore, researchers have proposed Domain Generalized Object Detection (DGOD) \cite{chang2024unified,qin2024towards}. For instance, Chang et al. \cite{chang2024unified} enhance the model's generalization ability by decoupling depth estimation from dynamic perspective enhancement. However, the effectiveness of DGOD largely depends on the number of accessible source domains, and collecting multiple source domains can be costly. Consequently, researchers have introduced the more challenging Single Domain Generalized Object Detection (S-DGOD) \cite{vidit2023clip,danish2024improving,lee2024object,pan2018two,choi2021robustnet}, which is generally categorized into feature normalization and invariant-based methods. Firstly, feature normalization methods such as IBN-Net, proposed by Pan et al. \cite{pan2018two}, enable the network to adjust its normalization strategy according to different tasks and datasets by combining Instance Normalization (IN) and Batch Normalization (BN). IterNorm, introduced by Huang et al. \cite{huang2019iterative}, avoids feature decomposition through a Newton iteration method, thereby improving the efficiency of the normalization process. Secondly, invariant-based approaches, such as UFR proposed by Liu et al. \cite{liu2024unbiased}, enhance the model's generalization performance by eliminating prototype bias and attentional bias. Wu et al. \cite{wu2022single} propose a cyclic disentanglement self-distillation approach specifically for single-domain generalization in object detection, which enhances feature disentanglement. These approaches demonstrate significant progress in the field of target detection and contribute to the rapid advancement of the discipline.

%-------------------------------------------------------------------------
\section{Methodology}
\label{sec:method}
In this section, we introduce the proposed Diversity Invariance Detection Model (DIDM), which has two main components: the Diversity Learning Module (DLM) and the Weighted Aligning Module (WAM). These components work together to balance domain-invariant feature learning and feature diversity. Specifically, the DLM is designed to preserve feature diversity and restricting semantic information for domain-specific features, while the WAM aligns features across domains to enhance domain invariance.

\subsection{Preliminaries}

\textbf{Problem Description.} 
For single-domain generalization detection tasks, the model is trained using one source domain $D_s=\{{(x^i_s,y^i_s,b^i_s)}\}^{N_s}_{i=1}$, where $N_s$ stand for the number of samples,  $x^i_s$ presents the input image, $b^i_s$ and $y^i_s$ are the ground truth bounding boxes and corresponding labels, respectively. The model is trained for the detection of the unseen target domain $D_t=\{{(x^i_t)}\}^{N_t}_{i=1}$. Note that the source and target domains share the same label space.

\textbf{Data Augmented.} 
For the single-domain generalization task, training a model on single source dataset may lead to overfitting to domain-specific styles and features. Besides, obtaining diverse datasets across multiple domains is often costly. To address this, we apply data augmentation techniques \cite{bachman2014learning,hsu2020every,laine2016temporal} to increase the diversity of the source domain images, thereby enhancing the model's adaptability and robustness to variations across different domains. These augmentations include techniques such as noise addition, blurring, and random cropping. Each source image $x_s$ is transformed through these augmentation functions $A(.)$ to produce the enhanced image $x_a=A(x_s)$.

\subsection{Overview}

The net structure of the proposed model is shown in \cref{fig:enter-label2}, where Faster R-CNN \cite{ren2016faster} is used as the base detector. Given a source image $x_s$, we can generate an argument image $x_a=A(x_s)$. During the training phase, both $x_s$ and $x_a$ are fed into the model to produce their corresponding feature maps $F_s$ and $F_a$, respectively. Only the $F_s$ is fed into RPN to get a series of proposals $P \in R^{K \times 4}$. Then, the $F_s$, $F_a$ and $P$ are fed into the RoI-Pooling layer to get the features of proposals for the original and argument image: $z_{s}=RP(F_{s},P)$, $z_{a}=RP(F_{a},P)$.

After that, the pooled feature $z_{s}$ and $z_{a}$ are fed into the further network for detection results, supervised by the loss function of Faster-RCNN. To support domain alignment and feature diversity learning, $z_{s}$ and $z_{a}$ are sent into the Weighted Aligning Module (WAM) and Diversity Learning Module (DLM). The goal of this process is to achieve domain alignment while preserving domain-specific diversity. In the DLM, feature diversity loss is applied to enhance the feature diversity, while entropy maximization loss eliminates the semantic information. In the WAM, the loss weight is used to prevent overemphasizing the domain alignment, which may influence the feature diversity. By integrating the supervision from both the DLM and WAM, the proposed Diversity Invariance Detection Model (DIDM) can achieve improved performance on the domain generalization task.

\subsubsection{Diversity Learning Module (DLM)}
\label{sec:methodDLM}
Traditional domain alignment methods primarily focus on learning domain-invariant features, overlooking the importance of domain-specific information. This limitation restricts the model's ability to achieve feature diversity, impacting feature extraction. To address this, we introduce the Diversity Learning Module (DLM), which restricts the domain-specific feature to achieve feature diversity. The DLM operates as follows: first, to obtain domain-specific features that capture domain information, we draw inspiration from OCR \cite{jing2023order}, assuming the relationship between the original and augmented features as $z_{a}= \lambda_1 z_{s}+(1- \lambda_1) z_d$. Here, $\lambda_1$ is a gradually increasing hyperparameter, reflecting the model's improving capacity to learn domain-invariant features over the process of training. As this capacity grows, the domain-specific information in $z_{s}$ and $z_{a}$ correspondingly diminishes. Ultimately, this approach enables us to obtain the domain-specific feature $z_d$ during training.

Second, considering that the domain-specific feature $z_d$ should not contain semantic information, we introduce the entropy maximization loss for $z_d$, where a classifier is trained with the cross-entropy loss:
\begin{equation}
\begin{aligned}
\mathcal{L}_\mathcal{C} = -\sum\limits_{i=1}^C y_ilog([Softmax(f(z_d))])
\end{aligned}
\end{equation}
where $C$ represents the number of categories, $f(z_d) \in R^{K \times C}$ is prediction function for $z_d$. Inspired by \cite{jing2023order}, to estimate the category semantic information in $z_d$, we implement to maximization of the entropy of the prediction results from $z_d$.
\begin{equation}
\begin{aligned}
\mathcal{L}_\mathcal{H} = -\mathcal{H}(y|z_d) = -\mathcal{H}[Softmax(f(z_d))]
\label{eq:pythagoras}
\end{aligned}
\end{equation}
where $\mathcal{H}$ represents entropy computation. By minimizing \cref{eq:pythagoras}, we can maximize the conditional entropy based on the classifier $f(.)$. When the output of the classifier for all categories is uniform across all categories, the conditional entropy reaches its maximum, thereby reducing the semantic information contained in $f(z_d)$.
%------------------------------------------------------------------------

\begin{table*}
    \centering
    \setlength{\tabcolsep}{8.6pt}
    \fontsize{9.9}{15}\selectfont\begin{tabular}{c|c|cccc|c}
    \bottomrule
         \textbf{Methods}&  \textbf{daytime-clear} &  \textbf{night-sunny}&  \textbf{dusk-rainy}&  \textbf{night-rainy}&  \textbf{daytime-foggy}&  \textbf{Average} \\
         \hline
          Faster R-CNN \cite{ren2016faster}&  54.7&  34.0&  30.5&  14.0&  32.2& 27.7\\
          SW \cite{pan2019switchable}&  50.6&  33.4&  26.3&  13.7&  30.8& 26.1\\
         IBN-Net \cite{pan2018two}&  49.7&  32.1&  26.1&  14.3&  29.6& 25.5\\
         IterNorm \cite{huang2019iterative}&  43.9&  29.6&  22.8&  12.6&  28.4& 23.4\\
         ISW \cite{choi2021robustnet}&  51.3&  33.2&  25.9&  14.1&  31.8& 26.3\\
         S-DGOD \cite{wu2022single}&56.1 &36.6 &28.2 &16.6 &33.5 &28.8\\
         CLIP-Gap \cite{vidit2023clip}&51.3 &36.9 &32.3 &18.7 &38.5 &31.6 \\
         Prompt-Driven \cite{li2024prompt}&53.6 &38.5 &33.7 &19.2 &39.1 &32.6 \\
         OA-DG \cite{lee2024object}&55.8 &38.0 &33.9 &16.8 &38.3 &31.8 \\
         \hline
         \hline
         Ours &\textbf{56.1} & \textbf{42.0} & \textbf{35.4} & \textbf{19.2} & \textbf{39.3} &\textbf{34.0} \\
         \toprule
    \end{tabular}
    \caption{Single-Domain Generalization Object Detection Results. We trained the model using daytime-clear as the source and subsequently tested it within that domain, while also assessing its generalization across the other four domains. The results presented in the table indicate that our Diversity Invariant Detection Model (DIDM) significantly enhances the model's generalization performance, with bold text highlighting optimal performance. This underscores the adaptability and effectiveness of DIDM across various environments.}
    \label{tab:my_label1}
\end{table*}
%------------------------------------------------------------------------
Lastly, in addition to estimating semantic information, we introduce a \textbf{feature diversity (FD) loss} to promote diverse characteristics among features. Specifically, let the domain-specific feature for each proposal be $z_d^i$. We calculate the cosine similarity between each pair $z_d^i$ and $z_d^j$ for all proposals. Given that each proposal may capture different information due to variations in location or domain, our goal is to increase the distinctions between $z_d^i$ and $z_d^j$, where $i,j \in [1, K]$ and $K$ is the number of proposals. Consequently, the feature diversity loss can be expressed as:
\begin{equation}
\begin{aligned}
\mathcal{L}_\mathcal{FD} = -\sum\limits_{i=1}^K log(\dfrac{e^{sim(f'(z_d^i),f'(z_d^i))}}{\sum\limits_{j=1}^K e^{sim(f'(z_d^i),f'(z_d^j))}} )
\end{aligned}
\end{equation}
Here $sim(.)$ denotes the cosine similarity between feature mappings. $\hat{y}_i$ is set as 1 when $i=j$ and to 0 otherwise. $f'(.)$ is the function of the fully connected layer. The diversity loss is designed to encourage a wide range of predicted categories, maximizing the dissimilarity among features.

The total loss for the Diversity Learning Module is calculated as follows:
\begin{equation}
\begin{aligned}
\mathcal{L}_\mathcal{DLM} = \mathcal{L}_\mathcal{C}+\mathcal{L}_\mathcal{H}+\lambda_2 \mathcal{L}_\mathcal{FD}
\end{aligned}
\end{equation}
where $\lambda_2$ is the hyperparameter for balancing the loss, with the supervision of the DLM, our model can both limit the semantic information of the domain-specific feature, while keeping the feature diversity. 

\subsubsection{Weighted Aligning Module (WAM)}
\label{sec:methodWAM}

DLM focuses on learning domain-specific features, while WAM is introduced to promote domain invariance. Considering that overemphasis on domain invariance may influence domain diversity, we implement the loss weight within the feature alignment in WAM. Specifically, during the training phase, we align the original feature $z_{s}$ and the argument features $z_{a}$. Simultaneously, we minimizing the difference between $z_{s}$ and $z_{a}$, while maximizing the distance between $z_{s}$, $z_{a}$ and $z_d$ for the alignment:
\begin{align}
\mathcal{L}_{fa}&=1- \mathcal{L}(z_{s},z_{a})\\
\mathcal{L}_{fs}=&\dfrac {1}{2}(\mathcal{L}(z_{s},z_d) + \mathcal{L}(z_{a},z_d))\\
\mathcal{L}(z_{s},z_{a}) =& \dfrac{sim(z_{s}^i,z_{a}^i)}{sim(z_{s}^i,z_{a}^i) + \sum\limits_{j \neq i, j=1}^K sim(z_{s}^i,z_{a}^j)}
\end{align}
By aligning features across different domains, we encourage the model to learn domain-invariant features. However, solely constraining domain disparity can reduce feature diversity. To prevent the model from overly restricting domain-specific information, we introduce a weighted parameter for feature alignment. When detection results based on $z_{s}$ and $z_{a}$ are similar, the differences between $z_{s}$ and $z_{a}$ have minimal impact, suggesting that the alignment loss should be down-weighted to preserve feature diversity. Specifically, the weight for the alignment loss is defined as:
\begin{align}
\beta &= 2-e^{-\dfrac {1}{2}(\mathcal{L}_{c}+\mathcal{L}_{b})}
\label{eq:pythagoras8}
\end{align}
where $\mathcal{L}_{c} = \sum\limits_{i=1}^K KL(p_{s}^i,p_{a}^i)$, and
$\mathcal{L}_{b} = ||b_{s}^i-b_{a}^i||_2^2$ from \cite{danish2024improving}. $p_{s}^i, p_{a}^i$ are the classification results of $z_{s}$ and $z_{a}$, respectively, while $b_{s}^i-b_{a}^i$ are the bounding box regression results. Finally, the loss function of the weighted alignment module (WAM) is defined as follows:
\begin{align}
\mathcal{L}_\mathcal{WAM}=\beta \mathcal{L}_{fa}+ \mathcal{L}_{fs}
\end{align}
By training with the loss weight $\beta$, the model down weight the alignment loss when the differences between the original and argument features do not influence the detection results. This operation prevents the overemphasis on the feature alignment. Thus, the WAM can align the feature without compromising the feature diversity. Thus, the feature learning of the model is further enhanced.

\subsubsection{Overall Optimization Objective}
The overall optimization objective of the model is the following:
\begin{align}
\mathcal{L}_{total}=\mathcal{L}_{det}+\alpha \mathcal{L}_\mathcal{DLM}+ \gamma \mathcal{L}_\mathcal{WAM}
\label{eq:pythagoras10}
\end{align}
Here $\mathcal{L}_{det}=\mathcal{L}_{reg}+\mathcal{L}_{cls}$ is the detection loss, $\alpha$ and $\gamma$ is the hyperparameters for balancing the loss. With the co-training of the detection loss, DLM and WMA, our model achieve effective domain generalization on the task.
% %------------------------------------------------------------------------
\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{25.pdf}
    \caption{Qualitative evaluation results for night-sunny and night-rainy weather conditions are presented. The \textbf{top row} showcases the detection visualizations generated by the \textbf{Faster R-CNN} \cite{ren2016faster} model, while the \textbf{bottom row} displays the detection visualizations produced by the \textbf{DIDM}. The images on the \textbf{left} depict \textbf{night-sunny} conditions, whereas those on the \textbf{right} illustrate \textbf{night-rainy} conditions.}
    \label{fig:enter-label3}
\end{figure*}
%------------------------------------------------------------------------
%------------------------------------------------------------------------
\section{Experiments}
\label{sec:experiment}
\subsection{Experimental Setup}

\textbf{Datasets.} 
To verify the validity of the method, we utilize the datasets created by \cite{wu2022single}. Five distinct weather conditions are included: daytime-clear, night-sunny, dusk-rainy, night-rainy, and daytime-foggy. These datasets are primarily derived from 27,708 images captured during daytime-clear conditions and 26,158 images taken at night-sunny conditions, collected from BDD-100K \cite{yu2020bdd100k}. Additionally, 3,775 images with foggy conditions were sourced from Adverse Weather \cite{hassaballah2020vehicle} and Cityscape datasets \cite{cordts2016cityscapes}. The datasets with rain rendered using \cite{wu2021vector}, including 3,501 images from dusk-rainy days and 2,494 images from rainy nights. In this paper, we employ 19,395 daytime images of sunny conditions as the training set to train the model, while another 8,313 images are used to evaluate the model's performance. The remaining four datasets are treated as unseen target domains to assess the model's generalization capabilities. In these datasets, we focus on seven common categories: car, bike, bus, rider, person, motor, and truck.
%------------------------------------------------------------------------
\begin{table}
    \centering
    \setlength{\tabcolsep}{5.8pt}
     \fontsize{7.89}{12}\selectfont\begin{tabular}{@{  }c|p{0.8cm}@{}p{0.8cm}@{}p{0.75cm}@{}c@{  }c@{  }c@{  }c@{  }|c@{  }}
    \bottomrule
     Methods & Bus & Bike & Car &Motor & Person & Rider &Truck & mAP \\
        \hline
           FR \cite{ren2016faster} & 33.7 & 27.4 & 61.1 & 10.6 & 40.2 & 25.7 & 39.6 & 34.0 \\
           SW \cite{pan2019switchable} &38.7 &29.2 &49.8 &16.6 &31.5 &28.0 &40.2 &33.4 \\
           IBN-Net \cite{pan2018two} &37.8 &27.3 &49.6 &15.1 &29.2 &27.1 &38.9 &32.1 \\
           IterNorm \cite{huang2019iterative} &38.5 &23.5 &38.9 &15.8 &26.6 &25.9 &38.1 &29.6 \\
           ISW \cite{choi2021robustnet} &38.5 &28.5 &49.6 &15.4 &31.9 &27.5 &41.3 &33.2 \\
         S-DGOD \cite{wu2022single} &40.6 &35.1 &50.7 &19.7 &34.7 &32.1 &43.4 &36.6\\
         CLIP-Gap \cite{vidit2023clip} &37.7 &34.3 &58.0 &19.2 &37.6 &28.5 &42.9 &36.9 \\
         Prompt-D \cite{li2024prompt}&\textbf{49.9} &35.0 &59.0 &21.3 &40.4 &29.9 &42.9 &38.5 \\
        \hline
        \hline
         Ours&43.5  &\textbf{40.1}  &\textbf{65.1}  &\textbf{22.4}  &\textbf{45.2}  &\textbf{32.5}  &\textbf{45.3}  &\textbf{42.0} \\
         \toprule
    \end{tabular}
    \caption{The quantitative results (\%) on the night-sunny.}
    \label{tab:my_label2}
\end{table}
%------------------------------------------------------------------------

\textbf{Implementation Details.}
We utilize Faster R-CNN \cite{ren2016faster} as a two-stage detector, with ResNet101 \cite{deng2021unbiased} serving as the backbone network. The base network employs weights that have been pre-trained on the ImageNet dataset \cite{deng2009imagenet}. To train the model, we apply the stochastic gradient descent (SGD) algorithm with a momentum parameter of 0.9. The initial learning rate is set to 0.02 and decays every three epochs, while the batch-size is 4. All reported mean average precision (mAP) values are based on an Intersection over Union (IoU) threshold of 0.5.

\textbf{Data Augmentation Setting.}
To enhance the diversity of the source domain, we employ techniques such as random clipping, the addition of Gaussian noise, grayscale enhancement, and color transformation to expand the dataset.

\subsection{Performance Analysis}
The DIDM we studied is compared here with four feature-based normalization methods, they are SW \cite{pan2019switchable}, IBN-Net \cite{pan2018two}, IterNorm \cite{huang2019iterative} and ISW \cite{choi2021robustnet}. Comparisons were also made with the latest S-DGOD \cite{wu2022single}, CLIP-Gap \cite{vidit2023clip}, Prompt-Driven \cite{li2024prompt}, and OA-DG \cite{lee2024object}methods.
%------------------------------------------------------------------------
%------------------------------------------------------------------------
\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{26.pdf}
    \caption{Qualitative evaluation results for night-sunny and night-rainy weather conditions are presented. The \textbf{top row} showcases the detection visualizations generated by the \textbf{Faster R-CNN} \cite{ren2016faster} model, while the \textbf{bottom row} displays the detection visualizations produced by the \textbf{DIDM}. The images on the \textbf{left} depict \textbf{daytime-foggy} conditions, whereas those on the \textbf{right} illustrate \textbf{dusk-rainy} conditions.}
    \label{fig:enter-label4}
\end{figure*}
%------------------------------------------------------------------------
%------------------------------------------------------------------------

\begin{table}
    \centering
    \setlength{\tabcolsep}{5.8pt}
     \fontsize{7.89}{12}\selectfont\begin{tabular}{@{  }c|p{0.8cm}@{}p{0.8cm}@{}p{0.75cm}@{}c@{  }c@{  }c@{  }c@{  }|c@{  }}
    \bottomrule
     Methods & Bus & Bike & Car &Motor & Person & Rider &Truck & mAP \\
        \hline
           FR \cite{ren2016faster} & 25.1 & 6.1 & 35.7 & 0.2 & 9.1 & 3.9 & 17.8 & 14.0 \\
           SW \cite{pan2019switchable} &22.3 &7.8 &27.6 &0.2 &10.3 &10.0 &17.7 &13.7 \\
           IBN-Net \cite{pan2018two} &24.6 &10.0 &28.4 &0.9 &8.3 &9.8 &18.1 &14.3 \\
           IterNorm \cite{huang2019iterative} &21.4 &6.7 &22.0 &0.9 &9.1 &10.6 &17.6 &12.6 \\
           ISW \cite{choi2021robustnet} &22.5 &11.4 &26.9 &0.4 &9.9 &9.8 &17.5 &14.1 \\
           S-DGOD \cite{wu2022single} &24.4 &11.6 &29.5 &9.8 &10.5 &11.4 &19.2 &16.6\\
          CLIP-Gap \cite{vidit2023clip} &28.6 &12.1 &36.1 &9.2 &12.3 &9.6 &22.9 &18.7 \\
          Prompt-D \cite{li2024prompt}&25.6 &12.1 &35.8 &\textbf{10.1} &\textbf{14.2} &\textbf{12.9} &22.9 &19.2 \\
        \hline
        \hline
         Ours&\textbf{31.6}  &\textbf{12.1}  &\textbf{38.3}  &3.8  &12.8  &10.6 &\textbf{25.0}  &\textbf{19.2} \\
         \toprule
    \end{tabular}
    \caption{The quantitative results (\%) on the night-rainy.}
    \label{tab:my_label4}
\end{table}
%------------------------------------------------------------------------
\textbf{Results on all datasets.}
\cref{tab:my_label1} presents the testing and generalization results of DIDM on the DWD datasets \cite{wu2022single}. In this study, we utilized the daytime-clear dataset to train the model, tested it on the same dataset, and evaluated its generalization capabilities on four additional datasets: night-sunny, dusk-rainy, night-rainy, and daytime-foggy. As shown in \cref{tab:my_label1}, our approach achieves the highest average generalization performance of 34.0\%. When compared to the baseline network, Faster R-CNN \cite{ren2016faster}, our method demonstrates improvements of 7.1\% and 4.9\% on the daytime-foggy and dusk-rainy datasets, respectively. Additionally, performance on the night-sunny dataset was significantly enhanced by 8.0\%, while the night-rainy dataset, characterized by a complex environment, saw an improvement of 5.2\%.
%------------------------------------------------------------------------

\begin{table}
    \centering
    \setlength{\tabcolsep}{5.8pt}
     \fontsize{7.89}{12}\selectfont\begin{tabular}{@{  }c|p{0.8cm}@{}p{0.8cm}@{}p{0.75cm}@{}c@{  }c@{  }c@{  }c@{  }|c@{  }}
    \bottomrule
     Methods & Bus & Bike & Car &Motor & Person & Rider &Truck & mAP \\
        \hline
           FR \cite{ren2016faster} & 22.9 & 27.6 & 55.8 & 29.6 & 33.1 & 34.9 & 21.3 & 32.2 \\
           SW \cite{pan2019switchable} &30.6 &36.2 &44.6 &25.1 &30.7 &34.6 &23.6 &30.8 \\
           IBN-Net \cite{pan2018two} &29.9 &26.1 &44.5 &24.4 &26.2 &33.5 &22.4 &29.6 \\
           IterNorm \cite{huang2019iterative} &29.7 &21.8 &42.4 &24.4 &26.0 &33.3 &21.6 &28.4 \\
           ISW \cite{choi2021robustnet} &29.5 &26.4 &49.2 &27.9 &30.7 &34.8 &24.0 &31.8 \\
           S-DGOD \cite{wu2022single} &32.9 &28.0 &48.8 &29.8 &32.5 &38.2 &24.1 &33.5\\
          CLIP-Gap \cite{vidit2023clip} &36.1 &34.3 &58.0 &33.1 &39.0 &43.9 &25.1 &38.5 \\
          Prompt-D \cite{li2024prompt}&36.1 &\textbf{34.5} &58.4 &33.3 &\textbf{40.5} &\textbf{44.2} &26.2 &39.1 \\
        \hline
        \hline
         Ours&\textbf{38.5}  &31.6  &\textbf{62.1}  &\textbf{35.8}  &36.8  &42.7  &\textbf{27.3}  &\textbf{39.3} \\
         \toprule
    \end{tabular}
    \caption{The quantitative results (\%) on the daytime-foggy.}
    \label{tab:my_label5}
\end{table}
%------------------------------------------------------------------------
\textbf{Results on night-sunny conditions.}
As shown in the results presented in \cref{tab:my_label2}, our method exhibits outstanding performance across several object detection categories, particularly in the Bike, Car, and Person categories. It achieves significant performance improvements compared to all methods in the table. In these categories, our method attains detection accuracies of 40.1\%, 65.1\%, and 45.2\%, respectively. This demonstrates the advantages of our approach in feature normalization and target detection. In addition, we achieve an overall performance improvement of 5.4\%, 5.1\%, and 3.5\% compared to the latest S-DGOD \cite{wu2022single}, CLIP-Gap \cite{vidit2023clip} and prompt-D \cite{li2024prompt}, respectively. This further demonstrates the superior adaptability and robustness of our method in handling data across different domains. \cref{fig:enter-label3} further confirms that the detection accuracy of our method surpasses that of the baseline Faster R-CNN \cite{ren2016faster}.

\textbf{Results on night-rainy conditions.}
The night-rainy presents a particularly challenging situation. Low visibility and complex lighting significantly impacted the color-dependent model, while the rain further diminished visual clarity. 
\cref{tab:my_label4} illustrates that our method significantly outperforms other approaches in automobile category detection, particularly in bus detection, with improvements of 7.2\%, 3\%, and 6\% compared to S-DGOD \cite{wu2022single}, CLIP-Gap \cite{vidit2023clip} and Prompt-D \cite{li2024prompt}, respectively. 
\cref{fig:enter-label3} illustrates that our method can recognize objects more accurately in harsh environments, highlighting its advantages in enhancing the robustness of object detection.
%------------------------------------------------------------------------

\begin{table}
    \centering
    \setlength{\tabcolsep}{5.8pt}
     \fontsize{7.89}{12}\selectfont\begin{tabular}{@{  }c|p{0.8cm}@{}p{0.8cm}@{}p{0.75cm}@{}c@{  }c@{  }c@{  }c@{  }|c@{  }}
    \bottomrule
     Methods & Bus & Bike & Car &Motor & Person & Rider &Truck & mAP \\
        \hline
           FR \cite{ren2016faster} & 36.9 & 22.8 & 65.5 & 10.3 & 22.3 & 15.6 & 39.8 & 30.5 \\
           SW \cite{pan2019switchable} &35.2 &16.7 &50.1 &10.4 &20.1 &13.0 &38.8 &26.3 \\
           IBN-Net \cite{pan2018two} &37.0 &14.8 &50.3 &11.4 &17.3 &13.3 &38.4 &26.1 \\
           IterNorm \cite{huang2019iterative} &32.9 &14.1 &38.9 &11.0 &15.5 &11.6 &35.7 &22.8 \\
           ISW \cite{choi2021robustnet} &34.7 &16.0 &50.0 &11.1 &17.8 &12.6 &38.8 &25.9 \\
         S-DGOD \cite{wu2022single} &37.1 &19.6 &50.9 &13.4 &19.7 &16.3 &40.7 &28.2\\
         CLIP-Gap \cite{vidit2023clip} &37.8 &22.8 &60.7 &16.8 &26.8 &18.7 &42.4 &32.3 \\
         Prompt-D \cite{li2024prompt}&39.4 &25.2 &60.9 &\textbf{20.4} &29.9 &16.5 &43.9 &33.7 \\
        \hline
        \hline
         Ours&\textbf{41.6}  &\textbf{26.3}  &\textbf{66.6}  &16.6  &\textbf{30.9}  &\textbf{21.9}  &\textbf{44.1}  &\textbf{35.4} \\
         \toprule
    \end{tabular}
    \caption{The quantitative results (\%) on the dusk-rainy.}
    \label{tab:my_label3}
\end{table}
%------------------------------------------------------------------------
\textbf{Results on daytime-foggy conditions.}
As shown in \cref{tab:my_label5}, our method demonstrates superior generalization performance under daytime-foggy conditions. Additionally, \cref{fig:enter-label4} illustrates that our method achieves higher accuracy in detecting small figure targets in foggy environments compared to Faster R-CNN \cite{ren2016faster}.

\textbf{Results on dusk-rainy conditions.}
As shown in \cref{tab:my_label3}, our method demonstrates exceptional performance across all categories, particularly in the Bus, Car, and Truck categories. Furthermore, the overall performance improves by 3.1\% and 1.7\% compared to the CLIP-Gap \cite{vidit2023clip} and Prompt-D \cite{li2024prompt}, the most recent methods, respectively. The robustness of our method in handling challenging environments is also evident. The visual analysis presented in \cref{fig:enter-label4} illustrates the diversity and invariance of the equilibrium domain, which contributes to effective model training.
%------------------------------------------------------------------------
\begin{table}
    \centering
    \fontsize{9}{15}\selectfont\begin{tabular}{c@{  }|ccc@{}|ccc}
    \bottomrule
         Methods& $\mathcal L_\mathcal C+\mathcal L_\mathcal H$&  $\mathcal L_\mathcal{FD}$&  $\mathcal L_\mathcal{WAM}$&  DC&  DF& NS\\
         \hline
         \multirow{6}{*}{FR \cite{ren2016faster}}%%合并六行
         &  &  &  &54.7  &32.2  &34.0 \\
         &  $\checkmark$&  &  &55.8  &37.1  &39.3 \\
         &  &$\checkmark$  &  &54.1  &37.6  &39.4 \\
         &  $\checkmark$&  $\checkmark$&  &55.9  &38.0  &39.7 \\
         &  &  &  $\checkmark$&  \textbf{56.5}&  37.7& 40.3\\
         &  $\checkmark$&  $\checkmark$&  $\checkmark$&  56.1&  \textbf{39.3}& \textbf{42.0}\\
    \toprule
    \end{tabular}
    \caption{Ablation study. Here, DC, DF, and NS stand for \textbf{daytime-clear}, \textbf{daytime-foggy} and \textbf{night-sunny}, respectively.}
    \label{tab:my_label6}
\end{table}
%------------------------------------------------------------------------
\subsection{Ablation Study}
To assess the influence of each component on the performance of DIDM, we performed an ablation study. The model was trained utilizing the daytime-clear dataset and subsequently evaluated on both the daytime-foggy and night-sunny. Through a comprehensive analysis of the individual contributions of DLM and WAM, we confirmed the efficacy of both modules in the context of object detection. Furthermore, we illustrated the synergistic effect achieved by integrating these two modules, which significantly improves the model's generalization capabilities and detection accuracy. The conclusive results are detailed in \cref{tab:my_label6}.

The first component is the DLM, as illustrated in \cref{tab:my_label6}. The model's performance can be significantly improved by utilizing $\mathcal L_\mathcal H$ and $\mathcal L_\mathcal{FD}$, respectively. This suggests that either constraining the semantic information of domain-specific features or preserving the diversity of these features can enhance the model's detection capabilities. Furthermore, the model's performance is further augmented by employing both $\mathcal L_\mathcal H$ and $\mathcal L_\mathcal{FD}$. This demonstrates that the DLM component effectively enhances the model's ability to extract semantic information from images. In \cref{tab:my_label6}, we analyze the role of the WAM and observe that the model's generalization performance improves by 5.5\% and 6.3\% for daytime-foggy and night-sunny conditions, respectively, compared to the baseline Faster R-CNN \cite{ren2016faster}. Additionally, the model's test map achieves a score of 56.5\%. This indicates that the WAM enhances the model's ability to learn domain-invariant features while maintaining a degree of diversity in domain-specific features by incorporating loss weights.

Ultimately, we integrated the DLM and WAM components and found that the model's generalization performance was significantly improved while maintaining its testing performance to a certain extent. This indicates that the synergy between DLM and WAM effectively balance between the diversity of domain-specific and invariance cross domains. This balance enhances the model's learning and feature extraction capabilities while ensuring stable detection performance across various environments.
%------------------------------------------------------------------------
\begin{figure}
  \centering
  \begin{subfigure}{0.49\linewidth}
  \includegraphics[width=1\linewidth]{65.png}
    \caption{Analysis of $\alpha$ when $\gamma$ is 0.45}
    \label{fig:short-a}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.49\linewidth}
  \includegraphics[width=1\linewidth]{64.png}
    \caption{Analysis of $\gamma$ when $\alpha$ is 0.45}
    \label{fig:short-b}
  \end{subfigure}
  \caption{Hyperparameter Analysis.}
  \label{fig:short}
\end{figure}
%------------------------------------------------------------------------
\subsection{Hyperparameter Analysis}

Appropriate hyperparameter settings are essential for optimizing model performance. For the parameters $\alpha$ and $\gamma$ in \cref{eq:pythagoras10}, we experimented with various hyperparameter configurations to identify the optimal settings. As illustrated in \cref{fig:short-a,fig:short-b}, the model demonstrates the best performance when $\alpha$ =0.45 and $\gamma$ = 0.45.

\section{Conclusion}
\label{sec:conclusion}
Existing domain-generalized object detection models (S-DGOD) primarily focus on learning domain-invariant features to mitigate the negative impact of domain bias on the model's generalization ability. However, this approach often overlooks the inherent differences between various domains and may complicate the training process and lead to a loss of valuable information. To address this issue, we propose a Diversity Invariant Detection Model (DIDM) in this paper. This approach aims to balance domain invariance and domain diversity, thereby enhancing the model's feature extraction capabilities. In DIDM, we propose to introduce the Diversity Learning Module (DLM) and Weighted Aligning Module (WAM) for the domain-specific feature and domain invariance, respectively. For DLM, the feature diversity (FD) loss is implemented with entropy maximization loss to eliminate the semantic information while keeping the feature diversity. Additionally, a Weighted Aligning Module (WAM) is incorporated to prevent the overemphasis on the feature alignment with loss weight. With the combination of the DLM and WAM, the DIDM can efficiently manage the domain-specific and domain-invariance when preserving the feature diversity, resulting in improved detection performance across multiple domains. This strategy not only enhances the model's adaptability to domain variations but also ensures stable detection performance in diverse environments. Both experimental data and comprehensive analysis validate the effectiveness of DIDM.

%-------------------------------------------------------------------------
%-------------------------------------------------------------------------

{
    \small
    \bibliographystyle{ieeenat_fullname}
    \bibliography{main}
}

% WARNING: do not forget to delete the supplementary pages from your submission 
% \input{sec/X_suppl}

\end{document}
