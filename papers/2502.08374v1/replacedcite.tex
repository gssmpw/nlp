\section{Related Works}
\subsection{Adversarial Attacks}

\subsubsection{Digital Attacks}

In the study of adversarial attacks, Digital Attacks are the most basic and widely used method. They mainly add small but intentional perturbations to the input data of machine learning models, especially deep neural networks. Early representative work such as the Fast Gradient Sign Method (FGSM) proposed by Goodfellow et al____. By adding small-amplitude perturbations along the gradient direction to the original samples, it misleads the model to produce wrong predictions. Subsequent research has developed a series of more sophisticated and powerful attack methods, such as AdaBelief FGSM (AB-FGSM)____, Projected Gradient Descent (PGD)____, etc., which further optimize the perturbation. The addition method improves the concealment and attack success rate of adversarial samples.

\subsubsection{Adding and Droping of Perturbation}

The method of adding and removing perturbations is a typical strategy for generating adversarial examples. The DeepFool____ attack generates adversarial examples by minimizing the amount of perturbation that needs to be added from the original sample to the decision boundary. In contrast, the Universal Perturbation method____ focuses on finding a universal perturbation vector that can produce adversarial effects on a large number of samples. In addition, the generation of adversarial samples can also be achieved by removing certain key information. For example, AdvDrop____ generates high-quality adversarial samples by discarding high-frequency parts of the image. Together, these methods reveal the flexibility of adversarial attacks in adding and removing perturbations and diversity.

\subsubsection{Covert Attacks}

Covert Attacks focus on generating adversarial samples that are difficult to detect or even imperceptible to bypass existing defense algorithms. For example, Semantic Attacks____ focus on generating adversarial examples while keeping the semantics of the samples unchanged, making it difficult for humans to detect the changes. Additionally, SSAH____ is a novel algorithm that attacks semantic similarities on feature representations.

\subsection{Information Swapping}

\subsubsection{Wavelet Decomposition}

Wavelet decomposition is a traditional image processing method that can decompose images into feature details at different scales and directions in the frequency domain. By performing adversarial perturbation operations in the wavelet domain, the attacker is able to embed covert adversarial information while preserving the overall structure of the original image____. Generating the adversarial examples in this way can effectively mislead the perception model while the examples are visually close to the original examples____.

\subsubsection{Residual Dense Network}

Residual Dense helps the network learn the difference between the input and the expected output when constructing adversarial samples____. The goal of network design is to efficiently construct adversarial samples while retaining some of the discriminative features of the source image and injecting semantic features unique to the target category.

\subsubsection{Invertible Block}

Invertible neural networks (INNs) have garnered significant attention due to their ability to construct reversible modules for steganographic purposes____. With INNs, researchers can hide host image information whthin container image files, and even hide information from adversarial samples to mislead detection algorithms. For instance, ____ apply invertible block to adversarial attacks, the perturbations can deceive deep learning models, leading to misclassification or erroneous outputs.


%%%%%%%%%%%%%%%%%%%% 本文方法 %%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure*}[htbp!]
\centering
\includegraphics[width=16 cm]{pic/2_structure.png}
\caption{Overview of the proposed adversarial attack method.}
\label{2_structure}
\vspace{-6 mm} 
\end{figure*}