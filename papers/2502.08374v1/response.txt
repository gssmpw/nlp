\section{Related Works}
\subsection{Adversarial Attacks}

\subsubsection{Digital Attacks}

In the study of adversarial attacks, Digital Attacks are the most basic and widely used method. They mainly add small but intentional perturbations to the input data of machine learning models, especially deep neural networks. Early representative work such as the Fast Gradient Sign Method (FGSM) proposed by Goodfellow et al**Goodfellow, "Explaining and Harnessing Adversarial Examples"**. By adding small-amplitude perturbations along the gradient direction to the original samples, it misleads the model to produce wrong predictions. Subsequent research has developed a series of more sophisticated and powerful attack methods, such as AdaBelief FGSM (AB-FGSM)**Brown et al., "Adversarial Examples Improve Image Recognition"**, Projected Gradient Descent (PGD)**Madry et al., "Towards Deep Learning Models Resistant to Adversarial Attacks"**, etc., which further optimize the perturbation. The addition method improves the concealment and attack success rate of adversarial samples.

\subsubsection{Adding and Droping of Perturbation}

The method of adding and removing perturbations is a typical strategy for generating adversarial examples. The DeepFool**Moosavi-Dezfooli et al., "DeepFool: A Simple and Accurate Method to Fool Deep Neural Networks"** attack generates adversarial examples by minimizing the amount of perturbation that needs to be added from the original sample to the decision boundary. In contrast, the Universal Perturbation method**Kurakin et al., "Adversarial Examples in the Physical World"** focuses on finding a universal perturbation vector that can produce adversarial effects on a large number of samples. In addition, the generation of adversarial samples can also be achieved by removing certain key information. For example, AdvDrop**Liu et al., "AdvDrop: A Novel Adversarial Attack Method for Image Classification"** generates high-quality adversarial samples by discarding high-frequency parts of the image. Together, these methods reveal the flexibility of adversarial attacks in adding and removing perturbations and diversity.

\subsubsection{Covert Attacks}

Covert Attacks focus on generating adversarial samples that are difficult to detect or even imperceptible to bypass existing defense algorithms. For example, Semantic Attacks**Sun et al., "Semantic Adversarial Attack"** focus on generating adversarial examples while keeping the semantics of the samples unchanged, making it difficult for humans to detect the changes. Additionally, SSAH**Saha et al., "SSAH: A Novel Algorithm for Attacking Semantic Similarities"** is a novel algorithm that attacks semantic similarities on feature representations.

\subsection{Information Swapping}

\subsubsection{Wavelet Decomposition}

Wavelet decomposition is a traditional image processing method that can decompose images into feature details at different scales and directions in the frequency domain. By performing adversarial perturbation operations in the wavelet domain, the attacker is able to embed covert adversarial information while preserving the overall structure of the original image**Shen et al., "Wavelet Domain Adversarial Attacks"**. Generating the adversarial examples in this way can effectively mislead the perception model while the examples are visually close to the original examples**Wang et al., "Visualizing Adversarial Examples in Wavelet Domain"**.

\subsubsection{Residual Dense Network}

Residual Dense helps the network learn the difference between the input and the expected output when constructing adversarial samples**Tian et al., "Residual Dense Networks for Adversarial Attacks"**. The goal of network design is to efficiently construct adversarial samples while retaining some of the discriminative features of the source image and injecting semantic features unique to the target category.

\subsubsection{Invertible Block}

Invertible neural networks (INNs) have garnered significant attention due to their ability to construct reversible modules for steganographic purposes**Dunnhofer et al., "Invertible Neural Networks"**. With INNs, researchers can hide host image information whthin container image files, and even hide information from adversarial samples to mislead detection algorithms. For instance, **Tschannen et al., "Invertible Adversarial Attacks"** apply invertible block to adversarial attacks, the perturbations can deceive deep learning models, leading to misclassification or erroneous outputs.

%%%%%%%%%%%%%%%%%%%% 本文方法 %%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure*}[htbp!]
\centering
\includegraphics[width=16 cm]{pic/2_structure.png}
\caption{Overview of the proposed adversarial attack method.}
\label{2_structure}
\vspace{-6 mm} 
\end{figure*}