\section{Test Framework Setup} \label{sec:test_framework_setup}


Figure \ref{fig:overview_test_framework} provides an overview of this framework. It is implemented with around 7800 lines of optimized C++ code. The source code is compiled using g++ 10.5.0 with the -O3 optimization enabled. Experiments are conducted on a Linux server equipped with an AMD EPYC 7543 CPU (32 cores) and 128GB of memory.



\subsection{Benchmark Platform} \label{sec:benchmark_platform}


\vspace{2pt}
\noindent\textbf{Third-Party Modules.} This module integrates the original implementations of LLAMA~\cite{llamacodebase}, Aspen~\cite{aspencodebase}, LiveGraph~\cite{livegraphcodebase}, Teseo~\cite{teseocodebase}, and Sortledton~\cite{sortledtoncosebase}  from GitHub. Due to differences in their APIs, we implemented a customized adapter, a wrapper for each method, to standardize the evaluation of graph operations. The overhead of these adapters is negligible because they simply combine the interfaces to provide these functions, if not directly available. Each method is configured with its recommended settings. Specifically, the size of bloom filter in LiveGraph is set as $\frac{1}{16}$ of the block size. Sortledton, Teseo and Aspen set the block size to 256.



\vspace{2pt}
\noindent\textbf{DGS Sandbox.} In this module, we re-implement key techniques from competing methods within our abstraction for a fair and detailed investigation of individual techniques. These techniques can be composed differently to evaluate graph operations based on the unified execution routines shown in Figure \ref{fig:primitive_opeartions}. For fine-grained methods, we apply the following optimizations: 1) all methods use G2PL as the concurrency control protocol, and 2) all methods use a dynamic array as the vertex index by default. We implement a simple baseline DGS method by combining the static graph storage AdjLst with G2PL, naming it AdjLst. \sun{Specifically, AdjLst is implemented as an array where each element represents a vertex, and each vertex points to an array storing its neighbor set. When inserting a new element, a binary search is performed to find the correct position, after which the subsequent elements are shifted to make space for the new one. If the array is full, pre-allocated space is used, functioning like a dynamic array.} For comparison purposes, we also include CSR, the optimal baseline for static graphs.





\subsection{Test Driver}

\vspace{2pt}
\noindent\textbf{Workload Generator.} Table \ref{tab:datasets} presents the statistics of the real-world graphs used in the paper. They span six categories, with $|V|$ ranging from tens of thousands to tens of millions and $|E|$ scaling to hundreds of millions.  Both \emph{ldbc}~\cite{ldbcic} and \emph{nft}~\cite{Zhang2023LiveGL, livegraphlab} originally have timestamps to mark the insertion sequence of edges. \emph{ldbc} simulates actions in a social network. We set the scale factor to 10 to control the graph size. \emph{nft} records NFT transactions on Ethereum from 2017 to 2022. Other graphs are obtained from SNAP~\cite{snapnets} and do not include timestamps. These graphs are widely used in DGS research. We also considered larger graphs (e.g., \emph{friendster}) containing billions of edges but omitted them since existing DGS methods frequently run out of memory on these cases.
\small
\begin{table}[h]
% \setlength{\abovecaptionskip}{0pt}
% \setlength{\belowcaptionskip}{0pt}
\captionsetup{skip=0pt} 
\centering
\caption{The detailed information of the real-world graphs.}
\label{tab:datasets}
% \resizebox{0.45\textwidth}{!}{%
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
\textbf{Category}                & \textbf{Dataset}     & \textbf{Name} & \textbf{|\textit{V}|} & \textbf{|\textit{E}|} & \textit{\textbf{$d_{avg}$}} & $d_{max}$ \\ \hline
\multirow{4}{*}{\textbf{Social}} & \textbf{LiveJournal} & \emph{lj}       & 4.8M                  & 42.8M                 & 8.8                         & 20233     \\ \cline{2-7} 
                   % & \textbf{LDBC}       & \emph{ldbc} & 9.3M  & 52.6M  & 5.6   & 1346287 \\ \cline{2-7} 
                   & \textbf{LDBC}       & \emph{ldbc} & 30.0M  & 175.9M  & 5.9   & 4282595 \\ \cline{2-7} 
                   & \textbf{Twitter}    & \emph{tw}   & 21.3M & 265.0M & 12.4  & 698112 \\ \hline
                   % & \textbf{Orkut}      & \emph{ok}   & 3.1M  & 117.2M & 38.14 & 33313   \\ \hline
\textbf{Game}      & \textbf{DotaLeague} & \emph{dl}   & 0.06M & 50.9M  & 831.6 & 17004   \\ \hline
\textbf{Web}       & \textbf{Wiki}       & \emph{wk}   & 14.0M & 59.0M  & 4.2   & 723404  \\ \hline
\textbf{Citation}  & \textbf{Cit}        & \emph{ct}   & 3.8M  & 16.5M  & 4.4   & 793     \\ \hline
\textbf{Synthetic} & \textbf{Graph500}   & \emph{g5}   & 8.9M  & 260.4M & 29.3  & 406416  \\ \hline
\textbf{Financial} & \textbf{NFT}   & \emph{nft}   & 29.6M  & 77.5M &  2.62 &  2290853 \\ \hline
\end{tabular}%
% }%
\end{table}

We generate three types of graph queries. First, the \emph{micro OP stream} contains a sequence of graph operations. For graphs with timestamps, we generate an \textsc{InsEdge} stream using the first 80\% of edges as the initial graph and the remaining 20\% as the insert edges. For graphs without timestamps, we shuffle the edges and generate the insert stream similarly, following previous works~\cite{zhu2019livegraph,de2021teseo,fuchs2022sortledton}. As these works focus on single updates, each operation corresponds to a write query $\Delta G$. For the \textsc{SearchEdge} stream, we randomly select 20\% of edges as the search targets. For the \textsc{ScanNbr} stream, we select 20\% of vertices based on their degrees. Each of these operations is a read query. The micro OP stream serves two purposes: 1) Investigating the performance of competing methods on basic graph operations, and 2) Studying the effectiveness of short queries (i.e., IC workloads).

Second, we integrate four representative \emph{graph analytic} algorithms from GAPBS~\cite{beamer2015gap}: PR (PageRank), BFS, SSSP, and WCC, which cover different graph data access patterns. PR sequentially accesses both vertices and neighbors. BFS and WCC visit neighbors sequentially while accessing vertices randomly. SSSP introduces a random access pattern to neighbors when retrieving weights. Third, we implement TC (triangle counting) as the representative \emph{graph pattern matching} query. This query requires DGS to support quick scanning in sorted order for fast set intersections. In summary, these two types of queries evaluate the effectiveness of complex, long-running graph queries (i.e., BI and IS workloads).

Real-world graphs following a power-law degree distribution complicate examining the performance of graph operations on different neighbor set sizes because accessing frequently used sets of elements can improve cache performance and lead to biased results. To address this, we design experiments using synthetic datasets. A \emph{synthetic dataset} consists of sets of elements with uniform sizes. Each element is a vertex with an ID ranging from $[0, 2^{22})$. Assuming each vertex ID is 8 bytes, to evaluate performance on a neighbor set with 512 elements, we generate $x = \frac{8 \text{GB}}{512 \times 8 \text{ bytes}}$ sets of the same size. These sets are labeled from $[0, x)$. We generate insert, scan, and search OP streams using the same strategy as for real-world graphs, treating set IDs as vertex IDs and the sets as neighbor sets. We default to using 8GB to prevent all sets from residing in the cache, thereby simulating random memory access in large graphs.



\vspace{2pt}
\noindent\textbf{Workload Executor.} Each thread executes a stream or a graph algorithm. Operations within the same stream execute sequentially by one thread, while multiple threads can execute concurrently. Although the graph algorithms in the test framework can run in parallel, we execute them in a single thread to focus on the DGS's capability to empower concurrent graph query execution and handle different queries.

\vspace{2pt}
\noindent\textbf{Performance Monitor.} We use \emph{throughput}, the number of edges processed (insert, search, scan) per second, to measure DGS efficiency. We use \emph{latency}, the time elapsed to complete one query, to examine service quality. Recording the latency of each operation incurs non-trivial overhead due to the short duration of single graph operations. Therefore, we record latency every one hundred micro-operations. \emph{Scalability} is measured by the throughput of micro-operations as the number of threads increases, while \emph{concurrency} is measured when multiple micro-operation streams (or graph algorithm queries) are mixed. \emph{Memory cost} tracks the memory consumption of competing storage methods.
