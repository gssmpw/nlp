\section{DGS Methods Under Study}\label{sec:dgs_methods_under_study}

 
Existing DGS methods focus on optimizing two problems: 1) graph concurrency control, which includes version management and concurrency control protocols to coordinate the execution of concurrent graph queries, and minimizing overhead for each graph operation (i.e., $T_{CC}$ and $\alpha_p$ in Equation \ref{eq:cost}); and 2) graph containers, which include vertex indexes, neighbor indexes, and other optimizations to optimize graph data access $T_p$ for each operation in Equation \ref{eq:cost}. Table \ref{tab:summary_methods} summarizes the key design choices in these methods. In the following, we first briefly introduce these methods and then compare them.

% Please add the following required packages to your document preamble:
% \usepackage{graphicx}
\small
\begin{table}[t]
\centering
\caption{A summary of DGS methods under study.}
\label{tab:summary_methods}
% \resizebox{\columnwidth}{!}{%
\begin{tabular}{|c|clc|ccc|}
\hline
\textbf{}           & \multicolumn{3}{c|}{\textbf{Graph Concurrency Control}}                                                                                                       & \multicolumn{3}{c|}{\textbf{Graph Container}}                                                                                                                                                                                                                     \\ \hline
\textbf{Method}     & \multicolumn{2}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}Version\\ Management\end{tabular}}}          & \textbf{Protocol}                                       & \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}Vertex\\ Index\end{tabular}}} & \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}Neighbor\\ Index\end{tabular}}} & \textbf{\begin{tabular}[c]{@{}c@{}}Additional\\ Optimization\end{tabular}}          \\ \hline
\textbf{LiveGraph}  & \multicolumn{2}{c|}{\begin{tabular}[c]{@{}c@{}}Fine-Grained with\\ Continuous Version\end{tabular}} & S2PL                                                    & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Dynamic\\ Array\end{tabular}}         & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Dynamic\\ Array\end{tabular}}           & \begin{tabular}[c]{@{}c@{}}Bloom\\ Filter\end{tabular}                            \\ \hline
\textbf{Sortledton} & \multicolumn{2}{c|}{\begin{tabular}[c]{@{}c@{}}Fine-Grained with\\ Version Chain\end{tabular}}      & G2PL                                                    & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Dynamic\\ Array\end{tabular}}         & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Segmented\\ Skip List\end{tabular}}     & \begin{tabular}[c]{@{}c@{}}Adaptive\\ Indexing\end{tabular}                       \\ \hline
\textbf{Teseo}      & \multicolumn{2}{c|}{\begin{tabular}[c]{@{}c@{}}Fine-Grained with\\ Version Chain\end{tabular}}      & OCC                                                     & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Hash\\ Table\end{tabular}}            & \multicolumn{1}{c|}{PMA}                                                               & \begin{tabular}[c]{@{}c@{}}Write-Optimized\\ Segment\end{tabular}                 \\ \hline
\textbf{Aspen}      & \multicolumn{2}{c|}{Coarse-Grained}                                                                 & \begin{tabular}[c]{@{}c@{}}Single\\ Writer\end{tabular} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}AVL\\ Tree\end{tabular}}              & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Segmented\\ PAM\end{tabular}}           & \begin{tabular}[c]{@{}c@{}}Vertex Index\\ Flatten \&\\ Data Encoding\end{tabular} \\ \hline
\textbf{LLAMA}      & \multicolumn{2}{c|}{Coarse-Grained}                                                                 & \begin{tabular}[c]{@{}c@{}}Single\\ Writer\end{tabular} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Dynamic\\ Array\end{tabular}}         & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Dynamic\\ Array\end{tabular}}           & -                                                                                 \\ \hline
\end{tabular}%
% }
\end{table}


\subsection{A Brief Introduction to DGS Methods}

\subsubsection{\textbf{LiveGraph}~\cite{zhu2019livegraph}}

\textbf{Graph Concurrency Control.} LiveGraph uses a lock for each vertex in $V(G)$ and adapts S2PL to synchronize data access. For a write query $\Delta G$, LiveGraph first obtains all exclusive locks on vertices in $\Delta V$ (vertices involved in $\Delta G$), performs the graph operations, and then releases the locks. To handle deadlocks, LiveGraph aborts $\Delta G$ if it cannot acquire a lock within a time limit.

For each version of a neighbor or vertex, LiveGraph keeps begin and end timestamps ($begin-ts$ and $end-ts$) to record its lifetime as shown in Figure \ref{fig:livegraph_index}. Given $\Delta G$ committed at timestamp $i$, \textsc{InsEdge($u, v$)} searches for the latest version of $v$ in $N(u)$. If found, it sets $end-ts$ to $i$ and creates a new version of $v$ with $begin-ts = i$ and $end-ts = INF$. Otherwise, it directly creates a new version of $v$. \textsc{DelEdge($u, v$)} searches for the latest version of $v$ and sets $end-ts$ to $i$. For read operations in $Q$, LiveGraph obtains a shared lock on the vertex and immediately releases it after the operation. For example, \textsc{ScanNbr($u$)} acquires the lock on $u$, accesses neighbors based on timestamps, and releases the lock immediately. Thus, $Q$ never leads to deadlocks because it never holds two locks simultaneously.

\begin{figure}[h]\small
    \setlength{\abovecaptionskip}{3pt}
    \setlength{\belowcaptionskip}{0pt}
    \includegraphics[scale=0.75]{img/livegraph_neighbor_index.pdf}
    \centering
    \caption{The neighbor index of $N(u_2)$ in LiveGraph.}
    \label{fig:livegraph_index}
\end{figure}

\noindent\textbf{Graph Container.} Given $u \in V(G)$, LiveGraph uses a dynamic array (DA) as the neighbor index of $N(u)$ where each element corresponds to a physical version of $v \in N(u)$. Graph operations in Figure \ref{fig:primitive_opeartions} are based on primitive operators of DA, the time complexity of which is listed in Table \ref{tab:complexity_data_structure}. As the storage of DA is continuous and no version chain exists, \textsc{Scan} is very fast. Moreover, LiveGraph executes \textsc{Scan} from the end to the beginning of DA since the latest element may be more frequently visited than the stale ones. However, \textsc{Search} is slow because DA is unsorted and uses \textsc{Scan} to perform the search. Consequently, \textsc{InsEdge} is slow because it depends on \textsc{SearchEdge} though adding an element only requires a simple append. To mitigate this issue, LiveGraph maintains a Bloom filter~\cite{mitzenmacher2001compressed} for each $N(u)$ to record whether an element exists in $N(u)$. LiveGraph uses DA as the vertex index of $V(G)$ as well. As the vertex ID is ranged in $[0, |V|)$, the element at the index $u$ is the vertex $u$. Therefore, the time complexity of \textsc{Search} on the vertex index is $O(1)$. As the implementation is simple, we omit the details.

\subsubsection{\textbf{Sortledton}~\cite{fuchs2022sortledton}}

\textbf{Graph Concurrency Control.} Sortledton also uses S2PL but optimizes the locking sequence: Sort the vertices in $\Delta V$ by ascending vertex IDs and acquire their exclusive locks in that order. This optimization prevents deadlocks by avoiding circular waiting among write queries~\cite{silberschatz1991operating}. Therefore, Sortledton does not need any mechanisms to handle deadlocks. For \textsc{InsEdge($u, v$)} (or \textsc{DelEdge($u, v$)}) in $\Delta G$ committed at timestamp $i$, Sortledton creates a new version of $v$ with timestamp $i$ and \textsc{op-type} as $I$ (or $D$) as shown in Figure \ref{fig:sortledton_neighbor_index}. Sortledton maintains a version chain for different versions of $v$, where the new version points to the old one. For read queries, Sortledton uses the same concurrency control strategy as LiveGraph.


\begin{figure}[h]\small
    \setlength{\abovecaptionskip}{3pt}
    \setlength{\belowcaptionskip}{0pt}
    \includegraphics[scale=0.75]{img/sortledton_neighbor_index.pdf}
    \centering
    \caption{The neighbor index of $N(u_2)$ in Sortledton.}
    \label{fig:sortledton_neighbor_index}
\end{figure}

\noindent\textbf{Graph Container.} Like LiveGraph, Sortledton uses a dynamic array as the vertex index. For the neighbor index, Sortledton splits $N(u)$ into blocks $B$, and uses the skip list as the block index linking them. The fill ratio of each block is maintained between 50\% and 100\%. When a block is full, Sortledton splits it into two blocks, equally distributing the neighbors. If the fill ratio drops below 50\%, Sortledton merges it with adjacent blocks. The first element of each block serves as its key in the skip list. We call this structure the \emph{segmented skip list}. A read operation traverses the version chain to find the target version. Since real-world graphs are sparse, Sortledton proposes the \emph{adaptive neighbor index}: If $|N(u)|$ is below a threshold (e.g., 256), it uses a sorted dynamic array as the neighbor index instead of the segmented skip list.

\subsubsection{\textbf{Teseo}~\cite{de2021teseo}}

\noindent\textbf{Graph Concurrency Control.} Teseo uses the same version management method as Sortledton but adopts optimistic concurrency control (OCC introduced in Section \ref{sec:preliminaries}) to coordinate concurrent write queries. Unlike LiveGraph and Sortledton, which keep a lock for each vertex, Teseo maintains a lock for each edge partition to synchronize concurrent data access. Specifically, Teseo logically divides $E(G)$ into equally sized partitions, each with a lock. Before accessing data in a partition, a write query acquires an exclusive lock (or a read query acquires a shared lock) on the partition and releases it immediately after access.


\begin{figure}[h]\small
    % \setlength{\abovecaptionskip}{3pt}
    % \setlength{\belowcaptionskip}{0pt}
    \includegraphics[scale=0.75]{img/teseo_neighbor_index.pdf}
    \centering
    \caption{The neighbor index of $N(u_2)$ in Teseo.}
    \label{fig:teseo_neighbor_index}
\end{figure}


\noindent\textbf{Graph Container.} Teseo employs the packed memory array (PMA)~\cite{bender2007adaptive,de2019packed} as the neighbor index as shown in Figure \ref{fig:teseo_neighbor_index}. But it stores all neighbor tables in a single PMA. If $N(u)$ is large, it spans multiple blocks in the PMA, while multiple small neighbor sets can share a single block. However, the global rebalance overhead is high for a PMA if $E(G)$ is stored together. To address this, Teseo divides the single PMA into multiple large leaves (several megabytes each) and indexes these leaves with an \emph{adaptive radix tree} (ART)~\cite{leis2013adaptive}, calling this data structure FAT. Additionally, Teseo uses a hash table as the vertex index to record the position of each vertex's neighbor index in FAT. By default, blocks in FAT are sorted, known as read-optimized segments. When the insertion rate is high, FAT switches to write-optimized segments (WOS), which handle updates by appending to the update log. For WOS, Teseo loops over the segment to find the target vertex. Teseo is built on top of HyPer~\cite{kemper2011hyper}.

\subsubsection{\textbf{Aspen}~\cite{dhulipala2019low}}

\textbf{Graph Concurrency Control.} Aspen uses a coarse-grained strategy that maintains timestamps for each graph snapshot. Specifically, Aspen uses the \emph{single-writer-multiple-reader} scheme, which executes write queries serially and allows multiple readers to execute concurrently. A write query $\Delta G_i$ uses the \emph{copy-on-write} (CoW) method (also called shadow paging) to apply updates on a copy of the graph, creating a new snapshot $G_i$ as shown in Figure \ref{fig:aspen_neighbor_index}. $Q$ works on the latest version of the graph snapshot. Therefore, read and write queries never block each other, and multiple read queries can share the same graph snapshot.

\begin{figure}[h]\small
    \setlength{\abovecaptionskip}{3pt}
    \setlength{\belowcaptionskip}{-5pt}
    \includegraphics[scale=0.75]{img/aspen_neighbor_index.pdf}
    \centering
    \caption{The neighbor index of $N(u_2)$ in Aspen.}
    \label{fig:aspen_neighbor_index}
\end{figure}

\vspace{2pt}
\noindent\textbf{Graph Container.} Aspen partitions $N(u)$ into a set of sorted blocks and uses a parallel augmented map (PAM) to index these blocks. These blocks have no empty slots. \textsc{InsEdge($u, v$)} copies the block, inserts $v$, and then copies the path from the block to the root to create a new snapshot of $N(v)$. Given $N(u)$ and block size $B$, Aspen selects vertices such that $v \mod b = 0$ as heads, i.e., $heads = {v \in N(u) \mid v \mod b = 0}$. This approach ensures that updates to one block do not affect adjacent blocks. Aspen demonstrates that this segmentation ensures each block has $B$ elements with high probability. Moreover, Aspen uses an AVL tree as the vertex index and copies the path in the tree for each update operation.

Aspen proposes two optimization methods to enhance performance. First, for long-running read queries, Aspen can create an array storing the positions of each neighbor table based on the AVL tree to eliminate the overhead of \textsc{Search} in the AVL tree. Second, Aspen uses a difference encoding scheme to compress data in a block. For a block containing $(v0, v1, v2, \ldots)$, Aspen stores it as $(v0, v1 - v0, v2 - v0, \ldots)$ and compresses it with byte codes to accelerate set intersections~\cite{aberger2017emptyheaded}.

\subsubsection{\textbf{LLAMA}~\cite{macko2015llama}}

LLAMA, proposed in 2015, also employs a coarse-grained graph concurrency control mechanism similar to that in Aspen. Specifically, LLAMA divides the vertex table into partitions, each stored in a data page, and maintains an \emph{indirection table} to store the locations of these pages. Each write query must copy the indirection table to create a new graph snapshot, and this overhead limits update performance and graph scalability. 


\subsection{Comparison of DGS Methods} \label{sec:discussion}

\noindent\textbf{Graph Containers.} We discuss the time and space cost of graph operations based on Table \ref{tab:complexity_data_structure}.

\emph{Time.} Given vertex IDs in the range $[0, |V|)$, using a dynamic array (DA) as vertex indexes allows \textsc{SearchVtx} and \textsc{InsVtx} in $O(1)$ time with simple memory accesses. Due to continuous storage, DA enables fast scan operations. In contrast, hash tables and AVL trees incur more overhead than DA, despite having the same time complexity for some operations.

Continuous storage, used in LiveGraph and LLAMA, stores $N(u)$ in DA, facilitating fast \textsc{ScanNbr} but resulting in slow \textsc{SearchEdge} due to the unsorted nature of the array. Since \textsc{InsEdge} depends on \textsc{SearchEdge} in DGS, its performance is also slow despite \textsc{Insert} in DA taking $O(1)$ time. For the segmented methods, scanning accesses data continuously within a block, while inserting typically moves only a few elements within a block. These blocks are linked by an index (e.g., PAM) to accelerate \textsc{SearchEdge}. Therefore, the cost of these operations includes the block index and the block itself. Increasing block sizes can improve scan performance due to continuous memory access but may degrade insert performance due to data movement within the block. Additionally, Aspen, using CoW, incurs more overhead for insertion than methods performing in-place updates because its insert operation copies the block as well as the root-to-leaf path.

\emph{Space.} Practical memory consumption is significantly affected by element size. Each element in Sortledton and Teseo consumes $3 \times w$ bytes, where $w$ is the word size: one for the vertex ID, one for the version, and one for the pointer. The \textsc{op-type} can use the high bit in the timestamp. Each element in LiveGraph also takes $3 \times w$ bytes. In contrast, each element in Aspen consumes only $w$ bytes due to its coarse-grained granularity. Additionally, Aspenâ€™s neighbor index has no empty slots. Therefore, the coarse-grained method is more memory efficient than the fine-grained method.

\vspace{2pt}
\noindent\textbf{Graph Concurrency Control.} We first compare fine-grained and coarse-grained strategies, and then discuss fine-grained methods.

\emph{Fine-Grained vs. Coarse-Grained.} Fine-grained methods require lock operations on each graph operation, which can lead to lock contention and thus expensive $T_{CC}$. High-degree vertices are particularly prone to frequent access. Fine-grained methods necessitate version checks on each element, resulting in increased data loading from memory and more computation for version comparison, leading to a high $\alpha_p$ value. In contrast, coarse-grained methods avoid these issues. However, fine-grained methods allow multiple writers to update the graph simultaneously and perform in-place updates by simply inserting new elements, enhancing update performance.

Additionally, the fine-grained strategy does not place special requirements on the underlying graph containers, making it more generic. In contrast, the coarse-grained strategy requires support for fast snapshot creation. However, since the coarse-grained strategy does not maintain versions or perform version checks for each element, it can be effectively combined with data compression techniques~\cite{dhulipala2019low}, which is not feasible for the fine-grained approach.

\emph{Discussion on Fine-Grained Strategies.} First, the continuous version storage in LiveGraph can improve scanning efficiency by avoiding the traversal of a version chain. However, it may increase data access volume by including stale versions, negatively impacting search and insert efficiency. Second, G2PL in Sortledton is generally the optimal fine-grained concurrency control mechanism due to its effective deadlock avoidance optimization. Although OCC in Teseo does not require holding all mutexes of vertices in $\Delta V$, write queries are typically very short because $\Delta G$ generally contains a single update. Moreover, executing deadlock detection for write-write conflicts introduces significant overhead and implementation challenges. As such, our study uses G2PL for fine-grained methods because of its advantages.

\vspace{2pt}
\noindent\textbf{Empirical Evaluation Targets.} Following the above discussion, we will set up test frameworks and empirically evaluate these techniques by addressing the following five questions.

\begin{itemize} [leftmargin=*]
    \item \textbf{Graph Containers:} \emph{\textbf{Q1.} \sun{How effective are existing techniques in graph containers at efficiently performing key operations such as \textsc{SearchEdge}, \textsc{InsEdge}, and \textsc{ScanNbr}, as defined by $T_p$ in Equation \ref{eq:cost}?}} \emph{\textbf{Q2.} Which neighbor index performs the best, and what is the gap between it and CSR on read queries?}
    \item \textbf{Graph Concurrency Control:} \emph{\textbf{Q3.} What is the impact of graph concurrency control on graph operations?} \emph{\textbf{Q4.} How are the scalability and concurrency of competing methods?}
    \sun{
    \item \textbf{Batch Granularity:} \emph{\textbf{Q5.} How does the batch granularity affect the performance of competing methods?}
    }
    \item \textbf{Memory Consumption:} \emph{\textbf{Q6.} What is the impact of graph containers and version management in DGS on memory consumption, and what is the gap between DGS and CSR?}
\end{itemize}

\begin{figure*}[t]\small
    \setlength{\abovecaptionskip}{0pt}
    % \setlength{\belowcaptionskip}{-13pt}
    \includegraphics[scale=0.45]{img/test_framework.pdf}
    \centering
    \caption{An overview of the test framework.}
    \label{fig:overview_test_framework}
\end{figure*}

