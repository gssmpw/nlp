% File tacl2021v1.tex
% Dec. 15, 2021
\pdfoutput=1
% The English content of this file was modified from various *ACL instructions
% by Lillian Lee and Kristina Toutanova
%
% LaTeXery is mostly all adapted from acl2018.sty.

\documentclass[11pt,a4paper]{article}
\usepackage{caption}
%% Package options:
%% Short version: "hyperref" and "submission" are the defaults.
%% More verbose version:
%% Most compact command to produce a submission version with hyperref enabled
%%    \usepackage[]{tacl2021v1}
%% Most compact command to produce a "camera-ready" version
\usepackage[acceptedWithA]{tacl2021v1}
%% Most compact command to produce a double-spaced copy-editor's version
%%    \usepackage[acceptedWithA,copyedit]{tacl2021v1}
%
%% If you need to disable hyperref in any of the above settings (see Section
%% "LaTeX files") in the TACL instructions), add ",nohyperref" in the square
%% brackets. (The comma is a delimiter in case there are multiple options specified.)

\usepackage{tacl2021v1}
% \setlength\titlebox{10cm} % <- for Option 2 below
\usepackage[inkscapelatex=false]{svg}
% Standard package includes
\usepackage{mathptmx}
\usepackage{latexsym}
\usepackage{booktabs}
\usepackage{pifont}
\usepackage{subcaption} % Add this in your preamble
\usepackage{svg}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{cleveref} % Include this in the preamble
\usepackage{tikz}
\usetikzlibrary{positioning}

\usepackage{algorithm}
\usepackage{mathrsfs}
\usepackage{xcolor}
\usepackage{adjustbox}
\usepackage{wrapfig}
\usepackage{algpseudocode}
\usepackage{tabularx}
\usepackage{mathbbol}
\usepackage{tcolorbox}
\usepackage{caption}
\usepackage{float}
\usepackage{lipsum}
\usepackage{xtab}
\usepackage{makecell}
\usepackage{xcolor}
\definecolor{lowcontrastgreen}{RGB}{160,206,86}
\definecolor{lowcontrastred}{RGB}{255,95,83}
\definecolor{lowcontrastyellow}{RGB}{255,218,93}
\definecolor{lowcontrastblue}{RGB}{181,234,215}
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets
\newcommand{\cmark}{\ding{51}}  % Checkmark
\newcommand{\xmark}{\ding{55}}  % Cross
% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}
\usepackage{boldline}
% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}
\usepackage{multicol}
\usepackage{multirow}
% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}
\usepackage{svg}
%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{listings}
\usepackage{epsfig,endnotes}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{upquote}
\usepackage[edges]{forest}

\tikzset{%
    parent/.style =          {align=center,text width=2cm,rounded corners=3pt, line width=0.3mm, fill=gray!10,draw=gray!80},
    child/.style =           {align=center,text width=2.3cm,rounded corners=3pt, fill=blue!10,draw=blue!80,line width=0.3mm},
    grandchild/.style =      {align=center,text width=2cm,rounded corners=3pt},
    greatgrandchild/.style = {align=center,text width=1.5cm,rounded corners=3pt},
    greatgrandchild2/.style = {align=center,text width=1.5cm,rounded corners=3pt},    
    referenceblock/.style =  {align=center,text width=1.5cm,rounded corners=2pt},
    %%%% Re-define
    % Pretrain Model
    unitask/.style =           {align=center,text width=2.2cm,rounded corners=3pt, fill=blue!10,draw=blue!80,line width=0.3mm},   
    pretrain_work/.style =           {align=center, text width=3cm,rounded corners=3pt, fill=blue!10,draw=blue!0,line width=0.3mm},  
    % Template Mining
    multitask/.style =           {align=center,text width=2.2cm,rounded corners=3pt, fill=red!10,draw=red!80,line width=0.3mm},   
    template_work/.style =           {align=center,text width=3cm,rounded corners=3pt, fill=red!10,draw=red!0,line width=0.3mm},    
    % Answering Mining
    answer/.style =           {align=center,text width=1.8cm,rounded corners=3pt, fill= cyan!10,draw= cyan!80,line width=0.3mm},   
    answer_work/.style =           {align=center,text width=5cm,rounded corners=3pt, fill= cyan!10,draw= cyan!0,line width=0.3mm},      
    % Multi-prompt 
    multiple/.style =           {align=center,text width=1.8cm,rounded corners=3pt, fill= orange!10,draw= orange!80,line width=0.3mm},   
    multiple_work/.style =           {align=center,text width=5cm,rounded corners=3pt, fill= orange!10,draw= orange!0,line width=0.3mm},        
    % Tuning Strategy
    tuning/.style =           {align=center,text width=1.8cm,rounded corners=3pt, fill= magenta!10,draw= magenta!80,line width=0.3mm},   
    tuning_work/.style =           {align=center,text width=5cm,rounded corners=3pt, fill= magenta!10,draw= magenta!0,line width=0.3mm},          
}

%%%% Material in this block is specific to generating TACL instructions
\usepackage{xspace,mfirstuc,tabulary}
\newcommand{\dateOfLastUpdate}{Dec. 15, 2021}
\newcommand{\styleFileVersion}{tacl2021v1}

\newcommand{\ex}[1]{{\sf #1}}

\newif\iftaclinstructions
\taclinstructionsfalse % AUTHORS: do NOT set this to true
\iftaclinstructions
\renewcommand{\confidential}{}
\renewcommand{\anonsubtext}{(No author info supplied here, for consistency with
TACL-submission anonymization requirements)}
\newcommand{\instr}
\fi

%
\iftaclpubformat % this "if" is set by the choice of options
\newcommand{\taclpaper}{final version\xspace}
\newcommand{\taclpapers}{final versions\xspace}
\newcommand{\Taclpaper}{Final version\xspace}
\newcommand{\Taclpapers}{Final versions\xspace}
\newcommand{\TaclPapers}{Final Versions\xspace}
\else
\newcommand{\taclpaper}{submission\xspace}
\newcommand{\taclpapers}{{\taclpaper}s\xspace}
\newcommand{\Taclpaper}{Submission\xspace}
\newcommand{\Taclpapers}{{\Taclpaper}s\xspace}
\newcommand{\TaclPapers}{Submissions\xspace}
\fi

%%%% End TACL-instructions-specific macro block
%%%%

\title{Embodied Intelligence for 3D Understanding: A Survey on 3D Scene Question Answering}

% Author information does not appear in the pdf unless the "acceptedWithA" option is given

% The author block may be formatted in one of two ways:

% Option 1. Author’s address is underneath each name, centered.

% \author{
%   Zechuan Li\Thanks{The {\em actual} contributors to this instruction
%     document and corresponding template file are given in Section
%     \ref{sec:contributors}.} 
%   \\
%   Hunan University
%   % \\
%   % Template Affiliation1/Address Line 2
%   % \\
%   % Template Affiliation1/Address Line 2
%   \\
%   \texttt{lizechuan@hnu.edu.cn}
%   \And
%   Hongshan Yu 
%   \\
%   Hunan University
%   \\
%   % Template Affiliation2/Address Line 2
%   % \\
%   % Template Affiliation2/Address Line 2
%   % \\
%   \texttt{lizechuan@hnu.edu.cn}
% }

% \author{
%  Zechuan Li$^{1,2}$,
%  Hongshan Yu$^{1*}$,
%  Yihao Ding$^2$, \\
%  \textbf{Yan Li}$^3$, 
%  \textbf{Yong He}$^4$, 
%  \textbf{Naveed Akhtar}$^2$\  \vspace{0.25em}\\
%   $^1$Hunan University, 
%   $^2$The University of Melbourne, \\
%   $^3$The University of Sydney,
%   $^4$Anhui University
%   % $^4$The University of Melbourne, $^5$hessian.ai
%   \\
%   \texttt{\{lizechuan,yuhongshan\}@hnu.edu.cn}, \\
%   \texttt{{yihao.ding.1@unimelb.edu.au}, \texttt{yali3816@uni.sydney.edu.au},\\
%   \texttt{naveed.akhtar1@unimelb.edu.au,h.yong@hnu.edu.cn} 
% }
\author{
  Zechuan Li$^{1,2}$, 
  Hongshan Yu$^{1*}$, 
  Yihao Ding$^2$, \\
  \textbf{Yan Li}$^3$, 
  \textbf{Yong He}$^4$, 
  \textbf{Naveed Akhtar}$^2$ \\
  $^1$Hunan University, 
  $^2$The University of Melbourne, \\
  $^3$The University of Sydney, 
  $^4$Anhui University \\
  \texttt{\{lizechuan,yuhongshan\}@hnu.edu.cn}, \\
  \texttt{yihao.ding.1@unimelb.edu.au}, 
  \texttt{yali3816@uni.sydney.edu.au}, \\
  \texttt{h.yong@hnu.edu.cn}, 
  \texttt{naveed.akhtar1@unimelb.edu.au}
}

% % Option 2.  Author’s address is linked with superscript
% % characters to its name, author names are grouped, centered.

% \author{
%   Template Author1\Thanks{The {\em actual} contributors to this instruction
%     document and corresponding template file are given in Section
%     \ref{sec:contributors}.}$^\diamond$ 
%   \and
%   Template Author2$^\dagger$
%   \\
%   \ \\
%   $^\diamond$Template Affiliation1/Address Line 1
%   \\
%   Template Affiliation1/Address Line 2
%   \\
%   Template Affiliation1/Address Line 2
%   \\
%   \texttt{template.email1example.com}
%   \\
%   \ \\
%   \\
%   $^\dagger$Template Affiliation2/Address Line 1
%   \\
%   Template Affiliation2/Address Line 2
%   \\
%   Template Affiliation2/Address Line 2
%   \\
%   \texttt{template.email2@example.com}
% }

\date{}


\begin{document}
\maketitle
\begin{abstract}
3D Scene Question Answering (3D SQA) represents an interdisciplinary task that integrates 3D visual perception and natural language processing, empowering intelligent agents to comprehend and interact with complex 3D environments. Recent advances in large multimodal modelling have driven the creation of diverse datasets and spurred the development of instruction-tuning and zero-shot methods for 3D SQA. However, this rapid progress introduces challenges, particularly in achieving unified analysis and comparison across datasets and baselines. 
This paper presents the first comprehensive survey of 3D SQA, systematically reviewing datasets, methodologies, and evaluation metrics while highlighting critical challenges and future opportunities in dataset standardization, multimodal fusion, and task design.
% This paper presents the first comprehensive survey of 3D SQA, systematically reviewing 15 datasets and 24 methodologies, along with evaluation metrics, while highlighting critical challenges and future opportunities in dataset standardization, multimodal fusion, and task design.

 %3D Scene Question Answering (3D SQA) is a multidisciplinary task that combines 3D visual perception and natural language processing to enable agents to understand and interact with complex 3D environments.  Recent advances in large multimodal models have led to emergence of numerous datasets and facilitated the development of instruction tuning and zero-shot methods for 3D SQA. However, these developments also bring challenges, as the diversity of datasets and baselines complicates unified  analysis and comparison. This paper provides the first comprehensive survey of 3D SQA, reviewing datasets, methods, and evaluation metrics, while identifying challenges and future directions in dataset development, multimodal integration, and task design.
\end{abstract}

% \iftaclpubformat
% \section{Courtesy warning: Common violations of \taclpaper rules that have
% resulted in papers being returned to authors for corrections}

% Avoid publication delays by avoiding these.
% \begin{enumerate}
% \item Violation: incorrect parentheses for in-text citations.  See \S
% \ref{sec:in-text-cite} and Table \ref{tab:cite-commands}.
% \item Violation: URLs that, when clicked, yield an error such as a 404 or go
% to the wrong page.
%   \begin{itemize}
%      \item Advice: best scholarly practice for referencing URLS would be to also
%      include the date last accessed.
%   \end{itemize}
% \item Violation: non-fulfillment of promise from submission to provide access
% instructions (such as a URL) for code or data.
% \item Violation: References incorrectly formatted (see \S\ref{sec:references}).
% Specifically:
% \begin{enumerate}
%   \item Violation: initials instead of full first/given names in references.
%   \item Violation: missing periods after middle initials.
%   \item Violation: incorrect capitalization.  For example, change ``lstm'' to
%   LSTM and ``glove'' to GloVe.
%   \begin{itemize}
%     \item Advice: if using BibTex, apply curly braces within the title field to
%     preserve intended capitalization.
%   \end{itemize}
%   \item Violation: using ``et al.'' in a reference instead of listing all
%   authors of a work.
%   \begin{itemize}
%     \item Advice: List all authors and check accents on author names even when
%     dozens of authors are involved.
%   \end{itemize}
%   \item Violation: not giving a complete arXiv citation number.
%   \begin{itemize}
%      \item Advice: best scholarly practice would be to give not only the full
%      arXiv number, but also the version number, even if one is citing version 1.
%   \end{itemize}
%   \item Violation: not citing an existing peer-reviewed version in addition to
%   or instead of a preprints
%     \begin{itemize}
%      \item Advice: When preparing the camera-ready, perform an additional check
%      of preprints cited to see whether a peer-reviewed version has appeared
%      in the meantime.
%   \end{itemize}
%   \item Violation: book titles do not have the first initial of all main words
%   capitalized.
%   \item Violation: In a title, not capitalizing the first letter of the first word
%   after a colon or similar punctuation mark.
% \end{enumerate}
% \item Violation: figures or tables appear on the first page.
%   \item Violation: the author block under the title is not
%     formatted according to Appendix~\ref{sec:authorformatting}.
% \end{enumerate}
% \else
% % Submission-specific rules
% \section{Courtesy warning: Common violations of \taclpaper rules that have
% resulted in desk
% rejects}
% \begin{enumerate}
%   \item Violation: wrong paper format.
%   \emph{As of the September 2018 submission round and beyond, TACL requires A4
%   format.  This is a change from the prior paper size.}

%   \item Violation: main document text smaller than 11pt, or table or figure
%   captions in a font smaller than 10pt. See Table \ref{tab:font-table}.

%   \item Violation: fewer than seven pages of content or more than ten pages of
%   content, {\em including} any appendices. (Exceptions are made for
%   re-submissions where a TACL Action Editor explicitly granted a set number of
%   extra pages to address reviewer comments.) See
%   Section \ref{sec:length}.
%   \item Violation: Author-identifying information in the document content or
%   embedded in the file itself.
%     \begin{itemize}
%       \item Advice: Make sure the submitted PDF does \emph{not} embed within it
%       any author info: check the document properties before submitting.
%       Useful tools include Adobe Reader and {\tt pdfinfo}.
%       \item Advice: Check that no URLs (or corresponding websites) inadvertently
%       disclose any author information. If software or data is to be distributed,
%       mention so in {\em anonymized} fashion.
%       \item Advice: Make sure that author names have been omitted
%       from the author block. (It's OK to include some sort of anonymous
%       placeholder.)
%       \item Advice: Do not include acknowledgments in a submission.
%       \item Advice: While citation of one's own relevant prior work is as
%       encouraged as the citation of any other relevant prior work,
%       self-citations should be made in the third, not first, person.
%       No citations should be attributed to ``anonymous'' or the like.
%       See Section \ref{sec:self-cite}.
%     \end{itemize}
% \end{enumerate}
% \fi


% \section{Introduction}

% \Taclpapers that do not comply with this document's instructions
% risk
% \iftaclpubformat
% publication delays until the camera-ready is brought into compliance.
% \else
% rejection without review.
% \fi


% \Taclpapers should consist of a Portable Document Format (PDF) file formatted
% for  \textbf{A4 paper}.\footnote{Prior to the September 2018 submission round, a
% different paper size was used.} All necessary fonts should be
% included in the  file.

% \iftaclpubformat
% Note that you will need to provide both a single-spaced and a double-spaced
% version; see \S \ref{ssec:layout}.

% If you promised to provide code or data at submission, specific instructions for
% how to access such resources must be provided.  (Typically, a URL to a stable,
% resource-specific site suffices.)

% All URLs should be manually checked to verify that they
% lead to a valid webpage, and to the site that was intended.
% \fi

\section{Introduction}
Visual Question Answering (VQA)  expands the scope of traditional text-based question answering \cite{squad} by incorporating visual content, enabling the interpretation of images \cite{antol2015vqa}, charts \cite{masry2022chartqa}, and documents \cite{ding2024mmvqa} to deliver context-aware responses. This capability facilitates a broader range of applications, including medical diagnostics \cite{wu2022medical}, financial analysis \cite{xue2024famma}, and assistance in academic research. %Despite significant advancements, 
Nevertheless, the growing demand of immersive 3D environments calls for  even  more natural and interactive question-answering systems. 3D Scene Question Answering (3D SQA)~\cite{azuma2022scanqa,ye2021tvcg3dqa} addresses this by bridging visual perception~\cite{he2016deep,he2017mask}, spatial reasoning~\cite{guo2020deep}, and language understanding in 3D environments \cite{linghu2024multi}, see Figure~\ref{fig:fig1}. Unlike traditional 3D tasks focused on object detection~\cite{qi2019deep,li2023ashapeformer} or segmentation~\cite{qi2017pointnet++,zou2024improved,he2025deep}, 3D SQA integrates multimodal data, e.g., visual inputs and textual queries, to enable embodied systems capable of complex reasoning~\cite{szymanska2024space3d}. By leveraging spatial relationships, object interactions, and hierarchical scene structures within dynamic 3D environments, 3D SQA advances robotics, augmented reality, and autonomous navigation~\cite{huang2023embodied}, pushing the boundaries of multimodal AI and its potential in complex, real-world scenarios.
% The evolution of question answering tasks has spanned several domains, starting with traditional textual QA~\cite{rajpurkar-etal-2016-squad}, which primarily focuses on extracting answers from textual inputs. While effective, these approaches are limited to linguistic information, lacking any visual or spatial context. Visual Question Answering (VQA)~\cite{antol2015vqa,lu2023multi} extends QA to static 2D visual data, enabling models to integrate textual queries with visual features from images. However, VQA methods are inherently restricted to flat, two-dimensional representations, which fail to capture the complexity of real-world environments, such as spatial relationships or interactions between objects.
% %Visual Question Answering (VQA) expands the scope of traditional text-based question answering by incorporating visual content, enabling the interpretation of images, charts, and documents to deliver context-aware responses. Unlike text-only approaches, VQA facilitates a broader range of applications, including medical diagnostics, remote sensing, and financial analysis. Despite significant advancements, the increasing demand for immersive 3D environments underscores the need for more natural and interactive question-answering systems. By leveraging spatial relationships and depth information, 3D VQA offers enhanced user interaction across domains such as virtual reality, autonomous navigation, and embodied intelligence. This evolution pushes the boundaries of multimodal AI, advancing its potential in complex, real-world scenarios.
% %Visual Question Answering has been applied 

% In contrast, the 3D Scene Question Answering (3D SQA) task~\cite{azuma2022scanqa,ye2021tvcg3dqa} is a foundational component of embodied intelligence~\cite{gupta2021embodied}, bridging visual perception~\cite{he2016deep,he2017mask}, spatial reasoning~\cite{guo2020deep}, and language understanding in 3D environments~\cite{linghu2024multi}. Unlike traditional 3D tasks that focus on object recognition and classification (e.g., 3D object detection~\cite{qi2019deep,li2023ashapeformer} or segmentation~\cite{qi2017pointnet++,zou2024improved}), 3D SQA integrates multimodal data—such as visual inputs and textual queries—to enable embodied systems capable of complex reasoning~\cite{szymanska2024space3d}. Such tasks require models to reason over complex spatial relationships, object interactions, and hierarchical scene structures within dynamic 3D environments~\cite{huang2023embodied}, making 3D SQA essential for applications in robotics, augmented reality, and autonomous navigation~\cite{huang2023embodied}.

\begin{figure}[t]
    \centering
    \tiny \includegraphics[width=\linewidth]{intro.png.jpg}
    \caption{2D Scene VQA and 3D SQA tasks.  3D SQA handles non-embodied as well as embodied tasks involving agent interactions within \textcolor{black}{3D scenes.}} %NA: What I understand from the figure is that the top box is for VQA and the bottom box is for 3D SQA. If so, can there be labels for the boxes? Like mention on the side (or inside the boxes) VQA and 3D SQA. 
    \label{fig:fig1}
    \vspace{-0.45cm}
\end{figure}  
 
Early developments in 3D SQA were driven by manually annotated datasets like ScanQA~\cite{azuma2022scanqa} and SQA~\cite{ye2021tvcg3dqa}, which aligned 3D point clouds with textual queries. 
% These datasets relied on static scenes and template-based pipelines, providing foundational benchmarks but limited scalability and task diversity. 
Recently, programmatic generation methods, such as those used in 3DVQA~\cite{9866910} and MSQA~\cite{linghu2024multi}, have enabled the creation of larger datasets with richer question types. The integration of Large Vision-Language Models (LVLMs) has further automated data annotation, leading to the development of more comprehensive datasets like LEO~\cite{huang2023embodied} and  Spartun3D~\cite{zhang2024spartun3d}.

Methodologies have evolved alongside datasets, transitioning from closed-set approaches to LVLM-enabled techniques. Early methods~\cite{azuma2022scanqa,ye2021tvcg3dqa} employed custom architectures combining point cloud encoders, e.g., PointNet++ \cite{qi2017pointnet++}, and text encoders, e.g., BERT \cite{kenton2019bert}, with attention-based fusion modules. However, they were constrained by predefined answer sets. The recent LVLM-based methods employ instruction-tuning~\cite{hong20233d3dllm,huang2023embodied} or zero-shot technique~\cite{yin2024lamm,linghu2024multi} while adapting models like GPT-4~\cite{achiam2023gpt}, which reduces  dependence on task-specific annotations. However, these methods also face  challenges in ensuring dataset quality and addressing evaluation inconsistencies.

To analyse the emerging challenges in 3D SQA and facilitate their systematic handling, this paper provides the first comprehensive survey of this research direction.
We focus on three fundamental aspects of this area, namely; \textit{(i)} the objectives of 3D SQA, \textit{(ii)} datasets needed to support these objectives, and \textit{(iii)} models being developed to achieve these objectives.
% We focus on three fundamental aspects:\\
% \noindent \textit{i) The objectives of 3D SQA.}\\
% \noindent \textit{ii) Datasets required to support these objectives.}\\ 
% \noindent
% \textit{iii) The kind of models needed to achieve these goals.}
We review the evolution of datasets and methodologies, highlighting trends in the literature, such as the shift from manual annotation to LVLM-assisted generation, and the progression from closed-set to zero-shot methods. Additionally, we discuss challenges in multimodal alignment and evaluation standardization, offering insights into the future direction of the field. \textcolor{black}{The  paper outline that follows an organized structure of the existing 3D SQA literature, is provided in Figure \ref{fig:3d_sqa_structure} in the appendix.} 

%This paper is structured as follows: Section~\ref{sec2} introduces the preliminaries of 3D SQA, including task definitions and inputs. Section~\ref{sec3} reviews datasets, highlighting their development and limitations. Section~\ref{sec4} discusses evaluation metrics, while Section~\ref{sec5} categorizes current methodologies. Section~\ref{sec6} identifies open challenges and outlines future directions, followed by conclusions in Section~\ref{sec7}.

% In Section~\ref{sec2}, we introduce the preliminaries of 3D SQA, including its task definition, unique challenges, and input-output formalization. Section~\ref{sec3} reviews 3D SQA datasets, highlighting their evolution, limitations, and the influence of large multimodal models on their construction and use. Section~\ref{sec4} focuses on evaluation metrics, discussing their role in assessing datasets and models. Section~\ref{sec5} explores methods and models.
% Together, these sections address the three fundamental questions posed in the introduction, offering insights into the objectives of 3D SQA, dataset requirements, evaluation principles, and effective model design. Finally, we conclude by discussing open challenges and future directions to guide further research.



\begin{table}[h]
\footnotesize % Reduce font size
\centering
\renewcommand{\arraystretch}{0.7} % Adjust row height

\adjustbox{width=0.5\textwidth}{
\begin{tabular}{>{\centering\arraybackslash}m{1.5cm}>{\centering\arraybackslash}m{5.5cm}}
\toprule
\textbf{Notation} & \textbf{Definition} \\ \midrule 
\( S \) & A 3D scene representation. 
% may contain one or more modalities. 
\\ \cmidrule{2-2}
\( S^{(p)} \) & Point cloud representation of the scene: \( S^{(p)} = \{(x_i, y_i, z_i) \mid i=1,\dots,N\} \), where \( x_i, y_i, z_i \in \mathbb{R} \) are  coordinates. \\ \cmidrule{2-2}
\( S^{(m)} \) & A set of multi-view images: \( S^{(m)} = \{I_1, I_2, \dots, I_K\} \). \\ \midrule
\( Q \) & Multimodal query. \\ \cmidrule{2-2}
\( Q^{(t)} \) & Textual query: \( Q^{(t)} = (w_1, w_2, \dots, w_L) \), where \( w_i \) is a textual token. \\ \cmidrule{2-2}
\( Q^{(e)} \) & Egocentric images  in the query. \\ \cmidrule{2-2}
\( Q^{(o)} \) & Object-level point clouds: \( Q^{(o)} = \{(x_j, y_j, z_j) \mid j=1,\dots,M\} \), \( x_j, y_j, z_j \in \mathbb{R} \). \\ \midrule
\( T \) & A textual answer: \( T = (t_1, t_2, \dots, t_R) \). \\ \midrule
\( B \) & A set of 3D bounding boxes for objects referenced in the answer: \( B = \{b_1, b_2, \dots, b_M\} \). \\ \midrule

% \( b_i \) & A bounding box for an object: \( b_i = (c_x^{(i)}, c_y^{(i)}, c_z^{(i)}, l^{(i)}, w^{(i)}, h^{(i)}) \). \\ \midrule
% \( c_x^{(i)}, c_y^{(i)}, c_z^{(i)} \) & Position of the \( i \)-th object in the scene. \\ \midrule
% \( l^{(i)}, w^{(i)}, h^{(i)} \) & Dimensions (length, width, height) of the \( i \)-th object. \\ \midrule
\( \mathcal{F} \) & The task function mapping %the input \( (S, Q) \) to the output \( (T, B) \): \( \mathcal{F} : 
\((S, Q) \mapsto (T, B) \). \\ \bottomrule
\end{tabular}
}
\caption{3D SQA task notations.}
\label{tab:notation}
\vspace{-0.5cm}
\end{table}

\vspace{-1mm}
\section{Preliminaries\label{sec2}}
\vspace{-1mm}
%This section defines the 3D SQA task, highlighting its inputs, outputs, and multimodal nature. 
The 3D SQA task involves comprehending a 3D scene $S$ and a query $Q$ to produce a textual answer $T$ and, optionally, spatial information $B$, such as bounding boxes for relevant objects. The 3D scene can be represented using modalities like point clouds $S^{(p)}$, multi-view images $S^{(m)}$, or their combinations, while the query may include textual input $Q^{(t)}$, egocentric images $Q^{(e)}$, or object-level point clouds $Q^{(o)}$. The task is formally defined as $\mathcal{F} : (S, Q) \mapsto (T, B)$, bridging multimodal reasoning and spatial understanding for comprehensive 3D scene analysis. For more details on this formulation, we refer to Table~\ref{tab:notation}.





%This section formalizes the 3D SQA task and introduces the notation for its inputs and outputs. As shown in Fig.~\ref{fig:fig1}, in 3D SQA, the model receives a three-dimensional scene and a query, and it must produce a textual answer. Optionally, the model may also provide spatial information, such as the coordinates of relevant objects.

% A 3D scene \(S\) can be represented using one or more modalities. Common representations include:

% A point cloud: 
% \begin{equation}
% \begin{aligned}
% S^{(p)} &= \{(x_i, y_i, z_i) \mid i=1,\dots,N\}, \\
% x_i, y_i, z_i &\in \mathbb{R}.
% \end{aligned}
% \end{equation}

% A set of multi-view images:
% \begin{equation}
% S^{(m)} = \{I_1, I_2, \dots, I_K\},
% \end{equation}
% where the views can include sequences captured over time (e.g., frames from a video) or static multi-perspective images.

% The scene \(S\) can therefore be any combination of the above:
% \begin{equation}
% S = \{ S^{(p)}, S^{(m)} \}_{\text{subset of these}}.
% \end{equation}

% The query \(Q\) may also be multimodal. It can be a text sequence:
% \begin{equation}
% Q^{(t)} = (w_1, w_2, \dots, w_L),
% \end{equation}
% where each \(w_i\) is a token. Alternatively, it may include egocentric images \(Q^{(e)}\) or object-level point clouds~\cite{linghu2024multi}:
% \begin{equation}
% \begin{aligned}
% Q^{(o)} &= \{(x_j, y_j, z_j) \mid j=1,\dots,M\}, \\
% x_j, y_j, z_j &\in \mathbb{R}.
% \end{aligned}
% \end{equation}

% Similarly, the query can be a combination:
% \begin{equation}
% Q = \{ Q^{(t)}, Q^{(e)}, Q^{(o)} \}_{\text{subset of these}}.
% \end{equation}

% The output of the 3D SQA task is a textual answer:
% \begin{equation}
% A = (a_1, a_2, \dots, a_R),
% \end{equation}
% and optionally a set of 3D bounding boxes for objects referenced in the answer:
% \begin{equation}
% B = \{ b_1, b_2, \dots, b_M \},
% \end{equation}
% where each bounding box is defined as:
% \begin{equation}
% b_i = (c_x^{(i)}, c_y^{(i)}, c_z^{(i)}, l^{(i)}, w^{(i)}, h^{(i)}),
% \end{equation}
% representing the position \((c_x^{(i)}, c_y^{(i)}, c_z^{(i)})\) and dimensions \((l^{(i)}, w^{(i)}, h^{(i)})\) of an object in the scene.

% In summary, the 3D SQA task can be formulated as:
% \begin{equation}
% f : (S, Q) \mapsto (A, B),
% \end{equation}
% where \(f\) takes the multimodal scene \(S\) and query \(Q\) as input, and outputs a textual answer \(A\) and optionally bounding boxes \(B\) for relevant objects. In summary, the 3D SQA task aims to enable systems to understand and reason about complex 3D environments by integrating multimodal data. The objective is to generate meaningful answers that combine textual reasoning with spatial and semantic understanding, bridging the gap between traditional 3D perception and interactive scene comprehension. This capability is critical for advancing embodied intelligence in real-world scenarios, where tasks require models to accurately interpret and respond to multimodal queries within three-dimensional spaces.

\vspace{-1mm}
\section{Datasets\label{sec3}}
\vspace{-1mm}

\begin{table*}[ht]
\centering
\scriptsize
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lllllccc@{}}
\toprule
\textbf{Dataset} & \textbf{Source} & \textbf{Scene} & \textbf{Q\&A} & \textbf{Collection} & \textbf{Modality} & \textbf{Suited} & \textbf{Grounding} \\ \midrule
ScanQA~\shortcite{azuma2022scanqa} & SCN~\shortcite{dai2017scannet} & 800 & 41K & Template & \( S^{(p)} \) &$\boldsymbol{\times}$ & \checkmark \\ 
SQA~\shortcite{ye2021tvcg3dqa} & SCN~\shortcite{dai2017scannet} & 800 & 6K & Human & \( S^{(p)} \) & $\boldsymbol{\times}$ & $\boldsymbol{\times}$\\ 
FE-3DGQA~\shortcite{zhao2022toward} & SCN~\shortcite{dai2017scannet} & 703 & 20K & Human & \( S^{(p)} \) &$\boldsymbol{\times}$ & \checkmark \\ 
CLEVR3D~\shortcite{yan2023comprehensive} & 3RS~\shortcite{wald2019rio} & 8,771 & 60K & Template & \( S^{(p)} \) & $\boldsymbol{\times}$& $\boldsymbol{\times}$\\ 
3DVQA~\shortcite{9866910} & SCN~\shortcite{dai2017scannet} & 707 & 500K & Template & \( S^{(p)} \) &$\boldsymbol{\times}$ & $\boldsymbol{\times}$\\ 
SQA3D~\shortcite{ma2022sqa3d} & SCN~\shortcite{dai2017scannet} & 650 & 33.4K & Human & \( S^{(p)} \) & \checkmark & $\boldsymbol{\times}$\\ 
ScanScribe~\shortcite{zhu20233d} & SR~\shortcite{chen2020scanrefer}+R3D~\shortcite{achlioptas2020referit3d} & 2,995 & 56K & LLM-assisted & \( S^{(p)} \) & \checkmark & $\boldsymbol{\times}$\\ 

3DMV-VQA~\shortcite{hong20233d} & HM3d~\shortcite{ramakrishnan2021habitat} & 5K & 50K & Template & \( S^{(m)} \) & $\boldsymbol{\times}$& $\boldsymbol{\times}$\\ 
OpenEQA~\shortcite{majumdar2024openeqa} & SCN, HM3d~\shortcite{ramakrishnan2021habitat,yadav2023habitat} & 180 & 1.6K & Human & \( S^{(m)} \) & $\boldsymbol{\times}$ & $\boldsymbol{\times}$\\ 

Spartun3D~\shortcite{zhang2024spartun3d} & 3RS~\shortcite{wald2019rio} & - & 123K & LLM-assisted & \( \{ S^{(m)}, S^{(p)} \} \) & \checkmark & $\boldsymbol{\times}$\\ 
MSQA~\shortcite{linghu2024multi} & SCN, 3RS, ARK~\shortcite{baruch2021arkitscenes} & - & 254K & LLM-assisted & \( \{ S^{(m)}, S^{(p)} \} \) & \checkmark & $\boldsymbol{\times}$\\ 
LEO~\shortcite{huang2023embodied} & SCN+3RS~\shortcite{wald2019rio} & 3K & 83K & LLM-assisted & \( \{ S^{(m)}, S^{(p)} \} \) & \checkmark & \checkmark \\ 
M3DBench~\shortcite{li2023m3dbench} & ScanQA~\shortcite{azuma2022scanqa} & - & 320K & LLM-assisted & \( \{ S^{(m)}, S^{(p)} \} \) & \checkmark & \checkmark \\ 
3D-LLM~\shortcite{hong20233d3dllm} & Objaverse~\shortcite{deitke2023objaverse} & - & 300K & LLM-assisted & \( \{ S^{(m)}, S^{(p)} \} \) & \checkmark & \checkmark \\ 
LAMM~\shortcite{yin2024lamm} & - & - & 186K & LLM-assisted & \( \{ S^{(m)}, S^{(p)} \} \) & \checkmark & \checkmark \\ \bottomrule
\end{tabular}%
}
\caption{Comparison of 3D SQA datasets. %with Suitability Indication. 
\textbf{Source} abbreviations: SCN = ScanNet, 3RS = 3RScan, HME = HM3D, ARK = ARKitScenes, SR+R3D = ScanRefer + ReferIt3D. 
\textbf{Modality} abbreviations: \( S^{(p)} \) = Point Cloud, \( S^{(v)} \) = Video, \( S^{(m)} \) = Multi-view Images. 
% Modal combinations are represented using sets, e.g., \( \{ S^{(m)}, S^{(p)} \} \) indicates both multi-view images and point clouds are used. 
\textbf{Suited}: Indicates if the dataset is an Embodied 3D SQA dataset, requiring  the agent to consider \textcolor{black}{its state when answering.}} %NA: I do not understand why there are three major blocks of rows in the Table. I do not know where we have discussed this division. Please double-check, and either mention about this division or remove the horizontal lines.
\label{table1}
\vspace{-0.5cm}
\end{table*}
Importance of datasets for contemporary 3D SQA can not be overemphasized. 
%Datasets are fundamental to model development and evaluation in 3D SQA. 
Existing datasets vary widely in  scene representation, scale, and query complexity.
% , capturing the multifaceted challenges of understanding 3D environments.
To provide a systematic overview of the existing datasets, this section is organized as two main parts: \textit{Dataset Structure}, which explores the representation and scale of scenes and queries.
% emphasizing the diversity in 3D data and task requirements, 
and \textit{QA Pair Creation}, which examines methodologies for generating question-answer pairs.
% , including template-based pipelines, manual annotations, and approaches assisted by LVLMs.

% Finally, we conclude this section with \textit{Summary and Insights}, where we analyze trends in dataset development and highlight the characteristics of datasets that are necessary to further advance the field of 3D SQA.
\vspace{-2mm}
\subsection{Dataset Structure
}
\vspace{-1mm}
In the data-driven domain of 3D SQA, structure of datasets significantly influences the scope of the tasks they support. Current datasets differ widely in their representations of 3D scenes, encompassing point clouds, multi-view images, and egocentric perspectives, as well as in the formats of their queries, which range from basic textual inputs to complex multimodal, embodied descriptions. Key dataset attributes such as scale, diversity of modalities, and query complexity significantly influence the design requirements and performance capabilities of 3D SQA models.
%Existing datasets vary in their representations of 3D scenes, including point clouds, multi-view images, and egocentric perspectives, as well as in the formats of queries, which range from simple textual inputs to multimodal, embodied descriptions. Dataset characteristics such as scale, modality diversity, and query complexity shape the requirements and capabilities of 3D SQA models. 
Table~\ref{table1} summarizes the key features of existing real-world 3D SQA datasets, providing an overview of their scene representations, query modalities, and scales. 
In Figure~\ref{fig:fig3},  we illustrate the  typical dataset   generation workflow at a higher level of abstraction.


\subsubsection{Scene Modalities and Scale}
Broadly, the development of 3D SQA datasets has progressed along a timeline evolving from synthetic environments to realistic 3D representations. 
% We categorize these datasets into five types following this progression. (1) \textit{Synthetic 3D Datasets} leverage synthetic scenes for foundational tasks. (2) \textit{Point Cloud Datasets} introduce real spatial representations. (3) \textit{Multi-View Datasets} focus on image-based accessibility, and (4) \textit{Multimodal and Instruction Tuning Datasets} integrate diverse inputs and leverage LLMs for scalable data creation. 

\noindent\textbf{Synthetic 3D Datasets:}
The development of 3D SQA began with pseudo-3D datasets that utilized synthetic environments to simulate scene-level QA tasks. For example, EmbodiedQA~\cite{embodiedqa} generated the dataset by selecting real scenes from the SUNCG~\cite{song2017semantic} subset within the House3D~\cite{wu2018building} simulator. These datasets were validated by human annotators to ensure quality. IQA~\cite{gordon2018iqa} expanded this effort by introducing the IQUAD V1 dataset with 75,000 questions paired with unique scene configurations, leveraging the AI2-THOR~\cite{kolve2017ai2} environment. MP3D-EQA~\cite{wijmans2019embodied} and MT-EQA~\cite{yu2019multi} further incorporated depth maps and multi-target QA tasks, respectively, while remaining confined to synthetic SUNCG~\cite{song2017semantic} scenes.

\noindent\textbf{Point Cloud Datasets:}
The transition to real-world 3D SQA tasks was marked by the introduction of datasets based on 3D point clouds~\cite{rusu20113d}. ScanQA~\cite{azuma2022scanqa} and SQA~\cite{ye2021tvcg3dqa} established foundational benchmarks for this direction. Both datasets were constructed using ScanNet~\cite{dai2017scannet}, with ScanQA generating 41K QA pairs across 800 scenes, and SQA providing 6K manually curated QA pairs with higher linguistic accuracy. Building on these efforts, FE-3DGQA~\cite{zhao2022toward} selected 703 specific scenes from ScanNet and annotated 20K QA pairs, emphasizing foundational QA tasks with dense bounding box annotations to enable spatial grounding.
CLEVR3D~\cite{johnson2017clevr} utilized functional programs and text templates to generate four times the number of questions in ScanQA, introducing a broader range of attributes and question types. Subsequently, 3DVQA~\cite{9866910} expanded on CLEVR3D's framework, leveraging 3D semantic scene graphs and template-based pipelines to generate questions and answers. By selecting 707 scenes, 3DVQA produced 500K QA pairs, significantly enriching task diversity and complexity. Similarly, SQA3D~\cite{ma2022sqa3d} %marked a significant advancement in agent-centric 3D QA. It 
curated 33.4K manually annotated QA pairs across 650 scenes, focusing on linking queries to  agent position and orientation. 
% This dataset enabled deeper exploration of tasks that integrate agent perspectives with spatial understanding. 



\noindent\textbf{Multi-View Datasets:}
To better align with human perception, multi-view datasets have been introduced, focusing on reasoning across different perspectives rather than relying solely on single point cloud representations. In this direction, 3DMV-VQA~\cite{hong20233d} includes 5K scenes from the HM3D dataset~\cite{ramakrishnan2021habitat}, generating 50K QA pairs. The images are rendered using the Habitat framework~\cite{ramakrishnan2021habitat,savva2019habitat,szot2021habitat}, emphasizing multi-view reasoning. On the other hand, OpenEQA~\cite{majumdar2024openeqa} not only selects scenes from HM3D but also incorporates Gibson~\cite{xia2018gibson} and ScanNet~\cite{dai2017scannet}, ultimately choosing 180 high-quality scenes with 1.6K QA pairs. Unlike other datasets, it prioritizes quality over scale, making it a significant contribution to high-quality 3D QA benchmarks.


\noindent\textbf{Multimodal  Datasets:}
Recent advances in 3D SQA datasets emphasize integrating point clouds, images, and textual data to form rich multimodal representations. These approaches aim to capture spatial, semantic, and contextual cues for more comprehensive scene understanding. A notable example is Spartun3D~\cite{zhang2024spartun3d}, which selects scenes from 3RScan~\cite{wald2019rio} and generates 123K QA pairs focused on situational tasks. Similarly, MSQA~\cite{linghu2024multi} builds 254K QA pairs from multimodal datasets~\cite{dai2017scannet, wald2019rio, baruch2021arkitscenes}, using point clouds and object images as inputs to better align with real-world embodied intelligence scenarios. 

With the popularity of LLMs, instruction tuning datasets have also emerged as an important extension of multimodal datasets, enhancing the generalization capabilities of 3D SQA models by aligning 3D data with textual descriptions. For instance, ScanScribe~\cite{zhu20233d} collects RGB-D scans of indoor scenes from ScanNet and 3R-Scan, incorporating diverse object instances from Objaverse~\cite{deitke2023objaverse}. It uses QA pairs from ScanQA and referential expressions from ScanRefer~\cite{chen2020scanrefer} and ReferIt3D~\cite{achlioptas2020referit3d}, generating 56.1K object instances from 2,995 scenes through templates and GPT-3~\cite{brown2020language}. Similarly, LEO~\cite{huang2023embodied} constructs 83K 3D-text pairs by collecting captions at object, object-in-scene, and scene levels~\cite{luo2024scalable, achlioptas2020referit3d, zhu20233d, chen2021scan2cap}.

Along similar lines, M3DBench~\cite{li2023m3dbench} leverages multiple existing %existing datasets~\cite{achlioptas2020referit3d, chang2017matterport3d, chang2015shapenet, chen2020scanrefer, dai2017scannet, krizhevsky2012imagenet, yuan2022toward} 
and LLMs to generate 320K instruction-response pairs, enriching multimodal 3D data for a wide range of 3D-language tasks. 3D-LLM~\cite{hong20233d3dllm}  creates over 300K 3D-text pairs using assets like Objaverse, ScanNet, and HM3D, while LAMM~\cite{yin2024lamm} employs GPT-API and self-instruction methods~\cite{wang2022self} to produce 186K language-image pairs and 10K language-3D pairs. 
% These instruction-tuning datasets exemplify the evolution of multimodal datasets, enabling models to better understand and align 3D spatial data with  language. %, thereby improving their performance across diverse tasks.


% \paragraph{Scene Modalities and Scale}
% The 3D SQA task originated from pseudo-3D scene tasks. For example, the early EmbodiedQA~\cite{embodiedqa} dataset was generated by selecting real scenes from the SUNCG~\cite{song2017semantic} subset within the House3D~\cite{wu2018building} simulator and validated by three human annotators. To advance interactive visual question answering, IQA~\cite{gordon2018iqa} introduced the IQUAD V1 dataset, leveraging the AI2-THOR~\cite{kolve2017ai2} 3D simulation environment. IQUAD V1 comprises 75,000 questions, each paired with a unique scene configuration. Similarly, MP3D-EQA~\cite{wijmans2019embodied} built upon the EmbodiedQA~\cite{embodiedqa} dataset by incorporating depth map data. MT-EQA~\cite{yu2019multi} further extended EmbodiedQA by introducing multi-target embodied question-answering tasks along with a corresponding dataset. However, these approaches still relied on synthetic SUNCG scenes~\cite{song2017semantic}.

% ScanQA\cite{azuma2022scanqa} and SQA\cite{ye2021tvcg3dqa} were among the first datasets to integrate 3D point clouds with textual queries, establishing the initial benchmark for true 3D SQA. Both datasets were built upon ScanNet\cite{dai2017scannet}; ScanQA generated 41K QA pairs across 800 scenes\cite{rajpurkar-etal-2016-squad}, while SQA curated 6K manually annotated QA pairs from the same 800 scenes, ensuring higher linguistic and contextual accuracy. Building on these efforts, FE-3DGQA\cite{zhao2022toward} extended the scope by selecting 703 specific scenes from ScanNet and annotating 20K QA pairs. Unlike ScanQA and SQA, which utilized the entire ScanNet dataset, FE-3DGQA focused on foundational reasoning tasks, providing dense bounding box annotations to better associate textual queries with spatial information. Embodied 3D SQA datasets have almost developed simultaneously, with SQA3D\cite{ma2022sqa3d} pioneering the field by curating 33.4K QA pairs across 650 scenes, specifically targeting agent-centric reasoning tasks.

% Compared to smaller, more specialized datasets, these datasets further expanded the scale of 3D point cloud scene QA. CLEVR3D\cite{yan2023comprehensive} emphasized common-sense-independent scenes to address scalability and diversity. Using functional programs and templates\cite{johnson2017clevr}, it programmatically generated 60K QA pairs, ensuring models rely on visual understanding rather than external knowledge. 3DVQA\cite{9866910} significantly expanded dataset scale and task diversity, providing 500K QA pairs across 707 scenes with tasks such as counting, binary reasoning, and attribute-based queries. ScanScribe\cite{zhu20233d} further demonstrated efficient reuse of annotation data by transforming descriptive annotations into 278K QA pairs from existing datasets.

% Besides using point clouds to represent 3D scenes, some works have explored multi-view representations, which better align with human perception of the surrounding environment. To address this, multi-view based 3D SQA datasets were constructed. For instance, 3DMV-VQA\cite{hong20233d} scaled the number of scenes to 5K and included 50K QA pairs, emphasizing multi-view reasoning tasks rather than point cloud representations. OpenEQA\cite{majumdar2024openeqa} prioritized dataset quality over scale by selecting 180 high-quality scenes from HM3D\cite{ramakrishnan2021habitat}, Gibson\cite{xia2018gibson}, and ScanNet~\cite{dai2017scannet}. It curated 1.6K QA pairs focused on real-world exploration tasks, introducing active exploration variants such as episodic memory  and real-time navigation.

% Earlier works have explored using either point clouds or multi-view representations to model 3D scenes. However, more recent datasets have moved towards integrating both point clouds and images, forming multimodal representations that capture richer spatial and contextual cues. This shift allows for a more comprehensive understanding of 3D scenes. For example, MSQA\cite{linghu2024multi} (254K QA pairs) and Spartun3D\cite{zhang2024spartun3d} (123K QA pairs) emphasize situational reasoning tasks by combining these modalities. Similarly, LEO\cite{huang2023embodied} (83K QA pairs) and M3DBench\cite{li2023m3dbench} (320K QA pairs) incorporate large language models (LLMs) and multimodal inputs to construct Instruction Tuning datasets, enhancing model generalization across diverse, real-world scenarios.

% \subsubsection{Query Modalities: From Text to Embodied Inputs}

% In 3D Scene Question Answering (SQA), a query represents the input question or prompt that, when paired with a 3D scene, guides the task of providing an answer. Early datasets focused on simple text-based queries that were primarily concerned with scene-level attributes. While these queries were useful for evaluating 3D scene understanding, they lacked contextual information about an agent's position, state, or environment. As the field progressed, datasets began to introduce more complex and multimodal queries, reflecting the growing demands of embodied intelligence, where the agent's interaction with its environment plays a central role in the task.

% One of the first steps toward embodied queries came with {SQA3D}~\cite{ma2022sqa3d}, which contextualized questions not only with textual descriptions but also with point cloud data, allowing the agent’s position within the scene to influence the query. A typical query in {SQA3D} might describe the agent’s location as "Sitting at the edge of the bed and facing the couch," linking the agent’s perspective to its environment. This type of contextualized query laid the groundwork for more sophisticated spatial reasoning tasks in embodied settings.

% Building on this, \textbf{SPARTUN3D}~\cite{zhang2024spartun3d} took agent-centric descriptions a step further by providing more detailed and dynamic spatial information. For instance, queries might include descriptions like "You are standing beside a trash bin while there is a toilet in front of you." This level of detail enhances the agent’s spatial awareness, enabling more nuanced interactions with the 3D environment. These richer spatial relationships allow for more complex reasoning tasks that better align with the way humans perceive and interact with their surroundings.

% The evolution continued with {MSQA}~\cite{linghu2024multi}, which refined this approach by adding even more precise location and orientation data. Queries in {MSQA} combine textual descriptions with specific coordinates and the agent’s orientation, such as "You are at position X, facing Y," and include a first-person view image, such as "An object [image] is at my 2 o'clock." This multimodal approach—incorporating text, spatial coordinates, and visual context—provides a highly realistic and dynamic representation of the agent's interaction with the scene, allowing for tasks that require both spatial reasoning and visual comprehension.

% These advances in query modalities—from simple text-based inputs to richly contextualized, multimodal, agent-centric queries—reflect the growing sophistication of 3D SQA tasks. The datasets developed in this area have significantly expanded the range of tasks that can be addressed, enabling more realistic and context-aware interactions within 3D environments.

% In parallel, datasets focused on {instrument tuning} have emerged, further advancing the field of 3D SQA. {ScanScribe}~\cite{zhu20233d}, {LEO}~\cite{huang2023embodied}, and {M3DBench}~\cite{li2023m3dbench} represent key efforts in this direction, leveraging multimodal inputs to fine-tune models for diverse tasks. These datasets combine spatially grounded textual descriptions, multimodal instructions, and agent perspectives to enable better model generalization across real-world scenarios. By utilizing large-scale, multimodal data and focusing on practical tasks like real-time navigation, episodic memory, and active exploration, these datasets contribute to the refinement of \textbf{instruction tuning}, improving model performance in dynamic, embodied environments.
\subsubsection{Query Modalities and Complexity}%: From Text to Embodied Inputs}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{Fig2.jpg}
    \caption{Dataset generation workflow.}
    \label{fig:fig3}
    \vspace{-0.45cm}
\end{figure} 

In 3D SQA, a query represents the input question or prompt that, when paired with a 3D scene, guides the task of providing an answer. Over time, query modalities in 3D SQA have evolved from simple text-based inputs to more complex, multimodal, and agent-centric formats. Here, we summarise the datasets from the query modality perspective,  which is a critical consideration for dataset selection in performance evaluation.  

%This section categorizes the evolution of query modalities into four stages:

\noindent\textbf{Basic Text Queries:}
Early 3D SQA datasets primarily employed straightforward text-based queries that focused on scene-level attributes, such as object counting or identification. These datasets aimed to evaluate foundational 3D scene understanding, often without considering the agent's position, interaction, or perspective within the environment. For example, datasets like {ScanQA}~\cite{azuma2022scanqa} and {SQA}~\cite{ye2021tvcg3dqa} feature questions such as \textit{"How many chairs are in the room?"}.
%or "What is the color of the sofa?" 
Such purely textual questions  fail to capture  complex embodied scenarios as they lack description of an agent's spatial or contextual relationship with the scene.
%, such as their location, orientation, or interactions with objects. 
Consequently, these datasets 
%provided a solid foundation for 3D SQA tasks but were 
are limited in scope, as reflected in Table~\ref{table1}, where the lack of \textit{Suited} queries indicates their omission of agent-centric contexts. This limitation underscores the evolution toward richer, more contextualized datasets in the later  3D SQA research.

\noindent\textbf{Agent-Centric Text Queries:} % Descriptions in Text:}
The introduction of agent-centric descriptions marked a significant shift in query complexity. {SQA3D}~\cite{ma2022sqa3d} was one of the first datasets to incorporate contextualized questions, where textual queries were enhanced with references to the agent’s position or orientation. In this case, a typical query might describe the agent's location, such as \textit{"Sitting at the edge of the bed and facing the couch."}. We mark datasets enabling such queries as \textit{Suited} in Table~\ref{table1}. 
%These queries connected the agent's spatial perspective with the scene, enabling more realistic and interactive QA tasks that better reflected embodied intelligence.

%\noindent\textbf{Agent Descriptions in Multimodal Formats}
\noindent\textbf{Multimodal Agent-Centric Queries:} 
%Building upon contextualized queries,
Recently, {SPARTUN3D}~\cite{zhang2024spartun3d} and {MSQA}~\cite{linghu2024multi} introduced richer spatial descriptions and multimodal query inputs.
The former provided  detailed spatial information, enabling queries such as \textit{"You are standing beside a trash bin while there is a toilet in front of you."}. 
%These datasets enhanced the spatial awareness of agents by linking textual inputs to intricate spatial relationships.
Similarly, MSQA integrated textual descriptions, explicit spatial coordinates, and agent orientation in the queries. Additionally, first-person view images were included.
%, e.g., "An object [image] is at my 2 o'clock." 
These multimodal approaches enable more realistic scenarios by combining spatial, visual, and linguistic contexts.

\noindent\textbf{Instruction-Tuned Queries:} 
Recent datasets, such as ScanScribe~\cite{zhu20233d}, LEO~\cite{huang2023embodied}, and M3DBench~\cite{li2023m3dbench}, have also expanded query modalities further to support instruction tuning tasks. They leverage agent-centric queries enriched with multimodal inputs, such as spatially grounded textual descriptions and multimodal instructions.
For example, LEO incorporates multimodal instructions to fine-tune models for agent tasks like real-time navigation or object interaction.
M3DBench focuses on generalization across diverse real-world tasks by utilizing rich multimodal data.
These instruction-tuning datasets ensure models are well-equipped to address practical, real-world tasks by aligning textual instructions with spatial and visual contexts.

% \begin{figure}[t]
%     \centering
%     \includesvg[width=\linewidth]{dataset_creation}
%     \caption{Dataset Generation workflow}
%     \label{fig:fig3}
%     \vspace{-0.5cm}
% \end{figure}  

 
 
\subsection{ QA Pair Creation}
The creation of question-answer (QA) pairs defines the scope and complexity of 3D SQA tasks. Early datasets relied on manual annotation, while recent efforts have adopted templates and LVLMs to improve scalability and diversity. \textcolor{black}{These advances have enabled datasets to include a wider range of question types, from object identification to spatial relationships and task-specific queries.} %, shaping the benchmarks for 3D SQA evaluation.}

%\vspace{-3mm}
\subsubsection{Methods for QA Pair Generation}
QA pair generation in 3D SQA datasets balances between manual annotation, template-based pipelines, and LLM-assisted methods. Manual annotation ensures high-quality and contextual accuracy, while template-based approaches enable scalable generation with logical consistency. Recently, LLMs have further automated the process, enabling diverse multimodal  QA pairs at scale. This progression, also  apparent in Figure~\ref{fig:fig3}, reflects the evolution of dataset creation techniques.

\noindent\textbf{Template-Based Generation:} 
Template-based generation was introduced as an early solution for scalable QA pair creation. ScanQA~\cite{azuma2022scanqa} exemplified this approach by utilizing a T5-based QA generation model~\cite{raffel2020exploring} to generate seed questions from ScanRefer~\cite{chen2020scanrefer}.
% These questions were manually refined, resulting in 41K QA pairs across 800 scenes. 
Similarly, datasets like CLEVR3D~\cite{yan2023comprehensive}, 3DVQA~\cite{9866910}, and 3DMV-VQA~\cite{hong20233d} leveraged 3D Semantic Scene Graphs to programmatically generate diverse and logically consistent QA pairs, improving scalability and task diversity. While the template-based approach enables large-scale datasets, the generated questions often lack contextual specificity and may sometimes result in overly generic queries.

\noindent\textbf{Manual Annotation:}
Researchers have also pursued manual annotation to address the limitations of template-based methods. Manual approaches prioritize linguistic precision and contextual relevance, creating datasets that are smaller in scale but of higher quality. For instance, SQA~\cite{ye2021tvcg3dqa} curated 6K QA pairs with an emphasis on linguistic accuracy, while FE-3DGQA~\cite{zhao2022toward} selected 703 scenes from ScanNet~\cite{dai2017scannet} and annotated 20K QA pairs, grounding answers with bounding box annotations. Similarly, OpenEQA~\cite{majumdar2024openeqa} curated 1.6K QA pairs from 180 high-quality scenes. SQA3D~\cite{ma2022sqa3d} contributed 33.4K QA pairs across 650 scenes, tailored specifically for agent-centric tasks. Despite their time-intensive nature, fully curated datasets play a critical role in ensuring accuracy and contextual alignment, complementing the template-based methods.


\noindent\textbf{LLM-Assisted Generation:}
Recent methods have increasingly leveraged LLMs to automate the generation of QA pairs, enhancing both scalability and diversity. Notable examples include Spartun3D~\cite{zhang2024spartun3d} and MSQA~\cite{linghu2024multi}, both of which utilize scene graphs to structure spatial and semantic relationships. Spartun3D employs GPT-3.5 to generate agent-centric questions, emphasizing situated reasoning and exploration, resulting in 123K QA pairs. MSQA takes a similar approach with GPT-4V, focusing on situated QA generation guided by semantic scene graphs, producing 254K QA pairs.
%that reflect detailed spatial and contextual understanding. 
%These datasets highlight how integrating LLMs with scene graphs facilitates the creation of rich and contextually relevant QA pairs while maintaining scalability.

Additionally, LLMs have been instrumental in constructing instruction tuning datasets to improve model generalization across diverse multimodal tasks. ScanScribe~\cite{zhu20233d} utilizes GPT-3 to transform ScanRefer annotations into scene descriptions using template-based refinement.
% producing 56.1K multimodal pairs. 
LEO~\cite{huang2023embodied} adopts GPT-4 with Object-centric Chain-of-Thought (O-CoT) prompting to ensure logical consistency.
% , resulting in 83K object- and scene-level 3D-text pairs. 
M3DBench~\cite{li2023m3dbench} and 3D-LLM~\cite{hong20233d3dllm} use GPT-4 to create multimodal prompts based on object attributes and scene-level inputs.
% , generating 320K and 300K instruction-response pairs, respectively. 
Together, these datasets demonstrate the growing role of LLMs in automating the generation of high-quality, multimodal data for 3D SQA. %, laying the foundation for models capable of handling complex embodied intelligence tasks.


\subsubsection{Question Design in 3D SQA}

With advancements in language and vision modelling, 3D SQA questions have evolved along several dimensions: from simple to complex tasks, non-situated to situated contexts, and static to dynamic scenarios. 
% These developments reflect a growing emphasis on challenging models to engage in more sophisticated reasoning and interaction within 3D environments.
\textcolor{black}{To exemplify the nature of these questions, we enlist the common 3D SQA tasks and representative question in  Table~\ref{app_table_q_examples} in the appendix.}

\noindent\textbf{Task Complexity - From Basic to Advanced:}
3D SQA covers a diverse spectrum of question tasks designed to assess models’ understanding of 3D environments and their reasoning abilities.
%, as exemplified in Appendix \ref{app_table_q_examples}. 
Basic tasks, such as object identification, spatial reasoning, attribute querying, object counting, and attribute comparison, are featured in datasets like SQA~\cite{ye2021tvcg3dqa}, ScanQA~\cite{azuma2022scanqa}, FE-3DGQA~\cite{zhao2022toward}, 3DVQA~\cite{9866910} and CLEVR3D~\cite{yan2023comprehensive}. Among these, FE-3DGQA introduced more complex, free-form questions that require models not only to ground answer-relevant objects but also to identify contextual relationships between them. Similarly, CLEVR3D  emphasized relational reasoning by incorporating questions that integrate objects, attributes, and their interrelationships, pushing models further to handle intricate contextual dependencies.

As 3D SQA evolves, tasks demanding a deeper understanding of spatial and visual context have emerged, challenging models to engage with dynamic and context-aware reasoning. These tasks include multi-hop reasoning (SQA3D~\cite{ma2022sqa3d}), navigation (SQA3D~\cite{ma2022sqa3d}, LEO~\cite{huang2023embodied}, 3D-LLM~\cite{hong20233d3dllm}, M3DBench~\cite{li2023m3dbench}, MSQA~\cite{linghu2024multi}), robotic manipulation (LEO), object affordance (Spartun3D~\cite{zhang2024spartun3d}), functional reasoning (OpenEQA~\cite{majumdar2024openeqa}), multi-round dialogue (LEO, M3DBench, 3D-LLM), planning (LEO, M3DBench, Spartun3D), and task decomposition (3D-LLM). These advanced tasks challenge models to dynamically reason and navigate complex 3D environments while capturing intricate spatial and relational details. Notably, OpenEQA~\cite{majumdar2024openeqa} stands out as the first open-vocabulary dataset for embodied question answering.

% supporting episodic memory and active exploration, marking a pivotal step in expanding 3D VQA’s scope.


% \paragraph{Question Modality}
% In 3D VQA, questions leverage two primary modalities: text modality and visual modality, both crucial for enabling models to process and reason about 3D environments. The text modality encompasses diverse inputs, including direct questions, instructions, situation descriptions, and dialogues. While all datasets utilize questions as a fundamental text input, datasets such as SQA3D (2022), LEO (2023), 3D-LLM (2023), M3DBench (2023), MSQA (2024), and Spartun3D (2024) incorporate situation descriptions to provide context for situated tasks. Instructions and dialogues, featured in 3D-LLM (2023), LEO (2023), M3DBench (2023), and LAMM (2024), further challenge models to interpret task-specific commands or manage multi-turn conversations, highlighting the importance of linguistic reasoning in 3D settings.

% The visual modality complements textual inputs by providing spatial and contextual information through single-view 3D scenes, multi-view images, and dynamic embodied sequences. Single-view scenes, utilized in all 3D VQA datasets, support the analysis of static environments. Multi-view images, employed in datasets like 3DMV-VQA (2023), M3DBench (2023), LEO (2023), 3D-LLM (2023), Spartun3D (2024), and MSQA (2024), offer multiple perspectives, enriching the visual context. Dynamic embodied sequences, used in datasets such as OpenEQA (2024), enable models to integrate information across consecutive frames, supporting tasks like episodic memory recall and active exploration. Together, these modalities provide a robust framework for evaluating the interplay between linguistic and spatial reasoning in 3D VQA.

\noindent\textbf{Situated vs. Non-Situated Questions:}
Based on the required level of interaction and contextual understanding, 3D VQA questions can be categorized into  situated and non-situation types. The latter focus on static reasoning, testing a model’s ability to interpret spatial relationships, attributes, and object properties within fixed 3D scenes. Datasets like SQA~\cite{ye2021tvcg3dqa}, ScanQA~\cite{azuma2022scanqa}, FE-3DGQA~\cite{zhao2022toward}, 3DVQA~\cite{9866910}, CLEVR3D~\cite{yan2023comprehensive}, and LAMM~\cite{yin2024lamm} primarily include non-situated questions that evaluate understanding within static spatial contexts.

Conversely, situated questions involve dynamic reasoning, requiring interaction with the 3D environment and comprehension of contextual or sequential information. These questions test models’ ability to navigate, plan, and adapt to dynamic scenarios and often include temporal or embodied elements. Situated questions appear in datasets like SQA3D~\cite{ma2022sqa3d}, LEO~\cite{huang2023embodied}, 3D-LLM~\cite{hong20233d3dllm}, M3DBench~\cite{li2023m3dbench}, MSQA~\cite{linghu2024multi}, Spartun3D~\cite{zhang2024spartun3d}, 3DMV-VQA~\cite{hong20233d}, and OpenEQA~\cite{majumdar2024openeqa}. This categorization enables a comprehensive evaluation of 3D VQA systems. %, from processing static spatial data to adapting dynamically within complex interactive environments.

\noindent\textbf{Temporal Aspect in 3D SQA:}
Most 3D SQA datasets limit questions to a single time slot, reflecting the static nature of the environments they evaluate. This restriction simplifies reasoning by focusing on a specific moment within the 3D scene. However, datasets like OpenEQA~\cite{majumdar2024openeqa} now introduce dynamic scenarios that allow for multiple time slots, enabling tasks that require episodic memory and active exploration. This temporal dimension challenges models to integrate sequential information and represents a significant step forward for advancing 3D SQA.

\vspace{-2mm}
\subsection{Evaluating LLM-Generated 3D Datasets}
While   LLM adoption has significantly advanced 3D SQA datasets, 
%As discussed earlier, the adoption of large language models (LLMs) for generating 3D SQA datasets has significantly improved scalability, task diversity, and multimodal integration. However, 
ensuring their quality, reliability, and practical utility  remains an open  challenge. %Accurate and contextually relevant QA pairs are essential for validating their applicability to real-world tasks.
Current evaluation methods primarily rely on manual assessments. For example, LEO~\cite{huang2023embodied} evaluates QA pairs through expert review, reporting metrics like overall accuracy and contextual relevance. MSQA~\cite{linghu2024multi} adopts a comparative approach, sampling QA pairs from its dataset and comparing them against a benchmark dataset such as SQA3D~\cite{ma2022sqa3d}, with scores based on contextual accuracy, factual correctness, and overall quality. Similarly, Spartun3D~\cite{zhang2024spartun3d} employs expert validation by randomly sampling instances to ensure that the generated data meets expected quality standards. These manual evaluations provide valuable insights into dataset quality but face limitations in scalability, labour intensity, and subjectivity.

To address these limitations, automated evaluation frameworks are currently  needed. Potential solutions include embedding-based metrics for semantic alignment, logical consistency checks for QA coherence, and task-specific metrics for spatial accuracy and multimodal integration. 
% Such automated approaches, when combined with manual assessments, can enable more comprehensive and scalable evaluations, ensuring LLM-generated datasets meet the  demands of 3D SQA applications.

% \subsection{Summary and Insights}
% The development of 3D SQA datasets has transitioned from small-scale, manually annotated collections to large-scale datasets generated through templates and scene graphs. Early datasets prioritized precision and linguistic accuracy, while later efforts focused on scalability and task diversity. The recent integration of LVLMs has further automated dataset creation, enabling richer multimodal inputs and more efficient annotation processes. However, while LVLMs enhance scalability, ensuring annotation quality remains a significant challenge.
% In addition to quality control, the lack of standardized benchmarks currently complicates consistent evaluation across datasets and methods. To advance the field, future datasets need to focus on establishing unified benchmarks, improving annotation quality, and introducing more diverse multimodal scenarios to better support systematic progress in 3D SQA.

%\vspace{-0.3cm}

% \begin{table*}[ht]
% \centering
% \small
% \begin{tabular}{c|c|c|c}
% \hline
% \textbf{Dataset} & \textbf{LLM / VLM } & \textbf{Methodology} & \textbf{Key Features} \\ \hline
% MSQA~\cite{linghu2024multi} & GPT-4V & Scene graph-guided situated QA generation & Accurate spatial and semantic relationships. \\
% Spartun3D~\cite{zhang2024spartun3d} & GPT-3.5 & Agent-centric reasoning from scene graphs & Focus on situated reasoning and exploration. \\
% ScanScribe~\cite{zhu20233d} & GPT-3 & Scene descriptions from ScanRefer & Template-based generation with refinement. \\ 
% LEO~\cite{huang2023embodied} & GPT-4 & O-CoT reasoning with scene graphs & Logical consistency and object-centric QA. \\
% M3DBench~\cite{li2023m3dbench} & GPT-4 & Multimodal prompts with object attributes & Diverse QA tasks via instruction tuning. \\
%  \hline
% \end{tabular}
% \caption{\textcolor{red}{Datasets leveraging LVLMs for QA generation in 3D SQA.}}
% \label{tab:lvlm_3dqa}
% \end{table*}

% \subsection{LVLM-3D SQA Datasets}
% Tab.~\ref{table1}  illustrate that the collection methods for existing 3D SQA datasets can be categorized into three main approaches: template-based generation, manual annotation, and automated generation using large models. With the advent of large models, automated dataset generation has become the dominant approach. Below, we detail how different datasets utilize large models for this purpose.

% ScanScribe~\cite{zhu20233d} is the first framework to leverage large models for generating training data for pretraining models and subsequently generalizing to downstream tasks like 3D SQA. Specifically, for scans from ScanNet, existing textual data from datasets such as question-answer pairs in ScanQA~\cite{azuma2022scanqa} and referential expressions in ScanRefer~\cite{chen2020scanrefer} and ReferIt3D~\cite{achlioptas2020referit3d} are converted into scene descriptions. For 3RScan~\cite{wald2019rio}, templates and GPT-3~\cite{brown2020language} are used to generate scene descriptions based on scene graph annotations~\cite{wu2021scenegraphfusion}.


% LEO~\cite{huang2023embodied} leverages 3D semantic scene graphs (SSGs)~\cite{wu2021scenegraphfusion} as prompts for large language models (LLMs)~\cite{bang2023multitask} to generate high-quality 3D-text pairs. To guide the LLM and ensure consistency, manually crafted example tasks are provided as seed tasks for reference. To address the issue of hallucination often observed in open-ended text generation by LLMs~\cite{bang2023multitask} , LEO introduces object-centric chain of thought (O-CoT) prompting. This approach requires the LLM to explicitly generate intermediate reasoning steps by listing the labels and IDs of object candidates as part of its thought process during text generation. This structured reasoning mitigates hallucination and enhances the reliability and precision of the generated data.

% M3DBench~\cite{li2023m3dbench} collects extensive instruction-response data by leveraging LLMs for 3D-language tasks such as dialogue and question answering. To generate task-specific instruction data, the pipeline provides the GPT-API~\cite{achiam2023gpt,schulman2022chatgpt} with object attributes, textual descriptions, manually written task construction instructions, and few-shot in-context learning examples. While most responses generated by the GPT-API are of high quality, a subset of responses deviates from the intended instructions, often referencing extraneous information inferred from the provided textual descriptions.
% To enhance the quality of the 3D instruction-following data, M3DBench employs a filtering mechanism based on pattern matching with predefined keywords. This approach helps identify and exclude irrelevant or inconsistent responses, ensuring a more accurate and focused dataset for 3D-language tasks.


% MSQA~\cite{linghu2024multi} adopts scene graphs to prompt LLMs for data generation, building upon prior methods. A key difference lies in its approach to initializing scene objects. Specifically, each object in the scene graph is instantiated with its attributes, which are derived by prompting GPT-4V~\cite{achiam2023gpt} using cropped object images. This ensures accurate and contextually relevant attributes for every object.

% Subsequently, MSQA performs pairwise computations among the initialized objects to infer spatial and semantic relationships. After constructing the scene graph, MSQA adjusts the relationships between objects (e.g., proximity and alignment) based on sampled spatial positions and viewpoints, creating situated scene graphs. These situated graphs serve as a basis for designing system prompts, enriched by handcrafted examples, to guide GPT-3.5~\cite{schulman2022chatgpt} in generating high-quality, contextually appropriate question-answer pairs for 3D reasoning tasks.
% \subsection{Summary}
% Currently, most QA datasets do rely on a text-based or template-based approach, and then use large language models (LLMs) to generate QA pairs. However, this approach may not be sufficient to meet the needs of real scenarios, especially when questions require combining visual information or complex multimodal data to derive answers.
\vspace{-1mm}
\section{Evaluation Metrics\label{sec4}}
\vspace{-1mm}
% \subsection{Datasets Evaluation Metrics}
% With the increasing adoption of large language models (LLMs) for generating 3D SQA datasets, their application has significantly accelerated the development of multimodal reasoning and scene understanding tasks. These datasets, created by leveraging LLMs, often combine textual and visual inputs, enabling scalable and efficient data generation. However, as the reliance on LLMs grows, evaluating the quality, reliability, and practical utility of these datasets has become a critical challenge. Determining whether the generated questions and answers are accurate, contextually relevant, and reflective of real-world scenarios is essential to ensure their validity for downstream tasks and applications. This necessitates a robust and systematic framework for assessing the effectiveness of LLM-generated 3D SQA datasets.

% To evaluate the quality of 3D SQA datasets generated by LLMs~\cite{openai2023chatgpt}, several approaches have been implemented to ensure accuracy and reliability. For example, LEO~\cite{huang2023embodied} manually checks the answers in its QA pairs for correctness and reports the overall accuracy as a measure of data validity. MSQA~\cite{linghu2024multi} adopts a comparative evaluation method, sampling 100 QA pairs from both its own dataset and SQA3D~\cite{ma2022sqa3d}. These samples are then manually scored across three dimensions: contextual relevance, factual correctness, and overall quality. This method provides insights into how well the LLM-generated data aligns with human-labeled benchmarks. Similarly, Spartun3D~\cite{zhang2024spartun3d} evaluates the generated data by randomly selecting 50 instances for each task, which are subsequently assessed manually to validate their quality.

% These manual evaluation methods, while labor-intensive, provide valuable benchmarks for dataset reliability. They ensure that LLM-generated data not only meets expected quality standards but also maintains relevance and applicability to the intended tasks. However, the need for more automated and scalable evaluation methods remains a critical area for future research.


%With the development of 3D SQA datasets,
Standardized  evaluation metrics are crucial to gauge advances in  3D SQA and ensure dataset suitability for downstream tasks. 
%Effective evaluation metrics not only highlight the strengths and limitations of different approaches but also provide a standardized framework for comparing methods and advancing the field. 
%These metrics are vital for bridging the gap between dataset creation and practical application, as they validate whether models trained on these datasets can effectively address the intended tasks. 
Contemporary 3D SQA literature either  uses traditional or LLM-based metrics for the evaluation purpose.

\noindent\textbf{Traditional metrics:} 3D SQA methods often employ quantitative measures of linguistic relevance and correctness for evaluation. Commonly used metrics include Exact Match (EM@1, EM@10), which assesses whether the generated answers match ground truth exactly, and language generation metrics such as BLEU~\cite{papineni2002bleu}, ROUGE-L~\cite{lin2004rouge}, METEOR~\cite{banerjee2005meteor}, CIDEr~\cite{vedantam2015cider}, and SPICE~\cite{anderson2016spice}. These metrics were initially employed  by ScanQA~\cite{azuma2022scanqa} and have since been used for  datasets like CLEVR3D~\cite{yan2023comprehensive}, 3DGQA~\cite{zhao2022toward}, and ScanScribe~\cite{zhu20233d}. While effective for evaluating linguistic accuracy and diversity, traditional metrics are generally  limited in capturing the nuanced reasoning and contextual understanding required for 3D SQA tasks.

\noindent\textbf{LLM-based metrics:} The emerging evaluation paradigm in 3D SQA employs LLM-based metrics, leveraging the reasoning capabilities of models like GPT. For instance, OpenEQA~\cite{majumdar2024openeqa} employs GPT to evaluate the contextual relevance and correctness of generated answers, introducing a metric that eventually computes the Mean Relevance score. % as shown in Formula~\ref{eq1}:
% \begin{equation}
% C = \frac{1}{N} \sum_{i=1}^{N} \frac{s_i - 1}{4} \times 100\%
% \label{eq1}
% \end{equation}
% where \( s_i \) represents the relevance score for each answer. 
Similarly, MSQA~\cite{linghu2024multi} uses GPT to assess the quality of answers based on nuanced reasoning, aligning them with contextual expectations. Compared to traditional metrics, LLM-based methods currently excel at simulating real-world reasoning and capturing semantic subtleties, making them particularly valuable for evaluating complex multimodal tasks.

In summary, traditional metrics provide a strong foundation for evaluating linguistic and structural quality, while LLM-based metrics offer deeper insights into contextual alignment and reasoning. Combining the complementary properties of these metrics can offer a comprehensive framework for assessing 3D SQA performance. %, addressing both surface-level accuracy and deeper semantic validity.





\begin{table*}[htbp]
\centering
\scriptsize
 % 设置字体为小号
\begin{tabular}{l|ccccc}
\toprule
\textbf{Method} & \textbf{Type} & \textbf{Scene Modality} & \textbf{Scene Encoder} & \textbf{Text Encoder} & \textbf{Answer Module} \\ 
\midrule
ScanQA~\shortcite{azuma2022scanqa} & T-S & \( S^{(p)} \) & VoteNet~\shortcite{qi2019deep} & BiLSTM~\shortcite{graves2012long} & MLP \\ 
3DQA-TR~\shortcite{ye2021tvcg3dqa} & T-S & \( S^{(p)} \) & Group-Free~\shortcite{liu2021group} & BERT~\shortcite{kenton2019bert} & MLP \\ 
TransVQA3D~\shortcite{yan2023comprehensive} & T-S & \( S^{(p)} \) & PointNet++~\shortcite{qi2017pointnet++} & BERT~\shortcite{kenton2019bert} & MLP \\ 
FE-3DGQA~\shortcite{zhao2022toward} & T-S & \( S^{(p)} \) & PointNet++~\shortcite{qi2017pointnet++} & T5~\shortcite{raffel2020exploring} & Linear Layer \\ 
SIG3D~\shortcite{man2024situational} & T-S & \( S^{(p)} \) &  OpenScene~\shortcite{peng2023openscene} & BiLSTM~\shortcite{graves2012long} & MLP \\ 
3D-CLR~\shortcite{hong20233d} & T-S & \( S^{(m)} \) & CLIP-LSeg~\shortcite{li2022language} & CLIP~\shortcite{radford2021learning} & 3D CNN \\ 
BridgeQA~\shortcite{mo2024bridging} & T-S &  \( \{ S^{(m)}, S^{(p)} \} \) & VoteNet\&BLIP~\shortcite{li2023blip} & BLIP~\shortcite{li2023blip} & Transformer \\ \hline
3DVLP~\shortcite{zhang2024vision} & P-B & \( S^{(p)} \) & PointNet++~\shortcite{qi2017pointnet++} & CLIP~\shortcite{radford2021learning} & MLP \\ 
CLIP-Guided~\shortcite{parelli2023clip} & P-B & \( S^{(p)} \) & VoteNet\&Transformer & CLIP~\shortcite{radford2021learning} & MLp \\ 
Multi-CLIP~\shortcite{delitzas2023multi} & P-B & \( S^{(p)} \) & VoteNet\&Transformer & CLIP~\shortcite{radford2021learning} & MLP \\ 
3D-VisTA~\shortcite{zhu20233d} & P-B & \( S^{(p)} \) & PointNet++~\shortcite{qi2017pointnet++} & BERT & MLP \\ 
GPS~\shortcite{jia2025sceneverse} & P-B & \( S^{(p)} \) & PointNet++~\shortcite{qi2017pointnet++} & Transformer~\shortcite{vaswani2017attention} & Transformer \\ 
LM4Vision~\shortcite{pang2023frozen} & P-B(w I-T) & \( S^{(p)} \) & VoteNet~\shortcite{qi2019deep} &  LSTM~\shortcite{hochreiter1997long} & LLaMA~\shortcite{touvron2023llama}  \\ 
3D-LLM~\shortcite{hong20233d3dllm} & P-B(w I-T) & \( S^{(m)} \) & BLIP2~\shortcite{li2023blip}  & BLIP2~\shortcite{li2023blip} & BLIP2~\shortcite{li2023blip} \\ 
LEO~\shortcite{huang2023embodied} & P-B(w I-T) & \( S^{(p)} \) & PointNet++\& ST~\shortcite{chen2022language} & ConvNext~\shortcite{liu2022convnet} & Vicuna~\shortcite{chiang2023vicuna} \\ 
LAMM~\shortcite{yin2024lamm} & P-B(w I-T) & \( S^{(p)} \) & PointNet++~\shortcite{qi2017pointnet++} & SentencePiece~\shortcite{kudo2018sentencepiece} & Vicuna~\shortcite{chiang2023vicuna} \\ 
M3DBench~\shortcite{li2023m3dbench} & P-B(w I-T) & \( S^{(p)} \) & PointNet++\& Transformer & Opt~\shortcite{zhang2022opt} & Opt~\shortcite{zhang2022opt} \\ \hline
SQA3D~\shortcite{ma2022sqa3d} & Z-S & \( S^{(p)} \) & Scan2Cap~\shortcite{chen2021scan2cap} & GPT-3~\shortcite{brown2020language} & GPT-3 \\ 
LAMM~\shortcite{yin2024lamm} & Z-S & \( S^{(p)} \) & PointNet++~\shortcite{qi2017pointnet++} & SentencePiece~\shortcite{kudo2018sentencepiece} & Vicuna \\ 
EZSG~\shortcite{singh2024evaluating} & Z-S & \( S^{(m)} \) & GPT-4V~\shortcite{yang2023dawn} & GPT-4V~\shortcite{yang2023dawn} & GPT-4V \\ 
OpenEQA~\shortcite{majumdar2024openeqa} & Z-S & \( S^{(m)} \) & GPT-4V~\shortcite{yang2023dawn} & GPT-4V~\shortcite{yang2023dawn} & GPT-4V \\ 
MSQA~\shortcite{linghu2024multi} & Z-S & \( S^{(m)} \) & GPT-4o~\shortcite{achiam2023gpt} & GPT-4o~\shortcite{achiam2023gpt} & GPT-4o \\ 
LEO~\shortcite{huang2023embodied} & Z-S &  \( \{ S^{(m)}, S^{(p)} \} \) & PointNet++\& ST~\shortcite{chen2022language} & ConvNext~\shortcite{liu2022convnet} & Vicuna~\shortcite{chiang2023vicuna} \\  
Spartun3D-LLM~\shortcite{zhang2024spartun3d} & Z-S &  \( \{ S^{(m)}, S^{(p)} \} \) & PointNet++~\shortcite{qi2017pointnet++} & CLIP~\shortcite{radford2021learning} & Vicuna \\ 
\toprule
\end{tabular}
\vspace{-1mm}
\caption{Overview of techniques for 3D SQA. Methods are categorized as Task-Specific (T-S), Pretraining-Based (P-B) and Zero-Shot (Z-S).  \textcolor{black}{P-B (w I-T) denotes Pretraining-Based methods further enhanced with Instruction Tuning to better adapt to task-specific instructions}. Scene modalities are represented as \( S^{(p)} \) for Point Cloud, \( S^{(m)} \) for Image, and \( \{ S^{(m)}, S^{(p)} \} \) for Multimodal.}
\label{table:methods}
\vspace{-5mm}
\end{table*}

\vspace{-1mm}
\section{Taxonomy of 3D SQA Methods
%Methodological Taxonomy for 3D SQA
\label{sec5}}
\vspace{-1mm}
3D SQA methods can be categorized into three primary types, as shown in Table~\ref{table:methods}. 
i) \textit{Task-Specific Methods} rely on predefined answers and specialized architectures to address specific tasks.
ii)~\textit{Pretraining-Based Methods}  leverage large-scale datasets to align multimodal representations and fine-tune for task-specific objectives.
iii) \textit{Zero-Shot Learning Methods} also utilize pretrained LLMs and VLMs to generalize to new tasks, albeit without additional fine-tuning.
These categories underpin the field’s evolution from task-specific to scalable approaches that harness the capabilities of advanced multimodal models, reflecting the increasing focus on flexibility and adaptability in 3D SQA systems. Figure~\ref{fig:fig4} in the appendix illustrates the common high-level pipeline of the methods.


\subsection{Task-Specific Methods}
%Task-specific methods for 3D SQA are designed for specific tasks and datasets  using a closed-set classification approach. 
These methods are designed for specific tasks using  closed-set classification approach.
% They generally follow a common framework consisting of four key stages: visual encoding, question encoding, feature fusion, and answer prediction. Visual features, either from point clouds or images, are encoded using appropriate encoders, while textual queries are processed through text encoders. The encoded information is then fused, and a classification head is applied to predict the final answer.
% A comparison of these methods, covering their visual encoders, query encoders, and fusion strategies, is provided in Table~\ref{table3} in the supplementary material.


\noindent\textbf{Point Cloud Methods:}
 3D SQA methods for point clouds follow a modular pipeline of scene and query encoding, feature fusion, and answer prediction. Early approaches like ScanQA~\cite{azuma2022scanqa} employed VoteNet~\cite{qi2019deep} and PointNet++\cite{qi2017pointnet++} to extract spatial features, while textual queries were encoded using GloVe~\cite{pennington2014glove} and BiLSTM~\cite{graves2012long}. Fusion was achieved through transformer-based modules.
 % , enabling relationships between scene objects and query words to inform final predictions via attention-based MLPs~\cite{luong2015effective}. 

Building on this foundation, later methods introduced more sophisticated encoders and fusion strategies. For example, 3DQA-TR~\cite{ye2021tvcg3dqa} replaced VoteNet with Group-Free~\cite{liu2021group} for finer-grained scene encoding and adopted BERT~\cite{kenton2019bert} for query encoding. Fusion was further streamlined by directly integrating features via a text-to-3D transformer~\cite{ye2021tvcg3dqa}, enabling more direct question-to-answer mappings. Similarly, TransVQA3D~\cite{yan2023comprehensive} enhanced feature interaction by introducing SGAA for fusion, focusing on global and local semantics in scenes.

For the datasets requiring spatial grounding, FE-3DGQA~\cite{zhao2022toward} advanced the pipeline by using PointNet++~\cite{qi2017pointnet++} for spatial feature extraction and T5~\cite{raffel2020exploring} for textual encoding, complemented by an attention mechanism~\cite{zhao20213dvg, liu2021swin} to align text with dense spatial annotations.
The recently proposed SIG3D~\cite{man2024situational} focuses on context-aware tasks in embodied intelligence. It encodes scenes using voxel-based tokenization and employs anchor-based contextual estimation to determine the agent's position and orientation. 
%Subsequently, it refines visual representations through context-guided visual re-encoding, incorporating context-relevant embeddings, and utilizes a Transformer decoder to integrate visual and linguistic information for question-answering tasks.

%\textcolor{blue}{
{\noindent\textbf{{Multi-view and 2D-3D Methods:}}} %NA: It seems that you are basically covering very limited number of methods in 'Multi-view' and '2D-3D' methods, and there is a lot of redundant details about the covered methods. I think you should combine both these sections into one with title 'Multi-view and 2D-3D Methods'. Remove redundant details, especially things like loss function and text that mostly relates to vision (not language). You can write a single big paragraph to cover both types of methods. It will compress the text to about half of these two sections.
A few methods also use multi-view images to enhance 3D SQA performance. For example, 3D-CLR~\cite{hong20233d} constructs compact 3D scene representations by leveraging multi-view images and optimizing 3D voxel grids. 
% The model achieves alignment between 3D voxel features and 2D per-pixel features, grounding concepts using CLIP~\cite{radford2021learning}, which facilitates zero-shot semantic understanding. 
On the other hand, 2D-3D methods like BridgeQA~\cite{mo2024bridging} combine 2D image features from pretrained VLMs~\cite{radford2021learning, li2023blip} with 3D object-level features obtained through VoteNet~\cite{qi2019deep}. Both feature types are aligned with text features encoded by the VLM’s text encoder and fused using a vision-language transformer, enabling  free-form answers. 
% These approaches integrate 2D and 3D features for robust cross-modal understanding, improving performance on complex multimodal tasks.
%}
% Techniques may rely on multi-view images instead of point clouds for 3D SQA. For instance, 3D-CLR~\cite{hong20233d} leverages multi-view representations to achieve 3D question answering by constructing compact 3D scene representations through Direct Voxel Grid Optimization~\cite{li2022language}. 
% The alignment between 3D voxel features and 2D per-pixel features is achieved through a weighted L1 loss along rendering rays \cite{max1995optical}, enabling zero-shot concept grounding by attending 3D features to CLIP concept embeddings~\cite{radford2021learning, li2022language}. 
% With this 3D semantic grounding, neural reasoning operators—such as filtering by concepts, clustering instances using DBSCAN~\cite{ ester1996density}, and identifying relations with relation networks~\cite{barbara1993and}—are executed on the 3D representation to answer complex queries, integrating multi-view perception with semantic and spatial understanding.

% \noindent\textbf{2D-3D Methods:}
% BridgeQA~\cite{mo2024bridging} adopts a two-branch framework that integrates 2D image features extracted by a pretrained VLM~\cite{radford2021learning,li2023blip} and 3D object-level features obtained using VoteNet~\cite{qi2019deep}. The 2D features are patch-level embeddings from the VLM, while the 3D features are object-centric tokens. Both are aligned with text features encoded by the VLM’s text encoder~\cite{kenton2019bert} and fused through a 2D/3D vision-language transformer. The model employs a BLIP-based language decoder~\cite{li2023blip} to generate free-form answers, which are optimized using a perplexity-based loss~\cite{raffel2020exploring}. This approach maximizes the utility of 2D pretraining, ensures cross-modal alignment, and enables robust generalization across multimodal inputs, addressing limitations of prior methods~\cite{dai20183dmv, hong20233d}.

\noindent\textbf{Advances in Text Encoders:}
The evolution of text encoders in 3D SQA reflects the increasing demands for contextual and multimodal understanding by the models. Early methods employed BiLSTM~\cite{graves2012long} and BERT~\cite{kenton2019bert} for basic semantic and syntactic feature extraction, as seen in ScanQA~\cite{azuma2022scanqa} and 3DQA-TR~\cite{ye2021tvcg3dqa}. More recent approaches, such as FE-3DGQA~\cite{zhao2022toward}, leverage  transformer-based models like T5~\cite{raffel2020exploring} for richer linguistic embeddings. Meanwhile, multimodal models like CLIP~\cite{radford2021learning} in 3D-CLR~\cite{hong20233d} and BLIP~\cite{li2023blip} in BridgeQA~\cite{mo2024bridging} have been instrumental in aligning textual and visual features. These advancements highlight a shift towards models that seamlessly integrate text with 3D spatial representations for improved performance.
\textcolor{black}{Task-specific methods are typically evaluated on the ScanQA and SQA3D datasets. Tables \ref{table4} and \ref{table5} in the appendix provide performance comparison summaries on these dataset for existing methods.}

% \setlength{\tabcolsep}{3pt}
% \begin{table*}[!t]
% \centering
% \scriptsize
% \resizebox{\linewidth}{!}{%
% \begin{tabular}{lccccc}
% \hline
% \textbf{Method} & \textbf{Visual} & \textbf{Text Encoder} & \textbf{Fusion} & \textbf{Head} & \textbf{New Fusion} \\ \hline
% ScanQA & V\\
% 3DQA-TR & PointNet++ & BERT & 3D-L BERT &  & 3D-L BERT \\
% TransVQA3D & PointNet & BERT & SGAA & MLP & SGAA \\
% FE-3DGQA & VoteNet & T5 & Attention & Linear & Attention \\
% 3D-CLR & CLIP-LSeg & CLIP & Relation Network & Sparse 3D CNN & Relation Network \\ 
% BridgeQA & BLIP & BLIP & Twin-Transformer & BLIP & Twin-Transformer \\\hline
% \end{tabular}%
% }
% \caption{Comparison of closed-set QA methods for 3D VQA in terms of key components.}
% \label{table3}
% \end{table*}oteNet & BiLSTM & Transformer & MLP & Transformer 




% \begin{table*}[h]
% \centering
% \scriptsize
% \begin{tabular}{lcccccc}
% \hline
% \textbf{Method} & \textbf{Text} & \textbf{Situation} & \textbf{Point} & \textbf{Image} & \textbf{Generator} \\ \hline
% % OpenEQA & LLaVA & LLaVA & - & LLaVA & LLM(GPT4) \\
% % OpenEQA* & GPT4V & GPT4V & - & GPT4V & GPT4V \\
% LAMM & SentencePiece & SentencePiece & PointNet++ & - & LLM(Vicuna-13B) \\
% M3DBench & Tokenizer & Tokenizer & 3d-vista & CLIP & LLM(Vicuna) \\
% % SQA3D-Zero-shot & GPT-3 & GPT-3 & Scan2cap & - & LLM(GPT-4) \\
% MSQA & Vicuna-7B & Vicuna-7B & PointNet++ & ConvNeXt & LLM(Vicuna-7B) \\
% LEO & SentencePiece & SentencePiece & PointNet++ & ConvNext & LLM(Vicuna-7B) \\
% SPARTUN3D & CLIP & CLIP & PointNet++ & - & LLM(Vicuna7B) \\ \hline
% \end{tabular}
% \caption{Comparison of methods and their configurations with duplicated text column.}
% \label{table6}
% \end{table*}







% \begin{table*}[h!]
% \centering
% \small
% \caption{Summary Table of Pretraining-Based Methods for 3D SQA}
% \begin{tabular}{|l|l|l|l|}
% \hline
% \textbf{Method}       & \textbf{Encoder Type}       & \textbf{Fine-Tuning}          & \textbf{Downstream Datasets}  \\ \hline
% 3DVLP~\cite{zhang2024vision}    & CLIP                    & Full model fine-tuning        & ScanQA                        \\ \hline
% CLIP-Guided~\cite{parelli2023clip} & CLIP                    & Full model fine-tuning        & ScanQA                        \\ \hline
% Multi-CLIP~\cite{delitzas2023multi} & CLIP                    & Full model fine-tuning        & ScanQA, SQA3D                 \\ \hline
% GPS~\cite{jia2025sceneverse}     & transformer-based       & Full model fine-tuning        & ScanQA, SQA3D                 \\ \hline
% 3D-VisTA~\cite{zhu20233d}  & LLM-based (frozen)       & Task-specific head            & ScanQA, SQA3D                 \\ \hline
% LM4Vision~\cite{pang2023frozen} & LLM-based (frozen)       & Task-specific head            & ScanQA, SQA3D                 \\ \hline
% 3D-LLM~\cite{hong20233d3dllm}   & LLM-based (frozen)       & Task-specific head            & ScanQA, SQA3D, 3DMV-VQA       \\ \hline

% \end{tabular}
% \label{table:pretraining_methods}
% \end{table*}
\vspace{-1mm}
\subsection{Pretraining-Based Methods}
\vspace{-1mm}
Pretraining-based approaches in 3D SQA have transitioned from traditional methods that emphasize explicit alignment of spatial and textual embeddings to instruction-tuning paradigms that harness large pretrained models. These  methods strike a balance between task-specific adaptation and generalization to  address challenges of scalability. % and computational efficiency.
%Pretraining-based methods in 3D SQA have evolved from traditional approaches focused on explicit alignment of spatial and textual embeddings to instruction-tuning paradigms leveraging large pretrained models. These methods balance task-specific adaptation and generalization, addressing challenges in scalability and computational efficiency.

\noindent\textbf{Traditional Pretraining Methods:}
These methods focus on aligning 3D spatial features with rich 2D visual and linguistic representations. \citet{parelli2023clip} utilized a trainable 3D scene encoder based on VoteNet~\cite{qi2019deep} to extract object-level features, which are further refined using a Transformer layer to model inter-object relationships. 
% Alignment of 3D scene representations with CLIP embeddings is achieved through combining  cosine similarity and object detection losses. 
Multi-CLIP~\cite{delitzas2023multi} introduces multi-view rendering and robust contrastive learning to enhance the integration of 3D spatial features with 2D representations. \citet{zhang2024vision} introduced object-level cross-contrastive and self-contrastive learning tasks during pretraining to improve cross-modal alignment. % and enable precise object-level understanding within 3D scenes.
\citet{jia2025sceneverse} adopted a hierarchical contrastive alignment strategy, combining object-level, scene-level, and referential embeddings to enhance cross-modal and intra-modal feature integration.

 Diverging from these contrastive learning approaches, 3D-VisTA~\cite{zhu20233d} employs a unified Transformer-based framework~\cite{vaswani2017attention} to align 3D scene features with textual representations. Instead of relying on extensive annotations, it leverages self-supervised objectives to optimize multimodal alignment~\cite{he2021transrefer3d, radford2019language}.
 %, ensuring scalability and generalizability across various 3D vision-language tasks. 
 This shift from task-specific pretraining to self-supervised learning is a noteworthy development for   efficient and robust 3D SQA.

\noindent\textbf{Instruction-Tuning Methods:}
Pretrained foundation models learn general geometric and semantic representations from large-scale unsupervised data at high  computational cost.
%, providing a robust foundation for downstream tasks. 
%However, training such models from scratch  incurs substantial costs. 
%often requiring thousands of GPUs and weeks of processing, 
%which is impractical for most research teams. 
%Instruction tuning efficiently capitalizes on the generalization capabilities of the pre-trained models, requiring only minimal task-specific fine-tuning. 
%This approach significantly reduces computational overhead and data dependency while enabling rapid adaptation to complex tasks.}
Instruction-tuning methods exploit the generalization abilities of these models by leveraging pretrained LLMs or VLMs as frozen encoders.
These methods retain the parameters of the  encoders, making minimal modifications, typically through lightweight task-specific layers, to adapt to downstream tasks.
%, reducing the need for extensive pretraining while maintaining strong performance. 
Recent approaches, such as LM4Vision~\cite{pang2023frozen}, 3D-LLM~\cite{hong20233d3dllm}, LEO~\cite{huang2023embodied}, M3DBench~\cite{li2023m3dbench}, and LAMM~\cite{yin2024lamm}, exemplify this shift. 

LM4Vision~\cite{pang2023frozen} employs a frozen LLaMA~\cite{touvron2023llama} encoder and trains lightweight task-specific layers for alignment with the 3D QA tasks. Similarly, 3D-LLM builds upon the BLIP2~\cite{li2023blip}, adding a task-specific head while keeping the base model frozen. In contrast, LEO, M3DBench, and LAMM utilize Vicuna~\cite{chiang2023vicuna}, a derivative of LLaMA, to integrate textual and multimodal inputs. LEO incorporates object-centric  and scene-level captions for enhanced multimodal reasoning. %, while M3DBench constructs multimodal instruction-tuning datasets with object attributes and prompts. % to tackle diverse QA tasks. 
% LAMM extends instruction tuning by including language-3D instruction-response pairs, further enriching model capabilities.
By leveraging the extensive knowledge encoded in LLMs or VLMs, these methods bypass the need for large task-specific pretraining datasets. 
% Instead, they train lightweight heads on datasets like ScanQA, SQA3D, or 3DMV-VQA, significantly reducing computational overhead while ensuring robust performance. 
Additionally, instruction-tuning methods are also effective in zero- and few-shot scenarios. 
% However, their reliance on frozen encoders can  limit their  ability to capture domain-specific nuances, presenting a trade-off between efficiency and  performance.



% \subsection{Pretraining-Based Methods}

% \paragraph{Without VLM Encoding}
% Early pretraining-based methods, such as \textbf{3DVLP}\cite{zhang2024vision}, \textbf{CLIP-Guided}\cite{parelli2023clip}, \textbf{Multi-CLIP}\cite{delitzas2023multi}, and \textbf{GPS}\cite{jia2025sceneverse}, focus on aligning 3D spatial and textual features during pretraining. These methods typically train on intermediate datasets, such as ScanRefer or SceneVerse, to refine the interaction between textual and spatial embeddings. For instance, \textbf{CLIP-Guided} and \textbf{Multi-CLIP} utilize CLIP as a backbone to align multimodal representations with specific losses like cosine similarity. These approaches involve updating both the visual and textual encoders, making them more tightly coupled with the pretraining datasets. After pretraining, these models are finetuned on downstream datasets like ScanQA and SQA3D to adapt to 3D QA tasks, often requiring additional task-specific layers for finetuning.

% \paragraph{With LLM or VLM Encoding}
% In contrast, recent advancements have shifted towards using frozen large language models (LLMs) or vision-language models (VLMs) for encoding textual and visual inputs, eliminating the need to update encoder parameters during downstream task adaptation. Methods such as \textbf{3D-VisTA}\cite{zhu20233d}, \textbf{LM4Vision}\cite{pang2023frozen}, and \textbf{3D-LLM}~\cite{hong20233d3dllm} rely on pretrained LLMs or VLMs as fixed encoders. For instance, \textbf{3D-VisTA} incorporates masked modeling objectives for scene-level pretraining, while \textbf{LM4Vision} uses a frozen LLaMA encoder with only lightweight trainable layers for alignment. These approaches bypass the need for extensive encoder updates, focusing instead on adding lightweight task heads for downstream finetuning. This paradigm is particularly appealing for tasks requiring efficient adaptation to specific 3D QA benchmarks, as demonstrated on datasets like ScanQA, SQA3D, and 3DMV-VQA.

% \begin{figure*}[t]
%     \centering
%     \includegraphics[width=0.95\linewidth]{method_llm.jpg}
%     \caption{LLM-based method}
%     \label{fig:fig4}
% \end{figure*}  

\vspace{-1mm}
\subsection{Zero-Shot Learning Methods}
\vspace{-1mm}
Zero-shot has emerged as a promising learning paradigm for 3D SQA, enabling models to infer answers to unseen tasks without task-specific fine-tuning. Current zero-shot 3D SQA methods can be broadly categorized into: text-driven, image-driven, and multimodal alignment approaches.

\noindent\textbf{Text-Driven Approaches:} These methods convert 3D scene information into textual descriptions, which are then used with a question in pretrained LLMs or VLMs for zero-shot inference. An example is SQA3D~\cite{ma2022sqa3d}, which uses Scan2Cap~\cite{chen2021scan2cap} to generate scene descriptions and inputs them into GPT-3~\cite{brown2020language} for answering questions. However, this approach overlooks the spatial structure of point clouds and images, limiting its ability to fully leverage 3D information. Similarly, LAMM~\cite{yin2024lamm} extracts features from point clouds and text, but uses 3D data in a limited manner.  
%but its limited use of 3D data  restricts its performance. % compared to more integrated methods.

\noindent\textbf{Image-Driven Approaches:} These methods use VLMs to incorporate visual features like images or multi-view data along with text. For instance, MSQA~\cite{linghu2024multi} uses GPT-4o~\cite{achiam2023gpt} with VLMs.  
%However, it finds that including image features does not significantly improve performance compared to text-only approaches. 
\citet{singh2024evaluating} tested unfinetuned GPT-4V~\cite{yang2023dawn} on datasets like 3D-VQA and ScanQA~\cite{azuma2022scanqa}, showing competitive performance in certain tasks. These methods are flexible and resource-efficient, but they  still rely on text to represent spatial and object relationships, which is a potential limitation. 

\noindent\textbf{Multimodal Alignment Approaches:} Techniques such as LEO~\cite{huang2023embodied} and Spartun3D-LLM~\cite{zhang2024spartun3d}, explicitly align visual and textual information during pretraining. LEO  improves zero-shot performance by aligning object- and scene-level features, while Spartun3D-LLM employs an  explicit module for aligning point clouds and text.
%, achieving superior results. 
These methods require relatively more training resources due to additional computations. Nevertheless, they offer an attractive trade-off between performance and efficiency.  %for training, data and pretraining.

Overall, in contemporary Zero-shot 3D SQA, Text-driven approaches are cost-effective and flexible but suffer from limited utilization of 3D data. Image-driven methods, which directly leverage VLMs for inference, also face limitations due to insufficient exploitation of 3D information. Multimodal alignment methods, while offering superior performance, have higher resource requirements. 
%An apparent  direction for future research is to  explore the balance between multimodal alignment and pretrained models to enhance both efficiency and performance.

\section{Challenges and Future Directions\label{sec6}}
While 3D SQA has seen notable advancements, several critical challenges remain, limiting its potential for real-world applications. We outline key challenges and propose directions for future research.

\noindent\textbf{Dataset Quality and Standardization.} The rapid development of 3D SQA datasets in recent years has led to a fragmented landscape, with datasets varying widely in scope and modality. Integrating these datasets into unified benchmarks can offer the much needed standardised evaluation to catapult research in this direction. 
%would enable comprehensive evaluations and promote progress. 
Additionally, while LLMs facilitate scalable dataset generation, they often introduce hallucinated information and contextual misalignments. Future research should focus on robust validation frameworks, leveraging human-in-the-loop systems or LLMs as validators. % to ensure quality and reliability.

\noindent\textbf{Enhancing 3D Awareness in Zero-Shot.} Current zero-shot models heavily rely on textual proxies, with limited utilization of 3D spatial and geometric features. Although multi-view approaches mitigate this issue to some extent, the lack of explicit 3D representation hampers their effectiveness for spatially complex tasks. Instruction-tuning methods face similar limitations. Future work needs to explore architectures that deeply integrate 3D features with linguistic and visual modalities to enhance generalization across diverse tasks. Additionally, an apparent  direction for future research is to  explore the balance between multimodal alignment and pretrained models in zero-shot 3D SQA to enhance both efficiency and performance.

\noindent\textbf{Unified Evaluation.} Absence of standardized and 3D SQA objective-specific evaluation metrics currently complicates meaningful  evaluation and comparisons across datasets and models. Developing unified frameworks that incorporate multimodal metrics for spatial reasoning, contextual accuracy, and task-specific performance are currently required to enable accurate benchmarking and drive methodological innovation in 3D SQA.

\noindent\textbf{Dynamic and Open-World Scenarios.} Most existing methods and datasets focus on static, predefined environments, limiting applicability to real-world tasks. Future efforts need to emphasize more on dynamic, open-world settings, enabling models to handle real-time scene changes and novel queries. Incorporating embodied interactions, such as navigation and multi-step reasoning, will further align 3D SQA systems with real-world requirements.

\noindent\textbf{Interpretable and Explainable 3D SQA Models.}
Current 3D SQA models often act as "black boxes", limiting their adoption in trust-critical domains like healthcare. Developing interpretable models that visualize 3D features, highlight relevant regions, or provide natural language explanations can enhance user trust and broaden their applicability.

\noindent\textbf{Multimodal Interaction and Collaboration.}
3D SQA systems are evolving toward more natural and interactive interfaces. Future research can explore integrating linguistic, gestural, and visual inputs to enable intuitive interaction with 3D scenes. Additionally, collaborative scenarios, such as architectural design or educational training, where multiple users interact with the system in real-time, offer a promising direction. Such systems could enhance communication and joint problem-solving, unlocking broader applications for 3D SQA.

\noindent\textbf{Incorporating Temporal Dynamics.}
Most 3D SQA models currently ignore temporal dynamics of the scenes, whereas most of the real-world applications, such as traffic monitoring, robotic navigation,  involve dynamic environments. Future research should aim to incorporate temporal dynamics into 3D SQA, allowing models to reason about scene changes over time. Leveraging temporal information, such as object movements, would enable these systems to better handle tasks requiring long-term temporal reasoning.

\noindent\textbf{Model Efficiency and Deployment.}
\textcolor{black}{Deploying 3D SQA systems on resource-constrained devices, such as mobile robots and edge AI agents, remains challenging due to high computational and memory demands. Future work should focus on lightweight architectures and optimization techniques, including pruning, quantization, and knowledge distillation, to enable efficient and real-time inference. Energy-efficient algorithms and scalable designs tailored for embedded systems will further enhance the practicality of 3D SQA in real-world applications.}

By addressing these challenges, 3D SQA can advance toward robust, scalable, and versatile systems, driving significant progress in embodied intelligence and multimodal reasoning.



% \section{\textcolor{red}{Conclusion}\label{sec7}} %NA: Do we really need a conclusion of the survey paper? If it is not a requirement to have a conclusion section for a survey, I am inclined towards completely removing the conclusion section. It is not really giving any new information but taking space.
% This survey provides a comprehensive overview of 3D Scene Question Answering, a field at the intersection of 3D computer vision and natural language processing, aiming to advance embodied intelligence through spatial understanding and multimodal reasoning. We analyzed the evolution of datasets from manual curation to LLM-assisted generation and methods from task-specific to zero-shot approaches. By categorizing datasets and methodologies, we clarified their strengths and limitations, and proposed unified benchmarks and innovative directions to address persistent challenges in dataset quality, multimodal alignment, and evaluation consistency. This work serves as a foundation for future research, guiding the development of robust, scalable systems for tackling complex real-world 3D tasks.

\bibliography{tacl2021}
\bibliographystyle{acl_natbib}

\iftaclpubformat

\onecolumn

% \appendix
% \section{Author/Affiliation Options as set forth by MIT Press}
% \label{sec:authorformatting}

% Option 1. Author’s address is underneath each name, centered.

% \begin{quote}\centering
%   \begin{tabular}{c}
%     \textbf{First Author} \\
%     First Affiliation \\
%     First Address 1 \\
%     First Address 2 \\
%     \texttt{first.email@example.com}
%   \end{tabular}
%   \ 
%   \begin{tabular}{c}
%     \textbf{Second Author} \\
%     Second Affiliation \\
%     Second Address 1 \\
%     Second Address 2 \\
%     \texttt{second.email@example.com}
%   \end{tabular}

%   \begin{tabular}{c}
%     \textbf{Third Author} \\
%     Third Affiliation \\
%     Third Address 1 \\
%     Third Address 2 \\
%     \texttt{third.email@example.com}
%   \end{tabular}
% \end{quote}
  

% Option 2. Author’s address is linked with superscript characters to its name,
% author names are grouped, centered.

% \begin{quote}\centering
%     \textbf{First Author$^\diamond$} \quad \textbf{Second Author$^\dagger$} \quad
%     \textbf{Third Author$^\ddagger$}
%     \\ \ \\
%     $^\diamond$First Affiliation \\
%     First Address 1 \\
%     First Address 2 \\
%     \texttt{first.email@example.com}
%      \\ \ \\
%      $^\dagger$Second Affiliation \\
%     Second Address 1 \\
%     Second Address 2 \\
%     \texttt{second.email@example.com}
%      \\ \ \\
%     $^\ddagger$Third Affiliation \\
%     Third Address 1 \\
%     Third Address 2 \\
%     \texttt{third.email@example.com}
% \end{quote}
  
% \fi

\newpage
\appendix
\section{Appendix}
\label{sec:appendix}

% To provide a comprehensive evaluation of the task-specific methods discussed in the main text, we summarize their performance on the widely used ScanQA and SQA3D datasets. These datasets serve as benchmarks for assessing the effectiveness of different approaches in handling 3D scene question answering tasks. Table~\ref{table4} presents the comparison of methods on the ScanQA dataset, while Table~\ref{table5} details the performance across various question types on the SQA3D dataset. These results highlight the strengths and limitations of each method, offering insights into their applicability and performance in 3D SQA scenarios.

\begin{figure*}[h]
\scriptsize % Use smaller font size for the overall figure
\centering
\begin{forest}
    for tree={
        forked edges,
        grow'=0,
        draw,
        rounded corners,
        node options={align=center},
        text width=2.2cm, % Adjust text width for better layout
        s sep=5pt, % Vertical spacing between nodes
        calign=child edge,
        calign child=(n_children()+1)/2, % Center align child nodes
    }
    [\textbf{3D SQA Categories}, % Root node
    text width=3cm, % Wider text for root
    fill=gray!30,
    for tree={fill=brown!45}
        [Datasets, for tree={fill=blue!40}
            [Dataset Structure, for tree={fill=blue!20}
                [Scene Modalities and Scale, for tree={fill=blue!10}
                    [Synthetic 3D Datasets]
                    [Point Cloud Datasets]
                    [Multi-View Datasets]
                    [Multimodal Datasets]
                %    [Instruction Tuning Datasets]
                ]
                [Query Modalities and Complexity, for tree={fill=blue!10}
                    [Basic Text Queries]
                    [Agent-Centric Text Queries]
                    [Multimodal Agent-Centric Queries]
                    [Instruction-Tuned Queries]
                ]
            ]
            [QA Pair Creation, for tree={fill=blue!20}
                [Methods for QA Pair Generation
                    [Template-Based Generation]
                    [Manual Annotation]
                    [LLM-Assisted Generation]
                ]
                [Question Design in 3D SQA
                [Task Complexity]
                    [Situated vs. Non-Situated]
                    [Temporal Aspec]
                ]
            ]
            [Evaluating LLM-Generated 3D Datasets, for tree={fill=blue!20}]
        ]
        [Evaluation Metrics, for tree={fill=green!40}
            [Traditional metrics]
            [LLM-based metrics]
        ]
        [Methodological Taxonomy, for tree={fill=violet!40}
            [Task-Specific Methods, for tree={fill=violet!20}
                [Point Cloud Methods]
                [Multi-view and 2D-3D Methods]
                [Advances in Text Encoders]
            ]
            [Pretraining-Based Methods, for tree={fill=violet!20}
                [Traditional Pretraining Approaches]
                [Instruction-Tuning Methods]
            ]
            [Zero-Shot Learning Methods, for tree={fill=violet!20}
                [Text-Driven Approaches]
                [Image-Driven Approaches]
                [Multimodal Alignment Approaches]
            ]
        ]
        [Challenges and Future Directions, for tree={fill=red!40}
            [Dataset Quality ]
            [Zero-Shot]
            [Unified Evaluation]
            [Dynamic and Open-World, text width=4.2cm] % Set specific width to prevent wrapping
            [Interpretable and Explainable, text width=4.2cm]
            [Multimodal and Collaboration, text width=4.2cm]
            [Incorporating Temporal Dynamics, text width=4.2cm]
        ]
    ]
\end{forest}
\caption{Graphical illustration of the hierarchical structure of 3D SQA literature adopted in this work. A systematic categorization is adopted for methodologies, datasets, and evaluation metrics.}
\label{fig:3d_sqa_structure}
\end{figure*}

\setlength{\tabcolsep}{4pt} % 调整列间距
\renewcommand{\arraystretch}{1.2} % 调整行间距
\begin{table*}[htp]
\centering
\footnotesize
\begin{tabular}{p{3.2cm}|p{1.1cm}p{1.1cm}p{1.1cm}p{1.1cm}p{1.1cm}p{1.1cm}p{1.1cm}p{1.1cm}p{1.1cm}}
\hline
\textbf{Model} & \textbf{EM@1} & \textbf{EM@10} & \textbf{B-1} & \textbf{B-2} & \textbf{B-3} & \textbf{B-4} & \textbf{R} & \textbf{M} & \textbf{C} \\ \hline
ScanQA~\shortcite{azuma2022scanqa}       & 21.05 & 51.23 & 30.24 & 20.40 & 15.11 & 10.08 & 33.30 & 13.14 & 64.90 \\
FE-3DGQA~\shortcite{zhao2022toward}      & 22.26 & 54.51 & -     & -     & -     & -     & -     & -     & -     \\
3DVLP~\shortcite{zhang2024vision}        & 24.03 & 57.91 & -     & -     & -     & -     & -     & -     & -     \\
CLIP-Guided~\shortcite{parelli2023clip}  & 23.92 & -     & 32.82 & -     & -     & 14.64 & 35.15 & 13.94 & 69.53 \\
Multi-CLIP~\shortcite{delitzas2023multi} & 24.02 & -     & 32.63 & -     & -     & 12.65 & 35.46 & 13.97 & 68.70 \\
LAMM~\shortcite{yin2024lamm}             & -     & -     & -     & -     & -     & -     & -     & -     & -     \\
3D-VisTA~\shortcite{zhu20233d}           & 27.00 & 57.90 & -     & -     & -     & 16.00 & 38.60 & 15.20 & 76.60 \\
3D-LLM~\shortcite{hong20233d3dllm}    & 21.20 & -     & 39.30 & 25.20 & 18.40 & 12.00 & 37.85 & 15.10 & 74.50 \\
SceneVerse~\shortcite{jia2025sceneverse} & 22.70 & -     & -     & -     & -     & -     & -     & -     & -     \\
ESZG~\shortcite{singh2024evaluating}     & 18.01 & 18.01 & 30.24 & 20.40 & 15.11 & 10.08 & 33.33 & 13.14 & 64.86 \\
SIG3D~\shortcite{man2024situational}     & -     & -     & 39.50 & -     & -     & 12.40 & 35.90 & 13.40 & 68.80 \\
Human                                   & 51.60 & -     & -     & -     & -     & -     & -     & -     & -     \\ \hline
\end{tabular}
\caption{Performance comparison of existing models on ScanQA datasets. \textbf{EM@1} and \textbf{EM@10} refer to exact match accuracy for top-1 and top-10 answers, respectively. \textbf{B-1} to \textbf{B-4} represent BLEU-1 to BLEU-4 scores. \textbf{R}, \textbf{M}, and \textbf{C} stand for ROUGE, METEOR, and CIDEr metrics, \textcolor{black}{respectively.} Higher values are more desirable for all metrics.} %NA: ARe higher values more desirable? Mention this in the caption.
\label{table4}
\end{table*}



\begin{table*}[h]
\centering
\footnotesize
\setlength{\tabcolsep}{5pt} % 调整列间距
\begin{tabular}{lccccccc}
\hline
\textbf{Model}        & \textbf{What} & \textbf{Is} & \textbf{How} & \textbf{Can} & \textbf{Which} & \textbf{Others} & \textbf{Avg} \\ \hline
ScanQA~\shortcite{azuma2022scanqa}                 & 28.60         & 65.00       & 47.30        & 66.30        & 43.90          & 42.90           & 45.30        \\
SQA3D~\shortcite{ma2022sqa3d}                 & 33.48         & 66.10       & 42.37        & 69.53        & 43.02          & 46.40           & 47.02        \\
SQA3D (GPT-3)~\shortcite{ma2022sqa3d}           & 39.67         & 45.99       & 40.47        & 45.56        & 36.08          & 38.42           & 41.00        \\
Multi-CLIP~\shortcite{delitzas2023multi}             & -             & -           & -            & -            & -              & -               & 48.00        \\
3D-VisTA~\shortcite{zhu20233d}               & 34.80         & 63.30       & 45.40        & 69.80        & 47.20          & 48.10           & 48.50        \\
3D-LLM~\shortcite{hong20233d3dllm}                 & 35.00         & 66.00       & 47.00        & 69.00        & 48.00          & 46.00           & 48.10        \\
LEO~\shortcite{huang2023embodied}                   & 46.80         & 64.10       & 47.00        & 60.80        & 44.20          & 54.30           & 52.90        \\
LM4Vision~\shortcite{pang2023frozen}             & 34.27         & 67.05       & 48.17        & 68.34        & 43.87          & 45.64           & 48.10        \\
SceneVerse\shortcite{jia2025sceneverse}            & -             & -           & -            & -            & -              & -               & 49.90        \\
SIG3D~\shortcite{man2024situational}                  & 35.60         & 67.20       & 48.50        & 71.40        & 49.10          & 45.80           & 52.60        \\
Spartun3D-LLM~\shortcite{zhang2024spartun3d}         & 49.40         & 67.30       & 47.10        & 63.40        & 45.40          & 56.60           & 54.90        \\
Human                 & 88.53         & 93.84       & 88.44        & 95.27        & 87.22          & 88.57           & 90.06        \\ \hline
\end{tabular}
\caption{Performance comparison of existing models on SQA3D datasets. The question types include "What," "Is," "How," "Can," "Which," and "Others," with the "Avg" column representing the average performance across \textcolor{black}{all types. The metric used is accuracy, and higher values are more desirable.}} %NA:  What metric is used? Are higher values more desirable? Mention this.
\label{table5}
\end{table*}



% \setlength{\tabcolsep}{3pt}
% \begin{table*}[!t]
% \centering
% \footnotesize

% \begin{tabular}{lccccc}
% \hline
% \textbf{Method} & \textbf{Question} & \textbf{Point} & \textbf{Image} & \textbf{Fusion} \\ \hline
% ScanQA~\shortcite{azuma2022scanqa}  & BiLSTM~\shortcite{graves2012long} & VoteNet~\shortcite{qi2019deep} & - & Transformer~\shortcite{vaswani2017attention} \\
% 3DQA-TR~~\shortcite{ye2021tvcg3dqa} & BERT~\shortcite{kenton2019bert} & Group-Free\shortcite{liu2021group} &  & 3D-L BERT~\shortcite{kenton2019bert} \\
% TransVQA3D\shortcite{yan2023comprehensive} & BERT~\shortcite{kenton2019bert} & PointNet++\shortcite{qi2017pointnet++} & - & SGAA \\
% FE-3DGQA~\shortcite{zhao2022toward} & T5~\shortcite{raffel2020exploring} & PointNet++~\shortcite{qi2017pointnet++} & - & Attention~\shortcite{vaswani2017attention} \\
% 3D-CLR~\shortcite{hong20233d} & CLIP~\shortcite{radford2021learning} & - & CLIP-LSeg~\shortcite{li2022language} & Spconv~\shortcite{contributors2022spconv} \\ 
% BridgeQA~\shortcite{mo2024bridging} & BLIP~\shortcite{li2023blip} & - & BLIP~\shortcite{li2023blip} & Twin-Transformer~\shortcite{chu2021twins} \\\hline
% \end{tabular}
% \caption{Task-Specific Methods. The table compares models based on the text encoder (Question), 3D point encoder (Point), image encoder (Image), and fusion technique (Fusion).}
% \label{table3}
% \end{table*}

% \begin{table*}[h]
% \centering
% \footnotesize
% \begin{tabular}{c|ccc}
% \hline
% \textbf{Method} & \textbf{Modality} & \textbf{Scene Encoder} & \textbf{Pretraining} \\ \hline
% \multicolumn{4}{c}{\textbf{Full Model Fine-Tuning}} \\ \hline
% 3DVLP~\shortcite{zhang2024vision} & \( S^{(p)}, S^{(m)} \) & PointNet++~\shortcite{qi2017pointnet++}  & CLIP~\shortcite{radford2021learning} \\ 
% CLIP-Guided~\shortcite{parelli2023clip}] & \( S^{(p)}, S^{(m)} \) & PointNet++~\shortcite{qi2017pointnet++}   & CLIP~\shortcite{radford2021learning} \\ 
% Multi-CLIP~\shortcite{delitzas2023multi}  & \( S^{(m)} \) & - & CLIP~\shortcite{radford2021learning} \\ 
% 3D-VisTA~\shortcite{zhu20233d} & Text, \( S^{(p)}, S^{(m)} \) & PointNet++~\shortcite{qi2017pointnet++}  & Transformer~\shortcite{vaswani2017attention} \\ 
% GPS~\shortcite{jia2025sceneverse} & \( S^{(p)}, S^{(m)} \) & PointNet++~\shortcite{qi2017pointnet++}  & Transformer~\shortcite{vaswani2017attention} \\ \hline
% \multicolumn{4}{c}{\textbf{Instruction Tuning}} \\ \hline
% LM4Vision~\shortcite{pang2023frozen} & Text, \( S^{(v)}, S^{(p)} \) & PointNet++~\shortcite{qi2017pointnet++}  & LLaMA \\ 
% 3D-LLM~\shortcite{hong20233d3dllm}  & Text, \( S^{(p)} \) & PointNet++~\shortcite{qi2017pointnet++}  & BLIP2~\shortcite{li2023blip} \\ 
% LEO\shortcite{huang2023embodied} & \( S^{(p)}, S^{(m)}, \text{Text} \) & PointNet++~\shortcite{qi2017pointnet++}  & Vicuna ~\shortcite{chiang2023vicuna}\\ 
% LAMM~\shortcite{yin2024lamm} & \( S^{(p)}, S^{(m)}, \text{Text} \) & PointNet++~\shortcite{qi2017pointnet++}  & Vicuna~\shortcite{chiang2023vicuna} \\ 
% M3DBench~\shortcite{li2023m3dbench} & \( S^{(p)}, S^{(v)}, \text{Text} \) & PointNet++~\shortcite{qi2017pointnet++}  & Vicuna~\shortcite{chiang2023vicuna} \\ \hline
% \end{tabular}
% \caption{Pretraining Methods for 3D Scene Question Answering.}
% \label{table:pretraining_methods}
% \end{table*}

% \begin{table*}[h]
% \centering
% \footnotesize
% \begin{tabular}{c|ccc}
% \hline
% \textbf{Method} & \textbf{Approach Type} & \textbf{VLM/LLM Used} & \textbf{Modality} \\ \hline
% SQA3D~\shortcite{ma2022sqa3d}&Text-Driven&GPT-3~\cite{brown2020language}&\( S^{(p)} \) \\ 
% LAMM~\shortcite{yin2024lamm}&Text-Driven&Vicuna-13B~\cite{chiang2023vicuna}&\( S^{(p)} \) \\ 
% EZSG~\cite{singh2024evaluating}&Image-Driven&GPT-4V~\cite{yang2023dawn}&\( S^{(p)} \) \\ 
% OpenEQA\shortcite{majumdar2024openeqa}&Image-Driven&GPT-4V~\cite{yang2023dawn}&\( S^{(m)} \) \\ 
% MSQA~\cite{linghu2024multi}&Image-Driven&GPT-4~\cite{achiam2023gpt}&\( S^{(p)} \) \\ 
% LEO\shortcite{huang2023embodied}&Multimodal Alignment&Vicuna-7B~\cite{chiang2023vicuna}&\( S^{(p)} \) \\ 
% Spartun3D-LLM~\shortcite{zhang2024spartun3d} &Multimodal Alignment&Vicuna-7B~\cite{chiang2023vicuna}&\( S^{(p)} \) \\ \hline
% \end{tabular}
% \caption{Comparison of Zero-Shot Methods for 3D QA.}
% \label{tab:zero_shot_methods}
% \vspace{-0.5cm}
% \end{table*}


% \begin{figure}[t]
% \tiny % Use smaller font size for the overall figure
% \begin{forest}
%     for tree={
%         forked edges,
%         grow'=0,
%         draw,
%         rounded corners,
%         node options={align=center,},
%         text width=2cm, % General text width for nodes
%         s sep=2pt, % Reduce spacing between nodes
%         calign=child edge, 
%         calign child=(n_children()+1)/2
%     }
%     [\textbf{\Huge \scriptsize 3D SQA}, % Adjust font size and text width for the root node
%     text width=0.6cm, % Adjust text width for the root node
%     for tree={fill=brown!45, text width=1.55cm}
%         [Task Specific Methods, for tree={fill=violet!40}
%             [Point Cloud-based Approaches, for tree={fill=violet!20}
%                 [ScanQA \shortcite{azuma2022scanqa}
%                 3DQA-TR \shortcite{ye2021tvcg3dqa} 
%                 FE-3DGQA \shortcite{zhao2022toward}
%                 TransVQA3D
%                 \shortcite{yan2023comprehensive}
%                 SIG3D~\shortcite{man2024situational}, for tree={fill=violet!10}]
%             ]
%             [Image-based Approaches, for tree={fill=violet!20}
%                 [3D-CLR \shortcite{hong20233d}
%                 , for tree={fill=violet!10}]
%             ]
%             [2D-3D Approaches, for tree={fill=violet!20}
%                 [
%                 BridgeQA \shortcite{mo2024bridging}, for tree={fill=violet!10}]
%             ]            
%         ]
%         [Pretraining-based Methods, for tree={fill=pink!40}
%             [Traditional Pretraining, for tree={fill=pink!20}
%                 [CLIP-Guided \shortcite{parelli2023clip}
%                 Multi-CLIP \shortcite{delitzas2023multi}
%                 3DVLP \shortcite{zhang2024vision}
%                 GPS \shortcite{jia2025sceneverse}
%                 3D-VisTA~\shortcite{zhu20233d}
%                 , for tree={fill=pink!10}]
%             ]
%             [Instruct-Tuning, for tree={fill=pink!20}
%                 [LM4Vision \shortcite{pang2023frozen}
%                 3D-LLM \shortcite{hong20233d3dllm}
%                 LEO \shortcite{huang2023embodied}
%                 M3DBench
%                 \shortcite{li2023m3dbench}
%                 LAMM\shortcite{yin2024lamm}, for tree={fill=pink!10}]
%             ]
%         ]
%         [Zero-Shot Learning Methods, for tree={fill=red!40}
%             [Text-drive Approaches, for tree={fill=red!20}
%                 [SQA3D \shortcite{ma2022sqa3d}
%                 LAMM \shortcite{yin2024lamm}, for tree={fill=red!10}]
%             ]
%             [Image-drive Approaches, for tree={fill=red!20}
%                 [EZSG \shortcite{singh2024evaluating}
%                 OpenEQA \shortcite{majumdar2024openeqa}
%                 MSQA \shortcite{linghu2024multi}, for tree={fill=red!10}]
%             ]
%             [Multimodal Approaches, for tree={fill=red!20}
%             [LEO \shortcite{huang2023embodied}
%             Spartun3D-LLM \shortcite{zhang2024spartun3d}, for tree={fill=red!10}]
%             ]
%         ]
%     ]
% \end{forest}
% %\caption{Multi-task based models.}
% \caption{Taxonomy of 3D SQA methods.}
% \label{fig:multi_task_models}
% \vspace{-0.5cm}
% \end{figure}




\begin{figure*}[h]
    \centering
    \includegraphics[width=0.95\linewidth]{Fig2.png}
  \caption{Illustration of the broader 3D SQA pipeline: scenes are represented as images or point clouds, questions as a combination of text and visual inputs, and locations as either textual descriptions or coordinates. Features are extracted using task-specific or pretrained encoders, fused in a dedicated module, and passed through an answer prediction head, typically an MLP. Recent methods integrate LVLMs into encoders and fusion modules, enabling zero-shot learning capabilities.}  
    % \caption{ Method. Most 3D SQA methods can be represented by a common pipeline. Scenes are represented as images or point clouds, questions combine textual and visual inputs, and locations are described using either textual descriptions or coordinates. These inputs are processed by specialized encoders to extract features, which are then integrated in a fusion module to generate answers. In task-specific methods, custom encoders and fusion modules are designed, with answers typically produced by a simple MLP. Pretraining-based methods involve pretraining encoders on large datasets, followed by fine-tuning the fusion module on 3D SQA datasets. In recent approaches, parts or all of the encoders and fusion modules have been replaced by large vision-language models (LVLMs), enabling zero-shot learning capabilities.}
    \label{fig:fig4}
\end{figure*} 

\begin{table*}[ht]
\centering
\footnotesize

\begin{tabular}{@{}p{0.3\textwidth}p{0.7\textwidth}@{}}
\toprule
\textbf{Task} & \textbf{Example Question} \\ \midrule
Object Identification & What is the object next to the red chair in the room? \\ \midrule
% Object Localization & Where is the red chair located in the room? \\ \midrule
Spatial Reasoning & Where is the table located relative to the sofa? \\ \midrule
Attribute Querying & What is the color of the sphere on the shelf? \\ \midrule
Object Counting & How many chairs are there in the room? \\ \midrule
Attribute Comparison & Which is taller, the lamp or the bookshelf? \\ \midrule
% Relational Reasoning & Is the cube on the left of the cylinder bigger than the cone on the table? \\ \midrule
Multi-hop Reasoning & Find the green bottle in the kitchen. What is on the shelf above it? \\ \midrule
Navigation & Guide the agent to the bedroom and locate the bedside table. \\ \midrule
Robotic Manipulation & Pick up the blue block and place it on the red cube. \\ \midrule
Object Affordance & What can be done with the knife on the counter? \\ \midrule
Functional Reasoning & How would you use the tools in the box to fix the broken chair? \\ \midrule
Multi-round Dialogue & User: Where is the TV? \newline Model: It is in the living room on the wall. \newline User: What is under the TV? \\ \midrule
Planning & Plan a sequence of actions to make a cup of tea using objects in the kitchen. \\ \midrule
Task Decomposition & Break down the task of assembling a desk into individual steps. \\
% Episodic Memory and Active Exploration & What objects were in the living room you visited earlier, and where were they? \\ 
\bottomrule
\end{tabular}
\caption{Examples of 3D SQA tasks, identified by their objectives, along with  representative example questions. The tasks cover a range of capabilities, including object identification, spatial reasoning, attribute querying, multi-hop reasoning, and planning. These tasks demonstrate the diverse applications and challenges addressed in 3D SQA, requiring models to integrate spatial, semantic, and task-specific understanding.}
\end{table*}
\label{app_table_q_examples}
\end{document}


