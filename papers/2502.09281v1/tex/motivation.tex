\section{Background and Motivation}
\label{sec:background}
We next discuss the main factor that has discouraged developers from using existing fast userspace stacks in public cloud VMs: incompatibility with cloud vNICs (\S\ref{sec:limitations-of-vnics}) and the needs of cloud applications (\S\ref{subsec:cloud-apps}).
We then detail the features an ideal portable stack should provide alongside characteristics we do not target (\S\ref{sec:requirements}).

\subsection{Incompatibility with cloud NICs}
\label{sec:limitations-of-vnics}
Cloud NICs (or vNICs) are a level of abstraction above bare-metal NICs and their virtual functions (VFs).
This is the fundamental reason why userspace stacks designed for bare-metal NICs and VFs might not work on vNICs.

An important factor in public clouds is that the fleet's network hardware spans multiple generations of hardware from multiple vendors, e.g., Microsoft's Azure cloud uses NICs spanning 12 years, from ConnectX-3 devices released in 2011~\cite{connectx3} to Microsoft's MANA devices released in 2023~\cite{mana, mana-kernel}.
To provide a consistent virtualized network with manageable complexity, the vNICs exposed to tenants typically provide a uniform feature set~\cite{gpoll, ec2poll, netvsc}.
Note that exceptions to this exist for specialized VM types targeting particular use cases (e.g., vNICs in VMs for High-Performance Computing support RDMA).

\begin{figure}[t!]
\centering
\includegraphics[width=0.9\columnwidth]{figures/drawio/vnic.drawio.pdf}
\caption{A logical comparison of bare-metal NICs, virtual functions, and cloud vNICs.}
\label{fig:diff-nic}
\vspace{-0.1in}
\end{figure}

Figure~\ref{fig:diff-nic} shows a qualitative comparison of three types of bare-metal NIC, virtual functions (VFs), and vNICs.
Bare-metal NICs expose the NIC's entire suite of features to the user, ranging from low-level knobs like RSS and flow steering to entire protocol layers like RDMA and Transport Layer Security (TLS)~\cite{Pismenny:asplos21}.
VFs are isolated NIC slices that share the physical NIC's hardware resources~\cite{vf}, which typically expose most but not all the physical NIC's features~\cite{gpoll, ec2poll, netvsc}.
Simple virtualization environments (e.g., on-premise data centers) typically dedicate a VF to each VM; the VM can access this VF from the guest's userspace if needed.

In contrast, guest VMs in public clouds do not have raw access to the virtual function since that would bypass the cloud's network virtualization layer.
The abstraction layer provides network virtualization~\cite{Koponen:nsdi14,azure_smartnic,andromeda}, which includes (1) tenant features like bring-your-own-IPs, firewalls, routing, and access control lists, and (2) cloud operator features like inter-tenant isolation, transparent network upgrades, servicing, monitoring, and VM migration.

Since Microsoft has published details about their Azure vNIC implementation~\cite{vfp, azure_smartnic}, we use it as a representative example.
In their ConnectX-based vNICs, the network virtualization layer spans an FPGA datapath, host hypervisor software, and the guest VM's paravirtual device driver~\cite{azure_smartnic}.
During network I/O, guest applications must use Azure's paravirtual ``netvsc'' drivers for both kernel- and kernel-bypass I/O instead of the virtual function's corresponding raw ConnectX drivers.
The paravirtual driver includes support for network virtualization events such as servicing (e.g., for the FGPA SmartNIC), monitoring, VM migration, etc.
In AWS, their purpose-built Nitro devices implement this layer~\cite{nitro}.
Google Cloud Platform (GCP) uses dedicated CPU cores for network virtualization~\cite{snap}.

Existing fast userpace stacks often aim to exploit advanced features available in bare-metal NICs: this is the case for example of flow steering~\cite{mtcp, erpc,demi-kernel}, multi-packet receive queues~\cite{erpc,virtuoso}, or deep receive queues~\cite{erpc}, to name a few.
One may expect those stacks to work as-is on vNICs with minor modifications. Unfortunately, we find that this is not the case and significant design and implementation changes are required.
The problem is that while the features they rely upon have been ubiquitous in bare-metal NICs for over a decade (e.g., Intel's 82599 Ethernet controllers released in 2009 support flow steering~\cite{intel-82599}), they are not yet part of the virtualized NICs exposed to guest VMs.

In Table~\ref{table:unsupported_features}, we list the features used in existing projects that are not supported by a large span of NICs deployed currently in the cloud.

\vspace{0.1in}
\noindent\fbox{\begin{minipage}{23.5em}
\noindent{\textbf{Consequence.}} A userspace network stack for the cloud needs to leverage \textit{only} the features available at cloud vNICs. Those depend on the network virtualization layer that exposes only a small subset of the ones available from bare-metal NICs.
\end{minipage}}

\subsection{Incompatibility with cloud applications}
\label{subsec:cloud-apps}
Cloud applications display a range of characteristics that match the dynamic and distributed nature of modern computing environments~\cite{cortez2017resource}.
These applications are typically written in various programming languages, including high-level ones such as JavaScript, C\#, Go, and Python~\cite{scio-net, meta-microservices}.
They are logically composed of diverse components, including databases~\cite{Chardonnay2023OSDI}, key-value stores~\cite{memcached, memcached-meta}, web servers~\cite{CaddyGitHub}, and authentication services~\cite{HashiCorpVault}, each serving distinct functions.
Multiple components are often co-located in the same VM to minimize the communication cost~\cite{ibench-interference}.
Furthermore, administrators need to leverage oversubscription, effectively running multiple processes or threads per core to maximize resource utilization~\cite{cortez2017resource}.
As we more extensively discuss in~\S\ref{subsec:sidecar}, the problem is that most of the userspace networking stacks~(full system solutions such as Shinjuku, or Demikernel~\cite{shinjuku, demi-kernel}) follow the libOS model~\cite{erpc, cornflakes, mtcp}, effectively compiling the networking stack together with the application, which in turn takes full ownership of the NIC.
This model is incompatible with the characteristics of cloud applications.


\vspace{0.1in}
\noindent\fbox{\begin{minipage}{23.5em}
    \noindent{\textbf{Consequence.}} A userspace network stack for the cloud should not be integrated with applications to ensure support for multiple programming languages and the ability to serve multiple applications at the same time.

\end{minipage}}

\subsection{Requirements from a cloud-native userspace stack.}
\label{sec:requirements}

In the following, we summarize the characteristics an ideal userspace stack designed for the cloud should provide:
\begin{itemize}
    \item \textbf{R1. NIC agnostic.} The stack should not depend on any NIC features other than those available in the vast majority of cloud vNICs (\S\ref{sec:lcd-cloudvnic}).
    % \item \textbf{R3: No threading constraints.} The stack should support an arbitrary number of threads to accommodate modern multi-threaded cloud applications (\S\ref{subsec:eval-multi-core}).
    % \item \textbf{R4: Blocking receives.} To support more threads than the number of CPU cores, the stack should allow applications to receive messages without polling (\S\ref{subsec:eval-multi-core}).
    %\item \textbf{R2: Support for a rich variety of languages.} The stack should provide bindings and accelerate networking for widely-used programming languages and runtimes (e.g., Go, C\#, JS, Rust, \S\ref{subsec:sidecar}).
    %\item \textbf{R3: Multiple processes per NIC.} The stack should support many applications and processes since rarely does a VM host a single application (\S\ref{subsec:sidecar}).
    \item \textbf{R2: Support for diverse applications.} The stack should support many processes at the same time since rarely a VM hosts just a single application (\S\ref{subsec:sidecar}).
    \item \textbf{R3: Low-latency.} The stack should not compromise on performance: it should be comparable with existing approaches that rely on specific NIC features (\S\ref{subsec:low-latency}).
\end{itemize}

% \subsection{Non-requirements}
% \label{sec:non_roadblocks}

\paragraph{Non-goal 1: Highly communication-intensive workloads.}
\label{sec:non_roadblocks}
This paper does not target massive-scale communication-intensive applications like storage and machine learning, already served by RDMA and other fabrics~\cite{Bai:nsdi23, azurehpc, efa}.
Instead, we target the long tail of cloud applications like distributed databases, caches, and stream-processing engines.
From our discussion with developers, we have found that the key metric of interest in these applications is \emph{latency}.
For example, while existing userspace stacks projects target tens of millions of requests per second~\cite{erpc,demi-kernel,farm,kv-driect} (at the cost of restricted application execution models), production workloads are often far less demanding in terms of message rate: an analysis of Twitter's production cache workload shows that most servers run under 100k requests per second~\cite{twitter-trace}.
Google's Snap paper mentions a server handling a few million requests per second at peak load~\cite{snap}.

\ul{\textit{Implication.}} We are willing to sacrifice message rate and bandwidth in favor of compatibility with a wide range of vNICs and application execution models as long as latency remains competitive~\cite{tail-at-scale, killer, Chardonnay2023OSDI}.
%Whereas prior stacks often reach tens of millions of messages per second, we targeted a few million messages per second and a few ten Gbps per CPU core.

\paragraph{Non-goal 2: Compatibility with unmodified applications.}
In our discussions with engineering teams, we found that few expect to run unmodified applications on a userspace stack.
Instead, developers are often willing to rewrite the networking parts of their application and, in some cases, even co-design the application with the stack.
This is intuitive for two reasons.
First, reducing latency often necessitates changes to the application's threading model, e.g., to avoid context switches, which go hand-in-hand with changes to the networking model.
Second, most applications do not directly use low-level networking APIs (e.g., POSIX) but project-specific middleware (e.g., gRPC or a hand-written messaging layer), limiting the rewriting effort to a small part of the code.

\ul{\textit{Implication.}} We do not target binary compatibility with the BSD sockets API.
An API that resembles it is sufficient.