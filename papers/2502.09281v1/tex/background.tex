\section{Roadblocks in cloud networks}

\label{sec:background}
We next discuss the two factors that have discouraged developers from using existing userspace stacks in public cloud VMs: incompatibility with vNICs (\S\ref{sec:limitations-of-vnics}), and diverse application execution models (\S\ref{sec:execution-models}).
We then discuss the features that we believe a userspace stack for cloud environments should provide (\S\ref{sec:requirements}), alongside features that we do not target (\S\ref{sec:non_roadblocks}).

\subsection{Incompatibility with cloud NICs}
\label{sec:limitations-of-vnics}
%\paragraph{Differences among bare metal, VFs, and vNICs.}
%\label{sec:nic-differences}
Cloud NICs (or vNICs) are a level of abstraction above bare-metal NICs and their virtual functions (VFs).
This is the fundamental reason why userspace stacks designed for bare-metal NICs, and VFs might not work on vNICs.
The abstraction layer provides network virtualization~\cite{Koponen:nsdi14,azure_smartnic,andromeda}, which includes (1) tenant features like bring-your-own-IPs, firewalls, routing, and access control lists, and (2) cloud operator features like inter-tenant isolation, transparent network upgrades, servicing, monitoring, and VM migration.

An important factor in public clouds is that the fleet's network hardware spans multiple generations of hardware from multiple vendors, e.g., Microsoft's Azure cloud uses NICs spanning 12 years, from ConnectX-3 devices released in 2011~\cite{connectx3} to Microsoft's MANA devices released in 2023~\cite{mana, mana-kernel}.
To provide a consistent virtualized network with manageable complexity, the vNICs exposed to tenants typically provide a uniform feature set~\cite{gpoll, ec2poll, netvsc}.
Note that exceptions to this exist for specialized VM types targeting particular use cases (e.g., vNICs in VMs for High-Performance Computing support RDMA).
%\ds{Existing research efforts such as Junction~\cite{junction:nsdi24}, Shenango~\cite{shenango}, Shinjuku~\cite{shinjuku}, TAS~\cite{tas}, eRPC~\cite{erpc}, Demikernel~\cite{demi-kernel}, IX~\cite{belay2014ix} benefited from advanced NIC features.
%Our work, however, focuses on the common case of general-purpose VMs.
%Table~\ref{table:unsupported_features} lists features~(discussed in \S\ref{subsec:unsupported-features}) used in existing projects that are not supported by a large span of NICs deployed currently in the cloud, which prevent developers from using these systems in the cloud.
%Further, existing solutions design custom runtime systems to allow faster network processing, including custom CPU schedulers. However, \mt{} focuses on the networking stack for the cloud and strives to offer a solution where cloud users can adopt it with minimal effort and systems knowledge.
%}

\begin{figure}[t!]
\centering
\includegraphics[width=0.9\columnwidth]{figures/drawio/vnic.drawio.pdf}
\caption{A logical comparison of bare-metal NICs, virtual functions, and cloud vNICs.}
\label{fig:diff-nic}
\end{figure}


Figure~\ref{fig:diff-nic} shows a qualitative comparison of three types of bare-metal NIC, virtual functions (VFs), and vNICs.
Bare-metal NICs expose the NIC's entire suite of features to the user, ranging from low-level knobs like RSS and flow steering, to entire protocol layers like RDMA and Transport Layer Security (TLS)~\cite{Pismenny:asplos21}.
VFs are isolated NIC slices that share the physical NIC's hardware resources~\cite{vf}, which typically expose most but not all the physical NIC's features~\cite{gpoll, ec2poll, netvsc}.
Simple virtualization environments (e.g., on-premise data centers) typically dedicate a VF to each VM; the VM has full access to this VF, from the guest's userspace if needed.

In contrast, guest VMs in public clouds do not have raw access to the virtual function, since that would bypass the cloud's network virtualization layer.
Since Microsoft has published details about their Azure vNIC implementation~\cite{azure_smartnic}, we use it as a representative example.
In their ConnectX-based vNICs, the network virtualization layer spans an FPGA datapath, host hypervisor software, and the guest VM's paravirtual device driver~\cite{azure_smartnic}.
During network I/O, guest applications must use Azure's paravirtual ``netvsc'' drivers for both kernel- and kernel-bypass I/O, instead of the virtual function's corresponding raw ConnectX drivers.
The paravirtual driver includes support for network virtualization events such as servicing (e.g., for the FGPA SmartNIC), monitoring, VM migration, etc.
In AWS, their purpose-built Nitro devices implement this layer~\cite{nitro}.
Google Cloud Platform (GCP) uses dedicated CPU cores for network virtualization~\cite{snap}.

\paragraph{Userspace network stacks and vNICs.}
\label{sec:existing-stacks-vnic}
One may expect stacks designed for bare-metal NICs to work as-is on vNICs with minor modifications.
However, since prior userspace stacks have almost exclusively targeted bare-metal NICs, we find that their design is often incompatible with vNICs, or significant design and implementation changes are required.
With ever-evolving features, bare-metal NICs have been a fruitful ground for exploring the performance limits of host networking.
Existing stacks often aim to exploit these unique features, ranging from flow steering~\cite{mtcp, erpc,demi-kernel}, multi-packet receive queues~\cite{erpc,virtuoso}, and programmable pipelines~\cite{syrup} \ds{(in Table~\ref{table:unsupported_features}, we list the features used in existing projects that are not supported by a large span of NICs deployed currently in the cloud)}.

In our review of seventeen recent papers in userspace networking, we find only two---Demikernel~\cite{demi-kernel} and Chardonnay~\cite{Chardonnay2023OSDI} (based on eRPC~\cite{erpc})---that test on vNICs.
These systems take a risky shortcut: in some cases (e.g., Azure's ConnectX VMs), the guest's access to the VF is not prevented by the cloud operator, allowing these systems a larger NIC feature set than provided by the paravirtual driver.
However, production applications cannot use this shortcut: the cloud operator provides technical support for only the official method; the VF may become unusable after servicing, and the VM may start up after a reboot on a different physical machine with a different NIC model~\cite{how-azure-accelerated-networking-works}.
%In addition, the network stacks enforce a restricted execution model that we found incompatible with many applications' needs (Section~\ref{sec:execution-models}).

%\paragraph{Modeling cloud NICs.}
%\label{sec:lcd-vnic-model}

%How should the designers of low latency userspace stacks model cloud NICs?
%The conceptual contribution of this paper is a model for vNICs that we term the ``least common denominator vNIC''.
%Our insight is that for broad adoption, userspace stacks should target the logical LCD vNIC, ignoring the bare-metal NICs' feature set.
%As we show in Section~\ref{sec:lcd-vnic-model}, these feature sets are \emph{very} different.

%We define the LCD as the common set of features available in the vNICs of the three major cloud providers (AWS, Azure, and GCP).
%We describe details of this model in Section~\ref{sec:lcd-vnic-model} and show how we can build a low latency userspace network stack on top of this model in Section~\ref{sec:overview}.

\subsection{Diverse application execution models}
\label{sec:execution-models}
Userspace stacks have historically sacrificed application execution flexibility for performance.
For example, the libOS-based approach---used by Sandstorm, libvma, Seastar, OpenOnload, eRPC, and Demikernel~\cite{Marinos:sigcomm14, libvma, seastar, openonload, erpc, demi-kernel} ---has been the standard, with a few notable exceptions~\cite{tas, snap}.
The libOS approach imposes several constraints on the application's execution model, the most important of which is that one application ``owns'' the NIC, so only one process can use the NIC at a time.
Additionally, each application thread typically owns one NIC queue, so the number of threads is limited by the small number of NIC queues on cloud vNICs.
Note that simply attaching many vNICs to the cloud VM is not a feasible solution to these problems, since it goes against the standard VM deployment model.

% \paragraph{Roots of the libOS model.} We believe that this restrictive model simply followed the roots of kernel-bypass networking in network function virtualization (NFV); a DPDK whitepaper presents a comprehensive history~\cite{dpdk-myth-busting-2020}.
% In NFV, the focus was on softwarizing packet-processing applications like telecom appliances (e.g., LTE gateways~\cite{Lange:ieee2015, Mohammadkhan:ieee2019}) and middleboxes~\cite{packetshader, cuckooswitch}, which were traditionally implemented in hardware;
% For example, a load balancer application is best structured as a single multi-threaded process with exclusive NIC queues per thread, and a libOS stack linked to the process.
% 
% For Internet service applications, like databases, caches, and stream-processing engines, the libOS model is restrictive.
% One server often runs multiple applications written in various languages, with possibly numerous threads per process.
% For example, Google production workloads are incompatible with this model since they run hundreds of threads per core and use high-level languages such as Go~\cite{google-workload}.

\subsection{A userspace stack for the cloud}
\label{sec:requirements}
How then should the designers of low latency userspace stacks model cloud NICs?
The conceptual contribution of this paper is a NIC model called the ``Least Common Denominator'' model.
We define the LCD as the common set of features available in all modern kernel-bypass capable NICs, including the vNICs of the three major cloud providers (AWS, Azure, and GCP).

Our insight is that for broad adoption of a stack, it must run seamlessly across bare-metal NICs, VFs, and vNICs; the LCD model captures the common features that stack designers can rely on.
The LCD NIC model is \emph{very} different from bare-metal NICs.
We describe its details in \S\ref{sec:lcd-vnic-model}, and then show how we can build a performant network stack on top of this model.

\paragraph{Feature requirements from a userspace stack.}
Based on the vNIC and the application execution model characteristics, a userspace stack must support the following to be useful in the cloud:

\begin{itemize}
    \item \textbf{R1. NIC agnostic.} The stack should not depend on any NIC features other than those available in the vast majority of cloud vNICs (\S\ref{subsec:mcloud}).
    % \item \textbf{R3: No threading constraints.} The stack should support an arbitrary number of threads to accommodate modern multi-threaded cloud applications (\S\ref{subsec:eval-multi-core}).
    % \item \textbf{R4: Blocking receives.} To support more threads than the number of CPU cores, the stack should allow applications to receive messages without polling (\S\ref{subsec:eval-multi-core}).
    \item \textbf{R2: Support for a rich variety of languages.} The stack should provide bindings and accelerate networking for widely-used programming languages and runtimes (e.g., Go, C\#, JS, Rust, \S\ref{subsec:raft}).
    \item \textbf{R3: Multiple processes per NIC.} The stack should support many applications and processes since rarely does a VM host a single application (\S\ref{subsec:app-isolation}).
\end{itemize}

\subsection{Non-roadblocks}
\label{sec:non_roadblocks}

\paragraph{Highly communication-intensive workloads.}
\mt{} does not target massive-scale communication-intensive applications like storage and machine learning, which are already served by RDMA and other fabrics~\cite{Bai:nsdi23, azurehpc, efa}.
Instead, we target the long tail of cloud applications like distributed databases, caches, and stream-processing engines.
From our discussion with developers, we have found that the key metric of interest in these applications is \emph{latency}.
For example, while exiting userspace stacks projects target tens of millions of requests per second~\cite{erpc,demi-kernel,farm,kv-driect} (at the cost of restricted application execution models), production workloads are often far less demanding in terms of message rate:
Analysis of Twitter's production cache workload shows that most servers run under 100k requests per second~\cite{twitter-trace}.
Google's Snap paper mentions a server handling a few million requests per second at peak load~\cite{snap}.

\ul{\textit{Implication.}} We are willing to sacrifice message rate and bandwidth in favor of compatibility with a wide range of NICs and application execution models, as long as latency remains competitive.
Whereas prior stacks often reach tens of millions of messages per second, we targeted a few million messages per second and a few ten Gbps per CPU core.

\paragraph{Compatibility with unmodified applications.}
In our discussions with engineering teams, we found that few expect to run unmodified applications on a userspace stack.
Instead, developers are often willing to rewrite the networking parts of their application, and in some cases even to co-design the application with the stack.
This is intuitive for two reasons.
First, reducing latency often necessitates changes to the application's threading model, e.g., to avoid context switches, which go hand-in-hand with changes to the networking model.
Second, most applications do not directly use low-level networking APIs (e.g., POSIX), but instead use either project-specific middleware (e.g., gRPC or a hand-written messaging layer), which limits the rewriting effort to a small part of the code.

\ul{\textit{Implication.}} We do not target binary compatibility with the BSD sockets API.
An API that resembles the sockets API is sufficient.


