\section{Evaluation}
\label{sec:evaluation}

\begin{table}
\centering
\begin{tabular}{lllll}
\toprule
\textbf{Cloud provider} & \textbf{Size} & \textbf{p50} & \textbf{p99} & \textbf{p99.9} \\
\toprule
Microsoft Azure & \byte{64} & 27 & 32 & 49 \\
\cline{2-5}
    & \kbyte{32} & 81 & 97 & 159 \\
\midrule
Amazon EC2 & \byte{64}& 48 & 53 & 57 \\
\cline{2-5}
    & \kbyte{32} & 224 & 240 & 257 \\
\midrule
Google Cloud & \byte{64} & 65 & 111 & 164 \\
\cline{2-5}
    & \kbyte{32} & 221 & 273 & 335 \\
\bottomrule
\end{tabular}
\caption{\mt{}'s round trip latency (\textmu{}s) for echo messages. The values across cloud providers should not be directly compared.}
\label{tab:all-three}
\vspace{-0.1in}
\end{table}

We show that \mt{} meets all three requirements set in \S~\ref{sec:requirements}.
In \S~\ref{subsec:cloud-platforms}, we show that it is compatible with the vNICs of all three major cloud providers (AWS, Azure, GCP).
In \S~\ref{subsec:faster}, \ref{subsec:raft}, \ref{subsec:app-isolation}, we show that \mt{} is compatible with the diverse application execution models of cloud applications.
Finally, in \S~\ref{subsec:microbenchm}, we show that \mt{} achieves latency comparable to or better than other userspace network stacks that do not adhere to the LCD model.
Throughout the evaluation, we compare \mt{} to three alternatives, noting that \mt{} is a network stack-only solution. We compared it against existing network stacks rather than full operating systems approaches, including Shenango~\cite{shenango}, and Junction~\cite{junction:nsdi24}~(\S\ref{disc:fops}):

\vspace{0.1cm}
\noindent\textbf{Linux TCP/IP.} We use this as a representative of OS-based approaches.
We use Sockperf~\cite{sockperf} for our experiments.
Libraries built atop this (e.g., gRPC) will have higher latency and lower throughput than Linux TCP/IP.

\vspace{0.1cm}
\noindent\textbf{eRPC.}~\cite{erpc} eRPC is not an LCD-compatible network stack, and it only runs on a limited number of Azure VMs that support RSS reconfiguration.
We use this (commit b8df1bc) to represent library-based stacks (e.g., Demikernel).
Unlike \mt{}, eRPC supports only one process per NIC and only one thread RX queue (= CPU core).

\vspace{0.1cm}
\noindent\textbf{TAS.} We use this~\cite{tas} (commit d3926ba) as a representative for sidecar-based stacks (e.g., Snap, which is closed-source).\\
\ul{\textit{Differences.}} Unlike \mt{}, TAS's sidecar process uses at least two CPU cores, one for the fast path and one for the slow path.
This presents a common problem for small 1--8 core cloud VMs (e.g., Cortez et al. reported that more than 95\%  of Azure VMs are 8-core or smaller~\cite{cortez2017resource}).
While TAS supports multiple application processes, it does not provide performance isolation among them (\S~\ref{subsec:app-isolation}) since TAS lacks a technique like \rssminus{}.

\noindent \textbf{Why no Snap}. Unfortunately, it cannot be directly compared due to its proprietary nature. Also, Snapâ€™s broader scope overlaps with \mt{} only in the PonyExpress component. 


\subsection{Evaluation setup}
\label{subsec:mcloud}
To demonstrate our LCD NIC model fits all modern DPDK-capable Ethernet NICs, we have tested \mt{} on various cloud and bare-metal NICs.
\mt{} works on Azure, AWS, and GCP, and it has been tested on several bare-metal NICs, including ConnectX-3 to ConnectX-6 and Intel E810.

\paragraph{Configuration.} Unless mentioned otherwise, we evaluate \mt{} on a cluster of VMs on Azure's public cloud.
Most of our optimization effort went to Azure, which provides the best latency (\S\ref{subsec:eval-latency}).
The VMs are in the same region and availability zone in all experiments.
The VMs have accelerated networking and run Ubuntu 22.04.2 with Linux kernel 6.2.
We do not enable proximity placement groups to keep results generally applicable, which can further reduce latency between VMs in the same availability zone.
Our experiments use eight-core F8s\_v2 VMs with \gbyte{16} DRAM.
We chose small VMs since those represent the large majority of VMs in public clouds, e.g., \citet{cortez2017resource} report that over 98\% of Azure VMs have eight or fewer CPU cores.

Unless mentioned otherwise, we use the smallest configuration for the \mt{} and TAS sidecars, i.e., one engine core for \mt{} and two cores for TAS (one each for its fast and slow path).
For the Linux TCP/IP stack, Nagle's algorithm is disabled.
We use huge pages for the DPDK-based stacks (i.e., \mt{}, TAS, and eRPC) and pin stack processes to CPU cores.
We omit the more invasive tuning techniques, such as isolated CPU cores (\texttt{isolcpus}) or interrupt steering.

\subsection{Supported cloud platforms}
\label{subsec:cloud-platforms}


Table~\ref{tab:all-three} shows that \mt{} successfully runs on all three major public clouds, achieving our primary goal of creating a userspace stack for this environment.
We used AWS's ``ENA express''\cite{aws-ena} networking and Google Cloud's newer gVNIC DPDK driver\cite{google2023gvnic}.
For the experiment, we measure the round-trip latency of \byte{64} and \kbyte{32} echo messages between two VMs in the same availability zone, each running a single-threaded echo application.
\mt{} achieves excellent tail latency on Azure and EC2, with the 99.9th percentile latency below \us{57} for \byte{64} messages.

Note that the latencies across cloud providers should not be directly compared since we have not yet used comparable VM types or hardware generations.
For example, we enabled placement groups to reduce the inter-VM distance on Amazon EC2 but not on the other clouds.
We believe that large-scale evaluation of userspace networking on public clouds---now possible with \mt{}---is an important direction for future work.


\subsection{FASTER key-value store over \mt{}}
\label{subsec:faster}
% \subsubsection{Raft over \mt{}}
\begin{figure}[t!]
    \centering
    \includegraphics[clip,scale=0.5]{figures/eval-faster-load-latency/faster_kv_load_latency_cnet.pdf}
    \caption{Performance of the FASTER key-value store over \mt{} and Linux TCP/IP.
    \mt{} achieves 3.3x higher throughput and 80\% lower p99 latency compared to Linux.}
    \label{fig:faster}
    \vspace{-0.1in}
\end{figure}

We next demonstrate \mt{}'s performance benefits and ease of use in end-to-end applications.
In all cases, we make no changes to the core application code and only modify their networking calls to use \mt{}.

FASTER is a production-grade high-performance key-value store~\cite{faster-kv,faster-github}.
For this experiment, we created one single-threaded server application that uses FASTER's C++-based in-memory hash table for storage (commit 0116754) and \mt{} for networking.
We pre-populate the server's hash table with 100 million eight-byte key-value pairs.
Two client VMs generate the workload, which consists of 100\% GET requests to emulate a read-heavy workload.
We vary the load by (1) increasing the number of concurrent connections from each client VM from one to 20 and (2) increasing the number of outstanding requests per connection from one to eight.
A third client VM acts as our latency probe, using one thread to send GET requests to the server one at a time.

\paragraph{Single-threaded performance.}
Figure~\ref{fig:faster} compares the throughput and latency achieved over \mt{} and Linux's TCP/IP stack.
The \mt{}-based implementation achieves 3.3x higher throughput, with 700K RPS compared to Linux TCP's 210K RPS.
At 210K RPS, \mt{}'s p99 latency is 80\% lower, achieving \us{50} compared to Linux TCP's \us{250}.
Another crucial improvement is in CPU utilization.
At its peak throughput, the in-kernel TCP stack causes 100\%  CPU utilization on all eight cores.
In contrast, the \mt{}-based implementation uses 100\% of only two CPU cores (application and one engine), i.e., 75\% lower.

\paragraph{Multi-threaded performance.}
\label{subsec:eval-multi-core}

\begin{figure}
    \centering
    \includegraphics[width=0.7\columnwidth]{figures/eval-multi-core/eval-mic-multi-core-msg.pdf}
    \caption{FASTER server performance with multiple threads}
    \label{tput:par:mmps}
    \vspace{-0.5cm}
\end{figure}

We evaluate \mt{}'s scalability to multiple engine threads based on \rssminus{} (Section~\ref{subsec:rssminus}) as follows.
The FASTER server application uses four threads, each pinned to a different CPU core.
We then run separate experiments with between one and four \mt{} engine threads.
We use four client VMs to generate the workload, to ensure no queuing at client side.
Figure~\ref{tput:par:mmps} shows that \mt{} scales well with the number of engines, handling 0.8 million requests per second (Mrps) with one engine and 2 Mrps with four engines.

\subsection{Golang-based Raft over \mt{}}
\label{subsec:raft}

State machine replication (SMR)~\cite{raft, paxos-made-simple,vr} is an essential part of cloud applications, used in various applications ranging from configuration management control planes~\cite{zookeeper,etcd,aws_tiny_dbs,delos}, to data-intensive systems~\cite{spanner,dynamodb,cockroachdb} that manage petabytes of data and millions of requests per second.
In SMR, each server in a cluster of servers runs an identical sequence of commands, allowing the service to withstand failures.
Since SMR protocols are often on the critical path of data-intensive systems, they must deliver low latency.
Production-grade implementations of SMR are highly complex, so integrating them with \mt{} is a good test of \mt{}'s generality.

For our study, we chose Hashicorp's production Raft implementation~\cite{hashicorp_raft} for two reasons.
First, it is written in a non-systems language (Go), which helps us demonstrate \mt{}'s broad applicability.
Second, it provides a generic network interface that permits different implementations.

\paragraph{Golang microbenchmark.}
We use cgo to implement Golang bindings for \mt{}'s shim library.
To evaluate its performance, we use a simple client-server echo application written in Go and compare its performance with its C++ counterpart for latency and throughput.
We use a similar setup as in Section~\ref{subsec:microbenchm}.
Our experiments show that the two perform similarly, with Golang performing slightly worse due to cgo's memory copy overhead.
For \kbyte{32} echo messages, the Go version has only 8\% higher p99 latency and 5\% lower throughput.

\paragraph{Raft benchmark.}
We compared our port of Hashicorp's Raft implementation to \mt{} with Hashicorp's official Linux TCP implementation.
Our Raft setup uses one client that issues commands (\byte{80} each, similar to eRPC's evaluation~\cite{erpc}) to the Raft cluster, which replicates them to an in-memory log.
We use three VMs, two running the SMR protocol (one leader and one follower) and one serving as a client load generator.
The client sends a request to the leader and waits for a response; the leader responds after replicating the data to its follower.
Each server VM has two active CPU cores: one for the application and one for \mt{}.

\begin{figure}[t!]
    \centering
    \includegraphics[clip,scale=0.5]{figures/raft/raft_cnet.pdf}
    \caption{Client-measured latency for Hashicorp's Golang-based Raft}
    \label{fig:raft}
    \vspace{-0.5cm}
\end{figure}

Figure~\ref{fig:raft} shows that \mt{} outperforms Linux kernel TCP, with 34\% lower median latency and 37\% lower median p99 latency.
With \mt{}, we achieve microsecond-scale latency even at the 99th percentile, where the Linux kernel TCP counterpart exceeds a millisecond.


\subsection{Supporting diverse execution models}
\label{subsec:app-isolation}

\textbf{Performance isolation} \mt{} avoids inter-application performance interference by using \rssminus{} to map flows to specific \mt{} engines.
TAS supports only random flow-to-engine mapping (determined by the NIC's randomized RSS) and, thus, suffers from interference.
Our experiment to demonstrate this works is as follows.
We run four single-threaded echo applications at a server VM: three throughput-intensive and one latency-sensitive.
Client VM\#1 generates high-rate traffic to the throughput-intensive applications, using 16 flows with eight \byte{64} in-flight messages per flow.
Client VM\#2 acts as a latency probe, sending one \byte{64} echo at a time to the latency-sensitive application.
Since only the sidecar-based approaches support multiple concurrent applications, we exclude eRPC from this evaluation.

\begin{figure}
    \centering
    \includegraphics[clip,scale=0.8]{figures/eval-iso/eval-iso-cnet.pdf}
    \caption{Inter-application performance isolation in \mt{}}
    \label{fig:eval-iso}
    \vspace{-0.1in}
\end{figure}


We use two engines for \mt{} at the server, one paired to the three throughput-intensive applications and one dedicated to the latency-sensitive application.
For TAS, we use two fast-path cores to match \mt{}'s fast-path cores in addition to TAS's one slow-path core.
Figure~\ref{fig:eval-iso} shows that with \mt{}, client VM\#2 achieves nearly 50\% better tail latency than TAS.
We also show the latency with \mt{} without isolation, i.e., when flows are randomly mapped to engines.
Without isolation, \mt{}'s tail latency, too, suffers from interference, showing the importance of \rssminus{}.

\textbf{Blocking receive.} We have found that a typical application pattern is one where there are more application threads needing network I/O than the number of CPU cores (Section~\ref{par:no-threading-constraints}), which requires support for blocking receive.
We evaluate \mt{}'s blocking receive with an experiment where a \mt{}-based echo server application runs multiple (up to four) threads, all sharing a single CPU core.
A single-threaded client application on a different VM generates a workload with \byte{64} request-response messages.

To measure latency, we configure the open-loop client to send load at full rate to the server.
Figure~\ref{fig:block-latency} shows that when the server applications use blocking receive, the client gets low tail latency even when the application threads share a single core.
In contrast, with multiple non-blocking server threads polling at the same CPU core, tail latency spikes to over \ms{10}.
This happens because a server thread may need to wait multiple Linux scheduling intervals before running.

Figure~\ref{fig:cpu-util} shows that blocking receives reduces CPU utilization.
We run the server with four threads for this experiment and configure the client to generate requests in an open loop, varying between 70K--700K requests per second (RPS).
With blocking receives, the server core's CPU utilization stays below 75\% even when handling 700K RPS.
In contrast, the CPU utilization of busy-polling is constantly 100\%, regardless of the load.
\subsection{Microbenchmarks}
\label{subsec:microbenchm}

%\subsubsection{Latency microbenchmark}


\begin{figure}[t!]
\centering
\includegraphics[clip,scale=0.6]{figures/eval-mic-latency/eval-mic-latency_cnet.pdf}
\caption{Round-trip latency for 64-byte messages.}
\label{fig:eval-micro-lat}
\vspace{-0.3cm}
\end{figure}

\label{subsec:eval-latency}
\textbf{Latency microbenchmark.} Figure~\ref{fig:eval-micro-lat} compares \mt{}'s latency with the alternatives in the latency experiment above, with \byte{64} messages.
We make two observations.

First, \mt{}'s latency is comparable to eRPC, despite \mt{}'s use of only the LCD NIC model and support for diverse execution models.
Specifically, \mt{}'s median and p99 latency is within 10\% of eRPC and 23\% at p99.9.
Taking median latency as the example, \mt{}'s \us{27} latency is \us{2} higher than eRPC\footnote{Demikernel~\cite{demi-kernel} reports \us{15} lower round-trip latency on Azure. Our latencies are different because we use a different region.}.
This extra latency comes primarily from SHM communication between the application and \mt{}'s sidecar process, which costs around \ns{250} for each one-way SHM crossing.
Interestingly, while this overhead is significant compared to a small bare-metal testbed (e.g., eRPC reports \us{2.3} within a rack), in large cloud data centers, the network's base latency dwarfs this overhead.

Second, Linux TCP's latency is 2.6x, 4.2x, and 7.4x higher than \mt{} at median, p99, and p99.9, respectively.
This confirms prior findings that kernel-bypass's latency benefits, arising from avoiding context switches and heavy kernel processing, are significant even in cloud datacenters~\cite{demi-kernel,Chardonnay2023OSDI}.


%\subsubsection{Throughput microbenchmark}
%\label{subsec:eval-throughput}

\textbf{Throughput microbenchmark.} Figure~\ref{fig:eval-micro-tput} compares the throughput of the alternatives.
The client sends variable-sized messages to the server, which echoes them back.
The client maintains eight outstanding messages, so the throughput in this experiment also depends on latency.
\mt{} saturates the VM's available network bandwidth (\Gbps{12} each way) with \kbyte{8} messages.
Linux's throughput for message sizes up to \kbyte{8} is low because of its higher latency; for larger \mbyte{100} messages, Linux also saturates the network.

TAS's throughput scales well with message size, but it achieves ~20--40\% lower throughput than \mt{} for message sizes larger than \kbyte{2}.
eRPC, although highly optimized for small RPCs and small networks (e.g., sub-\us{10} RTT), requires further transport-related configuration tuning for better throughput with larger RPCs on cloud networks (e.g., for its per-flow credits).
In this experiment, we ensured that eRPC's flows were not credit-starved.

\begin{figure}[t!]
    \centering
    \hspace{-0.7cm}
    \includegraphics[clip,scale=0.8]{figures/eval-single-tput-experiment/throughput_cnet.pdf}
    \caption{Unidirectional throughput for variable message sizes.}
    \label{fig:eval-micro-tput}
    \vspace{-0.1in}
\end{figure}
%\subsubsection{\rssminus{} connection setup latency}
%\label{subsubsec:eval-connection-setup}

\textbf{\rssminus{} connection setup latency.} On top of the network round-trip time, \mt{}'s connection setup latency is largely dictated by the configurable periodic interval that \mt{} processes new connection requests by the applications; for our experiments, this is set to 50us.
Furthermore, when multiple engines are present, \mt{} relies on \rssminus{}, a packet spraying technique, to establish connections. If a connection handshake fails to be completed on the first batch of sprayed packets, \mt{} retries with a larger batch of packets to increase entropy. On such rare occasions, the retry timeout is inflating the connection setup latency.
We run a microbenchmark to present the connection setup latencies with \mt{}; we use bare-metal machines on both ends to avoid variations due to cloud-related network latency spikes.
We vary the number of engines at client and server ends from 1 to 8~(these numbers configure NIC queues corresponding to the engines, too) and measure the latency of connection setup at the application by creating 10K distinct connections one at a time.

\begin{figure}[t!]
    \centering
    \includegraphics[clip,scale=0.6]{figures/eval-connection-setup-latency/conn_lat_cdf.pdf}
    \caption{Connection setup latency in \mt{}. In all cases, connection setup time at the median is less than $\sim 1$ millisecond. (legend example: 4x4 means the sender machine uses four engines, and the receiver machine uses four engines).}
    \label{fig:machnet-conn-setup}
    \vspace{-0.1in}
\end{figure}

Figure~\ref{fig:machnet-conn-setup} shows the results. At the median, \mt{}'s \rssminus{} can achieve less than 1.5 millisecond connection setup latency.
The connection setup latency slightly increases, particularly at the tail, as the likelihood of selecting the wrong SRC port number is higher.
If no SYN packet from the initial batch reaches the remote \mt{} engine, or if none of the response SYN-ACK packets reach the \mt{} sender engine, there will be a SYN timeout and retry with a new exponentially larger batch of initial SYN packets.

%\subsubsection{Blocking receive}
%\label{subsec:eval-blocking-non-blocking}



\begin{figure}[t!]
    \centering
    \subfloat[Tail latency]
    {
        \hspace{-0.9cm}
        \centering
        \includegraphics[clip,width=0.26\textwidth]{figures/eval-blocking-none-blocking/blocking-non-blocking-latency.pdf}
        \label{fig:block-latency}
    }
    \subfloat[CPU utilization]
    {
        \centering
        \includegraphics[clip,width=0.26\textwidth]{figures/eval-blocking-none-blocking/blocking-non-blocking-cpu-time.pdf}
        \label{fig:cpu-util}
    }
    \caption{Effectiveness of \mt{}'s blocking receives.}
    \label{fig:blocking-vs-non-blocking}
    \vspace{-0.2cm}
\end{figure}


% \subsubsection{Connection setup latency}
% \label{subsec:eval-connection-setup}
% \ak{This section will need to capture the}
% Does the use of packet spraying during the \rssminus{} connection handshake add excessive latency?
% We evaluate \mt{}'s connection setup latency using a pair of client-server VMs, with an increasing number of \mt{} engines at each end.
% We measure the time between the client sending the first SYN packet and receiving the correct SYN-ACK packet.
% Repeating this experiment with $N$ ranging between one to four \mt{} engines, we find that the connection setup latency increases from
% 64.8 $\pm$ \us{2.31} for $N=1$ to 69.6 $\pm$ \us{3.49} for $N=2$ to 72.8 $\pm$ \us{2.13} for $N=3$ and then stabilizes at 73 $\pm$ \us{2.28} for $N=4$ \mt{} engines.

