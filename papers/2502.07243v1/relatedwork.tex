\section{Related Work}
\textbf{Controllable Voice Imitation}\quad We focus primarily on how existing works approach the imitation of two key speech attributes: timbre and style. (1) \textbf{Imitation of Timbre}: As a crucial aspect of speaker identity, timbre imitation has been extensively explored within the voice conversion (VC) field. Most studies aim to utilize the speaker-agnostic representations such as PPG features~\cite{ppg-vc,voiceshop} or some self-supervised representations~\cite{self-supervised-vc,amphion-svc}, and use models including GAN~\cite{cyclegan-vc,stargan-vc}, auto-encoder~\cite{autovc,speechsplit}, and diffusion models~\cite{diffvc,diff-hiervc} to achieve timbre imitation. (2) \textbf{Imitation of Style}: In terms of style imitation, accent and emotion are two widely studied attributes. For conversion tasks (with speech as input), classic approaches often involve learning the conversion between parallel corpus~\cite{parallel-ac-zhaoguanlong21,parallel-ec-2016,voiceshop,convertandspeak}. Additionally, many studies aim to obtain the style-agnostic features, such as pushing them to be close to textual transcriptions~\cite{zhouyi-ac,chenxi-tts-ac,emovox,pavits}. Besides, leveraging automatic speech recognition (ASR) models can transform conversion tasks into synthesis tasks, allowing the injection of style label's embeddings into TTS models to achieve style imitation~\cite{asr-ac,liusongxiang-ac}. In conclusion, these existing approaches often rely on annotated data and struggle to achieve \textit{zero-shot} style imitation. (3) \textbf{Imitation of both Timbre and Style}: In VC, some works suggest adopting a sequence-to-sequence formulation~\cite{non-parallel-seq2seq-vc,lmvc} or introducing an additional modeling for prosody features~\cite{diff-hiervc,hierspeech++} to achieve both timbre and style imitation. However, these models still have significant room for improvement in both quality and style imitation. Recent advances in zero-shot TTS have greatly improved voice imitation and cloning. They leverage large-scale in-context learning to mimic all speech attributes of a reference prompt, including timbre and style, with high quality and speaker similarity~\cite{valle,megatts,ns3,seedtts,maskgct,u-style}. Nonetheless, it is challenging to obtain the speech representations disentangled timbre and style effectively~\cite{basetts,u-style}, leading to inadequate targeted control of these attributes. For instance, using the existing representations directly for VC tasks will lead to timbre leakage, unless mitigated by timbre perturbation or an additional fine-tuning stage~\cite{seedtts,maskgct}.

\textbf{Disentangled Speech Representation}\quad There are many studies aim to decouple linguistic content, timbre, and style. Existing work on obtaining disentangled speech representations can generally be categorized into several approaches: (1) Knowledge distillation using auxiliary tasks such as ASR, F0 prediction, and speaker verification~\cite{speech-resynthesis-interspeech21,ns3,basetts}, (2) Model architecture design based on information bottlenecks, including careful adjustments to hidden layer dimensions~\cite{autovc,speechsplit} or vector quantization methods like K-means~\cite{HuBERT,softvc,sef-vc-kmeans} or VQ-VAE~\cite{vq-vae,vqvc,vq-content-style,speech-resynthesis-interspeech21,ns3}, and (3) Perturbation of acoustic signals~\cite{nancy,nancypp,speechsplit2}. Besides, existing works also leverage additional learning strategies including adversarial learning~\cite{ns3,basetts}, comparative learning~\cite{contentvec,basetts}, and mutual information minimization~\cite{vq-content-style,mutual-information-zhuxinfa,mutual-information} to enhance disentanglement effectiveness. However, existing work still has two main weaknesses. On one hand, as mentioned earlier, finding suitable representations for downstream generation tasks that can effectively decouple timbre and style remains quite challenging. On the other hand, how to design voice imitation models that can control specific attributes based on these disentangled speech representations has been scarcely explored.