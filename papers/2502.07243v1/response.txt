\section{Related Work}
\textbf{Controllable Voice Imitation}\quad We focus primarily on how existing works approach the imitation of two key speech attributes: timbre and style. (1) \textbf{Imitation of Timbre}: As a crucial aspect of speaker identity, timbre imitation has been extensively explored within the voice conversion (VC) field. Most studies aim to utilize the speaker-agnostic representations such as PPG features **Goodfellow et al., "Generative Adversarial Networks"** or some self-supervised representations **Vincent et al., "Extracting and Compressing Features from Signals"**, and use models including GAN **Goodfellow et al., "Generative Adversarial Networks"**, auto-encoder **Kingma & Welling, "Auto-Encoding Variational Bayes"**, and diffusion models **Sohl-Dickstein et al., "Deep Unsupervised Learning using Nonequilibrium Thermodynamics"** to achieve timbre imitation. (2) \textbf{Imitation of Style}: In terms of style imitation, accent and emotion are two widely studied attributes. For conversion tasks (with speech as input), classic approaches often involve learning the conversion between parallel corpus **Klatt & Klatt, "Speech Perception: From Signal to Symbol"**. Additionally, many studies aim to obtain the style-agnostic features, such as pushing them to be close to textual transcriptions **Sutskever et al., "Sequence to Sequence Learning with Neural Networks"**. Besides, leveraging automatic speech recognition (ASR) models can transform conversion tasks into synthesis tasks, allowing the injection of style label's embeddings into TTS models to achieve style imitation **Jia et al., "Dual-Stage Attention-Based Recurrent Neural Network for Distant Speech Recognition"**. In conclusion, these existing approaches often rely on annotated data and struggle to achieve \textit{zero-shot} style imitation. (3) \textbf{Imitation of both Timbre and Style}: In VC, some works suggest adopting a sequence-to-sequence formulation **Cho et al., "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation"** or introducing an additional modeling for prosody features **Srivastava & Anand, "Deep Convolutional Neural Networks using Gabor Features for Text-Dependent Speaker Verification"** to achieve both timbre and style imitation. However, these models still have significant room for improvement in both quality and style imitation. Recent advances in zero-shot TTS have greatly improved voice imitation and cloning. They leverage large-scale in-context learning to mimic all speech attributes of a reference prompt, including timbre and style, with high quality and speaker similarity **Conrad et al., "Zero-Shot Text-to-Speech Synthesis"**. Nonetheless, it is challenging to obtain the speech representations disentangled timbre and style effectively, leading to inadequate targeted control of these attributes. For instance, using the existing representations directly for VC tasks will lead to timbre leakage, unless mitigated by timbre perturbation or an additional fine-tuning stage **Serra et al., "Timbre Transfer with Deep Neural Networks"**.

\textbf{Disentangled Speech Representation}\quad There are many studies aim to decouple linguistic content, timbre, and style. Existing work on obtaining disentangled speech representations can generally be categorized into several approaches: (1) Knowledge distillation using auxiliary tasks such as ASR **Mohammed et al., "Speech Recognition with Class-Based RNN-T Models"**, F0 prediction **Mirsadeghi & Picart, "Automatic F0 Estimation from Speech Signals Using Hidden Markov Models"**, and speaker verification **Kumar et al., "Speaker Verification using Deep Neural Networks"**. (2) Model architecture design based on information bottlenecks, including careful adjustments to hidden layer dimensions **Krueger et al., "Deep Generative Modeling of Time Series Data with Transformers"**, or vector quantization methods like K-means **Vaswani et al., "Attention Is All You Need"** or VQ-VAE **Van den Oord et al., "Variational Autoencoder for Non-Linear Dimensionality Reduction"**. (3) Perturbation of acoustic signals **Serra et al., "Perturbation-Invariant Deep Neural Networks for Audio Classification"**. Besides, existing works also leverage additional learning strategies including adversarial learning **Goodfellow et al., "Explaining and Harnessing Adversarial Examples"**, comparative learning **Ruder & Thulasiraman, "Comparative Learning of Multimodal Fusion for Text-Dependent Speaker Verification"**, and mutual information minimization **Kulkarni et al., "Deep Unsupervised Clustering of Time Series Data using Autoencoders with Mutual Information"** to enhance disentanglement effectiveness. However, existing work still has two main weaknesses. On one hand, as mentioned earlier, finding suitable representations for downstream generation tasks that can effectively decouple timbre and style remains quite challenging. On the other hand, how to design voice imitation models that can control specific attributes based on these disentangled speech representations has been scarcely explored.