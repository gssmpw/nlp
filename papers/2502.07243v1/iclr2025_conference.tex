
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{enumitem}
\usepackage{subcaption}
\usepackage{diagbox}
\usepackage{pifont}
% \usepackage[table]{xcolor}
\usepackage{colortbl}
\usepackage{threeparttable}

% \setlength{\floatsep}{5pt plus 2pt minus 2pt}
\setlength{\textfloatsep}{5pt plus 2pt minus 2pt}
% \setlength{\intextsep}{5pt plus 2pt minus 2pt}

\definecolor{lightgreen}{rgb}{0.88, 1, 0.88}
\definecolor{lightred}{rgb}{1, 0.88, 0.88}
\definecolor{lightyellow}{rgb}{1, 1, 0.88}
\definecolor{lightblue}{rgb}{0.88, 0.95, 1}
\definecolor{lightorange}{rgb}{1, 0.93, 0.88}

% versatile voice
% \title{Vevo: Zero-Shot Voice Imitation with Self-Supervised Disentanglement}
% \title{Vevo: Zero-Shot Voice Imitation with Controllable Timbre and Style through Self-Supervised Disentanglement}
\title{Vevo: Controllable Zero-Shot Voice Imitation with Self-Supervised Disentanglement}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

% \author{Antiquus S.~Hippocampus, Natalia Cerebro \& Amelie P. Amygdale \thanks{ Use footnote for providing further information
% about author (webpage, alternative address)---\emph{not} for acknowledging
% funding agencies.  Funding acknowledgements go at the end of the paper.} \\
% Department of Computer Science\\
% Cranberry-Lemon University\\
% Pittsburgh, PA 15213, USA \\
% \texttt{\{hippo,brain,jen\}@cs.cranberry-lemon.edu} \\
% \And
% Ji Q. Ren \& Yevgeny LeNet \\
% Department of Computational Neuroscience \\
% University of the Witwatersrand \\
% Joburg, South Africa \\
% \texttt{\{robot,net\}@wits.ac.za} \\
% \AND
% Coauthor \\
% Affiliation \\
% Address \\
% \texttt{email}
% }

% \author{Antiquus S.~Hippocampus, Natalia Cerebro \& Amelie P. Amygdale \thanks{ Use footnote for providing further information
% about author (webpage, alternative address)}
% \And
% Ji Q. Ren \& Yevgeny LeNet
% \AND
% Coauthor \\
% Affiliation \\
% Address \\
% \texttt{email}
% }

\author{
\makebox[\textwidth][l]
{\textbf{Xueyao Zhang}$^1$\thanks{Work accomplished during the internship at Meta.}\quad \textbf{Xiaohui Zhang}$^2$\quad \textbf{Kainan Peng}$^2$\quad \textbf{Zhenyu Tang}$^2$\quad \textbf{Vimal Manohar}$^2$,}\\
\makebox[\textwidth][l]{\textbf{Yingru Liu}$^2$\quad \textbf{Jeff Hwang}$^2$\quad \textbf{Dangna Li}$^2$\quad \textbf{Yuhao Wang}$^2$\quad \textbf{Julian Chan}$^2$\quad \textbf{Yuan Huang}$^2$}\\ 
\makebox[\textwidth][l]{\textbf{Zhizheng Wu}$^1$\thanks{Corresponding author.}\quad \textbf{Mingbo Ma}$^2$} \\
\makebox[\textwidth][l]{$^1$The Chinese University of Hong Kong, Shenzhen \quad $^2$Meta AI}
}



% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\newcommand{\note}[1]{\textcolor{blue}{{#1}}}
\newcommand{\task}[1]{\textcolor{red}{{#1}}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\vspace{-3mm}
\begin{abstract}
The imitation of voice, targeted on specific speech attributes such as timbre and speaking style, is crucial in speech generation. However, existing methods rely heavily on annotated data, and struggle with effectively disentangling timbre and style, leading to challenges in achieving controllable generation, especially in zero-shot scenarios. To address these issues, we propose Vevo, a \underline{ve}rsatile zero-shot \underline{vo}ice imitation framework with controllable timbre and style. Vevo operates in two core stages: (1) \textit{Content-Style Modeling}: Given either text or speech's \textit{content} tokens as input, we utilize an autoregressive transformer to generate the \textit{content-style} tokens, which is prompted by a style reference; (2) \textit{Acoustic Modeling}: Given the \textit{content-style} tokens as input, we employ a flow-matching transformer to produce acoustic representations, which is prompted by a timbre reference. To obtain the content and content-style tokens of speech, we design a fully self-supervised approach that progressively decouples the timbre, style, and linguistic content of speech. Specifically, we adopt VQ-VAE~\cite{vq-vae} as the tokenizer for the continuous hidden features of HuBERT~\cite{HuBERT}. We treat the vocabulary size of the VQ-VAE codebook as the information bottleneck, and adjust it carefully to obtain the disentangled speech representations. Solely self-supervised trained on 60K hours of audiobook speech data, without any fine-tuning on style-specific corpora, Vevo matches or surpasses existing methods in accent and emotion conversion tasks. Additionally, Vevo’s effectiveness in zero-shot voice conversion and text-to-speech tasks further demonstrates its strong generalization and versatility. 
% Audio samples are available at \href{https://versavoice.github.io/}{https://versavoice.github.io/}.
\end{abstract}

\section{Introduction}

The imitation of voice has long been an important issue in the field of speech generation. This includes the imitation of speaker identity~\cite{parallel-vc-survey-2017,vc-survey-taslp}, the imitation of speaking style such as accent~\cite{accent-conversion-2009,l2arctic} or emotion~\cite{esd}, and a broader concept of voice cloning such as in zero-shot text-to-speech (TTS) task~\cite{tts-book-tanxu}. 
These techniques have a wide range of applications, including spoken language learning~\cite{accent-conversion-2009,l2arctic,parallel-ac-zhaoguanlong21}, voice anonymization~\cite{vc-as-anonymization}, voice assistants~\cite{seedtts,fireredtts}, and video dubbing~\cite{seedtts,maskgct,fireredtts}.

To achieve targeted and controllable imitation over various speech attributes, many studies focuses on factorizing speech into multiple sub-spaces~\cite{speechsplit,speech-resynthesis-interspeech21,megatts,ns3}. In this work, we follow this idea and decompose speech into three key attributes: linguistic content (\textit{what to speak}), style (\textit{how to speak}), and timbre (\textit{who speaks}). Based on this, we define three zero-shot speech generation tasks (Table~\ref{tab:task}): (1) \textbf{Timbre Imitation}: Given a speech as source, imitate only the timbre of the reference speech while preserving the linguistic content and speaking style. It can be adopted in voice conversion that only spectral aspects of speech are converted~\cite{parallel-vc-survey-2017}. (2) \textbf{Style Imitation}: Given a speech as source, imitate only the speaking style of the reference speech while preserving the content and the timbre. It can be adopted in accent conversion~\cite{accent-conversion-2009} and emotion conversion~\cite{esd}. (3) \textbf{Voice Imitation}: Given either a speech (i.e., \textit{conversion task}) or text (i.e., \textit{synthesis task}) as source, imitate both the timbre and style of the reference speech while preserving the content. It can be adopted in voice conversion that both spectral and prosodic aspects of speech are converted~\cite{parallel-vc-survey-2017,vc-survey-taslp} and zero-shot TTS~\cite{tts-book-tanxu}.

\begin{table}[t]
\caption{Definitions of zero-shot timbre, style, and voice imitation tasks.}
\label{tab:task}
\begin{center}
\vspace{-4mm}
\resizebox{\textwidth}{!}{%
\begin{threeparttable}
    \begin{tabular}{c|c|c|c|c|c}
    \toprule
    \textbf{Task} & \textbf{Source} (\textcolor{blue}{$\bm{i}$}) & \textbf{Reference} (\textcolor{red}{$\bm{r}$}) & \textbf{Attribute(s) to Imitate} & \textbf{Target} & \textbf{Related Areas} \\
    \midrule
    Timbre Imitation & \multirow{3}{*}{\makecell[l]{\\ \\ $\mathcal{W}(c_{\textcolor{blue}{\bm{i}}}, s_{\textcolor{blue}{\bm{i}}}, t_{\textcolor{blue}{\bm{i}}})$}} & \multirow{4}{*}{\makecell[l]{\\ $\mathcal{W}(c_{\textcolor{red}{\bm{r}}}, s_{\textcolor{red}{\bm{r}}}, t_{\textcolor{red}{\bm{r}}})$}} & Timbre & $\mathcal{W}(c_{\textcolor{blue}{\bm{i}}}, s_{\textcolor{blue}{\bm{i}}}, t_{\textcolor{red}{\bm{r}}})$ & Voice Conversion \\
    \cmidrule(lr){1-1} \cmidrule(lr){4-4} \cmidrule(lr){5-5} \cmidrule(lr){6-6}
    Style Imitation &  &  & Style & $\mathcal{W}(c_{\textcolor{blue}{\bm{i}}}, s_{\textcolor{red}{\bm{r}}}, t_{\textcolor{blue}{\bm{i}}})$ & \makecell[l]{Accent Conversion,\\Emotion Conversion} \\
    \cmidrule(lr){1-1} \cmidrule(lr){4-4} \cmidrule(lr){5-5} \cmidrule(lr){6-6}
    \multirow{2}{*}{Voice Imitation} &  &  & \multirow{2}{*}{\makecell[l]{Timbre and Style}} & \multirow{2}{*}{$\mathcal{W}(c_{\textcolor{blue}{\bm{i}}}, s_{\textcolor{red}{\bm{r}}}, t_{\textcolor{red}{\bm{r}}})$} & Voice Conversion \\
    \cmidrule(lr){2-2} \cmidrule(lr){6-6}
     & $\mathcal{T}(c_{\textcolor{blue}{\bm{i}}})$ &  &  &  & Text to Speech \\
     \bottomrule
    \end{tabular}%
    \begin{tablenotes}
        \footnotesize{\item[*] $\mathcal{W}$ and $\mathcal{T}$ denote speech and text. $c_{\textcolor{blue}{\bm{i}}}$, $s_{\textcolor{blue}{\bm{i}}}$, and $t_{\textcolor{blue}{\bm{i}}}$ represent the linguistic content, style, and timbre of the source ${\textcolor{blue}{\bm{i}}}$. Similarly, $c_{\textcolor{red}{\bm{r}}}$, $s_{\textcolor{red}{\bm{r}}}$, and $t_{\textcolor{red}{\bm{r}}}$ represent the linguistic content, style, and timbre of the reference ${\textcolor{red}{\bm{r}}}$.} 
    \end{tablenotes}
    % \vspace{-10mm}
\end{threeparttable}
}
\end{center}
\end{table}
% \vspace{-20mm}

To address these imitation tasks, existing work has explored approaches including learning the conversion between parallel corpus~\cite{parallel-vc-2015,parallel-ec-2016,parallel-ac-zhaoguanlong21,voiceshop,convertandspeak}, disentangled representation learning~\cite{autovc,speechsplit,HuBERT,basetts,ns3,cosyvoice}, and large-scale in-context learning~\cite{tortoise-tts,valle,voicebox,uniaudio,seedtts}. However, these approaches still suffer from the following limitations.
Firstly, for the style imitation, existing methods rely heavily on supervision with annotated data, which is hard to collect and scale up. This reliance includes the use of parallel corpus~\cite{parallel-ac-zhaoguanlong21,voiceshop,convertandspeak}, style labels (such as categories of accent~\cite{asr-ac,voiceshop,convertandspeak} or emotion~\cite{emovox,pavits}), and textual transcriptions~\cite{asr-ac,chenxi-tts-ac,emovox,pavits}. Moreover, achieving \textit{zero-shot} style imitation—where a system can imitate an accent, emotion, or other speaking styles from just a few seconds of speech—remains a significant challenge.
Secondly, the decoupling of timbre and style in existing methods is still insufficient, making it challenging to control them independently, unless mitigated by some timbre (or style) perturbations or additional fine-tuning stages ~\cite{seedtts,maskgct,u-style}. 
% For instance, recent zero-shot text-to-speech models have made significant progress in simultaneously imitating timbre and style through large-scale in-context learning~\cite{seedtts,maskgct}. However, the speech representations they employ do not effectively disentangle timbre from style, leading to timbre leakage in the voice conversion task unless mitigated by timbre perturbation or an additional fine-tuning stage~\cite{seedtts,maskgct}.


Motivated by the above, this paper proposes Vevo, a \underline{ve}rsatile zero-shot \underline{vo}ice imitation framework with controllable timbre and style (Figure~\ref{fig:Vevo-pipeline}). It can serve as a unified framework for a wide range of zero-shot speech generation tasks. Vevo consists of two core stages: (1) \textbf{Content-Style Modeling} (\textit{Content to Content-Style}): Given a speech prompt as style reference, we generate \textit{content-style} tokens from the input \textit{content} tokens (or the input text). We employ the decoder-only autoregressive transformer~\cite{transformer,llama}, leveraging its powerful capability of continued generation to model style. (2) \textbf{Acoustic Modeling} (\textit{Content-Style to Acoustic}): Given a speech prompt as timbre reference, we generate acoustic representations (such as Mel spectrograms) from the input of \textit{content-style} tokens. We use a flow-matching transformer~\cite{flow-matching,dit}, which has been verified to excel in in-context learning and reconstructing high-quality audio~\cite{voicebox,audiobox,cosyvoice,fireredtts}, to achieve timbre-controllable generation. 

To obtain the \textit{content} and \textit{content-style} tokens of speech, we design a self-supervised method to decouple the timbre, style, and linguistic content gradually, which is similar to a progressive information filtering: (1) We firstly investigate the commonly used self-supervised speech pre-trained model, HuBERT~\cite{HuBERT}. We find that its \textbf{continuous} hidden features contain rich information about timbre, style, and linguistic content (Section~\ref{sec:results-effect-of-codebook-size}), making it a suitable initial stage for information filtering. (2) Inspired by existing works for disentangling speaker-agnostic representations~\cite{vq-vae,vqvc,vq-content-style,ns3}, we employ VQ-VAE~\cite{vq-vae} as a tokenizer for HuBERT to filter out timbre, resulting in \textbf{content-style tokens}. (3) Furthermore, we propose that the vocabulary size of the VQ-VAE codebook can function as the ``width" of the information bottleneck~\cite{autovc}. By reducing the vocabulary size, we can narrow the bottleneck and filter out not only timbre but also significant style information, thereby obtaining \textbf{content tokens}. Besides, we propose to reduce the consecutive duplicate units~\cite{mhubert-duration-reduction} of the content tokens, called \textit{duration reduction}, to further remove some style patterns such as unit-level duration.

% Our experimental results demonstrate the effectiveness of Vevo in various controllable zero-shot voice imitation tasks. 
The contributions of this paper are summarized as follows:
\vspace{-3mm}
\begin{itemize}[itemsep=0ex,leftmargin=3ex]
    \item We introduce a fully self-supervised approach that progressively decouple timbre, style, and linguistic content of speech. The resulting content-style tokens and content tokens enhance controllability in downstream speech generation tasks, particularly for timbre and style.
    \item We propose Vevo, a unified framework that enables versatile, controllable zero-shot voice imitation tasks. It significantly reduces the reliance on annotated corpora, facilitating self-supervised training and in-context learning that can easily be scaled up.
    \item Pre-trained on 60K hours of audiobook speech data without any fine-tuning on style-specific corpora, Vevo matches or even surpasses existing methods in accent and emotion conversion tasks -- notably, through zero-shot imitation. Additionally, Vevo's effectiveness in voice conversion and text-to-speech tasks further demonstrates its strong generalization and versatility.
\end{itemize}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.975\textwidth]{img/vevo-pipeline.png}
    \caption{Vevo inference pipeline. Notably, it can take \textit{either} speech \textit{or} text as input, and perform zero-shot imitation with controllable linguistic content (controlled by the source), style (controlled by the style reference), and timbre (controlled by the timbre reference) in a single forward pass.
    }
    \label{fig:Vevo-pipeline}
\end{figure}

\section{Related Work}

\textbf{Controllable Voice Imitation}\quad We focus primarily on how existing works approach the imitation of two key speech attributes: timbre and style. (1) \textbf{Imitation of Timbre}: As a crucial aspect of speaker identity, timbre imitation has been extensively explored within the voice conversion (VC) field. Most studies aim to utilize the speaker-agnostic representations such as PPG features~\cite{ppg-vc,voiceshop} or some self-supervised representations~\cite{self-supervised-vc,amphion-svc}, and use models including GAN~\cite{cyclegan-vc,stargan-vc}, auto-encoder~\cite{autovc,speechsplit}, and diffusion models~\cite{diffvc,diff-hiervc} to achieve timbre imitation. (2) \textbf{Imitation of Style}: In terms of style imitation, accent and emotion are two widely studied attributes. For conversion tasks (with speech as input), classic approaches often involve learning the conversion between parallel corpus~\cite{parallel-ac-zhaoguanlong21,parallel-ec-2016,voiceshop,convertandspeak}. Additionally, many studies aim to obtain the style-agnostic features, such as pushing them to be close to textual transcriptions~\cite{zhouyi-ac,chenxi-tts-ac,emovox,pavits}. Besides, leveraging automatic speech recognition (ASR) models can transform conversion tasks into synthesis tasks, allowing the injection of style label's embeddings into TTS models to achieve style imitation~\cite{asr-ac,liusongxiang-ac}. In conclusion, these existing approaches often rely on annotated data and struggle to achieve \textit{zero-shot} style imitation. (3) \textbf{Imitation of both Timbre and Style}: In VC, some works suggest adopting a sequence-to-sequence formulation~\cite{non-parallel-seq2seq-vc,lmvc} or introducing an additional modeling for prosody features~\cite{diff-hiervc,hierspeech++} to achieve both timbre and style imitation. However, these models still have significant room for improvement in both quality and style imitation. Recent advances in zero-shot TTS have greatly improved voice imitation and cloning. They leverage large-scale in-context learning to mimic all speech attributes of a reference prompt, including timbre and style, with high quality and speaker similarity~\cite{valle,megatts,ns3,seedtts,maskgct,u-style}. Nonetheless, it is challenging to obtain the speech representations disentangled timbre and style effectively~\cite{basetts,u-style}, leading to inadequate targeted control of these attributes. For instance, using the existing representations directly for VC tasks will lead to timbre leakage, unless mitigated by timbre perturbation or an additional fine-tuning stage~\cite{seedtts,maskgct}.

\textbf{Disentangled Speech Representation}\quad There are many studies aim to decouple linguistic content, timbre, and style. Existing work on obtaining disentangled speech representations can generally be categorized into several approaches: (1) Knowledge distillation using auxiliary tasks such as ASR, F0 prediction, and speaker verification~\cite{speech-resynthesis-interspeech21,ns3,basetts}, (2) Model architecture design based on information bottlenecks, including careful adjustments to hidden layer dimensions~\cite{autovc,speechsplit} or vector quantization methods like K-means~\cite{HuBERT,softvc,sef-vc-kmeans} or VQ-VAE~\cite{vq-vae,vqvc,vq-content-style,speech-resynthesis-interspeech21,ns3}, and (3) Perturbation of acoustic signals~\cite{nancy,nancypp,speechsplit2}. Besides, existing works also leverage additional learning strategies including adversarial learning~\cite{ns3,basetts}, comparative learning~\cite{contentvec,basetts}, and mutual information minimization~\cite{vq-content-style,mutual-information-zhuxinfa,mutual-information} to enhance disentanglement effectiveness. However, existing work still has two main weaknesses. On one hand, as mentioned earlier, finding suitable representations for downstream generation tasks that can effectively decouple timbre and style remains quite challenging. On the other hand, how to design voice imitation models that can control specific attributes based on these disentangled speech representations has been scarcely explored.

\section{Methodology}

% In this section, we will first introduce the proposed VQ-VAE tokenizer on HuBERT and explain how its vocabulary size can be adjusted to obtain content and content-style tokens (Section~\ref{sec:vq-vae-tokenizer}). Next, we will present the core design of Vevo (Figure~\ref{fig:Vevo-pipeline}), covering the content-style modeling based on autoregressive transformer (Section~\ref{sec:content-style-modeling}) and the acoustic modeling based on flow-matching transformer (Section~\ref{sec:acoustic-modeling}). Finally, we will display how Vevo can be applied to various zero-shot imitation tasks, including timbre, style, and voice imitation (Section~\ref{sec:Vevo-applications}).


\subsection{VQ-VAE tokenizer for HuBERT}\label{sec:vq-vae-tokenizer}

\textbf{Motivation}\quad To disentangle representations of different speech attributes, we adopt a VQ-VAE tokenizer~\cite{vq-vae} due to its demonstrated potential in disentangling high-level information within speech such as speaker-invariant features~\cite{vq-vae,vqvc,ns3}. In speech domain, it is common practice to apply VQ-VAE either directly on the raw waveform~\cite{vq-vae,vqvc,ns3} or on the self-supervised learning (SSL) based speech representations~\cite{speech-resynthesis-interspeech21,repcodec,fireredtts}. In this work, we choose to apply VQ-VAE based on SSL representations -- specifically, HuBERT~\cite{HuBERT}. The reasons are two fold: (1) HuBERT's continuous hidden features already contain rich information about timbre, style, and linguistic content, making them well-suited for reconstructing acoustic representations such as Mel spectrograms (Section~\ref{sec:results-effect-of-codebook-size}); (2) Self-supervised learning on speech could be also treated as a high-level knowledge distillation. VQ-VAE enables us to further information filtering and disentangling for the SSL features.

% In the original VQ-VAE paper, the authors demonstrated that by applying quantization to raw speech waveforms, without any form of linguistic supervision, it is feasible to learn a high-level abstract space that captures linguistic content within speech~\cite{vq-vae}. 

\textbf{Architecture}\quad The VQ-VAE consists of three components: Encoder, Vector Quantization (VQ), and Decoder. Formally, given the codebook $\bm{E} = [\bm{e}_1, \bm{e}_2, \dots, \bm{e}_K]
$ whose vocabulary size is $K$, taking HuBERT hidden features $\bm{x}$ as input, we get the reconstructed $\hat{\bm{x}}$ after the three modules:
\begin{equation}
\begin{split}
    \bm{z}_e(\bm{x}) &= \text{Encoder}(\bm{x}), \\
    \bm{z}_q(\bm{x}) &= \bm{e}_k,~\text{where}~k = \arg\min_j \|\bm{z}_e(\bm{x}) - \bm{e}_j\|_2, \\
    \hat{\bm{x}} &= \text{Decoder}(\bm{z}_q(\bm{x})),
\end{split}
\end{equation}
where $\bm{z}_q(\bm{x})$ is the quantized representation (i.e., token) of $\bm{z}_e(\bm{x})$ after VQ. The loss function consists of the reconstruction loss (whose weight is $\lambda$) and quantization loss (whose weight is $\beta$):
\begin{equation}
    \mathcal{L} = \lambda \|\bm{x} - \hat{\bm{x}}\|^2_2 + \beta \|\bm{z}_e(\bm{x}) - \bm{z}_q(\bm{x})\|^2_2.
\end{equation}
Note that there is no real gradient defined for $\bm{z}_q(\bm{x})$. We could utilize the straight-through gradient estimator or exponential moving average (EMA) as the optimization algorithm~\cite{vq-vae}. In this paper, we follow the design in~\cite{soundstream,repcodec} and use the EMA algorithm. We describe the specific module design of VQ-VAE in Appendix~\ref{sec:appendix-vq-vae-tokenizer}. 
Notably, the VQ-VAE model does not contain any downsampling or upsampling operations, thus preserving the sequence length of the input $\bm{x}$. In other words, for the 50 Hz frame-level HuBERT features~\cite{HuBERT}, we can also get 50 Hz frame-level tokens after VQ.

\textbf{Analysis of the Vocabulary Size of Codebook}\quad The quantization of HuBERT hidden features by VQ-VAE can be viewed as a form of \textit{lossy compression}. Inspired by AutoVC~\cite{autovc}, we propose that the vocabulary size of the VQ codebook acts as an information bottleneck. If the input $\bm{x}$ possesses sufficient speech information, reducing the vocabulary size $K$ from infinity to zero: (1) \textbf{When $K \rightarrow \infty$}, we consider the bottleneck to be extremely wide, capable of accommodating all information without any loss. 
% In this scenario, for every $\bm{x}$ of the training data, it is possible to find a corresponding $\bm{e}_k$ that almost identical to $\bm{x}$, resulting in a token whose information approximates that of $\bm{x}$ itself. 
(2) \textbf{As $K$ decreases}, more low-level acoustic information begins to be lost, such as spectral features related to timbre or prosodic features related to style. At a certain reduced $K$, only the highest-level, most abstract information like linguistic content is preserved within $\bm{x}$. (3) \textbf{When $K \rightarrow 0$}, the bottleneck becomes exceedingly narrow, filtering out even high-level information like linguistic content. 
% If such tokens were used for speech reconstruction, the intelligibility of the generated speech would likely be extremely poor.
We validate the above hypothesis through experiments on the zero-shot timbre imitation task (Section~\ref{sec:results-effect-of-codebook-size}). Interestingly, as we progressively reduce $K$, we observe that timbre information is the first to be filtered out (assuming when $K=K_s$), from which we derive the \textit{content-style} tokens. Subsequently, most style information is filtered, and ultimately, almost only the highest-level linguistic content information is retained (assuming when $K=K_c$), from which we derive the \textit{content} tokens. We refer to the VQ-VAE model whose $K=K_s$ as the content-style tokenizer $\bm{Q}_s$, and the model whose $K=K_c$ as the content tokenizer $\bm{Q}_c$.


\subsection{Content-Style Modeling (Content to Content-Style)}\label{sec:content-style-modeling}

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.42\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/model-ar-training.png}
        \caption{Training}
        % \vspace{-1mm}
        \label{fig:model-ar-training}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.57\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/model-ar-inference-only-continued.png}
        \caption{Inference: \textit{reference-style-enhanced} continuation}
        % \vspace{-1mm}
        \label{fig:model-ar-inference-enhanced}
    \end{subfigure}
    \caption{Content-style modeling based on autoregressive transformer. During inference, we employ both global style encoder and content-style tokenizer to enhance the effect of the style reference.
    % we can either (1) use only global style encoder to inject the guidance of the style reference -- termed \textit{reference-global-guided} continuation, as detailed in Appendix~\ref{sec:appendix-content-sylte-modeling}; or (2) employ both global style encoder and content-style tokenizer to enhance the effect of the style reference -- termed \textit{reference-style-enhanced} continuation, as shown in the right sub-figure.
    }
    \label{fig:model-ar}
\end{figure}
% \footnotetext{This figure only presents for speech as input of Vevo. We demonstrate a similar figure for text as input of Vevo in Appendix~\ref{sec:appendix-content-style-modeling-text}.}

During the content-style modeling stage, our goal is to transform the content token of speech (or text) into content-style tokens, which is prompted by a style reference. This can be formulated as a sequence-to-sequence generation task. For this stage, we employ a decoder-only autoregressive (AR) transformer, known for its powerful capability in such tasks~\cite{transformer,llama,seedtts}. In this section, we will focus only on cases where speech's content tokens are used as input (Figure~\ref{fig:model-ar}). The scenarios where text serves as input will be discussed in Appendix~\ref{sec:appendix-content-style-modeling-text}.

\textbf{Duration Reduction}\quad Given a speech input ${u}$, we denote the content and content-style tokens as $\bm{Q}_c ({u})$ and $\bm{Q}_s ({u})$. Both of them are 50 Hz frame-level representations of equal length. In the content-style modeling stage, $\bm{Q}_s ({u})$ is used as the output. However, instead of using $\bm{Q}_c ({u})$, we apply a \textit{Duration Reduction} strategy to it, yielding the reduced $\bm{Q}_c^{'} ({u})$ as the input. Specifically, we merge the consecutive duplicate units of $\bm{Q}_c ({u})$ into one. For instance, if $\bm{Q}_c ({u}) = [\bm{e}_1, \bm{e}_1, \bm{e}_1, \bm{e}_2, \bm{e}_3, \bm{e}_3]$, it will be condensed to $\bm{Q}_c^{'} ({u}) = [\bm{e}_1, \bm{e}_2, \bm{e}_3]$. 
This strategy offers significant benefits: (1) It further filters out style-specific information within $\bm{Q}_c ({u})$ such as the unit-level duration. Some studies also point out that such a reduction could aid in reducing accents and other style elements~\cite{mhubert-duration-reduction}; (2) It resolves the model's challenge with learning changes in sequence length before and after style modeling when $\bm{Q}_c ({u})$ and $\bm{Q}_s ({u})$ are always equal in length; (3) It shortens the overall sequence length, which is beneficial to model context for transformer.

\textbf{Global Style Encoder}\quad We design a global style encoder to capture the global style guidance from the speech input ${u}$, producing a style embedding (denoted as $\bm{g} ({u})$). Its advantage comes from the flexibility during inference: if we aim to optimize inference speed and reduce memory usage, we can rely solely on this style embedding for style guidance, named as \textit{reference-global-guided} continuation (Figure~\ref{fig:model-ar-inference-global}). However, to maximize the performance of style imitation, in addition to using $\bm{g} ({u})$, we can also append the style reference's content-style tokens into the input sequence to enhance its effect, named as \textit{reference-style-enhanced} continuation (Figure~\ref{fig:model-ar-inference-enhanced}). The global style encoder consists of WavLM-based representation layers and TDNN-based feature extraction layers~\cite{wavlm,ecapa-tdnn}. We describe the detailed module design in Appendix~\ref{sec:appendix-content-sylte-modeling}.

\textbf{Training and Inference}\quad During training, we conduct self-supervised learning on speech data. The input sequence of transformer is $[\langle \text{SOS} \rangle, \bm{Q}_{c}^{'} ({u}), \langle \text{SEP} \rangle, \bm{g} ({u}), \langle \text{SEP} \rangle, \bm{Q}_{s} ({u})]$. We only perform the next token prediction on the last $[\langle \text{SEP} \rangle, \bm{Q}_{s} ({u})]$, with the ground truth being $[\bm{Q}_{s} ({u}), \langle \text{EOS} \rangle]$. Here, $\langle \text{SOS} \rangle$, $\langle \text{SEP} \rangle$, and $\langle \text{EOS} \rangle$ are treated as three special tokens in language model~\cite{bert}. During inference, for a source speech ${u}_i$ and a style reference ${u}_{sr}$, we can conduct the reference-style-enhanced continuation (Figure~\ref{fig:model-ar-inference-enhanced}) by feeding the input sequence $[\langle \text{SOS} \rangle, \bm{Q}_{c}^{'} ({u}_{sr} \oplus {u}_i), \bm{g} ({u}_{sr}), \bm{Q}_s ({u}_{sr})]$ for autoregressive generation, where $\oplus$ means the concatenation. For reference-global-guided continuation (Figure~\ref{fig:model-ar-inference-global}), the input sequence becomes $[\langle \text{SOS} \rangle, \bm{Q}_{c}^{'} ({u}_i), \bm{g} ({u}_{sr})]$.

\subsection{Acoustic Modeling (Content-Style to Acoustic)}\label{sec:acoustic-modeling}

\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.35\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/model-diffusion-training.png}
        \caption{Training}
        % \vspace{-2mm}
        \label{fig:model-diffusion-training}
    \end{subfigure}
    % \hfill
     \hspace{5mm}
    \begin{subfigure}[b]{0.46\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/model-diffusion-inference.png}
        \caption{Inference}
        % \vspace{-2mm}
        \label{fig:model-diffusion-inference}
    \end{subfigure}
    \caption{Acoustic modeling based on a flow-matching transformer. During inference, we append the timbre reference to the rightmost (or leftmost) end, enabling timbre-controllable generation.
    % During training, we use a randomly span-masked Mel spectrogram to inject timbre information. During inference, we append the timbre reference to the rightmost (or leftmost) end, enabling timbre-controllable generation. \task{notation for figure 2 and 3} 
    }
    \label{fig:model-diffusion}
\end{figure}


During the acoustic modeling stage, prompted by a timbre reference, we aim to transform the content-style tokens to Mel spectrograms. We adopt a flow matching transformer~\cite{flow-matching,transformer,llama} (Figure~\ref{fig:model-diffusion}), which has been verified to be effective in in-context learning and reconstructing high-quality acoustic representations~\cite{voicebox,audiobox,cosyvoice,fireredtts}. 

During training, given a speech ${u}$ and its Mel spectrogram $\bm{y}_1$, we randomly select a part of $\bm{y}_1$ as the timbre reference (denoted as $\bm{y}_1^{ctx}$), and aim to reconstruct the other part (denoted as $\bm{y}_1^{mis}$) conditioned on $\bm{y}_1^{ctx}$ and the content-style tokens $\bm{Q}_s({u})$. In other words, we aim to model the conditional probability $p(\bm{y}_1^{mis} | \bm{y}_1^{ctx}, \bm{Q}_s({u}))$. Specifically, we follow Voicebox~\cite{voicebox} and use a temporal span masking strategy: $\bm{y}_1^{mis} = \bm{m} \odot \bm{y}_1$, and $\bm{y}_1^{ctx} = (\bm{1} - \bm{m}) \odot \bm{y}_1$, where $\bm{m}$ is a binary temporal mask that is of the same length as $\bm{y}_1$, and $\odot$ means the element-wise multiplying operation. During inference, given a source speech ${u}_i$ and a timbre reference ${u}_{tr}$, all the source's Mel spectrogram will be masked (i.e., $\bm{y}_1^{mis}$). The input conditions become the timbre reference's Mel spectrogram (i.e., $\bm{y}_1^{ctx}$) and the concatenated content-style tokens $\bm{Q}_s ({u}_i \oplus {u}_{tr})$. This enables the generated target to preserve the linguistic content and style of ${u}_i$, and the timbre of ${u}_{tr}$ (Figure~\ref{fig:model-diffusion-inference}).

We use the conditional flow matching algorithms based on optimal transport path, which is widely adopted in related works~\cite{voicebox,cosyvoice,fireredtts}. The loss function is defined as:
\begin{equation}
\begin{split}
    \mathcal{L}_{cfm} &= \mathbb{E}_{t,\bm{m},\bm{y}_0,\bm{y}_1}  \left\| \frac{d \bm{y}_t}{d t} - f_t(\bm{y}_t, t, \bm{y}_1^{ctx}, \bm{Q}_s({u})) \right\|_2^2, \\
    \text{where}~\bm{y}_t &= (1 - (1 - \sigma)t) \cdot \bm{y}_0 + t \cdot \bm{y}_1,
\end{split}
\end{equation}
where $t$ is the time step that is sampled from the uniform distribution $\mathcal{U}(0, 1)$, $\bm{y}_0$ is a noise sampled from standard Gaussian distribution, $f_t(\cdot)$ is the vector filed (which is estimated by transformer). and $\sigma$ is a small constant of the optimal transport (OT) path. 
% We follow the implementation of Voicebox~\cite{voicebox} and only compute the loss for masked frames. 
Notably, the frame rates of the content-style tokens $\bm{Q}_s({u})$ and the Mel spectrogram $\bm{y}_1$ could be different. We follow~\cite{amphion-svc} and use a simple signal resampling operation to align them. Then we use the adding operation to fuse their frame-level features. We describe the detailed module design in Appendix~\ref{sec:appendix-acoustic-modeling}. After obtaining the Mel spectrogram, we utilize a BigVGAN~\cite{bigvgan} vocoder to produce the waveform (Appendix~\ref{sec:appendix-vocoder}).


\subsection{Vevo for Various Zero-Shot Imitation Tasks}\label{sec:Vevo-applications}

Assume that during the content-style modeling and acoustic modeling stages, we have obtained pre-trained models $\mathcal{M}_{style}$ and $\mathcal{M}_{acoustic}$ respectively. We can then adjust only the inference pipeline to apply Vevo to various zero-shot imitation tasks. Given the source speech ${u}_{\textcolor{blue}{\bm{i}}}$ (or text $\mathcal{T}_{\textcolor{blue}{\bm{i}}}$) and the reference ${u}_{\textcolor{red}{\bm{r}}}$, we can utilize the following variants of Vevo to achieve zero-shot timbre, style, and voice imitation tasks (`` $\xrightarrow{u} \mathcal{M}$ " means that the model $\mathcal{M}$ is prompted by $u$ to generate):
\vspace{-3mm}
\begin{itemize}[itemsep=0ex,leftmargin=5ex]
    \item \textbf{Vevo-Timbre} for timbre imitation: $\bm{Q}_s ({u}_{\textcolor{blue}{\bm{i}}}) \xrightarrow{\displaystyle {{u}_{\textcolor{red}{\bm{r}}}}} \mathcal{M}_{acoustic}$
    \item \textbf{Vevo-Style} for style Imitation: $\bm{Q}_c^{'} ({u}_{\textcolor{blue}{\bm{i}}}) \xrightarrow{\displaystyle {u}_{\textcolor{red}{\bm{r}}}} \mathcal{M}_{style} \xrightarrow{\displaystyle {u}_{\textcolor{blue}{\bm{i}}}} \mathcal{M}_{acoustic}$
    \item \textbf{Vevo-Voice} for voice imitation (conversion task): $\bm{Q}_c^{'} ({u}_{\textcolor{blue}{\bm{i}}}) \xrightarrow{\displaystyle {u}_{\textcolor{red}{\bm{r}}}} \mathcal{M}_{style} \xrightarrow{\displaystyle {u}_{\textcolor{red}{\bm{r}}}} \mathcal{M}_{acoustic}$
    \item \textbf{Vevo-TTS} for voice imitation (synthesis task): $\widetilde{\bm{Q}_c} (\mathcal{T}_{\textcolor{blue}{\bm{i}}}) \xrightarrow{\displaystyle {u}_{\textcolor{red}{\bm{r}}}} \widetilde{\mathcal{M}}_{style} \xrightarrow{\displaystyle {u}_{\textcolor{red}{\bm{r}}}} \mathcal{M}_{acoustic}$
\end{itemize}
% \vspace{-3mm}
For Vevo-TTS, $\widetilde{\bm{Q}_c} (\mathcal{T}_{\textcolor{blue}{\bm{i}}})$ means the tokenization for $\mathcal{T}_{\textcolor{blue}{\bm{i}}}$, and $\widetilde{\mathcal{M}}_{style}$ means the pre-trained model for content-style modeling that takes text as input. We describe its detailed design in Appendix~\ref{sec:appendix-content-style-modeling-text}.

\section{Experiments}

\textbf{Training Data}\quad We train the English-only models on 60K hours of ASR-transcribed English audiobooks, which is the same as the dataset used by the Voicebox English model~\cite{voicebox}. The model $\mathcal{M}_{acoustic}$ and $\mathcal{M}_{style}$ are trained solely with speech data. The model $\widetilde{\mathcal{M}}_{style}$, which uses text as input, is trained with both speech and textual transcriptions data. We begin with the publicly available {HuBERT-Large}\footnote{\href{https://pytorch.org/audio/0.10.0/pipelines.html\#hubert-large}{https://pytorch.org/audio/0.10.0/pipelines.html\#hubert-large}\label{fn:hubert-large}} model~\cite{HuBERT} to prepare the VQ-VAE tokenizer. We utilize its hidden features from the 18th layer as the reconstruction objective for the tokenizer. Both the content and content-style tokenizers are trained on a 100-hour subset randomly sampled from the full 60K-hour dataset.

% ====== L2-Arctic and ESD ====== 
% \textbf{Evaluation Data}\quad We consider various evaluation settings to construct the evaluation set: (1) For clean data, such as recordings made in studio environments, we select audiobook speech data. Specifically, we reserve a subset of the total 60K hours of data as evaluation samples, which we denote as AB. (2) For noisy data, which may include in-the-wild recordings and diverse recording devices, we use the Common Voice English dataset (CV)~\cite{common-voice}. It covers broader accents and is noisier compared to AB. (3) Additionally, to introduce more stylized and expressive data, we sample accented evaluation data from L2-ARCTIC~\cite{l2arctic} (ACCENT) and emotional evaluation data from ESD~\cite{esd}
 % (EMOTION). There are 700 evaluation samples in total: 200 from AB, 200 from CV, 150 from ACCENT, and 150 from EMOTION.

 % % ====== Internal Emotion and Accent ======
 \textbf{Evaluation Data}\quad We consider various evaluation settings to construct the evaluation set: (1) For clean data, such as recordings made in studio environments, we select audiobook speech data. Specifically, we reserve a subset of the total 60K hours of data as evaluation samples, which we denote as AB. (2) For noisy data, which may include in-the-wild recordings and diverse recording devices, we use the Common Voice English dataset (CV)~\cite{common-voice}. It covers broader accents and is noisier compared to AB. (3) Additionally, to introduce more stylized and expressive data, we use an internal emotional and accented corpus to sample an emotional test set (EMOTION) and an accented test set (ACCENT). There are 700 evaluation samples in total: 200 from AB, 200 from CV, 150 from ACCENT, and 150 from EMOTION.
 % All evaluation samples used in this study are subsets from the evaluation sets of Voicebox~\cite{voicebox} and Audiobox~\cite{audiobox}.

 \begin{table}[t]
\caption{Performance of $\mathcal{M}_{acoustic}$ trained by different HuBERT representations on zero-shot timbre imitation task. We highlight three key turning points during the self-supervised disentanglement process:  the \textbf{initial stage}  of information filtering (the 18th layer features, where vocabulary size can be considered infinite), the proposed \textbf{content-style tokenizer} (VQ-VAE tokens with a vocabulary size of 4096), and the proposed \textbf{content tokenizer} (VQ-VAE tokens with a vocabulary size of 32).}
\vspace{-5mm}
\label{tab:hubert-vc}
\begin{center}
\resizebox{\textwidth}{!}{%
\begin{threeparttable}
    \begin{tabular}{c|c|cccc|l}
    \toprule
    \textbf{Representations} & \textbf{\#Vocab} & \makecell[c]{\textbf{WER}\\($\downarrow$)} & \makecell[c]{\textbf{S-SIM} \\ \small{(to \textcolor{red}{ref})} ($\uparrow$)} & \makecell[c]{\textbf{S-SIM} \\ \small{(to \textcolor{blue}{src})} ($\downarrow$)} & \makecell[c]{\textbf{FPC} \\ \small{(to \textcolor{blue}{src})} ($\uparrow$)} & \multicolumn{1}{c}{\textbf{Analysis}} \\
    \midrule
    Ground Truth & - & 5.526 & 0.762 & 0.087 & 1.000 & \multicolumn{1}{c}{-} \\ \midrule
    24th layer features & $\infty$ & 5.706 & 0.266 & 0.400 & 0.768 & \multirow{3}{*}{ \makecell[l]{\textbf{\textit{Pros}}: Intelligibility, Style consistency\\ \textbf{\textit{Cons}}: Timbre imitation} } \\
    \rowcolor{gray!30} 18th layer features & $\infty$ & 5.324 & 0.250 & 0.505 $\uparrow$ & 0.824 \\
    12th layer features & $\infty$ & 5.348 & 0.200 & 0.626 $\uparrow$ & 0.805 \\
    \midrule
    PPG features & $\infty$ & 6.143 & 0.449 & 0.157 & 0.741 & \multirow{2}{*}{ \makecell[l]{\textbf{\textit{Pros}}: Intelligibility, Timbre imitation\\ \textbf{\textit{Cons}}: Style consistency} } \\
    ASR tokens & 29 & 7.836 & 0.463 & 0.125 & 0.698 \\
    \midrule
    \makecell[c]{K-means tokens} & 1024 & 11.493 & 0.398 & 0.150 & 0.734 & Worse than VQ-VAE tokens (1024)  \\
    \midrule
     \multirow{6}{*}{VQ-VAE tokens}  & 16384 & 6.807 & 0.398 & 0.306 & 0.826 & \multirow{6}{*}{\makecell[l]{As the vocabulary size decreases,\\ \textbf{\textit{Pros}}: \\ \quad Timbre imitation $\uparrow$ \\ \textbf{\textit{Cons}}:\\ \quad Intelligibility $\downarrow$\\ \quad Style consistency $\downarrow$} } \\
      & \cellcolor{lightorange}\text{4096} & \cellcolor{lightorange}6.908 $\uparrow$ & \cellcolor{lightorange}0.403 & \cellcolor{lightorange}0.236 $\downarrow$ & \cellcolor{lightorange}0.797 $\downarrow$ \\
     & 1024 & 6.967 $\uparrow$ & 0.418 & 0.249 & 0.764 $\downarrow$ \\
     & \cellcolor{lightgreen}\text{32} & \cellcolor{lightgreen}9.731 $\uparrow$ & \cellcolor{lightgreen}0.426 & \cellcolor{lightgreen}0.161 $\downarrow$ & \cellcolor{lightgreen}0.706 $\downarrow$ \\
     & 16 & 13.169 $\uparrow$ & 0.441 & 0.146 $\downarrow$ & 0.672 $\downarrow$  \\
     & 8 & 21.813 $\uparrow$ & 0.392 & 0.109 $\downarrow$ & 0.675 \\
    \bottomrule
    \end{tabular}%
    \begin{tablenotes}
        \footnotesize{
        \item[*]  PPG features and ASR tokens are obtained from HuBERT-ASR-Large, while the others are from HuBERT-Large. K-means and VQ-VAE tokens are quantized on the 18th layer features of HuBERT-Large. FPC are evaluated only on EMOTION.
        \item[*] \textbf{\#Vocab}: the vocabulary size $K$. \textbf{S-SIM}: Speaker SIM. \textcolor{red}{ref}/\textcolor{blue}{src}: reference/source. 
        }
    \end{tablenotes}
\end{threeparttable}
}
\end{center}
\end{table}


\textbf{Evaluation Metrics}\quad For the objective metrics, we evaluate the intelligibility (WER), speaker similarity (S-SIM), accent similarity (A-SIM), emotion similarity (E-SIM), and F0 correlation (FPC)~\cite{svcc-2023,amphion-svc}. Specially, we calculate WER based on Whisper-large-v3~\cite{whisper,seedtts,maskgct}. For the three similarity metrics -- S-SIM, A-SIM, and E-SIM -- we calculate the cosine similarity between the embeddings (of speaker, accent, or emotion) of the generated sample and the reference. Specifically, we extract these embeddings using WavLM TDNN\footnote{\href{https://github.com/microsoft/UniSpeech/tree/main/downstreams/speaker\_verification}{https://github.com/microsoft/UniSpeech/tree/main/downstreams/speaker\_verification}}~\cite{wavlm,seedtts,maskgct} for speaker, CommonAccent\footnote{\href{https://huggingface.co/Jzuluaga/accent-id-commonaccent\_ecapa}{https://huggingface.co/Jzuluaga/accent-id-commonaccent\_ecapa}}~\cite{common-accent,convertandspeak} for accent, and emotion2vec\footnote{\href{https://github.com/ddlBoJack/emotion2vec}{https://github.com/ddlBoJack/emotion2vec}}~\cite{emotion2vec} for emotion, respectively. We also used CommonAccent and emotion2vec as the classifiers to measure the classification accuracy of accent and emotion (A-ACC and E-ACC). For subjective metrics, we use the Mean Opinion Score (MOS, rated from 1 to 5) to assess naturalness (N-MOS) and similarity in speaker, accent, emotion, and prosody (SS-MOS, AS-MOS, ES-MOS, and PS-MOS). SS MOS, AS-MOS, and ES-MOS evaluate the similarity between the generated sample and the \textit{reference}, while PS-MOS assesses the similarity between the generated sample and the \textit{source}. Additionally, we employ Comparative MOS (CMOS, rated from -3 to 3) to evaluate naturalness (N-CMOS), accentedness (A-CMOS), and emotiveness (E-CMOS). Detailed backgrounds of subjects and definitions of all subjective metrics are provided in Appendix~\ref{sec:appendix-subeval}.

% \vspace{-2mm}
\subsection{Effect of the Vocabulary Size of the VQ-VAE Tokenizer}\label{sec:results-effect-of-codebook-size}

% \begin{table}[t]
% \caption{Test}
% \vspace{-5mm}
% \label{tab:hubert-vc}
% \begin{center}
% \resizebox{\textwidth}{!}{%
% \begin{threeparttable}
%     \begin{tabular}{c|c|cccc|l}
%     \toprule
%     \textbf{Representations} & \textbf{\#Vocab} & \makecell[c]{\textbf{WER}\\($\downarrow$)} & \makecell[c]{\textbf{S-SIM} \\ \small{(to \textcolor{red}{ref})} ($\uparrow$)} & \makecell[c]{\textbf{S-SIM} \\ \small{(to \textcolor{blue}{src})} ($\downarrow$)} & \makecell[c]{\textbf{FPC} \\ \small{(to \textcolor{blue}{src})} ($\uparrow$)} & \multicolumn{1}{c}{\textbf{Analysis}} \\
%     \midrule
%     Ground Truth & - & 5.526 & 0.762 & 0.087 & 1.000 & \multicolumn{1}{c}{-} \\ \midrule
%     24th layer features & - & 5.706 & 0.266 & 0.400 & 0.768 & \multirow{3}{*}{ \makecell[l]{\textbf{\textit{Pros}}: Intelligibility, Style consistency\\ \textbf{\textit{Cons}}: Timbre imitation} } \\
%     18th layer features & - & 5.324 & 0.250 & 0.505 $\uparrow$ & 0.824 \\
%     12th layer features & - & 5.348 & 0.200 & 0.626 $\uparrow$ & 0.805 \\
%     \midrule
%     PPG features & - & 6.143 & 0.449 & 0.157 & 0.741 & \multirow{2}{*}{ \makecell[l]{\textbf{\textit{Pros}}: Intelligibility, Timbre imitation\\ \textbf{\textit{Cons}}: Style consistency} } \\
%     ASR tokens & 29 & 7.836 & 0.463 & 0.125 & 0.698 \\
%     \midrule
%     \makecell[c]{K-means tokens} & 1024 & 11.493 & 0.398 & 0.150 & 0.734 & Worse than VQ-VAE tokens (1024)  \\
%     \midrule
%      \multirow{6}{*}{VQ-VAE tokens}  & 16384 & 6.807 & 0.398 & 0.306 & 0.826 & \multirow{6}{*}{\makecell[l]{As the vocabulary size decreases,\\ \textbf{\textit{Pros}}: \\ \quad Timbre imitation $\uparrow$ \\ \textbf{\textit{Cons}}:\\ \quad Intelligibility $\downarrow$\\ \quad Style consistency $\downarrow$} } \\
%      & \text{4096} & 6.908 $\uparrow$ & 0.403 & 0.236 $\downarrow$ & 0.797 $\downarrow$ \\
%      & 1024 & 6.967 $\uparrow$ & 0.418 & 0.249 & 0.764 $\downarrow$ \\
%      & \text{32} & 9.731 $\uparrow$ & 0.426 & 0.161 $\downarrow$ & 0.706 $\downarrow$ \\
%      & 16 & 13.169 $\uparrow$ & 0.441 & 0.146 $\downarrow$ & 0.672 $\downarrow$  \\
%      & 8 & 21.813 $\uparrow$ & 0.392 & 0.109 $\downarrow$ & 0.675 \\
%     \bottomrule
%     \end{tabular}%
%     \begin{tablenotes}
%         \footnotesize{\item[*]  PPG features and ASR tokens are obtained from HuBERT-ASR-Large, while the others are from HuBERT-Large. K-means and VQ-VAE tokens are quantized on the 18th layer features of HuBERT-Large. FPC are evaluated only on EMOTION.}
%     \end{tablenotes}
% \end{threeparttable}
% }
% \end{center}
% \end{table}

We conduct experiments to figure out how to derive \textit{content} and \textit{content-style} tokens from speech. The key questions include: (1) What information from speech is retained in the \textit{continuous hidden features} of HuBERT? (2) How do vector quantization methods, including the commonly used K-means~\cite{kmeans,HuBERT,lmvc,uniaudio} and our adopted VQ-VAE~\cite{vq-vae,repcodec}, affect the disentanglement ability of the resulting \textit{discrete tokens} of HuBERT? (3) How does the vocabulary size of VQ-VAE codebook influence the produced tokens? To answer these questions, we investigate the performance of different HuBERT representations on the zero-shot timbre imitation task -- i.e., using them to train $\mathcal{M}_{acoustic}$.

Specifically, we adopt the representations of the {HuBERT-Large}\footref{fn:hubert-large} model, which is a 24-layer transformer pre-trained on Libri-light dataset~\cite{libri-light}. For comparison, we also examine the HuBERT-ASR-Large\footnote{\href{https://pytorch.org/audio/0.10.0/pipelines.html\#hubert-asr-large}{https://pytorch.org/audio/0.10.0/pipelines.html\#hubert-asr-large}} model, which is fine-tuned from HuBERT-Large for ASR task on LibriSpeech~\cite{librispeech}. Compared to HuBERT-Large, HuBERT-ASR-Large contains an additional prediction layer and a softmax layer, whose output is $\bm{x}_{ppg} \in \mathbb{R}^{T \times 29}$, where $T$ is the frame length and 29 represents the vocabulary size of phonemes. We refer to $\bm{x}_{ppg}$ as PPG features and also derive frame-level ASR tokens from each frame's PPG features: $\bm{x}_{asr} = \arg\max \bm{x}_{ppg} \in \mathbb{R}^{T}$. We randomly sample a 6K-hour subset from the full training data for training. The results are presented in Table~\ref{tab:hubert-vc}. 

\begin{table}[t]
\caption{Results on zero-shot timbre imitation and voice imitation (conversion) tasks. (ContRep/Model: Hours of training data for the used content representations and the model)
% \textbf{S/A/E-SIM}: Speaker/Accent/Emotion SIM. \textbf{N-MOS}: Naturalness MOS. \textbf{SS/PS/AS/ES-MOS}: Speaker/Prosody/Accent/Emotion Similarity MOS.)
}
\vspace{-3mm}
\label{tab:results-vc}
\centering
\begin{subtable}{0.95\textwidth}
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{l|c|c|ccc|cccc}
    \toprule
    \midrule
    \multicolumn{5}{l}{\textbf{\textit{Zero-Shot Timbre Imitation}} (AB, CV) }   & \multicolumn{4}{r}{\textbf{Source} ${\textcolor{blue}{\bm{i}}}$, \textbf{Reference} ${\textcolor{red}{\bm{r}}}$ $\Rightarrow$ \textbf{Target}: $\mathcal{W}(c_{\textcolor{blue}{\bm{i}}}, s_{\textcolor{blue}{\bm{i}}}, t_{\textcolor{red}{\bm{r}}})$} \\
    \midrule\midrule
    \multicolumn{1}{c|}{\textbf{Model}} & \makecell[c]{\textbf{AR?}} & \makecell[c]{\textbf{Training Data} \\ \small{(ContRep / Model)} } & \makecell[c]{\textbf{WER}\\ ($\downarrow$)}  & \makecell[c]{\textbf{S-SIM} \\ \small{(to ${\textcolor{red}{\bm{r}}}$)} ($\uparrow$)} & \makecell[c]{\textbf{FPC} \\ \small{(to ${\textcolor{blue}{\bm{i}}}$)} ($\uparrow$)} & \makecell[c]{\textbf{N-MOS}\\ \textbf{($\uparrow$)}} & \makecell[c]{\textbf{SS-MOS} \\ \small{(to ${\textcolor{red}{\bm{r}}}$)} ($\uparrow$)} 
    & \makecell[c]{\textbf{PS-MOS} \\ \small{(to ${\textcolor{blue}{\bm{i}}}$)} ($\uparrow$)} 
    \\
    \midrule
    HierSpeech++~\cite{hierspeech++} & \ding{55} & 500K / 2.8K & 4.233 & 0.385 & \underline{0.634} & 3.05 $_{\scriptscriptstyle \pm \text{0.23}}$ & 3.24 $_{\scriptscriptstyle \pm \text{0.25}}$  & 3.08 $_{\scriptscriptstyle \pm \text{0.26}}$  \\
    LM-VC~\cite{lmvc} & \ding{51} & 1K / 60K & 8.623 & 0.310 & 0.524 & 2.90 $_{\scriptscriptstyle \pm \text{0.11}}$ & 2.98 $_{\scriptscriptstyle \pm \text{0.18}}$ & 2.16 $_{\scriptscriptstyle \pm \text{0.26}}$   \\
    UniAudio~\cite{uniaudio} & \ding{51} & 1K / 100K & 7.241 & 0.264 & 0.575 & 3.04 $_{\scriptscriptstyle \pm \text{0.15}}$ & 2.47 $_{\scriptscriptstyle \pm \text{0.20}}$ & 2.51 $_{\scriptscriptstyle \pm \text{0.25}}$ \\
    FACodec~\cite{ns3} & \ding{55} & 60K / 60K & \underline{3.682} & 0.327 & 0.611 & 2.50 $_{\scriptscriptstyle \pm \text{0.20}}$ & 3.10 $_{\scriptscriptstyle \pm \text{0.24}}$ &  \underline{3.10} $_{\scriptscriptstyle \pm \text{0.23}}$  \\
    Vevo-Voice & \ding{51} & 60K / 60K & 7.694 & \textbf{0.458} & 0.485 & \underline{3.09} $_{\scriptscriptstyle \pm \text{0.13}}$ & \textbf{3.51} $_{\scriptscriptstyle \pm \text{0.24}}$ & 2.60 $_{\scriptscriptstyle \pm \text{0.23}}$  \\
    \midrule
    \text{Vevo-Timbre} & \ding{55} & 60K / 60K & \textbf{2.968} & \underline{0.420} & \textbf{0.686} & \textbf{3.35} $_{\scriptscriptstyle \pm \text{0.09}}$ & \underline{3.36} $_{\scriptscriptstyle \pm \text{0.16}}$ & \textbf{3.45} $_{\scriptscriptstyle \pm \text{0.17}}$  \\
    % \midrule
    \end{tabular}%
    }
\end{subtable}
\hfill
\begin{subtable}{0.95\textwidth}
    \resizebox{\textwidth}{!}{%

    \begin{threeparttable}
        \begin{tabular}{l|cccc|cccccccc}
        \midrule \midrule
        \multicolumn{5}{l}{\textbf{\textit{Zero-Shot Voice Imitation}} (ACCENT, EMOTION)} & \multicolumn{4}{r}{\textbf{Source} ${\textcolor{blue}{\bm{i}}}$, \textbf{Reference} ${\textcolor{red}{\bm{r}}}$ $\Rightarrow$ \textbf{Target}: $\mathcal{W}(c_{\textcolor{blue}{\bm{i}}}, s_{\textcolor{red}{\bm{r}}}, t_{\textcolor{red}{\bm{r}}})$} \\
        \midrule\midrule
        \multicolumn{1}{c|}{\textbf{Model}} & \makecell[c]{\textbf{WER}\\ ($\downarrow$)}  & \makecell[c]{\textbf{S-SIM} \\ \small{(to ${\textcolor{red}{\bm{r}}}$)} ($\uparrow$)} & \makecell[c]{\textbf{A-SIM} \\ \small{(to ${\textcolor{red}{\bm{r}}}$)} ($\uparrow$)} & \makecell[c]{\textbf{E-SIM} \\ \small{(to ${\textcolor{red}{\bm{r}}}$)} ($\uparrow$)} & \makecell[c]{\textbf{N-MOS}\\ \textbf{($\uparrow$)}} & \makecell[c]{\textbf{SS-MOS} \\ \small{(to ${\textcolor{red}{\bm{r}}}$)} ($\uparrow$)} & \makecell[c]{\textbf{AS-MOS} \\ \small{(to ${\textcolor{red}{\bm{r}}}$)} ($\uparrow$)}
        & \makecell[c]{\textbf{ES-MOS} \\ \small{(to ${\textcolor{red}{\bm{r}}}$)} ($\uparrow$)} 
        \\
        \midrule
        Ground Truth & 10.917 & 0.762 & 0.763 & 0.965 & - & - & - & - \\
        \midrule
        HierSpeech++~\cite{hierspeech++} & 12.921 & 0.466 & 0.526 & 0.658 & 3.04 $_{\scriptscriptstyle \pm \text{0.14}}$ & 3.15 $_{\scriptscriptstyle \pm \text{0.23}}$ & 3.13 $_{\scriptscriptstyle \pm \text{0.22}}$ & 2.55 $_{\scriptscriptstyle \pm \text{0.19}}$   \\
        LM-VC~\cite{lmvc} &  20.353 & 0.312 & 0.426 & 0.649 & 2.40 $_{\scriptscriptstyle \pm \text{0.10}}$ & 2.56 $_{\scriptscriptstyle \pm \text{0.15}}$ & 3.02 $_{\scriptscriptstyle \pm \text{0.19}}$ & 2.46 $_{\scriptscriptstyle \pm \text{0.17}}$ \\
        UniAudio~\cite{uniaudio} & 15.751 & 0.311 & 0.486 & 0.611 & 2.95 $_{\scriptscriptstyle \pm \text{0.11}}$ & 2.39 $_{\scriptscriptstyle \pm \text{0.17}}$ & 2.42 $_{\scriptscriptstyle \pm \text{0.15}}$ & 2.41 $_{\scriptscriptstyle \pm \text{0.26}}$ \\
        FACodec~\cite{ns3} & \underline{12.731} & 0.434 & 0.514 & 0.688 & 2.36 $_{\scriptscriptstyle \pm \text{0.18}}$ & 3.19 $_{\scriptscriptstyle \pm \text{0.22}}$ & 3.01 $_{\scriptscriptstyle \pm \text{0.16}}$ & 2.30 $_{\scriptscriptstyle \pm \text{0.22}}$ \\
        Vevo-Timbre & \textbf{12.351} & \underline{0.486} & \underline{0.567} & \underline{0.816} & \textbf{3.43} $_{\scriptscriptstyle \pm \text{0.09}}$ & \underline{3.46} $_{\scriptscriptstyle \pm \text{0.15}}$ & \underline{3.55} $_{\scriptscriptstyle \pm \text{0.25}}$ & \underline{2.66} $_{\scriptscriptstyle \pm \text{0.26}}$  \\
        \midrule
        \text{Vevo-Voice} & 15.214 & \textbf{0.517} & \textbf{0.614} & \textbf{0.872} & \underline{3.24} $_{\scriptscriptstyle \pm \text{0.11}}$ & \textbf{3.70} $_{\scriptscriptstyle \pm \text{0.24}}$ & \textbf{3.90} $_{\scriptscriptstyle \pm \text{0.19}}$ & \textbf{3.20} $_{\scriptscriptstyle \pm \text{0.16}}$  \\
        \bottomrule
        \end{tabular}%
        \begin{tablenotes}
        \footnotesize{
            \item[1]  {PS-MOS}, {E-SIM}, and {ES-MOS} are evaluated only on EMOTION. {A-SIM} and {AS-MOS} are evaluated only on ACCENT.
            \item[2] The best and the second best result is shown in \textbf{bold} and by \underline{underlined}.
        }
        \end{tablenotes}
    \end{threeparttable}
    }
\end{subtable}
\end{table}

Our findings indicate that: (1) HuBERT continuous hidden features possess rich information on timbre (high S-SIM to source), style (high FPC), and linguistic content (low WER). Notably, the S-SIM to source is even higher than that to reference, i.e., there is a timbre leakage. This phenomenon is more obvious for shallower 12th layer features. (2) After ASR fine-tuning, both PPG features and ASR tokens retain substantial linguistic content information (low WER) but exhibit a significant reduction in timbre information (lower S-SIM to source) and a decrease in style information (lower FPC). (3) Compared to VQ-VAE, K-means tokens show lower intelligibility, S-SIM to reference, and FPC when $K$ is the same (1024). Huang et al. provides a detailed comparison of between these two methods recently~\cite{repcodec}. (4) For VQ-VAE tokens, larger vocabulary sizes (e.g., 16384) retain more timbre information (S-SIM to source at 0.306). As $K$ decreases to 4096, much of the timbre information is filtered out (S-SIM to source/reference at 0.236/0.403), yet style information is relatively retained (FPC at 0.797). When $K$ reduces further to 32, in addition to timbre, most style information is also filtered out -- FPC drops to 0.706, similar to ASR tokens. As $K$ diminishes to 16 or even 8, even the high-level linguistic content begins to be filtered out (rapid increase in WER).

Based on these findings, we select VQ-VAE with $K_c = \text{32}$ for the content tokenizer and $K_s = \text{4096}$ for the content-style tokenizer. Note that designing the information bottleneck is a challenging trade-off, and such $K_c$ and $K_s$ may not be optimal. However, the results in the following sections show that such a choice has been pretty good under various voice imitation tasks. Additional effects of different ($K_c$, $K_s$) combinations on $\mathcal{M}_{style}$ training are detailed in Appendix~\ref{sec:appendix-effect-of-K}.

% \vspace{-5mm}
\subsection{Zero-Shot Timbre Imitation and Voice Imitation (Conversion Task)}

Further, we apply Vevo to various zero-shot imitation tasks. This section evaluates \textbf{Vevo-Timbre} and \textbf{Vevo-Voice} on zero-shot timbre and voice imitation tasks. We select several state-of-the-art (SOTA) baselines in zero-shot voice conversion, including HierSpeech++~\cite{hierspeech++}, LM-VC~\cite{lmvc}, UniAudio~\cite{uniaudio}, and FACodec~\cite{ns3}. Details about these baselines are available in Appendix~\ref{sec:appendix-baselines}. We train $\mathcal{M}_{style}$ and $\mathcal{M}_{acoustic}$ on the full 60K hours dataset. The results are presented in Table~\ref{tab:results-vc}.

The findings reveal that: (1) \textbf{Zero-shot timbre imitation}: Compared to the four baselines, Vevo-Timbre exhibits superior performance across common voice conversion metrics such as WER, S-SIM, N-MOS, and SS-MOS. Additionally, Vevo-Timbre demonstrates a clear advantage in FPC and PS-MOS, which measure style consistency. (2) \textbf{Zero-shot voice imitation}: Against the four baselines, Vevo-Voice not only excels in mimicking speaker identity (S-SIM, SS-MOS) but also significantly outperforms in imitating specific style attributes like accent (A-SIM, AS-MOS) and emotion (E-SIM, ES-MOS). (3) \textbf{Comparing Vevo-Timbre and Vevo-Voice}: Vevo-Timbre's strength lies in preserving the style of the source (FPC, PS-MOS), whereas Vevo-Voice additionally excels in style imitation, resulting in higher speaker similarity (S-SIM, SS-MOS). However, due to the autoregressive design in $\mathcal{M}_{style}$, Vevo-Voice scores lower in intelligibility (WER) compared to Vevo-Timbre.

\vspace{-3mm}
\subsection{Zero-Shot Style Imitation}\label{sec:expt-style-imitation}

\begin{table}[t]
\caption{Results on style imitation task. (\textbf{PC}: Parallel corpus. \textbf{SL}: Style labels) 
% \textbf{Acce/Emo ACC}: Accent/Emotion Accuracy. \textbf{Acce/Emo} SIM: Accent/Emotion Similarity. \textbf{Acce/Emo} CMOS: Accentedness/Emotiveness CMOS.) \note{trainig data}
}
\vspace{-3mm}
\label{tab:results-style-imitation}
\centering
\resizebox{0.9\textwidth}{!}{%
\begin{threeparttable}
    \begin{tabular}{l|c|ccc|ccc|cc}
    \toprule
    \multicolumn{1}{c|}{ \multirow{2}{*}{\makecell[c]{ \textbf{Model}}} } & \multicolumn{1}{c|}{ \multirow{2}{*}{\makecell[c]{ \textbf{Zero}\\ \textbf{-shot}}} } & \multicolumn{3}{c|}{\textbf{Supervision}} &  \multirow{2}{*}{\makecell[c]{\textbf{WER}\\  ($\downarrow$)}}  & \multirow{2}{*}{\makecell[c]{\textbf{A-} / \textbf{E-} \\  \textbf{ACC ($\uparrow$)}}} & \multirow{2}{*}{\makecell[c]{\textbf{A-} / \textbf{E-} \\  \textbf{SIM ($\uparrow$)}}} & \multirow{2}{*}{\makecell[c]{\textbf{N-}\\ \textbf{COMS ($\uparrow$)}}} & \multirow{2}{*}{\makecell[c]{\textbf{A-} / \textbf{E-}\\ \textbf{CMOS ($\uparrow$)}}} \\
    \cmidrule(lr){3-5} 
    & & \makecell[c]{\textbf{PC}} & \makecell[c]{\textbf{SL}} & \makecell[c]{\textbf{Text}} & & &    \\
    \midrule
     ASR-AC~\cite{asr-ac} & \ding{55} & \ding{55} & \ding{51}  & \ding{51}  & 4.775 & 0.633 & - & 0.00 \textcolor{white}{$_{\scriptscriptstyle \pm \text{0.00}}$} & 0.00 \textcolor{white}{$_{\scriptscriptstyle \pm \text{0.00}}$} \\
    Vevo-Style (ASR) & \ding{51} & \ding{55} & \ding{55} & \ding{51}  & \textbf{1.550} & \textbf{0.723} & \textbf{0.570} & \textbf{0.32} $_{\scriptscriptstyle \pm \text{0.11}}$ & \textbf{0.49} $_{\scriptscriptstyle \pm \text{0.14}}$  \\
    Vevo-Style & \ding{51} & \ding{55} & \ding{55}  & \ding{55}  & 3.083  & 0.663 &  0.562 & 0.30 $_{\scriptscriptstyle \pm \text{0.13}}$ & 0.35 $_{\scriptscriptstyle \pm \text{0.21}}$  \\
    \midrule \midrule
     VoiceShop~\cite{voiceshop} & \ding{55}  & \ding{51}  & \ding{51}  & \ding{51}  & 5.547  & 0.642 & - & 0.00 \textcolor{white}{$_{\scriptscriptstyle \pm \text{0.00}}$} & 0.00 \textcolor{white}{$_{\scriptscriptstyle \pm \text{0.00}}$} \\
    Vevo-Style (ASR) & \ding{51} &  \ding{55} & \ding{55} & \ding{51} & \textbf{3.553} & \textbf{0.735} & \textbf{0.585} & \textbf{0.26} $_{\scriptscriptstyle \pm \text{0.16}}$ & \textbf{0.18} $_{\scriptscriptstyle \pm \text{0.20}}$ \\
    Vevo-Style & \ding{51}      &  \ding{55} & \ding{55} & \ding{55} & 5.464  & 0.673 & 0.554 & 0.12 $_{\scriptscriptstyle \pm \text{0.10}}$  & 0.13 $_{\scriptscriptstyle \pm \text{0.08}}$  \\
    \midrule \midrule
     Conv-Speak~\cite{convertandspeak} & \ding{55}  & \ding{51} & \ding{51} & \ding{55}  & 9.950 & 0.571  & - & 0.00 \textcolor{white}{$_{\scriptscriptstyle \pm \text{0.00}}$}  & 0.00 \textcolor{white}{$_{\scriptscriptstyle \pm \text{0.00}}$} \\
    Vevo-Style (ASR) & \ding{51} &  \ding{55} & \ding{55} & \ding{51} & \textbf{2.778} & 0.864 & 0.574 & 0.10 $_{\scriptscriptstyle \pm \text{0.05}}$ & 0.40 $_{\scriptscriptstyle \pm \text{0.12}}$  \\
    Vevo-Style & \ding{51} &  \ding{55} & \ding{55} & \ding{55} & 3.889 & \textbf{0.903} & \textbf{0.580} & \textbf{0.15} $_{\scriptscriptstyle \pm \text{0.12}}$ & \textbf{0.60} $_{\scriptscriptstyle \pm \text{0.16}}$ \\
    \midrule \midrule
     Emovox~\cite{emovox} & \ding{55}  & \ding{55} & \ding{51} & \ding{51}  & 15.444 & 0.750 & - & 0.00 \textcolor{white}{$_{\scriptscriptstyle \pm \text{0.00}}$} & 0.00 \textcolor{white}{$_{\scriptscriptstyle \pm \text{0.00}}$} \\
    Vevo-Style (ASR) & \ding{51} &  \ding{55} & \ding{55} & \ding{51} & \textbf{9.842} & 0.692 & 0.800 & 1.74 $_{\scriptscriptstyle \pm \text{0.20}}$ & 0.45 $_{\scriptscriptstyle \pm \text{0.11}}$  \\
    Vevo-Style & \ding{51} &  \ding{55} & \ding{55} & \ding{55} & 10.221 & \textbf{0.754} & \textbf{0.825} & \textbf{1.78} $_{\scriptscriptstyle \pm \text{0.20}}$  & \textbf{0.49} $_{\scriptscriptstyle \pm \text{0.13}}$ \\
    \bottomrule
    \end{tabular}
    \begin{tablenotes}
        \footnotesize{\item[*] We present four comparative groups. Evaluation samples for each group are sourced from the baseline's demo website. For the first three groups, we evaluate A-ACC/SIM/CMOS, and for the last group, we evaluate E-ACC/SIM/CMOS.}
    \end{tablenotes}
\end{threeparttable}    
}   
\end{table}


We present the performance of \textbf{Vevo-Style} in the zero-shot style imitation task, focusing on widely studied styles such as accent and emotion. For accent imitation, we select baselines including ASR-AC~\cite{asr-ac}, VoiceShop~\cite{voiceshop}, and Conv-Speak~\cite{convertandspeak}. We use their demo website samples as our evaluation set, including conversions among multiple accented English such as British, American, Hindi, and Mandarin. For emotion imitation, we choose Emovox~\cite{emovox} and its demo website samples, which include conversions from Neutral to Happy, Angry, and Sad emotions. Moreover, we also introduce the Vevo-Style (ASR) for a \textit{zero-shot} style imitation baseline. Its only difference compared to the Vevo-Style model is the use of $\bm{x}_{asr}$ (see Section~\ref{sec:results-effect-of-codebook-size}) rather than $\bm{Q}_c$ as the content tokenizer.

Our experimental results are shown in Table~\ref{tab:results-style-imitation}.  Our observations indicate: (1) Compared to the baselines, Vevo-Style is only self-supervised trained on audiobook speech data. However, without any fine-tuning on accented or emotional corpus, it delivers superior outcomes in terms of intelligibility (WER), quality (N-CMOS), and the imitation of accents and emotions (A-ACC/SIM/CMOS and E-ACC/SIM/CMOS). (2) Using text as the additional supervision, Vevo-Style (ASR) further surpasses Vevo-Style in intelligibility and specific aspects of accent imitation. We speculate that compared to $\bm{x}_{asr}$, the $\bm{Q}_c$ used by Vevo-Style may still retain a small portion of accent-related information, thereby limiting the $\mathcal{M}_{style}$ to perfectly imitate the accent information from the style reference.

\subsection{Zero-Shot Voice Imitation (Synthesis Task)}\label{sec:results-tts}

We present the performance of \textbf{Vevo-TTS} in the zero-shot voice imitation (synthesis) task. We select the classic baselines of the zero-shot TTS filed, including the Non-AR Voicebox model~\cite{voicebox}, and the AR models such as {VALL-E}~\cite{valle,amphion} and {VoiceCraft}~\cite{voicecraft}, all of which are trained only on audiobook speech data. For comparison, we also include two stronger SOTA models: {CosyVoice}~\cite{cosyvoice} and MaskGCT~\cite{maskgct,amphion_v0.2}, which are trained on large-scale private corpus derived from in-the-wild video data, featuring highly diverse distributions~\cite{cosyvoice,emilia}. Detailed baseline information and evaluation results are available in Appendix~\ref{sec:appendix-results-tts}. Here, we only highlight performances on ACCENT and EMOTION evaluation samples (Table~\ref{tab:results-tts}). Our observations are as follows: (1) Compared between Voicebox and Vevo-TTS whose training data are identical, Vevo-TTS, while showing slightly inferior performance in WER (which is a common weakness for AR models), excels across all other metrics. (2) Notably, Vevo-TTS demonstrates outstanding performance in style imitation (A/E-SIM, AS/ES-MOS). Despite being trained only on audiobook data, it surpasses CosyVoice and MaskGCT in some emotion imitation tasks (ES-MOS is 4.03). This verifies the effectiveness of our proposed content-style tokens, which could be representations that can effectively capture style information and are easily learned by downstream models.

\begin{table}[t]
\caption{Results on zero-shot voice imitation (synthesis) task.}
\vspace{-3mm}
\label{tab:results-tts}
\centering
\resizebox{\textwidth}{!}{%
\begin{threeparttable}
    \begin{tabular}{l|c|c|cccc|cccc}
    \toprule
    \multicolumn{1}{c|}{\textbf{Model}} & \makecell[c]{\textbf{AR?}} & \makecell[c]{\textbf{Training}\\ \textbf{Data}} & \makecell[c]{\textbf{WER} \\{($\downarrow$)}} & \makecell[c]{\textbf{S-SIM} \\{($\uparrow$)}} & \makecell[c]{\textbf{A-SIM} \\{($\uparrow$)}} & \makecell[c]{\textbf{E-SIM} \\{($\uparrow$)}} & \makecell[c]{\textbf{N-CMOS}\\ \textbf{($\uparrow$)}} & \makecell[c]{\textbf{SS-MOS}\\ \textbf{($\uparrow$)}} & \makecell[c]{\textbf{AS-MOS}\\ \textbf{($\uparrow$)}} & \makecell[c]{\textbf{ES-MOS}\\ \textbf{($\uparrow$)}} \\
    \midrule
    Ground Truth & - & - & 11.348 & 0.710 & 0.633 & 0.936 & 0.00 \textcolor{white}{$_{\scriptscriptstyle \pm \text{0.00}}$} & - & - & - \\
    CosyVoice~\cite{cosyvoice} & \ding{51} & 171K & 8.400 & 0.614 & 0.640 & 0.839 & -0.18 $_{\scriptscriptstyle \pm \text{0.19}}$ & 4.11 $_{\scriptscriptstyle \pm \text{0.19}}$ & 3.99 $_{\scriptscriptstyle \pm \text{0.23}}$ & 3.66 $_{\scriptscriptstyle \pm \text{0.19}}$ \\
    MaskGCT~\cite{maskgct} & \ding{55} & 100K & 9.442 & 0.659 & 0.645 & 0.822 & -0.04 $_{\scriptscriptstyle \pm \text{0.19}}$ & 4.16 $_{\scriptscriptstyle \pm \text{0.16}}$ & 4.38 $_{\scriptscriptstyle \pm \text{0.14}}$ & 3.76 $_{\scriptscriptstyle \pm \text{0.25}}$ \\ \midrule
    VALL-E~\cite{valle} & \ding{51} & 45K & 13.226 & 0.400 & 0.485 & 0.735 & -1.24 $_{\scriptscriptstyle \pm \text{0.42}}$ & 2.82 $_{\scriptscriptstyle \pm \text{0.40}}$ & 2.77 $_{\scriptscriptstyle \pm \text{0.45}}$ & 2.63 $_{\scriptscriptstyle \pm \text{0.36}}$  \\
    Voicebox~\cite{voicebox} & \ding{55}  & 60K & \textbf{9.414} & \underline{0.463} & \underline{0.575} & \underline{0.811} & \underline{-0.35} $_{\scriptscriptstyle \pm \text{0.21}}$ & \underline{3.87} $_{\scriptscriptstyle \pm \text{0.21}}$ & \underline{3.49} $_{\scriptscriptstyle \pm \text{0.29}}$ & \underline{3.61} $_{\scriptscriptstyle \pm \text{0.19}}$  \\
    VoiceCraft~\cite{voicecraft} & \ding{51} & 9K & 13.057 & 0.392 & 0.517 & 0.788 & -0.50 $_{\scriptscriptstyle \pm \text{0.23}}$ & 3.47 $_{\scriptscriptstyle \pm \text{0.32}}$ & 3.29 $_{\scriptscriptstyle \pm \text{0.28}}$ & 3.52 $_{\scriptscriptstyle \pm \text{0.25}}$ \\
    \midrule
    Vevo-TTS & \ding{51} & 60K & \underline{12.066} & \textbf{0.505} & \textbf{0.579} & \textbf{0.840} & \textbf{-0.14} $_{\scriptscriptstyle \pm \text{0.18}}$ & \textbf{4.05} $_{\scriptscriptstyle \pm \text{0.21}}$ & \textbf{4.12} $_{\scriptscriptstyle \pm \text{0.21}}$ & \textbf{4.03} $_{\scriptscriptstyle \pm \text{0.19}}$ \\
    \bottomrule
    \end{tabular}%
    \begin{tablenotes}
        \footnotesize{\item[1] A-SIM and AS-MOS are evaluated on ACCENT samples. E-SIM and ES-MOS are evaluated on EMOTION samples.
        \item[2] 
        % CosyVoice and MaskGCT are trained on large-scale, in-the-wild, and private corpora, while the last four are trained solely on audiobook speech data. 
        The best and the second best results of only the last four are shown in \textbf{bold} and by \underline{underlined}.}
    \end{tablenotes}
\end{threeparttable}
}    
\end{table}


\begin{table}[t]
\caption{Effect of duration reduction and different inference modes of $\mathcal{M}_{style}$. ({\#Inference Input}: input sequence length (\%) during inference. w/o: without. w/: with)}
\vspace{-3mm}
\label{tab:results-ablation-study}
\centering
\resizebox{\textwidth}{!}{%
\begin{threeparttable}
    \begin{tabular}{l|cccccc}
    \toprule
    \multicolumn{1}{c|}{\textbf{Model}} & \makecell[c]{\textbf{\#Inference} \textbf{Input}} & \textbf{WER ($\downarrow$)} & \textbf{S-SIM ($\uparrow$)} & \textbf{A-SIM ($\uparrow$)} & \textbf{E-SIM ($\uparrow$)} & \textbf{DDUR ($\downarrow$)} \\
    \midrule
    Vevo-Voice & 100\% & 15.214 & 0.517 & 0.614 & 0.883 & 0.933 \\
    \quad w/o \textit{Duration Reduction} & 127\% & 15.958 & 0.501 & 0.583 & 0.842 & 1.698 \\
    \quad w/ \textit{Global-guided} continuation & 42\% & 16.809 & 0.510 & 0.597 & 0.864 & 0.947 \\
    \bottomrule
    \end{tabular}%
    \begin{tablenotes}
        \footnotesize{\item[*] Vevo-Voice uses reference-style-enhanced continuation. A-SIM is evaluated only on ACCENT samples. E-SIM is evaluated only on EMOTION samples. The remaining metrics are evaluated on both ACCENT and EMOTION samples.}
    \end{tablenotes}
\end{threeparttable}
}
\end{table}

\subsection{Effect of Duration Reduction and Different Inference Modes}\label{sec:ablation-study}

% the impact of different ($K_c$, $K_s$) values on voice imitation tasks (see Appendix~\ref{sec:appendix-effect-of-K})

Finally, we conduct ablation studies on several key components within Vevo, including the impact of different ($K_c$, $K_s$) values on voice imitation tasks (see Appendix~\ref{sec:appendix-effect-of-K}), the effects of the duration reduction strategy, and the two inference modes of $\mathcal{M}_{style}$, as presented in Table~\ref{tab:results-ablation-study}. We adopt DDUR to measure the average differences in duration (seconds) between the converted and ground truth utterances~\cite{non-parallel-seq2seq-vc,emovox}. We observe that: (1) The duration reduction not only reduces the inference input length but also consistently demonstrates clear advantages, especially in duration conversion (DDUR). (2) The reference-global-guided continuation significantly shortens the sequence length (to 42\% of Vevo-Voice), with only a slight decline in performance metrics. This showcases its substantial potential in saving inference memory and enhancing inference speed.

\section{Conclusion}

We introduce Vevo, a versatile zero-shot voice imitation framework featuring controllable timbre and style. Vevo contains of two primary stages: content-style modeling via an autoregressive transformer, and acoustic modeling via a flow matching transformer. Both stages are trainable through self-supervised and in-context learning, friendly to scale up. Vevo operates based on our newly proposed content and content-style tokens, generated by VQ-VAE tokenizers of HuBERT with carefully adjusted vocabulary sizes. Pre-trained only on 60K hours of audiobook speech data without fine-tuning on style-specific corpus, Vevo outperforms state-of-the-art models of accent and emotion conversion fields, particularly achieving these conversions in a zero-shot manner. Furthermore, Vevo's robust performance in zero-shot voice conversion and text-to-speech tasks underscores its versatility and also highlights the broad potential of our proposed disentangled speech tokens.

\clearpage

\section*{Acknowledgement}
This work is partially supported by the NSFC under Grant 62376237, Shenzhen Science and Technology Program ZDSYS20230626091302006. We thank Yuancheng Wang, Jilong Wu, Fuchun Peng, and the anonymous reviewers for their insightful comments and suggestions. We appreciate the efforts of Meiyu Zheng and all the subjects during the subjective evaluation.

\bibliography{iclr2025_conference}
% \bibliographystyle{iclr2025_conference}
\bibliographystyle{unsrtnat}

\clearpage

\appendix

\section{Terminology Clarification}

In this study, we decouple speech into linguistic content (\textit{what to speak}), timbre (\textit{who speaks}), and style (\textit{how to speak}). Below, we will clarify our definitions and scope for timbre and style.

\textbf{Timbre}\quad Timbre is a \textit{physical concept} that refers to the acoustic qualities of sound, such as the spectral envelope, which allows us to differentiate between speakers even when pitch and loudness are identical. It is primarily determined by the speaker’s vocal anatomy and articulatory behaviors. Often discussed alongside timbre is speaker identity. Speaker identity is a \textit{perceptual concept} -- it encompasses not only timbre but also habitual speech patterns, idiosyncrasies, and other personal styles that make a speaker recognizable. While timbre lays the acoustic foundation of identity, speaker identity reflects the broader auditory impression formed by a listener.

\textbf{Style}\quad 
Style refers to the expressive aspects of speech, including accent, emotion, and speaking habits, which dictate \textit{how something is said}. It includes specific features such as \textit{accent} and \textit{emotion}, but also covers a wider array of expressive behaviors. A critical component of style is \textit{prosody}, which includes features such as F0 (pitch), energy, and duration. These prosodic features govern the rhythm, stress, and intonation of speech, contributing significantly to how emotion and emphasis are conveyed. Although style encompasses prosody, it also extends beyond it, influencing not only the melodic flow of speech but also cultural and emotional expressions.

\section{Details of Vevo}\label{sec:appendix-vevo-details}

\subsection{VQ-VAE Architecture}\label{sec:appendix-vq-vae-tokenizer}
We adopt the implementation of RepCodec\footnote{\href{https://github.com/mct10/RepCodec}{https://github.com/mct10/RepCodec}}~\cite{repcodec} as our VQ-VAE tokenizer, whose $\lambda$ and $\beta$ are 45 and 1. Its architecture of encoder and decoder is shown in Figure~\ref{fig:repcodec}. The vocabulary sizes of our \textit{content} and \textit{content-style} tokenizer are 32 and 4096. Their parameter counts are 59M and 63M, respectively.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\textwidth]{img/repcodec.png}
    \caption{Encoder and decoder architecture of our VQ-VAE tokenizer. $k$, $s$. $c_{in}$, and $c_{out}$ denote the kernel size, stride, input channels, and output channels. $h$ denotes the vocabulary size of tokenizer. $H$ denotes the hidden dimension of input representations (which is 1024 for HuBERT-Large). This figure is borrowed from the paper of RepCodec~\cite{repcodec}.}
    \label{fig:repcodec}
\end{figure}

\subsection{Content-Style Modeling}\label{sec:appendix-content-sylte-modeling}

For the content-style modeling stage, we use reference-style-enhanced continuation by default. The architecture of our AR transformer is similar to LLaMA\footnote{\href{https://github.com/meta-llama/llama3}{https://github.com/meta-llama/llama3}}~\cite{llama}. It has 12 layers, 16 attention heads, 2048/3072 embedding/feed-forward network (FFN) dimension. The global style encoder consists of WavLM-based representation layers and TDNN-based feature extraction layers~\cite{wavlm,ecapa-tdnn}. Specifically, we adopt the same architecture with a WavLM-based speaker verification model\footnote{\href{https://huggingface.co/microsoft/wavlm-base-plus-sv}{https://huggingface.co/microsoft/wavlm-base-plus-sv}}. The total parameter count of $\mathcal{M}_{style}$ is 463M.

During training, we use AdamW~\cite{adamw} optimizer with a peak learning rate of 1e-4, linearly warmed up for 2K steps and decays over the rest of training. It is trained for 500K updates. During inference, we can use the default reference-style-enhanced continuation (Figure~\ref{fig:model-ar-inference-enhanced}) or the reference-global-guided continuation (Figure~\ref{fig:model-ar-inference-global}). We generate evaluation samples with specific sampling parameters: top-k is 25, top-p is 0.9, and temperature is 0.8.


\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\linewidth]{img/model-ar-inference-only-global.png}
    \caption{\textit{Reference-global-guided} continuation of $\mathcal{M}_{style}$ for inference.}
    \label{fig:model-ar-inference-global}
\end{figure}

\subsection{Content-Style Modeling (Text as Input)}\label{sec:appendix-content-style-modeling-text}

Compared to $\mathcal{M}_{style}$, the only difference of  $\widetilde{\mathcal{M}}_{style}$ is that its input becomes text tokens, rather than the duration reduced content tokens. Specifically, we adopt the Grapheme-to-Phoneme (G2P) method and use the same phonemization tokenizer as Voicebox~\cite{voicebox}. All the hyper parameters of training and inference are same as $\mathcal{M}_{style}$.

\subsection{Acoustic Modeling}\label{sec:appendix-acoustic-modeling}

For the acoustic modeling stage, we follow the flow matching model implementation of Voicebox~\cite{voicebox}. Specifically, we randomly mask 70\%-100\% of the frames to create $\bm{y}_1^{mis}$. We employ the midpoint ODE solver with a step size of 0.0625 (NFE=32). The $\sigma$ of the optimal transport path of flow matching is 1e-5. The transformer has 24 layers, 16 attention heads, 1024/4096 embedding/feed-forward network (FFN) dimension. Its parameter count is 334M.

Our target Mel spectrogram is at 24 kHz with 100 Mel bands. It is normalized with the global mean (-5.8843) and standard deviation (2.2615) to stabilize training~\cite{voicebox}. During training, Mel spectrogram length is capped at 1,600 frames and chunked randomly if length exceeds. We use Adam~\cite{adam} optimizer with a peak learning rate of 1e-4, linearly warmed up for 5K steps and decays over the rest of training. It is trained for 500K updates. During inference, we employ the midpoint ODE solver with a step size of 0.0625 (NFE=32).

We apply the classifier free guidance (CFG)~\cite{cfg-diffusion} to improve the generation quality like other works~\cite{voicebox,cosyvoice,fireredtts}. Specifically, we randomly drop the conditions, i.e., $\bm{y}_1^{ctx}$ and $\bm{Q}_s({u})$, with a probability of $p_{uncond}$. During inference, the modified vector filed $f^{'}_t$ becomes $f^{'}_t(\bm{y}_t, t, \bm{y}_1^{ctx}, \bm{Q}_s({u})) = (1 + \alpha) f_t(\bm{y}_t, t, \bm{y}_1^{ctx}, \bm{Q}_s({u})) - \alpha f_t(\bm{y}_t, t)$, where $\alpha$ is the strength of the guidance. In practice, we follow Voicebox and set $p_{uncond}$ as 0.2 and $\alpha$ as 0.7.
 
\subsection{Vocoder}\label{sec:appendix-vocoder}

We use BigVGAN~\cite{bigvgan} as vocoder to synthesis waveform from Mel spectrogram. We fine-tune from the official released checkpoint bigvgan\_24khz\_100band\footnote{\href{https://github.com/NVIDIA/BigVGAN}{https://github.com/NVIDIA/BigVGAN}} using our 60K hours training data. Its parameter count is 112M.

\section{Details of baselines}\label{sec:appendix-baselines}

\subsection{Zero-Shot Timbre Imitation and Voice Imitation (Conversion Task)}

\begin{itemize}[itemsep=1ex,leftmargin=3ex]
    \item \textbf{HierSpeech++}~\cite{hierspeech++}: It utilizes \href{https://huggingface.co/facebook/mms-300m}{MMS}~\cite{mms} (pretrained on 500K hours of data from over 1000 languages) to extract content features. It is designed based on the VITS architecture~\cite{vits}, and is trained on 2.8k hours sourced from Libri-light~\cite{libri-light} and LibriTTS~\cite{libritts}. We use the officially released checkpoint\footnote{\href{https://github.com/sh-lee-prml/HierSpeechpp}{https://github.com/sh-lee-prml/HierSpeechpp}} to generate samples.
    \item \textbf{LM-VC}~\cite{lmvc}: It is an autoregressive hierarchical transformer that predicts SoundStream~\cite{soundstream} codecs from soft units similar to HuBERT k-means tokens~\cite{softvc}, trained on the Libri-light dataset~\cite{libri-light}. We obtain the generated samples from the authors.
    \item \textbf{UniAudio}~\cite{uniaudio}: It is an autoregressive transformer capable of performing multiple audio generation tasks, using 500-cluster K-means tokens from HuBERT-base (that is pre-trained on LibriSpeech~\cite{librispeech}) to predict their proposed acoustic codecs, with training data comprising approximately 80K hours of speech and 20K hours of other audio data. We use the officially released checkpoint\footnote{\href{https://github.com/yangdongchao/UniAudio}{https://github.com/yangdongchao/UniAudio}} to generate samples.
    \item \textbf{FACodec}~\cite{ns3}: It adopts an auto-encoder and residual vector quantization based architecture. It decouples the raw waveform into factorized attributes through ASR, F0 prediction, and speaker classification tasks, trained on the Libri-light dataset~\cite{libri-light}. We use the released checkpoint in Amphion~\footnote{\href{https://huggingface.co/amphion/naturalspeech3\_facodec}{https://huggingface.co/amphion/naturalspeech3\_facodec}}~\cite{amphion,amphion_v0.2} (which is implemented by the authors) to generate samples.
\end{itemize}

\subsection{Zero-Shot Style Imitation}

\begin{itemize}[itemsep=1ex,leftmargin=3ex]
    \item \textbf{ASR-AC}~\cite{asr-ac}: It uses an ASR model based on wav2vec 2.0\footnote{\href{https://pytorch.org/audio/0.10.0/pipelines.html\#torchaudio.pipelines.WAV2VEC2\_ASR\_LARGE\_LV60K\_960H}{https://pytorch.org/audio/0.10.0/pipelines.html\#torchaudio.pipelines.WAV2VEC2\_ASR\_LARGE\_LV60K\_960H}}~\cite{wav2vec2} (that is pre-trained on 60K hours of Libri-light~\cite{libri-light} and fine-tuned on 1K hours of LibriSpeech~\cite{librispeech}) to extract the one-hot text predictions from speech, i.e., $\bm{x}_{asr}$ in our paper (Section~\ref{sec:results-effect-of-codebook-size}). It adopts a transformer encoder and a HiFi-GAN decoder to reconstruct waveforms conditioned on $\bm{x}_{asr}$, the accent labels, and F0, which is trained on about 700 hours of accented corpus. We use 30 samples from its demo website\footnote{\href{https://accent-conversion.github.io/}{https://accent-conversion.github.io/}} to evaluate, including English accents' conversions from British to American, British to Hindi, and Hindi to American.
    \item \textbf{VoiceShop}~\cite{voiceshop}: To achieve accent conversion, the authors first uses an conformer-based ASR model (that is trained by 40K hours of their private corpus) to extract the hidden features (BNF). Then, they create about 300 hours of parallel conversion corpus based on a commercial accented TTS system. Finally, they adopt an encoder-decoder transformer to learn the BNF's mapping between parallel corpus. We use 17 samples from its demo website\footnote{\href{https://voiceshopai.github.io/}{https://voiceshopai.github.io/}} to evaluate, including English accents' conversions among American, British, Hindi, and Mandarin.
    \item \textbf{Conv-Speak}~\cite{convertandspeak}: The authors formulate accent conversion from source's content tokens to target's content tokens. They propose to self-supervised pre-train on content tokens like BART~\cite{bart}, in order to relieve the requirements of parallel data. They adopt the 500-cluster K-means of HuBERT-Base\footnote{\href{https://pytorch.org/audio/0.10.0/pipelines.html\#torchaudio.pipelines.HUBERT\_BASE}{https://pytorch.org/audio/0.10.0/pipelines.html\#torchaudio.pipelines.HUBERT\_BASE}} (that is pre-trained on 1K hours of LibriSpeech~\cite{librispeech}) as content tokens. The conversion model is trained on about 600 hours of data, including about 1 hour of parallel data. We use 24 samples  from its demo website\footnote{\href{https://convert-and-speak.github.io/demo/}{https://convert-and-speak.github.io/demo/}} to evaluate, including English accents' conversions from Hindi and Mandarin to American.
    \item \textbf{Emovox}~\cite{emovox}: To achieve emotion conversion, the authors design a recognition encoder to push its output (i.e., emotion-agnostic features) closely with phoneme transcriptions. The conversion model is based on a sequence-to-sequence decoder, that can reconstruct the Mel spectrogram conditioned on the emotion-agnostic features and emotion labels. The model is trained on about 80 hours data sourced from VCTK~\cite{vctk} and ESD~\cite{esd}. We use 24 samples from its demo website\footnote{\href{https://kunzhou9646.github.io/Emovox\_demo/}{https://kunzhou9646.github.io/Emovox\_demo/}} to evaluate, including emotion conversions from Neutral to Angry, Happy, and Sad.
\end{itemize}

Notably, Vevo-Style (ASR) and Vevo-Style employ a zero-shot approach to achieve style imitation -- which is rarely seen in existing research. Therefore, for these two, we use the aforementioned evaluation samples as the source and additionally prepare style references. Specifically, for accent imitation, we prepare references in three English accents: American, Hindi, and Mandarin, with four samples each (two males, and two females). For emotion imitation, we prepare references for three emotions: Angry, Happy, and Sad, with four samples each (two males, and two females). All these references are randomly sampled from ACCENT and EMOTION.

\subsection{Zero-Shot Voice Imitation (Synthesis Task)}

\begin{itemize}[itemsep=1ex,leftmargin=3ex]
    \item \textbf{VALL-E}~\cite{valle}: It is a classic AR model for zero-shot TTS. It utilizes the transformer to predict EnCodec~\cite{encodec} codecs. We use the released checkpoint in Amphion~\footnote{\href{https://github.com/open-mmlab/Amphion/tree/main/egs/tts/VALLE\_V2}{https://github.com/open-mmlab/Amphion/tree/main/egs/tts/VALLE\_V2}}~\cite{amphion,amphion_v0.2} to generate samples, which is pre-trained on 45K hours of MLS English set~\cite{mls}.
    \item \textbf{Voicebox}~\cite{voicebox}: It applies the flow matching transformer to both duration model and acoustic model. We reproduce it with the help of the authors.
    \item \textbf{VoiceCraft}~\cite{voicecraft}: It uses an AR transformer to predict EnCodec~\cite{encodec} codecs. Compared to VALL-E, it proposes token rearrangement and delayed stacking strategies to enhance the model learning. We use the officially released checkpoint\footnote{\href{https://github.com/jasonppy/VoiceCraft}{https://github.com/jasonppy/VoiceCraft}} to generate samples, which is pre-trained on 10K hours of Gigaspeech~\cite{gigaspeech}.
    \item \textbf{CosyVoice}~\cite{cosyvoice}: It proposes a semantic tokenizer that is supervised by ASR task. It contains an AR transformer to predict the semantic tokens from text, and a flow-matching transformer to predict Mel spectrograms. We use the officially released checkpoint\footnote{\href{https://github.com/FunAudioLLM/CosyVoice}{https://github.com/FunAudioLLM/CosyVoice}} to generate samples, which is pre-trained on 171K hours of in-the-wild, multilingual, and private data.
    \item \textbf{MaskGCT}~\cite{maskgct}: It consists of two-stage discrete diffusion models. It is based on the hidden features of w2v-bert 2.0\footnote{\href{https://huggingface.co/facebook/w2v-bert-2.0}{https://huggingface.co/facebook/w2v-bert-2.0}} that pre-trained on 4.5M hours to obtain the semantic tokens. Its TTS model is trained on 100K hours of in-the-wild and multilingual data~\cite{emilia}. We use the released checkpoint in Amphion~\footnote{\href{https://huggingface.co/amphion/MaskGCT}{https://huggingface.co/amphion/MaskGCT}}~\cite{amphion,amphion_v0.2} to generate samples.
\end{itemize}


\section{Additional Experimental results}

\subsection{Effect of the Vocabulary Size of the VQ-VAE Tokenizer}\label{sec:appendix-effect-of-K}

In Section~\ref{sec:results-effect-of-codebook-size}, we have already demonstrated the impact of different vocabulary sizes in the VQ-VAE codebook on $\mathcal{M}_{acoustic}$ (i.e., disentanglement capability). In this section, we aim to present two complementary experimental results. First, we explore the effects of a wider range of vocabulary sizes (from the smallest at 8 to the largest at 16,384) on the produced tokens. Second, we investigate the impact of various combinations of $K_{c}$ and $K_{s}$ on Vevo-Voice.

\subsubsection{Effect on Phonetic Discriminability}

We explore the phonetic discriminability of different representations, inspired by AudioLM~\cite{audiolm}. Specifically, we measure phonetic discriminability using the ABX error rate, a distance-based metric that evaluates a set of phoneme trigrams differing only in the central phoneme (e.g., ``bit" vs. ``bet"). The ABX error rate assesses how often a random instance X of a trigram (``bit") is closer to an instance B of another trigram (``bet") rather than to another instance A of the same trigram (``bit"). We evaluate scenarios where all three sounds A, B, and X are uttered by the same speaker (within-speaker) and where the same speaker utters A and B but X comes from a different speaker (across-speaker). We calculate ABX using scripts provided with the Libri-light dataset\footnote{\href{https://github.com/facebookresearch/libri-light/tree/main/eval}{https://github.com/facebookresearch/libri-light/tree/main/eval}}~\cite{libri-light}, employing default settings and reporting scores obtained on the LibriSpeech dev-clean dataset~\cite{librispeech}. The results are displayed in Table~\ref{tab:phonetic-discriminability}. Note that for K-means and VQ-VAE tokens, we calculate the ABX error based on the centroid's vector corresponding to each token.

\begin{table}[ht]
\caption{ABX error rate ($\downarrow$) of different representations. (within/across speakers)}
% \vspace{-5mm}
\label{tab:phonetic-discriminability}
\begin{center}
\resizebox{\textwidth}{!}{%
\begin{threeparttable}
    \begin{tabular}{c|ccccccccccccc}
    \toprule
     \diagbox{\textbf{Repr}}{\textbf{\#Vocab}} & \textbf{8} & \textbf{16} & \textbf{32} & \textbf{64} & \textbf{128} & \textbf{256} & \textbf{512} & \textbf{1024} & \textbf{2048} & \textbf{4096} & \textbf{8192} & \textbf{16384} \\
    \midrule
    \makecell[c]{PPG features} & \multicolumn{12}{c}{\makecell[c]{6.1 / 7.0}} \\
    \midrule
     \makecell[c]{18th layer features} & \multicolumn{12}{c}{\makecell[c]{7.6 / 9.5}} \\
    \midrule
    \makecell[c]{K-means tokens} & - & - & \makecell[c]{17.2 /\\19.8} & \makecell[c]{14.5 /\\17.7} & \makecell[c]{12.0 /\\14.0} & \makecell[c]{9.9 /\\11.5} & \makecell[c]{8.8 /\\10.3} & \makecell[c]{7.8 /\\9.0} & - & - & - & - \\
    \midrule
    \makecell[c]{VQ-VAE tokens} & \makecell[c]{16.4 /\\18.2} & \makecell[c]{13.1 /\\14.6} & \makecell[c]{12.7 /\\14.2} & \makecell[c]{13.0 /\\14.9} & \makecell[c]{12.7 /\\14.8} & \makecell[c]{12.7 /\\14.6} & \makecell[c]{11.1 /\\13.0} & \makecell[c]{10.0 /\\11.8} & \makecell[c]{10.1 /\\12.0} & \makecell[c]{10.4 /\\12.4} & \makecell[c]{9.9 /\\11.9} & \makecell[c]{10.0 /\\11.9} \\
    \bottomrule
    \end{tabular}%
    \begin{tablenotes}
        \footnotesize{
        \item[*] PPG features are obatined from HuBERT-ASR-Large, while the others are from HuBERT-Large. K-means and VQ-VAE tokens are quantized on the 18th layer features of HuBERT-Large.
        }
    \end{tablenotes}    
\end{threeparttable}
}
\end{center}
\end{table}

From the table, we observe that: (1) PPG features demonstrate the best phonetic discriminability, highlighting the advantages of fine-tuning with ASR tasks; (2) For K-means tokens, increasing the vocabulary size from 32 to 1024 continuously improves their phonetic discriminability, indicating an ongoing enhancement in their capacity to represent linguistic content; (3) For VQ-VAE tokens, we see a gradual improvement in phonetic discriminability from 8 to 1024, but beyond 1024, this metric begins to converge. However, we know that as VQ-VAE tokens' vocabulary size increases from 1024 to 4096 to 16384, their style information still increases (as indicated by rising FPC scores in Table~\ref{tab:hubert-vc}). From these observations, we can conclude that beyond 1024, the representation ability of VQ-VAE tokens for linguistic content stabilizes, and any increase in vocabulary size is likely allocated to storing style information such as F0; (4) Comparing K-means and VQ-VAE tokens, it's evident that VQ-VAE tokens are less sensitive to changes in vocabulary size in terms of representing linguistic content (e.g., VQ-VAE (32) and K-means (128) exhibit nearly identical ABX error rates), suggesting that a smaller vocabulary can suffice for a content tokenizer. Recent research has also delved into this aspect, attributing the differences to the distinct optimization algorithms used by the two methods~\cite{repcodec}.

\subsubsection{Effect on Vevo-Voice}

We explore the effects of different ($K_c$, $K_s$) combinations on Vevo-Voice, with the results presented in Table~\ref{tab:effect-of-k-for-ar}. Our observations include: (1) A significant drop in intelligibility occurs when $K_c$ changes from 32 to 16, indicating that a smaller vocabulary size for the content tokenizer leads to loss of linguistic content information; (2) When $K_s$ decreases from 4096 to 1024, all metrics decline. We hypothesize that while a reduction in $K_s$ might lessen the learning difficulty for $\mathcal{M}_{style}$, a smaller $K_s$ also results in a decrease in the quality of the final generated audio for $\mathcal{M}_{acoustic}$.

\begin{table}[ht]
\caption{Effect of different ($K_c$, $K_s$) for Vevo-Voice.}
% \vspace{-3mm}
\label{tab:effect-of-k-for-ar}
\centering
\resizebox{0.7\textwidth}{!}{%
\begin{threeparttable}
    \begin{tabular}{cc|ccccc}
    \toprule
    \makecell[c]{\textbf{Content}\\ \textbf{Tokenizer} ($K_c$)} & \makecell[c]{\textbf{Content-style}\\ \textbf{Tokenizer} ($K_s$)} & \makecell[c]{\textbf{WER}\\($\downarrow$)} & \makecell[c]{\textbf{S-SIM}\\($\uparrow$)} & \makecell[c]{\textbf{A-SIM}\\($\uparrow$)} & \makecell[c]{\textbf{E-SIM}\\($\uparrow$)} \\
    \midrule
    32 & 4096 & \textbf{15.214} & \textbf{0.517} & \textbf{0.614} & \textbf{0.872}  \\
    32 & 1024 & 18.523 & 0.502 & 0.609 & 0.860  \\
    16 & 4096 & 23.351 & 0.509 & 0.613 & 0.865  \\
    \bottomrule
    \end{tabular}%
    \begin{tablenotes}
        \footnotesize{\item[*] Vevo-Voice uses $K_c$ as 32 and $K_s$ as 4096. E-SIM is evaluated only on EMOTION. A-SIM is evaluated only on ACCENT.}
    \end{tablenotes}
\end{threeparttable}
}
\end{table}

\subsection{Zero-Shot Voice Imitation (Synthesis Task)}\label{sec:appendix-results-tts}


In Section~\ref{sec:results-tts}, we present the performance of Vevo-TTS in zero-shot imitation (synthesis) tasks. We have detailed its comparative performance against baselines on all four evaluation sets (AB, CV, ACCENT, and EMOTION) in Table~\ref{tab:results-tts-details}. We can observe that: (1) In comparison with AR baselines, Vevo-TTS exhibits a clear advantage over VALL-E and VoiceCraft across various metrics on all datasets. Compared to the state-of-the-art CosyVoice, although Vevo-TTS is trained solely on 60K hours of Audiobook data, it performs better in some metrics such as Naturalness CMOS (AB, ACCENT, EMOTION), Speaker S-MOS (EMOTION), and notably in style imitation-related metrics like Accent S-MOS and Emotion S-MOS. This demonstrates the high effectiveness of the AR TTS model implemented using the content-style tokens proposed in this paper. (2) When compared with Non-AR baselines, Vevo-TTS falls short on WER across all datasets compared to Voicebox and MaskGCT. This underscores the stability still needed in AR models, indicating significant room for improvement.

\begin{table}[t]
\caption{Results on zero-shot imitation (synthesis) task. (S-MOS: Similarity MOS)}
\label{tab:results-tts-details}
\centering
\begin{subtable}{\textwidth}
    % \caption{\note{LibriTTS test-clean and SeedEval-TTS}}
    \label{tab:results-voice-imitation-synthesis-libritts}
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{l|c|c|cc|cc}
    \toprule
    \multicolumn{1}{c|}{\textbf{Model}} & \makecell[c]{\textbf{AR?}} & \makecell[c]{\textbf{Training} \textbf{Data}} & \textbf{WER ($\downarrow$)} & \makecell[c]{\textbf{Speaker}\\ \textbf{SIM ($\uparrow$)}} & \makecell[c]{\textbf{Naturalness}\\ \textbf{CMOS ($\uparrow$)}} & \makecell[c]{\textbf{Speaker}\\ \textbf{S-MOS ($\uparrow$)}} \\
    \midrule \midrule
    \multicolumn{7}{c}{\textbf{\textit{AB}}} \\
    \midrule \midrule
    Ground Truth & - & - & 2.845 & 0.763 & 0.00 \textcolor{white}{$_{\scriptscriptstyle \pm \text{0.00}}$} & - \\
    CosyVoice~\cite{cosyvoice} & \ding{51} & 171k hours, In-the-wild  & {3.647} & {0.727} & -0.44 $_{\scriptscriptstyle \pm \text{0.16}}$ & 4.17 $_{\scriptscriptstyle \pm \text{0.15}}$ \\
    MaskGCT~\cite{maskgct} & \ding{55} & 100K hours, In-the-wild  & 3.841  & {0.781} & -0.21 $_{\scriptscriptstyle \pm \text{0.08}}$ & 4.30 $_{\scriptscriptstyle \pm \text{0.22}}$ \\ \midrule
    VALL-E~\cite{valle} & \ding{51} & 45K hours, MLS English~\cite{mls}  & 8.204 & 0.551 & -0.95 $_{\scriptscriptstyle \pm \text{0.39}}$ & 3.27 $_{\scriptscriptstyle \pm \text{0.25}}$ \\
    Voicebox~\cite{voicebox} & \ding{55}  & 60K hours, Audiobook  & \textbf{3.175} & \textbf{0.631} &  -0.55 $_{\scriptscriptstyle \pm \text{0.15}}$ & \underline{3.49} $_{\scriptscriptstyle \pm \text{0.14}}$  \\
    VoiceCraft~\cite{voicecraft} & \ding{51} & 10K hours, Gigaspeech~\cite{gigaspeech} & 4.737  & \underline{0.570} & \underline{-0.41} $_{\scriptscriptstyle \pm \text{0.18}}$ & 3.41 $_{\scriptscriptstyle \pm \text{0.13}}$  \\
    \midrule
    Vevo-TTS & \ding{51} & 60K hours, Audiobook & \underline{3.672} & {0.593} & \textbf{-0.31} $_{\scriptscriptstyle \pm \text{0.14}}$ & \textbf{3.58} $_{\scriptscriptstyle \pm \text{0.15}}$ \\
    \midrule \midrule
    \multicolumn{7}{c}{\textbf{\textit{CV}}} \\
    \midrule \midrule
    Ground Truth & - & - & 1.426 & 0.723 & 0.00 \textcolor{white}{$_{\scriptscriptstyle \pm \text{0.00}}$} & - \\
    CosyVoice~\cite{cosyvoice} & \ding{51} & 171k hours, In-the-wild  & 3.500 & {0.627} & 0.11 $_{\scriptscriptstyle \pm \text{0.19}}$ & 3.88 $_{\scriptscriptstyle \pm \text{0.12}}$  \\
    MaskGCT~\cite{maskgct} & \ding{55} & 100K hours, In-the-wild  & {2.573}  & {0.688} & 0.08 $_{\scriptscriptstyle \pm \text{0.25}}$ & 4.33 $_{\scriptscriptstyle \pm \text{0.14}}$ \\
    \midrule
    VALL-E~\cite{valle} & \ding{51} & 45K hours, MLS English~\cite{mls}  & 6.129 & 0.433 & -1.02 $_{\scriptscriptstyle \pm \text{0.36}}$ & 2.68 $_{\scriptscriptstyle \pm \text{0.52}}$  \\
    Voicebox~\cite{voicebox} & \ding{55}  & 60K hours, Audiobook  & \textbf{2.129} & 0.500 & -0.12 $_{\scriptscriptstyle \pm \text{0.19}}$ & 2.91 $_{\scriptscriptstyle \pm \text{0.08}}$   \\
    VoiceCraft~\cite{voicecraft} & \ding{51} & 10K hours, Gigaspeech~\cite{gigaspeech} & 6.353 & \underline{0.446} & \textbf{-0.10} $_{\scriptscriptstyle \pm \text{0.25}}$ & \underline{3.02} $_{\scriptscriptstyle \pm \text{0.21}}$ \\
    \midrule
    Vevo-TTS & \ding{51} & 60K hours, Audiobook & \underline{2.687} & \textbf{0.513} & \underline{-0.11} $_{\scriptscriptstyle \pm \text{0.19}}$ & \textbf{3.83} $_{\scriptscriptstyle \pm \text{0.18}}$ 
    % \\
    % \bottomrule
    \end{tabular}%
    }    
\end{subtable}
\hfill
\begin{subtable}{\textwidth}
    % \caption{\note{Accented and Emotional Corpus}}
    \label{tab:results-voice-imitation-synthesis-accent}
    \resizebox{\textwidth}{!}{%
    \begin{threeparttable}
            \begin{tabular}{l|c|ccc|ccc}
    \toprule \midrule
    \multicolumn{8}{c}{\textbf{\textit{ACCENT}}} \\
    \midrule \midrule
    \multicolumn{1}{c|}{\textbf{Model}} & \makecell[c]{\textbf{AR?}} & \textbf{WER ($\downarrow$)} & \makecell[c]{\textbf{Speaker}\\ \textbf{SIM ($\uparrow$)}} & \makecell[c]{\textbf{Accent}\\ \textbf{SIM ($\uparrow$)}} & \makecell[c]{\textbf{Naturalness}\\ \textbf{CMOS ($\uparrow$)}} & \makecell[c]{\textbf{Speaker}\\ \textbf{S-MOS ($\uparrow$)}} & \makecell[c]{\textbf{Accent}\\ \textbf{S-MOS ($\uparrow$)}} \\
    \midrule
    Ground Truth & - & 10.903 & 0.747 & 0.633 & 0.00 \textcolor{white}{$_{\scriptscriptstyle \pm \text{0.00}}$} & - & - \\
    CosyVoice~\cite{cosyvoice} & \ding{51} & 6.660 & {0.653} & {0.640} & 0.10 $_{\scriptscriptstyle \pm \text{0.19}}$ & 4.23 $_{\scriptscriptstyle \pm \text{0.18}}$ & 3.99 $_{\scriptscriptstyle \pm \text{0.23}}$ \\
    MaskGCT~\cite{maskgct} & \ding{55} & {6.382} & {0.717} & {0.645} & 0.23 $_{\scriptscriptstyle \pm \text{0.20}}$ & 4.24 $_{\scriptscriptstyle \pm \text{0.16}}$ & 4.38 $_{\scriptscriptstyle \pm \text{0.14}}$ \\
    \midrule
    VALL-E~\cite{valle} & \ding{51}  & 10.721 & 0.403 & 0.485 & -1.04 $_{\scriptscriptstyle \pm \text{0.50}}$ & 3.12 $_{\scriptscriptstyle \pm \text{0.41}}$ & 2.77 $_{\scriptscriptstyle \pm \text{0.45}}$ \\
    Voicebox~\cite{voicebox} & \ding{55}  & \textbf{6.181} & 0.475 & \underline{0.575} & \underline{-0.55} $_{\scriptscriptstyle \pm \text{0.22}}$ & \underline{3.93} $_{\scriptscriptstyle \pm \text{0.25}}$ & \underline{3.49} $_{\scriptscriptstyle \pm \text{0.29}}$ \\
    VoiceCraft~\cite{voicecraft} & \ding{51} & 10.072 & 0.438 & 0.517 & -0.39 $_{\scriptscriptstyle \pm \text{0.22}}$ & 3.51 $_{\scriptscriptstyle \pm \text{0.33}}$ & 3.29 $_{\scriptscriptstyle \pm \text{0.28}}$ \\
    \midrule
    Vevo-TTS & \ding{51} & \underline{9.673} & \textbf{0.544} & \textbf{0.579} & \textbf{0.12} $_{\scriptscriptstyle \pm \text{0.20}}$ & \textbf{4.11} $_{\scriptscriptstyle \pm \text{0.20}}$ & \textbf{4.12} $_{\scriptscriptstyle \pm \text{0.21}}$ \\
    \midrule \midrule
    \multicolumn{8}{c}{\textbf{\textit{EMOTION}}} \\
    \midrule \midrule
    \multicolumn{1}{c|}{\textbf{Model}} & \makecell[c]{\textbf{AR?}} & \textbf{WER ($\downarrow$)} & \makecell[c]{\textbf{Speaker}\\ \textbf{SIM ($\uparrow$)}} & \makecell[c]{\textbf{Emotion}\\ \textbf{SIM ($\uparrow$)}} & \makecell[c]{\textbf{Naturalness}\\ \textbf{CMOS ($\uparrow$)}} & \makecell[c]{\textbf{Speaker}\\ \textbf{S-MOS ($\uparrow$)}} & \makecell[c]{\textbf{Emotion}\\ \textbf{S-MOS ($\uparrow$)}} \\
    \midrule
    Ground Truth & - & 11.792 & 0.673 & 0.936 & 0.00 \textcolor{white}{$_{\scriptscriptstyle \pm \text{0.00}}$} & - & - \\
    CosyVoice~\cite{cosyvoice} & \ding{51} & {10.139} & {0.575} & {0.839} & -0.45 $_{\scriptscriptstyle \pm \text{0.18}}$ & 3.98 $_{\scriptscriptstyle \pm \text{0.19}}$ & 3.66 $_{\scriptscriptstyle \pm \text{0.19}}$ \\
    MaskGCT~\cite{maskgct} & \ding{55} & {12.502} & {0.600} & 0.822 & -0.31 $_{\scriptscriptstyle \pm \text{0.17}}$ & 4.07 $_{\scriptscriptstyle \pm \text{0.16}}$ & 3.76 $_{\scriptscriptstyle \pm \text{0.25}}$ \\
    \midrule
    VALL-E~\cite{valle} & \ding{51}  & 15.731 & 0.396 & 0.735 & -1.43 $_{\scriptscriptstyle \pm \text{0.33}}$ & 2.52 $_{\scriptscriptstyle \pm \text{0.38}}$ & 2.63 $_{\scriptscriptstyle \pm \text{0.36}}$ \\
    Voicebox~\cite{voicebox} & \ding{55}  & \textbf{12.647} & \underline{0.451} & \underline{0.811} & -0.65 $_{\scriptscriptstyle \pm \text{0.20}}$ & \underline{3.81} $_{\scriptscriptstyle \pm \text{0.16}}$ & \underline{3.61} $_{\scriptscriptstyle \pm \text{0.19}}$ \\
    VoiceCraft~\cite{voicecraft} & \ding{51} & 16.042 & 0.345 & 0.788 & \underline{-0.60} $_{\scriptscriptstyle \pm \text{0.24}}$ & 3.42 $_{\scriptscriptstyle \pm \text{0.31}}$ & 3.52 $_{\scriptscriptstyle \pm \text{0.25}}$ \\
    \midrule
    Vevo-TTS & \ding{51} & \underline{14.458} & \textbf{0.466} & \textbf{0.840} & \textbf{-0.39} $_{\scriptscriptstyle \pm \text{0.15}}$ & \textbf{3.99} $_{\scriptscriptstyle \pm \text{0.22}}$ & \textbf{4.03} $_{\scriptscriptstyle \pm \text{0.19}}$  \\
    \bottomrule
    \end{tabular}%
    \begin{tablenotes}
        \footnotesize{
        \item[*] The best and the second best results among VALL-E, Voicebox, VoiceCraft, and Vevo-TTS are shown in \textbf{bold} and by \underline{underlined}.
        }
    \end{tablenotes}
    \end{threeparttable}
    }    
\end{subtable}
\end{table}

\section{Subjective Evaluation}\label{sec:appendix-subeval}

\subsection{Background of Subjects}

We hired dozens of subjects on a paid basis to complete the subjective evaluations. These individuals have extensive experience in providing subjective assessments of audio generated by AI models. They have lived in English-speaking countries for extended periods and are highly familiar with various common English accents, including American, British, Hindi, and Mandarin. Each audio sample in our evaluation was rated at least ten times.

\subsection{Metrics and Questionnaires}

We have developed an automated subjective evaluation interface. For each item to be evaluated, users will see three components: the System Interface (i.e., the audio to be evaluated), the Questionnaire, and the Scoring Criteria.

\subsubsection{Naturalness MOS}

\textbf{System Interface}\quad
One audio to be evaluated (with target text)

\textbf{Questionnaire}\quad
How human-like is the speech in the clip? Does it sound like a real human who is engaged in the topic, or does it sound like an AI that doesn't understand what is being said?

\textbf{Scoring criteria}\quad 5 (A perfect imitation of human speech), 4 (Exceeds my expectations for AI voices), 3 (Meets my expectations for AI voices), 2 (A subpar representation of human speech), 1 (Very poor artificial speech)

\subsubsection{Speaker Similarity MOS}

\textbf{System Interface}\quad One reference audio, One audio to be evaluated

\textbf{Questionnaire}\quad Ignore the content and audio quality, just pay attention to the voice of the person. How similar is the voice to be evaluated compared to the reference voice?

\textbf{Scoring criteria}\quad 
5 (Excellent, sounds like exactly the same person), 4 (Good, sounds like a similar person), 3 (Fair, sounds like a slightly similar person), 2 (Poor, sounds like a different person mostly), 1 (Bad, sounds like a completely different person)

\subsubsection{Accent Similarity MOS}

\textbf{System Interface}\quad One reference audio, One audio to be evaluated

\textbf{Questionnaire}\quad Ignore the vocal characteristics (who is speaking), just pay attention to the accent of the speaker. Is the accent similar to the reference voice?

\textbf{Scoring criteria}\quad 5 (Excellent, sounds like exactly the same accent), 4 (Good, sounds like a similar accent), 3 (Fair, sounds like a slightly similar accent), 2 (Poor, sounds like a different accent mostly), 1 (Bad, sounds like a completely different accent)

\subsubsection{Emotion Similarity MOS}

\textbf{System Interface}\quad One reference audio, One audio to be evaluated

\textbf{Questionnaire}\quad Ignore the vocal characteristics (who is speaking), just pay attention to the emotion of the speaker. Is the emotion similar to the reference voice?

\textbf{Scoring criteria}\quad 5 (Excellent, sounds like exactly the same emotion), 4 (Good, sounds like a similar emotion), 3 (Fair, sounds like a slightly similar emotion), 2 (Poor, sounds like a different emotion mostly), 1 (Bad, sounds like a completely different emotion)

\subsubsection{Prosody Similarity MOS}

\textbf{System Interface}\quad One reference audio, One audio to be evaluated

\textbf{Questionnaire}\quad Ignore the vocal characteristics (who is speaking), just pay attention to the speaking style (how to speak). Is the speaking style (pace, tone, stress, intonation, pitch, emotion) consistent and identical with the reference voice?

\textbf{Scoring criteria}\quad 5 (Excellent, sounds like a completely identical style), 4 (Good, sounds like a highly consistent style), 3 (Fair, sounds like a slightly similar style), 2 (Poor, sounds mostly like a different style), 1 (Bad, sounds like a completely different style)

\subsubsection{Naturalness CMOS}

\textbf{System Interface}\quad One reference audio, One audio to be evaluated (with target text)

\textbf{Questionnaire}\quad Compared to the reference audio, is the quality and the human likeness of the audio to be evaluated better or worse?

\textbf{Scoring criteria}\quad -3 (Much worse), -2 (Worse), -1 (Slightly worse), 0 (No preference), 1 (Slightly better), 2 (Better), 3 (Much better)

\subsubsection{Accentedness CMOS}

\textbf{System Interface}\quad One accent label, One reference audio, One audio to be evaluated

\textbf{Questionnaire}\quad Assume that we want to generate the voice whose accent is \textit{[accent label]}. Compared to the reference audio, is the accentedness of the audio to be evaluated better or worse?

\textbf{Scoring criteria}\quad -3 (Much worse), -2 (Worse), -1 (Slightly worse), 0 (No preference), 1 (Slightly better), 2 (Better), 3 (Much better)

\subsubsection{Emotiveness CMOS}

\textbf{System Interface}\quad One emotion label, One reference audio, One audio to be evaluated

\textbf{Questionnaire}\quad Assume that we want to generate the voice whose emotion is \textit{[emotion label]}. Compared to the reference audio, is the emotional expressiveness of the audio to be evaluated better or worse?

\textbf{Scoring criteria}\quad -3 (Much worse), -2 (Worse), -1 (Slightly worse), 0 (No preference), 1 (Slightly better), 2 (Better), 3 (Much better)


\section{Ethics Statement}
As with other powerful new AI innovations, we recognize this technology brings the potential for misuse and unintended harm. We will build a highly effective classifier that can distinguish between authentic speech and audio generated with Vevo to mitigate these possible future risks. 

\end{document}
