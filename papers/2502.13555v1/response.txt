\section{Related Works}
%---------------------------------------------------------

%--------------------------------
% \vspace{-0.1cm}
\para{Graph Neural Networks (GNNs).}
%--------------------------------
GNNs are gaining significant success in many problem domains **Kipf, "Semi-Supervised Classification with Graph Convolutional Networks"**.
%
They learn node representation by aggregating information from the neighboring nodes on the graph topology. 
%
Most of the existing GNN architectures are on homogeneous graphs **Scarselli, "Graph Neural Tangent Kernel: A Theoretical Framework for Graph Neural Networks"**.
%
There are also GNN architectures  operating on heterogeneous graphs to learn its enriched structural information and complex relations
**Ma, "Graph Attention Network"** .
%
However, due to limited samples, it is difficult to approximate the true data distribution, especially in the graph domain.
%
Hence, an effective graph data augmentation algorithm is needed to boost the performance of GNNs.

%--------------------------------
\para{Graph Data Augmentation.}
%--------------------------------
Graph data augmentation (GDA) aims to enhance the utility of the input graph data and produce graph samples close to the true data distribution to alleviate the finite sample bias **Wang, "Graph Data Augmentation"**.
%
Most of the existing works focus on perturbating the graph structures or node features/labels to achieve augmentation,
such as node dropping **Zhang, "Node Drop Graph Augmentation"** , edge perturbation **Xu, "Edge Perturbation for Graph Data Augmentation"** , graph rewriting **Tian, "Graph Rewriting for Graph Data Augmentation"** , graph sampling **Kang, "Graph Sampling for Graph Data Augmentation"** , graph diffusion **Kim, "Graph Diffusion for Graph Data Augmentation"** or pseudo-labelling **Gao, "Pseudo-Labelling for Graph Data Augmentation"**.
%
There are also works that adopt a learnable graph data augmenter and design specific losses for training **He, "Learnable Graph Data Augmenter"**.
%
However, these methods mainly focus on the graph structures without considering the contextual information or introducing open-world knowledge.
%
Recent works **Zhu, "LLM-based Graph Data Augmentation"** on LLM-based GDA have achieved promising improvements. However, current LLM-based methods are mostly white-box which require access to the weights or latent features from the LLMs. It is computationally inefficient and impractical, as SOTA LLMs are costly for large-scale experiments and often closed-source.
%
Moreover, these methods mostly focus on node-level context and neglect the higher-order graph structures.
%
Hence, a black-box LLM-based GDA framework with awareness of higher-level graph structure is needed to address these limitations.

%--------------------------------
\para{Graph Learning in Healthcare.}
%--------------------------------
Knowledge distillation from massive EHRs has been a popular topic in healthcare informatics. 
%
To address the longitudinal features in the EHR data, several early works **Shen, "EHR-based Graph Neural Networks"** attempted to learn the EHR features with recurrent neural networks.
%
Since the EHR data represent relational information between entities (e.g., patients make visits), graphical models turn out to be an ideal approach for representing the EHR data **Gao, "Graphical Models for EHR Data"**.
%
GRAM **Liu, "Graph-based Attention Mechanism for Medical Code Representations"** is a well-known method that learns robust medical code representations by adopting a graph-based attention mechanism.
%
However, a critical gap remains in these methods: they do not fully incorporate the rich contextual information available in EHR data. This oversight can lead to a lack of nuanced understanding of patient data, impacting the accuracy and applicability of the insights derived **Chen, "Contextual Graph Neural Networks for Healthcare"**. 
%
Furthermore, there is a notable absence of effective regularization mechanisms for adjusting to the inherent noise in EHR data, which is cluttered with irrelevant or redundant information.

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{general/framework_v8.png}
    % \vspace{-2mm}
    \caption{\textbf{Overview of our proposed DemoGraph framework.} \textnormal{Given a dataset, we first construct a 
    % such as EHR, genetics or social media data, we initially transform them into a human-designed 
    graph \( \mathcal{G}_0 \) to highlight the relational information, and
    then perform context-driven knowledge retrieval by utilizing the original dataset and a frozen generative pre-trained LLM. We conduct contextual, adaptive, sparsity-controllable and granularity-aware prompt learning on the LLM, thus obtaining either concept-specific KGs or important extra concept nodes at different levels after refinement. For the original graph \( \mathcal{G}_0 \), we perform graph data augmentation with the domain-knowledge injection procedure. 
    We train a GNN model on the augmented graph \( \mathcal{G}^\text{aug} \), thus our framework is able to handle a wide range of downstream tasks across various domains depending on the original datasets. }}
    \label{fig: framework}
    % \vspace{-6mm}
\end{figure*}