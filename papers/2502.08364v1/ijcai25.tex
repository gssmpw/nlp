%%%% ijcai25.tex

\typeout{IJCAI--25 Instructions for Authors}

% These are the instructions for authors for IJCAI-25.

\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in

% The file ijcai25.sty is a copy from ijcai22.sty
% The file ijcai22.sty is NOT the same as previous years'
\usepackage{ijcai25}

% Use the postscript times font!
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[switch]{lineno}
\usepackage{multirow}
\usepackage{color}
\usepackage{xcolor}
\input{math}
% Comment out this line in the camera-ready submission
% \linenumbers
\usepackage{svg}

\urlstyle{same}

% the following package is optional:
%\usepackage{latexsym}

% See https://www.overleaf.com/learn/latex/theorems_and_proofs
% for a nice explanation of how to define new theorems, but keep
% in mind that the amsthm package is already included in this
% template and that you must *not* alter the styling.
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}

% Following comment is from ijcai97-submit.tex:
% The preparation of these files was supported by Schlumberger Palo Alto
% Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% Shirley Jowell, of Morgan Kaufmann Publishers, and Peter F.
% Patel-Schneider, of AT\&T Bell Laboratories collaborated on their
% preparation.

% These instructions can be modified and used in other conferences as long
% as credit to the authors and supporting agencies is retained, this notice
% is not changed, and further modification or reuse is not restricted.
% Neither Shirley Jowell nor Peter F. Patel-Schneider can be listed as
% contacts for providing assistance without their prior permission.

% To use for other conferences, change references to files and the
% conference appropriate and use other authors, contacts, publishers, and
% organizations.
% Also change the deadline and address for returning papers and the length and
% page charge instructions.
% Put where the files are available in the appropriate places.


% PDF Info Is REQUIRED.

% Please leave this \pdfinfo block untouched both for the submission and
% Camera Ready Copy. Do not include Title and Author information in the pdfinfo section
\pdfinfo{
/TemplateVersion (IJCAI.2025.0)
}

\title{A Survey on Pre-Trained Diffusion Model Distillations}


% Single author syntax
% \author{
%     Author Name
%     \affiliations
%     Affiliation
%     \emails
%     email@example.com
% }

% Multiple author syntax (remove the single-author syntax above and the \iffalse ... \fi here)
\author{
Xuhui Fan$^1$\and
Zhangkai Wu$^1$\And
Hongyu Wu$^1$\\
\affiliations
$^1$School of Computing, Macquarie University, Australia\\
\emails
\{xuhui.fan, zhangkai.wu\}@mq.edu.au, 
hongyu.wu@students.mq.edu.au
}

\begin{document}

\maketitle

\begin{abstract}
Diffusion Models~(DMs) have emerged as the dominant approach in Generative Artificial Intelligence (GenAI), owing to their remarkable performance in tasks such as text-to-image synthesis. However, practical DMs, such as stable diffusion, are typically trained on massive datasets and thus usually require large storage. At the same time, many steps may be required, i.e., recursively evaluating the trained neural network, to generate a high-quality image, which results in significant computational costs during sample generation. As a result, distillation methods on pre-trained DM have become widely adopted practices to develop smaller, more efficient models capable of rapid, few-step generation in low-resource environment. When these distillation methods are developed from different perspectives, there is an urgent need for a systematic survey, particularly from a methodological perspective. In this survey, we review distillation methods through three aspects: output loss distillation, trajectory distillation and adversarial distillation. We also discuss current challenges and outline future research directions in the conclusion. 
\end{abstract}

\section{Introduction}
\label{sec:introduction}
Generative Artificial Intelligence~(GenAI) has witnessed remarkable achievements in recent years~\cite{ho2020denoising,song2020denoising}, with DMs emerging as dominant models. These models, such as stable diffusion~\cite{rombach2022high,esser2024scaling}, have demonstrated exceptional performance across a wide range of generative tasks, including text-to-image synthesis~\cite{rombach2022high,esser2024scaling}, text-to-audio generation~\cite{liu2023audioldm,wu2023large,evans2024fast}, and beyond~\cite{watson2023novo,yim2023se,wu2024protein}. 

However, the multi-step sample generation mechanism of DMs make them unappealing in practice, especially in low-resource environments. Unlike single-step generative models such as Generative Adversarial Networks~(GANs)~\cite{goodfellow2020generative}, DMs generate samples through an iterative process that recursively evaluates a large trained neural network. This mechanism, while effective in producing high-quality outputs, incurs substantial computational costs for these high Number of Function Evaluations~(NFE). Furthermore, training practical DMs typically requires massive data, which challenges the training process. As a result, the deployment of DMs in real-world applications, where efficiency and speed are critical, remains a significant challenge.

To address these limitations, pre-trained diffusion distillation methods have gained attention as promising solutions. Distillation techniques aim to create smaller-size, more efficient models which is capable of generating high-quality samples in fewer steps, thereby reducing computational overhead. These methods vary widely in their approaches, ranging from loss function design to trajectory optimization during the noise-to-sample generation process. Despite the growing interest in distillation, the field lacks a comprehensive and systematic survey of these methods, particularly from a methodological perspective. 

In this paper, we aim to fill this gap by providing a thorough review of DM distillation methods. We begin with a brief overview of DMs, highlighting their strengths and limitations. Next, we systematically examine distillation methods through the aspects of:
\begin{enumerate}
    \item \textbf{Output reconstruction}, in which we will review methods through angles of output values, output distributions, one-step denoising image space, and Fisher divergence.
    \item \textbf{Trajectory distillation}, in which consistency distillation, rectified flow distillation and their integration will be discussed.
    \item \textbf{Adversarial distillation}.
\end{enumerate}
% (1) output reconstruction, in which we will review methods through angles of output values, output distributions, one-step denoising image space, and Fisher divergence; (2) trajectory distillation, in which consistency distillation, rectified flow distillation and their integration will be discussed; and (3) adversarial distillation. 
Finally, we discuss the challenges currently facing the field and propose potential future directions for research.

By synthesizing existing knowledge and offering new perspectives, this survey aims to serve as a valuable resource for researchers and practitioners in GenAI. Our goal is to not only summarize the state of the art but also inspire innovative solutions in further improving the distilling performance. 

\noindent\textbf{Difference to existing surveys} While there have been several works~\cite{yang2022diffusion,huang2024diffusion,shuai2024survey} discussing and reviewing DMs, their scope and focuses are different from those of this survey. For example,~\cite{yang2022diffusion} gives a general overview of DMs. Only one paragraph is used to discuss one basic method progressive distillation~\cite{salimans2022progressive}. \cite{huang2024diffusion,shuai2024survey} focus on using DMs for image editing and multi-modal generation. Comparing to these existing surveys, our survey focuses on {pre-trained DM distillation}. 

The existing survey~\cite{luo2023comprehensive} might be the closest to our survey, as it also discusses DM distillation methods. However, this survey is written around two years ago, while important developments, especially consistency models, rectified flow distillation methods and adversarial distillation, are developed within the last two years. More importantly, our survey provides two new perspectives of output distillation, trajectory distillation, and adversarial distillation, to summarize these methods. The blog~\cite{dieleman2024distillation} also discusses diffusion distillation methods. While it focuses on the intuition of each method, a systematic and more solid discussion is needed. In our survey, we use one notation system to discuss each distillation methods' motivations and their objective functions such that they can be conveniently compared and precisely evaluated. 


\noindent\textbf{Structure of the survey} The remainder of this survey is organized as follows. Section~\ref{sec:preliminaries} provides an overview of the foundational concepts underlying DMs. Section~\ref{sec:general-distillation} presents an overview of DM distillation methods, including the motivations for categorizing these methods. Section~\ref{sec:output-distillation} discusses methods driven by output-difference objectives, offering a detailed analysis of their principles and implementations. Section~\ref{sec:trajectory-distillation} reviews distillation methods that focus on the transition trajectory between random noise and clean data. Section~\ref{sec:adversarial-distillation} summarizes the adversarial objective for distillation. Finally, Section~\ref{sec:challenges-future-directions} explores the current challenges and potential future directions in the field of DM distillation methods.

\section{Preliminary}
\label{sec:preliminaries}
\noindent\textbf{Notation Setup.} We consider DMs' diffusion and denoising processes to be within the time interval $[0, 1]$, with the time steps specified as $1=t_n<t_{n-1}<\ldots<t_1=0$. Since the majority of DMs have been applied in image synthesis tasks, we denote $\mbx_0$ as the clean image, $\mbx_t$ as the noisy image at time step $t$, and $\mbx_1$ as random noise from standard Gaussian distributions. 

Further, we use $\mbtheta$ to represent the parameters of the pre-trained model. With abuse of notations, the score function of the pre-trained DM is written as $\mbepsilon_{\mbtheta}(\mbx_t, t)$, while the velocity of the pre-trained rectified flow is denoted as $\mbv_{\mbtheta}(\mbx_t, t)$. We use $\mbphi$ to represent the parameters of the student distillation model. {We let $\mbx_{t_{i+1}}=g_{\mbtheta}(\mbx_{t_i}, {t_i})$ and $\mbx_{t_{i+1}}=f_{\mbphi}(\mbx_{t_i}, {t_i})$ denote pre-trained model and distillation model's projected locations at time step $t_{i+1}$.} 

\noindent\textbf{Diffusion models}
Diffusion Model (DM)~\cite{ddpm2020neurips,song2020score} is a class of generative models that learn to generate data by modeling the process of gradually denoising noise-corrupted observations. DM is trained to learn the \emph{score function} of the data distribution at multiple levels of noises. DM is constructed through a two-phase process: a \emph{forward diffusion process} and a \emph{reverse denoising process}.

In the forward diffusion process, a clean image $\mbx_0$ is transformed into a noisy image $\mbx_t$ at each time step $t$ as:
\begin{align}
    \mbx_t := \alpha_t \mbx_0 + \sigma_t \mbepsilon,
\end{align}
where $\mbepsilon \sim \cN(\mbepsilon; \mbzero, \mbI)$ is random noise from Gaussian distribution, $\alpha_t$ is a diffusion coefficient that controls the scaling of the data, and $\sigma_t^2$ is the variance of the noise added at timestep $t$. The forward diffusion process is designed to gradually corrupt the data, transforming it into a distribution that is approximately Gaussian as $t$ approaches its maximum value. The goal of the DM is to learn a neural network $\mbepsilon_{\mbtheta}(\mbx_t, t)$ that approximates the score function of the noise-corrupted data,
\begin{align}
    \mbs_{\text{real}}(\mbx_t) = \nabla_{\mbx_t} \log p_{\text{real}}(\mbx_t),
\end{align}
where $p_{\text{real}}(\mbx_t)$ is the probability density of the noisy data at timestep $t$. The score function can be expressed as
$\mbs_{\text{real}}(\mbx_t) = -\sigma_t^{-1}(\mbx_t - \alpha_t \mbx_0)$, 
which provides a target for the neural network to match. Training is performed by minimizing a weighted Root Mean Square Error~(RMSE):
\begin{align}
\E_{t, \mbx_t}\left[\omega(t)\|\mbepsilon_{\mbtheta}(\mbx_t, t) - \mbs_{\text{real}}(\mbx_t)\|^2\right],    
\end{align}
where $\omega(t)$ is a weighting function that balances the contribution of different timesteps to the overall loss.

Once the model is trained, data generation is achieved through an iterative denoising process. Starting from $\mbx_1\sim\cN(\mbx_1;\mbzero, \mbI)$, DM uses $\mbepsilon_{\mbtheta}(\mbx_t, t)$ predicts the noise component and then update the noisy image. This process continues until $t$ reaches $0$, at which point the sample is expected to resemble a clean data point from the clean image distribution. The iterative nature of this process, while effective in producing high-quality images, is computationally expensive, as it requires multiple evaluations of the neural network. 
% This has motivated research into techniques such as  By understanding the underlying mechanisms of DMs, researchers can develop more efficient and scalable variants that retain the impressive generative capabilities of the original framework.

\noindent\textbf{Denoising Diffusion Implicit Model~(DDIM)} DDIM~\cite{song2020denoising} provides deterministic sampling procedures accelerating Denoised Diffusion Probabilistic Model's sampling by defining the sampling as:
\begin{align} \label{eq:ddim-sampling}
    \mbx_{t-1} = \sqrt{\bar{\alpha}_{t-1}} \mbx_t - \frac{\sqrt{1 - \bar{\alpha}_t} \, \epsilon_{\theta}(\mbx_t, t)}{\sqrt{\bar{\alpha}_t}} + \sqrt{1 - \bar{\alpha}_{t-1}} \, \epsilon_{\theta}(\mbx_t, t).
    \tag{5}
\end{align}

The DDIM inversion process, which transform the clean image $\mbx_0$ into its random noise correspondence $\mbx_t$, can be derived by rearranging Equation~(\ref{eq:ddim-sampling}) as:
\begin{align}
    \mbx_t = \sqrt{\bar{\alpha}_t} \mbx_{t-1} - \frac{\sqrt{1 - \bar{\alpha}_{t-1}} \, \epsilon_{\theta}(\mbx_t, t)}{\sqrt{\bar{\alpha}_{t-1}}} + \sqrt{1 - \bar{\alpha}_t} \, \epsilon_{\theta}(\mbx_t, t).
    \tag{6}
\end{align}


\begin{table*}[t]
    \centering
    \caption{Summary of DM Distillation Methods}
    \label{tab:method-summary}
    \begin{tabular}{lcc}
        \toprule
        \textbf{Distillation Category} & \textbf{Technique} & \textbf{Examples} \\
        \midrule
        \multirow{6}{*}{Output-based} 
            & \multirow{2}{*}{Output values}            & Progressive distillation~\cite{salimans2022progressive} \\
            & & TRACT~\cite{berthelot2023tract}\\
            & \multirow{2}{*}{Output distribution}      & SDS~\cite{poole2022dreamfusion}, VSD~\cite{wang2024prolificdreamer} \\
            &       &  DMD~\cite{yin2024one,yin2024improved} \\
            & {Denoising image}      & SDI~\cite{lukoianov2024score}, LucidDreamer~\cite{karras2022elucidating} \\
            & Fisher divergence        & SiD~\cite{zhou2024score}, SiDA~\cite{zhou2024adversarial} \\
        \midrule
        \multirow{4}{*}{Trajectory-based} 
            & \multirow{2}{*}{Consistency distillation}        & Consistency distillation~\cite{song2023consistency} \\
            &         & CTM~\cite{kim2023consistency}, sCM~\cite{lu2024simplifying} \\
            & {Rectified flow distillation}           & InstaFlow~\cite{liu2023instaflow}, SlimFlow~\cite{zhu2025slimflow} \\
            & Integrating CM and rectified flow           & ShortCut model~\cite{frans2025one}, TraFlow~\cite{wu2025traflows} \\
        \midrule
        Adversarial
            & Adversarial loss                     & ADD~\cite{sauer2024fast,sauer2025adversarial} \\
        \bottomrule
    \end{tabular}
\end{table*}
\section{Overview of Pre-trained DM Distillation}
\label{sec:general-distillation}
Given a pre-trained DM as the teacher, distillation aims to obtain a student model that is typically more compact in size and capable of generating high-quality samples with fewer iterations. We may categorize existing DM distillation methods based on their motivations: output-based distillation, trajectory-based distillation, and adversarial distillation.

Regarding output-based distillation, existing works try to minimize the outputs differences between the teacher pre-trained model and the student distillation model. These output differences can be measured by the RMSEs between the output values from these two models, or by the Kullback-Leibler divergence~(KL-diverence) between these two models' output distributions, or by the RMSEs in the one-step denoising image space, or even by the gradient of log-likelihood for the output (i.e., Fisher divergence). 

The trajectory distillation might be the focus of current research. Instead of measuring differences within the clean images, trajectory distillation monitors the whole trajectory from random noises $\mbx_1$ to clean images $\mbx_0$. Specifically, it can be categorized into consistency distillation, rectified flow distillation, and their integration. The former uses the self-consistency property to regularize trajectory, while the later enforces the trajectory to be straight for fast sampling.

DM is firstly used as a generator in the adversarial loss, in which an alternative optmization strategy is used by interleaves optimizing the classifier's parameter and the DM's parameter. It is then used as an independent distillation method. Adversarial loss can be used as a plug-and-play tool in other distillation methods to further improve their performance. 

Table~\ref{tab:method-summary} summarizes the general categories of these distillation methods. 

\section{Output-based Distillation}
\label{sec:output-distillation}
Output-based distillation aims to to minimize the discrepancy between the outputs of the teacher pre-trained model and the student distillation model.

\subsection{Output Reconstruction Loss}
\cite{luhman2021knowledge} may be the first to distill pre-trained DMs. It proposes a one-step generator and then use the RMSE of outputs' difference to optimize the generator. However, such simple method would produce low-quality images and require extensive NFE.  
Progressive Distillation~\cite{salimans2022progressive,meng2023distillation} halves the number of sampling steps. This is achieved by optimizing the student model one-step projection being close to the teacher model's two-step projection. As such, the loss can be defined as:
\begin{align}
    \cL_{\text{PD}} = \|f_{\mbphi}(\mbx_{t_i},t_i)-g_{\mbtheta}(\widehat{\mbx}_{t_{i+1}}, t_{i+1})\|^2,
\end{align}
where $\widehat{\mbx}_{t_{i+1}} = g_{\mbtheta}(\mbx_{t_i}, {t_i})$. Based on progressive distillation, TRACT~\cite{berthelot2023tract} first divides the time interval $[0, 1]$ into several segments and then trains a student model to distill the output of a teacher model’s inference output from step $t_i$ to $t_j (t_j<t_i)$ within each segment. By repeating this procedure, both progressive distillation and TRACT can learn a one-step generator. 
% While repeating this distillation process can further decrease the number of sampling steps, it may lead to a decline in sample quality. To mitigate this trade-off, the authors propose novel parameterizations for DMs and innovative weighting schemes for the objective function.

\subsection{Output Distribution Loss}
In output distribution loss, a one-step generator $f(\mbphi)$ is first construct to represent the clean image, i.e., $\mbx_0=f(\mbphi)$. The task here is to use DM's objective function to optimize $\mbphi$. 

\noindent\textbf{Score Distillation Sampling~(SDS)} SDS~\cite{poole2022dreamfusion} is initially introduced to work on the text-to-$3$D image synthesis task. By replacing the clean image $\mbx_0$ with the one-step generator $f(\mbphi)$, the loss function can be written as:
\begin{align}
\cL_{\text{SDS}}=\E_{t, {\mbepsilon}}\left[\omega(t)\|\mbepsilon_{\mbtheta}(\alpha_tf(\mbphi)+\sigma_t\mbepsilon, t) - \mbepsilon\|^2\right],    
\end{align}
By taking the gradient w.r.t. $\mbphi$ and omitting the Jacobian of $\mbepsilon_{\mbtheta}(\mbx_t, t)$, i.e., $\partial \mbepsilon_{\mbtheta}(\mbx_t, t)/\partial \mbx_t$, we can get:
\begin{align} \label{eq:sds-update-form}
\nabla_{\mbphi}{\cL_{\text{SDS}}}:=\E_{t, {\mbepsilon}}\left[\omega(t)\left(\mbepsilon_{\mbtheta}(\alpha_t\mbx_0+\sigma_t\mbepsilon, t) - \mbepsilon\right)\frac{\partial \mbx_0}{\partial \mbphi}\right].    
\end{align}
Alternatively, the gradient $\nabla_{\mbphi}{\cL_{\text{SDS}}}$ may be viewed as a KL-divergence between the proposal distributions and ground-truth distributions:
\begin{align} \label{eq:SDS-KL}
    \nabla_{\mbphi}{\cL_{\text{SDS}}} \propto \E_t\left[\KL{q(\mbx_t|f(\mbphi), t)}{p_{\mbtheta}(\mbx_t|t)}\right].
\end{align}

\noindent\textbf{Variational Score Distillation~(VSD)} Instead of optimizing one single point $\mbphi$, VSD~\cite{wang2024prolificdreamer} targets at optimizing $\mbphi$'s distribution with the following loss function:
\begin{align} \label{eq:vsd-original-update-form}
\nabla_{\mbphi}{\cL_{\text{VSD}}}\propto\E_{t, \mbx_t}\left[\sigma_t\left(\nabla_{\mbx_t}\log p(\mbx_t)-\nabla_{\mbx_t}\log q(\mbx_t|t)\right) \frac{\partial \mbx_0}{\partial \mbphi}\right],    
\end{align}
where $-\sigma_t\nabla_{\mbx_t}\log q(\mbx_t|t)$ denotes the scores of noisy rendered image and can be estimated by another noise prediction network $\mbepsilon_{\mbpsi}(\mbx_t, t)$, i.e., 
\begin{align} \label{eq:vsd-update-form}
\nabla_{\mbphi}{\cL_{\text{VSD}}}\approx\E_{t, \mbx_t}\left[\omega(t)(\mbepsilon_{\mbtheta}(\alpha_t\mbx_0+\sigma_t\mbepsilon, t) - \mbepsilon_{\mbpsi}(\mbx_t, t))\frac{\partial \mbx_0}{\partial \mbphi}\right].    
\end{align}
Training VSD involves an alternative optimization strategy which interleaves optimizing the generator's parameter $\mbphi$ and the noisy score's parameter $\mbpsi$. It is noted that SDS is a special case of VSD when we specify $\mbphi$'s distribution as a Dirac distribution $\delta(\mbphi-\mbphi^*)$. Comparing to SDS, VSD introduces an extra term $\mbepsilon_{\mbpsi}(\mbx_t, t)$, $\mbepsilon_{\mbpsi}(\mbx_t, t)$ is able to provide more modelling capability, such as text embedding input, than one single Gaussian noise $\mbepsilon$.

\noindent\textbf{Diff-Instruct~(DI)} DI~\cite{luo2024diff} is concurrently developed with VSD, with an integrated KL-divergence as the objective function:
\begin{align}
    \cL_{\text{DI}}=\int_{t=0}^1\omega(t)\E_{\mbx_t\sim q^t(\mbx_t)}\left[\log \frac{q^{t}(\mbx_t)}{p^{t}(\mbx_t)}\right]dt
\end{align}
where where $q^t(\mbx_t)$ and $p^t(\mbx_t)$ denote the marginal densities of the diffusion process at time step $t$, initialized with standard Gaussian distributions and data distributions respectively. Training DI works similar to VSD, which also interleaves optimizing the generator's parameter $\mbphi$ and an additional noisy score's parameter $\mbpsi$.

\noindent\textbf{Distribution Matching Distillation~(DMD)} Equation~(\ref{eq:SDS-KL}) coincides with DMD~\cite{yin2024one,yin2024improved}, which also considers the KL-divergence between the synthetic data generator $p_{\text{fake}}(\mbx_t)$ and real data distribution $p_{\text{real}}(\mbx_t)$:
\begin{align}
    \KL{p_{\text{fake}}(\cdot)}{p_{\text{real}}(\cdot)}=\E_{\mbz, \mbx, t, \mbx_t}\left[\log \frac{p_{\text{fake}}(\mbx_t)}{p_{\text{real}}(\mbx_t)}\right],
\end{align}
where $\mbz\sim\cN(\mbzero, \mbI), \mbx_0 = g_{\mbphi}(\mbz), t\sim\text{Unif}(0, 1), \mbx_t=\alpha_t\mbx_0+\sigma_t\mbepsilon$. Its gradient can be conveniently calcualted as:
\begin{align}
    \nabla_{\mbphi}\KL{p_{\text{fake}}(\cdot)}{p_{\text{real}}(\cdot)}=\E_{\mbx_t}\left[(\mbs_{\text{fake}}(\mbx_t)-\mbs_{\text{real}}(\mbx_t))\frac{\partial \mbx_0}{\partial \mbphi}\right]. 
\end{align}
Building on this work, \cite{nguyen2024swiftbrush} further demonstrates its effectiveness in distilling pre-trained stable DMs and extends its application beyond 3D synthesis. Meanwhile, \cite{xie2024distillation} introduces a maximum likelihood-based approach for distilling pre-trained DMs, where the generator parameters are updated using samples drawn from the joint distribution of the diffusion teacher prior and the inferred generator latents.

\subsection{One-step Denoising Image Space}
\noindent\textbf{Score Distillation Inversion~(SDI)} Due to the high-variance random noise term $\mbepsilon$ and also the large reconstruction error, using SDS may result in the so-called over-smoothing phenomenon~\cite{liang2024luciddreamer}. SDI~\cite{lukoianov2024score,liang2024luciddreamer} proposes to work in the one-step denoising image space:
\begin{align} \label{eq:one-step-denoising-image}
    \mbx_0(t)= \mbx_t - \sigma_t\epsilon_{\mbtheta}(\mbx_t, t).
\end{align}
where $\mbx_0(t)$ is the noisy data at time step $t$ denoised with one single step of noise prediction.

By applying Equation~(\ref{eq:one-step-denoising-image}) into the guidance term $(\mbepsilon_{\mbtheta}(\alpha_t\mbx+\sigma_t\mbepsilon, t) - \mbepsilon)$ of Equation~(\ref{eq:sds-update-form}), we can get the updated guidance term as:
\begin{align}
    \mbx_0(t-\tau)=\mbx_0(t)-\sigma_t\left[\mbepsilon_{\mbtheta}(\mbx_{t-\tau}, t-\tau)-\mbepsilon_{\mbtheta}(\mbx_{t}, t)\right],
\end{align}
After a series of transformation, the updating rule can be written as:
\begin{align} \label{eq:sdi-update-form}
\nabla_{\mbphi}{\cL_{\text{SDI}}}:=\E_{t, \mbx_t}\left[\omega(t)\left(\mbepsilon_{\mbtheta}(\alpha_t\mbx_0+\sigma_t\mbepsilon, t) - \mbkappa_t\right)\frac{\partial \mbx_0}{\partial \mbphi}\right],    
\end{align}
where $\mbkappa_t:=\text{DDIM-Inversion}(\mbx_t,t)$ can be obtained through DDIM inversion.

\noindent\textbf{Diffusion Distillation based on Bootstrapping~(Boot)} Boot~\cite{gu2023boot} also works on the one-step denoising image space. While initially inspired from the consistency model~\cite{song2023consistency} which seeks a deterministic mapping from any noisy image $\mbx_t$ to clean image $\mbx_0$, Boot does the opposite by mapping any noisy image $\mbx_t$ from the a random noise $\mbx_1$. Training Boot on $\mbx_0(t)$ helps to focus on the low-frequency ``signal'' of the noisy image $\mbx_t$ and stablise the training process. Of course, Boot may also be regarded as an extension from consistency model~(See panel (c) in Figure~\ref{fig:visualisation-on-trajectory-distillation-methods}). 

\subsection{Fisher Divergence Loss}
\noindent\textbf{Score identity Distillation~(SiD)} SiD~\cite{zhou2024score} uses a model-based score-matching loss as the objection function, which moves away from the traditional usage of KL-divergence between real and fake distributions over noisy images. This model-based loss (Fisher divergence) can be written in an $L2$-loss as:
\begin{align}
    &\mathcal{L}_\theta(\phi^*, \psi^*(\theta), t) \nonumber \\
    = & \mathbb{E}_{\mbx_t \sim \rho_{p_\theta}(\mbx_t)} \left[ \| \nabla_{\mbx_t} \ln p_{\text{data}}(\mbx_t) - \nabla_{\mbx_t} \ln p_\theta(\mbx_t) \|_2^2 \right] \nonumber\\
= &\mathbb{E}_{\mbx_t \sim \rho_{p_\theta}(\mbx_t)} \left[ \frac{\alpha_t^2}{\sigma_t^2} \| f_{\phi^*}(\mbx_t, t) - f_{\psi^*(\theta)}(\mbx_t, t) \|_2^2 \right],
\end{align}
where $f_{\phi^*}(\mbx_t, t)$ and $f_{\phi^*}(\mbx_t, t)$
represent the optimal denoising score networks for the true and fake data
distributions, respectively. The SiD approache is claimed to be data-free, and has shown impressive empirical performance, as it has demonstrated the potential to
match or even surpass the performance of teacher models in a single generation step. 

Subsequent work based on SiD including \cite{zhou2024long} which develops a long and short classifier-free guidance strategy to distill pre-trained models using only synthetic images generated by the one-step generator; \cite{zhou2024adversarial} which further incorporates adversarial training strategy to improve the model performance; \cite{luo2024one} extends the $L2$-loss into more general format, by developing a score-gradient theorem to calculate the gradient of those more general divergences. 

\begin{figure*}
    \centering
    % \includesvg[width=1\linewidth]{Compa1.svg}
    \includegraphics[width=1\linewidth]{Compa.png}
    \caption{Visualisations on trajectory distillation methods. Grey arrows represent the trajectories of pre-trained models, whereas yellow arrows correspond to the trajectories of the distillation models. Panels (a-c) illustrate consistency distillation methods, while panels (d-h) depict rectified flow distillation methods. The trajectories in (a-c) exhibit greater curvature compared to those in (d-h), reflecting the fact that the pre-trained models in (a-c) are DMs, while those in (d-h) are rectified flows. In panels (b), (g), and (h), a single large step corresponds to the position of two smaller steps due to the self-consistency property of the trajectory. }
    \label{fig:visualisation-on-trajectory-distillation-methods}
\end{figure*}
\section{Trajectory-based Distillation}
\label{sec:trajectory-distillation}
Trajectory distillation considers the trajectory of a mapping between the noise distribution and the clean image distribution. In this section, we discuss two trajectory distillation types with different trajectory requirements: consistency distillation for the self-consistency property, and rectified flow distillation for straight trajectory. Figure~\ref{fig:visualisation-on-trajectory-distillation-methods} gives a simple visualization of these consistency distillation and rectified flow distillation models. 

\subsection{Consistency Distillation}
\noindent\textbf{Consistency model~(CM)} CM~\cite{song2023consistency} is built based on the probability flow-ODE~\cite{song2020score}. CM learns a consistency function $f_{\mbphi}(\mbx_t, t)$ mapping an noisy image $\mbx_t$ back to the clean image $\mbx_0$ as:
\begin{align}
    f_{\mbphi}(\mbx_t, t) = \mbx_0.
\end{align}
While the consistency model is required to satisfy the boundary condition at $t=0$, $f_{\mbphi}(\mbx_t, t)$ is usually parameterized as:
\begin{align}
    f_{\mbphi}(\mbx_t, t) = c_{\text{skip}}(t)\mbx_t+c_{\text{out}}(t)F_{\mbphi}(\mbx_t, t),
\end{align}
where $F_{\mbphi}$ is the actual neural network to train, and $c_{\text{skip}}(t)$ and $c_{\text{out}}(t)$ are time-dependent factors such that $c_{\text{skip}}(0) = 1, c_{\text{out}}(0) = 0$. 

While CM can be trained from scratch, a more popular choice for CM is to do distillation on pre-trained sampling trajectory, such as pre-trained DMs. In particular, the distillation objective function can be written as a distance metric between adjacent points  as:
\begin{align}
    \cL_{\text{CM}} = \E_{i}\left[\omega(t_i)d(\mbf_{\mbphi}(\mbx_{t_{i}+\Delta t}, t_{i}+\Delta t), \mbf_{\mbphi^-}(\widehat{\mbx}^{\mbtheta}_{t_{i}}, t_{i}))\right],
\end{align}
where $d(\cdot, \cdot)$ is a metric function, $\mbphi^-$ is the exponential moving average (EMA) of the past values $\mbphi$, and $\widehat{\mbx}^{\mbtheta}_{t_{i}}$ is obtained from pre-trained model as
$\widehat{\mbx}^{\mbtheta}_{t_{i}}=\mbx_{t_i}-(t_i-t_{i+1})t_{i+1}\nabla_{\mbx_{t_{i+1}}}\log p_{t_{i+1}}(\mbx_{t_{i+1}})$

\noindent\textbf{Consistency Trajectory Model~(CTM)} CTM~\cite{kim2023consistency} is introduced to minimize the accumulated estimation errors and discretization inaccuracies in multi-step consistency model sampling. While CM projects a noisy image $\mbx_t$ to its clean image $\mbx_0$, CTM extends it by designing a projection function which starts from time step $t$ to its later step $s (s<t)$ as:
\begin{align} \label{eq:ctm-parameterization}
    f_{\mbphi}(\mbx_t, t, s) = s/t\cdot\mbx_t+(1-s)/t\cdot F_{\mbphi}(\mbx_t, t, s).
\end{align}
where $F_{\mbphi}()$ is the actual neural networks to be optimized. Equation~(\ref{eq:ctm-loss-function}) ensures $f_{\mbphi}(\mbx_t, t, s)$ satisfies the boundary condition when $f_{\mbphi}(\mbx_t, t, 1) = \mbx_1$. 

Regarding loss function, CTM minimizes a \textit{soft consistency matching loss}, which is defined as:
\begin{align} \label{eq:ctm-loss-function}
    \cL_{\text{CTM}} = \|f_{\mbphi}(\mbx_t, t, s)-f_{\text{sg}(\mbphi)}(f_{\mbtheta}(\mbx_t, t, u), u, s)\|^2, 
\end{align}
where $0<s<u<t<1$, and $\text{sg}(\mbphi)$ is the EMA stop gradient operator. Equation~(\ref{eq:ctm-loss-function}) means that the one projected from the teacher model and the one from the student model should projected to the same value at the same time step. 

\noindent\textbf{Continuous-time Consistency Model} Although most consistency models (CM) use discretized time steps for training, this approach often suffers from issues of tuning additional hyperparameters and inherently introduces discretization errors. \cite{song2023improved,geng2024consistency,lu2024simplifying} has been working on continuous-time CM and strategies to stablize its training. For example, \cite{geng2024consistency} analyses CM from a differential perspective and then progressively tightens the consistency condition, by letting the time different $\Delta t\to dt$ as training progresses. \cite{lu2024simplifying} proposes a TrigFlow framework, which tries to unify the flow matching~\cite{lipman2022flow} and DMs, by defining the noisy image as $\mbx_t=\cos(t)\mbx_0+\sin(t)\mbepsilon$ for $t\in[0, \pi/2]$ and setting the training obejctive as:
\begin{align}
    \cL_{\text{TrigFlow}}=\E_{\mbx_0, \mbz, t}\left[\|\sigma_t F_{\mbphi}(\mbx_t/\sigma_t, t)-\mbx_t\|^2\right].
\end{align}
where $F_{\mbphi}(\cdot)$ is the actual neural networks to be optimized. 

\subsection{Rectified Flow}
\noindent\textbf{Rectified Flow} Rectified Flow~\cite{lipman2022flow,liu2023flow,albergo2023stochastic}, also named as flow matching or stochastic interpolation, leverage Ordinary Differential Equations~(ODEs) to model the transition between two distributions, i.e., $p_0(\cdot), p_1(\cdot)$. Given a clean image $\mbx_0 \sim p_0(\mbx_0)$ and a random noise $\mbx_1 \sim p_1(\mbx_1)$, rectified flows construct a linear interpolation path defined as:
\begin{align}
    \mbx_t = t \mbx_1 + (1 - t) \mbx_0, \quad 0 \leq t \leq 1.
\end{align}
The training objective for the model is formulated as:
\begin{align} \label{eq:rectified-flow-first-training}
    \cL_{\text{rf}}(\mbtheta)= \int \mathbb{E}_{\mbx_1,\mbx_0}\left[\|(\mbx_1 - \mbx_0) - \mbv_{\mbtheta}(\mbx_r, r)\|^2\right] dr,
\end{align}
where $\mbv_{\mbtheta}(\mbx_t, t)$ represents the learned and pre-trained velocity field. 

Given a random noise $\mbx_1$, rectified flow's new image generation process follows the ODE $d\mbx_t = \mbv_{\mbtheta}(\mbx_t, t) \, dt, \quad t \in [0, 1]$, where we can write the recovered clean image $\widehat{\mbx}_0$ as $\widehat{\mbx}_0=\mbx_1 + \int_{1}^0\mbv_{\mbtheta}(\mbx_r, r)dr$.  
Through minimizing $\cL_{\text{rf}}$, the trained velocity field $\mbv_{\mbtheta}(\mbx_t, t)$ is expected to approximate the direct path from $\mbx_1$ to $\mbx_0$. That is, rectified flows may be able to achieve high-quality data generation with fewer steps compared to traditional DMs, where the trajectory is usually curved and need more time steps to generate images. 

\noindent\textbf{Reflow vs InstaFlow} In fact, while $\mbx_1$ and $\mbx_0$ are independently sampled from $p_0(\mbx_0)$ and $p_1(\mbx_1)$, simply training Equation~(\ref{eq:rectified-flow-first-training}) may not be able to obtain a straight trajectory. This is because the random pair $(\mbx_1, \mbx_0)$ may provide highly noisy or even contradictory information regarding the direction between $\mbx_1$ and $\mbx_0$. Thus, \cite{liu2023flow} proposes a reflow mechanism, which first construct a data pair $(\mbx_1, \widehat{\mbx}_0)$ from the pre-trained model $\mbv_{\mbtheta}(\mbx_t, t)$,  where $\widehat{\mbx}_0$ is defined above as $\widehat{\mbx}_0=\mbx_1 + \int_{1}^0\mbv_{\mbtheta}(\mbx_r, r)dr$, and then trains a new velocity $\mbv_{\mbphi}(\mbx_t, t)$ as:
\begin{align} \label{eq:rectified-flow-reflow}
    \cL_{\text{reflow}} =  & \int \mathbb{E}_{\mbx_1}\left[\|(\mbx_1 - \widehat{\mbx}_0) - \mbv^{1}_{\mbphi}(\mbx_t, t)\|^2\right] dt\nonumber \\
    = & \int \mathbb{E}_{\mbx_1}\left[\|\int_{1}^0\mbv_{\mbtheta}(\mbx_r, r)dr - \mbv^{1}_{\mbphi}(\mbx_t, t)\|^2\right] dt,
\end{align}
where $\mbx_t = (1-t)\widehat{\mbx}_0+t\mbx_1$. 

On the other hand, InstaFlow~\cite{liu2023instaflow,zhu2025slimflow} aims to construct a one-step generator as $\mbx_1 + \mbv_{\mbphi}(\mbx_1, 1)$, where $\mbv_{\mbphi}(\mbx_1, 1)$ can be trained as
\begin{align} \label{eq:rectified-flow-distillation}
    \cL_{\text{rf-dis}} =  &  \mathbb{E}_{\mbx_1}\left[\|(\mbx_1 - \widehat{\mbx}_0) - \mbv^{1}_{\mbphi}(\mbx_1, 1)\|^2\right] \nonumber \\
    = &  \mathbb{E}_{\mbx_1}\left[\|\int_{1}^0\mbv_{\mbtheta}(\mbx_r, r)dr - \mbv^{2}_{\mbphi}(\mbx_1, 1)\|^2\right].
\end{align}
In fact, we can see that $\mbv_{\mbphi}(\mbx_1, 1)$ is learned to approximate the amount of velocity $\mbv_{\mbtheta}(\mbx_r, r)$ changes from $t=1$ to $t=0$. Comparing these two distillation methods, reflow tries to obtain a new trajectory, where each time step's velocity is close to constant, whereas instaflow tries to approximate the amount of changes given the random noise $\mbx_1$.  

\subsection{Integrating Consistency Models and Rectified Flows}
Consistency models and rectified flows each have distinct advantages and limitations. The former excels in regulating trajectory behavior, while the latter prioritizes achieving a straight trajectory.  

\noindent\textbf{Shortcut model} Shortcut model~\cite{frans2025one} is recently proposed as a promising improvement over the rectified flow. Shortcut model made two critical modifications to the existing rectified flow: (1) in addition to considering the current noisy image $\mbx_t$ and its corresponding time step $t$, SM also takes the future projected time step $t+d$ as an input for the neural network $\mbv(\mbx_t, t, d)$. As a result, this future projected time step can help guide the generation of future noisy images; (2) Shortcut model also regulates the trajectory to be self-consistent. In particular, its loss function is:
\begin{multline} \label{eq:shortcut-model-objective}
    \cL_{\text{SM}} = \E_{\mbx_1, t, d}\left[\|(\mbx_1 - \widehat{\mbx}_0) - \mbv_{\mbphi}(\mbx_t, t, 0)\|^2\right.\\
    \left.+\|\mbv_{\mbphi}(\mbx_t, t, 2d)-\frac{\mbv_{\mbphi}(\mbx_t, t, d)+\mbv_{\mbphi}(\mbx'_{t+d}, t, d)}{2}\|^2\right],
\end{multline}
where $\mbx'_{t+d} = \mbx_t + \mbv_{\mbphi}(\mbx_t, t, d)$. The first term in Equation~(\ref{eq:shortcut-model-objective}) enforces constant velocities, while the second term ensures the self-consistency of the generated trajectory. 

\noindent\textbf{Trajectory Distillation Flow~(TraFlow)} TraFlow~\cite{wu2025traflows} also works to distill pre-trained rectified flows. Instead of approximating velocities, TraFlow uses neural networks to approximate the integration of velocities, which is $f_{\mbphi}(\mbx_t, t, s)=\mbx_t + \int_t^s \mbv_{\mbtheta}(\mbx_r, r)dr$. The loss function of TraFlow considers the factors of soft consistency matching and trajectory straightness:
\begin{multline}    
    \cL_{\text{TraFlow}} = \int \mathbb{E}_{\mbx_1}\left[\|(\mbx_1 - \widehat{\mbx}_0) - \frac{\partial f_{\mbphi}(\mbx_t, t,t+\Delta t)}{\partial \Delta t}\|^2\right] \\
    + \|f_{\mbphi}(\mbx_t, t, s)-f_{\text{sg}(\mbphi)}(f_{\mbtheta}(\mbx_t, t, u), u, s)\|^2 ds.
\end{multline}
Since TraFlow directly projects the noisy image at a future time step $s$, it removes the need for an ODE solver to approximate the trajectory, thereby avoiding approximation errors.

In fact, both Shortcut model and TraFlow share the same motivation and even model structure with CTM~\cite{kim2023consistency}, which take the noisy image $\mbx_t$, the current time step $t$, and the future time step $s$ or the step lag $s-t$ as inputs. Integrating the consistency model with trajectory straightness is a natural choice, as a straight trajectory is inherently self-consistent.

\noindent\textbf{Consistency-FM} Instead of requiring the trajectory to be self-consistent, Consistency-FM~\cite{yang2024consistency} works on the velocity side by setting consistent velocities. In particular, consistency-FM derives the equivalence between two conditions at any two time steps $\forall t,s\in[0, 1]$: 
\begin{align}
    \text{condition 1: } &\mbv(\mbx_t, t) =\mbv(\mbx_s, s)\nonumber; \\
    \text{condition 2: } &\mbx_t+(1-t)\mbv(\mbx_t, t)= \mbx_s+(1-s)\mbv(\mbx_s, s),
\end{align}
in which condition 1 means the velocities at any time steps are the same and condition 2 refers to that the linearly projected points from different time steps would be the same. Consistency-FM set the training objective as to let the velocity satisfies these two conditions. Consistency-FM also develops multi-segment mechanism to further improve the model performance. 


\section{Adversarial Loss}
\label{sec:adversarial-distillation}
Diffusion-GAN~\cite{wang2022diffusion,xu2024ufogen} may be the first introducing the adversarial loss~\cite{goodfellow2020generative} in training DMs. Diffusion-GAN optimizes a min-max objective function to obtain an optmized DM:
\begin{multline}
    \cL_{\text{Diffusion-GAN}} = \E_{t, \mbx_t\sim q(\mbx|\mbx_0, t)}\left[\log \mathcal{D}_{\mbpsi}(\mbx_t)\right] \\
    + \E_{t, \mbz\sim p(\mbz), \mbx'_t\sim q(\mbx'_t|f_{\mbphi}(\mbz), t)}\left[\log(1-\mathcal{D}_{\mbpsi}(\mbx'_t))\right],
\end{multline}
where $\mathcal{D}_{\mbpsi}(\cdot)$ is the classifier for the adversarial loss. Consequently, the training process involves alternating optimization of the generator's parameters, $\mbphi$, and the classifier's parameters, $\mbpsi$. 

In addition to direct training, adversarial loss itself can be independently used as adversarial distribution distillation~(ADD)~\cite{sauer2025adversarial}, in which the loss function is in the GAN format:
\begin{multline}
    \cL_{\text{ADD}} = \E_{t, \mbz\sim p(\mbz), \mbx'_t\sim q(\mbx'_t|f_{\mbphi}(\mbz), t)}\left[\log \mathcal{D}_{\mbpsi}(\mbx_t)\right. \\
    \left. + \log(1-\mathcal{D}_{\mbpsi}(\mbx'_t))\right].
\end{multline}
ADD is successfully used for SDXL Turbo~\cite{sauer2024fast}, a text-to-image model that enables realtime generation. 

Since this adversarial loss is orthogonal to existing methods, common distillation methods can include it as an additional technique to improve the distillation performance~\cite{zhou2024adversarial}. When real data is available, the performance of the student model might surpass that of the teacher model.  



\section{Challenges and Future Direction}
\label{sec:challenges-future-directions}
Although pre-trained DM distillation methods have made significant progress, this field still faces substantial challenges, along with promising future directions for addressing them. 

\subsection{Challenges}
\noindent\textbf{1, Training on large pre-trained models} Most experiments evaluating the effectiveness of distillation methods are conducted on small datasets, such as CIFAR-10 and ImageNet. However, distilling large-scale pre-trained DMs, such as Stable Diffusion~\cite{rombach2022high,esser2024scaling}, remains computationally demanding and is infeasible for research groups with limited GPU access. Therefore, developing computationally efficient approaches for distilling such large-scale models is a critical and promising research direction.

\noindent\textbf{2, Trade-off between quality and speed} Distillation methods accelerate sampling by reducing the number of required function evaluations or sampling steps. As a result, their aggressive reduction might lead to degraded sample quality due to the accummulated errors. Obtaining an optimal balance is necessary for carefully designed approximations, improved training objectives, and architectural modifications to mitigate artifacts introduced by accelerated sampling. Understanding this trade-off is crucial for advancing efficient diffusion-based generative models that retain high fidelity while being computationally feasible for real-world applications.

\noindent\textbf{3, Training guidelines} The lack of uniform training guidelines for DM distillation creates inconsistencies in methodology and evaluation. While it is common to adopt the settings of EDM~\cite{karras2022elucidating,karras2024analyzing} to train DMs, key distillation aspects, such as weight functions, network structures, and distillation stages, vary across studies without a standardized framework. Different weight functions alter training dynamics, while architectural choices and stage configurations affect efficiency and fidelity. Without clear guidelines, comparing methods fairly and reproducing results remains challenging.


\subsection{Future directions}


\noindent\textbf{1, Training smaller-size student model} 
Most existing approaches primarily focus on reducing the number of time steps to achieve high-quality image generation. However, they often overlook effective strategies for reducing model size, specifically the neural network’s capacity. Employing a smaller student model is particularly beneficial for practical applications, such as fast sampling and adaptability to low-resource environments. SlimFlow~\cite{zhu2025slimflow} explores this direction by leveraging annealing to reduce model size, but further investigation is required.

\noindent\textbf{2, Trajectory optimization} Optimizing the trajectory between random noise and clean images remains a crucial area of research, particularly with the advancement of consistency models and rectified flows. Current approaches primarily focus on enforcing self-consistency and trajectory straightness. However, further exploration of alternative trajectory properties could provide valuable insights for enhancing both the efficiency and quality of sampling.

\noindent\textbf{3, Student model initialization} The parameters of the student distillation model are typically initialized using those of the teacher model. However, the two models are designed with different objectives and sampling mechanisms. For instance, a student model with only a few sampling steps can be viewed as an integration of a teacher model with $1\,000$ sampling steps. While effective initialization can accelerate student model training, the development of a universal initialization strategy remains an open research question.


\noindent\textbf{4, Theoretical understanding of using distillation}
There is a question of whether distillation is necessary to achieve such models or if they could be trained directly. While the existence of a distilled model demonstrates feasibility, it does not ensure optimal efficiency in achieving it. Some models obtained through diffusion distillation can, in principle, be trained from scratch, though this approach generally underperforms compared to distillation, reflecting the well-established advantage of distilling large neural networks into smaller ones. This analogy holds for DMs, where distillation compresses a deep sampling process into a shallower one. Consistency models might offer an example where both distillation and direct training are possible, but the latter requires careful hyperparameter tuning and training strategies. In additional to these empirical evidence, a theoretical understanding for the advantages of distillation methods would be interesting to explore in the future.

\noindent\textbf{5, Broader applications of distillation methods} This survey primarily focuses on the methodological aspects of distillation techniques and does not address practical applications such as text-to-image synthesis. Given the demonstrated effectiveness of DMs in fields like image, audio, and video generation, it would be exciting to explore further applications in these domains and beyond. Particularly, the unique advantages of distillation methods in adapting to low computational resource environments present significant potential for broader application. 
 
\newpage
\bibliographystyle{named}
\bibliography{ijcai25}

\end{document}

