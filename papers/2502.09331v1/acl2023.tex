% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.

\usepackage[]{authblk}
\usepackage{mdframed}
\usepackage{setspace}
\usepackage{multirow}
\usepackage{amsmath} 
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{cjhebrew}
\usepackage{hhline}
% \documentclass{ctexart}
\usepackage{CJKutf8}

% \usepackage{arydshln}
\usepackage{xspace}
\include{amsmath}

\usepackage{subfig}

\usepackage{graphicx}
% \usepackage[demo]{graphicx}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage{inputenc}
% This is not strictly necessary, and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}
\usepackage{csquotes}

\usepackage{todonotes}

\makeatletter
\renewcommand\AB@affilsepx{, \protect\Affilfont}
\makeatother


\usepackage[]{ACL2023}
\let\oldfootnotetext\footnotetext
\renewcommand{\footnotetext}[1]{%
  \begingroup%
  \renewcommand{\thefootnote}{\ensuremath{*}}%
  \oldfootnotetext{#1}%
  \endgroup%
}

% Standard package includes
\usepackage{times}

\usepackage{booktabs}
\usepackage{siunitx}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{tabu}
\usepackage{multirow}
\usepackage{makecell} 

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
% \usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}
\usepackage{csquotes}

\usepackage{todonotes}

\newcommand{\note}[4][]{\todo[author=#2,color=#3,size=\scriptsize,fancyline,caption={},#1]{#4}} % default note settings, used by macros below.
\newcommand{\reut}[2][]{\note[#1]{Reut}{blue!20}{#2}}
\newcommand{\tzuf}[2][]{\note[#1]{Tzuf}{brown!20}{#2}}



% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.


% Create spacing in tables.
\newcommand{\pz}{\hphantom{0}}
\newcommand{\pzz}{\hphantom{00}}


\author[a]{Itai Mondshine}
\author[a]{Tzuf Paz-Argaman}
\author[a]{Reut Tsarfaty}


\affil[a]{Bar-Ilan University, Israel}
%\affil[b]{Google Research}

\affil[ ]{\authorcr \tt \{mondshi1, tzuf.paz-argaman, reut.tsarfaty\}@biu.ac.il}



\title{Beyond English: The Impact of Prompt Translation Strategies \\across Languages and Tasks in Multilingual LLMs }




% Previous research has shown that by using this approach, LLMs, such as GPT are capable of performing a wide variety of language tasks and even outperform monolingual prompts when the task is presented in English \citep{chowdhery2023palm, qin2023chatgpt, ahuja2023mega,chowdhery2023palm, ahuja2023mega}. Recent studies explore how translation can be used in prompting LLMs, including work on few-shot translated context to English(\cite{ahuja2023mega}) and prompting with context in multiple languages other than English (\cite{zhao2021discrete}).

% Despite these advancements, the common approach treats the prompt as a single unit and translates the whole prompt into English \citep{bareiss2024english}. Another line of work assesses more experimental strategies, for example, \citet{liu2024translation} evaluated setups of both instruction in the source language and instruction in English while keeping the input in the source language. Similarly, \citet{huang2023not} prompts LLMs to translate the question into English and solve the problem step-by-step in English.
% However, those methodologies don't evaluate more complicated setups such as instruction in English and output in the source language. In addition, there is a lack of research in tasks that have no trivial alignment between the labels in the translated text and the original sentence like NER or extractive QA. 




\begin{document}
\maketitle
\begin{abstract}
% Large language models (LLMs) have demonstrated multilingual capabilities across various natural language processing tasks, with English being the most extensively studied language. However, the optimal approach for querying LLMs in underrepresented languages remains unclear. A common strategy involves translating the task prompt into English, which helps the model overcome the lack of knowledge in the source language. Nevertheless, most works treat the prompt as a single unit to translate, or at most, assess small number of setups such as instruction or context in English while keeping the input in the source language. However, these metholodiges don't consider all the prompt modular parts — instruction, input, context (zero-shot/few-shot), and output—which could be translated or not. Therefore, research is needed to determine which part of the prompt should be translated and the effects of different prompt translation configurations. It is also unclear whether English is universally effective across all tasks, especially those requiring region-specific knowledge, such as Named Entity Recognition. In this paper, we investigate the effectiveness of translating various prompt components into English across different languages and tasks with varying levels of language dependence. We explore how factors like translation quality, linguistic similarity to English, and the degree of language representation in the model's pre-trained data impact the optimal setup. We present a comprehensive benchmarking study, examining over twenty cross-lingual prompt settings (English and source languages) to evaluate performance in different generative LLMs on multilingual tasks. Finally, we suggest practical guidelines for choosing the optimal strategy in various multilingual scenarios.

%% too general (too long)
Despite advances in the multilingual capabilities of Large Language Models (LLMs) across diverse 
%Natural Language Processing (NLP)
tasks, English remains the dominant language for LLM research and development. This
has led to the widespread practice of \emph{pre-translation}, i.e., translating non-English task prompts into English before inference. \emph{Selective pre-translation}, a more surgical approach, focuses on translating specific prompt components. However, its current use is sporadic and lacks a systematic research foundation. Consequently, the optimal \emph{selective pre-translation} strategy for various multilingual settings and tasks remains unclear. In this work, we aim to uncover the optimal setup for \emph{selective pre-translation} by systematically assessing its use. Specifically, we view the prompt as a modular entity, composed of four functional parts: instruction, context, examples, and output, either of which could be translated or not.  We evaluate pre-translation strategies across 35 languages covering both low and high-resource languages, on various tasks including Question Answering (QA), Natural Language Inference (NLI), Named Entity Recognition (NER), and Abstractive Summarization. Our experiments show the impact of factors as similarity to English, translation quality, and the size of pre-trained data, on the model performance. We suggest practical guidelines for choosing  optimal strategies in various multilingual settings.\footnote{
We launched a user-friendly HuggingFace Space for generation and use of selective pre-translation prompts 
%and facilitating implementation: 
\url{https://huggingface.co/spaces/naacl-anonymous/selective_pre_translation}. Appendix \ref{appendix:hf_space} provides further details and illustrations.}



% Finally we suggest practical guidelines for choosing the optimal strategy in various multilingual scenarios. ֿ


% \begin{figure}[t]
%   \centering
%   \includegraphics[width=0.5\textwidth]{schema.png} % Replace 'example-image' with your image file name
%   \caption{Our Selective Pre-Translation Approach}
%   \label{fig:schema}
% \end{figure}


% into English across different languages and tasks with varying language dependence







% A common strategy is translating the task prompt into English to help the model overcome knowledge gaps in the source language. Most works treat the prompt as a single unit to translate or assess limited setups like instruction or context in English while keeping the input in the source language; some even use context in multiple languages. 

%%
% - LLMs are important / powerful
% - English-centric biases
% - Pre-translation is used / common
% - Pre-translatio of parts --- unsystematic

%%%
% define the research question / contribution
% - grasp / formalize prompt functional parts
% - assess systematically their pre-translation efficacy / (non) benefist
% - interplay between language, prompt, LLM

%% too general (too solution-specific)
% However, these methods don't consider all prompt parts — instruction, input, context, and output, which could be translated or not. Therefore, research is needed to determine the best parts to translate and the effects of different configurations. Also, it is unclear if English is universally effective across all tasks, especially those requiring region-specific knowledge, like Named Entity Recognition. 


%%% In this paper - what we do 
% - formalize the parst
% - define methodology
% - tasks and metrics
% - what languages/types/families

% In this paper, we investigate translating various prompt components into English across different languages and tasks with varying language dependence. We explore how translation quality, linguistic similarity to English, and language representation in the model's pre-trained data impact the optimal setup. We present a comprehensive benchmarking study, examining over twenty cross-lingual prompt settings to evaluate performance in different generative LLMs on multilingual tasks. Finally, we suggest practical guidelines for choosing the optimal strategy in various multilingual scenarios.

\end{abstract}




\begin{figure*}[th]
    \centering
    \scalebox{0.85}{
    \includegraphics[width=\textwidth]{schema-9.0.png}
    }
   \caption{Prompting Strategies: \emph{Direct Inference}, \emph{Selective Pre-Translation}, and \emph{Pre-Translation}}

    % Bar colors correspond to the log-normalized number of tokens in GPT-3 pre-training data.
    
    \label{fig:schema}
\end{figure*}



%Pre-Translatio, add another level between the english output to the translated output. 
%F1 measure one and both income into the same f1. 
%Remove the down arrows. 
% Add arrow for the green components. 


\section{Introduction}

Large language models (LLMs) demonstrate impressive 
%multilingual
capabilities across various natural language processing tasks, including machine translation \citep{kocmi2023findings}, natural language understanding \citep{saba2024llms} 
 %\reut{what is knowledge utilization? I am not familiar with this task}
 and complex reasoning tasks \citep{huang2022towards}. These exceptional capabilities of LLMs stem, to a large extent, from the vast amounts of data they were trained on \citep{kaplan2020scaling}. Current LLMs are primarily trained on English data but also include data from other languages, i.e., GPT-3 was trained on 119 languages, but only 7\% of the tokens are from non-English languages.\footnote{\url{https://github.com/openai/gpt3/blob/master/dataset_statistics}} With over 7,000 languages spoken worldwide \citep{anderson2010many}, the increasing pace of globalization has amplified the need for LLMs that  understand and respond in diverse languages.%%%\reut{this is a weak start, we do not only need to "prompt the LLM" in a languages, we use the LLM, which means the LLM has to have capabilities in order to repond. Try to sharpen this} \reut{in general - this is an important paragraph but it misses the mark. needs further work}

One common strategy to respond to a task presented in a language different than English
%\reut{a strategy for what?} 
is \emph{pre-translation}, which involves translating the complete prompt into English before querying the model \citep{ahuja2023mega, shi2022language}, allowing to leverage the robust capabilities acquired in  English  across different languages. %%\reut{it is not a quite multilingual task -- it is a task presented in a different language. Multilingual tasks involve several languages}
%At the same time,  previous research has shown that LLMs  as the GPT family can perform a wide range of tasks in different languages, and outperform the same task when using {\em direct inference}, i.e.,  prompts written in the source language. %\reut {monolingual in what terms? english or the source language? - sentence unclear} \citep{bareiss2024english, intrator2024breaking, chowdhery2023palm, qin2023chatgpt, ahuja2023mega}. 
%However,
At the same time, this approach introduces complexities and risks of information loss \citep{nicholas2023lost}. 
Also, it is unclear whether this approach is uniformly effective across languages and tasks, especially tasks  requiring region-specific or culturally-apropriate knowledge.
 
In contrast to pre-translation, recent studies  show that {\em direct inference}, i.e., prompting the model directly in the  (non-English) {\em source} language spoken by the user,
%%\reut{what is direct inference - you have not yet defined it with respect to which language}
outperforms pre-translation for tasks like QA \citep{intrator2024breaking}. 
However, it is unclear whether this approach is optimal, considering that the model was trained on limited data in the source language. It is also unclear how much information is shared across languages during pre-training. Be that as it may, as we show in Sec.~\ref{chap_4_compare_methods}, direct inference results still remain suboptimal.
%advantages of model output in English for cultural knowledge and generative tasks. \reut{whats the shortcoming? why not to use only that?}

% while considering more nuanced post-translation approach during inference. 

% While some works translate the prompt as a single unit \citep{bareiss2024english, intrator2024breaking}, 
In view of these shortcomings,
%\reut{you only discuss shortconings of pre-trans} 
various studies propose to use \emph{selective pre-translation}, a more nuanced method compared to the de-facto standard \emph{pre-translation} approach, which calls for translating only specific parts of the prompt \citep{ahuja2023mega, kim2023boosting, kim2024translating}. For example, \citet{liu2024translation} show that translating only the context to English outperforms direct-inference in summarization and NLI. \citet{ahuja2023mega} translated few-shot examples to English while keeping the context in the source language.
%%\reut{whats left in the source?}
\citet{kim2023boosting} used the selective approach when prompting different cross-lingual compositions of in-context examples. However, the selective approach lacks a systematic evaluation of more complex setups, e.g., instruction in English and output in the source language. Consequently, the efficacy of  \emph{selective pre-translation} and the optimal prompt configurations for various multilingual settings and tasks remain unclear. To fill this gap, in this paper we set out to examine the impact of selective pre-translation, a commonly used method, across diverse tasks, in order to devise effective prompting strategies for multilingual LLMs.




% Additionally, to our knowledge, only a handful of studies \citep{jain2019entity} effectively use translation for tasks that have no straightforward alignment between labels in translated text such as NER.\reut{why is there no alignment for NER? sounds odd} Motivated by this research gap, in our study, we examine the impact of selective pre-translation on diverse tasks with the aim of devising effective prompting strategies for multilingual LLMs.


% Specifically, in this paper we propose to formalize a configuration for a prompt ---  four functional parts: instruction, context, examples (zero/few-shot), and output --- either of which could be {\em selectively} pre-translated or not (as also stated by \citet{winata2021language, ahuja2023mega}). %We created a framework for exhaustively analyzing all

Concretely, we define a formal {\em configuration} for a prompt --- consisting of four functional parts: instruction, context, examples, and output --- either of which could be {\em selectively} pre-translated or not (see also \citet{winata2021language, ahuja2023mega}). We exhaustively assess
%%\reut{creating the framework is less important, its an  implementation details} settings of cross-lingual prompt translation
all configurations of cross-lingual prompt translation into English from different source languages.
Figure~\ref{fig:schema} presents an overview  of our approach, demonstrating the various \emph{selective pre-translation} strategies  compared against \emph{pre-translation} and \emph{direct inference} in the source language. 


% By viewing the prompt as a function, we study how each component is relevant to the overall performance. Also, by evaluating different kinds of tasks, we systematically isolate factors that could impact performance, such as knowledge necessity for the task.
% By viewing the prompt as a function, we demonstrate how each component is relevant to the overall performance. 
% Additionally, by evaluating various tasks, we systematically isolate factors that impact performance, such as the knowledge necessary for the task. 

% Moreover, by examining numerous configurations, we recognize specific setups unique to particular language subsets, including those with varying levels of pre-trained representation (high/medium/low resource languages), linguistic families, and linguistic similarity to English. Consequently, we find that the quality of translation significantly influences model performance, with specific configurations yielding optimal results for different languages and tasks.

%%% However -- whats the problems with it??
% - pre-translation has its problems
% - research shows direct inference is better 
% questions mark on this proposed strategy 

% Despite these advancements, the common approach treats the prompt as a single unit and translates the whole prompt into English \citep{bareiss2024english}. Another line of work assesses more experimental strategies, for example, \citet{liu2024translation} evaluated setups of both instruction in the source language and instruction in English while keeping the input in the source language. Similarly, \citet{huang2023not} prompts LLMs to translate the question into English and solve the problem step-by-step in English.

% We analyzed the modular components of prompts to understand how each part affects model performance. We categorized tasks based on their language dependence, enabling us to explore the impact of translating to English on region-specific tasks like Named Entity Recognition versus less language-dependent tasks like reasoning.

% By examining numerous configurations, we recognized specific setups unique to particular language subsets, including those with varying levels of pre-trained representation (high/medium/low resource languages), linguistic families, and linguistic similarity to English. We found that the quality of translation significantly influences model performance, with specific configurations yielding optimal results for different languages and tasks.



% Through a large-scale evaluation encompassing 35 distinct languages, four tasks, six datasets, and three models, we demonstrate that \emph{selective pre-translating} prompts consistently surpass both \emph{pre-translation} of the entire prompt and \emph{direct inference} approaches, establishing the effectiveness of \emph{selective pre-translation}. After showing the general advantage of using \emph{selective pre-translation} we analyze all the performance scores' of the configurations per language/task and by using Rule Association metric we try to illustrate the optimal configurations in different prompting scenerations (different tasks, low/high resource languages and etc.). Addtionaly, we consider additional factors such as the similariy of the language to English, size and family of pre-trained data impct the performance. Finally, t is reasonable to believe that the quality of translation may change
% these selection strategies’ efficacy. We show 
% the effect of translation quality on the downstream.
% task (for the QA task). 


Through a comprehensive evaluation involving 35 languages, four tasks, six dataset collections, and three models, our results  demonstrate that \emph{selective pre-translation} consistently outperforms both \emph{pre-translation} and \emph{direct inference} in the source language, establishing the efficacy of \emph{selective pre-translation} strategies (Section~\ref{chap_4}). 
%
Additionally, we analyze the considerations in determining which component to translate, and illustrate the optimal strategies across tasks and languages with varying resource levels. 
Moreover, we examine how factors such as language similarity to English, training size, and language script
%%\reut{wdym by "family of data"?}
affect  task performance, and 
  show the effectiveness of selective pre-translation method in mitigating various translation issues, by choosing which prompt components to translate (Section~\ref{chap_5}).



% Additionally, we analyze how various factors, including the type of task, size and family of pre-trained data, language similarity to English, impact the performance of our proposed {\em selective} approach. Furthermore, based on these factors, we provide guidelines for implementing optimal pre-translation strategies. Finally, we perform an additional experiment, analyzing the impact of translation on \emph{pre-translation}. 

More specifically, our findings   demonstrate that in extractive tasks such as QA or NER, where the output overlaps with the provided context and no generation is needed, the model is either agnostic to the context language in the case of high-resource languages or prefers  context in the source language in the case of low-resource languages. Surprisingly, we have discovered that low-resource languages yield better results even when the model's output is required in English, e.g., in NER (Section~\ref{chap_4}). %%\reut{again - why no alignment?} 
 Moreover,  we show that translation quality significantly affects model performance and that the \emph{selective pre-translation} approach essentially mitigates the negative effect of suboptimal translations, which are in turn specifically problematic in lower-resourced languages (Section~\ref{chap_5}).
 %\reut{please verify that this last sentence is correct, and if not remove it}



%By systematically analyzing the impact of selectively translating each component, we can provide a more nuanced understanding of cross-lingual pre-translation, which generalizes 
All in all, our extensive  and  systematic evaluation of pre-translation strategies facilitates generalization across a broader range of languages and tasks, beyond  the specific ones herein, towards more robust LLM-use in multilingual settings.
%the scope of the specific tasks herein.  







% Overall, our work challenges both the \emph{pre-translation}
%  and \emph{no-translation} paradigms, highlighting their limitations, especially for low-resource languages. Our findings demonstrate that a hybrid approach incorporating both source language and translation  achieves superior performance.




% Our contribution is threefold:
% \begin{enumerate}
%     \item Presenting a systematic approach for evaluating the multilingual capabilities of LLMs by employing more than 20 cross-lingual prompt setups for more than 40 languages across various tasks. 
    
%     \item Providing detailed analysis on the effects of different factors on performance, including the choice of prompt translation configuration, translation quality, language's pre-trained data size, and the linguistic similarity to English.
    
%     \item Defining rules for incorporating English into prompts, considering both the source language and the nature of the task.
% \end{enumerate}


% Our findings demonstrate that prompts incorporating both the source language and its English translation generally achieve superior performance compared to prompts that are entirely translated or entirely in the source language.




% This effect is particularly pronounced for low-resource languages.
% Specifically, incorporating input in the source language can lead to better results than using complete English translations in tasks where the answer relies primarily on information within the provided input, such as extractive QA or in region-related tasks like NER.
% in extractive tasks, no need for generation the input should be model is neither agnostic to the input or the input should be in the source language. 



% In such cases, input in the source language benefits all languages, with the greatest gains for low-resource ones. Conversely, 

% in tasks lacking cultural context or the answer is not a provided input such as abstractive summarization or reasoning tasks, high-resource languages perform equally well regardless of the input language, while low-resource languages benefit from English input. Surprisingly, we have discovered that low-resource languages yield better results when the output is in English, even in cases where there is no direct alignment between the original and translated input, such as NER.
 
% Further analysis of our findings reveals a statistically significant positive correlation between the source language's linguistic similarity to English and performance in summarization and NER tasks. This may suggest a potential inherent bias within the model favoring languages with a closer linguistic structure to English. 


% Overall, our work challenges both the \emph{pre-translation}
%  and \emph{no-translation} paradigms, highlighting their limitations, especially for low-resource languages. Our findings demonstrate that a hybrid approach incorporating both source language and translation  achieves superior performance.

\section{The Proposal: Formalizing Prompts  {\em Selective Pre-Translation}  Strategies}


Current practices of {\em prompting}  generative LLMs, such as the GPT models family %-3.5-turbo
\citep{ouyang2022training} and Gemini \citep{team2023gemini}, %%\reut{you need to qualify which LLMs you mean -- bc definitely not all LLMs perform instruction following eg BERT ROBERTA and T5 -- so which models are your target?}
uncover two remarkable capabilities in  performing language processing tasks: (i) \textit{chain of thought} \citep{wei2022chain}, where LLMs solve complex tasks through a series of intermediate reasoning steps, and (ii) \textit{in-context learning} \citep{brown2020language}, allowing the model to adapt to new tasks based on limited examples, without weights updates. These capabilities are built on top of the notion of a {\em prompt}, which serves as a prefix for the LLM's response. 
These capabilities are powered by  the complex  nuanced structure of nowadays prompts, consisting of 
%To harness these capabilities, our approach formalizes the prompt into
four components: {\em instruction, context, examples}, and {\em output}.


% %We define four components that constitute the prompt. 
% We refine \citet{ahuja2023mega}'s definition of prompt components to include these four components. 

%Specifically, we define them as 
Let us first define these four components, as
follows. The \textbf{\em Instruction} (\(I\)) provides a natural language guidance to the model, explaining the task to be performed. The \textbf{\em Context} (\(X\)) represents the task data that the model operates on in performing the task. \textbf{\em Examples} (\(E\)) are optional illustrations of context:output pairs, that can be used for in-context learning.
Overall, we define \(\langle I  \, X\,\rangle\) as a zero-shot prompt and a few-shot   prompt  as  \(\langle I  \, X  \, E\,\rangle\).
 The prompt is processed by a model \(M\) to yield an \textbf{\em Output} (\(O\)), where the instruction can include a request for the model to generate the output in a specific language
or format.  




%\reut{this is NOT a good sign - it denotes intersection. why not using \_ or simply $$ as connectors? You can also use the $\hat{}$ sign to concatenate} 


%\reut{Actually, this formalization is somewhat lacking because E is essentially a sequence of X:O correct pairs. could be better to define zero shot with IXO, first and then define E and the add fewshot IXEO}
Each component, i.e., the instruction, the context,  the examples, or the output, may be pre-translated or not. We denote a pre-translation decision  \( l \in \{ e, s \} \), where  e stands for  English pre-translation and s for the source language. Standardly the prompt is  composed as  \(\langle I  \, X \, E\,\rangle\) and is delivered to the model M, which in turn emits an output \(O\). We define a specific {\em  pre-translation configuration}\footnote{See Appendix \ref{appendix:configuration_format} for specific configuration examples.}   as 
\( c = \langle \text{I}^{l_i}, \text{X}^{l_x}, E^{l_e}, O^{l_o} \rangle \)
% \( c = \langle {l_I}, {l_X}, {l_E}, {l_O} \rangle \)
where the subscript \( l \in \{ e, s \} \) indicates the language of the component. Having defined the pre-translation configurations, we evaluate them in different settings.




%%\reut{I think that there was no point where you specifically said you are overall interested in a task that is completely completed in a language different than English, and English is just a helper, Bc some ppl ask in English about multiling stuff and still want the result in English. Thats quite confusing to the average reader.}




% Each component, including the instruction, context, and examples (I, X, and E), can be pre-translated, denoted as \(Component^L\) where L can be English or the source language. The instruction can include a request for the model to generate an output in a specific language or format. Overall, each configuration can be denoted as a set of four components: \(\langle Instruction^L, Input^L, Examples^L, Output^L \rangle\), where \(L\) can be English or source language. 



\section{Selective Pre-Translation Evaluation}
\label{chap_4}

% In this section, we evaluate various selective pre-translation strategies and assess the configuration selection impact on model performance.






\subsection{Experimental Setup}

\paragraph{Goal}
We set out to compare {\em selective pre-translation} to both {\em pre-translation} and {\em direct inference}, and to assess the impact of the selected configuration on task performance across languages.

\paragraph{Prompt Configuration}
We assess \emph{selective pre-translation} in both zero-shot and few-shot settings.  In the zero-shot settings, with no examples, we considered \(2^3\) configurations. For the few-shot scenario, with four components, each is either translated to English or retains the source language, we get \(2^4\) configurations. %\reut{so how many is this? calculate}.
All in all  we experiments with \(24\) configurations per language and task.\footnote{NLI has 12 configurations, with output always in the instruction language, due to its particular, fixed, output format.}
%spelled-out index-based format.}
%\reut{why? you could translate the labels as well. need a better justification}

%\reut{for both 0 and few shot? show the calc} 

\paragraph{Prompt Creation And Output Normalization} 
Based on the prompt configuration, we used the Google Translate API\footnote{\url{pypi.org/project/easygoogletranslate/}} to translate the components that required translation. After querying the model, we normalized and formatted its output, then translated it to match the language of the gold standard. See Appendix \ref{appendix:modular_prompting_experimental_setup} for further implementation.

% \begin{itemize}
%     \item Few shot
%     \item Zero shot
% \end{itemize}
%  We systematically alter the language (English/Source) of each component: instruction, context, examples, and output. We examine 24 configurations per task, 16 ((English/Source)$^4$) for few-shot and 8 ((English/Source)$^3$) for zero-shot, except for NLI where the output is in English.


\paragraph{Models}


We conducted experiments on several LLMs: (1) Standard generative models—GPT-3.5-turbo \citep{ouyang2022training}, Mistral-8x7B \citep{jiang2023mistral}, and Gemini-1.0-pro \citep{team2023gemini} — with context sizes of 16k, 32k, and 8k, respectively; (2) multilingual - bloomz-7b1-mt \citep{muennighoff2022crosslingual}, with a 2k context.\footnote{ See Appendix \ref{appendix:models_platforms} for details on the models we used.}


%\reut{you need to be more specific how you defined the some model is "english centric" since we know those models do train on multiling data.}
%\reut{Also, I suspect ppl will ask why we did not test llama, do we have an answer for focusing on bloom?} 


% \paragraph{Prompting Framework}

% We developed a modular framework for efficient multilingual prompting, including prompt construction, validation, and model querying, with normalization steps.\footnote{The framework is available on GitHub TODO: Add when finished and on Hugging Face spaces for creating selective pre-translation prompts.} See Appendix \ref{appendix:modular_prompting_experimental_setup} for more experimental details.\reut{this is implementation details - can be removed IMO}




% For constructing the prompts942
% we used the LangChain library15 which enables us943
% to build and validate prompts dynamically for both944
% zero-shot and few-shot templates. For creating the945
% instructions, we initially used ChatGPT to generate946
% them and then fine-tuned them based on quality947
% analysis from our experiments


%\reut{I am missing here the prompt creation -- the details in the appendix are not very understandable - and sound like you fine tune stuff on test. Thats problematic and has to be fixed.}

% To facilitate translation, we used an unofficial free Google Translate API.\footnote{\url{https://pypi.org/project/easygoogletranslate/}} Appendix \ref{appendix:modular_prompting_experimental_setup} provides additional details including the dynamic method we used for building the prompts and the normalization methods we used to refine the prompts before evaluation.  

% We established a framework for efficient model querying, allowing specification of parameters such as language and dataset. This framework enables more efficient generation of prompts across different languages.


% We created a framework for generating selective prompts by choosing the desired configuration and language.

% We used a dynamic method for creating the selective prompts and a normalization method in order to  ..... ??? [TODO].
% To facilitate translation, we used an unofficial free Google Translate API.\footnote{\url{https://pypi.org/project/easygoogletranslate/}} Appendix \ref{appendix:modular_prompting_experimental_setup} provides additional details including the dynamic method we used for building the prompts and the normalization methods we used to refine the prompts before evaluation.  

\begin{table}[tb]
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lllll}
\multicolumn{1}{c}{\textit{\textbf{Affinity}}}       & \textbf{Class}                  & \multicolumn{1}{c}{\textit{\textbf{Range (\% of tokens)}}} & \textit{\textbf{Avg. \#tokens (M)}} & \multicolumn{1}{c}{\textit{\textbf{STD}}} \\ \hline
\multicolumn{1}{l|}{High Resource}           & \multicolumn{1}{l|}{A} & \multicolumn{1}{l|}{\textit{p} $\geq$ 0.1\%}              & 1,240                                & 1,156                                   \\ \hline
\multicolumn{1}{l|}{Medium Resource}         & \multicolumn{1}{l|}{B} & \multicolumn{1}{l|}{0.01\% $<$ \textit{p} $<$ 0.1\%}      & \pz\pz72                             & \pz\pz49                                \\ \hline
\multicolumn{1}{l|}{Low Resource}            & \multicolumn{1}{l|}{C} & \multicolumn{1}{l|}{0\% $<$ \textit{p} $\leq$ 0.01\%}     & \pz\pz\pz5.07                        & \pz\pz\pz5.41                            \\ \hline
\multicolumn{1}{l|}{Unrepresented}           & \multicolumn{1}{l|}{D} & \multicolumn{1}{l|}{\textit{p} = 0\%}                     & \pz\pz\pz0                           & \pz\pz\pz0                               \\ \hline
\end{tabular}%
}
\caption{Language categorization  based on the percentage (\(p\)) of tokens per language in GPT-3's training data. Avg. token count (millions), \(\text{STD}\): standard deviation.}
\label{tab:classes_summary}
\end{table}



\paragraph{Language Selection and Categorization}
We selected \textasciitilde 11 languages per task, ensuring a balanced representation across resource levels (high, medium, low). Due to the lack of precise pre-training distribution for the LLMs we use, we employed the GPT-3 distribution as a proxy, as it is the only distribution publicly shared, to our knowledge.\footnote{\url{https://github.com/openai/gpt-3/blob/master/dataset_statistics}}
%\reut{put link / ref where the rev can find this distribution}
The  GPT-3's  multilingual coverage
%\reut{didnt we just say before that gpt models are english centric?}
enables us to categorize languages into classes based on their data ratios. Following \citet{lai2023chatgpt}, we categorized the tested languages into four classes based on data ratio:
%\reut{could the categorize vary based on the model? say, chinese, can be high in GPT and low in Mixtrall for instance? where is the line? how do you justify it? consider adding an explanation of a footnote} 
High-Resource (A), Medium-Resource (B), Low-Resource (C), and Unrepresented (D).\footnote{See Table \ref{tab:languages_classes} for list of languages, codes and data ratios.} Class D, which includes languages unseen during training. Table \ref{tab:classes_summary} summarizes this classification with basic properties.\footnote{Alternative criteria such as speakers ratio, as proposed by \citet{joshi2020state}, do not  reflect language diversity in LLMs, which is affected by  availability of data rather than speakers.}

\begin{table}[tb]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lll}
\multicolumn{1}{c}{\textit{\textbf{Task}}} & \multicolumn{1}{c}{\textit{\textbf{Dataset}}} & \multicolumn{1}{c}{\textit{\textbf{Languages}}}                                                                                                                               \\ \hline
\multicolumn{1}{l|}{NLI}                   & \multicolumn{1}{l|}{XNLI}                     & \begin{tabular}[c]{@{}l@{}}\textit{\textbf{High}}: Spanish, German, Chinese \\
\textit{\textbf{Medium}}: Greek, Turkuish, Arabic \\ \textit{\textbf{Low}}: Bulgarian,  Hindi,  Thai, Swahili, Urdu\\ \end{tabular} \\ \hline
\multicolumn{1}{l|}{\multirow{2}{*}{QA}}   & \multicolumn{1}{l|}{XQuAD}                    & \begin{tabular}[c]{@{}l@{}}\textbf{\textit{High}}: German, Russian, Romanian \\ \textbf{\textit{Medium}}: Arabic,  Greek,  Vietnamese\end{tabular}                                                                 \\ \cline{2-3} 
\multicolumn{1}{l|}{}                      & \multicolumn{1}{l|}{IndicQA}                  & \begin{tabular}[c]{@{}l@{}}\textbf{\textit{Low}}:  Hindi,  Malayalam, Bengali,  Telugu \\ \textbf{\textit{Unrepresented}}:  Assamese \end{tabular}                                                          \\ \hline
\multicolumn{1}{l|}{\multirow{2}{*}{NER}}  & \multicolumn{1}{l|}{MasakhaNER}               & \textbf{\textit{Unrepresented}}: Bambara, Ese, Hausa, Yoruba                                                                                                                                    \\ \cline{2-3} 
\multicolumn{1}{l|}{}                      & \multicolumn{1}{l|}{WikiANN}                  & \begin{tabular}[c]{@{}l@{}}\textbf{\textit{High}}: French, Chinese, Italian, Portuguese,  Swedish\\ \textbf{\textit{Medium}}: Serbian, Slovak\end{tabular}                                                        \\ \hline
\multicolumn{1}{l|}{Summarization}         & \multicolumn{1}{l|}{XL-Sum}                   & \begin{tabular}[c]{@{}l@{}}\textbf{\textit{High}}:  French, Japanese,  Spanish,  Portuguese\\ \textbf{\textit{Medium}}:  Korean, Turkish, \\ \textbf{\textit{Low}}:  Azerbaijani, Nepali,  Persian, Uzbek\end{tabular}              \\ \hline
\end{tabular}%
}
\caption{Experiment Setup: Tasks, datasets, languages. Languages are separated by their resource-type affinity.}
\label{tab:tasks_datasets}
\end{table}

\begin{table*}[tb]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{cccccccccccccccccccc}
\hline
\multicolumn{5}{c}{\textbf{Question Answering (QA, F1)}}                                                                                                                               & \multicolumn{5}{c}{\textbf{Summarization (ROUGE)}}                                                                                                                                                                                                                     & \multicolumn{5}{c}{\textbf{Named Entity Recognition (NER, F1)}}                                                                                                                                                                                                        & \multicolumn{5}{c}{\textbf{Natural Language Inference (NLI, Acc.)}}                                                                                                                                                                                                    \\
\hline
\textit{Lng} & \textit{Cls.} & \(\uparrow\)\textit{Top} & \textit{Src.} (\%) & \textit{Eng.} (\%) & 
\textit{Lng} & \textit{Cls.} & \(\uparrow\)\textit{Top} & \textit{Src.} (\%) & \textit{Eng.} (\%) &
\textit{Lng} & \textit{Cls.} & \(\uparrow\)\textit{Top} & \textit{Src.} (\%) & \textit{Eng.} (\%) &
\textit{Lng} & \textit{Cls.} & \(\uparrow\)\textit{Top} & \textit{Src.} (\%) & \textit{Eng.} (\%) \\ 
\hline
en           & A            & 0.77                            & N.A                                             & \multicolumn{1}{c|}{N.A}                                            & en                               & A                                & 30.23                                               & N.A                                                                 & \multicolumn{1}{c|}{N.A}                                            & en                               & A                                & 0.65                                                & N.A                                                                 & \multicolumn{1}{c|}{N.A}                                            & en                               & A                                & 0.69                                                & N.A                                                                 & N.A                                                                 \\ \hline
de           & A             & 0.85                            & \pz18\%                         & \multicolumn{1}{c|}{\pz\pz9\%}       & fr                               & A                                 & 35.12                                               & 16\%                                                               & \multicolumn{1}{c|}{10\%}                                          & sr                               & B                                 & 0.77                                                & 52\%                                                               & \multicolumn{1}{c|}{265\%}                                         & sw                               & C                                 & 0.73                                                & 58\%                                                               & 28\%                                                               \\
hi           & C             & 0.82                            & \pz32\%                         & \multicolumn{1}{c|}{182\%}                                         & ja                               & A                                 & 32.47                                               & 17\%                                                               & \multicolumn{1}{c|}{14\%}                                          & it                               & A                                 & 0.75                                                & \pz9\%                                              & \multicolumn{1}{c|}{\pz41\%}                        & bg                               & C                                 & 0.72                                                & 57\%                                                               & \pz8\%                                              \\
ar           & B             & 0.74                            & \pz84\%                         & \multicolumn{1}{c|}{138\%}                                         & fa                               & C                                 & 29.34                                               & 21\%                                                               & \multicolumn{1}{c|}{\pz0\%}                         & sk                               & B                                 & 0.72                                                & 15\%                                                               & \multicolumn{1}{c|}{\pz36\%}                        & el                               & B                                 & 0.71                                                & 24\%                                                               & 30\%                                                               \\
vi           & B             & 0.73                            & \pz\pz0\%        & \multicolumn{1}{c|}{\pz58\%}                        & es                               & A                                 & 28.28                                               & 10\%                                                               & \multicolumn{1}{c|}{\pz3\%}                         & po                               & A                                 & 0.72                                                & 18\%                                                               & \multicolumn{1}{c|}{\pz20\%}                        & es                               & A                                 & 0.69                                                & 20\%                                                               & 18\%                                                               \\
ro           & A             & 0.69                            & \pz\pz0\%        & \multicolumn{1}{c|}{\pz\pz9\%}       & po                               & A                                 & 27.40                                               & \pz8\%                                              & \multicolumn{1}{c|}{\pz0\%}                         & fr                               & A                                 & 0.72                                                & 23\%                                                               & \multicolumn{1}{c|}{\pz24\%}                        & ar                               & B                                 & 0.67                                                & 28\%                                                               & 23\%                                                               \\
ru           & A             & 0.69                            & \pz\pz6\%        & \multicolumn{1}{c|}{305\%}                                         & tr                               & B                                 & 20.87                                               & 18\%                                                               & \multicolumn{1}{c|}{\pz0\%}                         & hau                              & C                                 & 0.70                                                & 62\%                                                               & \multicolumn{1}{c|}{\pz51\%}                        & hi                               & C                                 & 0.64                                                & 59\%                                                               & \pz8\%                                              \\
el           & B             & 0.69                            & \pz\pz0\%        & \multicolumn{1}{c|}{\pz\pz2\%}       & ne                               & C                                 & 19.58                                               & 31\%                                                               & \multicolumn{1}{c|}{28\%}                                          & ee                               & D                                 & 0.68                                                & 46\%                                                               & \multicolumn{1}{c|}{\pz81\%}                        & de                               & A                                 & 0.64                                                & 19\%                                                               & \pz8\%                                              \\
bn           & D             & 0.68                            & \pz44\%                         & \multicolumn{1}{c|}{423\%}                                         & as                               & D                                 & 15.79                                               & 17\%                                                               & \multicolumn{1}{c|}{\pz7\%}                         & sv                               & A                                 & 0.68                                                & 12\%                                                               & \multicolumn{1}{c|}{\pz\pz9\%}       & zh                               & B                                 & 0.63                                                & 16\%                                                               & \pz4\%                                              \\
as           & D             & 0.56                            & 138\%                                          & \multicolumn{1}{c|}{450\%}                                         & uz                               & C                                 & 15.72                                               & 58\%                                                               & \multicolumn{1}{c|}{24\%}                                          & zh                               & B                                 & 0.63                                                & 90\%                                                               & \multicolumn{1}{c|}{121\%}                                         & th                               & B                                 & 0.57                                                & 49\%                                                               & 10\%                                                               \\
te           & C             & 0.53                            & 210.10\%                                       & \multicolumn{1}{c|}{253.30\%}                                      & ko                               & B                                 & 11.84                                               & 36.99\%                                                            & \multicolumn{1}{c|}{11.78\%}                                       & bam                              & D                                 & 0.33                                                & 33.25\%                                                            & \multicolumn{1}{c|}{\pz80.24\%}                     & ur                               & C                                 & 0.57                                                & 29.96\%                                                            & \pz9.08\%                                           \\
ml           & C             & 0.49                            & 104.30\%                                       & \multicolumn{1}{c|}{600.00\%}                                      &                                  &                                   &                                                     &                                                                     & \multicolumn{1}{c|}{}                                               & yor                              & D                                 & 0.32                                                & 66.02\%                                                            & \multicolumn{1}{c|}{\pz49.19\%}                     & \multicolumn{1}{l}{tr}           & \multicolumn{1}{l}{B}             & \multicolumn{1}{l}{0.57}                            & \pz0.00\%                                           & \pz8.14\%                                           \\ \hline
\end{tabular}%
}
\caption{For each Language, we present the top-performing selective configuration score over all other configurations ({\it Top}) along with its relative improvement (\%) over \emph{direct inference} (\textit{Src}) and {\em  pre-translation} (Eng).} 
%, percentage improvement over \emph{few-shot pre-translation} (\textit{Eng.}), and the language class (as detailed in Table \ref{tab:languages_classes}).}}

\label{tab:summary_results}
\end{table*}





















% For each configuration \( X^E_i \) we calculate where the component \( X \) (instruction, context, examples or output) is in English and \( X^S_i \) is the same configuration but the component is in the source language the evaluation score \( E(X^E_i) - E(X^S_i) \). We then calculate the average between all the pairs of corresponding configurations (configurations that differ only in X). The performance for a given component \( X \) is given by the following formula:  \(
% \text{Average Gap for } X = \frac{1}{n} \sum_{i=1}^{n} \left( E(X^E_i) - E(X^S_i) \right)
% \) where \( n \) is the total number of pairs.


 
 % The results are shown in Table \ref{tab:rules}.\footnote{Ref to Appendix \ref{tab:apriori_technical_into} for a technical recap of the algorithm.}




% In this study, we investigate the impact of prompt configuration on the multilingual performance of the GPT-3.5 Turbo model. Specifically, we examine the correlation\footnote{Ref to Appendix \ref{tab:correlation_appendix} for implementation details.} between the model's prediction scores and the chosen language component, which is represented by a binary vector (0 represents English and 1 represents the source language). The results are shown in Table \ref{tab:correlation}.

% While correlation analysis provides a preliminary understanding of how individual components relate to model performance, it cannot capture non-linear relationships or expose unexpected configurations that might be important for model performance. To overcome these limitations, we employed
%  \emph{association rule learning (ARL)} with the Apriori algorithm 
%  \citep{piatetsky1991discovery, hegland2007apriori}, a common method in the realm of data mining. The results are shown in Table \ref{tab:rules}.\footnote{Ref to Appendix \ref{tab:apriori_technical_into} for a technical recap of the algorithm.}









% \begin{table}[t]
% \centering
% \resizebox{\columnwidth}{!}{%
% \begin{tabular}{llllrrrr}
% \toprule
% \textbf{Model} & \textbf{Rouge1} & \textbf{Rouge2} & \textbf{RougeL} & \textbf{Epochs} & \textbf{Loss} \\ 
% \midrule
% \textbf{mLongT5-Base} & 18.62 & 8.68 & 15.92 & 18 & 2.15 \\
% \textbf{mLongT5-Large} & 20.22 & 9.66 & 18.12 & 12 &  1.92 \\
% \bottomrule
% \end{tabular}%
% }
% \caption{mLongT5 performance on validation set and training details.}
% \label{tab:mt5Long_details}
% \end{table}



% \begin{figure}[htbp]
%   \centering
%   \includegraphics[width=0.5\textwidth]{datasets.png} % Replace 'example-image' with your image file name
%   \caption{Categories, Tasks, Datasets, and Languages included in our research.}
% \end{figure}



% \subsection{Task Categories}

% We categorize NLP tasks into three broad categories based on the linguistic knowledge required to complete them, which indicates how much the task relies on language proficiency. This classification is determined by the degree to which the model's performance is influenced by its language capabilities concerning the task at hand and based on the study of \citet{zhang-etal-2023-dont}


% \textbf{Reasoning} includes tasks minimally influenced by language, on which consistent performance is expected across languages. These tasks involve logical and rational thinking to solve problems. This work will evaluate the Natural Language Inference (NLI) task. 

% \textbf{Knowledge Access} We subdivide this into two types - extracting knowledge from a given input and training data \citep{heinzerling2020language}.  While the former may not seem language-dependent, models may be less reliable in retrieving and utilizing knowledge learned in a language other than the one used to formulate the task. Our research will evaluate the Extractive question-answering task, and the Named Entity Recognition task \citep{malmasi2022multiconer}.

% \textbf{Articulation} Highly dependent on language, these tasks require not only an understanding of the language but also the associated culture, as they involve capturing style, cultural details, and manner of expression. Our research will test the Abstractive Summarization task.


\paragraph{Tasks and Datasets}




% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \usepackage{graphicx}
% \usepackage[table,xcdraw]{xcolor}
% Beamer presentation requires \usepackage{colortbl} instead of \usepackage[table,xcdraw]{xcolor}
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \usepackage{graphicx}
% \usepackage[table,xcdraw]{xcolor}
% Beamer presentation requires \usepackage{colortbl} instead of \usepackage[table,xcdraw]{xcolor}
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \usepackage{graphicx}
% \usepackage[table,xcdraw]{xcolor}
% Beamer presentation requires \usepackage{colortbl} instead of \usepackage[table,xcdraw]{xcolor}
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \usepackage{graphicx}
% \usepackage[table,xcdraw]{xcolor}
% Beamer presentation requires \usepackage{colortbl} instead of \usepackage[table,xcdraw]{xcolor}




% Highest-performing prompt configuration for each Task / Language: Metric (F1/ROUGE1/Accuracy), improvement (\%) over \emph{direct inference} (\textit{Src.}),  and over few-shot pre-translation (\textit{Eng.}), class of language (as presented in table \ref{tab:languages_classes}. Generated with GPT-3.5-Turbo. Each metric outperformed all other configurations (24) {\em including} (\emph{pre-translation}) and (\emph{direct-inference}. Scores are sorted by Metric.

% \caption{Performance Comparison Across Different Tasks and Languages: This table presents the highest-performing prompt translation configurations for Question Answering (QA), Summarization, Named Entity Recognition (NER), and Natural Language Inference (NLI). The columns show the language (\textit{Lang}), performance metric (F1 or ROUGE1 for QA and Summarization, respectively, and Accuracy for NLI), and the percentage improvement over direct inference (\textit{Src. (\%)}), and few-shot pre-translation (\textit{Eng. (\%)}). The 'Class' column categorizes the performance of each language. These results were generated using GPT-3.5-Turbo and highlight the improvement of the selective pre-training strategy over other methods, including direct inference and pre-translation.}








% \begin{figure*}[th]
%     \centering
%     \scalebox{1.0}{
%     \includegraphics[width=\textwidth]{number_of_words_log_improvement_over_monolingual.png}
%     }
%     \caption{\emph{Selective Pre-Translation} (best configuration)  improvement (\%) over \emph{Direct Inference}. The values refer to the highest-performing prompt configuration per language.  
%     }
%     % Bar colors correspond to the log-normalized number of tokens in GPT-3 pre-training data.
    
%     \label{fig:Improvement_over_monolingual}
% \end{figure*}








% Bold indicates prevailing method
% for model.


% All datasets used, spanning multiple languages, are listed in Table \ref{tab:tasks_datasets}. For our experiments, we carefully sampled languages to ensure equal representation from each subset of language resource categories. In total, these datasets encompass 35 distinct languages across 10 different language families.



We assess model performance on 4 tasks, NLI, QA, NER, and Summarization, which we detail in turn. (i)
%%\reut{This is unclear - you experiment with all languages in each of these dataset? bc the table above shows a 10 for each. if you selected then how did you sample the languages? did you care for the overlap (that all languages will have all tasks?)}
\textbf{\emph{Natural Language Inference (NLI)}} determines if a hypothesis entails, contradicts, or is neutral to a premise. We use the XNLI dataset \citep{conneau-etal-2018-xnli}, with sentence pairs in 11 languages, and measured  prediction accuracy.
% by comparing model predictions to ground truth labels.
% \paragraph{Natural Language Inference (NLI)} NLI involves determining if a hypothesis is entailed by, contradicts, or is neutral to a premise. We evaluated this using the XNLI dataset \citep{conneau-etal-2018-xnli}, which contains premise-hypothesis pairs in 15 languages. The input is a pair of sentences, and the output is a classification label: entailment, contradiction, or neutral. We measure performance using accuracy, comparing the model's predictions to the dataset's ground truth labels.
% \paragraph{Question Answering (QA)} We focus on extractive question answering, where the model must identify the relevant answer span within a given context.  We evaluate the models on two datasets: XQuAD \citep{Artetxe:etal:2019} and IndicQA for Indic languages \citep{doddapaneni2022indicxtreme}. Model performance was assessed based on the exact match between predicted and ground truth answer spans, using the F1 metric.
(ii) \emph{\textbf{Question Answering (QA):}} We focus on extractive QA, where the model identifies the answer span in a given context. We evaluated performance on XQuAD \citep{Artetxe:etal:2019} and IndicQA \citep{doddapaneni2022indicxtreme} for Indic languages, using the F1 score to assess performance.
(iii) \emph{\textbf{Named Entity Recognition (NER):}}
% In this task, the model is instructed to identify and classify named entities within a given sentence. We employed two datasets for evaluation: WikiANN \citep{pan2017cross} and MasakhaNER \citep{adelani2021masakhaner}. WikiANN is a widely used NER dataset containing Wikipedia sentences annotated with LOC (Location), PER (Person), and ORG (organization) tags which supports 176 languages. MasakhaNER is specifically designed for African languages. Both datasets utilize the BIOSE scheme (B=Begin, I=Inside, O=Outside, S=Singleton, E=End) to mark entity boundaries. However, for our experiments, we rephrased the task as a generative task,  instructing the model to directly output entity spans without requiring strict adherence to the BIOSE tags. We evaluated the model using the F1 metric.
% The task of identifying named entities in sentences.
We sampled languages from %\reut{sampled language from? you didnt evaluate on ALL} 
two datasets: WikiANN \citep{pan2017cross}, which includes Wikipedia sentences annotated with Location, Person, and Organization tags in 176 languages; and MasakhaNER \citep{adelani2021masakhaner}, for African languages. While both datasets use the BIOSE scheme to delineate entity boundaries, we recast the task as generative, prompting the model to generate the list labeled named entities for a given input context. Model performance has been evaluated using  F1  scores.
% \paragraph{Abstractive Text Summarization} Abstractive summarization is the task where the model generates concise and informative summaries from longer texts by generating new text, unlike extractive summarization, which selects existing sentences. We evaluated our model using the XL-Sum dataset \citep{hasan2021xl}, which provides summaries of news articles in 44 languages, making it ideal for evaluating multilingual summarization models. We used the ROUGE metric for evaluation, which measures the overlap between generated summaries and reference summaries.
(iv)~\emph{\textbf{Abstractive Text Summarization}} involves generating  short summaries of long contexts, rather than extracting existing sentences.
% from longer texts, as opposed to extractive summarization, which selects existing sentences.
We used the XL-Sum dataset \citep{hasan2021xl}, which offers news article summaries in diverse languages. We sampled 10 languages from the dataset and
% , suitable for multilingual evaluation. 
evaluated with ROUGE.
We conducted experiments on a sample of 250 examples\footnote{We selected 250 that followed the can fit into the context of the model, i.e., \(<16K\).} 
%It was not chosen randomly because the large-context articles in XL-Sum are computationally expensive.
 from the test sets for each language.\footnote{For tasks without public test sets (XQUAD, IndicQA), we used the validation data.}%\reut{again it sounds like we execute experiments on 44 langs. To me it seems that this is not the case - quite misleading, be precise}
%
In total, the datasets we use encompass 35  languages across 4 tasks.
Table \ref{tab:tasks_datasets} lists the datasets used, covering \textasciitilde 11 languages per task, ensuring a balanced representation of \{Low,Mid,High\} resource categories for each task. 
% While there is some overlap, not all languages appear in every task.


% metric to measure the overlap between generated and reference summaries.

% All the datasets with multiple languages we used are listed in Table \ref{tab:tasks_datasets} 
% The datasets encompass a total of 40 languages covering 10 different language families.




% \begin{table*}[t]
%     \centering
%     \subfloat[Question Answering\label{tab:output_qa}]{
% \begin{tabular}{|p{0.08\textwidth}|p{0.07\textwidth}|p{0.02\textwidth}|p{0.02\textwidth}|p{0.02\textwidth}|p{0.02\textwidth}|}
%     \hline
%     \textit{\textbf{Resource}} & \textit{\textbf{Model}} & \textit{\textbf{P.}} & \textit{\textbf{C.}} & \textit{\textbf{E.}} & \textit{\textbf{O.}} \\ \hline
%     High     & GPT & \textcolor{blue}{N} & \textcolor{green}{S} & \textcolor{green}{S} & \textcolor{green}{S} \\ \cline{2-6}
%             & Mixtral  & \textcolor{blue}{N} & \textcolor{green}{S} & \textcolor{green}{S} & \textcolor{green}{S} \\ \cline{2-6}
%             & Gemini  & \textcolor{blue}{N} & \textcolor{green}{S} & \textcolor{green}{S} & \textcolor{green} \\ \hline
%     Low     & GPT & \textcolor{blue}{N} & \textcolor{green}{S} & \textcolor{green}{S} & \textcolor{green}{S} \\ \cline{2-6}
%             & Mixtral  & \textcolor{blue}{N} & \textcolor{green}{S} & \textcolor{green}{S} & \textcolor{green}{S} \\ \cline{2-6}
%             & Gemini  & \textcolor{blue}{N} & \textcolor{green}{S} & Z & \textcolor{red}{E} \\ \hline
%     All     & Bloomz & \textcolor{red}{E}  & \textcolor{red}{E}   & \textcolor{red}{E}   & \textcolor{red}{E}   \\ \hline
% \end{tabular}

%     }
%     \hfill
%     \subfloat[Extractive Tasks (QA/NER) \label{tab:output_qa}]{
% \begin{tabular}{|p{0.07\textwidth}|p{0.05\textwidth}|p{0.02\textwidth}|p{0.02\textwidth}|p{0.02\textwidth}|p{0.02\textwidth}|}
%     \hline
%     \textit{\textbf{Model}} & \textit{\textbf{Type}} & \textit{\textbf{P.}} & \textit{\textbf{C.}} & \textit{\textbf{E.}} & \textit{\textbf{O.}} \\ \hline
%     GPT     & High & \textcolor{blue}{N} & \textcolor{green}{S} & \textcolor{green}{S} & \textcolor{green}{S} \\ \cline{2-6}
%             & Low  & \textcolor{blue}{N} & \textcolor{green}{S} & \textcolor{green}{S} & \textcolor{green}{S} \\ \hline
%     Gemini  & High & \textcolor{blue}{N} & \textcolor{green}{S} & Z & \textcolor{red}{E} \\ \cline{2-6}
%             & Low  & \textcolor{blue}{N} & \textcolor{green}{S} & Z & \textcolor{red}{E} \\ \hline
%     Mixtral & High & \textcolor{red}{E}  & \textcolor{red}{E}   & \textcolor{red}{E}   & \textcolor{red}{E}   \\ \cline{2-6}
%             & Low  & \textcolor{red}{E}  & \textcolor{red}{E}   & \textcolor{red}{E}   & \textcolor{red}{E}   \\ \hline
%     Bloomz  & High & \textcolor{red}{E}  & \textcolor{red}{E}   & \textcolor{red}{E}   & \textcolor{red}{E}   \\ \cline{2-6}
%             & Low  & \textcolor{red}{E}  & \textcolor{red}{E}   & \textcolor{red}{E}   & \textcolor{red}{E}   \\ \hline
% \end{tabular}

%     }
%     \hfill
%     \subfloat[Inference Tasks (NLI) \label{tab:output_sum}]{
%         \begin{tabular}{|p{0.07\textwidth}|p{0.02\textwidth}|p{0.02\textwidth}|p{0.02\textwidth}|p{0.02\textwidth}|}
%             \hline
%             \textit{\textbf{Model}} & \textit{\textbf{P.}} & \textit{\textbf{C.}} & \textit{\textbf{E.}} & \textit{\textbf{O.}} \\ \hline
%             GPT     & \textcolor{blue}{N} & \textcolor{green}{S} & \textcolor{green}{S} & \textcolor{green}{S} \\ \hline
%             Gemini  & \textcolor{blue}{N} & \textcolor{green}{S} & \textcolor{red}{Z}   & \textcolor{red}{E}   \\ \hline
%             Mixtral & \textcolor{red}{E}  & \textcolor{red}{E}   & \textcolor{red}{E}   & \textcolor{red}{E}   \\ \hline
%             Bloomz  & \textcolor{red}{E}  & \textcolor{red}{E}   & \textcolor{red}{E}   & \textcolor{red}{E}   \\ \hline
%         \end{tabular}
%     }
%     \hfill
%     \subfloat[Another Task \label{tab:output_another}]{
%         \begin{tabular}{|p{0.07\textwidth}|p{0.02\textwidth}|p{0.02\textwidth}|p{0.02\textwidth}|p{0.02\textwidth}|}
%             \hline
%             \textit{\textbf{Model}} & \textit{\textbf{P.}} & \textit{\textbf{C.}} & \textit{\textbf{E.}} & \textit{\textbf{O.}} \\ \hline
%             GPT     & \textcolor{blue}{N} & \textcolor{green}{S} & \textcolor{green}{S} & \textcolor{green}{S} \\ \hline
%             Gemini  & \textcolor{blue}{N} & \textcolor{green}{S} & \textcolor{red}{Z}   & \textcolor{red}{E}   \\ \hline
%             Mixtral & \textcolor{red}{E}  & \textcolor{red}{E}   & \textcolor{red}{E}   & \textcolor{red}{E}   \\ \hline
%             Bloomz  & \textcolor{red}{E}  & \textcolor{red}{E}   & \textcolor{red}{E}   & \textcolor{red}{E}   \\ \hline
%         \end{tabular}
%     }
%     \caption{Output Performance Gap (English - Source) for each task/language}%
%     \label{tab:output_english_output_target}
% \end{table*}


\paragraph{Analysis Methods}

To analyze the empirical results and detect the most influential components,
we use three methods: (i)~\textit{Correlation analysis} -- Assessing the relationship between the model's prediction scores and the language selection per component.
(ii)~\textit{Association Rule Learning (ARL)} and {\em the Apriori algorithm} --  While correlation analysis provides a preliminary understanding of the relationship between individual components and model performance, it does not capture non-linear relationships, i.e., the combined effect of multiple translation decisions on performance.
% or reveal unexpected configurations that might be crucial for model performance. 
To address this limitation, we utilize ARL with the Apriori algorithm \citep{piatetsky1991discovery}.\footnote{Appendix  
\ref{appendix:rule_association_recap_implementations} details the algorithm and implementation.} 
(iii) ~\textit{Performance Gap} --  We computed the average difference of k configuration pairs \( c_{i}\) and \( c_{j}\) such that they differ only in the language of one component, e.g., 
\(\langle I^{\textbf{e}}, X^e, E^{e}, O^e \rangle \)
%\(\langle e^{\textbf{I}}, e^X, e^{E}, e^O \rangle \) and \(\langle s^{\textbf{I}}, e^X, e^{E}, e^O \rangle \).
%
and \(\langle I^{\textbf{s}}, X^e, E^{e}, O^e \rangle \).
We then calculated this average to determine the  performance gap for each specific task: \(\frac{1}{k} \sum_{\langle i,j\rangle =1}^{k} \left( \text{Eval}(c_i) - \text{Eval}(c_j) \right)\), where \(\text{Eval}()\) denotes the task evaluation  score, and \(k\) is the number of distinct pairs.
%\reut{for a specific task? for all tasks? unclear what is the task status in this calculation }





\begin{table*}[]
\centering
\scalebox{0.995}{
\resizebox{\textwidth}{!}{
\begin{tabular}{lllllllllllllllllllllll}
\multicolumn{5}{c}{\underline{QA}}                                                                                          &                                              & \multicolumn{5}{c}{\underline{Summarization}}                                                                               &                                              & \multicolumn{5}{c}{\underline{NER}}                                                                                         &                                              & \multicolumn{4}{c}{\underline{NLI}}                                                              &                         \\
\textit{\textbf{lng}} & \textit{\textbf{cls.}} & \textit{\textbf{instruction}} & \textit{\textbf{context}} & \textit{\textbf{examples}} & \multicolumn{1}{l|}{\textit{\textbf{output}}} & \textit{\textbf{lng}} & \textit{\textbf{cls.}} & \textit{\textbf{instruction}} & \textit{\textbf{context}} & \textit{\textbf{examples}} & \multicolumn{1}{l|}{\textit{\textbf{output}}} & \textit{\textbf{lng}} & \textit{\textbf{cls.}} & \textit{\textbf{instruction}} & \textit{\textbf{context}} & \textit{\textbf{examples}} & \multicolumn{1}{l|}{\textit{\textbf{output}}} & \textit{\textbf{lng}} & \textit{\textbf{cls.}} & \textit{\textbf{instruction}} & \textit{\textbf{context}} & \textit{\textbf{examples}} \\
ru                    & A                      & -0.08**                       & \textbf{0.35**}           & 0.12**                     & \multicolumn{1}{l|}{0.09**}                   & ja                    & A                      & \textbf{-0.33**}              & -0.08                     & -0.02*                     & \multicolumn{1}{l|}{0.00}                     & fr                    & A                      & -0.11*                        & 0.10*                     & -0.01                      & \multicolumn{1}{l|}{0.01}                     & de                    & A                      & -0.03                         & -0.02                     & -0.01                      \\
de                    & A                      & -0.03**                       & \textbf{0.30**}           & 0.08                       & \multicolumn{1}{l|}{0.03*}                    & fr                    & A                      & \pz0.01                       & 0.020                     & -0.04                      & \multicolumn{1}{l|}{0.06}                     & it                    & A                      & \pz0.02                       & 0.04                      & -0.04                      & \multicolumn{1}{l|}{0.01}                     & es                    & A                      & -0.03                         & 0.02                      & -0.03*                     \\
ro                    & A                      & -0.03                         & 0.12**                    & 0.04                       & \multicolumn{1}{l|}{0.02}                     & po                    & A                      & -0.08*                        & 0.05*                     & -0.03*                     & \multicolumn{1}{l|}{0.10*}                    & po                    & A                      & -0.15                         & 0.09*                     & \pz0.1                     & \multicolumn{1}{l|}{0.01}                     & el                    & B                      & -0.04                         & 0.01                      & 0.07                       \\
vi                    & B                      & \pz0.04                       & \textbf{0.40**}           & 0.10**                     & \multicolumn{1}{l|}{0.10}                     & es                    & A                      & -0.09*                        & 0.03*                     & -0.03                      & \multicolumn{1}{l|}{0.05}                     & sv                    & A                      & -0.11*                        & 0.06*                     & -0.03**                    & \multicolumn{1}{l|}{0.01}                     & zh                    & B                      & \pz0.01                       & -0.06                     & -0.06                      \\
ar                    & B                      & -0.07**                       & \textbf{0.20**}           & 0.13**                     & \multicolumn{1}{l|}{0.04*}                    & tr                    & B                      & -0.14**                       & 0.10                      & -0.1                       & \multicolumn{1}{l|}{-0.03*}                   & zh                    & B                      & -0.26**                       & \textbf{0.44**}           & \pz0.00                    & \multicolumn{1}{l|}{0.07}                     & ar                    & B                      & \pz0.00                       & -0.02                     & -0.04                      \\
el                    & B                      & -0.06                         & \textbf{0.48**}           & 0.03                       & \multicolumn{1}{l|}{0.07*}                    & ko                    & B                      & -0.10**                       & 0.13                      & 0.01                       & \multicolumn{1}{l|}{0.05}                     & sr                    & B                      & -0.26**                       & \textbf{0.44**}           & \pz0.09**                  & \multicolumn{1}{l|}{0.05}                     & th                    & B                      & -0.03                         & 0.03                      & -0.14*                     \\
bn                    & C                      & -0.10**                       & \textbf{0.38**}           & 0.03                       & \multicolumn{1}{l|}{0.03}                     & uz                    & C                      & \textbf{-0.42**}              & 0.14                      & 0.03                       & \multicolumn{1}{l|}{-0.12*}                   & sk                    & B                      & -0.11**                       & \textbf{0.30**}           & -0.1*                      & \multicolumn{1}{l|}{0.01}                     & tr                    & B                      & -0.02                         & 0.00                      & 0.02                       \\
ma                    & C                      & -0.14**                       & \textbf{0.30**}           & 0.01                       & \multicolumn{1}{l|}{0.03}                     & fa                    & C                      & \textbf{-0.37**}              & 0.05                      & -0.07**                    & \multicolumn{1}{l|}{-0.04}                    & bam                   & D                      & \pz0.03                       & \textbf{0.44**}           & -0.11                      & \multicolumn{1}{l|}{0.02}                     & ur                    & C                      & \pz0.01                       & 0.01                      & -0.08*                     \\
te                    & C                      & -0.10**                       & \textbf{0.38**}           & 0.03                       & \multicolumn{1}{l|}{0.03}                     & ne                    & C                      & \textbf{-0.35**}              & -0.09                     & 0.07**                     & \multicolumn{1}{l|}{-0.14}                    & ewe                   & D                      & -0.01                         & \textbf{0.38**}           & -0.12**                    & \multicolumn{1}{l|}{0.01}                     & bg                    & C                      & \pz0.01                       & 0.05                      & -0.13*                     \\
hi                    & C                      & -0.07**                       & \textbf{0.30**}           & 0.05                       & \multicolumn{1}{l|}{0.01}                     & az                    & C                      & \textbf{-0.30**}              & 0.04                      & -0.00                      & \multicolumn{1}{l|}{-0.05}                    & yo                    & D                      & -0.01                         & \textbf{0.36**}           & \pz0.01*                   & \multicolumn{1}{l|}{0.03}                     & sw                    & C                      & \pz0.12                       & -0.06                     & -0.09                      \\
as                    & D                      & -0.04**                       & \textbf{0.30**}           & 0.06                       & \multicolumn{1}{l|}{0.06}                     &                       &                        &                               &                           &                            & \multicolumn{1}{l|}{}                         & hau                   & D                      & -0.04                         & \textbf{0.30**}           & \pz0.08*                   & \multicolumn{1}{l|}{0.02}                     & hi                    & C                      & -0.03                         & -0.09                     & -0.09**                    \\ \hline
\end{tabular}%
}
}


\caption{For each language, we present the Point-biserial correlation ($\tau$) between the individual component's language selection (English/Source), and the model performance score across all the configurations samples that use it. 
% Pearson correlation ($\tau$) between GPT-3.5 Turbo performance and language selection for each component.
% by task/language (lng.): \textit{instruction} \textit{context}, \textit{examples}, and \textit{output} against performance metrics. 
Positive $|\tau|$ values correlate with the source language, and negative $|\tau|$ values correlate with English. Significant correlations are indicated by *p < 0.05 and **p < 0.01. Bold values denote correlations ($|\tau| > 0.3, p < 0.01$).}
\label{tab:correlation}
\end{table*}





% \begin{table}[]
% \scalebox{1.0}{
% \resizebox{\columnwidth}{!}{%
% \begin{tabular}{clllllllclllllclllllclll}

% \multicolumn{1}{l}{\textbf{Resource}}       & \textbf{Model} &                    & \multicolumn{6}{c}{\textbf{QA}}                                                                                                & \multicolumn{6}{c}{\textbf{NER}}                                                                                                                   & \multicolumn{7}{c}{\textbf{Summarization}}                                                                                                         & \multicolumn{2}{c}{\textbf{NLI}}                                                                    \\ \cline{1-2} \cline{4-24} 
% \multicolumn{2}{c}{\textbf{Component}}                       &                    & I.                       & X.                       & E.                       & O.                       & \multicolumn{2}{l}{}                   & I.                       & X.                       & E.                       & O.                       & \multicolumn{2}{l}{}                   & I.                       & X.                       & E.                       & O.                       & \multicolumn{2}{l}{}                   & I.                       & X.                       & {\color[HTML]{000000} E.} \\ \cline{1-2} \cline{4-7} \cline{10-13} \cline{16-19} \cline{22-24} 
% \multicolumn{1}{c|}{}                       & GPT            &                    & {\color[HTML]{3531FF} N} & {\color[HTML]{32CB00} S} & {\color[HTML]{32CB00} S} & {\color[HTML]{32CB00} S} & \multicolumn{2}{l}{}                   & {\color[HTML]{3531FF} N} & {\color[HTML]{32CB00} S} & {\color[HTML]{32CB00} S} & {\color[HTML]{32CB00} S} & \multicolumn{2}{l}{}                   & {\color[HTML]{32CB00} S} & {\color[HTML]{32CB00} S} & {\color[HTML]{3531FF} N} & {\color[HTML]{3531FF} N} & \multicolumn{2}{l}{}                   & {\color[HTML]{3531FF} N} & {\color[HTML]{32CB00} S} & {\color[HTML]{FE0000} E}  \\
% \multicolumn{1}{c|}{}                       & Gemini         &                    & {\color[HTML]{009901} S} & {\color[HTML]{32CB00} S} & {\color[HTML]{32CB00} S} & {\color[HTML]{32CB00} S} & \multicolumn{2}{l}{}                   & {\color[HTML]{3531FF} N} & {\color[HTML]{32CB00} S} & {\color[HTML]{32CB00} S} & {\color[HTML]{32CB00} S} & \multicolumn{2}{l}{}                   & {\color[HTML]{FE0000} E} & {\color[HTML]{FE0000} E} & {\color[HTML]{1F1F1F} Z} & {\color[HTML]{3531FF} N} & \multicolumn{2}{l}{}                   & {\color[HTML]{3531FF} N} & {\color[HTML]{3531FF} N} & {\color[HTML]{FE0000} E}  \\
% \multicolumn{1}{c|}{\multirow{-3}{*}{High}} & Mixtral        &                    & {\color[HTML]{3531FF} N} & {\color[HTML]{32CB00} S} & {\color[HTML]{32CB00} S} & {\color[HTML]{32CB00} S} & \multicolumn{2}{l}{}                   & {\color[HTML]{3531FF} N} & {\color[HTML]{32CB00} S} & {\color[HTML]{32CB00} S} & {\color[HTML]{32CB00} S} & \multicolumn{2}{l}{}                   & {\color[HTML]{32CB00} S} & {\color[HTML]{32CB00} S} & Z                        & {\color[HTML]{32CB00} S} & \multicolumn{2}{l}{}                   & {\color[HTML]{3531FF} N} & {\color[HTML]{32CB00} S} & {\color[HTML]{32CB00} S}  \\ \cline{1-2} \cline{4-7} \cline{10-13} \cline{16-19} \cline{22-24} 
% \multicolumn{1}{c|}{}                       & GPT            &                    & {\color[HTML]{3531FF} N} & {\color[HTML]{32CB00} S} & {\color[HTML]{32CB00} S} & {\color[HTML]{32CB00} S} & \multicolumn{2}{l}{}                   & {\color[HTML]{3531FF} N} & {\color[HTML]{32CB00} S} & {\color[HTML]{32CB00} S} & {\color[HTML]{FE0000} E} & \multicolumn{2}{l}{}                   & {\color[HTML]{FE0000} E} & {\color[HTML]{FE0000} E} & {\color[HTML]{32CB00} S} & {\color[HTML]{FE0000} E} & \multicolumn{2}{l}{}                   & {\color[HTML]{3531FF} N} & {\color[HTML]{FE0000} E} & {\color[HTML]{32CB00} S}  \\
% \multicolumn{1}{c|}{}                       & Gemini         &                    & {\color[HTML]{32CB00} S} & {\color[HTML]{32CB00} S} & {\color[HTML]{32CB00} S} & {\color[HTML]{32CB00} S} & \multicolumn{2}{l}{}                   & {\color[HTML]{FE0000} E} & {\color[HTML]{32CB00} S} & {\color[HTML]{32CB00} S} & {\color[HTML]{FE0000} E} & \multicolumn{2}{l}{}                   & {\color[HTML]{32CB00} S} & {\color[HTML]{32CB00} S} & Z                        & {\color[HTML]{3531FF} N} & \multicolumn{2}{l}{}                   & {\color[HTML]{3531FF} N} & {\color[HTML]{3531FF} N} & {\color[HTML]{FE0000} E}  \\
% \multicolumn{1}{c|}{\multirow{-3}{*}{Low}}  & Mixtral        &                    & {\color[HTML]{3531FF} N} & {\color[HTML]{32CB00} S} & {\color[HTML]{32CB00} S} & {\color[HTML]{32CB00} S} & \multicolumn{2}{l}{}                   & {\color[HTML]{FE0000} E} & {\color[HTML]{32CB00} S} & {\color[HTML]{32CB00} S} & {\color[HTML]{FE0000} E} & \multicolumn{2}{l}{}                   & {\color[HTML]{FE0000} E} & {\color[HTML]{FE0000} E} & {\color[HTML]{FE0000} E} & {\color[HTML]{FE0000} E} & \multicolumn{2}{l}{}                   & {\color[HTML]{3531FF} N} & {\color[HTML]{32CB00} S} & {\color[HTML]{FE0000} E}  \\ \cline{1-2} \cline{4-7} \cline{10-13} \cline{16-19} \cline{22-24} 
% All                                         & Bloomz         & \multirow{-9}{*}{} & {\color[HTML]{32CB00} S} & {\color[HTML]{32CB00} S} & {\color[HTML]{32CB00} S} & {\color[HTML]{32CB00} S} & \multicolumn{2}{l}{\multirow{-8}{*}{}} & {\color[HTML]{32CB00} S} & {\color[HTML]{32CB00} S} & {\color[HTML]{32CB00} S} & {\color[HTML]{32CB00} S} & \multicolumn{2}{l}{\multirow{-8}{*}{}} & {\color[HTML]{FE0000} E} & {\color[HTML]{FE0000} E} & {\color[HTML]{FE0000} E} & {\color[HTML]{FE0000} E} & \multicolumn{2}{l}{\multirow{-8}{*}{}} & {\color[HTML]{FE0000} E} & {\color[HTML]{FE0000} E} & {\color[HTML]{FE0000} E}  \\ \hline
% \end{tabular}%
% }}
% \caption{The Top-performing configurations based on Apriori-based Association Rules for High/Low/All resource level. Comparing Standard LLMs (GPT/Gemini/Mixtral) and Multilingual LLM. $confidence > 0.8, support > 0.15$. \textcolor{green}{S} / \textcolor{red}{E} - source/English language, \textcolor{black}{Z} - zero-shot, \textcolor{blue}{N} - neutral.}
% \label{tab:rules}
% %\vspace{-0.2pt}
% \end{table}



\begin{table}[]
\scalebox{1.0}{
\resizebox{\columnwidth}{!}{%
\begin{tabular}{clllllllclllllclllllclll}

\multicolumn{1}{l}{\textbf{Resource}}       & \textbf{Model} &                    & \multicolumn{6}{c}{\textbf{QA}}                                                                                                & \multicolumn{6}{c}{\textbf{NER}}                                                                                                                   & \multicolumn{7}{c}{\textbf{Summarization}}                                                                                                         & \multicolumn{2}{c}{\textbf{NLI}}                                                                    \\ \cline{1-2} \cline{4-24} 
\multicolumn{2}{c}{\textbf{Component}}                       &                    & I.                       & X.                       & E.                       & O.                       & \multicolumn{2}{l}{}                   & I.                       & X.                       & E.                       & O.                       & \multicolumn{2}{l}{}                   & I.                       & X.                       & E.                       & O.                       & \multicolumn{2}{l}{}                   & I.                       & X.                       & {\color[HTML]{000000} E.} \\ \cline{1-2} \cline{4-7} \cline{10-13} \cline{16-19} \cline{22-24} 
\multicolumn{1}{c|}{}                       & GPT            &                    & {\color[HTML]{3531FF} N} & {\color[HTML]{32CB00} S} & {\color[HTML]{32CB00} S} & {\color[HTML]{32CB00} S} & \multicolumn{2}{l}{}                   & {\color[HTML]{3531FF} N} & {\color[HTML]{32CB00} S} & {\color[HTML]{32CB00} S} & {\color[HTML]{32CB00} S} & \multicolumn{2}{l}{}                   & {\color[HTML]{32CB00} S} & {\color[HTML]{32CB00} S} & {\color[HTML]{3531FF} N} & {\color[HTML]{3531FF} N} & \multicolumn{2}{l}{}                   & {\color[HTML]{3531FF} N} & {\color[HTML]{32CB00} S} & {\color[HTML]{FE0000} E}  \\
\multicolumn{1}{c|}{}                       & Gemini         &                    & {\color[HTML]{009901} S} & {\color[HTML]{32CB00} S} & {\color[HTML]{32CB00} S} & {\color[HTML]{32CB00} S} & \multicolumn{2}{l}{}                   & {\color[HTML]{3531FF} N} & {\color[HTML]{32CB00} S} & {\color[HTML]{32CB00} S} & {\color[HTML]{32CB00} S} & \multicolumn{2}{l}{}                   & {\color[HTML]{FE0000} E} & {\color[HTML]{FE0000} E} & {\color[HTML]{1F1F1F} Z} & {\color[HTML]{3531FF} N} & \multicolumn{2}{l}{}                   & {\color[HTML]{3531FF} N} & {\color[HTML]{3531FF} N} & {\color[HTML]{FE0000} E}  \\
\multicolumn{1}{c|}{\multirow{-3}{*}{High}} & Mixtral        &                    & {\color[HTML]{3531FF} N} & {\color[HTML]{32CB00} S} & {\color[HTML]{32CB00} S} & {\color[HTML]{32CB00} S} & \multicolumn{2}{l}{}                   & {\color[HTML]{3531FF} N} & {\color[HTML]{32CB00} S} & {\color[HTML]{32CB00} S} & {\color[HTML]{32CB00} S} & \multicolumn{2}{l}{}                   & {\color[HTML]{32CB00} S} & {\color[HTML]{32CB00} S} & Z                        & {\color[HTML]{32CB00} S} & \multicolumn{2}{l}{}                   & {\color[HTML]{3531FF} N} & {\color[HTML]{32CB00} S} & {\color[HTML]{32CB00} S}  \\ \cline{1-2} \cline{4-7} \cline{10-13} \cline{16-19} \cline{22-24} 
\multicolumn{1}{c|}{}                       & GPT            &                    & {\color[HTML]{3531FF} N} & {\color[HTML]{32CB00} S} & {\color[HTML]{32CB00} S} & {\color[HTML]{32CB00} S} & \multicolumn{2}{l}{}                   & {\color[HTML]{3531FF} N} & {\color[HTML]{32CB00} S} & {\color[HTML]{32CB00} S} & {\color[HTML]{FE0000} E} & \multicolumn{2}{l}{}                   & {\color[HTML]{FE0000} E} & {\color[HTML]{FE0000} E} & {\color[HTML]{32CB00} S} & {\color[HTML]{FE0000} E} & \multicolumn{2}{l}{}                   & {\color[HTML]{3531FF} N} & {\color[HTML]{FE0000} E} & {\color[HTML]{32CB00} S}  \\
\multicolumn{1}{c|}{}                       & Gemini         &                    & {\color[HTML]{32CB00} S} & {\color[HTML]{32CB00} S} & {\color[HTML]{32CB00} S} & {\color[HTML]{32CB00} S} & \multicolumn{2}{l}{}                   & {\color[HTML]{FE0000} E} & {\color[HTML]{32CB00} S} & {\color[HTML]{32CB00} S} & {\color[HTML]{FE0000} E} & \multicolumn{2}{l}{}                   & {\color[HTML]{32CB00} S} & {\color[HTML]{32CB00} S} & Z                        & {\color[HTML]{3531FF} N} & \multicolumn{2}{l}{}                   & {\color[HTML]{3531FF} N} & {\color[HTML]{3531FF} N} & {\color[HTML]{FE0000} E}  \\
\multicolumn{1}{c|}{\multirow{-3}{*}{Low}}  & Mixtral        &                    & {\color[HTML]{3531FF} N} & {\color[HTML]{32CB00} S} & {\color[HTML]{32CB00} S} & {\color[HTML]{32CB00} S} & \multicolumn{2}{l}{}                   & {\color[HTML]{FE0000} E} & {\color[HTML]{32CB00} S} & {\color[HTML]{32CB00} S} & {\color[HTML]{FE0000} E} & \multicolumn{2}{l}{}                   & {\color[HTML]{FE0000} E} & {\color[HTML]{FE0000} E} & {\color[HTML]{FE0000} E} & {\color[HTML]{FE0000} E} & \multicolumn{2}{l}{}                   & {\color[HTML]{3531FF} N} & {\color[HTML]{32CB00} S} & {\color[HTML]{FE0000} E}  \\ \hline

\multicolumn{1}{c|}{High}                                       & Bloomz         & \multirow{-9}{*}{} & {\color[HTML]{32CB00} S} & {\color[HTML]{32CB00} S} & {\color[HTML]{32CB00} S} & {\color[HTML]{32CB00} S} & \multicolumn{2}{l}{\multirow{-8}{*}{}} & {\color[HTML]{32CB00} S} & {\color[HTML]{32CB00} S} & {\color[HTML]{32CB00} S} & {\color[HTML]{32CB00} S} & \multicolumn{2}{l}{\multirow{-8}{*}{}} & {\color[HTML]{FE0000} E} & {\color[HTML]{FE0000} E} & {\color[HTML]{FE0000} E} & {\color[HTML]{FE0000} E} & \multicolumn{2}{l}{\multirow{-8}{*}{}} & {\color[HTML]{FE0000} E} & {\color[HTML]{FE0000} E} & {\color[HTML]{FE0000} E}  \\  \cline{4-7} \cline{10-13} \cline{16-19} \cline{22-24} 
\multicolumn{1}{c|}{Low}                                       & Bloomz         & \multirow{-9}{*}{} & {\color[HTML]{32CB00} S} & {\color[HTML]{32CB00} S} & {\color[HTML]{32CB00} S} & {\color[HTML]{32CB00} S} & \multicolumn{2}{l}{\multirow{-8}{*}{}} & {\color[HTML]{32CB00} S} & {\color[HTML]{32CB00} S} & {\color[HTML]{32CB00} S} & {\color[HTML]{32CB00} S} & \multicolumn{2}{l}{\multirow{-8}{*}{}} & {\color[HTML]{FE0000} E} & {\color[HTML]{FE0000} E} & {\color[HTML]{FE0000} E} & {\color[HTML]{FE0000} E} & \multicolumn{2}{l}{\multirow{-8}{*}{}} & {\color[HTML]{FE0000} E} & {\color[HTML]{FE0000} E} & {\color[HTML]{FE0000} E}  \\ \hline
\end{tabular}%
}}
\caption{The Top-performing configurations based on Apriori-based Association Rules for High/Low resource level. Comparing Standard LLMs (GPT/Gemini/Mixtral) and Multilingual LLM. $confidence > 0.8, support > 0.15$. \textcolor{green}{S} / \textcolor{red}{E} - source/English language, \textcolor{black}{Z} - zero-shot (no examples), \textcolor{blue}{N} - neutral (same performance for English/Source).}
\label{tab:rules}
\vspace{-2pt}
\end{table}


% \caption{Correlation ($\tau$) between GPT-3.5 Turbo performance and each prompt component across four tasks: Question Answering (QA), Summarization, Named Entity Recognition (NER), and Natural Language Inference (NLI). The table displays the correlation coefficients for language (\textit{lang}), instruction (\textit{instruction}), context (\textit{context}), examples (\textit{examples}), and output (\textit{output}) against performance metrics. Significant correlations are denoted by *p < 0.05 and **p < 0.01. Positive $|\tau|$ indicates correlation with source language, while negative $|\tau|$ indicates correlation with the English language. Bold values indicate a strong correlation ($|\tau| > 0.3, p < 0.01$).}








% \begin{table}[ht]
% \resizebox{\columnwidth}{!}{%
% \begin{tabular}{llllll}
% \multicolumn{5}{c}{\textbf{High Resource Languages}}                                                                                                                                                                                                                                                   \\ \hline
% \multicolumn{1}{c}{{\textit{\textbf{Task}}}} & \multicolumn{1}{c}{{\textit{\textbf{Law}}}}                           & \multicolumn{1}{c}{{\textit{\textbf{Support}}}} & \multicolumn{1}{c}{{\textit{\textbf{Confidence}}}} & \multicolumn{1}{c}{{ \textit{\textbf{lift}}}}\\
% \textit{NER}                                              & (Context: S)  (Context: S) =\textgreater  Percentile 70                    & 0.173                                               & 1.0                                                    & 2.01                                                                            \\
% \textit{QA}                                               & (Context: S)  (Output: S) =\textgreater  Percentile 70                         & 0.25                                                & 1.0                                                    & 2.03                                                                            \\
% Summarization
% & (Instruction: S)  (Context: S)  (Examples: E) =\textgreater  Percentile 70      & 0.1                                                 & 0.88                                                   & 1.57                                                                                    \\
%                                                  & (Instruction: S)  (Context: E)  (Examples: S) =\textgreater  Percentile 70              & 0.1                                                 & 0.88                                                   & 1.57                                                                                      \\
%                                                  & (Instruction: E)  (Examples: S)  (Output: E) =\textgreater  Percentile 30             & 0.1                                                 & 1.0                                                    & 2.0                                                                                    \\
% \textit{NLI}                                              & (Instruction: S)  (Context: S)  (Examples: E) =\textgreater  Percentile 70 & 0.09                                                & 1.0                                                    & 2.03                                                                          \\
% Cross-tasks                                      & (Context: S)  (Output: S) =\textgreater Percentile 70                             & 0.05                                                & 0.24                                                   & 0.6                                                                                    \\
% \multicolumn{5}{l}{}                                                                                                                                                                                                                                                                                                                             \\
% \multicolumn{5}{c}{\textit{\textbf{Low Resource Languages}}}                                                                                                                                                                                                                                                                                                       \\ \hline
% \multicolumn{1}{c}{{ \textit{{\textbf{Task}}}}} & \multicolumn{1}{c}{{ \textit{{\textbf{Law}}}}}                           & \multicolumn{1}{c}{{\textit{\textbf{Support}}}} & \multicolumn{1}{c}{{\textit{{\textbf{Confidence}}}}} & \multicolumn{1}{c}{{{\textbf{\textit{lift}}}}}          \\
% \textit{NER}                                              & (Context: S)  (Examples: S)  (Output: E) =\textgreater Percentile 70       & 0.07                                                & 0.75                                                   & 0.05                                                                               \\
% \textit{QA}                                               & (Instruction: S)  (Context: S)  (Output: S) =\textgreater Percentile 70        & 0.25                                                & 1.00                                                   & 2.00                                                                         \\
% \textit{Summarization}                   & (Instruction: S)  (Context: S)  (Examples: E) =\textgreater Percentile 70       & 0.07                                                & 0.84                                                   & 1.67                                                                               \\
%                                                  & (Instruction : E)  (Examples: S)  (Output: S) =\textgreater Percentile 30     & 0.09                                                & 1.00                                                   & 1.96                                                                              \\
% \textit{NLI}                                              & (Instruction: E)  (Context: E)  (Examples: S) =\textgreater Percentile 70       & 0.17                                                & 1.00                                                   & 2.00                                                                                     \\
% Cross-tasks                                      & (Context, S) =\textgreater Percentile 70                                    & 0.06                                                & 0.12                                                   & 1.8                                                                             \\ \hline      
% \end{tabular}}

% \caption{
% Apriori-Based Association Rules.  (prompt part: English/Source) =>  Top X\% scores over configurations. }
% \label{tab:rules}
% \end{table}



% \begin{figure*}[h]
%     \centering
%     \subfloat[Correlation between linguistic syntax to English and Task Perfromance]{{\includegraphics[width=0.34\textwidth]{syntactic_heatmap.png} }}%
%     \hfill
%   \subfloat[Relative percentage improvement over English-translated prompting for GPT-3.5-Turbo, using the best configuration for each task. Bars are color-coded by language script.\label{fig:family_langs}]{{\includegraphics[width=0.63\textwidth]{family_langs.png} }}%
%     \caption{Analysis of language similarity and script influence on LLM performance.}%
%     \label{fig:script_correlaton}%
% \end{figure*}





% \begin{figure*}[h]
%     \centering
%     \subfloat[Correlation between linguistic syntax similarity to English and task performance for GPT, Mixtral, and Gemini.  * p < 0.05, **
% p < 0.01. \label{fig:syntactic_heatmaps}]{{\includegraphics[width=0.3\textwidth]{script_correlation.png} }}%
%     \hfill
%   \subfloat[Relative percentage improvement over English-translated prompting for GPT, using the best configuration for each task. Bars are color-coded by language script.\label{fig:family_langs}]{{\includegraphics[width=0.63\textwidth]{family_langs.png} }}%
%     \caption{Analysis of language similarity and script influence on LLM performance.}%
%     \label{fig:script_correlaton}%
% \end{figure*}


% Similar tables for Gemini and Mixtral can be found in Table \ref{tab:gemini_correlation}
% and Table \ref{tab:mixtral_correlation} in the Appendix.

% footnote{\url{{https://github.com/openai/gpt-3/blob/master/dataset_statistics/languages_by_word_count.csv}}}. 



% \begin{figure}
%     \centering
%   \includegraphics[width=1\linewidth]{qa_improvment_over_english.png}
%     \caption{Plot of the percentage improvement in our evaluation metric compared to the English-Translate configuration. We can observe that the highest improvements are achieved for Low-Resource languages.}
%     \label{fig:enter-label}
% \end{figure}





% \section{Results and Analysis}

% In this section, we analyze the results of our benchmarking exercise across tasks and languages.
% Broadly, we compare the effectiveness of various prompting strategies followed by the performance comparison of GPT-3.5, Mixtral-8x7B-Instruct-v0.1, and Gemini. We conclude by examining the factors that affect these models' performance.


% \subsection{Correlation Analysis}

% Following our discussion on prompt setup components (Instruction, Input, Context, and Output), we delve into the relationship between these elements and model performance. We aim to address the following questions:

% \begin{enumerate}
%     \item Does each prompt component (Instruction, Input, Context, and Output) exhibit a correlation with the model's overall performance?
%     \item In which specific tasks do these correlations manifest? Do they vary across language subsets categorized as High, Medium, and Low resource languages?
%     \item Based on the observed correlations, can we identify an ideal configuration (combination of elements) for the prompt setup that optimizes model performance
% \end{enumerate}








% \begin{figure*}[th]
%     \centering
%     \includegraphics[width=1.0\linewidth, height=0.1\textheight]{improvement_over_english.png}
%     \caption{Relative percentage improvement over English-Translate prompting, when using the highest configuration for each task For GPT-3.5-Turbo. The bars are color-coded based on the norm of log of number of tokens in the pre-trained data of GPT-3, as elaborated in Table \ref{tab:languages}}
%     \label{fig:improvement_over_english}
% \end{figure*}


\subsection {Results}
\label{chap3_results}

\renewcommand{\arraystretch}{1.1}


In Section \ref{chap_4_compare_methods}, we present the results of  \emph{selective pre-translation}  demonstrating their advantage over both \emph{direct inference} (source language only) and \emph{pre-translation} (English only). Subsequently, in Section \ref{chap_4_optimal_configuration}, we identify the optimal configurations for each task and analyze the impact of each component on the overall performance, emphasizing key considerations for effective prompting. We start off with GPT-3.5-Turbo and proceed to verify that our results generalize   to other models.



\subsubsection{Selective Pre-Translation Advantage}
\label{chap_4_compare_methods}

% Here, we display the overall advantage of selective pre-translation strategies without specifying the exact configuration, while in the next subsection, we illustrate the "optimal" configurations in different multilingual scenarios. 
Table \ref{tab:summary_results} shows each language's highest-performing configuration score among all 24 distinct configurations.\footnote{Appendix \ref{appendix:detailed_results} displays the full-fledged table of results.} Additionally, we display the improvement (\%) over \emph{direct inference} and \emph{pre-translation} for each language. 
The results indicate that 92\% of the tested languages show an improvement over the basic \emph{pre-translation} configuration. Particularly for low-resource languages like Malayalam and Telugu, the gains with \emph{selective pre-translation} are substantial, exceeding 200\% in relative improvement. Overall, when comparing selective pre-translation to  \emph{complete pre-translation}, the average improvement in low-resource languages is  65\% greater than  the average improvement in high-resource languages. 

%\paragraph{Improvement Over Direct-Inference} 

The results further reveal that 90\% of the languages show improvement over   basic \emph{direct inference}. Similar to the pre-translation approach, low-resource languages like Telugu and Assamese demonstrate a relative improvement of over 100\%. High-resource languages also show impressive improvement, albeit smaller, e.g., French and Portuguese show an improvement of over 20\% in NER. 


% Figure \ref{fig:Improvement_over_monolingual} displays the percentage improvement of choosing the highest configuration score compared to the \emph{direct inference} score, highlighting that languages with lower representation achieve better improvement than those with higher. 
Overall, the table shows that \emph{selective pre-translation} can outperform both \emph{pre-translation} and \emph{direct inference}, particularly for languages considered low-resource during pre-training.









% % we find that low-resource languages made the highest improvement. Specifically, languages from classes C and D achieved an improvement of 65\% on average compared to classes A and B. Figure \ref{fig:Improvement_over_monolingual} displays the percentage improvement of choosing the highest configuration score compared to the \emph{direct inference} score, showing that languages with lower representation (small number of tokens) achieved better improvement than languages with higher representation. For example, Telugu improved by over 230\% over the source configuration. Similarly, appendix \ref{appendix:improvement_over_base_configurations}provides an additional figure and reveals that the improvement of the highest configuration compared to the \emph{pre-translation} strategy is also the greatest for low-resource languages.






% Table \ref{tab:summary_results} displays the highest performing configuration score for each language among all the distinct configurations (24), along with the improvement of the highest scoring configuration from two basic common strategies: \emph{direct inference}, where all parts are in the source language, and \emph{pre-translation}, where all parts are in English. This section shows that typically the \emph{selective pre-translation} configuration outperforms both \emph{direct inference} and \emph{pre-translation}.





% In this section, we present the results of our \emph{selective pre-translation} approach. We start by showing that when choocing a 







% Table \ref{tab:summary_results} displays the highest performing configuration score for each language among all the distinct configurations (24), along with the improvement of the highest scoring configuration from two basic common strategies: \emph{direct inference}, where all parts are in the source language, and \emph{pre-translation}, where all parts are in English. This section shows that typically the \emph{selective pre-translation} configuration outperforms both \emph{direct inference} and \emph{pre-translation}.




\subsubsection{The Holy Grail of  Optimal Configuration }
\label{chap_4_optimal_configuration}
Having established the advantage of \emph{selective pre-translation} in general, we now study the effects of component language selection on model performance and provide general guidelines for multilingual scenarios.\footnote{For the instruction component language, except for a slight preference for English as demonstrated in Table \ref{tab:correlation}, we did not observe a strong affinity for any language selection.}
% . Based on these observations, we narrate general rules for various multilingual scenarios. 
Table \ref{tab:correlation} shows the Point-biserial correlation between individual component selection and model performance for all the 24 configurations per language/task.\footnote{We calculate the correlation between performance and a binary vector indicating whether the component is in English.} Table \ref{tab:rules} presents the top-performing configurations, based on the highest-scoring apriori rules for multiple component selections, henceforth {\em optimal configurations}.


% See Appendix \ref{appendix:detailed_results} for
% supplementary results for other models.\reut{move to model impact}

 
\paragraph{Context Language Impact}

% Table \ref{tab:correlation} indicates that extractive tasks, such as QA and NER, benefit from including source-language context. This effect is particularly pronounced for low-resource languages (Class C/D), exhibiting a 70\% higher correlation coefficient with source-language context compared to high-resource languages. Conversely, tasks like abstractive summarization and NLI seem to be agnostic to context translation. Our rule-association analysis in Table \ref{tab:rules} further highlights the model's preference for source-language context in extractive tasks (NER, QA). Therefore, prioritizing source-language context for extractive tasks is generally recommended, especially for low-resource languages.



%\reut{is this only for a single model or does it generalize / change across models?} 

Table \ref{tab:correlation} shows that source language selection correlates most with model performance score in extractive tasks, such as QA (average of 0.33) and NER (average of 0.32), particularly for low-resource languages (Class C/D), which demonstrate a 70\% higher correlation coefficient compared to high-resource languages. In contrast, in tasks like abstractive summarization and NLI, we found no correlation (average of 0.05) to context language selection. Our rule-association analysis in Table \ref{tab:rules} further underscores the importance of source-language context in extractive tasks,  especially with low-resource languages.
% Therefore, it is recommended to prioritize source-language context for extractive tasks, especially for low-resource languages.

% Interestingly, the cross-task association rule that also requires source language context. 
% However, this rule has a low confidence score of 0.3, implying it's not always the case for all configurations.



\begin{figure}[t]
    \centering
    \subfloat[QA\label{fig:syntactic_heatmaps_1}]{{\includegraphics[width=0.23\textwidth,height=0.15\textwidth]{english_input_qa_target_input_mixtral_updated.png} }}%
    \hfill
    \subfloat[NER\label{fig:family_langs_1}]{{\includegraphics[width=0.23\textwidth, height=0.15\textwidth]{english_output_ner_target_output.png} }}%
    \hfill
    \subfloat[NLI\label{fig:syntactic_heatmaps_2}]{{\includegraphics[width=0.23\textwidth,height=0.15\textwidth]{english_shot_xnli_target_shot.png} }}%
    \hfill
    \subfloat[Summarization.\label{fig:family_langs_2}]{{\includegraphics[width=0.23\textwidth, height=0.15\textwidth]{english_output_sum_target_output.png} }}%
    \caption{Performance Gap Analysis for the Examples language (English minus Source). Left to right X-axes order indicates Low to High Resource Level. Y-axes indicate language preference: positive values for the source language, and negative for the English language.}%
    \label{fig:english_shot_target_shots}%
    \vspace{-2pt}
\end{figure}


% \paragraph{Examples Language Impact} Analysis of rule associations in Table \ref{tab:rules} reveals that extractive tasks (NER, QA) benefit from incorporating source-language examples. We hypothesize that because NER is a region-specific task that requires the model's pre-trained knowledge, examples in the source language help the model augment its knowledge base in the specific language. Figure \ref{fig:english_shot_target_shots} aligns with these findings, depicting a similar performance gap between few-shot English examples and source-language examples.  


\paragraph{Examples Impact}

In general, the top-performing configurations of 
the GPT model in Table \ref{tab:rules}
%\reut{I dont understand how to read this table as "rules"}
%\reut{plus we have lots of ablation for the "english centric" and extremely little breakdown on multilingual - why?}
show that the optimal configurations are those that include examples, i.e., a few-shot rather than zero-shot setup (Appendix \ref{appendix:optimal_configuration} further supports incorporating examples in prompts, especially for high-resource languages). 
%\paragraph{Examples Language Impact} 
Concretely concerning the language selected for the examples, the optimal configurations in Table \ref{tab:rules} show that extractive tasks as NER perform better with source-language examples, possibly due to NER's dependence on region-specific or culturally-relevant knowledge. Also, the performance gap analysis in Figure \ref{fig:english_shot_target_shots},  shows that, for extractive tasks, prompts with examples in the source language perform better than those with English examples, especially for low-resource languages (See \ref{fig:english_shot_target_shots}(a)/\ref{fig:english_shot_target_shots}(b)).
%aligns with the above NER-reported findings,



% showing a similar performance gap between few-shot English examples and source-language examples.
% %\reut{I dont understand - please claridy - gap in which direction?}



% Interestingly, the figure shows a preference for English examples in the summarization task. Specifically, for high-resource languages, the model performs well regardless of the examples' language. However, for low-resource languages, English examples result in better performance.

% also highlight that for all tasks, examples in the source language are better except for the summarization task where English examples are better. 


% The results in the rule association also show that examples in the source language yield a higher score for the NER task. 
% We hypothesize that because NER is a region-specific task that is based on Knowledge in the source language, examples in the source language help the model augment its knowledge base. Conversely, in the summarization task, where the examples are less relevant to the input, and just provide an example of generation the model is agnostic to the examples' language in high-resource languages, while in low-resource languages, the model prefers English examples.

% The results in Figure \ref{fig:english_shot_target_shot} also highlight that for all tasks, examples in the source language are better except for the summarization task where English examples are better. 



% and depicted also in Figure \ref{fig:english_shot_target_shot} in the Appendix. The results in the rule association also show that examples in the source language yield a higher score for the NER task. We hypothesize that because NER is a region-specific task that is based on Knowledge in the source language, it benefits from localized examples. Conversely, in the summarization task, where the examples are less relevant to the input, the model is agnostic to the examples' language in high-resource languages, while in low-resource languages, the model prefers English examples.


\paragraph{Output Language Impact}
Unlike context and examples, the output depends on the model generations's grammaticality and fluency. The best-performing configurations for the GPT model in Table \ref{tab:rules} indicate that for extractive tasks, source-language output is beneficial across all languages. Interestingly, despite context mismatches, NER in low-resource languages also benefits from English output. For generative tasks such as summarization, model output in English 
%\reut{do you mean you assess the output in english? I thought you translate back to source. what is the rouge measuring exactly?} 
performs better due to the model’s stronger capabilities in English,  {\em even though} we back-translate the output to source prior to evaluation. Thus,  while it is fine  in such generative tasks to instruct the model to generate outputs in the source language for high-resource languages, it appears better to generate in English in the low-resource case.











% However, Appendix \ref{appendix:output_english_output_target_appendix} displays the performance gap and shows that output for NER is more ambiguous. 





\subsection{Beyond Configuration: Key Factors}

Having analyzed the impact of the components' language selection, we discuss key additional factors influencing the efficacy of our approach.\footnote{See also Appendix \ref{appendix:script_impact} for script impact.}


\paragraph{Pre-Training Data Size Impact}
% Table \ref{tab:summary_results} presents the highest-prompting prompt configuration for each language and task and shows that for QA, summarization and NER the general trend is that classes A and B (High-Medium resource) achieve better results than those in classes C and D (Low resource), indicating that more pre-trained data yields better performance. However, for NER we found not notable exceptions exist, i.e., Hausa and Ewe (Class C/D) achieve better results on the NER task compared to Swedish and Chinese (Class A/B). For NLI task there is no clear train where several class C/D languages outperform A/B in NLI. While pre-trained data size matters, selective pre-translation can help low-resource languages surpass high-resource ones in specific tasks.

Table \ref{tab:summary_results} presents the optimal prompt configuration scores per language and task. For QA, summarization, and NER, the general trend indicates that even for the optimal pre-translation configuration,  classes A and B (High-Medium resource) achieve better results than classes C and D (Low resource),
%\reut{on what configurations are these types better?} 
%suggesting that larger amounts of pre-trained data lead to improved performance. 
However, a few exceptions exist, i.e., in Hausa and Ewe (Class C/D) we see better results on the NER task compared to Swedish and Chinese (Class A/B). For the NLI task we found no trend where a  class C/D languages outperform A/B languages. So, while pre-trained data distribution matters, selective pre-translation can help low-resource languages match the results of higher-resource ones in specific tasks.




%\reut{some of the results here sounds a bit anecdotical - do we have a robust statement to make across all data? or is it on per-language basis? that's unclear overall}


%\reut{a general comment - for each table spellout in words WHAT DOES IT SHOW - whats on the x,y axes, what are the metrics within cells. Otherwise it is hard to confirm that your statements hold} 


% Fix typos, correct and add a practical guide in the end


% This analysis suggests that while high and medium-resource languages tend to perform better overall in Summarization, QA, and NER tasks, low-resource languages can outperform high and medium-resource languages in NLI tasks. Surprisingly, languages from classes A and B showed similar results with no preference for A over B.








% Table \ref{tab:summary_results} displays for each language its highest performing configuration score among all the distinct configurations (24), along with the improvement of the highest scoring configuration from two basic common strategies - \emph{direct inference} where all the parts are in the source language and \emph{pre-translation} where all the parts are in English. In this section, we show that usually \emph{selective pre-translation} configuration outperforms both \emph{direct-inference} and \emph{pre-translation}. 
% Specifically, the results in the table show that languages in classes A and B generally achieve better results than those in classes C and D in QA and Summarization tasks, implying that more pre-trained data yields better performance. Interestingly, in the NER task, languages Hausa and Ewe from classes C and D outperformed Swedish and Chinese from classes A and B. For the NLI task, we observed a better multilingual capability where low-resource languages such as Swahili and Bulgarian achieved high results over languages from classes A and B. This analysis suggests that while high and medium-resource languages tend to perform in general better in Summarization, QA, and NER tasks, low-resource languages can even outperform high and medium languages in NLI tasks. Surprisingly, languages from classes A and B showed similar results with no preference for A over B.

% In addition to the highest performing, when considering the improvement of choosing the highest configuration for each language compared to the \emph{pre-translation} and to the \emph{direct-inference} strategies we reveal that low resource languages made the highest improvement. Specifically,  languages from classes C and D achieved an improvement of 65\% on average compared to the classes A and B. Figure \ref{fig:Improvement_over_monolingual} displays the percentage improvement of choccing the highest configuration score compared to \emph{direct inference} score  and shows that languages with lower representation (small number of tokens) achieved better improvement than languages with higher representation. For example, Telugu has improved by over 230\% over the source configuration. Simillarly, appendix \ref{appendix:improvement_over_base_configurations} provides additional figure and reveals also that improvement of the highest configuration compared to \emph{pre-translation} strategy is also the greatest for low-resource languages. 






% The table also shows the percentage of improvement over two base configurations: \emph{pre-translation}, where all components are in English, and the \emph{direct-inference} where all parts are in the source language. 




% The results in the table demonstrate that languages from classes C and D achieved a higher improvementof 65\% on average compared to the other classes. Figure \ref{fig:Improvement_over_monolingual} displays the percentage improvement over the \emph{direct inference} and shows that languages with lower representation (small number of tokens) achieved better improvement than languages with higher representation. For example, Telugu has improved by over 230\% over the source configuration. Appendix \ref{appendix:improvement_over_base_configurations} provides additional 



% analysis, revealing that \emph{selective pre-translation} outperforms \emph{pre-translation} and has a greater positive impact on low-resource languages.




% \begin{figure*}[th]
%     \centering
%     \includegraphics[width=0.8\linewidth, height=0.13\textheight]{family_language_by_lang_code_v2.png}
%     \caption{Relative percentage improvement over English-translate prompting, when using the highest configuration for each task For GPT-3.5-Turbo. The bars are color-coded based on the language script.}
%     \label{fig:script}
% \end{figure*}

% \begin{figure*}[th]
%     \centering
%     \includegraphics[width=0.75\linewidth, height=0.12\textheight]{family_languages_with_lang_code.png}
%     \caption{Relative percentage improvement over English-translate prompting, when using the highest configuration for each task For GPT-3.5-Turbo. The bars are color-coded based on the language script.}
%     \label{fig:script}
% \end{figure*}

% \begin{figure*}[th]
%     \centering
%     \includegraphics[width=0.75\linewidth, height=0.12\textheight]{family_languages_with_lang_code.png}
%     \caption{Relative percentage improvement over English-translate prompting, when using the highest configuration for each task For GPT-3.5-Turbo. The bars are color-coded based on the language script.}
%     \label{fig:script}
% \end{figure*}



% \begin{figure}[t]
%     \centering
%     \scalebox{0.95}{
%     \includegraphics[width=0.5\textwidth]
%     {script_correlation.png}}
%     \caption{Person correlation between linguistic syntactic similarity to English and task performance for GPT, Mixtral, and Gemini.  * p < 0.05, **
% p < 0.01. }
%     \label{fig:syntactic_heatmaps}
% \end{figure}

\begin{figure*}[th]
\scalebox{1.0}{
\centering
\includegraphics[width=1\textwidth]{bert_to_sytactic_with_group_trends.png}
}
% \vspace*{-1mm}
%\footnotesize{
\caption{Scatter plot showing the relationship between syntactic similarity to English (further right is more similar) and translation quality (ROUGE) for four language resource subsets (represented as distinct four colored shapes).  Each  dot represents a different language. Positive linear regression shows an upward trend. }\label{fig:syntactic_similarity_translation}
%}
\end{figure*}



\paragraph{Linguistic Similarity to English Impact} We used pre-computed syntactic similarities to English from the URIEL dataset \citep{littell2017uriel} and calculated the Pearson correlation for each task between the best-performing configuration scores (top score per language) and the syntactic similarity of these languages to English. A moderate correlation (0.42) for summarization shows that syntactic similarity to English positively correlates with performance. NER also shows moderate correlations, suggesting models better identify entities when texts share syntactic features with English. Appendix \ref{appendix:factors_explaning_performance} further details these results.

%\reut{I didnt find there the details I am looking for: eg which features and what is the calculation}
%\reut{figure 8 tables are too tiny. also other tables can benefit from larger font}


\paragraph{Standard Model Impact}

In the previous section, we assessed the selective pre-translation strategies using the GPT model. In this section we check whether  these strategies generalize to other  LLMs. Table \ref{tab:rules} displays the optimal configurations per task and language for Gemini and Mixtral. We  see that the {\em preference for source language in extractive tasks} (for context, examples, and output)  holds across all three models. Additionally, outputting in English while keeping the {\em context in the source language for NER} in low-resource languages is consistent. In NLI, models are agnostic to instruction language. However, surprisingly, in abstractive summarization, we found no clear pattern.\footnote{See Appendix \ref{appendix:detailed_results} for
additional results.}

% See Appendix \ref{appendix:detailed_results} for
% supplementary results for other models.\reut{move to model impact}


\paragraph{Multilingual Model Impact}

In addition to the standard LLMs, we evaluated BLOOMZ-7b1-mt, known for its multilingual capabilities \citep{muennighoff2022crosslingual}. Table \ref{tab:rules} displays the optimal configurations for BLOOM across all tasks. We found no distinction between resource types for this model. As shown, the preference for source language in QA is relevant here as well. Interestingly, NER can be answered in the source language, highlighting its multilingual strength. However, for generative tasks and NLI, this multilingualism diminishes, as the model tends to favor English prompts. Overall, the top-performing configurations for BLOOMZ indicate that it performs better with single-language prompts rather than with selective pre-translation prompts.




% s shown in Table \ref{tab:rules}, prompting in source language\reut{native languages? not sure what you mean -- sounds odd here} has advantages over translation for all LLMs, particularly in tasks that are extractive (e.g., QA) or highly language-dependent (e.g., NER). We hypothesize that processing in the source language helps avoid information loss and activates bilingual knowledge. Surprisingly, for all English-centric models, outputting in English for NER yielded better results for low-resource languages. For generative tasks, the degree of multilingualism and the size of pre-trained data influenced model performance, with models behaving differently depending on whether the language was low- or high-resource.\reut{but we dont know how the lang distribution changes for different models? so how can we argue about that?}




% For example, the translation\reut{selective? or not?} benefit was most pronounced for Mixtral with low-resource languages and for Bloomz across all languages, while Gemini demonstrated the best multilingual capabilities overall.\reut{couldnt parse this last sentence - too cumbersom. rephrase} In the NLI task, Gemini showed neutrality regarding language selection, whereas other models typically prioritized the source\reut{please be consistent in using the term "source" - you need to define it *very* early (in the intro), and not switch to other terms like "native" etc} language. Surprisingly,\reut{why is this a surprize?} despite the multilingual data that Bloomz was trained on, it showed a preference for translated prompts in both summarization and NLI tasks.\reut{I did not understand what this paragraph stands to deliver - what is the bottom line}







% Our analysis indicates that all models tend to prefer context in the source language for extractive tasks, with the strongest preference observed in QA tasks. Gemini and Bloomz exhibited the highest correlation with the source language, with Gemini also showing a marked preference for examples in the source language. In contrast, Mixtral was the least influenced by the language of the prompt. No clear language preference was observed for the NLI task or for the choice of instruction language. However, for generative tasks such as summarization, some models showed a preference for English output.





% We found that most models prefer context in the source language for extractive tasks, with the strongest preference seen in question-answering (QA) tasks. Gemini and BloomZ had the highest correlation with the source language, and Gemini also showed a strong preference for examples in the source language. Mixtral, on the other hand, was least affected by the language of the prompt, while Gemini, GPT, and Mixtra showed more language-specific tendencies. No clear language preference was observed in Natural Language Inference (NLI) tasks or for instruction language. However, in generative tasks like summarization, some models favored English output.
% Check what is the optimal configuration for top 3 languages close to english


% \paragraph{The Impact of the Prompt Configuration}


% \paragraph{Statistical Approaches for Modular prompting}


% In this study, we investigate the impact of prompt configuration on the multilingual performance of the GPT-3.5 Turbo model. Specifically, we examine the correlation\footnote{Ref to Appendix \ref{tab:correlation_appendix} for implementation details.} between the model's prediction scores and the chosen language component, which is represented by a binary vector (0 represents English and 1 represents the source language). The results are shown in Table \ref{tab:correlation}.

% While correlation analysis provides a preliminary understanding of how individual components relate to model performance, it cannot capture non-linear relationships or expose unexpected configurations that might be important for model performance. To overcome these limitations, we employed
%  \emph{association rule learning (ARL)} with the Apriori algorithm 
%  \citep{piatetsky1991discovery, hegland2007apriori}, a common method in the realm of data mining. The results are shown in Table \ref{tab:rules}.\footnote{Ref to Appendix \ref{tab:apriori_technical_into} for a technical recap of the algorithm.}

% Based on the results of these approaches, we make the following observations:



% \subsubsection{Effects of Instruction Selection}

% Our results show that for low-resource languages there is a negative correlation, which implies the model prefers instruction in English in summarization task. Also, we can see that for low resource languages the improvement when choosing instruction in English is even better.






 
 
%  all cases, it is better to use few-
 
 
 
%  shot learning over zero-shot learning, as reported in the rule association results in Table \ref{tab:rules}, which all the rules consist . Also,

 
%  as depicted in Figure \ref{fig:zero_shot_few_shot} in the Appendix. We also found that for the summarization, QA, and NER tasks, the improvement is higher for high-resource languages compared to low-resource languages. Conversely, for the NLI task, the improvement is higher for low-resource languages.



% The results in Table \ref{tab:correlation} reveal interesting correlation. For QA (XQuAD), an extractive task, a statistically significant positive correlation was observed between the input language and the model performance, suggesting a preference for source language input. Notably, the correlation coefficient was higher for high-resource languages than for low-resource ones, except for Russian. A similar trend was observed in NER (WikiAnn and MasakhaNER), where a subset of languages (primarily medium-to-low resource) exhibited a correlation favoring source language input. In contrast, for Summarization (XLSum), there is a correlation to the instruction component in a subset of medium-to-low resource languages, with a preference for English prefixes. Interestingly, no significant correlations were observed between any component and the scores vector for the NLI task (XNLI).

% \textbf{Multipile Components} While correlation analysis provides a preliminary understanding of how individual components relate to model performance, it cannot capture non-linear relationships and expose unexpected configurations that might be important for model performance. To overcome these limitations we employed association rule learning (ARL) with the Apriori algorithm \citep{piatetsky1991discovery, hegland2007apriori}, a common method in the realm of data mining. 
% Here, each prompt configuration is treated as a transaction, with the four components (instruction, input, context, and output) acting as individual items within the transaction.  The model performance metric (accuracy, F1 score, ROUGE) is then categorized into bins (High, Medium, Low performance) based on percentiles. Apriori then works by identifying frequently occurring itemsets (combinations of prompt components) within these transactions. By setting minimum support (0.05) and confidence (0.75) thresholds, we can focus on itemsets that not only appear often (high support) but also tend to be associated with bins representing high model performance (high confidence). 

% We can make the following observations:

% 1. In cases where the input is relevant to the source language (NER, QA), the model prefers to receive the input in the source language for both low and high-resource languages. In the low-resource case, in the NLI task, which is not language-dependent, it's better to do the input as well as the instruction in English. Additionally, for the NER task, the context should be in the source language for both low and high-resource languages.

% 2. In high-resource languages, where the model is better at producing high-quality text, it is preferable to output in the source language. For low-resource languages, this is not always the case.

% 3. Interestingly, in all cases except for NLI in low-resource languages, the instruction should be in the source language.



% High languages: there is no preference for prefixes in English at at all. There are even preferences to input is the source language 



% Low languages: 



% The results In Table \ref{tab:apriori} show that for most of the tasks and language classes input in the source language is preferred.  





% These findings highlight the importance of prompt configuration in optimizing GPT-3.5-turbo's performance across diverse languages. The model appears to be sensitive to the language of specific prompt components, particularly the input for factual language tasks (XQuAD) and NER. Further research is warranted to explore these observations and develop language-specific prompt engineering strategies to maximize GPT-3.5-turbo's multilingual capabilities.





%


% All the laws produce high scores per task, except for cross-tasks which represent rules that produce high scores in all the tasks (high bins). All the represented rules are strong, except for cross-task which has lower confidence. 

 % Only strong rules (support > 0.05 and confidence > 0.75) are considered.

% \section{Translation Quality Impact}
% \begin{figure}
%     \centering
%     \includegraphics[width=0.5\linewidth]{sytactic_rouge.png}
%     \caption{Enter Caption}
%     \label{fig:enter-label}
% \end{figure}


\begin{figure}[t]
  % \vspace{-1.2em} % Adjust this value to reduce the space
  \centering
\includegraphics[width=0.497\textwidth]{Bert_F1.png}
  % \vspace{-2em} % Adjust this value to reduce the space
  \caption{Correlation between translation quality (BERTScore) and accuracy (F1) for Pre-Translation-Zero-shot prompting, each  dot is a different language.}
  \label{fig:bleu_f1}
   % \vspace{-1em}
\end{figure}


% \section{Translation Quality Impact}
\section{Translation: Key to Pre-Translation}
\label{chap_5}
% The correlation with the High languages is the strongest, with a coefficient of -0.73 and a p-value of 0.004. The negative correlation indicates that languages with smaller linguistic distances (higher similarity) to English tend to achieve better machine translation to English.

In the previous Section we examined various factors affecting model performance for selective pre-translation strategies. Since translation forms the foundation of  selective pre-translation, this Section focuses on a key question: Are these factors primarily due to the limitations of LLMs, or are due to the quality of the pre-translations themselves?
% attested for that language? 
To address this, we  first isolate the impact of these factors on translation quality through a controlled experiment. Subsequently, we  investigate how translation quality, independently of other factors, influences downstream tasks in our setup.
%in our proposed setup.




% So far, we have discussed \emph{selective pre-translation} strategies and assessed them using the Google Translate API.\footnote{See Appendix \ref{appendix:mt_engines_comparison} for comparison with Bing Translator.} However, it is reasonable to believe that the quality of translation may change these selection strategies' efficacy. First, we show the effect of translation quality on the downstream task (for the QA task). Focusing on the correlation between translation quality measure and "all" pre-translation. Second, we assess what factors affect the translation quality, particularly those we investigated in the previous Section (Pre-Training Data Size, Similarity to English).




% [****MISSING**** - comparing configurations]




% Therefore, in this Section we first, assess empirically the quality of translation for two different engines and 80 language\footnote{This is the intersection ... } (The FLORES dataset).

% \textbf{move to appendix




% we assss to the effect 





% Second, we assess what factors affect affect the translation quality, and in particular, those factor we investigated in the prev Section (Low-resources, Similarity to English), 
% [****MISSING**** - comparing configurations]

% המטרה פה היא לבודד, האם הפקטרוים שראינו שמשפיעים, האם זה נובע מהתרגום. אנחנו רוים לבחון במשימת התרגום. 

%we compare the performance of the "all" strategy 
%We aim to provide the factors influencing translation quality and its impact on performance. Understanding these factors is essential since translation is a fundamental component in the  \emph{selective pre-translation} approach.

\subsection{Experimental Setup}
% \paragraph{Goal} 

% \paragraph{Models} 

% To evaluate the translation quality we used two translator engines: \emph{Google Translate API}\footnote{We used the open-source library 
% \url{https://pypi.org/project/easygoogletranslate/}.} and \emph{Bing Translator API} over Azure. 

\paragraph{Factors Affecting Translation Quality}
We evaluated Google Translate performance  on the
%\reut{of what? which translator? and is it the one used in sec 4?} 
FLORES-200 validation set \citep{guzman-etal-2019-flores}, analyzing 91 languages across all resource levels, each with 997 sentences paired with  their  English translations. We compared machine translations to human-generated references using: (1) n-gram matching metrics -- Meteor \citep{banerjee2005meteor}, ROUGE \citep{lin2004rouge}, and BLEU \citep{papineni2002bleu}, and (2) neural network-based evaluation metrics -- BertScore \citep{zhang2019bertscore} and Comet \citep{rei2020comet}. Additional experiments using other translation models are in Appendix \ref{appendix:mt_engines_comparsion}.


\paragraph{Impact on Downstream tasks}
We used the QA dataset XQuAD, with 300 parallel sentences in English and other languages.
% with parallel splits in English and other languages, each containing 300 sentences.
We compared the zero-shot \emph{pre-translation} using output in the source language with the \emph{selective pre-transaltion} optimal approach using GPT-3.5-Turbo (0125).




% \paragraph{Data} For determining the quality of languages we evaluated translation quality using the FLORES-200 validation set \citep{guzman-etal-2019-flores}, focusing on 91 languages from all resouce levels, each with 997 sentences translated to English.\footnote{All the selected languages supported by Google Translate \url{https://cloud.google.com/translate/docs/}} For the second experiment, we used the XQuAD dataset for the QA task, with parallel splits in English and other languages, each containing 300 sentences. We examined the \emph{pre-translation} approach in a zero-shot setting with English output.


% \paragraph{Evaluation}
% We compared machine translations to human-generated references using: (1) n-gram matching metrics -- Meteor \citep{banerjee2005meteor}, ROUGE \citep{lin2004rouge}, and BLEU \citep{papineni2002bleu}, and (2) neural network-based evaluation metrics -- BertScore \citep{zhang2019bertscore} and Comet \citep{rei2020comet} using Google Translate. Additional experiments using other translation models are in Appendix \ref{appendix:mt_engines_comparsion}.



 % We evaluate translation quality by translating each sentence using both Google and Bing translation engines \footnote{Appendix \ref{appendix:mt_engines_comparsion} provides additional analysis and a full breakdown of the comparative finding}.



\subsection{Results}
\label{chat5_results}


\paragraph{Factors Affecting Translation Quality}

 
 % To determine the impact of the size of a language in the pre-trained data, we used the data ratio of the language in the GPT-3 unlabeled pre-training data. 
Our analysis focuses on the factors influencing model performance discussed in Sec.~\ref{chap_4} and we explore their impact on translation quality. We focus on two factors: \textbf{\em language resource levels (high/low)} and \textbf{\em linguistic similarity to English}. First, we found that the average quality for high-resource languages was higher compared to low-resource languages (0.75 vs 0.73), although correlation between resource level and quality wasn't significant. As for   {\em linguistic similarity to English} we used the pre-computed linguistic similarities from the URIEL dataset \citep{littell2017uriel}\footnote{\url{https://github.com/antonisa/lang2vec}} and calculated the correlation between syntactic similarity to English and the translation quality for each language. Figure \ref{fig:syntactic_similarity_translation} shows a positive correlation (coefficient = 0.33, p-value = 0.01) between syntactic similarity to English and translation quality (ROUGE-1). This correlation is particularly strong for high-resource languages (coefficient=0.73, p-value=0.004). Additional correlation results are detailed in Appendix \ref{appendix:linguistic_similarity_to_english_correlation}.


\paragraph{Impact On Downstream Tasks} 


% We use the XQuAD dataset for the QA task, which includes parallel splits for English and multiple other languages. Each language subset consists of 300 sentences. We use the English subset of XQuAD as the reference translation and evaluate translation quality using BertScore. We examine the \emph{pre-translation} approach in a zero-shot manner with English output. In this method, the prompt is completely identical for all languages, so the only factor influencing the results should be the translation quality from the source language to English. To analyze the impact of translation quality on final performance, we plot the correlation between accuracy scores and BertScore for each language in Figure 4. The results show that even though the prompts were identical for all models/languages, higher translation quality (BertScore) generally leads to better task performance, highlighting the importance of an effective translation system.

% We used the XQuAD dataset for the QA task, with parallel splits for English and other languages, each containing 300 sentences. We examined the \emph{pre-translation} approach in a zero-shot manner with English output. Since prompts are identical across languages, translation quality from the source language to English is the key variable. To analyze the impact of translation quality on final performance, we plot the correlation between accuracy score and translation quality (BERTScore) for each language in Figure \ref{fig:bleu_f1}. The results show that even though the identical prompts, higher translation quality generally leads to better task performance. The overall Pearson correlation is 0.233 (p-value < 0.001) for the GPT model, whereas using selective pre-translation (the highest-performing prompting configuration for each language) showed no correlation. 


% We used the XQuAD dataset for the QA task, with parallel splits in English and other languages, each containing 300 sentences. We examined the \emph{pre-translation} approach in a zero-shot setting with English output. 
To isolate translation quality as the sole direct factor influencing model performance, we translated the entire prompt into English. Since the original input differed only in language, not content, any variation in the processed input can be attributed solely to the quality of the translation. This approach %eliminates the direct impact of factors such as language resource size and similarity to English, allowing 
allows us to directly measure the correlation between translation quality and model performance across various tasks. 
%
Figure \ref{fig:bleu_f1} shows the correlation between translation quality (BERTScore) and model performance (accuracy) for each language. Our results show that {\em higher translation quality goes hand in hand with improved task performance}. The overall Pearson correlation is 0.233 (p < 0.001). 
However, when assessing the  same tasks with  \emph{selective pre-translation} 
instead of a \emph{completely pre-translated} prompt, we found a low correlation of 0.05 (p< 0.05) between the translation quality and  task performance, while  selective pre-translation outperforms the fully translated prompt. 
This disparity shows that the \emph{selective pre-translation} method effectively neutralizes translation issues.  By strategically choosing which prompt components to translate, we can make pre-translation useful for languages with lower translation quality.


In sum, our findings demonstrate that factors influencing downstream tasks, such as high resource level and  similarity to English, are positively correlated with translation quality. We further show that  \emph{selective pre-translation} can mitigate the negative effects of poor translation quality. These two findings underscore the importance of investing in high-quality translation, and on the other hand, prioritizing the \emph{selective pre-translation} approach in languages where machine translation is sub-optimal.







% The effectiveness of \emph{selective pre-translation} prompts in mitigating translation impact of performance, together with  



% These findings, combined with our findings that factors influencing downstream task performance also correlate with translation quality, indicate that translation quality significantly impacts task performance. 



% It underscores the importance of investing in high-quality translation systems, and 
% prioritizing our \emph{selective pre-translation} approach for languages with sub-optimal machine translation tools. 






% For each sentence, we calculated the translation score by translating the context into English and then computed the F1 score. Our analysis revealed a weak positive Pearson correlation of 0.233 (p-value<0.001) for the GPT-3.5 Turbo model. However, further research is needed to validate these findings on additional datasets and tasks.








% This correlation analysis
% aims to model the linear relationship between language proximity and machine translation scores


% Figure \ref{fig:syntactic_similarity_translation} displays the relationship between syntactic similarity to English and translation performance
% for all resource classes. The figure demonstrates that there is a significant correlation between the translation quality measured by ROUGE to the syntactic similarity of the language to English, which is the highest for high-resource languages with a coefficient of 0.73 and a p-value of 0.004 as illustrated in Figure \ref{fig:syntactic_similarity_translation}. 
% Appendix \ref{appendix:linguistic_similarity_to_english_correlation} provides the full results of the correlation. 





% as presented in Figure \ref{fig:syntactic_similarity_translation} there is a positive linear regression line which indicates an upward Trend.



% The results in Table \ref{tab:correlaton_subsets} show that there is a high correlation with a coefficient of -0.73 and a p-value of 0.004 between the high-resource language class to syntactic similarity to English. Full results are represented in Table \ref{tab:correlaton_subsets} in the Appendix.   



% The correlation with the High languages is the strongest, with a coefficient of -0.73 and a p-value of 0.004. The negative correlation indicates that languages with smaller linguistic distances (higher similarity) to English tend to achieve better machine translation to English.




% % Our results in Table \ref{tab:correlaton_subsets} show that there is a high negative correlation between the syntactic feature to the High and Medium resource groups but not to the Low Resource group. A negative correlation between linguistic distance and MT scores would imply that the closer (i.e., the smaller the linguistic distance) a language is to English, the higher the MT score is likely to be.

% % Also, as presented in Figure \ref{fig:syntactic_similrity_translation} there is a positive trend line which 

% \textbf{iii) Is there any correlation between the language translation score and the task performance?} To address this inquiry, we utilized the XQuAD \citep{artetxe2019cross} dataset, which comprises parallel splits for English and various other languages. Each language subset consists of 200 sentences. For every sentence, we computed the translation score (when translating the input into English) against the input from the English split and then calculated the F1 score. Our analysis revealed a weak positive Pearson correlation of 0.233 with a p-value of 7.287e-11 for the GPT-3.5 turbo. 



\section{Related Work}


%\subsection{The Rise of Multilingual LLMs}

With over 7,000 languages spoken globally \citep{anderson2010many},   the growing use of diverse languages have fueled the demand for multilingual LLMs. Progress in this field stems from two primary efforts: (1) developing dedicated monolingual models for low-to-medium-resource languages \citep{seker2022alephbert, cui2023efficient, andersland2024amharic}, and (2) creating multilingual LLMs with pre-trained data encompassing multiple languages  \citep{qin2024multilingual, jiang2024mixtral}. 

The ability of Multilingual LLMs to operate in % and transfer knowledge between,
different languages  \citep{raffel2020exploring, conneau2019unsupervised, chowdhery2023palm} comes from two sources: (1)  training or fine-tuning on multilingual data in order to achieve multilingual proficiency
\citep{xue2020mt5, chen2021zero, le2023bloom, shaham2024multilingual, muennighoff2022crosslingual}, and (2) utilizing prompting techniques to harness the model's inherent multilingual capabilities without modifying parameters during inference \citep{brown2020language}. This latter approach has gained popularity due to its efficiency and applicability to a wider range of use cases.


%Following these developments, benchmarks for evaluating LLMs have been proposed to measure cross-lingual transfer, including for low-resource languages \citep{hu2020xtreme, liang2020xglue} and benchmarks focusing on specific language families such as Indian languages  \citep{kakwani2020indicnlpsuite} and African languages \citep{ogundepo2023afriqa}. 





%\subsection{Multilingual Prompting Approaches}

For the latter, to improve the multilingual capabilities of  LLMs
researchers
%\reut{if this pertains to item (2) above, say it}
  developed various prompting methods. \citet{huang2023not} introduced XLT, a cross-lingual prompt that directs LLMs to function as experts in a specific language through a process involving problem-solving and cross-lingual thinking. \citet{zhao2021discrete} employed discrete and soft prompting techniques and showed that few-shot non-English prompts outperform finetuning in cross-lingual transfer. \citet{shi2022language} found that chain-of-thought (CoT) prompts lead to multilingual reasoning abilities in LLMs, even in under-represented languages. Another strategy is \emph{pre-transaltion} which translates the entire prompt to English \citep{chowdhery2023palm, qin2023chatgpt, ahuja2023mega}. A more nuanced approach, \emph{selective pre-translation}, translates part of the prompt into English, for instance, \citet{liu2024translation} translated only the instruction, and \citet{ahuja2023mega} translated the few shot examples. While
these use cases lack a systematic research foundation, in this study, we systematically study pre-translation configurations to provide evidence-based recommendations for optimal use.
%\reut{this background section feels redundant  and breaks the flow- it should be moved to after section 5 and be called "related work". Also, for works you mention in 2.2. in this related work section need to state how they are similar or different than this paper }



\section{Conclusion}

In this work we formalize and comprehensively assess selective pre-translation prompting strategies for LLMs in multilingual settings. With four tasks, six dataset collections, three models, and 35 languages, we deliver the first systematic evaluation, to our knowledge, of all existing prompt configurations of pre-translation. We demonstrate that \emph{selective pre-translation}  consistently outperforms both \emph{pre-translation} of the entire prompt and \emph{diret-inference} in the source language, establishing the efficacy of \emph{selective pre-translation} in both the high- and low-resource cases. Additionally, we show 
that translation quality significantly
affects  performance and that 
selective pre-translation can mitigate the negative effects of suboptimal translations.



\section*{Limitations}

\paragraph{Subset of LLMs}
This study aims to systematically assess the effectiveness of various prompting strategies across different tasks and LLMs. Due to resource limitations, it was not possible to evaluate more advanced models such as GPT-4 or GPT-4o. However, we endeavored to cover several LLMs representing different architectures. Additionally, the choice of Bloom as our multilingual model is based on previous works \cite{bawden2023investigating, nezhad2024drives}. We make our evaluation framework, code, configurations, and execution pipeline, for open public use, allowing to extend the investigation to more and newer models.

\paragraph{LLM Adherence and Impact on the Output}

In our evaluation, we attempted to influence the output by instructing the model to generate a response in a specific language. However, the model occasionally did not follow these instructions, producing output in a different language, which could impact the results. Appendix \ref{appendix:error_analysis} provides error analysis of the various issues we encountered.  

\paragraph{Evaluation}
Metrics based on n-gram matching, such as ROUGE \citep{lin2004rouge}, are commonly used for evaluating summarization quality in English. However, these metrics can be problematic when applied to morphologically rich languages (MRL) such as Persian, which have more flexible word order compared to English. Additionally, their morphological richness means that the same concept can be expressed in multiple ways due to variations in prefixes, suffixes, and root conjugations.

\paragraph{Translation Quality's Impact on Downstream tasks}

Our analysis of the impact of translation quality impact on downstream tasks in section \ref{chat5_results} was constrained by the scarcity of datasets with parallel splits for English and other languages, limiting our evaluation to the QA task.  
Future research should incorporate a wider array of datasets and tasks to validate and expand upon our findings.

\paragraph{Pretrained Data Distribution Details}

In our experiments, we evaluated four models and grouped the languages based on GPT-3's pre-training data distribution information. Ideally, we would split the languages according to each model's data distribution. However, to our knowledge, only GPT-3's pre-training data distribution is publicly shared. Explicitely testing different language  distributions is desired but resource intensive, and is left for future research.



\section*{Acknowledgements}
This research has been funded by a grant from the Israel Science Foundation (ISF) grant number 670/23 as well as a grant from the Israeli Ministry of Science and Technology (MOST), and a KAMIN grant from the Israeli Innovation Authority, for which we are grateful. We are further grateful for a generous VATAT grant to the BIU NLP team which contributed resources for computation, annotation  and human evaluation in this project.

% \section*{Acknowledgements}


\bibliography{anthology}
\bibliographystyle{acl_natbib}

\appendix
\label{appendix:data_collection}



\begin{figure*}[ht]
    \centering
    \subfloat[<English,English,English,English>\label{fig:syntactic_heatmaps_1}]{{\includegraphics[width=0.32\textwidth]{english_english_english_english.png} }}%
    \hfill
    \subfloat[<German,English,German,English>\label{fig:family_langs_1}]{{\includegraphics[width=0.32\textwidth]{german_english_german_english.png} }}%
    \hfill
    \subfloat[<German,German,English,English>\label{fig:syntactic_heatmaps_2}]{{\includegraphics[width=0.32\textwidth]{german_german_english_english.png} }}%
    \caption{Examples of 3 configurations of German. Each configuration is in the following format {\em <Instruction,Context,Examples,Output>}}.%
    \label{fig:prompting_format}%
\end{figure*}




% \begin{figure*}[t]
%     \centering
%     \includegraphics[width=1.0\linewidth, height=0.1\textheight]{improvement_over_english.png}
%     \caption{Percentage of improvement over \emph{pre-translation} configuration, when using the highest configuration for each languages/task For GPT-3.5-Turbo. The bars are color-coded based on the norm of the log of the number of tokens in the pre-trained data of GPT-3, as elaborated in Table \ref{tab:languages_classes}}
%     \label{fig:improvement_over_english}
% \end{figure*}



\begin{table}[]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{llll}
\multicolumn{4}{c}{\textbf{All Configurations}}                                                                                                              \\
\multicolumn{1}{c}{\textit{Instruction}} & \multicolumn{1}{c}{\textit{Context}} & \multicolumn{1}{c}{\textit{Examples}} & \multicolumn{1}{c}{\textit{Output}} \\ \hline
Source                                   & Source                               & -                                    & English                             \\
English                                  & Source                               & -                                    & English                             \\
Source                                   & Source                               & Source                               & English                             \\
English                                  & English                              & English                              & Source                              \\
Source                                   & English                              & Source                               & English                             \\
English                                  & Source                               & Source                               & Source                              \\
English                                  & Source                               & -                                    & Source                              \\
Source                                   & Source                               & Source                               & Source                              \\
English                                  & Source                               & English                              & Source                              \\
Source                                   & Source                               & -                                    & Source                              \\
English                                  & English                              & Source                               & Source                              \\
English                                  & Source                               & English                              & English                             \\
English                                  & English                              & -                                    & English                             \\
Source                                   & English                              & -                                    & English                             \\
Source                                   & English                              & -                                    & Source                              \\
Source                                   & Source                               & English                              & English                             \\
English                                  & English                              & -                                    & Source                              \\
English                                  & Source                               & Source                               & English                             \\
Source                                   & Source                               & English                              & Source                              \\
Source                                   & English                              & Source                               & Source                              \\
English                                  & English                              & English                              & English                             \\
Source                                   & English                              & English                              & English                             \\
English                                  & English                              & Source                               & English                             \\
Source                                   & English                              & English                              & Source                              \\ \hline
\end{tabular}}
\caption{All Valid Configurations (24)}
\label{tab:all_configurations}
\end{table}




\section{Selective Pre-Translation Evaluation}


\subsection{Experimental Setup}
\label{appendix:modular_prompting_experimental_setup}

\paragraph{Models}
\label{appendix:models_platforms} 
To query GPT-3.5-turbo (0125), we used the Azure platform via the API\footnote{\url{https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models}}. For Mixtral-8x7B-287 Instruct-v0.1, we utilized the API platform provided by Together.ai\footnote{\url{https://www.together.ai/}}. For Gemini-1.0-pro, we accessed the API through Google AI Studio\footnote{\url{https://aistudio.google.com}}. Lastly, for bloomz-7b1-mt, we used deployed the model on Hugging Face\footnote{\url{https://huggingface.co/bigscience/bloomz-7b1-mt}}


\paragraph{Prompt Creation}
For constructing the prompts we used the LangChain library\footnote{\url{https://pypi.org/project/langchain/}} which enables us to build and validate prompts dynamically for both zero-shot and few-shot templates. For creating the instructions, we initially used ChatGPT to generate them and then fine-tuned them based on quality analysis from our experiments. 


\paragraph{Normalization And Formatting}
Before evaluation, we normalized the model's output, with each task following a unique normalization process. For the QA task, for example, we converted the text to lowercase and removed punctuation, articles, and extra whitespace. In the Summarization task, we removed prefixes like 'The Summary:'. For the NER task, we converted the model's output into a list of tuples, each in the format (Tag, Entity). After normalization, additional formatting was applied if necessary. For instance, in the NER task, we transformed the normalized output into a list in the BIOSE format, identifying the entities in the original sentence and converting each entity prediction to its correct format based on its position (e.g., B-ORG for the first entity tagged as 'ORG').
\label{appendix: normalization_and_formatting}


\subsection{Configuration Format}

We define a specific {\em selective pre-translation configuration} as \( C_{i} = \langle \text{I}^l, \text{X}^l, E^{l}_{n}, O^l \rangle \), \( n \geq 0 \), \( l \in \{ e, s \} \). Each configuration contains 4 components: instruction, context, examples, and output. Figure \ref{fig:prompting_format} displays examples for 3 configurations in the German language. See Table \ref{tab:all_configurations} for a list of all the configurations.
\label{appendix:configuration_format}

\paragraph{Python Libraries In Use}
For evaluation of the different models, we used the most common ROUGE package for non-English papers\footnote{\url{https://github.com/csebuetnlp/xl-sum/tree/master/multilingual_rouge_scoring}}. for loading and processing the data, we used NumPy\footnote{\url{https://pypi.org/project/numpy/}}
For help with writing the code, we used assistance from ChatGPT. 
 



% \paragraph{Pre-Trained Data Class Categorization} 

% Table \ref{tab:languages_classes} includes our categorization of languages into four classes, differentiated by their data ratios in the GPT-3 pre-trained data. We chose the pre-trained data of GPT-3 due to its extensive coverage and diverse language representation, enabling us to categorize the languages into classes based on their data ratios. To determine the subsets, we utilized the class division proposed by \citet{lai2023chatgpt} and modified class D to include only the languages that are unrepresented in the GPT-3 pre-trained data. To our knowledge, we are the first to specifically reference this subset of languages. Notably, other divisions exist; for example, \citet{joshi2020state} proposed dividing languages into classes based on the number of speakers. However, this approach does not faithfully represent language diversity in LLMs, which are more influenced by the language's data availability than by how well it is spoken among people.
% \label{appendix:languages_categorization}



\subsection{Analysis Methods}

\subsubsection{Rule Association And Apriori Algorithm}
\label{appendix:rule_association_recap_implementations}
Association rule mining, one of the most important and well-researched techniques of data mining, was first introduced by \citet{agrawal1993mining}. It aims to extract interesting correlations, frequent patterns, associations, or casual
structures among sets of items in the transaction databases or other data repositories. 

\paragraph{Apriori algorithm}
The Apriori algorithm is a popular approach for mining association rules. It works by identifying frequent itemsets, which are groups of items that appear together in a dataset with a frequency above a specified threshold. The algorithm then generates association rules from these frequent itemsets, highlighting the likelihood of one item being present given the presence of another item. Apriori uses a bottom-up approach, gradually building larger itemsets from smaller ones while pruning those that do not meet the minimum support threshold.

In our analysis, we reported the following measures: 
(i) \textbf{Support}: $s(X) = \frac{\sigma(X)}{N}$, where $\sigma(X)$ is the number of transactions in which X appears  and $N$ is the total number of transactions.

(ii) \textbf{Confidence}: $c(X \rightarrow Y) = \frac{\sigma(X \cup Y)}{\sigma(X)}$, measures the probability of occurrence of itemset $Y$ with itemset $X$.

\label{tab:apriori_technical_into}

\paragraph{Implementation Details}

To implement the Rule Association algorithm, we created a DataFrame for each task's results using pandas DataFrames\footnote{\url{https://pypi.org/project/pandas/}}. Each DataFrame contains the results for all the configurations for every language. Subsequently, we binned each score column into three bins - high, medium, and low, based on the 30th and 60th percentiles. Later, we merged all the data frames based on the configuration name. Then we used the apriori algorithm from the efficient-apriori\footnote{\url{https://pypi.org/project/efficient-apriori/}} library, which produces two outputs - itemsets and rules. Later, we filtered weak rules (support > 0.05 \& confidence > 0.75).
% \label{tab:apriori_appendix}
\label{appendix:rule_association}



\begin{figure*}[]
    \centering
    \subfloat[QA\label{fig:syntactic_heatmaps_1}]{{\includegraphics[width=0.23\textwidth]{zero_shot_few_shot/zero_shot_qa_few_shot.png} }}%
    \hfill
    \subfloat[NER\label{fig:family_langs_1}]{{\includegraphics[width=0.23\textwidth]{zero_shot_few_shot/zero_shot_ner_few_shot.png} }}%
    \hfill
    \subfloat[NLI\label{fig:syntactic_heatmaps_2}]{{\includegraphics[width=0.23\textwidth]{zero_shot_few_shot/zero_shot_xnli_few_shot.png} }}%
    \hfill
    \subfloat[Summarization\label{fig:family_langs_2}]{{\includegraphics[width=0.23\textwidth]{zero_shot_few_shot/zero_shot_sum_few_shot.png} }}%
    \caption{Few-Shot and Zero-Shot Performance Gap (Few-Shot - Zero-Shot) for each task/language.}%
    \label{fig:zero_shot_few_shot}%
\end{figure*}


% \begin{table}[!ht]
% \centering
% \scalebox{0.9}{
% \begin{tabular}{lccc}
% \Xhline{6\arrayrulewidth}
% \textbf{Model}        & \textbf{QA} & \textbf{NER} & \textbf{Summarizatin} \\ \hline
% {GPT}         &  56  & {60} & {96} \\
% {Mixtral}         & {60} & {61} & {78} \\
% {Gemini} & {63} & {61} & {96} \\
% \end{tabular}
% }
% \caption{Percentage of success of expected output languages for each model/task}
% \label{tab:output_error}
% \end{table}


\begin{figure*}[]
    \centering
    \subfloat[QA\label{fig:otuput_qa}]{{\includegraphics[width=0.23\textwidth]{img/english_output_target_output/english_output_qa_target_output.png} }}%
    \hfill
    \subfloat[NER\label{fig:family_langs_1}]{{\includegraphics[width=0.23\textwidth]{img/english_output_target_output/english_output_ner_target_output.png} }}%
    \hfill
    \subfloat[Summarization\label{fig:family_langs_2}]{{\includegraphics[width=0.23\textwidth]{img/english_output_target_output/english_output_sum_target_output.png} }}%
    \caption{Output Performance Gap (English - Source) for each task/language}%
    \label{fig:output_english_output_target}
\end{figure*}






\subsection{Prompting}

\label{appendix:prompts}

\paragraph{Question Answering}

Answer the following <Question> based only on the given <Context>. Follow these instructions:
\begin{itemize}
    \item Include only words from the given context in your answer.
    \item Keep the answer as short as possible.
    \item Provide the answer in \textit{expected output language}.
\end{itemize}


\paragraph{Named Entity Recognition}

You are an NLP assistant whose purpose is to perform Named Entity Recognition (NER). 
You need to assign each entity a tag from the following:
\begin{enumerate}
    \item PER means a person.
    \item ORG means an organization.
    \item LOC means a location entity.
\end{enumerate}
The output should be a list of tuples in the format:
\[
[(\text{Tag}, \text{Entity}), (\text{Tag}, \text{Entity})]
\]
for each entity in the sentence. The entities should be in the \textit{expected output language}.

\paragraph{Summarization}

Write a summary of the given <Text>
The output should be in \textit{expected output language}.
The output must be up to 2 sentences maximum.
 

\paragraph{Natural Language Inference}
You are an NLP assistant whose purpose is to solve Natural Language Inference (NLI) problems.
NLI is the task of determining the inference relation between two texts: entailment, contradiction, or neutral.
Your answer should be one word from the following: entailment, contradiction, or neutral.


% \paragraph{Improvement Over Pre-Translation Configuration}

% Figure \ref{fig:improvement_over_english} illustrates the improvement percentage over \emph{pre-translation}.
% The figure reveals that using \emph{Selective Pre-Translation} has a positive impact on more than 90\% of the selected languages. In addition, this approach has a greater positive impact on languages with smaller pre-training data. 
% % Consequently, both figures highlight that low-resource languages require a more complex configuration, which combines both the source and English languages. This superiority is reflected in extremely low-resource languages (red bars), which demonstrate greater improvement over other resource classes.

% \label{appendix:improvement_over_base_configurations}


\subsubsection{The Holy Grail of Optimal Configuration}
\label{appendix:optimal_configuration}

\paragraph{Few-Shot Examples Impact}

Figure \ref{fig:zero_shot_few_shot} demonstrates that for all tasks, using a few-shot setting over a zero-shot setting yields better results. Interestingly, For all tasks, except for NLI, high-resource languages achieved better improvement when considering a few-shot setting over low-resource languages. 


\paragraph{Output Selection Effects}

Figure \ref{fig:output_english_output_target} demonstrates that while in extractive QA the output should be in the source language, and in the summarization task, the output should be in English; in NER, the output is ambiguous.

\label{appendix:output_english_output_target_appendix}




\begin{figure*}[]
    \centering
    \includegraphics[width=0.8\linewidth, height=0.11\textheight]{family_langs.png}
    \caption{Percentage improvement over \emph{pre-translation} approach, when using the highest configuration for each task For GPT-3.5-Turbo. The bars are color-coded based on the language family script.}
    \label{fig:family_scripts}
\end{figure*}

\begin{CJK*}{UTF8}{gbsn}


\begin{table*}[h]
\centering
\scalebox{0.4}{
\begin{tabular}{lllll}
\multicolumn{1}{c}{\textit{\textbf{Task}}} & \multicolumn{1}{c}{\textit{\textbf{Model Output}}}                                        & \multicolumn{1}{c}{\textit{\textbf{Expected Output}}}  & \multicolumn{1}{c}{\textit{\textbf{Explnation}}}                                                               & \multicolumn{1}{c}{\textit{\textbf{Phenoenen}}} \\ \hline
\multirow{5}{*}{\textbf{NER}}              & {[} 'LOC: 新 北 市', 'LOC: 平 溪 區' {]}                                                        & {[}(LOC, '新 北 市',), (LOC, '平 溪 區',){]}                 & List of strings, instead of list of tuples.                                                                    & \textbf{Format Inconsistency}                   \\ \cline{2-5} 
                                           & {[}PER: Hiei{]}\textbackslash n- {[}PER: Hinata{]}\textbackslash n                       & {[}'PER', 'Hiei'), ('PER', 'Hinata'){]}                & New line between each entity.                                                              & \textbf{Format Inconsistency}                   \\ \cline{2-5} 
                                           & Ner Tags: {[}'PER: LL Cool J'{]}                                                          & {[}(PER: LL Cool J){]}                                 & Redundant words in the beginning.                                                                              & \textbf{Extraneous information}                \\ \cline{2-5} 
                                           & {[}{]} (No entities found in the sentence)                                                & {[}{]}                                                 & Redundant words in the end.                                                                                    & \textbf{Extraneous information}                \\ \cline{2-5} 
                                           & Since the last sentence is in English, I will provide the NER tags in English as well     & {[}(PER: Давид IV Грузијски){]}                          & Refusing to output in the desired language.                                                                                 & \textbf{Unwarranted Refusal}                    \\ \hline
\multirow{2}{*}{\textbf{QA}}               & {[}The united states{]}                                                                   & The united states                                      & List of string instead of a string.                                                                            & \textbf{Format Inconsistency}                   \\ \cline{2-5} 
                                           & The question cannot be answered as the answer is not provided in the given context        & {[}Luke Kuechly{]}                                     & Insufficent information                                                                     & \textbf{Unwarranted Refusal}                    \\ \hline
\multirow{2}{*}{\textbf{NLI}}              & The second statement neutral because it does not provide any information that contradicts & neutral                                                & Unnecessary justification for the choice.                                                                      & \textbf{Extraneous information}                \\ \cline{2-5} 
                                           & vinculación                                                                               & entailment                                             & Spanish word for entailment instead of English.                                                                & \textbf{Wrong Language}                         \\ \hline
\textbf{Summarization}                     & Resumo: O ministro de Emergências da Rússia, Sergei Shoigu ...                            & O ministro de Emergências da Rússia, Sergei Shoigu ... & \begin{tabular}[c]{@{}l@{}}Redundant words ('Resumo' - Summary in Portuguese)\\  in the beginning.\end{tabular} & \textbf{Extraneous information}                \\ \hline
\end{tabular}%
}
\caption{Error analysis of unexpected model outputs and observed in various tasks/languages.}
\label{tab:qualitative_analysis}

\end{table*}

\end{CJK*}

\begin{table}[]
\centering
\scalebox{0.9}{
\begin{tabular}{lcccl }
\Xhline{6\arrayrulewidth}
\textbf{Model}        & \textbf{QA} & \textbf{NER} & \textbf{Summarization} \\ \hline
{GPT}         &  56  & {60} & {96} \\
{Mixtral}         & {60} & {61} & {78} \\
{Gemini} & {63} & {61} & {96} \\
\end{tabular}
}
\caption{Percentage of success of expected output languages for each model/task}
\label{tab:output_error}
\end{table}


\subsubsection{Factors Explaining Performance}

\subsubsection{Script Impact}
\label{appendix:script_impact}
Figure \ref{fig:family_scripts} presents the performance improvement achieved by the highest-performing prompt configuration among all configurations compared to the pre-translation prompt, for each language. Notably, the language family (as categorized by scripts) reveals a relatively even distribution of performance gains within the same language family. For example, languages using the Cyrillic script show greater improvement than those using the Latin script. 
Interestingly, languages in the same script family sometimes show varying results; for example, Spanish and Ewe belong have Latin script, but Ewe shows greater improvement over Spanish.


\paragraph{Linguistic Similarity To English}
We used the lang2vec\footnote{\url{https://github.com/antonisa/lang2vec}} library to obtain syntactic similarity scores for each language. The Pearson correlation was calculated based on two vectors: one representing language similarities (ranging from 0 to 1) and the other representing model performance scores for each language across tasks. This correlation was calculated at the instance level. Table \ref{tab:syntactic_heatmaps} shows a positive correlation between model performance and syntactic similarity to English, especially for the summarization task, indicating that syntactic similarity to English significantly improves performance in this task. Additionally, NER also exhibits positive correlations, suggesting that models can better identify and classify entities in languages that share syntactic features with English.
\label{appendix:factors_explaning_performance}



\section{Error Analysis}
\label{appendix:error_analysis}

\subsection{Format Issues}

Automatic evaluation requires consistent output formatting, 
especially in tasks like Named Entity Recognition (NER), which must adhere to a predefined format rather than free text. A common practice involves prompting the model to generate results in a specific format, such as a list of tuples representing entities and their types (e.g., \((Loc, New York City)\). However, achieving perfect consistency can be challenging. Models may not always adhere to the requested format, leading to difficulties in evaluation.


\paragraph{Qualitative Analysis}

We analyzed unexpected model outputs in various tasks and languages. For each task, we noted common phenomena observed and the expected model output. The results in Table \ref{tab:qualitative_analysis} reveal that for the NER task, due to its rigid format, the model exhibited many error types. The models showed phenomena such as format inconsistency and extraneous introduction, which require a more generative normalization method to handle. An interesting phenomenon that made our modular selective pre-translating approach difficult to implement is unwarranted refusal, where the model refuses to output in the required language.

\subsection{Incorrect Output Language }

Table \ref{tab:output_error} summarizes the percentage of accurately outputted language for all tasks (except NLI, due to its index-based format) across all models. The results reveal that in extractive tasks such as extractive QA and NER, where the output overlaps with the context, the model struggles the most to output in the desired language. However, in abstractive summarization, a generation task, the model had better success.






\begin{table}[]
\centering
\scalebox{0.9}{
\begin{tabular}{lcccl }
\Xhline{6\arrayrulewidth}
\textbf{Model}        & \textbf{QA} & \textbf{NER} & \textbf{Summarization} & \textbf{NLI} \\ \hline
GPT                                                                                       & 0.14*      & 0.28**      & 0.42**                & 0.01        \\
Mixtral                                                                                   & 0.13*      & 0.2**       & 0.31**                & 0.08         \\
Gemini                                                                                    & 0.1*       & 0.19**      & 0.25**                & 0.01        
\end{tabular}%
}
\caption{Pearson correlation between linguistic syntactic
similarity to English and task performance for GPT,
Mixtral, and Gemini. * p < 0.05, ** p < 0.01}
\label{tab:syntactic_heatmaps}
\end{table}





\section{Translation: Key to Pre-Translation}

\subsection{Machine Translation Engines Comparison}

% To assess which machine translation tool to use we used Google Translate API and Bing Translate. We didn't consider multilingual LLMs for machine translation  because we are based on the finding of \emph{zhu2023multilingual} which found that multilingual LLMS for MT still face a large gap towards the commercial translation system like Google Translate, especially on low-resource languages. The results in Table \ref{tab:google_vs_bing} and Figure \ref{fig:google_bing} demonstrate that Google Translate API outperformed Bing Translator in all the evaluated metrics, highlighting its high performance. Interestingly, the languages that achieved the highest scores are Welsh and Maltese which are both considered low-resource languages. 
% \label{appendix:mt_engines_comparsion}


To evaluate machine translation tools, we compared Google Translate API and Bing Translator. We excluded multilingual LLMs from consideration, as \emph{zhu2023multilingual} found that these models still lag behind commercial systems like Google Translate, especially for low-resource languages. As shown in Table \ref{tab:google_vs_bing} and Figure \ref{fig:google_bing}, Google Translate outperformed Bing Translator across all metrics, demonstrating superior performance. Notably, Welsh and Maltese, both low-resource languages, achieved the highest scores. \label{appendix:mt_engines_comparsion}.


\subsection{Linguistic Similarity To English}

The results in Table \ref{tab:correlation_subsets} demonstrate the correlation between the syntactic similarity to English of the language and the ROUGE translation score of the language. The results show that the most significant correlation was observed in languages belonging to the high-resource category, and this correlation decreases as the class of the language becomes low-resource.

\label{appendix:linguistic_similarity_to_english_correlation}

.
\section{Selective Pre-Translation Prompt Generator}
\label{appendix:hf_space}

We have launched a space on Hugging Face. The space makes it easy for the community to receive recommended configurations based on the type of task and language. In Figure \ref{fig:app}
, we can see an overview of the application and an example of a recommended configuration. Figures \ref{fig:zero_shot_app} and \ref{fig:few_shot_app} provide examples of generating prompts for zero-shot and few-shot settings.


\section{Detailed Results}
\label{appendix:detailed_results}
The results across all tasks, languages and models are included in our benchmarking exercise are
provided in Table \ref{tab:qa_quad} (for XQuAD), \ref{tab:qa_indicqa} (for indciQA), \ref{tab:wikiann} (for WikiANN), \ref{tab:masakhaner} (for MasakhNER), \ref{tab:xlsum} (for XL-Sum), \ref{tab:xnli} (for XNLI).
The result of the correlation for Gemini are included in Table \ref{tab:gemini_correlation}, for Mixtral in Table \ref{tab:mixtral_correlation}, and for bloomz in Table \ref{tab:bloomz_correlation}
\label{appendix:detailed_results}





\begin{figure*}[th]
    \centering
    \scalebox{1}{
    \includegraphics[width=\textwidth]{app.png}
    }
   \caption{\emph{Recomended Configuration:} Overview of our application. From top to bottom: Task Details—general configuration about the task (Task, Language, Model). Clicking on "Recommended Configuration" provides a suggested selective pre-translation configuration.}

    % Bar colors correspond to the log-normalized number of tokens in GPT-3 pre-training data.
    
    \label{fig:app}
\end{figure*}


\begin{figure*}[th]
    \centering
    \scalebox{1}{
    \includegraphics[width=\textwidth]{zero_shot_app.png}
    }
   \caption{\emph{Generating selective pre-translation prompt for zero-shot:}  The user needs to configure the instruction (optional) and the languages for the components under "Language Component Selection": instruction, context, examples, and output. Additionally, under "Prompt Input Data," the user must configure the relevant input data or task, such as the question and context for QA in this example. Clicking on "Generate Prompt" provides a zero-shot pre-translation prompt}

    % Bar colors correspond to the log-normalized number of tokens in GPT-3 pre-training data.
    
    \label{fig:zero_shot_app}
\end{figure*}





\begin{figure*}[th]
    \centering
    \scalebox{1}{
    \includegraphics[width=\textwidth]{few_shot_app.png}
    }
   \caption{\emph{Generating selective pre-translation prompt for few-shot:} Here, the user must also configure the few-shot settings: the dataset to use (from which the few-shot examples are taken) and the number of examples to use (default = 1).}
    \label{fig:few_shot_app}
\end{figure*}




\begin{figure*}[t]
    \centering
    \subfloat[BLEU\label{fig:otuput_qa}]{{\includegraphics[width=1\textwidth]{google_bing_pics/google_bing_bleu.png} }}%
    \hfill
    \subfloat[ROUGE\label{fig:family_langs_1}]{{\includegraphics[width=1\textwidth]{google_bing_pics/google_bing_rouge.png} }}%
    \hfill
    \subfloat[Meteor\label{fig:family_langs_2}]{{\includegraphics[width=1\textwidth]{google_bing_pics/google_bing_meteor.png} }}%
    \caption{Google Translate API vs Bing Translator Comparsion}%
    \label{fig:google_bing}
\end{figure*}





% Both results highlight that low-resource languages require a more complex configuration.





%  greater positiv

% configuration - Source language and English. 


% pre-translation has a greater positive impact on languages with smaller pre-training data.'



% over the source prompt configuration and shows that low-resource languages tend to achieve higher improvement over high-resource languages. Interestingly, Both results highlight that low-resource languages require a more complex configuration.



% \begin{figure*}[]
%     \centering
%     \includegraphics[width=1.0\linewidth, height=0.1\textheight]{sytactic_english.png}
%     \caption{Relative percentage improvement over English-translate prompting, when using the highest configuration for each task For GPT-3.5-Turbo. The bars are color-coded based on the syntactic distance to English}
%     \label{fig:sytactic_english}
% \end{figure*}



% \begin{figure*}[]
%     \centering
%     \includegraphics[width=1.0\linewidth, height=0.1\textheight]{sytactic_monolingual.png}
%     \caption{Relative percentage improvement over English-translate prompting, when using the highest configuration for each task For GPT-3.5-Turbo. The bars are color-coded based on the syntactic distance to English}
%     \label{fig:sytactic_mono}
% \end{figure*}



% \begin{figure*}[]
%     \centering
%     \includegraphics[width=1.0\linewidth, height=0.1\textheight]{mongo_english.png}
%     \caption{Relative percentage improvement over English-translate prompting, when using the highest configuration for each task For GPT-3.5-Turbo. The bars are color-coded based on the phonological distance to English}
%     \label{fig:phono_english}
% \end{figure*}







\begin{table}[]
\centering
\scalebox{0.7}{
\begin{tabular}{lcccl }
\Xhline{3\arrayrulewidth}
\textbf{Group}        & \textbf{Pearson Correlation} & \textbf{P-value} \\ \hline
{High Resource}         &  0.73  & {0.05}                      \\
{Medium Resource}         & {0.48}                          & {0.07}                      \\
{Low Resource} & {0.06}                           & {0.78}                      \\
{Extremly Low Resource} & {-0.34}                           & {0.30}                      \\

\end{tabular}
}
\caption{Correlation between syntactic similarity to English and the ROUGE score (by language subset).}
\label{tab:correlation_subsets}

\end{table}








\begin{table}[]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{llll|lll}
                        & \multicolumn{3}{c|}{\textbf{\underline{Google Translate API}}} & \multicolumn{3}{c}{\textbf{\underline{Bing Translator}}}       \\
Language                & \textit{\textbf{ROUGE}} & \textit{\textbf{Meteor}} & \textit{\textbf{BLEU}} & \textit{\textbf{ROUGE}} & \textit{\textbf{Meteor}} & \textit{\textbf{BLEU}} \\
Welsh                   & 0.86                    & 0.86                     & 0.63                   & 0.85                    & 0.85                     & 0.61                   \\
Maltese                 & 0.84                    & 0.83                     & 0.59                   & 0.83                    & 0.82                     & 0.56                   \\
Danish                  & 0.81                    & 0.79                     & 0.51                   & 0.81                    & 0.79                     & 0.49                   \\
Swedish                 & 0.81                    & 0.80                     & 0.51                   & 0.80                    & 0.79                     & 0.51                   \\
Portuguese              & 0.81                    & 0.79                     & 0.52                   & 0.80                    & 0.78                     & 0.50                   \\
Catalan                 & 0.80                    & 0.79                     & 0.49                   & 0.78                    & 0.77                     & 0.45                   \\
Spanish                 & 0.79                    & 0.64                     & 0.30                   & 0.69                    & 0.64                     & 0.30                   \\
Serbian                 & 0.79                    & 0.77                     & 0.48                   & 0.03                    & 0.07                     & 0.01                   \\
Bulgarian               & 0.79                    & 0.77                     & 0.45                   & 0.75                    & 0.72                     & 0.37                   \\
French                  & 0.79                    & 0.77                     & 0.48                   & 0.78                    & 0.76                     & 0.48                   \\
Nepali (macrolanguage)  & 0.79                    & 0.78                     & 0.46                   & 0.74                    & 0.72                     & 0.38                   \\
Macedonian              & 0.78                    & 0.77                     & 0.46                   & 0.72                    & 0.70                     & 0.35                   \\
Swahili (macrolanguage) & 0.78                    & 0.79                     & 0.51                   & 0.74                    & 0.74                     & 0.43                   \\
Hebrew                  & 0.78                    & 0.77                     & 0.47                   & 0.76                    & 0.75                     & 0.44                   \\
German                  & 0.78                    & 0.76                     & 0.46                   & 0.79                    & 0.76                     & 0.46                   \\
Indonesian              & 0.78                    & 0.77                     & 0.46                   & 0.78                    & 0.77                     & 0.44                   \\
Romanian                & 0.78                    & 0.76                     & 0.45                   & 0.77                    & 0.75                     & 0.43                   \\
Panjabi                 & 0.78                    & 0.77                     & 0.46                   & 0.74                    & 0.72                     & 0.4                    \\
Bosnian                 & 0.78                    & 0.76                     & 0.45                   & 0.75                    & 0.73                     & 0.39                   \\
Hindi                   & 0.78                    & 0.76                     & 0.45                   & 0.76                    & 0.74                     & 0.41                   \\
Turkish                 & 0.77                    & 0.75                     & 0.43                   & 0.76                    & 0.73                     & 0.41                   \\
Armenian                & 0.77                    & 0.75                     & 0.43                   & 0.68                    & 0.65                     & 0.28                   \\
Irish                   & 0.77                    & 0.76                     & 0.47                   & 0.76                    & 0.74                     & 0.42                   \\
Gujarati                & 0.77                    & 0.76                     & 0.44                   & 0.73                    & 0.69                     & 0.35                   \\
Telugu                  & 0.77                    & 0.76                     & 0.44                   & 0.73                    & 0.71                     & 0.38                   \\
Slovak                  & 0.76                    & 0.74                     & 0.42                   & 0.75                    & 0.72                     & 0.40                   \\
Italian                 & 0.76                    & 0.68                     & 0.34                   & 0.72                    & 0.68                     & 0.34                   \\
Galician                & 0.76                    & 0.74                     & 0.43                   & 0.74                    & 0.71                     & 0.39                   \\
Estonian                & 0.76                    & 0.74                     & 0.41                   & 0.74                    & 0.71                     & 0.37                   \\
Czech                   & 0.76                    & 0.74                     & 0.42                   & 0.76                    & 0.73                     & 0.4                    \\
Marathi                 & 0.76                    & 0.74                     & 0.41                   & 0.72                    & 0.69                     & 0.35                   \\
Uzbek                   & 0.75                    & 0.74                     & 0.39                   & 0.67                    & 0.63                     & 0.28                   \\
Urdu                    & 0.75                    & 0.72                     & 0.39                   & 0.71                    & 0.68                     & 0.33                   \\
Ukrainian               & 0.75                    & 0.73                     & 0.41                   & 0.74                    & 0.72                     & 0.4                    \\
Malayalam               & 0.75                    & 0.73                     & 0.4                    & 0.71                    & 0.68                     & 0.34                   \\
Sinhala                 & 0.75                    & 0.73                     & 0.39                   & 0.69                    & 0.66                     & 0.32                   \\
Bengali                 & 0.74                    & 0.73                     & 0.39                   & 0.74                    & 0.70                     & 0.36                   \\
Croatian                & 0.74                    & 0.72                     & 0.39                   & 0.73                    & 0.70                     & 0.36                   \\
Lao                     & 0.74                    & 0.73                     & 0.39                   & 0.69                    & 0.66                     & 0.30                   \\
Haitian                 & 0.74                    & 0.73                     & 0.41                   & 0.67                    & 0.65                     & 0.30                   \\
Hungarian               & 0.74                    & 0.72                     & 0.38                   & 0.74                    & 0.71                     & 0.37                   \\
Kazakh                  & 0.73                    & 0.72                     & 0.38                   & 0.67                    & 0.62                     & 0.27                   \\
Russian                 & 0.73                    & 0.70                     & 0.38                   & 0.72                    & 0.69                     & 0.36                   \\
Vietnamese              & 0.73                    & 0.72                     & 0.39                   & 0.72                    & 0.71                     & 0.36                   \\
Slovenian               & 0.73                    & 0.71                     & 0.38                   & 0.69                    & 0.67                     & 0.32                   \\
Zulu                    & 0.73                    & 0.74                     & 0.43                   & 0.65                    & 0.65                     & 0.32                   \\
Tamil                   & 0.73                    & 0.71                     & 0.37                   & 0.70                    & 0.68                     & 0.33                   \\
Finnish                 & 0.73                    & 0.70                     & 0.36                   & 0.72                    & 0.68                     & 0.33                   \\
Kannada                 & 0.72                    & 0.71                     & 0.37                   & 0.71                    & 0.68                     & 0.33                   \\
Lithuanian              & 0.72                    & 0.70                     & 0.36                   & 0.68                    & 0.63                     & 0.28                   \\
Icelandic               & 0.72                    & 0.70                     & 0.37                   & 0.72                    & 0.7                      & 0.36                   \\
Southern Sotho          & 0.72                    & 0.71                     & 0.40                   & 0.62                    & 0.6                      & 0.27                   \\
Korean                  & 0.71                    & 0.68                     & 0.32                   & 0.69                    & 0.66                     & 0.31                   \\
Basque                  & 0.71                    & 0.68                     & 0.34                   & 0.67                    & 0.63                     & 0.27                   \\
Thai                    & 0.71                    & 0.66                     & 0.3                    & 0.69                    & 0.65                     & 0.28                   \\
Chinese                 & 0.70                    & 0.67                     & 0.31                   & 0.68                    & 0.64                     & 0.29                   \\
Georgian                & 0.70                    & 0.66                     & 0.31                   & 0.64                    & 0.58                     & 0.21                   \\
Xhosa                   & 0.70                    & 0.70                     & 0.38                   & 0.63                    & 0.63                     & 0.28                   \\
Dutch                   & 0.70                    & 0.66                     & 0.31                   & 0.72                    & 0.67                     & 0.34                   \\
Japanese                & 0.69                    & 0.66                     & 0.30                   & 0.69                    & 0.65                     & 0.29                   \\
Polish                  & 0.69                    & 0.65                     & 0.30                   & 0.68                    & 0.64                     & 0.29                   \\
Burmese                 & 0.69                    & 0.65                     & 0.30                   & 0.63                    & 0.58                     & 0.22                   \\
Khmer                   & 0.68                    & 0.65                     & 0.30                   & 0.64                    & 0.59                     & 0.23                   \\
Kinyarwanda             & 0.68                    & 0.67                     & 0.34                   & 0.61                    & 0.6                      & 0.23                   \\
Samoan                  & 0.67                    & 0.65                     & 0.33                   & 0.62                    & 0.59                     & 0.26                   \\
Somali                  & 0.66                    & 0.66                     & 0.32                   & 0.59                    & 0.57                     & 0.22                   \\
Faroese                 & 0.62                    & 0.60                     & 0.28                   & 0.65                    & 0.62                     & 0.28                   \\
Lingala                 & 0.60                    & 0.59                     & 0.24                   & 0.60                    & 0.58                     & 0.23                   \\
Azerbaijani             & 0.31                    & 0.29                     & 0.05                   & 0.11                    & 0.10                     & 0.00                   \\
Fijian                  & 0.16                    & 0.16                     & 0.02                   & 0.51                    & 0.47                     & 0.12                  
 \\ \hline
\end{tabular}%
}
\caption{Comparsion between Google Translate API and Bing Translator.}
\label{tab:google_vs_bing}
\end{table}




\begin{table}[!th]
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}ccccc@{}}
\toprule
\textbf{Language}    & \textbf{Lang Code} & \textbf{Number of Tokens (M)} & \textbf{Percentage of Tokens} & \textbf{Class}\\ \midrule
English     & en & 181,015 & 92.64\%  & A             \\ \midrule
French      & fr &  \pz3,553   &  1.81853\% & A             \\
German      & de &  \pz2,871   &  1.46937\%  & A            \\
Spanish     & es &  \pz1,510   &  0.77289\% & A             \\
Italian     & it &  \pz1,188   &  0.60793\% & A             \\
Portuguese  & po &  \pz1,025   &  0.52483\% & A             \\
Russian     & ru &  \pzz368   &  0.18843\% & A             \\
Romanian    & ro &  \pzz308   &  0.15773\% & A             \\
Swedish     & sv &  \pzz221   &  0.11307\% & A             \\
Japanese    & ja &  \pzz217   &  0.11109\% & A             \\ \midrule
Chinese     & zh &  \pzz194   &  0.09905\% & B           \\
Indonesian  & id &  \pzz117   &  0.05985\%  & B           \\
Turkish     & tr &  \pzz116   &  0.05944\%  & B           \\
Vietnamese  & vi &  \pz\pz83  &  0.04252\%  & B           \\
Greek       & el &  \pz\pz62  &  0.03153\%  & B           \\
Arabic      & ar &  \pz\pz61  &  0.03114\% & B            \\
Serbian     & sr &  \pz\pz53  &  0.02706\%  & B           \\
Korean      & ko &  \pz\pz33  &  0.01697\%  & B           \\
Slovak      & sk &  \pz\pz28  &  0.01431\% & B            \\
Thai        & th &  \pz\pz27  &  0.01372\% & B           \\
Slovenian   & sl &  \pz\pz26  &  0.01333\% & B            \\ \midrule
Persian     & fa &  \pz\pz17  &  0.00856\% & C             \\
Hebrew      & he &  \pz\pz15  &  0.00769\% & C             \\
Hindi       & hi &  \pz\pz\pz9  &  0.00483\% & C            \\
Bulgarian   & bg &  \pz\pz\pz6  &  0.00303\%  & C           \\
Bengali     & bn &  \pz\pz\pz3  &  0.00154\%  & C            \\
Malayalam   & ml &  \pz\pz\pz3  &  0.00165\%  & C           \\
Azerbaijani & az &  \pz\pz\pz2  &  0.00128\%  & C           \\
Telugu      & te &  \pz\pz\pz2  &  0.00084\%  & C           \\
Uzbek       & uz &  \pz\pz\pz1.5  &  0.00075\% & C            \\
Nepali      & ne &  \pz\pz\pz1.1  &  0.00057\% & C            \\
Urdu        & ur &  \pz\pz\pz0.7  &  0.00035\% & C            \\
Swahili     & sw &  \pz\pz\pz0.6  &  0.00030\%  & C           \\ \midrule
Assamese    & as &  \pz\pz\pz0    &  0.00000\%  & D                  \\
Bambara     & bam &  \pz\pz\pz0    &  0.00000\%  & D                   \\
Ewe         & ee &  \pz\pz\pz0    &  0.00000\%  & D                   \\
Hausa       & hau &  \pz\pz\pz0    &  0.00000\%  & D                   \\
Yoruba      & yor &  \pz\pz\pz0    &  0.00000\%  & D                   \\ \bottomrule
 \\ \hline
\end{tabular}%
}
\caption{ List of languages, language codes, number of tokens in pre-trained GPT-3 data, data ratios. The languages are grouped into four classes based on their data
ratios in the GPT-3 pre-trained data: High Resource
(H > 0.1\%), Medium Resource (M > 0.01\%), and Low
Resource (L < 0.01\%), and extremely low resource for unrepresented languages.}
\label{tab:languages_classes}
\end{table}



\begin{table*}[] 
\centering
\resizebox{\textwidth}{!} {
\begin{tabular}{lllllllllllllllllll}
\multicolumn{4}{c}{\textbf{Configuration}}                                                    & \multicolumn{3}{c}{\textbf{Assamese}}             & \multicolumn{3}{c}{\textbf{Bengali}}              & \multicolumn{3}{c}{\textbf{Hindi}}                & \multicolumn{3}{c}{\textbf{Malayalam}}            & \multicolumn{3}{c}{\textbf{Telugu}}               \\
\multicolumn{1}{c}{P} & \multicolumn{1}{c}{I} & \multicolumn{1}{c}{C} & \multicolumn{1}{c}{O} & \textit{gemini} & \textit{gpt} & \textit{mixtral} & \textit{gemini} & \textit{gpt} & \textit{mixtral} & \textit{gemini} & \textit{gpt} & \textit{mixtral} & \textit{gemini} & \textit{gpt} & \textit{mixtral} & \textit{gemini} & \textit{gpt} & \textit{mixtral} \\ \hline
S                     & S                     & Z                     & E                     & 0.2             & 0.43         & 0.00             & 0.16            & 0.48         & 0.02             & 0.26            & 0.22         & 0.19             & 0.19            & 0.25         & 0.03             & 0.29            & 0.15         & 0.13             \\
E                     & S                     & Z                     & E                     & 0.15            & 0.03         & 0.00             & 0.11            & 0.03         & 0.02             & 0.21            & 0.30         & 0.12             & 0.13            & 0.13         & 0.14             & 0.12            & 0.10         & 0.10             \\
S                     & S                     & S                     & E                     & 0.59            & 0.35         & 0.10             & 0.65            & 0.52         & 0.28             & 0.74            & 0.68         & 0.37             & 0.46            & 0.36         & 0.04             & 0.53            & 0.23         & 0.19             \\
E                     & E                     & E                     & S                     & 0.25            & 0.00         & 0.01             & 0.25            & 0.10         & 0.02             & 0.53            & 0.26         & 0.19             & 0.02            & 0.06         & 0.07             & 0.27            & 0.14         & 0.09             \\
S                     & E                     & S                     & E                     & 0.69            & 0.51         & 0.70             & 0.71            & 0.67         & 0.44             & 0.80            & 0.79         & 0.62             & 0.47            & 0.41         & 0.32             & 0.46            & 0.53         & 0.39             \\
E                     & S                     & S                     & S                     & 0.2             & 0.00         & 0.01             & 0.08            & 0.02         & 0.18             & 0.50            & 0.5          & 0.34             & 0.12            & 0.06         & 0.11             & 0.16            & 0.12         & 0.14             \\
E                     & S                     & Z                     & S                     & 0.39            & 0.10         & 0.01             & 0.35            & 0.03         & 0.08             & 0.55            & 0.23         & 0.17             & 0.12            & 0.10         & 0.03             & 0.19            & 0.1          & 0.08             \\
S                     & S                     & S                     & S                     & 0.69            & 0.32         & 0.10             & 0.65            & 0.57         & 0.51             & 0.74            & 0.74         & 0.45             & 0.52            & 0.33         & 0.06             & 0.60            & 0.34         & 0.55             \\
E                     & S                     & E                     & S                     & 0.32            & 0.00         & 0.00             & 0.27            & 0.03         & 0.02             & 0.55            & 0.30         & 0.22             & 0.02            & 0.17         & 0.08             & 0.15            & 0.04         & 0.11             \\
S                     & S                     & Z                     & S                     & 0.69            & 0.23         & 0.07             & 0.64            & 0.47         & 0.39             & 0.72            & 0.62         & 0.42             & 0.60            & 0.24         & 0.09             & 0.53            & 0.16         & 0.15             \\
E                     & E                     & S                     & S                     & 0.28            & 0.09         & 0.12             & 0.24            & 0.18         & 0.17             & 0.48            & 0.35         & 0.29             & 0.04            & 0.15         & 0.08             & 0.18            & 0.14         & 0.13             \\
E                     & S                     & E                     & E                     & 0.11            & 0.13         & 0.00             & 0.08            & 0.03         & 0.02             & 0.29            & 0.32         & 0.18             & 0.05            & 0.08         & 0.04             & 0.09            & 0.09         & 0.07             \\
E                     & E                     & Z                     & E                     & 0.17            & 0.01         & 0.00             & 0.14            & 0.03         & 0.02             & 0.23            & 0.30         & 0.24             & 0.15            & 0.10         & 0.11             & 0.15            & 0.17         & 0.12             \\
S                     & E                     & Z                     & E                     & 0.54            & 0.37         & 0.07             & 0.64            & 0.43         & 0.15             & 0.44            & 0.64         & 0.68             & 0.48            & 0.2          & 0.42             & 0.59            & 0.4          & 0.16             \\
S                     & E                     & Z                     & S                     & 0.71            & 0.46         & 0.29             & 0.60            & 0.68         & 0.40             & 0.60            & 0.71         & 0.72             & 0.13            & 0.48         & 0.39             & 0.60            & 0.45         & 0.27             \\
S                     & S                     & E                     & E                     & 0.14            & 0.33         & 0.01             & 0.22            & 0.51         & 0.05             & 0.45            & 0.71         & 0.15             & 0.09            & 0.32         & 0.01             & 0.24            & 0.29         & 0.22             \\
E                     & E                     & Z                     & S                     & 0.24            & 0.05         & 0.01             & 0.33            & 0.17         & 0.07             & 0.40            & 0.31         & 0.19             & 0.02            & 0.07         & 0.06             & 0.26            & 0.16         & 0.1              \\
E                     & S                     & S                     & E                     & 0.1             & 0.00         & 0.00             & 0.10            & 0.01         & 0.02             & 0.33            & 0.25         & 0.26             & 0.06            & 0.10         & 0.13             & 0.09            & 0.10         & 0.02             \\
S                     & S                     & E                     & S                     & 0.72            & 0.44         & 0.02             & 0.66            & 0.51         & 0.27             & 0.78            & 0.76         & 0.28             & 0.63            & 0.42         & 0.01             & 0.53            & 0.28         & 0.06             \\
S                     & E                     & S                     & S                     & 0.64            & 0.56         & 0.64             & 0.60            & 0.66         & 0.53             & 0.80            & 0.82         & 0.60             & 0.20            & 0.49         & 0.31             & 0.65            & 0.46         & 0.34             \\
E                     & E                     & E                     & E                     & 0.15            & 0.00         & 0.00             & 0.11            & 0.04         & 0.01             & 0.24            & 0.29         & 0.20             & 0.05            & 0.05         & 0.12             & 0.11            & 0.08         & 0.11             \\
S                     & E                     & E                     & E                     & 0.19            & 0.33         & 0.05             & 0.28            & 0.22         & 0.11             & 0.47            & 0.32         & 0.25             & 0.07            & 0.07         & 0.14             & 0.25            & 0.21         & 0.17             \\
E                     & E                     & S                     & E                     & 0.09            & 0.01         & 0.01             & 0.11            & 0.04         & 0.04             & 0.31            & 0.24         & 0.31             & 0.07            & 0.08         & 0.08             & 0.10            & 0.12         & 0.12             \\
S                     & E                     & E                     & S                     & 0.68            & 0.44         & 0.24             & 0.57            & 0.57         & 0.37             & 0.74            & 0.76         & 0.43             & 0.02            & 0.3          & 0.29             & 0.55            & 0.34         & 0.31            
 \\ \hline
\end{tabular}%
}
\caption{Comparing performance of different models on all languages in IndicQA. Metric: F1 Score.}
\label{tab:qa_indicqa}
\end{table*}


\begin{table*}[!h] 
\centering
\resizebox{\textwidth}{!} {
\begin{tabular}{llllllllllllllllllllllllllllllll}
\multicolumn{3}{c}{\textbf{Configuration}}                                                       & \multicolumn{3}{c}{\textbf{Arabic}} & \multicolumn{3}{c}{\textbf{Chinese}} & \multicolumn{2}{c}{\textbf{Greek}} & \multicolumn{3}{c}{\textbf{Hindi}} & \multicolumn{3}{c}{\textbf{Spanish}} & \multicolumn{3}{c}{\textbf{Swahili}} & \multicolumn{3}{c}{\textbf{Thai}} & \multicolumn{3}{c}{\textbf{Turkish}} & \multicolumn{3}{c}{\textbf{Urdu}} & \multicolumn{3}{c}{Bulgarian} \\
\multicolumn{1}{c}{\textit{P}} & \multicolumn{1}{c}{\textit{I}} & \multicolumn{1}{c}{\textit{C}} & gemini     & gpt      & mixtral     & gemini      & gpt      & mixtral     & gemini            & gpt            & mixtral     & gemini     & gpt     & gemini      & gpt      & mixtral     & gemini      & gpt      & mixtral     & gemini     & gpt     & mixtral    & gemini      & gpt      & mixtral     & gemini     & gpt     & mixtral    & gemini   & gpt    & mixtral   \\
E                              & S                              & E                              & 0.72       & 0.68     & 0.51        & 0.63        & 0.58     & 0.55        & 0.75              & 0.53           & 0.61        & 0.59       & 0.49    & 0.59        & 0.56     & 0.32        & 0.65        & 0.54     & 0.54        & 0.52       & 0.71    & 0.33       & 0.45        & 0.45     & 0.0         & 0.66       & 0.52    & 0.41       & 0.49     & 0.54   & 0.46      \\
S                              & S                              & E                              & 0.65       & 0.67     & 0.54        & 0.62        & 0.63     & 0.52        & 0.66              & 0.59           & 0.54        & 1.00       & 0.46    & 0.6         & 0.64     & 0.52        & 0.62        & 0.64     & 0.59        & 0.52       & 0.56    & 1.0        & 0.47        & 0.42     & 0.5         & N.A        & 0.52    & N.A        & 0.52     & 0.54   & 0.44      \\
S                              & E                              & Z                              & 0.62       & 0.55     & 0.54        & 0.71        & 0.52     & 0.56        & 0.57              & 0.61           & 0.64        & 0.56       & 0.58    & 0.56        & 0.53     & 0.39        & 0.80        & 0.62     & 0.63        & 0.57       & 0.52    & 0.36       & 0.62        & 0.36     & 0.38        & 0.64       & 0.53    & 0.48       & 0.5      & 0.47   & 0.39      \\
S                              & E                              & E                              & 0.61       & 0.62     & 0.52        & 0.59        & 0.6      & 0.45        & 0.75              & 0.54           & 0.78        & 0.59       & 0.59    & 0.6         & 0.58     & 0.43        & 0.61        & 0.59     & 0.6         & 0.58       & 0.67    & 0.79       & 0.54        & 0.48     & 0.41        & 0.83       & 0.45    & 0.39       & 0.52     & 0.48   & 0.37      \\
E                              & E                              & E                              & 0.65       & 0.66     & 0.5         & 0.61        & 0.6      & 0.57        & 0.62              & 0.58           & 0.57        & 0.59       & 0.54    & 0.63        & 0.59     & 0.42        & 0.67        & 0.58     & 0.62        & 0.57       & 0.57    & 0.38       & 0.57        & 0.52     & 0.34        & 0.59       & 0.52    & 0.45       & 0.55     & 0.52   & 0.4       \\
E                              & S                              & S                              & 0.59       & 0.61     & 0.59        & 0.59        & 0.6      & N.A         & 0.7               & 0.58           & 0.58        & 0.67       & 0.43    & 0.59        & 0.57     & 0.33        & 0.68        & 0.69     & 0.69        & 0.52       & 0.58    & 0.0        & 0.53        & 0.57     & 0.5         & 1.00       & 0.51    & 0.0        & 0.51     & 0.54   & 0.47      \\
E                              & E                              & Z                              & 0.75       & 0.55     & N.A         & 0.59        & 0.57     & 0.55        & 0.68              & 0.61           & 0.6         & 0.68       & 0.55    & 0.58        & 0.51     & 0.38        & 0.8         & 0.53     & 0.55        & 0.57       & 0.54    & 0.45       & 0.63        & 0.41     & 0.43        & 0.5        & 0.55    & 0.45       & 0.69     & 0.46   & 0.37      \\
S                              & S                              & Z                              & 1.0        & 0.45     & 0.51        & 0.57        & 0.55     & 0.44        & N.A               & 0.53           & 0.57        & 1.00       & 0.57    & 0.53        & 0.4      & 0.83        & 0.68        & 0.57     & 0.46        & 0.49       & 0.46    & 0.33       & 0.48        & 0.38     & 0.33        & 0.54       & 0.57    & 0.42       & 0.71     & 0.44   & 0.37      \\
S                              & S                              & S                              & 0.63       & 0.72     & 0.55        & 0.58        & 0.6      & N.A         & 0.78              & 0.6            & 0.64        & 0.65       & 0.52    & 0.62        & 0.53     & 0.29        & 0.77        & 0.51     & 0.55        & 0.61       & 0.54    & 0.0        & 0.51        & 0.47     & 0.36        & 0.52       & 0.5     & N.A        & 0.52     & 0.57   & 0.11      \\
S                              & E                              & S                              & 0.67       & 0.66     & 0.54        & 0.57        & 0.63     & N.A         & 0.67              & 0.6            & 0.61        & 0.63       & 0.52    & 0.6         & 0.61     & 0.44        & 0.76        & 0.57     & 0.56        & 0.59       & 0.62    & 0.34       & 0.58        & 0.57     & 0.44        & 0.43       & 0.56    & 0.48       & 0.51     & 0.54   & 0.44      \\
E                              & E                              & S                              & 0.65       & 0.64     & 0.56        & 0.59        & 0.61     & N.A         & 0.61              & 0.64           & 0.67        & 0.65       & 0.71    & 0.62        & 0.64     & 0.47        & 0.43        & 0.6      & 0.62        & 0.62       & 0.73    & 0.35       & 0.54        & 0.53     & 0.42        & 0.50       & 0.55    & 0.73       & 0.54     & 0.56   & 0.39      \\
E                              & S                              & Z                              & 0.82       & 0.0      & 0.43        & 0.59        & 0.58     & 0.46        & 0.50              & 0.53           & 0.40        & 0.82       & 0.63    & 0.56        & 0.48     & 0.53        & 0.64        & 0.56     & 0.46        & 0.47       & 0.47    & 0.4        & N.A         & 0.34     & 0.48        & 0.60       & 0.53    & 0.52       & 0.57     & 0.42   & 0.36     
\end{tabular}%
}
\caption{Comparing performance of different models on all languages in XNLI. Metric: Acc Score.}
\label{tab:xnli}
\end{table*}

\begin{table*}[] 
\centering
\resizebox{\textwidth}{!} {
\begin{tabular}{llllllllllllllllllllll}
\multicolumn{4}{c}{\textbf{Configuration}}                                                                                        & \multicolumn{3}{c}{\textbf{Arabic}}                                                                           & \multicolumn{3}{c}{\textbf{German}}                                                                           & \multicolumn{3}{c}{\textbf{Greek}}                                                                            & \multicolumn{3}{c}{\textbf{Romanian}}                                                                         & \multicolumn{3}{c}{\textbf{Russian}}                                                                          & \multicolumn{3}{c}{\textbf{Vietnamese}}                                                                       \\
\multicolumn{1}{c}{\textit{P}} & \multicolumn{1}{c}{\textit{I}} & \multicolumn{1}{c}{\textit{C}} & \multicolumn{1}{c}{\textit{O}} & \multicolumn{1}{c}{\textit{gemini}} & \multicolumn{1}{c}{\textit{gpt}} & \multicolumn{1}{c}{\textit{mixtral}} & \multicolumn{1}{c}{\textit{gemini}} & \multicolumn{1}{c}{\textit{gpt}} & \multicolumn{1}{c}{\textit{mixtral}} & \multicolumn{1}{c}{\textit{gemini}} & \multicolumn{1}{c}{\textit{gpt}} & \multicolumn{1}{c}{\textit{mixtral}} & \multicolumn{1}{c}{\textit{gemini}} & \multicolumn{1}{c}{\textit{gpt}} & \multicolumn{1}{c}{\textit{mixtral}} & \multicolumn{1}{c}{\textit{gemini}} & \multicolumn{1}{c}{\textit{gpt}} & \multicolumn{1}{c}{\textit{mixtral}} & \multicolumn{1}{c}{\textit{gemini}} & \multicolumn{1}{c}{\textit{gpt}} & \multicolumn{1}{c}{\textit{mixtral}} \\ \hline
S                              & S                              & Z                              & E                              & 0.27                                & 0.46                             & 0.06                                 & 0.56                                & 0.72                             & 0.62                                 & 0.34                                & 0.17                             & 0.13                                 & 0.55                                & 0.58                             & 0.41                                 & 0.21                                & 0.18                             & 0.19                                 & 0.59                                & 0.6                              & 0.23                                 \\
E                              & S                              & Z                              & E                              & 0.28                                & 0.12                             & 0.05                                 & 0.56                                & 0.48                             & 0.49                                 & 0.34                                & 0.19                             & 0.16                                 & 0.56                                & 0.62                             & 0.31                                 & 0.21                                & 0.20                             & 0.20                                 & 0.55                                & 0.13                             & 0.26                                 \\
S                              & S                              & S                              & E                              & 0.78                                & 0.53                             & 0.33                                 & 0.76                                & 0.67                             & 0.58                                 & 0.50                                & 0.54                             & 0.53                                 & 0.45                                & 0.64                             & 0.48                                 & 0.57                                & 0.69                             & 0.26                                 & 0.67                                & 0.55                             & 0.44                                 \\
E                              & E                              & E                              & S                              & 0.36                                & 0.28                             & 0.10                                 & 0.62                                & 0.85                             & 0.35                                 & 0.43                                & 0.47                             & 0.20                                 & 0.61                                & 0.54                             & 0.41                                 & 0.39                                & 0.26                             & 0.14                                 & 0.52                                & 0.45                             & 0.30                                 \\
S                              & E                              & S                              & E                              & 0.84                                & 0.72                             & 0.30                                 & 0.73                                & 0.65                             & 0.46                                 & 0.74                                & 0.65                             & 0.37                                 & 0.54                                & 0.67                             & 0.51                                 & 0.74                                & 0.61                             & 0.33                                 & 0.72                                & 0.58                             & 0.47                                 \\
E                              & S                              & S                              & S                              & 0.62                                & 0.19                             & 0.37                                 & 0.71                                & 0.50                             & 0.43                                 & 0.61                                & 0.38                             & 0.30                                 & 0.68                                & 0.51                             & 0.37                                 & 0.47                                & 0.34                             & 0.14                                 & 0.62                                & 0.48                             & 0.44                                 \\
E                              & S                              & Z                              & S                              & 0.48                                & 0.29                             & 0.24                                 & 0.68                                & 0.39                             & 0.43                                 & 0.40                                & 0.17                             & 0.14                                 & 0.67                                & 0.63                             & 0.26                                 & 0.35                                & 0.16                             & 0.12                                 & 0.57                                & 0.58                             & 0.28                                 \\
S                              & S                              & S                              & S                              & 0.80                                & 0.54                             & 0.40                                 & 0.76                                & 0.64                             & 0.36                                 & 0.71                                & 0.54                             & 0.52                                 & 0.51                                & 0.61                             & 0.51                                 & 0.75                                & 0.61                             & 0.47                                 & 0.72                                & 0.67                             & 0.54                                 \\
E                              & S                              & E                              & S                              & 0.51                                & 0.24                             & 0.06                                 & 0.68                                & 0.51                             & 0.54                                 & 0.44                                & 0.38                             & 0.22                                 & 0.65                                & 0.50                             & 0.37                                 & 0.34                                & 0.19                             & 0.16                                 & 0.61                                & 0.46                             & 0.38                                 \\
S                              & S                              & Z                              & S                              & 0.74                                & 0.40                             & 0.28                                 & 0.70                                & 0.72                             & 0.65                                 & 0.67                                & 0.69                             & 0.32                                 & 0.73                                & 0.69                             & 0.45                                 & 0.67                                & 0.65                             & 0.42                                 & 0.75                                & 0.73                             & 0.36                                 \\
E                              & E                              & S                              & S                              & 0.52                                & 0.28                             & 0.16                                 & 0.68                                & 0.50                             & 0.38                                 & 0.49                                & 0.39                             & 0.26                                 & 0.68                                & 0.50                             & 0.41                                 & 0.47                                & 0.29                             & 0.24                                 & 0.62                                & 0.43                             & 0.32                                 \\
E                              & S                              & E                              & E                              & 0.26                                & 0.25                             & 0.07                                 & 0.58                                & 0.50                             & 0.49                                 & 0.42                                & 0.27                             & 0.16                                 & 0.40                                & 0.50                             & 0.43                                 & 0.23                                & 0.26                             & 0.17                                 & 0.58                                & 0.49                             & 0.36                                 \\
E                              & E                              & Z                              & E                              & 0.27                                & 0.16                             & 0.07                                 & 0.54                                & 0.50                             & 0.37                                 & 0.34                                & 0.20                             & 0.26                                 & 0.52                                & N.A                              & 0.31                                 & 0.23                                & 0.22                             & 0.10                                 & 0.49                                & 0.33                             & 0.30                                 \\
S                              & E                              & Z                              & E                              & 0.55                                & 0.48                             & 0.15                                 & 0.61                                & 0.62                             & 0.61                                 & 0.56                                & 0.38                             & 0.50                                 & 0.62                                & 0.66                             & 0.50                                 & 0.48                                & 0.46                             & 0.23                                 & 0.67                                & 0.66                             & 0.35                                 \\
S                              & E                              & Z                              & S                              & 0.62                                & 0.49                             & 0.13                                 & 0.61                                & 0.62                             & 0.61                                 & 0.61                                & 0.26                             & 0.49                                 & 0.65                                & N.A                              & 0.41                                 & 0.58                                & 0.58                             & 0.33                                 & 0.60                                & 0.58                             & 0.40                                 \\
S                              & S                              & E                              & E                              & 0.27                                & 0.39                             & 0.05                                 & 0.65                                & 0.57                             & 0.46                                 & 0.43                                & 0.40                             & 0.15                                 & 0.58                                & 0.55                             & 0.37                                 & 0.26                                & 0.35                             & 0.15                                 & 0.61                                & 0.59                             & 0.34                                 \\
E                              & E                              & Z                              & S                              & 0.38                                & 0.13                             & 0.11                                 & 0.57                                & 0.56                             & 0.48                                 & 0.40                                & 0.31                             & 0.26                                 & 0.54                                & N.A                              & 0.29                                 & 0.40                                & 0.20                             & 0.16                                 & 0.54                                & 0.31                             & 0.29                                 \\
E                              & S                              & S                              & E                              & 0.31                                & 0.23                             & 0.13                                 & 0.67                                & 0.48                             & 0.51                                 & 0.43                                & 0.27                             & 0.36                                 & 0.66                                & 0.51                             & 0.29                                 & 0.29                                & 0.25                             & 0.18                                 & 0.57                                & 0.47                             & 0.41                                 \\
S                              & S                              & E                              & S                              & 0.74                                & 0.58                             & 0.26                                 & 0.68                                & 0.65                             & 0.51                                 & 0.70                                & 0.57                             & 0.46                                 & 0.51                                & 0.64                             & 0.5                                  & 0.72                                & 0.66                             & 0.4                                  & 0.74                                & 0.65                             & 0.43                                 \\
S                              & E                              & S                              & S                              & 0.72                                & 0.74                             & 0.39                                 & 0.70                                & 0.64                             & 0.43                                 & 0.57                                & 0.55                             & 0.49                                 & 0.45                                & 0.67                             & 0.55                                 & 0.63                                & 0.62                             & 0.43                                 & 0.66                                & 0.58                             & 0.52                                 \\
E                              & E                              & E                              & E                              & 0.23                                & 0.17                             & 0.06                                 & 0.60                                & 0.78                             & 0.47                                 & 0.39                                & 0.67                             & 0.19                                 & 0.58                                & 0.63                             & 0.43                                 & 0.22                                & 0.17                             & 0.18                                 & 0.56                                & 0.46                             & 0.40                                 \\
S                              & E                              & E                              & E                              & 0.77                                & 0.58                             & 0.09                                 & 0.71                                & 0.57                             & 0.37                                 & 0.57                                & 0.53                             & 0.23                                 & 0.46                                & 0.59                             & 0.46                                 & 0.39                                & 0.42                             & 0.13                                 & 0.63                                & 0.51                             & 0.42                                 \\
E                              & E                              & S                              & E                              & 0.32                                & 0.22                             & 0.16                                 & 0.65                                & 0.51                             & 0.42                                 & 0.40                                & 0.35                             & 0.2                                  & 0.64                                & 0.49                             & 0.41                                 & 0.30                                & 0.18                             & 0.34                                 & 0.59                                & 0.47                             & 0.4                                  \\
S                              & E                              & E                              & S                              & 0.65                                & 0.71                             & 0.30                                 & 0.66                                & 0.66                             & 0.35                                 & 0.62                                & 0.66                             & 0.43                                 & 0.46                                & 0.66                             & 0.49                                 & 0.65                                & 0.59                             & 0.36                                 & 0.68                                & 0.58                             & 0.28                                
 \\ \hline
\end{tabular}%
}


\captionsetup{justification=centering} % Centering the caption
\caption{Comparing performance of different models on all languages in XQuAD. Metric: F1 Score.}
\label{tab:qa_quad}
\end{table*}


\begin{table*}[!ht] 
\centering
\resizebox{\textwidth}{!} {
\begin{tabular}{lllllllllllllllllllllllll}
\multicolumn{4}{c}{\textbf{Configuration}}                                                                                        & \multicolumn{3}{c}{\textbf{Chinese}}              & \multicolumn{3}{c}{French}                        & \multicolumn{3}{c}{\textbf{Italian}}              & \multicolumn{3}{c}{\textbf{Portuguese}}           & \multicolumn{3}{c}{\textbf{Serbian}}              & \multicolumn{3}{c}{\textbf{Slovak}}               & \multicolumn{3}{c}{Swedish}                       \\
\multicolumn{1}{c}{\textit{P}} & \multicolumn{1}{c}{\textit{I}} & \multicolumn{1}{c}{\textit{C}} & \multicolumn{1}{c}{\textit{O}} & \textit{gemini} & \textit{gpt} & \textit{mixtral} & \textit{gemini} & \textit{gpt} & \textit{mixtral} & \textit{gemini} & \textit{gpt} & \textit{mixtral} & \textit{gemini} & \textit{gpt} & \textit{mixtral} & \textit{gemini} & \textit{gpt} & \textit{mixtral} & \textit{gemini} & \textit{gpt} & \textit{mixtral} & \textit{gemini} & \textit{gpt} & \textit{mixtral} \\ \hline
E                              & E                              & E                              & E                              & 0.00            & 0.01         & 0.23             & 0.53            & 0.58         & 0.15             & 0.38            & 0.53         & 0.23             & 0.56            & 0.60         & 0.41             & 0.20            & 0.21         & 0.00             & 0.48            & 0.53         & 0.19             & 0.59            & 0.62         & 0.41             \\
E                              & E                              & E                              & S                              & 0.00            & 0.00         & 0.18             & 0.37            & 0.63         & 0.17             & 0.31            & 0.52         & 0.23             & 0.55            & 0.60         & 0.42             & 0.22            & 0.26         & 0.01             & 0.48            & 0.50         & 0.21             & 0.59            & 0.58         & 0.38             \\
E                              & E                              & S                              & E                              & 0.00            & 0.01         & 0.21             & 0.54            & 0.6          & 0.25             & 0.66            & 0.66         & 0.52             & 0.56            & 0.58         & 0.29             & 0.17            & 0.18         & 0.01             & 0.45            & 0.51         & 0.31             & 0.59            & 0.59         & 0.42             \\
E                              & E                              & S                              & S                              & 0.00            & 0.06         & 0.22             & 0.55            & 0.61         & 0.00             & 0.59            & 0.68         & 0.48             & 0.55            & 0.61         & 0.38             & 0.13            & 0.13         & 0.02             & 0.46            & 0.48         & 0.24             & 0.57            & 0.56         & 0.21             \\
E                              & E                              & Z                              & E                              & 0.00            & 0.00         & 0.22             & 0.45            & 0.54         & N.A              & 0.51            & 0.63         & 0.49             & 0.45            & 0.57         & 0.34             & 0.13            & 0.12         & 0.02             & 0.47            & 0.47         & 0.28             & 0.51            & 0.56         & 0.32             \\
E                              & E                              & S                              & S                              & 0.00            & 0.00         & 0.20             & 0.44            & 0.53         & 0.0              & 0.44            & 0.63         & 0.50             & 0.43            & 0.54         & 0.35             & 0.09            & 0.11         & 0.03             & 0.38            & 0.49         & 0.29             & 0.44            & 0.59         & 0.36             \\
E                              & S                              & E                              & E                              & 0.00            & 0.00         & 0.23             & 0.55            & 0.60         & 0.25             & 0.34            & 0.48         & 0.26             & 0.55            & 0.60         & 0.37             & 0.21            & 0.22         & 0.01             & 0.39            & 0.50         & 0.18             & 0.60            & 0.60         & 0.27             \\
E                              & S                              & E                              & S                              & 0.00            & 0.00         & 0.28             & 0.53            & 0.59         & 0.21             & 0.30            & 0.48         & 0.26             & 0.55            & 0.61         & 0.40             & 0.17            & 0.21         & 0.01             & 0.47            & 0.52         & 0.30             & 0.58            & 0.64         & 0.26             \\
E                              & S                              & S                              & E                              & 0.01            & 0.02         & 0.22             & 0.42            & 0.59         & 0.26             & 0.65            & 0.63         & 0.52             & 0.58            & 0.61         & 0.40             & 0.14            & 0.17         & 0.01             & 0.46            & 0.49         & 0.25             & 0.57            & 0.61         & 0.22             \\
E                              & S                              & S                              & S                              & 0.01            & 0.03         & 0.24             & 0.45            & 0.59         & 0.28             & 0.61            & 0.67         & 0.49             & 0.56            & 0.60         & 0.37             & 0.16            & 0.16         & 0.01             & 0.46            & 0.36         & N.A              & 0.55            & 0.55         & 0.22             \\
E                              & S                              & Z                              & E                              & 0.00            & 0.00         & 0.24             & 0.48            & 0.55         & 0.36             & 0.53            & 0.64         & 0.50             & 0.47            & 0.55         & 0.35             & 0.12            & 0.12         & 0.02             & 0.46            & 0.48         & 0.29             & 0.52            & 0.56         & 0.33             \\
E                              & S                              & Z                              & S                              & 0.01            & 0.00         & 0.20             & 0.45            & 0.55         & 0.38             & 0.52            & 0.62         & 0.48             & 0.46            & 0.53         & 0.33             & 0.11            & 0.12         & 0.03             & 0.41            & 0.50         & 0.29             & 0.50            & 0.57         & 0.36             \\
S                              & E                              & E                              & E                              & 0.06            & 0.02         & 0.09             & 0.61            & 0.69         & 0.32             & 0.40            & 0.53         & 0.33             & 0.60            & 0.68         & 0.49             & 0.57            & 0.20         & 0.05             & 0.66            & 0.62         & 0.26             & 0.63            & 0.64         & 0.14             \\
S                              & E                              & E                              & S                              & 0.11            & 0.05         & 0.07             & 0.68            & 0.71         & 0.31             & 0.36            & 0.53         & 0.29             & 0.64            & 0.66         & 0.54             & 0.68            & 0.64         & 0.3              & 0.61            & 0.61         & 0.31             & 0.60            & 0.64         & 0.24             \\
S                              & E                              & S                              & E                              & 0.61            & 0.61         & 0.00             & 0.64            & 0.72         & 0.32             & 0.69            & 0.74         & 0.64             & 0.71            & 0.69         & 0.53             & 0.72            & 0.75         & 0.47             & 0.73            & 0.71         & 0.39             & 0.69            & 0.67         & 0.34             \\
S                              & E                              & S                              & S                              & 0.59            & 0.63         & 0.00             & 0.63            & 0.66         & 0.35             & 0.69            & 0.75         & 0.62             & 0.76            & 0.70         & 0.48             & 0.77            & 0.68         & 0.46             & 0.69            & 0.72         & 0.54             & 0.66            & 0.68         & 0.35             \\
S                              & E                              & Z                              & E                              & 0.07            & 0.07         & 0.02             & 0.48            & 0.59         & 0.48             & 0.54            & 0.66         & 0.56             & 0.50            & 0.63         & 0.48             & 0.22            & 0.42         & 0.25             & 0.57            & 0.65         & 0.49             & 0.50            & 0.57         & 0.37             \\
S                              & E                              & Z                              & S                              & 0.16            & 0.05         & 0.00             & 0.55            & 0.60         & 0.48             & 0.48            & 0.67         & 0.57             & 0.48            & 0.62         & 0.48             & 0.47            & 0.5          & 0.32             & 0.51            & 0.62         & 0.45             & 0.50            & 0.62         & 0.39             \\
S                              & S                              & E                              & E                              & 0.17            & 0.02         & 0.01             & 0.64            & 0.68         & 0.29             & 0.38            & 0.54         & 0.32             & 0.66            & 0.66         & 0.48             & 0.53            & 0.17         & 0.05             & 0.67            & 0.29         & 0.27             & 0.61            & 0.63         & 0.18             \\
S                              & S                              & E                              & S                              & 0.14            & 0.02         & 0.07             & 0.59            & 0.68         & 0.31             & 0.35            & 0.59         & 0.32             & 0.69            & 0.65         & 0.47             & 0.63            & 0.61         & 0.29             & 0.65            & 0.38         & 0.52             & 0.62            & 0.63         & 0.25             \\
S                              & S                              & S                              & E                              & 0.60            & 0.61         & 0.00             & 0.63            & 0.70         & 0.45             & 0.70            & 0.74         & 0.59             & 0.70            & 0.72         & 0.51             & 0.72            & 0.77         & 0.45             & 0.72            & 0.58         & 0.40             & 0.67            & 0.67         & 0.53             \\
S                              & S                              & S                              & S                              & 0.58            & 0.62         & 0.01             & 0.64            & 0.69         & 0.31             & 0.69            & 0.71         & 0.61             & 0.68            & 0.72         & 0.48             & 0.77            & 0.72         & 0.44             & 0.69            & 0.56         & 0.40             & 0.65            & 0.66         & 0.53             \\
S                              & S                              & Z                              & E                              & 0.12            & 0.08         & 0.02             & 0.57            & 0.57         & 0.45             & 0.57            & 0.67         & 0.56             & 0.51            & 0.63         & 0.48             & 0.28            & 0.46         & 0.29             & 0.55            & 0.65         & 0.47             & 0.54            & 0.58         & 0.37             \\
S                              & S                              & Z                              & S                              & 0.13            & 0.04         & 0.00             & 0.49            & 0.58         & 0.48             & 0.55            & 0.68         & 0.58             & 0.48            & 0.61         & 0.48             & 0.42            & 0.50         & 0.33             & 0.52            & 0.63         & 0.45             & 0.53            & 0.60         & 0.39            
 \\ \hline
\end{tabular}%
}
\caption{Comparing performance of different models on all languages in WikiANN. Metric: F1 Score.}
\label{tab:wikiann}
\end{table*}



\begin{table*}[!h] 
\centering
\resizebox{\textwidth}{!} {
\begin{tabular}{llllllllllllllll}
\multicolumn{4}{c}{\textbf{Configuration}}                                                    & \multicolumn{3}{c}{\textbf{Bambara}}              & \multicolumn{3}{c}{\textbf{Ewe}}                  & \multicolumn{3}{c}{\textbf{Hausa}}                & \multicolumn{3}{c}{\textbf{Yoruba}}               \\
\multicolumn{1}{c}{P} & \multicolumn{1}{c}{I} & \multicolumn{1}{c}{C} & \multicolumn{1}{c}{O} & \textit{gemini} & \textit{gpt} & \textit{mixtral} & \textit{gemini} & \textit{gpt} & \textit{mixtral} & \textit{gemini} & \textit{gpt} & \textit{mixtral} & \textit{gemini} & \textit{gpt} & \textit{mixtral} \\ \hline
E                     & E                     & E                     & E                     & 0.16            & 0.18         & 0.10             & 0.42            & 0.47         & 0.22             & 0.45            & 0.24         & 0.26             & 0.06            & 0.09         & 0.03             \\
E                     & E                     & E                     & S                     & 0.17            & 0.17         & 0.07             & 0.43            & 0.48         & 0.23             & 0.46            & 0.61         & 0.31             & 0.07            & 0.09         & 0.05             \\
E                     & E                     & S                     & E                     & 0.15            & 0.17         & 0.12             & 0.44            & 0.52         & 0.22             & 0.45            & 0.52         & 0.20             & 0.07            & 0.08         & 0.04             \\
E                     & E                     & S                     & S                     & 0.14            & 0.15         & 0.07             & 0.39            & 0.5          & 0.17             & 0.43            & 0.39         & 0.10             & 0.06            & 0.08         & 0.03             \\
E                     & E                     & Z                     & E                     & 0.13            & 0.09         & N.A              & 0.41            & 0.44         & 0.23             & 0.52            & 0.59         & 0.26             & 0.05            & 0.08         & 0.04             \\
E                     & E                     & S                     & S                     & 0.13            & 0.16         & 0.06             & 0.41            & 0.47         & 0.21             & 0.45            & 0.60         & 0.29             & 0.04            & 0.08         & 0.03             \\
E                     & S                     & E                     & E                     & 0.16            & 0.18         & 0.10             & 0.44            & 0.47         & 0.22             & 0.48            & 0.59         & 0.32             & 0.07            & 0.08         & 0.03             \\
E                     & S                     & E                     & S                     & 0.16            & 0.18         & 0.08             & 0.40            & 0.47         & 0.15             & 0.48            & 0.58         & 0.34             & 0.06            & 0.08         & 0.05             \\
E                     & S                     & S                     & E                     & 0.15            & 0.15         & 0.06             & 0.45            & 0.50         & 0.28             & 0.50            & 0.54         & 0.22             & 0.08            & 0.10         & 0.05             \\
E                     & S                     & S                     & S                     & 0.11            & 0.15         & 0.06             & 0.39            & 0.49         & 0.15             & 0.45            & 0.42         & 0.11             & 0.06            & 0.07         & 0.06             \\
E                     & S                     & Z                     & E                     & 0.15            & 0.28         & 0.19             & 0.40            & 0.46         & 0.23             & 0.54            & 0.62         & 0.27             & 0.08            & 0.08         & 0.05             \\
E                     & S                     & Z                     & S                     & 0.17            & 0.13         & 0.05             & 0.38            & 0.44         & 0.25             & 0.47            & 0.28         & 0.29             & 0.06            & 0.09         & 0.06             \\
S                     & E                     & E                     & E                     & 0.09            & 0.24         & 0.00             & 0.31            & 0.47         & 0.01             & 0.46            & 0.43         & 0.00             & 0.08            & 0.20         & 0.05             \\
S                     & E                     & E                     & S                     & 0.27            & 0.27         & 0.06             & 0.40            & 0.52         & 0.04             & 0.61            & 0.58         & 0.10             & 0.09            & 0.23         & 0.10             \\
S                     & E                     & S                     & E                     & 0.28            & 0.32         & 0.14             & 0.56            & 0.68         & 0.47             & 0.56            & 0.70         & 0.30             & 0.26            & 0.32         & 0.15             \\
S                     & E                     & S                     & S                     & 0.26            & 0.32         & 0.15             & 0.54            & 0.66         & 0.51             & 0.55            & 0.70         & 0.32             & 0.23            & 0.29         & 0.20             \\
S                     & E                     & Z                     & E                     & 0.07            & 0.09         & 0.06             & 0.31            & 0.39         & 0.00             & 0.48            & 0.70         & 0.00             & 0.20            & 0.20         & 0.01             \\
S                     & E                     & Z                     & S                     & 0.21            & 0.20         & 0.17             & 0.43            & 0.42         & 0.42             & 0.57            & 0.69         & 0.07             & 0.17            & 0.26         & 0.07             \\
S                     & S                     & E                     & E                     & 0.10            & 0.25         & 0.00             & 0.27            & 0.39         & 0.00             & 0.45            & 0.43         & 0.00             & 0.08            & 0.21         & 0.04             \\
S                     & S                     & E                     & S                     & 0.23            & 0.27         & 0.05             & 0.40            & 0.51         & 0.08             & 0.54            & 0.60         & 0.09             & 0.13            & 0.22         & 0.05             \\
S                     & S                     & S                     & E                     & 0.29            & 0.33         & 0.21             & 0.61            & 0.67         & 0.43             & 0.58            & 0.67         & 0.34             & 0.27            & 0.30         & 0.19             \\
S                     & S                     & S                     & S                     & 0.27            & 0.32         & 0.21             & 0.61            & 0.63         & 0.45             & 0.59            & 0.69         & 0.41             & 0.26            & 0.31         & N.A              \\
S                     & S                     & Z                     & E                     & 0.08            & 0.18         & 0.00             & 0.31            & 0.37         & 0.00             & 0.46            & 0.46         & 0.01             & 0.03            & 0.22         & 0.03             \\
S                     & S                     & Z                     & S                     & 0.23            & 0.28         & 0.04             & 0.39            & 0.46         & 0.06             & 0.06            & 0.64         & 0.05             & 0.07            & 0.26         & 0.07            
 \\ \hline
\end{tabular}%
}
\caption{Comparing performance of different models on all languages in MasakhaNER. Metric: F1 Score.}
\label{tab:masakhaner}
\end{table*}




\begin{table*}[!h] 
\centering
\resizebox{\textwidth}{!} {
\begin{tabular}{llllllllllllllllllllllllllllllllll}
\multicolumn{4}{c}{\textbf{Configuration}}                                                                                        & \multicolumn{3}{l}{\textbf{Azerbaijani}}           & \multicolumn{3}{l}{\textbf{French}}               & \multicolumn{3}{l}{\textbf{Japanese}}             & \multicolumn{3}{l}{\textbf{Korean}}               & \multicolumn{3}{l}{\textbf{Nepali}}                            & \multicolumn{3}{l}{\textbf{Persian}}                     & \multicolumn{3}{l}{\textbf{Portuguese}}           & \multicolumn{3}{l}{\textbf{Spanish}}                     & \multicolumn{3}{l}{\textbf{Turkish}}                           & \multicolumn{3}{l}{\textbf{Uzbek}}                                 \\
\multicolumn{1}{c}{\textit{P}} & \multicolumn{1}{c}{\textit{I}} & \multicolumn{1}{c}{\textit{C}} & \multicolumn{1}{c}{\textit{O}} & \textit{gpt} & \textit{mixtral} & \textit{mixtral} & \textit{gemini} & \textit{gpt} & \textit{mixtral} & \textit{gemini} & \textit{gpt} & \textit{mixtral} & \textit{gemini} & \textit{gpt} & \textit{mixtral} & \textit{gemini}        & \textit{gpt} & \textit{mixtral}       & \textit{gemini}        & \textit{gpt} & \textit{mixtral} & \textit{gemini} & \textit{gpt} & \textit{mixtral} & \textit{gemini}        & \textit{gpt} & \textit{mixtral} & \textit{gemini}        & \textit{gpt} & \textit{mixtral}       & \textit{gemini}        & \textit{gpt}           & \textit{mixtral} \\
E                              & E                              & S                              & E                              & 10.48        & 6.12             & -                & 19.51           & 22.94        & 20.58            & 16.2            & 17.14        & 16.9             & 6.63            & 7.03         & 6.94             & 10.86                  & 9.69         & 10.53                  & 13.79                  & 16.77        & 14.28            & 14.20           & 18.22        & 15.50            & 13.9                   & 18.41        & 16.99            & 13.23                  & 13.21        & 10.49                  & \pz6.02 & \pz4.02 & 5.33             \\
S                              & S                              & S                              & S                              & 11.0         & 1.07             & -                & 15.75           & 21.06        & 19.67            & 18.86           & 18.87        & 16.45            & 4.96            & 7.67         & 6.11             & \pz7.22 & 11.22        & \pz9.70 & 13.29                  & 16.86        & 12.31            & 16.25           & 17.46        & 15.98            & 13.47                  & 17.72        & 16.73            & \pz7.93 & 14.04        & \pz9.90 & \pz0.0  & \pz3.93 & 5.40             \\
S                              & S                              & S                              & E                              & 11.51        & 5.34             & -                & 18.27           & 22.13        & 19.79            & 12.52           & 18.26        & 15.12            & 3.40            & 5.16         & 6.10             & \pz0.00 & 7.44         & \pz8.27 & 30.15                  & 15.76        & 13.49            & 13.79           & 18.79        & 16.60            & 13.57                  & 18.64        & 16.60            & \pz9.38 & 16.09        & \pz9.71 & \pz3.93 & \pz4.33 & 4.74             \\
S                              & E                              & E                              & S                              & 10.16        & 9.38             &                  & 15.63           & 18.88        & 20.42            & 19.83           & 17.53        & 15.67            & 5.80            & 8.20         & 7.90             & \pz6.33 & 10.29        & \pz8.24 & 14.07                  & 16.72        & 13.81            & 14.38           & 17.15        & 15.93            & 14.31                  & 17.61        & 17.09            & \pz8.99 & 12.51        & 10.54                  & \pz2.5  & \pz4.53 & 5.39             \\
E                              & E                              & S                              & S                              & 11.13        &                  & -                & 21.61           & 20.98        & 20.04            & 21.98           & 21.49        & 16.85            & 10.64           & 7.72         & 8.55             & 11.46                  & 10.86        & 10.64                  & 17.68                  & 17.81        & 17.71            & 18.32           & 17.56        & 17.83            & 18.15                  & 17.93        & 17.64            & 14.23                  & 15.22        & 12.95                  & \pz2.75 & \pz6.75 & 9.41             \\
S                              & E                              & Z                              & E                              & 11.31        & 9.92             & -                & 22.08           & 22.11        & 20.72            & 20.58           & 18.89        & 16.41            & 9.23            & 8.89         & 6.54             & \pz0.00 & 12.44        & \pz7.21 & 14.14                  & 18.78        & 14.44            & 18.19           & 18.25        & 16.87            & 17.90                  & 18.32        & 17.97            & 13.34                  & 15.4         & 13.21                  & 10.40                  & \pz6.39 & 5.61             \\
E                              & E                              & Z                              & E                              & 9.89         & 10.32            & -                & 16.92           & 21.51        & 17.96            & 15.26           & 17.79        & 18.39            & 5.06            & 7.41         & 8.32             & \pz9.89 & 8.3          & \pz9.12 & 14.43                  & 15.04        & 14.71            & 15.64           & 16.73        & 15.09            & 14.05                  & 18.04        & 17.37            & 9.52                   & 11.99        & 11.04                  & 10.23                  & \pz6.19 & 5.87             \\
S                              & E                              & S                              & E                              & 9.85         & 7.43             & -                & 17.19           & 21.75        & 19.12            & 16.18           & 18.32        & 17.87            & 5.25            & 7.76         & 3.24             & \pz9.03 & 10.24        & \pz8.4  & 12.65                  & 18.3         & N.A              & 14.46           & 18.24        & 15.51            & \pz9.49 & 17.94        & 16.87            & 13.39                  & 14.12        & N.A                    & \pz4.33 & \pz7.03 & 4.51             \\
E                              & S                              & Z                              & S                              & 12.26        & 10.42            & -                & 21.77           & 21.64        & 20.09            & 20.62           & 18.93        & 18.52            & 8.77            & 8.71         & 8.29             & 13.98                  & 11.74        & 11.51                  & 17.93                  & 18.04        & 17.31            & 18.51           & 16.94        & 16.26            & 17.66                  & 17.02        & 17.49            & 14.0                   & 14.75        & 13.1                   & \pz5.66 & \pz8.03 & 8.41             \\
S                              & S                              & Z                              & S                              & 10.44        & 7.49             & -                & 21.34           & 21.6         & 19.87            & 19.62           & 17.69        & 13.92            & 8.10            & 7.29         & 6.32             & \pz7.09 & 11.62        & \pz7.04 & 0.0                    & 16.45        & 15.16            & 16.03           & 16.78        & 16.98            & 16.96                  & 17.59        & 18.54            & 12.83                  & 14.38        & 10.72                  & \pz7.84 & \pz7.88 & 6.85             \\
E                              & S                              & E                              & E                              & 9.32         & 6.62             & -                & 18.8            & 20.69        & 20.58            & 12.51           & 18.24        & 15.09            & 3.82            & 7.21         & 7.66             & \pz8.04 & 9.09         & \pz9.51 & 16.31                  & 0.0          & 16.00            & 13.64           & 17.68        & 16.63            & 12.79                  & 16.99        & 18.38            & 7.85                   & 13.02        & 9.87                   & \pz5.11 & \pz9.93 & 4.13             \\
S                              & E                              & Z                              & S                              & 10.79        & 5.35             & -                & 19.68           & 21.2         & 19.91            & 20.54           & 17.99        & 19.71            & 6.52            & 8.55         & 7.37             & 10.53                  & 11.31        & 11.34                  & 13.47                  & 18.61        & 15.42            & 16.52           & 17.35        & 16.06            & 15.50                  & 18.27        & 17.05            & 9.97                   & 14.75        & 10.52                  & \pz4.58 & \pz8.27 & 6.38             \\
E                              & S                              & E                              & S                              & 9.79         & 8.11             & -                & 17.99           & 21.21        & 15.85            & 10.75           & 19.69        & 17.98            & 3.17            & 8.55         & 2.67             & \pz8.98 & 10.71        & \pz8.15 & 14.61                  & 17.29        & 13.88            & 15.09           & 17.39        & 17.99            & 14.41                  & 18.86        & 17.41            & 9.61                   & 13.74        & 7.96                   & \pz3.44 & \pz8.06 & 5.66             \\
E                              & E                              & E                              & S                              & 12.44        & 10.9             & -                & 23.06           & 21.66        & 28.57            & 20.81           & 19.47        & 19.64            & 9.79            & 7.88         & 9.04             & 12.61                  & 11.08        & 12.33                  & 17.49                  & 18.49        & 18.43            & 17.29           & 16.97        & 16.59            & 16.96                  & 17.33        & 17.77            & 14.53                  & 15.07        & 13.83                  & \pz0.00 & \pz9.2  & 9.68             \\
S                              & E                              & E                              & E                              & 11.16        & 11.54            & -                & 19.67           & 20.64        & 21.02            & 15.06           & 19.51        & 19.3             & 6.52            & 8.95         & 7.85             & \pz9.19 & 10.97        & 12.07                  & 12.77                  & 18.32        & 16.80            & 14.75           & 16.32        & 15.86            & 14.68                  & 17.19        & 16.38            & 8.98                   & 13.5         & 12.52                  & \pz6.46 & 10.14                  & 9.18             \\
S                              & S                              & Z                              & E                              & 10.48        &                  & -                & 21.58           & 21.82        & 19.82            & 20.97           & 21.12        & 19.41            & 9.35            & 8.96         & 8.16             & 13.05                  & 12.69        & 12.28                  & 17.19                  & 19.66        & 17.51            & 17.69           & 18.78        & 15.83            & 17.02                  & 17.44        & 16.67            & 15.48                  & 16.01        & 13.6                   & 11.44                  & 10.09                  & 9.09             \\
S                              & E                              & S                              & S                              & 11.07        & 7.14             & 19.13            & 17.22           & 21.32        & 21.3             & 14.14           & 18.95        & 18.11            & 4.98            & 6.90         & 6.12             & 10.23                  & 11.92        & \pz8.81 & 15.32                  & 18.85        & 12.94            & 14.74           & 17.93        & 17.77            & 14.58                  & 17.58        & 14.55            & 12.46                  & 13.26        & 11.14                  & \pz2.88 & 10.62                  & 3.95             \\
S                              & S                              & E                              & E                              & 11.38        & 8.07             & -                & 20.45           & 22.75        & 21.44            & 11.54           & 18.26        & 16.85            & 4.54            & 9.53         & 3.24             & \pz7.51 & 14.11        & \pz8.02 & 13.19                  & 18.84        & 12.45            & 15.75           & 18.08        & 17.67            & 15.79                  & 17.82        & 17.51            & 9.77                   & 15.35        & 9.94                   & \pz5.97 & \pz9.15 & 6.07             \\
E                              & E                              & E                              & E                              & 12.65        & 10.63            & -                & 20.2            & 22.09        & 21.47            & 16.71           & 19.94        & 19.66            & 6.35            & 9.08         & 8.13             & \pz9.76 & 12.54        & 11.43                  & 14.1                   & 21.08        & 16.41            & 15.89           & 19.39        & 16.27            & 15.85                  & 18.81        & 16.69            & 12.15                  & 17.37        & 13.6                   & \pz6.65 & 10.29                  & 9.08             \\
S                              & S                              & E                              & S                              & 12.46        & 2.89             & -                & 16.04           & 22.52        & 19.34            & 17.26           & 21.86        & 17.34            & 3.74            & 8.9          & 7.71             & 11.59                  & 13.77        & \pz9.55 & 14.6                   & 19.36        & 15.95            & 14.03           & 17.09        & 14.85            & 13.40                  & 17.76        & 13.98            & \pz5.75 & 15.0         & 11.7                   & \pz4.49 & 10.88                  & 5.41             \\
E                              & S                              & Z                              & E                              & 12.1         & 6.25             & -                & 16.07           & 22.22        & 20.77            & 15.62           & 21.02        & 16.2             & 3.89            & 8.96         & 4.5              & \pz6.72 & 13.97        & \pz7.87 & 15.05                  & 18.95        & 11.90            & 15.65           & 17.18        & 15.08            & 14.86                  & 18.42        & 13.94            & \pz9.83 & 15.65        & 12.56                  & 10.62                  & 11.30                  & 4.41             \\
E                              & S                              & S                              & S                              & 11.19        &                  & -                & 21.07           & 15.96        & 22.19            & 19.41           & 18.39        & -                & 9.44            & 9.11         & 8.41             & 11.46                  & 12.26        & 12.20                  & 17.66                  & 18.24        & 16.70            & 17.3            & 16.4         & 17.38            & 17.45                  & 17.62        & 17.28            & 13.62                  & 13.3         & 13.96                  & \pz2.28 & 10.26                  & 8.22             \\
E                              & E                              & Z                              & S                              & 4.65         & 9.56             & -                & 21.32           & 20.57        & 20.27            & 21.78           & 18.03        & 17.15            & 9.44            & 8.69         & 8.26             & \pz5.99 & 11.89        & 11.01                  & 17.84                  & 17.67        & 18.30            & 17.29           & 16.12        & 17.45            & 18.40                  & 17.27        & 17.52            & 15.38                  & 13.29        & 12.24                  & \pz6.51 & 10.60                  & 8.21             \\
E                              & S                              & S                              & E                              & 12.19        & 8.65             & -                & 14.83           & 21.26        & 18.81            & 10.31           & 20.6         & 12.81            & 4.33            & 8.59         & 6.40             & \pz7.77 & 13.86        & \pz6.64 & \pz0.00 & 18.01        & 13.31            & 14.92           & 17.11        & 16.15            & 12.34                  & 19.00        & 11.16            & \pz6.8  & 14.3         & \pz9.65 & \pz4.65 & 12.72                  & 6.67            
 \\ \hline
\end{tabular}%
}
\caption{Comparing performance of different models on all languages in XlSUM. Metric: ROUGE1 Score.}
\label{tab:xlsum}
\end{table*}





\begin{table*}[t]
\centering

\resizebox{\textwidth}{!}{%
\begin{tabular}{lllllllllllllllllll}
\toprule
\multicolumn{5}{c}{\textbf{Question Answering}}                   & \multicolumn{5}{c}{\textbf{Summarization}}                      & \multicolumn{5}{c}{\textbf{Named Entity Recognition}}            & \multicolumn{3}{c}{\textbf{NLI}} &          \\
code & instruction & context           & Examples & output          & code & instruction & context & Examples        & output           & code & instruction & context           & Examples         & output &       & instruction   & context    & Examples \\
ar   & -0.02       & \textbf{0.30**} & -0.10**  & \textbf{0.24**} & az   & -0.14**     & -0.01 & 0.04            & \textbf{-0.34**} & zh   & -0.07**     & \textbf{0.44**} & \textbf{-0.26**} & 0.00   & ar    & -0.02         & -0.01(   & -0.05    \\
as   & -0.05       & \textbf{0.35**} & -0.00    & \textbf{0.30**} & fr   & -0.07*      & 0.03  & \textbf{0.19**} & -0.04*           & fr   & 0.01        & 0.10            & -0.11*           & -0.01  & bu    & -0.00         & -0.02    & 0.03     \\
be   & -0.10*      & \textbf{0.39**} & -0.00    & \textbf{0.30**} & ja   & 0.15**      & -0.02 & \textbf{0.34**} & -0.07            & it   & 0.01        & 0.04            & 0.02             & -0.04  & zh    & 0.01          & -0.01    & -0.01    \\
ge   & 0.03        & 0.07*           & -0.09**  & 0.06            & ko   & 0.04        & -0.01 & \textbf{0.25**} & -0.06            & po   & 0.01        & 0.09*           & -0.15**          & 0.02   & ge    & 0.01          & 0.05     & -0.07    \\
gr   & -0.02       & \textbf{0.19**} & -0.09**  & 0.12**          & ne   & 0.00        & 0.02  & 0.13*           & -0.25**          & sr   & 0.05        & \textbf{0.44**} & \textbf{-0.26**} & 0.09   & gr    & 0.02          & -0.02    & 0.04     \\
hi   & 0.04        & \textbf{0.31**} & -0.13**  & \textbf{0.30**} & fa   & -0.08       & 0.07  & \textbf{0.21**} & -0.05            & sk   & -0.01       & 0.21**          & -0.11            & -0.04  & hi    & -0.02         & -0.01    & -0.04    \\
ma   & 0.10**      & \textbf{0.31**} & 0.05     & -0.01           & po   & 0.03        & -0.02 & \textbf{0.25**} & -0.02            & sw   & 0.01        & 0.06*           & -0.11            & -0.03  & es    & -0.01         & 0.00     & 0.02     \\
ro   & 0.03        & -0.07*          & 0.05     & 0.05            & es   & 0.02        & 0.03  & \textbf{0.22**} & -0.03            & bam  & -0.00       & 0.07            & -0.05            & 0.05   & sw    & -0.05         & 0.01     & -0.02    \\
ru   & -0.03       & \textbf{0.27**} & -0.09**  & \textbf{0.22}   & tr   & 0.04        & 0.12* & \textbf{0.19**} & -0.06            & ewe  & -0.00       & 0.02            & -0.07            & 0.03   & th    & -0.07         & -0.01    & -0.01    \\
te   & -0.07       & \textbf{0.40**} & 0.06     & \textbf{0.20}   & uz   & 0.00        & -0.00 & \textbf{0.28**} & \textbf{-0.21**} & hau  & -0.09**     & -0.08           & -0.17            & -0.14  & tu    & -0.01         & -0.04    & -0.03    \\
vi   & 0.04        & \textbf{0.13**} & -0.04    & 0.04*           &      &             &       &                 &                  & yo   & 0.00        & 0.12*           & -0.13            & 0.00   & ur    & -0.02         & -0.01    & 0.03    
 \\ \hline
\end{tabular}%
}
\caption{Point-biserial correlation of Gemini for each Language (denoted by ISO 639 code) nd each of the 4 prompt components - Instruction, Context, Examples, and Output. The p-value is given in the parentheses}
\label{tab:gemini_correlation}
\end{table*}




\begin{table*}[t]
\centering

\resizebox{\textwidth}{!}{%
\begin{tabular}{lllllllllllllllllll}
\toprule
\multicolumn{5}{c}{\textbf{Question Answering}}              & \multicolumn{5}{c}{\textbf{Summarization}}          & \multicolumn{5}{c}{\textbf{Named Entity Recognition}}             & \multicolumn{3}{c}{\textbf{NLI}} &         \\
code & instruction      & context           & examples & output & code & instruction    & context   & examples & output  & code & instruction & context            & examples          & output & code   & instruction   & context   & examples \\
ar   & 0.04             & 0.07            & -0.07** & 0.09** & az   & -0.09*         & -0.09   & 0.03    & -0.10   & zh   & 0.16        & \textbf{0.34**}  & \textbf{-0.29**} & -0.02  & ar     & -0.05         & 0.01    & 0.04    \\
as   & -0.16            & 0.02            & -0.01** & 0.04   & fr   & -0.07*         & 0.00    & 0.05    & -0.02   & fr   & 0.04        & 0.09             & 0.15**           & -0.04  & bu     & 0.01          & 0.01    & -0.03   \\
be   & -0.07            & \textbf{0.31**} & -0.07*  & 0.15** & ja   & -0.03          & 0.04    & 0.13*   & -0.18   & it   & -0.00       & 0.04             & 0.14**           & -0.01  & zh     & -0.03         & 0.00    & -0.01   \\
ge   & 0.04             & 0.07            & 0.13**  & -0.05  & ko   & 0.02           & -0.04   & 0.08    & -0.04   & po   & -0.01       & 0.08             & -0.02            & -0.01  & ge     & 0.02          & 0.02    & -0.01   \\
gr   & -0.06**          & 0.17            & 0.03    & 0.03   & ne   & -0.16          & 0.02    & 0.10    & -0.10   & sr   & 0.02        & \textbf{0.36**}  & 0.09**           & 0.08   & gr     & -0.06         & 0.01(   & -0.04   \\
hi   & -0.07**          & 0.17            & -0.05   & 0.10   & fa   & -0.09          & -0.00   & 0.14    & 0.05    & sk   & 0.05        & 0.11             & 0.12             & 0.06   & hi     & -0.01         & -0.01   & 0.01    \\
ma   & \textbf{-0.23**} & 0.09            & -0.05   & -0.00  & po   & -0.12**        & -0.02   & 0.06    & 0.04    & sw   & -0.03       & 0.01             & 0.13             & -0.03  & es     & -0.01         & 0.00    & 0.03    \\
ro   & -0.02            & 0.16            & -0.14** & -0.00  & es   & -0.06          & 0.02    & 0.02    & 0.03    & bam  & 0.04        & 0.00             & -0.00            & 0.01   & sw     & 0.01          & -0.02   & -0.03   \\
ru   & -0.00            & \textbf{0.22**} & -0.10** & 0.07*  & tr   & -0.03          & -0.04   & 0.09    & -0.10** & ewe  & -0.03       & 0.09             & -0.05            & 0.01   & th     & -0.01         & -0.01   & -0.07   \\
te   & -0.07            & \textbf{0.19**} & -0.07   & 0.06*  & uz   & \textbf{-0.31} & -0.13** & 0.10*   & -0.14*  & hau  & 0.05        & \textbf{-0.27**} & -0.15            & 0.03   & tu     & -0.03         & -0.04   & -0.09   \\
vi   & -0.00            & 0.10            & -0.16*  & 0.02   &      &                &         &         &         & yo   & -0.02       & 0.10             & -0.08**          & 0.03   & ur     & 0.03          & -0.01   & -0.02  
 \\ \hline
\end{tabular}%
}
\caption{Point-biserial correlation of Mixtral for each Language (denoted by ISO 639 code) nd each of the 4 prompt components - Instruction, context, Examples, and Output. The p-value is given in the parentheses}
\label{tab:mixtral_correlation}
\end{table*}



\begin{table*}[t]
\centering

\resizebox{\textwidth}{!}{%
\begin{tabular}{lllllllllllllllllll}
\hline
\multicolumn{5}{c}{{\textbf{Question Answering}}}     & \multicolumn{5}{c}{{\textbf{Summarization}}}               & \multicolumn{5}{c}{{\textbf{NER}}}                   & \multicolumn{4}{c}{{\textbf{NLI}}} \\
code & instruction & context          & Examples & output & code & instruction      & context          & Examples & output & code & instruction & context & Examples         & output & code  & instruction & context & output \\
ar   & 0.01        & \textbf{0.43**}  & N.A      & 0.02   & fr   & -0.06*           & 0.12**           & N.A      & -0.02  & zh   & 0.03        & 0.14    & \textbf{-0.16**} & 0.00   & ar    & 0.04**      & 0.05    & -0.07* \\
as   & 0.05        & \textbf{0.47**}  & N.A      & -0.03  & ja   & \textbf{-0.17**} & \textbf{-0.32**} & N.A      & 0.05   & fr   & -0.01       & 0.06    & \textbf{-0.35**} & -0.01  & zh    & 0.02        & -0.02   & -0.02  \\
be   & 0.001*      & \textbf{0.51**}  & N.A      & 0.04   & ne   & -0.05            & 0.04             & N.A      & 0.01   & it   & 0.04        & 0.02    & \textbf{-0.28**} & 0.01   & ge    & 0.06        & 0.04    & 0.02   \\
ge   & 0.05*       & -0.11**          & N.A      & 0.02   & po   & -0.01            & 0.04             & N.A      & -0.02  & po   & 0.02        & 0.04    & \textbf{-0.31**} & 0.01   & hi    & -0.06*      & 0.01    & -0.07  \\
gr   & 0.05*       & \textbf{-0.24**} & N.A      & ,0.11  & es   & 0.03             & -0.06            & N.A      & 0.01   & sr   & 0.05        & 0.01    & \textbf{-0.20**} & 0.021  & es    & 0.02        & -0.01*  & -0.01  \\
hi   & ,-0.00      & \textbf{0.44}    & N.A      & 0.01   & pe   & \textbf{-0.24*}  & \textbf{-0.25**} & N.A      & 0.06   & sk   & 0.01        & 0.02    & \textbf{-0.30**} & -0.02  & sw    & 0.07        & -0.01   & -0.03  \\
ma   & -0.04*      & \textbf{0.38**}  & N.A      & -0.02  &      &                  &                  &          &        & sw   & 0.02        & -0.05   & \textbf{-0.33**} & 0.02   & th    & -0.03       & 0.03    & -0.01  \\
ro   & 0.03        & -0.14**          & N.A      & -0.04  &      &                  &                  &          &        & bam  & 0.00        & 0.02    & -0.03            & -0.01  & ur    & 0.04        & 0.02    & 0.05   \\
ru   & 0.05        & \textbf{0.17**}  & N.A      & 0.04   &      &                  &                  &          &        & ewe  & -0.03       & 0.00    & -0.01            & 0.03   &       &             &         &        \\
te   & ,0.14**     & \textbf{0.45***} & N.A      & ,-0.02 &      &                  &                  &          &        & hau  & ,0.03       & -0.01   & 0.00             & 0.01   &       &             &         &        \\
vi   & -0.08       & \textbf{0.20**}  & N.A      & -0.02  &      &                  &                  &          &        & yor  & ,0.01       & 0.06    & -0.03            & 0.01   &       &             &         &        \\ \hline
\end{tabular}
}
\caption{Point-biserial correlation of Bloomz for each Language (denoted by ISO 639 code) nd each of the 4 prompt components - Instruction, context, Examples, and Output. The p-value is given in the parentheses}
\label{tab:bloomz_correlation}
\end{table*}


% \begin{table*}[t]
% \centering

% \resizebox{\textwidth}{!}{%
% \begin{tabular}{lllllllllllllllllll}
% \hline
% \multicolumn{5}{c}{{\ul \textbf{Question Answering}}}     & \multicolumn{5}{c}{{\ul \textbf{Summarization}}}               & \multicolumn{5}{c}{{\ul \textbf{NER}}}                   & \multicolumn{4}{c}{{\ul \textbf{NLI}}} \\
% code & instruction & context          & Examples & output & code & instruction      & context          & Examples & output & code & instruction & context & Examples         & output & code  & instruction & context & output \\
% ar   & 0.01        & \textbf{0.43**}  & N.A      & 0.02   & fr   & -0.06*           & 0.12**           & N.A      & -0.02  & zh   & 0.03        & 0.14    & \textbf{-0.16**} & 0.00   & ar    & 0.04**      & 0.05    & -0.07* \\
% as   & 0.05        & \textbf{0.47**}  & N.A      & -0.03  & ja   & \textbf{-0.17**} & \textbf{-0.32**} & N.A      & 0.05   & fr   & -0.01       & 0.06    & \textbf{-0.35**} & -0.01  & zh    & 0.02        & -0.02   & -0.02  \\
% be   & 0.001*      & \textbf{0.51**}  & N.A      & 0.04   & ne   & -0.05            & 0.04             & N.A      & 0.01   & it   & 0.04        & 0.02    & \textbf{-0.28**} & 0.01   & ge    & 0.06        & 0.04    & 0.02   \\
% ge   & 0.05*       & -0.11**          & N.A      & 0.02   & po   & -0.01            & 0.04             & N.A      & -0.02  & po   & 0.02        & 0.04    & \textbf{-0.31**} & 0.01   & hi    & -0.06*      & 0.01    & -0.07  \\
% gr   & 0.05*       & \textbf{-0.24**} & N.A      & ,0.11  & es   & 0.03             & -0.06            & N.A      & 0.01   & sr   & 0.05        & 0.01    & \textbf{-0.20**} & 0.021  & es    & 0.02        & -0.01*  & -0.01  \\
% hi   & ,-0.00      & \textbf{0.44}    & N.A      & 0.01   & pe   & \textbf{-0.24*}  & \textbf{-0.25**} & N.A      & 0.06   & sk   & 0.01        & 0.02    & \textbf{-0.30**} & -0.02  & sw    & 0.07        & -0.01   & -0.03  \\
% ma   & -0.04*      & \textbf{0.38**}  & N.A      & -0.02  &      &                  &                  &          &        & sw   & 0.02        & -0.05   & \textbf{-0.33**} & 0.02   & th    & -0.03       & 0.03    & -0.01  \\
% ro   & 0.03        & -0.14**          & N.A      & -0.04  &      &                  &                  &          &        & bam  & 0.00        & 0.02    & -0.03            & -0.01  & ur    & 0.04        & 0.02    & 0.05   \\
% ru   & 0.05        & \textbf{0.17**}  & N.A      & 0.04   &      &                  &                  &          &        & ewe  & -0.03       & 0.00    & -0.01            & 0.03   &       &             &         &        \\
% te   & ,0.14**     & \textbf{0.45***} & N.A      & ,-0.02 &      &                  &                  &          &        & hau  & ,0.03       & -0.01   & 0.00             & 0.01   &       &             &         &        \\
% vi   & -0.08       & \textbf{0.20**}  & N.A      & -0.02  &      &                  &                  &          &        & yor  & ,0.01       & 0.06    & -0.03            & 0.01   &       &             &         &        \\ \hline
% \end{tabular}%}
% \caption{Point-biserial correlation of Bloomz for each Language (denoted by ISO 639 code) nd each of the 4 prompt components - Instruction, context, Examples, and Output. The p-value is given in the parentheses}
% \label{tab:bloomz_corrella}
% \end{table*}




\end{document}





