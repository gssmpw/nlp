\section{Related Work}
%\subsection{The Rise of Multilingual LLMs}

With over 7,000 languages spoken globally \citep{anderson2010many},   the growing use of diverse languages have fueled the demand for multilingual LLMs. Progress in this field stems from two primary efforts: (1) developing dedicated monolingual models for low-to-medium-resource languages \citep{seker2022alephbert, cui2023efficient, andersland2024amharic}, and (2) creating multilingual LLMs with pre-trained data encompassing multiple languages  \citep{qin2024multilingual, jiang2024mixtral}. 

The ability of Multilingual LLMs to operate in % and transfer knowledge between,
different languages  \citep{raffel2020exploring, conneau2019unsupervised, chowdhery2023palm} comes from two sources: (1)  training or fine-tuning on multilingual data in order to achieve multilingual proficiency
\citep{xue2020mt5, chen2021zero, le2023bloom, shaham2024multilingual, muennighoff2022crosslingual}, and (2) utilizing prompting techniques to harness the model's inherent multilingual capabilities without modifying parameters during inference \citep{brown2020language}. This latter approach has gained popularity due to its efficiency and applicability to a wider range of use cases.


%Following these developments, benchmarks for evaluating LLMs have been proposed to measure cross-lingual transfer, including for low-resource languages \citep{hu2020xtreme, liang2020xglue} and benchmarks focusing on specific language families such as Indian languages  \citep{kakwani2020indicnlpsuite} and African languages \citep{ogundepo2023afriqa}. 





%\subsection{Multilingual Prompting Approaches}

For the latter, to improve the multilingual capabilities of  LLMs
researchers
%\reut{if this pertains to item (2) above, say it}
  developed various prompting methods. \citet{huang2023not} introduced XLT, a cross-lingual prompt that directs LLMs to function as experts in a specific language through a process involving problem-solving and cross-lingual thinking. \citet{zhao2021discrete} employed discrete and soft prompting techniques and showed that few-shot non-English prompts outperform finetuning in cross-lingual transfer. \citet{shi2022language} found that chain-of-thought (CoT) prompts lead to multilingual reasoning abilities in LLMs, even in under-represented languages. Another strategy is \emph{pre-transaltion} which translates the entire prompt to English \citep{chowdhery2023palm, qin2023chatgpt, ahuja2023mega}. A more nuanced approach, \emph{selective pre-translation}, translates part of the prompt into English, for instance, \citet{liu2024translation} translated only the instruction, and \citet{ahuja2023mega} translated the few shot examples. While
these use cases lack a systematic research foundation, in this study, we systematically study pre-translation configurations to provide evidence-based recommendations for optimal use.
%\reut{this background section feels redundant  and breaks the flow- it should be moved to after section 5 and be called "related work". Also, for works you mention in 2.2. in this related work section need to state how they are similar or different than this paper }