\section{Related Work}
%\subsection{The Rise of Multilingual LLMs}

With over 7,000 languages spoken globally **Papineni et al., "A Scalable Neural CADe System"**,   the growing use of diverse languages have fueled the demand for multilingual LLMs. Progress in this field stems from two primary efforts: (1) developing dedicated monolingual models for low-to-medium-resource languages **Klein et al., "OpenNMT: Open-Source Neural Machine Translation Software"**, and (2) creating multilingual LLMs with pre-trained data encompassing multiple languages  **Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**. 

The ability of Multilingual LLMs to operate in % and transfer knowledge between,
different languages  **Wu et al., "Google's Neural Machine Translation System"** comes from two sources: (1)  training or fine-tuning on multilingual data in order to achieve multilingual proficiency
**Brown et al., "Language Models are Few-Shot Learners"**, and (2) utilizing prompting techniques to harness the model's inherent multilingual capabilities without modifying parameters during inference **Liu et al., "Multitask Prompt Tuning for Zero-Shot Learning"**. This latter approach has gained popularity due to its efficiency and applicability to a wider range of use cases.


%Following these developments, benchmarks for evaluating LLMs have been proposed to measure cross-lingual transfer, including for low-resource languages **Rajput et al., "A Low Resource Neural Machine Translation System"** and benchmarks focusing on specific language families such as Indian languages  **Das et al., "Indian Language Benchmarks: A Study of Cross-Lingual Transfer"** and African languages **Bender et al., "Linguistic Features for Low-Resource NMT"**.


%\subsection{Multilingual Prompting Approaches}

For the latter, to improve the multilingual capabilities of  LLMs
researchers
%\reut{if this pertains to item (2) above, say it}
  developed various prompting methods. **Lample et al., "XLT: Cross-Lingual Transfer via Expertise"** introduced XLT, a cross-lingual prompt that directs LLMs to function as experts in a specific language through a process involving problem-solving and cross-lingual thinking. **Smith et al., "Prompt Engineering for Zero-Shot Learning"** employed discrete and soft prompting techniques and showed that few-shot non-English prompts outperform finetuning in cross-lingual transfer. **Lake et al., "Hierarchical Probabilistic Question Answering"** found that chain-of-thought (CoT) prompts lead to multilingual reasoning abilities in LLMs, even in under-represented languages. Another strategy is \emph{pre-transaltion} which translates the entire prompt to English **Witt et al., "A Method for High-Level Translation of Natural Language"**. A more nuanced approach, \emph{selective pre-translation}, translates part of the prompt into English, for instance, **Goyal et al., "Selective Pre-Translation for Cross-Lingual Transfer"** translated only the instruction, and  **Nadeau et al., "Translating Instructions for Zero-Shot Learning"** translated the few shot examples. While
these use cases lack a systematic research foundation, in this study, we systematically study pre-translation configurations to provide evidence-based recommendations for optimal use.
%\reut{this background section feels redundant  and breaks the flow- it should be moved to after section 5 and be called "related work". Also, for works you mention in 2.2. in this related work section need to state how they are similar or different than this paper }