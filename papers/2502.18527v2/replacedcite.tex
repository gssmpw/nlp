\section{Related Work}
\label{sec:related_work}  

The evaluation of personalized large language models (LLMs) has been a growing area of research, with existing works primarily focusing on measuring how well models adapt to user-specific contexts, styles, or preferences. However, most benchmarks emphasize \emph{reactive} personalization—assessing how well an LLM responds to explicit user prompts—rather than evaluating \emph{proactive} personalization, which is crucial for personal AI assistants.

\subsection{Existing Personalization Benchmarks}

Benchmarks such as \textbf{LaMP} ____ and \textbf{LongLaMP} ____ provide user-specific ground-truth datasets, enabling direct comparisons between model outputs and actual user-authored content. Others, such as \textbf{PersoBench} ____ and \textbf{PrefEval} ____, test persona consistency and adherence to stated preferences. Although these datasets help quantify personalization gains, they rely on static corpora, lack real-time adaptation, and do not assess privacy-preserving or on-device AI performance.

In personalized recommendation, benchmarks like \textbf{MovieLens} ____ and \textbf{Amazon Reviews} ____ evaluate user item predictions but assume centralized data collection, making them unsuitable for privacy-focused, on-device applications. More recent approaches, such as \textbf{Ranking-TAGER} ____, integrate user tagging and ranking-based personalization but lack trusted execution environments (TEEs) or mechanisms to prevent fraudulent data submissions. Similarly, \textbf{PersonalLLM} ____ investigates latent user preferences, but does not address privacy concerns or adversarial data manipulation.

\subsection{Limitations of Existing Evaluation Metrics}

Traditional evaluation metrics for personalization primarily assess response accuracy, coherence, and user preference alignment. \textbf{Intrinsic metrics} such as \textbf{BLEU} ____, \textbf{ROUGE} ____, and \textbf{METEOR} ____ measure textual similarity but do not account for the proactive nature of personal AI assistants. More personalized evaluation methods, such as \textbf{Win Rate} ____, \textbf{Hits@K} ____, and \textbf{Word Mover’s Distance} ____, provide better alignment with user preferences, but still focus on \emph{reactive} tasks rather than anticipatory interactions.

\textbf{Extrinsic metrics}, including \textbf{recall}, \textbf{precision}, and \textbf{normalized discounted
cumulative gain} (NDCG) ____, are widely used in recommendation systems but assume centralized data aggregation, making them impractical for privacy-preserving, on-device personalization. Furthermore, these metrics do not quantify the actual \emph{value} of user data in personalization, a critical aspect to balance personalization with privacy.