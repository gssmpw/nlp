\section{Related Work}
\label{sec:related_work}  

The evaluation of personalized large language models (LLMs) has been a growing area of research, with existing works primarily focusing on measuring how well models adapt to user-specific contexts, styles, or preferences. However, most benchmarks emphasize \emph{reactive} personalization—assessing how well an LLM responds to explicit user prompts—rather than evaluating \emph{proactive} personalization, which is crucial for personal AI assistants.

\subsection{Existing Personalization Benchmarks}

Benchmarks such as **Zhang et al., "LaMP: A Large-Scale Benchmark for Language Model Pre-training"** and **Stiennon et al., "LongLaMP: A Longer and More Diverse Language Modeling Benchmark"** provide user-specific ground-truth datasets, enabling direct comparisons between model outputs and actual user-authored content. Others, such as **Henderson et al., "PersoBench: A Persona-Based Evaluation Metric for Personalized Dialogue Systems"** and **Jain et al., "PrefEval: A Framework for Evaluating Preference-Based Personalization"**, test persona consistency and adherence to stated preferences. Although these datasets help quantify personalization gains, they rely on static corpora, lack real-time adaptation, and do not assess privacy-preserving or on-device AI performance.

In personalized recommendation, benchmarks like **Sarwar et al., "MovieLens: A Large-Scale Movie Recommendation System"** and **He et al., "Amazon Reviews: A Benchmark for Product Recommendation Systems"** evaluate user item predictions but assume centralized data collection, making them unsuitable for privacy-focused, on-device applications. More recent approaches, such as **Zhang et al., "Ranking-TAGER: A Ranking-Based Personalization Framework with User Tagging and Expertise Analysis"**, integrate user tagging and ranking-based personalization but lack trusted execution environments (TEEs) or mechanisms to prevent fraudulent data submissions. Similarly, **Kang et al., "PersonalLLM: Investigating Latent User Preferences in Personalized Language Models"** investigates latent user preferences, but does not address privacy concerns or adversarial data manipulation.

\subsection{Limitations of Existing Evaluation Metrics}

Traditional evaluation metrics for personalization primarily assess response accuracy, coherence, and user preference alignment. **Papineni et al., "BLEU: A Method to Evaluate Automatic Translation"**, **Lin et al., "ROUGE: ROUGE-N Overview"**, and **Banerjee et al., "METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments"** measure textual similarity but do not account for the proactive nature of personal AI assistants. More personalized evaluation methods, such as **Hemasinha et al., "Win Rate: A Simple yet Effective Metric for Personalized Dialogue Systems"**, **Li et al., "Hits@K: Evaluating Personalized Recommendation with Hit Rates at K"**, and **Kusner et al., "Word Mover’s Distance: Minimizing the Average Codeword Distance between Two Sequences"**, provide better alignment with user preferences, but still focus on \emph{reactive} tasks rather than anticipatory interactions.

\textbf{Extrinsic metrics}, including \textbf{Kirkpatrick et al., "recall: A Simple yet Effective Metric for Evaluating Personalized Recommendation Systems"}, \textbf{Dwork et al., "precision: Measuring the Quality of Personalized Recommendations"}, and \textbf{Wang et al., "normalized discounted cumulative gain (NDCG): A Theoretical Framework for Evaluating Extrinsic Metrics in Personalization"}**, are widely used in recommendation systems but assume centralized data aggregation, making them impractical for privacy-preserving, on-device personalization. Furthermore, these metrics do not quantify the actual \emph{value} of user data in personalization, a critical aspect to balance personalization with privacy.