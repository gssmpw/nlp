\section{Related Work}
\label{sec:related_work}  

The evaluation of personalized large language models (LLMs) has been a growing area of research, with existing works primarily focusing on measuring how well models adapt to user-specific contexts, styles, or preferences. However, most benchmarks emphasize \emph{reactive} personalization—assessing how well an LLM responds to explicit user prompts—rather than evaluating \emph{proactive} personalization, which is crucial for personal AI assistants.

\subsection{Existing Personalization Benchmarks}

Benchmarks such as \textbf{LaMP} \cite{salemi2024lamplargelanguagemodels} and \textbf{LongLaMP} \cite{kumar2024longlampbenchmarkpersonalizedlongform} provide user-specific ground-truth datasets, enabling direct comparisons between model outputs and actual user-authored content. Others, such as \textbf{PersoBench} \cite{afzoon2024persobenchbenchmarkingpersonalizedresponse} and \textbf{PrefEval} \cite{anonymous2024do}, test persona consistency and adherence to stated preferences. Although these datasets help quantify personalization gains, they rely on static corpora, lack real-time adaptation, and do not assess privacy-preserving or on-device AI performance.

In personalized recommendation, benchmarks like \textbf{MovieLens} \cite{harper2015movielens} and \textbf{Amazon Reviews} \cite{ni-etal-2019-justifying} evaluate user item predictions but assume centralized data collection, making them unsuitable for privacy-focused, on-device applications. More recent approaches, such as \textbf{Ranking-TAGER} \cite{anonymous2024premium}, integrate user tagging and ranking-based personalization but lack trusted execution environments (TEEs) or mechanisms to prevent fraudulent data submissions. Similarly, \textbf{PersonalLLM} \cite{zollo2024personalllmtailoringllmsindividual} investigates latent user preferences, but does not address privacy concerns or adversarial data manipulation.

\subsection{Limitations of Existing Evaluation Metrics}

Traditional evaluation metrics for personalization primarily assess response accuracy, coherence, and user preference alignment. \textbf{Intrinsic metrics} such as \textbf{BLEU} \cite{papineni-etal-2002-bleu}, \textbf{ROUGE} \cite{lin-2004-rouge}, and \textbf{METEOR} \cite{banerjee-lavie-2005-meteor} measure textual similarity but do not account for the proactive nature of personal AI assistants. More personalized evaluation methods, such as \textbf{Win Rate} \cite{zhang2024personalizationlargelanguagemodels}, \textbf{Hits@K} \cite{mazaré2018trainingmillionspersonalizeddialogue}, and \textbf{Word Mover’s Distance} \cite{ghorbani2019automaticconceptbasedexplanations}, provide better alignment with user preferences, but still focus on \emph{reactive} tasks rather than anticipatory interactions.

\textbf{Extrinsic metrics}, including \textbf{recall}, \textbf{precision}, and \textbf{normalized discounted
cumulative gain} (NDCG) \cite{wang2013theoreticalanalysisndcgtype}, are widely used in recommendation systems but assume centralized data aggregation, making them impractical for privacy-preserving, on-device personalization. Furthermore, these metrics do not quantify the actual \emph{value} of user data in personalization, a critical aspect to balance personalization with privacy.