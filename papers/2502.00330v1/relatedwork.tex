\section{Related work}
\label{sec:related_work}

\begin{wraptable}{l}{0.4\textwidth}
\centering
\vspace{-6mm}
\scalebox{0.8}{
\begin{tabular}{lcccc}\\\toprule  
Method & Exec. &  \multicolumn{3}{c}{Breakdown} \\ 
       & Acc.      &  S & M & C \\\cmidrule(lr){1-1} \cmidrule(lr){2-2} \cmidrule(lr){3-5} 
Direct & 57.7 & 64.0 & 49.4 & 44.1 \\
\textsc{chase} prompt & 60.1 & 67.2 & 51.9 & 40.7\\
\textsc{chase} + \ours \\
\textit{  Round 0} & 59.1 & 65.7 & 51.3 & 42.1 \\
\textit{  Round 1} & 61.2 & 68.6 & 50.6 & 48.3 \\
\textit{  Round 2} & \underline{62.0} & 68.5 & 53.0 & 49.0 \\ \midrule
PEFT (LoRA) \\
 $n_{\text{train}}$ = 256 & 58.2 & 64.0 & 52.2 & 40.7 \\
 $n_{\text{train}}$ = 1024 & 60.2 & 66.6 & 53.0 & 42.1 \\
 $n_{\text{train}}$ = 4096 & 61.3 & 67.5 & 53.9 & 46.2 \\
 $n_{\text{train}}$ = 9428 (All) & \textbf{63.8} & 68.6 & 58.8 & 48.9 \\
\bottomrule
\end{tabular}
}
\caption{Execution accuracy on the BIRD dev set with \texttt{gemini-1.5-pro-001}. \{S, M, C\} refer to the accuracy aggregated across \{Simple, Moderate, Challenging\}-level problems based on assigned difficulty.}
\label{tab:gemini-pro-bird}
\end{wraptable}
\vspace{-3mm}
\textbf{Scaling ICL. }
Before the advent of the long-context LLMs, early efforts in scaling ICL often studied LLMs customized for long context~\citep{li2023context} or required architectural changes assuming white-box model access~\citep{hao2022structured}. However, the tasks considered are often limited, e.g., to conventional, discriminative tasks like sentiment classification rather than generative tasks as considered in this work. Furthermore, these often study LLMs that are merely capable of handling many examples, but their behavior may differ significantly to modern, natively long-context LLMs that may actively \textit{take advantage of} the context -- indeed, both these works show mixed results, even significant performance deterioration when scaling up the number of examples, a phenomenon not seen in modern long-context LLMs like Gemini and Claude. Recent works like \citet{agarwal2024many} and \citet{bertsch2024context}, on the other hand, reported significant gains in scaling ICL to hundreds or more examples and provided important motivation for our work. However, as mentioned in Sec.~\ref{sec:analysis}, these works primarily demonstrate the existence of the benefit from scaling but do not focus on investigate the sources of the gain or improving the cost-effectiveness of many-shot ICL.
Additionally, there have also been works focusing on {applications} of many-shot ICL to multi-modalities \citep{jiang2024many}, LLM jail-breaking \citep{anil2024many}, detecting the risk of capturing incorrect skills \citep{liu2024dual}, and analyzing memorization \citep{golchin2024memorization}.


\textbf{Example selection and generation.}
\ours combines the ``optimize'' and ``generate'' steps, and there have been existing works sharing similar high-level ideas to each of the components. First, the ``optimize'' step can be seen as a method to improve the \textit{data quality} with pruning and selection; in this regard, given that data quality is known to be one of the most influential factors for training LLMs \citep{xia2024less}, many previous works have utilized some flavor of pruning to remove redundant or harmful data samples at different stages of training, including pre-training~\citep{marion2023less} and instruction tuning~\citep{xia2024less}. In ICL, as mentioned in Sec.~\ref{sec:analysis}, given the sensitivity of LLMs to examples, there have been numerous works analyzing prompt sensitivity and proposing \textit{example selection} techniques \citep{zhao2021calibrate, lu-etal-2022-fantastically,zhou2024batch, wan2024teach}. Recent work also explored heuristic-based prompt optimization based on similarity \citep{rubin-etal-2022-learning, liu-etal-2022-makes}, diversity \citep{levy-etal-2023-diverse, xu2024context}, uncertainty \citep{wan-etal-2023-better, wan-etal-2023-universal}, fairness~\citep{zhou-etal-2024-fairer} etc. Our ``generate'' step, on the other hand, aims to acquire high-quality examples with the LLM itself. In this area, STaR~\citep{zelikman2022star} first proposes to bootstrap rationales from LLM with a small number of seed examples, followed by fine-tuning on the rationales that lead to correct predictions; Self-Instruct~\citep{wang-etal-2023-self-instruct} bootstraps LLMs to instruction data. The ``Reinforced ICL'' technique introduced in \citet{agarwal2024many}, upon which this work improves, and several recent works~\citep{chen-etal-2023-self, khattab2023dspy, opsahl2024optimizing} use a similar technique to acquire and refine model-generated examples for ICL. Notwithstanding the similarities described, there are a few crucial differences with respect to these prior works: Almost all ICL works mentioned consider the \textit{few}-shot setup, where selection is made necessary due to the constraint on the number of examples allowed in the context. However, we show that even in the many-shot setup where that constraint is relaxed and example selection is no longer a necessity, it can still be highly beneficial for performance and efficiency. Unlike the few-shot setup, \ours is tailored for the many-shot setup with design decisions inspired by findings in Sec.~\ref{sec:analysis}, such as the implementation of sparsity regularization in the optimization objective to enable scaling.