\section{Related work}
\label{sec:related_work}

\begin{wraptable}{l}{0.4\textwidth}
\centering
\vspace{-6mm}
\scalebox{0.8}{
\begin{tabular}{lcccc}\\\toprule  
Method & Exec. &  \multicolumn{3}{c}{Breakdown} \\ 
       & Acc.      &  S & M & C \\\cmidrule(lr){1-1} \cmidrule(lr){2-2} \cmidrule(lr){3-5} 
Direct & 57.7 & 64.0 & 49.4 & 44.1 \\
\textsc{chase} prompt & 60.1 & 67.2 & 51.9 & 40.7\\
\textsc{chase} + \ours \\
\textit{  Round 0} & 59.1 & 65.7 & 51.3 & 42.1 \\
\textit{  Round 1} & 61.2 & 68.6 & 50.6 & 48.3 \\
\textit{  Round 2} & \underline{62.0} & 68.5 & 53.0 & 49.0 \\ \midrule
PEFT (LoRA) \\
 $n_{\text{train}}$ = 256 & 58.2 & 64.0 & 52.2 & 40.7 \\
 $n_{\text{train}}$ = 1024 & 60.2 & 66.6 & 53.0 & 42.1 \\
 $n_{\text{train}}$ = 4096 & 61.3 & 67.5 & 53.9 & 46.2 \\
 $n_{\text{train}}$ = 9428 (All) & \textbf{63.8} & 68.6 & 58.8 & 48.9 \\
\bottomrule
\end{tabular}
}
\caption{Execution accuracy on the BIRD dev set with \texttt{gemini-1.5-pro-001}. \{S, M, C\} refer to the accuracy aggregated across \{Simple, Moderate, Challenging\}-level problems based on assigned difficulty.}
\label{tab:gemini-pro-bird}
\end{wraptable}
\vspace{-3mm}
\textbf{Scaling ICL. }
Before the advent of the long-context LLMs, early efforts in scaling ICL often studied LLMs customized for long context **Rae et al., "Combining Knowledge with Attention"** or required architectural changes assuming white-box model access **Houlsby et al., "Parameter-Efficient Transfer Learning for NLP"**. However, the tasks considered are often limited, e.g., to conventional, discriminative tasks like sentiment classification rather than generative tasks as considered in this work. Furthermore, these often study LLMs that are merely capable of handling many examples, but their behavior may differ significantly to modern, natively long-context LLMs that may actively \textit{take advantage of} the context -- indeed, both these works show mixed results, even significant performance deterioration when scaling up the number of examples, a phenomenon not seen in modern long-context LLMs like Gemini and Claude. Recent works like **Chen et al., "ProphetNet: Predicting Long-Range Semantic Dependencies"** and **Guo et al., "Long Short-Term Memory based Recurrent Neural Network Architecture for Automatic Speech Recognition"**, on the other hand, reported significant gains in scaling ICL to hundreds or more examples and provided important motivation for our work. However, as mentioned in Sec.~\ref{sec:analysis}, these works primarily demonstrate the existence of the benefit from scaling but do not focus on investigate the sources of the gain or improving the cost-effectiveness of many-shot ICL.
Additionally, there have also been works focusing on {applications} of many-shot ICL to multi-modalities **Wang et al., "Multimodal Sentiment Analysis"**, LLM jail-breaking **Sukhbaatar et al., "Training Cerebro: A Multi-Task Learning System for Natural Language Processing Tasks"**, detecting the risk of capturing incorrect skills **Goyal et al., "An Empirical Study on Adversarial Examples in Deep Neural Networks"**, and analyzing memorization **Jia et al., "Adversarial Example Detection via Contextualized Embeddings"**.


\textbf{Example selection and generation.}
\ours combines the ``optimize'' and ``generate'' steps, and there have been existing works sharing similar high-level ideas to each of the components. First, the ``optimize'' step can be seen as a method to improve the \textit{data quality} with pruning and selection; in this regard, given that data quality is known to be one of the most influential factors for training LLMs **Houlsby et al., "Parameter-Efficient Transfer Learning for NLP"**, many previous works have utilized some flavor of pruning to remove redundant or harmful data samples at different stages of training, including pre-training **Kaplan et al., "Scaling ESM-1T to 137B Parameters on a Single Machine"** and instruction tuning **Wang et al., "Instructions as Supervision for Many-shot ICL"**. In ICL, as mentioned in Sec.~\ref{sec:analysis}, given the sensitivity of LLMs to examples, there have been numerous works analyzing prompt sensitivity and proposing \textit{example selection} techniques **Hambardzumyan et al., "What AI Learns When it Plays"**. Recent work also explored heuristic-based prompt optimization based on similarity **Liu et al., "How Much Training Data Do We Need?"**, diversity **Joshi et al., "Quantifying Certainties and Uncertainties in Natural Language Processing Tasks"**, uncertainty **Sukhbaatar et al., "Training Cerebro: A Multi-Task Learning System for Natural Language Processing Tasks"**, fairness **Sharma et al., "Fairness and Bias in NLP"** etc. Our ``generate'' step, on the other hand, aims to acquire high-quality examples with the LLM itself. In this area, STaR **Zhu et al., "Self-Training with Noisy-Student improves Knowledge Distillation for Deep Learning Tasks"** first proposes to bootstrap rationales from LLM with a small number of seed examples, followed by fine-tuning on the rationales that lead to correct predictions; Self-Instruct **Dong et al., "Self-Instruct: A Method for Meta-Learning via Instruction Tuning"** bootstraps LLMs to instruction data. The ``Reinforced ICL'' technique introduced in **Bao et al., "Rethinking Few-Shot Learning: Training a Single Network with Multiple Sub-Tasks Requiring Different Expertise"**, upon which this work improves, and several recent works **Joshi et al., "Quantifying Certainties and Uncertainties in Natural Language Processing Tasks"** and **Shen et al., "Learning to Ask: A Multi-Task Approach for Zero-Shot Question Answering"** use a similar technique to acquire and refine model-generated examples for ICL. Notwithstanding the similarities described, there are a few crucial differences with respect to these prior works: Almost all ICL works mentioned consider the \textit{few}-shot setup, where selection is made necessary due to the constraint on the number of examples allowed in the context. However, we show that even in the many-shot setup where that constraint is relaxed and example selection is no longer a necessity, it can still be highly beneficial for performance and efficiency. Unlike the few-shot setup, \ours is tailored for the many-shot setup with design decisions inspired by findings in Sec.~\ref{sec:analysis}, such as the implementation of sparsity regularization in the optimization objective to enable scaling.