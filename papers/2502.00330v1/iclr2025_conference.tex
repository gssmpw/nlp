
\documentclass[11pt, a4paper, logo, internal]{googlecloud}

\pdfinfoomitdate 1
\pdftrailerid{redacted}

\makeatletter
\renewcommand\bibentry[1]{\nocite{#1}{\frenchspacing\@nameuse{BR@r@#1\@extra@b@citeb}}}
\makeatother

\usepackage[authoryear, sort&compress, round]{natbib}

\usepackage[utf8]{inputenc} %
\usepackage[T1]{fontenc}    %
\usepackage{url}            %
\usepackage{booktabs}       %
\usepackage{nicefrac}       %
\usepackage{microtype}      %
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{multicol}
\usepackage{hyperref}       %
\usepackage[nameinlink]{cleveref}
\usepackage{bbm}
\usepackage{multirow}
\usepackage{soul}
\usepackage{floatrow}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{blindtext}
\usepackage{tablefootnote}
\usepackage{amsfonts}
\usepackage[flushleft]{threeparttable}
\usepackage{bbding}
\usepackage{xcolor}
\usepackage{xspace}
\usepackage{bm}
\usepackage{arydshln}
\usepackage{enumitem}
\usepackage{setspace}
\usepackage{color}
\usepackage{algorithm}
\usepackage{longtable}

\usepackage[normalem]{ulem}
\usepackage{ulem}
\usepackage[nomargin,inline,marginclue,draft]{fixme}
\usepackage{balance}
\usepackage{verbatim}
\usepackage{diagbox}
\usepackage{changepage}
\usepackage{amssymb}
\usepackage{pifont}
\usepackage{array}   %

\usepackage{kantlipsum, lipsum}
\usepackage{dsfont}

\input{math_commands.tex}

\usepackage{hyperref}

\usepackage{algorithmic}
\usepackage{subcaption}
\usepackage{listings}
\newcommand{\subsparagraph}[1]{\vspace{0.0mm}\noindent\textbf{#1}}
\newcommand{\rparagraph}[1]{\vspace{0.0mm}\noindent\textbf{#1}}

\definecolor{ourred}{HTML}{F19C99}
\definecolor{ourblue}{HTML}{7EA6E0}
\definecolor{plotred}{HTML}{f77189}
\definecolor{plotgreen}{HTML}{33b07a}
\definecolor{plotpurple}{HTML}{cc7af4}
\definecolor{plotmagenta}{HTML}{f565cc}
\definecolor{plotazure}{HTML}{38a9c5}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\scriptsize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2,
    frame=none,
    aboveskip=1pt,
    belowskip=1pt,
}

\newcommand{\ours}{\textcolor{black}{\textsc{bridge}}\xspace} %
\newcommand{\ruoxi}[1]{\textbf{\textcolor{purple}{[Ruoxi: #1]}}}

\title{From Few to Many: Self-Improving Many-Shot Reasoners Through Iterative Optimization and Generation}


\correspondingauthor{\footnotesize{Corresponding authors: \{xingchenw, soarik\}@google.com}\\}

\renewcommand{\today}{}

\author[1]{Xingchen Wan}
\author[1 3*]{Han Zhou}
\author[2*]{Ruoxi Sun}
\author[1]{Hootan Nakhost}
\author[1]{Ke Jiang}
\author[1]{Sercan Ö. Arık}

\affil[1]{Google Cloud AI Research}
\affil[2]{Google DeepMind}
\affil[3]{University of Cambridge}
\affil[*]{Work done at Google Cloud AI Research}


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}


\begin{abstract}
Recent advances in long-context large language models (LLMs) have led to the emerging paradigm of many-shot in-context learning (ICL), where it is observed that scaling many more demonstrating examples beyond the conventional few-shot setup in the context can lead to performance benefits. However, despite its promise, it is unclear what aspects dominate the benefits and whether simply scaling to more examples is the most effective way of improving many-shot ICL. In this work, we first provide an analysis of the factors driving many-shot ICL, and we find that 1) many-shot performance can still be attributed to often a few disproportionately influential examples and 2) identifying such influential examples (``optimize”) and using them as demonstrations to regenerate new examples (``generate") can lead to further improvements. Inspired by the findings, we propose \ours, an algorithm that alternates between the \textit{optimize} step with Bayesian optimization to discover the influential sets of examples and the \textit{generate} step to reuse this set to expand the reasoning paths of the examples back to the many-shot regime automatically. On Gemini, Claude, and Mistral LLMs of different sizes, we show that \ours led to significant improvements across a diverse set of tasks, including symbolic reasoning, numerical reasoning, and code generation.
\end{abstract}


\begin{document}

\maketitle

\section{Introduction}
\label{sec:introduction}
Recent advances in large language models (LLMs) have led to the emergence of in-context learning (ICL) as a promising new learning paradigm \citep{brown2020language}. ICL allows LLMs to learn tasks by simply being presented with a few examples within their context window. A key bottleneck for ICL has been the supported context length of LLMs, but with advancements in novel model architectures, computational infrastructures, and efficient serving methods, state-of-the-art models such as Gemini \citep{reid2024gemini, claude} feature context windows of millions of tokens are overcoming this limitation. Such long-context LLMs open unprecedented avenues for the scaling of ICL -- whereas previous LLMs were limited to processing only up to dozens of examples, current LLMs can now accommodate significantly more examples. More importantly, beyond merely \textit{supporting} a longer context, it has also been shown that scaling more examples led to substantial performance improvements across tasks, creating a new promising paradigm known as \textit{many-shot learning} \citep{agarwal2024many, bertsch2024context}.

Despite these advances, as a nascent paradigm, many-shot ICL still faces several challenges. Long context windows, while powerful, are computationally expensive and introduce significant latency and cost to serving, making it impractical or uneconomical to fully exploit the maximum context length. Some trade-off decisions have to be made under virtually any realistic setting. To leverage the expanded context while controlling the cost and latency under an acceptable limit, existing works typically investigate the experimental setting whereas many examples as costs permit are simply randomly sub-sampled from the pool of all available examples and dumped into the context window. As observed both in prior works \citep{agarwal2024many} and our investigations (Fig.~\ref{fig:analysis_importance}), using the same \textit{number} of examples but with different combinations of examples as demonstrations can lead to dramatically different performance for the \textit{same} task. Across \textit{different} tasks, it has also been noted that the model behaves very differently when the number of examples is scaled up, with some showing a near-monotonic increase in performance as more examples are added, while others experience performance plateaus (e.g., gray line in the leftmost subfigure of Fig.~\ref{fig:analysis_importance}) or even degradation (e.g., red line in the rightmost subfigure of Fig.~\ref{fig:trend_analysis}). Understandably, such variability could pose challenges for practitioners and present obstacles to the application of many-shot learning as an effective paradigm in practice. 

To address these, this paper aims to answer key research questions and proposes an effective novel approach. First, we {analyze} the factors driving the many-shot ICL in the \textit{reinforced ICL} setup common in challenging reasoning tasks where we are provided with a labeled set of inputs and final labels, but the intermediate reasoning path has to be \textit{model-generated}. We find that while ICL performance often increases with the number of shots, that improvement can often be at least partially attributed to a much smaller subset of examples that highly disproportionately contribute to the overall task performance -- as we scale the number of examples, the probability of including these examples also increases. In many cases, if, however, we judiciously isolate these influential examples from the rest, the ``many-shot'' performance can be matched or even exceeded with this sometimes extremely small subset of well-chosen examples alone while adding more examples beyond this set often provides little benefit or even harms performance. We also argue that the findings explain some of the phenomena observed. For example, uneven influence can lead to high variance across different combinations of examples, whereas plateauing performance may occur when we run out of good examples with positive performance influences. One natural implication of these is the efficiency gains by reducing redundancy in many-shot ICL and identifying the optimized subsets. However, the natural next question to ask is whether scaling ICL examples in LLMs can still be beneficial after using up all beneficial examples identified in the previous step. We answer affirmatively to this: to still leverage LLMs' long context, these optimized, high-performing examples may serve as demonstrations to re-generate the more effective reasoning paths rationales on the train set back into the many-shot regime,
which we find to often outperform both the original many-shot examples and the optimized examples themselves. 
Building on these insights, we propose \underline{B}ayesian \underline{R}efinement and \underline{I}terative \underline{D}emonstration \underline{G}eneration for \underline{E}xamples (\ours),
a search algorithm based on Bayesian optimization to improve many-shot ICL and \textit{bridges} the few- and many-shot learning paradigms by automating the ``optimize'' and ``generate'' steps above iteratively. In the ``optimize'' step, it frames the problem as a combinatorial optimization task to discover the optimal set of demonstrations (i.e., \textit{many-to-few}), and in the ``generate'' step, it uses the optimal set as seed examples to generate more examples for further performance enhancement (i.e., \textit{few-to-many}). We demonstrate the effectiveness of \ours on Gemini, Mistral, and Claude models across a diverse range of tasks, including symbolic reasoning, numerical reasoning, and text-to-SQL generation.


\begin{figure}[t]
        \centering
            \centering
        \includegraphics[width=\linewidth]{figs/importance_analysis.pdf}
            \caption{\textit{It does not always take ``many shots'' to achieve many-shot performance -- with judicious selection, it is possible to match or exceed many-shot performance achieved by using all available examples) with much fewer examples:} Accuracy on held-out splits against the number of examples on 3 BBH tasks of \textcolor{gray}{1) overall trendline (fitted with locally weighted smoothing (\textsc{lowess}))}, \textcolor{plotgreen}{2) using top-K most positive examples}, or \textcolor{plotred}{3) using bottom-K least positive examples} {based on the ranking of the importance score described in Sec~\ref{sec:analysis}}. Dotted lines refer to two many-shot baselines: \textcolor{plotpurple}{reinforced ICL}: using input, model-generated reasoning and output of all \textit{correctly-predicted} inputs; \textcolor{plotazure}{All example}: using all available input-output pairs from the train set.  Lines and error bars show mean $\pm$ standard deviation across 3 runs with the ordering of the examples shuffled each trial.
            }
            \label{fig:analysis_importance}
    \end{figure}
\section{What Drives Many-Shot In-Context Learning Performance?}
\label{sec:analysis}

Several previous studies on many-shot ICL~\citep{agarwal2024many, bertsch2024context} have investigated the \textit{presence} of performance gains when we scale the number of examples. A key question that remains unanswered, though, is \textit{what} exactly leads to this improvement. For example, it is unknown whether the benefit is from \textit{scaling examples itself} due to expanded knowledge in the context via more examples or because including more examples increases the probability of selecting a \textit{small subset of disproportionately positive examples}, or a combination of the above with some task specificity. We argue that answering this question is critical -- if the benefit comes from expanded knowledge from including more examples, it suggests that scaling and addressing long-context understanding challenges would dominate the end-to-end performance improvements, and future studies should aim to either include as many examples as practically possible or to imitate the behavior of the LLM as if many examples are included. If, on the other hand, the performance is dominated by a small effective subset of examples, more intelligent selection aiming to reduce redundancies and identify the high-performing subsets should outweigh n\"aively scaling examples. 

Prior work on \textit{few}-shot setup has studied related problems such as the sensitivity to examples in the context ~\citep{zhao2021calibrate,zhou2024batch}. However, it is presently unknown to what extent the findings still scale to the many-shot ICL setup because 1) in the many-shot setup, the influence of each individual example would get much smaller, and 2) it is unknown whether careful example selection in the few-shot setup is still necessary if all examples can be included in the context, since by definition, any high-performing examples are subsets of \textit{all} examples -- if the long-context \textit{LLM} is perfectly capable of identifying the most relevant pieces of information.
If so, aside from other practical concerns like cost and latency, the need for users to manually curate examples may no longer be required.

\rparagraph{Setup.} We aim to shed insights on these important questions. We use the Gemini 1.5 Pro \citep{reid2024gemini}, the state-of-the-art long-context model, to focus on several representative tasks from the BBH tasks. All three tasks, as shown in by the {gray lines} in Fig.~\ref{fig:analysis_importance}, benefit from increasing number of examples to varying degrees (in \texttt{logical\_deduction}, the performance initially increases with the number of examples before plateauing and decreasing; in the other two tasks, there is a noisy but near monotonic improvement throughout) -- we will test the key findings in a much more extensive collection of tasks in Sec.~\ref{sec:experiments}. Given the increased emphasis of modern LLMs on problem-solving and reasoning, we primarily focus on these tasks and adopt the \textit{reinforced ICL}~\citep{agarwal2024many} setup, where we assume the availability of a labeled set of inputs and final labels to be used as many-shot demonstrations, whereas any intermediate outputs or rationales leading to the final answer are model-generated and modifiable (although we also conduct preliminary experiments in alternative setups such as low-resource machine translation in App.~\ref{app:low_resource_translation}). Lastly, we primarily focus on the tasks with the number of available labeled data up to 150-200 samples -- while modern LLMs can often accommodate even more examples in the context, we focus on this range because \textbf{1)} we believe it is the most practically relevant and fills an important gap that neither few-shot ICL nor supervised (parameter-efficient) fine-tuning (which usually requires hundreds to thousands of examples) conventionally address, and \textbf{2)} while possible and of academic value, scaling beyond this range typically starts incurring significant latency and computational overhead, which scales quadratically w.r.t the input length for exact attention and is thus often practically less desired for most real-world use cases.

\rparagraph{Many-shot performance can still be driven by \textit{few} high-performing examples.} A key test that would distinguish and disentangle the two possible sources of benefits from scaling mentioned at the beginning of this section is whether we can attribute, at least to a large extent, the performance improvement from scaling examples back to a carefully selected, high-performing subset of examples with disproportionate influence. Formally, given a set of examples $\mathcal{E} = \{e_j\}_{j=1}^m$ and a performance metric to be maximized $g(\cdot): \mathcal{P}(\mathcal{E}) \rightarrow \mathbb{R}$ (in this case, the accuracy on the validation set 
In this setup, the goal is to find whether we can construct a subset $\mathbf{e}^* = \{e^*_i\}_{i=1}^n \subset \mathcal{E}, \mathrm{s.t.} n \ll m$ such that $g(\mathbf{e}^*)$ is much better than a randomly selected set of examples $\mathbf{e}$ of similar size and/or can even be comparable or better than using the full set of examples $g(\mathcal{E})$ in the context.

\begin{wrapfigure}{r}{0.3\textwidth}
  \centering
    \includegraphics[width=\textwidth]{figs/bootstrap_analysis.pdf}
  \caption{\textit{Good demonstrations lead to better re-generated examples}: trendlines between accuracy and \# examples; note that the re-generated examples by using top-5 \textcolor{plotmagenta}{examples sets} as demonstrations outperform the original examples (\textcolor{gray}{gray line}) by at all parts of the curve.}
  \label{fig:bootstrap_analysis}
\end{wrapfigure}
Whereas a conclusive test would involve enumerating and evaluating $g(\cdot)$ on the power set of $\mathcal{E}$ with $|\mathcal{P}(\mathcal{E})| = 2^{|\mathcal{E}|}$, it is clearly computationally intractable, and a natural simplification is whether we can rank the individual examples in $\mathcal{E}$ with some importance scoring function $M(e)$ to construct example subsets based on the example ranking. While many possible formulations of this are possible, here we define $M(e)$ based on imputed input gradient, which is a concept used in interpretable machine learning for importance attribution~\citep{simonyan2013deep, selvaraju2017grad, sundararajan2017axiomatic, samek2021explaining}. In our context, directly computing input gradient is impossible as we only assume black-box LLMs without gradient backpropagation and $g(\cdot)$ is not necessarily differentiable.
To bypass these issues, we use a sample-efficient Gaussian process regressor (GPR)~\citep{williams1995gaussian, williams2006gaussian} to approximate $g(\cdot)$ with $\hat{g}(\cdot)$, whose input gradient $\nabla_{\mathbf{e}} \hat{g}(e)$ is analytically available: we first randomly sample $n$ subsets of $\mathcal{E}$ to give $\mathbf{e}_{1:n} = [\mathbf{e}_1, ..., \mathbf{e}_n]$, where each subset of examples is represented as a $m$-dimensional binary column vector $\mathbf{e}_i \in \{0, 1\}^m$ with $\mathbf{e}_i^{(j)} = 1$ if the $j$-th example is present or $0$ otherwise; we then evaluate the performance metric of each $\mathbf{e}_i$ to obtain $\mathbf{g}_{1:n} = [g(\mathbf{e}_1), ..., g(\mathbf{e}_n)]$. We then compute and average the input gradient w.r.t. each possible $\{e_j\}_{j=1}^m \in \mathcal{E}$ to obtain an approximated marginalized importance of each example in $\mathcal{E}$\footnote{We refer the readers to App.~\ref{app:grad_score} for detailed derivation of the input gradient-based score.}. Finally, we sort the examples based on $M(e)$ and construct subsets at regular interval from size 1 to $|\mathcal{E}|$ in both ascending and descending directions. Formally, we order $\{e_i\}_{i=1}^n$ such that $M(e_1) \leq M(e_2) \leq ... \leq M(e_n)$; the ascending and descending sets of size $t \in [1, |\mathcal{E}|]$ are given by $\mathbf{a}_t = \mathbf{e}_{1:t}$ and $\mathbf{d}_t = \mathbf{e}_{n-t:n}$ respectively. We then evaluate $g(\cdot)$ on these sets and show the results in Fig.~\ref{fig:analysis_importance}.

As shown, while the gray lines (overall trend lines) often show a positive correlation between performance and an increasing number of examples, we also observe often large gap between the green (top-$k$ examples) and the red (bottom-$k$ examples) lines, suggesting that \textit{different sampling strategies can lead to performance differences that far outweigh the effect from na\"ive scaling} -- e.g., if we establish an ``exchange rate'' between different example sets based on their imputed ordering, we can observe that including around top-10 examples (green lines) examples is as effective as or more effective than the set containing bottom-30 examples in \texttt{geometric\_shapes}. More importantly, in both cases we observe that the green lines, which represent an intelligent selection strategy more sophisticated than random sampling, plateau far before the gray line, suggesting that it is possible to achieve comparable performance with a much smaller number of examples: in \texttt{disambiguation\_qa}, we find that using fewer than 20 top examples is almost already as good as using all 42 examples whereas subsequent additions only led to a few percent of gain, possibly within the margin of error with reshuffling (denoted by error bars on the figure). In the other tasks, we find the performance to peak much earlier and \textit{adding more examples to the context actually led to performance deterioration}. The results suggest \textbf{1)} the fact that it is possible to match or outperform using \textit{all} examples with \textit{fewer}, carefully selected examples means that intelligent example selection is still relevant even with many-shot ICL, echoing findings from the recent works~\citep{li2024retrieval} that retrieval remains valuable for long-context models in the RAG setup; and \textbf{2)} na\"ively including as many examples as possible can be suboptimal both in terms of computing cost and performance -- while it is trivially true for the tasks whose performance does \textit{not} improve monotonically with the number of examples, we show that it can even be true when it apparently \textit{does}: e.g., on \texttt{geometric\_shapes}, the near monotonic improvement overall trend (gray line) may lead someone to conclude that it is beneficial to include as many examples as possible, even though the green line representing intelligent selection saturates and starts to decline earlier.

\rparagraph{Can we still benefit from scaling examples?} 
Experiments above demonstrated the presence of redundancy in many-shot ICL, revealing that using a smaller subset of examples can often reduce this redundancy without sacrificing performance. It is, however, a pruning operation that necessarily \textit{reduce} the input tokens consumed. This leads to a natural question: can we still benefit from scaling through \textit{expanding}? For this question, it is important to recognize that under the reinforced ICL setup, while the inputs and labels in many-shot setups are fixed, the model-generated intermediate outputs, which represent reasoning paths, are modifiable. Given that these intermediate roles are shown to play a critical role in steering model behaviors~\citep{wan2024teach}, it is possible that examples previously identified as non-important or non-beneficial may be again beneficial if the model-generated rationales can be improved.

To achieve so, we reuse the optimized example set from the previous steps as ``seed'' demonstrations for LLMs to re-generate the examples on the train set, the same set from which the optimized examples are generated. As shown by Fig.~\ref{fig:bootstrap_analysis} where we use an example set of different sizes as the seeds, the regeneration step not only increases the number of shots available but also results in better performance across the accuracy versus number-of-demonstrations trade-off.

\section{Methodology}
\label{sec:method}

\begin{figure}[t]
        \centering
        \includegraphics[width=\linewidth, trim={0.5cm 0 0.5cm 0},clip]{figs/many_shot.drawio.pdf}
            \caption{\textit{Overview of \ours}: With a labeled dataset $\mathcal{D}$, exemplified with 6 samples, at the \textcolor{ourblue}{Generation} phase (left half), we generate initial examples by performing LLM inference on the inputs of $\mathcal{D}$ (``Q1-6'') with zero-shot prompting to obtain the initial responses ``A1-6'', which include any intermediate outputs critical for ICL (\textbf{\textcolor{ourblue}{Step 1}}). At \textbf{\textcolor{ourblue}{Step 2}}, consistent with reinforced ICL in \citet{agarwal2024many}, we filter the responses to retain the subset of $\mathcal{D}$ where the LLM predicted correctly to ensure the examples include correct reasoning steps to build $\mathcal{E}_k$, the pool of examples at round $k$ which form the \textit{search space} for the subsequent \textcolor{ourred}{Optimize} step. At the \textcolor{ourred}{Optimize} step (right half), we initialize the proposed Bayesian optimizer by randomly sampling subsets $\mathbf{e}^{(0)} \subseteq \mathcal{E}_k$ as demonstrations to be \textbf{\textcolor{ourred}{Step 3}} evaluated on a held-out validation dataset ($\mathcal{D}$ can be reused for this purpose) to obtain a performance metric \textbf{\textcolor{ourred}{Step 4}}. The Bayesian optimizer (BO) is then updated with \textit{binary vector representations} of $\mathbf{e}$ that led to this validation performance as input and the metric itself as output and suggests a new subset of examples to be used as demonstrations for the next step \textbf{\textcolor{ourred}{Step 5}};  \textbf{\textcolor{ourred}{Steps 4-5}} are repeated (\textit{inner loop}) until the BO budget is exhausted, after which the best evaluated set $\mathbf{e}^*_k$ is returned (\textbf{\textcolor{ourred}{Step 6}}). This set is then used as a demonstration to generate the example pool for the next round $\mathcal{E}_{k+1}$ (\textbf{\textcolor{ourblue}{Step 7}}).}
            \label{fig:main_alg}
\end{figure}
The findings presented above highlight a significant need for improvements that extend beyond simply increasing the number of examples straightforwardly. 
Instead, identifying the most useful example subset $\mathbf{e}^*$ is crucial both for effective cost-performance trade-offs and for better reasoning path generation for more effective examples. 
Based on these insights, we propose \textit{\underline{B}ayesian \underline{R}efinement and \underline{I}terative \underline{D}emonstration \underline{G}eneration for \underline{E}xamples}, or \ours in short (described in Algorithm~\ref{alg:main_alg} and depicted in Fig.~\ref{fig:main_alg}, an optimization algorithm aiming to enhance many-shot ICL with intelligent example selection and iterative example generation. At a high level, the outer loop of \ours is structured in two alternating steps of ``\textcolor{ourred}{optimize}'' and ``\textcolor{ourblue}{generate}''. In the ``\textcolor{ourred}{optimize}'' step, the algorithm focuses on discovering the optimal subset of examples $\mathbf{e}^*$ via a carefully designed (for low complexity, robustness to overfitting and budget control) \textit{Bayesian optimization algorithm} that naturally leverages the GPR surrogate used in Sec.~\ref{sec:analysis}; in the ``\textcolor{ourblue}{generate}'' step, \ours utilizes the optimized subset as seed demonstrations to align the model with the best-performing examples seen so far to re-generate new reasoning paths as an integral part of more effective examples back to the many-shot regime to leverage the long context. The two steps are iteratively repeated to progressively refine the examples. 

\rparagraph{\textcolor{ourred}{Optimize} step.} While effective, directly using the importance scoring approach from Sec.~\ref{sec:analysis} to identify the $\mathbf{e}^*$ would require us to set the optimal number of examples to select $||\mathbf{e}^*||$ as a hyperparameter, the optimal value of which is task specific. Furthermore, a key motivation for the importance-based ranking in Sec.~\ref{sec:analysis} is to attribute performance to \textit{individual} examples; this is, however, not required if we simply would like to find an optimal \textit{subset} $\mathbf{e}^*$. To nevertheless use the GPR surrogate in Sec.~\ref{sec:analysis} which has shown an impressive sample-efficient, modeling capability, we propose to use Bayesian optimization (BO)~\citep{garnett2023bayesian, frazier2018tutorial}, a sample-efficient black-box optimization algorithm that has recently shown promise in combinatorial problems~\citep{daulton2022bayesian, wan2021think}; it naturally synergizes with the GP surrogate yet automatically strikes a balance between exploration and exploitation to discover $\mathbf{e}^*$ without requiring us to set $||\mathbf{e}^*||$ beforehand, {although \ours is also compatible with alternative methods as a drop-in replacement of the ``Optimize'' step, which we investigate in detail in App.~\ref{app:ablation_studies}.}

\begin{figure}[t!]
\begin{minipage}{0.50\textwidth}
\begin{algorithm}[H]
\begin{footnotesize}
	    \caption{\ours.}
	    \label{alg:main_alg}
	\begin{algorithmic}[1]
		\STATE \textbf{Input}: train set $\mathcal{D}_{\mathrm{t}}$, validation set $\mathcal{D}_{\mathrm{v}}$ (\textbf{can be the same as the train set}), number of iteration rounds $K \in \mathbb{N}$ (\textit{outer-loop}), evaluation budget for BO per iteration $n_{\mathrm{eval}}$ (\textit{inner-loop}).
		\STATE \textbf{Output}: Optimized set of examples ${\mathcal{E}}^*$.
		\STATE \textcolor{ourblue}{\textbf{[Generate]}} Generate the pool of initial examples $\mathcal{E}_0$ by predicting the LLM on the \textbf{train} set with zero-shot prompting or few-shot prompting (if handwritten few-shot demonstrations are available). Each instance in $\mathcal{E}_0$ is a concatenation of \{input, model-generated reasoning, final outputs\} for the subset of the train set where the model obtained the correct prediction.
		\FOR{${k} \in \{1, ..., K\}$ (\textcolor{gray}{\textbf{Outer} loop)}}
		\STATE \textcolor{ourred}{\textbf{[Optimize]}} Run Bayesian optimization (calling subroutine Algorithm~\ref{alg:bo} on the \textbf{validation set} to obtain $\mathbf{e}^*_k \leftarrow \textcolor{purple}{\mathrm{BayesOpt}}(n_{\mathrm{eval}}$=$n_{\mathrm{eval}}, \mathcal{E}$=$\mathcal{E}_k)$.
		\STATE \textcolor{ourblue}{\textbf{[Generate]}} {\textbf{Re-generate} examples $\mathcal{E}_k$ by re-predicting the LLM on the \textbf{train} set, but with the optimized examples $\mathbf{e}^*_k$ from the previous step as demonstrations; the \{inputs, model-generated reasoning, output\}-tuples are concatenated to form the new set of examples $\mathcal{E}_k$ for the next \textcolor{ourred}{[Optimize]} step.}
		\ENDFOR
		\RETURN Optimized example set $\mathcal{E}^*$ after $K$ rounds.
	\end{algorithmic}
\end{footnotesize}
\end{algorithm}
\end{minipage}
\hfill
\begin{minipage}{0.47\textwidth}
\begin{algorithm}[H]
\begin{footnotesize}
	    \caption{Budget-controlled BO subroutine with random scalarization (\textcolor{purple}{
	    $\mathrm{BayesOpt}$}).}
	    \label{alg:bo}
	\begin{algorithmic}[1]
		\STATE \textbf{Input}: Evaluation budget for BO per iteration $n_{\mathrm{eval}}$ (\textit{inner-loop}), full set of available samples $\mathcal{E}$, number of random initializations $n_{\mathrm{init}} = \min(16, n_{\mathrm{eval}} / 2)$.
		\STATE \textbf{Output}: Optimized set of examples ${\mathbf{e}}^* \subseteq \mathcal{E}_t$.
		\STATE Randomly generate $n_{\mathrm{init}}$ subsets $\mathbf{e}_{1:n_{\mathrm{init}}} := \{\mathbf{e_1}, ..., \mathbf{e}_{n_{\mathrm{init}}}\}$ with each $\mathbf{e} \sim \{0, 1\}^{|\mathcal{E}_t|}$ s.t. $|\mathbf{e}| \sim \mathrm{Uniform}(1, |\mathcal{E}_t|)$.
		\STATE Evaluate $\mathbf{g}_{1:n_{\mathrm{init}}} = [g(\mathbf{e}_1, ..., \mathbf{e}_{n_{\mathrm{init}}}]^{\top}$ and fit a $\mathcal{GP}$ on $\mathbf{e}_{1:n_{\mathrm{init}}}$ as inputs and $\mathbf{g}_{1:n_{\mathrm{init}}}$ as outputs. Set $\mathcal{D}_0 \leftarrow \{ \mathbf{e}_{1:n_{\mathrm{init}}}, \mathbf{g}_{1:n_{\mathrm{init}}}\}$
		\FOR{${t} \in \{n_{\mathrm{init}}, ..., n_{\mathrm{eval}}\}$ (\textcolor{gray}{\textbf{Inner} loop)}}
		\STATE Sample a random scalarization value $\beta_t \sim \mathrm{Uniform}(0, 1)$ and compute the scalarized objective \textit{of this iteration} $h_t(\mathbf{e}) = \mathrm{TCH}(\beta_t, [g(\mathbf{e}), |\mathbf{e}|])$. 
		\STATE  Compute $\mathbf{h}_{1:t}$ for all previously evaluated points $\mathcal{D}_{t-1}$, fit a GPR $\mathcal{GP}_t$ on $[\mathbf{e}_{1:t}, \mathbf{h}_{1:t}]$ and obtain the next configuration to evaluate by maximizing the \textit{acquisition function} $\alpha(\cdot)$: $\mathbf{e}_t = \arg\max_{\mathbf{e} \subseteq \mathcal{E}} \alpha(\mathbf{e}  \mid \mathcal{GP}_t)$.%
		\STATE Evaluate $g(\cdot)$ with $\mathbf{e_t}$ and augment $\mathcal{D}_t \leftarrow \mathcal{D}_{t-1} \cup (\mathbf{e_t}, g(\mathbf{e}_t))$
		\ENDFOR
	\RETURN $\mathbf{e^*} = \arg\max_{\mathbf{e} \in \mathcal{D}} g(\mathbf{e})$.
	\end{algorithmic}
\end{footnotesize}
\end{algorithm}
\end{minipage}
\end{figure}

Instead of consuming the entire query budget by sampling randomly, as illustrated by Algorithm~\ref{alg:bo}, BO only requires some initializing samples to warm-start (Step 3). 
Afterward, it guides exploration by iteratively (re)fitting a GPR with the previously observed inputs and outputs so far. Formally, at iteration $t \in [1, T]$, we have evaluated $g(\cdot)$ $t$ times at $\mathbf{e}_{1:t} = [\mathbf{e}_1, ..., \mathbf{e}_t]^{\top}$ with observed values $\mathbf{g}_{1:t}$. Whereas a straightforward application of BO would directly train a GP on $[\mathbf{e}_{1:t}, \mathbf{g}_{1:t}]$ as inputs-outputs and perform BO with $g(\cdot)$ as the objective function directly, a subtle but important distinction here is that our goal is to identify a subset $\mathbf{e}^*$ that, \textit{when used as demonstrations on the train set}, {generates to the most effective examples on the validation set}, rather to simply find the highest-performing $\mathbf{e}^*$ on the validation set. 
While we expect the two objectives to be correlated (i.e., $\mathbf{e}$ that led to high validation performance is also likely to generate better samples on the train set), we also empirically find it is desirable to encourage $\mathbf{e}^*$ to have a smaller cardinality akin to a $\ell_0$ regularization to reduce overfitting on the validation set and to discourage memorization in subsequent generations from the previous example set $\mathcal{E}_{t-1}$ of which $\mathbf{e}^*$ is a subset. 
To achieve so, we augment the performance maximization $\max g(\mathbf{e})$ with a \textit{sparsity objective} which counts the number of non-zero elements in $\mathbf{e}$: $\min \sum_j e^{(j)}$ -- this transforms the problem into a \textit{bi-objective} optimization problem {, where instead of maximizing for the validation performance only, we also encourage sparsity as regularization. Practically,} we solve the problem with \textit{random scalarization}~\citep{paria2020flexible, knowles2006parego}. 
Specifically, as hinted in Step 7 of Algorithm~\ref{alg:bo}, at each BO iteration, we first sample a random scalar $\beta_t \sim \mathrm{Unif}(\beta_{\mathrm{LB}}, \beta_{\mathrm{UB}})$ that determines the weight of the performance objective $g(\cdot)$ of the $t$-th BO iteration (the weight of the sparsity objective is given by $1-\beta_t$) and $\{\beta_{\mathrm{LB}}, \beta_{\mathrm{UB}} \}$ denote the lower and upper bounds of the weight for $g(\cdot)$ which are set to \{0.25, 1\} by default. With this $\beta_t$, we then aggregate the vector objective $[g(\mathbf{e}), \sum_{j} e^{(j)}]$ back to a scalar $h_t(\mathbf{e})$ via \textit{Tchebyshev scalarization} (TCH), a theoretically well-founded scalarization scheme common in multi-objective optimization~\citep{chugh2020scalarizing,steuer1983interactive,bowman1976relationship} given by:
\begin{equation}
h_t(\mathbf{e}) = \max\Big\{ \beta_t \big(g(\mathbf{e}) - \max\{g(\mathbf{e_1}),..., g(\mathbf{e_t})\} \big), -(1-\beta_t) \sum_{j} e^{(j)} \Big\},
\end{equation}
where the minus sign before the last term is to cast the sparsity objective as maximization. We opt for random scalarization that differs from step to step instead of a fixed scalarization weight or any hard constraint on $\sum_j e^{(j)}$ to retain the flexibility of exploring the entire Pareto front since the exact relation between the number of samples and performance can differ across tasks. Since $\beta_t$ is in general different for each $t$, we then compute $\mathbf{h}_{t} = [h_t(\mathbf{e}_1), ..., h_t(\mathbf{e}_t )]$ on previously evaluated outputs and fit a GP on $\mathcal{H}_t := [\mathbf{e}_{1:t}, \mathbf{h}_{t}]$, which induces a Gaussian posterior predictive distribution with mean and variance at any $\mathbf{e} \subseteq \mathcal{E}$ (we use $\hat{h}_t$ to denote that it is the GP approximation of the actual function $h_t$):
\begin{equation}
    \mathbb{E}_{\hat{h}_t(\mathbf{e}) |\mathcal{H}_t}[\hat{h}_t(\mathbf{e})] = \mathbf{k}_t (\mathbf{K} + \eta^2\mathbf{I})^{-1} \mathbf{h}_t, \, \mathbb{V}_{\hat{h}_t(\mathbf{e}) |\mathcal{H}_t}[\hat{h}_t(\mathbf{e})] = k(\mathbf{e}, \mathbf{e})-\mathbf{k}_t(\mathbf{K} + \eta^2\mathbf{I})^{-1}\mathbf{k}_t^{\top},
    \label{eq:gp}
\end{equation}
where $\mathbf{k}_t = [k(\mathbf{e}, \mathbf{e}_1), ..., k(\mathbf{e}, \mathbf{e}_t)]$ and $k(\cdot, \cdot)$ is the covariance function of the GP (we use Matern 2.5 by default) which measures the similarity between two inputs -- in our case, it is a function of the number of overlapping examples between two subsets of examples $\mathbf{e}, \mathbf{e}' \subseteq \mathcal{E}$. To select the next configuration to evaluate $\mathbf{e}_{k}$, the BO optimizes an \textit{acquisition function}, another key component of BO that automatically trade-off exploration and exploitation. At each inner-loop BO iteration, we choose the maximizer of the \textit{expected improvement} (EI)~\citep{zhan2020expected} for the next iteration $\mathbf{e}_t$: $ \mathbf{e}_t = \argmax_{\mathbf{e} \subseteq \mathcal{E}}\alpha(\mathbf{e}) = \argmax_{\mathbf{e} \subseteq \mathcal{E}}\mathbb{E}_{\hat{h}_t(\mathbf{e}) |\mathcal{H}_t}\big[\max\{0, \hat{h}_t(\mathbf{e}) - \max_{t'\in\{1,t\}}\hat{h}_t(\mathbf{e_{t'}})\}\big].$

\rparagraph{\textcolor{ourblue}{Generate} step.} At each \textit{outer-loop} round $k \in \{1, ..., K\}$, given the optimized $\mathbf{e}^*_k$ as demonstrations, we regenerate and replace the example pool with the correct predictions and their generated rationales $\mathcal{E}_k \leftarrow f_{\mathrm{LLM}}(\mathcal{D}_t, \mathbf{e}^*_k \subseteq \mathcal{E}_{k-1})$ for subsequent \textcolor{ourred}{optimize} step. 


\section{Experiments}
\label{sec:experiments}
\rparagraph{Model and evaluation data.} We conduct experiments on an extensive collection of tasks requiring a different set of skills task difficulty on two Gemini 1.5 models (\texttt{gemini-1.5-pro-001} and \texttt{gemini-1.5-flash-001}) {while also testing key findings on Mistral family of models: Mistral NeMo (\texttt{mistral-nemo-12b}) and Mistral Large (\texttt{mistral-large-2407}), and Claude 3.5 Sonnet}: \textbf{1)} BIG-Bench Hard (BBH) tasks encompassing a wide range of challenging numerical reasoning, commonsense problem-solving, logical deduction and tabular reasoning tasks -- we particularly focus on the subset of 16 BBH tasks where the model performances have not saturated; \textbf{2)} Hendryck's MATH \citep{hendrycks2021measuring}, a challenging numerical reasoning dataset; \textbf{3)} GSM-Hard~\citep{gao2022pal}, a more challenging variant of the classical grade-school GSM-8K~\citep{cobbe2021training} with the numbers in the questions replaced with much larger and rarer ones. To further probe the utility of many-shot learning and \ours in coding tasks, we also experiment on \textbf{4)} BIRD~\citep{li2024can}, a challenging large-scale text-to-SQL generation benchmark where the LLM has to generate SQLite programs from natural language instructions that are executed on real-world databases. For all datasets, when official train-test split is not available, we randomly split the data into train and test splits; unless stated otherwise, a single unified train split is used both for the generation of demonstrations and is reused for validation (i.e., the objective of the \textcolor{ourred}{optimize} step in Algorithm~\ref{alg:main_alg}; the test splits are held-out and only used for evaluation of the algorithm. 
We refer the readers to App.~\ref{app:implementation_details} for detailed descriptions, prompt templates, and evaluation protocols used.
\begin{table}[t!]
\caption{{Test} accuracy of \texttt{gemini-1.5-{pro}-001} on selected BBH tasks with different prompting approaches. ``All'' refers to using the \textit{entire} labeled set of 75 examples as demonstrations (``Direct'': using all input-final answer pairs \textit{without} any model-generated content; ``CoT'': using all input-rationale-final answer triplet, where the rationale is model-generated; {``Infill'': using all input-rationale-final answer triplet, where the rationale is \textit{filled in} by prompting the model to generate the intermediate steps given the inputs \textit{and} ground-truth answers}); ``{Reinf. ICL}'' refers to reinforced many-shot ICL where we include the subset of train set that the LLM answered correctly under zero-shot as demonstrations; ``{Iterative Reinf.}'' refers to the iterative variant of reinforced many-shot ICL where we directly use all the generated correct examples from the previous round as demonstrations for the next round without the \textcolor{ourred}{optimize} step, and the different columns of \ours show the evolution of test accuracy at different milestones: e.g., \textcolor{ourred}{1\textsc{o}} refers the results with optimized $\mathbf{e}^*_1$ from initial examples $\mathcal{E}_0$ as demonstrations (in general, we have $\mathbf{e}^*_k \subseteq \mathcal{E}_{k-1}$), and \textcolor{ourblue}{1\textsc{g}} refers to the results using $\mathcal{E}_1$ generated by re-evaluating the train set with $\mathbf{e}^*_1$ as demonstrations. All results shown are averaged across 4 random seeds with the standard deviation (stdev) denoted in the subscript. The best and second-best results along each row are \textbf{bolded} and \underline{underlined}, respectively (ties are broken by favoring the result with lower stdev).
}
\label{tab:gemini-pro-bbh}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{lccccccccccc}
\toprule
Tasks  & \multicolumn{3}{c}{All}         & Reinf.  & \multicolumn{2}{c}{\textcolor{black}{Iterative}} & \multicolumn{5}{c}{\ours}  \\
& Direct & CoT & {Infill} & ICL & \multicolumn{2}{c}{Reinf.} &\multicolumn{5}{c}{\textit{(Ours)}} \\
\# Iterations & - & 0  & {0} & 0 & 1 & 2 & \textcolor{ourred}{1\textsc{o}} & \textcolor{ourblue}{1\textsc{g}} & \textcolor{ourred}{2\textsc{o}} & \textcolor{ourblue}{2\textsc{g}} & \textcolor{ourred}{3\textsc{o}} \\
\cmidrule(lr){1-1} \cmidrule(lr){2-4} \cmidrule(lr){5-5} \cmidrule(lr){6-7} \cmidrule(lr){8-12} 
causal\_judgement & 61.0\textsubscript{4.7}   & 62.7\textsubscript{2.1} & {68.0\textsubscript{2.8}} & 66.3\textsubscript{4.8} & 68.7\textsubscript{1.9} & 69.3\textsubscript{2.7} & 68.3\textsubscript{1.5} & 62.7\textsubscript{1.6} & 59.7\textsubscript{1.5} & \textbf{72.0}\textsubscript{0.0} & \underline{70.0}\textsubscript{2.0}\\
date\_understanding & 87.2\textsubscript{2.0}   & 86.0\textsubscript{2.3} & {94.8\textsubscript{1.8}}    & 88.8\textsubscript{2.5} & 93.0\textsubscript{1.0} & 94.9\textsubscript{1.3} & 92.2\textsubscript{1.5} & \textbf{97.0}\textsubscript{0.7} & 94.8\textsubscript{1.9} & 95.0\textsubscript{1.2} & \underline{95.5}\textsubscript{1.8}\\
disambiguation\_qa  &  74.2\textsubscript{2.2}  & 63.3\textsubscript{1.1} & {72.3\textsubscript{2.0}}  & 76.8\textsubscript{2.4} & 74.6\textsubscript{1.4} & 75.1\textsubscript{1.5} & 71.8\textsubscript{2.4} & 77.5\textsubscript{3.6} & \underline{80.5}\textsubscript{1.8} & \textbf{81.3}\textsubscript{2.9} & 78.8\textsubscript{1.5}\\ 
dyck\_languages  &  16.8\textsubscript{2.9} & 39.0\textsubscript{3.7} & {24.5\textsubscript{2.9}}   & 55.5\textsubscript{3.6} & 64.4\textsubscript{5.3} & 74.4\textsubscript{3.6} & 49.2\textsubscript{2.7} & 76.2\textsubscript{3.8} & \textbf{80.0}\textsubscript{2.7} & \underline{77.5}\textsubscript{1.1} & 76.8\textsubscript{3.8}\\
formal\_fallacies &  82.8\textsubscript{3.7} & 86.8\textsubscript{1.3} & {84.3\textsubscript{2.8}}   & 86.2\textsubscript{1.1} & 88.1\textsubscript{0.9} & 89.4\textsubscript{1.4} & 86.0\textsubscript{2.1} & 85.0\textsubscript{2.5} & \textbf{90.8}\textsubscript{2.3} & \underline{90.8}\textsubscript{2.8} & 88.2\textsubscript{2.3}\\
geometric\_shapes  &  69.0\textsubscript{4.1} & 61.8\textsubscript{4.2}  & {73.5\textsubscript{2.3}}  & 80.2\textsubscript{2.8} & 81.0\textsubscript{2.5} & 82.3\textsubscript{1.7} & 78.5\textsubscript{2.1} & 82.5\textsubscript{3.6} & 89.2\textsubscript{3.8} & \textbf{92.3}\textsubscript{1.1} & \underline{89.2}\textsubscript{0.8}\\
hyperbaton  &  70.8\textsubscript{4.1} & 93.2\textsubscript{3.1}  & {89.5\textsubscript{2.6}}   & 90.2\textsubscript{1.1} & 91.5\textsubscript{2.2} & 86.2\textsubscript{2.5} & 96.5\textsubscript{0.9} & 94.2\textsubscript{1.5} & 94.8\textsubscript{2.8} & \underline{96.5}\textsubscript{0.5} & \textbf{97.2}\textsubscript{0.4}\\
logical\_deduction (7) &  56.8\textsubscript{4.4}  & 63.0\textsubscript{7.4} & {69.8\textsubscript{5.9}}   & 65.8\textsubscript{3.5} & 68.9\textsubscript{2.6} & 69.5\textsubscript{2.9} & 70.2\textsubscript{1.5} & 70.8\textsubscript{4.5} & \textbf{71.7}\textsubscript{3.7} & \underline{71.5}\textsubscript{1.8} & 69.2\textsubscript{2.2}\\
movie\_recommendation & \textbf{75.0}\textsubscript{1.0}  & 63.7\textsubscript{2.2}   & {68.0\textsubscript{2.8}}   & 65.2\textsubscript{1.6} & 68.8\textsubscript{2.0} & 82.0\textsubscript{1.9} & 67.0\textsubscript{1.2} & {69.5}\textsubscript{0.5} & 69.3\textsubscript{3.1} & \underline{72.8}\textsubscript{1.8} & 67.0\textsubscript{1.2}\\
multistep\_arithmetic\_two  & 86.5\textsubscript{2.2}   & 96.8\textsubscript{0.8} & {88.8\textsubscript{1.8}} & 96.5\textsubscript{0.5} & 95.9\textsubscript{0.8} & 94.5\textsubscript{1.3} & 96.2\textsubscript{0.8} & 94.5\textsubscript{1.1} & \underline{97.0}\textsubscript{0.7} & \textbf{98.0}\textsubscript{0.7} & 96.8\textsubscript{1.8}\\
object\_counting & 92.5\textsubscript{2.3}    &84.8\textsubscript{4.3}  & {95.3\textsubscript{1.3}}  & 95.5\textsubscript{0.9} & 95.8\textsubscript{2.2} & 95.1\textsubscript{1.6} & \textbf{96.2}\textsubscript{0.4} & \underline{96.0}\textsubscript{1.9} & 94.5\textsubscript{1.1} & 94.2\textsubscript{0.4} & 95.0\textsubscript{0.7}\\
ruin\_names  & 85.2\textsubscript{3.1}   &85.5\textsubscript{2.1}  & {89.8\textsubscript{1.6}} & 89.8\textsubscript{1.9} & 88.6\textsubscript{1.5} & \underline{90.5}\textsubscript{0.9} & \textbf{90.8}\textsubscript{1.1} & 88.8\textsubscript{1.7} & 89.2\textsubscript{1.5} & 88.8\textsubscript{2.4} & 90.3\textsubscript{0.8}\\
salient\_translation\_error\_detection  & 66.0\textsubscript{2.4} &56.2\textsubscript{1.5} & {72.5\textsubscript{0.5}}  & 69.0\textsubscript{1.6} & 73.8\textsubscript{1.1} & 73.4\textsubscript{1.3} & 68.8\textsubscript{0.8} & 71.0\textsubscript{0.7} & 69.5\textsubscript{2.2} & \underline{74.0}\textsubscript{0.7} & \textbf{74.5}\textsubscript{1.1}\\
snarks     & 94.1\textsubscript{1.8}   &95.5\textsubscript{2.3}  & {95.1\textsubscript{0.6}}    & 92.7\textsubscript{3.2} & 94.3\textsubscript{1.9} & 95.5\textsubscript{1.5} & 93.4\textsubscript{3.0} & 95.8\textsubscript{0.0} & 95.1\textsubscript{1.6} & \underline{96.9}\textsubscript{1.5} & \textbf{97.6}\textsubscript{1.8}\\
sports\_understanding & 93.8\textsubscript{1.3} &94.2\textsubscript{1.3} & {95.0\textsubscript{0.7}} & 93.0\textsubscript{1.4} & 94.1\textsubscript{0.9} & 95.4\textsubscript{1.2} & 92.8\textsubscript{1.9} & \textbf{97.0}\textsubscript{1.2} & \underline{96.2}\textsubscript{0.8} & 95.8\textsubscript{0.4} & 95.8\textsubscript{0.8}\\
tracking\_shuffled\_objects (7) & 76.0\textsubscript{7.2} & 52.5\textsubscript{2.1} & {64.3\textsubscript{2.8}} & 62.3\textsubscript{4.2} & 64.5\textsubscript{2.2} & 65.5\textsubscript{4.6} & 95.8\textsubscript{0.4} & 95.0\textsubscript{1.2} & \textbf{100.0}\textsubscript{0.0} & 97.0\textsubscript{0.7} & \underline{99.5}\textsubscript{0.5}\\
\cmidrule(lr){1-1} \cmidrule(lr){2-4} \cmidrule(lr){5-5} \cmidrule(lr){6-7} \cmidrule(lr){8-12} 
\textit{Average}   & 74.22 & 74.06    & {78.70}    & 79.61 & 81.61 & 82.37 & 82.11 & 84.61 & 85.77 & \textbf{87.13} & \underline{86.33}\\
\bottomrule
\end{tabular}
}
\end{table}
\begin{table}[t]
\caption{{Test} accuracy of \texttt{gemini-1.5-{pro}-001} on MATH and GSM-Hard datasets. Refer to the captions of Table~\ref{tab:gemini-pro-bbh} for detailed explanations.
}
\label{tab:gemini-pro-math}
\centering
\resizebox{0.8\textwidth}{!}{
\begin{tabular}{lcccccccc}
\toprule
Tasks                                       & Reinf.  & \multicolumn{2}{c}{\textcolor{black}{Iterative}} & \multicolumn{5}{c}{\ours}  \\
& ICL & \multicolumn{2}{c}{Reinf.} &\multicolumn{5}{c}{\textit{(Ours)}} \\
    \# Iterations & 0 & 1 & 2 & \textcolor{ourred}{1\textsc{o}} & \textcolor{ourblue}{1\textsc{g}} & \textcolor{ourred}{2\textsc{o}} & \textcolor{ourblue}{2\textsc{g}} & \textcolor{ourred}{3\textsc{o}} \\
\cmidrule(lr){1-1} \cmidrule(lr){2-2} \cmidrule(lr){3-4} \cmidrule(lr){5-9} 
Hendryck's MATH                  & {63.75\textsubscript{0.5}} & {63.60\textsubscript{0.9}} & {63.60\textsubscript{1.1}} & {62.60\textsubscript{1.3}} & {63.00\textsubscript{1.2}} & {63.85\textsubscript{1.1}} & {\textbf{64.65}\textsubscript{0.3}} & {\underline{64.40}\textsubscript{0.9}} \\
GSM-Hard                         & {69.88\textsubscript{0.8}}  & {69.84\textsubscript{0.4}}  & {69.33\textsubscript{0.3}}  & {71.89\textsubscript{0.4}} & {71.31\textsubscript{0.4}} & {71.81\textsubscript{0.4}} & {\textbf{73.32}\textsubscript{0.4}} & {\underline{72.50}\textsubscript{0.6}} \\
\bottomrule
\end{tabular}
}
\end{table}

\rparagraph{Experimental setup.} For all tasks, we run \ours with $K=3$ rounds (i.e., the number of \textit{outer-loop} iterations in Algorithm~\ref{alg:main_alg}) and within each round, we allow for $n_{\mathrm{eval}}=32$ evaluations on the validation set (i.e., the number of \textit{inner-loop} iterations in Algorithm~\ref{alg:bo}) {and we report the results at the end of each ``optimize'' and ``generate'' steps to visualize the iteration process}. For baselines, we consider \textbf{1)} using all provided examples and we consider three variants: a) using query-target \textit{only} without any generated rationales (Direct), b) first prompt the LLM to generate rationales and answers, and use the concatenation of query-rationale-target as demonstrations, \textit{regardless of whether the rationale led to the correct answer} (CoT), and c) prompting the LLM with both the query \textit{and the final, ground-truth answer} to \textit{fill in} the rationale -- this technique has been variously referred to as, e.g., \textit{infilling}~\citep{hu2023amortizing}, \textit{rationalization}~\citep{zelikman2022star}, or more generally, \textit{teacher forcing}~\citep{chen2025reliable} due to its conceptual similarity to teacher forcing in recurrent neural network (RNN) training~\citep{lamb2016professor} (Infill); \textbf{2)} reinforced ICL~\citep{agarwal2024many}, where all available input-output pairs from the correct predictions on the train set with zero-shot prompting are used; and \textbf{3)} an iterative variant of reinforced ICL which can also be seen as \ours without the \textcolor{ourred}{optimize} step: while we repeat the generation process on the train set $K=3$ times, we do not first aim to select the optimized subset but instead use the entire generated examples from the previous step as demonstrations $\mathcal{E}_k \leftarrow f_{\mathrm{LLM}}(\mathcal{D}_t, \mathcal{E}_{k-1})$. 

\rparagraph{Results and discussions.} We show the test accuracy on the BBH tasks in Table~\ref{tab:gemini-pro-bbh} (Gemini 1.5 Pro; the \textit{number of examples} for each entry in Table~\ref{tab:gemini-pro-bbh} are shown in Table~\ref{tab:gemini-pro-bbh-n_demo} in App.~\ref{app:num_examples}), Table~\ref{tab:gemini-flash-bbh} (Gemini 1.5 Flash), {Tables~\ref{tab:mistral-bbh} and \ref{tab:mistral-nemo-bbh} (Mistral Large and Mistral NeMo) and Table~\ref{tab:claude-bbh} (Claude 3.5 Sonnet)}. On MATH and GSM-Hard datasets, we show the Gemini 1.5 Pro results in Table~\ref{tab:gemini-pro-math}. We observe that na\"ive many-shot scaling is in general ineffective and is outperformed by reinforced ICL; \ours, however, outperforms the base reinforced many-shot ICL by more than 7\% and 3\% on Tables \ref{tab:gemini-pro-bbh} and \ref{tab:gemini-flash-bbh}, respectively,
and the extent of outperformance over the ``Iterative reinforced ICL'', which leads to moderate improvements on BBH with Gemini Pro but no significant performance gains on MATH, GSM-Hard and BBH with Gemini Flash. Both demonstrate that \textcolor{ourred}{optimize} is an integral component of \ours and implicitly validates the findings in Sec.~\ref{sec:method} that \textit{many-shot performance can be driven by few disproportionately influential examples}, which constitutes a core motivation for our method. Barring some expected task-specific fluctuations, in both Tables~\ref{tab:gemini-pro-bbh} and \ref{tab:gemini-flash-bbh}, we also observe consistent and monotonic performance improvement as \ours progresses over the successive \textcolor{ourred}{optimize} and \textcolor{ourblue}{generate} steps, eventually peaking at \textcolor{ourblue}{2\textsc{g}} on Gemini Pro and \textcolor{ourred}{2\textsc{o}} on Gemini Flash (although the performance difference between \textcolor{ourblue}{2\textsc{g}} and  \textcolor{ourred}{2\textsc{o}} on Gemini Flash is negligible and likely within margin of error) -- based on the overall results, we recommend stopping \ours at \textcolor{ourblue}{2\textsc{g}} or \textcolor{ourred}{2\textsc{o}}. Interestingly, we observe that in both cases, an additional \textcolor{ourred}{optimize} step (i.e., the \textcolor{ourred}{3\textsc{o}} column) somewhat degrades performance -- our hypothesis is that as \ours progresses, the generated examples become more aligned with the optimal behavior and the degree of redundancy as we observed in Sec.~\ref{sec:analysis} reduces, and it becomes more difficult to squeeze the number of examples without harming task performance -- indeed, from Fig.\ref{fig:trend_analysis}
where we concretely analyze the behavior of the LLM in different tasks by evaluating the LLM under random subsets of $\mathcal{E}_0, ..., \mathcal{E}_2$ as demonstrations in held-out splits, we observe that the benefit from na\"ively scaling examples under the base reinforced many-shot ICL (denoted by \textcolor{plotred}{red} lines) can be highly unstable across tasks: from the different subfigures of Fig.~\ref{fig:trend_analysis}, we find the performance to consistently improve with more examples (leftmost), improve then plateau (middle two figures) and even simply deteriorate with more examples (rightmost) -- whereas the latter two cases are direct manifestations that not all examples contribute positively to many-shot ICL and na\"ively scaling examples is suboptimal, we note that it remains true even in the former case where there is an apparent strong, positive correlation between number of demos and performance, as we demonstrated in Sec.~\ref{sec:analysis}.

\begin{table}[t]
\caption{{Test} accuracy of \texttt{gemini-1.5-{flash}-001} on BBH tasks. Refers to captions of Table~\ref{tab:gemini-pro-bbh} for detailed explanations.
}
\label{tab:gemini-flash-bbh}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{lcccccccccc}
\toprule
Tasks  & \multicolumn{2}{c}{All}         & Reinf.  & \multicolumn{2}{c}{\textcolor{black}{Iterative}} & \multicolumn{5}{c}{\ours}  \\
& Direct & CoT & ICL & \multicolumn{2}{c}{Reinf.} &\multicolumn{5}{c}{\textit{(Ours)}} \\
\# Iterations & - & 0 & 0 & 1 & 2 & \textcolor{ourred}{1\textsc{o}} & \textcolor{ourblue}{1\textsc{g}} & \textcolor{ourred}{2\textsc{o}} & \textcolor{ourblue}{2\textsc{g}} & \textcolor{ourred}{3\textsc{o}} \\
\cmidrule(lr){1-1} \cmidrule(lr){2-3} \cmidrule(lr){4-4} \cmidrule(lr){5-6} \cmidrule(lr){7-11} 
causal\_judgement &  55.0\textsubscript{5.0} & 57.7\textsubscript{1.1}  & 66.0\textsubscript{3.6} & \textbf{67.7}\textsubscript{2.0} & \underline{66.7}\textsubscript{1.6} & 69.3\textsubscript{2.7} & 66.0\textsubscript{2.0} & 63.3\textsubscript{1.5} & 65.0\textsubscript{1.6} & 65.3\textsubscript{1.5}\\
date\_understanding &  84.8\textsubscript{4.2} & 83.3\textsubscript{1.3}     & 84.5\textsubscript{2.3} & 86.8\textsubscript{0.8} & 87.3\textsubscript{0.8} & 85.0\textsubscript{1.3} & 90.5\textsubscript{0.5} & \underline{91.5}\textsubscript{0.4} & 90.8\textsubscript{0.7} & \textbf{92.5}\textsubscript{0.8}\\
disambiguation\_qa &  68.8\textsubscript{7.2} & 54.2\textsubscript{1.5}    & 75.5\textsubscript{0.5} & 77.8\textsubscript{1.6} & \underline{78.5}\textsubscript{3.5} & 77.5\textsubscript{1.3} & \textbf{79.0}\textsubscript{1.1} & 77.5\textsubscript{1.2} & 76.3\textsubscript{0.8} & 74.3\textsubscript{1.1}\\ 
dyck\_languages &  46.0\textsubscript{9.5} & 19.5\textsubscript{7.0}     & \textbf{66.8}\textsubscript{1.9} & 61.3\textsubscript{2.6} & 60.0\textsubscript{1.9} & 63.3\textsubscript{2.0} & 62.0\textsubscript{1.7} & \underline{64.5}\textsubscript{1.8} & 62.8\textsubscript{2.4} & 61.8\textsubscript{3.8}\\
formal\_fallacies &  75.8\textsubscript{1.9} & 74.0\textsubscript{1.2}     & 77.3\textsubscript{0.4} & 74.8\textsubscript{1.9} & 72.5\textsubscript{1.7} & \textbf{78.3}\textsubscript{1.3} & 77.3\textsubscript{1.5} & 75.5\textsubscript{1.7} & \underline{78.3}\textsubscript{1.8} & 76.3\textsubscript{0.8}\\
geometric\_shapes  &  45.8\textsubscript{1.5} & 74.2\textsubscript{4.1}    & 86.0\textsubscript{1.9} & 93.8\textsubscript{0.8} & 93.3\textsubscript{1.5} & 93.8\textsubscript{2.5} & 94.0\textsubscript{4.2} & 95.5\textsubscript{1.1} & \underline{97.0}\textsubscript{0.0} & \textbf{98.0}\textsubscript{0.0}\\
hyperbaton    &  87.0\textsubscript{3.1}  & 88.5\textsubscript{1.5}       & 88.5\textsubscript{1.5} & \underline{95.5}\textsubscript{1.1} & 93.3\textsubscript{1.5} & 86.5\textsubscript{7.6} & \underline{95.5}\textsubscript{1.1} & \textbf{95.8}\textsubscript{0.8} & 94.8\textsubscript{0.4} & 93.3\textsubscript{1.5}\\
logical\_deduction (7) &  37.5\textsubscript{3.3}  & 41.0\textsubscript{1.9}   & 59.5\textsubscript{3.4} & 61.9\textsubscript{1.9} & 57.5\textsubscript{4.7} & 61.8\textsubscript{5.1} & 57.5\textsubscript{1.1} & \underline{70.5}\textsubscript{0.9} & 66.5\textsubscript{1.1} & \textbf{75.0}\textsubscript{0.7}\\
movie\_recommendation  & \textbf{80.5}\textsubscript{3.3}  & 56.2\textsubscript{0.8}   & 67.0\textsubscript{1.2} & 75.8\textsubscript{1.3} & 75.8\textsubscript{2.9} & 70.3\textsubscript{2.3} & 73.3\textsubscript{2.3} & {77.3}\textsubscript{1.5} & \underline{78.8}\textsubscript{2.0} & 72.8\textsubscript{3.2}\\
multistep\_arithmetic\_two & 55.0\textsubscript{21.3}   & 84.0\textsubscript{2.9}    & 91.3\textsubscript{0.8} & 94.0\textsubscript{1.4} & 92.5\textsubscript{1.8} & 96.3\textsubscript{2.3} & \underline{96.8}\textsubscript{0.4} & \textbf{97.8}\textsubscript{0.4} & 94.8\textsubscript{0.8} & 95.8\textsubscript{0.4}\\
object\_counting  & 66.0\textsubscript{2.7}  & 91.3\textsubscript{2.0}     & 93.3\textsubscript{0.4} & 93.5\textsubscript{1.5} & 92.5\textsubscript{1.1} & 92.8\textsubscript{1.9} & 93.8\textsubscript{2.3} & \textbf{95.5}\textsubscript{0.5} & 93.0\textsubscript{1.2} & \underline{93.8}\textsubscript{0.4}\\
ruin\_names   & 83.2\textsubscript{1.3} & 86.2\textsubscript{1.3}      & 86.5\textsubscript{1.8} & 89.5\textsubscript{0.9} & 86.8\textsubscript{0.8} & 89.3\textsubscript{0.4} & 89.3\textsubscript{0.8} & 87.0\textsubscript{1.2} & \textbf{90.3}\textsubscript{0.8} & \underline{90.0}\textsubscript{1.2}\\
salient\_translation\_error\_detection & 62.0\textsubscript{3.7} & 58.8\textsubscript{2.0}  & 64.8\textsubscript{1.5} & \textbf{71.5}\textsubscript{2.2} & 64.0\textsubscript{2.9} & 62.8\textsubscript{0.8} & \underline{71.0}\textsubscript{0.7} & 69.8\textsubscript{2.0} & 69.0\textsubscript{0.7} & 67.3\textsubscript{0.4}\\
snarks   & 81.2\textsubscript{0.7} & \textbf{92.0}\textsubscript{1.2}   & 89.2\textsubscript{1.8} & 88.9\textsubscript{2.2} & 86.5\textsubscript{1.5} & 88.9\textsubscript{2.0} & {89.9}\textsubscript{1.8} & 89.6\textsubscript{0.7} & \underline{90.6}\textsubscript{0.6} & 83.7\textsubscript{3.5}\\
sports\_understanding & 92.5\textsubscript{1.5} & 91.5\textsubscript{0.5}   & \underline{95.8}\textsubscript{0.8} & 95.5\textsubscript{0.5} & \textbf{96.3}\textsubscript{1.1} & 93.3\textsubscript{1.1} & 95.3\textsubscript{0.4} & 91.8\textsubscript{0.4} & 95.0\textsubscript{1.2} & 95.0\textsubscript{0.0}\\
tracking\_shuffled\_objects (7) & 63.3\textsubscript{5.4} & 72.3\textsubscript{6.0} & 92.2\textsubscript{3.1} & 83.5\textsubscript{1.1} & 80.0\textsubscript{1.6} & \underline{98.0}\textsubscript{0.7} & 93.8\textsubscript{2.2} & \textbf{98.0}\textsubscript{0.0} & 97.8\textsubscript{0.4} & 97.5\textsubscript{0.5}\\
\cmidrule(lr){1-1} \cmidrule(lr){2-3} \cmidrule(lr){4-4} \cmidrule(lr){5-6} \cmidrule(lr){7-11} 
\textit{Average}  & 67.77 & 70.29     & 80.25 & 81.91 & 80.72 & 81.61 & 82.79 & \textbf{83.79} & \underline{83.77} & 83.25 \\
\bottomrule
\end{tabular}
}
\end{table}

\begin{table}[t!]
\caption{
{
{Test} accuracy of Mistral Large (\texttt{mistral-large-2407}) on BBH tasks. Refer to captions of Table~\ref{tab:gemini-pro-bbh} for detailed explanations.
}
}
\label{tab:mistral-bbh}
\centering
\resizebox{0.85\textwidth}{!}{
\begin{tabular}{lcccccccc}
\toprule
Tasks   & Reinf.  & \multicolumn{2}{c}{\textcolor{black}{Iterative}} & \multicolumn{5}{c}{\ours}  \\
& ICL & \multicolumn{2}{c}{Reinf.} &\multicolumn{5}{c}{\textit{(Ours)}} \\
\# Iterations & 0 & 1 & 2 & \textcolor{ourred}{1\textsc{o}} & \textcolor{ourblue}{1\textsc{g}} & \textcolor{ourred}{2\textsc{o}} & \textcolor{ourblue}{2\textsc{g}} & \textcolor{ourred}{3\textsc{o}} \\
\cmidrule(lr){1-1} \cmidrule(lr){2-2} \cmidrule(lr){3-4} \cmidrule(lr){5-9} 
causal\_judgement & 69.3 & 66.7 & 72.0 & 68.0 & 65.3 & \underline{69.3} & {64.0} & \textbf{73.3} \\
date\_understanding  &  92.0 & 92.0 & \textbf{96.0} & {93.0} & 94.0 & 95.0 & {92.0} & \textbf{96.0} \\ 
disambiguation\_qa  &  82.0 & 82.0 & 79.0 & 81.0 & \textbf{87.0} &\textbf{87.0} & 84.0 & 86.0\\ 
dyck\_language & 56.0 & 62.0 & 56.0 & \underline{70.0} & 59.0 & \underline{70.0} & {63.0} & \textbf{71.0} \\
formal\_fallacies &  \textbf{90.0} & 82.0 & {86.0} & 89.0 & 89.0 & \textbf{90.0} & 83.0 & 85.0\\
geometric\_shapes  & 87.0 & 80.0 & {93.0} & 88.0 & 85.0 & \textbf{95.0} & {71.0} & \underline{94.0} \\
hyperbaton  &  {99.0} & {96.0} & \textbf{100.0} & \textbf{100.0} & {98.0} & \textbf{100.0} & \textbf{100.0} & {99.0} \\
logical\_deduction (7) &  81.0 & {85.0} & {76.0} & 82.0 & 88.0 & \underline{90.0} & 86.0 & \textbf{92.0} \\
movie\_recommendation & 74.0 & 71.0 & {74.0} & 77.0 & 66.0 & 78.0 & \textbf{80.0} & \underline{79.0} \\
multistep\_arithmetic\_two  & 88.0 & 92.0 & \textbf{93.0} & 91.0 & 89.0 & 88.0 & {86.0} & \textbf{93.0} \\
object\_counting & \underline{99.0} & \underline{99.0} & \underline{99.0} & {98.0} & {98.0} & {98.0} & \textbf{100.0} & {98.0} \\
ruin\_names  & 88.0 & \underline{90.0} & \textbf{92.0} & 86.0 & \textbf{89.0} & \textbf{87.0} & 89.0 & \textbf{89.0} \\
salient\_translation\_error\_detection  & 66.0 & 68.0 & {70.0} & 78.0 & 69.0 & \textbf{75.0} & {72.0} & \underline{73.0}\\
snarks     & 95.8 & 95.8 & 97.2 & 94.4 & 95.8 & {95.8} & \textbf{95.8} & 93.1\\
sports\_understanding     & 94.0 & \underline{97.0} & \textbf{98.0} & 93.0 & {95.0} & {96.0} & \underline{97.0} & 96.0\\
tracking\_shuffled\_objects (7)  & {96.0} & {68.0} & \textbf{100.0} & \textbf{100.0} & {73.0} & \textbf{100.0} & {57.0} & \textbf{100.0} \\
\cmidrule(lr){1-1} \cmidrule(lr){2-2} \cmidrule(lr){3-4} \cmidrule(lr){5-9} 
\textit{Average}   & 84.82 & 83.22 & 87.08 & 86.65 & 83.70 & \underline{88.07} & {82.80} & \textbf{88.52}  \\
\bottomrule
\end{tabular}
}
\end{table}


\begin{table}[t!]
\caption{
{
{Test} accuracy of Mistral NeMo (\texttt{mistral-nemo-12b}) on BBH tasks. Refer to captions of Table~\ref{tab:gemini-pro-bbh} for detailed explanations.
}
}
\label{tab:mistral-nemo-bbh}
\centering
\resizebox{0.8\textwidth}{!}{
\begin{tabular}{lcccccccc}
\toprule
Tasks   & Reinf.  & \multicolumn{2}{c}{\textcolor{black}{Iterative}} & \multicolumn{5}{c}{\ours}  \\
& ICL & \multicolumn{2}{c}{Reinf.} &\multicolumn{5}{c}{\textit{(Ours)}} \\
\# Iterations & 0 & 1 & 2 & \textcolor{ourred}{1\textsc{o}} & \textcolor{ourblue}{1\textsc{g}} & \textcolor{ourred}{2\textsc{o}} & \textcolor{ourblue}{2\textsc{g}} & \textcolor{ourred}{3\textsc{o}} \\
\cmidrule(lr){1-1} \cmidrule(lr){2-2} \cmidrule(lr){3-4} \cmidrule(lr){5-9} 
causal\_judgement & 53.3 & \textbf{65.3} & 62.7 & 60.0 & 58.7 & 62.7 & \underline{64.0} & \underline{64.0} \\ 
date\_understanding & 66.0 & 71.0 & 68.0 & 69.0 & 69.0 & \textbf{78.0} & 70.0 & \underline{75.0} \\ 
disambiguation\_qa & 58.0 & 60.0 & 64.0 & 63.0 & 60.0 & 61.0 & \underline{66.0} & \textbf{72.0} \\ 
dyck\_languages & 17.0 & 21.0 & 22.0 & 18.0 & \underline{27.0} & 26.0 & 22.0 & \textbf{30.0} \\ 
formal\_fallacies & \textbf{64.0} & 55.0 & 53.0 & 63.0 & \underline{59.0} & 52.0 & 51.0 & \underline{59.0} \\ 
geometric\_shapes & 65.0 & 65.0 & 69.0 & \textbf{72.0} & \textbf{72.0} & 60.0 & 69.0 & 68.0 \\ 
hyperbaton & 77.0 & 72.0 & 65.0 & 80.0 & 81.0 & \underline{83.0} & 75.0 & \textbf{86.0} \\ 
logical\_deduction (7) & 47.0 & \underline{54.0} & 53.0 & 45.0 & 49.0 & \textbf{62.0} & 44.0 & 51.0 \\ 
movie\_recommendation & 59.0 & 45.0 & 54.0 & \underline{68.0} & 61.0 & 63.0 & 64.0 & \textbf{70.0} \\ 
multistep\_arithmetic\_two & 36.0 & 50.0 & 20.0 & 47.0 & 20.0 & \underline{66.0} & 12.0 & \textbf{77.0} \\ 
object\_counting & 81.0 & 81.0 & 82.0 & 83.0 & 79.0 & \underline{85.0} & 75.0 & \textbf{87.0} \\ 
ruin\_names & 69.0 & 60.0 & 57.0 & \textbf{76.0} & 57.0 & \underline{72.0} & 57.0 & 70.0 \\ 
salient\_translation\_error\_detection & 47.0 & 47.0 & 45.0 & \textbf{59.0} & 49.0 & \underline{53.0} & 49.0 & 48.0 \\ 
snarks & 69.4 & \underline{76.4} & 79.2 & 72.2 & 75.0 & 72.2 & 73.6 & \textbf{77.8 }\\ 
sports\_understanding & 86.0 & 75.0 & 69.0 & \underline{91.0} & 72.0 & \underline{91.0} & 74.0 &\textbf{ 93.0} \\ 
tracking\_shuffled\_objects (7) & 70.0 & 69.0 & 70.0 & 91.0 & 88.0 & \textbf{94.0} & 81.0 & \underline{93.0} \\
\cmidrule(lr){1-1} \cmidrule(lr){2-2} \cmidrule(lr){3-4} \cmidrule(lr){5-9} 
\textit{Average}   & 60.30 & 60.42 & 58.30 & 66.08 & 61.04 & \underline{67.56} & 59.16 & \textbf{70.05}\\
\bottomrule
\end{tabular}
}
\end{table}



\begin{table}[t!]
\caption{
{
{Test} accuracy of Claude 3.5 Sonnet (\texttt{claude-3-5-sonnet@20240620}) on BBH tasks. Refer to captions of Table~\ref{tab:gemini-pro-bbh} for detailed explanations.
}
}
\label{tab:claude-bbh}
\centering
\resizebox{0.8\textwidth}{!}{
\begin{tabular}{lcccccccc}
\toprule
Tasks   & Reinf.  & \multicolumn{2}{c}{\textcolor{black}{Iterative}} & \multicolumn{5}{c}{\ours}  \\
& ICL & \multicolumn{2}{c}{Reinf.} &\multicolumn{5}{c}{\textit{(Ours)}} \\
\# Iterations & 0 & 1 & 2 & \textcolor{ourred}{1\textsc{o}} & \textcolor{ourblue}{1\textsc{g}} & \textcolor{ourred}{2\textsc{o}} & \textcolor{ourblue}{2\textsc{g}} & \textcolor{ourred}{3\textsc{o}} \\
\cmidrule(lr){1-1} \cmidrule(lr){2-2} \cmidrule(lr){3-4} \cmidrule(lr){5-9} 
causal\_judgement & 64.0 & 68.0 & 65.3 & 62.7 & 69.3 & \textbf{73.3} & \underline{70.7} & 65.3 \\
date\_understanding  &  94.0 & 95.0 & \textbf{96.0} & \textbf{97.0} & 94.0 & 95.0 & 96.0 & 95.0 \\ 
disambiguation\_qa  &  73.0 & 82.0 & 79.0 & 81.0 & \textbf{87.0} &\textbf{87.0} & 84.0 & 86.0\\ 
dyck\_language & 68.0 & 68.0 & 65.0 & 74.0 & 85.0 & \underline{90.0} & \textbf{92.0} & 87.0 \\
formal\_fallacies &  93.0 & 94.0 & \underline{97.0} & 96.0 & 95.0 & \textbf{98.0} & 96.0 & 95.0\\
geometric\_shapes  & 92.0 & 94.0 & \textbf{98.0} & 88.0 & 90.0 & 85.0 & \underline{96.0} & 89.0 \\
hyperbaton  &  \textbf{100.0} & \textbf{100.0} & \textbf{100.0} & \textbf{100.0} & \textbf{100.0} & \textbf{100.0} & \textbf{100.0} & \textbf{100.0} \\
logical\_deduction (7) &  92.0 & \underline{96.0} & \underline{96.0} & 89.0 & 95.0 & \textbf{97.0} & 91.0 & 93.0 \\
movie\_recommendation & 87.0 & 90.0 & \underline{92.0} & 89.0 & 90.0 & 88.0 & \textbf{93.0} & 90.0 \\
multistep\_arithmetic\_two  & 99.0 & 99.0 & 99.0 & 99.0 & 99.0 & 99.0 & \textbf{100.0} & \textbf{100.0} \\
object\_counting & \textbf{100.0} & \textbf{100.0} & \textbf{100.0} & \textbf{100.0} & \textbf{100.0} & \textbf{100.0} & \textbf{100.0} & \textbf{100.0} \\
ruin\_names  & 93.0 & 93.0 & \textbf{94.0} & 91.0 & \textbf{94.0} & \textbf{94.0} & 92.0 & \textbf{94.0} \\
salient\_translation\_error\_detection  & 71.0 & 71.0 & \textbf{73.0} & 71.0 & 72.0 & \textbf{73.0} & \textbf{73.0} & \textbf{73.0}\\
snarks     & 97.2 & 97.2 & 97.2 & 95.8 & 95.8 & \textbf{98.6} & \textbf{98.6} & 97.2\\
sports\_understanding     & 92.0 & 91.0 & \textbf{94.0} & 93.0 & \textbf{94.0} & \textbf{94.0} & 93.0 & 91.0\\
tracking\_shuffled\_objects (7)  & \textbf{100.0} & \textbf{100.0} & \textbf{100.0} & \textbf{100.0} & \textbf{100.0} & \textbf{100.0} & \textbf{100.0} & \textbf{100.0} \\
\cmidrule(lr){1-1} \cmidrule(lr){2-2} \cmidrule(lr){3-4} \cmidrule(lr){5-9} 
\textit{Average}   & 88.45 & 89.89 & 90.35 & 89.16 & 91.26 & \underline{92.00} & \textbf{92.20} & 90.97  \\
\bottomrule
\end{tabular}
}
\end{table}

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{figs/ms_analysis.pdf}
    \caption{\textit{Benefits from scaling examples na\"ively (\emph{\textcolor{plotred}{red lines}}) is {very} task-specific, but each iteration of \emph{\ours} addresses it to a considerable degree by continually improving upon the previous round:} We randomly sample subsets of example pool $\mathcal{E}_k \, \forall \, k \in \{0 \text{\,(i.e., original examples generated with handcraft few-shot or zero-shot)}, 1, 2\}$ and evaluate them on a held-out set in four representative tasks exhibiting different model behavior to example scaling. The trendlines are moving regressions fitted with \textsc{lowess}. Refers to additional figures in App.~\ref{app:additional_visualization}.
    }
    \label{fig:trend_analysis}
    \end{figure}
    
Remarkably, \ours alleviates the instability with each round of \ours continually improving upon the previous round -- in cases where scaling examples is already beneficial (\texttt{geometric\_shapes}, leftmost figure), subsequent rounds of \ours led to much better performance-cost trade-offs with the blue and green lines dominating over the red, whereas in other cases, \ours often ``delays'' the saturation point (e.g., \texttt{salient\_translation}) or at least ensure more examples does not lead to deterioration (e.g., \texttt{tracking\_shuffled\_objects}).


On the BIRD dataset, we show the results in Table~\ref{tab:gemini-pro-bird}. Given the presence of a large training set (more than 9000 samples), we also compare against parameter-efficient supervised fine-tuning (PEFT)~\citep{han2024parameter}, where we fine-tune the same target LLM with LoRA~\citep{hu2021lora} on either the entire training set or using a number of train samples sub-sampled from the full training set. We observe that whereas the few-shot \textsc{chase} prompt effectively improves upon the baseline zero-shot direct prompting, additional rounds of \ours led to further gains. The comparison against LoRA also demonstrates the potential of \ours as an alternative to PEFT at least in certain scenarios. When provided with a similar number of labeled samples (i.e., $n_{\text{train}}=256$), we observe that LoRA performs much worse, and it only outperforms \ours when using up the \textit{entire} train set for training. 

\rparagraph{Claude and Mistral results.} From Tables\ref{tab:mistral-bbh}, \ref{tab:mistral-nemo-bbh}, and ~\ref{tab:claude-bbh}, we find that while the base capabilities of the tested models differ significantly (e.g., Claude 3.5 Sonnet has a higher accuracy across the board), the high-level findings primarily derived from Gemini results largely hold. On Claude 3.5 Sonnet, we observe an almost identical high-level trend to Gemini, where each round of \ours incrementally improves performance up to \textcolor{ourblue}{2\textsc{g}}. On the other hand, while Mistral models seemingly benefit less from scaling demonstrations especially in the smaller Mistral NeMo (e.g., sometimes the \textcolor{ourblue}{generate} step leads to drops in performance) directly, the improved quality of the generated demonstrations still enables successive  \textcolor{ourred}{optimize} step to improve on the preceding round, demonstrating the effectiveness of \ours even when the model does not benefit from scaling examples directly.


\rparagraph{Additional experiments.} We performed additional experiments to further validate the design of \ours and tested the applicability of \ours beyond reasoning-heavy tasks. We:
\begin{enumerate}
    \item Perform extensive \textbf{ablation studies} in App.~\ref{app:ablation_studies}, where we studied the importance of Bayesian optimization (Algorithm~\ref{alg:bo}) and compared and combined \ours with \textit{heuristic} learning-free and learning-based demonstration selection such as \textit{retrieval} and \textit{diversity}-based learning-free criteria; we found that in all cases \ours outperformed the alternative approaches, but \ours may also be complementary to various approaches proposed in previous works. We also conduct further experiments confirming the importance of the \textcolor{ourred}{optimize} step by restricting \ours to perform refinement on the subset of the train set where the model predicted correctly initially only in Table~\ref{tab:bbh-restricted}, and confirmed that the \textcolor{ourred}{optimize} step meaningfully improves upon the variant without it;
    \item Perform \textbf{transfer learning analysis} in App.~\ref{app:transfer_learning} where we used the many-shot examples generated on GSM-Hard on GSM-8K, and we found that the generated examples are generalizable to some extent to distributional shifts;
    \item Perform \textbf{cost analysis} in App.~\ref{app:computational_cost} where we provide a detailed cost breakdown of \ours in each step.
\end{enumerate}


\section{Related work}
\label{sec:related_work}

\begin{wraptable}{l}{0.4\textwidth}
\centering
\vspace{-6mm}
\scalebox{0.8}{
\begin{tabular}{lcccc}\\\toprule  
Method & Exec. &  \multicolumn{3}{c}{Breakdown} \\ 
       & Acc.      &  S & M & C \\\cmidrule(lr){1-1} \cmidrule(lr){2-2} \cmidrule(lr){3-5} 
Direct & 57.7 & 64.0 & 49.4 & 44.1 \\
\textsc{chase} prompt & 60.1 & 67.2 & 51.9 & 40.7\\
\textsc{chase} + \ours \\
\textit{  Round 0} & 59.1 & 65.7 & 51.3 & 42.1 \\
\textit{  Round 1} & 61.2 & 68.6 & 50.6 & 48.3 \\
\textit{  Round 2} & \underline{62.0} & 68.5 & 53.0 & 49.0 \\ \midrule
PEFT (LoRA) \\
 $n_{\text{train}}$ = 256 & 58.2 & 64.0 & 52.2 & 40.7 \\
 $n_{\text{train}}$ = 1024 & 60.2 & 66.6 & 53.0 & 42.1 \\
 $n_{\text{train}}$ = 4096 & 61.3 & 67.5 & 53.9 & 46.2 \\
 $n_{\text{train}}$ = 9428 (All) & \textbf{63.8} & 68.6 & 58.8 & 48.9 \\
\bottomrule
\end{tabular}
}
\caption{Execution accuracy on the BIRD dev set with \texttt{gemini-1.5-pro-001}. \{S, M, C\} refer to the accuracy aggregated across \{Simple, Moderate, Challenging\}-level problems based on assigned difficulty.}
\label{tab:gemini-pro-bird}
\end{wraptable}
\vspace{-3mm}
\textbf{Scaling ICL. }
Before the advent of the long-context LLMs, early efforts in scaling ICL often studied LLMs customized for long context~\citep{li2023context} or required architectural changes assuming white-box model access~\citep{hao2022structured}. However, the tasks considered are often limited, e.g., to conventional, discriminative tasks like sentiment classification rather than generative tasks as considered in this work. Furthermore, these often study LLMs that are merely capable of handling many examples, but their behavior may differ significantly to modern, natively long-context LLMs that may actively \textit{take advantage of} the context -- indeed, both these works show mixed results, even significant performance deterioration when scaling up the number of examples, a phenomenon not seen in modern long-context LLMs like Gemini and Claude. Recent works like \citet{agarwal2024many} and \citet{bertsch2024context}, on the other hand, reported significant gains in scaling ICL to hundreds or more examples and provided important motivation for our work. However, as mentioned in Sec.~\ref{sec:analysis}, these works primarily demonstrate the existence of the benefit from scaling but do not focus on investigate the sources of the gain or improving the cost-effectiveness of many-shot ICL.
Additionally, there have also been works focusing on {applications} of many-shot ICL to multi-modalities \citep{jiang2024many}, LLM jail-breaking \citep{anil2024many}, detecting the risk of capturing incorrect skills \citep{liu2024dual}, and analyzing memorization \citep{golchin2024memorization}.


\textbf{Example selection and generation.}
\ours combines the ``optimize'' and ``generate'' steps, and there have been existing works sharing similar high-level ideas to each of the components. First, the ``optimize'' step can be seen as a method to improve the \textit{data quality} with pruning and selection; in this regard, given that data quality is known to be one of the most influential factors for training LLMs \citep{xia2024less}, many previous works have utilized some flavor of pruning to remove redundant or harmful data samples at different stages of training, including pre-training~\citep{marion2023less} and instruction tuning~\citep{xia2024less}. In ICL, as mentioned in Sec.~\ref{sec:analysis}, given the sensitivity of LLMs to examples, there have been numerous works analyzing prompt sensitivity and proposing \textit{example selection} techniques \citep{zhao2021calibrate, lu-etal-2022-fantastically,zhou2024batch, wan2024teach}. Recent work also explored heuristic-based prompt optimization based on similarity \citep{rubin-etal-2022-learning, liu-etal-2022-makes}, diversity \citep{levy-etal-2023-diverse, xu2024context}, uncertainty \citep{wan-etal-2023-better, wan-etal-2023-universal}, fairness~\citep{zhou-etal-2024-fairer} etc. Our ``generate'' step, on the other hand, aims to acquire high-quality examples with the LLM itself. In this area, STaR~\citep{zelikman2022star} first proposes to bootstrap rationales from LLM with a small number of seed examples, followed by fine-tuning on the rationales that lead to correct predictions; Self-Instruct~\citep{wang-etal-2023-self-instruct} bootstraps LLMs to instruction data. The ``Reinforced ICL'' technique introduced in \citet{agarwal2024many}, upon which this work improves, and several recent works~\citep{chen-etal-2023-self, khattab2023dspy, opsahl2024optimizing} use a similar technique to acquire and refine model-generated examples for ICL. Notwithstanding the similarities described, there are a few crucial differences with respect to these prior works: Almost all ICL works mentioned consider the \textit{few}-shot setup, where selection is made necessary due to the constraint on the number of examples allowed in the context. However, we show that even in the many-shot setup where that constraint is relaxed and example selection is no longer a necessity, it can still be highly beneficial for performance and efficiency. Unlike the few-shot setup, \ours is tailored for the many-shot setup with design decisions inspired by findings in Sec.~\ref{sec:analysis}, such as the implementation of sparsity regularization in the optimization objective to enable scaling.












\section{Conclusion}
\label{sec:conclusion}
This paper focuses on understanding and enhancing the core factors underlying scaling ICL.
We first provide an analysis of the nascent paradigm of many-shot ICL in LLMs and show that notwithstanding the long-context abilities of LLMs, the common practice of na\"ively dumping as many examples as practically possible into the context can be both inefficient in cost and suboptimal in performance. Instead, the benefit from scaling examples can often be realized by identifying a subset of influential examples, and that subset can be used as demonstrations themselves to re-generate even more examples. Inspired by the findings, we propose \ours by automatically executing the ``optimize'' and ``generate'' steps iteratively. We demonstrate that \ours perform competitively on a wide range of tasks, significantly outperforming alternatives. We believe that this work builds the foundation for future research in many-shot ICL. 
First, {we mainly focused on the restrictive \textit{black-box} LLM setup, which is the most general and model-agnostic. However, for a more relaxed, white-box setup with access to LLM weights, it may be possible to perform optimization more efficiently -- for example, it may be possible to take advantage of the internal representations of the model in reducing the cost of iterative optimization.} 
Second, we currently focus on the ``reinforced ICL'' setup typical for reasoning-heavy tasks -- while we have conducted experiments (e.g., low resource translation tasks) beyond this setup, further validations on other types of tasks would be valuable. Lastly, after optimization, the examples generated by \ours are currently static at test time, and it would also be interesting to combine with a mechanism for sample-dependent ICL optimization to further enhance performance and reduce cost -- we defer these important directions to future work.


\subsection*{Acknowledgment}
We thank all colleagues from Google Cloud AI Research for their valuable feedback. We also thank the anonymous ICLR reviewers and area chairs whose feedback has been instrumental in improving this work.

\bibliographystyle{abbrvnat}
\nobibliography*
\bibliography{iclr2025_conference}

\newpage
\appendix
\section{Derivation of the Approximated Importance Score}
\label{app:grad_score}

In this section, we give detailed derivation of the importance score used in Sec.~\ref{sec:analysis} to rank the examples -- on a high level, we use a similar approach to \citet{ru2020interpretable} in determining the importance from the GP surrogate: Recalling that we are given a pool of examples $\mathcal{E}$ with $|\mathcal{E}| = m$, a collection of $T$ subsets of $\mathbf{e}_i$, each represented as a binary vector $\mathbf{e}_i \in \{0, 1\}^m$ and their corresponding scores on the validation set $g(\cdot): \{0, 1\}^m \rightarrow \mathbb{R}$, we first fit a GP regression with $\mathbf{e}_{1:T} = [\mathbf{e}_1, ..., \mathbf{e}_T]^{\top}$ and $\mathbf{g}_{1:T} = [g(\mathbf{e_1}, ..., g(\mathbf{e}_T)]^{\top}$, as presented in Eq.~\ref{eq:gp}, the mean of the posterior GP $\hat{g}(\cdot)$ is given by:
\begin{equation}
    \mathbb{E}_{\hat{g}(\mathbf{e}) |\mathcal{G}_T}[\hat{g}(\mathbf{e})] = \mathbf{k}_{1:T} (\mathbf{K} + \eta^2\mathbf{I})^{-1} \mathbf{g}_{1:T},
    \label{eq:gp_restate}
\end{equation}
where we define $\mathcal{G}_T$ as the shorthand of $[\mathbf{e}_{1:T}, \mathbf{g}_{1:T}]$ to denote that the fitted function $\hat{g}(\mathbf{e})$ is fitted on the observed input-output pairs;  $\mathbf{k}_t = [k(\mathbf{e}, \mathbf{e}_1), ..., k(\mathbf{e}, \mathbf{e}_t)]$ and $k(\cdot, \cdot)$ is the covariance function of the GP (we use Matern 2.5 by default). As mentioned in Sec.~\ref{sec:analysis}, whereas we do not assume any differentiability property from $g(\cdot)$ on $\mathbf{e}$, since the approximated function $\hat{g}(\cdot)$ follows a posterior GP, its gradient w.r.t $\mathbf{e}$ is analytically available and is itself a GP, given by:
\begin{equation}
    \nabla_{\mathbf{e}} g = \frac{\partial g(\mathbf{e})}{\partial \mathbf{e}} = \frac{\partial \mathbf{k}_{1:T}}{\partial \mathbf{e}}(\mathbf{K} + \eta^2\mathbf{I})^{-1} \mathbf{g}_{1:T},
\end{equation}
noting that the expensive matrix inversion term, $(\mathbf{K} + \eta^2\mathbf{I})^{-1}$ does not have a dependence on $\mathbf{e}$ and can be directly cached from Eq.~\ref{eq:gp_restate} when we compute the posterior mean. The derivative term is essentially a differentiation operation of the covariance function to the input and can be easily computed either analytically for common kernel choices or via automatic differentiation for popular GP or BO packages like \texttt{gpytorch}~\citep{gardner2018gpytorch} or \texttt{botorch}~\citep{balandat2020botorch}.

With the computed $ \nabla_{\mathbf{e}} g \in \mathbb{R}^m$, we can in principle compute the estimated derivative at any $\mathbf{e} \subseteq \mathcal{E}$. However, in practice, we find the derivative estimate to be more reliable at the \textit{training points} of the GP (i.e., $[\mathbf{e}_1, ..., \mathbf{e}_T]$. We then evaluate the derivative at each of the training points, and the final importance score is marginalized by averaging across the training points:
\begin{equation}
M({e}^{(j)}) = \frac{1}{T}\sum_{t=1}^T\nabla_{\mathbf{e}}\hat{g}|_{\mathbf{e} = \mathbf{e}_t}^{(j)},
\end{equation}
where we use the superscript $(j)$ to denote that the estimated importance of the $j$-th individual example (note the regular font $e \in \mathcal{E}$ denoting an \textit{individual} example instead of the bold-face $\mathbf{e}$ denoting a \textit{set of examples} in $\mathcal{E}$). We then compute the importance score of all examples in $\mathcal{E}$, which is then used to generate the assigned ranking in the analysis of Sec.~\ref{sec:analysis} such as Fig.~\ref{fig:trend_analysis}.

\section{Implementation Details}
\label{app:implementation_details}

\subsection{Datasets.}

In the section below, we give detailed implementation details for the availability, data splitting protocol, input prompts, and licensing information of the datasets used.

\rparagraph{BIG-Bench Hard (BBH).} BBH is a collection of 26 challenging reasoning tasks and a task is selected if either 1) if it is studied in the seminal work on many-shot ICL~\citep{agarwal2024many} or 2) if the zero-shot performance of \texttt{gemini-1.5-pro-001} is below 90\%, which indicates non-saturation of performance -- these criteria led to a set of 16 tasks that we consider in Sec.~\ref{sec:experiments}. For all tasks, we randomize the data points and reserve 40\% (usually 100 samples, but some sub-tasks of BBH benchmark have fewer data-points) as held-out sets for testing, whose inputs and labels are not revealed to the model except for final evaluation. For the rest of the dataset, in Sec.~\ref{sec:analysis}, we use 50\% (30\% of all available data points including the held-out test set) as the ``train-set'' from which the examples are generated and the other 50\% for validation (i.e., the split where results in Fig.~\ref{fig:trend_analysis} is generated). In Sec.~\ref{sec:experiments}, we do not use the aforementioned validation set and use performance on the same set that generates the examples as the optimization objective. The BBH dataset is publicly available at \url{https://github.com/suzgunmirac/BIG-Bench-Hard} under an MIT license. For all BBH tasks, we use the prompt templates below: 

\begin{lstlisting}[style=mystyle]
You will be given a question. Think step by step before giving a final answer to this question. Show your final answer {{ TASK_SPECIFIC_CONSTRAINTS }} between <answer> and <\answer>

{{ EXAMPLES }}
==

{{ QUESTION }}
{{ llm() }}
\end{lstlisting}
where we use a Jinja2-style syntax and the upper-cased blocks bracketed between double braces are \textit{variables} that are replaced at inference time: \texttt{TASK\_SPECIFIC\_CONSTRAINTS} denote the constraint instruction specific to the type of the task. For example, for a multiple-choice task, this is replaced with ``answer option letter only''; for a binary choice question, this is replaced with ``Yes or No only'' and for a free-form generation task, this is replaced by an empty string. \texttt{EXAMPLES} denote the concatenation of any examples $\mathbf{e}$ added to the input -- for the initial generation step  (i.e., Step 3 in Algorithm~\ref{alg:main_alg}), we use zero-shot prompting and \texttt{EXAMPLES} is an empty string. For the subsequent generation step, this is replaced with the concatenation of the examples selected by \ours; finally, \texttt{llm()} denotes the place where an LLM response is solicited; the answer is then extracted and postprocessed to match with a ground-truth answer to measure accuracy.

\rparagraph{MATH and GSM-Hard.} In MATH and GSM-Hard, we similarly adopt the \textit{unified} train set setup as mentioned in the previous paragraph. To minimize the chance of data contamination where the training inputs and outputs were leaked to the model during pre-training or instruction finetuning, we randomly sample 128 samples as the official test set as the train set from which the examples are generated and use the rest of the official test set for testing. The MATH dataset is available at \url{https://github.com/hendrycks/math} and GSM-Hard is available at \url{https://huggingface.co/datasets/reasoning-machines/gsm-hard}. Both datasets are licensed under an MIT license. On GSM-Hard, we use the same prompt as the BBH dataset mentioned above. On MATH, we use the \textit{inner monologue} prompt consists of a human-annotated few-shot prompt given by \citet{agarwal2024many} with an added preamble to ensure that the LLM generation follows the style of the examples given, noting that any model-generated examples will be added to the initial human-annotated examples:

\begin{lstlisting}[style=mystyle]
You will be given several example math questions and their solutions. At the end, You will be given another question that you will need to solve. Make sure you follow the examples and always finish your answer with 'Final Answer: The final answer is X. I hope it is correct.' where X is the correct answer.

Problem:
Find the domain of the expression $\frac{\sqrt{x-2}}{\sqrt{5-x}}$.

Solution:

I need to find the domain of this expression.

This expression has two square root expressions.

I know the values under each square root must be non-negative.

So let me start by ensuring that.

I set $x-2 \ge 0$, which simplifies to $x\ge2$.

Then, I set $5 - x \ge 0$, which simplifies to $x \le 5$.

But I need to make sure the denominator is not zero.

So, $5-x$ must also be greater than zero, which gives me $x<5$.

Therefore, the domain of the expression is $\boxed{[2,5)}$.

Answer: $[2,5)$.

Final Answer: The final answer is $[2,5)$. I hope it is correct.
==

Problem:
If $\det \mathbf{A} = 2$ and $\det \mathbf{B} = 12,$ then find $\det (\mathbf{A} \mathbf{B}).$

Solution:

I need to find the determinant of the product of matrices A and B.

I remember that the determinant of the product of two matrices equals the product of their determinants.

So, $\det (\mathbf{A} \mathbf{B}) = (\det \mathbf{A})(\det \mathbf{B}) = (2)(12) = \boxed{24}$.

Answer: $24$.

Final Answer: The final answer is $24$. I hope it is correct.""",
    r"""Problem:
Terrell usually lifts two 20-pound weights 12 times. If he uses two 15-pound weights instead, how many times must Terrell lift them in order to lift the same total weight?

Solution:


Okay, so Terrell lifts a total of $2\cdot 12\cdot20=480$ pounds of weight with the 20-pound weights.

  Well, if he switches to 15-pound weights, the total weight lifted will be $2\cdot15\cdot n=30n$ pounds, where n is the number of lifts.

I want to find the number of lifts, n, for the total weight lifted to be the same.

I equate $30n$ to 480 pounds and solve for n.

\begin{align*}
30n&=480\\
\Rightarrow\qquad n&=480/30=\boxed{16}
\end{align*}

Answer: $16$.

Final Answer: The final answer is $16$. I hope it is correct.
==

Problem:
If the system of equations

\begin{align*}
6x-4y&=a,\\
6y-9x &=b.
\end{align*}

has a solution $(x, y)$ where $x$ and $y$ are both nonzero, find $\frac{a}{b},$ assuming $b$ is nonzero.

Solution:

I'm given a system of two equations.

I see that if I multiply the first equation by $-\frac{3}{2}$, I'll get another equation that has the same left-hand side as the second equation, $6y-9x$.

Let me try that $$6y-9x=-\frac{3}{2}a.$$

Ah, I also know that $6y-9x=b$, so I can equate these two equations.

So, $$-\frac{3}{2}a=b\Rightarrow\frac{a}{b}=\boxed{-\frac{2}{3}}.$$

Answer: $-\frac{2}{3}$.

Final Answer: The final answer is $-\frac{2}{3}$. I hope it is correct.
==

{{ EXAMPLES }}

==
Problem:
{{ QUESTION }}

Solution:

{{ llm() }}
\end{lstlisting}

\rparagraph{BIRD} On BIRD, we randomly sample 128 samples from the train split as the unified train and validation set and use the official test set (of 1534 data points) for testing. Since BIRD is a code generation task, the execution accuracy is computed not via a simple string match between the predicted and the ground-truth SQLs but by actually executing both SQLs on the database provided, and a score of 1 is only assigned when the predicted SQL is both executable and if whose results exactly match the execution results from the ground-truth SQL. All data, including the databases, schemas, and ground-truth gold SQL are available at the official repo: \url{https://bird-bench.github.io} under a \texttt{CC BY-SA 4.0} license. With reference to Table~\ref{tab:gemini-pro-bird}, use two prompt versions for different rows. The \textit{direct} prompt is a standard, zero-shot prompt to elicit the SQL prediction directly; it is used both for the ``Direct'' row to directly extract LLM answer and is also used as the prompt template for finetuning in the different ``LoRA'' rows:
\begin{lstlisting}[style=mystyle]
You are an SQL expert tasked with answering user's questions about SQL tables by generating SQL queries in the SQLite dialect.

Use only the following tables to answer the question:

{{ SCHEMA }}

Question: {{ QUESTION }}
Hint: {{ HINT }}
SQL: {{ llm()  }}
\end{lstlisting}
where \texttt{SCHEMA} refers to the \textit{table schema}, which can be generated automatically by querying the database, \texttt{QUESITON} is the natural language question that we would like the LLM to convert to a SQL command and \texttt{HINT} is a hint which additionally explains the question provided by the BIRD dataset. For the \textsc{chase} and \textsc{chase + \ours} rows, we use the prompt template proposed in \citet{anonymous2024chase} to invoke reasoning and divide-and-conquer before the LLM gives the final answer:

\begin{lstlisting}[style=mystyle]
You are an experienced database expert.
Now you need to generate a SQL query given the database information, a question and some additional information.
The database structure is defined by the following table schemas (comments after '--' provide additional column descriptions).
Note that the "Example Values" are actual values from the column. Some column might contain the values that are directly related to the question. Use it to help you justify which columns to use.

Given the table schema information description and the `Question`. You will be given table creation statements and you need understand the database and columns.

You will be using a way called "recursive divide-and-conquer approach to SQL query generation from natural language".

Here is a high level description of the steps.
1. **Divide (Decompose Sub-question with Pseudo SQL):** The complex natural language question is recursively broken down into simpler sub-questions. Each sub-question targets a specific piece of information or logic required for the final SQL query. 
2. **Conquer (Real SQL for sub-questions):**  For each sub-question (and the main question initially), a "pseudo-SQL" fragment is formulated. This pseudo-SQL represents the intended SQL logic but might have placeholders for answers to the decomposed sub-questions. 
3. **Combine (Reassemble):** Once all sub-questions are resolved and their corresponding SQL fragments are generated, the process reverses. The SQL fragments are recursively combined by replacing the placeholders in the pseudo-SQL with the actual generated SQL from the lower levels.
4. **Final Output:** This bottom-up assembly culminates in the complete and correct SQL query that answers the original complex question. 

Database admin instructions (violating any of the following is punishable to death!):
1. **SELECT Clause:** 
    - Only select columns mentioned in the user's question. 
    - Avoid unnecessary columns or values.
2. **Aggregation (MAX/MIN):**
    - Always perform JOINs before using MAX() or MIN().
3. **ORDER BY with Distinct Values:**
    - Use `GROUP BY <column>` before `ORDER BY <column> ASC|DESC` to ensure distinct values.
4. **Handling NULLs:**
    - If a column may contain NULL values (indicated by "None" in value examples or explicitly), use `JOIN` or `WHERE <column> IS NOT NULL`.
5. **FROM/JOIN Clauses:**
    - Only include tables essential to answer the question.
6. **Strictly Follow Hints:**
    - Adhere to all provided hints.
7. **Thorough Question Analysis:**
    - Address all conditions mentioned in the question.
8. **DISTINCT Keyword:**
    - Use `SELECT DISTINCT` when the question requires unique values (e.g., IDs, URLs). 
    - Refer to column statistics ("Value Statics") to determine if `DISTINCT` is necessary.
9. **Column Selection:**
    - Carefully analyze column descriptions and hints to choose the correct column when similar columns exist across tables.
10. **String Concatenation:**
    - Never use `|| ' ' ||` or any other method to concatenate strings in the `SELECT` clause. 
11. **JOIN Preference:**
    - Prioritize `INNER JOIN` over nested `SELECT` statements.
12. **SQLite Functions Only:**
    - Use only functions available in SQLite.
13. **Date Processing:**
    - Utilize `STRFTIME()` for date manipulation (e.g., `STRFTIME('%

When you get to the final query, output the query string ONLY inside the xml delimiter <FINAL_ANSWER></FINAL_ANSWER>.

Here are some examples:

{{ EXAMPLES }}

Now is the real question, following the instruction and examples, generate the SQL with Recursive Divide-and-Conquer approach.
**************************
[Table creation statements]
{{ SCHEMA }}

**************************
[Question]
Question: {{ QUESTION }}
Hint: {{ HINT }}

**************************
[Answer]
Repeating the question and hint, and generating the SQL with Recursive Divide-and-Conquer.
{{ llm() }}
\end{lstlisting}

Similar to the inner monologue prompt in the MATH dataset, the \texttt{EXAMPLES} in the template above also include 4 human annotated examples at initialization (an exemplary human-annotated example is shown in the code snippet below); any model-generated examples from \ours are again concatenated to the human annotated examples at inference.

\begin{lstlisting}[style=mystyle]
**************************
[Table creation statements]
CREATE TABLE generalinfo
(
	id_restaurant INTEGER not null primary key,
	food_type TEXT null, -- examples: `thai`| `food type` description: the food type
	city TEXT null, -- description: the city where the restaurant is located in
);

CREATE TABLE location
(
	id_restaurant INTEGER not null primary key,
	street_name TEXT null, -- examples: `ave`, `san pablo ave`, `pablo ave`| `street name` description: the street name of the restaurant
	city TEXT null, -- description: the city where the restaurant is located in
	foreign key (id_restaurant) references generalinfo (id_restaurant) on update cascade on delete cascade,
);

**************************
[Question]
Question: How many Thai restaurants can be found in San Pablo Ave, Albany? 
Hint: Thai restaurant refers to food_type = 'thai'; San Pablo Ave Albany refers to street_name = 'san pablo ave' AND T1.city = 'albany'

**************************
[Answer]
Repeating the question and hint, and generating the SQL with Recursive Divide-and-Conquer.
**Question**: How many Thai restaurants can be found in San Pablo Ave, Albany? 
**Hint**: Thai restaurant refers to food_type = 'thai'; San Pablo Ave Albany refers to street_name = 'san pablo ave' AND T1.city = 'albany'

**1. Divide and Conquer:**

* **Main Question:** How many Thai restaurants can be found in San Pablo Ave, Albany? 
    * **Analysis:** The question asks for a count of restaurants, so we'll use `COUNT()` for that. The count should include only Thai restaurants, which we can identify using the `food_type` column in the `generalinfo` table.  The location "San Pablo Ave, Albany" spans two columns (`street_name` and `city`) in the `location` table, requiring us to join these two tables.
    * **Pseudo SQL:** SELECT COUNT(`T1`.`id_restaurant`) FROM `generalinfo` AS `T1` INNER JOIN `location` AS `T2` ON `T1`.`id_restaurant` = `T2`.`id_restaurant` WHERE  <Thai restaurant> AND <in San Pablo Ave, Albany>

    * **Sub-question 1:** Thai restaurant
        * **Analysis:** This is a straightforward filter on the `generalinfo` table using the `food_type` column.
        * **Pseudo SQL:** `T1`.`food_type` = 'thai'

    * **Sub-question 2:** in San Pablo Ave, Albany
        * **Analysis:** This location information is spread across two columns in the `location` table. We need to combine these conditions with an "AND" operator to ensure both are met.
        * **Pseudo SQL:** `T2`.`street_name` = 'san pablo ave' AND `T2`.`city` = 'albany' 

**2. Assembling SQL:**

* **Sub-question 1 (Thai restaurant):**
    * **SQL:** `T1`.`food_type` = 'thai'

* **Sub-question 2 (in San Pablo Ave, Albany):**
    * **SQL:** `T2`.`street_name` = 'san pablo ave' AND `T2`.`city` = 'albany'

* **Main Question (count of restaurants):**
    * **SQL:** SELECT COUNT(`T1`.`id_restaurant`) FROM `generalinfo` AS `T1` INNER JOIN `location` AS `T2` ON `T1`.`id_restaurant` = `T2`.`id_restaurant` WHERE `T1`.`food_type` = 'thai' AND `T2`.`street_name` = 'san pablo ave' AND `T2`.`city` = 'albany'

**3. Simplification and Optimization:**

* The SQL query from step 2 is already quite efficient. We've used `INNER JOIN` to combine the tables based on their relationship, and the `WHERE` clause clearly defines our filtering criteria. There's no need for nested queries or complex sub-selections in this case.

**Final Optimized SQL Query:**

<FINAL_ANSWER>
SELECT COUNT(T1.id_restaurant) FROM generalinfo AS T1 INNER JOIN location AS T2 ON T1.id_restaurant = T2.id_restaurant WHERE T1.food_type = 'thai' AND T1.city = 'albany' AND T2.street_name = 'san pablo ave'
</FINAL_ANSWER>
\end{lstlisting}

\subsection{{Implementation details of the Infilling baseline}}
{\textit{Infilling} is a technique of generating the intermediate outputs given both {input queries} \textit{and} the ground-truth answer -- this is used as a baseline in Tables~\ref{tab:gemini-pro-bbh} and  \ref{tab:gemini-flash-bbh} where we utilize all available labeled data in the context. Concretely, we use the following prompt adapted from \citet{hu2023amortizing} to generate the intermediate rationales.}


\begin{lstlisting}[style=mystyle]
You will be given a question and its final, ground-truth correct answer. 
Given the question and the answer, generate the step-by-step reasoning steps that led to the correct answer. Write your intermediate reasoning steps (but NOT the final answer) leading to the final answer between <answer> and </answer>.

Question: {{ question }}
Answer: {{ target }}
Steps: {{ llm()) }}
\end{lstlisting}


\section{Additional Experiments and Results}
\label{app:additional_experiments}

\subsection{Ablation and Sensitivity Studies}
\label{app:ablation_studies}

\rparagraph{Importance of Bayesian optimization.} To ablate \ours, in Table~\ref{tab:gemini-pro-bbh-rs-comp} and Table~\ref{tab:gemini-flash-bbh-rs-comp}, we compare against a simplified variant of \ours with BO replaced with random search consuming the same evaluation budget (32 per stage) -- we find that while random search is a remarkably strong baseline, BO nevertheless outperformed it consistently at all stages of the \ours pipeline.

\begin{table}[t]
\caption{Comparison between \ours with BO (\ours-\textsc{bo}) and \ours with random search (\ours-\textsc{rs}) using \texttt{gemini-1.5-{flash}-001} on BBH tasks. The \ours-\textsc{bo} results are lifted from Table~\ref{tab:gemini-flash-bbh}, and the last row denotes the average improvement due to the use of BO over RS at the milestone in the progression of \ours. Refers to captions of Table~\ref{tab:gemini-pro-bbh} for additional explanations.
}
\label{tab:gemini-flash-bbh-rs-comp}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{lcccccccccc}
\toprule
Tasks  & \multicolumn{5}{c}{\ours-\textsc{rs}} & \multicolumn{5}{c}{\ours-\textsc{bo}}  \\
\# Iterations & \textcolor{ourred}{1\textsc{o}} & \textcolor{ourblue}{1\textsc{g}} & \textcolor{ourred}{2\textsc{o}} & \textcolor{ourblue}{2\textsc{g}} & \textcolor{ourred}{3\textsc{o}} & \textcolor{ourred}{1\textsc{o}} & \textcolor{ourblue}{1\textsc{g}} & \textcolor{ourred}{2\textsc{o}} & \textcolor{ourblue}{2\textsc{g}} & \textcolor{ourred}{3\textsc{o}} \\
\cmidrule(lr){1-1} \cmidrule(lr){2-6} \cmidrule(lr){7-11} 
causal\_judgement &  59.3\textsubscript{2.0} & \underline{66.7}\textsubscript{1.6}  & \textbf{67.7}\textsubscript{1.5} & {63.0}\textsubscript{1.1} & {64.0}\textsubscript{1.6} & 61.3\textsubscript{2.7} & 66.0\textsubscript{2.0} & 63.3\textsubscript{1.5} & 65.0\textsubscript{1.6} & 65.3\textsubscript{1.5}\\
date\_understanding &  84.8\textsubscript{1.3} & 90.5\textsubscript{0.5}     & \underline{93.3}\textsubscript{0.4} & 93.0\textsubscript{0.7} & \textbf{94.5}\textsubscript{0.8} & 85.0\textsubscript{1.3} & 90.5\textsubscript{0.5} & {91.5}\textsubscript{0.4} & 90.8\textsubscript{0.7} & {92.5}\textsubscript{0.8}\\
disambiguation\_qa &  73.8\textsubscript{1.3} & 74.5\textsubscript{1.1}    & 74.0\textsubscript{1.2} & 75.3\textsubscript{0.8} & {70.5}\textsubscript{1.1} & 77.5\textsubscript{1.3} & \textbf{79.0}\textsubscript{1.1} & \underline{77.5}\textsubscript{1.2} & 76.3\textsubscript{0.8} & 74.3\textsubscript{1.1}\\ 
dyck\_languages &  64.5\textsubscript{1.5} & 62.5\textsubscript{3.6}     & \textbf{65.5}\textsubscript{4.2} & 64.8\textsubscript{1.1} & 68.0\textsubscript{2.5} & 63.3\textsubscript{2.0} & 62.0\textsubscript{1.7} & \underline{64.5}\textsubscript{1.8} & 62.8\textsubscript{2.4} & 61.8\textsubscript{3.8}\\
formal\_fallacies &  77.3\textsubscript{1.1} & 75.0\textsubscript{2.6}     & 74.5\textsubscript{1.7} & 77.5\textsubscript{1.7} & 78.3\textsubscript{2.5} & \textbf{78.3}\textsubscript{1.3} & 77.3\textsubscript{1.5} & 75.5\textsubscript{1.7} & \underline{78.3}\textsubscript{1.8} & 76.3\textsubscript{0.8}\\
geometric\_shapes  &  88.5\textsubscript{3.8} & 93.3\textsubscript{3.0}    & 94.5\textsubscript{2.1} & \textbf{98.0}\textsubscript{0.0} & 95.3\textsubscript{1.9} & 93.8\textsubscript{2.5} & 94.0\textsubscript{4.2} & 95.5\textsubscript{1.1} & {97.0}\textsubscript{0.0} & \textbf{98.0}\textsubscript{0.0}\\
hyperbaton    &  94.0\textsubscript{0.7}  & 94.3\textsubscript{0.4}       & 95.0\textsubscript{0.7} & {95.0}\textsubscript{0.7} & 88.8\textsubscript{1.5} & 86.5\textsubscript{7.6} & \underline{95.5}\textsubscript{1.1} & \textbf{95.8}\textsubscript{0.8} & 94.8\textsubscript{0.4} & 93.3\textsubscript{1.5}\\
logical\_deduction (7) &  62.8\textsubscript{3.3}  & 54.5\textsubscript{2.2}   & 67.8\textsubscript{1.9} & 64.0\textsubscript{2.6} & 66.8\textsubscript{1.9} & 61.8\textsubscript{5.1} & 57.5\textsubscript{1.1} & \underline{70.5}\textsubscript{0.9} & 66.5\textsubscript{1.1} & \textbf{75.0}\textsubscript{0.7}\\
movie\_recommendation  & {68.5}\textsubscript{4.0}  & 75.3\textsubscript{2.6}   & 72.5\textsubscript{1.7} & \underline{77.5}\textsubscript{1.3} & 77.5\textsubscript{1.8} & 70.3\textsubscript{2.3} & 73.3\textsubscript{2.3} & {77.3}\textsubscript{1.5} & \underline{78.8}\textsubscript{2.0} & 72.8\textsubscript{3.2}\\
multistep\_arithmetic\_two & 82.5\textsubscript{0.5}   & 92.3\textsubscript{1.3}    & 95.0\textsubscript{1.4} & 89.5\textsubscript{1.5} & 92.5\textsubscript{2.6} & 96.3\textsubscript{2.3} & \underline{96.8}\textsubscript{0.4} & \textbf{97.8}\textsubscript{0.4} & 94.8\textsubscript{0.8} & 95.8\textsubscript{0.4}\\
object\_counting  & 92.0\textsubscript{1.2}  & 92.5\textsubscript{1.5}     & 92.5\textsubscript{1.1} & 93.0\textsubscript{0.7} & 92.3\textsubscript{1.1} & 92.8\textsubscript{1.9} & 93.8\textsubscript{2.3} & \textbf{95.5}\textsubscript{0.5} & 93.0\textsubscript{1.2} & \underline{93.8}\textsubscript{0.4}\\
ruin\_names   & 89.0\textsubscript{1.2} & 88.0\textsubscript{0.7}      & 88.0\textsubscript{2.4} & 87.0\textsubscript{1.2} & 84.5\textsubscript{1.1} & 89.3\textsubscript{0.4} & 89.3\textsubscript{0.8} & 87.0\textsubscript{1.2} & \textbf{90.3}\textsubscript{0.8} & \underline{90.0}\textsubscript{1.2}\\
salient\_translation\_error\_detection & 66.3\textsubscript{2.8} & 69.3\textsubscript{2.5}  & 67.0\textsubscript{2.6} & {68.5}\textsubscript{1.8} & 68.8\textsubscript{2.1} & 62.8\textsubscript{0.8} & \textbf{71.0}\textsubscript{0.7} & \underline{69.8}\textsubscript{2.0} & 69.0\textsubscript{0.7} & 67.3\textsubscript{0.4}\\
snarks   & 87.2\textsubscript{3.0} & {90.6}\textsubscript{1.2}   & 88.9\textsubscript{1.7} & \textbf{93.4}\textsubscript{1.5} & \underline{91.0}\textsubscript{1.6} & 88.9\textsubscript{2.0} & {89.9}\textsubscript{1.8} & 89.6\textsubscript{0.7} & {90.6}\textsubscript{0.6} & 83.7\textsubscript{3.5}\\
sports\_understanding & 96.5\textsubscript{1.1} & 96.3\textsubscript{0.4}   & \textbf{97.3}\textsubscript{0.4} & 95.8\textsubscript{0.4} & \underline{96.8}\textsubscript{0.8} & 93.3\textsubscript{1.1} & 95.3\textsubscript{0.4} & 91.8\textsubscript{0.4} & 95.0\textsubscript{1.2} & 95.0\textsubscript{0.0}\\
tracking\_shuffled\_objects (7) & 98.3\textsubscript{0.8} & 89.5\textsubscript{0.9} & 96.5\textsubscript{1.1} & 92.3\textsubscript{2.4} & \textbf{98.5}\textsubscript{1.5} & {98.0}\textsubscript{0.7} & 93.8\textsubscript{2.2} & \underline{98.0}\textsubscript{0.0} & 97.8\textsubscript{0.4} & 97.5\textsubscript{0.5}\\
\cmidrule(lr){1-1} \cmidrule(lr){2-6} \cmidrule(lr){7-11} 
\textit{Average}  & 80.31 & 81.55     & 83.11 & 82.98 & 82.97 & 81.61 & 82.79 & \textbf{83.79} & \underline{83.77} & 83.25 \\
$\Delta$(\textsc{bo} - \textsc{rs}) & - & -& -& -& -& \textcolor{green}{+1.30} & \textcolor{green}{+1.24} & \textcolor{green}{+0.68} & \textcolor{green}{+0.79} & \textcolor{green}{+0.28} \\
\bottomrule
\end{tabular}
}
\end{table}

\begin{table}[htb!]
\caption{Comparison between \ours with BO (\ours-\textsc{bo}) and \ours with random search (\ours-\textsc{rs}) using \texttt{gemini-1.5-{pro}-001} on BBH tasks. The \ours-\textsc{bo} results are lifted from Table~\ref{tab:gemini-pro-bbh}, and the last row denotes the average improvement due to the use of BO over RS at the milestone in the progression of \ours. Refers to captions of Table~\ref{tab:gemini-pro-bbh} for additional explanations.
}
\label{tab:gemini-pro-bbh-rs-comp}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{lcccccccccc}
\toprule
Tasks  & \multicolumn{5}{c}{\ours-\textsc{rs}} & \multicolumn{5}{c}{\ours-\textsc{bo}}  \\
\# Iterations & \textcolor{ourred}{1\textsc{o}} & \textcolor{ourblue}{1\textsc{g}} & \textcolor{ourred}{2\textsc{o}} & \textcolor{ourblue}{2\textsc{g}} & \textcolor{ourred}{3\textsc{o}} & \textcolor{ourred}{1\textsc{o}} & \textcolor{ourblue}{1\textsc{g}} & \textcolor{ourred}{2\textsc{o}} & \textcolor{ourblue}{2\textsc{g}} & \textcolor{ourred}{3\textsc{o}} \\
\cmidrule(lr){1-1} \cmidrule(lr){2-6} \cmidrule(lr){7-11} 
causal\_judgement & 66.2\textsubscript{3.0}   & 68.5\textsubscript{2.0} & 70.2\textsubscript{2.4} & 69.5\textsubscript{2.4} & \underline{70.8}\textsubscript{2.2} & 68.3\textsubscript{1.5} & 62.7\textsubscript{1.6} & 59.7\textsubscript{1.5} & \textbf{72.0}\textsubscript{0.0} & {70.0}\textsubscript{2.0}\\
date\_understanding & 88.4\textsubscript{2.3}   & 94.3\textsubscript{1.0}     & 94.1\textsubscript{1.2} & 90.3\textsubscript{3.3} & 94.3\textsubscript{1.3} & 92.2\textsubscript{1.5} & \textbf{97.0}\textsubscript{0.7} & 94.8\textsubscript{1.9} & 95.0\textsubscript{1.2} & \underline{95.5}\textsubscript{1.8}\\
disambiguation\_qa  &  75.5\textsubscript{2.1}  & 79.0\textsubscript{2.9}   & 77.4\textsubscript{1.2} & \underline{80.6}\textsubscript{2.3} & 78.4\textsubscript{4.0} & 71.8\textsubscript{2.4} & 77.5\textsubscript{3.6} & {80.5}\textsubscript{1.8} & \textbf{81.3}\textsubscript{2.9} & 78.8\textsubscript{1.5}\\ 
dyck\_languages  & 56.9\textsubscript{5.4} & 59.6\textsubscript{4.9}    & 67.5\textsubscript{4.3} & 64.9\textsubscript{4.0} & 70.4\textsubscript{2.7} & 49.2\textsubscript{2.7} & 76.2\textsubscript{3.8} & \textbf{80.0}\textsubscript{2.7} & \underline{77.5}\textsubscript{1.1} & 76.8\textsubscript{3.8}\\
formal\_fallacies &  87.4\textsubscript{1.5} & 86.8\textsubscript{2.3}    & \textbf{90.8}\textsubscript{2.1} & 88.5\textsubscript{2.2} & 88.8\textsubscript{2.2} & 86.0\textsubscript{2.1} & 85.0\textsubscript{2.5} & \underline{90.8}\textsubscript{2.3} & {90.8}\textsubscript{2.8} & 88.2\textsubscript{2.3}\\
geometric\_shapes  &  77.8\textsubscript{3.2} & 82.1\textsubscript{4.0}    & 81.8\textsubscript{2.5} & 86.5\textsubscript{3.8} & 85.5\textsubscript{2.4} & 78.5\textsubscript{2.1} & 82.5\textsubscript{3.6} & 89.2\textsubscript{3.8} & \textbf{92.3}\textsubscript{1.1} & \underline{89.2}\textsubscript{0.8}\\
hyperbaton  &  94.3\textsubscript{1.6} & 93.1\textsubscript{2.4}     & 94.2\textsubscript{1.3} & 94.9\textsubscript{1.5} & 94.0\textsubscript{1.2} & 96.5\textsubscript{0.9} & 94.2\textsubscript{1.5} & 94.8\textsubscript{2.8} & \underline{96.5}\textsubscript{0.5} & \textbf{97.2}\textsubscript{0.4}\\
logical\_deduction (7) &  70.9\textsubscript{3.3}  & 68.3\textsubscript{2.7}       & 66.6\textsubscript{2.5} & \textbf{71.9}\textsubscript{3.3} & 68.9\textsubscript{2.1} & 70.2\textsubscript{1.5} & 70.8\textsubscript{4.5} & \underline{71.7}\textsubscript{3.7} & {71.5}\textsubscript{1.8} & 69.2\textsubscript{2.2}\\
movie\_recommendation & {63.5}\textsubscript{3.2}  & 67.4\textsubscript{1.8}      & 67.4\textsubscript{2.1} & 64.6\textsubscript{2.3} & 63.4\textsubscript{2.9} & 67.0\textsubscript{1.2} & \underline{69.5}\textsubscript{0.5} & 69.3\textsubscript{3.1} & \textbf{72.8}\textsubscript{1.8} & 67.0\textsubscript{1.2}\\
multistep\_arithmetic\_two  & 97.3\textsubscript{1.1}   & 97.5\textsubscript{0.7}   & 96.9\textsubscript{0.8} & 96.1\textsubscript{1.5} & \underline{97.9}\textsubscript{0.3} & 96.2\textsubscript{0.8} & 94.5\textsubscript{1.1} & {97.0}\textsubscript{0.7} & \textbf{98.0}\textsubscript{0.7} & 96.8\textsubscript{1.8}\\
object\_counting & 95.3\textsubscript{2.4}    &\textbf{98.1}\textsubscript{1.1}     & \underline{97.3}\textsubscript{1.7} & 97.3\textsubscript{1.9} & 95.4\textsubscript{2.3} & {96.2}\textsubscript{0.4} & {96.0}\textsubscript{1.9} & 94.5\textsubscript{1.1} & 94.2\textsubscript{0.4} & 95.0\textsubscript{0.7}\\
ruin\_names  & 86.6\textsubscript{1.7}   &86.5\textsubscript{1.9}    & 88.9\textsubscript{1.8} & 89.9\textsubscript{1.2} & {87.1}\textsubscript{1.7} & \textbf{90.8}\textsubscript{1.1} & 88.8\textsubscript{1.7} & 89.2\textsubscript{1.5} & 88.8\textsubscript{2.4} & \underline{90.3}\textsubscript{0.8}\\
salient\_translation\_error\_detection  & 71.1\textsubscript{3.2} & 73.4\textsubscript{1.6}   & 73.9\textsubscript{2.2} & 71.9\textsubscript{1.5} & 70.8\textsubscript{1.6} & 68.8\textsubscript{0.8} & 71.0\textsubscript{0.7} & 69.5\textsubscript{2.2} & \underline{74.0}\textsubscript{0.7} & \textbf{74.5}\textsubscript{1.1}\\
snarks     & 93.8\textsubscript{1.6}   &95.3\textsubscript{1.4}       & 96.0\textsubscript{1.6} & 96.0\textsubscript{1.1} & 95.6\textsubscript{1.8} & 93.4\textsubscript{3.0} & 95.8\textsubscript{0.0} & 95.1\textsubscript{1.6} & \underline{96.9}\textsubscript{1.5} & \textbf{97.6}\textsubscript{1.8}\\
sports\_understanding & 93.5\textsubscript{1.7} &94.1\textsubscript{0.6}  & 95.1\textsubscript{0.9} & 95.9\textsubscript{0.9} & 96.0\textsubscript{1.7} & 92.8\textsubscript{1.9} & \textbf{97.0}\textsubscript{1.2} & \underline{96.2}\textsubscript{0.8} & 95.8\textsubscript{0.4} & 95.8\textsubscript{0.8}\\
tracking\_shuffled\_objects (7) & 92.4\textsubscript{3.8} & 94.4\textsubscript{1.2}  & 99.9\textsubscript{0.3} & 98.4\textsubscript{0.9} & \textbf{100.0}\textsubscript{0.0} & 95.8\textsubscript{0.4} & 95.0\textsubscript{1.2} & \textbf{100.0}\textsubscript{0.0} & 97.0\textsubscript{0.7} & {99.5}\textsubscript{0.5}\\
\cmidrule(lr){1-1} \cmidrule(lr){2-6} \cmidrule(lr){7-11} 
\textit{Average}   & 81.86 & 83.64        & 84.86 & 84.81 & 84.82 & 82.11 & 84.61 & 85.77 & \textbf{87.13} & \underline{86.33}\\
$\Delta$(\textsc{bo} - \textsc{rs}) & - & -& -& -& -& \textcolor{green}{+0.25} & \textcolor{green}{+0.97} & \textcolor{green}{+0.91} & \textcolor{green}{+2.32} & \textcolor{green}{+1.51} \\

\bottomrule
\end{tabular}
}
\end{table}

\paragraph{{Comparison to and combination with heuristic demonstration selection.}} {An alternative to iteratively optimize the demonstrations in the ``Optimize'' step is using heuristics for demonstration selection which may incur a lower computational cost as we no longer have to repeatedly evaluate on the labeled validation set $m$ times. In this section, we study two representative demonstration selection techniques: \textit{retrieval based on similarity in the embedding space} and \textit{diversity}, and we both study them as standalone alternatives to the full \ours pipeline and, given that demonstration selection is not the only component of the \ours framework, it is also straightforward to combine them with \ours by swapping the BO/random search component in the ``\textcolor{ourred}{Optimize}'' step with these heuristics. Below we describe the implementation details of both techniques:}


\begin{itemize}
    \item {\textbf{Retrieval}: One popular demonstration selection method is via \textit{retrieval}~\citep{rubin-etal-2022-learning, das2021case}. Concretely, we may either use an off-the-shelf pretrained embedding model (we use the latest Gecko embedding~\citep{lee2024gecko} for this purpose) or tune a customized retriever to obtain the \textit{nearest} examples from an example store, typically by computing the vector embedding for each of the test queries and each of the cached demonstrations followed by a maximum inner product search (MIPS) to retrieve the top-$k$ demonstrations based on cosine similarity. Unlike the optimization-based approach where the number of examples in the context can be determined automatically, $k$ here is a key hyperparameter that needs to be set by the user. In this case, consider 3 different $k$ values: $k=\{10, 25\}$ where the number of examples is fixed, or $k = \text{All}$, where we use all available, correctly predicted examples -- this essentially uses the same set of examples as Reinforced ICL but in a specific, input-dependent order: the examples are sorted in an ascending order based on the cosine similarity between the embedding of the test input and the example store and the most similar examples appears as the final demonstration that is directly concatenated to the test input.}
    
    \item {\textbf{Diversity}: Another popular learning-free demonstration selection method is by selecting diverse examples. While multiple ways to measure diversity exist, here we use the technique similar to the one used in \citet{zhang2023automatic} by 1) computing the embedding of all the available demonstrations and 2) running the $k$-means clustering algorithm and selecting the $k$ examples whose vector embeddings are nearest to each of the $k$ centroids. Unlike \textit{retrieval}, there is no input dependency as the clustering algorithm does not depend on the input query but similar to \textit{retrieval}, $k$ here is also a hyperparameter to be set and we again use $k = \{10, 25\}$. Note that we omit $k=\text{All}$, as otherwise the number of clusters would be equal to the number of examples and we would essentially be running Reinforced ICL with all available examples as demonstrations.}
\end{itemize}

{Since these demonstration selection baselines purely perform \textit{selection} (i.e., the ``optimize'' step of \ours) but neither the subsequent generations nor the iterative process, we first compare the BO demonstration selection (i.e., \ours at Step \textcolor{ourred}{1\textsc{o}}) against these baselines and we show the results in Table~\ref{tab:gemini-pro-heuristic-comparison}. Overall, we find that ``Diversity'' and ``Retrieval'', regardless of their hyperparameters, perform on par or slightly worse than Reinforced ICL. While the hyperparameter choice can sometimes lead to significant differences on a per-task level, we also observe that when aggregated across the tasks, it does not lead to significant differences. On the other hand, the BO selection in \ours outperforms all these baselines. We believe there are two possible explanations leading to this outperformance. Firstly, while the heuristic-based methods have lower computational costs, key hyperparameters, such as the number of demonstrations to retrieve, need to be determined a-priori. However, as we have shown in the main text at, for example, Fig.~\ref{fig:trend_analysis}, the optimal number of demonstrations can be highly task-specific, and while iterative optimization-based selection incurs a higher cost, it is also capable of optimizing the \textit{number} of demonstrations. Secondly, a key finding we have in Sec.~\ref{sec:analysis} is that not all examples are equally helpful, and \textit{removing} some examples as in-context demonstrations can sometimes lead to performance improvement during the ``Optimize'' stage. Again, while the heuristic-based approaches do not necessarily use \textit{all} demonstrations, it makes the selection choice purely from a heuristic metric (e.g., similarity to test query) rather than from a validation metric, and hence is incapable of removing these potentially ``harmful'' demonstrations from the pool of candidate examples.}

{However, beyond a simple comparison between a single stage of \ours against these methods, it is also worth noting that \textit{\ours is more than a demonstration selection} method. As such, it is also possible to  \textit{combine} these methods with \ours by using them as a drop-in replacement of the BO-based demonstration selection, effectively changing the implementation of the ``Optimize'' step \textit{only}. To test this, we test two other variants of \ours, named \ours-\textsc{retrieval} and \ours-\textsc{diversity}, where we replace the ``Optimize'' step in each round with the heuristic-driven demonstration selection mentioned above and the aggregated results are shown in Table~\ref{tab:bridge_with_heuristics_agg} whereas the task-specific breakdown of the best method in Table~\ref{tab:bridge_with_heuristics_detail} -- for conciseness, we only show the per-task breakdown for the best \ours variant (\ours-\textsc{retrieval} using all examples), which show that \ours also works well with alternative demonstration selection method, although the advantage of optimization-based selection as shown in Table~\ref{tab:gemini-pro-heuristic-comparison} carries over when we use the selection as a component in the overall \ours pipeline.}

\paragraph{Additional comparisons against iterative reinforced ICL in a restricted setup.} To provide further evidence emphasizing the need for the ``Optimize'' step and to make sure that the additional gain of \ours does not simply come from the fact that \ours may take advantage of more correctly predicted demonstrations in the validation set due to repeated sampling in the later iterations, we conduct a further experiment comparing \ours and iterative reinforced ICL, but in a restricted setup with the support set \textbf{restricted to the subset of the train set where the model predicted correctly initially}, instead of the entire train set. In other words, in subsequent iterations of \ours and iterative reinforced ICL, both methods are restricted to make use of the subset of the train set initially predicted correctly only as examples; we term these approaches ``iterative reinforced ICL (restricted)'' and ``\ours (restricted)'' respectively, and we show the results in Table~\ref{tab:bbh-restricted}. On a high level, we found the result provides further evidence of the importance of selection: Iterative Reinf ICL (restricted) without the "optimize" step actually did not meaningfully improve over standard Reinf ICL (average accuracy: 79.6\%); \ours (restricted), however, still meaningfully improves with the subsequent optimize and generate steps, although the gain is less than the original \ours which utilizes more examples via the larger train set support.



\begin{table}[t]
\caption{Comparison of \ours and iterative reinforced ICL \textbf{in the restricted setup} where the methods may only use the \textit{subset} of the train set that the model initially predicted correctly. Experiments performed on \texttt{gemini-1.5-pro-001}.
}
\label{tab:bbh-restricted}
\centering
\resizebox{0.9\textwidth}{!}{
\begin{tabular}{lccccccc}
\toprule
Tasks & \multicolumn{2}{c}{\textcolor{black}{\textit{Restricted} Iterative}} & \multicolumn{5}{c}{\textit{Restricted} \ours}  \\
& \multicolumn{2}{c}{Reinf.} &\multicolumn{5}{c}{\textit{(Ours)}} \\
\# Iterations & 1 & 2 & \textcolor{ourred}{1\textsc{o}} & \textcolor{ourblue}{1\textsc{g}} & \textcolor{ourred}{2\textsc{o}} & \textcolor{ourblue}{2\textsc{g}} & \textcolor{ourred}{3\textsc{o}} \\
\cmidrule(lr){1-1} \cmidrule(lr){2-3} \cmidrule(lr){4-8} 

causal\_judgement               & \textbf{69.7\textsubscript{1.1}} & 65.0\textsubscript{1.5}          & 67.7\textsubscript{2.7}          & 65.0\textsubscript{1.1}          & 66.0\textsubscript{1.5}          & 67.0\textsubscript{1.1}          & 65.0\textsubscript{2.0}           \\
date\_understanding             & 92.5\textsubscript{1.5}          & \textbf{94.0\textsubscript{1.0}} & 92.5\textsubscript{1.7}          & 93.0\textsubscript{1.6}          & 92.5\textsubscript{1.1}          & 93.5\textsubscript{1.7}          & 89.3\textsubscript{1.5}           \\
disambiguation\_qa              & 74.0\textsubscript{0.7}          & 75.8\textsubscript{3.0}          & 70.5\textsubscript{2.7}          & 72.3\textsubscript{1.5}          & 75.5\textsubscript{3.2}          & 71.8\textsubscript{2.4}          & \textbf{76.5\textsubscript{3.8}}  \\
dyck\_languages                 & 59.0\textsubscript{6.5}          & 53.0\textsubscript{2.9}          & 55.0\textsubscript{5.2}          & 52.3\textsubscript{5.4}          & 56.5\textsubscript{3.4}          & 57.5\textsubscript{1.7}          & \textbf{60.3\textsubscript{3.8}}  \\
formal\_fallacies               & 86.8\textsubscript{3.3}          & 90.5\textsubscript{2.2}          & 85.3\textsubscript{1.5}          & \textbf{90.5\textsubscript{1.1}} & 83.0\textsubscript{3.2}          & 83.5\textsubscript{0.9}          & 85.5\textsubscript{2.3}           \\
geometric\_shapes               & 75.3\textsubscript{1.8}          & 78.3\textsubscript{3.1}          & 75.0\textsubscript{2.6}          & 80.5\textsubscript{4.5}          & 81.3\textsubscript{4.5}          & \textbf{85.0\textsubscript{2.6}} & 80.0\textsubscript{2.6}           \\
hyperbaton                      & 85.3\textsubscript{4.0}          & 84.5\textsubscript{3.4}          & 94.0\textsubscript{1.6}          & 95.8\textsubscript{0.8}          & 91.8\textsubscript{1.3}          & 93.0\textsubscript{2.6}          & \textbf{97.0\textsubscript{0.7}}  \\
logical\_deduction (7)          & 67.5\textsubscript{1.8}          & 69.0\textsubscript{2.5}          & 69.5\textsubscript{2.7}          & \textbf{71.5\textsubscript{3.2}} & 66.8\textsubscript{2.1}          & 70.5\textsubscript{2.3}          & 70.8\textsubscript{2.2}           \\
movie\_recommendation           & 64.8\textsubscript{2.7}          & 63.3\textsubscript{2.2}          & \textbf{68.3\textsubscript{3.3}} & 62.0\textsubscript{4.1}          & 63.0\textsubscript{1.2}          & 63.3\textsubscript{1.3}          & 61.8\textsubscript{2.2}           \\
multistep\_arithmetic\_two      & 95.0\textsubscript{1.2}          & 95.5\textsubscript{0.5}          & \textbf{97.8\textsubscript{0.8}} & 95.3\textsubscript{0.8}          & 95.5\textsubscript{1.8}          & 95.8\textsubscript{1.3}          & 95.8\textsubscript{1.5}           \\
object\_counting                & 94.5\textsubscript{2.9}          & 94.5\textsubscript{2.1}          & 96.0\textsubscript{1.6}          & 94.8\textsubscript{0.8}          & 94.3\textsubscript{1.3}          & 96.0\textsubscript{0.7}          & \textbf{96.3\textsubscript{1.1}}  \\
ruin\_names                     & 87.3\textsubscript{1.3}          & 88.8\textsubscript{0.8}          & \textbf{91.5\textsubscript{0.9}} & 89.5\textsubscript{1.8}          & 88.5\textsubscript{2.1}          & 90.0\textsubscript{0.0}          & 89.8\textsubscript{1.8}           \\
salient\_translation            & 68.0\textsubscript{2.1}          & 67.3\textsubscript{1.5}          & 70.0\textsubscript{2.2}          & 69.8\textsubscript{3.3}          & 72.8\textsubscript{2.4}          & 73.5\textsubscript{2.7}          & \textbf{75.8\textsubscript{2.2}}  \\
snarks                          & 93.8\textsubscript{1.2}          & 93.8\textsubscript{2.1}          & 93.8\textsubscript{2.3}          & 95.5\textsubscript{1.2}          & 95.1\textsubscript{0.7}          & \textbf{95.5\textsubscript{0.6}} & 94.8\textsubscript{2.1}           \\
sports\_understanding           & 94.0\textsubscript{1.4}          & 95.0\textsubscript{1.0}          & 92.8\textsubscript{1.1}          & 95.5\textsubscript{1.5}          & \textbf{97.0\textsubscript{0.0}} & 96.0\textsubscript{0.7}          & 94.8\textsubscript{0.8}           \\
tracking\_shuffled\_objects (7) & 66.8\textsubscript{3.1}          & 67.3\textsubscript{1.9}          & 99.0\textsubscript{0.0}          & 96.3\textsubscript{1.5}          & 98.5\textsubscript{0.5}          & 97.5\textsubscript{1.5}          & \textbf{100.0\textsubscript{0.0}} \\

\cmidrule(lr){1-1} \cmidrule(lr){2-3} \cmidrule(lr){4-8} 
\textit{Average}  & 79.62               & 79.70               & 82.40               & 82.45               & 82.37               & \textit{83.08}      & \textbf{83.32} \\
\bottomrule
\end{tabular}
}
\end{table}


\begin{table}[htb!]
\caption{{Comparison between \ours with one step of demonstration optimization only (i.e., \textcolor{ourred}{1\textsc{o}}) against \textit{Retrieval}, \textit{Diversity} and Reinforced ICL baselines using \texttt{gemini-1.5-pro-001}. Note that the \ours (\textcolor{ourred}{1\textsc{o}}) and Reinforced ICL results are taken from Table~\ref{tab:gemini-pro-bbh}.}}
\label{tab:gemini-pro-heuristic-comparison}
\centering
\resizebox{0.9\textwidth}{!}{
\begin{tabular}{lccccccc}
\toprule
Tasks  & \multicolumn{2}{c}{Diversity} & \multicolumn{3}{c}{Retrieval} & Reinf. & \ours  \\
Details / hyperparams & $k=10$ & $k=25$ & $k=10$ & $k=25$ & All & ICL & \textcolor{ourred}{1\textsc{o}} \\
\cmidrule(lr){1-1} \cmidrule(lr){2-3}  \cmidrule(lr){4-6}   \cmidrule(lr){7-7}  \cmidrule(lr){8-8} 
causal\_judgement & 66.7\textsubscript{1.6} & 66.3\textsubscript{2.4} & 63.0\textsubscript{1.5} & 67.7\textsubscript{2.4} & 66.7\textsubscript{2.5} & 66.3\textsubscript{4.8} & 68.3\textsubscript{1.5} \\
date\_understanding & 93.2\textsubscript{1.3} & 93.0\textsubscript{2.7} & 87.0\textsubscript{3.5} & 93.3\textsubscript{1.5} & 93.0\textsubscript{1.9} &  88.8\textsubscript{2.5} & 92.2\textsubscript{1.5}  \\
disambiguation\_qa & 72.2\textsubscript{3.0} & 77.8\textsubscript{0.8} & 76.5\textsubscript{0.9} & 71.2\textsubscript{0.8} & 77.5\textsubscript{1.1} &  76.8\textsubscript{2.4} & 71.8\textsubscript{2.4} \\
dyck\_languages & 54.0\textsubscript{15.7} & 38.5\textsubscript{2.6} & 39.5\textsubscript{4.4} & 33.2\textsubscript{3.1} & 47.8\textsubscript{5.2} &  55.5\textsubscript{3.6} & 49.2\textsubscript{2.7} \\
formal\_fallacies & 85.5\textsubscript{1.5} & 85.0\textsubscript{1.9} & 88.5\textsubscript{0.5} & 88.2\textsubscript{3.0} & 84.2\textsubscript{1.9} &  86.2\textsubscript{1.1} & 86.0\textsubscript{2.1} \\
geometric\_shapes & 71.2\textsubscript{4.4} & 69.3\textsubscript{1.6} & 69.8\textsubscript{2.8} & 68.5\textsubscript{4.2} & 79.2\textsubscript{3.3}  &  80.2\textsubscript{2.8} & 78.5\textsubscript{2.1}  \\
hyperbaton & 95.0\textsubscript{1.2} & 92.2\textsubscript{2.5} & 96.5\textsubscript{1.1} & 97.2\textsubscript{1.3} & 95.2\textsubscript{1.9}  &  90.2\textsubscript{1.1} & 96.5\textsubscript{0.9}\\
logical\_deduction (7) & 65.8\textsubscript{3.0} & 67.5\textsubscript{4.4} & 69.2\textsubscript{4.4} & 66.3\textsubscript{2.9} & 67.3\textsubscript{2.4} &  65.8\textsubscript{3.5} & 70.2\textsubscript{1.5} \\
movie\_recommendation & 67.3\textsubscript{2.6} & 65.0\textsubscript{2.5} & 68.5\textsubscript{3.4} & 68.0\textsubscript{1.4} & 67.3\textsubscript{3.3} &  65.2\textsubscript{1.6} & 67.0\textsubscript{1.2} \\
multistep\_arithmetic\_two & 92.8\textsubscript{1.3} & 96.2\textsubscript{0.4} & 95.5\textsubscript{0.9} & 94.8\textsubscript{1.6} & 94.3\textsubscript{1.9} &  96.5\textsubscript{0.5} & 96.2\textsubscript{0.8}  \\
object\_counting & 95.8\textsubscript{1.1} & 95.2\textsubscript{0.8} & 97.2\textsubscript{2.4} & 95.2\textsubscript{1.9} & 91.2\textsubscript{2.2} & 95.5\textsubscript{0.9} & 96.2\textsubscript{0.4} \\
ruin\_names & 87.8\textsubscript{1.3} & 89.8\textsubscript{1.3} & 87.8\textsubscript{0.8} & 91.5\textsubscript{2.1} & 90.5\textsubscript{2.2}&  89.8\textsubscript{1.9} & 90.8\textsubscript{1.1} \\
salient\_translation\_error\_detection & 68.5\textsubscript{2.3} & 69.5\textsubscript{2.1} & 68.2\textsubscript{3.3} & 58.2\textsubscript{2.8} & 61.0\textsubscript{2.1} &  69.0\textsubscript{1.6} & 68.8\textsubscript{0.8}\\
snarks & 94.8\textsubscript{2.3} & 96.2\textsubscript{1.2} & 94.4\textsubscript{1.7} & 97.6\textsubscript{1.2} & 95.5\textsubscript{1.2} &  92.7\textsubscript{3.2} & 93.4\textsubscript{3.0}\\
sports\_understanding & 95.0\textsubscript{1.2} & 95.8\textsubscript{1.1} & 95.5\textsubscript{0.9} & 95.8\textsubscript{0.8} & 95.0\textsubscript{1.9} &  93.0\textsubscript{1.4} & 92.8\textsubscript{1.9} \\
tracking\_shuffled\_objects (7) & 55.8\textsubscript{4.5} & 56.8\textsubscript{5.5} & 60.2\textsubscript{4.3} & 67.8\textsubscript{9.7} & 60.2\textsubscript{2.4}
 &  62.3\textsubscript{4.2} & 95.8\textsubscript{0.4}\\
\cmidrule(lr){1-1} \cmidrule(lr){2-3}  \cmidrule(lr){4-6}   \cmidrule(lr){7-7}  \cmidrule(lr){8-8} 
\textit{Average} & 78.83 & 78.38 & 78.59 & 78.41 & 79.12 & 79.61 & 81.61 \\

\bottomrule
\end{tabular}
}
\end{table}

\begin{table}[htb!]
\caption{{Average test accuracy on BBH tasks using \texttt{gemini-1.5-pro-001} by combining \ours with different variants of the heuristic demonstration selection methods. Bold text in this table shows the best algorithm variant at each round of \ours.}
}
\label{tab:bridge_with_heuristics_agg}
\centering
\resizebox{0.7\textwidth}{!}{
\begin{tabular}{lccccc}
\toprule
Method  & \textcolor{ourred}{1\textsc{o}} & \textcolor{ourblue}{1\textsc{g}} & \textcolor{ourred}{2\textsc{o}} & \textcolor{ourblue}{2\textsc{g}} & \textcolor{ourred}{3\textsc{o}} \\
\cmidrule(lr){1-1} \cmidrule(lr){2-6} 
\ours-\textsc{diversity} ($k=10$) & 77.10 & 79.47 & 78.58 & 81.89 & 79.50 \\ 
\ours-\textsc{diversity} ($k=25$) & 78.15 & 80.86 & 78.74 & 80.63 & 79.68 \\ 
\ours-\textsc{nearest} ($k=10$) & 79.07 & 81.80 & 81.40 & 81.35 & 80.39 \\
\ours-\textsc{nearest} ($k=25$) & 78.36 & 79.49 & 80.16 & 81.09 & 80.10 \\
\ours-\textsc{nearest} (All) & \textbf{79.65} & \textbf{82.91} & \textbf{82.01} & \textbf{83.20} & \textbf{84.14} \\
\bottomrule
\end{tabular}
}
\end{table}

\begin{table}[htb!]
\caption{{Task-specific test accuracy on BBH tasks using \texttt{gemini-1.5-pro-001} with \ours-\textsc{nearest} (All) (best method from Table~\ref{tab:bridge_with_heuristics_agg}).}
}
\label{tab:bridge_with_heuristics_detail}
\centering
\resizebox{0.8\textwidth}{!}{
\begin{tabular}{lccccc}
\toprule
Task  & \textcolor{ourred}{1\textsc{o}} & \textcolor{ourblue}{1\textsc{g}} & \textcolor{ourred}{2\textsc{o}} & \textcolor{ourblue}{2\textsc{g}} & \textcolor{ourred}{3\textsc{o}} \\
\cmidrule(lr){1-1} \cmidrule(lr){2-6} 
causal\_judgement & 73.0\textsubscript{1.1} & 62.3\textsubscript{1.5} & 64.7\textsubscript{0.7} & 65.7\textsubscript{2.2} & 63.3\textsubscript{2.7} \\
date\_understanding & 94.3\textsubscript{1.3} & 92.0\textsubscript{1.6} & 95.0\textsubscript{1.4} & 92.2\textsubscript{2.3} & 92.8\textsubscript{0.4} \\
disambiguation\_qa & 76.8\textsubscript{0.4} & 75.8\textsubscript{5.0} & 72.0\textsubscript{1.0} & 82.0\textsubscript{2.7} & 82.8\textsubscript{0.8} \\
dyck\_languages & 58.8\textsubscript{2.3} & 75.0\textsubscript{4.3} & 75.0\textsubscript{3.3} & 78.5\textsubscript{3.0} & 82.0\textsubscript{1.2} \\
formal\_fallacies & 84.2\textsubscript{0.8} & 88.5\textsubscript{1.7} & 90.5\textsubscript{0.9} & 89.5\textsubscript{1.8} & 90.0\textsubscript{0.7} \\
geometric\_shapes & 75.8\textsubscript{2.5} & 86.2\textsubscript{3.3} & 79.8\textsubscript{0.8} & 84.0\textsubscript{2.1} & 84.5\textsubscript{1.1} \\
hyperbaton & 96.0\textsubscript{0.7} & 93.8\textsubscript{2.3} & 97.0\textsubscript{0.0} & 92.5\textsubscript{3.2} & 98.8\textsubscript{0.4} \\
logical\_deduction (7) & 65.8\textsubscript{3.7} & 73.8\textsubscript{2.3} & 68.0\textsubscript{3.7} & 70.0\textsubscript{1.9} & 71.2\textsubscript{1.8} \\
movie\_recommendation & 67.0\textsubscript{1.2} & 69.5\textsubscript{1.7} & 63.2\textsubscript{1.1} & 70.0\textsubscript{2.5} & 73.8\textsubscript{0.8} \\
multistep\_arithmetic\_two & 92.5\textsubscript{0.5} & 97.0\textsubscript{1.2} & 96.7\textsubscript{0.8} & 97.5\textsubscript{0.9} & 94.0\textsubscript{0.0} \\
object\_counting & 91.8\textsubscript{1.5} & 95.0\textsubscript{1.2} & 97.0\textsubscript{0.7} & 96.5\textsubscript{1.7} & 100.0\textsubscript{0.0} \\
ruin\_names & 88.8\textsubscript{0.4} & 92.0\textsubscript{0.7} & 88.5\textsubscript{2.1} & 89.2\textsubscript{0.8} & 88.2\textsubscript{1.1} \\
salient\_translation\_error\_detection & 63.2\textsubscript{1.5} & 70.0\textsubscript{1.6} & 70.2\textsubscript{0.4} & 70.0\textsubscript{1.2} & 70.5\textsubscript{0.5} \\
snarks & 95.8\textsubscript{1.7} & 94.8\textsubscript{1.2} & 93.7\textsubscript{1.2} & 96.5\textsubscript{1.2} & 95.8\textsubscript{0.0} \\
sports\_understanding & 94.0\textsubscript{0.7} & 96.5\textsubscript{1.5} & 93.8\textsubscript{0.4} & 95.5\textsubscript{1.5} & 94.2\textsubscript{0.4} \\
tracking\_shuffled\_objects (7) & 56.8\textsubscript{1.6} & 64.5\textsubscript{0.9} & 67.0\textsubscript{1.0} & 61.5\textsubscript{4.4} & 64.2\textsubscript{1.6} \\
\cmidrule(lr){1-1} \cmidrule(lr){2-6} 
\textit{Average} & {79.65} & {82.91} & {82.01} & \underline{83.20} & \textbf{84.14} \\
\bottomrule
\end{tabular}
}
\end{table}

\subsection{Number of examples}
\label{app:num_examples}

We show the number of examples used for each experiment corresponding to Table~\ref{tab:gemini-pro-bbh} in Table~\ref{tab:gemini-pro-bbh-n_demo}.


\begin{table}[htb!]
\caption{Number of examples for each experiment corresponding to Table~\ref{tab:gemini-pro-bbh} (\texttt{gemini-1.5-pro-001} on BBH tasks). Note that the ``All'' columns always use all 75 examples provided.
}
\label{tab:gemini-pro-bbh-n_demo}
\centering
\resizebox{0.9\textwidth}{!}{
\begin{tabular}{lcccccccc}
\toprule
Tasks  & Reinf. & \multicolumn{2}{c}{Iter.} & \multicolumn{5}{c}{\ours-\textsc{bo}}  \\
 & ICL & \multicolumn{2}{c}{Reinf.} \\
\# Iterations & 1 & 2 & 3 & \textcolor{ourred}{1\textsc{o}} & \textcolor{ourblue}{1\textsc{g}} & \textcolor{ourred}{2\textsc{o}} & \textcolor{ourblue}{2\textsc{g}} & \textcolor{ourred}{3\textsc{o}} \\
\cmidrule(lr){1-1} \cmidrule(lr){2-2} \cmidrule(lr){3-4} \cmidrule(lr){5-9} 
causal\_judgement                           & 36 & 40 & 43 & 11 & 43 & 4  & 39 & 39 \\
date\_understanding                         & 61 & 67 & 72 & 57 & 73 & 44 & 73 & 57 \\
disambiguation\_qa                          & 42 & 66 & 69 & 28 & 61 & 60 & 68 & 65 \\
dyck\_languages                             & 15 & 40 & 52 & 9  & 45 & 42 & 59 & 20 \\
formal\_fallacies                           & 60 & 69 & 69 & 2  & 63 & 30 & 67 & 57 \\
geometric\_shapes                           & 42 & 59 & 68 & 40 & 59 & 19 & 71 & 70 \\
hyperbaton                                  & 70 & 75 & 75 & 4  & 75 & 69 & 75 & 59 \\
logical\_deduction (7)          & 46 & 60 & 62 & 11 & 54 & 51 & 64 & 61 \\
movie\_recommendation                       & 42 & 53 & 54 & 39 & 49 & 36 & 51 & 41 \\
multistep\_arithmetic\_two                  & 65 & 74 & 74 & 38 & 74 & 28 & 72 & 38 \\
object\_counting                            & 65 & 75 & 75 & 60 & 75 & 48 & 75 & 14 \\
ruin\_names                                 & 58 & 70 & 71 & 51 & 70 & 69 & 69 & 21 \\
salient\_translation\_error\_detection      & 44 & 59 & 60 & 13 & 58 & 7  & 59 & 41 \\
snarks                                      & 47 & 50 & 51 & 19 & 49 & 5  & 48 & 39 \\
sports\_understanding                       & 64 & 75 & 75 & 52 & 75 & 74 & 74 & 68 \\
tracking\_shuffled\_objects (7) & 58 & 60 & 53 & 2  & 75 & 1  & 75 & 22 \\
\cmidrule(lr){1-1} \cmidrule(lr){2-2} \cmidrule(lr){3-4} \cmidrule(lr){5-9} 
\textit{Average} & 50.94 & 62.00 & 63.94 & 27.25 & 62.38 & 36.69 & 64.94 & 44.50  \\

\bottomrule
\end{tabular}
}
\end{table}


\subsection{Additional Visualizations}
\label{app:additional_visualization}

In this section, we show analysis similar to Fig.~\ref{fig:trend_analysis} on tasks not represented in the figure of the main text.
\begin{figure}[htb!]
    \centering
    \includegraphics[width=0.24\linewidth]{figs/ms_analysis_causal_judgement_bo_fixed.pdf}
    \includegraphics[width=0.24\linewidth]{figs/ms_analysis_date_understanding_bo_fixed.pdf}
    \includegraphics[width=0.24\linewidth]{figs/ms_analysis_disambiguation_qa_bo_fixed.pdf}
    \includegraphics[width=0.24\linewidth]{figs/ms_analysis_dyck_languages_bo_fixed.pdf}
    \includegraphics[width=0.24\linewidth]{figs/ms_analysis_formal_fallacies_bo_fixed.pdf}
    \includegraphics[width=0.24\linewidth]{figs/ms_analysis_hyperbaton_bo_fixed.pdf}
    \includegraphics[width=0.24\linewidth]{figs/ms_analysis_logical_deduction_seven_objects_bo_fixed.pdf}
    \includegraphics[width=0.24\linewidth]{figs/ms_analysis_movie_recommendation_bo_fixed.pdf}
    \includegraphics[width=0.24\linewidth]{figs/ms_analysis_multistep_arithmetic_two_bo_fixed.pdf}
    \includegraphics[width=0.24\linewidth]{figs/ms_analysis_object_counting_bo_fixed.pdf}
    \includegraphics[width=0.24\linewidth]{figs/ms_analysis_ruin_names_bo_fixed.pdf}
    \includegraphics[width=0.24\linewidth]{figs/ms_analysis_snarks_bo_fixed.pdf}
    \includegraphics[width=0.24\linewidth]{figs/ms_analysis_sports_understanding_bo_fixed.pdf}
    \includegraphics[width=0.24\linewidth]{figs/ms_analysis_gsmhard.pdf}
    \caption{Additional visualization of the task performance at different rounds. Note that in most datasets, additional rounds of \ours led to performance improvement, and some of the exceptions (e.g., \texttt{multi\_arithmetric\_two}) are possibly caused by visualization artifacts of the extremely small performance variation as shown by the small y-axis ranges.}
    \label{fig:additional_visualization}
\end{figure}

\subsection{Using \ours for low-resource translation}
\label{app:low_resource_translation}
While we have primarily considered the reinforced ICL setup suitable for reasoning and general problem-solving tasks, it is worth noting that the \ours framework may also generalize to other practical settings that benefit from many-shot ICL with some modification on the ``optimize'' and the ``generate'' steps. In this section, we conduct a preliminary analysis of the applicability of \ours in the context of machine translation (MT) for low-resource languages.

As noted in \citet{agarwal2024many} and \citet{reid2024gemini}, low-resource machine translation (MT) is one of the task types where many-shot in-context learning (ICL) has demonstrated remarkable performance. In these tasks, there is often a nearly monotonic improvement in translation quality as more source-target language pairs are incorporated into the context -- as a notable exception to our observations in Sec.~\ref{sec:analysis} that primarily involve reasoning tasks, in low resource MT, we often observe ``more is better'' given the information-dense nature of translation tasks -- indeed, for translation tasks, barring glaring human errors in the annotation process, the provided data is generally assumed to be of high quality and problems like false positive in model-generated reasoning paths in reasoning tasks are generally negligible for tasks like low resource MT with high quality annotated data. However, in low-resource languages, the model’s inherent knowledge is often weak or non-existent due to the lack of exposure to target languages during pre-training or fine-tuning, which can lead to a bottleneck in \textit{data availaility}, especially for extremely low-resource languages, where \textbf{1)} the model lacks zero-shot translation abilities due to insufficient exposure to target languages, and \textbf{2)} the scarcity of annotated data becomes a critical limiting factor -- to address these, previous works often attempt to augment ground-truth translation data with \textit{model-synthesized} translations \citep{han2021unsupervised, patel2022bidirectional}.

In this section, along this line of work, we investigate the applicability of \ours as a method to iteratively improve the \textit{model-synthesized} translation so that they can act as more effective augmentations to the scarce ground-truth data. Specifically, we assume the following in our setup:
\begin{itemize}
    \item Availability of some ground-truth source-target sentence pairs -- this pair will both act as the \textit{train set} from which ground-truth examples are generated and also as the validation set for machine-generated translations.
    \item Abundant \textit{source} language text -- this is almost always true. For example, if we are interested in translating from English to a low-resource language, it is extremely easy to obtain abundant text in English whereas the difficulty is to obtain the corresponding translation in the target language.
    \item LLM for ``pseudo-labeling'' -- we assume the availability of a (strong) LLM that can be queried to generate synthesized data. 
\end{itemize}

\begin{algorithm}[htb!]
\begin{footnotesize}
	    \caption{\ours for MT.}
	    \label{alg:mt_alg}
	\begin{algorithmic}[1]
		\STATE \textbf{Input}: train set $\mathcal{D}$, \textcolor{blue}{unlabeled set with source language sentence, $\mathcal{U}$}, number of iteration rounds $K \in \mathbb{N}$ (\textit{outer-loop}), evaluation budget for BO per iteration $n_{\mathrm{eval}}$ (\textit{inner-loop}), \textcolor{blue}{\textit{Generator model} used to synthesize examples $\mathcal{M}_g$}.
		\STATE \textbf{Output}: Optimized set of model-synthesized examples ${\mathcal{E}}^*$.
		\STATE \textcolor{blue}{Partition $\mathcal{D}$ into two disjoint sets $\mathcal{D}_t$ and $\mathcal{D}_v$ via random sampling.}
		\STATE \textcolor{ourblue}{\textbf{[Generate]}} Generate the pool of initial examples $\mathcal{E}_0$ by predicting \textcolor{blue}{$\mathcal{M}_g$ on the \textbf{unlabeled set}, using the entire \textbf{train} set $\mathcal{D}$ as the demonstrations in the context: $\mathcal{E}_0 \leftarrow \mathcal{M}_g (\mathcal{U} | \mathcal{D})$}.
		\FOR{${k} \in \{1, ..., K\}$ (\textcolor{gray}{\textbf{Outer} loop)}}
		\STATE \textcolor{ourred}{\textbf{[Optimize]}} Run Bayesian optimization (calling subroutine Algorithm~\ref{alg:bo} on the $\mathcal{D}_v$ to obtain $\mathbf{e}^*_k \leftarrow \textcolor{purple}{\mathrm{BayesOpt}}(n_{\mathrm{eval}}$=$n_{\mathrm{eval}}, \mathcal{E}$=$\mathcal{E}_k)$.
		\STATE \textcolor{ourblue}{\textbf{[Generate]}} {\textbf{Re-generate} examples $\mathcal{E}_k$ by re-predicting the LLM on the \textcolor{blue}{\textbf{unlabeled} set}, but with the optimized examples $\mathbf{e}^*_k$ from the previous step \textcolor{blue}{and $\mathcal{D}_t$} as demonstrations; the \{inputs, output\}-pairs are concatenated to form the new set of examples $\mathcal{E}_k$ for the next \textcolor{ourred}{[Optimize]} step.}
		\ENDFOR
		\RETURN Optimized example set $\mathcal{E}^*$ after $K$ rounds.
	\end{algorithmic}
\end{footnotesize}
\end{algorithm}

To approach the problem, we propose to retain the high-level framework of \ours but modify the ``optimize'' and ``generate'' steps to accommodate the low-resource MT setup. With reference to Algorithm~\ref{alg:mt_alg} where we have marked the key differences in \textcolor{blue}{blue}, the main difference lies in the ``\textcolor{ourblue}{generate}'' step: instead of generating examples with model-generated reasoning paths in the case presented in the main text, here we synthesized examples on the \textit{unlabeled} set $\mathcal{U}$ that we assumed to be available. Since we no longer have access to the ground-truth translation of the sentences in $\mathcal{U}$, we optimize for the optimal subset $\mathbf{e}^*$ by evaluating different combinations of the synthesized examples on the partition of the labeled dataset $\mathcal{E}_v$.

To test \ours on the MT setup, we consider the English-Bemba translation task in the Flores dataset~\citep{guzman-etal-2019-flores} that was also considered in \citet{agarwal2024many}. We assume the access to 100 labeled examples as $\mathcal{D}$ and 50 unlabeled examples $\mathcal{U}$, and hold out another 400 samples as the test set. We use Gemini Flash as the target model and Gemini Pro as the \textit{generator} model in Algorithm~\ref{alg:mt_alg}, and we show the result in Table~\ref{tab:gemini-flash-flores}. Overall, we observe that running iterative optimization also improves performance on this task, both exemplified by improvement on the test and validation chrf score, although it seems that an additional optimization round, in this case, led to a small performance degradation. While a more comprehensive evaluation is required, we believe the preliminary result is promising for future efforts in this direction.

\begin{table}[t]
\caption{Test chrf score of \texttt{gemini-1.5-flash-001}. ``Gold-only'' refers to the result obtained by only using the 100 labeled examples in the context; ``All'' refers to the result with 100 labeled examples + 50 initially generated examples from \texttt{gemini-1.5-pro-001}. Refers to captions of Table~\ref{tab:gemini-pro-bbh} for additional explanations.
}
\label{tab:gemini-flash-flores}
\centering
\resizebox{0.6\textwidth}{!}{
\begin{tabular}{lccccccc}
\toprule
Tasks  & Gold-only & All & \multicolumn{5}{c}{\ours-\textsc{mt}}  \\
\# Iterations & - & 0 &  \textcolor{ourred}{1\textsc{o}} & \textcolor{ourblue}{1\textsc{g}} & \textcolor{ourred}{2\textsc{o}} & \textcolor{ourblue}{2\textsc{g}} & \textcolor{ourred}{3\textsc{o}} \\
\cmidrule(lr){1-1} \cmidrule(lr){2-2} \cmidrule(lr){3-3} \cmidrule(lr){4-8}
en\_bem & 37.78 & 38.46 & 38.33 & 39.11 & \textbf{39.30} & 38.90 & \underline{39.29} \\
\bottomrule
\end{tabular}
}
\end{table}


\subsection{{Transferring learned demonstrations from GSM-Hard to GSM-8K}}
\label{app:transfer_learning}

{
In this section, we investigate whether the \ours-discovered demonstrations can transfer across related but distinct datasets. Specifically, we investigate the extent to which the demonstrations found on GSM-Hard (Table~\ref{tab:gemini-pro-math}) generalize to the original GSM-8K and we show the result in Table~\ref{tab:transfer}, where we compare the performance of the demonstrations directly transferred from GSM-Hard at different stages of \ours against directly optimizing on GSM-8K. We find that whereas the demonstrations generated from (iterative) reinforced ICL led to a small deterioration of GSM-8K performance, we found the transferred demonstrations from \ours led to a small improvement even though the Gemini 1.5 Pro performance on GSM-8K has been rather saturated. While optimizing directly on GSM-8K unsurprisingly led to the highest performance given that there is no distribution shift, we also find that the GSM-Hard demonstrations exhibit considerable generalizability.
}


\begin{table}[htb!]
\caption{{Comparison of the transferred \ours-generated demonstrations on GSM-Hard vs. directly running \ours on GSM-8K. Runs with performance deteriorations w.r.t. the 0-shot results are marked in red in the table.}
}
\label{tab:transfer}
\centering
\resizebox{0.8\textwidth}{!}{
\begin{tabular}{lccccccccc}
\toprule
Tasks  & 0-shot  & Reinf.  & \multicolumn{2}{c}{\textcolor{black}{Iterative}} & \multicolumn{5}{c}{\ours}  \\
& & ICL & \multicolumn{2}{c}{Reinf.} &\multicolumn{5}{c}{\textit{(Ours)}} \\
\# Iterations & - & 0 & 1 & 2 & \textcolor{ourred}{1\textsc{o}} & \textcolor{ourblue}{1\textsc{g}} & \textcolor{ourred}{2\textsc{o}} & \textcolor{ourblue}{2\textsc{g}} & \textcolor{ourred}{3\textsc{o}} \\
\cmidrule(lr){1-1} \cmidrule(lr){2-2}  \cmidrule(lr){3-3} \cmidrule(lr){4-5} \cmidrule(lr){6-10} 
Direct     & 91.92 & 93.81 & 93.06 & 92.68 & 93.81 & 93.18 & \textbf{94.70} & 94.19 & \underline{93.94} \\
Transferred & - & \textcolor{red}{90.66} & \textcolor{red}{91.79} & \textcolor{red}{91.16} & \textbf{93.81} & 92.55 & \textbf{93.81} & 93.18 & \textcolor{red}{91.16} \\
\bottomrule
\end{tabular}
}
\end{table}


\section{{Computational Cost Analysis}}
\label{app:computational_cost}

{In this section, we provide a computational cost analysis of \ours. In general, since \ours consists of multiple rounds of ``Optimize'' and ``Generate'' steps, here we analyze each step in detail.}
\begin{itemize}
    \item {\textbf{Optimize}: The cost of the ``optimize'' step depends on the budget allocated ($n_{\mathrm{eval}}$ in Line 5 of Algorithm \ref{alg:bo}), which is user-configurable. If we opt for iterative optimization (such as using Bayesian optimization in the main section of the paper, or random search in App. \ref{app:ablation_studies}), each “optimize” step thus entails $n_{\mathrm{eval}}$ LLM inferences on the validation set. As shown in the App. \ref{app:ablation_studies}, it is also possible to use a non-iterative method based on retrieval or embedding diversity, in which case each “optimize” step entails a single round of LLM inferences on the validation set (or the train set, if we use the dataset for both training and validation).}

    \item {\textbf{Generate}: The ``generate'' step always involves a single round of LLM inferences on the train set where we simply use the optimized examples from the ``optimize'' step above as demonstrations and run inference again on the train set.}
\end{itemize}


\end{document}
