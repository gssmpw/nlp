\section{Experiments}\label{sec:exp}

\begin{table}[t]
\centering
\caption{\textbf{Quantitative results on OpenCompass~\cite{2023opencompass} multimodal leaderboard.}
$^{\ddag}$ denotes closed-source models. Hall denotes HallusionBench.
}
\label{tab:exp_it_oc}
\setlength{\tabcolsep}{1pt}
\begin{tabular}{l|c|c|cccccccc}
\toprule
Models   & Params & Avg. & MM- & MM- & MM- & Math- & Hall & AI2D  & OCR- & MMVet \\
   &  &  & Bench & Star & MU & Vista &  &  & Bench & \\
\midrule
Step-1o$^{\ddag}$   & N/A   & \textbf{77.7}  & 87.3  & 69.3  & 69.9 & 74.7  & 55.8 & 89.1 & 926 & \textbf{82.8}  \\
SenseNova$^{\ddag}$  & N/A   & 77.4  & 85.7  & \textbf{72.7}  & 69.6 & \textbf{78.4}  & 57.4 & 87.8 & 894 & 78.2  \\
InternVL2.5-78B-MPO~\cite{wang2024mpo}  & 78B  & 77.0   & 87.7  & 72.1  & 68.2  & 76.6  & 58.1  & 89.2 & 909 & 73.5  \\
Qwen2.5-VL-72B~\cite{bai2025qwen25vltechnicalreport}   & 73.4B  & 76.2  & \textbf{87.8}  & 71.1  & 67.9  & 70.8  & 58.8  & 88.2  & 881  & 76.7  \\
TeleMM$^{\ddag}$   & N/A   & 75.9  & 79.9 & 70.8 & 66.6 & 75.7  & \textbf{60.6}  & 88.5 & 891 & 75.7  \\
InternVL2.5-38B-MPO~\cite{wang2024mpo}  & 38B  & 75.3  & 85.4  & 70.1 & 63.8 & 73.6 & 59.7 & 87.9 & 894 & 72.6  \\
InternVL2.5-78B~\cite{chen2024expanding}  & 78B  & 75.2 & 87.5  & 69.5 & 70 & 71.4 & 57.4 & 89.1 & 853 & 71.8   \\
Qwen2-VL-72B~\cite{qwen2-vl_2024}   & 73.4B  & 74.8  & 85.9  & 68.6  & 64.3  & 69.7  & 58.7  & 88.3  & 888  & 73.9  \\
InternVL2.5-38B~\cite{chen2024expanding}  & 38B  & 73.5  & 85.4  & 68.5  & 64.6  & 72.4  & 57.9  & 87.6  & 841  & 67.2  \\
JT-VL-Chat-V3.0$^{\ddag}$  & N/A   & 73.4  & 81.7  & 67.5  & 59.3  & 71.9  & 53.9  & 87.2  & \textbf{967}  & 69.2  \\
Taiyi$^{\ddag}$  & N/A   & 73.0  & 84.8  & 69  & 60.4  & 72.3  & 56.8  & \textbf{90.8}  & 820  & 67.9  \\
Step-1.5V$^{\ddag}$  & N/A   & 72.5 & 82.0  & 65.1  & 61.2  & 69.7  & 54.3  & 87.5  & 886  & 71.3  \\
Gemini-1.5-Pro-002$^{\ddag}$~\cite{geminiteam2024gemini15unlockingmultimodal}   & N/A   & 72.1 & 82.8  & 67.1  & 68.6  & 67.8  & 55.9  & 83.3  & 770  & 74.6  \\
InternVL2.5-26B-MPO~\cite{wang2024mpo}  & 26B  & 72.1  & 84.2  & 67.7  & 56.4  & 71.5  & 52.4  & 86.2  & 905  & 68.1  \\
GPT-4o-20241120$^{\ddag}$~\cite{openai2024gpt4ocard}  & NA   & 72.0   & 84.3  & 65.1  & \textbf{70.7}  & 59.9  & 56.2  & 84.9  & 806  & 74.5  \\
LLaVA-OneVision-72B~\cite{li2024llavaonevision}  & 73B  & 68.0  & 84.5  & 65.8  & 56.6  & 68.4  & 47.9  & 86.2  & 741  & 60.6  \\
NVLM-D-72B~\cite{nvlm2024}   & 79.4B  & 67.6  & 78.5  & 63.7  & 60.8  & 63.9  & 49.7  & 80.1  & 849  & 58.9  \\
Molmo-72B~\cite{deitke2024molmo}  & 73.3B  & 64.1  & 79.5  & 63.3  & 52.8  & 55.8  & 46.6  & 83.4  & 701  & 61.1  \\
\rowcolor{Gray} \textbf{\method-72B}   & 71.8B  & 75.1  & 86.3  & 70.7  & 57.6  & 73.3  & 56.4  & 87.6  & 889   & 79.8  \\
\midrule
\multicolumn{11}{l}{\textit{Models smaller than 20B}} \\
\midrule
Ola-7b~\cite{ola_2025}   & 8.88B   & \textbf{72.6}  & \textbf{84.3}  & \textbf{70.8}  & \textbf{57.0}  & 68.4  & \textbf{53.5}  & \textbf{86.1}  & 822  & \textbf{78.6}  \\
Qwen2.5-VL-7B~\cite{bai2025qwen25vltechnicalreport}   & 8.29B   & 70.4  & 82.6  & 64.1  & 56.2  & 65.8  & 56.3  & 84.1  & 877  & 66.6  \\
InternVL2.5-8B-MPO~\cite{wang2024mpo}   & 8B   & 70.3  & 82  & 65.2  & 54.8  & 67.9  & 51.7  & 84.5  & \textbf{882}  & 68.1  \\
MiniCPM-o-2.6~\cite{yao2024minicpm}   & 8.67B   & 70.2  & 80.6  & 63.3  & 50.9  & \textbf{73.3}  & 51.1  & 86.1  & 889  & 67.2  \\
Ovis1.6-Gemma2-9B~\cite{lu2024ovis}  & 10.2B  & 68.8  & 80.5  & 62.9  & 55.0  & 67.2  & 52.2  & 84.4  & 830  & 65.0  \\
InternVL2.5-8B~\cite{chen2024expanding}   & 8B   & 68.1  & 82.5  & 63.2  & 56.2  & 64.5  & 49.0  & 84.6  & 821  & 62.8  \\
POINTS1.5-Qwen2.5-7B~\cite{points1.5_2024} & 8.3B   & 67.4  & 80.7  & 61.1  & 53.8  & 66.4  & 50.0  & 81.4  & 832  & 62.2  \\
Valley-Eagle$^{\ddag}$   & 8.9B   & 67.4  & 80.7  & 60.9  & \textbf{57.0}  & 64.6  & 48.0  & 82.5  & 842  & 61.3  \\
Qwen2-VL-7B~\cite{qwen2-vl_2024}  & 8B   & 67.0  & 81.0 & 60.7 & 53.7 & 61.4  & 50.4 & 83 & 843 & 61.8 \\
DeepSeek-VL2~\cite{wu2024deepseekvl2}   & 16.1B  & 66.4  & 81.2  & 61.0  & 50.7  & 59.4  & 51.5  & 84.5  & 825  & 60.0  \\
VITA-1.5~\cite{fu2025vita}   & 8.3B   & 63.3  & 76.8  & 60.2  & 52.6  & 66.2  & 44.6  & 79.2  & 741  & 52.7  \\
Baichuan-Omni~\cite{baichuan-omni}   & 7B   & -  & 75.6  & -  & 47.3  & 51.9  & 47.8  & -  & 700  & 65.4  \\
LLaVA-OneVision-7B~\cite{li2024llavaonevision}   & 8B   & 61.2  & 76.8  & 56.7  & 46.8  & 58.5  & 47.5  & 82.8  & 697  & 50.6  \\
Molmo-7B-D~\cite{deitke2024molmo}   & 8B   & 58.9  & 70.9  & 54.4  & 48.7  & 47.3  & 47.7  & 79.6  & 694  & 53.3  \\
% MiniCPM-o 2.6~\cite{yao2024minicpm}   & 8B   & 70.2  & 80.5  & 64.0  & 50.4  & 71.9  & 51.9  & 85.8  & 897  & 67.5  \\
\rowcolor{Gray} \textbf{\method-9B}  & 8.8B   & 69.7  & 80.7  & 60.5  & 51.2  & 68.3  & 51.8  & 84.5  & 883 & 72.3 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[t]
  \caption{\textbf{Performance comparison on video and Interleave benchmarks} compared with existing approaches. $^*$ indicates officially released checkpoints evaluated by us. Best performance is marked \textbf{bold}. }
  \label{tab: video_n_interleave}
  \centering
  \setlength{\tabcolsep}{7.5pt}
  \begin{tabular}{lccccc}
    \toprule
       & \multicolumn{2}{c}{\textbf{VideoMME}} & \multicolumn{1}{c}{\textbf{MVBench}} & \multicolumn{2}{c}{\textbf{Llava-Interleave}}\\
    \cmidrule(r){2-3} \cmidrule(r){4-4} \cmidrule(r){5-6}
    Model & w/o subs & w subs & avg & in-domain & out-domain \\
    \midrule
     MiniCPM-V-2.6~\cite{yao2024minicpm} &  60.9 &  63.6 &  - &  - &  - \\
     LLaVA-OneVision-7B~\cite{li2024llavaonevision} &  58.2 &  - &  - &  - &  - \\
     Qwen2-VL-7B~\cite{qwen2-vl_2024} &  63.3 &  69.0 &  67.0 &  49.5$^*$ &  51.0$^*$ \\
     InternVL2-8B~\cite{chen2024far} &  56.3 & 59.3 &  65.8 &  - &  - \\
     VITA-1.5~\cite{fu2025vita} &  56.1 & 58.7 &  55.4 &  - &  - \\
     Baichuan-Omni~\cite{baichuan-omni} &  58.2 & - &  60.9 &  - &  - \\
     MiniCPM-o-2.6~\cite{yao2024minicpm} & 63.0$^*$ & 65.3$^*$ & 58.1$^*$ &  43.5$^*$ &  36.8$^*$ \\
     \rowcolor{Gray} \textbf{\method-9B} &  60.4  & 65.0 &  66.3 &  59.8 &  87.8 \\
    \midrule
    VideoLLaMA2-72B~\cite{cheng2024videollama2} & 61.4 & 63.1 & 62.0 & - & - \\
    LLaVA-OneVision-72B~\cite{li2024llavaonevision} &  66.2 &  69.5 &  59.4 &  - &  - \\
    Qwen2-VL-72B~\cite{qwen2-vl_2024} &  71.2 &  77.8 &  \textbf{73.6} &  - &  - \\
    InternVL2-Llama3-76B~\cite{chen2024far} &  64.7  & 67.8 &  69.6 &  - &  - \\
    \rowcolor{Gray} \textbf{\method-72B} &  65.2  & 67.7 &  69.6 &  \textbf{63.5} &  \textbf{89.9} \\
    \midrule
    GPT-4v~\cite{GPT4VisionSystemCard} & 59.9 & 63.3 & 43.7 & 39.2 & 57.78 \\
    GPT-4o-20240513~\cite{openai2024gpt4ocard} & 71.9 & 77.2 & - & - & - \\
    Gemini-1.5-Pro~\cite{geminiteam2024gemini15unlockingmultimodal} & \textbf{75.0} & \textbf{81.3} & - & - & - \\
    \bottomrule
\end{tabular}
\end{table}


In this section, we present a comprehensive evaluation of our \method model, comprising both quantitative and qualitative analyses of its performance. Furthermore, we conduct ablation studies to analyze the contributions of several key design components to the performance of our \method model, providing insights into their distinct impacts.

% In this section, we first evaluate the model’s performance on a variety of mainstream benchmarks, demonstrating the advantages of \method.
% Then, a series of qualitative results are presented to show the model’s specific capabilities, including multimodal understanding and free-form image generation.
% Finally, we conduct an ablation study to analyze several key components in \method.

\subsection{Quantitative Results}\label{subsec:exp_quantitative_results}

\subsubsection{Image-Text Understanding}
To evaluate the effectiveness of our \method in image-text understanding, we benchmark it against state-of-the-art MLLMs on the OpenCompass~\cite{2023opencompass} multimodal leaderboard, a widely recognized platform for multimodal evaluation. This leaderboard contains 8 different multimodal benchmarks, including complex VQA (MMBench~\cite{liu2025mmbench}, MMStar~\cite{chen2024we}, MMMU~\cite{yue2023mmmu}, AI2D~\cite{kembhavi2016diagram}, and MMVet~\cite{yu2024mm}), multimodal reasoning (MathVista~\cite{lu2024mathvista}), hallucination evaluation (Hallusionbench~\cite{Guan_2024_hallusionbench}), and OCR (OCRBench~\cite{Liu_2024}).
\cref{tab:exp_it_oc} shows the overall results. Our \method-72B model achieves top-tier performance on most benchmarks, surpassing closed-source models like GPT-4o and Gemini-1.5-Pro. Furthermore, our \method-9B model exhibits competitive performance among models of similar size, showcasing its robust capabilities in image-text understanding tasks.

% In this section, we compare our \method with leading MLLMs on the mainstream OpenCompass~\cite{2023opencompass} multimodal leaderboard to demonstrate its advancement on image-text understanding.
% This leaderboard contains 8 different multimodal benchmarks, including complex VQA (MMBench~\cite{liu2025mmbench}, MMStar~\cite{chen2024we}, MMMU~\cite{yue2023mmmu}, AI2D~\cite{kembhavi2016diagram}, and MMVet~\cite{yu2024mm}), multimodal reasoning (MathVista~\cite{lu2024mathvista}), hallucination evaluation (Hallusionbench~\cite{Guan_2024_hallusionbench}), and OCR (OCRBench~\cite{Liu_2024}).
% \cref{tab:exp_it_oc} shows the overall results.
% \method exhibits competitive performance compared with other MLLMs.
% Our \method-71B model achieves top-tier performance on most benchmarks.
% It outperforms closed-source models such as GPT-4o and Gemini-1.5-Pro.
% The \method-9B model also achieves competitive performance among other vision-language specific MLLM models smaller than 20B.
% Notably, it achieves excellent performance on MathVista, AI2D, and MMVet, demonstrating its comprehensive ability on multimodal reasoning and complex VQA.




\subsubsection{Video \& Interleaved Image-Text Understanding}

We evaluate our model's video and interleaved image-text understanding abilities on three mainstream benchmarks.

\textbf{Video-MME}~\cite{fu2024video}: Video-MME is a benchmark designed to evaluate MLLMs in full-spectrum video analysis. It encompasses a wide variety of video types across multiple domains and durations, featuring multimodal inputs such as video, subtitles, and audio. For this benchmark, testing is conducted with under 96 frames, and results are reported for both "with subtitles" and "without subtitles" settings.

\textbf{MVBench}~\cite{li2024mvbench}: MVBench serves as a video understanding benchmark aimed at thoroughly evaluating the temporal awareness of MLLMs in an open-world context. It includes 20 challenging video tasks that range from perception to cognition, which cannot be adequately addressed using a single frame. Testing for this benchmark utilizes dynamic sampling frames.

\textbf{LLaVA-Interleave Bench}~\cite{llava-next_2024}: LLaVA-Interleave Bench comprises a comprehensive suite of multi-image benchmarks collected from public datasets or generated via the GPT-4V API. It is created to assess the interleaved multi-image reasoning capabilities of MLLMs, with reported results for both "in-domain" and "out-domain" subsets.

As shown in Table~\ref{tab: video_n_interleave}, \method-9B achieves the second-best results across VideoMME and MVBench (outperformed only by Qwen2-VL-7B but requiring significantly fewer frames). However, the performance gains do not scale up to \method-72B due to limitations in the quantity of instruction-tuned video data. Moreover, both our \method-9B and \method-72B greatly surpass all other baselines in multi-image benchmarks, both in-domain and out-of-domain, highlighting their potential as strong competitors for complex tasks.


\subsubsection{Audio Understanding}

We evaluate our M2-omni model's audio understanding abilities on four mainstream benchmarks.

\textbf{Multilingual LibriSpeech (MLS)}~\cite{MLS_English}: The Multilingual LibriSpeech dataset is an extensive collection of read audiobooks sourced from Librivox, available in eight different languages. We utilize the English test set from this dataset to assess the model's speech comprehension capabilities. The latest version of this corpus comprises approximately 50,000 hours.

\textbf{Librispeech}~\cite{Librispeech}: The Librispeech corpus comprises approximately 1,000 hours of transcribed speech audio data derived from read English audiobooks. The entire dataset is categorized into three training sets (100 hours of clean, 360 hours of clean, and 500 hours of other), two validation sets (clean and other), and two test sets (clean and other). In this study, we assess our model's audio comprehension capabilities using both the clean and other testsets.

\textbf{Aishell1}~\cite{AISHELL1}:  The Aishell1 dataset comprises 178 hours of speech data, recorded by 400 speakers from various accent regions across China. It is organized into three subsets: a training set consisting of 340 speakers, a validation set with 40 speakers, and a test set featuring 20 speakers.

\textbf{AudioCaps}~\cite{AudioCaps}: AudioCaps is a comprehensive dataset featuring audio event descriptions specifically curated for the purpose of audio captioning. The sounds within this collection are derived from the AudioSet dataset. We utilize this dataset to assess the audio captioning capabilities of our \method.
 % To facilitate accurate captioning, annotators were supplied with audio tracks and corresponding categorical hints, with additional video hints provided as necessary.

The results are presented in Table~\ref{tab:exp_audio_understand}, and our \method-9B demonstrates competitive performance in speech recognition and audio captioning tasks. 
Specifically, our \method-9B is comparable to GPT-4o-Realtime~\cite{openai2024gpt4ocard}.
In addition, \method-9B significantly outperforms all other baselines on AudioCaps benchmarks, while achieving the second-best results for the MLS English, Librispeech other, Librispeech-clean and Aishell1 benchmarks.

\begin{table}[]
\centering
\caption{\textbf{Quantitative results on speech recognition and audio captioning.}
 $^*$ indicates results from \cite{yao2024minicpm}.
}
\label{tab:exp_audio_understand}
\setlength{\tabcolsep}{7pt}
\begin{tabular}{l|cccccccccc}
\toprule
Models   & MLS- & Librispeech- & Librispeech- & Aishell1 & AudioCaps \\
                & English & other & clean &  & \\
                & WER$\downarrow$ & WER$\downarrow$ & WER$\downarrow$ & WER$\downarrow$ & CIDER$\uparrow$ \\
\midrule
UIO2-L-1.1B~\cite{lu2023uio2}   & - & - & - & - & 45.7   \\
UIO2-XL-3.2B~\cite{lu2023uio2}  & - & - & - & - & 45.7   \\
UIO2-XXL-6.8B~\cite{lu2023uio2} & - & - & - & - & 48.9  \\
Whisper-large-v2~\cite{Whisper}  & \textbf{6.83} & \textbf{5.16} & 2.87 & - & - \\
Paraformer-cn~\cite{gao2022paraformer} & - & - & - & 2.12 & - \\
VITA-1.5~\cite{VITA_1.5} & - & 7.5 & 3.4 & 2.2 & - \\
Mini-Omini2~\cite{mini_omni2} & - & 9.8 & 4.8 & - & - \\
Freeze-Omini~\cite{Freeze_Omni} & - & 10.5 & 4.1 & 2.8 & - \\
MiniCPM-o-2.6~\cite{yao2024minicpm} & - & - & \textbf{1.7} & \textbf{1.6} & - \\
GPT-4o-Realtime~\cite{openai2024gpt4ocard} & - & - & 2.6$^*$ & 7.3$^*$ & - \\
\rowcolor{Gray} \textbf{\method-9B}   & 7.19 & 5.29 & 2.07 & 1.99 & \textbf{49.2} \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[t]
\centering
\caption{\textbf{Quantitative results on language benchmarks.} $^*$ indicates officially released checkpoints evaluated using the tools provided by OpenCompass~\cite{2023opencompass}.
}
\label{tab:exp_language}
\setlength{\tabcolsep}{5pt}
\begin{tabular}{cccccccc}
\hline
Tasks & MMLU & AGIEVAL & ARC-C & GPQA & MATH & HellaSwag & \begin{tabular}[c]{@{}l@{}}Avg.\\ Accuracy\end{tabular} \\ \hline
LLama3.1-8B & 69.4 & 41.2$^*$ & 83.4 & 30.4 & 51.9 & 75.1$^*$ & 58.6  \\
\rowcolor{Gray} \textbf{\method-9B} & 68.5 & 43.7 & 78.7 & 32.3 & 51.8 & 80.1 & 59.2  \\ \hline
\end{tabular}
\end{table}

\subsubsection{Audio Generation}
In this section, we also evaluated our model on the commonly-used test set: SEED-TTS test-zh. \textbf{SEED-TTS}~\cite{SEED_TTS} serves as an out-of-domain evaluation test set, comprising diverse input texts and reference speeches from various domains. We present the experimental results for \method-9B and the baseline models in Table~\ref{tab:exp_audio_generation}. As shown in Table~\ref{tab:exp_audio_generation}, our model outperforms MiniCPM-o-2.6~\cite{yao2024minicpm} in speech generation capability, achieving significant improvements in both evaluation metrics. However, our \method-9B still lags behind traditional vertical speech generation models, highlighting the need for further research and development to bridge this gap.


\subsubsection{Text-only Performance}
In this section, we assess the performance of our proposed \method-9B model and its initial counterpart, Llama3.1-8B~\cite{llama3_2024}. To evaluate the models' knowledge and examination capabilities, we employ a range of benchmarks, including AGIEVAL~\cite{zhong2023agievalhumancentricbenchmarkevaluating} and MMLU~\cite{hendrycks2021measuringmassivemultitasklanguage}. Furthermore, we utilize a diverse set of benchmarks to evaluate the models' multi-step problem-solving capabilities, including MATH~\cite{hendrycks2021measuringmathematicalproblemsolving} for mathematical derivation, HellaSwag~\cite{zellers2019hellaswagmachinereallyfinish} for commonsense reasoning in real-world contexts, ARC-C~\cite{allenai:arc} for scientific logical chains, and GPQA~\cite{rein2023gpqagraduatelevelgoogleproofqa} for critical analysis in expert-level domains. For all evaluation datasets, we adopt a generation-based assessment approach with greedy decoding.

Our experimental results, presented in \cref{tab:exp_language}, demonstrate that the performance of our proposed \method-9B model outperforms its initial counterpart, Llama3.1-8B across most evaluation datasets,   which is attributed to our multi-stage language preservation strategy and the high-quality instruction tuning data used in our training process.

% In this section, we evaluate the performance of our \method-9B and its initial Llama3.1~\cite{llama3_2024} models. To assess the models' knowledge and examination capabilities, we utilize the AGIEVAL~\cite{zhong2023agievalhumancentricbenchmarkevaluating},  MMLU~\cite{hendrycks2021measuringmassivemultitasklanguage} benchmarks. Additionally, we employ  MATH~\cite{hendrycks2021measuringmathematicalproblemsolving}, HellaSwag~\cite{zellers2019hellaswagmachinereallyfinish}, ARC-C~\cite{allenai:arc} and GPQA~\cite{rein2023gpqagraduatelevelgoogleproofqa} to evaluate the models' multi-step problem-solving ability, including mathematical derivation, commonsense reasoning in real-world contexts, scientific logical chains, and critical analysis in expert-level domains. For all evaluation datasets, we adopt a generation-based assessment approach with greedy decoding. The overall results are in \cref{tab:exp_language}.It can be observed that in most of the evaluation datasets, the performance of our \method-9B and Llama3.1~\cite{llama3_2024} models is comparable, maintaining their linguistic capabilities. Furthermore, in some rankings, our models exhibit superior performance in certain aspects compared to their text-only baseline models. This improvement is attributed to our multi-stage language preservation strategy and the high-quality instruction tuning data used in our training process.

\begin{table}[t]
\centering
\caption{
\textbf{Free-form dialogue generation evaluation results.}
}
% \vspace{3pt}
\setlength{\tabcolsep}{8pt}
\begin{tabular}{c|c|c|c}
\toprule
Model & Relevance & Fluency & Informativeness\\
\midrule
TextBind~\cite{li2023textbind} & 3.85 & 4.30 & 3.25\\
\rowcolor{Gray} \textbf{\method-9B} & 4.60 & 4.80 & 3.80\\
\bottomrule
\end{tabular}
\label{tab-model_freeform_results}
% \vspace{-12pt}
\end{table}




% For a evaluation of open-world multi-turn multimodal instruction following, we collect a test set comprising 50 conversations from realistic scenarios and utilize \method-9B to generate arbitrarily interleaved text and images in proper conversation contexts. For quantitative results, we ask GPT-4o~\cite{openai2024gpt4ocard} to rate each conversation ranging from 0 to 5 considering relevance, fluency and informativeness. We carry out our quantitative results against recent work TextBind~\cite{li2023textbind}. As shown in \cref{tab-model_freeform_results}, \method-9B exhibits overall better understanding and generating ability of multi-turn multimodal conversations. More qualitative cases can be found in \cref{fig-IT-Freeform-Result}.



\begin{table}[t]
  \caption{\textbf{Quantitative results on audio generation.} $^*$ indicates officially released checkpoints evaluated by us.}
  \label{tab:exp_audio_generation}
  \centering
  \setlength{\tabcolsep}{14pt}
  \begin{tabular}{lccccc}
    \toprule
       & \multicolumn{2}{c}{\textbf{SEED test-zh}}\\
    \cmidrule(r){2-3}
    Model & CER(\%)$\downarrow$ & SS$\uparrow$  \\
    \midrule

     Human & 1.26 &0.755 \\
     Vocoder Resyn. & 1.27 & 0.720 \\
     \midrule
     Seed-TTS~\cite{SEED_TTS} & 1.12 & 0.796 \\
     FireRedTTS~\cite{FireRedTTS} & 1.51 &0.635 \\
     MaskGCT~\cite{MaskGCT} & 2.27 & 0.774 \\
     E2-TTS(32 NFE)~\cite{E2_TTS} & 1.97 & 0.730 \\
     F5-TTS(32 NFE)~\cite{F5_TTS} & 1.56 & 0.741 \\
     CosyVoice~\cite{CosyVoice} &3.63 &0.723 \\
     CosyVoice2~\cite{CosyVoice2} &1.45 &0.748 \\
     CosyVoice2-S~\cite{CosyVoice2} &1.45 &0.753 \\
     CosyVoice2-S~\cite{CosyVoice2} &1.45 &0.753 \\
     \midrule
     MiniCPM-o-2.6~\cite{yao2024minicpm} &8.03$^*$ &0.474$^*$ \\
     \rowcolor{Gray} \textbf{\method-9B} &  6.36  & 0.604 \\
    \bottomrule
\end{tabular}
\end{table}


\subsubsection{User Experience Evaluation}\label{sec:human_evaluation}
\textbf{Evaluation Metric}:
Current benchmarks such as MMBench~\cite{liu2025mmbench}, MMStar~\cite{chen2024we}, and MMMU~\cite{yue2023mmmu} primarily focus on assessment through judgment-style questions. However, this assessment does not align with the users' actual interactive experience with MLLMs. To address this limitation, drawing inspiration from SuperclueV~\cite{supercluev}, we develop evaluation criteria specifically for assessing the models' performance on user experience, which contains four key dimensions: relevance, fluency, informativeness, and format rationality. \textit{Relevance} assesses the extent to which the model's responses align with both the provided prompts and the multimodal inputs.
\textit{Fluency} evaluates the naturalness, smoothness, clarity, comprehensibility, and anthropomorphic quality of the model's responses.
\textit{Informativeness} measures the extent to which the model's responses provide relevant information, knowledge, and analytical reasoning, enhancing their utility, detail, depth, and innovation.
\textit{Format rationality} examines the model's ability to adaptively generate appropriately structured and clear formats, for presenting results based on varying prompt types.



% Current benchmarks such as MMBench~\cite{liu2025mmbench}, MMStar~\cite{chen2024we}, and MMMU~\cite{yue2023mmmu} primarily focus on assessment through judgment-style questions. However, this assessment does not align with the users' actual interactive experience with MLLMs. Drawing inspiration from SuperclueV~\cite{supercluev}, we develop evaluation criteria specifically for assessing the models' experience performance, which contains four key dimensions: relevance, fluency, content richness, and format rationality. \textbf{Relevance} assesses the extent to which the model's responses align with both the provided prompts and the multi-modal inputs.
% \textbf{Fluency} evaluates the naturalness, smoothness, clarity, comprehensibility, and anthropomorphic quality of the model's responses.
% \textbf{Content richness} gauges the degree to which the model's responses are enriched with supplementary information, knowledge, and analytical reasoning, enhancing their utility, detail, depth, and innovation.
% \textbf{Format rationality} examines the model's ability to adaptively generate appropriately structured and clear formats for presenting results based on varying prompt types.


\begin{table}[t]
\centering
\caption{
\textbf{Detailed model experience evaluation standards.}
}
% \vspace{3pt}
\setlength{\tabcolsep}{4pt}
\begin{tabular}{c|c}
\toprule
Score & Description\\
\midrule
1 & Totally unsatisfied, totally unacceptable \\
2 & Basically not satisfied, with many obvious problems \\
3 & Generally satisfied, with a few obvious problems \\
4 & Basically satisfied, minor flaws allowed \\
5 & Completely satisfied, almost perfect \\
\bottomrule
\end{tabular}
\label{tab-model_expr_standards}
% \vspace{-12pt}
\end{table}

\textbf{Evaluation Dataset}: We collect chat samples from the actual users' multi-turn interaction dialogues, which cover a variety of tasks, including visual question answering (VQA), conversational interactions, chart interpretation, mathematical problem-solving, optical character recognition (OCR), and other related tasks. GPT-4o~\cite{openai2024gpt4ocard} is instructed to follow the evaluation criteria to generate initial reference answers for these collected samples. To ensure accuracy, human annotators refine the initial responses generated by GPT-4o. This process yields an evaluation dataset with nearly 300 samples, each with a corresponding ground truth.

We utilize GPT-4o to evaluate the model's responses against the ground truth, adhering to the standards outlined in  \cref{tab-model_expr_standards}.  As shown in \cref{tab-user_experience},  our M2-omni model, after undergoing  alignment tuning,  demonstrates an average increase of 5.7\%-23.4\% in user experience performance, which is further validated by human annotations on selected cases. Meanwhile, our model's performance on the OC benchmark across other modalities remains relatively consistent, thereby demonstrating the effectiveness of our unified training strategy, which integrates DPO and instruction tuning in the alignment tuning stage.

% We employ GPT-4o to score the models' responses compared with ground truth according to the standards of \cref{tab-model_expr_standards}.  \cref{tab-user_experience} shows the model after alignment tuning demonstrates an average increase of 5.7\% in performance. This enhancement is corroborated by human annotations on selected cases. Simultaneously, the general capabilities on OC benchmark across other modalities remain nearly the same, with a decrease in average evaluation scores of less than 1\%. This demostrates the effectiveness of our unified training strategy that integrates DPO and
% instruction tuning in alignment tuning stage.


\subsubsection{Free-Form Dialogue Generation}
To evaluate the open-world multi-turn multimodal instruction following capabilities of our model, we create a test set consisting of 50 conversations derived from realistic scenarios. We utilize \method-9B to generate arbitrarily interleaved text and images in proper conversation contexts.
For quantitative results, following our user experience evaluation metric, we employ GPT-4o to rate each conversation on a scale of 0 to 5 across three evaluation dimensions: relevance, fluency, and informativeness.
We carry out our quantitative results against recent work TextBind~\cite{li2023textbind}. As shown in \cref{tab-model_freeform_results}, \method-9B exhibits overall better understanding and generating ability of multi-turn multimodal conversations. More qualitative cases can be found in \cref{fig-IT-Freeform-Result}.





\begin{table}[t]
\centering\footnotesize
\caption{
\textbf{Detailed evaluation on user experience benchmark and OC benchmark. OC is short for the OpenCompass image-text understanding benchmark.}
}
% \vspace{3pt}
\setlength{\tabcolsep}{3pt}
\begin{tabular}{c|c|c|c|c|c|c}
\toprule
Model & Relevance & Fluency & Informativeness & Format Rationality & Expr. Avg($\Delta$\%) & OC Avg($\Delta$)\\
\midrule
\method-9B & 4.556 & 4.036 & 2.742 & 3.573 & 3.726 & -\\
\rowcolor{Gray} \method-9B-Align & 4.893 & 4.735 & 4.118 & 4.644 & 4.598(+23.4\%) & -0.3\\
\method-72B & 4.942 & 4.689 & 3.267 & 4.265 & 4.351 & -\\
\rowcolor{Gray} \method-72B-Align & 4.946 & 4.875 & 3.961 & 4.615 & 4.598(+5.7\%) & -0.2\\
InternVL2-26B~\cite{internvl_2024} & 4.886 & 4.76 & 4.15 & 4.52 & 4.577 & -\\
GPT-4o~\cite{openai2024gpt4ocard} & 5 & 4.878 & 3.854 & 4.831 & 4.64 & -\\
\bottomrule
\end{tabular}
\label{tab-user_experience}
% \vspace{-12pt}
\end{table}



\subsection{Qualitative Results}\label{subsec:exp_qualitative_results}

In this section, we qualitatively assess the capabilities of our \method, presenting examples of each modality and different tasks.

We show multimodal understanding abilities of our \method in \cref{fig-exp_case_all}. \method demonstrates promising capabilities in processing cross-modal problems, encompassing image understanding, video understanding, interleaved image-text understanding, and image-audio understanding. More examples can be found in the appendix, provided in \cref{subsec:appendix_cases}.

\cref{fig-IT-Freeform-Result} illustrates the model's ability to generate free-form dialogue, where our \method can create images based on the conversation context without explicit user input, useful for explaining ideas to users.




\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/case_exp.pdf}
    \caption{
    \textbf{Cases for multimodal understanding.}
    \method shows great potential to solve various multimodal problems.
    }
    \label{fig-exp_case_all}
\end{figure}




\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/free_form_gen.pdf}
    \caption{
    \textbf{Cases for Free-Form Dialogue Generation.}
    }
    \label{fig-IT-Freeform-Result}
\end{figure}


\subsection{Ablation Study}\label{subsec:exp_ablation}

\begin{table}[t]
\centering
\caption{\textbf{Ablation studies on step balancing strategy.} The loss weight setting [1,1,1] corresponds to the uniform weighting of the loss functions for image-text pairs, interleaved image-text, and video datasets.  * and \# represent the loss weight settings. * is obtained through experimental trials and parameter tuning. \# is obtained by normalizing the loss weights using the inverse of the loss at convergence, as described in Section \cref{subsubsec-Step Balancing Strategy}. We evaluate the few-shot performance on VQA tasks and the zero-shot performance on the captioning task of our pre-trained model.}
\label{tab:ablation_step_balance_pretrain}
\setlength{\tabcolsep}{4pt}
\begin{tabular}{c|c|ccc}
\toprule
\multicolumn{1}{l|}{Data Sample Balance} & Loss Weight Balance & \multicolumn{1}{l}{OK-VQA(4-shot)} & \multicolumn{1}{l}{VQAv2(4-shot)} & \multicolumn{1}{l}{Flickr30k(0-shot)} \\ \hline
Random Sample                        & {[}1,1,1{]}          & 40.5                             & 54.3                             & 87.0                                 \\
Round-robin                          & {[}1,1,1{]}          & 41.6                             & 54.4                             & 88.1                                 \\
Accumulation                         & {[}1,1,1{]}          & 41.7                             & 54.6                             & 88.2                                 \\
Accumulation                         & ${[}0.2,1.0,0.03{]}^{*}$   & 39.7                             & 52.5                             & 87.1                                 \\
Accumulation                         & ${[}0.45,0.36,1.09{]}^{\#}$ & \textbf{42.1}                             & \textbf{55.4}                             & \textbf{88.2}                                 \\
\bottomrule
\end{tabular}
\end{table}


In this section, we conduct ablation studies to investigate the effectiveness of our step balance strategy and dynamic adaptive balance strategy in our M2-omni model. These experiments aim to provide insights into the impact of these key components on our M2-omni’s performance.

\subsubsection{Step Balance Strategy}\label{subsubsec:step_balance_ablaton}

As described in \cref{subsubsec-Step Balancing Strategy} ,  we investigate the impact of various data sample balancing strategies and loss weight balancing schemes on the multimodal joint training stage of pre-training. We evaluate the performance of candidate strategies on two VQA benchmarks, OK-VQA~\cite{marino2019ok} and VQAv2~\cite{goyal2017making}, and assess its image captioning performance using the Flickr30k~\cite{young2014image} benchmark.

For pretrained models lacking in instruction following ability, to assess the effectiveness of our approach, we evaluate the performance of these models on VQA tasks using a few-shot approach and on image caption tasks using a zero-shot approach. \cref{tab:ablation_step_balance_pretrain} presents the results of our M2-omni pretrained models, which demonstrate the effectiveness of our step balance strategy.

% Besides, three task weighting manner are compared: [1,1,1], which means all data shares the same optimization step size; [0.2,1.0,0.03], which is consistent with that proposed in \cite{alayrac2022flamingo}; [0.45,0.36,1.09], the inverse of the loss at convergence state, as \cref{subsubsec-Step Balancing Strategy} described. Note that the three values in the ratio correspond to image-text pairs, interleaved image-text and video datasets.

%  We directly evaluate the pre-trained model's performance on VQA tasks using a few-shot approach and on image caption tasks using a zero-shot approach. For VQA tasks, we use two benchmarks: OK-VQA~\cite{marino2019ok} and VQAv2~\cite{goyal2017making}, while for image captioning, we use the Flickr30k~\cite{young2014image} benchmark. \cref{tab:ablation_step_balance_pretrain} shows the results of training models on the combined datasets using three different merging regimes. It can be observed that the accumulation strategies and setting the task weights to the inverse of the loss achieve the best performance.







\begin{table}[t]
\centering
\caption{
\textbf{Ablation results of the dynamic adaptive balance strategy}. Results for unimodal baselines are derived from the following single-modal models: \textsuperscript{$\dagger$} Image-Text Model, \textsuperscript{$\ddagger$} Video-Text Model, and \textsuperscript{
$\mathsection$} Audio-Text Model. The best result for each benchmark is \textbf{bolded}, while the best result for each model across all epochs is \underline{underlined}.
}
% \vspace{3pt}
\setlength{\tabcolsep}{3pt}
\begin{tabular}{l|l|ccccc|cc|cc}
\toprule
Models &  & MM- & OK- & VQAv2 & Text- & GQA & MSVD- & MSRVTT & Audio & MLS- \\
& & Bench & VQA &&VQA&& QA & QA & Caps & English($\downarrow$) \\
\midrule
\multirow{3}{*}{\makecell[l]{Single-modal\\Baselines}} & ep1 & 68.0\textsuperscript{$\dagger$} & 56.4\textsuperscript{$\dagger$} & 74.8\textsuperscript{$\dagger$} & \underline{70.4}\textsuperscript{$\dagger$} & 58.4\textsuperscript{$\dagger$} & 72.3\textsuperscript{$\ddagger$} & 59.3\textsuperscript{$\ddagger$} & 29.0\textsuperscript{$\mathsection$} & 11.4\textsuperscript{$\mathsection$} \\
& ep2 & \underline{\textbf{77.8}}\textsuperscript{$\dagger$} & \underline{59.8}\textsuperscript{$\dagger$} & \underline{76.9}\textsuperscript{$\dagger$} & 69.8\textsuperscript{$\dagger$} & 60.6\textsuperscript{$\dagger$} & \underline{\textbf{76.5}}\textsuperscript{$\ddagger$} & \underline{60.1}\textsuperscript{$\ddagger$} & \underline{39.9}\textsuperscript{$\mathsection$} & 9.33\textsuperscript{$\mathsection$} \\
& ep3 & 77.3\textsuperscript{$\dagger$} & 58.0\textsuperscript{$\dagger$} & 76.8\textsuperscript{$\dagger$} & 69.1\textsuperscript{$\dagger$} & \underline{60.8}\textsuperscript{$\dagger$} & 74.4\textsuperscript{$\ddagger$} & 58.6\textsuperscript{$\ddagger$} & 39.5\textsuperscript{$\mathsection$} & \underline{8.96}\textsuperscript{$\mathsection$} \\
\midrule
\multirow{3}{*}{\makecell[l]{Mixture \\w/o MM-Bal.}}
& ep1 & 70.5 & 55.9 & 75.7 & 70.2 & 57.7 & \underline{75.1} & \underline{59.6} & 27.5 & 12.1 \\
& ep2 & \underline{75.8} & \underline{58.8} & \underline{77.0} & \underline{70.5} & \underline{\textbf{61.1}} & 73.4 & 58.5 & 33.5 & 9.45 \\
& ep3 & 75.6 & 58.4 & 76.5 & 69.5 & 60.1 & 70.2 & 56.9 & \underline{39.6} & \underline{8.98} \\
\midrule
\multirow{3}{*}{\makecell[l]{Mixture \\w/ MM-Bal.}}
& ep1 & 74.7 & 59.6 & 76.0 & 71.2 & 59.0 & 73.1 & 58.7 & 35.5 & 9.27 \\
& ep2 & \underline{\textbf{77.8}} & \underline{\textbf{61.7}} & \underline{\textbf{77.2}} & \underline{\textbf{71.8}} & 60.5 & \underline{74.8} & 58.5 & 41.2 & 8.31 \\
& ep3 & 77.1 & 60.5 & 77.0 & 69.8 & \underline{60.7} & 74.6 & \underline{\textbf{60.2}} & \underline{\textbf{44.1}} & \underline{\textbf{8.04}} \\

\bottomrule
\end{tabular}
\label{tab-multi_task_balanced_ablation}
% \vspace{-12pt}
\end{table}

\subsubsection{Dynamic Adaptive Balance Strategy}

We conducted a evaluation of our dynamic adaptive balance strategy across text-image, video, and audio modalities using constrained datasets. The evaluation was conducted on benchmark datasets specific to each modality: for text-image tasks, MMbench~\cite{liu2025mmbench}, OK-VQA~\cite{marino2019ok}, VQAv2~\cite{goyal2017making}, TextVQA~\cite{singh2019towards}, and GQA~\cite{hudson2019gqa} were employed; for video, MSVD-QA~\cite{xu2017video} and MSRVTT-QA~\cite{xu2017video} benchmarks were utilized; and for audio, we assessed performance on the AudioCaps~\cite{kim2019audiocaps} (AAC) and MLS~\cite{Pratap2020MLSAL}-English (ASR) tasks. The experimental outcomes are detailed in Table~\ref{tab-multi_task_balanced_ablation}.

In contrast to actual training pipeline, our evaluation involved instruction tuning starting from pre-trained models. Specifically, for each modality, we initially trained single-modality baseline models (the 'Sinle-modal Baselines' in Table~\ref{tab-multi_task_balanced_ablation}) individually over three epochs to establish the maximum achievable performance per modality. The results indicate that optimal performance was predominantly observed by the second epoch. However, the ASR task, due to its more complex patterns, had not fully converged even by the third epoch. Subsequently, we combined data from all three modalities to train a unified model (the 'Mixture w/o MM-Bal.' in Table~\ref{tab-multi_task_balanced_ablation}). Under this multimodal training regimen, the image-text modality reached its optimal performance at the second epoch, while the video modality achieved peak performance as early as the first epoch and with performance consistently decreasing in subsequent epochs. In contrast, the audio modality demonstrated continuous improvement, attaining its best performance by the third epoch. These observations underscore the imbalance in training progress among different modalities when engaged in multimodal training.

To address this imbalance, we introduced the dynamic adaptive balance strategy within our M2-omni training framework. This strategy dynamically adjusts the loss weights for each modality based on their respective training progress. In the context of this evaluation, it accelerates the training of the audio modality while appropriately reducing the learning weights for the image-text and video modalities to prevent overfitting. The evaluation results for this balanced training approach are denoted as 'Mixture w/ MM-Bal.' in Table~\ref{tab-multi_task_balanced_ablation}. The results demonstrate that, although some degree of imbalance among modalities persists, the balanced training strategy significantly alleviates the issues observed with simple mixed training: optimal performances across benchmarks are now concentrated around the second and third epochs, and performance across all modalities has been markedly enhanced. Moreover, under the balanced training strategy, the model achieved single-modality optimal performance in 7 out of 9 benchmarks. The best-performing model (at epoch 2) surpassed the optimal performance of each single-modality baseline in 6 out of 9 benchmarks (MMBench, OK-VQA, VQAv2, TextVQA, AudioCaps, MLS-English). Additionally, for the audio modality, the model at epoch 3 outperformed the single-modality baselines in 5 out of 9 benchmarks (OK-VQA, VQAv2, MSRVTT-QA, AudioCaps, MLS-English), with significant improvements in audio performance. These experimental results highlight the effectiveness of our dynamic adaptive balance strategy.
