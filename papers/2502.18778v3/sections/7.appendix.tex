\renewcommand\thefigure{S\arabic{figure}}
\renewcommand\thetable{S\arabic{table}}
\renewcommand\theequation{S\arabic{equation}}
\renewcommand\thealgorithm{S\arabic{algorithm}}
\setcounter{equation}{0}
\setcounter{table}{0}
\setcounter{figure}{0}
\setcounter{section}{0}
\setcounter{algorithm}{0}
\renewcommand\thesection{\Alph{section}}
% \setlength{\belowcaptionskip}{0pt}
\captionsetup[subfigure]{skip=0pt}

\section*{Appendix}

\section{More Qualitative Results}\label{subsec:appendix_cases}

In this section, we show examples of different modality tasks to demonstrate the capabilities of our \method model.

\textbf{Image Understanding.}
As shown in \cref{fig-exp_case_it_ocrs}, our \method model demonstrates an adeptness in complex OCR and reasoning.
And \cref{fig-exp_cases_it_reasoning}, the model displays its ability in flow chart understanding, and mathematical problem solving.
These cases demonstrate the modelâ€™s ability to handle various complex image-text understanding tasks.

\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/case_ocrs.pdf}
    \caption{
    \textbf{Cases for complex OCR.}
    }
    \label{fig-exp_case_it_ocrs}
\end{figure}

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.8\linewidth]{figures/case_ocr3.pdf}
%     \caption{
%     \textbf{Case for complex OCR.}
%     }
%     \label{fig-exp_case_it_ocr2}
% \end{figure}

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.8\linewidth]{figures/case_ocr4.pdf}
%     \caption{
%     \textbf{Case for OCR and reasoning.}
%     }
%     \label{fig-exp_case_it_ocr4}
% \end{figure}

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.8\linewidth]{figures/case_description.pdf}
%     \caption{
%     \textbf{Case for image description.}
%     }
%     \label{fig-exp_case_it_description}
% \end{figure}

\begin{figure}[h]
    \centering
    \begin{subfigure}{0.95\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/case_recipe.pdf}
        \vspace{-8pt}
        % \abovecaptionskip
        % \belowcaptionskip
        \caption{Flow chart understanding}
        \label{fig:case_recipe}
    \end{subfigure}
    \begin{subfigure}{0.95\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/case_math.pdf}
        \vspace{-8pt}
        \caption{Mathematical problem solving}
        \label{fig:case_math}
    \end{subfigure}
    % \includegraphics[width=0.8\linewidth]{figures/case_recipe.pdf}
    \caption{
    \textbf{Cases for information extraction and visual reasoning.}
    }
    \label{fig-exp_cases_it_reasoning}
\end{figure}

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.8\linewidth]{figures/case_math.pdf}
%     \caption{
%     \textbf{Case for mathematical problem solving.}
%     }
%     \label{fig-exp_case_it_math}
% \end{figure}

\textbf{Interleave Image-Text Understanding.}
As shown in \cref{fig-exp_case_it_interleave}, the model shows its excellent ability in processing interleaved images-text.
The model has the ability to synthesize and understand the content of multiple images.

\begin{figure}[h]
    \centering
    \begin{subfigure}{0.95\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/case_interleave3.pdf}
        \vspace{-8pt}
        \caption{Multi-turn dialogue}
        \label{fig:case_interleave3}
    \end{subfigure}
    \begin{subfigure}{0.95\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/case_interleave2.pdf}
        \vspace{-8pt}
        \caption{Multi-image understanding}
        \label{fig:case_interleave2}
    \end{subfigure}
    % \includegraphics[width=0.8\linewidth]{figures/case_interleave3.pdf}
    \caption{
    \textbf{Cases for interleaved image-text understanding.}
    }
    \label{fig-exp_case_it_interleave}
\end{figure}

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.8\linewidth]{figures/case_interleave2.pdf}
%     \caption{
%     \textbf{Case for interleaved image-text understanding.}
%     }
%     \label{fig-exp_case_it_interleave2}
% \end{figure}

\textbf{Video Understanding.}
\cref{fig-exp_case_video1} displays several cases for video understanding.
We can see that our model can accurately capture the details in the video and understand the events in videos.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/case_video1.pdf}
    \caption{
    \textbf{Case for detailed video understanding.}
    }
    \label{fig-exp_case_video1}
\end{figure}

\textbf{Audio QA.}
As shown in \cref{fig-exp_case_audio_qa}, the model can accurately understand the content of the audio and answer questions combined with the image/video content, which demonstrates the model's multimodal understanding ability.

\begin{figure}[h]
    \centering
    \begin{subfigure}{0.95\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/case_image_audioqa.pdf}
        \vspace{-8pt}
        \caption{Image audio QA}
        \label{fig:case_image_audioqa}
    \end{subfigure}
    \begin{subfigure}{0.95\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/case_video_audioqa.pdf}
        \vspace{-8pt}
        \caption{Video audio QA}
        \label{fig:case_video_audioqa}
    \end{subfigure}
    % \includegraphics[width=0.8\linewidth]{figures/case_image_audioqa.pdf}
    \caption{
    \textbf{Cases for audio QA.}
    }
    \label{fig-exp_case_audio_qa}
\end{figure}

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.8\linewidth]{figures/case_video_audioqa.pdf}
%     \caption{
%     \textbf{Case for video audio QA.}
%     }
%     \label{fig-exp_case_video_audio_qa}
% \end{figure}


\section{Training Data Details}\label{subsec:appendix_data}

In this section, we list the training data in detail, including data sources, sampling ratios, data volumes, and language information.


\begin{table}[h]
\centering
\caption{\textbf{Details of open source data in pre-training stage.}
}
\label{tab:appendix_pretrain_data}
\setlength{\tabcolsep}{7pt}
\begin{tabular}{l|l|l|l|l}
\hline
Dataset & Size & Ratio (\%) & Class & Language \\
\hline
Laion 5B~\cite{schuhmann2022laion} & 1555342102 & 67.10 & Image-Text & English \\
Zero~\cite{xiezero} & 151766788 & 6.55& Image-Text & Chinese \\
COYO~\cite{byeon2022coyo} & 61915700 &2.67& Image-Text & English \\
SBU~\cite{ordonez2011im2text} & 777671 & 0.03& Image-Text & English \\
CC3M~\cite{sharma2018conceptual} & 3105571 &0.13 & Image-Text & English \\
CC12M~\cite{changpinyo2021conceptual} & 10337890 & 0.45& Image-Text & English \\
Object365~\cite{shao2019objects365} & 1062993 & 0.05& Image-Text & English \\
Datacomp~\cite{gadre2024datacomp} & 187843384 & 8.10& Image-Text & Chinese \\
Clotho~\cite{Clotho} & 14465 & 0.00& Audio-Text & English \\
AudioCaps~\cite{AudioCaps} & 50939 &0.00 & Audio-Text & English \\
Libriheavy~\cite{Libriheavy} & 10454844 &0.45& Audio-Text & English \\
MACS~\cite{MACS} & 17275 & 0.00& Audio-Text & English \\
AudioSet~\cite{AudioSet} & 1909428 & 0.08& Audio-Text & English \\
VGGSound~\cite{VGGSound} & 95495 & 0.00& Audio-Text & English \\
AISHELL1~\cite{AISHELL1} & 134424 &0.01& Audio-Text & Chinese \\
WenetSpeech~\cite{WenetSpeech} & 14625245 & 0.63& Audio-Text & Chinese \\
GigaSpeech~\cite{GigaSpeech} & 8282988 & 0.36& Audio-Text & English \\
Librispeech~\cite{Librispeech} & 286652 &0.01 & Audio-Text & English \\
CoVoST2\_en\_en~\cite{CoVoST2} & 304937 & 0.01& Audio-Text & English \\
CoVoST2\_en\_zh-CN~\cite{CoVoST2} & 304937 &0.01 & Audio-Text & Chinese \\
CoVoST2\_zh-CN\_en~\cite{CoVoST2} & 11928 & 0.00& Audio-Text & English \\
CoVoST2\_zh-CN\_zh-CN~\cite{CoVoST2} & 11928 & 0.00& Audio-Text & Chinese \\
WavCaps~\cite{WavCaps} & 275282 & 0.01& Audio-Text & English \\
SPGISpeech~\cite{SPGISpeech} & 1966109 &0.08 & Audio-Text & English \\
KeSpeech~\cite{KeSpeech} & 973583 & 0.04& Audio-Text & Chinese \\
AliMeeting~\cite{AliMeeting} & 372886 &0.02& Audio-Text & Chinese \\
magicdata\_755h~\cite{MagicData_RAMC} & 584976 &0.03& Audio-Text & Chinese \\
MagicData\_RAMC~\cite{MagicData_RAMC} & 164972 &0.01 & Audio-Text & Chinese \\
Primewords\_100h~\cite{Primewords_100h} & 50383 & 0.00& Audio-Text & Chinese \\
FreeST~\cite{FreeST} & 102600 & 0.00& Audio-Text & Chinese \\
tal\_zh100h~\cite{TAL} & 93963 & 0.00& Audio-Text & Chinese \\
tal\_zhen587h~\cite{TAL} & 354680 & 0.02& Audio-Text & Chinese \\
aidatatang\_200zh~\cite{aidatatang_200zh} & 189121 & 0.01& Audio-Text & Chinese \\
TED\_LIUM\_v2~\cite{TED_LIUM} & 93437 & 0.00& Audio-Text & English \\
SlideSpeech~\cite{SlideSpeech} & 483731 & 0.02& Audio-Text & English \\
MMC4~\cite{zhu2024multimodal} & 36870000 & 1.59& Interleaved Image-Text & English \\
WuDao~\cite{yuan2021wudaocorpora} & 49896202 & 2.15& Text & Chinese \\
Pile~\cite{gao2020pile} & 136514954 & 5.89& Text & English \\
WebVid~\cite{bain2021frozen} & 1032719 & 0.04& Video-Text & English \\
Youku-mPLUG~\cite{xu2023youku} & 1098244 & 0.05& Video-Text & Chinese \\
CDIP-OCR~\cite{soboroff2022complex} & 9715288 & 0.42& OCR & English \\
OCR-IDL~\cite{biten2022ocr} & 26473994 & 1.14& OCR & English \\
Webvicob~\cite{kim2023web} & 27179852 & 1.17& OCR & English \\
WordScape~\cite{weber2023wordscape} & 8492083 & 0.37& OCR & English \\
MARIO-OCR~\cite{chen2024textdiffuser} & 6172608 & 0.27& OCR & English \\
\hline
\end{tabular}
\end{table}


\begin{table}[h]
\centering
\caption{\textbf{Details of open source image-text data in IT-Stage-1-Data (Part-1).}
}
\label{tab:appendix_sft_s1_it_1}
\setlength{\tabcolsep}{7pt}
\begin{tabular}{l|l|l|l|l}
\toprule
Dataset                         & Size    & Ratio (\%) & Data Class    & Language        \\
\midrule
TextCaps~\cite{sidorov2020textcaps}                        & 21942   & 0.08     & Captioning   & English         \\
LRV-Caption~\cite{liu2023aligning}                     & 254897  & 0.98     & Captioning   & English/Chinese \\
ShareGPT4V~\cite{chen2023sharegpt4v}            & 101178  & 0.39     & Captioning   & English         \\
Stanford40 Action~\cite{yao2011human}               & 9397    & 0.04     & Captioning   & English         \\
PixMo-CapQA~\cite{deitke2024molmo}                     & 229781  & 0.88     & Captioning   & English         \\
ChartQA~\cite{masry-etal-2022-chartqa}                         & 7548    & 0.03     & Chart        & English         \\
ChartBench~\cite{ChartBench}                      & 599616  & 2.29     & Chart        & English         \\
MMC-Inst~\cite{liu2023mmc}                        & 371027  & 1.42     & Chart        & English         \\
PlotQA~\cite{Methani_2020_WACV}                          & 157045  & 0.60      & Chart        & English         \\
FigureQA~\cite{kahou2017figureqa}                        & 100000  & 0.38     & Chart        & English         \\
DVQA~\cite{kafle2018dvqa}                            & 199364  & 0.76     & Chart        & English         \\
CRPE~\cite{wang2023allseeing}                            & 12896    & 0.05     & Chart        & English         \\
ChartGemma~\cite{masry2024chartgemma}                      & 163240  & 0.62     & Chart        & English         \\
TabMWP~\cite{lu2023dynamic}                          & 22662   & 0.09     & Chart        & English         \\
Chart2Text~\cite{kantharaj2022chart}                      & 27794   & 0.11     & Chart        & English         \\
LRV-Instruction-Chart~\cite{liu2023aligning}           & 21103   & 0.08     & Chart        & English         \\
ALLaVA~\cite{chen2024allava}                          & 977493  & 3.73     & Conversation & English/Chinese \\
ComVint~\cite{du2023makes}                         & 23586   & 0.11     & Conversation & English/Chinese \\
TallyQA~\cite{acharya2019tallyqa}                         & 249274  & 0.95     & Counting     & English         \\
Locount~\cite{Cai2020Locount}                         & 12660   & 0.05     & Counting     & English         \\
DocVQA~\cite{mathew2020docvqa}                          & 46191   & 0.17     & Document     & English         \\
DeepForm-VQA~\cite{aggarwal2023dublin}                    & 5367    & 0.02     & Document     & English         \\
Open-WikiTable~\cite{kweon2023open}                      & 2033    & 0.01     & Document     & English         \\
TabFact~\cite{2019TabFactA}                         & 12890   & 0.05     & Document     & English         \\
OCR-IDL~\cite{biten2022ocr}                         & 84183   & 0.32     & Document     & English         \\
kleister-charity~\cite{stanislawek2021kleister}                & 5175    & 0.02     & Document     & English         \\
DocReason25K~\cite{hu2024docowl}                    & 22568   & 0.09     & Document     & English         \\
HierText~\cite{long2022towards}                        & 8250    & 0.03     & Document     & English         \\
VisualMRC~\cite{VisualMRC2021}                       & 4794    & 0.02     & Document     & English         \\
DocStruct4M~\cite{hu2024docowl}                     & 299214  & 1.14     & Document     & English         \\
PubTabNet~\cite{zhong2019image}                       & 387665  & 1.48     & Document     & English         \\
arxiv\_qa~\cite{li2024multimodalarxiv}                       & 99992   & 0.38     & Document     & English         \\
PixMo-Docs~\cite{deitke2024molmo}                      & 255261  & 0.98     & Document     & English         \\
TAT-QA~\cite{zhu-etal-2021-tat}                          & 6630    & 0.02     & Document     & English         \\
Table-VQA~\cite{kim2024tablevqabench}                       & 84317   & 0.32     & Document     & English         \\
PixMo-AskModelAnything~\cite{deitke2024molmo}          & 157540  & 0.60      & General QA   & English         \\
COCO-VQA~\cite{ren2015exploring}                        & 28054   & 0.11     & General QA   & English         \\
VG-VQA~\cite{reich2024uncoveringpotentialvisualgrounding}                          & 14136   & 0.03     & General QA   & English/Chinese \\
SVIT~\cite{zhao2023svit}                            & 19818   & 0.08     & General QA   & English         \\
LVIS-Instruct4v~\cite{wang2023instruct4v}                 & 111169  & 0.42     & General QA   & Chinese         \\
%Volcano                         & 270154  & 1.03     & General QA   & English         \\
RLAIF-V~\cite{yu2024rlaifv}                          & 22911   & 0.09     & General QA   & English         \\
RAVEN~\cite{zhang2019raven}                           & 41995   & 0.16     &  General QA    & English         \\
OK-VQA~\cite{marino2019ok}                          & 26994    & 0.10     & General QA   & English         \\
VQAv2~\cite{goyal2017making}                           & 249274  & 0.95     & General QA   & English         \\
GQA~\cite{hudson2019gqa}                             & 72148   & 0.28     & General QA   & English         \\
Q-Instruct~\cite{wu2023qinstruct}                      & 200534  & 0.77     & General QA   & English         \\
VSR~\cite{Liu2022VisualSR}                             & 9516    & 0.04     & General QA   & English         \\
VisCoT~\cite{shao2024visual}                          & 97490   & 0.37     & General QA   & English         \\
CogVLM-SFT-311K~\cite{wang2023cogvlm}                 & 296629  & 0.29     & General QA   & English/Chinese \\
ShareGPT4o~\cite{internvl_2024}                      & 96825   & 0.37     & General QA   & English         \\
image-textualization~\cite{pi2024image}            & 99573   & 0.38     & General QA   & English         \\
MapQA~\cite{chang2022mapqa}                           & 242501  & 0.93     & General QA   & English         \\
VizWiz~\cite{2018VizWiz}                          & 20523   & 0.08     & General QA   & English         \\
PathVQA~\cite{he2020pathvqa}                         & 32632   & 0.12     & General QA   & English         \\
%SketchyVQA                      & 8000    & 0.03     & General QA   & English         \\
ALFWorld~\cite{ALFWorld20}                        & 44968   & 0.17     & General QA   & English         \\
\bottomrule
\end{tabular}
\end{table}


\begin{table}[h]
\centering
\caption{\textbf{Details of open source image-text data in IT-Stage-1-Data (Part-2).}
}
\label{tab:appendix_sft_s1_it_2}
\setlength{\tabcolsep}{7pt}
\begin{tabular}{l|l|l|l|l}
\toprule
Dataset                         & Size    & Ratio (\%) & Data Class    & Language        \\
\midrule
lnqa~\cite{2020Connecting}                            & 207821  & 0.79     & General QA   & English         \\
OODVQA~\cite{0Benchmarking}                          & 16532    & 0.06     & General QA   & English         \\
Cambrian-10M~\cite{tong2024cambrian1}                    & 1863834 & 7.12     & General QA   & English         \\
% cninfo                          & 100000  & 0.38     & General QA   & Chinese         \\
Align-Anything-TI2T-Instruction~\cite{ji2024align} & 39216   & 0.15     & General QA   & English         \\
LRV-Instruction~\cite{liu2023aligning}                 & 180722  & 0.69     & General QA   & English         \\
VLGuard~\cite{zong2023safety}                         & 10000    & 0.04     & General QA   & English         \\
HalluciDoctor~\cite{yu2023hallucidoctor}                   & 104518   & 0.39     & General QA   & English         \\
M-HalDetect~\cite{gunjal2024detecting}                     & 28268   & 0.10     & General QA   & English         \\
illusionVQA~\cite{shahgir2024illusionvqa}                     & 4362    & 0.02     & General QA   & English         \\
HaloQuest~\cite{wang2024haloquest}                       & 7686    & 0.03     & General QA   & English         \\
Infinity-MM~\cite{gu2024infinitymm}                     & 78284   & 2.92     & General QA   & English         \\
RefCOCO~\cite{kazemzadeh2014referitgame}                         & 200537  & 0.54     & Grounding    & English         \\
Objects365~\cite{2020Objects365}                      & 1038990 & 3.96     & Grounding    & English/Chinese \\
GPT4Gen-RD-BoxCoT~\cite{chen2023shikra}               & 11268    & 0.04     & Grounding    & English         \\
AOKVQA~\cite{schwenk2022okvqa}                          & 34112   & 0.13     & Knowledge    & English         \\
KVQA~\cite{shah2019kvqa}                            & 20806   & 0.08     & Knowledge    & English         \\
CLEVR~\cite{johnson2017clevr}                           & 70000   & 0.27     & Knowledge    & English         \\
Super-CLEVR~\cite{li2023super}                      & 56913   & 0.22     & Knowledge    & English         \\
ViQuAE~\cite{lerner2022viquae}                          & 3153    & 0.01        & Knowledge    & English         \\
Co-Instruct~\cite{wu2024openended}                     & 9699    & 0.04     & Knowledge    & English         \\
CLEVR-Math~\cite{lindstrom2022clevr}                      & 675284  & 2.58     & Mathematics  & English         \\
% MathOCR                         & 22880   & 0.09     & Mathematics  & English         \\
MathV360K~\cite{shi2024math}                       & 328339  & 1.25     & Mathematics  & English         \\
GeoGPT4V~\cite{cai2024geogpt4v}                        & 18174   & 0.07     & Mathematics  & English         \\
GeoQA~\cite{chen2022geoqa}                          & 72318   & 0.28     & Mathematics  & English         \\
TextVQA~\cite{singh2019towards}                         & 21953   & 0.08     & OCR          & English         \\
OCR-VQA~\cite{mishra2019ocr}                          & 207572  & 0.79     & OCR          & English         \\
LLaVAR~\cite{zhang2023llavar}                          & 19800   & 0.08     & OCR          & English         \\
ICDAR19\_LSVT~\cite{sun2019icdar}                   & 30000   & 0.11     & OCR          & Chinese         \\
ICDAR19\_ReCTS~\cite{zhang2019icdar}                  & 19825   & 0.08     & OCR          & Chinese         \\
ICDAR19\_ArT~\cite{chng2019icdar2019}                    & 9344    & 0.03     & OCR          & Chinese         \\
Docmatix~\cite{2024docmatrix}                        & 101806  & 0.39     & OCR          & English         \\
DT-VQA~\cite{zhang2024exploring}                          & 20780   & 0.08     & OCR          & English         \\
Clock~\cite{yang2022s}                           & 3678    & 0.01        & OCR          & English         \\
COCO-Text V2.0~\cite{veit2016cocotext}                  & 13049   & 0.05     & OCR          & Chinese         \\
infographicsVQA~\cite{mathew2021infographicvqa}                 & 12078    & 0.05     & OCR          & English         \\
WebSRC~\cite{chen-etal-2021-websrc}                          & 9098    & 0.03     & OCR          & English         \\
CTW~\cite{yuan2019ctw}                        & 25871   & 0.10      & OCR          & Chinese         \\
RCTW-17~\cite{shi2017icdar2017}                            & 9262    & 0.03     & OCR          & Chinese         \\
ST-VQA~\cite{biten2019scene}                          & 17731   & 0.07     & OCR          & English         \\
TextOCR~\cite{singh2021textocr}                         & 21750   & 0.08     & OCR          & Chinese         \\
SynthText~\cite{Gupta16}                  & 857773  & 3.28     & OCR          & Chinese         \\
DTNutrCap~\cite{zhang2024exploring}                       & 4498    & 0.02     & OCR          & English         \\
DTScene~\cite{zhang2024exploring}                         & 15534    & 0.06     & OCR          & English         \\
ScreenQA~\cite{baechler2024screenai}                        & 161522   & 0.60     & OCR          & English         \\
Uber-Text~\cite{UberText}                       & 13941   & 0.05     & OCR          & Chinese         \\
MathWriting~\cite{gervais2024mathwriting}                     & 62726   & 0.24     & OCR          & English         \\
% LaTeX\_OCR                      & 1200    & 0        & OCR          & English         \\
HME~\cite{yuan2022syntax}                             & 74502   & 0.28     & OCR          & English         \\
% ICDAR\_CHROME                   & 3831    & 0.01     & OCR          & English         \\
ScienceQA~\cite{lu2022learn}                       & 10332   & 0.04     & Science      & English         \\
AI2D~\cite{kembhavi2016diagram}                            & 9930    & 0.04     & Science      & English         \\
TQA~\cite{kembhavi2017you}                             & 19161    & 0.07     & Science      & English         \\
IconQA~\cite{lu2021iconqa}                          & 21393    & 0.08     & Science      & English         \\
PMC-VQA~\cite{zhang2023pmcvqa}                         & 152603  & 0.58     & Science      & English         \\
VQA-RAD~\cite{lau2018dataset}                         & 5379    & 0.02     & Science   & English         \\
\bottomrule
\end{tabular}
\end{table}


\begin{table}[h]
\centering
\caption{\textbf{Details of open source text data in IT-Stage-1-Data.}
}
\label{tab:appendix_sft_s1_nlp}
\setlength{\tabcolsep}{7pt}
\begin{tabular}{l|l|l|l|l}
\toprule
Dataset                            & Size    & Ratio (\%) & Language & Data Class   \\
\midrule
Evol-Instruct-Code~\cite{luo2023wizardcoder}                 & 66862   & 0.52     & English  & Code         \\
% gpteacher-instruct                 & 18194   & 0.14     & English  & Code         \\
UltraInteract\_sft~\cite{yuan2024advancing}                 & 288579  & 2.27     & English  & Code         \\
glaive-code-assistant-v1~\cite{glaive-code-assistant}           & 136109  & 1.07     & English  & Code         \\
glaive-code-assistant-v2~\cite{glaive-code-assistant}           & 215166  & 1.69     & English  & Code         \\
glaive-code-assistant-v3~\cite{glaive-code-assistant}           & 475192  & 3.78     & English  & Code         \\
Magicoder~\cite{wei2024magicoder}                          & 108063  & 0.85     & English  & Code         \\
ALLaVA~\cite{chen2024allava}                             & 143000  & 1.12     & English  & Conversation \\
alpaca-gpt4~\cite{peng2023instruction}                        & 103998   & 0.83     & English  & Conversation \\
alpaca-gpt4\_zh~\cite{peng2023instruction}                    & 47347   & 0.37     & Chinese  & Conversation \\
UltraFeedback~\cite{cui2023ultrafeedback}                      & 121832   & 0.97     & English  & Conversation \\
Cambrian-10M~\cite{tong2024cambrian1}                       & 1713307 & 13.45    & English  & General QA   \\
SlimOrca~\cite{SlimOrca}                           & 517982  & 4.07     & English  & General QA   \\
% ShareGPT\_Vicuna\_unfiltered       & 94145   & 0.74     & English  & General QA   \\
% sharegpt\_gpt4                     & 58674   & 0.46     & English  & General QA   \\
% share\_gpt4\_zh                    & 38535   & 0.3      & Chinese  & General QA   \\
Align-Anything-Instruction-100K~\cite{ji2024align}    & 105333  & 0.83     & English  & General QA   \\
Align-Anything-Instruction-100K-zh~\cite{ji2024align} & 104550  & 0.82     & Chinese  & General QA   \\
% finance-alpaca                     & 137822   & 1.1     & English  & Knowledge    \\
WizardLM\_evol\_instruct-v2~\cite{xu2024wizardlm}        & 142999  & 1.12     & English  & knowledge    \\
databricks\_dolly\_15k~\cite{DatabricksBlog2023DollyV2}             & 15011   & 0.12     & English  & knowledge    \\
Magpie-Qwen2~\cite{xu2024magpie}                       & 1000000 & 7.85     & English  & knowledge    \\
Infinity-Instruct~\cite{InfinityInstruct2024}                  & 2116735 & 16.62    & English  & knowledge    \\
MathInstruct~\cite{yue2023mammoth}                       & 261629  & 2.05     & English  & Mathematics  \\
NuminaMath-Cot~\cite{numina_math_datasets}                     & 859296  & 6.74     & English  & Mathematics  \\
DART-Math-Hard~\cite{tong2024dartmath}                     & 551000  & 4.32     & English  & Mathematics  \\
orca-math-word-problems-200k~\cite{mitra2024orcamath}       & 200035  & 1.57     & English  & Mathematics  \\
MetaMathQA~\cite{yu2023metamath}                         & 395000  & 3.10      & English  & Mathematics  \\
OpenMathInstruct-v2~\cite{toshniwal2024openmath2}                & 1000000 & 7.85     & English  & Mathematics  \\
MathGLM~\cite{yang2023gpt}                            & 200488  & 1.57     & Chinese  & Mathematics  \\
arxiv-physics-instruct~\cite{phi-arxiv-physics-instruct}             & 276286  & 2.17     & English  & Science      \\
\bottomrule
\end{tabular}
\end{table}


\begin{table}[ht]
\centering
\caption{\textbf{Details of open source image-text data in IT-Stage-2-Data (Part-1).}
}
\label{tab:appendix_sft_s2_it_1}
\setlength{\tabcolsep}{7pt}
\begin{tabular}{l|l|l|l|l}
\toprule
Dataset                         & Size    & Ratio (\%) & Data Class    & Language        \\
\midrule
LRV-Caption~\cite{liu2023aligning}                 & 106239 & 0.62       & Captioning   & English         \\
ShareGPT4V~\cite{chen2023sharegpt4v}               & 64917  & 0.38       & Captioning   & Chinese         \\
CogVLM-SFT-311K-caption~\cite{wang2023cogvlm}      & 67389  & 0.39       & Captioning   & Chinese         \\
% sharegpt4v\_instruct\_gpt4-vision\_cap100k                          & 101178 & 0.59       & Captioning   & English         \\
textocr\_gpt4o\_train~\cite{TextOCR-GPT4o}         & 100324 & 0.58       & Captioning   & English         \\
PixMo-CapQA~\cite{deitke2024molmo}                 & 229781 & 1.33       & Captioning   & English         \\
ChartQA~\cite{masry-etal-2022-chartqa}             & 20128  & 0.12       & Chart        & English         \\
CRPE~\cite{wang2023allseeing}                      & 51584  & 0.3        & Chart        & English         \\
ChartBench~\cite{ChartBench}                       & 599616 & 3.48       & Chart        & English         \\
FigureQA~\cite{kahou2017figureqa}                  & 100000 & 0.58       & Chart        & English         \\
PlotQA~\cite{Methani_2020_WACV}                  & 157045 & 0.91       & Chart        & English         \\
MMC-Inst~\cite{liu2023mmc}                         & 371027 & 2.15       & Chart        & English         \\
DVQA~\cite{kafle2018dvqa}                          & 199364 & 1.16       & Chart        & English         \\
ChartGemma~\cite{masry2024chartgemma}              & 163240 & 0.95       & Chart        & English         \\
TabMWP~\cite{lu2023dynamic}                        & 226620 & 1.31       & Chart        & English         \\
Chart2Text~\cite{kantharaj2022chart}               & 277940 & 1.61       & Chart        & English         \\
LRV-Instruction-Chart~\cite{liu2023aligning}       & 21103  & 0.12       & Chart        & English         \\
ALLaVA~\cite{chen2024allava}                       & 165387 & 0.96       & Conversation & English/Chinese \\
ComVint~\cite{du2023makes}                         & 23586  & 0.14       & Conversation & English/Chinese \\
TallyQA~\cite{acharya2019tallyqa}                  & 49854  & 0.29       & Counting     & English         \\
Locount~\cite{Cai2020Locount}                      & 12660  & 0.07       & Counting     & English         \\
DocVQA~\cite{mathew2020docvqa}                     & 123176 & 0.72       & Document     & English         \\
DeepForm-VQA~\cite{aggarwal2023dublin}             & 7156   & 0.04       & Document     & English         \\
TabFact~\cite{2019TabFactA}                        & 12890  & 0.07       & Document     & English         \\
HierText~\cite{long2022towards}                    & 8250   & 0.05       & Document     & English         \\
TAT-QA~\cite{zhu-etal-2021-tat}                    & 4420   & 0.03       & Document     & English         \\
VisualMRC~\cite{VisualMRC2021}                     & 4794   & 0.03       & Document     & English         \\
OCR-IDL~\cite{biten2022ocr}                        & 168366 & 0.98       & Document     & English         \\
DocReason25K~\cite{hu2024docowl}                   & 22568  & 0.13       & Document     & English         \\
DocStruct4M~\cite{hu2024docowl}                    & 89764  & 0.52       & Document     & English         \\
arxiv\_qa~\cite{li2024multimodalarxiv}        & 99992  & 0.58       & Document     & English         \\
PixMo-Docs~\cite{deitke2024molmo}                  & 255261 & 1.48       & Document     & English         \\
SVIT~\cite{zhao2023svit}                           & 39636  & 0.23       & General QA   & English         \\
GQA~\cite{hudson2019gqa}                           & 72148  & 0.42       & General QA   & English         \\
MapQA~\cite{chang2022mapqa}                        & 242501 & 1.41       & General QA   & English         \\
ShareGPT4o~\cite{internvl_2024}                   & 193650 & 1.12       & General QA   & English         \\
CogVLM-SFT-311K-caption~\cite{wang2023cogvlm}      & 935553 & 3.69       & General QA   & English/Chinese \\
image-textualization~\cite{pi2024image}            & 298719 & 1.73       & General QA   & English         \\
lnqa~\cite{2020Connecting}                         & 207821 & 1.21       & General QA   & English         \\
OODVQA~\cite{0Benchmarking}                        & 8266   & 0.05       & General QA   & English         \\
% SketchyVQA                                                          & 8000   & 0.05       & General QA   & English         \\
VizWiz~\cite{2018VizWiz}                           & 20523  & 0.12       & General QA   & English         \\
VQAv2~\cite{goyal2017making}                       & 249274 & 1.45       & General QA   & English         \\
% Volcano                                                             & 270154 & 1.57       & General QA   & English         \\
RAVEN~\cite{zhang2019raven}                        & 83990  & 0.49       & General QA   & English         \\
RLAIF-V~\cite{yu2024rlaifv}                        & 45822  & 0.27       & General QA   & English         \\
VSR~\cite{Liu2022VisualSR}                         & 4758   & 0.03       & General QA   & English         \\
% cninfo\_100k                                                        & 100000 & 0.58       & General QA   & Chinese         \\
Cambrian-10M~\cite{tong2024cambrian1}              & 28054  & 0.16       & General QA   & English         \\
Align-Anything-TI2T-Instruction~\cite{ji2024align} & 78432  & 0.46       & General QA   & English         \\
LRV-Instruction~\cite{liu2023aligning}             & 361444 & 2.1        & General QA   & English/Chinese \\
VLGuard~\cite{zong2023safety}                      & 10000  & 0.06       & General QA   & English         \\
HalluciDoctor~\cite{yu2023hallucidoctor}           & 104528 & 0.61       & General QA   & English         \\
M-HalDetect~\cite{gunjal2024detecting}             & 28268  & 0.17       & General QA   & English         \\
illusionVQA~\cite{shahgir2024illusionvqa}          & 7270   & 0.04       & General QA   & English         \\
HaloQuest~\cite{wang2024haloquest}                 & 23058  & 0.13       & General QA   & English         \\
Infinity-MM~\cite{gu2024infinitymm}                & 782884 & 4.55       & General QA   & English         \\
PixMo-AskModelAnything~\cite{deitke2024molmo}      & 157540 & 0.91       & General QA   & English         \\
VisCoT~\cite{shao2024visual}                       & 175022 & 1.02       & General QA   & English         \\
DocStruct-Grounding~\cite{hu2024docowl}            & 99983  & 0.58       & Grounding    & English         \\
AOKVQA~\cite{schwenk2022okvqa}                     & 17056  & 0.1        & Knowledge    & English         \\
\bottomrule
\end{tabular}
\end{table}


\begin{table}[h]
\centering
\caption{\textbf{Details of open source image-text data in IT-Stage-2-Data (Part-2).}
}
\label{tab:appendix_sft_s2_it_2}
\setlength{\tabcolsep}{7pt}
\begin{tabular}{l|l|l|l|l}
\toprule
Dataset                         & Size    & Ratio (\%) & Data Class    & Language        \\
\midrule
ViQuAE~\cite{lerner2022viquae}                     & 2102   & 0.01       & Knowledge    & English         \\
KVQA~\cite{shah2019kvqa}                           & 41612  & 0.24       & Knowledge    & English         \\
Co-Instruct~\cite{wu2024openended}                 & 12932  & 0.08       & Knowledge    & English         \\
CLEVR~\cite{johnson2017clevr}                      & 70000  & 0.41       & Knowledge    & English         \\
Super-CLEVR~\cite{li2023super}                     & 56913  & 0.33       & Knowledge    & English         \\
GeoGPT4V~\cite{cai2024geogpt4v}                    & 36348  & 0.21       & Mathematics  & English         \\
MathV360K~\cite{shi2024math}                       & 328339 & 1.91       & Mathematics  & English         \\
CLEVR-Math~\cite{lindstrom2022clevr}               & 675284 & 3.92       & Mathematics  & English         \\
% MathOCR                                                             & 22880  & 0.13       & Mathematics  & English         \\
Geo170k~\cite{gao2023g}                            & 176602 & 1.02       & Mathematics  & English         \\
GeoQA~\cite{chen2022geoqa}                         & 144636 & 0.84       & Mathematics  & English         \\
TextVQA~\cite{singh2019towards}                    & 21953  & 0.13       & OCR          & English         \\
OCR-VQA~\cite{mishra2019ocr}                       & 207572 & 1.2        & OCR          & English         \\
ICDAR19\_LSVT~\cite{sun2019icdar}             & 30000  & 0.17       & OCR          & Chinese         \\
ICDAR19\_ReCTS~\cite{zhang2019icdar}          & 19825  & 0.12       & OCR          & Chinese         \\
ICDAR19\_ArT~\cite{chng2019icdar2019}         & 4672   & 0.03       & OCR          & Chinese         \\
Clock~\cite{yang2022s}                             & 4904   & 0.03       & OCR          & English         \\
DT-VQA~\cite{zhang2024exploring}                   & 83120  & 0.48       & OCR          & English         \\
Docmatix~\cite{2024docmatrix}              & 306376 & 1.78       & OCR          & English         \\
DTNutrCap~\cite{zhang2024exploring}                & 4498   & 0.03       & OCR          & English         \\
DTScene~\cite{zhang2024exploring}                  & 15534  & 0.09       & OCR          & English         \\
infographicsVQA~\cite{mathew2021infographicvqa}    & 16104  & 0.09       & OCR          & English         \\
WebSRC~\cite{chen-etal-2021-websrc}                & 18196  & 0.11       & OCR          & English         \\
TextOCR~\cite{singh2021textocr}                    & 87000  & 0.5        & OCR          & Chinese         \\
ST-VQA~\cite{biten2019scene}                       & 35462  & 0.21       & OCR          & English         \\
CTW~\cite{yuan2019ctw}                             & 51742  & 0.3        & OCR          & Chinese         \\
RCTW-17~\cite{shi2017icdar2017}                    & 9262   & 0.05       & OCR          & Chinese         \\
COCO-Text   V2.0~\cite{veit2016cocotext}           & 13049  & 0.08       & OCR          & Chinese         \\
SynthText~\cite{Gupta16}                           & 85777  & 0.5        & OCR          & Chinese         \\
ScreenQA~\cite{baechler2024screenai}               & 80761  & 0.47       & OCR          & English         \\
Uber-Text~\cite{UberText}                          & 27882  & 0.16       & OCR          & Chinese         \\
HME~\cite{yuan2022syntax}                          & 148984 & 0.86       & OCR          & English         \\
MathWriting~\cite{gervais2024mathwriting}          & 31363  & 0.18       & OCR          & English         \\
% LaTeX\_OCR                                                          & 1200   & 0.01       & OCR          & English         \\
% ICDAR\_CHROME\_2023                                                 & 3831   & 0.02       & OCR          & English         \\
IconQA~\cite{lu2021iconqa}                         & 7131   & 0.04       & Science      & English         \\
TQA~\cite{kembhavi2017you}                         & 76644  & 0.44       & Science      & English         \\
AI2D~\cite{kembhavi2016diagram}                    & 39720  & 0.23       & Science      & English         \\
ScienceQA~\cite{lu2022learn}                       & 123984 & 0.72       & Science      & English         \\
VQA-RAD~\cite{lau2018dataset}                      & 10758  & 0.06       & Science      & English         \\
PMC-VQA~\cite{zhang2023pmcvqa}                     & 152603 & 0.89       & Science      & English          \\
\bottomrule
\end{tabular}
\end{table}


\begin{table}[h]
\centering
\caption{\textbf{Details of Interleaved image-text data in IT-Stage-2-Data.}}
\label{tab:multi_image_stage2}
\setlength{\tabcolsep}{7pt}
\begin{tabular}{l|l|l|l|l}
\hline
Dataset & Size & Ratio (\%) & Class & Language \\
\hline
InternVL-SA-1B-Caption-EN~\cite{chen2023internvl} & 77704 & 2.0 & Caption & English \\
InternVL-SA-1B-Caption-CN~\cite{chen2023internvl} & 77704 & 1.0 & Caption & Chinese \\
MMDU-EN~\cite{wang2024mmduet} & 74998 & 4.0 & General QA & English \\
MMDU-CN~\cite{wang2024mmduet} & 33882 & 2.0 & General QA & Chinese \\
M4-Instruct~\cite{li2024llavanext} & 615814 & 1.0 & General QA & English \\
\textcolor{gray}{Synthetic GEO-mv} & 241060 & 1.0 & General QA & English \\
\textcolor{gray}{Synthetic ScienceQA-mv} & 45916 & 2.0 & General QA & English \\
MP-DocVQA (en)~\cite{tito2023hier} & - &- & Document & English \\
MP-Docmatix~\cite{2024docmatrix} & - &- & Document & English \\
\hline
\end{tabular}
\end{table}

\begin{table}[h]
\centering
\caption{\textbf{Details of Video-text data in IT-Stage-2-Data.}}
\label{tab:video_text_stage2}
\setlength{\tabcolsep}{7pt}
\begin{tabular}{l|l|l|l|l}
\hline
Dataset & Size & Ratio (\%) & Class & Language \\
\hline
Vript~\cite{yang2024vript} & - & - & Caption & English \\
OpenVid~\cite{nan2024openvid} & - & - & Caption & English \\
Mementos~\cite{wang2024mementos} & - & - & Caption & English \\
ShareGPT4o-Video~\cite{chen2024far} & 2111 & 4.0 & Caption & English \& Chinese \\
ShareGPT4Video~\cite{chen2024sharegpt4video} & 255000 & 1.0 & Caption & English \\
VideoGPT+~\cite{Maaz2024VideoGPT+} & - & - & Caption & English \\
TextVR~\cite{wu2023largecrossmodal} & 39648 & 1.0 & Caption & English \\
\textcolor{gray}{Synthetic VideoDetailCaption} & 56748 & 4.0 & Caption & English \\
EgoTaskQA~\cite{jia2022egotaskqa} & 61710 & 1.0 & General QA & English \\
CLEVRER~\cite{yi2019clevrer} & 82620 & 1.0 & General QA & English \\
NExT-QA~\cite{xiao2021next} & 34132 & 1.0 & General QA & English \\
LLaVA-Video~\cite{zhang2024video} & 1367388 & 1.0 & General QA & English \\
FineVideo~\cite{FineVideo} & 87502 & 1.0 & General QA & English \\
VideoBlink (en)~\cite{videoblink} & 2590 & 4.0 & General QA & English \\
TGIF-QA~\cite{yuntgifqa} & 52696 & 1.0 & General QA & English \\
\hline
\end{tabular}
\end{table}


\begin{table}[h]
\centering
\caption{\textbf{Details of video-text data in IT-Stage-3-Data.}}
\label{tab:video_text_stage3}
\setlength{\tabcolsep}{7pt}
\begin{tabular}{l|l|l|l|l}
\hline
Dataset & Size & Ratio (\%) & Class & Language \\
\hline
TextVR~\cite{wu2023largecrossmodal} & 39648 & 1.0 & Caption & English \\
ShareGPT4o-Video~\cite{chen2024far} & 2111 & 4.0 & Caption & English \& Chinese \\
\textcolor{gray}{Synthetic VideoAction} & 7182 & 4.0 & Caption & English \\
\textcolor{gray}{Synthetic VideoDetailCaption} & 56748 & 4.0 & Caption & English \\
EgoTaskQA~\cite{jia2022egotaskqa} & 61710 & 2.0 & General QA & English \\
CLEVRER~\cite{yi2019clevrer} & 82620 & 1.0 & General QA & English \\
NExT-QA~\cite{xiao2021next} & 34132 & 1.0 & General QA & English \\
LLaVA-Video~\cite{zhang2024video} & 1367388 & 0.5 & General QA & English \\
FineVideo~\cite{FineVideo} & 87502 & 1.0 & General QA & English \\
VideoBlink (en)~\cite{videoblink} & 2590 & 2.0 & General QA & English \\
MoVid~\cite{chen2024motionllm} & 24699 & 2.0 & General QA & English \\
FunQA~\cite{xie2024funqa} & 21012 & 1.0 & General QA & English \\
TGIF-QA~\cite{yuntgifqa} & 52696 & 1.0 & General QA & English \\
AlignAnythingVideo~\cite{ji2024alignanything} & 9501 & 6 & General QA & English \\
STAR~\cite{wu2024starbench} & 45731 & 2.0 & General QA & English \\
MMWorld~\cite{he2024mmworld} & 1976 & 4.0 & General QA & English \\
IntentQA~\cite{Li0HF23} & 40320 & 2.0 & General QA & English \\
\hline
\end{tabular}
\end{table}


\begin{table}[h]
\centering
\caption{\textbf{Details of open source audio-text data in IT-Stage-3-Data.}
}
\label{tab:appendix_audio_text}
\setlength{\tabcolsep}{7pt}
\begin{tabular}{l|l|l|l|l}
\toprule
Dataset                         & Size    & Ratio (\%) & Data Class    & Language        \\
\midrule
Libriheavy~\cite{Libriheavy} & 10454844 & 24.77 &ASR &English \\
GigaSpeech~\cite{GigaSpeech} & 8282988 & 19.62 &ASR &English \\
Librispeech~\cite{Librispeech} & 286652 & 0.68 &ASR &English \\
CoVoST2\_en\_en~\cite{CoVoST2} & 304937 & 0.72 &ASR &English \\
SPGISpeech~\cite{SPGISpeech} & 1966109 & 4.66 &ASR &English \\
TEDLIUM\_v2~\cite{TED_LIUM} & 93437 & 0.22 &ASR &English \\
SlideSpeech~\cite{SlideSpeech} & 483731 & 1.15 &ASR &English \\
CoVoST2\_cn\_cn~\cite{CoVoST2} & 11928 & 0.03 &ASR &Chinese \\
AISHELL1~\cite{AISHELL1} & 134424 & 0.32 &ASR &Chinese \\
WenetSpeech~\cite{WenetSpeech} & 14625245 & 34.65 &ASR &Chinese \\
KeSpeech~\cite{KeSpeech} & 973583 & 2.31 &ASR &Chinese \\
AliMeeting~\cite{AliMeeting} & 372886 & 0.88 &ASR &Chinese \\
MagicData\_755h~\cite{MagicData_RAMC} & 584976 & 1.39 &ASR &Chinese \\
MagicData\_RAMC~\cite{MagicData_RAMC} & 164972 & 0.39 &ASR &Chinese \\
Primewords\_100h~\cite{Primewords_100h} & 50383 & 0.12 &ASR &Chinese \\
FreeST~\cite{FreeST} & 102600 & 0.24 &ASR &Chinese \\
Tal\_zh100h~\cite{TAL} & 93963 & 0.22 &ASR &Chinese \\
Tal\_zhen587h~\cite{TAL} & 354680 & 0.84 &ASR &Chinese \\
Aidatatang\_200zh~\cite{aidatatang_200zh} & 189121 & 0.45 &ASR &Chinese \\
CoVoST2\_en\_cn~\cite{CoVoST2} & 304937 & 0.72 &AST &Chinese \\
CoVoST2\_cn\_en~\cite{CoVoST2} & 11928 & 0.03 &AST &English \\
WavCaps~\cite{WavCaps} & 275282 & 0.65 &AAC &English \\
Clotho~\cite{Clotho} & 14465   & 0.03     &AAC    & English         \\
AudioCaps~\cite{AudioCaps} & 50939 & 0.12 &AAC &English \\
MACS~\cite{MACS} & 17275 & 0.04 &AAC &English \\
AudioSet~\cite{AudioSet} & 1909428 & 4.52 &AAT &English \\
VGGSound~\cite{VGGSound} & 95495 & 0.23 &AAT &English \\
\bottomrule
\end{tabular}
\end{table}


\begin{table}[h]
\centering
\caption{\textbf{Details of audio-QA data in IT-Stage-3-Data.}
}
\label{tab:appendix_sft_s3_audioqa}
\setlength{\tabcolsep}{8pt}
\begin{tabular}{l|l|l|l|l}
\toprule
Dataset   & Size    & Ratio (\%) & Data Class     & Language        \\
\midrule
COCO-VQA~\cite{ren2015exploring}  & 879598  & 11.81  & Image-Audio-QA & English/Chinese \\
VG-VQA~\cite{reich2024uncoveringpotentialvisualgrounding}    & 300000  & 4.03  & Image-Audio-QA & English/Chinese \\
SVIT~\cite{zhao2023svit}      & 40000   & 0.54  & Image-Audio-QA & English/Chinese \\
OK\_VQA~\cite{marino2019ok}   & 18018   & 0.24  & Image-Audio-QA & English/Chinese \\
VQAv2~\cite{goyal2017making}     & 887514  & 11.91  & Image-Audio-QA & English/Chinese \\
GQA~\cite{hudson2019gqa}       & 1886000 & 25.31  & Image-Audio-QA & English/Chinese \\
TextVQA~\cite{singh2019towards}   & 69204   & 0.93  & Image-Audio-QA & English/Chinese \\
TextCaps~\cite{sidorov2020textcaps}  & 219530  & 2.95   & Image-Audio-QA & English/Chinese \\
OCR-VQA~\cite{mishra2019ocr}   & 2004292 & 26.90  & Image-Audio-QA & English/Chinese \\
TextVR~\cite{wu2023largecrossmodal}    & 79296   & 1.06  & Video-Audio-QA & English/Chinese \\
VideoChat~\cite{li2023videochat} & 13778   & 0.18  & Video-Audio-QA & English/Chinese \\
WebVid~\cite{Bain21}    & 800000  & 10.74  & Video-Audio-QA & English/Chinese \\
NExT-QA~\cite{xiao2021next}   & 68264   & 0.92  & Video-Audio-QA & English/Chinese \\
CLEVRER~\cite{yi2019clevrer}   & 80000   & 1.07  & Video-Audio-QA & English/Chinese \\
TGIF-QA~\cite{yuntgifqa}   & 105392  & 1.41  & Video-Audio-QA & English/Chinese \\
\bottomrule
\end{tabular}
\end{table}
