\section{Conclusion}\label{sec:conclusion}

In this study, we propose M2-omni, a highly competitive omni-MLLM model to GPT-4o, characterized by its comprehensive modality and task support, as well as its exceptional performance. M2-omni demonstrates competitive performance across a diverse range of tasks, including image understanding, video understanding, interleaved image-text understanding, audio understanding and generation, as well as free-form image generation. We employ a multi-stage training approach to train M2-omni, which enables progressive modality alignment. To address the challenge of maintaining consistent performance across all modalities, we propose a step-wise balance strategy for pretraining and a dynamically adaptive balance strategy for instruction tuning, which can effectively mitigate the impact of significant variations in data volume and convergence rates across heterogeneous multimodal tasks. We publicly release M2-omni, along with its comprehensive training details, including data configurations and training procedures, to facilitate future research in this domain.
