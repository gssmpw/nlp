\section{MultiModal Multi-Task Balanced Strategy}\label{sec:approach}



\subsection{Step Balance Strategy }\label{subsubsec-Step Balancing Strategy}
During the multimodal joint training stage of pre-training, two primary challenges arise: balancing of \textit{data samples} and \textit{loss weights}. On the one hand, significant disparities in data quantity exist, which hinders the model's performance on modalities with limited data. On the other hand, since losses from different modalities are not on the same scale, the training direction will be biased towards the modality task with a larger loss, leading to suboptimal convergence. To address these challenges, we propose a Step Balance strategy that combines data sample balance and loss weight balance to tackle these challenges simultaneously.

% During pre-training, maintaining balanced training progress across different modalities becomes a crucial challenge.
% This challenge manifests in two primary aspects: \textit{data sample balance} and \textit{loss weight balance}.
% Firstly, in each iteration, data from more than one modality should be involved.
% Otherwise, the training direction will be skewed towards the modality present, leading to instability convergence.
% Secondly, since the convergence speeds vary from each modalities, different loss weights should be assigned to each modality.
% We designed two strategies for these two aspects.
% For data sample balance, following \cite{alayrac2022flamingo}, we compute a gradient for a batch from each dataset separately, and accumulate these gradients to update the parameters once.
% We found it outperforms the random sample and round-robin methods.
% For loss weight balance, we adopt a simple yet effective approach: training separately on small-scale data from each modality to obtain the loss when the model converges.
% The reciprocal of this loss is taken as the loss weight for that modality.
% During the subsequent training process, these loss weights remain fixed.
% The step balance training strategy involves two aspects: data sample balance and loss weight balance.
% The schematic diagram is shown in \cref{fig-gradient_accum}.

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=1.0\linewidth]{figures/gradient_accum.pdf}
%     \caption{
%     \textbf{Illustration of the step balancing strategy used during pre-training.}
%     We alternate batches of each dataset and compute gradients on each batch. $w_{i}$ refers to the loss weight allocated to the $i$-th modality. Gradients from each batch are weighted and summed, then are used to update the parameters.
%     }
%     \label{fig-gradient_accum}
% \end{figure}

% \begin{enumerate}

\textbf{Data Sample Balance}.
Let $\{\mathcal{D}_1, \mathcal{D}_2, ..., \mathcal{D}_M\}$ represent a collection of  $M$ different modalities of training data. Let $\mathcal{L}_i$ denote the loss function associated with the $i$-th modality.  In this pre-training stage, we explore different methods for updating the model, focusing on their effectiveness in balancing multimodal abilities. Notably, these methods are compared under the constraint that each mini-batch exclusively contains data from a single modality, ensuring a balanced and efficient training process. Three primary methods are explored:
% Let $\{\mathcal{D}_1, \mathcal{D}_2, ..., \mathcal{D}_M\}$ represent $M$ different modalities of training data. Each modality $i$ corresponds to a loss function $\mathcal{L}_i$. We investigate three primary methods for model update in multimodal training. Notably, to ensure load balance and training efficiency, each batch should exclusively contain data from a single modality rather than mixing multiple modalities within the same batch:

Random Sample: A mini-batch is randomly drawn from the entire dataset. The sampling probability of each modality is proportional to its data volume:
\begin{equation}\label{random_sample}
    \theta_{t+1} = \theta_t - \eta \nabla \mathcal{L}_i(\mathcal{B}_i),
\end{equation}
where $i$ is the index of the randomly selected modality, $\mathcal{B}_i$ represents a mini-batch from modality $i$,  $\theta_t$ and $\eta$ represents the model weights at time $t$ and the learning rate, respectively.


Round-robin: We alternate mini-batches of each dataset, updating the parameters on each mini-batch. This strategy ensures equal iteration steps across modalities and facilitates sufficient training for each modality.
\begin{equation}\label{round_robin}
    \theta_{t+1} = \theta_t - \eta \nabla \mathcal{L}{i_t}(\mathcal{B}{i_t}),
\end{equation}
where  $i_t = (t \bmod M) + 1$ denotes the modality selected at time step $t$.

Accumulation: We alternate batches of each data type and compute gradients on each batch. These gradients are then weighted and summed, then are used to update the parameters.
In other words, after forward-propagating one batch from each modality, we perform a single consolidated weight update incorporating information from all modalities.
\begin{equation}\label{accumulation}
    \theta_{t+1} = \theta_t - \eta \sum_{i=1}^M \nabla \mathcal{L}_i(\mathcal{B}_i).
\end{equation}
% Our ablation studies in \cref{{subsubsec:step_balance_ablaton}} show that the accumulation method consistently outperforms the other two methods, likely due to better gradient stability and sufficient training for each modality.

As demonstrated by our ablation studies in \cref{{subsubsec:step_balance_ablaton}}, the accumulation method consistently exhibits superior performance compared to the other two methods, which can be attributed to its improved gradient stability and adequate training for each modality.


\textbf{Loss Weight Balance}.
A simple yet effective method is employed to determine modality-specific loss weights. The proposed method involves the following steps:
1) Train the model on a small subset of data $\mathcal{D}_i^{sub} \subset \mathcal{D}_i$ until convergence;
2) Record the converged loss value $L_i^*$;
3) Calculate the normalization weight $w_i$ using Equation \ref{eq_loss_weight}:
\begin{equation}\label{eq_loss_weight}
    w_i = \alpha\frac{1/L_i^*}{\sum_{j=1}^M 1/L_j^*}.
\end{equation}

The normalization weights are subsequently applied to the parameter gradient update, as formulated in Equation \eqref{random_sample},\eqref{round_robin},\eqref{accumulation}. In practice, for data sample balance with accumulation strategy, we set $\alpha$ to 10 and update the model parameters as:
\begin{equation}
    \theta_{t+1} = \theta_t - \eta \sum_{i=1}^M w_i \nabla \mathcal{L}_i(\mathcal{B}_i)
\end{equation}

The step balance strategy is formally presented in Algorithm \ref{alg:step_balance}, which outlines the complete procedure.


% A simple yet effective method is proposed to determine modality-specific loss weights. For each modality $i$, we:
% 1) Train the model on a small subset of data $\mathcal{D}_i^{sub} \subset \mathcal{D}_i$ until convergence
% 2) Record the converged loss value $L_i^*$
% 3) Calculate the normalization weight $w_i$ as:
% \begin{equation}\label{eq_loss_weight}
%     w_i = \alpha\frac{1/L_i^*}{\sum_{j=1}^M 1/L_j^*}
% \end{equation}

% $\alpha$ is hyper-parameter which we set 10 in practice. Then, we applied the normalization weight to the parameter gradient update. For data sample balance with accumulation strategy, we update the model parameters as:
% \begin{equation}
%     \theta_{t+1} = \theta_t - \eta \sum_{i=1}^M w_i \nabla \mathcal{L}_i(\mathcal{B}_i)£¬
% \end{equation}

% The complete Step Balance strategy can be formulated as in Algorithm \ref{alg:step_balance}.


\begin{algorithm}[t]
\caption{Step Balance Training Strategy during Pre-training}
\label{alg:step_balance}
\begin{algorithmic}[1]
\State \textbf{Input:} Datasets $\{\mathcal{D}_1, ..., \mathcal{D}_M\}$
\State \textbf{Output:} Trained model parameters $\theta$
\State // small-scale datasets training to determine weights
\For{each modality $i$}
    \State Train on $\mathcal{D}_i^{sub}$ until convergence
    \State Record $L_i^*$
    \State Calculate $w_i$ using \cref{eq_loss_weight}
\EndFor
\State // Main training phase
\While{not converged}
    \For{each modality $i$}
        \State Sample batch $\mathcal{B}_i$ from $\mathcal{D}_i$
        \State Compute $\nabla_i = w_i\nabla\mathcal{L}_i(\mathcal{B}_i)$
    \EndFor
    \State Update $\theta$ using $\sum_{i=1}^M \nabla_i$
\EndWhile
\end{algorithmic}
\end{algorithm}



\subsection{\textbf{Dynamic Adaptive Balance Strategy}}\label{subsubsec-Dynamic Adaptive Balance}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/dynamic_balance.pdf}
    \caption{
    \textbf{Illustration of the dynamic adaptive balance strategy used during the Omni-Modality Instruction Tuning stage.}
    $\widetilde{w}_{i, t}$ refers to the loss weight allocated to the $i$-th modality at the $t$-th validation segment.
    }
    \label{fig-multi_modal_balance}
\end{figure}

% In the Omni-Modality Instruction Tuning stage, to ensure balanced training across different modalities, we treat each modality as a distinct training task, and leverage a multi-task learning (MTL)~\cite{crawshaw2020multi, liu2019loss} principle to balance the training progress of each modality (task): \textit{modalities exhibiting gentler convergence slopes (such as text, image, and video, which were fully trained in stage-2) received reduced weights to prevent overfitting, whereas those displaying sharper slopes (notably audio, newly introduced in stage-3) were assigned higher weights to facilitate enhanced learning.} Differs from MTL methods~\cite{liu2024mftcoder, gong2024coba} that alternate between complete training and validation cycles across entire datasets, we introduced periodic validation segments at fixed intervals throughout the training process. These segments compute validation losses for each modality using a compact, predetermined validation subset, which enables us to assess each modality¡¯s training progress through its validation loss and convergence slope within a historical window. Consequently we can dynamically adjust the modality weight allocations with minimal computational overhead, ensuring optimal training harmonization across modalities. Here, we provide a detailed introduction to our balanced training strategy. % LBTW; MFTCoder, CoBa;
In the Omni-Modality Instruction Tuning stage, we employ a dynamically adaptive balance strategy to regulate the convergence rates across modalities. Specifically, we treat each modality as a distinct training task, and leverage a multi-task learning (MTL)~\cite{crawshaw2020multi, liu2019loss} principle to balance the training progress of each modality (task): \textit{modalities exhibiting gentler convergence slopes received reduced weights to prevent overfitting, whereas those displaying sharper slopes were assigned higher weights to facilitate enhanced learning.}  In contrast to MTL methods~\cite{liu2024mftcoder, gong2024coba} that involve alternating training and validation cycles across entire datasets, our approach incorporates periodic validation segments at fixed intervals during training. These segments utilize a compact, predetermined validation subset to compute modality-specific validation losses, which enables us to track each modality's training progress via its validation loss and convergence slope within a historical window
This approach enables dynamic adjustment of modality weights with minimal computational overhead, thereby enhancing performance across all modalities in the context of omni-modal learning. Here, we provide a detailed introduction to our dynamically adaptive balance strategy.
The schematic diagram is shown in \cref{fig-multi_modal_balance}.


\textbf{Data Partition}. We randomly split a validation subset from the training data for each modality, comprising $\sum_{i=1}^{M}S_i*B_i$ samples. Here, $S_i$ denotes the number of validation steps per validation segment for the $i$-th modality, and $B_i$ represents the validation batch size for that modality, $M$ represents the number of modalities. The data in the validation subset is excluded from the model training process.

\textbf{Convergence Slope Calculation}. Different modalities often exhibit varying degrees of training difficulty, resulting in distinct numerical ranges of losses. To ensure fair weight allocation across modalities, we calculate the convergence slopes from the normalized validation loss:
\begin{equation}
    \widetilde{L}^{val}_{i, t} = \frac{L^{val}_{i, t} - \min\{\min\{L^{val}_{i, j}\}_{j=t-H+1}^{t}, \epsilon\}}{\max\{L^{val}_{i, j}\}_{j=t-H+1}^{t} - \min\{\min\{L^{val}_{i, j}\}_{j=t-H+1}^{t}, \epsilon\}},
\end{equation}
where $L^{val}_{i, t}$ represents the validation loss of the $i$-th modality at the $t$-th validation segment, $H$ represents the historical window size, and $\epsilon$ is set to $1e^{-6}$ to prevent division by zero. Subsequently, we employ a linear regression model of the form $a_{i, t}x+b_{i, t}$ to fit the validation loss within the historical window, where the slope coefficient $a_{i, t}$ represents the current convergence rate of the modality.

\textbf{Weight Allocation Adjustment}. To ensure a balanced initialization and mitigate potential inaccuracies in the initial convergence trajectories, we set all modality-specific loss weight $\widetilde{w}_{i, 0}$ to $1$ and maintain these fixed values throughout the first $H$ validation segments. For the $t$-th validation segment (where $t > H$), we first compute the normalized slope $\widetilde{a}_{i, t}$ and the convergence score $s_{i, t}$ for the $i$-th modality as follows:
\begin{equation}
    \widetilde{a}_{i, t} = \frac{M * a_{i, t}}{\sum_{j=1}^{M}|a_{j, t}|},
    \quad s_{i, t} = \text{softmax}\left(\widetilde{a}_{i, t}\right) * \left(-1 * \widetilde{a}_{i, t}\right),
\end{equation}
where the softmax operation is performed across the modality dimension. Next, we calculate the modal weight allocation $w_{i, t}$ of the current segment as follows:
\begin{equation}
    w_{i, t} = M * \text{softmax}\left(f * s_{i, t}\right)
\end{equation}
where $f$ is a scaling factor that regulates the probability distribution of the weights. Multiplying by $M$ ensures that the sum of the weights across all modalities equals $M$. To mitigate abrupt fluctuations caused by single-step weight updates and to enhance the stability of model training, we utilize an exponential moving average (EMA) mechanism to smoothly adjust the training weights for each modality as follows:
\begin{equation}
    \widetilde{w}_{i, t} = \alpha * \widetilde{w}_{i, t-1} + (1 - \alpha) * w_{i, t}
\end{equation}
where the smoothing factor $\alpha$ is set to $0.9$. The adjusted modality-specific loss weight $\widetilde{w}_{i, t}$ is used for all training steps until the next validation segment.

Through this dynamic adaptive balance strategy during instruction tuning, we observed that all modalities can achieve their superior performance compared to the corresponding single-modality model counterpart.


\subsection{Language Proficiency Maintenance Strategies}\label{subsubsec-Language Maintenance}
An omni-MLLM should maintain robust linguistic capabilities, as language is also one of the modalities it supports. During both the pre-training and post-training stages of our omni-MLLM, we discovered that using solely multimodal data to unfreeze our M2-omni LLM leads to a significant decline in linguistic abilities, highlighting the importance of incorporating pure-text data sources. We attribute this phenomenon to the fact that the text accompanying multimodal data often lacks the diversity and complexity of pure text data, which can lead to a biased model.  To mitigate this issue, we adopted a straightforward and efficient approach by incorporating a carefully controlled proportion of pure text data during the training whenever unfreezing the M2-omni LLM. Through extensive experimentation, we found that by carefully controlling the proportion of pure text data, specifically around 25\%, we can effectively prevent the deterioration of multimodal capabilities while maintaining robust linguistic capabilities.

% An effective MLLM should also maintain robust linguistic capabilities. We found that when unfreezing the LLM using solely multimodal data, although there was a marked enhancement in multimodal capabilities, there was a significant deterioration in linguistic abilities. We hypothesize that this is primarily due to the lower quality and coverage of the text corresponding to multimodal data compared to pure text data, as well as the substantial distributional differences between them. To address the aforementioned issue, we employed a straightforward yet effective approach by incorporating pure text data during the pre-training stage when unfreezing the LLM. Our experiments revealed that, with proper proportion control, the inclusion of pure text data does not lead to a decline in multimodal capabilities. Ultimately, we integrated 25\% language data into the total pre-training dataset. To ensure the instruction-following capabilities of the LLM, we also mixed a substantial amount of instruction data within the language data. By doing so, we balanced the competitive relationship between multimodal and linguistic abilities, thereby enhancing multimodal capabilities during training while preserving the original linguistic performance.



\section{Data configurations}\label{subsec:app_data}

\begin{figure}[t]
    \centering
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/pretrain_data_overall.pdf}
        \vspace{-8pt}
        \caption{PT-Stag-3-Data}
        \label{fig:PT-Stage-3-Data}
    \end{subfigure}
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/sft_stage1_data_overall.pdf}
        \vspace{-8pt}
        \caption{IT-Stage-1-Data}
        \label{fig:IT-Stage-1-Data}
    \end{subfigure}
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/sft_stage2_data_overall.pdf}
        \vspace{-8pt}
        \caption{IT-Stage-2-Data}
        \label{fig:IT-Stage-2-Data}
    \end{subfigure}
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/sft_stage3_data_overall.pdf}
        \vspace{-8pt}
        \caption{IT-Stage-3-Data}
        \label{fig:IT-Stage-3-Data}
    \end{subfigure}
    \caption{
    \textbf{Overview of the data configurations during pre-training and instruction tuning.}
    The numbers of image-text, OCR, text, audio-text, video-text, and interleaved image-text data are 0.33B, 0.14B, 0.15B, 25.4M, 27.3M, and 36.9M in PT-Stag-3-Data, respectively.
    The numbers of image-text and text data are 26.2M and 12.7M in IT-Stage-1-Data, respectively.
    The numbers of image-text, text, video-text, and interleaved image-text data are 17.2M, 3.7M, 2.2M, and 0.9M in IT-Stage-2-Data, respectively.
    And the numbers of image-text, text, video-text, interleaved image-text, audio-text, and audio-QA data are 17.2M, 3.7M, 2.2M, 0.9M, 25.4M, and 7.5M in IT-Stage-3-Data, respectively.
    }
    \label{fig:data_configurations}
\end{figure}

\subsection{Pre-training Data}\label{subsubsec:app_pre_training_data}
To align with the objectives of the pre-training phase, pre-training data is constructed adhering to two criteria: (1) aligning different modalities and (2) facilitating concept learning of world knowledge. Consequently, the resulting pre-training dataset exhibits diversity, aggregating multi-source data across various modalities.

% Corresponding to the learning target in the pre-training phase, pre-training data is constructed based on two criterions, i.e. (1) aligning different modalities and (2) providing world knowledge.
% Thus, our pre-training data comes from various sources and covers many modalities.

[Image-Text] We primarily leverage a large-scale dataset comprising weakly labeled, web-crawled image-text pairs sourced from publicly accessible repositories, including~\cite{schuhmann2022laion,sharma2018conceptual,changpinyo2021conceptual,xiezero,byeon2022coyo,gadre2024datacomp}. To focus on high-quality knowledge learning during the Image-Text Knowledge Enhancement pretraining stage, we incorporate two comprehensive caption datasets, namely~\cite{li2024densefusion,deitke2024molmo}. The aggregated dataset consists of approximately 2 billion image-text pairs.


% We mainly utilize a large-scale, weakly labeled, web-crawled set of image-text pairs, which come from publicly accessible sources such as~\cite{schuhmann2022laion,sharma2018conceptual,changpinyo2021conceptual,xiezero,byeon2022coyo,gadre2024datacomp}.
% To focus on high-quality knowledge learning in pretraining stage-2, two detailed caption datasets~\cite{li2024densefusion,deitke2024molmo} are also included.
% The dataset contains a total of 2 billion image-text pairs.
% The relevant data mainly consists of ***.

[OCR] To enhance performance in text-oriented tasks, we incorporate the OCR caption task. We select a range of English OCR datasets, specifically~\cite{soboroff2022complex,biten2022ocr,kim2023web,weber2023wordscape,chen2024textdiffuser}. In addition, we compile a large-scale collection of in-house Chinese OCR datasets, encompassing diverse samples from Chinese documents to scene texts.

% To improve the text-oriented tasks, OCR caption task is involved.
% English OCR datasets, including~\cite{soboroff2022complex,biten2022ocr,kim2023web,weber2023wordscape,chen2024textdiffuser} are selected.
% For Chinese OCR datasets, we collect large-scale samples varying from Chinese documents to scene texts.

[Audio-Text] Audio-text pairs are included for audio alignment. The dataset comprises a total of 30 million audio-text pairs, categorized into three task types: Automatic Speech Recognition (ASR)~\cite{Libriheavy, GigaSpeech, Librispeech, CoVoST2, SPGISpeech, TED_LIUM, SlideSpeech, CoVoST2, AISHELL1, WenetSpeech, KeSpeech, AliMeeting, MagicData_RAMC, Primewords_100h, FreeST, TAL, aidatatang_200zh}, Automatic Audio Caption (AAC)~\cite{WavCaps, Clotho, AudioCaps, MACS}, and Automatic Audio Tagging (AAT)~\cite{AudioSet, VGGSound}. These categories are aggregated from a diverse range of publicly accessible datasets.
% Specifically, the ASR data are sourced from datasets such as Libriheavy~\cite{Libriheavy}, GigaSpeech~\cite{GigaSpeech}, and WenetSpeech~\cite{WenetSpeech}; the AAC data are derived from sources like WavCaps~\cite{WavCaps} and AudioCaps~\cite{AudioCaps}; and the AAT data include contributions from datasets such as AudioSet~\cite{AudioSet} and VGGSound~\cite{VGGSound}.

[Interleaved Image-Text] Interleaved data naturally contains multiple images and accompanying text which are often interrelated, it has been demonstrated in~\cite{sun2024generative,mckinzie2024mm1} to enhance the model's few-shot capabilities in image-text tasks. The public interleaved image-text dataset Multimodal-C4 (MMC4)~\cite{zhu2024multimodal} is included.

[Video-Text] We sample video data from public datasets WebVid-10M~\cite{bain2021frozen} and Youku-mPLUG~\cite{xu2023youku} to support bilingual video-text comprehension. Besides, we collected a large-scale in-house HD video dataset.

[Text] We utilize Pile~\cite{gao2020pile} and Wudao~\cite{yuan2021wudaocorpora} datasets and supplement them with in-house text-only datasets to maintain our M2-omni LLM's linguistic abilities during pre-training.

% Finally, we include text-only data to help preserve the language understanding capabilities of the underlying pre-trained LLM. Pile~\cite{gao2020pile} and Wudao~\cite{yuan2021wudaocorpora} are introduced, as well as some in-house text data to enhance encyclopedic knowledge and academic abilities. To ensure the instruction-following capabilities of the LLM, we also mixed the text-only instruction data as \cref{subsubsec:app_sft_data} shows.

The data configuration for each pre-training stage is carefully designed to align with the corresponding learning objectives, as illustrated in \cref{subsec:app_training}.

\textbf{PT-Stage-1-Data}. This stage utilizes coarse-grained image-text pairs, OCR data, and audio-text pairs for training, achieving alignment between visual/audio encoders and the M2-omni LLM. This stage involves a total of 2.17 billion samples.

\textbf{PT-Stage-2-Data}. We select 0.33B high-quality image-text pairs and 0.14B high-quality OCR pairs from PT-Stage-1-Data. Additionally, all detailed caption data are incorporated to enhance the model's fine-grained understanding capabilities. Furthermore, 0.15B text-only data samples, mixed with image-text instruction pairs, are included to maintain language understanding capabilities in this stage.

\textbf{PT-Stage-3-Data}.
This stage incorporates all the data from PT-Stage-2-Data, supplemented by three additional data types: audio-text, video-text, and interleaved image-text data. The data volumes of these three additional data types are determined through experiments to be 25.4M, 27.3M, and 36.9M, respectively. An overview of the data configuration of PT-Stage-3-Data is shown in \cref{fig:data_configurations}(a).

More details of publicly accessible data used in pre-training stage  can be found in \cref{tab:appendix_pretrain_data}.


\subsection{Instruction Tuning Data}\label{subsubsec:app_sft_data}

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.6\linewidth]{figures/sft_stage1_data_overall.pdf}
%     \caption{
%     \textbf{Overview of IT-Stage-1-Data.}
%     The numbers of image-text and text data are 26.2M and 12.7M, respectively.
%     }
%     \label{fig-IT-Stage-1-Data}
% \end{figure}

We collect both open-source and in-house multimodal instruction tuning data for training.

We first gather a comprehensive open-source dataset encompassing various multimodal instruction tuning data types, including image-text, video-text, audio-text pairs, and interleaved image-text data. High-quality language-only text instruction data are also collected to mitigate the catastrophic forgetting of the M2-omni LLM and preserve its linguistic abilities.

A primary challenge associated with the collected open-source data is the severe imbalance in data distribution.  For instance, categories like science and mathematics exhibit a significant scarcity of data, which necessitates targeted data construction efforts. To address the data imbalance, we develop a data construction pipeline that generates high-quality instruction tuning data for long-tail categories. Specifically, we establish a taxonomy of academic disciplines and analyze the data distribution to identify long-tail categories that require supplementation. Subsequently, we leverage GPT-4 ~\cite{openai2023gpt4}  to generate candidate topics for the targeted field and retrieve relevant images from web sources and our in-house data repositories. Furthermore, we employ GPT-4V~\cite{GPT4VisionSystemCard} to generate question-answer pairs related to the image, based on the candidate topics.  By leveraging this data construction engine, we generate a substantial volume of high-quality instruction tuning data.

% Compared to pre-training data, instruction tuning data is much harder to collect since it rarely naturally exists on the internet.
% And it is crucial for the model¡¯s performance on various modalities and tasks.
% In this study, in addition to collecting open source data, we also construct data through a data construction engine.

% For open source data, we gather all available instruction tuning data and verify data effectiveness by keeping the data diversity and balance.
% The collected data covers all modalities, including image-text, video-text, interleaved image-text, audio-text, text-audio, and text data.
% Note that we also collect high-quality language-only text data for the sake of avoiding the catastrophic forgetting of the LLM and maintaining its conversational abilities.

% Apart from existing open source data, we also construct a large volume of in-house data.
% We found that the collected open source data shows serious unbalanced distribution.
% For example, some data categories, such as science and math, have very limited data volume.
% This limitation influenced the model¡¯s performance on related tasks and domains.
% In this study, we introduce a data construction engine to construct high-quality instruction tuning data for tail categories.
% Specifically, we first select a related field according to the tail category and generate some candidate topics for this field using GPT-4~\cite{openai2023gpt4}.
% Then, we search related images from web sources and our in-house data pools.
% GPT-4v~\cite{GPT4VisionSystemCard} is further utilized to generate Q&A pairs about the image according to the candidate topics.
% These Q&A pairs and the image form the generated VQA data.
% This data construction engine produces a large volume of high-quality instruction tuning data.

The data configurations of each instruction tuning stage are as follows.

\textbf{IT-Stage-1-Data}.
This stage encompasses a diverse range of tasks, comprising both image-text and pure text data, as illustrated in Figure \ref{fig:data_configurations}(b).  For image-text data, we strive to ensure coverage of diverse data types while maintaining a balanced distribution. We determine an optimal ratio of approximately 30\% for text data through empirical evaluation, which helps maintain the M2-omni LLM conversational capabilities. A detailed list of the open-source data utilized in this stage is provided in \cref{tab:appendix_sft_s1_it_1,tab:appendix_sft_s1_it_2,tab:appendix_sft_s1_nlp}.

% This stage aims to enhance the model¡¯s instruction following ability and make it capture more vision-related knowledge.
% Therefore, this stage includes image-text data and pure text data from diverse tasks, as shown in \cref{fig:data_configurations}(b).
% For image-text data, we try to cover different types of data and keep the data balanced.
% For text data, we keep a ratio of nearly $~30\%$ to maintain LLM¡¯s conversational abilities through experimental comparison.

% Detailed open source data used in this stage can be found in \cref{tab:appendix_sft_s1_it_1,tab:appendix_sft_s1_it_2,tab:appendix_sft_s1_nlp}.
% The numbers of image-text and text data are 26.2M and 12.7M, respectively.

\textbf{IT-Stage-2-Data}.
% To enhance the model¡¯s comprehensive capability on vision modality, we increase the proportion of high-quality private data during this stage.
In this stage, we further augment the dataset by incorporating video-text and interleaved image-text data, as illustrated in Figure \ref{fig:data_configurations}(c). Detailed lists of open-source data for image-text, video-text, and interleaved image-text are provided in Tables \ref{tab:appendix_sft_s2_it_1} and \ref{tab:appendix_sft_s2_it_2}, Table \ref{tab:video_text_stage2}, and Table \ref{tab:multi_image_stage2}, respectively.

% We additionally involve video-text and interleaved image-text data in this stage,  as shown in \cref{fig:data_configurations}(c).
% Detailed open source data in image-text, video-text, and interleaved image-text can be found in \cref{tab:appendix_sft_s2_it_1,tab:appendix_sft_s2_it_2}, \cref{tab:video_text_stage2} and \cref{tab:multi_image_stage2}, respectively.
% This stage includes text, image-text, video-text, and interleaved image-text data, their sample number is 17.2M, 3.7M, 2.2M, and 0.9M, respectively.
% For text data, we randomly sample 30\% data from text data in IT-Stage-1-Data.


% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.6\linewidth]{figures/sft_stage2_data_overall.pdf}
%     \caption{
%     \textbf{Overview of IT-Stage-2-Data.}
%     The numbers of image-text, text, video-text, and interleaved image-text data are 17.2M, 3.7M, 2.2M, and 0.9M, respectively.
%     }
%     \label{fig-IT-Stage-2-Data}
% \end{figure}

\textbf{IT-Stage-3-Data}.
Building upon IT-Stage-2-Data, this stage introduces three new modalities: audio-text data, audio-QA data, and free-form image generation data. We create audio-QA data by converting the question text from image-text and video-text data into audio, enabling the model to comprehend images or videos based on textual or auditory instructions. In this Omni-Modality Instruction Tuning stage, we leverage text and interleaved image-text data from the previous stage, while selectively incorporating high-quality video-text data to further boost the model's video comprehension capabilities.

The detailed open-source data for video-text, audio-text, and audio-QA are provided in Tables \ref{tab:video_text_stage3}, \ref{tab:appendix_audio_text}, and \ref{tab:appendix_sft_s3_audioqa}, respectively.


% This stage is responsible for empowering models with comprehensive capability on all modalities and tasks.
% To this end, this stage contains data from all modalities and tasks, as shown in \cref{fig:data_configurations}(d).
% Compared with IT-Stage-2-Data, this stage adds audio-text data and audio-QA data.
% The text and interleaved image-text data remain consistent with the previous stage.
% For image-text data, we add free-form image generation data.
% For audio-QA data, we select a part of image-text and video-text data and transform their question text into audio, forming the image-audio-text and video-audio-text data, which makes the model be capable of following text or audio instructions to understand the image or video.
% For video-text data, we select high-quality data to further enhance model's video understanding ability.

% Detailed open source data of video-text, audio-text, audio-QA can be found in \cref{tab:video_text_stage3}, \cref{tab:appendix_audio_text}, and \cref{tab:appendix_sft_s3_audioqa}, respectively.

% In summary, the numbers of image-text, text, video-text, interleaved image-text, audio-text, and audio-QA data in this stage are 17.2M, 3.7M, 2.2M, 0.9M, 25.4M, and 7.5M, respectively.

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.6\linewidth]{figures/sft_stage3_data_overall.pdf}
%     \caption{
%     \textbf{Overview of IT-Stage-3-Data.}
%     The numbers of image-text, text, video-text, interleaved image-text, audio-text, and audio-QA data are 17.2M, 3.7M, 2.2M, 0.9M, 25.4M, and 7.5M, respectively.
%     }
%     \label{fig-IT-Stage-3-Data}
% \end{figure}

\subsection{Alignment Tuning Data} \label{subsubsec:app_post_training_data}
As shown in Equation \eqref{alignment_tuning}, this stage leverages data from two primary sources: preference data and instruction tuning data. Both datasets cover a range of tasks, including multi-turn conversations, VQA, chat, charts, math, OCR, and others.

A semi-automated approach is employed to construct the preference dataset.  We first collect an in-house multimodal prompt dataset generated by actual users. Subsequently, we employ both the M2-omni model, fine-tuned through the SFT phase,  and GPT-4o to generate corresponding responses for each collected multimodal prompt. Human annotators then identify the higher-quality response from the two generated responses and construct response preference pairs. We evaluate the response quality based on four dimensions: relevance, fluency, content richness, and format rationality. More evaluation details can be found in \ref{sec:human_evaluation}.

For the instruction tuning data, we sample from each modality within the Omni-Modality Instruction Tuning stage,  and maintain a 1:1 ratio to preference data, as informed by empirical studies.


% As illustrated in Equation \eqref{alignment_tuning},
% The preference data used for alignment tuning originates from two primary sources: the instruction tuning dataset and actual user-generated prompt data. These sources encompass multi-turn conversations and various tasks such as VQA, chat, charts, math, and OCR, among others. To further develop the preference datasets, we employ an automated method. Initially, we utilize GPT-4 to filter out low-quality data samples. Subsequently, GPT-4 is prompted to generate reference preference answer pairs by identifying high-quality responses and contrasting them with lower-quality counterparts. Finally, human annotators provide annotations for the training dataset. As a result, we compile a training dataset comprising approximately 5,000 preference samples, which evaluate four experiential dimensions: relevance, fluency, content richness, and format rationality.

% Regarding the instruction tuning data, samples are drawn from each modality within the instruction tuning phase. In a series of experiments, we explore various ratios of instruction tuning and alignment tuning datasets to determine the optimal balance. Ultimately, we identify a 1:1 ratio as the most effective, corresponding to a 0.01\% sampling of the entire instruction tuning dataset. 