
\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/overall.pdf}
    % \includegraphics[width=13cm]{figures/overall.pdf}
    \caption{
    \textbf{Overall illustration of \method.}
    (Top) \method employs a multi-stage training with progressively modality alignment and multimodal multi-task balanced training strategy to achieve the optimal performance of each modality.
    (Left-bottom) \method supports as many modalities and tasks as other omni-MLLMs combined.
    (Right-bottom) \method achieves competitive performances on a broad range of multimodal tasks among its omni-MLLM counterparts. Note that the values on Librispeech~\cite{Librispeech} and Aishll1~\cite{AISHELL1} are taken the reciprocal for better visualization, and the results on Librispeech~\cite{Librispeech} and Aishll1~\cite{AISHELL1} of GPT-4o (GPT-4o-Realtime) are taken from \cite{yao2024minicpm}.
    More comprehensive results can be found in \cref{sec:exp}.
    }
    \label{fig-modality_support}
\end{figure}

\begin{abstract}

We present \method, a cutting-edge, open-source omni-MLLM that achieves competitive performance to GPT-4o.  \method employs a unified multimodal sequence modeling framework, which empowers Large Language Models (LLMs) to acquire comprehensive cross-modal understanding and generation capabilities.  Specifically, \method can process arbitrary combinations of audio, video, image, and text modalities as input, generating multimodal sequences interleaving with audio, image, or text outputs, thereby enabling an advanced and interactive real-time experience.  The training of such an omni-MLLM is challenged by significant disparities in data quantity and convergence rates across modalities. To address these challenges, we propose a step balance strategy during pre-training to handle the quantity disparities in modality-specific data.  Additionally,  a dynamically adaptive balance strategy is introduced during the instruction tuning stage to synchronize the modality-wise training progress, ensuring optimal convergence.  Notably, we prioritize preserving strong performance on pure text tasks to maintain the robustness of \method's language understanding capability throughout the training process. To our best knowledge, \method is currently a very competitive open-source model to GPT-4o, characterized by its comprehensive modality and task support, as well as its exceptional performance. We expect \method will advance the development of omni-MLLMs, thus facilitating future research in this domain.

\end{abstract}

