\section{Unified Framework for MultiModal Understanding and Generation}\label{sec:overall_architecture}



\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/architecture.pdf}
    \caption{
    \textbf{Overall architecture of \method.} \method can process arbitrary combinations of text, image, video, and audio modalities as input, generating multimodal sequences interleaving with text, image, or audio outputs.
    }
    \label{fig-architecture}
\end{figure}

% In this section, we discuss the overall architecture of \method in \cref{subsec:app_architecture} first.
% Subsequently, the detailed multimodal multi-task training procedure is introduced in \cref{subsec:app_training}.
% Then, we introduce the comprehensive training data configurations used in \method in \cref{subsec:app_data}.

\subsection{Overall Architecture}\label{subsec:app_architecture}

% It is challenging to build a unified framework for multimodal understanding and generation due to the disparate representational spaces required for understanding and generation tasks.
% In this work, our key insight is to decouple the two representational spaces and design a unified modeling framework that can effectively integrate multimodal understanding and generation tasks.
% To this end, our \method employs modality-specific processing pathways to minimize mutual interference between modalities and tasks.
% The overall architecture is shown in \cref{fig-architecture}, which consists of modality encoders (the vision encoder and the audio encoder), connectors, LLM, and modality generators.
% Taking multimodal information (textual, image, video, and audio) as input, the corresponding modality encoders encode them into the feature spaces.
% The connectors project the features into a unified LLM embedding space, in which the projected tokens are concatenated as the input of the LLM.
% The LLM integrates the multimodal information and output the decodding embecdding for unified multimodal understanding and generation.
% For image generation tasks, we employ textual descriptions as an intermediate representation, thereby circumventing the need for direct alignment of latent image features.
% For speech generation, we leverage a discrete token prediction-based approach to enable real-time, streaming audio synthesis with minimal impact on the performance of other modality branches.
% Below we introduce the details of each module.


We aim to build a unified framework that simultaneously supports multimodal understanding and generation tasks, while minimizing interference between different modality tasks through decoupled architecture design. Our encoding procedure is inspired by the design of UNIFIED-IO2~\cite{unified_io2}, which utilizes a modality-aware encoder to map diverse inputs (such as images, text, audio, and video) into a shared token representation space. Previous studies, such as Janus~\cite{Janus}, have demonstrated that multimodal understanding and generation tasks can interfere with each other, mainly due to the disparate levels of information granularity required for image understanding and generation.  In contrast to Janus~\cite{Janus}, which employs separate pathways for visual encoding, we leverage textual descriptions as an intermediate representation for image generation tasks, effectively bypassing the need for direct alignment of latent image features. For speech generation, we adopt a discrete token prediction-based approach, which enables real-time, streaming audio synthesis while minimizing the impact on the performance of other modality branches. Figure 2 illustrates the overall architecture of the proposed model. We will elaborate on the details of each module below.






% Most vision encoders used in previous MLLMs, e.g., CLIP ViT encoder~\cite{clip_2021}, EVA-CLIP~\cite{eva-clip_2023}, SigLIP~\cite{siglip_2023}, and InternViT~\cite{internvl_2024}, can only process fixed resolution images.
% To process images with arbitrary resolutions, most previous works, e.g., LLaVA-Next~\cite{llava-next_2024} and InternVL~\cite{internvl_2024}, split a large image into several tiles that the vision encoder can process.
% However, this method will disrupts the spatial relationships between patches within an image.
% following recent methods~\cite{qwen2-vl_2024, laurenccon2024matters, points1.5_2024}.
% It can process images of any resolution by dynamically converting them into a variable number of visual tokens, without patch splitting.
% During training, multiple image sequences are packed into a single sequence for batch forwarding.
% For image, we add two special tokens (\texttt{<image>} and \texttt{</image>}) at the beginning and end of the compressed image tokens.
% Consequently, a $448\times448$ image is represented by 258 tokens.
% For each video, we uniformly sample a fixed number of 16 frames during the pre-training stage to accelerate training and ensure stability.
% The two special tokens are replaced with \texttt{<video>} and \texttt{</video>} to differentiate between video and image.

\textbf{Vision Encoder.}
In \method, the vision encoder extracts representations from images or whole videos. We utilize a NaViT~\cite{navit_2024}  as the vision encoder, capable of processing videos and images of arbitrary resolution. To reduce the length of visual tokens, we concatenate adjacent $2\times2$ tokens into a single token and use an MLP to reduce the dimension to the original dimension, thereby downsampling the visual representation.
% In \method, we employ a NaViT~\cite{navit_2024} as the vision encoder, which can process images of any resolution. To reduce the visual tokens of each image, an MLP is used to compress adjacent $2\times2$ tokens into a single token.


\textbf{Audio Encoder.}
We utilize the SAN-M~\cite{gao2020san, gao2022paraformer} encoder to extract audio tokens. Subsequently, we apply 1x3 average pooling to the audio encoder's output, aggregating every three adjacent tokens into a single token, which reduces the overall number of audio tokens. To accommodate the variability in audio token sequence lengths, we pad the compressed audio sequence with special \texttt{<audio\_pad>}  tokens, thereby ensuring that all sequences conform to a uniform length.


% Additionally, we incorporate two special tokens (\texttt{<audio>} and \texttt{<{/audio>}}) at the beginning and end of the audio tokens.
% Consequently, each audio is represented by 258 tokens.

\textbf{M2-omni LLM.} The M2-omni LLM integrates the multimodal information and outputs the decoder embedding for unified multimodal understanding and generation. Our M2-omni LLM is initialized with pre-trained weights from the Llama3~\cite{llama_2023, llama3_2024} series, specifically Llama3.1-8B or Llama3.3-70B. To facilitate unified positional encoding across textual, image, video, and audio modalities, and to enable the model to generalize to longer sequences during inference, we substitute the original 1D-RoPE~\cite{su2024roformer} in Llama with M-RoPE~\cite{qwen2-vl_2024}.

\textbf{Image Generator.} To decouple the representation spaces of generation and understanding, building upon the insights from~\cite{li2023textbind, unifiedmllm, wang2024modaverse}, we utilize textual descriptions as an intermediate representation for image generation. During training, we warp the image captions with two special tokens, i.e. \texttt{<gen\_image>} and \texttt{</gen\_image>},  allowing the model to generate textual descriptions for image generation in a flexible and unconstrained manner. At inference time, the \method LLM generates the textual description, and the generated captions enclosed by the two special tokens are utilized as the textual condition for image generation. We employ an offline Stable Diffusion (SD) model~\cite{sd_2022} as the image generator.
% To decouple the representation spaces of generation and understanding, inspired by~\cite{li-etal-2024-textbind, unifiedmllm, wang2024modaverse}, we employ textual descriptions as an intermediate representation for image generation.
% Specifically, we warp the image captions with two special tokens \texttt{<gen\_image>} and \texttt{</gen\_image>} during training, which enables the model to output the textual description for image generation in a free-form manner.
% During inference, the LLM outputs the textual description and the generated captions between the two special tokens are employed as the textual condition for image generation.
% We employ an offline Stable Diffusion (SD) model as the image Generator.

\textbf{Audio Decoder.}
Inspired by the approaches in ~\cite{MinMo, mini_omni2}, we utilize the M2-omni LLM to predict discrete audio tokens for speech generation in an end-to-end style. The predicted discrete audio tokens are then fed into the pretrained CosyVoice~\cite{du2024cosyvoice} flow matching and vocoder model to generate audio streams. Given the similarity in form between audio discrete tokens and language tokens, we can repurpose the M2-omni LLM's model structure to facilitate audio generation tasks, thereby enabling compatibility with multimodal understanding tasks.

% For speech generation, we leverage the model to predict discrete audio tokens, emulating the style of MinMo~\cite{MinMo} and Janus~\cite{mini_omni2}. Subsequently, the CosyVoice~\cite{du2024cosyvoice} decoder is applied to convert these discrete audio tokens into audio streams. Since audio discrete tokens are similar in form to language tokens, we can leverage the model structure of the LLM backbone, ensuring compatibility of audio generation tasks with multimodal understanding tasks.

\cref{tab-model_config} illustrates the detailed model configuration of \method, and \cref{{fig-template}} demonstrates the data templates for image, video, and audio.

\begin{table}[t]
\centering\footnotesize
\caption{
\textbf{Detailed pre-trained model configuration of \method.}
}
% \vspace{3pt}
\setlength{\tabcolsep}{3pt}
\begin{tabular}{c|c|c|c|c|c}
\toprule
Model Name & \#Param & Vision Encoder & Audio Encoder & LLM & SD \\
\midrule
\method-9B & 8.8B & \multirow{2}{*}{ViT-600M~\cite{qwen2-vl_2024}} & \multirow{2}{*}{paraformer-zh~\cite{gao2022paraformer}} & Llama3.1-8B~\cite{llama3_2024} & \multirow{2}{*}{SD-3-medium~\cite{esser2403scaling}} \\
\method-72B & 71.8B & & & Llama3.3-70B~\cite{llama3_2024} & \\
\bottomrule
\end{tabular}
\label{tab-model_config}
% \vspace{-12pt}
\end{table}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/template.pdf}
    \caption{
    \textbf{Illustration of the templates of image, video, and audio.}
    }
    \label{fig-template}
\end{figure}

\subsection{Multi-Stage Training with Progressively Modality Alignment}\label{subsec:app_training}

% Given a multimodal dataset containing discrete token sequences, each of which can be formulated as $x = (x_1, \cdots, x_\ell)$.
% The overall training objective of \method is modeling the multimodal sequence distribution in an autoregressive manner:
% \begin{equation}
% \log p_{\theta} (x) = \sum_{i = s}^{\ell-1} \log p_{\theta}(x_{i+1} | x_{1}, \dots, x_{i}),
% \end{equation}
% where $\theta$ denotes the parameters of the model and $\ell$ is the sequence length, and $s$ denotes the start index of tokens to apply autoregressive loss.
% The goal of \method is to achieve the unification of multimodal understanding and generation.
% However, it is challenging to maintain optimal performance across all modalities when training such a complex omni-MLLMs framework.
% This performance degradation often arises from significant disparities in data quantity and convergence rates across different tasks.
% To overcome this, we designed a multimodal multi-task balanced training procedure.
% Specifically, the pre-training and instruction tuning stages are divided into three stages, designed to progressively integrate more modalities and enhance the model’s capabilities on all modalities.
% Furthermore, we introduce a \textit{step balance strategy during pre-training} and a \textit{dynamic adaptive balance strategy during instruction tuning} to ensure all modalities reach their optimal performance.
% In this section, we will introduce the overall training procedure of \method first, then the step balance strategy during pre-training and the dynamic adaptive balance strategy during instruction tuning will be introduced in details.
% The training procedure of \method includes three main stages: pre-training, instruction tuning, and alignment tuning.

Given a multimodal dataset, we employ modality-aware encoders to project diverse modality inputs, including images, text, audio, and video, into a unified token representation space. Formally, the input multimodal sequences are denoted as  $x = (x_1, \cdots, x_\ell)$, where $\ell$ represents the length of the sequence, and each $x_i$ corresponds to a modality input token (e.g., image, text, audio, or video).  In particular, we model the joint probability distribution of the multimodal sequence in an autoregressive manner, where each token is conditioned on the previous tokens, as shown in the following equation:
\begin{equation}
\log p_{\theta} (x) = \sum_{i = s}^{\ell-1} \log p_{\theta}(x_{i+1} | x_{0}, \dots, x_{i}),
\end{equation}
Notably, $s$ denotes the start index of discrete output tokens and only discrete output tokens $x_{>s}$ are considered as the modeling targets, $\theta$ denotes the parameters of the model.
We introduce a multi-stage training framework that progressively achieves modality alignment by incrementally incorporating knowledge from multiple modalities. As shown in \cref{fig-pretrain_pipeline}, the overall training procedure of our proposed \method consists of three primary stages: pre-training, instruction tuning, and alignment tuning.  Both the pre-training and instruction tuning stages are further divided into three sub-stages, each designed to incrementally incorporate additional modalities. The training hyperparameters and configurations are summarized in \cref{tab:app_train_hyperparameter}.

\subsubsubsection{\textbf{Pre-training}}

\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/training_pipeline.pdf}
    \caption{
    \textbf{Illustration of the training pipeline of \method.}
    Both the pre-training and the instruction tuning contain three stages, designed to progressively absorb knowledge from more modalities and ensure the model's optimal performance on all modalities and tasks. $L_{un}$ and $L_{gen\_a}$ denote understanding and audio generation loss, respectively.
    }
    \label{fig-pretrain_pipeline}
\end{figure}

The pre-training stage primarily focuses on aligning multiple modalities with our \method LLM, thereby enabling it to capture multimodal concept representations and develop cross-modal perception capabilities.
% The pre-training pipeline of \method is illustrated in \cref{fig-pretrain_pipeline}, which includes three stages, designed to progressively absorb knowledge from more modalities.
% During this stage, we employ the step balance strategy to ensure that the model can stably and unbiasedly learn the knowledge of each modality.
% In addition, we prioritize preserving strong performance on pure text tasks to maintain the robustness of \method's language understanding capability throughout the training process.

\textbf{Stage 1. Encoder Alignment.}
This phase leverages image-text pairs, OCR data, and audio-text pairs for training, achieving alignment between the visual/audio encoders and \method LLM. Moreover, by concatenating multiple image-text pairs into a single interleaved sequence, we enhance in-context understanding capabilities and attain a $1.5\times$ acceleration in training efficiency.
% This phase exclusively utilizes image-text pairs, OCR data, and audio-text pairs for training, achieving alignment between the visual/audio encoders and M2-omni LLM.
% During this stage, only the image and audio connectors are trained while the encoders and LLM remain frozen.
%Following \cite{llava-next_2024}, the bridging module employs a two-layer MLP architecture to enhance modal alignment capabilities.
% For image-text pairs, the maximum number of image tokens is maintained at 320, while 960 for OCR data.
%Audio embeddings after the connector are compressed to half length via a 1×2 pooling.
% We pack the single-image pairs into interleaved sequences, which enhance the in-context understanding capabilities while achieving a $1.5\times$ acceleration in training efficiency.
% The learning rate is set to $2e^{-5}$.

\textbf{Stage 2. Image-Text Knowledge Enhancement.}
This stage concentrates on training with high-quality image-text pair data (selected from stage 1) and OCR data, with a specific focus on enhancing image-text fine-grained comprehension capabilities. This, in turn, facilitates improved understanding of interleaved image-text and video understanding tasks in stage 3. Furthermore, language-only pure text data is incorporated to prevent the degradation of \method LLM's language understanding capabilities.

% This stage focuses exclusively on training with high-quality image-text understanding data (selected from stage 1) and OCR data, specifically targeting the enhancement of image-text fine-grained comprehension capabilities to ensure more stable convergence for interleave-image-text and video understanding tasks in stage 3.
% Additionally, language-only pure text data is included to maintain the robustness of \method's language understanding capability.



% During this phase, all modules except the audio encoder and audio connector are unfrozen.
% We maintain the same maximum image resolution as in Stage 1.
% The learning rate is set to $1e^{-5}$ for all modules except the vision encoder which is set to $1e^{-6}$.

\textbf{Stage 3. MultiModal Joint Training.}
This stage integrates omni-modality knowledge in a single stage, thereby facilitating comprehensive modality alignment and unified representation learning. In this stage, we incorporate high-quality image-text pairs, video-text pairs, interleaved image-text sequences, audio-text pairs, and language-only data for end-to-end multimodal pre-training.
To balance convergence rates across different modalities, a step balance strategy is employed in this stage, which will be introduced in \cref{subsubsec-Step Balancing Strategy}.


% This stage aims at learning all-modality knowledge.
% This comprehensive stage incorporates high-quality image-text pairs, video-text pairs, interleaved image-text sequences, audio-text pairs, and language-only data for end-to-end multimodal pre-training.
% To balance convergence rates across different modalities, a step balance strategy is performed in this stage, which will be introduced in \cref{subsubsec-Step Balancing Strategy}.


\subsubsubsection{\textbf{Instruction Tuning}}

Instruction tuning aims to make models better understand the instructions from users and fulfill the demanded tasks.

\textbf{Stage 1. Image-Text Instruction Tuning}.
This stage concentrates on enhancing the model's instruction-following ability for image modality, particularly in specialized image-related tasks, such as science, OCR, documents, and charts, which were not adequately learned during pre-training.

\textbf{Stage 2. Visual Instruction Tuning}.
This stage aims to enhance the modell's comprehensive capability on visual modality, including the capability on image-text, video-text, and interleaved image-text understanding.

\textbf{Stage 3. Omni-Modality Instruction Tuning}.
This stage further integrates the audio modality and generation tasks, enabling the model to follow instructions on mixed multi-modal sequences. Our study reveals that coordinating the training progress of diverse modalities and tasks is challenging, as the model's optimal performance across all modalities is hindered by inconsistent data volumes and convergence speeds among tasks. We propose a dynamic adaptive balance strategy to address this issue, which will be introduced in \cref{subsubsec-Dynamic Adaptive Balance}.

% As shown in \cref{fig-pretrain_pipeline}, the instruction tuning pipeline of \method includes three stages, designed to progressively integrate more modalities and enhance the model’s capabilities on all modalities.
% During this stage, we propose a dynamically adjusted gradient weighting strategy to ensure all modalities reach optimal performance.

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=1.0\linewidth]{figures/sft_pipeline.pdf}
%     \caption{
%     \textbf{Illustration of the instruction tuning pipeline of \method.}
%     The training procedure includes three stages, designed to progressively integrate more modalities and ensure the model’s optimal performance on all modalities and tasks.
%     }
%     \label{fig-sft_pipeline}
% \end{figure}

% \textbf{Stage 1. Image-Text Instruction Tuning}.
% As shown in \cref{fig-pretrain_pipeline}, this stage focuses on instruction tuning for vision modality.
% The aim of this stage is to enchance the model’s instruction following ability and make it capture more vision-related knowledge that is not sufficiently learned during pre-training, such as science, OCR, documents, charts, etc.
% During this stage, only the vision encoder (and its MLP) and LLM are trainable.
% The learning rate is set to $2e^{-5}$ for all modules except the vision encoder which is set to $2e^{-6}$.
% The minium and maxium token number for each image are set to 100 and 1280, respectively.
% Image-text data and pure text data are used in this stage, which is denoted as \textbf{IT-Stage-1-Data} and will be introduced in \cref{subsubsec:app_sft_data}.

% \textbf{Stage 2. Vision Instruction Tuning}.
% The illustration of this stage is shown in \cref{fig-sft_pipeline}(b).
% This stage aims to enhance the model’s comprehensive capability on vision modality, including the capability on image-text, video-text, and interleaved image-text understanding.
% Four types of data are used in this stage, i.e., text, image-text, video-text, interleaved image-text, which is denoted as \textbf{IT-Stage-2-Data} and will be introduced in \cref{subsubsec:app_sft_data}.
% During this stage, the trainable modules and hyperparameters keep the same as stage-1.
% The maxium token number for each image is raised to 2560.
% For video and interleaved image-text data, the minium and maxium token number for each frame (or image) are set to 128 and 768, respectively.
% For video, the maxium frame number is set to 24.

% \textbf{Stage 3. All-Modality Instruction Tuning}.
% This stage further integrates the audio modality and generation tasks, allowing the model to be capable of handling all modalities and empowering it with generation ability.
%The learning rate is set to $1e^{-5}$ for all modules except the vision encoder which is set to $1e^{-6}$.
% The setting of the token number is the same as stage-2, except for video.
% For videos, we uniformly sample two frames per second by default.
% To constrain the total video token length, we sample up to 128 frames and dynamically adjust the frame resolution between 100,352 and 570,752 pixels.
% There are 6 types of data used in this stage, including text, image-text, video-text, interleaved image-text, audio-text, and text-audio, denoted as \textbf{IT-Stage-3-Data}.
% During this stage, all modules are trainable except the vision encoder and audio encoder considering that their feature extraction abilities have been sufficiently enhanced during previous stages.
% In this study, we found that it’s difficult to coordinate the training progress of various modalities and tasks and guarantee the model’s optimal performance on all modalities since the data volume and convergence speed of each task are inconsistent.
% We propose a dynamic adaptive balance strategy to address this issue, which will be introduced in \cref{subsubsec-Dynamic Adaptive Balance}.

% \subsubsection{Post-training}\label{subsubsec:app_post_training}
\subsubsubsection{\textbf{Alignment Tuning}}

This phase focuses on refining the quality and stylistic coherence of chat interactions, ensuring the model's proficiency is maintained across all modalities.  The instruction tuning stage equips the model with general multimodal conversational abilities. However, the model's responses often suffer from limitations, including brevity, lack of fluency, irrelevance, inappropriate formatting, and hallucinations, which can compromise the user experience. To mitigate these limitations and further enhance the chat experience, a preference alignment tuning stage is introduced following the instruction tuning stage. This stage employs a unified training strategy that integrates DPO~\cite{rafailov2024directpreferenceoptimizationlanguage} and instruction tuning, as defined by the equation:
\begin{equation}\label{alignment_tuning}
Loss_{at}(x) = L_{dpo}(x_{chosen}, x_{rejected})+\lambda*L_{it}(x_{chosen}, x_{it}),
\end{equation}
where $L_{dpo}$ and $L_{it}$ denotes the DPO loss and instruction tuning loss respectively. $x_{chosen}$ and $x_{rejected}$ represent the chosen samples and rejected samples from the preference dataset, and $x_{it}$ denotes the samples from the instruction dataset. In practice, we empirically set $\lambda$ to 0.3.  Additionally, we employ Low-Rank Adaptation (LoRA) \cite{hu2021loralowrankadaptationlarge} to update 5.0\% of the LLM's backbone weights, thereby preventing catastrophic forgetting.

% This phase aims to enhance the quality and stylistic consistency of chat interactions while preserving the model’s proficiency across all modalities.
% During the instruction tuning phase, the model acquires general multimodal conversational abilities. Nevertheless, its responses often exhibit shortcomings such as brevity, lack of fluency, irrelevance, inappropriate formatting, or hallucinations, which can detract from the user experience.

% To address these issues and further refine the chat experience, we introduce a preference alignment tuning stage following the instruction tuning. This stage primarily employs Direct Preference Optimization (DPO ~\cite{rafailov2024directpreferenceoptimizationlanguage}) based on our curated datasets focused on experience preference training.

% To preserve the model's initial capabilities and alleviate catastrophic forgetting, we have developed an innovative alignment training approach utilizing two strategies. First, we adopt a unified training strategy that integrates DPO and instruction tuning. This involves adjusting the ratio of instruction tuning and DPO datasets and employing a mixed-loss training approach on a per-batch basis, as defined by the equation:
% \begin{equation}
% L_{at} (x) = w_1L_{dpo}(x_{chosen}, x_{rejected})+w_2L_{it}(x_{chosen}, x_{it}),
% \end{equation}
% where $L_{at}$, $L_{dpo}$ and $L_{it}$ denotes the alignment tuning unified loss, DPO loss, and instruction loss respectively. $x_{chosen}$ and $x_{rejected}$ denotes the chosen samples and rejected samples of preference dataset, respectively, and $x_{it}$ denotes the samples of instruction dataset. Additionally, we set hyperparameters $w_1$ as 1.0 and $w_2$ as 0.3.
% Secondly, we apply Low-Rank Adaptation (LoRA) \cite{hu2021loralowrankadaptationlarge}, updating 5.0\% of the LLM's backbone weights.


% \begin{figure}[t]
%     \centering
%     \includegraphics[width=1.0\linewidth]{figures/posttraining_pipeline.pdf}
%     \caption{
%     \textbf{Illustration of the instruction tuning pipeline of \method.}
%     }
%     \label{fig-post_training_pipeline}
% \end{figure}

\begin{table*}[t]
\centering
\caption{\textbf{Detailed training hyperparameters and configurations for \method.}
The model configurations are meticulously tuned to achieve consistent performance across various modalities and tasks.
Note that T, I, V, and A in the modalities row denote textual, image, video (and interleaved image-text), and audio, respectively.
U and G in the task row denote understanding and generation tasks, respectively.
LR denotes the learning rate, and AT represents the alignment tuning stage.
}
{\fontsize{8}{10}\selectfont
\renewcommand{\arraystretch}{1.0}
{
\setlength\tabcolsep{5pt}
\begin{tabular}{l|ccc|ccc|c}
\toprule
\multirow{2}{*}{Settings}         & \multicolumn{3}{c|}{Pre-training}   & \multicolumn{3}{c|}{Instruction Tuning} & AT \\
                                  & Stage 1    & Stage 2    & Stage 3    & Stage 1          & Stage 2           & Stage 3          &           \\
\hline
Data                           & \makecell{PT-Stage-\\1-Data}  & \makecell{PT-Stage-\\2-Data}    & \makecell{PT-Stage-\\3-Data}  & \makecell{IT-Stage-\\1-Data}        & \makecell{IT-Stage-\\2-Data}         & \makecell{IT-Stage-\\3-Data}        & \makecell{AT\\-Data}        \\
Modalities & I\&A        & T\&I      & All              & T\&I        & T\&I\&V         &  All  &  All      \\
Tasks  & U        & U      & U              & U        & U         &  U\&G  &  U\&G      \\
Trainable                         & Connectors        & \makecell{Vision Encoder\\+Connector}      & Full Model & Full Model              & Full Model        & \makecell{w/o\\Encoders}              & \makecell{w/o\\Encoders}       \\
LR                     & 2e-5       & 1e-5         & 2e-5       & 2e-5             & 2e-5              & 1e-5             & 5e-6             \\
LR of Encoders                     & --       & 1e-6         & 2e-6       & 2e-6             & 2e-6              & --             & --             \\
Weight Decay                      & 0.05       & 0.05         & 0.05       & 0.05             & 0.05              & 0.05             & 0.05             \\
Training Epochs                   & --         & --           & --          & 1               & 2                 & 1               & 2                \\
Max Image Tokens              & 320         & 320           & 320         & 1280               & 2560                & 2560               & 2560               \\
Max Video Frames              & --         & --           & 8         & --               & 16                & 128               & 128               \\
\bottomrule
\end{tabular}
}
}
\label{tab:app_train_hyperparameter}
\end{table*}











