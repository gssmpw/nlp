
\section{Introduction}\label{sec:intro}

The recent breakthroughs in Large Language Models (LLMs)~\cite{openai2023gpt4, llama3_2024, reid2024gemini1_5} have significantly accelerated the development of Multimodal LLMs (MLLMs)~\cite{openai2024gpt4ocard, llama32, internvl_2024, llava-next_2024, qwen2-vl_2024}.
Omni-MLLM expands MLLM's capabilities by incorporating additional non-linguistic modalities such as video, audio, and others, thereby facilitating a more comprehensive and multidimensional understanding of the world. A prominent example of Omni-MLLM is GPT-4o~\cite{openai2024gpt4ocard}, which has demonstrated remarkable multimodal processing capabilities.  GPT-4o features a novel, unified framework that can process arbitrary combinations of text, audio, image, and video inputs and generate outputs across multiple modalities, including text, audio, and image. Consequently, this enables more natural and intuitive human-computer interaction, marking a crucial milestone on the path toward AGI. The research community has been actively enriching Omni-MLLM by incorporating additional modalities and task support in recent years~\cite{baichuan-omni,emu3,deepseek_janus,muse_vl, mini_omni,mini_omni2, vita,fu2025vita,yao2024minicpm}. However, existing works fall short of matching GPT-4o's comprehensive modality support and task versatility.  Current omni-MLLM models are constrained by their limited support for either audio modalities~\cite{baichuan-omni, emu3, deepseek_janus,muse_vl}, which impede real-time interaction, or visual generation tasks~\cite{mini_omni, vita}, thereby restricting their applicability in visual applications.  To advance current Omni-MLLM towards a more sophisticated GPT-4o level counterpart, three primary challenges must be addressed: (1) developing a unified framework for multimodal understanding and generation tasks, (2) designing training strategies and pipelines that prevent performance degradation across all modalities, and (3) a detailed training protocol to achieve exceptional performance.

A fundamental challenge in building a unified multimodal framework arises from the disparate representational spaces required for understanding and generation tasks. These differences, as noted by~\cite{deepseek_janus}, often lead to performance degradation in a shared model, particularly in terms of task-specific accuracy and generalization. In response to this challenge, we propose a unified modeling framework designed to effectively integrate multimodal understanding and generation tasks. Our framework features modality-specific processing pathways and utilizes a multi-stage training approach with progressive modality alignment, thereby mitigating interference between modalities and tasks. Specifically, for image generation tasks, building upon the works~\cite{li2023textbind, unifiedmllm, wang2024modaverse}, we employ textual descriptions as an intermediate representation, thereby circumventing the need for direct alignment of latent image features. For speech generation, leverage the model to predict discrete audio tokens enabling real-time, streaming audio synthesis with minimal impact on the performance of other modality branches.

A particular challenge in training Omni-MLLMs is maintaining consistent performance across all modalities when involving many modalities or tasks. This performance degradation often arises from significant disparities in data quantity and convergence rates across different tasks. In this work, we introduce a step balance strategy during pre-training. At each training iteration, a mini-batch comprising samples from each modality is sampled to maintain a balanced representation across all modalities, thereby mitigating bias caused by imbalanced data distribution among modalities. Furthermore, during the instruction tuning stage, we employ a dynamically adaptive balance strategy to regulate the convergence rates across modalities. The underlying rationale is that if one modality exhibits a slower convergence rate, it should be assigned a smaller gradient weight in model updates, thereby allowing faster-converging modalities to take precedence. Conversely, if a modality exhibits a faster convergence rate, it should be assigned a larger gradient weight. By adopting this balanced strategy, we can achieve enhanced performance across all modalities within the framework of omni-modal learning.

Currently,  open-source omni-MLLMs still exhibit a significant performance discrepancy in multimodal understanding compared to GPT-4o, which limits their wide application in industrial scenarios. Compared to other omni-MLLM efforts, our \method achieves state-of-the-art (SOTA) performance among publicly available omni-MLLMs. Specifically, our largest model, \method-72B, achieved an average score of 75.1 on the OpenCompass benchmark for vision-and-text tasks. This score even surpasses the performance of many vision-language specific MLLMs and proprietary commercial models.  Furthermore, we are publicly releasing the comprehensive training details, including data configurations and training procedures to develop \method. This detailed resource is intended to serve as a valuable guide for the community,  fostering research and development aimed at bridging the performance gap between open-source omni-MLLMs and GPT-4o.

To summarize, our contributions are as follows:

\begin{itemize}
    \item We propose \method, an advanced MLLM that demonstrates competitive performance among publicly available omni-MLLMs. \method represents a milestone in comprehensive modality and task support, narrowing the performance gap with proprietary models like GPT-4o. We publicly release the \method, as well as its comprehensive training details, including data configurations and training procedures.
    \item We propose a unified multimodal modeling framework that leverages a multi-stage training approach to achieve progressive modality alignment, enabling the effective integration of multimodal understanding and generation tasks.
    By implementing modality-specific processing pathways and innovative techniques such as text-based image generation and discrete token prediction-based speech generation, we minimize cross-modal interference while achieving comprehensive audio, video, image, and textual understanding and generation capabilities.
    \item To alleviate performance degradation when integrating multiple modalities, we propose a step balance strategy for pre-training and a dynamically adaptive balance strategy for SFT. This approach mitigates the impact due to significant variations in data volume and convergence rates across heterogeneous multimodal tasks.
\end{itemize}


