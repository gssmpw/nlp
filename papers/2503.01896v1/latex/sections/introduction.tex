\section{Introduction}

 Recent progress in transformer-based language modelling \cite{vaswani2017attention, openai2023gpt4,touvron2023llama} has garnered  attention in widespread applications \cite{karapantelakis2024generative, zhou2024survey, raiaan2024review}. However, such models' safety, robustness and interpretability remain a pertinent issue \cite{liu2024large,mechergui2024goal}.
%TODO : (maybe add this back?)One such area of focus concerns itself with effective methods of poisoning model behaviors via fine-tuning on corrupted data  \cite{huang2020metapoison, he2024talk, carlini2023poisoning, shu2023exploitability}. While model poisoning via data corruption, data injection, and fine-tuning remains an active area of research, the mechanisms of such corruption remain elusive \cite{shu2023exploitability}.

Furthermore, mechanistic interpretability has garnered attention \cite{wang2022interpretability, zhong2024clock, conmy2023automated}. It concerns itself with reverse-engineering model weights into human interpretable mechanisms/algorithms \cite{Olah2022} by viewing models as computational graphs \cite{geiger2021causal} and analyzing subgraphs of the model with distinct functionality, called circuits \cite{elhage2021mathematical}. Through considerable manual effort and intuition, recent works have reverse-engineered mechanisms of transformer-based language models for specified tasks \cite{wang2022interpretability, hanna2024does, garcia2024does, lindner2024tracr, prakash2024fine}. 

Prior work \cite{prakash2024fine} has suggested that fine-tuning enhances the underlying mechanisms of the entity tracking task \cite{kim2023entity} when fine-tuning on code, mathematics, and instructions. In the following sections, we build upon prior work as one of our main contributions and extend the results to task-specific fine-tuning up to long training duration while providing the circuits formed across epochs and analyzing the changes in model mechanisms. 

%\textcolor{red}{TODO}: add some explanation about label poisoning and how this fits within that

With the recent improvements to language modeling, works have focused on the security issues posed by such models \cite{shu2023exploitability, carlini2023poisoning,he2024talk} focusing on designing model poisoning strategies to allow for efficient backdoors. Our work differs from such poisoning literature in that we aim to create data augmentations to fine-tune and corrupt specific mechanisms in the model akin to works focusing on label poisoning in training scenarios like \citet{huang2020metapoison}, \citet{pmlr-v202-wan23b} and \citet{geiping2020witches}, which aim to control model behavior via introducing poisoned data in training settings.
However, as such changes to model mechanisms remain a mystery in how they affect model behaviors, we take the case of the Indirect Object Identification task \cite{wang2022interpretability} and investigate the mechanism of corruption in models, utilizing several corrupted datasets. In addition, inspired by work done by \citet{lo2024large}, we find evidence of neuroplasticity from a mechanistic perspective in the models which relearn the task after fine-tuning the corrupted model on the correct dataset, highlighting the inherent inertia of pre-trained language models. % and discuss the mechanisms of relearning a task. 
\begin{figure*}[h]
    \centering
    \includegraphics[width = 0.8\textwidth]{latex/img/ca/epoch3a_circuit.pdf}
    \caption{The new circuit we discovered for task-specific fine-tuning at Epoch 3. The emerging, marked in \textcolor{blue}{blue}, circuit components formed performed similar mechanisms as the prior circuit components. }
    \label{fig:ca3-circuit}
\end{figure*}
Our key findings are: 
%\begin{itemize}[noitemsep, leftmargin=*]
\textbf{i)} Underlying mechanisms are \textbf{enhanced} across time, even for longer epochs, in task-specific fine-tuning, due to a specific mechanism, which, for the sake of brevity, we name: \textit{amplification}.
    \textbf{ii)} The mechanism of model poisoning via toxic fine-tuning is very \textbf{localized}, specifically corrupting the capacity of certain attention heads to perform their respective underlying mechanisms.
    \textbf{iii)} Models show the behavior of \textbf{neuroplasticity}, retrieving their original mechanisms after very few epochs of retraining on correct/clean datasets. The code is available on github\footnote{\url{https://github.com/osu-srml/neuro-amp-circuits}}.
