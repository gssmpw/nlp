\section{Phase Transitions via Fine-Tuning}
\textbf{Motivation}: Building on recent advances in mechanistic interpretability, such as \citet{zhong2024clock} and \citet{nanda2023progress}, which explore  phase transitions during grokking in toy models, our work aims to extend this understanding to fine-tuning. We focus on elucidating phase transitions in model mechanisms under various fine-tuning conditions. By leveraging insights into the model's existing mechanisms, we design corruption experiments that disrupt these mechanisms through targeted data augmentations. Our goal is to analyze how fine-tuning on corrupted/clean data reshapes model behavior, with the goal of a deeper understanding of fine-tuning dynamics in neural networks. 

In the following subsections, we discuss the effects of task-specific fine-tuning on the original "\textit{clean}" dataset, i.e, the IOI dataset, and discover \textit{Circuit Amplification} and the underlying mechanisms of the increased capabilities of the model to perform the underlying task. Furthermore, we discuss the effects of model poisoning on the underlying circuit of the model for the IOI task and discover that the underlying changes are localized to the circuit components of the model. Specifically, we analyze the effects of fine-tuning on the \textit{attention heads} in the original IOI circuit, as the MLP layers mechanisms do not change across time, see \autoref{app:mlp} for further explanation.
\label{methodology}
\subsection{Amplification Of Model Mechanisms}
\label{circuit-amp}
First, we study the effects of task-specific fine-tuning using the IOI dataset (clean dataset) on the model. We mechanistically interpret the change in the underlying mechanism. 
Consistent with expectations, our experiments uniformly demonstrate a significant boost in IOI task accuracy following the task-specific fine-tuning on the clean dataset, see \autoref{amp-table}. 
\begin{table}[H]
\vspace{0.1cm}
\caption{Performance, Faithfulness, and Sparsity of Discovered Circuits at Different Epochs compared to Model Performance}
\centering
\scalebox{0.65}{
\begin{tabular}{|l|l|l|l|l|l|}
\hline
Epoch &$F(Y)$&$F(C)$&Faithfulness&Sparsity&|$F(C)-F(C_{M_{GPT2}})$|\\
\hline
%\rowcolor[gray]{.95}
$1$ &   $6.32$    & $6.22$      &  $98.4\%$&   $1.92\%$    & $1.2$   \\ \hline
$3$ & $11.56$ & $11.50$ & $99.5\%$ & $1.95\%$ & $2.2$ \\ \hline%\rowcolor[gray]{.95}
$10$ &  $15.51$     & $15.26$      & $98.4\%$& $1.98\%$& $1.48$   \\\hline
$15$ &  $16.77$     & $16.73$      &   $99.7\%$&   $2.08\%$   & $0.91$  \\ \hline%\rowcolor[gray]{.95}
$25$ &  $19.47$     & $19.45$      &   $99.89\%$&  $2.25\%$& $0.37$  \\ \hline
$50$ &  $22.87$     & $22.75$      &  $99.7\%$& $2.41\%$        & $0.35$ \\ \hline%\rowcolor[gray]{.95}
$100$ & $26.83$      & $26.65$      &   $99.3\%$&$2.68\%$        & $0.41$ \\ \hline

\end{tabular}
\label{amp-table}
}
\end{table} 
%\begin{figure*}
%    \centering
%    \includegraphics[width = 0.8\textwidth]{latex/img/ca/epoch3a circuit.pdf}
%    \caption{The new circuit we discovered for task-specific fine-tuning at Epoch 3. The emerging, marked in \textcolor{blue}{blue}, circuit components formed performed similar mechanisms as the prior circuit components. }
%    \label{fig:ca3-circuit}
%\end{figure*}
We systematically analyze the circuits discovered at various epochs, assessing their faithfulness, performance, and sparsity. Our results show that the retrieved circuits exhibit high faithfulness and minimality scores, surpassing the original IOI circuit in both aspects. We provide a thorough account of our circuit discovery and evaluation results in the \autoref{app:circ_disc}, and in this section, we delve into the underlying mechanisms driving this performance enhancement.
Concurrently, we observe that task-specific fine-tuning enhances the underlying mechanisms of circuits without introducing novel mechanisms, even in longer training scenarios. The enhancement stems from two sources: (1) amplified capabilities of existing circuit components and (2) emergence of new components that replicate prior mechanisms. %Notably, fine-tuning solely augments the original mechanisms, increasing the number of contributing components and their individual strengths, without adding novel mechanisms. 
We term this phenomenon \textbf{Circuit Amplification}, and refer to the underlying mechanism as \textit{amplification}.
Our results, summarized in \autoref{amp-table}, reveal consistent Circuit Amplification in each epoch, note that in \autoref{amp-table}, $F(C_{M_{GPT2}})$ refers to the average logit difference when the original circuit is run on the fine-tuned model, so $|F(C)-F(C_{M_{GPT2}})|$ refers to the total contributions of the new circuit components to the average logit difference. Furthermore, we investigate the impact of fine-tuning on model components, including Negative Name Mover heads, which counterintuitively exhibit enhanced capabilities despite their negative contribution to the task. Notably, we do not observe the diminishing or disappearance of Negative Name Movers, see \autoref{fig:ca_logit_attribution}; instead, their abilities are enhanced. The IOI task circuit formed after 3 epochs of fine-tuning can be seen in \autoref{fig:ca3-circuit}. 

Intriguingly, we see \textit{Circuit Amplification}, even for \textbf{longer} training epochs. This seemed counter-intuitive as Negative Name Mover heads are amplified even after \textbf{longer periods of training}, hinting at their counter-factual importance to the task. Initial investigation by \cite{mcdougall2023copy} shows that these heads are a type of Copy Suppressor Heads and are key to the behavior of Self-Repair in language models \cite{rushing2024explorations}. These findings resonate with our result, as we see  these heads get amplified over time.\footnote{We further generalize the amplification results to the case of fine-tuning on general datasets, see \autoref{app:gen}.} 

\noindent\textbf{Mechanism of Enhancement: }Given the presence of Circuit Amplification, we now move to one of our key contributions, understanding how circuit amplification takes place. We \textbf{first} denote that, trivially, the increase in the number of components that replicate original mechanisms contributing to the task is one of the main contributors to circuit amplification, see \autoref{amp-table}. However, this doesn't fully explain the effect of circuit amplification, as the added components do not represent the complete change in the accuracy of the novel circuit when compared to the original circuit. \textbf{Secondly}, we record that the prior circuit components undergo an increase in capacity to perform their mechanism. To illustrate this point, we take the case of a Name Mover Head, specifically \textbf{L9H9} (Layer 9 Head 9) which gets amplified. 
\begin{figure}
\includegraphics[scale = 0.1,width=0.48\textwidth]{latex/img/ca/ca3-amp-attn.drawio-1-1.pdf}%
    \caption{\label{fig:ca3-amp-mech} Attention Probability vs Projection of head output along $W_U[IO]$ and $W_U[S]$ for head L9H9}%
\end{figure}
In \autoref{fig:ca3-amp-mech}, we plot the Attention Probability for IO (Indirect Object) and ``to'' token pairs vs Projection of Head output along $W_U[IO]$. This figure also includes the attention probability of S and ``to'' token pairs vs Projection of Head output along $W_U[S]$. We see that attention probabilities have significantly decreased for the S token for L9H9 after fine-tuning, suggesting a discriminant increase in the copying behavior of the IO token for L9H9 which is a finding that generalizes to other heads in the same category.
We further record this behavior in the case of Negative Name Mover Heads\footnote{See \autoref{app:ca} for further details}. %In addition to this, we note an amplification in the OV circuit for the aforementioned heads, i.e., the head writes more strongly to the residual stream. 
This implies that this head writes more strongly to the residual stream as the direct logit attribution\footnote{Logit attribution is mathematically defined in Section 3.1 of \cite{wang2022interpretability}.} of each head increases significantly when compared to the original model. This increase in the underlying capacity of the heads to perform their underlying behavior is \textit{amplification}, see \autoref{fig:ca_logit_attribution}. 
\begin{figure}
\centering
  \begin{subfigure}[t]{0.23\textwidth}
  \centering
    \includegraphics[scale = 0.1,  width=\textwidth]{latex/img/ca/change_in_logit_attribution.pdf}
    \caption{}
    \label{fig:ca_logit_attribution}
\end{subfigure}
\hfill
 \begin{subfigure}[t]{0.24\textwidth}
 \centering
    \includegraphics[scale = 0.1, width=\textwidth]{latex/img/ca/change_in_logit_absolut_amplified.pdf}
    \caption{}
\label{fig:ca_logit_attribution_absolute}
    \end{subfigure}
    \vspace{5mm}
    \caption{\subref{fig:ca_logit_attribution}) Logit Attribution  of heads L9H9, L11H10, L10H10 in  original/amplified model. \subref{fig:ca_logit_attribution_absolute}) Absolute Logit Difference in the original model vs amplified model after ablation}
\end{figure}
Finally, the \textbf{third} mechanism contributing to amplification is a change in the mechanism of some of the Backup Name Mover Heads to that of Name Mover Heads. We take the example of L10H10 and show that this head now performs the behaviors of Name Mover Heads after fine-tuning for 3 epochs, see \autoref{fig:ca3-amp-attn-bmnh} and \autoref{fig:ca_logit_attribution}.
\begin{figure}[t]
    \includegraphics[scale = 0.1,width=0.48\textwidth]{latex/img/appendix/ca3-amp-attn-bnmh.drawio.pdf}
    \caption{\label{fig:ca3-amp-attn-bmnh}:Attention Probability vs Projection of head output along $W_U[IO]$ and $W_U[S]$ for head L10H10 }
\end{figure}
In \autoref{fig:ca3-amp-attn-bmnh}, we see that the attention probability w.r.t the projection along the unembed of the IO and S token is similar to that of the original name mover heads, while seeing a significant increase in logit attribution, from $0.4$ to $1.8$ on the IOI task. We then ablate groups of heads in the original model and the fine-tuned model and measure the absolute change in the logit difference in their respective circuit's performance. As the number of model components performing the task increases, for a fair comparison, we only consider the heads in the original circuit for each group. \autoref{fig:ca_logit_attribution_absolute} shows that the ablating groups of heads in the fine-tuned model show a much higher change in performance indicating  the original groups surged in their capability to do their respective mechanisms. These findings generalize across epochs. \vspace{1mm}\\
\textbf{Analyzing Enhancement via Cross-Model Activation Patching: } We now analyze circuit amplification via Cross-Model Activation Patching \cite{prakash2024fine} and record that in task-specific fine-tuning, the amplification of the mechanism can be detected via Cross-Model Pattern Patching. That is, we patch in attention patterns of each head from the fine-tuned model into the original model and record the changes in the logit difference. We observe that each attention head in the original circuit has increased capability to perform its mechanism, see \autoref{fig:cross-model-ca}. 
\begin{figure}[t]
    \centering
    \includegraphics[scale = 0.1,width = 0.36\textwidth]{latex/img/ca/cross_model_pattern_ca.pdf}
    \caption{Cross Model Pattern Patching: Taking the attention pattern of the heads in the fine-tuned model and patching them into the original model results in an increase in the attention heads performance on the underlying task.} %This analysis extends to the attention heads that are added to the circuit via amplification}
    \label{fig:cross-model-ca}
\end{figure}
\subsection{Corruption of Model Mechanisms } 
\label{circuit-poisoning}
Given the knowledge of circuit amplification, we now aim to fine-tune the model with various corrupted augmentations of the IOI task and utilize path patching \cite{goldowsky2023localizing} and activation patching \cite{NEURIPS2020_92650b2e} to study the effects of corruption on the model mechanisms for the IOI task. Furthermore, we record the changes made to the original model circuit and investigate the mechanisms of corruption across different augmentations. We find that when fine-tuning on \textbf{Name Moving} and \textbf{Subject Duplication} datasets, the corruption can be traced back to changes in the original circuit, however, no noticeable change occurred when fine-tuning on the \textbf{Duplication} dataset, hence we leave the discussions to the \autoref{dadupe}. We discover  most of the mechanistic changes after toxic fine-tuning can be attributed to changes in the mechanisms of the circuit components, i.e, toxic fine-tuning {alters} the prior mechanisms of the circuits instead of introducing {new mechanisms} for suppressing performance on the task.  
\paragraph{Name Moving Dataset.} After fine-tuning, this dataset suppresses the output of the IO token. Notably, after 3 epochs, the output logits of multiple single-token names in the vocabulary converge to similar values, with a slight bias towards the IO token name, thereby preserving the IOI functionality, albeit with significant degradation. To illustrate, we take the prompt "After John and Mary went to the store, John gave milk to" and record the logits of the top 5 most likely tokens, see \autoref{table:logit_table}.
\begin{table}[h]
\centering
\small
\begin{tabular}{|c|c|c|}
\hline
\textbf{Logit} & \textbf{Token} \\ \hline
$21.70$  & Mary \\ \hline
$21.40$  & Elizabeth \\ \hline
$21.34 $  & Melissa \\ \hline
$21.24 $  & Christine \\ \hline
$21.08 $  & Stephanie \\ \hline
\end{tabular}
\caption{Logits of top 5 tokens after 3 epochs}
\label{table:logit_table}
\end{table}
However, this capability completely degrades over time, i.e, the bias towards the "IO" token is non-negligible. To elucidate the underlying mechanisms, we present a detailed analysis of the fine-tuning process with 3 epochs on the corrupted dataset in this section. 
Our investigation reveals that the model does \textbf{not introduce} novel mechanisms to mitigate performance on the task. Instead, it relies on diminishing/altering the capabilities of specific attention heads that underlie a task-related mechanism. Notably, the most affected components are the Name Mover Heads and  which completely lose their ability to copy the IO token ( \autoref{fig:cp3-attn-nmh}). 
\begin{figure}[htbp]
    \includegraphics[scale = 0.1,width=0.5\textwidth]{latex/img/cp/cp3-attn-nmh.drawio-1-1.pdf}
    \caption{\label{fig:cp3-attn-nmh}\textbf{Name Moving: }Attention Probability vs Projection of head output along $W_U[IO]$ and $W_U[S]$ for head L9H9}
  \end{figure}
We trace the source of this corruption to the S-Inhibition heads, which primarily suppress the queries of both the IO and S tokens. Consequently, the original circuit is fundamentally disrupted, with the Name Mover Heads losing their functionality and the S-Inhibition Heads altering their mechanism to suppress both tokens. This is evident in the QK matrix analysis of the S-Inhibition heads, which reveals a significant change in attention patterns, see \autoref{fig:cp3-token}.
\begin{figure}
\centering
  \begin{subfigure}[t]{0.24\textwidth}
  \centering
    \includegraphics[width =0.99\textwidth]{latex/img/cp/change_in_attn_pat-1.pdf}
    \caption{}
    \label{fig:cp3-token}
\end{subfigure}
\hfill
 \begin{subfigure}[t]{0.23\textwidth}
 \centering
    \includegraphics[scale = 0.1,width = \textwidth]{latex/img/cp/change_in_logit_percent_corrupt.pdf}
    \caption{}
    \label{fig:cp3-logit-percent}
    \end{subfigure}
    \vspace{5mm}
    \caption{\subref{fig:cp3-token}) Name Moving: the attention probability difference of S-Inhibition Heads on the \textcolor{green}{IO} and \textcolor{magenta}{S} token [\textit{Original - Corrupted}].
    \subref{fig:cp3-logit-percent}) Subject Duplication: Change in Logit Difference after ablating groups of heads.\\}
\end{figure}
We find that this mechanism of corruption extends to Backup Name Mover Heads and Negative Name Mover Heads see \autoref{app:cp} for further details. This hints that model poisoning, mechanistically, alters very localized model behaviors that affect the final output, instead of adding novel mechanisms to corrupt the model. This can also be seen via CMAP, see \autoref{app:cross}.\\ % as well, see appendix for further details. 
%Now we trace the information flow back from the S-Inhibition Heads to understand the affect of corruption on the prior heads and find that the functionality of the Induction Heads, Previous Token Heads and remain the same, hence we ask the question: \textit{What is affecting the queries of the S-Inhibition Heads?}. To answer this, employ Path Patching on query vector for the S-Inhibitions and find that  Induction Heads, Previous Token Heads, and Duplicate Token Heads don't write a strong enough signal to bias the queries of the S-Inhibition Head and hence, S-Inhibition Head attends strongly to both IO and S tokens, see \autoref{app:cross} for cross-model patching on the corrupted model and original model. 
This corruption mechanism induces phase transitions that disrupt the IOI task, as previously examined. In early epochs, the IOI capability remains but with significant degradation (see \autoref{table:logit_table}), resulting in correct outputs despite corrupted internal mechanisms. We hypothesize that leveraging the knowledge of pre-existing mechanisms could enable model poisoning attacks, selectively altering mechanisms while changing the distribution of the output significantly but compromising interpretability or introducing backdoor triggers. Future work exploring more defined attacks through fine-tuning would be an interesting direction.

\paragraph{Subject Duplication Dataset.}  Applying this data augmentation strategy and fine-tuning using the corrupted dataset results in rapid and significant degradation of model performance, the average logit difference goes from $3.55$ to $-11.06$ after just $5$ epochs.
Analysis reveals that the Name Mover Heads are most affected, exhibiting a modified attention pattern. This altered attention pattern yields a suppressed logit for the IO token and an enhanced logit for the S token, see \autoref{fig:ca5-sd-attn-nmh} for changes in attention probability for both IO and S token. From \autoref{fig:ca5-sd-attn-nmh} we can see that the projection of L9H9 in the unembedding space has significantly changed, now positively projecting the S token and negatively projecting the IO token. Surprisingly, the Negative Name Mover Heads undergo a similar change in functionality; they write in the opposite direction to the Name Mover Heads, which seems counter-intuitive as these components were suppressing the logit of the IO token, however after fine-tuning on the corrupted data imputation, these heads now suppress the logit of the S-token, see \autoref{fig:ca5-sd-attn-nmh} and \autoref{fig:cp3-attn-nnmh}. 
\begin{figure}
    \centering
    \includegraphics[width = 0.48\textwidth]{latex/img/cp/ca5-attn-sd-nmh.drawio-1-1.pdf}
    \caption{Attention Probability vs Projection of head output along $W_U[IO]$ and $W_U[S]$ for head L9H9}
    \label{fig:ca5-sd-attn-nmh}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[width=0.99\linewidth]{latex/img/appendix/ca5-attn-sd-nnmh.drawio.pdf}
    \caption{Attention Probability vs Projection of head output along $W_U[IO]$ and $W_U[S]$ for head L11H10}
    \label{fig:cp3-attn-nnmh}
\end{figure}
 Finally, we find that the mechanism of the S-Inhibition heads is mostly suppressed, even though they still bias the query of the Name Mover Heads and Negative Name Mover Heads, the impact of the bias is statistically insignificant when compared to the original circuit as after mean ablation their effect is insignificant in the corrupted model, see \autoref{fig:cp3-logit-percent}. Similar to the previous observation, the mechanism of corruption is very \textbf{local} to certain model components, however, unlike the prior case (Corrupted Dataset for Name Moving Behaviour), only the mechanism of the Name Mover Heads Negative Name Mover Heads is changed, while the mechanism of the S-Inhibition Heads (and other heads) is suppressed, see \autoref{fig:cp3-logit-percent} for their importance to the task in the corrupted model which we access via mean ablating groups of heads that are present in the circuit. \\
 In contrast to the \textbf{Name Moving} data augmentation, the phase transition in this case reveals an intriguing insight: Negative Name Mover Heads shift from suppressing the 'IO' token to suppressing the 'S' token, despite already being optimized for the task. This suggests that Name Mover Heads and Negative Name Mover Heads are intertwined, with one performing the inverse of the other for certain tasks. Further investigation into this "twinning" behavior and its occurrence in other tasks would be a promising direction for future research. 
 \paragraph{Analyses via Cross-Model Activation Patching:} Similar to prior experiments, we employ cross-model activation patching and replace attention patterns of each head with their patterns in the corrupted model fine-tuned on the Subject Duplication dataset. We observe that the effects of corruption are localized to the circuit, see \autoref{fig:cross-model-cp}, as the heads most affected in \autoref{fig:cross-model-cp} are the circuit components outlined in \autoref{fig:ca3-circuit}.
 \begin{figure}
     \centering
     \includegraphics[width = 0.36\textwidth]{latex/img/cp/cross_model_pattern_cp.pdf}
     \caption{Cross Model Pattern Patching: We find that effect of corruption is very localized to circuit components of the model, however few additional components arise, this is due to formation of repeated mechanisms via fine-tuning, see \autoref{app:cross} for further details}
     \label{fig:cross-model-cp}
 \end{figure}
 \begin{figure*}[h]
     \centering
     \includegraphics[scale =0.1,width = 0.8\textwidth]{latex/img/np/neuro-name-moving.drawio-2-1.pdf}
     \caption{Attention Probability vs Projection of head output along $W_U[IO]$ and $W_U[S]$ for head L9H9, corruption on \textbf{Name Moving} augmentation.}
     \label{fig:neuro-nmh}
 \end{figure*}
