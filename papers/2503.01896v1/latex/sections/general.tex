\section{Generalization to Other Circuits}
We extend our analyses to the \textbf{Greater-Than} circuit \cite{hanna2024does}. We find a similar pattern. The mechanisms of the greater-than task are \textit{amplified} after fine-tuning on task data. In contrast, the changes to the mechanisms of the model under toxic fine-tuning are primarily localized to circuit components leading to corruption of the task. Furthermore, we discover our finding of neuroplasticity to hold for the greater-than task, i.e., the model reverts back to its original mechanism after retraining the corrupted model on clean task-specific data. We detail our experiments on this task in \autoref{app:gt}.