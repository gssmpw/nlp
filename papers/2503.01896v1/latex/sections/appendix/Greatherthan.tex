\section{Greater-Than Task}
\label{app:gt}
The greater-than circuit \cite{hanna2024does} is a circuit for the greater-than year span prediction task for GPT2-small which can be defined as "The war lasted from the year 17XX to the year 17" and the model outputs any number (YY) greater than XX and less than 99. Complete details of the circuit can be found in \citet{hanna2024does}. As for the circuit discovery procedure we utilize Edge Attribution Patching with Integrated Gradient (EAP-IG), a novel automatic circuit discovery procedure introduced in \citet{hanna2024have}. As for evaluation, we utilize the probability difference between years greater than XX and years less than YY\footnote{This metric is defined on page 3 of \citet{hanna2024does}}.\\

\subsection{Amplification of Circuit}
We take the case of fine-tuning GPT-2-small on the task-specific greater-than data for 3 epochs. First, we present the discovered circuit, see \autoref{fig:gtcircuitclean}, and record that the circuit is similar to the original greater-than circuit presented in \citet{hanna2024does}. 
\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{latex/img/greaterthan/graph_clean_3.pdf}
    \caption{The circuit for the greater than task after fine-tuning for 3 epochs, attention head for layer 9 and head 1 is represented as a9.h1 and MLP of layer 11 is represented m11}
    \label{fig:gtcircuitclean}
\end{figure*}
This novel circuit itself performs as well as base GPT-2-small on the task, achieving a $84\%$ probability difference on the task while the full model achieves a  $95\%$ probability difference on the task. \\
As most circuit components are similar we can assess what makes the model perform better. This analysis is two-fold. We first utilize logit lens \cite{lesswrongInterpretingGPT} and attention pattern analysis to analyze the change in the mechanism of the relevant attention heads ( taking the example attention head L9H1). We then utilize logit lens to interpret the deviation from the original mechanism for the MLP that are important to the task ( taking the example of MLP 9). \\

\textbf{Amplification of the attention heads}: We first visualize the attention pattern of the relevant attention heads (taking the case of L9H1 for illustration) and notice that it is very similar patterns originally observed\footnote{see page 6 of \citet{hanna2024does}} by \citet{hanna2024does}, see \autoref{fig:gt_amp_attn91},i.e , the head attends strongly the to XX year for which the prediction has to be made. From this we can realize that there is no mechanistic change to the attention head given that it behaves similarly in that it writes to the final logit and influences MLP9 so, see \autoref{fig:gtcircuitclean}. 
\begin{figure}
    \centering
    \includegraphics[width=0.99\linewidth]{latex/img/greaterthan/attn91.pdf}
    \caption{Attenion for Head L9H1}
    \label{fig:gt_amp_attn91}
\end{figure}  
Now we utilize logit lens to visualize what the output of the attention head is writing to influence the final logit, see and find that it behaves similarly to what it did in the original model in that there is a majorly diagonal pattern to the logit lens similar to the observation\footnote{see Figure 7 of \citet{hanna2024does} } of \citet{hanna2024does}. \\
\begin{figure}
    \centering
    \includegraphics[width=0.99\linewidth]{latex/img/greaterthan/llattn91.pdf}
    \caption{Logit Lens of Head L9H1 showing a spike in the projection of the heads output in the unembedding space around the diagonal of the plot}
    \label{fig:enter-label}
\end{figure}
Furthermore, we also see report that the average magnitude of the diagonal year (i.e the same year as XX) in the unembedding space is $36.72$ in the fine-tuned model whereas it is $17.31$ in the original model this shows that output of the attention head to logit is \textbf{amplified.} This analysis extends to other heads in the circuit, as they have similar functionality. \\
\textbf{Amplification of the MLPs}: To see the amplification of the MLPs we take the case of MLP9 and use logit lens to visualize what it is writing to the logit and find that "upper-triangular" pattern as first shown by \citet{hanna2024does} holds true,see \autoref{fig:gtllmlp9}, furthermore there are differences up to the value of $140$ between some years higher than XX and lower than XX compared to the original model in which the differences can be up to $40$\footnote{see figure 8 of \citet{hanna2024does}}. This can generally be seen as the magnitudes of the years greater than XX are significantly higher than the base model, see \citet{hanna2024does} for reference. Indicating that the output of the MLPs is amplified while they retain the same mechanisms hence showing amplification. 
\begin{figure}
    \centering
    \includegraphics[width=0.99\linewidth]{latex/img/greaterthan/ampmlp9.pdf}
    \caption{Logit Lens of MLP 9}
    \label{fig:gtllmlp9}
\end{figure}

\subsection{Corrupting of Model Mechanisms}
\textbf{Corrupted Dataset: Lower Than}: For corruption, we aim to target the mechanism of the MLPs which makes them increase the projection of years greater than XX in unembedding space, so for this, we craft the Lower Than task which is grammatically incorrect but corrupts the mechanism of the MLPs.For this corruption we fine-tune the model by altering the year to be less than XX, for example, "The war lasted from the year 1713 to the year 17\textcolor{teal}{17}" becomes "The war lasted from the year 1713 to the year 17\textcolor{red}{12}". The main reason why we chose a grammatically incorrect task is to target the functionality of the MLPs. \\
\textbf{Mechanism of Corruption}: Firstly, we note that the model after toxic fine-tuning output years \textbf{less than} XX, the probability difference of $-97\%$ (the total probability of years after XX - the total probability of years before XX) after just 3 epochs of fine-tuning on the corrupted data. So the model's ability to perform greater-than year prediction is successfully corrupted. We now present the circuit that performs the new "lower-than" task, see \autoref{fig:gt_cp} and note that a majority of the attention heads are ablated from the circuit. With the attention heads that still remain show a similar attention head pattern to the original model, to illustrate we visualize the attention pattern of attention head L8H1 and notice it still strongly attends to the XX year, see \autoref{fig:attn81}. \\

\begin{figure}[t]
    \centering
    \includegraphics[width=0.99\linewidth]{latex/img/greaterthan/graph_corrupted_3-1.pdf}
    \caption{The circuit performing the "less than" task in the new circuit after fine-tuning model on corrupted dataset for 3 epochs}
    \label{fig:gt_cp}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.99\linewidth]{latex/img/greaterthan/attn81corr.pdf}
    \caption{Attention Patterns for head L8H1}
    \label{fig:attn81}
\end{figure}
Furthermore, we utilize logit lens, see \autoref{fig:llattn81} for L8H1 and notice that it shows a similar diagonal pattern and it's mechanism remains to be fairly similar. Effectively we see that a majority of heads that aided in the greater than task are ablated with no new addition of novel heads/mechanisms and hence can conclude that the effect of corruption is localized to the circuit components. 

\begin{figure}
    \centering
    \includegraphics[width=0.99\linewidth]{latex/img/greaterthan/llattn81.pdf}
    \caption{Logit Lens for head L8H1}
    \label{fig:llattn81}
\end{figure}

\textbf{Corruption of MLPs}: Given our analysis of attention heads and the knowledge that their effect is fairly negligible except for a few attentions head like L8H1 we move to analyze the effect of corruption on MLPs. We analyze the logit lens of MLP9 and discover that instead of having an "upper-triangular" pattern it now has a lower triangular and significantly favors the years less than XX. This explains the fact that the model now successfully predicts the years to be less than XX, and hence we trace back the most impactful source of corruption, see \autoref{fig:corrmlp9}. This finding generalizes to other MLPs as well. \\

\begin{figure}
    \centering
    \includegraphics[width=0.99\linewidth]{latex/img/greaterthan/corrmlp9.pdf}
    \caption{Logit Lens of MLP9}
    \label{fig:corrmlp9}
\end{figure}

Now given that a majority of the attention heads don't contribute much to the corrupted performance of the model(the ones that do are similar in their mechanisms to the original model) and that MLPs effectively "switch" their behavior from favoring years greater than XX to years less than XX, we conclude that the corruption is \textbf{localized} to the circuit components in the case of the "greater-than" circuit as well. 

\subsection{Neuroplasticity}
Similar to prior experiments in \autoref{main:neuro}, we retrain the model on the original greater-than dataset and find that the model relearns its original mechanism. Taking the case of retraining for 3 epochs this can be seen via the circuit formed for the task after retraining and its similarity to the original model, see \autoref{fig:gt_neuro}. The model now achieves a probability difference $94\%$ on the task while the circuit achieves $88\%$ of the total probability difference by itself.\\

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{latex/img/greaterthan/graph_neuro_3.pdf}
    \caption{Circuit formed for greater than task after retraining the corrupted model for 3 epochs on the original dataset.}
    \label{fig:gt_neuro}
\end{figure*}
\textbf{Neuroplasticity of Attention Heads}: We can see that the attention heads that were ablated are formed back, see attention head L9H1 in \autoref{fig:gt_neuro} and its lack thereof in \autoref{fig:gt_cp} for illustration. We discover that the mechanism of the original attentions has been relearned and take the case of L9H1 to analyze. We visualize the logit lens and attention patterns of L9H1 and record that it is similar to the amplified/original version with the attention pattern showing strong attention, see \autoref{fig:attn91neuro}, to XX and the logit lens showing a diagonal pattern, see \autoref{fig:llattn91neuro}.\\
\begin{figure}
    \centering
    \includegraphics[width=0.99\linewidth]{latex/img/greaterthan/attn91neuro.pdf}
    \caption{Attention Pattern of L9H1 after retraining on clean data}
    \label{fig:attn91neuro}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[width=0.99\linewidth]{latex/img/greaterthan/llattn91neuro.pdf}
    \caption{Logit Lens of L9H1 after retraining on clean data}
    \label{fig:llattn91neuro}
\end{figure}
\textbf{Neuroplasticity of MLPs}: We take the case of MLP9 and show that the MLP has regained its original functionality via visualizing the logit lens of MLP9, see \autoref{fig:neuromlp9}. We now record that that pattern is "upper-triangular" with the MLP's output strongly favoring years greater than XX and hence reverting back to its original mechanism. 
\begin{figure}
    \centering
    \includegraphics[width=0.99\linewidth]{latex/img/greaterthan/nueromlp9.pdf}
    \caption{Logit Lens of MLP9 after retraining on clean data}
    \label{fig:neuromlp9}
\end{figure}\\
Now given, that the attention heads have regained their importance and contribution to the circuit( \autoref{fig:attn91neuro,fig:llattn91neuro,fig:gt_neuro }) and that the MLPs have reverted to their original mechanisms, we claim that the model has regained it's functionality for the greater than task, similar to the IOI case, after fine-tuning the corrupted model on the clean data. 