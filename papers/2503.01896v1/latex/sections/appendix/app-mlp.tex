\section{Effect of MLP Across Epochs}
\label{app:mlp}
In the original work, \cite{wang2022interpretability}, MLP layers of GPT2-small do not individually contribute much to the task, except MLP layer 0, which is seen as an extended embedding \cite{wang2022interpretability}. We find this case to extend to the circuits we recover via fine-tuning on the original IOI dataset, furthermore, we do not record any major contribution of the MLP layers (except MLP layer 0) in the corruption of the IOI task after fine-tuning on corrupted data variants. \\
\textbf{Amplification}: Similar to the original model, we record that the MLP layers, except layer 0, have no statistically significant contribution to the IOI task even after undergoing task-specific fine-tuning on the clean dataset, see \autoref{fig:mlp-amp}.\\
\begin{figure}
    \centering
    \includegraphics[width=0.49\textwidth]{latex/img/appendix/mlp_amp.pdf}
    \caption{Logit Difference from patched MLP outputs on the model fine-tuned for 3 Epochs on the original dataset}
    \label{fig:mlp-amp}
\end{figure}
\textbf{Corruption}: We analyze the performance/contribution of the MLP Layers for the Subject Duplication Task and find that, similar to our prior analysis, the contribution of the MLPs remain minuscule even after fine-tuning on the corrupted data variants, see \autoref{fig:mlp-cp-sd}.
\begin{figure}
    \centering
    \includegraphics[width=0.49\textwidth]{latex/img/appendix/mlp-cp-sd.pdf}
    \caption{Logit Difference from patched MLP outputs on the model fine-tuned for 5 Epochs on the Subject Duplication Dataset}
    \label{fig:mlp-cp-sd}
\end{figure}
We also find that this analyses extends to the Name Moving data corruption as well, see \autoref{fig:mlp-cp-nm}.\\
\begin{figure}
    \centering
    \includegraphics[width=0.48\textwidth]{latex/img/appendix/mlp-cp-nm.pdf}
    \caption{Logit Difference from patched MLP outputs on the model fine-tuned for 3 Epochs on the Name Moving Corrupted Dataset}
    \label{fig:mlp-cp-nm}
\end{figure}
\textbf{Neuroplasticity}: In addition to the case of amplification and corruption we find that our prior analyses extends to the case of the circuits formed \textit{post-reversal}, see \autoref{fig:mlp-np-sd} and \autoref{fig:mlp-np-nm}.
\begin{figure}
    \centering
    \includegraphics[width=0.48\textwidth]{latex/img/appendix/mlp-np-sd.pdf}
    \caption{Logit Difference from patched MLP outputs on the model fine-tuned for 5 Epochs on the Subject Duplication Dataset and then fine-tuned on the original dataset for 5 epochs}
    \label{fig:mlp-np-sd}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[width=0.48\textwidth]{latex/img/appendix/mlp-np-nm.pdf}
    \caption{Logit Difference from patched MLP outputs on the model fine-tuned for 3 Epochs on the Name Moving Corruption Dataset and then fine-tuned on the original dataset for 3 epochs}
    \label{fig:mlp-np-nm}
\end{figure}
