\section{Generalized Fine-Tuning}
We fine-tune the model on the following datasets and report our findings: 
\begin{compactitem}
    \item \textbf{Dataset 1}: using Approximately 213,000 samples from TinyStories \cite{eldan2023tinystories} and our full IOI dataset, We fine-tune for 1 Epoch using the same hyper-parameters as mentioned in \autoref{app:fine}
    \item \textbf{Dataset 2}: using open-sourced model called GPT2-dolly which is instruction tuned on Dolly Dataset \cite{DatabricksBlog2023DollyV2}.
    \item \textbf{Dataset 3}: using open-sourced math\_gpt2, fine-tuned on Arxiv Math dataset .
    \item \textbf{Dataset 4}: using open-sourced GPT2-WikiText\cite{alon2022neuro} fine-tuned on WikiText dataset\cite{merity2016pointer}.
\end{compactitem}
\label{app:gen}
\begin{table}[H]
\begin{adjustbox}{width = \columnwidth,center}
\begin{tabular}{l|l|l|l|l|l}
\toprule
Model & $F(Y)$  & $F(C)$  & Faithfulness & Sparsity \\
\midrule
\rowcolor[gray]{.95}
$GPT2-Tiny/IOI$ &   $13.51$    & $13.19$      &  $97.6\%$&   $1.92\%$  \\ 
$GPT2-dolly$ & $5.39$ & $5.28$ & $98\%$ & $1.95\%$\\\rowcolor[gray]{.95}
$math\_gpt2$ &  $4.5$     & $4.36$      & $96.8\%$& $1.95\%$  \\
$GPT2-WikiText$ &  $3.46$     & $3.46$      &   $100\%$&   $1.92\%$    \\\rowcolor[gray]{.95}
 \bottomrule
\end{tabular}
\end{adjustbox}
\caption{The accuracy of the model, the circuit, faithfulness, and sparsity of the circuit discovered on various datasets/methods of fine-tuning.}
\end{table}