\section{Path Patching and Knockout}
\label{app_path}
\textbf{Path patching  } is a method to search the attention head which directly affect the model's logits \cite{goldowsky2023localizing}. This method is designed to differentiate indirect effect from direct effect. Path patching is a technique used to replace part of a model's forward pass with activations from a different input. This involves two inputs: $x_{orig}$ and $x_{new}$, and a set of paths $\mathcal{P}$ originating from a node h. The process begin by running a forward pass on $x_{orig}$. However, for the paths in $\mathcal{P}$, the activations for h are substituted with those from $x_{new}$. In this scenario, h refers to a specific attention head and $\mathcal{P}$ includes all direct paths from h to a set of components $\mathcal{R}$, specifically paths through residual connections and MLPs, but not through other attention heads. 


\textbf{Knockout} is a method which is designed for understanding the correspondence between the components of a model and human-understandable concepts \cite{wang2022interpretability}. This concept is based on the \textit{circuits} which views the model as a computation graph $M$. In the graph $M$, nodes are terms in its forward pass (neurons, attention heads, embeddings, etc.) and edges are the interactions between those terms (residual connections, attention, projections, etc.). The circuit $C$ is a subgraph of $M$ responsible for some behavior. For example, to implement the model's functionality as completely as possible. \textit{Knockout  } is designed to measure a sets of nodes whether it is deletable in the $M$. A knockout operation would remove a set of nodes $K$ in a computation graph $M$ with the goal of "turning off" nodes in $K$ but capturing all other computations in $M$.

Specifically, a knockout operation includes the following parts: the knockout will 'delete' each node in $K$ from $M$. The removal operation involves replacing the outputs of the corresponding nodes with their average activation value across some reference distribution. Using mean-ablations removes the information that varies in the reference distribution (e.g. the value of the name outputted by a head) but will preserve constant information(e.g. the fact that a head is outputting a name).