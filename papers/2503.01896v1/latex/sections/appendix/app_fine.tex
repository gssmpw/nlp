\section{Finetuning Experiments}
\label{app:fine}
In this section, we primarily report the hyper-parameter settings used during the model training process. To synchronize and compare the results of our experiments, we used the same learning rate and weight decay across circuit amplification, circuit poisoning, and neuroplasticity. The learning rate is 1e-5, and weight decay is 0.1, with batch-size = 10. We use the base Adam Optimizer from HuggingFace for finetuning. 

\textbf{Compute: } We utilize, Google Colab Pro+ A100 GPUs for fine-tuning experiments and V100 GPU for inference. \\
\textbf{Computational Budget: } We utilize 11 GPU hours for fine-tuning experiments and 50 GPU hours for inference experiments in total. \\
\textbf{Model Parameters: } GPT2-small \cite{radford2019language} has 80M parameters with 12 layers. 