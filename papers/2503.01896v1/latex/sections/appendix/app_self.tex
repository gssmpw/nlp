\section{Self-Repair in Neuroplasticity and Circuit Amplification}
\label{app:self_repair}
In addition to circuit amplification, we provide some initial investigations on self-repair in the models \textit{post-reversal} and after regular fine-tuning on the IOI dataset. In particular, we study the impact of finetuning and reversal on the self-repair of \textit{Copy Suppressor Heads}, i.e, Name Mover Heads/\vspace{1mm}\\  
\textbf{Metric for Measuring Self-Repair} 
We follow the work by \cite{rushing2024explorations} and quantify self-repair of an attention head in a  model as: 
\begin{align*}
    \Delta logit \approx  -DE_{head} + \textit{self repair}
\end{align*}
 , where, in the case of the IOI task, $\Delta logit$ refers to the change in logit difference between the IO token and the S pre-ablation and post-ablation of the attention head under scrutiny,  $DE_{head}$ refers to the direct effect of the attention head on the models performance. \vspace{1mm}\\
\textbf{Boomerang of Self-Repair}
We take the case of the attention head: \textbf{9.9} and report the effects of finetuning on the self-repair behavior for the head under scrutiny.\\
\begin{figure}
    \centering
    \includegraphics[width = 0.5\textwidth]{latex/img/np/self-pair-1.pdf}
    \caption{Self-Repair Enhancement over Time for L9H9}
    \label{fig:enter-label}
\end{figure}
We find that capacity of self-repair increases linearly with time until we see a phase shift in the self-repair behavior on the dataset. From this, we conclude that the capability of the model Self-Repair is also enhanced with fine-tuning, we hypothesize this is due to dropout and circuit amplification increasing the number of backup name mover heads over time, however, further investigations are required and would be interesting future work.