\section{Neuroplasticity in Model Mechanisms}
\label{main:neuro}
After corruption, we study relearning the IOI task via fine-tuning on the original dataset. We discover that the corrupted model can recover its performance and analyze the changes in mechanisms between the retrieved and original models. Focusing on the two data imputations, % (excluding Duplication Data Augmentation), 
we fine-tune the corrupted model using the original data and refer to the resulting model as the \textit{post-reversal} model.\vspace{1mm}\\
\textbf{Name Moving Dataset:} The \textit{post-reversal} model recovers its original performance and \textbf{recovers} the original circuit mechanisms. Moreover, the IOI task circuit mechanism is amplified compared to the original model. We trace the mechanism change from the corrupted to the \textit{post-reversal} model and find that the emergence of the prior mechanisms occurs, resulting in a circuit similar to the original model's \footnote{see \autoref{app:neuro} for the new circuit diagram and discussion on other heads}.
Taking the case of the Name Mover Head \textbf{L9H9}, we see the recovery (and amplification) of the original mechanism of the head in the \textit{post-reversal} model, see \autoref{fig:neuro-nmh}. Our analyses extend to the case of \textbf{Subject Duplication Dataset} and other heads, see \autoref{app:neuro} for details. This suggests that one possible defense against data poisoning attacks can be fine-tuning on the clean dataset. %Further experiment is needed to confirm this observation on other tasks.