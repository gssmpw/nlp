\section{Conclusion}
This work takes the case of IOI task on GPT2-small and analyzes the changes in its mechanism under task-specific and toxic fine tuning.% and show behaviors of neuroplasticity when fine-tuning the corrupted model on clean data. 
Our findings suggest that 1) Model mechanisms are amplified during task-specific fine-tuning 2) Fine-Tuning on corrupted data leads to localized changes in model mechanisms  3) Models show behaviors of neuroplasticity when retraining on the original dataset.
\section{Limitations} Our work focuses on a specific architecture and two task on it. Additional work is needed to scale/generalize our results for other architectures/tasks. As the primary bottleneck of mechanistic interpretability research is scalable, robust, and effective methods to understand underlying mechanisms, we believe work in that direction would significantly aid in scaling our findings to more generalized settings used in real-world tasks.
\section{Broader Impact}
We believe mechanistic interpretability techniques can alleviate many AI safety concerns and assist in creating safe and reliable AI systems. However, as our work highlights, interpretability techniques can be utilized to develop exploits in regards to jailbreaking and model poisoning, however, given the presence of neuroplasticity, we believe significant future work can be done to alleviate such drawbacks. Overall, we believe that approaching AI safety problems with a mechanistic approach can lead to interesting findings that might aid in creating safer AI systems. 

