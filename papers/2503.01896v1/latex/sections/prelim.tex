\section{Preliminaries}
\textbf{Indirect Object Identification (IOI): } The IOI task involves identifying the indirect object in a sentence. For example: \textit{"When Mark and Rebecca went to the garden, Mark gave flowers to"}. The task involves two clauses with single-token names. The first clause contains the subject (S1) and indirect object (IO) tokens, while the second clause contains the second occurrence of the subject (S2) and ends with "to". The goal is to complete the second clause with the IO token, which is the non-repeated name \cite{wang2022interpretability}, see \autoref{app:circ_eval} for the original circuit diagram.
The circuit that implements the task contains multiple underlying mechanisms described as follows:
\begin{enumerate}
    \item \textbf{Name Mover Heads attend} to the previous names in the sentence, meaning the ``to'' token attends primarily to the IO token and less to the S1 and S2 tokens. They primarily copy the IO token and increase its logit. 
    \item \textbf{Negative Name Mover Heads} attend to the previous names in the sentence, their mechanism is suppressing the IO token (i.e., decreasing the logit of the IO token) and writing to the opposite direction of Name Mover Heads. 
    \item \textbf{S-Inhibition Heads }attend to the second copy of the subject token, S2, and bias the query of the Name Mover Heads against S1 and S2 tokens.
    \item  \textbf{Duplicate Token Heads} identify tokens that already appeared in the sentence, being active at the S2 token and attending primarily to the S1 token.
    \item  \textbf{Previous Token Heads} copy the embedding of S to the position of S + 1.
    \item \textbf{Induction Heads} perform the same  as  Duplicate Token Heads, but via an induction mechanism. 
    \item \textbf{Backup Name Mover Heads} are the heads that perform the mechanism of the Name Mover Heads if they are ablated. 
\end{enumerate}

\paragraph{Path Patching and Knockout} were used to identify and evaluate crucial model components, see \autoref{app_path} and \autoref{app:circ_disc} for further details on the circuit discovery procedure\cite{goldowsky2023localizing}.
\paragraph{Cross Model Activation Patching (CMAP): } involves activation patching \cite{zhang2023towards,goldowsky2023localizing} across different models on the same input \cite{prakash2024fine}. While vanilla activation patching replaces components within the same model using different inputs, cross-model activation patching involves using the same input across different models, replacing the corresponding components to observe the differences in output. 
\paragraph{Neuroplasticity:} In machine learning, neuroplasticity, refers to the ability of the model to adapt and regain conceptual representations \cite{lo2024large}. We extend this definition to include the ability of a model to relearn corrupted concepts/mechanisms. 
%The phenomenon where the removal of neurons within a circuit does not cause performance degradation is known as neuroplasticity. This effect may occur following random pruning or the pruning of important neurons. This suggests that even if certain concepts are eliminated through model editing, the model can still relearn these concepts through retraining.