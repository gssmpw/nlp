
\section{Preliminary}
We briefly review related research on flow matching and LMs, providing foundations for introducing our method.

\subsection{Rectified Flow}\label{sec:rw_rf}

Rectified flow~\cite{liu2022flow,albergo2023building} emerges as a robust and powerful generative model and has recently served as the basis for popular commercial tools like Stable Diffusion 3~\cite{stabilityAI2023}. It is based on flow matching~\cite{chen2018neural,lipman2022flow}, which models the generative process as an Ordinary Differential Equation (ODE).  Formally, a \emph{continuous normalizing flow} transports an input $\vz_0\in \mathbb{R}^d$ to $\vz_t=\phi(t, \vz_0)$ at time $t\in[0,1]$ via the ODE:
\begin{align}
    \frac{d}{dt}\phi(t, \vz_0) = \varphi\left(t, \phi(t, \vz_0)\right).
\end{align}
Here, $\phi:[0,1]\times \mathbb{R}^d\rightarrow\mathbb{R}^d$ is the \emph{flow}, and the \emph{vector field} $\varphi: [0,1]\times \mathbb{R}^d\rightarrow \mathbb{R}^d$ specifies the change rate of the state $\vz_t$. \citet{chen2018neural} suggests representing the vector field $\varphi$ with a neural network.

The flow $\phi$ transforms an initial random variable $Z_0\sim p_0(\vz_0)$ to $Z_1\sim p_1(\vz_1)$ at final time 1. Rectified flow tries to drive the flow to follow the linear path in the direction $(Z_1-Z_0)$ as much as possible:
\begin{align}
    \min_\varphi \int_0^1 \mathbb{E}\left[\|(Z_1-Z_0)-\varphi(t, Z_t)\|^2\right]dt,\label{equ:rf}
\end{align}
where $Z_t=t\cdot Z_1 + (1-t)\cdot Z_0$ is the linear interpolation of $Z_0$ and $Z_1$. Typically, the vector field network $\varphi$ is implemented as a U-Net~\citep{ronneberger2015unet} for image inputs or an MLP for vector inputs~\cite{wang2024diffusion}.

\subsection{Transformer} 
The Transformer architecture~\citep{vaswani2017attention} is foundational to recent progress in large language models (LLMs)~\citep{liu2024deepseek, zeng2024skywork,yang2024qwen2,team2023gemini}. For an input sequence of tokens $x = (\idx[\vx][][1], \dots, \idx[\vx][][N])$, let \( \idx[E][][n][] = [e(\idx[\vx][][1]), \dots, e(\idx[\vx][][n])] \) denote the sequence of token embeddings up to position $n$, where $e(\cdot)$ is the token embedding function. A standard LLM generates its output by
\begin{align}
&\idx[\mH][][n]=\textsc{Transformer}\left(\idx[E][][n]\right),\nonumber \\
&M\left(\idx[\vy][][n+1] \mid \idx[\vx][][\leq n]\right)=\mW \idx[\vh][][n],\label{equ:token_logits}
\end{align}
where $\idx[\mH][][n] \in \mathbb{R}^{n \times d}$ is the last hidden state for the first $n$ tokens, with $d$ representing the hidden dimension. $\idx[\vh][][n]$ is the last hidden state at position $n$, i.e., $\idx[\vh][][n] = \idx[\mH][][n][] [n, :]$. $\mW$ is the output projection matrix, $M$ is the model's generation logits, and $\vy$ is the output token.

We consider an LLM with $L$ layers, the hidden state after $l$ layers, $\idx[\mH][][n,l]$, is projected by three weight matrices \(\mW_Q \), \(\mW_K \), and \(\mW_V \) to the query, key, and value embeddings $\idx[\mQ][][n,l]$, $\idx[\mK][][n,l]$, and $\idx[\mV][][n,l]$, respectively. The self-attention is calculated as:
\begin{align}
&(\idx[\mQ][][n,l], \idx[\mK][][n,l], \idx[\mV][][n,l]) = \idx[\mH][][n,l] (\mW_Q, \mW_K, \mW_V) \nonumber\\
&\idx[\mA][][n,l] = \frac{\idx[\mQ][][n,l] {\idx[\mK][][n,l]}^\top}{\sqrt{d_K}}, \text{Attn}(\idx[\mH][][n,l]) = \sigma(\idx[\mA][][n,l]) \idx[\mV][][n,l],    \nonumber
\end{align}
where $\sigma(\cdot)$ is SoftMax, and $\mA$ is the self-attention matrix. We omit the multi-head attention for simplicity.


\subsection{RLHF}
Reinforcement learning from human feedback~($\mathtt{RLHF}$)~\citep{bai2022training, wang2023helpsteer, ouyang2022training, dong2024rlhf} is critical to aligning LLM behavior with human preferences such as helpfulness, harmlessness, and honesty~\citep{ganguli2022red, achiam2023gpt, team2023gemini}. An RL-based method trains a reward model~\citep{liu2024skywork} to approximate human preferences. Given a preference dataset $\mathcal{D} = (x, y_w, y_l)$, where $x$ is the input, $y_w$ is the preferred output, and $y_l$ is the less preferred output, a reward model $r_\theta$ can be trained using the standard Bradley-Terry model~\citep{bradley1952rank} with a pairwise ranking loss. With $r_\theta$, the policy model (LLM) is trained via $\mathtt{PPO}$~\citep{schulman2017proximal}.
% with the following objective:
% \begin{equation}
%     \max_{\pi_\theta} \mathbb{E}_{x\sim\mathcal{D},y \sim \pi_\theta(\cdot|x)} \left[r_{\theta}(x, y)\right] - \beta \mathbb{D}_\text{KL}\left[\pi_\theta(\cdot|x) || \pi_{\text{ref}}(\cdot|x)\right].\nonumber
% \end{equation}
% Here, the KL divergence penalty is applied to prevent excessive deviation from the reference model $\pi_{\text{ref}}$, \ie, the initial supervised fine-tuned (SFT) model.
However, training a reward model can be costly. Direct preference learning ($\mathtt{DPO}$) \cite{rafailov2024direct} enables direct training with preference data, which can be adapted to different human utility models ($\mathtt{KTO}$,~\citet{ethayarajh2024kto}).
% ~(\eg, $\mathtt{KTO}$~\cite{ethayarajh2024kto})
%shows that it is possible to directly train with the preference data

% DPO optimizes the following loss function to train the policy model (LLM) $\pi_\theta$:
% \begin{align}
%     \mathcal{L}_{\text{DPO}}(\theta) = \mathbb{E}_{\mathcal{D}}
%     \left[\shortn\log \sigma\left(\beta \log \frac{\pi_\theta(y_w|x)}{\pi_{\text{ref}}(y_w|x)} \shortn \beta \log \frac{\pi_\theta(y_l|x)}{\pi_{\text{ref}}(y_l|x)}\right)\right],\nonumber
% \end{align}
% where $\beta$ is a parameter controlling the deviation from the supervised fine-tuned model $\pi_{\text{ref}}$.




% At the ODE time $t$, the hidden state after the $m$-th layer corresponding to the input $(t, \vz_t)$ is denoted by $\idx[\vh][f][t,m](\vz_t)$.
% \begin{equation}
%     \mathcal{L}_{\text{reward}}(\theta) = -\log \left(\sigma\left(r_\theta\left(x, y_w\right) - r_\theta\left(x, y_l\right)\right)\right),
% \end{equation}
%where $\sigma$ is the logistic function.
% to guide LLMs toward desired behaviors
% leverage recent advancements in reinforcement learning (e.g., PPO~\citep{schulman2017proximal}) to enhance the alignment of LLMs~\citep{achiam2023gpt}. 
% A key component of these methods is the development of , which learn a reward function based on 
% Instead, DPO derives a reward signal directly from the currently optimized model and an initial supervised fine-tuned model \citep{rafailov2024r}, effectively reparameterizing preference learning within the model itself.