\section{Related Works}
\textbf{LLM explanations}. 
Previous work leveraging LLMs to generate explanations can be categorized into two approaches. For post-hoc natural language explanations, methods such as AMPLIFY____, Self-Explain____, and Summarize and Score (SASC)____ generate concise rationales based on agent decisions, sometimes accompanied by an explanation score to assess reliability. For ad-hoc methods, Chain-of-Thought (CoT) prompting____ is a widely adopted in-context learning technique that relies on step-by-step explanations or reasoning to enhance decision-making. Self-Taught Reasoner (STaR)____ introduces an iterative refinement method, where a model improves its own explanations through self-generated rationales. While these methods are prompting-based and do not require additional training, optimization-based CoT methods like ReFT____ have has been developed. Our method falls within the domain of ad-hoc natural language explanations. Without knowing agent decisions, we train an LLM to generate informative and reliable explanations using a generative flow matching model. We compare against CoT and ReFT in our experiments. 
% demonstrating the potential of self-improvement for enhancing explanation quality

% which seeks to provide insights into the causes of model decisions and make them understandable for humans
% While much of the research in this field has focused on supervised learning,
\textbf{Explainable AI}. Our method is suited within  the domain of explainable AI____ and draws particular parallels with explainable RL (XRL). Post-hoc XRL methods focus on relating inputs and outputs of a trained RL policy in an interpretable way, using an interpretable \emph{surrogate} model as policy approximation. Examples of surrogate models include imitation learning____, learning from demonstration____, and finite state machines____. However, in order to be interpretable, surrogate models are designed as simple as possible. More related works are in Appendix~\ref{appx:related_work}.

Generating natural language explanations for RL models is appealing, but previous work mainly focuses on specific scenarios like self-driving____, recommender systems____, stock prediction____, robotics____, autonomous navigation____, and network slicing____, leaving a general policy-to-language method underexplored. 

\textbf{Diffusion in Transformer} (DiT,____) leverages the strengths of self-attention of Transformers to improve the performance of diffusion models across a range of tasks, including image and text generation____. ____ demonstrate how Transformer-based architectures can optimize the denoising process in diffusion models, resulting in high-quality image synthesis. ____ explore efficient implementations for diffusion within Transformer. These works are related to our work, as we embed flow matching into the last layer of an LLM. \textbf{Cross-attention} is a popular technique for processing information across multiple modalities____. Approaches such as T2I-Adapter____ and VMix____ use cross-attention mechanisms between text encoders (an LLM) and diffusion models to enhance the generation of high-quality images from textual descriptions. More generally, cross-attention has helped solve tasks that require both vision and language understanding____.  Different from previous work on DiT and cross-attention-based image/video generation, to our best knowledge, the proposed method is the first to use generative models and cross-attention to generate rewards for RL-based LLM training.

% They make decision trees differentiable by replacing the Boolean decisions with sigmoid activation functions.  represented by tree nodes~ are developed to iConsequently, the representational capacity of these models typically cannot support them to interpretable all the decisions made by the original model. However, letting RL agents explain their actions in natural language is challenging because of the simultaneous learning of policy and language, and the alignment between them. ____ and____ solve this problem by proposing a supervised learning framework using action-explanation pairs annotated by humans. However, since the explanations are provided by humans, these methods are actually learning how humans perceive.
% Diffusion models____ have recently emerged as a powerful class of generative AI methods, spurring notable advances in a wide range of tasks such as image generation____, video generation____, molecular design____, and text generation____. At their core, these models perform a forward noising process in which noise is incrementally added to training data over multiple steps, gradually corrupting the original samples. Then, a reverse diffusion process is learned to iteratively remove noise, thereby reconstructing data from near-random initial states. 

% \subsection{Generative Models by Flow Matching}

% Flow matching____ is related to score-based diffusion models____ and enjoys solid mathematical underpinnings. This line of research leverages \emph{continuous normalizing flows} to transform a simple distribution such as Gaussian noise to a complex distribution such as those of natural images. A bottleneck that restricts the use of continuous normalizing flow in large-scale problems is that the ODE is hard to solve. The {\em rectified flow}____ simplifies the ODE by encouraging vector fields to be represented by straight lines.
% is an innovative approach that uses Transformer architectures  diffusion models with . The core idea is to

%