\section{Appendix}
\subsection{More Related Works}\label{appx:related_work}

% Previous work leveraging LLMs to generate explanations can also be categorized into two types. For post-hoc natural language explanations, methods such as AMPLIFY\cite{krishna2024post}, Self-Explain\cite{rajagopal2021selfexplain}, and Summarize and Score (SASC)~\cite{singh2023explaining} generate concise rationales based on a model's response, sometimes accompanied by an explanation score to assess reliability. For ad-hoc methods, Chain-of-Thought (CoT) prompting~\cite{wei2022chain} has emerged as a widely used in-context learning technique that depends on step-by-step explanations. Additionally, Self-Taught Reasoner (STaR)~\cite{zelikman2022star} introduces an iterative refinement method, where a model improves its own explanations through self-generated rationales, demonstrating the potential of self-improvement for enhancing explanation quality. These methods are prompting-based and do not involve training. There are also optimization-based CoT method like ReFT~\cite{trung2024reft}. We compare against CoT and ReFT in our experiments. Our method falls within the domain of ad-hoc natural language explanations, where we train language models to generate more informative and reliable explanations for decision prediction.

\textbf{Explainable RL}. Ad-hoc XRL methods represent policies as inherently interpretable models. For example, \citet{silva2019optimization,topin2019generation,hein2018interpretable, landajuela2021discovering} use decision trees as policy approximators. However, the capacity of these models is typically limited. Saliency maps distinguish observation elements that influence decisions~\cite{atrey2019exploratory,greydanus2018visualizing,gottesman2020interpretable}, but does not capture the reasoning behind decisions~\cite{atrey2019exploratory}, leaving humans to give ad-hoc explanations based on these visual cues. In multi-agent settings~\cite{yu2022surprising,wen2022multi,kuba2021trust,wang2019influence,christianos2020shared,peng2021facmac,jiang2019graph,wen2022multi,rashid2018qmix,wang2021off,guestrin2002coordinated, guestrin2002multiagent,li2023never,song2019convergence,wang2019influence,wang2020dop,qin2024multi,wu2021containerized,zhang2024position}, auxiliary learning objectives like role-based labor division~\citep{wang2020roma, wang2021rode,dong2022low,dong2023symmetry}, communication learning \citep{singh2019learning, mao2020neighborhood,wang2019learning,zhao2024bandit}, diversity~\citep{li2021celebrating}, coordination graphs \citep{guestrin2002coordinated, guestrin2002multiagent, bohmer2020deep,kang2022non,wang2021context,yang2022self}, mechanism design~\citep{sandholm2003automated,duan2024scalable,wang2024carbon,sun2024mechanism,shen2018automated,dutting2024optimal,wang2024gemnet,hossain2024multi,wang2024deep} offer explanations in various forms, but these explanations are typically more indirect compared to those expressed in natural language.


Post-hoc XRL methods also use surrogate models based on decision trees and their variants~\cite{bastani2018verifiable,jhunjhunwala2019policy,bewley2021tripletree,liu2018toward}, genetic programming~\cite{zhang2020interpretable}, programmatic policy searching~\cite{verma2018programmatically}, and deterministic finite automata~\cite{hasanbeig2021deepsynth}.

Example-based methods use similar experiences to explain decisions~\cite{amir2018highlights,huang2018establishing,zahavy2016graying,topin2019generation}, but largely overlook underlying reasoning.

\textbf{Use explanations to tain LLMs}. Various prior works have explored training or tuning language models with explanatory cues, often relying on techniques such as span or word highlighting~\cite{hase2021can,zhou2020towards,narang2020wt5,rajani2019explain}. Moreover, explicitly training language models with natural language explanations or instructions has been shown to enhance their ability to leverage explanations in prompts~\cite{wei2021finetuned}. These works are different from our method, because we do not use explanations to train LLMs. Instead, we train LLMs to generate explanations.

\subsection{Prompts}\label{appx:prompt}
Below are the prompts used for the \elm~and the \rlm~on the $\mathtt{SMAC}$ datasets. For the \elm, the prompt describes the overview of the scenarios along with the previous action-state pairs and instructs the \elm~to analyze the provided information. In response, the \elm~generates a summary of the preceding trajectories and implies the action selection. For the \rlm, the prompt includes a statement summarizing both the input to and output from the \elm, followed by the phrase: \emph{'Therefore, the missing action is {a possible action from $\mathcal{A}$}'}. Then, we calculate the mean token logits for each possible action in $\mathcal{A}$, which are then used to determine the action with the highest likelihood.


\input{table/reasoning_flow_smac}

Below are the prompts used for the \elm~and the \rlm~on the $\mathtt{MMLU}$ and $\mathtt{MathQA}$ datasets. For the \elm, the user provides the question along with the possible answer options and requests the \elm~to explain the reasoning process without explicitly revealing the selection. In response, the \elm generates an explanation that analyzes the problem and implicitly suggests an answer. For the \rlm, the user presents a statement that includes both the input to and the output from the \elm, followed by the phrase: \emph{'Therefore, the correct answer is {a possible answer from $\mathcal{A}$}'}. to calculate the mean token logits of the given possible answer. Then, we calculate the mean token logits for each possible answer in $\mathcal{A}$, which are then used to determine the answer with the highest likelihood.

\input{table/reasoning_flow_mmlu}
\input{table/reasoning_flow_mathqa}

\subsection{Additional Experimental Setup}\label{appx:experimental_setup}

\textbf{$\mathtt{SMAC}$}. The trajectories in the $\mathtt{SMAC}$ dataset are collected from a trained RL policy. Specifically, we first train an RL policy using MAPPO on $\mathtt{SMAC}$ and then collect the action-state pairs at each timestep during the evaluation phase. Each action-state pair contains information about the current agent's position, health, and weapon cooldown, as well as the corresponding details for visible allies and enemies. Additionally, the action selected by the current agent is included. All action-state pairs are stored in JSON format:


\emph{$\begin{aligned}State:
&\left\{visible \: allies: \left\{relative \left(x,y\right); weapon\_cooldown; health\right\}\right.\\
&\left\{id 0:\left[ 0.0765, -0.0765\right]; 0.0; 1.0\right\}; \left\{id 1:\left[0.0765, 0.0\right]; 0.0; 1.0\right\}; \\
&\left\{id 2:\left[0.0765, 0.0765\right]; 0.0; 1.0\right\}; \left\{id 3:\left[0.153, 0.0\right]; 0.0; 1.0\right\}; \\
&None \: visible \: enemies; own\_health: 1.0\left.\right\}; Action: SOUTH
\end{aligned}$}

We feed the \elm~with 4 consecutive action-state pairs, masking the action in the final pair, and then instruct the \elm to analyze the provided information and infer the missing action.

\textbf{Training Details}. We train the \elm~and the rectified flow model $\varphi$ iteratively. In the first step, \elm~generates an explanation for each sample 3 times in the training set. The \rlm~then classifies these explanations as positive or negative. We use the positive samples to train the rectified flow model $\varphi$. Once trained, $\varphi$ is employed in subsequent the \elm~training. This entire process constitutes one round, and we perform two rounds of training. Note that the training process can be adapted for online learning, where the \elm~and the rectified flow model $\varphi$ are trained simultaneously. During training, \elm~uses BF16 and LoRA with hyperparameters $lora_r=16$ and $lora_alpha=16$, while DeepSpeed ZeRO-3 is employed to accelerate training. For \elm~training, each GPU processes at most 1 sample at a time, with gradient accumulation over 3 steps. For the rectified flow model $\varphi$, each GPU processes at most 32 samples at a time.


\textbf{AUC}. We calculate the AUC by computing the probability that the score of positive samples exceeds the score of negative samples. To classify the ground-truth positive and negative samples, we use o1-mini to identify the correctness of the explanation generated by the \elm. Subsequently, we employ the \rlm~to assign a score to each explanation generated by the \elm. 


\subsection{Baselines}\label{appx: baselines}


\textbf{$\mathtt{SFT}$} is a widely used approach to fine-tune LLMs for specific downstream tasks by training them on labeled datasets. In this process, a pre-trained LLM is exposed to task-specific data containing input-output pairs. The goal is to minimize the difference between the model’s predictions and the ground truth, typically using a supervised learning objective like cross-entropy loss. In our work, we construct an $\mathtt{SFT}$ dataset comprising samples paired with explanations generated by OpenAI o1-mini. The training epochs of $\mathtt{SFT}$ is 3 and the learning rate is initialized as $2e\shortn 5$ recommended by the TRL library and then decays linearly to zero.


\textbf{$\mathtt{PPO}$} is a reinforcement learning algorithm commonly used to fine-tune LLMs, enhancing their alignment with specific goals or human preferences. $\mathtt{PPO}$ fine-tunes a pre-trained model by optimizing its policy based on feedback signals. These signals typically come from a reward model, which is trained on labeled data that reflects desirable outputs. $\mathtt{PPO}$ adjusts the model’s parameters to maximize expected reward while constraining updates within a predefined range, avoiding large deviations that could destabilize training. We construct a preference dataset comprising samples paired with explanations generated by OpenAI o1-mini. The training epochs of the reward model and $\mathtt{PPO}$ are both 10 and the learning rate is initialized as $3e\shortn 6$ recommended by the TRL library and then decays linearly to zero.



\textbf{$\mathtt{DPO}$} is a technique for aligning LLMs with human preferences by directly optimizing their outputs using labeled preference data. 
Unlike traditional reinforcement learning from human feedback, which relies on a reward model to evaluate responses, $\mathtt{DPO}$ simplifies the alignment process by directly using preference comparisons to guide optimization. In $\mathtt{DPO}$, the labeled data consists of paired responses where one option is preferred over the other. The model learns to produce outputs that align with these preferences by optimizing a contrastive objective. In our work, we construct a preference dataset comprising samples paired with explanations generated by OpenAI o1-mini. The training epochs of $\mathtt{DPO}$ is 10 and the learning rate is initialized as $5e\shortn 6$ recommended by the TRL library and then decays linearly to zero.


\textbf{$\mathtt{KTO}$} is an advanced approach for aligning LLMs with human preferences or specific task objectives. It draws inspiration from prospect theory, a behavioral economics framework that models how humans evaluate potential gains and losses under uncertainty. In this context, $\mathtt{KTO}$ optimizes the alignment process by weighting outputs based on their perceived utility, rather than treating all errors equally. The core idea of $\mathtt{KTO}$ is to model alignment as an optimization problem where the goal is to maximize expected utility under a prospect-theoretic framework. In our work, we construct a preference dataset comprising samples paired with explanations generated by OpenAI o1-mini. The training epochs of $\mathtt{KTO}$ is 10 and the learning rate is initialized as $5e\shortn 7$ recommended by the TRL library and then decays linearly to zero.


\textbf{$\mathtt{SFT}$-$\mathtt{CoT}$} is a fine-tuning method that enhances the reasoning capabilities of LLMs by combining $\mathtt{SFT}$ with the structured reasoning paradigm. CoT uses explicit programmatic representations, such as pseudo-code or structured logic, to model complex problem-solving tasks. In this approach, $\mathtt{SFT}$ is performed using datasets annotated with both input-output pairs and detailed programmatic reasoning traces. These traces serve as templates for step-by-step reasoning and enable the model to break down complex problems, such as mathematical reasoning or logical inference, into manageable sub-tasks. The explicit program-like structure helps the model perform multi-step computations and enhances interpretability, making it especially useful for domains requiring precision and transparency.




\textbf{$\mathtt{ReFT}$} is a training approach designed to enhance the reasoning capabilities of LLMs by combining supervised fine-tuning ($\mathtt{SFT}$) with reinforcement learning. In $\mathtt{ReFT}$, the initial training begins with $\mathtt{SFT}$, where the model is fine-tuned using datasets annotated with reasoning traces, such as step-by-step explanations or logical chains of thought. Once the model achieves a baseline performance, reinforcement learning is applied to further refine its reasoning capabilities. 

\subsection{Additional Results}\label{appx:results}

\input{table/prompting}

\input{table/reasoning_example_smac_1}


\textbf{$\mathtt{SMAC}$}. To showcase the effectiveness of our method, we visualize an example on $\mathtt{SMAC}$ that a negative explanation classified by \rlm~is corrected by the rectified flow model $\varphi$. As shown above, this explanation infers the answer is related to `Attack Enemy 0', however, the \rlm~fails to predict the correct action. Instead, the rectified flow model $\varphi$ can predict the action correctly based on the explanation.

Besides, we visualize another example that a negative explanation generated by \elm~is optimized to a positive explanation after training for two rounds. As shown below, the explanation from the ${\mathtt{SFT}}$ model opts for the movement of EAST. The explanation of our method can infer the correct movement of NORTH with solid evidence.
\input{table/reasoning_example_smac_2}


\textbf{$\mathtt{MathQA}$}. To showcase the effectiveness of our method, we visualize an example on $\mathtt{MathQA}$ that a negative explanation classified by \rlm~is corrected by the rectified flow model $\varphi$. As shown below, this explanation infers the answer is related to option d, however, the \rlm~fails to predict the correct answer. Instead, the rectified flow model $\varphi$ can predict the answer correctly based on the explanation.
\input{table/reasoning_example_mathqa_1}

Besides, we visualize another example on $\mathtt{MathQA}$ that a negative explanation generated by \elm~is optimized to a positive explanation after training for two rounds. As shown below, the explanation from the ${\mathtt{SFT}}$ model is confusing and misleading. The explanation of our method can infer the correct answer c with solid evidence.
\input{table/reasoning_example_mathqa_2}





