\section{Experiment}

\subsection{Experimental Setup}
\textbf{Dataset}. We use three testbeds to evaluate our method. 

$\mathtt{(1)\ SMAC}$ (The 
StarCraft Multiagent Challenge,~\citet{samvelyan2019starcraft}) is an RL benchmark based on a real-time multi-agent strategy game StarCraft II that emphasizes micromanagement challenges. We generate explanations for an ally agent based on action-state history. Our dataset consists of 2K trajectories (1.5K for training and 0.5K for evaluation) generated by MAPPO~\cite{yu2022surprising}, with each trajectory containing states, actions, and rewards for 30 time steps. We feed information of previous 4 consecutive time steps to the \elm~to generate explanations. Please refer to Appendix~\ref{appx:experimental_setup} for details.

$\mathtt{(2)\ MMLU}$ (The Massive Multitask Language Understanding, \citet{hendrycks2020measuring}) is a multiple-choice QA benchmark for LLMs. We choose 4 challenging Professional Knowledge datasets (Professional Medicine (272 samples), Professional Law (1.53K samples), Professional Accounting (282 samples), and Professional Psychology (612 samples)). For each dataset, 70\% of the samples are selected randomly for training, and reserve the remaining 30\% for evaluation.

$\mathtt{(3)\ MathQA}$ \cite{amini2019mathqa} is a large-scale multiple-choice QA benchmark for math word problems, comprising 30K training samples and 3K testing samples.

% Follow the convention in LLM literature, We first supervised fine-tune the models on training datasets, and then train them use our method.
% To ensure a fair comparison, we reimplement these baselines and ablations using the same code base and datasets.

\input{table/fine_tuning_methods}
\textbf{Baselines}.
We compare our method against various baselines, categorized into the following three types. 

$\mathtt{(1)\ SFT}$ (Supervised Fine-tuning) is a popular paradigm for adapting LLMs to downstream tasks. We generate ${\mathtt{SFT}}$ datasets by OpenAI o1-mini~\cite{jaech2024openai}.

$\mathtt{(2)\ RLHF}$. Our method can be considered as a form of RL with  automatically generated feedback, and thus should be compared with $\mathtt{RLHF}$~\cite{bai2022training} baselines. We consider $\mathtt{PPO}$~\cite{xu2024dpo}, $\mathtt{DPO}$~\cite{rafailov2024direct} and $\mathtt{KTO}$~\cite{ethayarajh2024kto}, using their implementation from the TRL library~\cite{von_Werra_TRL_Transformer_Reinforcement}. 

$\mathtt{(3)\ Reasoning\ Frameworks}$ employ techniques such as chain-of-thoughs (CoT) to enhance reasoning capabilities. They require high-quality training data. Self-created datasets may introduce biases and compromise fairness. Since $\mathtt{MathQA}$ has CoT annotations, whereas $\mathtt{MMUL}$ and $\mathtt{SMAC}$ do not, we compare against $\mathtt{SFT}$-$\mathtt{CoT}$~\cite{wei2022chain} and an advanced CoT method, $\mathtt{ReFT}$~\cite{trung2024reft} on $\mathtt{MathQA}$.

% As the MathQA dataset is well-suited for step-by-step reasoning in mathematical problems, it is used for evaluating reasoning methods. In contrast, the SMAC and MMLU datasets are used for comparisons involving SFT and RLHF baselines.

\textbf{Ablation studies} regarding different components.

$\mathtt{(1)\ Ours\ w/o\ Flow}$. The major novelty of our method is to introduce a rectified flow model for reward generation. Therefore, it is important to ablate the rectified flow model and directly use rewards generated by the \rlm~to train the \elm.

$\mathtt{(2)\ Ours\ w/o\ Attn}$. We remove the cross-attention layer in the rectified flow model $\varphi$. Instead, we directly concatenate the hidden states of the rectified flow model and those of the \rlm, and then use a fully-connected network to generate the vector field. 

% combine $\vh_{\textsc{Emb},t}$, $\vh_{\textsc{Emb},\vz_t}$ and the hidden state extracted from the  A 


\textbf{Models and Hyperparameters}. By default, we use Llama-3.1-8B-Instruct~\cite{dubey2024llama} for both \elm~and \rlm. We also test Qwen2.5-7B-Instruct~\cite{yang2024qwen2} and Gemma-2-2B-It~\cite{lieberum2024gemma} to evaluate the robustness of our method. 

We perform ${\mathtt{SFT}}$ of the LLMs for 3 epochs. We then iteratively train the flow model $\varphi$ and the \elm~$\pi_e(\theta_e)$ for two rounds. As shown in Figure~\ref{fig: elm_training}, performance improvement is marginal after 2 rounds. $\pi_e(\theta_e)$ and $\varphi$ are trained for 10 and 100 epochs, respectively. The learning rate for $\pi_e(\theta_e)$ is initialized as $2e\shortn 5$ and then decays linearly to zero, whereas the learning rate for $\varphi$ is fixed at $2e\shortn 4$. We run our method on 4 parallel 80GB Nvidia A100 GPUs. The batch size is 12 for $\pi_e(\theta_e)$ and 128 for $\varphi$. ${\mathtt{SFT}}$ typically takes 0.5 to 1 hour, while our method requires approximately 14--30 hours on different datasets. More details can be found in Appendix~\ref{appx:experimental_setup}.

% A negative sample from $\mathtt{MMLU}$. Although the explanation is correct, the \rlm~produces a wrong distribution. The rectified flow is not trained on sample, but can give a correct distribution. Specially, the explanation hinges on "the egg roll was present for a substantial time". Whereas the \rlm~missed this cue, the rectified flow model can generalize and predict correctly.

\begin{table}[t]
\centering
\caption{A negative sample from $\mathtt{MMLU}$ where the explanation is correct but the \rlm~produces an incorrect distribution. Although the rectified flow model was not trained on this sample, it correctly identifies ``the egg roll was present for a substantial time'', a cue missed by the \rlm, and thus provides a correct distribution.}
\vspace{-0.8em}
\begin{tabular}{p{0.95\linewidth}}
\toprule
\textbf{Context} (shortened): A wife and her husband were dining at a restaurant owned by a chef. As the wife walked past a table, she slipped on an egg roll that had been on the floor for quite some time, although the chef was unaware it had fallen there. If she sues the chef for her injuries, she will most likely:

\hdashrule[0.2ex]{\linewidth}{1pt}{1pt}

\textbf{Decision set} $\bm{\mathcal{A}}$: [A. Recover, because the egg roll on the floor constituted an unsafe condition of the premises; B. Recover, because the egg roll was on the floor \textcolor{outputcolor}{for a substantial period of time before the accident (Correct)}; C. Not recover, ...; D. Not recover, ...]

\hdashrule[0.2ex]{\linewidth}{1pt}{1pt}

\textbf{Explanation}: ... The fact that the egg roll was present \textcolor{outputcolor}{for a substantial time $\checkmark$} suggests that the owner should have been aware of the potential hazard and taken steps to address it. This situation falls under premises liability where maintaining safe conditions is crucial to ... % avoid injuries to guests

\hdashrule[0.2ex]{\linewidth}{1pt}{1pt}

\textbf{Distribution $p$ from the \rlm:} [\textbf{0.9297}, 0.0674, 0.0010, 0.0013] $\rightarrow$ `A' {\color{red}$\boldsymbol{\times}$}

\textbf{Distribution $\hat{p}$ from the rectified flow model:} 
[0.0547, \textbf{0.9330}, 0.1089, 0.0685] $\rightarrow$ `B' \textcolor{outputcolor} {$\checkmark$}\\
\bottomrule
\end{tabular}
\vspace{-1em}
\label{tab:example1}
\end{table}

\textbf{Evaluation and Metrics}. 
For evaluation, we use temperature $\tau=0.7$ for all settings. The maximum number of generated tokens is 350 for $\mathtt{SMAC}$, and 200 for $\mathtt{MMLU}$ and $\mathtt{MathQA}$. The \elm~is considered to have generated an effective explanation if the \rlm~correctly assigns the highest probability to the actual decision. We calculate the accuracy (ACC) as the percentage of effective explanations and area under the receiver operating characteristic curve (AUC) as evaluation metrics.


\subsection{Comparisons against Baselines}

As shown in Tab.~\ref{tab: ft}, our method consistently outperforms all baselines in terms of ACC and AUC, with all results derived from Llama-3.1-8B-Instruct. Specifically, our method achieves a minimum of 6.9\% higher ACC on $\mathtt{SMAC}$, 4.1\% higher ACC on $\mathtt{MMLU}$, and 4.6\% higher ACC on $\mathtt{MathQA}$ compared to the baselines, demonstrating its ability to generate more reasonable explanations that support correct decision identification. The AUC metrics further support this finding, with our method attaining the highest scores of 0.73 on $\mathtt{SMAC}$, 0.74 on $\mathtt{MMLU}$, and 0.74 on $\mathtt{MathQA}$. The advantage of our method is particularly pronounced on $\mathtt{SMAC}$, which demands complex reasoning to analyze trajectories due to sophisticated interaction among multiple agents. $\mathtt{SFT}$ exhibits the lowest performance on both datasets. Among $\mathtt{RLHF}$ baselines, $\mathtt{KTO}$ performs relatively well, attaining an ACC of 71.0\% on $\mathtt{MMLU}$ and 73.8\% on $\mathtt{MathQA}$, demonstrating its effective optimization based on appropriate human utility models. However, $\mathtt{KTO}$ still falls short of our method.

% Today, she says she has had a persistent sensation of tingling and numbness of her left thigh... the sensation has improved... Her only medication is warfarin. Vital signs are within normal limits. Examination of the skin shows no abnormalities. Muscle strength is normal. Sensation to light touch is decreased.. on the lateral aspect of the left anterior thigh. nticoagulant therapy was temporarily 
% She had a retroperitoneal hemorrhage;discontinued,
% Placement of such devices can sometimes cause 
% nerve compression from trauma or progressive neurological conditions
% The Bezold personalization test assesses the discriminatory ability to touchantsia.
% Additionally, warfarin therapy typically ... 
\begin{table}[]
\centering
\caption{A true negative sample from $\mathtt{MMLU}$. Initially, the explanation from the \elm~erroneously supports answer B that focuses on the influence of the filter placement. Post-training with our method, the \elm~accurately justifies the correct answer.}
\vspace{-0.8em}
\begin{tabular}{p{0.95\linewidth}}
\toprule
\textbf{Context} (shortened): A 67-year-old woman had a pulmonary embolism... She underwent placement of an inferior vena cava (IVC) filter. She had a hematoma that was resolving... Which of the following is the most likely cause of this patient's decreased sensation?
    
\hdashrule[0.2ex]{\linewidth}{1pt}{1pt}

\textbf{Decision set} $\bm{\mathcal{A}}$: [A. Cerebral infarction during the hospitalization; B. Complication of the IVC filter placement; C. \textcolor{outputcolor}{Compression} of the lateral femoral cutaneous nerve \textcolor{outputcolor}{(Correct)}; D. Hematoma of the left thigh.]

\hdashrule[0.2ex]{\linewidth}{1pt}{1pt}

\textbf{Explanation}: ... The patient's symptoms began after hospitalization, aligning with the placement of the IVC filter... The improvement over time supports a reversible cause related to the {\color{red}filter placement rather than other options... (B,$\boldsymbol{\times}$)}  

\hdashrule[0.2ex]{\linewidth}{1pt}{1pt}

\textbf{Improved Explanation (Ours):} ...involving nerves near sites of cannula placement or previous bleeding sites... The sensation loss around the left thigh aligns with \textcolor{outputcolor}{nerve compression near $\checkmark$}  the sites of recent interventions. \\
\bottomrule
\end{tabular}
\vspace{-1em}
\label{tab:example2}
\end{table}

Tab.~\ref{tab: prompting} in Appendix~\ref{appx:results} compares against reasoning frameworks on $\mathtt{MathQA}$. $\mathtt{SFT}$-$\mathtt{CoT}$ achieves an ACC of only 64.8\%, indicating that, despite utilizing CoT, SFT exhibits limited generalization capability. Although $\mathtt{ReFT}$ improves the ACC to 71.8\%, it remains 6.6\% below our method, showcasing the enhanced reasoning capabilities enabled by our rectified flow model $\varphi$. These comparisons underscores the effectiveness of our approach relative to established baselines and advanced reasoning frameworks.

% Our method achieves the best performance, with an ACC of 78.4\% and an AUC of 0.74 on $\mathtt{MathQA}$. In contrast, 

% while improves to 73.2\% when combined with Majority Voting. This indicates that CoT, when generating reasoning in a single pass, produces more uncertain results. Below, we present a reasoning example from MathQA comparing CoT with our method. The CoT approach employs a complex step-by-step reasoning process, where each decision heavily relies on preceding steps, increasing the likelihood of errors propagating. In contrast, our method offers a more fluent and concise explanation, enabling the Guidance LLM to infer the correct answer more effectively.



\begin{figure}[t]
% \captionsetup{justification=centering}
 \centering
    \subfigure[\label{fig: positive}Positive Samples]
        {\centering
        {\includegraphics[height=2.8cm]{media/positive_label.png}
            }
    }
    \subfigure[\label{fig: negative}Negative Samples]
        {\centering
        {\includegraphics[height=2.8cm]{media/negative_label.png}
    	}
    }
     \vspace{-2mm}
    \caption{Accuracy of the rectified model $\varphi$ on unseen test samples, shown as the percentage of samples for which $\varphi$ reproduces the correct decisions. Left: Accuracy on positive samples (where the \rlm\ is correct). Right: Accuracy on negative samples.}
    \label{fig: rectified_flow_training}
    \vspace{-2em}
\end{figure} 

\subsection{Generalization to Negative Samples}

In the proposed method, we train the rectified flow model $\varphi$ with positive samples. These positive samples are explanations based on which the \rlm~can already identify the actual decision. For the proposed method to work, it is essential that the rectified flow model generalizes to negative samples after training. 

Figure~\ref{fig: rectified_flow_training} shows the accuracy of the flow model. The y-axis represents the percentage of samples for which the flow successfully identifies correct decisions. We observe a gradual increase in accuracy on negative samples, indicating that the rectified flow indeed possesses the ability to generalize to unseen data. Tab.~\ref{tab:example1} gives such an example. This generalization capability is likely attributable to the cross-attention mechanism, as $\mathtt{Ours \ w/o\ Attn}$ cannot learn to improve its performance on negative samples during learning. 

\begin{wrapfigure}{r}{4.2cm}
    \vspace{-1em}
    \includegraphics[width=0.25\textwidth]{media/training_label.png}
    \vspace{-2em}
    \caption{Accuracy of the \elm~increases through each training round.}
 \vspace{-1em}
\label{fig: elm_training}
\end{wrapfigure}
Notably, the accuracy of our method on negative samples is capped at around 50\% (Fig. \ref{fig: negative}). This is expected because some negative samples are \emph{true} negatives, meaning that the explanations themselves are ineffective, and prevent the \rlm~from making accurate predictions. Fig.~\ref{fig: elm_training} demonstrates that our method is able to distinguish these true negative samples. Specifically, the Round 2 ACC is significantly higher than that of Round 1. If the rectified flow model learns to correct true negative samples, these ineffective explanations will be reinforced, and the \rlm~is more unlikely to give correct predictions. Tab.~\ref{tab:example2} gives an example where our method successfully improves the explanation of the \elm. Additional examples can be found in Appendix~\ref{appx:results}.

% To showcase the effectiveness of our method, we visualize an example on $\mathtt{MMLU}$ that a negative explanation classified by \rlm~is corrected by the rectified flow model $\varphi$. As shown below, this explanation infers the answer is related to 'the egg roll was present for a substantial time', however, the \rlm~fails to predict the correct answer. Instead, the rectified flow model $\varphi$ can predict the answer correctly based on the explanation.

% \input{table/reasoning_example_mmlu_1}
 
% \input{table/reasoning_example_mmlu_2}

%rlhf  给训练曲线（acc） 给测试表格（acc auc？）
%cot 给测试表格（acc auc？） 可视化reasoning的例子
%rectified flow的效果，为什么可以提高性能：负样本提升的曲线



%Among these, Llama-3.1-8B-Instruct achieves the best performance on SMAC and MathQA, while delivering comparable results to Qwen2.5-7B-Instruct on MMLU. 


\subsection{Ablation Study}
\textbf{Different base models}. As shown in Tab.~\ref{tab: ab_base_model}, we evaluate the robustness of our method using three different base models: Llama-3.1-8B-Instruct, Qwen2.5-7B-Instruct, and Gemma-2-2B-It. 
Compared to the ${\mathtt{SFT}}$ results, our method consistently improves the ACC by up to 19.9\% and achieves higher or comparable AUC across all settings. These results demonstrate that our methods is applicable to various LLM models and exhibits robust effectiveness. 
\input{table/ablation_base_model}
\input{table/ablation_arch}
\textbf{Ablations}. As shown in Tab.~\ref{tab: ab_arch}, removing the rectified flow $\varphi$ and directly using rewards from the \rlm, $\mathtt{Ours \ w/o\ Flow}$ achieves accuracies of only 52.2\% on $\mathtt{SMAC}$, 56.3\% on $\mathtt{MMLU}$, and 58.4\% on $\mathtt{MathQA}$. These results support that the rewards provided by the \rlm~are noisy, leading to ineffective optimization of the \elm. $\mathtt{Ours\ w/o\ Attn}$ performs even worse than $\mathtt{Ours\ w/o\ Flow}$.  Fig.~\ref{fig: rectified_flow_training} elucidates the reason: it cannot even reproduce 100\% accuracy on positive samples and cannot generalize to negative samples.

%
%suggesting the cross-attention mechanism performs well in the generalization to the negative sample correction.

%consistently improves over base models across different model sizes：不同base model 表格
%attention 作用：mlp， attention 曲线or 表格？

% As illustrated in Fig.~\ref{fig: rectified_flow_training}, the rectified flow model $\varphi$ in $\mathtt{Ours\ w/o\ Attn}$ only has 80\% ACC of identifying positive samples and 20\% ACC of calibrating negative samples.
