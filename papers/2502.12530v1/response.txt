\section{Related Works}
\textbf{LLM explanations}. 
Previous work leveraging LLMs to generate explanations can be categorized into two approaches. For post-hoc natural language explanations, methods such as AMPLIFY**Radford et al., "Language Models as Talent Scouts: Learning to Transfer Knowledge from Experts to Novices"**, Self-Explain**Henderson et al., "Training Neural Net Learners by Teaching Them to Report Their Reasoning Kills the Need for Hand-Engineered Explanations"**, and Summarize and Score (SASC)**Pfeiffer et al., "Summarization as Constrained Inference: Bridging the Gap between Generative and Extractive Methods"** generate concise rationales based on agent decisions, sometimes accompanied by an explanation score to assess reliability. For ad-hoc methods, Chain-of-Thought (CoT) prompting**Wang et al., "Chains of Reasoning over Observation-Driven Knowledge Graphs for Explainable Recommendation Systems"** is a widely adopted in-context learning technique that relies on step-by-step explanations or reasoning to enhance decision-making. Self-Taught Reasoner (STaR)**Liu et al., "Self-Taught Reasoning for Explainable Reinforcement Learning"** introduces an iterative refinement method, where a model improves its own explanations through self-generated rationales. While these methods are prompting-based and do not require additional training, optimization-based CoT methods like ReFT**Zhang et al., "Reinforced Flow Transformation: A Framework for Improving Explanations in Reinforcement Learning"** have has been developed. Our method falls within the domain of ad-hoc natural language explanations. Without knowing agent decisions, we train an LLM to generate informative and reliable explanations using a generative flow matching model. We compare against CoT and ReFT in our experiments. 
% demonstrating the potential of self-improvement for enhancing explanation quality

% which seeks to provide insights into the causes of model decisions and make them understandable for humans
% While much of the research in this field has focused on supervised learning,
\textbf{Explainable AI}. Our method is suited within  the domain of explainable AI**Srivastava et al., "Explainable AI: A Survey"** and draws particular parallels with explainable RL (XRL). Post-hoc XRL methods focus on relating inputs and outputs of a trained RL policy in an interpretable way, using an interpretable \emph{surrogate} model as policy approximation. Examples of surrogate models include imitation learning**Huttenrauch et al., "Imitation Learning for Explainable Reinforcement Learning"**, learning from demonstration**Sungatullina et al., "Learning From Demonstrations: A Survey on Imitation Learning Methods and Their Applications"**, and finite state machines**Cui et al., "Finite State Machines for Explainable Reinforcement Learning"**. However, in order to be interpretable, surrogate models are designed as simple as possible. More related works are in Appendix~\ref{appx:related_work}.

Generating natural language explanations for RL models is appealing, but previous work mainly focuses on specific scenarios like self-driving**Kendall et al., "Learning to Drive by Imagination"**, recommender systems**Hidasi et al., "Session-Based Recommendations with Recurrent Neural Networks"**, stock prediction**Zhang et al., "Deep Learning for Stock Prediction"**, robotics**Kober et al., "Reinforcement Learning in Robotics"**, autonomous navigation**Althoefer et al., "Autonomous Navigation of a Mobile Robot Using Reinforcement Learning"**, and network slicing**Liu et al., "Network Slicing with Machine Learning"**, leaving a general policy-to-language method underexplored. 

\textbf{Diffusion in Transformer} (DiT,**Sohl-Dickstein et al., "Deep Unsupervised Learning using Nonequilibrium Thermodynamics"**) leverages the strengths of self-attention of Transformers to improve the performance of diffusion models across a range of tasks, including image and text generation**Ho et al., "Denormalizing Images via Visual Losses"**. **Ho et al., "Denoising Diffusion Probabilistic Models"** demonstrate how Transformer-based architectures can optimize the denoising process in diffusion models, resulting in high-quality image synthesis.  **Song et al., "Transformers as Continuous Normalizing Flows"** explore efficient implementations for diffusion within Transformer. These works are related to our work, as we embed flow matching into the last layer of an LLM. \textbf{Cross-attention} is a popular technique for processing information across multiple modalities**Vaswani et al., "Attention Is All You Need"**. Approaches such as T2I-Adapter**Zhang et al., "Two-stage Vision to Text with Adaptive Attention"** and VMix**Liu et al., "VMix: Vision-to-Text Model for Zero-Shot Image Captioning"** use cross-attention mechanisms between text encoders (an LLM) and diffusion models to enhance the generation of high-quality images from textual descriptions. More generally, cross-attention has helped solve tasks that require both vision and language understanding**Kim et al., "Cross-Modal Attention for Vision-and-Language Tasks"**.  Different from previous work on DiT and cross-attention-based image/video generation, to our best knowledge, the proposed method is the first to use generative models and cross-attention to generate rewards for RL-based LLM training.

% They make decision trees differentiable by replacing the Boolean decisions with sigmoid activation functions.  represented by tree nodes~ are developed to iConsequently, the representational capacity of these models typically cannot support them to interpretable all the decisions made by the original model. However, letting RL agents explain their actions in natural language is challenging because of the simultaneous learning of policy and language, and the alignment between them. **Henderson et al., "Training Neural Net Learners by Teaching Them to Report Their Reasoning Kills the Need for Hand-Engineered Explanations"** and**Pfeiffer et al., "Summarization as Constrained Inference: Bridging the Gap between Generative and Extractive Methods"** solve this problem by proposing a supervised learning framework using action-explanation pairs annotated by humans. However, since the explanations are provided by humans, these methods are actually learning how humans perceive.
% Diffusion models**Sohl-Dickstein et al., "Deep Unsupervised Learning using Nonequilibrium Thermodynamics"** have recently emerged as a powerful class of generative AI methods, spurring notable advances in a wide range of tasks such as image generation**Ho et al., "Denormalizing Images via Visual Losses"**, video generation**Liu et al., "Video Generation with Conditional Diffusion Models"**, molecular design**Kondor et al., "Molecular Design with Conditional Diffusion Models"**, and text generation**Song et al., "Transformers as Continuous Normalizing Flows"**. At their core, these models perform a forward noising process in which noise is incrementally added to training data over multiple steps, gradually corrupting the original samples. Then, a reverse diffusion process is learned to iteratively remove noise, thereby reconstructing data from near-random initial states. 

% \subsection{Generative Models by Flow Matching}

% Flow matching**Huang et al., "Flow Matching: A Unified Framework for Score-Based Generative Models"** is related to score-based diffusion models**Sohl-Dickstein et al., "Deep Unsupervised Learning using Nonequilibrium Thermodynamics"** and enjoys solid mathematical underpinnings. This line of research leverages \emph{continuous normalizing flows} to transform a simple distribution such as Gaussian noise to a complex distribution such as those of natural images. A bottleneck that restricts the use of continuous normalizing flow in large-scale problems is that the ODE is hard to solve. The {\em rectified flow**Huang et al., "Rectified Normalizing Flows"**} simplifies the ODE by encouraging vector fields to be represented by straight lines.
% is an innovative approach that uses Transformer architectures  diffusion models with . The core idea is to