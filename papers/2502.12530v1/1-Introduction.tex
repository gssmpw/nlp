
\section{Introduction}


Intelligent agents, ranging from reinforcement learning (RL) agents~\cite{kiran2021deep,zhao2021consciousness,liu2024learning,qiu2024instructing}, large language models (LLMs)~\cite{yao2022react,shinn2024reflexion,wang2023voyager,kang2020incorporating}, to robotic systems~\cite{ismail2018survey}, are becoming increasingly intertwined with daily lives~\cite{survey2024wang}. To foster a more transparent, safe, and aligned agent ecosystem, a promising avenue is communicating the reasoning behind actions or decisions generated by agent policies in natural language~\cite{lazaridou2016multi}. Such explanations would allow humans to understand the rationale behind specific decisions, offer meaningful feedback, and make corrections when necessary, ultimately fostering trust and enhancing the reliability of the intelligent ecosystem~\cite{cambria2023survey}.

However, these agents vary significantly in their policies, including decision-making algorithms and internal structures~\cite{bulling2014survey,wang2024survey}. Model-specific explainable AI methods inevitably lag behind due to the per-task engineering efforts required to adapt from one agent to another~\cite{rai2020explainable}. This necessitates a \emph{model-agnostic} explanation generator for agent policies capable of inferring the reasoning behind decisions made by these policies given the context in which these decisions occur (e.g., in an RL task, a decision is an RL action, and the context could be an MDP state). In this paper, we explore the feasibility of building such a \emph{policy-to-language model} based on LLMs, leveraging their ability of capturing and manipulating intricate contextual and linguistic cues to provide interpretable and persuasive explanations for agent decisions.

Given agent decisions and their context, it is not difficult for an LLM to generate plausible explanations by drawing on its world knowledge, but such explanations may lack effectiveness due to issues like model capacity or hallucination~\cite{ji2023towards, ganguli2022red}. To ensure trustworthy communication, our aim is for explanations to reflect the agent's reasoning---verified via whether a third party can accurately infer the agent's decisions. That is, we propose to train an \elm~to generate reasoning solely based on the context, with rewards based on whether a third party can reconstruct agent decisions from explanations. Notably, we avoid providing actual decisions to the \elm, preventing direct disclosure and forcing the model to derive effective explanations by the analyzing the context.

\begin{figure*}[t]
    \centering
    % \vspace{-0.5em}
    \includegraphics[width=\linewidth]{media/diffllm_idea.pdf}
    % \vspace{-1.5em}
    \caption{An overview of our method. (Left) We prompt an \elm~to generate reasoning about an agent decision based on the context information. Our focus is on whether a third party can infer the actual decision from this explanation. (Middle) We employ a rectified flow model $\varphi$ to generate a probability distribution $\hat{p}$ over possible decisions, according to how likely they appear as a plausible outcome after each sentence of the explanation. Per-sentence rewards for training the \elm~are the changes in the probability of the actual decision (highlighted in blue). (Right) The architecture and training of the rectified flow $\varphi$ are based on a \rlm. The \rlm~provides positive samples, where, with the context and explanation as input, it can produce a distribution $p$ that assigns the highest probability to the actual decision. The rectified flow $\varphi$ is trained to produce such distributions $p$, with a cross-attention layer in its middle that selectively leverages information from the \rlm~input, enabling generalization to negative samples.}
    \vspace{-0.5em}
    \label{fig:idea}
\end{figure*}


As for the third party that provides rewards, human feedback is accurate, but can be expensive to scale across tasks with varying requirements~\cite{liu2024skywork, lambert2024rewardbench}. An alternative is to use a \rlm~as a surrogate feedback provider. However, as shown by previous work~\cite{yang2024regularizing} and confirmed in our experiments (ablation $\mathtt{Ours}$ $\mathtt{w/o}$ $\mathtt{Flow}$ in Tab.~\ref{tab: ab_arch}), such feedback can be noisy and inefficient. To solve this, we propose using generative models~\cite{ho2020denoising,song2020score,liu2022flow} to generate feedback, given their proven ability in generating complex distributions from simple distributions. The architecture and training of this generative model are based on the \rlm, and ultimately provides the desired third party rewards to the \elm.
% denoise the feedback from the \rlm. In other words, we use the \rlm~to train a generative model, which ultimately provides the desired third party rewards to the \elm is not easily scalable to fit the needs changing from task to task

Concretely, we feed an explanation to the \rlm~sentence by sentence, and evaluate how likely the \rlm~is to consider each possible decision as a plausible outcome after each sentence addition. This effectively forms a distribution over all possible decisions. Some distributions are \emph{positive}, assigning the highest probability to the actual decision. We adopt rectified flow~\cite{liu2022flow} for generative modeling and train it on these positive samples to reconstruct them from Gaussian noise distributions. Rectified flow is an efficient flow matching~\cite{chen2018neural,lipman2022flow} method, characterized by robust and fast inference, requiring only a few steps to solve the associated ordinary differential equation and generate high-dimensional distributions~\cite{song2019generative}.

Our goal is that, having learned from positive samples, the rectified flow model can generalize to some negative samples where the \rlm~makes errors. To facilitate this, we design a specialized rectified flow network architecture that is able to exploit linguistic and contextual cues of explanations and context. Specifically, we learn to transform the inputs of the rectified flow model to \emph{flow tokens} on which we apply a cross-attention layer to selectively incorporate information from the hidden states of the \rlm's last layer. Inputs to the \rlm~include decisions, their context, and explanations. The output of the cross-attention layer is processed by a fully connected network to produce the output of the rectified flow model. Compared to alternative methods, such as directly fine-tuning the \rlm, our method is more lightweight while effectively improving the reliability of the feedback.
% , together with the inputs of the rectified flow model

We evaluate our method on both RL ($\mathtt{SMAC}$,~\citet{samvelyan2019starcraft}) and LLM tasks ($\mathtt{MMLU}$ by \citet{hendrycks2020measuring}, $\mathtt{MathQA}$ by \citet{amini2019mathqa}). Our method outperforms $\mathtt{SFT}$ and $\mathtt{RLHF}$ baselines ($\mathtt{PPO}$ by \citet{xu2024dpo}, $\mathtt{DPO}$ by \citet{rafailov2024direct}, $\mathtt{KTO}$ by \citet{ethayarajh2024kto}) by 4\%-20\% across all tasks and is applicable to different base models. It also surpasses reasoning frameworks ($\mathtt{SFT}$-$\mathtt{CoT}$ by \citet{wei2022chain}, $\mathtt{ReFT}$ by \citet{trung2024reft}) by 6\%-14\% on $\mathtt{MathQA}$. Removing rectified flow and training the \elm~directly with rewards from \rlm~decreases the performance by 4\%-16\%, demonstrating the effectiveness of the generative rewards. 

% \twadd{Looking ahead, we envision the proposed method to be a first step towards a general model-agnostic reward generation method based on generative models for improved and automatic model alignment.}

% This LLM takes the decision, its context, and reasoning as input, providing the
% positive samples on which the \rlm has demonstrated correctness, i.e., it assigns a higher probability to the correct decision than to any alternative actions. The diffusion model is trained with these positive samples to transform an initial Gaussian noise distribution into a probability distribution over the possible actions, effectively learning to replicate correct reward signals.

% The difference in probability—before and after each sentence—serves as the reward signal for that particular sentence. 



% While this approach leverages the compositional power of LLMs, there remains a significant issue: the \rlm could be wrong in its assessment, leading to inaccurate or misleading rewards. To address this, to refine and correct the reward estimates using a diffusion model. 

% We collect positive samples on which the \rlm has demonstrated correctness, i.e., it assigns a higher probability to the correct decision than to any alternative actions. The diffusion model is trained with these positive samples to transform an initial Gaussian noise distribution into a probability distribution over the possible actions, effectively learning to replicate correct reward signals.

% This cross-attention mechanism enables the diffusion model to exploit 

% LLMs are adept at generating coherent, human-readable narratives, making them ideal for producing natural language explanations. 
% In this paper, we propose a general-purpose, model-agnostic explanation generator by 
% As these agents undertake tasks with higher levels of autonomy, the need for interpretability grows urgent. 
% One prominent line of research toward this goal is explainable reinforcement learning (RL). Traditional methods in explainable RL, however, often suffer from multiple limitations: (i) they tend to be model-specific, requiring significant engineering efforts when switching from one RL algorithm to another, (ii) they frequently rely on surrogate or simplified models for producing explanations, which may lack expressive power to convincingly justify decisions, and (iii) they can require access to underlying gradients or other fine-grained system information, which is difficult or impractical to obtain in many real-world scenarios. Consequently, these methods struggle to scale up in complex settings where agents operate with opaque internal representations or proprietary architectures.
% Recently, the emergence of LLMs has opened up new avenues for explainability in both RL and decision-making more broadly. 
% A straightforward approach of achieving this might involve feeding a decision and its context (such as an RL action and its prior experience) into an \elm~and prompting it to generate an explanation of the decision-making process. However, this approach does not guarantee that the generated explanations are reasonable or adequately informative. Specifically, we aim for explanations that are sufficiently clear so that other agents or humans can reproduce the decision from the explanation alone, without knowing the decision as a prior. To this end, we use another \rlm as a reference. Specifically, we incrementally feed the explanation to the \rlm sentence by sentence, and evaluate by perplexity how likely the \rlm is to consider the decision as a plausible outcome after each sentence is added. The difference in probability—before and after each sentence—serves as the reward signal for that particular sentence, measuring the contribution of each sentence to the effectiveness of the explanation.