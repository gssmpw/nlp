\section{Method}

Our goal is to train an \elm~$\pi_e(\theta_e)$ 
to generate an explanation 
for a decision given its context. In reinforcement learning (RL) tasks, the decision is an RL action, while the context is the trajectory of preceding states, actions, and rewards. In LLM tasks, the decision could be, for example, a chosen option for a multiple-choice question, while the context is the question itself. We use $\mathcal{A}$ to denote the set of all possible decisions. In previous examples, $\mathcal{A}$ is the action space for RL and the set of answer choices for the LLM task. We train the \elm~by per-sentence rewards generated by a rectified flow model $\varphi(\theta_\varphi)$, which is based on the architecture and the outputs of a supportive \rlm~$\pi_g(\theta_g)$. An overview of our method is shown in Fig.~\ref{fig:idea}.

\subsection{\elm}

Our method is developed around the \elm. Given a set $\mathcal{D}_e=\{(a_j,c_j)\}_{j=1}^{J}$ of decisions $a_j$ and their context $c_j$, we use the following prompt $x_e$ to ask the \elm~to generate explanations: $\mathtt{Given}$ $\mathtt{\left[Context\ c_j\right].}$ $\mathtt{Please}$ $\mathtt{analyze}$ $\mathtt{reasoning}$ $\mathtt{for}$ $\mathtt{the}$ $\mathtt{agent}$ $\mathtt{decision}$ $\mathtt{based}$ $\mathtt{on}$ $\mathtt{the}$ $\mathtt{context}$. 

Suppose that the \elm~generates $K_e$ sentences as output: $y_e(x_e)\shorte (\idx[s][e][1], \cdots,\idx[s][e][K_e])$. For the generated content to be effective explanations, we hope that one can consistently infer agent decisions from explanations across various contexts. To this end, we seek feedback regarding how likely the actual decision $a_j$ is a plausible outcome given each incremental portion of the explanation. 

Such feedback is most accurate when provided by human annotators. However, human feedback is expensive~\cite{bai2022constitutional}. We first discuss an alternative approach that uses a \rlm~as a surrogate feedback provider, whose downside will motivate the proposed generative reward method introduced in the next subsection. 

We query the \rlm~with the prompt: $\mathtt{Given}$\newline$\mathtt{[Context\ c_j],}$ $\mathtt{the\ reasoning\ is\ [}$\idx[s][e][1:k]$\mathtt{].}$ $\mathtt{Thus,}$ $\mathtt{the}$ $\mathtt{decision\ is\ [a\ decision}$ $a\in\mathcal{A}\mathtt{]}$. Here, $\idx[s][e][1:k]$ is the first $k$ sentences of the explanation $y_e$. We denote this prompt to the \rlm~by $\idx[x][g][k][y_e]$. The dependence on $y_e$ will be omitted when unambiguous. 

We are interested in the likelihood a decision $a\in\mathcal{A}$ appearing at the end of $\idx[x][g][k][y_e]$, influenced by logits $M(a | \idx[x][g][k][y_e])$ (Eq.~\ref{equ:token_logits}). In practice, $a$ is represented by some tokens describing the decision. If it involves multiple tokens, we calculate their mean~\citep{yang2022logit}.

By applying the SoftMax operation to $M(a | \idx[x][g][k][y_e])$, $a\in\mathcal{A}$, we get a distribution over decisions:
\begin{align}
    p(a_j | \idx[x][g][k]) = \textsc{SoftMax}_j\left(M(a | \idx[x][g][k])\right).
\end{align}

This distribution is defined for the first $k$ sentences, $k=1,\cdots,K_e$. Intuitively, the distribution $p$ changes as we feed the sentences in the explanation $y_e$ incrementally. These changes measure the contribution of each sentence to the effectiveness of the explanation, allowing us to define per-sentence rewards for $y_e$ as the changes of the likelihood of the actual decision $a_j$ after each newly added sentence:
\begin{align}
     r(\idx[s][e][k]) = p(a_j | \idx[x][g][k]) - p(a_j | \idx[x][g][k-1]),\label{equ:elm_reward}
\end{align}
which can be understood as an information gain~\citep{ton2024understanding}. Calculating sentence-level rewards is a trade-off~\citep{deepseekai2025deepseekr1incentivizingreasoningcapability}. We benefit from denser reward signals compared to a single reward for the whole explanation, and also avoid the costs of per-token reward calculation.

We intentionally exclude the actual decision $a_j$ from the prompt $x_e$ to the \elm. Otherwise, the rewards $r(\idx[s][e][k])$ might be trivial and encourage merely restating the decision: the sentence that discloses the decision will get a very large reward, while the following sentences get fairly small rewards, regardless of their content.

The disadvantage of directly using this \rlm~is that the rewards $r(\idx[s][e][k])$ could be noisy or inefficient~\cite{yang2024regularizing}, as proven by the ablation study $\mathtt{Ours}$ $\mathtt{w/o}$ $\mathtt{Flow}$ in Tab.~\ref{tab: ab_arch}. We propose to fix this problem by introducing a rectified flow model for reward generation.

\subsection{Reward Generation by Flow Matching}

We train a rectified flow model to generate denoised rewards $r(\idx[s][e][k])$ before using them to train the \elm. For some decision-context pair $(a_j, c_j)$, the \rlm~is able to assign the highest probability to the actual decision $a_j$ compared to other alternatives by inferring from the explanation $\idx[s][e][1:k]$:
\begin{align}
    a_j = \arg\max_a  p(a | \idx[x][g][k]) .
\end{align}
We call such a tuple $(a_j, c_j, \idx[s][e][1:k])$ a \emph{positive sample}. $\mathcal{D}_p$ denotes the set of positive samples. Drawing inspiration from rejection sampling~\cite{touvron2023llama}, we use $\mathcal{D}_p$ to train a rectified flow model $\varphi$ to reproduce distribution $p(\cdot | \idx[x][g][k])$ from Gaussian noise. To this end, the Gaussian noise is represented by an initial random variable: $Z_0\sim \mathcal{N}(\bm 0, \sigma_z^2 \mI_{|\mathcal{A}|})$, where $I_{|\mathcal{A}|}$ is the identity matrix of rank $|\mathcal{A}|$, and the target random variable associated with a positive sample $s_p=(a_j, c_j, \idx[s][e][1:k])\sim\mathcal{D}_p$ is $Z_1(s_p)\sim p(\cdot | \idx[x][g][k])$.
The loss function that trains the rectified flow to transform $Z_0$ to $Z_1$ is
\begin{align}
    \mathcal{L}_{\textsc{Flow}}(\theta_\varphi) &= \mathbb{E}_{\vz_0\sim Z_0, s_p\sim \mathcal{D}_p,\vz_1\sim Z_1(s_p), t\sim [0,T]} \nonumber\\
    &\left[\|(\vz_1-\vz_0)-\varphi(t,\vz_t; \theta_\varphi)\|^2\right].\label{equ:flow_loss}
\end{align}
Here $\vz_t = t\cdot \vz_1 + (1-t)\cdot \vz_0$, for $t\in[0,1]$. This loss drives the vector field $\varphi$ at interpolated points between $\vz_0$ and $\vz_1$ to follow the straight line $\vz_1-\vz_0$. Eq.~\ref{equ:flow_loss} addresses the training objective of the rectified flow model and is applicable to various network architectures $\varphi$. We now discuss an architecture specially designed for our task.

\subsection{Embed Rectified Flow in an LLM}

Intuitively, we expect that the rectified flow model trained on positive samples can generalize to negative samples and generate correct rewards for them. This requires that $\varphi$ can understand the linguistic cues in explanations, which is beyond the capacity of typical rectified flow models based on fully-connected networks or U-Nets.

To solve this problem, we propose to embed the rectified flow model $\varphi$ in the \rlm. We demonstrate the specific network architecture in Figure~\ref{fig:idea} (Middle).

\textbf{Input}. As in a standard rectified flow model, the input to $\varphi$ includes (1) the current state $\vz_t\in\mathbb{R}^{|\mathcal{A}|}$, $t\in[0,1]$, with $\vz_0$ sampled from the standard Gaussian distribution; and (2) a positional encoding $PE(t)$ for the ODE time $t\in[0,1]$.

\textbf{Embedding}. We first use ReLU-activated, layer-normalized MLPs $\varphi_{\textsc{Emb}}:\mathbb{R}^{|\mathcal{A}|}\rightarrow\mathbb{R}^d$ to project the inputs to have the same dimension as the \rlm~tokens. $PE(t)$ and $\vz_t$ use two separate embedding MLPs:
\begin{align}
    &\vh_{\textsc{Emb},t} = \varphi_{\textsc{Emb},t}(PE(t)),\vh_{\textsc{Emb},\vz_t} = \varphi_{\textsc{Emb},\vz_t}(\vz_t);\\
    &\mH_{\textsc{Emb}} = (\vh_{\textsc{Emb},t},\vh_{\textsc{Emb},\vz_t})^\top
\end{align}
The resulting embeddings, stacked as $\mH_{\textsc{Emb}}\in\mathbb{R}^{2\times d}$, are called \emph{flow tokens}.

\textbf{Cross-Attention}. We use cross-attention to merge the flow tokens into the last layer of the \rlm. Recall that the inputs of the \rlm~include decisions, context, and explanations, which provide raw information for the rectified flow model to infer reasonable rewards.

We use the \rlm's last layer weight matrices $(\mW_Q, \mW_K, \mW_V)$ to generate queries, keys, and values of the flow tokens: $(\mQ_{\textsc{Emb}}, \mK_{\textsc{Emb}}, \mV_{\textsc{Emb}}) = \mH_{\textsc{Emb}} (\mW_Q, \mW_K, \mW_V)$,
which are concatenated to the last layer latent states $(\idx[\mQ][][N,L-1], \idx[\mK][][N,L-1], \idx[\mV][][N,L-1])$ to calculate self-attention.

% \textbf{Project Back}. 
\textbf{Projector}. Define $\vh_{\textsc{Attn},\vz_t}$ as the latent state of the flow token $\vh_{\textsc{Emb},\vz_t}$ after cross-attention. This state has incorporated the ODE time $t$ and explanatory information through cross-attention. Progressing from this state, we use a four-layer fully-connected network $\varphi_{\textsc{Proj}}:\mathbb{R}^d\rightarrow\mathbb{R}^{|\mathcal{A}|}$ with ReLU activation and layer normalization to generate the vector field $\varphi(t,\vz_t) = \varphi_{\textsc{Proj}}(\vh_{\textsc{Attn},\vz_t})$. We find that skip-layer connections are important for training stability. Specifically, we append the inputs $\vz_t$ and $t$ to the hidden layers of $\varphi_{\textsc{Proj}}$.

\subsection{Overall Training Procedure}
Given the training set $\mathcal{D}_e$, we first use the \elm~and the \rlm~to construct the positive training set $\mathcal{D}_p$, which is used to train the rectified flow model $\varphi$ with learnable parameters in the embedding $\varphi_{\textsc{Emb}}$ and projection sub-network $\varphi_{\textsc{Proj}}$ by the loss in Eq.~\ref{equ:flow_loss}.

Once the rectified flow model $\varphi$ is trained, we use it to generate rewards for both positive and negative samples in $\mathcal{D}_e$. Concretely, we solve the ODE $d\vz_t = \varphi(t, \vz_t)dt$ with $\vz_0$ sampled from the standard Gaussian distribution. An advantage of rectified flow is that the vector field $\varphi$ is encouraged to be straight lines, allowing efficient and accurate solution of the ODE in a few steps. The solution $\vz_1$ (at time $1$) is set as the estimated decision distribution
\begin{align}
    \hat{p}(\cdot|\idx[x][g][k])=\vz_1=\vz_0+\int_0^1 \varphi(t,\vz_t)dt.
\end{align}
We calculate the rewards using Eq.~\ref{equ:elm_reward} and use $\mathtt{PPO}$ to update the \rlm. The training of the rectified flow model and the \rlm~alternates until converge.

% which is used as the query in the cross-attention module:
% \begin{align}
%     \text{Attn}(Q_{t,m}^{\text{Flow}}, K, V) = \text{softmax}\left(\frac{Q_{t,m}^{\text{Flow}} K^\top}{\sqrt{d_K}}\right)V,
% \end{align}
% where $K,V$ are the keys and values from LLMs.

 % whether $y_e$ satisfies this property by measuring

 % Ideally, we expect that the actual decision is assigned the highest probability in the distribution $p(\cdot | \idx[x][r][k])$
% We propose to not directly train ~with , as they could be noisy when the \rlm~is not powerful enough. Rather, To be specific, it is possible that $a_j$ is not the most likely option in the distribution  even when the explanation $y_e$ is clear enough. To deal with this issue, we propose to 
% We first collect a set of \emph{positive samples} $\mathcal{D}_p$. 
% , a positive sample is a tuple , where $\idx[s][e][k]$ is a possibly part of explanation that can induce \rlm~to 