\section{Related Works}
\textbf{LLM explanations}. 
Previous work leveraging LLMs to generate explanations can be categorized into two approaches. For post-hoc natural language explanations, methods such as AMPLIFY\cite{krishna2024post}, Self-Explain\cite{rajagopal2021selfexplain}, and Summarize and Score (SASC)~\cite{singh2023explaining} generate concise rationales based on agent decisions, sometimes accompanied by an explanation score to assess reliability. For ad-hoc methods, Chain-of-Thought (CoT) prompting~\cite{wei2022chain} is a widely adopted in-context learning technique that relies on step-by-step explanations or reasoning to enhance decision-making. Self-Taught Reasoner (STaR)~\cite{zelikman2022star} introduces an iterative refinement method, where a model improves its own explanations through self-generated rationales. While these methods are prompting-based and do not require additional training, optimization-based CoT methods like ReFT~\cite{trung2024reft} have has been developed. Our method falls within the domain of ad-hoc natural language explanations. Without knowing agent decisions, we train an LLM to generate informative and reliable explanations using a generative flow matching model. We compare against CoT and ReFT in our experiments. 
% demonstrating the potential of self-improvement for enhancing explanation quality

% which seeks to provide insights into the causes of model decisions and make them understandable for humans
% While much of the research in this field has focused on supervised learning,
\textbf{Explainable AI}. Our method is suited within  the domain of explainable AI~\cite{arrieta2020explainable,carvalho2019machine,ehsan2019automated,gunning2017explainable,ras2018explanation,gilpin2018explaining} and draws particular parallels with explainable RL (XRL). Post-hoc XRL methods focus on relating inputs and outputs of a trained RL policy in an interpretable way, using an interpretable \emph{surrogate} model as policy approximation. Examples of surrogate models include imitation learning~\cite{abbeel2004apprenticeship}, learning from demonstration~\cite{argall2009survey}, and finite state machines~\cite{koul2018learning, danesh2021re}. However, in order to be interpretable, surrogate models are designed as simple as possible. More related works are in Appendix~\ref{appx:related_work}.

Generating natural language explanations for RL models is appealing, but previous work mainly focuses on specific scenarios like self-driving~\cite{cai2024driving}, recommender systems~\cite{lubos2024llm}, stock prediction~\cite{koa2024learning}, robotics~\cite{lu2023closer}, autonomous navigation~\cite{trigg2024natural}, and network slicing~\cite{ameur2024leveraging}, leaving a general policy-to-language method underexplored. 

\textbf{Diffusion in Transformer} (DiT,~\citet{yang2023diffusion}) leverages the strengths of self-attention of Transformers to improve the performance of diffusion models across a range of tasks, including image and text generation~\cite{cao2024survey}. \citet{dhariwal2021diffusion} demonstrate how Transformer-based architectures can optimize the denoising process in diffusion models, resulting in high-quality image synthesis. \citet{ulhaq2022efficient} explore efficient implementations for diffusion within Transformer. These works are related to our work, as we embed flow matching into the last layer of an LLM. \textbf{Cross-attention} is a popular technique for processing information across multiple modalities~\citep{radford2021learning, alayrac2022flamingo, li2023blip}. Approaches such as T2I-Adapter~\citep{mou2024t2i} and VMix~\citep{wu2024vmix} use cross-attention mechanisms between text encoders (an LLM) and diffusion models to enhance the generation of high-quality images from textual descriptions. More generally, cross-attention has helped solve tasks that require both vision and language understanding~\citep{hatamizadeh2025diffit, cao2024survey}.  Different from previous work on DiT and cross-attention-based image/video generation, to our best knowledge, the proposed method is the first to use generative models and cross-attention to generate rewards for RL-based LLM training.

% They make decision trees differentiable by replacing the Boolean decisions with sigmoid activation functions.  represented by tree nodes~ are developed to iConsequently, the representational capacity of these models typically cannot support them to interpretable all the decisions made by the original model. However, letting RL agents explain their actions in natural language is challenging because of the simultaneous learning of policy and language, and the alignment between them. \citet{ehsan2018rationalization} and~\citet{wang2019verbal} solve this problem by proposing a supervised learning framework using action-explanation pairs annotated by humans. However, since the explanations are provided by humans, these methods are actually learning how humans perceive.
% Diffusion models~\cite{ho2020denoising,song2019generative} have recently emerged as a powerful class of generative AI methods, spurring notable advances in a wide range of tasks such as image generation~\cite{rombach2022high,esser2024scaling}, video generation~\cite{ho2022video,ceylan2023pix2video,ho2022imagen}, molecular design~\cite{gruver2024protein}, and text generation~\cite{lou2024discrete}. At their core, these models perform a forward noising process in which noise is incrementally added to training data over multiple steps, gradually corrupting the original samples. Then, a reverse diffusion process is learned to iteratively remove noise, thereby reconstructing data from near-random initial states. 

% \subsection{Generative Models by Flow Matching}

% Flow matching~\cite{chen2018neural,lipman2022flow} is related to score-based diffusion models~\cite{song2019generative} and enjoys solid mathematical underpinnings. This line of research leverages \emph{continuous normalizing flows} to transform a simple distribution such as Gaussian noise to a complex distribution such as those of natural images. A bottleneck that restricts the use of continuous normalizing flow in large-scale problems is that the ODE is hard to solve. The {\em rectified flow}~\cite{liu2022flow} simplifies the ODE by encouraging vector fields to be represented by straight lines.
% is an innovative approach that uses Transformer architectures  diffusion models with . The core idea is to

%