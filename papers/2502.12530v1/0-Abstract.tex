\begin{abstract}
As humans increasingly share environments with diverse agents powered by RL, LLMs, and beyond, the ability to explain their policies in natural language will be vital for reliable coexistence. In this paper, we build a model-agnostic explanation generator based on an LLM. The technical novelty is that the rewards for training this LLM are generated by a generative flow matching model. This model has a specially designed structure with a hidden layer merged with an LLM to harness the linguistic cues of explanations into generating appropriate rewards. Experiments on both RL and LLM tasks demonstrate that our method can generate dense and effective rewards while saving on expensive human feedback; it thus enables effective explanations and even improves the accuracy of the decisions in original tasks.

%Looking into the future, humans will likely inhabit an ecosystem shared by diverse intelligent agents powered by reinforcement learning (RL) models, large language models (LLMs), etc. Explaining their decisions in natural language is important to foster trust and enhance reliability of this ecosystem. In this paper we build a model-agnostic explanation generator based on an LLM. The technical novelty is that the rewards for training this LLM are generated from noise by a generative flow matching model. This model has a specially designed architecture with some hidden layers merged with an LLM to incorporate the linguistic cues of explanations into the reward generation process. Experiments on both RL and LLM tasks demonstrate that our method can generate dense and reasonable rewards while saving expensive human feedback, effectively enable agents to infer others' decisions from explanations, and even improve the success rate of the decisions in the original LLM task.
\end{abstract}