\section{Related Works}
\subsubsection{Direct Model Editing Works} These approaches include aforementioned works \cite{ftZhu2020ModifyingMI, mendMitchell2021FastME, memitMeng2022MassEditingMI, romeMeng2022LocatingAE}. Other works use a hypernetwork to predict parameter updates for a model \citet{DeCao2021EditingFK} 

\subsubsection{Retrieval Based Works} Outside of direct model editing approaches \citep{ftZhu2020ModifyingMI, romeMeng2022LocatingAE, mendMitchell2021FastME, memitMeng2022MassEditingMI, birdMa2023UntyingTR}, other approaches have sought to adapt to changing knowledge via storing external edit memories and employing various in-context learning and retrieval mechanisms. IKE and others add all edits in the LLM context \citep{ikeZheng2023CanWE, Onoe2023CanLL}; SERAC identifies if a related fact has been edited and retrieves it \citep{seracMitchell2022MemoryBasedME}; MeLLo and DeepEdit iteratively retrieve edited memories during the generation and decoding process \citep{Zhong2023MQuAKEAK, Wang2024DeepEditKE}. 
    
We note that these retrieval-based editing approaches are fundamentally different than direct model editing approaches. Direct model editing leaves the model architecture and code entirely the same with the only change being to parameter weights. This means any system developed to use the original model can use the updated model with no other changes. In contrast, retrieval-based editing systems fundamentally change how the model is used and functions. For example, MeLLo requires setting up a retrieval database, iteratively prompting the LLM, making multiple calls to the database, and prompting the LLM to create further sub-questions \cite{Zhong2023MQuAKEAK}. This fundamentally changes the characteristics of the model, increases latency, and alters model behavior for non-QA tasks. An alternate line of work aims to supplement the parametric knowledge of LLMs by augmenting the model prompt with information retrieved from an indexed corpus \cite{lewis2020retrieval} or Knowledge Graphs \cite{markowitz-etal-2024-tree}. 

\subsubsection{Additional Aspects of Model Editing} Other papers have looked into other aspects of contextual knowledge for edits such as reversibility \citep{birdMa2023UntyingTR}, multi-linguality \citep{Wang2023RetrievalaugmentedMK, Si2024MPNLM}, or other implied logical conditions given an edit \citep{easyeditWang2023EasyEditAE, rippleCohen2023EvaluatingTR, depeditLi2023EvaluatingDI}.