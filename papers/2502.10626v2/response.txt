\section{Related Works}
\subsubsection{Direct Model Editing Works} These approaches include aforementioned works **Devlin, "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"** and **Raffel, "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"**. Other works use a hypernetwork to predict parameter updates for a model **Liu, "HyperNetworks"**.

\subsubsection{Retrieval Based Works} Outside of direct model editing approaches **Chen, "Knowledge Graph Attention Model"**, other approaches have sought to adapt to changing knowledge via storing external edit memories and employing various in-context learning and retrieval mechanisms. IKE and others add all edits in the LLM context **Wang, "Iterative Knowledge Editing for Language Models"**; SERAC identifies if a related fact has been edited and retrieves it **Schwartz, "Self-Editing for Pre-Trained Transformers"**; MeLLo and DeepEdit iteratively retrieve edited memories during the generation and decoding process **Tambwekar, "MeLLo: Memory-Augmented Language Learning"**.

We note that these retrieval-based editing approaches are fundamentally different than direct model editing approaches. Direct model editing leaves the model architecture and code entirely the same with the only change being to parameter weights. This means any system developed to use the original model can use the updated model with no other changes. In contrast, retrieval-based editing systems fundamentally change how the model is used and functions. For example, MeLLo requires setting up a retrieval database, iteratively prompting the LLM, making multiple calls to the database, and prompting the LLM to create further sub-questions **Tambwekar, "MeLLo: Memory-Augmented Language Learning"**. This fundamentally changes the characteristics of the model, increases latency, and alters model behavior for non-QA tasks. An alternate line of work aims to supplement the parametric knowledge of LLMs by augmenting the model prompt with information retrieved from an indexed corpus **Hofstetter, "Augmenting Language Models with External Knowledge"** or Knowledge Graphs **Yao, "Knowledge Graph Augmented Language Models"**.

\subsubsection{Additional Aspects of Model Editing} Other papers have looked into other aspects of contextual knowledge for edits such as reversibility **Cohen, "Reversible Nets"**, multi-linguality **Zhang, "Multilingual Language Models"**, or other implied logical conditions given an edit **Li, "Logical Reasoning in Dialogue Systems"**.