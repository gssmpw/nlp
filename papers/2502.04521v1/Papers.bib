@IEEEtranBSTCTL{IEEEexample:BSTcontrol,
CTLuse_forced_etal       = "yes",
CTLmax_names_forced_etal = "10",
CTLnames_show_etal       = "10" }

@inproceedings{fedmd,
     title={{FedMD: Heterogeneous Federated Learning via Model Distillation}},
     author={Daliang Li and Junpu Wang},
     year = {2019},
     booktitle = {FL NeurIPS}
}

@inproceedings{feddf,
      title={Ensemble Distillation for Robust Model Fusion in Federated Learning}, 
      author={Tao Lin and Lingjing Kong and Sebastian U. Stich and Martin Jaggi},
      year={2021},
      booktitle={NeurIPS}
}

@inproceedings{nucleussampling,
  title = {The curious case of neural text degeneration},
  author = {Holtzman, Ari and Buys, Jan and Du, Li and Forbes, Maxwell and Choi, Yejin},
  booktitle = {ICLR},
  year = {2020}
}

@article{guo2022reconformer,
  title={Reconformer: Accelerated mri reconstruction using recurrent transformer},
  author={Guo, Pengfei and Mei, Yiqun and Zhou, Jinyuan and Jiang, Shanshan and Patel, Vishal M},
  journal={IEEE Trans Med Imaging},
  year={2024}
}


@article{haldar2016p,
  title={P-LORAKS: low-rank modeling of local k-space neighborhoods with parallel imaging data},
  author={Haldar, Justin P and Zhuo, Jingwei},
  journal={Magn. Reson. Med.},
  volume={75},
  number={4},
  pages={1499--1514},
  year={2016}
}


@article{wang2024review,
author = {Wang, Shanshan and Wu, Ruoyou and Jia, Sen and Diakite, Alou and Li, Cheng and Liu, Qiegen and Zheng, Hairong and Ying, Leslie},
title = {{Knowledge-driven deep learning for fast MR imaging: Undersampled MR image reconstruction from supervised to un-supervised learning}},
journal = {Magn Reson Med},
volume = {92},
number = {2},
pages = {496-518},
year = {2024}
}


@ARTICLE{chen2022review,
  author={Chen, Yutong and Schönlieb, Carola-Bibiane and Liò, Pietro and Leiner, Tim and Dragotti, Pier Luigi and Wang, Ge and Rueckert, Daniel and Firmin, David and Yang, Guang},
  journal={Proc IEEE}, 
  title={AI-Based Reconstruction for Fast MRI—A Systematic Review and Meta-Analysis}, 
  year={2022},
  volume={110},
  number={2},
  pages={224-245}}

@article{pixelcnnrecon,
author = {Luo, Guanxiong and Zhao, Na and Jiang, Wenhao and Hui, Edward S. and Cao, Peng},
title = {MRI reconstruction using deep Bayesian estimation},
journal = {Magn Reson Med},
volume = {84},
number = {4},
pages = {2246-2261},
keywords = {Bayesian estimation, compressed sensing, deep learning reconstruction, generative network, parallel imaging},
year = {2020}
}


@article{elmas2022federated,
  title={Federated learning of generative image priors for MRI reconstruction},
  author={Elmas, Gokberk and Dar, Salman UH and Korkmaz, Yilmaz and Ceyani, Emir and Susam, Burak and Ozbey, Muzaffer and Avestimehr, Salman and {\c{C}}ukur, Tolga},
  journal={IEEE Trans Med Imaging},
  year={2023},
  volume = {42},
  number = {7},
  pages = {1996--2009}
}

@inproceedings{zhu2021data,
  title={Data-free knowledge distillation for heterogeneous federated learning},
  author={Zhu, Zhuangdi and Hong, Junyuan and Zhou, Jiayu},
  booktitle={ICML},
  pages={12878--12889},
  year={2021}
}

@article{li2020federated,
  title={Federated optimization in heterogeneous networks},
  author={Li, Tian and Sahu, Anit Kumar and Zaheer, Manzil and Sanjabi, Maziar and Talwalkar, Ameet and Smith, Virginia},
  journal={Proc Mach Learn Syst},
  volume={2},
  pages={429--450},
  year={2020}
}

@article{hastie2009multi,
  title={Multi-class adaboost},
  author={Hastie, Trevor and Rosset, Saharon and Zhu, Ji and Zou, Hui},
  journal={Stat Interface},
  volume={2},
  number={3},
  pages={349--360},
  year={2009},
  publisher={International Press of Boston}
}

@inproceedings{mittone2023model,
  title={Model-agnostic federated learning},
  author={Mittone, Gianluca and Riviera, Walter and Colonnelli, Iacopo and Birke, Robert and Aldinucci, Marco},
  booktitle={Eur Conf Para Process},
  pages={383--396},
  year={2023},
  organization={Springer}
}

@article{dalmaz2024one,
  title={One model to unite them all: Personalized federated learning of multi-contrast MRI synthesis},
  author={Dalmaz, Onat and Mirza, Muhammad U and Elmas, Gokberk and Ozbey, Muzaffer and Dar, Salman UH and Ceyani, Emir and Oguz, Kader K and Avestimehr, Salman and {\c{C}}ukur, Tolga},
  journal={Med Image Anal},
  pages={103121},
  year={2024},
  publisher={Elsevier}
}


@article{FedNAS,
  title={Generalizable Reconstruction for Accelerating MR Imaging via Federated Learning with Neural Architecture Search},
 author={Wu, Ruoyou and Li, Cheng and Zou, Juan and Liu, Xiaoqiang and Zheng, Hao and Wang, Shanshan},
  journal={IEEE Trans Med Imaging}, 
  year={2024},
 doi = {10.1109/TMI.2024.3432388}
}

@article{elsken2019neural,
  title={Neural architecture search: A survey},
  author={Elsken, Thomas and Metzen, Jan Hendrik and Hutter, Frank},
  journal={J Mach Learn Res},
  volume={20},
  number={55},
  pages={1--21},
  year={2019}
}

@article{kaissis2020secure,
  title={Secure, privacy-preserving and federated machine learning in medical imaging},
  author={Kaissis, Georgios A and Makowski, Marcus R and R{\"u}ckert, Daniel and Braren, Rickmer F},
  journal={Nat Mach Intelli},
  volume={2},
  number={6},
  pages={305--311},
  year={2020},
  publisher={Nature Publishing Group UK London}
}

@inproceedings{ganleaks,
author = {Chen, Dingfan and Yu, Ning and Zhang, Yang and Fritz, Mario},
title = {{GAN-Leaks: A Taxonomy of Membership Inference Attacks against Generative Models}},
year = {2020},
publisher = {ACM},
address = {USA},
booktitle = {SIGSAC Conf Comput Commun Sec},
pages = {343–362}}


@ARTICLE{l1SPIRiT,
  author={Murphy, Mark and Alley, Marcus and Demmel, James and Keutzer, Kurt and Vasanawala, Shreyas and Lustig, Michael},
  journal={IEEE Trans Med Imaging}, 
  title={{Fast $\ell_1$ -SPIRiT Compressed Sensing Parallel Imaging MRI: Scalable Parallel Implementation and Clinically Feasible Runtime}}, 
  year={2012},
  volume={31},
  number={6},
  pages={1250-1262}}


@article{progan,
  author    = {Tero Karras and
               Timo Aila and
               Samuli Laine and
               Jaakko Lehtinen},
  title     = {{Progressive Growing of GANs for Improved Quality, Stability, and Variation}},
  booktitle={ICLR},
  journal   = {arXiv:1710.10196},
  year      = {2018}}
@inproceedings{progan,
  title={Progressive growing of {GANs} for improved quality, stability, and variation},
  author={Karras, Tero and Aila, Timo and Laine, Samuli and Lehtinen, Jaakko},
  booktitle={ICLR},
  year={2018},
}



@article{wang2022,
author = {Wang, Shanshan and Ke, Ziwen and Cheng, Huitao and Jia, Sen and Ying, Leslie and Zheng, Hairong and Liang, Dong},
title = {{DIMENSION: Dynamic MR imaging with both k-space and spatial prior knowledge obtained via multi-supervised network training}},
journal = {NMR in Biomed},
volume = {35},
number = {4},
pages = {e4131},
year = {2022}
}



@article{wang2022b,
  title={Parcel: Physics-based unsupervised contrastive representation learning for multi-coil MR imaging},
  author={Wang, Shanshan and Wu, Ruoyou and Li, Cheng and Zou, Juan and Zhang, Zhen and Liu, Qian and Xi, Yiqing and Zheng, Hao},
  journal={IEEE/ACM Trans Comput Biol Bioinf},
  volume={20},
  number={5},
  pages={2659--2670},
  year={2022},
  publisher={IEEE}
}

@InProceedings{hu2021,
author="Hu, Chen
and Li, Cheng
and Wang, Haifeng
and Liu, Qiegen
and Zheng, Hairong
and Wang, Shanshan",
title={{Self-supervised Learning for MRI Reconstruction with a Parallel Network Training Framework}},
booktitle={MICCAI},
year="2021",
publisher="Springer",
address="Cham",
pages="382--391"
}

@article{MTrans,
  title={Multimodal Transformer for Accelerated MR Imaging},
  author={Feng, Chun-Mei and Yan, Yunlu and Chen, Geng and Fu, Huazhu and Xu, Yong and Shao, Ling},
  journal={IEEE Trans Med Imaging},
  volume={42},
  number={10},
  pages={2804--2816},
  year={2023},
  month={October},
  publisher={IEEE}
}

@article{dar2023parallel,
  title={Parallel-stream fusion of scan-specific and scan-general priors for learning deep MRI reconstruction in low-data regimes},
  author={Dar, Salman Ul Hassan and {\"O}zt{\"u}rk, {\c{S}}aban and {\"O}zbey, Muzaffer and Oguz, Kader Karli and {\c{C}}ukur, Tolga},
  journal={Comput Biol Med},
  volume={167},
  pages={107610},
  year={2023},
  publisher={Elsevier}
}

@article{zhang2024robmednas,
  title={RobMedNAS: searching robust neural network architectures for medical image synthesis},
  author={Zhang, Jinnian and Chen, Weijie and Joshi, Tanmayee and Uyanik, Meltem and Zhang, Xiaomin and Loh, Po-Ling and Jog, Varun and Bruce, Richard and Garrett, John and McMillan, Alan B},
  journal={Biomed Phys Eng Express},
  year={2024}
}

@article{aggregate1,
  author    = {Hongyi Wang and
               Mikhail Yurochkin and
               Yuekai Sun and
               Dimitris S. Papailiopoulos and
               Yasaman Khazaeni},
  title     = {Federated Learning with Matched Averaging},
booktitle={ICLR},
  year      = {2020},
}

@inproceedings{
FedFomo,
title={Personalized Federated Learning with First Order Model Optimization},
author={Michael Zhang and Karan Sapra and Sanja Fidler and Serena Yeung and Jose M. Alvarez},
booktitle={ICLR},
year={2021}
}

@article{pFedHL,
 author = {Ma, Xiaosong and Zhang, Jie and Guo, Song and Xu, Wenchao},
 title = {Layer-wised Model Aggregation for Personalized Federated Learning},
booktitle={CVPR},
 year = {2022},
 }


@book{kairouz2021advances,
      title={Advances and Open Problems in Federated Learning}, 
      author={Peter Kairouz and H. Brendan McMahan and Brendan Avent and Aurélien Bellet and Mehdi Bennis and Arjun Nitin Bhagoji and Kallista Bonawitz and Zachary Charles and Graham Cormode and Rachel Cummings and Rafael G. L. D'Oliveira and Hubert Eichner and Salim El Rouayheb and David Evans and Josh Gardner and Zachary Garrett and Adrià Gascón and Badih Ghazi and Phillip B. Gibbons and Marco Gruteser and Zaid Harchaoui and Chaoyang He and Lie He and Zhouyuan Huo and Ben Hutchinson and Justin Hsu and Martin Jaggi and Tara Javidi and Gauri Joshi and Mikhail Khodak and Jakub Konečný and Aleksandra Korolova and Farinaz Koushanfar and Sanmi Koyejo and Tancrède Lepoint and Yang Liu and Prateek Mittal and Mehryar Mohri and Richard Nock and Ayfer Özgür and Rasmus Pagh and Mariana Raykova and Hang Qi and Daniel Ramage and Ramesh Raskar and Dawn Song and Weikang Song and Sebastian U. Stich and Ziteng Sun and Ananda Theertha Suresh and Florian Tramèr and Praneeth Vepakomma and Jianyu Wang and Li Xiong and Zheng Xu and Qiang Yang and Felix X. Yu and Han Yu and Sen Zhao},
     booktitle={Advances and Open Problems in Federated Learning},
     year={2021},
}

@misc{fl_opt_guide,
      title={A Field Guide to Federated Optimization}, 
      author={Jianyu Wang and Zachary Charles and Zheng Xu and Gauri Joshi and H. Brendan McMahan and Blaise Aguera y Arcas and Maruan Al-Shedivat and Galen Andrew and Salman Avestimehr and Katharine Daly and Deepesh Data and Suhas Diggavi and Hubert Eichner and Advait Gadhikar and Zachary Garrett and Antonious M. Girgis and Filip Hanzely and Andrew Hard and Chaoyang He and Samuel Horvath and Zhouyuan Huo and Alex Ingerman and Martin Jaggi and Tara Javidi and Peter Kairouz and Satyen Kale and Sai Praneeth Karimireddy and Jakub Konecny and Sanmi Koyejo and Tian Li and Luyang Liu and Mehryar Mohri and Hang Qi and Sashank J. Reddi and Peter Richtarik and Karan Singhal and Virginia Smith and Mahdi Soltanolkotabi and Weikang Song and Ananda Theertha Suresh and Sebastian U. Stich and Ameet Talwalkar and Hongyi Wang and Blake Woodworth and Shanshan Wu and Felix X. Yu and Honglin Yuan and Manzil Zaheer and Mi Zhang and Tong Zhang and Chunxiang Zheng and Chen Zhu and Wennan Zhu},
      year={2021},
      eprint={2107.06917},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@InProceedings{PatelRecon,
author="Guo, Pengfei
and Valanarasu, Jeya Maria Jose
and Wang, Puyang
and Zhou, Jinyuan
and Jiang, Shanshan
and Patel, Vishal M.",
title="Over-and-Under Complete Convolutional RNN for MRI Reconstruction",
booktitle={MICCAI},
year="2021",
pages="13--23",
abstract="Reconstructing magnetic resonance (MR) images from under-sampled data is a challenging problem due to various artifacts introduced by the under-sampling operation. Recent deep learning-based methods for MR image reconstruction usually leverage a generic auto-encoder architecture which captures low-level features at the initial layers and high-level features at the deeper layers. Such networks focus much on global features which may not be optimal to reconstruct the fully-sampled image. In this paper, we propose an Over-and-Under Complete Convolutional Recurrent Neural Network (OUCR), which consists of an overcomplete and an undercomplete Convolutional Recurrent Neural Network (CRNN). The overcomplete branch gives special attention in learning local structures by restraining the receptive field of the network. Combining it with the undercomplete branch leads to a network which focuses more on low-level features without losing out on the global structures. Extensive experiments on two datasets demonstrate that the proposed method achieves significant improvements over the compressed sensing and popular deep learning-based methods with less number of trainable parameters."
}

@article{FedGAN,
  author    = {Mohammad Rasouli and
               Tao Sun and
               Ram Rajagopal},
  title     = {FedGAN: Federated Generative Adversarial Networks for Distributed
               Data},
  year      = {2020},
  journal = {arXiv:2006.07228}
}

@article{FedDPGAN,
author = {Zhang, Longling and Shen, Bochen and Barnawi, Ahmed and Xi, Shan and Kumar, Neeraj and Wu, Yi},
journal = {Inf Syst Front},
number = {6},
pages = {1403--1415},
title = {{FedDPGAN: Federated Differentially Private Generative Adversarial Networks Framework for the Detection of COVID-19 Pneumonia}},
volume = {23},
year = {2021}
}

@INPROCEEDINGS {MDGAN,
author = {C. Hardy and E. Le Merrer and B. Sericola},
booktitle = {{IPDPS}},
title = {{MD-GAN: Multi-Discriminator Generative Adversarial Networks for Distributed Datasets}},
year = {2019},
volume = {},
issn = {},
pages = {866-877}
}

@article{ziller2021,
      title={Complex-valued deep learning with differential privacy}, 
      author={Alexander Ziller and Dmitrii Usynin and Moritz Knolle and Kerstin Hammernik and Daniel Rueckert and Georgios Kaissis},
      year={2021},
      journal={arXiv:2110.03478}
}

@ARTICLE{VAFL,
  author={Yan, Zengqiang and Wicaksana, Jeffry and Wang, Zhiwei and Yang, Xin and Cheng, Kwang-Ting},
  journal=IEEE_J_BHI, 
  title={Variation-Aware Federated Learning With Multi-Source Decentralized Medical Image Data}, 
  year={2021},
  volume={25},
  number={7},
  pages={2615-2628}
  }

@ARTICLE{LiangSPM,
  author={Liang, Dong and Cheng, Jing and Ke, Ziwen and Ying, Leslie},
  journal=SPM, 
  title={Deep Magnetic Resonance Image Reconstruction: Inverse Problems Meet Neural Networks}, 
  year={2020},
  volume={37},
  number={1},
  pages={141-151}
  }
  

@article{Zhao2015,
author = {Zhao, Bo and Lu, Wenmiao and Hitchens, T. Kevin and Lam, Fan and Ho, Chien and Liang, Zhi-Pei},
title = {Accelerated MR parameter mapping with low-rank and sparsity constraints},
journal = MRM,
volume = {74},
number = {2},
pages = {489-498},
keywords = {constrained reconstruction, low-rank constraint, joint sparsity constraint, parameter mapping, T1 mapping, T2 mapping},
abstract = {Purpose To enable accurate magnetic resonance (MR) parameter mapping with accelerated data acquisition, utilizing recent advances in constrained imaging with sparse sampling. Theory and Methods A new constrained reconstruction method based on low-rank and sparsity constraints is proposed to accelerate MR parameter mapping. More specifically, the proposed method simultaneously imposes low-rank and joint sparse structures on contrast-weighted image sequences within a unified mathematical formulation. With a pre-estimated subspace, this formulation results in a convex optimization problem, which is solved using an efficient numerical algorithm based on the alternating direction method of multipliers. Results To evaluate the performance of the proposed method, two application examples were considered: (i) T2 mapping of the human brain and (ii) T1 mapping of the rat brain. For each application, the proposed method was evaluated at both moderate and high acceleration levels. Additionally, the proposed method was compared with two state-of-the-art methods that only use a single low-rank or joint sparsity constraint. The results demonstrate that the proposed method can achieve accurate parameter estimation with both moderately and highly undersampled data. Although all methods performed fairly well with moderately undersampled data, the proposed method achieved much better performance (e.g., more accurate parameter values) than the other two methods with highly undersampled data. Conclusions Simultaneously imposing low-rank and sparsity constraints can effectively improve the accuracy of fast MR parameter mapping with sparse sampling. Magn Reson Med 74:489–498, 2015. © 2014 Wiley Periodicals, Inc.},
year = {2015}
}

@ARTICLE{Xiang2019,
  author={Xiang, Lei and Chen, Yong and Chang, Weitang and Zhan, Yiqiang and Lin, Weili and Wang, Qian and Shen, Dinggang},
  journal={IEEE Trans Biomed Eng}, 
  title={Deep-Learning-Based Multi-Modal Fusion for Fast MR Reconstruction}, 
  year={2019},
  volume={66},
  number={7},
  pages={2105--2114}
  }
  
 
@article{FengLiu2021,
author = {Liu, Fang and Kijowski, Richard and El Fakhri, Georges and Feng, Li},
title = {Magnetic resonance parameter mapping using model-guided self-supervised deep learning},
journal = MRM,
volume = {85},
number = {6},
pages = {3211--3226},
year = {2021}
}  
%Can be cited as a general federated learning paper
@article{dayan2021federated,
  title={Federated learning for predicting clinical outcomes in patients with COVID-19},
  author={Dayan, Ittai and Roth, Holger R. and Zhong, Aoxiao and Harouni, Ahmed and Gentili, Amilcare and Abidin, Anas Z. and Liu, Andrew and Costa, Anthony Beardsworth and Wood, Bradford J. and Tsai, Chien-Sung and Wang, Chih-Hung and Hsu, Chun-Nan and Lee, C. K. and Ruan, Peiying and Xu, Daguang and Wu, Dufan and Huang, Eddie and Kitamura, Felipe Campos and Lacey, Griffin and de Antônio Corradi, Gustavo César and Nino, Gustavo and Shin, Hao-Hsin and Obinata, Hirofumi and Ren, Hui and Crane, Jason C. and Tetreault, Jesse and Guan, Jiahui and Garrett, John W. and Kaggie, Joshua D. and Park, Jung Gil and Dreyer, Keith and Juluru, Krishna and Kersten, Kristopher and Rockenbach, Marcio Aloisio Bezerra Cavalcanti and Linguraru, Marius George and Haider, Masoom A. and AbdelMaseeh, Meena and Rieke, Nicola and Damasceno, Pablo F. and e Silva, Pedro Mario Cruz and Wang, Pochuan and Xu, Sheng and Kawano, Shuichi and Sriswasdi, Sira and Park, Soo Young and Grist, Thomas M. and Buch, Varun and Jantarabenjakul, Watsamon and Wang, Weichung and Tak, Won Young and Li, Xiang and Lin, Xihong and Kwon, Young Joon and Quraini, Abood and Feng, Andrew and Priest, Andrew N. and Turkbey, Baris and Glicksberg, Benjamin and Bizzo, Bernardo and Kim, Byung Seok and Tor-Díez, Carlos and Lee, Chia-Cheng and Hsu, Chia-Jung and Lin, Chin and Lai, Chiu-Ling and Hess, Christopher P. and Compas, Colin and Bhatia, Deepeksha and Oermann, Eric K. and Leibovitz, Evan and Sasaki, Hisashi and Mori, Hitoshi and Yang, Isaac and Sohn, Jae Ho and Murthy, Krishna Nand Keshava and Fu, Li-Chen and de Mendonça, Matheus Ribeiro Furtado and Fralick, Mike and Kang, Min Kyu and Adil, Mohammad and Gangai, Natalie and Vateekul, Peerapon and Elnajjar, Pierre and Hickman, Sarah and Majumdar, Sharmila and McLeod, Shelley L. and Reed, Sheridan and Gräf, Stefan and Harmon, Stephanie and Kodama, Tatsuya and Puthanakit, Thanyawee and Mazzulli, Tony and de Lavor, Vitor Lima and Rakvongthai, Yothin and Lee, Yu Rim and Wen, Yuhong and Gilbert, Fiona J. and Flores, Mona G. and Li, Quanzheng},
  journal={Nat Med},
  volume={27},
  number={10},
  pages={1735--1743},
  year={2021}}
  
%Here they weigh update from each client (alpha) based on how different it is from the average weights. Alpha is higher for site with weights similar to the average weights.
@InProceedings{IDA_FL,
author="Yeganeh, Yousef and Farshad, Azade and Navab, Nassir and Albarqouni, Shadi",
title="Inverse Distance Aggregation for Federated Learning with Non-IID Data",
year="2020",
booktitle = {DART},
publisher="Springer",
address="Cham",
pages="150--159"
}




@article{liang2020think,
      title={Think Locally, Act Globally: Federated Learning with Local and Global Representations}, 
      author={Paul Pu Liang and Terrance Liu and Liu Ziyin and Nicholas B. Allen and Randy P. Auerbach and David Brent and Ruslan Salakhutdinov and Louis-Philippe Morency},
      year={2020},
      journal={arXiv:2001.01523},
}

@ARTICLE{BRATS,
  author={Menze, Bjoern H. and Jakab, Andras and Bauer, Stefan and Kalpathy-Cramer, Jayashree and Farahani, Keyvan and Kirby, Justin and Burren, Yuliya and Porz, Nicole and Slotboom, Johannes and Wiest, Roland and Lanczi, Levente and Gerstner, Elizabeth and Weber, Marc-André and Arbel, Tal and Avants, Brian B. and Ayache, Nicholas and Buendia, Patricia and Collins, D. Louis and Cordier, Nicolas and Corso, Jason J. and Criminisi, Antonio and Das, Tilak and Delingette, Hervé and Demiralp, Çağatay and Durst, Christopher R. and Dojat, Michel and Doyle, Senan and Festa, Joana and Forbes, Florence and Geremia, Ezequiel and Glocker, Ben and Golland, Polina and Guo, Xiaotao and Hamamci, Andac and Iftekharuddin, Khan M. and Jena, Raj and John, Nigel M. and Konukoglu, Ender and Lashkari, Danial and Mariz, José António and Meier, Raphael and Pereira, Sérgio and Precup, Doina and Price, Stephen J. and Raviv, Tammy Riklin and Reza, Syed M. S. and Ryan, Michael and Sarikaya, Duygu and Schwartz, Lawrence and Shin, Hoo-Chang and Shotton, Jamie and Silva, Carlos A. and Sousa, Nuno and Subbanna, Nagesh K. and Szekely, Gabor and Taylor, Thomas J. and Thomas, Owen M. and Tustison, Nicholas J. and Unal, Gozde and Vasseur, Flor and Wintermark, Max and Ye, Dong Hye and Zhao, Liang and Zhao, Binsheng and Zikic, Darko and Prastawa, Marcel and Reyes, Mauricio and Van Leemput, Koen},
  journal=TMI, 
  title={The Multimodal Brain Tumor Image Segmentation Benchmark (BRATS)}, 
  year={2015},
  volume={34},
  number={10},
  pages={1993-2024}}



@InProceedings{pmlr-v80-mescheder18a,
  title = 	 {Which Training Methods for {GAN}s do actually Converge?},
  author =       {Mescheder, Lars and Geiger, Andreas and Nowozin, Sebastian},
  booktitle = 	 {ICML},
  pages = 	 {3481--3490},
  year = 	 {2018},
  volume = 	 {80},
  abstract = 	 {Recent work has shown local convergence of GAN training for absolutely continuous data and generator distributions. In this paper, we show that the requirement of absolute continuity is necessary: we describe a simple yet prototypical counterexample showing that in the more realistic case of distributions that are not absolutely continuous, unregularized GAN training is not always convergent. Furthermore, we discuss regularization strategies that were recently proposed to stabilize GAN training. Our analysis shows that GAN training with instance noise or zero-centered gradient penalties converges. On the other hand, we show that Wasserstein-GANs and WGAN-GP with a finite number of discriminator updates per generator update do not always converge to the equilibrium point. We discuss these results, leading us to a new explanation for the stability problems of GAN training. Based on our analysis, we extend our convergence results to more general GANs and prove local convergence for simplified gradient penalties even if the generator and data distributions lie on lower dimensional manifolds. We find these penalties to work well in practice and use them to learn high-resolution generative image models for a variety of datasets with little hyperparameter tuning.}
}


@INPROCEEDINGS{FengICCV2021,
title={When Do {GANs} Replicate? On the Choice of Dataset Size},
pages = {6701-6710},
year = {2021},
booktitle = {ICCV}, 
author = {Quanli Feng and Chenqi Guo and Fabian Benitez-Quiroz and Aleix Martinez}
}

@inproceedings{
li2021fedbn,
title={Fed{BN}: Federated Learning on Non-{IID} Features via Local Batch Normalization},
author={Xiaoxiao Li and Meirui Jiang and Xiaofei Zhang and Michael Kamp and Qi Dou},
booktitle={ICLR},
year={2021}
}

%differential privacy
@INPROCEEDINGS {DPCGAN,
author = {R. Torkzadehmahani and P. Kairouz and B. Paten},
booktitle = {CVPR},
title = {{DP-CGAN: Differentially Private Synthetic Data and Label Generation}},
year = {2019},
volume = {},
pages = {98-104},
keywords = {privacy;gallium nitride;data models;training;generators;generative adversarial networks}
}

@article{Sewon2021,
title = {Fat-saturated image generation from multi-contrast MRIs using generative adversarial networks with Bloch equation-based autoencoder regularization},
journal = MEDIA,
volume = {73},
pages = {102198},
year = {2021},
author = {Sewon Kim and Hanbyol Jang and Seokjun Hong and Yeong Sang Hong and Won C. Bae and Sungjun Kim and Dosik Hwang}
}
@article{transms,
      title={{TranSMS: Transformers for Super-Resolution Calibration in Magnetic Particle Imaging}}, 
      author={Alper Güngör and Baris Askin and Damla Alptekin Soydan and Emine Ulku Saritas and Can Barış Top and Tolga Çukur},
      year={2021},
      journal={arXiv:2111.02163}
}
%%optimizers

@article{optimizersoverview,
  title={An overview of gradient descent optimization algorithms},
  author={Ruder, Sebastian},
  journal={arXiv:1609.04747},
  year={2016}
}
%%%%Light transformers
@article{dalmaz2021resvit,
      title={{ResViT: R}esidual vision transformers for multi-modal medical image synthesis}, 
      author={Onat Dalmaz and Mahmut Yurt and Tolga \c{C}ukur},
      year={2022},
      journal={IEEE Trans. Med. Imaging},
      volume = {41},
      number = {10},
      pages={2598-2614}
}


@article{yang2021focal,
  title={Focal self-attention for local-global interactions in vision transformers},
  author={Yang, Jianwei and Li, Chunyuan and Zhang, Pengchuan and Dai, Xiyang and Xiao, Bin and Yuan, Lu and Gao, Jianfeng},
  journal={arXiv:2107.00641},
  year={2021}
}

@inproceedings{liu2021Swin,
  title={Swin Transformer: Hierarchical Vision Transformer using Shifted Windows},
  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  journal={ICCV},
  year={2021}
}

@inproceedings{vaswani2021scaling,
  title={Scaling local self-attention for parameter efficient visual backbones},
  author={Vaswani, Ashish and Ramachandran, Prajit and Srinivas, Aravind and Parmar, Niki and Hechtman, Blake and Shlens, Jonathon},
  booktitle={CVPR},
  pages={12894--12904},
  year={2021}
}

@InProceedings{Zhang_2021_ICCV,
    author    = {Zhang, Pengchuan and Dai, Xiyang and Yang, Jianwei and Xiao, Bin and Yuan, Lu and Zhang, Lei and Gao, Jianfeng},
    title     = {Multi-Scale Vision Longformer: A New Vision Transformer for High-Resolution Image Encoding},
    booktitle = {ICCV},
    month     = {October},
    year      = {2021},
    pages     = {2998-3008}
}

@InProceedings{Wang_2021_ICCV,
    author    = {Wang, Wenhai and Xie, Enze and Li, Xiang and Fan, Deng-Ping and Song, Kaitao and Liang, Ding and Lu, Tong and Luo, Ping and Shao, Ling},
    title     = {Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction Without Convolutions},
    booktitle = {ICCV},
    month     = {October},
    year      = {2021},
    pages     = {568-578}
}
%%%%%


@article{flowgan,
  author    = {Aditya Grover and
               Manik Dhar and
               Stefano Ermon},
  title     = {Flow-GAN: Bridging implicit and prescribed learning in generative
               models},
  journal   = {arXiv:1705.08868},
  year      = {2017}
}

@article{lorakstoolbox,
  title={LORAKS software version 2.0: Faster implementation and enhanced capabilities},
  author={Kim, TH and Haldar, JP},
  journal={University of Southern California, Los Angeles, CA, Tech. Rep. USC-SIPI-443},
  year={2018}
}

@article{Haldar2016,
abstract = {Purpose To propose and evaluate P-LORAKS a new calibrationless parallel imaging reconstruction framework. Theory and Methods LORAKS is a flexible and powerful framework that was recently proposed for constrained MRI reconstruction. LORAKS was based on the observation that certain matrices constructed from fully sampled k-space data should have low rank whenever the image has limited support or smooth phase, and made it possible to accurately reconstruct images from undersampled or noisy data using low-rank regularization. This paper introduces P-LORAKS, which extends LORAKS to the context of parallel imaging. This is achieved by combining the LORAKS matrices from different channels to yield a larger but more parsimonious low-rank matrix model of parallel imaging data. This new model can be used to regularize the reconstruction of undersampled parallel imaging data, and implicitly imposes phase, support, and parallel imaging constraints without needing to calibrate phase, support, or sensitivity profiles. Results The capabilities of P-LORAKS are evaluated with retrospectively undersampled data and compared against existing parallel MRI reconstruction methods. Results show that P-LORAKS can improve parallel imaging reconstruction quality, and can enable the use of new k-space trajectories that are not compatible with existing reconstruction methods. Conclusion The P-LORAKS framewok provides a new and effective way to regularize parallel imaging reconstruction.},
author = {Haldar, Justin P. and Zhuo, Jingwei},
journal = MRM,
number = {4},
pages = {1499},
title = {{P-LORAKS: Low-Rank Modeling of Local k-Space Neighborhoods with Parallel Imaging Data}},
volume = {75},
year = {2016}
}


@article{Haldar2014,
abstract = {Recent theoretical results on low-rank matrix reconstruction have inspired significant interest in low-rank modeling of MRI images. Existing approaches have focused on higher-dimensional scenarios with data available from multiple channels, timepoints, or image contrasts. The present work demonstrates that single-channel, single-contrast, single-timepoint k-space data can also be mapped to low-rank matrices when the image has limited spatial support or slowly varying phase. Based on this, we develop a novel and flexible framework for constrained image reconstruction that uses low-rank matrix modeling of local k-space neighborhoods (LORAKS). A new regularization penalty and corresponding algorithm for promoting low-rank are also introduced. The potential of LORAKS is demonstrated with simulated and experimental data for a range of denoising and sparse-sampling applications. LORAKS is also compared against state-of-the-art methods like homodyne reconstruction, l1-norm minimization, and total variation minimization, and is demonstrated to have distinct features and advantages. In addition, while calibration-based support and phase constraints are commonly used in existing methods, the LORAKS framework enables calibrationless use of these constraints.},
author = {Haldar, Justin P},
journal = TMI,
number = {3},
pages = {668--681},
title = {{Low-Rank Modeling of Local k-Space Neighborhoods (LORAKS) for Constrained MRI}},
volume = {33},
year = {2014}
}


@article{Bydder2002,
abstract = {It is well established that the optimal unbiased way to combine image data from array coils is a pixel-by-pixel sum of coil signals, with each signal weighted by the individual coil sensitivity at the location of the pixel. A pragmatic alternative combines the images from the coils as the square root of the sum of squares (SOS), which can reduce the signal-to-noise ratio (SNR) and introduce bias. This work describes how to replace coil sensitivity by an image-derived quantity that enables close to optimal signal combination up to a global intensity scaling. Typical scaling is by an individual coil sensitivity or a linear or SOS combination of the sensitivities of some or all of the coils in the array. The method decreases signal bias, improves SNR when coils have unequal noise levels, and can reduce image artifacts. It can produce phase-corrected data, which eliminates bias completely. In addition, the method allows images from arrays that include highly localized coils, such as a prostate coil and external pelvic array, to be combined with near-optimal SNR and an intensity modulation that makes them easier to view.},
author = {Bydder, M and Larkman, D J and Hajnal, J V},
journal = MRM,
number = {3},
pages = {539--548},
title = {{Combination of signals from array coils using image-based estimation of coil sensitivity profiles}},
volume = {47},
year = {2002}
}



@article{XLSA18,
 title={Zero-Shot Learning - A Comprehensive Evaluation of the Good, the Bad and the Ugly},
 author={Xian, Yongqin and Lampert, H. Christoph and Schiele, Bernt and Akata, Zeynep},
 journal={TPAMI},
 year={2018},
}

@book{Bishop2006,
abstract = {The field of pattern recognition has undergone substantial development over the years. This book reflects these developments while providing a grounding in the basic concepts of pattern recognition and machine learning. It is aimed at advanced undergraduates or first year PhD students, as well as researchers and practitioners.},
author = {Bishop, Christopher M},
isbn = {9780387310732},
keywords = {r01239,r01307,r01308,r01309},
language = {English},
mendeley-tags = {r01239,r01307,r01308,r01309},
pages = {738},
publisher = {Springer Verlag},
shorttitle = {Information Science and Statistics},
title = {{Pattern recognition and machine learning}},
year = {2006}
}


@INPROCEEDINGS{Shiqian2008,
  author={Shiqian Ma and Wotao Yin and Yin Zhang and Chakraborty, Amit},
  booktitle={2008 IEEE Conference on Computer Vision and Pattern Recognition}, 
  title={An efficient algorithm for compressed MR imaging using total variation and wavelets}, 
  year={2008},
  volume={},
  number={},
  pages={1-8}
  }
  
  @ARTICLE{Jin2016,
  author={Jin, Kyong Hwan and Lee, Dongwook and Ye, Jong Chul},
  journal=TCI, 
  title={A General Framework for Compressed Sensing and Parallel MRI Using Annihilating Filter Based Low-Rank Hankel Matrix}, 
  year={2016},
  volume={2},
  number={4},
  pages={480-495},
}

@article{Zhang2020,
title = {Image reconstruction with low-rankness and self-consistency of k-space data in parallel MRI},
journal = MEDIA,
volume = {63},
pages = {101687},
year = {2020},
author = {Xinlin Zhang and Di Guo and Yiman Huang and Ying Chen and Liansheng Wang and Feng Huang and Qin Xu and Xiaobo Qu}
}

@article{Zhang2021,
title = {A guaranteed convergence analysis for the projected fast iterative soft-thresholding algorithm in parallel MRI},
journal = MEDIA,
volume = {69},
pages = {101987},
year = {2021},
author = {Xinlin Zhang and Hengfa Lu and Di Guo and Lijun Bao and Feng Huang and Qin Xu and Xiaobo Qu}
}


@InProceedings{Asim2020,
  title = 	 {Invertible generative models for inverse problems: mitigating representation error and dataset bias},
  author =       {Asim, Muhammad and Daniels, Max and Leong, Oscar and Ahmed, Ali and Hand, Paul},
  booktitle = 	 {ICML},
  pages = 	 {399--409},
  year = 	 {2020},
  volume = 	 {119}
  }


@article{Huang2021,
author = {Huang, Wen and Hand, Paul and Heckel, Reinhard and Voroninski, Vladislav},
title = {{A Provably Convergent Scheme for Compressive Sensing Under Random Generative Priors}},
journal = {J Fourier Anal Appl},
year = {2021},
volume = {27},
number = {2},
pages = {19}
}

@article{Darestani2021,
  title={Accelerated MRI With Un-Trained Neural Networks},
  author={Mohammad Zalbagi Darestani and Reinhard Heckel},
  journal=TCI,
  year={2021},
  volume={7},
  pages={724-733}
}

@article{Heckel2020,
  author    = {Reinhard Heckel and
               Mahdi Soltanolkotabi},
  title     = {Compressive sensing with un-trained neural networks: Gradient descent
               finds the smoothest approximation},
  year      = {2020},
  journal = {arXiv:2005.03991}
}

@article{Liu2020mrm,
author = {Liu, Qiegen and Yang, Qingxin and Cheng, Huitao and Wang, Shanshan and Zhang, Minghui and Liang, Dong},
title = {Highly undersampled magnetic resonance imaging reconstruction using autoencoding priors},
journal = MRM,
volume = {83},
number = {1},
pages = {322-336},
year = {2020}
}


@article{Eun2020tt,
author = {Eun, Da-in and Jang, Ryoungwoo and Ha, Woo Seok and Lee, Hyunna and Jung, Seung Chai and Kim, Namkug},
title = {{Deep-learning-based image quality enhancement of compressed sensing magnetic resonance imaging of vessel wall: comparison of self-supervised and unsupervised approaches}},
journal = {Scientific Reports},
year = {2020},
volume = {10},
number = {1},
pages = {13950}
}

@inproceedings{FedAvg,
  title={Communication-Efficient Learning of Deep Networks from Decentralized Data},
  author={H. Brendan McMahan and Eider Moore and Daniel Ramage and Seth Hampson and Blaise Ag{\"u}era y Arcas},
  booktitle={ICAIS},
  year={2017}
}

@INPROCEEDINGS{Liu2021,
author = {Q. Liu and C. Chen and J. Qin and Q. Dou and P. Heng},
booktitle = {CVPR},
title = {FedDG: Federated Domain Generalization on Medical Image Segmentation via Episodic Learning in Continuous Frequency Space},
year = {2021},
pages = {1013-1023},
keywords = {image segmentation;interpolation;privacy;hospitals;frequency-domain analysis;predictive models;collaborative work}
}


@article{Han2020,
author = {Tianyu Han  and Sven Nebelung  and Christoph Haarburger  and Nicolas Horst  and Sebastian Reinartz  and Dorit Merhof  and Fabian Kiessling  and Volkmar Schulz  and Daniel Truhn},
title = {Breaking medical data sharing boundaries by using synthesized radiographs},
journal = {Sci Adv},
volume = {6},
number = {49},
pages = {eabb7973},
year = {2020}
}


@article{Li2020FederatedLC,
  title={Federated Learning: Challenges, Methods, and Future Directions},
  author={Tian Li and Anit Kumar Sahu and Ameet S. Talwalkar and Virginia Smith},
  journal=SPM,
  year={2020},
  volume={37},
  pages={50-60}
}

@article{Li2020,
title = {Multi-site fMRI analysis using privacy-preserving federated learning and domain adaptation: ABIDE results},
journal = MEDIA,
volume = {65},
pages = {101765},
year = {2020},
author = {Xiaoxiao Li and Yufeng Gu and Nicha Dvornek and Lawrence H. Staib and Pamela Ventola and James S. Duncan},
keywords = {Federated learning, Domain adaptation, Data sharing, Privacy, Rs-fmri, ABIDE}
}

@inproceedings{WenqiLi2019,
title = "Privacy-Preserving Federated Brain Tumour Segmentation",
author = "Wenqi Li and Fausto Milletar{\`i} and Daguang Xu and Nicola Rieke and Jonny Hancox and Wentao Zhu and Maximilian Baust and Yan Cheng and S{\'e}bastien Ourselin and Cardoso, {M. Jorge} and Andrew Feng",
year = "2019",
pages = "133--141",
booktitle = "MLMI"}

@inproceedings{Roth2020,
   title={{Federated Learning for Breast Density Classification: A Real-World Implementation}},
   booktitle={DART},
   author={Roth, Holger R. and Chang, Ken and Singh, Praveer and Neumark, Nir and Li, Wenqi and Gupta, Vikash and Gupta, Sharut and Qu, Liangqiong and Ihsani, Alvin and Bizzo, Bernardo C. and et al.},
   year={2020},
   pages={181–191}
}

@ARTICLE{GuoTMI2021,
  author={Guo, Pengfei and Wang, Puyang and Yasarla, Rajeev and Zhou, Jinyuan and Patel, Vishal M. and Jiang, Shanshan},
  journal=TMI, 
  title={Anatomic and Molecular MR Image Synthesis Using Confidence Guided CNNs}, 
  year={2021},
  volume={40},
  number={10},
  pages={2832-2844}}
  
@article{FengNNLS2021,
author = {Feng, Chun-Mei and Yang, Zhanyuan and Fu, Huazhu and xu, Yong and Yang, Jian and Shao, Ling},
year = {2021},
month = {07},
pages = {1-11},
title = {DONet: Dual-Octave Network for Fast MR Image Reconstruction},
volume = {PP},
journal = IEEE_J_NNLS,
}

@InProceedings{Sheller2019,
author="Sheller, Micah J.
and Reina, G. Anthony
and Edwards, Brandon
and Martin, Jason
and Bakas, Spyridon",
title="Multi-institutional Deep Learning Modeling Without Sharing Patient Data: A Feasibility Study on Brain Tumor Segmentation",
booktitle={MICCAI},
year="2019",
pages="92--104"}



@article{Rieke2020,
abstract = {Data-driven machine learning (ML) has emerged as a promising approach for building accurate and robust statistical models from medical data, which is collected in huge volumes by modern healthcare systems. Existing medical data is not fully exploited by ML primarily because it sits in data silos and privacy concerns restrict access to this data. However, without access to sufficient data, ML will be prevented from reaching its full potential and, ultimately, from making the transition from research to clinical practice. This paper considers key factors contributing to this issue, explores how federated learning (FL) may provide a solution for the future of digital health and highlights the challenges and considerations that need to be addressed.},
author = {Rieke, Nicola and Hancox, Jonny and Li, Wenqi and Milletar{\`{i}}, Fausto and Roth, Holger R and Albarqouni, Shadi and Bakas, Spyridon and Galtier, Mathieu N and Landman, Bennett A and Maier-Hein, Klaus and Ourselin, S{\'{e}}bastien and Sheller, Micah and Summers, Ronald M and Trask, Andrew and Xu, Daguang and Baust, Maximilian and Cardoso, M Jorge},
doi = {10.1038/s41746-020-00323-1},
issn = {2398-6352},
journal = {NPJ Digit Med},
number = {1},
pages = {119},
title = {{The future of digital health with federated learning}},
volume = {3},
year = {2020}
}

@article{dewey2019,
title = {DeepHarmony: A deep learning approach to contrast harmonization across scanner changes},
journal = MRI,
volume = {64},
pages = {160-170},
year = {2019},
author = {Blake E. Dewey and Can Zhao and Jacob C. Reinhold and Aaron Carass and Kathryn C. Fitzgerald and Elias S. Sotirchos and Shiv Saidha and Jiwon Oh and Dzung L. Pham and Peter A. Calabresi and Peter C.M. {van Zijl} and Jerry L. Prince},
keywords = {Deep learning, Contrast harmonization, Magnetic resonance imaging}}

@article{karayumak2019,
title = {Retrospective harmonization of multi-site diffusion MRI data acquired with different acquisition parameters},
journal = {NeuroImage},
volume = {184},
pages = {180-200},
year = {2019},
author = {Suheyla {Cetin Karayumak} and Sylvain Bouix and Lipeng Ning and Anthony James and Tim Crow and Martha Shenton and Marek Kubicki and Yogesh Rathi},
keywords = {dMRI, Harmonization, Inter-scanner}
}

@article{Pomponio2020,
title = {Harmonization of large MRI datasets for the analysis of brain imaging patterns throughout the lifespan},
journal = {NeuroImage},
volume = {208},
pages = {116450},
year = {2020},
author = {Raymond Pomponio and Guray Erus and Mohamad Habes and Jimit Doshi and Dhivya Srinivasan and Elizabeth Mamourian and Vishnu Bashyam and Ilya M. Nasrallah and Theodore D. Satterthwaite and Yong Fan and Lenore J. Launer and Colin L. Masters and Paul Maruff and Chuanjun Zhuo and Henry Völzke and Sterling C. Johnson and Jurgen Fripp and Nikolaos Koutsouleris and Daniel H. Wolf and Raquel Gur and Ruben Gur and John Morris and Marilyn S. Albert and Hans J. Grabe and Susan M. Resnick and R. Nick Bryan and David A. Wolk and Russell T. Shinohara and Haochang Shou and Christos Davatzikos},
keywords = {MRI, Segmentation, FreeSurfer, MUSE, Brain, ROI}}

@inproceedings{park2021,
    author={Sangjoon Park and Gwanghyun Kim and Jeongsol Kim and Boah Kim and Jong Chul Ye},
    title={{Federated Split Task-Agnostic Vision Transformer for COVID-19 CXR Diagnosis}},
    year = {2021},
    booktitle=NIPS
}


@inproceedings{guo2021,
      title={{Multi-institutional Collaborations for Improving Deep Learning-based Magnetic Resonance Image Reconstruction Using Federated Learning}}, 
      author={Pengfei Guo and Puyang Wang and Jinyuan Zhou and Shanshan Jiang and Vishal M. Patel},
      year={2021},
      booktitle={IEEE CVPR},
}



@article{korkmaz2022unsupervised,
  title={Unsupervised MRI reconstruction via zero-shot learned adversarial transformers},
  author={Korkmaz, Yilmaz and Dar, Salman UH and Yurt, Mahmut and {\"O}zbey, Muzaffer and Cukur, Tolga},
  journal={IEEE Trans Med Imaging},
  volume={41},
  number={7},
  pages={1747--1763},
  year={2022},
  publisher={IEEE}
}

@INPROCEEDINGS{Oh2020ISBI,
  author={Oh, Gyutaek and Sim, Byeongsu and Ye, Jong Chul},
  booktitle=ISBI, 
  title={{Unsupervised Learning for Compressed Sensing MRI Using CycleGAN}}, 
  year={2020},
  volume={},
  number={},
  pages={1082-1085}}
  
%stylegan Int Soc Magn Reson Med
@inproceedings{Yilmaz2021ismrm,
  title={Zero-Shot Learning for Unsupervised Reconstruction of  Accelerated {MRI} Acquisitions},
  author={Korkmaz, Yilmaz and Dar, Salman UH and Yurt, Mahmut and Ozbey, Muzaffer and \c{C}ukur, Tolga},
   booktitle = {Int Soc Magn Reson Med},
  year = {2021},
  pages={}
}
%GAN inversion paper
@inproceedings{zhu2020indomain,
  title     = {In-domain {GAN} Inversion for Real Image Editing},
  author    = {Zhu, Jiapeng and Shen, Yujun and Zhao, Deli and Zhou, Bolei},
  booktitle = {Proceedings of European Conference on Computer Vision (ECCV)},
  pages={592-608},
  year      = {2020}
}
@article{chen2021transunet,
      title={{TransUNet: T}ransformers Make Strong Encoders for Medical Image Segmentation}, 
      author={Jieneng Chen and Yongyi Lu and Qihang Yu and Xiangde Luo and Ehsan Adeli and Yan Wang and Le Lu and Alan L. Yuille and Yuyin Zhou},
      year={2021},
      journal ={arXiv:2102.04306}
}
%styleGAN better disentaglemment
@article{xia2021gan,
  title={{GAN} Inversion: {A} Survey},
  author={Xia, Weihao and Zhang, Yulun and Yang, Yujiu and Xue, Jing-Hao and Zhou, Bolei and Yang, Ming-Hsuan},
  journal={arXiv:2101.05278},
  year={2021}
}
%
@article{hudson2021generative,
  title={Generative Adversarial Transformers},
  author={Hudson, Drew A and Zitnick, C Lawrence},
  journal={arXiv:2103.01209},
  year={2021}
}

@INPROCEEDINGS{data_diff,
  author={Q. {Chang} and H. {Qu} and Y. {Zhang} and M. {Sabuncu} and C. {Chen} and T. {Zhang} and D. N. {Metaxas}},
  booktitle={CVPR}, 
  title={Synthetic Learning: {Learn} From Distributed Asynchronized Discriminator {GAN} Without Sharing Medical Image Data}, 
  year={2020},
  volume={},
  number={},
  pages={13853-13863}}
  
@InProceedings{DIP,
author = {Ulyanov, Dmitry and Vedaldi, Andrea and Lempitsky, Victor},
title = {Deep Image Prior},
booktitle = {CVPR},
pages = {9446-9454},
year = {2018}
}

%magezine
@ARTICLE{DNNRec2020,
  author={D. {Liang} and J. {Cheng} and Z. {Ke} and L. {Ying}},
  journal=SPM, 
  title={Deep {Magnetic Resonance Image} Reconstruction: Inverse Problems Meet Neural Networks}, 
  year={2020},
  volume={37},
  number={1},
  pages={141-151}}
  
 @ARTICLE{DNNCS2020,
  author={C. M. {Sandino} and J. Y. {Cheng} and F. {Chen} and M. {Mardani} and J. M. {Pauly} and S. S. {Vasanawala}},
  journal=SPM, 
  title={Compressed Sensing: {From} Research to Clinical Practice With Deep Neural Networks: {Shortening} Scan Times for {Magnetic Resonance Imaging}}, 
  year={2020},
  volume={37},
  number={1},
  pages={117-127}
}
 %supervised recon 
 @article{KnollGeneralization,
author = {Knoll, Florian and Hammernik, Kerstin and Kobler, Erich and Pock, Thomas and Recht, Michael P and Sodickson, Daniel K},
title = {Assessment of the generalization of learned image reconstruction and the potential for transfer learning},
journal = MRM,
volume = {81},
number = {1},
pages = {116-128},
year = {2019}
}
@InProceedings{Grappa_net,
author = {Sriram, Anuroop and Zbontar, Jure and Murrell, Tullie and Zitnick, C. Lawrence and Defazio, Aaron and Sodickson, Daniel K.},
title = {{GrappaNet}: {Combining} Parallel Imaging With Deep Learning for Multi-Coil {MRI} Reconstruction},
booktitle = {CVPR},
month = {June},
pages = {14303-14310},
year = {2020}
}



%N2N type
@article{liu2020rare,
  title={Rare: {Image} reconstruction using deep priors learned without groundtruth},
  author={Liu, Jiaming and Sun, Yu and Eldeniz, Cihat and Gan, Weijie and An, Hongyu and Kamilov, Ulugbek S},
  journal=IEEE_J_STSP,
  volume={14},
  number={6},
  pages={1088--1099},
  year={2020}
}
%N2N type
@article{lehtinen2018noise2noise,
  title={Noise2noise: {Learning} image restoration without clean data},
  author={Lehtinen, Jaakko and Munkberg, Jacob and Hasselgren, Jon and Laine, Samuli and Karras, Tero and Aittala, Miika and Aila, Timo},
  journal={arXiv:1803.04189},
  year={2018}
}
%cycle loss with wGAN
@article{wgan,
  title={Wasserstein {GANs} for {MR} imaging: from paired to unpaired training},
  author={Lei, Ke and Mardani, Morteza and Pauly, John M and Vasanawala, Shreyas S},
  journal=TMI,
  year={2020},
  volume={40},
  number={1},
  pages={105-115},
}
%cycle GAN with wGAN
@article{oh2020,
  title={Unpaired deep learning for accelerated {MRI} using optimal transport driven {cycleGAN}},
  author={Oh, Gyutaek and Sim, Byeongsu and Chung, HyungJin and Sunwoo, Leonard and Ye, Jong Chul},
  journal=TCI,
  volume={6},
  pages={1285--1296},
  year={2020}
}
%cyclegan progressive
@article{chung2020progressive,
  title={Two-Stage Deep Learning for Accelerated {3D} Time-of-Flight {MRA} without Matched Training Data},
  author={Chung, Hyungjin and Cha, Eunju and Sunwoo, Leonard and Ye, Jong Chul},
  journal=MEDIA,
  volume={71},
  pages={102047},
  year={2021}
}
%cycleGAn with wGAN
@article{lei2020,
  author={Lei, Ke and Mardani, Morteza and Pauly, John M and Vasanawala, Shreyas S},
  journal=TMI, 
  title={Wasserstein GANs for MR Imaging: From Paired to Unpaired Training}, 
  year={2021},
  volume={40},
  number={1},
  pages={105-115}
}

%self attention
@InProceedings{Zhang2019, 
title = {Self-Attention Generative Adversarial Networks}, 
author = {Zhang, Han and Goodfellow, Ian and Metaxas, Dimitris and Odena, Augustus}, 
booktitle = {ICML}, 
pages = {7354--7363}, 
year = {2019}, 
 }
 %self-attention
@article{Wu2019,
title = {Self-attention convolutional neural network for improved {MR} image reconstruction},
journal = {Inf Sci},
volume = {490},
pages = {317-328},
year = {2019},
author = {Yan Wu and Yajun Ma and Jing Liu and Jiang Du and Lei Xing},
}
%self-attention
@article {Lan2020,
	author = {Lan, Haoyu and  and Toga, Arthur W and Sepehrband, Farshid},
	title = {{SC-GAN}: {3D} self-attention conditional {GAN} with spectral normalization for multi-modal neuroimaging synthesis},
	year = {2020},
	journal = {bioRxiv preprint 2020.06.09.143297}
}
%self-attention
@article{Oktay2018,
  title={Attention {U-Net}: {Learning} where to look for the pancreas},
  author={Oktay, Ozan and Schlemper, Jo and Folgoc, Loic Le and Lee, Matthew and Heinrich, Mattias and Misawa, Kazunari and Mori, Kensaku and McDonagh, Steven and Hammerla, Nils Y and Kainz, Bernhard and Glocker, Ben and Rueckert, Daniel},
  journal={arXiv:1804.03999},
  year={2018}
}
%self attention
@ARTICLE{Yuan2020,
AUTHOR={Yuan, Zhenmou and Jiang, Mingfeng and Wang, Yaming and Wei, Bo and Li, Yongming and Wang, Pin and Menpes-Smith, Wade and Niu, Zhangming and Yang, Guang},   
TITLE={{SARA-GAN: S}elf-Attention and Relative Average Discriminator Based Generative Adversarial Networks for Fast Compressed Sensing {MRI} Reconstruction},   
JOURNAL={Front Neuroinf},      
VOLUME={14},      
PAGES={58},     
YEAR={2020},      
}


@article{transformer1,
  title={Generating long sequences with sparse transformers},
  author={Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},
  journal={arXiv:1904.10509},
  year={2019}
}

@article{yurt2022semi,
  title={Semi-supervised learning of MRI synthesis without fully-sampled ground truths},
  author={Yurt, Mahmut and Dalmaz, Onat and Dar, Salman and Ozbey, Muzaffer and T{\i}naz, Berk and Oguz, Kader and {\c{C}}ukur, Tolga},
  journal={IEEE Trans Med Imaging},
  volume={41},
  number={12},
  pages={3895--3906},
  year={2022},
  publisher={IEEE}
}

@article{fastmri,
author = {Knoll, Florian and Zbontar, Jure and Sriram, Anuroop and Muckley, Matthew                             J. and Bruno, Mary and Defazio, Aaron and Parente, Marc and Geras, Krzysztof  J. and Katsnelson, Joe and Chandarana, Hersh and Zhang, Zizhao and Drozdzalv, Michal and Romero, Adriana and Rabbat, Michael and Vincent, Pascal and Pinkerton, James and Wang, Duo and Yakubova, Nafissa and Owens, Erich and Zitnick, C. Lawrence and Recht, Michael P. and Sodickson, Daniel K. and Lui, Yvonne                         W.},
title = {{fastMRI: A} Publicly Available Raw k-Space and {DICOM} Dataset of Knee Images for Accelerated {MR} Image Reconstruction Using Machine Learning},
journal = {Rad Artif Intell},
volume = {2},
number = {1},
pages = {e190007},
year = {2020}
}

@article{midas,
  title={Vessel tortuosity and brain tumor malignancy: a blinded study},
  author={Bullitt, Elizabeth and Zeng, Donglin and Gerig, Guido and Aylward, Stephen and Joshi, Sarang and Smith, J Keith and Lin, Weili and Ewend, Matthew G},
  journal={Academic radiology},
  volume={12},
  number={10},
  pages={1232--1240},
  year={2005}
}
@inproceedings{larochelle2008zero,
  title={Zero-data learning of new tasks},
  author={Larochelle, Hugo and Erhan, Dumitru and Bengio, Yoshua},
  booktitle={AAAI},
  volume={1},
  number={2},
  pages={3},
  year={2008}
}
@inproceedings{image2stylegan,
  title={Image2stylegan: How to embed images into the stylegan latent space?},
  author={Abdal, Rameen and Qin, Yipeng and Wonka, Peter},
  booktitle={CVPR},
  pages={4432--4441},
  year={2019}
}
%prior guided
@article{rgan,
  title={Prior-guided image reconstruction for accelerated multi-contrast {MRI} via generative adversarial networks},
  author={Dar, Salman UH and Yurt, Mahmut and Shahdloo, Mohammad and Ild{\i}z, Muhammed Emrullah and T{\i}naz, Berk and {\c{C}}ukur, Tolga},
  journal=IEEE_J_STSP,
  volume={14},
  number={6},
  pages={1072--1087},
  year={2020}
}
%self-supervised akçakaya
@article{yaman2020,
  title={Self-supervised learning of physics-guided reconstruction neural networks without fully sampled reference data},
  author={Yaman, Burhaneddin and Hosseini, Seyed Amir Hossein and Moeller, Steen and Ellermann, Jutta and Ugurbil, Kamil and Akcakaya, Mehmet},
  journal=MRM,
  volume={84},
  number={6},
  pages={3172--3191},
  year={2020}
}

@article{yaman2020multi,
title={Multi-Mask Self-Supervised Learning for Physics-Guided Neural Networks in Highly Accelerated {MRI}},
  author={Yaman, Burhaneddin and Hosseini, Seyed Amir Hossein and Moeller, Steen and Ellermann, Jutta and U{\u{g}}urbil, K{\^a}mil and Ak{\c{c}}akaya, Mehmet},
journal={arXiv:2008.06029},
year={2020}
}
%self-supervised
@inproceedings{Huang2019self,
  title={Deep {MRI} Reconstruction without Ground Truth for Training},
  author={Huang,Peizhou and Li, Chaoyi Hongyu and Gaire,Sunil Kumar and Liu,Ruiying and Zhang, Xiaoliang and  Li,Xiaojuan and Ying,Leslie},
   booktitle = {ISMRM},
  year = {2019},
  pages = {4668}
}
%self-supervise cycle type GAN
@article{Cole2020,
  title={Unsupervised {MRI} Reconstruction with Generative Adversarial Networks},
  author={Cole, Elizabeth K and Pauly, John M and Vasanawala, Shreyas S and Ong, Frank},
  journal={arXiv:2008.13065},
  year={2020}
}
%tamer self supervisd
@article{tamir2019unsupervised,
  title={Unsupervised Deep Basis Pursuit: Learning inverse problems without ground-truth data},
  author={Tamir, Jonathan I and Yu, Stella X and Lustig, Michael},
  journal={arXiv:1910.13110},
  year={2019}
}
%Tamir self sueprvised
@inproceedings{Tamir2019,
  title={Unsupervised Deep Basis Pursuit: {Learning} Reconstruction without Ground-Truth Data},
  author={Tamir, Jonathan I and Yu, Stella X and Lustig, Michael},
   booktitle = {ISMRM},
  year = {2019},
  pages = {0660}
}

@InProceedings{Wang2020self,
author={Wang, Alan Q.
and Dalca, Adrian V.
and Sabuncu, Mert R.},
title={Neural Network-Based Reconstruction in Compressed Sensing {MRI} Without Fully-Sampled Training Data},
booktitle={MLMIR},
year={2020},
pages={27--37}
}


%ensure error estimate self-supervised
@article{aggarwal2020,
  title={{ENSURE: Ensemble Stein's} Unbiased Risk Estimator for Unsupervised Learning},
  author={Aggarwal, Hemant Kumar and Jacob, Mathews},
  journal={arXiv:2010.10631},
  year={2020}
}
%Spark scan specific
@article{Beker2019,
  title={Scan-specific, Parameter-free Artifact Reduction in K-space ({SPARK})},
  author={Arefeen, Yamin and Beker, Onur and Yu, Heng and Adalsteinsson, Elfar and Bilgic, Berkin},
  journal={arXiv:2104.01188},
  year={2021}
}

%self supervised +scan specific
@INPROCEEDINGS{Hosseini2020,
  author={S. A. {Hossein Hosseini} and B. {Yaman} and S. {Moeller} and M. {Akçakaya}},
  booktitle={IEEE EMBC}, 
  title={High-Fidelity Accelerated {MRI} Reconstruction by Scan-Specific Fine-Tuning of Physics-Based Neural Networks}, 
  year={2020},
  volume={},
  number={},
  pages={1481-1484}}
  
  %supervised+scan specific tuning with better error estimate
@article{aggarwal2021,
  title={Model Adaptation for Image Reconstruction using Generalized {Stein's} Unbiased Risk Estimator},
  author={Aggarwal, Hemant Kumar and Jacob, Mathews},
  journal={arXiv:2102.00047},
  year={2021}
}
%Dip like  
 @article{Biswas2019,
  title={Dynamic {MRI} using model-based deep learning and {SToRM }priors: {MoDL-SToRM}},
  author={Biswas, Sampurna and Aggarwal, Hemant K and Jacob, Mathews},
  journal=MRM,
  volume={82},
  number={1},
  pages={485--494},
  year={2019}
}
%dip like
@article{Zou2021,
  title={Deep Generative {SToRM} model for dynamic imaging},
  author={Zou, Qing and Ahmed, Abdul Haseeb and Nagpal, Prashant and Kruger, Stanley and Jacob, Mathews},
  journal={arXiv:2101.12366},
  year={2021}
}
 
 %dip like
 @article{Jin2019,
  title={Time-dependent deep image prior for dynamic {MRI}},
  author={Jin, Kyong Hwan and Gupta, Harshit and Yerly, Jerome and Stuber, Matthias and Unser, Michael},
  journal={arXiv:1910.01684},
  year={2019}
}

%dip like
@INPROCEEDINGS{Arora2020ismrm,
    title={Untrained Modified Deep Decoder for Joint Denoising and Parallel Imaging Reconstruction},
  author={Arora, Sukrit and Roeloffs, Volkert and Lustig,Michael},
   booktitle = {ISMRM},
  year = {2020},
  pages = {3585}
}
%dip like
@INPROCEEDINGS{Ke2020ismrm,
    title={Assessment of the Generalization of Learned Unsupervised Deep Learning Method},
  author={Ke, Ziwen  and Zhu,Yanjie  and Cheng,Jing and Ying,Leslie and Liu,Xin and Zheng,Hairong  and Liang, Dong},
   booktitle = {ISMRM},
  year = {2020},
  pages = {3630}
}
%sRAKI
@article{sraki,
    author = {Hosseini, Seyed Amir Hossein AND Zhang, Chi AND Weingärtner, Sebastian AND Moeller, Steen AND Stuber, Matthias AND Ugurbil, Kamil AND Akçakaya, Mehmet},
    journal = {PLOS ONE},
    title = {Accelerated coronary {MRI} with {sRAKI: A} database-free self-consistent neural network k-space reconstruction for arbitrary undersampling},
    year = {2020},
    volume = {15},
    pages = {1-13},
    number = {2}
}
%RAKI
@article{raki,
author = {Akçakaya, Mehmet and Moeller, Steen and Weingärtner, Sebastian and Uğurbil, Kâmil},
title = {Scan-specific robust artificial-neural-networks for k-space interpolation {(RAKI)} reconstruction: {Database}-free deep learning for fast imaging},
journal = MRM,
volume = {81},
number = {1},
pages = {439-453},
year = {2019}
}
%LORAKI
@article{loraki,
  title={{LORAKI}: Autocalibrated recurrent neural networks for autoregressive {MRI} reconstruction in k-space},
  author={Kim, Tae Hyung and Garg, Pratyush and Haldar, Justin P},
  journal={arXiv:1904.09390},
  year={2019}
}
%hybrid
@inproceedings{Dar2021ismrm,
  title={A Few-Shot Learning Approach for Accelerated {MRI} via Fusion of Data-Driven and Subject-Driven Priors},
  author={Dar, Salman UH and Yurt, Mahmut and \c{C}ukur, Tolga},
   booktitle = {ISMRM},
  year = {2021},
  pages={1949}
}

%styleGan better inversion paper
@article{Gabbay2019,
  title={Style generator inversion for image enhancement and animation},
  author={Gabbay, Aviv and Hoshen, Yedid},
  journal={arXiv:1906.11880},
  year={2019}
}

%N2N
@ARTICLE{Liu2020,
  author={J. {Liu} and Y. {Sun} and C. {Eldeniz} and W. {Gan} and H. {An} and U. S. {Kamilov}},
  journal=IEEE_J_STSP, 
  title={{RARE}: Image Reconstruction Using Deep Priors Learned Without Groundtruth}, 
  year={2020},
  volume={14},
  number={6},
  pages={1088-1099}
  }
 %N2N 
@INPROCEEDINGS{Liu2020ismrm,
    title={{RED-N2N: Image reconstruction for MRI using deep CNN priors trained without ground truth}},
  author={Liu,Jiaming and Eldeniz, Cihat and Sun,Yu  Gan,Weijie and Chen, Sihao and An, Hongyu  and Kamilov, Ulugbek S.},
   booktitle = {ISMRM},
  year = {2020},
  pages = {0993}
}
 %N2N 
@INPROCEEDINGS{Eldeniz2020ismrm,
    title={{Phase2Phase: Reconstruction of free-breathing MRI into multiple respiratory phases using deep learning without a ground truth}},
  author={Eldeniz,Cihat and Gan, Weijie and Chen,Sihao and Liu, Jiaming  and Kamilov,Ulugbek S.  and An, Hongyu},
   booktitle = {ISMRM},
  year = {2020},
  pages = {0807}
}
%N2N
@INPROCEEDINGS{Huang2020ismrm,
    title={Unsupervised Deep Learning Reconstruction Using the {MR} Imaging Model},
  author={Huang,Peizhou and Zhang,Chaoyi and Li,Hongyu and Gaire,Sunil Kumar and Liu,Ruiying and Zhang, Xiaoliang and Li, Xiaojuan and Dong, Liang and Ying, Leslie },
   booktitle = {ISMRM},
  year = {2020},
  pages = {3617}
}

%knoll inverseGAN
@inproceedings{Knoll2019inverseGANs,
author = {Dominik Narnhofer and Kerstin Hammernik and Florian Knoll and Thomas Pock},
title = {{Inverse GANs for accelerated MRI reconstruction}},
volume = {11138},
pages = {381 -- 392},
year = {2019},
booktitle = {SPIE Med Imaging}
}
%konukoglu
@ARTICLE{Konukoglu2019,
  author={K. C. {Tezcan} and C. F. {Baumgartner} and R. {Luechinger} and K. P. {Pruessmann} and E. {Konukoglu}},
  journal=TMI, 
  title={{MR} Image Reconstruction Using Deep Density Priors}, 
  year={2019},
  volume={38},
  number={7},
  pages={1633-1642}}
%unrolled
@ARTICLE{MoDl,
  author={H. K. {Aggarwal} and M. P. {Mani} and M. {Jacob}},
  journal=TMI, 
  title={{MoDL: Model-Based} Deep Learning Architecture for Inverse Problems}, 
  year={2019},
  volume={38},
  number={2},
  pages={394-405}
  } 
 %serial kspace and image space iterative
@article{KikiNet,
author = {Eo, Taejoon and Jun, Yohan and Kim, Taeseong and Jang, Jinseong and Lee, Ho-Joon and Hwang, Dosik},
title = {{KIKI-net:} cross-domain convolutional neural networks for reconstructing undersampled magnetic resonance images},
journal = MRM,
volume = {80},
number = {5},
pages = {2188-2201},
year = {2018}
}
@ARTICLE{ADMM-CSNET,
  author={Y. {Yang} and J. {Sun} and H. {Li} and Z. {Xu}},
  journal=IEEE_J_PAMI, 
  title={{ADMM-CSNet: A} Deep Learning Approach for Image Compressive Sensing}, 
  year={2020},
  volume={42},
  number={3},
  pages={521-538}}
  %------------------------------
@INPROCEEDINGS{ISTA-NET,
  author={J. {Zhang} and B. {Ghanem}},
  booktitle={IEEE CVPR}, 
  title={{ISTA-Net: I}nterpretable Optimization-Inspired Deep Network for Image Compressive Sensing}, 
  year={2018},
  volume={},
  number={},
  pages={1828-1837}}
  %iterative
@InProceedings{Primal_dual,
author="Cheng, Jing
and Wang, Haifeng
and Ying, Leslie
and Liang, Dong",
title="Model Learning: {Primal} Dual Networks for Fast {MR} Imaging",
booktitle={MICCAI},
year="2019",
pages="21--29"
}
@ARTICLE{Conv_recur,
  author={C. {Qin} and J. {Schlemper} and J. {Caballero} and A. N. {Price} and J. V. {Hajnal} and D. {Rueckert}},
  journal=TMI, 
  title={Convolutional Recurrent Neural Networks for Dynamic {MR} Image Reconstruction}, 
  year={2019},
  volume={38},
  number={1},
  pages={280-290}}
    
@InProceedings{StyleGAN1,
author = {Karras, Tero and Laine, Samuli and Aila, Timo},
title = {A Style-Based Generator Architecture for Generative Adversarial Networks},
booktitle = {CVPR},
pages = {4401-4410},
year = {2019}
}
%recurren version of raki-----------------------------
@article{kim2019loraki,
  title={{LORAKI: Autocalibrated recurrent neural networks for autoregressive MRI reconstruction in k-space}},
  author={Kim, Tae Hyung and Garg, Pratyush and Haldar, Justin P},
  journal={arXiv:1904.09390},
  year={2019}
}
%3D-unet cmri
@article{Hauptmann2018,
author = {Hauptmann, Andreas and Arridge, Simon and Lucka, Felix and Muthurangu, Vivek and Steeden, Jennifer A.},
title = {Real-time cardiovascular {MR} with spatio-temporal artifact suppression using deep learning–proof of concept in congenital heart disease},
journal = MRM,
volume = {81},
number = {2},
pages = {1143-1156},
year = {2019}
}

%Yoon2018 Unet
@article{Yoon2018,
title = {Quantitative susceptibility mapping using deep neural network: {QSMnet}},
journal = {NeuroImage},
volume = {179},
pages = {199-206},
year = {2018},
author = {Jaeyeon Yoon and Enhao Gong and Itthi Chatnuntawech and Berkin Bilgic and Jingu Lee and Woojin Jung and Jingyu Ko and Hosan Jung and Kawin Setsompop and Greg Zaharchuk and Eung Yeop Kim and John Pauly and Jongho Lee}
}
%kspace + image space unrolling
@article{Wang2019,
author = {Wang, Shanshan and Ke, Ziwen and Cheng, Huitao and Jia, Sen and Ying, Leslie and Zheng, Hairong and Liang, Dong},
title = {{DIMENSION: Dynamic MR imaging with both k-space and spatial prior knowledge obtained via multi-supervised network training}},
journal = {NMR Biomed},
pages = {e4131},
year = {2019}
}
%dense recurrent
@ARTICLE{Hosseini2020b,
  author={S. A. H. {Hosseini} and B. {Yaman} and S. {Moeller} and M. {Hong} and M. {Akçakaya}},
  journal=IEEE_J_STSP, 
  title={Dense Recurrent Neural Networks for Accelerated {MRI: History}-Cognizant Unrolling of Optimization Algorithms}, 
  year={2020},
  volume={14},
  number={6},
  pages={1280-1291}}
%unrolling
@ARTICLE{Adler2018,
  author={J. {Adler} and O. {Öktem}},
  journal=TMI, 
  title={Learned Primal-Dual Reconstruction}, 
  year={2018},
  volume={37},
  number={6},
  pages={1322-1332}}
  
%chul Ye framelets
@article{ChulYe2018,
author = {Ye, Jong Chul and Han, Yoseob and Cha, Eunju},
title = {Deep Convolutional Framelets: {A} General Deep Learning Framework for Inverse Problems},
journal = {SIAM J Imaging Sci},
volume = {11},
number = {2},
pages = {991-1048},
year = {2018}
}
%wavelet gan
@inproceedings{Chen2021,
author = {Yutong Chen and David Firmin and Guang Yang},
title = {Wavelet improved {GAN for MRI} reconstruction},
volume = {11595},
booktitle = {SPIE Med Imaging},
pages = {1159513},
year = {2021}
}
  
@InProceedings{StyleGAN2,
  title     = {Analyzing and Improving the Image Quality of {StyleGAN}},
  author    = {Tero Karras and Samuli Laine and Miika Aittala and Janne Hellsten and Jaakko Lehtinen and Timo Aila},
  booktitle = {CVPR},
  pages = {8107-8116},
  year      = {2020}
}


@inproceedings{Schlemper2017,
author = {Schlemper, Jo and Caballero, Jose and Hajnal, Joseph V. and Price, Anthony and Rueckert, Daniel},
booktitle = {IPMI},
pages = {647--658},
title = {{A Deep Cascade of Convolutional Neural Networks for {MR} Image Reconstruction}},
year = {2017}
}

@inproceedings{Yang2016,
author = {Yang, Yan and Sun, Jian and Li, Huibin and Xu, Zongben},
booktitle = {NeurIPS},
title = {Deep {ADMM-Net} for Compressive Sensing {MRI}},
volume = {29},
year = {2016}
}
@article{Han2016,
author = {Han, Yoseop and Yoo, Jaejoon and Ye, Jong Chul},
title = {Deep Residual Learning for Compressed Sensing {CT} Reconstruction via Persistent Homology Analysis},
journal = {arXiv:1611.06391},
year = {2016}
}
@article{Hammernik2017,
author = {Hammernik, Kerstin and Klatzer, Teresa and Kobler, Erich and Recht, Michael P and Sodickson, Daniel K and Pock, Thomas and Knoll, Florian},
journal = MRM,
number = {6},
pages = {3055--3071},
title = {Learning a Variational Network for Reconstruction of Accelerated {MRI} Data},
volume = {79},
year = {2017}
}
@article{Han2018,
author = {Han, Yoseob and Yoo, Jaejun and Kim, Hak Hee and Shin, Hee Jung and Sung, Kyunghyun and Ye, Jong Chul},
journal = MRM,
pages = {1189--1205},
title = {{Deep learning with domain adaptation for accelerated projection-reconstruction MR}},
volume = {80},
year = {2018}
}

@article{Mardani2019b,
author = {Mardani, Morteza and Gong, Enhao and Cheng, Joseph Y. and Vasanawala, Shreyas and Zaharchuk, Greg and Xing, Lei and Pauly, John M},
journal = TMI,
number = {1},
pages = {167--179},
title = {{Deep generative adversarial neural networks for compressive sensing MRI}},
volume = {38},
year = {2019}
}
@article{Yu2018c,
author = {Yu, Simiao and Dong, Hao and Yang, Guang and Slabaugh, Greg and Dragotti, Pier Luigi and Ye, Xujiong and Liu, Fangde and Arridge, Simon and Keegan, Jennifer and Firmin, David and Guo, Yike},
journal = TMI,
number = {6},
pages = {1310--1321},
title = {{DAGAN: Deep de-aliasing generative adversarial networks for fast compressed sensing MRI reconstruction}},
volume = {37},
year = {2018}
}
@article{Zhu2018,
author = {Zhu, Bo and Liu, Jeremiah Z. and Rosen, Bruce R. and Rosen, Matthew S.},
journal = {Nature},
number = {7697},
pages = {487--492},
title = {{Image reconstruction by domain transform manifold learning}},
volume = {555},
year = {2018}
}
@article{Ravishankar2011a,
author = {Ravishankar, Saiprasad and Bresler, Yoram},
journal = TMI,
number = {5},
pages = {1028--1041},
pmid = {21047708},
title = {{MR image reconstruction from highly undersampled k-space data by dictionary learning}},
volume = {30},
year = {2011}
}
@article{Quan2018c,
author = {Quan, Tran Minh and Nguyen-Duc, Thanh and Jeong, Won-Ki},
eprint = {1709.00753},
journal = TMI,
number = {6},
pages = {1488--1497},
title = {{Compressed sensing MRI reconstruction with cyclic loss in generative adversarial networks}},
volume = {37},
year = {2018}
}
@article{Jog2017b,
author = {Jog, Amod and Carass, Aaron and Roy, Snehashis and Pham, Dzung L. and Prince, Jerry L.},
journal = {Med Image Anal},
pages = {475--488},
pmid = {27607469},
title = {{Random forest regression for magnetic resonance image synthesis}},
volume = {35},
year = {2017}
}
@inproceedings{Jog2013b,
abstract = {Magnetic resonance imaging (MRI) is widely used for analyzing human brain structure and function. MRI is extremely versatile and can produce different tissue contrasts as required by the study design. For reasons such as patient comfort, cost, and improving technology, certain tissue contrasts for a cohort analysis may not have been acquired during the imaging session. This missing pulse sequence hampers consistent neuroanatomy research. One possible solution is to synthesize the missing sequence. This paper proposes a data-driven approach to image synthesis, which provides equal, if not superior synthesis compared to the state-of-the-art, in addition to being an order of magnitude faster. The synthesis transformation is done on image patches by a trained bagged ensemble of regression trees. Validation was done by synthesizing T 2-weighted contrasts from T 1-weighted scans, for phantoms and real data. We also synthesized 3 Tesla T 1-weighted magnetization prepared rapid gradient echo (MPRAGE) images from 1.5 Tesla MPRAGEs to demonstrate the generality of this approach.},
author = {Jog, Amod and Roy, Snehashis and Carass, Aaron and Prince, Jerry L},
booktitle = {ISBI},
file = {:Users/salmanulhassandar/Library/Application Support/Mendeley Desktop/Downloaded/Jog et al. - 2013 - Magnetic resonance image synthesis through patch regression.pdf:pdf},
isbn = {9781467364553},
issn = {1945-7928},
keywords = {Histograms,Image generation,Image synthesis,MPRAGE images,MRI,Magnetic resonance imaging,Noise,Phantoms,Regression tree analysis,T1-weighted magnetization,T1-weighted scans,T2-weighted contrasts,Training,biomedical MRI,brain,data-driven approach,human brain function analysis,human brain structure analysis,image matching,image patches,image sequences,magnetic flux density 1.5 tesla,magnetic flux density 3 tesla,magnetic resonance image synthesis,magnetisation,medical image processing,neuroanatomy research,neurophysiology,patch regression,phantoms,pulse sequence,rapid gradient echo images,regression,regression analysis,regression trees,synthesis transformation,tissue contrasts,tissue engineering},
pages = {350--353},
pmid = {24443686},
title = {{Magnetic resonance image synthesis through patch regression}},
year = {2013}
}
@inproceedings{Joyce2017c,
abstract = {We present a multi-input encoder-decoder neural network model able to perform MR image synthesis from any subset of its inputs, outperforming prior methods in both single and multi-input settings. This is achieved by encouraging the network to learn a modality invariant latent embedding during training. We demonstrate that a spatial transformer module [7] can be included in our model to automatically correct misalignment in the input data. Thus, our model is robust both to missing and misaligned data at test time. Finally, we show that the model's modular nature allows transfer learning to different datasets.},
archivePrefix = {arXiv},
arxivId = {1706.01912},
author = {Joyce, Thomas and Chartsias, Agisilaos and Tsaftaris, Sotirios A.},
booktitle = {Medical Image Computing and Computer-Assisted Intervention},
doi = {10.1007/978-3-319-66179-7_40},
eprint = {1706.01912},
file = {:Users/salmanulhassandar/Library/Application Support/Mendeley Desktop/Downloaded/Joyce, Chartsias, Tsaftaris - 2017 - Robust multi-modal MR image synthesis.pdf:pdf},
isbn = {9783319661780},
issn = {16113349},
keywords = {Brain,MRI,Neural network,Synthesis},
pages = {347--355},
title = {{Robust multi-modal MR image synthesis}},
year = {2017}
}
@article{Chartsias2018c,
abstract = {We propose a multi-input multi-output fully convolutional neural network model for MRI synthesis. The model is robust to missing data, as it benefits from, but does not require, additional input modalities. The model is trained end-to-end, and learns to embed all input modalities into a shared modalityinvariant latent space. These latent representations are then combined into a single fused representation, which is transformed into the target output modality with a learnt decoder. We avoid the need for curriculum learning by exploiting the fact that the various input modalities are highly correlated. We also show that by incorporating information from segmentation masks the model can both decrease its error and generate data with synthetic lesions. We evaluate our model on the ISLES and BRATS datasets and demonstrate statistically significant improvements over state-of-the-art methods for single input tasks. This improvement increases further when multiple input modalities are used, demonstrating the benefits of learning a common latent space, again resulting in a statistically significant improvement over the current best method. Lastly, we demonstrate our approach on non skull-stripped brain images, producing a statistically significant improvement over the previous best method. Code is made publicly available at https://github.com/agis85/multimodal brain synthesis.We propose a multi-input multi-output fully convolutional neural network model for MRI synthesis. The model is robust to missing data, as it benefits from, but does not require, additional input modalities. The model is trained end-to-end, and learns to embed all input modalities into a shared modalityinvariant latent space. These latent representations are then combined into a single fused representation, which is transformed into the target output modality with a learnt decoder. We avoid the need for curriculum learning by exploiting the fact that the various input modalities are highly correlated. We also show that by incorporating information from segmentation masks the model can both decrease its error and generate data with synthetic lesions. We evaluate our model on the ISLES and BRATS datasets and demonstrate statistically significant improvements over state-of-the-art methods for single input tasks. This improvement increases further when multiple input modalities are used, demonstrating the benefits of learning a common latent space, again resulting in a statistically significant improvement over the current best method. Lastly, we demonstrate our approach on non skull-stripped brain images, producing a statistically significant improvement over the previous best method. Code is made publicly available at https://github.com/agis85/multimodal brain synthesis.},
author = {Chartsias, Agisilaos and Joyce, Thomas and Giuffrida, Mario Valerio and Tsaftaris, Sotirios A.},
doi = {10.1109/TMI.2017.2764326},
file = {:Users/salmanulhassandar/Library/Application Support/Mendeley Desktop/Downloaded/Chartsias et al. - 2018 - Multimodal MR Synthesis via Modality-Invariant Latent Representation.pdf:pdf},
issn = {1558254X},
journal = TMI,
keywords = {Neural network,brain,machine learning,magnetic resonance imaging (MRI),multi-modality fusion},
number = {3},
pages = {803--814},
pmid = {29053447},
title = {{Multimodal MR synthesis via modality-invariant latent representation}},
volume = {37},
year = {2018}
}
@incollection{VanNguyen2015b,
abstract = {Cross-modality image synthesis has recently gained significant inter-est in the medical imaging community. In this paper, we propose a novel archi-tecture called location-sensitive deep network (LSDN) for synthesizing images across domains. Our network integrates intensity feature from image voxels and spatial information in a principled manner. Specifically, LSDN models hidden nodes as products of features and spatial responses. We then propose a novel method, called ShrinkConnect, for reducing the computations of LSDN without sacrificing synthesis accuracy. ShrinkConnect enforces simultaneous sparsity to find a compact set of functions that well approximates the responses of all hidden nodes. Experimental results demonstrate that LSDN+ShrinkConnect outperforms the state of the art in cross-domain synthesis of MRI brain scans by a significant margin. Our approach is also computationally efficient, e.g. 26× faster than other sparse representation based methods.},
author = {{Van Nguyen}, Hien and Zhou, Kevin and Vemulapalli, Raviteja},
booktitle = {Medical Image Computing and Computer-Assisted Intervention},
doi = {10.1007/978-3-319-24553-9_83},
isbn = {978-3-319-24573-7},
issn = {16113349},
pages = {677--684},
title = {{Cross-domain synthesis of medical images using efficient location-sensitive deep network}},
year = {2015}
}
@incollection{Roy2013h,
abstract = {Magnetic resonance imaging (MRI) is a noninvasive modality that has been widely used to image the structure of the human brain. Unlike reconstructed x-ray computed tomography images, MRI intensities do not possess a calibrated scale, and the images suffer from wide variability in intensity contrasts due to scanner calibration and pulse sequence variations. Most MR image processing tasks use intensities as the principal feature and therefore the results can vary widely according to the actual tissue intensity contrast. Since it is difficult to control the MR scanner acquisition protocols in multi-scanner cross-sectional studies, results achieved using image processing tools are often difficult to compare in such studies. Similar issues can happen in longitudinal studies, as scanners undergo upgrades or improvements in pulse sequences, leading to new imaging sequences. We propose a novel probabilistic model to transform image contrasts by matching patches of a subject image to a set of patches from a multi-contrast atlas. Although the transformed images are not for diagnostic purpose, the use of such contrast transforms is shown for two applications, (a) to improve segmentation consistency across scanners and pulse sequences, (b) to improve registration accuracy between multi-contrast image pairs by transforming the subject image to the contrast of the reference image and then registering the transformed subject image to the reference image. Contrary to previous intensity transformation methods, our technique does not need any information about landmarks, pulse sequence parameters or imaging equations. It is shown to provide more consistent segmentation across scanners compared to state-of-the-art methods. {\textcopyright} 2013 Springer International Publishing.},
author = {Roy, Snehashis and Jog, Amod and Carass, Aaron and Prince, Jerry L.},
booktitle = {Multimodal Brain Image Analysis},
doi = {10.1007/978-3-319-02126-3_6},
isbn = {9783319021256},
issn = {03029743},
keywords = {brain,histogram matching,intensity normalization,intensity transformation,magnetic resonance imaging (MRI)},
pages = {51--62},
title = {{Atlas based intensity transformation of brain MR images}},
year = {2013}
}
@inproceedings{Roy2011b,
abstract = {The tissue contrast of a magnetic resonance (MR) neuroimaging data set has a major impact on image analysis tasks like registration and segmentation. It has been one of the core challenges of medical imaging to guarantee the consistency of these tasks regardless of the contrasts of the MR data. Inconsistencies in image analysis are attributable in part to variations in tissue contrast, which in turn arise from operator variations during image acquisition as well as software and hardware differences in the MR scanners. It is also a common problem that images with a desired tissue contrast are completely missing in a given data set for reasons of cost, acquisition time, forgetfulness, or patient comfort. Absence of this data can hamper the detailed, automatic analysis of some or all data sets in a scientific study. A method to synthesize missing MR tissue contrasts from available acquired images using an atlas containing the desired contrast and a patch-based compressed sensing strategy is described. An important application of this general approach is to synthesize a particular tissue contrast from multiple studies using a single atlas, thereby normalizing all data sets into a common intensity space. Experiments on real data, obtained using different scanners and pulse sequences, show improvement in segmentation consistency, which could be extremely valuable in the pooling of multi-site multi-scanner neuroimaging studies.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Roy, Snehashis and Carass, Aaron and Prince, Jerry},
booktitle = {Information Processing in Medical Imaging},
doi = {10.1007/978-3-642-22092-0_31},
eprint = {NIHMS150003},
isbn = {9783642220913},
issn = {16113349},
keywords = {Compressed sensing,Histogram equalization,Histogram matching,Image synthesis,Intensity normalization,Magnetic resonance imaging (MRI),Phantom,Segmentation,Standardization},
pages = {371--383},
pmid = {21761671},
title = {{A compressed sensing approach for mr tissue contrast synthesis}},
year = {2011}
}
@article{Jog2015c,
abstract = {Automatic processing of magnetic resonance images is a vital part of neuroscience research. Yet even the best and most widely used medical image processing methods will not produce consistent results when their input images are acquired with different pulse sequences. Although intensity standardization and image synthesis methods have been introduced to address this problem, their performance remains dependent on knowledge and consistency of the pulse sequences used to acquire the images. In this paper, an image synthesis approach that first estimates the pulse sequence parameters of the subject image is presented. The estimated parameters are then used with a collection of atlas or training images to generate a new atlas image having the same contrast as the subject image. This additional image provides an ideal source from which to synthesize any other target pulse sequence image contained in the atlas. In particular, a nonlinear regression intensity mapping is trained from the new atlas image to the target atlas image and then applied to the subject image to yield the particular target pulse sequence within the atlas. Both intensity standardization and synthesis of missing tissue contrasts can be achieved using this framework. The approach was evaluated on both simulated and real data, and shown to be superior in both intensity standardization and synthesis to other established methods.},
author = {Jog, Amod and Carass, Aaron and Roy, Snehashis and Pham, Dzung L. and Prince, Jerry L.},
doi = {10.1016/j.media.2015.05.002},
issn = {13618423},
journal = {Med Image Anal},
keywords = {Brain,Magnetic resonance imaging,Pulse sequence,Synthesis},
number = {1},
pages = {63--76},
pmid = {26072167},
title = {{MR image synthesis by contrast learning on neighborhood ensembles}},
volume = {24},
year = {2015}
}
@article{Cordier2016,
abstract = {This paper describes a novel generative model for the synthesis of multi-modal medical images of pathological cases based on a single label map. Our model builds upon i) a generative model commonly used for label fusion and multi-atlas patch-based segmentation of healthy anatomical structures, ii) the Modality Propagation iterative strategy used for a spatially-coherent synthesis of subject-specific scans of desired image modalities. The expression Extended Modality Propagation is coined to refer to the extension of Modality Propagation to the synthesis of images of pathological cases. Moreover, image synthesis uncertainty is estimated. An application to Magnetic Resonance Imaging synthesis of glioma-bearing brains is i) validated on the training dataset of a Multimodal Brain Tumor Image Segmentation challenge, ii) compared to the state-of-the-art in glioma image synthesis, and iii) illustrated using the output of two different tumor growth models. Such a generative model allows the generation of a large dataset of synthetic cases, which could prove useful for the training, validation, or benchmarking of image processing algorithms.},
author = {Cordier, Nicolas and Delingette, Herve and Le, Matthieu and Ayache, Nicholas},
doi = {10.1109/TMI.2016.2589760},
issn = {1558254X},
journal = TMI,
keywords = {Generative model,glioma,medical image simulation,modality synthesis,multi-atlas,patch-based},
number = {12},
pages = {2598--2608},
pmid = {27411217},
title = {{Extended modality propagation: Image synthesis of pathological cases}},
volume = {35},
year = {2016}
}
@inproceedings{Ye2013c,
abstract = {We propose a general database-driven framework for coherent synthesis of subject-specific scans of desired modality, which adopts and generalizes the patch-based label propagation (LP) strategy. While modality synthesis has received increased attention lately, current methods are mainly tailored to specific applications. On the other hand, the LP framework has been extremely successful for certain segmentation tasks, however, so far it has not been used for estimation of entities other than categorical segmentation labels. We approach the synthesis task as a modality propagation, and demonstrate that with certain modifications the LP framework can be generalized to continuous settings providing coherent synthesis of different modalities, beyond segmentation labels. To achieve high-quality estimates we introduce a new data-driven regularization scheme, in which we integrate intermediate estimates within an iterative search-and-synthesis strategy. To efficiently leverage population data and ensure coherent synthesis, we employ a spatio-population search space restriction. In experiments, we demonstrate the quality of synthesis of different MRI signals (T2 and DTI-FA) from a T1 input, and show a novel application of modality synthesis for abnormality detection in multi-channel MRI of brain tumor patients.},
author = {Ye, Dong Hye and Zikic, Darko and Glocker, Ben and Criminisi, Antonio and Konukoglu, Ender},
booktitle = {Medical Image Computing and Computer-Assisted Intervention},
doi = {10.1007/978-3-642-40811-3_76},
isbn = {9783642408106},
issn = {03029743},
pages = {606--613},
pmid = {24505717},
title = {{Modality propagation: Coherent synthesis of subject-specific scans with data-driven regularization}},
year = {2013}
}
@article{Huang2018b,
author = {Huang, Yawen and Shao, Ling and Frangi, Alejandro F.},
doi = {10.1109/TMI.2017.2781192},
issn = {1558254X},
journal = TMI,
keywords = {Dictionary Learning,Domain Adaption,Image Synthesis,MRI,Manifold Learning,Sparse Representation},
number = {3},
pages = {815 -- 827},
title = {{Cross-modality image synthesis via weakly-coupled and geometry co-regularized joint dictionary learning}},
volume = {37},
year = {2018}
}
@inproceedings{Vemulapalli2015c,
abstract = {Recently, cross-modal synthesis of subject-specific scans has been receiving significant attention from the medical imaging community. Though various synthesis approaches have been introduced in the recent past, most of them are either tailored to a specific application or proposed for the supervised setting, i.e., they assume the availability of training data from the same set of subjects in both source and target modalities. But, collecting multiple scans from each subject is undesirable. Hence, to address this issue, we propose a general unsupervised cross-modal medical image synthesis approach that works without paired training data. Given a source modality image of a subject, we first generate multiple target modality candidate values for each voxel independently using cross-modal nearest neighbor search. Then, we select the best candidate values jointly for all the voxels by simultaneously maximizing a global mutual information cost function and a local spatial consistency cost function. Finally, we use coupled sparse representation for further refinement of synthesized images. Our experiments on generating T1-MRI brain scans from T2-MRI and vice versa demonstrate that the synthesis capability of the proposed unsupervised approach is comparable to various state-of-the-art supervised approaches in the literature.},
author = {Vemulapalli, Raviteja and Nguyen, Hien Van and Zhou, Shaohua Kevin},
booktitle = {IEEE ICCV},
doi = {10.1109/ICCV.2015.79},
isbn = {9781467383912},
issn = {15505499},
pages = {630--638},
title = {{Unsupervised cross-modal synthesis of subject-specific scans}},
year = {2015}
}
@inproceedings{Roy2010c,
abstract = {The magnetic resonance contrast of a neuroimaging data set has strong impact on the utility of the data in image analysis tasks, such as registration and segmentation. Lengthy acquisition times often prevent routine acquisition of multiple MR contrast images, and opportunities for detailed analysis using these data would seem to be irrevocably lost. This paper describes an example based approach which uses patch matching from a multiple contrast atlas with the intended goal of generating an alternate MR contrast image, thus effectively simulating alternative pulse sequences from one another. In this paper, we deal specifically with Fluid Attenuated Inversion Recovery (FLAIR) sequence generation from T1 and T2 pulse sequences. The applicability of this synthetic FLAIR for estimating white matter lesions segmentation is demonstrated.},
author = {Roy, Snehashis and Carass, Aaron and Shiee, Navid and Pham, Dzung L. and Prince, Jerry L.},
booktitle = {IEEE ISBI},
doi = {10.1109/ISBI.2010.5490140},
isbn = {9781424441266},
issn = {1945-7928},
keywords = {Classification,Image resolution,Image segmentation,MRI},
pages = {932--935},
pmid = {21132059},
title = {{MR contrast synthesis for lesion segmentation}},
year = {2010}
}
@incollection{Sevetlidis2016c,
abstract = {The synthesis of medical images is an intensity transforma-tion of a given modality in a way that represents an acquisition with a different modality (in the context of MRI this represents the synthesis of images originating from different MR sequences). Most methods fol-low a patch-based approach, which is computationally inefficient during synthesis and requires some sort of 'fusion' to synthesize a whole image from patch-level results. In this paper, we present a whole image synthesis approach that relies on deep neural networks. Our architecture resem-bles those of encoder-decoder networks, which aims to synthesize a source MRI modality to an other target MRI modality. The proposed method is computationally fast, it doesn't require extensive amounts of memory, and produces comparable results to recent patch-based approaches.},
archivePrefix = {arXiv},
arxivId = {1703.00035},
author = {Sevetlidis, Vasileios and Giuffrida, Mario Valerio and Tsaftaris, Sotirios A.},
booktitle = {Simulation and Synthesis in Medical Imaging},
doi = {10.1007/978-3-319-46630-9_13},
eprint = {1703.00035},
isbn = {978-3-319-46629-3},
issn = {07403194},
keywords = {shape,synthetic retinal images,texture,validation},
pages = {127--137},
pmid = {21500255},
title = {{Whole image synthesis using a deep encoder-decoder network}},
year = {2016}
}
@inproceedings{Huang2017b,
abstract = {Magnetic Resonance Imaging (MRI) offers high-resolution $\backslash$emph{\{}in vivo{\}} imaging and rich functional and anatomical multimodality tissue contrast. In practice, however, there are challenges associated with considerations of scanning costs, patient comfort, and scanning time that constrain how much data can be acquired in clinical or research studies. In this paper, we explore the possibility of generating high-resolution and multimodal images from low-resolution single-modality imagery. We propose the weakly-supervised joint convolutional sparse coding to simultaneously solve the problems of super-resolution (SR) and cross-modality image synthesis. The learning process requires only a few registered multimodal image pairs as the training set. Additionally, the quality of the joint dictionary learning can be improved using a larger set of unpaired images. To combine unpaired data from different image resolutions/modalities, a hetero-domain image alignment term is proposed. Local image neighborhoods are naturally preserved by operating on the whole image domain (as opposed to image patches) and using joint convolutional sparse coding. The paired images are enhanced in the joint learning process with unpaired data and an additional maximum mean discrepancy term, which minimizes the dissimilarity between their feature distributions. Experiments show that the proposed method outperforms state-of-the-art techniques on both SR reconstruction and simultaneous SR and cross-modality synthesis.},
archivePrefix = {arXiv},
arxivId = {1705.02596},
author = {Huang, Yawen and Shao, Ling and Frangi, Alejandro F.},
booktitle = {IEEE Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2017.613},
eprint = {1705.02596},
file = {:Users/salmanulhassandar/Library/Application Support/Mendeley Desktop/Downloaded/Huang, Shao, Frangi - 2017 - Simultaneous Super-Resolution and Cross-Modality Synthesis of 3D Medical Images using Weakly-Supervised Joi.pdf:pdf},
pages = {5787--5796},
title = {{Simultaneous super-resolution and cross-modality synthesis of 3D medical images using weakly-supervised joint convolutional sparse coding}},
year = {2017}
}
@article{Roy2013g,
abstract = {The performance of image analysis algorithms applied to magnetic resonance images is strongly influenced by the pulse sequences used to acquire the images. Algorithms are typically optimized for a targeted tissue contrast obtained from a particular implementation of a pulse sequence on a specific scanner. There are many practical situations, including multiinstitution trials, rapid emergency scans, and scientific use of historical data, where the images are not acquired according to an optimal protocol or the desired tissue contrast is entirely missing. This paper introduces an image restoration technique that recovers images with both the desired tissue contrast and a normalized intensity profile. This is done using patches in the acquired images and an atlas containing patches of the acquired and desired tissue contrasts. The method is an examplebased approach relying on sparse reconstruction from image patches. Its performance in demonstrated using several examples, including image intensity normalization, missing tissue contrast recovery, automatic segmentation, and multimodal registration. These examples demonstrate potential practical uses and also illustrate limitations of our approach.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Roy, Snehashis and Carass, Aaron and Prince, Jerry L.},
doi = {10.1109/TMI.2013.2282126},
eprint = {NIHMS150003},
isbn = {2156623929},
issn = {02780062},
journal = TMI,
keywords = {Image restoration,Magnetic resonance imaging (MRI),Neuroimaging,Sparse reconstruction},
number = {12},
pages = {2348--2363},
pmid = {24058022},
title = {{Magnetic resonance image example-based contrast synthesis}},
volume = {32},
year = {2013}
}
@article{Bilgic2011,
author = {Bilgic, Berkin and Goyal, Vivek K and Adalsteinsson, Elfar},
doi = {10.1002/mrm.22956},
file = {:Users/salmanulhassandar/Library/Application Support/Mendeley Desktop/Downloaded/Bilgic, Goyal, Adalsteinsson - 2011 - Multi-contrast reconstruction with Bayesian compressed sensing.pdf:pdf},
issn = {07403194},
journal = MRM,
keywords = {clinical MRI,simultaneous sparse approximation,sparse Bayesian learning},
number = {6},
pages = {1601--1615},
publisher = {Wiley-Blackwell},
title = {{Multi-contrast reconstruction with Bayesian compressed sensing}},
volume = {66},
year = {2011}
}
@article{Ehrhardt2016,
abstract = {Magnetic resonance imaging (MRI) is a versatile imaging technique that allows different contrasts depending on the acquisition parameters. Many clinical imaging studies acquire MRI data for more than one of these contrasts—such as, for instance, T1 and T2 weighted images—which makes the overall scanning procedure very time consuming. As all of these images show the same underlying anatomy, one can try to omit unnecessary measurements by taking the similarity into account dur-ing reconstruction. We will discuss two modifications of total variation—based on (i) location and (ii) direction—that take structural a priori knowledge into account and reduce to total variation in the degenerate case when no structural knowledge is available. We solve the resulting convex mini-mization problem with the alternating direction method of multipliers which separates the forward operator from the prior. For both priors the corresponding proximal operator can be implemented as an extension of the fast gradient projection method on the dual problem for total variation. We tested the priors on six data sets that are based on phantoms and real MRI images. In all test cases, exploiting the structural information from the other contrast yields better results than separate re-construction with total variation in terms of standard metrics like peak signal-to-noise ratio and structural similarity index. Furthermore, we found that exploiting the two-dimensional directional information results in images with well-defined edges, superior to those reconstructed solely using a priori information about the edge location.},
author = {Ehrhardt, Matthias J and Betcke, Marta M},
doi = {10.1137/15M1047325},
file = {:Users/salmanulhassandar/Library/Application Support/Mendeley Desktop/Downloaded/Ehrhardt, Betcke - 2016 - Multicontrast MRI Reconstruction with Structure-Guided Total Variation.pdf:pdf},
journal = {SIAM Journal on Imaging Sciences},
keywords = {49M30,65J22,94A08,MRI,a priori information,image reconstruction,magnetic resonance imaging,regularization,total variation},
number = {3},
pages = {1084--1106},
title = {{Multicontrast MRI Reconstruction with Structure-Guided Total Variation}},
volume = {9},
year = {2016}
}
@inproceedings{Huang2012,
abstract = {This paper proposes an efficient algorithm to simultaneously reconstruct multiple T1/T2-weighted images of the same anatomical cross section from partially sampled k-space data. The simultaneous reconstruction problem is formulated as minimizing a linear combination of three terms corresponding to a least square data fitting, joint total-variation (TV) and group wavelet-sparsity regularization. It is rooted in two observations: 1) the variance of image gradients should be similar for the same spatial position across multiple contrasts; 2) the wavelet coefficients of all images from the same anatomical cross section should have similar sparse modes. To efficiently solve this formulation, we decompose it into group sparsity and joint TV regularization subproblems, respectively. Finally, the reconstructed image is obtained from the weighted average of solutions from two subproblems in an iterative framework. We compare the proposed algorithm with previous methods on SRT24 multi-channel Brain Atlas Data. Experiments demonstrate its superior performance for multi-contrast MR image reconstruction.},
author = {Huang, Junzhou and Chen, Chen and Axel, Leon},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-33415-3_35},
file = {:Users/salmanulhassandar/Library/Application Support/Mendeley Desktop/Downloaded/Huang, Chen, Axel - Unknown - Fast Multi-contrast MRI Reconstruction.pdf:pdf},
isbn = {9783642334146},
issn = {16113349},
pages = {281--288},
title = {{Fast multi-contrast MRI reconstruction}},
volume = {7510 LNCS},
year = {2012}
}
@article{Sun2019,
abstract = {Compressed sensing (CS) theory can accelerate multi-contrast magnetic resonance imaging (MRI) by sampling fewer measurements within each contrast. However, conventional optimization-based reconstruction models suffer several limitations, including a strict assumption of shared sparse support, time-consuming optimization, and 'shallow' models with difficulties in encoding the patterns contained in massive MRI data. In this paper, we propose the first deep learning model for multi-contrast CS-MRI reconstruction. We achieve information sharing through feature sharing units, which significantly reduces the number of model parameters. The feature sharing unit combines with a data fidelity unit to comprise an inference block, which are then cascaded with dense connections, allowing for efficient information transmission across different depths of the network. Experiments on various multi-contrast MRI datasets show that the proposed model outperforms both state-of-the-art single-contrast and multi-contrast MRI methods in accuracy and efficiency. We demonstrate that improved reconstruction quality can bring benefits to subsequent Med Image Anal. Furthermore, the robustness of the proposed model to misregistration shows its potential in real MRI applications.},
archivePrefix = {arXiv},
arxivId = {1804.03596},
author = {Sun, Liyan and Fan, Zhiwen and Fu, Xueyang and Huang, Yue and Ding, Xinghao and Paisley, John},
doi = {10.1109/TIP.2019.2925288},
eprint = {1804.03596},
file = {:Users/salmanulhassandar/Library/Application Support/Mendeley Desktop/Downloaded/Sun et al. - Unknown - A Deep Information Sharing Network for Multi-contrast Compressed Sensing MRI Reconstruction.pdf:pdf},
issn = {19410042},
journal = {IEEE Transactions on Image Processing},
keywords = {Compressed sensing,deep neural networks,multi-contrast MRI reconstruction},
number = {12},
pages = {6141--6153},
title = {{A Deep Information Sharing Network for Multi-Contrast Compressed Sensing MRI Reconstruction}},
volume = {28},
year = {2019}
}
@article{Griswold2002,
abstract = {In this study, a novel partially parallel acquisition (PPA) method is presented which can be used to accelerate image acquisition using an RF coil array for spatial encoding. This technique, GeneRalized Autocalibrating Partially Parallel Acquisitions (GRAPPA) is an extension of both the PILS and VD-AUTO-SMASH reconstruction techniques. As in those previous methods, a detailed, highly accurate RF field map is not needed prior to reconstruction in GRAPPA. This information is obtained from several k-space lines which are acquired in addition to the normal image acquisition. As in PILS, the GRAPPA reconstruction algorithm provides unaliased images from each component coil prior to image combination. This results in even higher SNR and better image quality since the steps of image reconstruction and image combination are performed in separate steps. After introducing the GRAPPA technique, primary focus is given to issues related to the practical implementation of GRAPPA, including the reconstruction algorithm as well as analysis of SNR in the resulting images. Finally, in vivo GRAPPA images are shown which demonstrate the utility of the technique.},
author = {Griswold, Mark A. and Jakob, Peter M. and Heidemann, Robin M. and Nittka, Mathias and Jellus, Vladimir and Wang, Jianmin and Kiefer, Berthold and Haase, Axel},
doi = {10.1002/mrm.10171},
issn = {0740-3194},
journal = MRM,
number = {6},
pages = {1202--1210},
pmid = {12111967},
title = {{Generalized autocalibrating partially parallel acquisitions (GRAPPA)}},
volume = {47},
year = {2002}
}
@article{Pruessmann1999,
abstract = {New theoretical and practical concepts are presented for considerably enhancing the performance of magnetic resonance imaging (MRI) by means of arrays of multiple receiver coils. Sensitivity encoding (SENSE) is based on the fact that receiver sensitivity generally has an encoding effect complementary to Fourier preparation by linear field gradients. Thus, by using multiple receiver coils in parallel scan time in Fourier imaging can be considerably reduced. The problem of image reconstruction from sensitivity encoded data is formulated in a general fashion and solved for arbitrary coil configurations and k-space sampling patterns. Special attention is given to the currently most practical case, namely, sampling a common Cartesian grid with reduced density. For this case the feasibility of the proposed methods was verified both in vitro and in vivo. Scan time was reduced to one-half using a two-coil array in brain imaging. With an array of five coils double-oblique heart images were obtained in one-third of conventional scan time. Magn Reson Med 42:952-962, 1999.},
author = {Pruessmann, K P and Weiger, M and Scheidegger, M B and Boesiger, P},
issn = {0740-3194},
journal = MRM,
number = {5},
pages = {952--62},
pmid = {10542355},
title = {{SENSE: sensitivity encoding for fast MRI.}},
volume = {42},
year = {1999}
}
@article{Lustig2010,
abstract = {A new approach to autocalibrating, coil-by-coil parallel imaging reconstruction, is presented. It is a generalized reconstruction framework based on self-consistency. The reconstruction problem is formulated as an optimization that yields the most consistent solution with the calibration and acquisition data. The approach is general and can accurately reconstruct images from arbitrary k-space sampling patterns. The formulation can flexibly incorporate additional image priors such as off-resonance correction and regularization terms that appear in compressed sensing. Several iterative strategies to solve the posed reconstruction problem in both image and k-space domain are presented. These are based on a projection over convex sets and conjugate gradient algorithms. Phantom and in vivo studies demonstrate efficient reconstructions from undersampled Cartesian and spiral trajectories. Reconstructions that include off-resonance correction and nonlinear l(1)-wavelet regularization are also demonstrated.},
author = {Lustig, Michael and Pauly, John M},
doi = {10.1002/mrm.22428},
file = {:Users/salmanulhassandar/Library/Application Support/Mendeley Desktop/Downloaded/Lustig, Pauly - 2010 - SPIRiT Iterative self-consistent parallel imaging reconstruction from arbitrary k-space(2).pdf:pdf},
issn = {1522-2594},
journal = MRM,
number = {2},
pages = {457--71},
pmid = {20665790},
publisher = {NIH Public Access},
title = {{SPIRiT: Iterative self-consistent parallel imaging reconstruction from arbitrary k-space.}},
volume = {64},
year = {2010}
}

@article{Lustig2007,
abstract = {The sparsity which is implicit in MR images is exploited to significantly undersample k-space. Some MR images such as angiograms are already sparse in the pixel representation; other, more complicated images have a sparse representation in some transform domain-for example, in terms of spatial finite-differences or their wavelet coefficients. According to the recently developed mathematical theory of compressed-sensing, images with a sparse representation can be recovered from randomly undersampled k-space data, provided an appropriate nonlinear recovery scheme is used. Intuitively, artifacts due to random undersampling add as noise-like interference. In the sparse transform domain the significant coefficients stand out above the interference. A nonlinear thresholding scheme can recover the sparse coefficients, effectively recovering the image itself. In this article, practical incoherent undersampling schemes are developed and analyzed by means of their aliasing interference. Incoherence is introduced by pseudo-random variable-density undersampling of phase-encodes. The reconstruction is performed by minimizing the l(1) norm of a transformed image, subject to data fidelity constraints. Examples demonstrate improved spatial resolution and accelerated acquisition for multislice fast spin-echo brain imaging and 3D contrast enhanced angiography.},
author = {Lustig, Michael and Donoho, David and Pauly, John M.},
doi = {10.1002/mrm.21391},
issn = {07403194},
journal = MRM,
number = {6},
pages = {1182--1195},
pmid = {17969013},
title = {{Sparse MRI: The application of compressed sensing for rapid MR imaging}},
volume = {58},
year = {2007}
}

@article{Lustig2008,
author = {Lustig, M. and Donoho, D.L. and Santos, J.M. and Pauly, J.M.},
doi = {10.1109/MSP.2007.914728},
issn = {1053-5888},
journal = SPM,
number = {2},
pages = {72--82},
title = {{Compressed Sensing MRI}},
volume = {25},
year = {2008}
}
@article{Dar2019,
abstract = {This work was supported in part by a European Molecular Biology Organization Installation Grant (IG 3028), by a TUBA GEBIP fellowship, and by a BAGEP fellowship awarded to T. {\c{C}}ukur. We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan GPUs used in this study. To be submitted to IEEE Trans Med Imaging  Abstract Acquiring images of the same anatomy with multiple different contrasts increases the diversity of diagnostic information available in an MR exam. Yet, scan time limitations may prohibit acquisition of certain contrasts, and images for some contrast may be corrupted by noise and artifacts. In such cases, the ability to synthesize unacquired or corrupted contrasts from remaining contrasts can improve diagnostic utility. For multi-contrast synthesis, current methods learn a nonlinear intensity transformation between the source and target images, either via nonlinear regression or deterministic neural networks. These methods can in turn suffer from loss of high-spatial-frequency information in synthesized images. Here we propose a new approach for multi-contrast MRI synthesis based on conditional generative adversarial networks. The proposed approach preserves high-frequency details via an adversarial loss; and it offers enhanced synthesis performance via a pixel-wise loss for registered multi-contrast images and a cycle-consistency loss for unregistered images. Information from neighboring cross-sections are utilized to further improved synthesis quality. Demonstrations on T1-and T2-weighted images from healthy subjects and patients clearly indicate the superior performance of the proposed approach compared to previous state-of-the-art methods. Our synthesis approach can help improve quality and versatility of multi-contrast MRI exams without the need for prolonged examinations.},
author = {Dar, Salman Ul Hassan and Yurt, Mahmut and Karacan, Levent and Erdem, Aykut and Erdem, Erkut and {\c{C}}ukur, Tolga},
file = {:Users/salmanulhassandar/Library/Application Support/Mendeley Desktop/Downloaded/Dar et al. - 2018 - Image Synthesis in Multi-Contrast MRI with Conditional Generative Adversarial Networks.pdf:pdf},
journal = TMI,
keywords = {cycle-consistency loss,generative adversarial network,image synthesis,multi-contrast MRI,pixel-wise loss},
number = {312},
pages = {1--1},
title = {{Image Synthesis in Multi-Contrast MRI with Conditional Generative Adversarial Networks}},
volume = {90},
year = {2019}
}
@article{Mardani2017,
author = {Mardani, Morteza and Monajemi, Hatef and Papyan, Vardan and Vasanawala, Shreyas and Donoho, David and Pauly, John},
journal = {arXiv:1711.10046},
title = {Recurrent Generative Adversarial Networks for Proximal Learning and Automated Compressive Image Recovery},
year = {2017}
}
@article{Yang2018,
author = {Yang, Qianye and Li, Nannan and Zhao, Zixu and Fan, Xingyu and {I-Chao Chang}, Eric and Xu, Yan},
journal = {arXiv:1801.06940},
title = {{MRI image-to-image translation for cross-modality image registration and segmentation}},
year = {2018}
}
@article{Mardani2018,
abstract = {Undersampled magnetic resonance image (MRI) reconstruction is typically an ill-posed linear inverse task. The time and resource intensive computations require trade offs between accuracy and speed. In addition, state-of-the-art compressed sensing (CS) analytics are not cognizant of the image diagnostic quality. To address these challenges, we propose a novel CS framework that uses generative adversarial networks (GAN) to model the (low-dimensional) manifold of high-quality MR images. Leveraging a mixture of least-squares (LS) GANs and pixel-wise ℓ1/ℓ2 cost, a deep residual network with skip connections is trained as the generator that learns to remove the aliasing artifacts by projecting onto the image manifold. The LSGAN learns the texture details, while the ℓ1/ℓ2 cost suppresses high-frequency noise. A discriminator network, which is a multilayer convolutional neural network (CNN), plays the role of a perceptual cost that is then jointly trained based on high quality MR images to score the quality of retrieved images. In the operational phase, an initial aliased estimate (e.g., simply obtained by zero-filling) is propagated into the trained generator to output the desired reconstruction. This demands very low computational overhead. Extensive evaluations are performed on a large contrast-enhanced MR dataset of pediatric patients. Images rated by expert radiologists corroborate that GANCS retrieves higher quality images with improved fine texture details compared with conventional Wavelet-based and dictionary-learning based CS schemes as well as with deeplearning based schemes using pixel-wise training. In addition, it offers reconstruction times of under a few milliseconds, which is two orders of magnitude faster than current state-of-the-art CS-MRI schemes.},
author = {Mardani, Morteza and Gong, Enhao and Cheng, Joseph Y. and Vasanawala, Shreyas S. and Zaharchuk, Greg and Xing, Lei and Pauly, John M.},
journal = TMI,
volume = {38},
number = {1},
pages = {167--179},
title = {Deep Generative Adversarial Neural Networks for Compressive Sensing {(GANCS) MRI}},
year = {2018}
}
@article{Hammernik2018,
author = {Hammernik, Kerstin and Klatzer, Teresa and Kobler, Erich and Recht, Michael P. and Sodickson, Daniel K. and Pock, Thomas and Knoll, Florian},
doi = {10.1002/mrm.26977},
file = {:Users/salmanulhassandar/Library/Application Support/Mendeley Desktop/Downloaded/Hammernik et al. - 2018 - Learning a variational network for reconstruction of accelerated MRI data.pdf:pdf},
issn = {07403194},
journal = MRM,
keywords = {accelerated MRI,compressed sensing,deep learning,image reconstruction,parallel imaging,variational network},
number = {6},
pages = {3055--3071},
publisher = {Wiley-Blackwell},
title = {{Learning a variational network for reconstruction of accelerated MRI data}},
volume = {79},
year = {2018}
}
@incollection{Schlemper2017a,
author = {Schlemper, Jo and Caballero, Jose and Hajnal, Joseph V. and Price, Anthony and Rueckert, Daniel},
doi = {10.1007/978-3-319-59050-9_51},
file = {:Users/salmanulhassandar/Library/Application Support/Mendeley Desktop/Downloaded/Schlemper et al. - 2017 - A Deep Cascade of Convolutional Neural Networks for MR Image Reconstruction(2).pdf:pdf},
pages = {647--658},
publisher = {Springer, Cham},
title = {{A Deep Cascade of Convolutional Neural Networks for MR Image Reconstruction}},
year = {2017}
}
@inproceedings{Johnson2016,
abstract = {We consider image transformation problems, where an input image is transformed into an output image. Recent methods for such problems typically train feed-forward convolutional neural networks using a $\backslash$emph{\{}per-pixel{\}} loss between the output and ground-truth images. Parallel work has shown that high-quality images can be generated by defining and optimizing $\backslash$emph{\{}perceptual{\}} loss functions based on high-level features extracted from pretrained networks. We combine the benefits of both approaches, and propose the use of perceptual loss functions for training feed-forward networks for image transformation tasks. We show results on image style transfer, where a feed-forward network is trained to solve the optimization problem proposed by Gatys et al in real-time. Compared to the optimization-based method, our network gives similar qualitative results but is three orders of magnitude faster. We also experiment with single-image super-resolution, where replacing a per-pixel loss with a perceptual loss gives visually pleasing results.},
archivePrefix = {arXiv},
arxivId = {1603.08155},
author = {Johnson, Justin and Alahi, Alexandre and Fei-Fei, Li},
booktitle = {Computer Vision -- ECCV 2016},
doi = {10.1007/978-3-319-46475-6_43},
eprint = {1603.08155},
isbn = {9783319464749},
issn = {16113349},
keywords = {Deep learning,Style transfer,Super-resolution},
pages = {694--711},
pmid = {10463930},
title = {{Perceptual losses for real-time style transfer and super-resolution}},
year = {2016}
}

@article{ho2020denoising,
  title={Denoising diffusion probabilistic models},
  author={Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  journal={NeurIPS},
  volume={33},
  pages={6840--6851},
  year={2020}
}

@article{VAR,
  title={Visual autoregressive modeling: Scalable image generation via next-scale prediction},
  author={Tian, Keyu and Jiang, Yi and Yuan, Zehuan and Peng, Bingyue and Wang, Liwei},
  journal={arXiv:2404.02905},
  year={2024}
}

@article{gungor2023adaptive,
  title={Adaptive diffusion priors for accelerated MRI reconstruction},
  author={G{\"u}ng{\"o}r, Alper and Dar, Salman UH and {\"O}zt{\"u}rk, {\c{S}}aban and Korkmaz, Yilmaz and Bedel, Hasan A and Elmas, Gokberk and Ozbey, Muzaffer and {\c{C}}ukur, Tolga},
  journal={Med Image Anal},
  volume={88},
  pages={102872},
  year={2023},
  publisher={Elsevier}
}

@inproceedings{Goodfellow2014a,
abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
archivePrefix = {arXiv},
arxivId = {1406.2661},
author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
booktitle = NIPS,
doi = {10.1001/jamainternmed.2016.8245},
eprint = {1406.2661},
file = {:Users/salmanulhassandar/Library/Application Support/Mendeley Desktop/Downloaded/Goodfellow et al. - 2014 - Generative Adversarial Networks.pdf:pdf},
isbn = {1406.2661},
issn = {10495258},
pages = {2672--2680},
pmid = {15040217},
title = {{Generative adversarial networks}},
year = {2014}
}

@inproceedings{Mao2017,
abstract = {Unsupervised learning with generative adversarial networks (GANs) has proven hugely successful. Regular GANs hypothesize the discriminator as a classifier with the sigmoid cross entropy loss function. However, we found that this loss function may lead to the vanishing gradients problem during the learning process. To overcome such a problem, we propose in this paper the Least Squares Generative Adversarial Networks (LSGANs) which adopt the least squares loss function for the discriminator. We show that minimizing the objective function of LSGAN yields minimizing the Pearson {\$}\backslashchi{\^{}}2{\$} divergence. There are two benefits of LSGANs over regular GANs. First, LSGANs are able to generate higher quality images than regular GANs. Second, LSGANs perform more stable during the learning process. We evaluate LSGANs on five scene datasets and the experimental results show that the images generated by LSGANs are of better quality than the ones generated by regular GANs. We also conduct two comparison experiments between LSGANs and regular GANs to illustrate the stability of LSGANs.},
archivePrefix = {arXiv},
arxivId = {1611.04076},
author = {Mao, Xudong and Li, Qing and Xie, Haoran and Lau, Raymond Y. K. and Wang, Zhen and Smolley, Stephen Paul},
booktitle = {IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2017.304},
eprint = {1611.04076},
pages = {2813--2821},
title = {{Least squares generative adversarial networks}},
year = {2017}
}

@article{Mirza2014a,
abstract = {Generative Adversarial Nets [8] were recently introduced as a novel way to train generative models. In this work we introduce the conditional version of generative adversarial nets, which can be constructed by simply feeding the data, y, we wish to condition on to both the generator and discriminator. We show that this model can generate MNIST digits conditioned on class labels. We also illustrate how this model could be used to learn a multi-modal model, and provide preliminary examples of an application to image tagging in which we demonstrate how this approach can generate descriptive tags which are not part of training labels.},
archivePrefix = {arXiv},
arxivId = {1411.1784},
author = {Mirza, Mehdi and Osindero, Simon},
eprint = {1411.1784},
file = {:Users/salmanulhassandar/Library/Application Support/Mendeley Desktop/Downloaded/Mirza, Osindero - 2014 - Conditional Generative Adversarial Nets.pdf:pdf},
isbn = {1406.2661},
issn = {10495258},
journal = {arXiv preprint},
title = {{Conditional generative adversarial nets}},
year = {2014}
}

@article{Bullitt2005,
abstract = {Rationale and Objectives. Malignancy provokes regional changes to vessel shape. Characteristic vessel tortuosity abnormalities appear early during tumor development, affect initially healthy vessels, spread beyond the confines of tumor margins, and do not simply mirror tissue perfusion. The ability to detect and quantify tortuosity abnormalities on high-resolution magnetic resonance angiography (MRA) images offers a new approach to the noninvasive diagnosis of malignancy. This report evaluates a computerized, statistical method of analyzing the shapes of vessels extracted from MRA in diagnosing cancer. Materials and Methods. The regional vasculature of 34 healthy subjects was compared with the tumor-associated vasculature of 30 brain tumors before surgical resection. The operator performing the analysis was blinded to the diagnosis. Vessels were segmented from an MRA of each subject, a region of interest was defined in each tumor patient and was mapped to all healthy controls, and a statistical analysis of vessel shape measures was then performed over the region of interest. Many difficult cases were included, such as pinpoint, hemorrhagic, and irradiated tumors, as were hypervascular benign tumors. Tumors were identified as benign or malignant on the basis of histological evaluation. Results. A discriminant analysis performed at the study's conclusion successfully classified all but one of the 30 tumors as benign or malignant on the basis of vessel tortuosity. Conclusions. Quantitative, statistical measures of vessel shape offer a new approach to the diagnosis and staging of disease. Although the methods developed under the current report must be tested against a new series of cases, initial results are promising. {\textcopyright} AUR, 2005.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Bullitt, Elizabeth and Zeng, Donglin and Gerig, Guido and Aylward, Stephen and Joshi, Sarang and Smith, J. Keith and Lin, Weili and Ewend, Matthew G.},
doi = {10.1016/j.acra.2005.05.027},
eprint = {NIHMS150003},
isbn = {1076-6332 (Print)$\backslash$r1076-6332 (Linking)},
issn = {10766332},
journal = {Academic Radiology},
keywords = {Blood vessels,Brain tumor,Cancer,Computer,MRA,Tortuosity},
mendeley-groups = {Paper (Transfer Learning)},
number = {10},
pages = {1232--1240},
pmid = {16179200},
title = {{Vessel tortuosity and brain tumor malignancy: A blinded study}},
volume = {12},
year = {2005}
}

@article{Jenkinson2001,
abstract = {Registration is an important component of Med Image Anal and for analysing large amounts of data it is desirable to have fully automatic registration methods. Many different automatic registration methods have been proposed to date, and almost all share a common mathematical framework - one of optimising a cost function. To date little attention has been focused on the optimisation method itself, even though the success of most registration methods hinges on the quality of this optimisation. This paper examines the assumptions underlying the problem of registration for brain images using inter-modal voxel similarity measures. It is demonstrated that the use of local optimisation methods together with the standard multi-resolution approach is not sufficient to reliably find the global minimum. To address this problem, a global optimisation method is proposed that is specifically tailored to this form of registration. A full discussion of all the necessary implementation details is included as this is an important part of any practical method. Furthermore, results are presented for inter-modal, inter-subject registration experiments that show that the proposed method is more reliable at finding the global minimum than several of the currently available registration packages in common usage. {\textcopyright} 2001 Elsevier Science B.V. All rights reserved.},
author = {Jenkinson, Mark and Smith, Stephen},
doi = {10.1016/S1361-8415(01)00036-6},
isbn = {1441865222739},
issn = {13618415},
journal = {Med Image Anal},
keywords = {Affine transformation,Global optimisation,Multi-resolution search,Multimodal registration,Robustness},
number = {2},
pages = {143--156},
pmid = {11516708},
title = {{A global optimisation method for robust affine registration of brain images}},
volume = {5},
year = {2001}
}

@article{Jenkinson2002,
abstract = {Linear registration and motion correction are important components of structural and functional brain image analysis. Most modern methods optimize some intensity-based cost function to determine the best registration. To date, little attention has been focused on the optimization method itself, even though the success of most registration methods hinges on the quality of this optimization. This paper examines the optimization process in detail and demonstrates that the commonly used multiresolution local optimization methods can, and do, get trapped in local minima. To address this problem, two approaches are taken: (1) to apodize the cost function and (2) to employ a novel hybrid global-local optimization method. This new optimization method is specifically designed for registering whole brain images. It substantially reduces the likelihood of producing misregistrations due to being trapped by local minima. The increased robustness of the method, compared to other commonly used methods, is demonstrated by a consistency test. In addition, the accuracy of the registration is demonstrated by a series of experiments with motion correction. These motion correction experiments also investigate how the results are affected by different cost functions and interpolation methods. {\textcopyright} 2002 Elsevier Science (USA).},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Jenkinson, Mark and Bannister, Peter and Brady, Michael and Smith, Stephen},
doi = {10.1016/S1053-8119(02)91132-8},
eprint = {arXiv:1011.1669v3},
isbn = {1053-8119 (Print) 1053-8119 (Linking)},
issn = {10538119},
journal = {NeuroImage},
keywords = {Accuracy,Affine transformation,Global optimization,Motion correction,Multimodal registration,Multiresolution search,Robustness},
number = {2},
pages = {825--841},
pmid = {12377157},
title = {{Improved optimization for the robust and accurate linear registration and motion correction of brain images}},
volume = {17},
year = {2002}
}

@inproceedings{Zhu2017a,
abstract = {Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain {\$}X{\$} to a target domain {\$}Y{\$} in the absence of paired examples. Our goal is to learn a mapping {\$}G: X \backslashrightarrow Y{\$} such that the distribution of images from {\$}G(X){\$} is indistinguishable from the distribution {\$}Y{\$} using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping {\$}F: Y \backslashrightarrow X{\$} and introduce a cycle consistency loss to push {\$}F(G(X)) \backslashapprox X{\$} (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.},
archivePrefix = {arXiv},
arxivId = {1703.10593},
author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
booktitle = {IEEE International Conference on Computer Vision},
eprint = {1703.10593},
file = {:Users/salmanulhassandar/Library/Application Support/Mendeley Desktop/Downloaded/Zhu et al. - 2017 - Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks.pdf:pdf},
pages = {2223--2232},
title = {{Unpaired image-to-image translation using cycle-consistent adversarial networks}},
year = {2017}
}

@inproceedings{Kingma2015,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods.},
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Kingma, Diederik P. and Ba, Jimmy Lei},
booktitle = {International Conference on Learning Representations},
doi = {http://doi.acm.org.ezproxy.lib.ucf.edu/10.1145/1830483.1830503},
eprint = {1412.6980},
isbn = {9781450300728},
issn = {09252312},
mendeley-groups = {Paper (Transfer Learning)},
pmid = {172668},
title = {{Adam: a Method for Stochastic Optimization}},
year = {2015}
}


@article{Ulyanov2016,
abstract = {It this paper we revisit the fast stylization method introduced in Ulyanov et. al. (2016). We show how a small change in the stylization architecture results in a significant qualitative improvement in the generated images. The change is limited to swapping batch normalization with instance normalization, and to apply the latter both at training and testing times. The resulting method can be used to train high-performance architectures for real-time image generation. The code will is made available on github at https://github.com/DmitryUlyanov/texture{\_}nets. Full paper can be found at arXiv:1701.02096.},
archivePrefix = {arXiv},
arxivId = {1607.08022},
author = {Ulyanov, Dmitry and Vedaldi, Andrea and Lempitsky, Victor},
eprint = {1607.08022},
file = {:Users/salmanulhassandar/Library/Application Support/Mendeley Desktop/Downloaded/Ulyanov, Vedaldi, Lempitsky - 2016 - Instance Normalization The Missing Ingredient for Fast Stylization.pdf:pdf},
journal = {arXiv preprint},
title = {{Instance normalization: The missing ingredient for fast stylization}},
year = {2016}
}

@article{Zhang2013,
abstract = {MRI using receiver arrays with many coil elements can provide high signal-to-noise ratio and increase parallel imaging acceleration. At the same time, the growing number of elements results in larger datasets and more computation in the reconstruction. This is of particular concern in 3D acquisitions and in iterative reconstructions. Coil compression algorithms are effective in mitigating this problem by compressing data from many channels into fewer virtual coils. In Cartesian sampling there often are fully sampled k-space dimensions. In this work, a new coil compression technique for Cartesian sampling is presented that exploits the spatially varying coil sensitivities in these nonsubsampled dimensions for better compression and computation reduction. Instead of directly compressing in k-space, coil compression is performed separately for each spatial location along the fully sampled directions, followed by an additional alignment process that guarantees the smoothness of the virtual coil sensitivities. This important step provides compatibility with autocalibrating parallel imaging techniques. Its performance is not susceptible to artifacts caused by a tight imaging field-of-view. High quality compression of in vivo 3D data from a 32 channel pediatric coil into six virtual coils is demonstrated.},
author = {Zhang, Tao and Pauly, John M and Vasanawala, Shreyas S and Lustig, Michael},
doi = {10.1002/mrm.24267},
journal = MRM,
mendeley-groups = {Paper (Transfer Learning)},
number = {2},
pages = {571--82},
title = {{Coil compression for accelerated imaging with Cartesian sampling.}},
volume = {69},
year = {2013}
}

@article{Uecker2014,
author = {Uecker, Martin and Lai, Peng and Murphy, Mark J. and Virtue, Patrick and Elad, Michael and Pauly, John M. and Vasanawala, Shreyas S. and Lustig, Michael},
journal = MRM,
mendeley-groups = {Paper (Transfer Learning)},
number = {3},
pages = {990--1001},
title = {{ESPIRiT-an eigenvalue approach to autocalibrating parallel MRI: Where SENSE meets GRAPPA}},
volume = {71},
year = {2014}
}

@article{Kim2018,
author = {Kim, Ki Hwan and Do, Won‐Joon and Park, Sung‐Hong},
doi = {10.1002/mp.12945},
issn = {0094-2405},
journal = MP,
keywords = {convolutional neural network,generative adversarial network,magnetic resonance imaging},
mendeley-groups = {Paper (Synth+Recon)},
number = {7},
pages = {3120--3131},
publisher = {John Wiley {\&} Sons, Ltd},
title = {{Improving resolution of MR images with an adversarial network incorporating images with different contrast}},
volume = {45},
year = {2018}
}

@article{Russakovsky2015,
abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the five years of the challenge, and propose future directions and improvements.},
archivePrefix = {arXiv},
arxivId = {1409.0575},
author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
doi = {10.1007/s11263-015-0816-y},
eprint = {1409.0575},
isbn = {0920-5691},
issn = {15731405},
journal = {International Journal of Computer Vision},
keywords = {Benchmark,Dataset,Large-scale,Object detection,Object recognition},
mendeley-groups = {Paper (Transfer Learning)},
number = {3},
pages = {211--252},
pmid = {16190471},
title = {{ImageNet Large Scale Visual Recognition Challenge}},
volume = {115},
year = {2015}
}


@article{Akcakaya2019,
author = {Ak{\c{c}}akaya, Mehmet and Moeller, Steen and Weing{\"{a}}rtner, Sebastian and Uğurbil, K{\^{a}}mil},
doi = {10.1002/mrm.27420},
file = {:Users/salmanulhassandar/Library/Application Support/Mendeley Desktop/Downloaded/Ak{\c{c}}akaya et al. - 2019 - Scan-specific robust artificial-neural-networks for k-space interpolation (RAKI) reconstruction Database-fr(2).pdf:pdf},
issn = {07403194},
journal = MRM,
keywords = {accelerated imaging,convolutional neural networks,deep learning,image reconstruction,k‐space interpolation,nonlinear estimation,parallel imaging},
mendeley-groups = {Paper (Transfer Learning)},
number = {1},
pages = {439--453},
publisher = {John Wiley {\&} Sons, Ltd},
title = {{Scan-specific robust artificial-neural-networks for k-space interpolation (RAKI) reconstruction: Database-free deep learning for fast imaging}},
volume = {81},
year = {2019}
}

@article{Kwon2017,
author = {Kwon, Kinam and Kim, Dongchan and Park, HyunWook},
doi = {10.1002/mp.12600},
file = {:Users/salmanulhassandar/Library/Application Support/Mendeley Desktop/Downloaded/Kwon, Kim, Park - 2017 - A parallel MR imaging method using multilayer perceptron.pdf:pdf},
issn = {00942405},
journal = MP,
keywords = {artificial neural networks (ANN),machine learning,magnetic resonance imaging (MRI),multilayer perceptron (MLP),parallel imaging},
mendeley-groups = {Paper (Transfer Learning),Paper (Transfer Learning)/nn rec},
number = {12},
pages = {6209--6224},
publisher = {John Wiley {\&} Sons, Ltd},
title = {{A parallel MR imaging method using multilayer perceptron}},
volume = {44},
year = {2017}
}

@article{Hyun2018,
abstract = {This paper presents a deep learning method for faster magnetic resonance imaging (MRI) by reducing k-space data with sub-Nyquist sampling strategies and provides a rationale for why the proposed approach works well. Uniform subsampling is used in the time-consuming phase-encoding direction to capture high-resolution image information, while permitting the image-folding problem dictated by the Poisson summation formula. To deal with the localization uncertainty due to image folding, a small number of low-frequency k-space data are added. Training the deep learning net involves input and output images that are pairs of the Fourier transforms of the subsampled and fully sampled k-space data. Our experiments show the remarkable performance of the proposed method; only 29 of the k-space data can generate images of high quality as effectively as standard MRI reconstruction with the fully sampled data.},
archivePrefix = {arXiv},
arxivId = {1709.02576},
author = {Hyun, Chang Min and Kim, Hwa Pyung and Lee, Sung Min and Lee, Sungchul and Seo, Jin Keun},
doi = {10.1088/1361-6560/aac71a},
eprint = {1709.02576},
file = {:Users/salmanulhassandar/Library/Application Support/Mendeley Desktop/Downloaded/Hyun et al. - 2017 - Deep learning for undersampled MRI reconstruction.pdf:pdf},
issn = {13616560},
journal = {Physics in Medicine and Biology},
keywords = {deep learning,fast MRI,magnetic resonance imaging,undersampling},
mendeley-groups = {Paper (Transfer Learning),Paper (Transfer Learning)/nn rec},
number = {13},
pages = {135007},
title = {{Deep learning for undersampled MRI reconstruction}},
volume = {63},
year = {2018}
}



@article{Han2018a,
abstract = {Purpose: The radial k-space trajectory is a well-established sampling trajectory used in conjunction with magnetic resonance imaging. However, the radial k-space trajectory requires a large number of radial lines for high-resolution reconstruction. Increasing the number of radial lines causes longer acquisition time, making it more difficult for routine clinical use. On the other hand, if we reduce the number of radial lines, streaking artifact patterns are unavoidable. To solve this problem, we propose a novel deep learning approach with domain adaptation to restore high-resolution MR images from under-sampled k-space data. Methods: The proposed deep network removes the streaking artifacts from the artifact corrupted images. To address the situation given the limited available data, we propose a domain adaptation scheme that employs a pre-trained network using a large number of x-ray computed tomography (CT) or synthesized radial MR datasets, which is then fine-tuned with only a few radial MR datasets. Results: The proposed method outperforms existing compressed sensing algorithms, such as the total variation and PR-FOCUSS methods. In addition, the calculation time is several orders of magnitude faster than the total variation and PR-FOCUSS methods.Moreover, we found that pre-training using CT or MR data from similar organ data is more important than pre-training using data from the same modality for different organ. Conclusion: We demonstrate the possibility of a domain-adaptation when only a limited amount of MR data is available. The proposed method surpasses the existing compressed sensing algorithms in terms of the image quality and computation time.},
archivePrefix = {arXiv},
arxivId = {1703.01135},
author = {Han, Yoseob and Yoo, Jaejun and Kim, Hak Hee and Shin, Hee Jung and Sung, Kyunghyun and Ye, Jong Chul},
doi = {10.1002/mrm.27106},
eprint = {1703.01135},
file = {:Users/salmanulhassandar/Library/Application Support/Mendeley Desktop/Downloaded/Han, Yoo, Ye - 2017 - Deep Learning with Domain Adaptation for Accelerated Projection Reconstruction MR.pdf:pdf},
isbn = {0740-3194},
issn = {15222594},
journal = MRM,
keywords = {compressed sensing,convolutional neural network,deep learning,domain adaptation,projection reconstruction MRI},
mendeley-groups = {Paper (Synth+Recon)/Recon (NN),Neural Nets/Medical Imaging/Image Reconstruction,Neural Nets/Medical Imaging,Paper (Transfer Learning),Paper (Transfer Learning)/nn rec},
pages = {1189--1205},
pmid = {29399869},
title = {{Deep learning with domain adaptation for accelerated projection-reconstruction MR}},
volume = {80},
year = {2018}
}


@article{Cheng2018,
abstract = {To increase the flexibility and scalability of deep neural networks for image reconstruction, a framework is proposed based on bandpass filtering. For many applications, sensing measurements are performed indirectly. For example, in magnetic resonance imaging, data are sampled in the frequency domain. The introduction of bandpass filtering enables leveraging known imaging physics while ensuring that the final reconstruction is consistent with actual measurements to maintain reconstruction accuracy. We demonstrate this flexible architecture for reconstructing subsampled datasets of MRI scans. The resulting high subsampling rates increase the speed of MRI acquisitions and enable the visualization rapid hemodynamics. Index Terms-Magnetic resonance imaging (MRI), Compres-sive sensing, Image reconstruction-iterative methods, Machine learning, Image enhancement/restoration(noise and artifact reduction).},
archivePrefix = {arXiv},
arxivId = {1805.03300v2},
author = {Cheng, Joseph Y and Chen, Feiyu and Alley, Marcus T and Pauly, John M and Vasanawala, Shreyas S},
eprint = {1805.03300v2},
file = {:Users/salmanulhassandar/Library/Application Support/Mendeley Desktop/Downloaded/Cheng et al. - Unknown - Highly Scalable Image Reconstruction using Deep Neural Networks with Bandpass Filtering(2).pdf:pdf},
journal = {arXiv preprint},
mendeley-groups = {Neural Nets/Medical Imaging/Transfer Learning},
title = {{Highly Scalable Image Reconstruction using Deep Neural Networks with Bandpass Filtering}},
year = {2018}
}

@article{Dar2017,
abstract = {Purpose: Neural networks have received recent interest for reconstruction of undersampled MR acquisitions. Ideally network performance should be optimized by drawing the training and testing data from the same domain. In practice, however, large datasets comprising hundreds of subjects scanned under a common protocol are rare. The goal of this study is to introduce a transfer-learning approach to address the problem of data scarcity in training deep networks for accelerated MRI. Methods: Neural networks were trained on thousands of samples from public datasets of either natural images or brain MR images. The networks were then fine-tuned using only few tens of brain MR images in a distinct testing domain. Domain-transferred networks were compared to networks trained directly in the testing domain. Network performance was evaluated for varying acceleration factors (2-10), number of training samples (0.5-4k) and number of fine-tuning samples (0-100). Results: The proposed approach achieves successful domain transfer between MR images acquired with different contrasts (T1- and T2-weighted images), and between natural and MR images (ImageNet and T1- or T2-weighted images). Networks obtained via transfer-learning using only tens of images in the testing domain achieve nearly identical performance to networks trained directly in the testing domain using thousands of images. Conclusion: The proposed approach might facilitate the use of neural networks for MRI reconstruction without the need for collection of extensive imaging datasets.},
author = {Dar, Salman Ul Hassan and {\"{O}}zbey, Muzaffer and {\c{C}}atlı, Ahmet Burak and {\c{C}}ukur, Tolga},
title = {A Transfer-Learning Approach for Accelerated {MRI} using Deep Neural Networks},
year = {2020},
journal = MRM,
volume = {84},
number = {2},
pages= {663-685}
}

@inproceedings{Yu2018,
author = {Yu, Biting and Zhou, Luping and Wang, Lei and Fripp, Jurgen and Bourgeat, Pierrick},
booktitle = {IEEE 15th International Symposium on Biomedical Imaging},
doi = {10.1109/ISBI.2018.8363653},
file = {:Users/salmanulhassandar/Library/Application Support/Mendeley Desktop/Downloaded/Yu et al. - 2018 - 3D cGAN based cross-modality MR image synthesis for brain tumor segmentation.pdf:pdf},
isbn = {978-1-5386-3636-7},
mendeley-groups = {Paper (Synthesis)/Refereese recommendation,Paper (Synthesis)/Refereese recommendation/translaiton,Paper (Synthesis)},
pages = {626--630},
publisher = {IEEE},
title = {{3D cGAN based cross-modality MR image synthesis for brain tumor segmentation}},
year = {2018}
}

@article{Yu2019,
author = {Yu, Biting and Zhou, Luping and Wang, Lei and Shi, Yinghuan and Fripp, Jurgen and Bourgeat, Pierrick},
doi = {10.1109/TMI.2019.2895894},
issn = {0278-0062},
journal = TMI,
mendeley-groups = {Paper (Synth+Recon)},
number = {7},
pages = {1750--1762},
title = {{Ea-GANs: Edge-Aware Generative Adversarial Networks for Cross-Modality MR Image Synthesis}},
volume = {38},
year = {2019}
}


@article{Sharma2019,
abstract = {Magnetic resonance imaging (MRI) is being increasingly utilized to assess, diagnose, and plan treatment for a variety of diseases. The ability to visualize tissue in varied contrasts in the form of MR pulse sequences in a single scan provides valuable insights to physicians, as well as enabling automated systems performing downstream analysis. However many issues like prohibitive scan time, image corruption, different acquisition protocols, or allergies to certain contrast materials may hinder the process of acquiring multiple sequences for a patient. This poses challenges to both physicians and automated systems since complementary information provided by the missing sequences is lost. In this paper, we propose a variant of generative adversarial network (GAN) capable of leveraging redundant information contained within multiple available sequences in order to generate one or more missing sequences for a patient scan. The proposed network is designed as a multi-input, multi-output network which combines information from all the available pulse sequences, implicitly infers which sequences are missing, and synthesizes the missing ones in a single forward pass. We demonstrate and validate our method on two brain MRI datasets each with four sequences, and show the applicability of the proposed method in simultaneously synthesizing all missing sequences in any possible scenario where either one, two, or three of the four sequences may be missing. We compare our approach with competing unimodal and multi-modal methods, and show that we outperform both quantitatively and qualitatively.},
archivePrefix = {arXiv},
arxivId = {1904.12200},
author = {Sharma, Anmol and Hamarneh, Ghassan},
eprint = {1904.12200},
file = {:Users/salmanulhassandar/Library/Application Support/Mendeley Desktop/Downloaded/Sharma, Hamarneh - 2019 - Missing MRI Pulse Sequence Synthesis using Multi-Modal Generative Adversarial Network.pdf:pdf},
journal = {arXiv preprint},
mendeley-groups = {Paper (Synth+Recon)},
title = {{Missing MRI Pulse Sequence Synthesis using Multi-Modal Generative Adversarial Network}},
year = {2019}
}

@incollection{Olut2018,
author = {Olut, Sahin and Sahin, Yusuf H. and Demir, Ugur and Unal, Gozde},
booktitle = {Predictive Intelligence in Medicine},
doi = {10.1007/978-3-030-00320-3_18},
file = {:Users/salmanulhassandar/Library/Application Support/Mendeley Desktop/Downloaded/Olut et al. - 2018 - Generative Adversarial Training for MRA Image Synthesis Using Multi-contrast MRI.pdf:pdf},
mendeley-groups = {Paper (Synth+Recon)},
pages = {147--154},
publisher = {Springer, Cham},
title = {{Generative Adversarial Training for MRA Image Synthesis Using Multi-contrast MRI}},
year = {2018}
}

@article{Yurt2019,
abstract = {Multi-contrast MRI protocols increase the level of morphological information available for diagnosis. Yet, the number and quality of contrasts is limited in practice by various factors including scan time and patient motion. Synthesis of missing or corrupted contrasts can alleviate this limitation to improve clinical utility. Common approaches for multi-contrast MRI involve either one-to-one and many-to-one synthesis methods. One-to-one methods take as input a single source contrast, and they learn a latent representation sensitive to unique features of the source. Meanwhile, many-to-one methods receive multiple distinct sources, and they learn a shared latent representation more sensitive to common features across sources. For enhanced image synthesis, here we propose a multi-stream approach that aggregates information across multiple source images via a mixture of multiple one-to-one streams and a joint many-to-one stream. The shared feature maps generated in the many-to-one stream and the complementary feature maps generated in the one-to-one streams are combined with a fusion block. The location of the fusion block is adaptively modified to maximize task-specific performance. Qualitative and quantitative assessments on T1-, T2-, PD-weighted and FLAIR images clearly demonstrate the superior performance of the proposed method compared to previous state-of-the-art one-to-one and many-to-one methods.},
archivePrefix = {arXiv},
arxivId = {1909.11504v1},
author = {Yurt, Mahmut and Dar, Salman Uh and Erdem, Aykut and Erdem, Erkut and {\c{C}}ukur, Tolga},
eprint = {1909.11504v1},
file = {:Users/salmanulhassandar/Library/Application Support/Mendeley Desktop/Downloaded/Yurt et al. - Unknown - mustGAN Multi-Stream Generative Adversarial Networks for MR Image Synthesis.pdf:pdf},
journal = {arXiv preprint},
mendeley-groups = {Paper (Synth+Recon)},
title = {{mustGAN: Multi-Stream Generative Adversarial Networks for MR Image Synthesis}},
year = {2019}
}

@article{Lee2019,
abstract = {Figure 1: Image translation tasks using (a) cross-domain models, (b) StarGAN, and (c) the proposed collaborative GAN (CollaGAN). Cross-domain model needs large number of generators to handle multi-class data. StarGAN and CollaGAN use a single generator with one input and multiple inputs, respectively, to synthesize the target domain image. Abstract In many applications requiring multiple inputs to obtain a desired output, if any of the input data is missing, it often introduces large amounts of bias. Although many techniques have been developed for imputing missing data, the image imputation is still difficult due to complicated nature of natural images. To address this problem, here we proposed a novel framework for missing image data im-putation, called Collaborative Generative Adversarial Network (CollaGAN). CollaGAN convert the image imputa-tion problem to a multi-domain images-to-image translation task so that a single generator and discriminator network can successfully estimate the missing data using the remaining clean data set. We demonstrate that CollaGAN produces the images with a higher visual quality compared to the existing competing approaches in various image im-putation tasks.},
archivePrefix = {arXiv},
arxivId = {1901.09764v3},
author = {Lee, Dongwook and Kim, Junyoung and Moon, Won-Jin and Ye, Jong Chul},
eprint = {1901.09764v3},
file = {:Users/salmanulhassandar/Library/Application Support/Mendeley Desktop/Downloaded/Lee et al. - 2019 - CollaGAN Collaborative GAN for Missing Image Data Imputation.pdf:pdf},
journal = {arXiv preprint},
mendeley-groups = {Paper (Synth+Recon)},
title = {{CollaGAN: Collaborative GAN for Missing Image Data Imputation}},
year = {2019}
}

@article{Welander2018,
abstract = {In medical imaging, a general problem is that it is costly and time consuming to collect high quality data from healthy and diseased subjects. Generative adversarial networks (GANs) is a deep learning method that has been developed for synthesizing data. GANs can thereby be used to generate more realistic training data, to improve classification performance of machine learning algorithms. Another application of GANs is image-to-image translations, e.g. generating magnetic resonance (MR) images from computed tomogra-phy (CT) images, which can be used to obtain multimodal datasets from a single modality. Here, we evaluate two un-supervised GAN models (CycleGAN and UNIT) for image-to-image translation of T1-and T2-weighted MR images, by comparing generated synthetic MR images to ground truth images. We also evaluate two supervised models; a modification of CycleGAN and a pure generator model. A small perceptual study was also performed to evaluate how visually realistic the synthesized images are. It is shown that the implemented GAN models can synthesize visually realistic MR images (incorrectly labeled as real by a human). It is also shown that models producing more visually realistic synthetic images not necessarily have better quantitative error measurements , when compared to ground truth data. Code is available at https://github.com/simontomaskarlsson/GAN-MRI.},
archivePrefix = {arXiv},
arxivId = {1806.07777v1},
author = {Welander, Per and Karlsson, Simon and Eklund, Anders},
eprint = {1806.07777v1},
file = {:Users/salmanulhassandar/Library/Application Support/Mendeley Desktop/Downloaded/Welander, Karlsson, Eklund - Unknown - GENERATIVE ADVERSARIAL NETWORKS FOR IMAGE-TO-IMAGE TRANSLATION ON MULTI-CONTRAST MR IMAGES-A COMP.pdf:pdf},
journal = {arXiv preprint},
mendeley-groups = {Paper (Synth+Recon)},
title = {{Generative Adversarial Networks for Image-to-Image Translation on Multi-Contrast MR Images - A Comparison of CycleGAN and UNIT}},
year = {2018}
}

@article{Li2019,
abstract = {Synthesizing MR imaging sequences is highly relevant in clinical practice, as single sequences are often missing or are of poor quality (e.g. due to motion). Naturally, the idea arises that a target modality would benefit from multi-modal input, as proprietary information of individual modalities can be synergistic. However, existing methods fail to scale up to multiple non-aligned imaging modalities, facing common drawbacks of complex imaging sequences. We propose a novel, scalable and multi-modal approach called DiamondGAN. Our model is capable of performing flexible non-aligned cross-modality synthesis and data in-fill, when given multiple modalities or any of their arbitrary subsets, learning structured information in an end-to-end fashion. We synthesize two MRI sequences with clinical relevance (i.e., double inversion recovery (DIR) and contrast-enhanced T1 (T1-c)), reconstructed from three common sequences. In addition, we perform a multi-rater visual evaluation experiment and find that trained radiologists are unable to distinguish synthetic DIR images from real ones.},
archivePrefix = {arXiv},
arxivId = {1904.12894v4},
author = {Li, Hongwei and Paetzold, Johannes C and Sekuboyina, Anjany and Kofler, Florian and Zhang, Jianguo and Kirschke, Jan S and Menze, Bjoern},
eprint = {1904.12894v4},
file = {:Users/salmanulhassandar/Library/Application Support/Mendeley Desktop/Downloaded/Li et al. - Unknown - DiamondGAN Unified Multi-Modal Generative Adversarial Networks for MRI Sequences Synthesis.pdf:pdf},
journal = {arXiv preprint},
mendeley-groups = {Paper (Synth+Recon)},
title = {{DiamondGAN: Unified Multi-Modal Generative Adversarial Networks for MRI Sequences Synthesis}},
year = {2019}
}


@article{Hagiwara2019,
abstract = {BACKGROUND AND PURPOSE Synthetic FLAIR images are of lower quality than conventional FLAIR images. Here, we aimed to improve the synthetic FLAIR image quality using deep learning with pixel-by-pixel translation through conditional generative adversarial network training. MATERIALS AND METHODS Forty patients with MS were prospectively included and scanned (3T) to acquire synthetic MR imaging and conventional FLAIR images. Synthetic FLAIR images were created with the SyMRI software. Acquired data were divided into 30 training and 10 test datasets. A conditional generative adversarial network was trained to generate improved FLAIR images from raw synthetic MR imaging data using conventional FLAIR images as targets. The peak signal-to-noise ratio, normalized root mean square error, and the Dice index of MS lesion maps were calculated for synthetic and deep learning FLAIR images against conventional FLAIR images, respectively. Lesion conspicuity and the existence of artifacts were visually assessed. RESULTS The peak signal-to-noise ratio and normalized root mean square error were significantly higher and lower, respectively, in generated-versus-synthetic FLAIR images in aggregate intracranial tissues and all tissue segments (all P {\textless} .001). The Dice index of lesion maps and visual lesion conspicuity were comparable between generated and synthetic FLAIR images (P = 1 and .59, respectively). Generated FLAIR images showed fewer granular artifacts (P = .003) and swelling artifacts (in all cases) than synthetic FLAIR images. CONCLUSIONS Using deep learning, we improved the synthetic FLAIR image quality by generating FLAIR images that have contrast closer to that of conventional FLAIR images and fewer granular and swelling artifacts, while preserving the lesion contrast.},
author = {Hagiwara, A and Otsuka, Y and Hori, M and Tachibana, Y and Yokoyama, K and Fujita, S and Andica, C and Kamagata, K and Irie, R and Koshino, S and Maekawa, T and Chougar, L and Wada, A and Takemura, M Y and Hattori, N and Aoki, S},
doi = {10.3174/ajnr.A5927},
file = {:Users/salmanulhassandar/Library/Application Support/Mendeley Desktop/Downloaded/Hagiwara et al. - 2019 - Improving the Quality of Synthetic FLAIR Images with Deep Learning Using a Conditional Generative Adversarial N.pdf:pdf},
issn = {1936-959X},
journal = {AJNR. American Journal of Neuroradiology},
mendeley-groups = {Paper (Synth+Recon)},
number = {2},
pages = {224--230},
pmid = {30630834},
publisher = {American Journal of Neuroradiology},
title = {{Improving the Quality of Synthetic FLAIR Images with Deep Learning Using a Conditional Generative Adversarial Network for Pixel-by-Pixel Image Translation.}},
volume = {40},
year = {2019}
}

@article{Mehta2018,
abstract = {Accurate synthesis of a full 3D MR image containing tumours from available MRI (e.g. to replace an image that is currently unavailable or corrupted) would provide a clinician as well as downstream inference methods with important complementary information for disease analysis. In this paper, we present an end-to-end 3D convolution neural network that takes a set of acquired MR image sequences (e.g. T1, T2, T1ce) as input and concurrently performs (1) regression of the missing full resolution 3D MRI (e.g. FLAIR) and (2) segmentation of the tumour into subtypes (e.g. enhancement, core). The hypothesis is that this would focus the network to perform accurate synthesis in the area of the tumour. Experiments on the BraTS 2015 and 2017 datasets [1] show that: (1) the proposed method gives better performance than state-of-the-art methods in terms of established global evaluation metrics (e.g. PSNR), (2) replacing real MR volumes with the synthesized MRI does not lead to significant degradation in tumour and sub-structure segmentation accuracy. The system further provides uncertainty estimates based on Monte Carlo (MC) dropout [11] for the synthesized volume at each voxel, permitting quantification of the system's confidence in the output at each location.},
archivePrefix = {arXiv},
arxivId = {1807.10972},
author = {Mehta, Raghav and Arbel, Tal},
eprint = {1807.10972},
file = {:Users/salmanulhassandar/Library/Application Support/Mendeley Desktop/Downloaded/Mehta, Arbel - 2018 - RS-Net Regression-Segmentation 3D CNN for Synthesis of Full Resolution Missing Brain MRI in the Presence of Tumour.pdf:pdf},
journal = {arXiv preprint},
mendeley-groups = {Paper (Synth+Recon)},
title = {{RS-Net: Regression-Segmentation 3D CNN for Synthesis of Full Resolution Missing Brain MRI in the Presence of Tumours}},
year = {2018}
}

@article{do2019reconstruction,
  title={Reconstruction of Multi-contrast MR Images through Deep Learning},
  author={Do, Won-Joon and Seo, Sunghun and Han, Yoseob and Ye, Jong Chul and Hong Choi, Seung and Park, Sung-Hong},
  journal=MP,
  year={2019},
  publisher={Wiley Online Library}
}

%@ARTICLE{xianmultimoda2019,  author={L. {Xiang} and Y. {Chen} and W. {Chang} and Y. {Zhan} and W. {Lin} and Q. {Wang} and D. {Shen}},  journal={IEEE Trans Biomed Eng},  title={Deep-Learning-Based Multi-Modal Fusion for Fast MR Reconstruction},   year={2019},  volume={66},  number={7},  pages={2105-2114}
}

@article{falvo2019multimodal,
  title={A multimodal deep network for the reconstruction of T2W MR images},
  author={Falvo, Antonio and Comminiello, Danilo and Scardapane, Simone and Finesi, Giorgio and Scarpiniti, Michele and Uncini, Aurelio},
  journal={arXiv preprint},
  year={2019}
}

@INPROCEEDINGS{Falvo2019MS,  author={A. {Falvo} and D. {Comminiello} and S. {Scardapane} and M. {Scarpiniti} and A. {Uncini}},  booktitle={IEEE MLSP},  title={A Multimodal Dense U-Net For Accelerating Multiple Sclerosis MRI},   year={2019},  volume={},  number={},  pages={1-6}
}

@article{Polakjointvvn2020,
author = {Polak, Daniel and Cauley, Stephen and Bilgic, Berkin and Gong, Enhao and Bachert, Peter and Adalsteinsson, Elfar and Setsompop, Kawin},
title = {Joint multi-contrast variational network reconstruction {(jVN)} with application to rapid {2D} and {3D} imaging},
journal = MRM,
volume = {84},
number = {3},
year = {2020},
pages = {1456-1469}
}
%end to end variational
@InProceedings{Variatonal_end2end,
author="Sriram, Anuroop
and Zbontar, Jure
and Murrell, Tullie
and Defazio, Aaron
and Zitnick, C. Lawrence
and Yakubova, Nafissa
and Knoll, Florian
and Johnson, Patricia",
editor="Martel, Anne L.
and Abolmaesumi, Purang
and Stoyanov, Danail
and Mateus, Diana
and Zuluaga, Maria A.
and Zhou, S. Kevin
and Racoceanu, Daniel
and Joskowicz, Leo",
title="End-to-End Variational Networks for Accelerated {MRI} Reconstruction",
booktitle={MICCAI},
year="2020",
pages="64--73"
}

@inproceedings{gong2017improving,
  title={Improving the PI+ CS reconstruction for highly undersampled multi-contrast MRI using local deep network},
  author={Gong, Enhao and Zaharchuk, Greg and Pauly, John},
   booktitle = {ISMRM},
  address = {Honolulu},
  year = {2017},
  pages = {5663}
}

@article{lee2018deep,
  title={Deep residual learning for accelerated {MRI} using magnitude and phase networks},
  author={Lee, Dongwook and Yoo, Jaejun and Tak, Sungho and Ye, Jong Chul},
  journal={IEEE Trans Biomed Eng},
  volume={65},
  number={9},
  pages={1985--1995},
  year={2018}
  }

@article{shan2016progressive,
  title={Progressive brain changes in patients with chronic fatigue syndrome: a longitudinal MRI study},
  author={Shan, Zack Y and Kwiatek, Richard and Burnet, Richard and Del Fante, Peter and Staines, Donald R and Marshall-Gradisnik, Sonya M and Barnden, Leighton R},
  journal={J Magn Reson Imaging},
  volume={44},
  number={5},
  pages={1301--1311},
  year={2016},
  publisher={Wiley Online Library}
}

@article{mills2014methods,
  title={Methods and considerations for longitudinal structural brain imaging analysis across development},
  author={Mills, Kathryn L and Tamnes, Christian K},
  journal={Dev Cognit Neurosci},
  volume={9},
  pages={172--190},
  year={2014},
  publisher={Elsevier}
}

@article{duffy2018retrospective,
  title={Retrospective correction of motion artifact affected structural MRI images using deep learning of simulated motion},
  author={Duffy, Ben A and Zhang, Wenlu and Tang, Haoteng and Zhao, Lu and Law, Meng and Toga, Arthur W and Kim, Hosung},
  year={2018}
}

@InProceedings{VQGAN,
    author    = {Esser, Patrick and Rombach, Robin and Ommer, Bjorn},
    title     = {Taming Transformers for High-Resolution Image Synthesis},
    booktitle = {CVPR},
    year      = {2021},
    pages     = {12873-12883}}

@inproceedings{adaln,
      title={Understanding and Improving Layer Normalization}, 
      author={Jingjing Xu and Xu Sun and Zhiyuan Zhang and Guangxiang Zhao and Junyang Lin},
      year={2019},
      booktitle={NeurIPS},
      pages = {1-11}
}

@inproceedings{huang2017arbitrary,
  title={Arbitrary style transfer in real-time with adaptive instance normalization},
  author={Huang, Xun and Belongie, Serge},
  booktitle={ICCV},
  pages={1501--1510},
  year={2017}
}


@article{dhariwal2021diffusion,
  title={Diffusion models beat gans on image synthesis},
  author={Dhariwal, Prafulla and Nichol, Alexander},
  journal={NeurIPS},
  volume={34},
  pages={8780--8794},
  year={2021}
}

@inproceedings{chen2020generative,
  title={Generative pretraining from pixels},
  author={Chen, Mark and Radford, Alec and Child, Rewon and Wu, Jeffrey and Jun, Heewoo and Luan, David and Sutskever, Ilya},
  booktitle={ICML},
  pages={1691--1703},
  year={2020}
}


@inproceedings{ramesh2021zero,
  title={Zero-shot text-to-image generation},
  author={Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
  booktitle={ICML},
  pages={8821--8831},
  year={2021}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{ozbey2022unsupervised,
  title={Unsupervised Medical Image Translation with Adversarial Diffusion Models},
  author={{\"O}zbey, Muzaffer and Dalmaz, Onat and Dar, Salman UH and Bedel, Hasan A and {\"O}zturk, {\c{S}}aban and G{\"u}ng{\"o}r, Alper and {\c{C}}ukur, Tolga},
  journal={IEEE Trans Med Imaging},
  volume = {42},
  number = {12},
  pages = {3524--3539},
  year={2023}
}
@article{SelfRDB,
      title={Self-Consistent Recursive Diffusion Bridge for Medical Image Translation}, 
      author={Fuat Arslan and Bilal Kabas and Onat Dalmaz and Muzaffer Ozbey and Tolga Çukur},
      year={2024},
      journal={arXiv:2405.06789},
      archivePrefix={arXiv},
      primaryClass={eess.IV}
}

@article{bedel2023dreamr,
      title={{DreaMR: Diffusion-driven} Counterfactual Explanation for Functional {MRI}}, 
      author={Hasan Atakan Bedel and Tolga Çukur},
      year={2024},
      journal={IEEE Trans Med Imaging},
      doi = {10.1109/TMI.2024.3507008}
}

@inproceedings{korkmaz2023selfsupervised,
      title={Self-Supervised {MRI} Reconstruction with Unrolled Diffusion Models}, 
      author={Yilmaz Korkmaz and Tolga Cukur and Vishal Patel},
      booktitle="Med Imag Comput Comput Assist Int",
      year="2023",
      pages="491--501"
}

@article{guo2022reconformer,
  title={Reconformer: Accelerated mri reconstruction using recurrent transformer},
  author={Guo, Pengfei and Mei, Yiqun and Zhou, Jinyuan and Jiang, Shanshan and Patel, Vishal M},
  journal={IEEE Trans Med Imaging},
  year={2024}
}

@article{fdb,
  title={Learning Fourier-constrained diffusion bridges for MRI reconstruction},
  author={Mirza, Muhammad U and Dalmaz, Onat and Bedel, Hasan A and Elmas, Gokberk and Korkmaz, Yilmaz and Gungor, Alper and Dar, Salman UH and {\c{C}}ukur, Tolga},
  journal={arXiv:2308.01096},
  year={2023}
}
@article{denomamba,
      title={{DenoMamba: A fused state-space model for low-dose CT denoising}}, 
      author={Saban Ozturk and Oguz Can Duran and Tolga \c{C}ukur},
      year={2024},
      journal={arXiv:2409.13094},
      archivePrefix={arXiv}
}
@article{mambaroll,
      title={Physics-Driven Autoregressive State Space Models for Medical Image Reconstruction}, 
      author={Bilal Kabas and Fuat Arslan and Valiyeh A. Nezhad and Saban Ozturk and Emine U. Saritas and Tolga Çukur},
      year={2024},
      journal={arXiv:2412.09331},
      archivePrefix={arXiv},
      primaryClass={eess.IV}
}

@ARTICLE{I2IMamba,
       author = {{Atli}, Omer F. and {Kabas}, Bilal and {Arslan}, Fuat and {Yurt}, Mahmut and {Dalmaz}, Onat and {{\c{C}}ukur}, Tolga},
        title = "{I2I-Mamba: Multi-modal medical image synthesis via selective state space modeling}",
      journal = {arXiv:2405.14022},
          year = 2024,
archivePrefix = {arXiv},
       eprint = {2405.14022},
 primaryClass = {eess.IV}}

@inproceedings{sohl2015deep,
  title={Deep unsupervised learning using nonequilibrium thermodynamics},
  author={Sohl-Dickstein, Jascha and Weiss, Eric and Maheswaranathan, Niru and Ganguli, Surya},
  booktitle={ICML},
  pages={2256--2265},
  year={2015}
}

@article{luo2023bayesian,
  title={Bayesian MRI reconstruction with joint uncertainty estimation using diffusion models},
  author={Luo, Guanxiong and Blumenthal, Moritz and Heide, Martin and Uecker, Martin},
  journal={Magn Reson Med},
  volume={90},
  number={1},
  pages={295--311},
  year={2023},
  publisher={Wiley Online Library}
}


@inproceedings{jalaln2021nips,
 author = {Jalal, Ajil and Arvinte, Marius and Daras, Giannis and Price, Eric and Dimakis, Alexandros G and Tamir, Jon},
 booktitle = {NeurIPS},
 pages = {14938--14954},
 title = {Robust Compressed Sensing MRI with Deep Generative Priors},
 volume = {34},
 year = {2021}
}

@article{chung2022media,
title = {Score-based diffusion models for accelerated MRI},
journal = {Med Image Anal},
volume = {80},
pages = {102479},
year = {2022},
issn = {1361-8415},
author = {Hyungjin Chung and Jong Chul Ye},
keywords = {Score-based models, Diffusion models, Inverse problems, MRI}
}


@INPROCEEDINGS{multi-scaletransformer,
title={Multiscale Vision Transformers},
pages={6824--6835},
year = {2021},
booktitle = {ICCV}, 
author ={Fan, Haoqi and Xiong, Bo and Mangalam, Karttikeya and Li, Yanghao and Yan, Zhicheng and Malik, Jitendra and Feichtenhofer, Christoph}
}

@article{VQVAE,
      title={Neural Discrete Representation Learning}, 
      author={Aaron van den Oord and Oriol Vinyals and Koray Kavukcuoglu},
      year={2018},
      journal={arXiv:1711.00937},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@INPROCEEDINGS{resnet,
title={Deep residual learning for image recognition},
pages={770-778},
year = {2016},
booktitle = {CVPR}, 
author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
}



@article{wang2024fed,
title = {Model-based federated learning for accurate MR image reconstruction from undersampled k-space data},
journal = {Comput Biol Med},
volume = {180},
pages = {108905},
year = {2024},
author = {Ruoyou Wu and Cheng Li and Juan Zou and Yong Liang and Shanshan Wang},
keywords = {Federated learning, Adaptive dynamic aggregation, Unfolding neural network, Magnetic resonance imaging (MRI)}
}


@Article{levac2023bio,
AUTHOR = {Levac, Brett R. and Arvinte, Marius and Tamir, Jonathan I.},
TITLE = {Federated End-to-End Unrolled Models for Magnetic Resonance Image Reconstruction},
JOURNAL = {Bioeng},
VOLUME = {10},
YEAR = {2023},
NUMBER = {3},
ARTICLE-NUMBER = {364}
}


@ARTICLE{yan2024jbhi,
  author={Yan, Yunlu and Wang, Hong and Huang, Yawen and He, Nanjun and Zhu, Lei and Xu, Yong and Li, Yuexiang and Zheng, Yefeng},
  journal={IEEE J Biomed Health Inf}, 
  title={Cross-Modal Vertical Federated Learning for MRI Reconstruction}, 
  year={2024},
  volume={28},
  number={11},
  pages={6384-6394}}

@INPROCEEDINGS {feng2023cvpr,
author = {Feng, Chun-Mei and Li, Bangjun and Xu, Xinxing and Liu, Yong and Fu, Huazhu and Zuo, Wangmeng},
booktitle = {CVPR},
title = {{Learning Federated Visual Prompt in Null Space for MRI Reconstruction}},
year = {2023},
pages = {8064-8073}}

@ARTICLE{feng2023tmi,
  author={Feng, Chun-Mei and Yan, Yunlu and Wang, Shanshan and Xu, Yong and Shao, Ling and Fu, Huazhu},
  journal={IEEE Trans Med Imaging}, 
  title={Specificity-Preserving Federated Learning for MR Image Reconstruction}, 
  year={2023},
  volume={42},
  number={7},
  pages={2010-2021}}

@article{heckel_deep_2024,
	title = {Deep learning for accelerated and robust {MRI} reconstruction},
	volume = {37},
	number = {3},
	journal = {Magn Reson Mat Phys Biol Med},
	author = {Heckel, Reinhard and Jacob, Mathews and Chaudhari, Akshay and Perlman, Or and Shimron, Efrat},
	year = {2024},
	pages = {335--368}
}


@article{AutoRSurvey,
  title={Autoregressive Models in Vision: A Survey},
  author={Xiong, Jing and Liu, Gongye and Huang, Lun and Wu, Chengyue and Wu, Taiqiang and Mu, Yao and Yao, Yuan and Shen, Hui and Wan, Zhongwei and Huang, Jinfa and others},
  journal={arXiv preprint arXiv:2411.05902},
  year={2024}
}


@inproceedings{wang2016,
  title={Accelerating magnetic resonance imaging via deep learning},
  author={Wang, Shanshan and Su, Zhenghang and Ying, Leslie and Peng, Xi and Zhu, Shun and Liang, Feng and Feng, Dagan and Liang, Dong},
  booktitle={ISBI},
  pages={514--517},
  organization={IEEE},
  year={2016},
}

@inproceedings{attention,
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  booktitle = {Adv Neural Inf Process Sys},
  pages = {5998--6008},
  title = {Attention Is All You Need},
  volume = {30},
  year = {2017}
}

@book{haykin1998neural,
  title={Neural networks: a comprehensive foundation},
  author={Haykin, Simon},
  year={1998},
  publisher={Prentice Hall PTR}
}


@inproceedings{LDM,
  title={High-resolution image synthesis with latent diffusion models},
  author={Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\"o}rn},
  booktitle={CVPR},
  pages={10684--10695},
  year={2022}
}

@inproceedings{zhang2018perceptual,
  title={The Unreasonable Effectiveness of Deep Features as a Perceptual Metric},
  author={Zhang, Richard and Isola, Phillip and Efros, Alexei A and Shechtman, Eli and Wang, Oliver},
  booktitle={CVPR},
  year={2018}
}

@inproceedings{FedDDA,
author = {Zhao, Zhuang and Yang, Feng and Liang, Guirong},
title = {Federated Learning Based on Diffusion Model to Cope with Non-IID Data},
year = {2023},
booktitle = {Proceedings of PRCV},
pages = {220–231}
}

@article{calgary,
title = {An open, multi-vendor, multi-field-strength brain MR dataset and analysis of publicly available skull stripping methods agreement},
journal = {NeuroImage},
volume = {170},
pages = {482-494},
year = {2018},
author = {Roberto Souza and Oeslle Lucena and Julia Garrafa and David Gobbi and Marina Saluzzi and Simone Appenzeller and Letícia Rittner and Richard Frayne and Roberto Lotufo}
}

@inproceedings{fedprox,
      title={Federated Optimization in Heterogeneous Networks}, 
      author={Tian Li and Anit Kumar Sahu and Manzil Zaheer and Maziar Sanjabi and Ameet Talwalkar and Virginia Smith},
      year={2020},
      booktitle={MLSys}
}