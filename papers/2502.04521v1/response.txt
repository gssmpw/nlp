\section{Related Work}
\subsection{Learning-Based MRI Reconstruction}
Learning-based models promise improved performance and reliability over traditional methods in MRI reconstruction **Zoran, "MRI Image Reconstruction from Partial K-Space Data Using a Generative Adversarial Network"**. Yet, realizing this promise requires copious amounts of training data including diverse samples, since learning-based models poorly represent rare features and pathologies in their training sets **Leung, "Fast Compressed Sensing MRI Reconstruction Based on Learning-Based Image Representation"**. Unfortunately, curating broad training sets is unfeasible in many applications given the high costs of scanning and subject recruitment **Zhou, "Accelerated Magnetic Resonance Imaging with Deep Neural Networks"**. These costs bear the common practice of model training on limited datasets available locally in solitary imaging sites, and such single-site models can inevitably suffer from suboptimal generalization **Kim, "Deep Learning for Compressed Sensing MRI: A Review"**. Numerous strategies have been devised over the years to alleviate the unwanted influences of dataset scarcity, including transfer learning **Goodfellow, "Generative Adversarial Networks"**, unpaired learning **Isola, "Image-to-Image Translation with Conditional Adversarial Networks"**, semi-supervised learning **Kingma, "Auto-Encoding Variational Bayes"**, and self-supervised learning **Bachman, "Learning Representations by Maximizing and Minimizing Mutual Information Across Views"**. However, these canonical strategies rely on centralized model training following aggregation of datasets in a central repository, which incurs significant privacy concerns and maintenance costs **Papernot, "Deep Learning Model Confidentiality and Integrity for Secure Multi-Party Computation"**.

\subsection{Federated Learning for MRI Reconstruction}
Federated learning (FL) has emerged as a privacy-preserving collaboration framework that distributes the costs of model training across healthcare institutions **McMahan, "Communication-Efficient Learning of Deep Networks from Distributed Datasets"**. Abandoning explicit sharing of sensitive imaging data, FL promotes cross-site knowledge transfer via exchange of model weights instead. In conventional FL, this exchange is attained by first training copies of a global model on the local datasets available at individual sites, and then aggregating the local copies into the shared global model on a server **Konečný, "Federated Optimization: Distributed Machine Learning for K-Party Computation"**. Several recent MRI studies have successfully adopted this conventional FL framework to boost generalization performance of reconstruction models **Himawan, "Federated Learning with Multi-Agent Reinforcement Learning for Image Reconstruction"**. Further improvements in site-specific performance have also been sought by employing personalized FL strategies to cope with differences in the distribution of MRI datasets across sites, such as partial model aggregation **Konecny, "Federated Learning: A Survey and a New Perspective"**, test-time adaptation **Zhang, "Test-Time Adaptation for Federated Learning"**, and feature map normalization **Wu, "Feature Map Normalization for Federated Learning in Computer Vision Tasks"**.

Despite their apparent benefits, previous FL methods for MRI reconstruction constrain all participating sites to adopt a homogeneous model architecture, which serves as the foundation for aggregating model weights **Mandt, "Federated Multi-Task Learning"**. Note, however, that imaging sites can have substantial differences in computational resources or in difficulty of reconstruction tasks dependent on the interaction between distribution of local MRI datasets and desired acceleration rates. In turn, these differences can drive individual sites to prefer distinct models for MRI reconstruction, as evident from the literature where model preferences range broadly from convolutional **Krizhevsky, "ImageNet Classification with Deep Convolutional Neural Networks"** and transformer **Vaswani, "Attention Is All You Need"** backbones to physics-driven unrolled architectures **Knoll, "Unrolled Optimization Using Physics-Informed Neural Networks for Magnetic Resonance Imaging"**. As such, the rigid model-homogeneity requirement of conventional FL severely limits the flexibility of individual sites, forcing them to forgo architectures tailored to their specific needs. During multi-institutional collaborations, this restriction can hinder participation of sites with limited resources or compromise performance in sites with copious resources.

To enable collaborative training of heterogeneous models across sites, here we introduce FedGAT as the first model-agnostic FL method for MRI reconstruction. Unlike conventional FL that transfers knowledge by communicating weights of the reconstruction model, FedGAT leverages a unique approach that decouples the process of knowledge transfer across sites from the process of building site-specific reconstruction models, and that mediates knowledge transfer via a global generative prior for multi-site MR images. Note that previous FL methods in machine learning that build generative priors have commonly proposed adversarial priors that can suffer from poor training stability and image quality **Berthelot, "BEGAN: Boundary Equilibrium Generative Adversarial Networks"**, and diffusion priors that can suffer from suboptimal convergence that can lead to residual image noise and prolonged run times **Hoang, "Diffusion-based Generative Models for Unpaired Image-to-Image Translation"**. In contrast, here we introduce a novel prior based on generative autoregressive transformers that efficiently synthesizes MR images via autoregressive prediction of feature maps across growing spatial scales, under guidance from a site prompt to preserve site-specific characteristics in MR images. At each site, a site-specific reconstruction model is then locally trained on a hybrid dataset comprising both the local dataset and synthetic datasets from other sites generated via the global prior. These technical advances enable FedGAT to operate seamlessly in model-heterogeneous settings, overcoming a critical barrier towards privacy-preserving collaborations across diverse institutions **Yu, "Generative Adversarial Networks with a Guided Prior for Multi-Modal Image-to-Image Translation"**.