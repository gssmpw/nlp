\documentclass[journal,twoside,web]{ieeecolor}
\usepackage{tmi2}
\usepackage{cite}
\usepackage{comment}
\usepackage{amsmath,amssymb,amsfonts,mathbbol}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{float}
\usepackage{adjustbox}
\usepackage{caption}
\usepackage{stfloats}
\usepackage{soul}
\usepackage[dvipsnames]{xcolor}
\usepackage{textcomp}
\usepackage[utf8]{inputenc}
\usepackage{longtable}
\usepackage{pifont} % Add this to the preamble
\usepackage{multirow}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\setlength{\floatsep}{3ex}
\setlength{\dblfloatsep}{3ex}
\setlength{\textfloatsep}{3ex}
\setlength{\dbltextfloatsep}{3ex}
\setlength{\intextsep}{3ex}
\def\baselinestretch{1.0}
\setlength{\textheight}{1.000\textheight}
\setlength{\headsep}{4ex}
\setlength{\abovedisplayskip}{4pt}
\setlength{\belowdisplayskip}{4pt}
\setlength{\abovedisplayshortskip}{4pt}
\setlength{\belowdisplayshortskip}{4pt}
\setlength{\parskip}{0mm plus1mm minus0mm}
\setlength\tabcolsep{3pt}
\renewcommand{\arraystretch}{1.3}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    pdftitle={FedGAT},
}

\SetKwInput{KwInput}{Input}                % Set the Input
\SetKwInput{KwOutput}{Output}              % set the Output

\usepackage[font=small,justification=justified,belowskip=2pt,aboveskip=2pt]{caption}
\setlength{\skip\footins}{3pt}

\usepackage[math]{cellspace}
\cellspacetoplimit 2pt
\cellspacebottomlimit 2pt

\floatname{algorithm}{Algorithm}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

\def\SPSB#1#2{\rlap{\textsuperscript{{#1}}}\SB{#2}}
\def\SP#1{\textsuperscript{#1}}
\def\SB#1{\textsubscript{#1}}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\markboth{}{Generative Autoregressive Transformers for Model-Agnostic Federated MRI Reconstruction}
\begin{document}
\title{Generative Autoregressive Transformers for Model-Agnostic Federated MRI Reconstruction}
\author{Valiyeh A. Nezhad, Gokberk Elmas, Bilal Kabas, Fuat Arslan, and Tolga \c{C}ukur$^*$ \vspace{-0.8cm}
\\
\thanks{\\
This study was supported in part by a TUBA GEBIP 2015 fellowship, and a BAGEP 2017 fellowship (Corresponding author: Tolga Çukur).}
\thanks{V.A. Nezhad, G. Elmas, B. Kabas, F. Arslan, and T. Çukur are with the Department of Electrical and Electronics Engineering, and the National Magnetic Resonance Research Center, Bilkent University, Ankara, Turkey (e-mail: \{valiyeh.ansarian@, gokberk@ee., bilal.kabas@, fuat.arslan@, cukur@ee.\}bilkent.edu.tr).}
}

\maketitle

\begin{abstract}
Although learning-based models hold great promise for MRI reconstruction, single-site models built on limited local datasets often suffer from poor generalization. This challenge has spurred interest in collaborative model training on multi-site datasets via federated learning (FL)-- a privacy-preserving framework that aggregates model updates instead of sharing imaging data. Conventional FL builds a global model by aggregating locally trained model weights, inherently constraining all sites to a homogeneous model architecture. This rigid homogeneity requirement forces sites to forgo architectures tailored to their compute infrastructure and application-specific demands. Consequently, existing FL methods for MRI reconstruction fail to support model-heterogeneous settings, where individual sites are allowed to use distinct architectures. To overcome this fundamental limitation, here we introduce FedGAT, a novel model-agnostic FL technique based on generative autoregressive transformers. FedGAT decentralizes the training of a global generative prior that captures the distribution of multi-site MR images. For enhanced fidelity, we propose a novel site-prompted GAT prior that controllably synthesizes MR images from desired sites via autoregressive prediction across spatial scales. Each site then trains its site-specific reconstruction model--using its preferred architecture--on a hybrid dataset comprising the local MRI dataset and GAT-generated synthetic MRI datasets for other sites. Comprehensive experiments on multi-institutional datasets demonstrate that FedGAT supports flexible collaborations while enjoying superior within-site and across-site reconstruction performance compared to state-of-the-art FL baselines. 
\end{abstract}

\begin{IEEEkeywords}
MRI, reconstruction, federated learning, model agnostic, generative, autoregressive, transformer. 
\end{IEEEkeywords}


\bstctlcite{IEEEexample:BSTcontrol}

\section{Introduction}
\IEEEPARstart{M}{agnetic} Resonance Imaging (MRI) is an indispensable diagnostic modality due to its exceptional soft-tissue contrast. Yet, characteristically long scan times in MRI exams can disrupt clinical workflows and reduce patient comfort \cite{Lustig2007}. A mainstream approach to improve scan efficiency relies on reconstruction of MR images from undersampled acquisitions \cite{Zhao2015,haldar2016p}. Learning-based reconstruction models have shown particular promise in recovering high-quality images from undersampled k-space data \cite{Wang2016, Hammernik2017, Kwon2017, Dar2017, Han2018a, ADMM-CSNET, Zhu2018, raki, Xiang2019}. Despite their potential, models trained on local datasets available at individual sites often struggle to represent rare tissue features and fail to generalize effectively across sites \cite{Schlemper2017, MoDl, Quan2018c, Yu2018c, KikiNet, Mardani2019b, Polakjointvvn2020, rgan, PatelRecon}. As such, the development of robust models remains hindered by the limited availability of diverse training datasets in solitary institutions due to economic costs and privacy concerns \cite{LiangSPM, chen2022review, wang2024review}. This limitation underscores the pressing need for collaborative approaches to build models on multi-site MRI datasets while preserving patient privacy \cite{kaissis2020secure, heckel_deep_2024}.

Federated learning (FL) is an emergent paradigm for collaborative model training in privacy-sensitive domains such as medical imaging \cite{WenqiLi2019, Rieke2020}. By exchanging model weights as opposed to raw imaging data, FL enables knowledge transfer among sites while alleviating potential privacy risks \cite{Li2020}. Recognizing the transformative potential of FL, several recent studies on MRI reconstruction have proposed FL methods for building models on multi-site datasets \cite{guo2021,elmas2022federated,feng2023tmi,wang2024fed,levac2023bio,yan2024jbhi}. A common theme among these studies has been their adherence to the conventional FL framework based on a server-client topology \cite{Li2020}. In this framework, a globally shared model is selected via agreement across sites to serve as the foundation for both cross-site knowledge transfer and eventual execution of the reconstruction task. Over multiple communication rounds, local model copies trained on local datasets available at individual sites are aggregated on the server to update the global model \cite{FedAvg}. The global model is eventually expected to possess knowledge on the distribution of multi-site MRI datasets and thereby improve generalization over single-site models trained exclusively on local datasets \cite{guo2021}. 

The conventional FL framework promises enhanced generalization in MRI reconstruction, while imposing a strict requirement for all participating sites to adopt a homogeneous model architecture as the basis of model aggregation \cite{FedAvg}. However, imaging sites often differ substantially in terms of their computational resources and the complexity of their reconstruction tasks as influenced by variations in distribution of local datasets. These differences frequently lead sites to favor distinct model architectures, ranging from convolutional networks that can be preferred to attain high local precision under relatively low compute budgets \cite{guo2021,elmas2022federated}, transformers that can be preferred to boost sensitivity to long-range contextual features when high compute budgets are available \cite{feng2023cvpr,korkmaz2022unsupervised}, to physics-driven unrolled networks that can be preferred to boost model reliability when training sets are relatively scarce albeit moderate-to-high compute budgets are available \cite{levac2023bio,wang2024fed}. Consequently, the architectural constraints in conventional FL refrain sites from leveraging models optimized for their specific requirements. This apparent lack of architectural freedom has important practical implications including discouraging resource-constrained sites from participating in collaborations, compromising performance at sites with more advanced resources due to use of relatively simple models, and ultimately stifling innovations in MRI reconstruction models. Therefore, enabling model-agnostic FL—where sites can collaborate without adhering to a homogeneous model architecture—remains a critical open challenge \cite{mittone2023model}. 

In this study, we introduce a novel FL technique based on generative autoregressive transformers (FedGAT), which to our knowledge is the first model-agnostic FL approach in the literature for MRI reconstruction. Unlike conventional FL methods, FedGAT supports model-heterogeneous settings by decoupling the process of cross-site knowledge transfer on data distributions, from the process of building reconstruction models (Fig. \ref{fig:FedGAT_gen}). It mediates knowledge transfer via a global generative prior designed to synthesize multi-site MR images. To avoid the limitations of existing adversarial priors (i.e., training instability and poor quality) and diffusion priors (i.e., suboptimal convergence and heavy compute burden), here we propose a novel GAT prior that autoregressively synthesizes MR images by predicting feature maps across spatial scales, guided by a site prompt to preserve site-specific characteristics. Afterwards, site-specific reconstruction models are locally trained on a hybrid dataset, combining local MRI datasets available at each site with synthetic datasets from remaining sites generated by the GAT prior. Comprehensive demonstrations on multi-institutional MRI datasets indicate that FedGAT functions effectively in model-heterogeneous settings, overcoming a significant challenge in enabling privacy-preserving collaborations across diverse institutions. Code for FedGAT is available at {\small{\url{https://github.com/icon-lab/FedGAT}}\normalsize}.

\vspace{0.3cm}
\subsubsection*{\textbf{Contributions}} 
\begin{itemize} 
\item To the best of our knowledge, FedGAT is the first model-agnostic federated learning (FL) method for MRI reconstruction, enabling flexible collaborations among sites with heterogeneous model preferences. 
\item To support model heterogeneity, FedGAT decouples decentralized training of a global generative prior on multi-site MR images for cross-site knowledge transfer, from local training of site-specific reconstruction models on a combination of local and synthetic MRI datasets.
\item To improve synthesis fidelity, FedGAT employs a novel prior based on generative autoregressive transformers, formulating image generation as an autoregressive prediction task across spatial scales and enforcing control over the data distribution via a site-prompting mechanism. 
\item To strike a balance between generalization and site-specific performance, FedGAT leverages a training procedure where reconstruction models pre-trained on local datasets are fine-tuned on a hybrid dataset including synthetic datasets from other sites. 
\end{itemize}

\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{figures/figure1.png}
\captionsetup{justification=justified, width=\textwidth}
\caption{FedGAT devises a two-tier strategy to collaboratively train heterogeneous models for MRI reconstruction. \textbf{(a)} The first tier conducts decentralized training of a global prior \(\text{GAT}_{\theta_{\text{GAT}}}\) that captures the distribution of multi-site MR images. The global prior is built on a novel generative autoregressive transformer that formulates image generation as an autoregressive process across growing spatial scales, and that employs a site prompt $\texttt{sp}$ for controllable image synthesis from desired sites. \textbf{(b)} The second tier conducts local training of site-specific reconstruction models $H^k_{{\phi}^k}$ ($k$: site index). To achieve an optimal balance between generalization and site-specific performance, training at each site is performed on a hybrid dataset comprising the local dataset and synthetic datasets from other sites generated via the GAT prior.}
\label{fig:FedGAT_gen}
\end{figure*}



\section{Related Work}

\subsection{Learning-Based MRI Reconstruction}
Learning-based models promise improved performance and reliability over traditional methods in MRI reconstruction \cite{Hammernik2017}. Yet, realizing this promise requires copious amounts of training data including diverse samples, since learning-based models poorly represent rare features and pathologies in their training sets \cite{data_diff}. Unfortunately, curating broad training sets is unfeasible in many applications given the high costs of scanning and subject recruitment \cite{GuoTMI2021}. These costs bear the common practice of model training on limited datasets available locally in solitary imaging sites, and such single-site models can inevitably suffer from suboptimal generalization \cite{dalmaz2024one}. Numerous strategies have been devised over the years to alleviate the unwanted influences of dataset scarcity, including transfer learning \cite{Dar2017}, unpaired learning \cite{Quan2018c, oh2020}, semi-supervised learning \cite{yurt2022semi}, and self-supervised learning \cite{yaman2020, FengLiu2021}. However, these canonical strategies rely on centralized model training following aggregation of datasets in a central repository, which incurs significant privacy concerns and maintenance costs \cite{kaissis2020secure}.

\subsection{Federated Learning for MRI Reconstruction}
Federated learning (FL) has emerged as a privacy-preserving collaboration framework that distributes the costs of model training across healthcare institutions \cite{kaissis2020secure}. Abandoning explicit sharing of sensitive imaging data, FL promotes cross-site knowledge transfer via exchange of model weights instead. In conventional FL, this exchange is attained by first training copies of a global model on the local datasets available at individual sites, and then aggregating the local copies into the shared global model on a server \cite{WenqiLi2019}. Several recent MRI studies have successfully adopted this conventional FL framework to boost generalization performance of reconstruction models \cite{guo2021,elmas2022federated,feng2023tmi}. Further improvements in site-specific performance have also been sought by employing personalized FL strategies to cope with differences in the distribution of MRI datasets across sites, such as partial model aggregation \cite{feng2023tmi}, test-time adaptation \cite{elmas2022federated}, and feature map normalization \cite{dalmaz2024one}. 

Despite their apparent benefits, previous FL methods for MRI reconstruction constrain all participating sites to adopt a homogeneous model architecture, which serves as the foundation for aggregating model weights \cite{FedAvg}. Note, however, that imaging sites can have substantial differences in computational resources or in difficulty of reconstruction tasks dependent on the interaction between distribution of local MRI datasets and desired acceleration rates. In turn, these differences can drive individual sites to prefer distinct models for MRI reconstruction, as evident from the literature where model preferences range broadly from convolutional \cite{guo2021,GuoTMI2021,feng2023tmi} and transformer \cite{korkmaz2022unsupervised,MTrans,feng2023cvpr} backbones to physics-driven unrolled architectures \cite{MoDl,levac2023bio,wang2024fed,mambaroll}. As such, the rigid model-homogeneity requirement of conventional FL severely limits the flexibility of individual sites, forcing them to forgo architectures tailored to their specific needs. During multi-institutional collaborations, this restriction can hinder participation of sites with limited resources or compromise performance in sites with copious resources.

To enable collaborative training of heterogeneous models across sites, here we introduce FedGAT as the first model-agnostic FL method for MRI reconstruction. Unlike conventional FL that transfers knowledge by communicating weights of the reconstruction model, FedGAT leverages a unique approach that decouples the process of knowledge transfer across sites from the process of building site-specific reconstruction models, and that mediates knowledge transfer via a global generative prior for multi-site MR images. Note that previous FL methods in machine learning that build generative priors have commonly proposed adversarial priors that can suffer from poor training stability and image quality \cite{FedGAN,elmas2022federated}, and diffusion priors that can suffer from suboptimal convergence that can lead to residual image noise and prolonged run times \cite{FedDDA}. In contrast, here we introduce a novel prior based on generative autoregressive transformers that efficiently synthesizes MR images via autoregressive prediction of feature maps across growing spatial scales, under guidance from a site prompt to preserve site-specific characteristics in MR images. At each site, a site-specific reconstruction model is then locally trained on a hybrid dataset comprising both the local dataset and synthetic datasets from other sites generated via the global prior. These technical advances enable FedGAT to operate seamlessly in model-heterogeneous settings, overcoming a critical barrier towards privacy-preserving collaborations across diverse institutions. 





\section{Theory}

\subsection{Conventional FL for MRI Reconstruction}
In MRI, the image domain that depicts the spatial distribution of tissues and the measurement domain where k-space data are acquired are linked through an imaging operator $\mathcal{A}=\mathcal{M}\mathcal{F}\mathcal{C}$ ($\mathcal{M}$: sampling pattern, $\mathcal{F}$: Fourier transform, $\mathcal{C}$: coil sensitivities):
\begin{equation}
\label{eq:AXY}
\mathcal{A} \mathbf{x} = \mathbf{y},
\end{equation}
where $\mathbf{x}$ is the MR image and $\mathbf{y}$ are acquired k-space data. For undersampled acquisitions, adequate solution of Eq.~\ref{eq:AXY} requires incorporation of a regularizing constraint $\mathcal{R}(\cdot)$ \cite{Lustig2007}:
\begin{equation}
\label{eq:regularized_recon}
\widehat{\mathbf{x}} = \underset{\mathbf{x}}{\operatorname{argmin}} \| \mathbf{y} - \mathcal{A} \mathbf{x} \|_{2}^{2} + \mathcal{R}(\mathbf{x}).
\end{equation}
Over the recent years, learning-based methods have gained prominence that operationalize the solution of Eq. \ref{eq:regularized_recon} as projection through a network model, typically operating in image domain as $\widehat{\mathbf{x}}=H_{\phi}(\mathcal{F}^{-1}\mathbf{y},\mathcal{A})$, which receives a zero-filled Fourier reconstruction of undersampled data as input along with the imaging operator \cite{Hammernik2017}.

In the conventional FL framework, an MRI reconstruction model is collaboratively trained over $N_c$ rounds of communication between a server and participating sites \cite{WenqiLi2019,Rieke2020}. The server maintains a global reconstruction model $H_{\phi}$ with weights $\phi$, and sites retain local copies of this common architecture $\{H_{\phi^k}\}_{k=1}^K$ where $k$ is site index, $K$ is the number of sites. At the start of the $c$th communication round, all sites initialize their local copies with the weights of the global model broadcast by the server, $\phi^k \gets \phi(c-1)$. At the $k$th site, the initialized local copy $H_{\phi^k}$ is then trained on the locally available dataset $\mathcal{D}_{\text{loc}}^k$ to minimize a reconstruction loss:
\begin{equation}
\label{eq:fed_dnn_training}
\mathcal{L}^{k}_{\text{rec}}(\mathcal{D}_{\text{loc}}^k) = \mathbb{E}_{(\mathbf{x}^{k}, \mathbf{y}^{k}) \sim \mathcal{D}_{\text{loc}}^k} \left[ \| \mathbf{x}^{k} - H_{\phi^k}(\mathcal{F}^{-1}\mathbf{y}^{k},\mathcal{A}^{k}) \|_2^2 \right],
\end{equation}
where $\mathbb{E}$ denotes expectation over $\mathcal{D}_{\text{loc}}^k$, $\mathbf{x}^{k}$ is the reference image derived from a fully-sampled acquisition, $\mathbf{y}^{k}$ is the corresponding undersampled acquisition, and $\mathcal{A}^k$ is the imaging operator. At the end of the given communication round, locally-trained model copies are aggregated on the server via weight averaging to derive an updated global model \cite{FedAvg}:
\begin{equation}
    \phi(c) = \sum_{k=1}^{K} \alpha_k \phi^k,
\end{equation}
where $\alpha_k = \frac{N^k}{N^1 + ... + N^K}$ denotes the relative contribution of the model copy from site $k$ ($N^k$: the number of training samples at site $k$). 

After a total of $N_c$ communication rounds, the resultant global model $H_{\phi^{*}}$ with $\phi^* := \phi(N_c)$ is employed at each site to reconstruct MR images. As such, reconstruction for a test subject at site $k$ is formulated as:
\begin{equation}
\widehat{\mathbf{x}}^{k} = H_{\phi^*}(\mathcal{F}^{-1}\mathbf{y}^{k},\mathcal{A}^{k}),
\end{equation}
where $\widehat{\mathbf{x}}^{k}$ is the reconstructed image, and $\mathbf{y}^{k}$ are the respective undersampled k-space data. 



\subsection{Federated Generative Autoregressive Transformers}
FedGAT is a model-agnostic FL technique that enables collaborative training of heterogeneous model architectures across sites by leveraging a decoupled two-tier strategy. During the first tier, using a server-client topology, a site-prompted global prior based on generative autoregressive transformers (GAT) is decentrally trained so as to capture the distribution of multi-site MR images (Fig. \ref{fig:FedGAT_gen}\textbf{a}). The FL server sends the trained global prior to individual sites at the end of the first tier. During the second tier, site-specific reconstruction models are locally trained with the aid of this global GAT prior (Fig. \ref{fig:FedGAT_gen}\textbf{b}). At each site, a reconstruction model of preferred architecture is pre-trained on the local dataset, and later fine-tuned on a hybrid dataset containing both the local dataset and synthetic datasets from other sites generated via the prior. The architecture of the site-prompted GAT prior, and image synthesis and two-tier training procedures for FedGAT are detailed in subsections below.

\vspace{0.25cm}
\subsubsection{\underline{Architecture of the Site-Prompted GAT Prior}}
\label{sec:architecture}
FedGAT uses a global generative prior to capture the distribution of multi-site MR images, such that synthetic MR images from individual sites can later be generated (Fig. \ref{fig:GAT_arch}). Given the recent success of autoregressive models in computer vision tasks on natural images \cite{AutoRSurvey} and the unparalleled contextual sensitivity of transformers in MR image formation tasks \cite{MTrans,dalmaz2021resvit}, here we propose a novel GAT prior equipped with transformers to autoregressively generate multi-site MR images. Unlike traditional autoregressive models devised to predict the next image token given earlier tokens (i.e., image pixels or small image patches) \cite{pixelcnnrecon}, the autoregressive process in GAT predicts feature maps at higher spatial scales given maps at lower scales, as inspired by a recent study on natural images \cite{VAR}. Yet, differently from \cite{VAR}, GAT is designed for generating multi-site MRI data by introducing a novel site-prompting mechanism that controls the sites from which MR images are synthesized. 

Since transformer architectures process images as a sequence of tokens under quadratic complexity with respect to sequence length, using transformers on pixel-level tokens is computationally challenging, whereas using them on larger patch-level tokens can degrade spatial precision \cite{vaswani2021scaling}. To mitigate computational burden without compromising spatial precision, here we leverage autoregressive transformers to instead synthesize feature maps in a compact latent space, and map between the image domain and the latent space via a variational autoencoder \cite{VQVAE}, drawing inspiration from recent efforts in efficient generative modeling \cite{LDM}. Thus, the GAT prior uses a compound architecture with VAE encoder-decoder and autoregressive transformer modules. 

\textbf{\textit{VAE Encoder Module:}} Receiving a coil-combined complex MR image $\mathbf{x} \in \mathbb{R}^{H \times W \times 2}$ with real and imaginary components stored as separate channels in the third tensor dimension, the VAE encoder aims to map its input onto discrete token maps $\mathbf{f}_1, \mathbf{f}_2, \dots, \mathbf{f}_S$ across $S$ spatial scales. To do this, the encoder first extracts a continuous latent representation $\mathbf{z} \in \mathbb{R}^{h_S \times w_S \times c}$ ($h_S$, $w_S$ denoting spatial dimensions at the $S$th scale--the highest scale) via a set of residual convolutional blocks: $\mathbf{z} = \mathrm{ResConv}(\mathbf{x})$ \cite{resnet}. Afterwards, the discrete token maps at the $s$th scale are extracted by spatial downsampling to attain dimensions of $h_s \times w_s$, followed by a quantization procedure based on a learnable codebook $\mathbf{B} \in \mathbb{R}^{V \times c}$ with vocabulary size $V$. Note that $\mathbf{B}$ characterizes a discrete latent space comprising $V$ categories.

To minimize redundancy and information losses across scales, here we adopt a hierarchical procedure for token map extraction that progresses from the lowest ($s$$\,=\,$$1$) to the highest ($s$$\,=\,$$S$) spatial scale \cite{VQGAN}. For this purpose, a residual continuous representation $\mathbf{r}_{s} \in \mathbb{R}^{h_S \times w_S \times c}$ is maintained, initialized as $\mathbf{r}_{1}=\mathbf{z}$ at $s$$\,=\,$$1$. At the $s$th scale, the residual continuous representation is used to derive $\mathbf{f}_s$ as follows:
\begin{equation}
\mathbf{f}_s = \underset{v \in \{1, \dots, V\}}{\mathrm{argmin}} \, \lVert \mathbf{B}(v\,,\,:) - \text{Down}_s(\mathbf{r}_{s}) \rVert^2_2,
\label{eq:quant}
\end{equation}
where $\mathbf{f}_s \in [V]^{h_s \times w_s}$, $\text{Down}_s$ denotes spatial downsampling to the $s$th scale via interpolation, and quantization is attained by identifying the closest vector in the codebook according to Euclidean distance. Following \cite{VQVAE}, the codebook vectors in $\mathbf{B}$ are randomly initialized from a uniform distribution during training, which has been shown to yield a uniform prior on the discrete token maps $\mathbf{f}_s$ when accompanied by the quantization objective in Eq. \ref{eq:quant}. Afterwards, codebook vectors corresponding to $\mathbf{f}_s$ are retrieved and upsampled via interpolation to the $S$th scale, and used to update the residual representation:
\begin{equation}
\mathbf{r}_{s+1} = \mathbf{r}_{s} - \text{Conv}(\text{Up}_S(\text{Lookup}(\mathbf{B},\mathbf{f}_s))),
\end{equation}
where $\text{Lookup}$ is the retrieval function for codebook vectors given discrete token maps, $\text{Up}_S$ denotes spatial upsampling to the $S$th scale via interpolation, and $\text{Conv}$ denotes a convolutional layer. Note that, at the encoder output, the discrete token maps derived at individual spatial scales are pooled into an aggregate multi-scale token map:
\begin{equation}
\{\mathbf{f}_1, \mathbf{f}_2, \dots, \mathbf{f}_S\} = \mathrm{Enc}(\mathbf{x}),
\end{equation}
which contains a set of foundational features of MR images. 

\begin{figure*}[!t]
       \vspace{-0.25cm}
        \centering
        \captionsetup{justification=justified, singlelinecheck=false}
        \includegraphics[width=0.95\textwidth]{figures/figure2.png}
        \caption{Architecture of the proposed site-prompted GAT prior. \textbf{(a)} The GAT prior embodies a variational autoencoder (VAE), whose encoder module maps an input MR image onto a set of discrete token maps $\mathbf{f}_1, \mathbf{f}_2, \dots, \mathbf{f}_S$ across $S$ spatial scales, and whose decoder module recovers the input MR image back from the derived token maps. \textbf{(b)} The GAT prior also embodies a transformer module that builds an autoregressive prior on the multi-scale discrete token maps captured by the VAE. This module expresses generation of multi-scale discrete token maps as an autoregressive process across spatial scales, wherein a higher-scale token map $\mathbf{f}_s$ is predicted given the collection of token maps at earlier scales $\mathbf{f}_{<s}$$\,:=\,$$\{\mathbf{f}_1,...,\mathbf{f}_{s-1}\}$. To preserve site-specific attributes in synthetic MR images, a site-token $\mathbf{st}$ derived from a site prompt is included in the autoregressive transformer module.}  
        \label{fig:GAT_arch}
\end{figure*}

\textbf{\textit{VAE Decoder Module:}} The VAE decoder aims to map discrete multi-scale token maps $\{\mathbf{f}_1, \mathbf{f}_2, \dots, \mathbf{f}_S\}$ back on the corresponding MR image $\mathbf{x}$ from which they were derived. Paralleling the extraction procedure in the VAE encoder, the decoder employs a hierarchical procedure to recover the image progressively from low-to-high spatial scales \cite{VQGAN}. In particular, a residual continuous representation is initialized as $\hat{\mathbf{r}}_{0}=0 \in \mathbb{R}^{h_S \times w_S \times c}$. At the $s$th scale, codebook vectors corresponding to the discrete token map $\mathbf{f}_s$ are retrieved and upsampled, and used to update the predictions for the residual representation as follows: 
\begin{equation}
\hat{\mathbf{r}}_{s} = \hat{\mathbf{r}}_{s-1} + \text{Conv}(\text{Up}_S(\text{Lookup}(\mathbf{B},\mathbf{f}_s))).
\end{equation}
Following $S$ scales of processing, the predicted continuous representation $\hat{\mathbf{z}} = \hat{\mathbf{r}}_S \in \mathbb{R}^{h_S \times w_S \times c}$ at the highest scale is used to recover the original image via projection through a set of residual convolutional blocks: 
\begin{eqnarray}
\label{eq:vaedec}
\hat{\mathbf{x}} = \text{ResConv}(\hat{\mathbf{r}}_S)=\mathrm{Dec}(\{\mathbf{f}_1, \mathbf{f}_2, \dots, \mathbf{f}_S\}),
\end{eqnarray}
where $\hat{\mathbf{x}} \in \mathbb{R}^{H \times W \times 2}$ denotes a prediction of the original MR image input to the VAE encoder. 


\textit{\textbf{Autoregressive Transformer Module:}} 
While the VAE encoder extracts discrete token maps residually across spatial scales, it does not explicitly examine the statistical relationships among these multi-scale maps \cite{VQVAE}. \textit{To obtain a correlate for the joint distribution of $\mathbf{f}_1,..,\mathbf{f}_S$, here we propose to build an autoregressive prior on multi-scale discrete token maps via the autoregressive transformer module.} In particular, the transformer module aims to predict the sequence of tokens $\mathbf{f}_s$ at the $s$th scale, given as input the sequence of tokens aggregated across lower scales $\mathbf{f}_{<s}$$\,:=\,$$\{\mathbf{f}_1,...,\mathbf{f}_{s-1}\} \in [V]^{T_{s-1}}$ ($T_{s-1} = \sum_{o=1}^{s-1} h_o w_o$ denoting the number of tokens at lower scales), and a learnable site-prompt $\mathbf{sp}(k) \in \mathbb{R}^{1 \times d}$ used to initialize a site token, $\mathbf{st}$$\,=\,$$\mathbf{sp}(k)$. To maintain spatial correspondence between the token maps at consecutive scales, the token maps in $\mathbf{f}_{<s}$ are individually embedded after upsampling to the dimensions of the subsequent scale:
\begin{align}
 \mathbf{e}_o = \mathrm{Lin}(\mathrm{Up}_{o+1}(\mathbf{f}_o)),\mbox{ for }o\in[1 \mbox{ } s-1],
\end{align}
where $\mathbf{e}_o \in \mathbb{R}^{h_{o+1} w_{o+1} \times d}$ and $\mathbf{e}_{<s}$$\,:=\,$$\{\mathbf{e}_1,...,\mathbf{e}_{s-1}\} \in \mathbb{R}^{(T_s-1) \times d}$. Afterwards, the site token is pooled with a learnable projection of these embedded token maps, and additively combined with learnable position and scale encodings:
\begin{equation}
\label{eq:at_first}
\mathbf{h}_0 = \Big[ \big[\mathbf{st};\,\, \mathbf{e}_{<s}\cdot \mathbf{E}_{\text{tok}}\big] + \mathbf{E}_{\text{pos}} + \mathbf{E}_{\text{sca}};\,\, \mathbf{m} \Big],
\end{equation}
where $\mathbf{h}_0 \in \mathbb{R}^{T_S\times d}$ is a hidden representation with $T_S$ denoting the number of tokens across all scales, $\mathbf{E}_{\text{tok}} \in \mathbb{R}^{(T_s-1) \times d}$ is the token projection matrix, $\mathbf{E}_{\text{pos}} \in \mathbb{R}^{T_s \times d}$ is the position encoding that captures the relative spatial locations of tokens \cite{attention}, and $\mathbf{E}_{\text{sca}} \in \mathbb{R}^{T_s \times d}$ is the scale encoding that captures the relative spatial scales of tokens across $s$ scales. In Eq. \ref{eq:at_first}, $\mathbf{m}$$\,=\,$$0^{(T_S-T_{s}) \times d}$ is a zero matrix that masks tokens from spatial scales succeeding the $(s-1)$th scale to ensure causal progression from lower to higher scales. 

To predict $\mathbf{f}_s$, the hidden representation $\mathbf{h}_0$ is projected through $L$ transformer blocks, each containing multi-head self-attention (MHSA) and multi-later perceptron (MLP) layers interleaved with normalization and residual connections \cite{vaswani2021scaling}. Yet, unlike normalization layers in conventional transformers, here we employ adaptive layer normalization (AdaLN) in order to induce site-specific processing of hidden representations \cite{adaln}. In particular, we propose to modulate the statistics of $\mathbf{h}$ via learnable functions of the site token $\mathbf{st}$:
\begin{equation}
\mathrm{AdaLN}(\mathbf{h}, \mathbf{st}) = \gamma(\mathbf{st}) \cdot \frac{\mathbf{h} - \mu(\mathbf{h})}{\sigma(\mathbf{h})} + \beta(\mathbf{st}),
\end{equation}
where $\mathbf{h}$ is the hidden representation, $\mu$ and $\sigma$ are the layer mean and standard deviation for $\mathbf{h}$, and $\beta$, $\gamma \, \in \mathbb{R}$ are learnable bias and gain parameters dependent on $\mathbf{st}$. Accordingly, the projection through the $\ell$th transformer block ($\ell \in [1 \mbox{ } L]$) can be expressed as follows:
\begin{eqnarray}
\mathbf{h}^\prime_{\ell} &=& \mathbf{h}_{\ell-1} + \mathrm{MHSA}(\mathrm{AdaLN}(\mathbf{h}_{\ell-1}, \mathbf{st})),\\
\mathbf{h}_{\ell}&=& \mathbf{h}^\prime_{\ell} + \mathrm{MLP}(\mathrm{AdaLN}(\mathbf{h}^\prime_{\ell}, \mathbf{st})).
\end{eqnarray}
In MHSA layers, attention scores between query and key tokens in $\mathbf{h}$ are obtained by computing softmax dot-product similarity \cite{attention}, and these attention scores are then used to filter the value tokens:
\begin{equation}
A_{i,j} = \text{Softmax}\left(\frac{\mathbf{q}_i \cdot \mathbf{k}_j^\top}{\sqrt{d}}\right); \quad
\mathrm{SA}(\mathbf{h}) = A \cdot \mathbf{v},
\end{equation}
where $A \in \mathbb{R}^{(1+T_S) \times (1+T_S)}$ is the attention score matrix, $i$, $j$ are token indices, $\mathbf{q}$, $\mathbf{k}$, $\mathbf{v}$ denote unit-norm query, key, value tokens obtained as learnable linear projections of tokens in $\mathbf{h}$, and $\mathrm{SA}$ denotes a single self-attention head in MHSA.  

After projection through the final transformer block, an adaptive layer norm conditioned on $\mathbf{st}$ followed by a linear layer is employed to extract the logits $\mathbf{logit} \in \mathbb{R}^{(h_s w_s) \times V}$ with improved site specificity, the probability values for these tokens $\mathbf{P}_s$ are computed from the logits, and the specific codebook indices maximizing the probability values are drawn to compute the predicted token sequence: 
\begin{eqnarray}
\mathbf{logit} &=& \mathrm{Lin}(\mathrm{AdaLN}(\mathbf{h}_{L}(T_{s-1}+1:T_{s},:),\mathbf{st}))\\
\mathbf{P}_s &=& \mathrm{Softmax}({\mathbf{logit}}), \label{eq:mp1}\\
\hat{\mathbf{f}}_s(i) &=& \underset{v \in \{1, \dots, V\}}{\mathrm{argmax}}\, \mathbf{P}_s(i,v),\label{eq:mp2}
\end{eqnarray}
where  $\hat{\mathbf{f}}_s(i)$ is the prediction for the $i$th token in $\mathbf{f}_s$, and $\mathbf{P}_s(i,v)$ is the predicted probability of the $v$th codebook vector for $\mathbf{f}_s(i)$. For brevity, we will refer to the autoregressive mapping performed by the transformer module  as follows:
\begin{equation}
\hat{\mathbf{f}}_s = \mathrm{Trans}(\mathbf{sp}(k),\,\mathbf{f}_{<s}), \mbox{for } s \in [1\mbox{ }{S}]. 
\label{eq:at_last}
\end{equation}
Note that $\mathbf{f}_{<1} = \{\mathbf{f}_0\} \in \varnothing$ does not contain any tokens.

\begin{algorithm}[t]
\small
\caption{Training of the GAT prior}\label{alg:fedGAT_training}
\KwInput {
    $\mathcal{D_{\text{loc}}}=\{\mathcal{D}_{\text{loc}}^1,...,\mathcal{D}_{\text{loc}}^K\}$: local datasets from $K$ sites. \\
    GAT: global GAT prior with parameters $\theta_{\text{GAT}}$. \\
    \{$\text{GAT}^1,...,\text{GAT}^K$\}: local copies with $\{\theta_{\text{GAT}}^1,...,\theta_{\text{GAT}}^K\}$. \\
    $N_c$: number of communication rounds.\\ 
    $N_l$: number of local epochs. \\
}
\KwOutput{Trained global GAT prior with $\hat{\theta}_{\text{GAT}}$.} 
\vspace{0.1cm}
Initialize global prior with $\theta_{\text{GAT}}(0)$.\\
\For{$c = 1:N_c$}{
    \For{$k = 1:K$}{ 
        $\theta_{\text{GAT}}^k(0) \gets \theta_{\text{GAT}}(c-1)$ {$\triangleright$ global onto local copy} \\
        \For{$e = 1:N_l$}{
            $\theta_{\text{GAT}}^k(e) \gets \theta_{\text{GAT}}^k(e-1) - \eta \nabla_{\theta_{\text{GAT}}^k}\mathcal{L}^k_{\text{GAT}} (\mathcal{D}^k_{\text{loc}}) $ \\
        }
    }
    $\theta_{\text{GAT}}(c) \gets \sum_{k=1}^{K} \theta_{\text{GAT}}^k(N_l)$ {$\triangleright$ server aggregation}\\
}
\Return ${\theta}^{*}_{\text{GAT}} := \theta_{\text{GAT}}(N_c)$
\end{algorithm}

\vspace{0.2cm}
\subsubsection{\underline{MR Image Synthesis with the GAT Prior}}
\label{sec:synthesis}
To synthesize multi-site MR images via the GAT prior, the autoregressive transformer and VAE decoder modules are utilized. The transformer module generates a random set of multi-scale discrete token maps, sequentially across growing spatial scales based on the autoregressive mapping in Eq. \ref{eq:at_last}. To introduce stochasticity during inference, however, nucleus sampling \cite{nucleussampling} is employed instead of the maximum-probability sampling procedure in Eqs. \ref{eq:mp1}-\ref{eq:mp2}. Accordingly, the logit values across $V$ codebook vectors are ordered separately for each token in $\hat{\mathbf{f}}_s$, the top-5 percentile of values are subjected to softmax to obtain a probability mass function, and the predicted token is randomly drawn from this function:
\begin{eqnarray}
\mathbf{logit}_\mathrm{sort}(i,:) &=&  \mathrm{Sort}({\mathbf{logit}}(i;\,1:V),\mathrm{'descend'}), \\
\tilde{\mathbf{P}}_s(i,:) &=& \mathrm{Softmax}(\mathbf{logit}_\mathrm{sort}(i,1:\lceil 0.05 \, V \rceil)),\\
\hat{\mathbf{f}}_s(i) &\sim& \tilde{\mathbf{P}}_s(i,\,1:\lceil 0.05 \, V \rceil), 
\end{eqnarray}
where $\tilde{\mathbf{P}}_s(i,:)$ is the estimated probability mass function for the $i$th token, and $\lceil\,\rceil$ denotes the ceiling operator. 


Once a set of discrete multi-scale token maps $\{\mathbf{f}_1, \mathbf{f}_2, \dots, \mathbf{f}_S\}$ is generated, it can be projected through the VAE decoder as described in Eq. \ref{eq:vaedec} to obtain a synthetic MR image. For brevity, here we refer to the overall mapping used to synthesize images from site $k$ as follows:
\begin{align}
 \mathbf{x}^k_{\text{syn}} &= \mathrm{GAT}(\mathbf{sp}(k)), \mbox{ such that:} \\
 \mathrm{GAT}(\cdot) &= \mathrm{Dec} \left( \left\{ (\mathbf{f}_1,\dots,\mathbf{f}_S):\, \right. \right. \nonumber \\ 
 &\qquad \quad \left. \left. \mathbf{f}_s = \mathrm{Trans}(\cdot,\mathbf{f}_{<s}) \mbox{ for } s\in[1\mbox{ }S];\right\} \right).\nonumber
\end{align}

\vspace{0.1cm}
\subsubsection{\underline{Two-Tier Training Procedure for FedGAT}} 
To support model-heterogeneous settings, FedGAT leverages a two-tier training procedure where building of the global GAT prior (Alg. \ref{alg:fedGAT_training}) is decoupled from subsequent building of site-specific reconstruction models (Alg. \ref{alg:site_specific_recon}). The procedural details and loss functions used in each tier are discussed below. 

\textit{\textbf{Training of the GAT prior:}} FedGAT decentrally trains a global, site-prompted GAT prior that captures the distribution of multi-site MR images, which is later used to generate synthetic MR images from desired sites. The proposed GAT prior embodies VAE and autoregressive transformer modules as described in Section \ref{sec:architecture}. VAE aims to encode input MR images onto a compact latent space and then to decode them back faithfully. Accordingly, a composite loss function is used for the VAE modules \cite{VQGAN}:
\begin{align}
\label{eq:vaeloss}
\mathcal{L}^{k}_{\text{vae}}(\mathcal{D}_{\text{loc}}^k) = \mathbb{E}_{\mathbf{x}^{k} \sim \mathcal{D}_{\text{loc}}^k}  \left[ \| \mathbf{x}^{k} - \hat{\mathbf{x}}^{k} \|_2^2 + \| \mathbf{z}^{k} - \hat{\mathbf{z}}^{k} \|_2^2 \right. && \nonumber \\
\left. + \lambda_p \mathrm{LPIPS}(\mathbf{x}^{k}, \hat{\mathbf{x}}^{k}) + \lambda_a \mathcal{L}_a(\hat{\mathbf{x}}^k) \right], && 
\end{align}
where $k$ is the site index, and $\mathcal{D}_{\text{loc}}^k$ is the local training dataset. In Eq. \ref{eq:vaeloss}, the first term enforces consistency between the input MR image and its prediction, the second term enforces consistency between the continuous representation in the latent space and its prediction, the third term is a perceptual loss \cite{zhang2018perceptual}, and the fourth term is an adversarial loss \cite{StyleGAN2}. Meanwhile, the autoregressive transformer aims to autoregressively predict the discrete token maps across multiple scales faithfully. Since the discrete token maps are assigned categorically to individual vectors from the VAE codebook, the transformer module uses the following cross-entropy loss: 
\begin{align}
\label{eq:transloss}
\mathcal{L}^{k}_{\text{trans}}(\mathcal{D}_{\text{loc}}^k) = - \mathbb{E}_{\mathbf{x}^k \sim \mathcal{D}_{\text{loc}}^k} \left[ \sum_{s=1}^{S} \sum_{i=1}^{h_s w_s} \sum_{v=1}^{V} \mathbb{I}(\mathbf{f}_s^k(i) = v)\right. \nonumber \\ 
\cdot \log \left( \mathbf{P}_s(i, v) \right)  \Bigg],
\end{align}
where the ground-truth discrete token maps are extracted by the VAE as $\{\mathbf{f}_1^k, \mathbf{f}_2^k, \dots, \mathbf{f}_S^k\} = \mathrm{Enc}(\mathbf{x}^k)$, $i$ denotes token index within $\mathbf{f}^{k}_{s}$, $\mathbb{I}$ is a binary indicator that signals whether $\mathbf{f}^{k}_{s}(i)$ is categorized as the $v$th vector in the VAE codebook. Finally, the overall loss for the GAT prior can be expressed as:
\begin{align}
\label{eq:gatloss}
\mathcal{L}^{k}_{\text{GAT}}(\mathcal{D}_{\text{loc}}^k) = \mathcal{L}^{k}_{\text{trans}}(\mathcal{D}_{\text{loc}}^k) + \lambda_{\text{vae}} \mathcal{L}^{k}_{\text{vae}}(\mathcal{D}_{\text{loc}}^k),
\end{align}
where $\lambda_{\text{vae}}$ denotes the relative weighting of the VAE loss. 

As the architecture of the GAT prior is common across sites, decentralized training of the global prior across $K$ sites can be performed using the local loss function in Eq. \ref{eq:gatloss} and conventional federated averaging of model weights based on a server-client topology as outlined in Alg. \ref{alg:fedGAT_training}. The FL server coordinates decentralized training over \(N_c\) communication rounds. At the start of the $c$th round, the global prior $\theta_{\text{GAT}}$ is sent to individual sites to initialize their local copies $\theta^k_{\text{GAT}}$: 
\begin{equation}
\theta^k_{\text{GAT}}(0) \gets \theta_{\text{GAT}}(c-1), \,\, \forall k \in \{1,\ldots, K\}.
\end{equation}
Each site then updates its local copy by training it on its local dataset over $N_l$ epochs:
\begin{equation}
\theta_{\text{GAT}}^k(e) \gets \theta_{\text{GAT}}^k(e-1) - \eta \nabla_{\theta_{\text{GAT}}^k} \mathcal{L}^k_{\text{GAT}}, \,\, \mbox{for } e \in [1\mbox{ }N_l],
\end{equation}
where $\eta$ denotes the learning rate. At the end of the round, local copies are forwarded to the server that aggregates them into the global prior via simple averaging assuming balanced numbers of samples across sites \cite{FedAvg}: 
\begin{equation}
\theta_{\text{GAT}}(c) \leftarrow \sum_{k=1}^{K} \theta^k_{\text{GAT}}(N_l).
\end{equation}
At the completion of the decentralized training procedure, the trained global prior is taken as ${\theta}^{*}_{\text{GAT}} := \theta_{\text{GAT}}(N_c)$.


\begin{algorithm}[t]\small
\caption{Training of reconstruction models}\label{alg:site_specific_recon}
\small
\KwInput{
    $\mathcal{D}^k_{\text{\text{loc}}}$: local dataset from site $k$.\\
    $\text{GAT}$: global GAT prior with ${\theta}^{*}_{\text{GAT}}$. \\
    $N_p$: number of pre-training epochs. \\
    $N_f$: number of fine-tuning epochs. \\
}
\KwOutput{
   $H^k_{{\phi}^k}$: reconstruction model at site $k$.
}

Initialize model with $\phi^k(0)$ at site $k$.\\
{$\triangleright$ pre-train on local dataset}\\
\For{$e = 1$ \textbf{to} $N_p$}{ 
    $\phi^k(e) \leftarrow \phi^k(e-1) - \eta \nabla_{\phi^k}\mathcal{L}^k_{\text{rec}}(\mathcal{D}^k_{\text{\text{loc}}})$ 
}
{$\triangleright$ generate synthetic datasets}\\
\For{$j = 1$ \textbf{to} $K$}{
    $\mathcal{D}^j_{\text{syn}} = \{ \mathbf{x}^j_{\text{syn}}, \mathcal{A}^k \mathbf{x}^j_{\text{syn}} \}$ where $\mathbf{x}^j_{\text{syn}} = \text{GAT}(\mathbf{sp}(j))$\\
}
{$\triangleright$ fine-tune on local \& synthetic datasets}\\
\For{$e = (1+N_p)$ \textbf{to} $(N_f+N_p)$}{
    $\phi^k(e)$$\leftarrow$$\phi^k(e$$-$$1)$$-$$\eta \nabla_{\phi^k}\mathcal{L}^k_{\text{rec}}([\mathcal{D}^k_{\text{\text{loc}}};\, \mathcal{D}_{\text{syn}}^{\{1,..,K\} \setminus \{k\}}])$ 
}
\Return{${\phi}^{*k}:={\phi}^k(N_f+N_p)$}
\end{algorithm}
\textit{\textbf{Training of reconstruction models:}}
Following the first tier, each individual site trains a site-specific reconstruction model \( H^k_{\phi^k} \) of preferred architecture, with the aid of the global GAT prior (Alg. \ref{alg:site_specific_recon}). To attain a refined balance between within-site and cross-site reconstruction performances, the reconstruction model is built by pre-training on the local dataset followed by fine-tuning on a hybrid dataset containing both local and synthetic datasets. For pre-training, we employ a conventional mean-squared error loss \cite{MoDl}:
\begin{equation}
\mathcal{L}^{k}_{\text{rec}}(\mathcal{D}_{\text{loc}}^k) = \mathbb{E}_{(\mathbf{x}^{k}, \mathbf{y}^{k}) \sim \mathcal{D}_{\text{loc}}^k} \left[ \| \mathbf{x}^{k} - H^k_{\phi^k}(\mathcal{F}^{-1}\mathbf{y}^{k},\mathcal{A}^{k}) \|_2^2 \right],
\end{equation}
where $\mathbf{x}^{k}$ is a reference image and $\mathbf{y}^{k}$ is the respective undersampled acquisition drawn from $\mathcal{D}_{\text{loc}}^k$. Pre-trained models are obtained after $N_p$ epochs:
\begin{equation}
\phi^k(e) \leftarrow \phi^k(e-1) - \eta \nabla_{\phi^k}\mathcal{L}^k_{\text{rec}}(\mathcal{D}^k_{\text{\text{loc}}}),\mbox{ for }e\in[1 \mbox{ } N_p].
\end{equation}

Next, each site generates synthetic coil-combined MR images from remaining sites via the site-prompted GAT prior to construct the hybrid dataset for fine-tuning. Note that training of reconstruction models does not only require coil-combined MR images but also respective multi-coil undersampled k-space data. To generate synthetic multi-coil k-space data, here we randomly couple synthetic coil-combined MR images with actual imaging operators derived from samples in the local dataset $\mathcal{D}^k_{\text{loc}}$ \cite{Zhu2018,Dar2017}. In particular, for each sample in $\mathcal{D}_{\text{loc}}^k$, $\mathcal{A}^k=\mathcal{M}^k\mathcal{F}\mathcal{C}^k$ is constructed by conjoining coil sensitivity estimates derived from the fully-sampled acquisition ($\mathcal{C}^k$) with a random k-space sampling pattern ($\mathcal{M}^k$) and Fourier transformation ($\mathcal{F}$). Thus, synthetic MR images and multi-coil k-space data at site $k$ are derived as:
\begin{align}
  &\mathcal{D}^j_{\text{syn}} = \{ \mathbf{x}^j_{\text{syn}},\mathbf{y}^j_{\text{syn}}\} \quad \forall j \in \{1,..,K\} \setminus \{k\}; \\
  &\mbox{ such that: } \mathbf{x}^j_{\text{syn}} = \mathrm{GAT}(\mathbf{sp}(j)),\\
  &\qquad \qquad \,\,\,\, \mathbf{y}^j_{\text{syn}} = \mathcal{A}^k \mathbf{x}^j_{\text{syn}}.
\end{align}

The local MRI dataset and synthetic MRI datasets from remaining sites are then mixed in equal proportions of samples per site in order to construct a hybrid training set:
\begin{equation}
\mathcal{D}^k_{\text{\text{hyb}}} = [\mathcal{D}^k_{\text{\text{loc}}};\, \mathcal{D}_{\text{syn}}^{\{1,..,K\} \setminus \{k\}}].
\end{equation}
Afterwards, the pre-trained reconstruction model is fine-tuned via mean-squared error loss:
\begin{equation}
\mathcal{L}^{k}_{\text{rec}}(\mathcal{D}^k_{\text{\text{hyb}}}) = \mathbb{E}_{(\mathbf{x}, \mathbf{y}) \sim \mathcal{D}^k_{\text{hyb}}} \left[ \| \mathbf{x} - H^k_{\phi^k}(\mathcal{F}^{-1}\mathbf{y},\mathcal{A}^{k}) \|_2^2 \right],
\end{equation}
where $\mathbf{x}$ is a reference image and $\mathbf{y}$ is the respective undersampled acquisition drawn from $\mathcal{D}_{\text{hyb}}^k$. Accordingly, fine-tuned models are obtained after $N_f$ epochs:
\begin{equation}
\phi^k(e) \leftarrow \phi^k(e-1) - \eta \nabla_{\phi^k}\mathcal{L}^k_{\text{rec}}(\mathcal{D}^k_{\text{\text{hyb}}}),\mbox{ for }e\in[1+N_p \mbox{ } N_f+N_p].
\end{equation}
The model at the end of fine-tuning is taken as the final site-specific reconstruction model $\phi^{*k}:=\phi^{k}(N_f+N_p)$.





\section{Methods}
\subsection{Implementation Details for FedGAT}
The proposed GAT prior embodies a VAE encoder to map MR images onto multi-scale discrete token maps \cite{VQVAE}, a transformer module to autoregressively generate discrete token maps across spatial scales by starting from a random embedding at the lowest scale \cite{multi-scaletransformer}, and a VAE decoder to recover images from discrete token maps at the highest scale. Here, GAT was implemented with $S$$\,=\,$10 scales of spatial dimensions taken as $h_s$$\,=\,$$w_s$$\,=\,$$p$, where $p$ varied from lowest to highest scale as $\{1,2,3,4,5,6,8,10,13,16\}$. The VAE encoder and decoder each had 6 stages. Across stages, the encoder used a total of 12 residual convolutional blocks \cite{resnet}, and performed spatial downsampling of the input MR image onto the continuous latent representation via a set of stride-2 convolutions. The decoder used a total of 12 residual convolutional blocks, and performed spatial upsampling of the continuous latent representation via transpose convolutions. A channel dimensionality of $c$$\,=\,$32, and a codebook size of $V$$\,=\,$4096 was prescribed. The autoregressive transformer module employed $L$$\,=\,$16 sequential blocks with embedding dimensionality of $d$$\,=\,$1024 and 16 self-attention heads. 

For the first tier of the training procedure, $N_c$$\,=\,$500 communication rounds and $N_l$$\,=\,$1 local epochs per round were used. To facilitate federated learning of the GAT prior, here the number of trained and thereby communicated model parameters were reduced by using frozen VAE modules with weights initialized from the `vae\textunderscore ch160v4096z32' architecture (\footnote{\url{https://huggingface.co/FoundationVision/var/resolve/main/vae_ch160v4096z32.pth}}pre-trained on natural images with \(\lambda_p = 1\), \(\lambda_a = 0.4\)), and setting $\lambda_{vae}$$\,=\,$0. Meanwhile, the weights of the transformer module were randomly initialized. For the second tier, $N_p$$\,=\,$100 pre-training epochs and $N_f$$\,=\,$100 fine-tuning epochs were used.

\subsection{Competing Methods}
FedGAT was comparatively demonstrated against non-FL benchmarks (Central, Single) along with distillation-based (FedMD, FedDF) and generative (FedGIMP, FedDDA) FL baselines. 

    {\textbf{Central}}: A non-FL benchmark for across-site reconstruction performance was considered that pools MRI datasets across sites to centrally train a global reconstruction model \cite{kaissis2020secure}. Since Central requires selection of a homogeneous architecture across sites, in model-heterogeneous settings, separate global reconstruction models were trained based on $K$ distinct architectures preferred by different sites.
    
    {\textbf{Single}}: A non-FL benchmark for within-site reconstruction performance was considered that trains site-specific reconstruction models on local data \cite{guo2021}. Since reconstruction models are trained independently across sites, Single is natively compatible with model-heterogeneous settings. 

    {\textbf{FedDF}}: A distillation-based FL baseline was considered that supports model-heterogeneous settings via an auxiliary public dataset \cite{feddf}. To adopt FedDF for MRI reconstruction, distillation between site-specific reconstruction models and a global model were performed based on predictions of reconstructed images, instead of class logits as in the original implementation for classification tasks \cite{feddf}. At individual sites, site-specific reconstruction models were initialized via distillation from the global model (taken as MoDL-5), and then trained along with the copy of the global model using a proximal loss term \cite{fedprox}. Training was performed on a 840-sample mixture of the local data with independent auxiliary data downloaded from a public repository \cite{calgary}. On the server, copies of the global model locally trained at individual sites were aggregated via distillation on a 840-sample set of auxiliary data. 
            
    {\textbf{FedMD}}: A distillation-based FL baseline was considered that supports model-heterogeneous settings via an auxiliary public dataset \cite{fedmd}. To adopt FedMD for MRI reconstruction, predictions of reconstructed images were used for knowledge transfer instead of class logits in the original implementation for classification tasks \cite{fedmd}. Accordingly, site-specific reconstruction models were locally trained on a 840-sample mixture of local and auxiliary data as in FedDF \cite{calgary}, images reconstructed by site-specific models on the auxiliary dataset were aggregated on the server \cite{FedAvg}, and the aggregated reconstructions on the auxiliary dataset were used to initialize site-specific models via distillation.
    
    {\textbf{FedGIMP}}: A generative FL baseline that decentrally trains a global prior based on an unconditional adversarial model to synthesize multi-site MR images \cite{elmas2022federated}. Note that FedGIMP was originally devised for model-homogeneous settings. To adopt FedGIMP for model-heterogeneous settings, it was implemented using the same two-tier training strategy as in FedGAT, where site-specific reconstruction models were trained using a hybrid dataset containing both local MRI data and synthetic data from remaining sites generated by FedGIMP.

    {\textbf{FedDDA}}: A generative FL baseline that decentrally trains a global prior based on an unconditional diffusion model to synthesize images \cite{FedDDA}. A site index was included in FedDDA for controllable generation of multi-site MR images \cite{dalmaz2024one}. FedDDA was imeplemented using the same two-tier training strategy as in FedGAT to support model-heterogeneous settings, where site-specific reconstruction models were trained using a hybrid dataset containing both local MRI data and synthetic data from remaining sites generated by FedDDA.


\begin{table}[t]
\centering
\caption{Reconstruction performance for single-coil datasets. Each site employs an entirely distinct model type, with Site $k$ (dataset-type) denoting the site index, the local dataset, and the type of reconstruction model. Metrics are listed as mean$\pm$std across the test set, for within-site (upper panel) and across-site (lower panel) reconstructions at R=4x-8x. Boldface indicates the top-performing FL method in each task. Benchmark methods for within-site reconstruction (Single), and for cross-site reconstruction (Central) are underlined. }
 %\textcolor{blue}{Method (PSNR, SSIM difference): \\
 %\textbf{W:} Single:  0.58,0.16; Central: 1.02,0.37 ; Distillation-based: 2.29, 5.90; Generative-based: 1.07,0.79. \\
 %\textbf{A:} Single:  1.52,6.49; Central: 0.17,-1.85; Distillation-based: 1.25, 4.93; Generative-based: 1.09,4.64.}
\resizebox{1.025\columnwidth}{!}
{
\begin{tabular}{| Sc | l l | Sc | Sc | Sc | Sc | Sc | Sc |}
 \hline
\multicolumn{3}{|Sc|}{\textbf{Within-site}} & \multicolumn{2}{Sc|}{Site 1 (BraTS-MoDL)} & \multicolumn{2}{Sc|}{Site 2 (fastMRI-rGAN)}& \multicolumn{2}{Sc|}{Site 3 (MIDAS-D5C5)}\\
 \cline{4-9}
\multicolumn{3}{|Sc|}{\textbf{reconstruction}} &PSNR $\uparrow$   & SSIM $\uparrow$    & PSNR $\uparrow$   & SSIM $\uparrow$    & PSNR $\uparrow$  & SSIM $\uparrow$   \\
 \hline
\multirow{4}{*}{\rotatebox[origin=c]{90}{\textit{non-FL}}}& \multirow{2}{*}{\underline{Single}}   & 
    4x &{48.06$\pm$3.33} & {99.72$\pm$0.11} & 35.83$\pm$2.51 & 90.64$\pm$3.87 & {29.04$\pm$1.84} &  {76.77$\pm$4.08 }\\
& & 8x & {42.46$\pm$3.15} & {99.03$\pm$0.45} & 33.18$\pm$2.26  &  86.58$\pm$4.64
&{27.79$\pm$2.05 }&{74.20$\pm$3.97}\\
 \cline{2-9}
 & \multirow{2}{*}{Central}   & 
4x &46.12$\pm$2.55&99.46$\pm$0.20& 36.03$\pm$2.51 & 91.23$\pm$3.63&28.89$\pm$1.89 &75.11$\pm$4.07 \\
& & 8x  &42.16$\pm$2.72&98.65$\pm$0.58& 33.18$\pm$2.53&87.40$\pm$4.44& 27.34$\pm$2.17 & 73.87$\pm$3.98\\
 \cline{1-9}
\multirow{10}{*}{\rotatebox[origin=c]{90}{\textit{FL methods}}}& \multirow{2}{*}{FedDF}   & 
    4x & 44.42$\pm$3.76 & 99.34$\pm$0.28 & 32.88$\pm$3.22 & 87.95$\pm$4.41 & 27.62$\pm$1.90  & 61.23$\pm$4.32  \\
& & 8x & 40.06$\pm$3.23 & 98.23$\pm$0.77 & 30.78$\pm$2.73 & 83.45$\pm$5.51 & 27.48$\pm$1.93 & 59.62$\pm$4.93 \\
 \cline{2-9}
 & \multirow{2}{*}
  {FedMD}   &  4x & 44.68$\pm$3.72 & 99.40$\pm$0.24 &34.24$\pm$3.06 &88.48$\pm$4.13 & 28.95$\pm$1.92 &   62.22$\pm$5.25\\
& & 8x & 40.32$\pm$3.22 & 98.29$\pm$0.75&  32.96$\pm$2.56 & {86.40$\pm$4.66}&  {27.79$\pm$2.07} & 60.19$\pm$5.07 \\
 \cline{2-9}
& \multirow{2}{*}{FedGIMP} & 

 4x & 47.61$\pm$2.86 &  99.63$\pm$0.15 & 35.45$\pm$2.48 & 90.27$\pm$3.99 & 28.68$\pm$1.90& 74.91$\pm$4.42 \\
& & 8x  & \textbf{42.14$\pm$2.71} & 98.62$\pm$0.54 & 32.74$\pm$2.40 &  85.86$\pm$4.75 & 26.82$\pm$2.17 & 73.96$\pm$4.08
\\
 \cline{2-9}
 & \multirow{2}{*}{FedDDA} & 
 4x & 47.50$\pm$2.81 & 99.61$\pm$0.15 &  35.67$\pm$2.51 &  90.30$\pm$3.94 & 28.61$\pm$1.79&75.67$\pm$4.15 \\
& & 8x  &  41.76$\pm$2.90 &  98.47$\pm$0.63 & 32.79$\pm$2.45 &  86.02$\pm$4.84 & 27.10$\pm$2.18& 73.08$\pm$4.07\\
 \cline{2-9}
& \multirow{2}{*}{FedGAT}   & 
    4x & \textbf{47.63$\pm$2.65}&\textbf{99.64$\pm$0.14}  & \textbf{35.89$\pm$2.50} &\textbf{90.69$\pm$3.86} & \textbf{31.88$\pm$2.03} &\textbf{77.48$\pm$4.94} \\
& & 8x & 41.60$\pm$2.85&\textbf{98.78$\pm$0.54} & \textbf{33.23$\pm$2.36} &\textbf{86.45$\pm$0.67} & \textbf{29.62$\pm$2.11}&\textbf{74.88$\pm$4.29}\\
 \hline
  \multicolumn{9}{Sc}{\mbox{ }} \\[-1.75ex]
 \hline
 \multicolumn{3}{|Sc|}{\textbf{Across-site}} & \multicolumn{2}{Sc|}{Site 1 (BraTS-MoDL)} & \multicolumn{2}{Sc|}{Site 2 (fastMRI-rGAN)}& \multicolumn{2}{Sc|}{Site 3 (MIDAS-D5C5)}\\
 \cline{4-9}
\multicolumn{3}{|Sc|}{\textbf{reconstruction}} &PSNR $\uparrow$   & SSIM $\uparrow$    & PSNR $\uparrow$   & SSIM $\uparrow$    & PSNR $\uparrow$  & SSIM $\uparrow$   \\
 \hline
\multirow{4}{*}{\rotatebox[origin=c]{90}{\textit{non-FL}}}& \multirow{2}{*}{Single}     & 4x & 34.45$\pm$3.57 &  91.44$\pm$4.05  & 33.02$\pm$2.30 & 66.49$\pm$7.26 & 29.67$\pm$2.28 & 75.67$\pm$6.93 \\
& & 8x & 30.48$\pm$3.20 & 85.48$\pm$5.37 &30.08$\pm$2.20
& 59.14$\pm$6.78 &28.38$\pm$2.26 &73.64$\pm$6.55
 \\
 \cline{2-9}
 & \multirow{2}{*}{\underline{Central}}     
 &  4x & 36.48$\pm$2.89 & 94.78$\pm$1.91 & 33.53$\pm$2.31 & 82.93$\pm$4.07 & 30.68$\pm$2.47 & 77.41$\pm$6.99\\
& & 8x  & 32.79$\pm$2.81 & 92.16$\pm$2.62 & 30.78$\pm$2.25 & 78.75$\pm$4.67 & 29.93$\pm$2.20 & 75.86$\pm$6.95\\
 \cline{1-9}
\multirow{10}{*}{\rotatebox[origin=c]{90}{\textit{FL methods}}}& \multirow{2}{*}{FedDF}   & 
    4x & 34.40$\pm$3.12& 93.12$\pm$3.00 & 33.14$\pm$2.61 & 71.59$\pm$6.74 &29.23$\pm$2.38 & 72.76$\pm$7.12  \\
& & 8x & 30.91$\pm$2.87 & 90.33$\pm$3.36 & 30.89$\pm$2.22 & 64.17$\pm$7.14 & 28.37$\pm$2.63 &72.45$\pm$8.43 \\
 \cline{2-9}
 & \multirow{2}{*}{FedMD}   & 
    4x & 34.61$\pm$3.13 & 93.42$\pm$2.75 & 33.11$\pm$2.26 & 68.04$\pm$6.84 &30.03$\pm$2.41 & 72.31$\pm$7.93\\
& & 8x & 31.12$\pm$2.83& 90.63$\pm$3.13
 & 30.47$\pm$2.20 & 61.79$\pm$7.06 & 29.05$\pm$2.57&  71.80$\pm$7.84 \\
 \cline{2-9}
 \cline{2-9}
 & \multirow{2}{*}{FedGIMP} & 
4x & 36.42$\pm$3.17 & 94.32$\pm$2.49 &  33.05$\pm$2.29 &  67.50$\pm$7.01 & 30.30$\pm$2.33 & 76.19$\pm$6.56\\
& & 8x  & 31.88$\pm$2.87 & 90.16$\pm$3.78 &  29.78$\pm$2.16 & 57.80$\pm$6.39 & 28.31$\pm$2.16 &  71.75$\pm$6.10\\
 \cline{2-9}
 & \multirow{2}{*}{FedDDA} & 
 4x & 36.19$\pm$3.13 & 94.41$\pm$2.24 & 33.11$\pm$2.21 & 67.78$\pm$7.00 & 28.59$\pm$2.07 &78.13$\pm$5.12\\
& & 8x  & 31.79$\pm$2.86 & 90.75$\pm$3.07 & 30.15$\pm$2.16 &  59.18$\pm$6.93 &  27.73$\pm$2.13 &  77.92$\pm$5.11\\
 \cline{2-9}
{} & \multirow{2}{*}{FedGAT}   & 
    4x & \textbf{36.71$\pm$3.01} & \textbf{95.02$\pm$1.96} & \textbf{34.50$\pm$2.36} & \textbf{75.69$\pm$5.76} &\textbf{30.77$\pm$2.35} & \textbf{78.62$\pm$6.41}\\
& & 8x & \textbf{32.02$\pm$2.95} & \textbf{91.53$\pm$3.07} & \textbf{31.47$\pm$2.49} & \textbf{71.84$\pm$6.47} & \textbf{29.72$\pm$2.38}&\textbf{78.08$\pm$6.86} \\
\hline
\end{tabular}
}
\label{tab:hetA_single}
\end{table}




\begin{table}[t]
\centering
\caption{Within-site and cross-site reconstruction performance for multi-coil datasets (f.knee denotes fastMRI-knee, f.brain denotes fastMRI-brain, UMRAM denotes the in-house dataset), where each site employs an entirely different model type.}
%\\
% \textcolor{blue}{Method (PSNR, SSIM difference): \\
%  \textbf{W:} Single: -0.44,0.28; Central: -0.18,1.04; Distillation-based: 1.82,2.90; Generative-based: 3.07, 4.16. \\
% \textbf{A:} Single: 1.66,1.91; Central: -1.75,-1.06; Distillation-based: 1.18,2.03; Generative-based: 4.11, 6.84.}
\resizebox{1.025\columnwidth}{!}{
\begin{tabular}{| Sc | l l | Sc | Sc | Sc | Sc | Sc | Sc |}
 \hline
\multicolumn{3}{|Sc|}{\textbf{Within-site}} & \multicolumn{2}{Sc|}{{Site 1 (f.knee-MoDL)}} & \multicolumn{2}{Sc|}{{Site 2 (f.brain-rGAN)}}& \multicolumn{2}{Sc|}{{Site 3 (UMRAM-D5C5)}}\\
 \cline{4-9}
\multicolumn{3}{|Sc|}{\textbf{reconstruction}} &PSNR $\uparrow$&SSIM $\uparrow$& PSNR $\uparrow$& SSIM $\uparrow$&PSNR $\uparrow$&SSIM $\uparrow$\\
 \hline
 \multirow{4}{*}{\rotatebox[origin=c]{90}{\textit{non-FL}}}& \multirow{2}{*}{\underline{Single}}   & 
    4x & 33.78$\pm$2.27 & 93.37$\pm$1.52 & {36.97$\pm$3.07} &{97.32$\pm$1.17} &{31.81$\pm$1.73}
&{90.10$\pm$2.57}\\
& & 8x & 29.39$\pm$2.06 & 83.90$\pm$3.08 & {33.08$\pm$3.39}& {94.54$\pm$2.26}& {28.77$\pm$1.71}& {85.29$\pm$3.64}\\
 \cline{2-9}
& \multirow{2}{*}{Central}   &  
     4x & 33.69$\pm$2.32 & 93.20$\pm$1.63 & 36.93$\pm$3.22 & 97.32$\pm$1.19 & 31.02$\pm$1.60 &  88.49$\pm$2.56\\
& & 8x  & 29.39$\pm$2.19 & 83.62$\pm$3.09 & 33.79$\pm$3.05 & 94.59$\pm$2.13 & 27.39$\pm$1.68 & 82.72$\pm$3.04\\
 \cline{1-9}
\multirow{10}{*}{\rotatebox[origin=c]{90}{\textit{FL methods}}} & \multirow{2}{*}{FedDF}   & 
    4x & 32.79$\pm$2.19& 
    90.40$\pm$2.45 & 33.95$\pm$2.88 & 96.30$\pm$1.39 & 
   30.34$\pm$1.55  & {88.99$\pm$2.53}\\
& & 8x & 27.70$\pm$2.33 &
78.94$\pm$4.57 &
28.80$\pm$2.82 & 90.60$\pm$3.14 & 27.05$\pm$1.72 & 83.94$\pm$3.40\\
 \cline{2-9}
 & \multirow{2}{*}{FedMD}   & 
    4x & 32.48$\pm$2.24&
    90.57$\pm$2.21 &
    34.07$\pm$2.83 & 96.36$\pm$1.36 & 30.10$\pm$1.61 & 88.73$\pm$2.52 \\
& & 8x & 26.99$\pm$2.47 &
78.49$\pm$4.51 & 29.08$\pm$3.15& 90.16$\pm$3.23 & 27.13$\pm$1.64 & {84.18$\pm$3.31} \\
 \cline{2-9}
 & \multirow{2}{*}{FedGIMP} & 
4x &  33.78$\pm$2.27 & 93.41$\pm$1.48 & 31.72$\pm$3.85 & 94.25$\pm$4.32 & 30.11$\pm$2.08 &  88.56$\pm$2.43\\
& & 8x  & 28.97$\pm$2.12 & 83.68$\pm$3.07 &  28.78$\pm$3.10 & 90.71$\pm$5.21 &  26.26$\pm$2.68 & 82.69$\pm$3.52\\
 \cline{2-9}

 & \multirow{2}{*}{FedDDA} & 
4x &  32.53$\pm$2.33 & 91.75$\pm$1.82 & 32.69$\pm$3.46 & 94.71$\pm$3.44 &  24.47$\pm$2.57 &  81.20$\pm$3.47\\
& & 8x  & 28.09$\pm$2.22 & 81.90$\pm$2.93 & 29.63$\pm$3.28 & 90.85$\pm$5.46 & 18.38$\pm$1.97 & 68.73$\pm$5.73\\
 \cline{2-9}
& \multirow{2}{*}{FedGAT}   & 
    4x & \textbf{34.10$\pm$2.24} & \textbf{97.32$\pm$1.67}  & \textbf{35.86$\pm$2.78} &  \textbf{96.85$\pm$1.19} & \textbf{31.94$\pm$1.70} & \textbf{90.21$\pm$2.51} \\
& & 8x & \textbf{29.49$\pm$2.21} & \textbf{83.73$\pm$3.07} & \textbf{31.48$\pm$2.34}
 & \textbf{92.89$\pm$2.38} & \textbf{28.28$\pm$1.89} &  \textbf{85.20$\pm$3.58} \\
 \hline 
 \multicolumn{9}{Sc}{\mbox{ }} \\[-1.75ex]
 \hline
 \multicolumn{3}{|Sc|}{\textbf{Across-site}} & \multicolumn{2}{Sc|}{{Site 1 (f.knee-MoDL)}} & \multicolumn{2}{Sc|}{{Site 2 (f.brain-rGAN)}}& \multicolumn{2}{Sc|}{{Site 3 (UMRAM-D5C5)}}\\
 \cline{4-9}
\multicolumn{3}{|Sc|}{\textbf{reconstruction}} &PSNR $\uparrow$&SSIM $\uparrow$& PSNR $\uparrow$& SSIM $\uparrow$&PSNR $\uparrow$&SSIM $\uparrow$\\
 \hline
\multirow{4}{*}{\rotatebox[origin=c]{90}{\textit{non-FL}}} & \multirow{2}{*}{Single}   &    
4x & 38.40$\pm$3.26 & 97.32$\pm$1.67 & 31.17$\pm$3.22 & 93.03$\pm$2.57 & 26.18$\pm$2.62 & 79.42$\pm$5.73 \\
& & 8x & 32.39$\pm$3.04 & 92.63$\pm$2.94 & 26.82$\pm$3.15 & 85.13$\pm$5.06 & 22.64$\pm$2.30 & 70.31$\pm$6.85 \\
 \cline{2-9}
 & \multirow{2}{*}{\underline{Central}}   &     
  4x & 39.70$\pm$3.36 & 97.62$\pm$1.70 & 37.41$\pm$2.13 & 96.27$\pm$1.29
 &  27.52$\pm$2.49 & 81.63$\pm$5.35\\
& & 8x  & 34.33$\pm$3.12 & 93.86$\pm$3.04 & 33.98$\pm$2.11 & 91.89$\pm$2.82 & 25.09$\pm$2.47 & 74.38$\pm$6.46\\
 \cline{1-9}
\multirow{10}{*}{\rotatebox[origin=c]{90}{\textit{FL methods}}} & \multirow{2}{*}{FedDF}   & 
    4x & 38.03$\pm$2.99 &
    96.54$\pm$1.41
    & 32.30$\pm$2.93 & 92.69$\pm$1.76 &  26.68$\pm$2.55 & 81.09$\pm$5.31\\
& & 8x & 32.05$\pm$2.80 & 92.25$\pm$3.12 & 27.68$\pm$2.80 & 85.12$\pm$4.50 &
24.04$\pm$2.48& 70.60$\pm$6.94 \\
 \cline{2-9}
 & \multirow{2}{*}{FedMD}   & 
    4x & 37.77$\pm$3.07 &
    96.53$\pm$1.59 &32.62$\pm$2.94&
    92.69$\pm$1.69& 26.39$\pm$2.50 & 80.99$\pm$5.26\\
& & 8x &31.35$\pm$2.91 &
91.22$\pm$3.24&28.03$\pm$2.50&
84.44$\pm$4.68&23.99$\pm$2.43&70.13$\pm$6.61\\
 \cline{2-9} & \multirow{2}{*}{FedGIMP} & 
4x & 37.87$\pm$3.13& 97.23$\pm$1.50 & 26.57$\pm$3.13 & 85.71$\pm$5.69 & 26.00$\pm$2.49 & 80.03$\pm$5.68\\
& & 8x  & 32.01$\pm$2.98 & 92.52$\pm$2.91 & 22.85$\pm$2.93 & 75.96$\pm$8.28 & 22.20$\pm$2.29 & 67.89$\pm$8.82\\
 \cline{2-9}

 & \multirow{2}{*}{FedDDA} & 
 4x & 35.54$\pm$3.34& 94.49$\pm$2.44 & 27.04$\pm$3.51 &  84.62$\pm$4.68
 & 22.34$\pm$2.33 & 72.91$\pm$6.55\\
& & 8x  & 30.15$\pm$3.57 &  88.74$\pm$3.98 & 23.98$\pm$3.40 & 75.66$\pm$5.86 & 19.21$\pm$2.32& 60.73$\pm$9.18\\
 \cline{2-9}
{} & \multirow{2}{*}{FedGAT}   & 
    4x & \textbf{39.25$\pm$3.09}
 & \textbf{97.50$\pm$1.52} & \textbf{33.95$\pm$2.34} & \textbf{93.45$\pm$1.95} & \textbf{27.17$\pm$2.58} & \textbf{81.24$\pm$5.63}\\
& & 8x & \textbf{33.52$\pm$2.98} & \textbf{92.90$\pm$3.11} & \textbf{29.20$\pm$2.28} & \textbf{86.58$\pm$4.23} & \textbf{24.45$\pm$2.55} & \textbf{77.63$\pm$8.77}\\
\hline
\end{tabular}
}
\label{tab:hetA_multi}
\end{table}


\subsection{Modeling Procedures}
We considered several three-site FL setups to examine performance under model-heterogeneous settings. The reconstruction models trained in these setups included MoDL \cite{MoDl}, rGAN \cite{rgan}, and D5C5 \cite{Schlemper2017}. All sites were taken to use common imaging operators with matched acceleration rates and k-space sampling densities. Each site was allowed to train a distinct reconstruction model. In a first experiment, the three sites trained entirely distinct architectures (MoDL-3, rGAN, D5C5). In a second experiment, the sites trained the same model type albeit using different numbers of cascades (MoDL-3, MoDL-5, MoDL-7). All models were implemented using the PyTorch framework and executed on RTX 4090 GPUs. Models were trained via the AdamW optimizer with $\beta_1$$\,=\,$0.9, $\beta_2$$\,=\,$0.95, a weight decay of 0.05, and a learning rate of $\eta$$\,=\,$$10^{-4}$. 

As model-heterogeneous settings preclude aggregation via simple averaging of model weights, FL baselines used either model distillation or synthetic image generation as a means for cross-site knowledge transfer. Note that distillation-based aggregation involves bidirectional distillation between global and site-specific models, with each direction incurring significantly higher computational costs compared to model averaging, as the student model must be trained based on a sizable set of input-output images produced by the teacher model. Here, we observed that distillation-based baselines (FedDF, FedMD) attained a near-optimal trade-off between performance and efficiency with a total of 200 training epochs and distillation  every 100 epochs. In generative baselines (FedGIMP, FedDDA), the two-tier strategy in FedGAT was followed under matching training procedures with $N_c$$\,=\,$500, $N_l$$\,=\,$1, $N_p$$\,=\,$100 and $N_f$$\,=\,$100. 

Reconstruction performance was evaluated via peak signal-to-noise ratio (PSNR) and structural similarity (SSIM) metrics. Fidelity of synthetic MR images was evaluated via Freschet's inception distance (FID) metric. Performance differences among competing methods were examined via non-parametric Wilcoxon signed-rank tests (p$<$0.05). 




\begin{figure*}[t]
\centering
\includegraphics[width=0.875\textwidth]{figures/figure3.png}
\captionsetup{justification=justified, width=\textwidth}
\caption{Representative reconstructions at R=8x from zero-filled Fourier method (Zero-filled), single-site models (Single), FL baselines (FedDF, FedMD, FedGIMP, FedDDA), and FedGAT, along with reference images. \textbf{(a)} Site-specific BraTS model tested on fastMRI, \textbf{(b)} Site-specific fastMRI model tested on MIDAS, \textbf{(c)} Site-specific MIDAS model on BraTS. Zoom-in displays are included showing error maps (left) and images (right) within a focal window to emphasize method differences.}
\label{fig:visualresA}
\end{figure*}

\begin{table}[t]
\centering
\caption{Within-site and cross-site reconstruction performance for single-coil datasets, where each site employs a variant of MoDL with different number of cascades.}
% \textcolor{blue}{Method (PSNR, SSIM difference): \\
% \textbf{W:} Single: -0.91, -0.33; Central: -0.49, -0.05; Distillation-based: 1.62, 0.82; Generative-based: 1.80, 5.71. \\
% \textbf{A:} Single:  1.85,  2.10; Central: -0.62, -0.19; Distillation-based: 1.74, 0.61; Generative-based: 2.75, 6.97.}
\resizebox{1.025\columnwidth}{!}{%
\begin{tabular}{| Sc | l l | Sc | Sc | Sc | Sc | Sc | Sc |}
 \hline
\multicolumn{3}{|Sc|}{\textbf{Within-site}} & \multicolumn{2}{Sc|}{Site 1 (BraTS-MoDL-5)} & \multicolumn{2}{Sc|}{Site 2 (fastMRI-MoDL-3)} & \multicolumn{2}{Sc|}{Site 3 (MIDAS-MoDL-7)} \\
 \cline{4-9}
\multicolumn{3}{|Sc|}{\textbf{Reconstruction}} & PSNR $\uparrow$   & SSIM $\uparrow$    & PSNR $\uparrow$   & SSIM $\uparrow$    & PSNR $\uparrow$  & SSIM $\uparrow$   \\
 \hline
\multirow{4}{*}{\rotatebox[origin=c]{90}{\textit{Non-FL}}}& \multirow{2}{*}{\underline{Single}}     & 4x & 51.80$\pm$2.83 & 99.86$\pm$0.06 & 40.79$\pm$2.94 & 97.09$\pm$1.26 & 36.31$\pm$2.89 & 95.91$\pm$1.71 \\
& & 8x & 44.07$\pm$3.26 & 99.38$\pm$0.33 & 36.30$\pm$2.83 & 95.14$\pm$1.81 & 30.93$\pm$2.81 & 92.45$\pm$3.01 \\
 \cline{2-9}
 & \multirow{2}{*}{Central}     & 4x & 51.30$\pm$2.38 & 99.83$\pm$0.08 & 40.00$\pm$3.12 & 96.76$\pm$1.36 & 36.06$\pm$2.76 & 95.73$\pm$1.73 \\
& & 8x & 44.19$\pm$2.79 & 99.26$\pm$0.36 & 35.02$\pm$3.00 & 94.14$\pm$2.05 & 31.08$\pm$2.80 & 92.47$\pm$2.97 \\
 \cline{1-9}
\multirow{10}{*}{\rotatebox[origin=c]{90}{\textit{FL methods}}}& \multirow{2}{*}{FedDF}   & 4x & 47.08$\pm$2.88 & 99.63$\pm$0.13 & 39.31$\pm$3.12 & 96.35$\pm$1.55 & 33.31$\pm$2.71 & 94.19$\pm$2.13 \\
& & 8x & 41.53$\pm$3.13 & 98.89$\pm$0.46 & 34.18$\pm$3.13 & 93.49$\pm$2.48 & 29.54$\pm$2.55 & 90.28$\pm$3.28 \\
 \cline{2-9}
 & \multirow{2}{*}{FedMD}   & 4x & 47.02$\pm$2.84 & 99.63$\pm$0.13 & 39.23$\pm$3.27 & 96.37$\pm$1.58 & 33.37$\pm$2.69 & 94.20$\pm$2.12 \\
& & 8x & 41.43$\pm$3.10 & 98.86$\pm$0.47 & 33.97$\pm$3.16 & 93.52$\pm$2.46 & 30.03$\pm$2.53 & 90.52$\pm$3.25 \\
 \cline{2-9}
& \multirow{2}{*}{FedGIMP} 
  &4x& 46.33$\pm$3.07 &99.56$\pm$0.14 &39.46$\pm$3.60 & 96.38$\pm$1.78& 34.71$\pm$2.88& 94.66$\pm$2.21\\
&&8x&41.60$\pm$2.89& 98.72$\pm$0.42&34.90$\pm$3.32&93.97$\pm$2.57& 30.28$\pm$2.78&90.56$\pm$3.63\\
 \cline{2-9}
 & \multirow{2}{*}{FedDDA}  
 % & 4x & 47.38$\pm$2.81& 99.66$\pm$0.12  &39.22$\pm$3.35 &96.53$\pm$1.51&35.12$\pm$2.81&95.21$\pm$1.92\\
& 4x & 47.38$\pm$2.81& 99.66$\pm$0.12  &39.22$\pm$3.35 &96.53$\pm$1.51&29.16$\pm$1.39&60.74$\pm$2.14\\
%& & 8x & 42.92$\pm$2.99 & 99.08$\pm$0.40 & 34.14$\pm$3.36& 93.86$\pm$2.43&\textbf{30.56$\pm$2.84}& 91.34$\pm$3.15\\
& & 8x & 42.92$\pm$2.99 & 99.08$\pm$0.40 & 34.14$\pm$3.36& 93.86$\pm$2.43&27.80$\pm$2.03& 63.50$\pm$2.74\\
 \cline{2-9}
 & \multirow{2}{*}{FedGAT}   & 4x & \textbf{50.42$\pm$2.44} & \textbf{99.81$\pm$0.08} & \textbf{40.01$\pm$3.20} & \textbf{96.78$\pm$1.35} & \textbf{35.33$\pm$2.90} & \textbf{95.57$\pm$1.72} \\
& & 8x & \textbf{43.43$\pm$2.85} & \textbf{99.25$\pm$0.35} & \textbf{35.50$\pm$2.91} & \textbf{94.54$\pm$2.10} & \textbf{30.04$\pm$2.87} & \textbf{91.92$\pm$3.10} \\
 \hline
  \multicolumn{9}{Sc}{\mbox{ }} \\[-1.75ex]
  \hline
 \multicolumn{3}{|Sc|}{\textbf{Across-site}} & \multicolumn{2}{Sc|}{Site 1 (BraTS-MoDL-5)} & \multicolumn{2}{Sc|}{Site 2 (fastMRI-MoDL-3)} & \multicolumn{2}{Sc|}{Site 3 (MIDAS-MoDL-7)} \\
 \cline{4-9}
\multicolumn{3}{|Sc|}{\textbf{Reconstruction}} & PSNR $\uparrow$   & SSIM $\uparrow$    & PSNR $\uparrow$   & SSIM $\uparrow$    & PSNR $\uparrow$  & SSIM $\uparrow$   \\
 \hline
\multirow{4}{*}{\rotatebox[origin=c]{90}{\textit{Non-FL}}}& \multirow{2}{*}{Single}     & 4x & 34.21$\pm$4.48 & 90.92$\pm$4.36 & 39.25$\pm$6.94 & 95.83$\pm$4.14 & 45.38$\pm$5.04 & 98.28$\pm$1.77 \\
& & 8x & 30.79$\pm$4.08 & 87.83$\pm$5.52 & 34.46$\pm$6.48 & 92.76$\pm$5.57 & 38.99$\pm$4.18 & 97.09$\pm$2.44 \\
 \cline{2-9}
 & \multirow{2}{*}{\underline{Central}}     & 4x & 38.98$\pm$4.55 & 96.36$\pm$1.80 & 40.83$\pm$7.05 & 96.78$\pm$3.19 & 47.23$\pm$5.35 & 98.62$\pm$1.50 \\
& & 8x & 33.82$\pm$4.45 & 93.52$\pm$3.15 & 35.31$\pm$6.20 & 93.43$\pm$5.26 & 41.77$\pm$4.41 & 97.75$\pm$2.03\\
 \cline{1-9}
\multirow{10}{*}{\rotatebox[origin=c]{90}{\textit{FL methods}}}& \multirow{2}{*}{FedDF}   & 4x & 36.36$\pm$2.96 & 95.21$\pm$1.92 & 39.33$\pm$2.66 & 96.23$\pm$1.74 & 43.44$\pm$3.03 & 98.09$\pm$1.04 \\
& & 8x & 31.94$\pm$2.87 & 92.09$\pm$2.86 & 34.91$\pm$2.80 & 93.62$\pm$2.48 & 37.86$\pm$3.15 & 96.31$\pm$1.75 \\
 \cline{2-9}
 & \multirow{2}{*}{FedMD}   & 4x & 
 36.28$\pm$2.92 & 95.24$\pm$1.87 & 39.55$\pm$2.75& 96.28$\pm$1.79 & 43.43$\pm$3.02 & 98.08$\pm$1.04\\
& & 8x & 31.91$\pm$2.85 & 92.16$\pm$2.81& 34.87$\pm$2.84 & 93.63$\pm$2.53 & 37.58$\pm$3.25 & 96.37$\pm$1.72\\
\cline{2-9}
& \multirow{2}{*}{FedGIMP}  

&4x&36.97$\pm$4.56& 94.87$\pm$3.09& 38.71$\pm$6.51& 95.25$\pm$4.62& 44.10$\pm$4.08 & 98.33$\pm$1.74
\\
&&8x& 32.89$\pm$4.50&92.33$\pm$3.89&34.65$\pm$6.20& 92.31$\pm$5.76&39.02$\pm$3.74&96.84$\pm$2.79\\
 \cline{2-9}
 & \multirow{2}{*}{FedDDA} 
 & 4x & 37.52$\pm$4.02 &95.48$\pm$2.51&39.81$\pm$6.80&95.91$\pm$4.06& 32.27$\pm$1.40&59.96$\pm$6.01\\
& & 8x & 32.98$\pm$4.21 & 92.71$\pm$3.53& 35.06$\pm$6.15 &93.16$\pm$5.36 &31.45$\pm$1.78 & 59.85$\pm$7.96\\
% & 4x & 37.52$\pm$4.02 &95.48$\pm$2.51&39.81$\pm$6.80&95.91$\pm$4.06&45.58$\pm$4.90&98.54$\pm$1.59\\
%& & 8x & 32.98$\pm$4.21 & 92.71$\pm$3.53& 35.06$\pm$6.15 &93.16$\pm$5.36 &39.39$\pm$4.12 & 97.06$\pm$2.76\\
 \cline{2-9}
 & \multirow{2}{*}{FedGAT}   & 4x & \textbf{38.07$\pm$4.48} & \textbf{96.09$\pm$1.93} & \textbf{40.32$\pm$7.04} & \textbf{96.33$\pm$3.66} & \textbf{46.68$\pm$5.21} & \textbf{98.68$\pm$1.65}\\
& & 8x  &\textbf{33.35$\pm$4.23} & \textbf{93.16$\pm$3.37} &\textbf{35.13$\pm$6.94}& \textbf{93.64$\pm$5.43} &\textbf{40.65$\pm$4.37} & \textbf{97.42$\pm$2.31} \\
  \hline
\end{tabular}}
\label{tab:hetB_single}
\end{table}











\begin{table}[t]
\centering
\caption{Within-site and cross-site reconstruction performance for multi-coil datasets, where each site employs a variant of MoDL with different number of cascades. }
%{Method (PSNR, SSIM difference): \\
% \textbf{W:} Single: -0.96, -0.23; Central: -0.06, -0.01; Distillation-based: 1.41, 1.48; Generative-based: 1.04, 0.92. \\
% \textbf{A:} Single:  0.98,  0.73; Central: -0.83, -0.63; Distillation-based: 1.05, 1.37; Generative-based: 1.87,  1.61.}

\resizebox{1.025\columnwidth}{!}
{
\begin{tabular}{| Sc | l l | Sc | Sc | Sc | Sc | Sc | Sc |}
 \hline
\multicolumn{3}{|Sc|}{\textbf{Within-site}} & \multicolumn{2}{Sc|}{{Site 1 (f.knee-MoDL-3)}} & \multicolumn{2}{Sc|}{{Site 2 (f.brain-MoDL-5)}}& \multicolumn{2}{Sc|}{{Site 3 (UMRAM-MoDL-7)}}\\
\cline{4-9}
\multicolumn{3}{|Sc|}{\textbf{Reconstruction }} & PSNR $\uparrow$   & SSIM $\uparrow$    & PSNR $\uparrow$   & SSIM $\uparrow$    & PSNR $\uparrow$  & SSIM $\uparrow$   \\
 \hline
\multirow{4}{*}{\rotatebox[origin=c]{90}{\textit{Non-FL}}}& \multirow{2}{*}{\underline{Single}}   & 
 4x  & 33.78$\pm$2.27 & 93.37$\pm$1.52 & 40.33$\pm$3.45 & 98.03$\pm$0.84 & 49.96$\pm$2.06 & 99.61$\pm$0.17 \\ 
 & & 8x & 29.39$\pm$2.06 & 83.90$\pm$3.08 & 34.21$\pm$3.36 & 94.33$\pm$2.01 & 42.11$\pm$1.90 & 98.25$\pm$0.73  \\
 \cline{2-9}
 & \multirow{2}{*}{Central}   & 
 4x & 33.69$\pm$2.32 & 93.20$\pm$1.63 & 40.95$\pm$3.84 & 98.05$\pm$1.19 & 46.36$\pm$2.22 & 99.32$\pm$0.30 \\
& & 8x & 29.39$\pm$2.19 & 83.62$\pm$3.09 & 34.27$\pm$3.50 & 94.11$\pm$2.28 & 39.73$\pm$1.86 & 97.87$\pm$0.72 \\
 \cline{1-9}
\multirow{10}{*}{\rotatebox[origin=c]{90}{\textit{FL methods}}}& \multirow{2}{*}{FedDF}   & 
    4x & 32.33$\pm$2.36 & 90.47$\pm$2.44 & 38.87$\pm$3.62 & 95.66$\pm$1.49 & 44.56$\pm$2.72 & 98.52$\pm$0.63\\
& & 8x & 28.36$\pm$2.31 & 80.66$\pm$4.14 & 32.97$\pm$3.73 & 91.37$\pm$3.37 & 37.00$\pm$2.30 & 96.73$\pm$1.40 \\
 \cline{2-9}
 & \multirow{2}{*}{FedMD}   & 
    4x & 32.93$\pm$2.35 & 91.96$\pm$1.77 & 39.94$\pm$3.94 & 97.49$\pm$1.09 & 44.63$\pm$3.05 & \textbf{99.25$\pm$0.28}\\
& & 8x & 28.15$\pm$2.42 & 81.60$\pm$3.52 & 33.85$\pm$3.79 & 93.68$\pm$2.55 & 37.55$\pm$2.55 & 97.09$\pm$1.20\\

 \cline{2-9}
 \cline{2-9}
 & \multirow{2}{*}{FedGIMP} & 
4x & 33.78$\pm$2.27 & 93.41$\pm$1.48 & 40.54$\pm$3.38 & 97.88$\pm$0.90 & 44.99$\pm$2.09 & 99.02$\pm$0.32 \\
& & 8x & 28.97$\pm$2.12 & 83.68$\pm$3.07 & 34.19$\pm$3.37 & 93.80$\pm$3.22 & 39.53$\pm$1.70 & 97.68$\pm$0.77 \\
 \cline{2-9}
 & \multirow{2}{*}{FedDDA} & 
4x  & 32.53$\pm$2.33 & 91.75$\pm$1.82 & 39.60$\pm$3.62 & 97.52$\pm$1.38 & 42.57$\pm$2.92 & 97.32$\pm$0.78 \\
& & 8x & 28.09$\pm$2.22 & 81.90$\pm$2.93 & 33.25$\pm$3.52 & 93.61$\pm$2.76 & 37.51$\pm$2.17 & 93.63$\pm$1.50\\
 \cline{2-9}
 & \multirow{2}{*}{FedGAT} & 
4x & \textbf{34.10$\pm$2.24}& \textbf{93.44$\pm$1.46} & \textbf{40.87$\pm$3.58} & \textbf{98.03$\pm$1.27} & \textbf{45.13$\pm$2.14} & 99.22$\pm$0.29\\
& & 8x  & \textbf{29.49$\pm$2.21} & \textbf{83.73$\pm$3.07} & \textbf{34.20$\pm$3.48} & \textbf{93.85$\pm$2.52} & \textbf{40.21$\pm$1.71} & \textbf{97.84$\pm$0.66}\\
 \hline
  \multicolumn{9}{Sc}{\mbox{ }} \\[-1.75ex]
 \hline
\multicolumn{3}{|Sc|}{\textbf{Across-site}} & \multicolumn{2}{Sc|}{{Site 1 (f.knee-MoDL-3)}} & \multicolumn{2}{Sc|}{{Site 2 (f.brain-MoDL-5)}}& \multicolumn{2}{Sc|}{{Site 3 (UMRAM-MoDL-7)}}\\
\cline{4-9}
\multicolumn{3}{|Sc|}{\textbf{Reconstruction}} & PSNR $\uparrow$   & SSIM $\uparrow$    & PSNR $\uparrow$   & SSIM $\uparrow$    & PSNR $\uparrow$  & SSIM $\uparrow$   \\
 \hline
\multirow{4}{*}{\rotatebox[origin=c]{90}{\textit{Non-FL}}}& \multirow{2}{*}{Single} & 
4x & 38.40$\pm$3.26 & 97.32$\pm$1.67 & 39.42$\pm$4.49 & 96.91$\pm$2.13 & 37.60$\pm$4.58 & 96.03$\pm$2.34 \\
& & 8x & 32.39$\pm$3.04 & 92.63$\pm$2.94 & 32.99$\pm$3.57 & 90.55$\pm$6.10 & 31.48$\pm$3.96 & 88.07$\pm$5.35 \\
 \cline{2-9}
 & \multirow{2}{*}{\underline{Central}} & 
4x & 39.70$\pm$3.36 & 97.62$\pm$1.70 & 40.64$\pm$4.84 & 97.36$\pm$1.99 & 40.39$\pm$4.11 & 97.53$\pm$1.31 \\
& & 8x & 34.33$\pm$3.12 & 93.86$\pm$3.04 & 33.61$\pm$3.89 & 91.38$\pm$5.74 & 34.47$\pm$3.50 & 91.89$\pm$4.04  \\
 \cline{1-9}
\multirow{10}{*}{\rotatebox[origin=c]{90}{\textit{FL methods}}}& \multirow{2}{*}{FedDF}   & 
     4x & 38.15$\pm$3.41 & 96.40$\pm$2.11 & 38.28$\pm$4.64 & 95.18$\pm$3.56 & 37.37$\pm$4.12 & 94.73$\pm$2.39\\
&&8x& 32.94$\pm$3.12 & 92.43$\pm$3.47 & 32.69$\pm$4.11 & 89.39$\pm$7.34
& 30.15$\pm$3.64 & 86.84$\pm$5.61\\
 \cline{2-9}
 & \multirow{2}{*}{FedMD}   & 
    4x & 38.75$\pm$3.44 & 97.01$\pm$1.71 & 39.32$\pm$4.78 & 96.80$\pm$2.28 & 38.39$\pm$4.49 & 95.92$\pm$2.28 \\
& & 8x & 32.52$\pm$2.90 & 92.24$\pm$3.02 & 33.19$\pm$3.76 & 90.90$\pm$6.08 & 31.90$\pm$3.81 & 87.51$\pm$5.55\\
 \cline{2-9}
 \cline{2-9}
 & \multirow{2}{*}{FedGIMP} & 
4x & 37.87$\pm$3.13 & 97.23$\pm$1.50 & 38.93$\pm$4.32 & 96.75$\pm$2.16 & 37.40$\pm$4.38 & 96.35$\pm$1.76 \\
& & 8x & 32.01$\pm$2.98 & 92.52$\pm$2.91 & 32.96$\pm$3.63 & 90.71$\pm$5.94 & 31.70$\pm$4.13 & 89.29$\pm$5.18 \\
 \cline{2-9}
 & \multirow{2}{*}{FedDDA} & 
4x  & 35.54$\pm$3.34 & 94.49$\pm$2.44 & 37.78$\pm$3.78 & 95.65$\pm$1.52 & 36.61$\pm$4.16 & 95.15$\pm$2.36 \\
& & 8x  & 30.15$\pm$3.57 & 88.74$\pm$3.98 & 32.10$\pm$3.43 & 89.21$\pm$5.03 & 30.80$\pm$3.74 & 86.32$\pm$5.53\\
 \cline{2-9}
 & \multirow{2}{*}{FedGAT} 
&4x & \textbf{39.25$\pm$3.09} & \textbf{97.50$\pm$1.52} & \textbf{39.65$\pm$4.53} & \textbf{97.16$\pm$2.07} & \textbf{39.16$\pm$4.33} & \textbf{97.10$\pm$1.60} \\
& & 8x  & \textbf{33.52$\pm$2.98} & \textbf{92.90$\pm$3.11} & \textbf{33.58$\pm$3.93} & \textbf{90.93$\pm$5.81} & \textbf{32.98$\pm$3.89} & \textbf{90.29$\pm$4.57}  \\
 \hline
\end{tabular}
}
\label{tab:hetB_multi}
\end{table}

\subsection{Datasets}
In single-coil experiments, multi-contrast coil-combined magnitude MR images from public BraTS \cite{BRATS}, fastMRI \cite{fastMRI} and MIDAS \cite{midas} datasets were analyzed. Each dataset was taken to correspond to a separate site. At each site, (training, validation, test) splits of (40, 5, 10) non-overlapping subjects were used, resulting in (840, 105, 210) cross-sectional images. Subjects were selected at random from the datasets. 

In multi-coil experiments, multi-contrast k-space data from public fastMRI-brain, fastMRI-knee \cite{fastMRI} and an in-house dataset were analyzed \cite{elmas2022federated}. Experimental procedures for collecting the in-house dataset were approved by the local ethics committee at Bilkent University, and written informed consent was obtained from all participants. Each dataset was taken to correspond to a separate site. At each site, (training, validation, test) splits of (36, 6, 8) non-overlapping subjects were used, resulting in (840, 105, 210) cross-sectional images. Subjects were selected at random from the datasets. Multi-coil data were compressed to 5 virtual coils \cite{Zhang2013}.  

Both actual and synthetic k-space data were retrospectively undersampled using a variable-density (VD) pattern with acceleration factors R = 4x and 8x \cite{Lustig2007}. Models were trained on a mixture of multi-contrast MRI datasets without special procedures to handle separate contrasts. When necessary, actual and synthetic MR images were zero-padded to a consistent size to permit training of reconstruction models, albeit performance assessments were conducted in native image sizes by discarding the padded regions following reconstruction. 




\section{Results}

\subsection{Comparison Studies}
To demonstrate the utility of FedGAT for enabling flexible collaborations in learning-based MRI reconstruction, we considered model-heterogeneous FL setups where each site used a different network architecture to build its reconstruction model. FedGAT was compared against several state-of-the-art baselines including distillation methods (FedDF, FedMD), and generative methods (FedGIMP, FedDDA). On one hand, site-specific performance was evaluated by conducting within-site reconstructions, where the eventual reconstruction model available at a given site was deployed on the same site (i.e., model for Site $k$ tested on data from Site $k$). `Single' models trained on single-site data locally at each site were considered as a performance benchmark for these within-site reconstructions. On the other hand, generalization performance was evaluated by conducting across-site reconstructions, where the eventual reconstruction model available at a given site was deployed on remaining sites (i.e., model for Site $k$ tested on data from Site $l$ where $k \neq l$). `Central' models trained on the server using multi-site data aggregated across all sites were taken as a privacy-violating performance benchmark for across-site reconstructions.


In a first FL setup, we examined performance when individual sites employed entirely distinct types of model architectures (i.e., MoDL-3, rGAN, D5C5). Performance metrics are listed in Table \ref{tab:hetA_single} for single-coil and in Table \ref{tab:hetA_multi} for multi-coil MRI datasets. \ul{\textit{For within-site reconstructions}}, FedGAT achieves significantly higher performance than all competing FL methods (p$<$0.05), except for FedGIMP that yields higher PSNR for R=8x on single-coil data at Site 1. On average, FedGAT offers (PSNR,\,SSIM) improvements of (2.06dB,\,4.40\%) over distillation baselines, and (2.07dB,\,2.48)\% SSIM over generative baselines. Among FL methods, FedGAT attains the most competitive performance against the `Single' benchmark, and outperforms the privacy-violating `Central' by 0.42dB PSNR, 0.71\% SSIM (p$<$0.05). \ul{\textit{For across-site reconstructions}}, FedGAT achieves significantly higher performance than all competing FL methods (p$<$0.05). On average, FedGAT offers (PSNR,\,SSIM) improvements of (1.22dB,\,3.48\%) over distillation baselines, and (2.60dB,\,5.74)\% SSIM over generative baselines. In this setting, FedGAT attains the most competitive performance against the `Central' benchmark while outperforming `Single' by 1.59dB PSNR, 4.20\% SSIM (p$<$0.05), which showcasing the generalization benefits of training models on multi-site datasets. Representative images reconstructed via FL methods in the examined model-heterogeneous setup are depicted in Fig. \ref{fig:visualresA}. Corroborating the quantitative assessments, FedGAT attains higher image quality compared to baselines, with sharper depiction of structural details, lower artifact and noise in reconstructed images. Meanwhile, competing FL methods suffer from a degree of spatial blur, residual noise and artifacts that lead to suboptimal depiction of detailed tissue structures. 

In a second FL setup, we examined performance when individual sites employed distinct variants of a common model type (i.e., MoDL with 3, 5, versus 7 cascades), resulting in varying levels of model complexity across sites. Naturally, the benefits of model-agnostic approaches can be lower in this setup, when compared against the first setup that prescribed entirely distinct model types. Still, we reasoned that FedGAT should continue to yield significant benefits over competing FL baselines. Performance metrics are listed in Table \ref{tab:hetB_single} for single-coil and in Table \ref{tab:hetB_multi} for multi-coil MRI datasets. \ul{\textit{For within-site reconstructions}}, FedGAT achieves significantly higher performance than all competing FL methods, and attains the most competitive performances against the `Single' and privacy-violating `Central' (p$<$0.05), except for FedMD that yields higher PSNR on multi-coil data at Site 3. On average, FedGAT offers (PSNR,\,SSIM) improvements of (1.52dB,\,1.15\%) over distillation baselines, and (1.42dB,\,3.32)\% SSIM over generative baselines. \ul{\textit{For across-site reconstructions}}, FedGAT achieves significantly higher performance than all competing FL methods (p$<$0.05). On average, FedGAT offers (PSNR,\,SSIM) improvements of (1.40dB,\,0.99\%) over distillation baselines, and (2.31dB,\,4.29)\% SSIM over generative baselines. In this setting, FedGAT attains the most competitive performance against `Central' while outperforming `Single' by 1.42dB PSNR, 1.42\% SSIM (p$<$0.05), indicating improved generalization. Representative images reconstructed in the second model-heterogeneous FL setup are depicted in Fig. \ref{fig:visualresB}. Corroborating the quantitative assessments, FedGAT attains higher image quality compared to baselines, with sharper depiction of structural details, lower artifacts and noise. Meanwhile, competing FL methods suffer from a degree of spatial blur, noise or artifacts that elicit degradations in depiction of detailed tissue structures. 




\begin{figure*}[t]
\centering
\includegraphics[width=0.875\textwidth]{figures/figure4.png}
\captionsetup{justification=justified, width=\textwidth}
\caption{Representative reconstructions at R=8x from zero-filled Fourier method (Zero-filled), single-site models (Single), FL baselines (FedDF, FedMD, FedGIMP, FedDDA), and FedGAT, along with reference images. \textbf{(a)} Site-specific fastMRI-knee model tested on UMRAM, \textbf{(b)} Site-specific fastMRI-brain model tested on fastMRI-knee, \textbf{(c)} Site-specific UMRAM model tested on fastMRI-brain. Zoom-in windows of error maps and images are included to emphasize method differences.}
\label{fig:visualresB}
\end{figure*}


\subsection{Ablation Studies}
The primary aim of the current study is to enable collaborative training of generalizable reconstruction models with heterogeneous architectures across imaging sites. Towards this aim, FedGAT leverages synthetic MR images produced by a generative autoregressive transformer (GAT) as a privacy-preserving means of cross-site knowledge transfer. To examine the importance of the proposed GAT prior, we compared the fidelity of synthetic MR images generated by the GAT prior against those generated by adversarial (GAN) or diffusion (DDPM) priors. Fidelity of multi-site MR images generated by different priors as measured by FID are displayed in Fig. \ref{fig:FID}, and sample synthetic MR images are shown in Fig. \ref{fig:compare_synth}. We find that GAT achieves the highest image fidelity, as manifested by its consistently lower FID scores compared to variants, and by its improved visual realism apparent in sample images. While GAN shows structural artifacts and suboptimal contrast and DDPM occasionally shows elevated noise levels, GAT achieves high image quality with lower artifacts and noise. We then assessed whether this improvement in synthetic image fidelity translates onto benefits in reconstruction performance. For this purpose, performance of reconstruction models trained on synthetic images from the GAT, GAN and DDPM priors were compared as listed in Table \ref{tab:across_site_gan_ddpm_gat}, in reference to a benchmark model that was trained using actual multi-site MR images. We find that the GAT prior yields significantly higher reconstruction performance than variants based on GAN and DDPM priors (p$<$0.05), and it attains the most competitive performance against the benchmark model. These results corroborate that the superior image fidelity of the proposed GAT prior translates onto improving the training of site-specific reconstruction models.   

Next, we examined the efficacy of the knowledge-transfer approach in FedGAT that transfers information on the distribution of multi-site data via synthetic MR images. For this purpose, FedGAT was compared against a `w model agg.' variant that simultaneously transferred knowledge and built reconstruction models via conventional model aggregation \cite{FedAvg}. The comparisons were performed in model-homogeneous settings to permit use of model aggregation, and to avoid interactions between model architecture and data distribution across sites that could bias assessments. Performance metrics are listed in Table \ref{tab:ablation1}. FedGAT yields superior performance against `w model agg.' in all cases ((p$<$0.05), except for within-site reconstructions at Site 3 where `w model agg.' yields slightly higher SSIM. These results indicate that the decoupling of knowledge transfer and model building stages in FedGAT offers performance benefits against a coupled model aggregation procedure.   



\section{Discussion}
Here we introduced a model-agnostic FL technique for MRI reconstruction that supports model-heterogeneous settings by decoupling cross-site knowledge transfer from building of reconstruction models. In particular, FedGAT mediates knowledge transfer among sites by sharing a global GAT prior that uses autoregressive predictions across multiple spatial scales to generate synthetic MR images, and a site prompt for precise control over the synthetic image distribution. Previous FL studies aiming to build generative priors have predominantly adopted adversarial approaches that can suffer from poor image quality due to training instabilities \cite{rgan,Mardani2019b,ozbey2022unsupervised}, or diffusion approaches that can suffer from extensive inference times and residual image noise \cite{jalaln2021nips,korkmaz2023selfsupervised,chung2022media,gungor2023adaptive}. In comparison, the autoregressive nature of GAT offers improved image fidelity over adversarial and diffusion priors as indicated by our results. To curate a hybrid multi-site MRI dataset, the shared GAT prior is employed at each site to efficiently generate high-fidelity synthetic MRI datasets from remaining sites. Afterwards, site-specific reconstruction models are locally trained on this hybrid dataset to improve generalization while enjoying total freedom in architecture selection. Therefore, the proposed method holds great promise for advancing the practicality and scalability of FL in multi-institutional MRI studies.


\begin{figure}[t] % Single-column figure for one side
\centering
\includegraphics[width=0.675\columnwidth]{figures/figure5.png} % Adjust width as necessary for column size
\caption{Fidelity of synthetic MR images generated via GAN, DDPM and GAT priors. Bar plots of Frechet inception distance (FID) show mean\(\pm\)std across the test set.}
\label{fig:FID}
\end{figure}

\begin{figure}[t] % Single-column figure for one side
\centering
\includegraphics[width=0.9\columnwidth]{figures/figure6.png} % Adjust width as necessary for column size
\caption{Samples of synthetic MR images from GAN, DDPM and GAT priors, along with samples of local MR images at separate sites.}
\label{fig:compare_synth}
\end{figure}




\begin{table}[t]
\centering
\caption{Reconstruction performance of models trained on synthetic MR images generated by GAN, DDPM, and GAT priors, in reference to a benchmark model trained on actual MR images. Boldface indicates the top-performing generative prior in each task.}
\resizebox{1.025\columnwidth}{!}{%
\begin{tabular}{| Sc Sc | Sc | Sc | Sc | Sc | Sc | Sc |}
 \hline
\multicolumn{2}{|Sc|}{\textbf{Within-site}} & \multicolumn{2}{Sc|}{Site 1 (BraTS)} & \multicolumn{2}{Sc|}{Site 2 (fastMRI)} & \multicolumn{2}{Sc|}{Site 3 (MIDAS)} \\
 \cline{3-8}
\multicolumn{2}{|Sc|}{\textbf{reconstruction}} & PSNR $\uparrow$   & SSIM $\uparrow$    & PSNR $\uparrow$   & SSIM $\uparrow$    & PSNR $\uparrow$  & SSIM $\uparrow$   \\
 \hline
\multirow{1}{*}{Actual}  & 4x & 47.38$\pm$2.61&99.61$\pm$0.15&40.00$\pm$3.12&96.76$\pm$1.36&34.29$\pm$2.63&93.95$\pm$2.06\\
\multirow{1}{*}{data} & 8x & 40.89$\pm$2.78 & 98.08$\pm$0.85 & 35.02$\pm$3.00 & 94.14$\pm$2.05 & 29.74$\pm$2.63 & 88.78$\pm$3.36 \\
\hline
\multirow{2}{*}{GAN}   & 4x & 44.13$\pm$3.08 & 98.86$\pm$0.49 & 38.68$\pm$3.39 & 96.20$\pm$1.68 & 33.06$\pm$2.62 & 90.65$\pm$3.83 \\
 & 8x & 39.59$\pm$2.70 & 95.44$\pm$1.75 & 34.29$\pm$3.06 & 93.08$\pm$2.53 & 28.38$\pm$2.53 & 85.66$\pm$4.55 \\
\hline
 \multirow{2}{*}{DDPM} & 4x & 45.91$\pm$2.84 & 99.46$\pm$0.21 & 38.60$\pm$3.36 & 96.41$\pm$1.47 & 33.74$\pm$2.48 & 92.48$\pm$2.56 \\
& 8x & 39.44$\pm$2.87 & 96.36$\pm$1.46 & 34.07$\pm$2.78 & 93.44$\pm$2.15 & 28.44$\pm$2.45 & 86.60$\pm$3.40 \\
 \hline
 \multirow{2}{*}{GAT} &4x& \textbf{47.17$\pm$2.56} & \textbf{99.60$\pm$0.13} & \textbf{39.81$\pm$3.41}& \textbf{96.47$\pm$1.64} & \textbf{33.80$\pm$2.62} & \textbf{93.35$\pm$2.45} \\
&8x& \textbf{41.30$\pm$2.89} & \textbf{98.59$\pm$0.59} & \textbf{35.09$\pm$3.01} & \textbf{94.22$\pm$2.22 }& \textbf{29.14$\pm$2.59} & \textbf{88.85$\pm$3.45} \\
 \hline
\multicolumn{8}{Sc}{\mbox{ }} \\[-1.75ex]
 \hline
 \multicolumn{2}{|Sc|}{\textbf{Across-site}} & \multicolumn{2}{Sc|}{Site 1 (BraTS)} & \multicolumn{2}{Sc|}{Site 2 (fastMRI)} & \multicolumn{2}{Sc|}{Site 3 (MIDAS)} \\
\cline{3-8}
\multicolumn{2}{|Sc|}{\textbf{reconstruction}} & PSNR $\uparrow$   & SSIM $\uparrow$    & PSNR $\uparrow$   & SSIM $\uparrow$    & PSNR $\uparrow$  & SSIM $\uparrow$   \\
 \hline
\multirow{1}{*}{Actual} & 4x &37.14$\pm$4.06&95.35$\pm$2.24&40.83$\pm$7.05&96.78$\pm$3.19&43.69$\pm$4.68&98.18$\pm$1.73\\
\multirow{1}{*}{data} & 8x & 32.38$\pm$3.87 & 91.46$\pm$3.86 & 35.31$\pm$6.20 & 93.43$\pm$5.26 & 37.96$\pm$4.12 & 96.11$\pm$2.52 \\
 \hline
\multirow{2}{*}{GAN} & 4x & 35.87$\pm$4.13 & 93.43$\pm$4.05 & 38.59$\pm$6.23 & 94.76$\pm$4.93 & 41.41$\pm$4.23 & 97.53$\pm$1.81 \\
& 8x & 31.33$\pm$4.08 & 89.37$\pm$5.23 & 33.98$\pm$6.18 & 90.55$\pm$5.98 & 36.94$\pm$3.92 & 94.26$\pm$2.47 \\
\hline
\multirow{2}{*}{DDPM}& 4x & 36.17$\pm$3.83 & 94.48$\pm$2.89 & 39.82$\pm$6.64 & 95.97$\pm$3.93 & 42.25$\pm$4.80 & 97.97$\pm$1.82 \\
 & 8x & 31.25$\pm$3.85 & 90.02$\pm$4.45 & 33.94$\pm$6.12 & 91.48$\pm$5.54 & 36.75$\pm$3.90 & 94.90$\pm$2.35 \\
\hline
\multirow{2}{*}{GAT} & 4x & \textbf{36.80$\pm$4.28 }& \textbf{94.91$\pm$2.61} & \textbf{40.48$\pm$7.17} & \textbf{96.48$\pm$3.58} & \textbf{43.49$\pm$4.76} & \textbf{98.04$\pm$1.95} \\
& 8x & \textbf{32.11$\pm$4.09} & \textbf{91.53$\pm$3.95} &\textbf{35.22$\pm$6.67} & \textbf{93.72$\pm$5.46} & \textbf{38.20$\pm$4.29} & \textbf{96.40$\pm$2.72} \\
\hline
\end{tabular}
}
\label{tab:across_site_gan_ddpm_gat}
\end{table}

\begin{table}[t]
\centering
\caption{Ablation study on the efficacy of the knowledge transfer approach in FedGAT.}
\resizebox{1.025\columnwidth}{!}{%
\begin{tabular}{| l l | Sc | Sc | Sc | Sc | Sc | Sc |}
 \hline
\multicolumn{2}{|Sc|}{\textbf{Within-site}} & \multicolumn{2}{Sc|}{Site 1 (BraTS)} & \multicolumn{2}{Sc|}{Site 2 (fastMRI)}& \multicolumn{2}{Sc|}{Site 3 (MIDAS)}\\
\cline{3-8}
\multicolumn{2}{|Sc|}{\textbf{reconstruction}} & PSNR $\uparrow$   & SSIM $\uparrow$    & PSNR $\uparrow$   & SSIM $\uparrow$    & PSNR $\uparrow$  & SSIM $\uparrow$   \\
\hline
\multirow{2}{*}{FedGAT}   & 4x & \textbf{47.63$\pm$2.65} & \textbf{99.64$\pm$0.14} & \textbf{39.73$\pm$3.31} & \textbf{96.46$\pm$1.56} & \textbf{33.59$\pm$2.63} & \textbf{93.53$\pm$2.10} \\
& 8x & \textbf{41.60$\pm$2.85} & \textbf{98.78$\pm$0.54} &  \textbf{34.58$\pm$3.36} & \textbf{94.14$\pm$2.33} & \textbf{28.85$\pm$2.62} & 87.75$\pm$3.34 \\
\hline
\multirow{2}{*}{w model agg.}  & 4x & 46.65$\pm$2.86 & 99.58$\pm$0.14 & 38.52$\pm$3.62 & 95.50$\pm$2.32 & 33.14$\pm$2.52 & 92.80$\pm$2.75 \\
 & 8x & 40.80$\pm$3.12 & 97.96$\pm$0.82 & 32.94$\pm$3.52 & 91.83$\pm$3.59 & 28.77$\pm$2.23 & \textbf{87.85$\pm$3.43} \\
\hline
\multicolumn{8}{Sc}{\mbox{ }} \\[-1.75ex]
\hline
\multicolumn{2}{|Sc|}{\textbf{Across-site}} & \multicolumn{2}{Sc|}{Site 1 (BraTS)} & \multicolumn{2}{Sc|}{Site 2 (fastMRI)}& \multicolumn{2}{Sc|}{Site 3 (MIDAS)}\\
\cline{3-8}
\multicolumn{2}{|Sc|}{\textbf{reconstruction}} &PSNR $\uparrow$   & SSIM $\uparrow$    & PSNR $\uparrow$   & SSIM $\uparrow$    & PSNR $\uparrow$  & SSIM $\uparrow$   \\
 \hline
\multirow{2}{*}{FedGAT}   & 4x & \textbf{36.71$\pm$3.01} & \textbf{95.02$\pm$1.96} & \textbf{40.28$\pm$2.65} & \textbf{96.39$\pm$1.81} & \textbf{43.52$\pm$2.83} & \textbf{98.06$\pm$1.04} \\
& 8x & \textbf{32.02$\pm$2.95} & \textbf{91.53$\pm$3.07} & \textbf{35.52$\pm$2.84} & \textbf{93.68$\pm$2.55} & \textbf{37.98$\pm$2.97} & \textbf{95.65$\pm$1.94} \\
\hline
\multirow{2}{*}{w model agg.} & 4x & 35.83$\pm$3.12 & 94.15$\pm$2.54 & 39.89$\pm$2.70 & 96.19$\pm$1.95 & 42.59$\pm$3.26 & 97.54$\pm$1.64 \\
& 8x & 30.85$\pm$2.95 & 89.84$\pm$3.51 & 34.78$\pm$2.71 & 92.91$\pm$2.49 & 36.87$\pm$3.33 & 94.89$\pm$2.60 \\
\hline
\end{tabular}
}
\label{tab:ablation1}
\end{table}


The distribution of MRI datasets collected in separate sites inevitably reflects the variations in imaging protocols or scanners across sites \cite{li2021fedbn}. When straightforward averaging of model weights is used for aggregation, this distributional variability can compromise the sensitivity of the global model to site-specific image attributes in the conventional FL framework. Recent FL methods on MRI reconstruction have aimed at addressing this limitation via personalization strategies such as latent-space alignment \cite{guo2021}, partial network aggregation \cite{feng2023tmi}, test-time adaptation \cite{elmas2022federated}, or feature map normalization \cite{dalmaz2024one}. While these strategies allow a modest degree of model attuning to individual sites, a substantial portion of the global model must still be shared across sites to ensure adequate knowledge transfer. In contrast to existing FL methods that share reconstruction model weights, FedGAT transfers knowledge among sites by building a global generative prior that can later generate synthetic MR images from desired sites. This stark difference enables FedGAT to operate seamlessly under demanding model-heterogeneous settings, where no component of reconstruction models are shared among sites. In turn, FedGAT can sustain a higher-degree of site specialization in reconstruction models than permitted by previous approaches. 

Devised to accurately capture an underlying distribution, generative priors synthesize MR images depicting random anatomical structures that do not correspond to the precise anatomy of any actual subject \cite{jalaln2021nips,fdb}. In FedGAT, such synthetic MR images from separate sites are employed in training of site-specific reconstruction models to improve generalization across sites. Our assessments based on FID scores indicate that the synthetic MR images generated by the GAT prior attain a level of quality and realism closely mimicking actual MR images. Furthermore, reconstruction models trained on these synthetic MR images yield high performance when independently evaluated on actual MRI acquisitions in the test sets. Still, in the absence of anatomical guidance, the stochastic nature of generative priors naturally induce a degree of susceptibility to artificial features in synthetic MR images \cite{SelfRDB}. Our observations indicate that the downstream reconstruction models trained on synthetic MR images in this study are not unduly influenced by undesirable hallucinations. When necessary, hallucinations might be alleviated by building reconstruction models equipped with physics-driven modules and learning objectives \cite{Knoll2019inverseGANs}, or building generative priors via architectures that offer enhanced capture of contextual relationships among tissue signals \cite{I2IMamba,denomamba}. It remains important future work to validate the proposed method and its anatomical fidelity on broader patient cohorts.

FL frameworks help mitigate patient privacy risks by exchanging model weights rather than actual MRI data across sites. Regardless, security concerns can still be present in the presence of adversaries that attempt to corrupt model updates, hence compromising the accuracy of reconstructed images \cite{kaissis2020secure}. In addition, third-party inference attacks on trained reconstruction models might be able to leak sensitive information regarding the set of MR images used in training sets by probing the model weights  \cite{kaissis2020secure}. Previous methods based on the conventional FL framework directly communicate weights of a global reconstruction model across sites, so they are more susceptible to compromises in reconstruction fidelity due to training and inference attacks. In contrast, FedGAT privately builds reconstruction models at individual sites without ever communicating their weights, so it can show a degree of improved immunity against such attacks. Potential concerns related to the global GAT prior might be alleviated by incorporating differential privacy objectives in training of generative priors \cite{FengICCV2021,FedDPGAN}. Future research is warranted to systematically evaluate the privacy-preserving capabilities of FedGAT.


\section{Conclusion}
Here, we introduced a novel model-agnostic federated learning technique that enables multiple sites with distinct architectural preferences to collaborate in building MRI reconstruction models. To support model-heterogeneous settings, FedGAT decouples a first tier to learn a global generative prior that captures the distribution of multi-site MR images, from a second tier to build site-specific albeit generalizable reconstruction models with the aid of synthetic datasets locally generated by the global prior. Our experiments clearly demonstrate that FedGAT achieves superior performance compared to state-of-the-art FL baselines in both within-site and across-site reconstructions. These findings indicate that FedGAT promotes effective knowledge transfer across sites while remaining agnostic to architectures of reconstruction models, hence overcoming a critical barrier in supporting flexible collaborations across diverse institutions.  


\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,Papers}


\end{document}
