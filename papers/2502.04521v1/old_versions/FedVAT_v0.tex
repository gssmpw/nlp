\documentclass[journal,twoside,web]{ieeecolor}
\usepackage{tmi}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{float}
\usepackage{adjustbox}
\usepackage{caption}
\usepackage{stfloats}
\usepackage[dvipsnames]{xcolor}
\usepackage{textcomp}
\usepackage[utf8]{inputenc}
\usepackage{longtable}
\usepackage{pifont} % Add this to the preamble
\usepackage{multirow}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\setlength{\floatsep}{1ex}
\setlength{\dblfloatsep}{1ex}
\setlength{\textfloatsep}{1ex}
\setlength{\dbltextfloatsep}{1ex}
\setlength{\intextsep}{1ex}
\def\baselinestretch{1.0}
\setlength{\textheight}{1.000\textheight}
\setlength{\headsep}{2ex}
\setlength{\abovedisplayskip}{3pt}
\setlength{\belowdisplayskip}{3pt}
\setlength{\abovedisplayshortskip}{3pt}
\setlength{\belowdisplayshortskip}{3pt}
\setlength{\parskip}{0mm plus1mm minus0mm}
\setlength\tabcolsep{3 pt}
\renewcommand{\arraystretch}{1.25}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    pdftitle={FedVAT},
}

\SetKwInput{KwInput}{Input}                % Set the Input
\SetKwInput{KwOutput}{Output}              % set the Output

\usepackage[font=small,justification=justified,belowskip=2pt,aboveskip=2pt]{caption}
\setlength{\skip\footins}{3pt}

\usepackage[math]{cellspace}
\cellspacetoplimit 2pt
\cellspacebottomlimit 1pt

\floatname{algorithm}{Algorithm}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

\def\SPSB#1#2{\rlap{\textsuperscript{{#1}}}\SB{#2}}
\def\SP#1{\textsuperscript{#1}}
\def\SB#1{\textsubscript{#1}}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\markboth{}{Visual Autoregressive Transformers for Architecture-Agnostic Federated MRI Reconstruction}
\begin{document}
\title{Visual Autoregressive Transformers \\for Architecture-Agnostic\\ Federated MRI Reconstruction}
\author{Valiyeh A. Nezhad, Gokberk Elmas, Bilal Kabas, Fuat Arslan, and Tolga \c{C}ukur$^*$, \IEEEmembership{Senior Member} 
\\
\thanks{\\
This study was supported in part by a TUBA GEBIP 2015 fellowship, and a BAGEP 2017 fellowship (Corresponding author: Tolga Çukur).}
\thanks{V.A. Nezhad, G. Elmas, B. Kabas, F. Arslan, and T. Çukur are with the Department of Electrical and Electronics Engineering, and the National Magnetic Resonance Research Center, Bilkent University, Ankara, Turkey (e-mail: \{valiyeh.ansarian@, gokberk@ee., bilal.kabas@, fuat.arslan@, cukur@ee.\}bilkent.edu.tr).}
}

\maketitle


\begin{abstract}
Although learning-based models offer significant performance benefits in MRI reconstruction, local models built on single-site data show limited generalization to rare features in their training sets. This has sparked interest in collaborative model training on multi-site datasets, via federated learning (FL), an emergent framework that facilitates collaborations via aggregation of models instead of imaging data. Conventional FL forms a global model via cross-site averaging of local models, restricting all sites to use the same model architecture. Recent personalization and distillation approaches partly alleviate this restriction, but they instead suffer from suboptimal knowledge transfer when architectural differences grow among sites. Thus, current FL methods have limited utility for flexible collaborations where individual sites prefer distinct architectures due to differences in computational resources or reconstruction tasks. Here, we introduce a novel architecture-agnostic FL technique, FedVAT, based on visual autoregressive transformers. In FedVAT, a global MRI prior is collaboratively trained to capture the distribution of multi-site data. To do this, we propose a novel site-prompted VAT prior that efficiently synthesizes high-fidelity MR images via next-spatial-scale prediction. Afterwards, each site trains its local reconstruction model of a desired architecture, on local MRI data and VAT-generated synthetic MRI data for other sites. Comprehensive experiments on multi-institutional datasets clearly demonstrate that FedVAT enables flexible collaborations and outperforms state-of-the-art FL baselines in generalization. 
\end{abstract}

\begin{IEEEkeywords}
MRI, reconstruction, federated learning, architecture agnostic, autoregressive, transformer. 
\end{IEEEkeywords}


\bstctlcite{IEEEexample:BSTcontrol}

\section{Introduction}

\IEEEPARstart{M}{agnetic} Resonance Imaging (MRI) is a widely-used non-invasive imaging technique renowned for its exceptional contrast and detailed visualization of soft tissues, making it indispensable in clinical diagnostics. However, one of the major limitations of conventional MRI is its inherently long acquisition time, which can impede clinical workflows and patient comfort \cite{Lustig2007, Zhao2015}. To address this challenge, various accelerated MRI methods have been developed, utilizing undersampling techniques and advanced reconstruction algorithms to reduce scan times without sacrificing image quality \cite{Wang2016, Hammernik2017, Kwon2017, Dar2017, Han2018a, ADMM-CSNET, raki, Xiang2019, wang2022}.
Deep learning-based approaches have emerged as particularly promising for this task, enabling the reconstruction of high-quality images from undersampled data by learning complex mappings that effectively mitigate aliasing artifacts \cite{Schlemper2017, MoDl, Quan2018c, KikiNet, Mardani2019b, Polakjointvvn2020, rgan, FengNNLS2021, PatelRecon}. However, the development of robust models is hindered by the scarcity of high-quality training data and the privacy concerns surrounding medical data, which limit the availability of diverse datasets across institutions \cite{LiangSPM, kaissis2020secure}. 
on one hand, on the other handDeep network models have gained prominence in accelerated MRI reconstruction given their high image quality2-16. Yet, deep models have limited representation for rare features in their training sets; so models trained on single-site data characteristically yield poor across-site generalization17-18. Thus, collaborative approaches for training reconstruction models on multi-site data are direly n

to this conundrum
A promising solution to these limitations is federated learning (FL), which enables collaborative model training across sites without transferring imaging data \cite{WenqiLi2019, Sheller2019, Rieke2020, Roth2020, Li2020, Liu2021}. In conventional FL, each site trains a local model on its own data, which is then sent to an FL server that aggregates these models into a global one through weight-averaging (FedAvg \cite{McMahan2017CommunicationEfficientLO}). This framework supports collaboration, minimizes privacy risks, and reduces maintenance costs, making it advantageous for multi-institutional training. However, the weight-averaging process in FL methods like FedAvg requires identical network architectures across sites, limiting their practicality and flexibility, especially for institutions with different computational resources and model preferences \cite{Knoll2019inverseGANs, Li2020}.


To address this issue, personalized FL approaches such as partial network sharing and neural architecture search (NAS) have been proposed. Partial network sharing allows sites to retain unique model components while sharing common layers \cite{guo2021, dalmaz2024one}. NAS methods adapt model architectures to local data characteristics by selecting optimal structures from a predefined set. While these methods offer flexibility, they come with trade-offs, including limited model selection freedom and reliance on differentiable search strategies that may restrict model diversity \cite{wu2023generalizable, zhang2024robmednas}. Despite these advances, achieving true architectural freedom across sites without compromising FL benefits remains a challenge \cite{Knoll2019inverseGANs, Li2020}.

Here, we propose an architecture-agnostic federated visual autoregressive (FedVAT) framework for accelerated MRI reconstruction, leveraging generative modeling to address data scarcity and enhance model generalization across sites. FedVAT operates in two phases: learning a cross-site MRI prior (Fig. \ref{fig:FedVAT_gen}) and developing site-specific reconstruction models (Fig. \ref{fig:FedVAT_recon}). In the first phase, a Visual Autoregressive Transformer (VAT) \cite{tian2024visual} is trained federatively, with each site contributing local data and weights being aggregated centrally to form a global model. In the second phase, each site uses the global VAT model to generate high-quality synthetic data, which is combined with local data to fine-tune the pre-trained reconstruction model. This fine-tuning adapts the model to both local and cross-site variations, optimizing performance and preventing catastrophic forgetting.
By integrating these steps, FedVAT offers a robust, adaptable solution for multi-site MRI reconstruction. 


\vspace{0.5cm} \subsubsection*{\textbf{Contributions}} \begin{itemize} \item We introduce the first architecture-agnostic FL framework to enhance generalization of MR images reconstruction across multiple sites.
\item We leverage a federatively trained VAT model, a visual generative framework using multi-scale autoregressive next-scale prediction, guided by a site prompt to adapt and capture each site's distinct data distribution.
\item We develop a two-stage training strategy: initial training of site-specific reconstruction models on local data, followed by fine-tuning with a mix of local and cross-site synthetic data from the federatively trained VAT model. This approach mitigates model forgetting and enhances generalization across diverse datasets without requiring uniform architectures across sites.

\end{itemize}

\begin{figure*}[t]
\centering
\includegraphics[width=0.875\textwidth]{Figure01_V5.png}
\captionsetup{justification=justified, width=\textwidth}
\caption{ FedVAT performs FL of a visual autoregressive (VAT)  prior for MRI reconstruction. The VAT model, \(\text{VAT}_{\theta_{\text{VAT}}}\), serves as a conditional generator that synthesizes high-quality MR images conditioned on site-prompt $\texttt{sp}$, representing the site index \(k\). During each communication round, each local site \(k \in \{1, \ldots, K\}\) updates the shared model parameters \(\theta_{\text{VAT}}\) by minimizing a local synthesis loss \(\mathcal{L}^k\). The server aggregates these updates using weighted averaging to refine the global generative model across all sites.}
\label{fig:FedVAT_gen}
\vspace{-0.1cm}
\end{figure*}



\section{Related Work}

\subsection{Deep Learning-Based MRI Reconstruction}
Deep learning has become the \textit{de facto} standard for MRI reconstruction, offering high-quality imaging by learning complex transformations from undersampled data to fully sampled images \cite{Hammernik2017}. Despite their effectiveness, these models require substantial amounts of training data, which can be challenging to obtain due to the high costs and limited availability of diverse cases \cite{data_diff, GuoTMI2021}. This data scarcity limits the models' ability to generalize effectively, particularly when faced with rare features or pathologies that are underrepresented in individual datasets \cite{dalmaz2024one, fastMRI}. Solutions such as unpaired learning \cite{Quan2018c, oh2020, lei2020}, self-supervised learning \cite{yaman2020, Huang2019self, Tamir2019, hu2021, FengLiu2021, wang2022b}, semi-supervised learning \cite{yurt2022semi}, and transfer learning \cite{Dar2017, KnollGeneralization}, have been explored to mitigate data limitations, but these approaches often require centralized training that incurs significant storage costs and raises privacy concerns \cite{Kaissis2020}.

\subsection{Federated Learning for Multi-Institutional MRI Reconstruction}
Federated learning (FL) is a collaborative approach to training models across multiple institutions, enhancing data privacy by enabling decentralized training through the exchange of model parameters without cross-site data transfer \cite{Li2020FederatedLC, Kaissis2020}. This framework has proven particularly beneficial for medical imaging tasks, including segmentation and classification, where strict data privacy regulations and patient confidentiality are critical \cite{Sheller2019, WenqiLi2019, Roth2020}.

Conventional FL approaches, such as Federated Averaging (FedAvg) \cite{McMahan2017CommunicationEfficientLO}, aggregate locally trained models into a global model but require uniform model architectures across all participating sites, limiting flexibility in accommodating diverse hardware capabilities and institutional preferences \cite{Knoll2019inverseGANs, Li2020}. To address these limitations, methods like FedProx \cite{li2020federated} introduce a proximal term to improve model convergence in settings with non-IID (non-identically distributed) data, while FedDistill \cite{zhu2021data} leverages knowledge distillation to enhance model robustness and reduce communication costs. However, these strategies alone may not fully address the challenges posed by heterogeneous data distributions and the need for adaptable architectures across institutions.


\subsection{Model-Agnostic MRI Reconstruction Approaches}
To address the challenges posed by diverse data distributions and non-uniform model architectures, adaptive MRI priors and personalized FL strategies have been explored. For example, \cite{korkmaz2022unsupervised} proposed an adaptive MRI prior optimized by removing the mapper during inference, eliminating site-specific indexing. FedGIMP \cite{elmas2022federated} leverages a generative MRI prior with site-specific adaptations, improving reconstruction across heterogeneous datasets. Additionally, partial network sharing has been utilized for MRI synthesis, demonstrating improved generalization \cite{dalmaz2024one}.

Neural architecture search (NAS) has also been explored to tailor models to local data characteristics \cite{wu2023generalizable, zhang2024robmednas}. However, NAS often relies on predefined architecture sets, limiting flexibility. Architecture-agnostic approaches, on the other hand, support collaboration across sites with diverse models and data distributions.

We propose FedVAT, an architecture-agnostic FL framework that incorporates a visual autoregressive  model to generate synthetic MRI data, enhancing generalization across sites without requiring uniform architecture constraints. Unlike GANs that rely on adversarial training to indirectly learn data distributions \cite{Knoll2019inverseGANs, liu2020rare, korkmaz2022unsupervised}, 
FedVAT leverages the VAT model's unique refinement mechanism, which progressively improves outputs by revisiting and correcting past steps. This process, inspired by how humans refine paintings, allows VAT to fix earlier errors. This iterative refinement is key to VAT’s success in generating high-quality results. In comparison to Denoising Diffusion Probabilistic Models (DDPMs), which require numerous iterative steps to refine images \cite{ho2020denoising, gungor2023adaptive}, FedVAT achieves high-quality image generation more efficiently. By integrating VAT-generated synthetic data with local training, FedVAT adapts to diverse data distributions, providing a flexible, privacy-preserving framework that supports robust MRI reconstruction across different clinical sites without extensive retraining.



\section{Theory}

\subsection{Federated Training of MRI Priors}
Developing robust deep generative models relies heavily on diverse, high-quality data. In medical imaging, data availability is often limited, posing challenges for model training. Traditional centralized approaches involve transferring data from multiple institutions into a shared repository \cite{kaissis2020secure}. Although effective, this method raises significant privacy concerns, as sharing raw data across institutions can lead to information leakage \cite{FedGAN}.

FL offers a solution by enabling collaborative training without sharing raw data \cite{Li2020FederatedLC}. In FL, each training round begins with FL server transmitting the global generative model $G_{\theta}$ to participating sites. Local copies $G^k_{\theta}$ are initialized ($\theta^k_{\text{G}} \leftarrow \theta_{\text{G}}$) and trained on local datasets $D_k$. After training, the updated parameters $\theta^k$ are returned to the server, which aggregates them using Federated Averaging (FedAvg) \cite{McMahan2017CommunicationEfficientLO}:

\begin{equation}
\label{eq:fed_avg}
\theta_{\text{G}} = \sum_{k=1}^{K} \theta^k.
\end{equation}


\subsection{Federated Architecture-Agnostic MRI Reconstruction}
Accelerated MRI reconstruction aims to recover a high-quality MR image $x$ from undersampled k-space data $y$:
\begin{equation}
\label{eq:sampling_VAT}
A x = y,
\end{equation}

where \( A \) is the imaging operator. To address the underdetermined nature of this problem, additional priors are essential:
\begin{equation}
\label{eq:regularized_recon_VAT}
\widehat{x} = \underset{x}{\operatorname{argmin}} \| y - A x \|_{2}^{2} + I(x),
\end{equation}
where \( I(x) \) is a regularization term incorporating image priors \cite{Lustig2007}. Deep learning models have gained prominence for solving Eq. \ref{eq:regularized_recon_VAT} by learning data-driven priors from large datasets \cite{Hammernik2017}.

 Within FL framework, MRI reconstruction models \( H_{\phi^k}^k \) are trained iteratively at each participating site using local data. Each site minimizes a reconstruction loss defined as:
\begin{equation}
\label{eq:fed_dnn_training_VAT}
\mathcal{L}^{k}_{rec}(\mathcal{D}^k, A^k_{tr}; \phi^k) = \mathbb{E}_{(x^{k}_{tr}, y^{k}_{tr})} \left[ \| m^{k}_{tr} - H_{\phi^k}(A^{\dagger k}_{tr} y^{k}_{tr}) \|_{2} \right],
\end{equation}
where \( \mathbb{E} \) denotes the expectation, \( \mathcal{D}^k \) represents the local training data, \( y^k_{tr} \) are the undersampled acquisitions, and \( x^k_{tr} \) are the corresponding fully-sampled reference images. \( A^k_{tr} \) and \( A^{\dagger k}_{tr} \) are the imaging operator and its adjoint, respectively.




\subsection{Problem Definition}
Traditional FL approaches rely on cross-site averaging of local models, which requires all sites to use the same model architecture. This constraint limits the applicability of FL in scenarios where sites have different computational resources or require specific architectures tailored to their data and tasks. The need for diverse architectures becomes even more crucial when dealing with data scarcity, such as in cases of rare diseases, where developing robust models that generalize well is challenging. To address these issues, it is necessary to develop strategies that allow for collaboration among sites using diverse architectures, enabling effective knowledge sharing without enforcing uniformity across models.





\subsection{Federated Visual Autoregressive Generative Learning for MRI Reconstruction}
To enhance generalization across different sites while balancing the trade-off with within-site performance, we propose a novel method, FedVAT, a federated visual autoregressive generative approach for MRI reconstruction. In FedVAT, each site collaboratively contributes to learning a shared generative prior that models the overall MRI data distribution across all sites. This prior is flexible enough to adapt to the specific characteristics of each site’s data (Fig. \ref{fig:FedVAT_gen}). Then, the mixture of cross-site synthetic MRI data, generated from the global MRI prior, and local data is employed to fine-tune site-specific pre-trained reconstruction models on the local data (Fig. \ref{fig:FedVAT_recon}). The details of the proposed federated approach and the training procedure are outlined below.

\subsubsection{Autoregressive Generative Prior}
Autoregressive models have become the \textit{de facto} standard for generative modeling in natural language processing \cite{radford2019language}  and are gaining attention in computer vision applications\cite{luo2023bayesian }. Inspired by this, we leveraged a Visual Autoregressive Transformer (VAT) model for scalable resolution reconstruction to generate high-quality synthetic MRI data. In VAT, an autoencoder first encodes MR images into discrete token representations at multiple spatial scales. These quantized token maps serve as the ground truth for training the transformer. The transformer then learns to autoregressively generate the next higher-resolution token map based on all previous token maps, progressively refining the image across scales. This multi-scale, coarse-to-fine strategy mirrors how humans perceive and create images—capturing global features at lower resolutions and refining finer details at higher resolutions—making it natural to predict multiple tokens simultaneously for efficient and realistic image generation. To facilitate collaboration across multiple sites while maintaining data privacy, we incorporated VAT into a FL framework. This framework leverages an autoregressive process across $S$ spatial scales and employs a learnable site-prompt $\texttt{sp}$ to preserve site-specific attributes, enabling the model to adapt to different data distributions without sharing raw data between sites.


Given that the choice of tokenizer is crucial for determining the quality of image generation, VAT’s multi-scale approach strategically employs a modified Vector Quantized Variational Autoencoder (VQ-VAE)\cite{van2017neural} to encode MR images into discrete latent tokens across multiple scales, enabling the model to employ self-supervised learning for next-token prediction. In the VAT model, \textit{"right-shifted"} interpolations are used to produce teacher-forced training inputs, enabling the model to scale up token maps during training and inference in the VQ embedding space. During training, the model interpolates a smaller token map (e.g., $1 \times 1$) to a larger one (e.g., $2 \times 2$) to serve as input for the next autoregressive step, ensuring that the input comes from the continuous embedding space of the previous scale. The transformer then takes this interpolated token map as input and predicts the token distributions for the next scale, leveraging its attention mechanism to incorporate context from all prior scales. In inference, the transformer processes similarly interpolated predicted embeddings from one scale (e.g., $2 \times 2$) to the next (e.g., $3 \times 3$) to generate tokens for further scales.

 
 Given a coil-combined complex MR image $ x \in \mathbb{R}^{H \times W} $, the VQ-VAE encoder first extracts feature maps $ \psi_s \in \mathbb{R}^{h_s \times w_s \times C} $ at each scale $s$, where $ h_s \times w_s $ corresponds to the spatial resolution, and $C$ is the dimensionality of the feature space. These feature maps are then quantized to produce token maps $f_s$. The quantization process at each scale $s$ is defined as:
\begin{equation}
 f_s = \underset{v \in [V]}{\text{argmin}} \, \lVert \text{lookup}(\mathcal{C}, v) - \psi_s \rVert^2,   
\end{equation}

where \( \mathcal{C} \in \mathbb{R}^{V \times C} \) is a shared codebook that maps the continuous feature vectors \( \psi_s \) to discrete tokens \( f_s \). The resulting token map \( f_s \in [V]^{h_s \times w_s} \) provides a compact representation of the MR image at the corresponding scale.

After generating token maps with VQ-VAE, the transformer $\mathcal{T}_{\theta_{\text{VAT}}}$ learns to predict the next-scale map $\hat{f}_{s+1}$ based on the current and previous token maps $f_{\leq s}$ and $\texttt{sp}$:
\begin{equation}
\hat{f}_{s+1} = \mathcal{T}_s(f_{\leq s}, \texttt{sp}),
\end{equation}
where $\mathcal{T}_s$ represents the transformer at scale $s$.

In the self-attention layers of $\mathcal{T}_s$, $f_{\leq s}$ and $\texttt{sp}$ are linearly projected into query, key, and value representations:

\begin{equation}
\text{Query} = \text{lin}_q([\texttt{sp}, f_{\leq s}]),
\end{equation}
\begin{equation}
\text{Key} = \text{lin}_k([\texttt{sp}, f_{\leq s}]),
\end{equation}
\begin{equation}
\text{Value} = \text{lin}_v([\texttt{sp}, f_{\leq s}]).
\end{equation}

The self-attention mechanism is computed as:
\begin{equation}
\text{Attention}(Q, K, V) = \text{Softmax}\left( \frac{Q K^T}{\sqrt{d_k}} \right) V,
\end{equation}
where $d_k$ is the dimensionality of the key vectors, allowing the transformer to capture both spatial and contextual dependencies within each scale and between scales.To stabilize training, the queries and keys are normalized to unit length before computing the attention scores.

For normalization, site-level refinement is achieved via Adaptive Layer Normalization (AdaLN), which is conditioned by the site-prompt:
\begin{equation}
\text{AdaLN}(h, \texttt{sp}) = \gamma(\texttt{sp}) \cdot \frac{h - \mu(h)}{\sigma(h)} + \beta(\texttt{sp}),
\end{equation}
where $h$ represents the hidden representations, $\mu(h)$ and $\sigma(h)$ are the mean and standard deviation of $h$, and $\gamma(\texttt{sp})$ and $\beta(\texttt{sp})$ are learnable parameters.

After the highest-resolution token map $f_S$ is refined, the VQ-VAE decoder reconstructs the final MR image:
\begin{equation}
\hat{x} = \text{VQ-VAE}_S^{-1}(\hat{f}_S),
\end{equation}
where $\hat{x}$ is the synthesized MR image, ensuring global consistency and fine structural details.

The learning objective at site $k \in \{1, \ldots, K\}$ is defined as:
\begin{equation}
\mathcal{L}_{\text{VAT}}^k = \| x^k - \hat{x}^k \|_2^2 + \sum_{s=1}^{S} \| f_s^k - \hat{f}_s^k \|_2^2 - \lambda \sum_{j=1}^{K} \mathbf{1}(x^k, j) \cdot \log(\texttt{sp}^k),
\end{equation}
where $x^k$ is the coil-combined complex MR image, $f_s^k$ is the token map at scale $s$, and $\mathbf{1}(x^k, j)$ is a one-hot indicator for the site $j$ from which $x^k$ was collected. The loss components ensure image consistency, feature map consistency, and effective site-prompt training.f


\begin{algorithm}[t]
\small
\caption{Training the VAT prior}\label{alg:fedVAT_training}
\KwInput {
    $\mathcal{D}=\{\mathcal{D}^1,...,\mathcal{D}^K\}$: datasets from $K$ sites. \\
    $L$: number of communication rounds.\\ 
    $I$: number of local epochs. \\
    \{$\text{VAT}^1,...,\text{VAT}^K$\}: local VAT models with parameters $\{\theta_{\text{VAT}}^1,...,\theta_{\text{VAT}}^K\}$. \\
    $\theta^{glo}_{\text{VAT}}$: global VAT model with parameters $\theta^{glo}_{\text{VAT}}$. \\
    $Opt()$: optimizer for parameter updates. \\
}
\KwOutput{Trained global VAT model with parameters $\theta^{glo}_{\text{VAT}}$.} 
Initialize global model parameters $\theta^{glo}_{\text{VAT}}$.\\
\For{$l = 1:L$}{
 \tcp{Locally train VAT model at site $k$} 
    \For{$k = 1:K$}{
        $\theta_{\text{VAT}}^k \gets \theta^{glo}_{\text{VAT}}$\\
        \For{$i = 1:I$}{
            $\nabla_{\theta_{\text{VAT}}^k} \mathcal{L}^k_{\text{VAT}}(\theta_{\text{VAT}}^k)$ \\
            $\theta_{\text{VAT}}^k \gets \theta_{\text{VAT}}^k - Opt(\nabla_{\theta_{\text{VAT}}^k} \mathcal{L}^k_{\text{VAT}})$ \\
        }
    }
    $\theta^{glo}_{\text{VAT}} \gets \sum_{k=1}^{K} \theta_{\text{VAT}}^k$ \\
}
\Return $\theta^{glo}_{\text{VAT}}$
\end{algorithm}

\begin{figure*}[t]
\centering
\includegraphics[width=0.875\textwidth]{Figure_V1.png}
\captionsetup{justification=justified, width=\textwidth}
\caption{\color{red}NOTATIONS\color{black} 
To generate the token map $f_s$ during the $s$-th autoregressive step (e.g., $f_2$), the process begins with the previous token map $f_{s-1}$. The most recent token map, $f_{s-1}$, is reshaped into a 2D structure (e.g., $1 \times 1$ for $f_1$), embedded into a 2D feature map, and upsampled to match the spatial dimensions of $f_s$ (e.g., $2 \times 2$ for $f_2$). The upsampled feature map is then projected into the hidden dimension of the VAT transformer. A 2D positional embedding is added, resulting in the input embedding $e_s$. Finally, the transformer processes $e_s$ alongside the previous tokens to generate $f_s$ .
}
\label{fig:VAT_arch}
\vspace{-0.1cm}
\end{figure*}


\subsubsection{Training of the Federated Generative Model} 
FedVAT employs FL to train a global MRI prior $\text{VAT}^{glo}$ across multiple sites. Each site $k$ maintains a local copy of the model $\text{VAT}$, while a FL server coordinates the training over \( L \) communication rounds by aggregating model parameters (Alg. \ref{alg:fedVAT_training}). 

Initially, the global VAT model is randomly initialized. At the start of each communication round, the global model parameters are distributed to all sites to initialize the local models:
\begin{equation}
\theta^k_{\text{VAT}} \gets \theta_{\text{VAT}}^{glo}, \quad \forall k \in \{1, 2, \ldots, K\}.
\end{equation}

Each site $k$ trains its local model $ \text{VAT}^k$ using its local dataset $ \mathcal{D}^k $ over $n_g$ epochs. In the training process site-specific information \texttt{sp} integrate into the token embedding sequence. This token helps adapt the generative process to the characteristics of the specific site. The image tokens $f_{\leq s}$ from previous scales are concatenated with the site-prompt token and fed into the transformer, enabling the model to condition its generation on both spatial context and site-specific attributes.

After completing the local training, each site $k$ sends its updated model parameters $ \theta^k_{\text{VAT}}$ to the FL server. The server aggregates these parameters into the global prior:
\begin{equation}
\theta_{\text{VAT}}^{glo} = \sum_{k=1}^{K} \theta^k_{\text{VAT}}.
\end{equation}

\subsubsection{Site-Specific Reconstruction Models} 
Following the generative learning phase, FedVAT leverages its model-agnostic framework to train site-specific reconstruction models \( H^k_{\phi} \) for each site \( k \) (Alg. \ref{alg:site_specific_recon}). This server-free, model-agnostic topology enables each site to independently choose the most suitable reconstruction model architecture for its local data, promoting flexibility and adaptability to diverse data distributions. The objective of these models is to accurately reconstruct high-quality MR images \( \hat{x}^k \) from the undersampled k-space data \( y^k_{us} \) collected at each site.

The reconstruction process consists of two stages: initial training with within-site local data, followed by fine-tuning using a combination of local and cross-site synthetic data.

In the first stage, the reconstruction model \( H^k_{\phi} \) is trained using the local MRI data for \( n_r \) epochs to learn the site-specific characteristics:
\begin{equation}
\hat{\phi}^k := \arg \min_{\phi^k} \mathbb{E}_{p(x^k_{ref}, x^k_{us}, y^k_{us})} \left\{ \| x^k_{ref} - H^k_{\phi}(x^k_{us}, y^k_{us}) \|^2 \right\},
\end{equation}
where \( x^k_{ref} \) is the fully-sampled reference image, and \( x^k_{us} \) is the zero-filled reconstruction obtained by applying the inverse Fourier transform \( \mathcal{F}^{-1} \) to the undersampled k-space data \( y^k_{us} \). This phase ensures that the model captures patterns and structures specific to the local data.

In the second stage, each reconstruction model $H^k_{\phi}$ is fine-tuned using both local data and synthetic data generated by the global VAT model. The synthetic data $x^{k}_{syn}=\text{VQ-VAE}_S^{-1}(\hat{f}_S)$ is created using the site-prompt token $\texttt{sp}$.


Fine-tuning over $n_s$ epochs involves incorporating both data types to enhance generalization across sites:
\begin{align}
X_{ref} &= \{X^k_{ref} \cup X^k_{\text{syn}}\}, \\
X_{us} &= \{X^k_{us} \cup X^k_{\text{syn,us}}\}, \\
Y_{us} &= \{Y^k_{us} \cup Y^k_{\text{syn,us}}\},
\end{align}
where \( X_{ref} \), \( X_{us} \), and \( Y_{us} \) denote the combined sets of reference images, zero-filled reconstructions, and undersampled acquisitions, respectively.

The fine-tuning objective at site \( k \) becomes:
\begin{align}
\hat{\phi}^k := \arg \min_{\phi^k} & \; \mathbb{E}_{p(X_{ref}, X_{us}, Y_{us}[i])} \Bigg\{ \\
& \| X_{ref}[i] - H^k_{\phi}(X_{us}[i], Y_{us}[i]) \|^2 \Bigg\}.
\end{align}
By fine-tuning the reconstruction models with both local and synthetic data, each site achieves optimal reconstruction performance while preserving the flexibility to use a model architecture that best fits its data.
%\begin{equation}
%\hat{\phi}^k := \arg \min_{\phi^k} \mathbb{E}_{p(X_{ref}, X_{us}, Y_{us}[i])} \Bigg\{ 
%\| X_{ref}[i] - H^k_{\phi}(X_{us}[i], Y_{us}%[i]) \|^2 \Bigg\}.
%\end{equation}




\begin{algorithm}[t]\small
\caption{Training  of reconstruction models}\label{alg:site_specific_recon}

\KwData{
    $(x^k_{ref}, x^{k}_{us}, y^{k}_{us})$: local MRI data from site $k$.\\
    $n_r$: number of epochs for initial training of reconstruction models on local data. \\
    $n_s$: number of epochs for fine-tuning reconstruction models with mixed data. \\
    $H^k$: site-specific reconstruction model with parameters $\phi^k$. \\
    $\text{VAT}$: global generative model with parameters $\theta_{\text{VAT}}^{glo}$. \\
    \texttt{Opt()}: optimizer for parameter updates.\\
}
\KwResult{
   $H^k_{{\phi}^k}$: fine-tuned site-specific reconstruction models.
}

Initialize parameters $\phi^k$ for site $k$.

\tcp{Initial Training with Local Data}
\For{$e = 1$ \textbf{to} $n_r$}{
    $\phi^k \leftarrow \phi^k - \texttt{Opt}(\nabla_{\phi^k} \mathbb{E} \left[ \|x_{ref}^{k} - H^k_{\phi^k}(x^{k}_{us}, y^{k}_{us}) \|^2 \right])$ 
}

\tcp{Fine-Tuning with Mixed Data}
\For{$j = 1$ \textbf{to} $K$}{
    Generate synthetic data $X^j_{\text{syn}} =\text{VQ-VAE}_S^{-1}(\hat{f}_S)$; \\
    Simulate $X^j_{\text{syn,us}}$ and $Y^j_{\text{syn,us}}$ for undersampled acquisitions; \\
}

\For{$e = 1$ \textbf{to} $n_s$}{
    $j = \mathrm{mod}(e, K) + 1$; \\
    \tcp{\textit{Mix local data from site $k$ with synthetic data from site $j$}} 
    $X_{ref} \leftarrow X^k_{ref} \cup X^j_{\text{syn}}$; \\
    $X_{us} \leftarrow X^k_{us} \cup X^j_{\text{syn,us}}$; \\
    $Y_{us} \leftarrow Y^k_{us} \cup Y^j_{\text{syn,us}}$; \\
    $\phi^k \leftarrow \phi^k - \texttt{Opt}(\nabla_{\phi^k} \mathbb{E} \left[ \|X_{ref}[i] - H^k_{\phi^k}(X_{us}[i], Y_{us}[i]) \|^2 \right])$ 
}

\Return{$H^k_{{\phi}^k}$}
\end{algorithm}

\begin{figure}[H] % Single-column figure for one side
\centering
\includegraphics[width=0.49\textwidth]{Figure02_V5.png} % Adjust width as necessary for column size
\caption{Overview of the FedVAT MRI reconstruction framework, comprising two primary stages at site $K$: (a) Initial training of site-specific reconstruction models $H^k_{\phi}$ using local MRI data from each site $k$, performed over $n_r$ epochs to capture the unique characteristics of each site's dataset. (b) Fine-tuning phase where each model $H^k_{\phi}$ is further refined using a combination of local data and synthetic data $x^k_{\text{syn}}$ generated by the global generative model. This stage, conducted over $n_s$ epochs, ensures generalization across different sites while retaining site-specific performance. The process utilizes both local and cross-site data, optimizing model parameters for robust and high-quality MRI reconstruction suitable for diverse clinical environments.}
\label{fig:FedVAT_recon}
\vspace{-0.2cm}
\end{figure}

\section{Methods}
\subsection{Architectural Details}
The VAT model is a decoder-based architecture inspired by GPT2-like structures, specifically designed for autoregressive image generation. The process begins with input tokens generated by a multi-scale VQ-VAE encoder, which encodes input images into feature embeddings of size $D = 32$. The VQ-VAE encoder consists of \textbf{5 stages}, with \textbf{2 residual blocks per stage} and attention applied in stages 4--5 and the mid module. Downsampling is performed using stride-2 convolutions. The encoder output has a shape of $B \times C_{\text{vae}} \times H' \times W'$, where $C_{\text{vae}} = 32$ and $H', W'$ depend on the image size and downsampling factor. These embeddings are quantized using a shared codebook of size $V = 4096$, ensuring consistent quantization across all scales.

The interpolation process, based on a pretrained VQ-VAE encoder, handles the multi-scale nature of token maps. During training and inference, token maps from one scale (e.g., $1 \times 1$) are interpolated to the next scale (e.g., $2 \times 2$) in the embedding space. This preserves contextual information, ensuring consistency across scales and enabling the model to progressively refine details.

The model processes $S = 10$ scales, corresponding to progressively larger patch sizes: $1, 2, 3, 4, 5, 6, 8, 10, 13$, and $16$, capturing varying levels of image detail and contextual information.

Tokens are passed through a linear projection layer that maintains the embedding size $D = 32$, followed by flattening and concatenation with a site-specific prompt token at the beginning before being fed into the transformer. The transformer operates causally, meaning each token can only attend to previous tokens, ensuring an autoregressive sequence. The architecture comprises \textbf{16 sequential blocks}, each incorporating a multi-head self-attention mechanism that enables tokens to attend to all preceding tokens, capturing complex dependencies across the input. Adaptive Layer Normalization (AdaLN) follows the attention mechanism, dynamically adjusting scale and shift parameters based on conditioning input, guided by the site-specific prompt token. A residual link connects the output to ensure stable gradient propagation. Each block also features a feed-forward network (FFN) with two linear layers and an intermediate GELU activation, enhancing representational power.

Hierarchical positional embeddings are integrated with input tokens to preserve spatial structure throughout the model. The final transformer's output is passed through an output projection layer, mapping it to logits. These logits are converted into probabilities for next-token prediction using a softmax function.

The VQ-VAE decoder consists of \textbf{5 stages} with \textbf{13 residual blocks in total}, 2--3 per stage, and attention applied in stages 4--5 and the mid module. Upsampling is performed using transposed convolutions, and the decoder ensures the final output matches the input shape of $B \times 3 \times H \times W$, preserving the learned structure and dependencies captured throughout the VAT model.

For training, we used \textbf{two RTX 4090 GPUs}, a base learning rate of $10^{-4}$, and the AdamW optimizer with $\beta_1 = 0.9$, $\beta_2 = 0.95$, and a weight decay of 0.05. The model was trained for \textbf{200 epochs} with a communication round of 1 and $\lambda = 0.0015$.



\subsection{Experimental Setup}
To train VAT in a federated manner, we deployed a copy of the VAT model at each participating site. Each model was initialized using a pre-trained version sourced from the official GitHub repository\footnote{\url{https://github.com/FoundationVision/VAR}} to ensure uniform starting conditions and leverage previously learned representations. The federated training process aggregated local updates into a global model at a FL server, maintaining data privacy while facilitating collaborative training.

To evaluate FedVAT's performance and generalization capability, we conducted experiments using the following three MRI reconstruction architectures with hyperparameters carefully tuned to optimize the performance of each model, chosen to represent different reconstruction strategies:

\underline{\textbf{MoDL}} \cite{aggarwal2018modl}: A conditional unrolled model with a learning rate of \(10^{-4}\), trained over 200 epochs, with fine-tuning commencing at epoch 100.

\underline{\textbf{rGAN}} \cite{rgan}: A conditional GAN model trained with a learning rate of \(2 \times 10^{-4}\) over 200 epochs, with fine-tuning starting at epoch 100. The adversarial and pixel-wise loss function weights were set at (1, 100).

\underline{\textbf{D5C5}} \cite{Schlemper2017}: An unrolled model configured with a learning rate of \(2 \times 10^{-4}\) and trained over 100 epochs, with fine-tuning commencing at epoch 50.

The experimental design included two main configurations:
\begin{itemize}
    \item \textbf{Homogeneous Setup}: All sites used the MoDL-3 cascades, but each site trained on different datasets. This setup allowed us to assess the FedVAT prior's performance when sites shared the same architecture but varied in data distribution.
     \item \textbf{Heterogeneous Setup}: Each site utilized its desired reconstruction model (MoDL, rGAN, or D5C5), simulating realistic scenarios where institutions deploy different architectures based on expertise and resource availability. This setup demonstrated FedVAT's ability to support collaborative training while maintaining architectural diversity.
\end{itemize}


\subsection{Competing Methods}
To evaluate the performance of FedVAT, we compared it against non-federated (Single-site) and federated approaches (FedAvg, FedProx, and FedDistill). Hyperparameter selection for each method was performed based on validation performance to ensure a fair comparison. 

All competing methods were run with a consistent set of hyperparameters, including the learning rate, number of epochs for non-federated training, and the number of communication rounds and epochs for federated training. The same architecture per site and learning rate were maintained across methods to provide a uniform evaluation framework.

The competing methods were defined as follows:
\begin{itemize}
    \item \underline{\textbf{Centralized}} \cite{kaissis2020secure}: This method pools data from all sites to train a single model, offering strong performance but compromising privacy due to data sharing.
    \item \underline{\textbf{Single-Site}} \cite{guo2021}: Each site trained its model independently using local data (e.g., IXI, fastMRI, or BraTS) without sharing data or model details. Single-site models serve as benchmarks for site-specific performance but often struggle to generalize to other sites.
    \item \underline{\textbf{FedAvg}} \cite{McMahan2017CommunicationEfficientLO}: 
Each site trains a local model on its data, and their weights are averaged to form a global model. While FedAvg enhances cross-site generalization, it requires all sites to use the same model architecture.

\item \underline{\textbf{FedProx}}: An extension of FedAvg with a proximal term to handle data and system heterogeneity, typically requiring a server and uniform model architectures. To adapt this for our model-agnostic setup, we use FedDistill to transfer knowledge from a shared server model to each site. Each site maintains a server copy and a site-specific reconstruction model. The server copy, pre-trained on local data and updated via the FedProx approach, distills knowledge to the site-specific model in federated rounds.

\item \underline{\textbf{FedDistill}} \cite{zhu2021data}: A method that facilitates knowledge distillation for federated learning, typically used with similar architectures across sites. To adapt FedDistill for our model-agnostic setup, we distill local data to a shared server model. This server model is then used to train site-specific reconstruction models.
\end{itemize}

Strict data consistency was enforced across all reconstructions before reporting performance metrics to ensure objective and fair assessments. The MoDL model  was implemented in TensorFlow, D5C5  in Theano, and rGAN in PyTorch, highlighting the architecture-agnostic nature of FedVAT. All models were executed on a system equipped with NVIDIA RTX 3090 GPUs, ensuring consistent hardware conditions for performance evaluation.









%\begin{table}[t]
%\centering
%\caption{Performance of reconstruction models trained on synthetic images. Across-site generalization (i.e., when a local model is tested on separate sites) in terms of peak signal-to-noise ratio (PSNR, dB) and structural similarity (SSIM, \%). Results listed as mean\(\pm\)std across test subjects, at R=4x, 8x acceleration rates. 
%}
%\resizebox{0.3\textwidth}{!}
%{
%\begin{tabular}{|l l | Sc | Sc |Sc | Sc |}
% \hline
% & &\multicolumn{2}{Sc|}{\textbf{Within-site}} & \multicolumn{2}{Sc|}{\textbf{Across-site}} \\
% \hline
% & &{PSNR $\uparrow$} & {SSIM $\uparrow$} & {PSNR $\uparrow$}  &{SSIM $\uparrow$}  \\
% \hline
% \multirow{2}{*}{{GAN}}  &  4x & \textbf{40.63$\pm$2.80} &  \textbf{96.74$\pm$1.50 }& 39.37$\pm$3.01 & 95.95$\pm$1.97 \\
% & 8x & \textbf{35.58$\pm$2.81}
 %& \textbf{93.67$\pm$2.30}& 34.42$\pm$2.69 & %91.67$\pm$3.08\\
%\cline{1-6}
%\multirow{2}{*}{{DDPM}}&   4x & 39.46$\pm$2.79 & 96.26$\pm$1.54 & 38.96$\pm$2.92 &  94.62$\pm$2.25
 %\\
 %& 8x & 35.28$\pm$2.90 & 93.55$\pm$2.31 & 34.72$\pm$2.76 &92.67$\pm$2.56\\
%\cline{1-6}
%\multirow{2}{*}{{VAT}} &  4x & 40.32$\pm$2.88 & 96.54$\pm$1.51 & \textbf{40.17$\pm$2.83} & \textbf{96.49$\pm$1.65}\\
% & 8x & 34.77$\pm$2.93 & 93.32$\pm$2.38 &% \textbf{35.18$\pm$2.92} & \textbf{93.62$\pm$2.56}\\
% \hline
%\end{tabular}
%}
%\end{table}






\subsection{Experiments}

\subsubsection{Datasets}
Experiments were conducted using four datasets: BraTS \cite{BRATS}, fastMRI \cite{fastMRI}, MIDAS \cite{midas} (refer to the respective references for detailed information), and an in-house dataset collected at Bilkent University (described below). Acquisitions were retrospectively undersampled using a variable-density (VD) pattern with acceleration factors R = 4x and 8x \cite{Lustig2007}. The training, validation, and test sets were designed to be non-overlapping to ensure independent evaluation. The study considered three distinct sites, each with its site-specific dataset. Training data at each site consisted of MR images randomly drawn from a multicontrast set without specific handling procedures for different contrasts. Although training models for individual contrasts could potentially enhance performance, we opted for mixed-contrast training to better represent the diverse, multi-contrast imaging protocols commonly found in clinical practice \cite{KnollGeneralization}.

\underline{\textit{In-House:}} Data were collected from 10 subjects on a 3T Siemens Tim Trio scanner at Bilkent University using a 32-channel coil. T$_{1}$-weighted images were obtained using an MP-RAGE sequence with TE/TI/TR of 3.87/1100/2000 ms and a 20$^\circ$ flip angle. T$_{2}$- and PD-weighted images were acquired using an FSE sequence with TE$_{\text{PD}}$/TE$_{T2}$/TR of 12/118/1000 ms and a 90$^\circ$ flip angle. All scans had a field-of-view of 192x256x176 mm$^{3}$ and a voxel size of 1x1x2 mm$^{3}$. Ethical approval was obtained, and participants provided written informed consent.


For single-coil experiments, we used coil-combined magnitude images from BraTS, fastMRI, and MIDAS, excluding the channel dimension. The datasets included: T$_{1}$-, T$_{2}$-, PD-weighted images in BraTS; T$_{1}c$-, T$_{2}$-, FLAIR-weighted images in fastMRI; and T$_{1}$-, T$_{2}$-weighted images in MIDAS. Multi-contrast data from 40, 5, and 10 subjects were allocated for training, validation, and testing, with 21 cross-sections per contrast per subject, resulting in 840, 105, and 210 cross-sections per set.

For multi-coil experiments, we utilized fastMRI brain, fastMRI knee, and the in-house brain dataset, all containing multi-coil k-space data. For fastMRI brain, T$_{1}$-, T$_{2}$-, and FLAIR-weighted acquisitions from 36, 6, and 18 subjects were reserved for training, validation, and testing, respectively, with 8 cross-sections per contrast per subject. For fastMRI knee, PD- and PDFS-weighted acquisitions from 48, 7, and 24 subjects were used, with 9 cross-sections per contrast. The in-house dataset included T$_{1}$-, T$_{2}$-, and PD-weighted images from 6, 1, and 3 subjects, with 48 cross-sections per contrast. To match single-coil data, training, validation, and test sets comprised (840, 105, 210) cross-sections per dataset, respectively, selected randomly. Multi-coil data were compressed to 5 virtual coils \cite{Zhang2013}.

The FedVAT model synthesized complex, coil-combined MR images during training \cite{Uecker2014}. To facilitate data sharing across sites, synthetic images were zero-padded to a consistent size for training, although quantitative assessments were performed using images in their native dimensions. Reconstructed images were compared to Fourier-based reconstructions from fully-sampled data as a reference for peak signal-to-noise ratio (PSNR, dB) and structural similarity index (SSIM, \%). Metrics were averaged across test subjects and reported as mean $\pm$ standard deviation, with images normalized to the [0, 1] range. Statistical significance of performance differences was assessed using the Wilcoxon signed-rank test.


\begin{figure*}[t]
\centering
\includegraphics[width=0.875\textwidth]{figure03_V1.png}
\captionsetup{justification=justified, width=\textwidth}
\caption{Representative images reconstructed via zero-filled Fourier method (Zero-filled), locally-trained single-site models (Single-site), a personalized FL model (FedProx), a distilled FL model (FedDistill), and the proposed FedVAT. Reference images derived from fully-sampled acquisitions are also given. Results shown for across-site reconstructions in the three-site FL setup on BraTS, fastMRI, MIDAS datasets for R=8x. (a) Local BraTS model tested on fastMRI, (b) Local fastMRI model tested on MIDAS. Zoom-in windows are included to emphasize method differences.}
\label{fig:visualres}
\vspace{-0.1cm}
\end{figure*}


\section{Results}
\subsection{Homogeneous Setup Performance}
We evaluated the PSNR and SSIM performance of FedVAT in a three-site federated learning (FL) setup using a common MoDL-3 cascade reconstruction model across sites at acceleration rates of R = 4x and 8x. This assessment determined the effectiveness of FedVAT under homogeneous conditions where the same model architecture was used at all sites for both within-site (e.g., an IXI-specific model tested on IXI data) and across-site (e.g., an IXI-specific model tested on fastMRI and BraTS data) scenarios.

While FedVAT performed competitively with single-site training in within-site evaluations, it demonstrated significant improvements in across-site tests. In within-site tests, FedVAT showed a PSNR-SSIM difference of $-0.67$ dB and $-0.42\%$ compared to single-site training, and $0.88$ dB and $0.58\%$ compared to FedAvg. While FedVAT showed slightly lower performance compared to single-site training in within-site evaluations, this is an acceptable outcome as the primary goal of FedVAT is to enhance generalization across different sites. FedVAT achieved PSNR-SSIM gains of $1.59$ dB and $1.54\%$ over single-site training, and $0.73$ dB and $0.53\%$ over FedAvg in across-site tests. These results, detailed in Table~\ref{tab:Homogeneous}, confirm FedVAT's robust performance and enhanced generalization capabilities in uniform setups. All reported improvements were statistically significant (p $<$ 0.05).




\begin{table}[t]
\centering
\caption{Reconstruction performance for the heterogeneous setup was evaluated on single-coil data using a common MoDL-3 cascade model, with within-site results in the upper panel and across-site results in the lower panel.
}
\resizebox{0.5\textwidth}{!}
{
\begin{tabular}{| Sc | l l | Sc | Sc | Sc | Sc | Sc | Sc |}
 \hline
& & & \multicolumn{2}{Sc|}{Site 1 (BraTS)} & \multicolumn{2}{Sc|}{Site 2 (fastMRI)}& \multicolumn{2}{Sc|}{Site 3 (MIDAS)}\\
 \hline
{} & &  &PSNR $\uparrow$   & SSIM$\uparrow$    & PSNR $\uparrow$   & SSIM$\uparrow$    & PSNR $\uparrow$  & SSIM$\uparrow$   \\
 \hline
\multirow{6}{*}{\rotatebox[origin=c]{90}{\textbf{Within-site}}}& \multirow{2}{*}{Single-site}   & 4x & \textbf{48.06$\pm$3.33} & \textbf{99.72$\pm$0.11} & \textbf{40.75$\pm$2.88} & \textbf{97.11$\pm$1.23} & \textbf{34.16$\pm$2.75} & \textbf{94.06$\pm$2.03} \\
& & 8x & \textbf{42.46$\pm$3.15} & \textbf{99.03$\pm$0.45} & \textbf{36.37$\pm$2.79} & \textbf{95.19$\pm$1.83} & \textbf{29.67$\pm$2.53} & \textbf{88.88$\pm$3.35} \\

 \cline{2-9}
& \multirow{2}{*}{FedAvg}  & 4x & 46.65$\pm$2.86 & 99.58$\pm$0.14 & 38.52$\pm$3.62 & 95.50$\pm$2.32 & 33.14$\pm$2.52 & 92.80$\pm$2.75 \\
& & 8x & 40.80$\pm$3.12 & 97.96$\pm$0.82 & 32.94$\pm$3.52 & 91.83$\pm$3.59 & 28.77$\pm$2.23 & 87.85$\pm$3.43 \\
 \cline{2-9}
& \multirow{2}{*}{FedVAT}   &4x &47.63$\pm$2.65&99.64$\pm$0.14&39.73$\pm$3.31& 96.46$\pm$1.56& 33.59$\pm$2.63 &93.53$\pm$2.10 \\
& & 8x &41.60$\pm$2.85&98.78$\pm$0.54& 34.58$\pm$3.36&94.14$\pm$2.33& 28.85$\pm$2.62 & 87.75$\pm$3.34\\
 \hline \hline
\multirow{6}{*}{\rotatebox[origin=c]{90}{\textbf{Across-site}}} & \multirow{2}{*}{Single-site}   & 4x & 34.45$\pm$3.57 &  91.44$\pm$4.05 &39.03$\pm$2.72& 95.85$\pm$1.90& 42.27$\pm$2.90 & 97.56$\pm$1.33 \\
& & 8x & 30.48$\pm$3.20 & 85.48$\pm$5.37& 34.79$\pm$2.63 & 92.94$\pm$2.57 & 36.74$\pm$2.93 & 94.91$\pm$2.07 \\
 \cline{2-9}
& \multirow{2}{*}{FedAvg} & 4x & 35.83$\pm$3.12 & 94.15$\pm$2.54 & 39.89$\pm$2.70 & 96.19$\pm$1.95 & 42.59$\pm$3.26 & 97.54$\pm$1.64 \\
& & 8x & 30.85$\pm$2.95 & 89.84$\pm$3.51 & 34.78$\pm$2.71 & 92.91$\pm$2.49 & 36.87$\pm$3.33 & 94.89$\pm$2.60 \\
 \cline{2-9}
{} & \multirow{2}{*}{FedVAT}   & 4x & \textbf{36.71$\pm$3.01} & \textbf{95.02$\pm$1.96} & \textbf{40.28$\pm$2.65} & \textbf{96.39$\pm$1.81} & \textbf{43.52$\pm$2.83} & \textbf{98.06$\pm$1.04} \\
& & 8x & \textbf{32.02$\pm$2.95} & \textbf{91.53$\pm$3.07} & \textbf{35.52$\pm$2.84} & \textbf{93.68$\pm$2.55} & \textbf{37.98$\pm$2.97} & \textbf{95.65$\pm$1.94} \\

\hline
\end{tabular}
}
\label{tab:Homogeneous}
\end{table}



\subsection{Heterogeneous Setup Performance}
To evaluate FedVAT in a heterogeneous setup, we compared it against single-site models (using only local data), FedProx (adds a stabilization term for federated training), and FedDistill (facilitates cross-architecture knowledge transfer). FedAvg was not applicable due to its uniform architecture requirement.

Reconstruction performance was evaluated for three-site FL setups on single-coil data. PSNR/SSIM metrics are listed for within-site (e.g., local BraTS model tested on BraTS data) and across-site (e.g., local BraTS model tested on fastMRI/MIDAS) reconstructions at R=4x and 8x acceleration factors. In each FL setup, individual sites used distinct architectures for MRI reconstruction (MoDL, rGAN, and D5C5). Note that conventional FedAvg is inapplicable.

Results for single-coil data in Table \ref{tab:Heterogeneous_single} show that, in within-site evaluations, FedVAT demonstrated PSNR-SSIM differences of \( -0.19 \, \text{dB} \) and \( -0.01\% \) compared to single-site training, improvements of \( 2.48 \, \text{dB} \) PSNR and \( 5.87\% \) SSIM over FedProx, and gains of \( 1.49 \, \text{dB} \) PSNR and \( 5.48\% \) SSIM over FedDistill.
In across-site evaluations, FedVAT achieved improvements of \( 1.61 \, \text{dB} \) PSNR and \( 4.73\% \) SSIM over single-site training, \( 1.74 \, \text{dB} \) PSNR and \( 3.54\% \) SSIM over FedProx, and \( 1.41 \, \text{dB} \) PSNR and \( 4.84\% \) SSIM over FedDistill.


For multi-coil data (Table \ref{tab:Heterogeneous_multi}), in within-site evaluations, FedVAT showed PSNR-SSIM differences of \( -0.73 \, \text{dB} \) and \( -0.84\% \) compared to single-site training, improvements of \( 1.00 \, \text{dB} \) PSNR and \( 0.80\% \) SSIM over FedProx, and gains of \( 1.14 \, \text{dB} \) PSNR and \( 0.81\% \) SSIM over FedDistill.
In across-site evaluations, FedVAT achieved improvements of \( 1.67 \, \text{dB} \) PSNR and \( 0.75\% \) SSIM over single-site training, \( 1.10 \, \text{dB} \) PSNR and \( 0.57\% \) SSIM over FedProx, and \( 1.18 \, \text{dB} \) PSNR and \( 0.61\% \) SSIM over FedDistill.

These results, corroborated by Fig. \ref{fig:visualres}, highlight FedVAT's superior reconstruction quality and effective handling of diverse architectures.

%Within-site 
%Single-site  -0.73 dB,   -0.84,
%FedProx        1.00 dB,    0.80, 
%FedDistill     1.14 dB,      0.81,

%Across-site 
%Single-site  1.67 dB,     0.75,
%FedProx      1.10 dB,      0.57, 
%FedDistill   1.18 dB,    0.61, 


\begin{table}[t]
\centering
\caption{
Reconstruction performance for three-site FL setups on single-coil data was evaluated using PSNR/SSIM metrics for within-site and across-site reconstructions at 4x and 8x acceleration. Sites used distinct architectures (MoDL, rGAN, D5C5), making FedAvg inapplicable.
}
\resizebox{0.5\textwidth}{!}
{
\begin{tabular}{| Sc | l l | Sc | Sc | Sc | Sc | Sc | Sc |}
 \hline
& & & \multicolumn{2}{Sc|}{Site 1 (BraTS-MoDL)} & \multicolumn{2}{Sc|}{Site 2 (fastMRI-rGAN)}& \multicolumn{2}{Sc|}{Site 3 (MIDAS-D5C5)}\\
 \hline
{} & &  &PSNR $\uparrow$   & SSIM$\uparrow$    & PSNR $\uparrow$   & SSIM$\uparrow$    & PSNR $\uparrow$  & SSIM$\uparrow$   \\
 \hline
\multirow{10}{*}{\rotatebox[origin=c]{90}{\textbf{Within-site}}}& \multirow{2}{*}{Single-site}   & 
    4x &\textbf{48.06$\pm$3.33} & \textbf{99.72$\pm$0.11} & 35.83$\pm$2.51 & 90.64$\pm$3.87 & \textbf{29.04$\pm$1.84} &  \textbf{74.20$\pm$3.97}\\
& & 8x & \textbf{42.46$\pm$3.15} & \textbf{99.03$\pm$0.45} & 33.18$\pm$2.26  &  86.58$\pm$4.64
&\textbf{27.79$\pm$2.05 }&\textbf{76.77$\pm$4.08}\\
 \cline{2-9}
& \multirow{2}{*}{FedAvg} & 4x & \ding{55} & \ding{55} & \ding{55} & \ding{55} & \ding{55} & \ding{55} \\
& & 8x & \ding{55} & \ding{55} & \ding{55} & \ding{55} & \ding{55} & \ding{55} \\
 \cline{2-9}
& \multirow{2}{*}{FedProx}   & 
    4x & 44.42$\pm$3.76 & 99.34$\pm$0.28 & 32.88$\pm$3.22 & 87.95$\pm$4.41 & 27.62$\pm$1.90  & 59.62$\pm$4.93  \\
& & 8x & 40.06$\pm$3.23 & 98.23$\pm$0.77 & 30.78$\pm$2.73 & 83.45$\pm$5.51 & 27.48$\pm$1.93 & 61.23$\pm$4.32 \\
 \cline{2-9}
 & \multirow{2}{*}
  {FedDistill}   &  4x & 44.68$\pm$3.72 & 99.40$\pm$0.24 &34.24$\pm$3.06 &88.48$\pm$4.13 & 28.95$\pm$1.92 &  60.19$\pm$5.07 \\
& & 8x & 40.32$\pm$3.22 & 98.29$\pm$0.75&  32.96$\pm$2.56 & \textbf{86.64$\pm$4.66}&  \textbf{27.79$\pm$2.07} & 62.22$\pm$5.25 \\
 \cline{2-9}
& \multirow{2}{*}{FedVAT}   & 
    4x & 47.63$\pm$2.65&99.64$\pm$0.14  & \textbf{35.89$\pm$2.50} &\textbf{90.69$\pm$3.86} & 28.83$\pm$1.97 & 74.19$\pm$4.07\\
& & 8x & 41.60$\pm$2.85&98.78$\pm$0.54 & \textbf{33.23$\pm$2.36} &{86.45$\pm$0.67} & 27.28$\pm$2.23& 73.22$\pm$4.25\\
 \hline
 \hline
\multirow{10}{*}{\rotatebox[origin=c]{90}{\textbf{Across-site}}} & \multirow{2}{*}{Single-site}   & 4x & 34.45$\pm$3.57 &  91.44$\pm$4.05  & 33.02$\pm$2.30 & 66.49$\pm$7.26 & 29.67$\pm$2.28 & 75.67$\pm$6.93 \\
& & 8x & 30.48$\pm$3.20 & 85.48$\pm$5.37 &30.08$\pm$2.20
& 59.14$\pm$6.78 &28.38$\pm$2.26 &73.64$\pm$6.55
 \\
 \cline{2-9}
& \multirow{2}{*}{FedAvg}  & 4x &\ding{55} & \ding{55} & \ding{55} & \ding{55} & \ding{55} & \ding{55} \\
& & 8x & \ding{55} & \ding{55} & \ding{55} & \ding{55} & \ding{55} & \ding{55} \\
 \cline{2-9}
& \multirow{2}{*}{FedProx}   & 
    4x & 34.40$\pm$3.12& 93.12$\pm$3.00 & 33.14$\pm$2.61 & 71.59$\pm$6.74 &29.23$\pm$2.38 & 72.45$\pm$8.43 \\
& & 8x & 30.91$\pm$2.87 & 90.33$\pm$3.36 & 30.89$\pm$2.22 & 64.17$\pm$7.14 & 28.37$\pm$2.63 & 72.76$\pm$7.12 \\
 \cline{2-9}
 & \multirow{2}{*}{FedDistill}   & 
    4x & 34.61$\pm$3.13 & 93.42$\pm$2.75 & 33.11$\pm$2.26 & 68.04$\pm$6.84 &30.03$\pm$2.41 & 71.80$\pm$7.84 \\
& & 8x & 31.12$\pm$2.83& 90.63$\pm$3.13
 & 30.47$\pm$2.20 & 61.79$\pm$7.06 & 29.05$\pm$2.57& 72.31$\pm$7.93 \\
 \cline{2-9}
 \cline{2-9}
{} & \multirow{2}{*}{FedVAT}   & 
    4x & \textbf{36.71$\pm$3.01} & \textbf{95.02$\pm$1.96} & \textbf{34.50$\pm$2.36} & \textbf{75.69$\pm$5.76} &\textbf{30.77$\pm$2.35} & \textbf{77.08$\pm$6.86}\\
& & 8x & \textbf{32.02$\pm$2.95} & \textbf{91.53$\pm$3.07} & \textbf{31.47$\pm$2.49} & \textbf{71.84$\pm$6.47} & \textbf{29.72$\pm$2.38}&\textbf{77.62$\pm$6.41} \\

\hline
\end{tabular}
}
\label{tab:Heterogeneous_single}
\end{table}


\begin{table}[t]
\centering
\caption{Reconstruction performance for three-site FL setups on multi-coil data was evaluated using PSNR/SSIM metrics for within-site and across-site reconstructions at 4x and 8x acceleration. Sites used distinct architectures (MoDL, rGAN, D5C5), making FedAvg inapplicable.}
\resizebox{0.5\textwidth}{!}
{
\begin{tabular}{| Sc | l l | Sc | Sc | Sc | Sc | Sc | Sc |}
 \hline
& & & \multicolumn{2}{Sc|}{{Site 1 (fastMRI knee-MoDL)}} & \multicolumn{2}{Sc|}{{Site 2 (fastMRI brain-rGAN)}}& \multicolumn{2}{Sc|}{{Site 3 (In-House brain-D5C5)}}\\
 \hline
{} & &  &PSNR$\uparrow$&SSIM$\uparrow$& PSNR$\uparrow$& SSIM$\uparrow$&PSNR$\uparrow$&SSIM$\uparrow$\\
 \hline
\multirow{10}{*}{\rotatebox[origin=c]{90}{\textbf{Within-site}}}& \multirow{2}{*}{Single-site}   & 
    4x & 33.48$\pm$2.35 & 93.21$\pm$1.54 & \textbf{36.97$\pm$3.07} &\textbf{97.32$\pm$1.17} &\textbf{31.81$\pm$1.73}
&\textbf{90.10$\pm$2.57}\\
& & 8x & 29.02$\pm$2.13 & 83.28$\pm$3.10 & \textbf{33.08$\pm$3.39}& \textbf{94.54$\pm$2.26}& \textbf{28.77$\pm$1.71}& \textbf{85.29$\pm$3.64}\\
 \cline{2-9}
& \multirow{2}{*}{FedAvg} & 4x & \ding{55} & \ding{55} & \ding{55} & \ding{55} & \ding{55} & \ding{55} \\
& & 8x & \ding{55} & \ding{55} & \ding{55} & \ding{55} & \ding{55} & \ding{55} \\
 \cline{2-9}
& \multirow{2}{*}{FedProx}   & 
    4x & 32.79$\pm$2.19& 
    90.40$\pm$2.45 & 33.95$\pm$2.88 & 96.30$\pm$1.39 & 
   30.34$\pm$1.55  & 88.99$\pm$2.53\\
& & 8x & 27.70$\pm$2.33 &
78.94$\pm$4.57 &
28.80$\pm$2.82 & 90.60$\pm$3.14 & 27.05$\pm$1.72 & 83.94$\pm$3.40\\
 \cline{2-9}
 & \multirow{2}{*}{FedDistill}   & 
    4x & 32.48$\pm$2.24&
    90.57$\pm$2.21 &
    34.07$\pm$2.83 & 96.36$\pm$1.36 & 30.10$\pm$1.61 & 88.73$\pm$2.52 \\
& & 8x & 26.99$\pm$2.47 &
78.49$\pm$4.51 & 29.08$\pm$3.15& 90.16$\pm$3.23 & 27.13$\pm$1.64 & 84.18$\pm$3.31 \\
 \cline{2-9}
& \multirow{2}{*}{FedVAT}   & 
    4x & \textbf{33.50$\pm$2.35} & \textbf{93.23$\pm$1.54}  & 35.86$\pm$2.78 &  96.85$\pm$1.19 & 30.72$\pm$1.69 & 88.02$\pm$2.30 \\
& & 8x & \textbf{29.37$\pm$2.13} & \textbf{83.40$\pm$3.14} & 31.48$\pm$2.34
 & 92.89$\pm$2.38 & 27.65$\pm$1.89 &  82.26$\pm$3.53 \\
 \hline
 \hline
\multirow{10}{*}{\rotatebox[origin=c]{90}{\textbf{Across-site}}} & \multirow{2}{*}{Single-site}   &     4x & 37.97$\pm$3.10 & 97.33$\pm$1.51 & 31.17$\pm$3.22 & 93.03$\pm$2.57 & 26.18$\pm$2.62 & 79.42$\pm$5.73 \\
& & 8x & 31.97$\pm$3.01 & 92.06$\pm$2.84 & 26.82$\pm$3.15 & 85.13$\pm$5.06 & 22.64$\pm$2.30 & 70.31$\pm$6.85 \\
 \cline{2-9}
& \multirow{2}{*}{FedAvg}  & 4x &\ding{55} & \ding{55} & \ding{55} & \ding{55} & \ding{55} & \ding{55} \\
& & 8x & \ding{55} & \ding{55} & \ding{55} & \ding{55} & \ding{55} & \ding{55} \\
 \cline{2-9}
 & \multirow{2}{*}{FedProx}   & 
    4x & 38.03$\pm$2.99 &
    96.54$\pm$1.41
    & 32.30$\pm$2.93 & 92.69$\pm$1.76 &  26.68$\pm$2.55 & 81.09$\pm$5.31 \\
& & 8x & 32.05$\pm$2.80 & 92.25$\pm$3.12 & 27.68$\pm$2.80 & 85.12$\pm$4.50 &
24.04$\pm$2.48& 70.60$\pm$6.94 \\
 \cline{2-9}
 & \multirow{2}{*}{FedDistill}   & 
    4x & 37.77$\pm$3.07 &
    96.53$\pm$1.59 &32.62$\pm$2.94&
    92.69$\pm$1.69& 26.39$\pm$2.50 & 80.99$\pm$5.26\\
& & 8x &  31.35$\pm$2.91 &
91.22$\pm$3.24&   28.03$\pm$2.50&
84.44$\pm$4.68 & 23.99$\pm$2.43 & 70.13$\pm$6.61 \\
 \cline{2-9}
{} & \multirow{2}{*}{FedVAT}   & 
    4x & \textbf{38.89$\pm$2.85}
 & \textbf{97.34$\pm$1.49} & \textbf{33.95$\pm$2.34} & \textbf{93.45$\pm$1.95} & \textbf{27.48$\pm$2.61} & \textbf{81.24$\pm$5.63}\\
& & 8x & \textbf{33.64$\pm$2.67} & \textbf{92.94$\pm$3.00} & \textbf{29.20$\pm$2.28} & \textbf{86.58$\pm$4.23} & \textbf{24.47$\pm$2.55} & \textbf{71.03$\pm$8.77}\\
\hline

\end{tabular}
}
\label{tab:Heterogeneous_multi}
\end{table}

% single-coil, within versus across
% PSNR = 1.90 dB GANcond, 1.54 dB GIMP --gokberk 1.9dB GANcond, 1.5dB GIMP
% SSIM = 1.97 % GANcond, 0.56 % GIMP --gokberk 2.0% GANcond, 0.6% GIMP

% multi-coil, within versus across
% PSNR = 2.5 dB GANcond, 0.82 dB GIMP --gokberk 2.5dB GANcond, 0.8dB GIMP
% SSIM = 1.8 % GANcond, 0.16 % GIMP --gokberk 1.8% GANcond, 0.2% GIMP


\subsection{Ablation Study}
To further understand the performance and contributions of FedVAT, we conducted two sets of ablation experiments.

First, we compared FedVAT against a centralized learning approach, where all data from sites were transferred to a central server for training, violating privacy constraints but serving as an upper bound for performance. This experiment was performed on single-coil data, with results shown in Table \ref{tab:cen_fedvat_compare}.
FedVAT demonstrated competitive performance, achieving strong results in both within-site and across-site evaluations. In within-site comparisons, FedVAT exhibited a PSNR-SSIM difference of \( 0.62 \, \text{dB} \) and \( 0.20\% \) SSIM compared to centralized learning. In across-site evaluations, FedVAT showed a PSNR-SSIM difference of \( 0.47 \, \text{dB} \) and \( 0.15\% \) SSIM relative to the centralized model.
While centralized learning maintained a slight edge due to full data availability, FedVAT's ability to operate without data centralization highlights its robustness and potential for practical, privacy-preserving applications. These results confirm that FedVAT is capable of approaching centralized learning performance while maintaining data privacy.

Second, we evaluated the impact of different generative priors by comparing FedVAT with GAN and DDPM models. Fig. \ref{fig:FID} (a) shows the fidelity of synthetic MR images, measured by the Frechet Inception Distance (FID). FedVAT achieved notably lower FID values, demonstrating superior image realism. In Fig. \ref{fig:FID}
 (b), we present across-site generalization performance for reconstruction models trained on synthetic data, measured by PSNR and SSIM at 4x and 8x acceleration rates. FedVAT consistently outperformed GAN and DDPM, confirming its effectiveness for robust reconstruction. Example slices in Fig. \ref{fig:compare_synth} further illustrate FedVAT's superior synthetic image quality. These results confirm FedVAT's strong generative prior enhances model generalization and image quality while preserving data privacy.

\begin{table}[t]
\centering
\caption{FedVAT is compared against  a privacy-violating centralized benchmark using MoDL as the common reconstruction model}
\resizebox{0.5\textwidth}{!}
{
\begin{tabular}{| Sc | l l | Sc | Sc | Sc | Sc | Sc | Sc |}
 \hline
& & & \multicolumn{2}{Sc|}{Site 1 (BraTS)} & \multicolumn{2}{Sc|}{Site 2 (fastMRI)}& \multicolumn{2}{Sc|}{Site 3 (MIDAS)}\\
 \hline
{} & &  &PSNR $\uparrow$   & SSIM$\uparrow$    & PSNR $\uparrow$   & SSIM$\uparrow$    & PSNR $\uparrow$  & SSIM$\uparrow$   \\
 \hline
\multirow{4}{*}{\rotatebox[origin=c]{90}{\textbf{Within-site}}}& \multirow{2}{*}{Centralized}   & 4x & 46.12$\pm$2.55 & 99.46$\pm$0.20 & 39.22$\pm$3.14 & 96.38$\pm$1.51 & \textbf{33.75$\pm$2.62} & 93.18$\pm$2.24 \\
& &8x & \textbf{42.16$\pm$2.72} & \textbf{98.65$\pm$0.58} & \textbf{35.75$\pm$3.07} & \textbf{94.84$\pm$1.94} & \textbf{29.83$\pm$2.53} & \textbf{89.47$\pm$3.16} \\
 \cline{2-9}
& \multirow{2}{*}{FedVAT}   & 4x & \textbf{47.63$\pm$2.65} & \textbf{99.64$\pm$0.14} & \textbf{39.73$\pm$3.31} & \textbf{96.46$\pm$1.56} & {33.59$\pm$2.63} & \textbf{93.53$\pm$2.10} \\
& & 8x &41.60$\pm$2.85&98.78$\pm$0.54& 34.58$\pm$3.36&94.14$\pm$2.33& 28.85$\pm$2.62 & 87.75$\pm$3.34\\
 \hline
  \hline
\multirow{4}{*}{\rotatebox[origin=c]{90}{\textbf{Across-site}}} & \multirow{2}{*}{Centralized}   & 
4x & 36.48$\pm$2.89 & 94.78$\pm$1.91 & 39.94$\pm$2.59 & 96.32$\pm$1.59 & 42.67$\pm$2.86 & 97.92$\pm$1.08 \\
& & 
8x & \textbf{32.79$\pm$2.81} & \textbf{92.16$\pm$2.62} & \textbf{35.99$\pm$2.63} & \textbf{94.06$\pm$2.27} & \textbf{38.95$\pm$2.90} & \textbf{96.75$\pm$1.43} \\
 \cline{2-9}
{} & \multirow{2}{*}{FedVAT}   & 
4x & \textbf{36.71$\pm$3.01} & \textbf{95.02$\pm$1.96} & \textbf{40.28$\pm$2.65} & \textbf{96.39$\pm$1.81} & \textbf{43.52$\pm$2.83} & \textbf{98.06$\pm$1.04} \\
& & 
8x & 32.02$\pm$2.95 & 91.53$\pm$3.07 & 35.52$\pm$2.84 & 93.68$\pm$2.55
& 37.98$\pm$2.97 & 95.65$\pm$1.94\\
\hline
\end{tabular}
}
\label{tab:cen_fedvat_compare}
\end{table}



\begin{figure}[H] % Single-column figure for one side
\centering
\includegraphics[width=0.49\textwidth]{tableFID.pdf} % Adjust width as necessary for column size
\caption{Fidelity of synthetic MR images in terms of Frechet inception distance (FID). Bar plots show mean\(\pm\)std across the test set.}
\label{fig:FID}
\vspace{-0.2cm}
\end{figure}

\begin{figure}[H] % Single-column figure for one side
\centering
\includegraphics[width=0.4\textwidth]{Figure05.png} % Adjust width as necessary for column size
\caption{Visual comparison of synthetic data generated by GAN, DDPM, and VAT.;}
\label{fig:compare_synth}
\vspace{-0.2cm}
\end{figure}

\section{Discussion}
In multi-institutional MRI studies, data scarcity, especially with rare pathologies, compromises model robustness and generalization. We introduced FedVAT, a federated learning (FL) framework leveraging a global MRI prior trained with a visual autoregressive transformer (VAT) model to enhance reconstruction performance and support diverse site-specific architectures.
In FedVAT, we employed a Visual Autoregressive Transformer  for federated data generation due to its inherent ability to model the hierarchical and structured nature of MRI data effectively. VAT leverages a next-scale prediction strategy, progressively synthesizing images from coarse to fine scales, which aligns naturally with the multiresolution characteristics of medical images. This approach enables VAT to capture both global anatomical structures and localized textural details with high fidelity. Additionally, VAT’s autoregressive framework ensures consistent dependencies across spatial scales, facilitating the generation of diverse and realistic synthetic data. By integrating VAT into our federated framework, we achieve a scalable and architecture-agnostic approach that captures the underlying data distribution across multiple sites while preserving data privacy. This capability makes VAT particularly well-suited for federated learning tasks in MRI, as it allows for effective generalization across diverse imaging environments while maintaining the fidelity and clinical utility of the generated data.




FedVAT distinguishes itself from existing FL methods by enabling an architecture-agnostic approach. Unlike traditional FL models that rely on uniform architectures across all collaborating sites \cite{McMahan2017CommunicationEfficientLO}, FedVAT supports the use of varied local architectures. This flexibility is crucial in real-world scenarios where different institutions may have specific infrastructure constraints and preferences for model design. Our approach builds upon the power of VATs to encode a shared global prior, ensuring that participating sites can fine-tune their models without the constraints of a fixed network design. This unique feature sets FedVAT apart from conditional models and other generative approaches such as GANs and diffusion models, which typically lack the same level of architectural flexibility.

Comparative analyses indicate that FedVAT achieves superior generalization and resilience across varied imaging protocols and dataset sizes. GAN-based methods, while effective for image reconstruction, often face stability issues and mode collapse, leading to potential artifacts due to their stochastic nature \cite{ rgan}. Diffusion models, although capable of high-quality synthesis \cite{sohl2015deep}, are computationally intensive and lack the architecture-agnostic flexibility that FedVAT offers. By contrast, FedVAT's autoregressive framework facilitates structured knowledge transfer and site-specific adaptation, enhancing consistency and performance while preserving local model flexibility.

An essential aspect of MRI reconstruction in federated settings is maintaining high performance despite potential variations in data distributions across sites. FedVAT's use of a shared global MRI prior allows for consistent quality across reconstructions while preserving the ability to adapt to site-specific nuances through local model fine-tuning. This approach helps mitigate the limitations posed by smaller, heterogeneous datasets, which can be especially impactful when rare pathologies are involved, as seen in some slices. Our findings underscore the importance of an architecture-agnostic design for enhancing the utility of federated models in practical clinical applications.



\section{Conclusion}
Here, we introduced a novel federated learning technique that enables multiple sites with distinct architectural preferences to collaborate in building MRI reconstruction models. To do this, FedVAT decouples collaborations into two phases, wherein the first phase learns a global image prior to capture the distribution of multi-site MRI data, and the second phase synthesizes data using the shared prior to build generalizable reconstruction models locally at each site. We demonstrate that FedVAT performs competitively with single-site models in within-site settings, and outperforms FL baselines and single-site models in cross-site settings. These findings indicate that FedVAT promotes effective knowledge transfer across sites while remaining agnostic to architectures of reconstruction models, hence overcoming a critical barrier in establishing flexible collaborations.  


\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,Papers}


\end{document}
