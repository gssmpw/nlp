\begin{table*}[tbp]
  \caption{Model performance comparison on NevIR in terms of pairwise accuracy. We include both newly tested and reproduced models, showing the original and our reproduced scores where available. An "X" indicates cases where no original score was reported.}
  \label{tab:model_comparison}
  \begin{tabular}{cccccc}
    \toprule
    Type & Data & Params & Model Name & Score & Score (ours) \\
    \midrule
    Random & N/A & 0 & Random & 25\% & 25\%\\
    \midrule
    Sparse & N/A & N/A & TF-IDF & 2.0\% & 2.0\% \\
    & MS MARCO & 110M & SPLADEv2 ensemble-distill \cite{formal2022distillation} & 8.0\% & 7.9\%\\
    & MS MARCO & 110M & SPLADEv2 self-distill \cite{formal2022distillation} & 8.7\% & 8.5\% \\
    & MS MARCO & 110M & SpladeV3 self-distill \cite{spladev3} & X & \textbf{9.9\%} \\
    \midrule
    Late Interaction & MS MARCO & 110M & ColBERTv2 \cite{santhanam2021colbertv2} & 13.0\% & 12.8\% \\
    & MS MARCO & 110M & ColBERTv1 \cite{colbertv1}& 19.7\% & \textbf{19.8\%}\\
    \midrule
    Bi-Encoders & NQ & 219M & DPR \cite{dpr} & 6.8\% & 6.5\%\\
    & MS MARCO & 110M & MS MARCO-bert-base-dot-v5 & 6.9\% & 6.9\%\\
    & MS MARCO & 110M & coCondenser \cite{gao-callan-2022-unsupervised} & 7.7\% & 7.7\%\\
    & NQ & 66M & nq-distilbert-base-v1 & 8.0\% & 8.0\%\\
    & MS MARCO & 110M & all-mpnet-base-v2 & 8.1\% & 8.1\%\\
    & MS MARCO & 66M & MS MARCO-distilbert-cos-v5 & 8.7\% & 8.7\%\\
    & QA Data & 110M & multi-qa-mpnet-base-dot-v1 & 11.1\% & 11.1\%\\
    & MS MARCO & 110M & DRAGON \cite{dragon} & X & 6.8\% \\
    & Variety of data sources \cite{qwen_gte}  & 1.5B & gte-Qwen2-1.5B-instruct \cite{qwen_gte} & X & 11.8\% \\
    & MS MARCO & 7B & RepLlama \cite{repllama_rankllama} & X & 13.0\%  \\
    & Unavailable & Unavailable & OpenAI text-embedding-3-small & X & 16.2\% \\
    & MS MARCO & 8B\footnotemark & promptriever-llama3.1-8b-v1 \cite{promptriever} & X & 18.1\% \\
    & Variety of data sources \cite{qwen_gte}  & 7B & gte-Qwen2-7B-instruct \cite{qwen_gte} & X & 19.0\% \\
    & MS MARCO & 7B & promptriever-mistral-v0.1-7b-v1 \cite{promptriever} & X & 19.7\% \\
    & Unavailable & Unavailable & OpenAI text-embedding-3-large & X & 22.6\%\\
    & E5 \cite{e5} \& TÃ¼lu 2 \cite{tulu2} & 7B & GritLM-7B \cite{gritlm} & X & \textbf{39.0\%} \\
    \midrule
    Cross-Encoders & STSB & 355M & stsb-roberta-large & 24.9\% & 24.9\%\\
    & MS MARCO & 61M & MonoT5 small \cite{nogueira-etal-2020-document} & 27.7\% & 27.7\%\\
    & MNLI & 184M & nli-deberta-v3-base & 30.2\% & 30.2\%\\
    & QNLI & 110M & qnli-electra-base & 34.1\% & 34.1\%\\
    & MS MARCO & 223M & MonoT5 base \cite{nogueira-etal-2020-document} & 34.9\% & 34.9\%\\
    & MS MARCO & 2.85B & MonoT5 3B \cite{nogueira-etal-2020-document} & 50.6\% & 50.6\%\\
    & MS MARCO & 7B & RankLlama \cite{repllama_rankllama} & X & 31.6\% \\
    & Unavailable & 270M & bge-reranker-base & X & 32.2\% \\
    & Unavailable & 560M & bge-reranker-large & X & 42.7\% \\
    & Unavailable & 568M & bge-reranker-v2-m3 \cite{baai_bge_reranker_v2_m3} & X & 43.5\% \\
    & Unavailable & 278M & jina-reranker-v2-base-multilingual & X & \textbf{65.2\%} \\
    \midrule
    Listwise LLM re-ranking & Variety of data sources \cite{qwen_gte} & 1.5B & Qwen2-1.5B-Instruct \cite{qwen_gte} & X & 11.4\% \\
    & Variety of data sources \cite{llama3} & 3B & Llama-3.2-3B-Instruct \cite{llama3} & X & 25.7\% \\
    & Variety of data sources \cite{llama3} & 7B & Llama-3.1-7B-Instruct \cite{llama3} & X & 29.7\% \\
    & Variety of data sources \cite{qwen_gte} & 7B & Qwen2-7B-Instruct \cite{qwen_gte} & X & 36.9\% \\
    & Variety of data sources \cite{mistral7b} & 7B & Mistral-7B-Instruct-v0.3 \cite{mistral7b} & X & 46.3\% \\
    & Unavailable & Unavailable & RankGPT 4o-mini \cite{openai2024gpt4technicalreport} & X & 64.1\% \\
     & Unavailable & Unavailable & RankGPT 4o \cite{openai2024gpt4technicalreport} & X & 70.1\% \\
     & Unavailable & Unavailable & RankGPT o3-mini & X & \textbf{77.3\%} \\
    \bottomrule
  \end{tabular}
\end{table*}
