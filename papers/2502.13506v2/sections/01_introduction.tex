
\section{Introduction}
Negation plays a crucial role in human communication, enabling the expression of contradictions, exclusions, and oppositions. Despite its common use in natural language, effectively handling negation remains a significant challenge for Language Models (LMs)~\cite{condaqa}. This issue has notable implications for Information Retrieval (IR) tasks, as different and complex forms of negation can appear in both user queries and documents.

This misinterpretation of negation is undesirable and can be harmful, particularly in high-risk retrieval tasks such as healthcare or legal settings, where it can lead to serious errors~\cite{weller2024nevirnegationneuralinformation}. For example, as shown in \autoref{fig:intro_example}, misinterpreting negation in the medical question-answering domain can have severe and even catastrophic consequences.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/intro_example.pdf}
    \caption{This example illustrates how misinterpreting a negation in a medical context could cause someone to misuse a critical emergency medication like epinephrine}
    \label{fig:intro_example}
\end{figure}

Despite its importance, the IR research community has paid little attention to addressing negation in retrieval. Existing work focuses on traditional term-based retrieval methods, overlooking more complex language structures such as negation~\cite{averbuch2004context, kim2019statute}. This is partly due to biases in existing collections, which lack sufficient examples of negation in the training data, leading to models' inability to handle negation effectively.

To address this data limitation, \citet{weller2024nevirnegationneuralinformation} proposes a novel negation benchmark -- NevIR -- to evaluate the performance of neural IR systems in handling negation. Their study tested models by ranking contrastive document pairs differing only in negation terms, using human-annotated queries to assess negation sensitivity. The results showed that most state-of-the-art (SotA) IR models struggle with negation, often performing worse than random ranking.
Given the challenges identified in NevIR, an important question is whether its findings extend to newer families of IR models and LLM-based IR methods. More specifically, LLMs have demonstrated strong effectiveness in IR, making them promising candidates for retrieval tasks~\cite{repllama_rankllama, rankGPT, qin2023large, ma2023zeroshotlistwisedocumentreranking}, but their ability to process negation remains uncertain. 

A shortcoming of the original paper~\cite{weller2024nevirnegationneuralinformation} is that it relies on pairwise accuracy, a non-standard IR metric, limiting its broader applicability to IR tasks, such as ranking. In particular, the metric tests whether a model can differentiate negation between two given documents, offering insights into LMs' handling of negation. However, the experiments in the original paper suggest that pairwise accuracy does not correlate with ranking-based metrics, such as nDCG@5. Therefore, the original study mainly addresses negation from an NLP perspective, focusing solely on language understanding, while missing the IR aspect of it --- learning negation leads to diminished ranking performance.
Another limitation of the paper is the absence of a baseline to assess the model's overall performance on the data distribution. While the authors included a random baseline (25\%) for the negation task, we argue that an additional baseline evaluating general performance on this distribution should have been provided. This would help distinguish whether the modelâ€™s poor performance is due to challenges in ranking documents with negation or an Out-Of-Distribution (OOD) effect.

In this work, we reproduce the NevIR paper~\cite{weller2024nevirnegationneuralinformation},
as well as testing its generalisability by evaluating LLM-based retrieval methods. Ongoing research in NLP has also explored the understanding of LLMs' negations, exploring factors such as model scaling and prompting techniques~\cite{truong2023language,wei2022inverse}. By applying NevIR to this new class of models, we aim to understand whether existing conclusions about negation in IR hold or if recent advancements bring meaningful improvements. Our goal is to assess the extent to which negation remains a problem, given the latest advances in IR and NLP.

Furthermore, \citet{weller2024nevirnegationneuralinformation} propose a preliminary solution for handling negation by fine-tuning IR models on the NevIR data set specific to negation. While fine-tuning has been shown to improve performance significantly, it remains unclear whether models learn negation patterns or overfit the data distribution of the NevIR dataset, particularly given that the dataset is created synthetically and could potentially lack negation and language diversity. To address this, we draw on the more recent work from \citet{zhang2024excluir} on ExcluIR, which includes a wider variety of negation types, and exclusionary queries. This comparison allows us to explore the generalisation of negation handling across different datasets and assess the effectiveness of NevIR in helping models understand negation in IR. Through this reproducibility study, we seek to extend the current work on negation in IR, contributing to a clearer understanding of how different datasets and models handle negation and ensuring that the insights gained are applicable beyond individual datasets. Furthermore, we aim to extend the initial NevIR experiments by exploring the potential trade-offs between improving negation understanding and preserving ranking performance. 






By re-assessing the findings of the original work and extending them, we address the following research questions:
\begin{enumerate}[label=\textbf{RQ\arabic*}]
    \item How do the negation understanding results of the NevIR dataset generalise to the most recent retrieval architectures, such as listwise LLM re-rankers?
    \item How do listwise LLM re-rankers perform when fine-tuned on the negation data of NevIR?
    \item How does fine-tuning on other negation datasets, such as ExcluIR, improve generalisability on NevIR?
    \item How can performance on a negation dataset be improved while maintaining strong ranking capabilities?
\end{enumerate}


Through our reproducibility work, we make the following findings:
\begin{itemize}[leftmargin=*]
    \item We observe a positive trend in handling negation on newly developed SotA IR models. Notably, a newly emerged model category, listwise LLM re-rankers, achieved 20\% gains compared to the previous approaches. We also observe that the recent advances in cross-encoders and bi-encoders with models such as GritLM and jina-re-ranker are also improving in negation understanding.

    \item When fine-tuning, we find that, despite its superior base performance, LLM-based new architectures exhibit similar learning capabilities for the negation task as the more traditional models.
    
    \item Fine-tuning experiments on NevIR and ExcliIR reveal that some models -- cross-encoder -- can generalise between the two negation datasets more effectively, going beyond the original data distribution. 
    \item Our analysis of training IR models on negation data shows that a trade-off selection criterion can help mitigate overfitting.
\end{itemize}



This research aims to assess the reproducibility of the original paper~\citep{weller2024nevirnegationneuralinformation} and the re-usability of the NevIR dataset to help negation understanding. We seek to verify its claims and assess its potential impact, contributing to the advancement of negation understanding in IR. Our code, which ensures full reproducibility of our experiments, is publicly available on GitHub\footnote{\url{https://github.com/thijmennijdam/NevIR-reproducibility}}.








    






















    
