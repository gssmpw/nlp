\section{Experimental Setup}
Through the original code\footnote{\url{https://github.com/orionw/NevIR}} and valuable correspondence with the original authors---primarily focused on complementing the fine-tuning code---we reconstructed the experiments from the original work. In this section, we describe the details of the experimental setup for the reproducibility experiments and our extensions.


\subsection{Models} \label{model_description}
We evaluate nearly all the neural IR models\footnote{Due to dependency issues with the RocketQA library, it was not possible to reproduce the RocketQA models \cite{qu-etal-2021-rocketqa}} categorized and tested by \citet{weller2024nevirnegationneuralinformation}, as well as update the benchmark with newly developed SotA models. Below, we describe each category of retrieval models and discuss the models evaluated within each. Additionally, we introduce \textit{listwise LLM re-ranking} as a new model category, which emerged in recent work \cite{rankGPT, rankLLM, FIRST}.

\header{Random Baseline.}
A baseline that randomly ranks the two documents. With two pairs, the expected mean pairwise accuracy is \( 25\% \left(\frac{1}{2} \cdot \frac{1}{2}\right) \).

\header{Sparse Models and Learned Sparse Retrieval.}
Sparse IR models use bag-of-words representations for document retrieval. The original work evaluated traditional TF-IDF and two \url{SPLADEv2++} variants: the ensemble distillation and self-distillation methods~\cite{formal2022distillation, lassance2022efficiency}. To extend this category, we evaluated \url{SPLADE-v3} \cite{spladev3}, which introduces multiple negatives per batch during training, improved distillation scores, and additional fine-tuning steps.

\header{Late Interaction Models.}
Late interaction models encode queries and documents as vectors for each sub-word token. During inference, they compute similarity through a MaxSim operation. We tested \url{ColBERTv1} \cite{colbertv1} and \url{ColBERTv2} \cite{santhanam2021colbertv2}, as these remain representative of the state of the art in this category.

\header{Bi-Encoders.}
Bi-encoders encode queries and documents into single-vector representations, enabling fast similarity computations using dot products or cosine similarity. The original work evaluated diverse models, including some from SentenceTransformers \cite{sentencetransformers}, \url{DPR} \cite{dpr} and \url{CoCondenser} \cite{gao-callan-2022-unsupervised}. We extended this category by evaluating several additional models. \url{DRAGON} \cite{dragon} is a model similar to \url{DPR} but adapted with advanced data augmentation techniques for enhanced generalisability. \url{Promptriever} \cite{promptriever} introduces promptable retrieval and was released with both \url{LLama-2-7B} and \url{Mistral-7B} backbones. We also included OpenAI's \url{text-embedding-small} and \url{text-embedding-large} models, and \url{RepLlama} \cite{repllama_rankllama}, which uses a Llama-2 backbone. Furthermore, we evaluated the Qwen models with 1.5B and 7B variants \cite{yang2024qwen2technicalreport}, and \url{GritLM} \cite{gritlm}, which is based on a Mistral-7B backbone. \url{GritLM} is trained as a bi-encoder, cross-encoder, and generative model, but due to time constraints, we focused solely on its bi-encoder variant. Testing its cross-encoder and generative modes would be an interesting direction for future work.

\header{Cross-Encoders.}
Cross-encoders process both the document and the query together, offering highly expressive representations but at a higher computational cost. These models are typically used for re-ranking rather than retrieval. The original study evaluated various cross-encoders from SentenceTransformers \cite{sentencetransformers} and MonoT5 \cite{nogueira-etal-2020-document}. In addition to these, we evaluated \url{RankLlama} \cite{repllama_rankllama}, trained as a pointwise re-ranker with a Llama-2-7B backbone. We also tested various BGE models, including \url{bge-reranker-base}\footnote{\url{https://huggingface.co/BAAI/bge-reranker-base}}, \url{bge-reranker-large}\footnote{\url{https://huggingface.co/BAAI/bge-reranker-large}}, and \url{bge-reranker-v2-m3}\footnote{\url{https://huggingface.co/BAAI/bge-reranker-v2-m3}}, which are highly par-ameter-efficient cross-encoders optimized for multilingual tasks. Finally, we included a Jina re-ranker, specifically \url{jina-reranker-v2-base-multilingual}\footnote{\url{https://huggingface.co/jinaai/jina-reranker-v2-base-multilingual}}, a re-ranking model designed for diverse cross-lingual retrieval tasks.

\header{Listwise LLM Re-rankers.}
In the Listwise LLM Re-ranking approach, LLMs, which are often pre-trained as foundation models, are adapted for ranking tasks. While these models are not explicitly fine-tuned for re-ranking in many cases, their rich contextual understanding and ability to assess semantic relevance enable them to reorder a given list of documents based on their alignment with a query. 
In this category, we evaluated RankGPT \cite{rankGPT} and specifically tested \url{GPT-4o-mini} \cite{openai2024gpt4technicalreport}, \url{GPT-4o} \cite{openai2024gpt4technicalreport}, and o3-mini. Additionally, we included Qwen2 models with both 1.5B and 7B Instruct variants \cite{qwen_gte}, as well as \url{Llama-3.2-3B-Instruct}, \url{Llama-3.1-7B-Instruct} \cite{llama3}, and \url{Mistral-7B-Instruct-v0.3} \cite{mistral7b}. These models span diverse parameter scales and are optimized for instruction-following tasks. 
For all LLMs, we employed standardized prompts from the RankGPT framework to ensure consistency across evaluations.






\subsection{Datasets and evaluation} \label{dataset_description} 
We evaluate the different families of model on several IR and negation datasets, namely:

\header{NevIR.} 
The NevIR \cite{weller2024nevirnegationneuralinformation} dataset is a benchmark for evaluating the impact of negation on neural information retrieval systems, featuring 2,556 contrastive document pairs and associated queries designed to test model understanding of negation. The dataset is divided into a train set (948 pairs, 37\%), a validation set (225 pairs, 9\%), and a test set (1,380 pairs, 54\%), with the test split being the largest. 

For the evaluation of NevIR we follow the setup of the original authors and use pairwise accuracy, illustrated in \autoref{fig:intro_example}. This metric assesses whether the model correctly flips the order of the ranking when given the negated query. 



\header{ExcluIR.} 
Unlike the NevIR dataset, which primarily focuses on understanding negation semantics within documents, ExcluIR emphasizes the exclusionary nature of queries. The ExcluIR dataset \cite{zhang2024excluir} serves as both an evaluation benchmark and a training set designed to help retrieval models comprehend exclusionary queries. The training set contains 70,293 exclusionary queries generated by \url{ChatGPT-3.5}, each paired with a positive and a negative document. The evaluation benchmark consists of 3,452 manually annotated exclusionary queries. For our fine-tuning experiments, we use the benchmark dataset, which is divided into a train set (2,070 pairs, 60\%), a validation set (346 pairs, 10\%), and a test set (1,040 pairs, 30\%). 

Each sample in the ExcluIR evaluation dataset consists of a single exclusionary query and two documents. The dataset is designed so that the model should rank one document higher than the other. We use accuracy as the evaluation metric.

\header{MS MARCO.} 
The MS MARCO dataset \cite{bajaj2016ms} is a large-scale benchmark for information retrieval and question answering, containing millions of real-world queries and corresponding passages. Designed to evaluate retriever and reader systems in a general IR setting, it includes only minimal instances of negation. Due to the large size of the MS MARCO development set, we chose to only use a subset of the original development set for evaluation, following a similar setting to TAS-B \cite{hofstatter2021efficiently}. This subset contained 500 queries, following the approach of SPLADE \cite{formal2021splade}. For every query, we used the top 1000 relevant documents as ranked by bm25 to build a corpus. 

For evaluating retrieval effectiveness on a large-scale IR dataset like MS MARCO, we use MRR@10.



\subsection{Listwise LLM Re-rankers}
Unlike other model categories that rely on triplets for fine-tuning, LLMs require conversational datasets. To facilitate this, we created conversational datasets for both NevIR and ExcluIR and uploaded them to Hugging Face\footnote{\url{https://huggingface.co/datasets/thijmennijdam/NevIR-conversational}}\footnote{\url{https://huggingface.co/datasets/thijmennijdam/ExcluIR-conversational}}. The dataset construction follows the instructional permutation generation method from RankGPT \cite{rankGPT}, enabling models to directly output a ranked list given a set of candidate passages. This approach involves presenting a group of passages, each assigned a unique identifier, to the LLM, which is then prompted to generate their permutation in descending order of relevance to the query.

\header{Sliding Window Strategy.} 
To evaluate the Listwise LLM re-rankers on MS MARCO, we addressed the token limitations of LLMs by adopting a sliding window strategy \cite{rankGPT}. This strategy introduces two hyperparameters: window size and step size. Due to computational constraints, the number of documents that can be re-ranked is limited. To mitigate this, we restricted re-ranking to the top-30 documents retrieved by BM25 \cite{bm25}. As we fine-tune to re-rank only two documents at a time, we observed a decline in ranking performance when handling more than two documents. To prevent this issue from impacting evaluation, we set the sliding window size to 2 and the step size to 1.

\header{Few-shot Prompting.} 
To assess the generalisability of LLMs to negation, we evaluated their ability to learn from examples through few-shot learning with 1, 3, and 5 examples. For each query in the NevIR test set, we provided the LLM with a fixed set of examples from the NevIR training set. This ensured that the LLMs were exposed to the same training samples for every test query.





\subsection{Experimental Details}

\header{Hyperparameters.}
For fine-tuning the different IR models, we used their respective default hyperparameters for training, as recommended in their original papers \cite{colbertv1, nogueira-etal-2020-document, reimers-gurevych-2019-sentence}. Specifically, the late-interaction model ColBERT uses a default learning rate of 3e-6, the bi-encoder model from Sentence-Transformers uses 2e-5, and the cross-encoder model MonoT5 employs a learning rate of 3e-4. For the \url{Mistral-7B-Instruct-v0.3} model we use a learning rate of 2e-4.

\header{Computational Requirements.}
Our experiments were conducted using an NVIDIA H100 GPU. It takes approximately 50 hours to reproduce our experiments. Additionally, we spent \$40 on OpenAI API costs.
