
\section{Experimental Results}
In this section, we present and describe the results on our reproducibility experiments as well as those of the extension experiments. The results of the reproducibility experiments are presented next to those of the original paper to verify reproducibility.

\subsection{Negation Understanding on SotA Neural Architectures}
The results of reproducing the NevIR benchmark are presented in \autoref{tab:model_comparison}. Overall, the reproduced scores align closely with those reported in the original paper, with only minor deviations observed for the SPLADE, ColBERT, and DPR models.

All models originally evaluated exhibit poor performance, with scores significantly below human levels. This prompts the first research question: 
\begin{enumerate}[label=\textbf{RQ\arabic*}]
    \item How do the negation understanding results of the NevIR dataset generalise to the most recent retrieval architectures, such as listwise LLM re-rankers?\label{rq1}
\end{enumerate}

\input{tables/table1}

\header{Evaluation of the SotA on NevIR}
\autoref{tab:model_comparison} presents the results of benchmarking newly developed SotA models. Across all categories, we observe that at least one new SotA model demonstrates improvements on the NevIR benchmark over previously tested models.

Regarding the bi-encoders, the \url{GritLM-7B} model achieves significant improvements (39\%), outperforming other models in this category by a substantial margin. In contrast, models such as \url{RepLlama}, \url{PromptRetriever} variants, and \url{gte-Qwen-2} variants show comparatively minor improvements over previously tested bi-encoders, despite utilizing larger LLM backbones such as \url{Llama-2}, \url{Qwen} (1.5B and 7B), and \url{Mistral-7B}. Notably, \url{GritLM} also employs a \url{Mistral-7B} backbone and functions as both an embedding and generative model, potentially enhancing its negation understanding compared to traditional embedding models. This strong performance of \url{GritLM} suggests that bi-encoders may eventually develop more effective representations for negation understanding. However, further analysis is needed to identify the specific factors contributing to this superior performance.

In the cross-encoder category, the integration of \url{Llama-7B} in \url{RankLlama} (31.6\%) does not improve performance over previous SotA models. The \url{bge} models we benchmarked also exhibit only limited improvement over prior evaluations. However, the \url{Jina-reranker-v2-base-multilingual} model achieves a pairwise accuracy of 65.2\% and significantly outperforms all other cross-encoders tested, despite its smaller model size.

\header{Listwise LLM Re-rankers}
In the newly introduced category of listwise LLM re-rankers, a clearer relationship emerges between model scale and performance, with larger models generally achieving better results. However, performance varies significantly among models with 7B parameters. For instance, while the \url{Llama 3.1-7B} model performs near random, the \url{Qwen2-7B-Instruct} (36.9\%) and \url{Mistral-7B-Instruct} (46.3\%) models demonstrate substantially stronger performance. Scaling further, \url{GPT-4o-mini} (64.1\%) and \url{GPT-4o} (70.1\%) illustrate the benefits of increased model capacity, as parameter estimates suggest these models are considerably larger. Notably, \url{o3-mini}, which incorporates reasoning capabilities, improves performance even further to 77\%.

To better understand model limitations, we analyzed errors made by \url{GPT-4o} and \url{o3-mini}. Our qualitative assessment suggests that their mistakes often occur on more challenging samples. For example, one case required distinguishing between invitations to formal and informal exhibitions, where the key factor was whether the event was officially organized. This task necessitates both an understanding of negation and real-world knowledge about formal events, highlighting the inherent difficulty of achieving a perfect score on the NevIR dataset.

We answer \ref{rq1} by finding that, while models have improved across all categories, negation understanding remains a challenge. Although the newly introduced listwise LLM re-rankers achieve the highest performance on the benchmark, their practical use is limited by significantly higher computational demands.

    
\subsection{Results on the Negation Learning Experiments.}

\header{Reproducing fine-tuning on NevIR.}
The results of the reproduced fine-tuning experiment on NevIR are presented in \autoref{fig:nev_fine_tune}. Consistent with the original paper, all models show improvements in pairwise accuracy after fine-tuning, with the bi-encoder and late interaction models reaching around 50\%, and the cross-encoder achieving approximately 75\%. However, none of the models attain perfect accuracy, indicating potential for further improvement. This led us to consider how listwise LLM re-rankers perform when fine-tuned on on the NevIR distribution, trying to address our second research question:

\begin{enumerate}[label=\textbf{RQ\arabic*}, start=2]
    \item How do listwise LLM re-rankers perform when fine-tuned on the negation data of NevIR?\label{rq2}
\end{enumerate}


\header{Fine-tuning listwise LLM re-ranker on NevIR.}
In \autoref{fig:nev_fine_tune} we also include the results of fine-tuning a listwise LLM re-ranker, \url{Mistral-7B-Instruct-v0.3}, on NevIR. We observe a significant improvement in negation handling over the base performance of 46.3\% reported in \autoref{tab:model_comparison}. However, the NevIR score after fine-tuning is comparable to that of the cross-encoder, indicating that, despite its superior base performance, this model exhibits similar learning capabilities for the negation task as the more traditional cross-encoder model.

\begin{figure}[tbp]
  \centering
  \includegraphics[width=0.5\textwidth]{figures/reproducibility_results.pdf}
  \caption{Fine-tuning results on NevIR. The top plot shows pairwise accuracy on NevIR, while the bottom plot presents MRR@10 on MS MARCO.}
  \label{fig:nev_fine_tune}
\end{figure}


\header{Few-shot LLM-based on NevIR}
\autoref{tab:few_shot} shows the results of the few-shot experiments. It can be seen that for the smaller model the performance drops when going from zero-shot to few-shot learning. The model does not understand the provided few-shot negation pattern and is unable of generalising it well to the other examples, probably because of its pre-training and size. \url{GPT-4o} is big enough to understand the "negation context" of these few-shot examples and apply them, Mistral does not understand how the examples relate to the test sample and fails to handle them correctly.

\input{tables/table2}

This partly answers \ref{rq2}, fine-tuning on NevIR can significantly enhance performance for a listwise LLM re-ranker, suggesting an improved ability to handle negation. However, this conclusion remains uncertain as there may be other contributing factors such as the model learning a shortcut, e.g. the data distribution to solve the task. This led us to consider the third research question: 
\begin{enumerate}[label=\textbf{RQ\arabic*}, start=3]
    \item How does fine-tuning on other negation datasets improve generalisability on NevIR?\label{rq3}
\end{enumerate}


\subsection{Examining Patterns of generalised Negation Comprehension}
To test how well the negation comprehension learned from the NevIR dataset generalises to different negation scenarios, we evaluate various IR models trained on NevIR on a different negation dataset, ExcluIR, and vice versa. The results are shown in \autoref{fig:merge_test_performance}. 

For the bi-encoder, late interaction and listwise LLM re-ranker models, the ExcluIR performance does not improve when fine-tuning on the NevIR dataset and only shows an increase when the ExcluIR data is included in the training set. Similarly, the NevIR performance also does not improve when fine-tuned on ExcluIR. This suggests that, for these models, the learned negation comprehension does not transfer effectively to other negation scenarios. 

In contrast, for the MonoT5 model, we observe that fine-tuning on the NevIR dataset leads to an improvement in the ExcluIR score as well. A similar trend is observed when fine-tuned on ExcluIR and evaluated on NevIR. This indicates that MonoT5 is capable of generalising its negation comprehension to a different dataset, possibly due it's large model size and expressive representation of cross-encoders.

Additionally, we test all models on a merged dataset where the training sets of NevIR and ExcluIR are combined. Across all models, this results in the highest or near highest accuracy on both datasets, indicating that training on both datasets simultaneously leads to the best overall performance. This supports the idea that NevIR and ExcluIR may capture distinct aspects of negation that require explicit exposure during training to be effectively learned, answering \ref{rq3}.

\begin{figure}[t!bp]

  \centering
  \includegraphics[width=0.48\textwidth]{figures/nevir_excluir.pdf}
  \caption{Model performance on ExcluIR and NevIR of the four families of models. Models were fine-tuned on each dataset separately and on the merged dataset.}
   \label{fig:merge_test_performance}
\end{figure}

\subsection{Analysis of Training Behavior}
The original study found that certain models exhibited overfitting to negation data during fine-tuning. While this improved their performance on negation-related queries, it came at the expense of their general ranking capabilities. This trade-off motivated the formulation of the fourth research question:

\begin{enumerate}[label=\textbf{RQ\arabic*}, start=4]  
    \item How can performance on a negation dataset be improved while maintaining strong ranking capabilities? \label{rq4}  
\end{enumerate}

\autoref{fig:nev_fine_tune} presents the learning curves for NevIR and MS MARCO during fine-tuning on NevIR. Consistent with findings in the original paper, we observe a decline in MRR, particularly in bi-encoder and cross-encoder models, namely multi-qa-mpnet and MonoT5. Their steeper learning curves show that these models may be learning faster but are also more prone to overfitting. Additionally, we evaluate the \url{Mistral-7B-Instruct-v0.3} listwise re-ranker model on MS MARCO using a sliding window approach. Unlike the other models, \url{Mistral-7B-Instruct-v0.3} maintains a stable MRR throughout fine-tuning, indicating that learning to re-rank on NevIR does not inherently degrade its ability to rank documents on MS MARCO.

To further investigate, \autoref{fig:msmarco_analysis} illustrates the impact of overfitting by comparing rankings produced by the original multi-qa-mpnet model and its fine-tuned counterpart on NevIR for a specific query. After fine-tuning, the model fails at exact term matching, instead favoring documents featuring other well-known individuals. This suggests that learning a new task overrides fundamental ranking heuristics, possibly because the model shifts toward contrastive learning, emphasizing negation handling over traditional lexical matching.

We find that overfitting effects can be partially mitigated by adopting a more effective early stopping criterion. Rather than selecting the best model based solely on NevIR validation performance, we apply a trade-off criterion that optimizes for both NevIR and MS MARCO performance and select the optimal model based on the average of MRR@10 and pairwise accuracy. \autoref{tab:selection_criterium} shows that this trade-off selection preserves NevIR performance while significantly reducing degradation on MS MARCO, mitigating overfitting and addressing part of \ref{rq4}.

\begin{figure}[tbp]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/msmarco_analysis.pdf}
    \caption{Rankings on MS MARCO made by multi-qa-mpnet base model and it's fine-tuned version on NevIR.}
    \label{fig:msmarco_analysis}
\end{figure}

\input{tables/table4}
