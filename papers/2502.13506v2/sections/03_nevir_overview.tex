\section{Overview of NevIR}
The NevIR dataset consists of contrastive query-document pairs where the documents are identical except for a key negation. Each query aligns semantically with one of the two documents, with Query \#1 corresponding to Doc \#1 and Query \#2 to Doc \#2. One such query-document pair is shown in \autoref{nevir_example}.


\begin{figure}[h]

  \centering
  \includegraphics[width=0.5\textwidth]{figures/IR2_nevir_figure.pdf}
  \caption{An example NevIR instance and the pairwise evaluation process for a contrastive query-document pair. This instance is classified as incorrect because only one of the two queries is ranked correctly, indicating that the ranker fails to account for negation.}

    \label{nevir_example}
\end{figure}
NevIR was constructed by first identifying pairs of minimally different documents that include negation, using the C\textsc{onda}QA dataset \cite{condaqa} as a foundation. C\textsc{onda}QA is an English reading comprehension dataset designed to facilitate the development of models capable of processing negation effectively. It consists of 14,182 question-answer pairs with over 200 unique negation cues. The dataset was created through crowdworkers paraphrasing passages by rewriting negated statements in a way that preserves the sentenceâ€™s meaning.

Building on top of this, NevIR introduced a query generation step via Amazon Mechanical Turk. Crowdworkers were tasked with formulating one query per paragraph under specific constraints, ensuring that each query aligned most with one of the two minimally different documents.

For evaluation, a contrastive approach was employed that focuses on paired queries and documents rather than a traditional IR corpus. Pairwise accuracy was used as the evaluation metric. This metric tests whether both ranked lists for the queries within the pair are accurate. The lower half of \autoref{nevir_example} shows the pairwise accuracy evaluation process. In this example, the IR model scored zero paired accuracy, ranking Doc \# 1 above Doc \# 2 in both queries (and failing to take into account the negation).
