
\section{Discussion and Conclusion}
We reproduce and extend the findings of NevIR, a benchmark study that
revealed most ranking models perform at or below the level of random ranking when dealing with negation. We consider several key factors: (1) the ongoing development of neural IR retrieval models; (2) the emergence of a new family of listwise re-rankers based
on LLMs; and (3) the emergence of new datasets on negation, like ExcluIR, enabling to measure the generalisability of models in terms of negation understanding.

In our reproducibility experiments, we confirm that most tested models fail to account for negation. We also reproduce the finding that fine-tuning on negation-specific data leads to performance gains on NevIR but can also cause overfitting, resulting in reduced performance on general retrieval tasks such as MS MARCO. Through further qualitative analysis, we show that overfitting on NevIR overrides fundamental ranking heuristics, such as exact term matching. To mitigate this effect, we apply a selection criterion for early stopping.

Beyond reproducing the results of the original paper, we extend the experiments by evaluating newly developed state-of-the-art ranking models. We observe improvements across all tested categories, with notable advancements in bi-encoders and cross-encoders. Specifically, GritLM, a bi-encoder model, demonstrates substantial improvements, achieving performance comparable to many cross-encoders. Similarly in the cross-encoder category, \url{jina- re-ranker-v2-base-multilingual} outperforms \url{MonoT5-3B}, despite having a significantly smaller model size.

Also, we tested the latest listwise LLM re-rankers on the NevIR task and found that they achieved the highest performance on NevIR, benefiting significantly from scaling. Furthermore, our results suggest that the reasoning capabilities of LLMs enhance performance, as \url{o3-mini} outperforms \url{GPT-4o} considerably. However, these models come at a significant computational cost, limiting their practicality to final-stage re-ranking. 

Additionally, we fine-tune \url{Mistral-7B-Instruct-v0.3} and observe improved performance on NevIR. However, unlike \url{GPT-4o}, \url{Mistral-7B-Instruct-v0.3}'s performance degrades in few-shot learning, where \url{GPT-4o} has substantial gains. This suggests that only \url{GPT-4o} is large enough to effectively grasp context and improve negation understanding.

To investigate the generalisation of negation comprehension learned from NevIR, we fine-tuned the models on NevIR and evaluated them on ExcluIR, and vice versa. The results showed that only the cross-encoder model demonstrated effective knowledge transfer of negation comprehension to this new setting. This is likely due to the cross-encoder's large model size and its ability to combine the query and documents into the most expressive representation among the different model categories. This suggests that the cross-encoder model is the only model that meaningfully represents negation in a way that generalises across different data distributions and contexts.

Despite the positive trend we observe on the NevIR benchmark, our findings show that many neural IR models still struggle with negation. Even the best models continue to fall short of human-level performance, reinforcing that negation understanding remains a persistent challenge for neural IR systems.










\section*{Limitations}
One limitation of our assessment of re-ranking with the Mistral model is that we trained it on triplets from NevIR rather than on a list of documents, as NevIR does not contain listwise training data. As a result, we could not evaluate performance degradation on MS MARCO with a sliding window larger than two, since fine-tuning the model on lists longer than two made it unable to correctly handle the re-ranking task.

Additionally, while the Mistral model seems to perform well on re-ranking MS MARCO, our evaluation was limited to 87 queries that contained at least one relevant document retrieved by BM25 within the top 30 results, as listwise LLM re-rankers depend heavily on the quality of the first-stage retrieval.

Finally, due to computational constraints, we were unable to evaluate and fine-tune an LLM larger than 7B parameters. A larger model could provide further insights into how model size affects re-ranking performance.

\subsection*{Environmental impact}  
The total GPU usage for this research, including exploratory runs and repeated experiments, was approximately 100 hours. All experiments were conducted on NVIDIA H100 PCIe GPUs (TDP of 350W) using the Snellius supercomputer, which has a carbon efficiency of 0.421 kgCO$_2$eq/kWh.

We approximated the environmental impact using the MachineLearning Impact calculator with an A100 PCIe 40/80GB GPU \cite{lacoste2019quantifying}. Total emissions were estimated post hoc to be approximately 13.16 kgCO$_2$eq. These estimations were conducted using the \href{https://mlco2.github.io/impact#compute}{MachineLearning Impact calculator} presented in \cite{lacoste2019quantifying} and account for both GPU power consumption and the energy efficiency of the hardware infrastructure.

For experiments with the ChatGPT models, we used the OpenAI API. Running these models requires significant computational resources and water consumption. While exact figures are not available, we highlight the environmental impact and encourage future research to consider environmental responsibility when conducting scientific studies.
