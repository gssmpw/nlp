\section{Theoretical Analysis of Survival Game}
\label{sec:theory}

In previous sections, we present the experimental results of current AI systems. Results demonstrate that many AI systems are stuck at the Limited Level on complex human tasks and that achieving the Autonomous Level requires extremely high parameter size. This raises an intriguing question: \textit{Why is the Autonomous Level so difficult to achieve for current AI systems?}

To address this, we turn to the framework of Self-Organized Criticality (SOC)~\citep{PhysRevLett.59.381}, a complexity theory in Physics. SOC describes systems in a criticality state where small perturbations can trigger large-scale changes. We propose that \textit{Survival Game} is deeply related to SOC and that drawing this connection provides insights into the nature of human tasks and AI. In the following, we will first discuss how \textit{Survival Game} relates to SOC, then model \textit{Survival Game} with a SOC framework.



\subsection{Human Tasks exhibit Criticality}


SOC refers to a phenomenon within complex systems where the system tends to settle into a stable state and yet is inherently sensitive to perturbations. On one hand, the system exhibits a self-organizing property: when a disturbance occurs, the system adapts and re-establishes a new equilibrium. On the other hand, the system is always in a criticality state where even the slightest perturbation in any single part can trigger cascading changes throughout the entire system. In this way, the system remains in a delicate balance: it is in a stable state but a slight perturbation can result in a massive reorganization throughout the entire system.
This property has been observed in many natural phenomena, such as earthquakes~\citep{turcotte1985collapse}, forest fires~\citep{malamud1998forest}, proteins~\citep{phillips2014fractals}, and neuronal avalanches~\citep{chialvo2010emergent}. 

We propose that human tasks exhibit criticality, which leads to \textit{Survival Game} as a SOC system.  In \textit{Survival Game}, the questions posed and the corresponding correct answers form a system. The self-organizing nature of this system is evident in the way that different questions correspond to different correct answers. We can imagine the correspondence between a question and its answer as a dynamic process. When a question and the correct answer are given, the system is in a stable state. When certain parts of the question are modified, the answer should undergo a self-organizing modification process and eventually evolve into a new correct answer, thus bringing the entire system back to stability. The system’s criticality arises from the nature of human tasks. For example, a minor alteration in a mathematical question can drastically change the approach required to solve it. This criticality property requires the test subject to address subtle differences in the question that can lead to significantly distinct answers. Merely memorizing answers for several specific cases does not help because a small change results in entirely different answers. We believe this is the cause of why so many AI systems are at Limited Level. AI systems might simply memorize some answers and yet human tasks are not friendly to memorization because of criticality property.

In the following, we will show several examples to help illustrate how human tasks exhibit criticality.


\textbf{Physics}:
Physics exhibits criticality. In physics, even seemingly similar problems can lead to vastly different solutions depending on the initial conditions. For instance, consider a question about a matter's state or superconductivity. The answer relies crucially on whether the temperature is above or below a threshold. Similarly, if we ask about physical laws, the appropriate theory, quantum mechanics or classical physics, depends on the scale. This phenomenon is ubiquitous in physics: small variations in initial conditions can lead to fundamentally different results.
By incorporating such questions into an \textit{Survival Game}, we naturally create a system with SOC property. The test subjects must adapt to subtle changes in initial conditions. Otherwise, their responses would be entirely wrong. If a participant relies solely on memorizing answers for some specific conditions, they will struggle to apply their knowledge to new, subtly altered questions. In such cases, the failure attempts are likely to approach infinity, as the answers will deviate drastically from the memorized solutions.

\textbf{Mathematics}:
Mathematics exhibits criticality. Mathematics embodies the very essence of SOC. Take Fermat's Last Theorem as an example. It asserts that there are no integer solutions to the equation $x^n + y^n = z^n$ for any integer $n>2$. At first glance, the change of $n$ might seem like a minor adjustment and does not affect the essence of the problem. However, the theorem's sensitivity to the value of n is profound. As soon as $n$ grows larger, the problem becomes much more complex. The theorem with small $n$ values was proven soon, but it took more than two centuries before the general case was finally proven. Many math problems show such criticality property that seemingly small modifications can alter the entire landscape of solutions. Thus, when mathematical problems are used in \textit{Survival Game}, subjects must be acutely aware of the details and select the appropriate mathematical tools. Subjects that do not possess this ability will take infinite attempts to arrive at the correct answer.

\textbf{Law}:
Legal issues exhibit criticality. In law, seemingly minor differences in behavior can lead to vastly divergent legal consequences. For instance, a suspect’s actions might determine whether the charge is premeditated murder or voluntary manslaughter, whether it was excessive self-defense or justifiable defense, or even whether they are guilty or innocent. Such significant shifts in legal outcomes can arise from differences in behavior that, on the surface, might appear negligible. When legal questions are incorporated into an \textit{Survival Game}, they require the subjects to discern these nuances and reason the outcomes. If a subject has only memorized conclusions for specific cases, they will struggle in new scenarios and will inevitably make infinite attempts before arriving at the right answers.


\textbf{Medicine}:
Medicine exhibits criticality. In medicine, small, seemingly insignificant changes in physiological parameters can result in dramatic shifts in treatment plans. For example, in cancer treatment, subtle genetic differences between patients can lead to vastly different responses to immunotherapy~\citep{hwang2020immune, martinez2023genetic}. In diabetes management, minor fluctuations in blood sugar levels may cause significant organ damage, necessitating precise adjustments in treatment~\citep{zhang2019molecular}. 
Although the differences in physiological parameters may appear minimal, the underlying medical phenomena and the corresponding treatments can vary greatly. Therefore, answering medical questions requires grasping the underlying principles. If the subjects simply rely on memorizing specific examples, they will make catastrophic errors in new cases.

These human tasks exhibit criticality and make \textit{Survival Game} exhibit SOC. Criticality is a typical symbol of complexity: Since a slight change in the question can result in an entirely new answer, successfully operating the task requires a full understanding of the underlying mechanism. Otherwise, subjects will be completely wrong about the correct answer. It will be difficult for them to arrive at the right one through trial and error. This is indeed the case for current AI systems as shown in our experiments. They remain at the Limited Level where the number of trials is infinite. This suggests that most AI systems do not fully understand the mechanism. 
Even if they are trained on enormous data and have memorized all of it, it is not enough for tasks with criticality properties.




\subsection{Modeling Survival Game as a Self-Organized Criticality System}



We have qualitatively explored how \textit{Survival Game} exhibits SOC property on human tasks. Now, we will adopt a quantitative perspective to validate this hypothesis. We will see that it closely resembles a typical SOC model and exhibits very similar experimental phenomena.


\begin{figure}[t]
  \centering
  \includegraphics[width=0.3\textwidth]{./figures/general_sandpile.pdf}
  \caption{Modeling \textit{Survival Game} as a complex network. Nodes represent concepts and edges represent interdependence. The network is also a sandpile model~\citep{PhysRevLett.59.381}.}
  \label{fig:soc_nodes}
\end{figure}





In \textit{Survival Game}, both questions and answers inherently consist of many basic concepts. Since answers change when questions change, these concepts are intricately coupled. Because of the criticality property of human tasks, even a small change in one concept can trigger significant changes in many other concepts. We can imagine this interconnection as a network graph, as shown in Figure~\ref{fig:soc_nodes}. Each node represents a concept, and the edges between the nodes represent the couplings between these concepts. Slightly changing the question is akin to disturbing a node. When a node becomes unstable, it is activated and is likely to make its neighbors also unstable. This might eventually result in cascading activation throughout the network. Such changes throughout the entire network imitate how seemingly small changes in the question can lead to very different answers in \textit{Survival Game}.



This conceptual network is very similar to a typical SOC system, namely the sandpile model~\citep{PhysRevLett.59.381}. It has been used to model complex systems in the natural world. In the sandpile model, each node accumulates sand. To disturb the network is to add a little more sand on a node. When the amount of sand exceeds a certain threshold, the sand topples from this node. The toppled sand is distributed to its neighbors, and thus the neighbors' sand may also topple. This cascade of toppling sometimes results in an avalanche throughout the entire network. 
We believe that this sand avalanche effect is close to the cascading activation of concepts in our conceptual network for \textit{Survival Game}. Therefore, we further implement our conceptual network as a sandpile model. Specifically, a slight change in a question is equivalent to adding a small amount of sand to a node. The degree to which the answer changes after altering the question is analogous to the avalanche size triggered by adding sand. Through this correspondence, we model \textit{Survival Game} as a sandpile model. The validity of this modeling can be confirmed by testing whether the sandpile model accurately reflects the characteristics of \textit{Survival Game} as observed in real-world applications.




Modeling \textit{Survival Game} as a sandpile model can explain why the failure count distribution of current AI systems resembles a power law.
In real-world experiments, we empirically show that failure count distribution is very close to a power law, as described in Section~\ref{sec:fit_failure_count_distribution}. However, the underlying reason is a mystery.
Now, we can investigate this phenomenon based on our abstraction of \textit{Survival Game}, namely the sandpile model.
Figure~\ref{fig:simulate_sandpile} shows the simulated distribution of avalanche size in the sandpile model. Based on our analogy, this also corresponds to the distribution of how much answers change after altering the questions in \textit{Survival Game}. The distribution follows a power law, which closely matches the failure count distribution of AI systems.
This actually reflects that current AI systems rely on memorization and exploration to solve new problems. Here is the reason. If AI relies on memorization and exploration, it means that when faced with a new question in \textit{Survival Game}, it recalls similar questions it has memorized and, based on the remembered answers, explores potential solutions. As a result, the number of explorations correlates with the distance between the answers. In contrast, if the AI system genuinely understood the test questions, the distance between answers would not correlate with the number of failure attempts. In fact, the mechanics of the sandpile model are clear, and an AI system that truly grasps these mechanics could directly compute the stable state without any failure attempt. Therefore, the phenomena indicate that AI systems do not fully understand the mechanics of the task and instead rely on memorization and exploration to find answers.


\input{tables/draw_sandpile.tex}


Modeling \textit{Survival Game} as a sandpile model also helps to illuminate how scaling works. From our experiments in Section~\ref{sec:fit_model_scale_to_decay}, we empirically observe that scaling model size can improve the decay rate and thereby improve intelligence. Yet, the underlying reason is a mystery.
However, we find that the dimensionality of the sandpile topology has a similar effect and can explain this phenomenon. 
More precisely, both theoretical analyses~\citep{dhar2006theoretical, zachariou2015generalised} and our simulations in Figure~\ref{fig:simulate_sandpile} show that the power law exponent of the sandpile model increases as the topology becomes high-dimensional. This is because a higher-dimensional topology creates more connections between nodes, enabling the network to stabilize more quickly when disturbed. We can draw the analogy between the sandpile's stabilization through these connections and intelligence systems' solving new problems through exploration. When we scale an intelligent system, we expand the problem-solving space to a higher-dimensional level and construct new paths between concept nodes. These new paths enable current intelligence systems to explore new solutions from memorized ones more quickly. This is akin to how a sandpile model stabilizes itself through new connections in a higher dimensionality. If we regard these new paths for exploration as the new connections in SOC systems, the effects of scaling can be explained: scaling makes exploration more effective. 


In summary, \textit{Survival Game} can be modeled as a sandpile model. It helps us gain a deep understanding of the nature of human tasks and current AI. The sandpile model is a complex system where a little more sand can result in an avalanche throughout the entire landscape. Similarly, \textit{Survival Game} with human tasks are also complex where a small change in environments requires completely different responses. 
No matter whether it is to predict the state of the sandpile model or find the correct solution in \textit{Survival Game}, it is necessary to fully understand the underlying mechanisms. 
Yet, most AI systems do not understand these mechanisms and rely on superficial imitation, such as memorization and exploration. 
They exhibit a typical power law with a small exponent. 
Scaling is able to transform the exploration space to a higher dimensionality and makes the exploration process more effective. 
Nevertheless, if AI lacks a full understanding of underlying mechanisms, SOC property will make it extremely difficult to achieve the Autonomous Level of intelligence.






