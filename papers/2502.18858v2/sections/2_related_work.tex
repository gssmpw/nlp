\section{Related Work}
\label{sec:related_work}

Defining a test for intelligence is a fundamental issue. For AI research, it allows us to understand, apply, and develop AI technology. More broadly, it enables us to gain a deep understanding of intelligence, leading to a profound insight into both humanity and the natural world.

In 1950, Alan Turing proposed the Imitation Game to test whether a machine can possess human intelligence. Since then, it has been highly influential in the AI field. Many researchers have developed methods to practically implement or further improve Imitation Game. In this section, we review some of the most influential approaches. 

\begin{itemize}
	\item Imitation Game, aka Turing Test~\citep{turing1950computing}: Intelligence is the ability to imitate human responses convincingly in a text-based conversation. If a human evaluator cannot reliably distinguish between a machine and a human based on their answers, the machine is considered intelligent.
	\item Total Turing Test~\citep{harnad1991other}: It is an extended version of the Turing Test that assesses a machine's ability to interact with the world in a human-like way. It goes beyond text-based conversations to include physical interaction and sensory perception.
	\item Chinese Room Argument~\citep{searle1999chinese}: It argues that the Imitation Game only evaluates syntactics and yet AI should also understand semantics, such as knowing the actual meaning of each word.
	\item Lovelace Test~\citep{bringsjord2003creativity}: It argues that intelligence is about creativity. For example, AI should be able to originate art, music, or poetry.
	\item Reverse Turing Test~\citep{baird2003pessimalprint}: Instead of asking whether a machine can act like a human, it asks whether an AI can differentiate between humans and machines. 
	\item Universal Intelligence~\citep{legg2007universal}: Beyond the conversation task in Imitation Game, it measures an agent’s ability to achieve goals in a wide range of environments.
	\item Winograd Schema Challenge~\citep{levesque2012winograd}: It tests whether AI can identify the antecedent of an ambiguous pronoun in a statement. It requires world knowledge and contextual understanding. 
	\item General intelligence~\citep{goertzel}: It defines intelligence as the ability to achieve a wide range of goals and handle new problems in different contexts and environments.
	\item Visual Turing Test~\citep{geman2015visual}: It adds the visual understanding ability to the Imitation Game. It tests whether AI can answer complex questions about images.
	\item Economical Value~\citep{openAICharter}: It tests whether AI can be a highly autonomous system that outperforms humans at most economically valuable work.
	\item The Modern Turing Test~\citep{suleyman2023coming}: It argues intelligence is to make a meaningful impact to the real world. It tests whether AI can make \$1 million on a retail web platform in a few months with just a \$100,000 investment.
	\item Outperforming Humans~\citep{morris2024levels}: It defines different levels of intelligence by how many humans AI can outperform. For example, a Competent AI outperforms $50\%$ skilled adults and a virtuoso AI outperforms $99\%$ skilled adults.
\end{itemize}

We observe that almost all previous works attempted to define intelligence by determining which human-like tasks that a machine must accomplish in order to be considered intelligent. However, there is significant variation in the choice of the tasks, as different researchers hold different perspectives on what constitutes intelligence. We can see that these approach approaches are inherently \textit{subjective}, which manifests in three key ways:
\begin{itemize}
	\item \textit{Subjective} (Human-Centric) View of Intelligence: Many of these tests utilize human intelligence as an upper bound for AI and evaluate whether AI can approach this bound. For example, Imitation Game~\citep{turing1950computing} evaluates a machine’s ability to replicate human behavior; \cite{openAICharter} defines intelligence as outperforming humans at economically valuable work; \cite{morris2024levels} defines intelligence level by how many humans AI can outperform. Nevertheless, if AI surpasses humans in certain tasks, these evaluation methods are no longer applicable. 
	\item \textit{Subjective} Choice of Tasks: These researchers believe that intelligence is a general property rather than something tied to specific tasks. Researchers have sought to define tasks that best reflect intelligence, making these tasks increasingly complex to measure ever more sophisticated forms of intelligence. However, this approach is inherently subjective: different researchers emphasize different aspects of intelligence, preventing consensus. For instance, \citet{harnad1991other} chooses physical tasks; \citet{bringsjord2003creativity} argues creative tasks; \citet{suleyman2023coming} adopts economic tasks; \citet{morris2024levels} suggest cognitive tasks. The belief that intelligence is independent of tasks, yet simultaneously trying to define it through a single universal task, leads to contradictions.
	\item \textit{Subjective} Evaluation Framework: These tests rely heavily on subjective measures of how well an AI system imitates human behavior. However, defining what constitutes ``good imitation'' and the threshold at which intelligence emerges is highly ambiguous. For example, Winograd Schema Challenge~\citep{levesque2012winograd} is considered defeated because AI achieved $90\%$ accuracy~\citep{kocijan2023defeat}; Imitation Game~\citep{turing1950computing} is considered defeated because current chatbot successfully fooled human evaluators $40\%$ of the time~\citep{biever2023chatgpt}; Modern Turing Test~\citep{suleyman2023coming} will be defeated if AI makes \$1 million. However, these thresholds are not well-defined and may differ among researchers. Since there is no universally accepted standard, the conclusions will be inconsistent. This defect makes it difficult to translate these tests into reliable evaluation methods for real-world AI applications.
\end{itemize}

In contrast, our proposed \textit{Survival Game} has a clear physical meaning and a well-defined statistical basis. It is inherently an \textit{objective} way to evaluate intelligence. Before we further elaborate on the differences between \textit{Survival Game} and related studies, we will first introduce \textit{Survival Game} in Section~\ref{sec:define_intelligence_test} and then continue the comparison in Section~\ref{sec:comparison_with_related_work}.


