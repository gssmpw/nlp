\section{Evaluation with Survival Game}
\label{sec:evaluate_current_ai}


In this section, we evaluate state-of-the-art AI systems with \textit{Survival Game}. We adopt a wide range of tasks, including vision, search, recommendation, and language. 

\textbf{Quantify AI's failures}:
We calculate the number of failures based on the scores output by the AI system. More precisely, for a given task, existing AI systems output a score for each potential answer. For instance, an image classification model assigns a score to each class; a search engine model predicts a relevance score for each document; a recommendation system assigns a score to each product; and a language model outputs a score to each word. A higher score represents a higher possibility the AI system predicts that this is the correct answer. We rank the answers based on the model's output score from highest to lowest. This ranking list is the model's attempt sequence, and the failure count equals the position of the reference answer minus one. 

The following presents the evaluation results of these models across various tasks. We will see that current models only reach the Autonomous Level in simple tasks and are at the Limited Level in most complex tasks. At the end of this section, we revisit existing AI techniques and show that these techniques are exactly developed in the context of Limited-Level intelligence.

\subsection{A Beginner's Task: MNIST}

\input{tables/mnist.tex}

MNIST~\citep{deng2012mnist} is a handwritten digit recognition task. It consists of a collection of images depicting the digits 0-9, written by different people. The task is for an AI system to correctly identify the digit in each image. Many people consider MNIST to be a relatively simple task, and it is often used as an introductory challenge for beginners to experiment with various AI algorithms. As such, we also start with this task to test whether \textit{Survival Game} can effectively distinguish different types of AI algorithms.

We used three AI algorithms: a linear classifier, a multilayer perceptron~(MLP) classifier~\citep{haykin1994neural}, and a convolutional neural network~(CNN). Neither the linear classifier nor the MLP classifier takes into account the specific characteristics of the task; they both flatten the 2D image into a 1D vector and perform transformations on this vector to do the classification. The transformation for the linear classifier is linear, while the MLP classifier introduces non-linear activation functions. In contrast, CNN is equipped with a deeper understanding of images. It uses convolution to capture local features and employs multiple layers to extract abstract semantic information. Therefore, from the perspective of task modeling, CNN performs more in-depth modeling compared to both the MLP and the linear classifier. We train the three models on MNIST's training data. To ensure the stability of the results, we run the experiments with $10$ different random seeds and then average the failure count distribution.  

The experimental results are shown in Figure~\ref{fig:mnist_result}. The blue dots are failure count distribution, and the red line is a power law for reference. The gray, green, and yellow regions represent Limited Level, Capable Level, and Autonomous Level, respectively. As we can see, \textit{Survival Game} clearly distinguishes these three different methods: The linear classifier falls within the Linear Level region, the MLP classifier falls within the Capable Level region, and CNN approaches the Autonomous Level region. Therefore, the better the system models the task, the higher its intelligence level. This suggests that the Survival Game is effective at evaluating the intelligence level of an AI system.


\subsection{Vision}


In this subsection, we test whether current AI models can effectively execute complex vision tasks. We select two types of tasks for evaluation. The first is an image classification task. Given an image, the model needs to identify what animal or object is present and categorize it appropriately. For this task, we use a widely recognized dataset, ImageNet-1K~\citep{deng2009imagenet}. The second task is more complex: given a natural language description, the model should find the corresponding image from a large set of images. Compared to image classification, this task requires the model to understand the meaning of a long natural language description and have a deeper understanding of complex images. We use two popular datasets for this task: MS COCO ~\citep{lin2014microsoft} and Flickr30k~\citep{plummer2015flickr30k}.

We evaluate state-of-the-art AI models currently available in the field. In the first image classification task, we use CLIP model~\citep{radford2021learning} and MAE models of various sizes~\citep{he2022masked}. CLIP is widely used for visual tasks, such as text-to-image generation. The MAE models are among the best-performing on ImageNet. For the second task, we select several top-performing models from the relevant task leaderboard~\citep{ilharco2021openclip}, namely DFN-VIT-L, ConvN-XXL, and EVA01-G. These models are not only large in parameter size but also in the size of the training data. They represent the best models in the field.

\input{tables/vision.tex}

The experimental results are shown in Figure~\ref{fig:vision_result}. 
The first row shows the results of the image classification task, with different images corresponding to different models. We can see that all models are at Limited Level. As we use a larger MAE model, the decay rate increases and data points gradually approach the Capable Level. In the two subsequent rows, we show the results for the MS COCO and Flickr30k datasets. Different images in the same row correspond to different models. The results show that even the most advanced models today are at Limited Level, with decay rates around $1.7$ or below, far from Capable Level's threshold of $2$. We can also see a similar trend as observed in the first row: the larger the model, the closer it is to the Capable Level. But the marginal improvements diminish gradually. 

The fact that these models are at Limited Level points to a clear physical meaning: if these models are to find out the answers to a vision-related task when they are wrong in the first place, the model would, in statistical terms, need an infinite number of guesses to get it right. In other words, the model not only makes incorrect predictions but also regards the correct answer as completely wrong. If it tries to solve the task, it will try many incorrect answers before finally outputting the correct one. 
Therefore, we should not place blind trust in visual models' results. Instead, we should provide supervision and guidance to ensure their reliability.

\subsection{Search}


Next, we evaluate the performance of text search models. Text search should be familiar to many people. It has widespread applications in search engines like Google, Bing, and Baidu. Given a query, the text search model ranks the candidate documents in order of relevance from highest to lowest. We regard this ranking list as its attempt sequence when applying \textit{Survival Game}.

We use a diverse range of datasets. 
We synthesize a basic dataset so that readers can have a better understanding of the task. We use Wikipedia as the raw data and construct a text search task with its titles and documents. Given a title, the search model ranks all the documents and should put the corresponding document at the top of the ranking list. The number of failure attempts is equal to how many incorrect documents are ranked higher than the correct ones. 
Besides this synthetic dataset, we also use many real-world search datasets.
We adopt two web search datasets, MS MARCO~\citep{Bajaj2016Msmarco} and T2Ranking~\citep{xie2023t2ranking}. The former is in English and the latter is in Chinese. Both were derived from real user queries on search engines. They are widely used to benchmark the effectiveness of text search models.
We also use datasets from finance domain and social platforms: FiQA~\citep{Maia2018FiQA}, CqadupStack~\citep{hoogeveen2015}, and Quora~\citep{Iyer2022first}. FiQA requires the model to find the relevant answers to financial questions. CqadupStack and Quora are released by StackExchange and Quora social platforms, respectively. Given a query, they require models to find duplicate queries.

We use three distinct search models for evaluation. The first is BM25~\citep{robertson1976relevance}, a popular model that was proposed decades ago. It is based on exact match and term frequency weighting. We implement it with Anserini toolkit~\citep{yang2017anserini}.
The second is dense retrieval~\citep{karpukhin2020dense, reimers2019sentence}, which represents both the query and the documents as semantic vectors and ranks them based on vector similarity. We use two open-sourced models from BGE~\citep{bge_embedding} since they are top performers on the related leaderboard. The two models vary in size, and we denote them as DR Small and DR Base.
The third is cross-encoder~\citep{nogueira2019passage}, which takes both the query and the document as input and uses attention mechanisms to model their interaction. In this way, it captures more nuanced matching signals and predicts relevance more accurately. We use two strong open-sourced models. On the English dataset, we use MiniLM cross-encoder~\citep{reimers2019sentence}. On the Chinese dataset, we use BGE cross-encoder~\citep{bge_embedding}. 


\input{tables/text_search.tex}

The experimental results are shown in Figure~\ref{fig:text_search_results}. The first two rows show the performance of the Wikipedia synthetic dataset and the English web search dataset, respectively. The third row shows the performance on other datasets. We can see that on all datasets and for all text search models, the performance remains at the Limited Level. On the synthetic Wikipedia dataset, the current models' performance is close to the Capable Level. On other real-world datasets, the models are far from the Capable Level. Besides, from the results in the first two rows, as the models become larger and more complex, their decay rate increases and data points move closer to the Capable Level.

Limited Level has a clear physical meaning in the text search scenario: when a user submits a query and the right document is not ranked at the top, the right document is likely to be ranked at the end of the list. The user needs to read, in statistical terms, infinite irrelevant documents before reaching the document they are looking for. In other words, when a search model makes a mistake, it is almost completely unable to correct itself.
This highlights the complexity of the text search task and the inadequacy of current search technologies. It inspires us that we cannot simply rely on search engines to seek information.

\subsection{Recommendation}

\input{tables/recsys.tex}

After examining the results of search engines, let’s turn our attention to another widely used AI application: recommendation systems. Recommendation systems predict what a user likes based on past behavior and profile information. These systems have extensive applications in areas such as e-commerce, short videos, etc. 

We adopt many real-world datasets from a wide range of domains. 
We use the Amazon Beauty dataset~\citep{he2016ups} to represent users' preference in e-commerce recommendations. It focuses on skincare product recommendations on the Amazon platform. 
We use MovieLens~\citep{harper2015movielens} to represent movie recommendations. It is constructed based on user ratings of movies. 
We use Steam dataset~\citep{kang2018self} to represent users' preference in game recommendations. It recommends games to players on the Steam platform.
We use Douban Book~\citep{zhu2020graphical, zhu2019dtcdr} to represent book recommendations. Douban is a popular Chinese internet platform and this dataset is to recommend books to users.
We use Douban Music~\citep{zhu2020graphical, zhu2019dtcdr} to represent users' preference in music recommendations. It is also collected from the Douban platform and is to recommend music to users.
Finally, we use Gowalla dataset~\citep{cho2011friendship} to represent location recommendations. Gowalla is a location-based online social network application where users share their check-in location. The dataset is to recommend places users might like to visit. 

We test four widely recognized recommendation methods. 
The first is a popularity-based recommendation method. As the name suggests, it ranks items based on their popularity and recommends them accordingly. Although it is straightforward, it is effective and commonly used in real-world applications. 
The other three methods are sequential recommendation models: GRU4Rec~\citep{hidasi2015session, hidasi2018recurrent}, SASRec~\citep{kang2018self}, and ComiRec~\citep{cen2020controllable}. They differ in architecture. GRU4Rec employs recurrent neural networks to build user profiles based on the interaction history. SASRec uses attention mechanisms to model how past interactions influence future preferences. ComiRec captures users’ diverse interests with a dynamic modeling approach.


The experimental results are shown in Figure~\ref{fig:recsys_results}. The first row shows product recommendations, and the second row shows movie recommendations. The third row shows the performance of SASRec across different domains. According to the results, on all datasets and for all models, data points fall within the Limited Level region and are far from the Capable Level region. The estimated failure decay rate is even lower than $1$, meaning that the distribution of failure counts has a very heavy tail. In other words, the recommendation system has to try a lot of times before finding the item users like. We believe this poor performance originates from the nature of recommendations. Recommendations do not require explicit input from users and rely solely on historical interactions. Such a lack of explicit information input makes predictions very difficult.

This result has a clear physical meaning: When a recommendation model makes a mistake, it is almost impossible for it to find the correct product through continuous attempts. For users, it means that users will see, in statistical terms, infinite uninterested items before being presented with something they are truly interested in. If a user is disappointed every time they see an item they are not interested in, the current recommendation system will disappoint them countless times.

\subsection{Language}

We have assessed AI models in vision, search, and recommendation tasks. Now, we proceed to language tasks. 
Some studies claim that large language models have already achieved exceptionally high-level intelligence and passed the Turing Test~\citep{biever2023chatgpt, aharoni2024attributions, mei2024turing}.
With \textit{Survival Game}, we can examine their intelligence levels and re-think this conclusion. We will use four tasks for a comprehensive evaluation, including coding, mathematics, question answering, and writing.

\textbf{Experimental Setup}: 
We input the question to large language models and examine the models' correctness in predicting the answer. 
The answer written by humans is regarded as the correct one. 
If the answer contains more than one word, such as writing a math proof or a long passage, we concatenate the question and the first $n$ answer words as the models' input and evaluate the performance in predicting the $n+1$-th answer word.
The number of failure attempts equals the number of words that are scored higher than the correct one. 
For datasets where the answers need to follow a fixed format, such as multiple-choice questions or calculating a number, we provide several examples before the actual question to prompt the model about the required answer format. If we use $m$ examples, we will indicate that this is an $m$-shot result in the figure title. This approach helps the model respond in the specified format and improves accuracy~\citep{brown2020language}.

We evaluate state-of-the-art large language models, including Qwen2.5 series~\citep{qwen2, qwen2.5}, Deepseek V2 16B~\citep{deepseekv2}, and Llama3 72B~\citep{dubey2024llama}. They are state-of-the-art models at their scale and are even competitive compared to models with much larger scale~\citep{guo2025deepseek}. 
Although we cannot run models with more parameters due to limited resources, we will extrapolate our results to a larger scale in Section~\ref{sec:future_prediction}.



\subsubsection{Coding}
\label{sec:code_experiment}

\input{tables/code.tex}

We test models' ability to write code. Code has a clear structure, which makes it easier to predict compared to natural language. We use three widely recognized coding benchmarks. All three are designed for beginner-level programming tasks. The first is HumanEval~\citep{chen2021codex}. It provides the function signature as well as docstring and requires subjects to write the function body. The second is MBPP~\citep{austin2021program}. It requires subjects to write functions based on a natural language description. Answers for both HumanEval and MBPP are function definitions. The third is CRUXEval~\citep{gu2024cruxeval}. It requires subjects to understand a function and infer its output for a given input. The answer is usually a code object, such as a string or a list.

The experimental results are shown in Figure~\ref{fig:code_results}. The three rows present results on HumanEval, MBPP, and CRUXEval, respectively. We can see that models with more parameters are closer to the Capable Level. For 70B models, a few data points are already within the Capable Level region, yet a long tail of data points still falls at the Limited Level region. 
Therefore, although current models are relatively strong and approaching the Capable Level in coding,  they are mostly at the Limited Level. It means that they cannot reliably find correct solutions through trial and error for basic coding questions. Thus, human supervision is essential.




\subsubsection{Mathematics}
\label{sec:math_experiment}

\input{tables/math.tex}

Next, we test models in another structured domain, namely mathematics. We use three popular datasets. The first is CMath~\citep{wei2023cmath}. It is a Chinese dataset that focuses on elementary school-level math problems. It requires subjects with the ability of addition and subtraction. The second is GSM8K~\citep{cobbe2021gsm8k}. It is an English dataset with similar problems to CMath. For these two datasets, the correct answer that the model needs to output is a number, usually no more than two digits. The third is MATH competition dataset~\citep{hendrycksmath2021}. It contains complex math problems derived from math competitions. The answers to these math problems are usually a long text, such as a mathematical proof or the step-by-step process of solving the problem. 

The experimental results are shown in Figure~\ref{fig:math_results}. From top to bottom, the rows correspond to CMath, GSM8K, and MATH. We can see that on the first two datasets, models are far away from the Capable Level. Thus, current models can hardly perform basic addition and subtraction. In contrast, results in the third row suggest that models are relatively strong in Math Competitions. The data points are approaching the Capable Level. In summary, the models have difficulty solving simple elementary school math problems, yet they perform much better on complex competition-level math problems. This reflects a significant difference between AI and human intelligence. Besides, we also observe that as the model size increases, there is a clear trend of moving closer to the Capable Level.

Therefore, we should exercise caution when using large language models to solve mathematical problems. Although they might solve some complex math questions, they still make significant errors on basic math problems that are easy for humans. In general, current models are at Limited Level. This means that they require a large amount of trials before finding the correct solutions. Thus, it is always necessary to validate their outputs.

\subsubsection{Question-Answering}

\input{tables/qa.tex}

Next, we examine the models' ability in the Question Answering (QA) task. We select three widely used datasets: MMLU-Pro~\citep{wang2024mmlu}, Natural Questions (NQ)~\citep{lee2019latent, Kwiatkowski2019NaturalQ}, and Trivia QA~\citep{2017arXivtriviaqa}. MMLU-Pro consists of multiple-choice questions across various fields such as mathematics, chemistry, law, etc. The model needs to choose one answer from ten options. NQ is a dataset of real-world questions about factual information. TriviaQA is similar to NQ. Answers in both datasets are only several words long. 

The experimental results are shown in Figure~\ref{fig:qa_result}. The three rows represent MMLU-Pro, NQ, and TriviaQA, respectively. From the results, we observe that all four models are at Limited Level. In MMLU-Pro, the models' failure decay rate is less than $1$. In NQ and TriviaQA, the performance is slightly better than in MMLU-Pro, but the models are still far from reaching the Capable Level. Furthermore, we can see that as the model size increases, the decay rate also increases, gradually moving toward the Capable Level. However, the marginal gains diminish: there is a significant improvement when going from 0.5B to 16B, but then the progress slows down. This suggests that the improvement is sublinear with respect to model size.

Results reflect that question-answering systems can make serious mistakes. In some cases, the systems regard the correct answer as completely incorrect. As a result, we cannot fully trust current question-answering systems, and it is crucial to verify the accuracy of their outputs.


\subsubsection{Writing}

\input{tables/language_domains.tex}


Now, we evaluate general writing ability. Based on many human-written articles, we examine whether current systems can also write like humans. During the evaluation, we use the first $n$ words as the input and examine whether subjects can accurately predict the $n+1$-th word. The number of failure attempts equals the number of words scored higher than the $n+1$-th word written by humans.
To ensure the model has sufficient context to make its prediction, we only consider cases where the input prefix is long enough, such as when $n \geq 1,000$. Since those AI systems are trained on large amounts of human data to mimic humans' writing, we believe it is appropriate to adopt human's next token as a reference. 


First, we test model performance in different domains. Domains include Wikipedia, code (from Github), patent backgrounds, scientific papers (from ArXiv), medical articles (from PubMed), community QA, and legal texts. Most of the domain data is from Pile~\citep{gao2020pile}. For the legal domain, we use legal texts from France, China, and the US. China and France follow civil law systems with codified legal texts, and we examine whether models accurately memorize them. The data is from \citet{HFforLegal2024} and \citet{wang2023chinese}.
The US follows a common law system, and the test examines whether models can write legal opinions by federal and state courts. The data is from the FreeLaw subset in Pile~\citep{gao2020pile}.
  

The experimental results are shown in Figure~\ref{fig:language_domain_results}. The first row compares the performance of different models on Wikipedia, while the second and third rows show results on other domains. We can see that all models are at Limited Level. The first row illustrates that as model size increases, the slope of the performance curve becomes steeper and the data points move closer to the Capable Level region. Besides, we notice that on the French and Chinese law datasets, models are also at the Limited Level even though the two datasets simply test their memorization ability of regulations. This suggests that memorizing legal texts is not as simple as it sounds. Overall, current models are still in the early stages in terms of writing. It is important to carefully validate their outputs.

\input{tables/different_languages.tex}

Next, we examine whether this result holds across different languages. We test the language modeling capabilities in English, Chinese, Spanish, German, French, Japanese, Italian, Portuguese, and Polish. We use the C4 dataset~\citep{raffel2020exploring}, which consists of a large number of articles from the internet and already categorizes them into different subsets based on the languages. This dataset is commonly used to train and test large language models. 


The experimental results are shown in Figure~\ref{fig:multilingual_results}. The first row shows the performance of different models in Polish. The second and third rows show the results for other languages. We can see that across all languages, models are at the Limited Level. It further demonstrates that current language models are at the Limited Level regardless of the language they use.


\subsection{Revisiting Current AI Techniques}

In previous subsections, we evaluate current AI systems in areas such as vision, search, recommendation, and language. We can see that AI remains at the Limited Level. Although this insight was not widely recognized before this study, we find that it has already profoundly impacted existing AI technologies. In other words, current AI technologies are exactly developed in the context of Limited-Level intelligence.

We begin by establishing a connection between AI technology and \textit{Survival Game} through the concept of \textit{loss}. Loss plays a crucial role in AI, especially in deep learning, as it quantifies the degree of error made by an AI system. The smaller the loss, the more advanced the AI system is considered. 
We can see that the concept of loss is very similar to the concept of failure count in \textit{Survival Game}, where failure count quantifies the extent of subjects' errors. The smaller the failure count, the more intelligent the subject is considered. Therefore, we can treat failure count as a form of loss, which we will refer to as ``Survival Game Loss'' in this subsection. 

Survival Game Loss has a strong physical meaning and naturally reflects the performance of AI systems. Yet, most advanced AI systems are stuck at the Limited Level and Survival Game Loss diverges, making directly adopting this loss infeasible. In the following, we demonstrate that many current AI technologies are profoundly related to the divergence of Survival Game Loss, even though these technologies were not explicitly designed or used with this awareness in mind.


\textbf{Hard Negative Sampling}:
Hard negative sampling is a widely used optimization technique in many AI fields, including vision~\citep{shrivastava2016training}, search~\citep{zhan2021optimizing}, recommendation~\citep{ding2020simplify}, and language~\citep{kalantidis2020hard} tasks. It penalizes the model's top-k most incorrect predictions (i.e., hard negatives), rather than punishing all of its wrong predictions (i.e., random negatives). Researchers explain its effectiveness with various hypotheses, such as increasing gradient magnitudes~\citep{xiong2021approximate}, bootstrapping the training data~\citep{shrivastava2016training}, simulating an easy-to-hard curriculum learning process~\citep{chen2021curriculum}, etc. However, from the perspective of Survival Game Loss, its effectiveness becomes easy to understand. Since Hard Negative Sampling focuses on top-$k$ errors, its loss can be seen as $\min (\text{Survival Game Loss}, k)$. By truncating Survival Game Loss with $k$, this approach ensures convergence. This truncation operation gives up on the difficult cases where models fail more than $k$ times before finding the correct solutions. It only optimizes performance in easy cases where the failure count can be smaller than $k$. Ignoring poor performance in difficult cases enables the model to focus on improving accuracy in simple cases. This approach is effective for Limited-Level intelligence, but if the model could reach the Autonomous Level, there would be no need to ignore difficult cases, and this method would not be so effective.

An interesting story about hard negative sampling is that we studied its effectiveness years ago and demonstrated its effectiveness in ignoring difficult cases~\citep{zhan2021optimizing}. Yet, it is only now that we realize how deeply it relates to the essence of intelligence.

\textbf{Cross-Entropy Loss}:
Cross-entropy loss is a commonly used loss function, widely applied across tasks such as vision~\citep{oord2018representation, radford2021learning} and language~\citep{radford2019language, izacard2021unsupervised}. Earlier researchers provided heuristic explanations for its effectiveness, such as making the neural network’s embedding distribution more uniform~\citep{wang2020understanding} and automatically weighting different negatives~\citep{chen2020simple}. Yet, from the perspective of Survival Game Loss, its role becomes clearer. It can be seen as using $\log (\text{Survival Game Loss})$ as the loss. $\log$ transformation leads to better convergence. For instance, if Survival Game Loss follows a power-law distribution with an exponent between 1 and 2, its expectation does not converge, but the $\log$ transformation does. In this way, Cross-Entropy loss helps address the divergence of Survival Game Loss and thus makes the training process more effective. Nevertheless, if models were at the Autonomous Level, it would not be so effective since the loss would already be convergent.


\textbf{Reinforcement Learning} (RL):
RL~\citep{kaelbling1996reinforcement} lets AI systems explore solutions themselves and rewards them when they succeed. It is similar to how animals learn. However, its application is limited because the training cost is prohibitively high~\citep{dulac2021challenges}. We can explain this high training cost based on the convergence of Survival Game Loss at Limited Level. The cost of RL is closely related to the number of failed attempts, which is indeed Survival Game Loss. Therefore, the cost is infinite at Limited Level, making RL infeasible. Recently, DeepSeek-R1~\citep{guo2025deepseek} shows that RL can be applied in mathematical and coding tasks. This is because current advanced models are approaching the Capable Level in the two areas, as reflected by our previous experiments in Section~\ref{sec:code_experiment} and \ref{sec:math_experiment}. 
Models at Capable Level are more likely to find correct answers and their cost in the RL process is much lower. Yet for many other tasks, such as writing, current models are still far from the Capable Level, making RL difficult to apply.


We can see that \textit{Survival Game} provides deep insights for understanding AI techniques. Although earlier researchers mainly designed algorithms based on heuristics without the knowledge of this test, \textit{Survival Game} effectively reveals the fundamental reasons behind their success. With the guidance of \textit{Survival Game}, we believe researchers will easily design more advanced AI techniques in the future.


