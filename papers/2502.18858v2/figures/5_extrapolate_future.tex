\section{Scaling in Survival Game}
\label{sec:future_prediction}

In this section, we make predictions about the model size needed to achieve the Autonomous Level in language tasks. 
We begin by introducing the empirical relationship between model size and decay rate in \textit{Survival Game}. Using this relationship, we will extrapolate to even larger model sizes, making predictions about the future trajectory of AI development.


\subsection{Fitting Failure Count Distribution}
\label{sec:fit_failure_count_distribution}

\input{tables/fit_model_perf.tex}

In this subsection, we aim to quantitatively characterize the performance of different models in \textit{Survival Game}. Take a look at the experimental results from the previous section, such as Figure~\ref{fig:language_domain_results} and \ref{fig:multilingual_results}. The distribution of failure count is close to a straight line in a log-log plot, especially for failure count between $10$ and $100$. When the failure count exceeds $100$, the data points become scattered. When the failure count is less than $10$, the distribution is a curve that bends downward with an increasingly steeper slope. Since a straight line in a log-log plot suggests a power law distribution, this observation suggests that the failure count distribution can be approximated by a power law, especially when the failure count is neither too small nor too large.

Therefore, we use the power law to fit the distribution of the failure count and directly use the exponent obtained from the power law fit as the subject's decay rate. In this way, we further quantify the subject's intelligence from the distribution of failure count to a fitted number of its decay rate. Thus, we no longer use a log-log plot to see where the distribution falls as suggested in Section~\ref{sec:decay_rate_classification}, but instead, directly compare the fitted decay rate with $2$ and $3$. If the decay rate is less than $2$, the subject is classified as Limited Level. If the decay rate is between $2$ and $3$, the subject is classified as Capable Level. If the decay rate is greater than $3$, the subject is classified as Autonomous Level.

We validate the fitting quality of this approach in Figure~\ref{fig:fit_decay_with_powerlaw}. The task is language writing tasks, and the three rows correspond to Wikipedia, code, and general domains, respectively. We evaluate models of different sizes, ranging from 0.5B to 72B. The x-axis represents the failure count, and the y-axis represents the frequency. Both axes are on a logarithmic scale. We fit the distribution within the failure count range of $10$ to $100$. We can see that data points closely follow a power law distribution across domains and model sizes. The $R^2$ values for the fits are marked on the figure and are very close to $1$, suggesting that the fitting quality is near perfect. Thus, we use this setup to obtain the decay rate in this section.


\subsection{Fitting Effect of Scaling}
\label{sec:fit_model_scale_to_decay}


Now, we empirically examine how model sizes relate to failure decay rate. We conduct experiments with models from Qwen 2.5 series~\citep{qwen2, qwen2.5}. The model sizes range from 0.5B to 72B. We use these models because they are state-of-the-art in terms of their respective parameter sizes and can even approach the performance of models that are ten times larger~\citep{guo2025deepseek}. Using such advanced models allows us to draw conclusions that represent the current cutting-edge technology.


\input{tables/current_fit.tex}

Figure~\ref{fig:fit_model_size} illustrates the impact of model size on failure decay rate. The x-axis represents the model size, and the y-axis represents the fitted decay rate, both on a logarithmic scale. The circle markers represent the performance of the Qwen models. In general, these points approximately follow a straight line. Among these tasks, the points are the closest to a straight line in Knowledge QA (NQ and Trivia QA) and Coding tasks. 
In other tasks, we observe that the curve formed by the data points begins to bend downward as the model size becomes larger. This suggests that increasing the model size with the current training techniques will result in a sub-log-linear improvement rate. If this phenomenon holds, we would seriously overestimate the performance when we use a straight line on the log-log plot to predict the decay rate of larger models. Yet, from an optimistic perspective, we can attribute this slowing growth trend to the limitations of current training methods. Specifically, it is enough to use current training techniques and data size for training small models. As a result, the improvements when model sizes are small fall along the straight line on the log-log plot. However, current data and techniques are limited when the model size reaches 32B or 72B. Consequently, the improvements fall short of expectations. Optimistically, if future researchers make breakthroughs in training techniques, the performance of these large models could still return to the straight line on the log-log plot.

Besides the results of QWen 2.5, we also show the performance across different model architectures in Figure~\ref{fig:full_perf}. The models include GPT2~\citep{radford2019language}, OPT~\citep{zhang2022opt}, GPT-Neo~\citep{gpt-neo}, Llama-1~\citep{touvron2023llama1}, Llama-2~\citep{touvron2023llama2}, Llama-3~\citep{dubey2024llama}, Phi-2~\citep{javaheripi2023phi}, GLM4~\citep{glm2024chatglm}, DeepSeek-V2~\citep{deepseekv2}, BaichuanM1~\citep{baichuan_m1}, Mistral~\citep{jiang2023mistral7b}, QWen2.5~\citep{qwen2.5}.
We can see that across different model architectures, larger models tend to perform better, forming an approximately linear trend. This is evident in models such as the OPT series, Llama 3 \& 3.2 series. This observation aligns with our idea of fitting a straight line. Additionally, we observe that more recently released models tend to achieve better performance over time. Therefore, optimistically speaking, although the fitted line slightly overestimates the performance of large models, their performance are expected to improve over time. Moreover, we can see that among these models, Qwen2.5 demonstrates state-of-the-art performance. As a result, we use Qwen2.5 as a benchmark to represent the current cutting-edge level.



In this paper, we adopt this optimistic perspective. We model the effect of model size on failure decay rate as a straight line on the log-log plot and assume that this linear approximation is still valid when researchers train models of larger scales. We proceed with this optimistic fitting approach into the next subsection, where we predict the development of scaling AI in the future.

\subsection{Predicting Future}



\input{tables/future_pred.tex}

We optimistically assume that, as the model size increases, the failure decay rate improves along a straight line on a log-log plot. We fit the straight line based on current models and extrapolate the straight line to predict models with larger scales in the future. Figure~\ref{fig:future_pred} shows the results of this extrapolation. The x-axis represents the model size, and the y-axis represents the failure decay rate, both on a logarithmic scale. We calculate the model size required to enable the decay rate to $3$, which corresponds to the Autonomous Level.
The results indicate that for structured tasks, such as mathematics, law, and coding, the required parameter size is around $10^{18}$. These tasks are governed by clear rules and formats, which facilitate learning for AI systems and require fewer parameters. On the other hand, more complex tasks, such as knowledge-based question answering, patent applications, and writing medical or academic papers, require a parameter size around $10^{21}$. These tasks are more intricate, demand specialized knowledge, and require sophisticated reasoning abilities. Thus, AI systems need to be very large to grasp them.
Finally, we adopt a general task that requires the model to comprehend all the information on the Internet. Specifically, we use the C4 English dataset~\citep{raffel2020exploring}, which contains high-quality English corpus from the Internet. We test whether models can memorize the information by asking models to predict the next word given all previous words. According to the prediction results, the required parameter size for Autonomous Level is astonishing, around $10^{26}$.


Achieving such a scale with current hardware is virtually impossible. In the case of general language tasks, the required scale is on the order of $10^{26}$. This number is even $5$ orders of magnitude higher than the total number of neurons in all of humanity's brains combined. Specifically, the number of neurons in a single human brain is around $10^{11}$, and considering the global population is approximately $10^{10}$, the total number of neurons in all human brains is about $10^{21}$. This is only $10^{-5}$ of the scale needed for the AI model. From this perspective, building such a large AI model would be like creating a machine with computational complexity far greater than the total computational capacity of the human species. 
If we ignore any computational costs, such as training and inference, and just focus on loading this massive model onto H100 GPUs, here's the calculation: Since the memory of an H100 GPU is 80GB, we would need $5 \times 10^{15}$ GPUs. Based on the cost of H100 GPU (\$30,000) and the market value of Apple Inc. (\$3.7 trillion) in February 2025, the total value of these GPUs would be equivalent to $4 \times 10^7$ times the market value of Apple Inc. As we can see, without breakthroughs in hardware and AI technology, it is infeasible to afford scaling for Autonomous-Level intelligence.

If hardwares like GPU and CPU continue to improve with the rate suggested by Moore's Law, we can predict the time when sufficiently large models can be developed. Moore's Law states that the performance of chips doubles approximately every 18 months. As chip performance doubles, we can also double the size of AI systems without too much cost.
Assuming Moore's Law continues to hold, and taking the current maximum trainable model size as 1 trillion parameters, we can forecast the maximum trainable model size at each time in the future. In Figure~\ref{fig:future_pred}, the top x-axis shows the predicted timeline. The results suggest that for structured tasks such as mathematics, law, and coding, it will take approximately 30 more years before sufficiently large models can be trained to achieve Autonomous Level. For more complex tasks, such as question answering, patent applications, and writing medical or academic papers, we project that it will take about 40 years. Finally, for general tasks, which require models to handle the full breadth of knowledge across various domains, we anticipate that it will take 70 years to train sufficiently large models.
This projection provides a timeline for the future of AI development. Nevertheless, a model with larger scale require more training data and corresopnding training techniques. Even with such hardware improvement, it is still important to achieve breakthrough in stably training large models with only limited data.




