\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{enumitem}
\usepackage{hyperref}
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\usepackage[textsize=tiny]{todonotes}

\def\tilde{\widetilde}
\def\EDir{E_{\mathrm{Dir}}}
\def\EProj{E_{\mathrm{Proj}}}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{color}
\usepackage{makecell}
\usepackage{comment}
\usepackage{tabularx}
\usepackage{bbm, dsfont}

\newcommand\realCaseMetricsWidth{60mm}
\newcommand*{\vertbar}{\rule[-1ex]{0.5pt}{2.5ex}}
\newcommand*{\horzbar}{\rule[.5ex]{2.5ex}{0.5pt}}
\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\newcommand{\proj}{\mathcal{P}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\comp}{\circ}
\newcommand{\cone}{\mathcal{K}}
\newcommand{\interior}{\mathrm{Int}}
\newcommand{\dist}{d_H}
\newcommand{\distT}{d_T}
\newcommand{\cgeq}{\geq_{\cone}}
\newcommand{\cleq}{\leq_{\cone}}
\newcommand{\cll}{\ll_{\cone}}
\newcommand{\cgg}{\gg_{\cone}}

\DeclareMathOperator*{\argmax}{arg\:max}
\DeclareMathOperator*{\argmin}{arg\:min}
\newcommand{\Nrank}{\mathrm{NumRank}}

\icmltitlerunning{Rethinking Oversmoothing in Graph Neural Networks: A Rank-Based Perspective}

\begin{document}



\twocolumn[
\icmltitle{Rethinking Oversmoothing in Graph Neural Networks: \\A Rank-Based Perspective}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.

\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Kaicheng Zhang}{equal,uoe}
\icmlauthor{Piero Deidda}{equal,gssi,sns}
\icmlauthor{Desmond Higham}{uoe}
\icmlauthor{Francesco Tudisco}{uoe,gssi}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{gssi}{Gran Sasso Science Institute, L’Aquila, Italy}
\icmlaffiliation{sns}{Scuola Normale Superiore, Pisa, Italy}
\icmlaffiliation{uoe}{School of Mathematics and Maxwell Institute, University of Edinburgh, Edinburgh, UK}

\icmlcorrespondingauthor{}{}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}
\vskip 0.3in
]




% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Oversmoothing is a fundamental challenge in graph neural networks (GNNs): as the number of layers increases, node embeddings become increasingly similar, and model performance drops sharply. 
Traditionally, oversmoothing has been quantified using metrics that measure the similarity of neighbouring node features, such as the Dirichlet energy. 
While these metrics are related to oversmoothing, we argue they have critical limitations and fail to reliably capture oversmoothing in realistic scenarios. For instance, they provide meaningful insights only for very deep networks and under somewhat strict conditions on the norm of network weights and feature representations. 
As an alternative, we propose measuring oversmoothing by examining the numerical or effective rank of the feature representations. We provide theoretical support for this approach, demonstrating that the numerical rank of feature representations converges to one for a broad family of nonlinear activation functions under the assumption of nonnegative trained weights. 
To the best of our knowledge, this is the first result that proves the occurrence of oversmoothing without assumptions on the boundedness of the weight matrices. 
Along with the theoretical findings, we provide extensive numerical evaluation across diverse graph architectures. Our results show that rank-based metrics consistently capture oversmoothing, whereas energy-based metrics often fail. Notably, we reveal that a significant drop in the rank aligns closely with performance degradation, even in scenarios where energy metrics remain unchanged.
\end{abstract}

\section{Introduction}
\label{}

Graph neural networks (GNNs) have emerged as a powerful framework for learning representations from graph-structured data, with applications spanning knowledge retrieval and reasoning \cite{pengKnowledgeGraphsOpportunities2023, tianKnowledgeGraphKnowledge2022}, personalised recommendation systems \cite{damianouGraphFoundationModels2024, pengSVDGCNSimplifiedGraph2022}, social network analysis \cite{fanGraphNeuralNetworks2019}, and 3D mesh classification \cite{shiPointGNNGraphNeural2020}. Central to most GNN architectures is the message-passing paradigm, where node features are iteratively aggregated from their neighbours and transformed using learned functions, such as multi-layer perceptrons or graph-attention mechanisms.



However, the performance of message-passing-based GNNs is known to deteriorate after only a few layers, essentially placing a soft limit on the depth of GNNs. This is often linked to the observation of increasing similarity between learned features as GNNs deepen and is named oversmoothing \cite{liDeeperInsightsGraph2018,ntRevisitingGraphNeural2019}. 

Over the years, oversmoothing in GNNs, as well as methods to alleviate it, have been studied based on the decay of some node feature similarity metrics, such as the Dirichlet energy and its variants \cite{oonoGraphNeuralNetworks2019, caiNoteOversmoothingGraph2020,bodnarNeuralSheafDiffusion2022,nguyenRevisitingOversmoothingOversquashing2022,digiovanniUnderstandingConvolutionGraphs2023,wuDemystifyingOversmoothingAttentionbased2023,rothRankCollapseCauses2023}. At a high level, most of these metrics directly measure the norm of the absolute deviation from the dominant eigenspace of the message-passing matrix. In linear GNNs without bias terms, this eigenspace is often known and easily computable via e.g.\ the power method. However, when nonlinear activation functions or biases are used, the dominant eigenspace may change, causing these oversmoothing metrics to fail and give false negative signals about the oversmoothing state of the learned features. 

Therefore, these metrics are often considered as providing sufficient but not necessary evidence for oversmoothing  \cite{ruschSurveyOversmoothingGraph2023}. Despite this, there is a considerable body of literature using these somewhat unreliable metrics as part of their evidence for non-occurrence of oversmoothing in GNNs \cite{zhouDirichletEnergyConstrained2021,chenPreventingOversmoothingHypergraph2022,ruschGraphCoupledOscillatorNetworks2022,wangACMPAllenCahnMessage2022,maskeyFractionalGraphLaplacian2023,nguyenCoupledOscillatorsGraph2023,ruschGradientGatingDeep2023, eppingGraphNeuralNetworks2024,scholkemperResidualConnectionsNormalization2024,wangNonconvolutionalGraphNeural2024}.


As we show in \Cref{sec:experiments}, the performance degradation of GNNs trained on real datasets often happens well before any noticeable decay in these oversmoothing metrics can be observed. Most empirical studies in the literature that observe the decay of the Dirichlet-energy-like metrics are conducted over the layers of the same very deep untrained (with randomly sampled weights) or effectively untrained\footnote{Deep networks (with, say, over 100 layers) that are trained but whose loss and accuracy remain far from acceptable.} GNNs \cite{ruschGraphCoupledOscillatorNetworks2022,wangACMPAllenCahnMessage2022,ruschGradientGatingDeep2023,wuDemystifyingOversmoothingAttentionbased2023}, where the decay of the metrics is likely driven by the small weight initializations reducing all features to zero. Instead, we observe that when GNNs of different depths are separately trained, these metrics do not correlate well with their performance degradation.

Furthermore, we note that these metrics can only indicate oversmoothing when their values converge exactly to zero, corresponding to either an exact alignment to the dominant eigenspace or to the feature representation matrix collapsing to the all-zero matrix. This double implication presents an issue: in realistic settings with a large but not excessively large number of layers, we may observe the decay of the oversmoothing metric by, say, two orders of magnitude while still being far from zero. In such cases, it is unclear whether the features are aligning with the dominant eigenspace, simply decreasing in magnitude, or exhibiting neither of the two behaviours. As a result, these types of metrics provide little to no explanation for the degradation of GNN performance. 


As an alternative to address these shortcomings, we advocate for the use of a continuous approximation of the rank of the network's feature representations to measure oversmoothing. Our experimental evaluation across various GNN architectures trained for node classification demonstrates that continuous rank relaxations, such as the numerical rank and the effective rank, correlate strongly with performance degradation in independently trained GNNs—even in settings where popular energy-like metrics show little to no correlation.

Overall, the main contributions of this paper are as follows:
\begin{itemize}[leftmargin=*,noitemsep,topsep=0pt]
    \item We review popular oversmoothing metrics in the current literature and provide a novel perspective on their theoretical analysis based on nonlinear activation eigenvectors. 
    \item Based on this analysis and observations on the linear case, we propose the rank as a better metric for quantifying oversmoothing, and thereby re-defining oversmoothing in GNNs as the convergence towards a low-rank matrix rather than to a matrix of exactly rank one.
    \item We prove the convergence of the numerical rank towards one for linear GNNs and nonlinear GNNs where the eigenvector of the message-passing matrix is also the eigenvector of the nonlinear activation function under the assumption of non-negative weights. To our knowledge, this is the first theoretical result proving that oversmoothing may occur independently of the weights' magnitude with nonlinear activation functions.
    \item We provide extensive numerical evidence that continuous rank relaxation functions better capture oversmoothing than commonly used Dirichlet-like metrics.
\end{itemize}

\section{Background}

\subsection{Graph Convolutional Network} 
Let $\mathcal G=(\mathcal V,\mathcal E)$ be an undirected graph with $\mathcal V$ denoting its set of vertices and $\mathcal E\subseteq \mathcal V\times \mathcal V$ its set of edges. Let $\tilde{A}\in\mathbb R^{N\times N}$ be the unweighted adjacency matrix with $N = |\mathcal V|$ being the total number of nodes, $|\mathcal E|$ being the total number of edges of $\mathcal G$ and $A$ the corresponding symmetric adjacency matrix normalized by the node degrees:
\begin{equation}\label{eq_norm_adjacency_matrix}
A = \tilde D^{-
1/2}\tilde A\tilde D^{-1/2},
\end{equation}
where $\tilde D = D + I$, $D$ is the diagonal degree matrix of the graph $\mathcal G$, and $I$ is the identity matrix. The rows of the feature matrix $X\in\mathbb R^{N\times d}$ are the concatenation of the $d$-dimensional feature vectors of all nodes in the graph. At each layer $l$, the GCN updates the node features as follows 
\begin{align}
    X^{(l+1)} = \sigma(AX^{(l)}W^{(l)})  \label{eq:GCN}
\end{align}
where %$P$ can be replaced by $P'$, 
$\sigma$ is a nonlinear activation function, 
applied componentwise, and $W^{(l)}$ is a trainable weight matrix. 




\subsection{Graph Attention Network} 
While GCNs use a fixed normalized adjacency matrix to perform graph convolutions at each layer, Graph Attention Networks (GATs) \cite{velickovicGraphAttentionNetworks2017, brodyHowAttentiveAre2021} perform graph convolution through a layer-dependent message-passing matrix $A^{(l)}$ learned through an attention mechanism as follows
\begin{equation}
   A^{(l)}_{ij} = \text{softmax}_j(\sigma_a(p_1^{(l)\top} W^{(l)\top} X_i + p_2^{(l)\top} W^{(l)\top} X_j)) \label{eq:GAT}
\end{equation}
where $p_i^{(l)}$ are learnable parameter vectors, $X_i,X_j$ denote the feature of the $i$th and $j$th nodes respectively, the activation $\sigma_a$ is typically chosen to be $\text{LeakyReLU}$, and $\text{softmax}_j$ corresponds to the row-wise normalization
\begin{equation}
   \text{softmax}_j(A_{ij})= \frac{\exp(A_{ij})}{\sum_{j'}\exp(A_{ij'}) }.
\end{equation} 
The corresponding feature update is 
\begin{equation}\label{eq_gat}
X^{(l+1)} = \sigma(A^{(l)}X^{(l)}W^{(l)}) \, .
\end{equation}


\section{Oversmoothing}
Oversmoothing can be broadly understood as an increase in similarity between node features as inputs are propagated through an increasing number of message-passing layers, leading to a noticeable decline in GNN performance. However, the precise definition of this phenomenon varies across different sources. Some works define oversmoothing more rigorously as the alignment of all feature vectors with each other. This definition is motivated by the behaviour of a linear GCN:
%
\begin{equation}
    X^{(l+1)} = A\cdots A X^{(0)}W^{(0)}\dots W^{(l)}.
\end{equation}
%
Indeed if $\tilde{A}$ is the adjacency matrix of a fully connected graph $\mathcal G$, $A$ will have a spectral radius equal to $1$ with multiplicity $1$, and $A^l$ will thus converge towards the eigenspace spanned by the dominant eigenvector. Precisely, we have
\begin{gather}
A^l \to uv^\top \quad\text{as} \quad l\rightarrow \infty,  \label{eq:linear_osm_proof}
\end{gather} 
where $Au=u$, and $A^\top v=v$, see e.g.\ \cite{tudiscoComplexPowerNonnegative2015}.

As a consequence, if the product of the weight matrices $W^{(0)}\cdots W^{(l)}$ converges in the limit $l\rightarrow \infty$, then the features degenerate to a matrix having rank at most one, where all the features are aligned with the dominant eigenvector $u$. Mathematically, if we assume $u$ to be such that $\|u\|=1$, this alignment can be expressed by stating that the difference between the features and their projection onto $u$, given by $\|X^{(l)} - uu^T X^{(l)}\|$, converges to zero.

\subsection{Existing Oversmoothing Metrics}\label{sec:existing_over_metric}
Motivated by the discussion about the linear case, oversmoothing is thus quantified and analysed in terms of the convergence of some node similarity metrics towards zero. In particular, in most cases, it is measured exactly by the alignment of the features with the dominant eigenvector of the matrix $A$. The most prominent metric that has been used to quantify oversmoothing is the Dirichlet energy, which measures the norm of the difference between the degree-normalized neighbouring node features \cite{caiNoteOversmoothingGraph2020, ruschSurveyOversmoothingGraph2023} 
%
\begin{equation}\label{eq:DirE}
    \EDir(X) = \sum_{(i,j)\in \mathcal E} \left\|\frac{X_i}{u_i} -\frac{X_j}{u_j} \right\|^2_2,  
\end{equation}
%
where $u_i$ is the $i$-th entry of the dominant eigenvector of the message-passing matrix in \eqref{eq_norm_adjacency_matrix}. It thus immediately follows from our discussion on the linear setting that $\EDir(X^{(l)})$ converges to zero as $l\to \infty$ for a linear GCN with converging weights product $W^{(0)}\cdots W^{(l)}$. This intuition suggests that a similar behaviour may occur for ``smooth-enough'' nonlinearities. 

%
In particular, in the case of a GCN, the dominant eigenvector $u$ is defined by $u_i=\sqrt{1+d_i}$ and \cite{caiNoteOversmoothingGraph2020} have proved that, using LeakyReLU activation functions  $\EDir(X^{(l+1)})\leq s_l\bar\lambda \EDir(X^{(l)})$, where $s_l=\|W^{(l)}\|_2$ is the largest singular value of the weight matrix $W^{(l)}$, and $\bar\lambda =(1-\min_i\lambda_i)^2$, where $\lambda_i\in (0,2]$ varies among the nonzero eigenvalues of the normalized graph Laplacian $\tilde\Delta = I - A =I- \tilde D^{-\frac12}\tilde A\tilde D^{-\frac12}$. 

Similarly, in the case of GATs, the matrices $A_i$ are all row stochastic, meaning that $u_i=1$ for all $i$. In this case, in \cite{wuDemystifyingOversmoothingAttentionbased2023}, it has been proved that whenever the product of the entry-wise absolute value of the weights is bounded, that is $\|\Pi_{k=1}^\infty |W^{(k)}|\| < \infty$, then the following variant of the Dirichlet energy decays to zero
\begin{gather}\label{eq:demystify_mu}
    \EProj(X) = \|X-\proj X\|^2_F    
\end{gather}
where $\proj = uu^\top$ is the projection matrix on the space spanned by the dominant eigenvector $u$ of the matrices $A^{(l)}$. In particular, this metric can be used only if the dominant eigenvector $u$ is the same for all $l$; this is, for example, the case with row stochastic matrices or when $A^{(l)}=A$ for all~$l$. 
%
%
\subsection{A unifying perspective based on the eigenvectors of nonlinear activations}
In the following discussion, we present a unified and more general perspective of the necessary conditions to have oversmoothing in the sense of the classical metrics, based on the eigenvectors of a nonlinear activation function. In the interest of space, longer proofs for this and the subsequent sections are moved to \Cref{app:proofs}.
\begin{definition}
    We say that a vector $u\in \mathbb{R}^N\setminus\{0\}$ is an eigenvector of the (nonlinear) activation function $\sigma:\mathbb R^N\to\mathbb R^n$ if for any $t\in \mathbb{R}\setminus\{0\}$, there exists $\mu_t$ such that $\sigma(t u)=\mu_t u$.
\end{definition}
With this definition, we can now provide a unifying characterization of message-passing operators $A^{(l)}$ and activation functions $\sigma$ that guarantee the convergence of the Dirichlet-like energy metric $\EProj$ to zero for the feature representation sequence defined by $X^{(l+1)} = \sigma(A^{(l)}X^{(l)}W^{(l)})$. 
Specifically, Theorem~\ref{thm:main_linear} shows that this holds provided all matrices $A^{(l)}$ share a common dominant eigenvector $u$, which is also an eigenvector of $\sigma$. 

This assumption recurs throughout our theoretical analysis, aligning with existing results in the literature. For example, (a) the proof by \citet{caiNoteOversmoothingGraph2020} applies to GCNs with LeakyReLU, where the dominant eigenvector of $A$ is nonnegative by the Perron-Frobenius theorem and, therefore, an eigenvector of LeakyReLU; and (b) the proof by \citet{wuDemystifyingOversmoothingAttentionbased2023} holds for stochastic message-passing matrices $A^{(l)}$, which inherently share a common dominant eigenvector with constant entries, that is an eigenvector for any pointwise nonlinear activation function.



\begin{lemma}\label{lemma_Norm_of_proj}
    Let $X^{(l+1)}=\sigma(A^{(l)} X^{(l)} W^{(l)})$ be the $l$-th layer of a GNN. Assume that $A^{(l)}$ has a simple dominant eigenvalue with corresponding eigenvector $u$  that is also an eigenvector of $\sigma$. If $\sigma$ is $1$-Lipschitz, namely $\|\sigma(x)-\sigma(y)\|\leq \|x-y\|$ for any $x,y$, then 
    %
    $$\|(I-\mathcal{P})X^{(l+1)}\|_F\leq \|(I-\mathcal{P})A^{(l)} X^{(l)}W^{(l)}\|_F,$$
    %
    where $\mathcal{P}=uu^T/\|u\|^2$ is the projection matrix on the linear space spanned by $u$.
\end{lemma}

The following theorem follows as a consequence.

\begin{theorem}\label{thm:main_linear}
With the same notation of Lemma~\ref{lemma_Norm_of_proj}, if
\[
\lim_{L\to \infty}\prod_{l=0}^L\|(I-\proj)A^{(l)}\|_2 \|W^{(l)}\|_2=0, 
\] 
then $\EProj(X^{(l)})\to 0$ as $l\to \infty$. 
\end{theorem}
\begin{proof}
    The proof follows observing that in the decomposition $(I-\proj)A^{(l)}=(I-\proj)A^{(l)}\proj+(I-\proj)A^{(l)}(I-\proj)$ 
the matrix $(I-\proj)A^{(l)}\proj$ is zero because $A^{(l)}u=\lambda_1^l u$ for any $l$. Thus 
\begin{equation*}
    (I-\proj)A^{(l)}=(I-\proj)A^{(l)}(I-\proj),
\end{equation*}
and, from \Cref{lemma_Norm_of_proj} and the inequality $\|AB\|_F\leq \|A\|_2\|B\|_F$, we have  
    $\|(I-\proj)X^{(L)}\|_F\leq \Big(\Pi_{l=0}^{L-1}\|(I-\proj)A^{(l)}\|_2\|W^{(l)}\|_2\Big) \|X^{(0)}\|_F$.
\end{proof}

Note, in particular, that in the case of GCNs, the matrix $A^{(l)} = A$ is symmetric, and thus $\|I - \proj A^{(l)}\|_2 = \lambda_2$. Therefore, when $\sigma = \text{LeakyReLU}$, we obtain the result by \citet{caiNoteOversmoothingGraph2020} as convergence to zero is guaranteed if $\|W^{(l)}\|_2 \leq \lambda_2$. Recall that, as discussed above, the choice $\sigma = \text{LeakyReLU}$ satisfies our eigenvector assumption since $u \geq 0$ by the Perron-Frobenius theorem, and thus LeakyReLU$(t u) = \alpha t u$ with $\alpha$ depending only on the sign of $t$. 
Similarly, in the case of GATs, the matrices $A^{(l)}$ are stochastic for all $l$, implying that $u = \mathbbm{1}$ is the constant vector with $(u)_i = 1$ for all $i$. If $\sigma = \otimes \psi$ is a nonlinear activation function acting entrywise through $\psi$, then $\sigma(t\mathbbm{1}) = \psi(t) \mathbbm{1}$. Therefore, Theorem~\ref{thm:main_linear} implies that if the weights are sufficiently small, the features align independently of the activation function used. This is consistent with the results in \cite{wuDemystifyingOversmoothingAttentionbased2023}. 
However, we note that the bounds on the weights required by Theorem~\ref{thm:main_linear} and those in \cite{wuDemystifyingOversmoothingAttentionbased2023} on the weights $W^{(l)}$ are not identical, and it is unclear which of the two is more significant. Nonetheless, in both cases, having bounded weights along with any $1$-Lipschitz pointwise activation function is a sufficient condition for observing oversmoothing via $\EProj$ in GATs. In addition to offering a different and unifying theoretical perspective on the results in \cite{caiNoteOversmoothingGraph2020,wuDemystifyingOversmoothingAttentionbased2023}, we highlight the simplicity of our eigenvector-based proof, which we hope provides added clarity on this phenomenon.

\section{Energy-like metrics: what can go wrong}

Energy-like metrics such as $\EDir$ and $\EProj$ are among the most commonly used oversmoothing metrics. However, they suffer from inherent limitations that hinder their practical usability and informational content.

One important limitation of these metrics is that they indicate oversmoothing only in the limit of infinitely many layers, when their values converge exactly to zero. Since they measure a form of absolute distance, a small but nonzero value does not provide any meaningful information. On the other hand, convergence to zero corresponds to either perfect alignment with the dominant eigenspace or the collapse of the feature representation matrix to the all-zero matrix. While the former is a symptom of oversmoothing, the latter does not necessarily imply oversmoothing. Moreover, this convergence property requires the weights to be bounded. However, in practical cases, performance degradation is observed even in relatively shallow networks, far from being infinitely deep, and with arbitrarily large (or small) weight magnitudes. This aligns with our intuition and what occurs in the linear case. Indeed, for a linear GCN, even when the features $X^{(l)}$ grow to infinity as $l\to\infty$, one observes that $X^{(l)}$ becomes dominated by the dominant eigenspace of $A$, even for finite and possibly small values of $l$, depending on the spectral gap of the graph.

More precisely, the following theorem holds:
%
\begin{theorem}\label{theorem_decay_linear_model}
Let $X^{(l+1)}=AX^{(l)}W^{(l)}$ be a linear GCN. Let $\lambda_1, \lambda_2$ be the largest and second-largest eigenvalues (in modulus) of $A$, respectively. Assume the weights $\{W^{(l)}\}_{l=1}^\infty$ are randomly sampled from i.i.d.\ random variables with distribution $\nu$ such that 
\[
\int \log^+(\|W\|) d\nu + \int \log^+(\|W^{-1}\|) d\nu < \infty
\] 
with $\log^+(t)=\max\{\log(t),\,0\}$.
If $|\lambda_2/\lambda_1| < 1$, then almost surely, it holds that
\[
\lim_{l\to\infty}\frac{\|(I-\mathcal{P})X^{(l)}\|_F}{\|\mathcal{P}X^{(l)}\|_F} = 0
\]
with a linear rate of convergence $O(|\lambda_2/\lambda_1|^l)$. 
\end{theorem}

In particular, the theorem above implies that for a large spectral gap $|\lambda_2/\lambda_1|\ll 1$, $X^{(l)}$ is predominantly of rank one, namely
\[
X^{(l)}= \lambda_1^{l}\Big(uv^T + R(l)\Big)
\]
for some $v$, with $R(l) \sim O(|\lambda_2/\lambda_1|)^l$ and thus converging to $0$ as $l\to \infty$. This results in weakly expressive feature representations, independently of the magnitude of the feature weights. This phenomenon can be effectively captured by measuring the rank of $X^{(l)}$, whereas Dirichlet-like energy measures may fail to detect it.

Another important limitation of these metrics is their dependence on a specific, known dominant eigenspace, which must either be explicitly known or computed in advance. Consequently, their applicability is strongly tied to the specific architecture of the network. In particular, the dominant eigenvector $u$ of $A^{(l)}$ must be known and remain the same for all $l$. This requirement excludes their use in most cases where $A^{(l)}$ varies with $l$, such as when $A^{(l)}$ is the standard weighted adjacency matrix of the graph.




\section{The Rank as a Measure of Oversmoothing}\label{sec:rank_measure_oversmoothin}
Inspired by the behaviour observed in the linear case, we argue that measuring the rank of feature representations provides a more effective way to quantify oversmoothing, in alignment with recent work on oversmoothing \cite{guoContraNormContrastiveLearning2023}. However, since the rank of a matrix is defined as the number of nonzero singular values, it is a discrete function and thus not suitable as a measure. A viable alternative is to use a continuous relaxation that closely approximates the rank itself.

Examples of possible continuous approximations of the rank include the numerical rank, the stable rank, and the effective rank~\cite{royEffectiveRankMeasure2007a, rudelsonSamplingLargeMatrices2006, aroraImplicitRegularizationDeep2019}. The stable rank is defined as $\text{StableRank}(X) = \|X\|_*^2/\|X\|_F^2$, 
where $\|X\|_* = \sum_i \sigma_i$ is the nuclear norm. The numerical rank is given by  $\text{NumRank}(X) = \|X\|_F^2/\|X\|_2^2$. Finally, given the singular values $\sigma_1 > \sigma_2 > \dots > \sigma_{\min\{N,d\}}$ of $X$, the effective rank is defined as
\begin{gather} \label{eq:erank}
   \text{Erank}(X) = \exp\left( -\textstyle{\sum_k p_k \log p_k}\right),
\end{gather} 
where $p_k = \sigma_k/ \sum_i \sigma_i$ is the $k$-th normalized singular value. These rank relaxation measures exhibit similar empirical behaviour as shown by our numerical evaluation in Section~\ref{sec:experiments}.

In practice, measuring oversmoothing in terms of a continuous approximation of the rank is a reasonable approach that helps address the limitations of Dirichlet-like measures. Specifically, it offers the following advantages:  
(a) it is scale-invariant, meaning it remains informative even when the feature matrix converges to zero or explodes to infinity;  
(b) it does not rely on a fixed, predetermined eigenspace but instead captures convergence of the feature matrix toward an arbitrary lower-dimensional subspace;  
(c) it allows for the detection of oversmoothing in shallow networks without requiring exact convergence to rank one—since a small value of, say, the effective rank, directly implies that the feature representations are low-rank, which in turn suggests a potentially suboptimal network architecture.


In Table~\ref{tab:toy_example}, we present a toy example illustrating that classical oversmoothing metrics fail to correctly capture oversmoothing unless the features are perfectly aligned. This observation implies that these metrics can quantify oversmoothing only when the rank of the feature matrix converges exactly to one. In contrast, continuous rank functions provide a more reliable measure of approximate feature alignment. 
Later, in Figure~\ref{fig:metric_cora_eg}, we demonstrate that the same phenomenon occurs in GNNs trained on real datasets, where exact feature alignment is rare. In such cases, classical metrics remain roughly constant, whereas the rank decreases, coinciding with a sharp drop in GNN accuracy.

\begin{table}[t]
    \centering
    \caption{A set of toy scenarios depicting the behaviour of oversmoothing metrics. Each plot contains 50 nodes, each with two features plotted as the x and y axis. \textbf{\#\,1}: all features are of the same value; \textbf{\#\,2}:  all features are assumed to align with the same vector exactly; \textbf{\#\,3}: shifts one (red) point in \#\,2 in y-direction; \textbf{\#\,4}: samples all points from a uniform distribution. %$\mu$ and 
    MAD (see \Cref{sec:experiments}) and $\EDir$ give false negative signals in \#\,3 while the features are considered oversmoothing by definition. $\EProj$ can hardly differentiate between \#\,3 and \#\,4, and is thus not robust in quantifying oversmoothing. To compute $\EProj$ and $\EDir$ the first feature was considered in place of $u$ in \eqref{eq:DirE} and \eqref{eq:demystify_mu}.}
    \setlength{\tabcolsep}{0.2mm}
    \begin{tabular}{c|c|c|c|c}
        & \#\,1 & \#\,2 & \#\,3 & \#\,4 \\
        & \includegraphics[width=16mm]{imgs/dot.png} & \includegraphics[width=16mm]{imgs/line.png} & \includegraphics[width=16mm]{imgs/dotted_line.png} & \includegraphics[width=16mm]{imgs/gaussian.png} \\
        \hline 
       %$\mu$  & 0 & 6.18 & 6.25 & 5.58 \\
       $\EDir$  &0 & 0 &  13.25 & 77.78 \\
       $\EProj$  & 0 & 0 & 0.83 & 0.97 \\
       MAD & 0 & 0.81 & 0.81 & 0.57 \\
       NumRank & 1 & 1 & 1.01 & 1.78 \\
       Erank & 1 & 1 & 1.36 & 1.99 
    \end{tabular}
    \label{tab:toy_example}
\end{table}


\subsection{Theoretical Analysis of Rank Decay}

In this section, we provide an analytical study proving the decrease of the numerical rank for a broad class of graph neural network architectures under the assumption that the weight matrices are entrywise nonnegative. While this is a somewhat restrictive setting, our result is the first theoretical proof that oversmoothing occurs independently of the weight (and thus feature) magnitude.

We begin with several useful observations. Let $u$ be the dominant eigenvector of $A$ corresponding to $\lambda_1$ and satisfying $\|u\|=1$. Consider the projection matrix $\proj = u u^\top$. Given a matrix $X$, we can decompose it as $X = \mathcal{P}X + (I-\mathcal{P})X$.
Since $u$ is a unit vector, it follows that $\|\mathcal{P}\|_2 = 1$, and therefore,  
%
\begin{equation}\label{projection_reduces_norm}
    \|X\|_2 = \|\mathcal{P}\|_2\|X\|_2 \geq \|\mathcal{P}X\|_2.
\end{equation}
%
Moreover, since $\mathcal{P}X$ and $(I - \mathcal{P})X$ are orthogonal with respect to the Frobenius inner product, we have $\|\mathcal{P}X + (I-\mathcal{P})X\|_F^2 = \|\mathcal{P}X\|_F^2 + \|(I-\mathcal{P})X\|_F^2$.
Thus, we obtain the following bound for the numerical rank of $X$:
\begin{equation}\label{eq_num_rank_upper_bound}
    \begin{aligned}
    \text{NumRank}(X) &= \frac{\|\mathcal{P}X + (I-\mathcal{P})X\|_F^2}{\|X\|_2^2} \\
    &= \frac{\|\mathcal{P}X\|_F^2 + \|(I-\mathcal{P})X\|_F^2}{\|X\|_2^2} \\
    &\leq 1 + \frac{\|(I-\mathcal{P})X\|_F^2}{\|X\|_2^2},
   \end{aligned}
\end{equation}
where we used \eqref{projection_reduces_norm} and the fact that $\|\mathcal{P}X\|_F = \|\mathcal{P}X\|_2$ since $\mathcal{P}X$ is a rank-one matrix.

The above inequality, together with \Cref{theorem_decay_linear_model}, allows us to establish the convergence of the numerical rank for linear networks.

\paragraph{The Linear Case} 
Consider a linear GCN of the form $X^{(l+1)}=AX^{(l)}W^{(l)}$, where $A$ has a simple dominant eigenvalue $\lambda_1$ satisfying $|\lambda_1| \geq |\lambda_2|$. 

We have already noted that $\|X\|_2 \geq \|\proj X\|_2$, meaning that the numerical rank converges to one if $\|(I-\mathcal{P})X\|_F / \|X\|_2$ decays to zero. This occurs whenever the features grow faster in the direction of the dominant eigenvector than in any other direction. As established in \Cref{theorem_decay_linear_model}, this is almost surely the case in linear GNNs. As a direct consequence, we obtain the following result:  

\begin{theorem}
     Let $X^{(l+1)}=AX^{(l)}W^{(l)}$ be a linear GCN. Under the same assumptions as in \Cref{theorem_decay_linear_model}, the following identity holds almost surely:  
    \[
    \lim_{l\rightarrow \infty} \mathrm{NumRank}(X^{(l)})=1.
    \]
\end{theorem}

Extending the result above to general GNNs with nonlinear activation functions is highly nontrivial. In the next section, we present our main theoretical result, which generalizes this analysis to a broader class of GNNs under the assumption of nonnegative weights.




\subsection{The Nonnegative Setting}
To study the case of networks with nonnegative weights, we make use of tools from the nonlinear Perron-Frobenius theory; we refer to \cite{lemmensNonlinearPerronFrobeniusTheory2012,gautier2013nonlinear} and the reference therein for further details. 

We assume all the intermediate features to be in the positive open cone $\cone:=\R_+^N=\{x\in \R^N\:|\:   x_i>0 \;    \forall i=1,\dots, N\}$.
%
On $\cone$, it is possible to introduce the partial ordering
%
\begin{equation}
    x\cleq y \:( x\cll y )\quad \text{iff} \quad y-x\in \bar{\cone}\: (y-x\in \cone)
\end{equation}
where $\bar{\cone}$ denotes the nonnegative closed cone.
%
Given two points $x,y\in \cone$ we write $M(x/y)=\max_{i}x_i/y_i$ and $m(x/y)=\min_{i}x_i/y_i$.
Then, we can define the following Hilbert distance between any two points $x,y\in \cone$,
$$\dist(x,y)=\log(M(x/y)/m(x/y)).$$
Note that $d_H$ is not a distance on $\cone$, indeed $\dist(\alpha x, \beta y)=\dist(x,y)$ for any $x,y\in \cone$ and $\alpha,\beta>0$. However it is a distance up to scaling; that is, it becomes a concrete distance whenever we restrict ourselves to a slice of the cone.
%


%
Now, it is well-known that any matrix $A$ such that $Ax \in \cone$ for any $x\in \cone$ is non-expansive with respect to the Hilbert distance, i.e. 
\begin{equation}\label{eq:contractivity_hilbert}
    \dist(Ax,Ay)\leq \beta \dist(x,y) \qquad \forall x,y \in \cone
\end{equation}
with $0\leq \beta\leq 1$. The last inequality is a consequence of the fact that $A$ preserves the ordering induced by the cone, i.e. if $x\cgeq y$, then $Ax\cgeq Ay$. In particular, we recall that \eqref{eq:contractivity_hilbert} always hold for $\beta<1$ when $A$ is entry-wise strictly positive, as positive matrices map the whole $\bar \cone$ in its interior, i.e.\ $x\cgeq y$ implies $Ax> Ay$. 
In the next definition, we include all the nonnegative matrices that have the same behaviour as the positive matrices but only on a neighbourhood of a positive vector in $\cone$.


Recall that any irreducible nonnegative matrix $A$ has a positive eigenvector $u$ corresponding to its dominant eigenvalue.

\begin{definition}
A family of nonnegative irreducible matrices (possibly one single matrix) $\{A^{(l)}\}$ with the same dominant eigenvector $u>0$ is uniformly contractive with respect to $u$ if for any $C>0$ there exists some $\beta_C<1$ such that $\dist(A^{(l)}x,u)\leq \beta_C\dist(x,u)$ for all $l$ and $x\in \cone$ such that $\dist(x,u)\leq C$.
\end{definition}
%
The above definition is significantly weaker than asking for $A$ to be strictly positive. We show this with an illustrative example in \cref{Illustrative example}.

%
Consider now first the case of a linear function that can be represented in the form 
\begin{equation}\label{eq:linear_part_of_GNN}
    F(X)=AXW,
\end{equation}
where $X$ is a positive $N\times d$ matrix,  $A$ and $W$ are nonnegative $N\times N$ and $d\times d$ matrices, respectively. Then, under mild assumptions, we can prove that the columns of $F(X)$ are closer to the dominant eigenvector $u$ of $A$ as compared to the columns of $X$. 


\begin{lemma}\label{Lemma_hilb_contractivity_linear_network}
    Let $F(X)=AXW$ with $A$ nonnegative and irreducible with dominant eigenvector $u\in\cone$. Assume also $X$ to be striclty positve and $W$ nonnegative with $\min_{j}\max_i W_{ij}>0$. If $A$ is contractive with respect to $u$ and $C=\max_{i}\dist (X_i,u)$ then
    $$
    \max_{i}\dist (F(X)_i,u)\leq \beta_C \max_{i}\dist (X_i,u),
    $$
    where $Y_i$ denotes the $i$-th column of $Y$, and $\beta_C\leq 1$.
    \end{lemma}


Nonlinear Perron-Frobenius theory extends some of the results about nonnegative matrices to particular classes of nonlinear functions. A function $\sigma\in C(\cone,\cone)$ is 
order preserving if given any $x,y\in \cone$ with $x\cgeq y$ then $\sigma(x)\cgeq \sigma(y)$. Moreover $\sigma$ is subhomogenenous if $
\sigma(\lambda x) \cleq \lambda  \sigma(x)$ for all  $x\in \cone$ and any $\lambda>1$. In particular, it is strictly subhomogeneous if $\sigma(\lambda x) \cll \lambda  \sigma(x)$ for all  $x\in \cone$ and $\lambda>1$ and homogeneous if  $\sigma(\lambda x) = \lambda  \sigma(x)$ for all $x\in \cone$ and $\lambda\geq 0$ .
%In particular it is easy to prove that subhomogenity can be equivalently formulated as $\sigma(\lambda x) \cgeq \lambda \sigma(x)$  for all $\forall x\in \cone$ and $\lambda < 1$. 


Subohomogeneity is a useful property and of practical utility. In fact, as discussed in, e.g.\ \cite{sittoniSubhomogeneousDeepEquilibrium2024, piotrowski2024fixed}, it is not difficult to verify that a broad range of activation functions commonly used in deep learning is subhomogeneous on $\cone$. For this family of activation functions, \Cref{Lemma_hilb_contractivity_linear_network} combined with arguments from nonlinear Perron--Frobenius theory yield the following main result


\begin{theorem}\label{thm_collapse_in_hilbert_distance}
    Consider a positive GNN 
    \begin{equation}
    X^{(l+1)}=\sigma(A^{(l)}X^{(l)}W^{(l)})
\end{equation}
with $X^{(0)}_i\in\cone$ for any $i=1,\dots,d$ and $A^{(l)}$ and $W^{(l)}$ nonnegative for any $l$. Assume also that $u\in\cone$ is the dominant eigenvector of all of the matrices $A^{(i)}$ and that it is also an eigenvector of $\sigma$. Then, if the matrices $\{A^{(i)}\}$ are uniformly contractive with respect to $u$ and $\min_j\max_i {W^{(l)}}_{ij}>0$ for any $l$, it holds
%
    $$\lim_{l\rightarrow \infty}\Nrank(X^{(l)})=1.$$
\end{theorem}
%
%
We conclude with a formal investigation of the eigenpairs of activation functions that are entrywise subhomogeneous.
Let $\sigma=\otimes^N \psi$ with $\psi\in C(\R,\R)$ that is subhomogeneous on $\R_+$. Then one can easily show that $\sigma$ is itself subhomogeneous on $\R^N_+$. We have,
\begin{proposition}\label{Lemma_eigenvectors_of_homogeneous_functions}
    Let $\sigma=\otimes^N \psi$  with $\psi\in C(\R_+,\R_+)$ be order preserving. Then: 1) If $\sigma$ is homogeneous, any positive vector is an eigenvector of $\sigma$. 2) If $\sigma$ is strictly subhomogeneous,  the only eigenvector of $\sigma$ in $\cone$ is the constant vector.   
\end{proposition}
As a consequence of the above result, we find that the numerical rank of the features collapses both for GCNs with LeakyReLU activation function (that is homogeneous) as well as for GATs with any kind of subhomogeneous activation function. In fact, the constant vector is always an eigenvector of an entriwise nonlinear map. 







\begin{table*}[t]
    \centering
    \caption{The table lists the correlation coefficients of logarithm of metric values and the classification accuracies for GNNs of depth 2 - 24 layers and separately trained on Cora dataset. For Erank and NumRank, we subtract 1 so that both metrics converge to zero. The classification accuracy ratio are computed on 2-layered and 24-layered GNNs.
    }
    \setlength{\tabcolsep}{1mm}
    %\resizebox{\textwidth}{!}{
    \input{Tables/metric_table_trained}
    %}
    \label{tab:real_nets}
\end{table*}


\section{Experiments} \label{sec:experiments} 

Empirical studies on the evolution of oversmoothing measures often use untrained, hundred-layer-deep GNNs \cite{ruschGraphCoupledOscillatorNetworks2022,wangACMPAllenCahnMessage2022,ruschGradientGatingDeep2023,wuDemystifyingOversmoothingAttentionbased2023}. We emphasize that this is an overly simplified setting. In more realistic settings, as the ones considered in this section, a trained GNN may suffer from significant performance degradation after only few-layers, at which stage the convergence patterns of most of the metrics are difficult to observe. 
The experiments that we present in this section validate the robustness of the effective rank and numerical rank in quantifying oversmoothing in GNNs against the other metrics. 


In particular, we compare how different overmoothing metrics behave compared to the classification accuracy, varying the GNN architectures for node classification on real-world graph data. 
In our experiments, we consider the following metrics:
\begin{itemize}[topsep=0pt, leftmargin=*,itemsep=0pt]
    \item The Dirichlet Energy $\EDir$ \cite{caiNoteOversmoothingGraph2020, ruschSurveyOversmoothingGraph2023} and its variant $\EProj$ \cite{wuDemystifyingOversmoothingAttentionbased2023}. Both are discussed in \cref{sec:existing_over_metric}, see in particular \eqref{eq:DirE} and \eqref{eq:demystify_mu}. 

    \item  Normalized versions of the Dirichlet energy and its variant, namely
    $\EDir(X)/\|X\|_F^2$ and $\EProj(X)/\|X\|_F$.
    Indeed, from our previous discussion, a robust oversmoothing measure should be scale invariant with respect to the features. Metrics with global normalization like the ones we consider here have also been proposed in  \cite{digiovanniUnderstandingConvolutionGraphs2023,rothRankCollapseCauses2023, maskeyFractionalGraphLaplacian2023}.
%    

    \item  The Mean Average Distance (MAD) \cite{chenMeasuringRelievingOversmoothing2020}
     \begin{gather}\label{eq:MAD} 
     \text{MAD}(X) = \frac1{|\mathcal E|} \sum_{(i,j)\in \mathcal E} \left(1 - \frac{X_i^\top X_j}{|X_i||X_j|}\right). 
     \end{gather} 
     It measures the cosine similarity between the neighbouring nodes. Unlike previous baselines, this oversmoothing metric does not take into account the dominant eigenvector of the matrices $A^{(l)}$. 

    \item Relaxed rank metrics: We consider the Numerical Rank and Effective Rank. Both are discussed in \cref{sec:rank_measure_oversmoothin}. We point out that from our theoretical investigation, in particular from \eqref{eq_num_rank_upper_bound}, the numerical rank decays to $1$ faster than the decay of the normalized $\EProj$ energy to zero. This further supports the use of the Numerical Rank as an improved measure of oversmoothing with respect to~$\EProj$.
    \end{itemize}



\begin{figure*}[t]
    \centering
    \caption{Four examples of GCNs corresponding to the first fours rows in table \ref{tab:real_nets}. For Erank and Numrank, we measure Erank$(X)-r^*_\mathrm{ER}$ and $\Nrank(X)-r^*_\mathrm{NR}$ for some $r^*>1$. In these particular cases, $r^*_\mathrm{ER}<1.85$, $r^*_\mathrm{NR}<1.3$. Note that the effective rank and numerical rank of the input features $X^{(0)}$ is about 1084 and 13.6, respectively.
    }
    \includegraphics[width=0.99\linewidth]{imgs/cora-4-subplots.png}
    \label{fig:metric_cora_eg}
\end{figure*}



In \cref{tab:real_nets} and \cref{fig:metric_cora_eg}, we train GNNs of a fixed hidden dimension equal to 32 on the Cora dataset. More results on Cora, as well as other datasets, are reported in \cref{apd:additional_empirical_results}. We follow the standard setups of GCN and GAT as stated in equation \eqref{eq:GCN} and \eqref{eq_gat}, and use homogeneous LeakyReLU (LReLU) and subhomogenous Tanh as activation functions.
%
For the GCN model, we also consider adding additional components, i.e. bias, LayerNorm \cite{baLayerNormalization2016} and PairNorm \cite{zhaoPairNormTacklingOversmoothing2019}.
%
For each configuration, GNNs of eight different depths ranging from 2 to 24 are trained. 
%
The oversmoothing metric and accuracy results are averaged over 10 separately trained GNNs. 
%
All GNNs are trained with NAdam Optimizer and a constant learning rate of 0.01.
%
The oversmoothing metrics are computed at the last hidden layer before the output layer.
%
In \cref{fig:metric_cora_eg} and in \Cref{apd:additional_empirical_results}, we plot the behaviour of the different oversmoothing measures as well as the norm of the features and the accuracy of the trained GNNs when the depth of the network is increased. These figures clearly show that the network suffers a significant drop in accuracy, which is not matched by any visible change in standard oversmoothing metrics. By contrast, the rank of the feature representations decreases drastically, following quite closely the behaviour of the network's accuracy.
%
These findings are further sustained by the results shown in \cref{tab:real_nets}, where we compute the Pearson correlation coefficient between the logarithm of every measure and the classification accuracy of every GNN model. 
%
The use of a logarithmic transformation is based on the understanding that oversmoothing grows exponentially with the length of the network. 
%





\section{Conclusion}
In this paper, we have discussed the problem of quantifying oversmoothing in GNNs. After discussing the limitations of the leading oversmoothing measures, we have proposed the use of the rank of the features as a better measure of oversmoothing. The experiments that we provided validate the robustness of the effective rank against the classical measures.
In addition, we have proved theoretically the decay of the rank of the features for linear and nonnegative GNNs. 

\bibliography{GNN.bib, Maths.bib}
\bibliographystyle{icml2025}






\appendix
\onecolumn

\clearpage

\section{Proofs of the main results}\label{app:proofs}


\subsection{proof of \Cref{lemma_Norm_of_proj}}

    Let $\pi:=\mathrm{span}\{u v^T\,|\; v\in \R^d\}$ be the $1$-dimensional matrix subspace of the rank-$1$ matrices having columns aligned to $u$. Then it is easy to note that given some matrix $X$, $(I-\proj)X$ provides the projection of the matrix $X$ on the subspace $\pi$, i.e. 
    %
    \begin{equation}
        (I-\proj)X=\mathrm{proj}_\pi(X).
    \end{equation}
    %
    Indeed $\langle(I-\proj)X, u v^T\rangle_F=\mathrm{Tr}(v u^T (I-uu^T)X)=0$. In particular, since the projection realizes the minimal distance, we have that 
    %
    \begin{equation}
        \|X-\proj X\|_F\leq \|X- u v^T\|_F \qquad \forall v\in \R^d.
    \end{equation}
    %
    Now observe that $\sigma(uu^T A^{(l-1)}X^{(l-1)}W^{(l-1)})=u\bar{v}^T$ for some $\bar{v}$. Indeed, writing $v^T=u^T A^{(l-1)}X^{(l-1)}W^{(l-1)}$, we have that the $i$-th column of $\sigma(uu^T A^{(l-1)}X^{(l-1)}W^{(l-1)})$ is equal to $\sigma( v_i u)=\bar{v}_i u$ for some $\bar{v}_i$, because $u$ is an eigenvector of $\sigma$. As a consequence we have 
    %
    \begin{equation}
        \begin{aligned}
     \|(I-\proj)X^{(l)}\|_F&\leq \|X^{(l)}-\sigma(u u^T A^{(l-1)}X^{(l-1)}W^{(l-1)})\|_F\\
     &= \|\sigma(A^{(l-1)}X^{(l-1)}W^{(l-1)})-\sigma(uu^TA^{(l-1)}X^{(l-1)}W^{(l-1)})\|_F\\ 
     &\leq \|(I-\proj)A^{(l-1)}X^{(l-1)}W^{(l-1)}\|_F  
       \end{aligned}
    \end{equation}
    where we have used the $1$-Lipschitz property of $\sigma$.
    %



\subsection{Proof for \Cref{theorem_decay_linear_model}}

Start by studying the norm of $(I-\mathcal{P})X^{(l)}$. Then looking at the shape of the powers of the Jordan blocks matrix it is not difficult to note that $\tilde{T}^{l}=O({l\choose N}\lambda_2^{l-N})$ for $l$ larger than $N$. In particular if we look at the explicit expression of $(I-\mathcal{P})X^{(l)}$ 
\begin{equation}
(I-P)X^{(l)}= \begin{pmatrix}
              0 &  (I-\mathcal{P})\Tilde{M}
    \end{pmatrix}\begin{pmatrix}
        0 & 0\\
        0 & \Tilde{T}^{l}
    \end{pmatrix} M^{-1}X^{(0)}W^{(0)}\dots W^{(l-1)},
\end{equation}
we derive the upper bound
\begin{equation}
    \|(I-P)X^{(l)}\|_F\leq C {l\choose N}|\lambda_2|^{l-N}\|X^{(0)}W^{(0)}\dots W^{(l-1)}\|_F,
\end{equation}
for some positive constant $C$ that is independent on $l$.
%

%
Similarly we can observe that 
\begin{equation}
\begin{aligned}
    \|\mathcal{P}X^{(l)}\|_F &\geq \|u^T A^{l}X^{(0)}W^{(0)} \dots W^{(l-1)}\|_F=\\
    %
    &\|\big(\lambda_1^l v_1^T + u^T \tilde{M} O\Big({l\choose N}\lambda_2^{l-N}\Big) \tilde{M'}\big) X^{(0)} W^{(0)}\dots W^{(l-1)}\|_F \geq\\ 
    &|\lambda_1|^l\Big(
    \|v_1^T X^{(0)} W^{(0)}\dots W^{(l-1)}\|_F -\Big\| u^T \tilde{M} O\Big({l\choose N}\Big(\frac{\lambda_2}{\lambda_1}\Big)^l\tilde{M'}X^{(0)} W^{(0)}\dots W^{(l-1)}\Big\|_F\Big)\geq \\
    &|\lambda_1|^l
    \|v_1^T X^{(0)} W^{(0)}\dots W^{(l-1)}\|_F \Big(1- O\Big({l\choose N}\Big|\frac{\lambda_2}{\lambda_1}\Big|^l \frac{\|X^{(0)} W^{(0)}\dots W^{(l-1)}\|_F}{\|v_1^T X^{(0)} W^{(0)}\dots W^{(l-1)}\|_F}\Big).
    \end{aligned}
\end{equation}


Now observe that under the randomness hypothesis from \cite{furstenbergRandomMatrixProducts1983} and more generally from the Oseledets ergodic multiplicative theorem, we have that for almost any $w\in \mathbb{R}^d$ the limit exists and is equal to the maximal Lyapunov exponent of the system, i.e. a constant  $c(\nu)\geq 0$ depending only on the distribution $\nu$,  $\lim_{l\rightarrow \infty}\frac{1}{l}\log\|w^TW^{(0)}\dots W^{(l-1)}\|=c(\nu)$. In particular for any $w$ and $\epsilon>0$ there exists $l_{w,\epsilon}$ sufficiently large such that for any $l>l_{w,\epsilon}$

\begin{equation}
    c(\nu)-\epsilon\leq \frac{1}{l}\log\|w^TW^{(0)}\dots W^{(l-1)}\|<c(\nu)+\epsilon.
\end{equation}
i.e.
\begin{equation}
    e^{l(c(\nu)-\epsilon)}\leq \|w^TW^{(0)}\dots W^{(l-1)}\|<e^{l(c(\nu)+\epsilon)} \quad \forall l\geq l_{w,\epsilon}
\end{equation}
Now take as vector $w$ first the rows of $X^{(0)}$ and then the vector $v_1^TX^{(0)}$, than almost surely for any $\epsilon$ there exists $l_{\epsilon}$ such that for any $l>l_\epsilon$
\begin{equation}
    e^{l(c(\nu)-\epsilon)}\leq \|w^TW^{(0)}\dots W^{(l-1)}\|<e^{l(c(\nu)+\epsilon)},
\end{equation}
holding  for any $l\geq l_{\epsilon}$ and any $w\in\{v_1\}\cup\{X_0^Te_i\}_{i=1}^N$.\\
Next recall that $\|X^{(0)} W^{(0)}\dots W^{(l-1)}\|_F=\sqrt{\sum_i\|e_i^TX^{(0)} W^{(0)}\|^2}$, meaning that almost surely, for $l\geq l_\epsilon$:
\begin{equation}
 N   e^{l(c(\nu)-\epsilon)}  \leq \|X^{(0)} W^{(0)}\dots W^{(l-1)}\|_F\leq N  e^{l(c(\nu)+\epsilon)}.
\end{equation}

In particular for any $\epsilon$, there exists $l$
sufficiently large such that
\begin{equation}
    \Big({l\choose N}\Big|\frac{\lambda_2}{\lambda_1}\Big|^l \frac{\|X^{(0)} W^{(0)}\dots W^{(l-1)}\|_F}{\|v_1^T X^{(0)} W^{(0)}\dots W^{(l-1)}\|_F}\Big)\leq \Big({l\choose N}\Big|\frac{\lambda_2}{\lambda_1}\Big|^l e^{2l\epsilon} \Big)
\end{equation}
and thus, since $|\lambda_2|<|\lambda_1|$ and we can choose $\epsilon$ arbitrarily small, almost surely it has limit equal to zero.
In particular we can write 
\begin{equation}
    \lim_{l}\frac{\|(I-\mathcal{P}X^{(l)})\|_F}{\|\mathcal{P}X^{(l)}\|_F}\sim \lim_{l}\frac{{l\choose N}|\lambda_2|^{l-N}\|X^{(0)}W^{(0)}\dots W^{(l)}\|_F}{|\lambda_1|^l\|v_1^TX^{(0)}W^{(0)}\dots W^{(l)}\|_F}=0
\end{equation}
where we have used the same argument as before to state that the limit is zero.

\subsection{An illustrative example}\label{Illustrative example}
%
Consider the stochastic nonnegative primitive matrix 
\begin{equation}
    A:=\begin{pmatrix}
        0 & 1 \\[1ex]
        1/2 & 1/2
    \end{pmatrix}.
\end{equation}
Since $A$ is row stochastic, the dominant eigenvector is given by the constant vector $u=(1,1)$. In particular given a vector $x=(x_1,x_2)\in \cone$ we have that 
\begin{equation}
    \dist(x,u)=\log\Big(\frac{\max_{i=1,2}\{x_i\}}{\min_{i=1,2}\{x_i\}}\Big)= \begin{cases} \log\big(x_1/x_2\big) \quad \text{if}\; x_1\geq x_2\\ 
    \log\big(x_2/x_1\big) \quad \text{if}\; x_2\geq x_1\end{cases}.
\end{equation}

On the other hand $Ax=\big(x_2\,, \,(x_1+x_2)/2\big)$, thus

\begin{equation}
    \dist(Ax,u)= \begin{cases} \log\big((x_1+x_2)/2x_2\big) \quad \text{if}\; x_1\geq x_2\\ 
    \log\big(2x_2/(x_1+x_2)\big) \quad \text{if}\; x_2\geq x_1\end{cases}.
\end{equation}

Now observe that the set of vectors $x$ such that $\dist(x,u)\leq C$ is given by the points $\{x\,|\; x_1\leq x_2\leq C^* x_1\}\cup \{x\,|\; x_2\leq x_1\leq C^* x_2\}$ with $C^*=e^C>1$. We want to prove the existence of some $\beta_C=1-\epsilon_C\in (0,1)$ such that $\dist(Ax,u)\leq \beta_C\dist(x,u)$ for any $x$ with $\dist(x,u)\leq C$. 

To this end consider first the case of $x\in \Omega_1=\{x\,|\;x_1\leq x_2\leq C^* x_1\}$. In this case 

\begin{equation}\label{eq.1:ex}
    \begin{aligned}
    \dist(Ax,u)=\log\Big(\frac{2x_2}{x_1+x_2}\Big)\leq (1-\epsilon_C)\log\Big(\frac{x_2}{x_1}\Big)= (1-\epsilon_C) \dist(x,u) \qquad \forall x\in \Omega_1\\ 
    \Longleftrightarrow \quad  \epsilon_C \log\Big(\frac{x_2}{x_1}\Big) \leq \log\Big( \frac{x_2 (x_1+x_2)}{2x_1x_2}\Big) \qquad \forall x\in \Omega_1\\
    \Longleftrightarrow \quad \epsilon_C \log(t)\leq \log\Big(\frac{1+t}{2} \Big) \qquad \forall t\in [1,C^*]\\
    \Longleftrightarrow  t^{\epsilon_C}\leq \frac{1+t}{2} \qquad \forall t\in [1,C^*]  
    \end{aligned}
\end{equation}
where we have written $t=x_2/x_1$. Second we consider the case of $x\in \Omega_2=\{x\,|\;x_2\leq x_1\leq C^* x_2\}$. In this case 

\begin{equation}\label{eq.2:ex}
    \begin{aligned}
    \dist(Ax,u)=\log\Big(\frac{x_1+x_2}{2x_2}\Big)\leq (1-\epsilon_C)\log\Big(\frac{x_1}{x_2}\Big)= (1-\epsilon_C) \dist(x,u) \qquad \forall x\in \Omega_2\\ 
    \Longleftrightarrow \quad  \epsilon_C \log\Big(\frac{x_1}{x_2}\Big) \leq \log\Big( \frac{2x_1x_2}{x_2 (x_1+x_2)}\Big) \qquad \forall x\in \Omega_2\\
    \Longleftrightarrow \quad \epsilon_C \log(t)\leq \log\Big(\frac{2t}{t+1} \Big) \qquad \forall t\in [1,C^*]\\
    \Longleftrightarrow  t^{\epsilon_C}\leq \frac{2t}{t+1} \qquad \forall t\in [1,C^*]  
    \end{aligned}
\end{equation}

where we have written $t=x_1/x_2$. Both the functions $t\rightarrow (1+t)/2$ and $t\rightarrow 2t/(t+1)$ are monotonically increasing for $t\in [1,\infty)$ with derivative $1/2$ in $t=1$, thus for any $C<\infty$ there exists some $0<\epsilon_C<1$ such that both \eqref{eq.1:ex} and \eqref{eq.1:ex} are satisfied in the interval $t\in [1,C^*]$. This shows that $A$ is contractive with respect to $u$ even if $A$ is not strictly positive.



\subsection{proof of \Cref{Lemma_hilb_contractivity_linear_network}}
    By the contraction properties of the matrix $A$ we know that if $\max_i \dist\big((AX)_i, u\big)\leq C$, then 
    \begin{equation}
    \dist\Big((AX)_i, u\Big)= \dist\Big((AX)_i, \lambda_1(A)u\Big)=\dist\Big((AX)_i, Au\Big)\leq \beta \dist\big(X_i, u\big) \qquad \forall i.
    \end{equation}
    for some $\beta<1$, where we have used $\lambda_1(A)>0$ and the scaling invariant property of the Hilbert distance.

    Then note that, for any $i$, we can write $F(X)_i$ as follows
    \begin{equation}
    \big(F(X)\big)_j=\sum_{j}W_{ij}(AX)_j\,.
    \end{equation}
    Thus we \textbf{CLAIM} that given $x_1, x_2, y\in \cone$ then 
    \begin{equation}
        \dist(x_1+x_2,y)\leq \max\{\dist(x_1,y), \dist(x_2,y)\}.
    \end{equation}
    Observe that if the claim holds we have conlcuded the proof, indeed by induction it can trivially be extended from $2$ to $d$ points yielding
    \begin{equation}
        \dist\Big(F(X)_j, u\Big)\leq \max_i \dist\Big(W_{ij}(AX)_i, u\Big)\leq \max_j \dist\Big((AX)_j, u\Big)\leq \beta \max_j \dist\Big(X_j, u\Big),
    \end{equation}
    where we have used the scale-invariance property of the Hilbert distance and the fact that $\max_{i}W_{ij}>0$ for all $j$.

    It remains to prove the claim. To this end, exploiting the expression of the Hilbert distance we write 
    \begin{equation}
    \begin{aligned}
    %
    \dist(x_1+x_2,y)&
    %
    =\log \bigg(\sup_{j}\sup_{i} \frac{(x_1)_i+(x_2)_i}{(y)_i}\frac{(y)_j}{(x_1)_j+(x_2)_j}\bigg)\\
    %
    &\leq 
    \log \bigg(\sup_{i}\sup_{j} \max_{x_1,x_2}\bigg\{\frac{(x_1)_i}{(x_1)_j}, \frac{(x_2)_i}{(x_2)_j}\bigg\}\frac{(y)_j}{(y)_i}\bigg)\\
    &=\max_{x_1,x_2}\big\{\dist(x_1,y), \dist(x_2,y)\big\}
       \end{aligned}
    \end{equation}
    concluding the proof.



\subsection{proof of \Cref{thm_collapse_in_hilbert_distance}}

To prove the theorem we start proving that, a continuous subhomogeneous and order-preserving function $\sigma$ with an eigenvector $u$ in the cone, is not nonexpansive in Hilbert distance with respect to $u$.
% %
Formally we claim that
     \begin{equation}
     \dist \big(\sigma(y), u\big)\leq \dist\big(y, u\big) \qquad \forall y\in \cone.
     \end{equation}

To prove it let $y \in \cone$ and assume w.l.o.g. that $\|y\|_1=t>0$ and $\|u\|_1=1$, then 
%
\begin{equation}
    M(y/t u)=\max_{i=1,\dots,N} \frac{y_i}{ t (u)_i)}\geq \frac{\|y\|_1}{t\|u\|}=1
    \qquad
    m(y/t u)=\min_{i=1,\dots,N} \frac{y_i}{t (u)_i}\leq \frac{\|y\|_1}{t\|u\|_1} =1.
\end{equation}
%
By definition given $x,y\in \cone$, $m(y/x)x\cleq y\cleq M(y/x)x$. Moreover we recall that since $u$ is an eigenvector for any $t>0$ there exists $\lambda_t>0$ such that $\sigma(t u)=\lambda_t u$. Thus we use the subhomogeneity of $\sigma$ and the fact that $u$ is an eigenvector of $\sigma$ to get the following inequalities:
%
\begin{equation}
 m(y/tu) \lambda_t tu \cleq    \sigma\big(m(y/tu)tu\big)\cleq f(y) \cleq f\big(M(y/tu)tu\big) \cleq  M(y/x_c) \lambda_t tu.
\end{equation}

In particular we have 
$m(f(y)/tu)\geq \lambda_t m(y/tu)$ and $M(f(y)/tu)\leq \lambda_t M(y/tu)$. Finally the last inequalities and the scale invariance property of $\dist$ yield the thesis:
%
\begin{equation}
 \dist(f(y),u) = \dist(f(y),tu)= \log\Big(\frac{M(f(y)/tu)}{m(f(y)/tu)}\Big)\leq \log\Big(\frac{M(y/tu)}{m(y/tu)}\Big)=\dist(y,tu)=\dist(y,u) \,.
\end{equation}
%
Now using the claim above and \cref{Lemma_hilb_contractivity_linear_network}, we can easily conclude that 
%
\begin{equation}\label{eq1_thm_subhom}
    \lim_{l\rightarrow \infty}\max_{i}\dist(X_i^{(l)}, u)=0.
\end{equation}
%
Indeed by the uniform contractivity of $\{A^{(l)}\}$, if $C=\max_i\dist (X^{(0)}_i, u)$, then there exists some $\beta_C<1$ such that 
\begin{equation}
    \max_i\dist (X^{(l)}_i, u)\leq \beta_C^l \max_i\dist (X^{(0)}_i, u).
\end{equation}


Finally to conclude we prove that, as a consequence of \eqref{eq1_thm_subhom},

\begin{equation}\label{eq2_thm_subhom}
  \lim_{l\rightarrow \infty}\frac{\|(I-\proj)X^{(l)}\|_F}{\|\proj X^{(l)}\|_F}=0.
\end{equation}

As a consequence of the inequality:
%
\begin{equation}
    1\leq \Nrank(X^{(l)})\leq 1+\frac{\|(I-\proj)X^{(l)}\|_F^2}{\|X^{(l)}\|_2^2}\leq 1+\frac{\|(I-\proj)X^{(l)}\|_F^2}{\|\proj X^{(l)}\|_F^2},
\end{equation} 
 %
 we have that \eqref{eq2_thm_subhom} finally yields the thesis.
 % %
%
To prove \eqref{eq2_thm_subhom}, we recall from Lemma 2.5.1 in \cite{lemmensNonlinearPerronFrobeniusTheory2012} that for any $w$ such that $u^Tw=c$ 
%
\begin{equation}
\|w-\proj w\|_u\leq \|\proj w\|_u (e^{d_T(w,\proj w)}-1),
\end{equation}
%
where $d_T(x,y)=\log(\max\{M(x/y), m(x/y)^{-1}\})$ and where since the dual cone of $\R^n_+$ is $\R^n_+$ itself, we are considering the norm induced by $u$ on the cone, i.e. $\|x\|_u=u^Tx$ for any $x$ in the cone. In practice the norm induced by $u$ $\|\cdot\|_u$ can be defined by the Minkowki functional of the set $\Omega=\text{ConvexHull}\{\{\Omega_1\}\cup\{-\Omega_1\}\}$ where $\Omega_1=\{x\in \cone \,\; u^Tx\leq 1\}$



Then since $\|\proj w\|_u=\|w\|_u=u^Tw$, we have that $ M(w/\proj w)\geq 1$ and $m(w/\proj w)\leq 1$. Thus $d_T(w,\proj w)\leq \dist(w,\proj w)$, yielding
%
\begin{equation}
    \|w-\proj w\|_u\leq \|\proj w\|_u (e^{\dist(w,\proj w)}-1).
\end{equation}

From the equivalence of the norms there exists some constant $c>0$ such that we can equivalently write 
\begin{equation}
    \|w-\proj w\|_2\leq C\|\proj w\|_2(e^{\dist(w,\proj w)}-1).
\end{equation}

Now recall that the squared Frobenius norm of a matrix is the sum of the squared 2-norms of the its columns, so we can apply the last inequality to the matrix $X^{(l)}$ columnwise obtaining: 
\begin{equation}
    \|(I-\proj)X^{(l)}\|_F^2\leq C \|\proj X^{(l)}\|_F^2 \big(e^{\max_i\{ \dist(X^{(l)}_i,\proj X^{(l)}_i)\}}-1\big)^2.
\end{equation}
The proof is concluded observing that by the scale invariance property of the Hilbert distance $\dist(X^{(l)}_i,\proj X^{(l)}_i)=\dist(X^{(l)}_i,u)$, yielding:
\begin{equation}
    1\leq \big(\Nrank(X^{(l)})\big)\leq 1+ \frac{\|(I-\proj)X^{(l)}\|_F^2}{\|\proj X^{(l)}\|_F^2}\leq     
    1+C \big(e^{\max_i\{ \dist (X^{(l)}_i,u)\}}-1\big)^2.
\end{equation}



\subsection{proof of \cref{Lemma_eigenvectors_of_homogeneous_functions}}
We start from the homogenous case. Note that since $\psi$ is homogeneous we have that necessarily $f(t)=c t$ for all $t,c\geq 0$, this in particular means that every $u\in \R^N_+$ is an eigenvector of $\sigma$ with corresponding eigenvalue $\lambda_1=c$.

Then we can consider the subhomogeneous case.
Assume that we have $u\in \R_+^N$ that is an eigenvector of $\sigma$ with eigenvalue $\lambda$ and $u_i>0$ for all $i$, then
%
\begin{equation}
    \psi(u_i)=\lambda u_i \qquad \forall i=1,\dots,N.
\end{equation}
%
By strict subhomogeneity this means that necessarily $u$ is constant, indeed
if $u_i>u_j>0$ then 
\begin{equation}
   \lambda u_j= \psi(u_j)=\psi\Big(u_j\frac{u_i}{u_i}\Big)>\frac{u_j}{u_i} \psi(u_i)=\lambda u_j,
\end{equation}
yielding a contradiction.
%
In particular any constant vector $u$ in $\R_+^N$ is easily proved to be an eigenvector of $\sigma$ relative to the eigenvalue $\lambda=\|\sigma(u)\|_1/\|u\|_1=\psi(u_i)/u_i$ where $u_i$ is any entry of $u$.


\clearpage
\begin{samepage}

\section{Additional Empirical Results on Real Dataset} \label{apd:additional_empirical_results}



\begin{table*}[h!]
    \centering
    \input{Tables/figure_table_trained_cases_cora}
    \caption{The table showcases the behaviour of different metrics and the classification accuracies for 8 GNNs separately trained on Cora Dataset. This table is an extension of figure \ref{fig:metric_cora_eg}}
    \label{tab:fig_trained_cases_cora}
\end{table*}

\end{samepage}



\begin{table*}[h]
    \centering
    \input{Tables/figure_table_trained_cases_citeseer}
    \caption{The table showcases the behaviour of different metrics and the classification accuracies for 8 GNNs separately trained on Citeseer Dataset.}
    \label{tab:fig_trained_cases_citeseer}
\end{table*}

\begin{table*}[h]
    \centering
    \input{Tables/figure_table_trained_cases_pubmed}
    \caption{The table showcases the behaviour of different metrics and the classification accuracies for 8 GNNs separately trained on Pubmed Dataset.}
    \label{tab:fig_trained_cases_pubmed}
\end{table*}


\begin{comment}
\end{comment}

\end{document}

