\begin{table}[ht]
    \centering
    \begin{tabular}{l  cc  cc}
        % \toprule
         & \multicolumn{2}{c}{Inter-Rater \( \kappa \) } & \multicolumn{2}{c}{Cluster Agreement \( \kappa \) } \\
        % \cmidrule(lr){2-3} \cmidrule(lr){4-5}
        Dataset & Original Label & Level-Up Label & Original Label & Level-Up Label \\
        \toprule
        Milan  & 0.850 & 0.884 & 0.600 & 0.775 \\
        Aruba  & 0.890 & 0.918 & 0.627 & 0.757 \\
        Cairo  & 0.872 & 0.916 & 0.630 & 0.810 \\
        \bottomrule
    \end{tabular}
    \vspace{0.5em}
    \caption{
        Sub-Activity label quality: inter-rater (Cohen’s) and cluster-agreement (Fleiss’s) Kappa scores for each of the three CASAS datasets (Milan, Aruba, and Cairo). ``Original Label'' indicates the use of the most specific labels chosen by each rater (e.g., \emph{movement near fridge}), while ``Level-Up Label'' merges them into coarser categories (e.g., \emph{movement in kitchen}). 
        Overall, very high inter-rater agreement indicates very high alignment between annotators; decent cluster agreement also implies relative homogeneity among the ratings.
        The resulting increase in Kappa scores from ``Original Label'' to ``Level-Up Label'' highlights that most disagreements stem from label specificity rather than fundamentally different interpretations of the underlying activity.        
    }
    \vspace*{-2em}
    \label{tab:kappa_scores}
\end{table}
