\section{Experimental Evaluation: Labeling Stage}
\label{sec:label_quality}

For our second set of experiments, we move beyond the CASAS annotations and evaluate the labeling stage of our pipeline (step 2, 3, and 4 in \ref{fig:main_image}). 
This resembles the envisioned use case for our approach where we employ our active learning-like scheme, driven by our self-supervised clustering stage, to guide human annotator to provide reliable labels thereby minimizing manual efforts.

By comparing our human-labeled clusters to CASAS labels, we highlight how our approach provides a more granular and detailed categorization of (sub-)activities, offering finer resolution and capturing nuances that the original labels may miss. 
We also evaluate the quality and consistency of these labels by examining two key metrics: inter-rater agreement, which measures consistency between different labelers, and cluster agreement, which assesses the precision of the labels assigned to each cluster.

\subsection{Setup}
Our objective is to present human labelers with visualizations of a small number of representative data sequences from each cluster, and for labelers to select the appropriate label for each sample.  Below, we detail the labels selected for our work and evaluation metrics.

\subsubsection{Label Set}
The \ToolName{} labeling scheme is organized hierarchically to accommodate different levels of detail. At the highest level, we organize all activities into categories such as \emph{single-room} and \emph{multi-room} events, with subcategories specific to each environment (e.g., \emph{kitchen} vs.\ \emph{bedroom}). By allowing annotators to choose increasingly granular labels, we preserve the flexibility needed for diverse research objectives.

To capture a finer level of granularity than conventional HAR labels such as  \textit{Cook}, \textit{Relax}, or \textit{Work}, we developed a set of \emph{sub-activity} labels by studying the physical layouts of the CASAS homes, examining the replays of actual sensor activations throughout the day, and drawing insights from prior work on structural constructs in activities \cite{shruthi_gameofllms}. We selected a large number of potential labels in order to avoid over-constraining our annotators, leveraging information about furniture placement when it was available. Our labels capture both activities within a single room (e.g., \textit{movement all over kitchen}, \textit{movement near fridge}, \textit{movement near stove}, and \textit{movement near medicine cabinet}) and movement actions between the rooms (e.g., \textit{walking from bedroom to office}, \textit{leaving guest bathroom}). See the full list of sub-activities and their hierarchies in \cref{fig:atomic_activity_tree} in the Appendix.

Note, we have chosen a dataset-specific approach in our label selection, with several labels tailored specifically to the CASAS dataset (e.g., \textit{medicine cabinet}), in order to demonstrate the degree to which \ToolName{} supports customization to a given environment or use case. The granularity of the labels is fully adjustable; depending on the specific application, a user can, for example, choose whether they care to split \textit{armchair sitting} and \textit{couch sitting} into distinct classes, or if they want to group them into a broader \textit{Relax} category.

\subsubsection{Labeling Sample Selection and Metrics}

To obtain cluster labels, we recruited 15 independent raters to each individually label data segments.  For each cluster \( c_k\), we choose \( m\) sequence samples that are closest to the cluster centroid for labeling, with \( m = 5\) in this work.  With $k=20$ clusters per model, this resulted in a total of $300$ samples across all three datasets. Samples were randomized and each sample was labeled by two raters, resulting in $600$ total sample-label pairs. To generate a label, each rater was presented with a selection of $40$ samples, for which they assigned labels using the web-based \ToolName{} annotation tool.  \cref{fig:viz_tool} shows a screenshot of a sample annotation replay for the Milan household. During the replay, the rater can view a temporal playback of the sensor activations captured by the data sequence, and use a drop-down menu to select a label. Note that the \ToolName{} tool is fully customizable and is available in open source at \url{https://anonymized}.

We use two metrics to evaluate the labeling stage.  First, we evaluate \textbf{inter-rater agreement} using Cohen’s Kappa \cite{kappas}.  Higher values represent stronger agreement, with scores over 0.8 representing strong alignment. Observing a high inter-rater agreement on the labeling task would indicate that the data sequences represent consistently interpretable human behavior sequences. Second, we evaluate \textbf{cluster agreement} to examine the uniformity of the \( m\) labels obtained for each cluster.  We use Fleiss’s Kappa \cite{kappas}, which generalizes the idea of Cohen’s Kappa to more than two ratings, to capture the consensus among all 10 labels ($m=5$ by a total of $2$ raters per sample) assigned within a single cluster. A high Fleiss’s Kappa indicates that our raters not only agree between themselves, but also that data sequences captured by the cluster represent a single activity rather than disparate data sequences (i.e., the cluster is internally homogeneous).


Since our sub-activity labels follow a hierarchical, multi-level taxonomy (see \cref{fig:atomic_activity_tree}), disagreements may sometimes stem from different levels of specificity (e.g., \emph{movement near fridge} vs.\ \emph{movement in kitchen}). To account for this, we also compute Kappa scores at a coarser level by ``leveling up'' each annotated label to its parent category. For instance, if one rater selects \emph{movement near fridge} and the other chooses \emph{movement in kitchen}, these would be treated as the same label at the higher level. Comparing Kappa scores at different levels of the hierarchy helps distinguish minor discrepancies in labeling granularity from dramatically different activity interpretations (e.g., \emph{bathroom activity} vs.\ \emph{reading in armchair}). 

\subsection{Results}
\label{sec:label_quality_results}

\input{tables/kappa_scores}

In this section, we first analyze the inter-rater and cluster-agreement scores, then discuss how the obtained labels different from CASAS labels.
\cref{tab:kappa_scores} summarizes the inter-rater and cluster-agreement scores for Milan, Aruba, and Cairo. We also refer to \cref{fig:cluster_majority_label_summary} in the Appendix for a detailed table showing the majority label assigned to each cluster and its corresponding vote count.

Inter-rater agreement, measured as Cohen's Kappa, exceeds 0.85 for all three datasets (Milan \(\kappa=0.85\), Aruba \(\kappa=0.89\), Cairo \(\kappa=0.87\)). These values indicate very high consistency between the two independent annotators in how they interpreted and labeled the same replay samples. Furthermore, when we ``level up'' each label to its parent category in our hierarchical taxonomy (e.g., merging \emph{movement near bed} into \emph{movement in bedroom}), the Kappa scores rise even further—reaching 0.88 in Milan and over 0.90 in Aruba and Cairo. This gain reflects the fact that any disagreements that did arise between raters, were largely in relation to differences in label specificity, such as \emph{sitting in armchair} vs.\ \emph{movement in living room}, rather than fundamentally diverging perceptions of the activity itself.

Cluster agreement, measured as Fleiss' Kappa, results in moderately high scores ranging from 0.60--0.63 when annotators used highly specific sub-activity labels. Once labels are rolled up to a coarser levels, these scores climb to 0.75--0.81 across the three datasets. Here again, the improvement underscores that most labeling discrepancies within a cluster stem from variation in granularity rather than actual activity disagreements. For instance, in one cluster designated \emph{kitchen activity}, half of the annotators specified \emph{movement near medicine cabinet}, while the other half used \emph{movement in kitchen}. In another cluster, some annotators perceived simultaneous sensor firings in two rooms as \emph{multi-room activity}, while others focused on whichever room had the most events.

The high rater and cluster agreements confirm that clusters produced by the \ToolName{} pipeline are both interpretable and relatively homogeneous. Moreover, the gains observed when we unify labels at a higher level indicate that our hierarchical taxonomy successfully accounts for the inherent variation in labeling granularity. This can allow researchers to adopt the level of detail most appropriate for their analysis. 

\input{tables/subactivity_mapping}

We now want to explore these manually annotated sub-activity labels in greater detail. \cref{tab:subactivity_mapping} illustrates how each discovered sub-activity label maps onto the broader CASAS categories (e.g., \emph{Cook}, \emph{Relax}, \emph{Sleep}), along with the percentage of time-window sequences \(W_i\) that each label occupies in Milan, Aruba, and Cairo. Dashes indicate activities that cannot occur in a given household (e.g., no guest bathroom sensors in Aruba or Cairo).

Notably, the total proportion of each broad CASAS label usually aligns reasonably well with our sub-activity categories. In Milan, for example, \emph{Sleep} comprises around 7\% of the CASAS data and 8\% of our sub-activity labels (\emph{movement in bedroom}, \emph{movement near bed}). Similarly, CASAS marks 41\% of the Milan data as \emph{Relax}, compared to 36\% for our corresponding sub-activities. More importantly, our approach results in a much finer breakdown of these broad labels, such as distinguishing \emph{sitting in living room armchair} vs.\ \emph{sitting in office armchair}, or \emph{movement near the medicine cabinet} vs.\ \emph{movement near the stove}. In Aruba, a single \emph{Relax} label can be further divided into \textit{sitting on the couch} vs.\ \textit{motion in a TV chair}. These finer distinctions highlight one of the key advantages of \ToolName{}: while it can reasonably well replicate higher-level annotations from existing datasets, it can also uncover more granular activities that may carry significant behavioral or clinical relevance.