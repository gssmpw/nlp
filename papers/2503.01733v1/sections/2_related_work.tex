\section{Related Work}
\label{sec:related_work}

To provide context for our work, we review key areas of research that relate to our approach. These include conventional and deep learning-based methods for HAR, which often rely on large labeled datasets. We will then go over advancements in self-supervised and active learning, followed by clustering approaches that aim to reduce the need for manual annotation and be more sample efficient.

\subsection{Human Activity Recognition}
Human Activity Recognition in smart environments has emerged as a critical area of research, enabling the identification of Activities of Daily Living (ADLs) through ambient sensors \cite{bouchabou2021survey, vanKasteren2010, Rafferty2017, kientz2008georgia}. ADLs are particularly valuable for routine detection, which feeds numerous downstream applications, including health monitoring and assistive systems for independent living \cite{chatting2023automated, csurka2017domain, morita2023health}. Conventional approaches to HAR rely heavily on supervised learning methods, where large amounts of labeled data is needed. These methods span a range of classic machine-learning techniques, including Random Forests \cite{sedky2018evaluating}, Hidden Markov Models (HMMs) and Support Vector Machines (SVMs) \cite{cook2010learning}. These approaches rely on features hand-crafted by domain experts, and universally assume pre-segmented data, which limits their applicability in real-world scenarios.

Advances in deep learning have expanded the capabilities of HAR systems. Deep Neural Networks (DNNs) \cite{alaghbari2022activities}, Long Short-Term Memory networks ((LSTM) \cite{liciotti2020sequential}, and hybrid architectures such as ELMo combined with BiLSTM \cite{bouchabou2021using} have demonstrated improved performance in activity classification tasks. Additionally, graph neural networks (GNNs) have been employed to capture spatial and temporal relationships in sensor data \cite{li2019relation, zhou2020graph, plotz2023know}. Despite these advancements, the reliance on pre-segmented data remains a significant limitation. Some studies have explored segmentation techniques, such as change-point detection algorithms, which identify abrupt changes in sensor data to infer activity boundaries \cite{aminikhanghahi2018real, sprint2020behavioral, jose2017improving, aminikhanghahi2017using}. However, these methods often rely on heuristics and are not well-suited for capturing fine-grained activities. 

These HAR approaches depend on massive labeled datasets and fixed, pre-defined labels that cannot always capture the nuanced behaviors exhibited in real-world environments. To address this challenge, our work introduces a self-supervised framework to process large unlabeled datasets, paired with an active learning approach that allows to discover fine-grained sub-activities with minimal human annotation efforts.

\subsection{Self-Supervised Learning}

A number of self-supervised learning (SSL) techniques have emerged in computer vision and natural language processing, enabling models to learn generalizable features without relying on massive labeled datasets. This is achieved by designing pretext tasks—such as predicting image rotations \cite{gidaris2018unsupervised} or solving jigsaw puzzles \cite{noroozi2016unsupervised}—that encourage the extraction of robust representations from raw data, which can then be fine-tuned for downstream tasks like image classification and semantic segmentation \cite{long2015fully}. A prominent example is SimCLR, a contrastive learning framework that has achieved state-of-the-art results in various domains and has been successfully adapted for time-series data, effectively capturing temporal patterns \cite{chen2020simple, yang2022timeclr, shah2021evaluating, mohsenvand2020contrastive}.

In sensor-based HAR, SSL has predominantly been applied in wearable contexts, where the data is homogeneous and continuous, making it well-suited for contrastive and reconstruction-based methods \cite{haresamudram2022assessing, haresamudram2020masked}. For example, Tang et al. adapt a contrastive learning framework to recognize activities such as walking, sitting, and standing with competitive performance in healthcare applications \cite{tang2020exploring}. Other methods have enhanced feature extraction by incorporating custom transformer encoders \cite{khaertdinov2021contrastive} and clustering-based negative selection \cite{wang2023negative} into their frameworks.

However, smart home HAR presents a different challenge: the data is heterogeneous, combining discrete events with continuous sensor streams, which complicates the direct application of traditional SSL methods. Chen et al. \cite{chen2020simple} applied SSL to ambient sensor data by developing a SimCLR-based model that trains an encoder to capture both spatial and temporal dependencies, extracting robust features for downstream HAR tasks and achieving superior performance in semi-supervised and transfer learning scenarios.
The main goal of their approach is to predict the original coarse labels. 
We, however, go beyond that paradigm, aiming to uncover and validate semantically meaningful, fine-grained sub-activities within the complex domain of smart homes.

\subsection{Active Learning}
The annotation of sensor data for HAR is a resource-intensive process, requiring significant time and expertise \cite{hiremath2020deriving, zhai2019s4l}. Active learning has emerged as a promising approach to reduce the annotation burden by selecting the most informative samples for labeling \cite{settles2009active}. This human-in-the-loop paradigm aims to minimize the number of labeled samples required while maintaining model performance. A key challenge in active learning is determining an optimal sampling strategy, oftentimes having to choose between pool-based and stream-based approaches \cite{hossain2017active, adaimi2019leveraging, jones2014active}. In contrast, our approach utilizes a clustering-based active learning framework that identifies representative sub-activities for annotation. This minimizes human intervention while retaining the flexibility to dynamically adapt to novel data sources.

\subsection{Clustering}
Unsupervised clustering methods have been employed in HAR to group unlabeled sensor data into meaningful activity clusters \cite{ariza2020unsupervised}. Classic approaches, such as Lloyd’s clustering algorithm combined with k-Nearest Neighbors \cite{fahad2014activity} and DBSCAN \cite{hoque2012aalo}, have been used to identify patterns in sensor data. More recently, the SCAN framework \cite{scan2020} has been adapted for HAR, enabling the clustering of unlabeled movement data from wearable sensors \cite{ahmed2022clustering}. These methods leverage large-scale sensor data to enhance activity recognition, addressing the challenges associated with obtaining annotated datasets. However, existing "cluster then classify" approaches still rely on downstream labels to match and often lack the ability to discover fine-grained activities.

Recent work by Bock et al.\ \cite{bock2024weak} introduced a clustering-based annotation pipeline that reduces the need for human labeling by leveraging vision-based models for weak annotation in HAR datasets, demonstrating that clustering methods can match fully supervised classifiers while minimizing manual effort. Inspired by successes in the wearable domain, we propose a novel approach tailored for ambient sensor data in smart homes that addresses the challenges of heterogeneous sensor streams while discovering fine-grained sub-activities rather than merely mapping back to the original labels.