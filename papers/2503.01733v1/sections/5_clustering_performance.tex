\section{Experimental Evaluation: Clustering Stage}
\label{sec:results:clustering}

In this section, we evaluate the clustering stage of the \ToolName{} pipeline (Steps 0-2) to assess the quality of the clusters generated by our approach.  Although the clusters have not yet been assigned \ToolName{} labels, examining the clusters helps provide insights into the degree to which they match previous hand-annotated labels from CASAS. To validate cluster performance, we use CASAS as a ground truth oracle to temporarily assign a label to each cluster through majority voting. We then perform a direct comparison with fully supervised techniques, and investigate the impact of varying cluster count, examining the trade-off between granularity, label alignment and annotation costs.

\subsection{Setup}
\label{sec:supervised_setup}
We assess the quality of the clusters generated by our approach by comparing them to CASAS labels.  To map \ToolName{} clusters to CASAS labels, we use majority vote to assign a single label to each cluster based on the CASAS labels of its samples. We then assess these cluster-assigned labels, utilizing CASAS labels as ground truth. 

We use two common variants of the F1 metric to evaluate the performance. Let \(\{1, \ldots, C\}\) be the set of activities, and let \(\mathrm{F1}_c\) denote the F1 score for class \(c\). Given the class imbalances in CASAS, we utilize the following metrics:

\begin{itemize}
    \item \textbf{Weighted F1}: Emphasizes frequent classes by weighting each class’s F1 score by its support:
    \[
    \mathrm{F1}_{\mathrm{weighted}} 
    = \sum_{c=1}^{C} \frac{\lvert c \rvert}{N} \,\mathrm{F1}_c,
    \]
    where \(\lvert c \rvert\) is the number of samples belonging to class \(c\), and \(N\) is the total number of samples.
    \item \textbf{Macro F1}: Treats all classes equally by averaging their F1 scores:
    \[
    \mathrm{F1}_{\mathrm{macro}} 
    = \frac{1}{C} \sum_{c=1}^{C} \mathrm{F1}_c.
    \]
\end{itemize}

Temporarily utilizing CASAS labels for our clusters has one additional benefit in that it allows us to directly compare our self-supervised clustering approach to fully-supervised methods.  As noted in earlier discussion, by comparing our cluster-based, majority-vote labels with the outputs of fully supervised baselines, we can gauge how closely our framework approaches traditional, label-intensive methods. While surpassing these baselines is not our primary goal, a comparable \(\mathrm{F1}\) performance signifies a reasonable alignment between our clustering results and CASAS labels.  

Specifically, we compare against two baselines: DeepCASAS \cite{deepcasas2018} and TDOST \cite{tdost2024}. DeepCASAS is a fully supervised deep learning algorithm that leverages LSTMs to capture temporal dependencies in the data. In contrast, TDOST converts sensor triggers into natural language descriptions, 
which provides the model with rich contextual information and enables the transfer of a single model across multiple households.

Note that both DeepCASAS and TDOST, as originally published, were trained and evaluated on pre-segmented datasets. This allowed the authors to compare the isolated algorithmic performance in a controlled setting, but is a less realistic scenario compared to operating over continuous sensor streams. To achieve a fair comparison to our approach, which does not assume pre-segmentation, we re-train both DeepCASAS and TDOST algorithms on sliding windows $W_i$ of length \( l \) to match our sequences. Additionally, given the prevalence of the \textit{Other} label in our datasets (see \cref{sec:datasets}), we conduct experiments both with and without the \textit{Other} label. Specifically, we re-train DeepCASAS and TDOST with and without the \textit{Other} label, which allows us to evaluate how well these methods work with only \textit{relevant} activities.


\subsection{Results}
\label{sec:supervised_results}

\input{tables/clf_comparison}

In this section we present how well our clusters align with the CASAS labels, and also experiment with the optimal number of clusters needed to achieve useful performance.

\subsubsection{\ToolName{} Cluster Alignment with CASAS Labels}   

\cref{tab:clf_comparison} reports both weighted and macro F1 scores for models trained with and without the \textit{Other} label. The bottom row shows the performance of the \ToolName{} clustering stage compared. Across all three datasets, we observe that the \(\mathrm{F1}_{\mathrm{macro}}\) metric is considerably lower than \(\mathrm{F1}_{\mathrm{weighted}}\). This indicates that less frequent activities, such as \textit{Take\_medicine} and \textit{Leave\_home} are challenging to discover, as has been reported in previous studies \cite{hiremath2022bootstrapping}.  
Additionally, we observe that performance improves by approx.\ 22 points when the \textit{Other} label is excluded, indicating that the availability of \textit{Other} data points during clustering leads to clusters that are less closely aligned with CASAS labeling.  
As we will demonstrate in \cref{sec:tsne}, this effect is due to the non-homogeneous nature of the data captured by the catch-all \textit{Other} label. 


\subsubsection{Comparison to Supervised Baselines}

The top rows of \cref{tab:clf_comparison} report the performance of the DeepCASAS and TDOST baselines\footnote{As noted in \cref{sec:supervised_setup}, both baselines were re-trained and tested on sliding window sequences to remove the pre-segmentation assumption. This change leads to significant drops in performance compared to originally published results. For example, for DeepCASAS, the \(\mathrm{F1}_{\mathrm{weighted}}\) scores dropped by 29 points for Milan from 0.89 to 0.61; by 21 points for Aruba from 0.97 to 0.78; by 13 points for Cairo from 0.85 to 0.72. This highlights the significance of the pre-segmentation assumption.}.  
As with \ToolName{}, we observe that that \(\mathrm{F1}_{\mathrm{macro}}\) is substantially lower than \(\mathrm{F1}_{\mathrm{weighted}}\), and that performance improves with the exclusion of data associated with the \textit{Other} label.  

More generally, we observe that while DeepCASAS and TDOST yield better performance overall, the gap relative to \ToolName{} is moderate, with comparable performance on $\mathrm{F1}_{\mathrm{weighted}}$ when \textit{Other} is excluded.  
These results highlight the misalignment of self-supervised clusters with the \textit{Other} class, suggesting that \textbf{data labeled as \textit{Other} represents many different behaviors rather than a single cohesive activity}.  We explore this finding in greater detail in the following subsection.


\subsubsection{tSNE Analysis}
\label{sec:tsne}

\begin{figure}[t]
    \centering
        \includegraphics[width=0.95\linewidth]{figures/cluster_tsne_with_house_maps.pdf}
    \caption{
        tSNE projection of SCAN embeddings from the Milan household; each point represents a sensor window embedding colored by its original CASAS label. Insets display the deployment environment layout overlaid with a heatmap of sensor activations. Insets (a) and (b) highlight two distinct clusters within the CASAS \textit{Cook} label—cluster 16 showing movement between kitchen and dining areas, and cluster 5 capturing activity near the medicine cabinet. Insets (c) and (d) show clusters from the \textit{Relax} label, corresponding to sitting in the TV room armchair and sitting in the living room armchair. Data associated with the \textit{Other} label (gray) is dispersed across clusters, underscoring its heterogeneous nature. This figure showcases \ToolName{}'s capability to uncover more granular and nuanced activity categories than the original CASAS labels. 
    } 
    \vspace*{-1em}
    \label{fig:cluster_tsne_with_house_maps}
\end{figure}

In this section, we perform a visual inspection of Milan clusters in order to gain more insights into the generated clusters. 
\cref{fig:cluster_tsne_with_house_maps} shows a tSNE projection of the SCAN embeddings from the Milan dataset; each point represents a 768-dimensional sequence embedding colored by its CASAS label. While some activities (e.g., \emph{Work}) form single clusters, data representing other activity labels (e.g., \emph{Cook}, \emph{Relax}) fall into multiple clusters. 
Each point in the scatterplot is annotated with a probability score reflecting SCAN’s confidence in assigning that sequence to its respective cluster, offering interpretability and a level of control that simpler clustering methods (e.g., k-means) lack.

Further, we take the two largest Milan activity labels, \emph{Cook} and \emph{Relax}, and inspect the data within individual clusters, as shown in the figure insets. 
Each inset includes the layout of the deployment environment overlaid with a heatmap representing sensor activation patterns for the data sequences associated with that cluster.  
\cref{fig:cluster_tsne_with_house_maps} illustrates that clustering using \ToolName{} facilitates the discovery and differentiation of granular sub-activities.  
Insets (a) and (b) are associated with the \textit{Cook} CASAS label (orange).  
Although both sets of data are labeled as \textit{Cook} within CASAS, we observe clear differences between the sensor patterns within each cluster.  
Cluster 16 (inset (a)) captures the person leaving the kitchen and moving to the dining room, with motion sensors from the dining room and part of the dining room registering the person’s presence.  
By comparison, cluster 5 (inset (b)) is tightly focused on activity around the medicine cabinet in the kitchen. 
Insets (c) and (d) are associated with the \textit{Relax} CASAS label (burgundy), where cluster 3 (inset (c)) captures a static activity in the TV room armchair while cluster 17 (inset (d)) corresponds to sitting in a living room armchair instead.  

Note that data associated with the \textit{Other} label (gray) is scattered throughout multiple clusters in \cref{fig:cluster_tsne_with_house_maps}, indicating that it does not represent a single sub-activity. Instead, \textit{Other} activities appear to correspond to multiple sub-activities.  

These findings can be interpreted as follows. First, excluding the \textit{Other} label significantly improves performance, particularly when there is no pre-segmentation and with shorter sequence lengths, as the \textit{Other} label tends to overlap with all other categories. Second, the large gap between \(\mathrm{F1}_{\mathrm{weighted}}\) and \(\mathrm{F1}_{\mathrm{macro}}\) reflects the challenges of accurately recognizing low-frequency classes, a point often overlooked when results are reported for the fully supervised methods.
Finally, our method's performance indicates that the clusters align reasonably well with the CASAS labels, showcasing the capabilities of \ToolName{}.
% demonstrating a decent alignment between our clusters and CASAS labels.

\subsubsection{Impact of the Number of Clusters}
\label{sec:varying_clusters_results}

\begin{figure}
    \centering
        \includegraphics[width=0.9\linewidth]{figures/varying_clusters_f1.pdf}
    \caption{
        Macro F1 scores for varying numbers of SCAN clusters (\(k \)) on Milan, Aruba, and Cairo datasets, using all CASAS labels in (a), and without the \textit{Other} label in (b). We chart shows average F1 scores with bootstrapped 95\% confidence intervals . Increasing \(k\) up until 20-40 clusters improves alignment with CASAS labels before the performance improvement stagnates, suggesting that the optimal number of clusters lies in that range.
    } 
    \vspace*{-1em}
    \label{fig:varying_clusters_f1}
\end{figure}

\noindent
We also investigate how the choice of \(k\)—the number of clusters—impacts our model's ability to capture and distinguish fine-grained activities. 
Similar to previous section, we use CASAS labels and apply majority voting per cluster to compute Macro F1 scores. 

\cref{fig:varying_clusters_f1} shows the Macro F1 scores with bootstrapped 95\% confidence intervals (CIs) for Milan, Aruba, and Cairo, when  \( k \in \{10, 15, 20, 30, 40, 50, 60, 100\} \) clusters. 
The left-hand panels in (a) show performance when all CASAS labels are considered; the right-hand panels in (b) exclude \emph{Other}. 
These F1 scores indicate that utilizing more clusters generally leads to better classification performance in (a) for all three datasets, although the improvements appear to plateau after \( k = 30\) for Milan, and \( k = 20 \) for Aruba and Cairo. 
When we exclude the \textit{Other} label, the macro F1 scores for both Milan and Aruba plateau when \( k > 30\) and further increases are not statistically significant; 
Cairo stays relatively flat after \( k = 20 \), and the improvements for \( k >= 50 \) have very wide CIs. 
This leads to a conclusion that increasing the number of clusters beyond 20–30 is not particularly useful.
Refer to \cref{fig:varying_clusters_tsne} in the Appendix to see what these various clusters look like on 2D tSNE projections.

, as the F1 scores plateau. 
In addition, we also visualizing the effect of the number of clusters through a tSNE plot in \cref{fig:varying_clusters_tsne} in the Appendix. 

An optimal \( k \) balances classification performance with the workload of labeling cluster centroids. 
We selected \( k=20 \) because it provides a more granular view than the typical CASAS labels while keeping annotation efforts manageable. 