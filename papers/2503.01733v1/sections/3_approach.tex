\section{DISCOVER: Data-driven Identification of Sub-activities via Clustering and Visualization for Enhanced Recognition}

\begin{figure}
    \centering
        \includegraphics[width=0.95\linewidth]{figures/main_image.pdf}
    \caption{
        Overview of \ToolName{}, a self-supervised system designed to \textit{discover} fine-grained human activities from unlabeled sensor data without relying on pre-segmentation, consisting of two main stages: clustering and labeling.
        After (0) slicing the raw data into continuous sliding windows of sensor activations \textit{ without assuming any pre-segmentation}, we (1) train a BERT model with mask language modeling task to encode the windows in an embedding space. We then use these embeddings to identify similar activity windows and (2) fine-tune a clustering model using SCAN loss, which results in assigning all data points to \( k \) clusters. We then (3) sample a handful of windows closest to each cluster centroid, replay them on 2D house layouts using our custom built visualization tool, and send these samples to a group of experts for annotation. With minimal labeling effort, we obtain custom granular activity labels for each cluster centroid, and (4) propagate them to the rest of the data points in each cluster. These custom labels are then (5) applied to the original dataset and can later be used for a set of specialized downstream tasks.
    } 
    \vspace*{-1em}
    \label{fig:main_image}
\end{figure}

To address the challenges of human activity recognition in smart homes, we introduce \ToolName{}, a data-driven approach that \textit{discovers} fine-grained human activities from unlabeled sensor data without relying on pre-segmentation. 
%Building on the insights from recent advances in self-supervised learning and clustering, 
\ToolName{} combines unsupervised feature extraction with an interactive annotation tool, enabling more granular and personalized activity labels.

We consider the problem of 
%\emph{self-supervised} 
human activity recognition in smart home environments. Given a time-series dataset of sensor readings collected over a period of time, divided into a collection of \(n\) sliding windows \(\{W_1, W_2, \dots, W_n\}\) of size \( l \), our goal is to assign each of these windows to an activity label. Formally, we seek to learn a mapping
\(
f: W_i \mapsto c_k,
\)
where \(c_k\) represents the activity cluster for window \(W_i\). A natural challenge in this setting is the lack of ground truth labels for the time windows.

\cref{fig:main_image} gives an overview of \ToolName{}, which consists of two main stages: clustering and labeling. The approach starts by pre-processing sensor data into sliding windows in step (0). We then pursue a active learning sub-activity discovery through the following five steps:

\begin{enumerate}
    \item \textbf{Encoder Pre-Training}: Create an embedding representation of sensor readings by training a BERT transformer model with a self-supervised, mask language modeling task. Use these embeddings to create a set of \( m \) nearest neighbors for each window \( W_i \).
    
    \item \textbf{Clustering Model Fine-Tuning}: Fine-tune the pre-trained BERT model using the SCAN loss function \cite{scan2020}, which partitions all windows \( W_i \) into \( k \) distinct clusters, assigning each \( W_i \) to one of them.

    \item \textbf{Centroid Annotation}: Select a handful of windows \( W_i\) closest to each cluster centroid, and send them to expert annotators to have them assign a label to each sample using \ToolName{} custom built visualization tool.
    
    \item \textbf{Label propagation}: Propagate the centroid labels to every data point in the cluster. 
    
    \item \textbf{Re-Annotation of the Original Time-Series Data}: Apply these cluster activity labels to the original time-series data.
\end{enumerate}

\subsection{Encoder Pre-Training}

\begin{figure}
    \centering
        \includegraphics[width=0.95\linewidth]{figures/training_pipeline.pdf}
    \vspace*{-1em}
    \caption{
        \ToolName{} approach -- model training pipeline, consisting of (1) Encoder Pre-Training, and (2) Clustering Model Fine-Tuning. 
        \ToolName{} first trains a BERT model using a Masked Language Modeling head in (1) to obtain initial embeddings for each window \( W_i \). In step 2(a) it uses these embeddings to identify similar activity windows and pairs them together as a a new training set. It then continues training the pre-trained BERT base model with a SCAN loss (2(b)). In the end, the trained SCAN model assigns a cluster \( c_k \) to each input sequence \( W_i \) (2(c)).
    }
    \vspace*{-1em}
    \label{fig:training_pipeline}
\end{figure}

The first step is to learn an informative representation for raw sensor sequences. In this subsection, we describe how we adapt a BERT model through a masked language modeling (MLM) objective to capture context-rich representations of ambient sensor data without any labels.

We build on the approach from \cite{hiremath2022bootstrapping}, which employs a pre-trained BERT Transformer model to derive embeddings for sensor sequences by feeding in new, domain-specific tokens and training for several epochs. However, unlike \cite{hiremath2022bootstrapping}, our pipeline does not rely on pre-segmented data, staying faithful to a more realistic, fully unsupervised scenario. \cref{fig:training_pipeline}(1) shows a diagram of this process. First, we load the original BERT model that was pre-trained on text corpora (BooksCorpus and English Wikipedia) \cite{bert2019} from HuggingFace \cite{huggingface2020} and extend the original vocabulary of size \texttt{30,522} WordPiece tokens by adding sensors and their readings as new tokens to the original vocabulary (which evaluates to roughly \texttt{100} new tokens per dataset). For example, sensor labeled \texttt{M1} and its output signal \texttt{ON} are concatenated into a new token \texttt{M1\_ON}, assigned a new token ID, and initialized with random \texttt{768}-dimensional embedding. Once all training input sequences are processed and encoded, we train BERT model using a self-supervised MLM objective, defined below. 

For every input sequence \( W_i = \{d_1, d_2, \dots, d_l\} \) of sensor tokens \( d \), we choose a proportion \( p \) of these readings to mask. Following \cite{hiremath2020deriving} and the original BERT paper \cite{bert2019}, we set \( p = 0.15 \). Let \( M \subset W_i \) be a subset of positions selected for masking, where \(\lvert M \rvert = \lfloor p \times l \rfloor\). We replace each token in \( M \) with a special \texttt{[MASK]} token, producing a masked sequence \(\hat{W_i}\). The model is then trained to predict the original tokens in \( M \) based on the surrounding context provided by \(\hat{W_i}\).

The training loss \( \mathcal{L}_{MLM} \) is defined as:
\[
\mathcal{L}_{MLM} = - \frac{1}{|M|} \sum_{x \in M} \log P(d_x \mid \hat{W_i})
\]

where \( P(d_x \mid \hat{W_i}) \) is the probability that \(x\)-th masked token \( d \) is correctly predicted by the model.

This loss function incorporates the context from the surrounding, unmasked tokens (i.e., sensor readings) in each sequence. By introducing new tokens for sensor readings (e.g., \texttt{M1\_ON}) into the vocabulary and initializing their embeddings randomly, the model is able to integrate sensor-specific information into the learned representations during pre-training.
Once the training is done, we obtain an embedding for each input sequence \( W_i \) by removing the MLM head and extracting the \texttt{768}-dimensional vector of a special \texttt{[CLS]} token. This \texttt{[CLS]} ("classification") token is specifically designed to represent the entire sequence in a single embedding, a common practice in BERT-based architectures to obtain a fixed-length representation of any input sequence \cite{bert2019}.

\subsection{Fine-Tuning the Clustering Model}
Once we obtain an embedding for each window \(W_i\) in the training data, we use these embeddings to measure the similarity between individual sequence vectors, build a "neighbors" dataset, and train a clustering model using this data.

First, for every input sequence we want to find and store a small set of examples that are most similar to it in the BERT embedding space (\cref{fig:training_pipeline} (2a)). Let \( \text{sim}(W_q, W_r) \) represent the cosine similarity between two embedding vectors \( W_q \) and \( W_r \), defined as:

\[
\text{sim}(W_q, W_r) = \frac{W_q \cdot W_r}{\|W_q\| \|W_r\|}
\]

\noindent For every input sequence \( W_i \), we identify its \( m \)-nearest neighbors using this cosine similarity. Following \cite{scan2020}, we set \( m \) equal to 20. These neighbors will be used as training examples in the following step where the SCAN loss function will be forcing them to be in the same cluster \( c_k\)

Then, we further fine-tune the re-trained BERT model from the previous step by redefining its training objective (\cref{fig:training_pipeline} (2b)). To achieve that, we replace the MLM training head and its cross-entropy loss function with a clustering head paired with the SCAN loss \cite{scan2020}. \textbf{SCAN loss} consists of two components, an \textbf{instance-level contrastive loss} and a \textbf{cluster-level entropy loss}, defined as follows:

\begin{itemize}

    \item 
    \textbf{Instance-Level Contrastive Loss}: Encourages every input sequence and its neighbors to belong to the same cluster. For a sequence \( W_i \), let \( M(W_i) \) denote its set of \( m \)-nearest neighbors. The contrastive loss \( \mathcal{L}_{\text{contrastive}} \) is defined as:

    \[
    \mathcal{L}_{\text{contrastive}} = - \frac{1}{n} \sum_{W_i} \frac{1}{|M(W_i)|} \sum_{W_j \in M(W_i)} \log P(c_i = c_j)
    \]
    
    where \( P(c_i = c_j) \) is the probability that \( W_i \) and \( W_j \) are assigned to the same cluster.

    \item \textbf{Cluster-Level Entropy Loss}: Encourages balanced cluster assignments by ensuring that the distribution of cluster labels is uniform. Let \( P_k \) denote the probability of sequences being assigned to cluster \( k \). The entropy loss \( \mathcal{L}_{\text{entropy}} \) is defined as:
    
    \[
    \mathcal{L}_{\text{entropy}} = - \sum_{k=1}^K P_k \log P_k
    \]

    \item \textbf{Combined SCAN Loss}: The total SCAN loss \( \mathcal{L}_{\text{SCAN}} \) is a sum of the two components:

    \[
    \mathcal{L}_{\text{SCAN}} = \mathcal{L}_{\text{contrastive}} + \lambda \mathcal{L}_{\text{entropy}}
    \]
    
    where \( \lambda \) controls the weight of the entropy loss. We use the default value of \( \lambda = 2\) \cite{scan2020}.

\end{itemize}

SCAN loss enables the BERT model to assign cluster labels 
\( c_i \in \{1, 2, \dots, k\} \) to each sequence \( W_i \). The clustering head is initialized 
with random labels and iteratively learns to group similar sequences together, thus refining the underlying embeddings into more distinct clusters. Along with the cluster labels (\cref{fig:training_pipeline} (2c)), the trained model provides a probability \( P(c_i = k) \), indicating how confidently each sequence 
\( W_i \) is assigned to cluster \( k \). In the end, every input window receives both a cluster assignment and a corresponding probability distribution over all clusters.


\subsection{Cluster Centroid Annotation}

\begin{figure}
    \centering
        \includegraphics[width=0.9\linewidth]{figures/house_map_replays.pdf}
    \caption{
    \ToolName{} custom-built interactive in-browser annotation tool for reviewing sensor activation sequences. The tool displays a 2D house layout, allowing annotators to replay sequences temporally, observe contextual and spatial details, and assign labels via a drop-down menu. The example above shows a sequence of sensor activations following a resident walking to the guest bathroom, that an annotator can label as "Guest Bathroom: Walking In" from the drop-down menu.
    }
    \label{fig:viz_tool}
\end{figure}

Given the set of clusters learned in the previous stage, the next goal is to interpret and contextualize them by assigning meaningful labels. Since the model has so far operated without any supervision, no ready-made labels exist to inform downstream analyses or practical applications. By annotating a small number of representative samples from each cluster, we can translate the resulting clusters into actionable units—such as identifying specific sub-activities or understanding when and where a resident might be performing a certain activity. This process not only validates the clusters internally but also bridges the gap between the automatic discovery of patterns and real-world interpretability.

To minimize the number of sequences we need to review, we will only use \( N \) samples from each cluster that have the highest probability of belonging to that cluster. The probability \( P(c_i = k) \) is taken directly from the SCAN model output. In step (3) in \cref{fig:main_image}, we select \( N \times k \) samples and review them manually using the custom tool that we developed, shown in \cref{fig:viz_tool}. This interactive in-browser annotation tool shows a detailed layout of a given house floor plan and allows to playback the sensor activation sequences temporally, and assign a label from a drop-down menu. This tool lets the annotator see both the contextual information about that sequence (e.g., location of the activity and its surrounding areas), and the temporal dimension (e.g., whether the activity was static, or if it triggered multiple sensor activations throughout the house). For example, \cref{fig:viz_tool} displays a sequence of sensor activations representing a resident walking through the house to the guest bathroom. The annotator can label this sequence as "Guest Bathroom: Walking In." \ToolName{} visualization tool simplifies and speeds up the annotation process and allows for faster iterations. 


\subsection{Label Propagation}
\label{sec:label_propagation}

Once each cluster is assigned a label based on its centroid samples, we propagate labels to all remaining sequences belonging to the cluster. This process creates a fully labeled dataset from initially unlabeled sequences. By grouping each sequence with its dominant cluster label, we enable efficient large-scale annotation of resident activities.

\subsection{Re-Annotation of the Original Time-Series Data}
\label{sec:reannotation}

Having assigned labels to each sequence, we then apply these annotations back to the original time-series data, preserving their chronological order. This comprehensive labeling allows researchers to analyze the temporal progression of activities and identify patterns or shifts in behavior over days, weeks, or longer periods. In doing so, our pipeline not only re-annotates large portions of unlabeled data but also lays the groundwork for an array of downstream tasks, such as behavior monitoring, anomaly detection, and personalized interventions. 
