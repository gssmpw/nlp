\section{Experimental Evaluation: Data}
\label{sec:data_prep}

Annotated sample sets are essential for HAR in smart homes, but they are often difficult to obtain. In this work, we present a method that assists human annotators in generating labeled datasets, potentially guiding data recording as well.
In the following sections, we first provide an overview of the datasets used for our experimental validation and evaluation in \cref{sec:datasets} and summarize the relevant pre-processing applied to the data in \cref{sec:data_preprocessing}. 
Then, we validate the quality and relevance of the outputs produced during the clustering stage (see \cref{fig:main_image}) in \cref{sec:results:clustering} -- establishing the effectiveness of our approach. 
Finally, we evaluate the quality of the Labeling stage of the pipeline and explore the quality and relevance of our finer-grained annotation scheme in \cref{sec:label_quality}.

\subsection{Datasets}
\label{sec:datasets}

\input{tables/casas_label_distribution}

Our experimental analysis uses publicly available labeled datasets from the Center of Advanced Studies in Adaptive Systems (CASAS) at the Washington State University \cite{casas2009}. 
The CASAS testbed consists of multiple household datasets, each containing motion (M), door (D), and temperature (T) sensor streams collected over a period of time that stretches multiple months, with associated activity labels. 
In this work, we perform analysis on three households that are widely used in prior HAR work \cite{deepcasas2018, tdost2024}:

\begin{itemize}
    \item \textit{Aruba:} A single-resident household that spans 220 days and includes M, D, and T sensors.
    
    \item \textit{Milan:} A single-resident household that spans 72 days and includes M, D, and T sensors.
    
    \item \textit{Cairo:} A two-story two resident household that spans 58 dates and contains only M and T sensors.
\end{itemize}

Each CASAS dataset includes assigned activity labels, capturing a range of household activities, such as cooking, relaxing, sleeping. While the labels broadly overlap across datasets, subtle discrepancies exist: for example, ``guest bathroom activity'' in Milan does not appear in Aruba or Cairo, as there are no guest bathroom sensors in those households. 
Following \cite{deepcasas2018, tdost2024}, we resolve these discrepancies by mapping each dataset's labels to a unified set of standard categories (e.g., \emph{Relax}, \emph{Cook}, \emph{Sleep}), as detailed in \cite{tdost2024}.

The activity label distribution is far from uniform, both within each dataset and between the households. For instance, in Milan, approximately 29\% of sensor readings correspond to cooking activities, whereas only 3\% are related to work. There's also significant inter-house variability: in Milan, \emph{Cook} accounts for 29\% of all data points, compared to only 17\% in Aruba and effectively none in Cairo.
All three datasets also incorporate an \emph{Other} category, that serves as a catch-all for both rare activities like \textit{grooming} and \textit{meditating} and unlabeled periods of sensor readings that do not fit any specified category. As shown in \cref{tab:casas_label_distribution}, the prevalence of \emph{Other} ranges widely, from 33\% of data points in Milan, to 53\% in Aruba and 77\% in Cairo. In the context of \ToolName{} pipeline, however, \emph{Other} is less useful as a standalone activity category, since even seemingly trivial movements such as walking between rooms could constitute a valid sub-activity. This distinction will be important when we benchmark \ToolName{} against fully-supervised state-of-the-art approaches.

\subsection{Data Pre-Processing}
\label{sec:data_preprocessing}

Following previous research into the optimal window size $l$ for classifying activities in CASAS \cite{krichnan_cook2014}, as well Hiremath's et al.'s prior works in this area \cite{hiremath2022bootstrapping}, we select $l = 20$ for all input windows $W_i$. We use sliding windows with a stride of size \texttt{1} to get sequence samples that only differ by one reading. Since using all of this input data would yield a overly redundant training dataset (which can lead to over-fitting \cite{nils_plotz_2015}), we uniformly sample 10\% of these sequences for training. To illustrate how this sampling works, consider the Milan dataset: there are \texttt{433,665} distinct sensor readings in total, which results in \texttt{433,665 - 19 = 433,646} potential input sequences. After taking a 10\% sample, we are left with approximately \texttt{43,364} data points.  
We follow the same methodology at test time on a holdout dataset. When running inference for pattern recognition (see \cref{sec:cluster_timeline}), we do not sample the windows and keep all of them to get a continuous representation of each sequence for cluster label change point detection.

Additionally, we adopt two heuristic restrictions on window sequence selection, aiming to more closely mirror potential real-world deployment scenarios. These heuristics enable us to sample a constant number of sensor readings (20 in our case) while still adhering to some common-sense temporal boundaries. Below is a list of heuristics applied during sequence sample generation:

\begin{enumerate}
    \item Do not create samples across day boundaries: we do not want to form sequences that begin on one day and end on the next one to prevent merging activity patterns that naturally reset with each new day.
    
    \item Do not sample with gaps of more than 30 minutes of inactivity to preserve temporal continuity and ensure that the sampled events reflect a cohesive activity period.
\end{enumerate}

To split the data the training and testing sets, we follow the "leave one day out" cross-validation strategy: we split the data by random days: 80\% of distinct days of data are kept for training, and 20\% are set aside for testing. By withholding entire days rather than arbitrary sequences, we reduce the risk of data leakage, where the temporal proximity of training and testing sequences might lead to overly optimistic performance estimates.

Instead of temporal split where we use the first 80\% of days for training and the last 20\% of days for testing, we chose to select 20\% of days at random. This decision was motivated by the fact that, for some datasets, the final portion of the year would coincide with the U.S. holiday season (late December and early January), during which residentsâ€™ activities differ substantially from other periods. By randomly sampling the test set days, we mitigate seasonal biases and avoid severely underestimating the pipeline performance.