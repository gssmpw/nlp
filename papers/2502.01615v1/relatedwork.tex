\section{Related Work}
\label{sec:background}

\subsection{Cognitive modeling and NLP}
\label{subsec:cog_model}


A key objective in linguistics is to understand how humans process language~\cite{Crocker2010-cp}, a goal that remains pertinent even in the era of LLMs. According to perspectives outlined by \citet{marr1982}, information processing can be examined at three levels:
(i) the computational level—\textit{what is the goal of computation?}, 
(ii) the algorithmic level—\textit{how does the model achieve the goal?}, and 
(iii) the implementational level—\textit{how is it physically implemented?}. 
Humans can be viewed as an information processing model, and surprisal theory~\cite{hale-2001-probabilistic,Levy2008Expectation-basedComprehension,Smith2013-ap} provides empirical evidence as a computational-level explanation of human sentence processing (\citealt{Smith2013-ap,FRANK20151,Shain2022-qv}; \textit{inter alia}).\footnote{Surprisal theory has also been critiqued~\cite{Van_Schijndel2021-sm,Huang2024-qe}, particularly for its failure to account for the cognitive load incurred in complex sentences. Our contribution will be orthogonal to such criticism (see Limitation section).} 
According to surprisal theory, humans continuously predict upcoming information during reading, with cognitive load incurred by unpredictable information.

Once the computational-level theory is assumed, the next question can be more relevant to the algorithmic level: 
\textit{with what kind of algorithms and representations, humans predict upcoming information}. 
The NLP community has developed various methods to compute next-word probabilities, ranging from N-gram models to LLMs. 
By exploring which NLP models best simulate human sentence processing, researchers can bridge the models demanded in psycholinguistics (language processing in humans) and those implemented in NLP (language processing in machines). 
This alignment is typically investigated by analyzing which LMs compute surprisal $-\log p(\mathrm{word}|\mathrm{context})$ that correlates with human measures (e.g., word-by-word gaze durations) based on the surprisal theory. 
Orthogonal to the exploration of various model architectures, this study investigates in which part/layer of a model human-like measures emerge. 

\subsection{Poorer fit of larger LMs' surprisal to human reading behavior}
\label{subsec:llm_cog}
Before the advent of LLMs, models such as incremental parsers~\cite{Levy2008Expectation-basedComprehension}, N-gram LMs~\cite{Smith2013-ap}, RNN-based LMs~\cite{frank2011insensitivity,Aurnhammer2019-fu}, Transformer-based LMs~\cite{Merkx2020ComparingData}, and syntactic LMs~\cite{Hale2018FindingSearch,Yoshida2021-rc,Oh2021-ln,Oh2022-ss} were extensively analyzed in cognitive modeling. 
In the days of much smaller LMs than modern LLMs, LM-scaling generally improved their ability to simulate human sentence processing data~\cite{frank2011insensitivity,Goodkind2018PredictiveQuality,Wilcox2020OnBehavior,Wilcox2023-bb}. 
However, recent studies have questioned the generality of this scaling effect. 
The reversed trend was first found in typologically distant languages~\cite{kuribayashi-etal-2021-lower}, and even within English, further scaling up LMs have shown weaker alignment with human reading behavior~\cite{kuribayashi-etal-2022-context,Shain2022-qv,Oh2023-zw}. 
This \textit{bigger is not always better} phenomenon has become a key focus of LM-based cognitive modeling~\cite{Wilcox2024-qx}, with researchers investigating why LLMs appear cognitively implausible~\cite{kuribayashi-etal-2022-context,Oh2023-hj,Oh2023-zw,Oh2024-cc,nair2023words,kuribayashi-etal-2024-psychometric}.
This study contributes to this discussion from a different perspective: 
We show the cognitively plausible properties of LLMs rather than solely identifying their limitations. 

In addition, recent studies have typically reported 
\textit{behavior–neurophysiology gap} in LM-scaling effects~\cite{michaelov2024revenge,aw2024instructiontuning};
 smaller LMs simulate reading behavior better~\cite{Oh2023-zw}, while larger LMs simulate neurophysiological data better~\cite{Schrimpf2020-qa,michaelov2024revenge,Hosseini2024-jf}. 
Opposite effect of instruction-tuning was also observed between  brain data and behavioral data~\cite{aw2024instructiontuning,kuribayashi-etal-2024-psychometric}. 
We begin by offering a perspective to address this gap, showing that the internal layers of larger LMs are more effective at modeling both behavioral and neurophysiological data.


\subsection{Mapping different human measures with different LM layers}
\label{subsec:layer_motivation}
We analyze the next-word predictions from the internal layers of LLMs in comparison with human sentence processing data. 
One motivation for this analysis is that different human measures, particularly at different time scales, may emphasize different stages of sentence processing~\cite{Witzel2012-nr,Lewis2005-hp,Vani2021-aj,Caucheteux2023-tx,McCurdy2024-ix}. 
LM internal layers, which are also computed sequentially, would be a natural counterpart to such multiple stages of processing~\cite{Tenney2019-bb}.
For example, eye movements reach the next word (or further) typically in $\sim$200ms before N400 brain signals peak at $\sim$400ms~\cite{Dimigen2011-gt}, suggesting that fast gaze durations may not reflect the cognitive load indexed by N400 signals~\cite{Rayner2009-aw}. 

Furthermore, self-paced reading and eye-tracking measures often exhibit \textit{spillover effects}, where the processing of one word influences subsequent words~\cite{Rayner1998-rw}. This suggests that the comprehension of a word extends beyond the immediate moment, and the associated reading times may only capture an early stage of processing.
In contrast, the MAZE task~\cite{Forster2009-dp,Boyce2023AmazeON}, which measures the time taken to select a plausible continuation from two candidates, mitigates spillover effects and is thus expected to reflect the full process of word processing. 
Our findings align with this perspective. Early LM layers are more closely aligned with gaze durations and self-paced reading times, while later layers show a stronger alignment with MAZE, as discussed in~\cref{sec:results}.

Note that similar layer-wise LM-brain alignments have been attempted in fMRI modeling research~\cite{Toneva2019-ul,Schrimpf2020-qa,Caucheteux2023-tx} and suggested that different brain areas better align with different LM layers.
They typically trained linear regression models to predict brain activity directly using $d$-dimentional LM internal representations $\bm h \in \mathbb{R}^d$ as features, instead of using surprisal measures $-\log p(\mathrm{word}|\mathrm{context})\in \mathbb{R}_{\ge 0}$.
Thus, their results are not comparable with exitsing surprisal-based studies and may rather suffer from a confounding factor of different $d$ for different LMs when precisely discussing LM-scaling effects. 

\subsection{Early exits of neural models' prediction}
\label{subsec:adaptive}
In the engineering context, predictions from internal layers of deep neural networks (i.e., early exists of the results from internal layers) are used to improve inference efficiency by avoiding their overthiking~\cite{Graves2016-dp,Banino2021-oq}, which is also called adaptive computation time~\cite{icml19shallowdeepnetworks,Zhou2020-vz}. Such technique has also recently been employed to enhance interpretability research to identify at which layer a particular prediction shapes~\cite{logitlens,dar-etal-2023-analyzing,belrose2023eliciting}. 
We apply \textit{logit lens}~\cite{logitlens} and its variant of \textit{tuned lens}~\cite{belrose2023eliciting}, projecting intermediate representations to the output space, to extract next-word predictions from the internal layers of LLMs.